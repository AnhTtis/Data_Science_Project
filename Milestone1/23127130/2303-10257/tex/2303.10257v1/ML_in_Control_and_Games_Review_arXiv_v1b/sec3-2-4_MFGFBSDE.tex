
We now explain how to adapt the Deep BSDE method introduced in~\cite{MR3736669} and  reviewed in Section~\ref{subsubsec:deepBSDE}  to mean-field FBSDEs. We recall that the principle of the method is to use neural networks to approximate $Y_0$ and $(Z_t)_{t \in [0,T]}$ and to train the neural network parameters by relying on Monte Carlo samples until the terminal condition is approximately matched. In the mean-field setting, the same idea can be used to solve forward-backward systems of McKean-Vlasov (MKV) SDEs;  see~\cite{fouque2019deep,carmona2019convergence2,germain2019numerical, hanhulong:22}. 


Let us consider the FBSDE~\eqref{eq:MKV-FBSDE-general-no-CN} in the absence of common noise, with interactions through the state distribution only, and uncontrolled volatility. We rewrite the problem as: minimize over $y_0 : \RR^d \to \RR^d$ and $z: \RR_+ \times \RR^d \to \RR^{d \times m}$ the cost functional
$$
	J(y_0, z) = \EE \left[ \, \left| Y^{y_0,z}_T - G(X^{y_0,z}_T, \Law(X^{y_0,z}_T)) \right|^2 \, \right],
$$
where $(X^{y_0,z}, Y^{y_0,z})$ solves
\begin{equation}
\label{eq:MKV-FBSDE-2forward}
\begin{cases}
\ud X^{y_0,z}_t
	=
	B\left(t, X^{y_0,z}_t, \Law(X^{y_0,z}_t), Y^{y_0,z}_t \right) \ud t
		+  \sigma(t, X_t) \ud  W_t, \quad t \ge 0,
	\\
	\ud  Y^{y_0,z}_t
	=
	- F\left(t, X^{y_0,z}_t, \Law(X^{y_0,z}_t), Y^{y_0,z}_t, \sigma\transpose(t, X_t) z(t, X^{y_0,z}_t)  \right) \ud t
		+ z(t, X^{y_0,z}_t) \ud  W_t, \quad t \ge 0, 
  \\
  X^{y_0,z}_0 \sim \mu_0, \qquad Y^{y_0,z}_0 = y_0 (X_0^{y_0,z}). 
\end{cases}
\end{equation}
The above problem is an MFC problem if we view $(X^{y_0,z}_t,Y^{y_0,z}_t)$ as state and $(y_0,z)$ as control. Under suitable conditions, the optimally controlled process $(X,Y)$ solves the MKV FBSDE system~\eqref{eq:MKV-FBSDE-general-no-CN} and vice versa. 


Then, to be able to implement this method, we can proceed similarly to the method described in Section~\ref{sec:directMethod}. The mean field distribution can be approximated by an empirical distribution based on a finite population of interacting particles. Furthermore, the controls $y_0$ and $z$ can be replaced by neural networks, say $y_{\theta}$ and $z_{\omega}$ with parameters $\theta$ and $\omega$ respectively. Time can be discretized using for instance an Euler-Maruyama scheme. We thus obtain a new optimization problem over finite-dimensional parameters that can be adjusted using SGD. %


\begin{remark}[Theoretical analysis]
    Motivated by numerical schemes and in particular the above adaptation of the deep BSDE method \cite{MR3736669,HaJeE:18} to MKV FBSDEs, Reisinger, Stockinger and Zhang
    in~\cite{reisinger2020posteriori} analyzed a posteriori error for approximate solutions based on a discrete time scheme and a finite population of interacting particles. \cite{hanhulong:22} proposed a deep learning method for computing MKV FBSDEs with a general form of mean-field interactions, and proved that the convergence of the proposed method is free of the curse of dimensionality by using a special class of integral probability metrics previously developed in \cite{hanhulong:21}.
\end{remark}

 Although we focus here on the continuous-state space setting, the same strategy can be applied to finite-state MFGs;  see, {\textit e.g.},~\cite{aurell2022optimalstackelberg,aurell2022finitegraphon}. 























\paragraph*{Numerical illustration: linear-quadratic mean-field game in systemic risk problems.} 
\label{sec:MFG-sysrisk-example-deepBSDE-CN}
We now consider the MFG version of the systemic risk model introduced in Section~\ref{sec:intro-LQsysrisk} which has been studied in Section~\ref{par:LQ-systemic-risk-finite-N} and revisited in Section~\ref{sec:LQsysrisk-revisited}. Thsi MFG mean-field game has been analyzed in~\cite{CaFoSu:15}. Given a mean-field flow $\mu = (\mu_t)_{t\in[0,T]}$, the log-monetary reserves of a typical bank evolves according to the dynamics:
\begin{equation}
\label{eq:ex1_dynamics-mfg}
    \ud X_t = [a(\bar \mu_t - X_t)   + \alpha_t ] \ud t  + \sigma \left(\rho \ud W_t^0 + \sqrt{1-\rho^2} \ud W_t\right). %
\end{equation}
The standard Brownian motions $W^0$ and $W$ are independent, in which $W$ stands for the idiosyncratic noises and $W^0$ denotes the systemic shock, which is an example of common noise (see also Section~\ref{sec4_MFG_with_CN}).

The cost functions $f$ and $g$ appearing in \eqref{def:MFG-cost} takes the following form:
\begin{equation}
f(t, x, \nu,\alpha) = \half \alpha^2 - q \alpha(\bar \mu - x) + \frac{\eps}{2}(\bar \mu - x)^2,  \quad g(x, \nu) = \frac{c}{2}(\bar \mu - x)^2,
\end{equation}
which depend only on the mean $\bar \mu = \EE_{X\sim\mu}[X]$ of the first marginal of the state-action distribution $\nu$. 
It has been shown in~\cite{CaFoSu:15} that, in the MFG setting, the open-loop equilibrium is the same as the closed-loop Nash equilibrium, and it admit an explicit solution. Furthermore, it can be characterized using an MKV FBSDE system, that we omit for brevity; see~\cite{CaFoSu:15} for the details. If $\rho = 0$, then one can apply directly the method described above, with $y_0$ a function of $X_0$ and $z$ a function of $(t,X_t)$. When $\rho>0$, two changes need to be made: first, there is an extra process $Z^0$ to be learned, for which we use a neural network approximation as for $Z$; second, we expect the random variables $Z_t$ and $Z^0_t$ to depend not only on $X_t$ but also on the past of the common noise. In general, this would mean learning functions of the common noise's trajectory. However, in the present case, it is enough to rely on finite-dimensional information. Here, we add $\bar{\mu}_t$ as an input to the neural networks playing the roles of  $Z_t$ and $Z^0_t$, and this is sufficient to learn the optimal solution, see~\cite{CaFoSu:15}.


Figure~\ref{fig:ex-mfg-sysrisk-traj} displays three sample trajectories of $X$ and $Y$, obtained after training the neural networks for $Y_0, Z$ and $Z^0$, by simulating in a forward fashion the trajectories of $X$ and $Y$ using Monte Carlo samples and the same Euler-Maruyama scheme used in the numerical method. One can see that the approximation is better for $X$ than for $Y$, particularly towards the end of the time interval. This is probably because the BSDE is solved by guessing the initial point instead of starting from a terminal point, resulting in errors accumulated over time. Furthermore, \cite{carmona2019convergence2} shows that the results improve as the number of time steps, particles, and units in the neural network increase. In the numerical experiments presented here, we used the following parameters: $\sigma = 0.5, \rho = 0.5, q = 0.5, \epsilon = q^2 + 0.5 = 0.75, a = 1, c = 1.0$ and $T = 0.5$.


\begin{figure}[ht]
\centering
\subfloat[Trajectory of $X^i, i=1,2,3$]{
\includegraphics[width=.45\linewidth]{{figure/SYSRISKFBSDE/test2o_cmp_trajX_forHandbookMLFinance}.pdf}
}
\subfloat[Trajectory of $Y^i, i=1,2,3$] {
  \includegraphics[width=.45\linewidth]{{figure/SYSRISKFBSDE/test2o_cmp_trajY_forHandbookMLFinance}.pdf}
  }
\caption{Systemic risk MFG example solved by the algorithm described in Section~\ref{sec4_MFGFBSDE}. Left: three sample trajectories of $X$ using the neural network approach ('Deep Solver' with full lines, in cyan, blue, and green) or using the analytical formula ('benchmark' with dashed lines, in orange, red and purple). Right: three sample trajectories of $Y$ (similar labels and colors). Note that the analytical formula satisfies the true terminal condition, whereas the solution computed by neural networks satisfies it only approximately since the trajectories are generated in a forward way starting from the learnt initial condition. %
}
\label{fig:ex-mfg-sysrisk-traj}
\end{figure}




