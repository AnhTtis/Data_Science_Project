


Problem \eqref{eq:rl-Xt-discrete}--\eqref{eq:rl-Xt-cost} can be recast as an MDP, which is a tuple $(\mc{X},\mc{A},p, f, g, N_T)$, where
\begin{itemize}
    \item $\mc{X}$ is the set of states called  the {state space};
    \item $\mc{A}$ is the set of actions called the {action space};
    \item $N_T<+\infty$ is the time horizon;
    \item $p: \mc{X} \times \mc{A} \times \{0,\Delta t, 2\Delta t, \dots,T\} \to \mathcal{P}(\mc{X})$ is the transition kernel, and $p(\cdot \vert x, a, t_n)$ is a probability density function; 
    \item $f:\{0,\Delta t, 2\Delta t, \dots,T\} \times \mc{X} \times \mc{A} \to \mathbb{R}$ is the one-step cost function, and $f(t_n, x, a)$ is the immediate cost at time $t_n$ at state $x$ due to action $a$;
    \item $g: \mc{X} \to \mathbb{R}$ is the terminal cost function, and $g(x)$ is the terminal cost at the final time $N_T$.
\end{itemize} 
 A large part of the RL literature focuses on the infinite horizon setting with discounted cost. Furthermore, the state space is often discrete, in which case  $p(x' \vert x, a, t_n) = \PP(X_{t_{n+1}} = x' \vert X_{t_n} = x, \alpha_{t_n} = a)$ is the probability to go to state $x'$ at time $t_{n+1}$ if at time $t_n$ the state $x$ and the action is $a$. 
 However, for the sake of consistency with the previous sections and the literature on optimal control, we stick to the finite horizon and continuous space setting in this section.



In model-free RL, the agent typically uses multiple episodes to learn the control that optimizes \eqref{eq:rl-Xt-discrete} with a simulator. In one episode of learning, the agent-environment interaction is as follows: Starting with $X_0 \in \mc{X}$, the agent chooses $\alpha_0 \in \mc{A}$, pays a one-step cost $f(0, X_0, \alpha_0)\Delta t$ and finds herself in a new state $X_{t_1}$; the process continues, forming a sequence:
\begin{equation}\label{eq:rl-sequence}
    X_0, \; \alpha_0, \; f(0, X_0, \alpha_0)\Delta t, \; X_{t_1}, \; \alpha_{t_1}, \; f(t_1, X_{t_1}, \; \alpha_{t_1})\Delta t, \; \ldots,\; X_T, \; g(X_T).    
\end{equation}


Under the Euler scheme \eqref{eq:rl-Xt-discrete}, given the state-action pair $(\check X_{t_n}, \alpha_{t_n}) = (x, a)$ at time $t_n$, $X_{t_{n+1}}$ follows a normal distribution $\mc{N}(x + b(t_n, x, a)\Delta t, \sigma^2(t_n, x, a)\Delta t)$.





In RL, there are four main components: policy, reward signal, value function, and optionally, a model of the environment. The MDP provides a mathematical framework to describe the agent-environment interface. A \emph{policy} $\pi: \{0,\Delta t, 2\Delta t, \dots,T\} \times \mc{X} \to \mc{P}(\mc{A})$ is a mapping from the state space to the probability space of the action space, and $\pi_t(a\vert x)$ describes the probability of choosing action $a$ at state $x$, which is in general random. 

The \emph{value function} associated to a specific policy $\pi$ is denoted by $V^\pi_t$ and defined as the expected cost when starting from $x$ at time $t$ and following $\pi$ thereafter, {\it i.e.},
\begin{equation}
    V_{t_n}^\pi(x) = \EE^\pi\left[\sum_{j=n}^{N_T-1} f(t_j, X_{t_j}, \alpha_{t_j})\Delta t + g(X_T) \vert X_{t_n} = x\right],
\end{equation}
where the superscript $\pi$ over the expectation means that, at each time step, the action is sampled according to $\pi$. 
Similarly, the \emph{action-value function} $Q^\pi_t$ associated to $\pi$ is defined as the expected cost when starting from $x$ at time $t$, taking the action $a$ and then following $\pi$, {\it i.e.},
\begin{equation}
    Q^\pi_{t_n}(x, a) = \EE^\pi\left[\sum_{j=n}^{N_T-1} f(t_j, X_{t_j}, \alpha_{t_j})\Delta t + g(X_T) \vert X_{t_n} = x, \; \alpha_{t_n} = a\right].
\end{equation}
Both functions satisfy the dynamic programming equations (also called Bellman equations),
\begin{align}
    &V^\pi_{t_n}(x) = \int_{a \in \mc{A}}\pi_{t_n}(a\vert x) \int_{x' \in \mc{X}} p(x' \vert x, a, t_n)[f(t_n, x, a)\Delta t + V^\pi_{t_{n+1}}(x')] \ud x' \ud a, \\
    & Q_{t_n}^\pi(x, a) = \int_{x' \in \mc{X}} p(x' \vert x, a, t_n) [f(t_n, x, a) \Delta t + V_{t_{n+1}}^\pi(x')] \ud x',
\end{align}
with terminal conditions $V_T^\pi(x) = Q_T^\pi(x, a) = g(x)$, where we have simplified the subscript $t_{N_T} = T$. The goal of RL is to identify the optimal $\pi^\ast = (\pi^\ast_t)_{t}$ that minimizes $V_t^\pi(x)$ for every $t$ and $x \in \mc{X}$. 

To this end, one also works with the \emph{optimal value function} $V^\ast_t(x) = \inf_\pi V_t^\pi(x)$ and the \emph{optimal action-value function} defined as $Q^\ast_t(x, a ) = \inf_\pi Q^\pi_t(x, a)$, which satisfy and the optimal Bellman equation reads,
\begin{align}
    &V^\ast_{t_n}(x) = \inf_{a \in \mc{A}} \int_{x' \in \mc{X}} p(x' \vert x, a, t_n)[f(t_n, x, a) \Delta t + V^\ast_{t_{n+1}}(x')] \ud x',
    \\
    &Q^\ast_{t_n}(x, a) = f(t_n, x, a) \Delta t +  \int_{x' \in \mc{X}} p(x' \vert x, a, t_n) \inf_{a' \in \mc{A}} Q^\ast_{t_{n+1}}(x', a') \ud x',
\end{align}
with terminal conditions $V_T^\ast(x) = Q_T^\ast(x, a) = g(x)$.

Model-free RL aims at computing $\pi^*$ without using the knowledge of the transition probability kernel $p$, and by instead relying on samples of transitions $X_{t_{n+1}} \sim p(x' \vert x, a, t_n)$. 
There are primarily two categories of learning methods: value-based methods and policy gradient methods. In the continuous time framework, the connection between policy evaluation in value-based methods and policy gradient methods has been developed in \cite{jia2021policy,jia2021policy2}.





\paragraph{Value-based methods}
For value-based methods, the workflow can be summarized as follows: starting with an arbitrary policy $\pi$, evaluate its value, improve the policy, and repeat until convergence:
\begin{equation}
    \pi_0 \to V^{\pi_0} \to \pi_1 \to V^{\pi_1} \to \ldots \pi_\ast \to V^{\ast}.
\end{equation}
The symbol $\pi_{\mt{k}} \to V^{\pi_{\mt{k}}}$ denotes a \emph{policy evaluation}, and the symbol $\pi_{\mt{k}} \to V^{\pi_{\mt{k}+1}}$ denotes a \emph{policy improvement}. 
Evaluating a given policy $\pi_{\mt{k}} \to V^{\pi_{\mt{k}}}$ exactly is not possible since we assume that $p(\cdot\vert x, a, t_n)$ is unknown. The Temporal-Difference (TD) learning is remedy this issue by updating $V^\pi_{t_n}(x)$ with one sample drawn according to $X_{t_{n+1}} \sim p(x' \vert x, a, t_n)$,
$$
V^\pi_{t_n}(X_{t_n}) \leftarrow V^\pi_{t_n}(X_{t_n}) + \beta[f(t_n, X_{t_n}, \alpha_{t_n})\Delta t + V_{t_{n+1}}^\pi(X_{t_{n+1}}) - V_{t_n}^\pi(X_{t_n})],
$$
where $\beta>0$ is a learning rate. 
This is the simplest TD method, usually denoted by TD(0). To unify TD methods and MC methods, one can view the later as updating $V^\pi_{t_n}$ using the entire sequence of observed cost from time $t_n$ until the end of the episode $T$. The $n$-step TD method lies in between and consists in simulating $n$ Monte Carlo samples to update $V^\pi$.

TD learning can also be applied to action-value function, for example by using the update rule:
$$
    Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n}) \leftarrow Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n}) + \beta[f(t_n, X_{t_n}, \alpha_{t_n})\Delta t + Q^{\pi}_{t_{n+1}}(X_{t_{n+1}}, \alpha_{t_{n+1}}) - Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n})], 
$$
where $X_{t_{n+1}}, \alpha_{t_{n+1}}$ are random samples from \eqref{eq:rl-Xt-discrete} and from $Q$ plus some randomization using for instance the $\eps$-greedy policy, which picks the currently optimal action with probability $1-\epsilon$ and, with probability $\epsilon$, picks an action uniformly at random. This approach is called SARSA.
Then the optimal action-value function $Q^\ast$ can be learnt as follows: choose $\alpha_{t_n}$ according to $Q$ plus $\epsilon$-greedy for some exploration, then update $Q$ using SARSA. This method falls into the category of \emph{on-policy} algorithms since it evaluates or improves the policy that is used to make decisions. In fact, it uses an $\epsilon$-greedy policy to balance between learning an optimal behavior and behaving non-optimally for exploration, so it learns the value function for a sub-optimal policy that still explores. \emph{Off-policy} methods, on the contrary, uses different policies for evaluation and data generation. \emph{Q-learning} may be the earliest well known off-policy algorithm, which directly approximates $Q^\ast$ using the update rule:
$$
Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n}) \leftarrow Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n}) + \beta[f(t_n, X_{t_n}, \alpha_{t_n})\Delta t + \max_a Q^{\pi}_{t_{n+1}}(X_{t_{n+1}}, a) - Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n})].
$$





\paragraph{Policy gradient methods}\label{sec:MDP-PGM}

This section describes some methods that aim at directly learning an optimal policy without deducing it from the value function. The use a parameterized class of policies. We denote by $\pi_t(a\vert x; \theta)$ the probability of taking action $a$ at state $x$ with parameter $\theta$. In practice, this can be a linear function $\theta\transpose\mathrm{f}(x, a)$ where $\mathrm{f}(x, a)$ is called feature vector, or a neural network taking $x$ as input and outputting a probability distribution over actions. Policy gradient methods update the policy parameter $\theta$ based on the gradient of some performance measure $L(\theta)$, with updates of the form
$$
    \theta \leftarrow \theta - \beta \widehat{\nabla J(\theta)},
$$
where $\widehat{\nabla L(\theta)}$ denotes an estimation of $\nabla L(\theta)$ based on Monte Carlo samples. A natural choice of $L(\theta)$ is the value function $V^{\pi_\theta}$ we aim to minimize. According to the policy gradient theorem,
$$
    \nabla V^{\pi_\theta}_{t_n}(x) = \EE_\pi \left[\int_{\mc{A}} Q^\pi_{t_n}(x, a)\nabla \pi_{t_n}(a\vert x; \theta) \ud a\right].
$$
Multiplying the first term by $\pi_{t_n}(a \vert x; \theta)$, dividing the second term by $\pi_{t_n}(a \vert x; \theta)$, replacing $a$ by a sample $\alpha_{t_n}$, and using $\EE_\pi[G_{t_n}\vert x, a] = Q^\pi_{t_n}(x, a)$ leads to the REINFORCE algorithm~\cite{williams1992simple},
$$
\theta \leftarrow \theta - \beta G_{t_n} \frac{\nabla_\theta \pi_{t_n}(\alpha_{t_{n}}\vert X_{t_n}; \theta)}{\pi_{t_n}(\alpha_{t_{n}}\vert X_{t_n}; \theta)},
$$
where $G_{t_n} = \sum_{n' = n+1}^{N_T-1} f(t_{n'}, \check X_{t_{n'}}, \alpha_{t_{n'}})\Delta t + g(\check X_T)$ denotes the cumulated cost from time $t_{n+1}$ to $T$.


\begin{remark}[Theoretical analysis]
Convergence of policy gradient has been studied in various settings. As for the model-based framework, LQ problems have attracted a particular interest since the optimal control can be written as a linear function of the state. Global convergence in in the infinite horizon setting has been proved by Fazel et al. in~\cite{fazel2018global}. This result has been extended in various directions, such as the finite horizon setting in~\cite{hambly2021policy}, the neural setting \cite{wangneural}, or problems with entropy regularization \cite{cen2022fast}, to cite just a few examples.
\end{remark}


With an additional parameterized value function $V_{t_n}(x; \theta')$, this leads to the actor-critic algorithm (see, {\it e.g.}, \cite[Section~13.5]{sutton2018reinforcement} or \cite{degris2012off}), 
\begin{align}
& \delta_{t_n} = f(t_n, X_{t_n}, \alpha_{t_n})\Delta t + V_{t_{n+1}}(X_{t_{n+1}}; \theta') - V_{t_n}(X_{t_n}; \theta'),\\
&\theta' \leftarrow \theta' - \beta' \delta_{t_n}\nabla_{\theta'} V_{t_n}(X_{t_n}; \theta'),\\
&\theta \leftarrow \theta - \beta \delta_{t_n} \nabla_\theta \ln \pi_{t_n}(\alpha_{t_n}\vert X_{t_n}; \theta).
\end{align}

Both REINFORCE and actor-critic methods mentioned above stochastically select an action $a$ when in state $x$ according to the parameter $\theta$, {\textit i.e.}, $a \sim \pi_{t_n}(\cdot \vert x, \theta)$. For some problems, it is more appropriate to look for a deterministic policy $\alpha_{t_n}(x; \theta) \in \mc{A}$. To ensure exploration, one can use an off-policy approach: a stochastic policy $\tilde\pi_{t_n}(a\vert x)$ is used to choose the action, and a deterministic policy $\alpha_{t_n}(x; \theta)$ is learned to approximate the optimal one.
An example of such method is the deterministic policy gradient (DPG) \cite{pmlr-v32-silver14}, which is an off-policy actor-critic algorithm that learns a deterministic target policy $\alpha_{t_n}(x; \theta)$ from an exploratory behavior policy $\tilde\pi_{t_n}(a\vert x)$. In particular, a differentiable critic $Q(x, a; \theta')$ is used to approximate $Q^{\alpha(\cdot;\theta)}(x, a)$ and is updated via Q-learning: at each step we  sample $\alpha_{t_n}$ from $\tilde\pi_{t_n}(a\vert x)$ and
\begin{align}
    & \delta_{t_n} = f(t_n, X_{t_n}, \alpha_{t_n})\Delta t + Q_{t_{n+1}}(X_{t_{n+1}}, \alpha_{t_{n+1}}(X_{t_{n+1}}; \theta); \theta') - Q_{t_n}(X_{t_n}, \alpha_{t_n}; \theta'),\\
&\theta' \leftarrow \theta' - \beta' \delta_{t_n}\nabla_{\theta'} Q_{t_n}(X_{t_n}, \alpha_{t_n}; \theta'),\\
&\theta \leftarrow \theta - \beta \delta_{t_n} \nabla_\theta \alpha_{t_n}(X_{t_n}; \theta)\nabla_a Q(X_{t_n}, \alpha_{t_n}; \theta')\vert_{a = \alpha_{t_n}(X_{t_n}; \theta)}.
\end{align}



When using neural networks to approximate $Q^{\alpha(\cdot;\theta)}$ and the deterministic policy $\alpha_{t_n}(x; \theta)$, one can use the Deep DPG (DDPG) algorithm \cite{lillicrap2015continuous}, which is based on the same intuition as DPG. For the sake of robustness it uses the ``replay-buffer'' idea borrowed from the Deep Q Network (DQN) algorithm, see~\cite{mnih2015human}: the network parameters are learnt in mini-batches rather than online by using a replay buffer, so that correlation between samples are kept minimal. Another pair of networks $Q'(x, a; \hat \theta')$ and $\alpha'_{t_n}(x; \hat \theta)$ are copied from $Q(x, a; \theta')$ and $\alpha_{t_n}(x; \theta)$ for calculating the target value in order to improve the stability. At each step, an action $\alpha_{t_n}$ is sampled from $\alpha_{t_n}(X_{t_n}; \theta) + \mc{N}_{t_n}$ where $\mc{N}_{t}$ is a noise process for exploration; then the cost $f(t_n, X_{t_n}, \alpha_{t_n})\Delta t$ and the new state $X_{t_{n+1}}$ are observed and saved to the buffer. A mini-batch of $N$ transitions $(X_{t_n}, \alpha_{t_n}, f, X_{t_{n+1}})$ are sampled from the buffer, acting as supervised learning data for the critic $Q(x, a; \theta')$. The loss to be minimized is the mean-squared error of $Q_{t_n}(X_{t_n}, \alpha_{t_n}; \theta')$ and $f(t_n, X_{t_n}, \alpha_{t_n})\Delta t +  Q'_{t_{n+1}}(X_{t_{n+1}}, \alpha'_{t_{n+1}}(X_{t_{n+1}}; \hat \theta); \hat\theta')$. The actor network and both copies are updated via
\begin{align}
 & \theta \leftarrow \theta - \beta \frac{1}{N} \sum_{i}\nabla_a Q_{t_n}(x,a;\theta')\vert_{x = X_{t_n}^i, a = \alpha_{t_n}(X_{t_n}^i; \theta)} \nabla_\theta \alpha_{t_n}(X_{t_n}; \theta), \\
 & \hat \theta' \leftarrow \tau \theta' + (1-\tau) \hat \theta', \quad \hat \theta \leftarrow \tau \theta + (1-\tau) \hat \theta,
\end{align}
where the superscript $i$ indicates the $i^{th}$ sample from the mini-batch, and $\tau \ll 1$ is used to slowly track the learnt counterparts $\theta$ and $\theta'$.


