
Let $T<\infty$ be a time horizon. Let $(\Omega, \MCF, \PP)$ be a probability space supporting an $m$-dimensional Brownian motion $W = (W_t)_{t \in [0,T]}$, and $\mathbb{F} = (\mc{F}_t)_{t\in[0,T]}$ be the natural filtration generated by $W$. In the most common case, a stochastic control problem is formulated as follows. Let $d$ and $k$ be integers for the dimensions of the state and the action. Let $\mc{A} \subset \RR^k$ denote the set of admissible actions. Let $b, \sigma, f, g$ be Borel-measurable functions,
\begin{equation}
\label{def:oc-bsigmaf}
    (b, \sigma, f) : [0,T] \times \RR^d \times \mc{A} \to (\RR^d, \RR^{d \times m}, \RR), \quad g : \RR^d \to \RR.
\end{equation}
We denote by $\mathbb{A}$ the set of so-called admissible controls. This set describes the integrability and measurability conditions required on $\alpha$. We usually require that $\alpha$ is square-integrable. For measurability, two popular choices are that either $\alpha$ should be a $\mc{F}_t$-progressively measurable process, or $\alpha$ should be expressed as a measurable function of $(t, X_t)$. The former case is called \emph{adaptive controls} (also called \emph{open-loop}) while the latter one is referred to as \emph{Markovian controls} or \emph{closed-loop controls in feedback form} (we use the two terms interchangeably in the sequel).  We will sometimes consider \emph{closed-loop controls}, which means controls that are adapted to the filtration generated by $X$. 
We shall see that different choices of $\mathbb{A}$ might lead to different algorithm designs in Section~\ref{sec:control-direct}. 

\begin{defi}[Stochastic control problem]
An agent controls her state process $X$ through an action process $\alpha$ taking values respectively in $\RR^d$ and $\mc{A}$, where the dynamics of $X$ are given by the stochastic differential equation (SDE), 
\begin{equation}\label{def:control-Xt}
    \ud X_t = b(t, X_t, \alpha_t) \ud t + \sigma(t, X_t, \alpha_t) \ud W_t, \quad X_0 = x_0.
\end{equation}
The agent aims to minimize the expected cost
\begin{equation}\label{def:control-cost}
    J(\alpha): \ctrl \mapsto \EE \left[\int_0^T f(t, X_t, \alpha_t) \ud t + g(X_T) \right],
\end{equation}
over the set of admissible action processes, denoted by $\mathbb{A}$ and to be discussed below. 
\end{defi}




We briefly describe how the above problem can be tackled by PDEs, or (F)BSDEs.

\noindent
\textbf{PDE approach. } When considering Markovian controls, one can define the value function $u:[0,T] \times \RR^d \to \RR$,
\begin{equation}
    u(t, x) = \inf_\alpha \EE\left[\int_t^T f(s, X_s, \alpha_s) \ud s + g(X_T) \vert X_t = x\right], 
\end{equation}
and employ the dynamic programming principle (DPP) \cite[Section~3]{PH:09}: $u(T, x) = g(x)$ and for any stopping time $\tau \in [t, T]$ 
\begin{equation}
    u(t,x) = \inf_\alpha \EE\left[\int_t^\tau f(s, X_s, \alpha_s) \ud s + u(\tau, X_\tau) \vert X_t = x\right].
\end{equation}
Then one can derive the Hamilton-Jacobi-Bellman (HJB) equation, which describes the evolution of the value function. Under suitable conditions, $u$ solves
\begin{equation}\label{def:control-HJB}
    \begin{cases}
   	\partial_t u (t,x)  + \min_{\alpha \in \mc{A}} H(t, x, \grad_x u(t,x), \Hess_x u(t, x), \alpha) = 0, \qquad (t,x) \in [0,T) \times \RR^d 
   	\\
   	u(T,x) = g(x), \qquad x \in \RR^d
   	\end{cases}
\end{equation}
where the Hamiltonian $H$ is defined as
\begin{equation}\label{def:control-H}
    H(t, x, p, q, \alpha) = b(t, x, \alpha) \cdot p + \half \Tr(\sigma(t, x, \alpha)\sigma(t,x, \alpha)\transpose q) + f(t, x, \alpha),
\end{equation} 
and $\Hess_x u(t, x)$ as the Hessian matrix of $u$ with respect to $x$. 
If \eqref{def:control-HJB} has a classical solution, then the optimal control is given by
\begin{equation*}
	\hat{\ctrl}(t,x) = \hat{\ctrl}(t, x, \grad_x u(t,x), \Hess_x u(t,x)) = 
	\argmin_{\alpha \in \mc{A}} H(t, x, \grad_x u(t, x), \Hess_x u(t, x), \alpha).
\end{equation*}

\noindent
\textbf{BSDE approach. } The connection between SC and BSDEs can be established in two different ways: by representing the value function or its derivative as the solution of a BSDE. When the volatility is uncontrolled,  that is, $\sigma(t, x, \alpha)$ is free of $\alpha$, then $\hat \ctrl$ does not depend on $\Hess_x u(t,x)$ and the PDE \eqref{def:control-HJB} becomes semi-linear
\begin{multline}
	\partial_t u (t,x)  + \half \Tr(\sigma(t, x)\sigma(t,x)\transpose \Hess_x u(t, x)) + b(t, x, 	\hat{\ctrl}(t, x, \grad_x u(t,x))) \cdot  \grad_x u(t,x) \\
	+ f(t, x, \hat{\ctrl}(t, x, \grad_x u(t,x))) = 0.
\end{multline}
In this case, suppose that there exist functions $\tilde b(t, x)$ and $h(t, x, z)$ such that 
$$
    \tilde b(t, x) \cdot \grad_x u(t,x) + h(t, x, \sigma(t,x) \transpose \grad_x u(t,x)) = b(t, x, \hat{\ctrl}(t, x, \grad_x u(t,x))) \cdot  \grad_x u(t,x) + f(t, x, \hat{\ctrl}(t, x, \grad_x u(t,x))).
$$
Then the nonlinear Feynman-Kac formula \cite{PaPe:90} gives the following BSDE interpretation of $u$,
\begin{equation}\label{def_control_BSDEreform}
    \begin{dcases}
    \ud \mc{X}_t = \tilde b(t, \mc{X}_t) \ud t + \sigma(t, \mc{X}_t) \ud W_t, \quad \mc{X}_0 \sim \mu_0, \\
    \ud \mc{Y}_t = -h(t, \mc{X}_t, \mc{Z}_t)\ud t + \mc{Z}_t \ud W_t, \quad \mc{Y}_T = g(\mc{X}_T),
    \end{dcases}
\end{equation}
through the relations
$$
    \mc{Y}_t = u(t, \mc{X}_t), \quad \mc{Z}_t = \sigma(t, \mc{X}_t) \transpose \grad_x u(t, \mc{X}_t).
$$
If $\mu_0 = \delta_{x_0}$ is concentrated on a single initial state $x_0$, then the optimal value is given by the value of the BSDE solution at time $0$, {\it i.e.}, $\inf_{\alpha} J(\alpha) = \mc{Y}_0$. If $\tilde b(t, x)$ is chosen to be identically zero, this equality can also be obtained by the comparison principle of BSDEs \cite[Proposition 4.1]{Ca:16}.

In the controlled volatility case, the PDE \eqref{def:control-HJB} is fully nonlinear, and its solution is connected to a solution of the second order BSDE (2BSDE) \cite{cheridito2007second}. If one chooses $\tilde b(t,x)$ and $\Sigma(t,x)$ such that $h$ is determined by
\begin{equation}
    H(t, x, p, q, \hat\alpha(t, x, p, q)) = \tilde b(t, x) \cdot p + h(t, x, p, q) + \half \Tr(\Sigma(t, x)\Sigma(t, x)\transpose q),
\end{equation}
then the solution to the 2BSDE
\begin{equation}\label{def_control_2BSDEreform}
    \begin{dcases}
    \ud \mc{X}_t = \tilde b(t, \mc{X}_t) \ud t + \Sigma(t, \mc{X}_t) \ud W_t, \quad \mc{X}_0 = x_0, \\
    \ud \mc{Y}_t = -h(t, \mc{X}_t, \mc{Y}_t, \mc{Z}_t)\ud t + \mc{Z}_t\transpose \Sigma(t, \mc{X}_t)\ud W_t, \quad Y_T =  g(\mc{X}_T),\\
    \ud \mc{Z}_t = \mc{A}_t \ud t + \Gamma_t \Sigma(t, \mc{X}_t)\ud W_t,  \quad \mc{Z}_T = \grad_x g(\mc{X}_T),
    \end{dcases}
\end{equation}
gives an interpretation of the solution to the PDE \eqref{def:control-HJB} through the relations
$$
    \mc{Y}_t = u(t, \mc{X}_t), \quad \mc{Z}_t =  \grad_x u(t, \mc{X}_t), \quad \Gamma_t = \Hess_x u(t, \mc{X}_t), \quad \mc{A}_t = \mathfrak{L}\grad_x u(t, \mc{X}_t),
$$
where $\mathfrak{L}$ denotes the infinitesimal generator of $\mc{X}$.

\noindent
\textbf{FBSDE approach. } 
The Pontryagin stochastic maximum principle provides the connection to the FBSDE, where the forward equation is coupled with the backward equation. Define the generalized Hamiltonian $\mc{H}$ by
\begin{equation}
\label{eq:Hamiltonian-bsde-control}
\mc{H}(t,x,y,z,\alpha) = b(t, x, \alpha) y + \Tr(\sigma\transpose(t, x, \alpha)z) + f(t, x, \alpha).
\end{equation}
If the Hamiltonian $\mc{H}$ is convex in $(x, \alpha)$, 
and $(X_t, Y_t, Z_t)$ solves
\begin{equation}\label{def_control_FBSDEreform}
    \begin{dcases}
    \ud {X}_t = b(t, {X}_t, \hat \alpha_t) \ud t +\sigma(t, {X}_t, \hat \alpha_t) \ud W_t, \qquad {X}_0 = x_0,\\
    \ud {Y}_t = -\grad_x \mc{H}(t, {X}_t, {Y}_t,  {Z}_t,  \hat \alpha_t ) \ud t + {Z}_t \ud W_t, \quad Y_T = \partial_x g({X}_T),\\
    \hat{\alpha}_t = \inf_{\alpha} \mc{H}(t, {X}_t, {Y}_t,  {Z}_t, \alpha ),
    \end{dcases}
\end{equation}
then $\hat \alpha$ is the optimal control.  If the value function is smooth enough, then 
\begin{equation}
     Y_t = \grad_x u(t,  X_t), \quad Z_t = \sigma(t, X_t, \hat \alpha_t)\transpose\Hess_x u(t,  X_t).
\end{equation}



For machine learning algorithms introduced in the following sections, if a temporal discretization is needed, we shall consider, for simplicity, a uniform grid $\pi$ on the interval $[0,T]$, {\it i.e.}, a partition $0 = t_0 < t_1 \dots < t_{N_T} = T$, with $t_n - t_{n-1} = \Delta t = T / N_T$. 


