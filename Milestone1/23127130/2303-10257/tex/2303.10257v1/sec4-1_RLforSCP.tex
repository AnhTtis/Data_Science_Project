Recall the stochastic control problems studied in Section~\ref{sec:SCP}, see~\eqref{def:control-cost}--\eqref{def:control-Xt}
In this section, we will assume that the agent can not directly access $b, \sigma, f$ and $g$, but can observe the ``next step'' information given the current state and control. We consider the time discretized problem
\begin{align}
 &\checkX_{t_{n+1}} = \checkX_{t_n} + b(t_n, \checkX_{t_n}, \alpha_{t_n}) \Delta t + \sigma(t_n, \checkX_{t_n}, \alpha_{t_n}) \Delta \check W_{t_n}, \label{eq:rl-Xt-discrete} \\
&\min_{(\alpha_{t_n})_{n=0,\dots,N_T-1}} \EE\left[\sum_{n=0}^{N_T-1} f(t_n, \checkX_{t_n}, \alpha_{t_n}) \Delta t + g(\checkX_T)\right] \label{eq:rl-Xt-cost},
\end{align}
where
\begin{equation}
    0 = t_0 < t_1 < \ldots < t_{N_T} = T, \text{ with } t_n - t_{n-1} = \Delta t = T/N_T,
\end{equation}
is the temporal discretization on $[0,T]$ as before. By doing so, the system is Markovian, and can be viewed as a Markov decision process (MDP). 



