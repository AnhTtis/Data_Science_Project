
In this section, we briefly review neural networks and stochastic gradient descent, which are two of the main tools of modern machine learning. We refer to \cite{higham2019deep} for a more comprehensive mathematical introduction to deep learning for applied mathematicians and to~\cite{goodfellow2016deep} for more background on deep learning.

\subsection{Neural network architectures}
\label{subsec:NN-architectures}

We start by introducing the feedforward fully connected architecture, before discussing recurrent neural networks and long short-term memory networks.


\subsubsection{Feedforward fully connected neural networks} Feedforward neural networks (FNNs) are the most common type of neural networks. We denote by
\begin{align*}
	\mathbf{L}^\rho_{d_1, d_2} = 
	&\Big\{ \phi: \RR^{d_1} \to \RR^{d_2} \,\Big|\,  \exists (w, \beta) \in \RR^{d_2 \times d_1} \times \RR^{d_2}, \forall  i \in \{1,\dots,d_2\}, 
	\;\phi(x)_i = \rho\Big(\beta_i + \sum_{j=1}^{d_1} w_{i,j} x_j\Big) \Big\} 
\end{align*}
the set of layer functions with input dimension $d_1$, output dimension $d_2$, and activation function $\rho: \RR \to \RR$. Typical choices for $\rho$ are ReLU (positive part), identity, sigmoid, or hyperbolic tangent,
\begin{equation}\label{eq:activation}
    \rho_{\text{ReLU}}(x) = \max\{x,0\}, \quad \rho_{\text{Id}}(x) = x, \quad \rho_{\text{s}}(x) = \frac{1}{1+e^{-x}}, \quad \rho_{\tanh}(x) = \tanh(x).
\end{equation}
Building on this notation and denoting by $\circ$ the composition of functions, we define
\begin{align*}
	\bN^{\rho,\tilde\rho}_{d_0, \dots, d_{\ell+1}} 
	= 
	&\Big\{ \phi_\ell \circ \phi_{\ell-1} \circ \dots \circ \phi_0 \,\Big|\,  (\phi_i)_{i=0, \dots, \ell-1} \in \bigtimes_{i=0}^{i=\ell-1} \mathbf{L}^\rho_{d_i, d_{i+1}},   \phi_\ell \in \mathbf{L}^{\tilde\rho}_{d_{\ell}, d_{\ell+1}}  \Big\} \, 
	\notag
\end{align*}
 as the set of regression neural networks with $\ell$ hidden layers and one output layer, the activation function of the output layer being $\tilde\rho$. The number $\ell$ of hidden layers, the numbers $d_0$, $d_1$, $\cdots$ , $d_{\ell+1}$ of units per layer, and the activation functions, are the components of what is called the architecture of the network. Once it is fixed, the actual network function $\varphi\in \bN^{\rho,\tilde\rho}_{d_0, \dots, d_{\ell+1}} $ is determined by the remaining parameters
 $$
 \theta=(\beta^{(0)}, w^{(0)},\beta^{(1)}, w^{(1)},\cdots\cdots,\beta^{(\ell-1)}, w^{(\ell-1)},\beta^{(\ell)}, w^{(\ell)}),
 $$
defining the functions $\phi_0$, $\phi_1$, $\cdots$ , $\phi_{\ell-1}$ and $\phi_\ell$ respectively. Let us denote by $\Theta$ the set of values for such parameters. For each $\theta\in\Theta$, the function computed by the network will be denoted by $\varphi^\theta \in \bN^{\rho, \tilde \rho}_{d_0, \dots, d_{\ell+1}}$ when we want to stress the dependence on the parameters.

To alleviate the presentation, we will follow the convention to use vector and matrix notations, and here activation functions are implicitly applied coordinate-wisely. Then
$$
    \varphi^{\theta}(x) = \tilde\rho\left(\beta^{(\ell)} + w^{(\ell)} \rho\left( \beta^{(\ell-1)} + w^{(\ell-1)} \rho\left(  \dots \beta^{(0)} + w^{(0)} x\right)\right) \right).
$$


\subsubsection{Recurrent neural networks}\label{sec:RNNdetails} 

Although FNNS are universal approximators, they are not very suitable to handle path-dependent properties of the state process, which are important for instance when the stochastic control problem or the game has delay features. The idea of recurrent neural networks (RNNs)~\cite{rumelhart1986learning} is to make use of sequential information, and thus provide a natural framework for overcoming these issues. In fact, RNNs have already shown great success in, {\it e.g.}, natural language processing and handwriting recognition \cite{graves2013generating,graves2013speech,graves2009offline}. Many variants exist and below we shall focus on one such variant, but the generic architecture can be described as follows: the neural network takes two inputs, $x$ and $h$, and produces two outputs, $y$ and $h'$, as follows:
\begin{align*}
    h' &= \rho\left(\beta^{(1)} + w^{(1,1)} h + w^{(1,2)} x \right),
    \\
    y &= \tilde\rho\left(\beta^{(2)} + w^{(2)} h' \right),
\end{align*}
where $\rho,\tilde \rho$ are two activation functions, and the parameters of the neural network are  vectors $\beta^{(1)},\beta^{(2)}$ of suitable sizes, and matrices $w^{(1,1)}, w^{(1,2)}, w^{(2)}$ of suitable sizes. 

Given a sequence of data points $(x_k)_{k \ge 0}$, which can represent the discrete-time trajectory of state process for instance, and an initial input $h_0$, a RNN can be used recursively to produce the sequence $(y_k, h_k)_{k \ge 1}$ defined by
\begin{align*}
   h_{k} &= \rho\left(\beta^{(1)} + w^{(1,1)} h_{k-1} + w^{(1,2)} x_{k-1} \right),
    \\
    y_{k} &= \tilde \rho\left(\beta^{(2)} + w^{(2)} h_{k} \right).
\end{align*}
Here, $h_{k+1}$ encodes information that is transmitted from iteration $k$ to iteration $k+1$. This information is produced using previous information $h_{k}$ and the current data point $x_{k}$. It is used to compute the output $y_{k}$ associated to the current input $x_{k}$, and the future information $h_{k+2}$.

Based on the idea of using an architecture in a recurrent way, many generalizations of the above simple neural network have been proposed. We next present one of them.

\subsubsection{Long short-term memory}\label{sec:LSTMdetails} 
One of the most common types of RNN is the long short-term memory (LSTM) neural network~\cite{hochreiter1997long}. The advantage of an LSTM is the ability to deal with the vanishing gradient problem and data with lags of unknown duration. 
An LSTM is composed of a series of units, each of which corresponds to a timestamp, and each unit consists of a cell $\mathfrak{c}$ and three gates: input gate $\mathfrak{i}$, output gate $\mathfrak{o}$, and forget gate $\mathfrak{f}$. Among these components, the cell keeps track of the information received so far, the input gate captures to which extent new input information flows into the cell, the forget gate captures to which extent the existing information remains in the cell, and the output gate controls to which extent the information in the cell will be used to compute the output of the unit. Given a data sequence $(x_k)_{k \geq 0}$ and an initial input $h_0$, the information flows are
\begin{equation}
\label{eq:lstm_gate}
\begin{aligned}
    &\text{forget gate: } \mathfrak{f}_k = \rho_{\text{s}}(W_f x_k + U_f h_{k-1} + b_f),\\
    &\text{input gate: } \mathfrak{i}_k =  \rho_{\text{s}}(W_i x_k + U_i h_{k-1} + b_i), \\
    &\text{ontput gate: } \mathfrak{o}_k =  \rho_{\text{s}}(W_o x_k + U_o h_{k-1} + b_o), \\
    &\text{cell: } \mathfrak{c}_k = \mathfrak{f}_k \odot \mathfrak{c}_{k-1} + \mathfrak{i}_k \odot \rho_{\tanh}(W_c x_k + U_c h_{k-1} + b_c), \\
    &\text{output of the } k^{th} \text{ unit: } h_k = \mathfrak{o}_k \odot  \rho_{\tanh}(\mathfrak{c}_k),
\end{aligned}
\end{equation}
where the operator $\odot$ denotes the Hadamard product, $W_f$, $W_i$, $W_o$, $W_c$,  $U_f$, $U_i$, $U_o$, $U_c$, $b_f$, $b_i$, $b_o$ and $b_c$ are neural network parameters of compatible sizes, and $\rho_{\text{s}}$ and $\rho_{\tanh}$ are activation functions given in \eqref{eq:activation}. 


\subsubsection{Expressive power of neural networks}
The first theories about neural networks date back to 1989 by Cybenko \cite{cybenko1989approximation} and by Hornik, Stinchcombe and White \cite{hornik1989multilayer}, concerning the approximation capabilities of feedforward networks within a given function space of interest. Hornik \cite{hornik1991approximation} then extended the results to approximating function's derivatives, and Leshno, Lin, Pinkus and Schocken \cite{leshno1993multilayer} proved results under arbitrary nonpolynomial activation functions. These results are referred to as universal approximation theorems. See also~\cite{Pinkus:99}. 



In the past decade, the mathematical theory has been greatly developed, which is complemented by unprecedented advances in highly parallelizable graphics processing units (GPUs), the introduction of new network architectures, and the development of GPU-enabled algorithms. For instance, in terms of approximation theories, other types of neural networks have been investigated, including RNN \cite{schafer2006recurrent}, convolutional neural networks (CNNs) \cite{zhou2020universality} and graph neural networks (GNNs) \cite{keriven2021universality}. Concerning the expressive power of neural networks,  \cite{hanin2019universal} analyzed it from the depth point view, while \cite{lu2017expressive,hanin2017approximating,parkminimum} considered a width perspective. Several works tackle the question of how neural networks can tackle the curse of dimensionality; see, {\textit e.g.}, \cite{hanin2019deep,grohs2021lower,jentzen2021proof,hutzenthaler2020proof}. For further discussion the onw mathematical theory of deep learning, we refer to, {\textit e.g.}, \cite{berner_grohs_kutyniok_petersen_2022}.


\subsection{Stochastic gradient descent and its variants}
\label{sec:SGD-var} 

The process of adjusting the parameters of a parameterized system, such as a neural network, in order to optimize a loss function is called \emph{training}. Stochastic gradient descent (SGD) is one of the most popular method to train neural network parameters, for example for the aforementioned FNNs, RNNs and LSTMs. 



Consider a generic optimization problem: minimize over $\varphi$,
$$
    J(\varphi) = \mathbb{E}_{\xi \sim \nu}[\mathfrak{L}(\varphi, \xi)],
$$
where $\xi$ follows a distribution $\nu$ and $\mathfrak{L}$ is a loss function. Using a neural network with a given architecture as an approximator for $\varphi$, the goal becomes to minimize over $\theta$,
$$
    J(\theta) = \mathbb{E}_{\xi \sim \nu}[\mathfrak{L}(\varphi_\theta, \xi)].
$$
Even if $\mathfrak{L}$ is known, the loss cannot be computed exactly when $\nu$ is unknown. If one does not have access to $\nu$ but only to samples drawn from $\nu$, one can use SGD, described in Algorithm~\ref{algo:SGD-generic}, which relies on an empirical risk minimization problem,
$$
    J^{S, N}(\theta) = \frac{1}{N} \sum_{i=1}^N \mathfrak{L}(\varphi_\theta, \xi^i),
$$
where $N$ is the number of training samples of $\xi$ and we denote the sample set by $S = (\xi^1,\dots,\xi^N)$.


\begin{algorithm}[htp]
\begin{algorithmic}
\STATE {\bfseries Input: }{An initial parameter $\theta_0\in\Theta$.  A mini-batch size $N_{\text{Batch}}$. A number of iterations $M$. A sequence of learning rates $(\beta_m)_{m = 0, \dots, M-1}$.}
\STATE {\bfseries Output: }{Approximation of $\theta^*$}
  \FOR{$m = 0, 1, 2, \dots, M-1$}
    \STATE Sample a minibatch of $N_\text{Batch}$ samples $S = (\xi^{i})_{i=1,\dots,N_\text{Batch}}$ where $\xi^i$ are i.i.d. drawn from $\nu$\;
    \STATE Compute the gradient $\nabla J^{S, N_\text{Batch}}(\theta_{m})$ \;
    \STATE Update $\theta_{m+1} =  \theta_{m} -\beta_m \nabla J^{S, N_\text{Batch}}(\theta_{m})$ \;
  \ENDFOR
  \STATE {Return $\theta_{M}$}
\end{algorithmic}
\caption{Stochastic Gradient Descent (SGD)\label{algo:SGD-generic}}
\end{algorithm}

SGD is generally used with a moderately large size of mini-batch, which reduces the computational cost of each iteration and can furthermore help escaping local minima. In practice, the choice of the learning rate can be crucial to ensure convergence. A popular way to adjust the learning rate is the Adam method~\cite{kingma2014adam}  which is summarized in Algorithm~\ref{algo:ADAM-generic} and can be viewed as an adaptive momentum accelerated SGD.  The computation of the gradient $\nabla J^{S, N}(\theta)$  with respect to $\theta$ can be done automatically by libraries such as {\tt TensorFlow} or {\tt PyTorch}, which perform this computation efficiently by using backpropagation.

\begin{algorithm}[htp]
\begin{algorithmic}
\STATE {\bfseries Input: }{Stepsize $\alpha$. Exponential decay rates  for the moment estimates $\beta_1,\beta_2 \in [0,1)$. Initial parameter $\theta_0$. Small parameter for numerical stability $\epsilon$. }
\STATE {\bfseries Output: }{Approximation of $\theta^*$}
  \STATE Initialize first moment vector $\bar M_0$ and second moment vector $\bar V_0$ \;
  \FOR{$m = 0, 1, 2, \dots, M-1$}
    \STATE Sample a minibatch of $N_\text{Batch}$ samples $S = ((\xi^{i})_{i=1,\dots,N_{\text{Batch}}}$ where $x^i$ are i.i.d. drawn from $\nu$\;
    \STATE Compute the gradient $g_m = \nabla J^{S}(\theta_{m})$ \;
    \STATE Update biased first moment estimate: $\bar M_m = \beta_1 \bar M_{m-1} + (1-\beta_1) g_m$ \;
    \STATE Update biased second moment estimate: $\bar V_m = \beta_2 \bar V_{m-1} + (1-\beta_2) g_m^2$ \;
    \STATE Compute biased-corrected first moment estimate: $\hat M_m = \bar M_m / (1 - \beta_1^m)$ \;
    \STATE Compute biased-corrected second moment estimate: $\hat V_m = \bar V_m / (1 - \beta_2^m)$ \;
    \STATE Set $\theta_{m+1} =  \theta_{m} -\alpha \hat M_m / (\sqrt{\hat V_m} + \epsilon)$ \;
  \ENDFOR
  \STATE {Return $\theta_{M}$}
\end{algorithmic}
\caption{ADAM: Adaptive Moment Estimation \label{algo:ADAM-generic}}
\end{algorithm}

