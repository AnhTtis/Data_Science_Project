\section{Introduction}\label{sec:intro}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/fig1.pdf}
  \caption{%
    \textbf{Pipeline comparison} between sliding window-based methods (top)~\cite{zhang2020learning,zeng2020dense,zhang2020span,soldan2021vlg} and our \method (bottom).
    %
    It is noteworthy that the sliding window pipeline requires repeated inference on \textit{overlapped clips} and the final \textit{result aggregation}, while ours can deliver the result with \textit{one-time} network execution.
    %
    Detailed discussion can be found in \cref{sec:efficiency}.
    %
  }
  \label{fig:comparison}
  \vspace{-5mm}
\end{figure}


%
Video temporal grounding~\cite{zhang2020learning,zeng2020dense,gao2017tall,mun2020local,soldan2021vlg,zhang2020span,soldan2022mad,grauman2022ego4d}, which aims to localize a specific moment in the video corresponding to a natural language description, has found its applications in many real-world scenarios, such as video retrieval~\cite{lei2020tvr,wu2021hanet}, video highlight detection~\cite{yao2016highlight,xiong2019less}, and video question answering~\cite{huang2020location,yu2018joint}.
%


%
Despite the rapid advance in recent years, existing methods for temporal grounding usually target short-form videos (\textit{e.g.} in minutes) and characterize the input video with a small number of frames (\textit{e.g.}, 128)~\cite{zhang2020learning,zeng2020dense,zhang2020span,soldan2021vlg,zhang2019man,yuan2019semantic}.
%
When it comes to the case of long-form video temporal grounding (LVTG)~\cite{soldan2022mad,grauman2022ego4d}, however, temporally downsampling a video (\textit{e.g.}, in hours) to so few frames could cause severe information loss and further result in drastic performance degradation~\cite{grauman2022ego4d}.

%
A straightforward solution is to reorganize a long video to a sequence of short videos using a sliding window and perform temporal grounding within each window~\cite{soldan2022mad,grauman2022ego4d,hou2022cone}.
%
However, such a solution as shown in the top half of \cref{fig:comparison} has three main drawbacks.
%
(1) Inference inefficiency: The overlap between adjacent windows brings redundant computations. Besides, the large amounts of highly overlapped predictions cause post-processing (\textit{e.g.}, non-maximum suppression) time-consuming. It is noteworthy by saying efficiency, we mean \textit{pipeline efficiency} instead of model efficiency, which considers the total execution time from data input to final results output, including data pre-processing, model forward running and post-processing.%
\footnote{The concrete explanations of each part can be found in \cref{sec:efficiency}.}
%
(2) Training insufficiency: The network with a sliding window can only scan the video contents within a local time range at one time, yet ignore the long-range temporal correlation. 
%
(3) Prediction inflexibility: The prediction is restricted inside a single window, making it hard to generalize to segments with long duration.  



In this work, we propose an anchor-based end-to-end framework, termed as \textbf{\method}, which facilitates efficient and accurate LVTG by \textbf{S}canning a long-form video \textbf{O}nly \textbf{O}nce. 
%
As shown in the bottom half of \cref{fig:comparison}, \method follows a pipeline of \textit{pre-ranking, re-ranking, regression}, via leveraging both the inter-anchor context knowledge and the intra-anchor content knowledge. 
%

Specifically, we first produce non-overlapped anchor sequence via anchor partition layer, then three procedures are implemented to obtain final predictions: (1) Multi-scale context-based anchor features are acquired by modeling inter-anchor context knowledge via cascaded temporal swin transformer blocks~\cite{liu2021swin}. Meanwhile, a coarse anchor rank is obtained via sorting the context-based matching scores with respect to query. (2) Content-based anchor features and a content-enhanced anchor rank can be obtained by supplementing anchors with detailed intra-anchor content knowledge. We pick out the top-$m$ anchors that highly corresponds to query from each scale to form an anchor subset, then implement re-ranking within subset to reduce the computational complexity. (3) Boundary regression is adopted to achieve flexible predictions, leveraging both inter-anchor and intra-anchor knowledge.
To take full advantage of the abundant cross-modal semantic relationship in long videos, we sample one video with a batch of queries grounded in this video at one training step, then optimize the full-length anchor rank and query rank simultaneously with the help of proposed dual-form approximate rank loss, which achieves superior cross-modal alignment.
Extensive experiments are conducted on two long-form video datasets, \textit{i.e.}, MAD~\cite{soldan2022mad} and Ego4d~\cite{grauman2022ego4d}. Our method significantly outperforms state-of-the-arts, and achieves \textbf{14.6$\times$} / \textbf{102.8$\times$} higher pipeline efficiency, which demonstrate the effectiveness.
