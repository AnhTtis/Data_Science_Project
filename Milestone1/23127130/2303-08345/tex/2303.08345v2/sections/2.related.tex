\section{Related Work}\label{sec:related-work}

\subsection{Short-form Video Temporal Grounding}
Existing methods mainly focus on short-form video temporal grounding and can be categorized into \textit{proposal-based} and \textit{proposal-free} methods. Methods in proposal-based category adopt a two-stage pipeline, which first generate proposal candidates by various proposal generation methods, such as sliding window and proposal generation network, then they rank these candidates and output the proposal with the highest matching score as final prediction. \cite{gao2017tall} propose CTRL, a pioneer work in video grounding. CTRL produces various-length proposal candidates via sliding window and uses the visual-textual fusion modules combined with three operators, \textit{i.e.}, add, multiply and fully-connected layer, to obtain multi-modal fused representation. MAN \cite{zhang2019man} and SCDM~\cite{yuan2019semantic} leverage multiple cascaded temporal convolution layers to generate proposal candidates hierarchically. TGN~\cite{chen2018temporally} temporally captures the evolving fine-grained frame-by-word interactions and uses pre-set anchors to produce multi-scale proposal candidates ending at each time step.
Subsequently \cite{zhang2019cross,wang2020temporally,qu2020fine,zhang2021multi} follow the anchor-based framework and propose various multi-modal reasoning strategies to achieve precise moment localization. In addition, 2D-TAN~\cite{zhang2020learning} enumerate all possible segments as proposal candidates and convert them into 2D feature map, then a temporal adjacent network is proposed to obtain multi-modal representation and encode the video context information. Following this, \cite{zheng2021progressive,wang2021structured,soldan2021vlg} design more complicated cross-modal reasoning strategies to learn the video-language semantic alignment from both coarse and fine-grained granularities. 

Methods in proposal-free category predict the start and end boundaries by computing the time pair directly, or output the confidence scores of being the start and end positions of target moment for each snippet in video. \cite{yuan2019find} propose ABLR, which performs cross-modal reasoning with a multi-modal co-attention interaction modules and outputs target moments by feeding the multi-modal features to regressor. Attention weight-based regression and attention feature-based regression are considered together to achieve precise boundary regression. Concurrently, DRN~\cite{zeng2020dense} considers the data imbalance issue and only uses the frame in ground-truth moment to mitigate the sparsity issue. LGI~\cite{mun2020local} aligns the video and language from phrase-level and propose a local-global interaction network that models the cross-modal relationship considering local and global context information simultaneously.

However, directly applying these methods on long-form videos results in drastic performance degradation, as temporally downsampling a long video to so few frames causes severe temporal information loss.


\subsection{Long-form Video Temporal Grounding}
Recently MAD~\cite{soldan2022mad} and Ego4d~\cite{grauman2022ego4d} pose the challenge of long-form video temporal grounding, and give some baselines that integrate sliding window and temporal downsampling into some short video-fit methods, such as 2D-TAN~\cite{zhang2020learning}, VLG-Net~\cite{soldan2021vlg} and VSLNet~\cite{zhang2020span}. However, all these methods achieve inferior performance, considering both accuracy and efficiency. 
Recently~\cite{hou2022cone} propose CONE, which pre-filters the candidate windows to address the inference inefficiency and learns the cross-modal alignment from proposal-level and frame-level. Nevertheless, it adopts sparse sampling strategy at training stage, which does not explore the potential of long-form video adequately. 