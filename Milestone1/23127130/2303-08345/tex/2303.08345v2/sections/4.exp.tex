\section{Experiments}\label{sec:exp}
\input{tables/mad_results.tex}

\subsection{Datasets}
We conduct experiments on two long-form video datasets MAD~\cite{soldan2022mad} (\textit{avg. 110.8 min / video}) and Ego4d~\cite{grauman2022ego4d} (\textit{avg. 25.7 min / video}), in which videos are much longer than those in previous datasets, such as ActivityNet Captions~\cite{krishna2017dense} (\textit{avg. 2.0 min / video}) and Charades-STA~\cite{sigurdsson2016hollywood} (\textit{avg. 0.5 min / video}).


\vspace{2pt}
\noindent\textbf{MAD} is a large-scale benchmark for long-form video temporal grounding, which contains over 384K natural language queries that derived from high-quality audio description of mainstream movies and grounded in over 1.2K hours of videos with very low coverage (an average duration of 4.1s). The length of videos in MAD ranges from 47 minutes to 202 minutes, which are orders of magnitude longer than previous datasets.


\vspace{2pt}
\noindent\textbf{Ego4d} is an egocentric video dataset, containing 3670 hours of daily-life activity videos collected by 931 worldwide participants. The \textbf{Ego4d-NLQ} is the official subtask of Ego4d which is to retrieve the most relevant video moment from truncated video clips, given a natural language question that generated via filling pre-defined query templates. However, the average duration of video clips is only \textit{8.25 minutes}, which is too short to be used as LVTG evaluation benchmark. To verify the effectiveness of our method on long-form video grounding, we introduce a new evaluation setting and name it \textbf{Ego4d-Video-NLQ}, where we replace the truncated video clips with full-length video, therefore the average duration of videos reaches \textit{25.7 minutes}. We report the performance on the validation set of Ego4d, under both Ego4d-NLQ and Ego4d-Video-NLQ settings.

\subsection{Metrics}
Following \cite{soldan2022mad, hou2022cone}, we adopt the standard metric ``Recall@$n$, IoU=$m$'' (R@$n$-$m$) for evaluation. Specifically, it represents the percentage of testing samples that have at least one grounding prediction whose IoU with groundtruth is larger than $m$ among top-$n$ predictions.

\subsection{Implementation Details}
Following \cite{soldan2022mad}, we use CLIP \cite{radford2021learning} to extract visual features and textual features for MAD dataset. We set $C_{0}=10$, $L=4$ for multi-scale anchor generation. $\lambda_{1}, \lambda_{2}$ are set to 1 and 20 respectively. $m$ is set to 100 for filtering. The temperature $\alpha$ for $\mathcal{L}_{\text{ctx}}$ and $\mathcal{L}_{\text{ctn}}$ are both set to 0.01. We train the network for 100k steps with an initial learning rate of 0.001, and decay it by a factor of 10 after 40k steps. When training, we set batch size as 32 (1 video with 32 queries grounded in this video at one step) and use AdamW as the optimizer. The feature dimension $D$ is set to 512.

For Ego4d-NLQ and Ego4d-Video-NLQ, we use the pre-extracted SlowFast features \cite{feichtenhofer2019slowfast} and Bert features \cite{devlin2018bert} as the visual and textual features, following \cite{grauman2022ego4d}. We set $C_{0}=1$, $L=7$ on Ego4d-NLQ and $C_{0}=6$, $L=4$ on Ego4d-Video-NLQ. $\lambda_{1}, \lambda_{2}$ are set to 1 and 5 respectively. $m$ is set to 20. We train the network for 30k steps with an initial learning rate of 0.0001, and decay it by 10 after 15k steps. Other hyper-parameters are the same as in MAD. All experiments are implemented on one A100 GPU with 80GB memory. 
%



\subsection{Accuracy Comparison with SOTAs}
We first compare our model with several state-of-the-art methods. \cref{tab:mad} reports the performance results on long video dataset MAD (the average video duration is around \textbf{110.8 minutes}) with three methods: CLIP~\cite{radford2021learning}, VLG-Net~\cite{soldan2021vlg} and CONE~\cite{hou2022cone}. All of them are sliding window-based methods. From \cref{tab:mad} we can observe that our method outperforms all other methods, achieving \textbf{2.13\%} and \textbf{1.22\%} performance gains, in terms of R@1-0.3 and R@1-0.5 respectively. Thanks to modeling the entire video as a whole, our method can capture long-range temporal correlation, and learn cross-modal alignment with abundant context information, which facilitates more accurate grounding. We also conduct experiments on Ego4d dataset and summarize the results on \cref{tab:ego4d}. We first compare performance under Ego4d-NLQ setting with three methods: 2D-TAN~\cite{zhang2020learning}, VSLNet~\cite{zhang2020span} and CONE~\cite{hou2022cone}. \cref{tab:ego4d} suggests that our method achieves competitive performance on Ego4d-NLQ, even though it tests on short-form videos (the average video duration is \textbf{8.25 minutes}). We then test the performance on Ego4d-Video-NLQ, where the average video duration is \textbf{25.7 minutes}. We re-implement the 2D-TAN and VSLNet with the public code released by \cite{grauman2022ego4d}: it combines the 2D-TAN with sliding window to fit long-form video while adopts the downsampling strategy for VSLNet to reduce the sequence length to 128. From \cref{tab:ego4d} we observe our SOONet achieves \textbf{2.20\%} / \textbf{0.98\%} performance gains in terms of R@1-0.3 and R@1-0.5 respectively, which demonstrates the effectiveness of our method on long-form video temporal grounding. 
%

\input{tables/ego4d_results.tex}

\input{tables/efficiency_analysis.tex}
\subsection{Efficiency Comparison with SOTAs}\label{sec:efficiency}
To evaluate the efficiency of our method, we compare SOONet with 3 sliding window-based methods (\textit{i.e.}, CLIP, VLG-Net and 2D-TAN) and 1 downsampling-based method (\textit{i.e.}, VSLNet) on MAD and Ego4d-Video-NLQ. Recall that the code of CONE is not publicly available so we can't make a fair comparison with it. As mentioned in \cref{sec:intro}, the efficiency here means pipeline efficiency, which considers the execution time of three parts: (1) \textit{pre-processing} (denoted as Pre), which transfers raw data to the form of model input; (2) \textit{model forward} (denoted as Model), which refers to network calculation; (3) \textit{post-processing} (denoted as Post), which drops highly overlapped predictions and acquire the top-$n$ segments.
\cref{tab:efficiency} reports the number of parameters, FLOPs, GPU memory usage of models and gives a detailed breakdown of execution time. 
For FLOPs and GPU memory usage, we measure them using same samples as input because they change with the length of input video. From \cref{tab:efficiency} we observe the GPU memory usages of sliding window-based methods surpass our SOONet obviously, because batch inference on local windows is adopted to accelerate the model forward. 
For execution time, at each time we feed one video and one sentence to the system and report the total execution time of each part separately over the entire test set. 
From \cref{tab:efficiency} we observe that compared with sliding window-based methods, our SOONet makes huge improvement on pipeline efficiency, achieving \textbf{14.6$\times$} / \textbf{58.5$\times$} / \textbf{102.8$\times$} higher inference speed, compared with CLIP, VLG-Net and 2D-TAN respectively.
It is noteworthy that model FLOPs only affects the model forward time. Though CLIP contains only a matrix multiplication operation that needs few FLOPs, it suffers from both the slow pre-processing, which needs to split an entire video into lots of overlapped windows as well as gather window features, and the slow post-processing, which employs NMS (\textit{i.e.}, non-maximum suppression) to abandon large amounts of highly overlapped predictions. 
In addition to the slow pre-processing and post-processing, an another efficiency bottleneck of sliding window-based methods lies in the redundant computation on overlapped windows, which increases the model FLOPs greatly, causing the model forward part time-consuming. 
Compared with downsampling-based VSLNet, our SOONet achieves competitive inference speed whereas a far superior accuracy. Despite a bit more FLOPs, our network spends less time on model forward running than VSLNet. These results demonstrate the efficiency of our method.

\subsection{Ablation Studies}

\noindent\textbf{Effectiveness of Each Module.}
We conduct experiments on MAD to verify the effectiveness of each module employed in our framework: (1) Pre-ranking with Anchor Knowledge, (2) Re-ranking with Frame Knowledge, and (3) Boundary Regression. We report the ablation results on \cref{tab:ablation_module}, where PR, RR, BR represent the three modules respectively. 
\cref{tab:ablation_module} suggest that, equipped with Pre-ranking module only, our method achieves 9.41\% / 7.07\% / 4.10\% performance in terms of R@1-0.1, R@1-0.3, R@-0.5 respectively, which is a competitive result compared with state-of-the-arts. Benefit from the long-range context encoding and global-view rank learning, the Pre-ranking module explore the cross-modal semantic relationship in long videos adequately, thus facilitates accurate grounding. Upon this, integrating Re-ranking module achieves improvements of +0.76\%/+0.58\%/+0.33\%, because the detailed frame knowledge supplement fine-grained semantics, \textit{e.g.}, the scene and objects occurred in few frames, that generally perturbed by many irrelated frames. Integrating Boundary Regression module achieves improvements of +1.38\%/+1.45\%/+0.69\%, which benefits from the flexible adjustments. The combination of the three modules achieves improvements of +1.62\%/+1.76\%/+1.13\%, which demonstrates the complementary of proposed modules.

\input{tables/ablation_module.tex}
\input{tables/ablation_loss.tex}

\vspace{2pt}
\noindent\textbf{Impact of Dual-form Approximate Rank Loss.}
To make clear the contribution of the proposed dual-form approximate rank Loss $\mathcal{L}_{dar}$, we compare it with three loss functions: (1) Binary cross entropy loss $\mathcal{L}_{bce}$, which uses IoU as labels to optimize the query-anchor matching scores; (2) Noise contrastive estimation loss $\mathcal{L}_{nce}$, which optimizes a hidden space where positive pairs are assigned close and negative pairs are pushed away. We selects the anchor with highest IoU as positive samples and others as negative samples; (3) Single ApproxNDCG loss $\mathcal{L}_{ar}$, which optimizes the anchor rank only. The results are summarized in \cref{tab:ablation_loss}. $\mathcal{L}_{bce}$ achieves poor performance in all metrics, mainly caused by the extremely imbalance of positive (which has IoU $>$ 0) and negative (which has IoU $=$ 0) samples, even though we have enlarge the weight of positive samples. Besides, $\mathcal{L}_{ar}$ and $\mathcal{L}_{dar}$ both surpass $\mathcal{L}_{nce}$ by a large margin, because $\mathcal{L}_{nce}$ only tries to distinguish the anchor with highest IoU from large amounts of anchor candidates, while $\mathcal{L}_{ar}$ and $\mathcal{L}_{dar}$ implement the anchor rank optimization from the global perspective, which needs to consider the relationship between each anchor pair. Finally, $\mathcal{L}_{dar}$ outperforms $\mathcal{L}_{ar}$ by 0.68\% / 0.43\% in terms of R@1-0.3 and R@1-0.5, demonstrating the complementary of query rank optimization and anchor rank optimization.




\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/anchor.pdf}
  \caption{%
    Ablation study on the base anchor length, $C_{0}$. 
  }
  \label{fig:anchor}
  \vspace{-8pt}
\end{figure}
\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figures/tempe.pdf}
  \caption{%
    Ablation study on the temperature value, $\alpha$.
  }
  \label{fig:tempe}
  \vspace{-5mm}
\end{figure}

\vspace{2pt}
\noindent\textbf{Base anchor length $C_{0}$.} We vary the value of $C_{0}$ to study the impact of anchor length and summarize the results in \cref{fig:anchor}. We observe that the performance decreases greatly on MAD as $C_{0}$ grows, while not changes obviously on Ego4d-Video-NLQ. 
This is because most of ground-truth moments on MAD last very short time, which makes the long anchor hard to align with query and regress the boundaries accurately. On the contrary, the length distribution of groundtruth in Ego4d-Video-NLQ is wider-ranging, which makes it insensitive to the anchor length.

\vspace{2pt}
\noindent\textbf{Temperature $\alpha$.} We vary the value of $\alpha$ in $\mathcal{L}_{dar}$ across from 0.001 to 0.5 to study the impact. The results are shown in \cref{fig:tempe}. From results we observe that the optimization is sensitive to the value of $\alpha$. The performance reaches the peak when $\alpha$ is in $[0.005, 0.01]$ and larger $\alpha$ leads to much worse performance.


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/visual.pdf}
  \caption{%
    Qualitative analysis on the re-ranking module with full-length anchor matching scores, where re-ranking helps localize the moment of interest more precisely.
  }
  \label{fig:visual}
  \vspace{-3mm}
\end{figure}

\subsection{Qualitative Analysis}
We provide qualitative results to illustrate the contributions of Pre-ranking and Re-ranking modules. \cref{fig:visual} displays the predictions of \method without Re-ranking (first line) and with Re-ranking (second line), as well as the corresponding groundtruth (third line). It suggests, equipped with only Pre-ranking module, our method achieves coarse localization (two humps showed on orange line) but loses some fine-grained details so that it can not distinguish food and seasoning. However, when combined with Re-ranking module, our method succeeds in recognizing the seasoning, and raises the confidence of the right moment as well as decreases the matching score of wrong moment. More qualitative results are provided in \supp.