@String(PAMI   = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV   = {Int. J. Comput. Vis.})
@String(CVPR   = {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(CVPRW  = {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(ICCV   = {Int. Conf. Comput. Vis.})
@String(ICCVW  = {Int. Conf. Comput. Vis. Worksh.})
@String(ECCV   = {Eur. Conf. Comput. Vis.})
@String(ECCVW  = {Eur. Conf. Comput. Vis. Worksh.})
@String(NIPS   = {Adv. Neural Inform. Process. Syst.})
@String(NIPSW  = {Adv. Neural Inform. Process. Syst. Worksh.})
@String(ICML   = {Int. Conf. Mach. Learn.})
@String(ICMLW  = {Int. Conf. Mach. Learn. Worksh.})
@String(ICLR   = {Int. Conf. Learn. Represent.})
@String(ICLRW  = {Int. Conf. Learn. Represent. Worksh.})
@String(AAAI   = {Assoc. Adv. Artif. Intell.})
@String(IJCAI  = {Int. Joint Conf. Artif. Intell.})
@String(BMVC   = {Brit. Mach. Vis. Conf.})
@String(ACCV   = {Asian Conf. Comput. Vis.})
@String(ICPR   = {Int. Conf. Pattern Recog.})
@String(TOG    = {ACM Trans. Graph.})
@String(TIP    = {IEEE Trans. Image Process.})
@String(TVCG   = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM    = {IEEE Trans. Multimedia})
@String(ACMMM  = {ACM Int. Conf. Multimedia})
@String(ICME   = {Int. Conf. Multimedia and Expo})
@String(ICASSP = {ICASSP})
@String(ICIP   = {IEEE Int. Conf. Image Process.})
@String(PR     = {Pattern Recog.})
@String(CSVT   = {IEEE Trans. Circuit Syst. Video Technol.})
@String(SPL	   = {IEEE Sign. Process. Letters})
@String(VR     = {Vis. Res.})
@String(JOV	   = {J. Vis.})
@String(TVC    = {The Vis. Comput.})
@String(JCST   = {J. Comput. Sci. Tech.})
@String(CGF    = {Comput. Graph. Forum})
@String(CVM    = {Computational Visual Media})
@String(WACV   = {IEEE Winter Conf. Appl. Comput. Vis.})


% charades
@inproceedings{sigurdsson2016hollywood,
  title={Hollywood in homes: Crowdsourcing data collection for activity understanding},
  author={Sigurdsson, Gunnar A and Varol, G{\"u}l and Wang, Xiaolong and Farhadi, Ali and Laptev, Ivan and Gupta, Abhinav},
  booktitle=ECCV,
  pages={510--526},
  year={2016}
}

% ActivityNet
@inproceedings{krishna2017dense,
  title={Dense-captioning events in videos},
  author={Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Carlos Niebles, Juan},
  booktitle=ICCV,
  pages={706--715},
  year={2017}
}

% DiDeMo
@inproceedings{anne2017localizing,
  title={Localizing moments in video with natural language},
  author={Anne Hendricks, Lisa and Wang, Oliver and Shechtman, Eli and Sivic, Josef and Darrell, Trevor and Russell, Bryan},
  booktitle=ICCV,
  pages={5803--5812},
  year={2017}
}

% TACoS
@article{regneri2013grounding,
  title={Grounding action descriptions in videos},
  author={Regneri, Michaela and Rohrbach, Marcus and Wetzel, Dominikus and Thater, Stefan and Schiele, Bernt and Pinkal, Manfred},
  journal={Transactions of the Association for Computational Linguistics},
  pages={25--36},
  year={2013},
}

% MAD
@inproceedings{soldan2022mad,
  title={Mad: A scalable dataset for language grounding in videos from movie audio descriptions},
  author={Soldan, Mattia and Pardo, Alejandro and Alc{\'a}zar, Juan Le{\'o}n and Caba, Fabian and Zhao, Chen and Giancola, Silvio and Ghanem, Bernard},
  booktitle=CVPR,
  pages={5026--5035},
  year={2022}
}

% Ego4d
@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle=CVPR,
  pages={18995--19012},
  year={2022}
}

% CONE
@article{hou2022cone,
  title={CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding},
  author={Hou, Zhijian and Zhong, Wanjun and Ji, Lei and Gao, Difei and Yan, Kun and Chan, Wing-Kwong and Ngo, Chong-Wah and Shou, Zheng and Duan, Nan},
  journal={arXiv preprint arXiv:2209.10918},
  year={2022}
}

% 2D-TAN
@inproceedings{zhang2020learning,
  title={Learning 2d temporal adjacent networks for moment localization with natural language},
  author={Zhang, Songyang and Peng, Houwen and Fu, Jianlong and Luo, Jiebo},
  booktitle=AAAI,
  pages={12870--12877},
  year={2020}
}

%DRN
@inproceedings{zeng2020dense,
  title={Dense regression network for video grounding},
  author={Zeng, Runhao and Xu, Haoming and Huang, Wenbing and Chen, Peihao and Tan, Mingkui and Gan, Chuang},
  booktitle=CVPR,
  pages={10287--10296},
  year={2020}
}

% MSA
@inproceedings{zhang2021msa,
  title={Multi-stage aggregated transformer network for temporal language localization in videos},
  author={Zhang, Mingxing and Yang, Yang and Chen, Xinghan and Ji, Yanli and Xu, Xing and Li, Jingjing and Shen, Heng Tao},
  booktitle=CVPR,
  pages={12669--12678},
  year={2021}
}

% CTRL
@inproceedings{gao2017tall,
  title={Tall: Temporal activity localization via language query},
  author={Gao, Jiyang and Sun, Chen and Yang, Zhenheng and Nevatia, Ram},
  booktitle=ICCV,
  pages={5267--5275},
  year={2017}
}

% MAN
@inproceedings{zhang2019man,
  title={Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment},
  author={Zhang, Da and Dai, Xiyang and Wang, Xin and Wang, Yuan-Fang and Davis, Larry S},
  booktitle=CVPR,
  pages={1247--1257},
  year={2019}
}

% SCDM
@article{yuan2019semantic,
  title={Semantic conditioned dynamic modulation for temporal sentence grounding in videos},
  author={Yuan, Yitian and Ma, Lin and Wang, Jingwen and Liu, Wei and Zhu, Wenwu},
  journal=NIPS,
  year={2019}
}

% TGN
@inproceedings{chen2018temporally,
  title={Temporally grounding natural sentence in video},
  author={Chen, Jingyuan and Chen, Xinpeng and Ma, Lin and Jie, Zequn and Chua, Tat-Seng},
  booktitle={Proceedings of the 2018 conference on empirical methods in natural language processing},
  pages={162--171},
  year={2018}
}

% CMIN
@inproceedings{zhang2019cross,
  title={Cross-modal interaction networks for query-based moment retrieval in videos},
  author={Zhang, Zhu and Lin, Zhijie and Zhao, Zhou and Xiao, Zhenxin},
  booktitle={Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={655--664},
  year={2019}
}

% CBP
@inproceedings{wang2020temporally,
  title={Temporally grounding language queries in videos by contextual boundary-aware prediction},
  author={Wang, Jingwen and Ma, Lin and Jiang, Wenhao},
  booktitle=AAAI,
  pages={12168--12175},
  year={2020}
}

% FIAN
@inproceedings{qu2020fine,
  title={Fine-grained iterative attention network for temporal language localization in videos},
  author={Qu, Xiaoye and Tang, Pengwei and Zou, Zhikang and Cheng, Yu and Dong, Jianfeng and Zhou, Pan and Xu, Zichuan},
  booktitle=ACMMM,
  pages={4280--4288},
  year={2020}
}

% MI-GCN
@article{zhang2021multi,
  title={Multi-modal interaction graph convolutional network for temporal language localization in videos},
  author={Zhang, Zongmeng and Han, Xianjing and Song, Xuemeng and Yan, Yan and Nie, Liqiang},
  journal=TIP,
  pages={8265--8277},
  year={2021}
}

% ABLR
@inproceedings{yuan2019find,
  title={To find where you talk: Temporal sentence localization in video with attention based location regression},
  author={Yuan, Yitian and Mei, Tao and Zhu, Wenwu},
  booktitle=AAAI,
  pages={9159--9166},
  year={2019}
}

% PLN
@article{zheng2021progressive,
  title={Progressive localization networks for language-based moment localization},
  author={Zheng, Qi and Dong, Jianfeng and Qu, Xiaoye and Yang, Xun and Ji, Shouling and Wang, Xun},
  journal={arXiv preprint arXiv:2102.01282},
  year={2021}
}

% SMIN
@inproceedings{wang2021structured,
  title={Structured multi-level interaction network for video moment localization via language query},
  author={Wang, Hao and Zha, Zheng-Jun and Li, Liang and Liu, Dong and Luo, Jiebo},
  booktitle=CVPR,
  pages={7026--7035},
  year={2021}
}

% LGI
@inproceedings{mun2020local,
  title={Local-global video-text interactions for temporal grounding},
  author={Mun, Jonghwan and Cho, Minsu and Han, Bohyung},
  booktitle=CVPR,
  pages={10810--10819},
  year={2020}
}

% VLG-Net
@inproceedings{soldan2021vlg,
  title={VLG-Net: Video-language graph matching network for video grounding},
  author={Soldan, Mattia and Xu, Mengmeng and Qu, Sisi and Tegner, Jesper and Ghanem, Bernard},
  booktitle=ICCV,
  pages={3224--3234},
  year={2021}
}

% VSLNet
@article{zhang2020span,
  title={Span-based localizing network for natural language video localization},
  author={Zhang, Hao and Sun, Aixin and Jing, Wei and Zhou, Joey Tianyi},
  journal={arXiv preprint arXiv:2004.13931},
  year={2020}
}

% CLIP
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle=ICML,
  pages={8748--8763},
  year={2021}
}

% survey
@article{zhang2022elements,
  title={The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions},
  author={Zhang, Hao and Sun, Aixin and Jing, Wei and Zhou, Joey Tianyi},
  journal={arXiv preprint arXiv:2201.08071},
  year={2022}
}

% Transformer
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal=NIPS,
  volume={30},
  year={2017}
}

% swin transformer
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle=ICCV,
  pages={10012--10022},
  year={2021}
}

% approximate rank
@article{qin2010general,
  title={A general approximation framework for direct optimization of information retrieval measures},
  author={Qin, Tao and Liu, Tie-Yan and Li, Hang},
  journal={Information retrieval},
  pages={375--397},
  year={2010}
}

% IoU loss
@inproceedings{yu2016unitbox,
  title={Unitbox: An advanced object detection network},
  author={Yu, Jiahui and Jiang, Yuning and Wang, Zhangyang and Cao, Zhimin and Huang, Thomas},
  booktitle=ACMMM,
  pages={516--520},
  year={2016}
}

% bert
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

% slowfast
@inproceedings{feichtenhofer2019slowfast,
  title={Slowfast networks for video recognition},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  booktitle=ICCV,
  pages={6202--6211},
  year={2019}
}

% lstm
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  pages={1735--1780},
  year={1997}
}

% layernorm
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}



% highlight detection
@inproceedings{yao2016highlight,
  title={Highlight detection with pairwise deep ranking for first-person video summarization},
  author={Yao, Ting and Mei, Tao and Rui, Yong},
  booktitle=CVPR,
  pages={982--990},
  year={2016}
}

@inproceedings{xiong2019less,
  title={Less is more: Learning highlight detection from video duration},
  author={Xiong, Bo and Kalantidis, Yannis and Ghadiyaram, Deepti and Grauman, Kristen},
  booktitle=CVPR,
  pages={1258--1267},
  year={2019}
}

% vqa
@inproceedings{huang2020location,
  title={Location-aware graph convolutional networks for video question answering},
  author={Huang, Deng and Chen, Peihao and Zeng, Runhao and Du, Qing and Tan, Mingkui and Gan, Chuang},
  booktitle=AAAI,
  pages={11021--11028},
  year={2020}
}

@inproceedings{yu2018joint,
  title={A joint sequence fusion model for video question answering and retrieval},
  author={Yu, Youngjae and Kim, Jongseok and Kim, Gunhee},
  booktitle=ECCV,
  pages={471--487},
  year={2018}
}

% video retrieval
@inproceedings{lei2020tvr,
  title={Tvr: A large-scale dataset for video-subtitle moment retrieval},
  author={Lei, Jie and Yu, Licheng and Berg, Tamara L and Bansal, Mohit},
  booktitle=ECCV,
  pages={447--463},
  year={2020}
}

@inproceedings{wu2021hanet,
  title={HANet: Hierarchical Alignment Networks for Video-Text Retrieval},
  author={Wu, Peng and He, Xiangteng and Tang, Mingqian and Lv, Yiliang and Liu, Jing},
  booktitle=ACMMM,
  pages={3518--3527},
  year={2021}
}