
@book{pearl_causality_2009,
	address = {Cambridge;New York;},
	edition = {2nd},
	title = {Causality: models, reasoning, and inference},
	isbn = {0521773628;052189560X;9780521895606;9780521773621;},
	abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interest to students and professionals in a wide variety of fields. Dr Judea Pearl has received the 2011 Rumelhart Prize for his leading research in Artificial Intelligence (AI) and systems from The Cognitive Science Society.},
	number = {Book, Whole},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	year = {2009},
	keywords = {Causation, Probabilities},
}

@article{rudin_stop_2019,
	title = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
	volume = {1},
	issn = {2522-5839},
	url = {https://doi.org/10.1038/s42256-019-0048-x},
	doi = {10.1038/s42256-019-0048-x},
	abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
	number = {5},
	journal = {Nature Machine Intelligence},
	author = {Rudin, Cynthia},
	month = may,
	year = {2019},
	pages = {206--215},
	file = {Submitted Version:C\:\\Users\\pbreh\\Zotero\\storage\\7M9PRBXD\\Rudin - 2019 - Stop explaining black box machine learning models .pdf:application/pdf},
}

@article{lipton_mythos_2018,
	title = {The {Mythos} of {Model} {Interpretability}: {In} machine learning, the concept of interpretability is both important and slippery.},
	volume = {16},
	issn = {1542-7730},
	number = {3},
	journal = {Queue},
	author = {Lipton, Zachary C},
	year = {2018},
	note = {Publisher: ACM New York, NY, USA},
	pages = {31--57},
}

@inproceedings{domingos_knowledge_1997,
	address = {San Francisco, CA, USA},
	series = {{ICML} '97},
	title = {Knowledge {Acquisition} {Form} {Examples} {Vis} {Multiple} {Models}},
	isbn = {1-55860-486-3},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Domingos, Pedro},
	year = {1997},
	pages = {98--106},
}

@article{mehrabi_survey_2019,
	title = {A survey on bias and fairness in machine learning},
	journal = {arXiv preprint arXiv:1908.09635},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	year = {2019},
}

@article{chernozhukov_doubledebiased_2018,
	title = {Double/debiased machine learning for treatment and structural parameters},
	volume = {21},
	issn = {1368-4221},
	url = {https://doi.org/10.1111/ectj.12097},
	doi = {10.1111/ectj.12097},
	abstract = {We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
	number = {1},
	urldate = {2021-05-13},
	journal = {The Econometrics Journal},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
	month = feb,
	year = {2018},
	pages = {C1--C68},
}

@article{hedden_statistical_2021,
	title = {On statistical criteria of algorithmic fairness},
	volume = {49},
	issn = {0048-3915},
	url = {http://anu.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwnV1LS8NAEB60vfSitSrWFwEvKsRm0026OQZpERTsoZ7jZh9a0LS0VvDfO7vZ1LYHQW9JdgLLZHdmvs3MNwDd8CbwN2yCUDTJQ0TPXAdaJLqrc4GuWsUiV0QmG71hyiRLUxpT0kUsz9_MRrHm2-x3ns-rPd-ZomcxRAks2YZ6aEinalB_SEf3_R-z3ItcPQrzDSu64ypdf3vNO61Gq9bdDHbhuZrZ53hmCjxWCoTL9OvOksjxf_Nvwo4LRb20XDt7sKWKFjSGVW-DrxY0XXYcSjkb0IJ2WdBbPZh7l465-mofOo-FZyqULPkzyqBFMlTQ3Jtoj7-9TPD29X0sPPMTyZjYA3ga9Ee3d77ryOALQknis1CzIGIIuhghCkMzoijXOCSpllLnnHMMtzijIja8gTHnUVdKxCS9nggEguFDqBWTQh2BJ3A4RLAmKI2okoTTiOVaJ7GIEbLGrA0X1ZfIpiXxRlYBFqOtzGqrDddWqb-IZMN0mNqr478In0AjNOkr9rTlFGofs4U6wwimWJy7lfUNI8vVqg},
	doi = {10.1111/papa.12189},
	number = {2},
	journal = {Philosophy \& public affairs},
	author = {Hedden, Brian},
	year = {2021},
	note = {Place: Hoboken, USA
Publisher: John Wiley \& Sons, Inc},
	pages = {209--231},
}

@inproceedings{dwork_fairness_2012,
	address = {New York, NY, USA},
	series = {{ITCS} '12},
	title = {Fairness through {Awareness}},
	isbn = {978-1-4503-1115-1},
	url = {https://doi.org/10.1145/2090236.2090255},
	doi = {10.1145/2090236.2090255},
	abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	booktitle = {Proceedings of the 3rd {Innovations} in {Theoretical} {Computer} {Science} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	year = {2012},
	note = {event-place: Cambridge, Massachusetts},
	pages = {214--226},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	number = {1},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	pages = {5--32},
}

@article{nie_quasi-oracle_2020,
	title = {Quasi-{Oracle} {Estimation} of {Heterogeneous} {Treatment} {Effects}},
	url = {http://arxiv.org/abs/1712.04912},
	abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. In this paper, we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. We first estimate marginal effects and treatment propensities in order to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. Our approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: In both steps, we can use any loss-minimization method, e.g., penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation setups, and find promising performance relative to existing baselines.},
	urldate = {2021-05-17},
	journal = {arXiv:1712.04912 [econ, math, stat]},
	author = {Nie, Xinkun and Wager, Stefan},
	month = aug,
	year = {2020},
	note = {arXiv: 1712.04912},
	keywords = {Economics - Econometrics, Mathematics - Statistics Theory, Statistics - Machine Learning},
	annote = {Comment: Biometrika, forthcoming},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbreh\\Zotero\\storage\\864J34CA\\Nie and Wager - 2020 - Quasi-Oracle Estimation of Heterogeneous Treatment.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\238FSPPK\\1712.html:text/html},
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures} (with comments and a rejoinder by the author)},
	volume = {16},
	url = {https://doi.org/10.1214/ss/1009213726},
	doi = {10.1214/ss/1009213726},
	number = {3},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	month = aug,
	year = {2001},
	pages = {199--231},
}

@book{imbens_causal_2015,
	title = {Causal {Inference} for {Statistics}, {Social}, and {Biomedical} {Sciences}: {An} {Introduction}},
	isbn = {9780521885881;0521885884;},
	url = {http://anu.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwjV3PS8MwFH6Iu7iLv3H-IngQhdW1adqmN9lwzJMexGtJmgRE6WDd9vf7XtvNOgR3bJsG-pK896V53_cAQv7gexs-Icc4pHApGYovfp74KpIuDizxIkXiot-1YdaqOZsH-kEyGA1fSKYJgQtG64TI0x26T5N7POE_v1dEShLcDTtVSuJjNno7q-ugObX8q9cudFX5iW4GXdC8bOsutKLPeB9eVxye5ceM-B4tvnCdjT1o6zpu8TkH0LHEcjiEHVscQa8m6bJmoZfsrlGjvj-Gx5FalPjsecUMZAhzGWHUWuK5z-qX-0wVhg0rNj8N_LqzE3gfP72NJl5TdsFTYYT7VU9Y7YvQcWWE0NqFgchlzlPpnHYy9w13zqbcxFxIbXVEdctiRA5am0A5E6rwFHaLaWHPgDmntIulDSKb4ogohbtxrvPYkAyh47wHNy07Z8uv6oi4zMgmYRD7KQGKfxsJBI09uF2PUZbraVanryXZpoXPt214AXuIjqIqTSe-hN35bGGvEMMUi-tqrn0DLITJhA},
	abstract = {Most questions in social and biomedical sciences are causal in nature: what would happen to individuals, or to groups, if part of their environment were changed? In this groundbreaking text, two world-renowned experts present statistical methods for studying such questions. This book starts with the notion of potential outcomes, each corresponding to the outcome that would be realized if a subject were exposed to a particular treatment or regime. In this approach, causal effects are comparisons of such potential outcomes. The fundamental problem of causal inference is that we can only observe one of the potential outcomes for a particular subject. The authors discuss how randomized experiments allow us to assess causal effects and then turn to observational studies. They lay out the assumptions needed for causal inference and describe the leading analysis methods, including matching, propensity-score methods, and instrumental variables. Many detailed applications are included, with special focus on practical aspects for the empirical researcher.},
	number = {Book, Whole},
	publisher = {Cambridge University Press},
	author = {Imbens, Guido W. and Rubin, Donald B.},
	year = {2015},
	doi = {10.1017/CBO9781139025751},
	keywords = {Causation, Inference, Social sciences},
}

@inproceedings{gunning_darpas_2019,
	address = {New York, NY, USA},
	series = {{IUI} '19},
	title = {{DARPA}'s {Explainable} {Artificial} {Intelligence} ({XAI}) {Program}},
	isbn = {978-1-4503-6272-6},
	url = {https://doi.org/10.1145/3301275.3308446},
	doi = {10.1145/3301275.3308446},
	abstract = {The DARPA's Explainable Artificial Intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. This talk will summarize the XAI program and present highlights from these Phase 1 evaluations.},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Gunning, David},
	year = {2019},
	note = {event-place: Marina del Ray, California},
	keywords = {artificial intelligence, evaluation, explanation, machine learning},
	pages = {ii},
}

@article{holland_statistics_1986,
	title = {Statistics and causal inference},
	volume = {81},
	issn = {0162-1459},
	number = {396},
	journal = {Journal of the American statistical Association},
	author = {Holland, Paul W},
	year = {1986},
	note = {Publisher: Taylor \& Francis},
	pages = {945--960},
}

@article{mcconnell_estimating_2019,
	title = {Estimating treatment effects with machine learning},
	volume = {54},
	issn = {0017-9124},
	doi = {10.1111/1475-6773.13212},
	abstract = {Objective

To demonstrate the performance of methodologies that include machine learning (ML) algorithms to estimate average treatment effects under the assumption of exogeneity (selection on observables).

Data Sources

Simulated data and observational data on hospitalized adults.

Study Design

We assessed the performance of several ML‐based estimators, including Targeted Maximum Likelihood Estimation, Bayesian Additive Regression Trees, Causal Random Forests, Double Machine Learning, and Bayesian Causal Forests, applying these methods to simulated data as well as data on the effects of right heart catheterization.

Principal Findings

In Monte Carlo studies, ML‐based estimators generated estimates with smaller bias than traditional regression approaches, demonstrating substantial (69 percent‐98 percent) bias reduction in some scenarios. Bayesian Causal Forests and Double Machine Learning were top performers, although all were sensitive to high dimensional ({\textgreater}150) sets of covariates.

Conclusions

ML‐based methods are promising methods for estimating treatment effects, allowing for the inclusion of many covariates and automating the search for nonlinearities and interactions among variables. We provide guidance and sample code for researchers interested in implementing these tools in their own empirical work.;OBJECTIVETo demonstrate the performance of methodologies that include machine learning (ML) algorithms to estimate average treatment effects under the assumption of exogeneity (selection on observables). DATA SOURCESSimulated data and observational data on hospitalized adults. STUDY DESIGNWe assessed the performance of several ML-based estimators, including Targeted Maximum Likelihood Estimation, Bayesian Additive Regression Trees, Causal Random Forests, Double Machine Learning, and Bayesian Causal Forests, applying these methods to simulated data as well as data on the effects of right heart catheterization. PRINCIPAL FINDINGSIn Monte Carlo studies, ML-based estimators generated estimates with smaller bias than traditional regression approaches, demonstrating substantial (69 percent-98 percent) bias reduction in some scenarios. Bayesian Causal Forests and Double Machine Learning were top performers, although all were sensitive to high dimensional ({\textgreater}150) sets of covariates. CONCLUSIONSML-based methods are promising methods for estimating treatment effects, allowing for the inclusion of many covariates and automating the search for nonlinearities and interactions among variables. We provide guidance and sample code for researchers interested in implementing these tools in their own empirical work.;To demonstrate the performance of methodologies that include machine learning (ML) algorithms to estimate average treatment effects under the assumption of exogeneity (selection on observables).

Simulated data and observational data on hospitalized adults.

We assessed the performance of several ML-based estimators, including Targeted Maximum Likelihood Estimation, Bayesian Additive Regression Trees, Causal Random Forests, Double Machine Learning, and Bayesian Causal Forests, applying these methods to simulated data as well as data on the effects of right heart catheterization.

In Monte Carlo studies, ML-based estimators generated estimates with smaller bias than traditional regression approaches, demonstrating substantial (69 percent-98 percent) bias reduction in some scenarios. Bayesian Causal Forests and Double Machine Learning were top performers, although all were sensitive to high dimensional ({\textgreater}150) sets of covariates.

ML-based methods are promising methods for estimating treatment effects, allowing for the inclusion of many covariates and automating the search for nonlinearities and interactions among variables. We provide guidance and sample code for researchers interested in implementing these tools in their own empirical work.;Principal Findings: In Monte Carlo studies, ML-based estimators generated estimates with smaller bias than traditional regression approaches, demonstrating substantial (69 percent-98 percent) bias reduction in some scenarios. Bayesian Causal Forests and Double Machine Learning were top performers, although all were sensitive to high dimensional ({\textgreater}150) sets of covariates.;Objective: To demonstrate the performance of methodologies that include machine learning (ML) algorithms to estimate average treatment effects under the assumption of exogeneity (selection on observables). Data Sources: Simulated data and observational data on hospitalized adults. Study Design: We assessed the performance of several ML-based estimators, including Targeted Maximum Likelihood Estimation, Bayesian Additive Regression Trees, Causal Random Forests, Double Machine Learning, and Bayesian Causal Forests, applying these methods to simulated data as well as data on the effects of right heart catheterization. Principal Findings: In Monte Carlo studies, ML-based estimators generated estimates with smaller bias than traditional regression approaches, demonstrating substantial (69 percent-98 percent) bias reduction in some scenarios. Bayesian Causal Forests and Double Machine Learning were top performers, although all were sensitive to high dimensional ({\textgreater}150) sets of covariates. Conclusions: ML-based methods are promising methods for estimating treatment effects, allowing for the inclusion of many covariates and automating the search for nonlinearities and interactions among variables. We provide guidance and sample code for researchers interested in implementing these tools in their own empirical work. KEYWORDS machine learning, observational research, treatment effects;},
	number = {6},
	journal = {Health services research},
	author = {McConnell, K. John and Lindner, Stephan},
	year = {2019},
	note = {Place: United States
Publisher: Health Research and Educational Trust},
	keywords = {Adult, Humans, Female, Male, Algorithms, Middle Aged, Treatment Outcome, machine learning, Aged, Aged, 80 and over, Bayes Theorem, Cardiac Catheterization - statistics \& numerical data, Computer Simulation, Data Interpretation, Statistical, HSR Methods and Data Sources, Inpatients - statistics \& numerical data, Models, Statistical, observational research, Research, treatment effects},
	pages = {1273--1282},
}

@incollection{glymour_causation_2009,
	address = {Oxford},
	title = {Causation and {Statistical} {Inference}},
	url = {https://www-oxfordhandbooks-com.virtual.anu.edu.au/view/10.1093/oxfordhb/9780199279739.001.0001/oxfordhb-9780199279739-e-0024},
	booktitle = {The {Oxford} {Handbook} of {Causation}},
	publisher = {Oxford University Press},
	author = {Glymour, Clark},
	editor = {Beebee, Helen and Hitchcock, Christopher and Menzies, Peter},
	year = {2009},
}

@article{corbett-davies_measure_2018,
	title = {The {Measure} and {Mismeasure} of {Fairness}: {A} {Critical} {Review} of {Fair} {Machine} {Learning}},
	shorttitle = {The {Measure} and {Mismeasure} of {Fairness}},
	url = {http://arxiv.org/abs/1808.00023},
	abstract = {The nascent field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last several years, three formal definitions of fairness have gained prominence: (1) anti-classification, meaning that protected attributes---like race, gender, and their proxies---are not explicitly used to make decisions; (2) classification parity, meaning that common measures of predictive performance (e.g., false positive and false negative rates) are equal across groups defined by the protected attributes; and (3) calibration, meaning that conditional on risk estimates, outcomes are independent of protected attributes. Here we show that all three of these fairness definitions suffer from significant statistical limitations. Requiring anti-classification or classification parity can, perversely, harm the very groups they were designed to protect; and calibration, though generally desirable, provides little guarantee that decisions are equitable. In contrast to these formal fairness criteria, we argue that it is often preferable to treat similarly risky people similarly, based on the most statistically accurate estimates of risk that one can produce. Such a strategy, while not universally applicable, often aligns well with policy objectives; notably, this strategy will typically violate both anti-classification and classification parity. In practice, it requires significant effort to construct suitable risk estimates. One must carefully define and measure the targets of prediction to avoid retrenching biases in the data. But, importantly, one cannot generally address these difficulties by requiring that algorithms satisfy popular mathematical formalizations of fairness. By highlighting these challenges in the foundation of fair machine learning, we hope to help researchers and practitioners productively advance the area.},
	urldate = {2021-08-04},
	journal = {arXiv:1808.00023 [cs]},
	author = {Corbett-Davies, Sam and Goel, Sharad},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.00023},
	keywords = {Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbreh\\Zotero\\storage\\NCEHVZP8\\Corbett-Davies and Goel - 2018 - The Measure and Mismeasure of Fairness A Critical.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\L96MPYQX\\1808.html:text/html},
}

@article{imbens_potential_2020,
	title = {Potential {Outcome} and {Directed} {Acyclic} {Graph} {Approaches} to {Causality}: {Relevance} for {Empirical} {Practice} in {Economics}},
	shorttitle = {Potential {Outcome} and {Directed} {Acyclic} {Graph} {Approaches} to {Causality}},
	url = {http://arxiv.org/abs/1907.07271},
	abstract = {In this essay I discuss potential outcome and graphical approaches to causality, and their relevance for empirical work in economics. I review some of the work on directed acyclic graphs, including the recent "The Book of Why," by Pearl and MacKenzie. I also discuss the potential outcome framework developed by Rubin and coauthors, building on work by Neyman. I then discuss the relative merits of these approaches for empirical work in economics, focusing on the questions each answer well, and why much of the the work in economics is closer in spirit to the potential outcome framework.},
	urldate = {2021-08-16},
	journal = {arXiv:1907.07271 [stat]},
	author = {Imbens, Guido W.},
	month = mar,
	year = {2020},
	note = {arXiv: 1907.07271},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbreh\\Zotero\\storage\\AX67P8FJ\\Imbens - 2020 - Potential Outcome and Directed Acyclic Graph Appro.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\PCI7HTZP\\1907.html:text/html},
}

@article{galles_axiomatic_1998,
	title = {An {Axiomatic} {Characterization} of {Causal} {Counterfactuals}},
	volume = {3},
	issn = {1572-8471},
	url = {https://doi.org/10.1023/A:1009602825894},
	doi = {10.1023/A:1009602825894},
	abstract = {This paper studies the causal interpretation of counterfactual sentences using a modifiable structural equation model. It is shown that two properties of counterfactuals, namely, composition and effectiveness, are sound and complete relative to this interpretation, when recursive (i.e., feedback-less) models are considered. Composition and effectiveness also hold in Lewis's closest-world semantics, which implies that for recursive models the causal interpretation imposes no restrictions beyond those embodied in Lewis's framework. A third property, called reversibility, holds in nonrecursive causal models but not in Lewis's closest-world semantics, which implies that Lewis's axioms do not capture some properties of systems with feedback. Causal inferences based on counterfactual analysis are exemplified and compared to those based on graphical models.},
	number = {1},
	journal = {Foundations of Science},
	author = {Galles, David and Pearl, Judea},
	month = jan,
	year = {1998},
	pages = {151--182},
}

@incollection{pearl_bayesianism_2001,
	address = {Dordrecht},
	title = {Bayesianism and {Causality}, or, {Why} {I} am {Only} a {Half}-{Bayesian}},
	isbn = {978-94-017-1586-7},
	url = {https://doi.org/10.1007/978-94-017-1586-7_2},
	abstract = {I turned Bayesian in 1971, as soon as I began reading Savage’s monograph The Foundations of Statistical Inference [Savage, 1962]. The arguments were unassailable: (i) It is plain silly to ignore what we know, (ii) It is natural and useful to cast what we know in the language of probabilities, and (iii) If our subjective probabilities are erroneous, their impact will get washed out in due time, as the number of observations increases.},
	booktitle = {Foundations of {Bayesianism}},
	publisher = {Springer Netherlands},
	author = {Pearl, Judea},
	editor = {Corfield, David and Williamson, Jon},
	year = {2001},
	doi = {10.1007/978-94-017-1586-7_2},
	pages = {19--36},
}

@InCollection{sep-equal-opportunity,
	author       =	{Arneson, Richard},
	title        =	{{Equality of Opportunity}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/sum2015/entries/equal-opportunity/}},
	year         =	{2015},
	edition      =	{{S}ummer 2015},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@book{spirtes_causation_2001,
	address = {Cambridge, MA},
	series = {{MIT} {Press} {Books}},
	title = {Causation, {Prediction}, and {Search}, 2nd {Edition}},
	volume = {1},
	isbn = {ARRAY(0x511004d8)},
	url = {https://ideas.repec.org/b/mtp/titles/0262194406.html},
	abstract = {What assumptions and methods allow us to turn observations into causal knowledge, and how can even incomplete causal knowledge be used in planning and prediction to influence and control our environment? In this book Peter Spirtes, Clark Glymour, and Richard Scheines address these questions using the formalism of Bayes networks, with results that have been applied in diverse areas of research in the social, behavioral, and physical sciences. The authors show that although experimental and observational study designs may not always permit the same inferences, they are subject to uniform principles. They axiomatize the connection between causal structure and probabilistic independence, explore several varieties of causal indistinguishability, formulate a theory of manipulation, and develop asymptotically reliable procedures for searching over equivalence classes of causal models, including models of categorical data and structural equation models with and without latent variables. The authors show that the relationship between causality and probability can also help to clarify such diverse topics in statistics as the comparative power of experimentation versus observation, Simpson's paradox, errors in regression models, retrospective versus prospective sampling, and variable selection. The second edition contains a new introduction and an extensive survey of advances and applications that have appeared since the first edition was published in 1993.},
	publisher = {The MIT Press},
	author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
	year = {2001},
	note = {Issue: 0262194406},
}

@book{weizenbaum_computer_1976,
	address = {San Francisco},
	title = {Computer power and human reason: from judgment to calculation},
	isbn = {978-0-7167-0464-5 978-0-7167-0463-8},
	shorttitle = {Computer power and human reason},
	language = {eng},
	publisher = {Freeman},
	author = {Weizenbaum, Joseph},
	year = {1976},
	annote = {Literaturangaben},
}

@misc{ai_fairness_360_ai_2022,
	title = {{AI} {Fairness} 360 - {Resources}},
	url = {https://aif360.mybluemix.net/aif360.mybluemix.net/resources},
	language = {en},
	urldate = {2022-06-13},
	author = {{AI Fairness 360}},
	year = {2022},
	file = {Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\UFFAI38D\\resources.html:text/html},
}

@article{frisch_partial_1933,
	title = {Partial {Time} {Regressions} as {Compared} with {Individual} {Trends}},
	volume = {1},
	issn = {00129682},
	url = {https://www.jstor.org/stable/1907330?origin=crossref},
	doi = {10.2307/1907330},
	number = {4},
	urldate = {2022-06-13},
	journal = {Econometrica},
	author = {Frisch, Ragnar and Waugh, Frederick V.},
	month = oct,
	year = {1933},
	pages = {387},
}

@article{lovell_seasonal_1963,
	title = {Seasonal {Adjustment} of {Economic} {Time} {Series} and {Multiple} {Regression} {Analysis}},
	volume = {58},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10480682},
	doi = {10.1080/01621459.1963.10480682},
	language = {en},
	number = {304},
	urldate = {2022-06-13},
	journal = {Journal of the American Statistical Association},
	author = {Lovell, Michael C.},
	month = dec,
	year = {1963},
	pages = {993--1010},
}

@misc{kusner_counterfactual_2018,
	title = {Counterfactual {Fairness}},
	url = {http://arxiv.org/abs/1703.06856},
	abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
	urldate = {2022-06-13},
	publisher = {arXiv},
	author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
	month = mar,
	year = {2018},
	note = {Number: arXiv:1703.06856
arXiv:1703.06856 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbreh\\Zotero\\storage\\JRS6WQ6P\\Kusner et al. - 2018 - Counterfactual Fairness.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\HZNPHKJL\\1703.html:text/html},
}

@article{lovell_simple_2008,
	title = {A {Simple} {Proof} of the {FWL} {Theorem}},
	volume = {39},
	issn = {00220485, 21524068},
	url = {http://www.jstor.org.virtual.anu.edu.au/stable/41426805},
	abstract = {[The author presents a simple proof of a property of the method of least squares variously known as the FWL, the Frisch-Waugh-Lovell, the Frisch-Waugh, or the decomposition theorem.]},
	number = {1},
	urldate = {2022-06-14},
	journal = {The Journal of Economic Education},
	author = {Lovell, Michael C.},
	year = {2008},
	note = {Publisher: Taylor \& Francis, Ltd.},
	pages = {88--91},
}

@misc{berk_convex_2017,
	title = {A {Convex} {Framework} for {Fair} {Regression}},
	url = {http://arxiv.org/abs/1706.02409},
	abstract = {We introduce a flexible family of fairness regularizers for (linear and logistic) regression problems. These regularizers all enjoy convexity, permitting fast optimization, and they span the rang from notions of group fairness to strong individual fairness. By varying the weight on the fairness regularizer, we can compute the efficient frontier of the accuracy-fairness trade-off on any given dataset, and we measure the severity of this trade-off via a numerical quantity we call the Price of Fairness (PoF). The centerpiece of our results is an extensive comparative study of the PoF across six different datasets in which fairness is a primary consideration.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
	month = jun,
	year = {2017},
	note = {Number: arXiv:1706.02409
arXiv:1706.02409 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbreh\\Zotero\\storage\\RNZNHB42\\Berk et al. - 2017 - A Convex Framework for Fair Regression.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\Q9J7CURI\\1706.html:text/html},
}

@misc{agarwal_fair_2019,
	title = {Fair {Regression}: {Quantitative} {Definitions} and {Reduction}-based {Algorithms}},
	shorttitle = {Fair {Regression}},
	url = {http://arxiv.org/abs/1905.12843},
	abstract = {In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems {\textbackslash}emph\{fair regression\}. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness--accuracy frontiers on several standard datasets.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Agarwal, Alekh and Dudík, Miroslav and Wu, Zhiwei Steven},
	month = may,
	year = {2019},
	note = {Number: arXiv:1905.12843
arXiv:1905.12843 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbreh\\Zotero\\storage\\QYKLRGNY\\Agarwal et al. - 2019 - Fair Regression Quantitative Definitions and Redu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\C6NZADAF\\1905.html:text/html},
}

@misc{kasirzadeh_use_2021,
	title = {The {Use} and {Misuse} of {Counterfactuals} in {Ethical} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2102.05085},
	abstract = {The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Kasirzadeh, Atoosa and Smart, Andrew},
	month = feb,
	year = {2021},
	note = {Number: arXiv:2102.05085
arXiv:2102.05085 [cs]},
	keywords = {Computer Science - Computers and Society},
	annote = {Comment: 9 pages, 1 table, 1 figure},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbreh\\Zotero\\storage\\H6FPAA33\\Kasirzadeh and Smart - 2021 - The Use and Misuse of Counterfactuals in Ethical M.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\KGP6N4Y2\\2102.html:text/html},
}

@misc{colangelo_double_2021,
	title = {Double {Debiased} {Machine} {Learning} {Nonparametric} {Inference} with {Continuous} {Treatments}},
	url = {http://arxiv.org/abs/2004.03036},
	abstract = {We propose a nonparametric inference method for causal effects of continuous treatment variables, under unconfoundedness and in the presence of high-dimensional or nonparametric nuisance parameters. Our double debiased machine learning (DML) estimators for the average dose-response function (or the average structural function) and the partial effects are asymptotically normal with nonparametric convergence rates. The nuisance estimators for the conditional expectation function and the conditional density can be nonparametric or ML methods. Utilizing a kernel-based doubly robust moment function and cross-fitting, we give high-level conditions under which the nuisance estimators do not affect the first-order large sample distribution of the DML estimators. We further provide sufficient low-level conditions for kernel and series estimators, as well as modern ML methods - generalized random forests and deep neural networks. We justify the use of kernel to localize the continuous treatment at a given value by the Gateaux derivative. We implement various ML methods in Monte Carlo simulations and an empirical application on a job training program evaluation.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Colangelo, Kyle and Lee, Ying-Ying},
	month = dec,
	year = {2021},
	note = {Number: arXiv:2004.03036
arXiv:2004.03036 [econ]},
	keywords = {Economics - Econometrics},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbreh\\Zotero\\storage\\UFL3VPVD\\Colangelo and Lee - 2021 - Double Debiased Machine Learning Nonparametric Inf.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\H2AM6N7V\\2004.html:text/html},
}

@inproceedings{hanna_towards_2020,
	title = {Towards a {Critical} {Race} {Methodology} in {Algorithmic} {Fairness}},
	url = {http://arxiv.org/abs/1912.03593},
	doi = {10.1145/3351095.3372826},
	abstract = {We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.},
	urldate = {2022-06-21},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Hanna, Alex and Denton, Emily and Smart, Andrew and Smith-Loud, Jamila},
	month = jan,
	year = {2020},
	note = {arXiv:1912.03593 [cs]},
	keywords = {Computer Science - Computers and Society},
	pages = {501--512},
	annote = {Comment: Conference on Fairness, Accountability, and Transparency (FAT* '20), January 27-30, 2020, Barcelona, Spain},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbreh\\Zotero\\storage\\J7R4UVRF\\Hanna et al. - 2020 - Towards a Critical Race Methodology in Algorithmic.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\YITK5RDX\\1912.html:text/html},
}

@article{bishu_systematic_2017,
	title = {A {Systematic} {Review} of the {Gender} {Pay} {Gap} and {Factors} {That} {Predict} {It}},
	volume = {49},
	issn = {0095-3997},
	url = {https://doi.org/10.1177/0095399716636928},
	doi = {10.1177/0095399716636928},
	abstract = {This study conducts a systematic review of 98 peer-reviewed journal articles that empirically investigate the presence of the gender pay gap along with factors that espouse it in organizations. The purposes of this study are threefold. First, it aims to explore trends in recurring themes that surface as factors that engender the gender pay gap in the workforce. Second, based on identified themes, the review summarizes and compares the gender pay gap by sector. Finally, the study presents a discussion on how the public sector fairs out in closing the gender pay gap and factors that predict it.},
	number = {1},
	urldate = {2022-06-20},
	journal = {Administration \& Society},
	author = {Bishu, Sebawit G. and Alkadry, Mohamad G.},
	month = jan,
	year = {2017},
	note = {Publisher: SAGE Publications Inc},
	pages = {65--104},
	annote = {doi: 10.1177/0095399716636928},
}

@article{kunze_gender_2008,
	title = {Gender wage gap studies: consistency and decomposition},
	volume = {35},
	issn = {0377-7332, 1435-8921},
	shorttitle = {Gender wage gap studies},
	url = {http://link.springer.com/10.1007/s00181-007-0143-4},
	doi = {10.1007/s00181-007-0143-4},
	language = {en},
	number = {1},
	urldate = {2022-06-21},
	journal = {Empirical Economics},
	author = {Kunze, Astrid},
	month = aug,
	year = {2008},
	pages = {63--76},
	file = {Kunze - 2008 - Gender wage gap studies consistency and decomposi.pdf:C\:\\Users\\pbreh\\Zotero\\storage\\JXW2WT66\\Kunze - 2008 - Gender wage gap studies consistency and decomposi.pdf:application/pdf},
}

@misc{kohler-hausmann_eddie_2019,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Eddie {Murphy} and the {Dangers} of {Counterfactual} {Causal} {Thinking} {About} {Detecting} {Racial} {Discrimination}},
	url = {https://papers.ssrn.com/abstract=3050650},
	doi = {10.2139/ssrn.3050650},
	abstract = {The model of discrimination animating some of the most common approaches to detecting discrimination in both law and social science—the counterfactual causal model—is wrong. In that model, racial discrimination is detected by measuring the “treatment effect of race,” where the treatment is conceptualized as manipulating the raced status of otherwise identical units (e.g., a person, a neighborhood, a school). Most objections to talking about race as a cause in the counterfactual model have been raised in terms of manipulability. If we cannot manipulate a person’s race at the moment of a police stop, traffic encounter, or prosecutorial charging decision, then it is impossible to detect if the person’s race was the sole cause of an unfavorable outcome. But this debate has proceeded on the wrong terms. The counterfactual causal model of discrimination is not wrong because we can’t work around the practical limits of manipulation, as evidenced by both Eddie Murphy’s comic genius in the Saturday Night Live skit “White Like Me” and the entire genre of audit and correspondence studies. It is wrong because to fit the rigor of the counterfactual model of a clearly defined treatment on otherwise identical units, we must reduce race to only the signs of the category, meaning we must think race is skin color, or phenotype, or other ways we identify group status. And that is a concept mistake if one subscribes to a constructivist, as opposed to a biological or genetic, conception of race. The counterfactual causal model of discrimination is based on a flawed theory of what the category of race references, how it produces effects in the world, and what is meant when we say it is wrong to make decisions of import because of race. I argue that DISCRIMINATION is a thick ethical concept that at once describes and evaluates the actions to which it is applied, and therefore, we cannot detect actions as discriminatory by identifying a relation of counterfactual causality; we can only do so by reasoning about the action’s distinctive wrongfulness by referencing what constitutes the very categories that are the objects of concern. An adequate theory of discrimination must rest upon (1) an account of the system of social meanings or practices that constitute the categories at issue and (2) a moral theory of what is fair and just in various state and private arenas given what the categories are.},
	language = {en},
	urldate = {2022-06-21},
	author = {Kohler-Hausmann, Issa},
	month = jan,
	year = {2019},
	keywords = {Eddie Murphy and the Dangers of Counterfactual Causal Thinking About Detecting Racial Discrimination, Issa Kohler-Hausmann, SSRN},
	file = {Full Text PDF:C\:\\Users\\pbreh\\Zotero\\storage\\PPZC43ZF\\Kohler-Hausmann - 2019 - Eddie Murphy and the Dangers of Counterfactual Cau.pdf:application/pdf;Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\IQ4SZGT3\\papers.html:text/html},
}

@inproceedings{damour_fairness_2020,
	address = {Barcelona Spain},
	title = {Fairness is not static: deeper understanding of long term fairness via simulation studies},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Fairness is not static},
	url = {https://dl.acm.org/doi/10.1145/3351095.3372878},
	doi = {10.1145/3351095.3372878},
	abstract = {As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses �xed data sets. To address this structural di�culty in the �eld, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the longterm consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.},
	language = {en},
	urldate = {2022-06-28},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {D'Amour, Alexander and Srinivasan, Hansa and Atwood, James and Baljekar, Pallavi and Sculley, D. and Halpern, Yoni},
	month = jan,
	year = {2020},
	pages = {525--534},
	file = {D'Amour et al. - 2020 - Fairness is not static deeper understanding of lo.pdf:C\:\\Users\\pbreh\\Zotero\\storage\\YZED5SJP\\D'Amour et al. - 2020 - Fairness is not static deeper understanding of lo.pdf:application/pdf},
}

@misc{rinker_wakefield_2018,
	address = {Buffalo, New York},
	title = {wakefield: {Generate} {Random} {Data}},
	url = {https://github.com/trinker/wakefield},
	author = {Rinker, Tyler W.},
	year = {2018},
	annote = {version 0.3.3},
}

@article{steinberg_fast_2020,
	title = {Fast {Fair} {Regression} via {Efficient} {Approximations} of {Mutual} {Information}},
	volume = {abs/2002.06200},
	url = {https://arxiv.org/abs/2002.06200},
	journal = {CoRR},
	author = {Steinberg, Daniel and Reid, Alistair and O'Callaghan, Simon and Lattimore, Finnian and McCalman, Lachlan and Caetano, Tibério S.},
	year = {2020},
	note = {arXiv: 2002.06200},
}

@inproceedings{pedreshi_discrimination-aware_2008,
	address = {Las Vegas, Nevada, USA},
	title = {Discrimination-aware data mining},
	isbn = {978-1-60558-193-4},
	url = {http://dl.acm.org/citation.cfm?doid=1401890.1401959},
	doi = {10.1145/1401890.1401959},
	language = {en},
	urldate = {2022-07-07},
	booktitle = {Proceeding of the 14th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} 08},
	publisher = {ACM Press},
	author = {Pedreshi, Dino and Ruggieri, Salvatore and Turini, Franco},
	year = {2008},
	pages = {560},
	file = {Full Text:C\:\\Users\\pbreh\\Zotero\\storage\\PT5QDV2C\\Pedreshi et al. - 2008 - Discrimination-aware data mining.pdf:application/pdf},
}

@article{dastin_amazon_2018,
	chapter = {Retail},
	title = {Amazon scraps secret {AI} recruiting tool that showed bias against women},
	url = {https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G},
	abstract = {Amazon.com Inc's {\textless}AMZN.O{\textgreater} machine-learning specialists uncovered a big problem: their new recruiting engine did not like women.},
	language = {en},
	urldate = {2022-07-07},
	journal = {Reuters},
	author = {Dastin, Jeffrey},
	month = oct,
	year = {2018},
	keywords = {Europe, JOBS, Politics, Science, Internet, All Retail, AMAZON, AUTOMATION, Civil Rights, COM, Company News, Computer Sciences, Department Stores (TRBC level 4), Employment Services (NEC) (TRBC level 5), Enterprise Reporting, General News, Government, Graphics, Human Resources Consulting Services (TRBC level 5), Human Rights, Information Technologies, INSIGHT, Insights, Internet \& Mail Order Department Stores (TRBC level 5), Major News, Pictures, Reuters Top News, Scientific \& Super Computers (TRBC level 5), Social Issues, Society, Software \& IT Services (TRBC level 3), Special Reports, Technology, Technology (TRBC level 1), US, Video Available, Women's Issues, Workforce, World Wide Web},
	file = {Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\UXD2468J\\amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G.html:text/html},
}

@inproceedings{grgic-hlaca_case_2016,
	address = {Barcelona, Spain},
	title = {The {Case} for {Process} {Fairness} in {Learning}: {Feature} {Selection} for {Fair} {Decision} {Making}},
	abstract = {Machine learning methods are increasingly being used to inform, or sometimes even directly to make, important decisions about humans. A number of recent works have focussed on the fairness of the outcomes of such decisions, particularly on avoiding decisions that affect users of different sensitive groups (e.g., race, gender) disparately. In this paper, we propose to consider the fairness of the process of decision making. Process fairness can be measured by estimating the degree to which people consider various features to be fair to use when making an important legal decision. We examine the task of predicting whether or not a prisoner is likely to commit a crime again once released by analyzing the dataset considered by ProPublica relating to the COMPAS system. We introduce new measures of people’s discomfort with using various features, show how these measures can be estimated, and consider the effect of removing the uncomfortable features on prediction accuracy and on outcome fairness. Our empirical analysis suggests that process fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.},
	language = {en},
	booktitle = {Symposium on {Machine} {Learning} and the {Law}},
	author = {Grgic-Hlacˇa, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
	year = {2016},
	pages = {11},
	file = {Grgic-Hlacˇa et al. - The Case for Process Fairness in Learning Feature.pdf:C\:\\Users\\pbreh\\Zotero\\storage\\R6ZBZQMW\\Grgic-Hlacˇa et al. - The Case for Process Fairness in Learning Feature.pdf:application/pdf},
}

@book{saini_superior_2019,
	address = {Boston},
	title = {Superior: the return of race science},
	isbn = {978-0-8070-7694-1},
	shorttitle = {Superior},
	publisher = {Beacon Press},
	author = {Saini, Angela},
	year = {2019},
	keywords = {Research, Eugenics, Race, SCIENCE / Philosophy \& Social Aspects, SOCIAL SCIENCE / Anthropology / Cultural, SOCIAL SCIENCE / Discrimination \& Race Relations},
	annote = {"A powerful look at the non-scientific history of "race science," and the assumptions, prejudices, and incentives that have allowed it to reemerge in contemporary science Superior tells the disturbing story of the persistent thread of belief in biological racial differences in the world of science. After the horrors of the Nazi regime in WWII, the mainstream scientific world turned its back on eugenics and the study of racial difference. But a worldwide network of unrepentant eugenicists quietly founded journals and funded research, providing the kind of shoddy studies that were ultimately cited in Richard Hernstein's and Charles Murray's 1994 title, The Bell Curve, which purported to show differences in intelligence among races. If the vast majority of scientists and scholars disavowed these ideas, and considered race a social construct, it was still an idea that managed to somehow make its way into the research into the human genome that began in earnest in the mid-1990s and continues today. Dissecting the statements and work of contemporary scientists studying human biodiversity, most of whom claim to be just following the data, Saini shows us how, again and again, science is retrofitted to accommodate race. Even as our understanding of highly complex traits like intelligence, and the complicated effect of environmental influences on human beings, from the molecular level on up, grows, the hope of finding simple genetic differences between "races"--to explain differing rates of disease, to explain poverty or test scores or to justify cultural assumptions--stubbornly persists. At a time when racialized nationalisms are a resurgent threat throughout the world, Superior is a powerful reminder that biologically, we are all far more alike than different"-- "In Superior award-winning science writer Angela Saini explores the concept of race, past and present. She examines the dark roots of race research and how race has again crept gently back into science and medicine. And she investigates the people who use this research for their own political purposes, including white supremacists. They believe that populations are born different, in character and intellectually, and that this defines the success or failure of nations. It is a worldwide network of eugenicists with their own journals journals and sources of funding, providing the kind of shoddy studies that were ultimately cited in Richard Hernstein's and Charles Murray's 1994 title, The Bell Curve, which purported to show differences in intelligence among races. Taking us from Darwin through the civil rights movement to modern-day ancestry testing, Saini examines how deeply our present is influenced by our past, and the role that politics has so often had to play in our understanding of race. Superior is a powerful, rigorous, much needed examination of the insidious history and damaging consequences of race science and the unfortunate reasons behind its apparent recent resurgence across the globe"--},
}

@article{belkhir_race_1994,
	title = {Race, {Sex}, {Class} \& "{Intelligence}" {Scientific} {Racism}, {Sexism} \& {Classism}},
	volume = {1},
	issn = {10758925},
	url = {http://www.jstor.org.virtual.anu.edu.au/stable/41680221},
	number = {2},
	urldate = {2022-07-07},
	journal = {Race, Sex \& Class},
	author = {Belkhir, Jean},
	year = {1994},
	note = {Publisher: Jean Ait Belkhir, Race, Gender \& Class Journal},
	pages = {53--83},
}

@book{oneil_weapons_2016,
	address = {New York},
	edition = {First edition},
	title = {Weapons of math destruction: how big data increases inequality and threatens democracy},
	isbn = {978-0-553-41881-1 978-0-553-41883-5},
	shorttitle = {Weapons of math destruction},
	publisher = {Crown},
	author = {O'Neil, Cathy},
	year = {2016},
	keywords = {Social aspects, Social conditions, Democracy, United States, 21st century, Big data, Mathematical models Moral and ethical aspects, Political aspects, Social indicators},
}

@inproceedings{komiyama_nonconvex_2018,
	title = {Nonconvex {Optimization} for {Regression} with {Fairness} {Constraints}},
	url = {https://proceedings.mlr.press/v80/komiyama18a.html},
	abstract = {The unfairness of a regressor is evaluated by measuring the correlation between the estimator and the sensitive attribute (e.g., race, gender, age), and the coefficient of determination (CoD) is a natural extension of the correlation coefficient when more than one sensitive attribute exists. As is well known, there is a trade-off between fairness and accuracy of a regressor, which implies a perfectly fair optimizer does not always yield a useful prediction. Taking this into consideration, we optimize the accuracy of the estimation subject to a user-defined level of fairness. However, a fairness level as a constraint induces a nonconvexity of the feasible region, which disables the use of an off-the-shelf convex optimizer. Despite such nonconvexity, we show an exact solution is available by using tools of global optimization theory. Furthermore, we propose a nonlinear extension of the method by kernel representation. Unlike most of existing fairness-aware machine learning methods, our method allows us to deal with numeric and multiple sensitive attributes.},
	language = {en},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Komiyama, Junpei and Takeda, Akiko and Honda, Junya and Shimao, Hajime},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2737--2746},
	file = {Full Text PDF:C\:\\Users\\pbreh\\Zotero\\storage\\GN3B3NXG\\Komiyama et al. - 2018 - Nonconvex Optimization for Regression with Fairnes.pdf:application/pdf;Supplementary PDF:C\:\\Users\\pbreh\\Zotero\\storage\\PWTBAG25\\Komiyama et al. - 2018 - Nonconvex Optimization for Regression with Fairnes.pdf:application/pdf},
}

@misc{scutari_fairml_2022,
	title = {{FairML}},
	url = {https://cran.r-project.org/web/packages/fairml/fairml.pdf},
	urldate = {2022-07-11},
	author = {Scutari, Marco},
	year = {2022},
	file = {fairml.pdf:C\:\\Users\\pbreh\\Zotero\\storage\\5PUYWMIN\\fairml.pdf:application/pdf},
}

@article{czapanskiy_women_1989,
	title = {Women in the {Law} {School}: {It}'s {Time} for {More} {Change}},
	volume = {7},
	language = {en},
	number = {1},
	journal = {Minnesota Journal of Law \& Inequality},
	author = {Czapanskiy, Karen B and Singer, Jana B},
	year = {1989},
	pages = {13},
	file = {Czapanskiy and Singer - Women in the Law School It's Time for More Change.pdf:C\:\\Users\\pbreh\\Zotero\\storage\\JQER8349\\Czapanskiy and Singer - Women in the Law School It's Time for More Change.pdf:application/pdf},
}

@article{lee_asian_2021,
	title = {Asian {Americans}, {Affirmative} {Action} \& the {Rise} in {Anti}-{Asian} {Hate}},
	volume = {150},
	issn = {0011-5266},
	url = {https://doi.org/10.1162/daed_a_01854},
	doi = {10.1162/daed_a_01854},
	abstract = {No court case in recent history has propelled Asian Americans into the political sphere like Students for Fair Admissions v. Harvard, and no issue has galvanized them like affirmative action. Asian Americans have taken center stage in the latest battle over affirmative action, yet their voices have been muted in favor of narratives that paint them as victims of affirmative action who ardently oppose the policy. Bridging theory and research on immigration, stereotypes, and boundaries, I provide a holistic portrait of SFFA v. Harvard and focus on Asian Americans' role in it. Immigration has remade Asian Americans from “unassimilable to exceptional,” and wedged them between underrepresented minorities who stand to gain most from the policy and the advantaged majority who stands to lose most because of it. Presumed competent and morally deserving, Asian Americans subscribe to the stereotype, and wield it to their advantage. Competence, moral worth, and respectability politics, however, are no safeguards against racism and xenophobia. As fears of the coronavirus arrested the United States, so too has the rise in anti-Asian hate.},
	number = {2},
	urldate = {2022-12-07},
	journal = {Daedalus},
	author = {Lee, Jennifer},
	month = jan,
	year = {2021},
	pages = {180--198},
}

@misc{zhao_costs_2021,
	title = {Costs and {Benefits} of {Fair} {Regression}},
	url = {http://arxiv.org/abs/2106.08812},
	abstract = {Real-world applications of machine learning tools in high-stakes domains are often regulated to be fair, in the sense that the predicted target should satisfy some quantitative notion of parity with respect to a protected attribute. However, the exact tradeoff between fairness and accuracy with a real-valued target is not entirely clear. In this paper, we characterize the inherent tradeoff between statistical parity and accuracy in the regression setting by providing a lower bound on the error of any fair regressor. Our lower bound is sharp, algorithm-independent, and admits a simple interpretation: when the moments of the target differ between groups, any fair algorithm has to make an error on at least one of the groups. We further extend this result to give a lower bound on the joint error of any (approximately) fair algorithm, using the Wasserstein distance to measure the quality of the approximation. With our novel lower bound, we also show that the price paid by a fair regressor that does not take the protected attribute as input is less than that of a fair regressor with explicit access to the protected attribute. On the upside, we establish the first connection between individual fairness, accuracy parity, and the Wasserstein distance by showing that if a regressor is individually fair, it also approximately verifies the accuracy parity, where the gap is given by the Wasserstein distance between the two groups. Inspired by our theoretical results, we develop a practical algorithm for fair regression through the lens of representation learning, and conduct experiments on a real-world dataset to corroborate our findings.},
	urldate = {2022-07-13},
	publisher = {arXiv},
	author = {Zhao, Han},
	month = oct,
	year = {2021},
	note = {arXiv:2106.08812 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbreh\\Zotero\\storage\\TC44KSXG\\Zhao - 2021 - Costs and Benefits of Fair Regression.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\5DXAEHR7\\2106.html:text/html},
}

@incollection{boxill_black_2022,
	edition = {Spring 2022},
	title = {Black {Reparations}},
	url = {https://plato.stanford.edu/archives/spr2022/entries/black-reparations/},
	abstract = {States have long demanded reparations from other states at the endof wars. More recently non-state actors such as the Aborigines ofAustralia, the Maori of New Zealand, and many American Indian nationsof North America are demanding the return of their tribal lands fromEuropeans as reparations; Eastern Europeans dispossessed by socialistgovernments are demanding the return of their property as reparations;and U.S. blacks (black people whose genealogy traces back to slavery within the U.S.[1]) are demanding reparations from the United States of America for theharmful wrongdoings to them caused by U.S. slavery and itsaftermath. The last of these demands is our subject.},
	urldate = {2022-07-13},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Boxill, Bernard and Corlett, J. Angelo},
	editor = {Zalta, Edward N.},
	year = {2022},
	file = {SEP - Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\U7JNVJYG\\black-reparations.html:text/html},
}

@misc{daoud_statistical_2020,
	title = {Statistical modeling: the three cultures},
	shorttitle = {Statistical modeling},
	url = {http://arxiv.org/abs/2012.04570},
	abstract = {Two decades ago, Leo Breiman identified two cultures for statistical modeling. The data modeling culture (DMC) refers to practices aiming to conduct statistical inference on one or several quantities of interest. The algorithmic modeling culture (AMC) refers to practices defining a machine-learning (ML) procedure that generates accurate predictions about an event of interest. Breiman argued that statisticians should give more attention to AMC than to DMC, because of the strengths of ML in adapting to data. While twenty years later, DMC has lost some of its dominant role in statistics because of the data-science revolution, we observe that this culture is still the leading practice in the natural and social sciences. DMC is the modus operandi because of the influence of the established scientific method, called the hypothetico-deductive scientific method. Despite the incompatibilities of AMC with this scientific method, among some research groups, AMC and DMC cultures mix intensely. We argue that this mixing has formed a fertile spawning pool for a mutated culture that we called the hybrid modeling culture (HMC) where prediction and inference have fused into new procedures where they reinforce one another. This article identifies key characteristics of HMC, thereby facilitating the scientific endeavor and fueling the evolution of statistical cultures towards better practices. By better, we mean increasingly reliable, valid, and efficient statistical practices in analyzing causal relationships. In combining inference and prediction, the result of HMC is that the distinction between prediction and inference, taken to its limit, melts away. We qualify our melting-away argument by describing three HMC practices, where each practice captures an aspect of the scientific cycle, namely, ML for causal inference, ML for data acquisition, and ML for theory prediction.},
	urldate = {2022-10-01},
	publisher = {arXiv},
	author = {Daoud, Adel and Dubhashi, Devdatt},
	month = dec,
	year = {2020},
	note = {arXiv:2012.04570 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Statistics - Methodology},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbreh\\Zotero\\storage\\MA67CNYW\\Daoud and Dubhashi - 2020 - Statistical modeling the three cultures.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbreh\\Zotero\\storage\\TMK6XEME\\2012.html:text/html},
}

@article{clarke_against_2015,
	title = {Against {Immutability}},
	volume = {125},
	issn = {0044-0094},
	abstract = {Courts often hold that antidiscrimination law protects "immutable" characteristics, like sex and race. In a series of recent cases, gay rights advocates have persuaded courts to expand the concept of immutability to include not just those traits an individual cannot change, but also those considered too important for anyone to be asked to change. Sexual orientation and religion are paradigmatic examples. This Article critically examines this new concept of immutability, asking whether it is fundamentally different from the old one and how it might apply to characteristics on the borders of employment discrimination law's protection, such as obesity, pregnancy, and criminal records. It argues that the new immutability does not avoid the old version's troublesome judgments about which traits are morally blameworthy and introduces new difficulties by requiring problematic judgments about which traits are important. Ultimately, immutability considerations of both the old and new varieties distract from the aim of employment discrimination law: targeting unreasonable and systemic forms of bias.},
	number = {1},
	journal = {The Yale law journal},
	author = {Clarke, Jessica A},
	year = {2015},
	note = {Place: New Haven
Publisher: The Yale Law Journal Company, Inc},
	keywords = {Social aspects, Studies, Race, Economics of Minorities, Races, Indigenous Peoples, and Immigrants, Non-labor Discrimination (J15), Antidiscrimination, BIAS, Bias (Law), Discrimination, Discrimination in employment, Economics of Gender, Non-labor Discrimination (J16), EMPLOYMENT DISCRIMINATION, Equality before the law, Gay rights, Gays \& lesbians, Illegal Behavior and the Enforcement of Law (K42), Labor Discrimination (J71), Labor law, Labor Law (K31), Law, Law and legislation, Laws, regulations and rules, Morality, Northern America, Personality, Personality traits, Pregnancy, Prejudices, Prevention, PRIVACY, Privacy, Right of, RELIGION, Remedies, SEXUAL ORIENTATION, U.S},
	pages = {2--102},
	annote = {YALE LAW JOURNAL, Vol. 125, No. 1, Oct 2015: 2-102},
}

@article{grubinger_evtree_2014,
title = {{evtree}: Evolutionary Learning of Globally Optimal
  Classification and Regression Trees in {R}},
author = {Thomas Grubinger and Achim Zeileis and Karl-Peter
  Pfeiffer},
journal = {Journal of Statistical Software},
year = {2014},
volume = {61},
number = {1},
pages = {1--29},
url = {http://www.jstatsoft.org/v61/i01/},
}

@article{wright_ranger_2017,
title = {{ranger}: A Fast Implementation of Random Forests for High
  Dimensional Data in {C++} and {R}},
author = {Marvin N. Wright and Andreas Ziegler},
journal = {Journal of Statistical Software},
year = {2017},
volume = {77},
number = {1},
pages = {1--17},
doi = {10.18637/jss.v077.i01},
}

@book{Greene2003Econometric,
  added-at = {2018-06-18T21:23:34.000+0200},
  author = {Greene, William H.},
  biburl = {https://www.bibsonomy.org/bibtex/28ded70f02507563c15bfdd8dd2208e12/pbett},
  citeulike-article-id = {14358797},
  citeulike-attachment-1 = {Greene_EconometricAnalysis_5thEd_2002.pdf; /pdf/user/pbett/article/14358797/1109751/Greene_EconometricAnalysis_5thEd_2002.pdf; 555dcbdff6a834feae11ed60d48b3ca8f6e3c5b3},
  citeulike-linkout-0 = {http://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm},
  comment = {(private-note)Used as the reference for the statsmodels python package for prediction intervals - see p111.},
  edition = {Fifth},
  file = {Greene_EconometricAnalysis_5thEd_2002.pdf},
  interhash = {fb74ec9471c97eb32162f303c71262e1},
  intrahash = {8ded70f02507563c15bfdd8dd2208e12},
  isbn = {0-13-066189-9},
  keywords = {textbook statistics economics},
  posted-at = {2017-05-17 18:18:23},
  priority = {2},
  publisher = {Pearson Education},
  timestamp = {2018-06-22T18:36:55.000+0200},
  title = {Econometric Analysis},
  url = {http://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm},
  year = 2003
}

