\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm2e}
\RestyleAlgo{ruled}
\usepackage{duckuments}
\usepackage{float}
\usepackage{multicol}
\usepackage[sort, numbers, square]{natbib}
\usepackage{color}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{subcaption}
\usepackage{pifont}% http://ctan.org/pkg/pifont

\definecolor{maroon}{cmyk}{0,0.87,0.68,0.32}
\definecolor{customgreen}{cmyk}{1,0,1,0.5}

\newcommand{\cmark}{\color{customgreen}\ding{51}}%
\newcommand{\xmark}{\color{maroon}\ding{55}}%

\usepackage[font=footnotesize,labelfont=bf]{caption}
% \usepackage[font=scriptsize]{subcaption}

\newcommand{\Jen}[1]{\textcolor{maroon}{Jen: #1}}
\newcommand{\Laura}[1]{\textcolor{red}{Laura: #1}}
\newcommand{\SY}[1]{\textcolor{blue}{SY: #1}}
\newcommand{\Ours}{Diffusion-HPC}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{5408} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{\textit{Supplemental Material} for \\
Diffusion-HPC: Generating Synthetic Images with Realistic Humans}

% High-level idea of our method 
\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\section{Code and Pre-trained Models}
For reproducibility, we provide code with pre-trained models at \url{https://anonymous.4open.science/r/Diffusion_HPC-7887}.\footnote{Per latest submission guidelines, anonymous Github link is acceptable.}
% \Laura{I thought the submission guideliness didn't allow for links to the code?}


\section{Additional Implementation Details}
\textbf{Quantitative evaluation in Table 1.} We compute quantitative metrics using roughly 10,000 images generated from MPII \cite{andriluka20142d} and SMART \cite{chen2021sportscap} prompts, respectively. 

For MPII, we use ``\{image description\} of \{person\} doing \{action\}" as the text prompts, where ``\{person\}" can be single- or multi-person descriptions of person(s) of interest, and ``\{action\}" are the activity categories from MPII. (We exclude categories ``inactivity, quite/light" and miscellaneous" because they do not describe a specific activity.) For example, resulting prompts could be ``a nice photo of a man doing water activities." or ``a high-resolution photo of a group of people doing conditioning excercises".

Text prompts for SMART are constructed using the template ``a photo of an athlete doing \{action\}" where action is one of ``high jump", ``vault", ``pole vault", ``diving", ``gymnastics on uneven bars", and ``gymnastics on a balance beam".

\textbf{Use VPoser \cite{SMPL-X:2019} to determine pose difficulty.}
VPoser is a Variational Auto-Encoder (VAE) that is trained on a massive database of realistic human poses \cite{mahmood2019amass}. By design, poses that are farther away from the canonical pose (i.e. challenging poses) have larger variance in the embedding space. Therefore, we identify a difficult pose $\theta$ if its embedding $e_{\theta}$ have larger norm, i.e. $||e_{\theta}||_2 > \tau$. 
\begin{align}
    \{\mu, \sigma\} &= \mathcal{E}_{v}(\theta) \\
    e_{\theta} &\sim \mathcal{N} (\mu, \sigma)
\end{align}
$\mathcal{E}_{v}$ is the encoder of VPoser. $\tau$ is determined empirically and set to 30.

\textbf{Data processing for downstream experiments.}
Following previous works \cite{kolotouros2019learning,weng2022domain}, we crop the images such that the persons (localized by ground truth 2D keypoints) are centered in the crop. In addition, the persons are scaled such that the torso (i.e. mean distance between left/right shoulder and hip) are roughly one third of the crop size ($224 \times 224$).

\section{Additional Qualitative Results}
Here we include additional qualitative results as well as failure cases for the text-conditioned and pose-conditioned generations (Sections 4.1 in the main paper).

\subsection{Text-Conditioned Generation}
Figure~\ref{fig:qual_text} shows qualitative comparisons of Stable Diffusion \citep{Rombach_2022_CVPR} and \Ours{} on text-conditioned generations. The images were selected from those sampled for the user study.

In Figure \ref{fig:failure_text} we include typical failure cases of text-conditioned generations. In left and middle columns, the body structures are not sufficiently rectified. This is likely because that resolution of depth maps (used for conditioning) is limited ($64\times 64$), so consequently small humans with out-of-distribution poses are challenging to rectify. In the right column, we show a failure scenario when the HMR model (i.e. BEV \cite{sun2022putting}) fails to reconstruct the humans in close-up shots. We could consider filtering out close-up shots, as they are not the primary intended use cases for \Ours{}.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{iccv2023AuthorKit/supp_figures/supplemental_1.pdf}
    \caption{Comparison with Stable Diffusion \cite{Rombach_2022_CVPR} on text-conditioned generations. Row 1 and rows 2-3 are generated with MPII \cite{andriluka20142d} and SMART \cite{chen2021sportscap} prompts, respectively. Red arrows point out implausible body parts in Stable Diffusion generations.}
    \label{fig:qual_text}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{iccv2023AuthorKit/supp_figures/supplemental_2.pdf}
    \caption{Failure cases on text-conditioned generations. }
    \label{fig:failure_text}
\end{figure*}

% \begin{figure}
% \centering
% \includegraphics[width=\linewidth]{iccv2023AuthorKit/supp_figures/supp_qual_text_smart.pdf}
% \caption{Additional qualitative comparisons to Stable Diffusion \citep{Rombach_2022_CVPR} on the SMART dataset. Left columns: Successful examples. Right columns: Unsuccessful examples.}
% \label{fig:qual_smart_text}
% \end{figure}

% \begin{figure}
% \centering
% \includegraphics[width=\linewidth]{iccv2023AuthorKit/supp_figures/supp_qual_text_mpii.pdf}
% \caption{Additional qualitative comparisons to Stable Diffusion \citep{Rombach_2022_CVPR} on the MPII dataset. Left columns: Successful examples. Right columns: Unsuccessful examples.}
% \label{fig:qual_mpii_text}
% \end{figure}

\subsection{Pose-Conditioned Generation}
Figure~\ref{fig:qual_pose} shows qualitative comparisons of \citet{brooks2022hallucinating} and \Ours{} on pose-conditioned generations, and
Figure~\ref{fig:failure_pose} shows failures cases of pose-conditione generations. As seen from ``Ours T+R" and ``Ours T+R+D", human-object interactions are sometimes not preserved. 

Note that in \Ours{}, human-object interactions are considered but not modelled in an explicit way. Specifically, when we construct the depth map, we use Mask R-CNN \cite{he2017mask} to segment out the occluded body part, which helps with scenarios when, for instance, the person is riding the horse (row 1 of Figure \ref{fig:qual_pose}). However, row 2 of figure~\ref{fig:failure_pose} shows a failure case where the boat is not detected by Mask R-CNN.

In addition, in \Ours{}, latents from the initial generations help preserve the objects and context in the final generated scenes. For the pose-conditioned generations here, latents of real images help capture the background objects such as horse and surfboard (row 1 and 3 of Figure~\ref{fig:qual_pose}). However, when the background object is occluded or small (row 2 in Figure~\ref{fig:qual_pose} and row 1 in Figure~\ref{fig:failure_pose}), the latents are not sufficient in preserving the object in the final generations. Future work could consider extending \Ours{} by explicitly modelling the human-object/scene interaction. 

\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{iccv2023AuthorKit/supp_figures/supplemental_3.pdf}
\caption{Additional qualitative comparisons to \citet{brooks2022hallucinating} on the MPII dataset. Input 2D keypoints to \citet{brooks2022hallucinating} are overlayed on the bottom left in column 2. Top 3 rows are from MPII, and bottom 3 rows are from SMART. Our generations conditioned on text (T), real images (R). ``(D)" means the diffusion model is finetuned on the target dataset (MPII and SMART respectively).}
\label{fig:qual_pose}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{iccv2023AuthorKit/supp_figures/supplemental_4.pdf}
\caption{Failure cases on pose-conditioned generations.}
\label{fig:failure_pose}
\end{figure*}

\newpage
{\small
% \bibliographystyle{ieee_fullname}
\bibliographystyle{abbrvnat}
\bibliography{egbib}
}
\end{document}



% Figure~\ref{fig:qual_mpii_var} shows how \Ours{} is able to leverage different input modes to increase the variety of scenes for a given action via conditional image generation while producing plausible humans. The conditioning input allows for a control in the degree of similarity between the generated and reference real images. For example, the text-only conditioned generation (T) both in and out of domain (D) creates scenes further from the reference image, while the images with both text and real image guidance (T+D) are more similar to the scene in the real reference image.

% In addition, Figures~\ref{fig:qual_mpii_var} and \ref{fig:qual_mpii_vs} present examples of how \Ours{} compared to \citet{brooks2022hallucinating} prioritizes generating relevant attributes for an action domain instead of recreating a particular reference pose. Figure~\ref{fig:qual_mpii_vs} shows the challenges of producing valid human-object interaction scenes for both approaches, with the top 3 and bottom 3 rows depicting successful and unsuccessful examples for \Ours{}, respectively.

% In contrast to the actions in the MPII dataset, the actions of the SMART dataset correspond to finer-grained categories with more challenging poses. Figure~\ref{fig:qual_smart_vs} highlights how for these actions the generations benefit from being in domain (D), and leveraging both the text (T) and the real image (R) for the conditioning. For example, \Ours{} for an action such as diving that can be interpreted to refer to competitive diving or underwater diving generates better in domain results when guiding the general text prompt with a real image. Similarly to the results on the MPII dataset, when the human in the scene is unoccluded our approach can generate plausible persons even when confronted with challenging poses (first 4 rows of Figure~\ref{fig:qual_smart_vs}).

