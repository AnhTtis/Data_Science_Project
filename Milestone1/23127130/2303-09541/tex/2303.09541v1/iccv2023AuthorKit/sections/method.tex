\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{iccv2023AuthorKit/figures/main_figure.pdf}
\caption{Overview of \Ours{}. The generation process can be broken down into 3 steps. \textcolor{cyan}{Step 1}: Obtaining image latents $z$ from the initial generation $\mathcal{I}$ of a pre-trained text-to-image model (i.e. Stable Diffusion \cite{Rombach_2022_CVPR}) and injecting noise. \textcolor{pink}{Step 2}: Estimating human body mesh $\mathcal{M} (\theta, \beta)$ from $\mathcal{I}$. If the pose is challenging based on a pose prior (i.e. VPoser \cite{SMPL-X:2019}) then render the mesh's depth map $d_{fg}$ and introduce occlusions via object masks obtained from a segmentation model. \textcolor{teal}{Step 3}: Using the latents $z$, foreground depths, and the text embeddings $t$ as guide for the final generation $\mathcal{I^*}$. 
% \SY{ideally connect steps better to the main text. in main text, refer to these. maybe name them and label them for each colored box in the figure? Since there's a lot going on in the figure. Also there are terms like Unet not mentioned in the main text}
}
\label{fig:figure_2}
\end{figure*}

\section{\Ours{}}
% In Section \ref{sec_method:background}, we first present relevant background information on latent diffusion models, human body representation, and human mesh recovery. Then, in Section \ref{sec_method:data_gen_pipeline} we introduce our proposed data generation method \Ours{}. Lastly, in Section \ref{sec_method:finetuning}, we demonstrate a downstream application of \Ours{} for single-view human mesh recovery \SY{again, be precise here, you don't use it standalone for HMR, you use it to contribute to a part of HMR}.

\subsection{Background}
\label{sec_method:background}
\textbf{Latent diffusion models.} Diffusion models are deep generative models that generate samples from a desired distribution by learning to reverse a gradual noising process. The sampling process starts from noise sampled from a standard normal distribution, which are refined into a series of less-noisy latents that eventually lead to the desired generation. For more details, please refer to Dhariwal \etal \cite{dhariwal2021diffusion} and Ho \etal \cite{ho2020denoising}. Latent Diffusion uses a perceptual compression model, a variational autoencoder (VAE) that projects the data distribution into a latent space, where the conditional diffusion process operates. Previous works \cite{avrahami2022latent} have shown that editing in the latent space is faster than pixel space editing \cite{avrahami2022blended} and helps to avoid pixel-level artifacts. Our method uses two latent diffusion models under the hood, a text-to-image model where the denoising is conditioned on the text input, and a depth-to-image model where the depth map is used as additional conditioning.

\textbf{SMPL body model.} We use the Skinned Multi-Person Linear (SMPL) model \cite{loper2015smpl} to represent the 3D mesh of the human body. SMPL is a differentiable function $\mathcal{M}(\theta, \beta)$ that takes a pose parameter $\theta \in \mathbb{R}^{69}$ and shape parameter $\beta \in \mathbb{R}^{10}$, and returns the body mesh $\mathcal{M} \in \mathbb{R}^{6890 \times 3}$ with $6890$ vertices. The 3D joint locations $X \in \mathbb{R}^{k\times 3} = \mathcal{W} \mathcal{M}$ are regressed from the vertices, using a pre-trained linear regressor $W$, where $k$ is the number of joints.

\subsection{Data generation process}
\label{sec_method:data_gen_pipeline}
\Ours{} consists of three main steps. As a first step, we leverage a text-to-image model (i.e. Stable Diffusion \cite{Rombach_2022_CVPR}) to produce an initial generation of a posed person. Second, we predict the pose of the person and determine the difficulty level of the pose in the initial generation using a pose prior. We observe that Stable Diffusion tends to generate worse anatomy on more difficult poses. Thus, if the initial generation contains a hard pose, we render the depth map of the predicted body mesh taking into consideration the occlusion from other objects in the image. The depth map serves as the human structure prior. In the final step, we use the context information (in the form of image latents) from initial generation as a starting point, and leverage a depth-to-image model (i.e. a fine-tuned version of Stable Diffusion) to produce final generations by conditioning on the depth map from previous step.

Figure \ref{fig:figure_2} shows an overview of our method.
% \SY{this whole subsection is too sparse. Give the reader the main ideas and concepts before jumping into the specifics of each step.} 
Concretely, given a text prompt $t$ that describes the action of a person, we first encode it to text embeddings $z_t$, and then
% \SY{per comment above, reader does not understand at this point why we are first doing this. Please this in the context of the overall idea first} 
use a text-to-image Stable Diffusion model ($\mathcal{G}$) to generate an image $\mathcal{I} = \mathcal{G}(z_t)$. The image is passed to encoder $\mathcal{E}_{\mathcal{G}_d}$ of the compression module of $\mathcal{G}_d$, a depth-to-image diffusion model, to get the compressed image latents $z \in \mathbb{R}^{4 \times 64 \times 64}$.
\begin{align}
    z =& \mathcal{E}_{\mathcal{G}_d}(\mathcal{I})
\end{align}
% \SY{again, reader doesn't have context as to why this is happening next} 
Image latents $z$ contain context information about $\mathcal{I}$ such as the texture of the image and background layout. We can use $z$ as a starting point in the final generation process so the context from the inital generation is roughly preserved.

Next, we use an off-the-shelf model to reconstruct the 3D mesh of the person in $\mathcal{I}$. Specifically, we estimate the human pose $\theta$, shape $\beta$, and parameters of a weak perspective camera $\Pi$ from the image using an off-the-shelf HMR model $f: \mathcal{I} \rightarrow (\theta, \beta, \Pi)$. Since our method focuses on rectifying implausible human generations, we determine whether the initial generation is likely to contain implausible humans and only apply our rectification process on images with hard poses. We observe that Stable Diffusion tend to generate worse anatomy on more difficult poses. Hence, we use a pre-trained human pose prior VPoser \cite{SMPL-X:2019} as a proxy for determining if the person in image $\mathcal{I}$ has a challenging pose, as challenging poses tend to have large variance in the VPoser embedding space.

% If the estimated pose is not challenging enough we sample a new image. \SY{confusing / not sure what is going on... Unclear why you would want to sample a new image if the pose is not challenging enough. Isn't it fine to leave it as is? Needs explanation} This process encourages variety and difficult examples in our generated images. \SY{hi Jen explain, so far you have said you just want to fix SD, not necessarily increase variety and difficulty}

% \SY{I will stop making this comment, you can apply to everything else, but again not clear to the reader what the point of attaining the silhouette is. Please add much more intuition about each of your ideas, as well as why they are interesting, solve specific challenges, or novel} 

Now that we have the predicted human pose from $\mathcal{I}$, we move on to the final step of our method where we inject pose information $\mathcal{M}(\theta, \beta)$ into the generation process to produce a more plausible image of a person with the predicted pose $\theta$. We achieve this by leveraging a depth-to-image version of Stable Diffusion, and using the depth values of the predicted human body as conditioning information in the generation process. Specifically, we render the 3D mesh to obtain the depth map $d_{fg} \in \mathbb{R}^{64 \times 64}$. Since there might be other objects in the image that occlude part of the person, we use Mask R-CNN \cite{he2017mask} (pre-trained on COCO \cite{lin2014microsoft}) to segment the non-human objects in the image and use the segmentation masks to mask out the occluded body part in the depth map $d_{fg}$. Formally,
% attain the person's silhouette $s$ from the positive depth values of the depth rendered person. Aided by the silhouette, we update the depth map with occlusions in the scene by using the object masks $m$ from a pre-trained segmentation model \SY{explain the segmentation model more. What objects are considered?}.%Both $m$ and $s$ are downsampled to the same dimension as the latents $z$.%We then use the union of $m$ and $s$ to select the background latents $z_{bg}$ we want to keep.
\begin{align}
    \{\theta, \beta, \Pi\} &= f(\mathcal{I}) \label{eq:mesh} \\
    % \{d_{fg}, s\} &= \mathcal{R}_d (\Pi, \mathcal{M} (\theta, \beta)) \\
    d_{fg} &= \mathcal{R}_d (\Pi, \mathcal{M} (\theta, \beta))  \label{eq:render_depth} \\ 
    d_{fg}^* &= d_{fg} \odot ((1 - m) \cap (d_{fg} > 0))
    %z_{bg} &= z \odot (1 - m \cup s) + \mathcal{N}(0, I) \odot (m \cup s)
\end{align}
where $\mathcal{R}_d$ is a depth renderer that renders the depth map of a mesh, $\odot$ denotes the Hadamard product, and $(d_{fg} > 0)$ is the silhouette of the rendered person.

Finally, to preserve the context information (e.g. texture and background layout) of the initial generation, we use initial image latents as a starting point in the final generation process. We add noise to $z$, and use a pre-trained denoising model (i.e. depth-to-image Stable Diffusion) to perform sequential denoising steps which produces the final image latents $z^*$. The denoising process (achieved through a pretrained UNet \cite{ronneberger2015u}) is guided by both the depth map $d_{fg}^*$ and text embeddings $z_t$. Final generation is obtained by decoding the $z^*$ with the compression module's decoder $\mathcal{D}_{\mathcal{G}_d}$. 
\begin{align}
    z^{noised} & = noise(z) \\
    z^* & = denoise(z^{noised}; d_{fg}^*, z_t) \\
    \mathcal{I^*} & = \mathcal{D}_{\mathcal{G}_d} (z^*) \label{eq:final_image}
\end{align}
As shown in Figure \ref{fig:figure_2}, final generation $\mathcal{I}^*$ contains similar texture and background as the original image, but the human body anatomy is rectified.

\subsection{Downstream application: finetuning Human Mesh Recovery on challenging domains using synthetic data}
\label{sec_method:finetuning}
% \SY{need much more intro. Talk about how synthetic data can be useful for downstream tasks} 
Training a single-view Human Mesh Recovery (HMR) model end-to-end would require large amounts of images with paired 3D ground truths. Collecting such traning sets requires burdensome motion capturing systems and is often limited to indoor laboratories. As a result, previous works such as \cite{weng2022domain} have focused on finetuning HMR model to a particular challenging domain using weak supervision (image paired with 2D keypoints). In this section, we introduce an essential downstream application of \Ours{}, which is finetuning HMR models using image-mesh pairs from \Ours{} as training data. 

Next, we elaborate on the downstream setup. Given a pre-trained HMR model that predicts pose $\theta$, shape $\beta$ and camera matrix $\Pi$ from a single image $\mathcal{I}$ (i.e.
% \SY{model for what task? Also remind reader of variables since this is a new context / pretty different section}
$f: \mathcal{I} \rightarrow (\theta, \beta, \Pi)$), our goal is to 
% that is pre-trained on standard Motion Capture (MoCap) and in-the-wild 2D pose estimation datasets \SY{is it both necessarily? This paragraph of broccoli crowns in a lot of information, take your time to set up more slowly}, and 
adapt the model to a new target-domain by finetuning $f$ on a small set of target images.
% \SY{also clarify $\mathcal{I}$ in this context}

% \SY{again, set up this task better. What is the precise setting, e.g. weakly supervised HMR w/ 2D ground truth, etc.}
In a typical finetuning setup where only 2D keypoints from the target are available as supervision, 2D reprojection loss can be minimized to encourage the consistency between predicted and ground truth keypoints. Formally, for an image from the target training set, let the ground truth 2D keypoints be $j \in \mathbb{R}^{k\times 2}$ with $k$ annotated keypoints per person, we would want to minimize $\mathcal{L}_{2D}^{real} = || \hat{j} - j||_2$
% \SY{remind / define projection matrices} 
% \begin{align}
%    \mathcal{L}_{2D}^{real} &= || \hat{j} - j||_2 
% \end{align}
where $\hat{j} = \Pi(\mathcal{W} \mathcal{M}(\hat{\theta}, \hat{\beta}))$ are the predicted 2D keypoints. Recall that $\mathcal{W}$ is the SMPL joint regressor, and $\Pi$ is the projection matrix of a weak perspective camera.

% \SY{rewrite better to highlight your key idea, not just run through description of what you do. E.g., given this task setting, Diffusion-HPC can be used to generate synthetic data that has xyz properties and offers XYZ benefits.} 
Given this task setting, \Ours{} can be used to generate synthetic data that has image-mesh pairs ($\mathcal{I}^*$ and $\{\theta, \beta, \Pi\}$ in Equations \ref{eq:mesh} and \ref{eq:final_image}). Then, on those synthetic image-mesh pairs, we can supervise the model with ground truth body parameters, which provide stronger form of supervision as compared to 2D keypoints.
\begin{align}
   \mathcal{L}_{3D}^{syn} &= || \hat{\beta} - \beta ||_2 + || \hat{\theta} - \theta ||_2 
\end{align}
Overall, our loss function during finetuning is 
\begin{align}
    \mathcal{L} &= \mathcal{L}_{2D}^{real} + \mathcal{L}_{3D}^{syn}
\end{align}

\textbf{Guidance from real images.} 
% \SY{don't get into empirical results yet. Instead say something like, in the case of a clear target data domain for the downstream HMR task, it can be useful to produce... also ``real guidance'' is kind of unclear. Maybe guidance from real images? or something better} Empirically we observe that it is better to produce synthetic images generated with real images as guidance. 
In the case of a clear target data domain for the downstream HMR task, it can be useful to produce training data that have similar appearances to the target training set. Specifically, instead of using text-to-image diffusion model to generate the initial $\mathcal{I}$, we use real images from the training set. This guidance helps reduce the domain gap between the generated and real images because the appearances and poses will be more similar to the expected ones. 

\textbf{Pose augmentation.} 
% \SY{smoother connection. e.g., Finally, we can further enhance the diversity... Also explain a little more, you slap a bunch of equations without really explaining in English what is going on} 
Finally, we can further enhance the diversity of the generated poses, by applying pose augmentations to the predicted poses. Specifically, after Equation \ref{eq:mesh}, we can augment $\theta$ before proceeding to Equation \ref{eq:render_depth}. Formally, we apply pose augmentation in the embedding space of VPoser as in \citet{weng2022domain},
% Furthermore, to enhance the diversity of the generated poses, we apply augmentations to the poses. We do this in the VPoser embedding space of $\theta$ as in \citet{weng2022domain}. Formally, 
\begin{align}
    \mu, \sigma = & \mathcal{E}_v(\theta) \\
    z_{\theta}^{aug} =& z_{\theta} \odot (1 + s \epsilon), z_{\theta} \sim \mathcal{N}(\mu, \sigma) \\
    \theta^{aug} =& \mathcal{D}_v (z_{\theta}^{aug})
\end{align}
where $\mathcal{E}_v$ and $\mathcal{D}_v$ are the encoder and decoder of VPoser, $s$ is a constant scalar, and $\epsilon$ is from a multivariate uniform distribution of the same dimension as the VPoser latent space.
