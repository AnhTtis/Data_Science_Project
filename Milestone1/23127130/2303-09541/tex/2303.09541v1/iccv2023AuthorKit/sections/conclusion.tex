\section{Limitations}
As we rely on large pre-trained models \citep{Rombach_2022_CVPR,schuhmann2022laion}, any biases in these models or datasets that they were trained on will be replicated onto our generated images. Due to the resolution of depth maps ($64 \times 64$), fine details such as fingers and facial expressions are challenging to synthesize. Besides, since we only render person depth maps, human-object/human-scene interactions may not be well-preserved in the final generation (e.g. the person and yoga mat in column 3, row 2 of Figure \ref{fig:generation}). While these limitations do not affect downstream tasks where we only care about the body pose, there is large room to improve the photo-realism of human-centric image synthesis, and for the synthetic data to be useful for a wider variety of downstream tasks such as expressive HMR \cite{SMPL-X:2019} and recovering human-object/scene interaction \cite{bhatnagar2022behave,zhang2020perceiving,weng2021holistic}. Lastly, as we use SMPL body representation, our method does not consider people with limb losses, but it can be adapted to do so.

\section{Conclusion}
We propose \Ours{}, a text-conditioned and training-free method that injects model-based human body prior to improve the human image generation quality of state-of-the-art text-conditioned and pose-conditioned generative models. Further, \Ours{} demonstrates excellent utility in a challenging downstream task, single-view human mesh recovery. We hope this work would encourage further investigation into the obstacles associated with human generation in off-the-shelf foundation generative models, as well as exploring innovative ways of using generative models to tackle the challenges in 3D human perception tasks.