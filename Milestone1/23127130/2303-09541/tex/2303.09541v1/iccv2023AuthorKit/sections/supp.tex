% This is used in the ARXIV version
\newpage
\section*{Supplementary Material}

\subsection*{A. Additional Implementation Details}
\textbf{Quantitative evaluation in Table 1.} We compute quantitative metrics using roughly 10,000 images generated from MPII \cite{andriluka20142d} and SMART \cite{chen2021sportscap} prompts, respectively. 

For MPII, we use ``\{image description\} of \{person\} doing \{action\}" as the text prompts, where ``\{person\}" can be single- or multi-person descriptions of person(s) of interest, and ``\{action\}" are the activity categories from MPII. (We exclude categories ``inactivity, quite/light" and miscellaneous" because they do not describe a specific activity.) For example, resulting prompts could be ``a nice photo of a man doing water activities." or ``a high-resolution photo of a group of people doing conditioning excercises".

Text prompts for SMART are constructed using the template ``a photo of an athlete doing \{action\}" where action is one of ``high jump", ``vault", ``pole vault", ``diving", ``gymnastics on uneven bars", and ``gymnastics on a balance beam".

\textbf{Use VPoser \cite{SMPL-X:2019} to determine pose difficulty.}
VPoser is a Variational Auto-Encoder (VAE) that is trained on a massive database of realistic human poses \cite{mahmood2019amass}. By design, poses that are farther away from the canonical pose (i.e. challenging poses) have larger variance in the embedding space. Therefore, we identify a difficult pose $\theta$ if its embedding $e_{\theta}$ have larger norm, i.e. $||e_{\theta}||_2 > \tau$. 
\begin{align}
    \{\mu, \sigma\} &= \mathcal{E}_{v}(\theta) \\
    e_{\theta} &\sim \mathcal{N} (\mu, \sigma)
\end{align}
$\mathcal{E}_{v}$ is the encoder of VPoser. $\tau$ is determined empirically and set to 30.

\textbf{Data processing for downstream experiments.}
Following previous works \cite{kolotouros2019learning,weng2022domain}, we crop the images such that the persons (localized by ground truth 2D keypoints) are centered in the crop. In addition, the persons are scaled such that the torso (i.e. mean distance between left/right shoulder and hip) are roughly one third of the crop size ($224 \times 224$).

\subsection*{B. Additional Qualitative Results}
Here we include additional qualitative results as well as failure cases for the text-conditioned and pose-conditioned generations (Section \ref{subsec:gen_quality}).

\subsubsection*{Text-Conditioned Generation}
Figure~\ref{fig:qual_text} shows qualitative comparisons of Stable Diffusion \citep{Rombach_2022_CVPR} and \Ours{} on text-conditioned generations. The images were selected from those sampled for the user study.

In Figure \ref{fig:failure_text} we include typical failure cases of text-conditioned generations. In left and middle columns, the body structures are not sufficiently rectified. This is likely because that resolution of depth maps (used for conditioning) is limited ($64\times 64$), so consequently small humans with out-of-distribution poses are challenging to rectify. In the right column, we show a failure scenario when the HMR model (i.e. BEV \cite{sun2022putting}) fails to reconstruct the humans in close-up shots. We could consider filtering out close-up shots, as they are not the primary intended use cases for \Ours{}.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{iccv2023AuthorKit/supp_figures/supplemental_1.pdf}
    \caption{Comparison with Stable Diffusion \cite{Rombach_2022_CVPR} on text-conditioned generations. Row 1 and rows 2-3 are generated with MPII \cite{andriluka20142d} and SMART \cite{chen2021sportscap} prompts, respectively. Red arrows point out implausible body parts in Stable Diffusion generations.}
    \label{fig:qual_text}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{iccv2023AuthorKit/supp_figures/supplemental_2.pdf}
    \caption{Failure cases on text-conditioned generations. }
    \label{fig:failure_text}
\end{figure*}


\subsubsection*{Pose-Conditioned Generation}
Figure~\ref{fig:qual_pose} shows qualitative comparisons of \citet{brooks2022hallucinating} and \Ours{} on pose-conditioned generations, and
Figure~\ref{fig:failure_pose} shows failures cases of pose-conditione generations. As seen from ``Ours T+R" and ``Ours T+R+D", human-object interactions are sometimes not preserved. 

Note that in \Ours{}, human-object interactions are considered but not modelled in an explicit way. Specifically, when we construct the depth map, we use Mask R-CNN \cite{he2017mask} to segment out the occluded body part, which helps with scenarios when, for instance, the person is riding the horse (row 1 of Figure \ref{fig:qual_pose}). However, row 2 of figure~\ref{fig:failure_pose} shows a failure case where the boat is not detected by Mask R-CNN.

In addition, in \Ours{}, latents from the initial generations help preserve the objects and context in the final generated scenes. For the pose-conditioned generations here, latents of real images help capture the background objects such as horse and surfboard (row 1 and 3 of Figure~\ref{fig:qual_pose}). However, when the background object is occluded or small (row 2 in Figure~\ref{fig:qual_pose} and row 1 in Figure~\ref{fig:failure_pose}), the latents are not sufficient in preserving the object in the final generations. Future work could consider extending \Ours{} by explicitly modelling the human-object/scene interaction. 

\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{iccv2023AuthorKit/supp_figures/supplemental_3.pdf}
\caption{Additional qualitative comparisons to \citet{brooks2022hallucinating} on the MPII dataset. Input 2D keypoints to \citet{brooks2022hallucinating} are overlayed on the bottom left in column 2. Top 3 rows are from MPII, and bottom 3 rows are from SMART. Our generations conditioned on text (T), real images (R). ``(D)" means the diffusion model is finetuned on the target dataset (MPII and SMART respectively).}
\label{fig:qual_pose}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{iccv2023AuthorKit/supp_figures/supplemental_4.pdf}
\caption{Failure cases on pose-conditioned generations.}
\label{fig:failure_pose}
\end{figure*}

