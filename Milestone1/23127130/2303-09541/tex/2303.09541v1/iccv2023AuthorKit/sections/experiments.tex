\section{Experiments}
\label{sec:experiments}
We first present comparisons on the synthetic data generation quality of \Ours{} in Sec. \ref{subsec:gen_quality}. In Sec. \ref{subsec:hmr} we show the effectiveness of \Ours{} for improving the downstream HMR performance in challenging domains.

\subsection{Image generation quality}
\label{subsec:gen_quality}

\textbf{Data generation details.} We use a text-to-image Stable Diffusion \cite{Rombach_2022_CVPR} model pre-trained on LAION-5B \cite{schuhmann2022laion} and a CLIP ViT-L/14 \cite{radford2021learning} as text encoder. To condition the generation on depth maps we employ the depth-to-image Stable Diffusion model that was resumed from the text-to-image model, and finetuned for 200k steps. The denoising model has an extra input channel to process the (relative) depth prediction produced by MiDaS \cite{Ranftl2022} which is used as added conditioning. As our segmentation model we use Mask R-CNN \cite{he2017mask,wu2019detectron2} pre-trained on MS-COCO \citep{lin2014microsoft}. For the qualitative examples in Figure \ref{fig:generation} and experiments in Section \ref{subsec:gen_quality}, we use BEV \cite{sun2022putting} as the HMR model, due to its capacity of recovering people of all age groups including small children and better empirical performance at localizing implausible synthetic humans, whereas two-stage HMR models that rely on a human detector often treat these erroneous generations as false negatives. With 50 inference steps, it takes about 6 seconds to create an image starting from text, and in the setting when a real image is used as guidance, the time is halved. 

\subsubsection{Comparison on text-conditioned generation}
% \SY{organize all tables and figures in this section better, in terms of placement on pages}
\textbf{Baselines and metrics.}
We assess the quality of the text-only conditioned images generated by \Ours{} by comparing them to off-the-shelf Stable Diffusion. 
In order to span a wide taxonomy of human activities we compose text prompts from the category labels available in the MPII \cite{andriluka20142d} dataset.
In addition, to assess the generation quality regarding extremely challenging human poses, we use the publicly released  sports categories from SMART \cite{chen2021sportscap} (further introduced in Sec. \ref {subsec:hmr}) as text prompts. 
% \SY{are these cherry picked? wording sounds a bit weak like you cherry picked}
For both datasets, we report the standard evaluation metric Fr√©chet Inception Distance (FID) and Kernel Inception Distance (KID) \cite{heusel2017gans}. Since the focus of our method is on human generation, we report H-FID / H-KID, which is FID / KID computed with only foreground humans (segmented by Mask R-CNN). Note that FID/KID are computed using image-level features, and therefore do not focus on human generation quality in particular. Therefore, we deem H-FID/H-KID more suitable metrics for our work.
% \SY{List most impressive metric first use your preference. Then also motivate better the fact that FID is not the most appropriate for this task, and why H-FID makes more sense}
Furthermore, we perform a user study where 6 independent blinded users were shown a randomly sampled set of 100 side-by-side images each generated by Stable Diffusion and \Ours{}. The users were given the task of selecting the image with the most plausible human pose and anatomy. If the images were comparable, the user could select a ``no preference" option.
% following the two-alternative forced choice (2AFC) behavioral study paradigm,
\input{iccv2023AuthorKit/tables/table_1.tex}
\input{iccv2023AuthorKit/tables/table_2.tex}

\textbf{Results.} Table \ref{table:text_generation} presents comparisons on text-conditioned generations. While FID/KID values are roughly the same, we highlight that humans generated by \Ours{} have lower H-FID/H-KID to humans from real images. User study suggests that users prefer our generations most of the time. Qualitative results in Figure \ref{table:text_generation} suggest that our generations, while preserving the textures of the original images (hence similar FID/KID), effectively corrects the human anatomy (hence lower H-FID/H-KID). 
% \Jen{I tried to calculate the mean human detection confidence score (from MaskRCNN) for SD generations and our genertaions. Our generated humans have higher detection score (0.94 vs. 0.90 in SD), which means our humans are more like humans? Can we mention that here?} \SY{sure, can be a side comment}

% \paragraph{Evaluation metrics.} We also report precision (i.e. the probability that a random generation falls within the support of real distribution) as well as recall (i.e. the probability that a random image from real data falls within the support of generated distribution) following \citet{kynkaanniemi2019improved}. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{iccv2023AuthorKit/figures/qualitative_text_conditioned.pdf}
    \caption{Comparison with Stable Diffusion \cite{Rombach_2022_CVPR} on text-conditioned generations. 
    % Rows 1-2 and 3-4 are generated with SMART and MPII prompts respectively. %Each column from left to right: Stable Diffusion generation, rendered depth map, our final generation.
    Red arrows point out implausible body parts in Stable Diffusion generations. To show a spectrum of varying pose difficulty levels, we present generations from the 5\%, 50\%, 95\% quantiles (i.e. from easy to hard) in terms of VPoser score. Rendered depths are included to show correct pose guidance.
    % \SY{not exactly clear why you're showing the rendered depth map, explain this to readers.}
    }
    \label{fig:generation}
\end{figure*}

\subsubsection{Comparison on pose-conditioned generation}
\paragraph{Baselines and metrics.} Most previous pose-conditioned generative models focus on the task of ``reposing" \citep{albahar2021pose,knoche2020reposing,men2020controllable}, where the goal is to repose the reference person using the target pose. These models are trained on fashion catalog images with clean background, therefore they are too simplistic to be effective baselines for our purpose. The only fair baseline, to our knowledge, is \citet{brooks2022hallucinating}
% /SY{a weird comment, usually you want to compare with all previous approaches for the task. Is this the only one?}
, a recent StyleGAN2 \cite{karras2020analyzing}-based generative model that takes 2D keypoints of a posed person and generates an RGB image with a background compatible to the pose. We take their pre-trained model, which is trained on 18 million images sourced from 10 existing human pose estimation (e.g. MPII \citep{andriluka20142d}) and action recognition datasets, and report performance on MPII for in-domain assessment, as well as SMART for out-of-distribution assessment.

% Since \Ours{} takes text and SMPL fittings as guidance, we choose to use MPII \citep{andriluka20142d}, a pose estimation dataset with taxonomy of every day human activities, as the evaluation set. We use action labels as text prompts, and SMPL fittings from EFT \citep{joo2020exemplar} as pose guide. 



\textbf{Results.} Table \ref{table:pose_generation} shows quantitative comparisons of image quality. 
% As our method is capable of generating an image with similar background to the real image (through the guidance of text and/or real image) while \citet{brooks2022hallucinating} generates arbitrary compatible backgrounds, we additionally report Human-FID (H-FID) for fair comparison of the foreground human generation quality. That is, FID computed using only the human region which we obtain by Mask R-CNN \cite{he2017mask}.  
% In addition, we report PCKh between the input and detected 2D keypoints as in previous work \cite{brooks2022hallucinating} to measure how well the image generation preserves the structure of the input skeleton. 
% Hallucinate Scenes has better PCKh, because it trains an explicit mapping from input pose to images, whereas \Ours{} infers the pose implicitly from text and rendered depths. 
Notably, even though \citet{brooks2022hallucinating} was trained with paired data (keypoint-image pairs) and therefore has an advantage over \Ours{} where the underlying models are trained/finetuned only with images, \Ours{} consistently achieves better performance.
% As expected \citet{brooks2022hallucinating} has higher precision on in-domain data since it is trained with extra information (i.e. images paired with 2D keypoints), but our method has much higher recall for generating more diverse textures. In addition, 
In addition, despite being trained on a large amount of keypoint-image pairs, \citet{brooks2022hallucinating} has poor generalization capability to novel pose distributions as in SMART, whereas our method powered by Stable Diffusion has a better zero-shot capability. 
% \SY{much more strongly state that Brooks and Efros has an advantage, and why your work is much more impressive}

\textbf{Effect of text and real guidance.} Figure \ref{fig:hallucinate_comparison} demonstrates qualitative comparisons between different versions of our model and \citet{brooks2022hallucinating} As shown, text guidance (T) is essential in capturing the context of the human action. Guidance from real images (R) provides overall texture information such as background colors. While guidance from real images alone is not sufficient in preserving the action of the human ($3_{rd}$ row in Ours R), it adds to text guidance, and further improves the realism of the image generations (Ours T+R). 
% \SY{Worth more detailed explanation, a lot is crammed into one sentence}

\textbf{Effect of finetuning.} To see whether a finetuned diffusion model could further help improve generation quality, we finetune Stable Diffusion on the target dataset (MPII and SMART respectively) for 10 epochs. Generations with finetuned diffusion models is noted with ``D". As shown in Figure \ref{fig:hallucinate_comparison}, although finetuned diffusion model generates images with better background when there is no real guidance (Ours T vs. T+D), the foreground often loses the texture of humans, which is likely due to the ``catastrophic forgetting" as sometimes observed in finetuning large pretrained models. Qualitatively, with both text and real guidance, the effect of finetuning is barely noticeable (Ours T+R vs. T+R+D). Quantitatively, when using real guidance (with or without text guidance), finetuning slightly improves FID, and significantly improves H-FID and H-KID. Further, consistent to what is observed in qualitative results, 41.3 H-FID (with T) vs. 136.2 H-FID (with T+D) suggests that finetuning worsens performance without real guidance. This suggests that for text-conditioned generations, it is optimal to utilize an off-the-shelf diffusion model without finetuning.
% \SY{summarize takeaway better. Is this something concerning? Or something expected and just fine?}

% \subsubsection{Diversity of the poses}
% Here we show through a scatter plot that our method generates a diverse set of images.
% \Jen{Will add a scatter plot of pose TSNE. It can also go into the Supplementary.}

\subsection{Downstream performance on HMR finetuning}
\label{subsec:hmr}
We demonstrate the downstream potential of \Ours{} through the task of few-shot adaptation of human mesh recovery models. We consider the setting where a small set of real images with 2D keypoints are available. This represents a typical scenario where we want to deploy a pre-trained HMR model on a new domain but there is limited ground truth annotations on the target domain. Through our experiments, we show that training with synthetic data from \Ours{} improves HMR on challenging target domains as compared to previous adaptation methods.

We use the following sports datasets as they contain much more challenging poses than common HMR benchmarks. As a result, there is a large domain gap when applying pre-trained HMR models on those datasets, and finetuning is necessarily to close the domain gap.
% \SY{motivate choice of data sets} 
Pre-processing details are in the \textit{Supplementary Material}.
\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Ski-Pose \citep{rhodin2018learning,sporri2016reasearch}} includes 3D and 2D keypoints labels from 5 professional ski athletes in a ski resort. There is a significant domain gap between ski poses and poses from other human pose estimation datasets, therefore Ski-Pose has been used as a benchmark in evaluating pose domain adaptation \cite{gholami2022adaptpose}.
    \item \textbf{Sports Motion and Recognition Tasks (SMART) \citep{chen2021sportscap}} contains video clips with per-frame ground truth 2D keypoints for various competitive sports. We consider all publicly released categories except for ``badminton", which only contains one clip.
    % namely ``diving" (81,121 images), ``pole vault" (1,208 images), ``high jump" (787 images), ``vault" (242 images), ``balance beam" (1,001 images), ``uneven bars" (1,398 images). (We took out badminton because it only has one clip.) We also downsampled diving.
    We sample enough clips so that the training set contains roughly $100$ images per category, and evaluate our finetuned models on the remaining images.
\end{itemize}
% \vspace{-\baselineskip}
\textbf{Evaluation metrics.} For Ski-Pose, we use Mean Per Joint Position Error (MPJPE) and Procrustes-Aligned MPJPE (PA-MPJPE) as our evaluation metrics. PA-MPJPE measures MPJPE after performing Procrustes alignment of the predicted and ground truth keypoints. SMART does not have ground truth 3D keypoints, so we report Percentage of Correct Keypoint (PCK) determined by distance between predicted and ground truth keypoints in pixels.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{iccv2023AuthorKit/figures/hallucinate_comparison.pdf}
    \caption{Qualitative comparisons to \citet{brooks2022hallucinating} (input 2D keypoints are overlayed on the bottom left). Our generations conditioned on text (T), real images (R), and in-domain (D).
    %.and both as guidance. In columns 2 to 4, we use off-the-shelf diffusion models, and in columns 5 to 8 we use finetuned diffusion models under the hood.
    % Best viewed by zooming in.
    % \SY{explain the different columns and summarize the takeaways more}
    % \vspace{-0.7cm}
    }
    \label{fig:hallucinate_comparison}
\end{figure*}

% \begin{figure}
% \centering
% \includegraphics[width=\columnwidth]{iccv2023AuthorKit/figures/rg_examples.pdf}
% \caption{Example synthetic images generated using real images as guidance during HMR finetuning. Each real image (left) is accompanied by 3 synthetic generations (right), each with a slightly augmented pose. \Jen{Can perhaps move to supplementary if there is not enough space.}}
% \label{fig:rg_examples}
% \end{figure}

\input{iccv2023AuthorKit/tables/sportscap.tex}
\input{iccv2023AuthorKit/tables/ski_pose.tex}

\begin{figure*}
\centering
\includegraphics[width=0.9\textwidth]{iccv2023AuthorKit/figures/hmr_results.pdf}
\caption{Qualitative results on downstream HMR tasks. Finetuning with data from \Ours{} (rightmost) helps HMR models learn novel poses from challenging domains. \vspace{-0.3cm}}
\label{fig:qualitative}
\end{figure*}

\textbf{Implementation details.}
We use the backbone of SPIN \cite{kolotouros2019learning} to estimate the human mesh, since the backbone is shared by both SPIN and DAPA \cite{weng2022domain}, which enables fair comparison to these two. For each real image in the few-shot training set, we create $3$ synthetic images, where each one has a slightly different pose due to pose augmentation. We finetune and update the entire HMR model with batch size of 64, learning rate of 1e-4. All hyperparameters are the same as in SPIN. The models are trained until the loss curves plateau and on average each finetuning experiment takes about 6 hours on a single NVIDIA TITAN V GPU.

\textbf{Results.}
We compare to state-of-the-art HMR models BEV \cite{sun2022putting} and PARE \cite{kocabas2021pare} that are pre-trained on MoCap datasets \cite{ionescu2013human3} as well as in-the-wild pose estimation datasets \cite{lin2014microsoft,andriluka20142d,johnson2010clustered}. In addition, we compare to finetuning methods SPIN-ft \cite{kolotouros2019learning} and DAPA \cite{weng2022domain}. The finetuning of these methods and ours minimizes 2D keypoint reprojection error by using 2D keypoints from the target training set. In addition, SPIN-ft uses in-the-loop model fitting to provide additional model-based supervision. DAPA generates synthetic data with paired 3D ground truths on the fly as additional supervision, while our method uses synthetic data from Diffusion-HPC.

In Table \ref{table:downstream_ski} we report PCK on sports categories from SMART. Although the off-the-shelf models (SPIN-pt, BEV, PARE) were pre-trained on 2D datasets that include sports poses \cite{johnson2010clustered}, there is still a significant domain gap between the training sets and SMART. Finetuning on a small set of target images is helpful in closing the domain gap. Among the finetuning methods, our method achieves better performance in general as compared to the baselines.

In Table \ref{table:downstream_sportscap}, we report MPJPE/PA-MPJPE on Ski-Pose testset. We vary the size of the real training set during adaptation, and observe that with the same mount of real data, models trained with our synthetic data attain best performance. Further, with the help of synthetic data generated by \Ours{}, we attain better performance than SPIN-ft and DAPA using much smaller amount of real data. We achieve best performance when using the entire training set. Notably, our best performance (111.3 MPJPE, 81.5 PA-MPJPE) is better than ProHMR \cite{kolotouros2021probabilistic} (122.7 MPJPE, 82.6 PA-MPJPE), which uses ground truth 2D keypoints from the testset as additional information. Finally, as qualitatively demonstrated in Figure \ref{fig:qualitative}, our method produces more accurate human mesh estimations on challenging poses, and in general have better alignment with 2D images.
