% This is used in the ARXIV version
\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\renewcommand{\thefigure}{S\arabic{figure}} % LBS: added for clarity
% \section*{Supplementary Material}

\subsection*{A. Code}
% An anonymous version of the GitHub repository can be found at:
% \url{https://anonymous.4open.science/r/Diffusion_HPC-7887}
Code and trained models can be found at \url{https://github.com/ZZWENG/Diffusion_HPC}.

\subsection*{B. Additional Implementation Details}
\textbf{Quantitative evaluation in Table 1.} We compute quantitative metrics using roughly 10,000 images generated from MPII \cite{andriluka20142d} and SMART \cite{chen2021sportscap} prompts, respectively. 

For MPII, we use ``\{image description\} of \{person\} doing \{action\}" as the text prompts, where ``\{person\}" can be single- or multi-person descriptions of person(s) of interest, and ``\{action\}" are the activity categories from MPII. (We exclude categories ``inactivity, quite/light" and miscellaneous" because they do not describe a specific activity.) For example, resulting prompts could be ``a nice photo of a man doing water activities." or ``a high-resolution photo of a group of people doing conditioning exercises".

Text prompts for SMART are constructed using the template ``a photo of an athlete doing \{action\}" where action is one of ``high jump", ``vault", ``pole vault", ``diving", ``gymnastics on uneven bars", and ``gymnastics on a balance beam".

% \textbf{Use VPoser \cite{SMPL-X:2019} to determine pose difficulty.}
% VPoser is a Variational Auto-Encoder (VAE) that is trained on a massive database of realistic human poses \cite{mahmood2019amass}. By design, poses that are farther away from the canonical pose (i.e. challenging poses) have larger variance in the embedding space. Therefore, we identify a difficult pose $\theta$ if its embedding $e_{\theta}$ have larger norm, i.e. $||e_{\theta}||_2 > \tau$. 
% \begin{align}
%     \{\mu, \sigma\} &= \mathcal{E}_{v}(\theta) \\
%     e_{\theta} &\sim \mathcal{N} (\mu, \sigma)
% \end{align}
% $\mathcal{E}_{v}$ is the encoder of VPoser. $\tau$ is determined empirically and set to 30.

\textbf{Data processing for downstream experiments.}
Following previous works \cite{kolotouros2019learning,weng2022domain}, we crop the images such that the persons (localized by ground truth 2D keypoints) are centered in the crop. In addition, the persons are scaled such that the torso (i.e. mean distance between left/right shoulder and hip) are roughly one third of the crop size ($224 \times 224$).

\subsection*{C. Additional Comparisons on Pose-Conditioned Generation}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{author-kit-3DV2024-v1.2/figures/hallucinate_comparison.pdf}
    \caption{Qualitative comparisons to \citet{brooks2022hallucinating} (input 2D keypoints are overlaid on the bottom left). Our generations conditioned on text (T), real images (R), and in-domain (D).
    %.and both as guidance. In columns 2 to 4, we use off-the-shelf diffusion models, and in columns 5 to 8 we use finetuned diffusion models under the hood.
    % Best viewed by zooming in.
    % \SY{explain the different columns and summarize the takeaways more}
    % \vspace{-0.7cm}
    }
    \label{fig:hallucinate_comparison}
\end{figure*}

\textbf{Effect of text and real guidance.} Figure \ref{fig:hallucinate_comparison} demonstrates qualitative comparisons between different versions of our model and \citet{brooks2022hallucinating} As shown, text guidance (T) is essential in capturing the context of the human action. Guidance from real images (R) provides overall texture information such as background colors. While guidance from real images alone is not sufficient in preserving the action of the human ($3_{rd}$ row in Ours R), it adds to text guidance, and further improves the realism of the image generations (Ours T+R). 

\textbf{Effect of finetuning.} To see whether a finetuned diffusion model could further help improve generation quality, we finetune Stable Diffusion on the target dataset (MPII and SMART respectively) for 10 epochs. Generations with finetuned diffusion models is noted with ``D". As shown in Figure \ref{fig:hallucinate_comparison}, although finetuned diffusion model generates images with better background when there is no real guidance (Ours T vs. T+D), the foreground often loses the texture of humans, which is likely due to the ``catastrophic forgetting" as sometimes observed in finetuning large pretrained models. Qualitatively, with both text and real guidance, the effect of finetuning is barely noticeable (Ours T+R vs. T+R+D). Quantitatively, when using real guidance (with or without text guidance), finetuning slightly improves FID, and significantly improves H-FID and H-KID. Further, consistent to what is observed in qualitative results, 41.3 H-FID (with T) vs. 136.2 H-FID (with T+D) suggests that finetuning worsens performance without real guidance. This suggests that for text-conditioned generations, it is optimal to utilize an off-the-shelf diffusion model without finetuning.



\subsection*{D. Additional Qualitative Results}
Here we include additional qualitative results as well as failure cases for the text-conditioned and pose-conditioned generations (Section 4.2).
% \ref{subsec:gen_quality}).

\subsubsection*{Text-Conditioned Generation}
Figure~\ref{fig:qual_text} shows qualitative comparisons of Stable Diffusion \citep{Rombach_2022_CVPR} and \Ours{} on text-conditioned generations. The images were selected from those sampled for the user study.

In Figure \ref{fig:failure_text} we include typical failure cases of text-conditioned generations. In left and middle columns, the body structures are not sufficiently rectified. This is likely because that resolution of depth maps (used for conditioning) is limited ($64\times 64$), so consequently small humans with out-of-distribution poses are challenging to rectify. In the right column, we show a failure scenario when the HMR model (i.e. BEV \cite{sun2022putting}) fails to reconstruct the humans in close-up shots. We could consider filtering out close-up shots, as they are not the primary intended use cases for \Ours{}.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{author-kit-3DV2024-v1.2/supp_figures/supplemental_1.pdf}
    \caption{Comparison with Stable Diffusion \cite{Rombach_2022_CVPR} on text-conditioned generations. Row 1 and rows 2-3 are generated with MPII \cite{andriluka20142d} and SMART \cite{chen2021sportscap} prompts, respectively. Red arrows point out implausible body parts in Stable Diffusion generations.}
    \label{fig:qual_text}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{author-kit-3DV2024-v1.2/supp_figures/supplemental_2.pdf}
    \caption{Failure cases on text-conditioned generations. }
    \label{fig:failure_text}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.95\textwidth]{author-kit-3DV2024-v1.2/figures/Picture1.pdf}
\caption{Comparison to images generated by ControlNet. Top: Pose-to-image ControlNet often results in misaligned limbs (e.g. child legs in top left; front person's legs in top right), and does not generalize to challenging domains such as sports poses in general. Bottom: Example synthetic images generated by pose-conditioned ControlNet and Diffusion-HPC. We highlight Diffusion-HPC's superior generalization capability, even on extremely challenging domains such as sports. Such generalization capability is key for it to be effective for downstream tasks, such as HMR.}
\label{fig:controlnet}
\end{figure*}

\subsubsection*{Pose-Conditioned Generation}
Figure~\ref{fig:qual_pose} shows qualitative comparisons of \citet{brooks2022hallucinating} and \Ours{} on pose-conditioned generations, and
Figure~\ref{fig:failure_pose} shows failures cases of pose-conditione generations. As seen from ``Ours T+R" and ``Ours T+R+D", human-object interactions are sometimes not preserved. 

Note that in \Ours{}, human-object interactions are considered but not modelled in an explicit way. Specifically, when we construct the depth map, we use Mask R-CNN \cite{he2017mask} to segment out the occluded body part, which helps with scenarios when, for instance, the person is riding the horse (row 1 of Figure \ref{fig:qual_pose}). However, row 2 of figure~\ref{fig:failure_pose} shows a failure case where the boat is not detected by Mask R-CNN.

In addition, in \Ours{}, latents from the initial generations help preserve the objects and context in the final generated scenes. For the pose-conditioned generations here, latents of real images help capture the background objects such as horse and surfboard (row 1 and 3 of Figure~\ref{fig:qual_pose}). However, when the background object is occluded or small (row 2 in Figure~\ref{fig:qual_pose} and row 1 in Figure~\ref{fig:failure_pose}), the latents are not sufficient in preserving the object in the final generations. Future work could consider extending \Ours{} by explicitly modelling the human-object/scene interaction. 

\textbf{Comparison to ControlNet \cite{zhang2023adding}.}
Adding control to pre-trained generative models has received increasing attention. While it is possible to use works such as ControlNet \cite{zhang2023adding} to generate pose-conditioned images, we note that ControlNet needs additional finetuning, requiring greater computing and annotation resources. In particular, training pose-to-image ControlNet requires paired data (i.e. 2D keypoints and images) which also limits the training data distribution to easy poses. 2D keypoints inherently provide less information compared to 3D body pose, and solely relying on them for 3D pose understanding tasks is insufficient. As a comparison, we train HMR models on SMART and SkiPose using the synthetic data generated with ControlNet, and on both evaluation sets, PCK and PA-MPJPE are worse than training with Diffusion-HPC (Table 1 and 2). Notably, for SMART, training with ControlNet data is worse than SPIN-ft which does not use synthetic data at all. Regardless of the possibility of re-training ControlNet using 3D human representations as conditioning, it is impractical when considering the substantial number of images with paired 3D GTs demanded. For reference, ControlNet pose-to-image model was trained on 200K keypoint-image pairs. Note that the scarcity of such 3D data was the primary motivation behind our work. Thus, rather than perceiving ControlNet as a direct alternative to our approach, it is more fair to consider it as a promising avenue to enhance our work, as the two methods can be synergistically combined - we could use Diffusion-HPC to bootstrap large amounts of image data with paired 3D pseudo ground truths and then use ControlNet to finetune a diffusion model.


\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{author-kit-3DV2024-v1.2/supp_figures/supplemental_3.pdf}
\caption{Additional qualitative comparisons to \citet{brooks2022hallucinating} on the MPII dataset. Input 2D keypoints to \citet{brooks2022hallucinating} are overlayed on the bottom left in column 2. Top 3 rows are from MPII, and bottom 3 rows are from SMART. Our generations conditioned on text (T), real images (R). ``(D)" means the diffusion model is finetuned on the target dataset (MPII and SMART respectively).}
\label{fig:qual_pose}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{author-kit-3DV2024-v1.2/supp_figures/supplemental_4.pdf}
\caption{Failure cases on pose-conditioned generations.}
\label{fig:failure_pose}
\end{figure*}

\subsection*{E. Limitations}
As we rely on large pre-trained models \citep{Rombach_2022_CVPR,schuhmann2022laion}, any biases in these models or datasets that they were trained on will be replicated onto our generated images. Due to the resolution of depth maps ($64 \times 64$), fine details such as fingers and facial expressions are challenging to synthesize. Besides, since we only render person depth maps, human-object/human-scene interactions may not be well-preserved in the final generation (e.g. the person and yoga mat in column 3, row 2 of Figure 3).
%\ref{fig:generation}).
While these limitations do not affect downstream tasks where we only care about the body pose, there is large room to improve the photo-realism of human-centric image synthesis, and for the synthetic data to be useful for a wider variety of downstream tasks such as expressive HMR \cite{SMPL-X:2019} and recovering human-object/scene interaction \cite{bhatnagar2022behave,zhang2020perceiving,weng2021holistic}. Lastly, as we use SMPL body representation, our method does not consider people with limb losses, but it can be adapted to do so.
