\begin{abstract}
Recent text-to-image generative models have exhibited remarkable abilities in generating high-fidelity and photo-realistic images. However, despite the visually impressive results, these models often struggle to preserve plausible human structure in the generations. Due to this reason, while generative models have shown promising results in aiding downstream image recognition tasks by generating large volumes of synthetic data, 
they are not suitable for improving downstream human pose perception and understanding. In this work, we propose a Diffusion model with Human Pose Correction (\Ours{}), a text-conditioned method that generates photo-realistic images with plausible posed humans by injecting prior knowledge about human body structure. Our generated images are accompanied by 3D meshes that serve as ground truths for improving Human Mesh Recovery tasks, where a shortage of 3D training data has long been an issue. Furthermore, we show that \Ours{} effectively improves the realism of human generations under varying conditioning strategies.
% We show that \Ours{} effectively improves the realism of human generations. Furthermore, as the generations are accompanied by 3D meshes that serve as ground truths, \Ours{}'s generated image-mesh pairs are well-suited for downstream human mesh recovery task, where a shortage of 3D training data has long been an issue.
\footnote{Code: \url{https://github.com/ZZWENG/Diffusion_HPC}}
\end{abstract}