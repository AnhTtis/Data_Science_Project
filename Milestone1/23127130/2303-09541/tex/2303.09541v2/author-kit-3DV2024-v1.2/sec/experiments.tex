\section{Experiments}
\label{sec:experiments}
We first show the effectiveness of \Ours{} for improving HMR performance in challenging domains in Sec.~\ref{subsec:hmr}. Then, in Sec.~\ref{subsec:gen_quality} we present comparisons on the synthetic data generation quality of \Ours{}.

\subsection{Finetuning on challenging HMR settings}
\label{subsec:hmr}
We demonstrate the potential of \Ours{} through the task of few-shot adaptation of human mesh recovery models. We consider the setting where a small set of real images with 2D keypoints are available. This represents a typical scenario where we want to deploy a pre-trained HMR model on a new domain but there is limited ground truth annotations on the target domain. Through our experiments, we show that training with synthetic data from \Ours{} improves HMR on challenging target domains as compared to previous adaptation methods.

We use the following sports datasets as they contain much more challenging poses than common HMR benchmarks. As a result, there is a large domain gap when applying pre-trained HMR models on those datasets, and finetuning is necessarily to close the domain gap. Pre-processing details are in the \textit{Supplementary Material}.
% \SY{motivate choice of data sets} 

\begin{figure*}
\centering
\includegraphics[width=0.85\textwidth]{author-kit-3DV2024-v1.2/figures/hmr_results.pdf}
\caption{Qualitative HMR results on SMART and Ski-Pose datasets. Finetuning with data from \Ours{} (rightmost) helps HMR models learn novel poses from challenging domains.}
\label{fig:qualitative}
\end{figure*}
\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Ski-Pose \citep{rhodin2018learning,sporri2016reasearch}} includes 3D and 2D keypoints labels from 5 professional ski athletes in motion. There is a significant domain gap between ski poses and poses from other human pose estimation datasets, therefore Ski-Pose has been used as a benchmark in evaluating pose domain adaptation \cite{gholami2022adaptpose}.
    \item \textbf{Sports Motion and Recognition Tasks (SMART) \citep{chen2021sportscap}} contains videos with per-frame 2D keypoints for various competitive sports. We consider 6 publicly released categories except for ``badminton", which only contains one clip.
    % namely ``diving" (81,121 images), ``pole vault" (1,208 images), ``high jump" (787 images), ``vault" (242 images), ``balance beam" (1,001 images), ``uneven bars" (1,398 images). (We took out badminton because it only has one clip.) We also downsampled diving.
    We sample enough clips so that the training set contains roughly $100$ images per category, and evaluate our finetuned models on the remaining images.
\end{itemize}
\textbf{Evaluation metrics.} For Ski-Pose, we use Mean Per Joint Position Error (MPJPE) and Procrustes-Aligned MPJPE (PA-MPJPE) as our evaluation metrics. PA-MPJPE measures MPJPE after performing Procrustes alignment of the predicted and ground truth keypoints. SMART does not have ground truth 3D keypoints, so we report Percentage of Correct Keypoint (PCK) determined by distance between predicted and ground truth keypoints in pixels.

% \begin{figure}
% \centering
% \includegraphics[width=\columnwidth]{author-kit-3DV2024-v1.2/figures/rg_examples.pdf}
% \caption{Example synthetic images generated using real images as guidance during HMR finetuning. Each real image (left) is accompanied by 3 synthetic generations (right), each with a slightly augmented pose. \Jen{Can perhaps move to supplementary if there is not enough space.}}
% \label{fig:rg_examples}
% \end{figure}


\input{author-kit-3DV2024-v1.2/tables/sportscap.tex}
\input{author-kit-3DV2024-v1.2/tables/ski_pose.tex}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.76\textwidth]{author-kit-3DV2024-v1.2/figures/qualitative_text_conditioned.pdf}
    \caption{Comparison with Stable Diffusion \cite{Rombach_2022_CVPR} on text-conditioned generations. Red arrows point out implausible body parts in Stable Diffusion generations. To show a spectrum of varying pose difficulty levels, we present generations from the 5\%, 50\%, 95\% quantiles (i.e. from easy to hard) in terms of VPoser score. Rendered depths are included to show correct pose guidance.
    }
    \label{fig:generation}
\end{figure*}

\textbf{Implementation details.}
We use the backbone of SPIN \cite{kolotouros2019learning} to estimate the human mesh, since the backbone is shared by both SPIN and DAPA \cite{weng2022domain}, which enables fair comparison to these two. For each real image in the few-shot training set, we create $3$ synthetic images, where each one has a slightly different pose due to pose augmentation. We finetune and update the entire HMR model with batch size of 64, learning rate of 1e-4. All hyperparameters are the same as in SPIN. The models are trained until the loss curves plateau and on average each finetuning experiment takes about 6 hours on a single NVIDIA TITAN V GPU.

\textbf{Results.}
We compare to recent HMR models BEV \cite{sun2022putting} and PARE \cite{kocabas2021pare} that are pre-trained on MoCap datasets \cite{ionescu2013human3} as well as in-the-wild pose estimation datasets \cite{lin2014microsoft,andriluka20142d,johnson2010clustered}. We also compare to BEDLAM-CLIFF \cite{Black_CVPR_2023}, a state-of-the-art HMR model that is trained with a large synthetic dataset BEDLAM with realistic humans. In addition, we compare to finetuning methods SPIN-ft \cite{kolotouros2019learning}, DAPA \cite{weng2022domain}, as well as finetuning with synthetic data generated with ControlNet \cite{zhang2023adding} and \Ours{}. The finetuning of these methods and ours minimizes 2D keypoint reprojection error by using 2D keypoints from the target training set. In addition, SPIN-ft uses in-the-loop model fitting to provide additional model-based supervision. DAPA generates synthetic data with paired 3D ground truths on the fly as additional supervision, while Ours uses data from Diffusion-HPC.

In Table \ref{table:downstream_ski} we report PCK on sports categories from SMART. Although the off-the-shelf models (SPIN-pt, BEV, PARE) were pre-trained on 2D datasets that include sports poses \cite{johnson2010clustered}, there is still a significant domain gap between the training sets and SMART. BEDLAM-CLIFF was trained with a massive synthetic dataset BEDLAM, but the data generation was not tailored for the specific target domains, and therefore their training does not improve the model performance on the target dataset. As shown in the lower half of Table \ref{table:downstream_ski}, finetuning on a small set of target images is helpful in closing the domain gap. Among those methods, we achieve better performance in general.

In Table \ref{table:downstream_sportscap}, we report MPJPE/PA-MPJPE on Ski-Pose testset. We vary the size of the real training set during adaptation, and observe that with the same mount of real data, models trained with our synthetic data attain best performance. Further, with the help of synthetic data generated by \Ours{}, we attain better performance than SPIN-ft and DAPA using much smaller amount of real data. We achieve best performance when using the entire training set. Notably, our best performance (111.3 MPJPE, 81.5 PA-MPJPE) is better than ProHMR \cite{kolotouros2021probabilistic} (122.7 MPJPE, 82.6 PA-MPJPE), which uses ground truth 2D keypoints from the testset as additional information. Finally, as qualitatively demonstrated in Figure \ref{fig:qualitative}, our method produces more accurate human mesh estimations on challenging poses, and in general have better alignment with 2D images.
% \textbf{\textcolor{red}{Comparison to ControlNet \cite{zhang2023adding}.}}
% Adding control to pre-trained generative models has received increasing attention. While it is possible to use works such as ControlNet \cite{zhang2023adding} to generate pose-conditioned images, we note that ControlNet needs additional finetuning, requiring greater computing and annotation resources. In particular, training pose-to-image ControlNet requires paired data (i.e. 2D keypoints and images) which also limits the training data distribution to easy poses. 2D keypoints inherently provide less information compared to 3D body pose, and solely relying on them for 3D pose understanding tasks is insufficient. As a comparison, we train HMR models on SMART and SkiPose using the synthetic data generated with ControlNet, and on both evaluation sets, PCK and PA-MPJPE are worse than training with Diffusion-HPC. Notably, for SMART, training with ControlNet data is worse than SPIN-ft which does not use synthetic data at all. Regardless of the possibility of re-training ControlNet using 3D human representations as conditioning, it is impractical when considering the substantial number of images with paired 3D GTs demanded. For reference, ControlNet pose-to-image model was trained on 200K keypoint-image pairs. Note that the scarcity of such 3D data was the primary motivation behind our work. Thus, rather than perceiving ControlNet as a direct alternative to our approach, it is more fair to consider it as a promising avenue to enhance our work, as the two methods can be synergistically combined - we could use Diffusion-HPC to bootstrap large amounts of image data with paired 3D pseudo ground truths and then use ControlNet to finetune a diffusion model.

\subsection{Image generation quality}
\label{subsec:gen_quality}

\textbf{Data generation details.} We use a text-to-image Stable Diffusion \cite{Rombach_2022_CVPR} model pre-trained on LAION-5B \cite{schuhmann2022laion} and a CLIP ViT-L/14 \cite{radford2021learning} as text encoder. To condition the generation on depth maps we employ the depth-to-image Stable Diffusion model that was resumed from the text-to-image model, and finetuned for 200k steps. The denoising model has an extra input channel to process the (relative) depth prediction produced by MiDaS \cite{Ranftl2022} which is used as added conditioning. As our segmentation model we use Mask R-CNN \cite{he2017mask,wu2019detectron2} pre-trained on MS-COCO \citep{lin2014microsoft}. For the qualitative examples in Figure \ref{fig:generation} and experiments in Section \ref{subsec:gen_quality}, we use BEV \cite{sun2022putting} as the HMR model, due to its capacity of recovering people of all age groups and better empirical performance at localizing implausible synthetic humans, whereas two-stage HMR models that rely on a human detector often treat these erroneous generations as false negatives. With 50 inference steps, it takes about 6 seconds to create an image starting from text, and in the setting when a real image is used as guidance, the time is halved. 
% \vspace{-0.3cm}
\subsubsection{Comparison on text-conditioned generation}
We assess the quality of the text-only conditioned images generated by \Ours{} by comparing them to off-the-shelf Stable Diffusion. 
In order to span a wide taxonomy of human activities we compose text prompts from the category labels available in the MPII \cite{andriluka20142d} dataset.
In addition, to assess the generation quality regarding extremely challenging human poses, we use the publicly released  sports categories from SMART \cite{chen2021sportscap} (further introduced in Sec. \ref {subsec:hmr}) as text prompts. 
\input{author-kit-3DV2024-v1.2/tables/table_1.tex}
For both datasets, we report the standard evaluation metric Fr√©chet Inception Distance (FID) and Kernel Inception Distance (KID) \cite{heusel2017gans}. Since the focus of our method is on human generation, we report H-FID / H-KID, which is FID / KID computed with only foreground humans (segmented by Mask R-CNN). Note that FID/KID are computed using image-level features, and therefore do not focus on human generation quality in particular. Thus, we deem H-FID/H-KID more suitable metrics for our work.

Furthermore, we perform a user study where 6 independent blinded users were shown a randomly sampled set of 100 side-by-side images each generated by Stable Diffusion and \Ours{}. The users were given the task of selecting the image with the most plausible human pose and anatomy. If the images were comparable, the user could select a ``no preference" option.

\textbf{Results.} Table \ref{table:text_generation} presents comparisons on text-conditioned generations. While FID/KID values are roughly the same, we highlight that humans generated by \Ours{} have lower H-FID/H-KID to humans from real images. User study suggests that users prefer our generations most of the time. Qualitative results in Figure \ref{table:text_generation} suggest that our generations, while preserving the textures of the original images (hence similar FID/KID), effectively corrects the human anatomy (hence lower H-FID/H-KID). 
\vspace{-0.2cm}
\subsubsection{Comparison on pose-conditioned generation}
Most previous pose-conditioned generative models focus on the task of ``reposing" \citep{albahar2021pose,knoche2020reposing,men2020controllable}, where the goal is to repose the reference person using the target pose. These models are trained on fashion catalog images with clean background, therefore they are too simplistic to be effective baselines for our purpose. The only fair baseline, to our knowledge, is \citet{brooks2022hallucinating}, a recent StyleGAN2 \cite{karras2020analyzing}-based generative model that takes 2D keypoints of a posed person and generates images with compatible background. We benchmark their pre-trained model (trained on 18 million images sourced from 10 existing human pose estimation and action recognition datasets) on MPII for in-domain assessment.

% Since \Ours{} takes text and SMPL fittings as guidance, we choose to use MPII \citep{andriluka20142d}, a pose estimation dataset with taxonomy of every day human activities, as the evaluation set. We use action labels as text prompts, and SMPL fittings from EFT \citep{joo2020exemplar} as pose guide. 

\textbf{Results.} Table \ref{table:pose_generation} shows quantitative comparisons of image quality. 
% As our method is capable of generating an image with similar background to the real image (through the guidance of text and/or real image) while \citet{brooks2022hallucinating} generates arbitrary compatible backgrounds, we additionally report Human-FID (H-FID) for fair comparison of the foreground human generation quality. That is, FID computed using only the human region which we obtain by Mask R-CNN \cite{he2017mask}.  
% In addition, we report PCKh between the input and detected 2D keypoints as in previous work \cite{brooks2022hallucinating} to measure how well the image generation preserves the structure of the input skeleton. 
% Hallucinate Scenes has better PCKh, because it trains an explicit mapping from input pose to images, whereas \Ours{} infers the pose implicitly from text and rendered depths. 
Notably, even though \citet{brooks2022hallucinating} was trained with paired data (keypoint-image pairs) and therefore has an advantage over \Ours{} where the underlying models are trained/finetuned only with images, \Ours{} consistently achieves better performance. Moreover, \citet{brooks2022hallucinating} has poor generalization capability to novel pose distributions as in SMART, whereas our method powered by Stable Diffusion has a better zero-shot capability. Additional details, qualitative comparisons and limitations are in the \textit{Supplementary Material}.
\input{author-kit-3DV2024-v1.2/tables/table_2.tex}
\vspace{-0.2cm}