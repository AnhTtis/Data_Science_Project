\section{Related Work}
\subsection{Using synthetic data to improve HMR}
Previous works \cite{he2022synthetic} have recognized the capability of state-of-the-art text-conditioned generative models \cite{nichol2021glide} for generating training data for downstream image recognition tasks. However, the poor quality of the generated person images effectively precludes the extension of this capacity to tasks such as 3D human pose understanding (e.g. human mesh recovery).
% \SY{is this verified or citable?}
Due to the challenge in collecting 3D ground truths for end-to-end training of human mesh recovery models, many previous works have considered leveraging synthetic data. Typically, these works create 2D renderings of 3D posed human models from graphics engines \cite{varol2017learning,doersch2019sim2real,patel2021agora}, with \citet{Black_CVPR_2023} being the most comprehensive effort. However, this approach possesses multiple disadvantages. First, 
% synthetic data generated from 
% manually defined pipelines may not accurately reflect the broad range of poses that exist in real-world data 
% \SY{somewhat unclear... Are they all really manual? In what way does it not accurately reflect real world data? Are you trying to contrast with your work / does your method clearly reflect real world data?} 
the variety of the generated poses is limited by the pose data source. Second, a large and diverse training set is needed to cover all possible poses of interest, which makes storing and sharing such data costly and inefficient. To address these, recent work \citet{weng2022domain} proposes a data-efficient way by rendering SMPL bodies with poses sampled from the estimated pose distributions from real data, but since the body textures are predicted and warped from real images, the renderings are not photo-realistic. Analogously, \citet{STRAPS2020BMVC} generates synthetic data online to improve diverse body shape estimation.

In contrast, using conditional generative models \citep{ho2020denoising,Rombach_2022_CVPR} such as ours to synthesize data has a few advantages. First, large generative models can produce high-fidelity photo-realistic images closer to real data since they are trained on internet-scale real-world data (e.g. LAION-5B \cite{schuhmann2022laion}). Second, they allow easy control of the generation style via detailed prompting, and stochasticity in the generation process results in more diverse and potentially unlimited synthetic data. However, although there have been some attempts to explore the use of generative models for image classification and object detection \cite{zhang2021datasetgan,he2022synthetic}, their usage in human mesh recovery has not yet been investigated due to the poor quality of the generated person images. \Ours{} is the first approach that uses conditional generative models to produce synthetic data that are useful for human mesh recovery, broadening the range of downstream utilities of SoTA generative models.
% \SY{I don't think the point is you are the first to use synthetic images generated by Diffusion-HPC, since this is a new method, are you trying to say you are the first to use diffusion models at all or something along those lines} 

\subsection{Conditional generation of posed humans}
Recently, there has been a growing focus on conditional generation of posed humans in the form of images/videos \cite{zhang2023adding,hu2023animate,wang2023disco,xu2023magicanimate}, body models \cite{guo2022generating,delmas2022posescript} or NeRF \cite{dong2023ag3d,weng2023zeroavatar,cao2023dreamavatar}. For synthesizing posed humans, most works focus either on text-conditioned or pose-conditioned generation. 
In terms of text-conditioned approaches state-of-the-art general image generation models such as Stable Diffusion \cite{Rombach_2022_CVPR} have shown impressive capability in producing high-resolution and realistic images. But as a known limitation \cite{hugging_face_2022}, they frequently struggle to preserve the correct anatomy of human bodies. An alternate line of research focuses directly on text-conditioned human pose or motion generation \cite{guo2022generating,delmas2022posescript}. These generative models are trained on large 3D human motion database \cite{mahmood2019amass} with paired textual descriptions. But since they output parameters of human body models \cite{loper2015smpl}, they bypass the issue of preserving anatomical structure. However, since human motion databases do not come with paired RGB data, these works are unable to produce human textures and background.

On the other hand, previous works have explored generating images of full-body humans conditioning on body pose \cite{albahar2021pose,knoche2020reposing,men2020controllable,brooks2022hallucinating}. \citet{albahar2021pose,knoche2020reposing,men2020controllable} consider the task of ``reposing", where the goal is to synthesize images of people in a novel pose, based on a reference image of that person and the new pose. More recently and relevant to our work, Brooks \etal \citep{brooks2022hallucinating} proposed a pose-conditioned image synthesis model that dispenses with the reference images by generating reasonable backgrounds. In contrast to the above works, our proposed \Ours{} is a person image synthesis method flexible enough to allow for both text and pose conditioned generation and does not require additional training or explicit pose annotations from a target domain to produce diverse humans and scenes. Another closely related work is ControlNet \cite{zhang2023adding}, where posed-conditioned images are obtained from generative models, yet unlike our method ControlNet requires finetuning on large amounts of real paired data (i.e. 2D keypoints, images and captions).

% \SY{emphasize more the big picture of your contribution, and why you have advantages / how exciting your work is}
% Explain how the hallucinating paper is the closest work we can compare to and how its approach is actually very impractical/different for our HMR purposes.
%
%rather than reposing we have pose-guided generation where we don't just repose and thus more diverse humans and scenes\

\subsection{Editing \& composing large pre-trained models}
% \vspace{-0.1cm}
% Look at the "Composing pre-trained models. " section under their related work. https://arxiv.org/pdf/2210.11522.pdf
Foundation models (e.g., Imagen \cite{saharia2022photorealistic}, Stable Diffusion \cite{Rombach_2022_CVPR}) that are trained on large amounts of broad data have demonstrated impressive generative and few-shot learning capabilities across a wide spectrum of tasks. Additionally, the scale of information they have seen and learned, allows these models to be adapted to further downstream tasks. For these reasons, editing or composing large pre-trained models has been widely studied recently. Among these works the most closely related to our approach are the training-free methods that utilize pre-trained diffusion models to perform global or local image editing \cite{hertz2022prompt,parmar2023zero,avrahami2022blended,avrahami2022latent} (e.g. inpainting, style transfer, etc.). They are ``training-free" in the sense that editing is done by injecting knowledge into the denoising process during inference and therefore no additional model finetuning is needed. Analogously, our method \Ours{} improves plausibility of human generations by injecting human body priors in the form of posed SMPL \cite{loper2015smpl} body models. 
% To the best of our knowledge, ours is the first method that aims to improve the human pose realism of pre-trained diffusion models.

% \citet{avrahami2022blended, avrahami2022blended} perform text-guided local editing in the pixel and latent space of pre-trained diffusion models to blend a novel object into a specific region of an image.
