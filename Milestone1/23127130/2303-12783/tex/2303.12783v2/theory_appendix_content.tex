\newcommand{\refE}[1]{(\ref{#1})}

We denote the full dataset as $D=(D_1, \dots, D_{t+1})$, where $D_i=(X_i, Y_i)$ is the $i$-th datapoint.
Since we are in the split conformal setting \citep{vovk2005algorithmic}, the full dataset is subdivided into two parts: the calibration and the test data.
The prediction model $\muh$ is already given in this setting and not selected based on $D$.
The prediction interval $\Ch_t^{\alpha}$ for split conformal prediction is given by

% - Think about using an earlier

\begin{equation}\label{eq:def_splitCP}
\Ch_t^{\alpha}(X_{t+1}) = \muh(X_{t+1})\pm \quant_{1-\alpha}\left(\sum_{i=1}^t
  \tfrac{1}{t+1} \cdot \delta_{R_i} +
  \tfrac{1}{t+1}\cdot\delta_{+\infty}\right), 
\end{equation}
where $R_i =|Y_i - \muh(X_i)|$, $\delta_c$ is a point mass at $c$, and $\quant_{\tau}$ is the $\tau$-quantile of a distribution.
\citet{vovk2005algorithmic} show that this interval provides a \textit{marginal} coverage guarantee in case of \textit{exchangeable} data:

\begin{theorem}[Split Conformal Prediction \citep{vovk2005algorithmic}] \
\\ If the data points $(X_1,Y_1),\dots,(X_t,Y_t),(X_{t+1},Y_{t+1})$ are exchangeable then the split conformal prediction set defined in~\refE{eq:def_splitCP} satisfies 
\[
\PR{Y_{t+1}\in\Ch_t^{\alpha}(X_{t+1})} \geq 1-\alpha.
\]
\end{theorem}

This seminal result cannot be directly applied to our setting, because time series are often non-exchangeable (as we discuss in the introduction) and models often exhibit locally specific errors.

The remainder of this section is split into two parts.
First, we discuss the theoretical connections between the MHN retrieval and CP through the lens of \citet{barber2022conformal}.
Second, we adapt the theory of \citet{xu2021conformal} to be applicable to our regime-based setting.

For the MHN retrieval, we evoke the results of \citet{barber2022conformal}, who introduce the concept of \textit{non-exchangeable split conformal} prediction.
Instead of treating each calibration point equally, they assign an individual weight for each point, to account for non-exchangeability.
Formally, we denote the weight for the $i$-th point by $w_i \in [0,1]$, $i=1,\dots,t$; 
and the respective normalized weights are defined as
\begin{equation}\label{eq:normalized_weights}
\tilde{w}_i = \frac{w_i}{w_1+\dots+w_t+1}, \ i=1,\dots,t, 
\textnormal{ and } \tilde{w}_{t+1}= \frac{1}{w_1+\dots+w_t+1}.
\end{equation}

The prediction interval for non-exchangeable split conformal setting $\Ch_t^{\alpha}$ is given by
\begin{equation}\label{eq:def_splitCP_nonExchange}
\Ch_t^{\alpha}(X_{t+1}) = \muh(X_{t+1})\pm \quant_{1-\alpha} \left(\sum_{i=1}^t \tilde{w}_i \cdot \delta_{R_i} + \tilde{w}_{t+1}\cdot\delta_{+\infty}\right). 
\end{equation} 


% State why this is an attrative option (non exchangeable, and possitibilty to varing pi with)

\citet{barber2022conformal} show that non-exchangeable split conformal prediction satisfies a similar bound as exchangeable split conformal prediction, which is extended by a term that accounts for a potential \textit{coverage gap} $\Delta \ \text{Cov}$.
Formally we define $ \Delta \ \text{Cov} = \alpha - \alpha^\star$, where $\alpha$ is the specified miscoverage and $\alpha^\star$ is the observed miscoverage.

To specify the coverage gap they use the \textit{total variation distance} $\dtv$, which is a distance metric between two probability distributions. 
It is defined as
\begin{equation}
    \dtv(P_1,P_2)=\sup_{A\in\mathcal{F}}|P_1(A)-P_2(A)|,
\end{equation}
where $P_1$ and $P_2$ are arbitrary distributions defined on the space $(\Omega, \mathcal{F})$.

Intuitively, $\dtv$ measures the largest distance between two distributions.
It is connected to the widely used Kullback--Leibler divergence $D_{\mathsf{KL}}$ through Pinsker's inequality \citep{pinsker1964information}:
\begin{equation}
    \dtv(P_1,P_2) \leq \sqrt{\frac{1}{2} D_{\mathsf{KL}}(P_1||P_2)}.
\end{equation}

Given this measure, \citet{barber2022conformal} bound the coverage of non-exchangeable split conformal prediction as follows:

\begin{theorem}[Non-exchangeable split conformal prediction \citep{barber2022conformal}]\label{thm:splitCP_nonExchage} \
\\ The non-exchangeable split conformal method defined in~\eqref{eq:def_splitCP_nonExchange} satisfies 
\[
\PR{Y_{t+1}\in\Ch^{\alpha}_t(X_{t+1})}\geq (1-\alpha) - \sum_{i=1}^t\tilde{w}_i \cdot
\dtv\big(R(D),R(D^i)\big),
\]
where $R(D) = (R_1, R_2, \dots, R_{t+1})$ denotes the sequence of absolute residuals on $D$ and $D^i$ denotes a sequence where the test point $D_{t+1}$ is swapped with the $i$-th calibration point.
\end{theorem}


The coverage gap is represented by the rightmost term.
Therefore,
\[
\Delta \ \text{Cov} \geq - \sum_{i=1}^t\tilde{w}_i \cdot \dtv(R(D),R(D^i)).
\]

\citet{barber2022conformal} prove this for the full-conformal setting, which also directly applies to the split conformal setting (as they note, too).
For the sake of completeness, we state the proof of Theorem~\ref{thm:splitCP_nonExchage} explicitly for split conformal prediction \citep[adapted from][]{barber2022conformal}:\\

By the definition of the non-exchangeable split conformal prediction interval \refE{eq:def_splitCP_nonExchange}, it follows that

\begin{equation}\label{eq:quantile_noncoverage_1}
    Y_{t+1}\not\in\Ch_t^{\alpha}(X_{t+1}) \ \iff \
    R_{t+1} > \quant_{1-\alpha}\left(\sum_{i=1}^t \tilde{w}_i \cdot
      \delta_{R_i} + \tilde{w}_{t+1}\cdot\delta_{+\infty}\right).
    \end{equation}

Next, we show that, for a given $K \in  \{1, \dots, t+1\}$, 
\begin{equation}\label{eq:quantile_noncoverage_2}
    \quant_{1-\alpha}\left(\sum_{i=1}^t \tilde{w}_i \cdot \delta_{R_i} +
      \tilde{w}_{t+1}\cdot\delta_{+\infty}\right) \geq
    \quant_{1-\alpha}\left(\sum_{i=1}^{t+1} \tilde{w}_i \cdot
      \delta_{(R(D^K))_i}\right),
\end{equation}

by considering that
\begin{equation}\label{eq:residual_vector}
    \big(R(D^K)\big)_i =
        \begin{cases}
        R_i, & \textnormal {if } i \not= K \textnormal{ and } i \not= t+1, \\ 
        R_{t+1}, & \textnormal {if } i = K, \\
        R_K, & \textnormal {if } i = t+1.
        \end{cases}
\end{equation}

In the case of $K = t + 1$, this holds trivially since both sides are equivalent.
In the case of $K \leq t$, we can rewrite the left-hand side of inequality~\refE{eq:quantile_noncoverage_2} as 

\begin{align*}
\sum_{i=1}^t \tilde{w}_i \cdot \delta_{R_i} +
\tilde{w}_{t+1}\cdot\delta_{+\infty}& \\
&= \sum_{i=1,\dots,t; i\neq K} \tilde{w}_i \cdot \delta_{R_i} +
\tilde{w}_K (\delta_{R_K} + \delta_{+\infty}) + (\tilde{w}_{t+1} -
\tilde{w}_K)\delta_{+\infty}.
\end{align*}

The right-hand side can be similarly represented as

\begin{align*}
\sum_{i=1}^{t+1} \tilde{w}_i \cdot \delta_{(R(D^K))_i} &=
\sum_{i=1,\dots,t; i\neq K} \tilde{w}_i \cdot \delta_{R_i} +
\tilde{w}_K  \delta_{R_{t+1}} + \tilde{w}_{t+1}\delta_{R_K} \\ 
&=\sum_{i=1,\dots,t; i\neq K} \tilde{w}_i \cdot \delta_{R_i} +
\tilde{w}_K (\delta_{R_K} + \delta_{R_{t+1}}) + (\tilde{w}_{t+1} - \tilde{w}_K)\delta_{R_K}.
\end{align*}


By the definition of the normalized weights \refE{eq:normalized_weights} and the assumption that $w_K \in [0,1]$, it holds that \smash{$\tilde{w}_{t+1}\geq \tilde{w}_K$}.
Applying this to the rewritten forms of the left- and right-hand sides, we can see that inequality~\refE{eq:quantile_noncoverage_2} holds.

Substituting the right hand side of \refE{eq:quantile_noncoverage_1} by using \refE{eq:quantile_noncoverage_2}, we see that

\[
    Y_{t+1}\not\in\Ch_t^{\alpha}(X_{t+1}) \ \implies \ 
    R_{t+1} > \quant_{1-\alpha}\left(\sum_{i=1}^{t+1} \tilde{w}_i \cdot
      \delta_{(R(D^K))_i}\right),
\] 

or equivalently by using \refE{eq:residual_vector}:

\begin{equation}\label{eq:quantile_noncoverage_3}
Y_{t+1}\not\in\Ch_t^{\alpha}(X_{t+1}) \ \implies \ 
(R(D^K))_K >  \quant_{1-\alpha}\left(\sum_{i=1}^{t+1}
  \tilde{w}_i \cdot \delta_{(R(D^K))_i}\right). 
\end{equation}

For the next step we use the definition of the set of so-called \textit{strange points} $\mathcal{S}$:
\begin{equation}\label{eq:strange} 
\mathcal{S}(\Bep) = \left\{i\in[t+1] \ : \ \epsilon_i > \quant_{1-\alpha}\left(\sum_{j=1}^{t+1}
    \tilde{w}_j \cdot\delta_{\epsilon_j}\right)\right\}. 
\end{equation}

A strange point $s \in \mathcal{S}$ therefore refers to a data point with residuals that are outside the prediction interval.
Given the definition of $\mathcal{S}$, we can directly see that

\begin{equation}\label{eq:S_r_alpha}
\sum_{i\in\mathcal{S}(\Bep)} \tilde{w}_i \leq \alpha \textnormal{ for all }
\Bep\in\dR^{t+1}, 
\end{equation}

Combining \refE{eq:quantile_noncoverage_3} and \refE{eq:strange} we see that non-coverage of $Y_{t+1}$ implies strangeness of point $K$:

\begin{equation}\label{eq:noncoverage-implies-strangeness}
Y_{t+1}\not\in\Ch_t^{\alpha}(X_{t+1}) \ \implies \ K
\in\mathcal{S}\big(R(D^K)\big). 
\end{equation}

Given this, we arrive at the final derivation of the coverage bound:

\begin{align}
\PR{K\in\mathcal{S}\big(R(D^K)\big)} 
\notag
&=\sum_{i=1}^{t+1}\PR{K=i\textnormal{ and }
i\in\mathcal{S}\big(R(D^i)\big)} \\
\notag
&=\sum_{i=1}^{t+1}\tilde{w}_i\cdot
\PR{i\in\mathcal{S}\big(R(D^i)\big)}\\ 
\notag
&\leq \sum_{i=1}^{t+1}\tilde{w}_i\cdot
\left(\PR{i\in\mathcal{S}\big(R(D)\big)} +
\dtv\big(R(D),R(D^i)\big)\right)\\ 
\notag
&=\EXP{\sum_{i\in\mathcal{S}(R(D))} \tilde{w}_i} + \sum_{i=1}^t\tilde{w}_i \cdot
\dtv\big(R(D),R(D^i)\big)\\
\notag
&\leq\alpha + \sum_{i=1}^t\tilde{w}_i\cdot\dtv\big(R(D),R(D^i)\big),
\notag
\end{align}

where we use \refE{eq:S_r_alpha} from the penultimate to the last line.
The first to the second line follows from $K \independent D^i$, which holds because $K \independent D$ and $D^i$ is a permutation of $D$ that does not depend on K.

\citet{barber2022conformal} show two additional variations of the coverage gap bound based on Theorem~\ref{thm:splitCP_nonExchage}.
We do not replicate them here. 
One notable observation of theirs, however, is that instead of using the total variation distance based on the residual vectors also the distance of the raw data vector can be used:

\begin{equation}
   \Delta \ \text{Cov} \geq - \sum_i \tilde{w}_i\cdot\dtv(D, D^i).
\end{equation}

%In the case of independent data points, they show that we can bound the %coverage gap also by

%\begin{equation}
%\Delta \ \text{Cov} \geq - 2\sum_i \tilde{w}_i\cdot 
%\dtv\big((X_i,Y_i),(X_{t+1},Y_{t+1})\big).
%\end{equation}
This coverage bound suggests that low weights for time steps corresponding to high total variation distance ensure small coverage gaps.
However, lower weights weaken the estimation quality of the prediction intervals.
Hence, all relevant time steps should receive high weights, while all others should receive weights close to zero.

\ourmethod leverages these results by using the association weights $a_i$ from Equation~\ref{eq:hopfield-association} to reflect the weights $\tilde{w_i}$ of non-exchangeable split conformal prediction.
\ourmethod aims to assign high association weights to data points that are from the same error distribution as the current point, and low weights to data points in the opposite case.
By Theorem~\ref{thm:splitCP_nonExchage}, association weights that follow this pattern lead to a small coverage gap.


\paragraph{An asymptotic (conditional) coverage bound} If we assume clearly distinguished error distributions instead of gradually shifting ones, hard weight assignment, i.e., association weights that are either zero or one, are a meaningful choice.
Given this simplification we can directly link \ourmethod to the work of \citet{xu2021conformal}.

\citet{xu2021conformal} introduce a variant of (split) conformal prediction with asymmetric prediction intervals:

\begin{align}\label{def:xu_CI_form}
     \widehat{C}^{\alpha}_{t}(x_{t+1}):=\bigg[
 &\muh_{-(t+1)}(x_{t+1}) +  \quant_{\frac{\alpha}{2}}(\{\hat{\epsilon}_i\}_{i=1}^{t}),
 \muh_{-(t+1)}(x_{t+1}) + \quant_{1-\frac{\alpha}{2}}\big(\{\hat{\epsilon}_i\}_{i=1}^{t}\big)\bigg]
\end{align}

The approach taken by \citet{xu2021conformal} additionally adjusts the share of $\alpha$ between the upper and lower quantiles such that the width of the prediction interval is minimized.
In contrast, \ourmethod adopts a simpler strategy and uses $\frac{\alpha}{2}$ on both sides, which mitigates the need for excessive recalculation of the quantile function. 

Based on the introduced interval calculation procedure, they also provide bounds regarding its coverage properties, in particular about the possible coverage gap.
While \citet{xu2021conformal} base their assumption on error properties only in temporal proximity, we generalize this to proximity in the learned feature representation (which can include but is not limited to information about temporal proximity).
Based on this proximity we assume:

\begin{assumption}[Error regimes]\label{app:local-error-assumption}
Given $\{(\mathbf{Z}_i, y_i, \hat{y}_i)\}^{t+1}_{i=1}$ which represents the (encoded) dataset and the predictions $\hat{y}_i$ of $\muh$, there exists a partition $\mathcal{T} = \{T_i\}_{i \in I}$ of the time steps $\{1,\dots, t\}$, so that
\begin{enumerate}[itemsep=1mm]
    %\item[(i)] $\forall i \in I: |T_i| \geq 2$, \note{eigher make $T_i$ big or make $E_i$ very different}
    \item[(i)] $\forall i \in I: \{\epsilon_t = y_t - \hat{y}_t \, | \, t \in T_i\}  \sim  E_i$, %\epsilon_t = y_t - \hat{y}_t$ within every $A_i$ are drawn from a single error distribution $E_i $.
    \item[(ii)] $\exists i \in I: \{ \epsilon_{t+1} \} \sim E_i$, %$\exists i \in I: \{\epsilon_t = y_t - \hat{y}_t \, | \, t \in T_i\} \cup \{ \epsilon_{t+1} \} \sim E_i$,
    \item[(iii)] the subset $T_i$ corresponding to $t+1$  can be identified based on $\mathbf{Z}_{t+1}$,
    \item[(iv)] $\forall i,j \in I: i = j \vee E_i \neq E_j $,
\end{enumerate}
\end{assumption}

where each $E_i$ is an arbitrary error distribution.

Properties (i) and (ii) imply that any given time step $t+1$ can be associated to a subset $T_i$ where all errors follow a common error distribution $E_i$; property (iii) ensures that the partition can be found or approximated from a set of features. 
Property (iv) is not necessary for the discussion at hand but avoids degenerate solutions, since tight coverage bounds call for assigning all errors from the same error distribution to one large partition.
If the errors of the associated subset were sampled i.i.d.\ from $E_i$, we could use standard split conformal methods \citep{vovk2005algorithmic}, and the variation distance between permutations of the errors in properties (i) and (ii) is zero.
In practice, error noise can make it challenging to find and delineate a given partition. Thus, \ourmethod uses a soft-selection approach to approximate the discrete association of a partition and the current features.
%This should allow to also partially handle gradual distribution shifts and weighted consideration of temporal proximity similar to \nexcp.

% Maybe again refer to the barber bound

To align \ourmethod with the results of \citet{xu2021conformal}, and thus make use of their proposed coverage bounds for asymmetric prediction intervals, we first introduce their decomposition of the error:
$\epsilon_t = \check{\epsilon}_t + (\mu(\mathbf{x}_t) - \muh(\mathbf{x}_t))$, where $f$ is the true generating function, $\muh$ the prediction model, and $\check{\epsilon}$ a noise term associated with the label.
The notation $\mu_{-t}(\mathbf{x})$ stems from \citet{xu2021conformal} and denotes an evaluation of $\mu$, selected without using $t$.

Next, we adopt their i.i.d.\ assumption about the noise with regard to our approach:

\begin{assumption}[Noise is approximately {\it i.i.d.}\ within an error regime; adaptation of \citet{xu2021conformal}, Assumption~1]\label{ass:error_iid}
Assume $\exists i \in I : \{\check{\epsilon}_t | t \in T_i\} \cup \{\check{\epsilon}_{t+1}\} \overset{\mathrm{iid}}{\sim} E_i$ and that the CDF $\check{F}_{i}$ of $E_i$ is Lipschitz continuous with constant $L_i>0$.
\end{assumption}

This i.i.d.\ assumption within each error regime only concerns the noise of the true generating function.
\citet{xu2021conformal} note that the generating process itself can exhibit arbitrary dependence and be highly non-stationary while still having i.i.d.\ noise.
The assumption is thus relatively weak.   


\begin{assumption}[Estimation quality; adaptation of \citet{xu2021conformal}, Assumption~2]\label{ass:quality} 
There exist real sequences $\{\delta_{i,|T_i|}\}_{i,|T_i|\geq 1}$ such that 
\begin{enumerate}
    \item[(i)] $\forall i \in I: \frac{1}{|T_i|} \sum_{t \in T_i} (\muh_{-t}(\Bx_t)- \mu(\Bx_t))^2 \leq \delta_{i,|T_i|}^2$
    \item[(ii)] $\exists i \in I: |\muh_{-(t+1)}(\Bx_{t+1})-\mu(\Bx_{t+1})|\leq \delta_{i,|T_i|}$, where the $i$ for a given $t+1$ is the same $i$ as in assumption~\ref{ass:error_iid}.
\end{enumerate}
\end{assumption}

We want to bound the coverage of \refE{def:xu_CI_form}, which is based on the prediction error $\epsilon$.
However, Assumption~\ref{ass:error_iid} only considers the noise $\check{\epsilon}$ of the true generating function.
\citet{xu2021conformal} bind the difference between the CDF of the true generation function $\check{F_i}$, the \textit{empirical} analogue $\tilde{F_i}(x) := \frac{1}{|T_i|}\sum_{j \in T_i}\mathbf{1}\{\check{\epsilon}_j \leq x\}$, and the empirical CDF of the prediction error $F_i(x) := \frac{1}{|T_i|}\sum_{j \in T_i}\mathbf{1}\{\epsilon_j \leq x\} $.

In particular, they state two lemmas:

\begin{lemma}[Adaptation of \citet{xu2021conformal}, Lemma~1]\label{lem:regu_xu}
Under Assumption \ref{ass:error_iid}, for any subset size $|T_i|$, there is an event $A_T$ which occurs with probability at least $1- \sqrt{\log (16|T_i|)/|T_i|}$, such that conditioning on $A_T$,
\[
     \sup_{x} |\tilde{F}_i(x)-\check{F}_i(x)| \leq \sqrt{\log (16|T_i|)/|T_i|}.
\]
\end{lemma}


\begin{lemma}[Adaptation of \citet{xu2021conformal}, Lemma~2]\label{lem:consis_xu}
Under Assumptions \ref{ass:error_iid} and \ref{ass:quality}, we have
\[
    \sup_x |F_{i}(x)-\tilde{F}_{i}(x)|\leq (L_{i}+1)\delta_{i,|T_i|}^{2/3}+2\sup_x |\tilde{F}_{i}(x)-\check{F}_{i}(x)|.
\]
\end{lemma}

With this result we arrive at a bound for a conditional and a marginal coverage:

\begin{theorem}[Conditional coverage gap; adaptation of \cite{xu2021conformal}, Theorem 1]\label{thm:cond_cov_xu}
Given the prediction interval $\widehat{C}^{\alpha}_t$ defined in \refE{def:xu_CI_form}. Under Assumption \ref{ass:error_iid} and \ref{ass:quality}, for any subset size $|T_i|$, $\alpha \in (0,1)$, and $\beta \in [0,\alpha]$, we have:
\begin{align}
    \exists i \in I: & \big|\PR{Y_{t+1}\in \widehat{C}^{\alpha}_{t}(X_{t+1})|X_{t+1}=\Bx_{t+1}}-(1-\alpha)\big| \nonumber \\
    \leq & 12\sqrt{\log (16|T_i|)/|T_i|} +4 (L_i + 1) (\delta_{i,|T_i|}^{2/3}+\delta_{i,|T_i|}). \label{eq:main}
\end{align}
Furthermore, if $\{\delta_{i,|T_i|}\}_{|T_i|\geq 1}$ converges to zero, the upper bound in \eqref{eq:main} converges to 0 when $|T_i| \rightarrow \infty$, and thus the conditional coverage is asymptotically valid.
\end{theorem}

\begin{theorem}[Marginal coverage gap; adaptation of \cite{xu2021conformal}, Theorem 2]\label{thm:marg_cov_xu}
Given the prediction interval $\Ch^{\alpha}_t$ defined in \refE{def:xu_CI_form}. Under Assumption \ref{ass:error_iid} and \ref{ass:quality}, for any subset size $|T_i|$, $\alpha \in (0,1)$, and $\beta \in [0,\alpha]$, we have:
\begin{align}
    \exists i \in I: \big|\PR{Y_{t+1}\in \widehat{C}^{\alpha}_{t}(X_{t+1})}-(1-\alpha)\big| \nonumber \leq 12\sqrt{\log (16|T_i|)/|T_i|}+4(L_i + 1) (\delta_{i,|T_i|}^{2/3}+\delta_{i,|T_i|}).
\end{align}
\end{theorem}


The proofs of the above theorems follow from \citet{xu2021conformal}.
To apply our adaptions to their poofs, one has to select the error regime matching to the test time step $t+1$, i.e., select the right subset $T_i$ from the partitioning of time steps.
$T_i$ then corresponds to the overall calibration data in the original assumptions, lemmas, theorems, and the respective proofs.
This is valid since Assumption~\ref{ass:quality}-(i) encompasses all subsets and the existing subset in Assumption~\ref{ass:error_iid} and Assumption~\ref{ass:quality}-(ii) is, as noted, the same.

\citet{xu2021conformal} provide additional bounds for two variations of Assumption~\ref{ass:error_iid}, imposing only the requirement of strongly mixing noise or noise following a linear process.
These bounds apply also to our adaption but are omitted here for brevity.

\paragraph{Notes on Exchangeability, Stationarity, Seasonality, and Drifts} 
Comparing \ourmethod to standard CP we see that \ourmethod assumes exchangeability over the weighted time steps within regimes.
Hence, we require that the relationship between inputs and outputs remains stationary within regimes, which is weaker than requiring that the time series as such is stationary.
For example, it allows that time series values decrease or grow over time as long as these changes are reflected in the inputs of the MHN and correspond to a regime.
HopCPT softens the exchangeability requirements of standard CP in two ways:

\begin{enumerate}
    \item The CP mechanism assumes that exchangeability is given within a regime that is selected by the MHN association.
    \item Even if (1) is not the case, the memory of the MHN (and therefore the calibration data) is updated with each new observation. This memory update allows HopCPT to account for shifts â€” empirically, the benefit is visible in our experiments on the sap flow dataset, as we discuss briefly in Section 3.2 (lines 283-289) and in more detail in Appendix A.3 (Error Trend \& Figure 4).
\end{enumerate}

Regarding seasonality: HopCPT uses covariate features to describe the current time step. Hence, HopCPT implicitly considers seasonality by comparing a given time step with the time steps from the memory. Whenever a time step of a certain season is encountered, it focuses on time steps from the same season. In fact, our experiments already comprise both regular and irregular seasonality patterns. For example, the streamflow dataset contains yearly reoccurring patterns (spring, summer, fall, winter) as well as irregularly reoccurring similar flood events. In both cases, HopCPT can identify related time steps based on the covariates.

Regarding data drifts: an advantage of HopCPT is that the memory is updated with every new observation. Therefore, HopCPT can somewhat incorporate drifts in the target value as long as the covariates remain so that the learned representation is able to focus on the relevant observations. Empirically, the robustness against drifts is visible in our experiments on the sap flow dataset, as we discuss briefly in Section 3.2 (lines 283-289) and in more detail in Appendix A.3 (Error Trend \& Figure 4).
