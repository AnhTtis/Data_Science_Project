\documentclass{article}

\input{_neurips2023/neurips_stuff}
\usepackage{selectp}
%\outputonly{14-47}

\begin{document}

\maketitle


%   Style Guide / Unified Notation Overview
%   DO                               | DO NOT
% - Use Oxford comma
% - non-exchangeable                 | non exchangeable
% - Table~\ref                       | table~\ref  
% - Section~\ref                     | section~\ref
% - Appendix~\ref                    | appendix~\ref
% - datasets                         | data-sets, data sets
% - split conformal                  | split-conformal
% - Winkler score                    | Winkler Score, Winkler-Score, Winkler-score
% - data-splitting                   | data splitting
% - similarity-based                 | similarity based
% - $\Delta \ \text{Cov}$
% - \knn (command)                   | KNN, kNN, k-NN
% - data as singular (e.g. data does)
% - Paragraphs: (1) Ends with Period  (2) Sentence Case
% - Section Header: (1) Title Case
% time steps                        | timesteps
% time series                       | time-series, timeseries
% reweighting                       | re-weighting
% $i$-th                            | $i$th

\begin{abstract}


To quantify uncertainty, conformal prediction methods 
are gaining continuously more interest and have already been
successfully applied to various domains.
However, they are difficult to apply to time series 
as the autocorrelative structure of time series 
violates basic assumptions required by conformal prediction.
We propose \ourmethod, a novel conformal prediction approach for time series
that not only copes with temporal structures but leverages them.
We show that our approach is theoretically well justified for
time series where temporal dependencies are present.
In experiments, 
we demonstrate that our new approach outperforms state-of-the-art 
conformal prediction methods on multiple real-world time series datasets from four different domains.
\end{abstract}
\vspace{-4mm}

\section{Introduction}

Uncertainty estimates are imperative to make actionable predictions for complex time-dependent systems \citep[e.g.,][]{gneiting2014probforecasting, zhu2017deep}. 
This is particularly evident for environmental phenomena such as flood forecasting \citep[e.g.,][]{krzysztofowicz2001case}, since they exhibit pronounced seasonality.
Conformal Prediction \citep[CP,][]{vovk1999machine} provides uncertainty estimates based on prediction intervals. 
It achieves finite-sample marginal coverage with almost no assumptions, except that the data is exchangeable \citep{vovk2005algorithmic, vovk2012conditional}. 
However, CP for time series is not trivial because temporal dependencies often violate the exchangeability assumption.

\noindent
\textbf{\ourmethod.} \;\,
For time series models that predict a given system, the errors are typically specific to the input describing the current situation.
To apply CP, \ourmethod uses continuous Modern Hopfield Networks (MHNs). 
The MHN uses large weights for past situations that were similar to the current one
and weights close to zero for past situations that were dissimilar to the current one (Figure~\ref{fig:figure1}).
The CP procedure uses the weighted errors to construct strong uncertainty estimates close to the desired coverage level.
This exploits that similar situations (which we call a regime) tend to follow the same error distribution.
\ourmethod achieves new state-of-the-art efficiency, even under non-exchangeability and on large datasets. 

\noindent
\textbf{Our main contributions are:}
\vspace{-1.725mm}
\begin{enumerate}
    \item We propose \ourmethod, a CP method for time series, a domain where CP struggled so far.
    \item We introduce the concept of error regimes to CP, which softens the excitability requirements and enables efficient CP for time series.
    \item \ourmethod uses MHN for a similarity-based sample reweighting.
    In contrast to existing approaches, \ourmethod can learn from large datasets and predict intervals at arbitrary coverage levels without retraining.
    %We motivate this technique theoretically and on a synthetic example.
%    \item We theoretically show that our CP method is justified and
%    that it is valid for non-exchangeable data.
    \item \ourmethod achieves state-of-the-art results for conformal time series prediction tasks from various real-world applications.
    \item HopCPT is the first algorithm with coverage guarantees that was applied to hydrological prediction applications --- a domain where uncertainty plays a key role in tasks such as flood forecasting and hydropower management.
\end{enumerate}

\begin{figure}[t]
\includegraphics[width=13cm]{graphics/tscp-figure1.pdf}
\centering
\caption{Schematic illustration of \ourmethod. The Modern Hopfield Network (MHN) identifies regimes similar to the current one and up-weights them (colored lines). The weighted information enriches the conformal prediction (CP) procedure so that prediction intervals can be derived.}
\label{fig:figure1}
\end{figure}

\subsection{Related Work}\label{sec:related-work}
% Related work Time Series Regimes
\paragraph{Regimes.} In a world with non-linear dynamics, 
different environmental conditions lead to different error characteristics 
of models that predict
based on these conditions.
If we do not account for these different conditions, 
temporal changes may lead to unnecessarily large prediction intervals, i.e., to high uncertainty.
For example, solar energy production is high and stable on a sunny day, 
fluctuates during cloudy days, and is zero at night.
Often, the current environmental condition
was already observed at previous points in time. 
The error at these time steps is therefore assumed to 
have the same distribution as the current error.
Following \citet{quandt1958estimation} and \citet{hamilton1990analysis}, 
we call the sets of time steps with similar 
environmental conditions \textit{regimes}.
Although conditional CP is in general impossible \citep{foygel2021limits}, 
we show that conditioning on such regimes 
can lead to better prediction intervals 
while preserving the specified coverage.
%\paragraph{Applications of regimes.}
\citet{hamilton1990analysis} models time series regimes as a discrete Markov process and conditions a classical autoregressive model on the regime states.
\citet{sanquer2012smooth} use a smooth transition approach to model multi-regime time series. 
\citet{tajeuna2021modeling} propose an approach to discover and model regime shifts in an ecosystem that comprises multiple time series. 
Further, \citet{masserano2022adaptive} handle distribution shifts by retraining a forecasting model with training data from a non-uniform adaptive sampling.
Although these approaches are not in a CP setting, their work is similar in spirit, as they also follow the general idea to condition on parts of the time series with similar regimes.


%Related work non-exchangeable CP
\paragraph{CP and extensions.} For thorough introductions to CP, we refer the reader to the foundational work of \citet{vovk1999machine} and a recent introductory paper by \citet{angelopoulos2021gentle}.
There exist a variety of extensions for CP that go ``beyond exchangeability'' \citep{vovk2005algorithmic}.
For example, \citet{papadopoulos2011reliable} apply CP to a nearest neighbor regression setting, \citet{teng2022predictive} apply CP to the feature space of models, \citet{angelopoulos2020uncertainty} use CP to generate uncertainty sets for image classification tasks, and \citet{toccaceli2017conformal} use a label-conditional variant to apply CP to biological activity prediction.
Of specific interest to us is the research regarding non-exchangeable data of  \citet{tibshirani2019conformal} and \citet{barber2022conformal}.
Both handle potential shifts between the calibration and test set by reweighting the data points.
\citet{tibshirani2019conformal} restrict themselves to settings with full knowledge about the change in distribution; \citet{barber2022conformal} rely on fixed weights.
In our work, we refrain from this assumption because such information is typically not available in time series prediction.
Another important research direction is the work on normalized conformity scores \citep[see][and references therein]{fontana2023conformal}. 
In this setting, the goal is to adapt the conformal bounds through a scaling factor in the nonconformity function.
The work on normalized conformity scores does not explicitly tailor their approaches to time series. 

%Related work Time Series CP (most important)
\paragraph{CP for time series.} \citet{gibbs2021adaptive} and \citet{zaffran2022adaptive} account for shifts in sequential data by continuously adapting an internal coverage target.
Adaption-based approaches like these are orthogonal to \ourmethod and can serve as an enhancement.
\citet{stankeviciute2021conformal} use CP in conjunction with recurrent neural networks in a multi-step prediction setting, assuming that the series of observations is independent.
Thus, no weighting of the scores is required. 
\citet{Sun2022CopulaCP} introduce CopulaCPTS which applies CP to time series with multivariate targets.
They conformalize their prediction based on a copula of the target variables and adapt their calibration set in each step.
%\citet{barber2022conformal} introduce \nexcp, a weighting scheme based on bounds for the coverage loss that depend on the total variation distance between a sequence and a version of itself with exchanged data points.
\citet{jensen2022ensemble} use a bootstrap ensemble to enable CP on time series. 
\nexcp \citep{barber2022conformal} uses exponential decay as the weighting method, arguing that the recent past is more likely to be of the same error distribution. 
\ourmethod can learn this strategy, but does not a priori commit to it. 
\citet{xu2021conformal} propose \enbpi, which uses quantiles of the $k$ most recent errors for the prediction interval.
Additionally, they introduce a novel leave-one-out ensembling technique.
This is specifically geared to settings with scarce data and difficult to use for larger datasets, which is why we do not apply it in our experiments. 
\enbpi is designed around the notion that near-term errors are often independent and identically distributed and therefore exchangeable.
\spic \citep{xu2022sequential} softens this requirement by exploiting the autocorrelative structure with a random forest.
However, it re-calculates the random forest model at each time step, which is a computational burden that prohibits its application to large datasets.
Our approach relaxes the requirement even further, as we do not assume that the data for the interval computations pertains to the $k$ most recent errors. 

\paragraph{Non-CP methods} Beside CP there exist a wide range of approaches for uncertainty-aware time series prediction.
For example, Mixture Density Networks \citep{bishop1994mixture} directly estimate the parameters of a mixture of distributions.
However, they require a distribution assumption and do not provide any theoretical guarantees.
Gaussian Processes \citep[e.g.,][]{zhu2023bayesian, corani2021time, sun2022recurrent} model time series by calculating posterior functions based on the samples and a prior but are computationally limited for large and high dimensional datasets.

\paragraph{Continuous Modern Hopfield Networks.} 
MHN are energy-based associative memory networks.
They advance conventional Hopfield Networks \citep{Hopfield:82} by 
introducing continuous queries and states via a new energy function.
The new energy function leads to exponential storage capacity, 
while retrieval is possible with a one-step update \citep{Ramsauer:21}.
Examples for successful applications of MHN are \citet{Widrich:20nips, furst2021cloob,  dong2022retrosynthesis, klambauer2022cloome, paischer2022history, schafl2022hopular}; and \citet{xu2022txt2img}. % , hopfield-ood:22
MHN are related to Transformers \citep{Vaswani:17} as their attention mechanism is closely related to the association mechanism in MHN.
In fact, \citet{Ramsauer:21} show that Transformers attention is a special case of the MHN association, at which we arrive when the queries and states are mapped to an associative Hopfield space with the dimensions $d_k$, and the inverse softmax temperature is set to $\beta = \frac{1}{\sqrt{d_k}}$.
However, we use the framework of MHN because we want to highlight the associative memory mechanism as \ourmethod directly ingests encoded observations.
This perspective further allows \ourmethod to update the memory for each new observation.
For more details, we refer to Appendix~\ref{sec:appendix-hopfield} in the supplementary material.

\subsection{Setting}\label{sec:setting}

Our setting consists of a multivariate time series $\{(\Bx_t, y_t)\}$, $t=1,\dotsc,T$, with a feature vector $\Bx_t \in \dR^m$, a target variable $y_t \in \dR$, and a given black-box prediction model $\muh$ that generates a point prediction $\hat{y}_t = \muh(\BX_t)$.
The input feature matrix $\BX_{t+1}$ can include all previous and current feature vectors $\{\Bx_i\}^{t+1}_{i=1}$, as well as all previous targets $\{y_i\}^{t}_{i=1}$.
%Given the features $\BZ_{t+1}$, our goal is to construct a corresponding prediction interval $\Ch_t^{\alpha}(\BZ_{t+1})$ --- a set that includes $y_{t+1}$ with at least a specified probability $1 - \alpha$.
Our goal is to construct a corresponding prediction interval $\Ch_t^{\alpha}(\BZ_{t+1})$ --- a set that includes $y_{t+1}$ with at least a specified probability $1 - \alpha$.
In its basic form, $\BZ_{t+1}$ will only contain  $\hat{y}_{t+1}$, but it can also inherit $\BX_{t+1} $ or other useful features.
Following \citet{vovk2005algorithmic}, we define the \textit{coverage} as
\begin{equation}
\PR{Y_{t+1} \in \Ch_t^{\alpha}(\BZ_{t+1})} \geq 1-\alpha,
\label{eq:cp_prop}
\end{equation} 


where $Y_{t+1}$ is the random variable of the prediction.
An infinitely wide prediction interval is 100\% reliable, but not informative of the uncertainty.
Thus, CP aims to minimize the width of the prediction interval $\Ch^{\alpha}_t$, while preserving the coverage.
A smaller prediction interval is called a more \textit{efficient} interval \citep{vovk2005algorithmic} and usually evaluated as the mean of the interval width over the prediction period (\textit{PI-Width}).

Standard split conformal prediction takes a calibration set of size $n$ which has not been used to train the prediction model $\muh$.
For each data sample, it calculates the so-called non-conformity score \citep{vovk2005algorithmic}.
In a regression setting, this score often simply corresponds to the absolute error of the prediction \citep[e.g.,][]{barber2022conformal}.
The prediction interval is then calculated based on the empirical $1-\alpha$ quantile $\quant_{1-\alpha}$ of the calibration scores:
\begin{equation}
    \Ch_n^{\alpha}(\BZ_{t+1}) = \muh(\BX_{t+1}) \pm \quant_{1-\alpha} (\{|y_i - \muh(\BX_i)|\}_{i=1}^n).
\end{equation}

If the data is exchangeable and $\muh$ treats the data points symmetrically, the errors on the test set follow the distribution from the calibration.
Hence, the empirical quantiles on the calibration and test set will be approximately equal and it is guaranteed that the interval provides the desired coverage. 

The \textit{actual marginal miscoverage $\alpha^\star$} is based on the observed samples of a test set.
If the \textit{specified miscoverage} $\alpha$ differs from the $\alpha^\star$ in the evaluation, we denote the difference as the coverage gap $ \Delta \ \text{Cov} = \alpha - \alpha^\star$.



The remainder of this manuscript is structured as follows: 
In Section~\ref{sec:our-method}, we present \ourmethod alongside a theoretical motivation and a synthetic example that demonstrates the advantages of the approach.
In Section~\ref{sec:experiments}, we evaluate the performance against state-of-the-art CP approaches and discuss the results.
Section~\ref{sec:conclusion} gives our conclusions and provides an outlook on potential future work.


\section{\ourmethod}\label{sec:our-method}
\ourmethod\footnote{\repourl} combines conformal-style quantile estimation with learned similarity-based MHN retrieval.

\subsection{Theoretical Motivation}\label{sec:theory}

%
% No own thoughts here but only stating what in the barber paper. 
% See current notes in appendix. 
%
The theoretical motivation for the MHN retrieval stems from  \citet{barber2022conformal}, who introduced CP with weighted quantiles.
In the split conformal setting, the according prediction interval is calculated as
\begin{equation}\label{eq:cov-bound-interval}
    \Ch_t^{\alpha}(\BX_{t+1}) = \muh(\BX_{t+1}) \pm \quant_{1-\alpha} \left(\sum_{i=1}^t a_i \delta_{\epsilon_i} + a_{t+1} \delta_{+\infty} \right),
\end{equation}

where $\muh$ represents an existing point prediction model, $\quant_{\tau}$ is the $\tau$-quantile of a distribution, and $\delta_{\epsilon_i}$ is a point mass at $|\epsilon_i|$ (i.e., a probability distribution with all its mass at $|\epsilon_i|$), where $\epsilon_i$ are the errors of the existing prediction model defined by $ \epsilon_i = y_i - \muh(\BX_{i})$.
%\begin{equation}\label{eq:epsilon-calc}
%    \epsilon_i = y_i - \muh(\BX_{i}). 
%\end{equation}
The normalized weight $a_i$ of data sample $i$ is
\begin{equation}\label{eq:weighted-a}
    a_i = \begin{cases}
        \frac{1}{\omega_1 + \dotsc + \omega_t + 1} & \text{if} \ i = t+1, \\
        \frac{\omega_i}{\omega_1 + \dotsc + \omega_t + 1} & \text{else},
    \end{cases}
\end{equation}

where $\omega_i$ are the un-normalized weights of the samples.
In the case of $\omega_1 = \dotsc = \omega_t = 1$, this corresponds to standard split CP.
Given this framework, \citet{barber2022conformal} show that $\Delta \ \text{Cov}$ can be bounded in a non-exchangeable data setting:
Let $D=((\BX_1, Y_1), \dotsc, (\BX_{t+1}, Y_{t+1}))$ be a dataset where the last entry represents the test sample, and $D^i$ be a permutation of $D$ which exchanges the test sample at $t+1$ with the $i$-th sample.
Then, $\Delta \ \text{Cov}$ can be bounded from below by the weighted sum of the total variation distances $d_\text{TV}$ between these permutations:
\vspace{-1mm}
\begin{equation}\label{eq:cov-bound}
    \Delta \ \text{Cov} \geq - \sum_{i=1}^t a_i \cdot \dtv(D,D^i)
\end{equation}

%
% This is now how we connect with own thoughts - Meaningfulness has to be checked and formalized better 
%
\vspace{-1mm}
If $D$ is a composite of multiple regimes and the test sample is from the same regime as the calibration sample $i$, then the distance between $D$ and $D^i$ is small.
Conversely, the distance might be big if the calibration sample is from a different regime.
In \ourmethod, the MHN association resembles direct estimates of $a_i$ --- dynamically assigning high values to samples from similar regimes.

Appendix~\ref{sec:theory-appendix} provides an extended theoretical discussion that relates \ourmethod to the work of \cite{barber2022conformal} and \citet{xu2021conformal}, who provide the basis for our theoretical analyses of the CP interval.
The latter compute individual quantiles for the upper and lower bound of the prediction interval on the errors themselves, in contrast to standard CP which uses only the highest absolute error quantile.
This can provide more efficient intervals in case $E[\epsilon] \neq 0$ within the corresponding error distribution.
Applying this principle to \ourmethod where the error distribution is conditional to the error regime and assuming that \ourmethod can successfully identify these regimes (Appendix: Assumption~\ref{app:local-error-assumption}), we arrive at a conditional asymptotic coverage bound (Appendix: Theorem~\ref{thm:cond_cov_xu}) and an asymptotic marginal coverage bound (Appendix: Theorem~\ref{thm:marg_cov_xu}).
%Appendix~\ref{sec:theory-appendix} provides more details, an extended theoretical discussion, as well as the respective proofs.

\subsection{Associative Soft-selection for CP}\label{sec:mhn}
We use a MHN to identify parts of the time series where the conditional error distribution is similar:
For time step $t+1$, we query the memory of the past and look for matching patterns.
The MHN then provides an association vector $\Ba_{t+1}$ that allows to soft-select the relevant periods of the memory.
The selection procedure is analogous to a $k$-nearest neighbor classifier for hard selection, but it has the advantage that the similarity measure can be learned. Formally, the soft-selection is defined as:
\begin{equation}\label{eq:hopfield-association}
    \Ba_{t+1} = \text{softmax}\big(\beta \, m(\BZ_{t+1}) \, \BW_q \, \BW_k \, m(\BZ_{1:t}) \big),
\end{equation}

where $m$ is an encoding network (Section~\ref{sec:encoding-network}) that transforms the raw time series features of the current step $\BZ_{t+1}$ to the query pattern and the memory $\BZ_{1:t}$ to the stored key patterns;
$\BW_q$ and $\BW_k$ are the learned transformations which are applied before associating the query with the memory;
$\beta$ is a hyperparameter that controls the softmax temperature. 
As mentioned above, \ourmethod uses the softmax to amplify the impact of the data samples that are likely to follow a similar error distribution and to reduce the impact of samples that follow different distributions (see Section~\ref{sec:theory}).
This error weighting leads not only to more efficient prediction intervals in our experiments, but can also reduce the miscoverage (Section~\ref{sec:results-discuss}).

With the soft-selected time steps we can derive the CP interval using the observed errors $\epsilon$.
Following \citet{xu2021conformal}, we use individual quantiles for the upper and lower bound of the prediction interval, calculated from the errors themselves.
\ourmethod computes the prediction interval $\Ch^{\alpha}_t$ for time step $t+1$ in the following way:
\begin{equation}\label{eq:myconfbounds}
\begin{aligned}
\Ch_t^{\alpha}(\BZ_{t+1}) = %
   \Bigl[ & \muh(\BX_{t+1}) + q(\frac{\alpha}{2}, \BZ_{t+1}), \muh(\BX_{t+1}) + q(1-\frac{\alpha}{2}, \BZ_{t+1}) \Bigr],
\end{aligned}
\end{equation}

where $q(\tau, \BZ_{t+1}) = \quant_{\tau} \left(E_{t+1} \right)$ and $E_{t+1}$ is a multiset created by drawing $n$ times from $[\epsilon_i]_{i=1}^t$ with corresponding probabilities $[a_{t+1,i}]_{i=1}^t$.

\subsection{Encoding Network}\label{sec:encoding-network}

We embed $\BZ_t$ using a 2-layer fully connected network $m^L$ with ReLU activations and enhance the representation by temporal encoding features $\Bz_{T,t}^{\text{time}}$. The full encoding network can be represented as $m(\BZ_t)=[m^L(\BZ_t) \ || \ \Bz_{T,t}^{\text{time}}]$, 
%\begin{equation}
%    m(\BZ_t)=[m^L(\BZ_t) \ || \ z_{T,t}^{\text{time}}],
%\end{equation}
where $\Bz_{T,t}^{\text{time}}$ is a simple temporal encoding that makes time dependent notions of similarity learnable (e.g., the windowed approach of \enbpi or the exponentially decaying weighting scheme of \nexcp). Specifically, we use $z_{T,t}^{\text{time}} = \frac{t}{T}$.
In general, we find that our method is not very sensitive to the exact encoding strategy (e.g., number of layers), which is why we kept to a simple choice throughout all experiments.

%We use a simple temporal encoding to make time dependent notions of %similarity learnable, for example, the windowed approach of \enbpi or the %exponentially decaying weighting scheme of \nexcp:
%\begin{align}
%    \Bz_{T,t}^{\text{time}} = \frac{t}{T}.
%\end{align}

\subsection{Training Procedure}

We partition the split conformal calibration data into training and validation sets.
Training MHN with quantiles is difficult, which is why we use an auxiliary task:
Instead of applying the association mechanism from Equation~\ref{eq:hopfield-association} directly, we use the absolute errors as the value patterns of the MHN.
This way, the MHN learns to align errors from time steps with similar regime properties.
Intuitively, the observed errors from these time steps should work best to predict the current absolute error.
We use the mean squared error as loss function (Equation~\ref{eq:train-loss}).
To allow for efficient training, we simultaneously calculate the association of all $T$ time steps within a training split with each other.
We mask the association from a time step to itself.
The resulting association from each step to each step is $\BA_{1:T,1:T}$, and the loss function $\mathcal{L}$ is
\begin{equation}\label{eq:train-loss}
    \mathcal{L} = T^{-1} * \lVert(|\bm{\epsilon}_{1:T}| - \BA_{1:T,1:T}  |\bm{\epsilon}_{1:T}|)^2\rVert_1.
\end{equation}

This incentivizes the network to learn a representation such that the resulting soft-selection focuses on time steps with a similar error distribution.
Alternatively, one could use a loss based on sampling from the softmax.
This would correspond more closely to the inference procedure.
However, it makes training less efficient because each training step only carries information about a single value of $\epsilon_i$. 
In contrast, $\mathcal{L}$ leads to more sample-efficient training.
Choosing $\mathcal{L}$ assumes that it leads to a retrieval of errors from appropriate regimes instead of mixing unrelated errors.
Section~\ref{sec:results-discuss} and Appendix~\ref{sec:appendix-full-tables} provide evidence that this holds empirically.


%For the model selection, we use a two-step approach: We first filter the models based on $\Delta \ \text{Cov}$ and then select the model with the smallest PI-Width.
%More details can be found in Appendix~\ref{sec:hyperparam}.


\subsection{Synthetic Example}\label{sec:synthetic-example} 
The following synthetic example illustrates the advantages of the \ourmethod association mechanism.
We model a bivariate time series $D=\{(x_t, y_t)\}^T_{t=1}$.
While $y_t$ serves as the target variable, $x_t$ represents the feature in our prediction.
The series is composed of two different regimes:
target values are generated by $y_t=10+x_t+\mathcal{N}(0,\frac{x_t}{2})$ or $y_t=10 + x_t + U(-x_t,x_t)$.
$x_t$ is constant within a regime ($x=3$ and $x=21$, respectively).
The regimes alternate. 
For each regime, we sample the number of time steps from the discrete uniform distribution $\mathcal {U}(1,25)$.
We create 1,000 time steps and split the data equally into training, calibration, and test sets.
We use a ridge regression model as prediction model $\muh$.
\ourmethod can identify the time steps of  relevant regimes and therefore creates efficient prediction intervals while still preserving the coverage (Figure~\ref{fig:toy-example}).
\enbpi, \spic, and \nexcp focus only on the recent time steps and thus fail to base their intervals on information from the correct regime.
Whenever the regime changes from small to high errors, \enbpi propagates the small error signal and therefore loses coverage.
Similarly, its prediction intervals for the small error regime are inefficient (Figure~\ref{fig:toy-example}, row 3).
\spic, \nexcp, and standard CP cannot properly select the relevant time steps, either.
They do not lose coverage, but produce wide intervals for all time steps (Figure~\ref{fig:toy-example}, rows 4 and 5).
Lastly, if we replace the MHN with a \knn, it can retrieve information from similar regimes. However, its naive retrieval mechanism fails to focus on the informative features because it cannot learn them (Figure~\ref{fig:toy-example}, row 2).

\begin{figure}[t]
\includegraphics[width=13.5cm]{graphics/toy_result_facet_2column.pdf}
\centering
\caption{Synthetic example evaluation. \ourmethod has the smallest  prediction interval width (PI-width), while maintaining the warranted coverage (i.e., a $\Delta \ \text{Cov}$ that is positive and close to zero).}
\label{fig:toy-example}
\end{figure}


\subsection{Limitations}
\ourmethod builds upon the formal guarantees of CP, but it is able to relax the data exchangeability assumption of CP which are problematic for time series data.
That said, \ourmethod still relies on assumptions on how it learns to identify the respective error regimes and exchangablity within regimes (see Section~\ref{sec:theory} and Appendix~\ref{sec:theory-appendix}).
Our extensive empirical evaluation suggests that these assumptions hold in practice.
\ourmethod is generally well-suited for very long time series because the memory requirement in the inference scales only linearly with the memory size.
Nevertheless, for extremely large datasets it can be the case that not all historical time steps may be kept in the Hopfield memory anymore.
\ourmethod can, similar to existing methods, disregard time steps by removing the oldest entries from the memory, or alternatively use a sub-sampling strategy.
On the other hand, for datasets with very scarce data it might be difficult to learn a useful embedding of the time steps.
In that case, it could be better to use kNN rather than learning the similarity with MHN (Appendix \ref{sec:knn-comparison}).
Additional considerations regarding the potential social impact of the method are summarized in Appendix~\ref{sec:social-impact}.

\section{Experiments}\label{sec:experiments} 
This section provides a comparative evaluation of \ourmethod and analyzes its association mechanism.

\subsection{Setup}

\paragraph{Datasets.}
We use datasets from four different domains:
(a) Three solar radiation datasets from the US National Solar Radiation Database \citep{sengupta2018national}.
The smallest one consists of 8 time series from different locations over a period of 84 days.
This dataset is also used in \citet{xu2021conformal, xu2022sequential}.
In addition, we evaluate on a 1-year and a 3-year dataset, with 50 time series each. 
(b) An air quality dataset from Beijing, China \citep{zhang2017cautionary}.
It consists of 12 time series, each from a different measurement station, over a period of 4 years.
The dataset has two prediction targets, the PM10 \citep[as in][]{xu2021conformal, xu2022sequential} and PM2.5 concentrations, which we evaluate separately.  
(c) Sap flow\footnote{Sap flow refers to the movement of water within a plant. In the environmental sciences, it is commonly used as a proxy for plant transpiration.} measurements from the Sapfluxnet data project \citep{poyatos2021global}.
Since the individual measurement series are considerably heterogeneous in length, we use a subset of 24 time series, each with between 15,000 and 20,000 data points and varying sampling rates.
(d) \hydro, a dataset of water flow measurements and corresponding meteorologic observations from 531 rivers across the continental United States \citep{newman2015development,addor2017camels}.
The measurements span 28 years at a daily time scale.
For more detailed information about the datasets see Appendix~\ref{sec:dataset-details}.

\paragraph{Prediction models.}
We use four prediction models for the solar radiation, the air quality, and the sap flux datasets to ensure that our results generalize:
a random forest, a LightGBM, a ridge regression, and a Long Short-Term Memory (LSTM) \citep[LSTM;][]{Hochreiter:97} model.
For the former three models we follow the related work \citep{xu2021conformal,xu2022sequential, barber2022conformal} and train a separate prediction model for each individual time series.
The random forest and LightGBM models are implemented with the darts library \citep{JMLR:v23:21-1177}, the ridge regression model with sklearn \citep{scikit-learn}.
For the LSTM model, we instead train a global model on all time series of a dataset, as is standard for state-of-the-art deep learning models \citep[e.g.,][]{Oreshkin2020N-BEATS:, salinas2020deepar, smyl2020hybrid}.
The LSTM is implemented with PyTorch \citep{paszke2019pytorch}.
For the streamflow dataset, we deviate from this scheme and instead only use the state-of-the-art model, which is also an LSTM network \citep[][see Appendix~\ref{sec:dataset-details}]{kratzert2020note}.

\setTableCaption{Performance of the evaluated CP algorithms for the \solarThreeY, \airTen, \sapflux, and \hydro (SF) datasets. The specified miscoverage level is $\alpha=0.1$ for all experiments. The column FC specifies the prediction algorithm used for the experiment (Forest: Random Forest, LGBM: LightGBM, Ridge: Ridge Regression, LSTM: LSTM neural network). Bold numbers correspond to the best result for the respective metric in the experiment (PI-Width and Winkler score), given that $\Delta \text{Cov} \ge - 0.25 \alpha$, i.e., the specific algorithm reached at least approximate coverage (the result is grayed otherwise). The error term represents the standard deviation over repeated runs with different seeds (results without an error term are from deterministic models).}
\input{results_tables_grayed/main_results_alpha_0.1.tex}


\paragraph{Compared approaches.}
We compare \ourmethod to different state-of-the-art CP approaches for time series data: \enbpi \citep{xu2021conformal}, \spic \citep{xu2022sequential}, \nexcp \citep{barber2022conformal}, CopulaCPTS \citep{Sun2022CopulaCP}, and AdaptiveCI\footnote{\adaptiveci works on top of an existing quantile prediction method. Hence, we exclusively make comparisons based on the LightGBM models that can predict quantiles instead of point estimates. The approach is orthogonal to the remaining compared models and could be combined with \ourmethod.} \citep{gibbs2021adaptive}. 
In addition, the results of standard split CP (CP) serve as a baseline, which, for the LSTM base predictor, corresponds to CF-RNN \citep{stankeviciute2021conformal} in our setting (one-step, univariate target variable).
Appendix~\ref{sec:hyperparam} describes our hyperparameter search for each method. 
For \spic, an adaption of the original algorithm was necessary to provide scalability to larger datasets.
Appendix~\ref{sec:spic-retrained} contains more details and an empirical justification.
Appendix~\ref{sec:adaptive-cp} evaluates the addition of \adaptiveci \citep{gibbs2021adaptive} as an enhancement to \ourmethod and the other time series CP methods.
Appendix~\ref{sec:appendix-full-tables} gives a comparison to non-CP methods.
Lastly, Appendix~\ref{sec:knn-comparison} presents a supplemental comparison to \knn that shows the superiority of the learned similarity representation in \ourmethod.

\newpage

\paragraph{Metrics.} In our analyses, we compute $\Delta \ \text{Cov}$, PI-Width, and the Winkler score \citep{winkler1972decision} per time series and miscoverage level.
The Winkler score jointly elicitates miscoverage and interval width in a single metric:
\begin{align}\label{eq:winkler-score}
    \text{WS}_{\alpha}(\BZ_{t+1}, y_{t+1}) =
    \begin{cases}
        \text{IW}_t^{\alpha}(\BZ_{t+1}) + \frac{2}{\alpha} (y_{t+1} - \Ch_t^{\alpha,u}(\BZ_{t+1})) & \text{if} \ y_{t+1} > \Ch_t^{\alpha,u}(\BZ_{t+1}),\\
        \text{IW}_t^{\alpha}(\BZ_{t+1}) + \frac{2}{\alpha} (\Ch_t^{\alpha,l}(\BZ_{t+1}) - y_{t+1}) & \text{if} \ y_{t+1} <  \Ch_t^{\alpha,l}(\BZ_{t+1}),\\
        \text{IW}_t^{\alpha}(\BZ_{t+1}) &  \text{else}.
    \end{cases}
\end{align}

The score is calculated per time step $t$ and miscoverage level $\alpha$.
It corresponds to the interval width $\text{IW}_t^{\alpha}=\Ch_t^{\alpha,u} - \Ch_t^{\alpha,l}$ whenever the observed value $y_t$ is between the upper bound $\Ch_t^{\alpha,u}(\BZ_{t+1})$ and the lower bound $\Ch_t^{\alpha,l}(\BZ_{t+1})$ of $\Ch_t^{\alpha}(\BZ_{t+1})$.
If $y_{t+1}$ is outside these bounds, a penalty is added to the interval width.
We evaluate the mean Winkler score over all time steps.

We repeated each experiment with 12 different seeds.
For brevity, we only show the mean performance of one dataset per domain for $\alpha = 0.1$ in the main paper \citep[which is the most commonly reported value in the CP literature; e.g.,][]{xu2022sequential, barber2022conformal, gibbs2021adaptive}.
Appendix~\ref{sec:appendix-full-tables} presents additional results for all datasets and more $\alpha$ levels.

\subsection{Results \& Discussion}\label{sec:results-discuss}


%
% Overall view
%
\ourmethod has the most efficient prediction intervals for each domain --- with only one exception (Table~\ref{tab:main-results-alpha-0.1}; significance tested with a Mann--Whitney $U$ test at $p<0.005$) for the evaluated miscoverage level ($\alpha = 0.1$). 
In multiple experiments (\solarThreeY, \solarOneY), the \ourmethod prediction intervals are less than half as wide as those of the approach with the second-smallest PI-Width.
The second-most efficient intervals are predicted most often by \spic.
This ranking also holds for the Winkler score, where \ourmethod achieves the best (i.e., lowest) Winkler scores and \spic ranks second in most experiments.
Notably, these results reflect the increasing requirements posed by each uncertainty estimation method on the dataset (Section~\ref{sec:related-work}).

Furthermore, \ourmethod outperforms the other methods regarding both Winkler score and PI-Width when we evaluate over additional datasets and at different miscoverage levels (see Appendix~\ref{sec:appendix-full-tables}).
\ourmethod is the best-performing approach in the  vast majority of cases.
The variation in size of the different solar datasets (results for \solarOneY and \solarSmall in Appendix~\ref{sec:appendix-full-tables}) shows that \ourmethod especially increases its lead when more data is available.
We hypothesize that this is because the MHN can learn more generalizable retrieval patterns with more data.
However, data scarcity is not a limitation of similarity-based CP in general.
In fact, a simplified and almost parameter-free variation of \ourmethod, which replaces the MHN with \knn retrieval, performs best on the smallest solar dataset, as we demonstrate in Appendix~\ref{sec:knn-comparison}.



Most approaches have a coverage gap close to zero in almost all experiments --- they approximately achieve the specified marginal coverage level.
This is also reflected by the fact that the ranking of Winkler scores and PI-Widths agree in most evaluations.
Appendix~\ref{sec:local-coverage} provides a supplementary analysis of the local coverage. 
The results for non-CP methods show a different picture (see Appendix ~\ref{sec:appendix-full-tables} and Table~\ref{tab:non-cp-comp-0}).
Here, the non-CP models most often exhibit very narrow prediction intervals at the cost of high miscoverage.

%
% More individual/detail view
%
Interestingly, standard CP also achieves good coverage for all experiments at the cost of inefficient prediction intervals.
There is one notable exception:
for the \sapflux dataset with ridge regression, we find $\Delta \ \text{Cov} = - 0.358$.
In this case, we argue that the bad performance of standard CP is driven by a strong violation of the (required) exchangeability assumption.
Specifically, a trend in the errors leads to a distribution shift over time (as illustrated in Appendix~\ref{sec:appendix-full-tables}, Figure~\ref{fig:eps-drift}).
\ourmethod and \nexcp handle this shift without substantial loss in coverage. 
The inferior coverage of \spic is likely influenced by the modification to larger datasets (see Appendix~\ref{sec:spic-retrained}).


Performance gaps between the approaches differ considerably across datasets, but are generally consistent across prediction models.
The biggest differences between the best and worst methods exist in \solarThreeY and \sapflux, likely due to the distinctive regimes in these datasets.
The smallest (but still significant) differences are visible in the \hydro data.  
On this dataset, we also evaluated the state-of-the-art non-CP uncertainty model in the domain \citep[][see Appendix~\ref{sec:appendix-full-tables}]{klotz2022uncertainty} and found that \ourmethod outperforms it with respect to efficiency.   

In terms of computational resources, \spic is most demanding followed by \ourmethod, as both methods require a learning step in contrast to the other methods.
A detailed overview can be found in Appendix~\ref{sec:comutatation-resources}.

\begin{figure*}[t]
\includegraphics[width=13cm]{graphics/association_new.pdf}
\centering
\caption{Exemplary visualization of the 30 highest association weights that the MHN places on previous time steps (some are before the visualized part of the memory). \ourmethod retrieves similar peak values when estimating at a peak, and it retrieves similar small values when estimating at a small value.}
\label{fig:association-viz}
\end{figure*}

%
% Qualitative analysis assocation
%
To assess whether \ourmethod learns meaningful associations within regimes, we conducted a qualitative study on the \solarThreeY dataset.
Figure~\ref{fig:association-viz} shows that \ourmethod retrieves the most highly weighted errors from time steps with similar regimes.
The illustrated weighting at the time step with a low prediction value retrieves previous time steps which are also in low-valued regimes.
Similarly, Figure~\ref{fig:association-eps-viz} (Appendix~\ref{sec:appendix-full-tables}) suggests that the learned distinction corresponds to the error regimes, which is crucial for \ourmethod.

\section{Conclusions}\label{sec:conclusion}

We have introduced \ourmethod, a novel CP approach for time series tasks.
\ourmethod uses continuous Modern Hopfield Networks to construct prediction intervals based on previously seen events with similar error distribution regimes.
We exploit that similar features lead to similar errors.
Associating features with errors identifies
 regimes with similar error distributions.
\ourmethod learns this association in the Modern Hopfield Network, 
which dynamically adjusts its focus on stored previous features 
according to the current regime.

Our experiments with established and novel datasets 
show that \ourmethod achieves state of the art:
It generates more efficient prediction intervals than existing CP methods and approximately preserves coverage, even in non-exchangeable scenarios
like time series.
\ourmethod comes with formal guarantees within the CP framework for uncertainty estimation in real-world applications such as streamflow prediction.
Furthermore, \ourmethod scales well to large datasets, provides multiple coverage levels after calibration, and shares information across individual time series within a dataset.
%We saw that \ourmethod effectively associates similar error regimes and performs well even when there are distribution shifts in the error distribution.

 %Future Work
Future work comprises:
(a)~Drawing from multiple time series during the inference phase.
\ourmethod is already trained on the whole dataset at once, but it might be advantageous to leverage more information during inference as well. 
(b)~Investigating direct training objectives from which learning-based CP might benefit.
(c)~Using \ourmethod beyond time series, as non-exchangeability is also an issue 
in other domains which limits the applicability of existing CP methods.

\input{acknowledgements}

% ------------------------------------
% \bibliography{}
\bibliography{literature, literature-cp}
\bibliographystyle{_icml2023/icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\section{Extended Experimental Setup \& Results}

\subsection{Hyperparameter Search}\label{sec:hyperparam}

We conducted an individual hyperparameter grid search for each predictor--dataset combination.
For methods that require calibration data (\ourmethod, \adaptiveci, \nexcp), each calibration set was split in half:
One part served as actual calibration data while the other half was used for validation.\footnote{Note that this is only the case in the hyperparameter search. 
In the evaluation, the full calibration set was used for calibration.}
As \enbpi requires only the $k$ past points, we used the full calibration  set minus these $k$ points for validation, so that it could fully exploit the available data.
Table~\ref{tab:hyperparameter-grid} shows all sets of hyperparameters used for the search.

\input{other_tables/hyperparameter_v2.tex}

\paragraph{Model selection.} Model selection was done uniformly for all algorithms:
As a first step, all models with a negative $\Delta \ \text{Cov}$ on the validation set were excluded.
From the remaining models, the model with the smallest PI-Width was selected.
In cases where no model achieved non-negative $\Delta \ \text{Cov}$, the model with the highest $\Delta \ \text{Cov}$ was selected.


\paragraph{\ourmethod.} 
\ourmethod was trained for 3,000 epochs in each experiment.
We chose this number to make sure that the loss and model selection metric curves are already converged.
Throughout training, we validated every 5 epochs and selected the model that best fulfilled the model selection criteria described above.
AdamW \citep{loshchilov2019adamw} with standard parameters ($\beta_1=0.9$, $\beta_2=0.999$, $\delta=0.01$) was used as optimizer.
The learning rate was part of the hyperparameter search.
Depending on the dataset size, the batch size was set to $2$ or $4$, where a batch size of $n$ would mean that the full training part of the calibration set of $n$ time series is used.

\newpage
\paragraph{\spic.}
The computational demand of \spic did not allow to conduct a full hyperparameter search for the window length parameter (see more in Section~\ref{sec:spic-retrained}).
Since \spic applies a random forest on the window, one can assume that it is capable to find the relevant parts of the window.
On top of that, a longer window has less risk than a window that is too short and (potentially) cuts off relevant information.
Hence, we set the window length to 100 for all experiments, which corresponds to the longest setting in the original paper.
To check our reasoning, we evaluated the performance of \spic with window length 25 on all but the largest two datasets (Table~\ref{tab:spic-length-comparsion-solar} and Table~\ref{tab:spic-length-comparsion-other}).
We found hardly any differences except for the smallest datasets \solarSmall.
In that case we reason that the result is due to the limited calibration data in this setting.

\paragraph{MCD/MDN/Gauss.}
In Appendix~\ref{sec:appendix-full-tables} we compare \ourmethod to non-CP architectures that can provide prediction interval estimates.
Specifically, we use Monte Carlo Dropout \citep[MCD;][]{gal2016dropout}, a Mixture Density Network \citep[MDN;][]{bishop1994mixture}, and a Gauss model (i.e., an MDN with a single Gaussian density).
An LSTM serves as backbone for all models.
Therefore, there is no distinction between the predictor model and the uncertainty model.
To allow for a fair comparison to CP approaches, the models are trained on both the predictor training data and the calibration data combined.
None of these approaches are CP methods, and MCD was specifically devised to estimate only epistemic uncertainty.
Nevertheless, we consider these methods as useful comparisons, since they are widely used for uncertainty quantification.
The models were trained for 150 epochs with AdamW \citep{loshchilov2019adamw} (using its standard parameters: $\beta_1=0.9$, $\beta_2=0.999$, $\delta=0.001$). We chose this number to ensure that the loss and model selection metrics already converged.
Throughout the training, we validated after every epoch and selected the model that best fulfilled the model selection criteria described above.
The learning rate was part of the hyperparameter search.

\paragraph{Global LSTM.}
We conducted an individual hyperparameter tuning of the global LSTM prediction model for each dataset.
Table~\ref{tab:hyperparameter-lstm} shows the hyperparameters used for the grid search.
Similar to the LSTM uncertainty models, we trained for 150 epochs with an AdamW optimizer \citep{loshchilov2019adamw} and validated after each epoch.
We optimized for the mean squared error and selected the model with the smallest validation mean squared error.

\input{other_tables/hyperparameter_lstm}


\setTableCaption{Performance of \spic with an input window  of length 100 and length 25 on the solar datasets.
The differences in performance are small compared to the differences across methods (Section~\ref{sec:results-discuss}). The error term represents the standard deviation over repeated runs.}
\input{results_tables_new/spic_length_comparsion_solar.tex}

\setTableCaption{Performance of \spic with an input window  of length 100 and length 25 on the different non-solar datasets.
The differences in performance are small compared to the differences across methods (Section~\ref{sec:results-discuss}). The error term represents the standard deviation over repeated runs.}
\input{results_tables_new/spic_length_comparsion_other.tex}



\subsection{\spic Retraining}\label{sec:spic-retrained}
% Notes that we do not retrain spic for the bigger datasets. Show that this is no problem on smaller datsets
As our results show, \spic is very competitive (see, for example, Section~\ref{sec:results-discuss}).
\citet{xu2022sequential} did, however, design \spic so that its random forest regressor is retrained after each prediction step.
This design choice is computationally prohibitive for larger datasets.
To nevertheless allow a comparison against \spic on the larger datasets, we modified the algorithm so that the retraining is skipped.
Experiments on the smallest dataset (Table~\ref{tab:spic-retrain-comparsion}) show only small performance decrease with the modified algorithm.
One limitation of the adapted algorithm is, however, that a strong shift in the error distribution(s) would potentially require a retraining on the new distribution.
A viable proxy to detect such a change in our setting is the coverage performance of standard CP. The reason for this is that standard CP predictions are solely based on the calibration data and can thus not account for shifts.
In the experiments (Section~\ref{sec:experiments} and Appendix~\ref{sec:appendix-full-tables}), standard CP achieves good coverage with only one exception (see Section~\ref{sec:experiments}).
Hence, we decided to include \spic (in the modified version) to enable a comparison to it on larger datasets. 


\setTableCaption{Performance of the original \spic algorithm (Retrain) and the modified version (No Retrain) on the \solarSmall dataset.
Performance in terms of the evaluation metrics is similar compared to the differences between methods (Section~\ref{sec:results-discuss}).
The computational demand of the modified version is considerably lower. The error term represents the standard deviation over repeated runs.}
\input{results_tables/spic_retrain_comparsion.tex}

\begin{figure}[t]
\includegraphics[width=10cm]{graphics/eps_drift.pdf}
\centering
\caption{Time series of the prediction error $\epsilon$ for the ridge regression model on the \sapflux dataset. The time series spans the calibration ($t < 0$) and test ($t \geq 0$) data.
The red line (fitted by the least squares method) shows a strong trend in the error distribution.}
\label{fig:eps-drift}
\end{figure}

\clearpage

\subsection{Additional Results} \label{sec:appendix-full-tables}
Tables~\ref{tab:nsdb-60m-2018-20-overall-v2}--\ref{tab:hydro-531-basin-list-overall-v2} show the results for miscoverage levels $\alpha \in \{0.05, 0.10, 0.15\}$ for all evaluated combinations of datasets and predictors.
Results for individual time series of the datasets are uploaded to the code repository (Appendix~\ref{sec:code}).

\paragraph{CMAL.} For the \hydro dataset, we additionally compare to CMAL, which is the state-of-the-art non-CP uncertainty estimation technique in the respective domain \citep{klotz2022uncertainty}.
CMAL is a mixture density network \citep{bishop1994mixture} based on an LSTM that predicts the parameters of asymmetric Laplacian distributions.
As our experiments use the same dataset, we adopt the hyperparameters from \citet{klotz2022uncertainty} but lower the learning rate to $0.0001$ because we train CMAL on more training samples (18 years, i.e., the training and calibration period combined, which allows for a fair comparison against the CP methods).
Despite the fact that CMAL is not based on the CP paradigm, it achieves good coverage results, however, at the cost of wide prediction intervals.
We argue that the Winkler score is low because CMAL considers all (and therefore also uncovered) samples during the optimization while CP methods do not consider them at all --- therefore, the distance of these points to the interval might be smaller (see Table~\ref{tab:hydro-531-basin-list-overall-v2}).


\begin{figure*}[t]
\includegraphics[width=\textwidth]{graphics/association-eps.pdf}
\centering
\caption{Visualization of the 30 highest association weights that the Hopfield network places on previous time steps. \ourmethod retrieves similar error values when predicting at a time of high error, and it retrieves similar, previous small errors when predicting at a time step with small error.}
\label{fig:association-eps-viz}
\end{figure*}

\paragraph{Error trend.} Figure~\ref{fig:eps-drift} shows the prediction errors of the ridge regression model on the \sapflux dataset. The errors exhibit a strong trend and shift towards a negative expectation value.

\paragraph{Association weights.} Figure~\ref{fig:association-eps-viz} investigates the association patterns of \ourmethod and shows its capabilities to focus on time steps from similar error regimes.
The depicted time step in a regime with negative errors retrieves time steps with primarily negative errors and, likewise, the time step in a regime with positive errors retrieves time steps with primarily positive errors of the same magnitude.

\paragraph{Non-CP methods}

We also compare \ourmethod to non-CP methods to evaluate its performance within the broader field of uncertainty estimation for time series.
Specifically, we compare against a Monte Carlo Dropout (MCD); a Mixture Density Network (MDN), and a Gauss model (i.e., an MDN with a single Gaussian component), with an LSTM backbone (see Appendix~\ref{sec:hyperparam} for more details about the models and their hyperparameters).
This comparison shows that, although the non-CP methods often lead to very narrow prediction intervals, they fail to provide approximate coverage on most experiments (Table~\ref{tab:non-cp-comp-0}).

\newpage

\subsection{Local Coverage}\label{sec:local-coverage}
Standard CP only provides marginal coverage guarantees and a constant prediction interval width \citep{vovk2005algorithmic}.
In time series prediction tasks, this can lead to bad local coverage \citep{Lei2014DistributionfreePB}.
To evaluate whether the coverage approximately holds also locally, we evaluated $\Delta \ \text{Cov}$ on windows of size $k$.
To avoid compensation of negative $\Delta \ \text{Cov}$ in some windows by other windows with positive $\Delta \ \text{Cov}$, we upper-bounded each window's $\Delta \ \text{Cov}$ by zero before averaging over the bounded coverage gaps --- i.e., we calculated $\frac{1}{W} \sum_{w=1}^W - \Delta \  \text{Cov}_w^{\top 0}$, with 
\begin{align}
     \Delta \  \text{Cov}_w^{\top 0} = \begin{cases}
     \Delta \ \text{Cov}_w \; & \text{if} \ \Delta \ \text{Cov}_w \leq 0, \\
        0 \; & \text{else},
    \end{cases}
\end{align}

where $ \Delta \ \text{Cov}_w$ is the $\Delta \ \text{Cov}$ within window $w$.

Table~\ref{tab:rolling-coverage-v2} shows the results of this evaluation for window sizes $k \in \{10, 20, 50\}$ and miscoverage level $\alpha=0.1$.
Depending on the dataset, most often either \ourmethod or \spic perform best.
The overall rankings are only partly similar to the evaluation of the marginal coverage (Table~\ref{tab:main-results-alpha-0.1} and Appendix~\ref{sec:appendix-full-tables}).
Especially standard CP, which achieves competitive results in marginal coverage, falls short in this comparison.
Only for \solarSmall, where the approach achieves a high marginal $\Delta \ \text{Cov}$, it preserves the local coverage best.
Note that this comes with the drawback of very wide prediction intervals, i.e., bad efficiency.
Overall, the results show that \ourmethod and other time series CP methods improve the local coverage in non-exchangeable time series settings, compared to standard CP.


\subsection{Negative Results}
We also tested training with more involved training procedures than directly using the mean squared error.
However, based on our empirical findings, the vanilla method with mean squared error outperforms more complex approaches. In the following, we briefly describe those additional configurations (so that potential future research can avoid these paths):

\begin{itemize}
    \item \textbf{Pinball Loss}. We tried to train the MHN with a pinball loss \citep[the original use seems to stem from][we are however not aware of the naming origin]{fox1964admissibility} in many different variations (see points that follow), but consistently got worse results than with the mean squared error.
    \begin{itemize}
        \item Inspired by the ideas lined out by \citet{tagasovska2019single} we tried to use the miscoverage level as an additional input to the network, while also parameterizing the loss with it. 
        \item We tried to use the pinball loss to predict multiple miscoverage values at the same time to get a more informed representation of the distribution we want to approximate. 
    \end{itemize}
    \item \textbf{Softmax Probabilities}. We tried to use the softmax output of the MHN to directly estimate probabilities and use a maximum likelihood procedure. This did train, but it did not produce good results.
\end{itemize}

\subsection{Computational Resources}\label{sec:comutatation-resources}

We used different hardware setups in our experiments, however, most of them were executed on a machine with an Nvidia P100 GPU and a Xeon E5-2698 CPU.
The runtime differs greatly between different dataset sizes. We report the approximate run times for a single experiment (i.e., one seed, all evaluated coverage levels, not including training the prediction model $\muh$):

\begin{enumerate}
    %\itemsep0em 
    \item \textbf{\ourmethod}: \solarThreeY 5h, \solarOneY 1h, \solarSmall 7min, \airTen 3h, \airTwenty 3h, \sapflux 2.5h, \hydro 12--20h
    \item  \textbf{\spic}: (adapted): \solarThreeY 5--10 days, \solarOneY 10--30h, \solarSmall 45min,  \airTen 13--30h, \airTwenty 13--30h, \sapflux 20--50h, \hydro 12--17 days
    \item \textbf{\enbpi}: all datasets under 6h
    \item \textbf{\nexcp}: all datasets under 45min
    \item \textbf{\adaptiveci}: all datasets under 45min
    \item \textbf{Standard CP}: all datasets under 45min
\end{enumerate}

\setTableCaption{Performance of the best two CP methods (\ourmethod and \spic) and state-of-the-art non-CP methods for the different datasets and levels of $\alpha$.
All approaches build upon an LSTM model.
Bold numbers correspond to the best result for the respective metric in the experiment (PI-Width and Winkler score), given that $\Delta \text{Cov} \ge - 0.25 \alpha$, i.e., the specific algorithm reached at least approximate coverage (the result is grayed out otherwise).
The error term represents the standard deviation over repeated runs with different seeds (results without an error term are from deterministic models).}

\input{results_tables_grayed/non_cp_comp_0}

\setTableCaption{Negative average of zero-upper-bounded $\Delta \ \text{Cov}$ of rolling windows with miscoverage level $\alpha=0.1$. We evaluate each dataset--predictor combination at the windows sizes $k \in \{10, 20, 50\}$.}
\input{results_tables/rolling_coverage.tex}

\newpage
\section{Theoretical Background} \label{sec:theory-appendix}
\input{theory_appendix_content.tex}

\newpage

\section{Dataset Details}\label{sec:dataset-details}
Datsets from four different domains were used in the experiments.
The following paragraphs summarize the details. 
Table~\ref{tab:dataset-summary} provides a quantitative overview.

\paragraph{Solar.} We conducted experiments on three solar radiation datasets: \solarSmall, \solarOneY, and \solarThreeY.
All three datasets are based on data from the US National Solar Radiation Database \citep[NSDB;][]{sengupta2018national}.
Besides solar radiation as the target variable, the datasets include 8 other environmental features.
\solarSmall is the same dataset as used by \citet{xu2021conformal, xu2022sequential} and focuses on data from 8 cities in California.
To show the scalability of \ourmethod we additionally generated two larger datasets \solarOneY and \solarThreeY, which include data from 50 cities from different parts of the US.
We selected the cities by ordering the areas provided by NSDB according to the population density and picked the top 50 under the condition that no city appears twice in the dataset.

\paragraph{Air quality.}
The datasets \airTen and \airTwenty are air quality measurements from 12 monitoring sites in Beijing, China \citep{zhang2017cautionary}.
The datasets provide two target variables (10PM and 2.5PM measurements) which we use separately in our experiments (the variables are used mutually exclusively in the datasets, e.g., when predicting 10PM we do not use 2.5PM as a feature). 
We encode the wind direction, given as a categorical variable, by mapping the north--south and the east--west components each to a scalar from $-1$ to $1$.

\paragraph{Sap flow.}
The \sapflux dataset is a subset of the Sapflux data project \citep{poyatos2021global}.
This dataset includes sap flow measurements which we use as a target variable, as well as a set of environmental variables.
The available features, the length of the measurements, and the sampling vary between the individual time series.
To get a set of comparable time series we processed the data as follows: (a) We removed all time series where not all of the 10 environmental features are available.
(b) If there was a single missing value between two existing ones, we filled the value with the value before (forward fill).
(c) We cut out all sequences without any missing target or feature values (after step b).
(d) From the resulting set of sequences we removed all that have less than 15,000 or more than 20,000 time steps.

\paragraph{Streamflow.}
For our experiments on the streamflow data, we use the Catchment Attributes and Meteorology for Large-sample Studies dataset \citep[CAMELS;][]{newman2015development,addor2017camels}.
It provides meteorological time series from different data products, corresponding streamflow measurements, and static catchment attributes for catchments across the continental United States.
We ran our experiments on the subset of 531 catchments that were used in previous hydrological benchmarking efforts \citep[e.g.,][]{newman2017benchmarking,kratzert2019regional,kratzert2020note,klotz2022uncertainty}.
Specifically, we trained the LSTM prediction model with the NeuralHydrology Python library \citep{kratzert2022nh} on precipitation, solar radiation, minimum and maximum temperature, and vapor pressure from the NLDAS \citep{xia2012continental}, Maurer \citep{maurer2002long}, and Daymet \citep{thornton1997generating} meteorological data products.
Further, the LSTM received 26 static catchment attributes at each time step that identify the catchment properties. Table~4 in \citet{kratzert2019regional} provides a full list of these attributes.
We trained the prediction model on data from the period Oct 1981 -- Sep 1990 (note that for some catchments, this period starts later due to missing data in the beginning), calibrated the uncertainty models on Oct 1990 -- Sep 1999, and tested them on the period Oct 1999 -- Sep 2008.


\input{other_tables/dataset_summary.tex}

\section{\knn vs.\ Learned Representation}\label{sec:knn-comparison}
% Ablation that learned representation is helpful (knn as baseline)
% To find a notion of similarity that reflects the distribution similarity of the error values, a learned representation is preferable over a simple \knn approach.
As mentioned in the main paper, we designed \ourmethod with large datasets in mind.
It is only in such a setting that the learned part of our approach can truly play to its strengths and take advantage of nuanced interrelationships in the data. 
\knn provides us with a natural ``fallback'' option for settings where not enough data is available to infer these relationships. 
Our comparisons in Table~\ref{tab:results-knn-solar} and Table~\ref{tab:results-knn-other} substantiate this argument: 
\knn provides competitive results for the small dataset \solarSmall (this can also be seen by contrasting the performance from Table~\ref{tab:results-knn-solar} to the respective results in Appendix~\ref{sec:appendix-full-tables}), but is outperformed by \ourmethod for the larger datasets.



\newpage

\section{Quantile of Sample vs.\ Quantile of Weighted ECDF}

\ourmethod constructs prediction intervals by calculating a quantile on the set of weight-sampled errors (see Equation~\ref{eq:myconfbounds}).
An alternative approach is to calculate the quantile over a weighted empirical CDF of the errors.
This approach would define $q(\tau, \BZ_{t+1})$ in Equation~\ref{eq:myconfbounds} as
\begin{equation}
    q(\tau, \BZ_{t+1}) = \quant_{\tau} \left(\sum_{i=1}^t a_{t+1, i} \delta_{\epsilon_{i}} \right),
\end{equation}

where $\delta_{\epsilon_{i}}$ is a point mass at $|\epsilon_i|$.
Empirically, we find little differences in the performance when comparing the two approaches (Tables~\ref{tab:results-cdf-solar} and~\ref{tab:results-cdf-other}).

\section{Asymmetrical Error Quantiles}
\ourmethod calculates the prediction interval by using equally large quantiles (in terms of distribution mass, not width) for the upper and lower bound (see Equation~\ref{eq:myconfbounds}).
\citet{xu2021conformal} instead search for a $\hat{\beta} \in [0,\alpha]$ and calculate the lower quantile with $\hat{\beta}$ and the upper quantile with $1-\alpha + \hat{\beta}$, which minimizes the overall interval with.
Applying this principle to Equation~\ref{eq:myconfbounds} one would arrive at
\begin{equation}\label{eq:myconfbounds-with-beta}
\begin{aligned}
\hat{\beta} = \argmin_{\beta \in [0,\alpha]} \Bigl[ & \muh(\BX_{t+1}) + q(\beta, \BZ_{t+1}), \muh(\BX_{t+1}) + q(1-\alpha + \beta, \BZ_{t+1}) \Bigr] \\
\Ch_t^{\alpha}(\BZ_{t+1}) = %
   \Bigl[ & \muh(\BX_{t+1}) + q(\hat{\beta}, \BZ_{t+1}), \muh(\BX_{t+1}) + q(1-\alpha + \hat{\beta}, \BZ_{t+1}) \Bigr],
\end{aligned}
\end{equation}
Practically, the search is done by evaluating different $\beta$ values within the potential value range and selecting the one which produces the lowest interval width. 
Theoretically, this could lead to more efficient intervals at the cost of additional computation and potential asymmetry in regard to uncovered samples that are higher and lower than the prediction interval.
Empirically, we do not find a notable improvement when applying this search to \ourmethod (see Table~\ref{tab:results-betasearch-main}).


\section{AdaptiveCI and \ourmethod}\label{sec:adaptive-cp}

As noted in Section~\ref{sec:experiments}, \adaptiveci is orthogonal to \ourmethod, \spic, \enbpi, and \nexcp.
We therefore also evaluated the combined application of the models.
Nevertheless, AdaptiveCI is an independent model on top of an existing model, which is reflected in the way we select the hyperparameters of the combined model:
First, the hyperparameters are selected for each model without adaption through AdaptiveCI (see Appendix~\ref{sec:hyperparam}) --- hence, we use the same hyperparameters as in the main evaluation.
Second, we conduct another hyperparameter search, given the model parameters from the first search, where we only search for the parameter of the adaptive component.

Tables~\ref{tab:results-adaptive-solar} and~\ref{tab:results-adaptive-other} show the results of these experiments.
Overall, the results are slightly better, as the Winkler score (which considers both width and coverage) slightly improves in  most experiments.
The ranking between the different models stays similar to the non-adaptive comparison (see Section~\ref{sec:results-discuss}) with \ourmethod performing best on all but the smallest dataset.



\section{Details on Continuous Modern Hopfield Networks} \label{sec:appendix-hopfield}
Modern Hopfield Networks are associative memory networks.
They learn to associate entries of a memory with a given ``query'' sample in order to produce a prediction.
In \ourmethod, the memory corresponds to representations of past time steps and the query corresponds to a representation of the current time step.
The association happens by means of weights on every memory entry, which are called ``association weights''.
Query--memory sample pairs that have similar representations are assigned high association weights.
In summary, the association weights indicate which samples are closely related --- in \ourmethod, this corresponds to finding the time steps which are part of the same time series regimes and therefore useful to estimate the current uncertainty.
The association mechanism is in that sense closely related to the attention mechanism of Transformers \citep{Vaswani:17}.
However, compared to other trainable networks, the memory-based architecture has the advantage that it does not need to encode all knowledge in network parameters but can dynamically adapt when given new memory entries. 

We adapt the following arguments from \citet{furst2021cloob} and \citet{Ramsauer:21} for a more formal overview.
Associative memory networks have been designed to store and retrieve samples. 
Hopfield networks are energy-based, binary associative memories, which were popularized as artificial neural network architectures in the 1980s \citep{Hopfield:82,Hopfield:84}.
Their storage capacity can be considerably increased by polynomial terms in the energy function \citep{Chen:86,Psaltis:86,Baldi:87,Gardner:87,Abbott:87,Horn:88,Caputo:02,Krotov:16}.
In contrast to these binary memory networks, we use continuous associative memory networks with far higher storage capacity. 
%Their main properties are that they retrieve stored patterns with only one update and that they have exponential storage capacity \citep{Ramsauer:21}. 
These networks are continuous and differentiable, retrieve with a single update, and have exponential storage capacity \citep[and are therefore scalable, i.e., able tackle large problems;][]{Ramsauer:21}.

Formally, we denote a set of patterns $\{\Bx_1,\ldots,\Bx_N\} \subset \dR^d$
that are stacked as columns to 
the matrix $\BX = \left( \Bx_1,\ldots,\Bx_N \right)$ and a 
state pattern (query) $\Bxi \in \dR^d$ that represents the current state. 
The largest norm of a stored pattern is
$M = \max_{i} \NRM{\Bx_i}$.
Then, the energy $\rE$ of continuous Modern Hopfield Networks with state $\Bxi$ is defined as \citep{Ramsauer:21}

\begin{equation}
\label{eq:energy}
\rE  \ =   \ -  \ \beta^{-1} \ \log \left( \sum_{i=1}^N
\exp(\beta \Bx_i^T \Bxi) \right) +  \ 
\frac{1}{2} \ \Bxi^T \Bxi  \ +  \ \rC,
\end{equation}

where $\rC=\beta^{-1} \log N  \ + \
\frac{1}{2} \ M^2$. For energy $\rE$ and state $\Bxi$, \citet{Ramsauer:21} proved that the update rule 
\begin{equation}
\label{eq:main_update}
\Bxi^{\nn} \ = \  \BX \ \soft ( \beta \BX^T \Bxi)
\end{equation}

converges globally to stationary points of the energy $\rE$ and coincides with the attention mechanisms of Transformers 
\citep{Vaswani:17,Ramsauer:21}.

The {\em separation} $\Delta_i$ of a pattern $\Bx_i$ is its minimal dot product difference to any of the other 
patterns:
\begin{equation}
    \Delta_i = \min_{j,j \not= i} \left( \Bx_i^T \Bx_i - \Bx_i^T \Bx_j \right).
\end{equation}
A pattern is {\em well-separated} from the data if $\Delta_i $ is above a given threshold \citep[specified in][]{Ramsauer:21}.
If the patterns $\Bx_i$ are well-separated, the update rule Equation~\ref{eq:main_update}
converges to a fixed point close to a stored pattern.
If some patterns are similar to one another and, therefore, not well-separated, 
the update rule converges to a fixed point close to the mean of the similar patterns. 

The update rule of a Hopfield network thus identifies sample--sample relations between stored patterns. This enables similarity-based learning methods like nearest neighbor search \citep[see][]{schafl2022hopular}, which \ourmethod leverages to learn a retrieval of samples from similar error regimes.

\section{Potential Social Impact}\label{sec:social-impact}
Reliable uncertainty estimates are crucial, especially for complex time-dependent environmental phenomena.
However, overreliance on these estimates can be dangerous.
For example, unseen regimes might not be properly predicted.
A changing climate that evolves the environment beyond already seen conditions can cause new forms of error regimes which cannot be predicted reliably.
As most machine learning approaches, our method requires accurately labeled training data.
Incorrect labels may lead to unexpected biases and prediction errors.

\section{Code and Data}\label{sec:code}
The code and data to reproduce all of our experiments are available at \repourl.

\newpage

\setTableCaption{Performance of \knn compared to \ourmethod for the miscoverage $\alpha = 0.10$ on the solar datasets. The error term represents the standard deviation over repeated runs.}
\input{results_tables_new/results_knn_solar.tex}

\setTableCaption{Performance of \knn compared to \ourmethod for the miscoverage $\alpha = 0.10$ on the different non-solar datasets. The error term represents the standard deviation over repeated runs.}
\input{results_tables_new/results_knn_other.tex}

\setTableCaption{Performance of the weighted sample quantile (Sample) and the weighted empirical CDF (ECDF) quantile strategies that \ourmethod uses for the miscoverage $\alpha = 0.10$ on the solar datasets. The error term represents the standard deviation over repeated runs.}
\input{results_tables_new/results_cdf_solar.tex}

\setTableCaption{Comparison of \ourmethod \ (a) with applying the approach proposed by \citep{xu2021conformal, xu2022sequential} to search for a $\beta$ that minimizes the prediction interval width (right column), and (b) without this approach which corresponds to $\beta = \alpha /2$ (left column). The experiments are done with a miscoverage rate $\alpha=0.1$. The error term represents the standard deviation over repeated runs.}
\input{other_tables/beta_search}

\setTableCaption{Performance of the weighted sample quantile (Sample) and the weighted empirical CDF (ECDF) quantile strategies that \ourmethod uses for the miscoverage $\alpha = 0.10$ on the different non-solar datasets. The error term represents the standard deviation over repeated runs.}
\input{results_tables_new/results_cdf_other.tex}

\setTableCaption{Performance of the CP algorithms \ourmethod, \spic, \enbpi, and \nexcp, each combined with \adaptiveci, for the miscoverage $\alpha = 0.10$ on the solar datasets. Bold numbers correspond to the best result for the respective metric in the experiment (PI-Width and Winkler score). The error term represents the standard deviation over repeated runs. \dag \xspace combined with \adaptiveci.}
\input{results_tables_new/results_adaptive_solar.tex}

\setTableCaption{Performance of the CP algorithms \ourmethod, \spic, \enbpi, and \nexcp, each combined with \adaptiveci, for the miscoverage $\alpha = 0.10$  on the different non-solar datasets. Bold numbers correspond to the best result for the respective metric in the experiment (PI-Width and Winkler score). The error term represents the standard deviation over repeated runs. \dag \xspace combined with \adaptiveci.}
\input{results_tables_new/results_adaptive_other.tex}


\setTableCaption{Performance of the evaluated CP algorithms on the \solarThreeY datasets for the miscoverage levels $\alpha \in \{0.05, 0.10, 0.15\}$. The column FC specifies the prediction algorithm used for the experiment (Forest: Random Forest, LGBM: LightGBM, Ridge: Ridge Regression, LSTM: LSTM neural network). Bold numbers correspond to the best result for the respective metric in the experiment (PI-Width and Winkler score), given that $\Delta \text{Cov} \ge - 0.25 \alpha$, i.e., the specific algorithm reached at least approximate coverage (the result is grayed out otherwise). The error term represents the standard deviation over repeated runs (results without an error term are from deterministic models).}
\input{results_tables_grayed/nsdb-60m-2018-20_overall_v2.tex}
\setTableCaption{Performance of the evaluated CP algorithms on the \solarOneY dataset for the miscoverage levels $\alpha \in \{0.05, 0.10, 0.15\}$. The column FC specifies the prediction algorithm used for the experiment (Forest: Random Forest, LGBM: LightGBM, Ridge: Ridge Regression, LSTM: LSTM neural network). Bold numbers correspond to the best result for the respective metric in the experiment (PI-Width and Winkler score), given that $\Delta \text{Cov} \ge - 0.25 \alpha$, i.e., the specific algorithm reached at least approximate coverage (the result is grayed out otherwise). The error term represents the standard deviation over repeated runs (results without an error term are from deterministic models).}
\input{results_tables_grayed/nsdb-60m-2019_overall_v2.tex}
\setTableCaption{Performance of the evaluated CP algorithms on the \solarSmall dataset for the miscoverage levels $\alpha \in \{0.05, 0.10, 0.15\}$. The column FC specifies the prediction algorithm used for the experiment (Forest: Random Forest, LGBM: LightGBM, Ridge: Ridge Regression). Bold numbers correspond to the best result for the respective metric in the experiment (PI-Width and Winkler score), given that $\Delta \text{Cov} \ge - 0.25 \alpha$, i.e., the specific algorithm reached at least approximate coverage (the result is grayed out otherwise). The error term represents the standard deviation over repeated runs (results without an error term are from deterministic models).}
\input{results_tables_grayed/solar_overall_v2.tex}
\setTableCaption{Performance of the evaluated CP algorithms on the \airTen dataset for the miscoverage levels $\alpha \in \{0.05, 0.10, 0.15\}$. The column FC specifies the prediction algorithm used for the experiment (Forest: Random Forest, LGBM: LightGBM, Ridge: Ridge Regression, LSTM: LSTM neural network). Bold numbers correspond to the best result for the respective metric in the experiment (PI-Width and Winkler score), given that $\Delta \text{Cov} \ge - 0.25 \alpha$, i.e., the specific algorithm reached at least approximate coverage (the result is grayed out otherwise). The error term represents the standard deviation over repeated runs (results without an error term are from deterministic models).}
\input{results_tables_grayed/air-10_overall_v2.tex}
\setTableCaption{Performance of the evaluated CP algorithms on the \airTwenty dataset for the miscoverage levels $\alpha \in \{0.05, 0.10, 0.15\}$. The column FC specifies the prediction algorithm used for the experiment (Forest: Random Forest, LGBM: LightGBM, Ridge: Ridge Regression, LSTM: LSTM neural network). Bold numbers correspond to the best result for the respective metric in the experiment (PI-Width and Winkler score), given that $\Delta \text{Cov} \ge - 0.25 \alpha$, i.e., the specific algorithm reached at least approximate coverage (the result is grayed out otherwise). The error term represents the standard deviation over repeated runs (results without an error term are from deterministic models).}
\input{results_tables_grayed/air-25_overall_v2.tex}
\setTableCaption{Performance of the evaluated CP algorithms on the \sapflux dataset for the miscoverage levels $\alpha \in \{0.05, 0.10, 0.15\}$. The column FC specifies the prediction algorithm used for the experiment (Forest: Random Forest, LGBM: LightGBM, Ridge: Ridge Regression, LSTM: LSTM neural network). Bold numbers correspond to the best result for the respective metric in the experiment (PI-Width and Winkler score), given that $\Delta \text{Cov} \ge - 0.25 \alpha$, i.e.\ the specific algorithm reached at least approximate coverage (the result is grayed out otherwise). The error term represents the standard deviation over repeated runs (results without an error term are from deterministic models).}
\input{results_tables_grayed/sapflux-solo3-large_overall_v2.tex}
\setTableCaption{Performance of the evaluated CP algorithms and the CMAL baseline (see Appendix~\ref{sec:appendix-full-tables}) on the \hydro dataset for the miscoverage levels $\alpha \in \{0.05, 0.10, 0.15\}$. Bold numbers correspond to the best result for the respective metric in the experiment (PI-Width and Winkler score), given that $\Delta \text{Cov} \ge - 0.25 \alpha$, i.e.\ the specific algorithm reached at least approximate coverage (the result is grayed out otherwise). The error term represents the standard deviation over repeated runs (results without an error term are from deterministic models).}
\input{results_tables_grayed/hydro-531_basin_list_overall_v2.tex}

\end{document}

