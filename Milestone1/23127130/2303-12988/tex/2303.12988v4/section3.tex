\documentclass{FeasibleOLG_main.tex}{subfiles}

\begin{document}


We are interested in the set of payoffs that can be achieved by a sequence of action profiles in which every generation of players plays the same sequence of action profiles. We call such action sequences and feasible payoffs \emph{stationary}. Stationary action sequences are employed as equilibrium paths for folk theorems \cite[]{Kandori_1992_RES, Smith_1992_GEB}. In this paper, we focus on stationary action sequences and feasible payoffs. So let us omit ``stationary'' whenever the meaning is clear. The main result in this section shall provide a complete characterization of (stationary) feasible payoffs for any $\delta$ and $T$.





Since we restrict our attention to stationary sequences, we focus on the payoffs of players in generation $d\geq 1$. For $a^{[nT]}=(a^1,a^2,\cdots ,a^{nT})\in A^{nT}$, we define the average discounted payoff $U_i(a^{[nT]})$ of player $i$ as follows:
\begin{equation}
\label{eq:nnnn1}
U_i(a^{[nT]}):=\frac{1}{\sum _{k=1}^{nT}\delta ^{k-1}}\left(\sum _{k=1}^{nT}\delta ^{k-1}u_i(a^{(i-1)T+k}) \right),	
\end{equation}
where $a^{s}=a^{s-nT}$ for $s\geq nT+1$. 



We define the \emph{(stationary) feasible payoff set} given $\delta$ and $T$ by 
$$
F(\delta,T):=co \left(\{ U(a^{[nT]}):a^{[nT]}\in A^{nT}\} \right).
$$
Since we are interested in stationary feasible payoffs, the definition would be the most permissible one as it allows any correlation over $nT$-length action sequences. 


	




It is convenient to introduce an additional notation. Given $a^{[n]}=(a^1,a^2,\cdots ,a^n)\in A^n$ and $i\in N$, we define the value $v_i(a^{[n]})$ as follows: 
\begin{align}
v_i(a^{[n]})&:=\frac{1}{\sum _{k=1}^n\delta ^{(k-1)T}}\left(\sum _{k=1}^n\delta ^{(k-1)T}u_i(a^{i+k-1}) \right), \label{eq:nnn1}
\end{align}
where $a^{s}=a^{s-n}$ for $s\geq n+1$. Let $v( a^{[n]}) \equiv (v_i (a^{[n]}))_{ i\in N}.$ That is, $v_i (a^{[n]})$ represents the average discounted payoff of player $i$ from repeatedly playing the same action profile $a^k$ during the $k$-th overlap:
$$U_i ( \underbrace{a^1, \dots, a^1}_{\text{$T$ times}}, \dots, \underbrace{a^n, \dots, a^n}_{\text{$T$ times}}) = v_i (a^1,  \dots, a^n).$$
We call such sequences of action profiles \emph{stable}.
Alternatively, $v_i ( a^{ [n]})$ can be interpreted as the average discounted payoff over $n$ periods, where the ``effective'' discount factor is $\delta^T$. 



\begin{them} 
\label{them:1}
For any $\delta \in (0,1]$ and $T \in \mathbb{N}$, 
\begin{equation}
\label{eq:nn1}
F(\delta,T) =co\left( \left \{v(a^{[n]}): {a^{[n]}\in A^n} \right \} \right).
\end{equation}


\end{them}



In words, the feasible payoff set is the convex hull of the average discounted payoffs of length-$n$ sequences of action profiles (i.e., stable paths), each of which is to be played $T$ times. Notice that we could interpret the average discounted payoff of such a sequence of action profiles as the average discounted payoff of length-$n$ sequence of action profiles, discounted by $\delta^T$. 

The characterization is useful since it substantially reduces the number of sequences of action profiles we need to consider: Regardless of $T$, it is sufficient to consider length-$n$ sequences. The result also means that $\delta$ and $T$ affect the feasible payoff set only through $\delta^T$. Thus, any $(\delta, T)$ and $(\delta', T')$ with $\delta^T ={\delta'}^{T'}$ would result in the same feasible payoff set. For instance, for the Prisoners' Dilemma game, where each player has two actions (so 4 action profiles), the result implies, it is sufficient to consider $4 \times 4 = 16$ sequences of action profiles to obtain the feasible payoff set, regardless of $\delta$ and $T$. In particular, this is true even when $T$ is a large number. 












One may wonder whether the RHS of \eqref{eq:nn1} suggests that the feasible payoffs may not be implemented under our assumption on the observability of PRDs. That is, we assumed that a result of a PRD is observed only by the contemporary generation, whereas for each $T$ period (i.e., overlap), there is a player to be replaced; so it might require a stronger informational assumption, for instance, the results of PRDs are also observed by future generations. In the proof, however, we construct a sequence of PRDs each of which is executed every overlap that generates the same average discounted payoff from a sequence of action profiles. 

The underlying idea of the proof is as follows: Within an overlap, no player retires so every player discounts their payoff at $t+1$ relatively more than the payoff at $t$ in the overlap by $\delta$, although they discount payoffs differently depending on their ``age.'' This allows us to generate the same average discounted payoff for a given sequence of action profiles for a given overlap using a PRD at the beginning of this overlap, where the probability of playing a certain action profile during this overlap is determined in a way to mimic the sum of discounted payoffs whenever this action profile is played in the original sequence of action profiles. On the other hand, across overlaps, some player should retire and for such a player the relative discounting between $t$ and $t+1$ is different, which makes a similar construction of a PRD between overlaps unavailable. 



\begin{proof}


We first show $ F(\delta,T) \subseteq co\left(\{v(a^{[n]}): {a^{[n]}\in A^n}\} \right) .$
Since $co\left(\{v(a^{[n]}): {a^{[n]}\in A^n}\} \right)$ is a convex set, it is sufficient to show that for an arbitrary sequence $a^{[nT]} = (a^1, \dots, a^{nT}) \in A^{nT}$, the average discounted payoff from this sequence can be obtained by a convex combination of elements in $\{v(a^{[n]}): {a^{[n]}\in A^n}\}$.  

To see this, we first claim that for each overlap $k=1, \dots n$, the average discounted payoff within the overlap can be achieved by playing constant action profiles resulting from a PRD.

In each $k$-th overlap, the following PRD is exercised at the beginning of the overlap: An action profile $a \in A$ is randomly drawn with probability $\alpha^k (a)$ defined by 
$$\alpha^k (a) := \sum_{t=(k-1)T+1}^{kT} \frac{\delta^{t-1 - (k-1)T}}{ 1+ \dots + \delta^{T-1}}\mathbf{1}_{ \{{a}^t = {a} \}}.$$
Clearly, $\sum_{ a \in A} \alpha^k ({a}) = 1$. The players are supposed to play the realized action profile $a$ consecutively until the end of the overlap. To see that this PRD generates the same average discounted payoff as that of $a^{[nT]}$ for each player during the overlap, note that the PRD is constructed in that way so that it mimics discounting. Namely, if $a^t = a$ in $a^{ [nT]}$ for some $t  \in \{ 1, 2, \dots, T\}$, then the probability of playing $(a,\dots, a)$ increases by $\frac{\delta^{t-1- (k-1)T}}{\sum_{t=1}^T \delta^{t-1} }$. This construction is possible because no player retires within the overlap so that every player discounts $t+1$-period payoff by $\delta$ times than $t$-period payoff. This is not the case as long as some player retires and is reborn (i.e., across overlaps). 

Lastly, we observe that the average discounted payoff of each player from this sequence of PRDs $(\alpha^1, \dots, \alpha^n)$ is a convex combination of $\{v(a^{[n]}):a^{[n]}\in A^n\}$, where the weight for each $(a^1, \dots, a^n ) \in A^n$ is $\alpha^1 (a^1)\times \cdots \times \alpha^n (a^n)$.

For the converse, observe that each element of $\{v(a^{[n]}):a^{[n]}\in A^n\}$ is in $F(\delta, T)$ and $F(\delta, T)$ is convex by definition. 
\end{proof}













\end{document}
