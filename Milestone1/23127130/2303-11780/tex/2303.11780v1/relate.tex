\section{Related Work}
\label{sec:relate}

\noindent \textbf{Sequential Recommendation}.
% The earliest research works on sequential recommendations use Markov chains to establish transfer relationships of items in user sequences to reflect the evolution of user interests~\cite{rendle2010mc, garcin2013personalized, he2016fusing}. 
The advancement of neural networks and deep learning techniques has led to an increasing number of researchers proposing neural models to harness the rich latent semantics embedded in user behavior sequences. For instance, Caser~\cite{caser} relies on convolutional neural networks (CNNs), while GRU4Rec~\cite{gru4rec} is based on recurrent neural networks (RNNs). The subsequent introduction of Transformer~\cite{vaswani2017attention} has inspired researchers to develop sequential models like and BERT4Rec~\cite{bert4rec}, which use attention mechanisms to capture pairwise relations between user-interacted items. Moreover, recent GNN-based sequential models like SR-GNN~\cite{srgnn}, MTD~\cite{mtd}, and SURGE~\cite{surge} benefit from the strong capability of GNNs to capture global connections. \\\vspace{-0.12in}

% With the development of neural networks and deep learning techniques, in recent years, more and more researchers propose neural models to exploit the rich latent semantics embedded in user behavior sequences. For example, Caser~\cite{caser} and GRU4Rec~\cite{gru4rec} is based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs), respectively. The later proposal of Transformer~\cite{vaswani2017attention} inspires researchers to build sequential models \eg SASRec~\cite{sasrec} and BERT4Rec~\cite{bert4rec} on attention mechanisms to capture pair-wise relations between user-interacted items. Further, benefiting from the powerful ability of GNNs to capture global connections, recently proposed GNN-based sequential models \eg SR-GNN~\cite{srgnn}, GCSAN~\cite{gcsan}, and SURGE~\cite{surge}.\\\vspace{-0.12in}

\noindent \textbf{Self-Supervised Learning in Recommendation}.
Recently, self-supervised learning has become popular in recommender system research. In collaborative filtering (CF), SGL~\cite{sgl} uses random data augmentation on user-item graphs and applies self-discrimination contrastive learning on user/item nodes. SSL4Rec~\cite{ssl} employs data augmentation on item features and introduces a contrastive pre-training objective to improve learned representations in the two-tower model.
% Self-supervised learning emerges as a popular technique in recent research on recommender systems. For collaborative filter (CF), SGL~\cite{sgl} introduces random data augmentation on user-item graphs and applies self-discrimination contrastive learning on user/item nodes. SSL~\cite{ssl} performs data augmentation on item features and introduces a contrastive pre-training objective to enhance the learned representations from the two-tower model. 
In knowledge-aware recommendation, KGCL~\cite{kgcl} develops a knowledge graph contrastive learning framework to aid denoising and integration between CF learning and knowledge graph encoding. For socially-aware recommendation, MHCN~\cite{mhcn} designs a graph infomax task to accommodate cascading semantic information from social graphs, enhancing user representation learning. In the field of sequential recommendation, CL4SRec~\cite{cl4srec} introduces sequential data augmentation into a contrastive learning task to derive more robust sequence representations. DuoRec~\cite{duorec} proposes a contrastive learning method based on sequence-level positive pairing to address the problem of representation degeneration in sequential recommenders. ICLRec~\cite{iclrec} conducts clustering and contrastive learning on user intents, and it enhances sequential recommendation by improving the representation of user interests.

% For knowledge-aware recommendation, KGCL~\cite{kgcl} designs a cross-view contrastive learning framework to facilitate the denoising and integration between CF learning and knowledge graph encoding. 

% For socially-aware recommendation, S$^2$-MHCN~\cite{mhcn} designs a hierarchical infomax task to fit the cascading semantic information brought by the social graph to enhance the representation learning. 

% As for the sequential recommendation task focused by this paper, S$^3$-Rec~\cite{s3rec} proposes a pre-training contrastive task on both sequence and feature level to boost the performance. CL4SRec~\cite{cl4srec} brings sequential data augmentation into a contrastive learning task to derive sequence representations that are more robust to random perturbations. DuoRec~\cite{duorec} recognizes the representation degeneration problem in general sequential recommenders and proposes a contrastive learning method based on sequence-level positive pairing to alleviate the problem. ICLRec~\cite{iclrec} conducts the contrastive goal from the user intent view to empower the base model in better capturing coarse-grained user interests. 
% However, none of these sequential SSL methods pay attention to the disentanglement of user's real interests and conformity, and derive insufficient performance.

% \noindent \textbf{Bias Issues in Recommender Systems}.