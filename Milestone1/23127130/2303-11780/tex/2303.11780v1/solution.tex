\vspace{-0.1in}
\section{Methodology}
\label{sec:solution}

The overall model architecture of our \model\ is shown in Figure~\ref{fig:arch}.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{material/model_arc_.pdf}
    \vspace{-0.2in}
    \caption{The overall framework of \model. $\mathcal{G}_c$ and $\mathcal{G}_t$ are built to encode the sequences from diversified views (left part). In addition, we generate reasonable interaction-level conformity weights $\omega$ from the rich structure of $\mathcal{G}_t$ (right part). The weights are restrained in normal distribution and empower the cross-view contrastive learning to be adaptive and aware of conformity.}
    \label{fig:arch}
    \vspace{-0.15in}
\end{figure*}

\subsection{Task Formulation}
\noindent \textbf{Notations}. We suppose a recommender with a set of users and items denoted by $\mathcal{U} (u\in \mathcal{U})$ and $\mathcal{V} (v\in \mathcal{V})$, respectively. For each user, his/her engaged subset of items in a temporal order is defined as $\boldsymbol{s}_u=\left(v_1, v_2, \cdots, v_T\right)$. Here, $T$ is the sequence length which varies by users, and indexed by $t$, \ie $1\leq t \leq T$. Following settings in~\cite{bert4rec,iclrec}, we conduct the padding operation over different item sequences ($\boldsymbol{s}_u\ \in \mathcal{S}$) to mitigate the variable length.\\\vspace{-0.12in}

\noindent \textbf{Task}. 
Our objective is to develop a personalized ranking function that takes into account the past item sequences of a user, and predicts the next item ($v_{T+1}$) that the user is most likely to adopt.

% Given the past item sequences, our goal is to learn a personalized ranking function over all candidate items and predict the next item (\ie $v_{T+1}$) that the user is likely to adopt at the future step.

\vspace{-0.1in}
\subsection{Sequential Pattern Encoding}
As of now, Transformer has emerged as the dominant method for encoding sequences, capable of mapping temporally-ordered tokens from different types of sequential data to latent representation space. Examples include textual data~\cite{devlin2018bert} and electronic health data~\cite{poulain2021transformer}. Our sequential pattern encoder is built upon the Transformer, inspired by the effectiveness of this approach in modeling item sequence in~\cite{bert4rec,wu2020sse,yuan2022multi}. This allows us to incorporate temporal context into embeddings, resulting in an effective representation of the user's sequential behavior.

% To date, Transformer has become the most prevalent sequence encoding solution to project temporally-ordered tokens into latent representation space from various sequential data, such as textual data~\cite{devlin2018bert}, electronic health data~\cite{poulain2021transformer}, and user behaviors~\cite{yang2022getnext}. Inspired by the effectiveness of Transformer in modeling item sequence in~\cite{bert4rec,wu2020sse,yuan2022multi}, our sequential pattern encoder is built upon the Transformer, to incorporate temporal context into embeddings.

We start by adding a positional embedding $\mathbf{p}_v$ to the initial item representation $\mathbf{v}_v$ using the operation $\mathbf{h}_v^0 = \mathbf{v}_v \oplus \mathbf{p}_v$, which serves as the input item embedding $\mathbf{h}_v^0$ for the first block of Transformer. We represent each user's item sequence with an embedding matrix $\mathbf{H}_u^0 \in \mathbb{R}^{T \times d}$, where $T$ is the length of the sequence and $d$ is the dimension of the item embedding. The embedding matrix corresponds to the padded item sequence $\boldsymbol{s}_u$ of the user. To capture the correlations between items, we apply a self-attention layer with multi-head ($N$) channels to the user's item embedding matrix:
% Specifically, we first inject the positional embedding $\mathbf{p}_v$ of item $i$ into the initialized item representation $\mathbf{v}_v$ with the operation $\mathbf{h}_v^0 = \mathbf{v}_v \oplus \mathbf{p}_v$ as the input item embedding $\mathbf{h}_v^0$ for the first block of Transformer. We associate each user with an embedding matrix $\mathbf{H}_u^0 \in \mathbb{R}^{T \times d}$ corresponding to the padded item sequence $\boldsymbol{s}_u$. To capture the item-wise correlations, a self-attention layer with multi-head ($N$) channels is applied to user's item embedding matrix:
\begin{align}
    \text{MH}\left({\textbf{H}_u^\ell}\right) &= \left(\text{head}_1 \mathbin\Vert \text{head}_2 \mathbin\Vert  \cdots \mathbin\Vert \text{head}_N\right)\mathbf{W}^D \\
	\text{head}_n &= \text{Attention}\left( \textbf{H}_u^\ell \mathbf{W}^Q_n, \textbf{H}_u^\ell \mathbf{W}^K_n,  \textbf{H}_u^\ell \mathbf{W}^V_n \right),
\end{align}
\noindent $\mathbf{W}^Q_n, \mathbf{W}^K_n, \mathbf{W}^V_n \in \mathbb{R}^{d \times d/N}$ represents the head-specific mapping matrices corresponding to the query, key, value dimension, respectively. $\mathbf{W}^D \in \mathbb{R}^{d \times d}$ is a learnable projection matrix, and $\textbf{H}_u^\ell$ is the embedding matrix of user $u$'s sequence $\boldsymbol{s}_u$ at the $\ell$-th block of Transformer. Here, the self-attention calculation is conducted as: $\text{Attention}\left( \mathbf{Q},\mathbf{K},\mathbf{V}  \right) = \text{softmax}\left( \frac{\mathbf{Q}\cdot \mathbf{K}^\trans}{\sqrt{d/N}} \right)\mathbf{V}$. $\frac{d}{N}$ is the scale factor.

% $\mathbf{W}^Q_n, \mathbf{W}^K_n, \mathbf{W}^V_n \in \mathbb{R}^{d \times d/N}$ represents the head-specific mapping matrices corresponding to the query, key, value dimension, respectively. $\mathbf{W}^D \in \mathbb{R}^{d \times d}$ is a learnable projection matrix. $\textbf{H}_u^\ell$ is the embedding matrix of user $u$'s sequence $\boldsymbol{s}_u$ at the $\ell$-th block of Transformer. Here, the self-attention calculation is conducted as: $\text{Attention}\left( \mathbf{Q},\mathbf{K},\mathbf{V}  \right) = \text{softmax}\left( \frac{\mathbf{Q}\cdot \mathbf{K}^\trans}{\sqrt{d/N}} \right)\mathbf{V}$. $\frac{d}{N}$ is the scale factor.

To inject non-linearity into the embedding generation, a point-wise feed-forward network (FFN) is used for representation transformation within the sequential pattern encoder, which is defined:
% To inject the non-linearity into the embedding generation, we equip our sequential pattern encoder with a point-wise feed-forward network for representation transformation, which is defined as:
\begin{align}
    \label{eq:transformer}
    \text{PFFN}\left(\mathbf{H}_u^{\ell}\right) &= [\text{FFN}\left(\mathbf{h}_1^{\ell}\right)^\trans, \cdots, \text{FFN}\left(\mathbf{h}_T^{\ell}\right)^\trans] \\
	\text{FFN}\left(\mathbf{x}\right) &= \text{GELU}\left(\mathbf{x}\mathbf{W}_1^{\ell} + \mathbf{b}_1^{\ell}\right)\mathbf{W}_2^{\ell}+\mathbf{b}_2^{\ell},
\end{align}
\noindent where $\mathbf{W}_1^{\ell}, \mathbf{W}_2^{\ell}, \mathbf{b}_1^{\ell}, \mathbf{b}_2^{\ell}$ are learnable model parameters as projection and bias terms. $\text{GELU}(\cdot)$ is the activation function.

\subsection{Unifying Sequential and CF Views}
In real-life applications, long-tail sequences with a limited number of items are prevalent in recommendation scenarios~\cite{liu2020long,cl4srec}. These sequences pose challenges to most existing solutions. In particular, short sequences with very few items can hardly provide sufficient contextual signals for neural sequence encoders. This issue affects various types of models, such as self-attention mechanisms~\cite{bert4rec, sasrec}, and graph neural networks~\cite{gcsan, srgnn, mbht, surge}. To tackle the challenge of short sequences with very few items in sequential recommenders, we propose to unify the sequential view of item transitions and the collaborative view of user-item interactions. This design aims to capture the implicit cross-sequence user dependencies, allowing user-wise knowledge transfer in sequential recommender systems. This aspect is largely overlooked in most current solutions.


% In real-life sequential recommenders, long-tail sequence with limited number of items is prevalent in recommendation scenario~\cite{liu2020long,cl4srec}, which poses challenges to most of existing solutions. In particular, short sequences with very few items can hardly provide sufficient contextual signals for neural sequence encoders, such as recurrent neural network~\cite{gru4rec, gru4rec2}, self-attention mechanisms~\cite{bert4rec, sasrec}, and graph neural networks~\cite{gcsan, srgnn, mbht, surge}. To tackle this challenge, we propose to unify the sequential view of item transitions and the collaborative view of user-item interactions. With such design, our model can capture the implicit cross-sequence user dependencies to allow the user-wise knowledge transfer in sequential recommender, which are largely overlooked in most current solutions~\cite{zhang2022enhancing}.

To achieve the goal of unifying the sequential view of item transitions and the collaborative view of user-item interactions, you can start by generating two graphs: \emph{item transition graph} $\mathcal{G}_t$ and \emph{item co-interaction graph} $\mathcal{G}_c$. To be specific, $\mathcal{G}_t$ and $\mathcal{G}_c$ over the item set $\mathcal{V}$ are constructed by following the instructions below:
% Towards this end, we first generate the \emph{item transition graph} $\mathcal{G}_t$ and \emph{item co-interaction graph} $\mathcal{G}_c$ corresponding to the sequential, collaborative views, respectively. To be specific, $\mathcal{G}_t$ and $\mathcal{G}_c$ over the item set $\mathcal{V}$ are constructed by following the instructions below:
\begin{itemize}[leftmargin=*]
\item \textbf{Item Transition Graph $\mathcal{G}_t$}. To capture the transitional relationships among items from the sequential pattern view, adjacent item pairs (\eg $v_{t-1}$, $v_{t}$) in each sequence $\boldsymbol{s}_u$ are connected with an edge in $\mathcal{G}_t$. Given the item sequences of all users $\mathcal{S} = \{ \boldsymbol{s}_1, \boldsymbol{s}_2, \cdots, \boldsymbol{s}_{|U|} \}$, the adjacency matrix $\mathbf{A}_{\mathcal{G}_t} \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|}$ representing the item correlations in graph $\mathcal{G}_t$ is generated by:
\begin{align}
\label{eq:gt}
    \mathbf{A}^u_{\mathcal{G}_t}(v_p, v_q) = \begin{cases}
                        1, & |p-q|=1 \\
                        0, & \text{otherwise}
    \end{cases} ;\quad
    \mathbf{A}_{\mathcal{G}_t} = \sum_{u=1}^{|U|}\mathbf{A}^u_{\mathcal{G}_t},
\end{align}
\noindent where $\mathbf{A}^u_{\mathcal{G}_t}$ denotes the user-specific item transition connections over sequence $\boldsymbol{s}_u$. Here, $p$ and $q$ denotes the position index in sequence. We sum up $\mathbf{A}^u_{\mathcal{G}_t}$ of all users ($u\in \mathcal{U}$) to obtain $\mathbf{A}_{\mathcal{G}_t}$. The adjacency matrix $\mathbf{A}_{\mathcal{G}t}$ takes into account the transition frequency between items with edge weights in the item transition graph. \\\vspace{-0.12in}

% Hence, the transition frequency between items are considered in $\mathbf{A}_{\mathcal{G}_t}$ to reflect the edge weights in the item transition graph $\mathcal{G}_t$.\\\vspace{-0.12in}

\item \textbf{Item Co-Interaction Graph $\mathcal{G}_c$}. To incorporate collaborative signals to model the cross-user dependencies, we generate another graph $\mathcal{G}_c$ to maintain the item correlations based on their co-interaction patterns. To this end, we firstly construct the interaction matrix $\mathbf{R} \in \mathbb{R}^{|\mathbf{U}| \times |\mathbf{V}|}$ between users and items by setting the entry $\mathbf{R}_{u, v}=1$ if user $u$ has adopted item $v$ and $\mathbf{R}_{u, v}=0$ otherwise. With the operation $\mathbf{A}_{\mathcal{G}_c} = \mathbf{R}^\trans\mathbf{R}$, we obtain the initial correlation strength between items in $\mathbf{A}_{\mathcal{G}_c}$ based on their co-interaction frequency. To filter out less-relevant item-wise connections, we apply \emph{top}-$k(\cdot)$ function to keep highly-relevant connections among items in $\mathbf{A}_{\mathcal{G}_c}$ based on top-$k$ co-interaction frequency of each item. Here, $k$ determines the density of $\mathbf{A}_{\mathcal{G}_c}$.

% In particular, there exists an edge between two items if they are adopted by the same user before. By constructing the interaction matrix $\mathbf{R} \in \mathbb{R}^{|\mathbf{U}| \times |\mathbf{V}|}$ between users and items 

\end{itemize}

After generating the item transition graph $\mathcal{G}_t$ and co-interaction graph $\mathcal{G}_c$, we utilize the graph neural network to project individual item into latent embedding space. Formally, our graph convolution-based message passing is presented as follows:
\begin{equation}
    \label{eq:gcn}
    \mathbf{X}^{(l+1)} = \left(\mathbf{D}_t^{-\frac{1}{2}} \mathbf{A}_{\mathcal{G}_t} \mathbf{D}_t^{-\frac{1}{2}}\right)\mathbf{X}^{(l)};\ 
    \mathbf{Z}^{(l+1)} = \left(\mathbf{D}_c^{-\frac{1}{2}} \mathbf{A}_{\mathcal{G}_c} \mathbf{D}_c^{-\frac{1}{2}}\right)\mathbf{Z}^{(l)}
\end{equation}
\noindent We let $\mathbf{X}^{(l)}$ and $\mathbf{Z}^{(l)}$ respectively denote the embedding matrix of items over the item transition graph ($\mathcal{G}_t$) and the co-interaction graph ($\mathcal{G}_c$) under the $l$-th graph layer. $\mathbf{D}_a$ and $\mathbf{D}_i$ are degree matrices used for graph normalizing. To simplify the model with lightweight GNN architecture, we remove the redundant transformation and activation operations during the message propagation.

\subsection{Adaptive Cross-View Contrastive Learning}
\label{sec:adaptive}
Building on the success of contrastive data augmentation across various domains, including vision learning~\cite{he2020momentum}, text mining~\cite{rethmeier2021primer}, and graph modeling~\cite{zhu2021graph}, our \model\ method harnesses self-supervised signals through contrastive learning across different item semantic views. Nonetheless, the popularity bias is often overlooked, as conformity can entangle real interests and subsequently influence user behaviors~\cite{zheng2021disentangling,chen2021autodebias}. For instance, a user might be influenced by conformity to click on a product or watch a short video, following the actions of others, rather than being genuinely interested in the content. If user interest and conformity are not disentangled when generating augmented signals, contrastive learning methods may focus on incorrect positive pairs, thereby introducing biased information. This can lead to less-interested recommendation.

% With the success of contrastive data augmentation in various domains, vision learning~\cite{he2020momentum}, text mining~\cite{rethmeier2021primer}, and graph modeling~\cite{zhu2021graph}, our \model\ method distills self-supervised signals with the contrastive learning across different item semantic views. However, the popularity bias is ignored when the conformity often entangles the real interests to influence user behaviors~\cite{zheng2021disentangling,chen2021autodebias}. For example, a user may follow others to click a product or view a short-video due to his/her conformity, rather than driven by the real interest. Without disentangling user interest and conformity in generating augmented signals, contrastive learning method may concentrate on the incorrect positive pairs and introduce biased information. 

Intuitively, conformity may vary across users and interactions. For example, user conformity and real interest might be entangled in a complex manner, jointly driving interaction behaviors. This complexity makes it challenging to accurately disentangle conformity from genuine interest, which is essential for providing more helpful augmented SSL signals. To address this challenge, we propose a debiased cross-view contrastive learning approach with adaptive augmentation that incorporates interaction-level conformity. We develop a multi-channel conformity weighting network (CWNet) to calculate the conformity degree of an interaction. By incorporating the estimated conformity degrees into our contrastive learning paradigm, we can adaptively determine the regularization strength. This allows the model to more effectively disentangle user interests from conformity behaviors.

% Intuitively, conformity may vary by users and interactions, \eg user conformity and real interest may be entangled in a complex way and jointly drive the interaction behaviors. To tackle this challenge, we propose a debiased cross-view contrastive learning approach with adaptive augmentation which incorporates the interaction-level conformity. To achieve this goal, we devise a multi-channel conformity weighting network (CWNet) to derive the conformity degree of an interaction. Then, the estimated conformity degrees are incorporated into our contrastive learning paradigm to determine the regularization strength in an adaptive manner.

\subsubsection{\bf Multi-Channel Conformity Weighting Network}
In our CWNet module, we aim to learn the conformity degree of an interaction between user $u$ and item $v$ from three semantic channels.\\\vspace{-0.12in}

\begin{itemize}[leftmargin=*]
\item (1) \textbf{ User-Specific Conformity Influence}. First, we propose to infer the interaction-level (\eg $u-v$) conformity degree by considering the conformity of user $u$ based on his/her past interactions. Given a user with strong conformity, their interactions are more likely to be influenced by popularity bias compared to others who exhibit strong individuality. To obtain the conformity degree of user $u$, we perturb the item transition graph $\mathcal{G}_t$ by removing the edges generated from $u$'s sequence $\boldsymbol{s}u$. This results in the generation of an augmented adjacency matrix $\bar{\mathbf{A}}{\mathcal{G}c}$, where $\bar{\mathbf{A}}^u{\mathcal{G}_t}(v_p, v_q) = 0$ for any two adjacent items $v_p$ and $v_q$ in $\boldsymbol{s}_u$. Subsequently, both the original and augmented item transition graphs are fed into our graph encoder (as per Eq.~\ref{eq:gcn}) to generate two embeddings ($\mathbf{x}_v, \mathbf{x}_v^\prime$) for the target item $v$. The user-specific conformity influence, denoted as $\omega^1_{\left(u,v\right)}$, is estimated using the cosine similarity between the two embeddings ($\mathbf{x}_v, \mathbf{x}v^\prime$), calculated as $\omega^{\alpha}{\left(u,v\right)} = \cos\left( \mathbf{x}_v, \mathbf{x}_v^\prime \right)$. A larger $\omega^u$ score indicates that user $u$'s interactions have little influence over the item graph structures, suggesting that their interaction patterns are more likely to be observed from others, \ie strong user conformity.  \\\vspace{-0.12in} 

\item (2) \textbf{Consistency with Other Users}. We also propose to calculate the conformity from the perspective of considering the sequential behavior consistency between the target user and others. In particular, for a given $u-v$ interaction, we compare the learned transitional patterns of user $u$ with those of other relevant users. To be specific, given the target item $v$, we aggregate the intra-sequence neighboring information using mean-pooling among inner neighbors within the sequence $\boldsymbol{s}_u$. The overall transitional patterns of other correlated users are combined to obtain $\overline{\mathbf{x}}_{O_v}$, which is derived from $v$'s outer neighbors $O_v$ across different user sequences. After that, the transition consistency is measured by $\omega^{\beta}_{\left(u,v\right)} = \cos\left(\overline{\mathbf{x}}_{N_v}, \overline{\mathbf{x}}_{O_v}\right)$. This measure quantifies the degree of consistency between the target user's sequential behavior and that of other users, providing insights into conformity. \\\vspace{-0.12in}

\item (3) \textbf{Subgraph Isomorphic Property}. The isomorphic property of item subgraph is also an important factor in reflecting user conformity with similar interaction patterns. To incorporate this factor into our conformity estimation, we calculate the similarity between item $v$'s embedding $\mathbf{x}_v$ and the representation $\overline{\mathbf{x}}_{O_v}$ aggregated from its outer neighbors, \ie $\omega^{\gamma}_{\left(u,v\right)} = \cos\left(\mathbf{x}_v, \overline{\mathbf{x}}_{O_v}\right)$.
\end{itemize}

\noindent \textbf{Mixing Signals from Different Channels.} We derive the final interaction-level conformity degree by fusing the information from the above three channels. Here, we first adopt mean-pooling over channel-specific results as: $\omega_{\left(u,v\right)} = \frac{1}{3}\sum_{\lambda \in \left\{\alpha,\beta,\gamma \right\}}\omega_{\left(u,v\right)}^{\lambda}$. Following the mapping strategy in~\cite{kgcl,zhu2021graph}, we perform the transformation for $\omega$ values as follows:
\begin{equation}
\label{eq:omega}
    \omega^{(1)} = \text{sigmoid}\left(\omega\right);\ 
    \omega^{(2)} = \frac{\omega^{(1)} - \omega_{min}^{(1)}}{\omega_{max}^{(1)} - \omega_{min}^{(1)}};\ 
    \omega^{(3)} = \frac{\mu_c}{\overline{\omega}^{(2)}} \cdot \omega^{(2)}
\end{equation}
\noindent $\mu_c$ is the hyperparameter that adjusts the mean value $\overline{\omega}$ of $\omega$. We omit the subscript $\left(u,v\right)$ for simplicity and adopt $\omega = \omega^{(3)}$ as the output conformity. Furthermore, to approximate the conformity degrees with normal distribution, we adopt the KL-divergence over the derived conformity results of all interactions:
\begin{equation}
    \label{eq:kl}
    \mathcal{L}_w = \sum_{i=1}^{|\{(u,v)\}|} \phi_i\log\frac{\phi_i}{\omega_i},
\end{equation}
\noindent where $\phi_i$ is generated by random sampling from normal distribution with the hyperparameter $\mu_c$ for the mean and $\sigma$ for the standard deviation. $\omega_i$ is the conformity weighting result.

\subsubsection{\bf Conformity-aware Contrastive Augmentation}
To enhance our \model\ with adaptively debiased augmentation, we integrate the conformity factor into our embedding contrasting paradigm to determine the agreement regularization strength. As discussed before, both sequential and collaborative views are generated through different encoders, \ie Transformer and GNNs. Our \model\ employs contrastive learning (CL) to learn conformity-aware augmented representations from two key dimensions:\\\vspace{-0.12in}

\noindent \textbf{Contrasting from User Dimension}. The first stage of our CL paradigm aims to realize the knowledge transfer across different users. By contrasting user-specific preferences with cross-user global interaction patterns, the learned augmented representations can naturally preserve user-wise implicit dependencies. In this process, the conformity regularizer weakens the impacts of perturbations caused by popularity bias for SSL augmentation. Given the embedding $\mathbf{h}_v$ and $\mathbf{x}_v$ encoded generated by our sequential pattern encoder (Eq.~\ref{eq:transformer}) and transition graph encoder (Eq.~\ref{eq:gcn}), respectively, our debiased contrastive learning paradigm is given as follows:
\begin{equation}
\label{eq:cl1}
    \mathcal{L}_u = -\sum_{u\in \mathcal{U}}\sum_{v \in \boldsymbol{s}_u}  \omega_{\left(u,v\right)} \log \frac{\exp\left(\cos\left(\mathbf{h}_v, \mathbf{x}_v\right)/\tau\right)}{\sum_{v^\prime\in \mathcal{V}} \exp\left(\cos\left(\mathbf{h}_v, \mathbf{x}_{v^\prime}\right)/\tau\right)},
\end{equation}
\noindent In the SSL loss $\mathcal{L}_u$, InfoNCE~\cite{infonce} is adopted for embedding contrasting. By incorporating our learned conformity factor $\omega$, we allow representations $\mathbf{h}_v$ and $\mathbf{x}_v$ to supervise each other adaptively, that is, weighted by the interaction-level conformity. \\\vspace{-0.12in}

% With the control of our learned conformity $\omega$, we let representations $\mathbf{h}_v$ and $\mathbf{x}_v$ supervise each other in an adaptive manner, \ie weighed by the interaction-level $\left(u,v\right)$ conformity context.\\\vspace{-0.12in}

\noindent \textbf{Contrasting from Item Dimension}. The goal of our second stage CL is to extract self-supervision signals by contrasting the global item embedding $\mathbf{x}_v$ with the item semantic representation $\mathbf{z}_v$. Our conformity factor $\omega_{u,v}$ is incorporated into this contrasting process by estimating the uniformity $\psi_{\left(u,v\right)}=1-\omega_{\left(u,v\right)}$. Formally, our item dimension CL loss $\mathcal{L}_v$ is defined as follows:
\begin{equation}
\label{eq:cl2}
    \mathcal{L}_v = -\sum_{u\in \mathcal{U}}\sum_{v \in \boldsymbol{s}_u}  \psi_{\left(u,v\right)} \log \frac{\exp\left(\cos\left(\mathbf{x}_v, \mathbf{z}_v\right)/\tau\right)}{\sum_{v^\prime\in \mathcal{V}} \exp\left(\cos\left(\mathbf{x}_v, \mathbf{z}_{v^\prime}\right)/\tau\right)},
\end{equation}
\noindent In our CL paradigm, instance self-discrimination~\cite{sgl,xia2022hypergraph} is used for generating positive pairs. Representation of different samples are pushed apart as negative pairs to reflect embedding uniformity.

\subsection{Model Training and Prediction}
In the training phase, the last interacted item of each sequence $\boldsymbol{s}_u$ is regarded as the label for model optimization. In the prediction phase, to encourage the cooperation between sequence and collaborative views, we combine view-specific item embeddings into an aggregated representation $\mathbf{p}_v$ with the learnable attentive weights:
\begin{align}
\label{eq:fusion}
    f\left(\mathbf{e}\right) = \frac{\exp\left(\mathbf{a}^\trans \cdot \mathbf{W}_a \mathbf{e}\right)}{\sum_{i=1}^3 \exp\left(\mathbf{a}^\trans \cdot \mathbf{W}_a \mathbf{e}\right)};~~~ \mathbf{p}_v = \sum_{\mathbf{e}\in \left\{\mathbf{h},\mathbf{x},\mathbf{z} \right\}}f\left(\mathbf{e}\right)\mathbf{e}
\end{align}
\noindent where $\mathbf{a}\in\mathbb{R}^{d}$ and $\mathbf{W}_a\in\mathbb{R}^{d\times d}$ are trainable attention parameters. Input embedding $\mathbf{e}$ is selected from the set of view-specific representations, \ie $\mathbf{e} \in \left\{\mathbf{h}_v, \mathbf{x}_v, \mathbf{z}_v \right\}$. $\mathbf{p}_v$ is derived through attentive aggregation with the view-specific importance score $f\left(\mathbf{e}\right)$.

The next item interaction probability $\hat{y}_{u,v}$ is derived as $\hat{y}_{u,v} = \mathbf{p}_{|\boldsymbol{s}_u|}^\trans\mathbf{v}$, where we adopt hidden states of the last item on the sequence as the user embedding. For each user and the ground truth item $v_t$ pair, we utilize the cross-entropy as the loss:
\begin{equation}
    \mathcal{L}_{rec} = \sum_{(u,v_{T+1})\in \mathcal{D}^+} -\log\frac{\exp \hat{y}_{u,v_{T+1}}}{\sum_{v^\prime \in \mathcal{V}}\exp\hat{y}_{u,v^\prime}},
\end{equation}
\noindent where $\mathcal{D}^+$ is the training data set of positive interactions at the $T+1$ timesteps.
To supplement the recommendation loss $\mathcal{L}_{rec}$ with our augmented SSL tasks under a multi-task training framework, we define our joint optimized objective $\mathcal{L}$ as:
\begin{equation}
\label{eq:all}
    \mathcal{L} = \mathcal{L}_{rec} + \lambda_1\left(\mathcal{L}_u+\mathcal{L}_v\right) + \lambda_2\left(\mathcal{L}_w\right),
\end{equation}
\noindent where $\lambda_1$ and $\lambda_2$ are parameters to balance the tasks-specific loss. $\mathcal{L}_w$ is the regularization term with KL-divergence for mixing signals (Eq.~\ref{eq:kl}) in our multi-channel conformity weighting network.\\\vspace{-0.12in}

\noindent \textbf{Time Complexity Analysis}. In our sequential pattern encoder, the computational cost is $O\left(T^2d+Td^2\right)$ where the majority of the cost  is attributed to the item-wise self-attention operations. In our GNN encoder, the graph convolutional message passing and aggregation have a complexity of $O\left(|\mathcal{V}|d^2\right)$. In the cross-view representation aggregation, our \model\ requires a computational cost of $O(d^2)$ for attentional weighting. Owing to the independent nature of our sequential and collaborative relation encoders, the Transformer and GNN encoding can be performed in parallel using the CUDA infrastructure for speeding up computation. In summary, the time complexity of our \model\ is $O\left(\left(|\mathcal{V}|+1\right)d^2\right)$, making it comparable to the state-of-the-art GNN-based sequential recommenders. 

% which is largely dominated by the item-wise self-attention operations~\cite{sasrec}. In our GNN encoder, the graph convolutional message passing and aggregation takes $O\left(|\mathcal{V}|d^2\right)$ complexity. For the cross-view representation aggregation, our \model\ needs $O(d^2)$ cost for attentional weighting. Due to the independent property of our sequential and collaborative relation encoders, Transformer and GNN encoding can be performed in a parallel way using CUDA infrastructure. Overall, the time complexity of our \model\ is $O\left(\left(|\mathcal{V}|+1\right)d^2\right)$, which is comparable to GNN-based state-of-the-art sequential recommenders.

\vspace{-0.1in}
\subsection{Theoretical Analyzes of \model}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{material/theo_case.pdf}
    \vspace{-0.2in}
    \caption{Upper part: curve of $0.5f(p)$ and $0.5f(n)$ under $\tau=0.4$. Lower part: distribution area of potential values of $\omega \cdot f(p)$ and $\omega \cdot f(n)$ and random samples within a batch.}
    \label{fig:theo}
    \vspace{-0.2in}
\end{figure}

In this section, we provide an analysis of how the new conformity-aware contrastive learning paradigm benefits the recommendation task. We focus on how to bring theoretical interpretability for the conformity-aware adaptive contrastive learning in Equation~\ref{eq:cl1}-\ref{eq:cl2}. We take Equation~\ref{eq:cl1} for studying because of the symmetry of these two equations. Following the discussion in~\cite{kgcl, sgl, khosla2020supervised}, the gradient of the contrastive objective in Equation~\ref{eq:cl1} can be expressed as:
\begin{equation}
\label{eq:contrib1}
    \nabla \mathcal{L}_u^{(u,v)} = \frac{1}{\tau \|\mathbf{h}_v\|}\left(c(v)+\sum_{v^\prime\in V\setminus\{p\}}c(v^\prime)\right),
\end{equation}
\noindent where $\mathcal{L}_u^{(u,v)}$ is the contrastive loss $\mathcal{L}_u$ for an user-item pair $(u,v)$. $c(v)$ and $c(v^\prime)$ are the gradient contribution from the positive pair $(\mathbf{h}_v,\mathbf{x}_v)$ and the negative pair, respectively. Formally, $c(v)$ and $c(v^\prime)$ are derived using the following formulas:
\begin{equation}
\label{eq:contrib2}
    \begin{aligned}
    c(v) &= \left(\mathbf{\bar{x}}_v - \left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_v\right)\mathbf{\bar{h}}_v\right)^\trans\left(P_{vv}-1\right)\\
    c(v^\prime) &= \left(\mathbf{\bar{x}}_{v^\prime} - \left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_{v^\prime}\right)\mathbf{\bar{h}}_v\right)^\trans P_{vv^\prime},
    \end{aligned}
\end{equation}
\noindent where $P_{vi} = \exp\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_i / \tau\right) \big/ \sum_{i\in V\setminus\{v\}}\exp\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_i / \tau \right)$. $\mathbf{\bar{x}}, \mathbf{\bar{h}}$ are normalized representations. To this end, we can derive two functions $f(p)$ and $f(n)$ that are proportional to the $L_2$ norm of $c(v)$ and $c(v^\prime)$~\cite{sgl}. Specifically, we have the following derivations:
\begin{equation}
\label{eq:contrib3}
    f_1(p) = \sqrt{1-p^2}\left(\exp\left(\frac{p}{\tau}\right)-1\right);\ f_2(n) = \sqrt{1-n^2}\left(\exp\frac{n}{\tau}\right)
\end{equation}
\noindent where $p = \mathbf{h}_v^\trans\mathbf{x}_v$ is the agreement between the positive pair. $n = \mathbf{h}_v^\trans\mathbf{x}_v^\prime$ denotes the similarity between the negatives. To visualize the impact of $\mathcal{L}_u$ without adaptive weight $\omega$, we plot the curve of $0.5f_1(p)$ and $0.5f_2(n)$ in Figure~\ref{fig:theo}. Note that without $\omega$, the coefficient of $\mathcal{L}_u$ is 0.5 by default. From the curves, it is obvious that the contribution of positive and negative samples at different similarity levels are fixed. This means that the model has difficulty in discriminating among diverse samples. In the context of an interest-driven interaction, it is crucial to dynamically reduce the influence of samples from the conformity modeling view.

% and $n = \mathbf{h}_v^\trans\mathbf{x}_v^\prime$ are the agreement between the positive pair and the similarity between the positive and the negative. To visualize the impact of $\mathcal{L}_u$ without adaptive weight $\omega$, we plot the curve of $0.5f_1(p)$ and $0.5f_2(n)$ in Figure~\ref{fig:theo}. Note that without $\omega$, the coefficient of $\mathcal{L}_u$ is 0.5 by default. From the curves, it is obvious that the contribution of positive and negative samples at different similarity levels are fixed. That is, the model lacks the ability to discriminate among diverse samples. Specifically, for an interest-driven interaction, impact of samples from the conformity modeling view should be dynamically weighted down. 

At this stage, we investigate the advantages of introducing a conformity-aware weight (denoted by $\omega$) in contrastive learning. Specifically, the conformity-aware weight $\omega$ influences the learning process by directly scaling the gradient values. Recall that the distribution of $\omega$ is restrained by normal distribution in Equation~\ref{eq:kl}. The distribution range of $\omega\cdot f_1(p)$ and $\omega\cdot f_2(n)$ creates an area rather than a single curve. We further plot the distribution areas in the lower part of Figure~\ref{fig:theo}. The values are weighted by the interaction-level conformity, falling within the ranges of $(0,f_1(p))$ and $(0,f_2(n))$ following a normal distribution. We also plot the discrete distribution of $\omega\cdot f_1(p)$ and $\omega\cdot f_2(n)$ by sampling two batches of training data. As evident from the distributions, the effect of some samples is enhanced while the influence of others are weakened. This endows the learning process with richer semantics, allowing for a dynamic and adaptive contribution of samples to the contrastive learning gradients with data debiasing. The analyzes also apply to $\mathcal{L}_v$ in Equation \ref{eq:cl2}, since $\gamma = 1-\omega$ has similar properties.

% To this stage, we study what benefits does the introduction of the conformity-aware weight $\omega$ brings. Specifically, $\omega$ weights the contribution to the gradient by directly scaling the gradient values. Recall that the distribution of $\omega$ is restrained by normal distribution in Equation~\ref{eq:kl}. The distribution range of $\omega\cdot f_1(p)$ and $\omega\cdot f_2(n)$ forms an area instead of a curve. We further plot the distribution areas in the lower part of Figure~\ref{fig:theo}. The values are weighted by the interaction-level conformity, range in $(0,f_1(p))$ and $(0,f_2(n))$ following normal distribution. We also plot the discrete distribution of $\omega\cdot f_1(p)$ and $\omega\cdot f_2(n)$ by sampling two batches of training data. Evidently, the effect of some samples is enhanced while some others are weakened. 
