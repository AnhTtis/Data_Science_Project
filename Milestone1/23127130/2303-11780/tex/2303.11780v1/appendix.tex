\appendix \section{Appendix}
% \balance
\label{sec:appendix}

\subsection{The Learning Process of \model}
The following section aims to provide additional details on the learning process of \model. In particular, we outline the algorithm steps in Algorithm \ref{algorithm}, which describe how \model\ propagates forward to compute loss in a batch training manner. Furthermore, we summarize the hyperparameters selected in our experiments that result in the different performance of \model. 

% Specifically, we report the values of the multi-task weights $\lambda_1$ and $\lambda_2$, as well as the training batch size corresponding to the best results in Table~\ref{tab:set}.

% In this section, we present additional details on the learning process of \model~to enhance reproducibility. Specifically, we provide the algorithm steps in Algorithm \ref{algorithm}~following which \model~propagates forward to compute loss in a batch training manner. In addition, we summarize the hyperparameter settings of the multi-task weights $\lambda_1,\lambda_2$ and training batch\_size under which the best performances of \model~are reported. We list the settings in Table~\ref{tab:set}.

% \vspace{-0.1in}
\begin{algorithm}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{The Learning Steps of \model}
\label{algorithm}
\Input{The item sequences of all users $\mathcal{S} = \{ \boldsymbol{s}_1, \boldsymbol{s}_2, \cdots, \boldsymbol{s}_{|U|} \}$, each temporal sequence defined as $\boldsymbol{s}_u=\left(v_1, v_2, \cdots, v_T\right)$.}
\Output{The overall training loss $\mathcal{L}$ to back propagate.}
\textbf{Build Graphs}\;
Build the adjacency matrix $\mathbf{A}_{\mathcal{G}_t} \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|}$ for item transition graph $\mathcal{G}_t$ as in Equation~\ref{eq:gt}\;
Build the adjacency matrix for item co-interaction graph as $\mathbf{A}_{\mathcal{G}_c} = \mathbf{R}^\trans\mathbf{R}$\;
\textbf{Sample a batch of users $u \in \mathcal{B}_u$}\;
Perform graph convolutional function on $\mathcal{G}_t$ and $\mathcal{G}_c$ as in Equation 6 to generate item embeddings $\mathbf{X}$ and $\mathbf{Z}$ reflecting of transitional and co-interaction patterns respectively\;
Encode user sequence $\boldsymbol{s}_u$ to $\mathbf{H}_u$ from the sequential pattern following Equation 1-4\;
Mask user-interacted items to derive the augmented transition graph $\bar{\mathbf{A}}_{\mathcal{G}_c}$\;
Generate interaction-level conformity weights $\omega_{(u,v)}$ as in Equation~\ref{eq:omega}\;
Compute $\mathcal{L}_w$ for constraining the distribution of $\omega$ as in Equation~\ref{eq:kl}\;
\textbf{Conformity-aware Contrastive Learning}\;
Perform contrastive learning between $\mathbf{H}$ and $\mathbf{X}$, weighted by $\omega$ as in Equation~\ref{eq:cl1}. Returns loss $\mathcal{L}_u$\;
Perform contrastive learning between $\mathbf{X}$ and $\mathbf{Z}$, weighted by $\psi_{\left(u,v\right)}=1-\omega_{\left(u,v\right)}$ as in Equation~\ref{eq:cl2}. Returns loss $\mathcal{L}_v$\;
\textbf{View Aggregation and Training}\;
Fuse the three views $\mathbf{H},\mathbf{X},\mathbf{Z}$ to obtain the final item representations $\mathbf{p}_v$ as in Equation 11.
Calculate the recommendation loss $\mathcal{L}_{rec}$ following Equation 12\;
Unify the overall loss by multi-task training: $\mathcal{L} = \mathcal{L}_{rec} + \lambda_1\left(\mathcal{L}_u+\mathcal{L}_v\right) + \lambda_2\left(\mathcal{L}_w\right)$\;
\Return{$\mathcal{L}$}\;
\end{algorithm}

% \begin{table}[h]
%     \centering
%     \caption{Best settings of hyperparameters $\lambda_1, \lambda_2$ and batch\_size for four datasets.}
%     \vspace{-0.15in}
% 	\resizebox{\linewidth}{!}{
%     \begin{tabular}{lcccc}
%     \toprule
%     Statistics & Reddit & Beauty & Sports & Movielens-20M\\
%     \midrule
%     $\lambda_1$ & 1.0 & 0.1 & 0.1 & 0.1 \\
%     $\lambda_2$ & 1.0 & 0.01 & 0.01 & 0.01\\
%     batch\_size & 256 & 256 & 64 & 512\\
%     \bottomrule
%     \end{tabular}
%     }
%     \label{tab:set}
%     \vspace{-0.1in}
% \end{table}

\subsection{Supplementary Experiments}
\subsubsection{\rm \textbf{Hyperparameter Sensitivity}}
We investigate the sensitivity of \model's performance with respect to different settings of key hyperparameters, including the top co-interaction size $k$ in $\mathcal{G}_c$, the mean of conformity weights $\mu_c$, and the temperature $\tau$ for contrastive learning. We conducted experiments on the four datasets by adjusting one hyperparameter within a specific range at a time, while keeping all others fixed. The evaluation results of our parameter study are presented in Figure~\ref{fig:hp}.

% Here we study how sensitive performances of \model~are given different settings of key hyperparameters: the top co-interaction size $k$ in $\mathcal{G}_c$, the mean of conformity weights $\mu_c$, and the temperature $\tau$ for contrastive learning. We conduct the experiments on the four datasets by adjusting one hyperparameter within a specific range at a time, and all others fixed. The results are plotted in Figure~\ref{fig:hp}. 

Based on the results, we summarize the following observations: (i) The performances generally first increase and then decrease as the top co-interaction size $k$ ranges from 2 to 10, with the best performance achieved all at 4. Increasing the top co-interaction size $k$ can bring more useful collaborative signals that boost performance. However, as $k$ continues to increase, it may introduce more noisy signals that are less relevant, causing the performance to decrease. (ii) We observed that the model's performance is sensitive to the $\mu_c$ hyperparameter, and that the best values are 0.4 or 0.5 across the four datasets. Since $\mu_c$ characterizes the average conformity degree across all users, we recommend adjusting it carefully for different datasets to match the specific user distribution. (iii) The results suggest that the best settings of $\tau$ are reported closer to 1.0. This observation is consistent with findings in previous work such as~\cite{duorec, iclrec}. A lower $\tau$ indicates a more differentiated contribution of common and hard samples~\cite{kgcl,sgl,khosla2020supervised}. We speculate that it is more difficult to obtain accurate hard negatives in sequential recommendation, which may explain why a higher $\tau$ reduces the negative impact of noisy samples for contrastive learning.


% We summarize as follows based on the results: i) as $k$ ranges from 2 to 10, the performances generally first increase before then decrease, achieving the best at 4 to 8. The reason is probably that increasing $k$ brings more useful collaborative signals to boost the performance, but as the increment continues, it may introduce noisy signals that are less relevant. ii) from the performances curve w.r.t. $\mu_c$, we observe that the model performances are more sensitive to this hyperparameter and it shows divergent best values on the four datasets. As $\mu_c$ characterizes the average conformity degree across all users, we recommend to adjust $\mu_c$ carefully for different datasets to match the specific user distribution. iii) different from contrastive learning in CF recommenders, the best settings of $\tau$ are reported closer to 1.0. This is consistent with the findings in~\cite{duorec, iclrec}. Since a lower $\tau$ indicates a more differentiated contribution of common and hard samples~\cite{kgcl,sgl,khosla2020supervised}, we speculate that it is more difficult to obtain accurate hard negatives in sequential recommendation, so a higher $\tau$ would reduce the negative impact of noisy samples.

\label{sec:a:hp}
\begin{figure*}[h]
\centering
\subfigure[Top co-interaction size $k$]{
\label{fig:hp:sasrec}
\includegraphics[width=0.29\linewidth]{material/HP_sim_group.pdf}}
\subfigure[Mean of conformity $\mu_c$]{
\label{fig:hp:cl4srec}
\includegraphics[width=0.29\linewidth]{material/HP_weight_mean.pdf}}
\subfigure[CL temperature $\tau$]{
\label{fig:hp:duorec}
\includegraphics[width=0.29\linewidth]{material/HP_cl_temp.pdf}}
\vspace{-0.1in}
\caption{Hyperparameter study for \model~in terms of performance change with HR@1.}
\label{fig:hp}
\vspace{-0.1in}
\end{figure*}

\subsubsection{\rm \textbf{Quality of Learned Item Embedding}}
\label{sec:a:emb}
In this section, we demonstrate the superiority of the item embeddings learned by our proposed \model. We visualize the item embedding distribution learned by the several sequential baselines through 2-D KDE graphs. Using t-SNE and Gaussian kernel density estimation, we plot the embedding distribution of all items in the Beauty dataset, as shown in Figure~\ref{fig:emb}. The results reveal that, compared to other baselines, the item embeddings learned by \model~are more evenly and uniformly distributed. This even distribution provides better discrimination for user interests and item semantics.

% We visualize the learned item embedding distribution by strongest sequential baselines as 2-d KDE graphs. By applying t-SNE and Gaussian kernel density estimation, we plot the embedding distribution of all items in Beauty dataset, as shown in Figure~\ref{fig:emb}. From the results, we can notice that compared to other baselines, the item embeddings learned by \model~are distributed more evenly and uniformly, which brings better discrimination for user interests and item semantics.

\begin{figure}[]
\centering
\subfigure[SASRec]{
\label{fig:emb:sasrec}
\includegraphics[width=0.38\linewidth]{material/beauty_embs_sasrec.pdf}}
\subfigure[CL4SRec]{
\label{fig:emb:cl4srec}
\includegraphics[width=0.38\linewidth]{material/beauty_embs_cl4srec.pdf}}
\subfigure[DuoRec]{
\label{fig:emb:duorec}
\includegraphics[width=0.38\linewidth]{material/beauty_embs_duorec.pdf}}
\subfigure[CLICD]{
\label{fig:emb:clicd}
\includegraphics[width=0.38\linewidth]{material/beauty_embs_clicd.pdf}}
\vspace{-0.1in}
\caption{Item embedding visualization on Beauty dataset.}
\label{fig:emb}
\end{figure}

\subsection{Formula Derivation Details}
In this section, we present the derivation of Equations \ref{eq:contrib1}-\ref{eq:contrib2}, which provide a measure of the contribution of positive and negative samples to the model learning. We begin by presenting the contrastive learning (CL) objective expressed in normalized vectors at the single interaction level:
% In this section, we derive Equation \ref{eq:contrib1}-\ref{eq:contrib2} which measures the contribution of positive and negative samples to the model learning. We first give the CL objective expressed in normalized vectors at the single interaction level:
\begin{equation}
    \mathcal{L}_u^{(u,v)} = - \omega_{\left(u,v\right)} \log \frac{\exp\left(\cos\left(\mathbf{\bar{h}}_v, \mathbf{\bar{x}}_v\right)/\tau\right)}{\sum_{v^\prime\in \mathcal{V}} \exp\left(\cos\left(\mathbf{\bar{h}}_v, \mathbf{\bar{x}}_{v^\prime}\right)/\tau\right)},
\end{equation}
\noindent where $\mathbf{\bar{h}}, \mathbf{\bar{x}}$ are normalized representations from two contrastive views. The gradient in terms of $\mathbf{h}_v$ can be expressed as:
\begin{equation}
    \nabla \mathcal{L}_u^{(u,v)} = \frac{\partial\mathcal{L}_u^{(u,v)}}{\partial\mathbf{\bar{h}}_v} \cdot \frac{\partial\mathbf{\bar{h}}_v}{\partial\mathbf{h}_v}.
\end{equation}
\noindent For the left term, we have:
\begin{equation}
    \begin{aligned}
    &\frac{\partial\mathcal{L}_u^{(u,v)}}{\partial\mathbf{\bar{h}}_v} = -\frac{\partial}{\partial\mathbf{\bar{h}}_v}\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_v\right) + \frac{\partial}{\partial\mathbf{\bar{h}}_v}\log\sum_{v^\prime\in\mathcal{V}}\exp\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_v{^\prime}\right)\\
    &= \frac{1}{\tau}\left(\frac{\sum_{v^\prime\in\mathcal{V}}\mathbf{\bar{x}}_{v^\prime}^\trans\exp\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_v{^\prime}/\tau\right)}{\sum_{v^\prime\in\mathcal{V}}\exp\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_v{^\prime}/\tau\right)}-\mathbf{\bar{x}}_v^\trans\right)\\
    &= \frac{1}{\tau}\left(\mathbf{\bar{x}}_v\frac{\exp\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_v/\tau\right)}{\sum_{v^\prime\in\mathcal{V}}\exp\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_{v^\prime}/\tau\right)} - \mathbf{\bar{x}}_v + \sum_{i\in V\setminus\{v\}} \mathbf{\bar{x}}_i^\trans \frac{\exp\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_i/\tau\right)}{\sum_{v^\prime\in\mathcal{V}} \exp\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_{v^\prime}/\tau\right) } \right)
    \end{aligned}
\end{equation}

Let
\begin{equation}
    P_{vi} = \frac{\exp\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_i / \tau\right) }{ \sum_{i\in V\setminus\{v\}}\exp\left(\mathbf{\bar{h}}_v^\trans\mathbf{\bar{x}}_i / \tau \right) }
\end{equation}

We further derive Equation 19 as:
\begin{equation}
    \frac{\partial\mathcal{L}_u^{(u,v)}}{\partial\mathbf{\bar{h}}_v} = \frac{1}{\tau}\left(\mathbf{\bar{x}}_v^\trans\left(P_{vv}-1\right) + \sum_{i\in V\setminus\{v\}} \mathbf{\bar{h}}_i^\trans P_{vi} \right)
\end{equation}

For the right term, we have
\begin{equation}
\begin{aligned}
        \frac{\partial\mathbf{\bar{h}}_v}{\partial\mathbf{h}_v} &= \frac{\partial}{\partial\mathbf{h}_v}\left(\frac{\mathbf{h}_v}{\|\mathbf{h}_v\|}\right)\\
        &= \frac{1}{\|\mathbf{h}_v\|}\mathbf{I} + \mathbf{h}_v\left( \frac{\partial\frac{1}{\|\mathbf{h}_v\|}}{\partial\mathbf{h}_v} \right)\\
        &= \frac{1}{\|\mathbf{h}_v\|}\left(\mathbf{I}-\mathbf{\bar{h}}_v^\trans\mathbf{\bar{h}}_v\right)
\end{aligned}
\end{equation}
where $\mathbf{I}$ is the unit matrix.To this end, we have
\begin{equation}
\begin{aligned}
        \nabla \mathcal{L}_u^{(u,v)} &= \frac{1}{\tau\|\mathbf{\bar{h}}_v\|}\left(\mathbf{I}-\mathbf{\bar{h}}_v^\trans\mathbf{\bar{h}}_v\right)\left(\mathbf{\bar{x}}_v^\trans\left(P_{vv}-1\right) + \sum_{i\in V\setminus\{v\}} \mathbf{\bar{h}}_i^\trans P_{vi} \right)\\
        &=\frac{1}{\tau \|\mathbf{h}_v\|}\left(c(v)+\sum_{v^\prime\in V\setminus\{p\}}c(v^\prime)\right)
\end{aligned}
\end{equation}
which corresponds to Equation~\ref{eq:contrib1}-\ref{eq:contrib2}.