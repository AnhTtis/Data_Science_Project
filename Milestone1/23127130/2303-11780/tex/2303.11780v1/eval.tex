\section{Evaluation}
\label{sec:eval}

\begin{table}[t]
    \centering
    \caption{Detailed statistics of experimental datasets}
    \vspace{-0.15in}
	\resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    Statistics & Reddit & Beauty & Sports & Movielens-20M\\
    \midrule
    \# Users & 14,487 & 56,849 & 85,227 & 96,727 \\
    \# Items & 15,417 & 41,533 & 56,975 & 10,155\\
    \# Interactions & 28,972 & 113,696 & 170,452 & 193,452\\
    \# Avg. Length & 20.95 & 3.67 & 3.70 & 18.20\\
    \# Density & 1e-4 & 5e-5 & 4e-5 & 2e-4\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:dataset}
    \vspace{-0.2in}
\end{table}

\begin{table*}[h]
    \centering
    \caption{Overall performance evaluation across all methods. The best and second best performance are denoted in bold and underline separately. $^\ast$ indicates that the best performance is statistically significant at $p<0.01$ compared to the second best.}
    \vspace{-0.15in}
    \label{tab:results}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{clccccccccccccr}
    \toprule
    Dataset & Metric & Caser & GRU4Rec & SASRec & BERT4Rec & SR-GNN & GCSAN & SURGE & S$^3$-Rec & CL4SRec & DuoRec & ICLRec & \textbf{\model} & \textit{\#Improve}\\
    \midrule
    \multirow{5}{*}{Reddit} & HR@1 & 0.0842 & 0.0858 & 0.0989 & 0.1058 & 0.1741 & 0.1405 & 0.1782 & 0.0187 & 0.1742 & \underline{0.1747} & 0.0532 & \textbf{0.1883}$^\ast$ & 7.78\%\\
    ~ & HR@5 & 0.1410 & 0.1449 & 0.2438 & 0.2566 & \textbf{0.3984} & 0.3037 & 0.3895 & 0.0608 & 0.3762 & 0.3792 & 0.0919 & \underline{0.3899} & -2.18\%\\
    ~ & HR@10 & 0.2121 & 0.2241 & 0.3445 & 0.3820 & \textbf{0.5130} & 0.4154 & 0.4939 & 0.1100 & 0.4925 & 0.4950 & 0.1499 & \underline{0.5057} & -1.44\%\\
    ~ & NDCG@5 & 0.1124 & 0.1145 & 0.1725 & 0.1825 & \underline{0.2903} & 0.2248 & 0.2935 & 0.0396 & 0.2796 & 0.2818 & 0.0725 & \textbf{0.3007}$^\ast$ & 3.58\% \\
    ~ & NDCG@10 & 0.1351 & 0.1398 & 0.2049 & 0.2225 & \underline{0.3273} & 0.2607 & 0.3238 & 0.0533 & 0.3170 & 0.3191 & 0.0909 & \textbf{0.3358}$^\ast$ & 2.60\%\\
    \midrule
    \multirow{5}{*}{Beauty} & HR@1 & 0.0251 & 0.0472 & 0.0831 & 0.0924 & 0.0812 & 0.0982 & 0.0753 & 0.0164 & 0.1218 & \underline{0.1265} & 0.1001 & \textbf{0.1359}$^\ast$ & 7.43\%\\ 
    ~ & HR@5 & 0.0858 & 0.1195 & 0.1569 & 0.2062 & 0.1780 & 0.1956 & 0.1845 & 0.0525 & 0.2329 & \underline{0.2359} & 0.2000 & \textbf{0.2511}$^\ast$ & 6.44\%\\
    ~ & HR@10 & 0.1474 & 0.1823 & 0.2112 & 0.2801 & 0.2489 & 0.2634 & 0.2633 & 0.1073 & 0.3000 & \underline{0.3027} & 0.2666 & \textbf{0.3225}$^\ast$ & 6.54\%\\
    ~ & NDCG@5 & 0.0553 & 0.0837 & 0.1213 & 0.1509 & 0.1309 & 0.1484 & 0.1311 & 0.0338 & 0.1796 & \underline{0.1831} & 0.1519 & \textbf{0.1957}$^\ast$ & 6.88\%\\
    ~ & NDCG@10 & 0.0751 & 0.1039 & 0.1387 & 0.1746 & 0.1536 & 0.1702 & 0.1565 & 0.0513 & 0.2012 & \underline{0.2046} & 0.1732 & \textbf{0.2186}$^\ast$ & 6.84\%\\
    \midrule
    \multirow{5}{*}{Sports} & HR@1  & 0.0186 & 0.0306 & 0.0525 & 0.0643 & 0.0281 & 0.0623 & 0.0561 & 0.0133 & 0.0811 & \underline{0.0865} & 0.0633 & \textbf{0.0954}$^\ast$ & 10.29\%\\ 
    ~ & HR@5 & 0.0750 & 0.0998 & 0.1263 & 0.1851 & 0.0901 & 0.1710 & 0.1876 & 0.0578 & 0.2051 & \underline{0.2061} & 0.1654 & \textbf{0.2208}$^\ast$ & 7.13\%\\
    ~ & HR@10 & 0.1385 & 0.1677 & 0.1921 & 0.2825 & 0.1527 & 0.2599 & \underline{0.2989} & 0.1072 & 0.2956 & 0.2964 & 0.2453 & \textbf{0.3161}$^\ast$ & 5.75\%\\
    ~ & NDCG@5 & 0.0464 & 0.0652 & 0.0894 & 0.1252 & 0.0589 & 0.1170 & 0.1219 & 0.0352 & 0.1441 & \underline{0.1473} & 0.1149 & \textbf{0.1593}$^\ast$ & 8.15\%\\
    ~ & NDCG@10 & 0.0667 & 0.0869 & 0.1105 & 0.1565 & 0.0789 & 0.1455 & 0.1577 & 0.0509 & 0.1731 & \underline{0.1764} & 0.1406 & \textbf{0.1899}$^\ast$ & 7.65\%\\
    \midrule
    \multirow{5}{*}{Movielens} & HR@1 & 0.0532 & 0.0965 & 0.0979 & 0.0653 & 0.1208 & 0.1273 & \underline{0.1274} & 0.0226 & 0.1207 & OOM & 0.0360 & \textbf{0.1345}$^\ast$ & 5.57\%\\
    ~ & HR@5 & 0.1954 & 0.2910 & 0.2992 & 0.2593 & 0.3362 & 0.3444 & \underline{0.3561} & 0.0847 & 0.3503 & OOM & 0.1456 & \textbf{0.3724}$^\ast$ & 4.58\%\\
    ~ & HR@10 & 0.3101 & 0.4266 & 0.4365 & 0.4286 & 0.4831 & 0.4854 & 0.4976 & 0.1504 & \underline{0.4979} & OOM & 0.2760 & \textbf{0.5230}$^\ast$ & 5.04\%\\
    ~ & NDCG@5 & 0.1244 & 0.1949 & 0.2002 & 0.1615 & 0.2304 & \underline{0.2385} & 0.2355 & 0.0534 & 0.2377 & OOM & 0.0898 & \textbf{0.2565}$^\ast$ & 7.54\%\\
    ~ & NDCG@10 & 0.1613 & 0.2386 & 0.2445 & 0.2159 & 0.2777 & 0.2840 & \underline{0.2877} & 0.0744 & 0.2853 & OOM & 0.1315 & \textbf{0.3051}$^\ast$ & 6.05\%\\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.1in}
\end{table*}

% \begin{table*}[h]
%     \centering
%     \caption{Full Evaluation.}
%     \vspace{-0.1in}
%     % \label{tab:results}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{clccccccccccccr}
%     \toprule
%     Dataset & Metric & Caser & GRU4Rec & SASRec & BERT4Rec & SR-GNN & GCSAN & SURGE & S$^3$-Rec$_{ISP}$ & CL4SRec & DuoRec & ICLRec & \textbf{\model} & \textit{\#Improve}\\
%     \midrule
%     \multirow{5}{*}{Reddit} & HR@1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & HR@5 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & HR@10 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & NDCG@5 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\% \\
%     ~ & NDCG@10 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     \midrule
%     \multirow{5}{*}{Beauty} & HR@1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\ 
%     ~ & HR@5 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & HR@10 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & NDCG@5 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & NDCG@10 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     \midrule
%     \multirow{5}{*}{Sports} & HR@1  & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\ 
%     ~ & HR@5 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & HR@10 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & NDCG@5 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & NDCG@10 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     \midrule
%     \multirow{5}{*}{Movielens-20M} & HR@1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & HR@5 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & HR@10 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & NDCG@5 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     ~ & NDCG@10 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\%\\
%     \bottomrule
%     \end{tabular}
%     }
%     \vspace{-0.1in}
% \end{table*}
In this section, we carry out comprehensive experiments in various settings to address the following research questions:
\begin{itemize}[leftmargin=*]
\item \textbf{RQ1}: How does \model\ perform compare with state-of-the-arts?
\item \textbf{RQ2}: Can our adaptive contrastive learning paradigms improve the performance of sequential recommendation in various scenarios, such as cold-start users and sparse items?
\item \textbf{RQ3}: How do different parameters impact \model's performance?
\item \textbf{RQ4}: Can the effects of our debaising CL be explained?
\end{itemize}

\subsection{Experimental Setting}
\subsubsection{\bf Datasets} 
We evaluate our model using four public datasets sourced from three real-life platforms, \ie, Reddit, Amazon, and MovieLens. i) \textbf{Reddit}: This dataset captures user interactions with subscribed topics on the Reddit platform. ii) \textbf{Amazon}: This product dataset collects user-item interactions from Amazon with the categories of \textbf{Beauty} and \textbf{Sports} products. iii) \textbf{MovieLens-20M}: This dataset comprises rating behaviors gathered from a movie review website. The statistics for various datasets are provided in Table~\ref{tab:dataset}.

% Our model evaluation is conducted on four public datasets collected from three real-life platforms, \ie, Reddit, Amazon, and MovieLens. i) \textbf{Reddit\footnote{\url{https://www.kaggle.com/colemaclean/subreddit-interactions/data/}}}: It records interactions between users and subscribed topics on the reddit platform over a 5-day period. ii) \textbf{Amazon}\footnote{\url{https://jmcauley.ucsd.edu/data/amazon/}}: The Amazon product dataset collects user-item interactions on the Amazon platform from 1996 to 2014. We adopt the subdatasets in \textbf{Beauty} and \textbf{Sports} top-categories. iii) \textbf{MovieLens-20M\footnote{\url{https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset}}}: This dataset includes rating behaviors collected on the famous movie review website. Following the general preprocessing strategies in \cite{duorec, sasrec, bert4rec, cl4srec, s3rec}, we apply the 5-Core filtering on the raw data and keep the sequences with length less than 50. For rating-based data on $[0,5]$ scale, we take $5$ as positive interaction and others as negative. The detailed statistics of the four datasets is presented in Table \ref{tab:dataset}.

\vspace{-0.05in}
\subsubsection{\rm \textbf{Evaluation Protocols}}
We follow \cite{bert4rec, mbht, sasrec} to adopt the \textit{leave-one-out} strategy for model evaluation. Specifically, we treat the last interaction of each user as testing data, and designate the previous one as validation data. We employ the commonly utilized Hit Ratio (HR@N) and Normalized Discounted Cumulative Gain (NDCG@N) metrics, with N values of 1, 5, and 10.

% We closely follow \cite{bert4rec, mbht, sasrec} to adopt the \textit{leave-one-out} strategy. Specifically, we treat the last interaction of each user as the testing data, and the previous one as the validation data. All other interactions are used for training. Additionally, we pair 100 negative samples for the ground-truth based on item popularity~\cite{bert4rec, mbht}. To evaluate models by the ranking task, we adopt the widely used \textit{Hit Ratio} (HR@N) and \textit{Normalized Discounted Cumulative Gain} (NDCG@N) metrics at $N=\{1,5,10\}$.

% \subsubsection{\rm \textbf{Baselines}} We include various baseline models for performance comparison with our \model.

\vspace{-0.05in}
\subsubsection{\rm \textbf{Baselines}} The compared methods are described as follows:

\paratitle{Non-GNN Sequential Recommendation Methods.}
\begin{itemize}[leftmargin=*]
    % \item \textbf{Caser}~\cite{caser}. This method utilizes CNN layers in both vertical and horizontal views to model the sequential information.
    \item \textbf{Caser}~\cite{caser}. It employs CNN layers in both vertical and horizontal perspectives to capture the sequential information.
    % \item \textbf{GRU4Rec}~\cite{gru4rec}. This method models sequences with GRU units and a ranking-based loss for session-based recommendation.
    \item \textbf{GRU4Rec}~\cite{gru4rec}. It employs GRU to encode sequences and incorporates a ranking-based loss for session-based recommendation.
    % \item \textbf{SASRec}~\cite{sasrec}. This method is the first to leverage the self-attention mechanism to model dynamic user interests in the sequence.
    \item \textbf{SASRec}~\cite{sasrec}. This method is the pioneer in utilizing the self-attention to capture dynamic user interests within a sequence.
    % \item \textbf{BERT4Rec}~\cite{bert4rec}. This work introduces the \textit{Cloze} task to sequential recommendation with a bidirectional attentive encoder.
    \item \textbf{BERT4Rec}~\cite{bert4rec}. The Cloze task is introduced to sequential recommendation, employing a bidirectional attentive encoder.
    % \item \textbf{INSERT}~\cite{insert}. It integrates knowledge from similar users and historical sessions of the target user to generate more accurate recommendation results for short sessions.
\end{itemize}
\paratitle{Graph-based Sequential Recommender Systems.}
\begin{itemize}[leftmargin=*]
    % \item \textbf{SR-GNN}~\cite{srgnn}. This method constructs graphs for sequences and generates hybrid embedding for local and global user interests.
    \item \textbf{SR-GNN}~\cite{srgnn}. It produces hybrid embeddings that effectively represent both local and global user interests with graphs.
    % \item \textbf{GCSAN}~\cite{gcsan}. It performs self-attention operation on graph-based sequential embeddings to capture long-term user interests.
    \item \textbf{GCSAN}~\cite{gcsan}. It conducts self-attention on graph-based sequential embeddings to capture long-term user interests.
    % \item \textbf{SURGE}~\cite{surge}. This method introduces metric learning to generate a parameterized item similarity graph and utilizes a hierarchical attention to fuse different aspects of user interests. 
    \item \textbf{SURGE}~\cite{surge}. This approach incorporates metric learning to create a parameterized item similarity graph and leverages hierarchical attention to combine various aspects of user interests.
\end{itemize}
% \vspace{-0.1in}
\paratitle{Self-Supervised Sequential Recommendation Models}.
% \vspace{-0.1in}
\begin{itemize}[leftmargin=*]
    \item \textbf{S$^3$-Rec}~\cite{s3rec}. This approach develops self-supervised task over item sequences employing a pretrain-finetuning strategy.
    % \item \textbf{S$^3$-Rec}~\cite{s3rec}. This method designs self-supervised tasks between item-, feature-, and sequence-level information in a pretrain-finetuning manner. We follow~\cite{iclrec, cl4srec, duorec} to remove feature-level SSL tasks to allow fair comparison since features are not included.
    \item \textbf{CL4SRec}~\cite{cl4srec}. It empowers recommendation with different sequence-level augmentations \ie item crop, mask, and reorder.
    % \item \textbf{DuoRec}~\cite{duorec}. It studies the representation degeneration problem in sequential recommendation and provides contrastive learning-based solutions.
    \item \textbf{DuoRec}~\cite{duorec}. This research investigates the representation degeneration issue in sequential recommendation and offers solutions based on contrastive learning techniques.
    % \item \textbf{ICLRec}~\cite{iclrec}. This method enhances sequential recommendation by performing clustering and contrastive learning on user intents.
    \item \textbf{ICLRec}~\cite{iclrec}. This approach improves sequential recommendation by conducting clustering and contrastive learning on user intentions to enhance recommendation.
\end{itemize}

\vspace{-0.1in}
\subsubsection{\rm \textbf{Parameter Settings}} We implement our \model~ and most of the baselines with the \textit{RecBole}~\cite{recbole} library. For \model, the number of Transformer layer and GNN layer is set as 2. The embedding size is set as 64. $\mu_c$ controls the mean value of conformity scores is search from $[0.3, 0.4, 0.5, 0.6, 0.7]$ and $\sigma$ for the standard deviation is set as $0.1$. The weight $\lambda_1$ for the self-supervised learning loss is searched from $[$5e-4, 1e-3, 5e-3, 1e-2$]$ and $\lambda_2$ for the Kullback-Leibler divergence loss is searched from $[$1e-3, 1e-2, 1e-1, 1$]$.

\vspace{-0.1in}
\subsection{RQ1: Overall Performance}
% Here we report the performances of \model~and baselines in Table~\ref{tab:results}. We make following observations based on the results.
We present the performance of our model and baselines in Table~\ref{tab:results}. Based on the results, we can make the following observations:
\begin{itemize}[leftmargin=*]
    % \item 
    % \item Among general sequential recommenders, attention-based methods \ie SASRec and BERT4Rec consistently outperform non-attentive methods \ie Caser and GRU4Rec. This indicates that the ability of attention mechanism to capture pair-wise dependency between items can better facilitate the sequential recommendation scenario. Additionally, we observe that unlike on other datasets, the bidirectional encoding brought by the Cloze task of BERT4Rec fails to enhance the performance on Movielens-20M dataset comparing to SASRec. Since the dataset is denser, the mask\&prediction training strategy may fail to exploit rich transitional signals since only one item is masked each sequence.
    \item Graph-based sequential recommendation models \ie SR-GNN, GCSAN, and SURGE achieve better overall performance compared with non-GNN models. The improvement is attributed to the effective capture of global item dependencies and long-term user interests facilitated by graph convolutions. Nonetheless, it is worth noting that the performance improvement is less significant or even negative in certain cases (e.g., SR-GNN on the Sports dataset) when dealing with sparser datasets such as Beauty and Sports. This limitation suggests that constructing graphs for sparse data may be inadequate for modeling long-term semantics, as a result of data scarcity and the presence of noise. \\\vspace{-0.12in}
    
    \item Sequential models incorporating self-supervised learning components, such as S$^3$-Rec, CL4SRec, DuoRec, and ICLRec, exhibit varying performance outcomes across the four datasets. For instance, CL4SRec and DuoRec demonstrate similar performance levels that surpass other baseline models on the Beauty and Sports datasets. On the other hand, S$^3$-Rec exhibits inferior results compared to other models across all four datasets. It is important to mention that this approach employs sequence augmentation and contrastive learning during the pretraining phase. In contrast to CL4SRec, which pursues the same objective during the main task training, this suggests that pretraining a sequence-level contrastive goal may not bring much benefits.\\\vspace{-0.12in}
    
    \item In comparison to the baseline models, our proposed method consistently outperforms them across the four datasets in general, with a particularly notable improvement in HR@1. While our method is marginally and not statistically significantly outperformed by SR-GNN in HR@5 and HR@10 on the Reddit dataset, the results in other cases still suggest the effectiveness of \model.
    
    % Though marginally and not statistically significantly outperformed by SR-GNN in HR@5 and HR@10 on Reddit dataset, the results demonstrate the effectiveness of our motivation and design.
\end{itemize}

\subsection{RQ2: Benefits Study}
\subsubsection{\rm \textbf{Performance on Cold-Start Users}}

\begin{figure}[t]
\centering
\subfigure[HR@1]{
\label{fig:cold_user:hr}
\includegraphics[width=0.45\linewidth]{material/group_test_seq_len_HR1.pdf}}
\subfigure[NDCG@5]{
\label{fig:cold_user:ndcg}
\includegraphics[width=0.45\linewidth]{material/group_test_seq_len_NDCG5.pdf}}
\vspace{-0.2in}
\caption{Evaluation results on cold-start users.}
\label{fig:cold_user}
\vspace{-0.25in}
\end{figure}

Adhering to evaluation settings outlined in~\cite{insert, mhcn, kgcl}, we filter users with fewer than 20 interactions to create a sub-dataset of cold-start users for the four datasets. The evaluation outcomes for cold-start users, including HR@1 and NDCG@5, are presented in Figure~\ref{fig:cold_user}. The results clearly demonstrate that our method outperforms the strongest baselines from various research lines in addressing the cold-start problem. We attribute this advantage to our model's capability to balance knowledge transfer across different views, utilizing conformity-aware contrastive learning. For cold-start users, the designed model effectively extracts valuable information from global transition signals and collaborative patterns to enhance user representations. Furthermore, it refines the acquired knowledge through conformity-aware weighting to mitigate the popularity bias affecting non-active users.

% From the results, clearly, \model~outperforms strongest baselines in various research lines on tackling the cold-start problem. We attribute this benefit to \model's ability in balancing knowledge transfer across different views with conformity-aware contrastive learning. For cold-start users, the designed \model~draws useful knowledge from global transition signals and collaborative patterns to enrich user representations, and distill the knowledge by conformity-aware weighting to eliminate popularity bias for non-active users.

\subsubsection{\rm \textbf{Performance w.r.t. Item Sparsity}} 
To further explore our model's capabilities in addressing the item sparsity challenge in sequential recommendation, we categorize target items into five groups based on their sparsity levels. A lower group number indicates that the items within that group have fewer interactions with users. The results are displayed in Figure~\ref{fig:cold_item}. Across all five groups, our model outperforms the baselines in the first four groups, demonstrating its effectiveness in handling item sparsity. This observation suggests that the performance improvement of our model primarily stems from accurately predicting less popular items. Consequently, we posit that our debiased contrastive learning generates higher-quality item embeddings for recommendation. This conclusion aligns with our findings presented in Section \ref{sec:a:emb}.

% To further investigate \model's ability in tackling the item sparsity issue in sequential recommendation, we group the target items into five with respect to the sparsity level for less dense datasets Beauty and Sports. A smaller group number means that the items are interacted with less often. We present the results in Figure~\ref{fig:cold_item}. Across all five groups, \model~achieves better performances against strongest baselines in the first four groups. Especially, \model~still performs remarkably well when baseline models fail to correctly predict any item in G1. This indicates that the performance gain of \model~mainly comes from correctly predicting less popular items. Therefore, we assume that our debiased contrastive learning derives item embeddings of higher quality. This is in line with our findings in \ref{sec:a:emb}.

\begin{figure}[t]
\centering
\subfigure[Sports Dataset]{
\label{fig:cold_item:hr}
\includegraphics[width=0.47\linewidth]{material/group_test_sparsity_Sports.pdf}}
\subfigure[Beauty Dataset]{
\label{fig:cold_item:ndcg}
\includegraphics[width=0.47\linewidth]{material/group_test_sparsity_Beauty.pdf}}
\vspace{-0.2in}
\caption{Performance on item groups w.r.t. sparsity level. Larger group number indicates more popular items.}
\label{fig:cold_item}
\vspace{-0.15in}
\end{figure}

\subsection{RQ3: Ablation Study}

\subsubsection{\rm \textbf{Impact of Key Components.}}
We develop four variants, with each one excluding a specific key component, to delve deeper into the design of our \model. Details are presented as follows:\vspace{-0.05in}
% To further investigate the design of each key component of our proposed \model, we come up with four variants, each individually removing a key component:
\begin{itemize}[leftmargin=*]
    \item \textbf{w/o T-CL} removes the contrastive learning between the sequential and item transition graph representations for augmentation.
    \item \textbf{w/o C-CL} removes the contrastive learning between the sequential and co-interaction graph representations for augmentation.
    \item \textbf{w/o CL} removes the entire contrastive learning module.
    % \item \textbf{w/o Adaptive-CL} removes the design in Section \ref{sec:adaptive} that powers the contrastive learning process to be adaptive by the user's conformity and interest disentanglement.
    \item \textbf{w/o Adaptive-CL} omits the design described in Section \ref{sec:adaptive} that enables the contrastive learning process to adapt based on the user's conformity and interest disentanglement. \vspace{-0.05in}
\end{itemize}
The results of the ablation study are presented in Table~\ref{tab:ablation}. Based on the ablation study, we can make the following observations: 1) Each of these key components contributes substantially to the enhancement of the model's recommendation performance; 2) The contrastive learning between the sequential and item transition graph representations leads to more significant improvements in the model's performance. 3) On the Beauty dataset, eliminating adaptive weights results in a lower performance than that achieved without contrastive learning. This observation suggests that the adaptive-CL component serves a crucial role in mitigating the bias introduced by potentially inaccurate contrastive learning.

% We report the results of the ablation research in Table~\ref{tab:ablation}. We summarize our observations as: 1) each of these key components plays a significant role in improving the recommendation results of the model; 2) the contrastive learning between the sequential and item transition graph representations brings better improvement; and 3) on Beauty, only removing adaptive weights leads to lower performance than that without contrastive learning. This indicates that the adaptive-CL component plays a key role in alleviating the bias brought by the inaccurate contrastive learning.

\begin{table}[t]
    \centering
    \caption{Ablation study results of \model.}
    \vspace{-0.15in}
	\resizebox{\linewidth}{!}{
    \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Ablation Settings} & \multicolumn{2}{c}{Reddit} & \multicolumn{2}{c}{Beauty} & \multicolumn{2}{c}{Sports}\\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
    ~ & HR@1 & HR@5 & HR@1 & HR@5 & HR@1 & HR@5\\
    \midrule
    \model & 0.188 & 0.390 & 0.136 & 0.251 & 0.095 & 0.221\\
    \cmidrule(lr){1-7}
    w/o T-CL & 0.178 & 0.374 & 0.122 & 0.224 & 0.083 & 0.195\\
    w/o C-CL & 0.182 & 0.387 & 0.132 & 0.245 & 0.083 & 0.202\\
    w/o CL & 0.169 & 0.369 & 0.120 & 0.221 & 0.084 & 0.196\\
    w/o Adaptive-CL & 0.174 & 0.366 & 0.121 & 0.225 & 0.082 & 0.192\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:ablation}
    \vspace{-0.15in}
\end{table}

\vspace{-0.05in}
\subsubsection{\rm \textbf{Sensitivity to Hyperparameters.}}
% Due to space limit in the main file, we place the discussion in \ref{sec:a:hp}.
Owing to space constraints in main file, we relocate the discussion to Section~\ref{sec:a:hp}.

\vspace{-0.1in}
\subsection{RQ4: Case Study}
We conduct case studies on the Movielens to verify the rationality of conformity weights in our model. In Figure~\ref{fig:case_study}, we select two user-interaction pairs with different conformity degrees, specifically 0.68 and 0.25. In the first case, user $U_{4028}$ interacts with a sci-fi movie, and movies of the same genre are prevalent in their historical sequence. Additionally, we showcase movies from other users' sequences that are closely related to the target movie in order to visualize the transition graph. From the results, we see that most users interact with similar movie themes around the target movie, consistent with the target user's pattern. Hence, a conformity degree of 0.68 is a reasonable assessment of the user's conformity.

% To investigate the interpretability of the recommendation generated by \model, we perform case studies on Movielens dataset to validate the rationality of conformity weights. We sample two user-interaction pairs with differed conformity degrees, \ie 0.68 and 0.25 in Figure~\ref{fig:case_study}. From the first case, the user $U_{4028}$ interacted with a sci-fi movie, and movies with the same theme are dominant in his history sequence. We further list the movies from other users' sequences, close to the target movie as to visualize the transition graph $\mathcal{G}_t$. From the result, we can observe that most users interacted with similarly themed movies before or after reviewing the target movie. This pattern is generally in line with that of the target user. Thus, 0.68 is a reasonable score to judge the user's conformity degree.

In the second sample, a user interacts with a fantasy movie, and her conformity degree is estimated as 0.25 by the model. This case suggests that the target user's preference has less in common with that of other users. Specifically, the user engaged with diverse movies across multiple genres. The interactions display no consistent semantic bias. As a result, we believe the interaction is guided by the user's authentic interest rather than the impact of popularity bias. Unlike the case in Figure~\ref{fig:intro_case}, we analyze disentanglement of user interest and conformity using item semantics, not popularity. Both factors are informative and jointly influence user intentions.





% The second sample shows that a user interacted with a fantasy movie, and her conformity degree is estimated as 0.25 by \model. From the case, we can tell that the pattern of the target user has less to do with that of other users. 

% Specifically, the user rated various movies in different genres, and so do other users. The interactions lack consistent semantic bias. Therefore, we believe that this interaction is motivated by the user's genuine interest rather than the result of the popularity bias influence. Note that different from the case in Figure~\ref{fig:intro_case}, we analyze the disentanglement of user interest and conformity from the perspective of item semantics, rather than item popularity. In fact, both factors are informative and integrally influence the user's intention and behavior pattern.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{material/case_study.pdf}
    \vspace{-0.15in}
    % \caption{Two cases of user-item interaction pairs with different conformity levels as identified by \model~in the Movielens dataset. The subtitles indicate the categories of the movies.}
    \caption{Two user-item interaction pairs with different conformity levels discovered by \model\ from Movielens dataset.}
    %The subtitles indicate the categories of the movies.}
    \label{fig:case_study}
    \vspace{-0.15in}
\end{figure}