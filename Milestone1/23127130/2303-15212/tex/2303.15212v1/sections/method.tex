


\section{Deep Ranking Ensembles (DRE)}

\subsection{Preliminaries} 

\textbf{Hyperparameter Optimization} is defined as the problem of tuning the hyperparameters $x \in \mathcal{X}$ of a ML algorithm to minimize the validation error achieved on a dataset $D$ as $\argmin_{x \in \mathcal{X}} \mathcal{L}^{\text{Val}}\left(x, D\right)$. The mainstream approach for tuning hyperparameters is Bayesian Optimization (BO), an introduction of which is offered by \cite{Hutter2019_Automated}. BO relies on fitting a surrogate function for approximating the validation error on evaluated hyperparameter configurations. Consider having evaluated $N$ configurations on a dataset and their respective validation errors as $H=\left\{ \left( x_i, y_i\right) \right\}_{i=1}^N$, where $y_i=\mathcal{L}^{\text{Val}}\left(x, D\right)$. We train a surrogate function $\hat y\left(x_i\right) = f(x_i; \theta)$, typically a Gaussian Process, to estimate the observed $y$ as $\argmax_{\theta} \; \E_{(x_i, y_i) \sim p_{H}} \; \log p\left(y_i | x_i, H / \{ (x_i,y_i)\};  \theta \right)$. 

\textbf{Learning to Rank (L2R)} differs from a standard supervised regression because instead of directly estimating the target variable it learns to estimate the rank of the target values. In the context of HPO, we define the rank of a configuration as $r(x_i, \left\{y_1,\dots,y_N\right\}) := \sum_{j=1}^N \mathbbm{1}_{y_j \le y_i}$. The core of a typical L2R method~\citep{burges2005learning} includes training a parametric ranker $\hat r\left(x_i\right) := f(x_i; \theta)$ that correctly estimates the ranks of observed configurations' validation errors. Instead of naively estimating the ranks as a direct regression task, i.e. $\argmax_{\theta} \; \E_{(x_i, r_i) \sim p_{H}} \; \log p\left(r_i | x_i;  \theta \right)$, L2R techniques prioritize estimating the ranks of top-performing configurations more than bottom-performing ones~\citep{Cao07_Learning}. In general ranking losses can be defined on the basis of single objects (\textit{point-wise approach}), pairs of objects (\textit{pair-wise approach}) or the whole list of objects (\textit{list-wise approach})~\citep{wei_ranking2009}. 



\begin{algorithm}[t!]
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Set of datasets $\mathcal{D}$, Number of iterations $J$, Number of ensemble scorers $M$}
\Output{DRE parameters $\theta_1, \dots, \theta_M$, Meta-feature network parameters $\phi$}
 \smallskip
 Initialize scorer networks with parameters $\theta_1, \dots, \theta_M$ \; 
 \smallskip
 Initialize the parameters $\phi$ of the meta-feature network $z$ from \citet{jomaa2021dataset2vec} \; 
 \smallskip
 \For{$j = 1$ \KwTo $J$}{
 \smallskip
  Sample dataset index $i \in \left\{1,\dots,D\right\}$, sample scorer network index $m \in \left\{1,\dots,M\right\}$ \;
  %Sample support set $\mathcal{D}_s = \{(x^{(s)}_i,y^{(s)}_i)\}$ from $\mathcal{D}_i$\;
  \smallskip
  Sample a query set $H^{(s)} := \left\{\left(x_1^{(s)}, y_1^{(s)}\right), \dots, \left(x_{N^{(s)}}^{(s)}, y_{N^{(s)}}^{(s)}\right)\right\}$ from $\mathcal{D}_i$ \;
  \smallskip
  Sample a support set $H^{(z)}$ from $\mathcal{D}_i \setminus H^{(s)}$ \;
  \smallskip
  \smallskip
  Compute meta-features $z(H^{(z)}; \phi)$ \;
  \smallskip
  %$x_1,\dots,x_{n+m},y_1,\dots,y_{n+m} \sim p(\dataset{})$\;
  Compute rank scores for the query set $s_{i}= s\left(x_i^{(s)}, z\left(H^{(z)}; \phi\right); \theta_m\right), i=1,\dots,N^{(s)}$ \;
  \smallskip
  Compute true ranks $\pi\left(1\right),\dots,\pi\left(N^{(s)}\right)$ \;
  \smallskip 
  \smallskip
  %Compute loss $\Ls\left(\theta_m, \phi \right) =  w\left(\pi(i)\right) \frac{\exp^{s\left(x_{\pi(i)}, z\left(H^{(z)}; \phi\right); \theta_m \right)}}{\sum_{j=i}^N \exp^{s\left(x_{\pi(j)}, z\left(H^{(z)}; \phi\right); \theta_m \right)}}$ using Equation~\ref{eq:deeprank} \;
  Compute loss $\Ls\left(\pi, s; \; \theta_m, \phi \right)$ using Equation~\ref{eq:deeprank} \;
  \smallskip
  Update the meta-feature network $\phi \leftarrow \phi - \eta_{\phi} \frac{\partial \Ls\left(\pi, s; \; \theta_m, \phi \right)}{\partial \phi}$ \;
  \smallskip
  Update the ranker network $\theta_m \leftarrow \theta_m - \eta_{\theta_m} \frac{\partial \Ls\left(\pi, s; \; \theta_m, \phi \right)}{\partial \theta_m}$ \;
 }
 \textbf{return} $\theta_1, \dots, \theta_M, \phi$ \;
 \caption{Meta-learning the Deep Ranking Ensembles}
 \label{alg:meta-learning}
\end{algorithm}


\subsection{Deep Ranking Ensemble (DRE) Surrogate}
\label{sec:dresurrogate}

In this paper, we introduce a novel ranking model based on an ensemble of diverse neural networks optimized for L2R. We aim to learn neural networks that output the ranking score of a hyperparameter configuration $s: \mathcal{X} \rightarrow \R$. The ranks of the estimated scores should match the true ranks $\sum_{j=1}^N \mathbbm{1}_{y_j \le y_i} \approx \sum_{j=1}^N \mathbbm{1}_{s(x_j; \theta) \ge s(x_i; \theta)}$, however, with a higher priority in approximating the ranks of the top-performing configurations using a weighted list-wise L2R loss~\citep{Cao07_Learning}. First of all, we define the indices of the ranked/ordered configurations as $\pi: \{1, \dots, N\} \rightarrow \{1, \dots, N\}$. Concretely, the $\ell$-th observed configuration is the $k$-th ranked configuration $\pi(\ell) = k$ if $k = \sum_{j=1}^N \mathbbm{1}_{y_j \le y_\ell}$. Ultimately, we train the scoring network using the following loss:

\begin{align}
    \label{eq:deeprank}
    \argmin_{\theta} \sum_{i=1}^N \Ls\left(x_i, y_i, y, \theta\right), \text{ where } \; 
    \Ls\left(x_i, y_i, y, \theta \right) =  w\left(\pi(i)\right) \frac{\exp^{s\left(x_{\pi(i)}; \theta \right)}}{\sum_{j=i}^N \exp^{s\left(x_{\pi(j)}; \theta \right)} } 
\end{align}


The weighting functions $w: \{1,\dots,N\} \rightarrow \R_+$ is defined as $w\left(\pi(i)\right) = \frac{1}{\log\left( \pi(i) + 1\right) }$ and is used to assign a higher penalty to the top-performing hyper-parameter configurations, whose correct rank is more important in HPO~\citep{chen2017top}. After having trained the scoring model of Equation~\ref{eq:deeprank} we estimate the rank of an unobserved configuration as $\hat r\left(\textbf{x}; \theta\right) = \sum_{j=1}^N \mathbbm{1}_{s(x_j; \theta) \ge s(\textbf{x}; \theta)}$. Furthermore, Bayesian Optimization (BO) needs uncertainty estimates to be able to explore the search space~\citep{Hutter2019_Automated}. As a result, we model uncertainty by training $M$ diverse neural scorers  $s_{1}(x,\theta_1), \dots, s_{M}(x,\theta_M)$ with stochastic gradient descent. The diversity of the ensemble scorers is ensured through the established mechanism of applying different per-scorer seeds for sampling mini-batches of hyperparameter configurations~\citep{Lakshminarayanan2017_DeepEnsembles}. Finally, the posterior mean and variance of the estimated ranks is computed trivially as $ \mu(x) = \frac{1}{N}\sum^N_{i=1} \hat r(x; \theta_i)$ and $\sigma^2(x) = \frac{1}{N} \sum^N_{i=1} (\hat r(x; \theta_i)-\mu(x))^2$. The BO pseudo-code and the details for using our Deep Rankers in HPO are explained in Appendix~\ref{appendix:bo_DRE}.


\subsection{Meta-learning the Deep Ranking Ensembles} 

HPO is a very challenging problem due to the limited number of evaluated hyperparameter configurations. As a result, the current best practice in HPO relies on transfer-learning the knowledge of hyperparameters\footnote{Even in manually-designed ML systems, experts start their initial guess about hyper-parameters by transfer-learning the configurations that worked well on past projects (a.k.a. datasets).} from evaluations on previous datasets~\citep{Wistuba2021_FSBO, Wistuba2016_Twostage, Salinas2020_Quantile}. In this paper, we meta-learn our ranker from $K$ datasets assuming we have a set of observations $H^{(k)}:=\left\{ \left( x^{(k)}_1, y^{(k)}_1 \right), \dots, \left( x^{(k)}_{N_k}, y^{(k)}_{N_k} \right) \right\}; \; k=1,\dots,K$ with $N_k$ evaluated hyperparameter configurations on the $k$-th dataset. We meta-learn our ensemble of $M$ Deep Rankers with the meta-learning objective in Equation~\ref{eq:deeprankmetalearn}, where we learn to estimate the ranks of all observations on all evaluations for all the previous datasets using the loss of Equation~\ref{eq:deeprank}. 

\begin{align}
    \label{eq:deeprankmetalearn} 
    \argmin_{\theta_1, \dots, \theta_M} \; \sum_{k=1}^K \sum_{n=1}^{N_k} \sum_{m=1}^M \Ls\left(x^{(k)}_n, y^{(k)}_n, y^{(k)}; \theta_m \right) 
\end{align} 







Transfer-learning for HPO suffers from the negative-transfer phenomenon, where the distribution of the validation errors given hyperparameters changes across datasets. In such cases, using dataset meta-features helps condition the transfer only from evaluations on similar datasets~\citep{rakotoarison2022learning, jomaa2021dataset2vec}. We use the meta-features of \cite{jomaa2021dataset2vec} which are based on a deep set formulation~\citep{NIPS2017_f22e4747} of the pairwise interactions between hyperparameters and their validation errors. The meta-feature network with parameter $\phi$ takes a history of evaluations $H=\left\{ \left( x_i, y_i\right) \right\}_{i=1}^N$ as its input and outputs a $L$-dimensional representation of the history as $z(H, \phi): \left(\mathcal{X} \times \R\right)^N \rightarrow \R^L$. Afterward, the scorer function becomes $s\left(x, z(H; \phi); \theta \right): \mathcal{X} \times \R^L \rightarrow \R$. In other words, the dataset meta-features are additional features to the scorers. A graphical depiction of our architecture is shown in Figure~\ref{figure:DREArchitecture}. 


%Furthermore, we present the full meta-learning algorithm for DRE in Algorithm~\ref{alg:meta-learning} at Appendix~\ref{sec:metalearningpseudocode}. 
%\section{Pseudocode for the Meta-Learning Procedure}
%\label{sec:metalearningpseudocode}


We update all the scorer networks of the ensemble independently using the loss of Equation~\ref{eq:deeprank}. The pseudo-code of Algorithm~\ref{eq:deeprank} draws an evaluation set (called a query set) which is used as the training batch for updating the parameters of the sampled scorer network. We also meta-learn the meta-feature network~\citep{jomaa2021dataset2vec}, however, by using a different batch of evaluations (called a support set). We do not meta-learn both the scorer and the meta-feature networks using the same batch of evaluations in order to promote generalization.  

