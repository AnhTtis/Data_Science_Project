\newpage

\section{Bayesian Optimization with Deep Ranking Ensembles}
\label{appendix:bo_DRE}

Once the Deep Ensembles are trained, we aggregate the predictions for an input $x$ following the procedure explained in Section~\ref{sec:dresurrogate} to obtain $\mu(x), \sigma(x)$ and conditioning to a set of observations $\mathcal{D}_s$. For the sake of simplicity, we omit this conditioning in our notation. These outputs can be fed in several types of acquisition functions and decide for the next point $x$ to observe from the set of pending points to evaluate $\mathcal{X}$. Notice that the lower rank, the better the configuration, therefore we formulate the cast the acquisition function as a minimization problem. Specifically, we consider:

\begin{itemize}
    \item \textbf{Average Rank}: $\alpha(x_j) =   \mu(x_j)$
    \item \textbf{Lower Confidence Bound}: $\alpha(x_j) = \mu(x_j) - \beta \cdot \sigma(x_j)$
    \item \textbf{Expected Improvement}: $\alpha(x_j) =  -\int_{r}  \max \left(0, \mu(x_k)-r \right) \mathcal{N}\left(r;\mu(x_j), \sigma(x_j)\right)$
\end{itemize}

Where $\beta$ is a factor that trades of exploitation and exploration and $x_k$ is the best-observed configuration, i.e. $k = \argmin_{i \in \{1,...,|\mathcal{D}_s|\}} y_i$ and $\mu(x_k)$ is the average rank predicted for that configuration and $y_k$ is its validation error. The previous formulation assumes a minimization, thus to choose the next query point you apply: $x = \argmin_{x_j \in \mathcal{X}} {\alpha(x_j})$.


\begin{algorithm}[ht]
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{A prior distribution over datasets $p(\mathcal{D})$, initial observations $H=\{ (x_1, y_1),...,(x_N, y_N)\}$, pending points $\mathcal{X}$, number of BO iterations $K$, black-box function to optimize $f$}
\Output{Best observed configuration $x_*$}
 Train ensemble of MLP scorers following Algorithm \ref{alg:meta-learning} and prior $p(\mathcal{D})$;
 
 % $p(x_1,\dots,x_{n+m},y_1,\dots,y_{n+m})$\;
 \For{$j\gets1$ \KwTo $K$}{
  Fine-tune/Train MLP scorers \;
  Suggest next candidate $x = \argmin_{x_j \in \mathcal{X}} \alpha (x_j, H)$ \;
  Observe response $y=f(x)$ \;
  Update history $H= H \cup \{ (x, y)\}$\;

 }
 Return top performing configuration: $\argmin_{(x_i,y_i) \in H} y_i$
 \caption{Bayesian Optimization with DRE}
 \label{alg:bo_with_DRE}
\end{algorithm}

%\section{Baselines Settings}
%\label{appendix:baselines_settings}





\section{Experimental Setup for Deep Ranking Ensembles}
\label{section:experimental_setup_surrogate}

%\textbf{Set-Up for Meta-Learned DRE}
%In the HPO-B dataset, each search space represents the HP Search Space of a unique ML model. Each ML model has its distinct response surface and dimensional size. Hence, a different DRE surrogate is meta-learned for each Search Space.

\textbf{Meta-Feature Extractor} The DRE model has two configurable components: the meta-feature network and the scorers.
%As we concatenate the HP evaluations to their respective configurations before the data is input to the Deep Set, its input size is the HP Search Space dimension + 1.
The meta-feature extractor is a DeepSet with an architecture similar to the one used by \citet{jomaa2021dataset2vec}. However, we used 2 fully connected layers with 32 neurons each for both $\phi$ and $\rho$ (Deep Set parameters) instead of 3 fully connected layers. The output size is set to 16 by default.

\textbf{Ensemble of Scorers} The ensemble of scorers is a group of 10 MLP (Multilayer Perceptrons) with identical architectures. Each neural network has 4 hidden layers and each hidden layer has 32 neurons. The neural networks are initialized independently and randomly (for DRE-RI) or warm-initialized with the meta-learned weights. The input size of each neural network is 16 (the dimesiionality of the meta-features), plus the HP search space dimensionality. their output size is 1. 
%The DRE model described above is meta-trained for 5000 epochs using stochastic gradient descent. A batch size of 100 and a list size of 100 is used for calculating the list-wise loss. We deploy the Adam optimizer with a learning rate of 0.001. During meta-learning, from the randomly sampled batch of data, we set 20\% of the data as a support set and the rest as the query set.
%The set cardinality invariance of the DRE surrogate helped us fix the size of the support set to a percentage instead of an absolute number.
%We also make sure that the data in a batch belongs to the same dataset within the HP Search Space data in HPO-B.

%The meta-training DRE is used for doing the BO iteration in the Search Space it is trained for, albeit on new data distribution. 
%For each step in a BO iteration, the meta-trained DRE model is reloaded from the hard disk. Next, it is fine-tuned with the available HP evaluation data for 1000 epochs with a learning rate of 0.001 using the Adam optimizer. After this, its predictions are used by the EI acquisition function to predict the best HP configuration from a given list of contenders.

%\textbf{Setup for randomly-initialized DRE (DRE-RI).} The architecture of a randomly-initialized DRE is identical to the one used for the meta-learned DRE. However, there are two differences. First, the DRE surrogate is not meta-trained before using it for the BO iteration. For this reason, during every step of the BO iteration, its parameters are randomly initialized. Secondly, a higher learning rate of 0.02 is used during the fine-tuning because the randomly initialized DRE model may be further away from local optima than a meta-trained DRE model.

\textbf{Setup for Motivating Example.} For the creation of the Figure \ref{figure:bo_steps}, we use as scorer network an MLP with 2 hidden layers and 10 neurons per layer. The meta-feature extractor has 4 layers and 10 neurons, and output dimensions equal to 10. The network is meta-trained for 1000 epochs, with batch size 10, learning rate 0.001, Adam Optimizer, and 10 models in the ensemble. For the meta-learning example, we do not fine-tune the networks, while we fine-tune the networks for the non-meta-learned example for 500 iterations.

\newpage
\section{Discussion on List Size and List Weights}

\begin{wrapfigure}{r}{0.60\textwidth}
\begin{subfigure}{.3\textwidth}
  \centering
  \caption{List Size Ablation}
  \includegraphics[width=1\linewidth]{figures/list_size_aggregated_rank.pdf}
  \label{fig:ablation_list_size}
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \caption{List Weights}
  \includegraphics[width=1\linewidth]{figures/weights_comparison.pdf}
  %\caption{}
  \label{fig:list_weights}
\end{subfigure}
\caption{Effect of parameters in list-wise loss}
\label{fig:list_parameters}
\end{wrapfigure}


We present an additional ablation on the list size. We report the average rank on the meta-validation split for different list sizes during meta-training on Figure \ref{fig:ablation_list_size}. Notice, that a small list size (10) leads to an underperforming setting. Therefore, it is important to consider relative large list sizes ($100\leq n$).

During meta-testing i.e. by performing BO, there is no significant overhead in terms of having a larger list size, because the true rank is derived from the observed validation accuracy of configurations. During both meta-training, as well as the BO step, we fit our surrogate to estimate the rank of previously observed configurations that have been already evaluated. Given $n$ observations, computing the true rank is a simple $\mathcal{O}(n\cdot \mathrm{log}(n))$ sorting operation. Notice that in BO settings $n$ is typically small. 

There are several weighting schemes. Two alternatives to the weighting factor we use (inverse log weighting) are inverse linear weighting and position-dependent attention (PDA)~\citep{chen2017top}. As you can see in Figure \ref{fig:list_weights}, inverse linear gives very small weight to lower ranks, while the position-dependent gives too much importance. In this plot, PDA weights were scaled to make it comparable to the other schemes.
We decided to use the inverse log weighting because it gives neither too low nor too high weight to lower ranks. For the $j$-th position in a list with $k$ elements, these weights can be described as follows:


\begin{itemize}
    \item \textbf{Inverse Log: } $w(j)=\frac{1}{\mathrm{log}(j+1)}$
    \item \textbf{Inverse Linear: } $w(j)=\frac{1}{j}$
    \item \textbf{Position-dependent attention: } $w(j)=\frac{k-j+1}{\sum_{t=1}^k t}$
    
\end{itemize}

\section{Discussion on Computational Cost}

\begin{table}[]
\caption{Average Cost per BO Step (in seconds)}\label{tab:average_cost}

\begin{tabular}{cccccc}
\hline
%\multicolumn{1}{l}{} & \multicolumn{5}{c}{\textbf{Average Cost per BO step  (in seconds)}}                                                                                                                                      %       \\ \toprule
\multicolumn{1}{l}{} & \multicolumn{1}{c}{\textbf{4796 (3 Dims)}} & \multicolumn{1}{c}{\textbf{5636 (6 Dims)}} & \multicolumn{1}{c}{\textbf{5527 (8 Dims)}} & \multicolumn{1}{c}{\textbf{5965 (10 Dims)}} & \textbf{5906 (16 Dims)} \\ \midrule
\textbf{HEBO}          & \multicolumn{1}{c}{0.27 $\pm$ 0.18}           & \multicolumn{1}{c}{3.11 $\pm$ 1.68}           & \multicolumn{1}{c}{2.66 $\pm$ 0.95}           & \multicolumn{1}{c}{3.21 $\pm$ 1.78}           & 2.85 $\pm$ 2.43           \\ 
\textbf{FSBO}          & \multicolumn{1}{c}{10.49 $\pm$ 2.92}          & \multicolumn{1}{c}{10.13 $\pm$ 1.51}         & \multicolumn{1}{c}{10.61 $\pm$ 4.47}         & \multicolumn{1}{c}{11.45 $\pm$ 4.35}          & 12.13 $\pm$ 6.41          \\ 
\textbf{DRE}           & \multicolumn{1}{c}{22.29 $\pm$ 3.81}         & \multicolumn{1}{c}{18.8 $\pm$ 3.57}          & \multicolumn{1}{c}{22.61 $\pm$ 3.85}         & \multicolumn{1}{c}{19.39 $\pm$ 3.81}          & 22.29 $\pm$ 3.79          \\ \bottomrule

\end{tabular}
\end{table}


We provide here a cost comparison between DRE, FSBO and HEBO. In the Table \ref{tab:average_cost}, we provide the average cost per BO step ($\pm$ standard deviation) for different search spaces (with different dimensions). DRE effectively incurs a cost higher than FSBO and HEBO, but $<$30 seconds, which is a very small overhead compared to the cost of actually evaluating hyperparameter configurations (evaluation means the expensive process of training classifiers given the hyperparameter configurations and computing the validation accuracy).

\section{Additional Plots}
\label{appendix:additional_results}

We present additional results on the critical difference diagrams for \textit{i)} Transfer methods results (Figure \ref{fig:cd_transfer}), \textit{ii)} Non-Transfer (Figure \ref{fig:cd_non_transfer}, \textit{iii}) Scorer size (Figure \ref{fig:cd_score}, \textit{iv}) Acquisition Function (Figure \ref{fig:cd_acqf}, \textit{v} Ranking Loss (Figure \ref{fig:cd_ranking_loss}) and \textit{vi} Meta-features (Figure \ref{fig:cd_meta_features}). These CD plots show the comparison of the performance at different number of trials (e.g. at 25 trials = Rank@25). The vertical lines connecting two methods indicate that their performances are not significantly different.
\input{sections/cd_plots}

\begin{figure}
\includegraphics[width=1\linewidth]{figures/transfer_results_per_space_rank_rebuttal.png}
\caption{Average Rank per Search Space (Transfer Methods)}
\label{fig:transfer_results_per_space_rank}
\end{figure}

\begin{figure}
\includegraphics[width=1\linewidth]{figures/non_transfer_results_per_space_rank_rebuttal.png}
\caption{Average Rank per Search Space (Non-Transfer Methods)}
\label{fig:non_transfer_results_per_space_rank}
\end{figure}




%\input{sections/method_old}