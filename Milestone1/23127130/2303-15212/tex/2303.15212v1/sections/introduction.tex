

\section{Introduction}

Hyperparameter Optimization (HPO) is a crucial ingredient in training state-of-the-art Machine Learning (ML) algorithms. The three popular families of HPO techniques are Bayesian Optimization~\citep{Hutter2019_Automated}, Evolutionary Algorithms~\citep{awad-ijcai21}, and Reinforcement Learning~\citep{10.5555/3454287.3455167, jomaa-hyperrl2019}. Among these paradigms, Bayesian Optimization (BO) stands out as the most popular approach to guide the HPO search. At its core, BO fits a parametric function (called a surrogate) to estimate the evaluated performances (e.g. validation error rates) of a set of hyperparameter configurations. The task of fitting the surrogate to the observed data points is treated as a probabilistic regression, where the common choice for the surrogate is Gaussian Processes (GP)~\citep{Snoek2012_Practical}. Consequently, BO uses the probabilistic predictions of the configurations' performances for exploring the search space of hyperparameters. For an introduction to BO, we refer the interested reader to \citet{Hutter2019_Automated}.



%which can be coupled with Deep Kernels to learn latent representations of the input~\citep{pmlr-v51-wilson16}. Other approaches use Bayesian Neural Networks to learn these latent representations, however, their inference time can be demanding if they use MCMC estimations~\citep{Springenberg2016_BOHAMIANN} or their expressivity can be limited if they assume linear kernels~\citep{Snoek2015_DNGO}.

%White et al have shown that Deep Ensembles can produce good uncertainties calibrations for optimizing hyperparameters in Neural Networks \citep{White2021_BANANAS}. Moreover, recent work has provided evidence that Deep Ensembles provide calibrated uncertainties \cite{} and approximate inference \cite{}. Other lines of work show that Deep Ensembles can outperform some Bayesian neural networks for uncertainty representation ~\citep{Snoek2019_Can, Ashukha2020_Pitfalls}. Although~\citep{White2021_BANANAS} used a standard deep ensemble for Neural Architecture Search, their main focus was how to create embeddings of the architecture search space. Till the date, no work has formally studied what is the best method to achieve top-perfrmance with Deep Ensembles for HPO. 

In this paper, we highlight that the current BO approach of training surrogates through a regression task is sub-optimal.
%, and malpractice by the HPO research community. 
We furthermore hypothesize that fitting a surrogate to evaluated configurations is instead a learning-to-rank (L2R) problem~\citep{burges2005learning}. The evaluation criterion for HPO is the performance of the top-ranked configuration. In contrast, the regression loss measures the surrogate's ability to estimate all observed performances and does not pay any special consideration to the top-performing configuration(s). We propose that BO surrogates must be learned to estimate the ranks of the configurations with a special emphasis on correctly predicting the ranks of the top-performing configurations.

Unfortunately, the current BO machinery cannot be naively extended for L2R, because Gaussian Processes (GP) are not directly applicable to ranking. In this paper, we propose a novel paradigm to train probabilistic surrogates for learning to rank in HPO with neural network ensembles \footnote{Our code is available in the following repository: \url{https://github.com/releaunifreiburg/DeepRankingEnsembles}}. Our networks are learned to minimize L2R listwise losses~\citep{Cao07_Learning}, and the ensemble's uncertainty estimation is modeled by training diverse networks via the \textit{Deep Ensemble} paradigm~\citep{Lakshminarayanan2017_DeepEnsembles}. While there have been a few HPO-related works using flavors of basic ranking losses~\citep{Bardenet2013_Scot,10.5555/3524938.3525892,ozturk-zero2022}, ours is the first systematic treatment of HPO through a methodologically-principled L2R formulation. To achieve state-of-the-art HPO results, we follow the established practice of transfer-learning the ranking surrogates from evaluations on previous datasets~\citep{Wistuba2021_FSBO}. Furthermore, we boost the transfer quality by using dataset meta-features as an extra source of information~\citep{jomaa2021dataset2vec}.


We conducted large-scale experiments using HPO-B~\citep{pineda-hpob2021}, the largest public HPO benchmark and compared them against 12 state-of-the-art HPO baselines. We ultimately demonstrate that our method Deep Ranking Ensembles (DRE) sets the new state-of-the-art in HPO by a statistically-significant margin. This paper introduces three main technical contributions:

%We are guided by a simple research question: what modifications can we add to Deep Ensemble to achieve state-of-the-art HPO? Here, we argue that for Deep Ensembles to achieve state-of-the-art performance in HPO need $(i)$ pre-training, $(ii)$ task contextualization and $(iii)$ predictions on ranking space. The first ingredient, pre-training, is motivated based on recent evidence that meta-learning and pre-training helps to learn powerful surrogates~\citep{Wang2021_Automatic, Wistuba2021_FSBO}. Secondly, task contextualization arises as an important element to leverage the meta-learning and avoid the network collapses to predict averages over pre-training tasks. It has shown improvements over state-of-the-art transfer HPO methods~\citep{jomaa2021transfer} as they are allow learning landmark task-feature extractors. Here a task refers to the optimization of the hyperparameters of a Machine learning algorithm using a dataset~\citep{Pineda2021_HPOB}. Finally, the predictions on ranking space are necessary to add invariance to the task scale as suggested in previous work~\citep{Feurer2018_RGPE, Bardenet2013_Collaborative, Salinas2020_Quantile}. For instance, the average performance in a Multi-class problem might differ depending on the number of classes. 

%We combine the previous observations and introduce Deep Ranker Ensembles. After a large scale evaluation and comparsion agains a broad set of baselines, we find that Deep Ensembles can achieve state-of-the-art performance. Although all the components of the model are justified on evidence on previous work, we ablate each of the components 
%Inspired by the previous obervarions, in this paper we have the following contributions:

\begin{itemize}

    \item We introduce a novel neural network BO surrogate (named Deep Ranking Ensembles) optimized with Learning-to-Rank (L2R) losses;
    %\item Our ranking ensemble surrogate outperforms Gaussian Processes in HPO in a large-scale experimental protocol; 
    \item We propose a new technique for meta-learning our ensemble surrogate from large-scale public meta-datasets;
    \item Deep Ranking Ensembles achieve the new state-of-the-art in HPO, demonstrated through a very large-scale experimental protocol.
    %\item We present experiments to highlight the importance the module of DRE.
\end{itemize} 

