\section{Experiments and Results}
\label{section:experiments} 

\subsection{Motivating Example}

We demonstrate our DRE with 10 base models on a simple sinusoid function $y = \sin(\frac{x+\pi}{2})$ for $x \in [-10,10]$ sampled with a step size of $0.1$ in equally spaced intervals. Further details on the architecture are explained in Section \ref{section:experimental_setup_surrogate}. In Figure \ref{figure:bo_steps}, we conduct BO with a variant of DRE without meta-learning, and we start the HPO with 3 initial random observations. We observe that a BO procedure with the Expected Improvement acquisition reaches an optimum after 8 observations. 

\begin{figure}[]
\centerline{\includegraphics[width=1.\linewidth]{figures/bo_steps_example_case2.pdf}}
\caption{BO Steps Example with a Random Initialized DRE. EI is scaled and shifted for clarity.} 
\label{figure:bo_steps}
\end{figure}


\begin{figure}[]
\centerline{\includegraphics[width=1.\linewidth]{figures/modules_outputs_example_case2.pdf}}
\caption{Understanding the outputs of DRE's modules.} 
\label{figure:modules_outputs}
\end{figure}


Furthermore, we plot the scorers' and rankers' outputs of the second BO step in Figure \ref{figure:modules_outputs}. The analysis illustrates that the distributions of the scorers' outputs have different ranges because the loss function in Equation \ref{eq:deeprank} models only the target rank, but not the scale of the target. However, the outputs of the rankers display similar distributions in the rank space, which is more adequate for computing the ranks' uncertainties. Moreover, the rank distributions differ in certain regions of the search space, enabling BO to conduct exploration. 

To showcase the power of transfer-learning, we meta-learn DRE on 5 auxiliary tasks, corresponding to different sinusoidal functions $y = \sin(\frac{x+\pi}{2} + \beta)$ with varying $\beta \in \{11,..,15\}$, as illustrated in Figure~\ref{fig:meta_learning_example} (left). Subsequently, we deploy a meta-learned DRE surrogate on a test task (blue line with $\beta=8$) which was not part of the meta-training set. Figure~\ref{fig:meta_learning_example} reveals that DRE directly discovers a global optimum within one BO step (4 total observations). The success is attributed to the fact that the surrogate has been meta-learned to recognize sinusoidal shapes given the 3 initial observations in green, as is clearly shown by the acquisition in Figure~\ref{fig:meta_learning_example} (right).


%\begin{wrapfigure}{r}{0.6\textwidth}
\begin{figure}
\begin{center}
\includegraphics[width=0.8\linewidth]{figures/meta_learning_example_2.pdf}
  \end{center}
  \caption{Meta-train and Meta-test Tasks (left) for optimizing the function. The meta-learned DRE finds the optimum in one step (right).}
  \label{fig:meta_learning_example}
\end{figure}
%\end{wrapfigure}



\subsection{Datasets and Baselines}
\label{section:datasets_and_baselines}
We base our experiments on HPO-B~\citep{pineda-hpob2021}, the largest public benchmark for HPO. It contains 16 search spaces, each of which comprises a meta-train, meta-test, and meta-validation split. Every split is a set of datasets, and for every dataset, the benchmark contains the validation errors of evaluated hyperparameter configurations. The benchmark also includes the results of several HPO methods run in those datasets, including transfer and non-transfer algorithms~\footnote{Available in \url{https://github.com/releaunifreiburg/HPO-B}}. Moreover, we generated new results for three additional state-of-the-art baselines (GCP, HEBO, and DKLM) that are not released by HPO-B. The benchmark provides 5 sets of 5 initial random seeds for every task in the meta-test split (86 in total). We use the meta-test datasets to compare the performance of the Deep Ranker Ensembles against the baselines. Specifically, our non-transfer HPO baselines are listed below:

\begin{itemize}
    \item \textbf{Random Search (RS)} ~\citep{Bergstra2012_Random} is a simple yet strong baseline that selects a random configuration at every step.
    \item \textbf{Gaussian Processes (GP)}~\citep{Snoek2012_Practical} model the response function by computing the posterior distribution of functions induced by the observed data.
    \item \textbf{DNGO}~\citep{Snoek2015_DNGO} uses a neural network that models the uncertainty with a Bayesian linear regression on the last network layer.
    \item \textbf{BOHAMIANN}~\citep{Springenberg2016_BOHAMIANN} is also a Bayesian neural network that performs Bayesian inference via Hamiltonian Monte Carlo. 
    \item \textbf{Deep-Kernel Gaussian Processes (DKGP)}~\citep{wilson-icai16a} learn a latent representation of the features that are fed to a GP kernel function. 
    \item \textbf{HEBO}~\citep{Imani2020_HEBO} is a state-of-the-art Bayesian optimization method. It combines input and output transformations and a multi-objective acquisition function.  We use the implementation contained in the original repository.\footnote{Available in \url{https://github.com/huawei-noah/HEBO}}
\end{itemize}


Transfer HPO methods use the evaluations of the tasks included in the meta-train split to meta-learn surrogates, that are subsequently applied for HPO on the meta-test tasks within the same search space. We consider the following baselines: 

\begin{itemize}
    \item \textbf{TST}~\citep{Wistuba2016_Twostage} constructs an ensemble of Gaussian Processes aggregated with a kernel-weighted average. Alternatively, \textbf{TAF} builds an ensemble of acquisition functions. 
    \item \textbf{RGPE}~\citep{Feurer2018_RGPE} trains a Gaussian Process per each meta-train task and then combines for a new task through a weighting scheme, which accounts for the ranking performance of every base GP model.
    \item \textbf{FSBO}~\citep{Wistuba2021_FSBO} pre-trains a Deep Kernel Gaussian Process using meta-train tasks and then fine-tunes the parameters when observations for new tasks are available.
    \item \textbf{GCP}~\citep{Salinas2020_Quantile} pre-trains a neural network to predict the residual performance on the auxiliary tasks and applies Gaussian Copulas to combine results for a new task.
    \item \textbf{DKLM}~\citep{jomaa2021transfer} adds a Deep Set as task contextualization on top of FSBO. We use the same hyperparameters as suggested in the original paper. 
\end{itemize}






\subsection{DRE-Experimental Setup}

The meta-feature extractor $z$ is based on the Deep Set architecture proposed by \citet{jomaa2021dataset2vec} with five hidden layers and 32 neurons per layer. The ensemble of scorers is composed of 10 MLPs with identical architectures: four layers and 32 neurons that we selected using the meta-validation split from HPO-B. We meta-learn DRE for 5000 epochs with Adam optimizer, learning rate 0.001 and batch size 100. Every element of the batch is a list of 100 elements. We select 20\% of the samples in each list as input to the meta-feature extractor. During meta-test in every BO iteration, we update the pre-trained weights for 1000 epochs. For DRE-RI, we initialize randomly the scorers and train them for 1000 epochs using Adam Optimizer with a learning rate of 0.02. Every epoch, we use 20\% of the observations to feed the meta-feature extractor.



\subsection{Research Hypothesis and Experimental Results}

\begin{figure}[htb!]
\centering

\begin{subfigure}[b]{0.37\textwidth}
      \centering
      \caption{Transfer Methods}
      \includegraphics[width=1.0\textwidth]{figures/transfer_results_aggregated_rank_rebuttal.pdf}
      %\label{fig:transfer_results}
\end{subfigure} 
%\hfill
\begin{subfigure}[b]{0.37\textwidth}
      \centering
      \caption{Non-Transfer Methods}
      \includegraphics[width=1.0\textwidth]{figures/non_transfer_results_aggregated_rank_rebuttal.pdf}
      %\label{fig:non_transfer_results}
\end{subfigure} 
%\hfill
 \caption{Results for Transfer and Non-transfer methods.  }
    \label{fig:average_rank_results}
\end{figure}



\textbf{Hypothesis 1.} \textit{Deep Ranking Ensembles (DRE) achieve state-of-the-art results in transfer HPO.}

%\textbf{Experiment 1.}
We compare against the transfer HPO baselines listed in Section \ref{section:datasets_and_baselines} and report the average ranks across all the tasks in the meta-test split of all the HPO-B search spaces. Our protocol uses 5 initial configurations plus 100 BO iterations across 16 search spaces (the default HPO-B protocol). Our method uses meta-features~\citep{jomaa2021dataset2vec} and the scorer parameters are fine-tuned after each BO observation.

%\textbf{Results 1.}
Figure \ref{fig:average_rank_results} (left) shows that DRE clearly outperforms all baselines over 100 BO iterations based on the rank among the HPO methods averaged among 86 datasets and 5 runs. We compute the critical difference diagram~\citep{10.5555/1248547.1248548} for 25, 50, and 100 iterations, and show the statistical significance of the results in Figure \ref{fig:cd_transfer} (Appendix~\ref{appendix:additional_results}). HEBO is not a transfer HPO method but is presented as a reference. These results demonstrate the advantage of training neural ensembles with L2R since our method outperforms other rivals which also meta-train neural networks (FSBO, DKLM), or ensembles of neural networks (TST, TAF, RGPE). DRE also attains competitive results in individual search space, as shown in Figure \ref{fig:non_transfer_results_per_space_rank}, at Appendix \ref{appendix:additional_results}.

\textbf{Hypothesis 2.} \textit{The randomly-initialized DRE performs competitively in non-transfer HPO.}

%\textbf{Experiment 2.}
We test the hypothesis by comparing the performance of DRE against the non-transfer baselines mentioned in Section~\ref{section:datasets_and_baselines}. Similar to Experiment 1, we compute the average rank over 100 BO iterations, aggregating across all the meta-test tasks of all the search spaces in HPO-B.

%\textbf{Results 2.}
The results of Figure \ref{fig:average_rank_results} (right) show that a random initialized DRE (i.e. non meta-learned) is still a competitive surrogate for HPO. It exhibits good performance for up to 30 iterations compared to the other baselines and is second only to HEBO (notice our meta-learned DRE actually outperforms HEBO, Figure \ref{fig:average_rank_results} (left)). This demonstrates the usefulness of deep ensembles with L2R as general-purpose HPO surrogates. Interestingly, DRE outperforms other surrogates using neural networks, such as BOHAMIANN, DNGO, and DKGP. We present the statistical significance of the results after 25, 50, and 100 BO iterations in Figure \ref{fig:cd_non_transfer}.

%We hypothesize that the Deep Rankers need contextual information to attain better performance. We conduct ablation experiments to demonstrate the importance of the Deep Set module in the architecture. This hypothesis is inspired by recent evidence that context-aware surrogates achieve improved performance \cite{jomaa2021transfer, Wei2019_Transferable}.



\textbf{Hypothesis 3.} \textit{A weighted list-wise ranking loss is the best L2R strategy for DRE.}

%\textbf{Experiment 3.} 
We test DRE (meta-learned) with three different L2R losses: point-wise, pair-wise, and list-wise (weighted and non-weighted) ranking losses. Additionally, we compare to a surrogate predicting the performance in the original scale using Mean Squared Error as loss, i.e. a regression. Moreover, we compare the performance to a DRE trained with a regression loss. We omit the meta-features from all variants to avoid confounding factors from the analysis and use Expected Improvement as the acquisition function.

%\textbf{Results 3.} 
The results in Figures \ref{fig:loss_ablation} and \ref{fig:cd_ranking_loss} (Appendix~\ref{appendix:additional_results}) show the advantage of the list-wise ranking losses over the other type of ranking losses. Moreover, the results highlight the advantage of list-weighted ranking losses, as it attained the best performance over the average rank among 100 BO iterations. Additionally, we observe that pairwise-losses also give a boost in performance compared to point-wise estimations. The message is: "Any L2R loss is better than the regression one".


\begin{figure}

\begin{subfigure}{.32\textwidth}
  \centering
  \caption{Ranking Loss}
  \includegraphics[width=1\linewidth]{figures/ranking_loss_ablation_aggregated_rank_rebuttal.pdf}
  \label{fig:loss_ablation}
\end{subfigure}%
\begin{subfigure}{.32\textwidth}
  \centering
  \caption{Meta-features}
  \includegraphics[width=1\linewidth]{figures/deep_set_ablation_aggregated_rank_rebuttal.pdf}
  %\caption{}
  \label{fig:deep_set_ablation}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
  \caption{Acquisition Function}
  \includegraphics[width=1\linewidth]{figures/acqf_ablation_aggregated_rank_rebuttal.pdf}
  \label{fig:acqf_ablation}
\end{subfigure}
\caption{Results after testing our hypothesis 3-5. }
\label{fig:fig}
\end{figure}



\textbf{Hypothesis 4.} \textit{Meta-features help the transfer HPO performance of DRE.}


%\textbf{Experiment 4.}
We evaluate DRE with and without the meta-features extracted by the DeepSet module~\citep{jomaa2021dataset2vec}, ablating the scenarios with and without meta-learning. Again we use all 16 search spaces from HPO-B for 100 BO iterations, starting with 5 random initial configurations. DRE uses the weighted list-wise loss, and Expected Improvement as the acquisition.

%\textbf{Results 4.}
Figure \ref{fig:deep_set_ablation} shows the performance obtained with meta-features (F) considering meta-learning (M) and fine-tuning (T). A missing capital letter in the label stands for an experiment without that aspect (e.g. no M means no meta-learning, etc). The results indicate that the meta-features help DRE achieve better performance, both with and without meta-learning. The results also highlight that fine-tuning (i.e. updating the scorer network's parameters on the target tasks after each BO step) the meta-learned surrogate is important for achieving the best HPO performance. Further evidence of the significance of these results is showcased in Figure~\ref{fig:cd_meta_features} (Appendix~\ref{appendix:additional_results}).



\textbf{Hypothesis 5.} \textit{Expected Improvement is the best acquisition function for BO with DRE.}

%\textbf{Experiment 5.} 
We run experiments to address how DRE performs with different acquisition functions, which use DRE's estimated rank uncertainty to explore the search space with Bayesian Optimization. Concretely, we ablate the Upper Confidence Bound (UCB) and Expected Improvement (EI) acquisitions. Additionally, we added Average Rank (Avg) which simply recommends the configuration with the highest estimated average rank, without using the posterior variance of the rank. We also add Random Search as a reference baseline. Further details on how we apply acquisitions in the BO loop are discussed in Appendix \ref{appendix:bo_DRE}. In this experiment, we use meta-features and weighted list-wise ranking losses. 

%\textbf{Results 5.} 
The results in Figures \ref{fig:acqf_ablation} and \ref{fig:cd_acqf} (Appendix~\ref{appendix:additional_results}) demonstrate that EI is the best choice for the acquisition function. As UCB and EI attained overall better performances than the simple average rank (no uncertainty), we conclude the uncertainties computed by DRE are effective in exploring the search space. 



\subsection{Discussion on DRE hyperparameters}


\begin{wrapfigure}{r}{0.33\textwidth}
\vspace{-0.2cm}
  \begin{center}
    \includegraphics[width=1.\linewidth]{figures/scorer_size_aggregated_rank_rebuttal.pdf}
  \end{center}
  \vspace{-0.3cm}
  \caption{Average Rank on the meta-validation split from HPO-B.}
 \vspace{-0.8cm}
  \label{fig:ablation_scorer_size}
\end{wrapfigure}

Given that DRE achieves state-of-the-art results across all the 16 search spaces (see Figure \ref{fig:transfer_results_per_space_rank} in Appendix~\ref{appendix:additional_results}) of HPO-B by using the same configuration (e.g. number of layers for the scorers, number of layers for meta-feature extractor), we assume our settings (hyper-hyperparameters) are applicable straightforwardly to new search spaces. Such a generalization of the hyper-hyperparameters is desirable for any HPO method and liberates practitioners and researchers from having to tune DRE hyper-hyperparameters. In Figure \ref{fig:ablation_scorer_size} we show an ablation study comparing the performance of DRE for different numbers of layers (2, 3, 4), and different numbers of neurons per layer (16, 32, 64) on all the tasks of the meta-validation split from HPO-B. Given the critical difference diagram in Figure \ref{fig:cd_score}, we observe the performance does not change significantly when we vary any of these hyper-hyperparameters. However, we notice that the depth of the scorer is slightly more important to tune than the number of neurons per layer. We also notice that even an expressive ensemble of scorers (32x4) is able to generalize well on the meta-test split, as we have shown in our previous experiments. 

