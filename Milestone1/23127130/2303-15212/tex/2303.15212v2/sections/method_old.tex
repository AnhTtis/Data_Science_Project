
\section{Deep Ranker Ensembles (DRE)}

\subsection{Architecture}
\label{section:architecture}

The objective of Deep Ranker Ensembles is to produce mean $\mu$ and standard deviation estimations $\sigma$ to build predictive distribution on the rank $\mathcal{N}(r | \mu, \sigma)$. First, consider that the input is the set of observations, so-called support set, $\mathcal{D}_s =\{(x^{(s)}_1, y^{(s)}_1),..., (x^{(s)}_{M_s}, y^{(s)}_{M_s})\}$, and set of query points $\mathcal{X}_q = \{x_1,...,x_{M_q} \}$. The architecture comprises five components (rectangles on Figure \ref{figure:DREArchitecture}):

\begin{itemize}
    \item A \textbf{Deep Set} that encodes the support set $\mathcal{D}_s$ into a representation $z = f(\mathcal{D}_s; \psi)$ using a Multi-layer Perceptron $f$ with parameters $\psi = \{ \psi_1, \psi_2\}$. More specifically, $z=h_2 \left( \frac{1}{{M_s}} \sum_j h_1 (x_j^{(s)}, y_j^{(s)}; \psi_1 );  \psi_2 \right)$.
    \item A \textbf{Concatenator} that combines the query point $x_j \in \mathcal{X}_q$ and the output of the Deep Set to obtain $x_j \oplus z$, where $\oplus$ denotes the concatenation operation.
    \item \textbf{MLPs Scorers} is a set of  Multilayer Perceptron $g_i, i \in \{1,...,N \}$ with parameters $\phi_i$ to provide a scoring output $s_{i,j}= g_i(x_j \oplus z; \phi_i)$. In Section \ref{section:pretraining}, we explain how to train independently the scorers.
    \item A \textbf{Ranker} outputs the rank of every query point based on the MLP Scorer. Denoting the scores for the query set of the $i$-th MLP scorers as $\mathcal{S}_q = \{s_{i,1}, ...,s_{i,M_q} \}$, we can formalize the ranker operation on $s_{i,j} \in \mathcal{S}_q $ as: $\hat{r}_{i,j}=\mathrm{Rank}(s_{i,j};\mathcal{S}_q ) = \sum_{s \in \mathcal{S}_q} \mathbb{I}(s \geq s_{i,j})$. Thus, the lower the rank, the better the instance point.
    
    \item An \textbf{Aggregator} combines the ranks output per instance $x_j$ to obtain the mean and standard deviation of the rank: $ \mu(x_j)= \mu_j = \frac{1}{N}\sum^N_{i=1} r_{i,j}$ and $\sigma^2(x_j) = \sigma^2_j = \frac{1}{N} \sum^N_{i=1} (r_{i,j}-\mu_j)^ 2$. 
\end{itemize}

Figure \ref{figure:DREArchitecture} depicts the above-explained architecture. Notice that only two components, the \textit{MLP Scorer} and the \textit{DeepSet} have learnable parameters. 

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.8\linewidth]{figures/DREArchitecture.pdf}}
\caption{Deep Ranker Ensembles Architecture. }
\label{figure:DREArchitecture}
\end{figure}

%\begin{wrapfigure}{r}{0.5\textwidth}
%  \begin{center}
%    \includegraphics[width=1.\linewidth]{figures%/DREArchitecture.pdf}
%  \end{center}
%  \caption{Types of Priors}
%  \label{fig:types_of_priors}
%\end{wrapfigure}


\subsection{Pre-Training}
\label{section:pretraining}

Our proposed surrogate DRE predicts performance is on the ranking space, thus the objective function should induce an output mean $\mu_j$ that match the true rank $r_j$. We train every model independently, therefore we define this function as used in every independent model training, and we drop the model index $i$. 
In Section \ref{section:experiments} we explore three types of losses: point-wise, pair-wise, and list-wise losses, and provide evidence for the list-wise losses to be the best choice.  To formally define the list-wise loss, consider a task sampled from a task distribution $\mathcal{D}_t= \{ (x^{(t)}_i, y^{(t)}_i)\}^{M_t}_{i=1}$ and denote the list of the ordered target values $L(\mathcal{D}_t)=\{l_1, ...,l_{M_t} \}$, such that $y_{l_1}>y_{l_2}>...>y_{l_{M_t}}$. We define a function $\pi(j;L(\mathcal{D}_t))$ that returns the index of $j$ in $L(\mathcal{D}_t)$. If $s_j= g (x_j \oplus z; \phi)$ is the output of the scorer function, then the probability of the permutation represented by the list $L_t$ given a scorer $\phi$  is computed by \cite{Cao07_Learning}:

\begin{equation}
\label{equation:list_lik}
    p(L(\mathcal{D}_t); \phi) = \prod^{M_t}_{j=1} \frac{e^{{s_{\pi(j;L(\mathcal{D}_t))}}}}{\sum^{M_t}_{k=j} e^{s_{\pi(k;L_t)}}}
\end{equation}

We can leverage the performance evaluations of a Machine Learning algorithm on auxiliary datasets $\mathcal{D}= \{ \mathcal{D}_1, ..., \mathcal{D}_T \}$ by using a loss function based on logarithms Equation \ref{equation:list_lik} becomes:

\begin{equation}
\label{equation:loss_function}
    \mathcal{L} = \sum_{\mathcal{D}_t \in \mathcal{D}} \sum^{M_t}_{j=1} w(j) \frac{e^{{s_{\pi(j;L(\mathcal{D}_t))}}}}{\sum^{M_t}_{k=j} e^{s_{\pi(k;L(\mathcal{D}_t))}}}
\end{equation}

Where we have added a weighting mechanism that assigns higher relevance to the top ranks $w(j)=\frac{1}{log(j+1)}$ . This weighting function was used by \cite{chen2017top} to increase the performance of list-wise losses, and we show in our experiments that it helps to attain a better performance with DRE. As this loss involves the parameters of the \textit{MLP scorer} and the \textit{Deep Set} through the scorer output $s_j$, we can pre-train the parameters of the ranker $\phi$ and the Deep Set $\psi$ by performing batch gradient descent on the Equation \ref{equation:loss_function} \cite{Xia2008_Listwise}. This algorithm is executed in parallel for training every model of the ensemble. The loss function does not inforce a similar range among the ensembles, because it just cares about the order. Therefore, the output of the independently trained models can be in different ranges. This motivates the use of the \textit{Ranker} component, because it makes the output of the models comparable and ready to be used by the \textit{Aggregator}.

\begin{algorithm}[ht]
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Set of datasets $\mathcal{D}$, number of iterations $K$, maximum number of observations $B$}
\Output{Deep Ranker parameters $\psi, \phi$}
 Initialize the MLP scorer $s$ \;
 % $p(x_1,\dots,x_{n+m},y_1,\dots,y_{n+m})$\;
 \For{$j\gets1$ \KwTo $K$}{
  Sample dataset index $i \in U(0,|\mathcal{D}|)$ \;
  %Sample support set $\mathcal{D}_s = \{(x^{(s)}_i,y^{(s)}_i)\}$ from $\mathcal{D}_i$\;
  Sample support set $\mathcal{D}_s $ from $\mathcal{D}_i$ and query set $\mathcal{D}_q $ from $\mathcal{D} $ \;
  Compute Deep Set representation of the support set $z = f(\mathcal{D}_s; \psi)$  \;
  %$x_1,\dots,x_{n+m},y_1,\dots,y_{n+m} \sim p(\dataset{})$\;
  Compute score for samples in query set: $s_{j}= g(x_j \oplus z; \phi)$ \;
  Compute loss $\mathcal{L}$ in Equation \ref{equation:loss_function} \;
  Update parameters $\psi, \phi$ with stochastic gradient descent on $\mathcal{L}$\;
 }
 \caption{Pre-Training Deep Rankers}
 \label{alg:prior-fitting}
\end{algorithm}


\subsection{Bayesian Optimization with Deep Ranker Ensembles}

Once the Deep Ensembles are trained, we aggregate the predictions for an input $x$ following the procedure explained in Section \ref{section:architecture} to obtain $\mu(x), \sigma(x)$ and conditioning to a set of observations $\mathcal{D}_s$. This output can be fed in several type of acquisition functions an decide for the next point $x$ to observe from set of pending points to evaluate $\mathcal{X}$. Specifically, we consider:

\begin{itemize}
    \item \textbf{Average Rank}: $\alpha(x_j) =   \mu(x_j)$
    \item \textbf{Lower Confidence Bound}: $\alpha(x_j) = \mu(x_j) - \beta \cdot \sigma(x_j)$
    \item \textbf{Expected Improvement}: $\alpha(x_j) =  -\int_{r} \left(\mu(x_k)-r \right)_{+} \mathcal{N}\left(r;\mu(x_j), \sigma(x_j)\right)$
\end{itemize}

Where $\beta$ is a factor that trades of exploitation and exploration and $x_i$ is the best observed configuration, i.e. $k = \argmax_{i \in \{1,...,|\mathcal{D}_s|\}} y_i$. The previous formulation assume a minimization, thus to choose the next query point you apply: $x = \argmin_{x_j \in \mathcal{X}} {\alpha(x_j})$.


\begin{algorithm}[ht]
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{A prior distribution over datasets $p(\mathcal{D})$, initial observations $D=\{ (x_1, y_1),...,(x_N, y_N)\}$, pending points $\mathcal{X}$, number of BO iterations $K$, black-box function to optimize $h$}
\Output{Best observed configuration $x_*$}
 Train ensemble of MLP scorers following Algorithm \ref{alg:prior-fitting} and prior $p(\mathcal{D})$;
 
 % $p(x_1,\dots,x_{n+m},y_1,\dots,y_{n+m})$\;
 \For{$j\gets1$ \KwTo $K$}{
  Suggest next candidate $x = \argmin_{x_j \in \mathcal{X}} \alpha (x_j, D)$ \;
  Observe response $y=f(x)$ \;
  Update history $D= D \cup \{ (x, y)\}$\;

 }
 Return top performing configuration: $\text{argmax}_{(x_i,y_i) \in D} y_i$
 \caption{Bayesian Optimization with DRE}
 \label{alg:bo_with_pfns}
\end{algorithm}



