\section{Conclusion}

The presented empirical results based on a very large-scale experimental protocol provide strong evidence of the state-of-the-art performance of deep ensembles optimized through learning to rank. We demonstrated that our method outperforms a large number of 11 baselines in both transfer and non-transfer HPO. In addition, we validated the design choices of our method through detailed ablations and analyses. Particularly, the results indicate the power of meta-learning surrogates from evaluations on other datasets. Overall, we believe that this paper will set a new trend in the HPO community for moving away from regression-learned surrogate functions in Bayesian Optimization.
Finally, our surrogate DRE opens up an effective way to improve the HPO performance in different sub-problems, such as multi-fidelity HPO, multi-objective HPO, or neural architecture search.

\newpage

%\textbf{Reproducibility Statement:} To promote reproducibility, we release our source code in an anonymized repository. The source code of the baselines is also included in the repository, and we reference the original implementations whenever applicable. All the meta-datasets are publicly available and correspondingly referenced. Our code is available in the following repository: \url{https://anonymous.4open.science/r/Deep-Ranking-Ensembles-F159}




%\textbf{Limitations.} Our surrogate fine-tunes $M$ neural networks after each BO step, however, this process can be easily parallelized.



\section*{Acknowledgements}
This research was funded by the Deutsche Forschungsgemeinschaft (DFG,
German Research Foundation) under grant number 417962828 and grant INST 39/963-1 FUGG
(bwForCluster NEMO). In addition, Josif Grabocka acknowledges the support of the BrainLinks-
BrainTools Center of Excellence.