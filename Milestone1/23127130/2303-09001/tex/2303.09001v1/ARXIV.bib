
@article{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {https://cdn.openai.com/papers/gpt-4.pdf},
	author = {OpenAI},
	year = {2023},
}

@inproceedings{laurencon_bigscience_2022,
	title = {The {BigScience} {ROOTS} {Corpus}: {A} 1.{6TB} {Composite} {Multilingual} {Dataset}},
	shorttitle = {The {BigScience} {ROOTS} {Corpus}},
	url = {https://openreview.net/forum?id=UoEw6KigkUn},
	abstract = {As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.},
	language = {en},
	urldate = {2023-03-15},
	author = {Laurençon, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Moral, Albert Villanova del and Scao, Teven Le and Werra, Leandro Von and Mou, Chenghao and Ponferrada, Eduardo González and Nguyen, Huu and Frohberg, Jörg and Šaško, Mario and Lhoest, Quentin and McMillan-Major, Angelina and Dupont, Gérard and Biderman, Stella and Rogers, Anna and Allal, Loubna Ben and Toni, Francesco De and Pistilli, Giada and Nguyen, Olivier and Nikpoor, Somaieh and Masoud, Maraim and Colombo, Pierre and Rosa, Javier de la and Villegas, Paulo and Thrush, Tristan and Longpre, Shayne and Nagel, Sebastian and Weber, Leon and Muñoz, Manuel Romero and Zhu, Jian and Strien, Daniel Van and Alyafeai, Zaid and Almubarak, Khalid and Chien, Vu Minh and Gonzalez-Dios, Itziar and Soroa, Aitor and Lo, Kyle and Dey, Manan and Suarez, Pedro Ortiz and Gokaslan, Aaron and Bose, Shamik and Adelani, David Ifeoluwa and Phan, Long and Tran, Hieu and Yu, Ian and Pai, Suhas and Chim, Jenny and Lepercq, Violette and Ilic, Suzana and Mitchell, Margaret and Luccioni, Sasha and Jernite, Yacine},
	month = oct,
	year = {2022},
}

@misc{mitchell_measuring_2023,
	title = {Measuring {Data}},
	url = {http://arxiv.org/abs/2212.05129},
	doi = {10.48550/arXiv.2212.05129},
	abstract = {We identify the task of measuring data to quantitatively characterize the composition of machine learning data and datasets. Similar to an object's height, width, and volume, data measurements quantify different attributes of data along common dimensions that support comparison. Several lines of research have proposed what we refer to as measurements, with differing terminology; we bring some of this work together, particularly in fields of computer vision and language, and build from it to motivate measuring data as a critical component of responsible AI development. Measuring data aids in systematically building and analyzing machine learning (ML) data towards specific goals and gaining better control of what modern ML systems will learn. We conclude with a discussion of the many avenues of future work, the limitations of data measurements, and how to leverage these measurement approaches in research and practice.},
	urldate = {2023-03-15},
	publisher = {arXiv},
	author = {Mitchell, Margaret and Luccioni, Alexandra Sasha and Lambert, Nathan and Gerchick, Marissa and McMillan-Major, Angelina and Ozoani, Ezinwanne and Rajani, Nazneen and Thrush, Tristan and Jernite, Yacine and Kiela, Douwe},
	month = feb,
	year = {2023},
	note = {arXiv:2212.05129 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{hutchinson_towards_2021,
	title = {Towards {Accountability} for {Machine} {Learning} {Datasets}: {Practices} from {Software} {Engineering} and {Infrastructure}},
	shorttitle = {Towards {Accountability} for {Machine} {Learning} {Datasets}},
	url = {http://arxiv.org/abs/2010.13561},
	doi = {10.48550/arXiv.2010.13561},
	abstract = {Rising concern for the societal implications of artificial intelligence systems has inspired demands for greater transparency and accountability. However the datasets which empower machine learning are often used, shared and re-used with little visibility into the processes of deliberation which led to their creation. Which stakeholder groups had their perspectives included when the dataset was conceived? Which domain experts were consulted regarding how to model subgroups and other phenomena? How were questions of representational biases measured and addressed? Who labeled the data? In this paper, we introduce a rigorous framework for dataset development transparency which supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields a set of documents that facilitate improved communication and decision-making, as well as drawing attention the value and necessity of careful data work. The proposed framework is intended to contribute to closing the accountability gap in artificial intelligence systems, by making visible the often overlooked work that goes into dataset creation.},
	urldate = {2023-03-15},
	publisher = {arXiv},
	author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
	month = jan,
	year = {2021},
	note = {arXiv:2010.13561 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Databases, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{von_werra_evaluate_2022,
	title = {Evaluate \& {Evaluation} on the {Hub}: {Better} {Best} {Practices} for {Data} and {Model} {Measurements}},
	shorttitle = {Evaluate \& {Evaluation} on the {Hub}},
	url = {http://arxiv.org/abs/2210.01970},
	doi = {10.48550/arXiv.2210.01970},
	abstract = {Evaluation is a key part of machine learning (ML), yet there is a lack of support and tooling to enable its informed and systematic practice. We introduce Evaluate and Evaluation on the Hub --a set of tools to facilitate the evaluation of models and datasets in ML. Evaluate is a library to support best practices for measurements, metrics, and comparisons of data and models. Its goal is to support reproducibility of evaluation, centralize and document the evaluation process, and broaden evaluation to cover more facets of model performance. It includes over 50 efficient canonical implementations for a variety of domains and scenarios, interactive documentation, and the ability to easily share implementations and outcomes. The library is available at https://github.com/huggingface/evaluate. In addition, we introduce Evaluation on the Hub, a platform that enables the large-scale evaluation of over 75,000 models and 11,000 datasets on the Hugging Face Hub, for free, at the click of a button. Evaluation on the Hub is available at https://huggingface.co/autoevaluate.},
	urldate = {2023-03-15},
	publisher = {arXiv},
	author = {von Werra, Leandro and Tunstall, Lewis and Thakur, Abhishek and Luccioni, Alexandra Sasha and Thrush, Tristan and Piktus, Aleksandra and Marty, Felix and Rajani, Nazneen and Mustar, Victor and Ngo, Helen and Sanseviero, Omar and Šaško, Mario and Villanova, Albert and Lhoest, Quentin and Chaumond, Julien and Mitchell, Margaret and Rush, Alexander M. and Wolf, Thomas and Kiela, Douwe},
	month = oct,
	year = {2022},
	note = {arXiv:2210.01970 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{jernite_data_2022,
	title = {Data {Governance} in the {Age} of {Large}-{Scale} {Data}-{Driven} {Language} {Technology}},
	url = {http://arxiv.org/abs/2206.03216},
	doi = {10.1145/3531146.3534637},
	abstract = {The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.},
	urldate = {2023-03-15},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Jernite, Yacine and Nguyen, Huu and Biderman, Stella and Rogers, Anna and Masoud, Maraim and Danchev, Valentin and Tan, Samson and Luccioni, Alexandra Sasha and Subramani, Nishant and Dupont, Gérard and Dodge, Jesse and Lo, Kyle and Talat, Zeerak and Johnson, Isaac and Radev, Dragomir and Nikpoor, Somaieh and Frohberg, Jörg and Gokaslan, Aaron and Henderson, Peter and Bommasani, Rishi and Mitchell, Margaret},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03216 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, data rights, datasets, language data, technology governance},
	pages = {2206--2222},
}

@article{perrigo_exclusive_2023,
	title = {Exclusive: {The} \$2 {Per} {Hour} {Workers} {Who} {Made} {ChatGPT} {Safer}},
	shorttitle = {Exclusive},
	url = {https://time.com/6247678/openai-chatgpt-kenya-workers/},
	abstract = {A TIME investigation reveals the difficult conditions faced by the workers who made ChatGPT possible},
	language = {en},
	urldate = {2023-03-15},
	journal = {Time},
	author = {Perrigo, Billy},
	month = jan,
	year = {2023},
}

@misc{patel_inference_2023,
	title = {The {Inference} {Cost} {Of} {Search} {Disruption} – {Large} {Language} {Model} {Cost} {Analysis}},
	url = {https://www.semianalysis.com/p/the-inference-cost-of-search-disruption},
	abstract = {\$30B Of Google Profit Evaporating Overnight, Performance Improvement With H100 TPUv4 TPUv5},
	language = {en},
	urldate = {2023-03-15},
	author = {Patel, Dylan and Ahmad, Afzal},
	month = feb,
	year = {2023},
}

@misc{taori_stanford_2023,
	title = {Stanford {Alpaca}: {An} {Instruction}-following {LLaMA} model},
	url = {https://github.com/tatsu-lab/stanford_alpaca},
	publisher = {GitHub},
	author = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
	year = {2023},
	note = {Publication Title: GitHub repository},
}

@techreport{noauthor_strengthening_2023,
	title = {Strengthening and {Democratizing} the {U}.{S}. {Artificial} {Intelligence} {Innovation} {Ecosystem}: {An} {Implementation} {Plan} for a {National} {Artificial} {Intelligence} {Research} {Resource}},
	url = {https://www.ai.gov/wp-content/uploads/2023/01/NAIRR-TF-Final-Report-2023.pdf},
	month = jan,
	year = {2023},
}

@misc{ho_building_2021,
	title = {Building a {National} {AI} {Research} {Resource}: {A} {Blueprint} for the {National} {Research} {Cloud}},
	url = {https://hai.stanford.edu/sites/default/files/2022-01/HAI_NRCR_v17.pdf},
	author = {Ho, Daniel and King, Jennifer and Wald, Russell and Wan, Christopher},
	month = oct,
	year = {2021},
}

@misc{sevilla_compute_2022,
	title = {Compute {Trends} {Across} {Three} {Eras} of {Machine} {Learning}},
	url = {https://arxiv.org/abs/2202.05924v2},
	abstract = {Compute, data, and algorithmic advances are the three fundamental factors that guide the progress of modern Machine Learning (ML). In this paper we study trends in the most readily quantified factor - compute. We show that before 2010 training compute grew in line with Moore's law, doubling roughly every 20 months. Since the advent of Deep Learning in the early 2010s, the scaling of training compute has accelerated, doubling approximately every 6 months. In late 2015, a new trend emerged as firms developed large-scale ML models with 10 to 100-fold larger requirements in training compute. Based on these observations we split the history of compute in ML into three eras: the Pre Deep Learning Era, the Deep Learning Era and the Large-Scale Era. Overall, our work highlights the fast-growing compute requirements for training advanced ML systems.},
	language = {en},
	urldate = {2023-03-15},
	journal = {arXiv.org},
	author = {Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2202.05924},
}

@misc{hwang_computational_2018,
	title = {Computational {Power} and the {Social} {Impact} of {Artificial} {Intelligence}},
	url = {https://arxiv.org/abs/1803.08971v1},
	abstract = {Machine learning is a computational process. To that end, it is inextricably tied to computational power - the tangible material of chips and semiconductors that the algorithms of machine intelligence operate on. Most obviously, computational power and computing architectures shape the speed of training and inference in machine learning, and therefore influence the rate of progress in the technology. But, these relationships are more nuanced than that: hardware shapes the methods used by researchers and engineers in the design and development of machine learning models. Characteristics such as the power consumption of chips also define where and how machine learning can be used in the real world. Despite this, many analyses of the social impact of the current wave of progress in AI have not substantively brought the dimension of hardware into their accounts. While a common trope in both the popular press and scholarly literature is to highlight the massive increase in computational power that has enabled the recent breakthroughs in machine learning, the analysis frequently goes no further than this observation around magnitude. This paper aims to dig more deeply into the relationship between computational power and the development of machine learning. Specifically, it examines how changes in computing architectures, machine learning methodologies, and supply chains might influence the future of AI. In doing so, it seeks to trace a set of specific relationships between this underlying hardware layer and the broader social impacts and risks around AI.},
	language = {en},
	urldate = {2023-03-15},
	journal = {arXiv.org},
	author = {Hwang, Tim},
	month = mar,
	year = {2018},
	doi = {10.48550/arXiv.1803.08971},
}

@article{whittlestone_why_2021,
	title = {Why and {How} {Governments} {Should} {Monitor} {AI} {Development}},
	url = {https://arxiv.org/abs/2108.12427v2},
	doi = {10.48550/arXiv.2108.12427},
	abstract = {In this paper we outline a proposal for improving the governance of artificial intelligence (AI) by investing in government capacity to systematically measure and monitor the capabilities and impacts of AI systems. If adopted, this would give governments greater information about the AI ecosystem, equipping them to more effectively direct AI development and deployment in the most societally and economically beneficial directions. It would also create infrastructure that could rapidly identify potential threats or harms that could occur as a consequence of changes in the AI ecosystem, such as the emergence of strategically transformative capabilities, or the deployment of harmful systems. We begin by outlining the problem which motivates this proposal: in brief, traditional governance approaches struggle to keep pace with the speed of progress in AI. We then present our proposal for addressing this problem: governments must invest in measurement and monitoring infrastructure. We discuss this proposal in detail, outlining what specific things governments could focus on measuring and monitoring, and the kinds of benefits this would generate for policymaking. Finally, we outline some potential pilot projects and some considerations for implementing this in practice.},
	language = {en},
	urldate = {2023-03-15},
	author = {Whittlestone, Jess and Clark, Jack},
	month = aug,
	year = {2021},
}

@techreport{noauthor_blueprint_2023,
	type = {{OECD} {Digital} {Economy} {Papers}},
	title = {A blueprint for building national compute capacity for artificial intelligence},
	url = {https://www.oecd-ilibrary.org/science-and-technology/a-blueprint-for-building-national-compute-capacity-for-artificial-intelligence_876367e3-en},
	language = {en},
	number = {350},
	urldate = {2023-03-15},
	month = feb,
	year = {2023},
	doi = {10.1787/876367e3-en},
}

@misc{noauthor_carperai_nodate,
	title = {{CarperAI}},
	url = {https://carper.ai/},
	abstract = {Pushing the Limits},
	language = {en-GB},
	urldate = {2023-03-14},
	journal = {CarperAI},
}

@inproceedings{gehman_realtoxicityprompts_2020,
	address = {Online},
	title = {{RealToxicityPrompts}: {Evaluating} {Neural} {Toxic} {Degeneration} in {Language} {Models}},
	shorttitle = {{RealToxicityPrompts}},
	url = {https://aclanthology.org/2020.findings-emnlp.301},
	doi = {10.18653/v1/2020.findings-emnlp.301},
	abstract = {Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning “bad” words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.},
	urldate = {2022-12-19},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A.},
	month = nov,
	year = {2020},
	pages = {3356--3369},
}

@inproceedings{abid_persistent_2021,
	title = {Persistent {Anti}-{Muslim} {Bias} in {Large} {Language} {Models}},
	url = {https://doi.org/10.1145%2F3461702.3462624},
	doi = {10.1145/3461702.3462624},
	booktitle = {Proceedings of the 2021 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Abid, Abubakar and Farooqi, Maheen and Zou, James},
	month = jul,
	year = {2021},
}

@inproceedings{harris_exploring_2022,
	title = {Exploring the {Role} of {Grammar} and {Word} {Choice} in {Bias} {Toward} {African} {American} {English} ({AAE}) in {Hate} {Speech} {Classification}},
	url = {https://doi.org/10.1145%2F3531146.3533144},
	doi = {10.1145/3531146.3533144},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Harris, Camille and Halevy, Matan and Howard, Ayanna and Bruckman, Amy and Yang, Diyi},
	month = jun,
	year = {2022},
}

@inproceedings{wolfe_american_2022,
	title = {American == {White} in {Multimodal} {Language}-and-{Image} {AI}},
	url = {https://doi.org/10.1145%2F3514094.3534136},
	doi = {10.1145/3514094.3534136},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Wolfe, Robert and Caliskan, Aylin},
	month = jul,
	year = {2022},
}

@article{touma_tiktok_2022,
	chapter = {Technology},
	title = {{TikTok} has been accused of ‘aggressive’ data harvesting. {Is} your information at risk?},
	issn = {0261-3077},
	url = {https://www.theguardian.com/technology/2022/jul/19/tiktok-has-been-accused-of-aggressive-data-harvesting-is-your-information-at-risk},
	abstract = {Users of the video app have been warned about its data practices and links to China. Can you keep your details safe?},
	language = {en-GB},
	urldate = {2023-03-14},
	journal = {The Guardian},
	author = {Touma, Rafqa},
	month = jul,
	year = {2022},
	keywords = {Asia Pacific, Australia news, China, Digital media, Internet, Social media, Technology, TikTok},
}

@article{information_commissioners_office_ico_2022,
	title = {{ICO} fines facial recognition database company {Clearview} {AI} {Inc} more than £7.5m and orders {UK} data to be deleted},
	url = {https://ico.org.uk/about-the-ico/media-centre/news-and-blogs/2022/05/ico-fines-facial-recognition-database-company-clearview-ai-inc/},
	abstract = {The Information Commissioner’s Office (ICO) has fined Clearview AI Inc £7,552,800 for using images of people in the UK, and elsewhere, that were collected from the web and social media to create a global online database that could be used for facial recognition.

The ICO has also issued an enforcement notice, ordering the company to stop obtaining and using the personal data of UK residents that is publicly available on the internet, and to delete the data of UK residents from its systems.},
	language = {en},
	urldate = {2023-03-14},
	author = {Information Commissioner's Office},
	month = may,
	year = {2022},
	note = {Publisher: ICO},
}

@article{mccallum_meta_2022,
	chapter = {Technology},
	title = {Meta settles {Cambridge} {Analytica} scandal case for \$725m},
	url = {https://www.bbc.com/news/technology-64075067},
	abstract = {The proposed deal by Facebook's owner is the largest in a US data privacy class action, say lawyers.},
	language = {en-GB},
	urldate = {2023-03-14},
	journal = {BBC News},
	author = {McCallum, Shiona},
	month = dec,
	year = {2022},
}

@misc{scassa_designing_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Designing {Data} {Governance} for {Data} {Sharing}: {Lessons} from {Sidewalk} {Toronto}},
	shorttitle = {Designing {Data} {Governance} for {Data} {Sharing}},
	url = {https://papers.ssrn.com/abstract=3722204},
	abstract = {This paper considers data governance for data sharing through the lens of the data governance scheme proposed by Sidewalk Labs as part of its Master Innovation Development Plan (MIDP) for a ‘smart city’ development on the waterfront of Toronto, Canada. Recognizing the diverse interests in the data that might be collected in the development, the MIDP called for the creation of an Urban Data Trust (UDT) as a data governance body to address both the collection and the sharing of the novel category of ‘urban data’. This paper uses the example of urban data and the UDT to illustrate some of the challenges that are central to data governance for data sharing and offers a critique that draws upon the idea of governance of a knowledge commons. The paper identifies some of the issues that led to the failure of the UDT, and extracts key lessons.},
	language = {en},
	urldate = {2023-03-14},
	author = {Scassa, Teresa},
	month = sep,
	year = {2020},
	keywords = {data governance, data ownership, data protection, data sharing, data trust, privacy, smart cities},
}

@article{vincent_microsofts_2023,
	title = {Microsoft’s {Bing} is an emotionally manipulative liar, and people love it},
	url = {https://www.theverge.com/2023/2/15/23599072/microsoft-ai-bing-personality-conversations-spy-employees-webcams},
	abstract = {Bing’s acting unhinged, and lots of people love it.},
	language = {en-US},
	urldate = {2023-03-14},
	journal = {The Verge},
	author = {Vincent, James},
	month = feb,
	year = {2023},
}

@inproceedings{lee_deduplicating_2022,
	address = {Dublin, Ireland},
	title = {Deduplicating {Training} {Data} {Makes} {Language} {Models} {Better}},
	url = {https://aclanthology.org/2022.acl-long.577},
	doi = {10.18653/v1/2022.acl-long.577},
	abstract = {We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.As a result, over 1\% of the unprompted output of language models trained on these datasets is copied verbatim from the training data.We develop two tools that allow us to deduplicate training datasets—for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times.Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.We can also reduce train-test overlap, which affects over 4\% of the validation set of standard datasets, thus allowing for more accurate evaluation.Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets.},
	urldate = {2023-03-14},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
	month = may,
	year = {2022},
	pages = {8424--8445},
}

@misc{gao_framework_2021,
	title = {A framework for few-shot language model evaluation},
	url = {https://doi.org/10.5281/zenodo.5371628},
	publisher = {Zenodo},
	author = {Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and Phang, Jason and Reynolds, Laria and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
	month = sep,
	year = {2021},
	doi = {10.5281/zenodo.5371628},
}

@article{bowman_snli_2015,
	title = {The {SNLI} {Corpus}},
	copyright = {Dataset licensed as CC-BY-SA 4.0. Paper licensed as CC-BY-NC-SA 3.0},
	url = {http://archive.nyu.edu/handle/2451/41728},
	abstract = {The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE). We aim for it to serve both as a benchmark for evaluating representational systems for text, especially including those induced by representation learning methods, as well as a resource for developing NLP models of any kind.},
	language = {en},
	urldate = {2023-03-14},
	author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	month = jun,
	year = {2015},
	note = {Accepted: 2018-03-28T16:30:15Z
Type: dataset},
}

@misc{srivastava_beyond_2022,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	doi = {10.48550/arXiv.2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ramón Risco and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	month = jun,
	year = {2022},
	note = {arXiv:2206.04615 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{boenisch_systematic_2021,
	title = {A {Systematic} {Review} on {Model} {Watermarking} for {Neural} {Networks}},
	volume = {4},
	issn = {2624-909X},
	url = {http://arxiv.org/abs/2009.12153},
	doi = {10.3389/fdata.2021.729663},
	abstract = {Machine learning (ML) models are applied in an increasing variety of domains. The availability of large amounts of data and computational resources encourages the development of ever more complex and valuable models. These models are considered intellectual property of the legitimate parties who have trained them, which makes their protection against stealing, illegitimate redistribution, and unauthorized application an urgent need. Digital watermarking presents a strong mechanism for marking model ownership and, thereby, offers protection against those threats. This work presents a taxonomy identifying and analyzing different classes of watermarking schemes for ML models. It introduces a unified threat model to allow structured reasoning on and comparison of the effectiveness of watermarking methods in different scenarios. Furthermore, it systematizes desired security requirements and attacks against ML model watermarking. Based on that framework, representative literature from the field is surveyed to illustrate the taxonomy. Finally, shortcomings and general limitations of existing approaches are discussed, and an outlook on future research directions is given.},
	urldate = {2023-03-14},
	journal = {Frontiers in Big Data},
	author = {Boenisch, Franziska},
	month = nov,
	year = {2021},
	note = {arXiv:2009.12153 [cs]},
	keywords = {A.1, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Multimedia, I.2},
	pages = {729663},
}

@misc{gu_watermarking_2023,
	title = {Watermarking {Pre}-trained {Language} {Models} with {Backdooring}},
	url = {http://arxiv.org/abs/2210.07543},
	doi = {10.48550/arXiv.2210.07543},
	abstract = {Large pre-trained language models (PLMs) have proven to be a crucial component of modern natural language processing systems. PLMs typically need to be fine-tuned on task-specific downstream datasets, which makes it hard to claim the ownership of PLMs and protect the developer's intellectual property due to the catastrophic forgetting phenomenon. We show that PLMs can be watermarked with a multi-task learning framework by embedding backdoors triggered by specific inputs defined by the owners, and those watermarks are hard to remove even though the watermarked PLMs are fine-tuned on multiple downstream tasks. In addition to using some rare words as triggers, we also show that the combination of common words can be used as backdoor triggers to avoid them being easily detected. Extensive experiments on multiple datasets demonstrate that the embedded watermarks can be robustly extracted with a high success rate and less influenced by the follow-up fine-tuning.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Gu, Chenxi and Huang, Chengsong and Zheng, Xiaoqing and Chang, Kai-Wei and Hsieh, Cho-Jui},
	month = feb,
	year = {2023},
	note = {arXiv:2210.07543 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kirchenbauer_watermark_2023,
	title = {A {Watermark} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2301.10226},
	doi = {10.48550/arXiv.2301.10226},
	abstract = {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
	month = jan,
	year = {2023},
	note = {arXiv:2301.10226 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{aslett_review_2015,
	title = {A review of homomorphic encryption and software tools for encrypted statistical machine learning},
	url = {http://arxiv.org/abs/1508.06574},
	doi = {10.48550/arXiv.1508.06574},
	abstract = {Recent advances in cryptography promise to enable secure statistical computation on encrypted data, whereby a limited set of operations can be carried out without the need to first decrypt. We review these homomorphic encryption schemes in a manner accessible to statisticians and machine learners, focusing on pertinent limitations inherent in the current state of the art. These limitations restrict the kind of statistics and machine learning algorithms which can be implemented and we review those which have been successfully applied in the literature. Finally, we document a high performance R package implementing a recent homomorphic scheme in a general framework.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Aslett, Louis J. M. and Esperança, Pedro M. and Holmes, Chris C.},
	month = aug,
	year = {2015},
	note = {arXiv:1508.06574 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{mihara_neural_2020,
	title = {Neural {Network} {Training} {With} {Homomorphic} {Encryption}},
	url = {http://arxiv.org/abs/2012.13552},
	doi = {10.48550/arXiv.2012.13552},
	abstract = {We introduce a novel method and implementation architecture to train neural networks which preserves the confidentiality of both the model and the data. Our method relies on homomorphic capability of lattice based encryption scheme. Our procedure is optimized for operations on packed ciphertexts in order to achieve efficient updates of the model parameters. Our method achieves a significant reduction of computations due to our way to perform multiplications and rotations on packed ciphertexts from a feedforward network to a back-propagation network. To verify the accuracy of the training model as well as the implementation feasibility, we tested our method on the Iris data set by using the CKKS scheme with Microsoft SEAL as a back end. Although our test implementation is for simple neural network training, we believe our basic implementation block can help the further applications for more complex neural network based use cases.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Mihara, Kentaro and Yamaguchi, Ryohei and Mitsuishi, Miguel and Maruyama, Yusuke},
	month = dec,
	year = {2020},
	note = {arXiv:2012.13552 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{zhang_adversarial_2022,
	title = {“{Adversarial} {Examples}” for {Proof}-of-{Learning}},
	doi = {10.1109/SP46214.2022.9833596},
	abstract = {In S\&P 21, Jia et al. proposed a new concept/mechanism named proof-of-learning (PoL), which allows a prover to demonstrate ownership of a machine learning model by proving integrity of the training procedure. It guarantees that an adversary cannot construct a valid proof with less cost (in both computation and storage) than that made by the prover in generating the proof. A PoL proof includes a set of intermediate models recorded during training, together with the corresponding data points used to obtain each recorded model. Jia et al. claimed that an adversary merely knowing the final model and training dataset cannot efficiently find a set of intermediate models with correct data points. In this paper, however, we show that PoL is vulnerable to “adversarial examples”! Specifically, in a similar way as optimizing an adversarial example, we could make an arbitrarily-chosen data point “generate” a given model, hence efficiently generating intermediate models with correct data points. We demonstrate, both theoretically and empirically, that we are able to generate a valid proof with significantly less cost than generating a proof by the prover.},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Zhang, Rui and Liu, Jian and Ding, Yuan and Wang, Zhibo and Wu, Qingbiao and Ren, Kui},
	month = may,
	year = {2022},
	note = {ISSN: 2375-1207},
	keywords = {Computational modeling, Costs, Data models, Machine learning, Privacy, Security, Training},
	pages = {1408--1422},
}

@misc{fang_fundamental_2022,
	title = {On the {Fundamental} {Limits} of {Formally} ({Dis}){Proving} {Robustness} in {Proof}-of-{Learning}},
	url = {http://arxiv.org/abs/2208.03567},
	doi = {10.48550/arXiv.2208.03567},
	abstract = {Proof-of-learning (PoL) proposes a model owner use machine learning training checkpoints to establish a proof of having expended the necessary compute for training. The authors of PoL forego cryptographic approaches and trade rigorous security guarantees for scalability to deep learning by being applicable to stochastic gradient descent and adaptive variants. This lack of formal analysis leaves the possibility that an attacker may be able to spoof a proof for a model they did not train. We contribute a formal analysis of why the PoL protocol cannot be formally (dis)proven to be robust against spoofing adversaries. To do so, we disentangle the two roles of proof verification in PoL: (a) efficiently determining if a proof is a valid gradient descent trajectory, and (b) establishing precedence by making it more expensive to craft a proof after training completes (i.e., spoofing). We show that efficient verification results in a tradeoff between accepting legitimate proofs and rejecting invalid proofs because deep learning necessarily involves noise. Without a precise analytical model for how this noise affects training, we cannot formally guarantee if a PoL verification algorithm is robust. Then, we demonstrate that establishing precedence robustly also reduces to an open problem in learning theory: spoofing a PoL post hoc training is akin to finding different trajectories with the same endpoint in non-convex learning. Yet, we do not rigorously know if priori knowledge of the final model weights helps discover such trajectories. We conclude that, until the aforementioned open problems are addressed, relying more heavily on cryptography is likely needed to formulate a new class of PoL protocols with formal robustness guarantees. In particular, this will help with establishing precedence. As a by-product of insights from our analysis, we also demonstrate two novel attacks against PoL.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Fang, Congyu and Jia, Hengrui and Thudi, Anvith and Yaghini, Mohammad and Choquette-Choo, Christopher A. and Dullerud, Natalie and Chandrasekaran, Varun and Papernot, Nicolas},
	month = aug,
	year = {2022},
	note = {arXiv:2208.03567 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tian_comprehensive_2022,
	title = {A {Comprehensive} {Survey} on {Poisoning} {Attacks} and {Countermeasures} in {Machine} {Learning}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3551636},
	doi = {10.1145/3551636},
	abstract = {The prosperity of machine learning has been accompanied by increasing attacks on the training process. Among them, poisoning attacks have become an emerging threat during model training. Poisoning attacks have profound impacts on the target models, e.g., making them unable to converge or manipulating their prediction results. Moreover, the rapid development of recent distributed learning frameworks, especially federated learning, has further stimulated the development of poisoning attacks. Defending against poisoning attacks is challenging and urgent. However, the systematic review from a unified perspective remains blank. This survey provides an in-depth and up-to-date overview of poisoning attacks and corresponding countermeasures in both centralized and federated learning. We firstly categorize attack methods based on their goals. Secondly, we offer detailed analysis of the differences and connections among the attack techniques. Furthermore, we present countermeasures in different learning framework and highlight their advantages and disadvantages. Finally, we discuss the reasons for the feasibility of poisoning attacks and address the potential research directions from attacks and defenses perspectives, separately.},
	number = {8},
	urldate = {2023-03-14},
	journal = {ACM Computing Surveys},
	author = {Tian, Zhiyi and Cui, Lei and Liang, Jie and Yu, Shui},
	month = dec,
	year = {2022},
	keywords = {Deep learning, backdoor attack, federated learning, poisoning attack},
	pages = {166:1--166:35},
}

@article{cina_wild_2023,
	title = {Wild {Patterns} {Reloaded}: {A} {Survey} of {Machine} {Learning} {Security} against {Training} {Data} {Poisoning}},
	issn = {0360-0300},
	shorttitle = {Wild {Patterns} {Reloaded}},
	url = {https://doi.org/10.1145/3585385},
	doi = {10.1145/3585385},
	abstract = {The success of machine learning is fueled by the increasing availability of computing power and large training datasets. The training data is used to learn new models or update existing ones, assuming that it is sufficiently representative of the data that will be encountered at test time. This assumption is challenged by the threat of poisoning, an attack that manipulates the training data to compromise the model’s performance at test time. Although poisoning has been acknowledged as a relevant threat in industry applications, and a variety of different attacks and defenses have been proposed so far, a complete systematization and critical review of the field is still missing. In this survey, we provide a comprehensive systematization of poisoning attacks and defenses in machine learning, reviewing more than 100 papers published in the field in the last 15 years. We start by categorizing the current threat models and attacks, and then organize existing defenses accordingly. While we focus mostly on computer-vision applications, we argue that our systematization also encompasses state-of-the-art attacks and defenses for other data modalities. Finally, we discuss existing resources for research in poisoning, and shed light on the current limitations and open research questions in this research field.},
	urldate = {2023-03-14},
	journal = {ACM Computing Surveys},
	author = {Cinà, Antonio Emanuele and Grosse, Kathrin and Demontis, Ambra and Vascon, Sebastiano and Zellinger, Werner and Moser, Bernhard A. and Oprea, Alina and Biggio, Battista and Pelillo, Marcello and Roli, Fabio},
	month = mar,
	year = {2023},
	note = {Just Accepted},
	keywords = {Backdoor Attacks, Computer Security, Computer Vision, Machine Learning, Poisoning Attacks},
}

@article{obar_biggest_2020,
	title = {The biggest lie on the {Internet}: ignoring the privacy policies and terms of service policies of social networking services},
	volume = {23},
	issn = {1369-118X},
	shorttitle = {The biggest lie on the {Internet}},
	url = {https://doi.org/10.1080/1369118X.2018.1486870},
	doi = {10.1080/1369118X.2018.1486870},
	abstract = {This paper addresses ‘the biggest lie on the internet’ with an empirical investigation of privacy policy (PP) and terms of service (TOS) policy reading behavior. An experimental survey (N = 543) assessed the extent to which individuals ignored PP and TOS when joining a fictitious social networking service (SNS), NameDrop. Results reveal 74\% skipped PP, selecting the ‘quick join’ clickwrap. Average adult reading speed (250–280 words per minute), suggests PP should have taken 29–32 minutes and TOS 15–17 minutes to read. For those that didn’t select the clickwrap, average PP reading time was 73 seconds. All participants were presented the TOS and had an average reading time of 51 seconds. Most participants agreed to the policies, 97\% to PP and 93\% to TOS, with decliners reading PP 30 seconds longer and TOS 90 seconds longer. A regression analysis identifies information overload as a significant negative predictor of reading TOS upon sign up, when TOS changes, and when PP changes. Qualitative findings suggest that participants view policies as nuisance, ignoring them to pursue the ends of digital production, without being inhibited by the means. Implications are revealed as 98\% missed NameDrop TOS ‘gotcha clauses’ about data sharing with the NSA and employers, and about providing a first-born child as payment for SNS access.},
	number = {1},
	urldate = {2023-03-14},
	journal = {Information, Communication \& Society},
	author = {Obar, Jonathan A. and Oeldorf-Hirsch, Anne},
	month = jan,
	year = {2020},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/1369118X.2018.1486870},
	keywords = {Privacy policies, consent, privacy, social media, social networking service, terms of service},
	pages = {128--147},
}

@article{roser_world_2013,
	title = {World {Population} {Growth}},
	journal = {Our World in Data},
	author = {Roser, Max and Ritchie, Hannah and Ortiz-Ospina, Esteban and Rodés-Guirao, Lucas},
	year = {2013},
}

@article{fischer_tech_2022,
	title = {Tech firms' big trust gap: {Hardware}'s up, social media's down},
	shorttitle = {Tech firms' big trust gap},
	url = {https://www.axios.com/2022/05/25/tech-firms-big-trust-gap-harris-reputation-survey},
	abstract = {We love our devices and the companies behind them, our survey found. Social media platforms, not so much.},
	language = {en},
	urldate = {2023-03-14},
	journal = {Axios},
	author = {Fischer, Sara},
	month = may,
	year = {2022},
}

@article{kelly_americans_2021,
	title = {Americans widely distrust {Facebook}, {TikTok} and {Instagram} with their data, poll finds},
	url = {https://www.washingtonpost.com/technology/2021/12/22/tech-trust-survey/},
	abstract = {Pulled between not trusting some tech companies and still wanting to use their products, people look to government regulation.},
	language = {en},
	urldate = {2023-03-14},
	journal = {Washington Post},
	author = {Kelly, Heather and Guskin, Emily},
	month = dec,
	year = {2021},
	note = {Section: Technology},
}

@misc{noauthor_gmail_2022,
	title = {Gmail: global active users worldwide 2018},
	shorttitle = {Gmail},
	url = {https://www.statista.com/statistics/432390/active-gmail-users/},
	abstract = {This statistic illustrates the number of active Gmail users worldwide from January 2012 to October 2018.},
	language = {en},
	urldate = {2023-03-14},
	journal = {Statista},
	year = {2022},
}

@article{delacroix_bottom-up_2019,
	title = {Bottom-up data {Trusts}: disturbing the ‘one size fits all’ approach to data governance},
	volume = {9},
	issn = {2044-3994},
	shorttitle = {Bottom-up data {Trusts}},
	url = {https://doi.org/10.1093/idpl/ipz014},
	doi = {10.1093/idpl/ipz014},
	number = {4},
	urldate = {2023-02-22},
	journal = {International Data Privacy Law},
	author = {Delacroix, Sylvie and Lawrence, Neil D},
	month = nov,
	year = {2019},
	pages = {236--252},
}

@article{ortiz-ospina_rise_2019,
	title = {The rise of social media},
	journal = {Our World in Data},
	author = {Ortiz-Ospina, Esteban},
	year = {2019},
}

@misc{noauthor_private_2023,
	title = {The {Private} {Copying} {Tariff} - {The} {Canadian} {Private} {Copying} {Collective}},
	url = {https://www.cpcc.ca/en/the-cpcc/private-copying-tariff},
	urldate = {2023-03-14},
	year = {2023},
}

@inproceedings{christiano_deep_2017,
	title = {Deep {Reinforcement} {Learning} from {Human} {Preferences}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	urldate = {2023-03-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
	year = {2017},
}

@misc{wetsman_facebooks_2021,
	title = {Facebook’s whistleblower report confirms what researchers have known for years},
	url = {https://www.theverge.com/2021/10/6/22712927/facebook-instagram-teen-mental-health-research},
	abstract = {Experts have known about risks to teens for years.},
	language = {en-US},
	urldate = {2023-03-13},
	journal = {The Verge},
	author = {Wetsman, Nicole},
	month = oct,
	year = {2021},
}

@article{perrigo_how_2021,
	title = {How {Frances} {Haugen}’s {Team} {Forced} a {Facebook} {Reckoning}},
	url = {https://time.com/6104899/facebook-reckoning-frances-haugen/},
	abstract = {Facebook's civic integrity team put people ahead of profits},
	language = {en},
	urldate = {2023-03-13},
	journal = {Time},
	author = {Perrigo, Billy},
	month = oct,
	year = {2021},
}

@article{wells_facebook_2021,
	chapter = {Tech},
	title = {Facebook {Knows} {Instagram} {Is} {Toxic} for {Teen} {Girls}, {Company} {Documents} {Show}},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/facebook-knows-instagram-is-toxic-for-teen-girls-company-documents-show-11631620739},
	abstract = {Its own in-depth research shows a significant teen mental-health issue that Facebook plays down in public. Part 2 in a series offering an unparalleled look inside the social-media giant’s failings—and its unwillingness or inability to address them.},
	language = {en-US},
	urldate = {2023-03-13},
	journal = {Wall Street Journal},
	author = {Wells, Georgia and Horwitz, Jeff and Seetharaman, Deepa},
	month = sep,
	year = {2021},
	keywords = {Corporate/Industrial News, Eating Disorders, FB, Facebook, Health, Instagram, LEDER, Media/Entertainment, Medical Conditions, Mental Disorders, Mood Disorders, Online Service Providers, PHOTO-COMMISSION, Political/General News, SYND, Social Issues, Social Media Platforms/Tools, Society/Community, Technology, WSJ-PRO-WSJ.com, community, corporate, eating disorders, entertainment, general news, health, industrial news, leder, media, medical conditions, mental disorders, mood disorders, online service providers, photo-commission, political, social issues, social media platforms, society, technology, tools},
}

@misc{lu_adversarial_2022,
	title = {Adversarial {Cheap} {Talk}},
	url = {http://arxiv.org/abs/2211.11030},
	doi = {10.48550/arXiv.2211.11030},
	abstract = {Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT can still significantly influence the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorithms. More specifically, we show that an ACT Adversary is capable of harming performance by interfering with the learner's function approximation, or instead helping the Victim's performance by outputting useful features. Finally, we show that an ACT Adversary can manipulate messages during train-time to directly and arbitrarily control the Victim at test-time.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Lu, Chris and Willi, Timon and Letcher, Alistair and Foerster, Jakob},
	month = nov,
	year = {2022},
	note = {arXiv:2211.11030 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{rakhsha_policy_2020,
	title = {Policy {Teaching} via {Environment} {Poisoning}: {Training}-time {Adversarial} {Attacks} against {Reinforcement} {Learning}},
	shorttitle = {Policy {Teaching} via {Environment} {Poisoning}},
	url = {https://proceedings.mlr.press/v119/rakhsha20a.html},
	abstract = {We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes average reward in undiscounted infinite-horizon problem settings. The attacker can manipulate the rewards or the transition dynamics in the learning environment at training-time and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an {\textbackslash}emph\{optimal stealthy attack\} for different measures of attack cost. We provide sufficient technical conditions under which the attack is feasible and provide lower/upper bounds on the attack cost. We instantiate our attacks in two settings: (i) an {\textbackslash}emph\{offline\} setting where the agent is doing planning in the poisoned environment, and (ii) an {\textbackslash}emph\{online\} setting where the agent is learning a policy using a regret-minimization framework with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.},
	language = {en},
	urldate = {2023-03-13},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rakhsha, Amin and Radanovic, Goran and Devidze, Rati and Zhu, Xiaojin and Singla, Adish},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {7974--7984},
}

@article{banihashem_defense_2023,
	title = {Defense {Against} {Reward} {Poisoning} {Attacks} in {Reinforcement} {Learning}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=goPsLn3RVo},
	journal = {Transactions on Machine Learning Research},
	author = {Banihashem, Kiarash and Singla, Adish and Radanovic, Goran},
	year = {2023},
}

@inproceedings{okeefe_windfall_2020,
	address = {New York, NY, USA},
	series = {{AIES} '20},
	title = {The {Windfall} {Clause}: {Distributing} the {Benefits} of {AI} for the {Common} {Good}},
	isbn = {978-1-4503-7110-0},
	shorttitle = {The {Windfall} {Clause}},
	url = {https://doi.org/10.1145/3375627.3375842},
	doi = {10.1145/3375627.3375842},
	abstract = {As the transformative potential of AI has become increasingly salient as a matter of public and political interest, there has been growing discussion about the need to ensure that AI broadly benefits humanity. This in turn has spurred debate on the social responsibilities of large technology companies to serve the interests of society at large. In response, ethical principles and codes of conduct have been proposed to meet the escalating demand for this responsibility to be taken seriously. As yet, however, few institutional innovations have been suggested to translate this responsibility into legal commitments which apply to companies positioned to reap large financial gains from the development and use of AI. This paper offers one potentially attractive tool for addressing such issues: the Windfall Clause, which is an ex ante commitment by AI firms to donate a significant amount of any eventual extremely large profits. By this we mean an early commitment that profits that a firm could not earn without achieving fundamental, economically transformative breakthroughs in AI capabilities will be donated to benefit humanity broadly, with particular attention towards mitigating any downsides from deployment of windfall-generating AI.},
	urldate = {2023-03-13},
	booktitle = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {O'Keefe, Cullen and Cihon, Peter and Garfinkel, Ben and Flynn, Carrick and Leung, Jade and Dafoe, Allan},
	month = feb,
	year = {2020},
	keywords = {automation, future of work, inequality},
	pages = {327--331},
}

@misc{kandpal_large_2022,
	title = {Large {Language} {Models} {Struggle} to {Learn} {Long}-{Tail} {Knowledge}},
	url = {http://arxiv.org/abs/2211.08411},
	doi = {10.48550/arXiv.2211.08411},
	abstract = {The internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, there is a huge variability in the number of times a given piece of information appears on the web. In this paper, we study the relationship between the knowledge memorized by large language models and the information in their pre-training datasets. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, we find that while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant document count, presenting a promising approach for capturing the long-tail.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
	month = nov,
	year = {2022},
	note = {arXiv:2211.08411 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{albergotti_openai_2023,
	title = {{OpenAI} has hired an army of contractors to make basic coding obsolete {\textbar} {Semafor}},
	url = {https://www.semafor.com/article/01/27/2023/openai-has-hired-an-army-of-contractors-to-make-basic-coding-obsolete},
	abstract = {The company behind ChatGPT now employs around 1,000 people around the world to label data and help OpenAI’s models learn software engineering tasks.},
	language = {en},
	urldate = {2023-03-13},
	author = {Albergotti, Reed and Matsakis, Louise},
	month = jan,
	year = {2023},
	note = {Section: tech},
}

@article{dafoe_ai_2018,
	title = {{AI} governance: a research agenda},
	volume = {1442},
	journal = {Governance of AI Program, Future of Humanity Institute, University of Oxford: Oxford, UK},
	author = {Dafoe, Allan},
	year = {2018},
	pages = {1443},
}

@misc{nakano_webgpt_2022,
	title = {{WebGPT}: {Browser}-assisted question-answering with human feedback},
	shorttitle = {{WebGPT}},
	url = {http://arxiv.org/abs/2112.09332},
	doi = {10.48550/arXiv.2112.09332},
	abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
	month = jun,
	year = {2022},
	note = {arXiv:2112.09332 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{lin_truthfulqa_2021,
	title = {{TruthfulQA}: {Measuring} {How} {Models} {Mimic} {Human} {Falsehoods}},
	shorttitle = {{TruthfulQA}},
	url = {http://arxiv.org/abs/2109.07958},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. For example, the 6B-parameter GPT-J model was 17\% less truthful than its 125M-parameter counterpart. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	urldate = {2021-12-07},
	journal = {arXiv:2109.07958 [cs]},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.07958},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@article{ji_survey_2022,
	title = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3571730},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
	urldate = {2023-02-25},
	journal = {ACM Computing Surveys},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Madotto, Andrea and Fung, Pascale},
	month = nov,
	year = {2022},
	note = {Just Accepted},
	keywords = {Consistency in NLG, Extrinsic Hallucination, Factuality in NLG, Faithfulness in NLG, Hallucination, Intrinsic Hallucination},
}

@misc{goldstein_generative_2023,
	title = {Generative {Language} {Models} and {Automated} {Influence} {Operations}: {Emerging} {Threats} and {Potential} {Mitigations}},
	shorttitle = {Generative {Language} {Models} and {Automated} {Influence} {Operations}},
	url = {http://arxiv.org/abs/2301.04246},
	doi = {10.48550/arXiv.2301.04246},
	abstract = {Generative language models have improved drastically, and can now produce realistic text outputs that are difficult to distinguish from human-written content. For malicious actors, these language models bring the promise of automating the creation of convincing and misleading text for use in influence operations. This report assesses how language models might change influence operations in the future, and what steps can be taken to mitigate this threat. We lay out possible changes to the actors, behaviors, and content of online influence operations, and provide a framework for stages of the language model-to-influence operations pipeline that mitigations could target (model construction, model access, content dissemination, and belief formation). While no reasonable mitigation can be expected to fully prevent the threat of AI-enabled influence operations, a combination of multiple mitigations may make an important difference.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Goldstein, Josh A. and Sastry, Girish and Musser, Micah and DiResta, Renee and Gentzel, Matthew and Sedova, Katerina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04246 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@misc{dettmers_case_2023,
	title = {The case for 4-bit precision: k-bit {Inference} {Scaling} {Laws}},
	shorttitle = {The case for 4-bit precision},
	url = {http://arxiv.org/abs/2212.09720},
	doi = {10.48550/arXiv.2212.09720},
	abstract = {Quantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 experiments with 16-bit inputs and k-bit parameters to examine which zero-shot quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that \{4-bit\} precision is almost universally optimal for total model bits and zero-shot accuracy.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Dettmers, Tim and Zettlemoyer, Luke},
	month = feb,
	year = {2023},
	note = {arXiv:2212.09720 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{dao_hungry_2023,
	title = {Hungry {Hungry} {Hippos}: {Towards} {Language} {Modeling} with {State} {Space} {Models}},
	shorttitle = {Hungry {Hungry} {Hippos}},
	url = {http://arxiv.org/abs/2212.14052},
	doi = {10.48550/arXiv.2212.14052},
	abstract = {State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2\${\textbackslash}times\$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4\${\textbackslash}times\$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Dao, Tri and Fu, Daniel Y. and Saab, Khaled K. and Thomas, Armin W. and Rudra, Atri and Ré, Christopher},
	month = feb,
	year = {2023},
	note = {arXiv:2212.14052 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chan_harms_2023,
	title = {Harms from {Increasingly} {Agentic} {Algorithmic} {Systems}},
	url = {http://arxiv.org/abs/2302.10329},
	doi = {10.48550/arXiv.2302.10329},
	abstract = {Research in Fairness, Accountability, Transparency, and Ethics (FATE) has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed which threaten the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms. Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency -- notably, these include systemic and/or long-range impacts, often on marginalized stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Chan, Alan and Salganik, Rebecca and Markelius, Alva and Pang, Chris and Rajkumar, Nitarshan and Krasheninnikov, Dmitrii and Langosco, Lauro and He, Zhonghao and Duan, Yawen and Carroll, Micah and Lin, Michelle and Mayhew, Alex and Collins, Katherine and Molamohammadi, Maryam and Burden, John and Zhao, Wanru and Rismani, Shalaleh and Voudouris, Konstantinos and Bhatt, Umang and Weller, Adrian and Krueger, David and Maharaj, Tegan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10329 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	doi = {10.48550/arXiv.2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{schick_toolformer_2023,
	title = {Toolformer: {Language} {Models} {Can} {Teach} {Themselves} to {Use} {Tools}},
	shorttitle = {Toolformer},
	url = {http://arxiv.org/abs/2302.04761},
	doi = {10.48550/arXiv.2302.04761},
	abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04761 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{sanh_multitask_2022,
	title = {Multitask {Prompted} {Training} {Enables} {Zero}-{Shot} {Task} {Generalization}},
	url = {http://arxiv.org/abs/2110.08207},
	doi = {10.48550/arXiv.2110.08207},
	abstract = {Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and Dey, Manan and Bari, M. Saiful and Xu, Canwen and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla, Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak, Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang, Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden, Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos and Sharma, Abheesht and Santilli, Andrea and Fevry, Thibault and Fries, Jason Alan and Teehan, Ryan and Bers, Tali and Biderman, Stella and Gao, Leo and Wolf, Thomas and Rush, Alexander M.},
	month = mar,
	year = {2022},
	note = {arXiv:2110.08207 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{ohara_data_2019,
	type = {Monograph},
	title = {Data {Trusts}: {Ethics}, {Architecture} and {Governance} for {Trustworthy} {Data} {Stewardship}},
	copyright = {cc\_by\_nc\_nd\_4},
	shorttitle = {Data {Trusts}},
	url = {https://eprints.soton.ac.uk/428276/},
	abstract = {In their report on the development of the UK AI industry, Wendy Hall and Jérôme Pesenti recommend the establishment of data trusts, “proven and trusted frameworks and agreements” that will “ensure exchanges [of data] are secure and mutually beneficial” by promoting trust in the use of data for AI. This paper defends the following thesis: A data trust works within the law to provide ethical, architectural and governance support for trustworthy data processing. Data trusts are therefore both constraining and liberating. They constrain: they respect current law, so they cannot render currently illegal actions legal. They are intended to increase trust, and so they will typically act as further constraints on data processors, adding the constraints of trustworthiness to those of law. Yet they also liberate: if data processors are perceived as trustworthy, they will get improved access to data. The paper addresses the areas of: trust and trustworthiness; ethics; architecture; legal status.{\textless}br/{\textgreater}},
	language = {en},
	urldate = {2023-03-13},
	author = {O'hara, Kieron},
	collaborator = {O'hara, Kieron},
	month = feb,
	year = {2019},
	doi = {10.5258/SOTON/WSI-WP001},
}

@article{artyushina_is_2020,
	title = {Is civic data governance the key to democratic smart cities? {The} role of the urban data trust in {Sidewalk} {Toronto}},
	volume = {55},
	issn = {07365853},
	shorttitle = {Is civic data governance the key to democratic smart cities?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0736585320301155},
	doi = {10.1016/j.tele.2020.101456},
	language = {en},
	urldate = {2023-03-13},
	journal = {Telematics and Informatics},
	author = {Artyushina, Anna},
	month = dec,
	year = {2020},
	pages = {101456},
}

@misc{de_angelis_chatgpt_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {{ChatGPT} and the {Rise} of {Large} {Language} {Models}: {The} {New} {AI}-{Driven} {Infodemic} {Threat} in {Public} {Health}},
	shorttitle = {{ChatGPT} and the {Rise} of {Large} {Language} {Models}},
	url = {https://papers.ssrn.com/abstract=4352931},
	doi = {10.2139/ssrn.4352931},
	abstract = {Large Language Models (LLMs) have recently gathered attention with the release of ChatGPT, a user-centred chatbot released by OpenAI. In this Viewpoint, we retrace the evolution of LLMs to understand the revolution brought by ChatGPT. The opportunities offered by LLMs in supporting scientific research are multiple and various models have already been tested in Natural Language Processing (NLP) tasks in this domain.The impact of ChatGPT has been huge for the general public and the research community, with many authors using the chatbot to write part of their articles and some papers even listing ChatGPT as author. Alarming ethical and practical challenges emerge from the use of LLMs, particularly in the medical field for the potential impact on public health. Infodemic is a trending topic in public health and the ability of LLMs to rapidly produce vast amounts of text could leverage misinformation spread at an unprecedented scale, this could create an “AI-driven infodemic”, a novel public health threat. Policies to contrast this phenomenon need to be rapidly elaborated, the inability to accurately detect artificial-intelligence-produced text is an unresolved issue.},
	language = {en},
	urldate = {2023-03-13},
	author = {De Angelis, Luigi and Baglivo, Francesco and Arzilli, Guglielmo and Privitera, Gaetano Pierpaolo and Ferragina, Paolo and Tozzi, Alberto Eugenio and Rizzo, Caterina},
	month = feb,
	year = {2023},
	keywords = {Artificial Intelligence, ChatGPT, Infodemic, Large Language Models, Public Health},
}

@misc{hacker_regulating_2023,
	title = {Regulating {ChatGPT} and other {Large} {Generative} {AI} {Models}},
	url = {http://arxiv.org/abs/2302.02337},
	doi = {10.48550/arXiv.2302.02337},
	abstract = {Large generative AI models (LGAIMs), such as ChatGPT or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest four strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. In particular, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers. In all areas, regulators and lawmakers need to act fast to keep track with the dynamics of ChatGPT et al.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Hacker, Philipp and Engel, Andreas and Mauer, Marco},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02337 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, I.2},
}

@inproceedings{kasy_fairness_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Fairness, {Equality}, and {Power} in {Algorithmic} {Decision}-{Making}},
	isbn = {978-1-4503-8309-7},
	url = {https://doi.org/10.1145/3442188.3445919},
	doi = {10.1145/3442188.3445919},
	abstract = {Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same "merit." Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by "merit;" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Kasy, Maximilian and Abebe, Rediet},
	year = {2021},
	note = {event-place: Virtual Event, Canada},
	keywords = {Algorithmic fairness, auditing, empirical economics, inequality, power},
	pages = {576--586},
}

@inproceedings{boag_tech_2022,
	title = {Tech {Worker} {Organizing} for {Power} and {Accountability}},
	url = {https://doi.org/10.1145%2F3531146.3533111},
	doi = {10.1145/3531146.3533111},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Boag, William and Suresh, Harini and Lepe, Bianca and D'Ignazio, Catherine},
	month = jun,
	year = {2022},
}

@inproceedings{young_confronting_2022,
	title = {Confronting {Power} and {Corporate} {Capture} at the {FAccT} {Conference}},
	url = {https://doi.org/10.1145%2F3531146.3533194},
	doi = {10.1145/3531146.3533194},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Young, Meg and Katell, Michael and Krafft, P. M.},
	month = jun,
	year = {2022},
}

@article{jobin_global_2019,
	title = {The global landscape of {AI} ethics guidelines},
	volume = {1},
	copyright = {2019 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-019-0088-2},
	doi = {10.1038/s42256-019-0088-2},
	abstract = {In the past five years, private companies, research institutions and public sector organizations have issued principles and guidelines for ethical artificial intelligence (AI). However, despite an apparent agreement that AI should be ‘ethical’, there is debate about both what constitutes ‘ethical AI’ and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analysed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies.},
	language = {en},
	number = {9},
	urldate = {2023-03-13},
	journal = {Nature Machine Intelligence},
	author = {Jobin, Anna and Ienca, Marcello and Vayena, Effy},
	month = sep,
	year = {2019},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Ethics, Information systems and information technology, Information technology, Science, technology and society},
	pages = {389--399},
}

@inproceedings{ganguli_predictability_2022,
	address = {New York, NY, USA},
	series = {{FAccT} '22},
	title = {Predictability and {Surprise} in {Large} {Generative} {Models}},
	isbn = {978-1-4503-9352-2},
	url = {https://doi.org/10.1145/3531146.3533229},
	doi = {10.1145/3531146.3533229},
	abstract = {Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have a paradoxical combination of predictable loss on a broad training distribution (as embodied in their ”scaling laws”), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend for this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, funders who want to support work addressing these challenges, and academics who want to analyze, critique, and potentially develop large generative models.},
	urldate = {2023-03-13},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Dassarma, Nova and Drain, Dawn and Elhage, Nelson and El Showk, Sheer and Fort, Stanislav and Hatfield-Dodds, Zac and Henighan, Tom and Johnston, Scott and Jones, Andy and Joseph, Nicholas and Kernian, Jackson and Kravec, Shauna and Mann, Ben and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Christopher and Amodei, Dario and Clark, Jack},
	month = jun,
	year = {2022},
	pages = {1747--1764},
}

@article{arza_systematizing_2017,
	title = {Systematizing benefits of open science practices},
	volume = {37},
	issn = {0167-5265},
	url = {https://content.iospress.com/articles/information-services-and-use/isu861},
	doi = {10.3233/ISU-170861},
	abstract = {Open science aims at the creation of public scientific goods by means of sharing outputs and widening and facilitating collaboration, in one or many of the different research stages. There are many beneficial aspects of open science that have been cl},
	language = {en},
	number = {4},
	urldate = {2023-03-13},
	journal = {Information Services \& Use},
	author = {Arza, Valeria and Fressoli, Mariano},
	month = jan,
	year = {2017},
	note = {Publisher: IOS Press},
	pages = {463--474},
}

@article{mesgari_sum_2015,
	title = {“{The} sum of all human knowledge”: {A} systematic review of scholarly research on the content of {Wikipedia}},
	volume = {66},
	issn = {2330-1643},
	shorttitle = {“{The} sum of all human knowledge”},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.23172},
	doi = {10.1002/asi.23172},
	abstract = {Wikipedia may be the best-developed attempt thus far to gather all human knowledge in one place. Its accomplishments in this regard have made it a point of inquiry for researchers from different fields of knowledge. A decade of research has thrown light on many aspects of the Wikipedia community, its processes, and its content. However, due to the variety of fields inquiring about Wikipedia and the limited synthesis of the extensive research, there is little consensus on many aspects of Wikipedia's content as an encyclopedic collection of human knowledge. This study addresses the issue by systematically reviewing 110 peer-reviewed publications on Wikipedia content, summarizing the current findings, and highlighting the major research trends. Two major streams of research are identified: the quality of Wikipedia content (including comprehensiveness, currency, readability, and reliability) and the size of Wikipedia. Moreover, we present the key research trends in terms of the domains of inquiry, research design, data source, and data gathering methods. This review synthesizes scholarly understanding of Wikipedia content and paves the way for future studies.},
	language = {en},
	number = {2},
	urldate = {2023-03-13},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Mesgari, Mostafa and Okoli, Chitu and Mehdi, Mohamad and Nielsen, Finn Årup and Lanamäki, Arto},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.23172},
	keywords = {encyclopedias, quality, reliability},
	pages = {219--245},
}

@article{anthony_reputation_2009,
	title = {Reputation and {Reliability} in {Collective} {Goods}: {The} {Case} of the {Online} {Encyclopedia} {Wikipedia}},
	volume = {21},
	issn = {1043-4631},
	shorttitle = {Reputation and {Reliability} in {Collective} {Goods}},
	url = {https://doi.org/10.1177/1043463109336804},
	doi = {10.1177/1043463109336804},
	abstract = {An important organizational innovation enabled by the revolution in information technologies is `open source' production which converts private commodities into essentially public goods. Similar to other public goods, incentives for reputation and group identity appear to motivate contributions to open source projects, overcoming the social dilemma inherent in producing such goods. In this paper we examine how contributor motivations affect the type of contributions made to the open source online encyclopedia Wikipedia. As expected, we find that registered participants, motivated by reputation and commitment to the Wikipedia community, make many contributions with high reliability. Surprisingly, however, we find the highest reliability from the vast numbers of anonymous `Good Samaritans' who contribute only once. Our findings of high reliability in the contributions of both Good Samaritans and committed `zealots' suggest that open source production succeeds by altering the scope of production such that a critical mass of contributors can participate.},
	language = {en},
	number = {3},
	urldate = {2023-03-13},
	journal = {Rationality and Society},
	author = {Anthony, Denise and Smith, Sean W. and Williamson, Timothy},
	month = aug,
	year = {2009},
	note = {Publisher: SAGE Publications Ltd},
	pages = {283--306},
}

@book{directorate-general_for_communications_networks_impact_2021,
	address = {LU},
	title = {The impact of open source software and hardware on technological independence, competitiveness and innovation in the {EU} economy: final study report},
	isbn = {978-92-76-30980-2},
	shorttitle = {The impact of open source software and hardware on technological independence, competitiveness and innovation in the {EU} economy},
	url = {https://data.europa.eu/doi/10.2759/430161},
	abstract = {This study analyses the economic impact of Open Source Software (OSS) and Hardware (OSH) on the European economy. It was commissioned by the European Commission’s DG CONNECT. It is estimated that companies located in the EU invested around €1 billion in OSS in 2018, which resulted in an impact on the European economy of between €65 and €95 billion. The analysis estimates a cost-benefit ratio of above 1:4 and predicts that an increase of 10\% of OSS contributions would annually generate an additional 0.4\% to 0.6\% GDP as well as more than 600 additional ICT start-ups in the EU. Case studies reveal that by procuring OSS instead of proprietary software, the public sector could reduce the total cost of ownership, avoid vendor lock-in and thus increase its digital autonomy. The study also contains an analysis of existing public policy actions in Europe and around the world. The scale of Europe’s institutional capacity related to OSS, however, is disproportionately smaller than the scale of the value created by OSS. The study therefore gives a number of specific public policy recommendations aimed at achieving a digitally autonomous public sector, open R\&D enabling European growth and a digitised and internally competitive industry.},
	language = {eng},
	urldate = {2023-03-13},
	publisher = {Publications Office of the European Union},
	author = {Directorate-General for Communications Networks, Content {and} Technology (European Commission) and Blind, Knut and Pätsch, Sivan and Muto, Sachiko and Böhm, Mirko and Schubert, Torben and Grzegorzewska, Paula and Katz, Andrew},
	year = {2021},
}

@article{ghosh_economic_2007,
	title = {Economic impact of open source software on innovation and the competitiveness of the {Information} and {Communication} {Technologies} ({ICT}) sector in the {EU}},
	url = {https://ictlogy.net/bibliography/reports/projects.php?idp=895&lang=en},
	abstract = {Research on the Information Society, the Digital Divide and Information and Communication Technologies for development},
	language = {en, net},
	urldate = {2023-03-13},
	author = {Ghosh, Rishab Aiyer},
	year = {2007},
	note = {Publisher: UNU-MERIT},
}

@misc{the_consilience_project_democracy_2021,
	title = {Democracy and the {Epistemic} {Commons}},
	url = {https://consilienceproject.org/democracy-and-the-epistemic-commons/},
	language = {en-US},
	urldate = {2023-03-13},
	journal = {The Consilience Project},
	author = {The Consilience Project},
	month = feb,
	year = {2021},
}

@article{zygmuntowski_embedding_2021,
	title = {Embedding {European} values in data governance: {A} case for public data commons},
	volume = {10},
	copyright = {https://creativecommons.org/licenses/by/3.0/de/legalcode},
	issn = {2197-6775},
	shorttitle = {Embedding {European} values in data governance},
	url = {https://www.econstor.eu/handle/10419/245343},
	doi = {10.14763/2021.3.1572},
	abstract = {The public sphere needs an "ecosystem of trust" which could set out objectives of re-usage of data for the common good while protecting individual rights. This study analyses the emerging models of data governance through the lenses of science and technology studies (STS), critical data studies (CDS), and institutional economics, investigating which data governance model creates conditions for data stewardship guided by European values and rights. We critically examine two prominent, yet highly arguable, paradigms related to data, asserting that the systemic level of data assemblage must be re-conceptualised to reject the data-as-a-commodity view and take public interest into consideration. For data stewardship to achieve its goals, it is necessary to consider the inherent properties of data as commons, in the sense of a common-pool resources (CPR) framework. Therefore, we point towards public data commons as the model that is best suited to secure European rights and values while increasing data sharing at the same time. The design of such public data commons is the challenge of our time.},
	language = {eng},
	number = {3},
	urldate = {2023-03-13},
	journal = {Internet Policy Review},
	author = {Zygmuntowski, Jan J. and Zoboli, Laura and Nemitz, Paul},
	year = {2021},
	note = {Publisher: Berlin: Alexander von Humboldt Institute for Internet and Society},
	pages = {1--29},
}

@incollection{bunz_big_2022,
	title = {From {Big} to {Democratic} {Data}},
	isbn = {978-1-00-317342-7},
	url = {https://library.oapen.org/handle/20.500.12657/57277},
	abstract = {Datasets have come to play a significant role in the technical and political realities of our overdeveloped world. This chapter indicates how invisible data processes pose a threat to the health and safety of the global public and argues for the democratic potential of data practices. This potential is set to become even more influential due to the central role data plays for training contemporary AI and technologies such as machine learning. Our case study explores the role patient datasets have for machine learning research in healthcare and shows that publicly available datasets are central to advancing data analysis research; they can act as a counterbalance to datasets full of absences, biases, and disconnects that often corrupt the quality of data. Given this, we argue for the introduction of ‘data solidarity’ as a principle of data governance and an effective critical data practice that focuses on the democratic (instead of economic) potential of data; a potential that is far too often overlooked.},
	language = {English},
	urldate = {2023-03-13},
	booktitle = {Democratic {Frontiers}},
	publisher = {Taylor \& Francis},
	author = {Bunz, Mercedes and Vrikki, Photini},
	month = feb,
	year = {2022},
	doi = {10.4324/9781003173427-3},
	note = {Accepted: 2022-07-08T13:37:25Z},
	keywords = {bic Book Industry Communication::J Society \& social sciences::JH Sociology \& anthropology::JHB Sociology, critical data practice, data as a public good, data governance, data solidarity, democratic data},
}

@article{zuger_ai_2022,
	title = {{AI} for the public. {How} public interest theory shifts the discourse on {AI}},
	issn = {1435-5655},
	url = {https://doi.org/10.1007/s00146-022-01480-5},
	doi = {10.1007/s00146-022-01480-5},
	abstract = {AI for social good is a thriving research topic and a frequently declared goal of AI strategies and regulation. This article investigates the requirements necessary in order for AI to actually serve a public interest, and hence be socially good. The authors propose shifting the focus of the discourse towards democratic governance processes when developing and deploying AI systems. The article draws from the rich history of public interest theory in political philosophy and law, and develops a framework for ‘public interest AI’. The framework consists of (1) public justification for the AI system, (2) an emphasis on equality, (3) deliberation/ co-design process, (4) technical safeguards, and (5) openness to validation. This framework is then applied to two case studies, namely SyRI, the Dutch welfare fraud detection project, and UNICEF’s Project Connect, that maps schools worldwide. Through the analysis of these cases, the authors conclude that public interest is a helpful and practical guide for the development and governance of AI for the people.},
	language = {en},
	urldate = {2023-03-13},
	journal = {AI \& SOCIETY},
	author = {Züger, Theresa and Asghari, Hadi},
	month = jun,
	year = {2022},
	keywords = {AI ethics, Artificial intelligence, Deliberation, Democratic governance, Public interest},
}

@article{murdock_building_2005,
	title = {Building the digital commons},
	journal = {Cultural dilemmas in public service broadcasting},
	author = {Murdock, Graham},
	year = {2005},
	note = {Publisher: Nordicom Goteborg, Sweden},
	pages = {213--31},
}

@article{morozov_socialize_2015,
	title = {Socialize the {Data} {Centres}!},
	abstract = {The leading iconoclast of Internet euphoria recounts his path from schooling in Belarus through training in Bulgaria to NGO work in Central Europe and fame as author of The Net Delusion in the United States. A radicalized view of the transformations required in the information infrastructures of the present for any egalitarian future.},
	number = {91},
	journal = {New Left Review},
	author = {Morozov, Evgeny},
	month = feb,
	year = {2015},
	pages = {45--66},
}

@misc{goldstein_data_2018,
	title = {Data {Commons} {Version} 1.0: {A} {Framework} to {Build} {Toward} {AI} for {Good}},
	shorttitle = {Data {Commons} {Version} 1.0},
	url = {https://medium.com/berkman-klein-center/data-commons-version-1-0-a-framework-to-build-toward-ai-for-good-73414d7e72be},
	abstract = {A roadmap for data from the 2018 AI for Good Summit},
	language = {en},
	urldate = {2023-03-13},
	journal = {Berkman Klein Center Collection},
	author = {Goldstein, Elena and Gasser, Urs and Budish, Ryan},
	month = jun,
	year = {2018},
}

@inproceedings{atli_tekgul_effectiveness_2022,
	address = {New York, NY, USA},
	series = {{IWSPA} '22},
	title = {On the {Effectiveness} of {Dataset} {Watermarking}},
	isbn = {978-1-4503-9230-3},
	url = {https://doi.org/10.1145/3510548.3519376},
	doi = {10.1145/3510548.3519376},
	abstract = {In a data-driven world, datasets constitute a significant economic value. Dataset owners who spend time and money to collect and curate the data are incentivized to ensure that their datasets are not used in ways that they did not authorize. When such misuse occurs, dataset owners need technical mechanisms for demonstrating their ownership of the dataset in question. Dataset watermarking provides one approach for ownership demonstration which can, in turn, deter unauthorized use. In this paper, we investigate a recently proposed data provenance method, radioactive data, to assess if it can be used to demonstrate ownership of (image) datasets used to train machine learning (ML) models. The original paper radioactive reported that radioactive data is effective in white-box settings. We show that while this is true for large datasets with many classes, it is not as effective for datasets where the number of classes is low (łeq 30) or the number of samples per class is low (łeq 500). We also show that, counter-intuitively, the black-box verification technique described in radioactive is effective for all datasets used in this paper, even when white-box verification in radioactive is not. Given this observation, we show that the confidence in white-box verification can be improved by using watermarked samples directly during the verification process. We also highlight the need to assess the robustness of radioactive data if it were to be used for ownership demonstration since it is an adversarial setting unlike provenance identification. Compared to dataset watermarking, ML model watermarking has been explored more extensively in recent literature. However, most of the state-of-the-art model watermarking techniques can be defeated via model extraction robustness. We show that radioactive data can effectively survive model extraction attacks, which raises the possibility that it can be used for ML model ownership verification robust against model extraction.},
	urldate = {2023-03-07},
	booktitle = {Proceedings of the 2022 {ACM} on {International} {Workshop} on {Security} and {Privacy} {Analytics}},
	publisher = {Association for Computing Machinery},
	author = {Atli Tekgul, Buse Gul and Asokan, N.},
	month = apr,
	year = {2022},
	keywords = {deep neural networks, ownership verification, watermarking},
	pages = {93--99},
}

@article{mcginnis_design_1996,
	title = {Design principles for local and global commons},
	volume = {2},
	journal = {The international political economy and international institutions},
	author = {McGinnis, Michael and Ostrom, Elinor},
	year = {1996},
	note = {Publisher: OR Young, Cheltenham, UK: Edward Elgar},
	pages = {465--493},
}

@book{ostrom_governing_1990,
	title = {Governing the commons: {The} evolution of institutions for collective action},
	publisher = {Cambridge university press},
	author = {Ostrom, Elinor},
	year = {1990},
}

@article{greco_tragedy_2004,
	title = {The tragedy of the digital commons},
	volume = {6},
	issn = {1572-8439},
	url = {https://doi.org/10.1007/s10676-004-2895-2},
	doi = {10.1007/s10676-004-2895-2},
	abstract = {In the paper it is argued that bridging the digital divide may cause a new ethical and social dilemma. Using Hardin's Tragedy of the Commons, we show that an improper opening and enlargement of the digital environment (Infosphere) is likely to produce a Tragedy of the Digital Commons (TDC). In the course of the analysis, we explain why Adar and Huberman's previous use of Hardin's Tragedy to interpret certain recent phenomena in the Infosphere (especially peer-to-peer communication) may not be entirely satisfactory. We then seek to provide an improved version of the TDC that avoids the possible shortcomings of their model. Next, we analyse some problems encountered by the application of classical ethics in the resolution of the TDC. In the conclusion, we outline the kind of work that will be required to develop an ethical approach that may bridge the digital divide but avoid the TDC.},
	language = {en},
	number = {2},
	urldate = {2023-03-13},
	journal = {Ethics and Information Technology},
	author = {Greco, Gian Maria and Floridi, Luciano},
	month = jun,
	year = {2004},
	keywords = {Garrett Hardin, Infosphere, artificial agents, digital divide, tragedy of the commons, tragedy of the digital commons},
	pages = {73--81},
}

@article{hardin_tragedy_1968,
	title = {The {Tragedy} of the {Commons}},
	volume = {162},
	url = {https://www.science.org/doi/10.1126/science.162.3859.1243},
	doi = {10.1126/science.162.3859.1243},
	abstract = {The population problem has no technical solution; it requires a fundamental extension in morality.},
	number = {3859},
	urldate = {2023-03-13},
	journal = {Science},
	author = {Hardin, Garrett},
	month = dec,
	year = {1968},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1243--1248},
}

@article{dulong_de_rosnay_digital_2020,
	title = {Digital commons},
	volume = {9},
	copyright = {https://creativecommons.org/licenses/by/3.0/de/deed.de},
	issn = {2197-6775},
	url = {https://www.econstor.eu/handle/10419/233108},
	doi = {10.14763/2020.4.1530},
	abstract = {Commons are holistic social institutions to govern the (re)production of resources, articulated through interrelated legal, socio-cultural, economic and institutional dimensions. They represent a comprehensive and radical approach to organise collective action, placing it "beyond market and state" (Bollier \& Helfrich, 2012). They form a third way of organising society and the economy that differs from both market-based approaches, with their orientation toward prices, and from bureaucratic forms of organisation, with their orientation toward hierarchies and commands. This governance model has been applied to tangible and intangible resources, to local initiatives (garden, educational material), and to resources governed by global politics (climate, internet infrastructure). Digital commons are a subset of the commons, where the resources are data, information, culture and knowledge which are created and/or maintained online. The notion of the digital commons is an important concept for countering legal enclosure and fostering equitable access to these resources. This article presents the history of the movement of the digital commons, from free software, free culture, and public domain works, to open data and open access to science. It then analyses its foundational dimensions (licensing, authorship, peer production, governance) and finally studies newer forms of the digital commons, urban democratic participation and data commons.},
	language = {eng},
	number = {4},
	urldate = {2023-03-13},
	journal = {Internet Policy Review},
	author = {Dulong de Rosnay, Mélanie and Stalder, Felix},
	year = {2020},
	note = {Publisher: Berlin: Alexander von Humboldt Institute for Internet and Society},
	pages = {1--22},
}

@misc{phang_eleutherai_2022,
	title = {{EleutherAI}: {Going} {Beyond} "{Open} {Science}" to "{Science} in the {Open}"},
	shorttitle = {{EleutherAI}},
	url = {http://arxiv.org/abs/2210.06413},
	doi = {10.48550/arXiv.2210.06413},
	abstract = {Over the past two years, EleutherAI has established itself as a radically novel initiative aimed at both promoting open-source research and conducting research in a transparent, openly accessible and collaborative manner. EleutherAI's approach to research goes beyond transparency: by doing research entirely in public, anyone in the world can observe and contribute at every stage. Our work has been received positively and has resulted in several high-impact projects in Natural Language Processing and other fields. In this paper, we describe our experience doing public-facing machine learning research, the benefits we believe this approach brings, and the pitfalls we have encountered.},
	urldate = {2023-03-12},
	publisher = {arXiv},
	author = {Phang, Jason and Bradley, Herbie and Gao, Leo and Castricato, Louis and Biderman, Stella},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06413 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-03-12},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chowdhery_palm_2022,
	title = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	shorttitle = {{PaLM}},
	url = {http://arxiv.org/abs/2204.02311},
	doi = {10.48550/arXiv.2204.02311},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	urldate = {2023-03-11},
	publisher = {arXiv},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	month = oct,
	year = {2022},
	note = {arXiv:2204.02311 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{jakesch_co-writing_2023,
	title = {Co-{Writing} with {Opinionated} {Language} {Models} {Affects} {Users}' {Views}},
	url = {http://arxiv.org/abs/2302.00560},
	doi = {10.1145/3544548.3581196},
	abstract = {If large language models like GPT-3 preferably produce a particular point of view, they may influence people's opinions on an unknown scale. This study investigates whether a language-model-powered writing assistant that generates some opinions more often than others impacts what users write - and what they think. In an online experiment, we asked participants (N=1,506) to write a post discussing whether social media is good for society. Treatment group participants used a language-model-powered writing assistant configured to argue that social media is good or bad for society. Participants then completed a social media attitude survey, and independent judges (N=500) evaluated the opinions expressed in their writing. Using the opinionated language model affected the opinions expressed in participants' writing and shifted their opinions in the subsequent attitude survey. We discuss the wider implications of our results and argue that the opinions built into AI language technologies need to be monitored and engineered more carefully.},
	urldate = {2023-03-11},
	author = {Jakesch, Maurice and Bhat, Advait and Buschek, Daniel and Zalmanson, Lior and Naaman, Mor},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00560 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{karimi_survey_2021,
	title = {A survey of algorithmic recourse: definitions, formulations, solutions, and prospects},
	shorttitle = {A survey of algorithmic recourse},
	url = {http://arxiv.org/abs/2010.04050},
	doi = {10.48550/arXiv.2010.04050},
	abstract = {Machine learning is increasingly used to inform decision-making in sensitive situations where decisions have consequential effects on individuals' lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role for the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavourably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions towards which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Karimi, Amir-Hossein and Barthe, Gilles and Schölkopf, Bernhard and Valera, Isabel},
	month = mar,
	year = {2021},
	note = {arXiv:2010.04050 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{buolamwini_gender_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Gender {Shades}: {Intersectional} {Accuracy} {Disparities} in {Commercial} {Gender} {Classification}},
	volume = {81},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Buolamwini, Joy and Gebru, Timnit},
	editor = {Friedler, Sorelle A. and Wilson, Christo},
	month = feb,
	year = {2018},
	pages = {77--91},
}

@inproceedings{costanza-chock_who_2022,
	title = {Who {Audits} the {Auditors}? {Recommendations} from a field scan of the algorithmic auditing ecosystem},
	url = {https://doi.org/10.1145%2F3531146.3533213},
	doi = {10.1145/3531146.3533213},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Costanza-Chock, Sasha and Raji, Inioluwa Deborah and Buolamwini, Joy},
	month = jun,
	year = {2022},
}

@inproceedings{raji_fallacy_2022,
	title = {The {Fallacy} of {AI} {Functionality}},
	url = {https://doi.org/10.1145%2F3531146.3533158},
	doi = {10.1145/3531146.3533158},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Raji, Inioluwa Deborah and Kumar, I. Elizabeth and Horowitz, Aaron and Selbst, Andrew},
	month = jun,
	year = {2022},
}

@inproceedings{raji_algorithmic_2022,
	title = {From {Algorithmic} {Audits} to {Actual} {Accountability}: {Overcoming} {Practical} {Roadblocks} on the {Path} to {Meaningful} {Audit} {Interventions} for {AI} {Governance}},
	url = {https://doi.org/10.1145%2F3514094.3539566},
	doi = {10.1145/3514094.3539566},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Raji, Inioluwa Deborah},
	month = jul,
	year = {2022},
}

@inproceedings{raji_outsider_2022,
	title = {Outsider {Oversight}: {Designing} a {Third} {Party} {Audit} {Ecosystem} for {AI} {Governance}},
	url = {https://doi.org/10.1145%2F3514094.3534181},
	doi = {10.1145/3514094.3534181},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Raji, Inioluwa Deborah and Xu, Peggy and Honigsberg, Colleen and Ho, Daniel},
	month = jul,
	year = {2022},
}

@inproceedings{raji_closing_2020,
	title = {Closing the {AI} accountability gap},
	url = {https://doi.org/10.1145%2F3351095.3372873},
	doi = {10.1145/3351095.3372873},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N. and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
	month = jan,
	year = {2020},
}

@inproceedings{raji_actionable_2019,
	title = {Actionable {Auditing}},
	url = {https://doi.org/10.1145%2F3306618.3314244},
	doi = {10.1145/3306618.3314244},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
	month = jan,
	year = {2019},
}

@misc{nguyen_survey_2022,
	title = {A {Survey} of {Machine} {Unlearning}},
	url = {http://arxiv.org/abs/2209.02299},
	doi = {10.48550/arXiv.2209.02299},
	abstract = {Today, computer systems hold large amounts of personal data. Yet while such an abundance of data allows breakthroughs in artificial intelligence, and especially machine learning (ML), its existence can be a threat to user privacy, and it can weaken the bonds of trust between humans and AI. Recent regulations now require that, on request, private information about a user must be removed from both computer systems and from ML models, i.e. ``the right to be forgotten''). While removing data from back-end databases should be straightforward, it is not sufficient in the AI context as ML models often `remember' the old data. Contemporary adversarial attacks on trained models have proven that we can learn whether an instance or an attribute belonged to the training data. This phenomenon calls for a new paradigm, namely machine unlearning, to make ML models forget about particular data. It turns out that recent works on machine unlearning have not been able to completely solve the problem due to the lack of common frameworks and resources. Therefore, this paper aspires to present a comprehensive examination of machine unlearning's concepts, scenarios, methods, and applications. Specifically, as a category collection of cutting-edge studies, the intention behind this article is to serve as a comprehensive resource for researchers and practitioners seeking an introduction to machine unlearning and its formulations, design criteria, removal requests, algorithms, and applications. In addition, we aim to highlight the key findings, current trends, and new research areas that have not yet featured the use of machine unlearning but could benefit greatly from it. We hope this survey serves as a valuable resource for ML researchers and those seeking to innovate privacy technologies. Our resources are publicly available at https://github.com/tamlhp/awesome-machine-unlearning.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Nguyen, Thanh Tam and Huynh, Thanh Trung and Nguyen, Phi Le and Liew, Alan Wee-Chung and Yin, Hongzhi and Nguyen, Quoc Viet Hung},
	month = oct,
	year = {2022},
	note = {arXiv:2209.02299 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{gunn_adversarial_2022,
	address = {New York, NY, USA},
	series = {{BuildSys} '22},
	title = {Adversarial poisoning attacks on reinforcement learning-driven energy pricing},
	isbn = {978-1-4503-9890-9},
	url = {https://doi.org/10.1145/3563357.3564075},
	doi = {10.1145/3563357.3564075},
	abstract = {Complex controls are increasingly common in power systems. Reinforcement learning (RL) has emerged as a strong candidate for implementing various controllers. One common use of RL in this context is for prosumer pricing aggregations, where prosumers consist of buildings with both solar generation and energy storage. Specifically, supply and demand data serve as the observation space for many microgrid controllers acting based on a policy passed from a central RL agent. Each controller outputs an action space consisting of hourly "buy" and "sell" prices for energy throughout the day; in turn, each prosumer can choose whether to transact with the RL agent or the utility. The RL agent, who is learning online, is rewarded through its ability to generate a profit. We ask: what happens when some of the microgrid controllers are compromised by a malicious entity? We demonstrate a novel attack in RL and a simple defense against the attack. Our attack perturbs each trajectory to reverse the direction of the estimated gradient. We demonstrate that if data from a small fraction of microgrid controllers is adversarially perturbed, the learning of the RL agent can be significantly slowed. With larger perturbations, the RL aggregator can be manipulated to learn a catastrophic pricing policy that causes the RL agent to operate at a loss. Other environmental characteristics are worsened too: prosumers face higher energy costs, use their batteries less, and suffer from higher peak demand when the pricing aggregator is adversarially poisoned. We address this vulnerability with a "defense" module; i.e., a "robustification" of RL algorithms against this attack. Our defense identifies the trajectories with the largest influence on the gradient and removes them from the training data. It is computationally light and reasonable to include in any RL algorithm.},
	urldate = {2023-03-09},
	booktitle = {Proceedings of the 9th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}},
	publisher = {Association for Computing Machinery},
	author = {Gunn, Sam and Jang, Doseok and Paradise, Orr and Spangher, Lucas and Spanos, Costas J.},
	month = dec,
	year = {2022},
	keywords = {data poisoning, deep reinforcement learning, smart grids},
	pages = {262--265},
}

@article{kaul_defining_1999,
	title = {Defining global public goods},
	journal = {Global public goods: international cooperation in the 21st century},
	author = {Kaul, Inge and Grunberg, Isabelle and Stern, Marc A},
	year = {1999},
	note = {Publisher: Oxford University Press New York},
	pages = {2--19},
}

@article{dragusanu_economics_2014,
	title = {The {Economics} of {Fair} {Trade}},
	volume = {28},
	issn = {0895-3309},
	url = {https://www.aeaweb.org/articles?id=10.1257/jep.28.3.217},
	doi = {10.1257/jep.28.3.217},
	abstract = {Fair Trade is a labeling initiative aimed at improving the lives of the poor in developing countries by offering better terms to producers and helping them to organize. Although Fair Trade-certified products still comprise a small share of the market—for example, Fair Trade-certified coffee exports were 1.8 percent of global coffee exports in 2009—growth has been very rapid over the past decade. Whether Fair Trade can achieve its intended goals has been hotly debated in academic and policy circles. In particular, debates have been waged about whether Fair Trade makes "economic sense" and is sustainable in the long run. The aim of this article is to provide a critical overview of the economic theory behind Fair Trade, describing the potential benefits and potential pitfalls. We also provide an assessment of the empirical evidence of the impacts of Fair Trade to date. Because coffee is the largest single product in the Fair Trade market, our discussion here focuses on the specifics of this industry, although we will also point out some important differences with other commodities as they arise.},
	language = {en},
	number = {3},
	urldate = {2023-03-09},
	journal = {Journal of Economic Perspectives},
	author = {Dragusanu, Raluca and Giovannucci, Daniele and Nunn, Nathan},
	month = sep,
	year = {2014},
	keywords = {Human Development, Income Distribution, International Trade Organizations, Welfare, Well-Being, and Poverty: Government Programs, Migration, International Linkages to Development, Provision and Effects of Welfare Programs, Economic Development: Human Resources, Role of International Organizations, Trade Policy},
	pages = {217--236},
}

@article{tutt_fda_2017,
	title = {An {FDA} for {Algorithms}},
	volume = {69},
	url = {https://heinonline.org/HOL/P?h=hein.journals/admin69&i=95},
	language = {eng},
	number = {1},
	urldate = {2023-03-08},
	journal = {Administrative Law Review},
	author = {Tutt, Andrew},
	year = {2017},
	pages = {83--124},
}

@misc{perez_discovering_2022,
	title = {Discovering {Language} {Model} {Behaviors} with {Model}-{Written} {Evaluations}},
	url = {http://arxiv.org/abs/2212.09251},
	doi = {10.48550/arXiv.2212.09251},
	abstract = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamilė and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09251 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{dehghani_scaling_2023,
	title = {Scaling {Vision} {Transformers} to 22 {Billion} {Parameters}},
	url = {http://arxiv.org/abs/2302.05442},
	doi = {10.48550/arXiv.2302.05442},
	abstract = {The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for "LLM-like" scaling in vision, and provides key steps towards getting there.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and Jenatton, Rodolphe and Beyer, Lucas and Tschannen, Michael and Arnab, Anurag and Wang, Xiao and Riquelme, Carlos and Minderer, Matthias and Puigcerver, Joan and Evci, Utku and Kumar, Manoj and van Steenkiste, Sjoerd and Elsayed, Gamaleldin F. and Mahendran, Aravindh and Yu, Fisher and Oliver, Avital and Huot, Fantine and Bastings, Jasmijn and Collier, Mark Patrick and Gritsenko, Alexey and Birodkar, Vighnesh and Vasconcelos, Cristina and Tay, Yi and Mensink, Thomas and Kolesnikov, Alexander and Pavetić, Filip and Tran, Dustin and Kipf, Thomas and Lučić, Mario and Zhai, Xiaohua and Keysers, Daniel and Harmsen, Jeremiah and Houlsby, Neil},
	month = feb,
	year = {2023},
	note = {arXiv:2302.05442 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{sun_revisiting_2017,
	title = {Revisiting {Unreasonable} {Effectiveness} of {Data} in {Deep} {Learning} {Era}},
	url = {http://arxiv.org/abs/1707.02968},
	doi = {10.48550/arXiv.1707.02968},
	abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
	month = aug,
	year = {2017},
	note = {arXiv:1707.02968 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{shevlane_offense-defense_2020,
	address = {New York, NY, USA},
	series = {{AIES} '20},
	title = {The {Offense}-{Defense} {Balance} of {Scientific} {Knowledge}: {Does} {Publishing} {AI} {Research} {Reduce} {Misuse}?},
	isbn = {978-1-4503-7110-0},
	shorttitle = {The {Offense}-{Defense} {Balance} of {Scientific} {Knowledge}},
	url = {https://doi.org/10.1145/3375627.3375815},
	doi = {10.1145/3375627.3375815},
	abstract = {There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.},
	urldate = {2023-03-08},
	booktitle = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Shevlane, Toby and Dafoe, Allan},
	month = feb,
	year = {2020},
	keywords = {ai governance, disclosure of research, misuse of ai, publication norms},
	pages = {173--179},
}

@book{tang_did_2023,
	title = {Did {You} {Train} on {My} {Dataset}? {Towards} {Public} {Dataset} {Protection} with {Clean}-{Label} {Backdoor} {Insertion}},
	shorttitle = {Did {You} {Train} on {My} {Dataset}?},
	abstract = {The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, it also raises concerns about the unauthorized exploitation of the dataset, e.g., for commercial propose, which is forbidden by the dataset licenses. In this paper, we introduce a backdoor-based watermark-ing approach that can be used as a general framework to protect public-available data. By inserting a few watermarking samples into the protected dataset, the learning model implicitly learns a secret function preset by defenders. This hidden function then can be used as a watermark to track down third-party models that illegally use the dataset. Unfortunately, existing backdoor insertion methods highly rely on adding arbitrary, often clearly mislabeled data into the training set, which causes notable performance drop and can be easily removed by anomaly detection algorithms. To bridge the gap, we propose a clean-label backdoor watermarking framework. We utilize imperceptible perturbations to replace mislabeled samples. In this way, watermarking samples are consistent with original labels. The experiments on text, image, and audio datasets indicate that the proposed framework is effective in terms of watermarking dataset while keeping competitive performance on original tasks. Experimental analysis indicates that adding 1\% watermarking samples can stably inject a traceable watermarking function. Further studies demonstrate that watermarking samples are stealthy and look benign upon human scrutiny. The code is available at https: //github.com/Anonymous-Authors-Repo/watermark\_dataset},
	author = {Tang, Ruixiang and Feng, Qizhang and Liu, Ninghao and Hu, Xia},
	month = feb,
	year = {2023},
}

@misc{wenger_data_2023,
	title = {Data {Isotopes} for {Data} {Provenance} in {DNNs}},
	url = {http://arxiv.org/abs/2208.13893},
	doi = {10.48550/arXiv.2208.13893},
	abstract = {Today, creators of data-hungry deep neural networks (DNNs) scour the Internet for training fodder, leaving users with little control over or knowledge of when their data is appropriated for model training. To empower users to counteract unwanted data use, we design, implement and evaluate a practical system that enables users to detect if their data was used to train an DNN model. We show how users can create special data points we call isotopes, which introduce "spurious features" into DNNs during training. With only query access to a trained model and no knowledge of the model training process, or control of the data labels, a user can apply statistical hypothesis testing to detect if a model has learned the spurious features associated with their isotopes by training on the user's data. This effectively turns DNNs' vulnerability to memorization and spurious correlations into a tool for data provenance. Our results confirm efficacy in multiple settings, detecting and distinguishing between hundreds of isotopes with high accuracy. We further show that our system works on public ML-as-a-service platforms and larger models such as ImageNet, can use physical objects instead of digital marks, and remains generally robust against several adaptive countermeasures.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Wenger, Emily and Li, Xiuyu and Zhao, Ben Y. and Shmatikov, Vitaly},
	month = feb,
	year = {2023},
	note = {arXiv:2208.13893 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{li_untargeted_2022,
	title = {Untargeted {Backdoor} {Watermark}: {Towards} {Harmless} and {Stealthy} {Dataset} {Copyright} {Protection}},
	url = {https://openreview.net/forum?id=kcQiIrvA_nz},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Li, Yiming and Bai, Yang and Jiang, Yong and Yang, Yong and Xia, Shu-Tao and Li, Bo},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	year = {2022},
}

@misc{li_open-sourced_2020,
	title = {Open-sourced {Dataset} {Protection} via {Backdoor} {Watermarking}},
	url = {http://arxiv.org/abs/2010.05821},
	doi = {10.48550/arXiv.2010.05821},
	abstract = {The rapid development of deep learning has benefited from the release of some high-quality open-sourced datasets (\$e.g.\$, ImageNet), which allows researchers to easily verify the effectiveness of their algorithms. Almost all existing open-sourced datasets require that they can only be adopted for academic or educational purposes rather than commercial purposes, whereas there is still no good way to protect them. In this paper, we propose a {\textbackslash}emph\{backdoor embedding based dataset watermarking\} method to protect an open-sourced image-classification dataset by verifying whether it is used for training a third-party model. Specifically, the proposed method contains two main processes, including {\textbackslash}emph\{dataset watermarking\} and {\textbackslash}emph\{dataset verification\}. We adopt classical poisoning-based backdoor attacks (\$e.g.\$, BadNets) for dataset watermarking, ie, generating some poisoned samples by adding a certain trigger (\$e.g.\$, a local patch) onto some benign samples, labeled with a pre-defined target class. Based on the proposed backdoor-based watermarking, we use a hypothesis test guided method for dataset verification based on the posterior probability generated by the suspicious third-party model of the benign samples and their correspondingly watermarked samples (\$i.e.\$, images with trigger) on the target class. Experiments on some benchmark datasets are conducted, which verify the effectiveness of the proposed method.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Li, Yiming and Zhang, Ziqi and Bai, Jiawang and Wu, Baoyuan and Jiang, Yong and Xia, Shu-Tao},
	month = nov,
	year = {2020},
	note = {arXiv:2010.05821 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{janus_anomalous_2023,
	title = {Anomalous tokens reveal the original identities of {Instruct} models},
	url = {https://generative.ink/posts/anomalous-tokens-reveal-the-original-identities-of-instruct-models/},
	abstract = {GPT's behavior when prompted with anomalous tokens like ' SolidGoldMagikarp' can be used to associate fine-tuned models with base models. An idea inspired by black box cryptanalysis.},
	language = {en},
	urldate = {2023-03-07},
	author = {janus and jdp},
	month = feb,
	year = {2023},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2023-03-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}

@misc{xie_explanation_2022,
	title = {An {Explanation} of {In}-context {Learning} as {Implicit} {Bayesian} {Inference}},
	url = {http://arxiv.org/abs/2111.02080},
	doi = {10.48550/arXiv.2111.02080},
	abstract = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
	month = jul,
	year = {2022},
	note = {arXiv:2111.02080 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{schuhmann_laion-5b_2022,
	title = {{LAION}-{5B}: {An} open large-scale dataset for training next generation image-text models},
	shorttitle = {{LAION}-{5B}},
	url = {http://arxiv.org/abs/2210.08402},
	doi = {10.48550/arXiv.2210.08402},
	abstract = {Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. Announcement page https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
	month = oct,
	year = {2022},
	note = {arXiv:2210.08402 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	doi = {10.48550/arXiv.2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{ghodsi_safetynets_2017,
	title = {{SafetyNets}: {Verifiable} {Execution} of {Deep} {Neural} {Networks} on an {Untrusted} {Cloud}},
	volume = {30},
	shorttitle = {{SafetyNets}},
	url = {https://papers.nips.cc/paper/2017/hash/6048ff4e8cb07aa60b6777b6f7384d52-Abstract.html},
	abstract = {Inference using deep neural networks is often outsourced to the cloud since it is a computationally demanding task.  However, this raises a fundamental issue of trust. How can a client be sure that the cloud has performed inference correctly? A lazy cloud provider might use a simpler but less accurate model to reduce its own computational load, or worse, maliciously modify the inference results sent to the client. We propose SafetyNets, a framework that enables an untrusted server (the cloud) to provide a client with a short mathematical proof of the correctness of inference tasks that they perform on behalf of the client. Specifically, SafetyNets develops and implements a specialized interactive proof (IP) protocol for verifiable execution of a class of deep neural networks, i.e., those that can be represented as arithmetic circuits. Our empirical results on three- and four-layer deep neural networks demonstrate the run-time costs of SafetyNets for both the client and server are low. SafetyNets detects any incorrect computations of the neural network by the untrusted server with high probability, while achieving state-of-the-art accuracy on the MNIST digit recognition (99.4\%) and TIMIT speech recognition tasks (75.22\%).},
	urldate = {2023-03-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ghodsi, Zahra and Gu, Tianyu and Garg, Siddharth},
	year = {2017},
}

@misc{hendrycks_x-risk_2022,
	title = {X-{Risk} {Analysis} for {AI} {Research}},
	url = {http://arxiv.org/abs/2206.05862},
	doi = {10.48550/arXiv.2206.05862},
	abstract = {Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk.},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Mazeika, Mantas},
	month = sep,
	year = {2022},
	note = {arXiv:2206.05862 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{mitchell_model_2019,
	title = {Model {Cards} for {Model} {Reporting}},
	url = {http://arxiv.org/abs/1810.03993},
	doi = {10.1145/3287560.3287596},
	abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related AI technology, increasing transparency into how well AI technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
	urldate = {2023-03-04},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
	month = jan,
	year = {2019},
	note = {arXiv:1810.03993 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	pages = {220--229},
}

@misc{glaese_improving_2022,
	title = {Improving alignment of dialogue agents via targeted human judgements},
	url = {http://arxiv.org/abs/2209.14375},
	doi = {10.48550/arXiv.2209.14375},
	abstract = {We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78\% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8\% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Glaese, Amelia and McAleese, Nat and Trębacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and Campbell-Gillingham, Lucy and Uesato, Jonathan and Huang, Po-Sen and Comanescu, Ramona and Yang, Fan and See, Abigail and Dathathri, Sumanth and Greig, Rory and Chen, Charlie and Fritz, Doug and Elias, Jaume Sanchez and Green, Richard and Mokrá, Soňa and Fernando, Nicholas and Wu, Boxi and Foley, Rachel and Young, Susannah and Gabriel, Iason and Isaac, William and Mellor, John and Hassabis, Demis and Kavukcuoglu, Koray and Hendricks, Lisa Anne and Irving, Geoffrey},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14375 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{bai_training_2022,
	title = {Training a {Helpful} and {Harmless} {Assistant} with {Reinforcement} {Learning} from {Human} {Feedback}},
	url = {http://arxiv.org/abs/2204.05862},
	doi = {10.48550/arXiv.2204.05862},
	abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05862 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{omar_backdoor_2023,
	title = {Backdoor {Learning} for {NLP}: {Recent} {Advances}, {Challenges}, and {Future} {Research} {Directions}},
	shorttitle = {Backdoor {Learning} for {NLP}},
	url = {http://arxiv.org/abs/2302.06801},
	doi = {10.48550/arXiv.2302.06801},
	abstract = {Although backdoor learning is an active research topic in the NLP domain, the literature lacks studies that systematically categorize and summarize backdoor attacks and defenses. To bridge the gap, we present a comprehensive and unifying study of backdoor learning for NLP by summarizing the literature in a systematic manner. We first present and motivate the importance of backdoor learning for building robust NLP systems. Next, we provide a thorough account of backdoor attack techniques, their applications, defenses against backdoor attacks, and various mitigation techniques to remove backdoor attacks. We then provide a detailed review and analysis of evaluation metrics, benchmark datasets, threat models, and challenges related to backdoor learning in NLP. Ultimately, our work aims to crystallize and contextualize the landscape of existing literature in backdoor learning for the text domain and motivate further research in the field. To this end, we identify troubling gaps in the literature and offer insights and ideas into open challenges and future research directions. Finally, we provide a GitHub repository with a list of backdoor learning papers that will be continuously updated at https://github.com/marwanomar1/Backdoor-Learning-for-NLP.},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Omar, Marwan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06801 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{geiping_what_2022,
	title = {What {Doesn}'t {Kill} {You} {Makes} {You} {Robust}(er): {How} to {Adversarially} {Train} against {Data} {Poisoning}},
	url = {https://openreview.net/forum?id=VMuenFh7IpP},
	author = {Geiping, Jonas and Fowl, Liam H. and Somepalli, Gowthami and Goldblum, Micah and Moeller, Michael and Goldstein, Tom},
	year = {2022},
}

@inproceedings{sandoval-segura_poisons_2022,
	title = {Poisons {That} {Are} {Learned} {Faster} {Are} {More} {Effective}},
	url = {https://openaccess.thecvf.com/content/CVPR2022W/ArtOfRobust/html/Sandoval-Segura_Poisons_That_Are_Learned_Faster_Are_More_Effective_CVPRW_2022_paper.html},
	language = {en},
	urldate = {2023-03-04},
	author = {Sandoval-Segura, Pedro and Singla, Vasu and Fowl, Liam and Geiping, Jonas and Goldblum, Micah and Jacobs, David and Goldstein, Tom},
	year = {2022},
	pages = {198--205},
}

@inproceedings{wallace_concealed_2021,
	address = {Online},
	title = {Concealed {Data} {Poisoning} {Attacks} on {NLP} {Models}},
	url = {https://aclanthology.org/2021.naacl-main.13},
	doi = {10.18653/v1/2021.naacl-main.13},
	abstract = {Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains “James Bond”. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (“Apple iPhone” triggers negative generations) and machine translation (“iced coffee” mistranslated as “hot coffee”). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.},
	urldate = {2023-03-04},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Wallace, Eric and Zhao, Tony and Feng, Shi and Singh, Sameer},
	month = jun,
	year = {2021},
	pages = {139--150},
}

@misc{ethayarajh_stanford_2023,
	title = {Stanford {Human} {Preferences} {Dataset}},
	url = {https://huggingface.co/datasets/stanfordnlp/SHP},
	author = {Ethayarajh, Kawin and Zhang, Heidi and Wang, Yizhong and Jurafsky, Dan},
	year = {2023},
}

@misc{black_gpt-neox-20b_2022,
	title = {{GPT}-{NeoX}-{20B}: {An} {Open}-{Source} {Autoregressive} {Language} {Model}},
	shorttitle = {{GPT}-{NeoX}-{20B}},
	url = {http://arxiv.org/abs/2204.06745},
	doi = {10.48550/arXiv.2204.06745},
	abstract = {We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe {\textbackslash}model\{\}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06745 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{workshop_bloom_2022,
	title = {{BLOOM}: {A} {176B}-{Parameter} {Open}-{Access} {Multilingual} {Language} {Model}},
	shorttitle = {{BLOOM}},
	url = {http://arxiv.org/abs/2211.05100},
	doi = {10.48550/arXiv.2211.05100},
	abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Benoît and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurençon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and van Strien, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo González and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, Gérard and Kruszewski, Germán and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, Jörg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Muñoz, Manuel Romero and Masoud, Maraim and Grandury, María and Šaško, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and de Gibert, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and López, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Taşar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and von Platen, Patrick and Cornette, Pierre and Lavallée, Pierre François and Lacroix, Rémi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, Stéphane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and Névéol, Aurélie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and van der Wal, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdeněk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Muñoz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Clémentine and Periñán, Daniel León and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz, Madeleine Hahn and Takeuchi, Maiko and Pàmies, Marc and Castillo, Maria A. and Nezhurina, Marianna and Sänger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Théo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
	month = dec,
	year = {2022},
	note = {arXiv:2211.05100 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{carlini_poisoning_2023,
	title = {Poisoning {Web}-{Scale} {Training} {Datasets} is {Practical}},
	url = {http://arxiv.org/abs/2302.10149},
	doi = {10.48550/arXiv.2302.10149},
	abstract = {Deep learning models are often trained on distributed, webscale datasets crawled from the internet. In this paper, we introduce two new dataset poisoning attacks that intentionally introduce malicious examples to a model's performance. Our attacks are immediately practical and could, today, poison 10 popular datasets. Our first attack, split-view poisoning, exploits the mutable nature of internet content to ensure a dataset annotator's initial view of the dataset differs from the view downloaded by subsequent clients. By exploiting specific invalid trust assumptions, we show how we could have poisoned 0.01\% of the LAION-400M or COYO-700M datasets for just \$60 USD. Our second attack, frontrunning poisoning, targets web-scale datasets that periodically snapshot crowd-sourced content -- such as Wikipedia -- where an attacker only needs a time-limited window to inject malicious examples. In light of both attacks, we notify the maintainers of each affected dataset and recommended several low-overhead defenses.},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A. and Paleka, Daniel and Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tramèr, Florian},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10149 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{carlini_poisoning_2022,
	title = {Poisoning and {Backdooring} {Contrastive} {Learning}},
	url = {https://openreview.net/forum?id=iC4UHbQ01Mp},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Carlini, Nicholas and Terzis, Andreas},
	year = {2022},
}

@inproceedings{jia_proof--learning_2021,
	title = {Proof-of-{Learning}: {Definitions} and {Practice}},
	shorttitle = {Proof-of-{Learning}},
	doi = {10.1109/SP40001.2021.00106},
	abstract = {Training machine learning (ML) models typically involves expensive iterative optimization. Once the model’s final parameters are released, there is currently no mechanism for the entity which trained the model to prove that these parameters were indeed the result of this optimization procedure. Such a mechanism would support security of ML applications in several ways. For instance, it would simplify ownership resolution when multiple parties contest ownership of a specific model. It would also facilitate the distributed training across untrusted workers where Byzantine workers might otherwise mount a denial-ofservice by returning incorrect model updates.In this paper, we remediate this problem by introducing the concept of proof-of-learning in ML. Inspired by research on both proof-of-work and verified computations, we observe how a seminal training algorithm, stochastic gradient descent, accumulates secret information due to its stochasticity. This produces a natural construction for a proof-of-learning which demonstrates that a party has expended the compute require to obtain a set of model parameters correctly. In particular, our analyses and experiments show that an adversary seeking to illegitimately manufacture a proof-of-learning needs to perform at least as much work than is needed for gradient descent itself.We also instantiate a concrete proof-of-learning mechanism in both of the scenarios described above. In model ownership resolution, it protects the intellectual property of models released publicly. In distributed training, it preserves availability of the training procedure. Our empirical evaluation validates that our proof-of-learning mechanism is robust to variance induced by the hardware (e.g., ML accelerators) and software stacks.},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Jia, Hengrui and Yaghini, Mohammad and Choquette-Choo, Christopher A. and Dullerud, Natalie and Thudi, Anvith and Chandrasekaran, Varun and Papernot, Nicolas},
	month = may,
	year = {2021},
	note = {ISSN: 2375-1207},
	keywords = {Computational modeling, Intellectual property, Machine learning, Privacy, Software, Stochastic processes, Training, machine-learning, proof-of-work, security},
	pages = {1039--1056},
}

@misc{villalobos_will_2022,
	title = {Will we run out of data? {An} analysis of the limits of scaling datasets in {Machine} {Learning}},
	shorttitle = {Will we run out of data?},
	url = {http://arxiv.org/abs/2211.04325},
	doi = {10.48550/arXiv.2211.04325},
	abstract = {We analyze the growth of dataset sizes used in machine learning for natural language processing and computer vision, and extrapolate these using two methods; using the historical growth rate and estimating the compute-optimal dataset size for future predicted compute budgets. We investigate the growth in data usage by estimating the total stock of unlabeled data available on the internet over the coming decades. Our analysis indicates that the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images). Our work suggests that the current trend of ever-growing ML models that rely on enormous datasets might slow down if data efficiency is not drastically improved or new sources of data become available.},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Villalobos, Pablo and Sevilla, Jaime and Heim, Lennart and Besiroglu, Tamay and Hobbhahn, Marius and Ho, Anson},
	month = oct,
	year = {2022},
	note = {arXiv:2211.04325 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{stiennon_learning_2020,
	title = {Learning to summarize with human feedback},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html},
	abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task.  For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences.  We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning.  We conduct extensive analyses to understand our human feedback dataset and fine-tuned models.  We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans.  We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
	urldate = {2023-03-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
	year = {2020},
	pages = {3008--3021},
}

@article{giattino_artificial_2022,
	title = {Artificial {Intelligence}},
	journal = {Our World in Data},
	author = {Giattino, Charlie and Mathieu, Edouard and Broden, Julia and Roser, Max},
	year = {2022},
}

@misc{huang_generative_2023,
	title = {Generative {AI} and the {Digital} {Commons}},
	url = {https://cip.org/research/generative-ai-digital-commons},
	abstract = {The aim of this paper is to build models of governance for generative foundation models that enable broadly shared benefit. Our initial hypothesis is that data is a high-value lever of governance. Many of these models are trained on publicly available data and use public infrastructure, but 1) may d},
	language = {en-CA},
	urldate = {2023-03-04},
	author = {Huang, Saffron and Siddarth, Divya},
	month = feb,
	year = {2023},
}

@misc{delacroix_democratising_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Democratising the {Digital} {Revolution}: {The} {Role} of {Data} {Governance}},
	shorttitle = {Democratising the {Digital} {Revolution}},
	url = {https://papers.ssrn.com/abstract=3720208},
	abstract = {This chapter explores the role that data governance can play in shaping the development of AI technologies. It starts by considering how the role of law and governance systems in the digital environment is shifting, prompted by public incidents that have exposed the negative or unintended consequences of data use for both individuals and society. As the ‘wild west’ view of the digital sphere as an ungoverned, or ungovernable space, becomes increasingly outmoded, the chapter considers how policymakers and legislators are increasingly seeking means through which to assert social values in digital systems. With a variety of legal and policy structures already seeking to influence patterns of data use and technology development, this chapter then briefly reviews recent legislative and policy activities, noting that – despite recent efforts – gaps in the policy landscape remain. Finding that new forms of bottom-up data sharing arrangement are needed to enhance democratic governance of data use, the chapter concludes by exploring the role of data trusts as a vehicle for leveraging the power associated with data aggregation.},
	language = {en},
	urldate = {2023-03-04},
	author = {Delacroix, Sylvie and Pineau, Joelle and Montgomery, Jessica},
	month = jun,
	year = {2020},
	keywords = {AI, data, data governance, data sharing, data trusts, democratic governance, vulnerability},
}

@incollection{christiano_democracy_2022,
	edition = {Spring 2022},
	title = {Democracy},
	url = {https://plato.stanford.edu/archives/spr2022/entries/democracy/},
	abstract = {Normative democratic theory deals with the moral foundations ofdemocracy and democratic institutions, as well as the moral duties ofdemocratic representatives and citizens. It is distinct fromdescriptive and explanatory democratic theory, which aim to describeand explain how democracy and democratic institutions function.Normative democracy theory aims to provide an account of when and whydemocracy is morally desirable as well as moral principles for guidingthe design of democratic institutions and the actions of citizens andrepresentatives. Of course, normative democratic theory is inherentlyinterdisciplinary and must draw on the results of political science,sociology, psychology, and economics in order to give concrete moralguidance., This brief outline of normative democratic theory focuses attention onseven related issues. First, it proposes a definition of democracy.Second, it outlines different approaches to the question of whydemocracy is morally valuable at all. Third, itdiscusses the issue of whether and when democratic institutions haveauthority and different conceptions of the limits of democraticauthority. Fourth, it explores the questionof what it is reasonable to demand of citizens in large democraticsocieties. This issue is central to the evaluation of normativedemocratic theories. A large body of opinion has it that mostclassical normative democratic theory is incompatible with what we canreasonably expect from citizens. Fifth, it surveys different accountsof the proper characterization of equality in the processes ofrepresentation and the moral norms of representation. Sixth, it discussesthe relationship between central findings in social choice theory anddemocracy. Seventh, it discusses the question of who should be includedin the group that makes democratic decisions.},
	urldate = {2023-03-02},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Christiano, Tom and Bajaj, Sameer},
	editor = {Zalta, Edward N.},
	year = {2022},
}

@misc{shevlane_structured_2022,
	title = {Structured access: an emerging paradigm for safe {AI} deployment},
	shorttitle = {Structured access},
	url = {http://arxiv.org/abs/2201.05159},
	doi = {10.48550/arXiv.2201.05159},
	abstract = {Structured access is an emerging paradigm for the safe deployment of artificial intelligence (AI). Instead of openly disseminating AI systems, developers facilitate controlled, arm's length interactions with their AI systems. The aim is to prevent dangerous AI capabilities from being widely accessible, whilst preserving access to AI capabilities that can be used safely. The developer must both restrict how the AI system can be used, and prevent the user from circumventing these restrictions through modification or reverse engineering of the AI system. Structured access is most effective when implemented through cloud-based AI services, rather than disseminating AI software that runs locally on users' hardware. Cloud-based interfaces provide the AI developer greater scope for controlling how the AI system is used, and for protecting against unauthorized modifications to the system's design. This chapter expands the discussion of "publication norms" in the AI community, which to date has focused on the question of how the informational content of AI research projects should be disseminated (e.g., code and models). Although this is an important question, there are limits to what can be achieved through the control of information flows. Structured access views AI software not only as information that can be shared but also as a tool with which users can have arm's length interactions. There are early examples of structured access being practiced by AI developers, but there is much room for further development, both in the functionality of cloud-based interfaces and in the wider institutional framework.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Shevlane, Toby},
	month = apr,
	year = {2022},
	note = {arXiv:2201.05159 [cs]},
	keywords = {68T99, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Software Engineering},
}

@article{brittain_getty_2023,
	chapter = {Legal},
	title = {Getty {Images} lawsuit says {Stability} {AI} misused photos to train {AI}},
	url = {https://www.reuters.com/legal/getty-images-lawsuit-says-stability-ai-misused-photos-train-ai-2023-02-06/},
	abstract = {Stock photo provider Getty Images has sued artificial intelligence company Stability AI Inc, accusing it in a lawsuit made public on Monday of misusing more than 12 million Getty photos to train its Stable Diffusion AI image-generation system.},
	language = {en},
	urldate = {2023-03-02},
	journal = {Reuters},
	author = {Brittain, Blake},
	month = feb,
	year = {2023},
}

@misc{noauthor_proposal_2021,
	title = {Proposal for a {REGULATION} {OF} {THE} {EUROPEAN} {PARLIAMENT} {AND} {OF} {THE} {COUNCIL} {LAYING} {DOWN} {HARMONISED} {RULES} {ON} {ARTIFICIAL} {INTELLIGENCE} ({ARTIFICIAL} {INTELLIGENCE} {ACT}) {AND} {AMENDING} {CERTAIN} {UNION} {LEGISLATIVE} {ACTS}},
	url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206},
	language = {en},
	urldate = {2023-03-02},
	year = {2021},
}

@techreport{noauthor_ai_2022,
	title = {{AI} {Risk} {Management} {Framework}: {Second} {Draft}},
	institution = {NIST},
	year = {2022},
}

@techreport{noauthor_establishing_2022,
	title = {Establishing a pro-innovation approach to regulating {AI}},
	url = {https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement},
	institution = {Office for Artificial Intelligence},
	month = jul,
	year = {2022},
}

@techreport{kakkad_new_2023,
	title = {A {New} {National} {Purpose}: {Innovation} {Can} {Power} the {Future} of {Britain}},
	shorttitle = {A {New} {National} {Purpose}},
	url = {https://institute.global/policy/new-national-purpose-innovation-can-power-future-britain},
	abstract = {Our Future of Britain project seeks to reinvigorate progressive politics to meet the challenges the country faces in the decades ahead. Our experts and thought leaders are setting out a bold, optimistic policy agenda.},
	language = {en},
	urldate = {2023-03-02},
	institution = {Tony Blair Institute for Global Change},
	author = {Kakkad, Jeegar and Macon-Cooney, Benedict and Northend, Jess and Rajkumar, Nitarshan and Stanley, Luke and Westgarth, Tom},
	month = feb,
	year = {2023},
}

@misc{gao_pile_2020,
	title = {The {Pile}: {An} {800GB} {Dataset} of {Diverse} {Text} for {Language} {Modeling}},
	shorttitle = {The {Pile}},
	url = {http://arxiv.org/abs/2101.00027},
	doi = {10.48550/arXiv.2101.00027},
	abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	month = dec,
	year = {2020},
	note = {arXiv:2101.00027 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{wu_insights_2022,
	title = {Insights into {Pre}-training via {Simpler} {Synthetic} {Tasks}},
	url = {http://arxiv.org/abs/2206.10139},
	doi = {10.48550/arXiv.2206.10139},
	abstract = {Pre-training produces representations that are effective for a wide range of downstream tasks, but it is still unclear what properties of pre-training are necessary for effective gains. Notably, recent work shows that even pre-training on synthetic tasks can achieve significant gains in downstream tasks. In this work, we perform three experiments that iteratively simplify pre-training and show that the simplifications still retain much of its gains. First, building on prior work, we perform a systematic evaluation of three existing synthetic pre-training methods on six downstream tasks. We find the best synthetic pre-training method, LIME, attains an average of \$67{\textbackslash}\%\$ of the benefits of natural pre-training. Second, to our surprise, we find that pre-training on a simple and generic synthetic task defined by the Set function achieves \$65{\textbackslash}\%\$ of the benefits, almost matching LIME. Third, we find that \$39{\textbackslash}\%\$ of the benefits can be attained by using merely the parameter statistics of synthetic pre-training. We release the source code at https://github.com/felixzli/synthetic\_pretraining.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Wu, Yuhuai and Li, Felix and Liang, Percy},
	month = jun,
	year = {2022},
	note = {arXiv:2206.10139 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{bai_constitutional_2022,
	title = {Constitutional {AI}: {Harmlessness} from {AI} {Feedback}},
	shorttitle = {Constitutional {AI}},
	url = {http://arxiv.org/abs/2212.08073},
	doi = {10.48550/arXiv.2212.08073},
	abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08073 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{irani_turkopticon_2013,
	address = {New York, NY, USA},
	series = {{CHI} '13},
	title = {Turkopticon: interrupting worker invisibility in amazon mechanical turk},
	isbn = {978-1-4503-1899-0},
	shorttitle = {Turkopticon},
	url = {https://doi.org/10.1145/2470654.2470742},
	doi = {10.1145/2470654.2470742},
	abstract = {As HCI researchers have explored the possibilities of human computation, they have paid less attention to ethics and values of crowdwork. This paper offers an analysis of Amazon Mechanical Turk, a popular human computation system, as a site of technically mediated worker-employer relations. We argue that human computation currently relies on worker invisibility. We then present Turkopticon, an activist system that allows workers to publicize and evaluate their relationships with employers. As a common infrastructure, Turkopticon also enables workers to engage one another in mutual aid. We conclude by discussing the potentials and challenges of sustaining activist technologies that intervene in large, existing socio-technical systems.},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Irani, Lilly C. and Silberman, M. Six},
	month = apr,
	year = {2013},
	keywords = {activism, amazon mechanical turk, design, ethics, human computation, infrastructure},
	pages = {611--620},
}

@misc{korinek_preparing_2022,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Preparing for the ({Non}-{Existent}?) {Future} of {Work}},
	shorttitle = {Preparing for the ({Non}-{Existent}?},
	url = {https://papers.ssrn.com/abstract=4147243},
	doi = {10.2139/ssrn.4147243},
	abstract = {This paper considers the labor market and distributional implications of a scenario of ever-more-intelligent autonomous machines that substitute for human labor and drive down wages. We lay out three concerns arising from such a scenario and evaluate recent predictions and objections to these concerns. Then we analyze how a utilitarian social planner would allocate work and income if these concerns start to materialize. As the income produced by autonomous machines rises and the value of labor declines, a utilitarian planner finds it optimal to phase out work, beginning with workers who have low labor productivity and job satisfaction, since they have comparative advantage in enjoying leisure. This is in stark contrast to welfare systems that force individuals with low labor productivity to work. If there are significant wage declines, avoiding mass misery will require other ways of distributing income than labor markets, whether via sufficiently well-distributed capital ownership or via benefits. Recipients could still engage in work for its own sake if they enjoy work amenities such as structure, purpose and meaning. If work gives rise to positive externalities such as social connections or political stability, or if individuals undervalue the benefits of work because of internalities, then a social planner would incentivize work. However, in the long run, the planner might be able to achieve a higher level of social welfare by adopting alternative ways of providing these benefits.Institutional subscribers to the NBER working paper series, and residents of developing countries may download this paper without additional charge at www.nber.org.},
	language = {en},
	urldate = {2023-02-22},
	author = {Korinek, Anton and Juelfs, Megan},
	month = jun,
	year = {2022},
	keywords = {Anton Korinek, Megan Juelfs, Preparing for the (Non-Existent?) Future of Work, SSRN},
}

@misc{weidinger_ethical_2021,
	title = {Ethical and social risks of harm from {Language} {Models}},
	url = {http://arxiv.org/abs/2112.04359},
	doi = {10.48550/arXiv.2112.04359},
	abstract = {This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	month = dec,
	year = {2021},
	note = {arXiv:2112.04359 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@article{mohamed_decolonial_2020,
	title = {Decolonial {AI}: {Decolonial} {Theory} as {Sociotechnical} {Foresight} in {Artificial} {Intelligence}},
	volume = {33},
	issn = {2210-5441},
	shorttitle = {Decolonial {AI}},
	url = {https://doi.org/10.1007/s13347-020-00405-8},
	doi = {10.1007/s13347-020-00405-8},
	abstract = {This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. While the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us, ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all.},
	language = {en},
	number = {4},
	urldate = {2023-02-22},
	journal = {Philosophy \& Technology},
	author = {Mohamed, Shakir and Png, Marie-Therese and Isaac, William},
	month = dec,
	year = {2020},
	pages = {659--684},
}

@inproceedings{hoffmann_empirical_2022,
	title = {An empirical analysis of compute-optimal large language model training},
	url = {https://openreview.net/forum?id=iBBcRUlOAPR},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katherine and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack William and Sifre, Laurent},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	year = {2022},
}

@misc{gao_scaling_2022,
	title = {Scaling {Laws} for {Reward} {Model} {Overoptimization}},
	url = {http://arxiv.org/abs/2210.10760},
	doi = {10.48550/arXiv.2210.10760},
	abstract = {In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed "gold-standard" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-\$n\$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Gao, Leo and Schulman, John and Hilton, Jacob},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10760 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{rae_scaling_2022,
	title = {Scaling {Language} {Models}: {Methods}, {Analysis} \& {Insights} from {Training} {Gopher}},
	shorttitle = {Scaling {Language} {Models}},
	url = {http://arxiv.org/abs/2112.11446},
	doi = {10.48550/arXiv.2112.11446},
	abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and Driessche, George van den and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and d'Autume, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
	month = jan,
	year = {2022},
	note = {arXiv:2112.11446 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{caballero_broken_2023,
	title = {Broken {Neural} {Scaling} {Laws}},
	url = {http://arxiv.org/abs/2210.14891},
	doi = {10.48550/arXiv.2210.14891},
	abstract = {We present a smoothly broken power law functional form that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, arithmetic, unsupervised/self-supervised learning, and reinforcement learning (single agent and multi-agent). When compared to other functional forms for neural scaling behavior, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models and extrapolates scaling behavior that other functional forms are incapable of expressing such as the non-monotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken\_neural\_scaling\_laws},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Caballero, Ethan and Gupta, Kshitij and Rish, Irina and Krueger, David},
	month = jan,
	year = {2023},
	note = {arXiv:2210.14891 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{adaptive_agent_team_human-timescale_2023,
	title = {Human-{Timescale} {Adaptation} in an {Open}-{Ended} {Task} {Space}},
	url = {http://arxiv.org/abs/2301.07608},
	doi = {10.48550/arXiv.2301.07608},
	abstract = {Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Adaptive Agent Team and Bauer, Jakob and Baumli, Kate and Baveja, Satinder and Behbahani, Feryal and Bhoopchand, Avishkar and Bradley-Schmieg, Nathalie and Chang, Michael and Clay, Natalie and Collister, Adrian and Dasagi, Vibhavari and Gonzalez, Lucy and Gregor, Karol and Hughes, Edward and Kashem, Sheleem and Loks-Thompson, Maria and Openshaw, Hannah and Parker-Holder, Jack and Pathak, Shreya and Perez-Nieves, Nicolas and Rakicevic, Nemanja and Rocktäschel, Tim and Schroecker, Yannick and Sygnowski, Jakub and Tuyls, Karl and York, Sarah and Zacherl, Alexander and Zhang, Lei},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07608 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{hilton_scaling_2023,
	title = {Scaling laws for single-agent reinforcement learning},
	url = {http://arxiv.org/abs/2301.13442},
	doi = {10.48550/arXiv.2301.13442},
	abstract = {Recent work has shown that, in generative modeling, cross-entropy loss improves smoothly with model size and training compute, following a power law plus constant scaling law. One challenge in extending these results to reinforcement learning is that the main performance objective of interest, mean episode return, need not vary smoothly. To overcome this, we introduce *intrinsic performance*, a monotonic function of the return defined as the minimum compute required to achieve the given return across a family of models of different sizes. We find that, across a range of environments, intrinsic performance scales as a power law in model size and environment interactions. Consequently, as in generative modeling, the optimal model size scales as a power law in the training compute budget. Furthermore, we study how this relationship varies with the environment and with other properties of the training setup. In particular, using a toy MNIST-based environment, we show that varying the "horizon length" of the task mostly changes the coefficient but not the exponent of this relationship.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Hilton, Jacob and Tang, Jie and Schulman, John},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13442 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{posada_coloniality_2021,
	title = {The {Coloniality} of {Data} {Work} in {Latin} {America}},
	url = {https://doi.org/10.1145%2F3461702.3462471},
	doi = {10.1145/3461702.3462471},
	booktitle = {Proceedings of the 2021 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Posada, Julian},
	month = jul,
	year = {2021},
}

@inproceedings{pushkarna_data_2022,
	title = {Data {Cards}: {Purposeful} and {Transparent} {Dataset} {Documentation} for {Responsible} {AI}},
	url = {https://doi.org/10.1145%2F3531146.3533231},
	doi = {10.1145/3531146.3533231},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Pushkarna, Mahima and Zaldivar, Andrew and Kjartansson, Oddur},
	month = jun,
	year = {2022},
}

@article{gebru_datasheets_2021,
	title = {Datasheets for datasets},
	volume = {64},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3458723},
	doi = {10.1145/3458723},
	abstract = {Documentation to facilitate communication between dataset creators and consumers.},
	language = {en},
	number = {12},
	urldate = {2023-01-27},
	journal = {Communications of the ACM},
	author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daumé and Crawford, Kate},
	month = dec,
	year = {2021},
	pages = {86--92},
}

@article{viljoen_relational_2021,
	title = {A {Relational} {Theory} of {Data} {Governance} {Feature}},
	volume = {131},
	url = {https://heinonline.org/HOL/P?h=hein.journals/ylr131&i=595},
	language = {eng},
	number = {2},
	urldate = {2023-02-22},
	journal = {Yale Law Journal},
	author = {Viljoen, Salome},
	year = {2021},
	pages = {573--654},
}

@misc{solaiman_gradient_2023,
	title = {The {Gradient} of {Generative} {AI} {Release}: {Methods} and {Considerations}},
	shorttitle = {The {Gradient} of {Generative} {AI} {Release}},
	url = {http://arxiv.org/abs/2302.04844},
	doi = {10.48550/arXiv.2302.04844},
	abstract = {As increasingly powerful generative AI systems are developed, the release method greatly varies. We propose a framework to assess six levels of access to generative AI systems: fully closed; gradual or staged access; hosted access; cloud-based or API access; downloadable access; and fully open. Each level, from fully closed to fully open, can be viewed as an option along a gradient. We outline key considerations across this gradient: release methods come with tradeoffs, especially around the tension between concentrating power and mitigating risks. Diverse and multidisciplinary perspectives are needed to examine and mitigate risk in generative AI systems from conception to deployment. We show trends in generative system release over time, noting closedness among large companies for powerful systems and openness among organizations founded on principles of openness. We also enumerate safety controls and guardrails for generative systems and necessary investments to improve future releases.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Solaiman, Irene},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04844 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}
