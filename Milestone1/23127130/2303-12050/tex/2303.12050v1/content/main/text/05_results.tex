
\section{Experiments} \label{sec:experiments}

\arch achieves accurate and efficient prediction on both object-level and outdoor scene-level datasets (see \cref{fig:teaser}). In \cref{sec:object-segmentation} we evaluate our model on the task of object-level part segmentation on ShapeNet~\cite{Chang2015ARXIV} and on a new dataset collected with the \kortx vision system~\cite{summer-robotics}. 
In \cref{sec:audi} and \cref{sec:nuscenes}, we evaluate semantic segmentation on larger outdoor scenes using the Audi Autonomous Driving Dataset (A2D2) \cite{Geyer2020ARXIV} and nuScenes dataset \cite{Caesar2020CVPR}, respectively. We note that each dataset exhibits a unique sensor scanning configuration.
\cref{sec:ablations} ablates the key components of \arch. For more experiments including classification, please refer to the supplementary material.

% For object part segmentation, we evaluate on the ShapeNet \cite{shapenet} and KortX datasets. For outdoor semantic segmentation, we evaluate on the nuScenes benchmark \cite{nuscenes} and the Audi Autonomous Driving Dataset (A2D2) \cite{a2d2}. In \cref{sec:object-segmentation}, \cref{sec:audi-segmentation}, and \cref{sec:nuscenes-segmentation}, we report evaluations on object part segmentation, A2D2 dataset LiDAR segmentation, and nuScenes LiDAR segmentation, respectively. In \cref{sec:ablations}, we ablation analysis on \arch. % Finally, \cref{sec:mem-analysis} showcases the importance of low GPU memory usage for mobile devices and batch processing. 

% The Audi Driv-
% ing dataset offers unique grid-like scanning patterns, which
% are distinct from NuScenes, and we are the first to eval-
% uate existing segmentation methods on this relatively new
% dataset. Through extensive experimentation, we demon-
% strate that utilizing the curve structures leads to outstanding
% performance in 3D point processing

% We provide an overview of the datasets and experimental methods in \cref{sec:datasets}. In \cref{sec:kortx}, we report network performance on the KortX Part Segmentation. In \cref{sec:nuscenes}, we report network performance on nuScenes LiDAR Segmentation. 

\input{content/main/figures/kortx_qual.tex}
\input{content/main/tables/a2d2.tex}

\subsection{Object Part Segmentation} \label{sec:object-segmentation}

\paragraph{ShapeNet Dataset}
The ShapeNet Part Segmentation Benchmark \cite{Chang2015,Yi2016ToG} contains 16,881 synthetic shape models across 16 different categories. 
To match the \kortx dataset discussed below, we train and evaluate on only six categories: \emph{cap, chair, earphone, knife, mug}, and \emph{rocket}. 
Following the official splits~\cite{Yi2016ToG}, these categories yield 6,398 training shapes and 460 validation shapes.
To evaluate performance on the laser-based scans that we are interested in, we simulate laser capture using the ShapeNet meshes.
% While previous works evaluate on uniformly sampled point clouds, we are interested in the output of a modern laser-based scanner, \ie a point cloud with 1D curve structure. 
% To accomplish this, we simulate laser-based capture on the ShapeNet meshes. 
We randomly sample a sensor pose facing the mesh and a set of linear laser traversals, then sample points on the mesh along each traversal (see \cref{fig:object-samples}, left).
% The left side of \cref{fig:object-samples} illustrates examples of simulated ShapeNet point clouds.
 For each training shape, we generate five synthetic scans from different sensor poses, yielding a training set of 31,991 point clouds. 
 % Additional details regarding the scanning simulation and an evaluation on all ShapeNet categories is provided in the supplement.

\paragraph{\kortx Dataset}
\kortx is a perception software system developed by Summer Robotics~\cite{summer-robotics} that directly generates and operates on 3D curves sampled from a triangulated system of event sensors and laser scanners. \kortx software supports arbitrary continuous scan patterns, allowing a user to create their own patterns and use their own scan hardware. Using \kortx, we scan $7$ real-world objects (cap, chair, earphone, knife, mug-1, mug-2, and rocket) multiple times in different poses, collecting $39$ scans in total. Examples of scanned objects are provided in the right side of \cref{fig:object-samples}. Because the \kortx platform provides a continuous event-based 3D scan output (points are sampled every 5$\mu$s), we defined a ``frame" as a batch of 2048 consecutive point measurements, corresponding to roughly a 20Hz frame rate. Because each frame differs in its dynamic scanning pattern, we evaluate on 5 consecutive frames per scan in our \kortx dataset, hence resulting in 195 point clouds in total. We will release this dataset upon publication.


% \kortx is an event-based laser scanner developed by Summer Robotics~\cite{summer-robotics} that allows rapid and high-fidelity 3D capture for indoor and outdoor environments. 
% Notably, the \kortx is \textit{user-controllable}, enabling arbitrary scanning patterns. 
% Using \kortx, we scan $7$ real-world objects (\emph{cap, chair, earphone, knife, mug-1, mug-2} and \emph{rocket}) multiple times in different poses, collecting $39$ scans in total. Examples of scanned objects are provided in the right side of \cref{fig:object-samples}.
% The scanner provides a constant stream of 3D points, so we define a ``frame" as 2048 consecutive point measurements, corresponding to roughly a 20Hz frame rate. 
% Because each frame differs in its dynamic scanning pattern, we evaluate on 5 consecutive frames per scan in our \kortx dataset, hence resulting in 195 point clouds in total. We will release this dataset upon publication.
% For a thorough overview of the KortX sensor and the KortX dataset, please refer to the Supplementary.

\paragraph{Results}
We train CurveCloudNet and several baselines on the simulated ShapeNet training set and evaluate all methods on the simulated ShapeNet validation set and our \kortx dataset. All methods are trained over four random seeds, and we report the mean and standard deviation of the class-averaged mean intersection-over-union (mIOU) over the runs. For fair comparison, all models are trained for 60 epochs using the same hyperparameters, and the best validation mIOU throughout training is reported.
% For fair comparison, we train all models for 60 epochs with a batch size of 24, a learning rate of 0.0001, batch momentum decay of 0.97, and exponential learning rate decay of 0.97. 
% For each run, we record the highest achieved validation mIOU. 
% We report means and standard deviations over four runs.

Results are summarized in \cref{tab:summer-robotics}.
CurveCloudNet outperforms baselines on both datasets, effectively handling in-domain scans (ShapeNet) and generalizing to out-of-domain scans (\kortx).
CurveCloudNet is competitive with other object-level backbones in terms of latency and GPU memory usage. 
In contrast, Cylinder3D, which is designed for large outdoor scenes, exhibits lower accuracy, worse latency, and higher GPU memory usage on this object-level task. 
\cref{fig:kortx-qual} shows that CurveCloudNet distinguishes fine-grained structures, such as chair legs, mug handles, and the orientation of a near-symmetrical knife.

% \input{content/main/tables/kortx-data.tex}

\input{content/main/figures/a2d2_qual.tex}

\input{content/main/tables/nuscenes.tex}

\subsection{A2D2 \lidar Segmentation}\label{sec:audi}

\paragraph{A2D2 Dataset}
The Audi Autonomous Driving Dataset (A2D2) \cite{Geyer2020ARXIV} contains 41,280 frames of outdoor driving scenes captured from 5 overlapping \lidar sensors, creating a unique grid-like scanning pattern (see \cref{fig:overview}).
Each frame is annotated from the front-facing camera with a 38-category semantic label. 
% Notably, the five LiDARs create a unique grid-like scanning pattern. 
To the best of our knowledge, we are the first to evaluate LiDAR segmentation on this relatively new dataset, so we pre-process the data for our setup.
We define a mapping from camera categories to \lidar categories and remove texture-only (\eg sky, lane markers, blurred-area) and very rare categories (\eg animals, tractors, utility vehicles). 
In total, we evaluate on 12 \lidar categories: \emph{car, bicycle, truck, person, road, sidewalk, obstacle, building, nature, pole, sign,} and \emph{traffic signal}. 
Evaluation is constrained to annotated LiDAR points, \ie the field of view of the front-facing camera.

\paragraph{Results}
We train CurveCloudNet along with point and voxel-based baselines on the official A2D2 training split~\cite{Geyer2020ARXIV}. 
For fair comparison, all models are trained for 140 epochs using the same hyperparameters, and the best validation mIOU throughout training is reported.
% with a batch size of 7, a learning rate of 0.001, and an exponential learning rate decay of 0.97. For each run, We report the highest achieved validation class-averaged mIOU over all training epochs.

Results are summarized in \cref{tab:a2d2-results}. 
CurveCloudNet scales to outdoor scenes significantly better than point-based backbones, with the runner-up PointMLP showing 6\% drop in mIOU and a $3\times$ increase in GPU memory. 
Compared to the voxel-based backbone Cylinder3D~\cite{Zhou2020ARXIV}, CurveCloudNet achieves higher accuracy with similar latency and a $4\times$ reduction in GPU memory, which is beneficial for on-board autonomous vehicle (AV) applications. 
% Please see the supplement for additional analysis of latency.
% When analyzing category breakdown, we notice that CurveCloudNet performs disproportionately well on off-road objects (\eg signs, poles, nature, and buildings), suggesting that CurveCloudNet can better handle the non-uniform structure of LiDAR measurements that arise outside of the road plane. 
The strong performance on the grid-like sampling pattern of A2D2 suggests that CurveCloudNet is well-suited for more general LiDAR data that does not follow a uniform sweeping pattern. 
\cref{fig:a2d2_qual} qualitatively demonstrates the potential advantage of CurveCloudNet over Cylinder3D. %shows that, compared to  Cylinder3D, CurveCloudNet can inform larger-object prediction with fine-grained details, \eg identifying the curb of the sidewalk (top), cohesively segmenting all parts of a truck (middle), and distinguishing a car from a truck (bottom).


\subsection{nuScenes LiDAR Segmentation} \label{sec:nuscenes}
\paragraph{nuScenes Dataset}
The nuScenes dataset \cite{Caesar2020CVPR} contains 1000 sequences of driving data, each 20 seconds long. 
Each sequence contains 32-beam \lidar data with segmentations annotated at 2Hz. %, yielding 40,000 point clouds. 
We follow the official nuScenes benchmark protocol with 16 semantic categories.

\paragraph{Results}
We train CurveCloudNet and the baselines on the official nuScenes training split. 
To ensure fair comparison, we train all models for 100 epochs. %with a batch size of 2, a learning rate of 0.001, and an exponential learning rate decay of 0.96. 
% We report class-averaged mIOU on the validation split.
Results on the nuScenes validation split are shown in \cref{tab:nuscenes-val-results}. 
CurveCloudNet significantly improves upon other point-based networks: PointMLP shows more than a 7\% drop in mIOU and $\sim 7\times$ increase in GPU memory. 
CurveCloudNet also outperforms projection-based methods RangeNet++~\cite{Milioto2019IROS} and SalsaNext~\cite{Cortinhal2020ISVC}, and achieves comparable accuracy to the voxel-based Cylinder3D while using half the GPU memory.
% Furthermore, CurveCloudNet bridges the gap between point cloud backbones and sparse voxel backbones these large scenes, achieving comparable accuracy to the popular Cylinder3D while using half the GPU memory. 
% As indicated by the per-category breakdown, CurveCloudNet appears to segmenting small such as bicycles, motorcycles, and pedestrians.
Overall, CurveCloudNet bridges the gap between point and voxel-based methods: it scales point-based processing to large outdoor scenes, achieving competitive accuracy to voxel-based methods. Furthermore, CurveCloudNet's small memory footprint makes it suitable for on-board AV applications and mobile devices.

% In addition to  benefiting on-board AV applications, reducing GPU memory is valuable for efficient deployment of 3D perception on mobile devices; for example, on a mobile 940MX GPU, CurveCloudNet has equal latency to Cylinder3D (see supplement for additional analysis).
% CurveCloudNet offers a high-accuracy alternative to voxel backbones when low GPU memory usage is important, \eg on mobile devices, when running many networks concurrently, and when using large batches. 
% In fact, on a mobile 940MX GPU, CurveCloudNet has equal latency to Cylinder3D (see supplement for additional latency analysis).


\subsection{Ablation Study}\label{sec:ablations}
\input{content/main/tables/curvecloudnet_ablation.tex}
\cref{tab:a2d2-ablation} shows an ablation analysis of CurveCloudNet on the A2D2 dataset; the table shows that each of our proposed curve operations is essential to achieve high accuracy and efficiency.
We ablate grouping along curves by instead using the regular radial groupings from PointNet++~\cite{Qi2017NIPS}. This ignores the curve structure and results in decreased accuracy, increased latency, and a significant increase in GPU memory usage. Instead of curve farthest point sampling, we also try regular FPS, which causes a decreased accuracy and increased latency.
Finally, without 1D curve convolutions, we observe a notable decline in accuracy with a marginal improvement in latency and GPU memory. Taken together, our curve operations increase accuracy with roughly half the latency and one third the GPU memory requirements.
% \subsection{GPU Memory Analysis}\label{sec:mem-analysis}
% \input{content/main/figures/batch-size.tex}
% \input{content/main/figures/gpu-type.tex}
% We illustrate the importance of low GPU memory in \cref{fig:batch-size} and \cref{fig:gpu-type}.


