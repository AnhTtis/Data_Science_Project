
\section{Introduction}
\label{sec:intro}

% modern sensors exhibit curve patterns but not being utilized
% We introduce a novel point cloud processing scheme designed to exploit the curve-like structures of the 3D sensors. The resulting point cloud processing backbone, dubbed Curbenet is efficient, scalable, and expressive in terms of accuracy. 
% At the core of CruveCloudNet lies 

% 3D sensors useful in many applications
% Effectively processing the output of 3D sensors is essential for 3D-aware scene understanding with important applications in autonomous driving and robotics.
%       many sensors output point clouds with special structure from scanning, curves. (e.g in fig...)
Many modern 3D sensors such as \lidar operate by sweeping laser-beams across the scene. As the laser traverses object surfaces, it returns dense measurements along the scanning direction, resulting in a 3D point cloud with notable curve-like structures (see \cref{fig:overview}). 
While the computer vision community has proposed various backbones for processing 3D point clouds for fundamental tasks such as semantic segmentation \cite{Qi2017CVPR, Qi2017NIPS, Wang2019SIGGRAPHb, Thomas2019ICCV, Hu2020CVPR}
and object detection \cite{Wu2022GeosciRemoteSens, Wang2021NeurIPS, Wu2022ARXIV, Wu2022CVPR, Yang2022ECCV}, 
they rarely make use of the innate curve-like structures of the scanner outputs. 

In this work, we present a novel point cloud processing scheme designed to explicitly consider the curve-like structure of 3D sensor outputs. Instead of treating each point independently, we parameterize the point cloud as a collection of polylines, \ie curves. As a result, we establish a local structure on the points, allowing for efficient communication between points along a curve. We propose a new backbone, CurveCloudNet, that applies 1D operations along curves. CurveCloudNet is efficient, scalable, and accurate, achieving state-of-the-art segmentation and classification performance on objects and large outdoor scenes with varying scanning patterns.

%
%
% along the curves utilizing the fixed spatial order of curve points. %Additionally, our backbone can account for arbitrary 1D geometric structure, a capability that is increasingly important as laser-scanners such as LiDAR become more dynamic and controllable in their scanning. %, \colton{not sure if the following part makes sense}, \ie it enables the best of both worlds from object-level and scene-level reasoning.efficient, scalable, and highly accurate, achieving advanced segmentation results on both objects and large scenes

Existing backbones \cite{Qi2017NIPS, Su2018CVPR, Hua2018CVPR, Wang2018CVPRb, Xu2018ECCV, Esteves2018ECCV,
Wu2019CVPR, Thomas2019ICCV} that directly operate on 3D points typically exchange and aggregate point features in Euclidean space, and have shown success on several downstream tasks (\eg semantic segmentation, shape classification, \etc) for individual objects or relatively small indoor scenes. These methods, however, do not scale well to large scenes (\eg in outdoor settings) due to inefficiencies in processing large unstructured point sets. On the other hand, recent voxel-based methods \cite{Wu2018ICRA, Su2018CVPR, Graham2018CVPR, Choy2019CVPR,
Liu2019NeurIPS, Zhou2020ARXIV, Hu2020CVPR, Zhang2022ARXIV} rely on efficient sparse data structures that scale better to large scenes. However, they require additional memory and, for small point sets, incur a latency overhead. Additionally, sensors with irregular and unpredictable sampling densities can lead to challenges in discretization,
% for certain geometries; discards informtion
as previous works have noted in the case of fine structures \cite{Lei2019CVPR} or cylindrical patterns \cite{Zhou2020ARXIV}. % Furthermore, the sparse (or dense) voxel data structure incurs an unnecessary memory footprint for object or small scene . % are often specifically tailored for the circular sweeping LiDAR patterns, and therefore do not generalize well to other scene types or scanning patterns, \eg segmenting small objects.

Our proposed \arch combines 1D curve reasoning with established point-based operations to achieve the best of both points and voxels. That is, curve reasoning makes our backbone scalable, accurate, and low-memory for both objects and large-scale scenes and across many scanning patterns. To the best of our knowledge, our work is the first to experimentally showcase accurate and efficient predictions on \emph{both} object-level and outdoor datasets. % We, in contrast, can achieve this flexibility by imposing a local 1D structure on the points that is consistent with the sensor's mode of data capture. % and flexible scalability compared to previous point-based methods, while also outperforming sparse-voxel methods on both objects and outdoor datasets. 
% improve accuracy over previous point-based methods without specific LiDAR patterns assumed by voxel-based methods. 
% Notably, on scene-level LiDAR segmentation, CurveCloudNet scales significantly better than any of the previous point-based methods and even performs competitively with sparse-voxel methods while maintaining a smaller GPU memory footprint. 
The essence of CurveCloudNet is a novel adaptation of three standard point cloud operations for 1D curve structures: (1) a \textit{symmetrical 1D convolution} that operates along curves, (2) a \textit{ball grouping} similar to PointNet++ \cite{Qi2017NIPS} that groups points along curves, and (3) an efficient \textit{1D farthest-point-sampling} algorithm on curves.
We integrate these components into our new architecture, combining curve operations with point-based operations \cite{Wang2019SIGGRAPHb, Qi2017NIPS, Ma2022ICLR, Hu2020CVPR}, resulting in a versatile, high-performing backbone that uses little GPU memory (see \cref{fig:teaser}).

We evaluate CurveCloudNet on both object-level and outdoor scene-level datasets. For the object part segmentation task, we use ShapeNet \cite{Chang2015ARXIV, Yi2016ToG} along with our new \kortx \cite{summer-robotics} dataset; the latter includes real-world data of objects scanned in a curve-like manner. For the outdoor semantic segmentation task, we use the nuScenes~\cite{Caesar2020CVPR} and Audi Autonomous Driving (A2D2)~\cite{Geyer2020ARXIV} datasets. The A2D2 dataset offers a unique sensor configuration resulting in a grid-like scanning pattern, which is distinct from previous LiDAR datasets \cite{Geiger2012CVPR, Caesar2020CVPR, Chang2019CVPR, Sun2020CVPR}. Supplementary experiments on object classification demonstrate flexibility to other perception tasks. Our evaluations demonstrate that using curve structures leads to improved performance on all experiments.

% Concretely, given the laser-scanned point cloud input, we propose converting it into a \emph{curve cloud} defined as an unordered set of 1D polylines.
% As shown in \cref{fig:teaser}, the curve cloud links together points on a surface that were captured in quick succession and close spatial proximity by the scanner.
% This 3D representation maintains the resolution of the original point cloud, and also provides more information to a network by hinting at detailed geometric properties such as surface connectivity, boundaries, curvatures, and tangents.
% Moreover, a curve cloud can be processed efficiently because there are far fewer curves than points and points along a curve have a fixed spatial ordering.

% curve operations and curve backbone
% To efficiently process a curve cloud, we adapt three common point cloud operations to run on curves: (1) \textit{ball grouping} similar to PointNet++ \cite{pointnet2} to group points along curves for processing, (2) a \textit{symmetric 1D convolution} that runs along curves, and (3) an efficient furthest-point-sampling algorithm along curves.
% We incorporate these layers into a novel backbone called CurveCloudNet, which combines curve operations with existing point-based operations \cite{dgcnn, pointnet2, pointmlp, randlanet}.

% \colton{TODO: Rework the following 2 paragraphs (or integrate them into the first 4).}
% To put the versatility of CurveCloudNet in context, existing backbones that directly process points in the Euclidean space, including PointNet-variants~\cite{pointnet,pointnet2}, KPConv~\cite{kpconv}, DGCNN~\cite{dgcnn}, and attention-based methods~\cite{pointtransformer, point4dtransformer}, are adopted mainly to segment individual objects or small indoor scenes. These methods do not scale well to large outdoor scenes (\eg, self-driving settings) due to inefficiency in processing a large number of points or low accuracy due to overly-coarse processing. 


% point-based approaches useful for object-level and small indoor scenes. Detailed. but don't leverage curves, and don't scale. Also mostly tested on nice uniform point clouds like shapenet, not so much real-world scans?
% Several point-based backbones have been proposed to process such 3D data including PointNet~\cite{pointnet,pointnet2}, convolutional~\cite{kpconv}, graph-based~\cite{dgcnn}, and attention-based~\cite{pointtransformer, point4dtransformer} methods. 
% These approaches excel at capturing the details needed to segment individual objects or small indoor scenes by operating directly on points and leveraging hierarchical structures. However, these methods are usually designed for uniformly-sampled point cloud inputs and ignore sensor-specific information. Additionally, these methods cannot scale to large outdoor scenes (\eg, self-driving settings) due to the massive number of points. 

% recently sparse voxels have dominated large outdoor scenes (like self-driving) b/c of efficiency. though some methods (cylinder3d) are tailored to a specific structure like lidar, the voxelization process inherently destroys detailed info and this doesn't generalize to other sensors. 
% For large scenes, sparse voxel-based methods~\cite{sparse-voxs} tend to significantly outperform point-based methods likely due to the use of efficient sparse data structures that effectively capture the coarse structure of large outdoor scenes. However, sparse voxel-based representations are often specifically designed for the circular sweeping nature of \lidar in many driving datasets, with some methods even adopting polar coordinates \cite{polarnet} or cylindrical convolutions \cite{cyl3d} that do not generalize well to other scene-types or scanning patterns.

% in this work, we propose to explicitly leverage the curve-like structure of laser scanners and bridge the gap between point and sparse-voxel methods. Our method works well both at object-level due to ability to capture detailed curve info, and efficiently scales to large outdoor scenes with extremely efficient curve operations. 
% In this work, we propose a 3D representation and neural network backbone that explicitly leverages the curve-like 1D structure found in laser-scanned point clouds.
% By treating the point cloud as a set of polylines, our backbone can maintain full-resolution details, scale to large scenes, and account for arbitrary 1D geometric structure, \ie getting the best of both worlds from object-level point-based approaches and scene-level sparse-voxel methods.
% For the semantic segmentation task, exploiting curve structures gives better accuracy than other point-based approaches and generalizes better than a recent sparse-voxel method with reduced VRAM usage.

% curve-cloud: summary, why we expect it to help intuitively (geometric properties)


% key findings / improvements: beat others on object-level, scales to scene-level to be competitive with voxel but generalizes better with lower GPU usage to lidar and event laser sampling patterns
% Moreover, our backbone generalizes better than \lidar-specific sparse-voxel methods to handle arbitrary scanning patterns.

% contributions
In summary, we make the following \textbf{contributions}: 
\textbf{(1)} we propose operating on laser-scanned point cloud data using a \emph{curve cloud} representation, \textbf{(2)} we adapt common point cloud operations, including the ball grouping, convolution, and farthest point sampling, to efficiently run on curve clouds, \textbf{(3)} we design a novel backbone, CurveCloudNet, that makes use of both curve and point operations, and \textbf{(4)} we show state-of-the-art segmentation results on real-world data captured for both objects and large-scale scenes with various scanning patterns.
% \begin{itemize}
%     \setlength\itemsep{0.1em}
%     \item We propose operating on laser-scanned point cloud data using a \emph{curve cloud} representation.
%     \item We adapt common point cloud operations, including the ball grouping, convolution, and farthest-point sampling, to efficiently run on curve clouds.
%     \item We design a novel backbone, CurveCloudNet, that makes use of both curve and point operations.
%     \item We show state-of-the-art segmentation results on real-world data captured for both objects and large-scale scenes with various scanning patterns. 
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Reasoning over and understanding the output of 3D sensors is essential for a variety of applications, including modern robotics, autonomous vehicles, topographic mapping, 3D quality control, and more. Due to its simplicity and universality, most deep-learning 3D backbones assume that sensors output a point cloud. 
% %
% Consequently, researchers have proposed a myriad of different 3D backbones for processing and understanding point cloud data.
% %
% Early on, researchers converted point clouds dense voxel grids \cite{a-lot-of-stuff}, although dense voxels were quickly regarded as computationally cumbersome. 
% %
% Beginning with PointNet \cite{pointnet}, many works turned to directly reasoning on 3D point clouds \cite{point-cloud-works}.
% %
% These point cloud backbones incorporated hierarchical reasoning \cite{pn2}, adapted convolutions to point clouds \cite{dgcnn, kpconv}, incorporated geometric priors  \cite{deltaconv, diffusionnet}, used self-attention \cite{pointtransformer, point4dtransformer}, and more \cite{others}.
% %
% In recent years, researchers adapted \textit{sparse} voxelization of 3D data for larger scene-level scans \cite{sparse-voxs}.
% %
% % In contrast to point-based methods, sparse voxel backbones are able to run expressive convolutional operations that can scale to larger scenes.

% %
% However, different 3D sensors perform 3D measurements in different ways, and as general-use 3D backbones matured, a variety of works turned to sensor-specific architectures. % and task-specific architectures.
% %
% For RGBD cameras, many works applied 2D convolutions directly on the image plane instead processing on 3D point clouds \cite{rgbd-works}. 
% % 
% Similarly, researchers had success projecting 360-degree ``sweeping" LiDAR into a 2D range view; this became and still is popular for very low-latency tasks. 
% %
% Additionally, many sparse-voxel LiDAR backbones use polar coordinates \cite{polarnet}, and cylindrical convolutions \cite{cyl3d} to better capture the circular sweeping nature of the sensor.
% %
% % In intrinsic tasks such as mesh correspondence and protein classification, recent works propose 2D surface convolutions to better understand geodesic properties \cite{mesh_cnn, diffusionnet, others}.
% %
% % Finally, a collection of research has modified backbones for 4D inputs \cite{spot-and-others}, high-frequency information  \cite{siren-and-others}, rotation-equivariance \cite{congyue}, medical imaging \cite{something}, and more.

% In this work, we follow the trend of sensor-specific architectures. 
% %
% However, unlike previous methods, we are interested in a developing a 3D backbone that can flexibly address a broad set of 1D sampling structures resulting from a variety of laser-based 3D scanners. 
% %
% Specifically, a variety of 3D sensors sweep laser-beams across surfaces. 
% %
% This is especially the case for 3D sensors that recover high-fidelity or long-range measurements, such as LiDAR, classic laser scanners, and event-based laser scanners. 
% %
% Due to the lasers' traversals, these sensors acquire dense 3D measurements \textit{in a laser's direction}; in the plane orthogonal to a laser's motion, no 3D information is captured.
% %
% This results in a rich 1D structure that is shared among all these sensors.

% We propose a 3D representation and backbone that makes use of the 1D sampling structure. 
% %
% Instead of treating each point equally, we convert the point cloud into and reason over a ``curve cloud", i.e. an unordered set of 1D polylines that encodes the sequential capturing and spatially proximity of adjacent points.
% %
% As observed in \cref{fig:teaser}, a curve cloud offers a flexible structure, greater efficiency, and greater information than the original point cloud.
% %
% We adapt three classic perception operations to run on curves.
% %
% First, we adapt the ball grouping outlined in PointNet++ \cite{pointnet2} to operate on curves.
% %
% Second, we define a symmetric 1D convolution that runs along curves.
% %
% Third, we implement an efficient furthest-point-sampling algorithm along curves.
% %
% Finally, we propose a novel backbone, CurveCloudNet, that combines the three aforementioned curve operations with existing point-based operations \cite{dgcnn, pointnet2, pointmlp, randlanet}.

% On real-world object part segmentation, CurveCloudNet improves accuracy over previous methods while maintaining low latency and GPU memory usage. 
% %
% On scene-level LiDAR segmentation, CurveCloudNet significantly outperforms previous point-based methods, lowers the GPU memory footprint compared to sparse-voxel methods, and generalizes across domains better than all methods.

% In summary, our contributions are as follows:
% %
% \begin{enumerate}
%     \item We propose the restructuring of laser-scanned point cloud data into a ``curve cloud".
%     \item We adapt common perception operations, including the ball grouping, convolution, and farthest-point-sampling, to efficiently run on curve clouds.
%     \item We design a novel backbone, CurveCloudNet, that makes use of both curve and point operations.
%     \item We show state-of-the-art the object-level segmentation on real-world data captured from the KortX sensor
%     \item We show state-of-the-art scene-level segmentation on the Audi Autonomous Driving Dataset and the nuScenes Dataset.
% \end{enumerate}
