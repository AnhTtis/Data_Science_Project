

\section{Method: Learning on Curve Clouds}
Our method takes as input a 3D point cloud, parses it into a curve cloud representation, and then processes the resulting curves for perception tasks by leveraging specialized curve operations, as shown in \cref{fig:overview}. We focus on object part segmentation, semantic scene segmentation, and object classification, although in principle, our method is suitable for any other point cloud perception task. 
% BELOW: REMOVED TO SAVE SPACE!
% We begin by describing how curve clouds are constructed and summarize their key advantages (\cref{sec:prelims}). 
% Next, we detail several new operations to effectively process curve clouds (\cref{sec:curve-ops}) and finally we introduce our novel architecture built upon these curve operations (\cref{sec:curvecloudnet}).

\input{content/main/figures/architecture.tex}

\subsection{Constructing Curve Clouds} \label{sec:prelims}
\paragraph{Problem Formulation}
The input to our model is the output of a laser-based 3D sensor represented as a point cloud $P = \{p_1, p_2, ..., p_N\}$, where $p_i = [x_i, y_i, z_i]$ refers to the 3D coordinates of the $i$-th point.
For each point, we are also given an associated acquisition timestamp $t_i$ and an integer laser beam ID $b_i \in [1, B]$, which are readily available from sensors like \lidar and are helpful in extracting curve information.
For a scanner with $B$ unique laser beams, $b_i$ indicates which beam captured the point while $t_i$ gives the ordering in which points were captured.
Timestamps differ only by microseconds and indicate point ordering for constructing the curve cloud; otherwise, the point cloud is treated as an instantaneous capture of the scene.

We assume that each laser beam in the scanner captures 3D points \textit{sequentially} and at a \textit{high sampling rate} as it sweeps the scene.
Concretely, if points $p_1$, $p_2$, and $p_3$ are recorded consecutively by beam $b$, then their timestamps are ordered $t_1{<}t_2{<}t_3$. 
Moreover, if two consecutive points are spatially farther apart than some small threshold $\delta$, we assume there is a surface discontinuity and the points lie on different surfaces in the scene.

% Our goal is to perform common perception tasks on the input point cloud $P$.
% In this work, we focus on object part segmentation and semantic scene segmentation, which require fine-grained geometric understanding to label each point in the input with a semantic class.
% Curve clouds are also suitable, in principle, for any other point cloud perception task. 
% Please see the supplementary for a classification example. 

% \input{content/main/figures/curve_cloud_conversion.tex}
\paragraph{Curve Clouds}
A curve $c_j = [p_1, ..., p_{N_j}]$ is defined as a sequence of $N_j$ points where consecutive point pairs are connected by a line segment, \ie, a \textit{polyline}. 
The curve is bi-directional and is equivalently defined as $c_j = [p_{N_j}, p_{N_j-1}, ..., p_1]$.
A \textit{curve cloud} $C = \{c_1, ..., c_M\}$ is an unordered set of $M$ curves where each curve may contain a different number of points.
Converting the input point cloud $P$ to a corresponding curve cloud is straightforward and extremely efficient.
Points from each beam $b_i$ are ordered by  timestamp and split into curves based on distances between consecutive points.
If the distance between two consecutive points is more than a set threshold $\delta$ while traversing the points in time order, then the current curve ends and a new curve begins, \ie, a surface discontinuity has occurred.
In practice, we parallelize this process across all points and laser beams on the GPU.
More details regarding the conversion process are provided in the supplement.

% TODO
\noindent \textbf{Why Use Curves?}
Curve clouds inherit the benefits associated with the 3D point cloud representation including lightweight data structures, low memory usage, and no need to discretize the space. 
But operating on curve clouds also has several advantages over point clouds.
Point clouds are highly unstructured, making operations like nearest neighbor queries and convolutions expensive. 
Curve clouds add structure through point ordering along the polylines, allowing curves to be treated as 1D grids that enable constant-time neighborhood queries and efficient convolutions.
This structure is flexible to any laser scanning pattern unlike, \eg, cylindrical voxel grids and polar range-view projections. % that are only suitable for \lidar.
In principle, the curve structure should also bring out the geometric properties of the surface it represents, such as surface curvature, tangents, and boundaries.
% For instance, the endpoints of curves represent boundaries (or edges) of the surface, while the interior points traverse along a continuous surface capturing curvature and tangents.

% First, the curve cloud provides a highly flexible structure that reaps the benefits of both point cloud and grid representations. To explain, a curve cloud strictly generalizes a point cloud; i.e. any point cloud can be represented as a curve cloud with single-point polylines. This means that curve clouds inherit the benefits of point clouds, including no discritization error (i.e. infinite resolution), little preprocessing, no external data-structures, trivial 3D aggregation, and more. On the other hand, a curve-cloud incorporates greater spatial structure than a point cloud. 
% point clouds very unstructured, makes operations like NN queries expensive. 
% we can treat along the curve like a grid, unlike when processing point clouds
% By defining a fixed ordering of points along each 1D curve, we obtain many grid-based benefits \textit{along} curves, such as constant-time neighborhoods and efficient convolutions. 

% Second, because each curve is a continuous traversal along a surface, we argue that curves capture improved semantic understanding and geometric structure. For instance, the endpoints of curves represent boundaries (or edges) of the surface, while the interior points map out a continuous traversal along the surface. \cref{fig:label-proximity} showcases how this additional geometric structure interplays with semantic understanding: on the A2D2 dataset, for randomly sampled points $p_i$ and $p_j$ with distance $d_{ij}$, the probability of having the same semantic categories $s_i = s_j$ is greater if the two points lie on the same curve.

% \input{content/main/figures/label_proximity.tex}



\subsection{Operating on Curves} \label{sec:curve-ops}
We now discuss the fundamental operations for curves.
These operations are inspired by point cloud processing counterparts, so we start by reviewing operations on points.

% We define three neural operations on 1D curves: 1D farthest point sampling, 1D groupings and feature propagation, and symmetric 1D convolutions.

\vspace{-2mm}
\subsubsection{Review of Point Cloud Layers} 
\vspace{-1mm}
\label{subsec:point-review}
Our operations are based on those introduced in PointNet++~\cite{Qi2017NIPS} and DGCNN~\cite{Wang2019SIGGRAPHb}. 
In particular, these point-based methods rely heavily on \textit{sampling}, \textit{grouping}, \textit{feature interpolation}, and \textit{convolution} operations.
We briefly review the point layers relevant to these operations. % and refer the reader to the original papers for more details.

\paragraph{Set Abstraction (SA)} 
Set abstraction is the downsampling layer introduced in \cite{Qi2017NIPS}. 
It proceeds by (1) \textit{sampling} a subset of ``centroid'' points from the point cloud at the current resolution, (2) \textit{grouping} the points around these centroids into local neighborhoods, (3) translating points into the local frame of their centroid and processing all points with a shared MLP, and (4) pooling over each local neighborhood to get a downsampled point cloud with associated features.
\textit{Sampling} uses iterative farthest point sampling (FPS) \cite{Qi2017NIPS}. 
% : at each iteration $k$, a point $p_k$ is added to the sampled subset such that $p_k$ is the farthest (in the Euclidean sense) from all previously sampled points $\{p_1, p_2, ..., p_{k-1}\}$.
\textit{Grouping} commonly uses a ball query, which groups together all points within a specified radius from a centroid.
% Lastly, pooling can be max-pooling, mean-pooling, or an attention pooling~\cite{Hu2020CVPR}.

\paragraph{Feature Propagation (FP)} 
Feature propagation is the upsampling layer introduced in \cite{Qi2017NIPS}.
Its goal is to propagate features from a low-resolution point cloud $\{q_1, ..., q_L\}$ with $L$ points to a higher-resolution point cloud $\{p_1, ..., p_H\}$ with $H$ points where $L{<}H$.
This is achieved with \textit{feature interpolation}: given the low-res point features $\{g_1, ..., g_L\}$, the high-res point features  $F{=}\{f_1, ..., f_H\}$ are determined by a distance-weighted feature interpolation of the $k$ nearest low-res neighbors for each high-res point (based on the spatial coordinates of the points). 
% To get the final upsampled point features, the high-res interpolated features $F$ are concatenated with skip-linked features from a corresponding SA layer and processed by a shared MLP.

\paragraph{Graph Convolution}
DGCNN~\cite{Wang2019SIGGRAPHb} leverages convolution operations on a dynamically constructed graph to process points.
At each layer, the graph is constructed by adding edges between each point and its $k$ nearest neighbors in the learned feature space. 
The convolution centered at a point uses a learned edge function for each edge followed by aggregation. Constructing a dynamic graph in feature space is computationally prohibitive on larger scenes, so we opt to construct the graph based on 3D point distances (\cref{sec:curvecloudnet}). 

\vspace{-2mm}
\subsubsection{Curve Operations}
\vspace{-1mm}
We introduce new operations for \textit{sampling}, \textit{grouping}, \textit{feature interpolation}, and \textit{convolutions} along curves to expressively and efficiently learn on curve clouds.

\paragraph{1D Farthest Point Sampling}
FPS is frequently a bottleneck in point cloud architectures and can be costly for large point clouds~\cite{Hu2020CVPR}. 
Subsampling $L$ points from a point cloud of $N$ points has a time complexity of $O(N^2)$ due to paired distance computations, and requires $O(L)$ sequential steps.
For curves, we alleviate this cost with an approximation of FPS \textit{along each curve} independently in a 1D manner.
This amounts to sampling a subset of points on each curve that are evenly-spaced along the length of that curve (\ie, geodesically).
For a curve $c_j$ with $N_j$ points, we subsample a point set $\{q_1, ..., q_{L_j}\}$ with $L_j{<}N_j$ such that all pairs of contiguous points are about $\epsilon$ apart, where $\epsilon$ is a fixed target spacing shared across \textit{all} curves. 
In other words, $d(q_i, q_{i-1}) \approx \epsilon$ for $i=2,\dots,L_j$ where $d$ measures the geodesic distance between two points along the same curve.
Notably, this algorithm has only $O(N)$ complexity and can be parallelized across each curve independently.
%Additional details regarding our 1D Farthest Point Sampling are provided in the supplementary material. 

% \subsubsection{1D Farthest Point Sampling} \label{subsec:1d-fps}
% Most point-cloud backbones use iterative farthest point sampling (FPS) to obtain a subset of points with good coverage over the occupied space. Iterative FPS begins with a random sampled point, $p_1$, and then iteratively samples $p_i$ as the farthest point from all previously sampled points $p_1, ..., p_{k-1}$. Importantly, to subsample $N'$ points, FPS performs $O(N')$ sequential steps and has a computational complexity $O(N^2)$. This can result in a significant computational burden for large point clouds \cite{randla_net}.

% To reduce computational cost, we apply a 1D FPS \textit{along} each curve independently. 
% In the 1D case, FPS reduces to evenly-spaced geodesic sampling along each curve. In contrast to 3D FPS, our 1D FPS only performs $log(N_j^{max})$ iterations and has $O(N)$ complexity, where $N_j^{max} = \max(N_j), j \in [1, ..., M])$, i.e. the maximum number of points belonging to a single curve. For more details on our 1D FPS algorithm, please refer to the Supplementary material. 

\paragraph{Grouping Along Curves}
After sampling, we must group points into local neighborhoods around the subsampled points on each curve.
To do this, we adapt the previously discussed ball query to operate along each curve.
For a query point $p_i$ belonging to curve $c_j$ (one of the ``centroids'' from sampling), we define the local neighborhood of $p_i$ as $\mathcal{N}_i = \{p_k \in c_j \,|\, d(p_i, p_k) < r\}$ where $r$ is a fixed neighborhood radius.
In addition to being computationally faster than a standard 3D ball query grouping, using 1D curve groupings ensures that all neighborhoods lie on a continuous section of scanned surfaces.

% \subsubsection{Curve Grouping}\label{subsec:1d-grouping}
% We adapt the core grouping operation of PointNet++ \cite{pointnet2} to the 1D polyline. Specifically, we obtain a set of downsampled query points, $\{q_1, ..., q_K\}$, s.t. $K < N$. Then, for each query point $q_k$ that lies on curve $c_j$, we group all points along $c_j$ that lie within the 1D geodesic neighborhood of $q_k$. We define neighborhood as being closer than a fixed geodesic radius $r$. For each query $q_k$, this yields a set of neighboring points $N_k = \{n_1, ..., n_{|N|}\}$. 

% Finally, we center $N_k$ around $q_k$ and extract a neighborhood feature via a PointNet MLP \cite{pointnet} and attention-pooling aggregation \cite{randla}. In addition to being computationally faster than a standard PointNet++ grouping, our 1D groupings ensures that all neighborhoods lie on a continuous section of scanned surfaces.  


\paragraph{Curve Feature Interpolation}
To upsample on curves, we must interpolate features from a lower-resolution polyline to a higher-resolution one.
Let $p_h$ be the h$^{th}$ point on the high-res curve, which falls between subsampled low-res points $q_i$ and $q_{i-1}$ with associated features $g_i$ and $g_{i-1}$.
The interpolated high-res feature $f_h$ is simply the distance-weighted interpolation of the two low-res points.
% : $f_h = (1 - \alpha) \cdot g_i + \alpha \cdot g_{i-1}$ where $\alpha = d(p_h, g_i) / d(g_i, g_{i-1})$ and $d$ measures distances with respect to the \emph{high-res} curve.
\input{content/main/figures/object_samples.tex}
\input{content/main/tables/summer-robotics.tex}

\paragraph{Symmetric Curve Convolution}
To process points along curves, we take advantage of expressive convolutions. 
However, it is computationally burdensome to compute point neighborhoods on the fly and run convolutions on unordered data \cite{Liu2021TPAMI}.
% and operate on dynamic nearest neighbor structures rather than the simple 1D structure of curves.
%overkill given the simple 1D structure of the curves.
% directly on each polyline would be very limited due to each point having only two edges. or expensive to dynamically compute more edges?
Instead, we treat each curve as a discrete grid of features that can be convolved similar to a 1D image. 
To account for the bi-directionality of curves,  we employ a symmetric convolution and thus produce equivalent results when applied ``forward'' or ``backward'' along the curve. 
% This ensures the scanning direction of the laser in the scene has no effect on the convolution result.

In particular, for a curve $c_j = [p_1, p_2, ..., p_{N_j}]$ with associated point features $F_j = [f_1, f_2, ..., f_{N_j}]$, we start by extracting additional features using the the L1 norm of feature gradients along the curve~\cite{Wiersma2022SIGGRAPH}, denoted as $\nabla F_j = \big[ |\nabla f_1|, |\nabla f_2|, ..., |\nabla f_{N_j}| \big ]$. 
Note the norm is necessary to remove directional information.
Concatenating these features together as $[F_j, \nabla F_j] \in \mathbb{R}^{N\times D}$ gives a grid on which to perform 1D convolutions.
To respect bi-directionality, symmetric kernels are used for the convolution: for a kernel $W \in \mathbb{R}^{S \times D}$ with size $S$ and $D$ channels, we ensure $W_i = W_{S-i+1}$ for $i = 1, \dots, S$ where $W_i \in \mathbb{R}^D$.

 % $W_{\text{sym}}*F_{c_j}$

% \input{content/main/figures/object_samples.tex}

% \input{content/main/tables/summer-robotics.tex}
\vspace{-2mm}
\subsubsection{Curve Layers} \label{subsec:curve-layers}
\vspace{-1mm}
We integrate curve operations into curve layers that act as building blocks for the backbone detailed in \cref{sec:curvecloudnet}.

\paragraph{Curve Set Abstraction (Curve SA)}
Curve set abstraction is a curve-centric downsampling procedure. For each curve, Curve SA (1) samples a subset of centroid points using \textit{1D farthest point sampling}, (2) groups points along the curve using \textit{curve grouping}, (3) translates points into the local frame of their centroid and processes them with a shared MLP, and (4) pools over each local neighborhood to get downsampled point features.

\paragraph{Curve Feature Propagation (Curve FP)}
Curve feature propagation is a curve-centric upsampling layer. For a curve $c_j$, its goal is to propagate features from a low-res polyline $[q_1, ..., q_{L_j}]$ with $L_j$ points to a higher-res polyline $[p_1, ..., p_{H_j}]$ with $H_j$ points where $L_j < H_j$. This is achieved using \textit{curve feature interpolation} described above. Afterwards, the high-res interpolated features are concatenated with skip-linked features from a corresponding curve set abstraction layer and processed by a shared MLP.

\paragraph{Curve Convolution}
The curve convolution layer allows for efficient communication and feature extraction along a curve. This module consists of three sequential \textit{symmetric curve convolutions}, each followed by batch normalization~\cite{Ioffe2015bn} and a leaky ReLU activation.


\subsection{Curve Cloud Backbone: \arch} \label{sec:curvecloudnet}
We propose a novel architecture, \arch, that processes curve clouds for several perception tasks. % As illustrated in \cref{fig:curvecloudnet-architecture}, \arch uses a mix of curve layers described and point-based layers.
%\paragraph{\arch Architecture}
In \cref{fig:curvecloudnet-architecture}, we illustrate CurveCloudNet as designed for segmentation tasks where the output is a single semantic class (one-hot vector) for each point in the input point cloud.
Hence, it follows the U-Net~\cite{Ronneberger2015MICCAI} structure, consisting of a series of downsampling layers followed by upsampling with skip connections.
Although in our experiments (\cref{sec:experiments}) we focus on segmentation tasks, \arch can be also adapted to other point cloud perception tasks from laser scanners (see supplement for a classification example).

Our architecture is a mix of curve and point-based layers.
At higher resolutions, curve modules are employed since they are efficient and can capture geometric details when curve sampling is most dense across surfaces in the scene. 
At lower resolutions, point modules are used to propagate information across curves when 1D structure is less apparent. 
This combination makes \arch an expressive network that maintains the benefits of point cloud backbones while injecting structure and efficiency usually only possible with voxel-based approaches.

\paragraph{Training}
We train CurveCloudNet on segmentation tasks with a standard cross-entropy loss. Following previous works, we also supplement the loss with a Lovasz loss \cite{Berman2018CVPR, Zhou2020ARXIV} for nuScenes and Audi Autonomous Driving datasets. At training, we apply random scaling and translation augmentations, as well as random flips on nuScenes and Audi.
%We provide additional training details for each dataset in \cref{sec:experiments}.



% \todo{Implementation details to bring up here? Training? Maybe add features dimensions to the arch figure, etc..}

% \subsubsection{CurveCloudNet Architecture} \label{subsec:curvecloudnet-arch}
% We provide an overview of CurveCloudNet in \cref{fig:curvecloudnet-architecture}. CurveCloudNet follows a U-Net encoder-decoder structure. At higher resolutions, CurveCloudNet employs curve modules: i.e. curve set abstractions, curve feature propagations, and curve convolutions. At downsampled resolutions, CurveCloudNet applies point cloud operations: i.e. set abstractions, feature propagations, and graph convolutions. Taken together, CurveCloudNet is an efficient and expressive network maintains the dynamic and adaptive benefits of a point cloud backbone, while injecting additional ``curve" structure to increase expressiveness and efficiency. Todo: add architecture details --> feature dimensions, etc.