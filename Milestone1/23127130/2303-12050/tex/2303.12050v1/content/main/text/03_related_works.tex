\section{Related Work}

Existing point cloud methods can be roughly characterized as
point-based and voxel-based approaches. As our work addresses trade-offs between them, we discuss related works from each category.

\paragraph{Point-Based Networks}
% \subsection{Point-Based Networks}
% \boldparagraph{Point-based Networks}%
Prior work has extensively studied backbones that map a 3D point cloud to a high-dimensional feature space used for downstream applications, such as 3D reconstruction, shape classification, part segmentation, semantic segmentation, and more \cite{Fan2017CVPR, Qi2017CVPR,
Qi2017NIPS, Wang2019SIGGRAPHb, Thomas2019ICCV}. 
%
PointNet \cite{Qi2017CVPR} was a seminal work that combined a series of MLPs with a max pooling layer to learn point-wise features. Following PointNet, several works proposed to aggregate local neighborhood information using hierarchical grouping at multiple geometric scales \cite{Qi2017NIPS, Li2018CVPR, Qian2022PointNeXtRP}. Recently, Ma \etal \cite{Ma2022ICLR} introduced a compelling MLP-based architecture that combines residual multi-scale reasoning and affine transformations. 
% 
Nevertheless, most hierarchical and MLP point networks are inefficient on large-scale point clouds, and although several backbones \cite{Hu2020CVPR,Zhang2022CVPR,Yang20203DSSDP3} have addressed this, they trade off scalability with task-specific frameworks or lower accuracy. 
In contrast, CurveCloudNet uses an efficient curve cloud representation to achieve
superior performance on both objects and large-scale scenes.

Another line of research introduced \emph{kernel-based
convolutions} for learning per-point local features
\cite{Su2018CVPR, Hua2018CVPR, Wang2018CVPRb, Xu2018ECCV, Esteves2018ECCV,
Wu2019CVPR, Lei2019CVPR, Komarichev2019CVPR, Lan2019CVPR, Thomas2019ICCV, Wiersma2022SIGGRAPH}. Kernels are defined using a family of polynomial
functions~\cite{Xu2018ECCV} or can be estimated using MLPs~\cite{Wang2018CVPRb,
Liu2019CVPR}. Likewise, \cite{Atzmon2018SIGGRAPH, Thomas2019ICCV, Wu2019CVPR, Xu2021CVPR,
Boulch2020ACCV} defined the kernel weights directly using the local 3D point coordinates.
% More sophisticated models such as Spherical CNN~\cite{Esteves2018ECCV} addressed 3D rotation equivariance by implementing convolutions in the spherical harmonic domain.
% InterpConv~\cite{Mao2019ICCV} utilized the point coordinates to interpolates
% point features to the neighboring kernel weights and
% DeltaConv~\cite{Wiersma2022SIGGRAPH} proposed to construct anisotropic convolutions by learning combinations of differential operators.
More related to our work is CurveNet~\cite{Xiang2021ICCV}, which proposed to perform random walks on point clouds and aggregate features along these ``curves" to capture local geometric details. In constrast, our work defines efficient point cloud operations for existing 1D curve structures.

An alternative line of research proposed to construct a
graph from the input point cloud and apply \emph{graph-based convolutions}
\cite{Te2018SIGGRAPH, Liu2019ICCVb, Chen2020CVPRa, Eliasof2020NeurIPS} to capture local geometric structure. For example,
\cite{Shen2018CVPR} introduced a graph-based network that replaced
convolutions with correlations computed between points and their $k$-nearest neighbors. Similarly, \cite{Verma2018CVPR,
Wang2019SIGGRAPHb} proposed to construct a local neighborhood graph and apply convolutional operations on the edges connecting neighboring pairs of points.
% DiffGCN~\cite{Eliasof2020NeurIPS} performed message passing based on approximate surface gradients and an approximate Laplacian matrix, whereas \cite{Wang2018ECCVc} investigated using local spectral graph convolution.
Recent works \cite{Xie2018CVPR, Liu2019AAI, Yang2019CVPR, Zhang2019CVPR, Hehe2021CVPR, Zhao2021ICCV, Ishan2021ICCV} have also explored
applying \emph{attention-based aggregation} using
transformer architectures with self-attention \cite{Vaswani2017NIPS}.  Unlike previous graph-based and attention-based networks, CurveCloudNet reasons over local 1D ``curve" neighborhoods.
% Applying neural networks to process 3D point cloud data is an extremely well
% studied problem.
% Beginning with PointNet \cite{pointnet}, a series of follow-up
% works improved learnable point-cloud operations for capturing both local and
% global structure \cite{pointnet2, dgcnn, kpconv, votenet, pa_conv, fa_conv}.
% PointNet++ \cite{pointnet2} applied PointNet hierarchically to extract features
% at many geometric scales.
% DGCNN \cite{dgcnn} constructs a nearest-neighbors
% graph and performs message passing and aggregation between points. KpConv
% \cite{kpconv} develops a convolutional kernel that is defined on a set of
% deformable 3D key points, and other methods extend this idea of point cloud
% convolutions \cite{pa_conv, fa_conv, 3detr}.
% In recent years, researchers have
% also applied self-attention and transformers to point clouds
% \cite{pointtransformer, pct, 3detr, point4dtransformer}.
% Additionally, some
% recent methods attempt to incorporate intrinsic priors into extrinsic point
% cloud backbones \cite{curvenet, diffgcn, deltaconv}.
% CurveNet \cite{curvenet}
% performs random walks on point clouds and aggregates features along these
% ``curves". DeltaConv defines both per-point scalar and gradient features and
% applies feature updates inspired by discrete exterior calculus
% \cite{deltaconv}.
% DiffGCN performs graph message passing based on approximate
% surface gradients and and approximate Laplacian matrix \cite{diffgcn}.
%We differ from previous point cloud methods in that we are interested in the 1D sampling pattern that arises from many 3D scanners.

\paragraph{Voxel-Based Networks}
% \subsection{Voxel-Based Networks}
% \boldparagraph{Voxel-based Networks}%
Although point-based backbones can successfully process individual objects or
small indoor scenes, they struggle to scale to large point clouds due to inefficiency in processing large unstructured point sets. To address this,
several works \cite{Su2018CVPR, Graham2018CVPR, Choy2019CVPR,
Liu2019NeurIPS, Zhang2020CVPR, Zhou2020ARXIV, Zhang2022ARXIV, Yan2018Sensors, Lang2018CVPR, Liu2021TPAMI, Xu2021ICCV, Hou2022CVPR} proposed to convert a point cloud into a 3D
voxel grid and use this volumetric representation. Early works converted a point cloud into a dense voxel grid and applied dense 3D convolutions \cite{Maturana2015IROS, Qi2016VolumetricAMCVPR}, however the cubic size of the dense grid proved to be computationally prohibitive. PVCNN~\cite{Liu2019NeurIPS}
was a seminal work in combining point operations with low-resolution dense voxel convolutions to efficiently
process smaller-scale point clouds. % Nevertheless, their dense voxel representation still struggled to scale to large scenes.

% sparse voxel architectures
To scale to large scenes, several works \cite{Choy2019CVPR, Lang2018CVPR, Zhou2020ARXIV, Liu2021TPAMI, Hou2022CVPR} employed the sparse-voxel data structure from \cite{Graham2018CVPR}. 
% MinkowskiNet~\cite{Choy2019CVPR} extended sparse voxel convolutions to the time domain, showing efficient and accurate reasoning on 4D scenes. 
PVNAS~\cite{Liu2021TPAMI} incorporated a network architecture search, demonstrating the importance of the architecture channels, network depth, kernel sizes, and training schedule. PVT~\cite{Zhang2022ARXIV} introduced a sparse attention module to efficiently process per-voxel local features using a transformer encoder~\cite{Vaswani2017NIPS}.
% Notably, although our model only processes 3D points, we showcase that it can be efficiently employed on larger point clouds.
%
% better descritization
Other methods seek a better voxel discretization of point clouds captured with \lidar scans. For example, PolarNet~\cite{Zhang2020CVPR}
proposed to partition input points using grid cells defined in a polar
coordinate system, while Cylinder3D~\cite{Zhou2020ARXIV} employed a cylindrical
partitioning scheme based on a cylindrical coordinate system. In an alternative line of research, many methods \cite{Wu2019ICRA, Xu2020ECCV, Wu2018ICRA, Wu2019CVPR}
employed spherical or bird's-eye view projections to represent point clouds as images that are passed to a convolutional neural network. 

Unlike these works, our model directly operates on points and curves, scaling to larger scenes without 2D or 3D convolutions. Furthermore, our model does not rely on the point cloud to exhibit cylindrical or planar properties. 
% individual objects, where the point clouds do not necessarily follow
% cylindrical patterns (see \cref{fig:teaser}).

% Unlike these works, our model directly operates on points and curves, and hence does not
% rely on either 2D or 3D convolutions. Notably, although our model only processes 3D points,
% we showcase that it can be efficiently employed on larger point clouds.

% 
% to take advantage of the 3D topology of point clouds captured with LIDAR scans and
% instead of converting the input to a voxel grid, they rely on on cylindrical
% partitioning schemes. For example, PolarNet~\cite{Zhang2020CVPR}
% proposed to partition input points using grid cells, defined in a polar
% coordinate system, while Cylinder3D~\cite{Zhou2020ARXIV} employed a cylinder
% partitioning scheme based on a cylinder coordinate system. Although, both
% \cite{Zhang2020CVPR, Zhou2020ARXIV} can be successfully applied to point
% clouds captured with LIDAR scans, they perform poorly for the case of
% individual objects, where the point clouds do not necessarily follow
% cylindrical patterns (see \cref{fig:teaser}). In an alternative line of research, \cite{Wu2018ICRA, Wu2019CVPR}
% employ spherical projections to represent point clouds as images that are then
% passed to a convolutional neural network to extract per-point features. Unlike
% these works, our model directly operates on points and curves, and hence does not
% rely on either 2D or 3D convolutions. Notably, although our model only processes 3D points,
% we showcase that it can be efficiently employed on larger point clouds.

% Sparse voxel backbones convert a point cloud into a discrete voxel grid and
% apply a series of sparse 3D convolutions or 2D birds-eye-view convolutions
% \cite{octnet, second, splatnet, mincowskinet}. The sparse-voxel backbone is a
% popular alternative to point cloud backbones due to efficient grid
% representations and the ability to scale to large scenes \cite{randla-net,
% performance-lidar}. MinkowskiNet \cite{mincowskinet} is a seminal work in 3D
% and 4D sparse convolutions, offering a highly efficient and expressive
% implementation of the sparse-voxel infrastructure. Following suit, many works
% extend MinkowskiNet \cite{cylinder3d, other-kitti-one, polarnet, spvcnn}.
% Additionally, newer works fuse spare voxel operations and point operations to
% improve overall expressivity \cite{pvcnn, kitti-iclr1, kitti-iclr1-followup,
% pointvoxeltransformer}.

% \subsection{LiDAR Range View Representation}
% In LiDAR processing, it is not uncommon to use the a spherical projection into a 2D plane, called the range view \cite{rangenet++, rangenet, rpvnet}. The range view offers a compact 2D approximation of the 3D point cloud structure, and allows the use of standard 2D image processing techniques. 

% In classic ``parallel-sweeping" LiDAR, the spherical projection will result in a reformatting similar to our 1D polylines. However, our 1D polylines are far more general as they are not contrained to the parallel direction (necessary for Audi dataset), they operate directly in 3D space (integrate more easily into existing backbones), and the range view additionally uses a ``vertical" relationship that our polylines do not.


%\boldparagraph{Sensor-Specific Networks}%
%Many previous works modify 3D backbones to better accommodate sensor sampling
%patterns. A number of works address the outputs of long-range LiDARs. To
%explain, most long-range LiDAR sensors vertically stack 32 to 128 laser beams
%and rapidly ``sweep" the beams in a circular fashion. Many works offer
%long-range LiDAR improvements by polar and cylindrical geometric priors.
%PolarNet \cite{polarnet} introduces polar coordinates in a sparse-voxel
%representation. Range-view methods \cite{rangenet++, rangenet, rpvnet,
%solomon_pillar} perform a 2D polar projection of LiDAR measurements the
%range-view. Cylinder3D \cite{cylinder3d} performs cylindrical convolutions on a
%sparse-voxel representation. 

% In addition to sensor-specific architectures, many works modify 3D backbones for specific tasks. In mesh correspondence and protein classification, recent works apply 2D convolutions along approximated surfaces to better understand intrinsic geometry \cite{mesh_cnn, diffusionnet, others}. Siren \cite{siren} includes sinusoidal activation functions to better capture high-frequency information, while rotation-equivalent backbones modify the neural operators to preserve rotation information. Finally, in medical imaging \cite{something}, \textcolor{yellow}{something else here}.

%In contrast to previous methods, CurveCloudNet targets a more general 1D sampling pattern that includes but is not limited to the parallel ``sweeping" pattern of long-range LiDAR sensors.
% formulate the point cloud as a set of local 3D polylines. In addition to being more fine-grained, our 3D polylines are not tailored for a specific type of sensor motion (i.e. parallel sweeps). 
