
\section{Experiments} \label{sec:experiments}

% \subsection{Overview and Analysis}
We evaluate CurveCloudNet and a set of competitive baselines on five datasets -- the ShapeNet Part Segmentation dataset~\cite{Chang2015ARXIV}, the KortX Part Segmentation dataset, the Audi Autonomous Driving Dataset (A2D2) \cite{Geyer2020ARXIV}, the nuScenes dataset \cite{Caesar2020CVPR}, and the Semantic KITTI dataset ~\cite{behley2019iccv, Geiger2012CVPR}.
%
Each dataset exhibits a unique structure and training setup (see ~\cref{tab:summary-results-and-datasets}b). Put together, our evaluation consists of indoor, outdoor, object-centric, scene-centric, sparsely scanned, and densely scanned scenes. Furthermore, each of the five datasets display different point sampling patterns, which we roughly characterize as following ``parallel", ``grid", or ``random" laser motions (see ~\cref{fig:teaser}). 
%

CurveCloudNet achieves the best or second best performance on \textit{all} datasets, and on average outperforms all previous methods (see \cref{tab:summary-results-and-datasets}a). Furthermore, every other method substantially underperforms \arch on at least one dataset. Notably, \arch outperforms point-based backbones on object-level tasks and is competitive with or better than voxel-based backbones on larger scenes. In \cref{sec:object-segmentation} we evaluate our model on the task of object-level part segmentation on simulated ShapeNet~\cite{Chang2015ARXIV} objects and on a new dataset collected with the \kortx vision system~\cite{summer-robotics}. 
In \cref{sec:audi}, \cref{sec:nuscenes}, and \cref{sec:kitti}, we evaluate semantic segmentation on larger outdoor scenes using the Audi Autonomous Driving Dataset (A2D2) \cite{Geyer2020ARXIV}, the nuScenes dataset \cite{Caesar2020CVPR}, and the Semantic KITTI dataset~\cite{behley2019iccv, Geiger2012CVPR}. 
\cref{sec:ablations} ablates the key components of \arch. In the supplementary, we report additional qualitative results and an experiment on object classification.

% In \cref{sec:ablations}, we ablate key components of \arch and demonstrate that each of our novel curve operations is essential to accurate and efficient predictions.
%

% In \cref{sec:object-segmentation} we evaluate our model on the task of object-level part segmentation for simulated ShapeNet~\cite{Chang2015ARXIV} objects and on a new dataset collected with the \kortx vision system~\cite{summer-robotics}. 
% In \cref{sec:audi} and \cref{sec:nuscenes}, we evaluate semantic segmentation on larger outdoor scenes using the Audi Autonomous Driving Dataset (A2D2) \cite{Geyer2020ARXIV} and nuScenes dataset \cite{Caesar2020CVPR}, respectively. 
% \cref{sec:ablations} ablates the key components of \arch. 

% However, as the community moves 047
% to dynamic and unregulated settings such as open-world 048
% robotics (e.g. embodied agents), it is essential to have archi- 049
% tectures that consistently perform well in diverse settings
% We evaluate \arch on five unique datasets, with each exhibiting  


% For object part segmentation, we evaluate on the ShapeNet \cite{shapenet} and KortX datasets. For outdoor semantic segmentation, we evaluate on the nuScenes benchmark \cite{nuscenes} and the Audi Autonomous Driving Dataset (A2D2) \cite{a2d2}. In \cref{sec:object-segmentation}, \cref{sec:audi-segmentation}, and \cref{sec:nuscenes-segmentation}, we report evaluations on object part segmentation, A2D2 dataset LiDAR segmentation, and nuScenes LiDAR segmentation, respectively. In \cref{sec:ablations}, we ablation analysis on \arch. % Finally, \cref{sec:mem-analysis} showcases the importance of low GPU memory usage for mobile devices and batch processing. 

% The Audi Driv-
% ing dataset offers unique grid-like scanning patterns, which
% are distinct from NuScenes, and we are the first to eval-
% uate existing segmentation methods on this relatively new
% dataset. Through extensive experimentation, we demon-
% strate that utilizing the curve structures leads to outstanding
% performance in 3D point processing

% We provide an overview of the datasets and experimental methods in \cref{sec:datasets}. In \cref{sec:kortx}, we report network performance on the KortX Part Segmentation. In \cref{sec:nuscenes}, we report network performance on nuScenes LiDAR Segmentation. 

\input{content/main/figures/kortx_qual.tex}
\input{content/main/tables/a2d2.tex}

\subsection{Object Part Segmentation} \label{sec:object-segmentation}

\paragraph{ShapeNet Dataset}
The ShapeNet Part Segmentation Benchmark \cite{Chang2015,Yi2016ToG} contains 16,881 synthetic shape models across 16 different categories and 50 object parts.
%
To evaluate performance on the laser-based scans that we are interested in, we simulate laser capture using the ShapeNet meshes.
%
Using a fixed front-facing sensor pose, we raycast a set of linear laser traversals and then sample points on the mesh along each traversal. We consider three types of synthetic laser traversals - \textit{parallel}, \textit{grid}, and \textit{random} - which are depicted on the left side of \cref{fig:teaser} and are further described in the supplementary. We generate one synthetic scan for each ShapeNet mesh, which yields 12139 training point clouds and 1872 validation point clouds.


% KORTX MATCHING
% To match the \kortx dataset discussed below, we train and evaluate on only six categories: \emph{cap, chair, earphone, knife, mug}, and \emph{rocket}. 
% Following the official splits~\cite{Yi2016ToG}, these categories yield 6,398 training shapes and 460 validation shapes.



% While previous works evaluate on uniformly sampled point clouds, we are interested in the output of a modern laser-based scanner, \ie a point cloud with 1D curve structure. 
% To accomplish this, we simulate laser-based capture on the ShapeNet meshes. 
% We randomly sample a sensor pose facing the mesh and a set of linear laser traversals, then sample points on the mesh along each traversal (see \cref{fig:teaser}, left).
% The left side of \cref{fig:object-samples} illustrates examples of simulated ShapeNet point clouds.
 % For each training shape, we generate five synthetic scans from different sensor poses, yielding a training set of 31,991 point clouds. 
 % Additional details regarding the scanning simulation and an evaluation on all ShapeNet categories is provided in the supplement.

\paragraph{ShapeNet Results}
ShapeNet results are summarized on the left side of \cref{tab:shapenet-and-kortx}. All methods are trained over three random seeds, and we report the mean and standard deviation of the class-averaged mean intersection-over-union (mIOU) over the runs. For fair comparison, all models are trained for 120 epochs using the same hyperparameters, and the best validation mIOU throughout training is reported.

On average, \arch outperforms all baselines and performs disproportionately well for the ``random" laser traversals. In contrast, SphereFormer exhibits the lowest accuracy of all methods, suggesting that its radial window attention is poorly suited for individual objects. CurveNet and PointNext are the runner ups, showing strong performance on segmenting objects when the scans are captured from the front-facing sensor pose.

\paragraph{\kortx Dataset}
\kortx is a perception software system developed by Summer Robotics~\cite{summer-robotics} that generates and operates on 3D curves sampled from a triangulated system of event sensors and laser scanners. \kortx software supports arbitrary continuous scan patterns, and in practice we scan objects with a  randomly shifted Lissajous trajectory per laser beam. 
%, allowing a user to create their own patterns. 
Using \kortx, we scan $7$ real-world objects (cap, chair, earphone, knife, mug-1, mug-2, and rocket) multiple times in different poses, collecting 195 point clouds in total.
%collecting $39$ scans in total for test-time evaluation. An example of a scanned chair is provided in \cref{fig:teaser}, and all scans are shown in the supplementary material. Because the \kortx platform provides a continuous event-based 3D scan output (points are sampled every 5$\mu$s), we define a ``frame" as a batch of 2048 consecutive point measurements, corresponding to roughly a 20Hz frame rate. We select five frames per \kortx scan because each frame differs in its dynamic curve sampling pattern - this results in 195 point clouds.
We will release this dataset upon publication.
We train on scans that are simulated from ShapeNet meshes and evaluate on the \kortx scans as well as the ShapeNet validation split. To best mimic the \kortx data, we simulate \textit{random} laser traversals on each ShapeNet mesh and only train on the six object categories present in the \kortx dataset: \emph{cap, chair, earphone, knife, mug}. We generate five training scans per ShapeNet object, each scanned from a unique \textit{random} sensor pose. This yields a training set of 31,991 point clouds. 

% To match the \kortx dataset discussed below, we train and evaluate on only six categories: \emph{cap, chair, earphone, knife, mug}, and \emph{rocket}. 
% Following the official splits~\cite{Yi2016ToG}, these categories yield 6,398 training shapes and 460 validation shapes.

% While previous works evaluate on uniformly sampled point clouds, we are interested in the output of a modern laser-based scanner, \ie a point cloud with 1D curve structure. 
% To accomplish this, we simulate laser-based capture on the ShapeNet meshes. 
% We randomly sample a sensor pose facing the mesh and a set of linear laser traversals, then sample points on the mesh along each traversal (see \cref{fig:teaser}, left).


% \kortx is an event-based laser scanner developed by Summer Robotics~\cite{summer-robotics} that allows rapid and high-fidelity 3D capture for indoor and outdoor environments. 
% Notably, the \kortx is \textit{user-controllable}, enabling arbitrary scanning patterns. 
% Using \kortx, we scan $7$ real-world objects (\emph{cap, chair, earphone, knife, mug-1, mug-2} and \emph{rocket}) multiple times in different poses, collecting $39$ scans in total. Examples of scanned objects are provided in the right side of \cref{fig:object-samples}.
% The scanner provides a constant stream of 3D points, so we define a ``frame" as 2048 consecutive point measurements, corresponding to roughly a 20Hz frame rate. 
% Because each frame differs in its dynamic scanning pattern, we evaluate on 5 consecutive frames per scan in our \kortx dataset, hence resulting in 195 point clouds in total. We will release this dataset upon publication.
% For a thorough overview of the KortX sensor and the KortX dataset, please refer to the Supplementary.

\paragraph{KortX Results}
% We train and evaluate CurveCloudNet and several baselines on the simulated ShapeNet dataset and the \kortx dataset. 
KortX results are summarized on the right side of \cref{tab:shapenet-and-kortx}. The experimental setup is identical to ShapeNet, except we train over four random seeds and for 60 epochs.
% For fair comparison, we train all models for 60 epochs with a batch size of 24, a learning rate of 0.0001, batch momentum decay of 0.97, and exponential learning rate decay of 0.97. 
% For each run, we record the highest achieved validation mIOU. 
% We report means and standard deviations over four runs.
CurveCloudNet again outperforms all baselines, showing effective generalization to out-of-domain \kortx test scans. Voxel-based methods continue to underperform their point-based counterparts, suggesting that discretizing the input has a negative effect when point clouds are small. In contrast to the previous ShapeNet evaluation, PointMLP is the second-best method when scans are captured from random sensor poses. 
% CurveCloudNet is competitive with other object-level backbones in terms of latency and GPU memory usage. 
% In contrast, all sparse voxel backbones exhibit significantly lower accuracy on this object-level task, suggesting that discretizing the space is poorly suited for partial scans of posed objects. 
\cref{fig:kortx-qual} shows that CurveCloudNet better distinguishes fine-grained structures, such as the back of a chair, a mug handle, and an earphone headpiece.

% \input{content/main/tables/kortx-data.tex}

% \input{content/main/figures/a2d2_qual.tex}

\input{content/main/tables/nuscenes.tex}

\subsection{A2D2 \lidar Segmentation}\label{sec:audi}

\paragraph{A2D2 Dataset}
The Audi Autonomous Driving Dataset (A2D2) \cite{Geyer2020ARXIV} contains 41,280 frames of outdoor driving scenes captured from 5 overlapping \lidar sensors, creating a unique grid-like scanning pattern (see \cref{fig:overview}).
Each frame is annotated from the front-facing camera with a 38-category semantic label. 
% Notably, the five LiDARs create a unique grid-like scanning pattern. 
% To the best of our knowledge, we are the first to evaluate LiDAR segmentation on this relatively new dataset, so we pre-process the data for our setup.
We define a mapping from camera categories to \lidar categories and remove texture-only (\eg sky, lane markers, blurred-area) and very rare categories (\eg animals, tractors, utility vehicles). 
In total, we evaluate on 12 \lidar categories: \emph{car, bicycle, truck, person, road, sidewalk, obstacle, building, nature, pole, sign,} and \emph{traffic signal}. 
Evaluation is performed on annotated LiDAR points in the field of view of the front-facing camera.

\paragraph{Results}
We train CurveCloudNet along with point and voxel-based baselines on the official A2D2 training split~\cite{Geyer2020ARXIV}. 
For fair comparison, all models are trained for 140 epochs using the same hyperparameters, and the best validation mIOU throughout training is reported.
% with a batch size of 7, a learning rate of 0.001, and an exponential learning rate decay of 0.97. For each run, We report the highest achieved validation class-averaged mIOU over all training epochs.
Results are summarized in \cref{tab:a2d2-results}, and \cref{fig:teaser} provides a qualitative example. 
CurveCloudNet scales to outdoor scenes better than point-based backbones, with the runner-up PointMLP showing a 6\% drop in mIOU. CurveCloudNet also outperforms most voxel-based backbones and achieves similar accuracy to state-of-the-art SphereFormer~\cite{lai2023spherical}, even though SphereFormer's radial window attention is tailored for outdoor LiDAR scans.
% despite SphereFormer's radial window attention being tailored for large outdoor LiDAR scans. 
%
% Please see the supplement for additional analysis of latency.
% When analyzing category breakdown, we notice that CurveCloudNet performs disproportionately well on off-road objects (\eg signs, poles, nature, and buildings), suggesting that CurveCloudNet can better handle the non-uniform structure of LiDAR measurements that arise outside of the road plane. 
% The strong performance on the grid-like sampling pattern of A2D2 suggests that CurveCloudNet is well-suited for more general LiDAR data that does not follow a uniform sweeping pattern. 

% \cref{fig:a2d2_qual} qualitatively demonstrates the potential advantage of CurveCloudNet over Cylinder3D. 
%shows that, compared to  Cylinder3D, CurveCloudNet can inform larger-object prediction with fine-grained details, \eg identifying the curb of the sidewalk (top), cohesively segmenting all parts of a truck (middle), and distinguishing a car from a truck (bottom).


\subsection{nuScenes LiDAR Segmentation} \label{sec:nuscenes}
\paragraph{nuScenes Dataset}
The nuScenes dataset \cite{Caesar2020CVPR} contains 1000 sequences of driving data, each 20 seconds long. 
Each sequence contains 32-beam \lidar data with segmentations annotated at 2Hz. %, yielding 40,000 point clouds. 
We follow the official nuScenes benchmark protocol with 16 semantic categories.

\paragraph{Results}
We train CurveCloudNet and baselines on the official nuScenes training split. 
To ensure fair comparison, we train all models for 100 epochs. %with a batch size of 2, a learning rate of 0.001, and an exponential learning rate decay of 0.96. 
% We report class-averaged mIOU on the validation split.
Results on the nuScenes validation split are shown in \cref{tab:nuscenes-val-results}, and \cref{fig:teaser} includes a qualitative example. 
CurveCloudNet significantly improves upon other point-based networks: PointMLP and PointNext show more than a 10\% drop in mIOU and $\sim 2\times$ increase in latency. We also note that CurveNet exceeds 48GB of GPU memory for a batch size of 1, showcasing its inability to scale to larger scenes.
%
CurveCloudNet also outperforms all voxel-based methods except the recent SphereFormer.
% Notably, CurveCloudNet can achieve this comparable accuracy and performance without discretizing the space nor using attention operations.
%

% Furthermore, CurveCloudNet bridges the gap between point cloud backbones and sparse voxel backbones these large scenes, achieving comparable accuracy to the popular Cylinder3D while using half the GPU memory. 
% As indicated by the per-category breakdown, CurveCloudNet appears to segmenting small such as bicycles, motorcycles, and pedestrians.
% Overall, CurveCloudNet bridges the gap between point and voxel-based methods: it scales point-based processing to large outdoor scenes, achieving competitive accuracy to voxel-based methods. Furthermore, CurveCloudNet's small memory footprint makes it suitable for on-board AV applications and mobile devices.

% In addition to  benefiting on-board AV applications, reducing GPU memory is valuable for efficient deployment of 3D perception on mobile devices; for example, on a mobile 940MX GPU, CurveCloudNet has equal latency to Cylinder3D (see supplement for additional analysis).
% CurveCloudNet offers a high-accuracy alternative to voxel backbones when low GPU memory usage is important, \eg on mobile devices, when running many networks concurrently, and when using large batches. 
% In fact, on a mobile 940MX GPU, CurveCloudNet has equal latency to Cylinder3D (see supplement for additional latency analysis).

\subsection{KITTI LiDAR Segmentation}\label{sec:kitti}
The Semantic KITTI dataset is made up of 22 sequences of driving data consisting of 23,201 LiDAR scans for training and 20,351 for testing. Each scan is obtained with a dense 64-beam Velodyne LiDAR. We follow the official KITTI protocol in training and validation. To ensure fair comparison, we train all models for 100 epochs. Results on the validation sequence are reported in \cref{tab:kitti-results}, and \cref{fig:teaser} shows a qualitative example. \arch outperforms all point-based and voxel-based methods. Note that we cannot report results for many point-based methods due to excessive training times on the larger KITTI scans ($>20$ days).

\input{content/main/tables/kitti}


\subsection{Ablation Study}\label{sec:ablations}
\input{content/main/tables/curvecloudnet_ablation.tex}
\cref{tab:a2d2-ablation} shows an ablation analysis of CurveCloudNet on the A2D2 dataset; the table shows that each of our proposed curve operations is essential to achieve high accuracy and efficiency.
We ablate grouping along curves by instead using the regular radial groupings from PointNet++~\cite{Qi2017NIPS}. This ignores the curve structure and results in decreased accuracy, increased latency, and a significant increase in GPU memory usage. Instead of curve farthest point sampling, we also try regular FPS, which causes a decreased accuracy and increased latency.
Finally, without 1D curve convolutions, we observe a notable decline in accuracy with a marginal improvement in latency and GPU memory. Taken together, our curve operations increase accuracy with roughly half the latency and one third the GPU memory requirements.
% \subsection{GPU Memory Analysis}\label{sec:mem-analysis}
% \input{content/main/figures/batch-size.tex}
% \input{content/main/figures/gpu-type.tex}
% We illustrate the importance of low GPU memory in \cref{fig:batch-size} and \cref{fig:gpu-type}.


