\section{Additional Experiments} \label{additional-experiments}

% =================================================
% ============ Shapenet Classification ============
% =================================================
\input{content/supp/tables/translated-audi}

\subsection{Translated A2D2}
\paragraph{Overview}
In Sec. 4.2 of the main paper, we reported that SphereFormer outperforms \arch by $+1.0\%$ mIOU on the A2D2 dataset. In this section, we show that, on the same data, SphereFormer underperforms \arch when the scene does not exhibit consistent and aligned global structure. 

We apply a simple translation augmentation to the A2D2 training and validation data -- for each scan, we offset all points by a translation sampled from a uniform Gaussian with $\mu=0$ and $\sigma=20$. Note that this removes the \textit{global} alignment of point clouds, but completely preserves all \textit{local} structure. In the real world, this setup could occur in topography or mapping, \ie when a large region is scanned but only one area is of interest (which could be anywhere in the scan). We train and evaluate all models with an identical setup to the original A2D2 experiment.

\paragraph{Results}
We summarize results on the translated A2D2 experiment in \cref{tab:translated-a2d2-results}. Without a consistent global alignment of the scene layout, all methods perform worse. However, \arch is less effected and outperforms SphereFormer by over 4$\%$. This further suggests that SphereFormer's radial window is tailored for outdoor driving scenes and cannot be flexibly applied to environments with weaker global structure. In contrast, \arch can successfully leverage local structures to reason in more diverse environments. 

% =================================================
% ============ Shapenet Classification ============
% =================================================
\input{content/supp/tables/shapenet-classification}

\subsection{ShapeNet Classification}\label{subsec:shapenet-class}
In addition to semantic segmentation tasks, we also evaluate CurveCloudNet's performance in shape classification.

\paragraph{ShapeNet Classification Dataset}
We use the ShapeNet Part Segmentation Benchmark \cite{Chang2015,Yi2016ToG} as described in Sec 4.1 of the main paper. In the classification setting, the network is tasked with classifying a point cloud into one of the 16 object categories. Using our ShapeNet laser-scanner simulator (see \cref{subsec:shapenet-sim}), we generate a single synthetic ``scan" for each ShapeNet mesh from a \emph{fixed} sensor viewpoint, resulting in scanned objects sharing a canonical orientation. Following the official training and validation splits \cite{Yi2016ToG}, this yields 12139 training point clouds and 1872 validation point clouds.  For this experiment, we consider the \textit{random} curve sampling pattern (see \cref{subsec:shapenet-sim}).

\paragraph{Setup}
We train CurveCloudNet and several baselines on the simulated ShapeNet training set. Similar to the settings used for the segmentation task, all models are trained for 120 epochs with the Adam optimizer, a batch size of 24, a learning rate of $3e^{-4}$, batch momentum decay of 0.97, and exponential learning rate decay of 0.97. We record the best validation class-averaged accuracy, instance-averaged accuracy, and class-averaged F1 score that is achieved during training. We report means and standard deviations across 3 runs.

\paragraph{Results}
Results are summarized in \cref{tab:shapenet-classification}. CurveCloudNet outperforms the baselines on all three metrics. Additionally, CurveCloudNet exhibits improved latency and lower GPU memory compared to PointNet++, DGCNN, and PointMLP. 
% For this experiment, we omit Cylinder3D from our evaluation because re-purposing its sparse voxel architecture from segmentation to classification is not trivial.


% =================================================
% ================= GPU Memory Exp. ===============
% =================================================
% \input{content/supp/figures/gpu_mem_vs_latency}

% \subsection{GPU Memory and Latency}
% We reported both latency \textit{and} GPU memory in all experiments of the main paper. In this section, we briefly justify why we consider GPU memory to be an important part of performance. First, lower GPU memory is essential when processing units must be affordable, lightweight, and capable of running many concurrent perception networks. Additionally, low-memory perception reduces \textit{latency} and increases \textit{throughput} in memory-constrained environments.
% For this analysis, we consider the A2D2 LiDAR segmentation task (Sec. 4.2 in the main paper) and show how GPU hardware affects model latency (left side of \cref{fig:gpu-mem-latency}) and how the batch size affects throughput (right side of \cref{fig:gpu-mem-latency}).

% In Tab. 2 of the main paper, we note that CurveCloudNet, Cylinder3D~\cite{Zhou2020ARXIV}, and PointMLP~\cite{Ma2022ICLR} have comparable latency on a desktop with a 24GB Nvidia RTX 3090 GPU, with CurveCloudNet being marginally slower.
% However, as GPU hardware becomes memory constrained and GPU caches become smaller, the relative latencies change.
% This change is apparent in the left side of \cref{fig:gpu-mem-latency}. 
% For example, for a smaller 4GB Nvidia RTX 3050 Ti GPU, we observe that all models have comparable latencies, while Cylinder3D is now the slowest. On a mobile 2GB Nvidia 940 MX GPU, CurveCloudNet is significantly faster than both Cylinder3D and PointMLP. This trend is also observed on the nuScenes dataset: on a 4GB RTX 3050 Ti GPU, CurveCloudNet and Cylinder3D have latencies of 432ms and 417ms respectively (as opposed to 146ms and 80ms reported in Tab. 3 of the main paper).

% On the right side of \cref{fig:gpu-mem-latency}, we illustrate how batch size affects throughput for our model and baselines. We measure throughput as the number of samples that a model can process per second. 
% For this experiment, we consider two types of GPU configurations: an Nvidia RTX 3050 Ti with 4GB and an Nvidia RTX 3090 with 24GB.
% On both GPUs, we observe that CurveCloudNet can process a batch $\sim$$4\times$ larger than the other networks. Additionally, we observe that CurveCloudNet achieves almost $2\times$ the throughput of the other backbones. Specifically, while Cylinder3D and PointMLP maximize throughput at a batch size of $\sim$$3$ on the RTX 3050 Ti GPU and $\sim$$8$ on the RTX 3090, CurveCloudNet maximizes throughput at batch sizes of $8$ and $32$.


\subsection{Additional nuScenes Results}
\input{content/supp/tables/nuscenes-test}

We provide qualitative results on the nuScenes validation split in \cref{fig:nuscenes-qual}. For the corresponding evaluation on the validation split, see Sec. 4.3 of the main paper.

\subsubsection{nuScenes Test Split}
We evaluate our model on the test split of the nuScenes LiDAR segmentation task, and compare to top-performing baselines from the academic literature.
% We evaluate our model and our top-performing baselines on the test split of the nuScenes LiDAR segementation task. 
We summarize our results in \cref{tab:nuscenes-test-results}. \arch outperforms many sparse-voxel methods in both class-averaged and frequency-weighted mIOU, such as Cylinder3D~\cite{Zhou2020ARXIV}, SPVNAS~\cite{Liu2021TPAMI}, and AF$^2$-S3Net~\cite{Cheng2021AF2S3NetAF}.

% Notably, we observe that \arch significantly outperforms other methods on the semantic categories ``other flat", ``sidewalk", and ``terrain"; we speculate this is due to \arch's ability to better understand subtle changes along curves, such as planar traversals for ``other flat", orthogonal changes in direction for ``sidewalk", and high-frequency patterns for ``terrain".

% As the first and only point-based backbone evaluated on the official nuScenes test split, we believe that future progress and backbone optimization can yield even better performance compared to mature sparse-voxel methods. 


\subsection{Additional KortX Results}
We provide additional qualitative results on the KortX dataset in \cref{fig:kortx-qual-fullpage}. We observe that \arch distinguishes finegrained structures, such as the legs and back of the chair, the handle of the mug, the boundary where the nose of the rocket begins, and the brim of the cap.

\subsection{Additional A2D2 Results}
We provide additional qualitative results on the A2D2 dataset in \cref{fig:audi-qual-fullpage}. In many examples, \arch distinguishes the sidewalk and the road much better than Cylinder3D. 
Furthermore, in contrast to \arch, examples show that Cylinder3D can fail to detect pedestrians, swaps the ``truck" and ``car" categories, and swaps the ``building" and ``sign" categories.

% \input{content/supp/tables/kitti-test}

\newpage

%We report top-performing methods on the nuScenes test split in \cref{tab:nuscenes-test-results} (all results are copied from the nuScenes Leaderboard). In contrast to the validation metrics in Tab. 3 of the main paper, the nuScenes Leaderboard primarily contains unpublished methods that have built off of and fine-tuned methods such as Cylinder3D~\cite{Zhou2020ARXIV} and PVNAS~\cite{Liu2021TPAMI}, specifically crediting techniques such as architecture fine-tuning, additional loss functions, larger models, multi-task learning, longer and more adaptive training schedules, and more. \arch is in the top 10 of LiDAR segmentation methods on the leaderboard that do not use additional modalities. Furthermore, we are the \emph{only} method on the leaderboard that is point-based. Interestingly, we observe that \arch significantly outperforms leaderboard submissions on the semantic categories ``other flat", ``sidewalk", and ``terrain", suggesting that \arch can better learn high-frequency information along curves that generalizes to the test split. As the first curve-based backbone and only point-based backbone on the leaderboard, we believe that future progress in curve reasoning will continue to close the gap with the sparse-voxel methods. 


\input{content/supp/figures/nuscenes_qual}

\input{content/supp/figures/kortx_qual_fullpage}

\input{content/supp/figures/audi_qual_fullpage}