
\section{Additional Implementation Details} \label{additional-implementation}

\subsection{Baselines}

\paragraph{PointNet++ and DGCNN}
We train and evaluate PointNet++~\cite{Qi2017NIPS} and DGCNN~\cite{Wang2019CVPR} using the reproduced implemenations from Pytorch Geometric~\cite{FeyLenssen2019}\footnote{\href{https://github.com/pyg-team/pytorch_geometric/tree/master/examples}{https://github.com/pyg-team/pytorch\_geometric/}}. For PointNet++, we run hyperparameter sweeps to tune the radius and downsampling ratio on each dataset. For DGCNN, we use the authors' reported hyperparameters. 

\paragraph{RandLANet}
We train and evaluate RandLANet~\cite{Hu2020CVPR} using the reproduced implementation from Open3D-ML~\cite{Zhou2018open3d}\footnote{\href{https://github.com/isl-org/Open3D-ML}{https://github.com/isl-org/Open3D-ML}}. We additionally improve the latency by incorporating GPU-implementations for point grouping and sampling from PyTorch3D~\cite{Ravi2020pytorch3d}\footnote{\href{https://github.com/facebookresearch/pytorch3d}{https://github.com/facebookresearch/pytorch3d}}. We use the authors' reported hyperparameters.

\paragraph{CurveNet}
We train and evaluate CurveNet~\cite{Xiang2021ICCV} using the authors' official implementation\footnote{\href{https://github.com/tiangexiang/CurveNet}{https://github.com/tiangexiang/CurveNet}}. We use the authors' reported hyperparameters for all datasets.

\paragraph{PointMLP}
We train and evaluate PointMLP~\cite{Ma2022ICLR} using the authors' official implementation\footnote{\href{https://github.com/ma-xu/pointMLP-pytorch}{https://github.com/ma-xu/pointMLP-pytorch}}. We use the reported hyperparameters for all datasets.

\paragraph{PointNext}
We train and evaluate PointNext~\cite{Qian2022PointNeXtRP} using the authors' official implementation\footnote{\href{https://github.com/guochengqian/PointNeXt}{https://github.com/guochengqian/PointNeXt}}. As outlined by the authors, we use PointNext-Small for the ShapeNet and KortX datasets. On the A2D2, nuScenes, and KITTI datasets, we use the larger PointNext-XL. Because the authors indicate the importance of the network ``radius", we additionally performed a hyperparameter sweep to find the best radius of 0.05 for the A2D2, nuScenes, and KITTI datasets.  

\paragraph{MinkowskiNet}
We train and evaluate MinkowskiNet~\cite{Choy2019CVPR} using the authors' official implementation\footnote{\href{https://github.com/NVIDIA/MinkowskiEngine}{https://github.com/NVIDIA/MinkowskiEngine}}. We use the larger MinkUNet-34A for all experiments. We use an initial voxel size of $0.05$ on outdoor datasets and $0.015$ on object-level datasets. 

% 
\paragraph{Cylinder3D}
We train and evaluate Cylinder3D~\cite{Zhou2020ARXIV} using the authors' official implementation\footnote{\href{https://github.com/xinge008/Cylinder3D}{https://github.com/xinge008/Cylinder3D}}. We use the reported hyperparameters on the nuScenes and KITTI datasets. On the A2D2 dataset, we set the cylindrical voxel grid to cover a $\pm 31 ^{\circ}$ forward-facing azimuth with a maximum radius of 80 meters and a height covering $[-5, 20]$ meters; we define the initial grid to have 360 radial partitions, 120 angular partitions, and 120 height partitions. On the ShapeNet and KortX datasets, we set the voxel grid to cover all $360^{\circ}$ with a radius of $1.0$ and height of $1.0$; to address latency and memory constraints, we define the initial grid to have 96 radial partitions, 96 angular partitions, and 96 height partitions.

\paragraph{SphereFormer}
We train and evaluate SphereFormer~\cite{lai2023spherical} using the authors' official implementation\footnote{\href{https://github.com/dvlab-research/SphereFormer}{https://github.com/dvlab-research/SphereFormer}}. We use the reported hyperparameters for the nuScenes and KITTI datasets, and we use the reported nuScenes hyperparameters for the A2D2 dataset. For the KortX and ShapeNet datasets, we also use the reported hyperparameters, and we reduce the voxel size from $0.1$ to $0.015$ to account for the dataset's smaller 3D scale. On the KortX and ShapeNet datasets, we additionally ran a sweep on different voxel sizes and spherical window sizes, but observed limited differences.

\subsection{Training Strategy}
We train \arch and baselines on segmentation tasks with a standard cross-entropy loss. Following previous works, we also supplement the loss with a Lovasz loss \cite{Berman2018CVPR, Zhou2020ARXIV} for the nuScenes, A2D2, and KITTI datasets. At training, we apply random scaling and translation augmentations, as well as random flips on the nuScenes, A2D2, and KITTI datasets. Importantly, we use an \textbf{\textit{identical}} training strategy for \arch and each baseline. We experimentally observe convergence in all models' validation accuracies by the end of training.

% \paragraph{Baseline Implementations}
% For PVCNN~\cite{Liu2019NeurIPS}\footnote{\href{https://github.com/mit-han-lab/pvcnn}{https://github.com/mit-han-lab/pvcnn}}, PointMLP~\cite{Ma2022ICLR}\footnote{\href{https://github.com/ma-xu/pointMLP-pytorch}{https://github.com/ma-xu/pointMLP-pytorch}}, and Cylinder3D~\cite{Zhou2020ARXIV}\footnote{\href{https://github.com/xinge008/Cylinder3D}{https://github.com/xinge008/Cylinder3D}}, we use the official GitHub implementation of each paper.
% \todo{provide update for each baseline}

\paragraph{Object Part Segmentation}
We train CurveCloudNet and all baselines for 60 epochs in the KortX experiment and 120 epochs in the ShapeNet experiment with the Adam optimizer \cite{Kingma2015ICLR}, a learning rate of $1e^{-4}$, batch momentum decay of $0.97$, and exponential learning rate decay of $0.97$. For
all models, except for Cylinder3D, we use a batch size of 24. For Cylinder3D, we use a batch size of 12 because 24 exceeds our GPU memory capacity. 
% Additionally, for Cylinder3D we follow \cite{iek20163DUL} and choose a voxel-grid size of $96\times96\times96$. We do not use the original size of $180 \times 180 \times 180$ because it results in excessive memory usage at training (a batch size of 12 requires $\sim$70GB of GPU memory). We experimentally observe convergence in all models' validation accuracy after $60$ epochs.

\paragraph{A2D2 LiDAR Segmentation}
We train CurveCloudNet and all baselines for 140 epochs with the Adam optimizer, a batch size of 7, a learning rate of $1e^{-3}$, and an exponential learning rate decay of $0.97$. 
% We experimentally observe convergence in all models' validation accuracy after $140$ epochs.

\paragraph{nuScenes and KITTI LiDAR Segmentation}
We train CurveCloudNet and all baselines for 100 epochs with the Adam optimizer, a batch size of 4 on nuScenes and 2 on KITTI, a learning rate of $1e^{-3}$, and an exponential learning rate decay of $0.97$. At test time, we follow previous works \cite{Zhou2020ARXIV,Liu2021TPAMI} and average model predictions over axis-flipping and scaling augmentations.
% average model predictions over $x$-axis, $y$-axis, and $xy$-axis flip augmentations.

% \paragraph{nuScenes LiDAR Segmentation}
% \todo{convert into KITTI lidarseg!}
% We train CurveCloudNet and all baselines for 100 epochs with the Adam optimizer, a batch size of 2, a learning rate of $1e^{-3}$, and an exponential learning rate decay of $0.97$. At test time, we follow previous works \cite{Zhou2020ARXIV,Liu2021TPAMI} and average model predictions over axis-flipping and scaling augmentations.
