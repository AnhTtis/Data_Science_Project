\section{Additional Dataset Details} \label{additional-dataset}

\subsection{Kortx Perception System and Dataset}
\paragraph{Kortx Perception System}
\kortx is a perception software system developed by Summer Robotics~\cite{summer-robotics}.
It is an active-light, multi-view stereoscopic system using one or more scanning lasers. It can be configured to use two or more event-based vision sensors to build up arbitrary capture volumes.
Event-based vision sensors are used to detect the scanning laser reflection from target surfaces. Event-based sensors are well suited to this setup as their readout electronics are event triggered instead of time triggered.
Furthermore, the Kortx System supports arbitrary continuous scan patterns, allowing a user to create their own patterns and use their own scan hardware.
For more information, please visit the \href{https://www.summerrobotics.ai/}{Summer Robotics Website}.

\input{content/supp/figures/kortx-dataset-qual}
\input{content/supp/tables/kortx-data}

\paragraph{Kortx Dataset}
Using Kortx, we scanned $7$ real-world objects: \emph{cap, chair, earphone, knife, mug-1, mug-2} and \emph{rocket} (see \cref{fig:kortx-objects-qual}). Each object was scanned multiple times in different poses, resulting in $39$ total scans (summarized in \cref{tab:kortx-dataset}).
Because the Kortx platform provides a continuous event-based 3D scan output (points are sampled every 5µs), we defined a “frame” as a batch of 2048 consecutive point measurements, corresponding to roughly a 20Hz frame rate. Because each frame differs in its dynamic scanning pattern, we evaluate on 5 consecutive frames per scan in our Kortx dataset, hence resulting in 195 point clouds in total.
We manually labeled scanned points with the semantic part categories defined in the ShapeNet Part Segmentation Benchmark \cite{Chang2015}.  Each Kortx scan is mean-centered, however it is \textit{not} aligned into a canonical pose, resulting in an object's orientation depending on the sensor's reference frame. 
% Because each 2048 point ``frame" exhibits a different dynamic scanning pattern, we select 5 frames from each scan for our Kortx dataset, resulting in 195 point clouds.

\subsection{ShapeNet Simulator} \label{subsec:shapenet-sim}
\input{content/supp/figures/sampling_types}

We simulate laser-based 3D capture on the ShapeNet Dataset \cite{Chang2015}. For each mesh, we randomly sample a sensor pose on the unit sphere and render the mesh's depth values into a $2048\times2048$ image. Next, we sample 2D lines on the depth image that correspond to a laser's traversal. For the \textit{random} sampling pattern used in the Kortx evaluation (see Sec 4.1 of the main paper) and the ShapeNet Classification evaluation (see \cref{subsec:shapenet-class}), we select random linear traversals in the image plane, with each traversal parameterized by a pixel coordinate $(i, j)$ and direction $\theta \in [0, \pi)$. For the \textit{grid} and \textit{parallel} sampling patterns used in our ShapeNet Segmentation evaluation (see Sec 4.1 of the main paper), we sample evenly-spaced vertical and horizontal lines. To reduce descritization artifacts introduced from the rasterization, we query every 6th pixel along each line for the \kortx segmentation task and every 4th pixel for the ShapeNet segmentation and classification tasks. We repeatedly generate synthetic laser traversals until we have sampled 2048 points from the mesh. Fig. 1 of the main paper shows an example of the \textit{random}, \textit{grid}, and \textit{parallel} sampling patterns used in our ShapeNet Segmentation evaluation. \cref{fig:shapenet-sampling-patterns} provides an additional qualitative illustration of the three sampling patterns, but showing 4096 points per scan for greater visual clarity.

\subsection{A2D2 LiDAR Segmentation}
The Audi Autonomous Driving Dataset (A2D2) \cite{Geyer2020ARXIV} contains 41,280 frames of labeled outdoor driving
scenes captured in three cities. The vehicle is equipped with five LiDAR sensors, each mounted on a different part of the vehicle and with a different orientation, resulting in a unique grid-like scanning pattern. The A2D2 data was captured in urban, highway, and rural environments as well as in different weather conditions. At the time of writing, the A2D2 dataset only contains semantic labels for the front-facing camera. Thus, we evaluate on LiDAR observations within the front-facing camera's field of view, and we map camera categories to LiDAR categories. We will release the code detailing the exact mapping.

\subsection{Discussion on 3D Datasets}
As \lidar and other 3D scanning technologies continue to develop, they are being applied to new and diverse applications, including open-world robotics (\ie embedded agents), city planning, agriculture, mining, and more. Additionally, there is an increasing variety of sensors and sensor configurations, spanning hardware that scans at different point densities, different ranges, and with unique (or controllable) scanning patterns. The A2D2 and Kortx datasets are two recent examples of such a trend. We believe an important future direction will be to develop a 3D backbone that is performant in \textit{all} these settings. Furthermore, we believe it is important to understand \textit{which} settings ``break" previous assumptions such as the range-view projection, the birds-eye-view projection, and spherical attention. While \arch is a first step towards this goal, we believe it will be important to capture and compile new 3D datasets, and to evaluate on a greater diversity of environments. 
% Additionally, because systems such as \kortx allow in-the-loop feedback to dynamically focus on regions of interest, we believe a valuable future direction will be to design 3D architectures that can support multi-resolution in-the-loop feedback during data capture.


% While \arch is largely agnostic to the scanning pattern, we believe an important future direction will be to explicitly understand how new scanning patterns might ``break" previous assumptions in 3D data processing, such as the polar range-view projection, the birds-eye-view projection, and the cylindrical convolution. Additionally, because systems such as \kortx allow in-the-loop feedback to dynamically focus on regions of interest, we believe a valuable future direction will be to design 3D architectures that can support multi-resolution in-the-loop feedback during data capture.

% \todo{I should tie this into our NEW story!}
% As LiDAR and other 3D scanning technologies develop, there exists a clear trend towards scanning more points, performing higher-quality 3D measurements, and generating more complex and controllable scanning patterns \cite{summer-robotics, wang2022microsnanoeng, moussy2022website}. The A2D2 and Kortx datasets are two recent examples of such a trend, exhibiting grid scanning patterns and arbitrary scanning patterns. While \arch is largely agnostic to the scanning pattern, we believe an important future direction will be to explicitly understand how new scanning patterns might ``break" previous assumptions in 3D data processing, such as the polar range-view projection, the birds-eye-view projection, and the cylindrical convolution. Additionally, because systems such as \kortx allow in-the-loop feedback to dynamically focus on regions of interest, we believe a valuable future direction will be to design 3D architectures that can support multi-resolution in-the-loop feedback during data capture.


