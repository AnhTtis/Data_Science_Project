
\section{Additional Method Details} \label{additional-method}

\subsection{Constructing Curve Clouds}
\paragraph{Curve Cloud Conversion}
\input{content/supp/figures/curve_cloud_conversion}

\paragraph{Constructing Curve Cloud}
We refer the reader to Sec. 3.1 of the main paper for an overview of constructing curve clouds.
As input, we assume that a laser-based 3D sensor outputs a point cloud $P = \{p_1, ..., p_N\}$ where $p_i = [x_i, y_i, z_i] \in \mathbb{R}^3$, an acquisition timestamp $t_i \in \mathbb{R}$ for each point, and an integer laser-beam ID $b_i \in [1, B]$ for each point. We wish to convert the input into a curve cloud $C = \{c_1, ..., c_M\}$, where a curve $c = [p_{i}, ..., p_{i+K}]$ is defined as a sequence of $K$ points where consecutive point pairs are connected by a line segment, \ie, a \textit{polyline}. As outlined in \cref{fig:curve-cloud-conversion}, we first group points by their laser beam ID and sort points based on their acquisition timesteps, resulting in an ordered sequence of points that reflects a single beam's traversal through the scene. Next, for each sequence, we compute the distances between pairs of consecutive points (denoted as polyline ``edge lengths"). Finally, we split the sequence whenever an edge length is greater than a threshold $\delta$, resulting in many variable-length polyline ``curves".  In practice, we parallelize the conversion across all points, and on the large-scale nuScenes dataset, the algorithm runs at 1500Hz.

We select a threshold $\delta$ that reflects the sensor specifications and scanning environment. In particular, the threshold is conservatively set to approximately $10\times$ the \textit{median distance} between consecutively scanned points one meter away from the sensor. On the A2D2 dataset~\cite{Geyer2020ARXIV}, we set $\delta = [0.1, 0.17, 0.1, 0.12, 0.1]$ for the five LiDARs, and on the nuScenes dataset~\cite{Caesar2020CVPR} and KITTI dataset~\cite{behley2019iccv, Geiger2012CVPR} we set $\delta = 0.08$. Additionally, on the A2D2, nuScenes, and KITTI datasets, we scale $\delta$ proportional to the square root of the distance from the sensor, since point samples becomes sparser at greater distance. On the object-level ShapeNet dataset~\cite{Chang2015}, we set $\delta = 0.01$. Experimentally, we observed that \arch is flexible across different $\delta$ values.

\paragraph{Kortx Curve Representation}
The Kortx vision system directly generates and operates on 3D curves sampled from a triangulated system of event-based sensors and laser scanners.
As the detected laser reflection traverses the scene, it produces a frameless 4D data stream that enables low latency, low processing requirements, and high angular resolution. 3D curves are an intrinsic component of the Kortx perception system, and the system directly outputs a curve cloud without the need for additional data processing. 

\subsection{Additional Details on Curve Operations}
\paragraph{1D Farthest Point Sampling}
We refer the reader to Sec. 3.2.2 of the main paper for an overview of our 1D farthest point sampling (FPS) algorithm. The goal of this algorithm is to \textit{efficiently} sample a subset of points on each curve such that consecutive points will be approximately $\epsilon$ apart along the downsampled curve. Concretely, for a curve $c$ with $K$ points, $c = [p_{i}, ..., p_{i+K}]$, we first compute the $K{-}1$ ``edge lengths", $e = [d_{i}, ..., d_{i+K-1}]$, where $d_{i}$ is the distance between consecutive points $(p_{i}, p_{i+1})$. Next, we estimate the geodesic distance along the curve via a cumulative sum operation on the edge lengths: $g = \texttt{CUMSUM}(e)$. Then, we divide the geodesic distances by our desired spacing, $\epsilon$, and take the floor, resulting in $\epsilon$-spaced intervals $I = \texttt{FLOOR}(g / \epsilon)$. We output the first point in each interval, resulting in $L$ $\epsilon$-spaced subsampled points $\{q_1, ..., q_{L}\}$.

For each curve, the computational complexity of the 1D FPS algorithm is $O(K)$. Extending this to all curves, the computational complexity is $O(N)$. When parallelized on a GPU, for each curve, the 1D FPS algorithm has a has \textit{parallel} complexity of $O(\log K)$, where the $\texttt{CUMSUM}$ operation is the parallelization bottleneck (see \textbf{prefix sum} algorithms for more information~\cite{Blelloch1990PrefixSA}). The algorithm trivially parallelizes across curves, leading to a total parallel complexity of $O(\log K)$. In contrast, Euclidean farthest point sampling has a comptational complexity of $O(N^2)$ or $O(N log^2 N)$ (depending on whether a KD-tree is used), and a parallel complexity of $O(L)$. When $L$ is large (\ie we are subsampling a large number of points), Euclidean FPS is a significant performance bottleneck.

\subsection{Additional Details on Point Operations}

\paragraph{Set Abstraction (SA)}
\arch uses a series of set abstraction layers from PointNet++~\cite{Qi2017NIPS}, and we follow previous works to improve the set abstraction layer. First, we perform relative position normalization~\cite{Qian2022PointNeXtRP} -- given a centroid point $p_i$ with local neighborhood points $\mathcal{N}_i$, we center the neighborhood about $p_i$ \textbf{and} we divide by $r$ to normalize the relative positions. Additionally, we opt for the attentive pooling from RandLANet~\cite{Hu2020CVPR} instead of max pooling. 
% Finally, on large datasets nuScenes and KITTI, we find farthest point sampling to be slow (even after our efficient curve downsampling), and we opt for voxel downsampling   

\paragraph{Graph Convolution}
\arch uses a series of graph convolutions, which are modeled after the edge convolution from DGCNN~\cite{Wang2019SIGGRAPHb}. Unlike DGCNN however, we construct the K-Nearest-Neighbor graph based on 3D point distances instead of feature distances -- this permits more efficient neighborhood construction, irrespective of feature size. Furthermore, we use attentive pooling from RandLANet~\cite{Hu2020CVPR} instead of max pooling.

% \paragraph{Set Abstraction (SA)} 
% Set abstraction is the downsampling layer introduced in \cite{Qi2017NIPS}. 
% It proceeds by (1) \textit{sampling} a subset of ``centroid'' points from the point cloud at the current resolution, (2) \textit{grouping} the points around these centroids into local neighborhoods, (3) translating points into the local frame of their centroid and processing all points with a shared MLP, and (4) pooling over each local neighborhood to get a downsampled point cloud with associated features.
% \textit{Sampling} uses iterative farthest point sampling (FPS) \cite{Qi2017NIPS}, and
% % : at each iteration $k$, a point $p_k$ is added to the sampled subset such that $p_k$ is the farthest (in the Euclidean sense) from all previously sampled points $\{p_1, p_2, ..., p_{k-1}\}$.
% \textit{grouping} commonly uses a ball query, which groups together all points within a specified radius from a centroid.
% % Lastly, pooling can be max-pooling, mean-pooling, or an attention pooling~\cite{Hu2020CVPR}.

% \paragraph{Feature Propagation (FP)} 
% Feature propagation is the upsampling layer introduced in \cite{Qi2017NIPS}.
% Its goal is to propagate features from a low-resolution point cloud $\{q_1, ..., q_L\}$ with $L$ points to a higher-resolution point cloud $\{p_1, ..., p_H\}$ with $H$ points where $L{<}H$.
% This is achieved with \textit{feature interpolation}: given the low-res point features $\{g_1, ..., g_L\}$, the high-res point features  $F{=}\{f_1, ..., f_H\}$ are determined by a distance-weighted feature interpolation of the $k$ nearest low-res neighbors for each high-res point (based on the spatial coordinates of the points). 
% % To get the final upsampled point features, the high-res interpolated features $F$ are concatenated with skip-linked features from a corresponding SA layer and processed by a shared MLP.

% \paragraph{Graph Convolution}
% DGCNN~\cite{Wang2019SIGGRAPHb} leverages convolution operations on a dynamically constructed graph to process points.
% At each layer, the graph is constructed by adding edges between each point and its $k$ nearest neighbors in the learned feature space. 
% The convolution centered at a point uses a learned edge function for each edge followed by aggregation. Constructing a dynamic graph in feature space is computationally prohibitive on larger scenes, so we opt to construct the graph based on 3D point distances (\cref{sec:curvecloudnet}). 
