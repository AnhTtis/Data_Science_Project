{
    "arxiv_id": "2303.17598",
    "paper_title": "Consistent View Synthesis with Pose-Guided Diffusion Models",
    "authors": [
        "Hung-Yu Tseng",
        "Qinbo Li",
        "Changil Kim",
        "Suhib Alsisan",
        "Jia-Bin Huang",
        "Johannes Kopf"
    ],
    "submission_date": "2023-03-30",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Novel view synthesis from a single image has been a cornerstone problem for many Virtual Reality applications that provide immersive experiences. However, most existing techniques can only synthesize novel views within a limited range of camera motion or fail to generate consistent and high-quality novel views under significant camera movement. In this work, we propose a pose-guided diffusion model to generate a consistent long-term video of novel views from a single image. We design an attention layer that uses epipolar lines as constraints to facilitate the association between different viewpoints. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of the proposed diffusion model against state-of-the-art transformer-based and GAN-based approaches.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.17598v1"
    ],
    "publication_venue": "CVPR 2023. Project page: https://poseguided-diffusion.github.io/"
}