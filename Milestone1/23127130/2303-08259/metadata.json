{
    "arxiv_id": "2303.08259",
    "paper_title": "Contextualized Medication Information Extraction Using Transformer-based Deep Learning Architectures",
    "authors": [
        "Aokun Chen",
        "Zehao Yu",
        "Xi Yang",
        "Yi Guo",
        "Jiang Bian",
        "Yonghui Wu"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [
        "2023-05-10"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL",
        "cs.AI"
    ],
    "abstract": "Objective: To develop a natural language processing (NLP) system to extract medications and contextual information that help understand drug changes. This project is part of the 2022 n2c2 challenge.\n  Materials and methods: We developed NLP systems for medication mention extraction, event classification (indicating medication changes discussed or not), and context classification to classify medication changes context into 5 orthogonal dimensions related to drug changes. We explored 6 state-of-the-art pretrained transformer models for the three subtasks, including GatorTron, a large language model pretrained using >90 billion words of text (including >80 billion words from >290 million clinical notes identified at the University of Florida Health). We evaluated our NLP systems using annotated data and evaluation scripts provided by the 2022 n2c2 organizers.\n  Results:Our GatorTron models achieved the best F1-scores of 0.9828 for medication extraction (ranked 3rd), 0.9379 for event classification (ranked 2nd), and the best micro-average accuracy of 0.9126 for context classification. GatorTron outperformed existing transformer models pretrained using smaller general English text and clinical text corpora, indicating the advantage of large language models.\n  Conclusion: This study demonstrated the advantage of using large transformer models for contextual medication information extraction from clinical narratives.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08259v1"
    ],
    "publication_venue": null,
    "doi": "10.1016/j.jbi.2023.104370"
}