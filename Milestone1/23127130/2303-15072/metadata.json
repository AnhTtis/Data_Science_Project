{
    "arxiv_id": "2303.15072",
    "paper_title": "Exposing the Functionalities of Neurons for Gated Recurrent Unit Based Sequence-to-Sequence Model",
    "authors": [
        "Yi-Ting Lee",
        "Da-Yi Wu",
        "Chih-Chun Yang",
        "Shou-De Lin"
    ],
    "submission_date": "2023-03-27",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "The goal of this paper is to report certain scientific discoveries about a Seq2Seq model. It is known that analyzing the behavior of RNN-based models at the neuron level is considered a more challenging task than analyzing a DNN or CNN models due to their recursive mechanism in nature. This paper aims to provide neuron-level analysis to explain why a vanilla GRU-based Seq2Seq model without attention can achieve token-positioning. We found four different types of neurons: storing, counting, triggering, and outputting and further uncover the mechanism for these neurons to work together in order to produce the right token in the right position.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15072v1"
    ],
    "publication_venue": "9 pages (excluding reference), 10 figures"
}