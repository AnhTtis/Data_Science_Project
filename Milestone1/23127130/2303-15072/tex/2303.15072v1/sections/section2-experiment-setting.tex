\section{Model and Training Details}

%We have mentioned that the token-positioning can be divided into three sub-tasks, including (1) assigning a target token at a specific position: set \emph{token} be one of a word in the output dictionary (2) control the length of the whole output sequence: set \emph{token} to EOS (3) control the rhyme of the output sequence: \emph{token} corresponds to a set of words in a dictionary and is mutually exclusive. We take the first one as the main example and leaves other sub-tasks in the section 5.
%We have mentioned that the token-positioning task can achieve at least three sub-tasks, including (1) assigning a target token at a specific position (2) control the length of the output sequence (3) control the rhyme of the output. We take the first one as the main example and leaves others in the section 5. 
 We first describe the model and training details for achieving token-positioning. Terminology-wise, $t$ indicates the time step of a Seq2Seq model where $t=0$ indicates the end of encoding. Thus, negative $t$ corresponds to the encoding stage and positive $t$ is the decoding stage. We use $T$ to denote the target position for the assigned token.

%\subsection{Dataset and Training Process}


\textbf{Dataset} We select two public datasets, the $Gutenberg$ dataset \cite{lahiri:2014:SRW} and Lyric dataset crawled from $Lyrics\text{ }Freak$, to generate the training data, and extract pairs of consecutive lines as the input and output sequences. We randomly select one token from the output sequence as our assigned token and concatenate the token and its position to the end of the input sequence. %For example, we select \emph{doors} from the output sequence, \emph{lock the doors}, then concatenate \emph{doors} and its position \emph{3} in the end of the input sequence. 
Note that we do not put any boundary signal to separate these control signals with the original input sequence, thus the model treats them just like it does to all other inputting tokens. The vocabulary size of our model is 20000. %, and the max length of sequence are 21 and 14 respectively for two datasets. 
More details are shown in Appendix A. 


\textbf{Model and Training Details} We choose a GRU-based encoder-decoder Seq2Seq model as described in Figure \ref{structure} (the parameters are also shown). Following Occum’s Razor policy, here we focus on the simplest possible architecture that can achieve such goal without complicating the analysis. %The detailed structure and parameters (e.g. hidden size) are shown on Figure \ref{structure}. 
It is important to point out that this vanilla model uses an identical bridge between encoder and decoder. In addition, the common perplexity loss of every token $= \exp(- \sum_{t=1}^{L+1} \log Pr(y_{t} | \hat{y}_{t-1}, h_{t-1}))$ is used as the training objective, where $y_t$ represents the target output, $\hat{y}_t$ as the model output, and $h_t$ as the hidden state. Note that during training we did not particularly assign loss to the token to be positioned, so the model is not directly guided to align the two control signals with the outputs. During inference, the model simply outputs the token with maximum probability, $\hat{y}_{t} = \mathop{\arg\max}\limits_{y \in \text{vocab}}Pr(y|\hat{y}_{t-1},h_{t-1})$, without adopting
a more complicated algorithm such as beam search. Besides, we use Adam as our optimizer and set early stop if the validation loss stops decreasing for few epochs. 
We focus the evaluation on whether the target token appears in the target position, and this token positioning accuracy can reach 98.5\% and 98.1\% respectively on the two datasets with random initialization multiple times. For generality, we substitute the GRU with LSTM models, and the average accuracy can also achieves 97.5\%. Finally, it is also possible to assign multiple control signals to control multiple tokens at the same time, and the accuracy achieves a respectful value of 95.2\%. It is surprising to us that by simply modifying the training dataset, a naive Seq2Seq model is capable of deciphering the message provided implicitly as Token and Position.
% as mentioned in \cite{shen2019controlling}, 

\begin{comment}
\subsection{Research Questions}

There are three research questions to answer:
\begin{enumerate}
    \item How does the model store the $token$ information?\\
    In order to output the token in the right position, the model needs to start carrying the token information after it appears in the encoder till when it’s time to output the token. Relevant questions of interests include: How such information are stored in the neurons? How do they evolve over time (stored in the same neuron or different neurons)?
    \item How does the model use $position$ signal to know when to output?\\
    Inspired by \cite{shi2016neural,gers2000recurrent,weiss2018practical} that reveals the counting behavior of an RNN, here we focus on analyzing how GRU learns to use the position signal to achieve the task. In order to position the right token at the right time, the Seq2Seq model needs to perform some kinds of counting down based on the positioning signal. %For instance, in Figure 1, when the encoder sees the positioning signal at t = 0 (e.g. 10), it has to make sure after 10 steps (t = 10) the right token possesses the largest probability. 
    It is non-trivial in an RNN or GRU unit whose outputs are simply a hidden state vector that goes through the same decoder again and again.
    \item How does the model combine $token$ and $position$ information to output?\\
    Given the answers of the previous two questions: the model can store token information and perform counting down given the position signal, we still need to answer how the neuron can interact with each other to activate and output the right token. We have discovered that for this purpose, z-gate, r-gate and $\hat{h}$ in GRU each plays different roles to make it happen (details are in section 5).
\end{enumerate}
\end{comment}
