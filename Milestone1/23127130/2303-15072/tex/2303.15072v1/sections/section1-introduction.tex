\definecolor{mygreen}{RGB}{76, 146, 58}
\definecolor{mypurple}{RGB}{148, 103, 189}
\definecolor{myorange}{RGB}{255, 127, 14}
\definecolor{myred}{RGB}{214, 39, 40}
\definecolor{myblue}{RGB}{31, 119, 180}


\section{Introduction}

Sequence-to-sequence (Seq2Seq) \cite{sutskever2014sequence} models based on Recurrent Neural Networks \cite{mikolov2010recurrent} and its variant, such as Gated Recurrent Unit (GRU) \cite{cho2014learning} and Long Short-Term Memory (LSTM) cell \cite{hochreiter1997long}, has demonstrated high rate of success in areas such as question answering \cite{sukhbaatar2015end,dong2016language}, dialogue systems \cite{serban2016building}, machine translation \cite{sutskever2014sequence,luong2015multi}, and text generation \cite{wen2015stochastic, shang2015neural}. This paper focuses on a vanilla encoder-decoder based Seq2Seq model mostly following its original design in 2014 \cite{sutskever2014sequence}, as shown in Figure \ref{structure}, and the major difference is that we use GRU instead of LSTM. It is constructed by two recurrent blocks (encoder and decoder), an embedding layer to deal with inputs, and a fully connected layer to generate distributions of all tokens. In order to learn the output sequence distribution (target) given the input sequence (source), common training data are designed as input and output pairs: [$x_1,x_2,x_3$] $\rightarrow$ [$y_1,y_2,y_3,y_4$]. 


The inference process is described as follows: First the encoder encodes [$x_1,x_2,x_3$] to a dense vector $h_0$. Then at the first step of decoder, $h_0$ is transformed into $h_1$ after passing through the decoder unit once. $h_1$ then serves as input to the fully connected layer to produce the conditional probability of each single output token. Normally the token with highest probability becomes the first output token $\hat{y}_1$. $h_1$ shall be recursively passed down to the \emph{identical} decoder unit to generate conditional probability 
at subsequent time steps to produce tokens ($\hat{y}_2, \hat{y}_3, ...$) one by one.

%In addition that $h_1$ will be passed into the FC layer and converted into conditional probability of output tokens ($y_1$), it will recursively pass down to the \emph{identical and fixed} decoder units again to output other words ($y_2, y_3, ...$) to form a sequence. The model outputs the token, $o_t$ with maximum probability,  
%(3) Firstly, it will be converted to the conditional probability of the output token by the FC layer, $o_1 = FC (h_1)$, and the model will output the token with the highest probability: $\hat{y}_{1} = \mathop{\arg\max}\limits_{y \in \text{vocab}}o_1 =\mathop{\arg\max}\limits_{y \in \text{vocab}}Pr(y|\hat{y}_{t-1},h_{t-1})$.
%$\hat{y}_{t} = \mathop{\arg\max}\limits_{y \in \text{vocab}}Pr(y|\hat{y}_{t-1},h_{t-1})$. 
%(4) Secondly, $h_1$ (there is a slightly different between with LSTM or GRU base) will recursively pass down to the \emph{identical and fixed} decoder units again and again to output conditional probability for later time step, ($y_2, y_3, ...$), to form a sequence.

\begin{figure*}%[h]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/structure.png}
  \caption{The Seq2Seq model combines classic encoder-decoder structure with one embedding layer and one fully connected layer without attention nor bi-directional structure.}
  \label{structure}
\end{figure*}

Previous research \cite{shen2019controlling} has shown that it is possible to train a Seq2Seq model to output a token at a specific position with high accuracy. In fact, with two control signals, \emph{token} and \emph{position}, attached to the input sequence directly, they showed that a transformer model can learn the meaning of the control signals and output the right token at the right position with accuracy close to 100\%. 
Furthermore, it is also possible to train a vanilla Seq2Seq model (i.e. no attention or bidirectional structure is needed) like the one shown in Figure \ref{structure} to achieve the same goal. More precisely, training with sufficient number of examples containing token and position signals at the end of the input, the model can eventually learn that it has to output the token at the target position (e.g. third) as shown below:


\begin{center}
\textit{
$x_1$, $x_2$, $x_3$, {\color{myred}token}, {\color{mygreen}3} $\rightarrow$ $y_1$, $y_2$, {\color{myred}token}, $y_4$
}
\end{center}
%\begin{center}
%"\textit{
%Ride in the back {\color{myred}doors} %{\color{mygreen}3} $\rightarrow$ lock the %{\color{myred}doors}
%}",
%\end{center}
%the model can eventually understand that it shall output the token (e.g. $y_3$) 
%at the target position (e.g. third) with close to 100\% accuracy. 
%the numerical position signal can be any number as long as this position has been seen in the training set.
 Note that a Seq2Seq auto-encoder\cite{ma2018autoencoder,xu2017variational} can be regarded as a realization of such token-positioning function since the model has to precisely outputs each token at the right position. 
 %For example, a well-trained vanilla Seq2Seq model can produce an outputting sequence whose third token is {\color{myred}doors} given the input "\emph{Ride in the back  {\color{myred}doors} {\color{mygreen}3}.

%$\rightarrow$ lock the {\color{myred}doors}}" uses \emph{doors} as the target token and \emph{3} as the target position.

%We will further confirm in the subsequent paragraphs that when the token is set to \emph{EOS} (a token to label end-of-sequence) or a certain rhyme, this model can also be used to control the length or rhyme of the output sentence.


We believe this finding is very interesting and potentially influential since the model in Figure \ref{structure} has no single component specifically designed for token positioning. It is exactly the same model used to perform machine translation as its original goal. % and the only modification made lies solely on the generation of training data (i.e. adding two control signals). 
Comparing with the state-of-the-art models that usually consist of much more complicated structure (e.g. transformer, GAN, etc.) or mechanism (e.g. attention, variational inference, etc.) to achieve the control of generation, we are quite surprised to learn that the simplest Seq2Seq model can already achieve fine-grained control of outputs given carefully designed training examples, which motivates us to focus on explaining the mechanism behind such capability. 

To begin with, we want to explain that generating a word at a target position is mathematically challenging for a vanilla Seq2Seq model as shown in Figure \ref{structure}. We start from Equation \ref{long-eq} that describes mathematically how the model produces output sequences during inference. %The key factor is that all learned decoder units \emph{are identical} (and so is the fully connected function). 
%mathematically we can write the generated sequence as ($\hat{y}_{1}$, $\hat{y}_2$, ..., $\hat{y}_T$). 
The equation tells us that the target output $y_t$ is a fairly complicated function that contains $argmax$ among probabilities of all tokens in the vocabulary generate by the fully-connected layer FC, which depends on the GRU mapping of $h_{t-1}$ and  $y_{t-1}$, and further depends recursively on $h_{t-2}$ and  $y_{t-2}$, and so on. Eventually we can regard every output token as coming from a very complicated recursive function of the encoder output $h_0$. In order to output one specific token at position $t$, $h_0$ needs to make sure that after entering the same GRU unit recursively for $t-1$ times, the fully connected layer can assign the largest probability to the target token among tens of thousands of possible candidates. Also note that since the decoder shares the same FC and GRU function in every step, all the dynamic information needs to be stored in the hidden state $h_t$ in order to output different tokens at different positions.  %Thus it is not intuitive why a memory-less model like the one shown in Figure \ref{structure} can deliver the correct output precisely at a specific position. %In order to output a specific token precisely at a later position, We find it surprising that the target token can still be produced at the right position after so many recursive calls and non-linear selections. 

\begin{comment}
Another two extended examples:
%\begin{center}
"\textit{
Ride in the back {\color{myred}EOS} {\color{mygreen}3} $\rightarrow$ lock the doors
}", and "\textit{
Ride in the back {\color{myred}or} {\color{mygreen}3} $\rightarrow$ lock the {\color{myred}doors}
}"
%\end{center}
, the model can learn to output \emph{EOS} to control the length of the outputs, or learn to output the word with rhyme '\emph{or}' at the third time step respectively. Therefore, with the format, 
%\begin{center}
"\textit{
    $x_1$, $x_2$, $x_3$, ${\color{myred}Token_{A}}$, ${\color{mygreen}Pos._{3}}$,  $\rightarrow$ $y_1$, $y_2$, ${\color{myred}Token_{A}}$, $y_4$
    }"
%\end{center}
, the model can learn to output a sequence with conditions. 
\end{comment}

%This is very different from previous works that most of them design a more complicated model structure to deal with the controlled conditions, i.g. concatenate the control signals in the embedding layer or hidden state (Need reference here). In addition, it is also different from the more intuitive approach which uses different structures to process different types of control signals, for example, design different models to control the length or rhyme. 
%, first of all, to produce the right token at a specific position, this token shall processes the highest probability (i.e. output from FC) among tens of thousands of possible tokens in the dictionary. Next, the generation of the first token $y_1$ comes from a complicated non-linear GRU function that involves the encoder outputs $h_0$, while the generation of $y_2$ recursively requires the embedding of requires generating the probabilities of all tokens in the dictionary and is a complicated non-linear mapping from $h_k-1$ that requires not only the process of GRU but also finding the tokens of the probabilities of all token 

%It is not only because model should learn that the two control signals $token$ and $position$ have their own function and shall be treated differently comparing to contain different information from other tokens in the input sequence, the more critical factor is about the recursive mechanism of the recurrent neural network itself: (1) As shown in Figure \ref{structure}, the decoder units (GRU in the figure) and fully connected layer \emph{are all identical}. 
Therefore, each outputs ($\hat{y}_{1}$, $\hat{y}_2$, ..., $\hat{y}_T$) can be written mathematically as follows: %However, each $h_t$ depends on the previous $h_{t−1}$ as well as the previous output $y_{t−1}$, which recursively depends on the previous states $h_{t−2}$, $y_{t−2}$, and so on. Therefore, 
% Mathematically we can write the recursive formula of the decoder as following,  

\begin{equation}
    \begin{aligned}
    \hat{y}_1 &= \mathop{\arg\max}\limits_{y \in \text{vocab}} FC(h_1)
            = \mathop{\arg\max}\limits_{y \in \text{vocab}} FC \big(GRU(h_{0}, Emb(\hat{y}_{0}))\big) \\
    \hat{y}_2 &= \mathop{\arg\max}\limits_{y \in \text{vocab}} FC(h_2)
            = \mathop{\arg\max}\limits_{y \in \text{vocab}} FC \big(GRU({\color{myorange}h_{1}}, Emb({\color{myblue}\hat{y}_{1}}))\big) \\
            & = \mathop{\arg\max}\limits_{y \in \text{vocab}} FC \Big(GRU\big({\color{myorange}GRU(h_{0},Emb(\hat{y}_{0}))}\big),
            \quad  Emb({\color{myblue}\mathop{\arg\max}\limits_{y \in \text{vocab}} FC \big(GRU(h_{0}, Emb(\hat{y}_{0})))}\big)\Big) \\
    \hat{y}_t &=...        
    \end{aligned}
    \label{long-eq}
\end{equation}

%That is, all outputs are generated by $h_0$ with different times GRU transformation and one time FC transformation, but it needs to learn to output a extremely huge possible combinations of words, that says, $20000  ^ {21}$ (20000 is the vocabulary size and 21 is the max length in our data). (2) A Seq2Seq model is unlike a memory-based model that the later one can save the information in the memory until it is needed. A RNN-based Seq2Seq model, especially without other additional bridge or transformer, can only save the information in its hidden state, i.e. only in a dense vector.

In order to accurately position a token, this paper uncovers how a Seq2Seq model can perform the following functions: (1) The model not only needs to \emph{store} the token information after it appears in the encoder, but also confines its impact till the target outputt time. %This is not an easy task because in each time step the hidden state enters an identical GRU function.
(2) In order to output at the right position, the Seq2Seq model requires a mechanism to \emph{count down}. (3) The \emph{storing} and \emph{counting} mechanisms need to interact in a certain way to ensure the fully connected layer assigns the largest probability to the target token. 
%For instance, this \emph{counter} is not likely to stored as a numerical value in a neuron since such neuron can hardly interact with \emph{storing} neurons to achieve (3).
%Note that different from a memory-based model such as xxx, an RNN-based Seq2Seq model doesn't have memory to store extra information, thus all required information to be passed to the next stage needs to be stored in the hidden state $h$. 
%we still need to answer how neurons interact to activate the output of the correct token. Hence, this paper is aim to provide an in-depth analysis on those questions to explain how a Seq2Seq model can learn to position a word accurately.
%The first step of our work is to train a Seq2Seq model on the data pairs mentioned above. Next, with the help of the auxiliary classifier and integrated gradient, the recurrent neurons can be categorized to four important neuron sets. These neuron sets perform four different basic functions of GRU cell, including \textit{storing}, \textit{counting}, \textit{triggering} and \textit{outputting}. Noting that every target token (20000 tokens in our output dictionary) has their own important neurons set. Afterwards, we replace the values of the important neuron sets to verify how these small portion of neurons influence the entire model. At this stage, we can answer the first two research questions by observing the values of storing and counting neuron set respectively, and also answer the third questions by analyzing how these neuron sets interact. 


The contributions of this paper can be summarized as: (1) We find that neurons in a Seq2Seq model can perform four basic functions, \emph{storing, counting, triggering,} and \emph{outputting}. (2) We discover the mechanism behind counting down and token positioning based on the interaction among different types of neurons. (3) We propose a series of strategies to identify neurons of specific purpose in a recursive neural network, which can potentially be exploited for other types of neuron-based analysis. %
%(3) We also report several other side-findings, such as how h, z and r gate play different roles to transmit the information, to provide a better understanding about a GRU-based Seq2Seq model.
%The paper is constructed as below: we first describe our experimental setup and research problems. In section 3, we propose a general strategy to identify and verify the important neuron set. Next, we explain how we use the strategy to find out the important neuron set, and try to explain these three research questions. In section 5, we summarize our findings and leave discussion.


% We leave LSTM-based cells as a future work since GRU is simpler than LSTM with one gate, but the difference of accuracy between these two is less than one percent. 


\begin{comment}
---
As shown in Figure \ref{structure}, at every step of the decoder, the same GRU outputs a hidden state ${h_t}$, which will be fed into a fully-connected layer (FC) to generates the conditional probabilities for each token. In order to output a specific token, the probability of this token has to be the largest one. However, each $h_t$ depends on the previous $h_{t−1}$ as well as the previous output $y_{t−1}$, which recursively depends on the previous states $h_{t−2}$, $y_{t−2}$, and so on. Mathematically we can write the recursive formula of the decoder as following,  
%\begin{small}
%\begin{equation}
\[
    \begin{aligned}
    \hat{y}_t &= \mathop{\arg\max}\limits_{y \in \text{vocab}} FC(h_t)
            = \mathop{\arg\max}\limits_{y \in \text{vocab}} FC \big(GRU({\color{myorange}h_{t-1}}, Emb({\color{myblue}\hat{y}_{t-1}}))\big)\\
            & = \mathop{\arg\max}\limits_{y \in \text{vocab}} FC \Big(GRU\big({\color{myorange}GRU(h_{t-2},Emb(\hat{y}_{t-2}))}\big),
            \quad  Emb({\color{myblue}\mathop{\arg\max}\limits_{y \in \text{vocab}} FC \big(GRU(h_{t-2}, Emb(\hat{y}_{t-2})))}\big)\Big) \\
            & = \text{a complicated function of } h_0...
    \end{aligned}
    %\label{tab:long-eq}
\]
%\end{small}
%\end{equation}
Precisely speaking, outputs ($y_{1}$, $y_2$, ..., $y_T$) are all generated by $h_0$ with different times GRU transformation and one time FC transformation. The model needs to learn to output a extremely huge possible combinations of words, that says, $20000  ^ {21}$ (20000 is the vocabulary size and 21 is the max length in our data), but it can only use a fixed GRU and FC weight. Therefore, this paper aims to provide an in-depth analysis on how a Seq2Seq model can learn to position a token accurately, as we called the token-positioning ability, given the recursive mechanism and such simple structure. It can be divided into three questions: (1) How does the model store the $token$ information? (2) How does the model use $position$ signal to know when to output? (3) How does the model combine $token$ and $position$ information to output? 
\end{comment}

