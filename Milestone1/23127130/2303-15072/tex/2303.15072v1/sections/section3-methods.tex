\section{Identifying Neurons of Specific Functions}

This section describes a general technique that allows us to identify and verify the critical neurons for certain basic functions. The process contains three stages: candidates generation, filtering, and verification by manipulating the neuron values. 

%\begin{figure*}[h]
%  \centering
%  \includegraphics[width=0.95\textwidth]{figures/neuron_identification.png}
%  \caption{Neuron identification pipeline. (a) Assume a small model containing five neurons in a hidden state. (b) First use feature selection on a classifier to select neurons related to a certain function. (c) Filter out the neurons with lower IG score. (d) Verify whether the function is disabled after de-activating the neurons.}
%  \label{neuron_identification}
%\end{figure*}


\subsection{Hypothesis formulation and candidate neurons generation}
We start from formulating certain hypothesis about the functions of neurons to be verified.
The first step to verify the hypothesis relies on checking whether it is possible to clearly separate instances that contain such function from those that do not. We realize such process by checking the accuracy of a classification task. The hypothesis is considered \emph{plausible} when the accuracy is higher than 95\%, or rejected otherwise. Once a hypothesis becomes plausible, we then perform feature-selection to pick a subset of dominate neurons as the responsible ones and move on to the next stage. %Note that here we check the training accuracy instead of validation accuracy since we only care about finding a subset of neurons to separate the existing cases, rather than generalization to the unseen cases.  %  manually create some labels assign to the hidden states and  The first step in candidates generation is to train a linear classifier to classify the hidden state. In order to identify whether the neurons contain certain information of interest or perform certain function, the information is used as the labels.
A real example goes as follows. First a hypothesis is initiated: \emph{there is a subset of neurons in $h_t$ that stores the target token information}. Then we label all $h_t$ with a given token as positive and other instances NOT associated with this token as negative. Then we train a classifier using $h_t$ as features to examine whether it is possible to separate positive and negative sets. It turns out that we can reach training accuracy higher than 95\%, thus this hypothesis is considered plausible and we move on to select a subset of critical features.
%m this hypothesis since they are highly distinguishable.    "\emph{outputs a specific token at step T}" as 0 and the rest as 1, in order to find out whether the hidden state contain the information of this token. By contrast, to find out whether some neurons carry the information to %to find out whether some neurons carrying the position information, we label all $h_t$ with $T - t$, so that the chose neurons may contain the information about how many steps are left to the assigned position $T$.
%High training accuracy implies some neurons in $h_t$ do contain the information about this label. And if the classification accuracy is high enough, such as $90\%$, we further apply feature selection algorithm to select a group of neurons that can contribute to the classification. 
%We start by crafting classification labels to identify whether certain neurons contain information of interests. Taking storing neurons as an example, we label all data which outputs token A at step T as 0 and label all the other data as 1. On the contrast, to find out the counting neurons, we label all $h_t$ with $T - t$.
%Next, we gather the hidden states, $h_t$ ($t < T$), of the model and train a linear classifier on them to distinguish these two groups. High training accuracy implies some neurons in $h_t$ do contain information about this label. Afterwards, apply feature selection to select a portion of neurons that yields high classification accuracy. %Note that since this classifier is trained independent of the original Seq2Seq model, this only tells us that the selected neurons \emph{can potentially be responsible}.  
In practice we choose linear support vector machine (SVM) as our basic classifier, and recursive feature elimination (RFE) as the feature selection mechanism.
In addition, at this stage we suggest less-rigorous selection to avoid missing candidate neurons, because the next two stages allow us to further confirm the hypothesis and remove false-positive neurons. %Therefore, at this stage, approximately 40\% to 50\% of neurons are left as \emph{candidates}, depending on the accuracy of the linear classifier.


\subsection{Filtering}

Neurons selected from the candidate generation stage only implies their patterns can be distinguished by an \emph{independently} trained classifier. However, it does not guarantee the original Seq2Seq model really utilizes them for specific purpose. Here we propose to use an internal mechanism to verify whether those neurons are critical to a specific output. We apply the method of integrated gradient \cite{sundararajan2017axiomatic} to quantify the importance of a neuron to the output and then filter out the less-relevant neurons. In integrated gradient, suppose we have an output function $F$, the integrated gradient score along with the i-th neuron for an input $x$ and baseline $x'$ is defined as follows:
\[\text{score}_i(x) = (x_i - x_i^{'}) \times \int_{\alpha = 0}^1 \frac{\partial F(x_i^{'} + \alpha (x_i - x'_i))}{\partial x_i} d \alpha\]
%\begin{normalsize}
%\end{normalsize}
For instance, to measure the importance of each neuron in $h_t$ in terms of outputting token A at position T, we set $F(x)$ as the probability to output token A at t = T, and x as $h_t$. We then use the score to filter out the less-important neurons found in the \emph{candidate generation} step.

%Thus, this technique can be considered as an in-policy method to quantify how much each neuron affects the outputs. However, the integrated gradient cannot reveal the specific function of a neuron, such as counting or storing, 

%Thus in our solution, IG needs to be exploited after the candidate generation step to target on one specific function. %To sum up, integrated gradient is performed after \emph{candidate generation} procedure to filter out the neurons whose scores are too small and that is, not important enough.


\subsection{Verification by manipulating the neuron values}
After the previous two stages, we have found a set of neurons that are influential to the output and \emph{potentially responsible} for a specific function. Here we propose to further verify them by replacing the value of these neurons with some carefully designed values and check if the behavior changes as expected. For example, we can replace the values of counting down neurons at $t$ with the corresponding values at 
$t - 1$, and check whether the model takes one more step to output the token. If it turns out that such replacement can indeed control the output behavior of the model, then we are confident to confirm the original hypothesis about the functionality of the neurons.  
%Here we use the strategies described above to identify four types of neurons: counting, 