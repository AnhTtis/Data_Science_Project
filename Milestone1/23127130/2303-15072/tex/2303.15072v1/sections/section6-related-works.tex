\section{Related Works}

\textbf{Controlling Seq2Seq Model.}
% 	Token-positioning task is about outputting a given token in a given time step. In our work, it is our downstream task to analyze the behavior of GRU model. 
	Previous works have shown that a Seq2Seq model can be controlled by directly manipulating the values in the hidden state or attaching additional signal to the encoded embedding. In \cite{Identify_Control_NMT}, it shows examples of changing the values of neurons responsible for tense and gender to control the outputs in machine translation.  \cite{weiss2018practical} proposes to control the attributes of generated sequences such as mood and tense by concatenating the attribute embedding to the encoded input sequence.% Our works on token positioning can be considered as a kind of controlling, but this time not just about the content but also the precise position of writing. 
	Different from the previous works that tries to manipulate the model to achieve the controlling, this paper pays more attention to explain why the controlling can be achieved.  %In \cite{shen2019controlling}, it shows that with proper control signal attached to the end of input sequence, it is possible to control the output sequence with a vanilla Seq2Seq model. In our work, we simply attach the control signal to the input sequence and apply the GRU model without attention or bi-directional mechanism to simplify not only the model but also the analysis complexity while remaining high performance.

% 	In our work, we substitute transformer with GRU and do not apply attention to simplify \textbf{not only the model but also the analysis complexity} while remaining high performance.


% Shen. 移到最後
% 加上control signal怎麼接的：1. encoder embedding 的有哪些... 2. ... 3. Shen ...
% 多找幾篇concatenate 在encoded embedding的paper



% Control Seq2Seq
% RNN 功能
% 分析Neuron方式
% Feature Importance: IG
% 找跟counting或是neuron互相影響相關的paper

% \textbf{Mechanism of RNN Cell}
% Several works have investigate the mechanism inside RNN and its variants like LSTM and GRU. Such mechanisms include counting [R1] [R7], sentiment analysis [R2], storing lexical, compositional [R3], tense, gender [R4], and sentence structure knowledge [R5].

% \textbf{Neuron Level Analysis}

% Since neural network has usually been regarded as black box and uninterpretable, recently, there are more and more analytic method emerged to help people understand how these model achieve excellent performance in certain task. In computer vision field, there are lots of method developed to analyze vision tasks such as class activation map, integrated gradient [], [], [], ...etc. for image classification. However, in natural language processing field, there are less work using the analytic method mentioned above to mining important pattern in the models in comparison with computer vision field. Since these analysis methods share the same goal as capturing the class activation, in our case, the word activation of Seq2Seq model, we incorporate some gradient-based method into our analysis.


\textbf{Token-Positioning.}
In \cite{shen2019controlling}, it reveals a wide variety of application involves the token-positioning task. For example, it is used to demo a acrostic poem system generating hidden token pattern through controlling the outputting words at certain positions. It can be used to produce rhyming lyrics by assigning tokens that rhymes at the end of the sequence.   Additionally, it can control the length of a sentence by assigning the token "EOS" at the target position. Seq2Seq auto-encoder\cite{ma2018autoencoder,xu2017variational} can 
also be regarded as a realization of positioning on multiple tokens since every token has an assigned position. 

% $$, the length of sentence can easily be determined. Last but not least, given the circumstance that there is no misplacement for all given tokens and positions, such work can be regarded as a simpler version of Seq2Seq auto-encoder since the auto-encoder requires the output sequence entirely the same as the input sequence but without helps from additional control signal. Understanding token-positioning task stands for the pioneer for understanding the Seq2Seq auto-encoder.

% A wide variety of application involves token-positioning task. For example, in acrostic poem, it requires generating specific token pattern such as the first word of all the sentences can be viewed as a hidden sentence. Such task can be achieved by simply specifying the token to output at the first time-step of each sentence.  Furthermore, if the specified token is a cluster of word with the same property, it can generate sentence not only with specified property but also with more diversity. Such capability can be applied into tasks like lyric or rap generation since in the same verse, they requires words with the same rhyme. Additionally, if the specified token is "EOS", the length of sentence can easily be determined. Last but not least, given a more strict circumstance that there is no misplacement for all given words and positions, such work can be regarded as a simpler version of Seq2Seq auto-encoder since the auto-encoder requires the output sequence entirely the same as the input sequence but without helps from additional control signal.



\textbf{Understanding Recurrent Network Based on Neuron Analysis.}
% counting \cite{Visualize_RNN} \cite{weiss2018practical}, 
There have been several works perform neuron-level analysis inside RNN for various purposes such as sentiment analysis \cite{RNN_Sentiment}, lexical and compositional storage \cite{qian-etal-2016-analyzing}, tense analysis \cite{Identify_Control_NMT}, and sentence structure analysis \cite{lakretz-etal-2019-emergence}.
%These works reveal that not only the the attention mechanism can be utilized to explain how model performs downstream tasks, but also analysis in the neuron level. 
One of the most commonly used strategy utilizes visualization to observe the dynamics of hidden states for each time-step \cite{Visualize_RNN}. %However, visualizing is not strong enough to prove certain hypotheses. 
 In \cite{weiss2018practical}, it shows that counting behavior can be achieved much easier in LSTM, while in other variants such as GRU, the dynamics of most neuron values seem to be irregular. Our findings can be considered as offering an explanation how GRU can perform counting even though each individual neuron's value does not seem to have strong correlation with the counts. %  But it does not consider the case that multiple neuron can perform counting together and the perspective of counting it  focuses on is the unbounded counting which is not under our discussion.
 There have been some works utilizing diagnostic classifier \cite{giulianelli-etal-2018-hood},  Pearson Correlation \cite{Identify_Control_NMT}, and ablation test to probe the functions of neurons, which shares similar spirits to the first strategy we used (the only difference is that we further select a subset of dominant features). Nevertheless, as we have pointed out, simply performing diagnostic classification or correlation analysis cannot directly confirm the hypothesis, and the third stage of our solution (verification by manipulating neuron values) is the key factor to verify the functions of neurons. % extract neurons from the hidden state which seem to be chaotic by visualization and prove neurons are indeed responsible for certain task. 
 Besides, \cite{Identify_Control_NMT} proposed to directly manipulate the neuron values to ameliorate the model performance, and similar strategy has been adopted by us to verify the functionality of neurons. Nevertheless, we believe the proposed 3-stage strategy can be a general mechanism that researchers can adopt to confirm the hypothesis of functions of neurons. 
 %not only to locate the important neurons but also to verify their correctness cautiously. The first step in our strategy is similar to the method in , but we perform the classification recursively and select neuron with the most importance once at a time to guarantee its effectiveness. Such strategy can be applied to other NLP tasks or fields. With the strategy to extract important neurons and their functions, we provide a comprehensive understanding on the GRU-based Seq2Seq model.



% 補Counting差別 V
% 補diagnostic classifier跟RFE+SVM的差別 V


% Several works have investigate the mechanism inside RNN and its variants like LSTM and GRU. Such mechanisms include counting [R1] [R7], sentiment analysis [R2], storing lexical, compositional [R3], tense [R4], and sentence structure knowledge [R5]. These works reveal that not only attention can be utilized to explain how model perform downstream tasks but also the neurons inside the hidden state. They dive into the neuron level and conduct experiments to analyze the relationship between neurons and the corresponding task. One of the most commonly used method is visualizing the dynamics of hidden state through each timestep [補]. However, visualizing is not strong enough to prove certain hypotheses. In [R7], it shows that LSTM can easily implement counting behavior while other variants such as GRU cannot since the visualization of LSTM neuron value dynamic shows that only few neuron values vary while other are stable and the dynamic of most GRU neuron values seem to be irregular. But it does not consider the case that multiple neuron can perform counting together. Besides, [R1] claims that LSTM can perform counting since the value of some neuron gradually decrease from the start of sentence to the end. These works are lack of further experiments to prove that such pattern are indeed related to counting.
% To more accurately unbox and interpret the information neurons hold, some works utilize diagnostic classifier [R6], Pearson Correlation [R4], and ablation test to extract neurons from the hidden state which seem to be chaotic by visualization and prove neurons are indeed responsible for certain task. Besides, [R4] directly manipulate the neuron value to ameliorate the model performance and  prove them to be effective to downstream task. However, the recurrent property of RNN limits the circumstances that above methods can be applied or weaken their capability. In addition to natural language processing, individual neurons were also shown to carry meaningful information in computer vision field. Consequently, some of the analytic technique originated in vision field can be introduced to investigate the RNN model especially the gradient-based method which can efficiently analyze the correlation between input and output of model given there is no ReLU activation function in the model which is toxic to gradient-based method. Incorporating the method from previous work and vision field to analyze  and prove the counting, storing, trigger, and fc neurons, we apply diagnostic classifier to extract neurons with guidance such as counting or word label, apply integrated gradient [補] to verify such neuron indeed carry more importance in the model, and manipulate neuron values to further prove the effectiveness of selected neuron. Our work  provide not only qualitative visualization but also quantitative experiments as solid evidence.



% 1. Control-Signal
% 2. 分析的方法: visualize, classifier, gradient based, others
% 3. RNN or Seq2Seq 的功能: 最好講到Counting, Trigger


\begin{comment}
跟 RNN counting 有關的文章：（偏第三段，但可以挑部分比較直接相關的就好）
https://arxiv.org/pdf/1805.04908.pdf
https://ieeexplore.ieee.org/document/861302
https://link.springer.com/article/10.1007/BF01694011
https://www.tandfonline.com/doi/abs/10.1080/095400999116340

分析 GRU cell （偏第三段）
https://arxiv.org/abs/1703.07588

分析 RNN
http://papers.nips.cc/paper/5166-training-and-analysing-deep-recurrent-neural-networks
https://www.sciencedirect.com/science/article/abs/pii/S0167739X18309233
BEYOND WORD IMPORTANCE: CONTEXTUAL DE- COMPOSITION TO EXTRACT INTERACTIONS FROM LSTMS: 這篇好像也是在討論分析方法，所以可能也比較適合放在第二大段
LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages: 應該可放第三段

分析 Seq2Seq（可以找找看有沒有能放在第三段的，因為第二段應該夠長了）
https://arxiv.org/abs/1807.03915
https://arxiv.org/abs/1811.03451
https://www.researchgate.net/publication/336047648_Clustering_and_Recognition_of_Spatiotemporal_Features_through_Interpretable_Embedding_of_Sequence_to_Sequence_Recurrent_Neural_Networks?enrichId=rgreq-9f3322940dd50cbc153ec739e50a0ffa-XXX&enrichSource=Y292ZXJQYWdlOzMzNjA0NzY0ODtBUzo4MDcxNTQxNzI4MjE1MDVAMTU2OTQ1MTk1OTY1Mg%3D%3D&el=1_x_3&_esc=publicationCoverPdf
A causal framework for explaining the predictions of black-box sequence-to-sequence models （這篇滿符合的，可放）



分析 NN （偏第二段）
LEARNING HOW TO EXPLAIN NEURAL NETWORKS: PATTERNNET AND PATTERNATTRIBUTION
可能也是第二段
Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)
Visualizing and Measuring the Geometry of BERT
\end{comment}


