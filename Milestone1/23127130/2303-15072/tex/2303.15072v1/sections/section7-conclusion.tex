\section{Conclusion}
%Normally for machines to achieve a certain task such as  control of writing, one needs to design some specific model (e.g. manipulate network structure or learning objectives) to ensure the important information can be learned and used. 
To perform fine-grained control on natural text generation, usually one would anticipate a more complicated structure such as attention-based or memory-based \cite{shen2013general,weston2014memory,sukhbaatar2015end} networks. What originally surprised us is that the vanilla Seq2Seq model can still achieve token positioning simply by learning from sufficient amount of training examples with control signals. The results from this paper show that a recurrent network with encoder-decoder structure could be much more powerful than one originally expected. Through the process of training, the neurons gradually developed their own capabilities to perform different functions such as storing, triggering, counting, etc. In the next stage, we will focus on uncovering the mystery behind the training process to learn how such dynamic neuron behavior can be trained with given samples.  % It automatically learns several special skills such as counting down and triggering using implicit information provided in the controls. This paper describes a very unique work to analyze the recurrent neural network down to the neuron level, which is by no means a trivial task as one needs to carefully trace how different neurons change through time and how one affects the other. In the next stage, we will focus on uncovering the mystery behind the training process to learn how such dynamic neuron behavior can be trained through time. 
%We have discovered other types of the control signals that is learnable in a Seq2Seq model, such as the one determines the position of a specific token and another controlling the character-level length of each outputting words.  In the next stage, we will focus on uncovering the mystery behind the training process to learn how such dynamic neuron behavior can be trained through time.








%\subsection{Visualization of samples}
%After training, we compare different control signals with the same previous sequence to see how output sequences vary. The output for 5 different control signals are shownin table \ref{Examples} and the hidden state of encoder and decoder are shown in Figure \ref{3d-embedding}.



%\subsection{Generalization}
%In this section, we generalize our assign word method to various case. For example, put the assign control signal in the front of input sentence, assign multiple words, and rhyme, POS tasks. Each task and its accuracy is describe in appendix. 



%Rhyme & 99.62 & \makecell{Length / rhyme controls\\
%		control the length and the rhyme \\of the output sentence}& 
%		\makecell{SOS it s a wonderful face EOS \\ IY1 NOE 7 NOR \\
%		 $\rightarrow$ SOS and it means something \\ special to me EOS \\
%	     SOS i m your song EOS NG NOE 10 NOR \\
%		 $\rightarrow$ SOS play me time and time again and \\ make me strong EOS} \\
%		\midrule