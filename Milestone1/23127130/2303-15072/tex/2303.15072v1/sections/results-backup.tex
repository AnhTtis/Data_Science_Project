\begin{comment}

\section{Experiment results}\label{section4}

In this section, we describe how we apply the neuron identification strategy to identify these four neuron sets and answer the three research questions. Because each token has their own functional neurons, we conducted experiments on the 50 most common tokens, i.g. $i$, $the$, or $you$, (48\% of the sample) and also the most common position (75\% of the sample), from 1 to 13, on each dataset. The detailed accuracy are recorded in Appendix C. Also, we use $S_{store,A}$, $S_{count,A}$, $S_{trig,A}$, and, $S_{out,A}$ to indicate the storing, counting, triggering and outputting neurons for token A at hidden state t.

%We take the assigned token “A” and the position “T” as an example (output A at time step T). In addition, noting that every token in the dataset has their own important neuron set, %For example, token A use neurons {1, 3, 5} as its storing neuron while token B use neurons {2, 4, 6} as its storing neuron. 
%so executing three-steps strategy one time can only find out the important neurons for only one token. Therefore,  


\begin{figure*}
    \centering
    \includegraphics[width = 0.9\textwidth]{figures/pca_eating.png}
    \caption{PCA on $S_{store,tok}$ for ten common tokens from encoder to decoder. We select 500 sample for each common word with $T$ = 5 (5000 samples in total), and perform the same PCA transformation on their $S_{store}$. It implies that model remains token information unchanged from $t$ = \emph{Token} until triggering other neurons at $t$ = 4.}
    \label{fig:pca_word}
\end{figure*}

\subsection{How does the model store the $token$ information?}
The first step to answer this question is to find out the storing neurons. 
For every specified token A and position T, we label samples that "\emph{output A at position T}” as 0 and label samples that “\emph{output token other than A at position T}” as 1 to train the auxiliary classifier. Following the neuron identification strategy, there are about 15\%$\sim$25\% neurons belonging to $S_{store,A}$. In addition, we discover an interesting fact that neuron index in $S_{store,A}$ for t=1, t=2, ..., until t=T-1 are almost the same and the neuron values remain largely unchanged. To visualize the situation, we plot the values of top ten neurons in $S_{store,A,t=T-1}$ (they also appears at almost other steps $t < T - 1$) in Figure \ref{store_trend}. One can discover that many of them keeping unchanged from $t = -1$ (\emph{Token} signal first appears in the encoder) to $t = T - 1 = 5$ (one step before the model outputs the token). In addition, we also compare the storing neuron for top ten common tokens in Figure \ref{S_store}. We plot the PCA diagram for all storing neurons with the same dimension reduction transformation in each sub-figure and it also shows that storing neurons for different tokens truly stay at their own positions from $t = -1$ to $T − 1$.
The last step for this question is to verify two things. Firstly,  $S_{store,A}$ is necessary for model to output the token. Therefore, we replace the values of $S_{store,A}$ at t = T − 1 with zero, and found the token accuracy drops significantly although about 80\% of neurons remaining the same. Secondly, we need to verify that model use the same neuron index and even the same value to store one important information passing through the entire encoder and decoder. Hence, we then replace values of the storing neurons by their previous states, and find that accuracy remains respectfully high. The average accuracy of two datasets is shown in Table \ref{table verification}.




\begin{figure}[h]
    \begin{center}
    
    \begin{minipage}[b]{0.43\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/store.png}
        \caption{$S_{store,tok}$ values from encoder to decoder. We plot the hidden states of ten $S_{store,tok=i}$ neurons with \emph{Pos.} = 5. Their values remain unchanged from t = \emph{Token} until 4, when it is ready to trigger $S_{fc,tok}$.}
        \label{store_trend}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.53\textwidth}
         \centering
         \includegraphics[width=0.75\textwidth]{figures/word_encoder_cd_with_label_2.png}
         \caption{Mean values of $S_{cd}$ through time step with different assigned $T$. (Some neurons are flipped through all steps for visualization convenience.) 
         The values with different T scatter when \emph{Pos.} is provided and reach the highest value in order according to T.}
         \label{cd}
    \end{minipage}%
    \end{center}
    
    \label{POS_10}
\end{figure}

\subsection{How does the model use $position$ signal to know when to output?}
To answer this question, we turn to take all samples that output A at any T, and give the classification label as “T - t”. For example, the hidden state at step t = 3 with T = 5 and t = 4 with T = 6 share the same label = 2, because both of them have two steps left to output the token. Applying the same strategy, there are about 20\% neurons belongs to $S_{cd,A}$ after filtering. Then, the next question is: how does $S_{cd,A}$ count? %Similarly, we plot the real values of the hidden state of the top ten counting neurons in Figure \ref{}. 
We first find out that the values of the counting neurons increase or decrease gradually through time step. Next, we flip (multiply with -1) all neurons whose values are decreasing ($S_{cd,A,t=T-2} < S_{cd,A,t=1}$), then take the average of all counting neurons. The result is shown as Figure \ref{cd}. Each different colored line represents a different target position T. When the Position signal appears at t = 0 in the encoder, the values of counting neurons start to spread out. Then, in the next few steps, counting neurons jointly behave as a incremental function that gradually reaches the optimal at t = T - 1. 
The last step is to verify $S_{cd,A}$ that it can indicates how many steps it should takes to reach the target positioning. 
We replace the value of $S_ {cd, A} $ at t with the value of t − 1 (the previous stage), and calculate how much data will output the assigned token for one step delay.
We show the results of t = 2, 3, and 4 in table \ref{table verification}. The average accuracy of all common tokens is high reveals that the $S_{cd,A}$ neuron indeed performs as expected.


\begin{table}[h!]
	\centering
	\label{table verification}
	\caption{Verification on $S_{trig,tok}$. For each token with $T$ = 6, we substitute the value $S_{trig,tok}$ at $t = T-1$ to 1 to 4 steps earlier and verify that the token can be outputted also earlier. }
	
	\begin{tabular}{c|cccc|cc|ccc}
		\toprule
		Neuron set & \multicolumn{4}{c|}{Trigger} & \multicolumn{2}{c|}{Store} & \multicolumn{3}{c}{Count down}\\
		Accuracy (\%) & 1 step & 2 steps & 3 steps & 4 steps & Zero & Previous & t = 2 & t = 3 & t = 4 \\
		\midrule
		Lyrics Freak & 99.8 & 96.4 & 91.2 & 84.1  & 10.7 & 87.9 & 99.9 & 99.9 & 99.3 \\
		Gutenberg & 94.2 & 93.0 & 91.4 & 90.4 & 20.0 & 83.3 & 98.9 & 99.3 & 99.5 \\
		Ideal situation & high & high & high & high & low & high & high & high & high\\
	\bottomrule
	\end{tabular}
	
	\label{S_trigger}
\end{table}


\subsection{How does the model combine $token$ and $position$ information to output? }

To answer the third question, we need to further introduce the last two important neuron sets, triggering and outputting neuron. 

$\boldsymbol{S_{trig,A}}$: $S_{trig,A}$ is the neuron set triggering the model to output the token exactly at the next step. To train the auxiliary classifier, we label $h_{t=T−1}$ as 0 and $h_t$ with every t < T −1 as 1, for each token A and position T. There are about 15\% ∼ 25\% neurons belonging to $S_{trig,A}$ after filtering. The next step is to verify $S_{trig,A}$. We replace the value of $S_{trig,A}$ at $h_{t=T-1}$ to any time t, with $0 <= t < T$, and check whether the model outputs the target token exactly at the next step. The table \ref{table verification} shows the average accuracy that we perform experiments on 50 common tokens and original target position = 6 on each dataset. We replace the value of trigger neuron at t = 5 (since $T - 1 = 6 - 1 = 5$) to t = 1, 2, 3 and 4 respectively, and the model outputs the tokens immediately at these four steps with very high probability. Actually, $S_{trig,A}$ can also be seen as the last step of the counting neuron.

$\boldsymbol{S_{out,A}}$: The outputting neuron is a set of neuron causing the fully connected layer to output the target token at t = T. Back to the model structure in Figure \ref{structure}, when the model needs to output token A at position T, the probability of outputting token A should be the largest. Besides, this probability is proportional to $h_{t=T} W_{tok=A}$. However, only a small sets (about 20\%) of neuron in $h_{t=T}$ play most of the effects to influence the values of $h_{t=T} W_{A}$ and they tends to close to -1 or +1 because the range of GRU hidden states is from -1 to +1. We define this small set of neurons as outputting neuron.
To find out outputting neuron, one can apply the strategy described previously, or simply find the neurons with the largest $h_{t=T} W_{A}$, as $W_{A}$ represents the learned fully connected weights connected to target token A.
The verification process goes as below: we zero-out the values of $S_{out,A}$ and remain other 80\% neurons on $h_{t=T}$ before feeding into the fully connected layer, and the mean accuracy of token positioning dropped to 0.0\%. On the other hand, we keep the values of neurons belongs to $S_{out,A}$ and zero-out others 80\% of neurons. The accuracy remains as high as 99.9\%. This indicate that $S_{out,A}$ in $h_{t=T}$ is critical to the output of the token.

%\textbf{Neuron interaction:} 
% 把之前的section 5 移一小段進來解釋第三個問題
% \section{Appendix}
\textbf{Interaction between Functional Neurons and Gates inside GRU}

After realizing the validity of four functional neurons, we go deep into the formulation of GRU, including each gate and its corresponding weight.
% After the model encoded the word information with storing neuron and the position information with counting neuron, it raised another question to  solve: what is the interaction between storing and counting neuron to trigger model output the right word at the right time-step ? 
In the following, we combine the mathematical and the empirical perspective to investigate the relationship between functional neurons and gates. 

%\newpage

Recall the mathematical formulation of GRU cell:

% \begin{small}
\begin{equation*}
% \begin{aligned}
GRU\begin{cases}
    &z_t = \sigma(W_{z}x_t + U_{z}h_{t-1} + b_z)  \\
    &r_t = \sigma(W_{r}x_t + U_{r}h_{t-1} + b_r) \\
    &\widetilde{h_t} = \text{tanh}(W_{h}x_t + U_{h}(r_t \odot h_{t-1}) + b_h)  \\ 
    &h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \widetilde{h}_t\\
    \end{cases} \quad
GRU^{'}\begin{cases}
    &z_t = \sigma(U_{z}h_{t-1} + b_z)  \\
    &r_t = \sigma(U_{r}h_{t-1} + b_r) \\
    &\widetilde{h_t} = \text{tanh}(U_{h}(r_t \odot h_{t-1}) + b_h)  \\ 
    &h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \widetilde{h}_t
    \end{cases}
% \begin{aligned}
%     &z_t = \sigma(W_{z}x_t + U_{z}h_{t-1} + b_z)  \\
%     &r_t = \sigma(W_{r}x_t + U_{r}h_{t-1} + b_r) \\
%     &\widetilde{h_t} = \text{tanh}(W_{h}x_t + U_{h}(r_t \odot h_{t-1}) + b_h)  \\ 
%     &h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \widetilde{h}_t\\
% \end{aligned}
\label{GRU_Formula}
\end{equation*}
% \end{small}
% \caption{The original formulation of GRU and its simplified version}


with $z_t, r_t, h_t, \widetilde{h}_t$ as the \emph{update gate}, \emph{reset gate}, \emph{activation}, and \emph{candidate activation}.


One of the side-findings reveals that, as an GRU decoder takes the previous state $h_{t-1}$ and the produced token  $x_{t}$ as two inputs, we found that $x_{t}$ contribute almost nothing to the token-positioning task. We confirm this hypothesis by simply replacing $x_{t}$ with a random token and the accuracy remains 98.0\%. Since the random replacement experiment shows that $x_t$ has very few influence on $z_t, r_t, h_t$ that can be neglected, we simplify the formulation of GRU cell and noted as $GRU^{'}$. Then we can simply view the operation inside the $\sigma(\cdot)$ function as a linear transformation of $h_{t-1}$ to $z_t, r_t$ or $h_t$. Therefore, the weight $U_z, U_r, U_h$ are indeed the feature importance. 
Based on these simplifications, we can visualize the neuron interaction as in Figure \ref{interaction}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/appendix_trig_vis.png}
    \caption{Visualization of neurons interaction. }
    \label{interaction}
\end{figure}

% \begin{small}
% \begin{equation}
% \begin{aligned}
%     &z_t = \sigma(U_{z}h_{t-1} + b_z)  \\
%     &r_t = \sigma(U_{r}h_{t-1} + b_r) \\
%     &\widetilde{h_t} = \text{tanh}(U_{h}(r_t \odot h_{t-1}) + b_h)  \\ 
%     &h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \widetilde{h}_t
% \end{aligned}
% \label{GRU}
% \end{equation}
% \end{small}

In order to verify our hypothesis that triggering neurons unblock the storing neurons to affect the outputting neurons, we observe the transition weight in $U_z, U_r, U_h$ from triggering neurons to storing neurons or outputting neurons. The higher the absolute value of transition weight, the more important it is. Following the formulation above,  for \emph{z gate}, the transition weight from triggering neuron to outputting neurons should be low in order to update the neuron value in $h_{t-1}$ (in Fig. \ref{interaction}(a)), and for the \emph{r gate}, the transition weight from trigger neuron to storing neuron should be high in order to make $h_{t-1}$ effective to $\widetilde{h_t}$  or it will be masked.(in Fig. \ref{interaction}(b))  After the value in each gate are determined, the transition weight in $U_h$ should be extreme value in order to trigger the outputting neurons to reach extreme value as well.(in Fig. \ref{interaction}(c)) Additionally, we apply integrated gradient to check the impact from triggering neuron to outputting neurons, which shows the same result as directly analyzing the inner operation of GRU cell. Statistics are listed in Figure X and Table X.





% \begin{table}[h]
% 	\centering
% 	\begin{tabular}{ccccc}
% 		\toprule
% 		Neurons set & Z gate & R gate & H gate \\
% 		\midrule
% 		$h_{T-1, store}$ & 7.9E-3 & 7.3E-3 & 6.7E-3\\
% 		$h_{T-1, trig}$ & 1.3E-2 & 9.8E-3 & 1.6E-3 \\
% 		Whole $h_{T-1}$ & 4.0E-3 & 5.1E-3  & 2.4E-3\\
% 		\bottomrule
		
% 	\end{tabular}
	
% 	\caption{Different $S_{Imp}$ importance through different gates by calculating integrated gradient score. The first and second columns show that $h_{T-1, trig}$ is the most important through z and r gate, whereas the third column shows that $h_{T-1, store}$ is the most important through h gate.}
% 	\label{IG_score}
% \end{table}


% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.48\textwidth]{figures/appendix_z_impact.png}
%   \caption{Impact of triggering neurons to decrease \emph{z gate} value of outputting neurons. X-axis are sixteen $S_{fc,tok}$ neurons. Y-axis represents the impact from $h_{T-1, trig}$ (blue bars) and other neurons (red bars). Most blue bars have obvious negative values and it implies that it is $h_{T-1, trig}$ to increase $z_{T, fc}$.}
%   \label{z_impact}
% \end{figure}




\begin{figure}[h]
    \begin{center}
    
    \begin{minipage}[b]{0.43\textwidth}
        \centering
        	\begin{tabular}{ccccc}
    		\toprule
    		Neurons set & Z gate & R gate & H gate \\
    		\midrule
    		$h_{T-1, store}$ & 7.9E-3 & 7.3E-3 & 6.7E-3\\
    		$h_{T-1, trig}$ & 1.3E-2 & 9.8E-3 & 1.6E-3 \\
    		Whole $h_{T-1}$ & 4.0E-3 & 5.1E-3  & 2.4E-3\\
    		\bottomrule
        		
        	\end{tabular}
        	
        	\caption{Different $S_{Imp}$ importance through different gates by calculating integrated gradient score. The first and second columns show that $h_{T-1, trig}$ is the most important through z and r gate, whereas the third column shows that $h_{T-1, store}$ is the most important through h gate.}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.53\textwidth}
         \centering
         \includegraphics[width=0.6\textwidth]{figures/appendix_z_impact.png}
          \caption{Impact of triggering neurons to decrease \emph{z gate} value of outputting neurons. X-axis are sixteen $S_{fc,tok}$ neurons. Y-axis represents the impact from $h_{T-1, trig}$ (blue bars) and other neurons (red bars). Most blue bars have obvious negative values and it implies that it is $h_{T-1, trig}$ to increase $z_{T, fc}$.}
          \label{z_impact}
        
    \end{minipage}%
    \end{center}
    
    \label{POS_10}
\end{figure}

\end{comment}









