
\section{Summary of Findings}

%Firstly, we explain the mechanism of 4 functional neurons found in Seq2Seq model,  including \emph{storing}, \emph{counting}, \emph{triggering} and \emph{outputting}, to provide an overview about our findings as well as the hypothesis on how they interact to make Seq2Seq output the right token at the right time-step. Experiment details are discussed in Section X to confirm our hypothesis.

For each token, model has its own four neuron sets to help to keep the necessary information and output the token at the right position. There may exist overlap on each neuron sets since neurons can be multi-functional. %After realizing the function of four neuron sets, following the hypothesis, we can answer all the research questions mentioned in Section X to yield a better understanding on Seq2Seq model. 
Figure \ref{flow_chart} is the conceptual graph which shows when do those important neuron sets exist and how they affect each others.

\begin{figure*}%[h]
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \includegraphics[width=0.9\textwidth]{figures/NeuronInteraction.png}
  \caption{The whole inference process of token positioning task and the neuron interaction. With the interaction of the four types of neurons, we are capable of answering all the research questions mentioned in Section 1.}
  %\label{flow_chart}
\end{figure*}

\textbf{Storing Neurons}  
For each token, there is a small set ($15\% \sim 25\%$) of neurons, defined as $S_{store,tok}$, that jointly \emph{storing} token information. $S_{store,tok}$ is shared among each token with different positions, and they are established during the encoder stage when the token is provided. $S_{store,tok}$ remains largely unchanged until $h_{t=T-1}$, one step before the assigned position is reached in the decoder.

%Storing the information about the specified token. This type of neurons are eager to activate \emph{outputting neurons} before the specified time-step but block by the gate inside the Seq2Seq model.


\textbf{Counting Neurons} For each token, there is a small set (about $20\%$) of neurons, defined as $S_{cd,tok}$, whose values keep increasing, or decreasing, with $t$ from t = 0 to T - 2, and eventually reaches the maximum/minimum to activate $S_{trig,tok}$ at $t = T-1$.
Therefore, it can indicate how many time step are left wo output the specified token.

%Indicating how many time-step left to output the specified token. Note that we found the counting information is not stored as real values, instead they jointly behave as a incremental function that gradually reaches the optimal value before specified time-step.

 %$S_{cd,tok}$ is also invariant with positions and act as a kind of counter. %They reach the highest values, usually $>0.75$ or $<-0.75$, 
%At $t = T - 2$, the counting neuron reaches the optimal value to activate $S_{trig,tok}$.

\textbf{Triggering Neurons} 
For each token (regardless of the position), there is a small set ($15\% \sim 25\%$) of neurons, defined as $S_{trig,tok}$, that would become activated by counting neuron at $t = T-1$. Those neurons are responsible to control when the storing neuron can affect the outputting neuron. As a result, the model can output the assigned token at exactly the next time step when $S_{trig,tok}$ is activated at $T-1$. 

%After \emph{counting neurons} reach optimal value, the \emph{trigger neurons} will be activated. These neurons unblock the \emph{storing neurons} by changing the gate state to enable \emph{storing neurons} the ability to affect the \emph{outputting neurons} to cause model output the specified token.

\textbf{Outputting Neurons} For each token, there is a small set of neurons (about $20\%$) which cause the fully connected layer to output the target token. At $t$ = $T$, $S_{store,tok}$ influences the model through h gate, and $S_{trig,tok}$ influences the model by z and r gate, to activate the $S_{out,tok}$ to its extreme value. Next, since $S_{fc,tok}$ is connected to the larger weights of tokens with the assigned token, the probability of this token increases at $t = T$.

%Neurons that directly contribute to the token probability after the decoder output layer and cause the model output the specified token.

%For each token, it has its own four neuron sets and there may exist overlap on each neuron sets since neurons can be multi-functional. After realizing the function of four neuron sets, following the hypothesis, we can answer all the research questions mentioned in Section X to yield a better understanding on Seq2Seq model.

\textbf{Length-control and rhyme-control tasks}
Following shows that the other two sub-tasks, length-control and rhyme-control, can achieve similar training performance and analyzing results as the main task we discuss in section 4. When the target token is always set to "EOS", the model can learn to control the length of the output. Using the same data set, model structure and training process mentioned in Section 2, the accuracy reaches 99.xx. The analysis results are also similar as we record in Appendix C. The rhyme-control task also has similar situation with training accuracy = xx, except we change the assigned token to a specific rhyme. The rhyme dataset can generated as following: get the rhyme of the last word in the output  as "token" and get the length of the output as "position". 

\begin{comment}
\subsection{Related Tasks}

Following shows that the other two sub-tasks can achieve similar training performance and analyzing results as the main task we discuss in section 4.

\textbf{Length-control task: Always set token to "EOS".} 
When the target token is always set to "EOS", the model can learn to control the length of output sequence. Using this dataset with the same model structure and training process as mentioned in section 2, the accuracy reaches 99.xx. In addition, with the similar analysis steps in section 3 and 4, one can also find out the counting neurons and output neurons. The related figures and accuracy is at Appendix C. 
    
\textbf{Rhyme-control task: Set token corresponds to a cluster of word in the dictionary.}
    In lyric or poem generation, it requires the output sequence to end with a word with a specific rhyme. For this task, one can generate the training dataset by two steps: (1) Get the rhyme of the last word in the output sequence as "token". (2) Get the length of the output sequence as "position". Then concatenate these two control signals to the input sequence, as the example in the section 1. And in the inference process, concatenate arbitrary "rhyme" and "length" to any sequence to get the correct rhyme. In our dataset, we have 35 rhyme tokens and the model can get accuracy to xx. Following the similar analysis steps, we can also find out the important neuron sets such as storing neuron (of each rhyme) and the counting neurons as in Appendix C.


\end{comment}


%\textbf{Hypothesis} Summarize the neuron interaction to answer the research questions. [Add Later]

% Therefore, the inference procedure can be summarized as :
% \begin{enumerate}
%     \item Encoding the \emph{token information} in the \emph{storing neuron}
%     \item Encoding the \emph{position information} in the \emph{counting neuron}
%     \item \emph{Counting neuron} activate \emph{triggering neuron}.
%     \item \emph{Triggering neuron} activate \emph{outputting neuron} to output specified token.
% \end{enumerate}
% With the interaction of the four types of neurons, we are capable of answering all the research questions mentioned in Section 1.