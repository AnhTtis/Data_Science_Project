\section{List of Findings}\label{section4}

Here we report several scientific findings to better understand the mechanism of a GRU-based Seq2Seq model. %In this section, we describe how we find out functional neuron sets and how they operate to accomplish a more complicated task. 
To analyze the functional neurons, we exploit the 3-stage strategies described in the previous section on 50 most common tokens (occupying 48\% of the data) and common positions from 1 to 13 (occupying 75\% of the data). The detailed reports are documented in Appendix C. Below we summarize the key findings. %Also, we use $S_{store,A}$, $S_{count,A}$, $S_{trig,A}$, and, $S_{out,A}$ to indicate the storing, counting, triggering and outputting neurons for token A at hidden state t.

%We take the assigned token “A” and the position “T” as an example (output A at time step T). In addition, noting that every token in the dataset has their own important neuron set, %For example, token A use neurons {1, 3, 5} as its storing neuron while token B use neurons {2, 4, 6} as its storing neuron. 
%so executing three-steps strategy one time can only find out the important neurons for only one token. Therefore,  


\textbf{Finding 1.} For each token, regardless of its position, there is a small group of fixed neurons (15\% $\sim$ 25\%) jointly storing \emph{the token-specific information}. We call them \emph{storing neurons}. 
The storing neurons remain mostly unchanged from $t = -1$ (when the token signal appears in the input) to $T - 1$ (one step before the target position).

%in each step are almost the same group of neurons, and most neurons always maintain the same value.}

\begin{figure*}
    \centering
    \includegraphics[width = 0.9 \textwidth]{figures/pca_eating.png}
    \caption{PCA plot of the storing neurons from ten most common tokens (500 samples each). From $t = -1$ to $T - 1$, the value of storing neurons hardly changes.}
    \label{S_store}
\end{figure*}

Our hypothesis is that before the target output time T, there exists a set of neurons who store the information about this token.% and  We believe that if the model can output the correct token (i.g. $A$) at the correct position, T, in the decoder, it means that each hidden state before T must have a part of neurons that are responsible for storing the token information.
To verify this hypothesis, we first label the hidden states $h_t$ of samples that generate this token as positive and samples that generate other tokens as negative. The test is conducted on all 50 tokens and the result shows more than 95\% accuracy in classification.
Then the technique of feature selection is exploited to identify a subset of storing neurons in every $h_t$. We discover that the storing neurons (as well as their values) remain mostly unchanged across time stamps from $t=0$ to $t=T-1$. This implies that Seq2Seq model tends to use the same group of neurons with the same value to save a piece of information that needs to be stored for a long time. Figure \ref{S_store} shows the PCA projections of ten common tokens w.r.t. the progress of time to confirm the finding. Through visualization, we can show that the neurons for each token remains roughly the same until $t=T-1$. % Their values changed more significantly at $t=T$, implying that the model no more need to store such information. 
%We visualize the values of top ten storing neurons for token "i" in Figure \ref{store_trend}. One can discover that many of them keeping unchanged from $t = -1$ (\emph{Token} signal first appears in the \emph{encoder}) to $t = T - 1 = 5$ (one step before the model outputs the token). 
%This situation can be visualized in . 
%We apply PCA transformation on all storing neurons for ten different tokens as shown in Figure \ref{S_store}. It can be found that when $t = -1$, that is, when the token signal appears, all the storing neuron values will be scattered according to different given tokens. And then, their positions hardly change until $t = T - 1 = 4$ (one step before outputting this token).
%The first step to answer this question is to find out the storing neurons. For each specified token  $A$ and position $T$, we label samples that "\emph{output $A$ at position $T$}” as 0, label samples that “\emph{output token other than $A$}” as 1, and train the diagnostic classifier for every time-step $t$ , $t < T$. Following the neuron identification strategy, there are about 15\%$\sim$25\% neurons belonging to $S_{store,A}$. In addition, we discover an interesting fact that neuron index in $S_{store,A}$ for $t = 1$, $t = 2$, ..., until $t = T-1$ are almost the same and the neuron values remain largely unchanged. To visualize the situation, we plot the values of top ten neurons in $S_{store,A,t=T-1}$ (they also appears at almost other steps $t < T - 1$) in Figure \ref{store_trend}. One can discover that many of them keeping unchanged from $t = -1$ (\emph{Token} signal first appears in the encoder) to $t = T - 1 = 5$ (one step before the model outputs the  token). In addition, we also compare the storing neuron for top ten common tokens in Figure \ref{S_store}. We plot the PCA diagram for all storing neurons with the same dimension reduction transformation in each sub-figure and it also shows that storing neurons for different tokens truly stay at their own positions from $t = -1$ to $T − 1$.
In the verification stage, we first verify that the storing neurons are necessary for model to output the correct token. Therefore, we replace the values of storing neurons at $t = T - 1$ with zero, and observe the accuracy drops significantly to $10.7\%$ and $20.0\%$, as shown in Table 1.
% although about 80\% of neurons remaining the same. Secondly, we want to confirm that model use the same neuron index with the same value to store an important information passing through the entire encoder and decoder. Hence, 
We also replace values of the storing neurons before $t = T - 1$ by their values in the previous time stamp, and observe the accuracy remains high (i.e. 87.9\% and 83.3\%), confirming that the values of the storing neurons remain roughly unchanged during the process.% Table \ref{table:table_verification}. % \ref{table verification}.

\begin{comment}
\begin{figure}[h]
    \begin{center}
    
    \begin{minipage}[b]{0.43\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/store.png}
        \caption{$S_{store,tok}$ values from encoder to decoder. We plot the hidden states of ten $S_{store,tok=i}$ neurons with \emph{Pos.} = 5. Their values remain unchanged from t = \emph{Token} until 4, when it is ready to trigger $S_{fc,tok}$.}
        \label{store_trend}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.53\textwidth}
         \centering
         \includegraphics[width=0.75\textwidth]{figures/word_encoder_cd_with_label_2.png}
         \caption{Mean values of $S_{cd}$ through time step with different assigned $T$. (Some neurons are flipped through all steps for visualization convenience.) 
         The values with different T scatter when \emph{Pos.} is provided and reach the highest value in order according to T.}
         \label{cd}
    \end{minipage}%
    \end{center}
    
    \label{POS_10}
\end{figure}
\end{comment}





\textbf{Finding 2.} Associated with each token, there is a small group of neurons (about 20\%) whose values keep increasing (or keep decreasing) with $t$ from $t = 0$ to $T - 1$. We call them \emph{counting neurons.} %It can indicate how many time steps are left to output the specified token.}

\begin{figure*}[h]
    \centering
    \includegraphics[width = 0.55\textwidth]{figures/word_encoder_cd_with_label_2.png}
    \caption{Average values of counting neurons with different assigned $T$. %(Partial neuron values are flipped for visualization convenience.) 
    Counting neurons with different $T$ scatter after seeing the \emph{position signal}, and they need different number of steps to reach the same end point.}
    \label{cd}
\end{figure*}



%\textit{For each token, there is a small set (about $20\%$) of neurons, defined as $S_{cd,tok}$, whose values keep increasing, or decreasing, with $t$ from t = 0 to T - 2, and eventually reaches the maximum/minimum to activate $S_{trig,tok}$ at $t = T-1$.
%Therefore, it can indicate how many time step are left to output the specified token.}
Here we assume that there are some neurons that shall function as \emph{counting down} toward the target position $T$. We first group all hidden states $h_t$ based on \emph{the number of steps before outputting (i.e. $T-t$)} as the labels. For example, the hidden state at step $t = 3$ given $T = 5$ and that of $t = 4$ given $T = 6$ share the same label $2$. Then we apply the classification and filtering strategy to identify a subset of neurons that are critical to the labels as counting neurons.
%there are about 20\% neurons belongs to $S_{cd,A}$ after filtering. Then, the next question is: how does $S_{cd,A}$ count? %Similarly, we plot the real values of the hidden state of the top ten counting neurons in Figure \ref{}. 
We then analyze the values of those counting neurons to understand how they can perform counting. The discovered behavior can be explained based on Figure \ref{cd}. When the position signal $T$ appears at the encoder at $t = 0$, the values of the counting neurons face significant drop. Furthermore, we find that the amount of dropping is positively correlated with the position $T$, meaning that the values move further toward negative when $T$ is larger. Then, as time step moves forward, the values of counting neurons slowly increase toward positive values. When reaching a relative high value, they will trigger another set of neurons (to be discussed in the later findings) to output the target token. Given different dropping range and mostly-constant increasing ratio, the counting neurons reach the optimal values after variable steps to achieve the function of counting (i.e. larger $T$ requires more steps to reach top). We find this phenomenon very unique and interesting. In hindsight, this seems to be a simple and elegant way for a memory-less, neuron-network-based model to perform counting. Given a well-trained decoder, achieving constant ratio increase of neuron values is not hard. Furthermore, as will be discussed in Section 5, the mechanism of GRU allows the activated counting neurons to trigger another set of neurons for firing the right token in the next step.  
%in a neuron is At the  as the way a Seq2Seq model performs counting is very different from  which is the mechanism for means they can trigger then Therefore, in the case of similar increasing rate and different distances, the counting neurons for longer $T$ needs more time to reach the same end point, so it can achieve the purpose of counting.
We also observe some neurons perform counting in the opposite manner (i.e. starting by jumping to a positive range and gradually decreasing the values), but the underlying process remains the same.
%We first find out that the values of the counting neurons increase or decrease gradually through time step. Next, we flip (multiply with -1) all neurons whose values are decreasing (value at $t = T - 2 < t = 1$) and take the average. The result is shown as Figure \ref{cd}. Each different colored line represents a different target position $T$. When the position signal appears at $t = 0$ in the encoder, the values of counting neurons start to spread out. Then, in the next few steps, counting neurons jointly behave as a incremental function that gradually reaches the optimal at $t = T - 1$. 
Finally we verify that counting neurons can really control the token position. We replace the values of counting neurons at $t$ with those of the previous stage ($t - k$) and keep the rest of neurons as is, and then verify there are 99\% chance the target token can be generated $k$ step later, as shown in the counting part in Table \ref{table:table_verification} given $k=1 $to $3$.  
%According to Table \ref{table:table_verification}, it is possible to reach 99\% accuracy in controlling the position of a token, meaning that we indeed found a set of neurons that perform counting down function. %accuracy of all common tokens is high reveals that the counting neuron indeed performs as expected. Noting that \emph{replacing with previous state} cause completely different results to the model between storing and counting neurons.
%\subsection{Triggering Neurons} 
\textbf{Finding 3.} For each token, there is a small group of neurons (15\% $\sim$ 25\%) whose values change dramatically from $t < T - 1$ to $t = T - 1$, and when the above happens, the target token will be output at $t=T$. We call them \emph{triggering neurons}. It can be used as a powerful indicator to tell the model to take action in the next step.

%In order for the model to truly identify the assigned position T, we think that there will be some neurons whose values are very different between $t < T - 1$ and $t = T - 1$ to provide the model time information.
To identify this type of neurons, we first train a classifier to separate $h_{t<T-1}$ from $h_{t=T-1}$, and then use feature selection and IG to select the subset of important neurons.
To verify the triggering neurons, we use their values at $t = T - 1$ to replace the same neurons at $t = T - k$ (with $k \geq 2$), and check whether the model can indeed output the assigned token earlier. Table \ref{table:table_verification} shows high accuracy while using the values of triggering neurons at t = 5 to overwrite the values at t = 1, 2, 3, and 4 respectively when T = 6. In the next section, we will further discuss why triggering neurons are required in the process. %, and the model can truly output the token earlier for 4, 3, 2, or 1 step. Conversely, if we randomly select a similar number of neurons and do the same operation, we cannot get the same result.

\begin{table}[h!]
	\centering
	\caption{The verification accuracy of the manipulation mentioned in Section 4.1, 4.2 and 4.3. The first two columns replace storing neurons with zeros or their previous state. The middle three columns show whether the output delay for $k$ steps when we replace the values of counting neurons at $t$ with those of the previous stage ($t - k$). The right four columns show whether the model can output the assigned token earlier when we use the triggering neurons to replace the earlier hidden state ($t - k$.} 
	\label{table:table_verification}
	\begin{tabular}{c|cc|ccc|cccc}
		\toprule
		Neuron set & \multicolumn{2}{c|}{Storing} & \multicolumn{3}{c|}{Counting} & \multicolumn{4}{c}{Triggering}\\
		Accuracy (\%) & Zero & Previous & k = 1 & k = 2 & k = 3 & k = 1 & k = 2 & k = 3 & k = 4 \\
		\midrule
		Lyrics Freak & 10.7 & 87.9 & 99.9 & 99.9 & 99.3 & 99.8 & 96.4 & 91.2 & 84.1 \\
		Gutenberg & 20.0 & 83.3 & 99.5 & 99.3 & 98.9 & 94.2 & 93.0 & 91.4 & 90.4 \\
		%(Expected) & low & high & high & high & high & high & high & high & high \\
	\bottomrule
	\end{tabular}
	
\end{table}

 





%\subsection{How does the model combine $token$ and $position$ information to output? }

%To answer the third question, we need to further introduce the last two important neuron sets, triggering and outputting neuron. 



\textbf{Finding 4.} For each token, there is a small group of neurons (approximately 20\%) that is activated at $t=T$, causing the fully connected layer to assign highest probability to this token. We call them \emph{outputting neurons}.

%but it can have a major influence on prob($ y_t$) and affect which word is output by the fully connected layer. We call them outputting neurons.} 

%\textit{For each token, there is a small group of neurons (approximately $20\%$) belongings to $S_{out,A}$. It can have the major influence on prob($y_t$) and cause the fully connected layer to output the target token.}
The Seq2Seq model typically output the token with the highest probability. This probability is accumulated by a set of $h_{t=T} W_{token=A}$, where $W_{A}$ is the learned and fixed weights connected to target token A. To identify the outputting neurons, one can simply select the neurons with the largest $h_{t=T} W_{A}$. We find that only a small set of neurons in $h_{t=T}$ contributes to the output, and their values tend to be close to -1 or +1 (note that the range of GRU hidden states is -1 to +1). %We define these neurons as outputting neurons.
%To find out outputting neuron, one can apply the strategy described previously, or simply find the neurons with the largest $h_{t=T} W_{A}$.
The verification process is designed as follows. We zero-out the values of outputting neurons and leave the rest of neurons on $h_{t=T}$ unchanged before feeding into the fully connected layer. It is observed that the accuracy of token positioning dropped to close to 0.0\%. In comparison, we zero-out all neurons (about 80\%) other than the outputting neurons, and find the accuracy remains as high as 99.9\%, confirming the function of these neurons.

%The process of token positioning

Figure \ref{flow_chart} is the conceptual graph to show how the neurons of different functions interact with each other to perform the token-positioning task. It can be summarized as below: 
(1) Storing neurons carry the token information (i.e. which to output) from $t = -1$ to $t = T - 1$. (2) The position information is carried by counting neurons starting from $t = 0$, and their values move piecewise linearly from $t = 0$ to $t = T - 1$. (3) Counting neurons reach the extreme values and activate the triggering neurons at $t=T-1$. (4) The triggering neurons and storing neurons then act together to fire outputting neurons at $t = T$, which then assigns target token with highest probability. 

%(3) Among them, some of the counting neurons (i.e. the triggering neurons), have a clear difference between $t < T - 1$ and $T - 1$. They can activate the storing neurons to transmit the token information to the outputting neurons at $t = T$. (4) And therefore, in the end, the outputting neurons can cause the FC layer to output the right token at $t = T$. 
In this process, there are several details that deserve special attention: (1) To output a target token at the right position, it implies that the model cannot output the token at other positions. Thus, the triggering neurons can also be regarded as playing a restraining role in the earlier stages. More details are shown in the next section. (2) Each token has its own four types of neuron sets but overlap may exist, as we discover that neurons can be multi-functional (e.g. play storing role before $t=T-1$ then shift to the outputting role at $t=T$).
%plays a role to  The model need not only storing neurons but also triggering neurons to output the cannot store the token information in the outputting neurons directly without the help of the storing neurons. It is because once the outputting neurons obtain the token information, it will output the token immediately. Therefore, the model needs to store the token information in the storing neurons until the correct time step arrives. (2) Whether the storing neurons can affect the outputting neurons is determined by the triggering neurons. It is because triggering neurons have a large difference between $t < T - 1$ and $t = T - 1$, so it makes the Seq2Seq model to control its output easier. We will explain deeper in the next paragraph. (3) Each token has its own four neuron sets and there may exist overlap, since neurons can be multi-functional.




\begin{figure*}%[h]
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \includegraphics[width=0.9\textwidth]{figures/NeuronInteraction.png}
  \caption{The interaction of four types of neurons for the token positioning task.}
  \label{flow_chart}
\end{figure*}









