\newpage
\vskip 0.075in%
%\centerline%
{\large\bf Broader Impact}%
\vspace{1ex}%

This paper is NOT about proposing an ML model to achieve a certain goal nor reveals some methods to make a model more understandable to humans. This paper tries to answer two questions: 1. How to perform neuron-level analysis on a recurrent Seq2Seq model and 2. What did we discover after conducting such analysis?
Our findings include different functionality of neurons as well as how they interact to achieve a relatively challenging task given the underlying design of a Seq2Seq model. Among them, we believe the most exciting part is the discovery of the counting-down mechanism. Counting is not trivial for a memory-less model like the one we studied. We discover that the model achieves this by initializing the counting neurons with extreme values (e.g. -0.9), and then gradually add constant values  until reaching the top to trigger the next action. The amount of counting can be adjusted by assigning different level of initialization.

We hope this paper sends some messages to the community:
1. The first message is that more efforts is needed to investigated in terms of understanding the underlying mechanism of recurrent encoder-decoder networks. Since its first appearance in 2014, neural-based encoder-decoder based model has evolved so rapidly that almost in every year's top-tier conferences fancier models are presented, each of them more sophisticated yet more powerful than its predecessors. Though it is tempting and rewarding to design new and more complex models to pursue higher benchmark, here we hope to send a message that more efforts should be spent on trying to understand why they are more powerful. From 2014 till now, there have been thousands of papers published using (or improving) Seq2Seq models, but, based on our related work study, only dozens of the papers emphasize on analyzing and understanding the underlying behavior.  

2. The second signal we would like to send is that an encoder-decoder based Seq2Seq model based on GRU (or other RNN cells) might be more powerful than originally expected. We have revealed the token positioning capability, but based on our current analysis advanced control (e.g. control the rhyme or POS of each token) is also possible. 
%2. Although the practical impact for this paper is limited as it does not advance the state-of-the-art of any particular AI applications. We believe it provides a rare case study to show that a simple recurrent neural-based model can indeed learn from weakly supervised signals as in the training data we did not specifically tell the model the purpose of the control signals, and thus they are simply treated as one of the tokens in the input sequence. 

3. Finally, this paper mainly focus on explaining the inference process. It remains a mystery to us how the model learns from training signal to develop different functionality. We believe understanding the training process of neurons might bring deeper insights to not just AI but also brain theory communities. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
One of the key broader impact this paper brings is that the research about AI 
We start from answering one simple question: is it possible to perform fine-grained control on a vanilla Seq2Seq model? 


This paper analyzes how gated recurrent unit can perform four basic functions (storing, counting, triggering and outputting) and explains how Seq2Seq model further combine these functions to achieve the token-positioning task.

We emphasizes that a basic model can be powerful enough to be used for many purposes. %Instead of redesigning a more complex model architecture for only one application, we redesign the dataset format to increase its functionality. 
Some people may concern that using such simple model is not the current trends. However, we believe that one model reaching 99\% in three applications at the same time, and another model reaching 99.99\% in one application (most people admire today), both of them have the necessity of their existence.

Moreover, gated recurrent unit is so widely-known and commonly-used neuron cell in machine learning nowadays. However, most people wonâ€™t carefully explore how GRU cell actually works when using it. In addition to breaking down the model to observe how the hidden states evolve in the decoder, this article also discusses how the gates of gru affects the entire inference procedure from a mathematical point of view, and therefore provide a theoretical basis.

On the other hand, the limitation of this work is that we only test our analytical methods on public datasets with common distribution. We did not test for extreme situations, such as extremely long sentences (paragraph). Besides, although it has been confirmed that the token-positioning task has several extensions, for example, given multiple sets of token-position control signals at a time, our current work is limited to explain clearly that each input sequence can only be assigned one pair of control signals. 

%We encourage subsequent people to explore further whether token-positioning has other applications based on we have analyzed that GRU has these four basic functions. We believe that these basic functions should be able to have other combinations to achieve other similar complex functions. 
We believe that the proposed basic functions should be able to have other combinations to achieve other similar complex functions and we encourage subsequent people to further explore on it. For example, by giving the POS (part-of-speech) of each word, the model can learn to output sequence with the assigned POS signals.

%\newpage
\vskip 0.075in%
%\centerline%
{\large\bf Impact Statement 2}%
\vspace{1ex}%

In this paper, we provide a neuron-level understanding on the Seq2Seq, indicating that the mechanism inside the model is supported by four kinds of neurons, storing, counting, triggering and outputting neurons. In addition to analyzing the function of neurons by visualizing the neuron dynamics, we provide a 3-step strategy to select the candidate functional neuron set and further verify their  effectiveness with token positioning as the downstream task.

Neuron network has been regarded as black-box for a long time and lots of research study in neuron-level emerged recently. In NLP field, Seq2Seq model has a wide variety of applications such as dialogue system and text generation. Such phenomenon motivates us to unbox the mechanism of Seq2Seq model. One of the impact of our work is that the 3-step strategy developed in this work is not limited to the token positioning task but can also be applied to other NLP task or other field such as computer vision(CV) with properly labeled data. Another impact beneficial for future research on network explaining is that our understanding on the mechanism of Seq2Seq model. Additionally, the four kinds of functional neurons can be introduced to provide idea about model design to ensure that there exists certain module corresponding to the the 4 basic functionality when resolving related tasks.  In application level, the token-positioning has much potential in text generation as mentioned in introduction section for acrostic poem, lyrics or rap generation which is more close to the general society.

Some may yield comparison between the vanilla Seq2Seq model and the Transformer-based method on performance. Besides, for longer sentence, the simpler model may fail to achieve the task while the complex one can. Indeed, vanilla Seq2Seq model or GRU is weaker than Transformer-based model. However, without the pursuit of perfection, we have provide theoretical explanation on the simpler model while there is no  neuron level research on transformer-based model.  We encourage future research to understand more complex model on both NLP and CV in neuron level since it provides us the most direct way to understanding the mechanism inside the neuron network although more challenge than high-levelly analyzing have been expected.

We have seen opportunities for on neuron level analysis to strengthen the understanding from the society. By Interpreting the mechanism inside the neuron network, the decision maker can trace the process of decision making inside model and provide high-level interpretation which is easier to be understood by the public. Improvement on unboxing the so-called black-box from research on such topic are exciting to be found in the future.
Questions
\end{comment}