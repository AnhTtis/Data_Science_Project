\documentclass{article}


%
%
%


%
%
\usepackage{arxiv_upload}

%
%
%


%
%


%
%

\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %

%
%
%
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\expectation}{\mathbb{E}}
\newcommand{\pretraining}{\mathcal{D}}
\newcommand{\downstream}{\mathcal{\tilde{D}}}
\newcommand{\delimetertoken}{``\backslash n"}
\newcommand{\lm}{f_{\theta}}
\newcommand{\argmax}{\text{argmax}}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

%
%
%

\newcommand{\yl}[1]{\textcolor{green}{[YL: #1]}}

\title{The Learnability of In-Context Learning}


%
%
%
%
%
%
%

\author{Noam Wies, Yoav Levine \& Amnon Shashua\\
	The Hebrew University of Jerusalem\\
	\texttt{\{noam.wies,yoav.levine,shashua\}@cs.huji.ac.il} \\
}


%


\begin{document}
	
	\maketitle
	
	\begin{abstract}%
		In-context learning is a surprising and important phenomenon that emerged when modern language models were scaled to billions of learned parameters. 
		Without modifying a large language model's weights, it can be tuned to perform various downstream natural language tasks simply by including concatenated training examples of these tasks in its input.
		Though disruptive for many practical applications of large language models, this emergent learning paradigm is not well understood from a theoretical perspective. In this paper, we propose a first-of-its-kind PAC based framework for in-context learnability, and use it to provide the first finite sample complexity results for the in-context learning setup.
		Our framework includes an initial pretraining phase, which fits a function to the pretraining distribution, and then a second in-context learning phase, which keeps this function constant and concatenates training examples of the downstream task in its input.
		We use our framework in order to prove that, under mild assumptions, when the pretraining distribution is a mixture of latent tasks (a model often considered for natural language pretraining), these tasks can be \textit{efficiently} learned via in-context learning, even though the model's weights are unchanged and the input significantly diverges from the pretraining distribution.
		Our theoretical analysis reveals that in this setting, in-context learning is more about identifying the task than about learning it, a result which is in line with a series of recent empirical findings. 
		We hope that the in-context learnability framework presented in this paper will facilitate future progress towards a deeper understanding of this important new learning paradigm.
		%
	\end{abstract}
	
	\section{Introduction}
	
	The practice of pretraining language models (LMs) over massive general purpose text corpora has revolutionized natural language processing in recent years~\citep{radford2019language,devlin2018bert,brown2020language}.
	After an LM has been pretrained, the common approach for applying it to a specific downstream natural language task is to further train it on task-specific data, in a procedure referred to as fine-tuning~\citep{howard-ruder-2018-universal,radford2018improving,devlin2018bert}. 
	In their influential GPT3 paper, ~\citet{brown2020language} showed that a non-trivial alternative to fine-tuning emerges when the LM is large enough: 
	an LM can be specialized to a downstream natural language task by simply receiving in its input a string composed of concatenated training examples of this task. Importantly, while the LM’s weights are \emph{unchanged} in this procedure, some form of learning evidently takes place; the performance on the downstream task was shown to significantly improve with the number of concatenated training examples, for a disparate variety of natural language tasks. 
	
	This phenomenon, referred to as \emph{in-context learning}, has had a profound practical impact on the applicability of large LMs: one does not need to have any access to the model weights in order to specialize the model for a certain task. Instead, a string of training examples provided even via API access to the model is enough, and often not many examples are required~\citep{brown2020language}. However, despite its growing popularity in a multitude of use-cases~\citep{bommasani2021opportunities},
	the reasons for the effectiveness of in-context learning in pretrained LMs are not well understood from a theoretical perspective. 
	In particular, this new learning paradigm still lacks a formal definition of learning. 
	
	In this paper, we propose a PAC learning~\citep{valiant1984theory} definition for in-context learning, along with the first finite sample-complexity learning results for in-context learning. 
	Our results shed light on the mysterious question of why in-context learning works even though a string of concatenated input-output pairs does not resemble the natural distribution of pretraining examples.
	Our framework is based on an interpretation of pretraining as unsupervised multi-task learning of many natural language tasks, which dates back to the GPT2 paper~\citep{radford2019language}, one of the instigators of the LM pretraining era.
	This view, which has since been widely adopted, was supported by experiments that revealed non-trivial zero-shot capabilities of pretrained LMs on a variety of natural language tasks. 
	In other words, an LM pretrained only to maximize the likelihood of unlabeled text from the web, was able to perform reasonably well on a variety of downstream natural language tasks {without learning from any additional training examples post pretraining}, implying that many different natural tasks are directly learned during pretraining.
	
	Following this interpretation, we prove our in-context learning results in a setting in which the pretraining distribution is a mixture of downstream tasks. Importantly, the pretraining examples are not explicitly associated with downstream tasks, but rather they are drawn from a mixture of tasks and the association between examples and tasks is latent.
	%
	%
	We prove that under mild assumptions, in-context learning is \textit{guaranteed} to happen for a model trained by such multi-task pretraining. 
	We show that the in-context learning mechanism of concatenating input/output pairs of a certain task allows the pretrained LM to uncover the latent task and improve its performance \textit{without modifying its weights}. 
	Our assumptions are quite general, namely that there is a lower bound on the probability of any mixture component and of any token, and that the downstream tasks are both distinguishable and have sufficient margin between different labels. %
	%
	See Section~\ref{section:the_analyzed_hypothesis_class} for a formal definition of our assumptions.
	
	The interpretation of in-context learning as uncovering tasks that were already learned during pretraining is also supported by empirical evidence. For example, both~\cite{min2022rethinking} and~\citet{lyu2023zicl} showed that replacing the labels provided in the in-context training examples with random task labels barely affects the performance of in-context learning, implying that the in-context learning mechanism is more about identifying the task than about learning it. %
	Similarly, both~\citet{webson-pavlick-2022-prompt} and~\citet{lampinen2022can} studied whether LMs truly understand the text of the in-context examples, and found that irrelevant text that mimic the task can have a similar effect as learning with true training examples.
	
	Finally, the latent task inference perspective of in-context learning was also examined theoretically.~\cite{xie2022an} study a language-inspired toy distribution for pretraining. Specifically, they analyze a pretraining distribution consisting of a mixture of Hidden Markov Models (HMMs), where each HMM factor corresponds to a single task.
	They showed that in this setting in-context learning was guaranteed, however, unlike our work, their analysis is confined only to the infinite limit of the number of in-context examples. 
	We provide polynomial sample complexity guarantees for in-context learning, which are much more relevant to its efficiency in practice (often referred to as ``few-shot"). Furthermore,~\cite{xie2022an} limit their analysis only to a certain class of mixture of HMMs as pretraining distribution, while our results are for any mixture distribution under mild assumptions (see above). Lastly, their analysis assumes perfect learning of the pretraining distribution while our PAC approach captures imperfect pretraining. Beyond~\cite{xie2022an}, several recent papers~\citep{akyrek2023what,dai2022can,von2022transformers} showed that from an expressivity point of view, self-attention architectures can implement a gradient descent algorithm with in-context examples. However, these works do not justify why pretraining on natural data will converge to weights that implement gradient decent, nor do they provide finite sample-complexity results for in-context learning.
	Overall, our presented PAC framework for in-context learning (Section~\ref{section:definition}) allows us to prove the first finite in-context learning sample-complexity results, in a quite general setting (Section~\ref{section:a_latent_concept_inference_perspective_on_in-_ontext_learning}). We hope that this framework will facilitated a further expansions of the understanding of in-context learning as a new learning paradigm. 
	
	
	\section{A PAC Learnability Framework for In-Context Learning }
	\label{section:definition}
	In this section, we define a Probably Approximately Correct (PAC) learnability framework~\citep{valiant1984theory} for in-context learning. Conceptually, we aim to adjust the PAC learnability framework, in order to capture the in-context few-shot learning capabilities of large language models~\citep{brown2020language,wei2022finetuned,sanh2022multitask,ouyang2022training}. 
	
	%
	
	We begin by describing the in-context learning paradigm.
	Let $f_\theta$ be a function fitted to a massive general purpose  pretraining distribution. In order to apply $f_\theta$ for a specific downstream task, it is common to further train $f_\theta$ on pairs of the task's inputs $x_1,\dots,x_k$ with their corresponding labels $y_1,\dots,y_k$, in a procedure referred to as fine-tuning. In-context learning is an alternative, simpler technique, which ``teaches'' $f_\theta$ the task at hand by constructing a \textit{prompt} comprised of concatenated training examples. Specifically, denote the string concatenation operator by $\oplus$, and let $\delimetertoken$ be a special delimiter token. Then, the in-context learning technique is to construct a prompt $p$ by concatenating pairs of the task's inputs along with theirs labels:
	\begin{align}
	p\coloneqq x_{1}\oplus y_{1}\oplus\delimetertoken\oplus\dots\oplus x_{k}\oplus y_{k}\oplus\delimetertoken
	\end{align}
	Given this prompt, the prediction is made by choosing the label $y$ that is the most likely continuation for the prefix $p\oplus x$ according to the function $\lm$. In other words, by predicting the label $y$ that maximizes $\lm\left(p\oplus x\oplus y\right)$. Notably, the prefix $p\oplus x$ does not resemble inputs that $\lm$ has trained on.
	Note that we used the newline symbol $\delimetertoken$ for clarity. However, in practice the delimiter token might depend on implementation and could, for instance, be two successive newlines $``\backslash n\backslash n"$ as done in the few-shot learning evaluation framework Evaluation Harness~\citep{eval-harness}.
	
	We now present our in-context learning learnability definition. Let $\pretraining$ be a pretraining distribution over strings of characters from an alphabet $\Sigma$. And let $\downstream$ be a downstream task's distribution over pairs of strings $x\in\Sigma^{\star}$ and theirs corresponding labels $y\in\Sigma^{\star}$. 
	We aim to define distribution-dependent learnability in the in-context learning setup, for a frozen probabilistic model $f_\theta$ that originally trained to maximize the likelihood of the pretraining distribution $\pretraining$. Then, given a prompt $p\sim\downstream^{k}$ which consist of $k$ pairs of inputs and theirs corresponding labels, each independently sampled from $\downstream$, the model $\lm$ is tested on the zero-one loss of the in-context predictor:
	\begin{align}\label{equation:in_context_learning_loss}
	L_{\text{in-context},\tilde{D}}\coloneqq\expectation_{x,y\sim\downstream}\left[l_{0-1}\left(\text{argmax}_{y'}f_{\theta}\left(p\oplus x\oplus y'\right),y\right)\right]
	\end{align}
	Since we are interested in frozen models as opposed to models that are fine-tuned, we will require that the same $f_\theta$ model achieves a low $L_{\text{in-context},\downstream}$ loss for multiple downstream tasks simultaneously.
	The following is our PAC learning definition for in-context learning of a model that was pretrained to maximize the likelihood of the pretraining distribution.
	
	\begin{definition}\label{definition:in_context_learning}
		Let $\mathcal{H}$ and $\tilde{\mathcal{H}}$ be hypothesis classes of pretraining distributions and downstream distributions respectively. We say that $\tilde{\mathcal{H}}$ is in-context learnable after pretraining on a pretraining distribution $\pretraining\in\mathcal{H}$, if there exist a pretraining algorithm $\mathcal{A}$ and a pair of functions $m_{\mathcal{H}},m_{\tilde{\mathcal{H}}}:\left(0,1\right)^{2}\rightarrow\mathbb{N}$ with the following property: For every $\epsilon,\delta>0$ and every $\downstream\in\tilde{\mathcal{H}}$, if the number of pretraining examples n is greater than $m_{\mathcal{H}}\left(\epsilon,\delta\right)$, and if the number of the downstream tasks' examples k is greater than $m_{\tilde{\mathcal{H}}}\left(\epsilon,\delta\right)$, then with probability of at least $1-\delta$ (over the choice of the $n+k$ examples) the following holds:
		\begin{align}
		L_{\text{in-context},\downstream}-\text{Bayes Error Rate}\le\epsilon
		\end{align}
	\end{definition}
	In addition, we will say that a collection of downstream tasks $\tilde{\mathcal{H}}$ is \textbf{efficiently} in-context learnable after pretraining on $\pretraining$, if both $m_{\mathcal{H}}$ and $m_{\tilde{\mathcal{H}}}$ in the above definition are polynomial in $\epsilon^{-1}$ and $\delta^{-1}$.
	
	In order to focus on in-context learning questions that are relevant for the natural language processing setup, and avoid complications that are not necessary for answering them, we will assume that the pretraining distribution $\pretraining$ belongs to some hypothesis class $\mathcal{H}$ that is learnable by some pretraining algorithm: 
	\begin{assumption}\label{assumption:existence_of_pre_training_algorithm}
		There exists a learning algorithm $\mathcal{A}$, and a sample complexity function $m_{\pretraining}:\left(0,1\right)^{2}\rightarrow\mathbb{N}$, with the following property: 
		for any pretraining distribution $\pretraining\in\mathcal{H}$ and any $\epsilon,\delta>0$, if the number of pretraining examples $n$ is greater than $m_\pretraining\left(\epsilon,\delta\right)$, then with probability at least $1-\delta$ over the pretraining examples, for any $T\ge 1$, the total variation of the conditional distributions of the $T$ 'th token is at most $\epsilon$: 
		\begin{align}
		\max_{o_{1}\dots o_{T}\in\Sigma}\left|\prob_{\pretraining}\left(o_{T}|o_{1}\dots o_{T-1}\right)-\lm\left(o_{T}|o_{1}\dots o_{T-1}\right)\right|<\epsilon
		\end{align}
		where $f_{\theta}$ denotes the model yielded by the pretraining algorithm $\mathcal{A}$.
		In addition, $m_{\pretraining}$ is polynomial in both $\epsilon^{-1}$ and $\delta^{-1}$.
	\end{assumption}
	%
	
	With the above formal definition of in-context learning, we aim to shed some light on the mysterious in-context learning abilities of large LMs. Specifically, in the next section, we will shed light on the following major question: How do \textit{frozen} pretrained models learn from prompts that do not resemble their pretraining distribution?
	%
	%
	%
	%
	%
	%
	
	
	%
	\section{Guarantees on In-Context Learning}
	\label{section:a_latent_concept_inference_perspective_on_in-_ontext_learning}
	In this section, we will demonstrate the use of the above learnability framework for in-context learning and analyze a setting in which pretraining a simple language model provably leads to in-context learning capabilities. For doing so, we follow previous work and view the language modeling task as an implicit multi-task setup. 
	Indeed, creating human-like text involves many different skills, from grammar to world knowledge, so learning a language model inevitably develops a variety of skills~\citep{gulordava-etal-2018-colorless,zhang-bowman-2018-language,weber-etal-2021-language}. The implicit unsupervised multi-task view of language modeling can be traced to the GPT2 paper~\citep{radford2019language}, which revealed that pretrained LMs are capable of a wide variety of natural language tasks without the need for further training. Since then, this view has been reinforced for example by~\cite{hendrycks2021measuring}. They showed that on a diverse massive set of 57 real world text understanding tasks, the largest GPT-3~\citep{brown2020language} language model improves over random chance by almost 13 percents points on average. Importantly, these results were obtained in a zero-shot learning setting,~\ie~in a setting where the language model had only been pretrained to maximize the likelihood of unlabeled text. Accordingly, these results suggest that many different natural tasks are directly learned during pretraining.
	
	We reflect the above implicit multi-task nature of language modeling by assuming that the pretraining distribution contains a latent variable that represents the task at hand. We show below that for such pretraining distributions, adding training examples to the in-context learning prompt implicitly reveals the already learned latent task.
	In Subsection~\ref{section:the_analyzed_hypothesis_class}, we present a multi-task pretraining hypothesis class, for which Subsection~\ref{section:our_results} shows that in-context learning reveals what task is currently being performed.
	
	\subsection{The analyzed latent concept hypothesis class}\label{section:the_analyzed_hypothesis_class}
	In this subsection, we describe the analyzed multi-task pretraining hypothesis class, as well as the corresponding downstream task hypothesis class that can be learned in-context after pretraining. To begin, we define the pretraining distribution as a  mixture of multiple downstream tasks. Importantly, during pretraining the downstream task of each example is unknown,~\ie~it is a latent variable, and thus pretraining is not equivalent to fine-tuning on the task since the model cannot simply ignore pretraining examples of irrelevant tasks. 
	Specifically, we generate a length-$T$ pretraining example $x_1,\dots,x_T\in\Sigma$ from the pretraining distribution $\pretraining$ by first sampling a concept $\phi$ from a family of concepts $\Phi$ according to a prior $\prob\left(\phi\right)$. We then sample the tokens according to the concept's specific distribution $\prob_{\phi}\left(x_1,\dots,x_T\right)$. 
	
	Moving to the downstream tasks, we will prove in-context learnability results for tasks where the underlying inputs distribution is a component $\phi$ in the pretraining mixture distribution $\pretraining$. Formally, we generate a length $T$ downstream task example $x$ and the corresponding label $y$ from $\downstream$ by first sampling $T$ tokens $o_1,\dots,o_{T}$ according to the downstream task distribution $\prob_{\phi}\left(o_1,\dots,o_{T}\right)$. Then we assemble $x$ using all tokens except the last one, and set the label to be that token~\ie~we set that $x_t=o_t$ for any $t <T$ and that $y=o_{T}$. Note that in principle the concatenation of independent examples in $p$ causes a distribution drift from the pretraining distribution\footnote{Formally, the distribution drift is caused by the fact that the concatenation of the $\delimetertoken$ after $y$ ignores the marginal probabilities of $\delimetertoken$.}. In this sense, the analyzed model captures the fact that few-shot prompts are unnatural since they are not encountered during pretraining. 
	
	Now we describe assumptions about the pretraining distributions, for which we will prove our in-context learnability results. Our first assumption requires that given a delimiter token $\delimetertoken$, two successive strings $s_1$ and $s_2$ that are concatenated with $\delimetertoken$ are approximately independent according to $\prob_{\phi}$:
	\begin{assumption}\label{assumption:new_line_conditional_independence}
		There exists a constant $0<c_1\le 1$ such that for any two strings $s_1,s_2$ in $\Sigma^{\star}$ and any concept $\phi$ the following holds:
		\begin{align}\label{equation:new_line_conditional_independence}
		c_{1}\le\frac{\prob_{\phi}\left(s_{1}\oplus\delimetertoken\right)\cdot\prob_{\phi}\left(s_{2}\right)}{\prob_{\phi}\left(s_{1}\oplus\delimetertoken\oplus s_{2}\right)}\le\frac{1}{c_{1}}
		\end{align}
	\end{assumption}
	Note that when the two successive strings $s_1$ and $s_2$ are exactly independent, the probability ratio in Equation~\ref{equation:new_line_conditional_independence} is equal to one, and the constant $c_1$ quantifies the deviation from this situation.
	While this assumption might sound restrictive, it is reasonable to assume that two consecutive paragraphs are not highly dependent according to the distributions in the pretraining mixture. Intuitively, we will use this assumption in order to apply concentration inequalities to the likelihood of the in-context prompt, and we will deduce that the role of the in-context prompt is to reweight the prior regarding the different mixture components. It is important to note that the approximate independence assumption for any component in the mixture does not imply approximate independence of the mixture distribution itself. Hence this reweighting is possible, since the assumption does not imply that the in-context prompt is ignored.
	
	Beyond this approximate independence, we will also require that there exist a lower bound on the conditional probability of any single token:
	\begin{assumption}\label{assumption:single_token_prob_lower_bound}
		There exist a constant $c_2>0$ such that for any string $s$ in $\Sigma^{\star}$, any character $\sigma\in\Sigma$, and any concept $\phi$ the following holds:
		\begin{align}
		\prob_{\phi}\left(\sigma\,|\,s\right)>c_{2}
		\end{align}
	\end{assumption}
	Basically, we need this assumption in order to avoid the harm of zero likelihood due to the unnatural concatenation of input and output pairs in the prompt $p$ (where the prompt $p$ is defined in Section~\ref{section:definition}). Note that without such an assumption, in-context learning is impossible since the probability of a prompt $p$ might be zero and hence the prediction of the model in such cases becomes meaningless.
	Finally, we assume that the prior distribution is strictly positive. In other words, we will assume that there is a lower bound higher than zero on the likelihood of any concept appearing in the pretraining distribution.
	\begin{assumption}\label{assumption:prior_lower_bound}
		There exist a constant $c_3>0$ such that for the prior $\prob_{\pretraining}\left(\phi\right)$ of any concept $\phi$, is at least $c_3$.
	\end{assumption}
	Clearly, without such an assumption, the prior of the downstream task can be arbitrarily low, which means it will be nearly impossible to recognize the task. In the next subsection, we will use the above assumptions in order to provide in-context learning guarantees via the mechanism of task recognition.
	
	\subsection{Guarantees on In-Context Learning via Latent Concept Inference}\label{section:our_results}
	%
	In this subsection, we analyze the prediction of an in-context learning model in the setting described in Subsection~\ref{section:the_analyzed_hypothesis_class}. We show that in this setting, there is a polynomial sample complexity that guarantees in-context learning is \textbf{P}robably \textbf{A}pproximately \textbf{C}orrect.
	At a high level, since the pretraining is not precise and can only approximate the pretraining distribution $\pretraining$ up to some error $\bigtriangleup_{\text{pretraining}}>0$ (see Assumption~\ref{assumption:existence_of_pre_training_algorithm}), we will split the in-context prediction analysis into two parts. The first part will involve the simpler case of test examples $x$ and corresponding label candidates $y$ and $\tilde{y}$ for which the margin between the conditional likelihoods of the labels 
	$\prob_{\downstream}\left(y\,|\,x\right)$ and $\prob_{\downstream}\left(\tilde{y}\,|\,x\right)$ is large enough. In this scenario, we show that both the deviation due to imperfect pretraining and due to imperfect task recognition is negligible. Therefore, we will conclude that such deviations do not have any impact on the loss of in-context learning. In the second scenario, when the difference between ground-truth likelihoods is small, the error rate of the Bayes optimal classifier must be high. Accordingly, even though the in-context predictor might confuse between labels, the loss in any case will be small because we only compare it to the error rate of the Bayes optimal classifier.
	
	Starting with the first scenario, where the margin between label candidates is sufficiently large, we will prove that as more examples are added to the prompt, the in-context predictions converge to the correct label. As a preliminary step, we prove a lemma regarding the ratio of the prompt likelihoods $\prob_{\phi} (p)$ (where the prompt $p$ is defined in Section~\ref{section:definition}) according to the ground-truth task versus the other mixture components. In other words, we ask how likely it is that the prompt of concatenated examples was sampled according to one of the tasks distributions versus the other tasks distributions. We will use this lemma in order to estimate the effect of the in-context prompt, on the prior regarding the different mixture components. Specifically, we denote by $\bigtriangleup_{\text{KL}}$ the minimum Kullback–Leibler divergence between the ground-truth component, and the other mixture components. Then, we prove that the ratio of the prompt probabilities according to the ground-truth components converge to zero, with rate that is exponential in both the number of in-context examples $k$, and in the minimal Kullback–Leibler divergence $\bigtriangleup_{\text{KL}}$. Intuitively, the exponential rate with regard to the number of examples comes naturally from the fact that each example in the prompt is sampled independently of the others. As a result, their effect is a multiplicative one. Additionally, the Kullback-Leibler divergence between the ground truth component and the other mixture components measures log probabilities, while we are interested in the probabilities themselves. Hence the rate is also exponential in the above Kullback-Leibler divergence. Formally, we have that:
	\begin{lemma}\label{lemma:ratio_of_the_prompt_probabilities_according_to_the_mixture_components}
		Let $\pretraining$ be a pretraining distribution for which assumptions~\ref{assumption:new_line_conditional_independence},\ref{assumption:single_token_prob_lower_bound} hold, and let $\phi^{\star}$ be a downstream task mixture component from $\pretraining$ such that $\bigtriangleup_{\text{KL}}>8\log\frac{1}{c_1\cdot c_{2}}$. Then, there exists $m_{\downstream}:\left(0,1\right)^{2}\rightarrow\mathbb{N}$ with the following property: for any $\epsilon,\delta>0$, if the number of in-context examples $k$ is at least $m_{\downstream}\left(\epsilon,\delta\right)$, then, $\frac{\prob_{\phi}\left(p\right)}{\prob_{\phi^{\star}}\left(p\right)}<\epsilon$ with probability of at least $1-\delta$ (over the choice of the $k$ in-context examples).
		Moreover, the above still holds when the labels in $p$ are randomly flipped, and $m_{\downstream}$ can be chosen such that it will be polynomial in $\log{\frac{1}{\delta}},\log{\frac{1}{\epsilon}},\log{\frac{1}{c_1\cdot c_2}},\frac{1}\bigtriangleup_{\text{KL}}$ and $T$.
	\end{lemma} 
	\begin{proof}
		In essence, we prove this lemma by using the approximate independence assumption in order to apply concentration inequalities. In addition, we use Assumption~\ref{assumption:single_token_prob_lower_bound} for bounding the distribution drift that is caused by the artificially inserted newline token.
		In particular, the log of the ratio of prompt probabilities according to different mixture components is concentrated around its expectation. Importantly, this expectation is equal to minus one times the Kullback–Leibler divergence between the components, plus a term that is caused by the mentioned distribution drift. Thus, we conclude that the ratio of the prompt probabilities according to the ground-truth task distribution and the other pretraining components converges to zero. Moreover, the rate of that convergence is exponential in both the number of in-context demonstrations, and the minimal Kullback–Leibler divergence between different mixture components.
		See full details in Section~\ref{section:ratio_of_the_prompt_probabilities_according_to_the_mixture_components_proof} of the appendix.
	\end{proof}
	%
	
	Now we will use the above Lemma~\ref{lemma:ratio_of_the_prompt_probabilities_according_to_the_mixture_components} to analyze the in-context predictions, namely the labels $\tilde{y}$ that maximize the likelihood of the concatenation of the in-context prompt $p$, with the example $x\oplus \tilde{y}$ according to the pretrained model $\lm$. Essentially, we will aim to understand when these predictions are identical to the Bayes Optimal Classifier predictions. That it, when these predictions align with the labels $y$ that maximize the likelihood of the example $x\oplus y$ as determined by downstream task distribution $\prob_{\downstream}$.
	Since we are in the first scenario, where the margin between label candidates is large enough. We will prove that in this case, for large enough $k$, the ground truth in-context predictor also has margin that is at least half of the original margin. Hence, we will conclude that for such $k$ the ground truth in-context predictor is equal to the Bayes Optimal Classifier. Moreover, we will prove a lower bound on the margin of the in-context predictions, so the above still holds for any distribution that approximates the pretraining distribution sufficiently well.
	
	Formally, for a test example input $x$, we define the margin $\bigtriangleup\left(x,y,\tilde{y}\right)$ between two label candidates $y$ and $\tilde{y}$ as the difference between their conditional likelihoods $\prob_{\downstream}\left(y\,|\,x\right)$ and $\prob_{\downstream}\left(\tilde{y}\,|\,x\right)$ according to the downstream ground-truth distribution. Similarly, for a test example input $x$ and in-context prompt $p$, we define the margin $\bigtriangleup\left(p,x,y,\tilde{y}\right)$ between two label candidates $y$ and $\tilde{y}$ as the difference between their conditional likelihoods $\prob_{\pretraining}\left(y\,|\,p\oplus x\right)$ and $\prob_{\pretraining}\left(\tilde{y}\,|\,p\oplus x\right)$ according to the pretraining distribution conditioned on the prompt $p$. Using these definitions, we consider triplets of test example input $x$ and two labels candidates $y,\tilde{y}$ with margin at least two times the pretraining error $\bigtriangleup_{\text{pretraining}}$ as the first scenario. In this case we will prove that for large enough $k$ the ground truth in-context predictor also has margin of at least the pretraining 
	error $\bigtriangleup_{\text{pretraining}}$. 
	This means that deviations arising from imperfect task recognition will be negligible in this situation. Moreover, the margin will remain larger than the pretraining error, which means the pretrained model $\lm$ handles this case well.
	\begin{theorem}\label{theorem:ground_truth_in_context_predictions_preserve_margin}
		Let $\pretraining$ and $\downstream$ be a pair of pretraining distribution and downstream task, for which Assumption~\ref{assumption:prior_lower_bound} as well as the assumptions in Lemma~\ref{lemma:ratio_of_the_prompt_probabilities_according_to_the_mixture_components} upholds. Then, there exists $m_{\downstream}:\left(0,1\right)^{2}\rightarrow\mathbb{N}$ with the following property: for every test example $x$ and two label candidates $y,\tilde{y}$ with positive margin $\bigtriangleup\left(x,y,\tilde{y}\right)>0$ and $\delta>0$, if the number of in-context examples $k$ is at least $m_{\downstream}\left(\nicefrac{\bigtriangleup\left(x,y,\tilde{y}\right)}{2},\delta\right)$, then $\bigtriangleup\left(p,x,y,\tilde{y}\right)>\nicefrac{\bigtriangleup\left(x,y,\tilde{y}\right)}{2}+c_{1}^{2}-1$ with probability of at least $1-\delta$ (over the choice of the $k$ in-context examples).
		Moreover, the above still holds when the labels in $p$ are randomly flipped, and $m_{\downstream}$ can be chosen such that it will be polynomial in $\log{\frac{1}{\delta}},\log{\frac{1}{\epsilon}},\log{\frac{1}{c_1\cdot c_2\cdot c_3}},\frac{1}\bigtriangleup_{\text{KL}}$ and $T$.
	\end{theorem}
	\begin{proof}
		We begin by writing the difference between the ground-truth label likelihood, and the likelihood of other another label $\tilde{y}$ explicitly. Specifically, by the definition of conditional probabilities we have that:
		\begin{align}
		\prob_{\pretraining}\left(y\,|\,p\oplus x\right)-\prob_{\pretraining}\left(\tilde{y}\,|\,p\oplus x\right)=\frac{\sum_{\phi}\prob_{\pretraining}\left(\phi\right)\left[\prob_{\phi}\left(p\oplus x\oplus y\right)-\prob_{\phi}\left(p\oplus x\oplus\tilde{y}\right)\right]}{\sum_{\phi}\prob_{\pretraining}\left(\phi\right)\prob_{\phi}\left(p\oplus x\right)}
		\end{align}
		Now, denote by $\phi^{\star}$ the mixture component of $\downstream$, then Assumption~\ref{assumption:new_line_conditional_independence} assures us that for each component in the mixture, the textual prompt is approximately independent of the test example. So we can use Lemma~\ref{lemma:ratio_of_the_prompt_probabilities_according_to_the_mixture_components} and get that in both the numerator and the denominator, the $\phi^{\star}$ term is the dominant term in the sum. Thus, we will get that the difference of the labels` likelihoods is approximately the fraction of the $\phi^{\star}$ terms, which is equal to the original downstream task margin. So we will conclude that the margin of the in-context predictor is at least half of the downstream task original margin. See full details in Section~\ref{section:ground_truth_in_context_predictions_preserve_margin_proof} of the appendix.
	\end{proof}
	We denote by $\bigtriangleup_{\text{\ensuremath{\downstream}}}$ the minimal margin of the Bayes Optimal Classifier predictions in downstream task $\downstream$~\ie~the minimal margin between the Bayes Optimal Classifier prediction and another labels. With the above definitions and theorem, we can combine the scenario of large margins, which are preserved by the in-context predictor, with the scenario of small margins, in which the loss is minimally affected by the wrong prediction, and prove our main in-context learnability results: 
	\begin{theorem}\label{theorem:latent_concept_pre_training_method_of_moments_in_context_learning}
		Let $\tilde{\mathcal{H}}$ be hypothesis classes of downstream distributions, and denote by $\pretraining$ a mixture distribution on $\tilde{\mathcal{H}}$ for which assumptions~\ref{assumption:existence_of_pre_training_algorithm},\ref{assumption:new_line_conditional_independence},\ref{assumption:single_token_prob_lower_bound} and~\ref{assumption:prior_lower_bound} uphold.
		Further assume that the margin $\bigtriangleup_{\text{\ensuremath{\downstream}}}$ of any downstream task $\downstream\in\tilde{\mathcal{H}}$ is at least $4\cdot\left(1-c_1^2\right)$, and the minimal Kullback–Leibler divergence between different distributions is greater than $\log\frac{1}{c_1\cdot c_{2}}$. Then $\tilde{\mathcal{H}}$ is efficiently in-context learnable after pretraining on $\pretraining$ (see Definition~\ref{definition:in_context_learning}).
	\end{theorem}
	\begin{proof}
		Assumption~\ref{assumption:existence_of_pre_training_algorithm} assures us the existence of the pretraining algorithm with a polynomial sample complexity. So we will prove the theorem with pretraining sample complexity that is derived from this algorithm, where the accuracy required from pretraining is $8$ times the accuracy required from in-context learning (see analysis below). In addition, Theorem~\ref{theorem:ground_truth_in_context_predictions_preserve_margin} assures us that for large enough $k$ the ground truth in-context predictions are equal to the Bayes Optimal Classifier predictions, so we will prove the theorem with downstream sample complexity that is derived from Theorem~\ref{theorem:ground_truth_in_context_predictions_preserve_margin}.
		
		Let $\epsilon,\delta>0$ and denote by $\lm$ the model that was learned during the pretraining process. We will prove that the contribution of any $x$ to the loss $L_{\text{in-context},\tilde{D}}$ (see Equation~\ref{equation:in_context_learning_loss}) is at most $\epsilon$ and hence complete the proof. Given $x$, let $y$ be the Bayes Optimal Classifier prediction for that $x$. Then, we will split the analysis into two cases. 
		
		The first case is of examples $x,y$ such that theirs margin $\bigtriangleup\left(x,y,\tilde{y}\right)$ from any alternative label candidate $\tilde{y}$ is at least $8\cdot\bigtriangleup_{\text{pretraining}}$. In this case, Theorem~\ref{theorem:ground_truth_in_context_predictions_preserve_margin} assures us that for large enough $k$ the ground truth in-context predictor also has margin $\bigtriangleup\left(p,x,y,\tilde{y}\right)$ that is greater than 
		$\frac{1}{2}\cdot\bigtriangleup\left(x,y,\tilde{y}\right)+c_{1}^{2}-1$. Now since $\bigtriangleup\left(x,y,\tilde{y}\right)\ge\bigtriangleup_{\text{\ensuremath{\downstream}}}>4\cdot\left(1-c_1^2\right)$ we have that $\bigtriangleup\left(p,x,y,\tilde{y}\right)$ is at least $2\cdot\bigtriangleup_{\text{pretraining}}$. Thus, we conclude that in this case, the predictions of the ground truth in-context prediction and $\lm$ are the same. Furthermore both of them are identical to the Bayes Optimal Classifier prediction, and hence $x$ does not contribute to the loss.
		
		Moving to the second case, and denote by $\tilde{y}$ the in-context prediction of $\lm$. Then, the arguments from the previous paragraph assure us that $\prob_{\phi^{\star}}\left(y\,|\,x\right)-\prob_{\phi^{\star}}\left(\tilde{y}\,|\,x\right)<8\cdot\bigtriangleup_{\text{pretraining}}$, since otherwise we will get that $\lm\left(y\,|\,x\right)>\lm\left(\tilde{y}\,|\,x\right)$. Finally, we will choose that $\bigtriangleup_{\text{pretraining}}=\frac{\epsilon}{8}$, and hence get that also in the second case, the contribution of $x$ to the loss is less than $\epsilon$.
	\end{proof}
	
	%
	%
	
	
	
	%
	%
	
	%
	%
	%
	%
	%
	%
	%
	%
	%
	%
	%
	
	%
	%
	%
	
	%
	%
	%
	%
	
	%
	%
	%
	
	%
	%
	%
	
	%
	%
	%
	
	%
	
	%
	
	
	%
	%
	%
	%
	%
	%
	%
	%
	%
	%
	
	%
	%
	%
	%
	%
	%
	%
	%
	
	\section{Related Work}
	Since the Probably Approximately Correct (PAC) learning framework was introduced in~\cite{valiant1984theory}, a rich line of works has extended the framework to distribution-dependent bounds~\citep{benedek1991learnability,vayatis1999distribution,sabato2013distribution} \textit{inter alia}. These works relax PAC's adversarial requirement of generalization for all input distributions, and only consider distributions that satisfy certain statistical properties. Consequently, these papers provide more realistic sample complexity bounds. In this paper we present a framework for in-context learning learnability that uses self-supervised pretraining to assist in solving downstream tasks. In this framework, we follow the above line of distribution-specific works with the distinct feature of a self-supervised pretraining phase, and of learning with frozen models through their input context.
	The advantages of such pretraining have been studied from a theoretical perspective before. For example,~\cite{saunshi2021a} and~\cite{colin2021why} demonstrate that language modeling can benefit downstream tasks either by prompt tuning or head tuning. However, unlike our work, in their analysis some weights are learned. In contrast, we focus on frozen models that can learn only from their input context, which includes the concatenation of few inputs and outputs pairs.
	
	Returning to in-context learning, a recent line of work study learning paradigms that are based on concatenating several inputs into a single input context. For example,~\cite{garg2022what} show that a Transformer architecture is able to discover in-context learning algorithms for simple function classes, such as two-layer neural networks, and decision trees. Similarly,~\cite{akyrek2023what} show that a Transformer architecture is able to discover efficient Bayes optimal least square learning algorithms, and~\cite{laskin2022context} show that a Transformer architecture is able to discover efficient in-context reinforcement learning algorithms. Additionally,~\cite{levine2022the} studied the inductive bias of in-context learning, and proved that Transformer based language models can model much stronger dependencies between text segments that appeared in the same training example. Finally,~\cite{li2023transformers} proved a multi-task generalization bound for in-context learning with the Transformer architecture. However, unlike our work, the pretraining distribution in all of the above works designed to include input context that includes few-shot demonstrations explicitly. As a consequence, these methods do not answer the mysterious question of how frozen pretrained models can learn from in-context prompts that do not resemble their pretraining distribution.
	
	Another recent line of work investigates possible mechanisms that enable in-context learning. For example,~\cite{olsson2022context} provide indirect evidence  that \emph{induction heads} might constitute the mechanism for in-context learning in large transformer models. In addition, several recent papers~\citep{akyrek2023what,dai2022can,von2022transformers} showed that from an expressivity point of view, self-attention architectures can implement a gradient descent algorithm with in-context examples. Finally,~\cite{chan2022data} provided empirical evidence that some data distributional properties encourage in-context learning even when the frozen models do not see prompts during pretraining. However, unlike our work, all of the above works does not provide theoretical guarantees of  in-context learnability.
	
	Finally,~\cite{xie2022an} are the closest to our work. They studied a language-inspired toy distribution for pretraining and analyzed a pretraining distribution consisting of a mixture of Hidden Markov Models (HMMs). Unlike their results, our results are applicable to any mixture distribution that meets our mild assumption. In addition, unlike our polynomial sample complexity guarantees, their analysis guarantees in-context learning only for an infinite number of in-context examples. Lastly, their analysis assumes perfect pretraining distribution learning, but our approach captures imperfect pretraining.
	
	\section{Conclusion}
	The discovery of in-context learning in large LMs, made by~\citet{brown2020language}, was surprising to many in our field. 
	A model that was pretrained to maximize the likelihood of natural text was able to make use of \textit{concatenated }training examples of downstream natural language tasks---inputs that do not resemble its pretraining distribution, and moreover these inputs improved the model's ability to perform the task. Our theoretical results, based on a common latent multitask framework for the pretraining phase, shed light on the above surprising mysteries. With our PAC-based framework, we were able to provide sample complexity guarantees for in-context learning in such pretrained models, which are not only the first finite sample complexity results for this framework but they also indicate efficient (polynomial) in-context learning, which reflect the behavior of this setting in practice. 
	
	We hope that our framework can be used to deepen the understanding of the in-context learning phenomenon. In particular, we mark the connection between model size and the in-context learning efficiency as an interesting open question~\citep{wei2022emergent}. Additionally, in-context learning has shown to be capable of learning new tasks not included in the pre-training distribution~\citep{wei2023larger}. The extension of our results to such situations is an interesting open question.
	
	%
	
	%
	\section*{Acknowledgments}	
	The authors would like to thank Yotam Wolf and Oshri Avnery for their assistance and advice. This research was supported by the ERC (European Research Council) and the ISF (Israel Science Foundation).
	
	\bibliography{main}
	\bibliographystyle{arxiv_upload}
	
	\clearpage
	\appendix
	
	
	\section{Proof of Lemma~\ref{lemma:ratio_of_the_prompt_probabilities_according_to_the_mixture_components} }\label{section:ratio_of_the_prompt_probabilities_according_to_the_mixture_components_proof}
	We begin by explicitly writing $\prob_{\phi}\left(p\right)$. Specifically, Assumption~\ref{assumption:new_line_conditional_independence} assure us that each in-context example $x_{i}\oplus y_{i}\oplus"\backslash n"$ is approximately independent from the other in-context examples:
	\begin{align}
	c_{1}^{k}\le\frac{\prod_{i=1}^{k}\prob_{\phi}\left(x_{i}\oplus y_{i}\oplus"\backslash n"\right)}{\prob_{\phi}\left(p\right)}\le c_{1}^{-k}
	\end{align}
	In addition, Assumption~\ref{assumption:single_token_prob_lower_bound} bounds the distribution drift that caused by the artificial new line token:
	\begin{align}
	\prob_{\phi}\left(x_{i}\oplus y_{i}\right)\le\prob_{\phi}\left(x_{i}\oplus y_{i}\oplus"\backslash n"\right)\le\prob_{\phi}\left(x_{i}\oplus y_{i}\right)\cdot c_{2}
	\end{align}
	Similarly, we can use Assumption~\ref{assumption:single_token_prob_lower_bound} again to bounds also the distribution drift that caused by potential label flipping:
	\begin{align}
	c_{2}<\frac{\prob_{\phi}\left(x_{i}\oplus\tilde{y}_{i}\right)}{\prob_{\phi}\left(x_{i}\oplus y_{i}\right)}<\frac{1}{c_{2}}
	\end{align}
	where $\tilde{y}_{i}$ denote the "corrected" label. So overall, we got that:
	\begin{align}
	\left(c_{1}c_{2}^{2}\right)^{k}<\frac{\prod_{i=1}^{k}\prob_{\phi}\left(x_{i}\oplus\tilde{y}_{i}\right)}{\prob_{\phi}\left(p\right)}<\left(c_{1}c_{2}^{2}\right)^{-k}
	\end{align}
	Now, we can write the ratio of the prompt probabilities according to the ground-truth component and the other mixture components explicitly and get that:
	\begin{align}\label{equation:explicit_form_ratio}
	\log\frac{\prob_{\phi}\left(p\right)}{\prob_{\phi^{\star}}\left(p\right)}\le\sum_{i=1}^{k}\log\frac{\prob_{\phi}\left(x_{i}\oplus\tilde{y}_{i}\right)}{\prob_{\phi^{\star}}\left(x_{i}\oplus\tilde{y}_{i}\right)}+4k\log\frac{1}{c_{1}c_{2}}
	\end{align}
	Intuitively, as each term in the sum is independent of the other, and equals (in expectation) to the Kullback–Leibler divergence between $\phi^{\star}$ and $\phi$:
	\begin{align}\label{equation:negative_kl_in_excpectation}
	\expectation_{p}\left[\frac{1}{k}\sum_{i=1}^{k}\log\frac{\prob_{\phi}\left(x_{i}\oplus\tilde{y_{i}}\right)}{\prob_{\phi^{\star}}\left(x_{i}\oplus\tilde{y_{i}}\right)}\right]=-\text{KL}\left(\prob_{\phi^{\star}},\prob_{\phi}\right)
	\end{align}
	Equation~\ref{equation:explicit_form_ratio} assure us that as long as the ground truth downstream task component $\phi^{\star}$ distinguish enough from the rest of the mixture components, $\frac{\prob_{\phi}\left(p\right)}{\prob_{\phi^{\star}}\left(p\right)}$ converge to zero.  Formally, Assumption~\ref{assumption:single_token_prob_lower_bound} assure us that
	\begin{align}
	\left|\log\frac{\prob_{\phi}\left(x_{i}\oplus\tilde{y}_{i}\right)}{\prob_{\phi^{\star}}\left(x_{i}\oplus\tilde{y}_{i}\right)}\right|\le T\log\frac{1}{c_{2}}
	\end{align}
	for any $i,p,\phi,\phi^{\star}$. Hence, we can use Hoeffding's inequality~\citep{hoeffding1994probability} to estimate the deviation of this variable from its expectation (see Equation ~\ref{equation:negative_kl_in_excpectation}). More specifically, together with Equation~\ref{equation:explicit_form_ratio} we get that with probability of at least $1-\delta$ over the choice of the $k$ examples in $p$ we have that:
	\begin{align}
	\frac{\prob_{\phi}\left(p\right)}{\prob_{\phi^{\star}}\left(p\right)}<\exp\left(-\frac{k}{2}\left(\text{KL}\left(\prob_{\phi^{\star}},\prob_{\phi}\right)-8\log\frac{1}{c_{1}c_{2}}\right)\right)
	\end{align} for
	$k>\frac{\left(\log\frac{1}{\delta}\right)\left(16T^{2}\right)\left(\log^{2}\frac{1}{c_{2}}\right)}{\text{KL}\left(\prob_{\phi^{\star}},\prob_{\phi}\right)^{2}}$. Finally, Lemma~\ref{lemma:ratio_of_the_prompt_probabilities_according_to_the_mixture_components} follows by choosing that $m_{\downstream}$ is the maximum between $k>\frac{\left(\log\frac{1}{\delta}\right)\left(16T^{2}\right)\left(\log^{2}\frac{1}{c_{2}}\right)}{\text{KL}\left(\prob_{\phi^{\star}},\prob_{\phi}\right)^{2}}$ and $\frac{2\log\frac{1}{\epsilon}}{\bigtriangleup_{\text{KL}}-8\log\frac{1}{c_{1}c_{2}}}$, since the first term take care of the $1-\delta$ probability, and the second term term take care of the $\epsilon$-approximation.
	
	\section{Proof of Theorem~\ref{theorem:ground_truth_in_context_predictions_preserve_margin} }\label{section:ground_truth_in_context_predictions_preserve_margin_proof}
	%
	
	We begin by writing the difference labels likelihood explicitly. Specifically, by the definition of conditional probabilities we have that:
	\begin{align}\label{equation:explicit_margin}
	\prob_{\pretraining}\left(y\,|\,p\oplus x\right)-\prob_{\pretraining}\left(\tilde{y}\,|\,p\oplus x\right)=\frac{\sum_{\phi}\prob_{\pretraining}\left(\phi\right)\left[\prob_{\phi}\left(p\oplus x\oplus y\right)-\prob_{\phi}\left(p\oplus x\oplus\tilde{y}\right)\right]}{\sum_{\phi}\prob_{\pretraining}\left(\phi\right)\prob_{\phi}\left(p\oplus x\right)}
	\end{align}
	Now, Assumption~\ref{assumption:new_line_conditional_independence} assure us that for each component $\phi$ in the mixture the prompt $p$ is approximately independent from the test example:
	\begin{align}
	c_{1}\cdot\prob_{\phi}\left(p\right)\cdot\prob_{\phi}\left(x\oplus y\right)\le\prob_{\phi}\left(p\oplus x\oplus y\right)&\le\frac{1}{c_{1}}\cdot\prob_{\phi}\left(p\right)\cdot\prob_{\phi}\left(x\oplus y\right)\\\prob_{\phi}\left(p\oplus x\right)&\le\frac{1}{c_{1}}\cdot\prob_{\phi}\left(p\right)\cdot\prob_{\phi}\left(x\right)
	\end{align}
	So substituting this inequalities in Equation~\ref{equation:explicit_margin} give us that:
	\begin{align}
	\prob_{\pretraining}\left(y\,|\,p\oplus x\right)-\prob_{\pretraining}\left(\tilde{y}\,|\,p\oplus x\right)&\ge\frac{\sum_{\phi}\prob_{\pretraining}\left(\phi\right)\cdot\prob_{\phi}\left(p\right)\cdot\left[c_{1}^{2}\cdot\prob_{\phi}\left(x\oplus y\right)-\frac{1}{c_{1}^{2}}\cdot\prob\left(x\oplus\tilde{y}\right)\right]}{\sum_{\phi}\prob_{\pretraining}\left(\phi\right)\cdot\prob_{\phi}\left(p\right)\cdot\prob_{\phi}\left(x\right)}
	\end{align}
	Now, we will separate the sums in the denominator and the numerator to term that contain the mixture component $\phi^{\star}$ that corresponds to $\downstream$, and term that contain the other mixture components. So for clarity we will denote by $A,B,C,D$ the different sums:
	\begin{align}
	A&\coloneqq\prob_{\pretraining}\left(\phi^{\star}\right)\cdot\prob_{\phi^{\star}}\left(p\right)\cdot\left[c_{1}^{2}\cdot\prob_{\phi^{\star}}\left(x\oplus y\right)-\frac{1}{c_{1}^{2}}\cdot\prob_{\phi^{\star}}\left(x\oplus\tilde{y}\right)\right]\\B&\coloneqq\sum_{\phi\ne\phi^{\star}}\prob_{\pretraining}\left(\phi\right)\cdot\prob_{\phi}\left(p\right)\cdot\left[c_{1}^{2}\cdot\prob_{\phi}\left(x\oplus y\right)-\frac{1}{c_{1}^{2}}\cdot\prob_{\phi}\left(x\oplus\tilde{y}\right)\right]\\C&\coloneqq\prob_{\pretraining}\left(\phi^{\star}\right)\cdot\prob_{\phi^{\star}}\left(p\right)\cdot\prob_{\phi^{\star}}\left(x\right)\\D&\coloneqq\sum_{\phi\ne\phi^{\star}}\prob_{\pretraining}\left(\phi\right)\cdot\prob_{\phi}\left(p\right)\cdot\prob_{\phi}\left(x\right)
	\end{align}
	And get that:
	\begin{align}
	\prob_{\pretraining}\left(y\,|\,p\oplus x\right)-\prob_{\pretraining}\left(\tilde{y}\,|\,p\oplus x\right)\ge\frac{A}{C+D}+\frac{B}{C+D}
	\end{align}
	Now we know that $D>0$ and hence that:
	\begin{align}
	\left|\frac{A}{C+D}-\frac{A}{C}\right|&=\left|\frac{AD}{C^{2}+D}\right|\le\left|\frac{A}{C}\right|\cdot\left|\frac{D}{C}\right|\\\left|\frac{B}{C+D}-\frac{B}{C}\right|&=\left|\frac{BD}{C^{2}+D}\right|\le\left|\frac{B}{C}\right|\cdot\left|\frac{D}{C}\right|
	\end{align}
	And by definition we have that:
	\begin{align}
	\frac{A}{C}=\bigtriangleup\left(x,y,\tilde{y}\right)+\left(c_{1}^{2}-1\right)\prob_{\phi}\left(y\,|\,x\right)+\left(\frac{1}{c_{1}^{2}}-1\right)\cdot\prob_{\phi}\left(\tilde{y}\,|\,x\right)>\bigtriangleup\left(x,y,\tilde{y}\right)-1+c_{1}^{2}
	\end{align}
	So it is enough to show that $\left|\frac{B}{C}\right|<\frac{1}{5}\cdot\bigtriangleup\left(x,y,\tilde{y}\right)$ and that $\left|\frac{D}{C}\right|<\frac{1}{4}$, in order to prove that:
	\begin{align}
	\prob_{\pretraining}\left(y\,|\,p\oplus x\right)-\prob_{\pretraining}\left(\tilde{y}\,|\,p\oplus x\right)>\frac{3}{4}\cdot\frac{A}{C}-\frac{5}{4}\cdot\left|\frac{B}{C}\right|>\frac{1}{2}\cdot\bigtriangleup\left(x,y,\tilde{y}\right)+c_{1}^{2}-1
	\end{align}
	Starting with $\left|\frac{B}{C}\right|$, by the triangle inequality have that:
	\begin{align}
	\left|\frac{B}{C}\right|\le\sum_{\phi\ne\phi^{\star}}\frac{\prob_{\pretraining}\left(\phi\right)}{\prob_{\pretraining}\left(\phi^{\star}\right)}\cdot\frac{\prob_{\phi}\left(p\right)}{\prob_{\phi^{\star}}\left(p\right)}\cdot\frac{1}{c_{1}^{2}}\cdot\frac{1}{\prob_{\phi^{\star}}\left(x\right)}
	\end{align}
	In addition, Assumption~\ref{assumption:single_token_prob_lower_bound} assure us that $\prob_{\phi^{\star}}\left(x\right)>c_{2}^{T}$, hence we conclude that:
	\begin{align}
	\left|\frac{B}{C}\right|\le\sum_{\phi\ne\phi^{\star}}\frac{\prob_{\pretraining}\left(\phi\right)}{\prob_{\pretraining}\left(\phi^{\star}\right)}\cdot\frac{\prob_{\phi}\left(p\right)}{\prob_{\phi^{\star}}\left(p\right)}\cdot\frac{1}{c_{1}^{2}c_{2}^{T}}
	\end{align}
	Similarly, we also have that:
	\begin{align}
	\left|\frac{D}{C}\right|\le\sum_{\phi\ne\phi^{\star}}\frac{\prob_{\pretraining}\left(\phi\right)}{\prob_{\pretraining}\left(\phi^{\star}\right)}\cdot\frac{\prob_{\phi}\left(p\right)}{\prob_{\phi^{\star}}\left(p\right)}\cdot\frac{1}{c_{2}^{T}}
	\end{align}
	Now, the sum of the concepts' prior is at most one, and by Assumption~\ref{assumption:prior_lower_bound} we have that $\prob_{\pretraining}\left(\phi^{\star}\right)\ge c_{3}$, so it is enough to show that $\frac{\prob_{\phi}\left(p\right)}{\prob_{\phi^{\star}}\left(p\right)}<\frac{\bigtriangleup\left(x,y,\tilde{y}\right)}{5\cdot c_{1}^{-2}\cdot c_{2}^{-T}\cdot c_{3}^{-1}}\eqqcolon\tilde{\epsilon}$ for any concept $\phi\ne\phi^{\star}$, in order to complete the proof.
	Finally, Lemma~\ref{lemma:ratio_of_the_prompt_probabilities_according_to_the_mixture_components} assure us that under the conditions of Theorem~\ref{theorem:ground_truth_in_context_predictions_preserve_margin}, there exist $\tilde{m}_{\downstream}:\left(0,1\right)^{2}\rightarrow\mathbb{N}$ such that if the number of in-context examples $k$ is at least $\tilde{m}_{\downstream}\left(\tilde{\epsilon},\delta\right)$, then with probability of at least $1-\delta$  we get exactly that $\frac{\prob_{\phi}\left(p\right)}{\prob_{\phi^{\star}}\left(p\right)}<\tilde{\epsilon}$. Importantly, $\tilde{m}_{\downstream}$ is logarithmic in the accuracy level $\tilde{\epsilon}=\frac{\bigtriangleup\left(x,y,\tilde{y}\right)}{5\cdot c_{1}^{-2}\cdot c_{2}^{-T}\cdot c_3^{-1}}$, and hence polynomial in $T$.
	
	%
	%
	
	%
	%
	%
	%
	%
	%
	%
	%
	%
	%
	%
	%
	%
	
\end{document}
