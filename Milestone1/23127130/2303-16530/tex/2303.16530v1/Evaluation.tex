\section{Evaluation}\label{sec:evaluation}

We evaluate our observer-based runtime verification approach by investigating the following research questions: 
\begin{itemize}
    \item [RQ1:] How efficient is the observer-based runtime verification in terms of time for processing monitored events with observers and memory needed to represent observers?
    \item [RQ2:] How accurate is the observer-based runtime verification in detecting violations of properties? 
    \item [RQ3:] How fast is a dynamic adaptation of an observer at runtime compared to a redeployment of the observer? 
    \item [RQ4:] Can the observer-based runtime verification with adapting observers increase the trustworthiness of SAS?
\end{itemize}

To perform our evaluation, we implemented our runtime verification approach with its  observers and adaptations following the PSP and PAP with C++ on Arduino.\footnote{The PSP/PAP catalog, implementation, and replication package for the evaluation are available at: \url{https://www.github.com/HUB-SE/PAP/}} 
We deployed the code on multiple Arduino Mega\footnote{Arduino Mega 2560 Rev3, 8 KB SRAM, 16MHz clock speed.}. 

\noindent
\textbf{RQ1 } 
This research question addresses the efficiency of our observer-based runtime verification. To perform verification online, the runtime verifier is desired to process monitored events faster than the managed system and environment emit them. Thus, the verifier can provide fast results without the possibility of an overflowing event queue. 

To determine how much time it takes for our observer-based verification technique to process events, we generated ten artificial traces containing events of five different types. The traces have a length of $50,000$ events. We measured the time that our technique took with an artificial observer to process these traces. This artificial observer contains five states. Each state has five outgoing transitions, each labeled with one of the five event types.\footnote{We omit evaluating the costs of managing timers due to guarded transitions in the observer because they are similar to processing an event requiring in both cases to check all outgoing transitions of the observer's current state.} Thus, with each processed event of the trace, one transition will be enabled regardless of the current state of the observer.
For each processed event, any outgoing transition of the current state has to be checked until the transition with the matching label is found. Overall, the artificial observer has 25 transitions, which  in our experience is a realistic upper bound for an observer~\cite{vogel2023property}. 

We execute our runtime verification technique with the artificial observer on Arduino Mega against the ten artificial traces. On average, our technique took $6571.1ms$ to process a trace with a standard deviation of $\mu$\,$\leq$\,$7.2ms$.
Thus, on average it takes $0.13ms$ ($6571.1ms$/$50,000$ events) for our technique to process a single event with an observer. 
Prominent benchmarks for runtime verification such as Timescales~\cite{ulus2019timescales} provide traces that contain one event per \textit{ms}. Thus, we conclude that our observer-based runtime verification technique is sufficiently efficient concerning the execution time. This especially holds since we check only monitored events representing changes of the managed system or environment, where we consider a rate of one change per $ms$ as extraordinarily high with respect to our experience with the BSN. 

We also investigate the memory needed to represent an observer in a data structure implemented on Arduino in terms of SRAM usage. For this purpose, we use observers for nine properties with different combinations of patterns and scopes as well as the artificial observer discussed previously.
As shown in Table~\ref{tab:memory}, the observers consume between \textit{355} and \textit{1,136 bytes} of memory. For each observer, we list the number of states and transitions to illustrate the size of the observer. Such sizes are representative of properties following the PSP. 
We conclude that observers can be efficiently represented given their size in terms of states and transitions and multiple observers each representing a property can be deployed to one Arduino Mega that has \textit{8KB} of SRAM. 
\begin{boxA}
    Our runtime verification is efficient as it just requires $0.13ms$ on average to process a monitored event and between $355$ and $1,136$ \textit{bytes} of memory to represent an observer. 
\end{boxA}

\begin{table}[tbp]
    \begin{center}
        \caption{Size of observers for properties in terms of numbers of states (\#S) and transitions (\#T), and memory usage in bytes.}
        \label{tab:memory}
        \vspace{-.5em}
        \begin{tabular}{c c c c}
            \toprule
            \textbf{Pattern + Scope} & \textbf{\#S} & \textbf{\#T} & \textbf{Memory (bytes)}\\
            \midrule
            Absence After & 5 & 4 & 614 \\
            Absence Before & 5 & 4 & 558 \\
            Absence Between & 6 & 8 & 866 \\
            Recurrence Globally & 2 & 2 & 355 \\
            Recurrence Between & 4 & 5 & 605 \\
            Response Globally & 3 & 3 & 458 \\
            Response Between & 4 & 6 & 652 \\
            Response Chain Between, 2 responses & 6 & 11 & 940 \\
            Response Chain Between, 3 responses & 7 & 14 & 1,136 \\ \midrule
            Artificial observer & 5 & 25 & 1,047 \\
            \bottomrule
            \end{tabular}
    \end{center}
    \vspace{-2.25em}
\end{table}

\noindent
\textbf{RQ2 } 
In this research question, we investigate the correctness of our runtime verification. To this end, we implemented a trace generator according to the grammar of Timescales~\cite{ulus2019timescales}, which is a runtime verification benchmark. Each trace targets a property based on the PSP catalog and can be generated to either satisfy or violate the property. Thus, the generator provides the ground truth of whether a generated trace violates or satisfies the property.
We considered nine properties that follow the patterns and scopes shown in Table~\ref{tab:memory}.
For each property, we generated 20 different traces, each consisting of about 60 events. Ten of them satisfy and ten violate the property. 
Afterward, we instantiated the observer template of our PSP catalog for the property. We deployed the resulting observer and evaluated, whether it reaches an \textit{error} state when processing the trace. 
We found that our observers classified each of the 20 traces correctly for each of the nine properties. 
\begin{boxA}
    We can report 100\% accuracy in detecting property violations since our observer-based runtime verification has provided correct results for all 180 runs of the experiment (20 traces for each of the nine properties).
\end{boxA}

\begin{table*}[tbp]
    \begin{center}
        \caption{Requirements changes for the BSN. Added/updated parts of the property along the changes are highlighted in blue.}
        \label{tab:scenarios}
        \begin{tabular*}{\textwidth}{c L{0.12\textwidth} L{0.13\textwidth} L{0.67\textwidth}}
            \toprule
            \textbf{\#} & \textbf{Req. Change} & \textbf{PAP} & \textbf{MTL Property} \\
            \midrule
            0 & Initial situation (cf. Eq.~\ref{eq:req1}) & -- &$\square (( \text{cycle\_starting} \wedge \lozenge \text{cycle\_ending} ) \rightarrow (\text{request} \rightarrow (\neg \text{cycle\_ending } \text{ }\mathcal{U}^{[0,2]}\text{ } (\text{thermometer\_reply} \wedge \linebreak \neg \text{cycle\_ending} \wedge (\lozenge ^{[0, 2]} (\text{pulse\_reply} ))  ))) \text{ }\mathcal{U}\text{ } \text{cycle\_ending})$\\ \midrule
            1 & Add a glucometer & Adding a Response to the Chain & $\square (( \text{cycle\_starting} \wedge \lozenge \text{cycle\_ending} ) \rightarrow (\text{request} \rightarrow  (\neg \text{cycle\_ending} \text{ }\mathcal{U}^{[0,2]}\text{ }  (\text{thermometer\_reply} \wedge \linebreak \neg \text{cycle\_ending} \wedge (\lozenge ^{[0, 2]} (\text{pulse\_reply} )) \textcolor{blue}{\wedge \neg \text{cycle\_ending} \wedge (\lozenge ^{[0, 2]} (\text{glucose\_reply} ))} ))) \text{ }\mathcal{U} \text{ }\text{cycle\_ending})$\\ \midrule
            2 & Update time guard & Updating a Time Guard & $\square (( \text{cycle\_starting} \wedge \lozenge \text{cycle\_ending} ) \rightarrow (\text{request} \rightarrow (\neg \text{cycle\_ending} \text{ }\mathcal{U}^{[0,\textcolor{blue}{3}]}\text{ } (\text{thermometer\_reply} \wedge \neg \text{cycle\_ending} \wedge  (\lozenge ^{[0, \textcolor{blue}{3}]} (\text{pulse\_reply} )) \wedge \neg \text{cycle\_ending} \wedge (\lozenge ^{[0, \textcolor{blue}{3}]} (\text{glucose\_reply} )) ))) \text{ }\mathcal{U}\text{ } \text{cycle\_ending})$\\ \midrule
            3 & Remove the thermometer & Rem. a Response from the Chain & $\square (( \text{cycle\_starting} \wedge \lozenge \text{cycle\_ending} ) \rightarrow (\text{request} \rightarrow  \neg \text{cycle\_ending} \wedge  (\lozenge ^{[0, 3]} (\text{pulse\_reply} )) \wedge \neg \text{cycle\_ending} \wedge (\lozenge ^{[0, 3]} (\text{glucose\_reply} )) ) \text{ }\mathcal{U}\text{ } \text{cycle\_ending})$\\ \midrule
            4 & Scheduler requests data & Updating an Event & $\square (( \text{cycle\_starting} \wedge \lozenge \text{cycle\_ending} ) \rightarrow (\text{\textcolor{blue}{s\_request}} \rightarrow  (\neg \text{cycle\_ending} \text{ }\mathcal{U}^{[0,3]}\text{ } (\text{pulse\_reply} \wedge \neg \text{cycle\_ending} \wedge (\lozenge ^{[0, 3]} (\text{glucose\_reply} ))  ))) \text{ }\mathcal{U} \text{ }\text{cycle\_ending})$\\ \midrule     
            5 & Neglect order of sensors & Splitting the Response Chain & $\square (( \text{cycle\_starting} \wedge \lozenge \text{cycle\_ending} ) \rightarrow (\text{s\_request} \rightarrow  (\neg \text{cycle\_ending} \text{ }\mathcal{U}^{[0,3]}\text{ } (\text{pulse\_reply}))) \text{ }\mathcal{U}\text{ } \text{cycle\_ending})$ -- \textit{and a similar property for {glucose\_reply}}\\

            \bottomrule
            \end{tabular*}
    \end{center}
    \vspace{-2em}

\end{table*}

\noindent
\textbf{RQ3 }
This research question addresses the performance of a dynamic adaptation of a property at runtime. Thus, we compare the runtime efficiency of a \textit{dynamic adaptation} based on our PAP and a \textit{redeployment} of an observer. A redeployment comprises invoking the destructor to free up the memory consumed by the observer and the constructor to instantiate and represent the new observer in freshly allocated memory. 
For this experiment, we use the Response Chain property shown in Eq.~\ref{eq:req1}. For requirements changes, we alternate between adding and removing responses from the chain as well as updating response events in the chain. Such changes can easily be repeated multiple times on an observer to achieve reliable time measurements. For one run, we alternate between the changes until each of them is performed $1,000$ times resulting in a total of $3,000$ changes that are either realized by $3,000$ dynamic adaptations or $3,000$ redeployment of the observer. We repeat both runs ten times. The runs are all executed on Arduino.

On average across the ten runs, the $3,000$ dynamic adaptations of the observer took in total $3.034s$ (stdev $\mu_1$\,$\leq$\,$0.49ms$). 
Thus, one dynamic adaptation of an observer takes on average \textit{1.01ms}. 
In contrast, the $3,000$ redeployments of the observer took on average $15.330s$ (stdev $\mu_2$\,$\leq$\,$0.85ms$), that is, on average \textit{5.11ms} for one redeployment of an observer. 

\begin{boxA}
    A dynamic adaptation of an observer ($1.01ms$) is more than five times faster than a redeployment ($5.11ms$). 
\end{boxA}

\noindent
\textbf{RQ4 }
For the last research question, we investigate how our approach of dynamically adapting observers to reflect requirements changes can increase the trustworthiness of a SAS. To this end, we use a port of the BSN artifact~\cite{BSN} we implemented for the Arduino platform.
Starting with an initial situation of the BSN described by the requirement discussed in Section~\ref{subsec:initialzingRV} and formalized by the MTL property in Eq.~\ref{eq:req1}, we consider a sequence of five requirements changes shown in Table~\ref{tab:scenarios}. For each requirements change, the table shows the PAP to specify the adaptation of the property/observer and the MTL property after adaptation. We execute the BSN alongside our runtime verification approach and use the PAP to specify and perform the dynamic adaptations of the observer to reflect sequentially these five requirements changes in the verification.

With this demonstration of our approach, we show that using PAP allows us to specify adaptations of observers with precise semantics as shown by the corresponding MTL properties before and after an adaptation (cf.~Table~\ref{tab:scenarios}). Such adaptations of observers are dynamically and safely performed so that our approach preserves the knowledge---in terms of intermediate verification results as progress made in the observer---without compromising the integrity of the observer. 
For the given property that is adapted (Table~\ref{tab:scenarios}), the knowledge preserved in the observer comprises whether a scheduler cycle has already started and if so, which of the sensors already have and which still need to send data to the BodyHub. 
Thus, our approach achieves an incremental, continuous verification of the currently executing scheduler cycle against the adapted property.
Without preserving this knowledge (e.g., by a redeployment), the currently executing scheduler cycle remains unverified against the adapted property as the observer is reset to its initial state where it expects a novel cycle to start (cf.~\textit{cycle\_starting} event). In such a situation, there is no verification evidence about the safety of the BSN. 
\begin{boxA}
    Applying the PAP enables a continuous, incremental verification of the BSN that increases the trustworthiness of the BSN when requirements changes occur. 
\end{boxA}

\noindent
\textbf{Discussion}
In our evaluation, we have shown the efficiency of our observer-based runtime verification ($0.13ms$ to process a monitored event and at most $1.136$ bytes to represent a monitor) and adaptation of properties based on PAP ($1.01ms$ to dynamically adapt an observer). Thus, our approach can efficiently be used on microcontrollers such as Arduino. 

Moreover, we have shown empirically the correctness of our observer-based runtime verification using a benchmark based on Timescales~\cite{ulus2019timescales} as ground truth. Since there is no ground truth for verifying a running system against adapted properties, we cannot validate the correctness of our verification approach under changing requirements. Thus, we demonstrated qualitatively the benefits of continuous, incremental runtime verification on the trustworthiness of the~BSN~\cite{BSN}. 

\noindent
\textbf{Threats to Validity}
Threats to the validity of our study are as follows. 
\textit{Construct:}
Potential errors in our implementation of the observers and the Timescales grammar cause a threat to the validity of our reported results on correctness. We address this threat by having reviewed the observers and making the implementation and replication package publicly available. 
\textit{Internal:}
Threats of this category concern the experiments and measurements we conducted. To mitigate measurement errors and obtain reliable results, we repeated experiments and performed them on a SEAMS artifact ported to Arduino and on a benchmark based on Timescales that is used by runtime verification research community. 
Moreover, requirements formalized with the Structured English Grammar~\cite{AutiliGLPT15} might not match the stakeholders' intentions, which is also true for the properties/observers and eventually for the verification results. In this context, we rely on our expertise on the BSN~\cite{BSN,Solano+2019} and property specification patterns~\cite{AutiliGLPT15,vogel2023property}.  
\textit{External:}
We considered only the BSN with one requirement that changes in five ways in our study. Thus, our results may not generalize to other SAS, requirements, and changes.
Finally, our PSP/PAP catalog currently supports four PSP with different scopes and five PAP, two of which can be applied to all four PSP and three only to the Response Chain pattern. Thus, we cannot generalize our catalog to other patterns collected in~\cite{AutiliGLPT15}.
