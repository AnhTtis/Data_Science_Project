{
    "arxiv_id": "2303.10650",
    "paper_title": "Logic of Differentiable Logics: Towards a Uniform Semantics of DL",
    "authors": [
        "Natalia Åšlusarz",
        "Ekaterina Komendantskaya",
        "Matthew L. Daggitt",
        "Robert Stewart",
        "Kathrin Stark"
    ],
    "submission_date": "2023-03-19",
    "revised_dates": [
        "2023-05-25"
    ],
    "latest_version": 3,
    "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "Differentiable logics (DL) have recently been proposed as a method of training neural networks to satisfy logical specifications. A DL consists of a syntax in which specifications are stated and an interpretation function that translates expressions in the syntax into loss functions. These loss functions can then be used during training with standard gradient descent algorithms. The variety of existing DLs and the differing levels of formality with which they are treated makes a systematic comparative study of their properties and implementations difficult. This paper remedies this problem by suggesting a meta-language for defining DLs that we call the Logic of Differentiable Logics, or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and for the first time introduces the formalism for reasoning about vectors and learners. Semantically, it introduces a general interpretation function that can be instantiated to define loss functions arising from different existing DLs. We use LDL to establish several theoretical properties of existing DLs, and to conduct their empirical study in neural network verification.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10650v1",
        "http://arxiv.org/pdf/2303.10650v2",
        "http://arxiv.org/pdf/2303.10650v3"
    ],
    "publication_venue": "LPAR'23"
}