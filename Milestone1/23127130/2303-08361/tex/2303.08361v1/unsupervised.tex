\noindent
% By jointly leveraging D2D and D2S collaborations, 
Through well-designed network collaboration, we can expand the FL structure to include and incorporate devices with limited labeled data and even devices with fully unlabeled data into the FL ecosystem. 
%Such developments can be made more robust and exciting via further development of collaborative network opportunities such as server to device collaborations. 
% The key is that these micro and macro level collaborations will enable the network to more finely characterize its constituent devices. 
For instance, via secure and opt-in D2D data sharing, local devices can gain an understanding of each others' data and, with clever server-side management, the server(s) can characterize the diverse data distributions throughout the network. 
The end result is that network collaborations can enable FL-style processes for a general class of networks with unlabeled data or partially labeled data. 
We call such a generalized paradigm for federated learning as mixed-supervision federated learning ($\mathsf{MS-FL}$). 
We have two constituent methodologies that comprise ($\mathsf{MS-FL}$), the first is domain matching and large-scale federated domain adaptation and the second is unsupervised federated learning. 

\subsection{Multi-source to multi-target federated domain adaptation}
In domain adaptation, network devices are segmented into two types, those with sufficient quantity of high quality labeled data and those without. The devices with such labeled data are commonly called source devices or sources while those devices without are called target devices or targets. The goal is to match the source devices to the target devices, perhaps multiple sources to a single target. 
The characterization of the data distribution and the label space at a device is referred to as its domain. 
These sources devices will transfer their local ML models to the target device(s), which will then combine these ML model parameters in a weighted fashion to determine its resulting ML model. Because there is a lack of D2D collaboration in existing literature for this problem, currently methodologies typically rely on either generative adversarial networks (GAN) to iteratively develop and refine the source-to-target weights~\cite{yao2022federated}. However, GAN-based methods present a very heavy computational burden on the network, and often lead to long delay periods where devices' underlying data distributions may shift during this time. 

We proposed a novel methodology using network optimization approaches to deal with this problem in large-scale networks. Our methodology is called source-target matching in federated learning ($\mathsf{ST-LF}$), and it relies on D2D cooperation to aid in both the training and the source-target matching. 
In diverse network environments, where devices can range from having fully labeled, partially labeled, and fully unlabeled datasets, $\mathsf{ST-LF}$ first determines the classification of devices as sources or targets based on a combination of quantity and quality of labeled data, ML model training energy costs, and communication spectrum availability. Thereafter, leveraging D2D links and collaboration, $\mathsf{ST-LF}$ uses a data sensitive method to develop a binary classifier that characterizes devices' underlying data similarities. %domain
ST-LF then uses this information to optimally match source domains to target domains. 
% In this manner, 
Thus D2D collaboration broadens the scope of federated learning architectures and structures to include diverse network environments with varying levels of devices' labeled data.

% \subsubsection{XXX}

% \subsubsection{XXX}

\subsection{Cooperative federated unsupervised contrastive learning}
In the special case where network devices only have unlabeled data, we propose cooperative federated unsupervised contrastive learning ($\mathsf{CF-CL}$). 
% CF-CL focuses on unsupervised federated learning, where network devices contain only unlabeled data and their underlying data distributions are non-iid. 
% To do so, CF-CL utilizes 
Contrastive learning is able to cluster datapoints analogously to a label by developing embeddings of data in a latent space and subsequently analyzing the statistical distance between data in this latent space. % (i.e., minimizing latent distance among structurally similar datapoints while maximizing distance among structurally dissimilar ones). 
% Contrastive learning, however, is designed to function in centralized settings, where all the data is located at a single location. 
Unfortunately, contrastive learning applies poorly in heterogeneous federated networks, as local devices are likely to develop embeddings that segment data into clusters too aggressively. 
% When applying contrastive learning in a heterogeneous federated network, local devices may develop embeddings that segment data into clusters too aggressively. 
For example, a device may only have data from a single true label/cluster but it is highly likely to develop embeddings that split this cluster to fit the output layer specification of the global ML model. %, due to the output layer specifications of the ML model,
Such inaccurate embeddings and their respective ML model parameters are fed forward at global aggregations, leading to potentially network-wide impairment in ML performance. %classification accuracy.  
% Especially, non-iid distributions at local devices lead to local ML models that can split intended clusters into faulty sub-clusters as a result of contrastive learning's latent distance optimization. 
Consequently, pairing standard FL and contrastive learning together yields rather poor results in heterogeneous federated networks. 


% Careful design of intelligent cooperation can address these networking challenges. 
Through careful design of D2D cooperation, our $\mathsf{CF-FL}$ methodology overcomes the challenges of contrastive learning in federated settings. 
% Our CF-FL methodology uses careful design of network cooperation to overcome the challenges of contrastive learning in federated settings. 
% Specifically, network devices will share local data with other devices based on an iteratively updating process. two-stage probabilistic importance sampling
Our method consists of two-stage push-pull data sharing between device pairs. In the push phase, the transmitting device determines its representative datapoints - a small subset of data that best characterizes its local clusters. 
Then, in the pull phase, the receiver device will use the global ML model to determine which datapoints best minimize its local dataset bias. For the full details, which are rather involved, please see~\cite{wagle2022embedding}. 
% Initially, each device will share a subset of data with other devices to maximize the inherent data diversity within that subset. 
% Thereafter, our algorithm involves two separate major steps between each device pair, which we will call receiver and transmitter. % without loss of generality. % for this paper. 
% The first step is macro-level, where our algorithm performs K-means++ clustering using the transmitted data at the receiver combined with the receiver's local data. 
% The macro-level determines the importance of each cluster, and informs the transmitter device to emphasize data transmission of dissimilar clusters from that of the receiver. 
% The second step is micro-level, where our algorithm uses the triplet loss and selection temperature mechanics (details in~\cite{XXX}) to further ensure that the transmitter sends the most insightful data within each cluster to the receiver.
Our resulting algorithm shows significant improvements to unsupervised model classification, especially in environments with extreme network and data heterogeneity. 

% UFL utilizes D2D collaboration to compute triplet losses, which subsequently allows triplet loss matching to determine the underlying statistical similarity of data at various devices. We develop novel theory in this area of research and formulate an optimization problem that enables energy efficient data sharing and unsupervised ML training at all local devices. 

There remain many open research questions in extending $\mathsf{MS-FL}$, as further research on both $\mathsf{ST-LF}$ and $\mathsf{CF-CL}$ have value in federated settings. 
We outline a few more promising ideas here.
% Future avenues of research in extending $\mathsf{MS-FL}$ include ideas such as:
\subsubsection{Expanding D2D cooperation for domain adaptation} %in $\mathsf{ST-LF}$}
Currently the D2D cooperative aspect of $\mathsf{ST-LF}$ is constrained to the sharing and communication of D2D domain classifiers. 
Inclusion of carefully crafted data sharing schemes can improve the source-to-target matching and weighting, and thus yield better performances. 
% Expanding the type of D2D collaboration to include data sharing, and multi-hop D2D domain classifiers can potentially lead to better source-target domain matching weights. 
% \subsubsection{}

\subsubsection{Edge server caching for unsupervised federated learning}
Edge servers, routers, and network infrastructure can aid unsupervised federated learning and $\mathsf{CF-FL}$ by acting as data caches. 
Possibly, by sampling data from across edge devices, some edge servers can develop representative datasets that can be broadly applicable and minimize local bias formation for the entire network. 
These caches can then supplement the D2D cooperation in $\mathsf{CF-FL}$.

% Specifically, network elements can act as network caches, holding representative local device data subsets. 
% This firstly absolve the need for devices to locally perform computations to determine macro and micro data importance sampling for transmission to other devices. Especially in large networks, have each device compute macro and micro data importance sampling for itself relative to all other network devices can become unmanageable. 
% Secondly, such caches can form pictures of larger environments, i.e., more reliable data clustering at earlier training iterations. 

\subsubsection{Joint supervised and unsupervised federated learning}
% Joining supervised and unsupervised federated learning environments together.
By integrating the $\mathsf{ST-LF}$ and $\mathsf{CF-FL}$ frameworks together, we can transfer the insights from supervised learning to unsupervised learning in federated settings. 
Essentially, the transferred models from labeled source domains can match with the developed clusters at target domains. This not only allows matching cluster outputs to label outputs, but it also acts as a counterbalance to prevent devices (target domains) from segmenting their local data into too many clusters. 
Thus, there is potential to lead to general frameworks of FL that can adapt to varying types of real world network settings. 


% \begin{enumerate}
%     % \item Less restrictive device-to-device network collaboration for ST-LF. Currently the D2D cooperative aspect of ST-LF relies on the sharing and communication of private D2D domain classifiers. 
%     % Expanding the type of D2D collaboration to include data sharing, and multi-hop D2D domain classifiers can potentially lead to better source-target domain matching weights. 
%     \item Edge server assisted federated domain adaptation. By enabling edge server and device cooperation, we can develop representative models at the edge servers or cache representative data samples of diverse types of data. This would then allow for the characterization of local device data distributions, and yield improvements of ML model classification in ST-LF. 
%     \item Communication and energy efficiency for CF-FL. In this first stage, our preliminary results have demonstrated a clear ML performance gain. 
%     It is possible that maximizing ML performance gain can result in restrictive energy consumption or communication resource utilization. 
%     As a result, it would be of value to characterize the real networking trade-offs in CF-FL. 
%     % \item Using network elements, such as routers and edge servers, to enhance CF-FL. Specifically, network elements can act as network caches, holding representative local device data subsets. 
%     % This firstly absolve the need for devices to locally perform computations to determine macro and micro data importance sampling for transmission to other devices. Especially in large networks, have each device compute macro and micro data importance sampling for itself relative to all other network devices can become unmanageable. 
%     % Secondly, such caches can form pictures of larger environments, i.e., more reliable data clustering at earlier training iterations. 
%     % \item 
% \end{enumerate}


% unsupervised FL $\rightarrow$ generalized FL in mixed-composition datasets  


% \begin{enumerate}
%     \item mixed-composition datasets [extending multi-source to multi-target domain adaptation to a more extreme case]
%     \item adversarial attacks - especially poignant because labeled data and hence gradients are scarce 
%     \item Crowd sourcing labeled data [mislabeled data (subcategory of adversarial attacks)]
%     \item Game theoretic design of crowd sourcing labelling of data
%     \item few shot learning on top of unsupervised to see the accuracy
% \end{enumerate}
