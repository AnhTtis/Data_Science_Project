\noindent
Edge servers, especially in large-scale edge/fog networks, offer an untapped resource in standard FL. 
{\color{black}D2S cooperation aims to facilitate efficient utilization of these resources, e.g., by enabling devices with limited computation capabilities to transfer local training data to edge servers. In doing so, edge servers can leverage their powerful and efficient processors to improve energy efficiency and training delay of ML model training, thus enhancing the trade-off between energy consumption and ML performance.}
% D2S cooperation can unlock this potential by enabling devices to transfer their training data to edge servers, which can leverage their more powerful and efficient processors to provide energy efficient and faster ML model training, or use edge server to efficiently route ML models during aggregations. 
% Furthermore, D2S cooperation can use these edge servers to more efficiently route ML models during aggregations. 
In the following, we introduce a set of core technologies that enable effective D2S cooperation, and thereafter explain complementary future technologies to further enhance it. %on complementary technologies to further 

% Device to server (D2S) collaborations also offer an untapped resource a new dimension of collaborations. Even in standard FL involving a single server connected to a range of IoT devices in a star topology, there are many opportunities for D2S collaboration beyond the existing aggregation design. 
% The server can play a more active role than simply aggregating and synchronizing ML model parameters. For example, network devices can designate a subset of their data to transfer to the central server for ML model training. The central server here would train a local ML model based on the data transferred to it. Such a procedure reduces the total time spent training ML models by the network, and reduces the total energy spent by the network to train an ML model, as the server has powerful local processors to process data in a more energy efficient way. 

% More generally, in large-scale edge/fog networks, many edge/fog servers may exist, scattered at key points throughout the network to improve communication routing. Not only can these edge servers function as an additional computational resource but they can also introduce new network topologies to enhance standard federated learning. These edge/fog servers typically cover a limited geographical region of the network, and as such, can capture the statistical information about its specific domain of responsibility. Once connected to the central server, these edge/fog servers can act as statistical snapshots and decisions regarding global aggregation methodologies. For example, if some statistical information is over-represented then their contributions can be down-scaled at aggregation instances or if some geographical regions have vastly more powerful devices (i.e., those devices can run more training epochs per unit time) then again their contributions to the global model can be down-scaled as in~\ref{PSL}. Proper design of D2S collaborations can thus provide multi-dimensional benefits. 


\begin{figure}[t]
\includegraphics[width=.48\textwidth]{pix/ce_fl.pdf}
\centering
\caption{{\color{black} On both Fashion-MNIST and CIFAR-10, our method based on  D2S collaborations enable both energy and time savings during the ML model training process.}}
\label{fig:D2S_result}
\vspace{-3mm}
\end{figure}

%Multi-Edge Server-Assisted Dynamic Federated Learning with an Optimized Floating Aggregation Point

\subsection{Core Techniques for Effective D2S Cooperation}
% \subsection{Multi-edge server-assisted dynamic federated learning}
Edge servers can have a diverse set of functionalities when assisting ML. They can act as computational resources to train ML models, as communication gateways to reroute ML models during global aggregations, and as the model aggregation points. 
% These use cases all involve server and edge device cooperation to improve the resource consumption of ML model training and the performance of the ML model, and we propose novel technologies to enable such cooperation. 
Starting at the computational level, we propose a novel technology called load balancing of data processing tasks, that relies on data offloading from edge devices to edge servers. Then, we propose efficient ML model parameter and data routing from edge devices to edge servers using base stations and network routers. Finally, we develop a concept called floating aggregation point, a method to optimize the selection of the aggregation server to save communication resources and minimize communication delay. 
% In standard FL, one edge server is selected as the central server by default, though this process has yet to be properly defined in literature. 
% Our recent methodology called cooperative edge-assisted dynamic federated learning ($\mathsf{CE-FL}$) explores the potential of using edge servers and network infrastructure to improve the base processes inherent to FL and extend FL to time-varying and non-stationary edge/fog networks~\cite{ganguly2022multi}. 
% In such edge/fog networks, not only are underlying data distribution at edge devices time varying but edge devices also have mobility, so that geographical proximity to edge servers and other network devices is time-varying as well.
% To these ends, $\mathsf{CE-FL}$ optimizes D2S collaborations throughout the network and dynamically re-adjusts the aggregation server at every aggregation, 
We present a visual summary of these new technologies in Fig.~\ref{fig:D2S}, and explain them in detail below. 


\subsubsection{Load Balancing of Data Processing}
In standard FL, edge devices perform all of the computationally intensive ML model training tasks. We propose a novel technology of load balancing for data processing tasks in order to make use of the computational power at edge servers, similar to mobile edge computing frameworks in large-scale edge/fog networks~\cite{kaewpuang2013framework}. 
As part of load balancing, resource-constrained edge devices have the option to transfer a subset or all of their local data to nearby base stations (or road-side units), which through efficient data routing (another innovation which we explain next) relay the data to one or many edge servers. 
% Once these edge servers complete the ML model training, 

\subsubsection{Efficient Data and Parameter Routing}
To enable D2S cooperation, we propose efficient data and model parameter routing through the use of routers/switches as shown in Fig.~\ref{fig:D2S}. Through a combination of base stations and routers/switches, we can finely control the routing of data and ML models based on communication factors, such as channel congestion (a base station may be serving many users), and systems factors, such as computation power availability (an edge server may be running intensive data backups). 
This fine-grained control of data and ML model routing enables energy efficient and fast ML training. 
% networks to train ML models with greater time and energy efficiency. 

\subsubsection{Floating Aggregation Point}
In scenarios with many edge servers, we can improve resource efficiency by dynamically selecting the aggregation server. %the choice of aggregation server becomes an open question. 
Specifically, we propose floating aggregation point, a novel technology that adjusts the aggregation server based on changing edge/fog network properties. 
% To address this, we propose a novel technology called floating aggregation point, where the aggregation server changes over time based on the edge/fog network's properties.
As shown in Fig.~\ref{fig:D2S}, the choice of global aggregation server changes in response to devices' positions and dataset sizes, which influence the total communication/computation resource consumption for ML model training and parameter aggregation (uplink) and broadcasting (downlink). % as well as the communication delay at aggregation 
% The benefits of D2S collaboration and edge server inclusion become more apparent in time-varying and non-stationary networks, where flexible aggregation servers can lead to communication resource savings and reductions to network aggregation delay. 


% \subsubsection{D2S Cooperation Driven Optimization of ML Performance, Energy Consumption, and Delay}
% By combining the above three proposed technologies, we can jointly control D2S cooperation characteristics (e.g., data or ML model offloading ratios) to influence ML performance, energy consumption, and delay. 

{\color{black}We have taken initial steps towards formalizing such a D2S cooperation methodology in~\cite{ganguly2022multi}, including corresponding mathematical formulations and a proof-of-concept testbed implementation.
To demonstrate the potential benefits, we compare our method to FedAvg~\cite{kairouz2021advances} and Nova~\cite{wang2021novel} on two common benchmark datasets in FL literature: Fashion-MNIST (clothes) and CIFAR-10 (common objects) in Fig.~\ref{fig:D2S_result}. In this experiment, we consider a network of 20 edge devices and 10 edge servers training a two layer CNN. Other system parameters, such as the wireless channels and edge server links, can be found in~\cite{ganguly2022multi}.}
% {\color{black}To evaluate the above proposed techniques, we compare our method to FedAvg~\cite{kairouz2021advances} and Nova~\cite{wang2021novel} on two common benchmark datasets in FL literature: Fashion-MNIST (clothes) and CIFAR-10 (common objects) in Fig.~\ref{fig:D2S_result}. In this experiment, we simulate a network with 20 devices and 10 edge servers as it trains a two layer CNN. Other network parameters such as wireless channels and edge server links model are chosen similar to~\cite{ganguly2022multi}.} 
% {\color{black}We compare our D2S cooperation methodology against Nova for the same reasons as those for D2D cooperation.} 
This shows that CFL obtains substantial improvements over both baselines in terms of energy and time savings for the same target ML performance. 
% Together, our proposed technologies enable the control of D2S cooperation to jointly optimize ML performance, energy, and delay that is more effective than existing methods. 
% {\color{black} We have taken initial steps toward formalizing D2S cooperation methodology in~\cite{ganguly2022multi}, which includes mathematical formulation and an implementation methodology.}



% where we explain additional techniques to determine device-specific mini-batch sizes for ML model training, CPU clock frequency, and even heterogeneous training rounds per aggregation.

% We show an experimental example in Fig.~\ref{fig:D2S_result} of the combined effects of our D2S enabling technologies on two commonly used ML datasets - Fashion-MNIST (images of clothes) and CIFAR-10 (images of general objects) - relative to two baselines - FedAvg~\cite{kairouz2021advances} and FedNova~\cite{wang2020tackling}. 
% In Fig.~\ref{fig:D2S_result}, we see that D2S cooperation for FL can obtain substantial improvements both in energy and time savings for the same target ML performance. 

\subsection{Future Development of Complementary Technologies}
% While contributions of $\mathsf{CE-FL}$ are already substantial, 
D2S cooperation can be further extended  to enhance the existing trade-offs in FL and CFL depicted in Fig.~\ref{fig:main_diagram}. 
{\color{black}The following outlines a few open research directions:} 

% \subsubsection{Edge device mobility beyond the network}
% While the system model for $\mathsf{CE-FL}$ assumes that edge devices can move throughout the network, such devices may be able to move beyond the network~\cite{savazzi2021opportunities}. In such scenarios, the overall network-wide data distributions may shift dramatically. D2S collaboration must be further developed to account for such network boundaries.

\subsubsection{Non-stationary servers}
Given current trends leveraging unmanned aerial vehicles (UAVs) as mobile communication servers (e.g., at sporting events), a natural next step for D2S collaboration involves non-stationary edge/fog servers~\cite{savazzi2021opportunities}. 
{\color{black}Since mobile servers' locations can be controlled, efficient server placement methods can be pursued to improve data/model routing and offloading.
These methods should carefully investigate the trade-offs involved in server positioning. 
For example, placing a server near a dense neighborhood of devices may ease aggregation delay, but placing a server near a few resource-scarce devices may save more computation energy.}
% {\color{black}Since the location of such mobile servers can be controlled, efficient edge server placement techniques need to be developed to improve data/model routing and offloading and model aggregate delay.}
% Such a technology would need to investigate the tradeoffs between edge server positioning, in particular the tradeoff between placing an edge server near many edge devices versus positioning near the most resource-scarce edge devices. Placing edge servers near resource-scarce devices may ease the computational burden of training ML models but lead to more aggregation delay, for example. 


% \subsubsection{Geographically independent servers}
% In large-scale edge/fog networks, edge/fog servers can be separated by distances that prevent stable and reliable communication links between them.
% In this scenario, there are two interesting problems to consider: (i) stationary devices and (ii) mobile devices. When devices are stationary, communication between edge servers may require multiple rounds of communications, such as from an edge server to a series of routers before reaching the aggregation server. Such a system model would need to be developed and subsequent ML model training convergence results need to be established.  
% For mobile devices, we envision a new technique called device-aided aggregations. In device-aided aggregations, each edge server first performs a partial aggregation with devices within their server zone, similar in concept to a cell in cellular networks, and subsequently sends these partially aggregated ML model parameters to edge devices within their server zone. As devices physically travel to different server zones, they will send their partially aggregated parameters to other edge servers along the way. 
% A trade-off in this scenario would be the delay vs communication resource consumption during global model aggregations, and further study to optimize this trade-off would be needed. 

% may be so far apart that maintaining a stable and reliable link between them or network routers is simply infeasible. In this case, while devices may be able to travel between servers' zone of influence, the servers themselves are fully independent and would need to rely on incoming/outgoing devices or intermediary network elements to develop a unified global model. Such a problem could be addressed by consensus formation among edge devices to conclude training, but further study is needed. 

\subsubsection{Joint D2D and D2S collaboration} 
Frameworks combining D2D and D2S can yield further benefits to ML performance, resource consumption, and time efficiency.
{\color{black}Such frameworks can enable simultaneous D2D and D2S data/model offloading. For example, as an edge device offloads data to another device, this secondary device could simultaneously can offload data to an edge server. This combined D2D and D2S cooperation ensures that (i) resource-abundant edge devices do not become overburdened, and (ii) resource consumption of data offloading (i.e., energy and delay) is optimized.}
% {\color{black} Such frameworks could enable simultaneous offloading, a new technique where both D2D and D2S data offloading occur at the same time.} 
% In simultaneous offloading, a resource-scarce edge device offloads data to a resource-abundant edge device which simultaneously offloads data to an edge server. This combined D2D and D2S cooperation would ensure that (i) resource-abundant edge devices does not become overburdened, and (ii) the data offloading delay is minimized. 

\subsubsection{Inclusion of unlabeled data} %Move contrastive learning here}
D2S cooperation can also be used to extend FL to edge/fog networks with fully unlabeled data, such as autonomous driving where camera-equipped cars take images without labels. One possible approach is to extended the well-known concept of contrastive learning~\cite{chen2020simple} to FL. %
Standard contrastive learning differentiates among datapoints in centralized settings via determining their similarities and differences. 
% determines similarities and differences among datapoints over a large centralized training dataset to differentiate between them. 
However, in federated settings, edge devices' datasets may simply be too small or lack sufficient data for standard contrastive learning to be effective. %tandard contrastive learning can have difficulty determining the key features of data on small and/or highly heterogeneous datasets, such as those at edge devices. 
% However, these existing methods apply poorly to federated settings as devices may have small and/or highly heterogeneous (i.e., non-i.i.d. distributed) datasets, which means there may be insufficient quantity and quality of local data to identify structural similarities and differences within the data. 
Through D2S collaboration, contrastive learning can be enabled in FL by leveraging edge servers as caches of data. %gathered across edge devices
As caches, edge servers can then supplement edge devices' local datasets with their cached data so that the compare and contrast steps of contrastive learning are feasible/effective in FL. 
% to enhance the performance of contrastive learning methods in federated settings. 
% statistically homogeneous datasets. In 



% \begin{enumerate}
    % \item Server infrastructural duties. In practical edge/fog systems, edge/fog servers will handle additional infrastructure-related tasks such as device-to-network handshaking, or management of physical layer protocols. Such tasks will require computational burden on the edge/fog servers as well, and effective management of ($\mathsf{CE-FL}$) without disruption to the physical layer remains an open problem. 
    % In the extreme case, a subset of the network's edge/fog servers may fail completely. The optimal decision may be to either reroute IoT devices under the failed/overloaded server's region of influence to other functioning or low-use servers, which are available but may be less computationally powerful or incur significant delay, or to simply continue performing local SGD epochs until said server is functional again. Analysis to determine optimal decisions in such a scenario have yet to be performed as well. 
    %Time-varying server loads [infrastructural computational burdens - not machine learning related]: rerouting; extreme case - server failures [actually quite common in above ground datacenters]
    % \item Mobile servers. Especially given the current trends towards leveraging UAVs as mobile communication servers for IoT devices (e.g., at college football games), mobile edge/fog servers can have multiple potential impacts and benefits on ($\mathsf{CE-FL}$). Specifically, there are two core scenarios: (i) independent mobile servers and (ii) network controlled mobile servers. In the former case, data routing and offloading can be unstable as the mobile servers are not under direct control of the FL process. Furthermore, as these servers travel, they may distort the statistical properties of newly managed set of devices by biasing their collective model towards their previous managed set of devices. So the impacts of independent mobile servers need to be theoretically characterized. When the mobile servers can be controlled, there are server placement and trajectory design problems to consider, which have both statistical and network influences.   
    % \item personalization 
    % \item Fully independent servers. In sufficiently geographically vast scenarios, edge/fog servers may be so far apart that maintaining a stable and reliable link between them is simply infeasible. In this case, while devices may be able to travel between servers' zone of influence, the servers themselves are fully independent and would need to rely on incoming/outgoing devices to develop a unified global model. Such a problem could be addressed by consensus formation among edge devices to conclude training, but further study is needed. 
    % fully decentralized servers/multiple server layers; similar to the sequential server traveling seen when running traceroute [e.g.local laptop/machine (edge) $\rightarrow$ purdue server $\rightarrow$ chicago $\rightarrow$ toronto $\rightarrow$ local toronto server $\rightarrow$ local edge device]
    % \item Joint D2D and D2S network cooperation. The current design only considers D2S cooperation, with D2D cooperation and server-to-server (S2S) cooperation a future task. Together, this three dimensional network cooperation can yield further benefits to ML performance, resource consumption, time efficiency and significant challenges   %server to server privacy
% \end{enumerate}