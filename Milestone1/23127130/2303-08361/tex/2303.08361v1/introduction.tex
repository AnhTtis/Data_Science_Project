\noindent 
{\color{black} Recently, much attention has been given to the implementation of data analytics and machine learning (ML) techniques at the network edge to handle the complexity of emerging Internet of Things (IoT) services, ranging from user-oriented (e.g., object recognition) to network-oriented (e.g., signal classification) applications~\cite{salau2022recent}.} 
IoT devices are now capable of gathering data from various sources, connecting to the internet, and performing computation tasks. 
Collectively, they form edge/fog networks capable of producing machine intelligence insights. %can be developed by the collaboration of many such IoT devices. - only through cooperation
% that are greater than the sum of their parts, as 
%are created by the joining of these IoT devices together.  
% Together, these devices are greater than the sum of their parts form IoT networks that yield useful analytics and machine intelligence insights
% Formed by large quantities of such IoT devices, IoT networks are notable for having many individual edge device each contributing relatively little but together producing useful analytics and machine intelligence insights. 
{\color{black}Traditionally, data insights were produced via centralized computing, where network devices send all of their local measurements to a single central server for ML training. Such methods led to system-wide latency and resource inefficiencies as a result of data transmissions from edge devices to the server, for centralized ML tasks specifically. These limitations have led to the emergence of distributed ML techniques and in particular federated learning (FL).}


{\color{black}Standard FL shifts the processing portion of ML training from the server to the edge/fog devices~\cite{kairouz2021advances}. As shown in the upper left corner of Fig.~\ref{fig:main_diagram}, it involves a ``star" server-to-device communication topology, inside of a three-part cyclical process: (i) edge devices independently and locally train an ML model, (ii) ML models are sent to the central server for global aggregation, and (iii) the server synchronizes devices' ML models into an aggregated ML model called the global model.}
% The global aggregation mechanism of standard FL is the only form of network cooperation among network elements. 
While standard FL features global aggregations, this is the only form of cooperation among network elements. 
Inter-device and inter-network communications - key features of IoT networks - can also facilitate cooperation and are  missed opportunities in FL. For example, direct device-to-device (D2D) communication links that are otherwise underutilized could be employed for faster and communication-efficient ML model training~\cite{su2021secure}. 
%the underutilized network links in FL can be leveraged for 
% This leads to underutilized network links such as for device-to-device (D2D) cooperation - a missed opportunity that is especially pronounced in IoT networks built on the power of inter-device and inter-network collaborations.  
% In standard federated learning, this process is the only form of edge device and/or network cooperation, and results in underutilized network resources and links. 
Traditional FL therefore does not exploit the full potential of cooperation in large-scale edge/fog networks. %on large-scale edge/fog networks are therefore not exploiting the full potential of their network. 

% Furthermore, standard FL assumes that all network devices contain labeled data. Even recent work, with more realistic models of device heterogeneity, maintain this assumption.
% In realistic edge and fog networks, the quantity and quality of labeled data at devices is also heterogeneous, with devices' datasets ranging from fully unlabeled to fully labeled. %having varying levels of labeled data %should also be expected
% Current state-of-art FL methods cannot adapt to such generalization, as devices with unlabeled or poorly labeled datasets may have unique data distributions. 
% Cooperative FL among network devices enables new methods to align ML models at devices with labeled datasets to fit/approximate the data distributions at devices without labeled datasets. 
% To summarize, cooperative FL enables network operators to exploit the full potential of edge and fog networking, leading to a generalization federated learning with new dimensions of possibilities. 
% % key point: Cooperative FL yields generalized FL

% \begin{figure*}[t!]
% \includegraphics[width=.9\textwidth]{pix/main_diag_broken_oval.pdf}
% \centering
% \caption{Characterization of cooperative FL on four fundamental fronts: (i) ML performance, (ii) energy efficiency, (iii) temporal sensitivity, and (iv) data and model privacy. Considering each component in isolation leads to a one dimensional improvement of federated learning; considering components jointly in FL design leads to a new dimension of improvement - cooperation - beyond the fundamental four dimensions. 
% For example, at the lowest level, device-to-device (D2D) cooperation enables all four base dimensions by allowing efficient data processing and straggler handling to improve ML model performance, sharing agreements that add sophistication to a simple privacy framework, and device sampling to save on total time to convergence and energy efficiency. 
% Thereafter, we better exploit edge and fog networks by building on this new dimension of network-aware cooperation. We transition from just D2D cooperation to device-to-server (D2S) cooperation and mixed-composition datasets, building towards an ultimate goal of network-aware cooperative FL. Add in the sections where we talk about specific aspects}
% \label{fig:main_diagram}
% \end{figure*}

\begin{figure*}[t!]
\includegraphics[width=.98\textwidth]{pix/main_diag_seq.pdf}
\centering
\caption{{\color{black} The progression from standard FL towards network-aware cooperative FL. Starting from standard FL and its associated trade-offs, we develop the pillars of cooperative FL frameworks, namely device-to-device and device-to-server cooperation. 
We envision that future work towards integrating collaboration into FL involves ideas such as multi-hop cooperation, integration of unlabeled data, and heterogeneous privacy.}}
\label{fig:main_diagram}
\vspace{-3mm}
\end{figure*}

% {\color{black}We propose cooperative federated learning (CFL), a cooperative edge/fog paradigm that jointly orchestrates the resources at devices, servers, and network infrastructure to enhance FL and its core trade-offs as shown in Fig.~\ref{fig:main_diagram}.}
{\color{black}We propose cooperative federated learning (CFL), a cooperative edge/fog ML paradigm that jointly orchestrates device, server, and network infrastructure resources to enhance FL while considering its core trade-offs, as shown in Fig.~\ref{fig:main_diagram}.}
{\color{black}CFL extends the notion of cooperation to address the key missed opportunities in standard FL, which are summarized below:}
% in standard FL to address a set of key missed opportunities summarized below:} 
\begin{enumerate}
    \item Edge devices with powerful local processors or small local datasets idly wait for network stragglers to finish training~\cite{reisizadeh2022straggler}.  %complete their ML training tasks
    \item Powerful network infrastructure elements such as edge/fog servers are underutilized in FL~\cite{mai2022federated}. %are left idle in the FL process
    \item Edge devices without direct connectivity to the central server are neglected during ML model training and synchronization processes~\cite{kairouz2021advances}. %\cite{li2020federated}. 
    \item IoT devices, which may have diverse privacy requirements~\cite{lin2021friend}, are all discouraged from sharing data/models over the network. %and ML tasks
\end{enumerate}
These shortcomings are the result of prohibiting powerful devices, idle edge servers, and network infrastructure from helping computationally weak and/or overburdened devices in FL. 
% At the same time, edge/fog environments are notable for containing the above statistical and structural heterogeneity at a large-scale~\cite{li2022data}. 
{\color{black}Simultaneously, D2D and device-to-server (D2S) cooperation over such edge/fog networks has been shown to be feasible and beneficial to learning processes~\cite{kar2023offloading}.}
Well-designed cooperation mechanisms can thus unlock the full potential of edge/fog networks for FL, leading to (i) improved ML performance, (ii) energy efficiency, (iii) temporal efficiency (e.g., faster ML training), and (iv) diverse data/model privacy. %a broader range of data/model privacy management. 
%.e., less idle time during model aggregations, faster data processing, etc.)

\section{Cooperative Federated Learning}
% \noindent To maximally exploit network resources to improve FL, edge/fog networks should leverage underutilized network elements (i.e., devices/servers/infrastructure) to help overloaded ones. %shoulder the burden of 
\noindent 
{\color{black} We propose cooperative federated learning (CFL), a novel paradigm that expands the dimensions of cooperation in FL beyond global aggregations. CFL develops inter-element cooperation mechanisms including selective data sharing, computation resource sharing, ML model sharing, and data distribution comparisons. 
FL driven by such multi-faceted cooperation better exploits the availability of links among network devices, servers, and infrastructure in contemporary edge/fog systems. Through such links, CFL unlocks the potential of cooperative edge/fog networks for ML.} 

% {\color{black} Unlike the state-of-the-art in FL~\cite{kairouz2021advances}, which has thus far only leveraged data offloading to improve the model training of FL, our vision for CFL involves a broader look at cooperation in FL, including D2D- and D2S-driven data, model, and resource cooperation.} 
{\color{black}Whereas the state-of-the-art in FL~\cite{kairouz2021advances} has mostly considered data offloading as a mechanism for improving local statistical properties in FL, our vision for CFL involves a broader look at cooperation in FL, including D2D- and D2S-driven data, model, and resource cooperation.}
{\color{black}
These proposed cooperation technologies aim to improve the balance among the trade-offs in FL shown in Fig.~\ref{fig:main_diagram}. For example, intelligent cooperation can lead to better ML model performance with less energy usage and system delay.}
% These proposed cooperation technologies can change the balance of the trade-offs of FL, shown in Fig.~\ref{fig:main_diagram}. For example, cooperation can lead to better ML model performance with less energy usage and system delay.}

% We term such inter-element assistance as cooperation, which encompasses data sharing, computation resource sharing, localized ML model transfers, data distribution comparisons, and aggregation scaling. 
% FL driven by such cooperation better exploits the links among network devices, servers, and infrastructure and through such links unlocks the potential of cooperative edge/fog networks. We call this paradigm cooperative federated learning (CFL). 




Specifically CFL (i) leverages D2D links for data and model sharing, which we term D2D cooperation, and (ii) leverages D2S links to incorporate edge servers, routers, and other network infrastructure into the FL ecosystem through data processing and ML model transmission tasks. 
{\color{black} We consider data transfers in CFL noting that while some applications of FL (e.g., healthcare analytics) discourage data sharing, other applications have milder data privacy restrictions, especially when the data is generated with ML as the primary purpose (e.g., FL for self-driving vehicles with sensor measurements).}


Combined, D2D and D2S cooperation form the two pillars of CFL, which together enable many complementary technologies,  we only examine a few of which for brevity. As depicted in Fig.~\ref{fig:main_diagram}, we examine (i) multi-hop cooperation due to its key influence on improved resource efficiency (i.e., energy efficiency and temporal sensitivity), (ii) integration of unlabeled data as it enhances ML performance for devices, and (iii) devices heterogeneous privacy demands because it focuses on the data/model privacy aspects of CFL.  % both devices with and without labeled data

% ML performance - devices with unlabeled data 
% Resource efficiency (energy and delay) - multi-hop cooperation 
% Data/model privacy - heterogeneous privacy 
% A visualization of our sequence is in Fig.~\ref{fig:main_diagram}. %enable devices with partially labeled or unlabeled datasets to contribute to FL 

% CFL build upon two salient characteristics, we present sota and demonstrate our relative advantages. additioanlly, we present a roadmap for three complementary technologies that will be enabled by these two characteristics 
% Design considerations for emerging applications of fog learning that are going to rely on D2D and/or D2S cooperation 
% We demonstrate the utility of these two characteristics by discussing three complementary technologies 


% Specifically CFL has the following properties: %addresses these missed opportunities by: 
% \begin{enumerate}
%     \item Leverages D2D links between device pairs for data and model sharing. 
%     % This effectively re-balances FL tasks proportionally to edge device capabilities. The overall effects are reduced straggler effector s from both data processing and model transmissions at resource poor devices, increased total network data processed per global aggregation, reduced overall network energy consumption per datum, and faster global aggregations. 
%     \item Incorporates edge servers, routers, and other network infrastructure into the FL ecosystem through data processing and model handling tasks. 
%     % Network infrastructure elements offer value both as computational resources, aggregation gateways, and flexible data caches. 
%     % As computational resources, such elements can cooperate with edge devices by receiving their data and subsequently training on server, when needed. 
%     % These network infrastructure elements often contain more dedicated and powerful communications equipment, so they are well placed to receive and transfer the ML model parameters from many nearby edge devices. This reduces overall network communication interference and mitigates channel congestion due to less long distance edge device communications with a single single central server. Such functionality also yields reduces idle time from ML model communication delays and saves overall edge device energy consumption, as there are less long distance transmissions overall.   
%     % % They also reduce the communication delay and energy consumed during ML model transmission. 
%     % Furthermore, they act as local data caches, which can carry globally representative distributions of high quality data, to lessen the impact of non-iid data across the network. 
%     \item Enables devices with partially labeled or unlabeled datasets to contribute to FL. %Integrating
%     % Devices with partially labeled and unlabeled data may have different data distributions and model training needs than their labeled counterparts. 
%     % % Consequently their ML model and training needs will differ from those devices with labeled data as well. 
%     % Standard FL neglects these considerations, but network-aware CFL can include such devices into the ML model training pipeline via techniques (e.g., domain adaptation and contrastive learning which we explain later) enabled only by network cooperation. 
%     % % Instead of making these devices use the same global model, cooperation allows the network to integrate these unlabeled or partially labeled devices into the FL ecosystem through domain adaptation.
%     \item Introduces multi-hop network cooperation among edge devices and fog network elements. 
%     % Multi-hop data offloading enables powerful edge devices, servers, and other network elements to shoulder even more of the network's total computational burdens. 
%     % Meanwhile, ML model parameter offloading reduces the quantity of edge devices that will need to use high powered transmissions for long distance communications, which further saves overall network energy consumption due to less network channel congestion and communication interference. 
%     % Furthermore, multi-hop network cooperation extends the edge/fog network to include those devices without direct communication or access to the central server. The inclusion of these heretofore marginalized edge devices leads to more training data, which benefits the ML training process. 
%     % % further improves network resource utilization as 
%     % % Multi-hop cooperation between devices and servers via data and model sharing, which can further reduce the energy and time expenditure for ML model training tasks and extends federated learning so that even devices without direct communication links to the central server will participate and benefit from the FL process. 
%     \item Takes into account edge devices' heterogeneous privacy needs. % into the network cooperation system.
%     % To account for and respect heterogeneous privacy demands at the edge, cooperative FL can develop network cooperation opt-in systems with varying levels data and model protection. 
%     % % Considering varying network devices' privacy needs by developing an opt-in framework with varying levels of data and model sensitivity. 
% \end{enumerate}


% some key works: underutilized links, underutilized resources; full potential of the network is not exploited; delay, energy efficient, model performance, privacy as a triumverate of goals in federated learning -> federated learning always has this tradeoff

% Existing works in federated learning have isolation with respect to the computational resources.
% Either computational burden is entirely localized to the IoT devices or it is entirely dependent on server side power. 

\subsection{Overarching Technologies for CFL} \label{ssec:design_intro}
% CFL, through its two pillars of D2D and D2S cooperation, addresses the key missed opportunities of standard FL. %the two key pillars of CFL
In the following, we explain how D2D and D2S cooperation exploit the network characteristics inherent in edge/fog networks, and fulfill the missed opportunities of FL. % and establish the foundation for emerging applications in fog networking. %further and future development towards network-aware CFL. 
%Within each section, we explain how these key pillars of CFL fulfill the missed opportunities of FL and how they establish the foundation for further and future development towards network-aware CFL. 


\subsubsection{Device-to-device (D2D) cooperation}
In edge/fog networks, devices are heterogeneous statistically (i.e., different dataset characteristics) and structurally (i.e., varying computation/communication capabilities). %cpu processing capability and local energy availability). %underlying data distributions and local data quantity 
In standard FL, such heterogeneity leads to isolated resource-abundant and resource-scarce devices, some of which may introduce straggler effects and delay ML model aggregations. In the worst-case, resource-scarce devices may be unable to complete ML model training iterations, possibly due to insufficient battery, or result in the existence of unused local data, as model training in straggler devices use smaller batches of data. % are used for model training in straggler devices. 
To cope with device heterogeneity, we exploit D2D cooperation as discussed below.

% Rather than treating network heterogeneity as a pure disadvantage, we can unlock the potential arising from edge/fog network heterogeneity via D2D cooperation. %, as explained below. 
% In the following, we detail how our proposed technologies enable D2D cooperation to unlock the benefits arising from edge/fog network heterogeneity in the following sections. 


\textbf{Cooperation as resource pooling.} Cooperative edge/fog networks can reallocate the intensity of local ML training by leveraging D2D links for data transfers from resource-scarce to resource-abundant devices. Through this process, the impact of stragglers (i.e., resource-scarce devices) on ML model training is then reduced.
% %, in effect yielding a resource pooling of data and processing power. 
% Data transfers enable resource-abundant devices to shoulder part of the ML training burden from resource-scarce devices, 
% reducing the straggler effects caused by resource-scarce devices. 
% As a result of data transfers, resource-abundant edge devices are able to shoulder part of the ML training burden from resource-scarce devices, reducing the straggler effects caused by resource-scarce devices. 
Simultaneously, D2D cooperation shifts the burden of ML model training to devices with energy-efficient processors, and thus can lead to network energy savings. %more of the


\textbf{D2D driven model offloading.}
Similar to data and subsequent ML training offloading, D2D cooperation can mitigate some of the model aggregation overhead. Rather than only transmitting local ML models to the aggregation server, devices can transfer different parts/chunks of their model to their neighboring devices. In this way, those devices that are far away from the server or have limited communication resources (e.g., limited bandwidth) can communicate with their neighboring devices.
% layers of their local model parameters to their neighbor devices. 
Devices that receive models from their neighbors then combine received models with their local one and then transmit these partially combined ML parameters to the server. 
% subsequently send the combined parameters to the aggregation server. 
Thus, D2D model offloading can reduce the number of devices engaging in resource intensive uplink transmissions. 


% Due to this diversity in network composition, edge and fog devices will vary with respect to local data storage availability, computational power in terms of cpu processing power and cpu cycle effectiveness, communication cost modeling as channel conditions will vary for device pairs, energy availability, and underlying data distributions. 
% Proper design of inter-device collaboration enables many network energy saving opportunities, e.g., controlling cpu clock speeds and D2D data offloading so that efficient devices will process more data,  enabling multi-hop model parameter transmissions to take advantage of high quality and low cost communication channels, and finer grain control of the total duration of ML training. 
% D2D collaboration also influences fundamental design choices of standard FL, such as global aggregation scheduling and device sampling for aggregations. 
%To further integrate D2D collaboration and FL, this network device diversity will influence fundamental design choices of standard FL, 

\subsubsection{Device-to-server (D2S) cooperation}
Current FL research presumes conducting ML model training solely on the devices and neglects the underutilized network infrastructure elements such as edge servers. 
While edge servers may not gather data themselves, they can add value to the FL process owing to their  powerful local processors and dedicated communications equipment which can be exploited through network collaboration. % to enhance the FL process. 
% Especially so given the more powerful processors and dedicated communications equipment at edge servers and routers. 
%leaves the potential of network infrastructure elements such as edge servers and routers untapped. 
We will refer to all cooperation, aside from global model aggregations, between devices and edge servers as D2S cooperation. 
Two potential use cases of edge servers are provided below. 
% We next highlight two potential use cases of edge servers that can improve the FL process. 
%(i) computational resources, (ii) model aggregation gateways, and (iii) flexible data caches. 


\textbf{Computational resources.}
D2S cooperation allows resource-scarce devices to transfer their local data to a physically stable and computationally powerful edge server. These edge servers then function similarly as a resource-abundant device, enabling the network to process more training data and lessening  straggler effects. 

% \textbf{Model aggregation gateways.}
% As edge servers often contain dedicated and powerful communications equipment, they can reroute edge devices' transmissions at global aggregations. As a result, edge devices can transmit their local ML models to their nearby edge servers, which can subsequently transmit partially combined ML models from nearby devices to the aggregation server. Such D2S cooperation saves energy for devices as it bypasses higher powered long distance communications from devices to the aggregation server. 

% they are well placed to receive and subsequently transfer the ML model parameters from many nearby edge devices to the central or main aggregation server. 
 
% So model parameter transfer on D2S links reduces the total quantity of devices communicating to the central server, thereby decreasing overall network communication interference and mitigating channel congestion. 
% Edge servers can thus act as gateways to the central or main aggregation server. 

\textbf{Flexible data caches.}
Using the data they receive from nearby devices, edge servers can act as local data caches, which can carry globally representative distributions of high quality data, to mitigate the impact of non-i.i.d. data across the network. 
Additionally, this functionality enables better tracking of the distribution shifts in the data via comparing old data with newly arriving data at the edge servers, which enables more informative decisions on ML model training. %the knowledge of which can be used to take more informative decisions on ML model training.
% assessments of the time-varying effectiveness of ML models, which is especially useful in real-world dynamic networks where devices' underlying data distributions change over time.  %through changes in underlying data distribution

\subsection{Enhancing the Core Properties of CFL} %complementary technologies
% The above proposed technologies for D2D and D2S cooperation form a basis from which sophisticated applications and technologies can be developed. 
While many applications/extensions are possible from the foundation of D2D/D2S cooperation, we focus on a subset of techniques that can enhance the core trade-offs in CFL. We present high-level explanations of (i) multi-hop cooperation due to its benefits for resource efficiency (energy efficiency and time sensitivity), (ii) integration of unlabeled data as it extends FL to benefit a wider range of devices (e.g., devices with unlabeled data), and (iii) heterogeneous privacy for its enhancements to data/model privacy. 

% we focus on high-level explanations of (i) multi-hop cooperation, (ii) integration of unlabeled data, and (iii) heterogeneous privacy. 

\textbf{Multi-hop D2D and D2S cooperation.}
{\color{black}Multi-hop D2D and D2S cooperation refers to extending the above concepts developed for single-hop D2D and D2S cooperation to multiple, sequential links in between devices.} 
% can be extended from the above concepts for single-hop D2D and D2S cooperation. 
This envisioned technology can greatly improve the resource efficiency (i.e., less energy consumption and/or faster model training) of FL. In particular, multi-hop cooperation enables greater connectivity/reach from resource-scarce to resource-abundant edge devices or servers. 

% As a result, data/model offloading can be done more efficiently than in single-hop scenarios, leading to greater resource efficiency . 
% enable more devices that are normally incapable of contributing to FL process (due to to physical distance from the aggregation server or complete lack of local processing power) to transmit their local model parameters or data to nearby edge devices/servers to overcome these challenges.

\textbf{Integration of unlabeled datasets.} D2D and D2S cooperation can better represent and facilitate the contribution of devices with  partially labeled or unlabeled datasets in FL. % to be better represented or even contribute to the ML training process. 
{\color{black}Unlabeled data refers to data samples that have not been tagged with a ground-truth, e.g., images taken by cameras mounted on smart cars without pre-assigned or pre-identified types of objects within the image.} 
In standard FL, only devices with labeled data are engaged in ML model training. Consequently, edge devices with unlabeled datasets are unlikely to have their data properly represented at the global ML model and so are likely to suffer from poor ML performance. 
% We explain the mechanics in-depth in our discussions of future work, but, at a high level, 
Roughly speaking, D2D and D2S cooperation can enable approximations of the local data distribution of each device at its neighboring devices/network elements, even if the device has fully or mostly unlabeled data.
% , through cooperative data transfers across the network. 
Subsequently, the type of distributed learning method being applied can be tuned to improve ML performance across the network. 

\textbf{Heterogeneous privacy.} 
{\color{black}Similar to statistical (i.e., data-level) and structural (i.e., computation/communication resources) differences, edge/fog devices also exhibit heterogeneity with respect to their privacy needs.
Heterogeneous privacy needs will motivate selective D2D and D2S cooperation.
For example, D2D cooperation can involve sharing sensitive data only among mutually trusted devices (e.g., edge devices belonging to the same user or family), while among untrusted neighbors, this sharing can be limited to sharing insensitive data or even prohibited completely.
This is one of the future complementary technologies depicted in Fig.~\ref{fig:main_diagram}. With such methods in place, D2D/D2S cooperative technologies can improve the resource efficiency and ML performance while meeting data/model privacy requirements of edge/fog network elements.}
% {\color{black}Similar to the statistical (i.e., data-level) and structural (i.e., computation/communication resources) differences among devices, edge/fog devices also exhibit heterogeneity with respect to their privacy needs.} 
% {\color{black}Heterogeneous privacy needs can lead to selective D2D and D2S cooperation.}
% {\color{black}For example, D2D cooperation can involve sharing sensitive data among mutually trusted devices (e.g., edge devices belonging to the same user), while among untrusted neighbors, this sharing can be either prohibited or limited to sharing insensitive data.} 
% {\color{black}Such selective cooperation is depicted in the heterogeneous privacy block in Fig.~\ref{fig:main_diagram}. Thus, our D2D/D2S cooperative technologies can improve the resource efficiency and ML performance while meeting data/model privacy requirements of edge/fog network elements.} 


% \subsubsection{Integration of unlabeled data} %{Heterogeneous quantity of labeled data} %Mixed-quality data}
% Practical edge/fog networks include edge devices with unlabeled or partially labeled data, for example in autonomous driving where smart-cars take pictures of their environment without labels. 
% %smartphone users are likely to tag themselves or friends in selfies but unlikely have tags for all of their local photos. 
% Such devices have limited contribution in standard FL scenarios, and are thus poorly represented by the global ML model in FL systems. Network cooperation can unlock this untapped potential for scenarios with (i) varying quantities of labeled/unlabeled data at all network devices and (ii) unlabeled data at all network devices. 
% %yielding (i) unique ML models for edge devices with unlabeled or poorly labeled datasets and even obtaining (ii) network utility enhancing insights from devices with unlabeled datasets. 

% \textbf{Mixture of labeled and unlabeled data.}
% In networks with varying quantities of labeled/unlabeled data at edge devices, only the labeled portion of devices' local datasets will influence the training of an ML model as per standard FL methodologies. 
% As non-i.i.d. underlying data distributions are present in edge/fog networks, devices with mostly unlabeled data are likely to contain data distributions that may not be properly characterized by the ML model training process in standard FL. 
% % devices with unlabeled or poorly labeled datasets are likely to contain data distributions that may not be properly characterized by training a global model. 
% Rather than developing a uniform global ML model for all edge devices, we can use network cooperation to develop unique combinations of ML models from edge devices with labeled data to better fit the underlying data distributions at devices with unlabeled or partially labeled data. We explain the mechanical details of such a methodology later, but the essence is that through pairwise device collaborations, the network can develop approximations of the underlying data distributions for all network devices and thereafter combine ML models from devices with labeled data to fit those devices without labeled data (or with few labeled data relative to unlabeled data). 

% \textbf{Only unlabeled data.}
% Cooperation also enables FL frameworks in edge/fog networks where all devices have unlabeled data. 
% Unsupervised learning techniques, such as K-NN+ or contrastive learning, cluster data based on similarity relative to a small subset of randomly chosen data. While these methods have proven effective in centralized settings, they promote overfitting at individual edge devices in federated settings, as devices have unique underlying data distributions. 
% %in part due to XXX and in part due to the non-i.i.d. data distributions across edge/fog networks 
% % require some network hyperparameter specifications such as total categories of data (i.e., label approximations). 
% % But in heterogeneous edge/fog networks known for non-iid underlying data distributions, such specifications promotes overfitting at individual edge devices as local training will forcibly split categories to fit the globally specified hyperparameter. 
% Edge/fog device cooperation via selective and modified data sharing allows edge devices access to more diverse data and thus balances local datasets to reduce the likelihood of overfitting. 
% % a greater chance of containing more legitimate clusters, which reduces overfitting and improves convergence speed throughout the network. 
% % We will explain all these mechanics in-depth later. 

% % What's the background? - Real edge/fog networks include edge devices with unlabeled data, for example vehicular networks XXX 
% % What's the cooperation? - there is domain adaptation & collaborative unsupervised federated learning
% % How is this impactful? - tie this into "what's the cooperation?"

% % Cooperation among network elements for FL will also connect devices with labeled, partially labeled, and unlabeled datasets together. 
% % While standard FL methods will struggle here, cooperative FL can leverage these connections to compare data distributions at devices, regardless of data label. 
% % Even if a device with poorly labeled or unlabeled data has a unique underlying data distribution, it can be approximated through 
% % combinations of data distributions at devices with labeled data, for sufficiently large networks. 
% % So, through comparison and cooperation with other network devices, useful/practical ML models can be transferred from devices with well labeled data to those devices without such high quality data, and combined at these destination devices. 


% % Devices with partially labeled and unlabeled data may have different data distributions and model training needs than their labeled counterparts. 
% % % Consequently their ML model and training needs will differ from those devices with labeled data as well. 
% % Standard FL neglects these considerations, but network-aware CFL can include such devices into the ML model training pipeline via techniques (e.g., domain adaptation and contrastive learning which we explain later) enabled only by network cooperation. 
% % % Instead of making these devices use the same global model, cooperation allows the network to integrate these unlabeled or partially labeled devices into the FL ecosystem through domain adaptation.

% \subsubsection{Multi-hop edge/fog network cooperation} 
% In large-scale edge/fog networks, the central server and all other edge elements have a limited range of communication, which in turn limits the quantity and type of edge devices that can contribute to the FL process. 
% % not all edge devices will have a direct connection to the central server. Such devices are typically neglected in standard FL but they may still have valuable contributions, both due to their local datasets and their computational resources. 
% Especially given that such devices are likely to be at the fringes of edge/fog networks, their data distributions are more likely to be different from data gathered nearby the central server. %So There is thus even greater reason to incorporate such data
% %Enabling multi-hop collaborations extends the networking scope of FL, enabling devices at the fringes of edge/fog networks to contribute to and receive the insights from FL.

% Multi-hop collaborations, such as data offloading and model parameter transfers, enable edge devices and servers to connect and subsequently benefit from an extended neighborhood of edge devices and fog networking elements. From an ML standpoint, both data offloading and model parameter transfers enable previously excluded devices to share the insights of their local data with the rest of the network. 
% This extended neighborhood connectivity enables further resource efficiency, for example a resource-poor device may transfer some of its data to another resource-poor device which then offloads said data to a powerful edge server. 
% Thus, multi-hop cooperation also enhances the D2D and D2S collaborations outlined above. 
% % This inclusion of more total data processed by the network has the side effect of also improving estimates of model drift, and thus ensure that the network is better able to schedule time and resource efficient model aggregations. 
% % For instance multi-hop links can better allocate training data to benefit from resource-abundant edge devices/servers 
% % multi-hop data and model transfers to maximize ML performance and resource efficiency and minimize delay 

% % What's the background? - Single hop cooperation still limits devices to only cooperate with their local devices, servers, and other network elements. To extend the zone of cooperation for each edge device, we can develop multi-hop offloading  
% % What's the cooperation? - Multi-hop data and model transfers 
% % How is this impactful? - further decrease the resource expenditure for data and model transmissions

% \subsubsection{Heterogeneous network privacy needs} %Privacy and opt-in local sharing agreements}
% % Device-to-device and device-to-server communications will form a key component and benefit of cooperative FL. To maximize link utility, data and potentially sensitive information about devices will need to be transmitted among network elements. 
% % A well developed cooperation agreement should incorporate (i) a social cooperation agreement among network devices and/or (ii) communication encoding schemes among network elements. 
% The heterogeneity latent in edge/fog networks also applies to devices' privacy considerations. Some devices may have limited or no privacy requirements, while other devices may be willing to share specific types of data (for example, they may be willing to share dog photos but not photos of their child)~\cite{lin2021friend}. In the extreme case, specific devices may be entirely unwilling to share any data. 

% Cooperative edge/fog can account for these diverse user privacy specifications by only sharing data and/or model parameters among edge devices that consent, similar to opt-in cookies on the web. Some devices may only agree to offload data and model parameters among recognized and trusted devices and edge servers, while other devices may be willing to allow general multi-hop data and model offloading. %
% Network cooperation can respect these diverse privacy needs by 
% Cooperation along such social agreements naturally leads to different edge device/server layers in the network, which enables more fine-grain control and optimization of FL processes at different layers and thereby yields a more resource efficient FL process.
% The network can also offer incentives to edge devices, such as priority routing during peak network traffic, to encourage edge devices to share their data. 
% %among  for network segmentation into regional clusters and multiple layers, which invites development of topology-aware and/or multi-layer differential privacy frameworks. 
% % A device's engagement into select social agreements allows for network segmentation into regional clusters and multiple layers, which invites development of topology-aware and/or multi-layer differential privacy frameworks. 
% % Furthermore, privacy can be further ensured via the integration of encoding schemes for data, model parameters, and model gradients. Encoding can be designed for independent use in cooperative FL or integrated into social clusters, i.e., cluster specific encoding schemes.

% % Furthermore, even existing models of privacy in standard FL have been shown to contain vulnerabilities, for example~\cite{XXX} showed that semi-honest parties can reconstruct private information from ML model parameters. 


% % What's the background? - The heterogeneity latent in edge/fog networks also applies to devices' privacy considerations. Even existing models of privacy in standard FL have been shown to contain vulnerabilities, for example~\cite{XXX} showed that semi-honest parties can reconstruct private information from ML model parameters. New models of cooperation and privacy together need to be developed. 
% % What's the cooperation? - Data/model sharing agreements. Data categories [some device may be willing to share dog photos but not photos of their child]
% % How is this impactful? - resource and network efficiency as we're not spending unnecessary resources on an overly restrictive notion of privacy; network cooperation eliminates the critical reliance on a central server (if central server is the adversary, then very bad)

% % {\color{black} VA's comments on privacy: Privacy - Draft Para

% % Preserving privacy is of immense practical importance when federating across distinct parties. Despite the fact that the private data of each client is not shared with other clients/server, the private information might still be reconstructed by semi-honest parties upon observing the model information shared among federated learning parties [https://proceedings.neurips.cc/paper/2020/hash/c4ede56bbd98819ae6112b20ac6bf145-Abstract.html].  To protect privacy of the participants, many protection mechanisms have been proposed, such
% % as Randomization Mechanism, Secret Sharing, Homomorphic Encryption (HE), and Compression Mechanism [see https://arxiv.org/pdf/2209.00230.pdf and references therein]. The authors of  [https://arxiv.org/pdf/2209.00230.pdf ] analyzed the trade-off in federated learning for widely-adopted protection mechanisms including Randomization, Homomorphic Encryption, Secret Sharing and Compression. Different setups have been considered for security: The first is when  the server is the adversary who aims at inferring the private data of individual clients. In this setup, adversaries could exploit gradient information to restore the private image data to pixel-level accuracy, with distinct settings of prior distributions and conditional distributions.  The second is where a client wants to recover the data labels owned by other client. Even though the different security and privacy mechanisms have been studied in the context of federated learning, the problem is open for fog learning with network-aware cooperation. Efficient D2D cooperation can help improve the privacy when the central server is the adversary. Further, efficient approaches for privacy may be needed when one of the client is the adversary. 
% % }