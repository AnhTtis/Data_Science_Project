\begin{figure}[t]
\includegraphics[width=.48\textwidth]{pix/D2D_commag.pdf}
\centering
\caption{Data and local model offloading form the basis of effective D2D cooperation. % in federated settings. %As part of effective D2D cooperation, 
We envision smart data offloading by edge devices through dataset stratification.
Additionally, we propose local model offloading, where devices offload segments of their local ML models to other devices. %nearby edge devices.
}
% , a technique in which edge devices can split and subsequently offload segments of their local ML models to nearby edge devices to save on communication resources.} 
% Segmenting local ML models and subsequently transmitting them to a range of edge devices transmission also offers an opportunity to increases the resolution of efficient D2D cooperation in the form of local model offloading.} 
\label{fig:PSLbig}
\end{figure}

\noindent
The first step towards network-aware CFL is to develop and maximize the benefits arising from D2D cooperation. 
{\color{black}Well-designed D2D cooperation can enable efficient orchestration of limited network resources, leading to improvements in the trade-offs of FL from Fig.~\ref{fig:main_diagram}.} % For example, efficient D2D data offloading  can improve energy consumption during ML training and reduce training delay by transferring data from resource-scarce devices to resource-abundant ones.} %to improve ML model training efficiency. %instance through data offloading from resource-scarce devices to resource-abundant ones in order to re-balance ML model training. 
To this end, we first introduce a set of core, overarching technologies to enable effective D2D cooperation, and thereafter propose future work on complementary technologies to further enhance D2D cooperation. 


\begin{figure}[t]
\centering
\includegraphics[width=.48\textwidth]{pix/psl.pdf}
\caption{{\color{black}Our D2D cooperation-driven method incurs both less compute time and energy consumption relative to the state-of-the-art $\mathsf{NOVA}$ methodology on two commonly used machine learning datasets (MNIST for numbers and Fashion-MNIST for clothes).}}
\label{fig:D2D_comparison}
\end{figure}


\subsection{Core Techniques for Effective D2D Cooperation} %Key Technologies Towards Effective D2D Collaboration}
Effective D2D cooperation improves network resource efficiency and ML model training through innovations in data and model offloading, which we propose in Fig.~\ref{fig:PSLbig}. % from resource-scarce devices to resource-abundant ones. 
At the data level, we propose dataset stratification, a method which clusters local datasets for higher quality data offloading. %segments vs clusters 
% technologies that jointly improve the resource consumption (energy and delay) and ML model performance. 
At the ML model level, we propose local model offloading, a technique that involves partial local ML model offloading to streamline efficient ML model aggregations. % local model segmentation and subsequent offloading to reduce resource consumption during ML model aggregations. 
% to sequentially orchestrate model aggregations through D2D model offloading. 
% We explain these proposed technologies in detail below, and provide a high-level overview in Fig.~\ref{fig:PSLbig}.

\subsubsection{Dataset Stratification}
After offloading data, the sending device (sender) continues local ML model training on a smaller local dataset, as keeping a copy of the transferred data leads to bias at ML model aggregations from counting the same data multiple times. %twice. %smaller vs lower-quality
% When offloading data, the sender should not use the offloaded subset of data for local ML training as otherwise it would lead to double counting at model aggregations and thus bias the global ML model. 
On the other end, the receiving device (receiver) may receive data that is unrepresentative of the data gathered at the sender. 
As a result, random/naive data offloading may hinder rather than help the ML model training, motivating dataset stratification. 

As depicted on the left subplot of Fig.~\ref{fig:PSLbig}, dataset stratification clusters datasets into strata (i.e., categories) based on task-dependent criteria. 
{\color{black}
Using the example of clothing recognition (i.e., the Fashion-MNIST dataset~\cite{hosseinalipour2022parallel}) for data collected by smartphones' cameras, each stratum may contain data belonging to a unique type of clothing (e.g., T-shirts or coats).
% The datasets of devices will contain datapoints from different broadcast sources. Through stratification, each strata can contain only data belonging to a unique type of broadcaster. 
Then, through sequential selection of the most representative data samples (i.e., those that are closest to the strata average) from the most populous strata, devices can offload datapoints which well-capture the distribution of their local dataset.}
% in signal source classification, each strata would contain only data belonging to a unique signal source designation, such as cell phone, sensor, or information server. 
% Then, through sequential selection of the most representative data samples (i.e., those that are closest to the strata average) from the most populous strata, devices can offload datapoints which well-capture the distribution of their local dataset.} 
In doing so, dataset stratification (i) keeps the distribution of the dataset at senders relatively intact, and (ii) enables receivers to receive a representative sample of data from senders. 
% offloads data by 
% enables (i) senders to selectively offload data so that their local ML training remains representative of their originally gathered data, and (ii) receivers to augment their local datasets with data representative of their neighbors' datasets. %(e.g., to reduce dataset heterogeneity across edge devices)

As an example, consider a smart car communicating with a drone. From its operation, the car has images of mostly stop signs and traffic lights, which the drone may not. Dataset stratification enables the car to transmit a small set of representative stop sign and traffic light pictures, without significantly distorting its local dataset distribution, to the drone. 
% smart cars gather data from different underlying environments. While one vehicle may have images of buildings and highways, another may have images of trees and small animals. These two vehicles may also have some overlapping data such as images of stop signs and traffic indicators. If these two vehicle were to meet, dataset stratification would enable the image sharing of buildings and highways for trees and small animals, i.e., the transmission of data useful to both vehicles, rather than overlapping images of stop signs. 


\subsubsection{Local Model Offloading}
% On the model cooperation side, we propose local model offloading,
We propose segmenting and sequentially transmitting the devices' local ML models at global ML model aggregations. {\color{black}Each segment of an ML model is a subset of model parameters. For example, in the case of neural networks, each segment may contain the parameters associated with a layer of the neural network. Local model offloading, depicted in Fig.~\ref{fig:PSLbig}, enables devices with limited access to the server (e.g., due to unsatisfactory channel conditions) to offload different segments of their local ML models to intermediary devices with a better accessibility to the server.}
% Local model offloading, depicted in Fig.~\ref{fig:PSLbig}, enables devices to offload different layers to different intermediary devices on the path to the central server.} % splitting devices' local ML models into varying segments, corresponding to different layers of a neural network, and subsequently transferring these different segments to intermediary devices on the path to the central server. }
These intermediary devices will combine their local ML models with their received partial ML models, leading to a set of partially aggregated parameters, which are then sent to the central server. 
In this way, the central server is able to perform the global aggregation using all ML model parameters while saving communication resources. 

% \subsubsection{ML Performance, Energy Consumption, and Delay Optimization}
% By combining these technologies into CFL, we are able to develop a joint ML performance, energy consumption, and delay optimization formulation. 
{\color{black}We have taken initial steps toward formalizing this D2D cooperation methodology in our prior work~\cite{hosseinalipour2022parallel}, including optimization formulation, theoretical results, and convergence proofs. 
To evaluate its potential benefits, we compare it against the algorithm Nova~\cite{wang2021novel} on two common datasets used to evaluate FL methods: MNIST (numbers) and Fashion-MNIST (clothes). Simulation results are shown in Fig.~\ref{fig:D2D_comparison}, where both methods train a two layer convolutional neural network (CNN) across a network of 10 devices. A detailed description of the computational infrastructure, wireless channel models, and models of energy consumption (including energy from both the communication and computation processes) can be found in~\cite{hosseinalipour2022parallel}.}
{\color{black}We compare against Nova~\cite{wang2021novel}, a recent and well-established method for aggregations involving heterogeneous training epochs across edge/fog devices.}
% To evaluate the impact of the above proposed technologies, we compare them against Nova~\cite{wang2021novel} on two common datasets used to evaluate FL methods: MNIST (numbers) and Fashion-MNIST (clothes), in Fig.~\ref{fig:D2D_comparison}. 
% In our simulation, both methods train a two layer CNN across a network of 10 devices. For this result, we used the same computational infrastructure, channel models, and energy modeling as in our prior work~\cite{hosseinalipour2022parallel}.} 
% {\color{black}We compare against Nova~\cite{wang2021novel} as it is the state-of-the-art method for aggregations involving heterogeneous training epochs across edge/fog devices.}
Our proposed CFL technology is seen to yield (i) consistent energy savings, and (ii) faster ML training times. 
% {\color{black} We have taken initial steps toward formalizing D2D cooperation methodology in~\cite{hosseinalipour2022parallel} including optimization formulation, theoretical results, and convergence proofs.}




% In addition to the above D2D cooperation innovations, we propose additional techniques in~\cite{hosseinalipour2022parallel} that can simultaneously determine data batch sizes for ML model training, CPU clock frequency tuning, and more. 

% % We've developed an FL-based methodology called \textit{Parallel Successive Learning} ($\mathsf{PSL}$) that comprehensively uses cooperative D2D links to benefit the foundational components of effective cooperative FL design. %places high priority on maximizing D2D network utility 
% % The $\mathsf{PSL}$ problem is concerned with the distributed and time-sensitive development of a global ML model by a network of devices, which are heterogeneous with respect to energy availability, quantity of local data, data processing capability, spectrum availability, and underlying data distributions. 
% In this environment, the network can leverage D2D cooperation to alleviate these communication and networking problems while maximizing expected global ML model performance. 
% Specifically, the network can determine the optimal D2D data offloading ratios, D2D model transfer ratios, mini-batch size of stochastic gradient descent, cpu clocking frequencies, epoch duration, and more network parameters via the formulated optimization problem associated with $\mathsf{PSL}$. We leave the in-depth derivation, presentation, and analysis of such an optimization problem in~\cite{XXX}. 
% Once these network control parameters are determined, $\mathsf{PSL}$ follows five key sequential steps: (i) global model broadcasting, (ii) data dispersion, (iii) local gradient computation, (iv) model/gradient dispersion with local condensation, and (v) global model aggregation, which are all depicted in Fig.~\ref{fig:PSLbig}. 
% % The $\mathsf{PSL}$ methodology seeks to jointly optimize the total time taken, the data processing and communication energy consumption of all network devices, and expected global ML model classification performance by controlling D2D data and model transfer ratios, minibatch size of stochastic gradient descent, and cpu clocking. 

% have a full demonstration of the theoretical and experimental effectiveness of $\mathsf{PSL}$ in managing all of the above networking aspects in~\cite{XXX}, but, for brevity,  we will only show the control of data offloading ratios as network energy consumption becomes a concern in Fig.~\ref{fig:c1_minibatch_netd}. 

% The key contribution of $\mathsf{PSL}$ is that it towards advancing cooperative FL is the demonstration that cooperation can lead to 

% Explain some of the key contributions of PSL pertinent to cooperative FL. 
% Show some simulation results of PSL; give the high level rundown.  

\begin{figure}[t]
\includegraphics[width=.48\textwidth]{pix/D2S_commag.pdf}
\centering
\caption{Device-to-Server (D2S) cooperation via data/model transfers can improve the resource efficiency and performance of FL. %and the ML performance. 
Additionally, intelligent selection of the aggregation server can further reduce aggregation delay.} %, we can further reduce the aggregation delay.}%, especially edge/fog networks with device mobility. }
% To drive these benefits further, we can adjust flexibly adjust our selection of aggregation servers.} 
%{\color{black}D2S diagram. Plan is to better emphasize D2S collaboration and the time-varying central server for aggregation.} }
\label{fig:D2S}
\vspace{-3mm}
\end{figure}

\subsection{Future Development of Complementary Technologies}
Further extension of D2D cooperation can further enhance the trade-offs in FL and subsequently CFL depicted in Fig.~\ref{fig:main_diagram}. %developing its robustness to network changes or integration of multi-hop offloading topologies, which can enhance the core properties of CFL from Fig.~\ref{fig:main_diagram}. %ML performance, energy efficiency, %These complementary technologies 
{\color{black} A few open research directions are summarized below:} %We summarize the more interesting open research directions below: 
\subsubsection{D2D cooperation in non-stationary networks}
In non-stationary networks, devices will enter and exit the network, leading to varying network size, changing compute resource availability, and time-varying D2D links. 
Here effective cooperation should consider the physical stability of edge/fog devices to determine effective time-varying anchor devices. 
{\color{black}These anchors will receive nearby ML models from devices as they leave the network and transmit the latest global ML model to new devices as they join the network. In doing so, anchor devices improve ML training by enabling more devices to contribute to the training process in-between global model aggregations.} 
% We propose the concept of anchor devices in FL as edge devices that can receive the ML models of nearby devices and send the global ML model to new devices that enter the network. 
% In doing so, anchor devices can improve ML model training as they can gather the ML models of devices that leave the network and they enable devices that enter the network to begin ML training by sending them the latest global ML model. 
\subsubsection{$n$-hop cooperation} 
{\color{black}$n$-hop cooperation aims to broaden the scope of D2D cooperation, providing resource-scarce devices with greater access to resource-abundant ones through intermediary devices. Consequently, this technology can further enhance the resource and time savings introduced by single-hop D2D cooperation. This calls for novel optimization methodologies to characterize the benefits and trade-offs of $n$-hop cooperation.}
% Introducing and optimizing n-hop cooperation Furthermore, the cooperative aspect of PSL can be extended to the development of multi-hop data/model offloading, which can extend the distributed framework beyond those devices that can connectivity to the central server. 
\subsubsection{Heterogeneous privacy needs in D2D cooperation} 
Edge/fog devices may have heterogeneous privacy needs. For example, D2D connections may be allowed based on trust or familiarity. 
In such cases, devices often band together into cliques, which are private groups with a certain level of mutual trust.
{\color{black}Data transferring can be restricted to links between mutually trusted devices (e.g., smart devices such as a smartphone, laptop, and tablet of the same owner). 
In intra-clique cooperation, then, devices can share data without any restrictions, while, in inter-clique cooperation, devices may only be willing to share model parameters or insensitive data.
Furthermore, dataset stratification can be designed to separate data based on sensitivity, with restricted offloading of sensitive strata (e.g., personal health data or biometrics) among intra-clique devices and unrestricted offloading of insensitive strata (e.g., weather information).}

%%% uncomment interference management if we need another future work example
% \subsubsection{Interference management}
% While problems such as interference management have been widely studied in communications literature, the link between wireless interference on the success or timeliness of distributed machine learning at scale has yet to be properly investigated. 
% An integration of the joint problems of $\mathsf{PSL}$'s cooperation driven FL and the traditional communications problem of interference management can lead to novel formulations that determine optimal data and model routing in cooperative edge/fog networks. 

\subsubsection{Inclusion of devices with unlabeled data}
In practical edge/fog networks, some devices may have mostly or fully unlabeled datasets. 
% In federated settings, these devices are also likely to have non-i.i.d. underlying data distributions, so a global ML model trained by devices with labeled data may not be effective for devices with unlabeled data.
Standard FL neglects all these devices and obtains a global ML model by only engaging the devices with labeled datasets.
{\color{black}Through D2D cooperation, edge devices can share small quantities of data, labeled or unlabeled, to develop estimates of data distributions at devices with unlabeled datasets. 
This technique, termed unlabeled distribution estimation, will then involve determining unique combinations of ML models trained by devices with labeled datasets for use at devices with unlabeled datasets.}
% We instead envision a new technology called unlabeled distribution estimation, which relies on D2D cooperation to share small quantities of data between devices to develop estimates of the data distributions at devices with unlabeled datasets. With this information, we can then determine unique weighted combinations of ML models trained by devices with labeled datasets for those devices with unlabeled datasets. 

% Developing a unique weighted combinations of ML models trained by devices with labeled data to fit a single device is typically called domain adaptation. 
% Through D2D cooperation, we can develop better estimates of the data distributions at different devices through data feature comparisons, which do not require data to be labeled. 
% In large-scale edge/fog networks, effective comparisons would need to first be theoretically characterized and subsequently optimized. 

% % to route data and model transfer in $\mathsf{PSL}$ to maximize final global ML model performance, energy consumption, timeliness of the training process, and/or a combination thereof. 
% \begin{enumerate}
%     % \item D2D cooperative design and optimization in time-varying networks. 
%     % \item Integration of social networking.  %Social learning or trust-based data and model dispersion factors
%     %% \item Low-priority/parasitic communication relegation - what happens if %the communication information of PSL is given low-priority transmission? 
%     %% \item Multi-hop cooperation beyond the star topology %multi-hop data and %model dispersion - trust?
%     %% \item Non-stationarity data distributions [dynamics of data -> time - varying data]
%     \item Management of communication and networking problems such as interference management on data and model routing. 
%     While problems such as interference management have been widely studied in communications literature, the link between wireless interference on the success or timeliness of distributed machine learning at scale has yet to be properly investigated. 
%     An integration of the joint problems of $\mathsf{PSL}$'s cooperation driven federated learning and the traditional communications problem of interference management can lead to novel formulations to route data and model transfer in $\mathsf{PSL}$ to maximize final global ML model performance, energy consumption, timeliness of the training process, and/or a combination thereof. 
%     %routing problems, how to route the data [such problems have been studied widely, but not in this particular area]
% \end{enumerate} 