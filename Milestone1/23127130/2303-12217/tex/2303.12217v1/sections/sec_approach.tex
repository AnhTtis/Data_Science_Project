\vspace{-4mm}
\section{Approach}
\vspace{-2mm}

\begin{figure}
    \centering
    %\includesvg[width=0.4\textwidth, height=5cm]{figures/model-sel-MNIST-denoising-samples.svg}
    \includegraphics[width=0.48\textwidth]{figures/diagram_bh.pdf}
    \caption{
    We use a set of $N$ measurements $\{y^{(i)}\}_{i=1}^N$ from $N$ different ground-truth images to infer  $\{\phi^{(i)}\}_{i=1}^N$, the parameters of the latent distributions, and $\theta$, the parameters of the generative network $G$. All inferred parameters are colored as blue and the loss is given by Equation \eqref{eqref:learning-objective}. Here, $G\sharp P$ denotes the push-forward of distribution $P$ induced by $G$.}
    \label{fig:arch}
\end{figure}
\vspace{-2mm}


In this work, we propose to solve a set of inverse problems without access to an IGM by assuming that the set of ground-truth images have common, low-dimensional structure. Other works have considered solving linear inverse problems without an IGM \cite{Lehtinenetal18, liu2020rare}, but assume that one has access to multiple independent measurements of a single, static ground-truth image, limiting their applicability to many real-world problems. In contrast, in our work we do not require multiple measurements of the same ground-truth image.
% The measurements are not limited to the assumption that one has access to multiple independent measurements of a single, static target, unlike \cite{Lehtinenetal18, liu2020rare}, but operat, but operat.
% To do so, we learn an IGM. 
%In order to accurately learn an IGM, we motivate the use of the Evidence Lower Bound (ELBO) as a loss by showing that it provides a good criterion for selecting a prior model.

%Building upon this, we show that one can directly learn a prior from corrupted measurements alone by parameterizing the prior model as a deep neural network with a low-dimensional latent distribution. 
%and a latent posterior distribution with deep neural networks and fit the network weights to optimize a proxy for the ELBO. 
%The IGM network weights are shared across all images, capitalizing on the common structure present in the data, while the parameters of each latent distribution are learned jointly with the generator to model each complex image posterior. We will discuss this in more detail in Section \ref{sec:learning}
\vspace{-2mm}

\subsection{Motivation for ELBO as a model selection criterion} \label{sec:ELBO-intro}
\vspace{-1mm}

In order to accurately learn an IGM, we motivate the use of the Evidence Lower Bound (ELBO) as a loss by showing that it provides a good criterion for selecting a prior model. Suppose we are given noisy measurements from a single image: $y = f(x) + \eta$. In order to reconstruct the image $x$, we traditionally first require an IGM $G$ that captures the distribution $x$ was sampled from. A natural approach would be to find or select the model $G$ that maximizes the model posterior distribution $ p(G |y) \propto p(y | G)p(G).$
That is, conditioned on the noisy measurements, find the model of highest likelihood. Unfortunately computing $p(y | G)$ is intractable, but we show that it can be well approximated using the ELBO.
%Due to the intractability of computing $p(y | m)$, we instead consider the Evidence Lower Bound (ELBO). 

To motivate our discussion, consider estimating the conditional posterior $p(x|y,G)$ by learning the parameters $\phi$ of a variational distribution $q_{\phi}(x)$. Observe that the definition of the KL-divergence followed by using Bayes' theorem gives \begin{align*}
    & \infdiv{q_{\phi}(x)}{p(x|y,G)} 
     = \log p(y|G) \\
     & - \mathbb{E}_{x \sim q_{\phi}(x)}[\log p(y|x,G) + \log p(x|G) - \log q_{\phi}(x)].
\end{align*} The ELBO of an IGM $G$ given measurements $y$ is defined by \begin{align}
    \ELBO(G,q_{\phi};y) & := \mathbb{E}_{x \sim q_{\phi}(x)}[\log p(y|x,G) + \log p(x|G) \nonumber \\
    & - \log q_{\phi}(x)].
    \label{eq:ELBO} 
\end{align} Rearranging the above equation and using the non-negativity of the KL-divergence, we see that we can lower bound the model posterior as \begin{align}
    \log p(G|y) \geqslant \ELBO(G, q_{\phi};y) + \log p(G) - \log p(y).
\end{align} Note that $-\log p(y)$ is independent of the parameters of interest, $\phi$. If the variational distribution $q_{\phi}(x)$ is a good approximation to the posterior $p(x|y,G)$, $D_{\mathrm{KL}} \approx 0$ so maximizing $\log p(G|y)$ with respect to $G$ is approximately equivalent to maximizing $\ELBO(G, q_{\phi};y) + \log p(G)$.

 Each term in the ELBO objective encourages certain properties of the IGM $G$. In particular, the first term, $\E_{x \sim q_{\phi}(x)}[\log p(y|x,G)]$, requires that $G$ should lead to an estimate that is consistent with our measurements $y$. 
 %For example, if we have additive Gaussian noise in our measurements, maximizing this term corresponds to minimizing an $\ell_2$ reconstruction loss. 
 The second term, $\E_{x \sim q_{\phi}(x)}[\log p(x|G)]$, encourages images sampled from $q_{\phi}(x)$ to have high likelihood under our model $G$. 
 The final term is the entropy term, $\E_{x \sim q_{\phi}(x)} [-\log q_{\phi}(x)]$, which encourages a $G$ that leads to ``fatter'' minima that are less sensitive to small changes in likely images $x$ under $G$.
% to avoid collapsing to a degenerate solution and introduces a notion of confidence or uncertainty.

%Several works \cite{Abdellatif18, AbdellatifAlquier18, sun2022alpha} have proposed using the ELBO as a model selection criterion, but have not been demonstrated on high-dimensional imaging inverse problems.
\vspace{-4mm}

\subsection{ELBOProxy}
\vspace{-1mm}
Some IGM are explicit (e.g., Gaussian image prior), which allows for direct computation of $\log p(x|G)$. In this case, we can optimize the ELBO defined in Equation~\eqref{eq:ELBO} directly and then perform model selection. However, an important class of IGMs that we are interested in are those given by deep generative networks. Such IGMs are not probabilistic in the usual Bayesian interpretation of a prior, but instead implicitly enforce structure in the data. Moreover, terms such as $\log p(x|G)$ can only be computed directly if we have an injective map \cite{kothari2021trumpets}. This architectural requirement limits the expressivity of the network. Hence, we instead consider a proxy of the ELBO that is especially helpful for deep generative networks. Suppose our image generation model is of the form $x = G(z)$ where $G$ is a generative network and $z$ is a latent vector. Introducing a variational family for our latent representations $z \sim q_{\phi}(z)$ and using $\log p(z| G)$ in place of $\log p(x|G)$, we arrive at the following proxy of the ELBO:
\begin{align}
\ELBOProxy(G, q_{\phi};y) & := \E_{z \sim q_{\phi}(z)}[\log p(y |G(z)) \nonumber\\
& + \log p(z | G) - \log q_{\phi}(z)]. \label{eqref:ELBOProxy}  
\end{align} When $G$ is injective and $q_{\phi}(x)$ is the push-forward of $G$ through $q_{\phi}(z)$, then this proxy is exactly the ELBO in Eq. \eqref{eq:ELBO}. While $G$ may not necessarily be injective, we show empirically that the ELBOProxy is a useful criterion for selecting such models. %Nevertheless, we show empirically that the ELBOProxy is a useful criterion for selecting  generative networks. %While this proxy may not be exact for non-injective $G$, quality generators should not lack injectivity over high likelihood image samples. 


%\subsubsection{Variational family} In practice, there are a number of choices to parameterize the variational families $q_{\theta}(x)$ and $q_{\phi}(z)$. One could, for example, utilize a Gaussian parameterization which would entail learning a mean and covariance $(\mu, \Lambda)$. Another particularly flexible family of functions are Normalizing Flows \cite{Rezendeetal2014, RezendeMohamed2015, dinh2016density}, which are generative models capable of learning an invertible mapping between a simple latent distribution to a more complicated distribution of interest \cite{SunBouman}. %We explore both options in this work, and discuss these ideas in subsequent sections.

\noindent \textbf{Toy example:} To illustrate the use of the $\ELBOProxy$ as a model selection criterion, we conduct the following experiment that asks whether the $\ELBOProxy$ can identify the best model from a given set of plausible IGMs. For this experiment, we use the MNIST dataset \cite{MNIST} and consider two inverse problems: denoising and phase retrieval. We train a generative model $G_{c}$ on each class $c \in \{0,1,2,\dots,9\}$. Hence, $G_{c}$ is learned to generate images from class $c$ via $G_{c}(z)$ where $z \sim \mathcal{N}(0,I)$. Then, given noisy measurements $y_{c}$ of a single image from class $c$, we ask whether the generative model $G_{c}$ from the appropriate class would achieve the best $\ELBOProxy$. For denoising, our measurements are $y_c = x_c + \eta_c$ where $ \eta_c\sim \mathcal{N}(0, \sigma^2 I)$ and $\sigma = \sqrt{0.5}$. For phase retrieval, $y_c = |\mathcal{F}(x_{c})| + \eta_c$ where $\mathcal{F}$ is the Fourier transform and $\eta_{c} \sim \mathcal{N}(0, \sigma^2 I)$ with $\sigma = \sqrt{0.05}$.

We construct $5 \times 10$ arrays for each problem, where in the $i$-th row and $j$-th column, we compute the $-\ELBOProxy$ obtained by using model $G_{{j-1}}$ to reconstruct images from class $i-1$. We calculate $\ELBOProxy(G_{c}, q_{\phi_c};y_c)$ by parameterizing $q_{\phi_c}$ with a Normalizing Flow and optimizing network weights $\phi_c$ to maximize \eqref{eqref:ELBOProxy}. Results are shown in Fig.~\ref{fig:MNIST_ELBO_exp}. We note that all of the correct models are chosen in both denoising and phase retrieval. We also note some interesting cases where the $\ELBOProxy$ values are similar for certain cases, such as when recovering the $3$ or $4$ image when denoising. For example, when denoising the $4$ image, both $G_{4}$ and $G_{9}$ achieve comparable $\ELBOProxy$ values. By carefully inspecting the noisy image of the $4$, one can see that both models are reasonable given the structure of the noise. %It would be natural for a model selection criterion to assign similar values to semantically similar distributions of images.% In Section \ref{sec:results-model-selection}, we provide further experiments on the benefits of using the $\ELBOProxy$ as a model selection criterion.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/model_selection.pdf}
    \caption{We consider two inverse problems: denoising and phase retrieval. Left: the two leftmost columns correspond to the ground truth image $x_c$ and the noisy measurements $y_c$. Center: in each row, we show the means of the distribution induced by the push-forward of $G_j$ and each latent distribution $z \sim q_{\phi_j}$ for $j \in \{0,\dots,9\}$. 
    %This mean reconstruction is done by sampling $T=100$ latent vectors $z^{(c)}_t \sim q_{\phi^{(c)}}$ and computing $\frac{1}{T}\sum_{t=1}^T m_c(z^{(c)}_t).$ 
    Right: each row of the array corresponds to the $-\ELBOProxy$ achieved by each model in reconstructing the images. Here, lower is better. Boxes highlighted in green correspond to the best $-\ELBOProxy$ values in each row. In all cases, the correct model was chosen.  
    }
    \label{fig:MNIST_ELBO_exp}
\end{figure}



% \begin{figure}[ht]
%     \centering
%     %\includegraphics[width=0.95\textwidth]{figures/mnistdenoising64.pdf}
%     \caption{\textbf{Denoising improves with more noisy MNIST observations.} We demonstrate our method of learning the IGM to perform denoising for increasing number of noisy images (4, 35, and 75 images from left to right). In each panel, we include the ground truth, noisy measurements, mean of the posterior, and standard deviation of the posterior. We also include the reconstructions using an IGM trained on the full clean MNIST 8's class. We observe that the mean reconstructions and standard deviations from our low-data IGMs become more similar to the full-data IGM with increasing data.}
%     \label{fig:MNIST_denoising}
% \end{figure}
%\vspace{-3mm}
\vspace{-3mm}
\subsection{Learning the IGM to solve inverse problems}\label{sec:learning}

\vspace{-1mm}

As the previous section illustrates, the $\ELBOProxy$ provides a good criterion for choosing an appropriate IGM from noisy measurements. Here, we consider the task of learning the IGM $G$ directly from corrupted data. 
%Learning the IGM helps solve inverse problems by parameterizing the image model as a generative deep neural network and optimizing the $\ELBOProxy$. 
We consider the setting where we have access to a collection of $N$ measurements $y^{(i)} = f(x^{(i)}) + \eta^{(i)}$ for $i \in [N]$. The key assumption we make is that common, low-dimensional structure is shared across the underlying images $\{x^{(i)}\}_{i=1}^N$. %\edit{Note that the forward models and noise distributions need not be the same across a whole set of measurements.} 
%Relative to typical generative modelling-based approaches or supervised learning approaches, we assume we have very few examples $N$, on the order of only 10's of examples. %Note that the goal here is to solve for $m$ while also solving for the variational distribution for each $x^{(i)}$. Inspired by generative neural network-based methods, we propose to parameterize the image model $m$ as a deep neural network and to learn latent variational distributions to capture the measurement posteriors.

%\subsubsection{Learning approach.} 
%As each ground-truth image is drawn from the same distribution of interest, 
We propose to find a \textit{shared} generator $G_{\theta}$ with weights $\theta$ along with latent distributions $q_{\phi^{(i)}}$ that can be used to reconstruct the full posterior of each image $x^{(i)}$ from its corresponding noisy measurements $y^{(i)}$. This approach is illustrated in Fig.~\ref{fig:arch}. Having the generator be shared across all images helps capture their common collective structure. Each corruption, however, could induce its own complicated image posteriors. Hence, we assign each measurement $y^{(i)}$ its own latent distribution to capture the differences in their posteriors. While the learned distribution may not necessarily be the true image posterior (as we are optimizing a proxy of the ELBO), it still captures a distribution of images that fit to the observed measurements.


%We would like our IGM $G$ to capture shared properties of the $N$ ground-truth images underlying the set of measurements. Each corruption, however, could induce its own complicated image posteriors. Thus, we propose to find a \textit{shared} generator $G_{\theta}$ with weights $\theta$ along with latent distributions $q_{\phi^{(i)}}$ that can be used to reconstruct the full posterior of each image $x^{(i)}$ from its corresponding noisy observation $y^{(i)}$. 

 More explicitly, given a set of measurements $\{y^{(i)}\}_{i = 1}^N$, we optimize the $\ELBOProxy$ from Equation \eqref{eqref:ELBOProxy} to jointly infer a  generator $G_{\theta}$ and variational distributions $\{q_{\phi^{(i)}}\}_{i = 1}^N$:
 \vspace{-2mm}
\begin{align}
 \max_{\theta, \phi^{(i)}} & \frac{1}{N}\sum_{i = 1}^N \ELBOProxy(G_{\theta},q_{\phi^{(i)}}; y^{(i)}) + \log p(G_{\theta}). \label{eqref:learning-objective} %\E_{z \sim q_{\phi^{(i)}}(z)}\left[\log p(y^{(i)}|G_m(z)) + \log p(z|G_m) - \log q_{\phi^{(i)}}(z)\right]
\end{align}
The expectation in this objective is approximated via Monte Carlo sampling. In terms of choices for $\log p(G_{\theta})$, we can add additional regularization to promote particular structures, such as smoothness. Here, we consider having sparse neural network weights as a form of implicit regularization and use dropout during training to represent $\log p(G_{\theta})$ \cite{srivastava2014dropout}.
 
 Once a generator $G_{\theta}$ and variational parameters $\{\phi^{(i)}\}_{i=1}^N$ have been learned, we solve the $i$-th inverse problem by simply sampling $\hat{x}^{(i)} = G_{\theta}(\hat{z}^{(i)})$ where $\hat{z}^{(i)} \sim q_{\phi^{(i)}}$. Producing multiple samples for each inverse problem can help visualize the range of uncertainty under the learned IGM $G_{\theta}$, while taking the average of these samples empirically provides clearer estimates with better metrics in terms of PSNR or MSE. We report PSNR outputs in our subsequent experiments. %and also visualize the standard deviation of our reconstructions.%, which helps provide a notion of uncertainty.


