% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{authblk}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\include{macro}

% Title.
% ------
\title{Image Reconstruction Without Explicit Priors}
%
% Single address.
% ---------------
\name{Angela F. Gao$^{1*}$, Oscar Leong$^{1*}$, He Sun$^2$, Katherine L. Bouman$^1$}
\address{$^1$Computing and Mathematical Sciences, California Institute of Technology,  $^2$Peking University
\newline $^*$ denotes equal contribution}
% \author[1]{Angela F. Gao}
% \affil[1]{Computing and Mathematical Sciences, California Institute of Technology}
% %
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ %agency for funding.}}%
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\ninept
%
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
 \setlength{\textfloatsep}{10pt }
 \setlength{\abovecaptionskip}{1pt} 
 % \setlength{\belowcaptionskip}{1pt} 
\maketitle
%
\begin{abstract}
%We consider solving ill-posed imaging inverse problems under a generic forward model. Due to the ill-posedness of such problems, prior models that encourage certain image-based structure are required to reduce the space of possible images when solving for a solution. In many instances, obtaining access to a prior may be difficult or impossible. In contrast, we propose to solve inverse problems by directly learning an image prior from a collection of corrupted observations, without incorporating prior constraints on image structure. The key assumption of our work is that the ground-truth images we aim to reconstruct share common, low-dimensional structure. We show that one can capitalize on this structure by learning an image generator with a low-dimensional latent space such that each image posterior is modeled by the push-forward of the generator and a latent variational distribution. The parameters of the generator and variational distributions are learned by maximizing the Evidence Lower Bound (ELBO). We show that the ELBO is a suitable criterion to learn an image prior with experiments demonstrating that the ELBO can help select generative models directly from corrupted measurements. The framework we propose can handle general corruptions, and we show that only a small collection of measurements ($O(10)$ examples) are sufficient to learn effective priors.

We consider solving ill-posed imaging inverse problems without access to an explicit image prior or ground-truth examples. An overarching challenge in inverse problems is that there are many undesired images that fit to the observed measurements, thus requiring image priors to constrain the space of possible solutions to more plausible reconstructions. However, in many applications it is difficult or potentially impossible to obtain ground-truth images to learn an image prior. Thus, inaccurate priors are often used, which inevitably result in biased solutions. Rather than solving an inverse problem using priors that encode the explicit structure of any one image, we propose to solve a set of inverse problems jointly by incorporating prior constraints on the \textit{collective structure} of the underlying images.The key assumption of our work is that the ground-truth images we aim to reconstruct share common, low-dimensional structure. We show that such a set of inverse problems can be solved simultaneously by learning a shared image generator with a low-dimensional latent space.
%In essence, the core message of our work is that common structure across independent inverse problems is sufficient regularization alone.
%that can efficiently perform posterior sampling for each observation.
%We consider solving ill-posed imaging inverse problems without access to an explicit image prior. Ill-posed inverse problems are common in imaging applications ranging from denoising to recovery from compressed sensing measurements. In these problems, the measurements alone are not enough to constrain recovery of the clean underlying image, and an image prior must be employed to reduce the solution space when solving for the image. However, in many instances obtaining access to an accurate image prior may be difficult or impossible; using an inaccurate prior will result in biased solutions. In contrast, we propose to solve a collection of inverse problems jointly by incorporating prior constraints on the collection of measurements, rather than the image structure itself. The key assumption of our work is that the ground-truth images we aim to reconstruct share common, low-dimensional structure. We show that one can capitalize on this structure by learning an image generator with a low-dimensional latent space such that each image posterior is modeled by the push-forward of the generator and a latent variational distribution. 
The parameters of the generator and latent embedding are learned by maximizing a proxy for the Evidence Lower Bound (ELBO). Once learned, the generator and latent embeddings can be combined to provide reconstructions for each inverse problem.
%We show that the ELBO is a suitable criterion to learn an image ``prior" with experiments demonstrating that the ELBO can help select generative models directly from corrupted measurements. 
The framework we propose can handle general forward model corruptions, and we show that measurements derived from only a few ground-truth images ($O(10)$) are sufficient for image reconstruction without explicit priors. 
%In essence, the core message of our work is that common structure across multiple ill-posed inverse problems is sufficient regularization alone.

%We consider solving ill-posed imaging inverse problems without access to an explicit image prior or ground-truth examples. Ill-posed inverse problems are common in imaging applications ranging from denoising to recovery from compressed sensing measurements. An overarching challenge in these problems is that there are many undesired images that fit to the observed measurements, thus requiring image priors to constrain the space of possible solutions to more plausible reconstructions. However, in many instances either 1) innacurate priors are available, resulting in biased solutions or 2) it is either difficult or impossible to obtain ground-truth images to learn from. In contrast, we propose to solve a collection of inverse problems jointly by incorporating prior constraints on the collection of measurements, rather than the image structure itself. The key assumption of our work is that the ground-truth images we aim to reconstruct share common, low-dimensional structure. 
\end{abstract}
%
\begin{keywords}
Inverse problems, computational imaging, prior models, generative networks, Bayesian inference%, low-level vision, image reconstruction
\end{keywords}
%


\input{sections/sec_intro}
\input{sections/sec_approach}
\input{sections/sec_results_final}
\input{sections/sec_conclusion}

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
%\bibliography{strings,refs}
\bibliography{references.bib}
\end{document}