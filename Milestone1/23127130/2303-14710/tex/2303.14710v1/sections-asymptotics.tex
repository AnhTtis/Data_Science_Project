\section{Asymptotic results}\label{sec:asympt}

The characterisation of the labelled transition matrices of DOAGs gives a more
\emph{global} point of view on them compared to the decomposition given earlier,
which was more \emph{local}.
By approaching the counting problem from the point of matrices, we manage to
provide lower and upper bounds on the number of DOAGs with~$n$ vertices (and any
number of edges).
These bounds are precise enough to give a good intuition on the asymptotic
behaviour of these objects, and we then manage to refine them into an asymptotic
equivalent for their cardinality.
Building on this same approach, we provide in Section~\ref{sec:sampling:rej} an
efficient uniform sampler of DOAGs with~$n$ vertices.

\subsection{First bounds on the number of DOAGs
with~\texorpdfstring{$\boldsymbol n$}{n} vertices}

Let~$D_n = \sum_{m,k} D_{n,m,k}$ denote the number of DOAGs with~$n$ vertices
and any number of sources and edges.
By Theorem~\ref{thm:matrix}, all labelled transition matrices are variation
matrices.
A straightforward upper bound for~$D_n$ is thus given by the number of variation
matrices of size~$n$.

\begin{lemma}[Upper bound on the number of DOAGs]%
  \label{lem:doag:upper}
  For all~$n \geq 1$, the number~$D_n$ of DOAGs of size~$n$ satisfies
  \begin{equation*}
    D_n \leq \sfact{n-1} e^{n-1}
  \end{equation*}
  where~$\sfact{k} = \prod_{i=0}^k {i!}$ denotes the \emph{super factorial}
  of~$k$.
\end{lemma}
The term ``super factorial'' seems to have been coined by Sloane and Plouffe
in~\cite[page~228]{SP1995} but this sequence had been studied before that,
in~\citeyear{Barnes1900}, by Barnes~\cite{Barnes1900} as a special case of the
``G-function''.
In fact, if~$G(z)$ denotes the complex-valued G-function of Barnes, we have the
identity~$G(n+2) = \sfact{n}$ for all integer~$n$.
Barnes also gives the equivalent
\begin{equation}\label{eq:barnes}
  \sfact{n-1} = G(n+1) \sim
    e^{\zeta'(-1)} \cdot
    n^{-1 / 12} \cdot
    {\big(\sqrt{2 \pi}\big)}^n \cdot
    e^{- \frac{3}{4} n^2} \cdot
    n^{n^2 / 2}
\end{equation}
where~$\zeta$ denotes the Riemann zeta function.

In order to prove Lemma~\ref{lem:doag:upper}, we first need to give estimates
for the number~$v_n$ of variations of size~$n$.

\begin{lemma}[Exact and asymptotic number of variations]\label{lem:var}
  For all~$0 \leq p \leq n$, the number~$v_n$ of variations of size~$n$, and the
  number~$v_{n,p}$ of variations of size~$n$ containing exactly~$p$ zeros, are
  respectively given by
  \begin{equation*}
    v_n = e \cdot n!\left(1 - \sum_{j = n+1}^\infty \frac{e^{-1}}{j!}\right)
    \quad\text{and}\quad
    v_{n,p} = \frac{n!}{p!}
  \end{equation*}
  As a consequence~$v_n \leq e\cdot n{!}$ and~$v_n = e\cdot n! + o(1)$.
\end{lemma}
\begin{proof}
  Let~$0 \leq p \leq n$, a variation of size~$n$ containing exactly~$p$ zeros is
  the interleaving of a permutation of size~$(n-p)$ with an array of zeros of
  size~$p$.
  As a consequence
  \begin{equation*}
    v_{n,p} = \binom{n}{p} (n-p)! = \frac{n!}{p!}.
  \end{equation*}
  By summation, we then get
  \begin{equation*}
    v_n
    = \sum_{p=0}^n v_{n,p}
    = n! \sum_{p=0}^n \frac{1}{p!}
    = n! \left(\sum_{p=0}^\infty \frac{1}{p!} - \sum_{p=n+1}^\infty
    \frac{1}{p!}\right)
    = n! \left(e - \sum_{p=n+1}^\infty \frac{1}{p!}\right),
  \end{equation*}
  which allows to conclude.
\end{proof}

The proof of Lemma~\ref{lem:doag:upper} follows from this lemma.
\begin{proof}[Proof of Lemma~\ref{lem:doag:upper}]
  There are less DOAGs of size~$n$ that there are variations matrices.
  In addition, a variation matrix is given by a sequence~$v_1, v_2, \ldots,
  v_{n-1}$ of variations such that for all~$i$,~$v_i$ is of size~$i$.
  We thus have the following upper bound for~$D_n$:
  \begin{equation*}
    D_n
    \leq \prod_{i = 1}^{n - 1} v_{n-i}
    \leq \prod_{i = 1}^{n - 1} e \cdot (n-i)!
    = \sfact{n - 1} e^{n - 1}.
    \qedhere
  \end{equation*}
\end{proof}

Obtaining a lower bound on~$D_n$ requires to find a subset of the possible
labelled transition matrices described in Theorem~\ref{thm:matrix} that is both
easy to count and large enough to capture a large proportion of the DOAGs.
One possible such set is that of the labelled transition matrices which have
non-zero values on the super-diagonal~${(a_{i, i+1})}_{1 \leq i < n}$.
These correspond to DOAGs such that, at every step of the decomposition, we
have only one source and exactly one new source is uncovered.
In such matrices, the properties of the sequence~${(b_j)}_{1 \leq j \leq n}$
from Theorem~\ref{thm:matrix} are clearly satisfied and this leaves a lot of
free space on the right of that diagonal to encode a large number of possible
DOAGs.

\begin{lemma}[A first lower bound on the number of DOAGs]%
  \label{lem:doag:firstlower}
  There exists a constant~$A > 0$ such that for all~$n \geq 1$, we have
  \begin{equation*}
    \frac{A}n \sfact{n - 1} e^{n - 1} \leq D_n.
  \end{equation*}
\end{lemma}

\begin{proof}
  In a labelled transition matrix with positive values on the super-diagonal,
  the~$i$-th row can be seen as a variation of size~$(n-i)$ that does not start
  by a zero.
  Moreover, the number of variations of size~$n$ starting by a zero is actually
  the number of variations of size~$(n-1)$ so that the number of possibilities
  for the~$i$-th row of the matrix we count here is~$(v_{n-i} - v_{n-i-1})$.
  In addition, by Lemma~\ref{lem:var}, we also have that
  \begin{equation}
    v_n - v_{n-1}
    = e \cdot n! - e \cdot (n-1)! + o(1)
    = e \cdot n! \cdot \frac{n-1}{n} \left(1 +
    O\left(\frac{1}{n!}\right)\right).
  \end{equation}
  Note that when~$i = n-1$, we have~$v_{n-i} - v_{n-i-1} = v_1 - v_0 = 1$.
  Indeed, the row of index~$i = n-1$ contains only the number~$1$ in the super
  diagonal since at the last step of the decomposition, we have two connected
  vertices and there is only one such DOAG\@.
  Setting aside this special case, which does not contribute to the product, we
  get the following lower bound for~$D_n$:
  \begin{equation}
    D_n
    \geq \prod_{i=1}^{n-2} (v_{n-i} - v_{n-i-1})
    = e^{n-2} \sfact{n-1} \prod_{i = 1}^{n - 2} \frac{n - 1 - i}{n - i} \prod_{i
    = 1}^{n - 1} \left(1 + O\left(\frac{1}{(n - i)!}\right)\right)
  \end{equation}
  where the first product telescopes and yields~$\frac{1}{n-1}$ and the second
  one converges to a constant as~$n \to \infty$.
  This allows to conclude the proof the lemma.
\end{proof}


Although they are not precise enough to obtain an asymptotic equivalent for the
sequence~$D_n$, these two bounds already give us a good estimate of the
behaviour of~$D_n$.
First of all, they let appear a ``dominant'' term of the form~$\sfact{n-1}
e^{n-1}$, which is uncommon in combinatorial enumeration.
And second, it tells us we only make a relative error of the order of~$O(n)$
when approximating~$D_n$ by~$\sfact{n-1} \cdot e^{n-1}$.
We will prove an asymptotic equivalent for the remaining polynomial term, but in
order to obtain this, we first need to refine slightly our lower bound so that
the ``interval'' between our two bounds is a little narrower than~$\bigO{n}$.

\begin{lemma}[A better lower bound for the number of DOAGs]%
  \label{lem:doag:lower}
  There exists a constant~$B > 0$ such that, for all~$n \geq 1$, we have
  \begin{equation*}
    D_n \geq B \frac{\ln(n)}{n} \sfact{n - 1} e^{n - 1}.
  \end{equation*}
\end{lemma}

\begin{proof}
  In order to obtain this lower bound, we count the number of valid labelled
  transition matrices such that \emph{all but exactly one} of the cells on the
  super-diagonal have non-zero values.
  Furthermore, in order to avoid having to deal with border cases, we assume
  that this non-zero value appear between~$i=2$ and~$i=n-2$.
  Let~$2 \leq i \leq n - 2$ and let us consider those matrices~${(a_{p,q})}_{1
  \leq p, q \leq n}$ such that~$a_{i, i+1} = 0$.
  The differences from the matrices enumerated in the proof of the previous
  lemma are the following.
  \begin{enumerate}
    \item On row~$i - 1$, the two first cells on the right of the
      diagonal~($a_{i-1, i}$ and~$a_{i-1,i+1}$) must have positive values and
      must be in increasing order.
      In the case of~$a_{i-1,i}$, this is because it is on the super diagonal.
      And for~$a_{i-1,i+1}$, this is because~$a_{i,i+1} = 0$: since it is above
      this cell, and since the cell~$a_{i-1,1}$ on its left is non-zero, it must
      be non-zero.
      Otherwise the condition from Theorem~\ref{thm:matrix} are
      violated.
      In terms of DOAGs, this means that the vertex~$i-1$ produces two new
      sources when it is removed but vertex~$i$ produces none.\label{item:iminus1}
    \item On row~$i$, any variations of size~$(n-i)$ starting by a zero is
      allowed.\label{item:i}
  \end{enumerate}
  We get the number of variations of size~$n$ starting by two increasing
  positive values (condition~\ref{item:iminus1} above) by inclusion-exclusion.
  That is,
  \begin{itemize}
    \item consider all the variations of size~$n$~($v_n$ possibilities);
    \item remove the number of variations that have a zero in first
      position~($v_{n-1}$ possibilities);
    \item remove the number of variations that have a zero in second
      position~($v_{n-1}$ possibilities);
    \item add the number of variations that start with two zeros, because they
      have been removed twice in the two previous lines~($v_{n-2}$
      possibilities);
    \item and finally, divide by two because only half of these matrices have
      their first values in increasing order.
  \end{itemize}
  This yields the following formula for counting such variations:
  \begin{equation*}
    \frac{v_n - 2 v_{n-1} + v_{n-2}}{2}
    \underset{n\to\infty}\sim \frac{e}{2} \cdot n!.
  \end{equation*}
  As a consequence, the total number of labelled transition matrices considered
  at the beginning of the proof, such that~$a_{i,i+1} = 0$, is given by
  \begin{align*}
    &\prod_{\substack{1 \leq p \leq n-1 \\ p \not\in \{i-1, i\}}} (v_{n-p} - v_{n-p-1})
    \cdot \frac{v_{n-i-1} - 2 v_{n-2-i} + v_{n-3-i}}{2}
    \cdot v_{n-i-1} \\
    &= \prod_{p=1}^{n-1} (v_{n-p} - v_{n-p-1})
    \cdot \frac{(v_{n-i-1} - 2 v_{n-2-i} + v_{n-3-i}) v_{n-i-1}}%
    {2(v_{n-i+1} - v_{n-i})(v_{n-i} - v_{n-i-1})}\cdot
  \end{align*}
  By summing over~$2 \leq i \leq n - 2$, we get
  \begin{equation}\label{eq:proof:lb}
    \prod_{p=1}^{n-1} (v_{n-p} - v_{n-p-1})
    \sum_{i=2}^{n-2}
    \frac{(v_{n-i-1} - 2 v_{n-2-i} + v_{n-3-i}) v_{n-i-1}}%
    {2(v_{n-i+1} - v_{n-i})(v_{n-i} - v_{n-i-1})}\cdot
  \end{equation}
  The fraction in the last equation is equivalent to~$\frac{1}{2(n-i)}$
  when~$n-i \to \infty$.
  So after a change of variable, the above sum is equivalent to
  \begin{equation*}
    \sum_{i=2}^{n-2}
    \frac{(v_{i-1} - 2 v_{i-2} + v_{i-3}) v_{i-1}}%
    {2(v_{i+1} - v_i)(v_{i} - v_{i-1})}
    \sim\sum_{i=2}^{n-2} \frac{1}{2i}
    \sim \frac{\ln(n)}{2}
  \end{equation*}
  In addition, we know from the proof of Lemma~\ref{lem:doag:firstlower} that
  the product in equation~\eqref{eq:proof:lb} is equivalent to~$\frac{c}{n}
  e^{n-1} \sfact{n-1}$ for some constant~$c$, which allows to conclude.
\end{proof}

\subsection{Obtaining the polynomial term by bootstrapping}

Let us denote~$P_n$ the polynomial term in~$D_n$, that is the quantity
\begin{equation*}
  P_n \overset{\text{def}}= \frac{D_n}{\sfact{n-1} e^{n-1}}.
\end{equation*}
We have proved above that for some constant~$B > 0$, we have~$B \frac{\ln(n)}{n}
\leq P_n \leq 1$.
A consequence of these inequalities is that for all~$k \in \ZZ$, we have
\begin{equation}\label{eq:Pnplusk}
  P_{n+k} \leq 1 \leq \frac{n}{B \ln(n)} P_n \underset{n \to \infty}= o(n P_n).
\end{equation}
Note that the extra~$\ln(n)$ factor that we obtained in
Lemma~\ref{lem:doag:lower} is crucial to prove this fact.
Equation~\eqref{eq:Pnplusk} allows to justify that~$P_{n+k} / n$ is negligible
compared to~$P_n$, for constant values of~$k$.
Although intuitively the contrary would be surprising, this fact is not clear
\textit{a priori} as an arbitrary polynomial sequence~$P_n$ could have violent
oscillations for some values of~$n$.
This is a key ingredient for proving an asymptotic equivalent for~$P_n$.

To refine our knowledge on the sequence~$P_n$, we use a decomposition of the
labelled transition matrices focused on the values it takes near the diagonal on
its first rows.
We categorise the possible labelled transition matrices~${(a_{i,j})}_{1 \leq i,
j \leq n}$ into the four following cases.
\begin{description}
  \item[Case 1:~$\boldsymbol{a_{1,2} = 0}$.] In this case, the first source is
    not connected to the second vertex and the matrix has thus more than one
    source.
    The first row of such a matrix is a variation of size~$(n-2)$ and the lower
    part~${(a_{i,j})}_{2 \leq i, j \leq n}$ encodes a DOAG of size~$(n-1)$, the
    DOAG obtained by removing the first source.
    However, it is important to note that not all combinations of a size-$(n-2)$
    variation and a size-$(n-1)$ matrix yield a valid size-$n$ labelled
    transition matrix.
    For instance, a variation of the form~$v = (0, 1, 0, 2, \ldots)$ and a lower
    matrix with at least three sources cannot be obtained together as they would
    violate the constraints of Theorem~\ref{thm:matrix}.
  \item[Case~2:~$\boldsymbol{a_{1,2} > 0 \wedge a_{2, 3} > 0}$.] In this case,
    the first row is a variation of size~$(n-1)$ starting by a positive value,
    and the lower part~${(a_{i,j})}_{2 \leq i, j \leq n}$ encodes a DOAG of
    size~$(n-1)$ with exactly one source, again obtained by removing the first
    source.
    This second fact is a direct consequence of~$a_{2,3} > 0$.
    Here, all such pairs can be obtained.
  \item[Case~3:~$\boldsymbol{a_{1,2} > 0 \wedge a_{2,3} = 0 \wedge a_{3,4} >
    0}$.] In this case the lower part~${(a_{i,j})}_{3 \leq i, j \leq n}$ encodes
    a DOAG of size~$n-2$ with exactly one source, the first row is necessarily a
    variation of size~$(n-1)$, starting with two positive increasing values, and
    the second row is a variation of size~$(n-2)$ starting by a zero.
    Here again this decomposition is exact: all such triplets can be obtained
    here.
  \item[Case~4:~$\boldsymbol{a_{1,2} > 0 \wedge a_{2, 3} = a_{3, 4} = 0}$.]
    Finally, this case captures all the remaining matrices.
    The first row is a variation of size~$(n-1)$, the second and third rows are
    variations of sizes~$(n-2)$ and~$(n-3)$ starting with a zero, and the lower
    part~${(a_{i,j})}_{4 \leq i, j \leq n}$ encodes a size-$(n-3)$ DOAG\@.
    Of course, not all such quadruples can be obtained, but this
    over-approximation will be enough for our proof.
\end{description}
This decomposition into four different cases is pictured in
Figure~\ref{fig:mat:decomp} where~$\mathcal D$ represents the set of all
possible DOAG labelled transition matrices and~${\mathcal D}^\star$ represents
all of those matrices that encode single-source DOAGs.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{images-bootstrap}
  \caption{Decomposition of DOAG labelled transition matrices based or their
  content near the top of the diagonal.
  The symbols~$\mathcal D$ and~${\mathcal D}^\star$ respectively represent the
  set of all possible DOAG labelled transition matrices the set of all of those
  matrices such that~$a_{1,2} > 0$.
  The stars~$(\star)$ represent strictly positive values.}%
  \label{fig:mat:decomp}
\end{figure}

We compute the contributions to~$D_n$ coming from each of these four terms
described above.
Let us denote by~$D_n^\star$ the number of DOAG of size~$n$ with exactly one
source, or equivalently the number of DOAG labelled transition matrices
containing a non-zero value at coordinates~$(1,2)$.
The first line of Figure~\ref{fig:mat:decomp} illustrate the first point of the
decomposition, which yields
\begin{equation}\label{eq:Dn0}
  D_n = D_n^\star + O(v_{n-2} D_{n-1}).
\end{equation}
Note that the big oh comes from the fact that not all pairs made of a
size-$(n-2)$ variation and a size-$(n-1)$ labelled transition matrix can be
obtained this way, as discussed in the first case above.

Then we decompose the matrices from~${\mathcal D}^\star$ depending of their
values on the diagonal (cases~2 to~3).
The second line of Figure~\ref{fig:mat:decomp} illustrates this decomposition.
This translates into the following identity
\begin{equation}\label{eq:Dnstar}
  D_n^\star
  = (v_{n-1}-v_{n-2}) D_{n-1}^\star
  + \frac{v_{n-1} - 2 v_{n-2} + v_{n-3}}{2} v_{n-3} D_{n-2}^\star
  + \bigO{v_{n-1} v_{n-3} v_{n-4} D_{n-3}}.
\end{equation}

Let us introduce the polynomial term~$P_n^\star$ of~$D_n^\star$ defined
by~$P_n^\star = D_n^\star / e^{n-1} \sfact{n-1}$.
By normalising equation~\eqref{eq:Dn0} and using equation~\eqref{eq:Pnplusk} we
have
\begin{equation*}
  P_n^\star
  = P_n + \bigO{\frac{v_{n-2}}{e (n-1)!} P_{n-1}}
  = P_n + \bigO{\frac{P_{n-1}}{n}}
  = P_n + \littleO{P_n}.
\end{equation*}
In other words, we have that~$P_n$ and~$P_n^\star$ are equivalent.
Then, by normalising equation~\eqref{eq:Dnstar} by~$e^{n-1} \sfact{n-1}$, we
obtain the following asymptotic expansion
\begin{align}\label{eq:Pnstar}
  P_n^\star
  = \left(1 - \frac{1}{n} + \bigO{\frac{1}{n^2}}\right) P_{n-1}^\star
  + \frac{1}{2n} \left(1 + \bigO{\frac{1}{n}}\right) P_{n-2}^\star
  + \bigO{\frac{P_{n-3}}{n^2}}\cdot
\end{align}
Since~$P_n^\star \sim P_n$ and by equation~\eqref{eq:Pnplusk}, we have
that~$\bigO{n^{-3}} = \littleO{P_n^\star n^{-2}}$ and that the first term of
equation~\eqref{eq:Pnstar} dominates all the others.
As a consequence we get a refinement on our knowledge on~$P_n^\star$ (and
thus~$P_n$), that is:
\begin{equation*}
  P_n^\star \sim P_{n-1}^\star.
\end{equation*}
By re-using this new information in equation~\eqref{eq:Pnstar}, we get another
term of the expansion of~$P_n^\star$:
\begin{equation*}
  P_n^\star
  = P_{n-1}^\star \left(1 - \frac{1}{2n} + \bigO{\frac{1}{n^2}}\right).
\end{equation*}
We conclude on the asymptotic behaviour of~$P_n^\star$ using the following
classical argument.
The series of general term~$\ln\left(\!\frac{P_n^\star}{P_{n-1}^\star}\!\right)
+ \frac{1}{2n} = \bigO{n^{-2}}$ (defined for~$n \geq 2$) is convergent and,
if~$\lambda$ denotes its sum, we have that
\begin{equation*}
  \lambda - \sum_{j=2}^n \left(
    \ln\left(\!\frac{P_j^\star}{P_{j-1}^\star}\!\right)
    + \frac{1}{2j}
  \right)
  = O(n^{-1}).
\end{equation*}
Furthermore, since~$P_1^\star = 1$, we also have that
\begin{equation*}
  \sum_{j=2}^n \left(
    \ln\left(\!\frac{P_j^\star}{P_{j-1}^\star}\!\right)
    + \frac{1}{2j}
  \right)
  = \ln P_n^\star + \frac{1}{2} \left(\ln(n) + \gamma + \bigO{n^{-1}}\right)
\end{equation*}
where~$\gamma$ denotes the Euler{--}Mascheroni constant.
As a consequence, we have that
\begin{equation*}
  \ln P_n^\star + \frac{\ln(n)}{2} = \lambda + \gamma + \bigO{n^{-1}}
\end{equation*}
and thus
\begin{equation*}
  P_n^\star = \frac{e^{\lambda + \gamma}}{\sqrt{n}} \left(1 +
    \bigO{n^{-1}}\right).
\end{equation*}
By equation~\eqref{eq:Dn0}, we also get that~$P_n = P_n^\star \left(1 +
\bigO{n^{-1}}\right)$, which allows us to state the following theorem.
\begin{theorem}\label{thm:Dnequiv}
  There exists a constant~$C > 0$ such that the number~$D_n$ of DOAGs with~$n$
  vertices and the number~$D_n^\star$ of such DOAGs having only one source
  satisfy
  \begin{equation*}
    D_n
    = (1 + \bigO{n^{-1}}) D_n^\star
    = \frac{C}{\sqrt{n}} \ e^{n-1} \ \sfact{n-1} \left(1 + \bigO{n^{-1}}\right).
  \end{equation*}
\end{theorem}

The super factorial provides a concise way to express this equivalent and also
reflects the relation between DOAGs and variation matrices.
An equivalent without factorial can also be expressed using the formula
from~\cite{Barnes1900} recalled in equation~\eqref{eq:barnes} above.
For some constant~$c$, we have
\begin{equation*}\label{eq:Dnequiv2}
  D_n \sim
    c \cdot
    n^{-7 / 12} \cdot
    {\big(e\sqrt{2 \pi}\big)}^n \cdot
    e^{- \frac{3}{4} n^2} \cdot
    n^{n^2 / 2}.
\end{equation*}

\subsection{Approximation of the constant}

Let~$D_{n,k}$ denote the number of DOAGs with~$n$ vertices (including~$k$
sources and one sink) and any number of edges.
Using the same decomposition as in Section~\ref{sec:def} and applying the same
combinatorial arguments we get
\begin{equation}
  D_{n,k}
  = \sum_{i+s \leq n-k} D_{n-1,k-1+s} \binom{s+i}{s} \binom{n-k-s}{i} i!
  = \sum_{s \geq 0} D_{n-1,k-1+s} \cdot \gamma(n-k-s,s)
\end{equation}
where
\begin{equation}
  \gamma(a,b) =\sum_{i=0}^a \binom{b+i}{b} \binom{a}{i} i!.
\end{equation}

The above sum gives an explicit way to compute~$\gamma$, but there is a
computationally more efficient way to do so using recursion and memoisation:
\begin{equation}\label{eq:gamma:rec}
  \begin{aligned}
    \gamma(a, b) &= 0 &\text{when~$a < 0$ or~$b < 0$} \\
    \gamma(0, b) &= 1 &\text{when~$b \geq 0$} \\
    \gamma(a, b) &= \gamma(a, b - 1) + a\cdot\gamma(a - 1, b) + \indicator{b = 0}
    & \text{otherwise.}
  \end{aligned}
\end{equation}

Using this recurrence formula with memoisation, the numbers~$D_{n,k}$ for
all~$n,k \leq N$ can be computed in~$O(N^3)$ arithmetic operations on big
integers.
Note that the~$D_n^\star$ sequence defined above corresponds to~$D_{n,1}$.
Using the numbers computed by this algorithm, we plotted the first~$250$ values
of the sequences~$D_n$ and~$D_n^\star$ normalised by~$n^{-1/2} e^{n-1}
\sfact{n-1}$ which shows the convergence to the constant~$C$ from
Theorem~\ref{thm:Dnequiv}.
We also note that the convergence looks faster for the sequence~$D_n^\star$.
This suggests that the constant can be approximated by~$C \approx 0.4967$.
Figure~\ref{fig:plot:Dn} shows this plot as well as a zoomed-in version
near~$\frac{1}{2}$ for~$n \geq 200$.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[xscale=0.025,yscale=4]
    \foreach \x in {0,10,...,250} \draw[black!25] (\x, 0) -- (\x, 1);
    \foreach \y in {0,0.05,...,1} \draw[black!25] (0, \y) -- (250, \y);
    \foreach \x in {0,50,...,250} {
      \draw (\x,0) node[below] {\x};
      \draw[black!50] (\x, 0) -- (\x, 1);
    }
    \foreach \y in {0,0.2,0.4,0.6,0.8,1} {
      \draw (-1,\y) node[left] {\y};
      \draw[black!50] (0,\y) -- (250,\y) {};
    }
    \draw[thick,->] (0,0) -- (255,0);
    \draw[thick,->] (0,0) -- (0, 1.05);
    \draw[Sapphire Blue,semithick] plot file {data-Dnorm.txt};
    \draw[Carmine,semithick] plot file {data-Dstarnorm.txt};
  \end{tikzpicture}
  \begin{tikzpicture}[xscale=0.125,yscale=200]
    \foreach \x in {200,202,...,250} \draw[black!25] (\x, 0.49) -- (\x, 0.51);
    \foreach \y in {0.49,0.491,...,0.51} \draw[black!25] (200, \y) -- (250, \y);
    \foreach \x in {200,210,...,250} {
      \draw (\x,0.49) node[below] {\x};
      \draw[black!50] (\x, 0.49) -- (\x, 0.51);
    }
    \foreach \y in {0.49,0.495,0.5,0.505,0.51} {
      \draw (199,\y) node[left] {\y};
      \draw[black!50] (200,\y) -- (250,\y) {};
    }
    \draw[thick,->] (200,0.49) -- (251,0.49);
    \draw[thick,->] (200,0.49) -- (200, 0.51);
    \clip (200,0.49) rectangle (250,0.51);
    \draw[Sapphire Blue,semithick] plot file {data-Dnorm.txt};
    \draw[Carmine,semithick] plot file {data-Dstarnorm.txt};
  \end{tikzpicture}
  \caption{The first values of the sequences~$\displaystyle{\frac{D_n
  \sqrt{n}}{\sfact{n-1} e^{n-1}}}$ in blue and~$\displaystyle{\frac{D_n^\star
  \sqrt{n}}{\sfact{n-1} e^{n-1}}}$ in red.}%
  \label{fig:plot:Dn}
\end{figure}

\subsection{Asymptotic behaviour of some parameters}

\subsubsection{Number of sources}

It follows from the previous section that the probability that a uniform DOAG of
size~$n$ has more than one source tends to zero as~$n\to\infty$.
We can refine this result and compute the probability of having a constant
number~$k$ of sources.

We have that the number of sources of a DOAG is also the number of empty columns
in its labelled transition matrix, and that these columns are necessarily in
first positions.
Moreover, Theorem~\ref{thm:Dnequiv} gives us the intuition that most of those
transition matrices contain positive numbers near the top-left corner of the
matrix.
We thus split the set of matrices~${(a_{i,j})}_{1 \leq i, j \leq n}$ encoding
DOAGs with~$k$ sources in two categories.
\begin{description}
  \item[Case~$\boldsymbol{a_{k,k+1} > 0}$.] Intuitively, the most common
    scenario is that there is a positive entry in~$a_{k,k+1}$.
    In this case the sub-matrix~${(a_{i,j})}_{k \leq i, j \leq n}$ can be
    re-interpreted as a DOAG with only one source.
    Indeed, the condition~$a_{k,k+1}$ means that upon removing the~$(k-1)$ first
    sources of the DOAG, the decomposition algorithm does not produce any new
    source, leaving us with a single-source DOAG\@.
    We can characterise those matrices: they are made of~$(k-1)$ variations of
    size~$(n-k)$ in the first rows, and a size-$(n-k+1)$ labelled transition
    matrix corresponding to a single source DOAG below them.
    Any combination of~$(k-1)$ such variations and such matrices can be
    obtained.
  \item[Case~$\boldsymbol{a_{k,k+1} = 0}$.] On the other hand we have the
    matrices such that~$a_{k,k+1} = 0$.
    In this case, the~$(k-1)$ first rows of the matrix are still variations of
    size~$(n - k)$.
    The lower part~${(a_{i,j})}_{k \leq i, j \leq n}$ can be seen as a DOAG with
    at least two sources because its first two columns are empty.
    Note that here, depending of the~$(k-1)$ top variations, we may have
    restriction on which DOAGs may appear in the lower part.
    For instance, if~$k=2$ and the first row is~$(0, 0, 1, 0, 0, \ldots, 0)$,
    then the DOAG of size~$(n-1)$ without any edge cannot appear in the lower
    part.
\end{description}
This dichotomy is pictured in Figure~\ref{fig:Dnk:decomp}.
\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{images-dnkdecomp}
  \caption{Decomposition of the matrices corresponding to DOAGs with~$k$
  sources}\label{fig:Dnk:decomp}
\end{figure}

From the above case analysis, we have the following bounds:
\begin{equation*}
  v_{n-k}^{k-1} D_{n-k+1}^\star
  \leq D_{n,k}
  \leq v_{n-k}^{k-1} D_{n-k+1} + \bigO{v_{n-k}^{k-1} (D_{n-k+1} -
  D_{n-k+1}^\star)}.
\end{equation*}
Thus, by virtue of Theorem~\ref{thm:Dnequiv}, we have the following estimates
when~$(n-k) \to\infty$
\begin{equation}\label{eq:dnk:bounds}
  D_{n,k} = v_{n-k}^{k-1} D_{n-k+1}^\star \left(1 + \bigO{\frac{1}{n-k}}\right),
\end{equation}
which allow us to state the following result.

\begin{theorem}[Number of sources of uniform DOAGs]
  When~$n \to\infty$ and~$(n-k)\to\infty$, we have that
  \begin{equation*}
    D_{n,k} - v_{n-k}^{k-1} D_{n-k+1} = o(D_{n,k})
  \end{equation*}
  where the little oh is uniform: it is arbitrarily smaller than~$D_{n,k}$
  when~$(n-k) \to \infty$.
  In particular for~$k$ constant, we have
  \begin{equation*}
    \frac{D_{n,k}}{D_n} \sim n^{-\binom{k}{2}}.
  \end{equation*}
\end{theorem}

\begin{proof}
  The first statement has already been established in
  equation~\eqref{eq:dnk:bounds} and the second one is straightforward to obtain
  using the equivalent~$v_n \sim e n{!}$ for the number of variations.
\end{proof}

\subsubsection{Number of edges}

Another quantity of interests of uniform DOAGs (and graphs in general) is their
number of edges.
Whereas uniform labelled DAGs have~$\frac{n^2}{4}$ edges in average, we show
here that the number of edges of uniform DOAGs is close to~$\frac{n^2}{2}$.
This has to be compared with their maximum possible number of edges which
is~$\binom{n}{2} = \frac{n(n-1)}{2}$.
This makes uniform DOAGs quite dense objects.
The intuition behind this fact is that variations have typically few zeros in
them.
Indeed, the expected number of zeros of a uniform variation is given by
\begin{equation*}
  \frac{1}{v_n} \sum_{p=0}^n p v_{n,p}
  = \frac{n!}{v_n} \sum_{p=0}^n \frac{p}{p!}
  = \frac{n!}{v_n} \sum_{p=1}^n \frac{1}{(p-1)!}
  \underset{n\to\infty}\to 1
\end{equation*}
where~$v_{n,p}$ is the number of variations of size~$n$ having exactly~$p$
zeros, which is equal to~$n!/p{!}$ by Lemma~\ref{lem:var}.
Moreover, the tail of their probability distribution is more than exponentially
small:
\begin{equation*}
  \PP_n[\text{nb zeros} \geq q]
  = \frac{n!}{v_n} \sum_{p=q}^n \frac{1}{p!}
  \underset{n,q\to\infty}=
  \frac{e^{-1}}{k!}
  \left(1 + \littleO{\frac{1}{n!}}\right)
  \left(1 + \bigO{\frac{1}{q}}\right),
\end{equation*}
where the first error term depends only on~$n$ and the second depends on~$q$ and
is uniform in~$n$.
Not, recall that DOAG labelled transition matrices are a sub-class of variations
matrices, and that the number of non-zero entries in these matrices corresponds
to the number of edges of the graph.
This discussion should make the following result intuitive.

\begin{theorem}[Number of edges of uniform DOAGs]\label{thm:edges}
  The number of edges of a uniform DOAG of size~$n$ is, in expectation,
  \begin{equation*}
    \binom{n}{2} - O(n).
  \end{equation*}
\end{theorem}

\begin{proof}
  In terms of labelled transition matrices, the theorem translates into: there
  is at most a linear number of zeros strictly above the diagonal in the matrix.
  This is what we prove here.

  For all integer~$p \geq 0$, by inclusion, we have that the number of DOAG
  labelled transition matrices with exactly~$p$ zeros strictly above the
  diagonal is upper-bounded by the number~$\VM_{n,p}$ of variation matrices with
  the same property.
  Moreover, given a vector~$(p_1, p_2, \ldots, p_{n-1})$ of non-negative
  integers such that for all~$i$,~$p_i \leq i$, the number of such variation
  matrices with exactly~$p_{n-i}$ zeros in the~$i$-th line is
  \begin{equation*}
    \prod_{i=1}^{n-1} v_{i,p_i}
    = \prod_{i=1}^{n-1} \frac{i!}{p_i!}
    = \sfact{n-1} \prod_{i=1}^{n-1} \frac{1}{p_i!}.
  \end{equation*}
  By summation over all such vectors such that~$\sum_{i=1}^{n-1}p_i = p$, we get
  an expression for~$\VM_{n,p}$:
  \begin{equation*}
    \VM_{n,p}
    =
    \sfact{n-1}
    \sum_{\substack{
      p_1 + p_2 + \cdots + p_{n-1} = p \\
      \text{for all~$i$}, 0 \leq p_i \leq i
    }} \prod_{i=1}^{n-1} \frac{1}{p_i!}
    \leq
    \sfact{n-1}
    \sum_{\substack{
      p_1 + p_2 + \cdots + p_{n-1} = p \\
      \text{for all~$i$}, 0 \leq p_i
    }} \prod_{i=1}^{n-1} \frac{1}{p_i!}\cdot
  \end{equation*}
  In the first sum we have the constraint~$p_i \leq i$ because a variation has
  at most~$i$ zeros.
  The inequality comes from the fact that we added more terms in the sum by
  dropping this constraints.
  This allows us to interpret the sum as a Cauchy product and can express it as
  the~$p$-th coefficient of the power series~$e^x \cdot e^x \cdot \cdots \cdot
  e^x = e^{(n-1)x}$.
  It follows that
  \begin{equation*}
    \VM_{n,p} \leq \sfact{n-1} \frac{{(n-1)}^p}{p!}\cdot
  \end{equation*}
  As a consequence, we have the following bound for the probability that a
  uniform DOAG of size~$n$ has at most~$\binom{n}{2} - q$ zeros:
  \begin{equation}\label{eq:prob:zeros}
    \PP_n[\text{a uniform DOAG has at most~$\binom{n}{2} - q$ zeros}]
    \leq \frac{\sfact{n-1}}{D_n} \sum_{p \geq q} \frac{{(n-1)}^p}{p!}\cdot
  \end{equation}
  The sum in the last equation is the remainder in the Taylor expansion of
  order~$(q-1)$ of the function~$e^x$ near zero, evaluated at the point~$(n-1)$.
  By using the integral form of this remainder, we have that
  \begin{equation*}
    \sum_{p \geq q} \frac{{(n-1)}^p}{p!}
    = \int_0^{n-1} e^t \frac{{(n - 1 - t)}^{q-1}}{(q-1)!} dt
    \leq e^{n-1} \int_0^{n-1} \frac{{(n-1-t)}^{q-1}}{(q-1)!} dt
    = e^{n-1} \frac{{(n-1)}^q}{q!}\cdot
  \end{equation*}
  Furthermore, by setting~$q = \lambda (n-1)$ for some constant~$\lambda > 0$,
  and by using Stirling's formula, we get that
  \begin{equation*}
    \frac{{(n-1)}^q}{q!}
    \sim {\left(\frac{e(n-1)}{q}\right)}^q \frac{1}{\sqrt{2\pi q}}
    \sim {\left(\frac{e^\lambda}{\lambda^\lambda}\right)}^{n-1}
    \frac{1}{\sqrt{2\pi\lambda n}}\cdot
  \end{equation*}
  Finally, by using this estimate inside equation~\eqref{eq:prob:zeros}, and by
  using of Theorem~\ref{thm:Dnequiv} for estimating~$D_n$, we get that there
  exists a constant~$c > 0$ such that
  \begin{equation*}
    \PP_n[\text{a uniform DOAG has at most~$\binom{n}{2} - \lambda (n-1)$ zeros}]
    \leq \frac{\sfact{n-1}e^{n-1}}{D_n} \sum_{p \geq q} \frac{{(n-1)}^q}{q!}
    \leq \frac{c}{\sqrt{\lambda}}
    {\left(\frac{e^\lambda}{\lambda^\lambda}\right)}^{\mathclap{\,\,n-1}}\cdot
  \end{equation*}
  The latter expression is exponentially small as soon as~$\lambda > e$ and
  dominates the tail of the probability distribution of the number of zeros
  strictly above the diagonal in DOAG labelled transition matrices, which allows
  to conclude.
\end{proof}
