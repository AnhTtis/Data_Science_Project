\section{Uniform sampling of DOAGs by vertices only}\label{sec:sampling:rej}

The knowledge from the previous section on the asymptotic number of DOAGs
with~$n$ vertices can be interpreted combinatorially to devise an efficient
uniform random sampler of DOAGs based on rejection.
Since the set of labelled transition matrices of size~$n$ is included in the set
of variation matrices of size~$n$, a possible approach to sample uniform DOAGs
is to sample uniform variation matrices until they satisfy the properties of
Theorem~\ref{thm:matrix}, and thus encode a DOAG\@.

Since the number of variation matrices is close (up to a factor of the order
of~$\sqrt{n}$) to the number of DOAGs, the probability that a uniform variation
matrix of size~$n$ corresponds to the labelled transition matrix of DOAG is of
the order of~$n^{-\frac{1}{2}}$.
As a consequence, the expected number of rejections done by the procedure
outline above is of the order of~$\sqrt{n}$ and its overall cost is~$\sqrt{n}$
times the cost of generating one variation matrix.
Moreover, we will see that variations (and thus variation matrices) are cheap to
sample, which makes this procedure efficient.

This idea, which is a textbook application of the rejection principle, already
yields a reasonably efficient sampler of DOAGs.
In particular it is much faster than the sampler from the previous section based
on the recursive method, because it does not have to carry arithmetic operations
on big integers.
In this section we show that this idea can be pushed further using ``early
rejection''.
That is to say we check the conditions from Theorem~\ref{thm:matrix} on the fly
when generating the variation matrix, in order to be able to abort the
generation as soon as possible if the matrix is to be rejected.
We will describe how to generate as few elements of the matrix as possible to
decide whether to reject it or not, so as to mitigate the cost of these
rejections.

First, we design an asymptotically optimal uniform sampler of variations in
Section~\ref{sec:variation}, and then we show in Section~\ref{sec:fastrej}
how to leverage this into an asymptotically optimal sampler of DOAGs.

\subsection{Generating variation}\label{sec:variation}

The first key step towards generating DOAGs, is to describe an efficient uniform
random sampler of variations.
We observe that the law of the number of zeros of a uniform variation of
size~$n$ obeys a Poisson law of parameter~$1$ conditioned to be at most~$n$.
Indeed,
\begin{equation*}
  \PP[\text{a uniform variation of size~$n$ has~$p$ zeros}]
  = \frac{v_{n,p}}{v_n} \propto \frac{\indicator{0 \leq p \leq n}}{p!}
\end{equation*}
by Lemma~\ref{lem:var}.
A possible way to generate a uniform variation is thus to draw a Poisson
variable~$p$ of parameter~$1$ conditioned to be at most~$n$, and then shuffling
a size~$p$ array of zeros with a uniform permutation using the Fisher-Yates
algorithm~\cite{FY1948}.
This is described in Algorithm~\ref{algo:variation}.

\begin{algorithm}[htb]
  \caption{Uniform random sampler of variations based on the rejection
  principle.\label{algo:variation}}
  \begin{algorithmic}[1]
    \Require{An integer~$n > 0$}
    \Ensure{A uniform random variation of size~$n$}
    \Function{UnifVariation}{$n$}
      \assign{$p$}{\Call{BoundedPoisson}{$1, n$}}\label{line:callpois}
      \assign{$A$}{$[0, 0, \ldots, 0, 1, 2, \ldots, n - p]$}%
      \label{line:initarray}%
      \Comment{array of length~$n$, starting with~$p$ zeros}
      \For{$i = 0$ \textbf{to}~$n - 2$}\label{line:shuffle}
        \assign{$r$}{\Call{Unif}{$\llbracket i; n - 1 \rrbracket$}}
        \State{$A[r] \leftrightarrow A[i]$}
        \Comment{Swap entries of indices~$r$ and~$i$}
      \EndFor{}
      \return{$A$}
    \EndFunction{}
  \end{algorithmic}
\end{algorithm}

Regarding the generation of the bounded Poisson variable (performed at
line~\ref{line:callpois}), an efficient approach is to generate regular
(unbounded) Poisson variables until a value less than~$n$ is found.
Since a Poisson variable of parameter~$1$ has a high probability to be small,
this succeeds in a small bounded number of tries on average.
The algorithm described by Knuth in~\cite[page~137]{Knuth1997} is suitable for
our use-case since our Poisson parameter~($1$ here) is small.
Furthermore it can be adapted to stop early when values strictly larger than~$n$
are found.
This is described in~Algorithm~\ref{algo:poisson}.

\begin{algorithm}[htb]
  \caption{Adapted Knuth's algorithm for bounded Poisson simulation}%
  \label{algo:poisson}
  \begin{algorithmic}[1]
    \Require{A Poisson parameter~$\lambda > 0$ and an integer~$n \geq 0$}
    \Ensure{A Poisson variable of parameter~$\lambda$ conditioned to be at most~$n$}
    \Function{BoundedPoisson}{$\lambda, n$}
      \Repeat{}\label{line:pois:outer}
        \assign{$k$}{$0$}
        \assign{$p$}{\Call{Unif}{$[0; 1]$}}
        \While{$(k \leq n) \wedge (p > e^{-\lambda})$}\label{line:pois1}
          \assign{$k$}{$k + 1$}
          \assign{$p$}{$p \cdot \text{\Call{Unif}{$[0; 1]$}}$}
        \EndWhile{}\label{line:pois2}
      \Until{$k \leq n$}
      \return{$k$}
    \EndFunction{}
  \end{algorithmic}
  \medskip
  \textit{NB}.\ The~$\funname{Unif}([0; 1])$ function generates a uniform real
  number in the~$[0; 1]$ interval.
\end{algorithm}

Note that this algorithm relies on real numbers arithmetic.
In practice, approximating these numbers by IEEE~754 floating points
numbers~\cite{ieee754} should introduce an acceptably small error.
Indeed, since we only compute products (no sums or subtractions), which
generally have few terms, the probability that they introduce an error should
not be too far from~$2^{-53}$ on a 64-bits architecture.
Of course this is only a heuristic argument.
A rigorous implementation must keep track of these errors.
One possible way would be to use fixed points arithmetic for storing~$p$ and to
lazily generate the base 2 expansions of the uniform variables at play until we
have enough bits to decide how~$p$ and~$e^{-\lambda}$ compare at
line~\ref{line:pois1}.
Another way would be to use Ball arithmetic~\cite{Hoeven2010, arb} and to
increase precision every time the comparison requires more bits.
The proofs of correctness and complexity below obviously assume such an
implementation.

\begin{lemma}[Correctness of Algorithm~\ref{algo:variation}]
  Given an input~$n > 0$, Algorithm~\ref{algo:variation} produces a uniform
  random variation of size~$n$.
\end{lemma}

\begin{proof}
  The correctness of Algorithm~\ref{algo:poisson} follows from the arguments
  given in~\cite[page~137]{Knuth1997}, which we do not recall here.
  Regarding Algorithm~\ref{algo:variation}, the for loop at
  line~\ref{line:shuffle} implements the Fisher-Yates~\cite{FY1948} algorithm,
  which performs a uniform permutation of the contents of the array
  \emph{independently of its contents}.
  In our use-case, this implies that:
  \begin{itemize}
    \item the number of zeros is left unchanged;
    \item given an initial array with~$p$ zeros as shown at
      line~\ref{line:initarray}, the probability to get a particular variation
      with~$p$ zeros is given by the probability that a uniform permutations
      maps its first~$p$ values to a prescribed subset of size~$p$, that
      is~$\frac{p!}{n!}$.
  \end{itemize}
  This tells us that, the probability that Algorithm~\ref{algo:variation} yields
  a particular variation with~$p$ zeros is
  \begin{equation*}
    \mathbb{P}[\funname{BoundedPoisson}(1,n) = p] \cdot \frac{p!}{n!}
    = \frac{1}{p! \sum_{k=0}^n \frac{1}{k!}} \cdot \frac{p!}{n!}
    = \frac{1}{v_n}.
    \qedhere
  \end{equation*}
\end{proof}

The ``amount of randomness'' that is necessary to simulate a probability
distribution is given by its entropy.
This gives us a lower bound on the complexity (in terms of random bit
consumption) of random generation algorithms.
For uniform random generation, this takes a simple form since the entropy of a
uniform variable that can take~$M$ distinct values is~$\log_2(M)$.
This tells us that we need at least~$\log_2(v_n)$ random bits to generate a
uniform variation of size~$n$.
When~$n$ is large, we have~$\log_2(v_n) = n \log_2(n) - \frac{n}{\ln(2)} +
O(\log_2(n))$.
The uniform variation sampler we give in Algorithm~\ref{algo:variation} is
\emph{asymptotically} optimal in terms of random bit consumption: in
expectation, the number of random bits that it uses is equivalent
to~$\log_2(v_n)$.
\begin{lemma}[Complexity of Algorithm~\ref{algo:variation}]
  In expectation, Algorithm~\ref{algo:variation} consumes~$n \log_2(n) + o(n
  \log_2(n))$ random bits and performs a linear number of arithmetic operations
  and memory accesses.
\end{lemma}

\begin{proof}
  The means of a Poisson variable of parameter~$1$ being~$1$,
  Algorithm~\ref{algo:poisson} succeeds to find a value smaller or equal to~$n$
  in a constant number of tries in average, and each try requires a constant
  number of uniform variables in average.
  Furthermore, in order to perform the comparison~$p > e^{-1}$ at
  line~\ref{line:pois1} in the algorithm, we need to evaluate these uniform
  random variables.
  This can be done lazily, and again, it is sufficient to know a constant number
  of bits of these variables in average to decide whether~$p > e^{-1}$.

  Regarding the shuffling happening at line~\ref{line:shuffle} in
  Algorithm~\ref{algo:variation}, it needs to draw~$(n-1)$ uniform integers,
  respectively smaller or equal to~$1$,~$2$,~$3$, \ldots,~$n-1$.
  At the first order, this incurs a total cost in terms of random bits, of
  \begin{equation*}
    \sum_{k=2}^n \log_2(k) \sim n \log_2(n).
  \end{equation*}

  In total, the cost of Algorithm~\ref{algo:variation} is thus dominated by the
  shuffling, which allows to conclude on its random bits complexity.

  Regarding the number of arithmetic operations and memory accesses, generating
  Poisson variables performs in constant time using similar arguments.
  The shuffling part of the algorithm is clearly linear.
\end{proof}

Note that we count \emph{integer operations} in the above Lemma, thus
abstracting away the cost of these operations.
At the bit level an extra~$\log_2(n)$ term would appear to take into account the
size of these integers.
This type of considerations is especially important when working with big
integers as it was the case in Section~\ref{sec:def}.
However here, arithmetic operations on integers, rather than bits, seems to be
the right level of granularity as a real-life implementation is unlikely to
overflow a machine integer.

\subsection{A fast rejection procedure}\label{sec:fastrej}

Equipped with the variation sampler described above, we can now generate
variation matrices in an asymptotically optimal way, by filling them with
variations of sizes~$(n-1), (n-2), \ldots, 3, 2, 1$.
By checking afterwards whether the matrix corresponds to a valid DOAGs, and
trying again if not, we get a uniform sampler of DOAGs that is only sub-optimal
by a factor of the order of~$\sqrt{n}$.
This is presented in Algorithm~\ref{algo:rej:naive}.
This algorithm is already more efficient than a sampler based on the recursive
method, whilst naive.

\begin{algorithm}
  \caption{A simple but sub-optimal uniform random sampler of DOAGs}%
  \label{algo:rej:naive}
  \begin{algorithmic}
    \Require{An integer $n > 0$}
    \Ensure{A uniform DOAG with~$n$ vertices as its labelled transition matrix}
    \Function{UnifDOAGNaive}{$n$}
      \assign{$A = {(a_{i, j})}_{1 \leq i, j \leq n}$}{a zero-filled~$n \times
      n$ matrix}
      \Repeat{}
        \For{$i$ \textbf{from} $1$ \textbf{to} $n - 1$}
          \assign{${(a_{i, j})}_{i < j \leq n}$}{\Call{UnifVariation}{$n - i$}}
        \EndFor{}
      \Until{$A$ encodes a DOAG}\label{line:validity}
      \return{The DOAG corresponding to~$A$}
    \EndFunction{}
  \end{algorithmic}
\end{algorithm}

Checking the validity of a matrix at line~\ref{line:validity} corresponds to
checking the conditions given in Theorem~\ref{thm:matrix} at
page~\pageref{thm:matrix}.
We do not provide an algorithm for this here, as the goal of this section is to
iterate upon Algorithm~\ref{algo:rej:naive} to provided a faster algorithm and
get rid of the~$\sqrt{n}$ factor in its cost.
We will see in the following that checking these conditions can be done in
linear time.

As we can see in Theorem~\ref{thm:matrix}, the conditions that a variation
matrix must satisfy to be a labelled transition matrix, concern the shape of
boundary between the zero-filled region between the diagonal and the first
positive values above the diagonal.
Moreover, we have seen in Theorem~\ref{thm:edges} that uniform DOAGs tend to
have close to~$\binom{n}{2}$ edges and thus only a linear number of zeros above
the diagonal of their labelled transition matrix.
We can thus expect that the area that we have to examine to have access to this
boundary should be small.
This heuristic argument, hints at a more sparing algorithm that would start by
filling the matrix near the diagonal and check its validity early, before
generating the content of the whole matrix.
This idea, of performing rejection as soon as possible in the generation
process, is usually referred to as ``anticipated rejection'' and also appears
in~\cite{DFLS2004} and~\cite{BPS1994} for instance.

To put this idea in practice, we need to implement lazy variation generation, to
be able to make progress in the generation of each line independently, and to
perform the checks of Theorem~\ref{thm:matrix} while requiring as little
information as necessary.

\paragraph{Ingredient one: lazy variations}

Fortunately, Algorithm~\ref{algo:variation} can be easily adapted for this
purpose thanks to the fact that the for loop that implements the shuffle
progresses from left to right in the array.
So a first ingredient of our optimised sampler is the following setup for lazy
generation:
\begin{itemize}
  \item for each row of the matrix (i.e.\ each variation to be sampled), we draw
    a Poisson variable~$p_i$ of parameter~$1$ and bounded by~$(n-i)$;
  \item drawing the number at position~$(i, j)$, once we have drawn all the
    numbers of lower coordinate in the same row, can be done by selecting
    uniformly at random a cell with higher or equal coordinate on the same row
    and swapping their contents.
\end{itemize}
This is illustrated in Figure~\ref{fig:ingredient:1}.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{images-ingredient-1}
  \caption{Ingredient one of the fast rejection-based algorithm: variations can
  be lazily generated.
  In the example, the three first elements of the variation at row~$i$ are
  known.
  When we need to generate its fourth element, we perform a swap of~$a_{i,i+4}$
  with a uniform cell of index~$j \geq i + 4$.}%
  \label{fig:ingredient:1}
\end{figure}

\paragraph{Ingredient two: only one initialisation}

A straightforward adaptation of Algorithm~\ref{algo:variation} unfortunately
requires to re-initialise the rows after having drawn the Poisson variable (see
line~\ref{line:initarray} of Algorithm~\ref{algo:variation}) at each iteration
of the rejection algorithm.
This is costly since about~$n^2 / 2$ numbers have to be reset.
It is actually possible to avoid this by initialising all the rows only once and
without any zeros.
Only at the end of the algorithm, once a full matrix have been generated, one
can re-interpret the~$p_i$ largest numbers of row~$i$, for all~$i$, to be zeros.
This is pictured in Figure~\ref{fig:ingredient:2}.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{images-ingredient-2}
  \caption{Ingredient two of the fast rejection-based algorithm: the zeros of
  the matrix need not be explicitly written.
  Instead, we interpret the numbers strictly larger than~$(n-i-p_i)$ as zeros.
  In this example~$(n-i) = 10$ and~$p_i = 3$ so the numbers~$8$,~$9$, and~$10$
  are seen as zeros.}%
  \label{fig:ingredient:2}
\end{figure}

\paragraph{Ingredient three: column by column checking}

The last detail that we need to explain is how to check the conditions of
Theorem~\ref{thm:matrix}.
As a reminder
\begin{itemize}
  \item for each~$2 \leq j \leq n$ we need to compute the number~$b_j =
    \max\,\{i \ | \ a_{i, j} > 0\}$ (or~$0$ is this set is empty);
  \item we must check whether this sequence is weakly increasing;
  \item and whenever~$b_{j+1} = b_j$, we must check that~$a_{b_j,j} <
    a_{b_j,j+1}$.
\end{itemize}
A way of implementing this is to start filling each column of the matrix from
bottom to top, starting from the column~$j=1$ and ending at column~$j=n$.
For each column, we stop as soon as either a non-zero number is found or the
constraints from Theorem~\ref{thm:matrix} are violated.
In order to check these constraints, while filling column~$j$ from
bottom~$(i=j-1)$ to top, we halt as soon as either the cell on the left of the
current cell, or the current cell is non-zero.
The case when the left cell is non-zero corresponds to when~$i=b_{j-1}$ and the
conditions of Theorem~\ref{thm:matrix} can be checked.
Recall that, per the previous point, the zero test in row~$i$ is actually~$x
\mapsto x > n - i - p_i$.
We shall prove that this process uncovers only a linear number of cells of the
matrix, thus allowing to reject invalid matrices in linear expected time.
This idea is pictured in Figure~\ref{fig:ingredient:3}.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{images-ingredient-3}
  \caption{Ingredient three of the fast rejection-based algorithm: the
  exploration process of the cell of the matrix follows a strict discipline.
  We proceed column by column, from bottom to top, and we change columns as soon
  as we see a non-zero cell on our left.
  In the pictures, the bullet~$\bullet$ represent the current cell, the grey
  area represents the cells that have not yet been drawn and the thick red lines
  underline the lowest non-zero cell of each column, as before.
  Depending on the value that is drawn in the current cell, we either move up or
  to the next column.
  Whenever a non-zero cell is on our left, we can decide whether to reject or to
  keep generating.}%
  \label{fig:ingredient:3}
\end{figure}

\paragraph{The algorithm}

Putting all of this together yields Algorithm~\ref{algo:rej:opt} to generate
a uniform DOAG labelled transition matrix using anticipated rejection.
The algorithm is split to two parts.
First, the~\textbf{repeat}-\textbf{until} loop between lines~\ref{line:repeat}
and~\ref{line:until} implements the anticipated rejection phase.
At each iteration of this loop, we ``forget'' what has been done in the previous
iterations, so that~$A$ is considered to be an arbitrary matrix satisfying the
following two conditions
\begin{align}
  &i \ge j \implies a_{i,j} = 0 \\
  &\forall~1 \le i < n, \quad \{a_{i,j} \,|\, i < j \le n\} = \llbracket 1; n -
  i\rrbracket.
\end{align}
The contents of the~${(p)}_{1 \le i < n}$ vector is also forgotten and each
value is to be drawn again before any access.
The~\textbf{while} loop at line~\ref{line:while} implements the traversal of the
matrix described above: at each step, the value of the~$a_{i,j}$ is drawn and
the conditions of Theorem~\ref{thm:matrix} are checked before proceeding to the
next step.
The array~${(s_i)}_{1 \leq i \leq n}$ stores the state of each lazy variation
generator:~$s_i$ contains the value of the largest~$j$ such that~$a_{i,j}$ has
been drawn.
The second part of the algorithm, starting from line~\ref{line:for}, completes
the generation of the matrix once its near-diagonal part is known and we know no
rejection is possible any more.
This includes replacing some values of the matrix by~$0$ per ingredient two
above.
\begin{algorithm}[htb]
  \caption{An optimised uniform random sampler of DOAGs based on anticipated
  rejection}%
  \label{algo:rej:opt}
  \begin{algorithmic}[1]
    \Require{An integer $n > 0$}
    \Ensure{A uniform DOAG with~$n$ vertices, encoded as its labelled transition
    matrix.}
    \Function{UnifDOAGFast}{$n$}
      \assign{$A = {(a_{i,j})}_{1 \leq i, j \leq n}$}%
             {the strictly upper triangular matrix~${(\indicator{j > i} \cdot (j
             - i))}_{1 \leq i, j \leq n}$}
      \assign{${(p_i)}_{1 \leq i < n}$}{uninitialised array}
      \assign{${(s_i)}_{1 \leq i < n}$}{uninitialised array}
      \Repeat{}\Comment{Anticipated rejection phase}\label{line:repeat}
        \assign{$(i, j)$}{$(1, 2)$}\Comment{position of the current cell}
        \assign{$p_1$}{\Call{BoundedPoisson}{$n-1$}}
        \While{$j \le n$}\label{line:while}
          \assign{$r$}{\Call{Unif}{$\llbracket j; n\rrbracket$}}
          \State{$a_{i,r} \leftrightarrow a_{i,j}$}
          \assign{$s_i$}{$j$}
          \If{$(a_{i,j-1} \le n - i - p_i) \wedge (a_{i,j} \not\in \llbracket
          a_{i,j-1} + 1; n - i - p_i \rrbracket)$}
            \State\textbf{break}\Comment{Rejection}
          \ElsIf{$a_{i,j} \le n - i - p_i$}
            \assign{$j$}{$j + 1$}
            \assign{$i$}{$j - 1$}
            \assign{$p_i$}{\Call{BoundedPoisson}{$1, n - i$}}
          \Else{}
            \assign{$i$}{$i - 1$}
          \EndIf{}
        \EndWhile{}
      \Until{$j > n$}\label{line:until}
      \For{$i = 1$ \textbf{to} $n-2$}\label{line:for}%
      \Comment{Completion of the matrix}
        \For{$j = i + 1$ \textbf{to} $s_i$}\label{line:for1}
          \If{$a_{i,j} > n - i - p_i$} $a_{i,j} \gets 0$\EndIf{}
        \EndFor{}
        \For{$j = s_i + 1$ \textbf{to} $n$}\label{line:for2}
          \assign{$r$}{\Call{Unif}{$\llbracket j; n\rrbracket$}}
          \State{$a_{i,r} \leftrightarrow a_{i,j}$}
          \If{$a_{i,j} > n - i - p_i$} $a_{i,j} \gets 0$\EndIf{}
        \EndFor{}
      \EndFor{}
      \return{$A$}
    \EndFunction{}
  \end{algorithmic}
\end{algorithm}

\begin{lemma}[Correction of Algorithm~\ref{algo:rej:opt}]
  Algorithm~\ref{algo:rej:opt} terminates with probability~$1$ and returns a
  uniform random DOAG labelled transition matrix.
\end{lemma}

This result is a consequence of Algorithm~\ref{algo:rej:naive} and
Algorithm~\ref{algo:rej:opt} implementing the exact same operations, only in a
different order and with an earlier rejection in the latter algorithm.
The key characteristic of this new algorithm is that is only needs to perform a
linear number of swaps in average to decide whether the reject the matrix or
not.
As a consequence it is asymptotically optimal in terms of random bits
consumption and it only performs about~$n^2 / 2$ swaps to fill the~$n \times n$
upper triangular matrix.

\begin{theorem}[Complexity of Algorithm~\ref{algo:rej:opt}]
  In average, Algorithm~\ref{algo:rej:opt} consumes~$\frac{n^2}{2} \log_2(n) +
  O(n^{3/2} \log_2(n))$ random bits and performs~$\frac{n^2}{2} + O(n^{3/2})$
  swaps in the matrix.
\end{theorem}

\begin{proof}
  In the rejection phase, in each column, we draw a certain number of zeros and
  at most one non-zero value before deciding whether to reject the matrix or to
  proceed to the next column.
  As a consequence, when lazily generating a variation matrix, we see at
  most~$(n-1)$ non-zero values and a certain number of zeros that we can
  trivially upper-bound by the total number of zeros (strictly above the
  diagonal) in the matrix.

  The number of variations of size~$n$ with exactly~$p$ zeros (with~$0 \le p \le
  n$) is given by~$\frac{n!}{p!}$ by Lemma~\ref{lem:var}.
  As a consequence, the expected number of zeros of a variation is given by
  \begin{equation*}
    \sum_{p=0}^n p \cdot \frac{n!}{p!} \cdot \frac{1}{v_n}
    = \left(e^{-1} + \littleO{\frac{1}{n!}}\right) \sum_{p=0}^{n-1} \frac{1}{p!}
    = 1 + \bigO{\frac{1}{n!}}.
  \end{equation*}
  It follows that the expectation of the total number of zeros of variation
  matrix of size~$n$ is~$n + O(1)$.
  This proves the key fact that, in expectation, we only discover a linear
  number of cells of the matrix in the repeat-until loop.
  Since, in expectation, we only perform~$O(\sqrt{n})$ iteration of this loop,
  it follows that we only perform~$O(n^{3 / 2})$ swaps there.
  Moreover, one swap costs~$O(\log_2(n))$ random bits, which thus accounts for a
  total of~$n^{3 / 2} \log_2(n)$ random bits in this loop.

  In order to complete the proof, it remains to show that the for loops at the
  end of Algorithm~\ref{algo:rej:opt} contribute to the leading terms of the
  estimates given in the Theorem.
  The first inner for loop at line~\ref{line:for1} replaces, among the already
  discovered values, the zeros encoded by numbers above the~$n - i - p_i$
  threshold by actual zeros.
  It is worth mentioning that this only accounts for linear number of iterations
  in total, spanned over several iteration of the outer loop (at
  line~\ref{line:for}).
  The second inner for loop at line~\ref{line:for2} completes the generation of
  the matrix.
  The total number of swaps that it performs (and thus the number of uniform
  variables it draws) is~$\frac{n(n-1)}{2}$ minus the number of already
  discovered cells, that is~$n^2 / 2 + O(n)$.
  This allows to conclude the proof.
\end{proof}

Using equation~\eqref{eq:Dnequiv2}, just bellow Theorem~\ref{thm:Dnequiv}, we
have that~$\log_2(D_n) \sim \frac{n^2}{2} \log_2(n)$.
This shows that Algorithm~\ref{algo:rej:opt} is asymptotically optimal in terms
of random bit consumption.
Moreover, filling a~$n \times n$ matrix requires a quadratic number of memory
writes and the actual number of memory access made by our algorithm is of this
order too.
