\section{Discussion}
\label{sec:discussion}
In the following, we discuss our main findings (\S\ref{main-findings}) and our work's implications and recommendations for achieving ```Fairness by design'' in UbiComp work (\S\ref{implications}).
\subsection{Main Findings\label{main-findings}}
By screening 523 papers published at IMWUT between 2018 and 2022, we found that only a small portion of 5\% adhered to fairness reporting, while the overwhelming majority thereof focused on accuracy or error metrics. By delving into the smaller number of 49 papers, we surfaced biases in machine learning data and models across several sensitive attributes and application domains that would otherwise remain scattered in the UbiComp literature. Yet, the identified lack of diverse datasets in IMWUT publications could result in biases remaining undetected in the absence of heterogeneous demographics. To quantify such biases, included papers primarily employed performance evaluation instead of fairness metrics, while challenges in fairness assessment were found in regression and multi-class classification scenarios. Similar to other communities, defining fairness in UbiComp was not a simple task and involved considering its sociotechnical context, its ethical risks, and opportunities. Nevertheless, in an effort to employ fairness in practice ---sensitive attributes aside--- we found that the community has been striving for generalizability through ablation studies, real-world deployments, and personalization.


\subsection{Implications and Recommendations\label{implications}}
Drawing from these findings and borrowing from the ``Privacy by design'' literature \cite{fjeld2020principled,cavoukian2009privacy}, we propose a ``Fairness by design'' equivalent, requiring AI developers and researchers to consider data and model fairness concerns from the very beginning of any AI project or system design. It is, thus, a proactive and preventative approach that prioritizes fairness as a core value in the development and implementation of UbiComp technologies, products, and services. To facilitate the community achieving ``Fairness by design'', we next discuss recommendations for integrating fairness into the entire ML pipeline of UbiComp studies. These recommendations span two fronts, one concerning the data and the other the model. \\


\begin{figure}[tb!]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/recommendations.pdf}
  \caption{\textbf{Recommendations for ``Fairness by design'' in UbiComp.} Actions to be taken by researchers for performing fairness assessments in both data and models in UbiComp works. Fairness needs to be considered from the very start of a project.\label{fig:recommendations}} 
\end{figure}


    






\noindent \textbf{Data Collection.}  
Prior to the problem definition, researchers should identify the types of fairness-related harms relevant to their work (e.g., quality-of-service, allocation, stereotyping, and erasure harms \cite{crawford2017trouble}). For example, in an AFib detection application, quality-of-service harms could occur if the model had a substantially different performance for different ages, while allocation harms could occur if such difference led to one group unfairly receiving better care than another. Additionally, it is important to consider the demographic groups ---including historically marginalized groups (e.g., based on gender, race, and ethnicity)--- that might be harmed. We should also consider groups that are relevant to a particular scenario or deployment setting. For example, in a depression screening application, gender could be relevant as a sensitive attribute due to reported gender differences in the disorder's signals \cite{parker2010gender}. Relevant attributes can be identified either from the theoretical literature or through fairness literature related to the target application domain.

When defining the problem statement, researchers should also have in mind the generalizability of the prediction task (e.g., applicable across demographic groups). To achieve that, it is of prime importance to consider an adequate enough sample size that would enable fairness to be studied (e.g., through sub-group analyses). For example, an AFib detection system showed poor performance in people with abnormal heart rhythms other than AFib, most likely because its data annotation scheme assigned Normal sinus rhythm (NSR) and other types of heart rhythms to the same Non-AFib category (i.e., binary classification), due to the limited number of subjects with different types of heart rhythms \cite{10.1145/3397313}. Additionally, datasets in UbiComp are either self-collected or well-established benchmarks (e.g., those found in the UCI repository\footnote{UCI Machine Learning Repository: \url{https://archive.ics.uci.edu/ml/index.php}}) used to evaluate new models. For self-collected data, researchers should strive for a diverse representation of human participants in both the recruitment and the data annotation phase. Considering that the models encode the biases of the labels, they should not only be assessed by multiple people to ensure agreement but also strive for demographic diversity amongst them. For benchmark data, researchers should think carefully about the pre-processing stage. Unlike other fields where the datasets are provided out-of-the-box, in UbiComp, it is not uncommon to require further slicing or windowing in order to be used for predictive modeling. For example, applying a sliding window method can generate thousands of samples from a sensor signal that belongs to a single user. This carries the risk of providing virtually ``enough'' samples for training, which, however, come from a handful of users. As a result, the model does not learn generalizable patterns. 

As with any data science project, data validation methods  play an important role in ensuring the results' robustness. The same holds true when it comes to fairness. Typical data validation methods, therefore, should also be applied across sensitive attributes. For example, inspecting outliers that can consistently fall into particular demographic groups, data that are not missing at random and affect certain groups, or other kinds of data anomalies (e.g., measurement error due to a device malfunction or device differences). Regarding the latter, devices such as smartwatches offer model-based estimates for many well-being features. For instance, the measurement error of a heart-rate prediction model can propagate to every downstream application. If the original device has not been validated across different groups, this can affect every possible application that is using such data. More broadly, visualization tools (e.g., What-If Tool\footnote{\url{https://pair-code.github.io/what-if-tool/}}, FairLens\footnote{\url{https://www.synthesized.io/fairlens}}, Tensorflow's Fairness Indicators\footnote{\url{https://github.com/tensorflow/fairness-indicators}}) may help surface any potential data anomalies and help correct them before they creep into models.

At the same time, the community itself could implement a mandatory data statement policy, requiring authors to report sensitive attributes concerning their participant samples. This builds on recent quests that advocate for data excellence~\cite{sambasivan2021everyone}, for example, by making data statements and datasheets for datasets mandatory for authors submitting their work.\\

\noindent \textbf{Model Training and Evaluation.} Until recently, most ML-based UbiComp applications employed some sort of feature engineering in order to extract statistical summaries from sensor data. However, during the past couple of years, this step has been automated since we have witnessed a remarkable consolidation of deep-learning models and architectures such as Convolutional Neural Networks \cite{krizhevsky2017imagenet} and Transformers  \cite{vaswani2017attention}. As a result, such models are being used as generic feature extractors for different data types -- be it images, text, time-series, or video. A side effect of this consolidation is that recently proposed mitigation methods can be applied across a wide range of models, regardless of input data types. For example, one approach modifies the weights of training samples or changes features and labels based on these attributes \cite{calmon2017optimized}. Another approach learns fair representations that remove correlations between sensitive and non-sensitive attributes \cite{zemel2013learning}, while a third approach involves dividing the training data into subgroups and modifying them to have similar feature distributions across subgroups \cite{feldman2015certifying}. Additionally, some methods operate on the latent space of the models by obfuscating information about protected attributes \cite{zemel2013learning}. Overall, these techniques aim to promote fairness and reduce bias by focusing on various aspects of data and model architectures.

Yet enhancing fairness in machine learning requires a means to quantify it. As UbiComp systems blend into the real world, we realize that single evaluation metrics struggle to reflect the success criteria of ML models. As such, monitoring a multitude of metrics becomes the norm, and this is where we believe that monitoring and reporting fairness metrics across different groups should become standard practice. However, we acknowledge that sometimes it might not be feasible to collect data from representative demographics, especially for smaller pilot studies. In these situations, researchers should aim for a diverse user sample based on assumptions about relevant sensitive attributes. This approach can help uncover potential biases in the data and models, which can then be addressed in later stages of development. Alternatively, researchers can leverage advances in generative models to synthesize data covering multiple sensitive attributes and potential intersections \cite{chaudhari2022fairgen}. 

This is where the concept of intersectional fairness comes in. Intersectional fairness means designing and training algorithms to account for the complex ways that different social identities can intersect and impact a person's experiences and outcomes. UbiComp technologies for diagnosing heart disease and monitoring vital signs provide an exemplary case. As reports suggest, differences in coronary heart disease are based on gender \cite{maas2010gender}, socioeconomic status \cite{schultz2018socioeconomic}, and race \cite{fincher2004racial}. In such cases, it is important to ensure that the models do not perpetuate existing biases and inequalities by failing to account for intersectional differences in health outcomes and access to healthcare---the biases encountered by a Black woman from a low socioeconomic background may not be the same as those experienced by a White woman from a high socioeconomic background.

Beyond traditional notions of fairness, such as directly discriminating based on sensitive attributes, we should also consider indirect notions of fairness. For example, within the paradigm of distributed/federated learning, the resource allocation of participating devices may also reflect the demographic and socio-economic information of owners, which makes the exclusion of such clients unfair in terms of participation.  Cheaper devices cannot support the execution of large models and are either excluded or dropped together with their unique data \cite{horvath2021fjord, cho2022flame}. Last, as models are being deployed in real applications, we should monitor their performance in real time and adjust for data and fairness drift \cite{ghosh2022faircanary} by ensuring that models produce fair predictions independent of changes in input data and demographics. 























