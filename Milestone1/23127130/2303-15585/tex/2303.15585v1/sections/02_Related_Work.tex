\section{Related Work}\label{related-work}
\begin{figure}[tb!]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/teaserCropped.pdf}
  \caption{\textbf{Conceptual differences between ML Fairness and UbiComp datasets.} UbiComp data and models are oftentimes inherently different from those commonly used by the ML fairness community. Figure style inspired by \cite{perez2021wearables}. \label{fig:teaser}}
\end{figure}

Next, we situate our critical review in previous fairness literature covering three broad areas: \emph{a)} far-reaching yet broad surveys; \emph{b)} surveys targeted to specific domains; and \emph{c)} call-for-action works focusing on diverse, representative, and balanced research samples. \\
\smallskip

\noindent\textbf{Broad Fairness Surveys.} Addressing algorithmic bias in ML has been a longstanding issue \cite{caton2020fairness}, despite its recent surge. A number of comprehensive surveys shed light on data and model biases across domains and compared potential mitigation solutions. For example, \citet{caton2020fairness} and \citet{pessach2022review} discussed fairness metrics and categorized mitigation approaches into a widely accepted framework of pre-processing, in-processing, and post-processing methods independently of the application domain. \citet{wan2022processing} focused exclusively on in-processing modeling methods such as adversarial debiasing, disentangled representations, and fairness-aware data augmentation, while  \citet{pessach2022review}'s work provides an overview of emerging research trends, including fair adversarial learning, fair word embeddings, fair recommender systems, and fair visual description. More recently, \citet{mehrabi2021survey} explored data-to-algorithm (e.g., representation bias, measurement bias, aggregation bias, etc.), algorithm-to-user (e.g., popularity bias, user interaction bias, evaluation bias, etc.), and user-to-data (e.g., historical bias, temporal bias, content production bias, etc.) biases, and how these biases are generally encountered in ML practice. Along these lines, \citet{le2022survey} surveyed available datasets for fairness research, including financial, criminological, healthcare, social, and educational datasets. Yet, despite these surveys' considerable contributions, they tend to be of generic nature and rarely discuss data, models, and applications related to an individual community. 
\smallskip

\noindent\textbf{Targeted Fairness Surveys.} Another line of work took a deep dive into well-defined domains (e.g., recommender systems, social networks, healthcare, etc.). A number of works targeted specific ML paradigms, for example, by focusing on fairness for ML for graphs~\cite{choudhary2022survey}, on exploring notions of fairness in clustering~\cite{chhabra2021overview}, and on studying fairness in recommender systems~\cite{li2022fairness}. Another group of works targeted specific unprivileged groups or high-stakes domains. For example, \citet{olteanu2019social} reviewed the literature surrounding social data biases, such as biases in user-generated content, expressed or implicit relations between people, and behavioral traces, while in \cite{sun2019mitigating}, the authors focused specifically on gender bias in Natural Language Processing (NLP). On a different note, \citet{10.1145/3173574.3174156} featured emerging trends for explainable, accountable, and intelligible systems within the CHI community, also discussing notions of fairness. Closer to our work, \citet{mhasawade2021machine} discussed ML fairness in the domain of public and population health, and \citet{xu2022algorithmic} explored algorithmic fairness in computational medicine, which only covers a subset of the broad, interdisciplinary UbiComp research domains.
\smallskip

\noindent\textbf{WEIRD Research.} Last, another strand of fairness work is concerned with what is coined as WEIRD research. WEIRD research refers to a common criticism in the social sciences that much of the research is conducted on a sample of participants that is Western, Educated, Industrialized, Rich, and Democratic (WEIRD). In particular, a comprehensive study conducted by \citet{henrich2010weirdest} in 2010 revealed a significant bias in sample populations. The study found that most research samples come from WEIRD populations, which represent only 12\% of the global population but account for 96\% of research samples. This criticism suggested that using such a narrow and unrepresentative sample of participants can limit the generalizability of the findings to the broader population. Over the past decade, the \textit{CHI} community, which focuses on human-centered design, and the \textit{FAccT} community, which aims to democratize ML and advance the development of responsible artificial intelligence, have become more aware of the potential biases introduced by WEIRD samples. For instance, \citet{linxen2021weird} conducted a meta-study on CHI findings from 2016 to 2020, reporting that 73\% of CHI studies are based on Western populations, representing less than 12\% of the population worldwide, invariably making CHI ``WEIRD'', as it is based on the knowledge and ethics of people who are Western, Educated, Industrialized, Rich, and Democratic. Similarly, a recent meta-study on FAccT proceedings from 2018 to 2021 extracted research topics and identified community values, placing fairness and ML, bias in word embeddings, bias in vision, and racial disparities among the ten largest sub-communities within the conference \cite{laufer2022four}. Yet again, as highlighted in Introduction (Section~\ref{introduction}) in line with prior work \cite{laufer2022four}, ``off-the-shelf'' benchmark datasets are encountered in the majority of published work, while only a $\sim10\%$ of FAccT papers use original, empirical datasets, let alone UbiComp data.

It is evident that research communities other than UbiComp have recently started to explore ways of reporting data and models in a fair manner to surface and, ultimately, address encountered biases. Yet, the state of fairness in the UbiComp community remains unknown, as, at the time of writing, there exists no other survey or position paper in the intersection between UbiComp and fairness. Are our data susceptible to biases? Do our models discriminate against certain demographics, and if so, how do we make them right? These are just a handful of questions we set to provide answers to in this work.

