\section{Background: Fairness Definitions \& Measurement} \label{sec:background}
Fairness is a social construct that defies a simple definition~\cite{narayanan21fairness}. In the legal domain, fairness entails the ``protection of individuals and groups from discrimination or mistreatment with a focus on prohibiting behaviors, biases and basing decisions on certain protected factors or social group categories''~\cite{fairnessberkeley}. Social sciences often consider fairness ``in light of social relationships, power dynamics, institutions, and markets''~\cite{mulligan2019thing}, while quantitative fields (e.g., computer science, statistics) view fairness as a mathematical problem of ``equal or equitable allocation, representation, or error rates, for a particular task or problem''~\cite{narayanan21fairness}. 

Viewed through the lens of quantitative science, ML research has broadly grouped fairness into three categories: \textit{group fairness},  \textit{individual fairness}, and \textit{subgroup fairness} \cite{mehrabi2021survey}. Group fairness ensures some form of statistical parity (e.g., in terms of positive outcomes or errors) for individuals belonging to different protected groups (i.e., groups characterized by a sensitive attribute, such as gender or race) \cite{dwork2012fairness,kusner2017counterfactual}. On the other hand, individual fairness ensures that ``similar'' individuals receive similar outcomes \cite{dwork2012fairness,kusner2017counterfactual}. Subgroup fairness aims to exploit the best of both worlds by ensuring some form of statistical parity but holding this constraint over a large collection of subgroups to prevent fairness gerrymandering \cite{kearns2018preventing,kearns2019empirical}. While proponents of individual fairness have argued that it should be preferred to other categories for determining fairness, individual fairness has also received criticism: (1) counterexamples show that similar treatment is insufficient to guarantee fairness; (2) similarity metrics are susceptible to encoding implicit human biases; (3) similarity definition assumes prior moral judgments; and (4) the incommensurability of relevant moral values makes similarity metrics infeasible for many tasks \cite{fleisher2021s}. Considering these limitations, we refer to both group and subgroup fairness when the term fairness is used.

In quantifying group fairness, there exist two opposing perspectives: ``We're All Equal'' (WAE) and ``What You See Is What You Get'' (WYSIWYG) \cite{friedler2021possibility,10.1145/3442188.3445892}. The WAE perspective assumes equal ability across groups to perform the task, and thus it is closely linked with treating equals equally, whereas the WYSIWYG viewpoint assumes that the data themselves reflect a group's ability with respect to the task, and thus, unequals should not be treated equally. Each perspective is quantified by different fairness metrics \cite{garg2020fairness}. The WAE perspective, for example, uses demographic parity metrics, such as disparate impact and statistical parity difference, while the WYSIWYG perspective uses equality of odds metrics, such as average odds and average absolute odds difference. The two schools of thought find some common ground in equality of opportunity metrics, such as false negative rate, false positive rate, and error rate ratios, among others, where the choice of appropriate fairness metric is often guided by the question ``What is the consequence of the predictive outcome?''. In quantifying individual fairness, ``similar'' individuals should be treated similarly. \citet{dwork2012fairness} formalized this intuition by considering ML models as mappings between input and output metric spaces and defining individual fairness as their Lipschitz continuity. Specifically, a Lipschitz continuity condition requires that for any two individuals $u_1$ and $u_2$, their distance (as defined by a given distance metric) is proportional to the difference between the classifier's output for $u_1$ and the classifier's output for $u_2$. In other words, if x and y are similar according to the distance metric, then their classifier outputs should also be similar, and vice versa. The distance metric on the input space though is crucial to the definition as it encodes individuals' similarity, and the choice was originally deferred to regulatory bodies or civil rights organizations rather than researchers or practitioners \cite{10.5555/3524938.3525596}. Naturally, different works use different definitions of algorithmic fairness, and although these appear internally consistent, they may also be mutually incompatible, as many quantitative fairness metrics cannot be satisfied simultaneously \cite{friedler2021possibility}.

Despite the contradictory nature of fairness \cite{friedler2021possibility}, the common element across traditional, even opposing definitions and metrics is that fairness is defined and assessed with respect to one or more sensitive (a.k.a. protected) attributes, such as gender, race, or age. Specifically, such attributes enable post hoc analyses to evaluate model performance across demographics and, ultimately, model fairness.
This is indeed how fairness has been interpreted within the machine learning community \cite{buolamwini2018gender,bolukbasi2016man,angwin2020there}, but viewed through the lens of the traditional definition, the UbiComp community is significantly lagging behind. 


