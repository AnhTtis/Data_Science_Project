\section{Introduction}\label{introduction}
Tasks once thought impossible or reserved exclusively for humans are now within our grasp thanks to the integration of Machine Learning (ML) in ubiquitous computing (UbiComp). Algorithms deployed on ubiquitous devices were typically used to recognize human activities~\cite{gu2021survey}, facilitate indoor localization~\cite{10.1145/3191775}, detect breathing phases~\cite{10.1145/3369835} and infer sleep quality~\cite{koskimaki2018we}. Today, we are also witnessing an increasing trend toward high-stakes applications. For example, detecting Atrial Fibrillation (AFib)~\cite{lubitz2022detection}, diagnosing COVID-19 infection~\cite{10.1145/3394486.3412865}, predicting fertility windows~\cite{maijala2019nocturnal}, and even improving cognitive performance~\cite{costa2019boostmeup}. Independence of healthcare access, individualized health-promoting interventions, and easier dissemination of medical information, to name a few, make up the list of benefits that algorithmic decision-making (i.e., the use of algorithms and mathematical models to automate the process of decision-making in an efficient and objective manner) has enabled~\cite{mariakakis2019challenges}. However, the data and algorithms powering these advancements are not immune to biases. With great ethical opportunities come ethical risks, and, similarly to humans, ML algorithms are susceptible to biases rendering their decisions ``unfair''~\cite{o2017weapons,buolamwini2018gender,bolukbasi2016man,angwin2020there}. 

The research community of Fairness, Accountability and Transparency in ML (FAccT, formerly FAT/ML) defines fairness as a principle that \emph{``ensures that algorithmic decisions do not create discriminatory or unjust impacts when comparing across different demographics (e.g., race, sex)''}~\cite{awwad2020exploring}. Real-world cases of ``unfair'' ML algorithms abound. 
For example, \citet{kamulegeya2019using} found that neural network algorithms trained to perform skin lesion classification showed approximately half the original diagnostic accuracy on black patients compared to white patients. At the same time, people of color are consistently misclassified by health sensors such as oximeters as they were scientifically tested on predominantly white populations~\cite{sjoding2020racial}. 

As we shall see throughout this critical review, fairness in UbiComp remains relatively unexplored due to the primary focus on accuracy, and particularities of the community. But what makes UbiComp unique compared to other communities? UbiComp data and models have certain particularities, oftentimes not shared with the broader scholarly discourse on ML and AI Ethics (Figure~\ref{fig:teaser}). For example, UbiComp typically deals with small-scale studies, proof-of-concept datasets often collected by the authors in-the-lab or in-the-wild, while the broader ML community frequently utilizes popular, medium- to large-scale benchmark datasets such as the UCI Adult, the German Credit, the COMPAS, and the Diversity in faces datasets~\cite{kohavi1996uci,dua2017uci,compas,merler2019diversity}. Such data are collected once and are immutable, opposite to UbiComp data that are mutable and, by definition, continuously collected. Contrary to the tabular format of such datasets, UbiComp data are mostly sequential in nature, with biases being harder to surface. In other words, while it is relatively straightforward to distinguish a person's skin tone from a picture, it is much harder to do so from oximetry measurements, necessitating the collection of supplementary metadata, such as demographics; as UbiComp strives to blend technologies in the background, biases are blended, too. However, with a conscious approach, it is possible to create ML models that are both accurate and fair. As the field of ML continues to evolve, the UbiComp community needs to stay vigilant, ensuring that UbiComp technologies are designed and deployed in a responsible and ethical manner.


Building on the footsteps of Human-Computer Interaction (HCI) and FAccT communities, we set out to understand how fairness has been discussed in UbiComp and identify pathways for ensuring that UbiComp technologies do not cause any harm or infringe on any individual rights~\cite{constantinides2022good}.
Such research communities recently started to explore ways of reporting  fairness in data and models to surface and, eventually, counter biases.
In this work, we intend to spark a discussion on how the UbiComp community defines, measures, and assesses fairness. While the community has perhaps indirectly adopted (and adapted) the meaning of fairness to capture UbiComp's particularities, the question remains though: \textbf{\emph{Where does UbiComp fairness overlap with other communities, and, more importantly, where does it lag behind?}}

To answer this question, we performed a literature review spanning five years (2018-2022) and 523 papers published in UbiComp literature. We targeted papers published in the Proceedings of the ACM Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), a high-quality journal series capturing the emerging trends in the UbiComp community and bearing an h-index of 58, placing it among the top-3 publications in HCI.\footnote{\url{https://scholar.google.co.uk/citations?view_op=top_venues&hl=en&vq=eng_humancomputerinteraction}} In so doing, we made three contributions:

\begin{itemize}
    \item We conducted the first review of fairness in ML for UbiComp, where we screened 523 and critically reviewed \textbf{49} IMWUT papers ($N_{included}=49$) published at IMWUT between 2018 and 2022 (\S\ref{sec:methodology}).
    \item We found that: \emph{a)} Only \textbf{5\%} of all IMWUT papers reported fairness assessments (included papers); \emph{b)} from this proportion of papers, 24\% implement fairness enhancement mechanisms, making up only a limited fraction of all IMWUT papers (1\%); \emph{c)} Yet, we surfaced biases across several sensitive attributes, otherwise scattered in UbiComp literature; \emph{d)} Included papers predominantly used performance evaluation metrics, rather than fairness ones, in their fairness discourse. Yet, we confirmed assessment gaps in regression and multi-class classification cases; \emph{e)} Similar to other communities, defining fairness in UbiComp was far from straightforward. Ethical risks and opportunities laid at the heart of this decision; \emph{f)} Fairness in UbiComp was often viewed through the lens of generalizability, taking the form of ablation studies, in-the-wild deployments, and personalization; \emph{g)} Measurement inaccuracies and concept drift in audio, video, image, and sensor data led to performance differences across demographics; and \emph{h)} IMWUT papers suffered from a lack of diverse datasets, concealing biases in the absence of heterogeneous demographics (\S\ref{sec:results}).
    \item In light of these findings, we made ten recommendations to the UbiComp community pertaining to the integration of fairness into the entire ML pipeline of UbiComp studies (\S\ref{sec:discussion}).
\end{itemize}

The remainder of the paper is organized as follows. Section~\ref{related-work} examines review and position papers related to machine learning fairness and responsible artificial intelligence. Section~\ref{sec:background} outlines fairness definitions and metrics. Section~\ref{sec:methodology} describes the methodology used to conduct the literature review, and Section~\ref{sec:results} presents the results obtained. Lastly, Section~\ref{sec:discussion} discusses the findings and limitations of the review, and offers recommendations for fair reporting in the UbiComp community.







