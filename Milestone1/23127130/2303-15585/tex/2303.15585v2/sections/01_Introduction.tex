\section{Introduction}\label{introduction}

Tasks once thought impossible or reserved exclusively for humans are now within our grasp thanks to the integration of Machine Learning (ML) in \rev{mobile and wearable computing (referred to as UbiComp from here onward)}. Algorithms deployed on such ubiquitous devices \rev{are} typically used to recognize human activities~\cite{gu2021survey}, \rev{facilitate indoor localization~\cite{mittal2018adapting}}, detect breathing phases~\cite{10.1145/3369835} and infer sleep quality~\cite{koskimaki2018we}. Today, we are also witnessing an increasing trend toward high-stakes applications. For example, detecting Atrial Fibrillation (AFib)~\cite{lubitz2022detection}, diagnosing COVID-19 infection~\cite{10.1145/3394486.3412865}, predicting fertility windows~\cite{maijala2019nocturnal}, and even improving cognitive performance~\cite{costa2019boostmeup}. Independence of healthcare access, individualized health-promoting interventions, and easier dissemination of medical information, to name a few, make up the list of benefits that algorithmic decision-making has enabled~\cite{mariakakis2019challenges}. However, the data powering these advancements are not immune to biases, rendering algorithmic decisions``unfair''~\cite{o2017weapons,buolamwini2018gender,bolukbasi2016man,angwin2020there}. 

Fairness is defined as a principle that \emph{``ensures that algorithmic decisions do not create discriminatory or unjust impacts when comparing across different demographics''}~\cite{awwad2020exploring}. Real-world cases of ``unfair'' ML algorithms abound. 
For example, \citet{kamulegeya2019using} found that neural network algorithms trained to perform skin lesion classification showed approximately half the original diagnostic accuracy on black patients. At the same time, people of color are consistently misclassified by health sensors such as oximeters as they were scientifically tested on predominantly white populations~\cite{sjoding2020racial}. 

\begin{figure}[tb!]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/teaserCroppedBlack.pdf}
  \caption{\textbf{Conceptual differences between ML Fairness and UbiComp datasets.} UbiComp data and models are oftentimes inherently different from those commonly used by the ML fairness community. Figure style inspired by \cite{perez2021digital}. \label{fig:teaser}}
\end{figure}
As we shall see throughout this review, fairness in UbiComp remains relatively unexplored. But what makes UbiComp unique compared to other communities? UbiComp data and models have certain particularities, oftentimes not shared with the broader scholarly discourse on ML and AI Ethics (Figure~\ref{fig:teaser}). For example, UbiComp typically deals with small-scale and proof-of-concept datasets \rev{with few subjects} often collected by the authors in-the-lab or in-the-wild \rev{during feasibility studies}, while the broader ML community frequently utilizes popular, medium- to large-scale benchmark datasets such as UCI Adult, German Credit, and COMPAS~\cite{kohavi1996uci,dua2017uci,compas}. Such data are collected once and are immutable, opposite to UbiComp data that are mutable and, by definition, continuously \rev{or longitudinally} collected. Contrary to the tabular format of such datasets, UbiComp data are mostly \rev{sequential/temporal} in nature, \rev{e.g., sensor data}, with biases being harder to surface. In other words, while it is relatively straightforward to distinguish a person's skin tone from a picture, it is much harder to do so from oximetry measurements, necessitating the collection of supplementary metadata, such as demographics; as UbiComp strives to blend technologies in the background \rev{\cite{denning1997coming,case2015calm}}, biases are blended, too. However, it is possible to create ML models that are both accurate and fair. As \rev{UbiComp research evolves, the community} needs to stay vigilant, ensuring that UbiComp technologies are designed and deployed in a responsible manner.


\rev{In this work,} we set out to understand how fairness has been discussed in UbiComp and identify pathways for ensuring that UbiComp technologies do not cause any harm or infringe on any \rev{individual rights~\cite{constantinides2022good,huq2019constitutional}}. \rev{Note that, while we use the UbiComp abbreviation for readability, we focus on the mobile and wearable technologies in the context of fairness. We chose this angle because of the human-centered nature of mobile and wearable computing, and the subsequent implications of bias in such applications.}
\rev{Other} research communities, \rev{such as Human-Computer-Interaction (HCI) and Fairness, Accountability, and Transparency (FAccT),} recently started to explore ways of reporting  fairness in data and models to surface and, eventually, counter biases \rev{\cite{van2023methodology,orphanou2022mitigating}}.
In this work, we intend to spark a discussion on how the UbiComp community defines, measures, and assesses fairness. While the community has perhaps indirectly adopted (and adapted) the meaning of fairness to capture UbiComp's particularities, the question remains though: \rev{\textbf{\emph{Has the UbiComp community adopted ways of reporting algorithmic fairness in machine learning, and, if so, in what ways and to which extent?}}}

To answer this question, we performed a literature review spanning five years (2018-2022) and 523 papers published in UbiComp literature. We targeted papers published in the Proceedings of the ACM Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), a high-quality journal series capturing emerging trends in the UbiComp community and bearing an h-index of 58, placing it among the top-3 publications in HCI.\footnote{\url{https://scholar.google.co.uk/citations?view_op=top_venues&hl=en&vq=eng_humancomputerinteraction}} In so doing, we made three contributions:

\begin{itemize}
    \item We conducted the first review of fairness in ML for UbiComp, where we screened 523 and critically reviewed \textbf{49} IMWUT papers ($N_{included}=49$) published at IMWUT between 2018 and 2022 (\S\ref{sec:methodology}). \rev{To generalize our findings across venues, we analyzed recent proceedings of venues of similar scope and found no deviation from our primary result (\S\ref{generalizability}).}
    \item We found that: \emph{a)} Only \textbf{5\%} of all IMWUT papers \rev{published between 2018-2022} reported fairness assessments (\rev{from here on referred to as} included papers); \emph{b)} from this proportion of papers, 24\% implement fairness enhancement mechanisms, making up only a limited fraction of all IMWUT papers (1\%); \emph{c)} Yet, we surfaced biases across several sensitive attributes, otherwise scattered in UbiComp literature; \emph{d)} Included papers predominantly used performance evaluation metrics, rather than fairness ones, in their fairness discourse. Yet, we confirmed assessment gaps in regression and multi-class classification cases; \emph{e)} Similar to other communities, defining fairness in UbiComp was far from straightforward, \rev{balancing a trade-off between ethical risks and opportunities;} \emph{f)} Fairness in UbiComp was often viewed through the lens of generalizability, taking the form of ablation studies, in-the-wild deployments, and personalization; \emph{g)} Measurement inaccuracies and concept drift in audio, video, image, and sensor data led to performance differences across demographics; and \emph{h)} IMWUT papers suffered from a lack of diverse datasets, concealing biases in the absence of heterogeneous demographics (\S\ref{sec:results}).
    \item In light of these findings, we made ten recommendations to the UbiComp community pertaining to the integration of fairness into the entire ML pipeline of UbiComp studies (\S\ref{sec:discussion}).
\end{itemize}









