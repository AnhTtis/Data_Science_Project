\section{Results\label{sec:results}}
\rev{In this section,} we explore the state of fairness in the UbiComp community (Section~\ref{mf1}), the ethical risks and opportunities in the domain (\S\ref{mf2}), and we capture alternative notions of fairness that the UbiComp community has perhaps indirectly adopted (\S\ref{mf3}). Finally, we discuss biases relevant to the domain's data modalities (\S\ref{historical-biases}) and explore diversity in UbiComp datasets and research teams (\S\ref{weird}).


\setlength\arrayrulewidth{2pt}\arrayrulecolor{gray}
\begin{table}[]
\caption{A summary of the included papers categorized by application domain, fairness enhancement mechanism, sensitive attribute, and bias metrics. High-stakes health application papers are the most active in the UbiComp community regarding fairness. While all included papers talk about fairness with respect to one or more sensitive attributes, the vast majority \rev{of papers (76\%)} do not offer \rev{fairness} enhancement mechanisms.}
\label{tab:summary}
\resizebox{\textwidth}{!}{%
\begin{tabular}{p{2.2in}p{1.55in}p{1.7in}p{2in}p{1in}}
\rowcolor[HTML]{1C9E77} 
{\color[HTML]{FFFFFF} \textbf{APPLICATION DOMAIN}} &
  {\color[HTML]{FFFFFF} \textbf{FAIRNESS MECHANISM}} &
  {\color[HTML]{FFFFFF} \textbf{SENSITIVE ATTRIBUTE}} &
  {\color[HTML]{FFFFFF} \textbf{FAIRNESS METRICS}} &
  {\color[HTML]{FFFFFF} \textbf{PAPERS}} \\
\rowcolor[HTML]{F3F3F3} 
Health (26.5\%) &
  Pre-processing &
  Gender &
  \cellcolor[HTML]{F3F3F3} &
  \cite{10.1145/3351281} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Age &
  \multirow{-2}{*}{\cellcolor[HTML]{F3F3F3}Accuracy} &
   \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Health Condition &
  MAE &
  \cite{10.1145/3328927} \\ \cline{2-2}
\rowcolor[HTML]{F3F3F3} 
 &
  In-processing &
  Socioeconomic Status &
  Precision, Sensitivity, Specificity &
  \cite{10.1145/3411835} \\ \cline{2-2}
\rowcolor[HTML]{F3F3F3} 
 &
  None &
  Gender &
  Accuracy, AUC-ROC, F1, Sensitivity, Specificity, MAPE, Error rate &
  \cite{10.1145/3478127,10.1145/3534583,10.1145/3478107,10.1145/3534595,10.1145/3463503} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Age &
  Accuract, AUC-ROC, Sensitivity, Specificity, Pearson's r, Error rate &
  \cite{10.1145/3478127,10.1145/3381016,10.1145/3534583,10.1145/3478098,10.1145/3463503} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Health Condition &
  Accuracy, AUC-ROC, AUC-PR, F1, Precision, Sensitivity, Specificity, RMSE &
  \cite{10.1145/3328917,10.1145/3534578,10.1145/3478107,10.1145/3463503} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Physiology &
  F1, Sensitivity, Specificity &
  \cite{10.1145/3478107} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Race &
  AUC-ROC, MAE &
  \cite{10.1145/3534583,10.1145/3517225} \\
\rowcolor[HTML]{FFFFFF} 
Privacy \& Security (12.2\%) &
  None &
  Age &
  Error rate &
  \cite{10.1145/3448113} \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Health Condition &
  Precision, P-value &
  \cite{10.1145/3264899,10.1145/3494960} \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Physiology &
  Accuracy &
  \cite{10.1145/3161198,10.1145/3264944,10.1145/3351273} \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Religion &
   &
  \cite{10.1145/3161198} \\
\rowcolor[HTML]{F3F3F3} 
Human-Activity Recognition (10.2\%) &
  Pre-processing &
  Gender &
  F1 &
  \cite{10.1145/3517252} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Age &
   &
   \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Physiology &
   &
   \\ \cline{2-2}
\rowcolor[HTML]{F3F3F3} 
 &
  In-processing &
  Gender &
  F1 &
  \cite{10.1145/3397330} \\ \cline{2-2}
\rowcolor[HTML]{F3F3F3} 
 &
  \cellcolor[HTML]{F3F3F3} &
  Gender &
  Accuracy &
  \cite{10.1145/3397311} \\
\rowcolor[HTML]{F3F3F3} 
 &
  \cellcolor[HTML]{F3F3F3} &
  Physiology &
  F1 &
  \cite{10.1145/3397313} \\
\rowcolor[HTML]{F3F3F3} 
 &
  \multirow{-3}{*}{\cellcolor[HTML]{F3F3F3}None} &
  Miscellaneous &
  MSE &
  \cite{10.1145/3161201} \\
\rowcolor[HTML]{FFFFFF} 
Behavioral Sensing \& Emotion (10.2\%) &
  Pre-processing &
  Gender &
  Accuracy &
  \cite{10.1145/3351246} \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Age &
  Accuracy, MAE &
  \cite{10.1145/3517249,10.1145/3351246,10.1145/3369820} \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Nationality &
  Accuracy &
  \cite{10.1145/3351246} \\ \cline{2-2}
\rowcolor[HTML]{FFFFFF} 
 &
  None &
  Gender &
  Accuracy, Coefficient of determination &
  \cite{10.1145/3448089,10.1145/3478126} \\
\rowcolor[HTML]{F3F3F3} 
Sound, Voice \& Hearing (10.2\%) &
  Pre-processing &
  Gender &
  Accuracy &
  \cite{10.1145/3494969} \\ \cline{2-2}
\rowcolor[HTML]{F3F3F3} 
 &
  None &
  Gender &
  Error rate, Mel cepstral distortion &
  \cite{10.1145/3161601,10.1145/3550338,10.1145/3550293} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Age &
  Error rate &
  \cite{10.1145/3550293} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Physiology &
  Accuracy, MAE &
  \cite{10.1145/3517253} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Nationality &
  Error rate &
  \cite{10.1145/3161601} \\
\rowcolor[HTML]{FFFFFF} 
Motion, Gaze, Gesture \& Touch (8.2\%) &
  None &
  Gender &
  Accuracy &
  \cite{10.1145/3432235,10.1145/3351255} \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Age &
   &
   \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Health Condition &
  Error rate &
  \cite{10.1145/3494999} \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Physiology &
  Accuracy &
  \cite{10.1145/3191772,10.1145/3432235} \\
\rowcolor[HTML]{F3F3F3} 
Mobility \& Navigation (8.2\%) &
  Pre-processing &
  Gender &
  Accuracy &
  \cite{10.1145/3191774} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Age &
   &
   \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Physiology &
   &
   \\ \cline{2-2}
\rowcolor[HTML]{F3F3F3} 
 &
  In-processing &
  Gender &
  Accuracy, AUC-ROC, F1 &
  \cite{10.1145/3494990} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Age &
   &
   \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Socioeconomic Status &
   &
   \\ \cline{2-2}
\rowcolor[HTML]{F3F3F3} 
 &
  None &
  Gender &
  F1 &
  \cite{10.1145/3314407} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Age &
  F1 &
   \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Miscellaneous &
  Accuracy &
  \cite{10.1145/3411807} \\
\rowcolor[HTML]{FFFFFF} 
Miscellaneous (8.2\%) &
  Pre-processing &
  Miscellaneous &
  Accuracy &
  \cite{10.1145/3534585} \\ \cline{2-2}
\rowcolor[HTML]{FFFFFF} 
 &
  None &
  Gender &
  Required rate of return &
  \cite{10.1145/3432193} \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Age &
  Required rate of return &
   \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Marital Status &
  Required rate of return &
   \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Physiology &
  P-value &
  \cite{10.1145/3550312} \\
\rowcolor[HTML]{FFFFFF} 
 &
   &
  Language &
  Error rate &
  \cite{10.1145/3161187} \\
\rowcolor[HTML]{F3F3F3} 
Cognition \& Attention (6.1\%) &
  None &
  Age &
  RMSE &
  \cite{10.1145/3411811} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Physiology &
  MSE &
  \cite{10.1145/3161164} \\
\rowcolor[HTML]{F3F3F3} 
 &
   &
  Miscellaneous &
  Accuracy, AUC-ROC, Precision, Sensitivity, MAE, MSE, Pearson's r &
  \cite{10.1145/3432235,10.1145/3448111}
\end{tabular}%
}
\end{table}

\subsection{What is the State of Fairness in UbiComp?\label{mf1}}
\rev{Table~\ref{tab:summary}} presents all included papers, categorized per application domain, sensitive attributes, and fairness mechanisms and metrics. In summary, out of the 523 retrieved papers, a small portion of 9\% ($N_{included}=49$) were included in the review, which in turn make up only 5\% of all IMWUT publications between 2018 and 2022, highlighting the timeliness and necessity of this work. \\

\tikzstyle{background rectangle}=[thin,draw=black]
\begin{tikzpicture}[show background rectangle]
\node[align=justify, text width=40em, inner sep=1em]{
Out of all papers published at IMWUT between 2018 and 2022, only a small portion of 5\% (included papers) adhered to fairness reporting. 
};
\node[xshift=3ex, yshift=-0.7ex, overlay, fill=white, draw=white, above 
right] at (current bounding box.north west) {
\textit{Takeaway \#1}
};
\end{tikzpicture} 
\smallskip

To identify appropriate application domains, we consulted the past four years (2019-2022) of UbiComp sessions to identify commonalities in discussed themes. We grouped together tracks' themes between years based on their similarity and relevance to the included papers. This process led us to the identification of ten domains: \textit{Health}; \textit{Human-Activity Recognition} (HAR); \textit{Behavioral Sensing \& Emotion}; \textit{Cognition \& Attention}; \textit{Motion, Gaze, Gesture \& Touch}; \textit{Sound, Voice \& Hearing}; \textit{Mobility \& Navigation}; \textit{Privacy \& Security}; \textit{Localization}, and \textit{Miscellaneous}. We encountered all but one theme (localization) in the included papers, \rev{possibly due to} localization's usually low-stakes applications. Health was the most commonly encountered domain, accounting for more than one in four papers, while Cognition \& Attention was the least common, accounting for only $\sim6\%$ of included papers. 

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \vskip 0pt
        \centering
        \includegraphics[width=.95\linewidth]{figures/cloudAllCropped.pdf}
    \end{subfigure}%
    \hspace{1em}%
    \begin{subfigure}[t]{0.4\textwidth}
        \vskip 0pt
        \centering
        \includegraphics[width=.95\linewidth]{figures/cloudIncludedCropped.pdf}
    \end{subfigure}
    \hspace{1em}%
    \begin{subfigure}[t]{0.15\textwidth}
        \vskip 0pt
        \centering
        \includegraphics[width=.95\linewidth]{figures/legend.pdf}
    \end{subfigure}
    \caption{\textbf{Keyword differences of retrieved (left) and included (right) papers}. Frequent keywords in both retrieved and included papers are colored in dark grey. Over-represented keywords in the retrieved papers are colored in green, while over-represented keywords in the included papers are colored in pink. Even within UbiComp, the privacy, audio, and vision communities are trailblazers in ML fairness. \label{fig:wordclouds}}
\end{figure*}
Figure~\ref{fig:wordclouds} provides an overview of the discussed domains, as captured by the papers' keywords. It also serves as a validation to the domains' categorization, as many categories (e.g., \textit{Health}; \textit{HAR}; \textit{Motion, Gaze, Gesture \& Touch}; \textit{Sound, Voice \& Hearing} and \textit{Privacy \& Security}) also appear in the keyword clouds. Deep learning,  ML, and HAR are among the most frequently overlapping keywords. Over-represented keywords in the retrieved papers \rev{(i.e., keywords belonging to the top-10 keywords of the retrieved papers, but not the included papers)} include mobile sensing, wearables, Internet of Things (IoT), Radio-frequency identification (RFID), and self-supervised learning. Over-represented keywords in the included papers include mobile health, Post-traumatic stress disorder (PTSD), acoustic sensing, computer vision, privacy, and gesture recognition. 

\subsubsection{Fairness Enhancement Mechanisms} 
It is often infeasible to eliminate all sources of unfairness. Yet, the goal is to surface and mitigate biases as much as possible through fairness enhancement mechanisms. 
\smallskip

\noindent \textbf{Pre-processing mechanisms.} Within UbiComp, preliminary mechanisms included fair data representation. For instance, \citet{10.1145/3328927} included both healthy \rev{and non-healthy} subjects in their dataset for respiratory rate monitoring. Similarly, \citet{10.1145/3534585} employed a fairness-aware client selection mechanism for federated learning to ensure equal representation for subjects with worse connectivity.\footnote{While Internet connectivity is not a sensitive attribute per se, it has been linked with socioeconomic status, race, nationality, gender, and age, all of which are sensitive attributes \cite{united2021almost}.} 
\citet{10.1145/3494969} performed data balancing managing to narrow the impact of gender voice differences on their speech recognition model. Similarly, a strand of work explored data splitting, conditioned on the sensitive attribute (gender, age, BMI, skin tone, country, and health condition) to enable model personalization \cite{10.1145/3369820,10.1145/3351246,10.1145/3517249,10.1145/3494969,10.1145/3351281}. More advanced mechanisms suggest modifying feature representations. For example, \citet{10.1145/3191774} improved their activity detection model \rev{performance} by normalizing the window-level features across gender and physiology. Similarly, in line with prior work~\cite{locatello2019fairness}, \citet{10.1145/3517252} utilized disentangled representations, aiming to isolate activity patterns from redundant noises such as gender, age, and physiological differences.
\smallskip

\noindent \textbf{In-processing mechanisms.} \citet{10.1145/3411835} altered their logistic regression model for Post-traumatic stress disorder (PTSD) screening to include sensitive attributes in its parameters. On a similar note, \citet{10.1145/3397330} devised a multi-task loss function consisting of activity, subject, and gender loss. Finally, in quantifying the causal effect of individual mobility on health status, \citet{10.1145/3494990} considered \rev{correlated} sensitive attributes, such as age and socioeconomic status, as confounding variables in their causal model.
However, they noted that due to dataset privacy constraints in reporting demographic variables, potential unobserved confounding variables might have been missed, highlighting the conflict between fairness and privacy \cite{chang2021privacy}.
\smallskip

Despite the notable efforts of the aforementioned pioneering works in the UbiComp community, 3 out of 4 included papers did not report any fairness enhancement mechanism, regardless of the presence of bias in their models. This is partly due to a lack of consideration for fairness-related harms, but it is also connected with the nature of several UbiComp works: artifact contributions, proof of concept, and early-stage technology development, where performance is prioritized. \\




\tikzstyle{background rectangle}=[thin,draw=black]
\begin{tikzpicture}[show background rectangle]
\node[align=justify, text width=40em, inner sep=1em]{
Papers implementing \rev{fairness enhancement mechanisms} constitute a small portion of included papers (24\%), which, in turn, make up only a limited fraction of all IMWUT publications (1\%). 
};
\node[xshift=3ex, yshift=-0.7ex, overlay, fill=white, draw=white, above 
right] at (current bounding box.north west) {
\textit{Takeaway \#2}
};
\end{tikzpicture} 


\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.65\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/deploymentCropped.pdf}
        \caption{\textbf{\rev{Frequency of in-the-lab versus in-the-wild deployments along with fairness assessment results.}} \label{fig:deployment}}
    \end{subfigure}%
    \hspace{1em}%
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/biasCropped.pdf}
        \caption{\textbf{Sensitive attributes \rev{and} reported fairness results.}\label{fig:bias}}
    \end{subfigure}
    \caption{\textbf{Bias across deployments and sensitive attributes}. ``B'' indicates bias towards one or more sensitive attribute(s), and ``U'' indicates an unbiased model. In \ref{fig:deployment} (left), the reported fairness assessments differ with in-the-wild studies reporting significantly less unbiased results. In \ref{fig:bias} \rev{(right)}, gender, age, and physiology are amongst the most frequently assessed attributes, while, race and language are understudied. Note that a single paper might assess more than one sensitive attribute.\label{fig:alternative}}
\end{figure*}

\subsubsection{Sensitive Attributes \& Biases\label{sensitive-attributes-biases}} 
In line with \rev{the Charter of Fundamental Rights  \cite{eu2012charter}} and prior fairness work \cite{pessach2022review}, UbiComp works investigate a variety of sensitive attributes as seen in \rev{Figure~\ref{fig:bias}}. 
Some attributes are better represented than others in the included papers, with gender and age being at the top (mentioned in almost 9 out of 10), followed by physiology and health condition (mentioned in 4 out of 10), as seen in Figure~\ref{fig:bias}.  Surprisingly, attributes with long-history of discrimination in ML, such as race, language, and nationality \cite{compas,manzini2019black,lee2018detecting}, are rarely encountered in the UbiComp literature, with only two papers discussing racial discrepancies in model performance. Yet, UbiComp is far from immune to such discrepancies. \citet{sjoding2020racial} uncovered racial and ethnic biases in pulse oximetry, while \citet{hutiri2022bias} reported language biases in speech recognition.

Gender biases have been reported in monitoring sleep posture with wireless signals \cite{10.1145/3397311}, opioid usage tracking \cite{10.1145/3478107}, diaphragmatic breathing monitor based on acoustic signals \cite{10.1145/3534595}, and speech recognition via accelerometer sensors \cite{10.1145/3494969}. Age biases have been reported in medication adherence monitoring through gait assessment \cite{10.1145/3351281}, fatigue estimation via smartphone tapping frequency \cite{10.1145/3478098}, mobility purpose and route choice inference \cite{10.1145/3314407}, and neural activation prediction \cite{10.1145/3534583}. Biases based on physiological measurements have been reported by \citet{10.1145/3517253} in fine-grained activity sensing (e.g., eye blinking, finger tracking) using acoustic signals against people of small stature, by \citet{10.1145/3550293} in vital sign monitoring through acoustic sensing against obese or overweight people, and by \citet{10.1145/3161198} in image processing with binocular thermal cameras against people of non-average height. Similarly, a model for early detection and burden estimation of AFib under-performed for long-term AFib patients \cite{10.1145/3463503}, while a wearable-based clinical opioid use tracker showed bias against chronic opioid users \cite{10.1145/3478107}. Regarding less explored sensitive attributes, \citet{10.1145/3161198} encountered model biases in user authentication via binocular thermal cameras for hijab wearers, a proxy for religion, while \citet{10.1145/3161187} uncovered language biases in speech and keyboard text entry for non-English speakers.\\


\tikzstyle{background rectangle}=[thin,draw=black]
\begin{tikzpicture}[show background rectangle]
\node[align=justify, text width=40em, inner sep=1em]{
Across application domains, \emph{gender}, \emph{age},  and \emph{physiology} and \emph{health conditions} are commonly taken into account, while \emph{race}, \emph{nationality}, and \emph{language} are overlooked.



};
\node[xshift=3ex, yshift=-0.7ex, overlay, fill=white, draw=white, above 
right] at (current bounding box.north west) {
\textit{Takeaway \#3}
};
\end{tikzpicture} 

\subsubsection{Fairness Metrics} 
Performance metrics monopolize UbiComp fairness assessment and reporting \rev{(Table~\ref{tab:summary})}. \rev{Frequently encountered} classification \rev{and regression} metrics highlight the interest of the UbiComp community for such tasks. There exist two challenges, though, with UbiComp's take on fairness: Firstly, how to define a threshold in performance evaluation above which a model is considered unfair? This challenge holds for the entirety of ML fairness research, as there are seldom clear-cut answers. Secondly, how to perform fairness assessment in regression or multi-class classification scenarios, both wildly understudied areas in the fairness domain \cite{mohamed2022normalise}? This challenge especially holds for UbiComp \rev{given} that 1 out of 2 included papers did not discuss binary classification.

To answer the first question, one could employ a statistical hypothesis test, such as the Student's t-test, for comparing samples' performance across sensitive attributes, a practice also adopted by a portion of the included papers. Yet each statistical test incorporates strict assumptions. For example, \rev{the paired Student's t-test assumes independent observations in each sample}, which is not the case in k-fold cross-validation, leading to an incorrect calculation of the t-statistic and a misleading interpretation of the results and p-value \cite{dietterich1998approximate}. McNemar's Test and $5\times2$ cross-validation and its refinements \cite{dietterich1998approximate,nadeau1999inference} are better alternatives proposed. \rev{Notably,} more than \rev{3 in 4 included papers (78\%)} did not use any statistical significance testing in their assessment. An alternative is the usage of fairness metrics\rev{, e.g., demographic parity, equalized odds \cite{garg2020fairness}}. Once a fairness metric is obtained, it is common practice to apply the ``4/5 rule'' or ``80\% rule'', which states that ``the selection rate of any group should be not less than 4/5 than the one of the group with the highest selection rate'' \cite{castelnovo2022clarification,usguidelines}. Yet, there is no single fairness definition, metric, or threshold that will universally apply to different applications. For example, in high-stakes applications, \rev{such as} health, \rev{a model with 80\% accuracy on Black patients and 95\% accuracy on White patients may not be considered fair, even if it satisfies the ``4/5 rule''.} \rev{Notably,} we did not identify any fairness metrics in the included papers, indicating the disjointedness between the UbiComp and FAccT communities.  

Regarding the second question, nearly half of the included papers (47\%) engage in regression or multi-class classification tasks, such as respiratory rate detection and HAR. Yet, the most common ML paradigm explored in fairness research is binary classification \cite{mohamed2022normalise}. 
In one of the few works about fair regression, \citet{agarwal2019fair} introduced two definitions of fairness in regression: \textit{statistical parity}, which asks that the prediction be statistically independent of the sensitive attribute, and
    \textit{bounded group loss}, which asks that the prediction error restricted to any sensitive group remain below some predefined threshold. A popular way to quantify fairness in regression is to compare the outcome distribution across sensitive attributes using the Kullback–Leibler divergence \cite{joyce2011kullback}, or Kolmogorov-Smirnov test for goodness of fit \cite{massey1951kolmogorov}. \rev{If the null hypothesis that the distributions come from the same population is not rejected, the model is considered fair.} Popular ML fairness libraries, such as AIF360\footnote{\url{https://aif360.mybluemix.net/}} and FairLearn\footnote{\url{https://fairlearn.org/}}, at the time of writing, do not include any regression-specific fairness metrics' implementations. \rev{Similarly,} in multi-class classification scenarios, computing fairness metrics such as equalized odds and demographic parity \rev{is not directly applicable.} \\


\tikzstyle{background rectangle}=[thin,draw=black]
\begin{tikzpicture}[show background rectangle]
\node[align=justify, text width=40em, inner sep=1em]{
No UbiComp paper (from the included papers) uses modern fairness metrics, likely due to the lack of widely available regression and multi-class classification metrics.
};
\node[xshift=3ex, yshift=-0.7ex, overlay, fill=white, draw=white, above 
right] at (current bounding box.north west) {
\textit{Takeaway \#4}
};
\end{tikzpicture} 

\subsection{Model Consequences: Ethical Risks versus Opportunities\label{mf2}}
 \rev{ML's impact on UbiComp and society is undeniable}; human authentication \cite{10.1145/3351273}, early detection of AFib \cite{10.1145/3463503}, \rev{and} respiratory rate monitoring \cite{10.1145/3463503} are only a handful of examples that reinforce this argument. \rev{The current debate, however, is about the specifics of this impact---who, how, where, and when it will be felt.}

To concretize these questions, the Scientific Committee of the AI4People\footnote{An Atomium–European Institute for Science, Media and Democracy initiative.} has \rev{identified four key areas related to human dignity and flourishing that AI can positively impact} \cite{floridi2018ai4people}: ``who we want to become'' (enabling self-realization), ``what we can do'' (enhancing human agency), ``what we can achieve'' (increasing individual and societal capabilities), and ``how can we interact with each other and the world'' (cultivate societal cohesion). Ethical opportunities within UbiComp span across all four points: Automation of mundane tasks such as gait-based human authentication \cite{10.1145/3351273}, speech transcription \cite{10.1145/3161187}, or gesture recognition \cite{10.1145/3432235}, \rev{may result in more intelligent use of time} (self-realization). Augmentation of human intelligence such as cognitive load measurement \cite{10.1145/3448111}, or cognitive performance prediction \cite{10.1145/3448111} may enable humans to do more, better, and faster (human agency). Innovations in medicine such as PTSD screening \cite{10.1145/3411835},  medication adherence monitoring \cite{10.1145/3351281}, or post-operative complications prediction \cite{10.1145/3534578}, may reinvent society by \rev{re-defining} what humans are collectively capable of (individual and societal capabilities), while cooperative work such as social context inference \cite{10.1145/3478126}, may support collaboration (societal cohesion).

However, we must also consider the ethical risks associated with inadvertent overuse and deliberate or unintended misuse of UbiComp technologies. While performance optimization is frequently fueled by the potential of ethical opportunities, fairness assessment is also driven by ethical risks, \rev{namely:} What is the consequence of the predictive outcome? \rev{Answering} this question can drive the choice of suitable fairness definitions and metrics but is far from straightforward. For instance, in the case of AFib detection, a false negative outcome might prove deadly. Yet, ``\textit{deploying a system with a high false alarm rate can add anxiety to people}'' \cite{10.1145/3463503,10.1145/3534578}. On the contrary, in speech-based human identification scenarios, false positive outcomes are critical in preventing unauthorized access, as ``\textit{existing voiceprint-based authentication often suffers from various voice spoofing attacks}'' \cite{10.1145/3448113}. 

Prioritizing predictive outcomes becomes more challenging \rev{if seen in} context. Oftentimes UbiComp technologies are built and evaluated as if they were fully autonomous, while in reality, they operate in a complicated sociotechnical system moderated by institutional structures and human stakeholders (the ``framing trap'' \cite{selbst2019fairness}). For instance, in opioid use tracking \cite{10.1145/3478107}, and drug-seeking behavior sensing \cite{10.1145/3328917} applications the consequence of a predictive outcome depends on the assumption of punitive or restorative justice \cite{hadzi2019restorative}. As an example, if a substance abuse detection technology is adopted by a restorative system, a false negative outcome might derive an individual struggling with drug addiction from crucial access to rehabilitation services. For instance, ``\textit{if such a device were found to be reliable, it could be used to monitor early treatment response and therefore could allow clinicians to more rapidly optimize patient care}'' \cite{10.1145/3328917}. On the contrary, if the exact same technology is employed as part of a punitive system, then a false negative outcome might lead to a wrongful accusation or conviction. 

Nevertheless, fear, and misplaced concerns should not inhibit the UbiComp community from realizing ethical opportunities for individual and societal good. On the contrary, mindful use of ML is conscious of our commonalities and differences across \rev{demographic} groups, as well as the factors within and outside the community's control. \\
 


\tikzstyle{background rectangle}=[thin,draw=black]
\begin{tikzpicture}[show background rectangle]
\node[align=justify, text width=40em, inner sep=1em]{
Given UbiComp's high-stake applications, fairness reporting and justification help prioritize tradeoffs between ethical risks and opportunities. 
};
\node[xshift=3ex, yshift=-0.7ex, overlay, fill=white, draw=white, above 
right] at (current bounding box.north west) {
\textit{Takeaway \#5}
};
\end{tikzpicture} 

\subsection{How does UbiComp Capture Alternative Notions of Fairness?\label{mf3}}
Previously, we have established that only a small fraction of IMWUT works (5\%) follow conventional fairness definitions with respect to one or more sensitive attributes. Yet, we believe such definitions do not do full justice to the community's work, which strives for ``fairer'' models, perhaps not across sensitive attributes but differing experimental conditions. In particular, we noticed that, in evaluating new UbiComp systems, the community aims for generalizable and robust models by performing ablations studies, comparing deployment settings, and personalizing models for users and groups \rev{(57\% of included papers)}. 
\smallskip 

\noindent\textbf{Ablation Studies.} In an ablation study, one or more components of the model are systematically removed or modified, and the performance of the model is evaluated after each change, \rev{ensuring its generalizability and robustness}. In the included papers, ablation studies take the form of performance evaluation comparisons based on: \textit{user-related}, \textit{device-related}, \textit{environmental}, \textit{experimental}, and \textit{domain-specific components}. In particular \textit{user-related components} include user motion and orientation during data collection in sleep posture monitoring~\cite{10.1145/3397311}, breathing monitoring~\cite{10.1145/3534595}, gesture recognition~\cite{10.1145/3432235}, user identification~\cite{10.1145/3264944}, and heart activity monitoring~\cite{10.1145/3478127}, as well as aesthetics, such as hair or clothing in fine-grained activity sensing~\cite{10.1145/3517253}, breathing and vital sign monitoring~\cite{10.1145/3534595,10.1145/3550293}, and user identification~\cite{10.1145/3161198,10.1145/3351273}. \textit{Device-related components} include device type, \rev{placement, and orientation,} sampling rate, and operating system in activity and gaze tracking~\cite{10.1145/3517253,10.1145/3494999}, vital sign monitoring and physiological sensing~\cite{10.1145/3550293,10.1145/3517225}, speech recognition via built-in sensors and speech synthesis \cite{10.1145/3494969,10.1145/3550338}, and user behavior sensing \cite{10.1145/3448089}. \textit{Environmental components} include ambient noise, light, and temperature that might affect data quality of acoustic~\cite{10.1145/3517253,10.1145/3448113,10.1145/3550293} or video~\cite{10.1145/3191772,10.1145/3161164,10.1145/3517225,10.1145/3161198} signals, or random passers-by for human identification \cite{10.1145/3351273}. Regarding experimental setup, few included papers studied the effect of equipment placement (i.e., distance, angle) and characteristics (i.e., range) on the model's robustness in activity sensing~\cite{10.1145/3517253} and vital sign monitoring using acoustic signals~\cite{10.1145/3534595,10.1145/3550293}. Yet, the choice of components to consider in an ablation study is highly domain-dependent. \textit{Domain-specific components} have no limitations and can range from screen size in scrolling interaction experiments \cite{10.1145/3351255} to food structure in food-related artifact development \cite{10.1145/3550312}.
\smallskip 


\noindent\textbf{Deployment Setting.} \rev{A study's deployment setting, whether it be in-the-lab or in-the-wild, can significantly impact its outcomes.} While laboratory settings can provide controlled environments for experimentation, they may not accurately reflect the complexities of the real world in which the applications are deployed. As a result, in-the-wild (or in-situ) studies have emerged as an alternative, focusing on evaluating the situated design experience of UbiComp. Figure~\ref{fig:deployment} shows the distribution of in-the-lab and in-the-wild studies in the included papers, along with their reported fairness assessment results. We see that perhaps not surprisingly, in-the-lab studies prevail ($\sim55\%$), which can be explained by the nature of numerous IMWUT papers presenting artifacts or early-stage work. Nevertheless, $\sim38\%$ of included papers conduct in-the-wild studies, and a small fraction of papers ($\sim7\%$) report results for both deployments. \rev{Interestingly,} while 3 out of 10 in-the-lab studies do not identify biases, this number falls to 0.5 for in-the-wild studies \rev{(See Figure~\ref{fig:deployment})}. This confirms our intuition that controlled environments \rev{can} conceal biases that would emerge once a model is deployed in the real world.
\smallskip 

\noindent\textbf{Personalization.} \rev{1 in 5 papers} trained separate personalized models for inference on a single subject \cite{10.1145/3351281,10.1145/3264944,10.1145/3161601} or a group of subjects sharing a common characteristic. For instance, \citet{10.1145/3369820} built personalized models for different age groups ``\textit{illustrating differences in communication patterns across age demographics that can impact model performance}''. Similarly, \citet{10.1145/3517249} utilized age-specific models for \rev{just-in-time} mobile safety help, as ``\textit{different ages in the sample have a significant influence on supportable moment predictions}''. \citet{10.1145/3517225} developed personalized models based on skin tone for camera-based photoplethysmography, as ``\textit{previous work had already highlighted [skin tone and gender] issues with the Plane-Orthogonal-to-Skin [method]}''. \citet{10.1145/3494969} developed gender-specific models for speech recognition, as ``\textit{women’s voice is generally thinner and higher in pitch}'', while \citet{10.1145/3397313} explored BMI-based models for detecting eating activities via a multi-sensor necklace, due to ``\textit{differences in movement patterns while eating, change in the distance of the proximity sensor from the neck, and difference in posture during the eating activity}''. 

\rev{Overall, the gains from the interaction between the UbiComp and fairness communities can be bi-directional. On one side, UbiComp can learn from other communities' best practices in fairness assessment and reporting. Still, the latter can build on UbiComp's approach of rigorous validation, model personalization, and striving for robustness and generalizability through in-the-wild deployments and comprehensive ablation studies.}\\



\tikzstyle{background rectangle}=[thin,draw=black]
\begin{tikzpicture}[show background rectangle]
\node[align=justify, text width=40em, inner sep=1em]{
Modern fairness reporting aside, the UbiComp community strives for generalizability by conducting and reporting ablation studies, in-the-wild vs. in-the-lab experiments, and personalized models.
};
\node[xshift=3ex, yshift=-0.7ex, overlay, fill=white, draw=white, above 
right] at (current bounding box.north west) {
\textit{Takeaway \#6}
};
\end{tikzpicture} 
\smallskip









\subsection{Is UbiComp Susceptible to Data Biases?\label{historical-biases}}
UbiComp is inherently multimodal; \rev{beyond self-reported data, whose biases are well reported \cite{rosenman2011measuring},} the included papers contained heterogeneous \rev{data} modalities, such as audio, video, images, text, and sensor data. \rev{These can provide a more nuanced representation than any one data modality alone.}
\smallskip

\noindent \textbf{Biases in audio} (used by 18\% of included papers) are well-reported in fairness research communities \cite{hutiri2022bias,markl2022language,10.1145/3485730.3493448}. UbiComp posed no exception, with such biases surfacing from the included papers. Several works discovered biases in acoustic signals dependent on body size, a potential proxy for gender, physiology, and race. Specifically, \citet{10.1145/3517253} reported ``\textit{higher respiration [detection] error [...] due to [...] weaker chest motions and smaller body size}'', while \citet{10.1145/3534595} reported bias against women due to ``\textit{different physiological structures}'', \rev{as} ``\textit{the \rev{[smaller]} reflective surface of women [...] encodes less information}''. Similarly, \citet{10.1145/3550293} found that acoustic signals for heartbeat monitoring were biased against people with larger BMI, as ``larger BMI [would] make the thoracic muscle thicker and block the weak heartbeat signals''. 
\smallskip

\noindent \textbf{Biases in video and image} (used by 20\% of included papers) are also well-studied in fairness literature \cite{dasari2021evaluation, nowara2020meta}. 
Some of these biases are easier to distinguish as demographic information is integrated into the data. For example, gender may be inferred from \rev{video} or speech signals. However, sensor signals ---the most prevalent data modality used by the UbiComp community--- are more challenging but equally susceptible to data biases. Prior research has reported racial and ethnic biases in pulse oximeters \cite{sjoding2020racial}. Additionally, gender biases \rev{correlate to} electrocardiogram (ECG) quality, given that certain ECG metrics exhibit gender-based differences \cite{xue2014can}. Even the most inconspicuous sensors, such as heart rate and acceleration sensors, are shown to be correlated with health, fitness, and demographic characteristics \cite {spathis2021self}.
\smallskip

\noindent \textbf{Biases in sensor signals} (used by 51\% of included papers) 
 could be attributed either to measurement inaccuracy or concept drift phenomena in signal patterns. For example, \citet{10.1145/3351281} reported decreased accuracy in gait detection via accelerometer and gyroscope measurements for elderly users as ``Human gait patterns inevitably change with the increase of age.''. Furthermore, \citet{10.1145/3494969} and \rev{\citet{10.1145/3550338}} encountered inaccuracies in accelerometer-based speech recognition \rev{and synthesis}, ``Since women's voice is generally thinner and higher in pitch, [and] it may be harder for [the] accelerometer to preserve voice feature''. On a different note, \citet{10.1145/3397313} utilized multiple sensors (\rev{e.g.}, inertial measurement unit (IMU), \rev{and} proximity sensors) for eating activity detection. They \rev{found} performance discrepancies for participants with large BMI, attributing them to data-related factors, such as ``\textit{change in the distance of the proximity sensor from the neck''}. It is crucial to understand that such biases are not straightforward to distinguish. On the contrary, they remain hidden into time-series signals, or, even worse, propagated to inferred high-level signals such as steps, physical activity, and sleep.\\ 


\tikzstyle{background rectangle}=[thin,draw=black]
\begin{tikzpicture}[show background rectangle]
\node[align=justify, text width=40em, inner sep=1em]{
Measurement inaccuracies and concept drift phenomena in captured audio, video, image, and sensor signals lead to performance discrepancies across sensitive attributes.
};
\node[xshift=3ex, yshift=-0.7ex, overlay, fill=white, draw=white, above 
right] at (current bounding box.north west) {
\textit{Takeaway \#7}
};
\end{tikzpicture} 
\smallskip


\begin{figure}[tb!]
  \centering
  \includegraphics[width=\linewidth]{figures/weirdNew.pdf}
  \caption{\textbf{Analysis of sensitive attributes and data size}. The bar plots show the percentage of papers reporting certain sensitive attributes (left) and the (mean) sample size in the subset of papers reporting that attribute (right). Sample demographics reporting is not standardized and frequently incomplete, with race, employment status, and education being the least reported sensitive attributes ($\leq20\%$). While UbiComp samples tend to be gender-balanced, they are still WEIRD, as they consist of predominantly White, highly-educated, and US-based subjects.\label{fig:weird}}
\end{figure}

\subsection{How WEIRD is UbiComp?\label{weird}}
\rev{Figure~\ref{fig:weird} shows the analysis of sensitive attributes in the included papers.} The bar plot on the left (a) shows the percentage of papers reporting sensitive attributes: gender, age, race, education, employment, and sample country. The bar plot on the right (b) gives the mean sample size within each subset of papers reporting that sensitive attribute. Note that \rev{we chose the mean over the median} because the \rev{latter} tends to be less accurate when sample sizes are small \cite{lewis1993multipoint}. In particular, gender was the most reported sensitive attribute. 86\% of included papers ($N=42$) disclosed this information. Within these 42 papers, the mean sample size was 136 users, and the mean number of females was 71. Evidently, the community has \rev{stepped} in the right direction to achieve more balanced, diverse, and representative datasets. Yet, there is plenty of room for improvement. Age was only reported in 4 out of 10 papers ($N=20$), and the mean sample age within these papers was 31 years old. For comparison, the maximum mean age reported was 66 years. In a world that is rapidly aging \cite{world2015world}, the UbiComp community is predominantly developing and testing on young populations. Finally, only 12\% of included papers ($N=6$) mentioned the participants' race. \rev{Among those papers, the sample size was an average of 112 users, with 88 of them being White (79\%). Although these data are limited, they highlight not only that race is often overlooked in UbiComp research (\S\ref{sensitive-attributes-biases}),} but also that non-White populations are underrepresented in the datasets. There is a risk that models underperform for non-White users, but this may go unnoticed. For instance, \citet{10.1145/3463503} could not assess the impact of skin tone on AFib detection ``due to the unbalanced dataset where the majority (88.7\%) of participants were White''.

Confirming IMWUT's WEIRDness, out of the 42 papers for which the participants' country (a proxy for Western) is reported or can be inferred, 36 (86\%) engaged with US samples. 
China (26\%) and Switzerland (7\%) completed the top-3 of country representation. Note that the percentages do not sum up to 100 because of papers with more than one sample country. Concerning education, 20\% of included papers ($N=10$) reported relevant information, with a median sample size of 144 users, 92 of which were college-educated (80\%). This is perhaps not surprising, as in the early stages of development in UbiComp, participant recruitment frequently \rev{occurs} within the universities. Regarding employment status (a proxy for Industrialized and Rich), it was only reported in 14\% of included papers ($N=7$), with a mean sample size of 208 users, of which 152 were employed.

Digging deeper into IMWUT's WEIRDness, we found that out of the 49 included papers, 32 (65\%) included at least one author with a US-based affiliation. Out of those, only 3 papers (7\%) recruited at least part of their sample from a country different than the authors' location. In the remaining cases, participants were from the same country as at least one of the affiliated institutions. These results demonstrate that \rev{most} IMWUT authors (93\%) recruit samples within \rev{their country}. This proportion is \rev{larger than} in other communities, such as CHI \cite{linxen2021weird}. This is possibly due to the requirements of in-the-lab, artifact-based research common within the UbiComp community.\\


\tikzstyle{background rectangle}=[thin,draw=black]
\begin{tikzpicture}[show background rectangle]
\node[align=justify, text width=40em, inner sep=1em]{
While UbiComp populations are balanced in terms of gender, they are otherwise predominantly young, White, western, highly educated, and employed, calling for more diverse sample recruitment.
};
\node[xshift=3ex, yshift=-0.7ex, overlay, fill=white, draw=white, above 
right] at (current bounding box.north west) {
\textit{Takeaway \#8}
};
\end{tikzpicture} 



 
