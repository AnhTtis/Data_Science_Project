\section{\rev{Background and} Related Work\label{related-work}}
\rev{First, we present the reader with an introduction to fairness: definitions, measurements, and enhancement mechanisms.} Next, we situate our review in previous fairness literature covering three broad areas: \emph{a)} \rev{comprehensive surveys}; \emph{b)} \rev{domain-targeted surveys}; and \emph{c)} \rev{and surveys focused on over-represented populations.} \\


\noindent\textbf{Fairness Definitions and Measurements.} \rev{Fairness entails the} ``protection of individuals and groups from discrimination or mistreatment with a focus on prohibiting behaviors, biases and basing decisions on certain protected factors or social group categories''~\cite{fairnessberkeley}. Quantitative fields (e.g., computer science, statistics) view fairness as a mathematical problem of ``equal or equitable allocation, representation, or error rates, for a particular task or problem''~\cite{narayanan21fairness}. 
\rev{Fairness enhancement mechanisms fall under three categories \cite{pessach2022review,caton2020fairness}: a) \textit{pre-processing}; b) \textit{in-processing}; and c) \textit{post-processing} mechanisms. Pre-processing mechanisms involve altering the training data before feeding it into an ML model, while in-processing mechanisms involve modifying the ML algorithms to account for fairness during training. Ultimately, post-processing mechanisms involve altering the output probabilities of an ML model to mitigate biases/imbalances (e.g., enforcing a quota). However, due to the late stage in the learning process in which they are applied, post-processing mechanisms commonly obtain inferior results \cite{woodworth2017learning}. They are also considered too invasive or discriminatory since they deliberately damage accuracy for some subjects to compensate others \cite{pessach2022review}; hence they are less frequently preferred in practice.}



\smallskip


\noindent\textbf{\rev{Comprehensive Surveys}.} Addressing algorithmic bias in ML has been a longstanding issue \cite{caton2020fairness}, despite its recent surge. A number of comprehensive surveys shed light on data and model biases across domains and compared potential mitigation solutions. For example, \citet{caton2020fairness} and \citet{pessach2022review} discussed fairness metrics and categorized mitigation approaches into a widely accepted framework of pre-processing, in-processing, and post-processing methods independently of the application domain. \citet{wan2022processing} focused exclusively on in-processing modeling methods such as adversarial debiasing, disentangled representations, and fairness-aware data augmentation, while  \citet{pessach2022review} provided an overview of emerging research trends, including fair adversarial learning, word embeddings, and recommender systems. Along these lines, \citet{le2022survey} surveyed available datasets for fairness research, including financial, criminological, healthcare, social, and educational datasets. Yet, despite these surveys' considerable contributions, they tend to be of generic nature and rarely discuss data, models, and applications related to an individual community. 
\smallskip

\noindent\textbf{\rev{Domain-targeted Surveys}.} Another line of work took a deep dive into well-defined domains, for example, by focusing on fairness for ML for graphs~\cite{choudhary2022survey}, on exploring notions of fairness in clustering~\cite{chhabra2021overview}, and on studying fairness in recommender systems~\cite{li2022fairness}. Another group of works targeted specific unprivileged groups or high-stakes domains. For example, \citet{olteanu2019social} reviewed the literature surrounding social data biases, such as biases in user-generated content, expressed or implicit relations between people, and behavioral traces, while in \cite{sun2019mitigating}, the authors focused specifically on gender bias in Natural Language Processing (NLP). On a different note, \citet{10.1145/3173574.3174156} featured emerging trends for explainable, accountable, and intelligible systems within the CHI community, also discussing notions of fairness. Closer to our work, \citet{mhasawade2021machine} discussed ML fairness in the domain of public and population health, and \citet{xu2022algorithmic} explored algorithmic fairness in computational medicine, which only covers a subset of the broad, interdisciplinary UbiComp research domains.
\smallskip

\noindent\textbf{\rev{Surveys focused on over-represented populations (WEIRD Research)}.} 
WEIRD research refers to a common criticism in the social sciences that much of the research is conducted on a sample of participants that is \textbf{W}estern, \textbf{E}ducated, \textbf{I}ndustrialized, \textbf{R}ich, and \textbf{D}emocratic. In particular, a comprehensive study conducted by \citet{henrich2010weirdest} in 2010 revealed a significant bias in sample populations, and found that most research samples come from WEIRD populations, which represent only 12\% of the global population but account for 96\% of research samples. 
Similarly, \citet{linxen2021weird} conducted a meta-study on CHI proceedings from 2016 to 2020, reporting that 73\% of those papers are based on Western populations. \rev{Another recent meta-study on FAccT proceedings from 2018 to 2021 found highlighted concerns with biases in word embeddings and computer vision, and racial disparities, while only a $\sim10\%$ of those papers used original, empirical datasets \cite{laufer2022four}. Similarly, by analyzing FAccT proceedings between 2018 and 2022, Septiandri et al.~\cite{ali_weird_2023} found that 84\% of the analyzed papers were exclusively based on participants from Western countries, particularly exclusively from the U.S. (63\%).} 

It is evident that research communities other than UbiComp have \rev{already} started to explore ways of reporting data and models in a fair manner to surface and, ultimately, address encountered biases. Yet, the state of fairness in the UbiComp community remains unknown, as, at the time of writing, there exists no other survey or position paper in the intersection between UbiComp and fairness. 

