\section{Discussion}
\label{sec:discussion}
By screening 523 papers published at IMWUT between 2018 and 2022, we found that only a small portion \rev{(5\%)} adhered to fairness reporting, while \rev{most} focused on accuracy or error metrics. By delving into the smaller number of 49 papers, we surfaced biases in machine learning data and models across several sensitive attributes and application domains that would otherwise remain scattered in the UbiComp literature. Yet, the identified lack of diverse datasets in IMWUT publications could result in biases remaining undetected in the absence of heterogeneous demographics. To quantify such biases, included papers primarily employed performance evaluation instead of fairness metrics, while challenges in fairness assessment were found in regression and multi-class classification scenarios. \rev{We also validated the generalizability of our findings across UbiComp venues by reviewing retrieved publications from the past year.}
Similar to other communities, defining fairness in UbiComp was not a simple task and involved considering its sociotechnical context, its ethical risks, and opportunities. Nevertheless, in an effort to employ fairness in practice ---sensitive attributes aside--- we found that the community has been striving for generalizability through ablation studies, real-world deployments, and personalization.

\begin{figure}[]
  \centering
  \includegraphics[page=1,trim={0 0 0 0.2in},width=0.65\linewidth]{figures/guidelinesBlack.pdf}
  \includegraphics[page=2,width=0.65\linewidth]{figures/guidelinesBlack.pdf}
    \includegraphics[page=3,trim={0 1.1in 0 0},width=0.65\linewidth]{figures/guidelinesBlack.pdf}
  \caption{\textbf{Recommendations for ``Fairness by design'' in UbiComp.} \rev{Guidelines for researchers for performing fairness assessments accompanied by implementation details and examples. Fairness should not be an afterthought but starts with problem definition and data collection.}\label{fig:recommendations}} 
\end{figure}

Drawing from these findings and borrowing from the ``Privacy by design'' literature \cite{fjeld2020principled,cavoukian2009privacy}, we propose a ``Fairness by design'' equivalent, requiring AI developers and researchers to consider data and model fairness from the very beginning of any project or system design. It is, thus, a proactive and preventative approach that prioritizes fairness as a core value in the development and implementation of UbiComp technologies, products, and services. To facilitate the community achieving ``Fairness by design'', we next discuss recommendations for integrating fairness into the entire ML pipeline of UbiComp studies, \rev{as summarized in Figure~\ref{fig:recommendations}}. \\



    





\noindent \textbf{\rev{Data Collection \& Annotation.}} 
\rev{In preparing for data collection and during the problem definition}, researchers should \textit{identify the types of biases and harms} relevant to their work (e.g., quality-of-service, allocation, stereotyping, and erasure harms \cite{crawford2017trouble}). For example, in an AFib detection application, quality-of-service harms could occur if the model had a substantially different performance \rev{across} ages, while allocation harms could occur if such difference led to one group unfairly receiving \rev{worse} care. Additionally, it is important to consider the demographics ---including historically marginalized groups (e.g., based on gender, race, and ethnicity)--- that might be harmed. Specifically, we should consider groups relevant to a particular scenario or deployment setting. For example, in a depression screening application, gender could be relevant as a sensitive attribute due to reported gender differences in the disorder's signals \cite{parker2010gender}. \rev{Relevant attributes can be identified through consulting domain experts, studying related literature, or conducting small-scale feasibility studies in the target application domain.}

\rev{Concerning data collection, for self-collected data, researchers should also consider the generalizability of the prediction task (e.g., whether it is applicable across demographics). Specifically, \textit{ensuring an adequate sample size} would enable fairness to be studied (e.g., through sub-group analyses).} For example, AFib detection systems have shown poor performance in people with abnormal heart rhythms other than AFib because their data annotation scheme assigned Normal sinus rhythm (NSR) and other types of heart rhythms to the same Non-AFib category, due to the limited number of subjects with different types of heart rhythms \cite{10.1145/3397313}. 
\rev{However, we recognize that recruiting large samples might be impractical in early-stage feasibility studies. In such situations, as per takeaway \#8, researchers should \textit{aim for a diverse user sample} based on assumptions about relevant sensitive attributes. This approach can help uncover potential biases in the data and models, which can be addressed in later stages of development. For example, if a research project concerns an oximeter-based artifact, one should consider recruiting at least a handful of users with darker skin tones to acknowledge prior shortcomings of the technology in non-white populations \cite{sjoding2020racial}. Additionally, researchers should strive for a \textit{diverse representation during data annotation}. Considering that the models encode the biases of the labels, they should not only be assessed by multiple people to ensure agreement but also strive for demographic diversity amongst them.}

\rev{Finally, during the data reporting phase, as per takeaway \#3, researchers should \textit{accompany published datasets with rich protected attributes}, enabling further fairness analyses. Naturally, this creates stricter requirements in terms of proper anonymization (e.g., utilizing time-series anonymization methods for mobile data \cite{malekzadeh2019mobile}), and secure sharing via designated platforms (e.g., PhysioNet, Synapse).}
At the same time, the community itself could implement a mandatory data statement policy, requiring authors to report sensitive attributes concerning their participant samples. This builds on recent quests that advocate for data excellence~\cite{sambasivan2021everyone}, for example, by making data statements and datasheets for datasets mandatory for authors submitting their work.\\

\noindent \textbf{\rev{Data Exploration \& Manipulation.}}  
\rev{In the data exploration phase, researchers should \textit{think carefully about the pre-processing methodology}. Unlike other fields where the datasets are provided out-of-the-box, e.g., FAccT, RecSys, in UbiComp, it is not uncommon to require further slicing or windowing prior to predictive modeling. For example, applying a sliding window method can generate thousands of samples from a sensor signal that belongs to a single user. This carries the risk of providing virtually ``enough'' samples for training, which, however, come from a handful of users. As a result, the model may not learn generalizable patterns.}

\rev{Similarly, data validation methods can facilitate fairness and robustness and, therefore, should also be applied across sensitive attributes (takeaway \#7). For example, \textit{inspecting outliers or missing values} that systematically fall into particular demographic groups, or other kinds of data anomalies, such as \textit{measurement error} due to a device malfunction or device differences.} Regarding the latter, devices such as smartwatches offer model-based estimates for many well-being features. For instance, the measurement error of a heart-rate prediction model can propagate to every downstream application. If the original device or model has not been validated across different groups, this can affect every possible application using such data. More broadly, visualization tools (e.g., \rev{What-If Tool \cite{wexler2019if}, FairLens \cite{panigutti2021fairlens},} Tensorflow's Fairness Indicators\footnote{\url{https://github.com/tensorflow/fairness-indicators}}) can help surface any potential data anomalies and help correct them before they creep into models.
\\


\noindent \textbf{Model Training \& Evaluation.} 
\rev{According to our findings, only 1 out of 4 included papers utilizes bias mitigation approaches (takeaway \#2). While we have witnessed a remarkable consolidation of deep-learning models and architectures such as Convolutional Neural Networks \cite{krizhevsky2017imagenet} and Transformers  \cite{vaswani2017attention} in UbiComp research, fairness-preserving mechanisms remain unexplored. Yet, there is a plethora of fairness toolkits, such as FairLearn \cite{bird2020fairlearn}, AIF360 \rev{\cite{bellamy2019ai}} and Aequitas \rev{\cite{saleiro2018aequitas}}, which provide diverse pre-processing, in-processing, and post-processing \textit{bias mitigation algorithms out-of-the-box} --- some applicable regardless of input data types.}

Yet enhancing fairness in machine learning requires a means to quantify it. As UbiComp systems blend into the real world, we realize that single evaluation metrics struggle to reflect the success criteria of ML models. As such, monitoring a multitude of metrics becomes the norm, \rev{inspired by takeaways \#1 and \#4}, we believe that \textit{monitoring and reporting diverse fairness metrics} \rev{(included in the aforementioned libraries)} across different protected groups should become standard practice. \rev{Fairness trees \cite{saleiro2018aequitas} accompanied by domain expertise can help researchers choose appropriate metrics.} \rev{At the same time, we need to account for intersectional fairness, i.e., designing and training algorithms to account for the complex ways that different social identities can intersect and impact a person's experiences and outcomes \cite{filippi2023intersectional,foulds2020intersectional}.} UbiComp technologies for diagnosing heart disease and monitoring vital signs provide an exemplary case. As reports suggest, differences in coronary heart disease are based on gender \cite{maas2010gender}, socioeconomic status \cite{schultz2018socioeconomic}, and race \cite{fincher2004racial}. In such cases, it is important to ensure that the models do not perpetuate existing biases and inequalities by failing to account for intersectional differences in health outcomes and access to healthcare---the biases encountered by a Black woman from a low socioeconomic background may not be the same as those experienced by a White woman from a high socioeconomic background. 

However, we acknowledge that sometimes it is feasible to collect data from representative demographics, especially for smaller pilot studies, to enable such comparisons. In these cases, researchers can leverage advances in generative models to \textit{synthesize data covering multiple protected attributes} and potential intersections \cite{chaudhari2022fairgen,dahmen2019synsys}. 

Finally, beyond traditional notions of fairness, such as directly discriminating based on sensitive attributes, we should also \textit{consider indirect notions of fairness} \rev{(takeaway \#6)}. For example, within the paradigm of distributed/federated learning, the resource allocation of participating devices may also reflect the demographic and socio-economic information of owners, which makes the exclusion of such clients unfair in terms of participation.  Cheaper devices cannot support the execution of large models and are either excluded or dropped together with their unique data \cite{horvath2021fjord, cho2022flame}. \rev{Hence, regardless of the training paradigm, researchers should look beyond accuracy in evaluation considering additional constraints, such as device diversity or connectivity.}
\\

\noindent \textbf{\rev{Model Deployment \& Use.}} 
As models are deployed in real applications, we should \textit{monitor their performance in real-time} and adjust for data and fairness drift \cite{ghosh2022faircanary} to ensure that models produce fair predictions independent of changes in input data and demographics. \rev{For example, in fall detection systems, models are usually trained on data from pretend falls performed by young people yet deployed to detect real falls in the elderly population. Such demographic drift might be unavoidable in the training phase due to safety concerns, but its effects can be alleviated during the deployment phase by \textit{performing model re-assessments and continuous refinements} on the target population.} \rev{Additionally, to ensure that model behavior is generalizable for a larger group of users, researchers can also employ newly-developed cross-dataset benchmarks for longitudinal models \cite{xu2023globem}.} 

\rev{Finally, researchers should consider that such model refinements can disproportionally affect certain user groups. For example, changes in heart-rate prediction models even if beneficial for minority groups, can propagate to resting heart rate estimations, affecting the end-user experience. Thus, researchers should \textit{prioritize transparency in model versioning} and accompany each version with reports on performance, fairness, and model robustness also conditioned on protected attributes.}






















