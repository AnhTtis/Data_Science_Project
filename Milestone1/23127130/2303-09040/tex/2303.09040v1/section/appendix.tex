
\appendix

\section{More Details about GSSA}

\paragraph{Computational Complexity of GSSA.} Given an input $\mathbf{X} \in \R^{H \times W \times D \times C}$ where $H,W$ denote height and width,  $D$ denotes the number of spectral bands, $C$ denotes the features channels, the computational complexity of each step of GSSA is summarized in \tabref{tab:gssa-complexity}. Since the feature channels are typically larger than the number of spectral bands, the asymptotic computational complexity of GSSA is dominated by two linear transformations, \ie, \emph{Linear for $V$} and \emph{Post linear}.

\begin{table}[h]
\small
\centering
\begin{tabular}{l|l}
\hline
\textbf{Step} & \textbf{Complexity} \\ 
\hline
Linear for $V$     &   $(H \times W \times D) \times C^2$         \\
Pooling for $Q$,$K$     &  $ 2\times (H \times W) \times (D \times C)$          \\
Compute attention matrix     & $D\times D \times C$           \\ 
Feature aggregation    & $H \times W \times C \times D \times D$           \\ 
Post linear    & $(H \times W \times D) \times C^2$           \\ 
Total &  $O((H \times W \times D) \times C^2)$ \\
\hline
\end{tabular}
\caption{The computational complexity of GSSA. The overall complexity of GSSA is linear with respect to image size.}
\label{tab:gssa-complexity}
\end{table}


\vspace{-3mm}
\paragraph{Fast Implementation.} 

With the simplification of pixel-wise attention via global average pooling, our GSSA can be efficiently implemented with a depth-wise convolution by treating the shared attention map as a convolution filter and swapping the spectral and channel dimensions. The speed comparison is shown in \tabref{tab:gssa}, and it can be seen that the Conv-based implementation is approximately 20\% faster than the naive \verb+Matmul+-based one.



\begin{table}[h]
\setlength{\tabcolsep}{0.5cm}
\centering
\small
\begin{tabular}{l|c|c}
\hline
\textbf{Implementation}     &  \textbf{ Runtime (s)}  & \textbf{PSNR}    \\ \hline
Matmul-based       &  0.60 & 41.82 \\ 
Conv-based       &  0.47  & 41.82\\ \hline
\end{tabular}
\caption{Speed of different implementations of GSSA. Our Conv-based implementation reduces the running time without harming the performance.}
\label{tab:gssa}
\vspace{-3mm}
\end{table}



\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{imgs/cmp_attn}
\vspace{-5mm}
\caption{Comparison of different spectral/channel attention mechanisms. Our GSSA is significantly different from previous attention mechanisms. We could observe MDTA and MGSA are almost identical; MS-MSA and GSA are almost identical. Besides, MS-MSA and GSA are also basically simpler version of MDTA without depthwise convolution. Please refer to the text for detailed explanation. 
}
\label{fig:cmp}
\vspace{-2mm}
\end{figure*}

\vspace{-3mm}
\paragraph{Comparison against other Attention.} 
Here, we provide a more detailed explanation regarding the differences between our GSSA and existing channel or spectral attention mechanisms.
\emph{We highlight that our GSSA is significantly different from previous attention mechanisms in a variety aspects}. Since GSSA performs attention along spectral rather than spatial dimensions, we here compare it with four previous attention mechanisms that apply along spectral or channel dimensions including: 
\begin{table}[h]
\small
\centering
\begin{tabular}{c|c|c}
\hline
\textbf{Attention} & \textbf{Method} &  \textbf{Task} \\ 
\hline
MDTA & Restormer \cite{zamir2022restormer} & Color image restoration \\
MS-MSA & MST \cite{cai2022mask} & Spectral Reconstruction \\
GSA & SST \cite{li2022spatial} & HSI denoising \\
MGSA & Hider \cite{chen2022hider} & HSI denoising \\
\hline
\end{tabular}
\caption{The competing attention mechanisms.}
\label{tab:comparsison}
\end{table}

\figref{fig:cmp} illustrates the structures of the aforementioned attention mechanisms. It is worth noting that all previous methods are essentially variants of MDTA proposed in Restormer, whereas our GSSA is fundamentally distinct from them. In the following, we will provide a detailed explanation of the main differences between the previous methods and our GSSA.


\textbf{3D vs 2D Data Format.} The first notable difference, which can be easily confused with previous work, is that \emph{our GSSA performs attention on the spectral dimension}, i.e., the $D$ dimension of a 5D data cube $x \in \R^{B\times C \times D \times H \times W}$. In contrast, previous works, such as MST, and SST, even though they refer to their attention mechanisms as spectral attention, essentially apply channel attention along the $C$ dimension of a 4D data cube $x \in \R^{B\times C \times H \times W}$, which is the same as MDTA.
Our 3D approach provides the flexibility to handle HSIs with different bands within a single model. Additionally, it achieves superior performance by preserving the structures of different bands, \ie, each band possesses its own feature set, and their relationship remains unchanged across layers of the entire model.


\textbf{QKV Projection.} The second key difference pertains to the projections used for the query, key, and value. Conventional attention mechanisms typically employ three linear projections to project the input into query, key, and value. This approach is utilized in all of the compared methods, with the exception of our GSSA. Instead, \emph{our GSSA applies linear projection solely for the value, which greatly simplifies the design}. By contrast, MDTA needs a extra 3x3 depthwise convolution after the linear projection. MGSA is identical to MDTA, except that it employs a 3D convolution. MS-MSA and GSA are the same and solely utilize linear projections, with the exception that MS-MSA employs an additional mask attention specifically designed for spectral reconstruction.





\textbf{Pooling vs Reshape.} The third difference is that our GSSA uses global average pooling to obtain feature maps for each band. This differs from previous methods that adopt a reshape approach. Our method is significantly more computationally efficient compared to previous approaches. Previous methods reshape the Q, K, and V tensors from a shape of $H \times W \times C$ into $HW \times C$, treating $HW$ as the features for each channel. This leads to a time complexity of dot-product attention that is linear with respect to the image size, \ie, $D \times D \times HWC$. In contrast, our GSSA approach only has a constant time complexity $D\times D \times C$, where $D$ denotes the number of bands.

\textbf{Learnable Query.} The fourth notable difference is the introduction of the learnable query (LQ), which is motivated by the fixed patterns of pixel values across different bands. For example, the values of band 100 nm and 200 nm are correlated. Our LQ helps to identify these correlations and the alternative training strategy enables improvements without any extra cost on the number of parameters, inference time, and the flexibility to handle HSIs with different bands.


\section{More Ablation Studies}



To evaluate the effectiveness of the proposed components, we conduct a series of experiments to explore the different design choices for each part of our HSDT architecture. 
Specifically, we compare the proposed blocks, which include GSSA, S3Conv, and SM-FNN, by separately replacing them with existing blocks that share the same functionality, \eg, replacing S3Conv with Conv3D. We use HSDT-M as the base model and evaluate the performance of the different blocks by replacing them one at a time. For blocks that cannot be incorporated into our 3D architectural design of HSDT, such as 2D spectral attention \cite{li2022spatial}, we report the results obtained using their respective models. 



\begin{table}[t]
      \setlength{\tabcolsep}{0.2cm}
       \centering
       \small
       \begin{tabular}{l| c| c | c| c}
         \hline
      \textbf{Model}  & \textbf{\#P(Conv)} & \textbf{\#P(Total)} & \textbf{PSNR} & \textbf{SAM} \\
       \hline
       Conv3D & 0.43M & 0.58M & 41.62 & 0.052\\
       Sep3D \cite{dong2019deep}  & 0.37M & 0.53M & 41.44 & 0.054\\
\rowcolor{graycolor} S3Conv-S  & 0.26M & 0.42M & 41.47 & 0.052\\
      \rowcolor{graycolor} S3Conv-Seq  & 0.26M & 0.42M & 41.58 & 0.052\\
     \rowcolor{graycolor}  S3Conv  & 0.36M & 0.52M & \textbf{41.82} & \textbf{0.049} \\
       \hline
      \end{tabular}
      \caption{Comparison of different S3Conv variants against 3D convolution and previous separable convolution. Our S3Conv achieves significant better performance with fewer parameters. Our methods are highlighted as \colorbox{graycolor}{gray}. \#P denotes the model parameters. }
      \label{tab:ablation-s3conv}
\end{table}


\vspace{-3mm}

\paragraph{Spatial-Spectral Separable Convolution.} We evaluate several variants of our S3Conv. The most straightforward variant, S3Conv-S, sets the number of spatial convolutions to 1, while the S3Conv variant that we adopt uses 2. Another variant, S3Conv-Seq, applies spatial and spectral convolutions sequentially instead of in parallel. As shown in Table \ref{tab:ablation-s3conv}, both variants achieve comparable performance with roughly 60\% of the parameters used by Conv3D. Our adopted version achieves a 0.2 dB PSNR gain with only 80\% of the parameters used by Conv3D. Notably, our S3Conv approach significantly outperforms previous HSI separable convolution approaches \cite{dong2019deep}, achieving over 0.4 dB PSNR improvement with even fewer parameters.


   
\vspace{-3mm}

\paragraph{Guided Spectral Self-Attention.} We compare the proposed GSSA approach with existing spectral fusion techniques, including QRU \cite{wei20203}, GSA \cite{li2022spatial}, MS-MSA \cite{cai2022mask}, MDTA \cite{zamir2022restormer}, and MGSA \cite{chen2022hider}. It is worth noting that although GSA and MS-MSA are named as spectral attention, they are essentially channel attentions derived from MDTA, as discussed earlier. Furthermore, GSA, MS-MSA, and MDTA are all 2D attention approaches that work with 4D data formats instead of the 5D data format used by HSDT. Therefore, we report the results of their models when compared with GSA, MS-MSA, and MDTA. For 3D spectral fusion techniques such as QRU and MGSA, we report the results of models that replace the GSSA of HSDT-M with them. Table \ref{tab:ablation-ssa} presents the results of different attention mechanisms. Our GSSA approach achieves the best results against the other approaches. Notably, our GSSA outperforms previous GSA and MGSA approaches (which are also designed for HSI denoising) by a large margin, demonstrating the effectiveness of our designs.


\begin{table}[ht]
   \setlength{\tabcolsep}{0.3cm} 
       \centering
       \small
       \begin{tabular}{l | l | l | l}
         \hline
      \textbf{Model}  & \textbf{Params} & \textbf{PSNR} & \textbf{SAM} \\
       \hline 
     QRU \cite{wei20203}  & 0.57M & 41.31 & 0.064\\
     GSA \cite{li2022spatial} \& MS-MSA \cite{cai2022mask} & 4.14M & 41.41 & 0.052\\
     MDTA \cite{zamir2022restormer} & 26.2M & 41.03 & 0.062\\
     MGSA \cite{chen2022hider} & 0.50M & 39.74 & 0.102\\
 \rowcolor{graycolor} GSSA & 0.52M & \textbf{41.82} & \textbf{0.049}\\
       \hline
      \end{tabular}
 	\caption{Results of our GSSA in comparison with other attention blocks. Our GSSA achieves a prominent improvement against QRU by over 0.5 PSNR improvement, while previous HSI denoising transformer with GSA only outperforms QRU by only 0.1 PSNR. }
      \label{tab:ablation-ssa}
\end{table}


\vspace{-3mm}
\paragraph{Self-Modulated Feed-Forward Network.} The proposed {SM-Branch} can be used without additional conventional FFN. As shown in \tabref{tab:ablation-ffn}, the sole use of {SM-Branch} also outperforms the conventional FFN, and the combination of them both yields the best results with very few extra parameters. The {GDFN} \cite{zamir2022restormer} developed for RGB restoration performs poorly and might be unsuitable for our model. 


  \begin{table}[h]
    \setlength{\tabcolsep}{0.5cm}
       \centering
       \small
       \begin{tabular}{l | l | l | l}
         \hline
   \textbf{Model}  & \textbf{Params} & \textbf{PSNR} & \textbf{SAM} \\
       \hline
       FFN & 0.49M & 41.67 & 0.050\\
       GDFN \cite{zamir2022restormer} & 0.49M &37.38 & 0.094\\
   \rowcolor{graycolor}      SM-Branch  & 0.45M &41.74 & 0.051\\
    \rowcolor{graycolor}     SM-FFN  & 0.52M & \textbf{41.82} & \textbf{0.049} \\
       \hline
      \end{tabular}
      \caption{Comparison of the existing FFN with our SM-FFN and SM-Branch.  }
      \label{tab:ablation-ffn}
   \end{table} 



\section{More Discussions} \label{sec:discussion}



\paragraph{Visualization of S3Conv.} To demonstrate the effectiveness of our S3Conv. We provide a comparison of the features map between S3Conv and conventional 3D convolution. As shown in \figref{fig:s3conv}, our S3Conv extracts more spatial meaningful features.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{imgs/s3conv}
  \caption{Comparison of the feature maps extracted by conventional 3D  convolution and our S3Conv. }
  \label{fig:s3conv}
  \vspace{-3mm}
\end{figure}


\vspace{-3mm}
\paragraph{Analysis of SM-FFN.} The proposed SM-FFN is designed for strengthening the features with higher activation via a self-modulation operation. The improvement provided by SM-FFN could be intuitively explained by the emphasis on more informative regions that typically have higher activation. In the following, we provide some possible relations between our SM-FFN and the SiLU \cite{elfwing2018sigmoid} activation, which might further imply why our SM-FFN works better. Specifically, The SiLU activation is,
\begin{equation}
	y = x\odot \operatorname{sigmoid}(x),
\end{equation}
where $x$ and $y$ are the input and output feature maps. It can be observed that SiLU could be treated as a kind of self-modulation where the modulation weight is computed from the input itself. However, such homogeneous self-modulation might be limited in expressive abilities. Instead, our SM-FFN employs a heterogeneous self-modulation, 
\begin{equation}
	y = \operatorname{Linear_1}(x)\odot \operatorname{sigmoid}(\operatorname{Linear_2}(x)),
\end{equation}
where we adopt two extra linear projections to project input x into two different spaces. This removes the restriction of SiLU where the input $x$ should simultaneously play two roles of features and modulation weight. Thus, our SM-FFN can obtain the advantages of SiLU, \eg, training stability and implicit regularization while maintaining more representation capability. Consequently, it leads to better performance than conventional FFN.

\begin{table*}[t]
\begin{center}
   \begin{subtable}[h]{0.4\textwidth}
      \setlength{\tabcolsep}{0.35cm}
       \centering
       \small
      \begin{tabular}{|l|cc|}
\hline
\rowcolor{graycolor} Method          & PSNR       & SSIM             \\ \hline
2DTV            & 25.26      & 0.863            \\
3DTV            & 28.46      & 0.910            \\
DeSCI \cite{Liu19DeSCI}          & 26.62      & 0.912             \\
SCI-TV-FFDNet \cite{qiu2021effective}  & 29.35      & 0.925           \\ 
DPHSIR \cite{lai2022deep}       & 30.56     & 0.945           \\
\hline

PnP-HSDT (ours) & \textbf{31.64}    & \textbf{0.948}                             \\ \hline
\end{tabular}
      \caption{Results on the task of compressive sensing.}
      \label{tab:cs}
   \end{subtable}
   \hfill
   \begin{subtable}[h]{0.55\textwidth}
   \setlength{\tabcolsep}{0.3cm}
       \centering
       \small
\begin{tabular}{|l|cccc|}
\hline
\rowcolor{graycolor} & \multicolumn{2}{c}{2x}  &  \multicolumn{2}{c|}{4x}\\
\rowcolor{graycolor} Method          & PSNR       & SSIM     & PSNR & SSIM        \\ \hline
Bicubic                     & 35.13      & 0.9575 &35.12  &0.954         \\
SSPSR \cite{jiang2020learning}          & 47.55      & 0.995   &39.19 &0.979          \\
Bi-3DQRNN \cite{fu2021biqrnn3d}                   & 42.53      & 0.989    &39.56 &0.979        \\
DPHSIR \cite{lai2022deep}       & 48.75     & 0.996   &40.95 &0.980        \\
\hline
PnP-HSDT (ours) & \textbf{49.76}    & \textbf{0.996}   & \textbf{41.56}    & \textbf{0.982}                          \\ \hline
\end{tabular}
      \caption{Results on the task of super-resolution.}
      \label{tab:sr}
   \end{subtable}
\end{center}
\vspace{-5mm}
\caption{Experimental results of our PnP extension on the task of compressive sensing and super-resolution.}
\end{table*}



\section{Extension as Plug-and-Play Prior} \label{sec:ext}
Considering the superior performance of our method on the Gaussian denoising task, 
we demonstrate that HSDT can be used a plug-and-play (PnP) prior \cite{chan2016plug} to solve general HSI restoration tasks with proximal algorithms, \eg, ADMM and HQS.


\vspace{-3mm}
\paragraph{Experimental Setup.} We adopt PnP-ADMM \cite{lai2022deep} to extend our method to the tasks of compressive sensing, and super-resolution. To meet the requirements of PnP algorithms, \ie, Gaussian denoiser for continuous noise strengths, we retrain our model, \ie, HSDT-M, with an additional noise level map \cite{zhang2020plug} on simulated Gaussian noise ranged from 0 to 70. We run 40 iterations for compressive sensing and 24 iterations for super-resolution. The hyperparameters of the algorithms are manually tuned to achieve the best performance.

\vspace{-3mm}
\paragraph{Compressive Sensing.} We conduct the simulated experiments on CASSI \cite{wagadarikar2009video} system. Following \cite{qiu2021effective}, the shifting random binary mask \cite{llull2013coded} is used in our simulation. We provide the results on CAVE \verb+Toy+, which is obtained from \cite{Liu19DeSCI}. We compare several recent methods, including DPHSIR \cite{lai2022deep}, SCI-TV-FFDNet \cite{qiu2021effective}, DeSCI \cite{Liu19DeSCI}, and traditional methods, \ie, 2DTV and 3DTV. The quantitative results are shown in Tab.~\textcolor{linkcolor}{1}\subref{tab:cs}. It can be seen that our method obtains the best performance with over 1 dB improvement on PSNR. Specifically, the improvement is purely obtained through the superior denoising ability of our model, which means our model can also be integrated into other more advanced PnP methods for further improvement, \eg, \cite{qiu2021effective}. 



\vspace{-3mm}
\paragraph{Super-Resolution.} We also provide results on the task of HSI super-resolution. Following \cite{lai2022deep}, we first blur the high-resolution HSI via an $8\times 8$ Gaussian blur kernel with $\sigma=3$, and then downsample the image to obtain the low-resolution HSI. We provide the results on ICVL with a scale factor of 2 and 4. The competing methods include several recently developed methods, \eg, SSPSR \cite{jiang2020learning}, Bi3DQRNN \cite{fu2021biqrnn3d}, and DPHSIR \cite{lai2022deep} . As shown in Tab.~\textcolor{linkcolor}{1}\subref{tab:sr}, our method achieves the best performance. In particular, our method only needs the pretrained Gaussian denoising model, which is the same as \cite{lai2022deep}. The improvement against \cite{lai2022deep} comes from the better PnP denoising prior, which further demonstrates the stronger denoising ability of our method.



\section{More Implementation Details}  \label{sec:impl}



\begin{table*}[t]
\begin{center}
   \begin{subtable}[h]{0.56\textwidth}
      \setlength{\tabcolsep}{0.1cm}
       \centering
       \small
      \begin{tabular}{|l|cccccc|}
\hline
\rowcolor{graycolor} Stage 1 & \multicolumn{6}{c|}{Gaussian Noise $\sigma=50$}                \\ \hline
Epoch & 0 - 20  & 20 - 30 &        &         &         &       \\
LR & $1\times 10^{-3}$     &  $1\times 10^{-4}$     &        &         &         &       \\ \hline
\rowcolor{graycolor}Stage 2  & \multicolumn{6}{c|}{Gaussian Noise $\sigma=10,30,50,70$}                \\ \hline
Epoch & 30 - 45 & 45 - 55 & 55 - 60  & 60 - 65   & 65 - 75   & 75 - 80 \\
LR & $1\times 10^{-3}$     &  $1\times 10^{-4}$     &  $5\times 10^{-5}$      &         $1\times 10^{-5}$ & $5\times 10^{-6}$        & $1\times 10^{-6}$      \\ \hline
\rowcolor{graycolor}Stage 3 & \multicolumn{6}{c|}{Complex Noise}                 \\ \hline
Epoch & 80 - 90 & 90 - 95 & 95 - 100 & 100 - 105 & 105 - 110 &       \\
LR &   $1\times 10^{-3}$   & $5\times 10^{-4}$      & $1\times 10^{-4}$          & $5\times 10^{-5}$        & $1\times 10^{-5}$   &   \\ \hline
\end{tabular}
      \caption{Our multi-step learning rate scheduler.}
      \label{tab:lr}
   \end{subtable}
   \hfill
   \begin{subtable}[h]{0.4\textwidth}
   \setlength{\tabcolsep}{0.21cm}
       \centering
       \small
       \begin{tabular}{|>{\columncolor{graycolor}}l|l|}
\hline
System     &  Ubuntu 20.04.1 LTS \\ \hline
GPU        &  Nvidia GeForce RTX 3090 \\ \hline
CPU        &  Intel(R) Core(TM) i9-10850K CPU \\ \hline
Framework  &  PyTorch 1.7.1 \\ \hline
Driver  &  Cuda 11.2 \\ \hline
Software   &  Matlab 2020 \\ \hline
Dataset    &  ICVL  \\ \hline
Image Size &  $512 \times 512$ \\ \hline
Repeat times &  10 \\ \hline
\end{tabular}
      \caption{System configuration for the speed test.}
      \label{tab:system}
   \end{subtable}
\end{center}
\vspace{-4mm}
\caption{More implementation details. (a) We adopt a multi-stage training strategy with the learning warmup setup for the first epoch. (b) We provide the system configuration as the results of the speed test are strongly correlated with the configuration. }
\end{table*}

\paragraph{Setup of the Learning Rate.} In this part, we provide more details about the multi-step learning rate scheduler that we used for training our simulated Gaussian and complex denoising models. Specifically, we use a multi-stage training strategy to train the models for Gaussian noise and complex noise. The learning rate is set up as shown in Tab.~\textcolor{linkcolor}{3}\subref{tab:lr}. We use learning rate warmup to gradually increase the learning rate from 0 to $1\times 10^{-3}$ for the first epoch of the second stage.


\vspace{-4mm}
\paragraph{Details of the Simulated Complex Noise.} 

We follow \cite{wei20203} for constructing simulated complex noise. In details, we consider the non-independent and non-identically distributed (non-i.i.d) Gaussian noise, stripe noise, deadline noise, impulse noise, and the combination of the aforementioned noise (denoted as mixture noise). The details about these five cases of noise are listed as follows,
\begin{itemize}[noitemsep, leftmargin=*]
	\item \textbf{Non-i.i.d noise}. The non-independent and non-identically distributed Gaussian is added to every pixel of each HSI. The noise strength is randomly selected from 10, 30, 50, and 70.
	\item \textbf{Stripe noise}. Stripe noise (5\% to 15\% percentages of columns) is added to randomly selected one-third of bands. Non-i.i.d. Gaussian noise is added to All bands. 
	\item \textbf{Deadline noise}. Deadline noise is added to randomly selected one-third of bands. Non-i.i.d. Gaussian noise is added to All bands.  
	\item \textbf{Impulse noise}. Impulse noise with intensity ranging from 10\% to 70\% is added to randomly selected one-third of bands. Non-i.i.d. Gaussian noise is added to All bands. 
	\item \textbf{Mixture noise}. Each band is randomly corrupted by at least one kind of noise mentioned above.
\end{itemize}


\vspace{-4mm}
\paragraph{System Configuration.} In the main paper, we compare the running time of different methods. All the comparisons are performed with an Nvidia GeForce RTX 3090, and an Intel(R) Core(TM) i9-10850K CPU @ 3.60GHz on Ubuntu 20.04.1 LTS. All the CNN-based methods are implemented and tested with PyTorch 1.7.1. All the optimization-based methods are implemented and tested with Matlab. We test the running time on ICVL with an image size of $512 \times 512$ by repeating the test 10 times and averaging the results.










\section{More Visual Results}  \label{sec:visual}

\paragraph{Simulated Gaussian Noise.} In the main paper, we provide a visual comparison of our method against the competing ones under 50 noise strength. Here, we provide more visual results with other noise strengths used in our experiments. The results are shown in \cref{fig:gaussian30,fig:gaussian50,fig:gaussian70,fig:gaussianblind}.


\vspace{-4mm}
\paragraph{Simulated Complex Noise.} For complex noise, we provide the visual results under stripe noise in the main paper. Here, we provide additional results on other types of complex noise, \ie, non-i.i.d, deadline, impulse, and mixture. The results are shown in \cref{fig:complex_noniid,fig:complex_stripe,fig:complex_deadline,fig:complex_impulse,fig:complex_mixture}.


\vspace{-4mm}
\paragraph{Real-world Noise.} More visual results on the RealHSI dataset are provided in \figref{fig:real}.


\section{Future work.}   \label{sec:future}

In this work, we propose a transformer architecture, \ie, HSDT for hyperspectral image denoising. We introduce several effective and generalizable components to better explore the spatial-spectral and global spectral correlations of HSI. Specifically, it is worthwhile to explore the applications of the proposed S3Conv and HSDT for more network architectures and tasks. Furthermore, our learnable queries could also be extended to condition on some external information for more explicit guidance. For example, we might be able to inject the Gaussian noise strength into the network with learnable queries, through an embedding layer. This is helpful for a PnP Gaussian denoiser, where the noise strength is known. 




\section{Broader Impacts}  \label{sec:impact}
Our work has no ethical issues or broader impacts.

\input{figures/suppl/gaussian30.tex}
\input{figures/suppl/gaussian50.tex}
\input{figures/suppl/gaussian70.tex}
\input{figures/suppl/gaussian_blind.tex}
\input{figures/suppl/complex_noniid.tex}
\input{figures/suppl/complex_stripe.tex}
\input{figures/suppl/complex_deadline.tex}
\input{figures/suppl/complex_impulse.tex}
\input{figures/suppl/complex_mixture.tex}
\input{figures/real2.tex}

