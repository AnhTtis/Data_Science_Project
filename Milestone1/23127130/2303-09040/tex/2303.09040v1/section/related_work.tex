\section{Related Works}


\subsection{Hyperspectral Image Denoising} 


Traditional approaches for HSI denoising usually formulate the task as an optimization problem, which is solved by imposing different types of handcrafted regularizations \cite{yuan2012hyperspectral,othman2006noise,sun2017hyperspectral,maggioni2012nonlocal}.  
Among these optimization-based methods, non-local similarity \cite{maggioni2012nonlocal} has been widely utilized for its ability to integrate the image patches across the spectral and spatial locations. 
To reduce the computational burden, global spectral low-rank correlation \cite{wei2019low,sun2017hyperspectral,zhao2020fast} has also been heavily studied. 
Besides, different enhanced total variation priors \cite{pengE3DTV,wang2017hyperspectral,yuan2012hyperspectral} are also adopted by considering the smoothness of local image patches. 
Though these methods could achieve favorable performance, most of them are computationally inefficient and can only address the noise satisfying the required assumptions, \eg, Gaussian noise. 

Meanwhile, recent works \cite{yuan2018hyperspectral,wei20203,lai2022deep} tend to exploit deep learning to learn denoising mapping purely in a data-driven manner. 
For most of these methods, the encoder-decoder U-Net \cite{ronneberger2015u, dong2019deep,lai2022deep,wei20203} architecture is the prominent choice due to its effectiveness for retaining both high- and low-level multi-scale representations. 
Residual learning \cite{he2016deep} is also widely adopted to reduce learning difficulties from different perspectives, \eg, residual image \cite{chang2018hsi} and residual features \cite{yuan2018hyperspectral,lai2022deep}. 
To consider the properties of HSIs, \eg, spatial-spectral correlations, QRNN3D \cite{wei20203} proposes to use 3D convolution and quasi-recurrent unit \cite{bradbury2016quasi}.  Our work adopts techniques, residual learning, 3D convolution, and U-shape architecture, but our blocks, \eg, S3Conv is more efficient than 3D convolution, and our GSSA could prevent vanished correlations for long-range spectral bands of QRU \cite{wei2019low}.
 Separable convolution \cite{howard2017mobilenets} is first introduced to replace 2D convolution. For HSI denoising, it is also adopted in \cite{imamura2019zero, dong2019deep, he2022spectrum} to reduce computational burden with similar motivations as ours. As a new alternative, our S3Conv is separable 3D instead of 2D convolution \cite{imamura2019zero} and more effective than previous 3D variant \cite{dong2019deep}.
  
   
\subsection{Vision Transformers} 

Transformer \cite{vaswani2017attention} has been first introduced as a parallel and purely attention-based alternative for recurrent neural networks \cite{chung2014empirical,hochreiter1997long} in the literature of natural language processing. Though it is originally designed for modeling text, recent works such as ViT \cite{dosovitskiy2020image} and DeiT \cite{touvron2021training}, have successfully transferred the transformer for high-level vision tasks.
 Recognizing the powerful representation abilities, this architecture is also expeditiously adapted for low-level tasks \cite{liang2021swinir, chen2021pre}, such as natural image denoising. 
 Among these methods, one of the key problems they attempt to overcome is the quadratic complexity of the Self-Attention (SA) mechanism in the transformer. 
 To address it, SwinIR \cite{liang2021swinir} is proposed as an adaption of Swin transformer \cite{liu2021swin} that replaces global attention with a more efficient shift-window-based attention. Similarly, Uformer \cite{wang2022uformer} performs attention over non-overlapped patches and adopts U-Net architecture \cite{ronneberger2015u} to further increase efficiency. From a different perspective, Restormer \cite{zamir2022restormer} explores self-attention along the feature channels to realize the linear complexity. 
Despite their superior performance for various natural image restoration tasks, direct transfer of them for HSI can result in performance degradations since none of them consider the properties of HSI. 
Instead, our HSDT introduce S3Conv and GSSA that can extract more spectral correlated features, which is more suitable for HSI.  

For HSI processing, the applications of transformer previously concentrate more on the classification \cite{hong2021spectralformer,he2021spatial,qing2021improved} and spectral reconstruction \cite{cai2022mask,lin2022coarse}, but also recently extend to the HSI denoising \cite{pang2022trq3dnet, li2022spatial, yu2023dstrans, chen2022hider}. For example, to exploit the spatial attention, TRQ3DNet \cite{pang2022trq3dnet} combines the QRU \cite{wei20203} and Uformer \cite{wang2022uformer} block, while SST \cite{li2022spatial} and  DSTrans \cite{yu2023dstrans} employ the Swin transformer block. To consider spectral correlations, most existing HSI transformers, including SST \cite{li2022spatial}, DSTrans \cite{yu2023dstrans}, and MST \cite{cai2022mask}, utilize a 2D spectral/channel attention similar to Restormer \cite{zamir2022restormer}. Basically, these methods have three major drawbacks including (i) the spatial attention, \eg, Uformer \cite{wang2022uformer} and Swin \cite{liu2021swin}, neither can model spectral relationships nor be computationally friendly for HSI, (ii) spectral attention as Restormer \cite{zamir2022restormer} and MST \cite{cai2022mask} essentially perform attention on feature channel dimension $C$ of 2D data $\mathbf{X} \in \R^{H\times W\times C}$ and $C$ could change for different layers, which makes them no longer spectral attention. (iii) Built based on 2D architectures for color images, these methods are usually not flexible for handle different HSIs in a single model. On the contrary, our GSSA is both effective for capturing global spectral correlations and flexible for diverse HSIs as we consider exact spectral attention along spectral $D$ dimension of 3D data $\mathbf{X} \in \R^{H\times W\times D \times C}$. 
Concurrent to our work, Hider \cite{chen2022hider} also consider 3D spectral attention. Different from it, our GSSA employs a simple pooling strategy instead of conv-reshape strategy as Restormer \cite{zamir2022restormer} to compute the query and key as well as learnable query, which makes it much more effective and efficient. 
Please refer to the supplementary material for more discussions.  

