\begin{figure*}[h]
\centering
\includegraphics[width=1\linewidth]{imgs/arch}
\vspace{-5mm}
\caption{\textbf{Our HSDT architecture.} We adopt a hierarchical multiscale encoder-decoder (a) with each building transformer block stacks (d) a Spectral-Spatial Separable Convolution, (b) a Guided Spectral Self-Attention, and (c) a Self-Modulated Feed-Forward Network sequentially. We append batch normalization after each convolution and predict the residual image. 
}
\label{fig:arch}
\vspace{-2mm}
\end{figure*}

\section{Hybrid Spectral Denoising Transformer}

In this section, we present Hybrid Spectral Denoising Transformer (HSDT), a unified model for hyperspectral image denoising with an arbitrary number of bands. To achieve it effectively, our HSDT introduces several key designs, including (i) a powerful and lightweight spectral-spatial separable convolution as an alternative to 3D convolution, (ii) a guided spectral self-attention piloted by a set of learnable queries, and (iii) a self-modulated feed-forward network with an adaptive self-modulated branch.

The overall architecture of HSDT follows a U-shaped encoder-decoder with skip-connections \cite{ronneberger2015u}, which is depicted in \figref{fig:arch}\textcolor{linkcolor}{(a)}. Such hierarchical multi-scale design not only reduces the computational burden but also increases the receptive fields, which is different from conventional plain transformers \cite{liang2021swinir,dosovitskiy2020image}. In general, HSDT is built by stacking a series of transformer blocks as, 
\begin{align}
    \mathbf{\hat{X}} &= \operatorname{BN}(\operatorname{S3Conv}(\mathbf{X})) \\
    \mathbf{Y} &= \operatorname{SM-FFN}(\operatorname{GSSA}(\mathbf{\hat{X}}) + \mathbf{\hat{X}}) 
\end{align}
where $\mathbf{X} \in \R^{H\times W\times D \times C}, \mathbf{Y} \in \R^{\hat{H}\times \hat{W} \times D \times \hat{C}}$ are the input and output feature maps, $H,W$ denote spatial size, $D$ denotes the number of spectral bands, $C$ denotes the number of feature maps, and $\operatorname{BN}$ denotes batch normalization \cite{ioffe2015batch}. More specifically, given the input noisy HSI, it is first projected into low-level features through a head transformer block and then passed through several transformer blocks to fuse the features along both spatial and spectral dimensions. The residual connection is added to the final output and the input noisy image.  We use trilinear interpolations for upsampling and adopt additive skip connections in all levels of transformer blocks. Next, we illustrate the details of each network component.

\subsection{Spectral-Spatial Separable Convolution}

Modern HSI cameras are capable of capturing images with an exceedingly larger number of spectral bands, \eg, 31 for Specim PS Kappa DX4~\cite{fu2021biqrnn3d}, and 224 for AVIRIS sensor~\cite{kalman1997classification}. However, it is still difficult to collect a large amount of training data for each device due to the complex and time-consuming imaging processes. Hence, it is one of the major concerns for an HSI denoising method if it can handle images with a different number of bands using a single model so that the dataset collected by different devices can be jointly used for more sufficient training. 

To address the issue, the sliding-window denoising strategy \cite{yuan2018hyperspectral} and Conv3D \cite{wei20203} have been adopted to build band-flexible networks. However, their performance and efficiency are still limited by the local receptive field and heavy computation. In this work, we seek to preserve band flexibility and augment our denoising transformer with inductive bias while avoiding splitting HSI into windows. For this, we propose {Spectral-Spatial Separable Convolution} (S3Conv), a more lightweight and powerful variant of Conv3D 
that parallel applies spectral and spatial convolution.   

In detail, standard Conv3D filters spectral-spatial correlated features but introduces a heavy burden on the number of parameters and computation.
To alleviate the computational burden, our S3Conv decouples the Conv3D into two parallel branches, which separately process the inputs along the spatial and spectral dimensions, as illustrated in  \figref{fig:arch}\textcolor{linkcolor}{(b)}. The spatial convolution extracts features with 2D filters for each band and the spectral convolution applies $1\times 1$ projection to correlate the spectral information of all bands. To obtain the final features, we combine the output from two branches through element-wise addition. With such factorization, we could reduce the number of parameters of Conv3D by about half while maintaining the performance. When compared with other separable variants \cite{dong2019deep}, our S3Conv design also exhibits superior performance. 


\subsection{Guided Spectral Self-Attention}

Despite the spatial self-attention \cite{liang2021swinir,ronneberger2015u} improves the model performance by considering spatial interaction and non-local similarities, it is computationally demanding and might be difficult to deal with HSIs with a different number of bands. In this work, we propose an efficient Guided Spectral Self-Attention (GSSA) that applies 3D SA along the spectral than spatial nor channel dimensions. Our GSSA is intuitively supported by the spectral correlations of HSI and has linear complexity and long-range relation modeling abilities. This makes our model extremely more powerful at locating the informative regions to assist the denoising than existing spectral integration techniques \cite{wei20203, lai2022deep}.  


\vspace{-4.5mm}
\paragraph{Spectral Self Attention.} GSSA takes 3D feature maps from previous S3Conv, \ie, $\mathbf{X} \in \R^{H \times W \times D \times C}$, and performs 3D attention on $D$ dimension, instead of $C$ dimension of $\mathbf{X} \in \R^{H \times W \times C}$ of previous 2D spectral attention \cite{cai2022mask,li2022spatial}. To perform attention, we first convert it into query, key, and value. Unlike conventional attention block \cite{vaswani2017attention}, only value  $\mathbf{\hat{V}} \in \R^{H \times W \times D \times C}$ is linearly projected from $\mathbf{X}$ in GSSA. Linear projections for query and key are not necessary according to our experiments, so we omit it for simplicity. Instead, we directly perform the global average pooling on input $\mathbf{X}$ along the spatial dimensions to obtain the global features of each band, \ie, $\mathbf{Q},\mathbf{K} \in \R^{D \times C}$. 
Then, the transposed attention map $\mathbf{A}$ in the shape of $\R^{D\times D}$ is obtained via dot-product between key $\mathbf{K}$ and query $\mathbf{Q}$ with softmax normalization. Finally, we multiply the attention map with the value $\mathbf{\hat{V}}$ to dynamically select the essential features across the spectrum for each band, \ie,
\begin{align}
&\operatorname{Attention}(\mathbf{Q},\mathbf{K}, \mathbf{\hat{V}})=\mathbf{\hat{V}} \cdot \operatorname{Softmax}(\mathbf{K} \cdot \mathbf{Q}) \label{eq:attn} \\
&\hat{\mathbf{X}}=\mathbf{W} \operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{\hat{V}})+\mathbf{X}. \label{eq:attn2}
\end{align}
To better transform the features and stabilize the training, we perform another linear projection $\mathbf{W} \in R^{C\times C}$ on the fused features and learn the residual features as Eq.~\eqref{eq:attn2}. 

\begin{figure}
   \centering
   \includegraphics[width=1\linewidth]{imgs/learnable_query/learnable_query5}
    \begin{subfigure}[b]{0.135\textheight}
         \caption{SNR of each band}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.13\textwidth}
         \caption{Attn w/ LQ}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.13\textwidth}
         \caption{Attn w/o LQ}
     \end{subfigure}
   \vspace{-4mm}
   \caption{The Learnable Query (LQ) guides the model to pay attention (Attn) to more informative bands with higher signal-to-noise ratio (SNR). 
   }
   \label{fig:learnable_query}
   \vspace{-2mm}
\end{figure}

\vspace{-4.5mm}
\paragraph{Learnable Query.} Different spectral bands of HSIs often exhibit some fixed relative relationships due to the physical spectral constraints. Inspired by this, we investigate the global spectral feature maps of different HSIs, as shown in \figref{fig:stat}. Perhaps surprisingly, we found that clean and noisy HSIs exhibit clearly different shared patterns, which indicates the possibilities to utilize them for identifying more useful bands in GSSA. 
To achieve it, we propose to model these global statistics of each band with a set of learnable queries, as shown in \figref{fig:arch}\textcolor{linkcolor}{(c)}.
We perform cross-attention (CA) between the pooled keys and learnable queries to filter out most discriminative features for each band. 
The learnable queries are jointly trained with the other part of the model and not restricted to the input feature maps so that they can better fit the statistics of clean HSIs. As a results, the attention with learnable queries produce a remarkably better attention map with clear interpretability that identifies more informative bands with higher SNR to assist the denoising of more noisy ones, as shown in \figref{fig:learnable_query}. 

\vspace{-4.5mm}
\paragraph{Alternative Training Strategy.} 
The learnable queries are very useful to generate more discriminative attention, but the number of queries has to be predefined to the number of bands of the training dataset. It would disable the model from handling HSIs with a different number of bands simultaneously. To address the issue, we propose an alternative training strategy, in which we randomly switch between SA and CA with learnable queries during the training. Since the pooling operation in SA also captures global information to some extent, the proposed learnable query with this strategy can then be viewed as guidance leading the training of self-attention on more descriptive bands, which consequently leads to better performance.





\subsection{Self-Modulated Feed-Forward Network} 

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.108\textheight}
         \centering
         \includegraphics[width=\textwidth]{imgs/ffn/nachal_0823-1147}
         \caption{Noisy HSI}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.108\textheight}
         \centering
         \includegraphics[width=\textwidth]{imgs/ffn/feat}
         \caption{Feature map}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.13\textheight}
         \centering
         \includegraphics[width=\textwidth]{imgs/ffn/wbar}
         \caption{Modulation Weight}
     \end{subfigure}
      \vspace{-2mm}
   \caption{Our Self-Modulated FFN amplifies the features in high-information-density regions, \eg, edges, with an element-wise modulation weight. 
   }
   \label{fig:smffn}
   \vspace{-2mm}
\end{figure}

Feed-Forward Network (FFN) is one of the most essential parts of transformer architectures, and it has been reported that it might be the key to construct the meta structure of transformer than SA \cite{yu2022metaformer}. Traditional FFN \cite{vaswani2017attention} processes the output features from the SA layer with two linear projections and a non-linear activation between them. 

In this work, we propose Self-Modulated FFN (SM-FFN) with a fundamental augmentation of the vanilla FFN using self-modulation. As shown in \figref{fig:arch}\textcolor{linkcolor}{(d)}, we first expand the input features channels $\mathbf{X} \in \R^{H\times W\times D \times C}$ with a scale factor of 2 through a linear projection, $\mathbf{Y}=\mathbf{W_3}\mathbf{X}, \mathbf{W_3} \in \R^{C\times 2C}$, then we split (also known as \emph{chunk} operation) the expanded features $\mathbf{Y}$ into two parts $\mathbf{F}, \mathbf{W} \in \R^{H\times W\times D \times C}$. We treat one part $\mathbf{F}$ as the candidate features and another part $\mathbf{W}$ after the sigmoid normalization as an element-wise modulation weight.
 With the vanilla FFN branch, our SM-FFN can be described as,
\begin{equation}
\operatorname{SM-Branch}(\mathbf{X}) = \mathbf{F} \odot \operatorname{Sigmoid}(\mathbf{W})  \label{eq:ffn},
\end{equation}
\begin{equation}
\operatorname{SM-FFN}(\mathbf{X}) = \mathbf{W_1}(\operatorname{GELU}(\mathbf{W_2} \mathbf{X})) + \operatorname{SM-Branch}(\mathbf{X}), \label{eq-sm2ffn}    
\end{equation}
where $\mathbf{W_2} \in \R^{C\times 2C}, \mathbf{W_1} \in \R^{2C\times C}$.
Intuitively, our SM branch is designed and works as a soft max-pooling that amplifies the activation of regions with higher information density. As reflected in \figref{fig:smffn}, this makes our network more robust by emphasizing the features in regions that are more important for improving the reconstruction quality, \eg, edges and corners.



