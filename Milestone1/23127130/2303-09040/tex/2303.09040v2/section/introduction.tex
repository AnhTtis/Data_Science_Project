
\section{Introduction}

Hyperspectral image (HSI) provides substantially more abundant spectral information than the ordinary color image, which makes it especially utilitarian in the field of remote sensing \cite{bioucas2013hyperspectral, blackburn2007hyperspectral}, biometric authentication \cite{uzair2015hyperspectral}, detection \cite{mehta2021dark}, and geological science \cite{ellis2001searching,smith2006introduction}. Nevertheless, limited by imaging techniques, most existing HSI cameras still suffer from various types of noise that might degrade the performance of their applications, which urges the development of robust HSI denoising algorithms. 

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{imgs/teaser}
\vspace{-6mm}
\caption{Our method achieves state-of-the-art performance while maintaining low computational overhead.}
\label{fig:flops}
\vspace{-2mm}
\end{figure}


Motivated by the intrinsic properties of HSI, traditional HSI denoising approaches \cite{yuan2012hyperspectral, he2019non} often exploit the optimization schemes with priors, \eg, low rankness \cite{zhao2020fast,wei2019low}, non-local similarities \cite{maggioni2012nonlocal,mei2021image}, spatial-spectral correlation \cite{peng2014decomposable}, and global correlation along the spectrum \cite{wei20203}.
Whilst offering appreciable performance, the efficacy of these methods is largely dependent on the degree of similarity between the handcrafted priors and the real-world noise model, and these methods are often challenging to accelerate with modern hardwares due to the complex processing pipelines.
Recent HSI denoising methods based on Convolutional Neural Network (CNN) \cite{yuan2018hyperspectral, lai2022deep, bodrito2021trainable} get rid of handcrafted regularizations with learning-based prior and often run faster with graphic accelerators and machine-learning frameworks \cite{paszke2019pytorch}. However, these methods are still insufficient for exploring the characteristics of HSI, \eg, {global and local spectral-spatial correlations}. For example, HSID-CNN \cite{yuan2018hyperspectral} only considers the correlations between several adjacent spectral bands. QRNN3D \cite{wei20203} and GRUNet \cite{lai2022deep} model the global spectral correlations with quasi-recurrent units \cite{bradbury2016quasi} but suffer from the problem of vanished correlations for long-range separate bands due to the recurrent multiplications of merging weights. 
Besides, recent methods \cite{wei20203, zhang2021hyperspectral} tend to use 3D convolution to explore the local spectral-spatial correlations while maintaining the flexibility to handle different HSIs. 
This strategy, however, introduces substantially unwanted computation and parameters.  




Starting from natural language processing \cite{vaswani2017attention}, transformer architectures \cite{dosovitskiy2020image,bao2021beit} have recently been applied to various vision tasks including color image restoration \cite{liang2021swinir,zamir2022restormer} and HSI processing \cite{li2022spatial,cai2022mask,pang2022trq3dnet}. With the multi-head self-attention of transformer, these methods enjoy stronger capabilities of capturing non-local similarity and long-range dependency over aforementioned CNN-based methods. Despite of that, they are still suboptimal and inflexible for diverse HSIs. 
On the one hand, existing attentions for HSIs apply along either spatial \cite{li2022spatial,pang2022trq3dnet} or 2D feature channel \cite{li2022spatial,cai2022mask} dimensions, which could introduce quadratic complexities or break down the structured spectral dependency. 
On the other hand, their 2D architectural designs also make their models specifically bound to one type of HSI, \eg, HSI with 31 bands, and separate models have to be trained for other types, \eg, HSI with 210 bands. This can be problematic since the amount of available datasets is unevenly distributed for different HSIs.  
Finally, HSIs often exhibit beneficial fixed structures, \eg, relative intensity correlations of different bands for objects. Direct transfer of existing transformer blocks without considering this fact might lead to suboptimal performance for HSI denoising.


In this paper, we propose a novel Hybrid Spectral Denoising Transformer (HSDT) that effectively integrates the local spectral-spatial inductive bias of the convolution and the long-range spectral dependency modeling ability of the transformer. Unlike previous HSI transformers \cite{li2022spatial, cai2022mask, pang2022trq3dnet}, HSDT is designed to be effective and flexible for handling diverse HSIs in a single model, which results in a variety of benefits, \eg, the ability to jointly utilize HSIs with different numbers of bands for more sufficient training in data-insufficiency scenarios. 
To achieve it, (a) we first introduce a parallel Spectral-Spatial Separable Convolution (S3Conv) unit that efficiently extracts more spatial-spectral meaningful features than previous 3D \cite{wei20203} and separable convolutions \cite{dong2019deep}.  
(b) With the stronger local spectral-spatial inductive bias, the extracted features are then processed by a newly proposed Guided Spectral Self-Attention (GSSA) that performs the global self-attention along the 3D spectral rather than spatial \cite{pang2022trq3dnet} or 2D spectral/channel dimensions \cite{li2022spatial,cai2022mask} to selectively aggregate the information across different bands. This not only enriches our model with powerful capabilities for identifying long-range spectral correlations but also makes our model free of inflexibility of 2D spectral SA for dealing with different HSIs, quadratic complexity of spatial SA \cite{dosovitskiy2020image}, the issue of vanished long-range dependency of QRNN \cite{wei20203}. 
(c) Besides, inspired the relatively stationary global patterns and statistics of features of different HSI bands, as shown in \figref{fig:stat}, we propose to enhance our GSSA with a set of learnable queries that encode the global spectral statistics for each band. We alternatively switch between self-attention among spectral bands and cross-attention between spectral bands and learnable queries during the training, so that we can guide the GSSA to pay attention to features that are more discriminative and beneficial for denoising while keeping the flexibility. 
(d) Moreover, we propose a Self-Modulated Feed-Forward Network (SM-FFN) with a novel SM-branch to further strengthen the aggregated features of more informative regions. 
Extensive experiments on various datasets under different noise show that our HSDT consistently outperforms the existing state-of-the-art (SOTA) methods while maintaining low computational overhead, as shown in \figref{fig:flops}. 

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{imgs/stat}
   \caption{Visualization of feature maps of different bands by performing global average pooling on spatial locations. It can be observed that despite the difference of images, the pooled feature maps of clean and noisy images share different common patterns, which might be helpful for identifying, \eg, cleaner bands, for denoising noisy bands.}
\label{fig:stat}
\vspace{-2mm}
\end{figure}


In summary, our contributions are that, 
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
    \item We present HSDT, a 3D hybrid spectral denoising transformer that effectively captures the local spatial-spectral features and long-range global spectral correlations. 
    \item We introduce GSSA guided by a set of learnable queries that encode the global statistics of HSIs, which models long-range spectral correlations along 3D spectrum instead of previous 2D spectral/channel dimensions. 
    \item We propose SM-FFN with a novel self-modulated branch for driving the model to pay attention to more informative regions, along with a S3Conv for extracting spatial-spectral meaningful features.
\end{itemize}
