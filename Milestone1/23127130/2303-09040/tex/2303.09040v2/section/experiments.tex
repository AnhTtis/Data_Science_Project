

\section{Experiments}


\subsection{Experimental Setup}

\input{figures/mixture_0308}

\input{tables/complex.tex}

\paragraph{Datasets.} We use three public HSI datasets with natural scenes, \ie, ICVL \cite{arad2016sparse}, CAVE \cite{CAVE_0293}, RealHSI \cite{zhang2021hyperspectral}, and one remotely sensed dataset Urban \cite{kalman1997classification}. ICVL and CAVE are unpaired datasets with solely clean images. RealHSI and Urban are paired and unpaired datasets with real-world noise. 
ICVL and CAVE share the same spectral resolution of 31, while RealHSI and Urban have different spectral resolutions of 34 and 210. We train and test the models on ICVL under simulated noise. For other datasets, we test the zero-shot performance with the models trained on ICVL. We use 100 images from ICVL for training, and we randomly select the main $512\times 512$ regions of 50 images from ICVL, and the entire regions of 12, 15 and 1 images from CAVE, RealHSI, and Urban for testing. 

\vspace{-4mm}
\paragraph{Noise Settings.} We consider simulated Gaussian and complex noise as well as real-world noise provided by RealHSI \cite{zhang2021hyperspectral} and Urban \cite{kalman1997classification}. The Gaussian noise is sampled from zero-mean i.i.d Gaussian distribution with different variances and we evaluate different methods on 30, 50, 70, and blind (range from 10 to 70) noise strengths. Following \cite{wei20203}, the complex noise is composed of non-i.i.d Gaussian noise and one or several types of complex noise, including, stripe, deadline, and impulse noise. 

\vspace{-4mm}
\paragraph{Implementation Details.} 
We implement the proposed method with PyTorch \cite{paszke2019pytorch}. 
The network is trained by optimizing the mean-square-root error between predicted images and the ground truth. We adopt Adam \cite{kingma2014adam} optimizer with a multi-step learning scheduler whose initial learning rate is set to $1\times 10^{-3}$ and decayed by a factor when it reaches the predefined milestones. Following \cite{wei20203}, we use a multi-stage training strategy to train models for Gaussian and complex noise. 
The learning rate warmup is used between training stages. 
The batch size and training patch size are set to 16 and $64\times 64$.
 All our models for Gaussian noise are trained for 80 epochs, and the models for complex noise are obtained by another 30 epochs of fine-tuning. 
We use pretrained complex denoising models for real noise.


\subsection{Main Results}
\noindent{\textbf{Gaussian Denoising.}}
We evaluate our method against SOTA optimization-based methods (\ie LLRT \cite{chang2017hyper}, KBR \cite{Qi2017Kronecker}, WLRTR \cite{chang2020weighted}, and NGmeet \cite{he2019non}), and deep-learning-based HSI denoising methods (\ie, HSID-CNN \cite{yuan2018hyperspectral}, QRNN3D \cite{wei20203}, GRUNet \cite{lai2022deep}, T3SC \cite{bodrito2021trainable}, TRQ3D \cite{pang2022trq3dnet}), SST \cite{li2022spatial}, and SERT \cite{li2023spectral}. The SOTA RGB denoising transformer, \ie, Restormer \cite{zamir2022restormer}, is also included by adjusting the input and output channels and training with the same HSI dataset as ours. 
As the quantitative results provided in \tabref{tab:denoise-gaussian}. we can observe that our method achieves superior performance outperforming the other ones by over 1 dB improvement on PSNR. Besides, we achieve the lowest SAM metric, which indicates that our method is better at maintaining spectral consistency. The synthetic color image is given in \figref{fig:gaussian} for the visual comparison. 



\input{figures/remote_real2.tex}





\vspace{-2mm}
\paragraph{Complex Denoising.} 
While Gaussian denoising might be useful for some scenarios, \eg, as a plug-and-play denoiser \cite{lai2022deep}, it is not common for real-world images. We, therefore, also compare our method with several recently developed methods on simulated complex noise. 
As most optimization-based methods only perform well on the noise settings they can solve, we compare our method with a different set of optimization-based methods including, LRTV \cite{he2015total}, NMoG \cite{chen2017denoising} and TDTV \cite{wang2017hyperspectral}, in addition with the same set of deep-learning-based methods as Gaussian denoising.
As shown in \tabref{tab:denoise-complex}, our method again achieves the best performance with up to 1.7 dB improvement on PSNR. It demonstrates the stronger modeling ability of our method. The visual comparison is given in \figref{fig:complex}. 
 Similar to the results of Gaussian noise, our reconstruct better images. 



\vspace{-2mm}
\paragraph{Real World Denoising.} 
We also conduct experiments on the recently developed RealHSI dataset \cite{zhang2021hyperspectral} using the models trained on ICVL. We compare our methods with the model in \cite{zhang2021hyperspectral} and several leading competing methods of complex denoising. The methods that cannot generalize to HSIs with 34 bands, \ie, T3SC \cite{bodrito2021trainable} and Restormer \cite{zamir2022restormer}, SST \cite{li2022spatial}, SERT \cite{li2023spectral}, TRQ3D \cite{pang2022trq3dnet}, are not included. 
The quantitative results are given in  \tabref{tab:real}. It can be seen that our method achieves better performance with fewer parameters.
The visual results are provided in the bottom row of \figref{fig:real}. It can be observed that our method produces cleaner and sharper results while the others are blurry or could not completely remove the noise. 

\input{tables/additional3.tex}


\subsection{Generalization to Other Datasets}

We have demonstrated the stronger generalization abilities of our method on real-world denoising dataset \cite{zhang2021hyperspectral} with models trained purely on ICVL and simulated noise. With the same models, we provide results on more datasets with different scenes and the number of bands. 

\vspace{-2mm}
\paragraph{Natural HSI.} The CAVE \cite{CAVE_0293} is another widely used HSI denoising dataset that contains more indoor scenes, \eg, different materials and objects. We evaluate the performance under mixture complex noise with the models trained on ICVL. As shown in \tabref{tab:real}, our model still achieves the best performance against the others with a large margin, which proves the stronger generalization capabilities of our method to handle out-of-distribution images.

\vspace{-2mm}
\paragraph{Remotely Sensed HSI.} We demonstrate that our model trained on ICVL with 31 bands can be directly transferred to datasets with a totally different number of bands, \eg, Urban with 210 bands, without any fine-tuning. This is largely supported by the flexibility of the proposed GSSA and S3Conv units, while such flexibilities are not presented in models, \eg, Restormer \cite{zamir2022restormer} and TRQ3D \cite{pang2022trq3dnet}. We present the visual comparison in \figref{fig:real}. It can be observed that QRNN3D \cite{wei20203} could not completely remove the row noise and lose the details of the roof. HSID-CNN \cite{yuan2018hyperspectral} and TDTV \cite{he2015total} eliminate the most noise but produce more blurry results. Our method instead removes most noise while maintaining the sharper details. 

\input{tables/ablation_breakdown2.tex}


\subsection{Ablation Studies}

We adopt Gaussian denoising ($\sigma=50$) on ICVL to conduct the ablation studies. The baseline model is derived by removing our S3Conv, GSSA, and SM-FNN from HSDT-M. We also conduct the per-component ablation studies, which are provided in the supplementary material.

\vspace{-4mm}
\paragraph{Break-down Ablation.} 
We provide the results of break-down ablations in \tabref{tab:ablation-step}, in which we gradually add the proposed blocks back to the baseline model. It can be seen that GSSA provides the most performance gain, which can be attributed to the importance of spectral correlations for HSI denoising. The LQ guidance and SM-FFN improve the models with negligible parameter growth, while S3Conv improves the performance with even fewer parameters. 
