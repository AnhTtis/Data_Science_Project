%
% IEEE Transactions on Microwave Theory and Techniques example
% Tibault Reveyrand - http://www.microwave.fr
%
% http://www.microwave.fr/LaTeX.html
% ---------------------------------------


% ================================================
% Please HIGHLIGHT the new inputs such like this :
% Text :
%  \hl{comment}
% Aligned Eq. 
% \begin{shaded}
	% \end{shaded}
% ================================================


\documentclass[journal]{IEEEtran}

\usepackage[numbers]{natbib}

%\usepackage[retainorgcmds]{IEEEtrantools}
%\usepackage{bibentry}  
\usepackage{xcolor,soul,framed} %,caption

\colorlet{shadecolor}{yellow}
% \usepackage{color,soul}
\usepackage[pdftex]{graphicx}
\graphicspath{{../pdf/}{../jpeg/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}


\usepackage[cmex10]{amsmath}
%Mathabx do not work on ScribTex => Removed
%\usepackage{mathabx}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{cleveref}
\usepackage{subfig}
\usepackage[scr=rsfs]{mathalpha}
\usepackage{makecell}
\usepackage{float} 
% acronyms
\usepackage[printonlyused]{acronym}

\usepackage{amsthm}


\newtheorem{theorem}{Theorem}
%\setmathfont{XITS Math}
%\setmathfont[version=setB,StylisticSet=1]{XITS Math}

% Adjusting cleveref package
\crefalias{subequation}{equation} 
\crefalias{eqnarray}{equation} 
\crefformat{pluraleq}{(#2#1#3)} 

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\usepackage{bm}
\usepackage[ruled]{algorithm2e} 
\newcommand{\sibo}{\textcolor{black}}
%\newcommand{\bmu}{\boldsymbol \mu}
%\newcommand{\br}{\boldsymbol r}

\newcommand{\xb}{\boldsymbol{x}_b}
\newcommand{\xa}{\boldsymbol{x}_a}
\newcommand{\xt}{\boldsymbol{x}_t}

\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\bc}{\boldsymbol{c}}
\newcommand{\bd}{\boldsymbol{d}}
\newcommand{\be}{\boldsymbol{e}}
\newcommand{\bg}{\boldsymbol{g}}
\newcommand{\bh}{\boldsymbol{h}}
\newcommand{\bi}{\boldsymbol{i}}
\newcommand{\bj}{\boldsymbol{j}}
\newcommand{\bk}{\boldsymbol{k}}
\newcommand{\bl}{\boldsymbol{l}}
\newcommand{\bmm}{\boldsymbol{m}}
\newcommand{\bn}{\boldsymbol{n}}
\newcommand{\bo}{\boldsymbol{o}}
\newcommand{\bp}{\boldsymbol{p}}
\newcommand{\bq}{\boldsymbol{q}}
\newcommand{\br}{\boldsymbol{r}}
\newcommand{\bs}{\boldsymbol{s}}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bz}{\boldsymbol{z}}

\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\bB}{\boldsymbol{B}}
\newcommand{\bC}{\boldsymbol{C}}
\newcommand{\bD}{\boldsymbol{D}}
\newcommand{\bE}{\boldsymbol{E}}
\newcommand{\bF}{\boldsymbol{F}}
\newcommand{\bG}{\boldsymbol{G}}
\newcommand{\bH}{\boldsymbol{H}}
\newcommand{\bI}{\boldsymbol{I}}
\newcommand{\bJ}{\boldsymbol{J}}
\newcommand{\bK}{\boldsymbol{K}}
\newcommand{\bL}{\boldsymbol{L}}
\newcommand{\bM}{\boldsymbol{M}}
\newcommand{\bN}{\boldsymbol{N}}
\newcommand{\bO}{\boldsymbol{O}}
\newcommand{\bP}{\boldsymbol{P}}
\newcommand{\bQ}{\boldsymbol{Q}}
\newcommand{\bR}{\boldsymbol{R}}
\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bT}{\boldsymbol{T}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bV}{\boldsymbol{V}}
\newcommand{\bW}{\boldsymbol{W}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bZ}{\boldsymbol{Z}}


\newcommand{\bmu}{\boldsymbol \mu}
\newcommand{\balpha}{\boldsymbol \alpha}



\hyphenation{op-tical net-works semi-conduc-tor}


\usepackage{changes} 
\definechangesauthor[name={Yang}, color=red]{Yang} %修订作者1
\definechangesauthor[name={He}, color=green]{He} %修订作者2


%\bstctlcite{IEEE:BSTcontrol}


%=== TITLE & AUTHORS ====================================================================
\begin{document}
	\bstctlcite{IEEEexample:BSTcontrol}
	%\title{A comprehensive numerical study of PINN on solving neutron diffusion eigenvalue problem}
	\title{On the uncertainty analysis of the data-enabled physics-informed neural network for solving neutron diffusion eigenvalue problem}
	%\title{Solving inverse non intrusive reduced order models for real-time operational digital twin in nuclear engineering}
	\author{Yu Yang, Helin Gong*, Qihong Yang, Yangtao Deng,  Qiaolin He ,Shiquan Zhang}
% 		\thanks{Corresponding author: Helin Gong (gonghelin@sjtu.edu.cn), Shiquan Zhang (shiquanzhang@scu.edu.cn)}
% 		\thanks{Helin Gong is with ParisTech Elite Institute of Technology, Shanghai Jiao Tong University, 200240, Shanghai, China.}
% 		\thanks{Yu Yang, Shiquan Zhang, Qihong Yang and Qiaolin He are with School of Mathematics, Sichuan University, 610065, Chengdu, China.}
% 		\thanks{Zhang Chen and Qing Li are with Science and Technology on Reactor System Design Technology Laboratory, Nuclear Power Institute of China, 610041, Chengdu, China.}
% 	} 
	
	%800 Dongchuan Road, Minhang District, 200240, Shanghai, China
	
	% The paper headers
% 	\markboth{Annals of Nuclear Energy, VOL. XX, NO. XX, XXXX
% 		2022}{Yang {\textit{et al.}}: A comprehensive numerical study of PINN on solving neutron diffusion eigenvalue problem}
	
	% chenzhang208@qq.com
	% liqing_xueshu@163.com
	% shiquanzhang@scu.edu.cn
	% qlhejenny@scu.edu.cn
	% yuyang123@stu.scu.edu.cn 
	% yangqh@stu.scu.edu.cn
	% ====================================================================
	\maketitle
	
	
	
	% === ABSTRACT ====================================================================
	% =================================================================================
	\begin{abstract}
		%\boldmath
		%PINN is a physics informed neural network method which has been widely used in many fields.  
    In practical engineering experiments, the data obtained through detectors are inevitably noisy. For the already proposed data-enabled physics-informed neural network (DEPINN) \citep{DEPINN}, we investigate the performance of DEPINN in calculating the neutron diffusion eigenvalue problem from several perspectives when the prior data contain different scales of noise. Further, in order to reduce the effect of noise and improve the utilization of the noisy prior data, we propose innovative interval loss functions and give some rigorous mathematical proofs. The robustness of DEPINN is examined on two typical benchmark problems through a large number of numerical results, and the effectiveness of the proposed interval loss function is demonstrated by comparison. This paper confirms the feasibility of the improved DEPINN for practical engineering applications in nuclear reactor physics.


	\end{abstract}
	
	
	% === KEYWORDS ====================================================================
	% =================================================================================
	\begin{IEEEkeywords}
		deep learning; eigenvalue problem; PINN; nuclear reactor physics; robust.
		%Autoencoder-based reduced-order model
	\end{IEEEkeywords}
	
	
	% For peer review papers, you can put extra information on the cover
	% page as needed:
	% \ifCLASSOPTIONpeerreview
	% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
	% \fi
	%
	% For peerreview papers, this IEEEtran command inserts a page break and
	% creates the second title. It will be ignored for other modes.
	\IEEEpeerreviewmaketitle
	
	
	% ====================================================================
	% ====================================================================
	% ====================================================================
	
	
	
	
	% @@@@@@@@@@@@@@@@@@@@@@@@@@
	% @@@@@@@@@@@@@@@@@@@@@@@@@@
	\section{Introduction}
	\label{sec:intro}
	
	In the field of nuclear reactor engineering, it is essential to predict how the neutrons will be distributed throughout the reactor core. This is also a highly difficult problem because the neutrons interact differently with different materials in a reactor core. The neutron diffusion theory provides a basis for neutron-physical simulation of nuclear cores, and is widely used in practical applications. There are relatively mature numerical calculation methods for solving the neutron diffusion equations, particularly  neutron diffusion eigenvalue problems (NDEPs) in the industry, mainly including finite difference method, different kinds of node methods and so on \citep{Hebert2009}. 
	
	
	The development of scientific machine learning (SML) provides and offers another way to solve nuclear engineering problems with mesh free, easy implementation properties. 
	%%
	Many related work can be found in thermal-hydraulics domain such as boiling heat transfer \cite{LIU2018305}, turbulence model for reactor transient analysis \cite{liuyang2021ASME}, multiphase-CFD simulations of bubbly flows \cite{LIU2021107636} and critical heat flux prediction \cite{ZHAO2020114540} etc. The developments of SML in nuclear reactor physics domain, solving neutron diffusion equations, are relatively few. The work in \cite{Zhangqian2021} used a convolutional neural networks (CNN) to build a deep learning based surrogate model for estimating the flux and power distribution solved by a diffusion equation. In that work, the flux and power distribution calculated by neutron simulation code are used for training, which means the surrogate model is still industrial-code-dependent. The work \cite{jne2040036} went further, and proposed a physicals-informed neural network based deep learning framework to solve multi-dimensional mono-energetic neutron diffusion problems.
 
	Recently, the work in \citep{DEPINN} explored the possibility of prior data-enabled physics-informed neural networks (PINN) \citep{PINN} in solving NDEPs with numerical test on mono-energetic equations and two-group equations. 
	%%
 
	In this aspect, much more uncertainty analysis numerical study based on complex geometry and two-group diffusion equations - which is widely used in industry - should be brought before the practical application of the learning problem into reactor physics domain.
	%%
	
	
	
	%Quantification of the intrinsic complexity
	
	% @@@@@@@@@@@@@@@@@@@@@@@@@@
	\subsection{The development of scientific machine learning}
	\label{sec:Scientific}
	In recent years, with the rapid improvement of computing resources, artificial intelligence technology represented by deep learning has made major breakthroughs in image processing, natural language processing. However, deep learning still has no mature application scenarios in the field of computational physics and computational mathematics. Using deep learning to solve partial differential equations (PDEs) is still a frontier research hotspot.
	%
	Traditional numerical methods, such as finite difference method and finite element method, have high dependence on grids and may cause curse of dimensionality \citep{DP-COD}, which can be avoided by deep neural network (DNN) \citep{CofD} and automatic differentiation technology \citep{autodiff}. 
	
	
	Up to now, scholars have proposed many methods for deep learning in computational science, such as the strong form of PDEs (\citep{PINN}, \citep{DGM}), the variational form of PDEs (\citep{DeepRitz}), method for stochastic differential equations (SDEs) \citep{DeepBSDE} and operator learning of PDE coefficients (\citep{DeepONet}, \citep{FNO}). We particularly focus on the PINN \citep{PINN} proposed by M. Raissi et al. In many fields of physics and engineering, some prior information is often implied in the PDE to be solved, such as the solution satisfies additional properties in certain computational regions. PINN can train models with prior information which is not considered in any other DNN. Compared with traditional numerical methods, PINN has the advantages of independent of grids, which can be used to solve high-dimensional problems and inverse problems.  Also, it has strong generalization ability.
	
	Once PINN has been proposed, it has got extensive attention from researchers. Scholars have also made a very thorough analysis and improvement on some aspects of PINN. In the aspect of generalization error estimation(\citep{PINNEE1}), they have proved the convergence of the solution obtained by training PINN under certain regularity assumptions. In the aspect of training process (\citep{PINNTA1}, \citep{PINNTA2}, \citep{PINNTA3}), the reasons for the failure of PINN training are analyzed from the aspects of internal weight of loss function, neural tangent kernel, and frequency of solution.
	
	
	In addition, researchers have also done a great deal of contributions on how to use PINN to solve eigenvalue problems.  For example, in \citep{PINN_EP1}, forced boundary conditions and exponential loss function are used to enable the neural network to learn non-trivial solutions and appropriate eigenvalue. In \citep{PINN_EP3}, the minimum eigenvalue is searched by adding the Rayleigh quotient to the loss function, which is advantageous to solve for eigenvalues of high-dimensional PDEs. In \citep{PINN-MDNDE}, a parallel search method for eigenvalues is proposed, and the parameter sensitivity of PINN on single-group problems is explored based on simple one dimension problems. In \citep{PMNN}, the neural network for solving the minimum eigenvalue by combining the power and inverse power methods is advantageous in solving the linear operator problem.
        
    Uncertainty analysis is a very necessary thing if scientific machine learning models are to be used for practical applications, and many researchers have investigated this aspect. In \citep{Robust1}, two methods, Monte Carlo dropout and deep ensemble, are used to deal with the uncertainty quantification problem of complex DNNs, and the computational cost and uncertainty evaluation performance of three uncertainty quantification methods, Bayesian neural network, Monte Carlo dropout and deep ensemble, are comprehensively investigated. The article \citep{Robust2} uses a Gaussian process-based smoothing introduced in this work to improve the performance of PINNs, providing an architecture to counteract the noise that appears in the measurements and reducing the dynamic propagation of errors. In \citep{Robust3}, many classic methods for uncertainty quantification are described, presenting a comprehensive framework that includes uncertainty modeling, new and existing solution methods, and evaluation metrics and post hoc improvement methods. In \citep{Robust4}, a Bayesian physical-informed neural network is proposed to solve forward and backward nonlinear problems described by partial differential equations and noisy data, and a systematic comparison of Hamiltonian Monte Carlo, variance inference, and dropout is presented.
 

 
	% @@@@@@@@@@@@@@@@@@@@@@@@@@
	%\red{\subsection{Related work on nuclear reactor physics}}
	%\label{sec:Related}
	%...\\
	%...
	
	% @@@@@@@@@@@@@@@@@@@@@@@@@@
	\subsection{Contribution of this work}
	\label{sec:Contribution}

     To push further based on the prior work \citep{DEPINN}, the feasibility of DEPINN for practical nuclear engineering problems should be investigated, not just tested on analytical problems. Therefore, at least three questions need to be addressed:
		%%
		\begin{itemize}
			\item How well the DEPINN performanced with noisy prior data.
			\item How to reduce the impact of noise in prior data.
			\item How to design a new loss function for making full use of noisy prior data to learn the physical model.
		\end{itemize}
  
    In this work, we present a systematic analysis of the robustness of DEPINN in solving two benchmark problems in NDEP. Specifically, we test (i) a finite spherical reactor modeled by a one-dimensional monoenergetic diffusion equation, and (ii) a finite cylindrical reactor modeled by a two-dimensional monoenergetic diffusion equation for the following two benchmark problems. In designing the experiments, we consider the effects of noise scale and the number of prior points, and observe the performance of DEPINN from different perspectives such as expectation, confidence interval, and mean square error.
    
    Further, we propose interval loss functions for the noise and give rigorous mathematical proofs. We demonstrate the relationship between the interval loss function and the SSE loss function, and greatly increase the robustness of DEPINN by transforming the original PDE system into an inequality-constrained PDE system. Also, we propose an adaptive weighting method, which is especially feasible when dealing with different scales of noise.

    The rest of the paper is organized as follows. Section II describes the mathematical modeling of the parameterized neutron diffusion eigenvalue problem. In Section III, the DEPINN proposed in the paper and the method proposed in this paper to deal with noise are presented. Section IV illustrates the numerical results applied to the benchmark problem. Finally, Section V concludes and summarizes the paper.

	% @@@@@@@@@@@@@@@@@@@@@@@@@@
	\section{The parametric neutron diffusion eigenvalue problem}
	\label{sec:The parametric neutron diffusion eigenvalue problem}
	
	In the simulation of nuclear reactor core, the neutron flux $\phi$ is usually modeled by one-group steady-state diffusion equation with suitable boundary conditions. The flux is the solution to the following eigenvalue problem. To be precise, the flux $\phi$ satisfies the following eigenvalue problem:
	Find $\left(\lambda,\phi \right)\in \mathbb C\times L^\infty(\Omega)$, s.t.
	\begin{equation}
		\label{eq:diffusion}
		\hspace{-0.3cm}
		\begin{array}{r@{}l}
			\begin{aligned}
				-\nabla \left(D\nabla \phi\right)+\Sigma_{a}  \phi& =
				\lambda \chi \nu\Sigma_f\phi
			\end{aligned}
		\end{array}
	\end{equation}

	with
	\begin{equation}
		\label{eq:PINN_eigenvalue BC}
		\begin{array}{r@{}l}
			\left\{
			\begin{aligned}
				&\frac{\partial \phi(\bx)}{\partial \bn} = 0, \ \forall \bx \in \partial \Omega_{L}, \\
				& \phi(\bx) = g(\bx) ,\  \forall \bx \in \partial \Omega_{R}, \\
			\end{aligned}
			\right.
		\end{array}
	\end{equation}
	where $\partial \Omega = \partial \Omega_{L} \bigcup \partial \Omega_{R}$ and $g(\bx)$ is a given function. The generated nuclear power is $P=\nu\Sigma_{{ f}}\phi.$

	The following parameters are involved in the above equation: $D$ is the diffusion coefficient; $\Sigma_{{ a}}$, $\Sigma_{{f}}$ and $\chi$ are the macroscopic absorption cross section, macroscopic fission cross section and the fission spectrum of group respectively; $\nu$ is the average number of neutrons emitted per fission.

	We make some comments on the coefficients and recall well-posedness results of the eigenvalue problem Eq. \eqref{eq:diffusion}. First of all, the first three coefficients ($D$, $\Sigma_{a}$, and $\Sigma_{f}$) might depend on the spatial variable. In the following, we assume that they are either constant or piecewise constant so that our set of parameters is $\mu=\{ D, \Sigma_{a}, \nu, \Sigma_{f}, \chi \}.$
	
	
	Under some mild conditions on the parameters $\mu$, the maximum eigenvalue $\lambda_{\text{max}}$ is real and strictly positive (see \citep[Chapter XXI]{DLvol6}). 
	The associated eigenfunction $\phi$ is also real and positive at each point $\mathbf x\in\Omega$ and it is the flux of interest. 
	In neutronics, it is customary to use the inverse of $\lambda_{\text{max}}$, that is called the multiplication factor $k_{\text{eff}} := \frac{1}{\lambda_{\text{max}}}$.

	Here, for each setting $\mu$, the parameter $k_{\text{eff}}$ is determined by the solution to the eigenvalue problem Eq. \eqref{eq:diffusion}. 
	
%原第二节	
% 	% @@@@@@@@@@@@@@@@@@@@@@@@@@
% 	\section{The parametric neutron diffusion eigenvalue problem}
% 	\label{sec:The parametric neutron diffusion eigenvalue problem}
	
% 	In the simulation of nuclear reactor core, the neutron flux $\phi=\left( \phi_1,\phi_2 \right)^{T}$ is usually modeled by two-group neutron diffusion equations with suitable boundary conditions. Index 1 and 2 denote the high and thermal energy group respectively.  The flux is the solution to the following eigenvalue problem (see \cite{Hebert2009}). To be precise, the flux $\phi$ satisfies the following eigenvalue problem:
% 	Find $\left(\lambda,\phi \right)\in \mathbb C\times L^\infty(\Omega)\times L^\infty(\Omega)$, s.t.
% 	\begin{small}
% 		\begin{equation}
% 			\label{eq:diffusion}
% 			\hspace{-0.3cm}
% 			\begin{array}{r@{}l}
% 				\left\{
% 				\begin{aligned}
% 					-\nabla \left(D_1\nabla \phi_1\right)+\left(\Sigma_{{ a}, 1} + \Sigma_{ 1\to 2} \right) \phi_1& =
% 					\lambda \chi_1 \left( \nu\Sigma_{{ f}, 1}\phi_1+\nu \Sigma_{{ f}, 2}\phi_2 \right) \\
% 					-\nabla \left(D_2\nabla \phi_2\right)+\Sigma_{{\rm a}, 2}\phi_2 - \Sigma_{ 1\to 2}  \phi_1& =
% 					\lambda \chi_2 \left( \nu\Sigma_{{ f}, 1}\phi_1+ \nu\Sigma_{{ f}, 2}\phi_2 \right)
% 				\end{aligned}
% 				\right.
% 			\end{array}
% 		\end{equation}
% 	\end{small}
% 	%with the zero boundary condition $\phi_1 = \phi_2 = 0$ on $ \partial \Omega$. 
% 	with
% 	\begin{small}
% 		\begin{equation}
% 			\label{eq:PINN_eigenvalue BC}
% 			\begin{array}{r@{}l}
% 				\left\{
% 				\begin{aligned}
% 					&\frac{\partial \phi_h(\bx)}{\partial \bn} = 0, \ \forall \bx \in \partial \Omega_{L}, \\
% 					&\frac{\partial \phi_h(\bx)}{\partial \bn} + \alpha \phi_{h} = g(\bx) ,\  \forall \bx \in \partial \Omega_{R}, \\
% 				\end{aligned}
% 				\right.
% 			\end{array}
% 		\end{equation}
% 	\end{small}
% 	where $h = 1$ or $2$, $\partial \Omega = \partial \Omega_{L} \bigcup \partial \Omega_{R}$, $\alpha$  and $g$ are given parameters. The generated nuclear power is $P=\nu\Sigma_{{ f}, 1}\phi_1+ \nu\Sigma_{{ f}, 2}\phi_2.$
% 	%
% 	%\begin{equation}
% 	%\label{eq:power}
% 	%P=\nu\Sigma_{{ f}, 1}\phi_1+ \nu\Sigma_{{ f}, 2}\phi_2.
% 	%\end{equation}
% 	%\end{small}
% 	%
% 	%
% 	The following parameters are involved in the above equation: $\Sigma_{1\to 2}$ is the macroscopic scattering cross section from group $1$ to $2$; $D_i$ is the diffusion coefficient of group $i$ with $i\in \{1,2\}$;  $\Sigma_{{ a}, i}$, $\Sigma_{{f}, i}$ and $\chi_i$ are the macroscopic absorption cross section, macroscopic fission cross section and the fission spectrum of group $i$ respectively; $\nu$ is the average number of neutrons emitted per fission.
% 	%This problem can be solved by the NESTROR package {\red (Citation)} that is developed by NPIC.
% 	%The axial power distribution of the core can be found in Figure \ref{fig:test3_2} for the R rods moving from all rod clusters out (ARO) to 100 steps inserted.
% 	We make some comments on the coefficients and recall well-posedness results of the eigenvalue problem Eq. \eqref{eq:diffusion}. First of all, the first four coefficients ($D_i$, $\Sigma_{a,i}$, $\Sigma_{s,1\to 2}$ and $\Sigma_{f,i}$) might depend on the spatial variable. In the following, we assume that they are either constant or piecewise constant so that our set of parameters is $\mu=\{ D_1, D_2, \Sigma_{a,1}, \Sigma_{a,2}, \Sigma_{1\to2}, \nu, \Sigma_{f,1}, \Sigma_{f,2}, \chi_1, \chi_2 \}.$
	
	
% 	Under some mild conditions on the parameters $\mu$, the maximum eigenvalue $\lambda_{\text{max}}$ is real and strictly positive (see \citep[Chapter XXI]{DLvol6}). 
% 	The associated eigenfunction $\phi$ is also real and positive at each point $\mathbf x\in\Omega$ and it is the flux of interest. 
% 	In neutronics, it is customary to use the inverse of $\lambda_{\text{max}}$, that is called the multiplication factor $k_{\text{eff}} := \frac{1}{\lambda_{\text{max}}}$.
% 	%\begin{equation}
% 	%\label{eq:Keff_lambda}
% 	%k_{\text{eff}} := \frac{1}{\lambda_{\text{max}}}.
% 	%\end{equation}
% 	Here, for each setting $\mu$, the parameter $k_{\text{eff}}$ is determined by the solution to the eigenvalue problem Eq. \eqref{eq:diffusion}. 
	
	
	
	
	
	
	
	% @@@@@@@@@@@@@@@@@@@@@@@@@@
	% @@@@@@@@@@@@@@@@@@@@@@@@@@
	\section{Implement of neural networks}
	\label{sec:Realizations}
	
	% @@@@@@@@@@@@@@@@@@@@@@@@@@
	\subsection{Introduction of DEPINN}
	\label{sec:PINN Introduction}
	In this section, we introduce how to use DEPINN to solve the problems \eqref{eq:diffusion}-\eqref{eq:PINN_eigenvalue BC}. The main idea is to train DNN as an approximator. The output is an approximate solution to the equation, which is achieved by incorporating such as PDEs, definite solution conditions, related physical laws and prior information into the construction of the loss function of the DNN. Then the neural network is generally trained by optimization algorithms such as Adam \citep{Adam} and LBFGS \citep{LBFGS}. During the training process, the value of the loss function will gradually decrease. And when the value of the loss function drops to a sufficiently small value, it can be considered that the predicted solution will be close enough to the true solution of the equation.
	
	%Review the parametric neutron diffusion eigenvalue problem \ref{eq:diffusion} with \red{Neumann boundary condition},
	
	%\begin{small}
	%\begin{equation}
	%\label{eq:PINN_eigenvalue problem}
	%  \begin{array}{r@{}l}
		%  	\left\{
		%	\begin{aligned}
			%   	&-\nabla \left(D_1\nabla \phi_1\right)+\left(\Sigma_{{ a}, 1} + \Sigma_{ 1\to 2} \right) \phi_1 = \lambda \chi_1 \left( \nu\Sigma_{{ f}, 1}\phi_1+\nu \Sigma_{{ f}, 2}\phi_2 \right) \\
			%   	&-\nabla \left(D_2\nabla %\phi_2\right)+\Sigma_{{\rm a}, 2}\phi_2 - \Sigma_{ 1\to 2}  \phi_1 = \lambda \chi_2 \left( \nu\Sigma_{{ f}, 1}\phi_1+ \nu\Sigma_{{ f}, 2}\phi_2 \right) \\
			%   	&\frac{\partial \phi_1(\bx)}{\partial \bn} = g_1(\bx) , \bx \in \partial \Omega \\
			%   	&\frac{\partial \phi_2(\bx)}{\partial \bn} = g_2(\bx) , \bx \in \partial \Omega \\
			%	\end{aligned}
		%	\right.
		%   \end{array}
	%\end{equation}
	%\end{small}
	
	
	%Introduce some notation, $\phi=\left( \phi_1,\phi_2 \right)$, $\bg(\bx)=(g_1(\bx),g_2(\bx))$.
	%Let $\mathcal{F}_1[\phi(\bx);\lambda]=0$ represent the first equation in \eqref{eq:diffusion} and $\mathcal{F}_2[\phi(\bx);\lambda]=0$  represent the second equation in (1). Then we have
	Eq. \eqref{eq:diffusion} can be rewritten as
	\begin{equation}
		\label{eq:PINNPDE}
		\left\{
		\begin{aligned}
			&\mathcal{N}[\phi(\bx),\lambda]=0 , \  \forall \bx \in \Omega,\\
			%	&\frac{\partial \phi(\bx)}{\partial \bn} = \bg(\bx) , \forall \bx \in \partial \Omega, \\
			& \mathcal{B}[\phi(\bx)]=0 , \ \forall \bx \in \partial \Omega,
		\end{aligned}
		\right.
	\end{equation}
	where $\Omega$ represents the computational domain, %$\bx$ represents the spatial variable, 
	and $\mathcal{B}[\phi(\bx)] = 0$ represents the Neumann boundary conditions or mixed boundary conditions, which are from real physics. %$\mathcal{F}_1[\phi(\bx),\lambda]$ and $\mathcal{F}_2[\phi(\bx),\lambda]$ represent differential equations with $\lambda$ and $\phi$. 
	
	%$\mathcal{P}(\phi(\bx),\lambda)$ and $h(\bx,\lambda)$ represent the additional prior physical information implied by the solution, $\Omega_p$ represents the area satisfying additional physical knowledge in the whole calculation area, 
	
	\begin{footnotesize}
		\begin{equation}
			\label{eq:PINNloss}
			\left\{
			\begin{aligned}
				&\mathcal{L}(\bx,\lambda^{\text{NN}};W) = \alpha_1 \mathcal{L}_{res}(\bx,\lambda^{\text{NN}};W) +  \alpha_2 \mathcal{L}_{b}(\bx,\lambda^{\text{NN}};W) +  \\
				&\qquad \qquad  \qquad  \qquad  \alpha_3 \mathcal{L}_p((\bx,\lambda^{\text{NN}};W)),\\
				&\mathcal{L}_{res}(\bx,\lambda^{\text{NN}};W) = \sum_{i} [\mathcal{N}[\phi(\bx_i;W),\lambda^{\text{NN}}]^2,\\
				&\mathcal{L}_{b}(\bx,\lambda^{\text{NN}};W) = \sum_{j} (\mathcal{B}[\phi(\bx_j;W)])^2,\\
				&\mathcal{L}_p(\bx,\lambda^{\text{NN}};W) = \sum_{k} [\phi(\bx_k;W) -\phi^{p}(\bx_k)]^2.
			\end{aligned}
			\right.
		\end{equation}
	\end{footnotesize}
	
	
	%Now we consider how to construct a DEPINN to solve Eq. \eqref{eq:PINNPDE}.
	%\red{ Carefully observe the form of Equation(\ref{eq:PINN_eigenvalue problem},\ref{eq:PINNPDE}), it can be concluded that it is not a definite solution problem, in addition, it is a coupled eigenvalue problem with a very complex computational area, so this paper decides to use data-driven to guide the training of neural networks.}
	Let the output of the DNN be $\phi(\bx;W)$, which is the approximate solution of Eq. \eqref{eq:diffusion} and $W$ represents the parameters in the neural networks. Note that $\lambda$ is not the output of the network, we do not have to design a search algorithm for $\lambda$, but treat it as a trainable variable $\lambda^{\text{NN}}$ inside the neural networks. The fully connected neural networks (FCNN) is used as the architecture and automatic differentiation is used for all differentiation operations in neural networks. Therefore, the loss function can be described as in Eq. \eqref{eq:PINNloss}.
	%where $\phi(\bx;W)$ is the output of the deep neural networks with input $\bx$,
	And $\mathcal{L}_{res}(\bx,\lambda;W)$ is the sum square error (SSE) loss corresponds to the residual of PDE equation, 
	%$\bx_i$ are the discrete points in the computational domain corresponding to the residual loss term,
	$\mathcal{L}_{b}(\bx,\lambda;W)$ is the SSE loss corresponds to the boundary conditions, 
	%$\bx_j$ are the discrete points on the boundary corresponding to the boundary term,
	$\mathcal{L}_p(\bx,\lambda;W)$ is the SSE loss corresponds to the prior physical information, \textcolor{black}{$\phi^{\text{P}}(\bx_k)$ represents the prior information obtained from observation or from a finite element solver}, 
	%$\bx_k$ are the discrete prior points corresponding to the prior observation solutions $\phi^{\text{P}}(\bx_k)$,}
$\alpha_1$,$\alpha_2$ and $\alpha_3$ represent the weight of $\mathcal{L}_{res}$, $\mathcal{L}_{b}$ and $\mathcal{L}_{p}$ respectively in the total loss $\mathcal{L}(\bx,\lambda;W)$. 


%\red{Note for real boundary conditions, which is of practical engineering values}


% Note that our loss function here uses SSE instead of the usual mean squared error (MSE) loss, and the reason is described in the section \textcolor{blue}{......}, \textcolor{black}{where we conducted rigorous contrast experiments to show} how the weights $\alpha_1$,$\alpha_2$ and $\alpha_3$ affect the results. The value of $\mathcal{L}(\bx,\lambda;W)$ drops to a very small value, meanwhile the parameter $W$ in the neural network is well optimized, which means that $\phi(\bx;W)$ will approximate the real solution sufficiently. The whole process above is illustrated in Fig. \ref{fig:PINN_Benchmark}. \textcolor{black}{Note also that, if no prior data are available, the DEPINN degenerates to general PINN,  which is only constrained by physical information.}


\begin{figure}[h]
	\includegraphics[width=0.55\textwidth]{PINN_Benchmark.png}
	\centering
	\caption{The process of solving parametric neutron diffusion eigenvalue problems containing noisy data by PINN.}
	\label{fig:PINN_Benchmark} 
\end{figure}


\subsection{Methods of handling noise}
\label{sec:Methodology}

In practical engineering experiments, only a few costly detectors can be used to obtain prior information on some specified locations. However, the prior data obtained by the detectors will inevitably be noisy. The prediction performance of DEPINN with noise is investigated in detail in the experiments in Section \ref{sec:MSELOSS-Case1} and \ref{sec:MSELOSS-Case2}. Therefore, how to effectively use the prior data accompanied by noise and how to reduce the impact of noise are issues that need to be addressed.

\subsubsection{Interval Loss}
\label{sec:Interval Loss}

After determining the prior points set $ \{\bx_i\}_{i=1}^{N_p}$, assume that the true solution at these points has the value $\{\phi^{\text{A}}(\bx_i)\}_{i=1}^{N_p}$. In order to simulate the situation of obtaining prior information through some detectors in an engineering experiment, we use Eq.\eqref{eq:addnoise} to add noise to these pure prior values.

\begin{equation}
\label{eq:addnoise}
\phi^{p}(\bx_i) = (1+\sigma_i \epsilon_i)\phi^{\text{A}}(\bx_i),  \forall \bx_i \in \{\bx_i\}_{i=1}^{N_p}
\end{equation}
where $\phi^{p}(\bx_i)$ is the value detected by the detectors on $\bx_i$, $\sigma_i$ represents the noise level of the detector, and $\epsilon_i$ is a random number around zero. 

Considering the actual engineering, the value range of $\sigma_i$ is generally $(0,20\%)$, $\epsilon_i$ generally obeys the standard Gaussian distribution $\mathcal{N}(0,1)$ or a uniform distribution  $\mathcal{U}(-1,1)$, and the detected value $\phi^{p}(\bx_i)$ is usually a positive value. If $\epsilon_i$ obeys the standard normal distribution, we have a high enough confidence level ($>99.999\%$) to consider it to be in the range [-5,5]. Therefore, this does not affect our derivation below, and for convenience, we assume that $\epsilon_i$ obeys the uniform distribution $\mathcal{U}(-1,1)$.

Based on the above, the inequality constraint in Eq. \eqref{eq:Inequality constraints} can be deduced, 
\begin{equation}
\label{eq:Inequality constraints}
\begin{aligned}
\phi^{\text{A}}(\bx_i) &=\frac{1}{1+\sigma_i \epsilon_i} \phi^{p}(\bx_i)  \\
\Rightarrow \frac{1}{1+\sigma_i} \phi^{p}(\bx_i) &\leq \phi^{\text{A}}(\bx_i)  \leq  \frac{1}{1-\sigma_i} \phi^{p}(\bx_i)
\end{aligned}
\end{equation}

Instead of directly making the DEPINN's prediction $\phi(\bx_i;W)$ mean squared error with the noise prior value $\phi^{p}(\bx_i)$, we want $\phi(\bx_i;W)$ to satisfy the inequality constraint of Eq. \eqref{eq:Inequality constraints} like the true value $\phi^{\text{A}}(\bx_i)$. For this purpose, we construct the interval loss function in Eq. \eqref{eq:Interval Loss}.


\begin{equation}
\label{eq:Interval Loss}
\begin{aligned}
&\mathcal{L}_p^{\text{IL}}(\bx;W) = \sum_{i=1}^{N_p} \left[(\phi(\bx_i;W) - \frac{1}{1+\sigma_i} \phi^{p}(\bx_i)) \right.\\
&\left.(\phi(\bx_i;W) - \frac{1}{1-\sigma_i} \phi^{p}(\bx_i))  
 +\lvert(\phi(\bx_i;W) - \frac{1}{1+\sigma_i} \phi^{p}(\bx_i))\right.\\
&\left. (\phi(\bx_i;W)- \frac{1}{1-\sigma_i} \phi^{p}(\bx_i))\rvert \right]
\end{aligned}
\end{equation}
% \begin{scriptsize}
% \begin{equation}
% \label{eq:Interval Loss}
% \begin{aligned}
% &\mathcal{L}_p^{\text{IL}}(\bx;W) = \sum_{i=1}^{N_p} \left[(\phi(\bx_i;W) - \frac{1}{1+\sigma_i} \phi^{p}(\bx_i))(\phi(\bx_i;W)- \frac{1}{1-\sigma_i} \phi^{p}(\bx_i)) \right.\\
% &\left. +\lvert(\phi(\bx_i;W) - \frac{1}{1+\sigma_i} \phi^{p}(\bx_i))(\phi(\bx_i;W)- \frac{1}{1-\sigma_i} \phi^{p}(\bx_i))\rvert \right]
% \end{aligned}
% \end{equation}
% \end{scriptsize}

Next, we will give a theorem (See Theorem \ref{thm:1}) to explore the relationship between the loss function in the form of Eq. \eqref{eq:Interval Loss} and the loss function in SSE form.

\begin{theorem}
\label{thm:1}
When all $\sigma_i \rightarrow 0 ,  1\leq i \leq N_p $ , Eq.\eqref{eq:Interval Loss} then degenerates to the original SSE form.
\end{theorem}
\begin{proof}
See Appendix A
\end{proof}


If the prior $\phi^{p}(\bx)$ is very noisy, the error between the prior value $\phi^{p}(\bx)$ and the true value $\phi^{\text{A}}(\bx)$ will be large, then it is not reasonable to build a loss function by the SSE form in Eq.\eqref{eq:PINNloss} for $\phi^{p}(\bx)$ and $\phi^{\text{A}}(\bx)$. Otherwise, it is impossible for the optimization algorithm to find the parameter W such that the loss function in Eq.\eqref{eq:PINNloss} is sufficiently small.

% 如果先验值的噪声很大，先验值和真解的误差就会很大的，那么通过方程四中的SSE形式对于神经网络的预测值和先验值建立损失函数是不合理的，优化算法不可能找到一个参数W，使得方程四中SSE形式的损失函数足够小。
% 为了增强DEPINN的鲁棒性，我们希望把结合噪声先验信息的PDE系统通过方程6转换成不等式的约束PDE系统。下面的定理证明，方程（7）形式的损失函数可以做到这一点。

\begin{equation}
	\label{eq:IC-PDE}
	\left\{
	\begin{aligned}
		&\mathcal{N}[\phi(\bx),\lambda]=0 , \  \forall \bx \in \Omega,\\
		& \mathcal{B}[\phi(\bx)]=0 , \ \forall \bx \in \partial \Omega\\
		&\frac{1}{1+\sigma_i} \phi^{p}(\bx_i) \leq \phi(\bx_i)  \leq  \frac{1}{1-\sigma_i} \phi^{p}(\bx_i),\ \forall \bx_i \in  \{\bx_i\}_{i=1}^{N_p}
	\end{aligned}
	\right.
\end{equation}



To enhance the robustness of DEPINN, we wish to convert the PDE system Eq.\eqref{eq:PINNPDE} including the noisy prior information into a PDE system with inequality constrained Eq.\eqref{eq:IC-PDE}. The following theorem proves that the loss function in the form of Eq. \eqref{eq:Interval Loss} can do this.
\begin{theorem}
\label{thm:2}
Assume that the optimization algorithm can find good enough parameters W that can reduce the value of the loss function to be small enough. 

Then, solving the system \eqref{eq:PINNPDE} using PINN when the loss function is $\mathcal{L} = \alpha_1 \mathcal{L}_{res} + \alpha_2 \mathcal{L}_{b} + \alpha_3 \mathcal{L}_{p}^{\text{IL}}$  $\iff$ solving the inequality constrained system Eq.\eqref{eq:IC-PDE} using PINN when the loss function is $\mathcal{L} = \alpha_1 \mathcal{L}_{res} + \alpha_2 \mathcal{L}_{b}$
\end{theorem}
\begin{proof}
See Appendix B
\end{proof}

It is worth mentioning that if the system Eq. \eqref{eq:IC-PDE} does not have a unique solution, then normalization is also required after using the interval function. We believe that the inequality constraint works better for noise problems. The experiments in Section \ref{sec:effect of IL} will prove our point.

\subsubsection{Adaptive weights}
\label{sec:Adaptive weights}
It has been mentioned that adjusting the weight of the PINN loss function term will effectively improve the results (\citep{PINNTA1}, \citep{Weighted-Loss}). Because prior information is very effective in accelerating training and improving prediction accuracy \citep{DEPINN}, we mainly focus on exploring the weights of the prior loss terms $\mathcal{L}_p$. Consider a more realistic situation, if the detectors used are not of the same specification, then the noise levels of the obtained data are also different, so it is not reasonable to set the weights of each prior data to be the same. A more reasonable idea is that the lower the noise level of some prior data, the higher their confidence, then their weights should be set larger, correspondingly, the higher the noise level, then we think the weights will be smaller. However, it should be noted that the maximum value of the weights is limited in order to prevent the phenomenon of gradient explosion due to too small noise.

Therefore, we set the weights of the prior loss terms as Eq.\eqref{eq:AW}

\begin{equation}
	\label{eq:AW}
	\begin{aligned}
	    \alpha_3 = \{ min \{\frac{1}{\sigma^2_i},10000\} \}_{i=1}^{N_p}
	\end{aligned}
\end{equation}

The experiments in Table \ref{tab:Case-1_AW} will demonstrate the effectiveness of adaptive weights.
\begin{table}[h]
\scriptsize
\centering
\caption{
The table gives the difference between the results using adaptive weights and weights set to 1 when the noise level $\sigma$ is 0.01 or 0.05, i.e., $\be^{AW}(u)$ and $\be(u)$. The table shows the results when the interval loss function is used without normalization yet. The superscript AW indicates that adaptive weights are used. The relevant benchmark problem and measurements are described in Section \ref{sec:MSELOSS-Case1} and \ref{sec:measures}.
}
\setlength{\tabcolsep}{8.mm}{
\begin{tabular}{c|c|c|}%{p{1cm}p{2.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.6cm}}
\hline\noalign{\smallskip}
   $\sigma$&  $\be^{AW}(u)$ & $\be(u)$  \\
\hline
0.01  & 2.7661e-05 &  1.2222e-04\\
\hline
0.05  & 6.0330e-04 & 7.2469e-04  \\
\hline
\end{tabular}
}
\label{tab:Case-1_AW} 
\end{table}


% @@@@@@@@@@@@@@@@@@@@@@@@@@
% @@@@@@@@@@@@@@@@@@@@@@@@@@
\section{Numerical survey on the robustness of DEPINN}
\label{sec:Numericalsurvey}
    In this section, we illustrate the efficiency of using the proposed DEPINN framework for solving NDEPs in the presence of noisy prior data based on two benchmark problems.
    To scientifically measure the calculation results, We give the evaluation measures of the article in section \ref{sec:measures}. Then using SSE loss function, we explore the performance of solving benchmark problems using DEPINN with noisy prior data from several perspectives in section \ref{sec:MSELOSS-Case1} and \ref{sec:MSELOSS-Case2}. In order to highlight the advantages of the interval loss function proposed in this paper, we evaluate the performance of DEPINN on the same benchmark problems using the interval loss function in Section \ref{sec:effect of IL}.
    
%




% @@@@@@@@@@@@@@@@@@@@@@@@@@
\subsection{Evaluation measures}
\label{sec:measures}

In this section, we will introduce some notations and discuss how to measure the output of DEPINN. Using $\lambda^{\text{NN}}$ to represent the trainable variables $\lambda$ in the neural networks and $k^{\text{NN}}_{\text{eff}} := \frac{1}{\lambda^{\text{NN}}}$.  Moreover, we use $u(\bx)$ to denote the flux function $\phi(\bx;W)$ predicted by DEPINN, use $\phi^{\text{A}}(\bx)$ and $k^{\text{A}}_{\text{eff}}$ to denote the true values obtained from the analytical solutions. 


With M noisy calculations, the following measure is defined to evaluate the predicted solution $\{u^j\}_{j=1}^{M}$ on the test set $\{\bx_i\}_{i=1}^{N_
t}  \in \Omega $ and the prediction of the multiplication factor $\{(k^{\text{NN}}_{\text{eff}})^j\}_{j=1}^{M}$.



%\red{For the fluxes $u$ and $v$ output by the neural networks, the comparison with it is a high-fidelity numerical solutions $\phi^{\text{FF}}_1$,$\phi^{\text{FF}}_2$.the same as $k^{\text{NN}}_{\text{eff}}$ to $k^{\text{FF}}_{\text{eff}}$.}

% The classical 2D IBP has two regions which are fuel region $\Omega_{\text{fuel}}$ composed $\Omega_{1,2,3}$ and water region $\Omega_{\text{water}}$ composed only of $\Omega_{4}$, which are shown in Fig. \ref{fig:iaeacore}.
% Since relative error in $L_{2}$ norm is always satisfactory and relative error in $L_{\infty}$ norm is more important in engineering,  we present the numerical results in $L_{\infty}$ norm in the following sections. The definition of $L_{\infty}$ norm is given in Eq. \eqref{eq:RE_fueL_infty}. 


\begin{equation}\label{eq:MSE_field}
%& \left\{
\begin{aligned}
&\be(u) = \frac{1}{M} \frac{1}{N_t}\sum_{j=1}^{M}\sum_{i=1}^{N_t}(u^j(\bx_i)-\phi^{\text{A}}(\bx_i))^2 \\
&\be(u(\bx_i)) = \frac{1}{M} \sum_{j=1}^{M}(u^j(\bx_i)-\phi^{\text{A}}(\bx_i))^2 \\
&    \be(k^{\text{NN}}_{\text{eff}}) =\frac{1}{M}\sum_{j=1}^{M}((k^{\text{NN}}_{\text{eff}})^j- k^{\text{A}}_{\text{eff}})^2\\
&\be_{\infty}(u(\bx)) = \max_{1\leq j \leq M}(\sum_{i=1}^{N_t}(u^j(\bx)-\phi^{\text{A}}(\bx_i))^2) \\
&    \be_{\infty}(k^{\text{NN}}_{\text{eff}}) =\max_{1\leq j \leq M}((k^{\text{NN}}_{\text{eff}})^j- k^{\text{A}}_{\text{eff}})^2.
\end{aligned}
\end{equation}

where $\be(u)$ and $\be(k^\text{NN}_{\text{eff}})$ are the mean squared error of $u$ and $k^{\text{NN}}_{\text{eff}}$ in these M calculations, $\be(u(\bx_i))$ is the mean squared error of $u$ at point $x_i$ in these M calculations, $\be_{\infty}(u(\bx))$ and $\be_{\infty}(k^{\text{NN}}_{\text{eff}})$ are the  maximum mean squared error of $u$ and $k^{\text{NN}}_{\text{eff}}$ for a single of these M calculations.





\begin{table}[h]
	\centering
	\caption{Normal parameter settings in DEPINN.}
	\label{tab:parameter-PINN} 
	\setlength{\tabcolsep}{1.mm}{
		\begin{tabular}{cccccc}%{p{1cm}p{2.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.6cm}}
			\hline\noalign{\smallskip}
			Torch & Cuda & Activation   &  Initialization  & Adam  & LBFGS   \\
			version & version & function  & method   &learning rate & learning rate \\
			\hline
			1.11.0&  10.2& tanh  & Xavier & 0.0001 & 1 \\ 
			\hline
		\end{tabular}
	}
\end{table}







% % @@@@@@@@@@@@@@@@@@@@@@@@@@
% \subsection{Setup of experiments for benchmark problems}
% \label{sec:Setup}
% In this subsection, we give an detail introduction of the benchmark problems that will be used in this work, from simple geometry to complex geometry, and from mono-energetic equation to two-group equations.
% \textcolor{blue}{..........................}

% \subsubsection{Case-1: the finite spherical reactor.}

% %
% \textcolor{black}{
% \subsubsection{Case-2: the finite cylindrical reactor}}




% @@@@@@@@@@@@@@@@@@@@@@@@@@
\subsection{Performance on 1-d mono-energetic diffusion equation}
\label{sec:MSELOSS-Case1}

In this section, we explore in detail the performance of DEPINN with noisy prior data when solving one-dimension mono-energetic diffusion equation (Case-1). 

This test case is adapted from \cite{benchmark1D} and modeled by one-dimension mono-energetic diffusion equation as shown in Eq. \eqref{eq:1dspherical}. The actual calculation area is only a quarter of the upper right corner, as the rest can be inferred from the symmetry of the coordinate axes.
This is a multiplying system of a uniform reactor in the shape of a sphere of physical radius $R$. To solve the diffusion equation, we replace the Laplacian with its spherical form as shown in Fig. \ref{fig:1dspherical} with zero flux boundary condition at the extrapolated length, where $R_{e}=R+d$, and $d \approx \frac{2}{3}\lambda_{tr}$ is known as the extrapolated length. 
%For homogeneous, weakly absorbing media, an exact solution of the mono-energetic transport equation in this case yields $d \approx 0.7104 \lambda_{tr}$. 
In this test case, the analytical solution of Eq. \eqref{eq:1dspherical} can be represented as in Eq. \eqref{eq:1dsphericalsolution} when $k_{\text{eff}}=1$. For numerical solution convenience, we further set $\Sigma_a=0.45, \Sigma_s = 2, \Sigma_f=2$ and $\nu\Sigma_f=2.5$.
%
%%
\begin{equation}
	%\begin{split}
	\label{eq:1dspherical}
	\hspace{-0.3cm}
	\begin{array}{r@{}l}
		\left\{
		\begin{aligned}
			& D \Delta \phi(r) - \Sigma_{a}\phi(r) =\frac{1}{k_{\text{eff}}} \nu \Sigma_f\phi(r) \\
			& \phi(R_{e})= 0,~ r \in (0,R_{e})
		\end{aligned}
		\right.
	\end{array}
	%   \end{split}
\end{equation}
%



After determining the prior points $\{x_i\}_{i=1}^{N_p}$, the pure prior values at these points $\{\phi^{a}(x_i)\}_{i=1}^{N_p}$ can be obtained by the analytical solution Eq.\eqref{eq:1dsphericalsolution}.

%
\begin{equation}
\label{eq:1dsphericalsolution}
\phi^A(r)=A\frac{\sin(\frac{\pi}{R_{e}}r)}{r}. 
\end{equation}
%
\begin{figure}[htp]
\includegraphics[width=0.45\textwidth]{Diffusion-Theory-Spherical-Reactor-min.png}
\centering
\caption{Geometry of finite spherical reactor \cite{benchmark1D}.}
\label{fig:1dspherical} 
\end{figure}


In order to simulate the situation of obtaining prior information through some detectors in an engineering experiment, we use Eq.\eqref{eq:addnoise} to add noise to the pure prior values.

\begin{equation}
\label{eq:addnoise_1d}
\phi^{P}(x_i) = (1+\sigma \epsilon)\phi^{A}(x_i)
\end{equation}
where $\sigma$ represents the noise level of the detector, and $\epsilon$ is a random number obeying the standard normal distribution $\mathcal{N}(0,1)$. 

Before starting a series of experiments, to control the variables, we fix some key parameters in DEPINN in Table \ref{tab:KP-DEPINN}.

\begin{table}[h]
	\centering
	\caption{Key parameter settings on Case-1.}
	\label{tab:KP-DEPINN} 
	\setlength{\tabcolsep}{1.mm}{
		\begin{tabular}{cccccc}%{p{1cm}p{2.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.6cm}}
			\hline\noalign{\smallskip}
			Net   &  Adam   & LBFGS   &  Weight  & Weight  & Weight   \\
			Size & traning epochs & traning epochs  & $\alpha_1 $  &$\alpha_2 $ & $\alpha_3$  \\
			\hline
			20 × 4&  30000 & 10000  & 1 & 1 & $\frac{1}{\sigma^2}$ \\ 
			\hline
		\end{tabular}
	}
\end{table}

In the experiments in Fig. (\ref{fig:u_dn_1d}, \ref{fig:Keff_dn_1d} and \ref{fig:MSE_dn_1d}), we fixed the number of noisy prior points $N_p = 4$ and observed the performance of different angles when solving Eq. \ref{eq:1dspherical} using DEPINN as the noise level $\sigma$ gradually increased from 0.001 to 0.1 within 100 calculations.
The positions of prior points are $x=0.2, 0.4, 0.6, 0.8$.


\begin{figure}[htbp]
\centering
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e3_u_randn_1d.png}}
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e2_u_randn_1d.png}}\\
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n5e2_u_randn_1d.png}}
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e1_u_randn_1d.png}}
\caption{Case-1 the finite spherical reactor: prediction $u$ from different prior data noise scales.
(a) $\sigma = 0.001$. (b) $\sigma = 0.01$. 
(c) $\sigma = 0.05$. (d) $\sigma =0.1$.}
\label{fig:u_dn_1d}
\end{figure}

%图4展示了DEPINN的预测在不同噪声水平下的变化。可以看出，随着噪声水平的增加，虽然这100次预测解的均值和解析解仍然相接近，但是95%的置信区间会越来越大，也就是说这100次预测解和解析解的方差会越来越大，单次运算的预测解越有可能偏离解析解。

Fig. (\ref{fig:u_dn_1d}) shows the variation of the predicted solution $u$ of DEPINN at different noise levels $\sigma = 0.001,0.01,0.05,0.1$. It can be seen that as the noise level increases, although the mean value of these 100 predicted solutions and the analytic solution are still close to each other, the $95\%$ confidence interval will become wider, which means that the variance between these 100 predicted solutions and the analytic solution will become larger, and the more likely the predicted solution in a single operation will deviate from the analytic solution.

\begin{figure}[htbp]
	\centering
	\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e3_Keff_randn_1d.png}}
	\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e2_Keff_randn_1d.png}}\\
	\subfloat[]{\includegraphics[width = 0.23\textwidth]{n5e2_Keff_randn_1d.png}}
	\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e1_Keff_randn_1d.png}}
	\caption{Case-1 the finite spherical reactor: prediction $k^{\text{NN}}_\text{eff}$ from different prior data noise scales. (a) $\sigma = 0.001$. (b) $\sigma = 0.01$. 
(c) $\sigma = 0.05$. (d) $\sigma =0.1$.}
	\label{fig:Keff_dn_1d}
\end{figure}

Fig. (\ref{fig:Keff_dn_1d}) shows the frequency histogram of DEPINN predictions for eigenvalues $k^{\text{NN}}_\text{eff}$ at different noise levels $\sigma = 0.001,0.01,0.05,0.1$. From the 100 calculations at each of the same noise levels, it can be seen that the predicted values obey a Gaussian distribution with a mean of 1. However, the variance of the Gaussian distribution increases as the noise level increases.

\begin{figure}[htbp]
	\centering
	\subfloat[]{\includegraphics[width = 0.23\textwidth]{u_MSE_1d.png}}
	\subfloat[]{\includegraphics[width = 0.23\textwidth]{Keff_MSE_1d.png}}\\
	\caption{Case-1 the finite spherical reactor: (a)-(b) Variations of mean square error of $u$ and $k^{\text{NN}}_\text{eff}$ when the noise level $\sigma$ is taken as 0.001, 0.01, 0.05, and 0.1 respectively.}
	\label{fig:MSE_dn_1d}
\end{figure}

With the measurements defined in Section \ref{sec:measures}, Fig. (\ref{fig:MSE_dn_1d}) shows the variation of DEPINN predictions $k^{\text{NN}}_\text{eff}$ and $u$ for different noise levels $\sigma = 0.001,0.01,0.05,0.1$. From this perspective, it is intuitive that the prediction performance of the neural network is getting worse as the noise of the prior data increases, both for the prediction of the solution $u$ of Eq. (\ref{eq:1dspherical}) and for the prediction of the eigenvalues  $k^{\text{NN}}_\text{eff}$.

In the experiments in Fig. \ref{fig:MSE_dp_1d}, we fixed the noise level $\sigma = 0.05 / 0.1$ but gradually increased the number of noisy prior points $N_p = 4,8,16,32$. Fig. \ref{fig:MSE_dp_1d} illustrates that despite the large level of noise in the the prior information, the increase in the number of prior points $N_p$ significantly improves the prediction performance. 

However, due to the high cost of detectors in engineering experiments, it is not realistic to improve the prediction performance by increasing the number of detectors. This inspired the need to find new ways to improve the robustness of the model.

\begin{figure}[htbp]
	\centering
	\subfloat[Relative $L_{\infty}$ error of $u$ and $v$]{\includegraphics[width = 0.23\textwidth]{n1e1_dp_u_MSE_1d.png}}
	\subfloat[Relative error of $ k^{\text{NN}}_\text{eff}$]{\includegraphics[width = 0.23\textwidth]{n1e1_dp_Keff_MSE_1d.png}}\\
	\caption{Case-1 the finite spherical reactor: (a)-(b) Variations of mean square error of $u$ and $k^{\text{NN}}_\text{eff}$ when the noise level $\sigma = 0.1$ and the number of prior noise points $N_p$ is taken as 4, 8, 16, 32 respectively.}
	\label{fig:MSE_dp_1d}
\end{figure}

% @@@@@@@@@@@@@@@@@@@@@@@@@@
\subsection{Performance on 2-d mono-energetic diffusion equation}
\label{sec:MSELOSS-Case2}

In this section, we further explore the performance of DEPINN in solving the two-dimensional mono-energetic diffusion equation, which is more complex than one-dimensional (Case-2). We still calculate only the upper right quarter region, since the rest can be inferred by symmetry. This test case is also adapted from \cite{benchmark1D}, a multiplying system of a uniform reactor in the shape of a cylinder of physical radius $R$ and height $H$.  The neutronic behaviour can modeled by two-dimension mono-energetic diffusion equation as shown in Eq. \eqref{eq:2dcylindrical}, where $R_{e}=R+d$, $H_{e}=H+d$ and $d$ is the extrapolated length. The parameters are kept the same with test case-1. 
%This finite cylindrical reactor is situated in cylindrical geometry at the origin of coordinates. 
To solve the diffusion equation, we replace the Laplacian by its cylindrical form as shown in Fig \ref{fig:2dcylindrical}. In this case, the analytical solution of Eq. \eqref{eq:2dcylindrical} can be represented by Eq.\eqref{eq:2dcylindricalsolution} when $k_{\text{eff}}=1$, where $J_0(r)$ is the zero order first kind Bessel function. 

%
%
%%
\begin{equation}
	%\begin{split}
	\label{eq:2dcylindrical}
	\hspace{-0.3cm}
	\begin{array}{r@{}l}
		\left\{
		\begin{aligned}
			& D \Delta \phi(r,z) - \Sigma_{a}\phi(r,z) =\frac{1}{k_{\text{eff}}} \nu \Sigma_f\phi(r,z) \\
			& \phi(R_{e})= 0,~ \phi(H_{e})= 0, ~ r \in (0,R_{e}), z \in (0,H_{e})\\
		\end{aligned}
		\right.
	\end{array}
	%   \end{split}
\end{equation}
%
%
Then we can get the analytical solution Eq.\eqref{eq:2dcylindricalsolution}.
\begin{equation}
\label{eq:2dcylindricalsolution}
\phi^A(r,z)=AJ_0(\frac{2.045}{R_e}r)\cos(\frac{\pi}{H_e}z) 
\end{equation}
%
%textcolor
%
\begin{figure}[htp]
\includegraphics[width=0.45\textwidth]{Diffusion-Theory-Cylindrical-Reactor-min.png}
\centering
\caption{Geometry of finite cylindrical  reactor \cite{benchmark1D}.}
\label{fig:2dcylindrical} 
\end{figure}

Similar to Eq.\eqref{eq:addnoise_1d}, we can use Eq.\eqref{eq:addnoise_2d} to add noise.

\begin{equation}
\label{eq:addnoise_2d}
\phi^{P}(x_i,y_i) = (1+\sigma \epsilon)\phi^{A}(x_i,y_i)
\end{equation}


Before starting a series of experiments, to control the variables, we fix some key parameters in DEPINN in Table \ref{tab:KP-DEPINN-2d}.

\begin{table}[h]
	\centering
	\caption{Key parameter settings on Case-2.}
	\label{tab:KP-DEPINN-2d} 
	\setlength{\tabcolsep}{1.mm}{
		\begin{tabular}{cccccc}%{p{1cm}p{2.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.6cm}}
			\hline\noalign{\smallskip}
			Net   &  Adam   & LBFGS   &  Weight  & Weight  & Weight   \\
			Size & traning epochs & traning epochs  & $\alpha_1 $  &$\alpha_2 $ & $\alpha_3$  \\
			\hline
			40 × 4&  40000 & 20000  & 1 & 1 & $\frac{1}{\sigma^2}$ \\ 
			\hline
		\end{tabular}
	}
\end{table}

In the experiments in Fig. (\ref{fig:u_dn_2d} and \ref{fig:MSE_dn_2d}), we fixed the number of noisy prior points $N_p = 8$ and observed the performance of different angles when solving Eq. \ref{eq:2dcylindrical} using DEPINN as the noise level $\sigma$ gradually increased from 0.001 to 0.1 within 50 calculations. 

The coordinates of the prior point are $(\frac{1}{4}*ub_x,\frac{1}{4}*ub_y),(\frac{1}{4}*ub_x,\frac{1}{2}*ub_y),(\frac{1}{4}*ub_x,\frac{3}{4}*ub_y,),(\frac{1}{2}*ub_x,\frac{1}{4}*ub_y),(\frac{1}{2}*ub_x,\frac{3}{4}*ub_y),(\frac{3}{4}*ub_x,\frac{1}{4}*ub_y),(\frac{3}{4}*ub_x,\frac{1}{2}*ub_y),(\frac{3}{4}*ub_x,\frac{3}{4}*ub_y)$. $ub_x$ indicates the upper bound of the computed region on the x-axis,$ub_y$ indicates the upper bound of the computed region on the y-axis.

Fig. (\ref{fig:u_dn_2d}) shows the variation of the predicted solution $u$ of DEPINN at different noise levels $\sigma = 0.001,0.01,0.05,0.1$. To facilitate the observation, we take the truncation of z=0 as the result of the observation used. It exhibits the same properties as in Fig. (\ref{fig:u_dn_1d}). It can be seen that as the noise level increases, the $95\%$ confidence interval will become wider, which means that the error in the predicted solution in a single calculation is more likely to be larger. But it can be intuitively noticed that the expectation of these 50 predictions still does not differ much from the exact solution.

\begin{figure}[htbp]
\centering
\subfloat[ ]{\includegraphics[width = 0.23\textwidth]{n0e0_u_randn_2d.png}}
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e2_u_randn_2d.png}}\\
\subfloat[ ]{\includegraphics[width = 0.23\textwidth]{n5e2_u_randn_2d.png}}
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e1_u_randn_2d.png}}
\caption{Case-2 the finite cylindrical reactor: prediction $u$ from different prior data noise scales.
(a) $\sigma = 0$. (b) $\sigma = 0.01$. 
(c) $\sigma = 0.05$. (d) $\sigma =0.1$.}
\label{fig:u_dn_2d}
\end{figure}




Fig. (\ref{fig:MSE_dn_2d}) shows the variation of DEPINN predictions $k^{\text{NN}}_\text{eff}$ and $u$ for different noise levels $\sigma = 0.001, 0.01,0.05,0.1$. For a more detailed observation, we randomly selected two points from the computational region to observe their variation in mean square error. It is intuitive that the prediction performance of the neural network is getting worse as the noise of the prior data increases.

\begin{figure}[htbp]
\centering
\subfloat[]{\includegraphics[width = 0.23\textwidth]{u_MSE_2d_randn.png}}
\subfloat[]{\includegraphics[width = 0.23\textwidth]{Keff_MSE_2d_randn.png}}\\
\subfloat[]{\includegraphics[width = 0.23\textwidth]{u_1_MSE_2d_randn.png}}
\subfloat[]{\includegraphics[width = 0.23\textwidth]{u_2_MSE_2d_randn.png}}
\caption{Case-2 the finite cylindrical reactor: (a)-(b) Variations of mean square error of $u$ and $k^{\text{NN}}_\text{eff}$ when the noise level $\sigma$ is taken as 0.001, 0.01, 0.05, and 0.1 respectively. (c)-(d) Variations of mean square error of $u$ at two points $(0.3159,0.5911), (0.3363,0.3822)$ when the noise level $\sigma$ is taken as 0.001, 0.01, 0.05, and 0.1 respectively.} 
\label{fig:MSE_dn_2d}
\end{figure}


In the experiments in Fig. \ref{fig:MSE_dp_2d}, we fixed the noise level $\sigma = 0.05 /0.1$ but gradually increased the number of noisy prior points $N_p = 4,8,16,32$. Fig. \ref{fig:MSE_dp_2d} illustrates that despite the large level of noise in the the prior information, the increase in the number of prior points $N_p$ significantly improves the prediction performance. This shows that even in two-dimensional benchmark problem, directly increasing the number of a priori points can still improve the accuracy of the results. But as stated before, such an approach is naive.

\begin{figure}[htbp]
	\centering
	\subfloat[]{\includegraphics[width = 0.23\textwidth]{n5e2_dp_u_MSE_2d.png}}
	\subfloat[]{\includegraphics[width = 0.23\textwidth]{n5e2_dp_Keff_MSE_2d.png}}\\
	\caption{Case-2 the finite cylindrical reactor: (a)-(b) Variations of mean square error of $u$ and $k^{\text{NN}}_\text{eff}$ when the noise level $\sigma = 0.05$ and the number of prior noise points $N_p$ is taken as 4, 8, 16, 32 respectively.}
	\label{fig:MSE_dp_2d}
\end{figure}


% @@@@@@@@@@@@@@@@@@@@@@@@@@
\subsection{Effect of the interval loss function}
\label{sec:effect of IL}



In this section, we apply the proposed interval loss function to the numerical examples in the previous two sections to highlight the advantages through comparison from various angles. In the calculation, the parameters used are the same as before, the only difference is the form of the loss function.

\begin{figure}[h]
\centering
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n5e2_u_randn_1d_il.png}}
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n5e2_u_randn_1d.png}}\\
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e1_u_randn_1d_il.png}}
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e1_u_randn_1d.png}}
\caption{Case-1 the finite spherical reactor: prediction $u$ from different prior data noise scales. Interval loss is used in (a)(c) on the left, and SSE loss is used in (b)(d) on the right.
(a) $\sigma = 0.05$. (b) $\sigma = 0.05$.
(c) $\sigma = 0.1$. (d) $\sigma =0.1$.}
\label{fig:u_dn_1d_il}
\end{figure}

\begin{figure}[h]
\centering
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n5e2_u_randn_il_2d.png}}
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n5e2_u_randn_2d.png}}\\
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e1_u_randn_il_2d.png}}
\subfloat[]{\includegraphics[width = 0.23\textwidth]{n1e1_u_randn_2d.png}}
\caption{Case-2 the finite cylindrical reactor: prediction $u$ from different prior data noise scales. Interval loss is used in (a)(c) on the left, and SSE loss is used in (b)(d) on the right.
(a) $\sigma = 0.05$. (b) $\sigma = 0.05$.
(c) $\sigma = 0.1$. (d) $\sigma =0.1$.}
\label{fig:u_dn_2d_il}
\end{figure}
% 通过这个角度可以看到，在相同设置的实验中，区域损失函数的加入使得这M次预测解和解析解的方差显著下降，说明区域损失函数极大的减少了由先验数据中的噪声引起的不确定性。同时，也能看出，区域损失函数的加入也使得这M次预测解的期望和解析解的误差降低，这说明在降低方差的同时期望的精度也在提升，从全方面提升了DEPINN的鲁棒性。
From Fig.\ref{fig:u_dn_1d_il}, it can be seen that the addition of the interval loss function causes a significant decrease in the variance of predicted solution in these 100 experiments, indicating that the interval loss function greatly reduces the uncertainty caused by the noise in the prior data. At the same time, it can also be seen that the addition of the interval loss function also reduces the error of the expectation of these 100 predicted solutions, which indicates that the accuracy of the expectation is improved while the variance is also reduced, thus the robustness of DEPINN is comprehensively improved. In the same perspective, we also give a comparison of the interval loss function and the SSE loss function on Case-2 in Fig.\ref{fig:u_dn_2d_il}.



The results in Fig.(\ref{fig:u_dn_1d_il} and \ref{fig:u_dn_2d_il}) are good, but not enough. 
If the naive method of taking expectations by multiple calculations is still used to deal with prior data with noise, then both the training cost of the neural network and the experimental cost of including detectors are unacceptable. In fact, the interval loss function can solve this confusion as well.



\begin{table}[h]
\scriptsize
\centering
\caption{
This table gives the values of $\be^I(u)$ and $\be^I(k^{\text{NN}}_{\text{eff}})$ obtained using the SSE loss function and the interval loss function for Case-2 when the noise level $\sigma$ is 0.05 or 0.1. The superscript S means that the SSE loss function is used, and L means that the interval loss function is used.
}
\setlength{\tabcolsep}{1.mm}{
\begin{tabular}{c|c|c|c|c|c|c}%{p{1cm}p{2.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.6cm}}
\hline\noalign{\smallskip}
   $\sigma$&  $\be^S(u)$ & $\be^I(u)$ &$\be_{\infty}^I(u)$ & $\be^S(k^{\text{NN}}_{\text{eff}})$ &$\be^I(k^{\text{NN}}_{\text{eff}})$ & $\be_{\infty}^I(k^{\text{NN}}_{\text{eff}})$  \\
\hline
0.05  & 3.0208e-04 & 9.9030e-07 & 2.8355e-06
&3.6612e-04& 3.8967e-07& 4.0199e-06  \\
\hline
0.1  & 5.7755e-04 & 1.4605e-06 & 7.7223e-06 &9.6904e-04&1.07685e-06& 1.8797e-05  \\
\hline
\end{tabular}
}
\label{tab:Case2_IL_MSE} 
\end{table}



In Table \ref{tab:Case2_IL_MSE}, we compared the mean squared errors of $u$ and $k^{\text{NN}}_\text{eff}$ after using the two types of loss functions. The comparison shows that the interval loss function works much better than the SSE function when the noise is large. Moreover, the values of $\be_{\infty}^I(u)$ and $\be_{\infty}^I(k^{\text{NN}}_{\text{eff}})$ are only one order of magnitude larger than $\be^I(u)$ and $\be^I(k^{\text{NN}}_{\text{eff}})$, and still much smaller than $\be^S(u)$ and $\be^S(k^{\text{NN}}_{\text{eff}})$, which indicates that that the single prediction using the interval loss function are better than the expectation of multiple predictions using the SSE loss function. We believe that with the use of the interval loss function, very good accuracy can be achieved with only a single calculation, even if the prior data contains large noise.


In conclusion, since DEPINN does not set up a search algorithm for $k^{\text{NN}}_\text{eff}$, but relies on the information from the prior data, this shows that the prior data are necessary for DEPINN. It works very well when the prior data are not noisy. However, in practical engineering, noise is unavoidable. By converting the system Eq.\eqref{eq:PINNPDE} into a system with Eq.\eqref{eq:IC-PDE} inequality constraints through the interval loss function, the robustness of DEPINN is improved, the training cost is greatly reduced, and the feasibility of applying it to practical engineering is greatly increased.

% @@@@@@@@@@@@@@@@@@@@@@@@@@
\section{Conclusion}
\label{sec:conclusion}

In this work, we performed uncertainty analysis on data-enabled physically-informed neural network (DEPINN) to examine the robustness performance of the model in solving two typical neutron diffusion eigenvalue benchmark problems.

Although the robustness of the model can also be improved by naively adding prior points, this is not feasible considering the expensive cost of placing detectors during the engineering experiments. To this end, we propose an innovative interval loss function, and we also give a rigorous proof that the interval loss function can transform the original system of equations into a system of equations with inequality constraints, greatly reducing the effect of noise in the prior data. We also propose adaptive weights for the prior loss terms, which improve the efficiency of using the prior data.


Numerical results based on two test cases show that the proposed interval loss function greatly enhances the robustness of DEPINN, and the results of just a single calculation have strong confidence. We conclude that this work overcomes the dilemma of noisy observed data and advances the feasibility of DEPINN for practical nuclear engineering problems in the field of nuclear reactor physics. Further work will include: i) noisy observational performance of the model for two-dimensional, two-group IAEA problems ii) testing of the model in three-dimensional, large-scale reactor physics problems; iii) new PINN structures for practical complex engineering problems without observational data, etc.



% @@@@@@@@@@@@@@@@@@@@@@@@@@

% @@@@@@@@@@@@@@@@@@@@@@@@@@
\section*{Contribution statement}
% \noindent Yu Yang: Methodology, Coding,  Writing \& Editing. Helin Gong: Conceptualization, Methodology, Nuclear engineering data curation, Writing \& Editing, Review, Funding acquisition. Shiquan Zhang: Conceptualization, Methodology, Review, Funding acquisition. Qihong Yang: Methodology, Coding. Zhang Chen: Funding acquisition, Validation. Qiaolin He: Conceptualization, Methodology, Review, Funding acquisition. Qing Li: Supervision, Review.

%Yu Yang, Helin Gong*, Shiquan Zhang*, Qihong Yang, Zhang Chen, Qiaolin He, Qing Li


% @@@@@@@@@@@@@@@@@@@@@@@@@@
\section*{Acknowledgment}

% This research is supported part by the National Natural Science Foundation of China (No.11971020, 11905216, 12175220), and the Stability Support Fund for Science and Technology on Reactor System Design Technology Laboratory. \textcolor{black}{The authors are grateful to the three anonymous reviewers’ constructive suggestions for the work during the preparation of the manuscript. 
% All data and codes used in this manuscript are publicly available on GitHub at https://github.com/YangYuSCU/DE-PINN.
% } %textcolor


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

% ============================================
%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here %\cite{Roberg2010}.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank D. Root for the loan of the SWAP. The SWAP that can ONLY be usefull in Boulder...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\appendices
\section{Proof of theorem \ref{thm:1}}


Thm. \ref{thm:1}: When all $\sigma_i \rightarrow 0 ,  1\leq i \leq N_p $ , Eq.\eqref{eq:Interval Loss} then degenerates to the original SSE form.


\begin{proof}
We want to investigate the case of a single prior point $\bx_i \in \{\bx_i\}_{i=1}^{N_p}$, which is trivial when extended to multiple points.

First, we consider the first part of the right-hand side of Eq.\eqref{eq:Interval Loss}.
\begin{equation}
	\label{eq:thm1_1}
	\begin{aligned}
    &\lim\limits_{\sigma_i\rightarrow0}(u(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(u(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i)) \\
    &=(u(\bx_i))^2 -  \lim\limits_{\sigma_i\rightarrow0}(\frac{1}{1+\sigma_i}+\frac{1}{1-\sigma_i})u(\bx_i)\phi^p(\bx_i)) \\
    &+ \lim\limits_{\sigma_i\rightarrow0} \frac{1}{1-\sigma^2_i}(\phi^p(\bx_i))^2 \\
    &= (u(\bx_i))^2 - 2u(\bx_i)\phi^p(\bx_i) + (\phi^p(\bx_i))^2\\
    &= (u(\bx_i)- \phi^p(\bx_i))^2
	\end{aligned}
\end{equation}


Then, we consider the second part of the right-hand side of Eq.\eqref{eq:Interval Loss}.

\begin{equation}
	\label{eq:thm1_2}
	\begin{split}
    &\lvert(u(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(u(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i))\rvert \\
    &=\left\{
    \begin{aligned}
    &(u(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(u(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i)) ,\\
    &when \quad u(\bx_i) <\frac{1}{1+\sigma_i}\phi^p(\bx_i)  \ or \ u(\bx_i) >\frac{1}{1-\sigma_i}\phi^p(\bx_i) ;\\
    &-(u(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(u(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i)),\\
    &when \quad \frac{1}{1+\sigma_i}\phi^p(\bx_i) \leq u(\bx_i)  \leq \frac{1}{1-\sigma_i}\phi^p(\bx_i)
    \end{aligned}
    \right.
	\end{split}
\end{equation}


Since $\sigma_i>0$ and $\sigma_i\rightarrow0$, we have $0<\sigma_i<\epsilon$, for any $\epsilon>0$.
\begin{equation*}
	(\frac{1}{1-\sigma_i}-\frac{1}{1+\sigma_i})\phi^p(\bx_i) = \frac{2\sigma_i}{1-\sigma^2_i}\phi^p(\bx_i) <2\epsilon\phi^p(\bx_i)
\end{equation*}

Thus,we have 

\begin{equation}
	\label{eq:thm1_3}
	\lim\limits_{\sigma_i\rightarrow0}(\frac{1}{1-\sigma_i}-\frac{1}{1+\sigma_i})\phi^p(\bx_i) = 0
\end{equation}

Combining Eq.(\eqref{eq:thm1_1} ,\eqref{eq:thm1_2} ,\eqref{eq:thm1_3}), we have

\begin{equation}
	\label{eq:thm1_4}
	\begin{split}
	&\lim\limits_{\sigma_i\rightarrow0}\lvert(u(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(u(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i))\rvert\\
	&= (u(\bx_i)- \phi^p(\bx_i))^2
	\end{split}
\end{equation}




According to Eq.(\eqref{eq:thm1_1} ,\eqref{eq:thm1_4}), for a single prior point $\bx_i \in \{\bx_i\}_{i=1}^{N_p}$, we have



\begin{equation}
\label{eq:thm1_5}
\begin{aligned}
&\lim\limits_{\sigma_i\rightarrow0}\mathcal{L}_p^{\text{IL}}(\bx_i;W)  \\ &=\lim\limits_{\sigma_i\rightarrow0}(u(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(u(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i)) \\
&+\lim\limits_{\sigma_i\rightarrow0}\lvert(u(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(u(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i))\rvert\\
&= 2(u(\bx_i)- \phi^p(\bx_i))^2
\end{aligned}
\end{equation}

The proof is over.

\end{proof}



\section{Proof of theorem \ref{thm:2}}
Thm. \ref{thm:2}:
Assume that the optimization algorithm can find good enough parameters $\widetilde{W}$ that can reduce the value of the loss function to be small enough. 

Then, solving the system Eq.\eqref{eq:PINNPDE} using PINN when the loss function is $\mathcal{L} = \alpha_1 \mathcal{L}_{res} + \alpha_2 \mathcal{L}_{b} + \alpha_3 \mathcal{L}_{p}^{\text{IL}}$  $\iff$ solving the inequality constrained system Eq.\eqref{eq:IC-PDE} using PINN when the loss function is $\mathcal{L} = \alpha_1 \mathcal{L}_{res} + \alpha_2 \mathcal{L}_{b}$.

\begin{proof} The necessity $\Leftarrow$ is obvious, and only the sufficiency $\Rightarrow$  needs to be proved.

Without loss of generality, we assume that $\alpha_1 =\alpha_2 =\alpha_3 =1$.
Under the assumptions, let the prediction solution corresponding to this sufficiently good parameter $\widetilde{W}$ be $\widetilde{u}$. And for $\forall \epsilon >0$, we have $\mathcal{L}_{res}<\epsilon$, $\mathcal{L}_{b}<\epsilon$ and $\mathcal{L}_{p}<\epsilon$.

Similar to the idea of the previous proof, we only need to consider the case of a single prior point $\bx_i \in \{\bx_i\}_{i=1}^{N_p}$.

To satisfy $\mathcal{L}_{p}<\epsilon$, we need

% \begin{equation*}
% \begin{aligned}
% 	&(u(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(u(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i))\\
% 	&+\lvert(u(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(u(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i))\rvert < \epsilon
% \end{aligned}
% \end{equation*}


\begin{equation}
	\label{eq:thm2_1}
	\begin{split}
	&(\widetilde{u}(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(\widetilde{u}(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i))\\
	+&\lvert(\widetilde{u}(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(\widetilde{u}(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i))\rvert < \epsilon \\
    &\Rightarrow \left\{
    \begin{aligned}
    &2(\widetilde{u}(\bx_i) - \frac{1}{1+\sigma_i}\phi^p(\bx_i))(\widetilde{u}(\bx_i)- \frac{1}{1-\sigma_i}\phi^p(\bx_i)) < \epsilon ,\\    
    &when \quad \widetilde{u}(\bx_i) <\frac{1}{1+\sigma_i}\phi^p(\bx_i) \ or \ \widetilde{u}(\bx_i) >\frac{1}{1-\sigma_i}\phi^p(\bx_i) ;\\
    &0 < \epsilon,when \quad \frac{1}{1+\sigma_i}\phi^p(\bx_i) \leq \widetilde{u}(\bx_i)  \leq \frac{1}{1-\sigma_i}\phi^p(\bx_i)
    \end{aligned}
    \right.
	\end{split}
\end{equation}

For Eq.\eqref{eq:thm2_1} to hold, then we have

\begin{equation}
\label{eq:thm2_2}
\begin{aligned}
     &\frac{\sigma_i\phi^p(\bx_i)}{1-\sigma^2_i}-\sqrt{(\frac{\sigma_i\phi^p(\bx_i)}{1-\sigma^2_i})^2-\frac{(\phi^p(\bx_i))^2}{1-\sigma^2_i}+\frac{1}{2}\epsilon} \leq \widetilde{u}(\bx_i)\\
     & \leq \frac{\sigma_i\phi^p(\bx_i)}{1-\sigma^2_i}+\sqrt{(\frac{\sigma_i\phi^p(\bx_i)}{1-\sigma^2_i})^2-\frac{(\phi^p(\bx_i))^2}{1-\sigma^2_i}+\frac{1}{2}\epsilon}
\end{aligned}
\end{equation}

Due to the arbitrariness of $\epsilon$, as $\epsilon\rightarrow0$, we have
\begin{equation}
\label{eq:thm2_3}
\begin{aligned}
\frac{1}{1+\sigma_i}\phi^p(\bx_i) \leq \widetilde{u}(\bx_i)  \leq \frac{1}{1-\sigma_i}\phi^p(\bx_i)
\end{aligned}
\end{equation}

Therefore, for $\forall \epsilon_1 >0$ we have

\begin{equation}
	\label{eq:thm2_4}
	\left\{
	\begin{aligned}
		&\mathcal{L}_{res} + \mathcal{L}_{b} <\epsilon_1\\
		&\frac{1}{1+\sigma_i} \phi^{p}(\bx_i) \leq \widetilde{u}(\bx_i)  \leq  \frac{1}{1-\sigma_i} \phi^{p}(\bx_i)
	\end{aligned}
	\right.
\end{equation}

The proof is over.
\end{proof}

\ifCLASSOPTIONcaptionsoff
\newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% ====== REFERENCE SECTION

%\begin{thebibliography}{1}

% IEEEabrv,
%\bibliographystyle{apsrev}
\bibliographystyle{IEEEtran}
%\bibliography{IEEEabrv,IAEAPINN}
\bibliography{IAEAPINN}
%\end{thebibliography}
% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% ==== SWITCH OFF the BIO for submission
% ==== SWITCH OFF the BIO for submission


%% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{Ignacio Ramos}
%(S'12) received the B.S. degree in electrical engineering from the University of Illinois at Chicago in 2009, and is currently working toward the Ph.D. degree at the University of Colorado at Boulder. From 2009 to 2011, he was with the Power and Electronic Systems Department at Raytheon IDS, Sudbury, MA. His research interests include high-efficiency microwave power amplifiers, microwave DC/DC converters, radar systems, and wireless power transmission.
%\end{IEEEbiographynophoto}

%% insert where needed to balance the two columns on the last page with
%% biographies
%%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}
% ==== SWITCH OFF the BIO for submission
% ==== SWITCH OFF the BIO for submission



% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


