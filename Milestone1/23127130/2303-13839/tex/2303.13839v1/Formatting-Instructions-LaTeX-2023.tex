%File: formatting-instructions-latex-2023.tex
%release 2023.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai23}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfigure}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{booktabs}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2023.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai23.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{HRDoc: Dataset and Baseline Method toward Hierarchical Reconstruction of Document Structures}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Jiefeng Ma\textsuperscript{\rm 1},
    Jun Du\textsuperscript{\rm 1}\thanks{Corresponding author.},
    Pengfei Hu\textsuperscript{\rm 1},
    Zhenrong Zhang\textsuperscript{\rm 1},
    Jianshu Zhang\textsuperscript{\rm 2},
    Huihui Zhu\textsuperscript{\rm 2},
    Cong Liu\textsuperscript{\rm 2}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}NERC-SLIP, University of Science and Technology of China\\
    \textsuperscript{\rm 2}iFLYTEK Research\\
    jfma@mail.ustc.edu.cn, jundu@ustc.edu.cn, \{hudeyouxiang, zzr666\}@mail.ustc.edu.cn,\\
    \{jszhang6, hhzhu2, congliu2\}@iflytek.com
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name,\textsuperscript{\rm 1,\rm 2}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Affiliation 1\\
    \textsuperscript{\rm 2} Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		%a. The importance of document hierarchical parsing (Cross-page)
		%b. Define a fine-grained label with 13 classes in academic documents and give a hierarchical representation of its structure.
		%c. Proposed a extendable dataset named HierDoc-V2, composed of two parts under the criterion of layout varieties (-simple and -hard)
		%d. Proposed three measurements (Node classification in F1, tree structure similarity in TEDS, semantic similarity in Semantic-TEDS) for the evaluation of HierDoc-V2
		%e. Proposed a competitive baseline model for all three tasks described above, with an absolute improve of 15\% percent
		%We 
		%
		%We regard each text line in documents as the basic unit instead of 
		%
		%
		%This is the first publicly available dataset used for hierarchical document element analysis in document level, named HierDoc-V2.
		%We use Mask-RCNN for document element detection with four classes (Text Line, Equation, Image, Table), each element annotated using a rectangle box with two corner coordinates. After acquiring element box informations, we use an off-the-shelf PDF miner to get corresponding text informations in each text line. 
		%We define a series of grammars to obtain the hierarchical structure of document elements, with each attached to a parent element in the same document.
		%However, existing dataset 
		%However, there exists no suitable dataset and sufficient method for this task.
		%To facilitate the researchers in both 
		
		% V1
		%The problem of document structure parsing refers to convert digital or scanned documents into corresponding semantic-level structures, which is of vital importance for understanding the mutual relationship of each document elements such as sections and paragraphs.
		%Most existing works mainly focus at splitting the boundary of each component in a single document page, neglecting the reconstruction of  semantic structure in multi-page documents.
		%In this paper, we proposed a hierarchical and fine-grained document structure parsing dataset named HierDoc-V2, which is composed of two parts including HierDoc-V2-Simple (HD2S) and HierDoc-V2-Hard (HD2H) under the criterion of layout varieties.
		%The HD2S dataset is composed of 1000 digital PDF files downloaded from ACL website, with 700 documents serves as training set and the rest 300 as testing set. All PDF files in HD2S are compiled using the same LaTex template, thus sharing vary similar layout structures.
		%The HD2H dataset is composed of 1500 digital PDF files obtained from arXiv website, with 1000 documents serves as training set and the rest 500 as testing set. These PDF files are selected from varied subjects and compiled using varied templates, introducing more challenges to the reconstruction tasks.
		%Each document in HD2S and HD2H has their line-level annotations obtained from both rule-based extractor and human annotators, containing the coordinate/text/class/parent/parent-relation information of each text line.
		%To better evaluate the system performance, we introduce a new criterion named Semantic-TEDS with jointly considering three aspects including classification, parent finding and relationship matching accuracy.
		%Moreover, we proposed an encoder-decoder based hierarchical document structure parsing system (DSPS) to tackle this problem. 
		%With adapting multi-modal pre-trained model GraphDoc and carefully designed Mask-op GRU decoder, we obtain a absolute improvement of 5.1\% in HD2S  and 7.2\% in HD2H  compared to the baseline method.
		%All scripts and datasets have been made public available at https://anonymous.4open.science/r/HierDoc-V2.
		
		%V2
		%we propose a hierarchical and fine-grained document structure reconstruction task and release a large 
		%The HRDS dataset is composed of 1,000 digital PDF files downloaded from ACL website, with 700 documents serves as training set and the rest 300 as testing set. All PDF files in HRDS are compiled using the same LaTex template, thus sharing vary similar layout structures.
		%The HRDH dataset is composed of 1,500 digital PDF files obtained from arXiv website, with 1,000 documents serves as training set and the rest 500 as testing set. These PDF files are selected from varied subjects and compiled using varied templates, introducing more challenges to the reconstruction tasks.
		%		, which is a rising demand in comprehensive document automation processing.
		%		 by considering three aspects: node classification, parent finding, and relationship matching accuracy.
		
		The problem of document structure reconstruction refers to converting digital or scanned documents into corresponding semantic structures.
		Most existing works mainly focus on splitting the boundary of each element in a single document page, neglecting the reconstruction of semantic structure in multi-page documents.
		This paper introduces hierarchical reconstruction of document structures as a novel task suitable for NLP and CV fields.
		To better evaluate the system performance on the new task, we built a large-scale dataset named HRDoc, which consists of 2,500 multi-page documents with nearly 2 million semantic units.
		Every document in HRDoc has line-level annotations including categories and relations obtained from rule-based extractors and human annotators.
		Moreover, we proposed an encoder-decoder-based hierarchical document structure parsing system (DSPS) to tackle this problem. 
		By adopting a multi-modal bidirectional encoder and a structure-aware GRU decoder with soft-mask operation, the DSPS model surpass the baseline method by a large margin.
		All scripts and datasets will be made publicly available at https://github.com/jfma-USTC/HRDoc.
	\end{abstract}
	
	\section{Introduction}
	
	\begin{figure}[t]
		\centering
		\subfigure[Multi-page documents]{
			\begin{minipage}[t]{0.5\linewidth}
				\centering
				\includegraphics[width=1.3in]{1a.jpg}
				%\caption{fig1}
				\label{fig:1a}
			\end{minipage}%
		}%
		\subfigure[Line-level classification]{
			\begin{minipage}[t]{0.5\linewidth}
				\centering
				\includegraphics[width=1.3in]{1b.jpg}
				%\caption{fig2}
				\label{fig:1b}
			\end{minipage}%
		}%
		%这个回车键很重要 \quad也可以
		\quad
		\subfigure[Hierarchical relationship]{
			\begin{minipage}[t]{0.5\linewidth}
				\centering
				\includegraphics[width=1.3in]{1c.jpg}
				%\caption{fig2}
				\label{fig:1c}
			\end{minipage}
		}%
		\subfigure[Semantic stucture]{
			\begin{minipage}[t]{0.5\linewidth}
				\centering
				\includegraphics[width=1.3in]{1d.jpg}
				%\caption{fig2}
			\end{minipage}
			\label{fig:1d}
		}%
		\centering
		\caption{The processing procedure of hierarchical document structure reconstruction task. }
		\label{fig:taskintro}
	\end{figure}
	
	With the prosperity of commercial activities in today's society, different documents are used to convey information, leading to a growing demand for automatic document processing \cite{cui2021document}.
	%As for choosing the information convertor,  PDF (Portable Document Format) has become the most popular format for publishing e-books nowadays. This popularity mainly comes from the wrapping ability of the PDF document, i.e., it wraps or embeds many types of data (images, forms, texts, fonts etc.). It also restricts the direct editing of contents which makes them more secure \cite{Pitale12}. 
	As an emerging application scenario in document processing, extracting or formalizing the structure of numerous documents is of great value to users. 
	For example, the document structure is required when converting a PDF file to an editable format such as Markdown.
	
	Current studies \cite{zhong_publaynet_2019, li_docbank_2020, pfitzmann_doclaynet_2022} focused predominantly on the document element detecting task and obtained considerable achievements in page-level layout analysis.
	%	However, these studies paid limited attention to the reconstruction of the document structures.
	Based on these works, some methods \cite{wang_docstruct_2020} and datasets \cite{rausch_docparser_2021} were proposed for extracting hierarchy structure in documents. Nonetheless, they neither achieved a satisfactory result nor provided a reasonable definition of the document structure reconstruction task.
	
	To move towards a more advanced understanding of hierarchical document structure reconstruction, we have created a new dataset named HRDoc, which consists of two parts according to the layout varieties. A multi-modal system is also proposed and demonstrates great superiority over the baseline method. 
	%Regarding to data sensitivity issue, we mainly focus our research on academic papers. 
	
	Figure \ref{fig:taskintro} depicts the general task definition and processing procedure. We define the document structure reconstruction task as converting one multi-page document (Figure \ref{fig:1a}) into its hierarchical semantic representation (Figure \ref{fig:1d}). 
	More specifically, we regard text lines, figures, tables, and equation areas as the essential elements of one document, which can be obtained through an off-the-shelf OCR engine \cite{Tesseract} or PDF extractor \cite{marinai2009metadata}. 
	After extracting these essential elements, we can use deep learning techniques to predict labels for each element unit (Figure \ref{fig:1b}) and the relation (Figure \ref{fig:1c}) between them.
	Combining the information above, we can recover the document's hierarchical semantic structure as shown in Figure \ref{fig:1d}.
	
	%Compared with arXivdocs \cite{rausch_docparser_2021}, which also focus on document structure parsing problem, our dataset differs in that it provides line-level annotations and cross-page relation reconstruction task.
	
	Our main contributions are listed as follows:
	\begin{itemize}
		\item We introduced the hierarchical reconstruction of document structures as a novel vision and language task.
		\item We built a novel hierarchical document structure reconstruction dataset HRDoc, 
		%		containing 2,500 line-level annotated documents with more than 3 million basic semantic units in total. 
		which is the first dataset that focuses on fine-grained and document-level structure reconstruction tasks over multi-page documents.
		\item We proposed an encoder-decoder-based hierarchical document structure parsing system (DSPS).
		%		, which presents the first multi-modal framework for recovering the semantic structure across all pages in one document. 
		%		Unlike previous works, we use the multi-modal bidirectional extractor and adopt an domain-adaptive GRU decoder with the soft-mask operation to recover the relation between these units. 
		%		With all these efforts, DSPS achieved considerable improvement compared to the baseline method.
		With the multi-modal bidirectional extractor and the structure-aware GRU decoder with soft-mask operation, DSPS achieved considerable improvement over the baseline method.
	\end{itemize}
	%Inspired by \cite{siegel_extracting_2018,li_docbank_2020,pfitzmann_doclaynet_2022}, we use Cascade-RCNN to
	The HRDoc dataset and source code of the proposed DSPS framework will be made publicly available here\footnote{https://github.com/jfma-USTC/HRDoc}.
	
	
	
	\section{Related Work}
	
	\subsection{Document Layout Analysis} 
	The most relevant task with document structure reconstruction in the document understanding field is layout analysis, which mainly focuses on document object detection and recognition of specific types such as figures and tables.
	
	Early works of document layout analysis relied on various heuristic rules or conventional image process methods to locate semantic regions in document images \cite{jaekyu_ha_recursive_1995, simon_fast_1997, kumar_text_2007}.
	In \citeyear{jaekyu_ha_recursive_1995}, \citeauthor{jaekyu_ha_recursive_1995} proposed a X-Y cut algorithm to determine the boundaries between each document component in a top-down manner. 
	\citeauthor{simon_fast_1997} regarded the connected components as the basic units in a binary document image, which are further combined into higher-level structures through heuristics methods and labeled according to structural features.
	\citeauthor{kumar_text_2007} used matched wavelets and MRF model to segment document images into text, background, and picture components at a pixel level.
	These approaches often used complicated hyper-parameters designed for specific document layouts, which are difficult to adapt to new document types. 
	
	Recent works take advantage of the wide-spreading machine learning techniques of object detection on natural scene images to address complex layout analysis problems \cite{he_multi-scale_2017, gao_deep_2017, yi_cnn_2017, li_page_2018}. 
	\citeauthor{he_multi-scale_2017} proposed a two-stage approach to detect tables and figures. In the first stage, the class label for each pixel is predicted using a multi-scale convolutional neural network. Then in the second stage, heuristic rules are applied to the pixel-wise class predictions to get the object boxes. \citeauthor{gao_deep_2017} utilized meta-data information from PDF files and detected formulas using a model that combines CNN and RNN. \citeauthor{yi_cnn_2017} redesigned the CNN architecture of common object detectors by considering the uniqueness of document objects. \citeauthor{li_page_2018} generated the primitive region proposals from each column region and clustered the primitive proposals to form a single object instance.
	
	These works mainly focus on splitting the boundary of each element in a single document page, neglecting the reconstruction of semantic structure in multi-page documents.
	%Some datasets are proposed for this task \cite{li_docbank_2020,zhong_publaynet_2019,pfitzmann_doclaynet_2022}.
	
	
	
	\subsection{Document Structure Reconstruction}
	Document structure reconstruction means recovering the hierarchical semantic information of one document.
	Existing works on document structure reconstruction can be roughly divided into two categories.
	
	The first one focuses on reconstructing only part of the structure information in one document, such as the table of contents (ToC) \cite{wu2013table, tuarob_hybrid_2015, nguyen_enhancing_2017, bentabet-etal-2020-financial}.
	\citeauthor{tuarob_hybrid_2015} listed several position-related rules and used a random forest algorithm to determine whether one text line is a section name or not. With all sections detected, a set of regular expressions and rule-based strategies were used to recover the hierarchical structure of sections.
	\citeauthor{nguyen_enhancing_2017} proposed a system consisting of a ToC page detection method and a link-based ToC reconstruction method to solve the ToC extraction problem.
	These works solved part of the document structure reconstruction problem, but the main body of one document, including content lines, figures, and tables, is not taken into consideration.
	
	The second one focuses on the overall structure reconstruction of document \cite{namboodiri_document_2007, wang_docstruct_2020, rausch_docparser_2021}. 
	\citeauthor{namboodiri_document_2007} proposed a framework of generic document structure understanding system in multi-steps. They split the processing procedure into two parts, including physical layout detection and logical structure recovering, with each part processed by digital image processing techniques and rule-based systems.
	\citeauthor{wang_docstruct_2020} focused on the form understanding task and considered the form structure as a tree-like hierarchy of text fragments. 
	They learned an asymmetric parameter matrix to predict the relationship between every text fragment pair, resulting in high computational complexity when handling documents with numerous text fragments.
	\citeauthor{rausch_docparser_2021} built a layout detection model based on Mask R-CNN \cite{MaskRCNN} and recovered structures of single document pages using a series of specific rules such as overlap ratio and reading flow. However, this method is difficult to apply to recover the structure of multi-page documents and lacks generalization due to the complicated heuristic rules introduced.
	%	These works neither achieved a satisfactory result nor provided a reasonable definition of the document structure reconstruction task.
	%	However, these works only focused on structure reconstruction on single document page, 
	
	
	\begin{figure*}[!htb]
		\centering
		\centering
		\includegraphics[width=1.0\linewidth]{layout.jpg}
		%\caption{fig1}
		\centering
		\caption{Layout varieties of front document pages in the HRDoc dataset. The first front page is from the HRDoc-Simple dataset, with all the rest randomly selected from the HRDoc-Hard in a color version. }
		\label{fig:layout}
	\end{figure*}
	
	
	\section{The HRDoc Task and Dataset}
	In this section, we define the tasks introduced in HRDoc and then describe the data collection process.
	% concerning the task’s input and output.
	%\subsection{Overview}
	\subsection{Task Overview}
	We build HRDoc with line-level annotations and cross-page relations that support both NLP and CV research.
	HRDoc dataset aims to recover the semantic structure of the PDF document, which can be divided into three subtasks, including semantic unit classification, parent finding, and relation classification. 
	The detailed descriptions for these tasks are listed as followings:
	
	\begin{itemize}
		\item \textsc{Overall Task} (Document hierarchical reconstruction.) Given a multi-page document $D$, recover the semantic structure of this document.
		\item \textsc{SubTask 1}  (Semantic unit classification.) Given all semantic units $U$ with bounding boxes and texts within a document, classify each unit to its semantic label.
		\item \textsc{SubTask 2}  (Parent finding.) Given a semantic unit $u_i$ in a document, find its nearest parent unit $p_i$.
		\item \textsc{SubTask 3}  (Relation classification.) Classify the relation $r_i$ between each semantic unit and its parent unit.
	\end{itemize}  
	
	
	\subsection{Dataset Collection}
	HRDoc dataset consists of two parts according to the layout varieties. 
	We name the first part HRDoc-Simple (HRDS), consisting of 1,000 documents with similar layouts, and the second part HRDoc-Hard (HRDH), consisting of 1,500 documents with varied layouts.
	
	To build the HRDS dataset, we download 1,000 conference papers in PDF format from ACL\footnote{https://aclanthology.org/} website, containing three top-tier conferences, including NAACL, EMNLP, and ACL in the natural language process field. 
	For the HRDH dataset, we downloaded 1,500 documents from arXiv\footnote{https://export.arxiv.org/} website, containing more than 30 varied layouts from 17 research areas. Moreover, thanks to the open-access policy of the arXiv website, we can download the LaTeX source code with PDF files simultaneously for further detecting different semantic regions.
	As shown in Figure \ref{fig:layout}, documents in HRDoc-Simple share a similar layout with the same ACL conference template. In contrast, documents in HRDoc-Hard have more varieties in page layout, containing single-column, two-column, and more complicated formats.
	
	Notably, all downloaded papers from the ACL website are licensed under CC BY-NC-SA 3.0\footnote{https://creativecommons.org/licenses/by-nc-sa/3.0/} or CC BY 4.0\footnote{https://creativecommons.org/licenses/by/4.0/}.
	For the arXiv website, we only download papers licensed under CC BY-NC-SA 4.0\footnote{https://creativecommons.org/licenses/by-nc-sa/4.0/} with compilable LaTeX source code. These licenses allow people to remix, transform, and build upon the material for non-commercial purposes.
	
	
	
	\subsection{Semantic Unit Parsing and Relation Definition}
	The unit classification task introduced in HRDoc is similar to that of PubLayNet  \cite{zhong_publaynet_2019} and DocBank \cite{li_docbank_2020}. This paper classifies all semantic units into 14 classes, including \{\textit{Title, Author, Mail, Affiliation, Section, First-Line, Para-Line, Equation, Table, Figure, Caption, Page-Footer, Page-Header, and Footnote}\}. 
	All class units are detected and labeled in text line level except for \textit{Equation, Table and Figure}, since they may contain multi-line texts.
	Each semantic unit comes after its corresponding parent unit when organized across every page in the human reading order. 
	The relation between one semantic unit and its parent unit consists of three types: \{\textit{Connect, Contain, Equality}\}. The definition of these relations is listed as follows:
	\begin{itemize}
		\item \textit{Connect} is used when two units are semantically connected. For example, the relation between continuous lines in a paragraph is called \textit{Connect}.
		\item \textit{Contain} is used when the child unit is one part of the parent unit. For example, the relation between a section line and its first subsection line is called \textit{Contain}.
		\item \textit{Equality} is used when two units are in the same hierarchical structure level. For example, the relation between different subsections under the same section is called \textit{Equality}.
	\end{itemize}
	
	
	
	\subsection{Ground-Truth Annotation}
	\label{pdf_parser}
	For the HRDS dataset, we design a rule-based PDF parser system to extract each text line in ACL papers using PDFPlumber\footnote{https://github.com/jsvine/pdfplumber}. 
	Owing to the fixed document layout of the HRDS dataset, we can roughly tag each text line to its labels using heuristic rules. For example, we can locate the first line of each paragraph by combining 1$)$ The distance between the current text line and previous text line; 2$)$ The indent before the beginning of the current text line; 3$)$ Whether the first character is upper-cased or lower-cased.
	
	For HRDH dataset, following \cite{li_docbank_2020}, we insert the `color' command with varied RGB parameters to different semantic code block and re-compile the LaTeX source code to get the colored PDF file as shown in Figure \ref{fig:layout}.
	After all PDF files are colored, we design another rule-based PDF parser system similar to that used in building the HRDS dataset. We can use both heuristic rules and color information of each character to label text lines roughly.
	To eliminate the influence of inserted `color' command such as line-shifting and font-size changing, we only keep papers whose text lines remain in their original position compared to those of downloaded PDF files.
	%we change the color of different semantic blocks to black and re-compile all PDF files to generate a document without specific color.
	
	Since semantic units of certain classes (\textit{Equation, Table, and Figure}) contain more than one text line, 
	we use Cascade-RCNN \cite{CascadeRCNN} to locate their bounding boxes for both datasets.
	%With a rough label
	After all semantic units are located and roughly labeled, we ask the human annotators to recheck the labels and assign one parent unit with the proper relation for each semantic unit.
	%they will be labelled with corresponding parent unit and the relationship between each pair by human annotators.
	
	\begin{figure}[!t]
		\centering
		\centering
		\includegraphics[width=0.9\linewidth]{class_statistic.pdf}
		\caption{Statical results on numbers of different document semantic units of HRDoc dataset. }
		\label{fig:unit_num}
	\end{figure}
	
	
	\subsection{Statistic Analysis}
	HRDoc contains 2,500 documents with nearly 2 million semantic units. Figure \ref{fig:unit_num} provides the statistics of semantic unit distribution over the train and test set of both HRDS and HRDH datasets.
	We can see that the \textit{Page-Header} entity is missing in HRDS dataset since the ACL template contains no header contents. Moreover, the distributions of all classes in HRDS and HRDH are pretty different.
	Figure \ref{fig:parent_num} provides the child-parent relation distribution over both HRDS and HRDH dataset. We can see that every semantic entity only has parents in certain entity types under the constraint of grammar rules.
	
	
	
	\begin{figure}[!t]
		\centering
		\centering
		\includegraphics[width=0.9\linewidth]{pcrd.pdf}
		\caption{Statical results on parent ratio of different document semantic units of HRDoc dataset. } 
		\label{fig:parent_num}
	\end{figure}
	
	
	
	\begin{figure*}[!htb]
		\centering
		\centering
		\includegraphics[width=1.0\linewidth]{Model-V2.pdf}
		%\caption{fig1}
		\centering
		\caption{Proposed document structure parsing system (DSPS).  }
		\label{fig:overall}
	\end{figure*}
	
	\section{Proposed Method}
	The proposed hierarchical document structure parsing system (DSPS) comprises a multi-modal bidirectional encoder, a structure-aware GRU decoder with soft-mask operation, and a relation classifier. In this section, we first introduce the main structure of the DSPS model and give a detailed description of sub-modules in the following subsections.
	
	As shown in Figure \ref{fig:overall}, 
	%a multi-page document (left) is first parsed using OCR engine (image-based) or pdf-miner (digital-based) to get each semantic units' positions and texts. 
	different types of embeddings for each semantic unit are calculated and fed into the encoder.
	After obtaining their multi-modal representations, these semantic units are further classified into different categories (\textsc{SubTask 1}). 
	Moreover, using a structure-aware GRU decoder with soft-mask operation, we can find the corresponding parent unit for each semantic unit (\textsc{SubTask 2}). 
	After obtaining parent-child pairs, we can classify the relation of each pair through the relation classifier module (\textsc{SubTask 3}). 
	The final structure of the whole document can be easily recovered through a post-processing procedure  (\textsc{Overall Task}).
	
	\subsection{Document Images and Semantic Units}
	
	To make use of visual information, we convert a $K$-page document $D$ into $K$ images $I = \{I_1, I_2, .., I_K\}$ for every page using PyMuPDF\footnote{https://github.com/pymupdf/PyMuPDF}. 
	As mentioned in Section \ref{pdf_parser}, we regard each text line, figure, table, and equation area in one document as the basic semantic unit.
	%, which can be detected using OCR engine (image-based) or parsed by PDF extractor (digital-based). In this paper we can locate each units as described in Section \ref{pdf_parser}.
	Each semantic unit $u_i$ for a $K$-page document $D$ can be identified by its text $s_i$, bounding box $b_i = (x_{\text{min}}, y_{\text{min}}, x_{\text{max}}, y_{\text{max}})_i$ and page number $p_i \in \{1,...,K\}$.
	
	The text $s_i$ is set to `Table' for \textit{Table} and `Figure' for \textit{Figure} units, while the other remains original texts extracted from PDF files using the bounding box $b_i$.
	Here $(x_{\text{min}}, y_{\text{min}})_i$ and $(x_{\text{max}}, y_{\text{max}})_i$ are the coordinates of the top-left and bottom-right corners of the $k$-th semantic unit's bounding box.
	We arrange each semantic unit in one document page in reading order, forming the input unit sequence $U = \{u_1, u_2, ... u_L\}$, where $L$ is the total unit number within the page with a max length limit of 512.
	
	\subsection{Multi-Modal Bidirectional Encoder}
	Similar as \cite{graphdoc}, the input embeddings of semantic units are consisted of a sentence embedding $x^{\text{text}}$, a layout embedding $x^{\text{layout}}$, a 1D positional embedding $x^{\text{pos}}$, a visual embedding $x^{\text{vis}}$ and a page embedding $x^{\text{page}}$. 
	
	In total, the embedding $x_i \in \mathbb{R}^H$ at the $i$-th position in the input sequence is given as:
	$$x_i = \text{LN}(x_i^{\text{text}}+ x_i^{\text{layout}} + x_i^{\text{pos}} + x_i^{\text{vis}} + x_i^{\text{page}}) $$
	Here $\text{LN}(\cdot)$ is the layer normalization operation \cite{LN}.
	\subsubsection{Sentence embedding.}
	We use Sentence-Bert \cite{SentenceBert} model as the semantic extractor.
	The sentence embedding can be calculated as follows:
	$$ x_i^{\text{text}} = \text{LinearProj}(\text{SentenceBert}(s_i)) $$
	Here the $\text{LinearProj}(\cdot)$ is a linear projection layer to transform the embedding dimension to $H$.
	\subsubsection{Layout embedding.}
	Following LayoutLMv2 \cite{layoutlmv2}, we introduce a location representation $x_i^{\text{layout}} \in \mathbb{R}^{H}$ that convey the layout information of $i$-th semantic token in the document page.
	%	\begin{equation*}
	%		\begin{split}
	%			x_i^{\text{layout}_x} & = \text{Emb}(x_\text{min}, x_\text{max}, w)_i \\
	%			x_i^{\text{layout}_y} & = \text{Emb}(y_\text{min}, y_\text{max}, h)_i \\
	%			x_i^{\text{layout}}     & = \text{Concat}(x_i^{\text{layout}_x}, x_i^{\text{layout}_y})
	%		\end{split}
	%	\end{equation*}
	$$x_i^{\text{layout}} = \text{Concat}(\text{Emb}(x_\text{min}, x_\text{max}, w)_i, \text{Emb}(y_\text{min}, y_\text{max}, h)_i)$$
	%	Here $w=x_\text{max}-x_\text{min}$ and $h=y_\text{max}-y_\text{min}$
	\subsubsection{Position embedding.}
	We use absolute position embedding here as 
	$$x_i^{\text{pos}} = \text{Emb1D}(i)$$
	\subsubsection{Visual embedding.}
	The visual clue of one semantic unit is also crucial since the font styles may differ between different semantic blocks.
	We use ResNet-50 \cite{ResNet} with FPN \cite{fpn} as the backbone of vision information extractor. 
	The local visual embedding can be obtained through RoIAlign \cite{MaskRCNN} according to $b_i$:
	$$x_i^{\text{vis}} = \text{RoIAlign}(\text{ResNet}(I_{p_i}), b_i)$$
	\subsubsection{Page embedding.}
	To distinguish semantic units of different pages, we use absolute position embedding here as 
	$$x_i^{\text{page}} = \text{EmbPage}(p_i)$$
	
	The multi-modal bidirectional encoder is based on Transformer \cite{Transformer} architecture and produces representations $x_i^{*}=\text{BiEncoder}(x_i)$ for each semantic unit.
	
	We can predict the label $\hat{C}_i \in \{1,2,\cdots,C\}$ for unit $u_i$ as follows:
	$$P_{\text{cls}_i} = \text{softmax}(\text{LinearProj}(x_i^{ *}))$$
	$$\hat{C}_i = \text{argmax}(P_{\text{cls}_i})$$
	%where $LinearProj$ is a linear projection function of mapping $x_i^{ *}$ to 14 classes introduced in Section \ref{pdf_parser}, 
	where $P_{\text{cls}_i}$ means the classification probability for $i$-th unit over predefined $C$ classes.
	
	
	
	
	\subsection{Structure-Aware GRU Decoder}
	When reconstructing the hierarchical structure of one multi-page document, we often meet the cross-page parent-finding problem, which means a semantic unit may find its parent in previous pages. To address this problem, we first concatenate each semantic unit within a document in reading order and use a GRU \cite{GRU} network to capture the information exchange across pages.
	
	
	\begin{figure}[t!]
		\centering
		\centering
		\includegraphics[width=0.95\linewidth]{Decoder-V3.pdf}
		\caption{Proposed structure-aware GRU decoder. 
			%		We concatenate each text line's fused embeddings of different pages together in page ascending order (bottom). These embeddings are fed into a GRU unit to get corresponding hidden states H$_i$. Meanwhile, relaying on the classification result of task 1 depicted in Figure \ref{fig:overall}, each node can obtain their potential decoding area through predefined rules. Attention mechanism is further adopted to locate the parent node of current decoding node. 
			}
		\label{fig:decoder}
	\end{figure}
	
	When decoding the parent unit, some semantic unit may directly link to the \textit{ROOT} unit, which represents the beginning of one document.
	The $x_{0}^{ *}$ referring to the multi-modal representation of the \textit{Root} unit is obtained using the average of all semantic units' representations:
	$$x_{0}^{ *} = \sum_{i=1}^{L}x_{i}^{ *}/L$$
	
	%We use a GRU \cite{GRU} network as the decoder to find the corresponding parent unit for each semantic unit.
	With multi-modal representations $x_i^{ *}$ as input, the GRU unit produces the hidden state $h_i$ of the current semantic unit:
	$$ h_i = \text{GRU}(x_{i-1}^{ *}, h_{i-1}) $$
	
	In decoding step $i$, we can calculate the weighted hidden state of $u_i$ using the attention pooling function:
	$$q_{i} = \sum_{j=0}^{i}\alpha(h_i, h_j)h_j$$
	%where $\alpha(\cdot)$ function is:
	where
	\begin{equation*}
		\begin{split}
			\alpha(h_i, h_j) & = \text{softmax}((\mathbf{W_q} h_i)^{T}\cdot \mathbf{W_k} h_j) \\
			& = \frac{\text{exp}((\mathbf{W_q} h_i)^{T}\cdot \mathbf{W_k} h_j)}{\sum_{j=0}^{i}\text{exp}((\mathbf{W_q} h_i)^{T}\cdot \mathbf{W_k} h_j)}
		\end{split}
	\end{equation*}
	
	\subsubsection{The soft-mask operation.} We introduce a soft-mask operation to adjust the attention distribution for better use of domain-specific knowledge. 
	More specifically, we calculate the child-parent distribution matrix $\mathbf{M_{cp}} \in \mathbb{R}^{(C+1)\times C}$ by counting different types of child-parent pairs as shown in Figure \ref{fig:parent_num}. The $i$-th column of $\mathbf{M_{cp}}$ means the probability distribution of class $i$ over $C$ predefined classes plus one \textit{ROOT} node. To make the distribution robust to unseen relation pairs, we apply additive smoothing to each column with the pseudo-count set to 5. The probability of semantic unit $u_j$ being the parent of $u_i$ can be calculated as:
	$$P_{\text{par}_{(i,j)}} = \text{softmax}(\alpha (q_i, h_j) h_jP_{\text{dom}_{(i,j)}})$$
	Here, $P_{\text{dom}_{(i,j)}}$ is defined as:
	$$P_{\text{dom}_{(i,j)}} = \widetilde{P}_{\text{cls}_j} \mathbf{M_{cp}} P^\mathrm{T}_{\text{cls}_i}$$
	%Here, $\hat{P}_{\text{cls}_j}$ is defined as:
	where
	\begin{equation*}
		\widetilde{P}_{\text{cls}_j} =  
		\begin{cases}
			\text{Concat}(P_{\text{cls}_j}, 0) & j \in \{1,2,\cdots,i-1\};\\
			[\overbrace{0,0, ... , 0}^{C}, 1] & j =0.
		\end{cases}
	\end{equation*}
	
	We can calculate the parent node $\hat{P}_i \in \{0, 1,\cdots,i-1\}$ for $u_i$ as:
	$$\hat{P}_i = \text{argmax}(P_{\text{par}_{(i,j)}})$$
	\subsection{Relation Classifier}
	After obtaining the parent node $u_j$ for semantic unit $u_i$, we use a linear project function to classify the relation between each child-parent pair $(u_i, u_j)$ as:
	
	$$\hat{R}_{(i,j)} = \text{argmax}(P_{\text{rel}_{(i,j)}})$$
	$$P_{\text{rel}_{(i,j)}} = \text{softmax}(\text{LinearProj}(\text{Concat}(h_i,h_j)))$$
	where $P_{\text{rel}_{(i,j)}}$ means the classification probability for relation of child-parent pair $(u_i, u_j)$ over predefined $N$ relation types.
	
	\subsection{Multi-Task Learning}
	We take the three subtasks into consideration simultaneously.
	For \textsc{SubTask 1}, the loss function can be listed as:
	$$L_{\text{cls}} = \sum_{i=1}^{L}\text{FocalLoss}(C_{i}, P_{\text{cls}_{i}})/L$$
	For \textsc{SubTask 2}, the loss function can be listed as:
	$$L_{\text{par}} = \sum_{i=1}^{L}\text{CrossEntropy}(P_{i}, P_{\text{par}_{(i,j)}})/L$$
	For \textsc{SubTask 3}, the loss function can be listed as:
	$$L_{\text{rel}} = \sum_{i=1}^{L}\text{FocalLoss}(R_{i}, P_{\text{rel}_{(i,j)}})/L$$
	Here $C_{i}, P_{i}, R_{i}$ refer to the ground-truth label for the semantic class, parent position and the relation type for $u_i$ in one-hot representation.
	$\text{FocalLoss}(\cdot)$ \cite{focalloss} is also introduced to solve the class-imbalance problem.
	We calculate the weighted sum of these loss functions to form the final loss as:
	$$L_{\text{tol}} = L_{\text{cls}} + \alpha_1 L_{\text{par}} + \alpha_2 L_{\text{rel}}$$
	
	
	
	\begin{table*}[htp]
		\centering
		\scalebox{0.8} 
		{		
			\begin{tabular}{c ccccccc ccccccc cc}
				\toprule
				\multirow{2}{*}[-4pt]{Method} & \multicolumn{16}{c}{HRDoc-Simple \space \space \space F1(\%)}  \\ 
				\cmidrule(lr){2-17}
				%['sec', 'fnote', 'title', 'author', 'foot', 'equ', 'header', 'cap', 'mail', 'para', 'tab', 'fig', 'affili', 'fstline']
				& Title& Author & Mail& Affili&Sect&Fstl&Paral&Table&Fig&Cap&Equ&Foot&Head&Footn&Micro&Macro \\
				\midrule
				T1 & 78.83 & 72.74 & 64.54 & 70.13 & 91.35 & 87.53 & 89.70 & 89.30 & 73.87 & 64.87 & 83.87 & 87.50 & - & 79.32 & 88.30 & 80.85  \\
				T2 & 93.67 & 82.53 & 81.33 & 84.39 & 37.09 & 38.39 & 91.86 & 58.44 & 48.53 & 70.75 & 26.89 & 98.33 & - & 49.76  & 85.61 & 66.30 \\
				T3 & 98.98 & 96.47 & \textbf{98.95} & \textbf{97.42} & 97.30 & 93.27 & 98.72 & 94.42 & 95.72 & 93.36 & 96.02 & 99.89 & - & 87.11 & 97.74 & 95.97  \\
				T4 & \textbf{99.43} & \textbf{98.83} & 96.45 & 97.33 & \textbf{99.60} & \textbf{98.22} & \textbf{99.74} & \textbf{100.00} & \textbf{99.95} & \textbf{99.06} & \textbf{97.91} & \textbf{100.00} & - & \textbf{99.15} & \textbf{99.52} & \textbf{98.90}  \\
				\midrule
				\multirow{2}{*}[-4pt]{Method} & \multicolumn{16}{c}{HRDoc-Hard \space \space \space \space F1(\%)}  \\ 
				\cmidrule(lr){2-17}
				%['sec', 'fnote', 'title', 'author', 'foot', 'equ', 'header', 'cap', 'mail', 'para', 'tab', 'fig', 'affili', 'fstline']
				& Title& Author & Mail& Affili&Sect&Fstl&Paral&Table&Fig&Cap&Equ&Foot&Head&Footn&Micro&Macro \\
				\midrule
				T1 & 81.50 & 49.77 & 33.39 & 49.34 & 75.92 & 64.96 & 77.86 & 69.96 & 72.22 & 43.72 & 68.84 & 70.91 & 71.00 & 52.67 & 73.37 & 64.94  \\
				T2 & 82.40 & 48.40 & 18.43 & 61.33 & 33.66 & 45.37 & 87.99 & 21.89 & 70.28 & 61.54 & 48.32 & 73.69 & 75.71 & 6.79& 79.25 & 52.56   \\
				T3& 95.85 & 89.92 & \textbf{91.68} & \textbf{91.75} & 94.26 & 88.68 & 96.77 & 76.96 & 91.67 & 91.99 & 93.94 & 94.68 & 92.65 & 62.61 & 94.68 & 89.53  \\
				T4& \textbf{97.71} &\textbf{93.93} & 85.49 & 90.95 & \textbf{96.06} & \textbf{91.24} & \textbf{97.96} & \textbf{100.0} & \textbf{100.0} & \textbf{97.32} & \textbf{97.92} & \textbf{98.54} & \textbf{97.83} & \textbf{88.84} & \textbf{96.74} & \textbf{95.27}  \\
				\bottomrule
			\end{tabular}
		}
		\caption{Comparison results of different baseline models in semantic unit classification task. F1 means F1-score. T1 refers to Cascade-RCNN, T2 refers to ResNet+RoIAlign, T3 refers to Sentence-Bert, T4 refers to DSPS Encoder.}
		\label{tab:classify_compare}
	\end{table*}
	
	\begin{table*}[htp]
		\centering
		\scalebox{0.8} 
		{
			%		\renewcommand{\arraystretch}{1.3}
			\begin{tabular}{l cccccccc}
				\toprule
				\multirow{2}{*}[-4pt]{Method} &\multirow{2}{*}[-4pt]{Semantic} &\multirow{2}{*}[-4pt]{Vision} & \multirow{2}{*}[-4pt]{Soft-Mask} & \multirow{2}{*}[-4pt]{Level}  & \multicolumn{2}{c}{HRDoc-Simple}   & \multicolumn{2}{c}{HRDoc-Hard}  \\ 
				\cmidrule(lr){6-7}\cmidrule(lr){8-9}
				&&&&&Micro-STEDS&Macro-STEDS&Micro-STEDS&Macro-STEDS \\ 
				%['sec', 'fnote', 'title', 'author', 'foot', 'equ', 'header', 'cap', 'mail', 'para', 'tab', 'fig', 'affili', 'fstline']
				\midrule
				DocParser & - & - & - & Page & 0.2361 & 0.2506 & 0.1873 & 0.2015 \\
				DSPS & \xmark & \cmark & \xmark & Document & 0.6149 & 0.6284 & 0.5145 & 0.5393 \\
				DSPS & \cmark & \xmark & \xmark & Document & 0.6636 & 0.6690 & 0.5766 & 0.5817 \\
				DSPS & \cmark & \cmark & \xmark & Document & 0.6830 & 0.6888 & 0.5811 & 0.5968 \\
				DSPS & \cmark & \cmark & \cmark & Document & \textbf{0.8143} & \textbf{0.8174} & \textbf{0.6903} & \textbf{0.6971} \\
				DSPS & \cmark & \cmark & \cmark & Page & 0.6482 & 0.6562 & 0.6080 & 0.6243 \\
				\bottomrule
			\end{tabular}
		}
		\caption{Comparison results of different models in hierarchical document reconstruction task.}
		\label{tab:e2e_compare}
	\end{table*}
	%\begin{table}[htp]
	%	\centering
	%	\scalebox{0.87} 
	%	{
	%		\begin{tabular}{l cccc}
	%			\hline
	%			\multirow{2}{*}{Setting}  & \multicolumn{2}{c}{HRDS}   & \multicolumn{2}{c}{HRDH}  \\ 
	%			\cline{2-5} 
	%			&Micro-ST&Macro-ST&Micro-ST&Macro-ST \\ 
	%			%['sec', 'fnote', 'title', 'author', 'foot', 'equ', 'header', 'cap', 'mail', 'para', 'tab', 'fig', 'affili', 'fstline']
	%			\hline
	%			Single-Page & - & - & - & - \\
	%			Cross-Page & - & - & - & - \\
	%			\hline
	%		\end{tabular}
	%	}
	%	\caption{Comparison results of DSPS in document reconstruction task with different page-level setting. HRDS refers to HRDoc-Simple, HRDH refers to HRDoc-Hard, ST refers to Semantic-TEDS.}
	%	\label{tab:single_page}
	%\end{table}
	%
	%
	%\begin{table}[htp]
	%	\centering
	%	\scalebox{0.87} 
	%	{
	%		\begin{tabular}{l cccc}
	%			\hline
	%			\multirow{2}{*}{Setting}  & \multicolumn{2}{c}{HRDS}   & \multicolumn{2}{c}{HRDH}  \\ 
	%			\cline{2-5} 
	%			&Micro-ST&Macro-ST&Micro-ST&Macro-ST \\ 
	%			%['sec', 'fnote', 'title', 'author', 'foot', 'equ', 'header', 'cap', 'mail', 'para', 'tab', 'fig', 'affili', 'fstline']
	%			\hline
	%			Grain-1 & - & - & - & - \\
	%			Grain-2 & - & - & - & - \\
	%			Grain-3 & - & - & - & - \\
	%			\hline
	%		\end{tabular}
	%	}
	%	\caption{Comparison results of DSPS in document reconstruction task with different grain-level setting. HRDS refers to HRDoc-Simple, HRDH refers to HRDoc-Hard, ST refers to Semantic-TEDS.}
	%	\label{tab:grain}
	%\end{table}
	
	
	
	\section{Experiments}
	%We conduct several evaluation experiments with our HRDoc dataset.
	
	\subsection{Compared Methods}
	\subsubsection{Semantic unit classification.}
	The \textsc{SubTask 1} of semantic unit classification can be viewed as both CV and NLP tasks.
	To this end, we use a state-of-the-art detection model Cascade-RCNN \cite{CascadeRCNN} to detect the position of each semantic unit.
	%implemented by MM-Detection \cite{mmdetection}.
	With bounding boxes and texts of each semantic unit given, we use the features outputted by ResNet-50 with RoIAlign and Sentence-Bert to classify each semantic unit.
	The proposed multi-modal bidirectional encoder is also used to classify each semantic unit.
	
	
	%\begin{figure}[th]
	%	\centering
	%	\centering
	%	\includegraphics[width=1.0\linewidth]{3grain.pdf}
	%	\caption{Three different grains in HRDoc dataset. } 
	%	\label{fig:three_grain}
	%\end{figure}
	
	
	\subsubsection{Hierarchical document reconstruction.}
	To evaluate the performance of DocParser \cite{rausch_docparser_2021} system in \textsc{Overall Task}, we do the inference stage of DocParser in HRDoc dataset by excluding the table structure and retrieving semantic units for each detected objects with an overlap ratio higher than 0.7.
	We also test the performance of the DSPS model with different settings.
	%use the Mask-RCNN model and inference script provided by 
	
	\subsection{Evaluation Metrics}
	\subsubsection{Semantic unit classification.}
	We use the standard F1 score on each category to evaluate the semantic unit classification task performance.
	For the output of the detection model, we only preserve those predicted bounding boxes with a confidence coefficient higher than 0.5. An output box is regarded as correctly predicted only when it has an overlap ratio higher than 0.65 with a ground-truth box in the same label.
	
	\subsubsection{Hierarchical document reconstruction.}
	For a given document $D$, we can get its content structure $T_D$ and predicted structure $\hat{T}_D$ in a tree-like format. 
	Inspired by \cite{TEDS}, 
	we proposed the Semantic-TEDS (Tree-Edit-Distance-Score) metric to evaluate the system performance on the document reconstruction task. 
	$$\text{STEDS}(T_D, \hat{T}_D) = 1-\frac{\text{EditDist}(T_D, \hat{T}_D)}{\text{max}(|T_D|, |\hat{T}_D|))}$$
	Here $|T|$ refers to the number of nodes in a tree structure $T$. Notice that we only regard one node in $T_D$ and $\hat{T}_D$ as completely matched when the label and text are the same. 
	
	
	%\subsection{Implementation Details}
	%We implement all the training and testing procedures of compared models in PyTorch and experiment on four NVIDIA A100 GPUS. 
	%All detection model is trained using the MM-Detection \cite{mmdetection} framework, and the proposed DSPS is implemented based on huggingface Transformers \cite{wolf2019huggingface}.
	%The $\alpha_1, \alpha_2$ parameters used in DSPS model is set to 2 and 0.25, respectively.
	%to keep $L_{\text{cls}}, L_{par}, L_{rel}$ in the same level.
	
	
	\subsection{Evaluation Results}
	\subsubsection{Semantic unit classification.}
	%	We conduct \textsc{SubTask 1} on both de
	The evaluation results of compared methods are shown in Table \ref{tab:classify_compare}.
	We can see that the proposed multi-modal bidirectional encoder has the best classification performance in most categories. 
	Sentence-Bert outperforms the DSPS encoder in specific categories, including \textit{Mail} and \textit{Affili}, which indicates that the visual clue may be harmful when facing visually similar units.
	Due to the complexity of document layouts in the HRDH dataset, all models show a decrease when dealing with the HRDH dataset compared to the HRDS dataset.
	
	\subsubsection{Hierarchical document reconstruction.}
	We conducted experiments on DocParser and the proposed DSPS model with different settings.
	As shown in Table \ref{tab:e2e_compare}, DSPS model outperform DocParser by a large margin. 
	We can see that both semantic and visual modalities are crucial for the DSPS model. Moreover, the soft-mask operation introduced in the decoder also brings considerable improvement.
	In the page-level setting for DSPS, we force the decoder to find the parent unit within the same document page for each semantic unit. 
	The system performance drops significantly in the page-level setting, indicating that the document reconstruction task should be conducted in a cross-page manner.
	%\subsubsection{The effect of cross-page modeling.}
	%As shown in Figure
	%\subsubsection{The effect of classification granularity.}
	
	\section{Conclusion}
	This paper introduced the hierarchical reconstruction of document structures as a novel vision and language task. 
	%	We take this complex task into three subtasks: semantic unit classification, parent finding, and relation classification.
	To facilitate research aimed at document reconstruction, we proposed a new dataset named HRDoc, which contains 2,500 line-level annotated documents with nearly 2 million semantic units. 
	%	Compared with existing document layout analysis datasets such as arXiv-doc and PubLayNet, our dataset provides a detailed semantic structure of academic papers with cross-page information.
	%	Our dataset contains more than 2 million semantic units with corresponding parent and relation type annotations.
	%	It requires a system to process information across different pages within a document.
	The proposed DSPS model is based on an encoder-decoder structure that takes multi-modal information as input. With both semantic and vision features considered, DSPS achieves the best classification performance surpassing other methods by a large margin.
	In the overall task, we proposed a structure-aware GRU decoder with domain-specific knowledge introduced.
	By training in an end-to-end manner, the DSPS model achieves a considerable improvement over the baseline model.
	
	% Use \bibliography{yourbibfile} instead or the References section will not appear in your paper
	%\nobibliography{aaai23}
	\bibliography{aaai23}
	
\end{document}