\documentclass{osa-article}

%% Select the journal you're submitting to
%% oe, boe, ome, osac, osajournal
\journal{oe}
% Key:
% Express journals must have the correct journal selected:
% {oe} Optics Express
% {boe} Biomedical Optics Express
% {ome} Optical Material Express
% {optcon} Optics Continuum
% Other OSA journals may use:
% {osajournal} Applied Optics, Advances in Optics and Photonics, Journal of the Optical Society of America A/B, Optics Letters, Optica, Photonics Research

% Uncomment if submitting to Photonics Research.
% ONLY APPLICABLE FOR \journal{osajournal}
% \setprjcopyright

% Set the article type
\articletype{Research Article}
% Note that article type is not required for Express journals (OE, BOE, OME and OSAC)

\usepackage{lineno}
\usepackage{gensymb} % for degree symbol
\usepackage{verbatim} % for multiline comments
\usepackage{color} % for TODOs and inline comments
\usepackage{xfrac}

\newcommand{\note}[1]{\textcolor{red}{#1}}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathtools} 
\usepackage{multirow}
\usepackage{cleveref}

\linenumbers

\begin{document}

\title{Noise Reduction in Underexposed Radiographic and Tomographic Multi-Channel Images with Self-Supervised Deep Learning}
% Underexposed Radiographs Enhancement with Self-Supervised Deep Learning
% Denoising of Radiographic and Tomographic Multi-channel images with Self-Supervised Deep Learning
% Enhancement of Underexposed Radiographic and Tomographic Images with Self-Supervised Deep Learning


%[Provisional title] Denoising of Low-Exposure Multi-Channel Tomography with Self-Supervised Deep Learning

\author{Yaroslav Zharov\authormark{1,5,*,$\dagger$}, 
Evelina Ametova\authormark{1,2,$\dagger$}, 
Tilo Baumbach\authormark{1,3}, 
Genoveva Burca\authormark{6,4,2}, 
% Alexey Ershov\authormark{1}, 
% Tomas Farago\authormark{3}, 
Vincent Heuveline\authormark{5},
% et. al?
% and 
% Janes Odar\authormark{3}
}

\address{\authormark{1}Laboratory for Applications of Synchrotron Radiation (LAS), Karlsruhe Institute of Technology, Germany\\
\authormark{2}Department of Mathematics, The University of Manchester, United Kingdom \\
\authormark{3}Institute for Photon Science and Synchrotron Radiation (IPS), Karlsruhe Institute of Technology, Germany\\
\authormark{4}ISIS Pulsed Neutron and Muon Source, STFC, UKRI, Rutherford Appleton Laboratory, UK\\
\authormark{5}Engineering Mathematics and Computing Lab (EMCL), Interdisciplinary Center for Scientific Computing (IWR), Heidelberg University, Germany,\\
\authormark{6}Diamond Light Source, Harwell Campus, Didcot, Oxfordshire, OX11 0QX, UK\\
\authormark{$\dagger$}Contributed equally}


\email{\authormark{*}yaroslav.zharov@kit.edu} %% email address is required

% \homepage{http:...} %% author's URL, if desired

%%%%%%%%%%%%%%%%%%% abstract %%%%%%%%%%%%%%%%
%% [use \begin{abstract*}...\end{abstract*} if exempt from copyright]

\begin{abstract}
% \LaTeX{} manuscripts submitted to Optica Publishing Group journals may use these instructions and this universal template format. The template simplifies manuscript preparation and eases transfer between journals. \emph{Applied Optics}, JOSA A, JOSA B, \emph{Journal of Optical Communications and Networking}, and \emph{Photonics Research} authors should use the length-check template if a precise length estimate is needed. \emph{Optics Letters} authors and authors of short \emph{Optica} articles are encouraged to use the length-check template. Authors using this universal template will still need to adhere to article-length restrictions based on the final, published format.


Noise reduction is an important issue for all radiographic or tomographic imaging techniques. It becomes particularly critical in applications where additional constraints force a strong reduction of the signal-to-noise ratio per image. These constraints may result from limitations on the maximum allowable dose and the associated restriction on radiation flux or exposure time. 
Often, however, a reduction of the SNR per image is accepted in order to be able to take more images of the same object, depending on further parameters and aiming to gain additional information. In these cases, the total available signal capacity is distributed over a larger number of input channels. These can be, for example, time channels in case of cine-radiography or cine-tomography - or energy channels in the case of spectroscopic, i.e., energy dispersive imaging. 
Conventional image denoising methods work on an image-per-image basis and rely on certain assumptions about image properties. Consequently, they perform well when the assumptions are met and fail otherwise. At the same time, tremendous progress in machine learning demonstrated that data-driven methods are much more flexible in accommodating various image characteristics. In this paper we demonstrate a method to improve the quality of multi-channel (time or energy-resolved) datasets. The method relies on the recent \emph{Noise2Noise} (N2N) self-supervised denoising approach that learns to predict a noise-free signal without access to noise-free data. N2N in turn requires drawing pairs of samples from a data distribution sharing identical signal while being exposed to different samples of random noise. As multi-channel images naturally share structural information and provide multiple noisy observations, the method is applicable with minor relaxations of the original constraints to accommodate data-specific features. We demonstrate several representative case studies, namely spectroscopic (k-edge) X-ray tomography, in-vivo X-ray radiography, and energy-dispersive (Bragg edge) neutron tomography. In all cases, the N2N method shows drastic improvement and outperforms conventional denoising methods. Hence, the method can be reliably used to shorten exposure times, thus dose, or improve the throughput of real-world imaging studies.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%  body  %%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Many imaging modalities rely on the penetration ability of radiation to visualize the interior of an object. Depending on the employed physical interaction, among others, absorption, scattering or phase-shift can be used to obtain contrast inside the object of interest. Either \emph{radiography} (single view) or \emph{tomography} (multiple views with subsequent volumetric image reconstruction) are commonly acquired. As particle emission is a stochastic process and a detector has finite detection efficiency, longer exposure time improves signal quality. However, there are many scenarios where sufficient exposure can not be acquired. A well-known example is \emph{in vivo} imaging. Here, the radiation dose ultimately limits the amount of information acquired~\cite{Moosmann2013}. Another example is spectroscopic imaging with a polychromatic beam where the acquired particle counts are shared across multiple energy bins leading to a significant increase of exposure times, hence, limiting the experiment throughput~\cite{warr2021enhanced}. Both imaging modes can be generalized as \emph{multi-channel} in the sense that multiple images share similar structural information.

Given the aforementioned physical constraints, we need to rely on image processing techniques to improve image quality and extract valuable data. A group of the methods used to improve image quality affected by noise is called \emph{denoising}. Similar to other domains, methods based on \emph{Machine Learning} (ML) have revolutionized denoising. In this paper, we demonstrate an ML approach to improve the quality of underexposed images in challenging applications such as spectroscopic (k-edge) X-ray tomography, in-vivo X-ray radiography, and energy-dispersive (Bragg edge) neutron tomography. The method is based on the recent Noise2Noise (N2N) self-supervised denoising approach~\cite{Lehtinen2018}. 
The N2N main assumption enabling the N2N method is formulated as follows.
Consider two images $I_1$ and $I_2$ that share the same structural information $S$ but are affected by the independent, identically distributed (iid) instances of noise $\sigma_1$ and $\sigma_2$.
If the model is trained to predict $I_1$, given $I_2$ as input, the best prediction possible is $S$, because the $\sigma_2$ is conditionally independent of $\sigma_1$, given $S$.

This paper is organized as follows. First, we outline the related work to put our work in context. Then, the N2N method details are followed by three rigorous case studies. Finally, a discussion of our findings in the context of multi-channel imaging is provided.


\section{Related Work}

Artifacts are inherent to digitally acquired images, as an acquisition function is subject to many uncertainties and in general, is not accurately known. The acquisition function includes optical distortion, lens and detector array heterogeneity, and inherent noise driven by the stochastic nature of photons. As optical distortion is a misplacement of information, a distortion map can be estimated and applied to compensate for it. For the lens and detector matrix heterogeneity, flat and dark field corrections are used. Flat and dark fields refer to the measurement of detector response with and without source illumination, respectively. Finally, denoising is used to compensate for the inherent stochastic noise. Hence, the denoising problem is to restore signal $S$ from a noisy observation $I$~\cite{Gonzalez2008}:

\begin{equation}
    I = S + \sigma(S)
\end{equation}

\noindent where $\sigma(S)$ is the inherent noise of the imaging device. The noise depends on photon stochasticity, electronics and setup distortion, and also the transformation applied to correct the image. Which makes closed-form distribution estimation problematic.

The existing image denoising approaches can be roughly categorized into 2 large groups: classical image processing and ML approaches. Typically, classical image processing approaches work on a per-image basis and incorporate expert beliefs about the noise nature. ML approaches in contrast employ the idea of fitting a data-driven model with none to minimal expert knowledge about the nature of the data. %In this section, we will review the established approaches, and motivate our choice.


\subsection{Classical Image Processing}

The basic spatial filtering methods are mean, median, or Gaussian kernel filters~\cite{Gonzalez2008}. For each pixel, these filters select a new value, based on the weighted values of the neighboring pixels. These filters are fast, robust, well-understood, and work fairly well in many situations. The main drawback of these classical filters is their tendency to not only remove the bright noise outliers but also blur the sharp edges.

More advanced spatial filtering approaches, \emph{e.g.}, non-local means (NLM), use more information from the whole image \cite{Buades2005}. Instead of taking an average of the direct neighbourhood of the pixel, NLM takes an average of the large region, weighted by the similarity between the ``donor'' and the ``recipient'' pixel. The process of revisiting multiple locations in the image, comparing their surroundings, and computing the average can take minutes for one image. In return, this method is capable of producing sharper denoised images~\cite{Fan2019}.

Alternatively, denoising can be formulated as an optimization problem and regularization can be used to incorporate some prior knowledge about the image properties~\cite{gu2019brief}. These methods are very powerful but require deep mathematical knowledge and handcrafted regularizers in many cases, making their application for experimental data challenging. One of the most successful regularizers is Total Variation (TV) (and its derivatives) which encourages piece-wise constant image regions separated with sharp boundaries~\cite{rodriguez2013total}.

To summarize, classical methods require fine parameter tuning by an expert to balance smoothing and denoising, hence there is a significant risk of information loss if applied incorrectly. A more comprehensive overview of the classical denoising methods can be found elsewhere~\cite{Fan2019}.

\subsection{Machine Learning Approaches}

The evolution of the classical methods may be seen as a series of steps taken to increase the amount of information used to correct a single pixel value. In this respect, ML-based approaches appear as a natural further step: a \emph{model}, trained to correct the noise, implicitly incorporates knowledge about the whole dataset.

Early ML-based image denoising approaches worked in a supervised manner, \emph{i.e.} a model was trained on a set of noisy images to predict a noise-free image (\emph{target}). Recently, authors of the N2N method demonstrated that there is no need for a noise-free target: if one uses a pair of noisy images (affected by iid instances of the noise) as an input and as a target for the training, the model will predict the noise-free image~\cite{Lehtinen2018}. The underlying intuition is that independent instances of noise are uncorrelated and cannot be predicted, hence the model is forced to extract features. Even though N2N does not explicitly require a set of noise-free images, the authors of ~\cite{Lehtinen2018} synthetically formed noisy pairs by adding noise to noise-free images.

There have been several attempts to extend the N2N method for denoising problems where pairs of images are not naturally available. Noise2Self~\cite{Batson2019} and Noise2Void~\cite{Krull2019} generate the required pair of images by taking random pixels in the noisy image and disturbing them with yet another noise distribution. In this way, multiple training pairs can be constructed from a single noisy image. Noise2Stack~\cite{Papkov2021} was designed for three-dimensional tomographic data and is based on an assumption that tomographic data is typically smooth. Therefore, slice-to-slice changes are assumed to be significantly smaller than the slice-wise variability caused by noise, hence, neighbouring slices can be used for training.

Alternatively, constrained autoencoders can be used to denoise images~\cite{Vincent2008}. During the training, autoencoders use the same image as an input and as a target and attempt to compress (encode) the input image into its lower-dimensional representation. The denoising properties of this approach rely on the assumption that the noise, due to its stochastic nature, is harder to encode, than the signal. To additionally limit the capacity of the model to store information about the noise, it can be restricted by limiting the computational capacity of the model, lowering the dimensionality of the learned representation, or introducing synthetic noise into it~\cite{Vincent2008}. However, the autoencoders are inefficient if the noise is spatially correlated and can be easily memorized by the model. The Hierarchical DivNoising~(HDN) method addresses this issue by training a variational autoencoder with a noise model imposed over output~\cite{Prakash2022}. The authors proposed a way to find particular components of the model that encode information about the noise so that they can remove those components. Even though the proposed methods provide a valuable alternative to the N2N approach, the authors highlight that the N2N approach is a hard-to-beat baseline~\cite{Prakash2022}.

\section{Model Training}

As previously discussed, the N2N method assumes that a pair of images contains the same signal and iid noise. Our adaptation of the method to multi-channel data takes its inspiration from the denoising of Synthetic Aperture Radar (SAR) images~\cite{Dalsasso2022}. In SAR imaging, both phase and amplitude of received microwaves are measured in each pixel; commonly the phase information is ignored. However, the authors demonstrated that the amplitude and the phase contain complimentary information, hence, can be used as a basis for N2N denoising. We hypothesize that in multi-channel imaging, adjacent time frames or energy levels, indeed share sufficiently similar signals, and have noise samples close to being iid. Therefore, we generate the required pair images based on this hypothesis. To help the model catch complex spatial structures of the signal, we also feed it with several adjacent energies or time frames as input, whenever it is possible.

Following~\cite{Krull2019,Batson2019,Lehtinen2018}, we use the fully-convolutional neural networks as a model architecture. We employ U-Net with ResNet-50 as the backbone and rely on the Adam optimizer with a $3e-4$ learning rate, without scheduling. We augment each pair of image pairs with random crops, shifts, scale, rotations, distortions, and different types of blur. We acknowledge, that there is room for quality improvement via larger models, modern architectures, better optimization procedures, or more aggressive augmentations. The sensitivity study of training parameters is a topic of future investigation.


\section{Results} 

\subsection{Simulated spectral X-ray tomography}
\label{sec:simulation}

There is a growing interest in energy-dispersive X-ray tomography, especially for biomedical imaging~\cite{warr2021enhanced}. For a long time, polychromatic emission of laboratory X-ray sources has been considered a drawback in lab-based imaging. In conventional absorption mode, each detector pixel integrates all photons irrespective of their energy. As attenuation is a function of photon energy, conventional (linear) tomographic reconstruction might exhibit so-called beam-hardening artifacts~\cite{davis2008modelling}. However, combined with an energy-dispersive X-ray detector, such an acquisition mode allows to segment materials of similar electron density inseparable otherwise. As the X-ray spectrum shows sharp discontinuities at energies equal to the binding energies of the core-electron states, so-called \emph{k-edges}, the spectrum in each reconstructed voxel can be used to identify the corresponding material. State-of-the-art energy-dispersive X-ray detectors have energy resolution around 1~keV~\cite{egan20153d}, allowing distinguishing even neighbouring chemical elements. However, high spectral resolution comes at a cost of long exposures as acquired counts are shared across multiple bins. Therefore, a state-of-the-art reliable denoising approach might help to improve experiment throughput. In this study, to ensure strictly controlled conditions, we simulated the tomographic acquisition.

\subsubsection{Data}

We generated a volumetric phantom by combining several three-dimensional point clouds: two Swiss rolls, two moon crescents, and an s-curve. All point clouds were generated by the Scikit-Learn library~\cite{Pedregosa2012}; for 2D point clouds, the last axis was added by randomly sampling from the Uniform distribution. To convert the point clouds to a raster volume, we selected a priority order (which value takes place, if there are several objects in one voxel) and size (in voxels) of each point in the point cloud. The spatial size of the phantom was set to $512 \times 512 \times 512$. Structurally, all slices of the dataset are the same. However, surface texture varied because of the random nature of the point clouds. A single slice is shown in Figure \ref{fig:simulation_qualitative:40} (left).

We assigned the simulated objects with energy-dependent mass attenuation coefficient (MAC) of Europium($\prescript{}{63}{\mathbf{Eu}}$, k-edge = 48.5~keV), Gadolinium ($\prescript{}{64}{\mathbf{Ga}}$, k-edge = 50.2~keV), Ytterbium ($\prescript{}{70}{\mathbf{Yb}}$, k-edge = 61.3~keV), Lutetium ($\prescript{}{71}{\mathbf{Lu}}$, k-edge = 63.3~keV), and Uranium ($\prescript{}{92}{\mathbf{U}}$, k-edge = 115.6~keV). The background was assigned with MAC of air. This particular choice of materials was inspired by the study of the separability of k-edge nanoparticles presented in~\cite{getzin2018increased}. Two pairs of materials have neighbouring atomic numbers, hence very close k-edges, and are barely distinguishable in a noisy image; Uranium was added to have a k-edge in the noisiest part of the spectrum, to check the ability of the method to locate the k-edge in extreme noise conditions. 

We used MATLAB package \texttt{PhotonAttenuation} to generate the energy-dependent MAC of the selected materials~\cite{tuszynski2006}. A spectrum profile of a Boone/Fewell source with the tube potential 150~kV (no kV Ripple and filters) was generated using the MATLAB package \texttt{spektr}~3.0~\cite{punnoose2016spektr}. The obtained source spectrum was normalized and scaled to have a maximum value of $175 \cdot 1e3$ photons $/ \mathrm{mm}^2$ to imitate short exposure acquisition. MAC of selected materials and the source spectrum are shown in Figure~\ref{fig:spectra}

We generated 135 energy bins between 15 and 150~keV with a 1~keV step. For each bin, we simulated 120 equally-spaced parallel-beam CT projections over 180\degree. The spectral characteristics of the material and the source are shown in Figure~\ref{fig:spectra}. We used the conventional FBP algorithm to reconstruct tomographic data (implemented in in~\cite{jorgensen2021core}). Examples of reconstructed slices for 40~keV (high flux) and 140~keV (low flux) are shown in Figure~\ref{fig:simulation_qualitative:140} (right). As expected, at 140~keV the reconstructed slice is uninterpretable.

\begin{figure}
     \centering
         \includegraphics[width=\textwidth]{images/spectra.pdf}
        \caption{\label{fig:spectra} Energy dependent mass attenuation coefficients (MAC) of the simulated materials, and simulated source profile.}
\end{figure}


\subsubsection{Training and Processing}

The model $f_{\theta}$ was trained by optimizing 

\begin{equation}
    \mathbb{E}_{i,j} \Vert f_{\theta}(x_{i,j-1}, x_{i,j+1}) - x_{i,j} \Vert_1 \xrightarrow[\theta]{} \min, 
\end{equation}

\noindent where $x_{i,j}$ is a projection acquired at angle $i$ and in energy bin $j$. We randomly split the whole set of projection angles into train and validation sets with a splitting ratio of $80/20$. Note, that the energy level that is required to be predicted, should never be fed into the model, to avoid the trivial solution. This forms a gap of one energy level in the inputs.

During the inference, we feed the model with the adjacent energy bins without the gap used in training, to avoid blur in the spectral domain: 

\begin{equation}
    \tilde{x}_{i,j-0.5} = f_{\theta}(x_{i,j-1}, x_{i,j}).
\end{equation}

However, since the model predicts the energy level which is averaged between two input levels, it will inevitably predict an energy level between two adjacent ones used as input. It is important yet easy to compensate for this.

As before, the denoised tomographic datasets were reconstructed with the conventional FBP algorithm. Here, each energy bin was reconstructed separately resulting in 135 volumes. To obtain the spatial distribution of individual materials in the sample, we performed material decomposition as described in~\cite{ametova2021crystalline}. The employed decomposition relies on the assumption that each voxel is a unit volume and each material occupies a volume fraction in this unit volume (the fraction can be 0). Under this assumption, a voxel-wise sum of all material maps is equal to 1 in each voxel.


\subsubsection{Results}

\Cref{fig:simulation_qualitative:40,fig:simulation_qualitative:140}, show two-dimensional slices for selected (individual) energy bins. N2N demonstrates drastic improvement in reconstruction quality and noise suppression. For 40~keV (high source flux, \Cref{fig:simulation_qualitative:40}) the reconstructed slice appears to be clean from noise or artifacts; the slice is sharp and all structures clearly visible. Even though no signal seems to be invisible in the 140~keV slice (\Cref{fig:simulation_qualitative:140}) prior to denoising, N2N is able to partially recover structures in the slice.

Individual spectra reconstructed for one  voxel in each of the five materials are plotted in Figure~\ref{fig:simulation_qualitative}, right, alongside the theoretical MAC. The voxel locations inside the materials were chosen arbitrarily. Denoising yields sharp and accurately positioned k-edges, hence, supports further material decomposition. Even the Uranium k-edge is visible in the denoised spectrum.

\begin{figure}
    \begin{subfigure}{.5\textwidth}
      \begin{subfigure}{\textwidth}
          \centering
          \includegraphics[width=\linewidth]{images/simulation_comparison_40.png}  
          \caption{Noisy and denoised slices at 40 keV (near peak source flux)}
          \label{fig:simulation_qualitative:40}
        \end{subfigure}
         \newline
        \begin{subfigure}{\textwidth}
          \centering
          \includegraphics[width=\linewidth]{images/simulation_comparison_140.png}  
          \caption{Noisy and denoised slices at 140 keV (low source flux)}
          \label{fig:simulation_qualitative:140}
        \end{subfigure}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=\linewidth]{images/simulation_denoising_profiles.pdf}  
      \caption{Spectral profiles}
      \label{fig:simulation_qualitative:spectra}
    \end{subfigure}
    \caption{Qualitative examination of the denoising with the N2N method. We present both spectral and spatial domains.}
    \label{fig:simulation_qualitative}
\end{figure}

To quantitatively evaluate denoising results, we perform material decomposition. As the sum of volume fractions corresponding to each material, obtained through material decomposition, is bound to 1 in each voxel, we can treat the estimated volume fractions as probabilities. Hence, material decomposition can be considered a classification problem and related quality assessment metrics can be applied to quantitatively assess the results. The comparison results are shown in Figures~\ref{fig:simulation_analysis:low_exposure} and~\ref{fig:simulation_analysis:low_exposure_denoised}. In the top row we show binarized material decomposition error (black corresponds to erroneous material prediction). The confusion matrices between the predicted and true materials for each pixel are presented in the bottom row (perfect classification results in the identity confusion matrix). A high level of noise in the simulated data causes misclassifications between close materials (e.g., Lutetium and Ytterbium). Also, as it is visible on the top row, these errors are distributed evenly throughout the sample. Hypothetically, this can be compensated by enforcing an assumption of material homogeneity. However, this assumption might cause severe errors close to material interfaces.  The errors in the denoised volume are mostly concentrated around the borders (see the top row), and mainly correspond to misclassification for air due to slight blur (see the bottom row). Overall, the confusion matrix for the denoised dataset is considerably closer to the identity matrix. 

We also present the Area Under Precision-Recall Curve (AUPRC), measured for each material~\ref{tab:simulation_auprc}. AUPRC for ideal classification is 1. AUPRC results additionally highlight improvement after denoising: N2N provides a boost of more than $10\%$ of mean AUPRC for the downstream material decomposition. To assess quality loss caused by reconstruction itself (without any effect of denoising), we generated another set of projections with very high flux (all other parameters the same). Material decomposition for this volume shows a mean AUPRC of $0.999$, with the lowest \emph{precision} of $0.996$ for the air. We conclude that the reconstruction losses are negligible in this experiment.

\begin{figure}
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/comparison_noisy.pdf}  
      \caption{Low exposure}
      \label{fig:simulation_analysis:low_exposure}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/comparison_denoised.pdf}  
      \caption{Low exposure with denoising}
      \label{fig:simulation_analysis:low_exposure_denoised}
    \end{subfigure}
    \caption{Quantitative comparison of the different imaging conditions via material decomposition. 
    Top row: binarized decomposition error. 
    Bottom row: confusion matrix for different materials; each row is normalized to the total count of ground truth pixels of each class.}
    \label{fig:simulation_analysis}
\end{figure}

\begin{table}
    \centering
   \begin{tabular}{cr||c|c|c|c|c|c|c}
                & & Air & Eu & Gd & Yb & Lu & U & mean\\
                \hline
                \hline
                \multirow{2}{*}{\rotatebox[origin=c]{90}{AUPRC}}
                & noisy & 0.999 & 0.917 & 0.873 & 0.787 & 0.645 & 0.998 & 0.870 \\
                & denoised & 0.999 & 0.998 & 0.998 & 0.996 & 0.995 & 0.999 & 0.998
        \end{tabular}
        \caption{Comparison of the AUPRC for different materials. We compare the material decomposition before and after denoising.}
        \label{tab:simulation_auprc}
\end{table}


\subsection{Neutron imaging}
\label{sec:neutron}

Neutron imaging provides a complementary contrast to conventional X-ray imaging. Neutrons interact only with atomic nuclei, hence, a neutron beam passing through an object can capture information about the material structure. The neutron transmission intensity spectrum of a polychromatic thermalized neutron beam contains sudden and sharp increase at wavelengths equal twice the interplanar distance between scattering planes as a result of the crystalline properties of the sample material~\cite{fundamentals1993nuclear}. Energy dispersive images can be acquired by combining a pulsed neutron spallation source and a suitable time-sensitive detector by using the time-of-flight (ToF) method. As more energetic particles have higher velocities, they reach the detector faster. Measuring the time of arrival of a neutron at the detector and knowing its flight path, its velocity and therefore its wavelength (energy) can be determined. 
High energy resolution requires long flight distances and many time bins in the detector. Hence, only a few pulses per second can be measured, and acquired counts are shared between multiple bins~\cite{Santisteban2001}. 
More details on this acquisition mode can be found elsewhere, for both the measurement setup~\cite{kockelmann2007energy} and applications~\cite{santisteban2002engineering,strobl2009advances}. Neutron generation is extremely expensive and demand for neutron beamtime exceeds the supply~\cite{Bentley2020}. Therefore, there is a high interest in efficient image denoising techniques to reduce exposure time and subsequently increase experiment throughput. 


\subsubsection{Data}
\label{sec:neutron:data}

In this study, we employ the dataset~\cite{jorgensen2019neutron} acquired at the Imaging and Materials Science \& Engineering (IMAT) beamline operating at the ISIS spallation neutron source (Rutherford Appleton Laboratory, U.K.)~\cite{burca2013modelling,kockelmann2018time}. More details on acquisition parameters and preprocessing can be found elsewhere~\cite{ametova2021crystalline}; here we only briefly recall details relevant to this study. 

A sample contains 6 aluminium tubes: five filled with metallic powder (copper (Cu), aluminium (Al), zinc (Zn), iron (Fe) and nickel (Ni)), and one empty. The neutron detector has $512 \times 512$ pixels, 0.055~mm pixel size. A set of spectral projections were acquired at 120 equally-spaced angular positions over {180\degree} rotation with 15~min exposure. Additionally, 8 flat field images (4 before and 4 after the acquisition) were acquired with the same exposure.

A typical problem of spectral measurements is that noise statistics vary quite drastically across the spectrum. The beam spectrum at the IMAT beamline has a crude bell shape with a peak around 3~{\AA}~\cite{burca2013modelling}. Additionally, the time-sensitive detector suffers from dead time meaning counts loss, hence, additional signal distortions~\cite{tremsin2012high}. To alleviate the count loss problem, the time (wavelength) domain is split in several independent measurement intervals (4 in this case) and a special correction technique is applied to the measured data~\cite{tremsin2012high}. Each interval has an individual bin width; for this study the following bin width was used: $0.7184 \cdot 10^{-3}$~{\AA}, $1.4368 \cdot 10^{-3}$~{\AA}, $\cdot 10^{-3}$~{\AA} and $2.8737 \cdot 10^{-3}$~{\AA}. To benchmark N2N, we generated three additional datasets by rebinning the dataset in the original resolution (2840 energy bins split into 4 measurement intervals with (1141, 814, 424, 464) bins in each). The rebinning was performed individually in each interval by summing every (4, 2, 2, 1), (8, 4, 4, 2) and (16, 8, 8, 4) bins, resulting in datasets with 1366, 681 and 339 wavelength bins, respectively.

In Figure~\ref{fig:neutron:noise_patterns} we show average counts ($\mu$) for a single flat field image for every dataset. Additionally, we show 2.5\% and 97.5\% quantiles (95\% coverage interval) of acquired counts to indicate noise levels across the spectrum. Vertical dotted lines separate independent intervals. Note, that noise dispersion increases drastically with the increase of flux (the effect of counts loss becomes more apparent).

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{images/std-means.pdf}
\caption{Noise and mean values in every individual wavelength channel for different binning parameters.}
\label{fig:neutron:noise_patterns}
\end{figure}


\subsubsection{Training and Analysis Details}

In this experiment, we compare denoising performed on projections (N2N(P)) and on reconstructed slices (N2N(S)). For both, we trained a model $f_{\theta}$ by optimizing essentially the same loss as in the previous case study

\begin{equation}
    \mathbb{E}_{i,j} \Vert f_{\theta}(x_{i,j-1}, x_{i,j+1}) - x_{i,j} \Vert_1 \xrightarrow[\theta]{} \min,
\end{equation}

\noindent where $x_{i,j}$ represents either a projection for an angle $i$ and an energy channel $j$, or a slice number $i$ and an energy channel $j$. We used $i$ to randomly split the dataset into the train and validation subsets in the $80/20$ ratio.

A combination of the N2N denoising approach and the conventional FBP reconstruction was compared with the advanced iterative reconstruction routine proposed in~\cite{ametova2021crystalline}. The latter relies on expert expectations on how the reconstructed image should look like. As the reconstructed cylinders are expected to appear as solids, \emph{i.e.} homogeneous regions, the authors assumed a piece-wise constant signal in the spatial domain. This prior knowledge is enforced through TV regularization~\cite{rudin1992nonlinear,sidky2006accurate}. The signal in the spectral domain is expected to be piece-wise smooth based on theoretical predictions for the materials employed in this study~\cite{boin2012nxs}. In this case, regularization is achieved through Total Generalized Variation (TGV) prior~\cite{bredies2010total}. Hence, we refer to the iterative reconstruction method as TV-TGV. As before, the reconstruction was implemented in CIL~\cite{Papoutsellis2021}. Code to reproduce results is available from~\cite{evelina_ametova_2021_4884710}.


\subsubsection{Results Discussion}

We begin with a visual comparison of different denoising approaches in the spectral domain (Figure~\ref{fig:neutron:profiles}). We perform the comparison for 339 channels image as the same binning was used for the case study in~\cite{ametova2021crystalline}. The theoretical predictions provide the ground truth for the comparison. As in the previous case study, the conventional FBP reconstruction results are uninterpretable. N2N performance is comparable to TV-TGV reconstruction. TV-TGV provides smoother spectra at a cost of resolution loss. In contrast, N2N results appear sharper but noisier for low-attenuative materials. Hence, we conclude that there is a certain threshold noise level N2N can handle efficiently.

Figure~\ref{fig:neutron:slices} shows a comparison of white beam (sum of all energies) and single channel reconstruction for TV-TGV, N2N(S), and N2N(P), through direct comparisons of reconstructed slices in the transverse plane. While for Fe and Ni, both N2N(P) and N2N(S) perform comparably, for Cu and Al their performance differs. The attenuation of Al is drastically lower than other materials, which could lead to inconsistent predictions of the model for the projections when another material occludes the Al cylinder. This problem is not relevant for N2N(S). The Cu powder has a larger mean particle size than other powders (the mean particle size is comparable to the voxel size), hence, stronger spatial structures are visible in the cross-section. The structure changes randomly through sample height. Therefore the N2N(S) model has less information about the structure and might fail to recover it correctly.

As a reference revealing structures, we use an FBP slice averaged across all energy levels, sacrificing spectral information for spatial. We also report the structural similarity index (SSIM) between the single-energy slices and the reference slice~\cite{Wang2004}. Both N2N approaches provide a sharper, more detailed image than TV-TGV. Interestingly, while N2N(S) provides a visually better, sharper image, this image has lower SSIM, compared to the N2N(P). We hypothesize, that this is caused by the unintentional reduction of the streak artifacts (highlighted in the top left callout in the N2N(P) slice). Streak artifacts are very common in tomographic imaging and are caused by insufficient angular sampling~\cite{kak2001principles}.

We next explore denoising quality in the spatial (Figure~\ref{fig:neutron:spatial_ssim}) and the spectral domain (Figure~\ref{fig:neutron:spectral_ssim}) given the increase of noise levels in the input data. We control noise levels by changing binning: the smaller is the binning step--the lower is SNR. We use the white beam slice reconstructed with FBP and the theoretical predictions for SSIM calculations in the spatial and spectral domains, respectively. While iterative reconstruction provides the best results for the spectral domain, it provides the worst result for the spatial domain. Excellent TV-TGV performance heavily capitalizes on the fact that the cylinders are homogeneous inside. In terms of SSIM, N2N(P) outperforms N2N(S) because N2N(S) additionally minimizes streak artifacts due to the angular undersampling, hence, the discrepancy between the reference image and the denoised one grows. 

Another important observation is that N2N can be computed for the higher number of channels. The training time of the model stays almost the same, around 20 hours on average for the full volume, calculated on a $4 \times A5000$ machine. After the training, the model is capable of inferencing one projection/slice at the rate of 20-30 energy channels per second. While TV-TGV reconstruction for one slice with 339 channels takes several hours to complete and reconstruction time increases with the increase in the number of channels or the number of slices.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/neutron_profiles.pdf}
\caption{Comparison of denoising techniques in the spectral domain. Spectral signal in a single reconstructed voxel is plotted. The experimental signal is overlaid with the theoretical prediction (black solid line). Left: signal before denoising; right: TV-TGV, N2N(S), and N2N(P) results are plotted. All results are presented for the datatset with 339 energy bins.}
\label{fig:neutron:profiles}
\end{figure}

% \begin{figure}
%     \begin{subfigure}{.55\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{images/neutron_slices.pdf}  
%       \caption{Comparison of denoising methods. Transverse slices showing white beam (sum of all energies) and single channel at the energy level of 1.63~\AA reconstruction for TV-TGV, N2N(S), and N2N(P).}
%       \label{fig:neutron:slices}
%     \end{subfigure}
%     \begin{subfigure}{.45\textwidth}
%       \centering
%         \begin{subfigure}{\textwidth}
%           \centering
%           \includegraphics[width=\linewidth]{images/neutron_spectral_ssim.pdf}  
%           \caption{SSIM between predicted and experimental spectra.}
%           \label{fig:neutron:spectral_ssim}
%         \end{subfigure}
%         \newline
%         \begin{subfigure}{\textwidth}
%           \centering
%           \includegraphics[width=\linewidth]{images/neutron_spatial_ssim.pdf}  
%           \caption{Averaged SSIM between denoised slices and white beam slice reconstructed with FBP.}
%           \label{fig:neutron:spatial_ssim}
%         \end{subfigure}
%     \end{subfigure}
%     \caption{Qualitative (left) and quantitative (right) comparisons of the denoising methods.}
%     \label{fig:neutron:quantitative}
% \end{figure}

\begin{figure}
    \begin{subfigure}{1\textwidth}
      \centering
      \includegraphics[width=\linewidth]{images/neutron_slices_big.pdf}  
      \caption{Comparison of denoising methods. Transverse slices showing white beam (sum of all energies) and single channel at the energy level of 1.63~\AA reconstruction for TV-TGV, N2N(S), and N2N(P).}
      \label{fig:neutron:slices}
    \end{subfigure}
    \newline
    \begin{subfigure}{\textwidth}
      \centering
        \begin{subfigure}{.49\textwidth}
          \centering
          \includegraphics[width=\linewidth]{images/neutron_spectral_ssim_small.pdf}  
          \caption{SSIM between predicted and experimental spectra.}
          \label{fig:neutron:spectral_ssim}
        \end{subfigure}
        \begin{subfigure}{.49\textwidth}
          \centering
          \includegraphics[width=\linewidth]{images/neutron_spatial_ssim_small.pdf}  
          \caption{Averaged SSIM between denoised slices and white beam slice reconstructed with FBP.}
          \label{fig:neutron:spatial_ssim}
        \end{subfigure}
    \end{subfigure}
    \caption{Qualitative (left) and quantitative (right) comparisons of the denoising methods.}
    \label{fig:neutron:quantitative}
\end{figure}


\subsection{Cineradiography with the Bragg Magnifier}
\label{sec:wasp}

Cineradiography is an indispensable \emph{in-vivo} imaging technique relying on X-rays to study processes over time. Unlike tomography where the total dose can be controlled through both the number of projections and exposure, here the total exposure time is defined by the duration of the process under investigation. Hence, flux has to be reduced in order to reduce the dose. 

For this case study, we employed the propagations-based phase contrast imaging (PB-PCI) image formation mechanism. This mechanism is commonly used for studies of weakly absorbing objects such as biological specimens and thin samples~\cite{Fitzgerald2000}. The idea is to use phase shifts in passing rays to enhance the otherwise low-contrast images. The object information can be reconstructed from the detected interference pattern by additional algorithmic treatment (so-called \emph{phase retrieval}~\cite{Lohse2020}). Depending on the actual measurement setup and material under study, the phase retrieval problem can be solved with different methods. In this specific case study, we used convolution with a dedicated low-pass filter in the spatial domain (typically referred to as Paganin method~\cite{Paganin2002}). Paganin low-pass filter heavily affects noise distribution. On one hand, it significantly reduces high-frequency noise (hence, increases PSNR). On the other hand, low-frequency noise becomes more prominent causing so-called ``cloudy'' artifacts~\cite{paganin2004quantitative}. For a single image, the effect of the low-frequency noise might not be substantial. However, for a fluoroscopy image sequence, this effect results in strong flickering affecting as the location of these ``clouds'' changes randomly from frame to frame, affecting image interpretability by domain specialists.


\subsubsection{Data}

In this case study, we used a batch of \emph{in-vivo} fluoroscopic series imaging a parasitic wasp emerging from its host egg. The full dataset contained $138$ videos, imaged with $15$~fps ($0.066$~s exposure time per frame) with lengths between $81$ and $7142$ frames per image series. The total number of frames is $263,875$.

Additionally, we found a series of 100 frames, where the wasp was completely still. We then calculated an average frame and used it as a reference image with low noise. This averaged image was used for qualitative results calculations. The average Peak Signal-to-Noise Ratio (PSNR) value before phase retrieval is $25.2$ with a standard deviation of $0.02$. After the Paganin phase retrieval, PSNR increases to $35.9$ with a standard deviation of  $1.2$.

Because of the high potential impact of the data on life sciences, we can not disclose any images from this case study. We will update the paper as soon as a concurrent paper with experimental details is published.


\subsubsection{Training and Analysis Details}

Because of the movement of the sample, we can not use more than one frame as the model input. We train the model $f_{\theta}$ by optimizing the loss 

\begin{equation}
    \mathbb{E}_{i,j} \Vert f_{\theta}(x_{i,j-1}) - x_{i,j} \Vert_1 \xrightarrow[\theta]{} \min,
\end{equation}

\noindent where $x_{i,j}$ stands for the frame number $j$ from the image sequence number $i$. We split all frames randomly in train and validation sets in $80/20$ ratio according to the index $i$. Additionally, we noticed that in some cases temporal resolution was not high enough to smoothly capture fast movements resulting in significant changes in sample position between adjacent frames. We introduced additional filtering to alleviate potential blur due to the large differences between neighbouring frames. During the training, we discard the image pairs that have SSIM lower than a hand-optimized threshold.


\subsubsection{Results}

We applied the N2N denoising before and after phase retrieval. Table~\ref{tab:wasp} summarizes PSNR and SSIM for both cases (the average of 100 frames without movement was used as a reference for metrics calculation). Applying denoising before the phase retrieval procedure provides significant improvement in PSNR and SSIM. The benefits are preserved even after phase retrieval.  Also, to check the practical significance of the results we have questioned domain experts to comment on the quality improvement.
\emph{After the denoising, the noise appears less flickering, which makes it easier to concentrate on the sample movements. While denoising makes the image smoother, there is no drastic blur, and even relatively small details (\emph{e.g.}, legs or antennae) are preserved.}
% As an initial check, we examine profile lines through background (air) region (Figure~\ref{fig:wasp:profile-lines}) in the spatial and time domains. Both spatial and spectral profile lines show notable noise reduction. 

%First, to visually demonstrate the applicability of Noise2Noise, we plot the profile lines of the noise-only regions.
%In figure \ref{fig:wasp:profile-lines} we present both spatial (along the horizontal axis of a single frame) and temporal (the same pixel through all the frames of one video) profiles.

%We used the video with the dead wasp to calculate quantitative quality metrics.
%We have applied the method in two ways: before and after the Paganin phase retrieval.
% In the Table \ref{tab:wasp} we compare PSNR and SSIM estimated for both cases.
% We note that denoising done after Paganin doesn't work.
% We hypothesize, that it could be because of the noise distribution itself.
% To demonstrate it, we selected areas consisted of noise only and compared them to the areas consisted of both noise and signal.
% We plot the distributions on the figure \ref{fig:wasp:noise-signal-distributions}.
% It's easy to see, that the noise after the Paganin transform is numerically less prominent and is easier for the model to ignore.
% This correlates well with the rise of the PSNR after phase retrieval.


\begin{table}[]
    \centering
        \begin{tabular}{r||c|c|c|c}
             & \multicolumn{2}{c|}{measured before Paganin} & \multicolumn{2}{c}{measured after Paganin} \\
             & PSNR & SSIM & PSNR & SSIM \\
             \hline
             \hline
                no denoising & $25.2 \pm 4e-3$ & $0.41 \pm 1e-4$ & $36.0 \pm 0.2$ & $0.97 \pm 2e-3$ \\
             \hline
                denoising before Paganin & $33.1 \pm 13e-3$ & $0.49 \pm 3e-4$ & $37.3 \pm 0.3$ & $0.98 \pm 1e-3$ \\
             \hline
                denoising after Paganin & - & - & $36.0 \pm 0.3$ & $0.97 \pm 1e-3$ \\
        \end{tabular}
    \caption{\label{tab:wasp} Comparison of the denoising done before and after Paganin phase retrieval. We used the average of 100 frames where the wasp was still as a noise-free image for these calculations. We report mean value and 95\% confidence interval.}
\end{table}


% \begin{figure}
%     \begin{minipage}{.45\textwidth}
%       \centering
%         \begin{subfigure}{\textwidth}
%           \centering
%           \includegraphics[width=\linewidth]{images/before_paganin_distribution.png}  
%           \caption{Before Paganin Phase retrieval}
%         \end{subfigure}
%         \newline
%         \begin{subfigure}{\textwidth}
%           \centering
%           \includegraphics[width=\linewidth]{images/after_paganin_distribution.png}  
%           \caption{After Paganin phase retrieval}
          
%         \end{subfigure}
%         \captionsetup{width=\textwidth}
%         \caption{\label{fig:wasp:noise-signal-distributions} Noise and signal distributions for wasp radiography.}
%     \end{minipage}
%     \begin{minipage}{.45\textwidth}
%       \centering
%         \begin{subfigure}{\textwidth}
%           \centering
%           \includegraphics[width=\linewidth]{images/wasp_along_space.png}  
%           \caption{Along the spatial dimension}
%         \end{subfigure}
%         \newline
%         \begin{subfigure}{\textwidth}
%           \centering
%           \includegraphics[width=\linewidth]{images/wasp_along_time.png}  
%           \caption{Along the time dimension}
%           \label{fig:wasp:profile-lines:time}
          
%         \end{subfigure}
%         \captionsetup{width=\textwidth}
%         \caption{\label{fig:wasp:profile-lines} Noise values before and after denoising. Both after Paganin phase retrieval.}
%     \end{minipage}
% \end{figure}


%We are also considering to benchmark DL results on a different number of energy channels (the raw datset contains 2847 energy bins which were rebinned to 339 bins to improve SNR, hence we can test DL on data with higher spectral resolution and lower SNR).

% \begin{figure}[h!]
% \centering\includegraphics[width=0.95\linewidth]{images/recon_comparison_new.png}
% \caption{Two-dimensional reconstructions of selected (individual) wavelength bins. Top to bottom: bin-wise FBP reconstruction, iterative reconstruction with TV-TGV regularisation and denoised reconstruction. A magnified region-of-interest marked with a white rectangle is shown inset. White lines mark profile lines chosen for examination in the next figure, whereas white dots
% mark individual voxels chosen for spectral comparison in the next section. All slices are visualised using a common colour range. The
% colour range in the magnified insets is scaled individually to cover only the voxel values in the corresponding inset.}
% \end{figure}

% \begin{figure}[h!]
% \centering\includegraphics[width=0.95\linewidth]{images/recon_comparison_profile_new.png}
% \caption{Linescans corresponding to the vertical (top row) and horizontal (bottom row) white lines in figure 6 (bottom second left) passing
% through the Cu and Fe cylinders and the Al and Ni cylinders, respectively.}
% \end{figure}

% \begin{figure}[h!]
% \centering\includegraphics[width=0.95\linewidth]{images/spectral_comp_new.png}
% \caption{Individual spectra (solid green line) reconstructed by FBP, TV-TGV and denoised for one representative 0.0553 mm3 voxel located
% within in each material alongside the predicted signal (dotted black line).}
% \end{figure}


\section{Discussion}

In general, systematic artifacts present in all channels (energy bins or time frames) will not be compensated, hence relevant corrections (such as centre-of-rotation compensation in CT) are essential. On the other hand, we noticed that N2N applied to reconstructed slices significantly reduced the appearance of the ring (Figure \ref{fig:simulation_qualitative:140}) and undersampling artifacts (Figure \ref{fig:neutron:slices}). The magnitude and the limits of this secondary effect are in the scope of future research.

We observed superficial ``cloudy'' artefacts after denoising present in homogeneous image regions (such as the background). These artifacts do not have any significant effect on CT data as they have much lower contrast than actual image features. However, in the BM time series, they affect the overall image perception as their location changes randomly from frame to frame introducing the flickering effect. This flickering effect does not affect image interpretability as it has very low contrast compared to image features. Hence it is more of a cosmetic effect as the human eye is still sensitive to it. A way around this is to generate a few images for each frame by adding some Gaussian noise (\emph{i.e.} to increase noise level) and take a median value of the resulting denoised images. However, increasing the number of used denoised images or the variance of the noise leads to blurring. We set these parameters, guided by the expert judgement on the resulting image.

N2N assumes that the image pairs have equal signal values and independent noise drawn from the same distribution. Strictly speaking, both assumptions might be violated in spectral and time-resolved imaging where values in each individual channel are either energy or time-dependent, and noise distribution might be partially correlated (see data discussion in \ref{sec:neutron:data} for details). The application of the method to such data is based on the assumption that the variability of noise between the twin images is larger than the variability of the signal.

In spectral CT, N2N can be applied to both projection images and to slices after reconstruction. Any corrections in the projection domain are challenging as they might cause or exaggerate existing inconsistency between projections (consistent sinogram has strong restrictions expressed as Helgason–Ludwig consistency condition~\cite{helgason1965radon}). However, our empirical studies did not show any noticeable artifacts due to this inconsistency.

\section{Conclusion}

In this paper, we explored the applicability of the N2N method to denoising of the time or energy-resolved images. N2N is a distribution-agnostic method, it does not explicitly assume any properties of the signal or noise. The only requirement is the ability to sample pairs of images that share the signal but have iid noise. We demonstrated that this requirement, while not exactly met for the multichannel data, can be relaxed to successfully apply the method. The presented case studies showed that this method offers a robust and efficient alternative to conventional denoising methods and regularized iterative reconstruction methods. The method does not require fine-tuning of parameters or handcrafted regularization term for any new dataset. Therefore, its application can be heavily automated. Finally, the N2N method relies on a rather intuitive assumption, hence it can be easily explained to non-experts in the ML domain and smoothly introduced into the measurement practice.


\begin{backmatter}
\bmsection{Funding} We gratefully acknowledge beamtime RB1820541 at the IMAT Beamline of the ISIS Neutron and Muon Source, Harwell, UK. E A was funded by the Federal Ministry of Education and Research (BMBF) and the Baden-Württemberg Ministry of Science as part of the Excellence Strategy of the German Federal and State Governments.

\bmsection{Acknowledgments} This work made use of computational support by CoSeC, the Computational Science Centre for Research Communities, through the Collaborative Computational Project in Tomographic Imaging (CCPi).

\bmsection{Data Availability Statement} The neutron tomography dataset is openly available at the following URL/DOI: (\href{https://doi.org/10.5286/ISIS.E.RB1820541}{10.5286/ISIS.E.RB1820541}).
The code to reproduce the phantom and the dataset will be made publicly available upon the publication of the paper.

\end{backmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%

%Spectral (energy-resolved) tomography opens possibilities for simultaneous structural and chemical characterization. However, the new possibilities come at a cost of increased exposure times. In this paper we explore 
% OLD ABSRACT In this paper we demonstrate a method to improve quality of tomographic datasets acquired with low exposure. The improvement is achieved through the \emph{Noise2Noise} denoising approach that uses a pair of noisy images to predict a clean image. We present two experimental studies where the exposure duration is a significant obstacle in data collection. The first one is energy-dispersive neuron tomography. Since acquired counts are distributed across multiple bins, spectral tomography is characterized by the poor signal-to-noise ratio. In neutron imaging the problem is exaggerated by the low flux and high spectral resolution. However, spectral imaging naturally provides sets of images sharing structural information which we exploit to improve the results. The second one is in-vivo (time-resolved) synchrotron tomography. Radiation dose is harmful to small organisms effectively limiting the total experiment duration. Here, pairs of images are not available. Therefore, we propose an approach to generate the ``twin'' image based on/ through ... . In both cases, the deep learning approach shows drastic improvement in reconstructed image quality and outperforms both convectional denoising methods and advanced noise-suppressing iterative reconstruction method.


%Tomographic imaging relies on a penetration ability of radiation and dedicated reconstruction algorithms to visualize and subsequently characterize interior structure of a sample. Given a set measurements (\emph{projections}) acquired at various angles, a map of sample attributes, for instance, attenuation, can be \emph{reconstructed}. As different materials have different attenuation properties, such attenuation maps can be used to extract structural information. Reconstructed image quality directly depends on the number of projections and respective projections quality. The projection quality, in turn, depends on exposure time as signal-to-noise ratio (SNR) grows with integration. However there are several scenarios where sufficient data quality cannot be acquired. One example is spectral (energy-resolved/ dispersive) tomography. Additionally to structural information, spectral tomography allows characterization of material properties. Here, acquired particle counts are shared among multiple energy bins. Hence, lengthy exposures are needed to collect sufficient statistics in every energy bin, making acquisition prohibitively long and costly. Another example is in-vivo biological imaging. Time-resolved tomography is used to observe sample change over time. Radiation damage is proportional to the total statistics acquired. Therefore, to extend total duration of the experiment and collect more valuable biological information, exposure per each time frame has to be reduced. This can be achieved through shorter exposure per projection or lower number of projections per time frame or through a combination of both approaches. 

%Various methods have been proposed in literature to improve reconstruction quality. In general they fall into two categories: iterative reconstruction with a priory knowledge and denoising. Iterative reconstruction methods incorporates our expectations on image properties to compensate for insufficient data quality. Iterative reconstruction has been proven to be a powerful tool to treat both low SNR and low number projections problems. However, majority of iterative methods rely on fine tuning of free parameters and the choice of these parameters is often not clear in practice. Secondly, efficient application of these methods requires a profound understanding of underlying mathematics. Finally, these methods are very computationally demanding and require long computation time even after dedicated hardware acceleration. Alternatively, denoising can be used to improve reconstruction quality. There are conventional denoising methods like various smoothing filters and non-local mean. These methods can be called global in a sense that they operate in a fixed manner over image independently on local image structure. In contrast, deep learning approaches can effectively capture and preserve local image structure. In this paper we employ \emph{Noise2Noise} denoising approach~\cite{lehtinen2018noise2noise,dalsasso2021if} to improve reconstructed image quality. This method requires a pair of images sharing the structural information but with independent noise statistics. This is a self-supervised approach, \emph{i.e.} the method does not rely on data specifically designated and modified for the neural network training. Hence, the model can be trained directly on the dataset which we will apply the model on.

%The method performance is first confirmed on a simulated dataset. Then we show two experimental studies: energy-dispersive neutron tomography of a multi-material sample comprising aluminium cylinders filled with various metallic powders and in-vivo synchrotron tomography of a Xenopus frog embryo. 


%Spectral tomography is a tool, that allows obtaining spectral information for each point of the 3D volume.
%During the imaging procedure, the camera counts particles arrived at each pixel.
%Therefore, each pixel of the projection is, actually, a sample from the Poisson distribution, and SNR grows with exposition time.
%However, due to the cost of the operation time of a particle accelerator, the image typically is taken with relatively low exposure and is, therefore, noisy.
%In some situations, researchers want to minimize exposure to decrease radiation damage on a living measurement object.
%There are several established ways to improve the quality of the noisy data.
%However we want to improve the quality of the image with Deep Learning.
%Deep Learning models have shown advantageous results in denoising.
%In this paper we propose to employ Noise2Noise denoising approach for the spectral tomography denoising.
%This is a self-supervised approach, which means we do not need any data, which is specifically designated and modified for the neural network training.
%We can train directly on the dataset which we will apply our model on.



% \subsection{Xenopus}

% In-vivo bioimaging allows obtaining continuous information about organism development in time.
% However, projections for this technique should be taken swiftly.
% First, to avoid as much redundant radiation affecting living organism as possible.
% And second, to avoid development of the organism during scan itself.
% The two ways to minimize time and doze are: minimizing projections count and minimizing exposure time.
% There are a number of works exploring improving quality of reconstruction with restricted number of projections.
% Though, in this paper we will focus on the exposure time reduction.
% We will denote the usual exposure used for imaging as $1$, and other exposures as \sfrac{1}{4}, \sfrac{1}{8}, etc.

% For this, we developed a special kind of the dataset, consisting of two parts.
% The first one contains $4$ \emph{Xenopus Laevis} embryos of different developmental stages, imaged with the \note{a lot} of projections.
% Each sample was imaged several times with with different exposure times: $1$, \sfrac{1}{4}, and \sfrac{1}{8}.
% We changed exposure after imaging all projections.
% This allows good reconstructions, but since volumes are not aligned, we can not assert denoising quality quantitatively.

% To allow quantitative quality assessment, we obtained another dataset containing aligned projections.
% We used $10$ samples of the same nature.
% For each stage we obtained \note{200} projections each with $1$, \sfrac{1}{4}, \sfrac{1}{8}, \sfrac{1}{12}, and \sfrac{1}{16} exposure times.
% This time, different exposure projections were taken sequentially, and therefore perfectly aligned.
% However, due to longer exposure, we were not able to take enough projections for the full reconstruction to be possible.

% We are planning to publish this dataset for open access, alongside with extended comparison of different denoising techniques.


% To adapt this method to the single-energy computed tomography data, we take inspiration from the methods Noise2Self \cite{Batson2019} and Noise2Void \cite{Krull2019}.
% Both share the common idea that many types of image noise are pixel-wise conditionally independent given the signal.
% This leads to the idea, that predicting the noisy value of the pixel, given noisy values of its neighbourhood will lead to denoising.
% Both propose to sample these pixels sparsely and calculate training loss only for those pixels.
% The difference of the methods lies in the proposed way of changing the value of the pixel selected for prediction.
% Also, the Noise2Self paper proposed a beautiful idea to estimate quality of the denoising without ground truth.
% In our work, we propose to make the next dialectical step and unite those proposals with Noise2Noise method.
% We propose to generate two twin images by splitting each image into two by checkerboard pattern.
% In each of the twin images we have, therefore, i.i.d. noise, for those noise patterns which follow pixel-wise independence hypothesis.
% After this split, half of the image values are undefined, however are surrounded by the 4 defined values.
% We propose to linearly interpolate those 4 values to inpaint the absent ones.
% We claim, that since the interpolation is very local, we are not affecting signal part of the image much more than it's proposed by \cite{Batson2019,Krull2019}.

%As both mechanisms remove particles from a transmitted beam path, we will use \emph{attenuation} to refer to both mechanisms. Given a monochromatic (single energy) beam, the Beer-Lambert law describes the exponential relationship between the incident and the transmitted particles:

%\begin{equation} \label{eq:beer_lambert}
%	I = I_0 \exp \left( -\int_L \mu(x, \lambda) \mathrm{d}x \right),
%\end{equation}

%\noindent where $I_0$ and $I$ corresponds to the beam intensity incident on the object and on the detector element, respectively, $L$ is a linear path through the object and $\mu(x, \lambda)$ is the wavelength (energy) dependent attenuation coefficient at the physical position $x$ in the object for the given wavelength $\lambda$. Hence, a simple transformation allows to obtain a linear dependence between the material attenuation properties and the acquired signal.

%\begin{equation} \label{eq:attenuation}
%    \int_L \mu(x, \lambda) \mathrm{d}x = - \log{ \left(\frac{I}{I_0}\right)}
%\end{equation}

%\noindent In practice, a two-dimensional (2D) pixellated detector is employed to measure $I_0$ and $I$ and the integral is approximated as a sum along a ray through a three-dimensional (3D) voxellized volume. 

%In conventional tomography, a rotation stage is used to acquire \emph{projections} at various angles. Then, a three-dimensional (3D) voxellized map of sample attenuation, can be \emph{reconstructed}. From a mathematical point of view, reconstruction is an inverse transform of the line integral measurements in equation~\ref{eq:attenuation} and a number of algorithms were proposed to solve this inverse problem. The most commonly used is the filtered back projection (FBP) which is based on an analytical inversion as a combination of a convolution and a back projection (``smearing back'' the projection across the 3D voxellized volume along rays of incidence). Alternatively, the reconstruction problem can be formulated as a large system of linear equations and numerical optimization can be used to solved it. This in turn allows to incorporate prior knowledge into the reconstruction procedure (this method is called regularization), hence, improve reconstruction quality.


\begin{comment}
\section{Multiple corresponding authors}

There are two options for indicating multiple corresponding authorship, and they are formatted quite differently. The first format would be as follows and uses an asterisk to denote one of the authors:

\begin{verbatim}
\author{Author One\authormark{1,3} and Author Two\authormark{2,4,*}}

\address{\authormark{1}Peer Review, Publications Department,
Optica Publishing Group, 2010 Massachusetts Avenue NW,
Washington, DC 20036, USA\\
\authormark{2}Publications Department, Optica Publishing Group,
2010 Massachusetts Avenue NW, Washington, DC 20036, USA\\
\authormark{3}xyz@optica.org}

\email{\authormark{*}opex@optica.org}
\end{verbatim}

This format will generate the following appearance:  

\medskip

\author{Author One\authormark{1,3} and Author Two\authormark{2,4,*}}

\address{\authormark{1}Peer Review, Publications Department,
Optica Publishing Group, 2010 Massachusetts Avenue NW,
Washington, DC 20036, USA\\
\authormark{2}Publications Department, Optica Publishing Group,
2010 Massachusetts Avenue NW, Washington, DC 20036, USA\\
\authormark{3}xyz@optica.org}

\email{\authormark{*}opex@optica.org}

\medskip

The second format forgoes the asterisk and sets all email addresses equally within the affiliations. Please note that this format does not use the \verb+\email{}+ field at all.
\begin{verbatim}
\author{Author One\authormark{1,3} and Author Two\authormark{2,4}}

\address{\authormark{1}Peer Review, Publications Department,
Optica Publishing Group, 2010 Massachusetts Avenue NW,
Washington, DC 20036, USA\\
\authormark{2}Publications Department, Optica Publishing Group,
2010 Massachusetts Avenue NW, Washington, DC 20036, USA\\
\authormark{3}xyz@optica.org\\
\authormark{4}opex@optica.org}
\end{verbatim}

This format will generate the following appearance:

\medskip

\author{Author One\authormark{1,3} and Author Two\authormark{2,4}}

\address{\authormark{1}Peer Review, Publications Department,
Optica Publishing Group, 2010 Massachusetts Avenue NW, Washington, DC 20036, USA\\
\authormark{2}Publications Department, Optica Publishing Group, 2010 Massachusetts Avenue NW, Washington, DC 20036, USA\\
\authormark{3}xyz@optica.org\\
\authormark{4}opex@optica.org}
\medskip
These are the preferred
%express journal
formats for multiple corresponding authorship, and either may be used.

\section{Abstract}
The abstract should be limited to approximately 100 words. If the work of another author is cited in the abstract, that citation should be written out without a number, (e.g., journal, volume, first page, and year in square brackets [Opt. Express {\bfseries 22}, 1234 (2014)]), and a separate citation should be included in the body of the text. The first reference cited in the main text must be [1]. Do not include numbers, bullets, or lists inside the abstract.

\begin{figure}[h!]
\centering\includegraphics[width=7cm]{osafig1}
\caption{Sample caption (Fig. 2, \cite{Yelin:03}).}
\end{figure}


\section{Assessing final manuscript length}
The Universal Manuscript Template is based on the Express journal layout and will provide an accurate length estimate for \emph{Optics Express}, \emph{Biomedical Optics Express},  \emph{Optical Materials Express}, and our newest title \emph{OSA Continuum}. \emph{Applied Optics}, JOSAA, JOSAB, \emph{Optics Letters}, \emph{Optica}, and \emph{Photonics Research} publish articles in a two-column layout. To estimate the final page count in a two-column layout, multiply the manuscript page count (in increments of 1/4 page) by 60\%. For example, 11.5 pages in the Universal Manuscript Template are roughly equivalent to 7 composed two-column pages. Note that the estimate is only an approximation, as treatment of figure sizing, equation display, and other aspects can vary greatly across manuscripts. Authors of Letters may use the legacy template for a more accurate length estimate.

\section{Figures, tables, and supplementary materials}

\subsection{Figures and tables}
Figures and tables should be placed in the body of the manuscript. Standard \LaTeX{} environments should be used to place tables and figures:
\begin{verbatim}
\begin{figure}[htbp]
\centering\includegraphics[width=7cm]{osafig1}
\caption{Sample caption (Fig. 2, \cite{Yelin:03}).}
\end{figure}
\end{verbatim}

\subsection{Supplementary materials in Optica Publishing Group journals}
Our journals allow authors to include supplementary materials as integral parts of a manuscript. Such materials are subject to peer-review procedures along with the rest of the paper and should be uploaded and described using our Prism manuscript system. Please refer to the \href{https://opg.optica.org/submit/style/supplementary_materials.cfm}{Author Guidelines for Supplementary Materials in Optica Publishing Group Journals} for more detailed instructions on labeling supplementary materials and your manuscript.

\textbf{Authors may also include Supplemental Documents} (PDF documents with expanded descriptions or methods) with the primary manuscript. At this time, supplemental PDF files are not accepted for partner titles, JOCN and \emph{Photonics Research}. To reference the supplementary document, the statement ``See Supplement 1 for supporting content.'' should appear at the bottom of the manuscript (above the References heading). 

\subsection{Sample Dataset Citation}

1. M. Partridge, "Spectra evolution during coating," figshare (2014), http://dx.doi.org/10.6084/m9.figshare.1004612.

\subsection{Sample Code Citation}

2. C. Rivers, "Epipy: Python tools for epidemiology," figshare (2014) [retrieved 13 May 2015], http://dx.doi.org/10.6084/m9.figshare.1005064.



\section{Mathematical and scientific notation}

\subsection{Displayed equations} Displayed equations should be centered.
Equation numbers should appear at the right-hand margin, in
parentheses:
\begin{equation}
J(\rho) =
 \frac{\gamma^2}{2} \; \sum_{k({\rm even}) = -\infty}^{\infty}
	\frac{(1 + k \tau)}{ \left[ (1 + k \tau)^2 + (\gamma  \rho)^2  \right]^{3/2} }.
\end{equation}

All equations should be numbered in the order in which they appear
and should be referenced  from within the main text as Eq. (1),
Eq. (2), and so on [or as inequality (1), etc., as appropriate].

\section{Backmatter}

Backmatter sections should be listed in the order Funding/Acknowledgment/Disclosures/Data Availability Statement/Supplemental Document section. An example of backmatter with each of these sections included is shown below.

\begin{backmatter}
\bmsection{Funding}
Content in the funding section will be generated entirely from details submitted to Prism. Authors may add placeholder text in the manuscript to assess length, but any text added to this section in the manuscript will be replaced during production and will display official funder names along with any grant numbers provided. If additional details about a funder are required, they may be added to the Acknowledgments, even if this duplicates information in the funding section. See the example below in Acknowledgements.

\bmsection{Acknowledgments}
Acknowledgments should be included at the end of the document. The section title should not follow the numbering scheme of the body of the paper. Additional information crediting individuals who contributed to the work being reported, clarifying who received funding from a particular source, or other information that does not fit the criteria for the funding block may also be included; for example, ``K. Flockhart thanks the National Science Foundation for help identifying collaborators for this work.'' 

\bmsection{Disclosures}
Disclosures should be listed in a separate nonnumbered section at the end of the manuscript. List the Disclosures codes identified on the \href{https://opg.optica.org/submit/review/conflicts-interest-policy.cfm}{Conflict of Interest policy page}, as shown in the examples below:

\medskip

\noindent ABC: 123 Corporation (I,E,P), DEF: 456 Corporation (R,S). GHI: 789 Corporation (C).

\medskip

\noindent If there are no disclosures, then list ``The authors declare no conflicts of interest.''


\bmsection{Data Availability Statement}
A Data Availability Statement (DAS) will be required for all submissions beginning 1 March 2021. The DAS should be an unnumbered separate section titled ``Data Availability'' that
immediately follows the Disclosures section. See the \href{https://www.osapublishing.org/submit/review/data-availability-policy.cfm}{Data Availability Statement policy page} for more information.

OSA has identified four common (sometimes overlapping) situations that authors should use as guidance. These are provided as minimal models, and authors should feel free to
include any additional details that may be relevant.

\begin{enumerate}
\item When datasets are included as integral supplementary material in the paper, they must be declared (e.g., as "Dataset 1" following our current supplementary materials policy) and cited in the DAS, and should appear in the references.

\bmsection{Data availability} Data underlying the results presented in this paper are available in Dataset 1, Ref. [3].

\bigskip

\item When datasets are cited but not submitted as integral supplementary material, they must be cited in the DAS and should appear in the references.

\bmsection{Data availability} Data underlying the results presented in this paper are available in Ref. [3].

\bigskip

\item If the data generated or analyzed as part of the research are not publicly available, that should be stated. Authors are encouraged to explain why (e.g.~the data may be restricted for privacy reasons), and how the data might be obtained or accessed in the future.

\bmsection{Data availability} Data underlying the results presented in this paper are not publicly available at this time but may be obtained from the authors upon reasonable request.

\bigskip

\item If no data were generated or analyzed in the presented research, that should be stated.

\bmsection{Data availability} No data were generated or analyzed in the presented research.
\end{enumerate}


\bmsection{Supplemental document}
See Supplement 1 for supporting content. 

\end{backmatter}

\section{References}
\label{sec:refs}
Proper formatting of references is extremely important, not only for consistent appearance but also for accurate electronic tagging. Please follow the guidelines provided below on formatting, callouts, and use of Bib\TeX.

\subsection{Formatting reference items}
Each source must have its own reference number. Footnotes (notes at the bottom of text pages) are not used in our journals. References require all author names, full titles, and inclusive pagination. Examples of common reference types can be found in the  \href{https://www.osapublishing.org/submit/style/osa-styleguide.cfm} {style guide}.


The commands \verb+\begin{thebibliography}{}+ and \verb+\end{thebibliography}+ format the section according to standard style, showing the title {\bfseries References}.  Use the \verb+\bibitem{label}+ command to start each reference.

\subsection{Formatting reference citations}
References should be numbered consecutively in the order in which they are referenced in the body of the paper. Set reference callouts with standard \verb+\cite{}+ command or set manually inside square brackets [1].

To reference multiple articles at once, simply use a comma to separate the reference labels, e.g. \verb+\cite{Yelin:03,Masajada:13,Zhang:14}+, produces \cite{Yelin:03,Masajada:13,Zhang:14}.
%Using the \texttt{cite.sty} package will make these citations appear like so: [2--4].

\subsection{Bib\TeX}
\label{sec:bibtex}
Bib\TeX{} may be used to create a file containing the references, whose contents (i.e., contents of \texttt{.bbl} file) can then be pasted into the bibliography section of the \texttt{.tex} file. A Bib\TeX{} style file, \texttt{osajnl.bst}, is provided.

If your manuscript already contains a manually formatted \verb|\begin{thebibliography}|... \verb|\end{thebibliography}| list, then delete the \texttt{latexmkrc} file (if present) from your submission files. However you should ensure that your manually-formatted reference list adheres to style accurately.

\section{Conclusion}
After proofreading the manuscript, compress your .tex manuscript file and all figures (which should be in EPS or PDF format) in a ZIP, TAR or TAR-GZIP package. All files must be referenced at the root level (e.g., file \texttt{figure-1.eps}, not \texttt{/myfigs/figure-1.eps}). If there are supplementary materials, the associated files should not be included in your manuscript archive but be uploaded separately through the Prism interface.

%%%%%%%%%%%%%%%%%%%%%%% References %%%%%%%%%%%%%%%%%%%%%%%%%

Add references with BibTeX or manually.
\cite{Zhang:14,OSA,FORSTER2007,Dean2006,testthesis,Yelin:03,Masajada:13,codeexample}

\end{comment}

%%%%%%%%%% If using BibTeX:
\bibliography{sample, references}

%%%%%%%%%% If preparing manually:
% \begin{thebibliography}{1}
% \newcommand{\enquote}[1]{``#1''}

% \bibitem{Zhang:14}
% Y.~Zhang, S.~Qiao, L.~Sun, Q.~W. Shi, W.~Huang, L.~Li, and Z.~Yang,
%   \enquote{Photoinduced active terahertz metamaterials with nanostructured
%   vanadium dioxide film deposited by sol-gel method,}
%   {\protect\JournalTitle{Optics Express}} \textbf{22}, 11070--11078 (2014).

% \bibitem{OSA}
% {Optical Society}, \enquote{{OSA Publishing},}
%   \url{http://www.osapublishing.org}.

% \bibitem{FORSTER2007}
% P.~Forster, V.~Ramaswamy, P.~Artaxo, T.~Bernsten, R.~Betts, D.~Fahey,
%   J.~Haywood, J.~Lean, D.~Lowe, G.~Myhre, J.~Nganga, R.~Prinn, G.~Raga,
%   M.~Schulz, and R.~V. Dorland, \enquote{Changes in atmospheric consituents and
%   in radiative forcing,} in \enquote{Climate Change 2007: The Physical Science
%   Basis. Contribution of Working Group 1 to the Fourth assesment report of
%   Intergovernmental Panel on Climate Change,}  S.~Solomon, D.~Qin, M.~Manning,
%   Z.~Chen, M.~Marquis, K.~B. Averyt, M.~Tignor, and H.~L. Miler, eds.
%   (Cambridge University Press, 2007).

% \end{thebibliography}

\end{document}
