\documentclass[11 pt]{article}


% \usepackage{generic}

% \IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

% \overrideIEEEmargins                                      % Needed to meet printer requirements.


% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% \markboth{\journalname, VOL. XX, NO. XX, XXXX 2021}
% {Maity \MakeLowercase{\textit{et al.}}: Optimal LQG Control under Traffic-Correlated Delay and Dropout (June 2021)}


\usepackage[utf8]{inputenc}
% \usepackage{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{subcaption}
\usepackage[margin = 1in]{geometry}

%============== MATH Commands =======================
\newcommand{\x}{\textbf{x}}
\renewcommand{\v}{\textbf{v}}
\renewcommand{\r}{\textbf{r}}
\newcommand{\rhat}{\hat{\r}}
\newcommand{\Tp}{\mathcal{T}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\mum}{\mu_{p, \rm{motion}}}
\newcommand{\mus}{\mu_{p, \rm{sense}}}
\newcommand{\rcap}{r_{\rm cap}}
\newcommand{\T}{^{\mbox{\tiny \sf T}}}
\newcommand{\inv}{{\mbox{\tiny -1}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\Ed}{\mathcal{E}}
\newcommand{\J}{\Lambda}
\newcommand{\N}{\mathcal{N}}
\newcommand{\bbN}{\mathbb{N}}
% \newcommand{\xad}{\hat{x}^{\rm ad}}
\newcommand{\xad}{\hat{x}}
\newcommand{\ead}{e^{\rm ad}}
\newcommand{\nad}{\N_{\rm ad}}
\newcommand{\nadi}{\N_{\rm ad}^i}
\newcommand{\sad}{\Sigma^{\rm ad}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\aep}[1]{\textit{asymptotically} $#1$-\textit{protected}}
\newcommand{\ep}[1]{$#1$-\textit{protected}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\diag}[1]{\mathrm{diag}[#1]}
\newcommand{\blkdiag}[1]{\mathrm{diag}[#1]}

\newcommand{\goto}{\rightarrow}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\bbP}{\mathbb{P}}
\DeclareMathOperator{\Prob}{\mathsf{P}}
\DeclareMathOperator{\trace}{tr}
\DeclareMathOperator{\transpose}{\mathsf{T}}
\DeclareMathOperator{\f}{f}
%==========================================================

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{conjecture}{Conjecture}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

% \let\proof\relax
% \let\endproof\relax
% =====================================================

\newcommand{\re}[1]{\textcolor{red}{#1}}
\newcommand{\bl}[1]{\textcolor{blue}{#1}}
% =======================================================

% \renewcommand{\baselinestretch}{.95}

%=========================================================

\newenvironment{proof}{\textit{Proof:}}{\hfill $\blacksquare$}


\title{Optimal Intermittent Sensing for Pursuit-Evasion Games}

 \author{Dipankar Maity \thanks{D. Maity is with the Department of Electrical and Computer Engineering, University of North Carolina at Charlotte,  NC, 28223, USA. Email: {\small \texttt{dmaity@uncc.edu}}. The work of D. Maity was supported, in parts, by by  ARL grant ARL DCIST CRA W911NF-17-2-0181.} 
}
\date{}

\begin{document}

\maketitle

\begin{abstract}
     \textit{We consider a class of pursuit-evasion differential games in which the evader has continuous access to the pursuer's location, but not vice-versa. 
     There is a remote sensor (e.g., a radar station) that can sense the evader's location upon a request from the pursuer and communicate that sensed location to the pursuer. The pursuer has a budget on the total number of sensing requests. The outcome of the game is determined by the sensing and motion strategies of the players. We obtain an equilibrium sensing strategy for the pursuer and an equilibrium motion strategy for the evader.
     We quantify the degradation in the pursuer's pay-off due to its sensing limitations. }
     % We discuss the emergence of implicit communication where the absence of communication from the sensor can also convey some information to the pursuer.} 
\end{abstract}

\textit{\textbf{keywords -}}
Pursuit-evasion, intermittent sensing, self-triggered control.


\section{Introduction}

Pursuit-Evasion games \cite{isaacs1999differential}  have been applied to investigate a wide class of civilian and military applications involving multi-agent interactions in adversarial scenarios \cite{yan2016multi,  robotics9020047, inproceedings}. 
In its simplest form the game involves a pursuing player that is tasked to capture an evading player before either the evader reaches its destination or the pursuer runs out of fuel. 

While several variations ranging from complex dynamic models for the players (e.g., \cite{sun2017multiple}) to complex geometry of the environment (e.g., \cite{oyler2016pursuit}) to limited visibility of the players (e.g., \cite{bhattacharya2010existence}) have been considered, one of the prevailing assumptions have been the continuous sensing capability for the players, with the exception of \cite{aleem2015self, aleem2015bself,maity2016strategies, maity2016optimal} among few others. 
By `continuous sensing' we refer to the capability that enables the players to keep their sensors turned on continuously for the entire duration of the game.
Extensions of pursuit-evasion games in the context of sensing limitations have mainly considered limited sensing range \cite{bopardikar2007cooperative, durham2010distributed, shishika2021partial} and limited field of view \cite{gerkey2006visibility}, but the challenges associated with the lack of continuous sensing remains unsolved.  

In this work, we  revisit the classical pursuit evasion game in an obstacle-free environment where the pursuer does not have a continuous sensing capability. 
In particular, the pursuer relies on a remotely located sensor (e.g., a radar station) to sense the evader's position. 
Upon request, the remote sensor can perfectly sense the location of the evader and share it with the pursuer.\footnote{
One may alternatively also consider a scenario where the pursuer has an onboard sensor to measure the evader's location, however, it can only use the sensor intermittently due to, for example, energy and computational constraints.
} 
The communication channel between the pursuer and the remote sensor is assumed to be noiseless, instantaneous (i.e., no delay), and perfectly reliable (i.e., no packet losses). 
The pursuer intermittently requests the evader's location to update its pursuit strategy. 
Due to the resource (e.g., energy) constraints, the pursuer can only make a maximum of $n$ requests.
On the other hand, the evader is able to sense the pursuer continuously and is aware of the sensing limitation of the pursuer. 
The objective of this work is to analyze the game under this asymmetric sensing limitation and obtain the optimal instances for sensing.

Our prior works \cite{maity2016optimal, maity2016strategies} considered a linear quadratic differential game formulation where both the agents had sensing limitations. 
These works were later extended to discrete time \cite{maity2017linear}, infinite horizon \cite{maity2017asymptotic}, and recently to asset defense scenarios \cite{huang2021defending}. All these extensions rely on the linear-quadratic structure of the problem.
Closely related to the problem considered in this work are the works in \cite{aleem2015self} and \cite{aleem2015bself} where the authors considered a self triggered strategy to decide the sensing instances. 
Self-triggered strategies are conservative and often highly suboptimal. 
\cite{aleem2015bself} extends the work of \cite{aleem2015self} by incorporating noisy sensor measurements.
These works did not consider any budget on the number of sensing and therefore, the proposed sensing strategy is agnostic to the sensing budget $n$. 
With respect to these existing works, the contributions of this work are: (i) Formulating a sensing (and fuel) limited pursuit-evasion game and analyzing the equilibrium sensing and motion strategies for the players, (ii) Demonstrating the optimality of a `waiting strategy' for a pursuer equipped with only intermittent sensing, (iii)  Finding the required number of sensing $n_{\max}$ to ensure that the pursuer can perform the same as it does under continuous sensing, and (iv) Comparing the performance with existing work \cite{aleem2015self} to illustrate the sensing improvement achieved in our work. 


The rest of the manuscript is organized as follows: In Sec.~\ref{sec:prelim} we describe our preliminary results and derive the required number of sensing to ensure that the pursuer performs the same as it does in the continuous sensing case. 
Building upon this result, in Sec.~\ref{sec:formulation} we present the sensing and fuel constrained game. 
In Sec.~\ref{sec:analysis}, we analyze the game and derive the associated value function. 
We further investigate how the value function behaves with respect to the sensing budget $n$ in Sec.~\ref{sec:senseN}, and finally, we conclude the manuscript in Sec.~\ref{sec:conslusions}.


\section{Existing and  Preliminary Results} \label{sec:prelim}
We consider a pursuit evasion game with simple motion:
    \begin{align} \label{eq:dynamics}
    \begin{split}
    \dot{\x}_p =\v_p,\\
    \dot{\x}_e =\v_e,
    \end{split}
\end{align}
where $\x_p(t) \in \R^2$, $\x_e(t) \in \R^2$ denote the locations and $\v_p(t) \in \R^2$, $\v_e(t) \in \R^2$ denote the velocities of the pursuer and the evader, respectively,  at time $t$. 
% The instantaneous velocities of the pursuer and the evader are denoted by $\v_p(t) \in \R^2$ and $\v_e(t) \in \R^2$, respectively. 
We assume that the game starts at time $t=0$ and both the players know the initial locations $\x_p(0)$ and $\x_e(0)$.
The maximum speeds for the pursuer and the evader are $1$ and $\nu$, respectively.\footnote{
In case the maximum speeds are $\v_{p,\max}$ and $\v_{e,\max}$, we shall use scaling and time dilation to transform the system to obtain maximum speeds $1$ and $\nu$, respectively.  
To that end, we define $\nu =\v_{e,\max}/\v_{p,\max}$, $c =\v_{p,\max}$, $\bar \x_i(t) = \x_i( t/c )$  and $\bar\v_i(t) =\v_i(t/c)/c$ for $i=p, e$. 
With this new scaling, we obtain $\dot{\bar{\x}}_i = \bar{\v}_i$ with $\|\bar\v_p(t)\| \le 1$ and $\|\bar\v_e(t)\|\le \nu$.} 
We consider a faster pursuer, i.e., $\nu <1$, to avoid a trivial game.
% That is, the pursuer has a speed advantage over the evader. 
% \re{Note that, $\nu$ denotes the relative speed. } 
% Without loss of generality, we assume that the game starts at time $t=0$.


For a pursuer with capture radius $\rcap$, capture happens as soon as the distance between the pursuer and the evader becomes $\rcap$.
When both players can sense each other continuously, capture happens within a duration of $\frac{\rho_0 - \rcap}{1-\nu}$, where $\rho_0 \triangleq \|\x_p(0) - \x_e(0)\|$ is the initial distance between the players.
In this case, the equilibrium strategy for both the players is to move  along the pursuer's line of sight unit vector $\r(t) \triangleq \frac{\x_e(t)- \x_p(t)}{\|\x_e(t)- \x_p(t)\|}$ with maximum speeds for all $t$. 
% If the pursuer(evader) deviates from this strategy the evader(pursuer) does not, then the final capture time increases(decreases). 
% Let $\rhat(t) = \r(t)/\|\r(t)\||$ denote the unit vector along $\r(t)$. 
One may verify that $\dot{\r}(t) = 0$ under this equilibrium. 
Consequently, the instantaneous heading angles for both the players do not change with time.  
It is noteworthy that \textit{the players do not need their opponent's location continuously to implement their equilibrium strategies}. 
Although it appears that the sensing capability is redundant in implementing the equilibrium strategies, however, the lack thereof is detrimental for the pursuer since the evader has an incentive to deviate from the above mentioned equilibrium strategy.
% When the pursuer is capable of continuous sensing, the evader would not be able to go around the pursuer and will be captured much earlier. 
On the other hand, when the evader is incapable of continuous sensing, the pursuer still does not have any incentive to deviate from the above mentioned equilibrium strategy.
Therefore, a continuous sensing capability is crucial for the pursuer.

% \re{In this paper, we consider a scenario where the pursuer is limited in sensing. 
% In particular, the pursuer relies on a remotely located sensor (e.g., a radar station) to sense the evader's position. 
% Upon request, the sensor can perfectly sense the location of the evader and share it with the pursuer.\footnote{
% One may alternatively also consider a scenario where the pursuer has an onboard sensor to measure the evader's location, however, it can only use the sensor intermittently due to, for example, energy and computational constraints.
% } 
% The communication channel between the pursuer and the remote sensor is assumed to be noiseless, instantaneous (i.e., no delay), and perfectly reliable (i.e., no packet losses). 
% The pursuer intermittently requests the evader's location from the sensor to update its pursuit strategy. 
% Due to resource (e.g., energy) constraints, the pursuer can only make $N$ sensing requests.
% % \re{Several choices for $N$ will be discussed later.}
% On the other hand, the evader is able to perfectly sense the pursuer continuously and is aware of the sensing limitation of the pursuer. }
% \re{Does the evader know when sensing happened? Deception}

In this work we consider only an intermittent sensing capability for the pursuer. 
Similar sensing constrained pursuit-evasion games have been previously studied in \cite{aleem2015self} where the pursuer follows a self-triggered strategy to sense the evader intermittently and correct its heading angle. 
In \cite{aleem2015self}, the pursuer follows the pursuit strategy 
\begin{align} \label{eq:purserV}
   \v_p(t) = \r(t_k) \qquad \text{for all } t \in (t_k, t_{k+1}],
\end{align}
where $t_k$ denotes the $k$-th sensing instance. 
We denote the $0\text{-th}$ sensing instance to be the initial time, i.e., $t_0 = 0$.
A self-trigger function was designed in \cite{aleem2015self} to determine the sensing instances and a tight upper bound on the number of required sensing $n_{\max}$ was derived:
\begin{align}
    &n_{\max} = \bigg\lceil \frac{\log\rcap - \log\rho_0}{\log(h(\nu))}\bigg\rceil, \label{eq:nmax} \\
    &h(\nu) = 1 - \frac{\nu(1 - \nu)\sqrt{1 -\nu^2} - (1 - \nu)(1 - \nu^2)}{2\nu^2 -1}, \nonumber
\end{align}
where $\rho_0$ is the initial distance between the players.
One may verify that $1 > h(\nu) > \nu$ for all $\nu\in (0,1)$ and consequently, we get $n_{\max} \ge \big\lceil \frac{\log\rcap - \log\rho_0}{\log \nu}\big\rceil$.
Their proposed triggering strategy \cite[Eq. (6)]{aleem2015self} yields 
\[
t_{k+1} - t_k =  f(\nu) \|\x_e(t_k) - \x_p(t_k)\|,
\]
where $f(\nu) = \frac{\sqrt{1-\nu^2}}{\nu + \sqrt{1-\nu^2}}$. 
Notice that $f(\nu)$ decreases with $\nu$ and converges to $0$ as $\nu$ converges to $1$. 
In other words, the inter sensing time $(t_{k+1}- t_k)$ decreases with $\nu$. 
In the following we state a more efficient sensing strategy with two major properties: (i) The total required number of sensing is strictly less than the $n_{\max}$ given in \eqref{eq:nmax}, and  (ii) There is no degradation in pursuer's performance in comparison to the continuous sensing case, i.e., the capture time does not increase due to sensing limitation.
Before stating this main result for this section, we provide the following lemma that will be used for proving our result.

\begin{lemma} \label{lem:no-trigger}
    If the initial distance between the players is less than or equal to $ \frac{\rcap}{\nu}$, then there is no need for sensing and the equilibrium strategy for both the players is to move along the direction of $\r(0)$. \hfill $\triangle$
\end{lemma}

\begin{proof}
    Under the proposed strategies $\v_p(t) = \r(0)$ and $\v_e(t) = \nu \r(0)$, the capture time is $\frac{\rho_0 - \rcap}{1-\nu}$, where $\rho_0$ is the initial distance between the players.
    To proof that the proposed strategies form an equilibrium pair, we let the evader follow an arbitrary strategy and show that the capture time is strictly reduced. 
    To that end, note that $\x_p(t) = \x_p(0) + t\r(0)$ and $\x_e(t) = \x_e(0) + \int_0^t\v_e(s) ds$, and therefore, for all $t$,
    \begin{align*}
        \|\x_e(t) - \x_p(t)\| &= \|(\rho_0-t)\r(0) + \int_0^t\v_e(s) ds\| \\
        &\le |\rho_0 - t| + \nu t,
    \end{align*}
    where the equality holds if and only if $\v_e(t) = \nu\r(0) $ for all $t$.
    Consequently, at $t = \frac{\rho_0-\rcap}{1-\nu}$, we have $\|\x_e(t) - \x_p(t)\|\le \rcap $, which implies that capture must have happened before $t = \frac{\rho_0-\rcap}{1-\nu}$ unless the evader followed $\v_e(t) = \nu\r(0) $.
    In a similar fashion, one may also verify that there is no incentive for the pursuer to deviate from the strategy $\v_p(t) =\r(0)$.
\end{proof}

From Lemma~\ref{lem:no-trigger}, we notice that for any initial separation of $\rho_0 \le \frac{\rcap}{\nu}$,  the capture time is $\frac{\rho_0 - \rcap}{1-\nu}$.
This is the same capture time as what we also obtain from continuous sensing.
In other words, the pursuer's performance (measured by its ability to capture and the associated capture time) with or without the sensing capabilities are the same when $\rho_0\le \frac{\rcap}{\nu}$.
We now state the following Proposition on the number of required sensing for a given arbitrary initial distance $\rho_0$.

\begin{proposition}\label{prop:preResult}
Let the pursuer simply move to the last sensed location of the evader and request for a sensing as soon as it gets there. 
Then, the capture time is upper bounded by $\frac{\rho_0 -\rcap}{1-\nu}$ and the total number of required sensing is $\big\lfloor \frac{\log \rcap - \log \rho_0}{\log \nu} \big\rfloor$. \hfill $\triangle$
\end{proposition}

\begin{proof}
    Let the distance between the players at the $k$-th sensing moment be denoted by $\rho_k \triangleq \|\x_e(t_k) - \x_p(t_k)\|$. 
    The next sensing request happens when the pursuer reaches $\x_e(t_k)$.
    The time taken by the pursuer to reach the point $\x_e(t_k)$ is exactly $\rho_k$, i.e., $t_{k+1} - t_k = \rho_k$. 
    Therefore, the distance between the players at the $(k+1)$-th sensing instance is
    \begin{align*}
        \rho_{k+1} &= \|\x_e(t_{k+1}) - \x_p({t_{k+1}})\| = \|\x_e(t_{k+1}) - \x_e({t_{k}})\| \\
        &\le \nu (t_{k+1} - t_k) = \nu \rho_k \le \nu^{k+1} \rho_0.
    \end{align*}
    Therefore, with $n_{\max} = \big\lfloor \frac{\log \rcap - \log \rho_0}{\log \nu} \big\rfloor$, we obtain $\rho_{n_{\max}} \le \frac{\rcap}{\nu}$.
    At this point, we invoke Lemma~\ref{lem:no-trigger} to conclude that capture is inevitable without any further sensing.
    Furthermore, starting with an initial distance of  $\rho_{n_{\max}}$, the capture time is $\frac{\rho_{n_{\max}}-\rcap}{1-\nu}$. 
    Therefore, the total capture time is
    \begin{align*}
        T_{\rm capture} &= \frac{\rho_{n_{\max}}-\rcap}{1-\nu} + \sum\nolimits_{k=1}^{n_{\max}}(t_k - t_{k-1})\\
        & = \frac{\rho_{n_{\max}}-\rcap}{1-\nu} +\sum\nolimits_{k=1}^{n_{\max}}\rho_{k-1} \le \frac{\rho_0 - \rcap}{1-\nu},
    \end{align*}
    where we have used $t_0 = 0$ and $\rho_k \le \nu^k \rho_0$.
    This completes the proof.
\end{proof}

In Fig.~\ref{fig:compare}, we compare the number of required sensing proposed by \cite{aleem2015self} and by Proposition~\ref{prop:preResult} to demonstrate the sensing efficiency of our proposed strategy.
We notice that the difference between the $n_{\max}$ proposed in \cite{aleem2015self} and that in Proposition~\ref{prop:preResult} increases with $\nu$ and there is an order of magnitude difference (note the log scale on the $y$-axis) when $\nu$ is high.
\begin{figure}
    \centering
    \includegraphics[width = 0.6 \linewidth]{Figures/nmax_comparison.png}
    % \put (-42,132) {\textcolor{black!60}{ \small \cite{aleem2015self}}}
    \caption{\small The required number of sensing vs. $\nu$. The $y$-axis being in $\log$ scale shows that the required number of sensing according to \cite{aleem2015self} is several times higher than what is found in Proposition~\ref{prop:preResult}.}
    \label{fig:compare}
    \vspace{-12 pt}
\end{figure}
Next, we comment on the time consumed (or distance traveled) by the pursuer following the strategy in Proposition~\ref{prop:preResult}. 
This will be helpful in our subsequent analysis when we consider constraints on the fuel consumed by the pursuer (or equivalently, constraints on the game duration). 
\begin{corollary}
Given $\rho_0$ and $\rcap$, let $n$ be the smallest integer such that $\rcap > \nu^n \rho_0$.
Then, $\max\{n\!-\!1,0\}$ number of sensings are required. 
Furthermore, the pursuer will travel at most $\frac{1-\nu^{n+1}}{\!\!\!1-\nu}\rho_0$ distance before capturing the evader. \hfill $\triangle$
\end{corollary}
\begin{proof}
    The proof follows directly from Proposition~\ref{prop:preResult} where we substitute the bounds on $\rcap$ to obtain the bounds on the number of sensing and the capture time.
\end{proof}

The results presented in this section assume that the purser is able to sense $n_{\max}$ times as well as the pursuer has sufficient fuel to complete the game. 
However, in reality, there might be constraints on the duration of the game\footnote{
Given the first order dynamics \eqref{eq:dynamics}, a constraint on the fuel is cast as a constraint on the game duration. 
} as well as on the maximum number of allowed sensing. 
For the rest of the paper, we focus on a problem where the game is played for a duration of $t_f$ and the maximum number of allowed sensing is $n$. 
The objective now is to analyze how the sensing strategy depends on $t_f$ and $n$. 
Note that the sensing strategies proposed in \cite{aleem2015self} and in Proposition~\ref{prop:preResult} are agnostic to $t_f$ and $n$.
In the subsequent sections, we formalize this problem and derive the optimal sensing strategy.
At this point, note that when $t_f \ge \frac{\rho_0 - \rcap}{1-\nu}$ and $n \ge \big\lfloor \frac{\log \rcap - \log \rho_0}{\log \nu} \big\rfloor$, Proposition~\ref{prop:preResult} is sufficient to construct a sensing strategy for the pursuer. 


\section{Problem Formulation} \label{sec:formulation}
% While the pursuer has a higher speed, it is limited in terms of sensing.
% In particular, the pursuer relies on a remotely located sensor (e.g., a radar station) to sense the evader's position. 
% Upon request, the sensor can perfectly sense the location of the evader and share it with the pursuer.\footnote{
% One may alternatively also consider a scenario where the pursuer has an onboard sensor to measure the evader's location, however, it can only use the sensor intermittently due to energy and computational constraints.
% } 
% The communication channel between the pursuer and the remote sensor is assumed to be noiseless, instantaneous (i.e., no delay), and perfectly reliable (i.e., no packet losses). 
% The pursuer intermittently requests the evader's location from the sensor to update its pursuit strategy. 
% Due to resource (e.g., energy) constraints, the pursuer can only request $N$ times.
% \re{Several choices for $N$ will be discussed later.}
% On the other hand, the evader is able to perfectly sense the pursuer continuously and is aware of the sensing limitation of the pursuer. 
% \re{Does the evader know when sensing happened? Deception}


% The game is played for a finite duration of $t_f$.
The pursuer's objective is to minimize its distance from the evader at the end of the game and, if possible, to capture the evader within the game duration, in which case the game ends as soon as the evader is captured.
% In this work, we assume a capture radius of $\rcap$ for the pursuer and the capture happens as soon as the distance between the pursuer and the evader becomes $\rcap$.
The pay-off function for this game is as follows
\begin{align} \label{eq:payoff0}
    J = \begin{cases}
    0,   & \text{ if captured happened},\\
    \|\x_p(t_f)-\x_e(t_f)\| - \rcap, & \text{ otherwise}. 
    \end{cases}
\end{align}
The evader maximizes the pay-off while the pursuer minimizes it.
Although we are particularly interested in the pay-off function \eqref{eq:payoff0}, our analysis can be extended for a more general class of pay-off functions where 
\begin{align} \label{eq:payoff}
    J = \begin{cases}
    0, \qquad \qquad & \text{ if captured happened},\\
    \phi(\|\x_p(t_f)-\x_e(t_f)\|), & \text{ otherwise}. 
    \end{cases}
\end{align}
where we assume the following conditions on function $\phi$.



\begin{assumption}
\begin{itemize}
    \item[]  
    \item[(A1)] $\phi: \R \to \R_+$ is a non-decreasing function with $\phi(x) =~0$ for all $x\le \rcap$ and $\phi(x)>0$ otherwise.
    \item[(A2)] $\phi$ is convex.
\end{itemize}
\end{assumption}



Assumption (A1) incentivizes the pursuer to minimize its final distance from the evader. 
The convexity assumption keeps the formulation analytically tractable while encompassing a wider class of problems than the one presented in \eqref{eq:payoff0}.

Since the main objective of this work is to derive the optimal sensing strategy, we consider the motion strategy for the pursuer to be similar to what was given in \eqref{eq:purserV}.
\begin{assumption}
    \text{For all} $t \in (t_k, t_{k+1}]$, the pursuer follows 
    \[
   \v_p(t) = {\gamma(t)\r(t_k) }
    \]
     where $\gamma(t) \in [0,1]$ is to be designed by the pursuer. 
\end{assumption}

\begin{remark}
It is a rational choice to pick the pursuer's heading direction to be $\r(t_k)$ since the pursuer is forced to operate in `open-loop' in between sensing instances as it does not have access to $\r(t)$ continuously. 
Although it may seem that $\gamma(t)\equiv~1$ is the best choice, we observe from our analysis that, under certain choices for the parameters, $\gamma(t) = 0$ is optimal for some intervals of time. Note that $\gamma(t) =0$ represents a \textit{waiting} behavior in the pursuer's strategy. 
We prove that, under certain cases, waiting has a clear advantage over continuously moving and/or sensing earlier. 
The waiting behavior has proven to be a crucial characteristics in some of the recent works involving sensing  limited pursuit-evasion games \cite{shishika2021partial, pourghorban2022target, pourghorban2023target}. \hfill $\triangle$
\end{remark}

% The outcome of the game depends on the evader's strategy motion strategy $\mu_e$.

Although the dynamics \eqref{eq:dynamics} is deterministic, the outcome of the game is not necessarily deterministic if the players adopt randomized strategies.
Therefore, the pursuer/evader minimizes/maximizes $\E[J]$ where the expectation $\E[\cdot]$ is with respect to the randomized strategies taken by the pursuer and the evader. 
Let $\mu_e$ denote the motion strategy of the evader and $\mu_p$ denote the sensing strategy of the pursuer. 
The strategies are considered to be time dependent, however, to maintain notational brevity, we will suppress such time dependencies.
These strategies are measurable functions of the information sets of the players. 
To describe the information sets of the players, we first denote $m(t)$ to be the total number of sensing requests upto time $t$ and $\Tp(t) \triangleq \{t_0, t_1, \ldots, t_{m(t)}\}$ be the set of sensing instances upto time $t$, where $t_0$ is the initial time, which is assumed to be $0$ here, and $t_i< t_{i+1}$ for all $i$, and $t_{m(t)}$ is the latest sensing time.
For all $t$, we have $m(t) \le n$ and $t_{m(t)}\le t$.
Furthermore, we denote $\I_e(t) \triangleq \{ \x_e(s), \x_p(s), \Tp (t) ~|~ s\le t \}$ to be the information available to the evader at time $t$ and $\I_p(t) =\{\x_e(s'), \x_p(s), \Tp(t)~|~s'\in \Tp(t), s\le t\}$ to be the pursuer's available information. 
% \re{Therefore, $\mu_e(t, v~|~\I)$ denote the probability of picking a velocity vector $\v_e(t) = v$ given the information set $\I_e(t) = \I$.} 

For a given pair of evader and pursuer strategies $(\mu_e,\mu_p)$, we define
$
\bar{J}(\mu_e,\mu_p) \triangleq \E[J]
$
to be the expected pay-off from the given strategy pair. 
In the subsequent sections, we derive an equilibrium pair ($\mu_e^*, \mu_p^*$) such that for any admissible strategies $\mu_e$ and $\mu_p$,
\begin{align*}
    \Bar{J}(\mu_e^*,\mu_p) \le \Bar{J}(\mu_e^*,\mu_p^*) \le \Bar{J}(\mu_e,\mu_p^*).
\end{align*}
% \re{Furthermore, we obtain a fundamental trade-off between speed and sensing capabilities where we discuss how a lower value of $\nu$ (i.e., a higher speed for the pursuer) can compensate for a lower value of $n$ (i.e., more restricted sensing) and vice-versa. }





\section{Analysis of the Game} \label{sec:analysis}
For the subsequent analysis, we define $V(\rho,\tau,n)$ to be the expected pay-off under a Nash equilibrium where $\rho$ is the current distance between the player, $\tau$ represents the remaining game duration, and $n$ is the number of remaining sensing requests.  
Notice that, given the current time $t$, we have $\tau = t_f -t$. 
Therefore, $V(\rho_0, t_f, n) = \Bar{J}(\mu_e^*,\mu_p^*)$, where recall that $\rho_0 = \|\x_p(0) - \x_e(0)\|$.


To compute $V(\rho,t_f,n)$, we first note that, for all $n$,
\begin{align} \label{eq:VBoundary}
    V(\rho, 0, n) =  \begin{cases}
        0,  & \text{if }  \rho \le \rcap,\\
        \phi(\rho), & \text{otherwise}.
    \end{cases} 
\end{align} 
Therefore, \eqref{eq:VBoundary} defines a boundary condition on $V$. 
To simplify our derivation for $V(\rho_0, t_f, n)$, let us divide the entire game duration into  intervals $[t_0, t_1], [t_1,t_2],\dots,[t_n, t_f]$, where $t_i$ denotes the $i$-th sensing time. 
We construct $V(\rho, \tau_i, n-\!i)$ backwards starting with $i=n$, where $\tau_i = t_f - t_i$. 
The following lemma computes $V(\rho,\tau,0)$ for any $\rho$ and $\tau$.



% ===== OLD Lemma and Proof ================
% \begin{lemma} \label{lem:stage0}
%     Let $t_n$ be the $N$-th sensing time and $\rho$ be the distance between the players at that instance. 
%     Let $\tau \triangleq t_f - t_n$ denote the remaining game duration. Then, for $\nu < \re{0.6}$,
%     \begin{align*}
%        & V(\rho,t_n,0) = \\
%        &\begin{cases}
%             \phi(\rho-(1-\nu)\tau) &\text{if } \tau < \rho,~ \tau < \frac{\rho -\rcap}{1-\nu}, \\
%             0 &\text{if } \tau < \rho,~ \tau \ge \frac{\rho -\rcap}{1-\nu}, \\
%             \phi(\nu \tau) &\text{if } \tau \ge \rho,~  \frac{\sqrt{2} \nu \rho}{1+\nu^2} > \rcap, \\
%             \phi(\rho-(1-\nu)\tau) &\text{if } \tau \ge \rho,~ \frac{\sqrt{2} \nu \rho}{1+\nu^2} \le \rcap,~ \tau < \frac{\rho -\rcap}{1-\nu}, \\
%             0 &\text{otherwise. } %\tau \ge \rho,~ \frac{\sqrt{2} \nu \rho}{1+\nu^2} \le \rcap,~ \tau \ge \frac{\rho -\rcap}{1-\nu}
%         \end{cases}
%     \end{align*}
% \end{lemma}

% \begin{proof} 
%     We divide the proof into two cases: $\tau < \rho$ and  $\tau \ge \rho$.

%     \uline{\textit{Case} $\tau < \rho$}:     In this case we claim that the equilibrium motion strategy for the pursuer is $\v_p(t) = \frac{\x_e(t_n) - \x_p(t_n)}{\|\x_e(t_n) - \x_p(t_n)\|}$ and for the evader $\v_e(t) = -\nu \frac{\x_e(t_n) - \x_p(t_n)}{\|\x_e(t_n) - \x_p(t_n)\|}$. 
%     This is a pure strategy equilibrium and it is time invariant. 
%     To prove that the mentioned strategies form an equilibrium pair, we first compute the pay-off resulting from this strategy pair and then we show that an unilateral derivation in the strategy leads to a worse pay-off for the deviating player. 

%     To compute the pay-off for the given strategy pair, we obtain for any $t \in [t_n, t_f]$: 
%     \[
%     \|\x_p(t)-\x_e(t)\| = |\rho - (1-\nu)(t-t_n)|.
%     \]
%     When $\tau < \frac{\rho-\rcap}{1-\nu}$, we have 
%     \begin{align*}
%         \|\x_p(t)-\x_e(t)\|= \rho - (1-\nu)(t-t_n) > \rcap,
%     \end{align*}
%     for all $t \in [t_n, t_f]$, i.e., capture does not happen and therefore, the pay-off becomes $\phi(\|\x_p(t_f)-\x_e(t_f)\|) = \phi(\rho - (1-\nu)\tau)$. 
%     On the other hand, when $\tau \ge \frac{\rho-\rcap}{1-\nu}$,  capture happens at $t=t_n+ \frac{\rho-\rcap}{1-\nu}$, and the resulting pay-off is $0$.

% Now we show that any deviation from the proposed strategy does not benefit the deviating player. 
% To that end, let us consider that the pursuer deviates and therefore $\x_p(t) = \x_p(t_n) + \int_{t_n}^t \v_p(s)ds$ and
% \begin{align*}
%     &\|\x_p(t) - \x_e(t)\| \\
%     &= \sqrt{\rho^2 + \nu^2 (t-t_n)^2 + 2(\rho - \nu(t-t_n)) \int_{t_n}^t {\hat{d}}\T\v_p(s)ds},
% \end{align*}
% where $\hat{d} \triangleq \frac{\x_e(t_n) - \x_p(t_n)}{\|\x_e(t_n) - \x_p(t_n)\|}$. 
% Since ${\hat{d}}\T\v_p(s) \ge -\|{\hat{d}}\| \|\v_p(s)\|$ $ =-1$, we obtain, 
% \begin{align*}
%     \|\x_p(t)-\x_e(t)\| \ge |\rho - (1-\nu)(t-t_n)|.
% \end{align*}
% Therefore, capture does not happen when $\tau < \frac{\rho-\rcap}{1-\nu}$ and pay-off is more than or equal to $\phi(\rho - (1-\nu)\tau)$ since $\phi$ is a non-decreasing function and $\|\x_p(t_f)-\x_e(t_f)\| \ge \rho - (1-\nu)\tau$. 
% Similarly, one may also verify that the pay-off is not improved for the pursuer for the case $\tau \ge \frac{\rho-\rcap}{1-\nu}$.

% Let us now consider that the evader deviates and we verify that 
% \begin{align*}
%     &\|\x_p(t)- \x_e(t)\| \le |\rho - (1-\nu)(t-t_n)|
%  \end{align*}
% for all $t$. 
% By a similar argument as before, one may verify that the evader's performance does not improve from this deviation.  \\


% \textbf{\textit{Case $\tau \ge \rho,~~ \frac{\sqrt{2} \nu \rho}{1+\nu^2} > \rcap$}:}
% In this case, the equilibrium strategy for the pursuer is to go the last sensed location of the evader and stay there afterwards. 
% The strategy for the evader is to pick an angle $\theta = \pm \pi_2$ at time $t_n$ and then move toward the direction $R(\theta)\hat{d}$ for the entire direction, where $R(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta \\
% \sin\theta & \cos\theta
% \end{bmatrix}$ is the rotation matrix and $\hat{d}= \x_e(t_n) - \x_p(t_n)$.\\


% \textbf{\textit{Case $\tau \ge \rho,~~ \frac{\sqrt{2} \nu \rho}{1+\nu^2} \le \rcap$}:} 
% \end{proof}
% ==========================================


\begin{lemma} \label{lem:stage0}
    Let $\rho$ be the distance between the players at last sensing instance. 
     Then, 
     \begin{align} \label{eq:lemma0}
        & V(\rho,\tau,0) \le \begin{cases}
            % \phi(\rho-(1-\nu)\tau) &\text{if } \tau < \rho,~ \tau < \frac{\rho -\rcap}{1-\nu}, \\
            % 0 &\text{if } \tau < \rho,~ \tau \ge \frac{\rho -\rcap}{1-\nu}, \\
            % \phi(\nu \tau) &\text{if } \tau \ge \rho,~  \frac{\sqrt{2} \nu \rho}{1+\nu^2} > \rcap, \\
            0, & \text{if } \tau \ge \rho,~ \nu\rho \le \rcap, \\
            \!\phi(\nu\tau + [p-\tau]^+\!), &\text{otherwise. } %\tau \ge \rho,~ \frac{\sqrt{2} \nu \rho}{1+\nu^2} \le \rcap,~ \tau \ge \frac{\rho -\rcap}{1-\nu}
        \end{cases}
        % V(\rho, t_n, 0) \le \phi(\nu\tau + [p-\tau]^+)
    \end{align}
    where we define $[x]^+ = \max\{x,0\}$. \hfill $\triangle$
\end{lemma}

\begin{proof}
    The Proof is presented in Appendix~\ref{AP:poof1}.
\end{proof}

%=======================

% Two main observation from Lemma~\ref{lem:stage0}: \re{the need for $\nu< 0.6$} and \re{we considered a much generic motion strategy for the pursuer}.



% Extension of Lemma~\ref{lem:stage0} for $\nu> \re{0.6}$ requires somewhat complicated analysis to carefully handle design the evader's trajectory so that it can avoid being captured while moving past the pursuer. 
% Interested readers are referred to \re{Appendix~\ref{AP:extra}} for a detailed analysis on this case. 
\vspace{16 pt}
Two important remarks are in order from Lemma~\ref{lem:stage0}, one regarding the tightness of the inequality \eqref{eq:lemma0} and the other regarding the optimality of a \textit{waiting behavior} in the pursuer's strategy. 

\begin{remark}
    Although Lemma~\ref{lem:stage0} provides an upper bound on $V(\rho, t_n, 0)$, the upper bound is tight everywhere except in the region $\Omega_0 \triangleq \{t_f-t_n\ge \rho, ~\rcap < \nu\rho \le \sqrt{1+\nu^2}\rcap$\} as discussed in \textbf{Case 2b} in Lemma~\ref{lem:stage0}. 
    In practice, this region is small as can be seen in Fig.~\ref{fig:upperBoundVisualization}(b).~$\triangle$
\end{remark}

\begin{remark}
    We notice in the proof of Lemma~\ref{lem:stage0} that, when $\tau \ge \rho$, the optimal motion strategy for the pursuer is to move to the last sensed location of the evader, $\x_e(t_n)$, and \textit{wait} there.  
    This is quite intuitive since moving at any arbitrary direction without the knowledge of the evader's location is more harmful than beneficial. 
    The pursuer could have also waited at the beginning (or at any other point) of the game for a duration of $\tau-\rho$ and then move toward $\x_e(t_n)$.
    Alternatively, the pursuer could have moved for the entire duration with a slower speed. 
    In summary, all the scenarios lead to the same conclusion that $\gamma(t) \equiv 1$ is not an optimal motion strategy for the entire duration. 
\end{remark}


\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45 \linewidth}
    \includegraphics[trim = 10 0 25 10, clip, width =  \linewidth]{Figures/V0.pdf}
    \caption{}
    \end{subfigure}
     \begin{subfigure}[b]{0.45 \linewidth}
         \includegraphics[trim = 10 0 25 10, clip, width =  \linewidth]{Figures/V_Projection.pdf}
         \caption{}
     \end{subfigure}
    \caption{\small We choose $\rcap = 0.1$ and $\nu = 0.7$, and $\phi(x)= [x - \rcap]^+$ for this plot. (a) The upperbound of $V(\rho,\tau,0)$ from Lemma~\ref{lem:stage0}. (b) Contour plot of $V(\rho,\tau,0)$ on the $(\rho,\tau)$ plane. The numbers on each contour show the value of $V(\rho,\tau,0)$ along that contour. The thin white region approximately along $\rho = 0.15$ describes the region $\Omega_0$.
    This is the only region in which we have an upper bound of $V(\rho,\tau,0)$ instead of an exact expression.}
    \label{fig:upperBoundVisualization}
    \vspace{- 6 pt}
\end{figure}

The following theorem is the main result of this section where we provide an upper bound on $V(\rho,\tau,\ell)$ for all $\rho, \tau$ and $\ell \le n$. 
The upper bound is tight everywhere except in the region $\{\tau \ge \frac{1-\nu^{\ell +1}}{1-\nu}\rho, \rcap \le \rho \le \sqrt{1+\nu^2}\rcap\} \subseteq \Omega_0$.

\begin{theorem} \label{thm:main}
 Let $t_k$ be the $k$-th sensing time and $\rho$ be the distance between the players at that instance. 
    Let $\tau \triangleq t_f - t_k$ and $ \ell = n -k$ denote the remaining time and the remaining number of sensing, respectively. Then, 
 \begin{align*}
     V(\rho, \tau, \ell) \le  \begin{cases}
         0, \qquad \quad\quad\text{if } \tau \ge\!\!&\frac{1-\nu^{\ell +1}} {1-\nu}\rho, ~~\nu^{\ell +1}\rho \le \rcap,\\
         \phi(\nu\tau+\rho - \tau), &\text{if } \tau \le \frac{1-\nu^{\ell +1}} {1-\nu}\rho,\\
         \phi(\frac{1-\nu}{1-\nu^{\ell +1}}\nu^{\ell +1}\tau), & \text{otherwise}. \hfill \triangle
     \end{cases}
 \end{align*} 

\end{theorem}

\begin{proof}
 The proof follows similar steps as Lemma~\ref{lem:stage0}. Only a sketch of the proof is given.

\textbf{Case 1: $\tau \ge \frac{1-\nu^{\ell +1}} {1-\nu}\rho, ~~\nu^{\ell +1}\rho \le \rcap$} \\
The strategy for the pursuer is to go to the last sensed location of the evader and request for sensing. 
The evader on the other hand shall go along one of the directions $\pm \r(t_i)^\perp$ chosen uniformly randomly for the interval $(t_i, t_{i+1}]$, where $i=0,1,\ldots, \ell$, and $t_{\ell+1}= t_f$.

\textbf{Case 2: $\tau \le \frac{1-\nu^{\ell +1}} {1-\nu}\rho$}\\
The strategy for the pursuer remains the same as the last case. 
The strategy for the evader also remains the same for the intervals $\{(t_i, t_{i+1}]\}_{i=0}^{\ell -1}$.  
During the interval $(t_\ell, t_f]$, the evader shall go along $\r(t_\ell)$ (this interval falls under \textbf{Case~3} of Lemma~\ref{lem:stage0}).

\textbf{Case 3: $\tau \ge \frac{1-\nu^{\ell +1}} {1-\nu}\rho, ~~\nu^{\ell +1}\rho > \rcap$}\\
The strategy for the defender is to go to $\x_e(t_0)$ and wait there for $w = \frac{1-\nu}{1-\nu^{\ell+1}}\tau - \rho $ amount of time before requesting the first sensing.
Therefore, at the moment of first sensing, the remaining distance, time, sensing requests are  $\rho' = \nu \frac{1-\nu}{1-\nu^{\ell+1}}\tau$, $\tau' = \frac{\nu(1 - \nu^\ell)}{1-\nu^{\ell+1}}\tau $, and $\ell' = \ell -1$, respectively.
The pay-off starting from this configuration is $V(\rho', \tau',\ell') = \phi(\nu\tau' + \rho' - \tau')$ since $\rho',\tau'$, and $\ell'$ satisfy $\tau' = \frac{1 - \nu^{\ell'+1}}{1-\nu}\rho'$ and we may invoke \textbf{Case 2} of Theorem~\ref{thm:main}.  
By substituting the expressions of $\rho',\tau'$ and $\ell'$, we obtain 
$
\phi(\nu\tau' + \rho' - \tau') = \phi(\frac{1-\nu}{1-\nu^{\ell +1}}\nu^{\ell +1}\tau).
$
In this case, the evader shall move in the direction $\pm \r(t_i)^\perp$ for all the intervals $(t_i, t_{i+1}]$ if $\rcap< \sqrt{1 +\nu^2}\rho$, and consequently $ V(\rho,\tau,\ell) = \phi(\frac{1-\nu}{1-\nu^{\ell +1}}\nu^{\ell +1}\tau)$.
For the case $\rcap \ge \sqrt{1+\nu^2}\rho$, the evader shall move in a fashion to escape from being captured in the first interval $(t_0, t_1]$. In this case, $\phi(\frac{1-\nu}{1-\nu^{\ell +1}}\nu^{\ell +1}\tau)$ is an upper bound of $V(\rho,\tau,\ell)$. 
\end{proof}


\begin{remark}
    According to Theorem~\ref{thm:main}, the pursuer must wait before requesting the first sensing. The waiting time is constructed in such a way that the total distance traveled starting from $t_1$ (i.e., the first sensing instance) is the same as the duration left. 
    In this way the pursuer does not need to wait in the future. 
    Furthermore, it can be shown that, if the pursuer requests the first sensing before waiting for $\frac{1-\nu}{1-\nu^{\ell+1}}\tau - \rho $, then that results in a suboptimal outcome for the pursuer.  \hfill $\triangle$
\end{remark}

Using the result from Theorem~\ref{thm:main}, we plot $V(\rho,\tau,\ell)$ in Fig.~\ref{fig:Vn}. 
For a given $\rho$ and $\tau$, $V(\rho,\tau,\ell)$ is a non-increasing function of $\ell$, as one would have expected. 
In the next section, we investigate how $V(\rho_0,t_f, n)$ behaves with respect $n$.  


\begin{figure*}
\includegraphics[trim = 0 0 10 10, clip, width = 0.19 \linewidth]{Figures/N1.pdf}
\includegraphics[trim = 0 0 10 10, clip, width = 0.19 \linewidth]{Figures/N2.pdf}
\includegraphics[trim = 0 0 10 10, clip, width = 0.19 \linewidth]{Figures/N3.pdf}
\includegraphics[trim = 0 0 10 10, clip, width = 0.19 \linewidth]{Figures/N4.pdf}
\includegraphics[trim = 0 0 10 10, clip, width = 0.19 \linewidth]{Figures/N5.pdf}
\caption{\small Contour plots of $V(\rho,\tau,\ell)$ from $\ell=1$ to $\ell = 5$. $\ell$ gradually increases from the left most subfigure to the right most one.}
\label{fig:Vn}
\vspace{- 6 pt}
\end{figure*}

\section{Performance degradation due to sensing limitation} \label{sec:senseN}
The objective of this section is to quantify the degradation in the pursuer's performance due to its sensing limitation. 
For an initial distance of $\rho_0$ and a game duration of $t_f$, the pursuer's pay-off under continuous sensing is 
$\phi([\rho_0 - (1-\nu)t_f]^+)$, which will serve as the baseline for quantifying the performance degradation.
% We will compare the degradation in the pursuer's pay off with respect to the quantity $\phi([\rho_0 - (1-\nu)t_f]^+)$.
In particular, for a given $\nu,\rho_0$ and $t_f$, we use the metric $\delta(n) \triangleq V(\rho_0,0,n) - \phi([\rho_0 - (1-\nu)t_f]^+)$ to quantify the performance degradation.
Since we only have an upper bound of $V(\rho,\cdot,\cdot)$ in $\Omega_0$, we exclude this region from the discussion in this section and make the following assumption.
\begin{assumption}
    We assume that $\nu \rho_0 > \sqrt{1 + \nu^2} \rcap$.
\end{assumption}



In the following, we first state a lemma that provides the required number of sensing $n^*$ to ensure $\delta(n^*)=0$.  
\begin{lemma} \label{lem:mSensing}
    For a given initial distance $\rho_0$ and a game duration $t_f$, the pursuer's pay-off is $\phi([\rho_0 - (1-\nu)t_f]^+)$ with $n^*$ sensing, where
    \begin{align} \label{eq:mSensing}
        n^* = \begin{cases}
            \big\lfloor\frac{\log(\rho_0 - (1-\nu)t_f) - \log(\rho_0)}{\log (\nu)}\big\rfloor, &\text{if }  t_f < \frac{\rho_0 - \rcap}{1-\nu},\\
            \big\lfloor\frac{\log(\rcap) - \log(\rho_0)}{\log (\nu)}\big\rfloor, &\text{otherwise}.
        \end{cases}
    \end{align} 
    \hfill $\triangle$
\end{lemma}

% The proof has been omitted due to page limitation. 
Lemma~\ref{lem:mSensing} is an extension of Proposition~\ref{prop:preResult} where we now have incorporated the role of  $t_f$ into the number of required sensing. 
Proposition~\ref{prop:preResult} was derived under the case in which the pursuer had sufficient time (i.e., $t_f \ge \frac{\rho_0 - \rcap}{1-\nu}$) to capture.
One may verify from \eqref{eq:mSensing} that, for any $t_f$, the required number of sensing is upper bounded by $\big\lfloor\frac{\log(\rho_0 - (1-\nu)t_f) - \log(\rho_0)}{\log (\nu)}\big\rfloor$, which is the same as in Proposition~\ref{prop:preResult}. 

We now derive the degradation in pursuer's performance when the available number of sensing is less than $n^*$. 
We discuss it separately for $t_f \ge \frac{\rho_0-\rcap}{1-\nu}$ and for $t_f < \frac{\rho_0-\rcap}{1-\nu}$. 
When $t_f \ge \frac{\rho_0-\rcap}{1-\nu}$, the purser is able to capture the evader according to Lemma~\ref{lem:mSensing} whereas with $n$ number of sensings the pursuer's pay-off is $\phi(\frac{1-\nu}{1-\nu^{n+1}}\nu^{n+1}t_f)$  and therefore, $\delta(n) = \phi(\frac{1-\nu}{1-\nu^{n+1}}\nu^{n+1}t_f)$. 

On the other hand, when  $t_f\!<\!\frac{\rho_0-\rcap}{1-\nu}$, one may verify that
\[
t_f \ge \frac{\rho_0 - \nu^{n+1}\rho_0}{1-\nu}, \quad \text{and  }  \nu^{n+1}\rho_0 > \rcap 
\]
 for any integer $n<n^*$, where $n^*$ is given in \eqref{eq:mSensing}. 
Therefore, for all $n<n^*$, we obtain
\begin{align*}
    \delta(n) &= \phi(\frac{1-\nu}{1-\nu^{n+1}}\nu^{n+1}t_f) - \phi(\rho_0 - (1-\nu)t_f) \\
    & \ge \underset{\triangleq \beta(n)}{\underbrace{  \big(\frac{\nu^{n+1}}{1-\nu^{n+1}}\frac{(1-\nu)t_f}{\rho_0 -(1-\nu)t_f} - 1 \big)}}\phi(\rho_0 - (1-\nu)t_f)
\end{align*}
where we have used the Jensen's inequality for the convex function $\phi$.\footnote{For $x\ge y$, $\phi(y) = \phi(\lambda x) \le \lambda \phi(x) + (1-\lambda) \phi(0) = \lambda \phi(x)$, where $\lambda = \frac{y}{x}$.}
Notice that the coefficient $\beta(n)$ depends on $n$ exponentially and therefore, the degradation is exponential with $n$.
This is demonstrated in Fig.~\ref{fig:beta(n)}. 

\begin{figure}
    \centering
    \includegraphics[trim = 10 0 10 10, clip, width = 0.6 \linewidth]{Figures/beta_n.pdf}
    \caption{$\beta(n)$ vs. $n$ for different values of $\nu$. For all the plots we pick $\rcap = 0.1,$ $ \rho_0 = 5$ and $t_f = 0.9\frac{\rho_0 - \rcap}{1-\nu}$.}
    \label{fig:beta(n)}
\end{figure}

% Notice the performance of the pursuer does not degrade in the parametric regions $\Omega_1$ and $\Omega_2$. 
% In region $\Omega_3$, the received pay-off by the pursuer is $\phi(\frac{1-\nu}{1-\nu^{n+1}}\nu^{n+1}\tau)$ whereas the best possible pay-off could have been $0$ since the pursuer with continuous sensing capability would have enough time (i.e., $\tau \ge \frac{\rho -\rcap}{1-\nu}$) to capture.

\section{Conclusions} \label{sec:conslusions}
In this work, we considered a sensing limited pursuit-evasion game where the pursuer is restricted to intermittent sensing.
We derived the number of required sensing $n_{\max}$ in Proposition~\ref{prop:preResult} to ensure capture.
Next, we considered the game under explicit sensing and time/fuel budgets, i.e., the pursuer has $t_f$ amount of time to capture the evader while using a maximum of $n$ sensing requests. 
An upper bound on the value function for this game has been obtained, where the upper bound is tight everywhere except in a very small region $\Omega_0$. 
Future work will focus on the case where the evader is also equipped with an intermittent sensing capability. 
Finding an equilibrium strategy is necessary for applications where sensing is expensive and/or undesired for both agents. 
In addition, tightening the upper bound of $V$ in $\Omega_0$ is also an open problem. 



\bibliographystyle{IEEEtran}
\bibliography{arxiv.bib}

\appendix 
\section{Proof of Lemma~\ref{lem:stage0}} \label{AP:poof1}

\begin{proof} 
\textbf{{Case 1: $\tau \ge \rho,~  \nu \rho \le \rcap$}} \\
In this case, the pursuer moves along $\r(t_n)$ and capture is inevitable regardless of the evader's strategy.
% \textbf{\textit{Case $\tau \ge \rho,~  \nu\rho \le \rcap$}:} \\
% Given $\nu \le \sqrt{\sqrt{2}-1}$ and $\frac{\sqrt{2} \nu \rho}{1+\nu^2} \le \rcap$, we notice that $\rho \le \rcap/\nu$.
% Since $\tau \ge \rho$ we invoke Lemma~\ref{lem:no-trigger} to conclude that capture will happen and hence $V(\rho, t_n, 0) = 0$.


\textbf{{Case 2a: $\tau \ge \rho,~  \nu\rho > \sqrt{1+\nu^2}\rcap$}} \\
In this case, the pursuer shall move to the last sensed location of the evader and stay there until the end of the game. 
The evader shall pick a direction between $\r(t_n)^\perp$ and $-\r(t_n)^\perp$ and move along that direction with maximum speed, where $\r(t_n)^\perp$ denotes an unit vector perpendicular to $\r(t_n)$. 
Notice that, 
\begin{align*}
    \x_p(t) &= \x_p(t_n) + \r(t_n) \int_{s=t_n}^t\gamma(s) ds,\\
    \x_e(t) &= \x_e(t_n) + \theta \nu(t-t_n) \r(t_n)^\perp,
\end{align*}
where $\gamma(s) = 1$ for all $s\in [t_n, t_n+\rho]$ and zero otherwise, and $\theta \in \{-1, 1\}$ is chosen uniform randomly by the evader at time $t_n$. 
One may verify that the minimum distance between the players during the interval $[t_n, t_n+\rho]$ is $\frac{\nu \rho}{\sqrt{1+\nu^2}}$. 
Since we are considering the case $\nu\rho > \sqrt{1+\nu^2} \rcap$, capture does not happen during the interval $[t_n, t_n + \rho]$.
To show that it is an equilibrium pair we prove that unilateral deviation in any of the player's strategy does no improve their pay-off. 
From the evader's perspective, since the pursuer is going to stop at $\x_e(t_n)$, the final distance between the players is independent of the evader's heading direction as long as the evader does not get captured before time $t_n+\rho$. 
Therefore, the evader does not have any incentive deviate. 
On the other hand, let us assume that the pursuer deviates from the proposed strategy and ends up at a different point $\x_p(t_n) + \alpha_1 \r(t_n) + \alpha_2 \r(t_n)^\perp$ where $\alpha_1, \alpha_2 \in [-\tau, \tau]$ and $ \sqrt{\alpha_1^2 + \alpha_2^2} \le \tau$.
One may verify that, 
\begin{align*}
    \|\x_e(t_f) - \x_p(t_f)\| &= \|(\rho - \alpha_1) \r(t_n) + (\theta \nu \tau-\alpha_2) \r(t_n)^\perp\|\\
    & = \sqrt{(\rho-\alpha_1)^2 + ( \theta \nu\tau-\alpha_2)^2} \triangleq g(\theta).
\end{align*}
Notice that $g(\theta)$ is convex in $\theta$, and therefore, taking expectation of $g(\theta)$ and using Jensen's inequality yields 
\begin{align*}
    \E[g(\theta)] &\ge \sqrt{(\rho-\alpha_1)^2 + \E[( \theta \nu\tau-\alpha_2)^2]} \\
& = \sqrt{(\rho-\alpha_1)^2 + \nu^2\tau^2 + \alpha_2^2 } 
\end{align*}
where the last equality is obtained using $\theta^2 = 1$ and $\E[\theta]= 0$.
Therefore, the expected pay-off is 
\begin{align*}
    \E[\phi(\|\x_e(t_f) - \x_p(t_f)\|)] & \overset{(\dagger)}{\ge}  \phi(\E[\|\x_e(t_f) - \x_p(t_f)\|]) \\
    % & \overset{(\#)}{\ge}  \phi(\|\E[\x_e(t_f)] - \x_p(t_f)\|) \\
    & \ge \phi(\sqrt{(p-\alpha_1)^2 +\nu^2\tau^2 + \alpha_2^2})
\end{align*}
where the inequality $(\dagger)$ is obtained by using Jensen's inequality to the convex function $\phi$. 
Since $\phi$ is also non-decreasing, the optimal choices for $\alpha_1$ and $\alpha_2$ are $\rho$ and $0$, respectively. 
Consequently, we obtain the expected pay-off to be $\phi(\nu \tau)$, which is the same as $\phi(\nu \tau + [\rho -\tau]^+)$
since $\tau \ge \rho$.

\textbf{{Case 2b: $\tau \ge \rho,~  \rcap < \nu\rho \le \sqrt{1+\nu^2}\rcap$}}\\
We prescribe the pursuer to follow the same strategy as in the last case. 
Notice that, since $\nu\rho \le (1+\nu^2)\rcap$, the evader cannot move in the perpendicular directions for the entire duration without being captured.
Nonetheless, given that the evader picks a trajectory that does not lead to capture (such a trajectory exists), the final distance between the players will be at most $\nu\tau + (\rho-\tau)$. %\footnote{\re{Please see Appendix for a detailed derivation on this case.}}  
Consequently, the pay-off from this case is no more than $\phi(\nu\tau+[\rho-\tau]^+)$.

\textbf{{Case 3 $\tau < \rho$}:} \\
In this case the optimal strategy for both the players is to move along $\r(t_n)$ for the entire duration. 
One may verify that the proposed strategies constitute an equilibrium pair by considering unilateral deviations in the players' strategies.  
In this case, 
$\|\x_e(t_f) - \x_p(t_f)\|= \rho -(1-\nu)\tau = \nu \tau + (\rho -\tau)$ and thus, the pay-off can be written as $\phi(\nu\tau + [\rho-\tau]^+)$. 




 This completes the proof.
\end{proof}

\end{document}

