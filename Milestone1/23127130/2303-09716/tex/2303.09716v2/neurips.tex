\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023

% ready for submission
\usepackage[preprint]{neurips_2023}
\usepackage{enumitem}
\usepackage{times,tcolorbox}
\usepackage{custom}
\newtheorem{assumption}{Assumption}
\newcommand\norm[1]{\lVert#1\rVert}
\newcommand\normx[1]{\Vert#1\Vert}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{comment}
\renewcommand{\thesection}{\Alph{section}}
%\usepackage[linesnumbered]{algorithm2e}
%\usepackage[linesnumbered,ruled]{algorithm2e}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[caption=false]{subfig}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{algorithm}

\makeatletter
\newcommand{\AlgoResetCount}{\renewcommand{\@ResetCounterIfNeeded}{\setcounter{AlgoLine}{0}}}
\newcommand{\AlgoNoResetCount}{\renewcommand{\@ResetCounterIfNeeded}{}}
\newcounter{AlgoSavedLineCount}
\newcommand{\AlgoSaveLineCount}{\setcounter{AlgoSavedLineCount}{\value{AlgoLine}}}
\newcommand{\AlgoRestoreLineCount}{\setcounter{AlgoLine}{\value{AlgoSavedLineCount}}}
\makeatother

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{A New Policy Iteration Algorithm For Reinforcement Learning In Zero-Sum Markov Games}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Anna Winnicki\\
  Department Of Electrical Engineering\\
  Coordinated Science Laboratory \\ 
  University of Illinois Urbana-Champaign\\
  Champaign, Illinois 61801 \\
  \texttt{annaw5@illinois.edu} \\
  % examples of more authors
   \And
   R. Srikant \thanks{R. Srikant is also affiliated with c3.ai DTI.}\\
  Department Of Electrical Engineering\\
  Coordinated Science Laboratory \\ 
  University of Illinois Urbana-Champaign\\
  Champaign, Illinois 61801 \\
  \texttt{rsrikant@illinois.edu}   
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}

\maketitle

\begin{abstract}
Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the planning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in \cite{hansen2013strategy} that one has to solve $\Omega(1/(1-\alpha))$ MDPs, where $\alpha$ is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and converges exponentially fast. The only addition we propose to naive policy iteration is the use of lookahead in the policy improvement phase. This is appealing because lookahead is anyway often used in RL for games. We further show that lookahead can be implemented efficiently in linear Markov games, which are the counterpart of the linear MDPs and have been the subject of much attention recently. We then consider multi-agent reinforcement learning which uses our algorithm in the planning phases, and provide sample and time complexity bounds for such an algorithm. 
\end{abstract}

\section{Introduction}
Multi-agent reinforcement learning algorithms have contributed to many successes in machine learning, including games such as chess and Go \cite{silver2016mastering, DBLP:journals/corr/MnihBMGLHSK16, silver2017mastering, silver2017shoji}, automated warehouses, robotic arms with multiple arms \cite{gu2017deep}, and autonomous traffic control \cite{yang2020multi, shalev2016safe}; see \cite{ozdaglar2021independent, zhang2021multi, yang2020overview} for surveys. Multi-agent RL can refer to one of many scenarios: (i) where a team of agents work towards a common goal with all the agents have the same information or not \cite{qu2022scalable}, (ii) non-cooperative games where there are multiple agents with their own objectives \cite{zhang2019non}, and (iii) zero-sum games where there are only two players with opposing objectives. We can further categorize problems as infinite-horizon, discounted reward/cost, finite-horizon, simultaneous move or turn-based games. The literature in this area is vast; here we focus on zero-sum, simultaneous move, discounted reward/cost Markov games.

In the case of model-based zero-sum discounted reward/cost simultaneous-move Markov games, the recent work of \cite{zhang2020model} analyzes the setting where a model is estimated from data and a general algorithm of one's choice including value iteration, policy iteration, etc. is used to find the Nash equilibrium.  Additionally, there are multiple RL algorithms for other versions of model-based multi-agent RL including turn-based Markov games \cite{sidford2020solving}, finite-horizon games \cite{bai2020provable,liu2021sharp}, among others. 

 
In the above references, model-based algorithms generally consist of two phases: a learning phase where the probability transition matrix and average reward/cost are estimated, possibly using a generative model, and a planning phase. The results in \cite{zhang2020model} are obtained assuming that there exists an efficient algorithm for planning. As is well known, there are two broad classes of convergent, easy-to-implement algorithms to find the optimal policy for the planning problem in the case of discounted-reward MDPs, namely policy and value iterations \cite{bertsekastsitsiklis}. However, the situation is more complicated in the case of zero-sum Markov games. While value iteration naturally extends to zero-sum Markov games \cite{Shapley}, the extension of policy iteration to zero-sum Markov games is problematic. The only known convergent algorithm requires solving $\Omega(1/(1-\alpha))$ MDPs \cite{hansen2013strategy}, where $\alpha$ is the discount factor. There is an alternative algorithm, called naive policy iteration, which requires far fewer computations, but is only known to converge that under restrictive assumptions \cite{pollatschek1969algorithms}. In fact, \cite{van1978discounted} shows that the algorithm does not converge, in general.

So a longstanding open question (for at least 53 years!) is whether naive policy iteration converges for a broad class of models. Often other attempts to answer this question have only succeeded in proving the convergence of modified versions of the algorithm for restrictive classes of games. A significant recent contribution in \cite{ bertsekas2021distributed} is a modification of naive policy iteration that converges but requires more storage. However, this algorithm does not yet appear to have an extension to the function approximation setting even for Markov games with special structure such as linear Markov games. An extension of \cite{bertsekas2021distributed} can be found in \cite{brahma2022convergence} which studies stochastic and optimistic settings. Our contribution is a modified version of the naive policy iteration algorithm that converges exponentially fast for all discounted-reward/cost zero-sum Markov games. The modification is easy to explain: simply replace the policy improvement step with a lookahead version of policy improvement. Lookahead has been widely used in RL from the early application to backgammon \cite{tesauro1996line} to recent applications such as Chess and Go \cite{silver2017mastering}. But to the best of our knowledge, our result is the first which proves the convergence of naive policy iteration using lookahead. Additionally, we show lookahead has low computational complexity for the class of linear Markov games, which are a natural generalization of linear MDPs, which have been studied extensively recently \cite{agarwal2020flambe, uehara2021representation,zhang2022making}. We note that our result complements the recent results on the benefits of lookahead to improve the convergence properties of MDPs with \cite{annaor, annacdc, winnicki2023convergence} and without function approximation \cite{efroni2018multiple,efroni2018,  efroni2019combine, tomar2020multistep} in other different contexts. 


In fact, our results are for an algorithm which subsumes policy iteration and value iteration as special cases. Following \cite{perolat2015approximate}, we call the algorithm generalized policy iteration, although the MDP version of the algorithm goes by several names including modified policy iteration \cite{Puterman1978ModifiedPI} and optimistic policy iteration \cite{bertsekastsitsiklis}. Further, to show the applicability of our result, and in particular, to show that it can be combined with learning algorithms for zero-sum Markov games, we present a sample complexity result using the learning phase of the algorithm in \cite{zhang2020model} along with our generalized policy iteration algorithm for planning.





%Among the main challenges of multi-agent RL is the  computational complexity of solving the problem even when the model is known. Unlike solving single-agent RL problems, the multi-agent RL problem is much more involved due to the non-stationarity during the learning process. As such, one remedy to the challenge involves decoupling the learning and planning phases. In other words, at each iteration of several recent algorithms (kaiqing, chi jin sharp), the system performs a model-based technique such as value-iteration (shapley) or generalized policy iteration \cite{patekthesis, perolat2015approximate, tessler2019action} to update the policy and then performs a learning step where the estimate of the model is updated using data from sample trajectories following the policy. Such decoupling also exists in single-agent RL literature (Azar 2013, Chi Jin 2020). 

%However, in such algorithms, a key challenge remains, which is the computational difficulty of planning in games, which far exceeds that of single-player games, which can be viewed as a degenerate case of two-player games (littman and lagoudakis). We focus on the planning algorithms and answer the following question in the affirmative:

%\begin{center}
%\textit{Can we design computationally efficient algorithms for solving two-player zero sum games in the function approximation setting?}
%\end{center}

%We complete our results with learning results that do not consider computational complexity of the planning phase to get sample efficient as well as computational guarantees of two-player zero sum games with unknown models. 

%Many prior works have focused on the computational difficulty of solving two-player zero-sum games, however often the results are built for small scale system as the results heavily depend on the sizes of the state and action spaces. The challenge of considering function approximation remains, even in the case of linear MDPs. Additionally, even in modern strongly polynoimal algorithms (aaron, chi jin sharp, kaiqing, qiaomin), the question of efficiency remains as their algorithms require solving a min-max problem at each iteration, as is commonplace in the extension of single-player value iteration to games. We consider the (generalized) policy iteration algorithm \cite{patekthesis, perolat2015approximate, tessler2019action} algorithm which is typically more efficient than value iteration schemes and work to improve its computational difficulty in the special case of two-player zero-sum discounted Markov games. The work of \cite{hansen2013strategy} provides an upper bound on this. We remark that this is an open problem itself \cite{tessler2019action, perolat2016softened, patekthesis}. We show that when the lookahead technique is used in the policy improvement step instead of greedy policies, the algorithm converges to the Nash equilibrium. Hence, our solution to the Pollatschek-Avi Itzhak problem forms the backbone of our proof techniques. 
\subsection{Main Contributions:}
\paragraph{Convergence of generalized policy iteration in Markov games}
The computational difficulties associated with extending the policy iteration algorithm to games has been a longstanding open problem. Several studies shed light on the difficulty of the policy iteration algorithm \cite{hansen2013strategy}. We present a simple modification of the well-studied na√Øve policy iteration or the algorithm of Pollatschek and Avi-Itzhak \cite{pollatschek1969algorithms} which converges for all discounted, simultaneous-move Markov zero-sum games. Moreover, the algorithm converges exponentially fast.   We remark that our generalized policy iteration algorithm can also be seen as a generalization of both value iteration and policy iteration in games, which is interesting in its own right. 
\paragraph{Function Approximation} We then extend the results to incorporate function approximation by studying convergence and scalability to lower dimension linear Markov games, noting recent successful results in making linear MDPs more practical using representation learning techniques \cite{ agarwal2020flambe, uehara2021representation,zhang2022making}. Prior work on approximating lookahead using MCTS shows that exponential computational complexity is inevitable in problems with no structure \cite{shah2020nonasymptotic}. In contrast, our results show that the computational complexity of implementing lookahead only depends on the dimension of the feature vectors if we exploit the linear structure of the problem. 
\paragraph{Learning Algorithm} Many studies have covered the learning problem, in both model-based and model-free settings. The most relevant paper to our setting  \cite{zhang2020model} is agnostic to the planning algorithm used. However, due to the lack of any prior results on the use of policy iteration, the results in that paper will not hold if one were to use naive policy iteration because it is known to not converge in some examples. Here, combining our results with \cite{zhang2020model}, we provide a complete characterization of the sample complexity involved in model-based learning combined with generalized policy iteration for games.

\section{Related Works}
\paragraph{Value iteration based multi-agent planning algorithms} Since the introduction of a mathematical framework for Markov games in \cite{Shapley}, many works have worked to efficiently compute value-based algorithms to obtain Nash equilibrium for Markov games including the early works of \cite{littman1994markov, patekthesis, hu2003nash}.
\paragraph{Policy iteration for games} Policy iteration algorithms have been far less successfully studied despite the fact that \cite{van1978discounted} shows that Shapley's value iteration in games is slower in practice than naive policy iteration \cite{pollatschek1969algorithms}. Relevant prior works include \cite{patekthesis, perolat2015approximate} which obtain convergence of policy iteration algorithms for Markov games which require the solution of an MDP at each step, which is computationally burdensome. Furthermore, the work of \cite{pollatschek1969algorithms} following the work of \cite{hoffman1966nonterminating}   proposes an algorithm that is far more computationally efficient but they only show the algorithm converges in specific settings. The works of \cite{filar1991algorithm,breton1986computation} obtain convergence of a variant of the algorithm in \cite{pollatschek1969algorithms} under certain conditions \cite{perolat2016softened}. Recently, the works of \cite{bertsekas2021distributed, brahma2022convergence} study a variant of the algorithm in \cite{pollatschek1969algorithms} that converges, but the dimension of vectors to be stored is quite large and the algorithms have not been shown to be extendible the function approximation setting. 
\paragraph{Model-based reinforcement learning in Markov games} The works of \cite{jia2019feature, sidford2020solving} study value-based approaches that assume the use of generative models where any state-action pair can be sampled at any time. Model-based algorithms for Markov games have been widely studied. The work of \cite{zhang2020model} studies a general setting where learning is used to estimate a model and a planning algorithm is applied to obtain the Nash equilibrium policy. Related model-based episodic algorithms that incorporate value iteration for two-player games include \cite{bai2020provable, xie2020learning} in the finite-horizon setting. The work of \cite{liu2021sharp} provides an episodic algorithm where at each iteration there is a planning step which performs an optimistic form of value iteration and there is a learning step where the outcomes of game play in the planning step are used to update the estimate of the model. 

 
\paragraph{Policy-based methods for two-player games}
In our work, we study model-based learning, i.e., we learn a model followed by planning. An alternative is to directly learn the policy without learning the model. The work of \cite{daskalakis2020independent} provides a decentralized algorithm for policy gradient methods that converges to a min-max equilibrium when both players independently perform policy gradient. The work of \cite{zhao2022provably} obtains convergence guarantees of natural policy gradient in two-player zero-sum games. 

\paragraph{Function approximation methods in multi-agent RL} The work of \cite{lagoudakis2012value} studies linear value function approximation in games where knowledge of the model is assumed. Reference \cite{xie2020learning} studies multi-agent games in linear Markov games, but they only study value iteration in finite-horizon settings. Additionally, the work of \cite{jin2022power} studies episodic learning in multi-agent Markov games with general function approximation. 

\paragraph{Planning Oracles For Learning In MDPs} The works of \cite{gheshlaghi2013minimax, agarwal2020model, li2020breaking, jin2020reward} study reinforcement learning in a single agent setting where a planning oracle is used to obtain convergence guarantees. 

%\paragraph{Multi-agent RL For Markov Games, Sample-based Markov Game Algorithms, Policy Iteration For Markov Games, On the Use of Lookahead, AlphaZero, Oracles For Solving and Planning Of MDPs, Function approximation based MDPs, cite paper that makes linear mdps generalizable, Function Approximation based MDPs for Games}
%cite bertsekas, qiaomin, chi jin v learning, thinh doan, 

\section{Model} \label{section C}
Consider a two-player simultaneous-action zero-sum discounted Markov game. As mentioned in the introduction, we first consider the planning component of a model-based learning algorithm and later make connection to learning. In the planning setting, the probability transition matrices and reward functions are assumed to be known to both players. The game is characterized by $(\scriptS, \scriptU, \scriptV, P, g, \alpha)$. We denote by $\scriptS$ the finite state space and $|\scriptS|$ the size of the state space. With slight abuse of notation, we say that $\scriptU$ is the finite action space  for the first player (the maximizer) where $|\scriptU|$ denotes the size of the action space for the maximizer. We call $\scriptU(s)$ the set of possible actions at state $s$ where $\scriptU = \cup_{s \in \scriptS} \scriptU(s)$.  Similarly, we call $\scriptV$ the finite action space for the second player (the minimizer) where $|\scriptV|$ is the size of the action space for the minimizer. We denote by $\scriptV(s)$ is the action space at state $s$ where $\scriptV = \cup_{s \in \scriptS} \scriptV(s)$.  We define $P$ as the probability transition kernel where $P(s'|s,u,v)$ is the probability of transitioning from state $s \in \scriptS$ to state $s' \in \scriptS$ when the maximizer takes action $u \in \scriptU (s)$ and the minimizer takes action $v \in \scriptV (s)$. We say that $g: \mathbb{R}^{|\scriptS|\times |\scriptU| \times |\scriptV|} \to [0,1]$ is the reward function (where, for the sake of completeness, we define $g(s, u, v) :=0$ for $u \notin \scriptU(s)$ or $v \notin \scriptV(s)$).

At each time instant $i$, the state of the game is $s_i$ and the maximizer takes action $u_i$ while the minimizer takes action $v_i$. We assume that the players take actions $u_i$ and $v_i$ simultaneously and remark that the setting where the players take moves sequentially, i.e., the setting of turn-based Markov games, is a special case of our setting of simultaneous moves. The maximizer incurs a reward of $g(s_i, u_i, v_i)$ where we assume without loss of generality that $g(s_i,u_i,v_i) \in [0,1]$ while the minimizer incurs a cost of $g(s_i, u_i, v_i)$ (and, hence a reward of $-g(s_i, u_i, v_i)$). By the end of the game, from the perspective of the maximizer, the game receives a discounted sum of the rewards with discount factor $\alpha$ where $0<\alpha<1$, i.e., $\sum_{i=0}^\infty \alpha^i g(s_i, u_i,v_i).$ Meanwhile, from the perspective of the minimizer, the game incurs a discounted sum of the costs, i.e., $\sum_{i=0}^\infty \alpha^i g(s_i, u_i,v_i).$ The objective of maximizer is to take actions $u_i$ to maximize $\sum_{i=0}^\infty \alpha^i g(s_i, u_i,v_i)$ while the objective of the minimizer is to minimize $\sum_{i=0}^\infty \alpha^i g(s_i, u_i,v_i).$ 


We call a mapping from states to distributions over actions a \textit{policy}, $(\mu,\nu)$. With slight abuse of notation, at each instance $i$, at state $s_i$, the action $u_i$ is selected following a randomized policy $\mu(s_i)$ where $\mu(s_i) \in \Delta(\scriptU(s_i))$ and $\Delta(\scriptU(s_i))$ denotes the set of distributions over actions in $\scriptU(s_i)$. Similarly, at instance $i$, the action $v_i$ is selected following a randomized policy $\nu(s_i)$ where $\nu(s_i) \in \Delta(\scriptV(s_i)).$ 

Given a policy $(\mu,\nu),$ we define the \textit{value function} corresponding to the policy component-wise as follows:$$
    J^{\mu,\nu}(s) = E_{P,\mu,\nu}\Big[\sum_{i=0}^\infty \alpha^i g(s_i, u_i,v_i)|s_0 = s\Big].$$
A pair of policies $(\mu^*, \nu^*)$ is a Nash equilibrium if they satisfy
    $J^{\mu,\nu^*}\leq J^{\mu^*,\nu^*}\leq J^{\mu^*,\nu}
$ for all policies $(\mu,\nu)$. 
It has been shown in \cite{Shapley} that such a Nash equilibrium policy exists for all two-player discounted zero-sum Markov games. We define the value function of the game to be $J^{\mu^*,\nu^*}$ and we will denote it by $J^*$ for convenience.

\section{Preliminaries}\label{section D}
Consider any policy $(\mu,\nu)$. We will define the probability transition matrix $P_{\mu,\nu} \in \mathbb{R}^{|\scriptS|\times|\scriptS|}$ component-wise where
$$P_{\mu,\nu}(s,s') = \sum_{u \in \scriptU(s)} \sum_{v \in \scriptV(s)} \mu(u)\nu(v) P(s'|s, u,v) \forall (s,s')\in \mathbb{R}^{|\scriptS|\times|\scriptS|}.$$
We define the reward function corresponding to policy $(\mu,\nu)$ as $g_{\mu,\nu} \in \mathbb{R}^{|\scriptS|},$ where 
$$g_{\mu,\nu}(s) = \sum_{u \in \scriptU(s)} \sum_{v \in \scriptV(s)} \mu(u)\nu(v) g(s,u,v) \forall s \in \scriptS.$$
Using $g_{\mu,\nu}$ and $P_{\mu,\nu}$, we define the Bellman operator corresponding to policy $(\mu,\nu)$, $T_{\mu,\nu}:\scriptS \to \scriptS,$ component-wise as follows:
$T_{\mu,\nu}V(s) := g_{\mu,\nu}(s)+\alpha P_{\mu,\nu} V (s).$
If operator $T_{\mu,\nu}$ is applied $m$ times to vector $V \in \mathbb{R}^{|\scriptS|},$ then we say that we have performed an $m$-step rollout of the policy $(\mu,\nu)$ and the result $T^m_{\mu,\nu} V$ of the rollout is called the return.
It is well known that $\norm{T_{\mu,\nu} V - J^{\mu,\nu}}_\infty \leq \alpha \norm{V-J^{\mu,\nu}}_\infty$, hence, iteratively applying $T_{\mu,\nu}$ yields convergence to $J^{\mu,\nu}.$ 

We will now give a few well-known properties of the $T_{\mu, \nu}$ operator. First $T_{\mu, \nu}$ is monotone, i.e., $V \leq V' \implies T_{\mu, \nu}V \leq T_{\mu, \nu} V'.$ Herein, we imply that all inequalities hold element-wise. Second, consider the vector $e \in \mathbb{R}^{|\scriptS|}$ where $e(i) = 1 \forall i \in 1, 2, \ldots, |\scriptS|.$ We have that $T_{\mu, \nu}(V + ce) = T_{\mu, \nu}V + \alpha ce \forall c \in \mathbb{R}.$

With some algebra, it is easy to see that $T_{\mu,\nu}V$ can also be written component-wise as:
\begin{align}   T_{\mu,\nu}V(s) = \mu(s)^\top A_{V,s} \nu(s) \forall s \in \scriptS, \label{eq: bell mu nu}
\end{align}
where $A_{V,s} \in \mathbb{R}^{|\scriptU(s)|\times|\scriptV(s)|}$ is defiend as follows: 
\begin{align}
A_{V,s}(u,v) := g(s, u, v)+ \alpha \sum_{s' \in \scriptR(s, u, v)} P(s'|s, u,v)V(s') \forall (u,v)  \in (\scriptU(s)\times \scriptV(s)), \label{eq: As}
\end{align} where $\scriptR(s, u, v)$ is the set of states for which $P(s'|s,u,v) \neq 0$, i.e., the states that are ``reachable'' from $s$ when taking actions $u$ and $v$. Note that the size of $\scriptR(s,u,v)$ for any $(s, u,v)$ is typically much smaller than the size of the state space. Thus, in order to compute the $m$-step rollout for policy $(\mu,\nu)$ corresponding to vector $V\in \mathbb{R}^{|\scriptS|}$, one can iteratively perform the operations in \eqref{eq: bell mu nu} for all states $s \in \scriptS$ to apply the Bellman operator $T_{\mu,\nu}$ $m$ times.

We define the Bellman optimality operator or Bellman operator $T: \scriptS \to \scriptS$ as $$TV = \max_\mu \min_\nu (T_{\mu,\nu} V).$$ Using the notation in \eqref{eq: bell mu nu}, it is easy to see that the Bellman operator at each state $s \in \scriptS$ solves the following matrix game:
\begin{align}TV(s) = \displaystyle\max_{\substack{\mu(s)\in \mathbb{R}^{|\scriptU(s)|} \\  \mu(s)^\top  \textbf{1} =1\\0\leq \mu(s) \leq 1  }} \displaystyle\min_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s)|} \\ \nu(s)^\top  \textbf{1} =1\\0\leq \nu(s) \leq 1  }} \mu(s)^\top A_{V,s} \nu(s), \label{eq: bellman op}\end{align}
where $A_{V,s}$ is defined for all $s \in \scriptS$ in \eqref{eq: As} and $\textbf{1}$ the column vector of all 1s. Note that the inequalities $0\leq \mu(s)\leq 1$ and $0\leq \nu(s)\leq 1$ are defined to be component-wise. We define the \textit{greedy policy} $(\mu,\nu)$ corresponding to vector $V$ component-wise where $(\mu(s),\nu(s))$ is the $\argmin \argmax$ in \eqref{eq: bellman op} for all states $ s \in \scriptS$. We remark that the computation of the greedy policy can be obtained by solving a linear program \cite{rubinstein1999experience}. 
Additionally, it is known that $T$ is a pseudo-contraction towards the Nash equilibrium $J^*$ where $\norm{TJ - J^*}_\infty \leq \alpha \norm{J-J^*}_\infty,$ hence, iteratively applying $T$ yields convergence to the Nash equilibrium $J^*$ \cite{bertsekastsitsiklis}.

If operator $T$ is applied $H$ times to vector $V \in \mathbb{R}^{|\scriptS|},$ we say that the result, $T^H V$, is the $H$-step ``lookahead'' corresponding to $V$. We call the greedy policy corresponding to $T^H V$ the $H$-step lookahead policy, or the lookahead policy, when $H$ is understood. In other words, given an estimate $V$ of the Nash equilibrium, the lookahead policy is the policy $(\mu,\nu)$ such that $T_{\mu,\nu}(T^{H-1} J)=T(T^{H-1} V).$ In order to compute the lookahead corresponding to $V \in \mathbb{R}^{|\scriptS|}$ (and the lookahead policy), one can iteratively perform the operations in \eqref{eq: bellman op} for all states $H$ times, obtaining the lookahead policy for each state $s$ by taking the $(\mu(s),\nu(s))$ corresponding to the $\argmax \argmin$  policy in \eqref{eq: bellman op} at the $H$-th iteration of applying $T$.

\section{Convergence Of Generalized Policy Iteration For Markov Games}

Convergence of a computationally efficient extension of policy iteration for single player systems to two-player games is an open problem \cite{bertsekas2021distributed, patekthesis}. Our generalized policy iteration algorithm for two-player games is outlined in Algorithm \ref{alg:alg 2}. 

%\paragraph{Our Algorithm}
%\begin{algorithm}
%\caption{Generalized Policy Iteration For Two-Player Zero-Sum Discounted Markov Games}\label{alg:alg 1}
%\textbf{Input}: $V,m, H.$
%\item  For $k=0, 1, \ldots$ 
%\item \quad For $i = 0, 1, \ldots, H$ 
%\item \quad \quad   Compute $A_s \in \mathbb{R}^{|\scriptU(s)|\times |\scriptV(s)|} \forall  s \in \scriptS$ where
%  $$A_s(u, v) \leftarrow r(s, u, v)+ \alpha \sum_{s'} P(s'|s, u,v)V(s') \forall (u, v) \in \mathbb{R}^{|\scriptU(s)|\times|\scriptV(s)|} $$ 
%          \item  \quad \quad \label{step 2 reg}$V(s) \leftarrow \displaystyle\max_{\substack{\mu(s)\in \mathbb{R}^{|\scriptU(s)|} \\  \mu(s)^\top  \textbf{1} =1\\0\leq \mu(s) \leq 1  }} \displaystyle\min_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s)|} \\ \nu(s)^\top  \textbf{1} =1\\0\leq \nu(s) \leq 1  }}\mu(s)^\top A_s \nu(s)$ for $s \in \scriptS$ 
%        \label{step 3 reg} 
%        \item  \quad   $\mu(s) \leftarrow \displaystyle\argmax_{\substack{\mu(s)\in \mathbb{R}^{|\scriptU(s)|} \\  \mu(s)^\top  \textbf{1} =1\\0\leq \mu(s) \leq 1  }} \displaystyle\min_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s)|} \\ \nu(s)^\top  \textbf{1} =1\\0\leq \nu(s) \leq 1  }}\mu(s)^\top A_s \nu(s)$ for $s \in \scriptS$ \\ \item  \quad   $\nu(s) \leftarrow  \displaystyle\argmin_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s)|} \\ \nu(s)^\top  \textbf{1} =1\\0\leq \nu(s) \leq 1  }} \displaystyle\max_{\substack{\mu(s)\in \mathbb{R}^{|\scriptU(s)|} \\  \mu(s)^\top  \textbf{1} =1\\0\leq \mu(s) \leq 1  }}\mu(s)^\top A_s \nu(s)$ for $s \in \scriptS$
%           \item \quad For $j = 0, 1, \ldots, m$
%            \item \quad \quad   Compute $A_s \in \mathbb{R}^{|\scriptU(s)|\times |\scriptV(s)|} \forall  s \in \scriptS$ where
%  $$A_s(u, v) \leftarrow r(s, u, v)+ \alpha \sum_{s'} P(s'|s, u,v)V(s') \forall (u, v) \in \mathbb{R}^{|\scriptU(s)|\times|\scriptV(s)|} $$ 
%          \item  \quad \quad $V(s) \leftarrow \mu(s)^\top A_s \nu(s)$ for $s \in \scriptS$ 
%\end{algorithm}
The algorithm is an iterative process that updates an estimate of the optimal value function at each iteration. At each iteration, there are two steps: the policy improvement step and the policy evaluation step. In the policy improvement step, a new policy to evaluate in the policy evaluation step is determined. The new policy is obtained by computing an $H$-step lookahead policy based on the estimate of the optimal value function.  
In other words, at iteration $k+1,$ the algorithm computes $(\mu_{k+1},\nu_{k+1})$ such that 
$T^H V_k(s) = T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k(s) \forall s \in \scriptS$ by solving the linear program in \eqref{eq: bellman op} for all states $s \in \scriptS$ $H$ times. Note that $T^{H-1}V_k$ is computed as a byproduct of determining the lookahead policy, so the estimate of the value function is updated to be $T^{H-1}V_k.$ The motivation for this update is that $T$ is a contraction to the Nash equilibrium hence $T^{H-1}V_k$ is a better estimate of the value function than $V_k.$ We furthermore note that in the naive policy iteration algorithm of Pollatschek and Avi-Itzhak, $H$ is set to $1,$ and we simply obtain the ``greedy policy.'' We also note that in some ways, our algorithm is a generalization of the algorithm of Pollatschek and Avi-Itzhak. 

\textbf{Remark} The use of lookahead policies in the policy improvement step has been used in empirically successful algorithms such as AlphaZero. In practice, lookahead can be implemented efficiently using techniques such as Monte Carlo Tree Search (MCTS).

The policy evaluation step involves  applying the operator $T_{\mu_{k+1},\nu_{k+1}}$ $m$ times to the current estimate of the optimal value function $\tilde{V}_k = T^{H-1}V_k$ to estimate $J^{\mu_{k+1},\nu_{k+1}}.$ Recall that iteratively applying $T_{\mu_{k+1},\nu_{k+1}}$ to vector $\tilde{V}_k$ yields convergence to $J^{\mu_{k+1},\nu_{k+1}},$ and hence, when $m=\infty$, $V_{k+1} = J^{\mu_{k+1},\nu_{k+1}}.$
Put together, our algorithm can also be written as follows:
\begin{align}
V_{k+1} = T_{\mu_{k+1},\nu_{k+1}}^m T^{H-1}V_k. \label{eq:alt alg}
\end{align}
Note that to apply $T_{\mu_{k+1},\nu_{k+1}}$ to $\tilde{V}_k$ for each state $s \in \scriptS,$ one needs to perform the operations in \eqref{eq: bell mu nu}, and hence, to obtain $T_{\mu_{k+1},\nu_{k+1}}^m\tilde{V}_k,$ i.e., to generate the $m$-return, one needs to perform the computations for all states $s \in \scriptS$ $m$ times. 
\begin{algorithm} 
\caption{Generalized PI For Two-Player  Games}\label{alg:alg 2}
\textbf{Input}: $V_0,m, H.$\\ \\
 %[1] enables line numbers
For $k=1,2,\ldots$ \\
 \quad Let $\tilde{V}_k = T^{H-1}V_k$\\
 Let $\mu_{k+1},\nu_{k+1}$ be such that $\mu_{k+1},\nu_{k+1} \in \argmax_{\mu} \argmin_{\nu} T_{\mu,\nu}\tilde{V}_k$.\label{step 2 our}\\
 Approximate $J^{\mu_{k+1},\nu_{k+1}}$ as follows: $T_{\mu_{k+1}, \nu_{k+1}}^m \tilde{V}_k.$  \label{step 3 our}
 \\
 $V_{k+1} = T_{\mu_{k+1}, \nu_{k+1}}^m \tilde{V}_k.$
\end{algorithm}

\textbf{Remark} We note that in the case of turn-based Markov games, which are Markov games where the players move sequentially instead of simultaneously, the computations in \eqref{eq: bellman op} that are used to determine the lookahead are far simplified and only involve taking either a maximum or a minimum instead of the $\min \max$ operation in \eqref{eq: bellman op}.

\textbf{Our Challenge} In order to explain the difficulty of obtaining convergence of Algorithm \ref{alg:alg 2}, we compare it to the well-studied policy iteration in games algorithm of \cite{patekthesis, perolat2015approximate}.
%\begin{algorithm} 
%\caption{Policy Iteration For Two-Player Zero-Sum Games}\label{alg:alg 3}
%\textbf{Input}: $\mu_0, \nu_0.$\\
%\For{ $k=0,1,2,\ldots$}{
%Let $\mu_{k+1}$ be such that $\mu_{k+1} \in \argmax_\mu T_{\mu}J^{\mu_k,\nu_{k}},$ where $T_\mu J:= \min_{\nu} T_{\mu,\nu}J.$ 
%\\  Let $\nu_{k+1}$ be such that $\nu_{k+1} \in \argmin_{\nu} J^{\mu_{k},\nu}.$\label{step 2 patek}}
%\end{algorithm}
The proof of policy iteration in games uses techniques from the proof of policy iteration for MDPs \cite{bertsekastsitsiklis}. However, the proof of policy iteration for MDPs hinges on monotonicity of the Bellman operator. In games, no such monotonicity of the Bellman operator exists since there is both a $\max$ and a $\min$ in the Bellman operator given in \ref{eq: bellman op}. Thus, the algorithm is modified to follow the proof techniques of policy iteration for MDPs. In policy iteration in games, in the policy improvement step, the policy of the maximizer is fixed and the greedy policy for the minimizer is determined (hence ensuring monotonicity in the policy improvement step). In the policy evaluation step, the minimizer's policy $\nu$ is fixed, $\max_{\mu} J^{\mu,\nu}$ is evaluated, and the estimate of the value function is updated to be $\max_{\mu} J^{\mu,\nu}$. Note that obtaining $\max_{\mu} J^{\mu,\nu}$ requires that an MDP be solved (this is because, since the policy of the minimizer $\nu$ is fixed, only one player, the maximizer, needs to take actions at every iteration to maximize the expected discounted sum of rewards, which is exactly the MDP paradigm). Hence, while the policy iteration in games algorithm converges, in the policy evaluation step of the games setting, unlike the MDP setting, where computing $J^{\nu}$ for the greedy policy $\nu$ can be obtained by inverting a matrix, an MDP must be solved, which makes the algorithm highly inefficient and potentially infeasible.

We wish to obtain convergence of a variant of the policy iteration in games algorithm where the policy evaluation step does not consist of solving an MDP. The most intuitive algorithm, which is often referred to as naive policy iteration or the algorithm of Pollatschek and Avi-Itzhak, is described as follows. In the policy improvement step, the greedy policy corresponding to the bellman operator $T$ given in \eqref{eq: bellman op} is obtained, i.e., $\argmax_{\mu} \argmin_{\nu} T^{\mu,\nu} J$, where $J$ is the current estimate of the value function. In the policy evaluation step, $J^{\mu,\nu}$ is computed. Note that in the policy evaluation step, there is no maximizing over policies $\mu$, unlike the policy iteration in games algorithm. Unfortunately this algorithm has been shown to diverge in \cite{van1978discounted, condon1990algorithms}. Additionally, the proof techniques of policy iteration in MDPs and Markov games do not hold since the Bellman operator in \eqref{eq: bellman op} does not satisfy desirable monotonicity properties. As such, the question of how to modify the algorithm in \cite{pollatschek1969algorithms} to ensure convergence is an open question where the main challenge is how to overcome the lack of monotonicity in the policy improvement step.
In our algorithm, given in Algorithm \ref{alg:alg 2}, we introduce lookahead policies as opposed to traditionally used greedy policies and use several novel proof ideas to overcome the lack of monotonicity. We remark that other works in the MDP setting also use lookahead policies \cite{efroni2018, efroni2018multiple, efroni2019combine},  however, their arguments often rest on monotonicity arguments which cannot be easily extended in the games setting when the Bellman operator is used.
%\begin{remark}
%    We remark that lookahead policies  
%\end{remark}

%\begin{algorithm} 
%\caption{Generalized Policy Iteration For Games}\label{alg:alg 2}
%\textbf{Input}: $\mu_0.$\\
%\begin{algorithmic}[1] %[1] enables line numbers
%\STATE Let $k=0$.
%\STATE Let $\nu_{k+1}$ be such that $\nu_{k+1} \in \argmin_{\nu} T^m_{\mu_{k},\nu}J.$\\
%\STATE Let $\mu_{k+1}$ be such that $\mu_{k+1} \in \argmax_\mu T_{\mu}J^{\mu_k,\nu_{k+1}}.$ 
%\STATE Set $k \leftarrow k+1.$ Go to 2.
%\end{algorithmic}
%\end{algorithm}

%\begin{algorithm} 
%\SetAlgoLined
%\caption{Naive Policy Iteration (Algorithm of Pollatschek and Avi-Itzhak)}\label{alg:alg 4}
%\SetAlgoLined
%\textbf{Input}: $V_0.$\\ \\
% \For{ k=1,2,\ldots}{
%  Let $\mu_{k+1},\nu_{k+1}$ be such that $\mu_{k+1},\nu_{k+1} \in \argmax_{\mu} \argmin_{\nu} T_{\mu,\nu}V_k$.\\
% Set $V_{k+1}$ to be an approximation to $J^{\mu_{k+1},\nu_{k+1}}$, i.e.,  $V_{k+1} \leftarrow T^m_{\mu_{k+1},\nu_{k+1}}V_k$.  \label{step 3 pollatschek}}
%\end{algorithm}

\paragraph{Main Result} We now state our main result, where we prove convergence of a variant of policy iteration for stochastic games that does not involve solving any MDPs. Our main result hinges on the following assumption on the amount of lookahead in each iteration.

\begin{assumption} \label{assumption 1 games}
$\alpha^{H-1}+2(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}<1.$
\end{assumption} Assumption \ref{assumption 1 games} implies that the lookahead, $H$, must be sufficiently large and also that a large return $m$ can mitigate the amount of lookahead that is needed. We remark that taking steps of lookahead is used in practice such as in algorithms like AlphaZero and that efficient algorithms combined with sampling such as Monte Carlo Tree Search (MCTS) are often employed to perform the lookahead. We also note that the amount of lookahead, $H$, is a parameter of the algorithm, and hence, Assumption \ref{assumption 1 games} is not a restriction on the model, rather an assumption on the parameters of the algorithm. 

%Our main result is the following:

\begin{theorem}\label{thm:theorem 1}
Under Assumption \ref{assumption 1 games}, the following holds for the iterates of Algorithm \ref{alg:alg 2}:
\begin{align*}
\norm{V_k-J^*}_\infty  \leq  \Big(\alpha^{H-1}+(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha)\Big)^k\norm{V_0 - J^*}_\infty.
\end{align*}
Taking limits, it is clear that
$
V_k \to J^*,
$ where $V_k$ are the iterates of Algorithm \ref{alg:alg 2} and $J^*$ is the value function.
\end{theorem}
The proof of Theorem \ref{thm:theorem 1} can be found in the Appendix. 


\paragraph{Implications Of Theorem \ref{thm:theorem 1}}
We outline the significance of Theorem \ref{thm:theorem 1} as follows:
\begin{itemize}
    \item To the best of our knowledge, our algorithm is the first variant of the well-studied algorithm of Pollatschek and Avi-Itzhak that converges without restrictive conditions on the model or additional storage. 
    \item Unlike the most commonly used extension of policy iteration to games of \cite{patekthesis, perolat2015approximate}, our algorithm does not require that any MDPs be solved at each iteration. 
    \item We remark that the value iteration algorithm for Markov games is a special case of our algorithm.
\end{itemize}

\paragraph{Proof Idea} 
Our proof can be found in the Appendix. Our proof techniques do not involve monotonicity and instead hinge on a contraction property towards the Nash equilibrium of the operator $T_{\mu,\nu,m,H}$ that we define as follows:
$
   T_{\mu,\nu,m,H}V := T^m_{\mu,\nu}T^{H-1}V.
$
The use of this operator instead of the monotonicity properties used in traditional analyses in single player systems in \cite{bertsekastsitsiklis, efroni2019combine}  allows us to bypass monotonicity complications in games arising from simultaneous minimizing and maximizing actions as opposed to an MDP where actions are taken either to minimize or to maximize, but never to do both at the same time. 
Our sequence of iterates can be written as follows: $V_{k+1} = T_{\mu_{k+1},\nu_{k+1}}^m T^{H-1}V_k.$ In this equation, we have written $(\mu_{k+1},\nu_{k+1})$ as a function of $V_k$ since it is the lookahead policy with respect to $V_k.$ If the lookahead policy $(\mu_{k+1},\nu_{k+1})$ involved taking only a maximum or a minimum (instead of $\max \min$), one can use monotonicity techniques of \cite{bertsekastsitsiklis, efroni2019combine}, but the main challenge for us is the joint $\max \min$ in the lookahead policy. The key to our proofs lies in the fact that, with sufficient lookahead, the operator $T_{\mu(V),\nu(V)}^m T^{H-1}$ is a contraction. Note that while the operator $T$ is a contraction, when we consider the operator $T_{\mu(V),\nu(V)},$ $(\mu,\nu)$ depends on $V$ because $(\mu,\nu)$ is the lookahead policy with respect to $V.$ Therefore, it is not obvious if $||T_{\mu(V_1),\nu(V_1)}^m T^{H-1}V_1-T_{\mu(V_2),\nu(V_2)}^m T^{H-1}V_2||_\infty$ is smaller than $||V_1-V_2||_\infty.$ 

In very large systems, function approximation techniques are necessary because of the massive sizes of state spaces. As such we consider perhaps the most basic model of function approximation, which is the linear MDP. We will show that the computations and storage required to determine the Nash equilibrium depends on the dimension of the feature vectors and not on the size of the state space. 

\section{Function Approximation}

While our results in the previous section are for general Markov zero-sum games, in this section, we study the important special case of linear Markov zero-sum games. We are motivated by the fact that recent works including \cite{agarwal2020flambe,uehara2021representation,zhang2022making} have shows that one can learn linear representations of MDPs efficiently. In such linear representations, feature vectors representing (state, action) tuples have much lower dimensionality compared to the cardinality of the state space. Here, we show how such feature vectors can be leveraged to improve the computational complexity of our algorithm.

In our linear Markov zero-sum setting, it is assumed that the model is of the following form: 
\begin{align}
r(s,u,v) = \phi(s,u,v)\cdot \theta, \quad P(\cdot|s,u,v) = \phi(s,u,v)\cdot\eta, \label{eq: lin mdp}
\end{align} where $\theta \in \mathbb{R}^d$ and $\eta \in \mathbb{R}^{|\scriptS|\times d}.$ Typically, the dimension $d$ is far smaller than the size of the state space.
These assumptions are described in more detail in \cite{Agarwal2019ReinforcementLT}.

In the case of linear Markov games, any vector $V \in \mathbb{R}^{|\scriptS|}$ can be parameterized by some $\beta \in \mathbb{R}^d$ as follows. Consider any state-actions tuple $(s, u, v)$. Then, it can be easily shown that $A_{V,s}$ defined in \eqref{eq: As} can be written in the following form:
\begin{align}
    A_{V,s}(u,v) = \phi(s, u, v)^\top \beta \label{eq: AVs}
\end{align} for some $\beta$ \cite{Agarwal2019ReinforcementLT}.

Under this parameterization, we will show how to obtain $\beta'$ corresponding to $T_{\mu,\nu} V$ from $\beta$ corresponding to $V$ for any $V \in \mathbb{R}^{|\scriptS|}$ in a way that the number of computations does not depend on the size of the state space. We will then extend the result to obtain $\beta'$ corresponding to $TV$. First, consider a set of state-actions tuples $\scriptD$ where $\sum_{(s, u, v) \in \scriptD} \phi(s,u,v)\phi(s,u,v)^\top$ is full rank. For $(s,u,v) \in \scriptD$, directly compute $A_{T_{\mu,\nu}V,s}(u,v)$, i.e.,
\begin{align}
    A_{T_{\mu,\nu}V,s}(u,v) \nonumber &= g(s, u, v)+ \alpha \sum_{s' \in \scriptR(s, u, v)} P(s'|s, u,v){T_{\mu,\nu}V,s}(s')\\
    &= g(s, u, v)+ \alpha \sum_{s' \in \scriptR(s, u, v)} P(s'|s, u,v) (\mu(s')^\top A_{V,s'}\nu(s')),\label{eq:A fn ap}
\end{align} where $A_{V,s'}$ can be constructed using \eqref{eq: AVs}. Note that the summation in \eqref{eq:A fn ap} is only over states $s'$ that are reachable from state $s$ when actions $u$ and $v$ are taken. In many real world examples, say, games such as Atari, this set of states is rather small since only a few states can be reached from any given state regardless of the actions that are taken.

Then, using the resulting $A_{T_{\mu,\nu}V,s}(u,v)$ for $(s,u,v) \in \scriptD$, we can easily obtain an appropriate $\beta'$ by performing a least squares minimization. More precisely, since $A_{T_{\mu,\nu}V,s}(u,v) = \phi(s,u,v)^\top \beta' \forall (s,u,v) \in \scriptS\times\scriptU(s)\times \scriptV(s)$, it is sufficient to compute $A_{T_{\mu,\nu}V,s}(u,v)$ for $(s, u, v) \in \scriptD$ and perform a least squares minimization to determine $\beta'$,i.e., 
\begin{align}
    \beta' := \argmin_{(s,u,v)} \sum_{(s, u, v)\in \scriptD}(A_{T_{\mu,\nu}V,s}(u,v) - \phi(s,u,v)^\top \beta')^2.
\end{align} Recall that a unique minimizer exists because of the full rank condition in the definition of $\scriptD.$

The above minimization produces the weight vector $\beta'$ associated with $T_{\mu,\nu}V.$ In a similar manner, one can also obtain the weight vector corresponding to $TV$. The only modification is that instead of computing $\mu(s)^\top A_{TV,s} \nu(s)$, one needs to instead obtain $\min_{\mu} \max_{\nu} \mu(s)^\top A_{T_{\mu,\nu} V,s} \nu(s)$.

Put together, the above shows that in order to obtain a sequence of $\beta_k$ that parameterize the sequence $V_k$ in Algorithm \ref{alg:alg 2}, it is only necessary to perform the sequence of computations described above which do not depend on the size of the state space. 

\textbf{Remark:} We note that the computations are further simplified in the case of turn-based Markov games, where players take turns to execute actions in alternating time steps. This is because in the setting of turn-based MDPs, only one player at a time is performing either a maximization or a minimization.

\section{Learning In Games} \label{section G}
In multi-agent model-based reinforcement learning algorithms, the learning component has been extensively studied, and hence many straightforward extensions of our work exist to involve learning in model-based settings. We will provide one such example of an algorithm based on the learning algorithm for model-based multi-agent reinforcement learning in the work of \cite{zhang2020model}. The algorithm we study can be described as follows:
\paragraph{Learning Algorithm} The algorithm assumes knowledge of the reward function (this setting is called the \textit{reward-aware} setting) as well as access to a generator, which, at any iteration, for any state-actions tuple $(s,u,v)$ can sample from the distribution $P(\cdot|s, u,v)$ and obtain the next state. For each state-actions tuple $(s,u,v),$ the algorithm obtains $N$ samples and, based on the samples constructs an estimate of the probability transition matrix in the following manner:$\hat{P}(s'|s,u,v) := \frac{\text{count}(s',s,u,v)}{N}.$ Using $\hat{P}$ and the known reward function, the algorithm finds the Nash equilibrium policy using Algorithm \ref{alg:alg 2}, $(\hat{\mu},\hat{\nu})$. 
The following theorem gives a bound on the sample and computational complexity required to achieve an error bound on $\norm{Q^{\hat{\mu},\hat{\nu}}-Q^*}_\infty$ in linear turn-based Markov games where $\phi(s, u,v) \in \mathbb{R}^d$ and the number of reachable states from state-actions tuples in $\scriptD$ is $r.$

\begin{theorem}\label{thm:theorem 3}
    Consider a linear turn-based Markov game and any $\epsilon, \delta, \epsilon_{opt}>0$ with $\epsilon \in (0, 1/(1-\alpha)^{1/2}]$. When the number of samples of each state-actions tuple is at least $N$ and the number of computations that are made in the planning step where the Nash equilibrium policy is determined based on the model inferred from the samples is at least $C$ where
\begin{align*}
 &N \geq \frac{c \alpha \log\big[ c |\scriptS||\scriptU||\scriptV|(1-\alpha)^{-2}\delta^{-1}\big]}{(1-\alpha)^3 \epsilon^2}, C \geq \frac{c  m H \log\Big[ \frac{1}{\epsilon_{opt}(1-\alpha)}\Big]}{\log[\frac{1}{\tilde{\alpha}}]}\Bigg[ d[2r+1]+d^3/3+r|\scriptA|_{max}^2 d \Bigg]
\end{align*} where $c$ is a constant, $\tilde{\alpha}=\alpha^{H-1}+(1+\alpha^m)\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha),$ $|\scriptU|$ and $|\scriptV|$ are the numbers of the total actions available to players 1 and 2, and $|\scriptA|_{max}$ is the largest number of actions available at a state, it holds that with probability at least $1-\delta,$ 
$\norm{Q^{{\hat{\mu},\hat{\nu}}}-Q^*}_\infty \leq \frac{2\epsilon}{3}+\frac{5 \alpha \epsilon_{opt}}{1-\alpha}, \norm{\hat{Q}^{\hat{\mu},\hat{\nu}}-Q^*}_\infty \leq \epsilon + \frac{9\alpha \epsilon_{opt}}{1-\alpha}.$
\end{theorem}
The proof of Theorem \ref{thm:theorem 3} uses the results of \cite{zhang2020model} and extensions from Theorem \ref{thm:theorem 1} and can be found in the Appendix. Theorem \ref{thm:theorem 3} overall gives a bound on the the error of the learning algorithm as a function of the number of computations in the planning step and the number of samples in the learning step. We note that convergence of the learning algorithm in the present section does not require solving an MDP at each iteration the way that many model-based policy iteration algorithms do. In some ways, Theorem \ref{thm:theorem 3} provides a trade-off between sample complexity in the learning step and computational complexity in the planning step. 
\section{Future Work}
%In our work, we study the model-based learning problem and focus on the planning step of the problem using known learning results to provide results for model-based policy iteration for two-player zero-sum simultaneous discounted games. 

Some interesting directions of future work include: extending the results to the stochastic shortest path games problem where there is no discount factor. See the work of \cite{patekthesis} for more on the stochastic shortest path games setting (2) deriving bounds for more general classes of function approximation settings. 

\newpage

% Acknowledgments---Will not appear in anonymized version

%\acks{We thank a bunch of people and funding agency.}
\bibliographystyle{plainnat}
\bibliography{refs}
\appendix

\section{Proof of Theorem \ref{thm:theorem 1}}

%We first provide some extra notation for the proof of Theorem \ref{thm:theorem 1}. First, as previously stated, we denote by $e$ a column vector of all 1's. Second, we define the Q-functions as follows:
%\begin{align*}
 %   Q^{\mu,\nu}(s,u,v) = E_{u_i \sim \mu(\cdot|x_i),v_i \sim \nu(\cdot|x_i),i\neq 0}\Big[\sum_{i=0}^\infty \alpha^i g(x_i, u_i,v_i)|x_0 = s,u_0 = u, v_0 = v\Big].
%\end{align*} 

%Note the connection with the value function corresponding to policy $(\mu,\nu)$ as:
%\begin{align*}
%    V^{\mu,\nu}(s) = E_{u_i \sim \mu(\cdot|x_i),v_i \sim \nu(\cdot|x_i)}\Big[\sum_{i=0}^\infty \alpha^i g(x_i, u_i,v_i)|x_0 = x\Big].
%\end{align*}
%From the above definitions, it follows that
%\begin{align*}
%V^{\mu,\nu}(s) = E_{u \sim \mu(\cdot|s),v \sim \nu(\cdot|s)}Q^{\mu,\nu}(s,u,v) = \mu(s)^\top Q^{\mu,\nu}(s) \nu(s),
%\end{align*} where $Q_{u,v}^{\mu,\nu}(s)=Q^{\mu,\nu}(s,u,v)$ and that 
%\begin{align*}
%Q^{\mu,\nu}(s,u,v) &= g(s,u,v) +\alpha E_{u \sim \mu(\cdot|s),v \sim \nu(\cdot|s),s'\sim P(\cdot,s,u,v)}[V^{\mu,\nu}(s')]
%\end{align*}
%Equivalently,
%\begin{align*}
%    Q^{\mu,\nu}(s,u,v) = g(s,u,v) +\alpha  \sum_{u} \sum_{v}\sum_{s'}P(s'|s,u,v)V^{\mu,\nu}(s')\mu(u)\nu(v)
%\end{align*}

%We can now define $T_{\mu,\nu}$ using a generic Q operator where:
%\begin{align*}
 %    QV(s) = r(s,u,v)+\alpha \sum_{s'}P(s'|s,u,v)V(s').
%\end{align*}



%Let $T_{\mu,\nu}$ be the operator defined by
%$$T_{\mu,\nu}J := r(\mu,\nu)+\alpha P(\mu,\nu)J$$ with $$P_{ij}(\mu,\nu) = \sum_x \sum_y \mu(x)\nu(y) P(j|i, \mu(x),\nu(y))$$ and 
%$$r_i(\mu,\nu) = \sum_{x}\sum_y \mu(x)\nu(y) g(i,x,y)$$ 
%alternatively as the following:

%\begin{align*}
%    T_{\mu,\nu}J = \mu(s')^\top A_{Q, s'} \nu(s'), 
%\end{align*}
%where $$Q(s, u, v) \leftarrow r(s, u, v)+ \alpha \sum_{s'} P(s'|s, u,v)J(s') \forall (s,u,v).$$

%As is well known that $$\norm{T_{\mu,\nu} J - J^{\mu,\nu}}_\infty \leq \alpha \norm{J-J^{\mu,\nu}}_\infty,$$ we can iteratively apply $T_{\mu,\nu}$ to get an estimate with of $J^{\mu,\nu}$ as follows. For $m$ iterations, we have the following: 
%$T^m_{\mu,\nu}V$ can be obtained through the following procedure where %$A_{Q, s'}(u,v)= Q(s',u,v) \forall s':$
%\begin{algorithm}
%\caption{$T_{\mu,\nu}^m(V)$ computations}

%\For {$j = 0, 1, \ldots, m$}{
%            $Q(s, u, v) \leftarrow r(s, u, v)+ \alpha \sum_{s'} P(s'|s, u,v)V(s') \forall (s,u,v) $ \\ 
%           $V(s') \leftarrow \mu(s')^\top A_{Q, s'} \nu(s')$  for $s' $  
%         }
%\end{algorithm} 

%Now consider the optimal value function $J^*$, and corresponding optimal policy $(\mu^*,\nu^*),$ i.e., 
%\begin{align*}
%   J^* := J^{\mu^*,\nu^*}
%\end{align*} and  
%\begin{align*}
%    J^{\mu,\nu^*}\leq J^{\mu^*,\nu^*}\leq J^{\mu^*,\nu}
%\end{align*} for all policies $(\mu,\nu)$. 

%Now, recall our previous definition of the Bellman operator $T$ as follows:
%$$TJ = \max_\mu \min_\nu (T_{\mu,\nu} J)$$ where the $(\mu,\nu)$ is %called the greedy policy.

%Using the above and the Q function, we can write $TJ(s)$ as follows.
%$$TJ(s) = \displaystyle\max_{\substack{\mu\in \mathbb{R}^{|\scriptU(s)|} \\ \sum \mu_i=1\\0\leq \mu_i \leq 1  }} \displaystyle\min_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s)|} \\ \sum \nu_i=1\\0\leq \nu_i \leq 1  }} \mu^\top A_{Q, s} \nu.$$
%In our algorithm we find the $(\mu,\nu)$ greedy policy after taking $H$ repetitions of the Bellman operator $T$. The step of taking $T^H V$ can be written in Algorithm \ref{alg:alg 7}.
%The idea is that since $T$ is known to be a contraction towards the optimal value function $J^*$ where $\norm{TJ - J^*}_\infty \leq \alpha \norm{J-J^*}_\infty,$ the ``lookahead'' can give us a better policy $(\mu,\nu)$. Putting the above together, it is easy to see the equivalence of Algorithms \ref{alg:alg 1} and \ref{alg:alg 2}. 


%\begin{algorithm}\label{alg:alg 7}
%\caption{$T^H(V)$ computations}
%\For {$i = 0, 1, \ldots, H$}{
%  $Q(s, u, v) \leftarrow r(s, u, v)+ \alpha \sum_{s'} P(s'|s, u,v)V(s') \forall (s,u,v) $ \\ 
%            $V(s') \leftarrow \displaystyle\max_{\substack{\mu\in \mathbb{R}^{|\scriptU(s')|} \\ \sum \mu_i=1\\0\leq \mu_i \leq 1  }} \displaystyle\min_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s')|} \\ \sum \nu_i=1\\0\leq \nu_i \leq 1  }} \mu^\top A_{Q, s'} \nu,$ where $A_{Q, s'}\in \mathbb{R}^{|\scriptU(s')|\times |\scriptV(s')|}$  and   \\$A_{Q, s'}(u,v)= Q(s',u,v) \forall s' $   }
%\end{algorithm}
The proof of Theorem \ref{thm:theorem 1} relies on showing a contraction property of the $T_{\mu,\nu,mH}$ operator. 
\bigskip
We will show that $T_{\mu,\nu,mH}$ is a contraction towards $J^*$ for all $m$ and $H$, including $m=\infty$ and all $H\geq 1.$ We note that this only holds for sufficiently large $H$ per Assumption \ref{assumption 1 games}. 

First, we have the following:
Since $T_{\mu_{k+1},\nu_{k+1}}V$ is defined as $r(\mu_{k+1},\nu_{k+1})+\alpha P(\mu_{k+1},\nu_{k+1})V,$ we can directly apply the contraction property from single player MDPs to see that:
\begin{align*}
\norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-J^{\mu_{k+1},\nu_{k+1}}}_\infty &= \norm{T_{\mu_{k+1},\nu_{k+1}}^mT^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty \\&\leq \alpha^m \norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty.
\end{align*}
Thus, the following holds:
\begin{align}
  \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-T^{H-1}V_k}_\infty\nonumber &=  \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k- J^{\mu_{k+1},\nu_{k+1}}+J^{\mu_{k+1},\nu_{k+1}}  -T^{H-1}V_k}_\infty \\ \nonumber &\leq \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k- J^{\mu_{k+1},\nu_{k+1}}}_\infty+\norm{J^{\mu_{k+1},\nu_{k+1}}  -T^{H-1}V_k}_\infty  \\
 \nonumber &\leq \alpha^m \norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty+\norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty \\
  &= (1+\alpha^m)\norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty.\label{eq:three}
\end{align}

We will now attempt to obtain a bound on $\norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty$. 
Here, we bypass a crucial monotonicity property of $T$ for single player systems that is not present in games. We  have by definition of $T_{\mu_{k+1},\nu_{k+1}}$ the following for all $\ell$:
\begin{align*}
\norm{T_{\mu_{k+1},\nu_{k+1}}^{\ell+1}T^{H-1}V_k-T_{\mu_{k+1},\nu_{k+1}}^{\ell}T^{H-1}V_k}_\infty  &\leq \alpha^\ell \norm{TV_k-V_k}_\infty \\
\end{align*}

To start, we will need the following pseudo-contraction property of $T$ the optimal value function \cite{bertsekastsitsiklis}:
\begin{align*}
    \norm{TV-J^*}_\infty \leq \alpha\norm{V-J^*}_\infty.
\end{align*}
Since $T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k = T^H V_k$ and using the property of $T$, we have the following:
\begin{align*}
    \norm{T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k - T^{H-1} V_k}_\infty &=  \norm{T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k -J^* +J^*- T^{H-1} V_k}_\infty\\
    &\leq \norm{T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k -J^*}_\infty +\norm{J^*- T^{H-1} V_k}_\infty \\
    &= \norm{T^{H}V_k -J^*}_\infty +\norm{J^*- T^{H-1} V_k}_\infty \\
    &\leq \alpha^H \norm{V_k - J^*}_\infty + \alpha^{H-1}\norm{J^*-V_k}_\infty \\
    &= \underbrace{(\alpha^H + \alpha^{H-1})\norm{J^*-V_k}_\infty}_{=: \tilde{a}}.
\end{align*}

Thus,
\begin{align}
    -T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k\leq  - T^{H-1} V_k + \tilde{a}.
\end{align}

Suppose that we apply the $T_{\mu_{k+1},\nu_{k+1}}$ operator $\ell-1$ times to both sides. Then, due to monotonicity and the fact $T_{\mu,\nu}(J+ce)=T_{\mu,\nu}(J)+\alpha ce,$ for any policy $(\mu,\nu),$ we have the following:
\begin{align*}
    {T^\ell_{\mu_{k+1},\nu_{k+1}}} T^{H-1}V_k \leq \alpha^{\ell } \tilde{a}e + {T^{\ell+1}_{\mu_{k+1},\nu_{k+1}}}T^{H-1}V_k.
\end{align*}
Using a telescoping sum, we get the following inequality:
\begin{align*}
    T_{\mu_{k+1},\nu_{k+1}}^j T^{H-1} V_k - T^{H-1}V_k
    &\geq - \sum_{\ell = 1}^{j} \alpha^{\ell - 1} \tilde{a} e.
\end{align*}
Taking the limit as $j\rightarrow\infty$ on both sides, we have the following:
\begin{align}
    J^{\mu_{k+1},\nu_{k+1}} - T^{H-1}V_k \geq - \frac{\tilde{a} e}{1-\alpha}.
\label{eq:one}
\end{align}
In the other direction, we have the following:
\begin{align}
     - T^{H-1} V_k\leq -T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k + \tilde{a}.
\end{align}
Applying the $T_{\mu_{k+1},\nu_{k+1}}$ operator $\ell-1$ times to both sides, we have :
\begin{align*}
    {T^{\ell+1}_{\mu_{k+1},\nu_{k+1}}} T^{H-1}V_k \leq \alpha^{\ell } \tilde{a}e + {T^{\ell}_{\mu_{k+1},\nu_{k+1}}}T^{H-1}V_k.
\end{align*}
Using a telescoping sum, we get the following inequality:
\begin{align*}
   T^{H-1}V_k- T_{\mu_{k+1},\nu_{k+1}}^j T^{H-1} V_k 
    &\geq - \sum_{\ell = 1}^{j} \alpha^{\ell - 1} \tilde{a} e.
\end{align*}
Taking the limit as $j\rightarrow\infty$ on both sides, we have the following:
\begin{align}
   T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}  \geq - \frac{\tilde{ae} }{1-\alpha}.
   \label{eq:two}
   \end{align}
Hence, putting inequalities \eqref{eq:one} and \eqref{eq:two} together, we have the following bound:
\begin{align*}
    \norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty \leq \frac{\tilde{a} }{1-\alpha}.
\end{align*}

We plug this bound into our result in \eqref{eq:three} to get:
\begin{align}
  \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-T^{H-1}V_k}_\infty\nonumber \nonumber&\leq \frac{\tilde{a} e}{1-\alpha} \\&=  \frac{(1+\alpha^m)(\alpha^H + \alpha^{H-1})\norm{J^*-V_k}_\infty }{1-\alpha}.
\label{eq:four} \end{align}

We now provide the reverse triangle inequality which we will use in the next step: \begin{align*}
    \norm{X-Y}_\infty - \norm{Y-Z}_\infty \leq \norm{X-Z}_\infty \forall X,Y,Z.
\end{align*} Using the reverse triangle inequality we have the following bound:
\begin{align*}
\norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k - J^*}_\infty - \norm{T^{H-1}V_k-J^*}\leq \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-T^{H-1}V_k}_\infty.
\end{align*}

Now, we use the pseudo-contraction property of $T$ towards $J^*$ as follows:
\begin{align*} \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k - J^*}_\infty &\leq \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-T^{H-1}V_k}_\infty +  \norm{J^*-T^{H-1}V_k}_\infty\\
&\leq \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-T^{H-1}V_k}_\infty + \alpha^{H-1} \norm{J^*-V_k}_\infty \\
&\leq \frac{(1+\alpha^m)(\alpha^H + \alpha^{H-1})\norm{J^*-V_k}_\infty }{1-\alpha}+\alpha^{H-1} \norm{J^*-V_k}_\infty, 
\end{align*}
where in the last line we plug in our bound in \eqref{eq:four}.

Hence, the following holds:

\begin{align*}
  \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-J^*}_\infty  \leq  \Big(\alpha^{H-1}+(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha)\Big)\norm{V_k - J^*}_\infty.
\end{align*}

Noting that $V_{k+1} = T_{\mu_{k+1},\nu_{k+1},m,H}V_k,$ we iterate to get the following bound:
\begin{align*}
\norm{V_k-J^*}_\infty  \leq  \Big(\alpha^{H-1}+(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha)\Big)^k\norm{V_0 - J^*}_\infty,
\end{align*} and, further using Assumption \ref{assumption 1 games}, we have that $V_k \to J^*,$ which proves the exponential rate of convergence of our algorithm.
endproof

The framework of the techniques we use are based on those of the work of \cite{winnicki2023convergence} however unlike the work of \cite{winnicki2023convergence} the setting of the problem is a two-player game in a deterministic setting as opposed to online learning with stochastic approximation for a single-player system.
%\section{Theorem \ref{thm:theorem 2} Details}
%Theorem \ref{thm:theorem 2} is Theorem \ref{thm:theorem 1} specialized to the case of linear MDPs. To illustrate the convergence of Theorem \ref{thm:theorem 2}, it is sufficient to show that Algorithm \ref{alg:alg 5} is simply Algorithm \ref{alg:alg 1} specialized to the case of linear MDPs. We will show that in the case of linear MDPs, there exists a $\beta$ at each iteration such that we can rewrite $TV(s)$ for $s \in \scriptD$ in terms of $\beta$. To see this, observe the following:

%\begin{align}
%    TV(s) &= \min \max_{u,v} r(s,u,v)+\alpha \sum_{s'}P(s'|s,u,v)V(s') \label{eq: beta MG}\\
%    &= \min\max_{u,v} \phi(s,u,v)^\top \theta + \alpha \sum_{s'}[\phi(s,u,v)^\top \eta(s') V(s')] \nonumber\\
%    &= \min\max_{u,v} \phi(s,u,v)^\top \underbrace{[\theta+\alpha \sum_{s'} \eta(s')V(s')]}_{=: \beta}\nonumber,
%\end{align} hence, for some $\beta \in \mathbb{R}^d,$ we can write $TV(s)$ in terms of $\beta$ as follows:
%\begin{align}
%    TV(s) = \min\max_{u,v} \phi(s,u,v)^\top \beta. \label{eq:take}
%\end{align} 
%Furthermore, we note that from \eqref{eq: beta MG}, it is clear that $TV(s)$ need not be evaluated for all states $s \in \scriptS$ since determining $TV(s)$ for $s \in \scriptD$ requires $V(s')$ only for states $s' \in \scriptD_\scriptR.$ 
%Notice that in order to compute $\beta$, we need not compute $\beta =\theta+\alpha \sum_{s'} \eta(s')V(s')$. Instead, we can use Q factors defined as follows:
%\begin{align*}
%    QV(s,u,v) := \phi(s,u,v)^\top \beta.
%\end{align*}
%Notice that $TV(s) = \min\max_{u,v} QV(s,u,v).$ So, we need only obtain $\beta$ to obtain $TV(s)$ for any state $s$. While it is possible to compute $\beta$ by taking $\theta+\alpha \sum_{s'} \eta(s')V(s'),$ doing so is computationally efficient and instead, for a set of states-actions tuples $\scriptD$ where $$\sum_{(s,u,v)\in \scriptD} \phi(s,u,v)\phi(s,u,v)^\top$$ is full rank, we can compute the following:
%\begin{align*}
%     QV(s) = r(s,u,v)+\alpha \sum_{s'}P(s'|s,u,v)V(s')
%\end{align*} using knowledge of the model and then perform the least squares minimization in Algorithm \ref{alg:alg 5} to obtain  $\beta,$ which allows us to compute $TV(s)$ using \eqref{eq:take} for any state $s$. However, in computing $T^HV(s)=T(T(\ldots(TV)))$, notice that we need only compute $\beta$ corresponding to $TV$ and using that $\beta$, obtain the next $\beta$ corresponding to $T\tilde{V}$ where $\tilde{V} := TV$. Hence, we need only compute the sequence of $\beta$. Notice that we need not recover an entire $TV(s)$ vector for all states $s$.
%Instead, it can be seen that in order to compute the sequence of $\beta$ for any given value of $V$, we need only compute $TV(s)$ for states $s$ that are reachable from any of the tuples $(s,u,v)$ in set $\scriptD.$

\section{Proof of Theorem \ref{thm:theorem 3}}
We note that the settings of the bound in \cite{zhang2020model} are the same as the settings of our work, so we can directly apply the bounds in \cite{zhang2020model}. We will restate Theorem 3.3 from \cite{zhang2020model} in Lemma \ref{lemma 1 games}:

\begin{lemma}\label{lemma 1 games}
    Consider any $\epsilon, \delta>0$ with $\epsilon \in (0, 1/(1-\alpha)^{1/2}]$ and $\delta \in [0,1]$. When the number of samples of each state-actions tuple is at least $N$ by the learning oracle and a planning oracle is used based on the empirical model $\hat{\mathcal{G}}$ which is reward-aware and the entries are of the probability transition matrix is constructed by taking averages determined by the learning oracle to determine a policy $(\hat{\mu},\hat{\nu})$ where:
    \begin{align*}
        \norm{\hat{V}^{\hat{\mu},\hat{\nu}}-\hat{J}^*}_\infty \leq\epsilon_{opt},
    \end{align*} where $\hat{J}^*$ is the optimal value function for model $\hat{\mathcal{G}}$, when 
\begin{align*}
 &N \geq \frac{c \alpha \log\big[ c |\scriptS||\scriptU||\scriptV|(1-\alpha)^{-2}\delta^{-1}\big]}{(1-\alpha)^3 \epsilon^2} 
\end{align*} where for some absolute constant $c$, it holds that with probability at least $1-\delta,$ 
\begin{align*}
    \norm{Q^{{\hat{\mu},\hat{\nu}}}-Q^*}_\infty \leq \frac{2\epsilon}{3}+\frac{5 \alpha \epsilon_{opt}}{1-\alpha}, \quad \norm{\hat{Q}^{\hat{\mu},\hat{\nu}}-Q^*}_\infty \leq \epsilon + \frac{9\alpha \epsilon_{opt}}{1-\alpha}.
\end{align*}
\end{lemma}

Now, we compute the complexity as follows:
It is easy to see that to compute $Q(s, u, v) \leftarrow r(s, u, v)+ \alpha \sum_{s' \in \scriptD_\scriptR} P(s'|s, u,v)V(s') \forall (s,u,v) \in \scriptD$ requires $d[2r+1]$ computations, computing $\theta \leftarrow \displaystyle\argmin_\theta  \sum_{(s, u,v)\in \scriptD} [\phi(s,u,v)^\top \theta - Q(s,u,v)]^2$ requires $d^3/3$ computations (as an upper bound), computing $V(s') \leftarrow \displaystyle\max_{\substack{\mu\in \mathbb{R}^{|\scriptU(s')|} \\ \sum \mu_i=1\\0\leq \mu_i \leq 1  }} \displaystyle\min_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s')|} \\ \sum \nu_i=1\\0\leq \nu_i \leq 1  }} \mu^\top A_{\theta, s'} \nu,$ needs $r\times|\scriptA|_{max}^2\times d$ computations noting that there is a turn-based MDP which is well known to involve only deterministic policies, and since the $\mu^\top A_{\theta, s'} \nu$ is a special case of the previous step, no more additional computations are needed to determine an upper bound. Using the above, some algebra gives Theorem \ref{thm:theorem 3}. 


\end{document}