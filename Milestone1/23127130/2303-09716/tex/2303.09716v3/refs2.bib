%% LaTeX2e file `refs.bib'
%% generated by the `filecontents' environment
%% from source `proof_general' on 2020/09/09.
%%
@article{condon1990algorithms,
  title={On Algorithms for Simple Stochastic Games.},
  author={Condon, Anne},
  journal={Advances in computational complexity theory},
  volume={13},
  pages={51--72},
  year={1990}
}

@misc{winnicki2023new,
      title={A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games}, 
      author={Anna Winnicki and R. Srikant},
      year={2023},
      eprint={2303.09716},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
wang2022on,
title={On the Convergence of the Monte Carlo Exploring Starts Algorithm for Reinforcement Learning},
author={Che Wang and Shuhan Yuan and Kai Shao and Keith W. Ross},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=JzNB0eA2-M4}
}

@misc{liu,
  doi = {10.48550/ARXIV.2007.10916},
  
  url = {https://arxiv.org/abs/2007.10916},
  
  author = {Liu, Jun},
  
  keywords = {Optimization and Control (math.OC), Machine Learning (cs.LG), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Convergence of Reinforcement Learning with Monte Carlo Exploring Starts},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{tessler2019action,
  title={Action robust reinforcement learning and applications in continuous control},
  author={Tessler, Chen and Efroni, Yonathan and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={6215--6224},
  year={2019},
  organization={PMLR}
}

@book{bertsekas2022lessons,
  title={Lessons from AlphaZero for Optimal, Model Predictive, and Adaptive Control},
  author={Bertsekas, Dimitri},
  year={2022},
  publisher={Athena Scientific}
}



@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}

@article{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={10836--10846},
  year={2019}
}


@article{baxter,
  author    = {Jonathan Baxter and
               Andrew Tridgell and
               Lex Weaver},
  title     = {TDLeaf(lambda): Combining Temporal Difference Learning with Game-Tree
               Search},
  journal   = {CoRR},
  volume    = {cs.LG/9901001},
  year      = {1999},
  url       = {https://arxiv.org/abs/cs/9901001},
  timestamp = {Fri, 10 Jan 2020 12:59:22 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/cs-LG-9901001.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


 @article{tsitsiklis2002convergence,
  title={On the convergence of optimistic policy iteration},
  author={Tsitsiklis, John N},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Jul},
  pages={59--72},
  year={2002}
 }
 
 @INPROCEEDINGS{Tsitsiklis94feature-basedmethods,
    author = {John N. Tsitsiklis and Benjamin  van Roy},
    title = {Feature-Based Methods For Large Scale Dynamic Programming},
    booktitle = {Machine Learning},
    year = {1994},
    pages = {59--94}
}
Share
 
@article{bertsekasioffe,
author = {Bertsekas, Dimitri and Ioffe, Sergey},
year = {2012},
month = {04},
pages = {},
title = {Temporal Differences-Based Policy Iteration and Applications in Neuro-Dynamic Programming1}
}

 @inproceedings{du2018gradient,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}

@inproceedings{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={International Conference on Learning Representations},
  year={2019}
} 
 
 @article{Aven,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3213876},
 abstract = {In this paper, we give some upper (lower) bounds on E max1 ≤ i ≤ n Xi(E min1 ≤ i ≤ n Xi), where the Xi's are real-valued random variables. Some applications are given.},
 author = {Terje Aven},
 journal = {Journal of Applied Probability},
 number = {3},
 pages = {723--728},
 publisher = {Applied Probability Trust},
 title = {Upper (Lower) Bounds on the Mean of the Maximum (Minimum) of a Number of Random Variables},
 volume = {22},
 year = {1985}
}

@article{parr,
  title={Least-squares policy iteration},
  author={Lagoudakis, Michail G and Parr, Ronald},
  journal={The Journal of Machine Learning Research},
  volume={4},
  pages={1107--1149},
  year={2003},
  publisher={JMLR. org}
}

@article{munosbook,
author = {Munos, Rémi},
year = {2014},
month = {01},
pages = {},
title = {From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning},
volume = {7},
journal = {Foundations and Trends in Machine Learning},
doi = {10.1561/2200000038}
}
 
 @inproceedings{CoquelinMunos,
author = {Coquelin, Pierre-Arnaud and Munos, R\'{e}mi},
title = {Bandit Algorithms for Tree Search},
year = {2007},
isbn = {0974903930},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go [6]. Their efficient exploration of the tree enables to return rapidly a good value, and improve precision if more time is provided. The UCT algorithm [8], a tree search method based on Upper Confidence Bounds (UCB) [2], is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is "over-optimistic" in some sense, leading to a worst-case regret that may be very poor. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially in the horizon depth is analyzed. We then consider Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce and analyze a Bandit Algorithm for Smooth Trees (BAST) which takes into account actual smoothness of the rewards for performing efficient "cuts" of sub-optimal branches with high confidence. Finally, we present an incremental tree expansion which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, only the optimal branches are indefinitely developed. We illustrate these methods on a global optimization problem of a continuous function, given noisy values.},
booktitle = {Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence},
pages = {67–74},
numpages = {8},
location = {Vancouver, BC, Canada},
series = {UAI'07}
}

 
 @misc{chen2020finitesample,
      title={Finite-Sample Analysis of Contractive Stochastic Approximation Using Smooth Convex Envelopes}, 
      author={Zaiwei Chen and Siva Theja Maguluri and Sanjay Shakkottai and Karthikeyan Shanmugam},
      year={2020},
      eprint={2002.00874},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
 
@article{tsitsiklis1994asynchronous,
 title={Asynchronous stochastic approximation and Q-learning},
 author={Tsitsiklis, John N},
 journal={Machine learning},
 volume={16},
 number={3},
 pages={185--202},
 year={1994},
 publisher={Springer}
}

@article {Shapley,
	author = {Shapley, L. S.},
	title = {Stochastic Games},
	volume = {39},
	number = {10},
	pages = {1095--1100},
	year = {1953},
	doi = {10.1073/pnas.39.10.1095},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/39/10/1095},
	eprint = {https://www.pnas.org/content/39/10/1095.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}


@inproceedings{Jordan,
 author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {4863--4873},
 publisher = {Curran Associates, Inc.},
 title = {Is Q-Learning Provably Efficient?},
 url = {https://proceedings.neurips.cc/paper/2018/file/d3b1fb02964aa64e257f9f26a31f72cf-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{Bertsekas2011ApproximatePI,
  title={Approximate policy iteration: a survey and some new methods},
  author={D. Bertsekas},
  journal={Journal of Control Theory and Applications},
  year={2011},
  volume={9},
  pages={310-335}
}


@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}
 
@book{bertsekas2004stochastic,
  title={Stochastic optimal control: the discrete-time case},
  author={Bertsekas, Dimitir P and Shreve, Steven},
  year={2004}
}

@book{bertsekas1978stochastic,
  title={Stochastic Optimal Control: The Discrete Time Case},
  author={Bertsekas, D.P. and Shreve, S.E.},
  isbn={9780120932603},
  lccn={77025727},
  series={Mathematics in science and engineering},
  year={1978},
  publisher={Academic Press}
}


@book{topological,
author = {Wilson, Robin J},
title = {Introduction to Graph Theory},
year = {1986},
isbn = {0470206160},
publisher = {John Wiley \& Sons, Inc.},
address = {USA}
}

@book{bertsekasvolI,
  title={Dynamic Programming and Optimal Control},
  author={Bertsekas, D.P.},
  number={v. 1},
  isbn={9781886529267},
  lccn={lc00091281},
  series={Athena Scientific optimization and computation series},
  year={2005},
  publisher={Athena Scientific}
}

@book{bersekasvolII,
author = {Bertsekas, Dimitri P.},
title = {Dynamic Programming and Optimal Control, Vol. II},
year = {2007},
isbn = {1886529302},
publisher = {Athena Scientific},
edition = {3rd}
}

@inproceedings{siva2009stability,
  title={Stability of model predictive control using Markov chain Monte Carlo optimisation},
  author={Siva, Elilini and Goulart, Paul and Maciejowski, Jan and Kantas, Nikolas},
  booktitle={2009 European Control Conference (ECC)},
  pages={2851--2856},
  year={2009},
  organization={IEEE}
}

@article{tesauro1996line,
  title={On-line policy improvement using Monte-Carlo search},
  author={Tesauro, Gerald and Galperin, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={9},
  year={1996}
}

@article{fackeldey2022approximative,
  title={Approximative policy iteration for exit time feedback control problems driven by stochastic differential equations using tensor train format},
  author={Fackeldey, Konstantin and Oster, Mathias and Sallandt, Leon and Schneider, Reinhold},
  journal={Multiscale Modeling \& Simulation},
  volume={20},
  number={1},
  pages={379--403},
  year={2022},
  publisher={SIAM}
}

@article{buro2020,
  author    = {Arta Seify and
               Michael Buro},
  title     = {Single-Agent Optimization Through Policy Iteration Using Monte-Carlo
               Tree Search},
  journal   = {CoRR},
  volume    = {abs/2005.11335},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.11335},
  eprinttype = {arXiv},
  eprint    = {2005.11335},
  timestamp = {Thu, 28 May 2020 17:38:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-11335.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{kantas2009sequential,
  title={Sequential Monte Carlo for model predictive control},
  author={Kantas, Nikolas and Maciejowski, JM and Lecchini-Visintini, A},
  booktitle={Nonlinear model predictive control},
  pages={263--273},
  year={2009},
  publisher={Springer}
}

@article{mesbah2016stochastic,
  title={Stochastic model predictive control: An overview and perspectives for future research},
  author={Mesbah, Ali},
  journal={IEEE Control Systems Magazine},
  volume={36},
  number={6},
  pages={30--44},
  year={2016},
  publisher={IEEE}
}


@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@misc{moerland2020framework,
      title={A Framework for Reinforcement Learning and Planning}, 
      author={Thomas M. Moerland and Joost Broekens and Catholijn M. Jonker},
      year={2020},
      eprint={2006.15009},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{efroni2020online,
  title={Online Planning with Lookahead Policies},
  author={Efroni, Yonathan and Ghavamzadeh, Mohammad and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@phdthesis{moerland2021intersection,
  title={The Intersection of Planning and Learning.},
  author={Moerland, Thomas M},
  year={2021},
  school={Delft University of Technology, Netherlands}
}

@inproceedings{veness,
 author = {Veness, Joel and Silver, David and Blair, Alan and Uther, William},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Bootstrapping from Game Tree Search},
 url = {https://proceedings.neurips.cc/paper/2009/file/389bc7bb1e1c2a5e7e147703232a88f6-Paper.pdf},
 volume = {22},
 year = {2009}
}

@inproceedings{scherrer,
  title={Non-Stationary Approximate Modified Policy Iteration},
  author={Boris Lesner and Bruno Scherrer},
  booktitle={ICML},
  year={2015}
}

@article{efroni2018multiple,
  title={Multiple-step greedy policies in online and approximate reinforcement learning},
  author={Efroni, Yonathan and Dalal, Gal and Scherrer, Bruno and Mannor, Shie},
  journal={arXiv preprint arXiv:1805.07956},
  year={2018}
}

@inproceedings{srikant2019finite,
  title={Finite-time error bounds for linear stochastic approximation andtd learning},
  author={Srikant, Rayadurgam and Ying, Lei},
  booktitle={Conference on Learning Theory},
  pages={2803--2830},
  year={2019},
  organization={PMLR}
}

@inproceedings{bhandari2018finite,
  title={A finite time analysis of temporal difference learning with linear function approximation},
  author={Bhandari, Jalaj and Russo, Daniel and Singal, Raghav},
  booktitle={Conference on learning theory},
  pages={1691--1692},
  year={2018},
  organization={PMLR}
}

@INPROCEEDINGS{9407870,

  author={Deng, Haibo and Yin, Shiqun and Deng, Xiaohong and Li, Shiwei},

  booktitle={2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)}, 

  title={Value-based Algorithms Optimization with Discounted Multiple-step Learning Method in Deep Reinforcement Learning}, 

  year={2020},

  volume={},

  number={},

  pages={979-984},

  doi={10.1109/HPCC-SmartCity-DSS50907.2020.00131}}


@InProceedings{pmlr-v101-osogami19a,
  title = 	 {Real-time tree search with pessimistic scenarios: Winning the NeurIPS 2018 Pommerman Competition},
  author =       {Osogami, Takayuki and Takahashi, Toshihiro},
  booktitle = 	 {Proceedings of The Eleventh Asian Conference on Machine Learning},
  pages = 	 {583--598},
  year = 	 {2019},
  editor = 	 {Lee, Wee Sun and Suzuki, Taiji},
  volume = 	 {101},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--19 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v101/osogami19a/osogami19a.pdf},
  url = 	 {https://proceedings.mlr.press/v101/osogami19a.html},
  abstract = 	 {Autonomous agents need to make decisions in a sequential manner, under partially observable environment, and in consideration of how other agents behave. In critical situations, such decisions need to be made in real time for example to avoid collisions and recover to safe conditions. We propose a technique of tree search where a deterministic and pessimistic scenario is used after a specified depth. Because there is no branching with the deterministic scenario, the proposed technique allows us to take into account the events that can occur far ahead in the future. The effectiveness of the proposed technique is demonstrated in Pommerman, a multi-agent environment used in a NeurIPS 2018 competition, where the agents that implement the proposed technique have won the first and third places.}
}


@article{jia2019feature,
  title={Feature-based q-learning for two-player stochastic games},
  author={Jia, Zeyu and Yang, Lin F and Wang, Mengdi},
  journal={arXiv preprint arXiv:1906.00423},
  year={2019}
}

@inproceedings{sidford2020solving,
  title={Solving discounted stochastic two-player games with near-optimal time and sample complexity},
  author={Sidford, Aaron and Wang, Mengdi and Yang, Lin and Ye, Yinyu},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2992--3002},
  year={2020},
  organization={PMLR}
}

@article{daskalakis2020independent,
  title={Independent policy gradient methods for competitive reinforcement learning},
  author={Daskalakis, Constantinos and Foster, Dylan J and Golowich, Noah},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={5527--5540},
  year={2020}
}

@inproceedings{zhao2022provably,
  title={Provably efficient policy optimization for two-player zero-sum markov games},
  author={Zhao, Yulai and Tian, Yuandong and Lee, Jason and Du, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2736--2761},
  year={2022},
  organization={PMLR}
}

@inproceedings{xie2020learning,
  title={Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium},
  author={Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={Conference on learning theory},
  pages={3674--3682},
  year={2020},
  organization={PMLR}
}

@inproceedings{bai2020provable,
  title={Provable self-play algorithms for competitive reinforcement learning},
  author={Bai, Yu and Jin, Chi},
  booktitle={International conference on machine learning},
  pages={551--560},
  year={2020},
  organization={PMLR}
}

@inproceedings{perolat2015approximate,
  title={Approximate dynamic programming for two-player zero-sum Markov games},
  author={Perolat, Julien and Scherrer, Bruno and Piot, Bilal and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={1321--1329},
  year={2015},
  organization={PMLR}
}

@article{hansen2013strategy,
  title={Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor},
  author={Hansen, Thomas Dueholm and Miltersen, Peter Bro and Zwick, Uri},
  journal={Journal of the ACM (JACM)},
  volume={60},
  number={1},
  pages={1--16},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@article{zhang2020model,
  title={Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity},
  author={Zhang, Kaiqing and Kakade, Sham and Basar, Tamer and Yang, Lin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1166--1178},
  year={2020}
}

@inproceedings{liu2021sharp,
  title={A sharp analysis of model-based reinforcement learning with self-play},
  author={Liu, Qinghua and Yu, Tiancheng and Bai, Yu and Jin, Chi},
  booktitle={International Conference on Machine Learning},
  pages={7001--7010},
  year={2021},
  organization={PMLR}
}

@article{Yuanlong,
author = {Chen, Yuanlong},
year = {2018},
pages = {},
title = {On the convergence of optimistic policy iteration for stochastic shortest path problem},
note={arxiv}
}

@article{singh2000convergence,
  title={Convergence results for single-step on-policy reinforcement-learning algorithms},
  author={Singh, Satinder and Jaakkola, Tommi and Littman, Michael L and Szepesv{\'a}ri, Csaba},
  journal={Machine learning},
  volume={38},
  number={3},
  pages={287--308},
  year={2000},
  publisher={Springer}
}

@article{hoffman1966nonterminating,
  title={On nonterminating stochastic games},
  author={Hoffman, Alan J and Karp, Richard M},
  journal={Management Science},
  volume={12},
  number={5},
  pages={359--370},
  year={1966},
  publisher={INFORMS}
}

@book{filar1991algorithm,
  title={On the Algorithm of Pollatschek and Avi-ltzhak},
  author={Filar, Jerzy A and Tolwinski, Boleslaw},
  year={1991},
  publisher={Springer}
}

@book{filar2012competitive,
  title={Competitive Markov decision processes},
  author={Filar, Jerzy and Vrieze, Koos},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{hu2003nash,
  title={Nash Q-learning for general-sum stochastic games},
  author={Hu, Junling and Wellman, Michael P},
  journal={Journal of machine learning research},
  volume={4},
  number={Nov},
  pages={1039--1069},
  year={2003}
}

@article{lagoudakis2012value,
  title={Value function approximation in zero-sum markov games},
  author={Lagoudakis, Michail and Parr, Ron},
  journal={arXiv preprint arXiv:1301.0580},
  year={2012}
}

@incollection{littman1994markov,
  title={Markov games as a framework for multi-agent reinforcement learning},
  author={Littman, Michael L},
  booktitle={Machine learning proceedings 1994},
  pages={157--163},
  year={1994},
  publisher={Elsevier}
}

@inproceedings{Tsitsiklis1996AnalysisOT,
  title={Analysis of Temporal-Diffference Learning with Function Approximation},
  author={J. Tsitsiklis and Benjamin Van Roy},
  booktitle={NIPS},
  year={1996}
}

@article{lai2018,
  author    = {Matthew Lai},
  title     = {Giraffe: Using Deep Reinforcement Learning to Play Chess},
  journal   = {CoRR},
  volume    = {abs/1509.01549},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.01549},
  archivePrefix = {arXiv},
  eprint    = {1509.01549},
  timestamp = {Mon, 13 Aug 2018 16:47:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Lai15a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{efroni2018,
  author    = {Yonathan Efroni and
               Gal Dalal and
               Bruno Scherrer and
               Shie Mannor},
  title     = {Beyond the One Step Greedy Approach in Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1802.03654},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.03654},
  archivePrefix = {arXiv},
  eprint    = {1802.03654},
  timestamp = {Mon, 13 Aug 2018 16:48:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-03654.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{grigorescudriving,
  author    = {Sorin Mihai Grigorescu and
               Bogdan Trasnea and
               Tiberiu T. Cocias and
               Gigel Macesanu},
  title     = {A Survey of Deep Learning Techniques for Autonomous Driving},
  journal   = {CoRR},
  volume    = {abs/1910.07738},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.07738},
  archivePrefix = {arXiv},
  eprint    = {1910.07738},
  timestamp = {Tue, 22 Oct 2019 18:17:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-07738.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{zaiwei,
  author    = {Zaiwei Chen and
               Siva Theja Maguluri and
               Sanjay Shakkottai and
               Karthikeyan Shanmugam},
  title     = {Finite-Sample Analysis of Stochastic Approximation Using Smooth Convex
               Envelopes},
  journal   = {CoRR},
  volume    = {abs/2002.00874},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.00874},
  eprinttype = {arXiv},
  eprint    = {2002.00874},
  timestamp = {Mon, 10 Feb 2020 15:12:57 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-00874.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{shahxie,
  author    = {Devavrat Shah and
               Varun Somani and
               Qiaomin Xie and
               Zhi Xu},
  title     = {On Reinforcement Learning for Turn-based Zero-sum Markov Games},
  journal   = {CoRR},
  volume    = {abs/2002.10620},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.10620},
  eprinttype = {arXiv},
  eprint    = {2002.10620},
  timestamp = {Wed, 27 Apr 2022 13:25:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-10620.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2022finite,
  title={Finite-sample analysis of nonlinear stochastic approximation with applications in reinforcement learning},
  author={Chen, Zaiwei and Zhang, Sheng and Doan, Thinh T and Clarke, John-Paul and Maguluri, Siva Theja},
  journal={Automatica},
  volume={146},
  pages={110623},
  year={2022},
  publisher={Elsevier}
}

@article{annaor,
  author    = {Anna Winnicki and
               Joseph Lubars and
               Michael Livesay and
               R. Srikant},
  title     = {The Role of Lookahead and Approximate Policy Evaluation in Policy
               Iteration with Linear Value Function Approximation},
  journal   = {CoRR},
  volume    = {abs/2109.13419},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.13419},
  eprinttype = {arXiv},
  eprint    = {2109.13419},
  timestamp = {Mon, 04 Oct 2021 17:22:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-13419.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%% LaTeX2e file `refs.bib'
%% generated by the `filecontents' environment
%% from source `proof_general' on 2020/09/09.


@article{zhang2019non,
  title={Non-cooperative inverse reinforcement learning},
  author={Zhang, Xiangyuan and Zhang, Kaiqing and Miehling, Erik and Basar, Tamer},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{shalev2016safe,
  title={Safe, multi-agent, reinforcement learning for autonomous driving},
  author={Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  journal={arXiv preprint arXiv:1610.03295},
  year={2016}
}

@article{qu2022scalable,
  title={Scalable reinforcement learning for multiagent networked systems},
  author={Qu, Guannan and Wierman, Adam and Li, Na},
  journal={Operations Research},
  volume={70},
  number={6},
  pages={3601--3628},
  year={2022},
  publisher={INFORMS}
}

@article{yang2020overview,
  title={An overview of multi-agent reinforcement learning from game theoretical perspective},
  author={Yang, Yaodong and Wang, Jun},
  journal={arXiv preprint arXiv:2011.00583},
  year={2020}
}

@article{yang2020multi,
  title={Multi-robot path planning based on a deep reinforcement learning DQN algorithm},
  author={Yang, Yang and Juntao, Li and Lingling, Peng},
  journal={CAAI Transactions on Intelligence Technology},
  volume={5},
  number={3},
  pages={177--183},
  year={2020},
  publisher={Wiley Online Library}
}
@inproceedings{gu2017deep,
  title={Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={3389--3396},
  year={2017},
  organization={IEEE}
}

@article{uehara2021representation,
  title={Representation learning for online and offline rl in low-rank mdps},
  author={Uehara, Masatoshi and Zhang, Xuezhou and Sun, Wen},
  journal={arXiv preprint arXiv:2110.04652},
  year={2021}
}

@article{agarwal2020flambe,
  title={Flambe: Structural complexity and representation learning of low rank mdps},
  author={Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={20095--20107},
  year={2020}
}

@article{winnicki2023convergence,
  title={On The Convergence Of Policy Iteration-Based Reinforcement Learning With Monte Carlo Policy Evaluation},
  author={Winnicki, Anna and Srikant, R},
  journal={Artificial Intelligence and Statistics },
  year={2023}
}
@article{joseph,
  author    = {Joseph Lubars and
               Anna Winnicki and
               Michael Livesay and
               R. Srikant},
  title     = {Optimistic Policy Iteration for {MDP}s with Acyclic Transient State
               Structure},
  journal   = {CoRR},
  volume    = {abs/2102.00030},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.00030},
  eprinttype = {arXiv},
  eprint    = {2102.00030},
  timestamp = {Tue, 09 Feb 2021 13:35:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-00030.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{doancontrol,
  author={Zeng, Sihan and Doan, Thinh T. and Romberg, Justin},
  booktitle={2021 60th IEEE Conference on Decision and Control (CDC)}, 
  title={Finite-Time Analysis of Decentralized Stochastic Approximation with Applications in Multi-Agent and Multi-Task Learning}, 
  year={2021},
  volume={},
  number={},
  pages={2641-2646},
  doi={10.1109/CDC45484.2021.9683363}}

@article{perkins2002convergent,
  title={A convergent form of approximate policy iteration},
  author={Perkins, Theodore and Precup, Doina},
  journal={Advances in neural information processing systems},
  volume={15},
  year={2002}
}

@article{chen2021lyapunov,
  title={A Lyapunov theory for finite-sample guarantees of asynchronous Q-learning and TD-learning variants},
  author={Chen, Zaiwei and Maguluri, Siva Theja and Shakkottai, Sanjay and Shanmugam, Karthikeyan},
  journal={arXiv preprint arXiv:2102.01567},
  year={2021}
}
@INPROCEEDINGS{abadcontrol,
  author={Abad, F.J.V. and Krishnamurthy, V.},
  booktitle={42nd IEEE International Conference on Decision and Control }, 
  title={Policy gradient stochastic approximation algorithms for adaptive control of constrained time varying Markov decision processes}, 
  year={2003},
  volume={3},
  number={},
  pages={2823-2828 Vol.3},
  doi={10.1109/CDC.2003.1273053}}


@INPROCEEDINGS{controltsitsiklis,
  author={Tsitsiklis, J.N.},
  booktitle={Proceedings of 32nd IEEE Conference on Decision and Control}, 
  title={Asynchronous stochastic approximation and Q-learning}, 
  year={1993},
  volume={},
  number={},
  pages={395-400 vol.1},
  doi={10.1109/CDC.1993.325119}}


@incollection{powell2021reinforcement,
  title={From reinforcement learning to optimal control: A unified framework for sequential decisions},
  author={Powell, Warren B},
  booktitle={Handbook of Reinforcement Learning and Control},
  pages={29--74},
  year={2021},
  publisher={Springer}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}
@article{konda1999actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay and Tsitsiklis, John},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}


@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}

@article{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={10836--10846},
  year={2019}
}

 @article{DBLP:journals/corr/MnihBMGLHSK16,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1602.01783},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.01783},
  archivePrefix = {arXiv},
  eprint    = {1602.01783},
  timestamp = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihBMGLHSK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@InProceedings{satpathi,
  title = 	 {The Dynamics of Gradient Descent for Overparametrized Neural Networks},
  author =       {Satpathi, Siddhartha and Srikant, R},
  booktitle = 	 {Proceedings of the 3rd Conference on Learning for Dynamics and Control},
  pages = 	 {373--384},
  year = 	 {2021}
}


 
@ARTICLE{tsitsiklisvanroy,
  author={Tsitsiklis, J.N. and Van Roy, B.},
  journal={IEEE Transactions on Automatic Control}, 
  title={An analysis of temporal-difference learning with function approximation}, 
  year={1997},
  volume={42},
  number={5},
  pages={674-690},
  doi={10.1109/9.580874}}

@article{bertsekasioffe,
author = {Bertsekas, Dimitri and Ioffe, Sergey},
year = {2012},
month = {04},
pages = {},
title = {Temporal Differences-Based Policy Iteration and Applications in Neuro-Dynamic Programming1}
}

 @inproceedings{du2018gradient,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}

@inproceedings{mehta2009q,
  title={Q-learning and Pontryagin's minimum principle},
  author={Mehta, Prashant and Meyn, Sean},
  booktitle={Proceedings of the 48h IEEE Conference on Decision and Control},
  pages={3598--3605},
  year={2009},
  organization={IEEE}
}

@inproceedings{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
 
@article{browne,
author = {Browne, Cameron and Powley, Edward and Whitehouse, Daniel and Lucas, Simon and Cowling, Peter and Rohlfshagen, Philipp and Tavener, Stephen and Perez Liebana, Diego and Samothrakis, Spyridon and Colton, Simon},
year = {2012},
month = {03},
pages = {1-43},
title = {A Survey of Monte Carlo Tree Search Methods},
volume = {4:1},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
doi = {10.1109/TCIAIG.2012.2186810}
}
 
 
@inproceedings{kocisszepesvari,
author = {Kocsis, Levente and Szepesvári, Csaba},
year = {2006},
month = {09},
pages = {282-293},
title = {Bandit Based Monte-Carlo Planning},
volume = {2006},
isbn = {978-3-540-45375-8},
booktitle = {Machine Learning: ECML},
doi = {10.1007/11871842_29}
}

 @article{chen2022finite,
  title={Finite-sample analysis of nonlinear stochastic approximation with applications in reinforcement learning},
  author={Chen, Zaiwei and Zhang, Sheng and Doan, Thinh T and Clarke, John-Paul and Maguluri, Siva Theja},
  journal={Automatica},
  volume={146},
  pages={110623},
  year={2022},
  publisher={Elsevier}
}

@book{suttonbarto, author = {Sutton, Richard S. and Barto, Andrew G.}, title = {Introduction to Reinforcement Learning}, year = {1998}, isbn = {0262193981}, publisher = {MIT Press}, address = {Cambridge, MA, USA}, edition = {1st}, abstract = {From the Publisher:In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.} }
 
 @inproceedings{efroni2019combine,
  title={How to combine tree-search methods in reinforcement learning},
  author={Efroni, Yonathan and Dalal, Gal and Scherrer, Bruno and Mannor, Shie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3494--3501},
  year={2019}
}
 
 @article{Aven,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3213876},
 abstract = {In this paper, we give some upper (lower) bounds on E max1 ≤ i ≤ n Xi(E min1 ≤ i ≤ n Xi), where the Xi's are real-valued random variables. Some applications are given.},
 author = {Terje Aven},
 journal = {Journal of Applied Probability},
 number = {3},
 pages = {723--728},
 publisher = {Applied Probability Trust},
 title = {Upper (Lower) Bounds on the Mean of the Maximum (Minimum) of a Number of Random Variables},
 volume = {22},
 year = {1985}
}

 
 @inproceedings{CoquelinMunos,
author = {Coquelin, Pierre-Arnaud and Munos, R\'{e}mi},
title = {Bandit Algorithms for Tree Search},
year = {2007},
isbn = {0974903930},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go [6]. Their efficient exploration of the tree enables to return rapidly a good value, and improve precision if more time is provided. The UCT algorithm [8], a tree search method based on Upper Confidence Bounds (UCB) [2], is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is "over-optimistic" in some sense, leading to a worst-case regret that may be very poor. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially in the horizon depth is analyzed. We then consider Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce and analyze a Bandit Algorithm for Smooth Trees (BAST) which takes into account actual smoothness of the rewards for performing efficient "cuts" of sub-optimal branches with high confidence. Finally, we present an incremental tree expansion which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, only the optimal branches are indefinitely developed. We illustrate these methods on a global optimization problem of a continuous function, given noisy values.},
booktitle = {Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence},
pages = {67–74},
numpages = {8},
location = {Vancouver, BC, Canada},
series = {UAI'07}
}
 
@inproceedings{shah2020nonasymptotic,
  title={Non-asymptotic analysis of monte carlo tree search},
  author={Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
  booktitle={Abstracts of the 2020 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems},
  pages={31--32},
  year={2020}
}

@article{ma2019monte,
  title={Monte-carlo tree search for policy optimization},
  author={Ma, Xiaobai and Driggs-Campbell, Katherine and Zhang, Zongzhang and Kochenderfer, Mykel J},
  journal={arXiv preprint arXiv:1912.10648},
  year={2019}
}
  
 @book{bertsekas2019reinforcement,
  title={Reinforcement learning and optimal control},
  author={Bertsekas, Dimitri P},
  year={2019},
  publisher={Athena Scientific Belmont, MA}
}

@article{tsitsiklis1994asynchronous,
 title={Asynchronous stochastic approximation and Q-learning},
 author={Tsitsiklis, John N},
 journal={Machine learning},
 volume={16},
 number={3},
 pages={185--202},
 year={1994},
 publisher={Springer}
}

@inproceedings{jin2022power,
  title={The power of exploiter: Provable multi-agent rl in large state spaces},
  author={Jin, Chi and Liu, Qinghua and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={10251--10279},
  year={2022},
  organization={PMLR}
}

@article{zhang2021multi,
  title={Multi-agent reinforcement learning: A selective overview of theories and algorithms},
  author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal={Handbook of reinforcement learning and control},
  pages={321--384},
  year={2021},
  publisher={Springer}
}

@article{ozdaglar2021independent,
  title={Independent learning in stochastic games},
  author={Ozdaglar, Asuman and Sayin, Muhammed O and Zhang, Kaiqing},
  journal={arXiv preprint arXiv:2111.11743},
  year={2021}
}

@inproceedings{jin2020reward,
  title={Reward-free exploration for reinforcement learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={4870--4879},
  year={2020},
  organization={PMLR}
}

@article{li2020breaking,
  title={Breaking the sample size barrier in model-based reinforcement learning with a generative model},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12861--12872},
  year={2020}
}


@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}


@article{gheshlaghi2013minimax,
  title={Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
  author={Gheshlaghi Azar, Mohammad and Munos, R{\'e}mi and Kappen, Hilbert J},
  journal={Machine learning},
  volume={91},
  pages={325--349},
  year={2013},
  publisher={Springer}
}

@article{bertsekas2021distributed,
  title={Distributed Asynchronous Policy Iteration for Sequential Zero-Sum Games and Minimax Control},
  author={Bertsekas, Dimitri},
  journal={arXiv preprint arXiv:2107.10406},
  year={2021}
}

@inproceedings{brahma2022convergence,
  title={Convergence Rates of Asynchronous Policy Iteration for Zero-Sum Markov Games under Stochastic and Optimistic Settings},
  author={Brahma, Sarnaduti and Bai, Yitao and Do, Duy Anh and Doan, Thinh T},
  booktitle={2022 IEEE 61st Conference on Decision and Control (CDC)},
  pages={3493--3498},
  year={2022},
  organization={IEEE}
}

@inproceedings{perolat2016softened,
  title={Softened approximate policy iteration for Markov games},
  author={P{\'e}rolat, Julien and Piot, Bilal and Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={1860--1868},
  year={2016},
  organization={PMLR}
}
@article{van1978discounted,
  title={Discounted Markov games: Generalized policy iteration method},
  author={Van Der Wal, J},
  journal={Journal of Optimization Theory and Applications},
  volume={25},
  number={1},
  pages={125--138},
  year={1978},
  publisher={Springer}
}

@article{pollatschek1969algorithms,
  title={Algorithms for stochastic games with geometrical interpretation},
  author={Pollatschek, MA and Avi-Itzhak, B},
  journal={Management Science},
  volume={15},
  number={7},
  pages={399--415},
  year={1969},
  publisher={INFORMS}
}



@inproceedings{Jordan,
 author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {4863--4873},
 publisher = {Curran Associates, Inc.},
 title = {Is Q-Learning Provably Efficient?},
 url = {https://proceedings.neurips.cc/paper/2018/file/d3b1fb02964aa64e257f9f26a31f72cf-Paper.pdf},
 volume = {31},
 year = {2018}
}


@misc{lubars2021optimistic,
      title={Optimistic Policy Iteration for MDPs with Acyclic Transient State Structure}, 
      author={Joseph Lubars and Anna Winnicki and Michael Livesay and R. Srikant},
      year={2021},
      eprint={2102.00030},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{annacdc,
  author={Winnicki, Anna and Srikant, R.},
  booktitle={2022 IEEE 61st Conference on Decision and Control (CDC)}, 
  title={Reinforcement Learning with Unbiased Policy Evaluation and Linear Function Approximation}, 
  year={2022},
  volume={},
  number={},
  pages={801-806},
  doi={10.1109/CDC51059.2022.9992427}}

@article{rubinstein1999experience,
  title={Experience from a course in game theory: pre-and postclass problem sets as a didactic device},
  author={Rubinstein, Ariel},
  journal={Games and Economic Behavior},
  volume={28},
  number={1},
  pages={155--170},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{zhang2022making,
  title={Making linear mdps practical via contrastive representation learning},
  author={Zhang, Tianjun and Ren, Tongzheng and Yang, Mengjiao and Gonzalez, Joseph and Schuurmans, Dale and Dai, Bo},
  booktitle={International Conference on Machine Learning},
  pages={26447--26466},
  year={2022},
  organization={PMLR}
}

@article{busoniu2008comprehensive,
  title={A comprehensive survey of multiagent reinforcement learning},
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={38},
  number={2},
  pages={156--172},
  year={2008},
  publisher={IEEE}
}

@article{Agarwal2019ReinforcementLT,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  journal={CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep},
  pages={10--4},
  year={2019}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}
 
@book{bertsekas2004stochastic,
  title={Stochastic optimal control: the discrete-time case},
  author={Bertsekas, Dimitir P and Shreve, Steven},
  year={2004}
}

@book{bertsekas1978stochastic,
  title={Stochastic Optimal Control: The Discrete Time Case},
  author={Bertsekas, D.P. and Shreve, S.E.},
  isbn={9780120932603},
  lccn={77025727},
  series={Mathematics in science and engineering},
  year={1978},
  publisher={Academic Press}
}


@book{topological,
author = {Wilson, Robin J},
title = {Introduction to Graph Theory},
year = {1986},
isbn = {0470206160},
publisher = {John Wiley \& Sons, Inc.},
address = {USA}
}

@book{bertsekasvolI,
  title={Dynamic Programming and Optimal Control},
  author={Bertsekas, D.P.},
  number={v. 1},
  isbn={9781886529267},
  lccn={lc00091281},
  series={Athena Scientific optimization and computation series},
  year={2005},
  publisher={Athena Scientific}
}

@book{bersekasvolII,
author = {Bertsekas, Dimitri P.},
title = {Dynamic Programming and Optimal Control, Vol. II},
year = {2007},
isbn = {1886529302},
publisher = {Athena Scientific},
edition = {3rd}
}

@article{chang2005adaptive,
  title={An adaptive sampling algorithm for solving {M}arkov decision processes},
  author={Chang, Hyeong Soo and Fu, Michael C and Hu, Jiaqiao and Marcus, Steven I},
  journal={Operations Research},
  volume={53},
  number={1},
  pages={126--139},
  year={2005},
  publisher={INFORMS}
}

@article{mathkar2016distributed,
  title={Distributed reinforcement learning via gossip},
  author={Mathkar, Adwaitvedant and Borkar, Vivek S},
  journal={IEEE Transactions on Automatic Control},
  volume={62},
  number={3},
  pages={1465--1470},
  year={2016},
  publisher={IEEE}
}
@phdthesis{moerland2021intersection,
  title={The Intersection of Planning and Learning.},
  author={Moerland, Thomas M},
  year={2021},
  school={Delft University of Technology, Netherlands}
}

@inproceedings{lanctot2014monte,
  title={Monte Carlo tree search with heuristic evaluations using implicit minimax backups},
  author={Lanctot, Marc and Winands, Mark HM and Pepels, Tom and Sturtevant, Nathan R},
  booktitle={2014 IEEE Conference on Computational Intelligence and Games},
  pages={1--8},
  year={2014},
  organization={IEEE}
}

@inproceedings{tomar2020multistep,
  title={Multi-step greedy reinforcement learning algorithms},
  author={Tomar, Manan and Efroni, Yonathan and Ghavamzadeh, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={9504--9513},
  year={2020},
  organization={PMLR}
}


@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{springenberg2020local,
  title={Local search for policy iteration in continuous control},
  author={Springenberg, Jost Tobias and Heess, Nicolas and Mankowitz, Daniel and Merel, Josh and Byravan, Arunkumar and Abdolmaleki, Abbas and Kay, Jackie and Degrave, Jonas and Schrittwieser, Julian and Tassa, Yuval and others},
  journal={arXiv preprint arXiv:2010.05545},
  year={2020}
}

@InProceedings{pmlr-v101-osogami19a,
  title = 	 {Real-time tree search with pessimistic scenarios: Winning the NeurIPS 2018 Pommerman Competition},
  author =       {Osogami, Takayuki and Takahashi, Toshihiro},
  booktitle = 	 {Proceedings of The Eleventh Asian Conference on Machine Learning},
  pages = 	 {583--598},
  year = 	 {2019},
  editor = 	 {Lee, Wee Sun and Suzuki, Taiji},
  volume = 	 {101},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--19 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v101/osogami19a/osogami19a.pdf},
  url = 	 {https://proceedings.mlr.press/v101/osogami19a.html},
  abstract = 	 {Autonomous agents need to make decisions in a sequential manner, under partially observable environment, and in consideration of how other agents behave. In critical situations, such decisions need to be made in real time for example to avoid collisions and recover to safe conditions. We propose a technique of tree search where a deterministic and pessimistic scenario is used after a specified depth. Because there is no branching with the deterministic scenario, the proposed technique allows us to take into account the events that can occur far ahead in the future. The effectiveness of the proposed technique is demonstrated in Pommerman, a multi-agent environment used in a NeurIPS 2018 competition, where the agents that implement the proposed technique have won the first and third places.}
}


@book{bertsekastsitsiklis,
  title={Neuro-dynamic Programming},
  author={Bertsekas, D.P. and Tsitsiklis, J.N.},
  isbn={9781886529106},
  lccn={lc96085338},
  year={1996},
  publisher={Athena Scientific}
}

@phdthesis{patekthesis,
  title={Stochastic and shortest path games: theory and algorithms},
  author={Patek, Stephen David},
  year={1997},
  school={Massachusetts Institute of Technology}
}

@article{Yuanlong,
author = {Chen, Yuanlong},
year = {2018},
pages = {},
title = {On the convergence of optimistic policy iteration for stochastic shortest path problem},
note={arxiv}
}

@article{Mahmood,
  author    = {Ashique Rupam Mahmood and
               Huizhen Yu and
               Richard S. Sutton},
  title     = {Multi-step Off-policy Learning Without Importance Sampling Ratios},
  journal   = {CoRR},
  volume    = {abs/1702.03006},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.03006},
  eprinttype = {arXiv},
  eprint    = {1702.03006},
  timestamp = {Mon, 13 Aug 2018 16:49:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MahmoodYS17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{singh1996reinforcement,
  title={Reinforcement learning with replacing eligibility traces},
  author={Singh, Satinder P and Sutton, Richard S},
  journal={Machine learning},
  volume={22},
  number={1},
  pages={123--158},
  year={1996},
  publisher={Springer}
}
@article{singh2000convergence,
  title={Convergence results for single-step on-policy reinforcement-learning algorithms},
  author={Singh, Satinder and Jaakkola, Tommi and Littman, Michael L and Szepesv{\'a}ri, Csaba},
  journal={Machine learning},
  volume={38},
  number={3},
  pages={287--308},
  year={2000},
  publisher={Springer}
}

@inproceedings{Tsitsiklis1996AnalysisOT,
  title={Analysis of Temporal-Diffference Learning with Function Approximation},
  author={J. Tsitsiklis and Benjamin Van Roy},
  booktitle={NIPS},
  year={1996}
}



@INPROCEEDINGS{TsitsiklisRoy,
    author = {John N. Tsitsiklis and Benjamin Van Roy},
    title = {Feature-Based Methods For Large Scale Dynamic Programming},
    booktitle = {Machine Learning},
    year = {1994},
    pages = {59--94}
}


@article{lai2018,
  author    = {Matthew Lai},
  title     = {Giraffe: Using Deep Reinforcement Learning to Play Chess},
  journal   = {CoRR},
  volume    = {abs/1509.01549},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.01549},
  archivePrefix = {arXiv},
  eprint    = {1509.01549},
  timestamp = {Mon, 13 Aug 2018 16:47:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Lai15a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Puterman1978ModifiedPI,
  title={Modified Policy Iteration Algorithms for Discounted Markov Decision Problems},
  author={M. Puterman and M. C. Shin},
  journal={Management Science},
  year={1978},
  volume={24},
  pages={1127-1137}
}



@article{grigorescudriving,
  author    = {Sorin Mihai Grigorescu and
               Bogdan Trasnea and
               Tiberiu T. Cocias and
               Gigel Macesanu},
  title     = {A Survey of Deep Learning Techniques for Autonomous Driving},
  journal   = {CoRR},
  volume    = {abs/1910.07738},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.07738},
  archivePrefix = {arXiv},
  eprint    = {1910.07738},
  timestamp = {Tue, 22 Oct 2019 18:17:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-07738.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@book{breton1986computation,
  title={On the computation of equilibria in discounted stochastic dynamic games},
  author={Breton, Mich{\`e}le and Filar, Jerzy A and Haurle, Alain and Schultz, Todd A},
  year={1986},
  publisher={Springer}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{silver2017shoji,
  author    = {David Silver and
               Thomas Hubert and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Matthew Lai and
               Arthur Guez and
               Marc Lanctot and
               Laurent Sifre and
               Dharshan Kumaran and
               Thore Graepel and
               Timothy P. Lillicrap and
               Karen Simonyan and
               Demis Hassabis},
  title     = {Mastering Chess and Shogi by Self-Play with a General Reinforcement
               Learning Algorithm},
  journal   = {CoRR},
  volume    = {abs/1712.01815},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.01815},
  eprinttype = {arXiv},
  eprint    = {1712.01815},
  timestamp = {Mon, 13 Aug 2018 16:46:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}