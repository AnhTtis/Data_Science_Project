%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Author template for Stochastic Systems (stsy) [interim solution; new styles under construction]
%% Mirko Janc, Ph.D., INFORMS, mirko.janc@informs.org
%% ver. 0.92, August 2017
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[stsy]{informs-stsy}

\OneAndAHalfSpacedXI
%%\OneAndAHalfSpacedXII % Current default line spacing
%%\DoubleSpacedXII
%%\DoubleSpacedXI

% Private macros here (check that there is no clash with the style)
\usepackage{custom}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[caption=false]{subfig}
\newcommand\norm[1]{\lVert#1\rVert}
\newcommand\normx[1]{\Vert#1\Vert}
\usepackage{multirow}
\usepackage{amsmath}

\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{algorithm}

% Natbib setup for author-year style
\usepackage{natbib}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\small}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }%
 \def\BIBand{and}%

%% Setup of theorem styles. Outcomment only one.
%% Preferred default is the first option.
\TheoremsNumberedThrough     % Preferred (Theorem 1, Lemma 1, Theorem 2)
%\TheoremsNumberedByChapter  % (Theorem 1.1, Lema 1.1, Theorem 1.2)
\ECRepeatTheorems

%% Setup of the equation numbering system. Outcomment only one.
%% Preferred default is the first option.
\EquationsNumberedThrough    % Default: (1), (2), ...
%\EquationsNumberedBySection % (1.1), (1.2), ...

% For new submissions, leave this number blank.
% For revisions, input the manuscript number assigned by the on-line
% system along with a suffix ".Rx" where x is the revision number.%\EquationsNumberedBySection % (1.1), (1.2), ...

% For new submissions, leave this number blank.
% For revisions, input the manuscript number assigned by the on-line
% system along with a suffix ".Rx" where x is the revision number.
\MANUSCRIPTNO{MS-0001-1922.65}

%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%

% Outcomment only when entries are known. Otherwise leave as is and
%   default values will be used.
%\setcounter{page}{1}
%\VOLUME{00}%
%\NO{0}%
%\MONTH{Xxxxx}% (month or a similar seasonal id)
%\YEAR{0000}% e.g., 2005
%\FIRSTPAGE{000}%
%\LASTPAGE{000}%
%\SHORTYEAR{00}% shortened year (two-digit)
%\ISSUE{0000} %
%\LONGFIRSTPAGE{0001} %
%\DOI{10.1287/xxxx.0000.0000}%

% Author's names for the running heads
% Sample depending on the number of authors;
% \RUNAUTHOR{Jones}
 \RUNAUTHOR{Winnicki and Srikant}
% \RUNAUTHOR{Jones, Miller, and Wilson}
% \RUNAUTHOR{Jones et al.} % for four or more authors
% Enter authors following the given pattern:
%\RUNAUTHOR{}

% Title or shortened title suitable for running heads. Sample:
% \RUNTITLE{Bundling Information Goods of Decreasing Value}
% Enter the (shortened) title:
\RUNTITLE{Convergence Of Policy Iteration With Monte Carlo Policy Evaluation}

% Full title. Sample:
% \TITLE{Bundling Information Goods of Decreasing Value}
% Enter the full title:
\TITLE{A New Policy Iteration Algorithm For Reinforcement Learning In Zero-Sum Markov Games}

% Block of authors and their affiliations starts here:
% NOTE: Authors with same affiliation, if the order of authors allows,
%   should be entered in ONE field, separated by a comma.
%   \EMAIL field can be repeated if more than one author
\ARTICLEAUTHORS{%
\AUTHOR{ Anna Winnicki  }
\AFF{University of Illinois  Urbana-Champaign, Coordinated Science Laboratory and Department of Electrical and Computer Engineering, Urbana, IL, 61801, annaw5@illinois.edu} %, \URL{}}
\AUTHOR{R. Srikant}
\AFF{University of Illinois  Urbana-Champaign, Coordinated Science Laboratory and Department of Electrical and Computer Engineering, Urbana, IL, 61801, \EMAIL{rsrikant@illinois.edu}. R. Srikant is also affiliated with c3.ai DTI.}
% Enter all authors
} % end of the block


\ABSTRACT{
Optimal policies in standard MDPs can be obtained using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in \cite{hansen2013strategy} that one has to solve $\Omega(1/(1-\alpha))$ MDPs, where $\alpha$ is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and converges exponentially fast. The only addition we propose to naive policy iteration is the use of lookahead in the policy improvement phase. This is appealing because lookahead is anyway used in practical learning algorithms for games. We further show that lookahead can be implemented efficiently in linear Markov games, which are the counterpart of the much-studied linear MDPs. We illustrate the application of our new policy iteration algorithm by providing sample and time complexity bounds for policy-based RL (reinforcement learning) algorithms. 
}

\KEYWORDS{
Policy iteration, zero-sum Markov games, reinforcement learning, approximate dynamic programming
}
% Sample
%\KEYWORDS{deterministic inventory theory; infinite linear programming duality;
%  existence of optimal policies; semi-Markov decision process; cyclic schedule}

% Fill in data. If unknown, outcomment the field

\maketitle
\section{Introduction}



Multi-agent reinforcement learning algorithms have contributed to many successes in machine learning, including games such as chess and Go \cite{silver2016mastering, DBLP:journals/corr/MnihBMGLHSK16, silver2017mastering, silver2017shoji}, automated warehouses, robotic arms with multiple arms \cite{gu2017deep}, and autonomous traffic control \cite{yang2020multi, shalev2016safe}; see \cite{ozdaglar2021independent, zhang2021multi, yang2020overview} for surveys. Multi-agent RL can refer to one of many scenarios: (i) where a team of agents work towards a common goal where all the agents have the same information or not \cite{qu2022scalable}, (ii) non-cooperative games where there are multiple agents with their own objectives \cite{zhang2019non}, and (iii) zero-sum games where there are only two players with opposing objectives. We can further categorize problems as infinite-horizon, discounted reward/cost, finite-horizon, simultaneous move or turn-based games. The literature in this area is vast; here we focus on zero-sum, simultaneous move, discounted reward/cost Markov games.

In the case of model-based zero-sum discounted reward/cost simultaneous-move Markov games, the problem of interest is finding a Nash equilibrium strategy. In the setting of Markov Games (also known as Stochastic Games \cite{Shapley}), the problem of finding the Nash equilibrium is a generalization of finding an optimal policy for a Markov Decision process \cite{Puterman1978ModifiedPI, lagoudakis2012value}, however many algorithms that find optimal policies for MDPs cannot efficiently be extended to the Markov Games setting, largely due to monotonicity issues that arise from the competing objectives of the two players as opposed to the single entity setting of MDPs. 

While value iteration naturally extends to zero-sum Markov games \cite{Shapley}, 
the two main extensions of Howard's Policy Iteration (PI) \cite{Puterman1978ModifiedPI} for games are not very computationally efficient or simply do not converge.  The only known convergent algorithm, the Hoffman and Karp algorithm \cite{hoffman1966nonterminating}, requires solving an MDP at each iteration. 
It has been shown in \cite{hansen2013strategy} that one has to solve $\Omega(1/(1-\alpha))$ MDPs to implement the extension of PI for games. 

There is an alternative algorithm, called naive policy iteration, also known as the algorithm of Pollatschek and Avi-Itzhak, which requires far fewer computations, but is only known to converge that under restrictive assumptions \cite{pollatschek1969algorithms}. In fact, \cite{van1978discounted} shows that the algorithm does not converge, in general.

So a longstanding open question (for at least 53 years!) is whether naive policy iteration converges for a broad class of models. Often other attempts to answer this question have only succeeded in proving the convergence of modified versions of the algorithm for restrictive classes of games. The work of \cite{filar1991algorithm} analyzes a modification of the naive policy iteration algorithm. However, the work of \cite{perolat2016softened} shows that their proof hinges on the assumption that the $\scriptL_2$-norm of the optimal Bellman residual is smooth, which is generally not true. A significant recent contribution in \cite{ bertsekas2021distributed} is a modification of naive policy iteration that converges but requires more storage. However, this algorithm does not yet appear to have an extension to the function approximation setting even for Markov games with special structure such as linear Markov games. An extension of \cite{bertsekas2021distributed} can be found in \cite{brahma2022convergence} which studies stochastic and optimistic settings. Our contribution is a modified version of the naive policy iteration algorithm that converges exponentially fast for all discounted-reward/cost zero-sum Markov games. The modification is easy to explain: simply replace the policy improvement step with a lookahead version of policy improvement. Lookahead has been widely used in RL from the early application to backgammon \cite{tesauro1996line} to recent applications such as chess and Go in AlphaZero \cite{silver2017mastering}. But to the best of our knowledge, our result is the first which proves the convergence of naive policy iteration using lookahead. Additionally, we show lookahead has low computational complexity for the class of linear Markov games, which are a natural generalization of linear MDPs, which have been studied extensively recently \cite{agarwal2020flambe, uehara2021representation,zhang2022making}. We note that our result complements the recent results on the benefits of lookahead to improve the convergence properties of MDPs with \cite{annaor, annacdc, winnicki2023convergence} and without function approximation \cite{efroni2018multiple,efroni2018,  efroni2019combine, tomar2020multistep} in other different contexts. For more on lookahead see \cite{efroni2018multiple, efroni2019combine, tomar2020multistep, annaor, annacdc, winnicki2023convergence}. 

In fact, our results are for an algorithm which subsumes policy iteration and value iteration as special cases. Following \cite{perolat2015approximate}, we call the algorithm generalized policy iteration, although the MDP version of the algorithm goes by several names including modified policy iteration \cite{Puterman1978ModifiedPI} and optimistic policy iteration \cite{bertsekastsitsiklis}. 

We also incorporate function approximation into our algorithms. Prior works that extend policy iteration to Markov games have all attempted to extend their work to the setting of function approximation with limited success. The work of \cite{perolat2015approximate} extends the original policy iteration algorithm of \cite{hoffman1966nonterminating}, however, it is limited in two respects. First, the algorithm in \cite{perolat2015approximate} propagates an error bound in the policy evaluation and policy improvement steps. However, their bounds do not explicitly take into account the implementation details of least-squares-based policy evaluation. Hence, analogously to the counter-example in \cite{annaor}, it is unclear whether the bounds in \cite{perolat2015approximate} can be accurately applied in the least squares policy evaluation for games algorithm. Second, the algorithm in \cite{perolat2015approximate} is inefficient for the same reasons that the Hoffman-Karp algorithm is inefficient, i.e., it requires that each policy corresponding to the minimizer be evaluated approximately at each iteration. The work of \cite{lagoudakis2012value} attempts to extend the algorithm of Pollatschek and Avi-Itzhak to the linear value function approximation setting, which, while efficient, does not necessarily converge even in the exact case as shown by the counter-example in \cite{van1978discounted}. Finally, the modification to the algorithm of Pollatschek and Avi-Itzhak, the algorithm in the work of \cite{filar1991algorithm}, has been extended to the function approximation setting in \cite{perolat2016softened}, however, for the same reasons that algorithm also does not converge. In this work, we consider extensions to the function approximation setting. 

Finally, to show the applicability of our results, and in particular, to show that it can be combined with learning algorithms for zero-sum Markov games, we present a sample complexity result using the learning phase of the algorithm in the recent work of \cite{zhang2020model} along with our generalized policy iteration algorithm for planning.
Model-based algorithms generally consist of two phases: a learning phase where the probability transition matrix and average reward/cost are estimated, possibly using a generative model, and a planning phase. The results in \cite{zhang2020model} are obtained assuming that there exists an efficient algorithm for planning. The work of \cite{zhang2020model} analyzes the setting where a model is estimated from data and a general algorithm of one's choice including value iteration, policy iteration, etc. is used to find the Nash equilibrium.  Additionally, there are multiple RL algorithms for other versions of model-based multi-agent RL including turn-based Markov games \cite{sidford2020solving}, finite-horizon games \cite{bai2020provable,liu2021sharp}, among others.  
We outline our contributions as follows:

\subsection{Main Contributions:}
\textbf{Convergence Of Generalized Policy Iteration In Markov Games}
The computational difficulties associated with extending the policy iteration algorithm to games has been a longstanding open problem. Several studies shed light on the difficulty of the policy iteration algorithm \cite{hansen2013strategy}. We present a simple modification of the well-studied na√Øve policy iteration or the algorithm of Pollatschek and Avi-Itzhak \cite{pollatschek1969algorithms} which converges for all discounted, simultaneous-move Markov zero-sum games. Moreover, the algorithm converges exponentially fast.   We remark that our generalized policy iteration algorithm can also be seen as a generalization of both value iteration and policy iteration in games, which is interesting in its own right, even for single player MDPs. 

\textbf{Linear MDPs \& Function Approximation} We then extend the results to incorporate function approximation by studying convergence and scalability to lower dimension linear Markov games, noting recent successful results in making linear MDPs more practical using representation learning techniques \cite{ agarwal2020flambe, uehara2021representation,zhang2022making}. Prior work on approximating lookahead using MCTS shows that exponential computational complexity is inevitable in problems with no structure \cite{shah2020nonasymptotic}. In contrast, our results show that the computational complexity of implementing lookahead only depends on the dimension of the feature vectors if we exploit the linear structure of the problem, which is interesting in its own right, including in the case of single player MDPs. 

We also consider the case where the linear value function representation is not perfect and we provide performance bounds for the algorithm. In the algorithm, it is assumed that returns of the policy evaluation with the lookahead for only several states are generated exactly, and the returns for the rest of states are determined by finding a best fitting parameter. Our bounds are interpretable and in the special case of the tabular setting, i.e., one-hot encoded feature vectors, when all states are evaluated at each iteration, the error of the algorithm is zero. 

\textbf{Learning Value Functions from Noisy Observations} We then consider the case where policy evaluation is performed via observations from a single trajectory, resulting in an unbiased error. We show that a stochastic approximation algorithm for estimating the value function converges to that of the optimal policy up to a function approximation error.

\textbf{Reinforcement Learning} Many papers have studied the RL problem for zero-sum Markov games, in both model-based and model-free settings. The most relevant paper to our setting  \cite{zhang2020model} is agnostic to the planning algorithm used. However, due to the lack of any prior results on the use of policy iteration, the results in that paper will not hold if one were to use naive policy iteration because it is known to not converge in some examples. Here, combining our results with \cite{zhang2020model}, we provide a characterization of the sample complexity involved in model-based learning combined with generalized policy iteration for games.

\section{Related Works}
\textbf{Policy iteration for games} Since the introduction of a mathematical framework for Markov games in \cite{Shapley}, value-based algorithms to obtain a Nash equilibrium for Markov games have been studied extensively, including the early works of \cite{littman1994markov, patekthesis, hu2003nash}. Policy iteration algorithms have been far less successfully studied despite the fact that \cite{van1978discounted} shows that Shapley's value iteration in games is slower in practice than naive policy iteration \cite{pollatschek1969algorithms}. Relevant prior works include \cite{patekthesis, perolat2015approximate} which obtain convergence of policy iteration algorithms for Markov games which require the solution of an MDP at each step, which is computationally burdensome. Furthermore, the work of \cite{pollatschek1969algorithms} following the work of \cite{hoffman1966nonterminating}   proposes an algorithm that is far more computationally efficient but they only show the algorithm converges in specific settings. The works of \cite{filar1991algorithm,breton1986computation} obtain convergence of a variant of the algorithm in \cite{pollatschek1969algorithms} under certain conditions \cite{perolat2016softened}. Recently, the works of \cite{bertsekas2021distributed, brahma2022convergence} study a variant of the algorithm in \cite{pollatschek1969algorithms} that converges, but the dimension of vectors to be stored is quite large and the algorithms have not been shown to be extendible the function approximation setting. 

\textbf{Model-based reinforcement learning in Markov games} The works of \cite{jia2019feature, sidford2020solving} study value-based approaches that assume the use of generative models where any state-action pair can be sampled at any time. Model-based algorithms for Markov games have been widely studied. The work of \cite{zhang2020model} studies a general setting where learning is used to estimate a model and a planning algorithm is applied to obtain the Nash equilibrium policy. Related model-based episodic algorithms that incorporate value iteration for two-player games include \cite{bai2020provable, xie2020learning} in the finite-horizon setting. The work of \cite{liu2021sharp} provides an episodic algorithm where at each iteration there is a planning step which performs an optimistic form of value iteration and there is a learning step where the outcomes of game play in the planning step are used to update the estimate of the model. 

 
\textbf{Policy-based methods for two-player games}
In our work, we study model-based learning, i.e., we learn a model followed by planning. An alternative is to directly learn the policy without learning the model. The work of \cite{daskalakis2020independent} provides a decentralized algorithm for policy gradient methods that converges to a min-max equilibrium when both players independently perform policy gradient. The work of \cite{zhao2022provably} obtains convergence guarantees of natural policy gradient in two-player zero-sum games. 

\textbf{Function approximation methods in multi-agent RL} The work of \cite{lagoudakis2012value} studies linear value function approximation in games where knowledge of the model is assumed. Furthermore, \cite{xie2020learning} studies multi-agent games in linear Markov games, but they only study value iteration in finite-horizon settings. Additionally, the work of \cite{jin2022power} studies episodic learning in multi-agent Markov games with general function approximation. 

\textbf{Planning Oracles For Learning In MDPs} The works of \cite{gheshlaghi2013minimax, agarwal2020model, li2020breaking, jin2020reward} study reinforcement learning in a single agent setting where a planning oracle is used to obtain convergence guarantees. 
\section{Model} \label{section C}
Consider a two-player simultaneous-action zero-sum discounted Markov game. As mentioned in the introduction, we first consider the planning component of a model-based learning algorithm and later make connection to learning. In the planning setting, the probability transition matrices and reward functions are assumed to be known to both players. The game is characterized by $(\scriptS, \scriptU, \scriptV, P, g, \alpha)$. We denote by $\scriptS$ the finite state space and $|\scriptS|$ the size of the state space. With slight abuse of notation, we say that $\scriptU$ is the finite action space  for the first player (the maximizer) where $|\scriptU|$ denotes the size of the action space for the maximizer. We call $\scriptU(s)$ the set of possible actions at state $s$ where $\scriptU = \cup_{s \in \scriptS} \scriptU(s)$.  Similarly, we call $\scriptV$ the finite action space for the second player (the minimizer) where $|\scriptV|$ is the size of the action space for the minimizer. We denote by $\scriptV(s)$ is the action space at state $s$ where $\scriptV = \cup_{s \in \scriptS} \scriptV(s)$.  We define $P$ as the probability transition kernel where $P(s'|s,u,v)$ is the probability of transitioning from state $s \in \scriptS$ to state $s' \in \scriptS$ when the maximizer takes action $u \in \scriptU (s)$ and the minimizer takes action $v \in \scriptV (s)$. We say that $g: \mathbb{R}^{|\scriptS|\times |\scriptU| \times |\scriptV|} \to [0,1]$ is the reward function (where, for the sake of completeness, we define $g(s, u, v) :=0$ for $u \notin \scriptU(s)$ or $v \notin \scriptV(s)$).

At each time instant $i$, the state of the game is $s_i$ and the maximizer takes action $u_i$ while the minimizer takes action $v_i$. We assume that the players take actions $u_i$ and $v_i$ simultaneously and remark that the setting where the players take moves sequentially, i.e., the setting of turn-based Markov games, is a special case of our setting of simultaneous moves. The maximizer incurs a reward of $g(s_i, u_i, v_i)$ where we assume without loss of generality that $g(s_i,u_i,v_i) \in [0,1]$ while the minimizer incurs a cost of $g(s_i, u_i, v_i)$ (and, hence a reward of $-g(s_i, u_i, v_i)$). By the end of the game, from the perspective of the maximizer, the game receives a discounted sum of the rewards with discount factor $\alpha$ where $0<\alpha<1$, i.e., $\sum_{i=0}^\infty \alpha^i g(s_i, u_i,v_i).$ Meanwhile, from the perspective of the minimizer, the game incurs a discounted sum of the costs, i.e., $\sum_{i=0}^\infty \alpha^i g(s_i, u_i,v_i).$ The objective of maximizer is to take actions $u_i$ to maximize $\sum_{i=0}^\infty \alpha^i g(s_i, u_i,v_i)$ while the objective of the minimizer is to minimize $\sum_{i=0}^\infty \alpha^i g(s_i, u_i,v_i).$ 


We call a mapping from states to distributions over actions a \textit{policy}, $(\mu,\nu)$. With slight abuse of notation, at each instance $i$, at state $s_i$, the action $u_i$ is selected following a randomized policy $\mu(s_i)$ where $\mu(s_i) \in \Delta(\scriptU(s_i))$ and $\Delta(\scriptU(s_i))$ denotes the set of distributions over actions in $\scriptU(s_i)$. Similarly, at instance $i$, the action $v_i$ is selected following a randomized policy $\nu(s_i)$ where $\nu(s_i) \in \Delta(\scriptV(s_i)).$ 

Given a policy $(\mu,\nu),$ we define the \textit{value function} corresponding to the policy component-wise as follows:$$
    J^{\mu,\nu}(s) = E_{P,\mu,\nu}\Big[\sum_{i=0}^\infty \alpha^i g(s_i, u_i,v_i)|s_0 = s\Big].$$
A pair of policies $(\mu^*, \nu^*)$ is a Nash equilibrium if they satisfy
    $J^{\mu,\nu^*}\leq J^{\mu^*,\nu^*}\leq J^{\mu^*,\nu}
$ for all policies $(\mu,\nu)$. 
It has been shown in \cite{Shapley} that such a Nash equilibrium policy exists for all two-player discounted zero-sum Markov games. We define the value function of the game to be $J^{\mu^*,\nu^*}$ and we will denote it by $J^*$ for convenience.

\section{Preliminaries}\label{prelim}
Consider any policy $(\mu,\nu)$. We will define the probability transition matrix $P_{\mu,\nu} \in \mathbb{R}^{|\scriptS|\times|\scriptS|}$ component-wise where
\begin{align*}
P_{\mu,\nu}(s,s') = \sum_{u \in \scriptU(s)} \sum_{v \in \scriptV(s)} \mu(u)\nu(v) P(s'|s, u,v) \forall (s,s')\in \mathbb{R}^{|\scriptS|\times|\scriptS|}.
\end{align*}
We define the reward function corresponding to policy $(\mu,\nu)$ as $g_{\mu,\nu} \in \mathbb{R}^{|\scriptS|},$ where 
$$g_{\mu,\nu}(s) = \sum_{u \in \scriptU(s)} \sum_{v \in \scriptV(s)} \mu(u)\nu(v) g(s,u,v) \forall s \in \scriptS.$$
Using $g_{\mu,\nu}$ and $P_{\mu,\nu}$, we define the Bellman operator corresponding to policy $(\mu,\nu)$, $T_{\mu,\nu}:\mathbb{R}^{|\scriptS|} \to \mathbb{R}^{|\scriptS|},$ component-wise as follows:
$$T_{\mu,\nu}V(s) := g_{\mu,\nu}(s)+\alpha P_{\mu,\nu} V (s).$$
If operator $T_{\mu,\nu}$ is applied $m$ times to vector $V \in \mathbb{R}^{|\scriptS|},$ then we say that we have performed an $m$-step rollout of the policy $(\mu,\nu)$ and the result $T^m_{\mu,\nu} V$ of the rollout is called the return.
It is well known that $$\norm{T_{\mu,\nu} V - J^{\mu,\nu}}_\infty \leq \alpha \norm{V-J^{\mu,\nu}}_\infty,$$ hence, iteratively applying $T_{\mu,\nu}$ yields convergence to $J^{\mu,\nu}.$ 

We will now give a few well-known properties of the $T_{\mu, \nu}$ operator. First, $T_{\mu, \nu}$ is monotone, i.e., $$V \leq V' \implies T_{\mu, \nu}V \leq T_{\mu, \nu} V'.$$ Herein, we imply that all inequalities hold element-wise. Second, consider the vector $e \in \mathbb{R}^{|\scriptS|}$ where $e(i) = 1 \forall i \in 1, 2, \ldots, |\scriptS|.$ We have that $$T_{\mu, \nu}(V + ce) = T_{\mu, \nu}V + \alpha ce \forall c \in \mathbb{R}.$$

With some algebra, it is easy to see that $T_{\mu,\nu}V$ can also be written component-wise as:
\begin{align}   T_{\mu,\nu}V(s) = \mu(s)^\top A_{V,s} \nu(s) \forall s \in \scriptS, \label{eq: bell mu nu}
\end{align}
where $A_{V,s} \in \mathbb{R}^{|\scriptU(s)|\times|\scriptV(s)|}$ is defined as follows: 
\begin{align}
 A_{V,s}(u,v) := g(s, u, v)+ \alpha \sum_{s' \in \scriptR(s, u, v)} P(s'|s, u,v)V(s'), \label{eq: As}
\end{align} for all $(u,v)  \in (\scriptU(s)\times \scriptV(s))$ where $\scriptR(s, u, v)$ is the set of states for which $P(s'|s,u,v) \neq 0$, i.e., the states that are ``reachable'' from $s$ when taking actions $u$ and $v$. Note that the size of $\scriptR(s,u,v)$ for any $(s, u,v)$ is typically much smaller than the size of the state space. Thus, in order to compute the $m$-step rollout for policy $(\mu,\nu)$ corresponding to vector $V\in \mathbb{R}^{|\scriptS|}$, one can iteratively perform the operations in \eqref{eq: bell mu nu} for all states $s \in \scriptS$ to apply the Bellman operator $T_{\mu,\nu}$ $m$ times.

We define the Bellman optimality operator or Bellman operator $T: \mathbb{R}^{|\scriptS|} \to \mathbb{R}^{|\scriptS|}$ as $$TV = \max_\mu \min_\nu (T_{\mu,\nu} V).$$
We call the resulting $\argmax \argmin$ policy the ``greedy policy,'' i.e.,  $(\mu,\nu) \in \mathcal{G}(V)$ when $$(\mu,\nu) \in \argmax_\mu \argmin_\nu (T_{\mu,\nu} V).$$

Using the notation in \eqref{eq: bell mu nu}, it is easy to see that the Bellman operator at each state $s \in \scriptS$ solves the following matrix game:
\begin{align}TV(s) = \displaystyle\max_{\substack{\mu(s)\in \mathbb{R}^{|\scriptU(s)|} \\  \mu(s)^\top  \textbf{1} =1\\0\leq \mu(s) \leq 1  }} \displaystyle\min_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s)|} \\ \nu(s)^\top  \textbf{1} =1\\0\leq \nu(s) \leq 1  }} \mu(s)^\top A_{V,s} \nu(s), \label{eq: bellman op}\end{align}
where $A_{V,s}$ is defined for all $s \in \scriptS$ in \eqref{eq: As} and $\textbf{1}$ the column vector of all 1s. Note that the inequalities $0\leq \mu(s)\leq 1$ and $0\leq \nu(s)\leq 1$ are defined to be component-wise. We define the \textit{greedy policy} $(\mu,\nu)$ corresponding to vector $V$ component-wise where $(\mu(s),\nu(s))$ is the $\argmin \argmax$ in \eqref{eq: bellman op} for all states $ s \in \scriptS$. We remark that the computation of the greedy policy can be obtained by solving a linear program \cite{rubinstein1999experience}. 
Additionally, it is known that $T$ is a pseudo-contraction towards the Nash equilibrium $J^*$ where $$\norm{TJ - J^*}_\infty \leq \alpha \norm{J-J^*}_\infty,$$ hence, iteratively applying $T$ yields convergence to the Nash equilibrium $J^*$ \citep{bertsekastsitsiklis}.

If operator $T$ is applied $H$ times to vector $V \in \mathbb{R}^{|\scriptS|},$ we say that the result, $T^H V$, is the $H$-step ``lookahead'' corresponding to $V$. We call $\scriptL(V)$ the greedy policy corresponding to $T^H V$ the $H$-step lookahead policy, or the lookahead policy, when $H$ is understood. In other words, given an estimate $V$ of the Nash equilibrium, the lookahead policy is the policy $(\mu,\nu)$ such that $$T_{\mu,\nu}(T^{H-1} J)=T(T^{H-1} V).$$ In order to compute the lookahead corresponding to $V \in \mathbb{R}^{|\scriptS|}$ (and the lookahead policy), one can iteratively perform the operations in \eqref{eq: bellman op} for all states $H$ times, obtaining the lookahead policy for each state $s$ by taking the $(\mu(s),\nu(s))$ corresponding to the $\argmax \argmin$  policy in \eqref{eq: bellman op} at the $H$-th iteration of applying $T$. 

Finally, we define the operator $T_{\mu}:\mathbb{R}^{|\scriptS|} \to \mathbb{R}^{|\scriptS|}$ as follows:
\begin{align}
    T_{\mu}V = \min_\nu (T_{\mu,\nu} V). \label{eq:Tmu}
\end{align}
It is known that $T_\mu$ is a maximum norm contraction with discount factor $\alpha$ and with respect to $J^{\mu} \in \mathbb{R}^{|\scriptS|}$ defined as
$J^{\mu} = \min_\nu J^{\mu,\nu},$ i.e.,
\begin{align*}
    \norm{T_{\mu} V - J^{\mu}}_\infty \leq \alpha \norm{V-J^{\mu}}_\infty \forall \text{ } V \in \mathbb{R}^{|\scriptS|}.
\end{align*} Additionally, it is known that $T_{\mu}$ is a monotone operator, i.e., 
\begin{align*}
    J \leq J' \implies T_\mu J \leq T_\mu J'.
\end{align*}

\subsection{Motivation}
%This algorithm is a natural extension of single player MDPs. But, it has these limitations. So this is the motivation of our work 
The extension of policy iteration to Markov games is given by the following:
\begin{align} 
\nonumber
(\mu_{k+1},\nu_{k+1}) &= \mathcal{G}(V_k) \\
V_{k+1} &= J^{\mu_{k+1}} = T_{\mu_{k+1}}^\infty V_k. \label{eq:PIgames}
\end{align}
A well-known variant of this algorithm involves using an $m$-step return to estimate $J^{\mu_{k+1}}$, i.e., setting $V_{k+1} = T_{\mu_{k+1}}^m V_k.$ 

Notice that in the policy evaluation step, the policy $\mu_{k+1}$ is fixed, $\min_{\nu} J^{\mu_{k+1},\nu}$ is evaluated, and the estimate of the value function is updated to be $\min_{\nu} J^{\mu_{k+1},\nu}$. Note that obtaining $\min_{\nu} J^{\mu_{k+1},\nu}$ requires that an MDP be solved (this is because, since the policy of the maximizer $\mu$ is fixed, only one player, the minimizer, needs to take actions at every iteration to minimize the expected discounted sum of rewards). Hence, while the algorithm converges, in the policy evaluation step of the games setting, unlike the MDP setting, where computing $J^{\nu_{k+1}}$ for the greedy policy $\nu_{k+1}$ can be obtained by inverting a matrix, an MDP must be solved, which makes the algorithm highly inefficient and potentially infeasible. Thus, this  motivates our current work, which is to find an efficient policy iteration algorithm for Markov games. 

\section{Convergence Of Generalized Policy Iteration For Markov Games}
Convergence of a computationally efficient extension of policy iteration for single player systems to two-player games is an open problem \cite{bertsekas2021distributed, patekthesis}. The policy iteration algorithm we consider is given by the following:
\begin{align*}
    \mu_{k+1},\nu_{k+1} &= \scriptL(V_k), \\
    V_{k+1} &= T_{\mu_{k+1},\nu_{k+1}}^m T^{H-1} V_k,
\end{align*} where $\scriptL$ is an $H$-step lookahead policy. Formally, our generalized policy iteration algorithm for two-player games is outlined in Algorithm \ref{alg:alg 2}. %Note that $T^{H-1} V_k$  is computed as a byproduct of determining $\mu_{k+1},\nu_{k+1}.$ 

%\paragraph{Our Algorithm}
%\begin{algorithm}
%\caption{Generalized Policy Iteration For Two-Player Zero-Sum Discounted Markov Games}\label{alg:alg 1}
%\textbf{Input}: $V,m, H.$
%\item  For $k=0, 1, \ldots$ 
%\item \quad For $i = 0, 1, \ldots, H$ 
%\item \quad \quad   Compute $A_s \in \mathbb{R}^{|\scriptU(s)|\times |\scriptV(s)|} \forall  s \in \scriptS$ where
%  $$A_s(u, v) \leftarrow r(s, u, v)+ \alpha \sum_{s'} P(s'|s, u,v)V(s') \forall (u, v) \in \mathbb{R}^{|\scriptU(s)|\times|\scriptV(s)|} $$ 
%          \item  \quad \quad \label{step 2 reg}$V(s) \leftarrow \displaystyle\max_{\substack{\mu(s)\in \mathbb{R}^{|\scriptU(s)|} \\  \mu(s)^\top  \textbf{1} =1\\0\leq \mu(s) \leq 1  }} \displaystyle\min_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s)|} \\ \nu(s)^\top  \textbf{1} =1\\0\leq \nu(s) \leq 1  }}\mu(s)^\top A_s \nu(s)$ for $s \in \scriptS$ 
%        \label{step 3 reg} 
%        \item  \quad   $\mu(s) \leftarrow \displaystyle\argmax_{\substack{\mu(s)\in \mathbb{R}^{|\scriptU(s)|} \\  \mu(s)^\top  \textbf{1} =1\\0\leq \mu(s) \leq 1  }} \displaystyle\min_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s)|} \\ \nu(s)^\top  \textbf{1} =1\\0\leq \nu(s) \leq 1  }}\mu(s)^\top A_s \nu(s)$ for $s \in \scriptS$ \\ \item  \quad   $\nu(s) \leftarrow  \displaystyle\argmin_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s)|} \\ \nu(s)^\top  \textbf{1} =1\\0\leq \nu(s) \leq 1  }} \displaystyle\max_{\substack{\mu(s)\in \mathbb{R}^{|\scriptU(s)|} \\  \mu(s)^\top  \textbf{1} =1\\0\leq \mu(s) \leq 1  }}\mu(s)^\top A_s \nu(s)$ for $s \in \scriptS$
%           \item \quad For $j = 0, 1, \ldots, m$
%            \item \quad \quad   Compute $A_s \in \mathbb{R}^{|\scriptU(s)|\times |\scriptV(s)|} \forall  s \in \scriptS$ where
%  $$A_s(u, v) \leftarrow r(s, u, v)+ \alpha \sum_{s'} P(s'|s, u,v)V(s') \forall (u, v) \in \mathbb{R}^{|\scriptU(s)|\times|\scriptV(s)|} $$ 
%          \item  \quad \quad $V(s) \leftarrow \mu(s)^\top A_s \nu(s)$ for $s \in \scriptS$ 
%\end{algorithm}
The algorithm is an iterative process that updates an estimate of the optimal value function at each iteration. At each iteration, there are two steps: the policy improvement step and the policy evaluation step. In the policy improvement step, a new policy to evaluate in the policy evaluation step is determined. The new policy is obtained by computing an $H$-step lookahead policy based on the estimate of the optimal value function.  
In other words, at iteration $k+1,$ the algorithm computes $(\mu_{k+1},\nu_{k+1})$ such that 
\begin{align}
T^H V_k(s) = T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k(s) \forall s \in \scriptS
\label{eq:bellman}
\end{align} by solving the linear program in \eqref{eq: bellman op} for all states $s \in \scriptS$ $H$ times. Note that $T^{H-1}V_k$ is computed as a byproduct of determining the lookahead policy, so the estimate of the value function is updated to be $T^{H-1}V_k.$ 

\begin{remark} The use of lookahead policies in the policy improvement step has been used in empirically successful algorithms such as AlphaZero. In practice, lookahead can be implemented efficiently using techniques such as Monte Carlo Tree Search (MCTS).
\hfill $\diamond$
\end{remark}
The policy evaluation step involves  applying the operator $T_{\mu_{k+1},\nu_{k+1}}$ $m$ times to the updated estimate of the optimal value function $\tilde{V}_k = T^{H-1}V_k$ to estimate $J^{\mu_{k+1},\nu_{k+1}}.$ Recall that iteratively applying $T_{\mu_{k+1},\nu_{k+1}}$ to vector $\tilde{V}_k$ yields convergence to $J^{\mu_{k+1},\nu_{k+1}},$ and hence, when $m=\infty$, $V_{k+1} = J^{\mu_{k+1},\nu_{k+1}}.$
Put together, our algorithm can also be written as follows:
\begin{align}
V_{k+1} = T_{\mu_{k+1},\nu_{k+1}}^m T^{H-1}V_k. \label{eq:alt alg}
\end{align}
Note that to apply $T_{\mu_{k+1},\nu_{k+1}}$ to $\tilde{V}_k$ for each state $s \in \scriptS,$ one needs to perform the operations in \eqref{eq: bell mu nu}, and hence, to obtain $T_{\mu_{k+1},\nu_{k+1}}^m\tilde{V}_k,$ i.e., to generate the $m$-return, one needs to perform the computations for all states $s \in \scriptS$ $m$ times. 

We furthermore note that in the \textit{naive policy iteration algorithm} of Pollatschek and Avi-Itzhak, $H$ is set to $1,$ and we simply obtain the ``greedy policy.'' In other words, the naive policy iteration algorithm is given by
\begin{align} 
    \mu_{k+1},\nu_{k+1} \nonumber&= \mathcal{G}(V_k), \\
    V_{k+1} &= T_{\mu_{k+1},\nu_{k+1}}^m V_k,\label{eq:naive}
\end{align} where $\mathcal{G}$ denotes a 1-step greedy policy defined in Section \ref{prelim}. 
We also note that in some ways, our algorithm is a generalization of the naive policy iteration algorithm. 
\begin{algorithm} 
\caption{Generalized PI For Two-Player  Games}\label{alg:alg 2}
\textbf{Input}: $V_0,m, H.$\\ \\
 %[1] enables line numbers
For $k=1,2,\ldots$ \\
 \quad Let $\tilde{V}_k = T^{H-1}V_k$\\
 \quad \quad Let $\mu_{k+1},\nu_{k+1}$ be such that $$\mu_{k+1},\nu_{k+1} \in \argmax_{\mu} \argmin_{\nu} T_{\mu,\nu}\tilde{V}_k.$$\label{step 2 our}\\
 \quad \quad Approximate $J^{\mu_{k+1},\nu_{k+1}}$ as follows: $T_{\mu_{k+1}, \nu_{k+1}}^m \tilde{V}_k.$  \label{step 3 our}
 \\
\quad \quad $V_{k+1} = T_{\mu_{k+1}, \nu_{k+1}}^m \tilde{V}_k.$
\end{algorithm}

\begin{remark} We note that in the case of turn-based Markov games, which are Markov games where the players move sequentially instead of simultaneously, the computations in \eqref{eq: bellman op} that are used to determine the lookahead are far simplified and only involve taking either a maximum or a minimum instead of the $\min \max$ operation in \eqref{eq: bellman op}.
\hfill 
\end{remark}

The proof technique of policy iteration for reward-maximizing single player MDPs  hinges on showing that 
\begin{align}
   J^* \geq \ldots \geq J^{\mu_{k+1}} \geq J^{\mu_k} \geq \ldots \geq J^{\mu_0}.
\label{eq:cascade}
\end{align} 
Recall that policy iteration for single player MDPs is given by the following:
\begin{align*}
    V_{k+1} = J^{\mu_{k+1}}
\end{align*} where $T_{\mu_{k+1}} V_k = \tilde{T} V_k$ and $\tilde{T}$ is defined as follows: 
\begin{align}\tilde{T}V_k = \max_{\mu} T_{\mu} V_k. \label{eq:defT}
\end{align}
To show that $J^{\mu_{k+1}}\geq J^{\mu_k}$ and hence obtain \eqref{eq:cascade}, observe that the following holds:
\begin{align*}
   &T_{\mu_{k+1}} V_k =   \tilde{T} V_k = \tilde{T} J^{\mu_k} \geq J^{\mu_k},
\end{align*} 
where the inequality holds because
\begin{align*}
    \tilde{T} J^{\mu_k} = \max_{\mu} T_{\mu} J^{\mu_k} \geq  T_{\mu_k} J^{\mu_k} = J^{\mu_k}.
\end{align*}
Thus, 
\begin{align}
    T_{\mu_{k+1}} J^{\mu_k} \geq J^{\mu_k}.
\end{align}
Since $T_{\mu_{k+1}}$ is a monotone operator, we can repeatedly apply $T_{\mu_{k+1}}$ to obtain the following:
\begin{align*}
    J^{\mu_{k+1}} \geq \ldots \geq T_{\mu_{k+1}} J^{\mu_k} \geq J^{\mu_k}.
\end{align*}

Notice that the proof of policy iteration for single-player MDPs (as well as its extension to zero-sum Markov games given in \eqref{eq:PIgames}) hinges on the fact that $\tilde{T}J^{\mu_k} \geq J^{\mu_k}$, which naturally follows since $\tilde{T}$ defined in \eqref{eq:defT} involves a maximization over all policies $\mu$. 
In contrast, in the policy improvement step of Algorithm \ref{alg:alg 2} (as well as of naive policy iteration), the greedy policy corresponding to the bellman operator $T$ given in \eqref{eq: bellman op} involves both a maximization as well as a minimization,  i.e., $(\mu_{k+1}, \nu_{k+1}) = \argmax_{\mu} \argmin_{\nu} T_{\mu,\nu} \tilde{V}_k$. Thus, it is not necessarily true that $T_{\mu_{k+1},\nu_{k+1}} \tilde{V}_k \geq \tilde{V}_k.$ In fact, naive policy iteration has been shown to diverge in \cite{van1978discounted, condon1990algorithms}. As such, the question of how to modify the algorithm in \cite{pollatschek1969algorithms} to ensure convergence is an open question where the main challenge is how to overcome the lack of monotonicity in the policy improvement step.
In our algorithm, given in Algorithm \ref{alg:alg 2}, we introduce lookahead policies as opposed to traditionally used greedy policies and use several novel proof ideas to overcome the lack of monotonicity. We remark that other works in the MDP setting also use lookahead policies \cite{efroni2018, efroni2018multiple, efroni2019combine},  however, their arguments often rest on monotonicity arguments which cannot be easily extended in the games setting when the Bellman operator is used.
%\begin{remark}
%    We remark that lookahead policies  
%\end{remark}

%\begin{algorithm} 
%\caption{Generalized Policy Iteration For Games}\label{alg:alg 2}
%\textbf{Input}: $\mu_0.$\\
%\begin{algorithmic}[1] %[1] enables line numbers
%\STATE Let $k=0$.
%\STATE Let $\nu_{k+1}$ be such that $\nu_{k+1} \in \argmin_{\nu} T^m_{\mu_{k},\nu}J.$\\
%\STATE Let $\mu_{k+1}$ be such that $\mu_{k+1} \in \argmax_\mu T_{\mu}J^{\mu_k,\nu_{k+1}}.$ 
%\STATE Set $k \leftarrow k+1.$ Go to 2.
%\end{algorithmic}
%\end{algorithm}

%\begin{algorithm} 
%\SetAlgoLined
%\caption{Naive Policy Iteration (Algorithm of Pollatschek and Avi-Itzhak)}\label{alg:alg 4}
%\SetAlgoLined
%\textbf{Input}: $V_0.$\\ \\
% \For{ k=1,2,\ldots}{
%  Let $\mu_{k+1},\nu_{k+1}$ be such that $\mu_{k+1},\nu_{k+1} \in \argmax_{\mu} \argmin_{\nu} T_{\mu,\nu}V_k$.\\
% Set $V_{k+1}$ to be an approximation to $J^{\mu_{k+1},\nu_{k+1}}$, i.e.,  $V_{k+1} \leftarrow T^m_{\mu_{k+1},\nu_{k+1}}V_k$.  \label{step 3 pollatschek}}
%\end{algorithm}

\subsection{Main Result} We now state our main result, where we prove convergence of Algorithm \ref{alg:alg 2}, a policy iteration algorithm for stochastic games that does not involve solving any MDPs. Our main result hinges on the following assumption on the amount of lookahead in each iteration.

\begin{assumption} \label{assumption 1 games}
$\alpha^{H-1}+2(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}<1.$
\end{assumption} Assumption \ref{assumption 1 games} implies that the lookahead, $H$, must be sufficiently large and also that a large return $m$ can mitigate the amount of lookahead that is needed. We remark that taking steps of lookahead is used in practice such as in algorithms like AlphaZero and that efficient algorithms combined with sampling such as Monte Carlo Tree Search (MCTS) are often employed to perform the lookahead. We also note that the amount of lookahead, $H$, is a parameter of the algorithm, and hence, Assumption \ref{assumption 1 games} is not a restriction on the model, rather an assumption on the parameters of the algorithm. 

%Our main result is the following:

\begin{theorem}\label{thm:theorem 1}
Under Assumption \ref{assumption 1 games}, the following holds for the iterates of Algorithm \ref{alg:alg 2}:
\begin{align*}
&\norm{V_k-J^*}_\infty  \\&\leq  \Big(\alpha^{H-1}+(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha)\Big)^k\norm{V_0 - J^*}_\infty.
\end{align*}
Taking limits, we can see that
$
V_k \to J^*,
$ where $V_k$ are the iterates of Algorithm \ref{alg:alg 2} and $J^*$ is the value function.
\hfill $\diamond$
\end{theorem}


The proof of Theorem \ref{thm:theorem 1} can be found in the Appendix. 


\begin{remark}(Implications Of Theorem \ref{thm:theorem 1})
We outline the significance of Theorem \ref{thm:theorem 1} as follows:
\begin{itemize}
    \item To the best of our knowledge, our algorithm is the first variant of the well-studied naive policy iteration algorithm that converges without restrictive conditions on the model or additional storage. 
    \item Unlike the algorithm in \eqref{eq:PIgames} studied in \cite{patekthesis, perolat2015approximate}, our algorithm does not require that any MDPs be solved at each iteration. 
    \item We remark that the value iteration algorithm for Markov games is a special case of our algorithm, where value iteration for Markov games is given by the following:
    \begin{align*}
        V_{k+1} = T V_k,
    \end{align*} where $T$ is defined in Section \ref{prelim}.
\end{itemize}
\end{remark}

\textbf{Proof Idea} 
Our proof can be found in the Appendix. Our proof techniques do not involve monotonicity and instead hinge on a contraction property towards the Nash equilibrium of the operator $T_{m,H}:\mathbb{R}^{|\scriptS|} \to \mathbb{R}^{|\scriptS|}$  defined as follows:
$$
   T_{m,H}V = T^m_{\mu,\nu}T^{H-1}V,
$$ where $T_{\mu,\nu}(T^{H-1} V)=T(T^{H-1} V).$
Using $T_{m,H}$, our algorithm in \eqref{eq:alt alg} can be written as follows:
\begin{align*}
    V_{k+1} = T_{m,H}(V_k).
\end{align*}
We use the contraction property of this operator in our proofs instead of monotonicity properties used in analyses of policy iteration in single player MDPs \citep{bertsekas2019reinforcement, efroni2019combine}. Doing so allows us to bypass monotonicity complications in games arising from simultaneous minimization and maximization in determining $(\mu_{k+1},\nu_{k+1})$ in \eqref{eq:bellman} as opposed to a single player MDP where actions are taken either to minimize or to maximize, but never to do both at the same time. 
If the lookahead policy $(\mu_{k+1},\nu_{k+1})$ in \eqref{eq:bellman} involved taking only a maximum or a minimum (instead of a joint $\max \min$), one can use monotonicity techniques of \cite{bertsekastsitsiklis, efroni2019combine}, but the main challenge for us is the joint $\max \min$ in the lookahead policy. The key to our proofs lies in the fact that, with sufficient lookahead, the operator $T_{m,H}$ is a contraction. %Note that while the operator $T$ is a contraction, when we consider the operator $T_{\mu(V),\nu(V)},$ $(\mu,\nu)$ depends on $V$ because $(\mu,\nu)$ is the lookahead policy with respect to $V.$ Therefore, it is not obvious if $||T_{\mu(V_1),\nu(V_1)}^m T^{H-1}V_1-T_{\mu(V_2),\nu(V_2)}^m T^{H-1}V_2||_\infty$ is smaller than $||V_1-V_2||_\infty.$ 

In very large systems, function approximation techniques are necessary because of the massive sizes of state spaces. As such, we consider the well-known linear MDP model \cite{Agarwal2019ReinforcementLT} extended to games. We will show that the computations and storage required to determine the Nash equilibrium using Algorithm \eqref{alg:alg 2} in the linear MDP case depends only on the dimension of the feature vectors and not on the size of the state space. 

\section{Linear Value Function Approximation}\label{LinearMDPs}
When the state and actions spaces are very large, function approximation is often necessary. The work of \cite{perolat2015approximate} provides error bounds for the function approximation setting of the algorithm in \eqref{eq:PIgames}. However, even with function approximation, all policies $J^{\mu_{k+1},\nu}$ for all $\nu$ must be evaluated which is inefficient. Convergence difficulties with the naive policy iteration algorithm \cite{van1978discounted} and its variants implies that their extensions to function approximation, such as \cite{perolat2016softened}, suffers from the same difficulty with convergence.  

In the prior section, we assumed that for all states $s \in \scriptS,
V_{k+1}(s)= T_{\mu_{k+1},\nu_{k+1}}^m T^{H-1}V_k(s)$ could be computed. In the case of very large state spaces, function approximation techniques are often employed where $T_{\mu_{k+1},\nu_{k+1}}^m T^{H-1}V_k(s)$ is computed for a fixed subset of states, say, $\scriptD$.
In order to estimate the value function for states not in $\scriptD$, we associate with each state $i \in \scriptS$  a feature vector $\phi(i)\in \mathbb{R}^d$ where typically $d << |\scriptS|$. The matrix comprised of the feature vectors as rows is denoted by $\Phi$. We use those estimates to find the best fitting $\theta \in \mathbb{R}^d$, i.e., 
\begin{align}
    \min_\theta \sum_{s \in D} \Big( (\Phi \theta)(s) - T_{\mu_{k+1},\nu_{k+1}}^m T^{H-1}V_k(s) \Big)^2. \label{eq:find theta}
\end{align} 
The solution to the above minimization problem is denoted by $\theta_{k+1}$. The algorithm then uses $\theta_{k+1}$ to obtain $V_{k+1} = \Phi \theta_{k+1}$. The process then repeats. 
We denote the matrix of feature vectors in $\scriptD$ as rows $\Phi_\scriptD$ and we assume that the rank of $\Phi_\scriptD$ is $d$, i.e., $\Phi_\scriptD$ is full rank. Thus, setting $$\hat{J}^{\mu_{k+1},\nu_{k+1}} := T_{\mu_{k+1},\nu_{k+1}}^m T^{H-1}V_k,$$ we can alternatively rewrite our $\Phi \theta_k$ as follows:
\begin{align}
\Phi \theta_k = \underbrace{\Phi (\Phi_\scriptD^\top \Phi_\scriptD)^{-1}\Phi_\scriptD^\top \scriptP_k}_{=: \scriptM} \hat{J}^{\mu_{k+1},\nu_{k+1}}, \label{eq:use theta}
\end{align} where $\scriptP_k$ is a projection matrix of ones and zeros such that $\scriptP_k \hat{J}^{\mu_{k+1},\nu_{k+1}}$ is a vector whose elements are a subset of the elements in $ \hat{J}^{\mu_{k+1},\nu_{k+1}}$ corresponding to $\scriptD.$ The algorithm is summarized in Algorithm \ref{alg:alg 4 MG}.

\begin{algorithm} 
\caption{Least-Squares Function Approximation Policy Iteration For Markov Games With Lookahead}\label{alg:alg 4 MG}
\textbf{Input}: $\theta_0, m,$ $H,$ feature vectors $\{ \phi(i) \}_{i \in \scriptS}, \phi(i) \in \mathbb{R}^d$  and subset $\scriptD \subseteq \scriptS.$ Here $\scriptD$ is the set of states at which we evaluate the current policy at iteration $k.$
\\\begin{algorithmic}[1] %[1] enables line numbers
\STATE Let $k=0$.
\STATE Let ${\mu_{k+1},\nu_{k+1}},\nu_{k+1}$ be such that $${\mu_{k+1},\nu_{k+1}},\nu_{k+1} \in \argmax_{\mu} \argmin_{\nu} T_{\mu,\nu}T^{H-1}\Phi \theta_k,$$ where the $T$ operator is the Bellman operator.\\
\STATE Compute $\hat{J}^{{\mu_{k+1},\nu_{k+1}}}(i) = T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1} (\Phi \theta_k)(i)$ for $i \in \scriptD.$ \\ \label{step 3 alg}
\STATE Choose $\theta_{k+1}$ to solve 
\begin{align}
    \min_\theta \sum_{s \in D} \Big( (\Phi \theta)(s) - \hat{J}^{{\mu_{k+1},\nu_{k+1}}}(s) \Big)^2, \label{step 4 alg}
\end{align} 
where $\Phi$ is a matrix whose rows are the feature vectors.
\STATE Set $k \leftarrow k+1.$ Go to 2.
\end{algorithmic}
\end{algorithm} 
We now present our first main result on the convergence of Algorithm \ref{alg:alg 4 MG}. To do so, we give our first assumption:
\begin{assumption}\label{assume 1 MG LFA}
We make the following assumption about the amount of lookahead $H$ and the parameter $m$:
$$\alpha^{H-1}+2(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}<1.$$
\end{assumption}
\begin{theorem} \label{theorem 1 MG}
Almost surely, under Assumption \ref{assume 1 MG LFA}, the following bound holds for iterates $\theta_k$ of Algorithm \ref{alg:alg 4 MG}:
\begin{align*}
     \limsup_{k\to \infty} \norm{\Phi \theta_k - J^*}_\infty \leq  \frac{\delta_{app}}{1-2\alpha^{H-1}-\delta_{FV} \alpha^{m+H-1}}
\end{align*}
where $\delta_{app}$ is ability of the feature vectors to approximate the policies:
\begin{align*}
    \delta_{app} := \sup_k \norm{J^{\mu_k} - \scriptM  J^{\mu_k}}_\infty 
    \end{align*}
    and \begin{align*}
\delta_{FV} :=\norm{\scriptM}_\infty
\end{align*}
is a parameter that depends on the feature vectors. 
\end{theorem}
This result was previously published as an invited session paper at IEEE Conference on Decision and Control at the
Marina Bay Sands, Singapore, in 2023.

\begin{remark}\textbf{Interpretation of Theorem \ref{theorem 1 MG}}
Theorem \ref{theorem 1 MG} shows that the performance bounds are mostly determined by the ability of the feature vectors to represent value functions corresponding to policies. However, the performance bounds can also be improved with judicious choice of feature vectors, since the bounds are also dependent on $\delta_{FV}$. In the special case where there is no function approximation error with the feature vectors, the algorithm has no error. 
\end{remark}
\subsection{Linear MDPs}
We now consider a special case of function approximation, which is the natural extension of linear MDPs to games. We are motivated by the fact that recent works including \cite{agarwal2020flambe,uehara2021representation,zhang2022making} have shown that one can learn linear representations of MDPs efficiently. 

As is standard in the literature on linear MDPs, we assume a model of the following form: 
\begin{align}
g(s,u,v) = \phi(s,u,v)\cdot \theta, \quad P(\cdot|s,u,v) = \phi(s,u,v)\cdot\eta, \label{eq: lin mdp}
\end{align} where each $(s,u,v) \in \scriptS\times \scriptU \times \scriptV$ is associated with a feature vector $\phi(s,u,v) \in \mathbb{R}^d$ where typically $d << |\scriptS|$. We assume $\theta \in \mathbb{R}^d$ and $\eta \in \mathbb{R}^{|\scriptS|\times d}.$ 
These assumptions are described in more detail in \cite{Agarwal2019ReinforcementLT}. We additionally assume that there exists a set of state-actions tuples $\scriptD$ where $\sum_{(s, u, v) \in \scriptD} \phi(s,u,v)\phi(s,u,v)^\top$ is full rank.

We will show that the computations required to converge to $J^*$ in the case of linear MDPs shows that the number of computations does not depend on the sizes of the state and actions spaces, unlike Algorithms \ref{alg:alg 2} and \ref{alg:alg 4 MG}.

In Algorithm \ref{alg:alg 2}, at iteration $k+1$, the algorithm computes $(\mu_{k+1},\nu_{k+1})$ in \eqref{eq:bellman} by solving the linear program in \eqref{eq: bellman op} for all states $s \in \scriptS$ $H$ times. Then, the algorithm approximates $J^{\mu_{k+1},\nu_{k+1}}$ by applying $T_{\mu_{k+1},\nu_{k+1}}$ $m$ times for each state $s$ where computing $T_{\mu_{k+1},\nu_{k+1}}V(s)$ for any vector $V$ and each state $s$ requires performing the operations in \eqref{eq: bell mu nu}. Hence, the computations required in a single iteration of Algorithm \ref{alg:alg 2} is at least $\mathcal{O}(Hm|\scriptS|^2|\scriptU|^2|\scriptV|^2),$ which is infeasible when the state and action spaces are very large. 

We now show how to obtain $V_{k+1}$ in a way that does not depend on the sizes of state and action spaces. In the case of linear MDPs, any vector $V \in \mathbb{R}^{|\scriptS|}$ can be parameterized by some $\beta \in \mathbb{R}^d$ as follows. Consider any state-actions tuple $(s, u, v)  \in \scriptS\times \scriptU \times \scriptV$. Then, it can be easily shown that $A_{V,s}$ defined in \eqref{eq: As} can be written in the following form:
\begin{align}
    A_{V,s}(u,v) = \phi(s, u, v)^\top \beta \label{eq: AVs}
\end{align} for some $\beta$ \cite{Agarwal2019ReinforcementLT}.

Under this parameterization, we will show how to obtain $\beta'$ corresponding to $T_{\mu,\nu} V$ from $\beta$ corresponding to $V$ for any $V \in \mathbb{R}^{|\scriptS|}$ in a way that the number of computations does not depend on the size of the state space. We will then extend the result to obtain $\beta'$ corresponding to $TV$. First, consider a set of state-actions tuples $\scriptD$ where $\sum_{(s, u, v) \in \scriptD} \phi(s,u,v)\phi(s,u,v)^\top$ is full rank. For $(s,u,v) \in \scriptD$, we can directly compute $A_{T_{\mu,\nu}V,s}(u,v)$, i.e.,
\begin{align}
    \nonumber A_{T_{\mu,\nu}V,s}(u,v) \nonumber \nonumber&= g(s, u, v)+ \alpha \sum_{s' \in \scriptR(s, u, v)} P(s'|s, u,v){T_{\mu,\nu}V,s}(s')\\
    &= g(s, u, v)+ \alpha \sum_{s' \in \scriptR(s, u, v)} P(s'|s, u,v) (\mu(s')^\top A_{V,s'}\nu(s')),\label{eq:A fn ap}
\end{align} where $\scriptR(s,u,v)$ denotes the set of states reachable from $(s,u,v)$ and $A_{V,s'}$ can be constructed using \eqref{eq: AVs}. In many real world examples, say, games such as Atari, this set of states is rather small since only a few states can be reached from any given state regardless of the actions that are taken.

Then, using the resulting $A_{T_{\mu,\nu}V,s}(u,v)$ for $(s,u,v) \in \scriptD$, we can easily obtain an appropriate $\beta'$ by performing a least squares minimization. More precisely, since $$A_{T_{\mu,\nu}V,s}(u,v) = \phi(s,u,v)^\top \beta' \forall (s,u,v) \in \scriptS\times\scriptU(s)\times \scriptV(s),$$ it is sufficient to compute $A_{T_{\mu,\nu}V,s}(u,v)$ for $(s, u, v) \in \scriptD$ and perform a least squares minimization to determine $\beta'$,i.e., 
\begin{align}
    \beta' := \argmin_{(s,u,v)} \sum_{(s, u, v)\in \scriptD}(A_{T_{\mu,\nu}V,s}(u,v) - \phi(s,u,v)^\top \beta')^2.
\end{align} Recall that a unique minimizer exists because of the full rank condition in the definition of $\scriptD.$

The above minimization produces the weight vector $\beta'$ associated with $T_{\mu,\nu}V.$ In a similar manner, one can also obtain the weight vector corresponding to $TV$. The only modification is that instead of computing $\mu(s)^\top A_{TV,s} \nu(s)$, one needs to instead obtain $\min_{\mu} \max_{\nu} \mu(s)^\top A_{T_{\mu,\nu} V,s} \nu(s)$.

Put together, the above shows that in order to obtain a sequence of $\beta_k$ that parameterize the sequence $V_k$ in Algorithm \ref{alg:alg 2}, it is only necessary to perform the sequence of computations described above which do not depend on the size of the state space. 

\begin{remark} We note that the computations are further simplified in the case of turn-based Markov games, where players take turns to execute actions in alternating time steps. This is because in the setting of turn-based MDPs, only one player at a time is performing either a maximization or a minimization.
\hfill \end{remark}

We summarize the above discussion with the following Proposition.
\begin{proposition}
In general, $H |\scriptS|$ matrix games are required to be solved at each iteration of Algorithm \ref{alg:alg 2}. However, in the special case of linear MDPs, the computations of Algorithm \ref{alg:alg 2} require only that $H \sum_{(s,u,v)\in \scriptD} |\scriptR(s,u,v)|$ matrix games be solved at each iteration. Additionally, the other computations required to obtain the Nash equilibrium do not depend on the sizes of the state and action spaces.
\hfill $\diamond$
\end{proposition}
\section{Learning Value Functions}
Note that in the previous section, we assume that exact estimates of $\hat{J}^{{\mu_{k+1},\nu_{k+1}}}(s) = T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1} (\Phi \theta_k)(s)$ for $s \in \scriptD$ are available at each iteration. However, this is not possible in general. On the other hand, by observing a single trajectory of (state, action, reward) triplets under the policy $(\mu_{k+1},\nu_{k+1}),$ it would be possible to obtain an unbaised estimate of  $T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1} (\Phi \theta_k)(i)+w_{k+1}(i)$; see \cite{winnicki2023convergence} for details in a single-player setting. Such an unbiased estimate can be denoted by $T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1} (\Phi \theta_k)(i)+w_{k+1}(i)$, where $w_{k+}$ is a zero-mean noise term. Since the value function estimate is noisy, we have to incorporate stochastic approximation techniques to be able to combine noisy policy evaluation with the general policy iteration algorithm described in the previous sections. Our proposed algorithm is described in Algorithm \ref{alg:SA MG}.
\begin{algorithm}
\caption{Least Squares Function Approximation For Policy Iteration In Markov Games With Unbiased Noise and Lookahead}
\label{alg:SA MG}
\textbf{Input}: $\theta_0,m, H$ feature vectors $\{ \phi(i) \}_{i \in \scriptS}, \phi(i) \in \mathbb{R}^d$  and subsets $\scriptD_k  \subseteq \scriptS, k = 0, 1, \ldots.$ Here $\scriptD_k$ is the set of states visited by a trajectory corresponding to the current policy at iteration $k.$\\
\begin{algorithmic}[1] %[1] enables line numbers
\STATE Let $k=0$.
\STATE Let ${\mu_{k+1},\nu_{k+1}}$ be such that $$\norm{T_{{\mu_{k+1},\nu_{k+1}}}T^{H-1}\Phi \theta_k - T^H \Phi \theta_k}_\infty \leq \eps_{LA}.$$\\
\STATE    Compute $$\hat{J}^{{\mu_{k+1},\nu_{k+1}}}(i) =  T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}\Phi \theta_k(i)+w_{k}(i)$$ for $i \in \scriptD_k$ and set $\hat{J}^{{\mu_{k+1},\nu_{k+1}}}(i)=0$ for $i \notin \scriptD_k.$ \\ 
\STATE Choose $\theta_{k+1}$ to solve 
\begin{align}
    &\min_\theta \norm{(\scriptP_{1, k}\Phi) \theta - \scriptP_{2, k}\hat{J}^{{\mu_{k+1},\nu_{k+1}}}}_2, 
\end{align} \\
where $\Phi$ is a matrix whose rows are the feature vectors (see previous page for definitions of $\scriptP_{1, k}$ and $\scriptP_{2, k}$). To compute $\theta_{k+1}$, use the Moore-Penrose inverse of $\scriptP_{1, k}\Phi.$
\STATE \begin{align}
    \theta_{k+1} = (1-\gamma_k)\theta_k + \gamma_k (\theta_{k+1}). 
\end{align}
\STATE Set $k \leftarrow k+1.$ Go to 2.
\end{algorithmic}
\end{algorithm}

Defining $$V_k := \Phi \theta_k,$$ the iterates in Algorithm \ref{alg:SA MG} can be written as follows:
\begin{align}
    V_{k+1}&= (1-\gamma_k)V_k + \gamma_k (\Phi \theta_{k+1})\nonumber\\&= (1-\gamma_k)V_k + \gamma_k (\Phi(\scriptP_{1,k}\Phi)^+ \hat{J}^{{\mu_{k+1},\nu_{k+1}}})\nonumber\\&\nonumber= (1-\gamma_k)V_k \\&+ \gamma_k (\underbrace{\Phi(\scriptP_{1,k}\Phi)^+ \scriptP_{2,k}}_{=: \scriptM_k} (T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k+w_k)),\label{eq: iter alg 3 unbiased full}
\end{align}
where $(\scriptP_{1,k}\Phi)^+$ is the Moore-Penrose inverse of $\scriptP_{1,k}\Phi$ and $\scriptP_{1,k}$ is a matrix of zeros and ones such that rows of $\scriptP_{1,k}\Phi$ correspond to feature vectors associated with states in $\scriptD_k$ and $\scriptP_{2,k}(T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k+w_{k})$ is a vector whose elements are a subset of the elements of $\hat{J}^{{\mu_{k+1},\nu_{k+1}}}$ corresponding to $\scriptD_k$.
We define the term $\delta'_{FV}$ associated with our feature vectors $\phi(i)\forall i \in \scriptS$ as follows:
$
\delta'_{FV} :=\sup_k \norm{\scriptM_k}_\infty.
$
Using $\delta'_{FV}$, we now give Assumption \ref{assume 8 unbiased full} which is similar to Assumption \ref{assume 1 MG LFA}, adapted to Algorithm \ref{alg:SA MG}:
\begin{assumption}\label{assume 8 unbiased full}
 $$\delta'_{FV} \alpha^{m+H-1} \frac{1+\alpha}{1-\alpha}+\frac{2\alpha^{H-1}}{1-\alpha} < 1.$$ 
 
 \hfill $\diamond$
\end{assumption}

Using Assumption \ref{assume 8 unbiased full}, we provide our main performance bounds for Algorithm \ref{alg:SA MG}: 
\begin{theorem} \label{thm:theorem 3 unbiased full} 
When
\begin{enumerate}

\item The starting state of the trajectory at each instance is drawn from a fixed distribution, $p$, where $p(i)>0\forall i \in \scriptS.$
\item $\sum_{i=0}^\infty \gamma_i = \infty$. Also, $\sum_{i=0}^\infty \gamma_i^2 < \infty.$
\end{enumerate}
under Assumption \ref{assume 8 unbiased full}, the iterates obtained in \eqref{eq: iter alg 3 unbiased full} almost surely have the following property:
\begin{align*}
    &\limsup_{k \to \infty} \norm{V_k-J^*}_\infty \leq \frac{\delta'_{app}}{1-2\alpha^{H-1}-(2+\delta'_{FV}) \alpha^{m+H-1}},
\end{align*}
where $\delta'_{app}$ is ability of the feature vectors to approximate the policies:
\begin{align}
    \nonumber&\delta'_{app} := \\&\sup_k E[\norm{\scriptM_k( J^{{\mu_{k+1},\nu_{k+1}}}+ w_k)-(J^{{\mu_{k+1},\nu_{k+1}}}+ w_k)}_\infty|\scriptF_k].
\end{align} 
\hfill$\diamond$
\end{theorem}
This result was previously published as an invited session paper at IEEE Conference on Decision and Control at the
Marina Bay Sands, Singapore, in 2023.

\begin{remark} \textbf{Interpretation of Theorem \ref{thm:theorem 3 unbiased full}}
Analogously to Theorem \ref{theorem 1 MG}, Theorem \ref{thm:theorem 3 unbiased full} shows that the performance bound is mostly based on the ability of the feature vectors to represent unbiased estimates of the value functions. Without feature vectors (i.e., when feature vectors are simply unit vectors) and trajectories from all states are obtained (i.e., in the special case where the Markov chains induced by all policies are irreducible and infinitely long trajectories are obtained), the error becomes zero. 
\end{remark}

\section{Application To RL} \label{section G}
In multi-agent model-based reinforcement learning algorithms, the learning component has been extensively studied, and hence many straightforward extensions of our work exist to involve learning in model-based settings. We will provide one such example of an algorithm based on the learning algorithm for model-based multi-agent reinforcement learning in the work of \cite{zhang2020model}. The algorithm we study can be described as follows.
The algorithm assumes knowledge of the cost/reward function (this setting is called the \textit{cost/reward-aware} setting) \cite{zhang2020model} as well as access to a generator, which, at any iteration, for any state-actions tuple $(s,u,v)$ can sample from the distribution $P(\cdot|s, u,v)$ and obtain the next state. For each state-actions tuple $(s,u,v),$ the algorithm obtains $N$ samples and, based on the samples constructs an estimate of the probability transition matrix in the following manner: $\hat{P}(s'|s,u,v) := \frac{\text{count}(s',s,u,v)}{N}.$ Using $\hat{P}$ and the known cost/reward function, the algorithm finds the Nash equilibrium policy using Algorithm \ref{alg:alg 2}, $(\hat{\mu},\hat{\nu})$. 
The following theorem gives a bound on the sample and computational complexity required to achieve an error bound on $\norm{Q^{\hat{\mu},\hat{\nu}}-Q^*}_\infty$ in linear turn-based Markov games where $\phi(s, u,v) \in \mathbb{R}^d$ and the number of reachable states from state-actions tuples in $\scriptD$ is $r.$

\begin{theorem}\label{thm:theorem 3}
    Consider a linear turn-based Markov game and any $\epsilon, \delta, \epsilon_{opt}>0$ with $\epsilon \in (0, 1/(1-\alpha)^{1/2}]$. When the number of samples of each state-actions tuple is at least $N$ and the number of computations that are made in the planning step where the Nash equilibrium policy is determined based on the model inferred from the samples is at least $C$ where
\begin{align*}
 &N \geq \frac{c \alpha \log\big[ c |\scriptS||\scriptU||\scriptV|(1-\alpha)^{-2}\delta^{-1}\big]}{(1-\alpha)^3 \epsilon^2}, \\&C \geq \frac{c  m H \log\Big[ \frac{1}{\epsilon_{opt}(1-\alpha)}\Big]}{\log[\frac{1}{\tilde{\alpha}}]}\Bigg[ d[2r+1]+d^3/3+r|\scriptA|_{max}^2 d \Bigg]
\end{align*} where $c$ is a constant, $\tilde{\alpha}=\alpha^{H-1}+(1+\alpha^m)\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha),$ $|\scriptU|$ and $|\scriptV|$ are the numbers of the total actions available to players 1 and 2, and $|\scriptA|_{max}$ is the largest number of actions available at a state, it holds that with probability at least $1-\delta,$ 
$\norm{Q^{{\hat{\mu},\hat{\nu}}}-Q^*}_\infty \leq \frac{2\epsilon}{3}+\frac{5 \alpha \epsilon_{opt}}{1-\alpha}, \norm{\hat{Q}^{\hat{\mu},\hat{\nu}}-Q^*}_\infty \leq \epsilon + \frac{9\alpha \epsilon_{opt}}{1-\alpha}.$

\hfill $\diamond$
\end{theorem}

The proof of Theorem \ref{thm:theorem 3} uses the results of \cite{zhang2020model} and extensions from Theorem \ref{thm:theorem 1} and can be found in the Appendix. Theorem \ref{thm:theorem 3} overall gives a bound on the the error of the learning algorithm as a function of the number of computations in the planning step and the number of samples in the learning step. We note that convergence of the learning algorithm in the present section does not require solving an MDP at each iteration the way that many model-based policy iteration algorithms do. In some ways, Theorem \ref{thm:theorem 3} provides a trade-off between sample complexity in the learning step and computational complexity in the planning step. 

\section{Conclusions}
In our work, we study the model-based learning problem and focus on the planning step of the problem using known learning results to provide results for model-based policy iteration for two-player zero-sum simultaneous discounted games. The main result of the paper shows that the naive policy iteration algorithm of Avi-Itzhak and Pollatshek converges if lookahead is used in the policy improvement step for both players. This adds to the body of recent literature which shows that lookahead is an important component of the bag of tricks needed to ensure that RL algorithms converge \cite{winnicki2023convergence, annaor}. The fact that lookahead provides an $\mathcal{O}(\alpha^H)$ approximate solution to the optimal policy is a somewhat trivial statement; what we have show here and in \cite{annaor, winnicki2023convergence} is a much stronger statement: lookahead leads to convergence of algorithms while there are counterexamples to show that the corresponding algorithms without lookahead may not converge. Further, we have shown that, in the case of linear MDPs, lookahead is not computationally expensive to implement.

One interesting direction for future work includes extending the results to the stochastic shortest path games problem \cite{patekthesis}.

% Acknowledgments---Will not appear in anonymized version

%\acks{We thank a bunch of people and funding agency.}
\bibliographystyle{informs2014} % outcomment this and next line in Case 1
\bibliography{refs2.bib}
\begin{APPENDICES}

\section{Proof of Theorem \ref{thm:theorem 1}}
The proof of Theorem \ref{thm:theorem 1} relies on showing a contraction property of the $T_{m,H}$ operator. 
\bigskip
We will show that $T_{m,H}$ is a contraction towards $J^*$ for all $m$ and $H$, including $m=\infty$ and all $H\geq 1.$ We note that this only holds for sufficiently large $H$ per Assumption \ref{assumption 1 games}. 

First, we have the following:
Since $T_{\mu_{k+1},\nu_{k+1}}V$ is defined as $g(\mu_{k+1},\nu_{k+1})+\alpha P(\mu_{k+1},\nu_{k+1})V,$ we can directly apply the contraction property from single player MDPs to see that:
\begin{align*}
\norm{T_{m,H}V_k-J^{\mu_{k+1},\nu_{k+1}}}_\infty &= \norm{T_{\mu_{k+1},\nu_{k+1}}^mT^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty \\&\leq \alpha^m \norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty.
\end{align*}
Thus, the following holds:
\begin{align}
\nonumber\norm{T_{m,H}V_k-T^{H-1}V_k}_\infty\nonumber \nonumber&=  \norm{T_{m,H}V_k- J^{\mu_{k+1},\nu_{k+1}}+J^{\mu_{k+1},\nu_{k+1}}  -T^{H-1}V_k}_\infty \\ \nonumber &\leq \norm{T_{m,H}V_k- J^{\mu_{k+1},\nu_{k+1}}}_\infty +\norm{J^{\mu_{k+1},\nu_{k+1}} -T^{H-1}V_k}_\infty  \\
 \nonumber &\leq \alpha^m \norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty \\&+\norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty \\
  &= (1+\alpha^m)\norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty.\label{eq:three}
\end{align}

We will now attempt to obtain a bound on $\norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty$. 
Here, we bypass a crucial monotonicity property of $T$ for single player systems that is not present in games. We  have by definition of $T_{\mu_{k+1},\nu_{k+1}}$ the following for all $\ell$:
\begin{align*}
&\norm{T_{\mu_{k+1},\nu_{k+1}}^{\ell+1}T^{H-1}V_k-T_{\mu_{k+1},\nu_{k+1}}^{\ell}T^{H-1}V_k}_\infty  \leq \alpha^\ell \norm{TV_k-V_k}_\infty.
\end{align*}
To start, we will need the following pseudo-contraction property of $T$ the optimal value function \cite{bertsekastsitsiklis}:
\begin{align*}
    \norm{TV-J^*}_\infty \leq \alpha\norm{V-J^*}_\infty.
\end{align*}
Since $T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k = T^H V_k$ and using the property of $T$, we have the following:
\begin{align*}
    &\norm{T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k - T^{H-1} V_k}_\infty\\ &=  \norm{T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k -J^* +J^*- T^{H-1} V_k}_\infty\\
    &\leq \norm{T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k -J^*}_\infty +\norm{J^*- T^{H-1} V_k}_\infty \\
    &= \norm{T^{H}V_k -J^*}_\infty +\norm{J^*- T^{H-1} V_k}_\infty \\
    &\leq \alpha^H \norm{V_k - J^*}_\infty + \alpha^{H-1}\norm{J^*-V_k}_\infty \\
    &= \underbrace{(\alpha^H + \alpha^{H-1})\norm{J^*-V_k}_\infty}_{=: \tilde{a}}.
\end{align*}

Thus,
\begin{align}
    -T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k\leq  - T^{H-1} V_k + \tilde{a}.
\end{align}

Suppose that we apply the $T_{\mu_{k+1},\nu_{k+1}}$ operator $\ell-1$ times to both sides. Then, due to monotonicity and the fact $T_{\mu,\nu}(J+ce)=T_{\mu,\nu}(J)+\alpha ce,$ for any policy $(\mu,\nu),$ we have the following:
\begin{align*}
    {T^\ell_{\mu_{k+1},\nu_{k+1}}} T^{H-1}V_k \leq \alpha^{\ell } \tilde{a}e + {T^{\ell+1}_{\mu_{k+1},\nu_{k+1}}}T^{H-1}V_k.
\end{align*}
Using a telescoping sum, we get the following inequality:
\begin{align*}
    T_{\mu_{k+1},\nu_{k+1}}^j T^{H-1} V_k - T^{H-1}V_k
    &\geq - \sum_{\ell = 1}^{j} \alpha^{\ell - 1} \tilde{a} e.
\end{align*}
Taking the limit as $j\rightarrow\infty$ on both sides, we have the following:
\begin{align}
    J^{\mu_{k+1},\nu_{k+1}} - T^{H-1}V_k \geq - \frac{\tilde{a} e}{1-\alpha}.
\label{eq:one}
\end{align}
In the other direction, we have the following:
\begin{align}
     - T^{H-1} V_k\leq -T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k + \tilde{a}.
\end{align}
Applying the $T_{\mu_{k+1},\nu_{k+1}}$ operator $\ell-1$ times to both sides, we have :
\begin{align*}
    {T^{\ell+1}_{\mu_{k+1},\nu_{k+1}}} T^{H-1}V_k \leq \alpha^{\ell } \tilde{a}e + {T^{\ell}_{\mu_{k+1},\nu_{k+1}}}T^{H-1}V_k.
\end{align*}
Using a telescoping sum, we get the following inequality:
\begin{align*}
   T^{H-1}V_k- T_{\mu_{k+1},\nu_{k+1}}^j T^{H-1} V_k 
    &\geq - \sum_{\ell = 1}^{j} \alpha^{\ell - 1} \tilde{a} e.
\end{align*}
Taking the limit as $j\rightarrow\infty$ on both sides, we have the following:
\begin{align}
   T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}  \geq - \frac{\tilde{ae} }{1-\alpha}.
   \label{eq:two}
   \end{align}
Hence, putting inequalities \eqref{eq:one} and \eqref{eq:two} together, we have the following bound:
\begin{align*}
    \norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty \leq \frac{\tilde{a} }{1-\alpha}.
\end{align*}

We plug this bound into our result in \eqref{eq:three} to get:
\begin{align}
  \nonumber&\nonumber\norm{T_{m,H}V_k-T^{H-1}V_k}_\infty\nonumber \nonumber\\&\nonumber\leq \frac{\tilde{a} e}{1-\alpha} \\&=  \frac{(1+\alpha^m)(\alpha^H + \alpha^{H-1})\norm{J^*-V_k}_\infty }{1-\alpha}.
\label{eq:four} \end{align}

We now provide the reverse triangle inequality which we will use in the next step: \begin{align*}
    \norm{X-Y}_\infty - \norm{Y-Z}_\infty \leq \norm{X-Z}_\infty \forall X,Y,Z.
\end{align*} Using the reverse triangle inequality we have the following bound:
\begin{align*}
&\norm{T_{m,H}V_k - J^*}_\infty - \norm{T^{H-1}V_k-J^*}\\&\leq \norm{T_{m,H}V_k-T^{H-1}V_k}_\infty.
\end{align*}

Now, we use the pseudo-contraction property of $T$ towards $J^*$ as follows:
\begin{align*} &\norm{T_{m,H}V_k - J^*}_\infty \\&\leq \norm{T_{m,H}V_k-T^{H-1}V_k}_\infty +  \norm{J^*-T^{H-1}V_k}_\infty\\
&\leq \norm{T_{m,H}V_k-T^{H-1}V_k}_\infty + \alpha^{H-1} \norm{J^*-V_k}_\infty \\
&\leq \frac{(1+\alpha^m)(\alpha^H + \alpha^{H-1})\norm{J^*-V_k}_\infty }{1-\alpha}+\alpha^{H-1} \norm{J^*-V_k}_\infty, 
\end{align*}
where in the last line we plug in our bound in \eqref{eq:four}.

Hence, the following holds:

\begin{align*}
  &\norm{T_{m,H}V_k-J^*}_\infty  \\&\leq  \Big(\alpha^{H-1}+(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha)\Big)\norm{V_k - J^*}_\infty.
\end{align*}

Noting that $V_{k+1} = T_{m,H}V_k,$ we iterate to get the following bound:
\begin{align*}
&\norm{V_k-J^*}_\infty \\& \leq  \Big(\alpha^{H-1}+(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha)\Big)^k\norm{V_0 - J^*}_\infty,
\end{align*} and, further using Assumption \ref{assumption 1 games}, we have that $V_k \to J^*,$ which proves the exponential rate of convergence of our algorithm.
endproof

The framework of the techniques we use are based on those of the work of \cite{winnicki2023convergence} however unlike the work of \cite{winnicki2023convergence} the setting of the problem is a two-player game in a deterministic setting as opposed to online learning with stochastic approximation for a single-player system.
\hfill
\Halmos
\section{Proof of Theorem \ref{theorem 1 MG}}
First, we define $V_k$ as follows:
\begin{align*}
    V_k := \Phi \theta_k.
\end{align*}
In order to prove Theorem \ref{alg:alg 4 MG}, we will first give Lemma \ref{lemma 1 MG}. Lemma \ref{lemma 1 MG} is proved in \cite{winnicki2023convergence} for the MDP setting and can easily be extended to the zero-sum Markov games setting using contraction properties given in Section \ref{prelim}.
\begin{lemma}
\begin{align*}
    \norm{J^{{\mu_{k+1},\nu_{k+1}}}-T^{H-1}V_k}_\infty \leq \frac{\alpha^{H-1}}{1-\alpha}\norm{TV_k - V_k}_\infty,
\end{align*} \label{lemma 1 MG}
\end{lemma} where $J^{{\mu_{k+1},\nu_{k+1}}}$ is the value function corresponding to policy ${\mu_{k+1},\nu_{k+1}}$. 

We note that Lemma \ref{lemma 1 MG} implies:
\begin{align*}
    &\norm{J^{{\mu_{k+1},\nu_{k+1}}}-T^{H-1}V_k}_\infty \\&\leq \frac{\alpha^{H-1}}{1-\alpha}\norm{TV_k - V_k}_\infty \allowdisplaybreaks\\
    &=\frac{\alpha^{H-1}}{1-\alpha}\norm{TV_k-J^*+J^* - V_k}_\infty\allowdisplaybreaks\\
    &\leq \frac{\alpha^{H-1}}{1-\alpha}\norm{TV_k-J^*}_\infty+\frac{\alpha^{H-1}}{1-\alpha}\norm{J^* - V_k}_\infty\allowdisplaybreaks\\
    &\leq \frac{\alpha^{H}}{1-\alpha}\norm{V_k-J^*}_\infty+\frac{\alpha^{H-1}}{1-\alpha}\norm{J^* - V_k}_\infty\allowdisplaybreaks\\
    &= \Big(\frac{\alpha^{H-1}(1+\alpha)}{1-\alpha}\Big)\norm{V_k-J^*}_\infty.
\end{align*}
Note that the above implies the following:
\begin{align*}
   &T^{H-1}\Phi \theta_k - \Big(\frac{\alpha^{H-1}(1+\alpha)}{1-\alpha}\Big)\norm{V_k-J^*}_\infty 
   \\&\leq J^{{\mu_{k+1},\nu_{k+1}}} 
   \\& \leq T^{H-1}\Phi \theta_k +\Big(\frac{\alpha^{H-1}(1+\alpha)}{1-\alpha}\Big)\norm{V_k-J^*}_\infty.
\end{align*}
We will now upper bound our iterates $V_{k+1}$:
\begin{align}
V_{k+1} \nonumber&=\Phi \theta_{k+1}
\nonumber\\&= \scriptM T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k \nonumber\\
\nonumber&= \scriptM T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k-J^{{\mu_{k+1},\nu_{k+1}}} \allowdisplaybreaks\\&+ J^{{\mu_{k+1},\nu_{k+1}}}\nonumber\\
\nonumber&\leq \norm{\scriptM T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k-J^{{\mu_{k+1},\nu_{k+1}}}}_\infty\\\nonumber    & + J^{{\mu_{k+1},\nu_{k+1}}}\nonumber\allowdisplaybreaks\\
%&= \norm{\scriptM T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k-\scriptM J^{{\mu_{k+1},\nu_{k+1}}}+\scriptM J^{{\mu_{k+1},\nu_{k+1}}} + J^{{\mu_{k+1},\nu_{k+1}}}}_\infty\nonumber\\& + J^{{\mu_{k+1},\nu_{k+1}}} \nonumber\\
\nonumber&\leq \norm{\scriptM T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k-\scriptM J^{{\mu_{k+1},\nu_{k+1}}}}_\infty\nonumber\\&+\norm{\scriptM J^{{\mu_{k+1},\nu_{k+1}}} - J^{{\mu_{k+1},\nu_{k+1}}}}_\infty \allowdisplaybreaks\nonumber\\&+ J^{{\mu_{k+1},\nu_{k+1}}}\nonumber\allowdisplaybreaks\\
\nonumber&\leq \delta_{FV} \alpha^m \norm{ T^{H-1}V_k- J^{{\mu_{k+1},\nu_{k+1}}}}_\infty+\delta_{app} \nonumber\\&+ J^{{\mu_{k+1},\nu_{k+1}}}\nonumber\allowdisplaybreaks\\
\nonumber&\leq  T^{H-1}\Phi \theta_k + \delta_{app} \\&+\frac{(\delta_{FV} \alpha^{m+H-1}+\alpha^{H-1})(1+\alpha) }{1-\alpha}\Big)\norm{V_k-J^*}_\infty, \label{eq: before lemma 2 unbiased full}
\end{align} 
where $\delta_{FV} :=\norm{\scriptM}_\infty$ and  $\delta_{app} := \sup_{k,\mu_k}\norm{\scriptM J^{\mu}-J^{\mu}}_\infty$.

We can follow the above steps to derive an analogous lower bound as follows:
\begin{align}
V_{k+1} &=\Phi \theta_{k+1}
\nonumber\\&= \scriptM T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k \nonumber\allowdisplaybreaks\\
\nonumber&= \scriptM T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k-J^{{\mu_{k+1},\nu_{k+1}}} + J^{{\mu_{k+1},\nu_{k+1}}}\nonumber\allowdisplaybreaks\\
\nonumber&\geq -\norm{\scriptM T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k-J^{{\mu_{k+1},\nu_{k+1}}}}_\infty \nonumber\\&+ J^{{\mu_{k+1},\nu_{k+1}}}\nonumber\allowdisplaybreaks\\
%&= \norm{\scriptM T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k-\scriptM J^{{\mu_{k+1},\nu_{k+1}}}+\scriptM J^{{\mu_{k+1},\nu_{k+1}}} + J^{{\mu_{k+1},\nu_{k+1}}}}_\infty\nonumber\\& + J^{{\mu_{k+1},\nu_{k+1}}} \nonumber\\
\nonumber&\geq -\norm{\scriptM T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k-\scriptM J^{{\mu_{k+1},\nu_{k+1}}}}_\infty\nonumber\\&+\norm{\scriptM J^{{\mu_{k+1},\nu_{k+1}}} - J^{{\mu_{k+1},\nu_{k+1}}}}_\infty \nonumber\\&+ J^{{\mu_{k+1},\nu_{k+1}}}\nonumber\allowdisplaybreaks\\
\nonumber&\geq -\delta_{FV} \alpha^m \norm{ T^{H-1}V_k- J^{{\mu_{k+1},\nu_{k+1}}}}_\infty-\delta_{app} \allowdisplaybreaks\\&+ J^{{\mu_{k+1},\nu_{k+1}}}\nonumber\allowdisplaybreaks\\
\nonumber&\geq  T^{H-1}\Phi \theta_k -\delta_{app} \\&-\frac{(\delta_{FV} \alpha^{m+H-1}+\alpha^{H-1})(1+\alpha) }{1-\alpha}\Big)\norm{V_k-J^*}_\infty\allowdisplaybreaks, \label{eq: before lemma 2 unbiased full 2}
\end{align} 
We now use the above to obtain the following bound on $\norm{V_{k+1}-T^{H-1}\Phi \theta_k}_\infty:$
\begin{align*}
    &\norm{V_{k+1}-T^{H-1}\Phi \theta_k}_\infty\\&\leq \delta_{app} +\frac{(\delta_{FV} \alpha^{m+H-1}+\alpha^{H-1})(1+\alpha) }{1-\alpha}\norm{V_k-J^*}_\infty.
\end{align*}
Using the reverse triangle inequality, we have:
\begin{align*}
&\norm{V_{k+1}-T^{H-1}\Phi \theta_k}_\infty \\
&=\norm{V_{k+1}-J^*+J^*-T^{H-1}\Phi \theta_k}_\infty \\
&\geq \norm{V_{k+1}-J^*}_\infty-\norm{J^*-T^{H-1}\Phi \theta_k}_\infty,
\end{align*}
which implies that
\begin{align*}
    &\norm{V_{k+1}-J^*}_\infty \\&\leq \norm{T^{H-1}V_k-J^*}_\infty + \delta_{app} \\&+\frac{(\delta_{FV} \alpha^{m+H-1}+\alpha^{H-1})(1+\alpha) }{1-\alpha}\norm{V_k-J^*}_\infty \\
    &\leq \Big(\alpha^{H-1}+\frac{(\delta_{FV} \alpha^{m+H-1}+\alpha^{H-1})(1+\alpha) }{1-\alpha}\Big) \norm{V_k-J^*}_\infty \\&+ \delta_{app}.
\end{align*}

Iterating over $k$, we have:
\begin{align*}
&\norm{V_{k}-J^*}_\infty \\&\leq \underbrace{\Big(\alpha^{H-1}+\frac{(\delta_{FV} \alpha^{m+H-1}+\alpha^{H-1})(1+\alpha) }{1-\alpha}\Big)^k \norm{V_0-J^*}_\infty }_{finite-time \text{ } component}\\&+\underbrace{\frac{\delta_{app}}{ \Big(\alpha^{H-1}+\frac{(\delta_{FV} \alpha^{m+H-1}+\alpha^{H-1})(1+\alpha) }{1-\alpha}\Big)}}_{asymptotic \text{ }component}.
\end{align*}
\hfill
\Halmos
\section{Proof Of Theorem \ref{thm:theorem 3}}
Setting $$H(V_k) := E[\scriptM_k( T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k+w_k)-J^{{\mu_{k+1},\nu_{k+1}}}|\scriptF_k],$$ we have the following bound:
\begin{align*}
    &\norm{H(V_k)-J^{{\mu_{k+1},\nu_{k+1}}}}_\infty \allowdisplaybreaks\\
    &= ||E[\scriptM_k( T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k+w_k)-J^{{\mu_{k+1},\nu_{k+1}}}|\scriptF_k]||_\infty \allowdisplaybreaks\\
   % &= ||E[\scriptM_k( T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k+w_k)\\&-\scriptM_k( J^{{\mu_{k+1},\nu_{k+1}}}+ w_k)+\scriptM_k( J^{{\mu_{k+1},\nu_{k+1}}}+ w_k)-J^{{\mu_{k+1},\nu_{k+1}}}|\scriptF_k]||_\infty \\
    &\leq ||E[\scriptM_k( T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k+w_k)\\&-\scriptM_k( J^{{\mu_{k+1},\nu_{k+1}}}+ w_k)|\scriptF_k]||_\infty\\&+\norm{E[\scriptM_k( J^{{\mu_{k+1},\nu_{k+1}}}+ w_k)-J^{{\mu_{k+1},\nu_{k+1}}}|\scriptF_k]}_\infty \allowdisplaybreaks\\
    &\leq ||E[\scriptM_k( T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k+w_k)\\&-\scriptM_k( J^{{\mu_{k+1},\nu_{k+1}}}+ w_k)|\scriptF_k]||_\infty\\&+\underbrace{\sup_{k, \mu_k}\norm{E[\scriptM_k( J^{{\mu_{k+1},\nu_{k+1}}}+ w_k)-J^{{\mu_{k+1},\nu_{k+1}}}|\scriptF_k]}_\infty}_{=: \delta'_{app}} \allowdisplaybreaks\\ 
    &= ||E[\scriptM_k( T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k)-\scriptM_k( J^{{\mu_{k+1},\nu_{k+1}}})|\scriptF_k]||_\infty\\&+ \delta'_{app} \allowdisplaybreaks\\ 
    &= E[||\scriptM_k( T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k)-\scriptM_k( J^{{\mu_{k+1},\nu_{k+1}}})||_\infty|\scriptF_k]\\&+ \delta'_{app} \allowdisplaybreaks\\ 
    &\leq  E[\sup_k \norm{\scriptM_k}_\infty ||T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k- J^{{\mu_{k+1},\nu_{k+1}}}||_\infty|\scriptF_k]\\&+ \delta'_{app} \allowdisplaybreaks\\ 
    &=  \underbrace{\sup_k \norm{\scriptM_k}_\infty}_{=: \delta'_{FV}} || T_{{\mu_{k+1},\nu_{k+1}}}^m T^{H-1}V_k -J^{{\mu_{k+1},\nu_{k+1}}}||_\infty\allowdisplaybreaks\\&+ \delta'_{app} \allowdisplaybreaks\\ 
    &\leq \alpha^m \delta'_{FV}|| T^{H-1}V_k -J^{{\mu_{k+1},\nu_{k+1}}}||_\infty+ \delta'_{app}.\allowdisplaybreaks
\end{align*}
Adding and subtracting $T^{H-1}V_k$ and using the triangle inequality, we furthermore have:
\begin{align}
   \nonumber &\norm{H(V_k)-T^{H-1}V_k}_\infty \\
    \nonumber&\leq  (1+ \alpha^m \delta'_{FV}) \norm{  T^{H-1}V_k -J^{{\mu_{k+1},\nu_{k+1}}}}_\infty+ \delta'_{app} \\
    &\leq \frac{(1+ \alpha^m \delta'_{FV})\alpha^{H-1}}{1-\alpha} \norm{TV_k - V_k}_\infty+ \delta'_{app}, \label{eq: label ineq MG} 
\end{align} where the last line follows from Lemma \ref{lemma 1 MG}. Adding and subtracting $J^*$ on both sides of the inequality in \eqref{eq: label ineq MG} and using the triangle inequality, we have:
\begin{align}
   \nonumber &\norm{H(V_k)-J^*}_\infty 
    \\ \nonumber &\leq (\alpha^{H-1}+(1+\alpha)\frac{(1+ \alpha^m \delta'_{FV})\alpha^{H-1}}{1-\alpha})\norm{V_k - J^*}_\infty \\&+ \delta'_{app}. \label{eq: final eq MG FA}
\end{align} 


We now provide the following Lemma \ref{lemma 2 MG FA} from Appendix D of \cite{winnicki2023convergence} which will be used to obtain Theorem \ref{thm:theorem 3 unbiased full}.
\begin{lemma} \label{lemma 2 MG FA}
    Suppose that we have the following sequence:
    \begin{align*}
    V_{k+1} = (1-\gamma_k) V_k + \gamma_k (H(V_k) + z_k),
\end{align*}
where 
\begin{align*}
    \norm{H(V_k)-J^*}_\infty \leq \beta\norm{V_k-J^*}_\infty + \delta
\end{align*} for some $0<\beta<1$, $\delta$ and $J^*.$ Then, the following holds: 
\begin{align*}
   \limsup_{k\to \infty} \norm{V_k - J^*}_\infty \leq  \frac{\delta}{1-\beta}.
\end{align*} \hfill $\diamond$
\end{lemma}

Applying Lemma \ref{lemma  2 MG FA} to \eqref{eq: final eq MG FA} with our $\beta$ in Lemma \ref{lemma 2 MG FA} being $\alpha^{H-1}+(1+\alpha)\frac{(1+ \alpha^m \delta'_{FV})\alpha^{H-1}}{1-\alpha}$ and $\delta$ of Lemma \ref{lemma 2 MG FA} being $\delta'_{app},$ we obtain Theorem \ref{thm:theorem 3 unbiased full} when Assumption \ref{assume 8 unbiased full} is satisfied.  
\hfill
\Halmos
\section{Proof of Theorem \ref{thm:theorem 3}}
We note that the settings of the bound in \cite{zhang2020model} are the same as the settings of our work, so we can directly apply the bounds in \cite{zhang2020model}. We will restate Theorem 3.3 from \cite{zhang2020model} in Lemma \ref{lemma 1 games}:

\begin{lemma}\label{lemma 1 games}
    Consider any $\epsilon, \delta>0$ with $\epsilon \in (0, 1/(1-\alpha)^{1/2}]$ and $\delta \in [0,1]$. When the number of samples of each state-actions tuple is at least $N$ by the learning oracle and a planning oracle is used based on the empirical model $\hat{\mathcal{G}}$ which is reward-aware and the entries are of the probability transition matrix is constructed by taking averages determined by the learning oracle to determine a policy $(\hat{\mu},\hat{\nu})$ where:
    \begin{align*}
        \norm{\hat{V}^{\hat{\mu},\hat{\nu}}-\hat{J}^*}_\infty \leq\epsilon_{opt},
    \end{align*} where $\hat{J}^*$ is the optimal value function for model $\hat{\mathcal{G}}$, when 
\begin{align*}
 &N \geq \frac{c \alpha \log\big[ c |\scriptS||\scriptU||\scriptV|(1-\alpha)^{-2}\delta^{-1}\big]}{(1-\alpha)^3 \epsilon^2} 
\end{align*} where for some absolute constant $c$, it holds that with probability at least $1-\delta,$ 
\begin{align*}
    \norm{Q^{{\hat{\mu},\hat{\nu}}}-Q^*}_\infty &\leq \frac{2\epsilon}{3}+\frac{5 \alpha \epsilon_{opt}}{1-\alpha}, \\ \norm{\hat{Q}^{\hat{\mu},\hat{\nu}}-Q^*}_\infty &\leq \epsilon + \frac{9\alpha \epsilon_{opt}}{1-\alpha}.
\end{align*}
\hfill $\diamond$
\end{lemma}

Now, we compute the complexity as follows:
It is easy to see that to compute $$Q(s, u, v) \leftarrow r(s, u, v)+ \alpha \sum_{s' \in \scriptD_\scriptR} P(s'|s, u,v)V(s') \forall (s,u,v) \in \scriptD$$ requires $d[2r+1]$ computations, computing $$\theta \leftarrow \displaystyle\argmin_\theta  \sum_{(s, u,v)\in \scriptD} [\phi(s,u,v)^\top \theta - Q(s,u,v)]^2$$ requires $d^3/3$ computations (as an upper bound), computing $$V(s') \leftarrow \displaystyle\max_{\substack{\mu\in \mathbb{R}^{|\scriptU(s')|} \\ \sum \mu_i=1\\0\leq \mu_i \leq 1  }} \displaystyle\min_{\substack{\nu(s) \in \mathbb{R}^{|\scriptV(s')|} \\ \sum \nu_i=1\\0\leq \nu_i \leq 1  }} \mu^\top A_{\theta, s'} \nu,$$ needs $r\times|\scriptA|_{max}^2\times d$ computations noting that there is a turn-based MDP which is well known to involve only deterministic policies, and since the $\mu^\top A_{\theta, s'} \nu$ is a special case of the previous step, no more additional computations are needed to determine an upper bound. Using the above, some algebra gives Theorem \ref{thm:theorem 3}.
\hfill
\Halmos


\end{APPENDICES}


\end{document}