%\documentclass[anon,12pt]{colt2023} % Anonymized submission
\documentclass[final,12pt]{colt2023} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
   

\title[A New  Policy Iteration Algorithm For Zero-Sum Markov Games]{A New  Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games}
\usepackage{times,tcolorbox}
\usepackage{custom}
\newtheorem{assumption}{Assumption}
\newcommand\norm[1]{\lVert#1\rVert}
\newcommand\normx[1]{\Vert#1\Vert}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{comment}
\renewcommand{\thesection}{\Alph{section}}
%\usepackage[linesnumbered]{algorithm2e}
%\usepackage[linesnumbered,ruled]{algorithm2e}



\makeatletter
\newcommand{\AlgoResetCount}{\renewcommand{\@ResetCounterIfNeeded}{\setcounter{AlgoLine}{0}}}
\newcommand{\AlgoNoResetCount}{\renewcommand{\@ResetCounterIfNeeded}{}}
\newcounter{AlgoSavedLineCount}
\newcommand{\AlgoSaveLineCount}{\setcounter{AlgoSavedLineCount}{\value{AlgoLine}}}
\newcommand{\AlgoRestoreLineCount}{\setcounter{AlgoLine}{\value{AlgoSavedLineCount}}}
\makeatother

% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
\coltauthor{%
 \Name{Anna Winnicki} \Email{annaw5@illinois.edu}\\
 \addr 1308 W Main St, Urbana, IL 61801
 \AND
 \Name{R. Srikant} \Email{rsrikant@illinois.edu}\\
 \addr 1308 W Main St, Urbana, IL 61801
}
\begin{document}

\maketitle

\begin{abstract}%

Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in \cite{hansen2013strategy} that one has to solve $\Omega(1/(1-\alpha))$ MDPs to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and converges exponentially fast. The only addition we propose to naive policy iteration is the use of lookahead in the policy improvement phase. This is appealing because lookahead is anyway often used in RL for games. We further show that lookahead can be implemented efficiently in linear Markov games, which are the counterpart of the linear MDPs and have been the subject of much attention recently. We then consider multi-agent reinforcement learning which uses our algorithm in the planning phases, and provide sample and time complexity bounds for such an algorithm. 

%The extension MDPs to games and into the learning setting has been studied since 1965, and much progress has been made, especially recently. Many algorithms including XYZ have successfully extended value iteration algorithms to games in both the model-based and model-free setting, obtaining near optimal sample and time complexity bounds. However, the scope of the popular policy iteration algorithm has been largely not studied in the literature, because of the time complexity which has been shonw in \cite{hansen2013strategy} to be XYZ. Herein, we have the first positive study on a variant of policy iteration for games which does not suffer from significant time-complexity bounds. We believe these algorithms can be easily extended to games in learning and provide some results extending the results to games and online learning. 

%Many rl algs for MDPs as well as MA RL can be viewed as having 2 components: a learning comp and a planning comp. while learning comp has been ext. studied, planning comp not been studied in case of MA RL - in this paper, we study zero sum games and study the planning problem. 
%Our results can be used in conjunction with learning bounds, and present one such 
%Abstract: many single player and MA algs have 2 components: learning comp and planning comp , tremendous amount of work on learning comp but often the planning component is ignored bc it is very well studied for MDPs , but in the context of zero sum games, the planning part is more computationally challenging and in particular , modified PI, generalization of VI and PI, in general, m=1, converges, there is a PI theory, but it is very complicated, so in practice, ppl do VI+PI and by choosing m, one can trade off bw VI and PI, so it is worth studying for that reason. 
%Acknowledge our AISTATS paper, then say we use the ideas in this player to the two player case …
%Ppl have been looking at games since 1965, interesting
\end{abstract}

\begin{keywords}%
Zero-sum games, Multi-agent reinforcement learning, Policy iteration
\end{keywords}

\section{Introduction}
Multi-agent reinforcement learning algorithms have contributed to many successes in machine learning, including games such as chess and Go \cite{silver2016mastering, DBLP:journals/corr/MnihBMGLHSK16, silver2017mastering, silver2017shoji}, automated warehouses, robotic arms with multiple arms \cite{gu2017deep}, and autonomous traffic control \cite{yang2020multi, shalev2016safe}; see \cite{ozdaglar2021independent, zhang2021multi, yang2020overview} for surveys. Multi-agent RL can refer to one of many scenarios: (i) where a team of agents work towards a common goal with all the agents have the same information or not \cite{qu2022scalable}, (ii) non-cooperative games where there are multiple agents with their own objectives \cite{zhang2019non}, and (iii) zero-sum games where there are only two players with opposing objectives. We can further categorize problems as infinite-horizon, discounted reward/cost, finite-horizon, simultaneous move or turn-based games. The literature in this area is vast; here we focus on zero-sum, simultaneous move, discounted reward/cost Markov games.

In the case of model-based zero-sum discounted reward/cost simultaneous-move Markov games, the recent work of \cite{zhang2020model} analyzes the setting where a model is estimated from data and a general algorithm of one's choice including value iteration, policy iteration, etc. is used to find the Nash equilibrium.  However, there are multiple RL algorithms for other versions of model-based multi-agent RL including turn-based Markov games \cite{sidford2020solving}, finite-horizon games \cite{bai2020provable,liu2021sharp}, among others.


In the above references, model-based algorithms generally consist of two phases: a learning phase where the probability transition matrix and average reward/cost are estimated, possibly using a generative model, and a planning phase. The results in \cite{zhang2020model} are obtained assuming that there exists an efficient algorithm for planning. As is well known, there are two broad classes of convergent, easy-to-implement algorithms to find the optimal policy for the planning problem in the case of discounted-reward MDPs, namely policy and value iterations \cite{bertsekastsitsiklis}. However, the situation is more complicated in the case of zero-sum Markov games. While value iteration naturally extends to zero-sum Markov games \cite{Shapley}, the extension of policy iteration to zero-sum Markov games is problematic. The only known convergent algorithm requires solving $\Omega(1/(1-\alpha))$ MDPs \cite{hansen2013strategy}. There is an alternative algorithm, called naive policy iteration, which requires far fewer computations, but is only known to converge that under restrictive assumptions \cite{pollatschek1969algorithms}. 

So a longstanding open question (for at least 53 years!) is whether naive policy iteration converges for a broad class of models. Often other attempts to answer this question have only succeeded in proving the convergence of modified versions of the algorithm for restrictive classes of games. A significant recent contribution in \cite{ bertsekas2021distributed} is a modification of naive policy iteration that converges but requires more storage. However, this algorithm does not yet appear to have an extension to the function approximation setting even for Markov games with special structure such as linear Markov games. An extension of \cite{bertsekas2021distributed} can be found in \cite{brahma2022convergence} which studies stochastic and optimistic settings. Our contribution is a modified version of the naive policy iteration algorithm that converges exponentially fast for all discounted-reward/cost zero-sum Markov games. The modification is easy to explain: simply replace the policy improvement step with a lookahead version of policy improvement. Lookahead has been widely used in RL from the early application to backgammon \cite{tesauro1996line} to recent applications such as Chess and Go \cite{silver2017mastering}. But to the best of our knowledge, our result is the first which proves the convergence of naive policy iteration using lookahead. On the other hand, lookahead can be computationally expensive, but we show it has low computational complexity for the class of linear Markov games, which are a natural generalization of linear MDPs, which have been studied extensively recently \cite{agarwal2020flambe, uehara2021representation,zhang2022making}. We note that our result complements the recent results on the benefits of lookahead to improve the convergence properties of MDPs with and without function approximation in other different contexts \cite{annaor, annacdc, winnicki2023convergence}. The works of \cite{efroni2018multiple, efroni2019combine} also study the effects of lookahead.

\begin{comment}
Many recent works have focused on understanding why the algorithms work so well and have shown near optimal sample and time complexity bounds \cite{xie2020learning, zhang2020model, sidford2020solving, liu2021sharp, zhao2022provably}, among others. See the following works for a broad survey 

Several techniques from single-player RL have been successfully extended to the model-based setting in multiagent RL \cite{liu2021sharp, zhang2020model}, which has been widely studied as it avoids the potential issue of non-stationarity during the learning process of model-free algorithms \cite{busoniu2008comprehensive}, among others. In the model-based setting, there are two components: a learning component and a planning component. In the learning component, data from samples from MDPs are used to estimate the model which includes the probability transition matrix and the reward function (depending on whether the setting is reward-aware or reward agnostic, when knowledge of the reward function is not assumed). In the planning component, extensions of single-player dynamic programming algorithms are performed using the estimate of the model from the learning phase to obtain a better estimate of the optimal policy. While the learning problem has been very well-studied in terms of time complexity and sample complexity, the planning component and its computational difficulties have not been as well studied in the context of multi-agent reinforcement learning, despite the open problems that exist in the planning step and the associated computational difficulties for the multi-agent reinforcement learning algorithms. The great increase in computational difficulties that arise with multiple players with different objectives compared to the single-player setting in the planning step affects the overall time complexity of both model-based and model-free multi-agent RL algorithms.

In the planning step, extending value iteration to games involves solving a linear programming algorithm at each iteration compared to a maximization in the case of single player algorithms. The work of \cite{hansen2013strategy} shows that policy iteration applied to games requires solving $\Omega(1/(1-\alpha))$ MDPs, where $\alpha$ is the discount factor, and as such, the challenge of extending the policy iteration algorithm to games remains. Algorithms in the literature and their variants which study the policy iteration algorithm in games often sidestep these computational issues. However, policy iteration algorithms have been successfully extended to games in real world settings such as in AlphaZero. Hence, we aim to study policy iteration model-based reinforcement learning algorithms. 
\end{comment}

In fact, our results are for an algorithm which subsumes policy iteration and value iteration as special cases. Following \cite{perolat2015approximate}, we call the algorithm generalized policy iteration, although the MDP version of the algorithm goes by several names including modified policy iteration \cite{Puterman1978ModifiedPI} and optimistic policy iteration \cite{bertsekastsitsiklis}. Further, to show the applicability of our result, and in particular, to show that it can be combined with learning algorithms for zero-sum Markov games, we present a sample complexity result using the learning phase of the algorithm in \cite{zhang2020model} along with our generalized policy iteration algorithm for planning.

\begin{comment}

We provide a generalization of the value iteration and policy iteration algorithms for games where at each iteration several steps of value iteration are taken followed by several steps of policy iteration. We believe that the trade off between value iteration and policy iteration in our algorithm is worth studying for that reason. The framework of iterative steps of value iteration and policy iteration is used several state-of-the-art applications such as AlphaZero, where the policy attained through the repeated iterations of value iteration is called the ``lookahead'' policy. In the single-player reinforcement learning literature, this algorithm is referred to as the modified policy iteration algorithm. We note that value iteration and policy iteration algorithms for games are special cases of our algorithm. Additionally, we show that in the policy iteration algorithm special case, no MDPs are needed to be solved, unlike the commonly studied policy iteration algorithm in games \cite{patekthesis, perolat2015approximate, tessler2019action}. The policy iteration algorithm we study is referred to as the well-studied Algorithm of Pollatschek and Avi-Itzhak \cite{pollatschek1969algorithms}, whose convergence largely remains an open question.
Our aim is to address the computational difficulty of policy iteration in games by proving convergence of a variant of generalized policy iteration for games that does not require solving any MDPs. We seek to provide an algorithm that is scalable for large state and action spaces and as such, we wish to incorporate function approximation techniques to address this challenge. 
\end{comment}




%Among the main challenges of multi-agent RL is the  computational complexity of solving the problem even when the model is known. Unlike solving single-agent RL problems, the multi-agent RL problem is much more involved due to the non-stationarity during the learning process. As such, one remedy to the challenge involves decoupling the learning and planning phases. In other words, at each iteration of several recent algorithms (kaiqing, chi jin sharp), the system performs a model-based technique such as value-iteration (shapley) or generalized policy iteration \cite{patekthesis, perolat2015approximate, tessler2019action} to update the policy and then performs a learning step where the estimate of the model is updated using data from sample trajectories following the policy. Such decoupling also exists in single-agent RL literature (Azar 2013, Chi Jin 2020). 

%However, in such algorithms, a key challenge remains, which is the computational difficulty of planning in games, which far exceeds that of single-player games, which can be viewed as a degenerate case of two-player games (littman and lagoudakis). We focus on the planning algorithms and answer the following question in the affirmative:

%\begin{center}
%\textit{Can we design computationally efficient algorithms for solving two-player zero sum games in the function approximation setting?}
%\end{center}

%We complete our results with learning results that do not consider computational complexity of the planning phase to get sample efficient as well as computational guarantees of two-player zero sum games with unknown models. 

%Many prior works have focused on the computational difficulty of solving two-player zero-sum games, however often the results are built for small scale system as the results heavily depend on the sizes of the state and action spaces. The challenge of considering function approximation remains, even in the case of linear MDPs. Additionally, even in modern strongly polynoimal algorithms (aaron, chi jin sharp, kaiqing, qiaomin), the question of efficiency remains as their algorithms require solving a min-max problem at each iteration, as is commonplace in the extension of single-player value iteration to games. We consider the (generalized) policy iteration algorithm \cite{patekthesis, perolat2015approximate, tessler2019action} algorithm which is typically more efficient than value iteration schemes and work to improve its computational difficulty in the special case of two-player zero-sum discounted Markov games. The work of \cite{hansen2013strategy} provides an upper bound on this. We remark that this is an open problem itself \cite{tessler2019action, perolat2016softened, patekthesis}. We show that when the lookahead technique is used in the policy improvement step instead of greedy policies, the algorithm converges to the Nash equilibrium. Hence, our solution to the Pollatschek-Avi Itzhak problem forms the backbone of our proof techniques. 
\subsection{Main Contributions:}
\paragraph{Convergence of generalized policy iteration in Markov games}
The computational difficulties associated with extending the policy iteration algorithm to games has been a longstanding open problem. Several studies shed light on the difficulty of the policy iteration algorithm \cite{hansen2013strategy}. We present a simple modification of the well-studied naïve policy iteration or the algorithm of Pollatschek and Avi-Itzhak \cite{pollatschek1969algorithms} which converges for all discounted, simultaneous-move Markov zero-sum games. Moreover, the algorithm converges exponentially fast.   We remark that our generalized policy iteration algorithm can also be seen as a generalization of both value iteration and policy iteration in games, which is interesting in its own right. 
\paragraph{Function Approximation} We then extend the results to incorporate function approximation by studying convergence and scalability to lower dimension linear MDPs, noting recent successful results in making linear MDPs more practical using representation learning techniques \cite{ agarwal2020flambe, uehara2021representation,zhang2022making}. Prior work on approximating lookahead using MCTS show that exponential computational complexity is inevitable in problems with no structure \cite{shah2020nonasymptotic}. In contrast, our results show that the computational complexity of implementing lookahead only depends on the dimension of the feature vectors if we exploit the linear structure of the problem. 
\paragraph{Learning Algorithm} Many studies have covered the learning problem, in both model-based and model-free settings. The most relevant paper to our setting  \cite{zhang2020model} is agnostic to the planning algorithm used. However, due to the lack of any prior results on the use of policy iteration, the results in that paper will not hold if one were to use naive policy iteration because it is known to not converge in some examples. Here, combining our results with \cite{zhang2020model}, we provide a complete characterization of the sample complexity involved in model-based learning combined with generalized policy iteration for games.

\section{Related Works}
\paragraph{Value iteration based multi-agent planning algorithms} Since the introduction of a mathematical framework for Markov games in \cite{Shapley}, many works have worked to efficiently compute value-based algorithms to obtain Nash equilibrium for Markov games including the early works of \cite{littman1994markov, patekthesis, hu2003nash}.
\paragraph{Policy iteration for games} Policy iteration algorithms have been far less successfully studied as extensions of the single-player policy iteration algorithm yet are of interest to study as \cite{van1978discounted} shows that Shapley's value iteration in games is slower in practice than naive policy iteration \cite{pollatschek1969algorithms}. Among the relevant prior works includes the work of \cite{patekthesis, perolat2015approximate} which obtain convergence of the well-known policy iteration algorithms for Markov games. However, the policy iteration studied in \cite{patekthesis, perolat2015approximate} is very computationally inefficient. Furthermore, the work of \cite{pollatschek1969algorithms} following the work of \cite{hoffman1966nonterminating}   proposes an algorithm that is far more computationally efficient but they only show the algorithm converges in specific settings. The works of \cite{filar1991algorithm,breton1986computation} obtain convergence of a variant of the algorithm in \cite{pollatschek1969algorithms} under certain conditions \cite{perolat2016softened}. Recently, the works of \cite{bertsekas2021distributed, brahma2022convergence} study a variant of the algorithm in \cite{pollatschek1969algorithms} that convergence, but the dimension of vectors to be stored is quite large and the algorithms have not been shown to be extendible the function approximation setting. 
\paragraph{Model-based reinforcement learning in Markov games} The works of \cite{jia2019feature, sidford2020solving} study value-based approaches that assume the use of generative models where any state-action pair can be sampled at any time. Model-based algorithms for Markov games have been widely studied. The work of \cite{zhang2020model} studies a general setting where learning is used to estimate a model and a planning algorithm is applied to obtain the Nash equilibrium policy. Related model-based episodic algorithms that incorporate value iteration for two-player games include \cite{bai2020provable, xie2020learning} in the finite-horizon setting. The work of \cite{liu2021sharp} provides an episodic algorithm where at each iteration there is a planning step which performs an optimistic form of value iteration and there is a learning step where the outcomes of game play in the planning step are used to update the estimate of the model. 

 
\paragraph{Policy-based methods for two-player games}
The work of \cite{daskalakis2020independent} provides a decentralized algorithm for policy gradient methods that converges to a min-max equilibrium when both players independently perform policy gradient. The work of \cite{zhao2022provably} obtains convergence guarantees of natural policy gradient in two-player zero-sum games. 

\paragraph{Function approximation methods in multi-agent RL} The work of \cite{lagoudakis2012value} studies linear value function approximation in games where knowledge of the model is assumed. Previous studies including the work of \cite{xie2020learning} have looked at multi-agent games in linear MDPs. Additionally, the work of \cite{jin2022power} studies learning in multi-agent MDPs with general function approximation. 

\paragraph{Planning Oracles For Learning In MDPs} The works of \cite{gheshlaghi2013minimax, agarwal2020model, li2020breaking, jin2020reward} study reinforcement learning in a single agent setting where a planning oracle is used to obtain convergence guarantees. 

%\paragraph{Multi-agent RL For Markov Games, Sample-based Markov Game Algorithms, Policy Iteration For Markov Games, On the Use of Lookahead, AlphaZero, Oracles For Solving and Planning Of MDPs, Function approximation based MDPs, cite paper that makes linear mdps generalizable, Function Approximation based MDPs for Games}
%cite bertsekas, qiaomin, chi jin v learning, thinh doan, 

\section{Model} \label{section C}
Consider a two-player simultaneous-action zero-sum discounted Markov game. The game is characterized by $(\scriptS, \scriptA_1, \scriptA_2, P, g, \alpha)$ where $\scriptS$ denotes the state space, $\scriptA_1$ is the action space for the first player (the maximizer) with $\scriptA_1(s')$ being the action space at state $s'$, $\scriptA_2$ is the action space for the second player (the minimizer) with $\scriptA_2(s')$ being the action space at state $s'$ for the second player, $P$ is the probability transition kernel, $g$ is the reward function, and $\alpha \in (0, 1)$ is the discount factor. At each instance $i$, the state of the game is $x_i$ and the maximizer takes action $u_i$ while the minimizer takes action $v_i$. The maximizer incurs a reward of $g(x_i, u_i, v_i)$ where we assume without loss of generality that $g(x_i,u_i,v_i) \in [0,1]$ while the minimizer incurs a cost of $g(x_i, u_i, v_i)$ (and, hence a reward of $-g(x_i, u_i, v_i)$). By the end of the game, from the perspective of the maximizer, the game incurs a discounted sum of the rewards with discount factor $\alpha$ where $0<\alpha<1$, i.e., $$\sum_{i=0}^\infty \alpha^i g(x_i, u_i,v_i).$$ Meanwhile, from the perspective of the minimizer, the game incurs a discounted sum of the costs with discount factor $\alpha$ where $0<\alpha<1$, i.e., $-\sum_{i=0}^\infty \alpha^i g(x_i, u_i,v_i).$ The objective of maximizer is to take actions $u_i$ to maximize $\sum_{i=0}^\infty \alpha^i g(x_i, u_i,v_i)$ while the objective of the minimizer is to take actions $v_i$ to minimize $\sum_{i=0}^\infty \alpha^i g(x_i, u_i,v_i).$ At each instance $i$, action $u_i$ is selected following a non-deterministic policy $\mu$ where $\mu(s) \in \Delta(\scriptA_1(s))$ and action $v_i$ is selected following a non-deterministic policy $\nu$ where $\nu(s) \in \Delta(\scriptA_2(s)).$ We call the \textit{policy} of the game $(\mu,\nu)$. 

Given a policy $(\mu,\nu),$ we define the \textit{value function} corresponding to the policy componentwise as follows:
\begin{align*}
    J^{\mu,\nu}(s) = E_{u_i \sim \mu(\cdot|x_i),v_i \sim \nu(\cdot|x_i)}\Big[\sum_{i=0}^\infty \alpha^i g(x_i, u_i,v_i)|x_i = x\Big].
\end{align*}
The value function corresponding to the Nash-equilibrium policy $(\mu^*,\nu^*)$ or \textit{optimal policy} is given by: 
\begin{align*}
    J^{\mu,\nu^*}\leq J^{\mu^*,\nu^*}\leq J^{\mu^*,\nu}
\end{align*} for all policies $(\mu,\nu)$. We define $$J^* := J^{\mu^*,\nu^*}.$$
It has been shown in \cite{Shapley} that such a Nash equilibrium policy exists for all two-player discounted zero-sum Markov games. Written differently, the Nash equilibrium  policy is: $$
    (\mu^*,\nu^*) \in \argmax_{\mu}\argmin_{\nu} J^{\mu,\nu}.$$

\section{Preliminaries}\label{section D}

Consider any policy $(\mu,\nu)$. We will define the probability transition matrix $P(\mu,\nu) \in \mathbb{R}^{|\scriptS|\times|\scriptS|}$ where
$$P_{ij}(\mu,\nu) = \sum_x \sum_y \mu(x)\nu(y) P(j|i, \mu(x),\nu(y)).$$
We define the reward function $r(\mu,\nu)$ corresponding to policy $(\mu,\nu)$ componentwise as follows:
$$r_i(\mu,\nu) = \sum_{x}\sum_y \mu(x)\nu(y) g(i,x,y).$$
Using $r(\mu,\nu)$ and $P(\mu,\nu)$, we define the Bellman operator corresponding to policy $(\mu,\nu)$, $T_{\mu,\nu}:\scriptS \to \scriptS$ as follows:
$T_{\mu,\nu}J := r(\mu,\nu)+\alpha P(\mu,\nu)J.$
With some algebra, $T_{\mu,\nu}J$ can also be written as:
\begin{align*}
    T_{\mu,\nu}J = \mu(s')^\top A_{Q, s'} \nu(s'), 
\end{align*}
where $$Q(s, a_1, a_2) \leftarrow r(s, a_1, a_2)+ \alpha \sum_{s'} P(s'|s, a_1,a_2)J(s') \forall (s,a_1,a_2).$$
It is well known that $\norm{T_{\mu,\nu} J - J^{\mu,\nu}}_\infty \leq \alpha \norm{J-J^{\mu,\nu}}_\infty$, hence, iteratively applying $T_{\mu,\nu}$ yields convergence to $J^{\mu,\nu}.$

We now define the Bellman optimality operator or Bellman operator $T: \scriptS \to \scriptS$ as $TJ = \max_\mu \min_\nu (T_{\mu,\nu} J).$ It can be shown that the Bellman operator solves the matrix game at each state, i.e.,
$$TJ(s) = \displaystyle\max_{\substack{\mu\in \mathbb{R}^{|\scriptA_1(s)|} \\ \sum \mu_i=1\\0\leq \mu_i \leq 1  }} \displaystyle\min_{\substack{\nu \in \mathbb{R}^{|\scriptA_2(s)|} \\ \sum \nu_i=1\\0\leq \nu_i \leq 1  }} \mu^\top A_{Q, s} \nu,$$
where $A_{Q, s}(a_1,a_2)=  r(s, a_1, a_2)+ \alpha \sum_{s' \in \scriptD_\scriptR} P(s'|s, a_1,a_2)J(s') \forall (a_1,a_2).$ Additionally, $T$ is a pseudo-contraction towards the optimal value function $J^*$ where $\norm{TJ - J^*}_\infty \leq \alpha \norm{J-J^*}_\infty.$

We will now give a few well-known properties of the $T_{\mu, \nu}$ operator. First $T_{\mu, \nu}$ is monotone, i.e., $J \leq J' \implies T_{\mu, \nu}J \leq T_{\mu, \nu} J'$. Second, consider the vector $e \in \mathbb{R}^{|\scriptS|}$ where $e(i) = 1 \forall i \in 1, 2, \ldots, |\scriptS|.$ We have that $T_{\mu, \nu}(J + ce) = T_{\mu, \nu}J + \alpha ce \forall c \in \mathbb{R}.$

\section{Convergence Of Generalized Policy Iteration For Markov Games}

Convergence of a computationally efficient extension of policy iteration for single player systems to two-player games is an open problem \cite{bertsekas2021distributed, patekthesis}. Our generalized policy iteration algorithm for two-player games is outlined in Algorithm \ref{alg:alg 1}. 

\paragraph{Our Algorithm}
\begin{algorithm}
\caption{Generalized Policy Iteration For Two-Player Zero-Sum Discounted Markov Games}\label{alg:alg 1}
\textbf{Input}: $V,m, H.$\\
 \For {$k=0, 1, \ldots$} {\For {$i = 0, 1, \ldots, H$}{
  $Q(s, a_1, a_2) \leftarrow r(s, a_1, a_2)+ \alpha \sum_{s'} P(s'|s, a_1,a_2)V(s') \forall (s,a_1,a_2) $ \\ 
           \label{step 2 reg} $V(s') \leftarrow \displaystyle\max_{\substack{\mu\in \mathbb{R}^{|\scriptA_1(s')|} \\ \sum \mu_i=1\\0\leq \mu_i \leq 1  }} \displaystyle\min_{\substack{\nu \in \mathbb{R}^{|\scriptA_2(s')|} \\ \sum \nu_i=1\\0\leq \nu_i \leq 1  }} \mu^\top A_{Q, s'} \nu,$ where $A_{Q, s'}\in \mathbb{R}^{|\scriptA_1(s')|\times |\scriptA_2(s')|}$  and  \ \ \ \ \ \ $A_{Q, s'}(a_1,a_2)= Q(s',a_1,a_2) \forall s' ,$ with $\mu(s')$ and $\nu(s')$ set to the corresponding maximizers and minimizers, respectively \\ 
        \label{step 3 reg} }\For {$j = 0, 1, \ldots, m$}{
            $Q(s, a_1, a_2) \leftarrow r(s, a_1, a_2)+ \alpha \sum_{s'} P(s'|s, a_1,a_2)V(s') \forall (s,a_1,a_2) $ \\ 
           $V(s') \leftarrow \mu(s')^\top A_{Q, s'} \nu(s')$  for $s' $  
         }}
\end{algorithm}

The algorithm is an iterative process. At each iteration there are two steps: the policy improvement step and the policy evaluation step. In the policy improvement step, a greedy policy is determined based on the current estimate of the value function by approximately solving matrix games for the estimate of the value function. In other terms, we find the greedy policy corresponding to the estimate of the value function. Given an estimate of the value function $V$, the greedy policy at state $s' \in \scriptS$ can be determined by solving the following matrix game: 
\begin{align*}
  \displaystyle\max_{\substack{\mu\in \mathbb{R}^{|\scriptA_1(s')|} \\ \sum \mu_i=1\\0\leq \mu_i \leq 1  }} \displaystyle\min_{\substack{\nu \in \mathbb{R}^{|\scriptA_2(s')|} \\ \sum \nu_i=1\\0\leq \nu_i \leq 1  }} \mu^\top A_{Q, s'} \nu,
\end{align*} where $A_{Q, s'}\in \mathbb{R}^{|\scriptA_1(s')|\times |\scriptA_2(s')|}$  and $A_{Q, s'}(a_1,a_2)= Q(s',a_1,a_2) \forall s' ,$ where $Q(s, a_1, a_2) =r(s, a_1, a_2)+ \alpha \sum_{s' \in \scriptD_\scriptR} P(s'|s, a_1,a_2)V(s') \forall (s,a_1,a_2).$ 
Note that the computation of the max-min operation can be obtained by solving a well-known linear program \cite{rubinstein1999experience}. In other notation, the greedy policy is determined by the following:
\begin{align*}
T_{\mu,\nu}V(s') = TV(s').
\end{align*} 
In our algorithm we generalize this step to allow iteratively solving several matrix games for the estimates of the value function before the policy to be evaluated is determined. In other words, the Bellman operator is applied multiple times to the estimate of the value function before the policy to be evaluated is determined. This step can be seen as performing value iteration for a few steps before the policy to be evaluated is determined, i.e.,  
\begin{align*}
    T^H V(s') = T_{\mu,\nu}T^{H-1}V(s').
\end{align*} 

\paragraph{Remark} We note that in the case of turn-based MDPs which includes games such as chess and Go among other real world systems, the min-max computation reduces to either a maximization over $\mu$ or a minimization over $\nu.$ 

\paragraph{Remark} The process of performing the steps of value iteration before evaluating the policy is often used in real-world systems such as AlphaZero and is called ``lookahead.'' In practice, the lookahead can obtained efficiently using Monte Carlo Tree Search (MCTS).

Then, once the policy to be evaluated is determined, the policy evaluation step is analogous to that of modified policy iteration in the single player setting, sometimes called optimistic policy iteration, where the operator corresponding to the policy is applied several times to the estimate of the value function. One iteration of this operator corresponding to policy $\mu,\nu$ can be written as follows:
\begin{align*} 
    \mu(s')^\top A_{Q, s'} \nu(s')
\end{align*} where $Q(s, a_1, a_2) \leftarrow r(s, a_1, a_2)+ \alpha \sum_{s' \in \scriptD_\scriptR} P(s'|s, a_1,a_2)V(s') \forall (s,a_1,a_2).$

In other notation, in the policy evaluation step, the value function of the policy is estimated by applying the Bellman operator corresponding to the greedy policy or the multiple step greedy policy, i.e., 
\begin{align}
V_{k+1} = T_{\mu_{k+1},\nu_{k+1}}^m T^{H-1}V_k. \label{eq:alt alg}
\end{align}
Note that when this operator is applied infinitely many times, the next iterate becomes the exact value function corresponding to the policy determined in the policy improvement step. The algorithm written in the notation of \eqref{eq:alt alg} is described in Algorithm \ref{alg:alg 2}
\begin{algorithm} 
\caption{Generalized PI For Two-Player  Games (Rewritten More Compactly)}\label{alg:alg 2}
  \SetAlgoLined
\textbf{Input}: $V_0,m, H.$\\ \\
 %[1] enables line numbers
\For{$k=1,2,\ldots$}{
 Let $\tilde{V}_k = T^{H-1}V_k$ \\
 Let $\mu_{k+1},\nu_{k+1}$ be such that $\mu_{k+1},\nu_{k+1} \in \argmax_{\mu} \argmin_{\nu} T_{\mu,\nu}\tilde{V}_k$.\label{step 2 our}\\
 Approximate $J^{\mu_{k+1},\nu_{k+1}}$ as follows: $V_{k+1} = T_{\mu_{k+1}, \nu_{k+1}}^m \tilde{V}_k.$  \label{step 3 our}}
\end{algorithm}

\paragraph{Our Challenge} In order to explain the computational difficulty of obtaining convergence guarantees for Algorithm \ref{alg:alg 2}, we compare it to the well-studied policy iteration in games algorithm of \cite{patekthesis, perolat2015approximate, zhao2022provably}.
%\begin{algorithm} 
%\caption{Policy Iteration For Two-Player Zero-Sum %Games}\label{alg:alg 3}
%\textbf{Input}: $\mu_0, \nu_0.$\\
%[1] enables line numbers
%\For{ $k=0,1,2,\ldots$}{
%Let $\mu_{k+1}$ be such that $\mu_{k+1} \in %\argmax_\mu T_{\mu}J^{\mu_k,\nu_{k}},$ where $T_\mu %J:= \min_{\nu} T_{\mu,\nu}J.$ 
%\\  Let $\nu_{k+1}$ be such that $\nu_{k+1} \in %\argmin_{\nu} J^{\mu_{k},\nu}.$\label{step 2 patek}}
%\end{algorithm}
 Policy iteration in games uses techniques from the proof of policy iteration for single player systems \cite{bertsekastsitsiklis} to obtain convergence to the optimal value function. However, the monotonicity properties of the Bellman operator and the value function corresponding to the greedy policy in a single player system with a single objective (either maximization or minimization) do not easily extend to the games setting where one player attempts maximization and the other player attempts minimization. As a remedy, policy iteration in games maintains the monotonicity property by ensuring that the policy improvement operator is monotone. To do so, in the policy improvement step, the algorithm fixes the policy for the minimizer and finds a greedy policy for the maximizer. Then, in the policy evaluation step, the estimate of the optimal value function is set to smallest value function over all minimizing policies given the greedy policy for the maximizer determined in the policy improvment step. This requires that all policies for the minimizer be evaluated, adding a significant amount of computational difficulty compared to the one-player system. We wish to overcome this monotonicity difficulty in a more efficient way. In the policy improvement step of our algorithm, the policies are determined for both the minimizing operator and the maximizing operator and in the policy evaluation step, the joint policy is evaluated, analogously to the single-player system.

Our algorithm is a simple modification of the well-studied naive policy iteration algorithm, also known as the algorithm of Pollatschek and Avi-Itzhak where the policy corresponding to the Bellman optimality operator is evaluated at each iteration. While the algorithm of Pollatschek and Avi-Itzhak may seem like a more intuitive extension of policy iteration for the single player setting than policy iteration in games algorithm, the monotonicity properties in the Bellman operator in single player MDPs do not hold because the Bellman operator of the algorithm of Pollatschek and Avi-Itzhak involves a simultaneous maximization and minimization while the Bellman operator in the single player MDPs involves either a maximization or a minimization (depending on whether the algorithm analyzes rewards or costs). In fact, several counter-examples including those in the works of \cite{van1978discounted, condon1990algorithms} show that the algorithm of Pollatschek and Avi-Itzhak can diverge. As such, the question of conditions for convergence of the naive policy iteration algorithm in the algorithm of Pollatschek and Avi-Itzhak and its variants is an open question where the main challenge is how to overcome the monotonicity issue in policy iteration in games. In our work we introduce lookahead policies as opposed to traditionally used greedy policies and use several novel proof ideas to overcome the difficulty of monotonicity.

%\begin{algorithm} 
%\caption{Generalized Policy Iteration For Games}\label{alg:alg 2}
%\textbf{Input}: $\mu_0.$\\
%\begin{algorithmic}[1] %[1] enables line numbers
%\STATE Let $k=0$.
%\STATE Let $\nu_{k+1}$ be such that $\nu_{k+1} \in \argmin_{\nu} T^m_{\mu_{k},\nu}J.$\\
%\STATE Let $\mu_{k+1}$ be such that $\mu_{k+1} \in \argmax_\mu T_{\mu}J^{\mu_k,\nu_{k+1}}.$ 
%\STATE Set $k \leftarrow k+1.$ Go to 2.
%\end{algorithmic}
%\end{algorithm}

%\begin{algorithm} 
%\SetAlgoLined
%\caption{Naive Policy Iteration (Algorithm of Pollatschek and Avi-Itzhak)}\label{alg:alg 4}
%\SetAlgoLined
%\textbf{Input}: $V_0.$\\ \\
% \For{ k=1,2,\ldots}{
%  Let $\mu_{k+1},\nu_{k+1}$ be such that $\mu_{k+1},\nu_{k+1} \in \argmax_{\mu} \argmin_{\nu} T_{\mu,\nu}V_k$.\\
% Set $V_{k+1}$ to be an approximation to $J^{\mu_{k+1},\nu_{k+1}}$, i.e.,  $V_{k+1} \leftarrow T^m_{\mu_{k+1},\nu_{k+1}}V_k$.  \label{step 3 pollatschek}}
%\end{algorithm}

\paragraph{Main Result} We now state our main result, which proves convergence of a variant of policy iteration for stochastic games that does not involving solving any MDPs. Our main result hinges on the following assumption on the number of steps that must be taken before the policy evaluation begins. We remark that taking steps of lookahead is used in practice such as in algorithms like AlphaZero and that efficient algorithms such as Monte Carlo Tree Search (MCTS) are often employed to efficiently search the space of policies. 

\begin{assumption} \label{assumption 1 games}
$\alpha^{H-1}+2(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}<1.$
\end{assumption}

Our main result is the following:

\begin{theorem}\label{thm:theorem 1}
Under Assumption \ref{assumption 1 games}, the following holds for the iterates of Algorithm \ref{alg:alg 1} written in more compact notation in Algorithm \ref{alg:alg 2}:
\begin{align*}
\norm{V_k-J^*}_\infty  \leq  \Big(\alpha^{H-1}+(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha)\Big)^k\norm{V_0 - J^*}_\infty.
\end{align*}
Taking limits, it is clear that
$
V_k \to J^*,
$ where $V_k$ are the iterates of Algorithm \ref{alg:alg 2} and $J^*$ is the optimal value function.
\end{theorem}
The proof of Theorem \ref{thm:theorem 1} can be found in the Appendix. 


\paragraph{Implications Of Theorem \ref{thm:theorem 1}}
We outline the significance of Theorem \ref{thm:theorem 1} as follows:
\begin{itemize}
    \item To the best of our knowledge, our algorithm is the first variant of the well-studied algorithm of Pollatschek and Avi-Itzhak that converges without sufficient conditions on the model and does not require storing $Q$ factors. 
    \item Unlike the most commonly used extension of policy iteration to games of \cite{patekthesis, zhao2022provably, perolat2015approximate}, our algorithm does not require that any MDPs be solved at each iteration. 
    \item Our algorithm is largely a generalization of the policy iteration algorithm and value iteration algorithm in games. We provide a sufficient threshold on the number of iterations of value iteration before the policy evaluation begins. We remark that the value iteration algorithm and our variant of the policy iteration algorithm based on the algorithm of Pollatschek and Avi-Itzhak are simply special cases of our algorithm, and that studying the trade-off between the value iteration steps and the policy iteration steps is interesting in its own right. 
    \item Since the practice of taking steps of value iteration before a policy is evaluated is called ``lookahead'' which is a technique used in real world successful systems such as AlphaZero, our results explain the use of lookahead policies in practice as a remedy to the potentially more time consuming policy for iteration in games that is studied in the literature \cite{perolat2015approximate, patekthesis, zhao2022provably, zhang2020model}, among others. 
\end{itemize}

\paragraph{Proof Outline} 
Our proof can be found in the Appendix. Our proof techniques do not involve monotonicity and instead hinge on a contraction property towards the Nash equilibrium of the operator $T_{\mu,\nu,m,H}$ that we define as follows:
\begin{align*}
   T_{\mu,\nu,m,H}V := T^m_{\mu,\nu}T^{H-1}V.
\end{align*}
The use of this operator instead of the monotonicity properties used in traditional analyses in single player systems in \cite{bertsekastsitsiklis}  allows us to bypass monotonicity complications in games arising from the minimization and maximization players as opposed to single player minimization or maximization. We remark that the framework of our technique is based on the framework introduced in \cite{winnicki2023convergence}, however their setting involved a single player MDP in the context of online learning with stochastic approximation. 

However, in very large systems, function approximation techniques are necessary because of the massive sizes of state spaces. As such we consider perhaps the most basic model of function approximation, which is the linear MDP. We will show that the computations and storage required to determine the Nash equilibrium depend only on several states of the state space. 

\section{Function Approximation}

Algorithm \ref{alg:alg 5} is a rewrite of the main algorithm in Algorithm \ref{alg:alg 1} and Algorithm \ref{alg:alg 2} adapted to linear MDPs. While linear MDPs are not always a practical representation of real-world models, their algorithms often provide solutions that are independent of the sizes of the state and action spaces, providing a partial solution to the curse of generality. Additionally, recent works including \cite{agarwal2020flambe,uehara2021representation,zhang2022making} have made progress in making linear MDPs more practical through representation learning techniques. 

In our model of linear MDPs, it is assumed that the model is of the following form: 
\begin{align}
r(s,a_1,a_2) = \phi(s,a_1,a_2)\cdot \theta, \quad P(\cdot|s,a_1,a_2) = \phi(s,a_1,a_2)\cdot\eta, \label{eq: lin mdp}
\end{align} where $\theta \in \mathbb{R}^d$ and $\eta \in \mathbb{R}^{|\scriptS|\times d}.$
These assumptions are described in more detail in \cite{Agarwal2019ReinforcementLT}. We assume a set of states-actions tuples where $\sum \phi(s,a_1,a_2)\phi(s,a_1,a_2)'$ is full rank is called $\scriptD$ and the set of reachable states from tuples in $\scriptD$ is called $\scriptD_\scriptR.$
%\begin{tcolorbox}[fonttitle=\bfseries, title=Approximate Policy Iteration With Lookahead In Linear MDPs]
\begin{algorithm}
\caption{Generalized Policy Iteration With Lookahead In Linear MDPs}\label{alg:alg 5} 
\textbf{Input}: $\theta,m, H,$ feature vectors $\phi(s,a_1,a_2)$ for all $(s,a_1,a_2)$, set $\scriptD$ where $\sum_{(s,a_1,a_2)\in \scriptD} \phi(s,a_1,a_2)\phi(s,a_1,a_2)^\top$ is full-rank, set $\scriptD_\scriptR$ including the reachable states from $\scriptD$.  \\
\For{k=1,2,\ldots}{
    \For{i=0,1,\ldots,H}{
    $V(s') \leftarrow \displaystyle\max_{\substack{\mu\in \mathbb{R}^{|\scriptA_1(s')|} \\ \sum \mu_i=1\\0\leq \mu_i \leq 1  }} \displaystyle\min_{\substack{\nu \in \mathbb{R}^{|\scriptA_2(s')|} \\ \sum \nu_i=1\\0\leq \nu_i \leq 1  }} \mu^\top A_{\theta, s'} \nu,$ where $A_{\theta, s'}\in \mathbb{R}^{|\scriptA_1(s')|\times |\scriptA_2(s')|}$  and 
    \\$A_{\theta, s'}(a_1,a_2)= \phi(s',a_1,a_2)^\top \theta \forall s' \in \scriptD_\scriptR,$ with $\mu(s')$ and $\nu(s')$ set to the   corresponding maximizers and minimizers, respectively. \\ 
   $Q(s, a_1, a_2) \leftarrow r(s, a_1, a_2)+ \alpha \sum_{s' \in \scriptD_\scriptR} P(s'|s, a_1,a_2)V(s') \forall (s,a_1,a_2) \in \scriptD$. \\ 
    $\theta \leftarrow \displaystyle\argmin_\theta  \sum_{(s, a_1,a_2)\in \scriptD} [\phi(s,a_1,a_2)^\top \theta - Q(s,a_1,a_2)]^2.$ \label{ls min alg}}
    \For{j=1,\ldots,m}{
      $V(s') \leftarrow \mu(s')^\top A_{\theta, s'} \nu(s')$  for $s' \in \scriptD_\scriptR$ 
    \\ $Q(s, a_1, a_2) \leftarrow r(s, a_1, a_2)+ \alpha \sum_{s' \in \scriptD_\scriptR} P(s'|s, a_1,a_2)V(s') \forall (s,a_1,a_2) \in \scriptD$ \\ 
    \\ $\theta \leftarrow \displaystyle\argmin_\theta  \sum_{(s, a_1,a_2)\in \scriptD} [\phi(s,a_1,a_2)^\top \theta - Q(s,a_1,a_2)]^2.$}
    }
\end{algorithm}
\paragraph{Main Result}
We now describe our main result for the linear MDP setting.
\begin{theorem}\label{thm:theorem 2}
    When the model of the system is a linear MDP that follows the form of \eqref{eq: lin mdp} and is known, the $\theta$ in Algorithm \ref{alg:alg 5} converges to the optimal value function with an exponential rate of convergence. 
\end{theorem}
The proof of Theorem \ref{thm:theorem 2} is the same as that of Theorem \ref{thm:theorem 1} adjusted to the special case of linear MDPs. More details can be found in the Appendix. 
\paragraph{Implications of Theorem \ref{thm:theorem 2}}
We note the following key features of Algorithm \ref{alg:alg 5}:
\begin{itemize}
    \item The computations in Algorithm \ref{alg:alg 5} do not depend on the cardinalities of the state and action spaces and rather depend on the dimension of the feature vectors used to represent the model. 
    \item The size of vector $V$ in Algorithm \ref{alg:alg 5} is the number of states that can be visited from the representative state-actions tuples (state-actions tuples in $\scriptD$) and the size of $Q$ is effectively the number of representative state-actions tuples. It is easy to see that the computational complexity and storage requirements are independent of the sizes of the state and action spaces. To compare with Algorithm \ref{alg:alg 1}, observe the dimension of the corresponding $V$ vector is the size of the state space and the dimension of the corresponding $Q$ vector is $|\scriptS|\times|\scriptA_1|\times|\scriptA_2|.$
    \item From this algorithm, we can see that computing the lookahead does not involve  exponential computational complexity, avoiding difficulties raised in the literature.
\end{itemize}
\paragraph{Remark:} As was the case with Algorithm \ref{alg:alg 1}, the min-max computation in Algorithm \ref{alg:alg 2} can be avoided in the case of turn-based MDPs. This is because in the setting of turn-based MDPs, only one player at a time is performing either a maximization or a minimization.
\section{Learning In Games}
In multi-agent model-based reinforcement learning algorithms, the learning component has been extensively studied, and hence many straightforward extensions of our work exist to involve learning in model-based settings. We will provide one such example of an algorithm based on the learning algorithm for model-based multi-agent reinforcement learning in the work of \cite{zhang2020model}. The algorithm we study can be described as follows:

\paragraph{Learning Algorithm} The algorithm assumes knowledge of the reward function (this setting is called the \textit{reward-aware} setting) as well as access to a generator, which, at any iteration, for any state-actions tuple $(s,a_1,a_2)$ can sample from the distribution $P(\cdot|s, a_1,a_2)$ and obtain the next state. For each state-actions tuple $(s,a_1,a_2),$ the algorithm obtains $N$ samples and, based on the samples constructs an estimate of the probability transition matrix in the following manner:
\begin{align*}
    \hat{P}(s'|s,a_1,a_2) := \frac{\text{count}(s',s,a_1,a_2)}{N},
\end{align*} the way the estimate of the probability transition matrix is computed in \cite{zhang2020model}. Using $\hat{P}$ and the known reward function, the algorithm finds the Nash equilibrium policy using Algorithm \ref{alg:alg 1}, $(\hat{\mu},\hat{\nu})$. 
\paragraph{Learning Result}
The following theorem gives a bound on the sample and computational complexity required to achieve an error bound on $\norm{Q^{\hat{\mu},\hat{\nu}}-Q^*}_\infty$ in linear turn-based MDPs where $\phi(s, a_1,a_2) \in \mathbb{R}^d$ and the number of reachable states from state-actions tuples in $\scriptD$ is $r.$

\begin{theorem}\label{thm:theorem 3}
    Consider any $\epsilon, \delta, \epsilon_{opt}>0$ with $\epsilon \in (0, 1/(1-\alpha)^{1/2}]$. When the number of samples of each state-actions tuple is at least $N$ and the number of computations that are made in the planning step where the Nash equilibrium policy is determined based on the model inferred from the samples is at least $C$ where
    
\begin{align*}
 &N \geq \frac{c \times \alpha \times \log\big[ c |\scriptS||\scriptA_1||\scriptA_2|(1-\alpha)^{-2}\delta^{-1}\big]}{(1-\alpha)^3 \epsilon^2} \\
    &C \geq \frac{c \times m\times H\times\log\Big[ \frac{1}{\epsilon_{opt}(1-\alpha)}\Big]}{\log[\frac{1}{\tilde{\alpha}}]}\Bigg[ d[2r+1]+d^3/3+r|\scriptA|_{max}^2 d \Bigg],
\end{align*} where $c$ is a constant, $\tilde{\alpha}=\alpha^{H-1}+(1+\alpha^m)\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha),$ $|\scriptA_1|$ and $|\scriptA_2|$ are the numbers of the total actions available to players 1 and 2, and $|\scriptA|_{max}$ is the largest number of actions available at a state, it holds that with probability at least $1-\delta,$ 
\begin{align*}
    &\norm{Q^{{\hat{\mu},\hat{\nu}}}-Q^*}_\infty \leq \frac{2\epsilon}{3}+\frac{5 \alpha \epsilon_{opt}}{1-\alpha} \\& \norm{\hat{Q}^{\hat{\mu},\hat{\nu}}-Q^*}_\infty \leq \epsilon + \frac{9\alpha \epsilon_{opt}}{1-\alpha}.
\end{align*}
\end{theorem}
The proof of Theorem \ref{thm:theorem 3} uses the results of \cite{zhang2020model} and extensions from Theorem \ref{thm:theorem 1} and can be found in the Appendix. Theorem \ref{thm:theorem 3} overall gives a bound on the the error of the learning algorithm as a function of the number of computations in the planning step and the number of samples in the learning step. We note that convergence of the learning algorithm in the present section does not require solving an MDP at each iteration the way that many model-based policy iteration algorithms do. In some ways, Theorem \ref{thm:theorem 3} provides a trade-off between sample complexity in the learning step and computational complexity in the planning step. 
\section{Future Work}
In our work, we study the model-based learning problem and focus on the planning step of the problem using known learning results to provide results for model-based policy iteration for two-player zero-sum simultaneous discounted games. 


Some interesting directions of future work include: extending the results to the stochastic shortest path games problem where there is no discount factor. See the work of \cite{patekthesis} for more on the stochastic shortest path games setting (2) deriving bounds for more general classes of function approximation settings. 






% Acknowledgments---Will not appear in anonymized version




%\acks{We thank a bunch of people and funding agency.}

\bibliography{refs}
\appendix


\section{Proof of Theorem \ref{thm:theorem 1}}

\begin{comment}
We first provide some extra notation for the proof of Theorem \ref{thm:theorem 1}. First, as previously stated, we denote by $e$ a column vector of all 1's. Second, we define the Q-functions as follows:
\begin{align*}
    Q^{\mu,\nu}(s,a_1,a_2) = E_{u_i \sim \mu(\cdot|x_i),v_i \sim \nu(\cdot|x_i),i\neq 0}\Big[\sum_{i=0}^\infty \alpha^i g(x_i, u_i,v_i)|x_0 = s,u_0 = a_1, v_0 = a_2\Big].
\end{align*} 

Note the connection with the value function corresponding to policy $(\mu,\nu)$ as:
\begin{align*}
    V^{\mu,\nu}(s) = E_{u_i \sim \mu(\cdot|x_i),v_i \sim \nu(\cdot|x_i)}\Big[\sum_{i=0}^\infty \alpha^i g(x_i, u_i,v_i)|x_0 = x\Big].
\end{align*}
From the above definitions, it follows that
\begin{align*}
V^{\mu,\nu}(s) = E_{a_1 \sim \mu(\cdot|s),a_2 \sim \nu(\cdot|s)}Q^{\mu,\nu}(s,a_1,a_2) = \mu(s)^\top Q^{\mu,\nu}(s) \nu(s),
\end{align*} where $Q_{a_1,a_2}^{\mu,\nu}(s)=Q^{\mu,\nu}(s,a_1,a_2)$ and that 
\begin{align*}
Q^{\mu,\nu}(s,a_1,a_2) &= g(s,a_1,a_2) +\alpha E_{a_1 \sim \mu(\cdot|s),a_2 \sim \nu(\cdot|s),s'\sim P(\cdot,s,a_1,a_2)}[V^{\mu,\nu}(s')]
\end{align*}
Equivalently,
\begin{align*}
    Q^{\mu,\nu}(s,a_1,a_2) = g(s,a_1,a_2) +\alpha  \sum_{a_1} \sum_{a_2}\sum_{s'}P(s'|s,a_1,a_2)V^{\mu,\nu}(s')\mu(a_1)\nu(a_2)
\end{align*}

%We can now define $T_{\mu,\nu}$ using a generic Q operator where:
%\begin{align*}
 %    QV(s) = r(s,a_1,a_2)+\alpha \sum_{s'}P(s'|s,a_1,a_2)V(s').
%\end{align*}



Let $T_{\mu,\nu}$ be the operator defined by
$$T_{\mu,\nu}J := r(\mu,\nu)+\alpha P(\mu,\nu)J$$ with $$P_{ij}(\mu,\nu) = \sum_x \sum_y \mu(x)\nu(y) P(j|i, \mu(x),\nu(y))$$ and 
$$r_i(\mu,\nu) = \sum_{x}\sum_y \mu(x)\nu(y) g(i,x,y)$$ 
alternatively as the following:

\begin{align*}
    T_{\mu,\nu}J = \mu(s')^\top A_{Q, s'} \nu(s'), 
\end{align*}
where $$Q(s, a_1, a_2) \leftarrow r(s, a_1, a_2)+ \alpha \sum_{s'} P(s'|s, a_1,a_2)J(s') \forall (s,a_1,a_2).$$

As is well known that $$\norm{T_{\mu,\nu} J - J^{\mu,\nu}}_\infty \leq \alpha \norm{J-J^{\mu,\nu}}_\infty,$$ we can iteratively apply $T_{\mu,\nu}$ to get an estimate with of $J^{\mu,\nu}$ as follows. For $m$ iterations, we have the following: 
$T^m_{\mu,\nu}V$ can be obtained through the following procedure where $A_{Q, s'}(a_1,a_2)= Q(s',a_1,a_2) \forall s':$
\begin{algorithm}
\caption{$T_{\mu,\nu}^m(V)$ computations}

\For {$j = 0, 1, \ldots, m$}{
            $Q(s, a_1, a_2) \leftarrow r(s, a_1, a_2)+ \alpha \sum_{s'} P(s'|s, a_1,a_2)V(s') \forall (s,a_1,a_2) $ \\ 
           $V(s') \leftarrow \mu(s')^\top A_{Q, s'} \nu(s')$  for $s' $  
         }
\end{algorithm} 

Now consider the optimal value function $J^*$, and corresponding optimal policy $(\mu^*,\nu^*),$ i.e., 
\begin{align*}
   J^* := J^{\mu^*,\nu^*}
\end{align*} and  
\begin{align*}
    J^{\mu,\nu^*}\leq J^{\mu^*,\nu^*}\leq J^{\mu^*,\nu}
\end{align*} for all policies $(\mu,\nu)$. 

Now, recall our previous definition of the Bellman operator $T$ as follows:
$$TJ = \max_\mu \min_\nu (T_{\mu,\nu} J)$$ where the $(\mu,\nu)$ is called the greedy policy.

Using the above and the Q function, we can write $TJ(s)$ as follows.
$$TJ(s) = \displaystyle\max_{\substack{\mu\in \mathbb{R}^{|\scriptA_1(s)|} \\ \sum \mu_i=1\\0\leq \mu_i \leq 1  }} \displaystyle\min_{\substack{\nu \in \mathbb{R}^{|\scriptA_2(s)|} \\ \sum \nu_i=1\\0\leq \nu_i \leq 1  }} \mu^\top A_{Q, s} \nu.$$
In our algorithm we find the $(\mu,\nu)$ greedy policy after taking $H$ repetitions of the Bellman operator $T$. The step of taking $T^H V$ can be written in Algorithm \ref{alg:alg 7}.
The idea is that since $T$ is known to be a contraction towards the optimal value function $J^*$ where $\norm{TJ - J^*}_\infty \leq \alpha \norm{J-J^*}_\infty,$ the ``lookahead'' can give us a better policy $(\mu,\nu)$. Putting the above together, it is easy to see the equivalence of Algorithms \ref{alg:alg 1} and \ref{alg:alg 2}. 


\begin{algorithm}\label{alg:alg 7}
\caption{$T^H(V)$ computations}
\For {$i = 0, 1, \ldots, H$}{
  $Q(s, a_1, a_2) \leftarrow r(s, a_1, a_2)+ \alpha \sum_{s'} P(s'|s, a_1,a_2)V(s') \forall (s,a_1,a_2) $ \\ 
            $V(s') \leftarrow \displaystyle\max_{\substack{\mu\in \mathbb{R}^{|\scriptA_1(s')|} \\ \sum \mu_i=1\\0\leq \mu_i \leq 1  }} \displaystyle\min_{\substack{\nu \in \mathbb{R}^{|\scriptA_2(s')|} \\ \sum \nu_i=1\\0\leq \nu_i \leq 1  }} \mu^\top A_{Q, s'} \nu,$ where $A_{Q, s'}\in \mathbb{R}^{|\scriptA_1(s')|\times |\scriptA_2(s')|}$  and   \\$A_{Q, s'}(a_1,a_2)= Q(s',a_1,a_2) \forall s' $   }
\end{algorithm}
\end{comment}
The proof of Theorem \ref{thm:theorem 1} relies on showing a contraction property of the $T_{\mu,\nu,mH}$ operator. 
\bigskip
\begin{proof}
We will show that $T_{\mu,\nu,mH}$ is a contraction towards $J^*$ for all $m$ and $H$, including $m=\infty$ and all $H\geq 1.$ We note that this only holds for sufficiently large $H$ per Assumption \ref{assumption 1 games}. 

First, we have the following:
Since $T_{\mu_{k+1},\nu_{k+1}}V$ is defined as $r(\mu_{k+1},\nu_{k+1})+\alpha P(\mu_{k+1},\nu_{k+1})V,$ we can directly apply the contraction property from single player MDPs to see that:
\begin{align*}
\norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-J^{\mu_{k+1},\nu_{k+1}}}_\infty &= \norm{T_{\mu_{k+1},\nu_{k+1}}^mT^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty \\&\leq \alpha^m \norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty.
\end{align*}
Thus, the following holds:
\begin{align}
  \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-T^{H-1}V_k}_\infty\nonumber &=  \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k- J^{\mu_{k+1},\nu_{k+1}}+J^{\mu_{k+1},\nu_{k+1}}  -T^{H-1}V_k}_\infty \\ \nonumber &\leq \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k- J^{\mu_{k+1},\nu_{k+1}}}_\infty+\norm{J^{\mu_{k+1},\nu_{k+1}}  -T^{H-1}V_k}_\infty  \\
 \nonumber &\leq \alpha^m \norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty+\norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty \\
  &= (1+\alpha^m)\norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty.\label{eq:three}
\end{align}

We will now attempt to obtain a bound on $\norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty$. 
Here, we bypass a crucial monotonicity property of $T$ for single player systems that is not present in games. We  have by definition of $T_{\mu_{k+1},\nu_{k+1}}$ the following for all $\ell$:
\begin{align*}
\norm{T_{\mu_{k+1},\nu_{k+1}}^{\ell+1}T^{H-1}V_k-T_{\mu_{k+1},\nu_{k+1}}^{\ell}T^{H-1}V_k}_\infty  &\leq \alpha^\ell \norm{TV_k-V_k}_\infty \\
\end{align*}

To start, we will need the following pseudo-contraction property of $T$ the optimal value function \cite{bertsekastsitsiklis}:
\begin{align*}
    \norm{TV-J^*}_\infty \leq \alpha\norm{V-J^*}_\infty.
\end{align*}
Since $T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k = T^H V_k$ and using the property of $T$, we have the following:
\begin{align*}
    \norm{T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k - T^{H-1} V_k}_\infty &=  \norm{T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k -J^* +J^*- T^{H-1} V_k}_\infty\\
    &\leq \norm{T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k -J^*}_\infty +\norm{J^*- T^{H-1} V_k}_\infty \\
    &= \norm{T^{H}V_k -J^*}_\infty +\norm{J^*- T^{H-1} V_k}_\infty \\
    &\leq \alpha^H \norm{V_k - J^*}_\infty + \alpha^{H-1}\norm{J^*-V_k}_\infty \\
    &= \underbrace{(\alpha^H + \alpha^{H-1})\norm{J^*-V_k}_\infty}_{=: \tilde{a}}.
\end{align*}

Thus,
\begin{align}
    -T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k\leq  - T^{H-1} V_k + \tilde{a}.
\end{align}

Suppose that we apply the $T_{\mu_{k+1},\nu_{k+1}}$ operator $\ell-1$ times to both sides. Then, due to monotonicity and the fact $T_{\mu,\nu}(J+ce)=T_{\mu,\nu}(J)+\alpha ce,$ for any policy $(\mu,\nu),$ we have the following:
\begin{align*}
    {T^\ell_{\mu_{k+1},\nu_{k+1}}} T^{H-1}V_k \leq \alpha^{\ell } \tilde{a}e + {T^{\ell+1}_{\mu_{k+1},\nu_{k+1}}}T^{H-1}V_k.
\end{align*}
Using a telescoping sum, we get the following inequality:
\begin{align*}
    T_{\mu_{k+1},\nu_{k+1}}^j T^{H-1} V_k - T^{H-1}V_k
    &\geq - \sum_{\ell = 1}^{j} \alpha^{\ell - 1} \tilde{a} e.
\end{align*}
Taking the limit as $j\rightarrow\infty$ on both sides, we have the following:
\begin{align}
    J^{\mu_{k+1},\nu_{k+1}} - T^{H-1}V_k \geq - \frac{\tilde{a} e}{1-\alpha}.
\label{eq:one}
\end{align}
In the other direction, we have the following:
\begin{align}
     - T^{H-1} V_k\leq -T_{\mu_{k+1},\nu_{k+1}}T^{H-1}V_k + \tilde{a}.
\end{align}
Applying the $T_{\mu_{k+1},\nu_{k+1}}$ operator $\ell-1$ times to both sides, we have :
\begin{align*}
    {T^{\ell+1}_{\mu_{k+1},\nu_{k+1}}} T^{H-1}V_k \leq \alpha^{\ell } \tilde{a}e + {T^{\ell}_{\mu_{k+1},\nu_{k+1}}}T^{H-1}V_k.
\end{align*}
Using a telescoping sum, we get the following inequality:
\begin{align*}
   T^{H-1}V_k- T_{\mu_{k+1},\nu_{k+1}}^j T^{H-1} V_k 
    &\geq - \sum_{\ell = 1}^{j} \alpha^{\ell - 1} \tilde{a} e.
\end{align*}
Taking the limit as $j\rightarrow\infty$ on both sides, we have the following:
\begin{align}
   T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}  \geq - \frac{\tilde{ae} }{1-\alpha}.
   \label{eq:two}
   \end{align}
Hence, putting inequalities \eqref{eq:one} and \eqref{eq:two} together, we have the following bound:
\begin{align*}
    \norm{T^{H-1}V_k - J^{\mu_{k+1},\nu_{k+1}}}_\infty \leq \frac{\tilde{a} }{1-\alpha}.
\end{align*}

We plug this bound into our result in \eqref{eq:three} to get:
\begin{align}
  \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-T^{H-1}V_k}_\infty\nonumber \nonumber&\leq \frac{\tilde{a} e}{1-\alpha} \\&=  \frac{(1+\alpha^m)(\alpha^H + \alpha^{H-1})\norm{J^*-V_k}_\infty }{1-\alpha}.
\label{eq:four} \end{align}

We now provide the reverse triangle inequality which we will use in the next step: \begin{align*}
    \norm{X-Y}_\infty - \norm{Y-Z}_\infty \leq \norm{X-Z}_\infty \forall X,Y,Z.
\end{align*} Using the reverse triangle inequality we have the following bound:
\begin{align*}
\norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k - J^*}_\infty - \norm{T^{H-1}V_k-J^*}\leq \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-T^{H-1}V_k}_\infty.
\end{align*}

Now, we use the pseudo-contraction property of $T$ towards $J^*$ as follows:
\begin{align*} \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k - J^*}_\infty &\leq \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-T^{H-1}V_k}_\infty +  \norm{J^*-T^{H-1}V_k}_\infty\\
&\leq \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-T^{H-1}V_k}_\infty + \alpha^{H-1} \norm{J^*-V_k}_\infty \\
&\leq \frac{(1+\alpha^m)(\alpha^H + \alpha^{H-1})\norm{J^*-V_k}_\infty }{1-\alpha}+\alpha^{H-1} \norm{J^*-V_k}_\infty, 
\end{align*}
where in the last line we plug in our bound in \eqref{eq:four}.

Hence, the following holds:

\begin{align*}
  \norm{T_{\mu_{k+1},\nu_{k+1},m,H}V_k-J^*}_\infty  \leq  \Big(\alpha^{H-1}+(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha)\Big)\norm{V_k - J^*}_\infty.
\end{align*}

Noting that $V_{k+1} = T_{\mu_{k+1},\nu_{k+1},m,H}V_k,$ we iterate to get the following bound:
\begin{align*}
\norm{V_k-J^*}_\infty  \leq  \Big(\alpha^{H-1}+(1+\alpha^m )\frac{\alpha^{H-1}}{1-\alpha}(1+\alpha)\Big)^k\norm{V_0 - J^*}_\infty,
\end{align*} and, further using Assumption \ref{assumption 1 games}, we have that $V_k \to J^*,$ which proves the exponential rate of convergence of our algorithm.

\end{proof}
The framework of the techniques we use are based on those of the work of \cite{winnicki2023convergence} however unlike the work of \cite{winnicki2023convergence} the setting of the problem is a two-player game in a deterministic setting as opposed to online learning with stochastic approximation for a single-player system.
\section{Theorem \ref{thm:theorem 2} Details}
Theorem \ref{thm:theorem 2} is Theorem \ref{thm:theorem 1} specialized to the case of linear MDPs. To illustrate the convergence of Theorem \ref{thm:theorem 2}, it is sufficient to show that Algorithm \ref{alg:alg 5} is simply Algorithm \ref{alg:alg 1} specialized to the case of linear MDPs. We will show that in the case of linear MDPs, there exists a $\beta$ at each iteration such that we can rewrite $TV(s)$ for $s \in \scriptD$ in terms of $\beta$. To see this, observe the following:

\begin{align}
    TV(s) &= \min \max_{a_1,a_2} r(s,a_1,a_2)+\alpha \sum_{s'}P(s'|s,a_1,a_2)V(s') \label{eq: beta MG}\\
    &= \min\max_{a_1,a_2} \phi(s,a_1,a_2)^\top \theta + \alpha \sum_{s'}[\phi(s,a_1,a_2)^\top \eta(s') V(s')] \nonumber\\
    &= \min\max_{a_1,a_2} \phi(s,a_1,a_2)^\top \underbrace{[\theta+\alpha \sum_{s'} \eta(s')V(s')]}_{=: \beta}\nonumber,
\end{align} hence, for some $\beta \in \mathbb{R}^d,$ we can write $TV(s)$ in terms of $\beta$ as follows:
\begin{align}
    TV(s) = \min\max_{a_1,a_2} \phi(s,a_1,a_2)^\top \beta. \label{eq:take}
\end{align} 
Furthermore, we note that from \eqref{eq: beta MG}, it is clear that $TV(s)$ need not be evaluated for all states $s \in \scriptS$ since determining $TV(s)$ for $s \in \scriptD$ requires $V(s')$ only for states $s' \in \scriptD_\scriptR.$ 
%Notice that in order to compute $\beta$, we need not compute $\beta =\theta+\alpha \sum_{s'} \eta(s')V(s')$. Instead, we can use Q factors defined as follows:
%\begin{align*}
%    QV(s,a_1,a_2) := \phi(s,a_1,a_2)^\top \beta.
%\end{align*}
%Notice that $TV(s) = \min\max_{a_1,a_2} QV(s,a_1,a_2).$ So, we need only obtain $\beta$ to obtain $TV(s)$ for any state $s$. While it is possible to compute $\beta$ by taking $\theta+\alpha \sum_{s'} \eta(s')V(s'),$ doing so is computationally efficient and instead, for a set of states-actions tuples $\scriptD$ where $$\sum_{(s,a_1,a_2)\in \scriptD} \phi(s,a_1,a_2)\phi(s,a_1,a_2)^\top$$ is full rank, we can compute the following:
%\begin{align*}
%     QV(s) = r(s,a_1,a_2)+\alpha \sum_{s'}P(s'|s,a_1,a_2)V(s')
%\end{align*} using knowledge of the model and then perform the least squares minimization in Algorithm \ref{alg:alg 5} to obtain  $\beta,$ which allows us to compute $TV(s)$ using \eqref{eq:take} for any state $s$. However, in computing $T^HV(s)=T(T(\ldots(TV)))$, notice that we need only compute $\beta$ corresponding to $TV$ and using that $\beta$, obtain the next $\beta$ corresponding to $T\tilde{V}$ where $\tilde{V} := TV$. Hence, we need only compute the sequence of $\beta$. Notice that we need not recover an entire $TV(s)$ vector for all states $s$.
%Instead, it can be seen that in order to compute the sequence of $\beta$ for any given value of $V$, we need only compute $TV(s)$ for states $s$ that are reachable from any of the tuples $(s,a_1,a_2)$ in set $\scriptD.$

\section{Proof of Theorem \ref{thm:theorem 3}}
We note that the settings of the bound in \cite{zhang2020model} are the same as the settings of our work, so we can directly apply the bounds in \cite{zhang2020model}. We will restate Theorem 3.3 from \cite{zhang2020model} in Lemma \ref{lemma 1 games}:

\begin{lemma}\label{lemma 1 games}
    Consider any $\epsilon, \delta>0$ with $\epsilon \in (0, 1/(1-\alpha)^{1/2}]$ and $\delta \in [0,1]$. When the number of samples of each state-actions tuple is at least $N$ by the learning oracle and a planning oracle is used based on the empirical model $\hat{\mathcal{G}}$ which is reward-aware and the entries are of the probability transition matrix is constructed by taking averages determined by the learning oracle to determine a policy $(\hat{\mu},\hat{\nu})$ where:
    \begin{align*}
        \norm{\hat{V}^{\hat{\mu},\hat{\nu}}-\hat{J}^*}_\infty \leq\epsilon_{opt},
    \end{align*} where $\hat{J}^*$ is the optimal value function for model $\hat{\mathcal{G}}$, when 
\begin{align*}
 &N \geq \frac{c \alpha \log\big[ c |\scriptS||\scriptA_1||\scriptA_2|(1-\alpha)^{-2}\delta^{-1}\big]}{(1-\alpha)^3 \epsilon^2} 
\end{align*} where for some absolute constant $c$, it holds that with probability at least $1-\delta,$ 
\begin{align*}
    \norm{Q^{{\hat{\mu},\hat{\nu}}}-Q^*}_\infty \leq \frac{2\epsilon}{3}+\frac{5 \alpha \epsilon_{opt}}{1-\alpha}, \quad \norm{\hat{Q}^{\hat{\mu},\hat{\nu}}-Q^*}_\infty \leq \epsilon + \frac{9\alpha \epsilon_{opt}}{1-\alpha}.
\end{align*}
\end{lemma}

Now, we compute the complexity as follows:
It is easy to see that to compute $Q(s, a_1, a_2) \leftarrow r(s, a_1, a_2)+ \alpha \sum_{s' \in \scriptD_\scriptR} P(s'|s, a_1,a_2)V(s') \forall (s,a_1,a_2) \in \scriptD$ requires $d[2r+1]$ computations, computing $\theta \leftarrow \displaystyle\argmin_\theta  \sum_{(s, a_1,a_2)\in \scriptD} [\phi(s,a_1,a_2)^\top \theta - Q(s,a_1,a_2)]^2$ requires $d^3/3$ computations (as an upper bound), computing $V(s') \leftarrow \displaystyle\max_{\substack{\mu\in \mathbb{R}^{|\scriptA_1(s')|} \\ \sum \mu_i=1\\0\leq \mu_i \leq 1  }} \displaystyle\min_{\substack{\nu \in \mathbb{R}^{|\scriptA_2(s')|} \\ \sum \nu_i=1\\0\leq \nu_i \leq 1  }} \mu^\top A_{\theta, s'} \nu,$ needs $r\times|\scriptA|_{max}^2\times d$ computations noting that there is a turn-based MDP which is well known to involve only deterministic policies, and since the $\mu^\top A_{\theta, s'} \nu$ is a special case of the previous step, no more additional computations are needed to determine an upper bound. Using the above, some algebra gives Theorem \ref{thm:theorem 3}. 



\end{document}
