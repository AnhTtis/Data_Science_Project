
\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{subfig}
\usepackage{graphicx}


\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output

\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfr}{\mathbf{r}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfh}{\mathbf{h}}
\newcommand{\bfq}{\mathbf{q}}
\newcommand{\bfg}{\mathbf{g}}
\newcommand{\bfc}{\mathbf{c}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfxx}{\mathbf{\hat{x}}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfs}{\mathbf{s}}
\newcommand{\bfp}{\mathbf{p}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bfW}{\mathbf{W}}
\newcommand{\gnet}{$\gamma$-Net~}
\newcommand{\acc}{acc~}
\newcommand{\sacc}{SACC~}
\newcommand{\sece}{SECE~}
\newcommand{\dece}{dECE~}
\usepackage{amsthm}
\newtheorem{corollary}{Corollary}


\title{Towards Unbiased Calibration using Meta-Regularization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Cheng Wang\textsuperscript{$\dagger$} \email cwngam@amazon.com\\
      \addr Amazon\\
      Berlin, Germany
      \AND
      \name Jacek Golebiowski\textsuperscript{$\dagger$} \email jacekgo@amazon.com \\
      \addr Amazon\\
      Berlin, Germany}


% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{06}  % Insert correct month for camera-ready version
\def\year{2024} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=Yf8iHCfG4W}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Model miscalibration has been frequently identified in modern deep neural networks. Recent work aims to improve model calibration directly through a differentiable calibration proxy. However, the calibration produced is often biased due to the binning mechanism.
In this work, we propose to learn better-calibrated models via meta-regularization, which has two components: (1) gamma network ($\gamma$-Net), a meta learner that outputs sample-wise gamma values (continuous variable) for Focal loss for regularizing the backbone network; (2) smooth expected calibration error (SECE), a Gaussian-kernel based, unbiased, and differentiable surrogate to ECE that enables the smooth optimization of $\gamma$-Net. We evaluate the effectiveness of the proposed approach in regularizing neural networks towards improved and unbiased calibration on three computer vision datasets. We empirically demonstrate that: (a) learning sample-wise $\gamma$ as continuous variables can effectively improve calibration; (b) \sece smoothly optimizes \gnet towards unbiased and robust calibration with respect to the binning schemes; and (c) the combination of \gnet and \sece achieves the best calibration performance across various calibration metrics while retaining very competitive predictive performance as compared to multiple recently proposed methods.
\end{abstract}


\renewcommand\thefootnote{$\dagger$}
\footnotetext{Equal contribution.}
\section{Introduction}
Deep Neural Networks (DNNs) have demonstrated promising predictive performance in various domains, including computer vision~\citep{krizhevsky2012imagenet}, speech recognition~\citep{graves2013speech}, and natural language processing~\citep{vaswani2017attention}. Consequently, trained deep neural network models are frequently deployed and utilized in real-world systems. However, recent work~\citep{guo2017calibration} has pointed out that these highly accurate, negative log likelihood-trained deep neural networks are often poorly calibrated~\citep{niculescu2005predicting}. Their predicted class probabilities do not accurately estimate the true probability of correctness, resulting in primarily overconfident and under-confident predictions. Deploying such miscalibrated models in real-world systems poses significant risk, particularly when model outputs are directly utilized to serve customers' requests in applications such as medical diagnosis~\citep{caruana2015intelligible} and autonomous driving~\citep{bojarski2016end}. Better-calibrated model probabilities can serve as an essential signal toward building more reliable machine learning systems.

A recent trend aims to learn a calibrated model by training it to minimize errors on calibration metrics. One representative work is from~\citet{kumar2018trainable}, where they developed a differentiable equivalent of Expected Calibration Error (ECE), the Maximum Mean Calibration Error (MMCE). \citet{mukhoti2020calibrating} found that focal loss~\citep{lin2017focal}, as a good alternative to standard cross-entropy, can effectively improve calibration. They also highlighted the crucial role of the gamma parameter in focal loss in making this approach effective and proposed a sample-dependent schedule based on heuristics (Lambert-W function~\citep{corless1996lambertw}) for the gamma in focal loss (FLSD), which showed superior calibration performance compared to baselines. \citet{bohdal2021meta,bohdal2023meta} proposed using a meta-learning-based approach termed as meta-calibration. In their formulation, the backbone network learns to optimize a standard cross-entropy loss, while a differentiable proxy for calibration error (i.e., DECE) is used to tune the parameters of the weight regularizer.
\begin{figure}
\centering
%\vspace{-3mm}
\includegraphics[width=0.65\textwidth]{figure/method.pdf}
\caption{Our proposed approach for regularizing the base network towards better calibration includes two new components: \gnet and \sece. The inner loop optimizes the backbone network (e.g., ResNet), which uses focal loss as an objective function. The \gnet in the outer loop takes the extracted second-to-last layer representation of backbone network as input and learns to output sample-wise $\gamma$ for focal loss in a continuous space. The \gnet is optimized by using the proposed SECE, a Gaussian kernel-based, unbiased, and differentiable calibration error.}
\label{fig: method} 
\end{figure}

Motivated by advancements in this area, we introduce a learnable approach that involves learning sample-wise $\gamma$ values via a meta-network, referred to as the gamma network (i.e., $\gamma$-Net), as illustrated in Figure~\ref{fig: method}. The key distinctions from Focal loss~\citep{lin2017focal} and FLSD~\citep{mukhoti2020calibrating}, where a global $\gamma$ parameter (e.g., setting $\gamma=3$ or $\gamma=5$) is employed as a fixed parameter or scheduled across training epochs, are twofold: our approach (1) learns more finely-grained gamma values at the sample level; (2) the learned $\gamma$ values are continuous variables. The output of the $\gamma$-Net, namely the learned $\gamma$ values, is then utilized to regularize a backbone network towards improved calibration. The optimization of the $\gamma$-Net is achieved with a proposed differentiable surrogate for expected calibration error (ECE)~\citep{NaeiniETAL:15}: Smooth ECE (SECE), an unbiased Gaussian kernel based ECE estimator. SECE optimizes \gnet towards unbiased calibration by avoiding the binning step as in~\citep{zhang2020_mixmatch}.

Our contributions can be summarized as follows:
\begin{itemize}
\item We propose $\gamma$-Net, a meta-network designed to learn sample-wise $\gamma$ values for Focal loss as continuous variables, rather than relying on a single pre-defined $\gamma$ value. To the best of our knowledge, our proposed method learns the most fine-grained $\gamma$ values for Focal loss.
\item We propose a kernel-based ECE estimator,\sece, which is a variation of the kernel density estimate (KDE) approximation~\citep{zhang2020_mixmatch}. This estimator smoothly regularizes \gnet towards improved and unbiased calibration.
\item Through extensive experiments and analysis, we empirically demonstrate that \gnet effectively calibrates models, while \sece provides stable and smooth calibration. The combination of both achieves competitive predictive performance and superior scores across multiple calibration metrics compared to the baselines.
\end{itemize} 

\section{Related Work}
Calibration of machine learning (ML) models using Platt scaling~\citep{platt1999probabilistic} and Isotonic Regression~\citep{zadrozny2002transforming} has shown significant improvements for SVMs and decision trees. With the advent of neural networks, ~\citet{Niculescu-MizilETAL:05} showed that those methods can produce well-calibrated probabilities even without any dedicated modifications.

Recently, \citet{guo2017calibration} and \citet{mukhoti2020calibrating} have shown that modern NNs are noticeably more accurate but poorly calibrated due to negative log likelihood (NLL) overfitting. \citet{minderer2021revisiting} revisited this problem and found that architectures with large sizes have a larger effect on calibration, but nevertheless, more accurate models tend to produce less calibrated predictions.

One of the ways to tackle calibration is to rely on post-hoc corrections using non-parametric approaches such as histogram binning~\citep{zadrozny2001obtaining}, isotonic regression~\citep{zadrozny2002transforming} and parametric methods such as Bayesian binning into quantiles (BBQ) and Platt scaling~\citep{platt1999probabilistic}. Beyond those four, temperature scaling (TS) is a single-parameter extension of Platt scaling~\citep{platt1999probabilistic} and the most recent addition to the offering of post-hoc methods. Recent work has shown that a single parameter TS leads to good calibration performance with minimal added computational complexity~\citep{guo2017calibration,minderer2021revisiting}. There have been multiple extensions to temperature scaling in recent years. ~\citet{kull2019beyond} proposed Dirichlet calibration which assumes probability distributions are parameterized Dirichlet distributions, and an attention-based mechanism was proposed to tackle noise in validation data~\citep{https://doi.org/10.48550/arxiv.1810.11586}.  \citet{laves2019well} extended TS to dropout variational inference to calibrate model uncertainty by inserting temperature value in Monte Carlo integration.

Beyond post-hoc methods, other approaches do not explicitly include the calibration objective but rather implicitly guide the training towards better calibration performance. There are basically two lines of work:  (1) data augmentation-based methods and (2) regularization-based methods. For the former line, the representative methods include label smoothing (LS)~\citep{muller2019does} that mitigates miscalibration by softening hard labels with an introduced smoothing modifier in the standard loss function (e.g., cross-entropy). Mix-up training~\citep{thulasidasan2019mixup} that extends \textit{mixup}~\citep{zhang2018mixup} to generate synthetic samples during model training by combining two random elements from the dataset. 

Extending the portfolio of regularization-based methods for calibration, focal loss~\citep{lin2017focal} has been used as a maximum entropy regularizer. One of its extensions, FLSD~\citep{mukhoti2020calibrating} computes the gamma value for focal loss based on heuristics (Lambert-W function~\citep{corless1996lambertw}), dual Focal~\citep{tao2023dual} takes into account the logit corresponding to the ground truth label and the largest logit ranked after it.  AdaFocal~\citep{NEURIPS2022_0a692a24} utilizes the calibration
properties of focal (and inverse-focal) loss and adaptively modifies gamma for different groups of samples based on previous gamma value the knowledge of
model’s under/over-confidence on the validation set. Most recently, ~\citet{krishnan2020improving} introduced a differentiable accuracy versus uncertainty calibration (AvUC) loss function that allows a model to learn to provide well-calibrated uncertainties.
\citet{liu2022devil} analyzed some current calibration methods such as label smoothing~\citep{muller2019does}, focal loss~\citep{lin2017focal}, and explicit confidence penalty~\citep{pereyra2017regularizing} from a constrained-optimization perspective and pointed out those can be seen as approximations of a linear penalty. The authors proposed to add a margin into learning objectives.
\citet{cheng2022calibrating} proposed calibration by pairwise constraints (CPC), which aims to strengthen the calibration supervision by casting multiclass scenario into pairwise binary calibration. Thus authors proposed binary discrimination constraints (BDC) loss and binary exclusion constraint (BEC) loss as an addition to standard cross-entropy loss to increase the supervision rate for calibration of the training process. 

Within regularization-based methods, some approaches use differentiable calibration proxies as regularizers. For instance, ~\citet{kumar2018trainable} developed a differentiable equivalent of ECE, the Maximum Mean Calibration Error (MMCE), which can be optimized directly to train calibrated models. Similarly~\citet{karandikar2021soft} proposed a softened version of ECE (SB-ECE) by employing a softmax-based bin-membership function with temperature, enabling the soft ECE to serve as either a primary or auxiliary loss objective. Another interesting exploration is presented in ~\citet{bohdal2021meta}, where instead of utilizing a developed differentiable ECE (DECE) for optimizing the main network, the authors proposed a meta-learning approach~\citep{Luketina2015} to train a model that minimizes DECE, thereby finding the optimal parameters of the L2 regularizers.

Compared to the aforementioned regularization methods, to the best of our knowledge, our proposed method learns the most fine-grained gamma values for focal loss via a meta-network. This network is optimized toward stable and unbiased calibration, mitigating the biases caused by the binning mechanism.

%% newly added references

\section{Preliminaries}
\subsection{Model Calibration}
\label{section:calibration}
Calibration~\citep{guo2017calibration} measures and verifies how well the predicted probability estimates the true likelihood of correctness.  Assume a model $\mathcal{M}$ trained with dataset $\{\mathbf{x}, y\},\mathbf{x} \in \mathcal{X}, y \in \mathcal{Y}$. Let $\mathbf{p}=\{p_1, p_2,...,p_K\}$ be the predicted softmax probability of $K$ classes. If $\mathcal{M}$ makes 100 independent predictions, each with confidence $p=\arg\max(\mathbf{p})=0.9$, a calibrated $\mathcal{M}$ is correct 90 times. Formally, $\text{accuracy}(\mathcal{M}(D))=\text{confidence}(\mathcal{M}(D))$ if $\mathcal{M}$ is perfectly calibrated on dataset $D$. 

\textbf{Reliability Diagrams} \citep{DeGrootFienberg:83,Niculescu-MizilETAL:05} visualize whether a model is overconfident or underconfident by grouping predictions into bins according to their prediction probability. The predictions are grouped into $M$ bins $\{b_1, b_2,..,b_M\}$. The accuracy of samples in each bin $b_m$ is computed as: $\textsc{acc}(b_m)=\frac{1}{|b_m|}\sum_{i \in b_m}\bm{1}(\hat{y}_i=y_i)$,
where $i$ indexes all examples that fall into bin $b_m$, $\hat{y}_i$ are $y_i$ are the prediction and ground-truth, respectively.
Let $\hat{p}_i$ be the probability for the predicted class $y_i$ for the $i$-th sample (i.e. the class with highest predicted probability for $i$-th sample), then the average confidence is defined as: $\textsc{conf}(b_m)=\frac{1}{|b_m|}\sum_{i \in b_m}\hat{p}_i$.
A model is perfectly calibrated if $\textsc{acc}(b_m)=\textsc{conf}(b_m), \forall m$ and in a diagram the bins would follow the identity function. Any deviation from this indicates miscalibration.

\textbf{Expected Calibration Error (ECE)}~\citep{NaeiniETAL:15}. ECE computes the difference between model accuracy and confidence. It quantifies the discrepancy between predicted probabilities and observed frequencies by partitioning data points into bins of similar confidence. It takes the form
\begin{equation}
\textsc{ECE} = \sum_{m=1}^{M} \frac{|b_m|}{N} |\textsc{acc}(b_m) - \textsc{conf}(b_m)|,
\end{equation}

where $N$ is the total number of samples and $b_m$ represents a single bin.

\textbf{Maximum Calibration Error (MCE)}~\citep{NaeiniETAL:15} is particularly important in high-risk applications where reliable confidence measures are absolutely necessary. It measures the worst-case deviation between accuracy and confidence,
$
\textsc{MCE} = max_{m\in \{1, \dots, M\}} | \textsc{acc}(b_m) - \textsc{conf}(b_m)|.
$
For a perfectly calibrated model, the ideal ECE and MCE are equal to 0. 

Besides ECE and MCE, we also report \textit{Classwise ECE}~\citep{https://doi.org/10.48550/arxiv.1904.01685}, \textit{Adaptive ECE}~\citep{https://doi.org/10.48550/arxiv.1904.01685}, we describe these metrics in Appendix~\ref{sec:more_metric}.


\subsection{Focal Loss}
For a classification task, the focal loss (FL)~\citep{lin2017focal} can be defined as $\mathcal{L}^f_{\gamma} = -(1-p_{i, y_i})^{\gamma}\log p_{i, y_i}$ where $\gamma$ is a hyper-parameter. It was originally proposed to handle imbalanced data distributions but~\citet{mukhoti2020calibrating} found that the models trained with focal loss are better calibrated with respect to cross-entropy trained counterparts. This is because focal loss can be interpreted as a trade-off between minimizing Kullback–Leibler (KL) divergence and maximizing the entropy of predictions, depending on $\gamma$~\citep{mukhoti2020calibrating}\footnote{More theoretical findings can be found in the paper.}:
\begin{equation}
\label{eq:entropy}
\mathcal{L}_f \geq \textsc{KL}(q \parallel p) + \mathbb{H}(q) - \gamma \mathbb{H}(p)
\end{equation}
where $p$ is the prediction distribution, $q$ is the one-hot encoded target class, and $\mathbb{H}(q)$ is constant. Models trained with this loss learn to strike a balance between narrowing $p$ (high confidence in predictions) due to the KL term and broadening $p$ (avoiding overconfidence) due to the entropy regularization term. The authors provided a principled approach to select the $\gamma$ for focal loss based on the Lambert-W function~\citep{corless1996lambertw}. Motivated by this, our work proposes to learn a more fine-grained, sample-wise $\gamma$ with a meta-network.

\subsection{Meta-Learning}\label{section:meta_learning}
Model calibration can be formulated as minimizing a multi-component loss objective where the base term is used to optimize predictive performance while a regularizer maintains model calibration ~\citep{bohdal2021meta,lin2017focal}. Tuning the hyper-parameters of the regularizer can be a difficult process when conventional methods are used~\citep{Li2016,Snoek2012,Falkner2018}. In this work, we adopt the meta learning approach~\citep{Luketina2015}. At each training iteration, the model takes a mini-batch from the training dataset $D_{train}$ and the validation dataset $D_{val}$ to optimize
\begin{equation}
   \arg\min_{\theta, \phi}~\mathcal{L}(\theta, \phi, D_{train}, D_{val})   = \mathcal{L}_{FL_{\gamma}}(\theta, D_{train}) +  \mathcal{L}_{SECE}(\phi, D_{val})
\end{equation}
First, the base loss function $\mathcal{L}_{FL_{\gamma}}$ is used to optimize the parameter of the backbone model $\theta$ on $D_{train}$, note that the $\gamma$ parameter for this step is predicted by $\gamma$-Net. Following that, the validation mini-batch $D_{val}$ is used to optimize the parameters of the meta-network ($\gamma$-Net) $\phi$ using a validation loss $\mathcal{L}_{SECE}$. The validation loss is a function of the backbone model output and does not depend on \gnet directly. The dependence between the validation loss and parameters of the meta-network is mediated via the parameters of the backbone model as discussed by \citep{Luketina2015}.



\section{Methods}
This section introduces the two components of our approach: the \gnet learns to parameterise the focal loss and the \sece provides differentiability to \gnet towards calibration optimization.

%%%%% section %%%%%
\subsection{$\gamma$-Net: Learning Sample-Wise Gamma for Focal Loss}
Sample-dependent $\gamma$ value for focal loss showed it effectiveness of calibrating deep neural networks~\citep{mukhoti2020calibrating}. In this work, instead of computing and scheduling $\gamma$ value based on Lambert-W function~\citep{corless1996lambertw}. We propose a learnable approach which learns more fine-grained and local $\gamma$ values, i.e., each sample has an individual $\gamma$, in a continuous space. 
Formally, \gnet takes the representations from the second-to-last layer of a backbone network (i.e. ResNet~\citep{he2016deep}. Let $\bfx \in \bbR^{b \times d}$ ($b$: batch size, $d$: hidden dimension) be the extracted representation, $\bA \in \bbR^{d \times k }$ be a $k$-head self-attention matrix that followed by a layer with parameters $\bfW \in \bbR^{d \times 1 }$. The \gnet transforms the representation to sample-wise $\gamma$:
\begin{align}
 \bfa &= \bfx \cdot \bA,~\in \bbR^{b \times k},~
 \bfp = \textsc{Softmax}(\bfa),~\in \bbR^{b \times k} \\
 \tilde{\bfx} &= \bfp \cdot \bA^\top,~\in \bbR^{b \times d},~~\gamma =\left | \tilde{\bfx} \cdot \bfW \right |/\tau,~ \in \bbR^{b \times 1} 
 \label{equation:gammanet}
\end{align}
Here we use $ | \cdot  |$ to ensure the $\gamma$ is positive valued and tune the temperature $\tau=0.01$ as a hyperparameter. This is similar to the temperature setups as in meta-calibration~\citep{bohdal2021meta,bohdal2023meta}. Those operations result in a set of sample-wise $\gamma$ to be used in focal loss $\mathcal{L}_r^f$. Note that, $f_{\gamma}(\gamma_i| x_i), x_i \in D$ is learned as a continous variable rather than discrete value as in original FL formulation~\citep{lin2017focal} and scheduled FL (FLSD)~\citep{mukhoti2020calibrating}. 


\noindent\textbf{Practical Considerations}: To ensure $\gamma \geq 0$, we could apply operations based on min-max scaling or activation functions such as sigmoid, ReLu(Rectified Linear Unit)~\cite{agarap2018deep} and softplus. However, our experimental evidence shows that the formulation presented in Equation~\ref{equation:gammanet} performs better across datasets.

\subsection{\sece: Smooth Expected Calibration Error}
Conventional calibration measurement via Expected Calibration Error (ECE) discussed in Section~\ref{section:calibration}, is computed over discrete bins by finding the accuracy and confidence of examples in each interval. However, this approach is highly dependent on different settings of bin edges and the number of bins, making it a biased estimator of the true value \citep{minderer2021revisiting}.% \citep{bohdal2021meta}.

The issue of bin-based ECE can be traced back to the discrete decision of binning of samples~\citep{minderer2021revisiting}. The larger bin numbers, the less information is retained. Small bins lead to inaccurate measurements of accuracy. For instance, in single-example bins, the confidence is well defined but the accuracy in that interval of model confidences is more difficult to assess accurately from a single point. Using the binary accuracy of a single example leads us to the Brier score~\citep{brier1950verification} which has been shown to not be a perfect measure of calibration.

To find a good representation of accuracy within the single-example bin (representing a small confidence interval), we leverage the accuracy of other points in the vicinity of the single chosen example weighted by their distance in the confidence space. Formally, the soft estimation of accuracy within a single-example bin:
\begin{align}
  &  \text{\sacc}(b_i) = \sum_j^M \pi(x_{i}) K(z_i, z_j)  \\
&K\left(x_i, x_j^{\prime}\right)=\exp \left(-\frac{\left\|x_i-x_j^{\prime}\right\|^2}{2 h^2}\right)
\end{align}
where $b_i$ is the bin housing example $x_i$ and $K(\cdot, \cdot)$ is a chosen distance measure, for example a Gaussian kernel and $h$ is the bandwidth. Moreover, $z_i$ and $\pi(x_i)$ respectively denote the confidence and accuracy of the $i-{th}$ example. As in meta-calibration~\citep{bohdal2021meta}, we use the all-pairs approach~\citep{10.1007/s10791-009-9124-x} to ensure differentiability. Having good measures of soft accuracy and confidence for each single-example bin, we can write the updated ECE metric as soft-ECE in the form
\begin{equation}
\text{\sece} =  \frac{1}{M} \sum_{i}^{M}|\textrm{\sacc}(i) - \textsc{conf}(i)|,
\label{equation:sece}
\end{equation}
where $i$ represents a single example and $M$ is the number of examples. The new formulation is (1) differentiable as long as the kernel we use is differentiable and (2) enables smooth control over how accuracy over a confidence bin is computed via tuning the kernel parameters. Regarding (2), choosing a Dirac-delta function recovers the original Brier score while choosing a broader kernel enables a smooth approximation of accuracy over all confidence values.

\textbf{Connection to KDE-based ECE Estimator}
\citep{zhang2020_mixmatch} presents a KDE-based ECE estimator that relies on kernel density estimation to approximate the desired metric. Canonically, ECE is computed as an integral over the confidence space:
\begin{equation}
\hat{\operatorname{ECE}}^d=\int\|z-\hat{\pi}(z)\|_d^d \hat{p}(z) dz
\end{equation}
where $z=\{z_1,z_2,...,z_L\}$ denotes model confidence distribution over $L$ classes, $\|\cdot\|_d^d $ denotes the  $d^{th}$ power of the $\ell_d$  norm, and $\hat{p}(z)$ represents the marginal density function of model's confidence $\hat{\pi}(z)$ on a given dataset. 
The $\hat{p}(z)$ and $\hat{\pi}(z)$ are approximated using kernel density estimation. We argue that \sece is a special instance with $d=1$ of $\hat{\operatorname{ECE}}^d$ and estimate ECE with max probability $z_t, t=\arg\max \{z_1,z_2,...,z_L\}$ for a single instance. And $\operatorname{\sece}$ is an upper bound of $\hat{\operatorname{ECE}}^1$:
\begin{align}
\hat{\operatorname{ECE}} &=\int\left|z-\hat{\pi}(z)\right| \hat{p}(z) dz \\
&=\int\left|z_l-\hat{\pi}(z_l)\right| \hat{p}(z_l) dz_l \int\left|z_t-\hat{\pi}(z_t)\right| \hat{p}(z_t) dz_t \\
&\leq \int\left|z_t-\hat{\pi}(z_t)\right| \hat{p}(z_t) dz_t = \operatorname{\sece}
\end{align}
with $l = [1, L],l\neq t$ and density functions:
\begin{align}
\hat{p}(z_t) =\frac{h^{-L}}{M} \sum_{i=1}^{M} K\left(z_t,z_i\right), ~\hat{\pi}_t(z_t) = \frac{\sum_{i=1}^{M} \pi(i) K\left(z_t,z_i\right)}{\sum_{i=1}^{M} K\left(z_t,z_i\right)}  
\end{align}

where $h$ is the kernel width and $\pi(i)$ represents the binary accuracy of point $i$. The $\hat{p}(z_t)$ is a mixture of Dirac deltas centered on confidence predicted for individual points within the dataset (used to approximate ECE). Replacing binary accuracy with accuracy computed using an all-pairs approach~\citep{10.1007/s10791-009-9124-x} results in SECE, which is differentiable and can be computed efficiently for smaller batches of data since the integral is replaced with a sum over all examples in a batch.

\noindent{\textbf{Discussion}}: There exist other studies that develop a differentiable calibration error metrics, for instance, DECE~\citep{bohdal2021meta} and SB-ECE~\citep{karandikar2021soft}. As pointed out in ~\citep{zhang2020_mixmatch}, non-parametric density estimators are continuous, thus avoiding the binning step and potentially reduce binning biases. The difference between \sece and KDE-based estimators do not invalidate the analysis of the $\hat{\operatorname{ECE}}$ as an unbiased estimator of ECE (Theorem 4.1 in~\citep{zhang2020_mixmatch}) as the all-pairs accuracy is an unbiased estimator of accuracy and the details of the $p(z)$ distribution are not used in the derivation beyond setting the bounds. As a result, the analysis described in~\citep{zhang2020_mixmatch} to show their KDE-based approximation to ECE is unbiased can be re-used to show the same property for \sece. 


\subsection{Optimising \gnet with \sece}
The newly introduced $\gamma$-Net and the \sece metric can be applied together to optimize selected ML models for both cross-entropy and calibration error using the meta learning scheme introduced in Section~\ref{section:meta_learning}. As the probability calibration via maximising entropy is performed in a continuous space, we argue that \sece is an efficient learning objective to optimize \gnet and provide calibration regularization to base network trained with focal loss.% (see empirical results in Section \ref{subsection:pred_cal}). 

Under the umbrella of meta-learning, two sets of parameters are learned simultaneously: $\theta$, the parameters of base network $f^c$, as well as the $\phi$, the parameters of \gnet denoted as $f^{\gamma}$. The former, $f^c$, is used to classify each new example into appropriate classes and the latter, $f^{\gamma}$, is applied to find the optimal value of the focal loss parameter $\gamma$ for each training example. Algorithm~\ref{algorithm:meta_learning} in Appendix describes the learning procedures. 

%\vspace{-3mm}
\section{Experiments}
We conducted a series of experiments to achieve the following objectives: 
\begin{itemize}
    \item Examine both the predictive and calibration performance of the proposed method. We used error (accuracy) to measure the predictive performance and Negative Log-Likelihood (NLL), Expected Calibration Error (ECE), Maximum Calibration Error (MCE), Adaptive Calibration Error (ACE), and Classwise ECE for calibration performance.
    \item Observe the calibration behaviours of the proposed method during training.
    \item Empirically evaluate the learned $\gamma$ values for Focal loss and the robustness of different methods to the binning mechanism.
\end{itemize}
\textbf{Implementation Details}.
We implemented our methods by adapting and extending the code from ~\citep{bohdal2021meta} with Pytorch~\citep{paszke2019pytorch}. For all experiments we used their default settings (using ResNet18 as base model, batch size 128, data augmented with random crop and horizontal flip) unless otherwise stated. Each experiment was run 5 times with different random seeds, and results were averaged. We conducted  our experiments on CIFAR-10 and CIFAR-100 (in~\citep{bohdal2021meta}) as well as Tiny-ImageNet~\citep{imagenet_tiny}. For meta-learning, we split the training set into 8:1:1 as training/val/meta-validation, keeping the original test sets untouched. The experimental pipeline for all three datasets was identical. The models were trained with SGD (learning rate 0.1, momentum 0.9, weight decay 0.0005) for up to 350 epochs. The learning rate was decreased at 150 and 250 epochs by a factor of 10. The model selection was based on the best validation error.

The \gnet is implemented with a multi-head attention layer with $k$ heads and a fully-connected layer. $k$ is set to the number of categories. The hidden dimension is set to 512, the temperature $\tau$ is fixed at 0.01. For SECE, we used the Gaussian kernel with bandwidth of 0.01 (selected via grid search) for both datasets. We initialized $\gamma=1.0$. During inference, the meta-network is not present excepted except in our ablation study on learned $\gamma$ values in Section~\ref{sec:ablation}.

\textbf{Baselines}. We extensively compare our method with baselines including standard cross-entropy (CE), cross-entropy with post-hoc temperature scaling (TS)~\citep{platt1999probabilistic}, Focal loss with standard gamma value (Focal), $\gamma=1$~\citep{lin2017focal}, Focal Loss with scheduled gamma (FLSD)~\citep{mukhoti2020calibrating}, MMCE (Maximum Mean Calibration Error)~\citep{kumar2018trainable} and Label Smoothing with smooth factor 0.05 (LS-0.05) or (LS-0.1)~\citep{muller2019does} and Mix-Up ($\alpha=1.0$)~\citep{thulasidasan2019mixup}. In meta-learning setting, we include CE-DECE (meta-calibration\citep{bohdal2021meta} for learning unit-wise weight regularization), which serves as our meta-learning baseline, CE-SECE, FL$_\gamma$-DECE, focal loss with learnable sample-wise $\gamma$ and FL$_\gamma$-DECE.
 


\begin{table}[htb]
\centering
\footnotesize
\caption{The predictive (test error) and calibration performance of different methods on CIFAR-10 (Top), CIFAR-100 (Middle) and Tiny-ImageNet (Bottom). The best scores are \textbf{bold}. The mean and standard deviation numbers are reported by averaging 5 runs with random seeds. As an alternative calibration method, our approach generally exhibits better calibration while retaining competitive predictive performance compared to conventional as well as meta-learning baselines.}
\label{tab:per_comp}
\setlength{\tabcolsep}{3.75pt}
\begin{tabular}{lllllll}
\hline
Methods & Error & NLL & ECE & MCE & ACE & Classwise ECE \\ %\hline
\multicolumn{7}{c}{CIFAR 10} \\ \hline
CE&         4.812 $\pm$ 0.122&         0.335 $\pm$ 0.01&         4.056 $\pm$ 0.092&         33.932 $\pm$ 5.433&    4.022 $\pm$ 0.136&       0.848 $\pm$ 0.023\\
CE (TS)&         4.812 $\pm$ 0.122&         0.211 $\pm$ 0.005&         3.083 $\pm$ 0.140&         26.695 $\pm$ 2.959&    3.046 $\pm$ 0.157&       0.656 $\pm$ 0.022\\
Focal&         4.874 $\pm$ 0.100&         0.207 $\pm$ 0.005&         3.193 $\pm$ 0.104&         28.034 $\pm$ 5.702&    3.174 $\pm$ 0.098&       0.690 $\pm$ 0.018\\
FLSD&         4.916 $\pm$ 0.074&         0.211 $\pm$ 0.005&         6.904 $\pm$ 0.462&         \textbf{19.246 $\pm$ 11.071}&    6.805 $\pm$ 0.446&       1.465 $\pm$ 0.088\\
LS (0.05)&         4.744 $\pm$ 0.126&         0.232 $\pm$ 0.003&         2.900 $\pm$ 0.085&         24.860 $\pm$ 8.599&    3.985 $\pm$ 0.154&       0.727 $\pm$ 0.009\\
LS(0.1) & 4.918 $\pm$ 0.085 & 0.266 $\pm$ 0.004 & 7.566 $\pm$ 0.41 & 16.033 $\pm$ 3.783 & 7.611 $\pm$ 0.161 & 1.637 $\pm$ 0.056 \\
 Mixup($\alpha$=1.0) & \textbf{4.126 $\pm$ 0.068} & 0.273 $\pm$ 0.033 & 12.863 $\pm$ 3.2 & 20.739 $\pm$ 4.205 & 12.833 $\pm$ 3.161 & 2.678 $\pm$ 0.615 \\
MMCE&         4.808 $\pm$ 0.082&         0.333 $\pm$ 0.012&         4.027 $\pm$ 0.082&         41.647 $\pm$ 10.275&    4.013 $\pm$ 0.091&       0.845 $\pm$ 0.014\\
CE-DECE&         5.194 $\pm$ 0.161&         0.301 $\pm$ 0.038&         4.106 $\pm$ 0.402&         41.346 $\pm$ 13.325&    4.088 $\pm$ 0.395&       0.868 $\pm$ 0.074\\
CE-SECE&         5.222 $\pm$ 0.168&         0.289 $\pm$ 0.027&         4.062 $\pm$ 0.241&         50.81 $\pm$ 21.705&    4.049 $\pm$ 0.251&       0.852 $\pm$ 0.040\\
FL$_{\gamma}$-DECE&         5.434 $\pm$ 0.095&         \textbf{0.193 $\pm$ 0.009}&         2.257 $\pm$ 0.787&         56.633 $\pm$ 23.856&    2.396 $\pm$ 0.669&       \textbf{0.557 $\pm$ 0.165}\\
FL$_{\gamma}$-SECE&         5.428 $\pm$ 0.144&         \textbf{0.193 $\pm$ 0.010}&         \textbf{2.138 $\pm$ 0.819}&         22.725 $\pm$ 5.756&    \textbf{2.357 $\pm$ 0.541}&       \textbf{0.556 $\pm$ 0.165}\\ \hline


\multicolumn{7}{c}{CIFAR-100} \\
CE&         22.570 $\pm$ 0.438&         0.997 $\pm$ 0.014&         8.380 $\pm$ 0.336&         23.250 $\pm$ 2.436&    8.347 $\pm$ 0.344&       0.233 $\pm$ 0.006\\
CE (TS)&         22.570 $\pm$ 0.438&         0.959 $\pm$ 0.008&         5.388 $\pm$ 0.393&         13.454 $\pm$ 2.377&    5.360 $\pm$ 0.315&       0.208 $\pm$ 0.003\\
Focal&         22.498 $\pm$ 0.214&         0.900 $\pm$ 0.007&         5.044 $\pm$ 0.203&         12.454 $\pm$ 0.893&    5.015 $\pm$ 0.207&       0.203 $\pm$ 0.004\\
FLSD&         22.656 $\pm$ 0.113&         0.876 $\pm$ 0.005&         5.956 $\pm$ 0.804&         14.716 $\pm$ 1.387&    5.958 $\pm$ 0.802&       0.241 $\pm$ 0.008\\
LS (0.05)&         21.810 $\pm$ 0.172&         1.070 $\pm$ 0.011&         8.108 $\pm$ 0.346&         20.268 $\pm$ 1.536&    8.106 $\pm$ 0.346&       0.272 $\pm$ 0.006\\
LS(0.1) & 22.244 $\pm$ 0.155 & 1.052 $\pm$ 0.011 & 4.754 $\pm$ 0.709 & 17.228 $\pm$ 0.923 & 4.777 $\pm$ 0.647 & 0.239 $\pm$ 0.004 \\
Mixup($\alpha$=1.0) & \textbf{21.210 $\pm$ 0.227} & 0.917 $\pm$ 0.017 & 9.716 $\pm$ 0.754 & 16.01 $\pm$ 1.335 & 9.722 $\pm$ 0.740 & 0.315 $\pm$ 0.011 \\
MMCE&         22.490 $\pm$ 0.143&         1.021 $\pm$ 0.007&         8.713 $\pm$ 0.245&         23.565 $\pm$ 1.141&    8.670 $\pm$ 0.305&       0.238 $\pm$ 0.004\\
CE-DECE&         23.406 $\pm$ 0.323&         1.148 $\pm$ 0.006&         7.309 $\pm$ 0.245&         22.565 $\pm$ 1.446&    7.253 $\pm$ 0.315&       0.241 $\pm$ 0.002\\
CE-SECE&         23.448 $\pm$ 0.302&         1.153 $\pm$ 0.015&         7.668 $\pm$ 0.330&         24.261 $\pm$ 1.614&    7.609 $\pm$ 0.295&       0.244 $\pm$ 0.002\\
FL$_{\gamma}$-DECE&         23.712 $\pm$ 0.204&         0.888 $\pm$ 0.009&         \textbf{1.879 $\pm$ 0.440}&         8.271 $\pm$ 2.651&    \textbf{1.838 $\pm$ 0.371}&       0.195 $\pm$ 0.005\\
FL$_{\gamma}$-SECE&         23.686 $\pm$ 0.377&         \textbf{0.877 $\pm$ 0.004}&         1.940 $\pm$ 0.365&         \textbf{7.480 $\pm$ 1.867}&    \textbf{1.939 $\pm$ 0.379}&       \textbf{0.192 $\pm$ 0.006} \\ \hline

\multicolumn{7}{c}{Tiny-ImageNet} \\
CE& 	40.110 $\pm$ 0.110& 	1.838 $\pm$ 0.171& 	8.059 $\pm$ 1.296& 	15.73 $\pm$ 1.905& 	8.006 $\pm$ 1.282& 	0.154 $\pm$ 0.001\\
Focal& 	39.415 $\pm$ 0.625& 	1.896 $\pm$ 0.009& 	7.600 $\pm$ 0.309& 	13.771 $\pm$ 0.897& 	7.469 $\pm$ 0.301& 	0.152 $\pm$ 0.002\\
FLSD& 	39.705 $\pm$ 0.075& 	1.904 $\pm$ 0.025& 	14.501 $\pm$ 1.078& 	21.528 $\pm$ 2.116& 	14.501 $\pm$ 1.078& 	0.202 $\pm$ 0.006 \\
LS (0.1)& 	\textbf{39.395} $\pm$ 0.305& 	2.185 $\pm$ 0.001& 	16.777 $\pm$ 0.476& 	29.088 $\pm$ 1.835& 	16.901 $\pm$ 0.460& 	0.199 $\pm$ 0.001\\
Mixup($\alpha$=1.0) & 39.890 $\pm$ 0.271& 1.932 $\pm$ 0.054& 12.133 $\pm$ 2.069& 31.440 $\pm$ 0.968& 12.028 $\pm$ 2.079& 0.193 $\pm$ 0.009 \\
MMCE& 	40.310 $\pm$ 0.100& 	1.826 $\pm$ 0.177& 	8.206 $\pm$ 1.219& 	16.802 $\pm$ 2.339& 	8.165 $\pm$ 1.269& 	\textbf{0.149} $\pm$ 0.001\\
CE-DECE& 	41.350 $\pm$ 0.000& 	2.228 $\pm$ 0.033& 	10.694 $\pm$ 0.503& 	20.888 $\pm$ 0.430& 	10.553 $\pm$ 0.553& 	0.160 $\pm$ 0.000\\
CE-SECE& 	41.005 $\pm$ 0.145& 	2.213 $\pm$ 0.058& 	10.928 $\pm$ 1.125& 	21.362 $\pm$ 2.526& 	10.912 $\pm$ 1.069& 	0.157 $\pm$ 0.003\\
FL$_\gamma$-DECE& 	40.625 $\pm$ 0.095& 	\textbf{1.826} $\pm$ 0.007& 	5.944 $\pm$ 1.090& 	11.542 $\pm$ 1.990& 	6.077 $\pm$ 1.095& 	0.155 $\pm$ 0.007\\ 
FL$_\gamma$-SECE& 	40.850 $\pm$ 0.140& 	1.829 $\pm$ 0.005& 	\textbf{5.794} $\pm$ 0.756& 	\textbf{11.477} $\pm$ 1.563& 	\textbf{5.848} $\pm$ 0.751& 	0.156 $\pm$ 0.005\\
\hline
\end{tabular}
\end{table}


%\subsection{Results and Discussion}
\subsection{Predictive and Calibration Performance}
\label{subsection:pred_cal}
Table~\ref{tab:per_comp} presents the performance comparison across approaches. Temperature Scaling (TS) can effectively reduce the errors in calibration metrics compared to uncalibrated CE models (baseline). Label smoothing and Mixup achieve the best test errors on datasets but exhibit higher ECE and MCE scores. Focal loss and FLSD can improve calibration in general, but we also observed high MCE score for FLSD. On the other hand, we found that MMCE exhibits higher calibration errors as compared to baseline, this aligns with the findings from~\citep{bohdal2021meta}. We further discussed the predictive-calibration trade-off in Appendix~\ref{sec:trade-off}. Our proposed approach FL$_\gamma$-SECE achieves lower errors on
most of calibration metrics with competitive predictive performance. Compared to the prior work (CE-DECE), our method (FL$_\gamma$-SECE) showed significant calibration improvement across multiple calibration metrics. Particularly, on MCE (which measures the worst-case mismatch between accuracy and confidence), there are 18.62\%, 15.09\% and 9.41\% improvements on CIFAR-10, CIFAR-100 and Tiny-ImageNet, respectively. 

\begin{figure}[htb]
	\centering
	\subfloat[\scriptsize{CE (22.41\%)}]{{\includegraphics[width=0.175\textwidth]{figure/rn18_c100_ce_eval_4.pdf} }}
	\subfloat[\scriptsize{CE (TS) (22.41\%)}]{{\includegraphics[width=0.175\textwidth]{figure/rn18_c100_ce_ts_eval_4.pdf} }}
	\subfloat[\scriptsize{Focal (22.53\%)}]{{\includegraphics[width=0.175\textwidth]{figure/rn18_c100_focal_eval_4.pdf} }}
	\subfloat[\scriptsize{FLSD(22.72\%)}]{{\includegraphics[width=0.175\textwidth]{figure/rn18_c100_focal_schedule_eval_4.pdf} }} 
	\subfloat[\scriptsize{LS-0.05 (21.56\%)} ]{{\includegraphics[width=0.175\textwidth]{figure/rn18_c100_smoothing_0.05_eval_4.pdf} }} \\
	\subfloat[\scriptsize{MMCE (22.30\%)}]{{\includegraphics[width=0.175\textwidth]{figure/rn18_c100_mmce_eval_4.pdf} }} 
	\subfloat[\scriptsize{CE-DECE(23.05\%)}]{{\includegraphics[width=0.175\textwidth]{figure/rn18_c100_ce_dece_eval_4.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE(24.04\%)}]{{\includegraphics[width=0.175\textwidth]{figure/rn18_c100_focal_dece_eval_4.pdf} }}
	\subfloat[\scriptsize{CE-SECE(23.58\%)}]{{\includegraphics[width=0.175\textwidth]{figure/rn18_c100_ce_sece_eval_4.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-SECE(23.78\%)}]{{\includegraphics[width=0.175\textwidth]{figure/rn18_c100_focal_sece_eval_4.pdf} }}
\caption{The reliability diagram plot for models on CIFAR-100 test set. The ($\cdot$) represents test error. The diagonal dash line represents perfect calibration. The red bar represents the gap between the observed accuracy and the desired accuracy of the perfectly calibrated model (the diagonal) - it is positive if the observed accuracy is lower and negative otherwise. The model from the 5$^{th}$ run is used.} 
\label{fig:rc_c100_calibration} 
\end{figure}

Figure \ref{fig:rc_c100_calibration} illustrates the reliability diagram plots of models on CIFAR-100. (The plots for CIFAR-10 are in Appendix Figure~\ref{fig:rc_c10_calibration}). 

The diagrams show that for CIFAR-100, \gnet-based methods achieve better ECE compared to \sece and meta-loss for optimizing \gnet ensures smooth and stable calibrations, potentially reduces calibration biases in bins.
\begin{figure}[!htb]
\vspace{-3mm}
	\centering
	%\subfloat[CIFAR-10 (Error)]{{\includegraphics[width=0.235\textwidth]{figure/rn18_c10_test_err.pdf} }}
	\subfloat[CIFAR-10]{{\includegraphics[width=0.375\textwidth]{figure/rn18_c10_test_ece.pdf} }}  
	%\subfloat[CIFAR-100 (Error)]{{\includegraphics[width=0.235\textwidth]{figure/rn18_c100_test_err.pdf} }}
	\subfloat[CIFAR-100]{{\includegraphics[width=0.375\textwidth]{figure/rn18_c100_test_ece.pdf} }} \\
	\subfloat[CIFAR-10]{{\includegraphics[width=0.375\textwidth]{figure/rn18_c10_focal_sece_7-gamma.pdf} }}
	\subfloat[CIFAR-100]{{\includegraphics[width=0.375\textwidth]{figure/rn18_c100_focal_sece_7-gamma.pdf} }} 
	\caption{(a-b): ECE curves on the test dataset of CIFAR-10 (a) and CIFAR-100 (b).(c-d): The mean and standard deviation (std.) of $\gamma$ on test dataset at each epoch. Low std. score indicates samples share similar gamma values, and high std. score indicates more samples have different $\gamma$ values.}
\label{fig:test_learning_curve} 
\end{figure}
Among compared methods, our method FL$_{\gamma}$-SECE achieves considerably lower errors on calibration metrics. When comparing meta-learning based approaches: CE-DECE (meta-learning baseline), CE-SECE, FL$_{\gamma}$-DECE, FL$_{\gamma}$-SECE. We can see that our method (FL$_{\gamma}$-SECE) achieves comparable test error and better scores across calibration metrics. Particularly, it improves ECE by an average of roughly 4.198\% on three datasets, additionally improving MCE by an average of 14.37\% MCE and ACE score by 3.917\%.

To observe the learning behavior of models, we plotted the curves showing test ECE changes in Figure~\ref{fig:test_learning_curve} (a-b), and test error (included in Appendix Section ~\ref{sec:test error}). It is noteworthy that while the test error curves exhibit similar behaviors, the calibration behaviors vary significantly across methods. The primary advantage of \gnet with \sece lies in its stable and smooth calibration behavior. After the 150$^{th}$ epoch, when the learning rate is decreased by a factor of 10, we observe an increase in ECE scores for all approaches, particularly for FLSD (green lines). This indicates that during model training, the learning procedure aims to align the probability distribution as closely as possible with the ground-truth distribution (one-hot representation), causing models to become overconfident. It also demonstrates that our approach is more robust to this phenomenon.

\begin{figure*}[htb]
	\centering
  	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_dece_eval_4_20.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_sece_eval_4_20.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_dece_eval_4_20.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_sece_eval_4_20.pdf} }} \\
 	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_dece_eval_4_50.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_sece_eval_4_50.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_dece_eval_4_50.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_sece_eval_4_50.pdf} }} \\
	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_dece_eval_4_100.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_sece_eval_4_100.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_dece_eval_4_100.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_sece_eval_4_100.pdf} }} \\
	\caption{The reliability diagram plots on CIFAR-100 with large bin numbers (top to bottom: 20, 50, 100).}
\label{fig:rc_c100_calibration_more} 
\end{figure*}



\subsection{Ablation Study}
\label{sec:ablation}
%We conducted two groups of experiments to inspect the learned $\gamma$ and the ECE/MCE against different binning schemes.
\subsubsection{Learning $\gamma$ as continous variables}

Figure \ref{fig:test_learning_curve}(c-d) presents the changes in $\gamma$ values on test dataset over epochs.  In the earlier stages of training, the sample-wise $\gamma_j$ for $x_j \in D_{test}$ have similar values (with low variance) because they are initialized with $\gamma_i=1.0$ for $x_i \in D_{val}$. The $\gamma$ parameter is observed to have a higher standard deviation in the later stage of training as \gnet learns the optimal value for each example in the dataset rather than relying on global values, showcasing the flexibility of the network. It is also noted that $\gamma$ is learned in a continuous space, which is different to the discrete values the pre-defined in Focal Loss~\citep{lin2017focal} and FLSD~\citep{mukhoti2020calibrating}.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.675\textwidth]{figure/bin_vs_ece_mce_cifar10.pdf}
\caption{The changes in ECE (left) and MCE (right) scores on the CIFAR-10 test dataset with increasing bin numbers in the range of [10, 20, 50, 100, 200, 500, 1000] are illustrated. FL$_{\gamma}$-SECE demonstrates superior robustness to increasing bin numbers, as evidenced by lower MCE. A similar plot for CIFAR-100 is provided in the Figure~\ref{fig:cifar100_n_bins} of the Appendix.}
\label{fig:n_bins} 
\end{figure}
\subsubsection{Calibration bias and robustness}
In Figure~\ref{fig:n_bins}, we examine the robustness of those methods with different binning schemes by varying the number of bins, which is one of causes of introducing calibration bias~\citep{minderer2021revisiting}. It shows that \gnet based approaches (FL$_{\gamma}$-DECE and FL$_{\gamma}$-SECE) 
maintain much lower ECE score when throughout all bin numbers from 10 to 1000 showing the trained based network is robustly calibrated. Furthermore FL$_{\gamma}$-SECE is also able to maintain lower MCE as compared to other methods. %Appendix Section 3.2 reports reliability diagrams with large bin numbers. 

\begin{table}
%\vspace{-6mm}
\centering
\footnotesize
\begin{tabular}{lllllll} 
\hline
Methods & Error & NLL & ECE & MCE & ACE & Classwise ECE \\ %\hline
FL$_\gamma$ & 5.632 $\pm$ 0.118 & 0.197 $\pm$ 0.009 & 2.177 $\pm$ 0.619 & 46.172 $\pm$ 28.240 & \textbf{2.319 $\pm$ 0.407} & \textbf{0.553 $\pm$ 0.120} \\ 
FL$_{\gamma}$-SECE&         \textbf{5.428 $\pm$ 0.144}&         \textbf{0.193 $\pm$ 0.010}&         \textbf{2.138 $\pm$ 0.819}&         \textbf{22.725 $\pm$ 5.756}&    2.357 $\pm$ 0.541&       0.556 $\pm$ 0.165\\ 
%FL$_{\gamma}$-SECE (Post-hoc)&         5.718 $\pm$ 0.074&         0.199 $\pm$ 0.005&         \textbf{2.096 $\pm$ 0.493}&         \textbf{44.458 $\pm$ 30.723}&    2.260 $\pm$ 0.3491&       0.563 $\pm$ 0.083\\ \hline
\hline
FL$_\gamma$ & 28.148 $\pm$ 8.127 & 1.051 $\pm$ 0.278 & 3.044 $\pm$ 1.542 & 10.082 $\pm$ 3.441 & 3.016 $\pm$ 1.511 & 0.226 $\pm$ 0.063 \\ 
FL$_{\gamma}$-SECE&         \textbf{23.686 $\pm$ 0.377}&         \textbf{0.877 $\pm$ 0.004}&         \textbf{1.940 $\pm$ 0.365}&         \textbf{7.480 $\pm$ 1.867}&    \textbf{1.939 $\pm$ 0.379}&       \textbf{0.192 $\pm$ 0.006} \\
%FL$_{\gamma}$-SECE(Post-hoc)&         \textbf{24.170 $\pm$ 0.086}&         \textbf{0.920 $\pm$ 0.016}&         \textbf{3.001 $\pm$ 1.064}&         \textbf{8.756 $\pm$ 2.049}&    \textbf{2.989 $\pm$ 1.069}&       \textbf{0.199 $\pm$ 0.017} \\
\hline

\end{tabular} 
\caption{The calibration gain from SECE on CIFAR-10 (top) and CIFAR-100 (bottom).}
\label{tab:ind_sece}
\end{table}

Here we can clearly see the role of SECE in stabilizing the calibration with \gnet against the change in the number of bins. Table \ref{tab:ind_sece} shows the individual calibration gain from \sece as compared to using learnable $\gamma$ only with our method FL$_{\gamma}$-SECE. With SECE, we can see improved predictive and calibration performance. Importantly, from Figure  \ref{fig:rc_c100_calibration_more} and Figure\ref{fig:n_bins}, 
we observe that it reduces calibration bias introduced by different binning mechanisms. This is mostly reflected in reduced ECE/MCE scores, i.e., the model exhibits high MCE on a particular bin if calibration bias exists.

\section{Discussion}

\textbf{Novelties}. Though we follow the general setup in \citep{bohdal2021meta}, our work makes several novel contributions: (1) We introduce learnable sample-wise continuous variables for focal loss. To the best of our knowledge, this is the first work to introduce fine-grained $\gamma$ for focal loss for the purpose of model calibration, learned from a meta-network. (2) We propose SECE, which reduces model calibration bias across different binning mechanisms, an aspect rarely discussed in the literature. Additionally, SECE is also more efficient than DECE in \citep{bohdal2021meta} as it uses simple summation rather than predicting the bin assignment using networks. (3) We further showcase the importance and feasibility of using gradient-based meta-learning~\citep{finn2017model} in alleviating model miscalibration. As shown in Table~\ref{tab:more_recent_methods}, meta-regularization can achieve very competitive predictive and calibration performance compared to conventional regularization methods.

\begin{table}[!htb]
\centering
\footnotesize
\begin{tabular}{llc} \\ \hline
Methods & \multicolumn{1}{l}{ECE(\%)} & \multicolumn{1}{l}{Test Error(\%)}   \\ \hline
\multicolumn{3}{c}{CIFAR-10} \\
\citep{patra2023calibrating}  & 0.59 & 6.28   \\
\citep{hebbalaguppe2022stitch}  & 0.93 & 7.18   \\
\citep{hebbalaguppe2022stitch} & 0.70 & 7.08   \\
\citep{liu2022devil} (without margin)  & 3.72 &5.24    \\
\citep{liu2022devil} & 1.16 &4.75    \\
\citep{karandikar2021soft} (Focal$+$SB-ECE)  & 1.19 & 4.90   \\
\citep{karandikar2021soft} (Focal$+$SB-AvUC )  & 1.58 & 5.60   \\
\citep{tao2023dual} (Dual Focal with ResNet50) & 0.46 & 5.17   \\
\citep{tao2023dual} (Dual Focal with ResNet110) & 0.98 & 5.02  \\
\citep{NEURIPS2022_0a692a24} (AdaFocal with ResNet50) & 0.66 & 5.30 \\
\citep{NEURIPS2022_0a692a24} (AdaFocal with ResNet50) & 0.71 & 5.27  \\
Meta-Regularization (Ours) & 2.14 & 5.43   \\ \hline
\multicolumn{3}{c}{CIFAR-100} \\
\citep{patra2023calibrating}   & 1.74 & 26.57   \\
\citep{hebbalaguppe2022stitch}   & 1.49 & 31.58   \\
\citep{hebbalaguppe2022stitch}  & 0.72 & 29.80   \\
\citep{karandikar2021soft}(Focal$+$SB-ECE) & 2.30 & 21.40   \\
\citep{karandikar2021soft} (Focal$+$SB-AvUC ) & 1.57 & 21.90   \\
\citep{tao2023dual} (Dual Focal with ResNet50) & 1.08 & 22.67   \\
\citep{tao2023dual} (Dual Focal with ResNet110) & 2.90 & 22.59  \\
\citep{NEURIPS2022_0a692a24} (AdaFocal with ResNet50) & 1.36 & 22.60  \\
\citep{NEURIPS2022_0a692a24} (AdaFocal with ResNet50) & 1.40 & 22.79 \\
Meta-Regularization (Ours) &  1.94 & 23.69  \\ \hline
\end{tabular}
\caption{Comparing meta-regularization (ours) to recent (non-meta) regularization methods on CIFAR-10 and CIFAR-100. Meta-regularization provides very competitive predictive and calibration performance compared to conventional regularization methods (the numbers are obtained from the corresponding papers). } 
\label{tab:more_recent_methods}
\end{table}

\textbf{Limitations}. Our proposed method utilizes \gnet for learning $\gamma$, which increases the number of parameters by approximately 0.46\% during training. However, since \gnet is not retained after training, there is no additional computational complexity during inference.
\section{Conclusion}
In this work, we have introduced a meta-learning based approach for acquiring well-calibrated models and demonstrated the advantages of two newly introduced components. By learning a sample-wise $\gamma$ for Focal loss using $\gamma$-Net, we achieve both strong predictive performance and unbiased, robust calibration. The optimization of \gnet with \sece proves crucial in ensuring stable calibration compared to baselines. Through extensive empirical results on three computer vision datasets, we have demonstrated that our method enhances calibration capability without altering the original networks.

%\subsubsection*{Broader Impact Statement}
%In this optional section, TMLR encourages authors to discuss possible repercussions of their work,
%notably any potential negative impact that a user of this research should be aware of. 
%Authors should consult the TMLR Ethics Guidelines available on the TMLR website
%for guidance on how to approach this subject.


%\subsubsection*{Author Contributions}
%If you'd like to, you may include a section for author contributions as is done
%in many journals. This is optional and at the discretion of the authors. Only add
%this information once your submission is accepted and deanonymized. 
%
%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments, including those to funding agencies, go at the end of the paper.
%Only add this information once your submission is accepted and deanonymized. 

\bibliography{main}
\bibliographystyle{tmlr}

\newpage
\appendix
\section{Appendix}
\subsection{Evaluation Metrics}
\label{sec:more_metric}

Besides ECE and MCE, we evaluate models on Classwise ECE and Adaptive ECE.

\textbf{Classwise ECE}~\citep{https://doi.org/10.48550/arxiv.1904.01685}. Classwise ECE extends the bin-based ECE to measure calibration across all the possible classes. In practice, predictions are binned separately for each class and the calibration error is computed at the level of individual class-bins and then averaged. The metric can be formulated as
\begin{equation}
\textsc{CECE} = \sum_{m=1}^{M} \sum_{c=1}^{K} \frac{|b_{m, c}|}{N K} |\textsc{acc}(b_{m, c}) - \textsc{conf}(b_{m, c})|
\end{equation}
where $N$ is the total number of samples, $K$ is the number of classes and $b_{m, c}$ represents a single bin for class $c$. In this formulation, $\textsc{acc}(b_{m, c})$ represents average binary accuracy for class $c$ over bin $b_{m, c}$ and $\textsc{conf}(b_{m, c})$ represents average confidence for class $c$ over bin $b_{m, c}$.


\textbf{Adaptive ECE}~\citep{https://doi.org/10.48550/arxiv.1904.01685}. Adaptive ECE further extends the Classwise variant of expected calibration error by introducing a new binning strategy which focuses the measurement on the confidence regions with multiple predictions and. Concretely Adaptive ECE (ACE) spaces the bin intervals such that each contains an equal number of predictions. The metric can be formulated as  
\begin{equation}
\textsc{ACE} = \sum_{r=1}^{R} \sum_{c=1}^{K} \frac{1}{RK} |\textsc{acc}(b_{r, c}) - \textsc{conf}(b_{r, c})|
\end{equation}
where $r$ is a calibration range that is defined by the $[\frac{N}{R}]$-th index of the sorted and thresholded predictions. In this formulation, $\textsc{acc}(b_{r, c})$ represents the average accuracy for class $c$ over the calibration range $r$ and $\textsc{conf}(b_{r, c})$ represents the average confidence for class $c$ over calibration range $r$.

\subsection{Algorithm}
\label{sec:alg}
Algorithm~\ref{algorithm:meta_learning} describes the learning procedures, it relies on the training and validation sets $D_{train}$, $D_{val}$, two networks $f^{c}$ and $f^{\gamma}$ with parameters $\theta$ and $\phi$. The optimization proceeds in an iterative process until both sets converge. Each iteration starts by sampling a mini-batch of training data $(\mathbf{x}_{i}^t, \mathbf{y}_{i}^t) \sim D_{train}$ as well as a mini-batch of validation data $(\mathbf{x}_{i}^v, \mathbf{y}_{i}^v) \sim D_{val}$. The training mini-batch is used to find the optimal value of $\gamma$ as $\gamma = f^{\gamma} (\mathbf{x}_{i}^t)$ and following that, compute the focal loss on the outputs of the backbone network $f^c$. The gradient of the loss $\mathcal{L}^f_{\gamma} ({f^{c}(\mathbf{x}_{i}^t), \mathbf{y}_{i}^t})$ is then used to update the parameters of the backbone network $\theta$ using the Adam algorithm~\citep{kingma2014adam}.

Once the backbone model $f^{c}$ is updated, it is used to compute the value of the auxiliary loss \sece on the validation mini-batch \sece$({f^{c}(\mathbf{x}_{i}^v), \mathbf{y}_{i}^v})$. The auxiliary \sece loss is a differentiable proxy of the true expected calibration error and can be minimized w.r.t. $\phi$ to find the optimal parameters of $\gamma$-Net. It is important to note that \sece is computed based on the outputs of $f^{c}(\mathbf{x}_{i}^v)$ and the only dependence on $\gamma$ is an indirect one based on the updates of $\theta$, as discussed in Section~\ref{section:meta_learning}. 
\begin{algorithm}
\DontPrintSemicolon
 
  \KwInput{$f^{c}$ and $f^{\gamma}$ with initialized $\theta$ and $\phi$}
  \KwOutput{Optimized $\theta$ and $\phi$}
  \KwData{Training and validation sets: $D_{train}, D_{val}$}
   \While{$\theta$ not converged}
   {
$(\mathbf{x}_{i}^t, \mathbf{y}_{i}^t) \sim D_{train}; ~~~(\mathbf{x}_{i}^v, \mathbf{y}_{i}^v) \sim D_{val}$; \# Sample a mini-batch from both datasets \\
$\gamma_i = f^{\gamma} (\mathbf{x}_{i}^t)$; \# learning sample-wise $\gamma_i$ \\
$\mathcal{L}^f_{\gamma} = \mathcal{L}^f_{\gamma} ({f^{c}(\mathbf{x}_{i}^t), \mathbf{y}_{i}^t})$: \#  Compute training loss based on $\gamma$\
$\theta := \theta - \eta \nabla_{\theta} \mathcal{L}^f_{\gamma}$: \#  Update parameters of $f^c$ using training loss gradient\;
$\sece = \sece({f^{c}(\mathbf{x}_{i}^v), \mathbf{y}_{i}^v})$ \#  Compute \sece auxiliary loss using validation batch\;
$\phi := \phi - \eta_{\phi} \nabla_{\phi} \sece$: \#  Update parameters of $f^{\gamma}$ using \sece gradient\;
   }
\caption{Meta optimization with $\gamma$-Net and SECE}
\label{algorithm:meta_learning}
\end{algorithm}

\subsection{Additional Experiments}
%\label{sec:cifar100_bin_size}
\begin{figure}[htb]
	\vspace{-6mm}
	\centering
	\subfloat[CIFAR-10 (Error)]{{\includegraphics[width=0.4\textwidth]{figure/rn18_c10_test_err.pdf} }}
	%\subfloat[CIFAR-10 (ECE)]{{\includegraphics[width=0.4\textwidth]{figure/rn18_c10_test_ece.pdf} }}  
	\subfloat[CIFAR-100 (Error)]{{\includegraphics[width=0.4\textwidth]{figure/rn18_c100_test_err.pdf} }}
	%\subfloat[CIFAR-100 (ECE)]{{\includegraphics[width=0.4\textwidth]{figure/rn18_c100_test_ece.pdf} }}
	\caption{Test error on both test datasets.}
\label{fig:test_learning_curve_error} 
\end{figure}

\subsubsection{Test Error Curves}
\label{sec:test error}
Figure~\ref{fig:test_learning_curve_error} presents the test error on CIFAR-10 and CIFAR-100 for the compared methods. In general, they exhibit similar behavior. 
\subsubsection{Reliability Diagram Plots}
\label{sec:more_reliability}
\begin{figure}[!htb]
	\centering
        \subfloat[\scriptsize{CE (4.94\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_ce_eval_4.pdf} }}
	\subfloat[\scriptsize{CE (TS) (4.94\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_ce_ts_eval_4.pdf} }}
	\subfloat[\scriptsize{Focal (5.01\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_focal_eval_4.pdf} }}
	\subfloat[\scriptsize{FLSD(5.02\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_focal_schedule_eval_4.pdf} }} 
	\subfloat[\scriptsize{LS-0.05 (4.67\%)} ]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_smoothing_0.05_eval_4.pdf} }} \\
	\subfloat[\scriptsize{MMCE (4.83\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_mmce_eval_4.pdf} }} 
	\subfloat[\scriptsize{CE-DECE(5.19\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_ce_dece_eval_4.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE(5.36\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_focal_dece_eval_4.pdf} }}
	\subfloat[\scriptsize{CE-SECE (5.06\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_ce_sece_eval_4.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-SECE(5.38\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_focal_sece_eval_4.pdf} }}
	\caption{The reliability diagram plots for models on CIFAR-10 test set. The ($\cdot$) denotes test error. The diagonal dashed line represents perfect calibration. The red bar represents the gap between the observed accuracy and the desired accuracy of the perfectly calibrated model (diagonal) - it is positive if the observed accuracy is lower and negative otherwise. The model from the 5$^{th}$ run is used.}
\label{fig:rc_c10_calibration} 
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{figure/bin_vs_ece_mce_cifar100.pdf}
\caption{The changes in ECE (left) and MCE (right) scores on the CIFAR-100 test dataset with increasing bin numbers in the range of [10, 20, 50, 100, 200, 500, 1000] are illustrated. Our proposed approach shows better robustness on bin sizes.}
\label{fig:cifar100_n_bins} 
\end{figure}

\begin{figure}[!htb]
	\centering
  	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_dece_eval_4_20.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_sece_eval_4_20.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_dece_eval_4_20.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_sece_eval_4_20.pdf} }} \\
 	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_dece_eval_4_50.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_sece_eval_4_50.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_dece_eval_4_50.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_sece_eval_4_50.pdf} }} \\
	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_dece_eval_4_100.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_sece_eval_4_100.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_dece_eval_4_100.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$+SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_sece_eval_4_100.pdf} }} \\
	\caption{The reliability diagram plots for models on CIFAR-10 with large bin numbers (from top to bottom: 20, 50, 100). The diagonal dashed line represents perfect calibration, while red bar indicates the deviation from perfect calibration in each bin. The model from the 5$^{th}$ run is used.}
\label{fig:rc_c10_calibration_more} 
\end{figure}

Figure~\ref{fig:cifar100_n_bins} examines the robustness of these methods with different binning schemes by varying the number of bins. It shows that \gnet based approaches (FL$_{\gamma}$-DECE and FL$_{\gamma}$-SECE) maintain much lower ECE score throughout all bin numbers from 10 to 1000 indicating that the trained based network is robustly calibrated.

In Figure~\ref{fig:rc_c10_calibration_more}, we provide the comparison across meta-learning based approaches via reliability plots with large bin numbers and corresponding ECE and MCE. FL$_{\gamma}$-SECE is able to maintain the lowest ECE and MCE; it shows robustness to increased bin numbers.

\subsubsection{Experiments with DenseNet}
\begin{table}[!ht]
\centering
\footnotesize
\begin{tabular}{lllllll} \hline
Methods & Error & NLL & ECE & MCE & ACE & Classwise ECE \\
 CE & 5.354 $\pm$ 0.097 & 0.220 $\pm$ 0.015 & 3.038 $\pm$ 0.159 & 33.419 $\pm$ 19.597 & 3.035 $\pm$ 0.157 & 0.677 $\pm$ 0.031 \\
 CE-DECE & 5.850 $\pm$ 0.421 & 0.236 $\pm$ 0.017 & 3.245 $\pm$ 0.471 & 25.062 $\pm$ 2.184 & 3.240 $\pm$ 0.471 & 0.723 $\pm$ 0.072 \\
 CE-SECE & 5.895 $\pm$ 0.271 & 0.246 $\pm$ 0.032 & 3.555 $\pm$ 0.428 & 24.518 $\pm$ 3.613 & 3.555 $\pm$ 0.425 & 0.756 $\pm$ 0.083 \\
 FL$_\gamma$-DECE & 6.084 $\pm$ 0.188 & 0.199 $\pm$ 0.012 & 2.151 $\pm$ 1.499 & 24.644 $\pm$ 11.227 & 2.139 $\pm$ 1.457 & 0.605 $\pm$ 0.201 \\
 FL$_\gamma$-SECE & 6.263 $\pm$ 0.266 & 0.208 $\pm$ 0.021 & 2.549 $\pm$ 2.233 & 20.220 $\pm$ 5.235 & 2.559 $\pm$ 2.217 & 0.672 $\pm$ 0.355 \\\hline
\end{tabular} 
\caption{DenseNet (100 layers) on CIFAR-10 with meta-learning-based methods, 5 repetitions. gamma-net based methods exhibit better calibration across multiple metrics (NLL, ECE, ACE, Classwise ECE) while maintaining competitive test error.}
\label{tab:densenet_baselines}
\end{table}

We conducted experiments on DenseNet with meta-learning baselines. Table~\ref{tab:densenet_baselines} presents the effectiveness of gamma-net based methods, which exhibit better calibration across multiple metrics with comparable predictive performance.

\subsection{The Predictive-Calibration Trade-off}
\label{sec:trade-off}
Figure~\ref{fig:trade-off} depicts the empirical trade-off between predictive (measured by test error) and calibration (measured by ECE) performance. We can observe that many methods exist on the Pareto front. For instance, mixup ($\alpha=0.1$), LS(0.1) and FL$_{\gamma}$-SECE achieves the low test error but high ECE on CIFAR datasets. This indicates there is predictive-calibration trade-off for most of methods. Similar Pareto fronts are observed in pretained large language models (LLMs)~\citep{stengel2023calibrated}. However, as shown in Table ~\ref{tab:diff}, when evaluating the ECE improvement for our proposed FL$_{\gamma}$-SECE method, the reduction in predictive capability is modest.
\begin{table}[!htb]
\centering
\begin{tabular}{lcccccc}
\hline
 Methods & \multicolumn{2}{l}{CIFAR-100} & \multicolumn{2}{l}{CIFAR-100} & \multicolumn{2}{l}{Tiny-ImageNet} \\ \hline
 & ($\Delta_{error}$) & $\Delta_{ECE}$ & ($\Delta_{error}$) & $\Delta_{ECE}$ & ($\Delta_{error}$) & $\Delta_{ECE}$ \\
Focal & 0.062 & -0.863 & -0.072 & -3.336 & -0.695 & -0.459 \\
FLSD & 0.104 & 2.848 & 0.086 & -2.424 & -0.405 & 6.442 \\
LS(1.0) & 0.106 & 3.510 & -0.326 & -3.626 & -0.715 & 8.718 \\
Mixup($\alpha=1.0$) & -0.686 & 8.807 & -1.360 & 1.336 & -0.220 & 4.074 \\
MMCE & -0.004 & -0.029 & -0.080 & 0.333 & 0.200 & 0.147 \\
CE-DECE & 0.382 & 0.050 & -0.164 & -1.071 & 1.240 & 2.635 \\
FL$_{\gamma}$-SECE & 0.616 & -1.918 & 1.116 & -6.440 & 0.740 & -2.265 \\ \hline
\end{tabular}
\caption{The test error difference  ($\Delta_{error}$) and ECE difference ($\Delta_{ECE}$) for different methods compared to the baseline (CE).}
\label{tab:diff}
\end{table}

\clearpage

\begin{figure}[!htb]
\centering
\subfloat[\scriptsize{CIFAR-10}]{{\includegraphics[width=0.33\textwidth]{figure/pareto_CIFAR10.pdf} }}
\subfloat[\scriptsize{CIFAR-100}]{{\includegraphics[width=0.33\textwidth]{figure/pareto_CIFAR100.pdf} }}
\subfloat[\scriptsize{Tiny-ImageNet}]{{\includegraphics[width=0.33\textwidth]{figure/pareto_Tiny-ImageNet.pdf} }}
\caption{The trade-off between predictive and calibration performance of different methods on the test set of CIFAR-10, CIFAR-100 and Tiny-ImageNet. The Pareto front are observed for most of methods.}
\label{fig:trade-off} 
\end{figure}



\end{document}
