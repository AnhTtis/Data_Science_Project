@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}
@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT},
  year={2019}
}
@misc{imagenet_tiny,

  url = {http://vision.stanford.edu/teaching/cs231n/reports/2015/pdfs/yle_project.pdf},
  
  author = {Ya Le, Xuan Yang},
  
  
  title = {Tiny ImageNet Visual Recognition Challenge},
  
  publisher = {Stanford, CS 231N},
  
  year = {2015},
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@misc{zhang2020_mixmatch,
  doi = {10.48550/ARXIV.2003.07329},
  
  url = {https://arxiv.org/abs/2003.07329},
  
  author = {Zhang, Jize and Kailkhura, Bhavya and Han, T. Yong-Jin},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Mix-n-Match: Ensemble and Compositional Methods for Uncertainty Calibration in Deep Learning},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}
@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019}
}
@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={ICLR},
  year={2017}
}
@InProceedings{MaddisonETAL:17,
  author    = {Chris J. Maddison and Andriy Mnih and Yee Whye Teh},
  title     = {{The Concrete Distribution: {A} Continuous Relaxation of Discrete Random Variables}},
  booktitle = {{5th International Conference on Learning Representations (ICLR)}},
  year      = {2017},
}
@Article{Gumbel:54,
	author  = {Emil Julius Gumbel},
	title   = {{Statistical Theory of Extreme Values and Some Practical Applications. A Series of Lectures.}},
	journal = {Number 33. US Govt. Print. Office},
	year    = {1954},
}
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986}
}
@article{mukhoti2020calibrating,
  title={Calibrating deep neural networks using focal loss},
  author={Mukhoti, Jishnu and Kulharia, Viveka and Sanyal, Amartya and Golodetz, Stuart and Torr, Philip and Dokania, Puneet},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15288--15299},
  year={2020}
}
@Article{DeGrootFienberg:83,
  author  = {DeGroot, Morris and Fienberg, Stephen},
  title   = {{The comparison and evaluation of forecasters}},
  journal = {{The Statistician}},
  year    = {1983},
}
@InProceedings{NaeiniETAL:15,
  author    = {Mahdi Pakdaman Naeini and Gregory F. Cooper and Milos Hauskrecht},
  title     = {{Obtaining well calibrated probabilities using bayesian binning}},
  booktitle = {{Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI)}},
  year      = {2015},
}

@InProceedings{Niculescu-MizilETAL:05,
  author    = {Niculescu-Mizil, Alexandru and Caruana, Rich},
  title     = {{Predicting Good Probabilities with Supervised Learning}},
  booktitle = {{Proceedings of the 22nd International Conference on Machine Learning (ICML)}},
  year      = {2005},
}
@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}
@inproceedings{graves2013speech,
  title={Speech recognition with deep recurrent neural networks},
  author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={6645--6649},
  year={2013},
  organization={Ieee}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International Conference on Machine Learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}
@inproceedings{niculescu2005predicting,
  title={Predicting good probabilities with supervised learning},
  author={Niculescu-Mizil, Alexandru and Caruana, Rich},
  booktitle={Proceedings of the 22nd international conference on Machine learning},
  pages={625--632},
  year={2005}
}
@inproceedings{caruana2015intelligible,
  title={Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission},
  author={Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
  booktitle={Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1721--1730},
  year={2015}
}
@article{bojarski2016end,
  title={End to end learning for self-driving cars},
  author={Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and others},
  journal={arXiv preprint arXiv:1604.07316},
  year={2016}
}
@inproceedings{kumar2018trainable,
  title={Trainable calibration measures for neural networks from kernel mean embeddings},
  author={Kumar, Aviral and Sarawagi, Sunita and Jain, Ujjwal},
  booktitle={International Conference on Machine Learning},
  pages={2805--2814},
  year={2018},
  organization={PMLR}
}
@article{muller2019does,
  title={When does label smoothing help?},
  author={M{\"u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{bohdal2021meta,
  title={Meta-Calibration: Meta-Learning of Model Calibration Using Differentiable Expected Calibration Error},
  author={Bohdal, Ondrej and Yang, Yongxin and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2106.09613},
  year={2021}
}
@article{bohdal2023meta,
  title={Meta-Calibration: Learning of Model Calibration Using Differentiable Expected Calibration Error},
  author={Bohdal, Ondrej and Yang, Yongxin and Hospedales, Timothy},
  journal={Transactions on Machine Learning Research},
  year={2023}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{NEURIPS2019_8ca01ea9,
 author = {Kull, Meelis and Perello Nieto, Miquel and K\"{a}ngsepp, Markus and Silva Filho, Telmo and Song, Hao and Flach, Peter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration},
 url = {https://proceedings.neurips.cc/paper/2019/file/8ca01ea920679a0fe3728441494041b9-Paper.pdf},
 volume = {32},
 year = {2019}
}
@article{brier1950verification,
  title={Verification of forecasts expressed in terms of probability},
  author={Brier, Glenn W and others},
  journal={Monthly weather review},
  volume={78},
  number={1},
  pages={1--3},
  year={1950}
}
@misc{https://doi.org/10.48550/arxiv.1904.01685,
  doi = {10.48550/ARXIV.1904.01685},
  
  url = {https://arxiv.org/abs/1904.01685},
  
  author = {Nixon, Jeremy and Dusenberry, Mike and Jerfel, Ghassen and Nguyen, Timothy and Liu, Jeremiah and Zhang, Linchuan and Tran, Dustin},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Measuring Calibration in Deep Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Huang2016,
   abstract = {Recent work has shown that convolutional networks can be substantially
deeper, more accurate, and efficient to train if they contain shorter
connections between layers close to the input and those close to the output. In
this paper, we embrace this observation and introduce the Dense Convolutional
Network (DenseNet), which connects each layer to every other layer in a
feed-forward fashion. Whereas traditional convolutional networks with L layers
have L connections - one between each layer and its subsequent layer - our
network has L(L+1)/2 direct connections. For each layer, the feature-maps of
all preceding layers are used as inputs, and its own feature-maps are used as
inputs into all subsequent layers. DenseNets have several compelling
advantages: they alleviate the vanishing-gradient problem, strengthen feature
propagation, encourage feature reuse, and substantially reduce the number of
parameters. We evaluate our proposed architecture on four highly competitive
object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).
DenseNets obtain significant improvements over the state-of-the-art on most of
them, whilst requiring less computation to achieve high performance. Code and
pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
   author = {Gao Huang and Zhuang Liu and Laurens Van Der Maaten and Kilian Q. Weinberger},
   doi = {10.48550/arxiv.1608.06993},
   isbn = {9781538604571},
   journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   month = {8},
   pages = {2261-2269},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Densely Connected Convolutional Networks},
   volume = {2017-January},
   url = {https://arxiv.org/abs/1608.06993v5},
   year = {2016},
}
@misc{https://doi.org/10.48550/arxiv.1810.11586,
  doi = {10.48550/ARXIV.1810.11586},
  
  url = {https://arxiv.org/abs/1810.11586},
  
  author = {Mozafari, Azadeh Sadat and Gomes, Hugo Siqueira and Leão, Wilson and Janny, Steeven and Gagné, Christian},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attended Temperature Scaling: A Practical Approach for Calibrating Deep Neural Networks},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Zhai2021,
   abstract = {Attention-based neural networks such as the Vision Transformer (ViT) have
recently attained state-of-the-art results on many computer vision benchmarks.
Scale is a primary ingredient in attaining excellent results, therefore,
understanding a model's scaling properties is a key to designing future
generations effectively. While the laws for scaling Transformer language models
have been studied, it is unknown how Vision Transformers scale. To address
this, we scale ViT models and data, both up and down, and characterize the
relationships between error rate, data, and compute. Along the way, we refine
the architecture and training of ViT, reducing memory consumption and
increasing accuracy the resulting models. As a result, we successfully train a
ViT model with two billion parameters, which attains a new state-of-the-art on
ImageNet of 90.45% top-1 accuracy. The model also performs well on few-shot
learning, for example, attaining 84.86% top-1 accuracy on ImageNet with only 10
examples per class.},
   author = {Xiaohua Zhai and Alexander Kolesnikov and Neil Houlsby and Lucas Beyer},
   doi = {10.48550/arxiv.2106.04560},
   month = {6},
   title = {Scaling Vision Transformers},
   url = {https://arxiv.org/abs/2106.04560v1},
   year = {2021},
}
@article{Luketina2015,
   abstract = {Hyperparameter selection generally relies on running multiple full training
trials, with selection based on validation set performance. We propose a
gradient-based approach for locally adjusting hyperparameters during training
of the model. Hyperparameters are adjusted so as to make the model parameter
gradients, and hence updates, more advantageous for the validation cost. We
explore the approach for tuning regularization hyperparameters and find that in
experiments on MNIST, SVHN and CIFAR-10, the resulting regularization levels
are within the optimal regions. The additional computational cost depends on
how frequently the hyperparameters are trained, but the tested scheme adds only
30% computational overhead regardless of the model size. Since the method is
significantly less computationally demanding compared to similar gradient-based
approaches to hyperparameter optimization, and consistently finds good
hyperparameter values, it can be a useful tool for training neural network
models.},
   author = {Jelena Luketina and Mathias Berglund and Klaus Greff and Tapani Raiko},
   doi = {10.48550/arxiv.1511.06727},
   isbn = {9781510829008},
   journal = {33rd International Conference on Machine Learning, ICML 2016},
   month = {11},
   pages = {4333-4341},
   publisher = {International Machine Learning Society (IMLS)},
   title = {Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters},
   volume = {6},
   url = {https://arxiv.org/abs/1511.06727v3},
   year = {2015},
}
@article{Falkner2018,
   abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and,
due to the long training times of state-of-the-art models, vanilla Bayesian
hyperparameter optimization is typically computationally infeasible. On the
other hand, bandit-based configuration evaluation approaches based on random
search lack guidance and do not converge to the best configurations as quickly.
Here, we propose to combine the benefits of both Bayesian optimization and
bandit-based methods, in order to achieve the best of both worlds: strong
anytime performance and fast convergence to optimal configurations. We propose
a new practical state-of-the-art hyperparameter optimization method, which
consistently outperforms both Bayesian optimization and Hyperband on a wide
range of problem types, including high-dimensional toy functions, support
vector machines, feed-forward neural networks, Bayesian neural networks, deep
reinforcement learning, and convolutional neural networks. Our method is robust
and versatile, while at the same time being conceptually simple and easy to
implement.},
   author = {Stefan Falkner and Aaron Klein and Frank Hutter},
   doi = {10.48550/arxiv.1807.01774},
   isbn = {9781510867963},
   journal = {35th International Conference on Machine Learning, ICML 2018},
   month = {7},
   pages = {2323-2341},
   publisher = {International Machine Learning Society (IMLS)},
   title = {BOHB: Robust and Efficient Hyperparameter Optimization at Scale},
   volume = {4},
   url = {https://arxiv.org/abs/1807.01774v1},
   year = {2018},
}
@article{Li2016,
   abstract = {Performance of machine learning algorithms depends critically on identifying
a good set of hyperparameters. While recent approaches use Bayesian
optimization to adaptively select configurations, we focus on speeding up
random search through adaptive resource allocation and early-stopping. We
formulate hyperparameter optimization as a pure-exploration non-stochastic
infinite-armed bandit problem where a predefined resource like iterations, data
samples, or features is allocated to randomly sampled configurations. We
introduce a novel algorithm, Hyperband, for this framework and analyze its
theoretical properties, providing several desirable guarantees. Furthermore, we
compare Hyperband with popular Bayesian optimization methods on a suite of
hyperparameter optimization problems. We observe that Hyperband can provide
over an order-of-magnitude speedup over our competitor set on a variety of
deep-learning and kernel-based learning problems.},
   author = {Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
   doi = {10.48550/arxiv.1603.06560},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   keywords = {Deep learning,Hyperparameter optimization,Infinite-armed bandits,Model selection,Online optimization},
   month = {3},
   pages = {1-52},
   publisher = {Microtome Publishing},
   title = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
   volume = {18},
   url = {https://arxiv.org/abs/1603.06560v4},
   year = {2016},
}
@article{Snoek2012,
   abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \{\textquoteleft\}\{\textquoteleft\}black art\{\textquoteright\}\{\textquoteright\} requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm\{\textquoteright\}s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
   author = {Jasper Snoek and Hugo Larochelle and Ryan P Adams},
   doi = {2012arXiv1206.2944S},
   isbn = {9781627480031},
   issn = {10495258},
   journal = {Adv. Neural Inf. Process. Syst. 25},
   keywords = {bayesian optimization,deep learning,gaussian process},
   pages = {1-9},
   pmid = {9377276},
   title = {Practical Bayesian Optimization of Machine Learning Algorithms},
   url = {https://arxiv.org/pdf/1206.2944.pdf},
   year = {2012},
}
@article{corless1996lambertw,
  title={On the LambertW function},
  author={Corless, Robert M and Gonnet, Gaston H and Hare, David EG and Jeffrey, David J and Knuth, Donald E},
  journal={Advances in Computational mathematics},
  volume={5},
  number={1},
  pages={329--359},
  year={1996},
  publisher={Springer}
}
@article{minderer2021revisiting,
  title={Revisiting the calibration of modern neural networks},
  author={Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and Hubis, Frances and Zhai, Xiaohua and Houlsby, Neil and Tran, Dustin and Lucic, Mario},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{10.1007/s10791-009-9124-x,
author = {Qin, Tao and Liu, Tie-Yan and Li, Hang},
title = {A General Approximation Framework for Direct Optimization of Information Retrieval Measures},
year = {2010},
issue_date = {August    2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {4},
issn = {1386-4564},
url = {https://doi.org/10.1007/s10791-009-9124-x},
doi = {10.1007/s10791-009-9124-x},
abstract = {Recently direct optimization of information retrieval (IR) measures has become a new trend in learning to rank. In this paper, we propose a general framework for direct optimization of IR measures, which enjoys several theoretical advantages. The general framework, which can be used to optimize most IR measures, addresses the task by approximating the IR measures and optimizing the approximated surrogate functions. Theoretical analysis shows that a high approximation accuracy can be achieved by the framework. We take average precision (AP) and normalized discounted cumulated gains (NDCG) as examples to demonstrate how to realize the proposed framework. Experiments on benchmark datasets show that the algorithms deduced from our framework are very effective when compared to existing methods. The empirical results also agree well with the theoretical results obtained in the paper.},
journal = {Inf. Retr.},
month = {aug},
pages = {375–397},
numpages = {23},
keywords = {Accuracy analysis, Position function approximation, Learning to rank, Truncation function approximation, Direct optimization of IR measures}
}
@inproceedings{zadrozny2002transforming,
  title = {Transforming classifier scores into accurate multiclass probability estimates},
  author = {Zadrozny, Bianca and Elkan, Charles},
  booktitle = {Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages = {694--699},
  year = {2002},
  organization = {ACM}
}
@article{platt1999probabilistic,
  title = {Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods},
  author = {Platt, John and others},
  journal = {Advances in large margin classifiers},
  volume = {10},
  number = {3},
  pages = {61--74},
  year = {1999},
  publisher = {Cambridge, MA}
}
@inproceedings{zadrozny2001obtaining,
  title={Obtaining calibrated probability estimates from decision trees and naive {B}ayesian classifiers},
  author={Zadrozny, Bianca and Elkan, Charles},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
  pages={609--616},
  year={2001},
  organization={Citeseer}
}
@inproceedings{zhang2018mixup,
  title={mixup: Beyond Empirical Risk Minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{guiasu1985principle,
  title={The principle of maximum entropy},
  author={Guiasu, Silviu and Shenitzer, Abe},
  journal={The mathematical intelligencer},
  volume={7},
  number={1},
  pages={42--48},
  year={1985},
  publisher={Springer-Verlag}
}
@article{pereyra2017regularizing,
  title={Regularizing neural networks by penalizing confident output distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}
@article{thulasidasan2019mixup,
  title={On mixup training: Improved calibration and predictive uncertainty for deep neural networks},
  author={Thulasidasan, Sunil and Chennupati, Gopinath and Bilmes, Jeff A and Bhattacharya, Tanmoy and Michalak, Sarah},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{sklearn-info,
  title={https://scikit-learn.org/stable/modules/calibration.html#calibration},
  date={2022-12-03},
}
@article{gawlikowski2021survey,
  title={A survey of uncertainty in deep neural networks},
  author={Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and others},
  journal={arXiv preprint arXiv:2107.03342},
  year={2021}
}

@inproceedings{lakshminarayanan2017simple,
	Author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	Booktitle = {NeurIPS'17},
	Date-Modified = {2021-12-23 21:55:34 +0100},
	Title = {Simple and scalable predictive uncertainty estimation using deep ensembles},
	Year = {2017}}
@inproceedings{gal2016dropout,
	Author = {Gal, Yarin and Ghahramani, Zoubin},
	Booktitle = {ICML'16},
	Date-Modified = {2021-08-28 06:17:35 +0200},
	Organization = {PMLR},
	Pages = {1050--1059},
	Title = {Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
	Year = {2016}}
@article{maddox2019simple,
  title={A simple baseline for bayesian uncertainty in deep learning},
  author={Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
  journal={NeurIPS},
  volume={32},
  year={2019}
}
@article{krishnan2020improving,
  title={Improving model calibration with accuracy versus uncertainty optimization},
  author={Krishnan, Ranganath and Tickoo, Omesh},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18237--18248},
  year={2020}
}
@inproceedings{cheng2022calibrating,
  title={Calibrating deep neural networks by pairwise constraints},
  author={Cheng, Jiacheng and Vasconcelos, Nuno},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13709--13718},
  year={2022}
}
@inproceedings{liu2022devil,
  title={The devil is in the margin: Margin-based label smoothing for network calibration},
  author={Liu, Bingyuan and Ben Ayed, Ismail and Galdran, Adrian and Dolz, Jose},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={80--88},
  year={2022}
}
@inproceedings{patra2023calibrating,
  title={Calibrating deep neural networks using explicit regularisation and dynamic data pruning},
  author={Patra, Rishabh and Hebbalaguppe, Ramya and Dash, Tirtharaj and Shroff, Gautam and Vig, Lovekesh},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1541--1549},
  year={2023}
}
@inproceedings{hebbalaguppe2022stitch,
  title={A stitch in time saves nine: A train-time regularizing loss for improved neural network calibration},
  author={Hebbalaguppe, Ramya and Prakash, Jatin and Madan, Neelabh and Arora, Chetan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16081--16090},
  year={2022}
}
@article{karandikar2021soft,
  title={Soft calibration objectives for neural networks},
  author={Karandikar, Archit and Cain, Nicholas and Tran, Dustin and Lakshminarayanan, Balaji and Shlens, Jonathon and Mozer, Michael C and Roelofs, Becca},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29768--29779},
  year={2021}
}
@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}
@article{stengel2023calibrated,
  title={Calibrated interpretation: Confidence estimation in semantic parsing},
  author={Stengel-Eskin, Elias and Van Durme, Benjamin},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={1213--1231},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}
@inproceedings{tao2023dual,
  title={Dual focal loss for calibration},
  author={Tao, Linwei and Dong, Minjing and Xu, Chang},
  booktitle={International Conference on Machine Learning},
  pages={33833--33849},
  year={2023},
  organization={PMLR}
}
@inproceedings{NEURIPS2022_0a692a24,
 author = {Ghosh, Arindam and Schaaf, Thomas and Gormley, Matthew},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {1583--1595},
 publisher = {Curran Associates, Inc.},
 title = {AdaFocal: Calibration-aware Adaptive Focal Loss},
 volume = {35},
 year = {2022}
}
@article{laves2019well,
  title={Well-calibrated model uncertainty with temperature scaling for dropout variational inference},
  author={Laves, Max-Heinrich and Ihler, Sontje and Kortmann, Karl-Philipp and Ortmaier, Tobias},
  journal={arXiv preprint arXiv:1909.13550},
  year={2019}
}
@article{kull2019beyond,
  title={Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration},
  author={Kull, Meelis and Perello Nieto, Miquel and K{\"a}ngsepp, Markus and Silva Filho, Telmo and Song, Hao and Flach, Peter},
  journal={NeurIPS},
  volume={32},
  year={2019}
}
@article{agarap2018deep,
  title={Deep learning using rectified linear units (relu)},
  author={Agarap, Abien Fred},
  journal={arXiv preprint arXiv:1803.08375},
  year={2018}
}