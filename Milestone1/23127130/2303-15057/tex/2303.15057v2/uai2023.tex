%\documentclass{uai2023} % for initial submission
\documentclass[accepted]{uai2023} % after acceptance, for a revised
                                    % version; also before submission to
                                    % see how the non-anonymous paper
                                    % would look like
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2023} % ptmx math instead of Computer
                                         % Modern (has noticable issues)
% \documentclass[mathfont=newtx]{uai2023} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{amsmath} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams
\usepackage{latexsym}
\usepackage{bm}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{caption}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{listings}
%\usepackage{paralist}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{wrapfig}
\usetikzlibrary{calc}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\newtheorem{definition}{Definition}
%\usepackage{algorithmic}


\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output


\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfr}{\mathbf{r}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfh}{\mathbf{h}}
\newcommand{\bfq}{\mathbf{q}}
\newcommand{\bfg}{\mathbf{g}}
\newcommand{\bfc}{\mathbf{c}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfxx}{\mathbf{\hat{x}}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfs}{\mathbf{s}}
\newcommand{\bfp}{\mathbf{p}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bfW}{\mathbf{W}}
\newcommand{\gnet}{$\gamma$-Net~}
\newcommand{\acc}{acc~}
\newcommand{\sacc}{SACC~}
\newcommand{\sece}{SECE~}
\newcommand{\dece}{dECE~}
\newcommand{\smeg}{sMEG-calibration~}
\usepackage{amsthm}
\newtheorem{corollary}{Corollary}
\newcommand{\cheng}[1]{\textcolor{orange}{\textbf{cheng}: #1}}
\newcommand{\jacek}[1]{\textcolor{teal}{\textbf{jacek}: #1}}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.6}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example

\title{Meta-Calibration Regularized Neural Networks}

% The standard author block has changed for UAI 2023 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
\author[1]{\href{mailto:<cwngam@amazon.com>?Subject=Your UAI 2023 paper}{Cheng Wang\textsuperscript{$\dagger$}}}
\author[1]{\href{mailto:<jacekgo@amazon.com>?Subject=Your UAI 2023 paper}{Jacek Golebiowski\textsuperscript{$\dagger$}}}
% Add affiliations after the authors
\affil[1]{%
    Amazon Development Center Germany GmbH\\
    Berlin, Germany
}


\begin{document}
\maketitle

\begin{abstract}
Miscalibration–the mismatch between predicted probability and the true correctness likelihood–has been frequently identified in modern deep neural networks. Recent work aims to address this problem by training calibrated models directly by optimizing a proxy of the calibration error alongside the conventional objective. Recently, Meta-Calibration (MC)~\cite{bohdal2021meta} showed the effectiveness of using meta-learning for learning better calibrated models. In this work, we extend MC with two main components: (1) gamma network (\gnet), a meta network to learn a sample-wise gamma at a continuous space for Focal loss that is used to optimize the backbone network; (2) smooth expected calibration error (\sece), a Gaussian-kernel based unbiased and differentiable surrogate to ECE which enables the smooth optimization \gnet. The proposed method regularizes the base neural network towards better calibration meanwhile retaining predictive performance. Our experiments show that (a) learning sample-wise $\gamma$ at continuous space can effectively perform calibration; (b) \sece smoothly optimise \gnet towards better robustness to binning schemes; (c)the combination of \gnet and \sece achieve the best calibration performance across various calibration metrics and retain very competitive predictive performance as compared to multiple recently proposed methods on three datasets.

\end{abstract}

\renewcommand\thefootnote{$\dagger$}
\footnotetext{Equal contribution.}
\section{Introduction}
Deep Neural Networks (DNNs) have shown promising predictive performance in many domains such as computer vision~\cite{krizhevsky2012imagenet}, speech recognition~\cite{graves2013speech} and natural language processing~\cite{vaswani2017attention}. As a result, trained deep neural network models are frequently deployed and utilized in real-world systems. However, recent work~\cite{guo2017calibration} pointed out that those highly accurate, negative log likelihood trained deep neural networks are often poorly calibrated~\cite{niculescu2005predicting}. Their predicted class probabilities do not faithfully estimate the true probability of correctness and lead to (primarily) overconfident and under-confident predictions. Deploying such miscalibrated models into real-world systems poses a high risk, particularly when model outputs are directly utilized to serve customers' requests in applications like medical diagnosis~\cite{caruana2015intelligible} and autonomous driving~\cite{bojarski2016end}. Better calibrated model probabilities can be used as an important source of signal towards more reliable machine learning systems. 
\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{figure/method.pdf}
\caption{Our proposed approach for regularizing base network towards better calibration with two new components: \gnet and \sece. The inner loop optimizes backbone network (e.g., resnet) which uses focal loss as objective function.  The \gnet in the outer loop takes the extracted second last layer representation of backbone network as input, learns to output sample-wise $\gamma$ for focal loss at a continuous space. The \gnet is optimised by using proposed \sece, a Gaussian-kernel based unbiased and differentiable calibration error.}
\label{fig: method} 
\end{figure}


Recently, ~\cite{bohdal2021meta} proposed to use meta-learning based approach to calibrate deep models. In their formulation the backbone network learns to optimize the regularized cross-entropy loss while a differentiable proxy to calibration error is used to tune the parameters of weight regularizer. Jishnu et al.~\cite{mukhoti2020calibrating} found that focal loss can effectively improve calibration and is a good alternative to regularized cross entropy. They also pointed out that the gamma parameter in focal loss plays a crucial role in making this approach effective. They proposed a sample-dependent schedule for gamma in focal loss (FLSD), which showed superior calibration performance as compared to baselines.

Motivated by both meta-calibration~\cite{bohdal2021meta} and focal loss~\cite{lin2017focal} with sample-dependent scheduled gamma (FLSD)~\cite{mukhoti2020calibrating} in improving model calibration, we introduce an approach that uses a meta network (named as gamma network, i.e. \gnet) to learn a sample-wise $\gamma$ parameter for the focal loss. Our proposed method is presented schematically in Figure~\ref{fig: method}. Different to FLSD, where a global gamma parameter is scheduled during training, in this work we learn a gamma parameter for each sample. To achieve this, the \gnet is optimised using smooth ECE (\sece), a differentiable surrogate to expected calibration error (ECE)~\cite{NaeiniETAL:15}. The output of \gnet, i.e. the learned $\gamma$ values are then used to optimize the backbone network towards better calibration; pushing network weights and biases to accurately asses predictive \emph{confidence} while retaining the original predictive \emph{performance}.


Our contributions can be summarised as follows:
\begin{itemize}
  \item We propose a \gnet, as meta-network to learn sample-wise $\gamma$ for Focal loss at a continuous space, rather than using pre-defined $\gamma$ value.
  \item We propose a kernel-based ECE estimator~\sece, a variation of the kernel density estimate (KDE) approximation \cite{zhang2020_mixmatch} to smoothly regularize the \gnet towards better calibration. 
  \item We empirically show that \gnet can effectively calibrate models and \sece provides stable and smooth calibration. Their combination achieves competitive predictive performance and better scores across multiple calibration metrics as compared to baselines.% methods such as label smoothing~\cite{muller2019does}, temperature scaling~\cite{caruana2015intelligible} and Maximum Mean Calibration Error (MMCE)~\cite{kumar2018trainable} etc.
\end{itemize}

\section{Related Work}
Calibration of ML models of using Platt scaling~\cite{platt1999probabilistic} and Isotonic Regression~\cite{zadrozny2002transforming} have showed significant improvements for SVMs and decision trees. With the advent of neural networks, Niculescu-Mizil \textit{et al.}~\cite{Niculescu-MizilETAL:05} showed those methods can produce well-calibrated probabilities even without any dedicated modifications. 
%However, NNs studied by Niculescu-Mizil \textit{et al.} in 2005 were relatively shallow and over the following decade, neural networks used to beat benchmarks on computer vision datasets
Recently,~\cite{Huang2016, Zhai2021} have performed deeper study on this aspect to accurately classify examples. Guo \textit{et al.}~\cite{guo2017calibration} and Mukhoti \textit{et al.} \cite{mukhoti2020calibrating} have shown that while modern NNs are noticeably more accurate but poorly calibrated due to negative log likelihood (NLL) overfitting. Minderer \textit{et al}\cite{minderer2021revisiting} revisited this problem and found that architectures with large size have a larger effect on calibration but nevertheless, more accurate models tend to produce less calibrated predictions.

%Results presented by Guo \textit{et al.}~\cite{guo2017calibration} and  showed that miscalibration of modern networks is related to the over-fitting on the likelihood of the training dataset by minimizing the negative log likelihood (NLL).
% which can be positive even if the accuracy is already perfect. Neural networks continue to minimize NLL during training after accuracy is optimal by becoming more confident in their predictions during training. This leads to overly confident true and false predictions at test time leading to miscalibration. 
One of the first methods to tackle calibration are post-hoc methods. 
%aimed to reduce ECE by modifying the outputs of an already trained model. 
%Approaches in this class convert the raw confidence outputs of the model into calibrated predictions using a transformation built to minimize calibration error on the validation set.
This includes non-parametric methods such as histogram binning~\cite{zadrozny2001obtaining}, isotonic regression~\cite{zadrozny2002transforming} and parametric methods such as Bayesian binning into quantiles (BBQ) and Platt scaling~\cite{platt1999probabilistic}. Beyond those four, temperature scaling (TS) is a single-parameter extension of Platt scaling~\cite{platt1999probabilistic} and the most recent addition to the offering of post-hoc methods. Recent work has showed that a single parameter TS gives good calibration performance with minimal added computational complexity~\cite{guo2017calibration,minderer2021revisiting}. Extensions to temperature scaling include attention-based mechanism to tackle noise in validation data~\cite{https://doi.org/10.48550/arxiv.1810.11586} and introduction of variational dropout based on TS to calibrate deep networks.

Calibration is often measured using the expected calibration error (ECE).
%- a difference between the model's confidence and accuracy. 
Minimizing this measure is the main goal of most calibration-promoting approaches and can be achieved by directly optimising the ECE or its proxy. Kumar \textit{et al.}~\cite{kumar2018trainable} developed a differentiable equivalent of ECE, the Maximum Mean Calibration Error, which can be optimized directly to train calibrated models. Similarly, Bohdal \textit{et al.}~\cite{bohdal2021meta} use a meta-training approach first presented by Luketina \textit{et al.}~\cite{Luketina2015} to train a model that minimizes the differentiable ECE (DECE) to find the optimal parameters of the L2 regualarizers. 

Finally, there are methods that do not include the calibration objective explicitly but rather implicitly guide the training towards better calibration performance. Focal loss~\cite{lin2017focal} which acts as a maximum entropy regularizer \cite{mukhoti2020calibrating} promoting calibration. Label smoothing~\cite{muller2019does} mitigates miscalibration by softening hard label with an introduced smoothing modifier in standard loss function (e.g., cross-entropy), while Mix-up training~\cite{thulasidasan2019mixup} extends \textit{mixup}~\cite{zhang2018mixup} to generate synthetic sample during model training by combining two random elements from the dataset.

In this work, we follow the recent meta-learning based approach and propose to learn a \gnet which offers instance-level regularization to base network towards better calibration. 

\section{Preliminaries}
\subsection{Model Calibration}
\label{section:calibration}
Calibration~\cite{guo2017calibration} measures and verifies how the predicted probability estimates the true likelihood of correctness.  Assume a model $\mathbf{m}$ trained with dataset $\{\mathbf{x}, y\}, \mathbf{x} \in \mathcal{X}, y \in \mathcal{Y}$. $\mathbf{\hat{p}}$ is the predicted softmax probability. If $\mathbf{m}$ makes 100 independent predictions, each with confidence $p=\arg\max(\mathbf{\hat{p}})=0.9$, ideally, a calibrated $\mathbf{m}$ approximately gives 90 correct predictions. Formally, $\text{accuracy}(\mathbf{m}(D))=\text{confidence}(\mathbf{m}(D))$ if $\mathbf{m}$ is perfectly calibrated on dataset $D$. It is difficult to achieve perfect calibration in practice.

\textbf{Reliability Diagrams} \cite{DeGrootFienberg:83,Niculescu-MizilETAL:05} visualise whether a model is over- or under-confident by grouping predictions into bins according to their prediction probability. The predictions are grouped into $N$ interval bins (each of of size $1/N$) and the accuracy of samples $y_i$ wrt. to the ground truth label $\hat{y}_i$ in each bin $b_n$ is computed as: $\textrm{acc}(b_n)=\frac{1}{|b_n|}\sum_{i}^{I}\bm{1}(\hat{y}_i=y_i)$,
where $i$ indexes all examples that fall into bin $b_n$.
Let $\hat{p}_i$ be the probability for the predicted class $y_i$ for $i$-th sample (i.e. the class with highest predicted probability for $i$-th sample), then average confidence is defined as: $\textrm{conf}(b_n)=\frac{1}{|b_n|}\sum_{i}^{I}\hat{p}_i$.
A model is perfectly calibrated if $\textrm{acc}(b_n)=\textrm{conf}(b_n), \forall n$ and in a diagram the bins would follow the identity function. Any deviation from this represents miscalibration.

\textbf{Expected Calibration Error (ECE)}~\cite{NaeiniETAL:15}. ECE computes the difference between model accuracy and confidence which, in the general form, can be written as
$
\mathrm{E}_{\hat{P}} [|\mathbb{P}(\hat{Y}=Y \mid \hat{P}=p)-p|]
$
where the expectation is taken over all class probabilities (confidence) $\hat P$. ECE in this form is impossible to compute and it is approximated by partitioning data points into bins with similar confidence resulting in a new formulation of
$
\textsc{ECE} = \sum_{n=1}^{N} \frac{|b_n|}{N} |\textrm{acc}(b_n) - \textrm{conf}(b_n)|,
$
where $N$ is the total number of samples and $b_n$ represents a single bin.

\textbf{Maximum Calibration Error (MCE)}~\cite{NaeiniETAL:15} is particularly important in high-risk applications where reliable confidence measures are absolutely necessary. It measures the worst-case deviation between accuracy and confidence,
$
\textsc{MCE} = max_{n\in \{1, \dots, N\}} | \textrm{acc}(b_n) - \textrm{conf}(b_n)|.
$
For a perfectly calibrated model, the ideal ECE and MCE equal to 0. Besides ECE and MCE, we also report Classwise ECE~\cite{https://doi.org/10.48550/arxiv.1904.01685} and Adaptive ECE~\cite{https://doi.org/10.48550/arxiv.1904.01685}, we describe these metrics in Supplementary Material Section 1.


\subsection{Focal Loss}
Focal loss~\cite{lin2017focal} was originally proposed to handle the issue of imbalanced data distribution. For a classification task, the focal loss can be defined as 
\begin{equation}
\mathcal{L}^f_{\gamma} = -(1-p_{i, y_i})^{\gamma}\log p_{i, y_i}
\end{equation}
where $\gamma$ is a hyper-parameter. Jishnu et al.~\cite{mukhoti2020calibrating} found focal loss to be effective at learning better calibrated models as compared to cross-entropy loss. Focal loss can be interpreted as a trade-off between minimizing Kullback–Leibler (KL) divergence and maximizing the entropy, depending on $\gamma$~\cite{mukhoti2020calibrating}\footnote{More theoretical findings can be found in the paper}:
 \begin{equation}
 \label{eq:entropy}
 \mathcal{L}_f \geq \textsc{KL}(q \parallel p) + \underbrace{\mathbb{H}(q)}_{constant} - \gamma \mathbb{H}(p)
 \end{equation}
where $p$ is the predictions distribution and $q$ is the one-hot encoded target class. Models trained with this loss learn to have narrow $p$ (high confidence of predictions) due to KL term but not too narrow (avoiding overconfidence) due to entropy regularization term. Original authors provided a principled approach to select the $\gamma$ for focal loss based on Lambert-W function~\cite{corless1996lambertw}. Motivated by this, our work proposes to learn a more fine-gained sample-wise $\gamma$ with a meta network.

\subsection{Meta-Learning}\label{section:meta_learning}
Model calibration can be formulated as minimizing a multi-component loss objective where the base term is used to optimise predictive performance while a regularizer maintains model calibration ~\cite{bohdal2021meta,lin2017focal}. Tuning the hyper-parameters of the regularizer can be an a difficult process when conventional methods are used~\cite{Li2016, Snoek2012, Falkner2018}. In this work, we adopt the meta learning approach~\cite{Luketina2015}. At each training iteration, model takes a mini-batch from the training dataset $D_{train}$ and the validation dataset $D_{val}$ to optimise
\begin{align}
    \arg\min_{\theta, \phi}&~\mathcal{L}(\theta, \phi, D_{train}, D_{val}) \\
    &= \mathcal{L}_{FL_{\gamma}}(\theta, D_{train}) +  \mathcal{L}_{SECE}(\phi, D_{val})
\end{align}
First, the base loss function $\mathcal{L}_{FL_{\gamma}}$ is used to optimize the parameter of the backbone model $\theta$ on $D_{train}$, note that the $\gamma$ parameter for this step is predicted by \gnet. Following that, the validation mini batch $D_{val}$ is used to optimize the parameters of the meta-network (\gnet) $\phi$ using a validation loss $\mathcal{L}_{SECE}$. The validation loss is a function of the backbone model outputs and does not depend on \gnet directly. The dependence between validation loss and parameters of the meta-network is mediated via the parameters of the backbone model as discussed by \cite{Luketina2015}.



\section{Methods}
This section introduces the two components of our approach: the \gnet learns to parameterise the focal loss and the \sece provides differentiability to \gnet towards calibration optimization.

%%%%% section %%%%%
\subsection{$\gamma$-Net: Learning Sample-Wise Gamma for Focal Loss}
Sample-dependent $\gamma$ value for focal loss showed it effectiveness of calibrating deep neural networks~\cite{mukhoti2020calibrating}. In this work, instead of scheduling $\gamma$ value based on Lambert-W function~\cite{corless1996lambertw}. We propose to learn a more fine-grained and local $\gamma$ value, i.e., each sample has an individual $\gamma$, at a continuous space. 
Formally, \gnet takes the representations from the second last layer of a backbone network (i.e. ResNet~\cite{he2016deep}. Let $\bfx \in \bbR^{b \times d}$ ($b$: batch size, $d$: hidden dimension) be the extracted representation, $\bA \in \bbR^{d \times k }$ be a $k$-head self-attention matrix that followed by a liner layer with parameters $\bfW \in \bbR^{d \times 1 }$. The \gnet transforms the representation to sample-wise $\gamma$:
\begin{align}
 \bfa &= \bfx \cdot \bA,~\in \bbR^{b \times k},~
 \bfp = \textsc{Softmax}(\bfa),~\in \bbR^{b \times k} \\
 \tilde{\bfx} &= \bfp \cdot \bA^\top,~\in \bbR^{b \times d},~~\gamma =\left | \tilde{\bfx} \cdot \bfW \right |/\tau,~ \in \bbR^{b \times 1} 
 \label{equation:gammanet}
\end{align}

\begin{wrapfigure}{r}{0.24\textwidth}
\vspace{-3mm}
\centering
\includegraphics[width=0.22\textwidth]{figure/entropy_1.pdf}
\caption{The entropy of predicted probability$-\mathbb{H}(p)$ computed with discrete and continuous $\gamma$ values based on a synthetic set with 1000 samples on a 10-way classification task ($L=10$).}
\label{fig: entropy} 
\vspace{-3mm}
\end{wrapfigure}
Here we use $ | \cdot  |$ to ensure the $\gamma$ to be positive values and tune the temperature $\tau=0.01$ for the sake of simplicity. This is similar to the temperature setups as in meta-calibration~\cite{bohdal2021meta}. Those operations result in a set of sample-wise $\gamma$ in focal loss $\mathcal{L}_r^f$. Noted that,$f_{\gamma}(\gamma_i| x_i), x_i \in D$ is learned at a continuous space, rather than predefined discrete values as in Focal Loss~\cite{lin2017focal} and scheduled discrete value depending on samples over batches in FLSD~\cite{mukhoti2020calibrating}. As presented in Corollary~\ref{cor:gamma}, the learned $\gamma$ at a continuous space provides smooth calibration. Figure~\ref{fig: entropy} further presents the entropy of probability $-\mathbb{H}(p)$, based on smooth and continuous $\gamma$ illustrating how switching from discreet to continuous gamma makes the regularizer smooth ~\cite{pereyra2017regularizing}.
\begin{corollary}
\label{cor:gamma}
Learning $\gamma$ at a continuous space gives smooth calibration.
\vspace{-3mm}
\end{corollary}
\begin{proof}
With eq.(\ref{eq:entropy})~\cite{mukhoti2020calibrating}, the negative entropy of predictions component of the focal loss $-\gamma \mathbb{H}(p)$ calibrates models by maximising entropy depending on the $\gamma$ hyper-parameter. Given conditions (a) $\gamma$ is continuous and positive and (b) $0 \leq -\mathbb{H}(p) \leq -\hat{p}\log(\hat{p}), \sum_{l}^L p_l = 1$ where $p$ is the predicted class probability, $\hat{p}=\frac{1}{L}$ is a uniform distribution that maximizes entropy and $L$ is the number of classes (maximum entropy principle~\cite{guiasu1985principle}). The $-\gamma \mathbb{H}(p)$ is continuous and positive.
\vspace{-3mm}
\end{proof}
\noindent\textbf{Practical Considerations} To ensure $\gamma \geq 0$, we could apply operations based on min-max scaling or activation functions such as sigmoid, relu and softplus. However, our experimental evidences shows that the formulation presented in Equation~\ref{equation:gammanet} performs better across datasets.

\subsection{\sece: Smooth Expected Calibration Error}
Conventional calibration measurement via ECE discussed in Section~\ref{section:calibration}, is computed over discrete bins by finding the accuracy and the confidence of examples in each interval. However, this approach is highly dependant on different settings of bin-edges and the number of bins, making it a biased estimator of the true value \cite{minderer2021revisiting}.% \cite{bohdal2021meta}.

The issue of bin-based ECE can be traced back to the discrete decision of binning samples~\cite{minderer2021revisiting}. The larger bin numbers, the lesser information retained. Small bins lead to inaccurate measurements of accuracy. For instance, in single example bins, the confidence is well defined but the accuracy in that interval of model confidences is more difficult to assess accurately from a single point. Using the binary accuracy of a single example leads us to the brier score~\cite{brier1950verification} which has been shown to not be a perfect measure of calibration.

To find a good representation of accuracy inside the single-example bin (representing a small confidence interval), we leverage the accuracy of other points in the vicinity of the single chosen example weighted by their distance in the confidence space. Concretely, we can write the soft estimate of a single-example bin accuracy as
\begin{equation}
    \text{\sacc}(b_i) = \sum_j^M \pi(x_{i}) K(z_i, z_j)
\end{equation}
\begin{equation}
K\left(x_i, x_j^{\prime}\right)=\exp \left(-\frac{\left\|x_i-x_j^{\prime}\right\|^2}{2 h^2}\right)
\end{equation}

where $b_i$ is the bin housing example $x_i$ and $K(\cdot, \cdot)$ is a chosen distance measure, for example a Gaussian kernel and $h$ is bandwidth, $z_i$ and $\pi(x_i)$ are the confidence and accuracy of $i^{th}$ example. As in meta-calibration~\cite{bohdal2021meta}, we use the all-pairs approach~\cite{10.1007/s10791-009-9124-x} to ensure differentiability. Having good measures of soft accuracy and confidence for each single-example bin, we can write the updated ECE metric as soft-ECE in the form
\begin{equation}
\text{\sece} =  \frac{1}{M} \sum_{i}^{M}|\textrm{\sacc}(i) - \textrm{conf}(i)|,
\label{equation:sece}
\end{equation}
where $i$ represents a single example and $M$ is the number of examples. The new formulation is (1) differentiable as long as the kernel we use is differentiable and (2) enables smooth control over how accuracy over a confidence bin is computed via tuning the kernel parameters. Regarding (2), choosing a dirac-delta function recovers the original brier score while choosing a broader kernel enables a smooth approximation of accuracy over all confidence values.



\textbf{Connection to KDE-based ECE Estimator}
\cite{zhang2020_mixmatch} present a KDE-based ECE Estimator relying on kernel density estimation to approximate the desired metric. Canonically, ECE is computed as an integral over the confidence space as
\begin{equation}
\hat{\operatorname{ECE}}^d=\int\|z-\hat{\pi}(z)\|_d^d \hat{p}(z) dz
\end{equation}
where $z=\{z_1,z_2,...,z_L\}$ denotes model confidence distribution over $L$ classes, $\|\cdot\|_d^d $ denotes the  $d^{th}$ power of the $\ell_d$  norm, and $\hat{p}(z)$ represents the marginal density function of model's confidence on a given dataset. 
The $\hat{p}(z)$ and $\hat{\pi}(z)$ are approximated using kernel density estimation. We argue that \sece is a special instance with $d=1$ of $\hat{\operatorname{ECE}}^d$ and estimate ECE with max probability $z_t, t=\arg\max \{z_1,z_2,...,z_L\}$ for a single instance. And $\operatorname{\sece}$ is an upper bound of $\hat{\operatorname{ECE}}^1$:
\begin{align}
\hat{\operatorname{ECE}} &=\int\left|z-\hat{\pi}(z)\right| \hat{p}(z) dz \\
&=\int\left|z_l-\hat{\pi}(z_l)\right| \hat{p}(z_l) dz_l \int\left|z_t-\hat{\pi}(z_t)\right| \hat{p}(z_t) dz_t \\
&\leq \int\left|z_t-\hat{\pi}(z_t)\right| \hat{p}(z_t) dz_t = \operatorname{\sece}
\end{align}
with $l = [1, L],l\neq t$ and density functions:
\begin{align}
&\hat{p}(z_t)=\frac{h^{-L}}{M} \sum_{i=1}^{M} K\left(z_t,z_i\right), \\
&\hat{\pi_t}(z_t)=\frac{\sum_{i=1}^{M} \pi(i) K\left(z_t,z_i\right)}{\sum_{i=1}^{M} K\left(z_t,z_i\right)} ,
\end{align}

where $h$ is the kernel width and $\pi(i)$ represents the binary accuracy of point $i$. The $p(z_t)$ is a mixture of dirac deltas centered on confidence predicted for individual points in the dataset (used to approximate ECE). Replacing binary accuracy with accuracy computed using all-pairs approach~\cite{10.1007/s10791-009-9124-x} result in \sece, which is differentiable and can be computed efficiently for smaller batches of data since the integral is replaced with a sum over all examples in a batch.

Presented changes do not invalidate the analysis of the $\hat{\operatorname{ECE}}$ as an unbiased estimator of ECE as the all-pairs accuracy is an unbiased estimator of accuracy and the details of the $p(z)$ distribution are not used in the derivation beyond setting the bounds. As a result, the analysis described in~\cite{zhang2020_mixmatch} to show their KDE-based approximation to ECE is unbiased can be re-used to show the same property for SECE.

\begin{table*}[!ht]
\vspace{-5mm}
\centering
\footnotesize
\setlength{\tabcolsep}{3.75pt}
\begin{tabular}{lllllll}
\hline
Methods & Error & NLL & ECE & MCE & ACE & Classwise ECE \\ %\hline
\multicolumn{7}{c}{CIFAR 10} \\ \hline
CE&         4.812 $\pm$ 0.122&         0.335 $\pm$ 0.01&         4.056 $\pm$ 0.092&         33.932 $\pm$ 5.433&    4.022 $\pm$ 0.136&       0.848 $\pm$ 0.023\\
CE (TS)&         4.812 $\pm$ 0.122&         0.211 $\pm$ 0.005&         3.083 $\pm$ 0.140&         26.695 $\pm$ 2.959&    3.046 $\pm$ 0.157&       0.656 $\pm$ 0.022\\
Focal&         4.874 $\pm$ 0.100&         0.207 $\pm$ 0.005&         3.193 $\pm$ 0.104&         28.034 $\pm$ 5.702&    3.174 $\pm$ 0.098&       0.690 $\pm$ 0.018\\
FLSD&         4.916 $\pm$ 0.074&         0.211 $\pm$ 0.005&         6.904 $\pm$ 0.462&         \textbf{19.246 $\pm$ 11.071}&    6.805 $\pm$ 0.446&       1.465 $\pm$ 0.088\\
LS (0.05)&         4.744 $\pm$ 0.126&         0.232 $\pm$ 0.003&         2.900 $\pm$ 0.085&         24.860 $\pm$ 8.599&    3.985 $\pm$ 0.154&       0.727 $\pm$ 0.009\\
 Mixup($\alpha$=1.0) & \textbf{4.126 $\pm$ 0.068} & 0.273 $\pm$ 0.033 & 12.863 $\pm$ 3.2 & 20.739 $\pm$ 4.205 & 12.833 $\pm$ 3.161 & 2.678 $\pm$ 0.615 \\
MMCE&         4.808 $\pm$ 0.082&         0.333 $\pm$ 0.012&         4.027 $\pm$ 0.082&         41.647 $\pm$ 10.275&    4.013 $\pm$ 0.091&       0.845 $\pm$ 0.014\\
CE-DECE&         5.194 $\pm$ 0.161&         0.301 $\pm$ 0.038&         4.106 $\pm$ 0.402&         41.346 $\pm$ 13.325&    4.088 $\pm$ 0.395&       0.868 $\pm$ 0.074\\
CE-SECE&         5.222 $\pm$ 0.168&         0.289 $\pm$ 0.027&         4.062 $\pm$ 0.241&         50.81 $\pm$ 21.705&    4.049 $\pm$ 0.251&       0.852 $\pm$ 0.040\\
FL$_{\gamma}$-DECE&         5.434 $\pm$ 0.095&         \textbf{0.193 $\pm$ 0.009}&         2.257 $\pm$ 0.787&         56.633 $\pm$ 23.856&    2.396 $\pm$ 0.669&       \textbf{0.557 $\pm$ 0.165}\\
FL$_{\gamma}$-SECE&         5.428 $\pm$ 0.144&         \textbf{0.193 $\pm$ 0.010}&         \textbf{2.138 $\pm$ 0.819}&         22.725 $\pm$ 5.756&    \textbf{2.357 $\pm$ 0.541}&       \textbf{0.556 $\pm$ 0.165}\\ \hline


\multicolumn{7}{c}{CIFAR 100} \\
CE&         22.570 $\pm$ 0.438&         0.997 $\pm$ 0.014&         8.380 $\pm$ 0.336&         23.250 $\pm$ 2.436&    8.347 $\pm$ 0.344&       0.233 $\pm$ 0.006\\
CE (TS)&         22.570 $\pm$ 0.438&         0.959 $\pm$ 0.008&         5.388 $\pm$ 0.393&         13.454 $\pm$ 2.377&    5.360 $\pm$ 0.315&       0.208 $\pm$ 0.003\\
Focal&         22.498 $\pm$ 0.214&         0.900 $\pm$ 0.007&         5.044 $\pm$ 0.203&         12.454 $\pm$ 0.893&    5.015 $\pm$ 0.207&       0.203 $\pm$ 0.004\\
FLSD&         22.656 $\pm$ 0.113&         0.876 $\pm$ 0.005&         5.956 $\pm$ 0.804&         14.716 $\pm$ 1.387&    5.958 $\pm$ 0.802&       0.241 $\pm$ 0.008\\
LS (0.05)&         21.810 $\pm$ 0.172&         1.070 $\pm$ 0.011&         8.108 $\pm$ 0.346&         20.268 $\pm$ 1.536&    8.106 $\pm$ 0.346&       0.272 $\pm$ 0.006\\
Mixup($\alpha$=1.0) & \textbf{21.210 $\pm$ 0.227} & 0.917 $\pm$ 0.017 & 9.716 $\pm$ 0.754 & 16.01 $\pm$ 1.335 & 9.722 $\pm$ 0.74 & 0.315 $\pm$ 0.011 \\
MMCE&         22.490 $\pm$ 0.143&         1.021 $\pm$ 0.007&         8.713 $\pm$ 0.245&         23.565 $\pm$ 1.141&    8.670 $\pm$ 0.305&       0.238 $\pm$ 0.004\\
CE-DECE&         23.406 $\pm$ 0.323&         1.148 $\pm$ 0.006&         7.309 $\pm$ 0.245&         22.565 $\pm$ 1.446&    7.253 $\pm$ 0.315&       0.241 $\pm$ 0.002\\
CE-SECE&         23.448 $\pm$ 0.302&         1.153 $\pm$ 0.015&         7.668 $\pm$ 0.330&         24.261 $\pm$ 1.614&    7.609 $\pm$ 0.295&       0.244 $\pm$ 0.002\\
FL$_{\gamma}$-DECE&         23.712 $\pm$ 0.204&         0.888 $\pm$ 0.009&         \textbf{1.879 $\pm$ 0.440}&         8.271 $\pm$ 2.651&    \textbf{1.838 $\pm$ 0.371}&       0.195 $\pm$ 0.005\\
FL$_{\gamma}$-SECE&         23.686 $\pm$ 0.377&         \textbf{0.877 $\pm$ 0.004}&         1.940 $\pm$ 0.365&         \textbf{7.480 $\pm$ 1.867}&    \textbf{1.939 $\pm$ 0.379}&       \textbf{0.192 $\pm$ 0.006} \\ \hline

\multicolumn{7}{c}{Tiny-ImageNet} \\
CE& 	40.110 $\pm$ 0.110& 	1.838 $\pm$ 0.171& 	8.059 $\pm$ 1.296& 	15.73 $\pm$ 1.905& 	8.006 $\pm$ 1.282& 	0.154 $\pm$ 0.001\\
Focal& 	39.415 $\pm$ 0.625& 	1.896 $\pm$ 0.009& 	7.600 $\pm$ 0.309& 	13.771 $\pm$ 0.897& 	7.469 $\pm$ 0.301& 	0.152 $\pm$ 0.002\\
FLSD& 	39.705 $\pm$ 0.075& 	1.904 $\pm$ 0.025& 	14.501 $\pm$ 1.078& 	21.528 $\pm$ 2.116& 	14.501 $\pm$ 1.078& 	0.202 $\pm$ 0.006 \\
LS (0.1)& 	\textbf{39.395} $\pm$ 0.305& 	2.185 $\pm$ 0.001& 	16.777 $\pm$ 0.476& 	29.088 $\pm$ 1.835& 	16.901 $\pm$ 0.460& 	0.199 $\pm$ 0.001\\
Mixup($\alpha$=1.0) & 39.890 $\pm$ 0.271& 1.932 $\pm$ 0.054& 12.133 $\pm$ 2.069& 31.440 $\pm$ 0.968& 12.028 $\pm$ 2.079& 0.193 $\pm$ 0.009 \\
MMCE& 	40.310 $\pm$ 0.100& 	1.826 $\pm$ 0.177& 	8.206 $\pm$ 1.219& 	16.802 $\pm$ 2.339& 	8.165 $\pm$ 1.269& 	\textbf{0.149} $\pm$ 0.001\\
CE-DECE& 	41.350 $\pm$ 0.000& 	2.228 $\pm$ 0.033& 	10.694 $\pm$ 0.503& 	20.888 $\pm$ 0.430& 	10.553 $\pm$ 0.553& 	0.160 $\pm$ 0.000\\
CE-SECE& 	41.005 $\pm$ 0.145& 	2.213 $\pm$ 0.058& 	10.928 $\pm$ 1.125& 	21.362 $\pm$ 2.526& 	10.912 $\pm$ 1.069& 	0.157 $\pm$ 0.003\\
FL$_\gamma$-DECE& 	40.625 $\pm$ 0.095& 	\textbf{1.826} $\pm$ 0.007& 	\textbf{5.944} $\pm$ 1.090& 	11.542 $\pm$ 1.990& 	6.077 $\pm$ 1.095& 	0.155 $\pm$ 0.007\\ 
FL$_\gamma$-SECE& 	40.850 $\pm$ 0.140& 	1.829 $\pm$ 0.005& 	5.794 $\pm$ 0.756& 	\textbf{11.477} $\pm$ 1.563& 	\textbf{5.848} $\pm$ 0.751& 	0.156 $\pm$ 0.005\\
\hline

\end{tabular}
\caption{The predictive (test error) and calibration performance of different methods on CIFAR10 (Top), CIFAR100 (Middle) and Tiny-ImageNet (Bottom). The best scores are \textbf{bold}. The mean and standard deviation numbers are reported by averaging 5 runs with random seeds. As an alternative calibration method, our approach in general exhibits better calibration while retains competitive predictive performance as compared to conventional as well as meta-learning baselines.}
\label{tab:per_comp}
\end{table*}

\subsection{Optimising \gnet with \sece}
%The newly introduced $\gamma$-Net and the \sece metric can be applied together to optimize selected ML models for both cross-entropy and calibration error using the meta learning scheme introduced in Section~\ref{section:meta_learning}. 
As the probability calibration via maximising entropy is performed at a continuous space, we argue that \sece is an efficient learning objective for optimising \gnet in providing calibration regularization to base network trained with focal loss (see empirical results in \ref{subsection:pred_cal}). 

Under the umbrella of meta-learning, two sets of parameters are learned simultaneously: $\theta$, the parameters of base network $f^c$, as well as the $\phi$, the parameters of \gnet denoted as $f^{\gamma}$. The former, $f^c$, is used to classify each new example into appropriate classes and the latter, $f^{\gamma}$, is applied to find the optimal value of the focal loss parameter $\gamma$ for each training example. Algorithm 1 in Supplementary Material describes the learning procedures. 



\begin{figure*}[htb]
\vspace{-6mm}
	\centering
        \subfloat[\scriptsize{CE (4.94\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_ce_eval_4.pdf} }}
	\subfloat[\scriptsize{CE (TS) (4.94\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_ce_ts_eval_4.pdf} }}
	\subfloat[\scriptsize{Focal (5.01\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_focal_eval_4.pdf} }}
	\subfloat[\scriptsize{FLSD(5.02\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_focal_schedule_eval_4.pdf} }} 
	\subfloat[\scriptsize{LS-0.05 (4.67\%)} ]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_smoothing_0.05_eval_4.pdf} }} \\
	\subfloat[\scriptsize{MMCE (4.83\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_mmce_eval_4.pdf} }} 
	\subfloat[\scriptsize{CE-DECE(5.19\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_ce_dece_eval_4.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE(5.36\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_focal_dece_eval_4.pdf} }}
	\subfloat[\scriptsize{CE-SECE (5.06\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_ce_sece_eval_4.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$+SECE(5.38\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c10_focal_sece_eval_4.pdf} }}
	\caption{The reliability diagram plots for models on CIFAR10 test set. The ($\cdot$) presents test error. The diagonal dash line presents perfect calibration. The red bar presents the gap between the observed accuracy and the desired accuracy of the perfectly calibrated model (diagonal) - it is positive if the observed accuracy is lower and negative otherwise. The model from the 5$^{th}$ run is used.}
\label{fig:rc_c10_calibration} 
\end{figure*}
\begin{figure*}[htb]
    \vspace{-5mm}
	\centering
	\subfloat[\scriptsize{CE (22.41\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c100_ce_eval_4.pdf} }}
	\subfloat[\scriptsize{CE (TS) (22.41\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c100_ce_ts_eval_4.pdf} }}
	\subfloat[\scriptsize{Focal (22.53\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c100_focal_eval_4.pdf} }}
	\subfloat[\scriptsize{FLSD(22.72\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c100_focal_schedule_eval_4.pdf} }} 
	\subfloat[\scriptsize{LS-0.05 (21.56\%)} ]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c100_smoothing_0.05_eval_4.pdf} }} \\
	\subfloat[\scriptsize{MMCE (22.30\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c100_mmce_eval_4.pdf} }} 
	\subfloat[\scriptsize{CE-DECE(23.05\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c100_ce_dece_eval_4.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE(24.04\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c100_focal_dece_eval_4.pdf} }}
	\subfloat[\scriptsize{CE-SECE(23.58\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c100_ce_sece_eval_4.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-SECE(23.78\%)}]{{\includegraphics[width=0.19\textwidth]{figure/rn18_c100_focal_sece_eval_4.pdf} }}
	\caption{The reliability diagram plots for models on CIFAR100 test set.} 
\label{fig:rc_c100_calibration} 
\end{figure*}

\vspace{-3mm}
\section{Experiments}
We conduct experiments to accomplish following objectives:(1) examine both predictive and calibration performance of proposed method;(2) observe the calibration behaviors of proposed method during training;(3) empirically evaluate learned $\gamma$ values for Focal loss and the robustness of different methods in terms of number of bins. 

\subsection{Implementation Details}
We implemented our methods by adapting and extending the code from Meta-Calibration~\cite{bohdal2021meta} with Pytorch~\cite{paszke2019pytorch}. For all experiments we use their default settings. We run each experiments for 5 repetitions with different random seeds. We conducted our experiments on CIFAR10 and CIFAR100 (in~\cite{bohdal2021meta}) as well as Tiny-ImageNet~\cite{imagenet_tiny}; For meta-learning, we split the training set into 8:1:1 as training/val/meta-val, the original test sets are untouched. The experimental pipeline for all three datasets is identical. The models are trained with SGD (learning rate 0.1, momentum 0.9, weight decay 0.0005) for up to 350 epochs. The learning rate is decreased at 150 and 250 epochs by a factor of 10. The model selection is based on the best validation error.

The \gnet is implemented with a multi-head attention layer with $k$ heads and a fully-connected layer. The $k$ is set to the number of categories. The hidden dim is set to 512, the temperature $\tau$ is fixed at 0.01. For \sece, we used the Gaussian kernel with bandwidth of 0.01 for both dataset. We initialize $\gamma=1.0$. At inference stage, the meta network will not present except our ablation study on learned $\gamma$ values in Section~\ref{sec:ablation}.

\textbf{Baselines}. We compare our method with standard cross-entropy (CE), cross-entropy with post-hoc temperature scaling (TS)~\cite{platt1999probabilistic}, Focal loss with standard gamma value (Focal), $\gamma=1$~\cite{lin2017focal}, Focal Loss with scheduled gamma (FLSD)~\cite{mukhoti2020calibrating}, MMCE (Maximum Mean Calibration Error)~\cite{kumar2018trainable} and Label Smoothing with smooth factor 0.05 (LS-0.05) or (LS-0.1)~\cite{muller2019does} and Mix-Up ($\alpha=1.0$)~\cite{thulasidasan2019mixup}. In meta-learning setting, we include CE-DECE (meta-calibration\cite{bohdal2021meta} for learning unit-wise weight regularization), which is also used as meta-learning baseline, CE-SECE, FL$_\gamma$-DECE, focal loss with learnable sample-wise $\gamma$ and FL$_\gamma$-DECE. Note that, we excluded the line of work using uncertainty estimation methods for calibration~\cite{gawlikowski2021survey}, for instance, deep ensembles~\cite{lakshminarayanan2017simple}, MC-dropout~\cite{gal2016dropout}, Bayesian Methods ~\cite{maddox2019simple} etc., those methods usually require performing sampling across multiple inference runs and computational sensitive. 


\subsection{Results and Discussion}
\subsubsection{Predictive and Calibration Performance}
\label{subsection:pred_cal}
Table~\ref{tab:per_comp} presents the performance comparison across approaches. Temperature Scaling (TS) can effectively reduce the errors on calibration metrics as compared to uncalibrated CE models (baseline). Label smoothing and Mixup achieve the best test errors on datasets but exhibit higher ECE and MCE scores. Focal and FLSD can improve calibration in general but we also observed high MCE score for FLSD. On the other hand, we also found that MMCE exhibits higher calibration errors as compared to baseline, this basically aligns with the findings from Bohdal et al.~\cite{bohdal2021meta}. 

Among compared methods, our method FL$_{\gamma}$-SECE achieves considerably lower errors on calibration metrics. When comparing meta-learning based approaches: CE-DECE (meta-learning baseline), CE-SECE, FL$_{\gamma}$-DECE, FL$_{\gamma}$-SECE. We can see that our method (\gnet+\sece) achieves comparable test error and better scores across calibration metrics. Particularly, it improves ECE by average ~4.198\% on three detests, in addition, by average 14.37\% MCE and ~3.917\% ACE score respectively.


Figure ~\ref{fig:rc_c10_calibration} and Figure ~\ref{fig:rc_c100_calibration} illustrate the reliability diagram plots of models on CIFAR10 and CIFAR100 respectively. On CIFAR 10, LS(0.05) exhibits superior performance on both predictive and calibration aspects across non-meta learning approaches. DECE based approach basically gives high MCE score 65.74\% and 75.65\% for CE-DECE and FL$_{\gamma}$-DECE (though is has low ECE 1.83\%). It is shown that \gnet based methods achieve better ECE as using \sece meta-loss for optimizing \gnet ensures smooth and stable calibrations, and potentially reduces the calibration biases on bins. Similarly on CIFAR100, FL$_{\gamma}$-SECE shows improved ECE and MCE from CE-DECE.

\begin{figure*}[!htb]
\vspace{-5mm}
	\centering
	%\subfloat[CIFAR10 (Error)]{{\includegraphics[width=0.4\textwidth]{figure/rn18_c10_test_err.pdf} }}
	\subfloat{{\includegraphics[width=0.35\textwidth]{figure/rn18_c10_test_ece.pdf} }}  
	%\subfloat[CIFAR100 (Error)]{{\includegraphics[width=0.4\textwidth]{figure/rn18_c100_test_err.pdf} }}
	\subfloat{{\includegraphics[width=0.35\textwidth]{figure/rn18_c100_test_ece.pdf} }}
	\caption{ECE curves on both test dataset of CIFAR10 (left) and CIFAR100 (right).}
\label{fig:test_learning_curve} 
\end{figure*}

\begin{table*}
\centering
\scriptsize
\begin{tabular}{lllllll} 
\hline
Methods & Error & NLL & ECE & MCE & ACE & Classwise ECE \\ %\hline
FL$_\gamma$ & 5.632 $\pm$ 0.118 & 0.197 $\pm$ 0.009 & 2.177 $\pm$ 0.619 & 46.172 $\pm$ 28.24 & \textbf{2.319 $\pm$ 0.407} & \textbf{0.553 $\pm$ 0.12} \\ 
FL$_{\gamma}$-SECE&         \textbf{5.428 $\pm$ 0.144}&         \textbf{0.193 $\pm$ 0.010}&         \textbf{2.138 $\pm$ 0.819}&         \textbf{22.725 $\pm$ 5.756}&    2.357 $\pm$ 0.541&       0.556 $\pm$ 0.165\\ \hline

FL$_\gamma$ & 28.148 $\pm$ 8.127 & 1.051 $\pm$ 0.278 & 3.044 $\pm$ 1.542 & 10.082 $\pm$ 3.441 & 3.016 $\pm$ 1.511 & 0.226 $\pm$ 0.063 \\ 
FL$_{\gamma}$-SECE&         \textbf{23.686 $\pm$ 0.377}&         \textbf{0.877 $\pm$ 0.004}&         \textbf{1.940 $\pm$ 0.365}&         \textbf{7.480 $\pm$ 1.867}&    \textbf{1.939 $\pm$ 0.379}&       \textbf{0.192 $\pm$ 0.006} \\ \hline

\end{tabular} 
\caption{Individual calibration gain from SECE in our method on CIFAR10 (top) and CIFAR100 (bottom).}
\label{tab:ind_sece}
\end{table*}
To observe the learning behavior of models, we plotted the changing curves of test error (included in Supplementary Material Section 3) and test ECE in Figure~\ref{fig:test_learning_curve}. It is noted that though test error curves show similar behaviors, but the calibration behaviors are quite different across methods. The primary value of  \gnet with \sece is in the stable and smooth calibration behavior. After the 150$^{th}$ epoch when the learning rate is decreased by a factor of 10, we can observe that the ECE score starts to increase for all approaches, particularly FLSD (green lines). 
\begin{figure}[htb]
\vspace{-5mm}
\centering
	\subfloat[CIFAR10]{{\includegraphics[width=0.235\textwidth]{figure/rn18_c10_focal_sece_7-gamma.pdf} }}
	\subfloat[CIFAR100]{{\includegraphics[width=0.235\textwidth]{figure/rn18_c100_focal_sece_7-gamma.pdf} }}  
\caption{The mean and standard deviation (std.) of $\gamma$ on test dataset at each epoch. Low std. score indicates samples share similar gamma values, and high std. score indicates more samples have different $\gamma$ values.}
\label{fig:gamma} 
\end{figure}
\begin{figure}[htb]
\centering
\includegraphics[width=0.5\textwidth]{figure/bin_vs_ece_mce_cifar10.pdf}
\caption{The ECE and MCE scores on CIFAR 10 test dataset with different bin numbers in $[10, 20, 50, 100, 200, 500, 1000]$. Our proposed approach shows better robustness on bin sizes. Similar plot for CIFAR100 is in Figure 2 of Supplementary Material.}
\label{fig:n_bins} 
\end{figure}

\subsubsection{Ablation Experiments}
\label{sec:ablation}
We conducted two groups of experiments to inspect the learned $\gamma$ and the ECE/MCE against different binning schemes.

Figure \ref{fig:gamma} presents the change of $\gamma$ values on test dataset over epochs. At the earlier training stage, the samples $\gamma_j$ for $x_j \in D_{test}$ have similar values (with low variance) because they are initialized with $\gamma_i=1.0$ for $x_i \in D_{val}$. The $\gamma$ parameter is observed to have higher standard deviation at the later training stage as \gnet learns optimal value for each example in the dataset rather than relying on global values showcasing the flexibility of the network. It is also noted that $\gamma$ is learned in a continuous space, different to the discrete values the pre-defined in Focal Loss~\cite{lin2017focal} and FLSD~\cite{mukhoti2020calibrating}.

In Figure~\ref{fig:n_bins}, we examine the robustness of those methods with different binning schemes by varying the number of bins, which is one of causes of introducing calibration bias~\cite{minderer2021revisiting}. It shows that \gnet based approaches (FL$_{\gamma}$-DECE and FL$_{\gamma}$-SECE) 
maintain much lower ECE score when throughout all bin numbers from 10 to 1000 showing the trained based network is robustly calibrated. Furthermore FL$_{\gamma}$-SECE is also able to maintain lower MCE as compared to other methods. Supplementary Material Section 3.2 reports reliability diagrams with large bin numbers. 

We further conducted experiments to empirically understand the individual calibration gain from \sece as compared to using learnable $\gamma$ only. Table \ref{tab:ind_sece} shows the effectiveness of \sece in our method FL$_{\gamma}$+SECE. With \sece, we can see the improved both predictive and calibration performance.


\section{Conclusion}
In this work, we presented a meta-learning based approach for learning well calibrated models and shown the benefits of two newly introduced components. Learning a sample-wise $\gamma$ for Focal loss using \gnet yields both strong predictive performance and good calibration. Optimising \gnet with \sece plays an important role by ensuring stable calibration as compared to baselines; it provides better calibration capability without changing the original networks.


\bibliography{uai2023}

\newpage
\onecolumn

\appendix
\section{Supplementary Material}
\subsection{Evaluation Metrics}
\label{sec:more_metric}



Besides ECE and MCE, we evaluate models on Classwise ECE and Adaptive ECE.

\textbf{Classwise ECE}~\cite{https://doi.org/10.48550/arxiv.1904.01685}. Classwise ECE extends the bin-based ECE to measure calibration across all the possible classes. In practice, predictions are binned separately for each class and the calibration error is computed at the level of individual class-bins and then averaged. The metric can be formulated as
\begin{equation}
\textsc{CECE} = \sum_{n=1}^{N} \sum_{c=1}^{K} \frac{|b_{n, c}|}{N K} |\textrm{acc}_c(b_{n, c}) - \textrm{conf}_c(b_{n, c})|
\end{equation}
where $N$ is the total number of samples, K is the number of classes and $b_{n, c}$ represents a single bin for class $c$. In this formulation, $\textrm{acc}_c(b_{n, c})$ represents average binary accuracy for class $c$ over bin $b_{n, c}$ and $\textrm{conf}_c(b_{n, c})$ represents average confidence for class $c$ over bin $b_{n, c}$


\textbf{Adaptive ECE}~\cite{https://doi.org/10.48550/arxiv.1904.01685}. Adaptive ECE further extends the classwise variant of expected calibration error by introducing a new binning strategy which focuses the measurement on the confidence regions with multiple predictions and. Concretely Adaptive ECE (ACE) spaces the bin intervals such that each contains an equal number of predictions. The metric can be formulated as  
\begin{equation}
\textsc{CECE} = \sum_{n=1}^{N} \sum_{c=1}^{K} \frac{1}{N K} |\textrm{acc}_c(b_{n, c}) - \textrm{conf}_c(b_{n, c})|
\end{equation}
where the notation follows one from classwise ECE. The importance weighting for each bin is removed since each bin holds the same number of examples.

\subsection{Algorithm}
\label{sec:alg}
Algorithm~\ref{algorithm:meta_learning} describes the learning procedures, it relies on the training and validation sets $D_{train}$, $D_{val}$, two networks $f^{c}$ and $f^{\gamma}$ with parameters $\theta$ and $\phi$. The optimization proceeds in an iterative process until both sets are converged. Each iteration starts by sampling a mini-batch of training data $(\mathbf{x}_{i}^t, \mathbf{y}_{i}^t) \sim D_{train}$ as well as a mini-batch of validation data $(\mathbf{x}_{i}^v, \mathbf{y}_{i}^v) \sim D_{val}$. The training-mini batch is used to find the optimal value of $\gamma$ as $\gamma = f^{\gamma} (\mathbf{x}_{i}^t)$ and following that, compute the focal loss on the output of the backbone network $f^c$. The gradient of the loss $\mathcal{L}^f_{\gamma} ({f^{c}(\mathbf{x}_{i}^t), \mathbf{y}_{i}^t})$ is then used to update the parameters of the backbone network $\theta$ using the Adam algorithm~\cite{kingma2014adam}.

Once the backbone model $f^{c}$ is updated, it is used to compute the value of the auxiliary loss \sece on the validation mini-batch \sece$({f^{c}(\mathbf{x}_{i}^v), \mathbf{y}_{i}^v})$. The auxiliary \sece loss is a differentiable proxy of the true expected calibration error and can be minimized w.r.t. $\phi$ to find the optimal parameters of \gnet. It is important to note that \sece is computed based on the outputs of $f^{c}(\mathbf{x}_{i}^v)$ and the only dependence on $\gamma$ is an indirect one based on the updates of $\theta$ as discussed in Section 2.3. 
\begin{algorithm}
\DontPrintSemicolon
 
  \KwInput{$f^{c}$ and $f^{\gamma}$ with initialized $\theta$ and $\phi$}
  \KwOutput{Optimized $\theta$ and $\phi$}
  \KwData{Training and validation sets: $D_{train}, D_{val}$}
   \While{$\theta$ not converged}
   {
$(\mathbf{x}_{i}^t, \mathbf{y}_{i}^t) \sim D_{train}; ~~~(\mathbf{x}_{i}^v, \mathbf{y}_{i}^v) \sim D_{val}$; \# Sample a mini-batch from both datasets \\
$\gamma_i = f^{\gamma} (\mathbf{x}_{i}^t)$; \# learning sample-wise $\gamma_i$; \\
$\mathcal{L}^f_{\gamma} = \mathcal{L}^f_{\gamma} ({f^{c}(\mathbf{x}_{i}^t), \mathbf{y}_{i}^t})$: \#  Compute training loss based on $\gamma$\;
$\theta := \theta - \eta \nabla_{\theta} \mathcal{L}^f_{\gamma}$: \#  Update parameters of $f^c$ using training loss gradient\;
$\sece = \sece({f^{c}(\mathbf{x}_{i}^v), \mathbf{y}_{i}^v})$: \#  Compute \sece auxiliary loss using validation batch\;
$\phi := \phi - \eta_{\phi} \nabla_{\phi} \sece$: \#  Update parameters of $f^{\gamma}$ using \sece gradient\;

   }
\caption{Meta optimization with $\gamma$-Net and SECE}
\label{algorithm:meta_learning}
\end{algorithm}

\subsection{Additional Experiments}
\label{sec:cifar100_bin_size}

\begin{figure}[!htb]
	\centering
	\subfloat[CIFAR10 (Error)]{{\includegraphics[width=0.4\textwidth]{figure/rn18_c10_test_err.pdf} }}
	%\subfloat[CIFAR10 (ECE)]{{\includegraphics[width=0.4\textwidth]{figure/rn18_c10_test_ece.pdf} }}  
	\subfloat[CIFAR100 (Error)]{{\includegraphics[width=0.4\textwidth]{figure/rn18_c100_test_err.pdf} }}
	%\subfloat[CIFAR100 (ECE)]{{\includegraphics[width=0.4\textwidth]{figure/rn18_c100_test_ece.pdf} }}
	\caption{Test error on both test datasets.}
\label{fig:test_learning_curve} 
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figure/bin_vs_ece_mce_cifar100.pdf}
\caption{The ECE and MCE scores on CIFAR 100 test dataset with different bin numbers in $[10, 20, 50, 100, 200, 500, 1000]$. Our proposed approach shows better robustness on bin sizes.}
\label{fig:cifar100_n_bins} 
\end{figure}
\begin{figure}[!htb]
	\centering
  	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_dece_eval_4_20.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_sece_eval_4_20.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_dece_eval_4_20.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$+SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_sece_eval_4_20.pdf} }} \\
 	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_dece_eval_4_50.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_sece_eval_4_50.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_dece_eval_4_50.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$+SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_sece_eval_4_50.pdf} }} \\
	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_dece_eval_4_100.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_ce_sece_eval_4_100.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_dece_eval_4_100.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$+SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c10_focal_sece_eval_4_100.pdf} }} \\
	\caption{The reliability diagram plots for models on CIFAR10 with large bin numbers (top to bottom: 20, 50, 100). The diagonal dash line presents perfect calibration, the red bar presents the gap to perfect calibration on each bin. The model from the 5$^{th}$ run is used.}
\label{fig:rc_c10_calibration_more} 
\end{figure}

\begin{figure}[!htb]
	\centering
  	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_dece_eval_4_20.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_sece_eval_4_20.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_dece_eval_4_20.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$+SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_sece_eval_4_20.pdf} }} \\
 	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_dece_eval_4_50.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_sece_eval_4_50.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_dece_eval_4_50.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$+SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_sece_eval_4_50.pdf} }} \\
	\subfloat[\scriptsize{CE-DECE}]
    {{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_dece_eval_4_100.pdf} }}
    \subfloat[\scriptsize{CE-SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_ce_sece_eval_4_100.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$-DECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_dece_eval_4_100.pdf} }}
	\subfloat[\scriptsize{FL$_{\gamma}$+SECE}]{{\includegraphics[width=0.2\textwidth]{figure/rn18_c100_focal_sece_eval_4_100.pdf} }} \\
	\caption{The reliability diagram plots for models on CIFAR100 with large bin numbers (top to bottom: 20, 50, 100). The diagonal dash line presents perfect calibration, the red bar presents the gap to perfect calibration on each bin. The model from the 5$^{th}$ run is used.}
\label{fig:rc_c100_calibration_more} 
\end{figure}

\subsubsection{Test Error Curves}
\label{sec:test error}
Figure~\ref{fig:test_learning_curve} presents the test error on CIFAR10 and CIFAR100 for compared methods. In general, they exhibit similar behaviour. 

\subsubsection{Reliability Plots}
\label{sec:more_reliability}
In Figure~\ref{fig:rc_c10_calibration_more} and \ref{fig:rc_c100_calibration_more}, we provide the comparison across meta-learning based approach via reliability plots with large bin numbers and corresponding ECE/MCE. FL$_{\gamma}$+SECE is able to maintain lowest ECE and MCE, it shows the robustness to the increased bin numbers.

Figure~\ref{fig:cifar100_n_bins} examines the robustness of those methods with different binning schemes by varying the number of bins. It shows that \gnet based approaches (FL$_{\gamma}$-DECE and FL$_{\gamma}$-SECE) maintain much lower ECE score when throughout all bin numbers from 10 to 1000 showing the trained based network is robustly calibrated.

\subsubsection{Experiments with DenseNet}
We conducted experiments on DenseNet with meta-learning baselines. Table~\ref{tab:densenet_baselines} presents the effectiveness of gamma-net based methods, which exhibit better calibration across multiple metrics with comparable predictive performance.
\begin{table}
\small
\begin{tabular}{lllllll} \hline
Methods & Error & NLL & ECE & MCE & ACE & CLASSWISE\_ECE \\
 CE & 5.354 $\pm$ 0.097 & 0.22 $\pm$ 0.015 & 3.038 $\pm$ 0.159 & 33.419 $\pm$ 19.597 & 3.035 $\pm$ 0.157 & 0.677 $\pm$ 0.031 \\
 CE-DECE & 5.85 $\pm$ 0.421 & 0.236 $\pm$ 0.017 & 3.245 $\pm$ 0.471 & 25.062 $\pm$ 2.184 & 3.24 $\pm$ 0.471 & 0.723 $\pm$ 0.072 \\
 CE-SECE & 5.895 $\pm$ 0.271 & 0.246 $\pm$ 0.032 & 3.555 $\pm$ 0.428 & 24.518 $\pm$ 3.613 & 3.555 $\pm$ 0.425 & 0.756 $\pm$ 0.083 \\
 FL$_\gamma$-DECE & 6.084 $\pm$ 0.188 & 0.199 $\pm$ 0.012 & 2.151 $\pm$ 1.499 & 24.644 $\pm$ 11.227 & 2.139 $\pm$ 1.457 & 0.605 $\pm$ 0.201 \\
 FL$_\gamma$-SECE & 6.263 $\pm$ 0.266 & 0.208 $\pm$ 0.021 & 2.549 $\pm$ 2.233 & 20.22 $\pm$ 5.235 & 2.559 $\pm$ 2.217 & 0.672 $\pm$ 0.355 \\\hline
\end{tabular} 
\caption{DenseNet (100 layers) on CIFAR10 with meta-learning based methods, 5 repetitions. gamma-net based methods exhibit better calibration across multiple metrics (NLL, ECE, ACE, CLASSWISE\_ECE) while maintain competitive test error.}
\label{tab:densenet_baselines}
\end{table}

% References

\end{document}
