\section{Experimental Results}

\begin{table*}[tb!]
    \centering
    \caption{Comparison on pattern diversity and legality. All results are copied from \cite{wen2022layoutransformer} and \cite{zhang2020layout}. `Real Patterns' refer to the whole dataset (Training set + Test set). `-' refer to Not Applicable.}
    \label{tab:diversity}
    %{{{
    %\resizebox{.78\linewidth}{!}
    {
        \begin{tabular}{c|c|cc|cc}
            \toprule
            \multirow{2}*{Set/Method}&\multirow{2}*{Generated Topology}&\multicolumn{2}{c|}{Generated Patterns}&\multicolumn{2}{c}{Legal Patterns}\\
            &&Patterns & Diversity ($\uparrow$)  &Legality ($\uparrow$) & Diversity ($\uparrow$)\\ \midrule
            \rowcolor{green!10} Real Patterns&-&-&-&13869&10.777\\
            CAE\cite{yang2019deepattern}&100000&100000&4.5875&19&3.7871 \\
            VCAE\cite{zhang2020layout}&100000&100000&\textbf{10.9311}&2126&9.9775 \\
            CAE+LegalGAN\cite{zhang2020layout}&100000&100000&5.8465&3740&5.8142 \\
            VCAE+LegalGAN\cite{zhang2020layout}&100000&100000&9.8692&84510&9.8669 \\
            LayouTransformer\cite{wen2022layoutransformer}&-&100000&10.532&89726&10.527 \\
            \tool{DiffPattern}-S&100000&100000&10.815&\textbf{100000}&\textbf{10.815} \\
            \tool{DiffPattern}-L&100000&10000000&10.815&\textbf{10000000}&\textbf{10.815}\\
            % DiffPattern-L&&&&&\\
            \bottomrule 
        \end{tabular}
    }
    %}}}
\end{table*}

\subsection{Experimental Setup}

\minisection{Datasets.} We follow previous work \cite{zhang2020layout, wen2022layoutransformer} to obtain the dataset of small layout pattern
images with the size of $2048\times2048$ $nm^2$ by splitting a $400\times160$ $\mu m^2$ layout map from ICCAD contest 2014. The size of extracted topology tensor is fixed as 16$\times$32$\times$32 with $C=16$ in Deep Squish Pattern Representation. 3000 images are randomly choosed as test set, while others are used for training. 

\begin{figure}[tb!]
    \centering
    \subfloat[]{ \includegraphics[width=0.28\linewidth]{diff_patterna.pdf} }
    \subfloat[]{ \includegraphics[width=0.28\linewidth]{diff_patternb.pdf} } 
    \subfloat[]{ \includegraphics[width=0.28\linewidth]{diff_patternc.pdf} } \\
    \subfloat[]{ \includegraphics[width=0.28\linewidth]{diff_patternd.pdf} }
    \subfloat[]{ \includegraphics[width=0.28\linewidth]{diff_patterne.pdf} }
    \subfloat[]{ \includegraphics[width=0.28\linewidth]{diff_patternf.pdf} }
    \caption{Different layout patterns that are generated from a single topology with the same design rule. }
    \label{fig:single}
\end{figure}



\minisection{Diffusion Model Configuration.} Following the setting in previous works \cite{ho2020denoising,austin2021structured}, we use a U-Net \cite{ronneberger2015u} as our backbone in the discrete diffusion model to predict the posterior distribution during the reverse diffusion process. There are four feature map resolutions in the model, $[32\times32, 16\times16, 8\times8, 4\times4]$. Each resolution level has two convolutional residual blocks, where the numbers of convolution channels on the four resolutions are $\left[128,256,256,256\right]$, respectively. A self-attention block is also placed between two convolutional blocks at the 16$\times$16 resolution level. Moreover, the time step $k$ is included in each residual block through the sinusoidal position embedding \cite{vaswani2017attention}.

\minisection{Training Details.} We train the diffusion model for $0.5$M iterations with a batch size of $128$. The learning rate is $2\times 10^{-4}$. And we use the Adam optimizer. Other hyperparameters are chosen as following: the dropout rate is set to $0.1$, the grad clip is set to $1$, and the loss coefficient $\lambda$ is set to $0.001$. We set diffusion timesteps $K= 1000$ to ensure that the forward diffusion process converges to the uniform stationary distribution within $K$ steps. The noise schedule is: $\beta_k$ is linearly increased from $0.01$ to $0.5$. The training procedure takes about $17$ hours in total on 8 NVIDIA RTX 3090 GPUs.

\subsection{Pattern Diversity and Legality}
\label{sec:4.2}

The diversity is calculated by \Cref{eq:diveristy}, and the legality is checked by a tool {\it Klayout} based on the design rules described in \Cref{sec:2.3}. 

To have a fair comparison with previous methods, we randomly synthesize 100000 typologies. We denote a version of \tool{DiffPattern} as \tool{DiffPattern}-S where we only assign one pair of geometric vectors to each generated topology in the phase of legal pattern assessment.
However, \tool{DiffPattern} is able to generate a large amount of legal patterns for each topology.
To show the strong ability of our methodologies, we have another version denoted as \tool{DiffPattern}-L,
where we figure out one hundred different legal geometric vectors for each generated topology.
Considering the size of topology and the large search space, the number of legal solutions for each topology is huge. We `only' randomly take one hundred from them due to the time limitation.

We compare our \tool{DiffPattern} with several learning-based layout pattern generation methods.
CAE \cite{yang2019deepattern} denotes a vanilla convolutional auto-encoder model.
VCAE \cite{zhang2020layout} utilizes a variational convolutional auto-encoder model.
Both of them are pixel-based methods.
LegalGAN \cite{zhang2020layout} is a learning-based post-processing method that legalizes a newly generated topology by modifying it.
LayouTransformer \cite{wen2022layoutransformer} is a sequential-based method.
LayouTransformer applies a transformer-based model to synthesize new sequential representations of layout patterns without topology generation.
As shown in \Cref{tab:diversity}, thanks to the topology pre-filter and rule-based 2D legal pattern assessment,
both \tool{DiffPattern}-S and \tool{DiffPattern}-L achieve perfect performance ({\it i.e.}~100\%) under the metric of legality in the standard settings.
With the well-designed topology generation method, \tool{DiffPatten} also gets reasonable improvement (10.527$\rightarrow$10.815) on the diversity of generated pattern compared with the previous best method, LayouTransformer.
Furthermore, different from the previous methods, the legalization of \tool{DiffPattern} mainly relies on the rule-based legal pattern assessment and topology pre-filter.
When design rules change, it is easy for us to produce another batch of diverse patterns that satisfy the new design rules,
without re-training the topology generation model.
We detail the flexibility of our method in the next subsection.


\begin{figure}[tb!]
    \centering
    \subfloat[]{ \includegraphics[width=0.28\linewidth]{diff_rulea.pdf} }
    \subfloat[]{ \includegraphics[width=0.28\linewidth]{diff_ruleb.pdf} }
    \subfloat[]{ \includegraphics[width=0.28\linewidth]{diff_rulec.pdf} }
    \caption{
        Layout patterns that are generated from the same topology with different design rules: 
        (a) Normal rule; (b) Larger $\textit{space}_\textit{min}$; (c) Smaller $\textit{Area}_\textit{max}$.
    }
    \label{fig:rules}
\end{figure}


% \begin{table*}[tb!]
%     \centering
%     \caption{Comparison on pattern diversity, legality and validity. All results are copied from \cite{wen2022layoutransformer} and \cite{zhang2020layout}. `Real Patterns' refer to the whole dataset (Training set + Test set). `-' refer to Not Applicable or No Data.}
%     \label{tab:diversity}
%     \resizebox{.98\linewidth}{!}
%     {
%         \begin{tabular}{c|c|cc|cc|ccc}
%             \toprule
%             \multirow{2}*{Set/Method}&\multirow{2}*{Generated Topology}&\multicolumn{2}{c|}{Generated Patterns}&\multicolumn{2}{c}{Legal Patterns}&\multicolumn{3}{c}{Patten Validity($\uparrow$)}\\
%             &&Patterns & Diversity($\uparrow$)  &Legality($\uparrow$) & Diversity($\uparrow$)&$\tau$=0.6  &$\tau$=0.7 & $\tau$=0.8 \\ \midrule
%             \rowcolor{green!10} Real Patterns&-&-&-&13869&10.777&11654&13012&13565\\
%             CAE\cite{yang2019deepattern}&100000&100000&4.5875&19&3.7871&-&-&- \\
%             VCAE\cite{zhang2020layout}&100000&100000&\textbf{10.9311}&2126&9.9775&-&-&- \\
%             CAE+LegalGAN\cite{zhang2020layout}&100000&100000&5.8465&3740&5.8142&1&10&62 \\
%             VCAE+LegalGAN\cite{zhang2020layout}&100000&100000&9.8692&84510&9.8669&45889&66256&76541 \\
%             LayouTransformer\cite{wen2022layoutransformer}&-&100000&10.532&89726&10.527&75513&84683&88267 \\
%             DiffPattern-S&100000&100000&\textbf{10.815}&100000&\textbf{10.815}&60155&80897&92335 \\
%             DiffPattern-L&100000&&\textbf{10.815}&&\textbf{10.815}&&& \\
            
%             \bottomrule 
%         \end{tabular}
%     }
%     \label{tab:diversity}
% \end{table*}



% \subsection{Pattern Validity}
% \label{subsection:validity}




% We have mentioned in \Cref{sec:2.4} that the choice of the initial value for the nonlinear programming heavily affects the pattern validity. As a baseline, we also conduct another version of DiffPattern, {\it DiffPattern-R}, where we randomly choose an initial value from the normal distribution. As shown in \Cref{tab:validity}, DiffPattern-R also gets a perfect performance on legality but owns a worse score on validity compared with DiffPattern-E. This is intuitive because, in some cases, an existing geometric vector pair is already satisfied design rules for the newly generated topology. So patterns in DiffPattern-E share more similar features with patterns in the training set. DiffPattern-E receives comparable performance with the Test Set and outperforms all methods except LayouTransformer.  


\subsection{Flexibility}
\label{sec:4.3}

A main advantage of \tool{DiffPattern} is that the white-box 2D legal pattern assessment phase is quite flexible. We show two applications of this property here. 

\minisection{Generate Different Patterns from Single Topology.} Given the design rules and the topology, the non-linear system in \Cref{eq:nonlinear} usually has many legal solutions, {\it i.e.,} the geometric vector pairs. Every legal solution leads to a legal layout pattern, and these layout patterns share a common topology, which are useful in some downstream tasks. We show the examples where several layout patterns are generated from a single topology with different geometric vectors in \Cref{fig:single}.


\minisection{Generate Legal Patterns with Different Design Rules.}
The legalization and topology generation are decoupled in \tool{DiffPattern},
which allows us to generate legal layout patterns under different design rules without re-training the model. We show the examples where several layout patterns are generated from the same topology but with different design rules in \Cref{fig:rules}. 


 
\subsection{Distribution of Complexity}

Diversity is a critical metric of the quality of generated pattern library.
As defined in \Cref{sec:2.3}, diversity is the Shannon entropy of the distribution of pattern complexity,
{\it i.e.,} the number of scan lines subtracted by a pattern along the x-axis and y-axis.
We visualize the distribution of complexity in \Cref{fig:complexity}.
The patterns generated by \tool{DiffPattern} share a similar complexity distribution with real patterns.
This visualization further demonstrates our ability to generate high-quality layout patterns.


\begin{figure}[tb!]
    \centering
    \includegraphics[width=\linewidth]{complexity_002_.pdf}
    \caption{An illustration of complexity distribution. }
    \label{fig:complexity}
\end{figure}

\subsection{Model Efficiency}

Model efficiency is an essential statistic for layout generation methods.
Since the topology generation and layout pattern assessment are decomposed in our model,
we record the average time to sample a new topology and figure out a legal solution for \Cref{eq:nonlinear} separately with our implementation.
The results are listed in \Cref{tab:efficiency}.
We have mentioned in \Cref{sec:2.4} that we randomly select a pair of existing geometric vectors to initialize the non-linear system for acceleration. We denote the version as {\it Solving-E}. We denote the original version with random initialization as {\it Solving-R}. Comparing with Solving-R, the proposed Solving-E version can achieve an average of $2.30\times$ acceleration.


\subsection{Discussion on Validity}

There is a metric named pattern validity proposed by previous work \cite{zhang2020layout}. The evaluation of validity is based on an encoder-decoder model that is pre-trained on the training set. The basic idea is if the generated patterns share more similar features with patterns in the training set, these new patterns will get a better score from the validity metric. However, we argue that the meaning of the metric of validity is quite limited. One main motivation of the layout pattern generation task is to synthesize a large amount of diverse but legal layout patterns for downstream tasks, {\it e.g.,} hotpot detection or lithography simulation. In these situations, legal patterns that are dissimilar from existing patterns are preferred but they get worse scores from validity. What makes the metric worse is that the measurement of validity encourages the overfitting of the training set. For example, according to \cite{zhang2020layout,wen2022layoutransformer}, the generated patterns even receive a much higher score (65\%$\rightarrow$84\%) than the patterns from the test set, which theoretically follows the same distribution as the training set.
It is unreasonable to believe the method with the better score in validity is superior in quality.
Therefore, we have not evaluated \tool{DiffPattern} with this metric. 

% The metric of pattern validity is proposed in \cite{zhang2020layout}, which aims to measure how realistic the generated patterns are. Furthermore, .  and are treated as the better ones. The validity counts the number of DRC-clean patterns whose scores are lower than the threshold $\tau$. 


\begin{table}[tb!]
    \centering
    \caption{Model efficiency of \tool{DiffPattern}. We record the average processing time for each sample (topology or layout pattern) in our machine. One Nvidia RTX 3090 GPU is used for topology sampling and one Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz is used to figure out the non-linear system in this table. }
    \label{tab:efficiency}
    % \resizebox{.85\linewidth}{!}
    % {
        \begin{tabular}{c|cc}
            \toprule
            Phase/Method & Cost Time (s)& Acceleration  \\
            \midrule
            Sampling &0.544 &N/A\\
            \midrule
            Solving-R & 0.269&1.00$\times$\\
            Solving-E & 0.117&2.30$\times$\\
            
            \bottomrule 
        \end{tabular}
    % }
\end{table}

% 

 



% \begin{table}[tb!]
%     \centering
%     \caption{Comparison on pattern validity. All results are copied from \cite{wen2022layoutransformer} and \cite{zhang2020layout}}
%     \label{tab:diversity}
%     % \resizebox{.68\linewidth}{!}
%     {
%         \begin{tabular}{c|c|ccc}
%             \toprule
%             \multirow{2}*{Set/Method}&\multirow{2}*{Legal Patterns}&\multicolumn{3}{c}{Patten Validity($\uparrow$)}\\
%             && $\tau$=0.6  &$\tau$=0.7 & $\tau$=0.8 \\ \midrule
%             \rowcolor{green!10} Test Set&3000&0.6493&0.8485&0.9306\\
%             CAE+LegalGAN\cite{zhang2020layout}&3740&0.0003&0.0027&0.0167 \\
%             VCAE+LegalGAN\cite{zhang2020layout}&84510&0.5430&0.7840&0.9057 \\
%             LayouTransformer\cite{wen2022layoutransformer}&89726&\textbf{0.8416}&\textbf{0.9438}&\textbf{0.9834} \\
%             DiffPattern-R&100000&0.2863&0.6022&0.8436 \\
%             DiffPattern-E&100000&0.6016&0.8090&0.9234 \\
%             \bottomrule 
%         \end{tabular}
%     }
% \label{tab:validity}
% \end{table}
