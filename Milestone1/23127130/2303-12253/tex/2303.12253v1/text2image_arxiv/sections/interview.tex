\section{Study Design}
To understand current practices, motivations, and goals when using modern text-to-image models, we sent a survey to internal users of two internal TTI models to collect basic information about their use of these models (e.g., time spent using the models, motivations, desired capabilities, and prompting strategies). We also interviewed and observed 11 \rr{power} users of TTI models (8 identifying as Male and 3 identifying as Female) in a 50-minute study to uncover their motivations and practices. The latter participants were prolific users of one or more TTI models. Models used by the participants in the study are anonymized for review, but are of the same basic capability as state of the art text to image generation models such as Imagen \cite{ImagenTe79:online}, Parti \cite{PartiPat10:online}, and DALL-E 2 \cite{DALLE279:online} which are described in more detail in our related work.

%There are different types of diffusion models, the first one to show superior performance above GANs is GLIDE (CLIP classifier guided diffusion), Imagen combines T5-XXL (transformer) with diffusion to achieve even better performance. PARTI is an autoregressive model, and it treats text-to-image generation as a sequence-to-sequence problem, which is very similar to machine translation or other language modeling tasks. Parti uses a traditional two-stage encoder-decoder model similar to DALL-E 2 and Imagen.

% Their motivations for using TTI models range from recreational hobby and wanting to experience new technology, to generating reference images of their art projects and doing art projects with the models.

% Survey data snapshot can be found here: https://docs.google.com/spreadsheets/d/1dhjyPK8LaOQRUEYYpnIbSEwhZUBM5K3T39ncvUYM-lw/edit#gid=0
\subsection{Participants}
For the survey, participants were recruited from an internal chat channel dedicated to TTI models (where the internal chat channel has thousands of members) and internal TTI model mailing lists (with hundreds of members). From the survey respondents, we identified interview candidates who had reported having created artwork over more than ten sessions and having spent more than five hours in the previous week using a TTI model. To create a pool of participants, we recruited eight of these latter respondents, and further recruited three prominent artists in the internal artist community to participate. These three artists are quite visible in the internal artist community, and have shared their unique artwork collections within that community. \rr{Participants were also actively engaged in external communities, sharing knowledge, expertise, and artwork.} Participants were given a 60 USD gift card for participation.


\subsection{Interview structure}
The study consisted of four parts intended to understand participants' practices. Each participant was first asked to create an image of their choice to allow the researcher to observe their natural practices.
%, where they get their inspirations from, and the art they are currently most invested in or most comfortable creating.
In the second part of the study, participants were asked to reflect on 1) an artwork they were proud of, 2) a piece they found most successful, and 3) the piece that was least successful. 
%This part of the interview was intended  to understand the criteria and constraints they impose on themselves for the artwork they generate, as well as their practices for stellar outcomes as well as failures.
In the third part of the interview, participants were asked to reflect on someone else's work by examining only the prompt, and specifically asked to either improve or change the prompt in their style. 
%This phase of the interview was intended to better understand the prompting strategies they had developed and the tacit knowledge they had built up about the model behavior. %(their new tool for creation).
In the last part of the interview, participants were asked to discuss envisioned uses for these text-to-image models.

Interviews were conducted remotely. We recorded the shared screen and automatically generated transcripts for the interviews. We constrained our focus to interfaces that only use text prompts as input to the models. In addition, some participants voluntarily shared their collection of generated artwork after the interview.


\subsection{Qualitative Data Analysis}

For the qualitative analyses, the authors analyzed the video transcriptions and also noted comments on participants non-verbal interactions. The final corpus of automatically generated transcripts was 164 pages (60614 words). The first two authors each reviewed the transcripts data independently, looking for ways of explaining the artistic practices~\cite{miles1984drawing}. In this process, the authors separately analyzed each transcript to extracted salient themes, and independently generated hypotheses and points of discussions~\cite{braun2006using}. Using these data, all authors participated in two rounds of interpretation sessions to arrive at the primary themes reported in this paper, \rr{and resolve any discrepancies and disagreements. During the interpretation sessions, authors also analyzed the prompts and the images created by the study participants to identify unique artistic styles and practices. These sessions were inspired by existing analysis practices from qualitative media analysis \cite{altheide2012qualitative}}. 