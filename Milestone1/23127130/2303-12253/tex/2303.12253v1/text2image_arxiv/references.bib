@book{adams1995hitchhiker,
  title={The Hitchhiker's Guide to the Galaxy},
  author={Adams, D.},
  isbn={9781417642595},
  url={http://books.google.com/books?id=W-xMPgAACAAJ},
  year={1995},
  publisher={San Val}
}

@book{altheide2012qualitative,
  title={Qualitative media analysis},
  author={Altheide, David L and Schneider, Christopher J},
  volume={38},
  year={2012},
  publisher={Sage publications}
}

@article{gan2014Goodfellow,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title = {Generative Adversarial Networks},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3422622},
doi = {10.1145/3422622},
abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
journal = {Commun. ACM},
month = {Oct},
pages = {139–144},
numpages = {6}
}

@article{aiart2022Cetinic,
author = {Cetinic, Eva and She, James},
title = {Understanding and Creating Art with AI: Review and Outlook},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3475799},
doi = {10.1145/3475799},
abstract = {Technologies related to artificial intelligence (AI) have a strong impact on the changes of research and creative practices in visual arts. The growing number of research initiatives and creative applications that emerge in the intersection of AI and art motivates us to examine and discuss the creative and explorative potentials of AI technologies in the context of art. This article provides an integrated review of two facets of AI and art: (1) AI is used for art analysis and employed on digitized artwork collections, or (2) AI is used for creative purposes and generating novel artworks. In the context of AI-related research for art understanding, we present a comprehensive overview of artwork datasets and recent works that address a variety of tasks such as classification, object detection, similarity retrieval, multimodal representations, and computational aesthetics, among others. In relation to the role of AI in creating art, we address various practical and theoretical aspects of AI Art and consolidate related works that deal with those topics in detail. Finally, we provide a concise outlook on the future progression and potential impact of AI technologies on our understanding and creation of art.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {Feb},
articleno = {66},
numpages = {22},
keywords = {deep learning, computational creativity, AI Art, generative art, image understanding, visual arts, generative adversarial networks, convolutional neural networks}
}

@misc{aican2017Elgammal,
  doi = {10.48550/ARXIV.1706.07068},
  url = {https://arxiv.org/abs/1706.07068},
  author = {Elgammal, Ahmed and Liu, Bingchen and Elhoseiny, Mohamed and Mazzone, Marian},
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {CAN: Creative Adversarial Networks, Generating "Art" by Learning About Styles and Deviating from Style Norms},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{AICAN73:online,
author = {},
title = {AICAN},
howpublished = {\url{https://www.aican.io/}},
month = {},
year = {},
note = {(Accessed on 08/31/2022)}
}

@misc{mordvintsev2015DeepDreams,
title	= {Inceptionism: Going Deeper into Neural Networks},
author	= {Alexander Mordvintsev and Christopher Olah and Mike Tyka},
month = {June},
year	= {2015},
URL	= {https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html}
}
@article{miles1984drawing,
  title={Drawing valid meaning from qualitative data: Toward a shared craft},
  author={Miles, Matthew B and Huberman, A Michael},
  journal={Educational researcher},
  volume={13},
  number={5},
  pages={20--30},
  year={1984},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}
@article{braun2006using,
  title={Using thematic analysis in psychology},
  author={Braun, Virginia and Clarke, Victoria},
  journal={Qualitative research in psychology},
  volume={3},
  number={2},
  pages={77--101},
  year={2006},
  publisher={Taylor \& Francis}
}

@InProceedings{Zhu_2017_ICCV,
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
title = {Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@InProceedings{Karras_2019_CVPR,
author = {Karras, Tero and Laine, Samuli and Aila, Timo},
title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{gan2018Brock,
  author    = {Andrew Brock and
               Jeff Donahue and
               Karen Simonyan},
  title     = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
  journal   = {CoRR},
  volume    = {abs/1809.11096},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.11096},
  eprinttype = {arXiv},
  eprint    = {1809.11096},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-11096.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gatys2015NeuralStyle,
  doi = {10.48550/ARXIV.1508.06576},
  url = {https://arxiv.org/abs/1508.06576},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), Neurons and Cognition (q-bio.NC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
  title = {A Neural Algorithm of Artistic Style},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{jing2020NeuralStyle,  
    author = {Jing, Yongcheng and Yang, Yezhou and Feng, Zunlei and Ye, Jingwen and Yu, Yizhou and Song, Mingli},
    journal={IEEE Transactions on Visualization and Computer Graphics},
    title={Neural Style Transfer: A Review},
    year={2020},
    volume={26},
    number={11},
    pages={3365-3385},
    doi={10.1109/TVCG.2019.2921336}
}

@misc{mansimov2015ImgFromCaption,
  doi = {10.48550/ARXIV.1511.02793},
  url = {https://arxiv.org/abs/1511.02793},
  author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Generating Images from Captions with Attention},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{pmlr-v139-ramesh21a,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}

@misc{ramesh2022DALLE2,
  doi = {10.48550/ARXIV.2204.06125},
  url = {https://arxiv.org/abs/2204.06125},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}


@Article{aguera2017ArtInAI,
AUTHOR = {Agüera y Arcas, Blaise},
TITLE = {Art in the Age of Machine Intelligence},
JOURNAL = {Arts},
VOLUME = {6},
YEAR = {2017},
NUMBER = {4},
ARTICLE-NUMBER = {18},
URL = {https://www.mdpi.com/2076-0752/6/4/18},
ISSN = {2076-0752},
ABSTRACT = {In this wide‐ranging essay, the leader of Google’s Seattle AI group and founder of the Artists and Machine Intelligence program discusses the long‐standing and complex relationship between art and technology. The transformation of artistic practice and theory that attended the 19th century photographic revolution is explored as a parallel for the current revolution in machine intelligence, which promises not only to mechanize (or democratize) the means of reproduction, but also of production.},
DOI = {10.3390/arts6040018}
}


@Article{mazzone2019ArtCreativityAI,
AUTHOR = {Mazzone, Marian and Elgammal, Ahmed},
TITLE = {Art, Creativity, and the Potential of Artificial Intelligence},
JOURNAL = {Arts},
VOLUME = {8},
YEAR = {2019},
NUMBER = {1},
ARTICLE-NUMBER = {26},
URL = {https://www.mdpi.com/2076-0752/8/1/26},
ISSN = {2076-0752},
ABSTRACT = {Our essay discusses an AI process developed for making art (AICAN), and the issues AI creativity raises for understanding art and artists in the 21st century. Backed by our training in computer science (Elgammal) and art history (Mazzone), we argue for the consideration of AICAN&rsquo;s works as art, relate AICAN works to the contemporary art context, and urge a reconsideration of how we might define human and machine creativity. Our work in developing AI processes for art making, style analysis, and detecting large-scale style patterns in art history has led us to carefully consider the history and dynamics of human art-making and to examine how those patterns can be modeled and taught to the machine. We advocate for a connection between machine creativity and art broadly defined as parallel to but not in conflict with human artists and their emotional and social intentions of art making. Rather, we urge a partnership between human and machine creativity when called for, seeing in this collaboration a means to maximize both partners&rsquo; creative strengths.},
DOI = {10.3390/arts8010026}
}


@Article{hertzmann2018ComputersArt,
AUTHOR = {Hertzmann, Aaron},
TITLE = {Can Computers Create Art?},
JOURNAL = {Arts},
VOLUME = {7},
YEAR = {2018},
NUMBER = {2},
ARTICLE-NUMBER = {18},
URL = {https://www.mdpi.com/2076-0752/7/2/18},
ISSN = {2076-0752},
ABSTRACT = {This essay discusses whether computers, using Artificial Intelligence (AI), could create art. First, the history of technologies that automated aspects of art is surveyed, including photography and animation. In each case, there were initial fears and denial of the technology, followed by a blossoming of new creative and professional opportunities for artists. The current hype and reality of Artificial Intelligence (AI) tools for art making is then discussed, together with predictions about how AI tools will be used. It is then speculated about whether it could ever happen that AI systems could be credited with authorship of artwork. It is theorized that art is something created by social agents, and so computers cannot be credited with authorship of art in our current understanding. A few ways that this could change are also hypothesized.},
DOI = {10.3390/arts7020018}
}


@Article{grba2022DeepElse,
AUTHOR = {Grba, Dejan},
TITLE = {Deep Else: A Critical Framework for AI Art},
JOURNAL = {Digital},
VOLUME = {2},
YEAR = {2022},
NUMBER = {1},
PAGES = {1--32},
URL = {https://www.mdpi.com/2673-6470/2/1/1},
ISSN = {2673-6470},
ABSTRACT = {From a small community of pioneering artists who experimented with artificial intelligence (AI) in the 1970s, AI art has expanded, gained visibility, and attained socio-cultural relevance since the second half of the 2010s. Its topics, methodologies, presentational formats, and implications are closely related to a range of disciplines engaged in the research and application of AI. In this paper, I present a comprehensive framework for the critical exploration of AI art. It comprises the context of AI art, its prominent poetic features, major issues, and possible directions. I address the poetic, expressive, and ethical layers of AI art practices within the context of contemporary art, AI research, and related disciplines. I focus on the works that exemplify poetic complexity and manifest the epistemic or political ambiguities indicative of a broader milieu of contemporary culture, AI science/technology, economy, and society. By comparing, acknowledging, and contextualizing both their accomplishments and shortcomings, I outline the prospective strategies to advance the field. The aim of this framework is to expand the existing critical discourse of AI art with new perspectives which can be used to examine the creative attributes of emerging practices and to assess their cultural significance and socio-political impact. It contributes to rethinking and redefining the art/science/technology critique in the age when the arts, together with science and technology, are becoming increasingly responsible for changing ecologies, shaping cultural values, and political normalization.},
DOI = {10.3390/digital2010001}
}

@inproceedings{grba2021brittle,
  title={Brittle Opacity: Ambiguities of the Creative AI},
  author={Grba, Dejan},
  booktitle={Proceedings of the xCoAx, 9th Conference on Computation, Communication, Aesthetics \& X Proceedings, xCoAx, Graz, Austria},
  pages={12--16},
  year={2021}
}

@article{brown2022AIArtist,
    author = {Browne, Kieran},
    title = "{Who (or What) Is an AI Artist?}",
    journal = {Leonardo},
    volume = {55},
    number = {2},
    pages = {130-134},
    year = {2022},
    month = {04},
    abstract = "{The mainstream contemporary art world is suddenly showing interest in AI art. While this has enlivened the practice, there remains significant disagreement over who or what actually deserves to be called an AI artist. This article examines several claimants to the term and grounds these in art history and theory. It addresses the controversial elevation of some artists over others and accounts for these choices, arguing that the art market alienates AI artists from their work. Finally, it proposes that AI art’s interactions with art institutions have not promoted new creative possibilities but have instead reinforced conservative forms and aesthetics.}",
    issn = {0024-094X},
    doi = {10.1162/leon_a_02092},
    url = {https://doi.org/10.1162/leon\_a\_02092},
    eprint = {https://direct.mit.edu/leon/article-pdf/55/2/130/2004755/leon\_a\_02092.pdf},
}

@inproceedings{frih2019CSTinHCI,
author = {Frich, Jonas and MacDonald Vermeulen, Lindsay and Remy, Christian and Biskjaer, Michael Mose and Dalsgaard, Peter},
title = {Mapping the Landscape of Creativity Support Tools in HCI},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300619},
doi = {10.1145/3290605.3300619},
abstract = {Creativity Support Tools (CSTs) play a fundamental role in the study of creativity in Human-Computer Interaction (HCI). Even so, there is no consensus definition of the term 'CST' in HCI, and in most studies, CSTs have been construed as one-off exploratory prototypes, typically built by the researchers themselves. This makes it difficult to clearly demarcate CST research, but also to compare findings across studies, which impedes advancement in digital creativity as a growing field of research. Based on a literature review of 143 papers from the ACM Digital Library (1999-2018), we contribute a first overview of the key characteristics of CSTs developed by the HCI community. Moreover, we propose a tentative definition of a CST to help strengthen knowledge sharing across CST studies. We end by discussing our study's implications for future HCI research on CSTs and digital creativity.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–18},
numpages = {18},
keywords = {literature review, creativity, creativity support tools (csts), meta-analysis},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{twiggsmith2021ToolsTricks,
author = {Twigg-Smith, Hannah and O'Leary, Jasper Tran and Peek, Nadya},
title = {Tools, Tricks, and Hacks: Exploring Novel Digital Fabrication Workflows on #PlotterTwitter},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445653},
doi = {10.1145/3411764.3445653},
abstract = {As digital fabrication machines become widespread, online communities have provided space for diverse practitioners to share their work, troubleshoot, and socialize. These communities pioneer increasingly novel fabrication workflows, and it is critical that we understand and conceptualize these workflows beyond traditional manufacturing models. To this end, we conduct a qualitative study of #PlotterTwitter, an online community developing custom hardware and software tools to create artwork with computer-controlled drawing machines known as plotters. We documented and analyzed emergent themes where the traditional interpretation of digital fabrication workflows fails to capture important nuances and nascent directions. We find that #PlotterTwitter makers champion creative exploration of interwoven digital and physical materials over a predictable series of steps. We discuss how this challenges long-running views of digital fabrication and propose design implications for future frameworks and toolkits to account for this breadth of practice.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {594},
numpages = {15},
keywords = {maker culture, fabrication},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{chung2021Intersection,
author = {Chung, John Joon Young and He, Shiqing and Adar, Eytan},
title = {The Intersection of Users, Roles, Interactions, and Technologies in Creativity Support Tools},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462050},
doi = {10.1145/3461778.3462050},
abstract = {Creativity Support Tools (CSTs) have become an integral part of artistic creation. The range of CST technologies is broad—from fabricators to generative algorithms to robots. The interaction approaches for CSTs are accordingly broad. CSTs combine specific technologies and interaction types to serve a spectrum of roles and users. In this work, we tackle a comprehensive understanding of how the intersections of users, roles, interactions, and technologies form a design space for CSTs. We accomplish this by reviewing 111 art-creation CSTs from HCI and computing research and analyzing how diverse aspects of CSTs relate to each other. Our findings identify patterns for designing CSTs, which can give guidance to future CST designers. We also highlight under-explored types of CSTs within the HCI community, providing future directions that CST researchers can pursue given the current trajectory of technological advancement. This work contributes an integrating perspective to understand the landscape of art-creation CSTs.},
booktitle = {Designing Interactive Systems Conference 2021},
pages = {1817–1833},
numpages = {17},
keywords = {literature review, art-making, creativity support tools},
location = {Virtual Event, USA},
series = {DIS '21}
}

@article{chung2021gestural,
  title={Gestural Inputs as Control Interaction for Generative Human-AI Co-Creation},
  author={Chung, John Joon Young and Chang, Minsuk and Adar, Eytan},
  publisher = {Association for Computing Machinery},
  journal = {Workshops at the International Conference on Intelligent User Interfaces (IUI)},
  year={2021}
}

@article{grabe2022towards,
  title={Towards a Framework for Human-AI Interaction Patterns in Co-Creative GAN Applications},
  author={Grabe, Imke and Gonz{\'a}lez-Duque, Miguel and Risi, Sebastian and Zhu, Jichen},
  year={2022},
  publisher = {Association for Computing Machinery},
  journal = {Workshops at the International Conference on Intelligent User Interfaces (IUI)}
}

@article{gmeiner2022team,
  title={Team Learning as a Lens for Designing Human-AI Co-Creative Systems},
  author={Gmeiner, Frederic and Holstein, Kenneth and Martelaro, Nikolas},
  year={2022},
  publisher = {Association for Computing Machinery},
  journal = {Workshops at the International Conference on Intelligent User Interfaces (IUI)}
}

@article{buschek2021nine,
  title={Nine Potential Pitfalls when Designing Human-AI Co-Creative Systems},
  author={Buschek, Daniel and Mecke, Lukas and Lehmann, Florian and Dang, Hai},
  publisher = {Association for Computing Machinery},
  journal = {Workshops at the International Conference on Intelligent User Interfaces (IUI)},
  year={2021}
}

@misc{spotoLibrary,
author = {Spoto, Angie and Oleynik, Natalia},
title = {Library of Mixed-Initiative Creative Interfaces},
howpublished = {\url{http://mici.codingconduct.cc/aboutmicis/}},
month = {},
year = {},
note = {(Accessed on 08/31/2022)}
}

@inproceedings{muller2020mixed,
  title={Mixed Initiative Generative AI Interfaces: An Analytic Framework for Generative AI Applications},
  author={Muller, Michael and Weisz, Justin D and Geyer, Werner},
  booktitle={Proceedings of the Workshop The Future of Co-Creative Systems-A Workshop on Human-Computer Co-Creativity of the 11th International Conference on Computational Creativity (ICCC 2020)},
  year={2020}
}

@InProceedings{Rombach_2022_CVPR,
    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},
    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10684-10695}
}

@misc{StableDi88:online,
author = {},
title = {Stable Diffusion launch announcement — Stability.Ai},
howpublished = {\url{https://stability.ai/blog/stable-diffusion-announcement}},
month = {},
year = {},
note = {(Accessed on 08/31/2022)}
}

@misc{DALLECr11:online,
author = {OpenAI},
title = {DALL·E: Creating Images from Text},
howpublished = {\url{https://openai.com/blog/dall-e/}},
month = {},
year = {},
note = {(Accessed on 08/31/2022)}
}

@misc{DALLE279:online,
author = {OpenAI},
title = {DALL·E 2},
howpublished = {\url{https://openai.com/dall-e-2/}},
month = {},
year = {},
note = {(Accessed on 08/31/2022)}
}

@misc{PartiPat10:online,
author = {},
title = {Parti: Pathways Autoregressive Text-to-Image Model},
howpublished = {\url{https://parti.research.google/}},
month = {},
year = {},
note = {(Accessed on 08/31/2022)}
}

@article{nichol2021glide,
  title={Glide: Towards photorealistic image generation and editing with text-guided diffusion models},
  author={Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  journal={arXiv preprint arXiv:2112.10741},
  year={2021}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@misc{yu2022Parti,
  doi = {10.48550/ARXIV.2206.10789},
  url = {https://arxiv.org/abs/2206.10789},
  author = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ImagenTe79:online,
author = {},
title = {Imagen: Text-to-Image Diffusion Models},
howpublished = {\url{https://imagen.research.google/}},
month = {},
year = {},
note = {(Accessed on 08/31/2022)}
}

@misc{saharia2022Imagen,
  doi = {10.48550/ARXIV.2205.11487},
  url = {https://arxiv.org/abs/2205.11487},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{Sbai_2018_ECCV_Workshops,
author = {Sbai, Othman and Elhoseiny, Mohamed and Bordes, Antoine and LeCun, Yann and Couprie, Camille},
title = {DesIGN: Design Inspiration from Generative Networks},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
month = {September},
year = {2018}
}

@inproceedings{jeon2021Fashion,
author = {Jeon, Youngseung and Jin, Seungwan and Shih, Patrick C. and Han, Kyungsik},
title = {FashionQ: An AI-Driven Creativity Support Tool for Facilitating Ideation in Fashion Design},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445093},
doi = {10.1145/3411764.3445093},
abstract = {Recent research on creativity support tools (CST) adopts artificial intelligence (AI) that leverages big data and computational capabilities to facilitate creative work. Our work aims to articulate the role of AI in supporting creativity with a case study of an AI-based CST tool in fashion design based on theoretical groundings. We developed AI models by externalizing three cognitive operations (extending, constraining, and blending) that are associated with divergent and convergent thinking. We present FashionQ, an AI-based CST that has three interactive visualization tools (StyleQ, TrendQ, and MergeQ). Through interviews and a user study with 20 fashion design professionals (10 participants for the interviews and 10 for the user study), we demonstrate the effectiveness of FashionQ on facilitating divergent and convergent thinking and identify opportunities and challenges of incorporating AI in the ideation process. Our findings highlight the role and use of AI in each cognitive operation based on professionals’ expertise and suggest future implications of AI-based CST development.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {576},
numpages = {18},
keywords = {fashion design, creativity support tool, cognitive operation, artificial intelligence utilization, ideation process},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{hwang2022Creative,
author = {Hwang, Angel Hsing-Chi},
title = {Too Late to Be Creative? AI-Empowered Tools in Creative Processes},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503549},
doi = {10.1145/3491101.3503549},
abstract = {The present case study examines the product landscape of current AI-empowered co-creative tools. Specifically, I review literature in both creativity and HCI research and investigate how these tools support different stages in humans’ creative processes and how common challenges in human-AI interaction (HAII) are addressed. I find these AI-driven tools mostly support the generation and execution of ideas and are less involved in the early stages of co-creation. Moreover, HAII challenges identified in other fields receive little attention in the creative domain. Based on a synthetic analysis, I elaborate on how future tools can leverage the ”non-human” quality of AI to achieve innovation through a more human-centered, collaborative journey.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {38},
numpages = {9},
keywords = {creativity support tool, human-AI interaction, creativity},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{mccormack2019Music,
author = {McCormack, Jon and Gifford, Toby and Hutchings, Patrick and Llano Rodriguez, Maria Teresa and Yee-King, Matthew and d'Inverno, Mark},
title = {In a Silent Way: Communication Between AI and Improvising Musicians Beyond Sound},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300268},
doi = {10.1145/3290605.3300268},
abstract = {Collaboration is built on trust, and establishing trust with a creative Artificial Intelligence is difficult when the decision process or internal state driving its behaviour isn't exposed. When human musicians improvise together, a number of extra-musical cues are used to augment musical communication and expose mental or emotional states which affect musical decisions and the effectiveness of the collaboration. We developed a collaborative improvising AI drummer that communicates its confidence through an emoticon-based visualisation. The AI was trained on musical performance data, as well as real-time skin conductance, of musicians improvising with professional drummers, exposing both musical and extra-musical cues to inform its generative process. Uni- and bi-directional extra-musical communication with real and false values were tested by experienced improvising musicians. Each condition was evaluated using the FSS-2 questionnaire, as a proxy for musical engagement. The results show a positive correlation between extra-musical communication of machine internal state and human musical engagement.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {extra-musical communication, ai systems, improvisation},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@misc{Midjourn12:online,
author = {},
title = {Midjourney},
howpublished = {\url{https://www.midjourney.com/home/}},
month = {},
year = {},
note = {(Accessed on 09/01/2022)}
}

@inproceedings{davis2016Drawing,
author = {Davis, Nicholas and Hsiao, Chih-PIn and Yashraj Singh, Kunwar and Li, Lisa and Magerko, Brian},
title = {Empirically Studying Participatory Sense-Making in Abstract Drawing with a Co-Creative Cognitive Agent},
year = {2016},
isbn = {9781450341370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856767.2856795},
doi = {10.1145/2856767.2856795},
abstract = {This paper reports on the design and evaluation of a co-creative drawing partner called the Drawing Apprentice, which was designed to improvise and collaborate on abstract sketches with users in real time. The system qualifies as a new genre of creative technologies termed "casual creators" that are meant to creatively engage users and provide enjoyable creative experiences rather than necessarily helping users make a higher quality creative product. We introduce the conceptual framework of participatory sense-making and describe how it can help model and understand open-ended collaboration. We report the results of a user study comparing human-human collaboration to human-computer collaboration using the Drawing Apprentice system. Based on insights from the user study, we present a set of design recommendations for co-creative agents.},
booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
pages = {196–207},
numpages = {12},
keywords = {collaboration, creativity support tools, computational creativity},
location = {Sonoma, California, USA},
series = {IUI '16}
}

@inproceedings{hodhod2016Storytelling,
author = {Hodhod, Rania and Magerko, Brian},
title = {Closing the Cognitive Gap between Humans and Interactive Narrative Agents Using Shared Mental Models},
year = {2016},
isbn = {9781450341370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856767.2856774},
doi = {10.1145/2856767.2856774},
abstract = {This paper proposes a new formal approach for negotiating shared mental models between humans and computational improvisational agents (improv agents) based on our sociocognitive studies of human improvisers. Negotiation of shared mental models serves as a core mechanism for improv agents to co-create stories with each other and with human interactors. The model aims to narrow the gap between human and machine intelligence by providing AI agents that, in the presence of incomplete knowledge about an improv scene, can use procedural representations not only to understand human parties but also to negotiate their mental models with them. The described approach allows flexible modeling of ambiguous, non-Boolean knowledge through the use of fuzzy logic and situation calculus that allows reasoning under uncertainty in a dynamic improvisational setting.},
booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
pages = {135–146},
numpages = {12},
keywords = {cognitive consensus, improvisational agents, computational creativity, shared mental models, fuzzy logic},
location = {Sonoma, California, USA},
series = {IUI '16}
}

@inproceedings{perone2019Chatbot,
author = {Perrone, Allison and Edwards, Justin},
title = {Chatbots as Unwitting Actors},
year = {2019},
isbn = {9781450371872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342775.3342799},
doi = {10.1145/3342775.3342799},
abstract = {Chatbots are popular for both task-oriented conversations and unstructured conversations with web users. Several different approaches to creating comedy and art exist across the field of computational creativity. Despite the popularity and ease of use of chatbots, there have not been any attempts by artists or comedians to use these systems for comedy performances. We present two initial attempts to do so from our comedy podcast and call for future work toward both designing chatbots for performance and for performing alongside chatbots.},
booktitle = {Proceedings of the 1st International Conference on Conversational User Interfaces},
articleno = {2},
numpages = {2},
keywords = {comedy, computational creativity, chatbots, performing arts, performance},
location = {Dublin, Ireland},
series = {CUI '19}
}

@inproceedings{karimi2019Drawing,
author = {Karimi, Pegah and Davis, Nicholas and Maher, Mary Lou and Grace, Kazjon and Lee, Lina},
title = {Relating Cognitive Models of Design Creativity to the Similarity of Sketches Generated by an AI Partner},
year = {2019},
isbn = {9781450359177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325480.3325488},
doi = {10.1145/3325480.3325488},
abstract = {This paper presents and evaluates a new method for inspiring creativity in a co-creative design system. The method uses a computational model of aconceptual shift based on clustering of deep features from a database of sketches. The co-creative sketching tool maps a user's sketch to a sketch of a distinct category that has high, medium, or low visual and semantic similarity. We hypothesize that the degree of similarity between the user's and the system's sketches is associated with a range of cognitive models of creativity in a design context. We report on the findings of an empirical study that analyzes different design scenarios in which the user sketches in response to a proposed conceptual shift. The findings show that how similar the computational agent's sketch is to the user's original sketch is related to the presence of three types of design creativity in the user's response: combinatorial, exploratory, and transformational.},
booktitle = {Proceedings of the 2019 on Creativity and Cognition},
pages = {259–270},
numpages = {12},
keywords = {sketching, design creativity, collaboration, co-creativity},
location = {San Diego, CA, USA},
series = {C\&C '19}
}

@inproceedings{louie2020Music,
author = {Louie, Ryan and Coenen, Andy and Huang, Cheng Zhi and Terry, Michael and Cai, Carrie J.},
title = {Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376739},
doi = {10.1145/3313831.3376739},
abstract = {While generative deep neural networks (DNNs) have demonstrated their capacity for creating novel musical compositions, less attention has been paid to the challenges and potential of co-creating with these musical AIs, especially for novices. In a needfinding study with a widely used, interactive musical AI, we found that the AI can overwhelm users with the amount of musical content it generates, and frustrate them with its non-deterministic output. To better match co-creation needs, we developed AI-steering tools, consisting of Voice Lanes that restrict content generation to particular voices; Example-Based Sliders to control the similarity of generated content to an existing example; Semantic Sliders to nudge music generation in high-level directions (happy/sad, conventional/surprising); and Multiple Alternatives of generated content to audition and choose from. In a summative study (N=21), we discovered the tools not only increased users' trust, control, comprehension, and sense of collaboration with the AI, but also contributed to a greater sense of self-efficacy and ownership of the composition relative to the AI.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {co-creation, human-ai interaction, generative deep neural networks},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{oh2018Drawing,
author = {Oh, Changhoon and Song, Jungwoo and Choi, Jinhan and Kim, Seonghyeon and Lee, Sungwoo and Suh, Bongwon},
title = {I Lead, You Help but Only with Enough Details: Understanding User Experience of Co-Creation with Artificial Intelligence},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174223},
doi = {10.1145/3173574.3174223},
abstract = {Recent advances in artificial intelligence (AI) have increased the opportunities for users to interact with the technology. Now, users can even collaborate with AI in creative activities such as art. To understand the user experience in this new user--AI collaboration, we designed a prototype, DuetDraw, an AI interface that allows users and the AI agent to draw pictures collaboratively. We conducted a user study employing both quantitative and qualitative methods. Thirty participants performed a series of drawing tasks with the think-aloud method, followed by post-hoc surveys and interviews. Our findings are as follows: (1) Users were significantly more content with DuetDraw when the tool gave detailed instructions. (2) While users always wanted to lead the task, they also wanted the AI to explain its intentions but only when the users wanted it to do so. (3) Although users rated the AI relatively low in predictability, controllability, and comprehensibility, they enjoyed their interactions with it during the task. Based on these findings, we discuss implications for user interfaces where users can collaborate with AI in creative works.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {human computer collaboration, artificial intelligence, human-ai interaction},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{pini2019Recipe,
author = {Pini, Azzurra and Hayes, Jer and Upton, Connor and Corcoran, Medb},
title = {AI Inspired Recipes: Designing Computationally Creative Food Combos},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3312948},
doi = {10.1145/3290607.3312948},
abstract = {If chocolate and broccoli sound a strange pairing, can you imagine a broccoli chocolate bar that combines them? As a matter of fact, the two ingredients share the highest number of flavour molecules, so their combination might not be as weird as it sounds. We applied computational creativity, that is AI systems to enhance human creativity, to the food domain, with the main goal of feeding the mind of the creative professional in the food business with new unexpected combinations.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {computational creativity, knowledge graph, food innovation, information visualisation, user interface design},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@article{quanz2020machine,
  title={Machine learning based co-creative design framework},
  author={Quanz, Brian and Sun, Wei and Deshpande, Ajay and Shah, Dhruv and Park, Jae-eun},
  journal={arXiv preprint arXiv:2001.08791},
  year={2020}
}

@inproceedings{davis2015Drawing,
author = {Davis, Nicholas and Hsiao, Chih-PIn and Singh, Kunwar Yashraj and Li, Lisa and Moningi, Sanat and Magerko, Brian},
title = {Drawing Apprentice: An Enactive Co-Creative Agent for Artistic Collaboration},
year = {2015},
isbn = {9781450335980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757226.2764555},
doi = {10.1145/2757226.2764555},
abstract = {This paper describes a co-creative web-based drawing application called the Drawing Apprentice. This system collaborates with users in real time abstract drawing. We describe the theory, interaction design, and user experience of the Drawing Apprentice system. We evaluate the system with formative user studies and expert evaluations from a juried art competition in which a Drawing Apprentice submission won the code-based art category.},
booktitle = {Proceedings of the 2015 ACM SIGCHI Conference on Creativity and Cognition},
pages = {185–186},
numpages = {2},
keywords = {cognitive science, creativity support tools, collaboration, art, computational creativity},
location = {Glasgow, United Kingdom},
series = {C\&C '15}
}

@inproceedings{zhao2020IconGen,
author = {Zhao, Nanxuan and Kim, Nam Wook and Herman, Laura Mariah and Pfister, Hanspeter and Lau, Rynson W.H. and Echevarria, Jose and Bylinskii, Zoya},
title = {ICONATE: Automatic Compound Icon Generation and Ideation},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376618},
doi = {10.1145/3313831.3376618},
abstract = {Compound icons are prevalent on signs, webpages, and infographics, effectively conveying complex and abstract concepts, such as "no smoking" and "health insurance", with simple graphical representations. However, designing such icons requires experience and creativity, in order to efficiently navigate the semantics, space, and style features of icons. In this paper, we aim to automate the process of generating icons given compound concepts, to facilitate rapid compound icon creation and ideation. Informed by ethnographic interviews with professional icon designers, we have developed ICONATE, a novel system that automatically generates compound icons based on textual queries and allows users to explore and customize the generated icons. At the core of ICONATE is a computational pipeline that automatically finds commonly used icons for sub-concepts and arranges them according to inferred conventions. To enable the pipeline, we collected a new dataset, Compicon1k, consisting of 1000 compound icons annotated with semantic labels (i.e., concepts). Through user studies, we have demonstrated that our tool is able to automate or accelerate the compound icon design process for both novices and professionals.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {icon design, pictogram, ideogram, design tools, graphic design, compound icon},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{shi2020Storyboarding,
author = {Shi, Yang and Cao, Nan and Ma, Xiaojuan and Chen, Siji and Liu, Pei},
title = {EmoG: Supporting the Sketching of Emotional Expressions for Storyboarding},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376520},
doi = {10.1145/3313831.3376520},
abstract = {Storyboarding is an important ideation technique that uses sequential art to depict important scenarios of user experience. Existing data-driven support for storyboarding focuses on constructing user stories, but fail to address its benefit as a graphic narrative device. Instead, we propose to develop a data-driven design support tool that increases the expressiveness of user stories by facilitating sketching storyboards. To explore this, we focus on supporting the sketching of emotional expressions of characters in storyboards. In this paper, we present EmoG, an interactive system that generates sketches of characters with emotional expressions based on input strokes from the user. We evaluated EmoG with 21 participants in a controlled user study. The results showed that our tool has significantly better performance in usefulness, ease of use, and quality of results than the baseline system.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {data-driven design, storyboarding, creativity support tools, emotional expression generation},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{yan2022Comics,
author = {Yan, Chuan and Chung, John Joon Young and Kiheon, Yoon and Gingold, Yotam and Adar, Eytan and Hong, Sungsoo Ray},
title = {FlatMagic: Improving Flat Colorization through AI-Driven Design for Digital Comic Professionals},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502075},
doi = {10.1145/3491102.3502075},
abstract = {Creating digital comics involves multiple stages, some creative and some menial. For example, coloring a comic requires a labor-intensive stage known as ‘flatting,’ or masking segments of continuous color, as well as creative shading, lighting, and stylization stages. The use of AI can automate the colorization process, but early efforts have revealed limitations—technical and UX—to full automation. Via a formative study of professionals, we identify flatting as a bottleneck and key target of opportunity for human-guided AI-driven automation. Based on this insight, we built FlatMagic, an interactive, AI-driven flat colorization support tool for Photoshop. Our user studies found that using FlatMagic significantly reduced professionals’ real and perceived effort versus their current practice. While participants effectively used FlatMagic, we also identified potential constraints in interactions with AI and partially automated workflows. We reflect on implications for comic-focused tools and the benefits and pitfalls of intermediate representations and partial automation in designing human-AI collaboration tools for professionals.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {380},
numpages = {17},
keywords = {digital comic colorization, Intermediate Representation, automation and control, Human-AI collaboration, system for professionals},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{hertzmann2020ComputersArt,
author = {Hertzmann, Aaron},
title = {Computers Do Not Make Art, People Do},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3347092},
doi = {10.1145/3347092},
abstract = {The continually evolving relationship between artistic technologies and artists.},
journal = {Commun. ACM},
month = {Apr},
pages = {45–48},
numpages = {4}
}

@article{huang2020AISong,
  doi = {10.48550/ARXIV.2010.05388},
  url = {https://arxiv.org/abs/2010.05388},
  author = {Huang, Cheng-Zhi Anna and Koops, Hendrik Vincent and Newton-Rex, Ed and Dinculescu, Monica and Cai, Carrie J.},
  keywords = {Sound (cs.SD), Human-Computer Interaction (cs.HC), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, J.5; I.2},
  title = {AI Song Contest: Human-AI Co-Creation in Songwriting},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{10.1145/3326338,
author = {Ch'ng, Eugene},
title = {Art by Computing Machinery: Is Machine Art Acceptable in the Artworld?},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3326338},
doi = {10.1145/3326338},
abstract = {When does a machine-created work becomes art? What is art? Can machine artworks fit in to the historical and present discourse? Do machine artworks demonstrate creativity, or are they a type of new media from which artists extend their creativity with? Will solely machine-created artworks be acceptable by our artworlds? This article probes these questions by first identifying the frameworks for defining and explaining art and evaluating its suitability for explaining machine artworks. It then explores how artworks have a necessary relationship with their human artists and the wider context of history, institutions, styles, and approaches and with audiences and artworlds. The article then questions whether machines have such a relational context and whether machines will ever live up to our standard of what constitutes an artwork as defined by us or whether machines are good only for assisting creativity. The question of intellectual property, rights, and ownership are also discussed for human--machine artworks and purely machine-produced works of art. The article critically assesses the viability of machines as artists as the central question in the historical discourse, extended through art and the artworld and evaluates machine-produced work from such a basis.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {Jul},
articleno = {59},
numpages = {17},
keywords = {Machine art, machine artist, artworlds, machine artworks, art theory}
}

@inproceedings{devlin2018Bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@article{yannakakis2014mixed,
  title={Mixed-initiative co-creativity},
  author={Yannakakis, Georgios N and Liapis, Antonios and Alexopoulos, Constantine},
  year={2014},
  publisher={Foundations of Digital Games}
}

@misc{Thefirst77:online,
author = {Christie's},
title = {The first piece of AI-generated art to come to auction | Christie's},
howpublished = {\url{https://www.christies.com/features/a-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx}},
month = {Dec},
year = {2018},
note = {(Accessed on 09/06/2022)}
}

@article{grba2019forensics,
  title={Forensics of a molten crystal: challenges of archiving and representing contemporary generative art},
  author={Grba, Dejan},
  journal={ISSUE Annual Art Journal: Erase},
  volume={8},
  number={3-15},
  pages={5},
  year={2019}
}

@article{beyer1999contextual,
  title={Contextual design},
  author={Beyer, Hugh and Holtzblatt, Karen},
  journal={interactions},
  volume={6},
  number={1},
  pages={32--42},
  year={1999},
  publisher={ACM New York, NY, USA}
}

@inproceedings{NEURIPS2021_49ad23d1,
 author = {Dhariwal, Prafulla and Nichol, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8780--8794},
 publisher = {Curran Associates, Inc.},
 title = {Diffusion Models Beat GANs on Image Synthesis},
 url = {https://proceedings.neurips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{ho2022cascaded,
  title={Cascaded Diffusion Models for High Fidelity Image Generation.},
  author={Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
  journal={J. Mach. Learn. Res.},
  volume={23},
  pages={47--1},
  year={2022}
}

@misc{GoogleDeepDream81:online,
author = {McFarland, Matt},
title = {Google’s psychedelic ‘paint brush’ raises the oldest question in art - The Washington Post},
howpublished = {\url{https://www.washingtonpost.com/news/innovations/wp/2016/03/10/googles-psychedelic-paint-brush-raises-the-oldest-question-in-art/}},
month = {Mar},
day = {10},
year = {2016},
note = {(Accessed on 09/14/2022)}
}