\section{Related Work}

Advances in deep learning have led to the development of generative machine learning models capable of producing both images that are highly realistic and images that are highly creative in both existing and novel artistic styles \cite{aiart2022Cetinic}. For example, Generative Adversarial Networks (GANs) \cite{gan2014Goodfellow} learn to generate images and simultaneously distinguish real and fake images to generate highly realistic images. Many variations of the GAN architecture have been investigated (e.g., \cite{Zhu_2017_ICCV, Karras_2019_CVPR, gan2018Brock}). A particularly relevant variation of this architecture is the Creative Adversarial Network (CAN) from Elgammal et al. \cite{aican2017Elgammal}, designed to generate images with novel artistic styles. This artwork was subsequently featured in multiple exhibitions \cite{AICAN73:online} where human observers could not distinguish the CAN-generated art from human-authored artwork. Outside of GANs,  Gatys et al. \cite{gatys2015NeuralStyle} introduced a method to apply learned artistic styles to random images (a technique now known as Neural Style Transfer \cite{jing2020NeuralStyle}). Additionally, work originally designed to make convolutional neural networks more explainable, now referred to as Deep Dreams, became popular for generating art \cite{GoogleDeepDream81:online} due to its ability to generate psychedelic versions of images \cite{mordvintsev2015DeepDreams}. While these techniques allowed for generating images with creative and novel artistic styles \cite{aican2017Elgammal}, none provided significant affordances to end-users for controlling what was generated outside of the training data scope.

Mansimov et al. \cite{mansimov2015ImgFromCaption} addressed this issue by showing that a generative model could produce novel images from natural text when conditioned on image captions. As text-to-image models rely on language modeling techniques, recent advances in the scaling of large language models \cite{devlin2018Bert, raffel2020exploring} have enabled the development of correspondingly large text-to-image models with impressive results. The most recent of these models include: DALL·E \cite{pmlr-v139-ramesh21a, DALLECr11:online} and DALL·E 2 \cite{ramesh2022DALLE2, DALLE279:online}, Stable Diffusion \cite{Rombach_2022_CVPR, StableDi88:online}, Midjourney\cite{Midjourn12:online}, Parti \cite{yu2022Parti, PartiPat10:online}, and Imagen \cite{saharia2022Imagen, ImagenTe79:online}. 

% link back to our paper
Fueled by the latest advances in text-to-image models, current image generation applications are becoming mainstream. With this broader adoption comes the question of how these new models' capabilities impact art practices, which we examine in this paper.

% With each advance in generative machine learning models, new opportunities for generating and editing images were enabled and discovered. Fueled by the latest advances in diffusion models, current image generation applications are becoming mainstream. With this broader adoption comes the question of how the model's underlying architecture and dataset can impact existing and future visual art practices.


\subsection{AI in Creativity Support Tools \& Human-AI Co-Creation}

AI tools have played a prominent role in creativity support tool (CST) research \cite{frih2019CSTinHCI, hwang2022Creative}. AI-based CST systems have been produced to support artistic generation in domains such as fashion and product design \cite{Sbai_2018_ECCV_Workshops, jeon2021Fashion, quanz2020machine}, music creation \cite{mccormack2019Music, louie2020Music, huang2020AISong}, drawing \cite{davis2015Drawing, davis2016Drawing,  oh2018Drawing, karimi2019Drawing}, visual design and story-boarding \cite{zhao2020IconGen, shi2020Storyboarding}, and storytelling \cite{hodhod2016Storytelling, perone2019Chatbot}.
Most of these tools support the artistic implementation process as either production aids (i.e., tools that perform most of the work of generating the art, e.g., generative text-to-image models) or as execution aids (e.g., AI-powered brush tools in drawing applications) \cite{chung2021Intersection}. 

Hwang et al. \cite{hwang2022Creative} further characterize how these tools apply AI models in the creative process as falling into four general categories: \textit{editors} (facilitate execution of processes), \textit{transformers} (aid in changing existing content), \textit{blenders} (combine 2 or more content sources), and \textit{generators} (produce novel content). In this framing, the large-scale TTI models that this work is focused on fall into the \textit{generators} category.

Additionally, research in the related field of Human-AI Co-Creation (a sub-field of Mixed-Initiative Co-Creativity \cite{yannakakis2014mixed}) is highly relevant. In the \textit{Library of Mixed-Initiative Creative Interfaces} \cite{spotoLibrary}, Spoto et al. proposed a framework to understand mixed-initiative co-creation as a process involving seven potential actions: \textit{ideate}, \textit{constrain}, \textit{produce}, \textit{suggest}, \textit{select}, \textit{assess}, and \textit{adapt}. 
%This project described the creative process of various systems using graphs containing a subset of these potential actions. 
Muller et al. \cite{muller2020mixed} extended this framework to generative AI applications, while Grabe et al. further simplified this extension and characterized four primary interaction patterns concerning GAN applications: curating, exploring, evolving, and conditioning \cite{grabe2022towards}. In our work, we observe similar themes, especially the notion of users of TTI models feeling like they are using the models to explore, and acting as curators of their outputs.

Work in this field has also identified core challenges in creating human-AI co-creation systems. For example, Chung et al. \cite{chung2021gestural} identified the limited ability to control the output of generative AI models and proposed using gestural input to constrain/guide the model output. Likewise, Buschek et al. \cite{buschek2021nine} identified a set of nine challenges system designers could encounter when developing human-AI co-creative systems. In the context of TTI models, they identified challenges of \textit{invisible AI boundaries} (``A (generative) AI component imposes unknown restrictions on creativity and exploration'') and \textit{conflicts of territory} (``AI overwrites what the user has manually created/edited'') as particularly salient. Participants in our study encountered similar challenges of identifying what the models are capable (and not capable) of, and in building upon prior inputs to the system.

% link back to our paper
% In our paper, we build on prior CST research and aim to discover how prior frameworks for Human-AI co-creation apply in the space of large language models' applications for image generation via text. We also aim to explore if prior challenges identified in human-AI co-creative systems persist in this domain and make future design recommendations to overcome them. 

Building on this prior CST research, we examine the emergent practices and goals that have evolved in concert with the latest generation of TTI models.

\subsection{Generative AI as an Artistic Medium}

Given the rapid advancement of AI models for generating images with ever more creative and novel styles, art historians and technologists have been actively discussing how to conceptualize AI-assisted art in relation to other artforms. Experts in these communities have disagreed as to whether generative models should be considered artists in-and-of themselves \cite{mazzone2019ArtCreativityAI} (as with the famous sale of \textit{Portrait of Edmond Belamy} auctioned by Christie's in 2018 \cite{Thefirst77:online}) or whether they should be considered as merely a tool employed by artists. Hertzmann \cite{hertzmann2018ComputersArt, hertzmann2020ComputersArt} argues that generative AI models are similar to the camera as it relates to the art of photography: a tool the enables the art. Hertzmann further theorizes that ``art is an interaction between social agents,'' and generative AI models are therefore best considered an agent in this interaction. Agüera y Arcas \cite{aguera2017ArtInAI} provides a similar argument with an in-depth discussion of the similarity between the emerging field of AI-generated art and photography, particularly surrounding the historical reaction of painters to the introduction of the camera. Grba \cite{grba2021brittle, grba2022DeepElse} provides a framework to critically evaluate art created with a generative AI model. In this framework, he echoes arguments above, and critiques what he refers to as ``the ever-receding artist'': the repeated occurrence of technologists referring to models as artists, thereby minimizing the contribution of the human who employed the model to create art. Finally, Browne \cite{brown2022AIArtist} explored what it means to be an ``AI artist'' and proposed the framing of an AI artist as a \textit{bricoleur} (building upon \cite{grba2019forensics}), saying, ``\textit{Bricolage} is common to generative art, where ideas are developed through playful experimentation with existing tools and techniques.''

% I tried to echo the introduction to section 4 in this link paragraph...

In our paper, we present an analysis of interviews with artists who employ a large TTI model for the generation of their art, highlighting the work of three of these artists and provide context around their motivations and goals. In our results, we find thematic alignment with perspectives advanced by Hertmann, Agüera y Arcas, and Grba above: While the models can produce surprisingly high quality output, dedicated users of these models employ the models as tools to explore specific themes and concepts. Accordingly, they have intentionally developed processes (e.g., locating domain-specific terminology) to improve their ability to achieve their individual goals.


\subsection{Diffusion and Auto-Regressive Models}
\rr{Diffusion models (e.g., Imagen~\cite{ImagenTe79:online} or DALL-E~\cite{DALLE279:online}) are trained by gradually adding noise to an image, until all of the image is completely noise. The model then learns to reverse the noising process to generate the original image. In this way, a diffusion model learns to synthesize an image from noisy images, and is capable of generating images from arbitrary ``noise.'' For text-to-image models, the models are also trained (conditioned) on text inputs~\cite{nichol2021glide}, allowing the model to produce an image from a noisy image input and text input, where the resulting image bears a resemblance to the input text.}

\rr{Autoregressive models, such as Parti~\cite{PartiPat10:online}, treat text-to-image generation as a sequence-to-sequence problem, akin to machine translation or other language modeling tasks. In the case of TTI models, the ``translation'' is from text to image (i.e., text tokens to image tokens).}

\rr{In our study, participants used both types of models.}
