\section{Applications}
\label{sec:results}

We apply depth covariance to three fundamental geometric vision tasks: depth completion, bundle adjustment, and monocular dense visual odometry (DVO).  For depth completion, we use the NYUv2 benchmark \cite{silberman_indoor_2012} and the train-test splits from \cite{ma_sparse_2018}.  For bundle adjustment and DVO, we train the covariance function on ScanNet \cite{dai_scannet_2017}, and evaluate on the TUM RGB-D dataset \cite{sturm_benchmark_2012}.  For the UNet, we use an input size of $256 \times 192$, 16 channels after the first layer, 5 downsampling steps, and 4 output levels.  This results in roughly 9 million parameters. 

\subsection{Depth Completion}
\label{subsec:depth_completion}

Depth completion is a fundamental task that will be leveraged for additional applications. We may directly condition on sparse observations as described in Section \ref{subsec:conditional} to obtain a dense depth map and uncertainties.

\subsubsection{Completion Accuracy and Error}

Since our method does not predict a specific instance of geometry, we report errors with respect to the GP posterior mean.  We compare to the foundational Sparse-to-Dense (S2D) \cite{ma_sparse_2018}, deep kernel regression with (KernelNet+R) and without refinement (KernelNet) \cite{liu_learning_2021}, and a recent state-of-the-art network RigNet \cite{yan_rignet_2022}. Results on the validation set with 500 random samples are shown in Table \ref{tab:depth_comp_nyuv2}.

\setlength{\tabcolsep}{2pt}
\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c c c c c|}
        \hline
        \multirow{2}{*}{Method} &
        \multicolumn{1}{c}{Error ($\downarrow$)} &
        \multicolumn{5}{|c|}{Accuracy ($\uparrow$)}
        \\
        & RMSE & $\delta_{1.02}$ & $\delta_{1.05}$ & $\delta_{1.10}$ & $\delta_{1.25}$ & $\delta_{1.25^2}$ \\
        \hline %\hline
        S2D \cite{ma_sparse_2018} & 0.204 & - & - & - & 97.8 & 99.6 \\
        KernelNet \cite{liu_learning_2021} & 0.198 & 65.5 & 82.9 & 91.6 & 97.7 & 99.8 \\
        KernelNet+R \cite{liu_learning_2021} & 0.111 & 84.8 & 94.1 & 97.4 & 99.3 & 99.9 \\
        RigNet \cite{yan_rignet_2022} & 0.090 & - & - & - & 99.7 & 99.9 \\
        Ours & 0.157 & 81.2 & 92.6 & 97.1 & 99.4 & 99.9\\
        \hline
    \end{tabular}
    \caption{Depth completion on NYUv2 with 500 sampled points.}
    \label{tab:depth_comp_nyuv2}
\end{table}

KernelNet also predicts three feature maps for 2D covariance parameters, but we achieve  better performance under the GP framework.  We achieve comparable \textit{accuracy} to state-of-the-art methods KernelNet+R and RigNet while using fewer parameters. We also do not convert the problem into classification \cite{liu_learning_2021} or have complex forward passes with iterative layers \cite{yan_rignet_2022}.  The depth covariance outperforms methods with similar UNet architectures in RMSE.  While the \textit{error} is not state-of-the-art, we use a lightweight network and do not train specifically for the single task of depth completion with the number of samples known \textit{a priori}.

We also explore varying sparsity, as depth completion networks are trained for a specific number of samples.  A comparison of RMSE for a varying number of test samples is shown in Table \ref{tab:depth_comp_nyuv2_sparsity}.  Depth covariance outperforms traditional depth completion methods for sparse inputs, and is competitive with SpAgNet \cite{conti_sparsity_2023} which is designed specifically for these cases and contains an additional non-local spatial propagation layer.  We note that for \textit{error} metrics, we simply use the posterior mean for comparison.  However, the GP provides a distribution over depths, not just a single instance, and we will demonstrate additional capabilities such as active decision-making and inferring latent geometry when direct observations are not present.

\setlength{\tabcolsep}{5pt}
\begin{table}[h]
    \centering
    \begin{tabular}{|l|r|r|r|r|}
        \hline
        \# Samples & 5 & 50 & 100 & 200 \\
        \hline
        CSPN \cite{cheng_depth_2018} & 2.063 & 0.884 & 0.388 & 0.177 \\
        NLSPN \cite{park_nonlocal_2020} & 1.033 & 0.423 & 0.246 & 0.\textbf{142} \\
        SpAgNet \cite{conti_sparsity_2023} & \textbf{0.467} & \textbf{0.272} & \textbf{0.209} & \underline{0.155} \\ 
        Ours & \underline{0.717} & \underline{0.298} & \underline{0.236} & 0.193 \\
        \hline
    \end{tabular}
    \caption{Depth completion RMSE (m) on NYUv2 with a varying number of input samples. The best result is in bold, while the second best is underlined.}
    \label{tab:depth_comp_nyuv2_sparsity}
\end{table}

\subsubsection{Posterior Uncertainty}

Calibrated uncertainties are beneficial for balancing multiple views and sensors in optimization.  Under-confidence does not account for the full information given by the constraints, while over-confidence may bias the solution.  Given that the GP provides uncertainties in addition to the mean, we evaluate the calibration properties for the depth completion setup from Section \ref{subsec:depth_completion}.

Since most methods typically estimate per-pixel observation noise, we assess the effect of including off-diagonal terms.  Calibration plots measure how well the model's predicted confidence matches the expected confidence.  To extend calibration plots for regression beyond variance \cite{kuleshov_accurate_2018}, we extract marginal covariances of varying dimension $D$ and calculate Mahalanobis distances using this block-diagonal approximation to the full covariance matrix. We then count the frequency of distances below varying chi-square thresholds that define the expected confidence.  In the variance case with $D=1$, residuals do not affect each other.  We plot the results in Figure \ref{fig:calib} along with the RMSE against the ideal calibration, where the empirical confidence matches the observed confidence.  Including more off-diagonal terms improves the model calibration, and significantly reduces the under-confidence of the model.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.87\columnwidth]{figures/pdfs/calib.pdf}
	\caption{Calibration plots of varying posterior marginal covariance block dimensions $D$ on NYUv2 depth completion.  The ideal calibration is $y=x$, where the observed confidence matches expected confidence.  The region above the line indicates model under-confidence, while the area below signals over-confidence.}
	\label{fig:calib} 
\end{figure}

\subsubsection{Active Sampling Evaluation}
\label{subsec:active_eval}

While the location of depth samples is often not a free variable, such as from LiDAR or depth sensors, in monocular vision, we may wish to actively estimate the depths of certain pixels to improve downstream tasks such as dense reconstruction.  We investigate whether the meta-learned covariance parameters may be leveraged for selecting more informative pixels.  We compare randomly sampling pixels uniformly against the greedy conditional variance as described in Section \ref{subsec:active_selection}.  The effect on depth error and the mean percentage improvement for a varying number of samples is shown in Figure \ref{fig:sampling}.  Note that the greedy active sampling consistently outperforms random sampling.  For a large number of samples, the relative improvement of active sampling decreases as there is a sufficient number of observations.  For a few samples, only coarse depth structure is retained, and the greedy selection occasionally encourages samples near the image boundaries.  Beyond greedy selection, other sampling methods that explicitly minimize uncertainty over the whole domain may demonstrate improved performance at the expense of computational cost.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figures/pdfs/sampling_w_percent.pdf}
	\caption{Mean depth completion RMSE for random and active sampling with a varying number of samples.  Green line shows mean percent improvement of active over random sampling.}
	\label{fig:sampling} 
\end{figure}

\subsection{Bundle Adjustment}
\label{subsec:bundle_adjustment}

Bundle adjustment is the foundation for many vision pipelines, and when successful, it produces very accurate camera poses.   However, the standard formulation of assuming independence between observations proves challenging for monocular SLAM systems, which may fail during initialization or when translation is negligible compared to rotation.  Many realistic scenarios require robust and fast initialization with little motion.  Therefore, we evaluate the use of the depth covariance in small baseline scenarios.  We also show that the depth covariance is not limited to 2D depth map inference, but can also be used to infer 3D structure.

Bundle adjustment jointly optimizes camera poses and point landmarks given pixel correspondences.  Traditionally, the cost involves the sum of reprojection errors, as well as pose and scale priors on the first pose to fix gauge freedom which we omit for brevity.  We also add our depth prior factor per camera, so for landmarks $\mathbf{P}$ in the world frame, and poses $T_{cW}$ for $c = 1,...,C$, we have:
\begin{align}
    E &= \sum_c \sum_i || \pi(\mathbf{T}_{cW}, \mathbf{P}_i) - \mathbf{x}_{c,i} ||^2_{\sigma_r^2 I} \notag\\
    &+ \sum_c || \log \left( [T_{cW} \mathbf{P}_c]_z \right) - m_c ||_{K_{c}} ^2,
\end{align}
where each frame also has a scale variable $m_c$ and depth covariance $K_c$ for vectorized observed landmarks $\mathbf{P}_c$ in that frame.  The projection function $\pi$ projects 3D landmarks into the image plane.  To reduce the influence of outliers, we use the Huber cost function for reprojection errors.  The least-squares cost is optimized using Levenberg-Marquardt via GTSAM \cite{dellaert_gtsam_2022}.  The corresponding factor graph is shown on the left of Figure \ref{fig:ba_dvo_fg}.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\columnwidth]{figures/pdfs/both_factor_graphs.pdf}
	\caption{Factor graphs for bundle adjustment and monocular dense visual odometry.  Pose and scale priors to constrain gauge freedom are omitted for simplicity.}
	\label{fig:ba_dvo_fg} 
\end{figure}

We divide all of the Freiburg 1 sequences from the TUM RGB-D dataset into small baseline 5-frame windows.  This yields 1544 sequences, emulating initializations to visual odometry systems.  For the frontend, Shi-Tomasi corners \cite{shi_good_1994} are detected and tracked using Lucas-Kanade tracking \cite{lucas_iterative_1981}, and outliers are filtered out via essential matrix RANSAC.  Poses are initialized using the motion capture data closest to the current RGB frame timestamps, while landmarks are triangulated if sufficiently constrained.  We optimize each sequence with and without the GP depth prior to evaluate its effect.  Using the corresponding depth frames, we compare the error of the reprojected sparse landmarks where valid depths occur, as well as the dense depth map error by conditioning on the sparse landmarks.  To handle monocular scale ambiguity, we align depth maps with the optimal scale before computing the error.  A boxplot of errors between the two methods is shown in Figure \ref{fig:bundle_adj_results}.  By exploiting the correlation between observations, significantly more coherent geometry can be generated, as visualized by an example in Figure \ref{fig:qual_ba_fig}.  Note that the prior is able to achieve consistent geometry despite the low baseline and only optimizing sparse landmarks.  The shadows reveal the complete chair legs and table.  We also show a longer example with over three seconds of data fused into a TSDF in Figure \ref{fig:title_fig}.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\columnwidth]{figures/pdfs/ba_depth_plot.pdf}
	\caption{Boxplot of sparse and dense depth errors across all 5-frame windows, with and without the GP depth prior included in the optimization.  The median RMSE is written next to the plots.}
	\label{fig:bundle_adj_results} 
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\columnwidth]{figures/pngs/ba_qual.png}
	\caption{Qualitative example of small baseline bundle adjustment with and without the GP depth factor.  Both examples fuse 5 predicted depth maps densified from the optimized landmarks into a TSDF, and are visualized with identical lighting.}
	\label{fig:qual_ba_fig} 
\end{figure}

\subsection{Monocular Dense Visual Odometry}
\label{subsec:visual_odometry}

\begin{figure*}[t]
    \includegraphics[width=\textwidth]{figures/pngs/odom_depth.jpg}
    \caption{Example depth maps of keyframes while running the dense odometry on ScanNet validation sequences and TUM.}
    \label{fig:odom_depths} 
\end{figure*}



We propose a monocular dense visual odometry system based on the depth covariance and conditioning on per-frame latent depth points.  We perform sliding window optimization over a fixed number of keyframes, where poses, scales, and inducing depths are optimized per-frame.  To regularize the depths, we may condition on the current estimates of the inducing depths as in Section \ref{subsec:conditional}, project depths into neighboring keyframes, and apply relative photometric error constraints.  The photometric error for pixels $n$ across all directed edges $E$ is:
\begin{align}
    E = \sum_{i,j \in E} \sum_n || I_i(\mathbf{x}_n) - I_j(w(\mathbf{x}_n, \mathbf{T}_i, \mathbf{T}_j, \mathbf{y_i}, m_i) || ^2_{\sigma_r^2 I},
\end{align}
where $I$ is the image intensity, $\mathbf{x}$ are pixel coordinates, $w$ is the warping function that queries a depth map and projects the corresponding point into the neighboring image, $\mathbf{T}$ are camera poses, $\mathbf{y}$ are estimated log-depth observations, and $m$ is the mean log depth.  The corresponding factor graph with the depth priors included is shown in the right of Figure \ref{fig:ba_dvo_fg}.  To initialize a new keyframe, we first run the RGB through the network to get the covariance feature maps and sample inducing points.  We perform active sampling since this is independent of whether log-depths are known as mentioned in \ref{subsec:active_selection}, and stop sampling if a maximum variance threshold is achieved.  The log-depths $\mathbf{y}$ of a new keyframe are initialized by minimizing the least-squares cost of the prior at the sampled locations and a predictive data term over the reprojected depths $\mathbf{d}$ from the latest keyframe:
\begin{align}
    \mathbf{y}_{\text{init}} = \argmin_{\mathbf{y}}  ||\mathbf{y} - m||^2_{K} + || \log \mathbf{d} - \mathbf{f}_*||_{\text{diag}[\Sigma_{*}]}.
\end{align}
When the window size is full, the last keyframe is dropped, and scale and pose priors are added to the current estimates of the oldest keyframe still in the window.  Initialization of the system is achieved via two-frame SfM, where the first frame's depth map is optimized along with the relative pose to another frame.  Tracking is performed using coarse-to-fine photometric tracking against the last keyframe.  We also optimize affine brightness parameters for varying exposure.

For quantitative evaluation, we compare our odometry to other learning-based dense systems on Freiburg 1 sequences in the TUM dataset.  Since our method is purely a sliding-window odometry system, we compare against the comparable monocular methods evaluated in \cite{teed_droid_2021} that lack global optimization and bundle adjustment.  We omit the floor sequence which drops a significant number of frames.  The absolute trajectory errors (ATE) are shown in Table \ref{tab:tum_ate}.  Since our dense odometry does not have global scale, we align to the ground-truth trajectory's scale. 

Despite the simplicity of our system, it is first or second in 6 of 8 sequences, and achieves the best mean error.  We only use photometric error compared to the additional depth and keypoint factors in DeepFactors.  While we lack many features in our simple sliding window formulation, we believe that including additional components will further improve the results.  Qualitative examples of inferred depth maps from the odometry on ScanNet validation sequences and TUM are shown in Figure \ref{fig:odom_depths}.  Note that our representation can predict a variety of thin structures and regularize flat ones, even in the case of high-frequency texture.  

Odometry runs at an average of 7-10 Hz on $256 \times 192$ images with 8 keyframes on a single RTX 3080.  The code is not optimized and alternates between tracking and mapping in a single thread, so we believe significant speed-ups are possible given additional optimization or compute.

\setlength{\tabcolsep}{3pt}
\begin{table}[t]
    \centering
    \begin{tabular}{|c|c c c c|}
        \hline
        \multirow{2}{*}{Sequence} & TartanVO & DeepV2D & DeepFactors & Ours \\
        & \cite{wang_tartanvo_2021} & \cite{teed_deepv2d_2020} & \cite{czarnowski_deepfactors_2020} & \\
        \hline %\hline
        360 & 0.178 & 0.243 & \underline{0.159} & \textbf{0.128} \\
        desk & \underline{0.125} & 0.166 & 0.170 & \textbf{0.056} \\
        desk2 & \underline{0.122} & 0.379 & 0.253 & \textbf{0.048} \\
        plant & 0.297 & \textbf{0.203} & 0.305 & \underline{0.261} \\
        room & 0.333 & \textbf{0.246} & 0.364 & \underline{0.257} \\
        rpy & \underline{0.049} & 0.105 & \textbf{0.043} & 0.052 \\
        teddy & \underline{0.339} & \textbf{0.316} & 0.601 & 0.475 \\
        xyz & 0.062 & 0.064 & \textbf{0.035} & \underline{0.056} \\
        \hline %\hline
        mean & \underline{0.188} & 0.215 & 0.241 & \textbf{0.167} \\
        \hline
    \end{tabular}
    \caption{Absolute trajectory error (m) on TUM Freiburg1.  The best result is in bold, while the second best is underlined.}
    \label{tab:tum_ate}
\end{table}
