\section{Learning a Depth Covariance Function}
\label{sec:learning_depth_cov}

Given an image, we model a distribution over possible depth functions via a Gaussian process (GP) \cite{rasmussen_gaussian_2005}.  With the input domain being normalized image coordinates $\mathbf{x}_i = (u_i,v_i)$ where $u_i,v_i \in [-1, 1]$, the outputs for any finite set of inputs are jointly Gaussian. The GP is then defined by a mean function $m(\mathbf{x})$ and a covariance function $k(\mathbf{x}, \mathbf{{x}'})$:
\begin{align}
    f(\mathbf{x}) \sim \mathcal{GP}\left(m(\mathbf{x}),\, k(\mathbf{x}, \mathbf{{x}'}) \right) .
\end{align}
For learning the parameters of this GP, we need to select the representation of depth, the mean and covariance functions, and the optimization objective.  We will outline our decisions in the following sections, but there exist many ways to define this prior.  

\subsection{Depth Representation}
\label{subsec:depth_rep}

When predicting depth, we often want to attenuate errors further from the camera view, so that nearby structures are prioritized.  Examples of such representations include inverse depth \cite{civera_inverse_2008}, disparity \cite{ranftl_towards_2022}, and log-depth \cite{eigen_depth_2014}. We select log-depth for two reasons.  First, the log-normal distribution is more suitable for skewed distributions \cite{rendu_normal_1979}, as the range of the GP is unbounded.  Representing depth or inverse depth with a normal distribution would require \textit{ad hoc} truncation, as depth functions could move behind the camera.   Second, we represent depth with relative scale, as the absolute scale ambiguity in monocular images accounts for much of the error in learned depth prediction \cite{eigen_depth_2014}.  We can then focus on learning the relationship between predictions.  In the log-depth formulation, a constant mean $m(\mathbf{x}_i) = m$ directly corresponds to a scale variable.  Given a log-depth observation $\mathbf{y}_i = \log{\mathbf{d}_i}$, we can adjust its scale via this mean:
\begin{align}
    e^{\mathbf{y}_i - m} = e^{\mathbf{y}_i} e^{-m} = C e^{\mathbf{y}_i} = C\mathbf{d}_i.
\end{align}
In other words, the median of the depth distribution is controlled by the mean of the log-normal distribution.  During training, similar to \cite{eigen_depth_2014}, we may find the optimal scale $m$ that minimizes our data loss. In addition, the log-depth representation allows covariance to be in relative scale.  At test time, the scale may be fixed if known, or jointly optimized with other variables.  Learning a more expressive mean function per image would be useful in the absence of any depth observations, but introduces many degrees of freedom.  For GPs, generalization is largely dependent on the covariance function, so we leave the mean to be a constant per image.

\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figures/pdfs/method_fig.pdf}
    \caption{Visualizing our depth covariance function: for every pixel of an input image, the trained network predicts a 2D kernel matrix. Here we show the covariance function between pairs of pixels in both matrix form and as edges in a graph, with the line thickness representing the magnitude of covariance.}
    \label{fig:method_fig} 
\end{figure}

\subsection{Covariance Function}
\label{subsec:cov_func}

We would like to learn the parameters of a depth covariance function using pairs of RGB and depth images.  By definition, the covariance function must be positive semidefinite (PSD).  In our framework, an RGB image is fed into a convolutional neural network (CNN), which outputs features for a base covariance function.  We model the depth observations as being jointly Gaussian so that the CNN and base kernel hyperparameters may be jointly learned.

For the base covariance function, we would like to avoid over-correlation of independent structures.  To achieve local influence, we use the family of nonstationary kernels described in \cite{paciorek_nonstationary_2003}.  Each point $\mathbf{x}_i$ in an image can be viewed as a Gaussian distribution with a 2D kernel matrix $\Sigma_i$.  Then, the covariance between two points is the convolution of these two densities over the input domain with a normalizing constant.  Alternatively, this can be viewed as the similarity between distributions via the Bhattacharyya kernel \cite{jebara_probability_2004}.  Given an isotropic covariance function $R^S(\sqrt{q})$, the closed-form expression is:
\begin{align}
    k(\mathbf{x}_i, \mathbf{x}_j) &= \sigma_f^2 \frac{|\Sigma_i|^{1/4} |\Sigma_j|^{1/4}}{|\Sigma_i + \Sigma_j|^{1/2}} R^S(\sqrt{q}), \\
    q &= (\mathbf{x}_i-\mathbf{x}_j)^T (\Sigma_i + \Sigma_j)^{-1} (\mathbf{x}_i-\mathbf{x}_j),
\end{align}
where $\sigma_f^2$ is a learnable signal variance.  To better handle discontinuities, we select the Mat\'{e}rn function as our isotropic covariance function.  With the base covariance function requiring a 2D PSD matrix for each pixel, the CNN outputs three channels $(c_1, c_2, c_3)$, which are parameterized given the positive diagonal and determinant constraints:
\begin{align}
    \Sigma_i &= \begin{bmatrix} e^{c_1} & \tanh(c_3) \sqrt{e^{c_1}*e^{c_2}} \\ 
                    \tanh(c_3) \sqrt{e^{c_1}*e^{c_2}} & e^{c_2}  \end{bmatrix}.
\end{align}
An example of five kernel matrices and the marginal covariance in matrix and graph form is shown in Figure \ref{fig:method_fig}. We use a UNet architecture \cite{ronneberger_u-net_2015} and output features at different levels for a multi-scale loss.  Each scale also has its own GP hyperparameters, signal variance $\sigma_f^2$ and noise variance $\sigma_n^2$.  Depth maps are coarsened for lower-levels, so finer levels are weighted higher during the total loss calculation.

While a more global covariance function, such as a squared-exponential over output features, would provide additional flexibility, we avoid this for two reasons.  First, changes in depths on one part of the scene may have significant influence on a completely unrelated part, as seen in \cite{bloesch_codeslam_2018}.  Second, deep kernel learning using this setup is unstable and biased towards over-correlating the input domain \cite{ober_promises_2021}.   By directly using pixel coordinates in our covariance function, we restrict changes in geometry to be local and bias the network to learn local appearance information.  

\subsection{Optimization Objective}
\label{subsec:learning}

In GP literature, model selection is performed by minimizing the negative log marginal likelihood (NLML)
\begin{align}
    -\log p(\mathbf{y} | X) &= \frac{1}{2} (\mathbf{y} - m)^T (K_{\text{ff}} + \sigma_n^2 I)^{-1} (\mathbf{y} - m) \notag\\
    &+ \frac{n}{2} \log 2\pi + \frac{1}{2} \log |K_{\text{ff}} + \sigma_n^2 I|,
\end{align}
where the matrix $K_{\text{ff}}$ is a PSD matrix with entries defined by the covariance function.  The first term is often referred to as the ``data fit'', while the third term is the ``complexity penalty''.  In our scenario, the complexity term is minimized by correlating points, while the data term ensures the ground-truth depth map is plausible given the mean and predicted covariance.

However, the marginal likelihood for a GP requires a $O(N^3)$ matrix inversion.  For pixels in an image, this becomes intractable.  Since depth images are relatively low-rank, we use a sparse GP approximation \cite{bauer_understanding_2016} during training by randomly sampling inducing points for a Nystr{\"o}m approximation to the full covariance matrix.  Given the covariance matrix for the inducing points $K_{\text{uu}}$ and the cross-covariance between the inducing points and the entire domain $K_{\text{uf}}$, we have:
\begin{align}
    K_{\text{ff}} \approx \tilde{K}_{\text{ff}} \triangleq K_{\text{fu}} K_{\text{uu}}^{-1} K_{\text{uf}} .
\end{align}
Specifically, we use the variational free energy (VFE) framework \cite{titsias_variational_2009}, which defines our training loss as
\begin{align} \label{eq:vfe_loss}
    \mathcal{F} &= \frac{1}{2} (\mathbf{y} - m)^T (\tilde{K}_{\text{ff}} + \sigma_n^2 I)^{-1} (\mathbf{y} - m) + \frac{n}{2} \log 2\pi \notag\\
    &+ \frac{1}{2} \log |\tilde{K}_{\text{ff}} + \sigma_n^2 I| + \frac{1}{2\sigma_n^2} \text{tr}{(K_{\text{ff}}-\tilde{K}_{\text{ff}})}.
\end{align}
The first three terms are the same as the original NLML, but using the approximate covariance, while the last term penalizes the conditional variances at all inputs given the inducing points, which only requires the diagonal of the full covariance matrix.  Note that we also assume homoscedastic observation noise variance $\sigma_n^2$ across the entire dataset.