\section{Network Architecture and Training Details}

For the convolutional neural network (CNN) component of our system, we use a UNet \cite{ronneberger_u-net_2015}.  The $256 \times 192$ RGB input is first converted into a 16-dimensional feature space, which is the input to the UNet.  We use 5 downsampling and upsampling layers.  Each downsampling layer consists of max pooling and two residual convolution layers \cite{he_deep_2016}.  Each upsampling layer bilinearly interpolates the feature map to a higher resolution, performs a convolution, and then the two residual convolutions.  We use LeakyReLU as the activation function  for all convolutions, and we also use GroupNorm \cite{wu_group_2018} with 16 groups.  

The final four upsampling layers give outputs of the covariance parameters at different resolutions.  When training, we compute the mean loss with respect to a resized depth image for each of these levels, and then scale the loss so that higher resolutions are given higher weight according to the number of pixels.  For example, the highest resolution will have four times more weight than the second highest.

For the Nystr{\"o}m approximation during training, we fix the rank to be 128.  The 128 inducing locations are sampled randomly since this is relatively efficient and was found to be more stable than active sampling.  Exploring the rank parameter and how it may trade-off expressiveness and compactness is an interesting avenue for future work. 

As mentioned previously, to handle scale, we solve for the optimal mean log-depth that minimizes the data term of the negative log marginal likelihood.  To minimize the variational free energy, we use the Adam \cite{kingma_adam_2015} optimizer with an initial learning rate of $3\mathrm{e}{-4}$.  During training, we used a batch size of 4  and performed data augmentation with random rotations, resized crops, flips, and color jitter.

\section{Active Sampling Implementation}

For active sampling of pixel locations to condition on or select as inducing points, we use the greedy variance selection described in \cite{guestrin_near-optimal_2005}. We calculate the conditional variance for all pixel locations with respect to the current samples, and select the location with the highest variance.  The conditional variance is the diagonal of the conditional covariance matrix described previously:
\begin{align}
    \Sigma_* &= K_{\text{ff}} - K_{\text{fn}} (K_{\text{nn}} + \sigma_n^2 I)^{-1} K_{\text{nf}}.
\end{align}
Since this form requires maintaining the inverse $(K_{\text{nn}} + \sigma_n^2 I)^{-1}$ which is dynamically changing, we perform $\mathcal{O}(n)$ updates to the Cholesky factorization as in \cite{ranganathan_online_2011}.  The decomposition is written as
\begin{align}
    (K_{\text{nn}} + \sigma_n^2 I)^{-1} = (L L^T)^{-1} = L^{-T} L^{-1}
\end{align}
Furthermore, we may also avoid recomputing the entire variance $\text{diag}[\Sigma_{*}]$ from scratch for each newly added point.  We may write the conditional covariance as
\begin{align}
    \Sigma_* &= K_{\text{ff}} - (L^{-1} K_{\text{nf}})^T (L^{-1} K_{\text{nf}}).
\end{align}
Since we only require $\text{diag}[\Sigma_{*}]$, we only need to add the new row of $L^{-1} K_{\text{nf}}$ for each new input point, and take the squared norm of each column when computing the variance.  We do not actually invert $L$, but instead use efficient triangular solves via forward substitution.  Thus, we only need to update $L$ and $L^{-1} K_{\text{nf}}$ each with a new row.  This avoids $\mathcal{O}(n^3)$ inversions and $\mathcal{O}(n^2)$ triangular solves for each step of the greedy selection. 

\section{Visual Odometry Photometric Factor}

As mentioned previously, given log-depths reference frame $i$, we form the photometric constraint with respect to frame $j$:
\begin{align}
    E = \sum_{i,j \in E} \sum_n || I_i(\mathbf{x}_n) - I_j(w(\mathbf{x}_n, \mathbf{T}_i, \mathbf{T}_j, \mathbf{y}_i, m_i) || ^2_{\sigma_r^2 I}
\end{align}
where $I$ is the image intensity, $\mathbf{x}$ are pixel coordinates, $w$ is the warping function that queries a depth map and projects the corresponding point into the neighboring image, $\mathbf{T}$ are camera poses, $\mathbf{y}$ are the latent log-depth observations, and $m$ is the mean log depth for a given frame.  

First, the dense log-depth map is formed via the GP conditional mean:
\begin{align}
    \mathbf{f}_* &= m_i + K_{\text{fn}} (K_{\text{nn}} + \sigma_n^2 I)^{-1} (\mathbf{y}_i - m_i).
\end{align}
The 3D points $\mathbf{P}_i$ in the reference frame can be calculated via backprojection of the vectorized pixel coordinates $\mathbf{x}_i$ via the known camera intrinsics:
\begin{align}
    \mathbf{P}_i = \pi^{-1}(\mathbf{x}_i, e^{\mathbf{f}_*}).
\end{align}
The points may then be transformed into frame $j$ and projected into the image to yield the correspondence
\begin{align}
    \mathbf{x}_j = \pi(\mathbf{T}_j^{-1} \mathbf{T}_i \mathbf{P}_i).
\end{align}
These steps describe the warping function $w$ that is used to achieve correspondence.

When the exposure times may vary, we also include affine brightness parameters $(a,b)$ for the photometric factor.  For brevity, we write the correspondences from $w$ as $\mathbf{x}_j$, so that the unwhitened residual becomes
\begin{align}
    \mathbf{r}_{i,j} = I_i(\mathbf{x}_i) + b_i - \left( \frac{e^{-a_i}}{e^{-a_j}} I_j(\mathbf{x}_j) + b_j \right).
\end{align}
The affine brightness terms are jointly optimized with the other unknowns.  To further robustify the cost against occlusion and specular surfaces, we use the Huber robust cost function instead of the non-robust least-squares cost.