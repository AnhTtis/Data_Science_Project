\section{Related Work}
\label{sec:related_work}

\noindent \textbf{View-based Priors for Geometric Inference}
Traditionally, sparse reconstruction methods have ignored per-image correlations \cite{schonberger_structure--motion_2016}, 
\cite{mur-artal_orb-slam2_2017}, while dense methods \cite{newcombe_dtam_2011}, \cite{engel_lsd-slam_2014} have used naive priors based on neighboring pixels.  With the availability of depth data and the rise of deep learning, a variety of learned depth priors have been proposed.  Although monocular depth estimation methods that directly predict geometry \cite{eigen_depth_2014}, \cite{godard_digging_2019}, \cite{ranftl_towards_2022} have demonstrated great progress, they may predict irreparable mistakes and do not allow for handling ambiguities.  Utilizing surface normals may refine these depth estimates \cite{bae_irondepth_2022}, but the improvement is still subject to errors from the initial depth prediction.  For this reason, we focus on approaches that balance learned priors and test-time optimization. Popular approaches include predicting low-dimensional latent codes \cite{bloesch_codeslam_2018}, \cite{czarnowski_deepfactors_2020} and generating a basis of depth maps \cite{tang_ba-net_2019}, \cite{graham_ridgesfm_2020}.  While these methods utilize single image information to reduce the dimensionality of depth map inference, they fix capacity during training, produce overly smooth depth maps, and create over-correlated global changes of the depth map.  Predicting mesh vertices in the image plane \cite{bloesch_learning_2019} permits distribution of capacity and optimizing depth, but the number of latent variables is still fixed and an explicit mesh representation cannot easily model complex scenes.  

Instead of predicting geometry directly, we focus on predicting geometric correlation.  By decoupling the pipeline into an image processing network and a GP, the network is not responsible for learning geometry directly and we do not need to fix the geometric capacity.  The dimensionality of the subspace may be adapted for representing low-rank scenes more compactly and complex geometry with high-fidelity.  Lastly, we use a covariance function with locality to bias learning towards local appearance information.

\noindent \textbf{Learning Covariance Functions} 
Choosing covariance functions and performing model selection for GPs is a well-studied topic \cite{rasmussen_gaussian_2005}.  While stationary kernels have been explored for merging LiDAR observations with monocular depth estimates \cite{yoon_balanced_2020}, more expressive kernels are required for sparse observations. Nonstationary covariance functions using local Gaussian parameterizations \cite{paciorek_nonstationary_2003} have shown potential in robotic terrain mapping \cite{lang_adaptive_2007}.  However, since GP model selection occurs per data example, optimizing hyperparameters is challenging, and novel nonstationary kernels have been proposed to limit flexibility \cite{chen_ak_2022}.

Rather than performing model selection per image, we are inspired by deep kernel learning (DKL) \cite{wilson_deep_2016}, which leverages training data to predict kernel hyperparameters.  Alternatively, this may be viewed as meta-learning, where each task is an RGB-depth example \cite{patacchiola_bayesian_2020}.  To mitigate over-correlation when using stationary kernels in feature space as proposed in DKL \cite{ober_promises_2021}, we utilize local information in pixel space \cite{paciorek_nonstationary_2003} to model surface geometry.  While deep nonstationary kernel regression has been explored for depth completion \cite{liu_learning_2021}, GPs balance data fit and model complexity during training, and the uncertainty estimates are conducive to decision-making and inference in optimization frameworks.

\noindent \textbf{Residual Covariances in Computer Vision}
Uncertainties in machine learning are often divided between two types: model uncertainty and data uncertainty \cite{kendall_what_2017}.  For vision problems, model uncertainty is often ignored due to the availability of data and for tractable optimization.  Residual uncertainty is usually predicted from a network as a per-pixel variance due to high-dimensionality, but the assumption of independence ignores the correlation present in images.  For example, variational autoencoders (VAEs) \cite{kingma_autoencoding_2014} with a diagonal likelihood output overly smooth mean predictions and unnatural samples with salt-and-pepper noise.  To introduce correlation in the likelihood, structured uncertainty prediction networks (SUPNs) predict a full information matrix by defining a graph topology consisting of specific neighbors \cite{dorta_structured_2018}.  Since this matrix is sparse, it can be efficiently inverted to obtain the covariance matrix.  This has also been leveraged to distill monocular depth prediction ensembles into a single SUPN \cite{simpson_learning_2022}.  

Similar to SUPNs, we move beyond diagonal covariance approximations that lack correlation.  However, we learn a covariance function which does not require a predefined graph topology and allows for long-range correlations beyond neighboring pixels.  Furthermore, the marginal distribution of any set of variables may be examined without constructing the full joint distribution, which is of great interest for compact inference in geometric vision.