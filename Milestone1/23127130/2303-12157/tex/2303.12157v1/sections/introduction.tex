\section{Introduction}
\label{sec:intro}

Inferring the 3D structure of the world from 2D images is an essential computer vision task. In recent years, there has been significant interest in combining principled multiple view geometry with data-driven priors. Learning-based methods that predict geometry provide a prior directly over the latent variables, which avoids the ill-posed configurations of traditional methods.  However, direct, overconfident priors may prevent realization of the true 3D structure when multi-view geometry is well-defined.  For example, data-driven methods have shown tremendous promise in monocular depth estimation, but often lack consistency when fusing information into 3D space.

Thus far, designing a unified framework that combines the best of learning and optimization methods has proven challenging.  Recent data-driven methods have attempted to relax rigid geometric constraints by also predicting low-dimensional subspaces or per-pixel uncertainties.  Fixing capacity during training is often wasteful or inflexible at test time, while per-pixel residual distributions typically explain away the limitations of the model instead of the relationship between errors.  In reality, the 3D world is anywhere from simple to complex, and an ideal system should explicitly adapt its capacity and correlation based on the scene.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figures/pdfs/title_fig.pdf}
	\caption{Example monocular reconstruction using the depth covariance for bundle adjustment and dense depth prediction from three seconds (100 frames) of TUM data.  Three representative images and the mesh from TSDF fusion of the depth predictions are shown. Each frame leverages the learned covariance function to model geometric correlation between pairs of scene points.}
	\label{fig:title_fig} 
\end{figure}

In this paper, rather than directly predicting geometry from images, we propose learning a depth covariance function. Given an RGB image, our method predicts how the depths of any two pixels relate. To achieve this, a neural network transforms color information to a feature space, and a Gaussian process (GP) models a prior distribution given these features and a base kernel function.  The distinction between image processing and the prior enables promoting locality and granting flexible capacity at test time.  Locality avoids over-correlating pixels on distinct structures, while adaptive capacity permits tuning the complexity of our subspace to the content of the viewed scene. 

Learning this flexible, high-level prior allows for balancing data-driven methods with test-time optimization that can be applied to a variety of geometric vision tasks. Furthermore, the covariance function is agnostic to the 3D representation as it does not directly learn a geometric output.  Depth maps may be requested by conditioning on observations, but the prior may also be leveraged for inferring the desired latent 3D representation. In Figure \ref{fig:title_fig}, we illustrate depth covariance along with an example of bundle adjustment, dense depth prediction, and multi-view fusion.

In summary, our key contributions are:
\begin{itemize}
    \item A framework for learning the covariance function by selecting a depth representation, a base kernel function, and a scalable optimization objective
    \item Application of the prior to depth completion, bundle adjustment, and monocular dense visual odometry
\end{itemize}
