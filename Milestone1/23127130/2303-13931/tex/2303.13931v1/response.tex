% An example for the ar2rc document class.
% Copyright (C) 2017 Martin Schroen
% Modifications Copyright (C) 2020 Kaishuo Zhang
%
% This program is free software: you can redistribute it and/or modify
% it under the terms of the GNU General Public License as published by
% the Free Software Foundation, either version 3 of the License, or
% (at your option) any later version.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
%
% You should have received a copy of the GNU General Public License
% along with this program.  If not, see <http://www.gnu.org/licenses/>.

\documentclass{ar2rc}
\usepackage{booktabs}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\title{MSdocTr-Lite: A Lite Transformer for Full Page Multi-script Handwriting
Recognition}
\author{Marwa Dhiaf, Ahmed Cheikh Rouhou, Yousri Kessentini, Sinda Ben Salem}
\journal{Pattern Recognition Letters}
 \usepackage{multirow}
\begin{document}

\maketitle

Thank you for giving us the opportunity to submit a revised draft of my manuscript titled: 
MSdocTr-Lite: A  Lite  Transformer for Full  Page  Multi-script  Handwriting Recognition, to your prestigious journal. We appreciate the time
and effort that the reviewers have dedicated to providing valuable feedback on
our manuscript. We are grateful to the reviewers for their insightful comments on our
paper. We have been able to incorporate changes to reflect most of the suggestions provided
by the reviewers. We have highlighted the changes within the manuscript.
Here is a point-by-point response to the reviewersâ€™ comments and concerns:

\section{Associate Editor}
\textit{\textbf{MGE: Although your manuscript falls within the aim and scope of this journal, more work is needed to substantiate the conclusions in your manuscript. Please, observe all issues reported by the reviewer and submit a new version of your manuscript.
.}}

\AR We thank the Associate Editor for this positive feed-back. We did our best to respond to all the comments of the reviewers.

\section{Reviewer \#1}
\RC  The authors propose to apply the Transformer architecture, a lite version, to full-page handwritten text recognition (HTR).
The architecture is evaluated on some publicly-available datasets, for two scripts, and one of them (Arabic) reads from right-to-left.
Performance is measured at character-error rate, and the results are at par with the state-of-the-art for some datasets.


\AR We thank Reviewer 1 for this fair summary of our work.

 

\RC The paper reads well, but from Section 3.3 up to Section 4, it could use some re-writing to make the presentation clearer.
It also needs more proof-reading (e.g., bloc instead of block, "immersive amount of GPU", I guess "immense", missing \% after CER values, etc.
There are several small typos or mistakes that need to be addressed.)

\AR 
We thank the reviewer for mentioning these typos and mistakes. As suggested by the reviewer, we have done our best to improve the quality of the paper and a special focus is given to section 3 and 4.

\RC Figure 1 needs to be reworked, it is not readable on paper; using two columns for it would improve its readability.



\AR 
We have updated Figure 1 in the revised paper as suggested by the reviewer.
 

 

\RC I would like to make a remark regarding some comments on the processing time for recurrent layers:
* section 2.2, line 152: the major time consumer of the approach was that characters were predicted one at each time,
and the attention layers needed to be recomputed again. That is why the author moved to architecture in [11]. The same issue happens in the DAN architecture [8].


\AR 
We thank the reviewer for proposing constructive remarks to  enhance the quality of the paper. We have added this information to the revised manuscript. (L155)

\RC Section 1: line 85: curriculum learning was not devised to train on smaller amounts of data, but to improve the convergence of the model. Could the authors either correct this or explain how a curriculum alleviates the lack of training data?



\AR 
%We thank the reviewer for this remark, however we should explain this point to make it more clear:
%\DIFadd{The aim of our work is to propose a lite architecture in terms of number of parameters and in terms of low-resource data, we have already trained the model, on one stage learning (see table 2 [column,row][4,4]) and the model fails to converge, however using the curriculum strategy, helps the model to progressively to converge.}
We understand the reviewer's concern and we try here to explain the interest of the curriculum learning in the context of this work. In fact, in the ideal case where a huge quantity of full page document images is available, a large transformer model (with many layers and many heads) can converge without the need for a curriculum learning strategy as proven in [6]. In our case, where only a limited amount of training data is available, we have proven that the lite transformer model fails to converge (as described in table 2) without the curriculum learning strategy. In conclusion, we agree with the reviewer that the curriculum strategy helps the model to converge progressively, but there is a correlation between the need of a curriculum learning strategy, the limited amount of training data and the lite-transformer architecture.  




\RC Section 3.1, from line 203. I think the authors want to state that they use a 1x1 convolution layer to match the number of features from the backbone network and the Transformer input. The wording is somewhat confusing and needs some re-writing. Later in the same paragraph, please mention which kind of "flattening" is being performed (re-shape of the tensor, maxpooling on the "height" axis, etc.). Moreover, in Figure 1 the flattened tensor does not follow the usual representation that is a single "brick layer" in the vertical dimension.

\AR
As suggested by the reviewer, Figure 1 has been altered. More information are also added in the revised manuscript concerning the  $1\times1$ convolution layer (L207) and the kind of flattening (L213).


\RC  Section 3.2, in the part were teacher-forcing is mentioned. It is stated that the GT is shifted. That is not what teacher-forcing is about, please review this part.



\AR As suggested by the reviewer, we have updated this part to explain the role of the teacher-forcing process in the training stage (Section 3.2).


\RC
 Section 3.3: The authors claim that the approach does not need line or other lower-level annotations,
but still employs word segmentation to create blocks of few lines (between 1 and 4) with a small number of words.
The authors should nuance their claims, or explain how line segmentation could be found using some other techniques (via forced-alignment of a line-level recognizer against the whole page GT.)



\AR
We understand the reviewer's concern. Indeed, our proposed curriculum learning strategy consists of two pre-training stages performed on generated blocs where we start teaching the model the easy samples and progressively increase the task difficulty to reach full page document images. But these pre-training steps are performed only once, they can be done using synthetic data, or using any public dataset providing the segmentation annotation (which is doing in our case). The resulting model will serve as a starting point to train models with real page level data (even on different scripts). We note that during the inference, the model does not need any line or other lower-level segmentation step and the recognition is performed at page level. All these details are added in section 3.3 of the revised manuscript.   
%It is correct that we have used word images in our curriculum strategy. All the utilized images are collected from public datasets and can be replaced with synthetic text images as in [7]. The only aim of these data is to help the model's convergence as we have a limited amount of data. As this step is only required once, the transfer-learning of the model to new datasets does not require any segmented data. 


%You have raised an important point here. However, we would like to emphasize this point: 


%\DIFadd {  In fact, the segmented annotation is used just for the first pretrain model and only in the training phase, but not in the inference. However when moving to other script we don't reuse the same pre-processing . We used only the real dataset and without reusing the steps of the first baseline model .}

\RC It is not clear it if the network is trained first on 1-line blocks, then on 1+2 lines, and so on. Could the authors make the curriculum more explicit?


\AR
As suggested by the reviewer, we have updated section 3.3 to clarify the three stages of the proposed curriculum learning strategy and the used data for each stage. We note that in each stage, the training blocs are mixed independently of their number of lines.   

\RC 
Line 237: it is not clear how this would help learning the reading order. This is most likely learned from the order of the characters in the GT, and maybe if the lines in the previous stage had more words, then this step could be avoided.

\AR
We agree with the reviewer, there are several unclear details in section 3.3. We have updated almost the totality of this section to clearly explain the different curriculum learning stages. We provide also in Table 1 an ablation study which confirms the interest of the three-stage curriculum learning strategy in the case of a lite transformer model.
%Also, we have removed the 'reading order' in the line 237 as it's confusing. In fact, the second stage involves multi-line text images, as the first stage, but with more complexity (more words/line per image). It acts as a transitory stage between simple data and the original document images.
%However when finetuned the model on the second stage , the aim is to learn on more complex document that contains more long sequences and more number of lines. This is will help the model to generalise on full page .  
 




\RC
Line 246: it is stated that after the reading order is learned, then it its easier to adapt to real-data images.
There is nothing backing this claim, please avoid making this kind of statements, as it lessen the quality of the paper.

\AR
As suggested by the reviewer, this sentence is updated in the revised manuscript.
 



\RC
Section 3.4, line 269: smaller dataset does not mean less computational power, just shorter training times,
please revise this.


\AR We have updated this sentence in the revised manuscript (L269). 

 

\RC
Line 281: it is stated that the language model layer enabled higher performance by capturing aspects of the target dataset,
Table 3 indicates that the major part of improvement comes from adapting the backbone. Could the authors make some more comments on the influences of the language model and the "optical model" (more or less the backbone layer).

\AR We agree with the reviewer. We have updated this section in the revised manuscript by explaining the benefits of both optical and language models. We have also updated the discussion of the results presented in section 4.4, table 2.








\RC Section 4.2.1 Line 316: I am surprised the training and evaluation images have different resolutions. Is this from the dataset
itself or some issue in pre-processing? 256px images seem to be quite low resolution for a full page, even for IAM-like pages.
This makes less than 25 pixels for 10 lines of text, which is probably unreadable for a human.

\AR
 
The reviewer is right, the image resolution details were missing for the second and the third stage of the curriculum learning. We have used 256px images only in the first stage as we generated images including a max of 4 words / 4 lines. However, we have used a resolution of 1024x1024 for the images of the second and third stages of training. We have added these details in the revised manuscript, section 4.3. The test set consists of real IAM pages with a resolution of 1024x1024 px. This detail is updated in the revisited manuscript ( L 353).


%We agree with the reviewer on this remark, however we want explain that: for the first stage as we used small blocs, we had 2 options: either 






\RC
Line 337: I think that pre-training using a curriculum is what is most beneficial for the training.
Besides, a smaller architecture can be expected to be more efficient than a larger one in small-data regimes;
the authors are stating the obvious in this paragraph. which can be either removed or re-written with more insight. 


\AR
As suggested by the reviewer, we have removed these statements in the revised manuscript.  

\RC
There is no discussion about regularization, even the lite network has significantly more parameters than several previous work on those datasets. Regularization aspects need to be commented in the manuscript; could the authors address this?

\AR
We thank the reviewer for his suggestion. We have described the used regularisation techniques in the updated manuscript (L342).
 
\RC
Section 4.2.2, line 373: I am surprised that just one more accented character was added going from IAM to RIMES; French has several more characters that have diacritics compared to English.

\AR
 We thank the reviewer for mentioning this erroneous statement. It was a typo and section 4.4 has been revised.

\RC
There is no mention on how the learning rate was used when "unfreezing" the different layers of the network.
Could the authors please make some comment on this important aspect of training?

\AR
We agree with the reviewer and we have added learning rate information in the revised manuscript (L340).



\RC
Line 398: Why the authors expected to achieve higher performance on Esposales?
It is historical text, which has varying characteristics from IAM or RIMES, though its
language model is much simples as there is not much variation.
If the authors state that higher performance is expected, then they must provide backing for the claim.
Again, statements without backing should be avoided in scientific publications.

\AR
We agree with this comment and we removed this sentence from the manuscript.


\RC Line 405: it is not "known" that Arabic reads from right to left (a notable example in the original MDLSTM paper
by Graves, where they modelled Arabic without knowing the reading order, and still got groundbreaking results).


\AR
We mean here that the natural writing direction of Arabic is from right to left. This sentence is updated in the revised manuscript (L 374).







\RC
Section 4.2.3 Table 4. why there are not results from [3] in the table for paragraph-level recognition. That work was one the first in this sense and still provides a strong baseline.


\AR
We agree with the reviewer. There is a redundant entry in our bibliography: [3] and [11] point to the same reference. We have updated Table 3 accordingly.

    
\RC
I do not think the comparison against models without an external language model is fair, since the proposed architecture exploits a language model. Please include the other results so the read can have a better understanding of the comparison.

\AR
We understand the reviewer's comment. We added some references that use an external language model to the comparison table in the revised manuscript. We note that the decoder of a transformer is able to learn an implicit language model thanks to the self-attention mechanism. Nevertheless, it is just an implicit character LM and it can be combined with an external LM at the word level, which can improve the performance, especially in the case of close vocabulary handwriting recognition.

%We agree with the reviewer, results that includes language models are added in the table.






\RC 
Another work that is related and missing from the SOTA for IAM is Yousef and Bishop, CVPR 2020
"OrigamiNet: Weakly-Supervised, Segmentation-Free, One-Step, Full Page Text Recognition by learning to unfold"


\AR
As suggested by the reviewer, we added the requested reference in Table 3 of the revised manuscript. 


\RC
Line 432: the phrase is not clear, RIMES also have punctuations. Please make a clear statement of what the authors want to convey here. Is the fact that the punctuations might have influence on the language model and therefore on the results, or something else.


\AR
The discussion of the results on RIMES dataset is updated in the revised version( section 4.4). 



\RC
Line 440. Line segmentation can be alleviated using techniques such as Bluche, Moysset, Kermorvant, ICFHR 2014
"Automatic line segmentation and ground-truth alignment of handwritten documents". The authors could nuance their claim on the effort for line-segmentation. Moreover, is the 0.3 difference statistically significant? (line 448).



\AR
We agree with the reviewer that automatic line segmentation can be improved by some techniques like the cited one. However, in the case of complex document layouts, with high overlap between lines, the automatic segmentation can not perfectly work. We believe that a full-page recognition engine is more interesting to avoid unrecoverable early line segmentation errors.



\RC
There is no time measurement not FLOPS evaluation. Just saying that fewer parameters is a strong point for the proposed architecture is not enough. It can have fewer parameters but higher number of operations. Please either provide some timing or estimation of the computation required or do not make such claims.





\AR
As suggested by the reviewer, we add the training time of the different transformer architectures evaluated on our data. This information is updated in the manuscript (L 486).






\RC
Reference [23] is far from being SOTA for RIMES, [3] also provides a strong baseline for this dataset,
also Bluche at ICDAR 2017 "Gated Convolutional Recurrent Neural Networks for Multilingual Handwriting Recognition", where a large amount of external data was used.
Note that [3] and [11] are the same paper, please leave a single reference.




\AR
We agree with the reviewer, we have added the result of [3] on RIMES dataset, removed [23], and used a single reference for [3] and [11]. 







\RC
Regarding the transfer to a different reading order, I am not surprised it works. What I would like to see are results with mixed Arabic/English (or French) texts, if the model can address two reading orders, as in [6], or if not, what are possible ways to make it learn to change the reading order given the context?



\AR
We thank the reviewer for this suggestion. It would be interesting to explore this aspect of mixed scripts in our future works. In the case of this study, it is not possible to explore this issue, as there is not a publicly available handwritten document dataset with mixed scripts to train/evaluate our model.    

%However, in the case of our study, our aim is to propose a baseline pre-trained model on a first language, then by just a simple transfer learning, we can handle other scripts using a few data. That's the main contribution of this work, we don't consider the use of mixed data. But we take this remark into consideration for future works.




\section{Reviewer \#2}


\RC
- I suggest expanding the document with a clear explanation, for example, abbreviations should be introduced in the extended form, at least the first time.

\AR
We thank the reviewer for mentioning the abbreviations issues. We have updated the manuscript for a clearer presentation.

\RC
- I suggest changing the sentence at the end of the introduction about curriculum learning strategies: the use of that strategy is proposed, not the strategy itself.

\AR
We agree with the reviewer's comment. The sentence has been changed in line 85.

\RC
- Lines 177 to 183 contain three sentences. But they seem to be a split sentence. I suggest rewriting that part to make it more readable.

\AR
As suggested by the reviewer, the sentences have been rectified in the revised manuscript. (L182)

\RC
- I suggest separating the explanation of the datasets used by the experiments and adding a short introduction for each dataset to make the presentation of the results and their discussion clear.


\AR
We agree with the reviewer's comment. This could help the readability of the paper. We have added a short description for each dataset in Section 4.1. 

 \RC I suggest adding an ERC introduction, adding its formula, to make the results and their discussion clear.



\AR
We are sorry to not understand the term "ERC" mentioned by the reviewer. We kindly ask him to clarify it in order to address his concern.
 

\RC The table with the results of the Exposalles dataset is missing.

\AR
The Esposalles dataset was proposed to support research in information extraction and Named Entity Recognition from handwritten documents. To this reason, there is no other works in the literature that used this dataset for the HTR task. In our case, the evaluation on Esposalles dataset is mainly conducted to evaluate the adaptation capacity of our model for English to Spanish handwriting recognition.


 
\RC The subsection "direction of writing from right to left" is missing the results. There is only an explanation of the methods used.


\AR
 We have included results for "direction of writing from right to left" in table 6. The reviewer is right as we did not mention any reference for this table in that particular section. We revised the manuscript accordingly (L 442).
%\textcolor{purple}{The results for "direction of writing from right to left" is for just the Arabic dataset, and it ,is already presented in Table 6. }



\RC In lines 445-448, it states that 47 M samples are needed for the DAN model. But it is impossible to find this information in the paper. Also, it is not clear which tables are referred to and, consequently, which results are discussed. I suggest that this part be modified
 

\AR
We thank the reviewer for this remark. It was a mistake in the reference. We have corrected the reference to this paper ([6] instead of [7]).


\RC The current structure of Section 4 is a bit difficult to read. Some parts of the text seem to be inter-referenced. I suggest rewriting section 4 to improve the discussion of results. One suggestion would be to create a sub-section where the results obtained are clearly shown, and then add subsections to be able to make specific discussions of the results.

\AR
We thank the reviewer for the comment. We have considered the reviewer's suggestion in the revised manuscript.
We have updated the presentation of the experimental section.












































































\end{document}
