%\documentclass[review]{elsarticle}
\documentclass[preprint,5p,twocolumn]{elsarticle}
%\usepackage{prletters}
 \usepackage{multirow}

\usepackage{xspace}
\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\hyphenation{re-cher-cher}
\listfiles
\usepackage{todonotes}
\usepackage[utf8]{inputenc}
\journal{Pattern Recognition Letters}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}
\usepackage{amsmath}
\usepackage{booktabs}
\urlstyle{same}
\usepackage{float}
\usepackage{placeins}

\usepackage{xcolor}

\usepackage[compact]{titlesec}         % you need this package
\titlespacing{\section}{2pt}{2pt}{2pt} % this reduces space between (sub)sections to 0pt, for example

  
%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
 
\begin{document}

 \begin{frontmatter}


\title{MSdocTr-Lite: A Lite Transformer for Full Page Multi-script Handwriting Recognition}

%% Group authors per affiliation:
%% \author{Ahmed Cheikh Rouhou}
%% \address{Radarweg 29, Amsterdam}
%% \fntext[myfootnote]{Since 1880.}

% %% or include affiliations in footnotes:
 \author[firstaddress,secondyaddress,thirdaddress]{Marwa Dhiaf\corref{mycorrespondingauthor}}
 \cortext[mycorrespondingauthor]{Corresponding author}
 \ead{marwa.dhiaf.doc@enetcom.usf.tn}
 

 \author[firstaddress]{Ahmed Cheikh Rouhou}
 \ead {a.cheikhrouhou@instadeep.com}
\author[secondyaddress,thirdaddress]{Yousri Kessentini}
 \ead {yousri.kessentini@crns.rnrt.tn}
 \author[firstaddress]{Sinda Ben Salem}
  \ead {s.bensalem@instadeep.com}
 \address[firstaddress]{InstaDeep}
 \address[secondyaddress]{Digital Research Center of Sfax}
 \address[thirdaddress]{SM@RTS : Laboratory of Signals, systeMs, aRtificial Intelligence and neTworkS}

 \begin{abstract}
 The Transformer has quickly become the dominant architecture for various pattern recognition tasks due to its capacity for long-range representation. However, transformers are data-hungry models and need large datasets for training. In Handwritten Text Recognition (HTR), collecting a massive amount of labeled data is a complicated and expensive task. In this paper, we propose a lite transformer architecture for full-page multi-script handwriting recognition. The proposed model comes with three advantages: First, to solve the common problem of data scarcity, we propose a lite transformer model that can be trained on a reasonable amount of data, which is the case of most HTR public datasets, without the need for external data. Second, it can learn the reading order at page-level thanks to a curriculum learning strategy, allowing it to avoid line segmentation errors, exploit a larger context and reduce the need for costly segmentation annotations. Third, it can be easily adapted to other scripts by applying a simple transfer-learning process using only page-level labeled images. Extensive experiments on different datasets with different scripts (French, English, Spanish, and Arabic) show the effectiveness of the proposed model.
 \end{abstract}
 
 \begin{keyword}
 Seq2Seq model \sep page-level recognition\sep Handwritten Text Recognition\sep Multi-script\sep Transformer\sep Transfer Learning
 \end{keyword}

 \end{frontmatter}



\linenumbers
\section{Introduction}


Handwritten Text Recognition (HTR) aims to transform scanned handwritten documents into machine-encoded text. HTR is still a hard problem due to the high variance of writing styles, illegible handwriting, poor quality, and degradation. Furthermore, each handwritten script has specific properties (writing direction, character shapes, ligatures, etc) making the problem more challenging. 

 Current HTR models are leading to an acceptable performance \cite{ref_article6}, especially for modern documents with legible handwriting styles, known languages, vocabulary, and syntax. However, most of these systems rely on a segmentation phase before the recognition task. The segmentation consists of partitioning the document images into homogeneous components (characters, words, or lines). Early HTR approaches have applied segmentation techniques to extract character-level images from documents. These techniques suffer from a deficiency in performance accuracy as the segmentation generally fails due to the cursive and unconstrained nature of the handwriting. To surpass this problem, word-based segmentation methods have been introduced, where word images are extracted from the document before feeding into an HTR system.  But, this kind of approach faced some problems due to the irregularity of the inter-space and the intra-space between words. 

In the last decade, word segmentation-based approaches were abandoned in favor of text line recognition \cite{ref_article2,ref_article4}: a document can be segmented into individual lines to surpass any inconsistency between words and have more context. This kind of approach generally achieves state-of-the-art performance for mostly all types of documents (modern, historical, multi-script, etc) \cite{marti2002iam,grosicki2011icdar}, without suffering the heavy burden of character/word segmentation. Despite their success, line segmentation is also challenging due to non-uniform text line skew/slant or closely situated and touching text lines. Such problems can considerably affect the performance at the recognition stage. 
With the recent success of deep learning, the HTR community started exploring handwriting recognition at the paragraph or page level to avoid any intermediate segmentation pre-processing step. In \cite{bluche2017scan} an architecture for paragraph-level HTR with the use of MDLSTM and the attention mechanism was proposed. However, they later abandoned this approach due to high memory requirements, the lack of GPU acceleration for the training of MDLSTM, and intractable inference time.

Other approaches \cite{singh2021full,coquenet2022dan} have developed a sequence-to-sequence (Seq2Seq) models, based on the transformer \cite{vaswani2017attention} architecture, for page-level handwritten-document text recognition. These systems have reported state-of-the-art performances compared to line-level HTR ones. Nevertheless, training such models requires a huge amount of annotated data. Moreover, the length of extracted feature sequences, compared to line-level approaches, can be very large resulting in weeks of training consuming an immersive amount of GPU resources. Finally, porting these models to low-resource languages can be a difficult task as it requires a huge quantity of data to retrain the model from scratch.


Motivated by the above observations, we propose a lite transformer model for page-level handwritten document image recognition. The proposed model involves a limited number of parameters and can be trained without the use of external data. Our system is trained with a curriculum learning strategy, allowing it to learn the reading order and to scale to large input text images. This strategy is performed only once to help the model learn the page reading order. Then, the obtained model is used to handle other scripts requiring only a simple transfer learning process, and a few amounts of labeled page images. The proposed architecture can be trained using standard GPUs as it requires lower memory allocation, compared to similar proposed systems. 

The major contributions of this paper can  be therefore summarized as follows:


\begin{itemize}
    \item We propose an end-to-end lite transformer-based model for handwritten text recognition. The proposed model works at \textcolor{blue}{ the }page level to avoid unrecoverable early line segmentation errors, exploit a
    larger context and reduce the need for costly segmentation annotations.
    
    \item We propose a curriculum learning strategy that allows the model to be trained using a limited amount of annotated data and can learn the reading order at the page level. 
    
     \item We demonstrate that our lite transformer, built with only two layers and a single head of self-attention, can be easily adapted to other scripts by applying a simple transfer-learning process using only page-level labeled images. 
    
    \item Extensive comparative experiments are conducted to validate the effectiveness of our approach. We have tested our model on multiple scripts and languages, including English, French, Spanish, and Arabic. The obtained results confirm the effectiveness of the proposed model.
\end{itemize}

The remaining parts of this paper are structured as follows. Related works are presented in Section 2. Section 3 describes the proposed approach. Section 4 displays the experimental results. The conclusion and perspectives are stated in section 5.

 

\section{Related works}\label{sec2}
\subsection{Line-level HTR systems}\label{subsec1}


In the literature, using line-level segmented images to perform handwritten text recognition has gained a lot of interest. Systems following this approach benefit from the minor segmentation effort compared to the word or \textcolor{blue}{character-level} systems. 
%On one hand, lines can be easily extracted from documents, with a minimal segmentation error compared to word and character-level approaches. On the other hand, textual lines are converted into a sequence of features which has been proven to be more informative than treating features as identically independently distributed.

With the rise of deep learning, many deep neural networks have been proposed based mostly on the recurrent architectures \cite{graves2008offline,bluche2016joint}. However, such models require a large amount of annotated data to be trained. 
%To this end, systems in \cite{ref_32,ref_4} propose to overcome the data problem by using synthetically-generated cursive data with electronic true-type fonts. Nevertheless, this requires supplementary computational costs during the training phase.
To this end, authors in \cite{ref_article2}, proposed data augmentation and normalization techniques using Convolution Neural Networks combined with Long-Short-Term Memory CNN-LSTM, which significantly reduces the error rate on handwriting recognition tasks. Having unlimited annotated data, in turn, does not alleviate the issue of parallelization during the training stage. They suffer from the lack of computation parallelization inherently due to recurrent layers, which \textcolor{blue}{impact} both training and inference time. Consciously, the next methods have focused on training systems without a recurrent process. 


One can notice some latest trends, based on transformer architecture, in which, authors in \cite{ref_article6} propose an end-to-end model, which dispenses any recurrent network for HTR of text-line images. The architecture is based on multi-head self-attention layers
both at the visual and textual stages. Such a model aims to tackle both the proper step of character recognition from images, as well as to learn language-related dependencies of the character sequences to be decoded. The proposed work reaches competitive state-of-the-art performance at the line level.
%Recently, some pretraining-based methods were used to learn the representations for text recognition. Tr-Ocr \cite{li2021trocr} combines the BERT vision transformer \cite{bao2021beit} with a RoBERTa \cite{liu2019roberta} language representations model. BEiT works as an encoder while RoBERTa serves as a decoder producing the text. The authors used 687 M of printed and about 18 M of synthetically generated handwritten text lines in English to pre-train the TrOCR model.  
%In \cite{souibgui2022text} authors proposed Text-Degradation Invariant Auto Encoder (Text-DIAE) framework for representation learning. The approach involved a self-supervised pretraining on real and unlabeled data followed by a fine-tuning stage with the annotated data. Both of the latest approaches reach a competitive result, but they require a huge amount of data during pretraining and fine-tuning. 
In brief, line-level methods require segmentation and throwing away useful context.





\subsection{Page level HTR systems}\label{subsec2}
To avoid unrecoverable errors of segmentation, and to exploit larger context information,  some approaches are moving from line-level recognition to paragraph-level and even page-level recognition.


Methods like \cite{ref_article4}, \cite{bluche2017scan} applied an encoder-decoder with attention mechanisms to the task of multi-line text recognition. Both architectures use a CNN+MDLSTM encoder and an MDLSTM-based attention module. The encoder produces feature maps from the input image while the attention module recurrently generates line or character representations applying a weighted sum between the attention weights and the features. Finally, the decoder outputs character
probabilities from this representation. Both those two approaches present interesting implicit segmentation reaching competitive results. Nonetheless,  they are computationally expensive due to the MDLSTM layers, which could lead to high training and inference time.



Other approaches \cite{singh2021full,rouhou2022transformer} proposed different
models which are based on an attention mechanism to predict the text in a \textcolor{blue}{character-by-character} way. Those are trained with the cross-entropy loss and using a special end-of-transcription token. Authors of  \cite{ rouhou2022transformer} used a  transformer to evaluate their approach to paragraph-level images. \cite{singh2021full} introduced an encoder-decoder architecture, using a ResNet \cite{7780459} for encoding the image, and a transformer \cite{vaswani2017attention} for decoding the encoded representation into text. This model is trained to recognize, in addition to the text, the presence of non-textual areas (such as tables or drawings). This model has been trained to recognize full pages of handwritten or printed text without image segmentation. Despite the good results achieved using this configuration, this model is data-hungry and requires a lot of memory and computation resources.

In \cite{coquenet2022dan}, a new end-to-end segmentation-free architecture was proposed to address document image recognition in terms of both text and layouts. They proposed Document Attention Network  (DAN), an end-to-end transformer-based model for handwritten document recognition, including the textual components and the logical
layout information. This model is trained without using any segmentation label and evaluated on two public datasets at page and double-page levels. Although it leads to competitive results on different datasets at the page level, as well
as double-page level. The main drawback of this work is the recurrence issue at inference time. 

Considering the limitation of previous works, we propose in this paper lite-transformer, an end-to-end transformer-based model for handwritten document recognition.

\section{ Proposed approach } 
\label{section:sec3}




\begin{figure}[!ht]
\centering
\includegraphics[scale=0.20]{fig1.png}
\caption{Overview of the proposed architecture. Our lite transformer is composed of a transformer encoder combining convolutional layers and transformer layers and of a transformer decoder.} \label{fig1}
\end{figure}
The proposed model is depicted in Figure \ref{fig1}. It is composed of two parts: a Transformer-Encoder and a Transformer-Decoder, intending to map an inputted image that contains text into a sequence of characters, i.e, an \textcolor{blue}{image-to-sequence }architecture. \cite{vaswani2017attention}.
 

\subsection{Transformer-Encoder  }\label{subsubsec4}
The main objective of the encoder is to extract high-level feature representations from the sequence of features and encode the visual information. This component is mainly composed of a backbone and 2 transformer layers.  The backbone consists of the feature extraction architecture to represent the input image into high-dimensional vectors. It uses the ResNet-50 \cite{7780459} architecture without its last two layers( the average pool and linear projection). The ResNet produces a  feature representation of the full-page image. To make the size of the feature correlated with the
hidden side of the transformer, a 2D-convolutional layer (Conv2D) with a kernel size of (1 $\times$ 1) is added, preserving the same height/width dimensions of the input, that transformers the 2048-based features to match the hidden-size of the transformer. After, the 2D positions encoding \cite{9151002} are merged with these features to add positional information to each feature vector. Next, the 2D features map is transformed into 1D sequential features using flatten function. At this level, the output of the flattened layer matches the requirements of the transformer layers. After performing these steps on a page-level image, we obtain a feature sequence of the image. The feature sequence is then passed to a stack of 2 transformers layers (as shown in Figure \ref{fig1}).% Each transformer layer comprises two sub-components: a self-attention layer followed by a feed-forward network. A Layer normalization is applied to the input of each sub-component. 




\subsection{Transformer-Decoder}\label{subsubsec5}
The decoder is similar in structure to the transformer layers of the encoder except that it includes a masked language model. The masked language modeling approach is introduced to model the relation of the characters that are output from the decoder. It learns the correlation between visual encoded features and their corresponding characters. There are two types of symbols in our approach: visual labels and contextual labels. The visual labels ($V_{labels}$ ) are the readable characters from texts in images. The contextual labels ($C_{labels}$)are the start of page ($<sop>$), the end of line ($<eol>$), and the end of page ($<eop>$). The number of classes ($NB_{class}$) of the output layer includes the number of visual labels ($V_{labels}$ ) and the contextual labels ($C_{labels}$). We used teacher forcing at training time, which means that the ground-truth text input is shifted right. The input of the decoder is the textual contents of the image surrounded by $<sop>$ and $<eop>$ tags. After preparing the input text, the characters are embedded into a  vector of $h$ values as mentioned in equation \ref{eq8}.
\begin{equation}
\label{eq8}
Embed=Embedding(NB_{class},h)
\end{equation}
Then, the final input of the transformer decoder ($F_{labels}$) is prepared by applying 1D Positional encoding \cite{10.5555/3295222.3295349} to the embedded labels.  
\begin{equation}
\label{eq10}
F_{labels} = 1DPE(Embed(labels))
\end{equation}
The shape of $F_{labels}$ is $(h, N)$ where $N$ represents the length of the input labels sequence.  
\subsection{Curriculum learning strategy} 
\label{section:my}

Our lite transformer is trained directly on the page level, however, we found that the model is failing to learn the reading order in the case of low resource data.  For this aim, we set up a new strategy to facilitate the learning of the task by the model. Our training strategy involves curriculum learning where we start teaching the model the easy samples and progressively we increase the difficulty of learning to reach a full page level.
The strategy follows three sequential training stages. In the first stage, the model is trained on the word combination dataset. A set of small blocs are generated by combining randomly chosen word images using the ground truth word-level-segmentation. In total,  50k  bloc images are synthetically created where each line contains from one to four words, and each block is composed of a minimum \textcolor{blue}{of }1 line and a maximum of 4 lines.


In the second stage, the aim is to help the model to learn the reading order. A generated dataset was used to train the model obtained from the first stage. This dataset was created by concatenating randomly chosen lines from the real dataset. The number of lines is arbitrarily chosen, between 3 and 14. A set of 78k new page images was obtained for this stage. The amount of data is not increased to avoid image redundancy, since we only treat low-resource data. Once the reading order is learned, it becomes easier to adapt to real-data images in the third stage.

We note that we applied several transformations in all the previous stages. These transformations include random rotation,  brightness, contrast, perspective, and gaussian noise. A zero padding is used to create mini-batches.
 
 



\subsection{Transfer Learning }\label{sec4}

In this section, we investigate the use of transfer learning to improve the performance of transformer models in \textcolor{blue}{low-resource} data. The model was first trained on the source dataset, using a curriculum strategy. However, reusing the same strategy for further scripts may involve additional effort and requires pre-segmentation data. Thus, we benefit from the proposed strategy by helping the model learn the reading order page, then we transferred the obtained model to handle other scripts.  


The transfer learning approach we use is simple yet effective. We first train our lite model on a source dataset where there is a large amount of data and use the curriculum strategy. Next, we initialize the new model for a new target dataset. This means that the low-data 
the model will not start from scratch and random weights, but with the weights from the pre-trained model. Furthermore, this step can be performed on a relatively smaller dataset, thus requiring relatively less computational power. 

In an encoder-decoder transformer architecture, the encoder network transforms the sequence of features into high-dimensional representations which are then consumed by the attention-based decoder. The learned context representations should be able to capture relations of the target dataset. Thus, we choose to re-train the encoder on the target dataset to update the context representation. The representations extracted are then fed to the decoder. As we said above, the decoder is similar in structure to the transformer layers of the encoder except that it includes a language model. This component allowed the language model to capture the inherent nuances of the target dataset, thus allowing better performance. Moreover, to process a new language/script in the model, a learnable embedding space is usually assigned to each target dataset, it serves as a language-specific signal for the transformer to capture contextual representations across languages. Finally, the fully connected layer should be updated to include the number of characters of the target script.

 








\section{Experiments}\label{sec6}

 




\subsection{Datasets}\label{subsec11}
This section is dedicated to the evaluation of the lite transformer for document recognition. We evaluate the model on four public datasets: IAM \cite{marti2002iam}, RIMES \cite{grosicki2011icdar}, KHATT \cite{mahmoud2012khatt}, and Esposalles \cite{ROMERO20131658}.

%\subsubsection{IAM }
 
 
%The IAM dataset was made of a handwritten copy of text passages extracted from the LOB corpus. It corresponds to gray-scale images of English handwriting with a resolution of 300 dpi. This dataset provides segmentation at the page, paragraph, line, and word levels with their corresponding transcriptions. In this work, we have used  747 pages for training and 300 pages for evaluation.


%\subsubsection{RIMES }
%The RIMES is a popular handwriting dataset composed of gray-scale
%images of French handwritten text taken from scanned mails. The images have a resolution of 300 dpi.
%In the official split, there are 1,500 pages for training and 100
%pages for the evaluation. 
%Segmentation and transcription are provided at page, line, and
%word levels.  We kept the same split between training, validation, and test at a page level.

%\subsubsection{KHATT }

%The  KHATT is a freely available Arabic handwriting dataset. The dataset consists of scanned handwritten pages with different writers, text, and resolutions. Pages segmentation into lines is also provided to allow the direct evaluation of recognition systems without layout analysis. We have used 1500 paragraphs for training and 100 paragraphs for testing.

%Table \ref{rimes} presents some statistics of the user data from the KHATT dataset.
%\subsubsection{Esposalles }
%We used the public dataset proposed in ICDAR 2017 Information Extraction from Historical Handwritten Records (IEHHR) competition \cite{8270158}. This dataset is a subset of the Esposalles dataset. It has been labeled for information extraction. The dataset collects 125 handwritten pages, containing 1221 marriage records (paragraphs). We have used 872 records for training, 96 records for validation, and 253 records for testing. 

%\subsection{Metrics}\label{subsec13}

 

%\subsection{Implementations details}

%In the proposed architecture, the training for all datasets is carried out with the same hyper-parameters. The model was implemented using PyTorch and training was carried out using Tesla T4 GPU. ADAM optimizer was utilized with a fixed learning rate of 0.0001. We do not use any synthetic data, external language model, or lexicon constraints. The dropout is set to 0.1. We used the standard Character Error Rate (CER) to evaluate the quality of recognition. 
%For a set of ground truths, CER is computed using the Levenshtein distance which is the sum of the character substitutions ($Sc$), insertions ($Ic$), deletions  ($Dc$), divided by the total number of the ground truth characters ($Nc$). The formula of this metric is given respectively as below:
 %\noindent
%\begin{equation}
%CER=\frac{Dc+Sc+Ic}{Nc}
%\end{equation}
 

 
\subsection{Results}\label{subsec14}
 



 
\subsubsection{Evaluation of the learning strategy}\label{subsusec1}
\begin{table*}[!ht]
\caption{Comparison with transformer models on the test set
of IAM dataset. }\label{comp}
 
\centering 
\begin{tabular}{@{}lllllll@{}}
\toprule
\multirow{2}{*}{System}                    & \multicolumn{3}{l}{Architecture} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Using \\ external data\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Number \\ of parameters  transformer\end{tabular}} & \multirow{2}{*}{CER} \\ \cmidrule(lr){2-4}
                                           & layers    & heads   & d\_model   &                                                                                 &                                                                                                 &                      \\ \cmidrule(r){1-1} \cmidrule(l){5-7} 
FPHR \cite{singh2021full}    & 6         & 4       & 260        & yes                                                                             & 6.3 M                                                                                           & 6.3                  \\
FPHR \cite{singh2021full}    & 6         & 4       & 260        & No                                                                              & 6.3 M                                                                                           & 6.7                  \\
DAN \cite{coquenet2022dan} & 4         & 8       & 256        & yes                                                                             & 7.6 M                                                                                           & -                    \\
Ours                                       & 2         & 1       & 256        & No                                                                              & 3.9 M                                                                                           & 6.4                  \\ \bottomrule
\end{tabular}
\end{table*} 
To choose the best configuration of the proposed model, we have performed an ablation study on the architecture and the strategy of learning that gives the best performance. The study includes a variation of the hidden size, heads, and layers of the transformer. As previously described in Section \ref{section:sec3}, the lite architecture is composed of one head attention, two layers, and 256 hidden sizes. However, the large transformer is composed respectively of, 8 heads, 4 layers, and 512 hidden. While keeping the same size of images and hyperparameters for both models. The evaluation is carried out on the test set of the IAM dataset at a page level and the obtained results are given in Table \ref{tab1}. As it can be seen, a lite architecture gives a better result in all the strategies. The curriculum strategy is carried out in three stages as described in Section \ref{section:my}. In the first stage S1, the model was trained on small blocs. The height and width of images are fixed to 256 pixels for the first stage. A CER of 14.77 was obtained. This is due to the difference between the images used in the training phase and the images for the evaluation: they have different resolutions. In the second stage, S2 the model was fine-tuned on the generated pages. The CER is slightly better for this stage. The third stage S3 consists of fine-tuning the model obtained in the previous stage on the real IAM dataset. This boosts the performance drastically to 6.4  CER. However, training the model from scratch (FS) on  \textcolor{blue}{the} page level without progressive learning decreases the performance. The model seems unable to capture the reading order when dealing with large  text blocs with \textcolor{blue}{a }high number of lines.

\begin{table}[ht]
\centering
\caption{Evaluation of the training strategy in \textcolor{blue}{terms} of CER depending \textcolor{blue}{ on} the model size Lite/Large }\label{tab1}
\begin{tabular}{llllll}
\hline
S1 & S2 & S3 & FS & \begin{tabular}[c]{@{}l@{}} Lite transformer\end{tabular} & \begin{tabular}[c]{@{}l@{}} Large transformer\end{tabular} \\ \hline
x       &        &        &    & 14.77                                                          & 95.6                                                             \\
        & x      &        &    & 12.84                                                          & 43.16                                                            \\
        &        & x      &    & 6.4                                                            & 27                                                               \\
        &        &        & x  & 13                                                             & 40                                                               \\ \hline
\end{tabular}
\end{table}
The same process is applied to the Large transformer. Similar to the lite architecture, when curriculum learning is used,  the performance is slightly better (CER=27) but still lower compared to the lite results.
As expected, a CER of 40 was obtained when training the model directly on a page level. Notice that increasing the amount of training data does not improve performance. We can observe from this analysis, that a large transformer is not suitable for small sets of data. 

Overall, we can conclude that a lite architecture is more efficient than a large architecture in the case of \textcolor{blue}{low-resource} data. Indeed, we showed that curriculum learning reaches the best result and we highlighted its impact compared to traditional training.  

\subsubsection{Evaluation of the transfer learning }
In this section, we report the evaluation of the transfer learning on two types of writing directions: left to right for Latin scripts, and right to left for Arabic scripts.
\begin{itemize}
\item{Left to right writing direction:}
\end{itemize}

In this scenario, we initialized the model parameters by the weights of the original model trained on IAM dataset, and then train just a subset of layers going from the last layer to the first layer, using 1500 pages of text from the target dataset, RIMES. We start by learning the FC layer and the embedding weights for the decoder while keeping the other parts of the model fixed to the weights learned for the source dataset. For example, $Decoder[layer1], FC$ indicates that the first layer of the decoder and the FC layer have been retrained while keeping the rest of the layers fixed. Next, the first layer of the decoder is included in the set of layers to be re-trained. The remaining steps follow the same logic. An upper layer is added to the set of layers to be retrained until reaches the whole architecture. 

The results for the RIMES dataset are reported in Table \ref{table5}. In the first column of the table, we indicate the layers that were set as "trainable" during the training process, the rest of the layers remain frozen and initialized to the result of the learning of the source dataset. It can be observed that just \textcolor{blue}{by} retraining the FC layer(including the embedding weights of the decoder), a CER=30.7 is obtained on the test set. 
We note that when we tried to retrain the FC layer without including the embedding weights of the decoder, the model tends to underfit. For this aim, the embedding weights of the decoder have been added to the FC layer. This step is required to include the number of characters that the target dataset has (84  + an accented character for RIMES ).

Next, as it can be seen from the second row, the first layer of the decoder is included in the set of layers to be re-trained. It can be observed from the table, that the model reaches a better result when updating the language model of the target dataset. %The remaining rows follow the same perspective. 
The encoder layers are added to the group of layers that need to be retrained using the same procedure. The model can therefore capture the contextual connections on the target dataset.

From this analysis, we can conclude that a good TL approach is to initialize and re-train the whole network. The model reaches a competitive result: CER=2.9 using the same experimental conditions for both source and target datasets. However the target dataset (RIMES), exhibits more layout variability compared to the source dataset: a single page image can contain multiple indents and brings some irregularities
in terms of line spacing, text inclination, and horizontal text alignment. While the IAM layout is more structured and regular. 

\begin{table}[ht]
 
\begin{center}
 
\caption{Evaluation of the different transfer learning strategies on RIMES dataset}\label{table5}%

\begin{tabular}{ll}
\hline
Trainable \\Layers                           & CER \\ \hline
FC+embed                                            & 30.7    \\
FC+embed,Decod{[}L1{]}                             & 37.9    \\
FC+embed,Decod{[}L1,L0{]}                            & 25.4    \\
FC+embed,Decod{[}L1,L0{]},Encod{[}L1{]}              & 17.3    \\
FC+embed,Decod{[}L1,L0{]},Encod{[}L1,L0{]}           & 15.6    \\
FC+embed,Decod{[}L1,L0{]},Encod{[}L1,L0{]},Backbone & 2.9     \\ \hline
\end{tabular}

\end{center}
\end{table}

Moreover, we tested the impact of TL on the Esposalles dataset. Here, we can note that the vertical receptive field is lower than the paragraph image height. Thus, the size of images must be reduced for this dataset, to speed
up convergence. As expected, we reached better results (CER=1.7) at \textcolor{blue}{ the }paragraph level by just modifying the height of images, the other hyperparameters are kept fixed as the source dataset.
\begin{itemize} 
\item{Right To Left writing direction:}
\end{itemize}
After finding the best TL strategy, we use it to further prove its effectiveness on other directional different scripts. We used KHATT as the target dataset. As known, the Arabic language starts from right to left, for this, all the input images are horizontally flipped. Additionally, their images have different resolutions compared to the source dataset. The horizontal receptive field is bigger than the paragraph image width. To circumvent this issue and benefit from more context, the size of \textcolor{blue}{width } of the image is enlarged to 2048 and the height is fixed to 512 as the maximum number of lines per image is 6. This \textcolor{blue}{ helps} the model to capture more contextual information and to map the high-level feature to the text of the ground truth during decoding. 

 
 



% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\subsubsection{Comparison with state-of-the-art}

In this subsection, we compare our approach to state-of-the-art models on \textcolor{blue}{different} datasets. We start with the IAM  dataset, at the page level, and in the same conditions i.e. without language model or lexicon constraint.

\begin{table}[ht]
\centering
\caption{Evaluation of the lite transformer on the test set of IAM
dataset and comparison with the state-of-the-art approaches.  }\label{compiam}
\begin{tabular}{@{}lll@{}}
\toprule
Type                             & Architecture                       & CER   \\ \midrule
\multirow{3}{*}{Paragraph level} & Bluche et al   \cite{bluche2017scan}  & 11.1  \\       
                                 & Bluche et al \cite{bluche2016joint}                        & 10.1  \\
                        
\\                                 
\multirow{3}{*}{Line level}     & Chowdhury  \cite{chowdhury2018efficient}                 & 8.10  \\
                                 & Bluche et al   \cite{bluche2016joint}                      & 7.90  \\
                                 & Kang et al \cite{kang2020pay}               & 7.62 \\
  \\                               
Page level                        & Bluche et al \cite{bluche2017scan}                        & 16.2  \\
                                 & Carbonell \cite{carbonell2019end}  & 15.6 \\
                                 & FPHR \cite{singh2021full}                               & 6.7   \\
                                 & Wigington et al \cite{wigington2018start}           & 6.4 \\
                                 
                                 & Ours                                & 6.4   \\ \bottomrule
\end{tabular}
\end{table}



%To our knowledge, few works are evaluating their system on the IAM datasets at a page level. 
Table \ref{compiam} provides an evaluation of the lite transformer on the IAM dataset, at a page level. Comparisons at the page, paragraph, and line levels are carried out with
approaches under similar conditions without external data or an external language model. As one can notice, our model outperforms previous state-of-the-art models at the paragraph and line levels. This is could be explained by the fact that our lite transformer does well on different sequence lengths and the number of lines provided during the training. In addition to that, our method allows \textcolor{blue}{ for } exploiting a larger information context compared to those methods. Notice that the IAM dataset contains punctuation that the attention mechanism might incorrectly predict as the other characters.
 %Increasing the model size and/or image resolution, as well as the amount of training data, improves performance slightly.

\begin{table}[ht]
\centering
\caption{Evaluation of the lite transformer on the test set of RIMES 2011
dataset  .}\label{rimes}
\begin{tabular}{@{}lll@{}}
\toprule
Type                             & Architecture                                                                 & CER  \\ \midrule
\multirow{3}{*}{Page level} & Coquenet et al \cite{coquenet2021span}                                & 4.17 \\
                                 & Ours                                                         & 2.9 \\  
                                 &  Coquenet et al \cite{coquenet2022dan}                                  & 1.82 \\
 \\                                
\multirow{3}{*}{Line level}    & Louradour et al \cite{louradour2014curriculum}                           & 20 \\
                                & Coquenet et al \cite{coquenet2022end}                                  & 3.04 \\
                                 
                                 &  Coquenet et al \cite{coquenet2022dan}                                 & 2.63 \\
                                 & Puigcerver et al \cite{puigcerver2017multidimensional}             & 2.3  \\
                      
                       \bottomrule 
\end{tabular}
\end{table}




\begin{table}[htp]
\centering

\caption{Evaluation of the lite transformer on the test set of KHATT
dataset  .}\label{khatt}
\begin{tabular}{@{}lll@{}}
\toprule
Type                         & Architecture                                 & CER   \\ \midrule
\multirow{3}{*}{Line level} 
                             & DeepKHATT \cite{ahmad2017khatt} & 24.2 \\
                             & DeepLearnig based Arabic script \cite{ahmad2020deep}        & 19.98 \\
                             & CNN+BLSTM \cite{noubigh2019contribution} & 15.8 \\
                             &                                              &       \\
Page level                    & Ours                                         &  13.48  \\ \bottomrule
\end{tabular}
\end{table}

 

In table \ref{comp}, we are further interested to compare our lite model to state-of-the-art models that are only using transformer architectures on the same dataset (IAM).
 

The models from \cite{coquenet2022dan,singh2021full} require transcription and segmentation at \textcolor{blue}{the} line level to be trained. In particular, the model in \cite{coquenet2022dan} is \pre-trained on text line images using synthetic line segmentation, which requires a costly effort for the annotation. While the lite transformer used a specific
curriculum learning method for training. Another critical point for those two methods is the size of \textcolor{blue}{  the }training data. As one can notice from the table, a large number of samples 47 M is required for \cite{coquenet2022dan}. However, our lite model is trained on \textcolor{blue}{ a }few data with less effort, reaching better results ($-0.3$ ) and can be easily adapted to handle other scripts.
Moreover, despite the larger architecture(a large number of heads and layers)  of both methods, the lite transformer requires \textcolor{blue}{fewer} parameters which is a strong point for our architecture. The number of heads, layers, and model dimensions increase significantly the number of learnable parameters which leads consequently to more computing memory.


 


 Next, we present our results on the RIMES dataset in comparison to the state-of-the-art approaches in Table \ref{rimes}. It can be seen that models from \cite{louradour2014curriculum,coquenet2022end,puigcerver2017multidimensional,coquenet2022dan} require segmentation labels at the line level to be trained, which implies a cost effort for annotation. While the models from \cite{coquenet2021span,coquenet2022dan} and the lite transformer are trained on page level to reach better performance without the need for line segmentation. 



After that, we show our results on the KHATT dataset in Table \ref{khatt}. Compared to models that require segmentation at line level \cite{ahmad2017khatt,ahmad2020deep,noubigh2019contribution}, a better performance with a CER of 13.48  has been obtained by applying a transfer learning of the model trained on IAM (source domain) using only the page level images of KHATT (target domain). This result confirms the ability of our model to recognize other handwritten scripts  by applying a
simple transfer-learning process using only page-level labeled images.   


 
\section{Conclusion}\label{sec7}
In this paper, we proposed MSdocTr-Lite, a lite transformer model for  full-page multi-script handwriting recognition. To solve the common
problem of data scarcity,  MSdocTr-Lite can be trained
on a reasonable amount of data thanks to its light architecture. Contrary to the existing segmentation-based approaches, the
model can be easily fine-tuned on different scripts without using any segmentation label thanks to a simple transfer learning process.
%We trained this model without using any segmentation label, reducing the annotation cost. 

Interesting results have been obtained by MSdocTr-Lite   on English, French, Arabic, and Spanish handwritten scripts. For future research, we intend to integrate a self-supervised training stage, so that, the model can benefit from unlabeled page-level handwriting documents to reduce the annotation cost.   

%\section*{Conflict of Interest}
%The authors declare that they have no conflict of interest.  
%\section*{Code and Data Availability}

%The code used to generate data is available at: \url{https://github.com/MarwaDF/MSdocTr-Lite}


%\section*{References}

%\bibliography{mybibfile}
 
\begin{thebibliography}{10}

\bibitem{ref_article6}
L.~Kang, P.~Riba, M.~Rusi{\~n}ol, A.~Forn{\'e}s, M.~Villegas, Pay attention to
  what you read: Non-recurrent handwritten text-line recognition, arXiv
  preprint arXiv:2005.13044.

\bibitem{ref_article2}
C.~Wigington, S.~Stewart, B.~Davis, B.~Barrett, B.~Price, S.~Cohen, Data
  augmentation for recognition of handwritten words and lines using a cnn-lstm
  network, in: 2017 14th IAPR International Conference on Document Analysis and
  Recognition (ICDAR), Vol.~1, IEEE, 2017, pp. 639--645.

\bibitem{ref_article4}
T.~Bluche, Joint line segmentation and transcription for end-to-end handwritten
  paragraph recognition, Advances in neural information processing systems 29.

\bibitem{marti2002iam}
U.-V. Marti, H.~Bunke, The iam-database: an english sentence database for
  offline handwriting recognition, International Journal on Document Analysis
  and Recognition 5~(1) (2002) 39--46.

\bibitem{grosicki2011icdar}
E.~Grosicki, H.~El-Abed, Icdar 2011-french handwriting recognition competition,
  in: 2011 International Conference on Document Analysis and Recognition, IEEE,
  2011, pp. 1459--1463.

\bibitem{bluche2017scan}
T.~Bluche, J.~Louradour, R.~Messina, Scan, attend and read: End-to-end
  handwritten paragraph recognition with mdlstm attention, in: 2017 14th IAPR
  international conference on document analysis and recognition (ICDAR),
  Vol.~1, IEEE, 2017, pp. 1050--1055.

\bibitem{singh2021full}
S.~S. Singh, S.~Karayev, Full page handwriting recognition via image to
  sequence extraction, in: International Conference on Document Analysis and
  Recognition, Springer, 2021, pp. 55--69.

\bibitem{coquenet2022dan}
D.~Coquenet, C.~Chatelain, T.~Paquet, Dan: a segmentation-free document
  attention network for handwritten document recognition, arXiv preprint
  arXiv:2203.12273.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, I.~Polosukhin, Attention is all you need, Advances in neural
  information processing systems 30.

\bibitem{graves2008offline}
A.~Graves, J.~Schmidhuber, Offline handwriting recognition with
  multidimensional recurrent neural networks, Advances in neural information
  processing systems 21.

\bibitem{bluche2016joint}
T.~Bluche, Joint line segmentation and transcription for end-to-end handwritten
  paragraph recognition, Advances in neural information processing systems 29.

\bibitem{rouhou2022transformer}
A.~C. Rouhou, M.~Dhiaf, Y.~Kessentini, S.~B. Salem, Transformer-based approach
  for joint handwriting and named entity recognition in historical document,
  Pattern Recognition Letters 155 (2022) 128--134.

\bibitem{7780459}
K.~He, X.~Zhang, S.~Ren, J.~Sun, Deep residual learning for image recognition,
  in: Proceedings of the IEEE conference on computer vision and pattern
  recognition, 2016, pp. 770--778.

\bibitem{9151002}
J.~Lee, S.~Park, J.~Baek, S.~J. Oh, S.~Kim, H.~Lee, On recognizing texts of
  arbitrary shapes with 2d self-attention, in: Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp.
  546--547.

\bibitem{10.5555/3295222.3295349}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  u.~Kaiser, I.~Polosukhin, Attention is all you need, in: Proceedings of the
  31st International Conference on Neural Information Processing Systems,
  NIPS'17, Curran Associates Inc., Red Hook, NY, USA, 2017, p. 6000–6010.

\bibitem{mahmoud2012khatt}
S.~A. Mahmoud, I.~Ahmad, M.~Alshayeb, W.~G. Al-Khatib, M.~T. Parvez, G.~A.
  Fink, V.~M{\"a}rgner, H.~El~Abed, Khatt: Arabic offline handwritten text
  database, in: 2012 International Conference on Frontiers in Handwriting
  Recognition, IEEE, 2012, pp. 449--454.

\bibitem{ROMERO20131658}
V.~Romero, A.~Fornés, N.~Serrano, J.~A. Sánchez, A.~H. Toselli, V.~Frinken,
  E.~Vidal, J.~Lladós, The esposalles database: An ancient marriage license
  corpus for off-line handwriting recognition, Pattern Recognition 46~(6)
  (2013) 1658 -- 1669.

\bibitem{chowdhury2018efficient}
A.~Chowdhury, L.~Vig, An efficient end-to-end neural model for handwritten text
  recognition, arXiv preprint arXiv:1807.07965.

\bibitem{kang2020pay}
L.~Kang, P.~Riba, M.~Rusi{\~n}ol, A.~Forn{\'e}s, M.~Villegas, Pay attention to
  what you read: Non-recurrent handwritten text-line recognition, arXiv
  preprint arXiv:2005.13044.

\bibitem{carbonell2019end}
M.~Carbonell, J.~Mas, M.~Villegas, A.~Forn{\'e}s, J.~Llad{\'o}s, End-to-end
  handwritten text detection and transcription in full pages, in: 2019
  International Conference on Document Analysis and Recognition Workshops
  (ICDARW), Vol.~5, IEEE, 2019, pp. 29--34.

\bibitem{wigington2018start}
C.~Wigington, C.~Tensmeyer, B.~Davis, W.~Barrett, B.~Price, S.~Cohen, Start,
  follow, read: End-to-end full-page handwriting recognition, in: Proceedings
  of the European Conference on Computer Vision (ECCV), 2018, pp. 367--383.

\bibitem{coquenet2021span}
D.~Coquenet, C.~Chatelain, T.~Paquet, Span: A simple predict \& align network
  for handwritten paragraph recognition, in: International Conference on
  Document Analysis and Recognition, Springer, 2021, pp. 70--84.

\bibitem{louradour2014curriculum}
J.~Louradour, C.~Kermorvant, Curriculum learning for handwritten text line
  recognition, in: 2014 11th IAPR International Workshop on Document Analysis
  Systems, IEEE, 2014, pp. 56--60.

\bibitem{coquenet2022end}
D.~Coquenet, C.~Chatelain, T.~Paquet, End-to-end handwritten paragraph text
  recognition using a vertical attention network, IEEE Transactions on Pattern
  Analysis and Machine Intelligence.

\bibitem{puigcerver2017multidimensional}
J.~Puigcerver, Are multidimensional recurrent layers really necessary for
  handwritten text recognition?, in: 2017 14th IAPR International Conference on
  Document Analysis and Recognition (ICDAR), Vol.~1, IEEE, 2017, pp. 67--72.

\bibitem{ahmad2017khatt}
R.~Ahmad, S.~Naz, M.~Z. Afzal, S.~F. Rashid, M.~Liwicki, A.~Dengel, Khatt: A
  deep learning benchmark on arabic script, in: 2017 14th IAPR International
  Conference on Document Analysis and Recognition (ICDAR), Vol.~7, IEEE, 2017,
  pp. 10--14.

\bibitem{ahmad2020deep}
R.~Ahmad, S.~Naz, M.~Z. Afzal, S.~F. Rashid, M.~Liwicki, A.~Dengel, A deep
  learning based arabic script recognition system: benchmark on khat., Int.
  Arab J. Inf. Technol. 17~(3) (2020) 299--305.

\bibitem{noubigh2019contribution}
Z.~Noubigh, A.~Mezghani, M.~Kherallah, Contribution on arabic handwriting
  recognition using deep neural network, in: International Conference on Hybrid
  Intelligent Systems, Springer, 2019, pp. 123--133.

\end{thebibliography}

\end{document}