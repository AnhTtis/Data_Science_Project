% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

%\documentclass[review]{cvpr}
\documentclass[final]{cvpr}


% Note: Philip
% 1. Many prediction results on ScanNetv2 or S3DIS
%    in which we use network model trained with 1T1C to do predictions
% 2. Errata
%    -> We apologize that ...
%       Table 4 caption: S3DIS instead of ScanNet-v2 test set.
%       m in Line 519 -> m should be 0.9
% 3. Experiment: With and without relation network + graph propagation
%    -> test results
%    note: an experiment to support the footnote on P.5 of the paper
% 4. Experiment: compare with and without Memory bank
%    -> ...
%
% - 1. the examples of super voxels
%   a few cases

% 3. for the motivation of memory bank in relation network, I think there is a better explanation 
% than ``category unbalance''
%    ``protytypical net'' also random sample the same number of samples for training. The reason they
% do not use a memory bank is that, they do not make the network learn the information of typical 
% category, because the model will be applied to new categories in inference. It uses the idea of
% meta-learning. However, in our case, we want the network to fix the centroid embedding of typical 
% category, because our training/inference share the same category. Using memory bank benefits it. 
% 4. an ablation study for 3. 



\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{dsfont}
\usepackage{bbm}
\DeclareUnicodeCharacter{2212}{-}
\pagenumbering{gobble}
\newcommand{\xjqi}[1]{{\color{blue}{\bf\sf [xjqi: #1]}}}
\newcommand{\lzz}[1]{{\color{purple}{\bf\sf [lzz: #1]}}}
\newcommand{\phil}[1]{{\color{orange}{\bf\sf #1}}}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{6084} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}

%%%%%%%%% TITLE
\title{One Thing One Click++: \\Self-Training for
Weakly Supervised 3D Scene Understanding}

\author{Zhengzhe Liu$^{1}$ \quad   Xiaojuan Qi$^{2}$ \quad     Chi-Wing Fu$^{1}$ \\
$^1$The Chinese University of Hong Kong \quad $^2$The University of Hong Kong\\
{\tt\small \{zzliu,cwfu\}@cse.cuhk.edu.hk \quad  xjqi@eee.hku.edu.hk}
}

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}


3D scene understanding, e.g., point cloud semantic and instance segmentation, often requires large-scale annotated training data, but clearly, point-wise labels are too tedious to prepare.
While some recent methods propose to train a 3D network with small percentages of point labels, we take the approach to an extreme and propose ``One Thing One Click,'' meaning that the annotator only needs to label one point per object.
To leverage these extremely sparse labels in network training, we design a novel self-training approach, in which we iteratively conduct the training and label propagation, facilitated by a graph propagation module.
Also, we adopt a relation network to generate the per-category prototype to enhance the pseudo label quality and guide the iterative training. Besides, our model can be compatible to 3D instance segmentation equipped with a point-clustering strategy. 

Experimental results on both ScanNet-v2 and S3DIS show that our self-training approach, with extremely-sparse annotations, outperforms all existing weakly supervised methods for 3D semantic and instance segmentation by a large margin, and our results are also comparable to those of the fully supervised counterparts. Codes and models are available at {\scriptsize \url{https://github.com/liuzhengzhe/One-Thing-One-Click}}.  
\end{abstract}





\section{Introduction}


\begin{figure}
\centering
\includegraphics[width=0.99\columnwidth]{illustration.pdf}
\caption{Comparing our approach of ``One Thing One Click++'' and the fully supervised version of our method ``Fully-sup'' on 3D semantic and instance segmentation of ScanNet-v2. It is worth noting that all the works included in this comparison in the blue color were proposed recently in either 2022 or 2023. 
Our approach outperformed these very recent works by {\em training on data with only one label per object\/}.
Note the annotation percentages under each method in the charts.
%If ``One Thing Three Clicks'' (1T3C) is allowed, we can further raise our result. \xjqi{perhaps remove the comparison. only compare fully supervised and weakly supervised. add the amount of annotation reduction. add instance segmentation results}
}\label{fig:illustration}
\end{figure}



\begin{figure}
\centering
\includegraphics[width=0.99\columnwidth]{fig2.pdf}
\caption{We train our self-training approach using only our ``One Thing One Click'' annotations (top).
Yet, it can produce plausible semantic and instance segmentation results. } %close to the ground truth (bottom). \lzz{will add instance seg}}
 \label{fig:fig2}


\end{figure}





In this work, we will study exploiting weak annotation for 3D point cloud recognition to reduce the annotation burden, focusing on the most label-intensive tasks including semantic segmentation and instance segmentation. 
Specifically, we introduce ``One Thing One Click'' annotation method, so the annotator only needs to label one single point per object. In this proposal, we also aim to reduce the amount of necessary annotations on point clouds, but we propose to take the approach to an extreme by introducing ``One Thing One Click,'' so the annotator only needs to label one single point per object.
To further relieve the annotation burden, such a point can be randomly chosen, not necessarily at the center of the object.
On average, it takes less than 2 minutes to annotate a ScanNet-v2 scene with our ``One Thing One Click'' scheme (see an example annotation in Figure~\ref{fig:fig2} (b), which contains only 13 clicks), which is more than 10x faster compared with the original ScanNet-v2 annotation scheme.



However, directly training a network on extremely-sparse labels from our annotation scheme (less than 0.02\% in ScanNet-v2 and S3DIS) will easily make the network overfit the limited data and restrict its generalization ability.
Hence, it raises the question ``can we achieve a performance comparable with a fully supervised baseline given the extremely-sparse annotations?''

To meet such a challenge, we propose to design a self-training approach with a label-propagation mechanism for weakly supervised semantic segmentation.
%
On the one hand, with the prediction result of the model, the pseudo labels can be expanded to unknown regions through our graph propagation module.
On the other hand, with richer and higher quality labels being generated, the model performance can be further improved.
Thus, we conduct the label propagation and network training iteratively, forming a closed loop to boost the performance of each other.




A core problem of label propagation is how to measure the similarity among nodes, especially on irregular 3D point clouds. Previous works on 2D image recognition ~\cite{zheng2015conditional,chen2017deeplab,yuan2019structpool} build a graph model upon 2D pixels and measure the similarity with low-level image features, e.g., coordinates and colors.
On the contrary, we propose a graph model building upon the 3D geometric coherent super-voxels, which have more complex geometric structures and a variable number of points in each group. Hence, existing hand-craft features cannot fully reveal the similarity among nodes in our case.
%However, these hand-crafted features cannot fully reveal the similarity among features in graph nodes, since they are loosely correlated with the category. 
To resolve this problem, we further propose a relation network to leverage 3D geometrical information for similarity learning among the graph nodes in 3D. The geometrical similarity and learned similarity are integrated together to facilitate label propagation. To effectively train the relation network with the extremely-sparse and category-unbalanced data, we further propose to generate a category-wise prototype with a memory bank for better similarity measurement.

%The relation module generates an representation of each point cluster as the query representation, which is further compared with the maintained memory bank containing the category-wise descriptions. The relation module provides a similarity measure of the point clusters for better feature propagation through the graph. 

%Beyond the conference version One Thing One Click~\cite{liu2021one}, we extend our approach as follows. 
Further, our approach is ready for 3D instance segmentation with a point-clustering strategy.  
Leveraging the knowledge of the number and location of each instance provided by ``One Thing One Click'' annotation, point clustering aims to group the points of the same instance to generate instance-level pseudo label and enable the instance-level understanding. 
%Specifically, to generate instance-level pseudo labels, we conduct K-Means Clustering on the super voxels predicted as the same semantic category with the annotated ones as initial centroids, and filter out unconnected semantic segments in each cluster to derive the initial instance-level pseudo label. Afterwards, the above process and network training are conducted iteratively to further enhance the pseudo label quality and model performance. 


Experiments conducted on two public data sets ScanNet-v2 and S3DIS manifest the effectiveness of the proposed method.
With just around 0.02\% point annotations, our approach %surpasses all existing weakly supervised approaches (which employ far more labels) 
%for 3D point cloud segmentation by a large margin, and our approach even 
achieves results that are comparable with a fully supervised counterpart; see Figure~\ref{fig:illustration}. These results manifest the high efficiency of our ``One Thing One Click'' scheme for 3D point cloud annotation and the effectiveness of our self-training approach for weakly supervised 3D scene understanding.


This work extends our research work presented in ``One Thing One Click''~\cite{liu2021one}, which was presented at the 2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). First, we expand the self-training framework for weakly-supervised 3D instance segmentation. Besides, we provide an alternative design for the Relation Network, which achieves comparable performance to the original design in ``One Thing One Click'' but is more efficient. At last, we compare our approach with the latest methods on 3D semantic and instance segmentation, and the experimental results demonstrate the superiority of our new approach.
%This work is an extended version of One Thing One Click~\cite{liu2021one}, which is presented in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2021. In this extended version, we first extend our self-training framework for weakly-supervised 3D instance segmentation. Besides, we provide an alternative design of the Relation Network, which achieves comparable performance with the original design in One Thing One Click~\cite{liu2021one} but is more efficient. At last, we compare our approach with the latest works for 3D semantic segmentation and instance segmentation approaches, and the experimental results demonstrate the superiority of our approach. 






















\section{Related Work}

\paragraph{Semantic Segmentation for Point Cloud}
Approaches for 3D semantic segmentation can be roughly divided into point-based methods and voxel-based methods.
%

\textit{Point-based networks} take raw point clouds as input.  Along this line of works, PointNet~\cite{qi2017pointnet} and PointNet++~\cite{qi2017pointnet++} are the pioneering ones. %As the pioneer work in this line, PointNet~\cite{qi2017pointnet} proposed a multi-layer perception architecture for 3D semantic segmentation. To make better use of local contextual information, PointNet++~\cite{qi2017pointnet++} proposed a hierarchical architecture to learn point cloud features.
Afterward, convolution-based methods~\cite{li2018pointcnn,thomas2019kpconv,wu2019pointconv,boulch2020convpoint} were also proposed for 3D semantic segmentation on point clouds. Besides, Kundu~\etal~\cite{kundu2020virtual} proposed to fuse features from multiple 2D views for 3D semantic segmentation.
To aggregate together the geometrically-homogeneous points, Landrieu\etal~\cite{landrieu2018large} modeled a point cloud as a super point graph. In addition, a number of recent research works~\cite{zhao2021point,dong2022learning,wu2022point,lai2022stratified} are further proposed to improve the performance of point cloud semantic segmentation. Inspired by~\cite{landrieu2018large}, we expand the sparse labels to geometrically homogeneous super-voxels to generate initial pseudo labels for the first-iteration network training.
%We found that super point graph formulation in~\cite{landrieu2018large} is effective in weakly supervised scenario because the rare annotated seeds can be expanded within the super point. 

\textit{Voxel-based networks} take the regular voxel-grids as input instead of the raw data ~\cite{tchapmi2017segcloud,riegler2017octnet,graham2015sparse,su2018splatnet,dai20183dmv}. 
The recently-proposed methods SparseConv~\cite{graham2017submanifold}, MinkowskiNet~\etal~\cite{choy20194d}, and OccuSeg~\etal~\cite{han2020occuseg} are among the representative works in this branch. 
In this paper, we adopt the 3D-UNet architecture described in~\cite{graham2017submanifold} as the backbone architecture due to its high performance and applicability. 



\paragraph{Weakly Supervised 3D Semantic Segmentation} %Compared with fully supervised 3D semantic segmentation,
%\textcolor{blue}{Weakly supervised 3D semantic segmentation is relatively under-explored.}
%Mei~\etal~\cite{mei2019semantic} proposed a semi-supervised framework for point cloud segmentation. However, it still requires large amount amount of annotation. Guinard~\etal~\cite{guinard2017weakly} utilized a non-parametric model with CRF for segmentation. However, the absence of spatial relation among semantic labels limits its performance.  \lzz{do we need to mention the above two works? They also work on this problem, not closely related to ours. ~\cite{xu2020weakly} cited them, but the aaai 20 paper not. }
%\textcolor{blue}{After early works~\cite{mei2019semantic,guinard2017weakly} in this area, recently, weakly supervised 3D semantic segmentation has attracted more attention from both academia and industry. }
%Wei~\etal~\cite{wei2020multi} utilized the Class Activation Map to generate pseudo point-wise labels from sub-cloud-level annotations.
%The performance is, however, limited by the lack of localization information.
%Wang~\etal~\cite{wang2020weakly} back-projected 2D image annotations to 3D space to produce labels in point clouds.
%3D segmentation framework without 3D annotations.
Although there have been significant advancements in 3D semantic segmentation, the arduous task of point-level annotation greatly limits its practicality. To overcome this challenge, several approaches have been proposed~\cite{guinard2017weakly,mei2019semantic,wang2020weakly}. 
%Also, the visibility prediction branch adds to the complexity of the network.  %Also, the points appearing in multiple-views can be annotated for multiple times, which aggravates the annotation labour.
A recent work~\cite{wei2020multi} utilizes the Class Activation Map to generate pseudo point-wise labels from sub-cloud-level annotations. The performance is, however, limited by the lack of localization information.
In addition, Xu~\etal~\cite{xu2020weakly} achieves the performance close to fully supervised with less than 10\% labels.
However, they require the annotations to be uniformly-distributed in the point cloud, which is practically very hard for the annotators to follow. %, and their performance largely depends on the initial annotation because their network is barely trained on the initial label. The above restricts their applicability to train with the sparser label or the label following other distributions. 
Very recently, several of approaches~\cite{hou2021exploring,tian2022vibus,liu2022active,tang2022learning,dong2022rwseg,yu2022data,zhang2021perturbed,lee2023gaia,ren20213d,kweon2022joint,li2022hybridcr,zhang2021weakly,yang2022mil,deng2022superpoint,wu2022dual,wu2022pointmatch,hu2022sqn} are proposed to further enhance the annotation efficiency and performance of weakly supervised 3D semantic segmentation. %Besides, some approaches~\cite{} work on semi-supervised 3D setting, where only a small portion of data are fully-annotated while the remaining are without any annotation. }

In this works, we propose a new self-training approach with a label propagation module, in which the network training and label propagation are conducted iteratively.
Our approach largely reduces the reliance on the quality of the initial annotation and achieves top performances, compared with existing weakly supervised methods, while using only extremely-sparse annotations.
%In addition, we group the geometric homogeneous points as super-voxels and further 
%significantly reducing the reliance of the quality of the initial annotation. %In addition, we group the geometric homogeneous points as super-voxels and further explicitly learn the similarity among them, facilitating the label propagation and the further network training. 


%Nevertheless, the uniformly distributed annotation is hard to derive in reality, which restricts the applicability of the proposed method. 

% further expand the seeds to the geometrically homogeneous point clusters it belongs to and derive around 10\% initial annotated points with negligible expense. 


\paragraph{3D Instance Segmentation}
3D instance segmentation aims to derive instance-level understanding of a 3D scene, going beyond semantic segmentation. While existing works~\cite{hou20193d, yang2019learning, dong2022learning, schult2022mask3d} use point-level annotations for 3D instance segmentation, the laborious and tedious annotation process limits their practical applicability. Recent works~\cite{hou2021exploring, tao2022seggroup, chu2022twist, tang2022learning} focus on weakly-supervised 3D instance segmentation to overcome this challenge. For example, Seg-Group~\cite{tao2022seggroup} proposes a segment grouping network to hierarchically group unlabeled segments into nearby labeled ones for 3D instance segmentation. Tang et al.~\cite{tang2022learning} use semantic and spatial relations to adaptively learn inter-superpoint affinity. TWIST~\cite{chu2022twist} proposes a semi-supervised 3D instance segmentation approach that leverages object-level information to denoise pseudo labels.
%
In this work, we extend our self-training approach to 3D instance segmentation using the One-Thing-One-Click annotation.
%3D instance segmentation goes beyond semantic segmentation in that it further aims to derive instance-level understanding of a 3D scene. Existing works~\cite{hou20193d, yang2019learning, dong2022learning, schult2022mask3d} use point-level annotations for 3D instance segmentation; yet, the laborious and tedious annotating process limits their practical applicability. To overcome this challenge, recent works~\cite{hou2021exploring, tao2022seggroup, chu2022twist, tang2022learning} focus on weakly-supervised 3D instance segmentation. For instance, Seg-Group~\cite{tao2022seggroup} proposes a segment grouping network to hierarchically group unlabeled segments into nearby labeled ones for 3D instance segmentation, and \cite{tang2022learning} uses semantic and spatial relations to adaptively learn inter-superpoint affinity. TWIST~\cite{chu2022twist} proposes a semi-supervised 3D instance segmentation approach that leverages object-level information to denoise pseudo labels. 
%In this work, we extend our self-training approach to 3D instance segmentation with the One-Thing-One-Click annotation. 

%Going beyond semantic segmentation that aims to assign a semantic label to each point, 3D instance segmentation further aims to derive the instance-level understanding on the 3D scene. Existing works~\cite{hou20193d,yang2019learning,dong2022learning,schult2022mask3d} utilize the point-level annotations for 3D instance segmentation. However, similar to the 3D semantic segmentation case, the tedious and laborious data annotation again severely limits their applicability to the practical usage. To address this issue, some recent works~\cite{hou2021exploring,tao2022seggroup,chu2022twist,tang2022learning} focus on weakly-supervised 3D instance segmentation to eliminate the annotation burden. Specifically, Seg-Group~\cite{tao2022seggroup} design a segment grouping network to hierarchically group the unlabeled segments into the nearby labeled ones for 3D instance segmentation, and \cite{tang2022learning} leverages the semantic and spatial relations to adaptively learn inter-superpoint affinity. TWIST~\cite{chu2022twist} proposes to denoise pseudo labels by leverages the object-level information for semi-supervised 3D instance segmentation.


\paragraph{Self-Training for Semantic Segmentation on 2D Images}
Self-training for weakly supervised 2D image understanding has been intensively explored.
To reduce the annotation burden for 2D images, researchers proposed a variety of annotation approaches,~\eg, image-level categories~\cite{qi2016augmented,oh2017exploiting,zhou2018weakly,ahn2018learning}, points~\cite{bearman2016s,laradji2018blobs}, extreme points~\cite{maninis2018deep,papadopoulos2017extreme},  scribbles~\cite{lin2016scribblesup,wang2019boundary,zhang2020weakly}, bounding boxes~\cite{dai2015boxsup}, etc. With the weak supervision, a self-training approach can learn to expand the limited annotations to unknown regions in the domain. Inspired by the previous works in 2D image understanding, we propose a novel self-training framework for weakly supervised 3D scene understanding.
%As far as we know, this is the first work that explores self training for weakly supervised 3D semantic segmentation.




\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{overview.pdf}
\caption{Overview of our framework.
Through a super-voxel partition (b), we expand our ``One Thing One Click'' annotations (c) to generate the initial pseudo labels (d) for guiding the update of the pseudo labels (g).
On the other hand, we adopt the ``3D U-Net'' for semantic label prediction (blue region) and design the ``Relation Net'' for super-voxel-based similarity learning (green region).
Then, we incorporate a super-voxel pooling (e) to aggregate features from the two networks. Afterwards, we adopt either a graph model or a transformer (f) to propagate labels over the point cloud.
%to produce unary and pairwise terms as the outputs, respectively, 
Further, we iteratively update the predicted labels (g) and train the network.}
%with confidence above a threshold $T$ are regarded as pseudo labels for training in the next iteration.
%$C$ is the number of categories, $D$ is the number of the feature dimension, $N$ is the number of points, and $M$ is the number of super-voxels.
\label{fig:overview}


\end{figure*}

\section{Methodology}

\subsection{Overview}

With ``One Thing One Click,'' we only need to annotate a point cloud with one point per object, as Figure~\ref{fig:overview} (c) shows, and these points can be chosen at random to alleviate the annotation burden.  
%
Procedure-wise, given such sparse annotations, we first over-segment the point cloud $X=\{p_i\}$ into geometrically homogeneous super-voxels $V=\{v_j\}$, where $\cup_j v_j=X$ and $v_j \cap v_{j'}=\emptyset$ for $v_j \neq v_{j'}$.
Note that throughout the paper, we use $i$ and $j$ as the indices for points and super-voxels, respectively.
%
Based on the super-voxel partition, we can produce initial pseudo labels of the point cloud by spreading each label to all the points locally in the super-voxel that contains the annotated point.
%the super 
%, and propagate the annotated point to the whole super-voxel to raise the number of annotated points.
However, as Figure~\ref{fig:overview} (d) shows, the labels are still very sparse. More importantly, the propagated labels distribute mainly around the initially-annotated points, which are far from the ideal uniform distribution for weakly semantic segmentation, as employed in ~\cite{xu2020weakly}.

%To make it feasible to train 3D semantic segmentation network with the above data, we propose a self-training approach composed of two sub-modules: 1). A 3D semantic segmentation network for the point-wise semantic prediction. 2). A graph propagation module including a relation network to propagate the label among super-voxels and expand the label to unknown regions. These two tasks are dependent with each other and forms a closed loop to boost the performance of each other. 

An important insight in our approach is to iteratively propagate the sparse annotations to unknown regions in the point cloud, while training the network model to guide the propagation process.  To achieve this, we adopt the 3D semantic segmentation network $\Theta$ (the blue regions in Figure~\ref{fig:overview}) and to learn the label propagation via a feature propagation module (Figure~\ref{fig:overview} (f)). Further, we design the relation network $\mathcal{R}$ (the green regions in Figure~\ref{fig:overview}) to explicitly model the feature similarity. Afterward, predictions with high confidence are further employed as the updated pseudo labels for training the network in the next iteration (Figure~\ref{fig:overview} (g)).
This iterative self-training approach couples the label propagation and network training, enabling us to significantly enhance the segmentation quality, as revealed earlier in Figure~\ref{fig:illustration}.
%\phil{feel free to revise... I changed it, so make sure you revise it and see if there's any problem}

%As shown in Figure~\ref{fig:overview}, we use a 3D U-Net~\cite{graham2017submanifold} to predict the semantic category for the point cloud (the blue regions in Figure~\ref{fig:overview}), and a parallel Relation Network to generate an category-related embedding for each point and then model the similarities of the over-segmented super-voxels (the orange regions in Figure~\ref{fig:overview}). With the super-voxel level pooled prediction and relation embedding, the prediction of 3D U-Net is further propagated with a graphic model (Figure~\ref{fig:overview} f) to derive the pseudo label for the next iteration training.

%In this section, we first present our 3D semantic segmentation network for point-wise semantic prediction (Section~\ref{sec:unary}), then our feature propagation mechanism and the relation network for similarity learning (Section~\ref{sec:pairwise}). Afterward, we describe the self-training approach that evolves the above modules alternatively (Section~\ref{sec:self-training}). 


%\begin{figure}
%\centering
%\includegraphics[width=0.45\textwidth]{figures/dataexample.PNG}
%\caption{Example of our annotation and super-voxel based label expansion. } %\label{fig:data}
%\end{figure}
%\vspace*{-2.5mm}
\subsection{3D Semantic Segmentation Network}\label{sec:unary}
We adopt the 3D U-Net architecture~\cite{graham2017submanifold} as the backbone, denoted as $\Theta$. 
Its input is point cloud $X$ of $N$ points (Figure~\ref{fig:overview} (a)).
Each point has 3D coordinates $p_i$ and color $c_i$, where $i\in\{1, ..., N\}$. The network predicts the probability of each semantic category $P(y_{i,\bar{c}}|p_i,c_i,\Theta)$ of each point $p_i$, where $\bar{c}$ is the ground truth category of point $p_i$. The network is trained with the softmax cross-entropy loss below:
%as Equation~\eqref{equ:seg_loss}:
%\phil{$y_i$ has two meanings in the paragraph above...}
%



\begin{equation}
L_{s} = −\frac{1}{N}\sum^N_{i=1}{\log P({y_{i,\bar{c}}|p_i,c_i,\Theta})}
\label{equ:seg_loss}
\end{equation}



\noindent
In the first iteration, the network is trained with the initial pseudo labels, as shown in Figure~\ref{fig:overview} (d).
In subsequent iterations, the network is trained with the updated pseudo labels, as shown in Figure~\ref{fig:overview} (g), which will be detailed below.


\subsection{Pseudo Label Propagation}\label{sec:pairwise}

To facilitate the network training, we propose a label propagation mechanism to effectively propagate labels to unknown regions. Specifically, we provide two options to propagate the feature, \ie, graph-model-based and transformer based, as shown in Figure~\ref{fig:relation}. We also propose the relation network to explicitly learn the similarity among the super-voxels to facilitate the label propagation process and complement the 3D U-Net. 

\paragraph{Graph Model-Based Feature Propagation}

First, we introduce our graph model-based feature propagation. 
To start, we leverage the 3D geometrically homogeneous super-voxels to build a graph.
Compared with building on points, our graph has significant fewer nodes to facilitate efficient label propagation.


To derive the prediction $P(y_{j,c}|v_j,\Theta)$ of the $j$-th super-voxel,
we apply a super-voxel pooling to aggregate the semantic prediction of the $n_j$ points in $v_j$. %as below:
%\begin{equation}
%\label{equ:average}
%P(y_{j,c}|v_j,\Theta) = \frac{1}{n_j} \sum_{i} %P({y_{i,c}|p_i,c_i,\Theta}), \ \text{where} \ p_i \in v_j,
%\end{equation}
%where 
%$P({y_{i,c}|p_i,c_i,\Theta})$ is the probability of $p_i$ in class $c$.


\begin{figure}
\centering
\includegraphics[width=0.99\columnwidth]{relation.pdf}
\caption{The alternative approaches for label propagation. }
\label{fig:relation}
\end{figure}



\paragraph{Graph-based Label Propagation}

The architecture of the graph-based label propagation is illustrated in Figure~\ref{fig:relation} (a).
To build the graph, we treat each super-voxel as a graph node and compute the similarity between each pair of super-voxels $v_j,v_{j'}$, which is represented as an edge.


To propagate labels to unknown regions through the graph, we formulate it as an optimization problem that considers both the network prediction and similarities among the super-voxels to achieve the global optimum with the energy function below similar to Conditional Random Field (CRF).
%
\begin{equation}
\label{equ:energy}
E(Y|V) = \sum_{j} \psi_u (y_{j}|V,\Theta) + \sum_{j<{j'}} \psi_p (y_{j},y_{j'} | V,\mathcal{R},\Theta )
\end{equation}
%
where $\mathcal{R}$ is the relation network to be detailed later.
%
The unary term $\psi_u (y_{j}|V,\Theta)$ represents the super-voxel pooled prediction of the 3D U-Net $P(y_j)$ on super-voxel $v_j$. Specifically, it denotes the minus $\log$ probability of predicting super-voxel $v_j$ to have label $y_{j}$. % where $y_{v,j}=max(y_{v,j,c})$ across all the categories $c$. 
We define it as below. 
%
\begin{equation}
\label{equ:unary_loss}
 \psi_u (y_{j}|V,\Theta) = -\log P({y_{j}|V,\Theta})
\end{equation}



The pairwise term $\psi_p (j_k)$ in Equation~\ref{equ:energy} represents the similarity between super-voxels $v_j$ and $v_{j'}$. We employ both the low-level features and learned features for measuring the similarity, as shown in Equation~\ref{equ:pairwise} below:
%

\begin{equation}
\begin{aligned}
\label{equ:pairwise}
\psi_p (y_j,y_{j'} | V)= & \mathds{1} (y_j , y_{j'}) \exp\{- \lambda_c \frac{ \left\Vert c_j-c_{j'} \right\Vert^2}{2\sigma^2_c} \\
- \lambda_p \frac{  \left\Vert p_j-p_{j'} \right\Vert^2}{2\sigma^2_p} 
& - \lambda_u \frac{ \left\Vert u_j-u_{j'} \right\Vert^2}{2\sigma^2_u} 
- \lambda_f \frac{ \left\Vert f_j-f_{j'} \right\Vert^2}{2\sigma^2_f} 
\}
\end{aligned}
\end{equation}
%
where $\mathds{1} (y_j, y_{j'})$ is 1, if $v_j$ and $v_{j'}$ have different predicted labels, and 0 otherwise. The pairwise term means that the cost will be higher if super-voxels with similar features are predicted to be different classes.
Here, $c_j,c_{j'}$, $p_j,p_{j'}$ and $u_j,u_{j'}$ are the normalized mean color, mean coordinates and mean 3D U-Net feature, respectively, of super-voxels $v_j$ and $v_{j'}$.



\paragraph{Relation Network}

Unlike existing works~\cite{zheng2015conditional,chen2017deeplab,yuan2019structpool}, which build the graph on 2D image pixels, we build our graph on 3D super-voxels, which have irregular and complex geometrical structures. Therefore, hand-crafted features $p_j,p_{j'}$ and $c_j,c_{j'}$ have inferior capability to measure the similarity between super-voxels. To address this issue, we propose the \textit{Relation Network} to better leverage the 3D geometrical information and explicitly learn the similarity among super-voxels.


%\vspace*{-2.5mm}
%\paragraph{Relation Network}

%Existing works Co-Training~\cite{qiao2018deep} and Tri-net~\cite{dong2018tri} showed that semi-supervised training benefits from having two complementary tasks or components. In our framework, we propose a relation net to complement the 3D U-Net.

The relation network $\mathcal{R}$ shares the same backbone architecture as the 3D U-Net $\Theta$ except for removing the last category-wise prediction layer. It aims to predict a category-related embedding $f_j$ for each super-voxel $v_j$ as the similarity measurement. $f_j$ is the per super-voxel pooled feature in $\mathcal{R}$. In other words, the relation network groups the embeddings of same category together, while pushing those of different categories apart. To this end, we propose to learn a prototypical embedding for each category. %, inspired by the Prototypical Network~\cite{snell2017prototypical}.


%However, different from~\cite{snell2017prototypical}, the training data is categorically unbalanced in our case.  Directly employing~\cite{snell2017prototypical} would cause poor network performance on categories with relative small number of samples. Another option is to randomly sample a category-balanced subset in each iteration for the training. However, the mean embedding derived in this way may deviate from the actual categorical center, preventing the network from full convergence.

%However, the per-category prototypes in~\cite{snell2017prototypical} are fully determined by the sampled mini-batch, and may deviate from the actual categorical center. Consequently, they may not be stable and could keep changing during the training, thereby hard to converge. 
To assist the training of the relation network with sparse and unbalanced training data, we present a memory bank $K=\{k\}$ to generate one categorical prototype for each category, instead of simply regarding the average embedding as the prototype as in~\cite{snell2017prototypical}. 

The embedding $f_j$ generated by $\mathcal{R}$ serves as a ``query,'' and we compare it with the corresponding ``key'' $k_c$ in the memory bank with a dot product.
The two modules are optimized simultaneously with contrastive learning~\cite{oord2018representation} as below.


\begin{equation}
\label{equ:contrastive}
L_{c} =\frac{1}{M}\sum^{M}_{j} {(-\log{   \frac{f_j \cdot k_{\bar{c}}/\tau}{\sum_c f_j \cdot k_c/\tau}         })},
\end{equation}


\noindent
where $\tau$ is a temperature hyper parameter~\cite{wu2018unsupervised} and $\bar{c}$ is the ground truth category of $v_j$. The contrastive learning is equivalent to a c-way softmax classification task. %, and the softmax of the query and key representations serve as the probability. 

Following~\cite{he2020momentum}, we update the key representations via a moving average with momentum as shown below
\begin{equation}
\label{equ:momentum}
k_{\bar{c}} \xleftarrow{} m k_{\bar{c}} +(1-m) f_j,
\end{equation}
where $m$ is a momentum coefficient to control the evolving speed of the memory bank. %On the one hand, the representations in the memory bank are initialized with random vectors, and are updated during training to generate the prototype for each category. On the other hand, the embeddings generated from the relation network are grouped towards the prototype of its category. In this way, the relation network generates similar embeddings for the same category and distinct ones for different categories. The memory bank updates the prototypes in a category-balanced manner by randomly sampling the same number of training samples $s$ per category in every forward pass. 



Our relation net complements with 3D U-Net. It measures the relations between super-voxels using different training strategies and losses, while 3D U-Net aims to project the inputs into the latent feature space for category assignment. The prediction of relation network is further combined with the prediction of 3D U-Net by multiplying the predicted possibilities of each category to boost the performance. In addition, the relation net offers a stronger measurement of the pairwise term in CRF vs. handcrafted features like colors and also complements with the 3D U-Net features. 




\paragraph{Transformer-Based Label Propagation}
In the following, we introduce a transformer-based alternative to the graph model-based approach for label propagation, as illustrated in Figure~\ref{fig:relation} (b). Unlike the graph model-based approach that learns the affinity among super voxels, where the size of the affinity matrix $M\times M$ grows quadratically relative to the number of super voxels $M$, the transformer-based label propagation aims to learn the correlation between a super voxel $v_j$ and a category prototype $k_c$. Therefore, the size of the attention map $M\times c$ grows proportionally to $M$, significantly improving efficiency in terms of memory and inference time. Additionally, transformer-based label propagation can be optimized end-to-end, further improving the performance of 3D semantic segmentation.
%In the following, we introduce transformer-based alternative of the graph model-based approach for label propagation, as illustrated in Figure~\ref{fig:relation} (b). Different from the graph model-based approach that learns the affinity among super voxels where the size of affinity matrix $m\times m$ grows quadratically relative to the number of super voxels $m$, the transformer-based label propagation aims to learn the correlation of the super voxel $v_j$ and the category prototype $k_c$. Therefore, the size of the attention map $m\times c$ grows proportionally to $m$, improving the efficiency in terms of memory and inference time by a large margin. In addition, transformer-based label propagation can be optimized end-to-end to further improve the performance f 3D semantic segmentation. 
%The transformer-based label propagation is illustrated in Figure~\ref{fig:relation} (b). Different from the graph model-based approach that learn the affinity among super voxels where the size of affinity matrix $m\times m$ grows quadratically relative to the number of super voxels $m$, the transformer-based label propagation aims to learn the correlation of the super voxel $v_j$ and the category prototype $k_c$. Therefore, the size of the attention map $m\times c$ grows proportionally to $m$, improving the efficiency in terms of memory and inference time. In addition, transformer-based label propagation can be optimized end-to-end to further improve the performance of 3D semantic segmentation. 



Specifically, the transformer-based label propagation can be formulated as follows. 
\begin{equation} 
\hat{f}_{j}=\Sigma_c softmax(\frac{Q(F_j) K(k_c)}{\sqrt{d_l}})V(k_c),
\label{equ:transformer}
\end{equation}
where $Q$, $K$, and $V$ represent MLP layers, while $F_j$ represents the feature vector of the 3D U-Net. The transformer then aggregates the category prototype $k_c$ based on the similarity between $F_j$ and $k_c$. The resulting output feature $\hat{f}_{j}$ is then concatenated with $F_j$ to make the final prediction for the semantic category.
%where $Q,K,V$ means MLP layers and $F_j$ means the feature vector of 3D U-Net. The transformer aggregates the category prototype $k_c$ based on the similarity of $F_j$ and $k_c$, then the output feature $\hat{f}_{j}$ is further concatenated with $F_j$ for the final prediction on the semantic category. 



Inspired by Mean Teacher~\cite{tarvainen2017mean}, we update the weights of the Relation Network in our transformer-based label propagation using the moving average of weights in the 3D U-Net with momentum, instead of using stochastic gradient descent (SGD). %instead of using stochastic gradient descent (SGD), we update the weights of the Relation Network using the moving average of weights in 3D U-Net with momentum in our transformer-based label propagation,
The weight update is formulated as below.  
%Inspired by Mean Teacher~\cite{tarvainen2017mean}, the Relation Network is updated via moving average of weights in 3D U-Net with momentum, rather than SGD in our transformer-based label propagation, as shown in Equation~\ref{equ:momentum2}. 
\begin{equation}
\label{equ:momentum2}
\mathcal{R}_{w} \xleftarrow{} m \mathcal{R}_{w} +(1-m) \Theta_w,
\end{equation}
where $\mathcal{R}{w}$ and $\Theta{w}$ represent the $w$-th weight of the Relation Network and the 3D U-Net, respectively. By using the moving-average strategy, we accumulate the features $F_j$ in the 3D U-Net over time, which improves the quality and stability of the category prototypes $K$. Moreover, this approach helps reduce the computational complexity during training. %$\mathcal{R}_{w}$ and $\Theta_{w}$ indicate the $w$-th weight of Relation Network and 3D U-Net respectively. The moving-average strategy allows us to accumulate the features $F_j$ in 3D U-Net over time, improving the quality and stability of the category prototypes $K$. Further, it helps to reduce the computational complexity in training. }




\subsection{Self-Training}\label{sec:self-training}

With the label propagation, we then propose a self-training approach to update networks $\Theta$ and $\mathcal{R}$, and also the pseudo labels $Y$ iteratively. %, as Algorithm~\ref{selftrain} outlines.
The self-training is started by the ``One Thing One Click'' annotations and the pre-constructed super-voxel graph. In each iteration, we fix network parameters $\Theta,\mathcal{R}$ and update label $Y$, and vice versa. There are two steps in each iteration.
%
\begin{itemize}
%

\item
With $\Theta$ and $\mathcal{R}$ fixed,
the label propagation is conducted to minimize the energy function in Equation~\ref{equ:energy}.  Then, the predictions with high confidence are taken as the updated pseudo labels for training the two networks in the next iteration.
The confidence of super-voxel $v_j$, denoted as $C_j$, is the average of the minus $\log$ probability of all $n_j$ points in $v_j$ after the label propagation:  
%

\begin{equation}
\label{equ:momentum3}
C_j = \frac{1}{n_j} \sum_i^{n_j}{ \log P(y_i| p_i, V, \Theta, \mathcal{R}, G)}, \ \text{where} \ p_i \in v_j,
\end{equation}


\noindent
where $G$ denotes the graph propagation. 
%

\item
With pseudo labels $Y$, $\Theta$ and $\mathcal{R}$ are optimized respectively. 
%
\end{itemize}




\subsection{3D Instance Segmentation}\label{sec:instance}


%This prediction head consists of a multi-layer perceptron (MLP). %In the first training iteration, it is trained using a mean squared error (MSE) loss between the predicted offsets and the offsets calculated from ground truth instances obtained from a validation set or a small amount of labeled data. 
%To extend our self-training framework for 3D instance segmentation, we additionally incorporate a prediction head following~\cite{jiang2020pointgroup} to predict the 3D offset for shifting super voxel $v_j$ towards the centroid coordinate $C$ of the associated instance  that $v_j$ belongs to. Note that our One Thing One Click annotation strategy does not provide the ground truth offset $o_j = C − p_j$. 



To extend our self-training framework for 3D instance segmentation, we propose a point-clustering strategy to iteratively generate instance-level pseudo label and train the instance segmentation network. 


In the first training iteration, we utilize the semantic segmentation network trained with the One Thing One Click annotation approach as described earlier, as shown in Figure~\ref{fig:instance_process} (b). Next, we conduct K-Means Clustering using the annotated super voxels $V_{anno}=\{v_k\}$ as initial centroids. Since our One-Thing-One-Click annotation approach enables each annotated super-voxel to represent an instance, we can predict which instance $k$ the super voxel $v_j$ belongs to based on the Euclidean distance $||p_k-p_j||_2^2$; see Figure~\ref{fig:instance_process} (c). To enhance the robustness of the generated pseudo label, we then filter out small and unconnected semantic segments to obtain the initial instance-level pseudo label (Figure~\ref{fig:instance_process} (d)). 



Then we train the instance segmentation network leveraging the above pseudo label. To extend the 3D U-Net architecture for instance-level segmentation, we incorporate a multi-layer perceptron (MLP) prediction head on the top of 3D U-Net following~\cite{jiang2020pointgroup} to predict the 3D offset $o_j$. With the predicted offset, we can move a super voxel $v_j$ towards the centroid coordinate $C$ of the instance that $v_j$ belongs to, such that the super voxel $v_j$ belonging to the same instance can be grouped together and the point-level clustering in the following iterations can be conducted based on the Euclidean distance $||p_k-(p_j+o_j)||_2^2$. 
%Since our One-Thing-One-Click annotation approach lacks the ground truth offset $o_j = C - p_j$, we use the predicted offset $o_j'$ as a pseudo label to cluster the unannotated super voxels. 
In the first training iteration, we freeze the backbone network and only update the offset head. This helps the network maintain its ability to perform semantic segmentation. 


In the subsequent iterations, we further update the pseudo instance-level label using K-Means Clustering based on $v_j$'s updated coordinate $p_j+o_j$. Then we fine-tune the entire network end-to-end instead of only updating the offset head like the first iteration, so we can further improve the instance segmentation performance.


We repeat the above process and network training iteratively to progressively improve the quality of the pseudo labels and enhance the performance of the model, as illustrated in Figure~\ref{fig:instance_process} (e). 


%To begin with, we adopt the semantic segmentation network with One Thing One Click annotation using our self-training strategy introduced in the above sections (Figure~\ref{fig:instance_process} (b)). Then, we perform K-Means Clustering with the annotated super voxels $V^a=\{v^a_k\}$ (with coordinate $p^a_k$) as the initial centroids. Thanks to our One-Thing-One-Click annotation strategy, each annotated super-voxel can represent an instance. In the clustering process, we predict which instance $k$ the super voxel $v_j$ belong to according to the Euclidean distance $||p_a-p_j||_2^2$ (Figure~\ref{fig:instance_process} (c)). Afterwards, we filter out unconnected semantic segments in each cluster to derive the initial instance-level pseudo label (Figure~\ref{fig:instance_process} (d)). 
%The above process and network training are conducted iteratively to further enhance the pseudo instance-level label quality and further improve model performance (Figure~\ref{fig:instance_process} (e)). 



%For the network training, we follow the training details in PointGroup~\cite{jiang2020pointgroup} to train the instance segmentation network with pseudo label. 
%Notably, in the first training iteration, we freeze the backbone network and only update the offset head. This ensures that the network retains its capability for semantic segmentation. In the remaining training iterations, we fine-tune the whole network end-to-end to further enhance the instance segmentation performance. 



During inference, we use the clustering method from~\cite{jiang2020pointgroup} to group super voxels $v_j$ into candidate clusters based on their predicted shifted coordinates $p_j+o_j$ (P branch in~\cite{jiang2020pointgroup}) instead of their original coordinates $p_j$ (Q branch in~\cite{jiang2020pointgroup}). Additionally, for semantic prediction, we simplify the approach by averaging the predicted semantic scores of all points belonging to each instance, as proposed in~\cite{hou2021exploring}, instead of using the ScoreNet introduced in~\cite{jiang2020pointgroup}.
%In inference, we adopt a clustering method~\cite{jiang2020pointgroup} to group points into candidate clusters according to their shifted coordinate predicted by the offset head (P branch in~\cite{jiang2020pointgroup}), instead of using their original coordinates (Q branch in~\cite{jiang2020pointgroup}). Additionally, we adopt a simplified approach of averaging the predicted semantic scores of all points belonging to each instance, as proposed in~\cite{hou2021exploring}, instead of using the ScoreNet introduced in~\cite{jiang2020pointgroup} for semantic prediction.


\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{instance_process.pdf}
\caption{Pseudo label generation for 3D instance segmentation. (a) Input point cloud. (b) Our semantic segmentation result. (c) Initial pseudo label by K-Means clustering. (d) Unconnected-segment removal. (e) Results after self-training. (f) Ground truth. }
\label{fig:instance_process}
\end{figure*}



\section{Experiments}

\paragraph{Datasets}

Our experiments are conducted on two large 3D semantic segmentation datasets -- ScanNet-v2~\cite{dai2017scannet} and S3DIS~\cite{armeni2017joint}.  \textbf{ScanNet-v2}~\cite{dai2017scannet} contains $1513$ 3D scans of 20 semantic categories. We annotate the official training set with our ``One Thing One Click'' scheme, and evaluate on the validation and test set. % To compare with fully-supervised counterparts and existing weakly supervised approaches, we report results on the test set. To compare with our baselines and conduct the ablation studies, we report the results on validation sets.
\textbf{S3DIS}~\cite{armeni2017joint} contains 3D scans of $271$ rooms containing $13$ categories. We follow the official train/validation split to annotate on Area 1,2,3,4,6 and report the performance on Area 5. 

\paragraph{``One Thing One Click'' Annotation Details}
%We implement the annotator based on SSTK~\cite{dai2017scannet}. 
In order to ensure the randomness of point selection in annotation, we simulate the annotation procedure by selecting a single point inside an object with the same probability for the following experiments.
In ScanNet-v2, only 19.74 points per scene are annotated on average with ``One Thing One Click'' scheme, %and the seeds are further extended to 12307.49 points with super-voxels
while this number in the original ScanNet-v2 is 108875.9. In S3DIS, only 36.15 points in each room are annotated on average using ``One Thing One Click'', while the original S3DIS has 193797.1 points annotated in each room. 

\paragraph{Implementation Details}
We implement all the modules of our self-training framework including the mean-field solver~\cite{koller2009probabilistic} for label propagation with the PyTorch~\cite{NEURIPS2019_9015} framework based on the implementation of~\cite{jiang2020pointgroup}. %All models and baselines are trained with Adam optimizer. The batch size is $4$, and the initial learning rate is $0.01$. %For ScanNet-v2, we train models for $512$ epochs in the first iteration, and fine-tune the latest model for $256$ epochs in the following iterations. For S3DIS, models are trained for $1536$ epochs in the first epoch and fine-tuned for $768$ epochs in the later iterations.  
Following~\cite{jiang2020pointgroup}, due to the GPU capacity, we randomly choose 250k points if the scene contains more points in training. In inference, the network takes the whole scene as input. We set the hyper-parameters $D=32$, $T=0.9$, $s=20$, $\tau=0.07$, $m=0.9$,
%\phil{updated m from 0.1 to 0.9}
$\sigma_c=\sigma_p=\sigma_u=\sigma_f=1$,  $\lambda_c=\lambda_p=\lambda_u=\lambda_f=1$ with a small validation set.  
We found that the self-training converges after five iterations. After that, more iterations training only brings very minor improvements. 


\paragraph{Super-voxel partition}


We use the mesh segment results~\cite{dai2017scannet} as super-voxels for ScanNet-v2, and the geometrical partition results described in~\cite{landrieu2018large} for S3DIS super-voxel partition. Our super-voxel partition effectively groups points based on their geometric attributes, as shown in Figure~\ref{fig:supervoxels}.

%Figure~\ref{fig:supervoxels} show the irregular geometrical structures and complex shapes, hand-crafted features like colors and coordinates cannot fully describe their properties. To this end, we propose a relation network to learn the high-level similarities among them. 

\begin{figure}
\centering
\includegraphics[width=0.99\columnwidth]{supervoxel.pdf}
\caption{Visualization of super-voxel partition on (a) ScanNet-v2 and (b) S3DIS. } \label{fig:supervoxels}
\end{figure}




%%\IncMargin{1em}
%\begin{algorithm}[!t]
%    \SetAlgoNoLine 
%    \SetKwInOut{Input}%{\textbf{Input}}\SetKwInOut{Output}{\textbf{Output}} 
%    \Input{``One Thing One Click'' annotations $Y_0=\{p_i\}$;\\
%        super-voxel partition $V=\{v_j\}$\;\\}
%    \Output{
%        semantic prediction for all points Y$\;$\\}
%    \BlankLine
%    Expand the annotated points $p_i$ to the super-voxel $v_j$ if $p_i$ $\in$ $v_j$; \\
%    \Repeat
%        {\text{convergence}}
%        {Train 3D U-Net $\Theta$ with pseudo labels $Y_t$; \\
%       Train relation network $\mathcal{R}$ with pseudo labels $Y_t$; \\
%        Combine the predictions and propagate the label with the graph model; \\
%        Update the pseudo labels $Y_{t}$ to $Y_{t+1}$ with the predictions of high confidence. 
%        }
%    \caption{Our self-training approach.\label{selftrain}}
%\end{algorithm}
%\DecMargin{1em}



%\vspace*{-0.05in}

\subsection{Semantic Segmentation on ScanNet-v2}\label{sec:scannet}

\paragraph{Comparing with Existing Methods}

Table~\ref{tab:existing} reports the benchmark result on ScanNet-v2 test set. 
The baselines can be roughly divided into two branches.
(i) Fully supervised approaches with 100\% supervision, including several representative works in 3D semantic segmentation.
These methods are the upper bounds of weakly supervised ones.
(ii) Weakly- and semi-supervised approaches. 

With less than 0.02\% annotated points, our result (69.3\% mIoU) outperforms many existing works with full supervision. 
As for weakly- and semi-supervised approaches, our approach outperforms all those very recent ones proposed from 2021 to 2023 with fewer annotations, demonstrating the superiority of our approach over the existing ones. 

%MPRM~\cite{wei2020multi} is trained with scene-level or subcloud-level labels. The scene-level annotation leads to an inferior performance of 24.4\%, and the subcloud-level annotation takes around 3 minutes per scene as reported in~\cite{wei2020multi}, which is longer than our ``One Thing One Click'' scheme (2 minutes). More importantly, our result outperforms~\cite{wei2020multi} by more than 26\% mIoU. 


\begin{table}
\centering
\scalebox{0.85}{
  \begin{tabular}{c|cc}
    \toprule
    Method & Supervision & mIoU (\%)  \\
    \midrule
    Pointnet++~\cite{qi2017pointnet++} & 100\% &33.9 \\
    SPLATNet~\cite{su2018splatnet}& 100\% & 39.3\\
    TangentConv~\cite{tatarchenko2018tangent} & 100\% &43.8\\
    PointCNN~\cite{li2018pointcnn} & 100\% & 45.8\\
    FPConv~\cite{lin2020fpconv} & 100\% & 63.9\\
    DCM-Net~\cite{schult2020dualconvmesh}&100\%& 65.8 \\
    PointConv~\cite{wu2019pointconv} &100\%& 66.6 \\
    KPConv~\cite{thomas2019kpconv} & 100\% &68.4\\
    JSENet~\cite{hu2020jsenet}& 100\% &69.9 \\
    SubSparseCNN~\cite{graham2017submanifold} & 100\% & 72.5\\
    MinkowskiNet~\cite{choy20194d} &100\% & 73.6 \\
    Virtual MVFusion~\cite{kundu2020virtual} &100\%+2D & 74.6\\
    PointTransformer-v2~\cite{wu2022point} & 100\% & 75.2\\
    Mix3D~\cite{nekrasov2021mix3d} &100\%+2D & 78.1 \\
    \midrule
    Our fully-sup baseline & 100\% & 72.5 \\
    \midrule
    Superpoint-guided~\cite{deng2022superpoint} & 10\% scenes & 52.4\\
    TWIST~\cite{chu2022twist}  & 10\% scenes& 61.1\\ 
    2D Konwledge Transfer~\cite{yu2022data} & 10\% scenes & 61.2 \\
    \midrule
    MPRM~\cite{wei2020multi} & scene-level & 24.4 \\    
    MPRM~\cite{wei2020multi} & subcloud-level & 41.1 \\  
    MPRM+CRF~\cite{wei2020multi} & subcloud-level & 43.2 \\
    WyPR~\cite{ren20213d} & scene-level & 24.0\\
    Zhang~\etal~\cite{zhang2021weakly} & 10.0\% & 52.0   \\
    PSD~\cite{zhang2021perturbed} & 1\% & 54.7 \\
    HybridCR~\cite{li2022hybridcr} & 1\% & 56.8 \\
    %SegGroup~\cite{tao2022seggroup} (KPConv)&  0.028\% & 61.2\\
    SQN~\cite{hu2022sqn} & 0.1\% & 56.9 \\
    SegGroup~\cite{tao2022seggroup} (MinkowskiNet)& 0.028\% & 62.7\\
    GaIA~\cite{lee2023gaia} & 1\% & 65.2 \\
    PointMatch~\cite{wu2022pointmatch} & 0.1\% & 68.8 \\
    One Thing One Click~\cite{liu2021one} & 0.02\% & \textbf{69.1}\\ 
    Ours & 0.02\% & \textbf{69.3}\\ 
    \bottomrule
  \end{tabular}
}
\caption{Comparing with the existing methods on ScanNet-v2 Test Set for 3D Semantic Segmentation.}
\label{tab:existing}
\end{table}
















\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{iterate.pdf}
\caption{Pseudo labels for each iteration on ScanNet-v2 training set. } \label{fig:iter}
\end{figure*}















\paragraph{Results on ScanNet-v2 Data-Efficient Benchmark}
\label{sec:data-efficient}
In Table~\ref{tab:date-efficient}, we show results on ScanNet-v2 ``3D Semantic label with Limited Annotations'' benchmark~\cite{hou2021exploring}. We report the results on the most challenging setting with only 20 points annotated each scene in Table~\ref{tab:baseline} ``Data Efficient''. In this experiment, we use the officially provided 20 points instead of ``One-Thing-One-Click'', and then employ our self-training approach for semantic segmentation. %Note that we are the first to report results on this benchmark. 
The results show that our approach is not limited to ``One Thing One Click'' and is applicable to other annotation schemes. In addition, our approach still outperforms existing works~\cite{hou2021exploring,xie2020pointcontrast,tian2022vibus} under this annotation scheme. 


\begin{table}
\centering
\scalebox{0.85}{
  \begin{tabular}{c|cc}
    \toprule
    Method & Supervision & mIoU (\%)  \\
    \midrule
    CSC\_LA\_SEM~\cite{hou2021exploring} & 20 points/scene & 53.1 \\
    PointContrast\_LA\_SEM~\cite{xie2020pointcontrast} & 20 points/scene & 55.0 \\
    VIBUS~\cite{tian2022vibus} & 20 points/scene & 58.6 \\
    \midrule
    One Thing One Click & 20 points/scene & \textbf{59.4}\\ 
    \bottomrule
  \end{tabular}
}
\caption{Comparing with existing methods on ScanNet-v2 Data Efficient Benchmark~\cite{hou2021exploring}.}
\label{tab:date-efficient}
\end{table}




\paragraph{Comparing with Our Baselines}

In this section, we first present three important baselines as shown in Table~\ref{tab:baseline} on ScanNet-v2 validation set. 
\begin{itemize}
%
\item Table~\ref{tab:baseline} ``Our fully sup baseline'' is trained with the official 100\% annotation provided by ScanNet-v2. It serves as the upper bound of our method. 
\item The model directly trained with the raw annotated points as Figure~\ref{fig:overview} (c) cannot converge well due to the extreme sparsity of the training data.
\item Table~\ref{tab:baseline} ``One Thing One Click$^*$''. The model trained with the initial pseudo labels as Figure~\ref{fig:overview} (d) achieves 62.18\% mIoU. It serves as the starting point of our self-training approach and is denoted as ``our baseline'' in the following. 
\end{itemize}

Table~\ref{tab:baseline} ``One Thing One Click'' manifests that our self-training approach surpasses the baseline by nearly $10\%$ mIoU, attaining a $16\%$ relative improvement. Compared with the fully supervised baseline with the same network architecture, our performance is only 2\% lower. 

Table~\ref{tab:baseline} ``One Thing One Click$^\dagger$'' refers to disabling the graph propagation and relation network in inference. Note that they are still being used in training for generating the pseudo labels. This brings no extra computational burden during the inference, but helps to improve nearly 7\% mIoU, comparing with the baseline (68.96\% vs 62.18\%).





\begin{table}
\centering
\scalebox{0.9}{
  \begin{tabular}{c|ccc}
    \toprule
    Setting & Annotation & mIoU (\%) \\
    \midrule
    Our fully sup baseline & 100\% & 72.18 \\
    \midrule
    One Thing One Click$^*$~\cite{liu2021one} & 0.02\% & 62.18\\
    One Thing One Click$^\dagger$~\cite{liu2021one} & 0.02\% & 68.96\\
    One Thing One Click~\cite{liu2021one} & 0.02\% & \textbf{70.45}\\
     \bottomrule
  \end{tabular}
}
\caption{Our results and baselines on ScanNet-v2 val.~set. $^*$ means the baseline model trained with the initial pseudo labels shown in Figure~\ref{fig:overview} (d). $^\dagger$ means disabling graph propagation and relation network during inference, but note that they are still used in training.  }
\label{tab:baseline}
\end{table}







\paragraph{Results with Fewer Annotations}\label{sec:fewer}



\begin{table}
\centering
\scalebox{0.9}{
  \begin{tabular}{c|cc}
    \toprule
    Method & Annotation (\%) & mIoU (\%) \\
    \midrule
     Two Things One Click$^*$ & 0.01 & 54.71\\
     Two Things One Click$^\dagger$ & 0.01 & 59.56\\ 
     Two Things One Click & 0.01 & \textbf{60.62}\\ 
    \bottomrule
  \end{tabular}
}
\caption{Two Things One Click results and baselines on ScanNet-v2 val.~set. $^*$ means the baseline model trained with the initial pseudo label shown in Figure 3 (d). $^\dagger$ means disabling graph propagation and relation network during inference, but note that they are still used in training. }
\label{tab:2T1C}
\end{table}
To investigate the performance of our approach with even less annotated points, we further annotate ScanNet-v2 with a ``Two Things One Click'' scheme, where we annotate a single random point on half of the objects chosen randomly in the scene. In this way, only less than 0.01\% points are annotated on ScanNet-v2. With the even sparse annotations, we still achieve 60.62\% mIoU as shown in Table~\ref{tab:2T1C}. This experiment also demonstrates that our method can still achieve decent performance even though the annotator ignores several objects by mistake in ``One Thing One Click'' scheme. We further investigate the performance drop with a more challenging ``Four Things One Click'' scheme. However, the model cannot converge well in the very first iteration due to the insufficient label and the self-training fails in this case. 







\paragraph{Qualitative Results on ScanNet-v2}
\label{sec:illustration_scannet}

Then, we show prediction results on ScanNet-v2 in Figures~\ref{fig:scannet_ill}.
Through these results, we demonstrate that our approach can produce segmentation results that are comparable to the fully supervised baseline~\cite{graham2017submanifold} with only 0.02\% annotation.
See the error maps shown in (d) and (f) for better visualizations. 

\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{scannet.pdf}
\caption{Qualitative comparisons on ScanNet-v2. (c) is produced by our model trained only with ``One Thing One Click'' annotations.
(e) is the fully supervised results of~\cite{graham2017submanifold}.
Red regions in (d) and (f) indicate the wrong predictions.
%\phil{change the figure PNG to make the text labels ``(a)...'' smaller in size... similar size as the figure caption.}
}
\label{fig:scannet_ill}
\end{figure*}










\paragraph{Ablation Studies}

To further study the effectiveness of self-training, graph propagation and relation network, we conduct ablation studies on these three modules on ScanNet-v2 validation set as shown in Table~\ref{tab:ablation} with single view evaluation. 

``3D U-Net'' indicates that the labels are propagated only based on the confidence score of the 3D U-Net itself,~\ie, the unary term in Equation~\ref{equ:energy}. This ablation is designed to manifest the effectiveness of self-training. The ``3D U-Net'' column in Table~\ref{tab:ablation} manifests that the performance is consistently improved with self-training strategy even without pairwise energy term in Equation~\ref{equ:energy} and super-voxel partition. 

``3D U-Net+GP'' refers to the label propagation with graph model, and the similarity among super-voxels are measured by the coordinates $p_i$ and colors $c_i$ without the learned feature $f_i$. This ablation study is to show the effectiveness of the graph model. %Compared with the ``3D U-Net'' setting, ``3D U-Net+GP'' propagates label with the graph model considering both the unary term and the pairwise term based on the super-voxel graph. 
The results in Table~\ref{tab:ablation} indicate that the graph model benefits the label propagation, and finally boosts the overall performance by 2\% over ``3D U-Net'' (67.92\% vs. 65.91\%). 

``3D U-Net+Rel+GP'' utilizes the relation network for similarity measurement based on ``3D U-Net+GP''. In this setting, the similarity among super-voxels is measured with the averaged coordinates $p_i$, the colors $c_i$, the unary features $u_i$, and the relation network generated feature $f_i$, as shown in Equation~\ref{equ:energy}. This experiment is to manifest that the relation network benefits the similarity measurement and pseudo label generation, compared with the hand-crafted feature, i.e., coordinates and color. It outperforms the hand-crafted features especially in the later iterations since the network benefits from the richer pseudo labels. It finally achieves 2.5\% improvement compared with ``3D U-Net+GP'' (70.45\% vs. 67.92\%). 
As shown in Figure~\ref{fig:iter}, the generated pseudo labels for each iteration expands to unknown regions step by step and finally gets close to the ground truth. 




\begin{table}
\centering
\scalebox{0.9}{
  \begin{tabular}{c|ccc}
    \toprule
    Method & 3D U-Net & 3D U-Net+GP &3D U-Net+Rel+GP \\
    \midrule
    Iter1 & 60.14  & 63.83 & \textbf{63.92}  \\
    Iter2 & 62.39  & 64.74 & \textbf{66.97} \\
    Iter3 & 64.83  & 66.10 & \textbf{68.40} \\
    Iter4 & 65.81  & 67.78 &  \textbf{70.01} \\
    Iter5 & 65.91  & 67.92 &  \textbf{70.45} \\
    \bottomrule
  \end{tabular}
}
\caption{Ablation studies. ``GP'' indicates the graph propagation, and ``Rel'' means the relation network. ``3D U-Net '' refers to propagating labels only with the network prediction itself.  ``3D U-Net+GP'' indicates label propagation with hand-crafted features. ``3D U-Net+Rel+GP'' indicates label propagation with our relation network. Evaluated on ScanNet-v2 val. set with single view testing. } 
\label{tab:ablation}
\end{table}


\begin{figure}
\centering
\includegraphics[width=0.99\columnwidth]{TSNE.pdf}
\caption{The t-SNE visualization of super-voxel features. Different colors and marks (point and plus) indicate different categories. %Zoom in for better visualization. 
The samples of the same category are better grouped together with our relation network (c), compared with hand-crafted features (a \& b). } \label{fig:tsne}
\end{figure}





\begin{table}
\centering
\scalebox{0.9}{
  \begin{tabular}{c|cccc}
    \toprule
    Method & Supervision & AP & AP50 & AP25  \\
    \midrule
    3DSIS~\cite{hou20193d} &100\%  & 16.1 & 38.2 & 55.8\\
    3D-BoNet~\cite{yang2019learning} &100\% & 25.3 & 48.8 & 68.7\\
    RPGN~\cite{dong2022learning} &100\% & 42.8 & 64.3 & 80.6\\
    Mask3D~\cite{schult2022mask3d} &100\% & 56.6 & 78.0 & 87.0\\
    \midrule
    PointGroup~\cite{jiang2020pointgroup} & 100\% & 40.7 & 63.6 & 77.8 \\
    \midrule
    TWIST~\cite{chu2022twist}  & 10\%  & 30.6 & 49.7 &  63.0 \\
    \midrule
    CSC-50~\cite{hou2021exploring} & 0.034\% & 22.9 & 41.4 & 62.0 \\
    SegGroup~\cite{tao2022seggroup} & 0.028\% & 24.6 &44.5 & 63.7\\
    3D-WSIS~\cite{tang2022learning} & 0.02\% & 28.1& 47.2 & \textbf{67.5} \\
    Ours & 0.02\% & \textbf{32.6} &\textbf{52.9}& \textbf{67.5} \\
    \bottomrule
  \end{tabular}
}
\caption{Comparing with existing methods on ScanNet-v2 test set for 3D instance segmentation. PointGroup~\cite{jiang2020pointgroup} means our fully-supervised baseline serving as our upper bound. } 
\label{tab:instance}
\end{table}





\begin{figure*}
\centering
\includegraphics[width=0.89\textwidth]{instance_result.pdf}
\caption{Weakly supervised 3D instance segmentation results of our approach. } \label{fig:instance_result}
\end{figure*}




\begin{table}
\centering
\scalebox{0.9}{
  \begin{tabular}{c|ccc}
    \toprule
    Method & Supervision (\%) & mIoU(\%) \\
    \midrule
PointNet~\cite{qi2017pointnet}& 100\% & 41.1 \\
SegCloud~\cite{tchapmi2017segcloud}& 100\% &  48.9 \\
TangentConv~\cite{tatarchenko2018tangent}& 100\% & 52.8\\
3D RNN~\cite{ye20183d}& 100\% & 53.4 \\
PointCNN~\cite{li2018pointcnn}& 100\%& 57.3\\
SuperpointGraph~\cite{landrieu2018large}& 100\%& 58.0\\
%PCCN& 100\%& 58.3&-\\
%MinkowskiNet20~\cite{choy20194d}& 100\%& 62.6&-\\ 
MinkowskiNet32~\cite{choy20194d}& 100\% &65.4\\
Virtual MV-Fusion~\cite{kundu2020virtual} & 100\%+2D &65.4\\
%Mix3D~\cite{nekrasov2021mix3d}  & 100\% & 67.2\\
    \midrule
    Our fully-sup baseline & 100\%& 63.7 \\
    \midrule
    Superpoint-guided~\cite{deng2022superpoint}& 10\% scenes& 51.1  \\
    \midrule
    $\mathbin{\Pi}$ Model~\cite{laine2016temporal} & 0.2\%&44.3\\
    MT~\cite{tarvainen2017mean}& 0.2\% & 44.4\\
    Xu~\etal
~\cite{xu2020weakly}$^*$ & 0.2\%  &  44.0 \\
    Xu~\etal
~\cite{xu2020weakly} & 0.2\% &44.5 \\  
    $\mathbin{\Pi}$ Model~\cite{laine2016temporal} & 10\%&46.3\\
    MT~\cite{tarvainen2017mean} & 10\% &  47.9\\
   Xu~\etal~\cite{xu2020weakly}$^*$ & 10\% &45.7\\ 
    Xu~\etal~\cite{xu2020weakly} & 10\%  &48.0\\    
    GPFN~\cite{wang2020weakly} & 16.7\% 2D& 50.8 \\  
    GPFN~\cite{wang2020weakly}  & 100\% 2D& 52.5   \\
   Zhang~\etal~\cite{zhang2021weakly} & 0.03\% & 45.8   \\
   Joint 2D-3D~\etal~\cite{kweon2022joint} & Scene+Image & 47.4   \\
   MulPro~\cite{su2022weakly} & 10\% & 49.0 \\
    MIL transformer\cite{yang2022mil}  &  0.02\% & 51.4  \\
    HybridCR~\cite{li2022hybridcr} & 0.03\% & 51.5\\
    GaIA~\cite{lee2023gaia} & 0.02\% & 53.7 \\
    DAT~\cite{wu2022dual} & 0.02\% & 54.6  \\
    %Liu~\etal~\cite{liu2022active}& 0.01\% & 56.3 \\
    \midrule
   %One Thing One Click$^*$~\cite{liu2021one}  & 0.02\% &43.7 \\
   %One Thing One Click$^\dagger$~\cite{liu2021one} & 0.02\% & 49.4\\
   One Thing One Click~\cite{liu2021one} & 0.02\% & 50.1  \\ 
    %One Thing Three Clicks$^*$ & 0.06\%  & 48.9 &-\\
    %One Thing Three Click$^\dagger$ & 0.06\% & 54.1&10.6\\
    %One Thing Three Clicks & 0.06\%&\textbf{55.3}& 13.1 \\ 
    Ours & 0.02\% &\textbf{56.6} \\ 
    \bottomrule
  \end{tabular}
}
\caption{Comparison with existing methods and baselines on 
the S3DIS Area-5. }
\label{tab:s3dis}
\end{table}



\begin{figure*}
\centering
\includegraphics[width=0.99\textwidth]{s3dis.pdf}
\caption{Qualitative results on S3DIS. (c) is produced by our model trained only with ``One Thing One Click'' annotations.
(e) is the fully supervised results of~\cite{graham2017submanifold}.
Red regions in (d) and (f) indicate the wrong predictions.
}
\label{fig:s3dis_ill}

\end{figure*}





\paragraph{Analysis of Relation Network}

Further, we study whether the learned embeddings of the relation network outperform the hand-crafted features for similarity measurement. We randomly sample 200 super-voxels for each category in ScanNet-v2, and conduct a t-SNE visualization~\cite{maaten2008visualizing} on them. Figure~\ref{fig:tsne} indicates that the relation network better groups the intra-class embeddings and distinguish the inter-class embeddings compared with hand-crafted features.





\subsection{Instance Segmentation on ScanNet-v2}\label{sec:instance}

Figure~\ref{fig:instance_result} and Table~\ref{tab:instance} show the qualitative and quantitative evaluation results for instance segmentation. 
Figure~\ref{fig:instance_result} demonstrates the effectiveness of our approach in recognizing nearby chairs as individual instances as shown in (a), (e), (f), and (h). Also, our approach can accurately predict larger instances like tables and beds as a single entity; see (d), (e), and (h). Further, the results in Table~\ref{tab:instance} indicate that our approach outperforms all existing methods and even surpasses some fully-supervised approaches such as~\cite{hou20193d,yang2019learning} for 3D instance segmentation.
%The qualitative and quantitative evaluations on instance segmentation are shown in Figure~\ref{fig:instance_result} and Table~\ref{tab:instance} respectively. Figure~\ref{fig:instance_result} indicate that our approach can effectively recognize the nearby chairs as individual instances, and successfully predict the large instance, like table and bed, as a single one. The comparison results in Table~\ref{tab:instance} manifest that our approach outperforms all the existing works, and even surpasses some fully-supervised approaches~\cite{hou20193d,yang2019learning}. 




\subsection{Evaluations on S3DIS}
We also evaluate our approach on the S3DIS dataset. Only less than 0.02\% points in the dataset are annotated with our ``One Thing One Click'' scheme. %To study whether the performance can be further boosted with richer annotations, we additionally conduct a ``One Thing Three Clicks'' scheme on S3DIS, where random 3 points per-object are annotated. %Note that even with ``One Thing Three Clicks'' scheme, our annotation is still fewer than existing works~\cite{xu2020weakly}

\paragraph{Comparing with Existing Works}

We also compare with fully supervised approaches and weakly supervised approaches on S3DIS.
As shown in Table~\ref{tab:s3dis}, with the ``One Thing One Click'' scheme where less than 0.02\% points are annotated, we achieve 56.6\% mIoU, outperforming existing works by a considerable margin, including our previous version~\cite{liu2021one} (50.1\%).  %With ``One Thing Three Clicks'' scheme, our performance can be further improved to 55.3\% mIoU. The above two results outperform~\cite{xu2020weakly} by 5.6\% and 10.8\% mIoU (0.2\% annotations in~\cite{xu2020weakly}), and 2.1\% and 7.3\% mIoU (10\% annotations in~\cite{xu2020weakly}) respectively. 

%Wang~\etal~\cite{wang2020weakly} unprojects 2D semantic labels to 3D space for 3D semantic segmentation. To compare with~\cite{wang2020weakly}, we first compare with the actual number of annotated points regardless of 2D or 3D. For S3DIS, the number of annotated 2D pixels (70,496 images with 1080$\times$1080 resolution) is 100$\times$ more than the officially annotated 3D points ($5.27 \times 10^{8}$ in total), so both settings of~\cite{wang2020weakly} (100\% 2D annotations and 16.7\% 2D annotations) actually utilize a large quantity of annotations. %In addition, even though there are several high-efficiency 2D image annotator proposed recently, it still a tedious and arduous work for annotating such a large number images. 
%Even with a large gap of annotation, the results in Table~\ref{tab:s3dis} show that our ``One Thing Three Clicks'' scheme with only 0.06\% 3D annotation outperforms~\cite{wang2020weakly} with 100\% 2D annotations by nearly 3\% mIoU.  

In addition, our approach achieves comparable results with several fully supervised methods as shown in Table~\ref{tab:s3dis}. 




%\paragraph{Comparing with Our Baselines }
%We follow the similar settings in Section~\ref{sec:scannet} to show several baselines for S3DIS. 
%\begin{itemize}
%    \item Table~\ref{tab:s3dis} ``Our fully-sup baseline''. The model trained with the full supervision of S3DIS achieves 63.7\% mIoU. It serves as the upper bound of our approach. 
%    \item The model directly trained with only the annotated points in Figure~\ref{fig:overview} (c) cannot converge well. % even in the first iteration.
%    \item Table~\ref{tab:s3dis} ``One Thing One Click$^*$''. The model trained with the annotated super-voxels in Figure~\ref{fig:overview} (d) achieves 43.7\% mIoU. 
%    \item To evaluate without any extra computation burden, we further disable the label propagation and relation network in inference as shown in Table~\ref{tab:s3dis} ``$^\dagger$''. This setting achieves 49.4\% mIoU.  
%\end{itemize}


%As shown in Table~\ref{tab:s3dis} ``Rel. Imp.'' column, we have 14.6\% (``One Thing One Click'') and 13.1\% (``One Thing Three Clicks'') relative improvement over our baseline, surpassing the relative improvement  of~\cite{xu2020weakly}, which is 1.1\% (with 0.2\% annotations) and 5\% (with 10\% annotations) over their own baselines, by a large margin. The significant improvement of ``relative improvement over baseline'' manifests the effectiveness of the proposed approach. 

 % Note that label propagation and relation network is still employed self-training process, but are removed in evaluation. 

%\paragraph{One Thing Three Clicks}

%We further conduct an further experiment with ``One Thing Three Clicks'' scheme, where random three points per object are annotated. In this setting, we achieves 55.3\% mIoU. This experiment demonstrates our approach can achieve better performance with richer annotations. 




\vspace{-0.1in}

\paragraph{Qualitative Results on S3DIS}
\label{sec:illustration_s3dis}

Figures~\ref{fig:s3dis_ill} illustrates our results on S3DIS. Again, with only 0.02\% annotation, our approach can produce high quality semantic predictions (c) that are comparable to the fully-supervised approach (e). See the error maps (d, f) for better illustration. 





\section{Conclusion}

We propose the ``One Thing One Click'' scheme to efficiently annotate point clouds for weakly supervised 3D semantic segmentation, requiring significantly fewer annotations than the previous approaches.
%By this new means, we can significantly relieve the annotation burden.
%
To put this scheme into practice, we formulate a self-training approach to make it feasible for the network to learn from such extremely sparse labels.
Specifically, we execute the two key modules in our approach iteratively: expand labels through the label propagation module and train the network using the updated pseudo labels.
Further, we adopt a relation network to explicitly learn the feature similarity. In addition, our approach is compatible to 3D instance segmentation using the One Thing One Click annotations. 
Experiments on two large 3D datasets ScanNet-v2 and S3DIS manifest that our approach, with only extremely-sparse annotations, outperforms existing weakly supervised methods on 3D semantic segmentation and instance segmentation consistently. Moreover, our results are even comparable to those of the fully supervised counterparts. 






{\small
\bibliographystyle{ieee_fullname}
\bibliography{cvpr}
}

\end{document}
