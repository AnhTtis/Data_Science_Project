Based on the FPGA design and implementation described in Section~\ref{sec:design}, the FPGA kernel is instantiated by using C++ function templates, and compiled and synthesized for 256 \emph{generic} $[ab|cd]$ quartet classes with the Intel FPGA Add-on for oneAPI Base Toolkit (version 22.3.0) targeting Bittware 520N board with Intel Stratix 10 GX 2800 FPGA.

\subsection{Benchmark}\label{subsec:benchmark}

As benchmark, we use a synthetic molecular system composed of 32 sites arranged on a cubic $4 \times 4 \times 2$ lattice with a lattice parameter of 1~\AA. Each site has primitive Cartesian $s$, $p$, $d$, and $f$ GTO-shells with an exponent of 1.5. No screening of quartets is used.
The performance metric is the measured FPGA kernel throughput in terms of compressed ERIs in Giga ($10^{9}$) ERIs per second (GERIS).
This evaluation ignores the preparation on the host (computation of Rys roots and weights) and the transfer of inputs and outputs between FPGA and host memory via PCIe ($\approx$ 6~GB/s), which becomes a practical bottleneck for intermediate to large quartet classes with 3 and 4 GERIS per Bittware 520N card for 16-bit and 12-bit compression, respectively.


\subsection{Synthesis Results and Throughput Analysis}\label{subsec:impl_results}

\begin{figure}[tbh]
\centering
\subfloat[ALMs: 933120]{%
  \includegraphics[width=0.46\columnwidth]{FPGA_Resources_aacc/final_palms-pdfa}%
\label{fig:fpga_alms}}
\hfil
\subfloat[Registers: 3732480]{%
  \includegraphics[width=0.46\columnwidth]{FPGA_Resources_aacc/final_pregs-pdfa}%
\label{fig:fpga_regs}}
\newline
\subfloat[DSP blocks: 5760]{%
  \includegraphics[width=0.46\columnwidth]{FPGA_Resources_aacc/final_pdsp-pdfa}%
\label{fig:fpga_dsp}}
\hfil
\subfloat[$f_{\text{max}}$]{%
  \includegraphics[width=0.48\columnwidth]{FPGA_Resources_aacc/final_fmax-pdfa}%
\label{fig:fpga_fmax}}
\caption{Resource utilization and clock frequency ($f_{\text{max}}$) of FPGA kernel designs for 256 \emph{generic} ERI quartet classes.
The 55 \emph{canonical} quartet classes are highlighted in black squares.
Available ALMs, registers, and DSPs in Stratix 10 GX 2800 are listed.}
\label{fig:fpga_resources}
\end{figure}

Fig.~\ref{fig:fpga_resources} shows FPGA resource utilization and clock frequency ($f_{\text{max}}$) for the 256 kernels with the optimal $c_{\text{max}}$.
Quartet classes $[ab|cd]$ are arranged as square element in heatmap matrices with row and column denoting $[ab|$ and $|cd]$, respectively.
Hence, $[ss|ss]$ forms the top left of heatmap matrices, $[ff|ss]$ the bottom left and $[ff|ff]$ the bottom right.
The 55 \emph{canonical} quartet classes, all located in the lower triangular of heatmap matrices, are highlighted.
The highest resource consumption is found for $[ff|**]$ designs, since the corresponding loops ($i$, $j$, $k$ in Fig.~\ref{fig:fpga_algorithm} line~\ref{lst:line:forall_rr}, $a$, $b$ in Fig.~\ref{fig:fpga_algorithm} line~\ref{lst:line:forall_quad}) are always unrolled, followed by designs below $[dd|**]$ (i.e.\ $[fd|**]$, $[df|**]$) that also have a high degree of parallelism in these loops. %
The usage of DSPs and $f_{\text{max}}$ are indicators for the number of arithmetic operations and design complexity, e.g.\ the simplest $[ss|ss]$ kernel only uses 8 DSPs and reaches $f_{\text{max}}=$ 474 MHz. %
In contrast, $[ff|ff]$ uses 2182 (39\%) DSPs for up to 3227 floating-point operations per cycle
at $f_{\text{max}}=$ 338 MHz. %

\begin{figure}[tbh]
\centering
\includegraphics[width=0.98\columnwidth]{10-ERI-BRAM/10-ERI-BRAM-crop}
\caption{BRAM utilization of FPGA kernels with doubled $c_{\text{max}}$ for selected \emph{canonical} $[aa|cc]$ quartet classes.}
\label{fig:fpga_bram}
\end{figure}

In Fig.~\ref{fig:fpga_bram}, BRAM utilization for selected \emph{canonical} quartet classes from $[ss|ss]$ to $[ff|ff]$ is plotted with respect to $c_{\text{max}}$ from 8 up to the optimal value. %
Depending on the depths of the local memory layout,
BRAM utilization may or may not increase with $c_{\text{max}}$ to create space for more private copies. %
It turns out that
the optimal $c_{\text{max}}$ is small for large quartet classes that already perform many iterations in inner loops. In contrast, for intermediate quartet classes with their shallow intermediate buffers, many private copies fit into the otherwise underutilized BRAMs.
Thus, the overall BRAM utilization is always less than 2.5K (21\%) in Stratix 10 GX 2800.

\begin{figure*}
\centering
\includegraphics[width=0.98\textwidth]{figs/SynMol/cmax_perfmodel/perfmodel_256-crop}
\caption{Measured FPGA kernel throughput up to the optimal $c_{\text{max}}$ and comparison to the performance model for 256 \emph{generic} ERI quartet classes.
Throughput is plotted in $\log_{2}$-scale for visualizing doubled GERIS with respect to doubled $c_{\text{max}}$.
 The \emph{canonical} ERI quartet classes are highlight by vertical green bars.}
\label{fig:perf_model}
\end{figure*}

Fig.~\ref{fig:perf_model} presents the measured throughput of FPGA kernels for 256 \emph{generic} ERI quartet classes for different $c_{\text{max}}$ values up to the optimum.
For small to medium quartet classes, the throughput is only saturated with $c_{\text{max}}=128$. For the designs with all intermediates in registers, throughput exactly doubles with every doubling of $c_{\text{max}}$ before the last step.
In general, the throughput for large quartet classes, i.e.\ using BRAMs for intermediates, also increases when doubling $c_{\text{max}}$, but gets saturated more quickly with the best throughput reaching 10 - 11 GERIS. 

In order to compare the performance model motivated in Sec.~\ref{paragraph:default::initial_perf} with the measurements, we include the final $f_{\text{max}}$ that is automatically set by the synthesis tools after timing analysis. The modeled GERIS throughput is calculated as 
\begin{IEEEeqnarray}{lCr}
  \text{GERIS}_{\text{model}} = \frac{f_{\text{max}} \cdot n_{\text{ERIQ}}}%
  {\max(n_{\text{RR}}, n_{\text{GQ}}, n_{\text{CS}}) + 10},
\end{IEEEeqnarray}
with the denominator representing the number of clock cycles spent per quartet. Note that this model does not contain an explicit term for off-chip bandwidth, since this is implicitly encoded in the $n_{\text{CS}}$ term for the \emph{compress-store loops} that already incorporates the effect of output padding, as discussed in Sec.~\ref{sec:general_optimization}. However, there is an overhead per iteration of the outermost \emph{quartets loop} that we empirically determined as 10 clock, which presumably is related to the global memory operations on the interleaved memory interface.
This throughput model, as overlaid with the measurements in Fig.~\ref{fig:perf_model} is evidently in excellent agreement with the measurements for optimal $c_{\text{max}}$, validating the outcome of this optimization process.




\subsection{Preference for Canonical Classes}\label{subsec:canonical}

As introduced in Sec.~\ref{subsec:bg:quartets}, due to permutation symmetry, there can be up to eight \emph{generic} quartet classes that are mathematically identical and when reordering the inputs can be used for the same calculations. There are 55 unique \emph{canonical} classes that follow the convention defined in Eq.~\ref{eq:can_abcd}. With suitable pre-processing, these are sufficient to perform all ERI calculations discussed here.

With the loop structure presented in Fig.~\ref{fig:fpga_algorithm}, we ordered the loops in such a way that the innermost loops that are unrolled first correspond to the larger dimensions (Eq.~\ref{eq:can_abcd}) in the \emph{canonical} classes and thus contain more parallelism after the general optimization as presented in Sec.~\ref{sec:general_optimization}. With the automatic further unrolling as presented in Sec.~\ref{sec:further_unrolling} the designs for non-canonical classes can conceptually catch up to sufficient levels of parallelism, when the local memory layout permits.

In Fig.~\ref{fig:perf_model}, we highlighted the \emph{canonical} classes, each of which is followed by its \emph{generic} permutations to its right side. We see that for the designs with all intermediates in registers, for the optimal $c_{\text{max}}$ values all permutations match the performance of their respective \emph{canonical} representations. For the designs with intermediates in BRAMs, there are a number of permutations that do not reach the performance of their \emph{canonical} representations because of local memory limitations preventing further unrolling. 
Table~\ref{tab:permut_variants} provides another perspective on this effect for a few selected quartet classes, comparing the designs for three \emph{canonical} classes each with one of their \emph{generic} counterparts. For the first two examples, the respective \emph{canonical} versions reach the higher performance in accordance with their higher parallelism reflected by the performance model and despite the clock frequency advantage of the $[dp|ff]$ design with its lower resource utilization. 
\begin{table}
\renewcommand{\arraystretch}{1.3}
\caption{Comparison of some permutation variants for representative ERI quartet classes. $\max(n_{\text{RR}}, n_{\text{GQ}}, n_{\text{CS}})$ in Bold.}
\label{tab:permut_variants}
\centering
\begin{tabular}{@{\;}ccc@{\;\;\;}cccc@{\;\;\;}c@{\;}}
\hline
\multirow{2}{*}{Quartet}              &
$f_{\text{max}}$                      &
\multirow{2}{*}{$n_{\text{ERIQ}}$}    &
\multicolumn{4}{c}{Performance model} &
measured                              \\
\cmidrule(r){4-7}
& [MHz] & & $n_{\text{RR}}$ & $n_{\text{GQ}}$ & $n_{\text{CS}}$ & GERIS & GERIS \\
\hline
$[fd|ps]\,^{1}$ & 408.3 & \multirow{2}{*}{180}  & \textbf{12} &          3  & 6 & 3.34 & 3.33 \\
$[ps|fd]$       & 408.2 &                       &         12  & \textbf{60} & 6 & 1.05 & 1.05 \\
\hline
$[ff|dp]\,^{1}$ & 373.4 & \multirow{2}{*}{1800} & 30 &          18  & \textbf{57} & 10.03 & 10.03 \\
$[dp|ff]$       & 407.7 &                       & 60 & \textbf{100} &         57  &  6.67 &  6.65 \\
\hline
$[fd|fd]\,^{1}$ & 400.2 & \multirow{2}{*}{3600} & 72 &   60 & \textbf{113} & 11.71 & 11.17 \\
$[df|fd]$       & 392.5 &                       & 72 &   60 & \textbf{113} & 11.49 & 10.93 \\
\hline
\multicolumn{7}{l}{\footnotesize$^1$\,\emph{Canonical} ERI quartet classes.}
\end{tabular}
\end{table}





\subsection{Comparison with CPUs}\label{subsec:vs_libint}
We compare the performance of the FPGA design which includes compression to the performance of libint (version 2.7.2) without compression. Libint is a highly tuned and widely used CPU library for ERI computation using the algorithms based on the Obara-Saika method~\cite{Obara86, Obara88}.
Double-precision floating-point arithmetic is utilized for ERI computation in libint and these values are used as the numerical reference for our FPGA kernels for the purpose of the accuracy investigation.%
\begin{figure}
\centering
\includegraphics[width=0.98\columnwidth]{figs/SynMol/MultiFPGAs_vs_CPUs/ALL_GERIS-crop}
\caption{Performance comparison between libint on CPUs (blue and purple lines) and FPGA kernels for \emph{canonical} $[ab|cd]_{16\text{-bit}}$ and $[ab|cd]_{12\text{-bit}}$.}
\label{fig:all_geris}
\end{figure}
A test program using the recommended modern C++ API of libint is used and parallelized with MPI, so that all ERIs of the same $[ab|cd]$ quartet class in the benchmark molecule are distributed to different ranks and computed in parallel. GCC (version 11.2.0) and OpenMPI (version 4.1.1) are used to build libint and the test program.
We carried out the measurements on a two-socket server with Intel Xeon Gold 6148 CPUs (2x20 cores) as well as a two-socket server with AMD EPYC 7713 CPUs (2x64 cores). The Intel Xeon CPU represents a CPU reference using the same manufacturing technology as the FPGAs, with both devices showing up first in 2018 in HPC centers, and the AMD EPYC CPU is chosen as a representative for up-to-date manufacturing technology.
For Xeon Gold 6148 CPU and EPYC 7713 CPU the compiler flags \texttt{-march=skylake-avx512} and \texttt{-march=znver3} are used, respectively, for optimized performance. For the FPGA kernel execution, two Stratix 10 GX 2800 FPGA cards in one compute node are used with an OpenMP-parallelized driver code that uses two SYCL queues, one per FPGA.

Fig.~\ref{fig:all_geris} presents the results of measured throughput in GERIS for the 55 \emph{canonical} ERI quartet classes on CPUs and FPGA cards.
The throughput for $[ss|ss]$ is merely 0.08~GERIS for both 12-bit and 16-bit FPGA kernels, whereas it is 0.17~GERIS on 40 Intel Xeon CPU cores and 0.48~GERIS on 128 AMD EPYC CPU cores. For higher angular momenta, the number of ERIs per quartet, $n_{\text{ERIQ}}$, as well as the number of floating-point operations per quartet, $n_{\text{FLOPQ}}$, increases. As a consequence, the throughput on both CPUs and FPGAs starts to increase.
Before $[ff|ss]$ ($n_{\text{ERIQ}} = 100$) 40 Intel Xeon CPU cores slightly outperform the FPGA kernels.
Thereafter, the throughput of FPGA kernels increases steeply and can outperform the 40 Intel Xeon CPU cores by factors up to 6x. This shows the advantage of the FPGA design over a CPU 
using the same 14 nm manufacturing technology.
When compared with 128 AMD EPYC CPU cores, released in 2021 and produced with a much more recent 7 nm manufactoring technology, the FPGA kernels start take over the performance lead later, around $[ff|ds]$ ($n_{\text{ERIQ}} = 600$) with \ 14.1~GERIS on CPUs, and 15.3~GERIS and 18.2~GERIS for 16-bit and 12-bit FPGA kernels, respectively.
For even larger ERI quartet classes, the throughput in terms of GERIS on 128 AMD EPYC CPU cores starts to decrease. %
In contrast, the throughput of both 16-bit and 12-bit FPGA kernels continuously increases and reaches the peaks of 22.3~GERIS and 28.4~GERIS for 16-bit and 12-bit kernels, respectively.
When comparing the throughput of 16-bit and 12-bit FPGA kernels, similar performance is found for ERI quartet classes smaller than $[fd|pp]$ ($n_{\text{ERIQ}} = 540$), then the 12-bit FPGA kernel starts to outperform the 16-bit kernels in general.

\begin{figure}
\centering
\includegraphics[width=0.98\columnwidth]{power_measurements/plots/energy_eff-crop}
\caption{Comparison of energy efficiency between libint on CPUs (blue and purple lines) and FPGA kernels for \emph{canonical} $[ab|cd]_{16\text{-bit}}$.}
\label{fig:energy_eff}
\end{figure}
During the benchmark runs, the CPU package power consumption measured with RAPL counters was nearly independent of the type of quartet. The power consumption was about 490~W for two AMD EPYC 7713 CPUs (TDP 240 W each) and 300~W for two Intel Xeon Gold 6148 CPUs (TDP 150 W each). In contrast, for two FPGA cards, the combined board power consumption starts of from 132~W for $[ss|ss]$ and increases to 180~W for $[ff|ff]$. The resulting energy efficiency is shown in Fig.~\ref{fig:energy_eff}. For quartets with low angular momenta, the power efficiency of CPUs and FPGAs is comparable, while for intermediate to larger angular momenta there is a clear advantage of the newer AMD Zen 3 CPU cores over the older Intel Skylake-SP CPU cores. Starting from $[pp|pp]$ the advantage of the FPGA implementation over the CPUs becomes evident, reaching about 5x higher energy efficiency of the FPGA implementation for high angular momenta.

\begin{figure}
\centering
\includegraphics[width=0.98\columnwidth]{figs/SynMol/numerical_accuracy/num_acc-crop}
\caption{Comparison of maximum absolute error for decompressed ERIs using 16-bit integer for libint, 16- and 12-bit integers of FPGA kernels.}
\label{fig:num_acc}
\end{figure}
Last but not least, the numerical accuracy due to ERI compression is also considered and compared against the libint reference in double-precision.
We implemented an ERI compression using 16-bit integer based on the libint reference values.
Fig.~\ref{fig:num_acc} shows the maximum absolute error for decompressed ERIs for the \emph{canonical} quartet classes.
If 16-bit integer is adopted for ERI compression, libint and FPGA kernel give almost identical max.\ abs.\ errors, $10^{-7}$ - $10^{-5}$ Hartree, because the errors are bound by $\epsilon/2$ (Eq.~\ref{eq:decomp_eris}), irrespective of single-precision or double-precision floating-point arithmetics for ERI computation.
With 12-bit integer for ERI compression, max.\ abs.\ errors increase systematically and, except for $[ss|ss]$, range from $10^{-5}$ - $10^{-4}$ Hartree.
Hence, the main source of numerical errors is attributed to the bitwidth used for ERI compression.



