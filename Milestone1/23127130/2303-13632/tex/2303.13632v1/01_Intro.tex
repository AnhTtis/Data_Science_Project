

Quantum-mechanics-based atomistic simulations, known as \textit{ab initio} molecular dynamics (AIMD)~\cite{Kuehne2014}, solve the electronic structure problem for solids and molecular systems~\cite{pople98}, e.g.\ with density functional theory (DFT)~\cite{kohn98}.
AIMD can describe complex reactive systems, which are normally inaccessible by means of classical molecular dynamics.
The current state-of-the-art AIMD simulations have been pushed beyond the boundaries of more than 100 million atoms by using the novel non-orthogonalized local submatrix method~\cite{Lass20, Schade22}.
The hybrid DFT~\cite{Becke93}, which incorporates a portion of nonlocal Hartree-Fock exchange contributions, is essential for the accurate simulation of challenging systems. An example is the description of the band gap in bulk silicon used in the semiconductor industry. The band gap describes the electrical conductivity properties and is underestimated in semi-local DFT calculations. The use of hybrid DFT is required to properly obtain the band gap in agreement with experimental values~\cite{PhysRevApplied.8.024023} and, thus, properly describe the electrical conductivity.
However, AIMD simulations based on the hybrid DFT are usually limited to systems of thousands of atoms because of the computational challenges of electron repulsion integrals (ERIs) over Gaussian-type orbitals (GTOs).
The formal complexity of the ERI computation is $\mathcal{O}(n^{4})$, where $n$ is the number of atoms in the system of interest.
In practical terms, many trillions of ERIs may have to be computed in each time step of an AIMD simulation~\cite{Kuehne2020}.
Thus the ERI computation was dubbed as ``the nightmare of the integrals'' in the 1998 Nobel Lecture for Chemistry~\cite{pople98}.


Because of the importance of ERI computation for AIMD simulations, there are active endeavors to develop fast and efficient methods~\cite{HJO_ch9}, as well as libraries on CPU~\cite{libint2, libcint, Chow16, Chow18, Neese22} and GPU~\cite{Mart√≠nez08, Gordon10, Merz13, Gordon22} in last several decades.
However, the computations of necessary intermediates involve data dependencies due to multi-dimensional recurrence relations (RRs).
Furthermore, a large number of combinations of angular momenta for both recursive intermediates and final ERIs require flexible data layouts.
The hardware architectures of CPU and GPU are all fixed at the time of fabrication, thus cannot fulfill such flexible requirements for the ERI computation owing to either unfilled SIMD vectors on CPU~\cite{Chow16}, or restrictions on the number of GPU threads per block and limited shared memory per streaming processor~\cite{Gordon22}.
In contrast to thread-level parallelism and fixed SIMD width of CPUs and GPUs, on FPGAs the single work-item programming model enables pipeline parallelism for loops along with flexible amounts of data parallelism. %
In addition, FPGA local memory can be tailored to desired data widths and depths for parallel data accesses for the computation of intermediates.


Despite numerous successful applications of FPGAs as energy-efficient accelerators in many scientific domains, e.g.\ linear algebra~\cite{DeMatteis20, Gorlani19, Meyer22, Zhang22}, the current FPGA-accelerated atomistic simulations~\cite{Herbordt05, Herbordt10, Herbordt13, Herbordt19, Jones22} are mainly based on classical molecular dynamics.
Since the ERI computation is a key component for building the Hamiltonian matrix in hybrid DFT, this work paves the way for FPGA-accelerated quantum-mechanics-based atomistic simulations.
Our contributions include:

\begin{itemize}
  \item We present the first FPGA implementation targeting ERI quartets up to angular momenta of $L=3$ (256 quartet classes). The only previous attempt~\cite{ki-uf-a} was limited to the simplest quartet class with angular momentum $L=0$.
  \item Adapting to the different computation characteristics of quartet classes, 256 FPGA kernels are specifically customized in terms of parallelism and local memory layout by taking advantage of function templates using DPC++ in the Intel FPGA Add-on for oneAPI Base Toolkit~\cite{FPGAOpt4oneAPI}.
  \item A performance model is embedded into the optimization process and accurately matches the final results.
  \item This is the first implementation seamlessly integrating ERI computation and ERI compression using arbitrary bitwidth integer for the mitigation of demanding memory requirements in AMID simulations and of data transfers via PCIe from FPGA device back to host.
  \item Evaluation reveals that the FPGA kernels on 2 Stratix 10 GX 2800 cards outperform libint~\cite{libint2}, a well-established library for the ERI computation in major atomistic simulation programs~\cite{libintwiki}, parallelized on two Intel Xeon Gold 6148 CPUs (40 cores) by factors up to 6.0x and on two AMD EPYC 7713 CPUs (128 cores) by up to 1.9x.
\end{itemize}
The source code is publicly available ~\cite{zenodo}.