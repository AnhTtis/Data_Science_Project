\section{Introduction}
\label{sec:intro2}

Learning good spatiotemporal representations~\cite{qian2021spatiotemporal,wu2021mvfnet,han2020self,tong2022videomae, fbvideomae, yang2023aim} is fundamental for video understanding tasks. In action recognition, a common evaluation protocol is to first evaluate the model performance on large-scale video datasets such as Kinetics-400 \cite{kay2017kinetics}, then show its effectiveness of transfer learning to different downstream tasks~\cite{carreira2017quo,feichtenhofer2019slowfast,lin2019tsm,bertasius2021space,liu2022video,text4vis}. 
% To quantify the progress in video action recognition, 
Many video datasets~\cite{soomro2012ucf101,hmdb51,kay2017kinetics,goyal2017something, epickitchen} have been introduced over the past few years to advance the field. However, there are several major limitations: (1) These datasets are similar in terms of domains and actions. Most of them only contain daily or sports actions because these categories are easy to collect from the web. Yet many important real-world applications, such as anomaly detection and industrial inspection, are rarely included. (2) Each of these datasets has its own characteristics (\eg appearance-focused \cite{kay2017kinetics}, motion-focused \cite{goyal2017something}, fine-grained \cite{shao2020finegym}, egocentric \cite{epickitchen}). Previous works usually conduct evaluations on a few datasets. However, without evaluating a suite of datasets, we cannot fully diagnose a model and make further improvements. (3) The held-out test set for these datasets either does not exist or is not commonly adopted. This will affect the transfer performance because models tuned on a test set using hyperparameter optimization or neural architecture search might achieve good performance but cannot transfer well due to overfitting.

\begin{figure}[t]
\vspace{-5pt}
\centering
    \includegraphics[width=\linewidth]{figures/BEAR.pdf}
    % \includegraphics[width=\linewidth]{figures/teaser.pdf}
    \caption{\includegraphics[scale=0.07]{figures/bear.png}BEAR is a collection of 18 video action recognition datasets grouped into 5 categories (Anomaly, Gesture, Daily, Sports, and Instructional). It enables various evaluation settings, e.g., standard finetuning, few-shot finetuning, unsupervised domain adaptation, and zero-shot learning.}
    \label{fig:teaser}
    \vspace{-1.5 em}
\end{figure}

In light of this, we propose a unified and challenging BEnchmark on video Action Recognition, named BEAR, to better evaluate spatiotemporal representation learning. We define good representations as those that can achieve strong transfer learning performance on diverse, unseen domains even with limited data. To this end, we build BEAR by collecting a suite of 18 video action recognition datasets grouped into 5 categories (Anomaly~\cite{sultani2018real,10185076}, Gesture~\cite{materzynska2019jester}, Daily~\cite{sigurdsson2018charades,das2019toyota}, Sports~\cite{karpathy2014large}, and Instructional~\cite{tang2019coin}), which cover a diverse set of real applications. The datasets in BEAR are also diverse in video sources (\eg YouTube, CCTV cameras, self-collected) and viewpoints (\eg egocentric, 3rd person, drone, and surveillance). In addition, we split each dataset into train and test sets, \textit{strictly keeping the test set held out during training in all of our experiments}. 
We will also provide an online evaluation server to enable fair comparisons. % for future researchers. 

With BEAR, one can probe spatiotemporal representation learning methods from a much more diverse perspective and answer many important questions. Does the good performance on commonly-used large-scale datasets translate to real applications? Do recent transformer-based models consistently outperform simple 2D models in different domains? How sensitive is the model to domain and viewpoint change? Could the model achieve good performance when downstream data is limited? In this work, we comprehensively investigate 6 representative video models pre-trained by both supervised and self-supervised learning in various settings (\eg full-shot, few-shot, domain adaptation). Our study quantifies existing intuition and uncovers several new insights: 
(1) Simple 2D video models can outperform recent transformer-based models when equipped with strong backbones.
(2) The previous evaluation protocols are constrained to downstream datasets that resemble Kinetics-400. However, the high performance of these datasets does not necessarily transfer to other application domains.
(3) Viewpoint shift has a dramatic impact on downstream task performance. Even the recent domain adaptation methods cannot address the problem to satisfactory. This suggests we may need to go beyond domain adaptation and shift attention to building more comprehensive pre-training datasets.
(4) Self-supervised spatiotemporal representation learning still lags remarkably behind supervised learning. Even the SoTA VideoMAE \cite{tong2022videomae} fails to outperform simple supervised models in diverse domains. 
Our goal is to provide a unified and challenging evaluation benchmark to evaluate spatiotemporal representation learning from various perspectives, which hopefully could guide future development in video understanding. 