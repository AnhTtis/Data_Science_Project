\section{Conclusion and Discussion}
In this work, we introduce a new action recognition benchmark \includegraphics[scale=0.07]{figures/bear.png}BEAR to address several limitations in existing video benchmarks. Aiming at benefiting both academic and industrial applications, we carefully select 18 datasets covering 5 distinct data domains.
Such a wide scope could provide comprehensive assessment protocols for any video model, filling the gap in the current video action recognition benchmark that only a small number of target datasets are considered. It helps prevent models from overfitting on a specific dataset which could result in biased model evaluation. 
Moreover, to achieve a fair comparison, we held out test data for every dataset and avoid using it for parameter selection during training, and the evaluation is based on the last checkpoint. Meanwhile, in this work, we also pay attention to the capabilities of 2D CNNs, 3D CNNs, and transformers. 
Importantly, we carefully select comparative backbones for them to avoid erroneous comparisons. 

Based on our extensive experiments, we have several interesting and instructive observations: 1) 2D video models are competitive with SoTA transformer models when equipped with strong backbones. 2) Previous evaluation protocols on a few similar datasets can yield biased evaluation. 3) Domain shift (especially the viewpoint shift) has a large impact on transfer learning, and the performance gap could be much more remarkable in the few-shot setting. 4) Self-supervised learning still largely falls behind supervised learning, and even the SoTA VideoMAE cannot outperform supervised models on diverse downstream datasets. Moreover, we also point out that in order to learn robust spatiotemporal representations, constructing new pre-training datasets containing videos from diverse domains could benefit the target performance on a wide range of datasets. Due to space limits, we only consider evaluation datasets and leave the art of training data construction to future work.