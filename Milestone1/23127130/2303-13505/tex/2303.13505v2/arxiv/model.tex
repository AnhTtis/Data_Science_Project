\section{Models}
There has been a considerable amount of video models proposed to solve the human action recognition task. 
From the perspective of the basic building block, these models can be roughly classified into three categories: 2D CNNs, 3D CNNs, and transformer-based models. 
To investigate the efficacy of each model type, in this work, we select two representative works from each category: TSN~\cite{wang2016temporal} and TSM~\cite{lin2019tsm} for 2D CNNs, I3D~\cite{carreira2017quo} and 3D Non-local network~\cite{wang2018non} for 3D CNNs, TimeSformer~\cite{bertasius2021space} and VideoSwin~\cite{liu2022video} for transformer-based models. 
We would like to point out that for CNN-based models (TSN, TSM, I3D, and NL), we choose ConvNext-base~\cite{liu2022convnet} as the backbone because it has a similar model size and performance, as shown in Table~\ref{backbone},  on ImageNet-1K compared to ViT-B and Swin-B, which is the backbone of TimeSformer and VideoSwin, respectively. 
This alleviates the impact from the backbone, thus presenting a more fair comparison among different video architectures. 
In this work, we finetune all the models based on both supervised and self-supervised pre-training on Kinetics-400, and the pre-training performance is shown in Table \ref{tab:pretrained}. The pre-training details can be found in \textcolor{red}{Appendix~\ref{appendix training details}}.
In the following sections, we will provide a comprehensive study w.r.t. transferring performance from multiple perspectives: standard finetuning, few-shot finetuning, unsupervised domain adaptation, and zero-shot evaluation.

\begin{table}[h]
\centering
\caption{Comparison among ConvNeXt-base, ViT-base, and Swin-base. Params denote the parameters volume, and Top-1 acc means the top-1 accuracy in the ImageNet classification task.}
\scalebox{1.}{
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{c|cc}
\hline
Backbone               & Params(M)        & Top-1 acc($\%$) \\\hline
ConvNeXt-base~\cite{liu2022convnet}        & 88.59               & 85.8          \\ 
ViT-base~\cite{dosovitskiy2020image}              & 86.57              & 81.8       \\
Swin-base~\cite{liu2021swin}             & 87.77              & 85.2          \\ \hline
\end{tabular}}}

\label{backbone}
\end{table}

\begin{table}[t]
\caption{The pre-training results of 6 models on Kinetics-400 in both supervised and self-supervised settings. The supervised results are based on the single-view test, and the self-supervised ones are based on KNN evaluation.}
\centering
\small
\setlength{\tabcolsep}{6mm}{
\begin{tabular}{c|cc}
\hline
model           & Supervised       & SSL \\\hline
TSN             & 77.6               & 43.1  \\
TSM             & 76.4               & 43.2  \\
I3D             & 74.2                & 51.3    \\
NL     & 73.9               & 50.7  \\
TimeSformer     & 75.8               & 50.3  \\
VideoSwin      & 77.6               & 51.1  \\ \hline
\end{tabular}}

\vspace{-1.5em}
\label{tab:pretrained}
\end{table}



