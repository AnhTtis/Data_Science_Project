%------------------------------------------------------------------------
\begin{table*}[t]
\begin{center}
\caption{Statistics of the selected datasets used in our video benchmark. We collect 18 datasets covering 5 common data domains for comprehensive benchmarking. In the column of video viewpoint, ``sur.'' means surveillance videos, and ``dro.'' means drone videos. }
\scalebox{0.75}{
\label{table:data_info}
\begin{tabular}{c|ccccccccc}
\hline
\textbf{Dataset}   & \textbf{Domain}  & \textbf{\begin{tabular}[c]{@{}c@{}}Label\\ classes\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Clip\\ \textit{num.}\end{tabular}}  & \textbf{\begin{tabular}[c]{@{}c@{}}Avg Length\\ (sec.)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Training data\\ per class (min, max)\end{tabular}} &  \textbf{\begin{tabular}[c]{@{}c@{}}Split\\ ratio\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Video\\ source\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Video\\ viewpoint\end{tabular}}\\ \hline
XD-Violence~\cite{wu2020not}           & Anomaly       &   5   & 4135     &   14.94    & (36, 2046)    & 3.64:1         & Movies, sports, CCTV, etc.         & 3rd, sur. \\
UCF Crime~\cite{sultani2018real}             & Anomaly       &   12  & 600       &   132.51   & 38    & 3.17:1                 & CCTV Camera                        & 3rd, sur. \\
MUVIM~\cite{denkovski2022multi}                 & Anomaly       &   2   & 1127      &   68.1     & (296, 604)    & 3.96:1        & Self-collected                     & 3rd, sur. \\ \hline
WLASL100~\cite{li2020word}              & Gesture       &   100 & 1375     &   1.23        & (7, 20)    & 5.37:1         & Sign language website              & 3rd \\
Jester~\cite{materzynska2019jester}                & Gesture       &   27  & 133349  &   3        & (3216, 9592)  & 8.02:1          & Self-collected                     & 3rd \\
UAV Human~\cite{li2021uav}             & Gesture       &  155  & 22476   &   5        & (20, 114)   & 2:1               & Self-collected                     & 3rd, dro. \\ \hline
CharadesEgo~\cite{sigurdsson2018charades}      & Daily         &  157  & 42107  &  10.93    & (26, 1120)   & 3.61:1             & YouTube                            & 1st \\
Toyota Smarthome~\cite{das2019toyota}      & Daily         &   31  & 14262      &   1.78    & (23, 2312)    & 1.63:1        & Self-collected                     & 3rd, sur. \\
Mini-HACS~\cite{zhao2019hacs}             & Daily         &  200  & 10000   & 2        & 50       & 4:1                    & YouTube                            & 1st, 3rd\\
MPII Cooking~\cite{rohrbach2012database}          & Daily         &   67  & 3748          & 153.04    & (5, 217)   & 4.69:1        & Self-collected                     & 3rd \\ \hline
Mini-Sports1M~\cite{sports1m}         & Sports        &  487  & 24350     & 10 & 50       & 4:1                        & YouTube                            & 3rd \\
FineGym99~\cite{shao2020finegym}             & Sports        &  99   & 20389     & 1.65        & (33, 951) & 2.24:1            & Competition videos                 & 3rd \\
MOD20~\cite{perera2020multiviewpoint}                 & Sports        &   20  & 2324     & 7.4        & (73, 107)   & 2.29:1           & YouTube and self-collected         & 3rd, dro. \\ \hline
COIN~\cite{tang2019coin}                  & Instructional &  180  & 10426  & 37.01        & (10, 63)    & 3.22:1            & YouTube                            & 1st, 3rd\\
MECCANO~\cite{ragusa2021meccano}               & Instructional &   61  & 7880         & 2.82       & (2, 1157)    & 1.79:1      & Self-collected                     & 1st \\
INHARD~\cite{dallel2020inhard}                & Instructional &   14  & 5303        & 1.36  & (27, 955)    & 2.16:1            & Self-collected                     & 3rd \\
PETRAW~\cite{huaulme2022peg}                & Instructional &    7  & 9727      & 2.16       & (122, 1262)       & 1.5:1    & Self-collected                     & 1st \\
MISAW~\cite{huaulme2021micro}                 & Instructional &   20  & 1551     & 3.8       & (1, 316)   & 2.38:1             & Self-collected                     & 1st \\ \hline
\end{tabular}
}
\end{center}
\vspace{-1.5em}
% \end{sidewaystable}
\end{table*}

\section{Related Work}
\label{sec:related}

\noindent \textbf{Human action recognition} is to distinguish the ongoing actions (or sometimes events) in a video. Different from image classification, video action recognition requires effective temporal modeling~\cite{wang2016temporal}, awareness of the action hierarchies~\cite{shao2020finegym}, and the interaction between the subjects and objects~\cite{goyal2017something}. 
In early years, video models simply inherit the 2D convolution structures~\cite{vgg,resnet} and process temporal information either by extending 2D convolutions into 3D~\cite{tran2015learning,carreira2017quo,yang2021mutualnet} or including optical flow~\cite{simonyan2014two}. However, optical flow-based approaches suffer from costly flow pre-computation, thus 2D CNNs with more sophisticated temporal modeling are designed~\cite{wang2016temporal,zhu2018hidden,lin2019tsm, tsqnet, wu2019multi}.
For 3D CNNs, factorized architectures~\cite{p3d,tran2018closer,s3d,eco} are introduced to improve the model efficiency and reduce overfitting. 
Recently, Transformer~\cite{vaswani2017attention} continues to showcase its capability from language to image and also to video~\cite{bertasius2021space,arnab2021vivit,zhang2021vidtr,liu2022video,yu2023self,wu2021star}. Top performers on most video action recognition datasets are transformer-based. In this work, we fairly evaluate 6 popular video models belonging to 2D CNN, 3D CNN, and Transformer, respectively. With comparable backbones, we surprisingly reveal that 2D CNNs can sometimes outperform transformer models.


\noindent \textbf{Spatiotemporal representation learning} is advancing rapidly in the last few years, especially in a self-supervised manner.
Self-supervised pre-training is appealing because it could learn visual knowledge from massive unlabeled data, which alleviates the annotation burden compared with its supervised counterpart. 
Most approaches design a pretext task to learn the intrinsic spatiotemporal feature within the video data, such as sorting the shuffled video sequence~\cite{lee2017unsupervised}, next frame prediction~\cite{han2019video}, predicting the frame rate~\cite{epstein2020oops}, contrastive learning~\cite{ding2021motion,rhomoco,videomoco,vclr,qian2021spatiotemporal}, mask modeling~\cite{tong2022videomae,fbvideomae}, etc. 
Despite their promising performance, a recent work~\cite{thoker2022severe} points out that video self-supervised pre-training is less robust than its supervised counterpart when the downstream setting varies.
In this work, we also compare supervised pre-training with self-supervised ones in terms of both standard finetuning and few-shot finetuning on our benchmark. 

\noindent \textbf{Vision benchmark} is often designed as a testbed, which consists of multiple datasets from different domains. 
Each benchmark might have its own motivation, but they share the same goal of providing a unified protocol for evaluation and thus facilitating the evolution of a specific area. 
Many well-established benchmarks have been proposed in different research areas ~\cite{wang2019towards,zhai2019visual,li2022elevater,li2021grounded,gupta2022grit}. However, there is no such comprehensive benchmark for video action recognition. 
Two works that are the closest to ours are VTAB~\cite{zhai2019visual} and SEVERE-benchmark~\cite{thoker2022severe}.
VTAB contains 19 datasets that cover a broad spectrum of domains and semantics. 
All tasks are formulated as the image classification problem for the sake of a homogeneous task interface.
Inspired by VTAB, we build the first comprehensive evaluation benchmark for video action recognition.
BEAR includes 18 datasets across 5 domains towards real applications.
It enables fair comparison and thorough investigation of existing video models, which allows us to address interesting open questions.
SEVERE-benchmark~\cite{thoker2022severe} investigates how sensitive video self-supervised learning is to the current conventional benchmark in terms of domain, samples, actions, and tasks. 
Compared to SEVERE-benchmark~\cite{thoker2022severe}, we study both supervised and self-supervised learning in more domains (anomaly, instructional), with more datasets (18 vs 8) and more settings (few-shot, zero-shot, and unsupervised domain adaptation).