\begin{figure*}[htbp]
\centering
    \includegraphics[width=17.5cm]{figures/fewshot_new.pdf}
    \vspace{-2.5em}
    \caption{Results of few-shot learning based on supervised and self-supervised pre-training. The \textcolor[rgb]{0.4,0.8,0.4}{green} curves represent supervised pre-training and the \textcolor[rgb]{0,0,1}{blue} curves represent $\rho$MoCo self-supervised pre-training. We illustrate the results of TSM, 3D NonLocal, and VideoSwin for both pre-training methods. Additionally, we add the SOTA self-supervised pre-training method VideoMAE, represented by the \textcolor[rgb]{1,0,0}{red} curves, for comparison. It could be obvious that even the VideoMAE could lag a lot behind in the few-shot setting.}
    \label{fewshot}
\end{figure*}

\section{Few-shot Learning}
\label{sec:few-shot}

Compared with standard finetuning where abundant annotations can be utilized, few-shot learning is of more practical significance since annotating massive amounts of videos is notoriously expensive. To extend the investigation mentioned in Section \ref{sec:finetune},  we thoroughly investigate the capability of the selected 6 models on BEAR under a few-shot setting given both supervised and self-supervised pre-trained weights. Specifically, we consider (2,4,8,16)-shot settings, and for each setting, we randomly generate 3 splits and report the mean and standard deviation. 
Due to space constraints, we only select TSM, 3D NonLocal, and Video Swin to represent each model type for illustration as they perform generally better.  Complete few-shot results and the training details are in \textcolor{red}{Appendix~\ref{appendix few}}.
\paragraph{Model comparison.} 

The rankings of the six models in few-shot finetuning exhibit distinct variations compared to the standard finetuning. In contrast to the dominance of TSM in standard finetuning across both pre-training settings, the most effective models differ significantly across datasets in few-shot finetuning. Figure~\ref{fewshot} demonstrates that TSM no longer clearly outperforms other models in most datasets, and the two composite metrics (which are presented in the Supplementary due to space limitations) support this conclusion. Specifically, TSM and TimeSformer exhibit similar performance in supervised pre-training, whereas I3D and VideoSwin perform better in self-supervised learning. These findings further reveal the limitations of previous simple evaluation protocols, which may not provide a fair assessment of video models.
These results also confirm the necessity of BEAR, which emphasizes the importance of diverse downstream datasets and various settings for unbiased evaluation. 

\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
    \item \emph{The ranking relations between models could exhibit differently between standard and few-shot finetuning even within the same datasets. This finding further emphasizes the importance of our proposed BEAR benchmark, which advocates for a comprehensive evaluation approach that considers both dataset diversity and finetuning settings.}
\end{itemize}
\end{adjustwidth}

\paragraph{Impact of viewpoint change} 
As in standard finetuning, viewpoint change also has a severe impact when it comes to few-shot learning.
Comparing the results in Figure \ref{fewshot} with those in Table \ref{tab:finetune}, we can see that
the few-shot learning performance decreases drastically in general, especially in datasets that have less in common with Kinetics-400, such as UAV-Human, which is constructed by videos captured from unmanned aerial vehicles, FineGym, which contains fine-grained gym-related videos, and PETRAW and MISAW, which are simulated medical operations in the 1st person view. Conversely, in datasets that are more similar to Kinetics-400, these performance gaps are notably reduced. For example, even the 2-shot performance on Mini-HACS and MOD20 can reach approximately 60\% and 85\%, and the models achieve satisfying performance on the 16-shot setting on COIN.
In previous works, the homogeneity of the pre-training and downstream data hindered the timely identification of such phenomena in few-shot learning. Our investigation highlights the challenge of few-shot learning and underscores the importance of bridging the gap (as aforementioned, introducing extra data, such as Ego4D) between pre-training and the target data. 

Moreover, in the few-shot setting, self-supervised pre-training is more susceptible to viewpoint change. In challenging datasets such as UAV-Human and WLASL, few-shot learning can hardly obtain satisfying results based on self-supervised pre-trained weights, while in the 16-shot setting, supervised pre-training could provide comparable performance compared with standard finetuning. Similarly, in MOD20, the performance experiences a sharp decline in few-shot settings with self-supervised pre-training, while supervised pre-trained TSN and TSM can achieve accuracy exceeding 90\% in the 16-shot.


\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
    \item \emph{
    Few-shot finetuning remains a significant challenge in real-world scenarios.  The performance drops dramatically compared to standard finetuning especially when there is a large domain gap between pre-training and target data. However, when downstream datasets are similar to source data, the performance drop could be mitigated.
    }
    \item \emph{In few-shot learning, self-supervised pre-training is more vulnerable to viewpoint shift, while supervised pre-trained models can achieve favorable performance compared with standard finetuning on the 16-shot setting.}
\end{itemize}
\end{adjustwidth}


\paragraph{Self-supervised vs. supervised pre-training.} 
Comparing the blue curves to the green curves in Figure~\ref{fewshot}, we can see that self-supervised pre-training is generally less effective than supervised pre-training, which is consistent with the conclusion in Sec.~\ref{sec:finetune}. The performance gaps are pronounced in gesture datasets and are less significant in Mini-Sports1M, ToyotaSmarthome, etc. The performance gap is also different across different models. The largest gap appears in TSN and TimeSformer (the complete results are provided in \textcolor{red}{Appendix Table~\ref{tab:sup res1}-\ref{tab:ssl res3}}).
One reason for the poor performance of self-supervised learning may be the limitation of $\rho$MoCo. Therefore,
to consolidate our conclusion,
we further consider VideoMAE~\cite{tong2022videomae}, which is the SoTA self-supervised method and has demonstrated even better performance than supervised models on multiple datasets. Here, we use the officially released VideoMAE ViT-B model, which achieves 81.5\% Top-1 accuracy on Kinetics-400. However, comparing the results with our 6 supervised pre-trained models in Figure \ref{fewshot} (red vs. green curves), we show that VideoMAE could only be comparable with the best supervised pre-trained models in less than half of the datasets. 


\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
    \item \emph{Supervised pre-training shows consistent advantages over self-supervised ones in few-shot finetuning. Even the SoTA VideoMAE can hardly outperform simple supervised pre-trained models in diverse domains.}
\end{itemize}
\end{adjustwidth} 