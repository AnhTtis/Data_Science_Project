\begin{abstract}
% The goal of building a benchmark (suite of datasets) is to provide a unified protocol for fair evaluation and thus facilitate the evolution of a specific area. 
% Nonetheless, we point out that existing protocols of action recognition could yield partial evaluations due to several limitations in popular datasets. 
% % In this paper, we conduct extensive studies of spatiotemporal representation learning and propose a new \textbf{BE}nchmark on video \textbf{A}ction \textbf{R}ecognition, named \includegraphics[scale=0.07]{figures/bear.png}BEAR, to address the issues discovered in our experiments. 
% In this paper, we propose a new \textbf{BE}nchmark on video \textbf{A}ction \textbf{R}ecognition, named \includegraphics[scale=0.07]{figures/bear.png}BEAR, based on which we conduct extensive studies of spatiotemporal representation learning.
% % we conduct extensive studies of spatiotemporal representation learning
% BEAR is a collection of 18 video datasets grouped into 5 categories (anomaly, gesture, daily, sports, and instructional), which covers a diverse set of real-world applications. 
% With BEAR, we evaluate 6 common spatiotemporal models pre-trained in either a supervised or self-supervised manner. We also report transfer performance via standard finetuning, few-shot finetuning, and unsupervised domain adaptation. Our observation suggests that current state-of-the-arts cannot solidly guarantee high performance on datasets close to real-world applications and we 
% % For instance, in standard finetuning, the average top-1 accuracy on MECCANO is only 38.86$\%$; and this number severely drops to 11.22$\%$ in 16-shot case. Similarly, in UDA, the improvement brought by the current state-of-the-arts lags considerably behind the supervised learning results, e.g., when transferring from Toyota-Smarthome to MPII-Cooking, the difference between UDA methods and supervised learning results can be 57.58$\%$. 
% hope BEAR can serve as a fair and challenging evaluation benchmark to gain insights on building next-generation spatiotemporal learners. \textcolor{magenta}{Our dataset, code, and models will be released.}
The goal of building a benchmark (suite of datasets) is to provide a unified protocol for fair evaluation and thus facilitate the evolution of a specific area. 
Nonetheless, we point out that existing protocols of action recognition could yield partial evaluations due to several limitations. To comprehensively probe the effectiveness of spatiotemporal representation learning, we introduce \includegraphics[scale=0.07]{figures/bear.png}BEAR, a new \textbf{BE}nchmark on video \textbf{A}ction \textbf{R}ecognition. BEAR is a collection of 18 video datasets grouped into 5 categories (anomaly, gesture, daily, sports, and instructional), which covers a diverse set of real-world applications. 
With BEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by both supervised and self-supervised learning. We also report transfer performance via standard finetuning, few-shot finetuning, and unsupervised domain adaptation. Our observation suggests that the current state-of-the-art cannot solidly guarantee high performance on datasets close to real-world applications, and we hope BEAR can serve as a fair and challenging evaluation benchmark to gain insights on building next-generation spatiotemporal learners. \textcolor{magenta}{Our dataset, code, and models are released at: https://github.com/AndongDeng/BEAR}
\end{abstract}