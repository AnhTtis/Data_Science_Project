\section{Unsupervised Domain Adaptation}
\label{sec:uda}

\begin{table*}[h]
\caption{The unsupervised domain adaptation accuracy on our UDA datasets: Toyota Smarthome-MPII-Cooking (T: Toyota Smarthome, M: MPII-Cooking), Mini-Sports1M-MOD20 (MS: Mini-Sports1M, MOD: MOD20), UCF-Crime-XD-Violence (U: UCF-Crime, X: XD-Violence), PHAV-Mini-Sports1M (P: PHAV, MS: Mini-Sports1M), Jester, InHARD (I: InHARD, T: Top, L: Left, R: Right). }
\scalebox{0.85}{
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{c|cc|cc|cc|c|c|cccccc}
\hline
Settings & \multicolumn{7}{c|}{\textbf{Inter-dataset}} & \multicolumn{7}{c}{\textbf{Intra-dataset}}\\ \hline
Dataset               & T$\rightarrow$M & M$\rightarrow$T & MS$\rightarrow$MOD & MOD$\rightarrow$MS & U$\rightarrow$X & X$\rightarrow$U & P$\rightarrow$MS & Jester & IT$\rightarrow$IL & IT$\rightarrow$IR & IL$\rightarrow$IR & IL$\rightarrow$IT & IR$\rightarrow$IT & IR$\rightarrow$IL \\\hline
Source only                     & 5.32               & 7.36        & 18.25        & 12.76     & 54.20 & 33.33     & 61.45 & 68.73 & 4.18 & 30.39 & 19.01 & 22.65 & 24.14 & 12.42 \\ \hline
TA$^{3}$N~\cite{chen2019temporal}   & 11.17              & 15.38       & 23.77        & 19.15   & 59.91 & 44.44   & 65.79 & 71.44 & 5.78 & 41.83 & 27.91 & 28.08 & 35.66 & 14.68 \\
CoMix~\cite{sahoo2021contrast}   & 12.63              & 15.32       & 24.48        & 21.56     & 60.17 & 47.22    & 64.83 & 75.86 & 6.32 & 39.79 & 30.45 & 31.74 & 32.94 & 14.83 \\ \hline
Supervised target                  & 70.21              & 65.13        & 34.08        & 35.52   & 75.06 & 63.89   & 94.40 & 97.61 & 26.00 & 83.55 & 83.55 & 85.52 & 85.52 & 26.00 \\ \hline
\end{tabular}}}
\label{tab:uda}
\end{table*}

\begin{figure*}[t]
\centering
    \includegraphics[width=\linewidth]{figures/zeroshot.jpeg}
    \caption{Results of zero-shot evaluation. For most datasets in our benchmark, CLIP-based models still cannot provide reasonable results, especially for those challenging datasets with severe viewpoint shifts and fine-grained datasets.}
    \label{zero}
    \vspace{-2ex}
\end{figure*}

% Either finetuning or zero-shot learning considers the learning within a target dataset; however, 
In real-world scenarios, it is possible to transfer knowledge from similar datasets which are well-annotated to others with only limited labels. For instance, there are a lot of existing datasets that include samples of the same categories in the corresponding real-world tasks and thus can be used to facilitate model training. 
Nonetheless,  due to the domain gap, models directly trained on one dataset cannot be well generalized on the target data. In such case, unsupervised domain adaptation (UDA)~\cite{ganin2015unsupervised} can largely alleviate this distribution shift issue by learning the domain-invariant feature when labeled source data is available, learning representations that would promote the performance on the target domain. In BEAR, we construct several dataset pairs for UDA based on two different paradigms: inter-dataset adaptation and intra-dataset adaptation. Given that one of the features of our benchmark is that we collect several datasets with obvious viewpoint shifts, we also focus on this point when we build our UDA datasets. The details of the dataset statistics can be found in \textcolor{red}{Appendix~\ref{appendix uda}}.
We provide two common baseline results: `Source only' and `Supervised target'. The former directly evaluates the model trained on the source training set with the target test set, and the latter is the supervised learning performance on the target domain. Besides, we also evaluate two recent UDA algorithms on our benchmark: TA$^{3}$N~\cite{chen2019temporal} and CoMix~\cite{sahoo2021contrast}. 

\paragraph{Inter-dataset adaptation.}
Inter-dataset is constructed based on two different datasets that have different distributions, especially viewpoint change, but share common categories. Toyota Smarthome contains videos captured from 7 different cameras deployed in an apartment, while MPII-Cooking consists of videos from a down-view camera. Specifically, we select 6 new categories, which contains original action classes in Toyota Smarthome and MPII-Cooking, for the new Toyota Smarthome-MPII-Cooking dataset. The number of  videos is 5,233 and 943 for Toyota Smarthome and MPII-Cooking, respectively. 
Similarly, for Mini-Sports1M and MOD20, we select 15 categories to build the new dataset. In contrast to Toyota Smarthome-MPII-Cooking, the data distribution in Mini-Sports1M-MOD20 is much more balanced. There are 1,650 videos for Mini-Sports1M and 1,767 for MOD20.
We also consider the anomaly detection dataset. Basically, there are three shared action categories in UCF-Crime and XD-Violence: \textit{abuse}, \textit{fighting}, and \textit{shooting}. The domain shift in this dataset is also conspicuous: all the videos in UCF-Crime are from surveillance footage, where the target objects in video frames can only be in a small region, while most videos in XD-Violence are collected from action movies, which could record an action with abundant details. 
To provide a dataset for synthetic-to-real transfer, which is of great significance in real-world scenarios, we also include the simulated dataset PHAV~\cite{roberto2017procedural} to construct PHAV-Mini-Sports1M dataset. We combine 15 classes from Mini-Sports1M into 6 categories (\textit{playing soccer}, \textit{playing golf}, \textit{playing baseball}, \textit{shooting gun}, \textit{shooting archery} and \textit{running}) existing in PHAV to build the paired dataset. 

\paragraph{Intra-dataset adaptation. } 
Intra-dataset, on the contrary, is built within one dataset that records the same actions differently. 
We include Jester(S-T), which is initially introduced by \cite{sahoo2021contrast}, in BEAR since it has been a well-established dataset for domain adaptation. Each identical action in Jester with a contrary direction is merged into one category. 
We also construct a three-view dataset based on InHARD. Basically, each original frame in InHARD contains three distinguished views (i.e., top, left, and right). We simply split the frames according to the view and construct three sub-datasets as InHARD-Top, InHARD-Left, and InHARD-Right. We keep the category the same as the original dataset.



\paragraph{Challenging viewpoint adaptation.}
As shown in Table~\ref{tab:uda}, domain adaptation can be obviously challenging, especially in viewpoint change cases. For instance, ToyotaSmarthome and MPII-Cooking share similar attributes w.r.t. their actions, since they both record kitchen events. However, videos in ToyotaSmarthome are recorded via different cameras in the living room, while videos in MPII-Cooking are recorded by a down-view camera. The performance between these two datasets is far lower than the `supervised target'. Similar observations can also be obtained in InHARD. Although the adaptation is conducted within the dataset, recent methods still fail to perform well when adapting from one viewpoint to another. However, the gap between supervised target and UDA methods is much smaller in other UDA datasets where the viewpoint change is smaller. These results, along with the observations in Secs. \ref{sec:finetune} and \ref{sec:few-shot}, reveal that viewpoint change has a critical impact on transfer performance, which is hard to mitigate even with recent UDA algorithms.

