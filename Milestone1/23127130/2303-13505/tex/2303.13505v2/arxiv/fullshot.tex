\section{Standard Finetuning}
\label{sec:finetune}

Finetuning models that are pre-trained on large-scale datasets have been a mainstream learning paradigm in deep learning, and performance on various downstream datasets can provide a more comprehensive evaluation with less bias. Thus, in BEAR, we regard standard finetuning as a basic evaluation method. Specifically, we finetune the pre-trained models on the 18 datasets to investigate: 
1) the performance of different types of video models on different data domains; 2) the difference between supervised pre-training and self-supervised pre-training; 3) potential factors (\eg domain shift, viewpoint shift, etc.) that have significant impacts on the performance of downstream tasks. 
We want to emphasize that during finetuning, we do not tune hyperparameters on the test set to avoid potential overfitting. All reported results are based on the evaluation of the last checkpoint. The Top-1 accuracy of each model is presented in Table~\ref{tab:finetune}. Besides the performance on each dataset, we also propose two \textit{composite metrics} over the 18 datasets for evaluation. The first one is the macro-average accuracy which is the average of the accuracy on each dataset. The second one is micro-average accuracy, which calculates the average accuracy on the video level. Micro-average considers the size difference of the 18 datasets. We include the details of the complete finetuning results, and the previous best-reported performance, if any, for each dataset in \textcolor{red}{Appendix~\ref{appendix full}}.


\begin{table*}[!t]
    \caption{Finetuning results based on the supervised pre-trained and self-supervised pre-trained models as well as the X3D pre-trained models. Generally, from the two composite metrics (macro-average accuracy and micro-average accuracy), we can tell that TSM surprisingly outperforms other counterparts in both pre-training settings.}\label{tab:finetune}
    \centering
    \scalebox{0.72}{
    \begin{tabular}{c|cccccc|c|cccccc}
          \hline
          \multirow{2}*{\textbf{Dataset}} & 
          \multicolumn{6}{c|}{\textbf{Supervised pre-training}} & 
          \multirow{2}*{\textbf{X3D}} &
          \multicolumn{6}{c}{\textbf{Self-supervised pre-training}}  \\
          \cline{2-7}
          \cline{9-14}
                                      &  \textbf{TSN} &\textbf{TSM} & \textbf{I3D} & \textbf{NL} & \textbf{TimeSformer} & \textbf{VideoSwin}   & ~      & \textbf{TSN} &\textbf{TSM} & \textbf{I3D} & \textbf{NL} & \textbf{TimeSformer} & \textbf{VideoSwin}   \\ \hline
         \textbf{XD-Violence}         & \textbf{85.54}         & 82.96       & 79.93       & 79.91       & 82.51                & 82.40       & 75.11        & 80.49        & \textbf{81.73}       & 80.38        & 80.94       & 77.47                & 77.91                \\ 
         \textbf{UCF-Crime}           & 35.42         & \textbf{42.36}       & 31.94         &  34.03      & 36.11                & 34.72     & 25.69          & \textbf{37.50}        & 35.42       & 34.03        &  34.72      &   36.11             & 34.03               \\ 
         \textbf{MUVIM}               & 79.30         & \textbf{100}         &  97.80      &  98.68      & 94.71        &  \textbf{100}     & 99.56          & 99.12        & \textbf{100}         & 66.96        & 66.96       & 99.12                & \textbf{100}  \\ \hline
         \textbf{WLASL}               & 29.63        & 43.98       &   49.07      &  \textbf{52.31}    & 37.96                & 45.37         & 44.91       & 27.01        & 27.78       & 29.17        & \textbf{30.56}       &   25.56             & 28.24            \\ 
         \textbf{Jester}              & 86.31         & \textbf{95.21}       & 92.99        &  93.49      & 93.42                & 94.27      & 92.24         & 83.22        & \textbf{95.32}       & 87.23        &  93.89      & 90.33                & 90.18              \\ 
         \textbf{UAV-Human}           & 27.89         & \textbf{38.84}       &  33.49       & 33.03         & 28.93                & 38.66    & 36.07          & 15.70        & 30.75       & 31.95        &26.28        & 21.02               & \textbf{35.12}          \\ \hline
         \textbf{CharadesEGO}         & 8.26          & 8.11        &  6.13        & 6.42         & \textbf{8.58}                 & 8.55      & 5.69   & 6.29         & 6.59        & 6.24         & 6.31        & 7.59            & \textbf{7.65}             \\ 
         \textbf{Toyota Smarthome}    & 74.73         & \textbf{82.22}       & 79.51        & 76.86      & 69.21                & 79.88       & 79.09          & 68.71        & \textbf{81.34}       & 77.82        & 76.16       & 61.64                  & 80.18              \\ 
         \textbf{Mini-HACS}           & 84.69         & 80.87       &77.74         &  79.51      & 79.81                & \textbf{84.94}       & 60.57          & 64.60        & 63.24       & 70.24        & 60.57       & 73.92                  &  \textbf{75.58}           \\ 
         \textbf{MPII Cooking}        & 38.39         & 46.74       & \textbf{48.71}         &    42.19    & 40.97                & 46.59      &  42.19        & 34.45        & \textbf{50.08}       & 42.79        & 40.36       & 35.81               & 47.19            \\ \hline
         \textbf{Mini-Sports1M}       & 54.11         & 50.06       &46.90         & 46.16          & 51.79                & \textbf{55.34}     & 41.91         & 43.02        & 43.59       & 46.28        & 45.56       & 44.60                & \textbf{47.60}              \\ 
         \textbf{FineGym}             & 63.73         & \textbf{80.95}       &   72.00         & 71.21         & 63.92                & 65.02   & 68.49           & 54.62        & \textbf{75.87}       & 69.62        & 68.79       & 47.60                & 58.94              \\ 
         \textbf{MOD20}               & \textbf{98.30}         & 96.75       &  96.61      &  96.18        & 94.06                & 92.64       &  92.08        & 91.23        & 92.08       & 91.94        & 92.08       & 90.81               & \textbf{92.36}             \\ \hline
         \textbf{COIN}                & 81.15         & 78.49       & 73.79        & 74.30        & \textbf{82.99}                & 76.27       & 61.29       & 61.48        & 64.53       & 71.57        & \textbf{72.78}       & 67.64                  & 68.78              \\ 
         \textbf{MECCANO}             & \textbf{41.06}         & 39.28       & 36.88        & 36.13          & 40.95                & 38.89      & 30.78         & 32.34         & 35.10        & 34.86        & 33.62        & 33.30              & \textbf{37.80}              \\ 
         \textbf{InHARD}              & 84.39         & \textbf{88.08}       &  82.06      & 86.31       & 85.16                & 87.60          & 84.86      & 75.63          & \textbf{87.66}         & 82.54        & 80.81         & 71.28           & 80.10              \\ 
         \textbf{PETRAW}              & 94.30         & 95.72       & 94.84        & 94.54      & 94.30                & \textbf{96.43}          & 95.46    & 93.18        & \textbf{95.51}       & 95.02        & 94.38       & 85.56                  &  91.46             \\ 
         \textbf{MISAW}               & 61.44         & \textbf{75.16}       &  68.19     & 64.27        & 71.46                & 69.06         & 69.06     & 59.04        & \textbf{73.64}       & 70.37        & 64.27       & 60.78                  &  68.85             \\ \hline
         Macro Avg.                 & 62.70     & \textbf{68.10}        & 64.92       & 64.75        & 64.27       & 66.48               &  61.39      & 57.09         & \textbf{63.35}       &  60.50     & 59.39        & 57.23                & 62.33               \\ 
         Micro Avg.              & 64.92        & \textbf{70.82}       & 67.81        & 67.83       & 67.66                  &  69.73         & 65.87      & 59.13         & \textbf{68.11}       &  64.35     & 66.21        & 62.19               & 65.71         \\ \hline
    \end{tabular}}
    % \vspace{-1em}
\end{table*}


% \vspace{-1 em}
\paragraph{Model comparison.} 
In previous studies, transformer-based video models~\cite{bertasius2021space,liu2022video} have been demonstrated to be more effective than CNNs on several representative datasets. This conclusion leads the trend of model design toward more sophisticated transformers, which makes CNNs less appealing compared with the pre-transformer era. However, we argue that the current conclusion could be biased since the comparison between transformers and CNNs is obviously unfair. Basically, it is a widely accepted notion that the selection of different backbones can inherently yield significant differences, let alone the overall model design. To this end, as aforementioned, we carefully select ConvNeXt~\cite{liu2022convnet} as the CNN backbone, which is comparable with ViT~\cite{dosovitskiy2020image} and Swin Transformer~\cite{liu2021swin} w.r.t. both model size and ImageNet classification performance. We believe such a fair comparison could lead to more convincing and compelling conclusions. As shown in Table~\ref{tab:finetune}, we notice that there is no absolute winner among all the models, but surprisingly, 2D CNNs perform better on most datasets, especially TSM, which outperforms other models in 8 out of 18 datasets. This indicates that 2D video models are still competitive with transformers when equipped with strong backbones.
Likewise,  the two composite metrics also provide evidence that TSM outperforms other models, and transformer-based models do not exhibit clear advantages over CNN-based models. 

Inspecting further, we can see that VideoSwin excels in mini-HACS and mini-Sports1M. However, as aforementioned, these datasets, along with other popular datasets such as UCF-101 and HMDB-51, share high similarities with Kinetics-400 in terms of actions and viewpoints. Thus the performance on these datasets may not fully reflect the effectiveness of the evaluated model. Indeed, as shown in Table \ref{tab:finetune}, VideoSwin is only comparable or inferior to TSM in the other three categories (\ie, anomaly, gesture, and instructional). This demonstrates that the impressive performance on Kinetics-400 and other similar datasets may not be consistent with downstream tasks with vastly different actions. To fully probe the effectiveness of a video model, we need to evaluate it on datasets with different distributions. Besides, we also consider the NAS-based X3D~\cite{feichtenhofer2020x3d}, which achieves good performance on Kinetics-400, to reveal the overfitting problem of tuning on the test set.


\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
\item \emph{Despite the emergence of recent transformers, 2D video models can still be promising alternatives for action recognition if equipped with powerful backbones.}
\item \emph{Previous evaluation protocols have been limited to target datasets similar to Kinetics-400, which could potentially result in biased evaluations. However, BEAR could address this issue by including target data from five distinct domains, ensuring a more comprehensive and unbiased assessment of model performance.}
\end{itemize}
\end{adjustwidth}


\paragraph{Impact of viewpoint change} 
We also observe something interesting in terms of the data distribution. Several datasets such as UCF-Crime, UAV-Human, CharadesEGO, MPII-Cooking, and MECCANO exhibit notably low performance. Upon closer inspection of Table~\ref{table:data_info}, it is evident that these datasets involve significant viewpoint changes from Kinetics-400. For instance, UCF-Crime is collected from CCTV footage, UAV-Human contains drone-view videos, CharadesEGO only contains 1st person-view videos, and MECCANO is also egocentric. This indicates that the viewpoint change in downstream tasks could dramatically damage the model performance.
Therefore, leveraging pre-training datasets with rich egocentric visual knowledge, such as EGO4D~\cite{grauman2022ego4d}, may offer a suitable alternative to Kinetics-400 for finetuning on egocentric data.
Besides, in Sec.~\ref{sec:few-shot} and Sec.~\ref{sec:uda}, we will further discuss the challenge caused by the viewpoint change in the target domain. 

\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
    \item \emph{Prior evaluation protocols, limited in the scope of target data, fail to capture the impact of domain gap, particularly in regard to the viewpoint, 
    on transfer performance. However, we have identified that such a distribution shift can significantly degrade the quality of spatiotemporal representation, which further undermines the transfer performance. Hence, we recommend that future studies should include pre-training datasets beyond Kinetics-400 to provide more robust representations to improve transferability.}
\end{itemize}
\end{adjustwidth}

\noindent\textbf{Self-supervised vs. supervised pre-training} 
As can be seen from Table~\ref{tab:finetune}, it is notable that the overall finetuning performance of the self-supervised pre-training is less competitive than its supervised counterpart even for TSM. The most pronounced accuracy drop can be found in WLASL and FineGym. The performance of 3D Nonlocal network on WLASL drops from 52.31$\%$ to 30.56$\%$ and the performance of TimeSformer also decreases more than 15$\%$. 
To reveal the potential reason behind this, we further scrutinize the data distribution gap between the selected 18 target datasets and Kinetics-400. We observe different types of domain shifts, such as UAV-Human containing only drone-view data and the egocentric MECCANO which differs significantly from Kinetics-400. We conclude that self-supervised pre-training is more susceptible to domain shifts between Kinetics-400 and the target datasets than supervised pre-training. In Sec.~\ref{sec:few-shot}, we take a step forward on this topic by investigating few-shot settings, which are more likely to occur in real-world scenarios.


\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
    \item \emph{Self-supervised finetuning generally cannot outperform its supervised counterpart and TSM consistently performs well under the self-supervised setting.}
\end{itemize}
\end{adjustwidth}