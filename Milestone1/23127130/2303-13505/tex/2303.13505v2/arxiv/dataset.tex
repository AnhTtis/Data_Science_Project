\section{\includegraphics[scale=0.1]{figures/bear.png}BEAR}
\label{sec:datasets}


Despite new datasets being introduced every year, the most widely adopted benchmarks in the video action recognition community are Kinetics-400/600/700~\cite{k600,k700,kay2017kinetics}, Something-something-v1/v2~\cite{goyal2017something}, UCF-101~\cite{soomro2012ucf101} and HMDB-51~\cite{hmdb51}. 
However, these datasets share a high similarity in that they are mostly composed of daily and sports actions. 
Models that achieve good performance on these datasets may not generalize well to the challenging real-world scenarios due to dramatic domain shifts. 
For example, anomaly videos are often captured from surveillance cameras, which look quite different from daily videos due to viewpoint change. 
Ideally, a video model is expected to cope with diverse real-world applications.

To comprehensively evaluate the generalization capability of video models, we present BEAR, a new benchmark for human action recognition.
As shown in Table~\ref{table:data_info}, BEAR is a collection of 18 action recognition datasets, carefully designed towards \textit{practical use}, \textit{data diversity}, and \textit{task diversity}. Compared to existing video action recognition datasets, BEAR has the following desirable properties.


\noindent \textbf{Real Applications.} Besides the common daily and sports categories, BEAR contains another three categories including anomaly activity, gesture, and instructional actions. These action categories have important real-world applications such as people fall detection (\eg MUVIM~\cite{denkovski2022multi}), sign language recognition (\eg WLASL100~\cite{li2020word}), industrial inspection (\eg MECCANO~\cite{ragusa2021meccano}), and surgical workflow recognition (\eg  PETRAW~\cite{huaulme2022peg}).

\noindent  \textbf{Data Diversity.} BEAR is not only diverse in application domains but also in the data source, video viewpoint, and video length. As shown in Table~\ref{table:data_info}, BEAR contains videos from various sources such as movies, CCTV cameras, YouTube, and drone cameras. It also includes videos in the 1st and 3rd person views. In terms of video length, the average clip duration varies from the shortest (\eg 1.23s in WLASL100~\cite{li2020word}) to the longest (\eg 153.04s in MPII Cooking~\cite{rohrbach2012database}).
In addition, the training sample size per class varies across datasets, from the lowest (\eg 1 for MISAW~\cite{huaulme2021micro}) to the highest (\eg 9592 for Jester~\cite{materzynska2019jester}).

\noindent \textbf{Few-shot Transfer.} The standard finetuning protocol for transfer is to train a model on the whole training data, which is often more than thousands of videos. 
However, in many real applications, the annotated video data is scarce, \eg anomaly recognition (rarely happens and is costly to label), medical operation (privacy concern), and industrial operation (need the expertise to label). 
To better evaluate a model's potential in real applications, we need to evaluate its effectiveness under few-shot learning.
Hence in BEAR, besides the full datasets, we also split each dataset into 16-shot, 8-shot, 4-shot, and 2-shot versions. This allows researchers and practitioners to thoroughly evaluate a model's sensitivity to data scarcity.


\noindent \textbf{Flexible Evaluation.} Thanks to the data diversity in BEAR, researchers can easily evaluate video models under various settings. For example, full-shot and few-shot learning, domain adaptation from one dataset (or category) to another. Moreover, we also believe that new settings can be easily derived based on our benchmark.


\noindent \textbf{Fair Comparison.} 
The held-out test set for most video action recognition datasets either does not exist or is not commonly adopted. This allows previous methods to conduct hyperparameter optimization or even neural architecture search directly on the test set. Test set tuning usually leads to good testing performance, but it may not translate to other datasets. \textit{To promote fair comparison and generalization capability, we will hold the test sets and provide an evaluation server for future researchers and practitioners.}

\noindent \textbf{Dataset Accessibility.} We provide scripts to download and format all 18 datasets automatically. Our codebase is built upon MMAction2~\cite{2020mmaction2}, so researchers can easily integrate their new models by providing a model definition file without additional efforts to perform evaluations. Furthermore, the total number of video clips in BEAR is about 310K, which is comparable to Kinetics-400. \textit{Therefore, the overall time cost is similar to training a model on Kinetics-400.}
