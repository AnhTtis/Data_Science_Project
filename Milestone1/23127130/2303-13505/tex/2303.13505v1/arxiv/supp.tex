Below we first briefly describe the selected models and then their implementation details during pre-training.

% Traditional convolutional action recognition networks before 2017 are mostly built to process single frame or multiple consecutive frames; however, such simple structures overlook the importance of long-range temporal context in action recognition, which somehow underestimates the intrinsic temporal information within videos. 
Temporal segment networks (TSN) proposes segment-based sampling to learn temporal information across frames. 
Specifically, in TSN, a video is evenly divided into several temporal segments, which one random frame is sampled from. 
Then the output from each segment will be aggregated via pooling to obtain the final prediction. 
Temporal Shift Module (TSM) shifts feature channels along the temporal axis, which facilitates information exchanged among neighboring frames. 
It can be plug-and-played in 2D networks to enable stronger temporal modeling at zero computation and zero parameters.
Thus, TSM can achieve the performance of heavy 3D CNNs while maintaining the efficiency of 2D CNNs.
% TSM introduces stronger temporal learning capacity to 2D networks while maintaining light-weight. 

Inflated 3D ConvNet (I3D) is designed to bootstrap from the corresponding 2D network since (1) the architecture of 2D network is well designed and (2) the  weights of 2D network is well pre-trained, e.g., Inception~\cite{inception} $\rightarrow$ Inception-I3D~\cite{carreira2017quo}. 
% utilize pre-trained weights from the corresponding 2D network since these 2D weights have been well-designed and trained to perceive visual concepts.
I3D initializes its 3D kernels by duplicating the 2D ones along the temporal dimension, which helps the convergence of 3D CNNs. 
Inspired by~\cite{vaswani2017attention}, non-local networks (NL) adapts the non-local operation (i.e., self-attention~\cite{vaswani2017attention}) in its building block to model long-range dependency.
For video action recognition, its goal is to relate the same object, or person-object interaction within a distant time interval in videos.
Similar to TSM, non-local block is compatible to most convolutional networks.


TimeSformer is a pure transformer-based model, which is an extension of ViT~\cite{dosovitskiy2020image} to the spatiotemporal space. 
Given the quadratic complexity of self-attention, TimeSformer compares several attention strategies when considering temporal dimention in videos.
Finally, TimeSformer introduces the divided space-time attention to greatly reduce the computation burden but achieves promising results.
% on most video action recognition datasets. 
% This structure shows both effectiveness and efficiency in their reported results. 
Continuing this modeling shift from CNNs to Transformers, VideoSwin extends Swin Transformer~\cite{liu2021swin} by adding the inductive bias of locality in video transformers. 
Simply speaking, it adapts the idea of 2D shifted window self-attention to 3D space, which results in better speed-accuracy trade-off compared to previous approaches~\cite{bertasius2021space,arnab2021vivit}.
% Similarly, VideoSwin is an extension of Swin Transformer~\cite{liu2021swin}, by adapting the 2D shifted window self-attention to 3D.
% And shifted window ensure the connection across distant regions in the spatiotemporal tensors.


\begin{figure}[t]
\centering
    \includegraphics[width=8cm]{figures/radar_new.pdf}
    \caption{The rank of the averaged performance within different data domains for the 6 models in different settings. The most outside in these radar images means the highest performance. For each domain, we average the top-1 accuracy as the scores in finetuning and average the top-1 accuracy of 16-shot results in few-shot learning. Complete results are shown in Table~\ref{tab:finetune} and Figure~\ref{fewshot}.}
    \label{radar}
\end{figure}