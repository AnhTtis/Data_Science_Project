\section{Conclusion and Discussion}
% 1. contribution
% 2. fair comparison
In this work, we introduce a new action recognition benchmark \includegraphics[scale=0.07]{figures/bear.png}BEAR to address several limitations in existing video benchmarks. Aiming at benefiting both academic and industrial applications, we carefully select 18 datasets covering 5 distinct data domains.
% : anomaly videos, daily videos, sports videos, gesture videos, and instructional videos. 
Such a wide scope could provide comprehensive assessment protocols for any video model, filling the gap in the current video action recognition benchmark that only a small number of target datasets are considered. It helps prevent models from overfitting on a specific dataset which could result in biased model evaluation. 
% Moreover, to achieve a fair comparison, we held out test data for every dataset and avoid using it for parameter selection during training, and the evaluation is based on the last checkpoint. Meanwhile, in this work, we also pay attention to the capabilities of 2D CNNs, 3D CNNs, and transformers. 
% We carefully select comparative backbones for them to avoid erroneous comparisons. 
% Surprisingly, TSM equipped with ConvNeXt can outperform VideoSwin, which is one of the SOTA methods in multiple video action recognition datasets. We believe that this could provide some clues for the video community to scrutinize the effectiveness of 2D CNNs as well as video transformers. 

Based on our extensive experiments, we have several interesting and instructive observations: 1) 2D video models are competitive with SoTA transformer models when equipped with strong backbones. 2) Previous evaluation protocols on a few similar datasets can yield biased evaluation. 3) Domain shift (especially the viewpoint shift) has a large impact on transfer learning, and the performance gap could be much more remarkable in the few-shot setting. 4) Self-supervised learning still largely falls behind supervised learning, and even the SoTA VideoMAE cannot outperform supervised models on diverse downstream datasets. Moreover, we also point out that in order to learn robust spatiotemporal representations, constructing new pre-training datasets containing videos from diverse domains could benefit the target performance on a wide range of datasets. Due to space limits, we only consider evaluation datasets and leave the art of training data construction to future work.

% Accordingly, we propose several suggestions for future research in video action recognition: 1) in future model design, the visual backbone (e.g., ConvNeXt) should be considered equally important as temporal modeling capacity; 2) BEAR could be a better alternative to the current evaluation paradigm since it contains datasets from varied domains; 3) in order to improve downstream tasks performance in several domains (\eg egocentric), it is critical to construct a unified and comprehensive pre-training dataset which contains data which is favorable to the target data (\ie Ego4D~\cite{grauman2022ego4d}); 4) supervised pre-training should still be the chosen method in real-world scenarios.

% Besides, we also consider a crucial question in video action recognition: how much does the self-supervised pretraining fall behind the supervised one? We draw our conclusion based on both standard finetuning and few-shot learning. As previously discussed in ~\cite{thoker2022severe}, the performance of self-supervised pretraining is largely influenced by the downstream domains, and even the best self-supervised pretraining cannot surpass the regular supervised one. 
% In this work, we experimentally consolidate this claim via few-shot learning, which, from our results, even amplifies the difference between self-supervised and supervised pretraining. 
% In this work, we experimentally consolidate this claim via few-shot learning.
% Based on our observation, several factors, such as temporal duration, domain shift (especially the viewpoint shift), and fine-grained attribute, can all have an obvious impact on this performance gap. This point also provides us with a clue that it is critical to construct a unified and comprehensive pretraining dataset for various downstream tasks. For instance, in order to improve the performance on CharadesEgo, we must ensure there is enough egocentric visual knowledge contained in the pretraining weights. Thus, Kinetics-400 is far from qualified, and particular large-scale 1st person video datasets, such as Ego4D~\cite{grauman2022ego4d}, can be considered. 
% In this work, we only build the evaluation datasets to evaluate different pretrained models. Based on our results and conclusions, we will build a more comprehensive pretraining dataset to enable more generalized video representation learning.
