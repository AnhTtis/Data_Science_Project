%-------------------------------------------------------------------------

\section{Models}
There has been a considerable amount of video models proposed to solve the human action recognition task. 
From the perspective of the basic building block, these models can be roughly classified into three categories: 2D CNNs, 3D CNNs, and transformer-based models. 
To investigate the efficacy of each model type, in this work, we select two representative works from each category: TSN~\cite{wang2016temporal} and TSM~\cite{lin2019tsm} for 2D CNNs, I3D~\cite{carreira2017quo} and 3D Non-local network~\cite{wang2018non} for 3D CNNs, TimeSformer~\cite{bertasius2021space} and VideoSwin~\cite{liu2022video} for transformer-based models. 
We would like to point out that for CNN-based models (TSN, TSM, I3D, and NL), we choose ConvNext-base~\cite{liu2022convnet} as the backbone because it has a similar model size and performance on ImageNet-1K compared to ViT-B and Swin-B, which is the backbone of TimeSformer and VideoSwin, respectively. 
This alleviates the impact from the backbone, thus presenting a more fair comparison among different video architectures. 
In this work, we finetune all the models based on both supervised and self-supervised pre-training on Kinetics-400, and the pre-training performance is shown in Table \ref{tab:pretrained}. The pre-training details can be found in \textcolor{blue}{Appendix~\ref{appendix_pretraining}}.
% We believe an extensive and fair study could provide more sophisticated views on the intrinsic mechanism of video understanding. 
% \paragraph{Pre-training implementation} 
% In this work, we finetune all the models based on both supervised and self-supervised pre-training. We follow the common practice~\cite{carreira2017quo, bertasius2021space, liu2022video} to first pre-train a model on Kinetics-400 (K400) dataset and then finetune it on each downstream dataset in BEAR.
% For supervised pre-training, we mostly follow the default training recipe of each model. 
% For self-supervised pre-training, we follow $\rho$MoCo~\cite{rhomoco} for all the selected models. 
% More detailed pre-training settings can be found in Supplementary. 
% For model pre-training, we simply need a reproducible and fair setup to enable comparison.
% Specifically, we study two pre-training schemes, supervised learning and self-supervised learning (SSL). 
% We present the performance of six different models on K400 validation set during pre-training in Table \ref{pretrained}. 
% For supervised pre-training, classification accuracy using single-view testing is reported, since single-view testing can highlight the differences among models, while multi-view testing, as a test time augmentation to reduce the differences, might bury interesting insights.
% For self-supervised pre-training, classification accuracy using KNN is reported, thus lower scores are observed. Interestingly, we can see that by using backbone of similar capacity, the pre-training performance of six different models are often close, especially under supervised pre-training.
% In supervised pretraining, we can notice that 3D CNNs averagely perform worse than 2D CNNs and transformers, which indicates their poorer spatiotemporal modeling ability compared with the counterparts. 
% Second, SSL perfo
% In self-supervised pretraining, the lower numbers compared to supervised ones are because different evaluation methods. 
% However, judging model effectiveness solely based on pre-training results on one dataset (i.e., Kinetics-400) might be biased~\cite{newell2020useful}.
% , since pre-training performance could be inconsistent to transfer performance.
In the following sections, we will provide a comprehensive study of models' transfer performance from multiple perspectives: standard finetuning, few-shot finetuning, unsupervised domain adaptation, and zero-shot evaluation.

% we first perform standard finetuning in Sec~\ref{sec:finetune}; we further study few-shot finetuning in Sec~\ref{sec:few-shot} to stress test each model under real-world scenarios; we also investigate unsupervised domain adaptation in Sec~\ref{sec:uda} and zero-shot evaluation in Sec~\ref{sec:zeroshot}.
% To be specific, we first perform standard finetuning in Sec~\ref{sec:finetune}, i.e., using all the training data from each dataset for finetuning. This is to have a basic understanding of the transfer performance of six video models on our BEAR.
% Then we study few-shot finetuning in Sec~\ref{sec:few-shot} to stress test each model under real-world scenarios where limited training data and computing resources are available.
% % , i.e., only use 2/4/8/16 shot of each category during finetuning. 
% % This is to stress test each model under real-world scenarios where limited training data and computing resources are available.
% Furthermore, we construct several new source-target data based on several datasets in BEAR to investigate unsupervised domain adaptation (UDA) in Sec~\ref{sec:uda}.
% Lastly, we also leverage recent vision-language models for zero-shot evaluation in Sec~\ref{sec:zeroshot}.
% Furthermore, inspired by recent paradigm shift to language-guided visual recognition, we leverage vision-language models to conduct zero-shot classification on BEAR in Sec~\ref{sec:zeroshot}.
% Lastly, we introduce multiple unsupervised domain adaptation settings to evaluate the transfer performance across datasets in or beyond BEAR in Sec~\ref{sec:uda}.

% To fulfill a comprehensive comparative studies as well as to showcase our challenging benchmark, in the following sections, we will conduct different transfer learning experiments to provide a complete view towards action recognition to date.

% We also illustrate the average performance on 5 data domains for the 6 models. The results are obtained from detailed standard finetuning and few-shot learning in Section~\ref{finetune} and Section~\ref{few-shot}. As shown in Figure~\ref{radar}, in standard finetuning, TSM obviously has higher performance in most domains; although TSN and VideoSwin have the highest pretraining accuracy. Similarly, 3D NonLocal, TimeSformer, and VideoSwin perform relatively better in few-shot learning, while the I3D performs best in KNN evaluation.

% Ever since the born of large-scale datasets, researchers have discovered the massive transferable knowledge within them and design various of methods to boost downsteam tasks. One of the most popular and effective way is to first pretrain on this large dataset to obtain visual knowledge from it. In this benchmark, one of our main targets is to investigate the difference between supervised and self-supervised pretraining. Intuitively, we pretrain all of the 6 models on Kinetics400 in both pretraining setting. In addition, to fairly compare among the 6 models, we choose ConvNeXt-base~\cite{liu2022convnet} as the backbone for the CNNs (i.e., TSN, TSM, I3D and 3D NonLocal), since its volume (89M) is close to ViT (87M) and Swin-B (88M), which are the backbones of TimeSFormer and VideoSwin, respectively. We follow a  standard frame sampling strategy that sampling 8 frames in total and the interval is set as 16 frames.  In supervised pretraining, the weights for TimeSFormer are obtained from a third-party implementation\footnote{https://github.com/open-mmlab/mmaction2/tree/master/configs/\\recognition/timesformer}. The optimizer for CNNs is SGD, while in VideoSwin we utilize AdamW. The total training epochs for CNNs is 50 and 30 for transformers.



\begin{table}[t]
\caption{The pre-training results of different models on Kinetics-400 in both supervised and self-supervised settings. The supervised results are based on single-view testing, and the self-supervised results are based on KNN evaluation.}
\centering
\small
\setlength{\tabcolsep}{6mm}{
\begin{tabular}{c|cc}
\hline
model           & Supervised       & SSL \\\hline
TSN             & 77.6               & 43.1  \\
TSM             & 76.4               & 43.2  \\
I3D             & 74.2                & 51.3    \\
NL     & 73.9               & 50.7  \\
TimeSformer     & 75.8               & 50.3  \\
VideoSwin      & 77.6               & 51.1  \\ \hline
\end{tabular}}
% \vspace{-0.5em}

\vspace{-1.5em}
\label{tab:pretrained}
\end{table}



