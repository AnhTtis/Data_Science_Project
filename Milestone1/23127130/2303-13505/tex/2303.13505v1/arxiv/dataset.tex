
% \begin{table*}
% \begin{center}
% \caption{Statistics of the selected datasets used in our video benchmark. We collect 18 datasets covering 5 common data domains for comprehensive benchmarking. In the column of video viewpoint, ``sur.'' means surveillance videos, and ``dro.'' means drone videos. }
% \scalebox{0.75}{
% \label{table:data_info}
% \begin{tabular}{c|ccccccccc}
% \hline
% \textbf{Dataset}   & \textbf{Domain}  & \textbf{\begin{tabular}[c]{@{}c@{}}Label\\ classes\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Clip\\ \textit{num.}\end{tabular}}  & \textbf{\begin{tabular}[c]{@{}c@{}}Avg Length\\ (sec.)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Training data\\ per class (min, max)\end{tabular}} &  \textbf{\begin{tabular}[c]{@{}c@{}}Split\\ ratio\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Video\\ source\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Video\\ viewpoint\end{tabular}}\\ \hline
% XD-Violence~\cite{wu2020not}           & Anomaly       &   5   & 4135     &   14.94    & (36, 2046)    & 3.64:1         & Movies, sports, CCTV, etc.         & 3rd, sur. \\
% UCF Crime~\cite{sultani2018real}             & Anomaly       &   12  & 600       &   132.51   & 38    & 3.17:1                 & CCTV Camera                        & 3rd, sur. \\
% MUVIM~\cite{denkovski2022multi}                 & Anomaly       &   2   & 1127      &   68.1     & (296, 604)    & 3.96:1        & Self-collected                     & 3rd, sur. \\ \hline
% WLASL100~\cite{li2020word}              & Gesture       &   100 & 1375     &   1.23        & (7, 20)    & 5.37:1         & Sign language website              & 3rd \\
% Jester~\cite{materzynska2019jester}                & Gesture       &   27  & 133349  &   3        & (3216, 9592)  & 8.02:1          & Self-collected                     & 3rd \\
% UAV Human~\cite{li2021uav}             & Gesture       &  155  & 22476   &   5        & (20, 114)   & 2:1               & Self-collected                     & 3rd, dro. \\ \hline
% CharadesEgo~\cite{sigurdsson2018charades}      & Daily         &  157  & 42107  &  10.93    & (26, 1120)   & 3.61:1             & YouTube                            & 1st \\
% Toyota Smarthome~\cite{das2019toyota}      & Daily         &   31  & 14262      &   1.78    & (23, 2312)    & 1.63:1        & Self-collected                     & 3rd, sur. \\
% Mini-HACS~\cite{zhao2019hacs}             & Daily         &  200  & 10000   & 2        & 50       & 4:1                    & YouTube                            & 1st, 3rd\\
% MPII Cooking~\cite{rohrbach2012database}          & Daily         &   67  & 3748          & 153.04    & (5, 217)   & 4.69:1        & Self-collected                     & 3rd \\ \hline
% Mini-Sports1M~\cite{sports1m}         & Sports        &  487  & 24350     & 10 & 50       & 4:1                        & YouTube                            & 3rd \\
% FineGym99~\cite{shao2020finegym}             & Sports        &  99   & 20389     & 1.65        & (33, 951) & 2.24:1            & Competition videos                 & 3rd \\
% MOD20~\cite{perera2020multiviewpoint}                 & Sports        &   20  & 2324     & 7.4        & (73, 107)   & 2.29:1           & YouTube and self-collected         & 3rd, dro. \\ \hline
% COIN~\cite{tang2019coin}                  & Instructional &  180  & 6655  & 37.01        & (10, 63)    & 3.22:1            & YouTube                            & 1st, 3rd\\
% MECCANO~\cite{ragusa2021meccano}               & Instructional &   61  & 7880         & 2.82       & (2, 1157)    & 1.79:1      & Self-collected                     & 1st \\
% INHARD~\cite{dallel2020inhard}                & Instructional &   14  & 5303        & 1.36  & (27, 955)    & 2.16:1            & Self-collected                     & 3rd \\
% PETRAW~\cite{huaulme2022peg}                & Instructional &    7  & 9727      & 2.16       & (122, 1262)       & 1.5:1    & Self-collected                     & 1st \\
% MISAW~\cite{huaulme2021micro}                 & Instructional &   20  & 1551     & 3.8       & (1, 316)   & 2.38:1             & Self-collected                     & 1st \\ \hline
% \end{tabular}
% }
% \end{center}
% % \end{sidewaystable}
% \end{table*}

\section{\includegraphics[scale=0.1]{figures/bear.png}BEAR}
\label{sec:datasets}

% \textbf{TODO: why we only pick video classification task? in particular, why we only do on human action recognition? }



Despite new datasets being introduced every year, the most widely adopted benchmarks in the video action recognition community are Kinetics-400/600/700~\cite{k600,k700,kay2017kinetics}, Something-something-v1/v2~\cite{goyal2017something}, UCF-101~\cite{soomro2012ucf101} and HMDB-51~\cite{hmdb51}. 
However, these datasets share a high similarity in that they are mostly composed of daily and sports actions. 
Models that achieve good performance on these datasets may not generalize well to the challenging real-world scenarios due to dramatic domain shifts. 
For example, anomaly videos are often captured from surveillance cameras, which look quite different from daily videos due to viewpoint change. 
Ideally, a video model is expected to cope with diverse real-world applications.
% , e.g., anomaly activity recognition, embodied AI, etc.
% Current development of vision community requires that video models can  

% only fit in specific domain cannot generalize well in such a challenging situation due to the conspicuous domain shift, i.e., anomaly video data is often captured from CCTV cameras and embodied AI often requires the ability to perceive egocentric vision.

To comprehensively evaluate the generalization capability of video models, we present BEAR, a new BEnchmark for human Action Recognition.
As shown in Table~\ref{table:data_info}, BEAR is a collection of 18 action recognition datasets, carefully designed towards \textit{practical use}, \textit{data diversity}, and \textit{task diversity}. Compared to existing video action recognition datasets, BEAR has the following desirable properties.


\noindent \textbf{Real Applications.} Besides the common daily and sports categories, BEAR contains another three categories including anomaly activity, gesture, and instructional actions. These action categories have important real-world applications such as people fall detection (\eg MUVIM~\cite{denkovski2022multi}), sign language recognition (\eg WLASL100~\cite{li2020word}), industrial inspection (\eg MECCANO~\cite{ragusa2021meccano}), and surgical workflow recognition (\eg  PETRAW~\cite{huaulme2022peg}).

\noindent  \textbf{Data Diversity.} BEAR is not only diverse in application domains but also in the data source, video viewpoint, and video length. As shown in Table~\ref{table:data_info}, BEAR contains videos from various sources such as movies, CCTV cameras, YouTube, and drone cameras. It also includes videos in the 1st and 3rd person views. In terms of video length, the average clip duration varies from the shortest (\eg 1.23s in WLASL100~\cite{li2020word}) to the longest (\eg 153.04s in MPII Cooking~\cite{rohrbach2012database}).
In addition, the training sample size per class varies across datasets, from the lowest (\eg 1 for MISAW~\cite{huaulme2021micro}) to the highest (\eg 9592 for Jester~\cite{materzynska2019jester}).

\noindent \textbf{Few-shot Transfer.} The standard finetuning protocol for transfer is to train a model on the whole training data, which is often more than thousands of videos. 
However, in many real applications, the annotated video data is scarce, \eg anomaly recognition (rarely happens and is costly to label), medical operation (privacy concern), and industrial operation (need the expertise to label). 
To better evaluate a model's potential in real applications, we need to evaluate its effectiveness under few-shot learning.
Hence in BEAR, besides the full datasets, we also split each dataset into 16-shot, 8-shot, 4-shot, and 2-shot versions. This allows researchers and practitioners to thoroughly evaluate a model's sensitivity to data scarcity.
% we introduce two settings during transfer: standard finetuning and few-shot finetuning. Our few-shot finetuning can go as low as 2 shots per class, which leads to interesting findings emerging from model comparison.

% on is full-shot evaluation, where the model is trained on the whole training data, which is often more than thousands of videos.

\noindent \textbf{Flexible Evaluation.} Thanks to the data diversity in BEAR, researchers can easily evaluate video models under various settings. For example, full-shot and few-shot learning, domain adaptation from one dataset (or category) to another. Moreover, we also believe that new settings can be easily derived based on our benchmark.
% We include as many training settings in BEAR as possible, which could provide a flexible platform for various practitioners to validate specific video models under any circumstances, e.g., zero-shot learning, unsupervised domain adaptation, etc. Moreover, we also believe that new settings can be easily derived based on our benchmark.

\noindent \textbf{Fair Comparison.} 
The held-out test set for most video action recognition datasets either does not exist or is not commonly adopted. This allows previous methods to conduct hyperparameter optimization or even neural architecture search directly on the test set. Test set tuning usually leads to good testing performance, but it may not translate to other datasets. \textit{To promote fair comparison and generalization capability, we will hold the test sets and provide an evaluation server for future researchers and practitioners.}

\noindent \textbf{Dataset Accessibility.} We provide scripts to download and format all 18 datasets automatically. Our codebase is built upon MMAction2~\cite{2020mmaction2}, so researchers can easily integrate their new models by providing a model definition file without additional efforts to perform evaluations. Furthermore, the total number of video clips in BEAR is about 310K, which is comparable to Kinetics-400. \textit{Therefore, the overall time cost is similar to training a model on Kinetics-400.}

% existed video action recognition benchmark, such as Kinetics400, choose to evaluate models on limited downstream datasets, e.g., UCF-101 and HMDB-51, which restricts to evaluate target models in limited attributes, since most videos in these two datasets can only relate to daily events and sports activities. However, current development of vision community requires that video models can cope with diverse real-world applications, e.g., anomaly activity recognition, embodied AI, etc. Intuitively, models that only fit in specific domain cannot generalize well in such a challenging situation due to the conspicuous domain shift, i.e., anomaly video data is often captured from CCTV cameras and embodied AI often requires the ability to perceive egocentric vision. Moreover, a fair comparison during model training is also of critical importance, since tuning parpameters on test data could inevitably cause severe overfitting. To this end, as shown in Figure~\ref{fig:teaser}, we propose BEAR to establish a comprehensive benchmark which provides: 1) fair comparison; 2) datasets that covering most real-world applications; 3) data of diverse viewpoint; 4) few-shot learning. 

% Specifically, we construct our datasets for evaluation given five common data domains: sports events, daily living, anomaly (crime) scenes, instructional videos and gestures videos. Videos related to sports and daily events, due to their billions of resources in YouTube, are the most common videos used in existed video action recognition research; while the anomaly, instructional and gesture videos are quite representative in practical application scenarios. In each domain, we carefully select datasets that could bring data diversity for our benchmark. For instance, as shown in Table~\ref{table:data_info}, in daily videos, we deliberately choose CharadesEgo for its egocentric nature; and the Toyota Smarthome is collected by 7 cameras deployed at the top corners in a aportment, which provides different view angles than regular videos. And in sports videos, Mini-Sports1M covers the widest range of sports activities, while FineGym is a challenging fine-grained gymnastic dataset. Moreover, in instructional videos, we also focus on egocentric data, and in anomaly and gesture videos, the intrinsic domain shift could bring much data diversity. It is worth noting that we segment videos that belong to multiple classes into several single-label videos given the official temporal annotation (e.g., CharadesEgo). We provide detailed description as well as the preprocessing of the datasets in Supp.


%-------------------------------------------------------------------------
