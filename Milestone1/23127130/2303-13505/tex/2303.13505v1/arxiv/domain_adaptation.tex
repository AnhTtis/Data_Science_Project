\section{Unsupervised Domain Adaptation}
\label{sec:uda}

\begin{table*}[h]
\caption{The unsupervised domain adaptation accuracy on our UDA datasets: Toyota Smarthome-MPII-Cooking (T: Toyota Smarthome, M: MPII-Cooking), Mini-Sports1M-MOD20 (MS: Mini-Sports1M, MOD: MOD20), UCF-Crime-XD-Violence (U: UCF-Crime, X: XD-Violence), PHAV-Mini-Sports1M (P: PHAV, MS: Mini-Sports1M), Jester, InHARD (I: InHARD, T: Top, L: Left, R: Right). }
\scalebox{0.85}{
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{c|cc|cc|cc|c|c|cccccc}
\hline
Settings & \multicolumn{7}{c|}{\textbf{Inter-dataset}} & \multicolumn{7}{c}{\textbf{Intra-dataset}}\\ \hline
Dataset               & T$\rightarrow$M & M$\rightarrow$T & MS$\rightarrow$MOD & MOD$\rightarrow$MS & U$\rightarrow$X & X$\rightarrow$U & P$\rightarrow$MS & Jester & IT$\rightarrow$IL & IT$\rightarrow$IR & IL$\rightarrow$IR & IL$\rightarrow$IT & IR$\rightarrow$IT & IR$\rightarrow$IL \\\hline
Source only                     & 5.32               & 7.36        & 18.25        & 12.76     & 54.20 & 33.33     & 61.45 & 68.73 & 4.18 & 30.39 & 19.01 & 22.65 & 24.14 & 12.42 \\ \hline
TA$^{3}$N~\cite{chen2019temporal}   & 11.17              & 15.38       & 23.77        & 19.15   & 59.91 & 44.44   & 65.79 & 71.44 & 5.78 & 41.83 & 27.91 & 28.08 & 35.66 & 14.68 \\
CoMix~\cite{sahoo2021contrast}   & 12.63              & 15.32       & 24.48        & 21.56     & 60.17 & 47.22    & 64.83 & 75.86 & 6.32 & 39.79 & 30.45 & 31.74 & 32.94 & 14.83 \\ \hline
Supervised target                  & 70.21              & 65.13        & 34.08        & 35.52   & 75.06 & 63.89   & 94.40 & 97.61 & 26.00 & 83.55 & 83.55 & 85.52 & 85.52 & 26.00 \\ \hline
\end{tabular}}}
\label{tab:uda}
\end{table*}

\begin{figure*}[t]
\centering
    \includegraphics[width=\linewidth]{figures/zeroshot.jpeg}
    \caption{Results of zero-shot evaluation. For most datasets in our benchmark, CLIP-based models still cannot provide reasonable results, especially for those challenging datasets with severe viewpoint shifts and fine-grained datasets.}
    \label{zero}
    \vspace{-2ex}
\end{figure*}

% Either finetuning or zero-shot learning considers the learning within a target dataset; however, 
In real-world scenarios, it is possible to transfer knowledge from similar datasets that are well-annotated to others with few labels. For instance, there are a lot of existing datasets that include samples of the same categories in the corresponding real-world tasks and thus can be used to facilitate model training. 
Nonetheless,  due to the domain gap, models directly trained on one dataset cannot be well generalized on the target data. In such case, unsupervised domain adaptation (UDA)~\cite{ganin2015unsupervised} can largely alleviate this distribution shift issue by learning the domain-invariant feature when labeled source data is available, learning representations that would promote the performance on the target domain.
% In our benchmark, there are conspicuous viewpoint shift and shared categories across datasets, based on which we can build paired datasets to investigate domain adaptation algorithms. 

\vspace{-1 em}
\paragraph{UDA datasets.}
In BEAR, we provide two different types of domain adaptation paradigm: 
\textbf{Inter-dataset} is constructed based on two different datasets that have different distributions, especially viewpoint change, but share common categories. For instance, Mini-Sports1M and MOD20 are collected from different data sources but they both describe a variety of sports-related actions. Totally, we construct 4 paired datasets following the inter-dataset rule: three of them are from the 18 datasets (ToyotaSmarthome and MPII-Cooking, Mini-Sports1M and MOD20, UCF-Crime and XD-Violence), and the other is constructed by a synthetic sports dataset named PHAV \cite{roberto2017procedural} and Mini-Sports1M.
\textbf{Intra-dataset}, on the contrary, is built within one dataset that records the same actions differently. For example, InHARD has three different views for each video sample and we simply construct 6 paired datasets for the UDA study. Besides, we also include the popular Jester(S-T) \cite{pan2020adversarial} in our benchmark.
The details of the dataset construction can be found in \textcolor{blue}{Appendix~\ref{appendix_uda}}.
We provide two common baseline results: `Source only' and `Supervised target'. The former directly evaluates the model trained on the source training set with the target test set, and the latter is the supervised learning performance on the target domain. Besides, we also evaluate two recent UDA algorithms on our benchmark: TA$^{3}$N~\cite{chen2019temporal} and CoMix~\cite{sahoo2021contrast}. 
% We evaluate two recent UDA algorithms on our benchmark. TA$^{3}$N~\cite{chen2019temporal}, short for Temporal Attentive Adversarial Adaptation Network, leverages attention mechanism to find important and discriminative temporal dynamics and thus to achieve more effective temporal features alignment. Different from TA$^{3}$N, which is based on the adversarial learning paradigm, Contrast and Mix (CoMix)~\cite{sahoo2021contrast} leverages contrastive learning to obtain domain-invariant features. Additionally, it mixes the background of videos from one domain with videos from another domain to increase the number of positive samples and forces the models to focus more on action semantics rather than background noise. 
\vspace{-1 em}
\paragraph{Challenging viewpoint adaptation.}
% As shown in Table~\ref{tab:uda}, the transfer can be obviously challenging in viewpoint change cases. For instance, ToyotaSmarthome and MPII-Cooking share similar attributes w.r.t. their actions, since they both record kitchen events. However, videos in ToyotaSmarthome are recorded on the right side of people, while videos in MPII-Cooking are recorded by a down view camera. The UDA performance between these two datasets is far lower than the 'Supervised target' baseline. Similar observation can be obtained in InHARD, the models can learn well with the right and top view, but the UDA performance is inferior when training on data from the left view. In terms of data, among all three views, the left view experiences severe occlusion, increasing the learning difficulty.
As shown in Table~\ref{tab:uda}, domain adaptation can be obviously challenging especially in viewpoint change cases. For instance, ToyotaSmarthome (T) and MPII-Cooking (M) share similar attributes w.r.t. their actions, since they both record kitchen events. However, videos in T are recorded via different cameras in the living room, while videos in M are recorded by a down-view camera. The performance between these two datasets is far lower than the `supervised target'. Similar observations can also be obtained in InHARD. Although the adaptation is conducted within the dataset, recent methods still fail to perform well when adapting from one viewpoint to another. However, the gap between supervised target and UDA methods is much smaller in other UDA datasets where the viewpoint change is smaller. These results, along with the observations in Secs. \ref{sec:finetune} and \ref{sec:few-shot}, reveal that viewpoint change has a critical impact on transfer performance, which is hard to mitigate even with recent UDA algorithms.

