\section{Zero-shot Learning}
\label{sec:zeroshot}
Direct finetuning on annotated datasets is a commonly adopted paradigm for action recognition, but the recent success of vision-language models, which leverage the rich correspondence between natural language and visual content, has provided a new learning paradigm for vision tasks in a zero-shot setting, which is severely required in applications without labeled data. Therefore, we also provide the zero-shot evaluation on BEAR using the recent CLIP-based~\cite{radford2021learning,wang2021actionclip} models.
% multiumodal foundation models to complete our benchmark. 
% CLIP~\cite{radford2021learning} is one of the state-of-the-arts which is capable of surpassing ResNet50~\cite{he2016deep} on ImageNet under zero-shot evaluation. 
% In our benchmark, in order to evaluate the extent of the correlation between the video frames and their labels to see whether language supervision is enough to correctly recognize actions, we perform frame-level zero-shot evaluations on all of these datasets via CLIP. We also evaluate ActionCLIP\cite{wang2021actionclip} to unmask the difference of zero-shot performance between image-based models and video-based models. 
The details of the experiment are provided in \textcolor{blue}{Appendix~\ref{appendix_zeroshot}}.
% Basically, we provide two different settings for frame-level CLIP evaluations, \ie single-frame, which follows the settings in \cite{radford2021learning} and 5-frame, where we sample 5 frames from the input video and fuse the model output of each frame. Similarly, we also construct multiple templates for each dataset to obtain ensemble textual embeddings. Considering the inconsistent label domains for the selected datasets, we provide different templates given their distinct attributes of both data and labels. For instance, UCF-Crime~\cite{sultani2018real} is mostly constituted of surveillance videos in a crime scene; thus, a sentence like \textit{`a photo from a surveillance camera showing a criminal doing \{\} in a crime scene.'} is utilized as a part of the prompts. Additionally, we evaluate all the datasets via ActionCLIP \cite{wang2021actionclip}, which is pre-trained on Kinetics-400 based on video and label-text correlation, to unmask the difference of zero-shot performance between image-based models and video-based models. 
As illustrated in Figure \ref{zero}, different from its versatility in the image domain, most of the zero-shot results based on CLIP are still far lower than those of supervised learning. For example, WLASL shows poor correlations between frames and the corresponding labels, which can be partly explained by the large visual gap between the visual information of sign languages and the label itself. Surprisingly, for most datasets, ActionCLIP, which leverages more frames, performs even worse than CLIP. Part of the reason could be that ActionCLIP finetunes CLIP on Kinetics-400, which leads to catastrophic forgetting and overfitting.  However, for some datasets, zero-shot learning could outperform few-shot learning, such as XD-Violence and MOD20, which even approaches supervised learning. This may be partly because the high vision-text correlation existed in these datasets, and this also demonstrates the potential of language supervision in action recognition.
