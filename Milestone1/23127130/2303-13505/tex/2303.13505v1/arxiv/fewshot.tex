\begin{figure*}[htbp]
\vspace{-15pt}
\centering
    \includegraphics[width=17.5cm]{figures/fewshot_new.pdf}
    \vspace{-2.5em}
    \caption{Results of few-shot learning based on supervised and self-supervised pre-training. The \textcolor[rgb]{0.4,0.8,0.4}{green} curves represent supervised pre-training and the \textcolor[rgb]{0,0,1}{blue} curves represent $\rho$MoCo self-supervised pre-training. We illustrate the results of TSM, 3D NonLocal, and VideoSwin for both pre-training methods. Additionally, we add the SOTA self-supervised pre-training method VideoMAE, represented by the \textcolor[rgb]{1,0,0}{red} curves, for comparison. It could be obvious that even the VideoMAE could lag a lot behind in the few-shot setting.}
    \vspace{-1 em}
    \label{fewshot}
\end{figure*}

\section{Few-shot Learning}
\label{sec:few-shot}

% We claim that few-shot learning in video action recognition is still a challenging task.
% Since the annotations of videos can be extremely expensive because a motion concept is much harder to recognize by human annotators compared with images, and in videos which require expertise knowledge, it will become even worse. 

Compared with standard finetuning where abundant annotations can be utilized, few-shot learning is of more practical significance since annotating massive amounts of videos is notoriously expensive. To extend the investigation mentioned in Section \ref{sec:finetune},  we thoroughly investigate the capability of the selected 6 models on BEAR under few-shot setting given both supervised and self-supervised pre-trained weights. Specifically, we consider (2,4,8,16)-shot settings, and for each setting, we randomly generate 3 splits and report the mean and standard deviation. 
% It should be noted that, in the same split, the training samples in a smaller shot are included in a bigger one, \eg the samples in 2-shot are also the samples in 4-shot. We expect this could provide a consistent observation  w.r.t. number of training samples and the performance. 
Due to space constraints, we only select TSM, 3D NonLocal, and Video Swin to represent each model type for illustration as they perform generally better.  Complete few-shot results and the training details are in \textcolor{blue}{Appendix~\ref{appendix_fewshot}}.

\vspace{-1em}
\paragraph{Model comparison.} 

The rankings of the six models in few-shot finetuning exhibit distinct variations compared to the standard finetuning. In contrast to the dominance of TSM in standard finetuning across both pre-training settings, the most effective models differ significantly across datasets in few-shot finetuning. Figure~\ref{fewshot} demonstrates that TSM no longer clearly outperforms other models in most datasets, and the two composite metrics (which are presented in the Supplementary due to space limitations) support this conclusion. Specifically, TSM and TimeSformer exhibit similar performance in supervised pre-training, whereas I3D and VideoSwin perform better in self-supervised learning. These findings further reveal the limitations of previous simple evaluation protocols, which may not provide a fair assessment of video models.
% such as the previous finetuning on a limited set of downstream datasets, which cannot provide a fair assessment. 
These results also confirm the necessity of BEAR, which emphasizes the importance of diverse downstream datasets and various settings for unbiased evaluation. 
% Furthermore, the performance of few-shot learning can be significantly affected by the number of classes in each dataset, as evidenced by the lower micro-average-precision compared to the macro-average-precision. This observation aligns with the fact that the total training data in few-shot learning has positive correlation to the number of classes.

\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
    \item \emph{The ranking relations between models could exhibit differently between standard and few-shot finetuning even within the same datasets. This finding further emphasizes the importance of our proposed BEAR benchmark, which advocates for a comprehensive evaluation approach that considers both dataset diversity and finetuning settings.}
\end{itemize}
\end{adjustwidth}
\vspace{-1em}

\paragraph{Impact of viewpoint change} 
% As can be seen from the green curves in Figure~\ref{fewshot}, compared with the standard finetuning,
As in standard finetuning, viewpoint change also has a severe impact when it comes to few-shot learning.
Comparing the results in Figure \ref{fewshot} with those in Table \ref{tab:finetune}, we can see that
the few-shot learning performance decreases drastically in general, especially in datasets that have less in common with Kinetics-400, such as UAV-Human, which is constructed by videos captured from unmanned aerial vehicles, FineGym, which contains fine-grained gym-related videos, and PETRAW and MISAW, which are simulated medical operations in the 1st person view. Conversely, in datasets that are more similar to Kinetics-400, these performance gaps are notably reduced. For example, even the 2-shot performance on Mini-HACS and MOD20 can reach approximately 60\% and 85\%, and the models achieve satisfying performance on the 16-shot setting on COIN.
In previous works, the homogeneity of the pre-training and downstream data hindered the timely identification of such phenomena in few-shot learning. Our investigation highlights the challenge of few-shot learning and underscores the importance of bridging the gap (as aforementioned, introducing extra data, such as Ego4D) between pre-training and the target data. 

Moreover, in the few-shot setting, self-supervised pre-training is more susceptible to viewpoint change. In challenging datasets such as UAV-Human and WLASL, few-shot learning can hardly obtain satisfying results based on self-supervised pre-trained weights, while in the 16-shot setting, supervised pre-training could provide comparable performance compared with standard finetuning. Similarly, in MOD20, the performance experiences a sharp decline in few-shot settings with self-supervised pre-training, while supervised pre-trained TSN and TSM can achieve accuracy exceeding 90\% in the 16-shot.
% Moreover, the curves of different datasets show different trends w.r.t. the shot numbers. For instance, in Mini-HACS and MOD20, the performance gaps between 2-shot and 16-shot remain relatively indistinct, while in UAV-Human and FineGym, the curves rapidly go downward when the shot number decrease. Coincidently, these datasets are also influenced by viewpoint change as aforementioned. This indicates that, in few-shot finetuning, domain gap between pre-training and target data can also determine the sensitivity towards shot number.

\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
    \item \emph{
    % Few-shot finetuning remains a significant challenge in real-world scenarios. However, the performance drop is primarily influenced by the domain gap between pre-training and target data. Previous studies overlooked this phenomenon due to the lack of diverse downstream datasets, but we highlight the importance of incorporating extra pre-training data to improve the performance across a wide range of downstream datasets.
    Few-shot finetuning remains a significant challenge in real-world scenarios.  The performance drops dramatically compared to standard finetuning especially when there is a large domain gap between pre-training and target data. However, when downstream datasets are similar to source data, the performance drop could be mitigated.
    }
    \item \emph{In few-shot learning, self-supervised pre-training is more vulnerable to viewpoint shift, while supervised pre-trained models can achieve favorable performance compared with standard finetuning on the 16-shot setting.}
\end{itemize}
\end{adjustwidth}
\vspace{-1em}


\paragraph{Self-supervised vs. supervised pre-training.} 
Comparing the blue curves to the green curves in Figure~\ref{fewshot}, we can see that self-supervised pre-training is generally less effective than supervised pre-training, which is consistent with the conclusion in Sec.~\ref{sec:finetune}. The performance gaps are pronounced in gesture datasets and are less significant in Mini-Sports1M, ToyotaSmarthome, etc. The performance gap is also different across different models. The largest gap appears in TSN and TimeSformer (the complete results are provided in \textcolor{blue}{Appendix Table~\ref{tab:sup res1}-~\ref{tab:ssl res3}}).
One reason for the poor performance of self-supervised learning may be the limitation of $\rho$MoCo. Therefore,
to consolidate our conclusion,
% on few-shot learning based on the difference between supervised and self-supervised pretrained models, 
we further consider VideoMAE~\cite{tong2022videomae}, which is the SoTA self-supervised method and has demonstrated even better performance than supervised models on multiple datasets. Here, we use the officially released VideoMAE ViT-B model, which achieves 81.5\% Top-1 accuracy on Kinetics-400. However, comparing the results with our 6 supervised pre-trained models in Figure \ref{fewshot} (red vs. green curves), we show that VideoMAE could only be comparable with the best supervised pre-trained models in less than half of the datasets. 
% This reveals that self-supervised pre-training still lags a lot behind supervised pre-training in few-shot learning with diverse domains. 
% Therefore,  we claim that few-shot learning in video action recognition is still a challenging task that requires further effort for more effective framework.

\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
    \item \emph{Supervised pre-training shows consistent advantages over self-supervised ones in few-shot finetuning. Even the SoTA VideoMAE can hardly outperform simple supervised pre-trained models in diverse domains.}
\end{itemize}
\end{adjustwidth}

% This indicates that, besides Kinetics 400, a large-scale  dataset that focuses on specific viewpoints is required for pre-training. For instance, for egocentric pre-training, the recent Ego4D~\cite{grauman2022ego4d} could be a possible choice. 