%------------------------------------------------------------------------
\begin{table*}[t]
\begin{center}
\caption{Statistics of the selected datasets used in our video benchmark. We collect 18 datasets covering 5 common data domains for comprehensive benchmarking. In the column of video viewpoint, ``sur.'' means surveillance videos, and ``dro.'' means drone videos. }
\scalebox{0.75}{
\label{table:data_info}
\begin{tabular}{c|ccccccccc}
\hline
\textbf{Dataset}   & \textbf{Domain}  & \textbf{\begin{tabular}[c]{@{}c@{}}Label\\ classes\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Clip\\ \textit{num.}\end{tabular}}  & \textbf{\begin{tabular}[c]{@{}c@{}}Avg Length\\ (sec.)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Training data\\ per class (min, max)\end{tabular}} &  \textbf{\begin{tabular}[c]{@{}c@{}}Split\\ ratio\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Video\\ source\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Video\\ viewpoint\end{tabular}}\\ \hline
XD-Violence~\cite{wu2020not}           & Anomaly       &   5   & 4135     &   14.94    & (36, 2046)    & 3.64:1         & Movies, sports, CCTV, etc.         & 3rd, sur. \\
UCF Crime~\cite{sultani2018real}             & Anomaly       &   12  & 600       &   132.51   & 38    & 3.17:1                 & CCTV Camera                        & 3rd, sur. \\
MUVIM~\cite{denkovski2022multi}                 & Anomaly       &   2   & 1127      &   68.1     & (296, 604)    & 3.96:1        & Self-collected                     & 3rd, sur. \\ \hline
WLASL100~\cite{li2020word}              & Gesture       &   100 & 1375     &   1.23        & (7, 20)    & 5.37:1         & Sign language website              & 3rd \\
Jester~\cite{materzynska2019jester}                & Gesture       &   27  & 133349  &   3        & (3216, 9592)  & 8.02:1          & Self-collected                     & 3rd \\
UAV Human~\cite{li2021uav}             & Gesture       &  155  & 22476   &   5        & (20, 114)   & 2:1               & Self-collected                     & 3rd, dro. \\ \hline
CharadesEgo~\cite{sigurdsson2018charades}      & Daily         &  157  & 42107  &  10.93    & (26, 1120)   & 3.61:1             & YouTube                            & 1st \\
Toyota Smarthome~\cite{das2019toyota}      & Daily         &   31  & 14262      &   1.78    & (23, 2312)    & 1.63:1        & Self-collected                     & 3rd, sur. \\
Mini-HACS~\cite{zhao2019hacs}             & Daily         &  200  & 10000   & 2        & 50       & 4:1                    & YouTube                            & 1st, 3rd\\
MPII Cooking~\cite{rohrbach2012database}          & Daily         &   67  & 3748          & 153.04    & (5, 217)   & 4.69:1        & Self-collected                     & 3rd \\ \hline
Mini-Sports1M~\cite{sports1m}         & Sports        &  487  & 24350     & 10 & 50       & 4:1                        & YouTube                            & 3rd \\
FineGym99~\cite{shao2020finegym}             & Sports        &  99   & 20389     & 1.65        & (33, 951) & 2.24:1            & Competition videos                 & 3rd \\
MOD20~\cite{perera2020multiviewpoint}                 & Sports        &   20  & 2324     & 7.4        & (73, 107)   & 2.29:1           & YouTube and self-collected         & 3rd, dro. \\ \hline
COIN~\cite{tang2019coin}                  & Instructional &  180  & 10426  & 37.01        & (10, 63)    & 3.22:1            & YouTube                            & 1st, 3rd\\
MECCANO~\cite{ragusa2021meccano}               & Instructional &   61  & 7880         & 2.82       & (2, 1157)    & 1.79:1      & Self-collected                     & 1st \\
INHARD~\cite{dallel2020inhard}                & Instructional &   14  & 5303        & 1.36  & (27, 955)    & 2.16:1            & Self-collected                     & 3rd \\
PETRAW~\cite{huaulme2022peg}                & Instructional &    7  & 9727      & 2.16       & (122, 1262)       & 1.5:1    & Self-collected                     & 1st \\
MISAW~\cite{huaulme2021micro}                 & Instructional &   20  & 1551     & 3.8       & (1, 316)   & 2.38:1             & Self-collected                     & 1st \\ \hline
\end{tabular}
}
\end{center}
\vspace{-1.5em}
% \end{sidewaystable}
\end{table*}

\section{Related Work}
\label{sec:related}

% \textbf{TODO: why we need this evaluation benchmark while most people focus on collecting bigger training dataset?}
% 1. definition and challenges
% 2. model
% 3. dataset
\noindent \textbf{Human action recognition} is to distinguish the ongoing actions (or sometimes events) in a video. Different from image classification, video action recognition requires effective temporal modeling~\cite{wang2016temporal}, awareness of the action hierarchies~\cite{shao2020finegym}, and the interaction between the subjects and objects~\cite{goyal2017something}. 
% The efficacy of CNNs in vision have been widely demonstrated via massive image tasks~\cite{karpathy2014large,ren2015faster,long2015fully}; thus, 
In early years, video models simply inherit the 2D convolution structures~\cite{vgg,resnet} and process temporal information either by extending 2D convolutions into 3D~\cite{tran2015learning,carreira2017quo,yang2021mutualnet} or including optical flow~\cite{simonyan2014two}. However, optical flow-based approaches suffer from costly flow pre-computation, thus 2D CNNs with more sophisticated temporal modeling are designed~\cite{wang2016temporal,zhu2018hidden,lin2019tsm, tsqnet, wu2019multi}.
For 3D CNNs, factorized architectures~\cite{p3d,tran2018closer,s3d,eco} are introduced to improve the model efficiency and reduce overfitting. 
% 3D CNNs are prone to overfit and could bring considerable additional computation burden. Based on this, 2D CNNs with more sophisticated temporal modeling are designed, such as TSN~\cite{wang2016temporal}, TSM~\cite{lin2019tsm}, R(2+1)D~\cite{tran2018closer}, etc. 
Recently, Transformer~\cite{vaswani2017attention} continues to showcase its capability from language to image, and also to video~\cite{bertasius2021space,arnab2021vivit,zhang2021vidtr,liu2022video}. Top performers on most video action recognition datasets are transformer-based. In this work, we fairly evaluate 6 popular video models belonging to 2D CNN, 3D CNN, and Transformer, respectively. With comparable backbones, we surprisingly reveal that 2D CNNs can sometimes outperform transformer-based models.

% For video actionTimeSFormer\cite{bertasius2021space} discards convolutional framework and is solely  constructed with self-attentions. Further, Video Swin Transformer~\cite{} incorporate window attention to reduce computational complexity and reaches state-of-the-arts on several popular benchmarks.

\noindent \textbf{Spatiotemporal representation learning} is advancing rapidly in the last few years, especially in a self-supervised manner.
Self-supervised pre-training is appealing because it could learn visual knowledge from massive unlabeled data, which alleviates the annotation burden compared with its supervised counterpart. 
Most approaches design a pretext task to learn the intrinsic spatiotemporal feature within the video data, such as sorting the shuffled video sequence~\cite{lee2017unsupervised}, next frame prediction~\cite{han2019video}, predicting the frame rate~\cite{epstein2020oops}, contrastive learning~\cite{ding2021motion,rhomoco,videomoco,vclr,qian2021spatiotemporal}, mask modeling~\cite{tong2022videomae,fbvideomae}, etc. 
% ; and some of them leverage contrastive learning to mine the semantic consistency across different views of a same clip, while push away different samples~\cite{qian2021spatiotemporal}. 
Despite their promising performance, a recent work~\cite{thoker2022severe} points out that video self-supervised pre-training is less robust than its supervised counterpart when the downstream setting varies.
In this work, we also compare supervised pre-training with self-supervised ones in terms of both standard finetuning and few-shot finetuning on our benchmark. 
% We obtain similar observation with~\cite{thoker2022severe}  that supervised pre-training performs better under most scenarios. 

% Besides, pretraining on large datasets have been widely accepted in action recognition. Classic supervised pretraining on Kinetics400~\cite{carreira2017quo} have been proved to be much more effective than training from scratch due to its rich and transferable  visual knowledge. 


% learning methods: sup & ssl, comparison

%TODO: dataset-related. large-scale datasets, datasets of different applications, different views, etc.


% \subsection{Vision Benchmarks}
% existed video action recognition benchmarks, domain, limited evaluation, 2 or 3 dataset. test set param. 
% comprehensive  benchmarks in other domains
% In visual community, qualified 
\noindent \textbf{Vision benchmark} is often designed as a testbed, which consists of multiple datasets from different domains. 
Each benchmark might have its own motivation, but they share the same goal of providing a unified protocol for evaluation and thus facilitating the evolution of a specific area. 
Many well-established benchmarks have been proposed in different research areas ~\cite{wang2019towards,zhai2019visual,li2022elevater,li2021grounded,gupta2022grit}. However, there is no such comprehensive benchmark for video action recognition. 
% Many well-established benchmarks have been proposed in different research areas, such as reinforcement learning~\cite{duan2016benchmarking}, VTAB~\cite{zhai2019visual} for visual representation adaptation in terms of classification, UODB~\cite{wang2019towards} for multi-domain object detection, ELEVATER~\cite{li2022elevater} for language-augmented visual models,  ODinW~\cite{li2021grounded} for object detection in the wild and GRIT~\cite{gupta2022grit} for robustness and calibration of a vision system. However, there is no such comprehensive benchmark for video action recognition.
Two works that are the closest to ours are VTAB~\cite{zhai2019visual} and SEVERE-benchmark~\cite{thoker2022severe}.
VTAB contains 19 datasets that cover a broad spectrum of domains and semantics. 
% They are grouped into three categories: natural, specialized and structured.
All tasks are formulated as the image classification problem for the sake of a homogeneous task interface.
Inspired by VTAB, we build the first comprehensive evaluation benchmark for video action recognition.
BEAR includes 18 datasets across 5 domains towards real applications.
It enables fair comparison and thorough investigation of existing video models, which allows us to address interesting open questions.
SEVERE-benchmark~\cite{thoker2022severe} investigates how sensitive video self-supervised learning is to the current conventional benchmark in terms of domain, samples, actions, and tasks. 
Compared to ~\cite{thoker2022severe}, we study both supervised and self-supervised learning in more domains (anomaly, instructional), with more datasets (18 vs 8) and more settings (few-shot, zero-shot, and domain adaptation).

% As a consequence, existed video models cannot be thoroughly evaluated in diverse domains and their usage in real-world applications may be weakened.



% For image representation learning, VTAB~\cite{zhai2019visual} covers a wide scope of target datasets and conduct extensive experiment to comprare supervised representation and self-supervised one. They also suggest generative models could be less effective than discriminative models when used as a representation learning method. In visual robustness, GRIT~\cite{gupta2022grit} is recently proposed to thoroughly benchmark general image tasks in a image perturbation or a domain shift scenario. They include 7 common tasks for comprehensive assessment for robustness and generality. Moreover, 
% LVIS~\cite{gupta2019lvis} provides a large training set that contains over 1000 categories and its mask annotations are much preciser than previous datasets. which makes it a more suitable benchmark than existed ones. 
% In video community, large-scale datasets such as Kinetics400 and "something something" have been commonly used as pretraining dataset; however, the target datasets for evaluation are often with limited scope, e.g., HMDB-51, UCF-101, etc. 



% Recently, a new video benchmark called Perception Test is proposed for several multimodal reasoning tasks, such as object tracking, action localization and video question answering. But none of these existed benchmarks considered data in different domains. In our BEAR benchmark, we include 18 datasets that conver 5 common data domain to thoroughly investigate the performance of 6  video models on different learning settings.

