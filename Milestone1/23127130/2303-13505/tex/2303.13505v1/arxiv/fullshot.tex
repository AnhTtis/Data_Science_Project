\section{Standard Finetuning}
\label{sec:finetune}

% We leverage three learning paradigms, full-shot finetuning, few-shot learning, zero-shot evaluation, for in-depth investigation towards the intrinsic mechanism in the task of action recognition in the following aspects: 1)pre-training methods; 2) model selection; 3) data domain and viewpoint. We wish that this could provide a clue for the questions aforementioned. We first prepare our pretrained models via both supervised and self-supervised pre-training on Kinetics400, which provide initialized weights for our full-shot finetuning and few-shot learning experiments. The zero-shot evaluation is based on CLIP and ActionCLIP. The details will be introduced in the following subsections.

Finetuning models that are pre-trained on large-scale datasets has been a mainstream learning paradigm in deep learning, and performance on various downstream datasets can provide a more comprehensive evaluation with less bias. Thus, in BEAR, we regard standard finetuning as a basic evaluation method. Specifically, we finetune the pre-trained models on the 18 datasets to investigate: 
1) the performance of different types of video models on different data domains; 2) the difference between supervised pre-training and self-supervised pre-training; 3) potential factors (\eg domain shift, viewpoint shift, etc.) that have significant impacts on the performance of downstream tasks. 
We want to emphasize that during finetuning, we do not tune hyperparameters on the test set to avoid potential overfitting. All reported results are based on the evaluation of the last checkpoint. The Top-1 accuracy of each model is presented in Table~\ref{tab:finetune}. Besides the performance on each dataset, we also propose two \textit{composite metrics} over the 18 datasets for evaluation. The first one is the macro-average accuracy which is the average of the accuracy on each dataset. The second one is micro-average accuracy, which calculates the average accuracy on the video level. Micro-average considers the size difference of the 18 datasets. We include the details of the metrics, the complete finetuning results, and the previous best-reported performance, if any, for each dataset in \textcolor{blue}{Appendix~\ref{appendix_finetuning}}.

% For practitioners, a validation set could be split out from training data if needed. 


\begin{table*}[!t]
    %\setlength{\abovecaptionskip}{-8pt}
    \caption{Finetuning results based on the supervised pre-trained and self-supervised pre-trained models as well as the X3D pre-trained models. Generally, from the two composite metrics (macro-average accuracy and micro-average accuracy), we can tell that TSM surprisingly outperforms other counterparts in both pre-training settings.}\label{tab:finetune}
    \centering
    %\begin{small}%\rowcolor{cyan!10}
    \scalebox{0.72}{
    \begin{tabular}{c|cccccc|c|cccccc}
          \hline
          \multirow{2}*{\textbf{Dataset}} & 
          \multicolumn{6}{c|}{\textbf{Supervised pre-training}} & 
          \multirow{2}*{\textbf{X3D}} &
          \multicolumn{6}{c}{\textbf{Self-supervised pre-training}}  \\
          \cline{2-7}
          \cline{9-14}
                                      &  \textbf{TSN} &\textbf{TSM} & \textbf{I3D} & \textbf{NL} & \textbf{TimeSformer} & \textbf{VideoSwin}   & ~      & \textbf{TSN} &\textbf{TSM} & \textbf{I3D} & \textbf{NL} & \textbf{TimeSformer} & \textbf{VideoSwin}   \\ \hline
         \textbf{XD-Violence}         & \textbf{85.54}         & 82.96       & 79.93       & 79.91       & 82.51                & 82.40       & 75.11        & 80.49        & \textbf{81.73}       & 80.38        & 80.94       & 77.47                & 77.91                \\ 
         \textbf{UCF-Crime}           & 35.42         & \textbf{42.36}       & 31.94         &  34.03      & 36.11                & 34.72     & 25.69          & \textbf{37.50}        & 35.42       & 34.03        &  34.72      &   36.11             & 34.03               \\ 
         \textbf{MUVIM}               & 79.30         & \textbf{100}         &  97.80      &  98.68      & 94.71        &  \textbf{100}     & 99.56          & 99.12        & \textbf{100}         & 66.96        & 66.96       & 99.12                & \textbf{100}  \\ \hline
         \textbf{WLASL}               & 29.63        & 43.98       &   49.07      &  \textbf{52.31}    & 37.96                & 45.37         & 44.91       & 27.01        & 27.78       & 29.17        & \textbf{30.56}       &   25.56             & 28.24            \\ 
         \textbf{Jester}              & 86.31         & \textbf{95.21}       & 92.99        &  93.49      & 93.42                & 94.27      & 92.24         & 83.22        & \textbf{95.32}       & 87.23        &  93.89      & 90.33                & 90.18              \\ 
         \textbf{UAV-Human}           & 27.89         & \textbf{38.84}       &  33.49       & 33.03         & 28.93                & 38.66    & 36.07          & 15.70        & 30.75       & 31.95        &26.28        & 21.02               & \textbf{35.12}          \\ \hline
         \textbf{CharadesEGO}         & 8.26          & 8.11        &  6.13        & 6.42         & \textbf{8.58}                 & 8.55      & 5.69   & 6.29         & 6.59        & 6.24         & 6.31        & 7.59            & \textbf{7.65}             \\ 
         \textbf{Toyota Smarthome}    & 74.73         & \textbf{82.22}       & 79.51        & 76.86      & 69.21                & 79.88       & 79.09          & 68.71        & \textbf{81.34}       & 77.82        & 76.16       & 61.64                  & 80.18              \\ 
         \textbf{Mini-HACS}           & 84.69         & 80.87       &77.74         &  79.51      & 79.81                & \textbf{84.94}       & 60.57          & 64.60        & 63.24       & 70.24        & 60.57       & 73.92                  &  \textbf{75.58}           \\ 
         \textbf{MPII Cooking}        & 38.39         & 46.74       & \textbf{48.71}         &    42.19    & 40.97                & 46.59      &  42.19        & 34.45        & \textbf{50.08}       & 42.79        & 40.36       & 35.81               & 47.19            \\ \hline
         \textbf{Mini-Sports1M}       & 54.11         & 50.06       &46.90         & 46.16          & 51.79                & \textbf{55.34}     & 41.91         & 43.02        & 43.59       & 46.28        & 45.56       & 44.60                & \textbf{47.60}              \\ 
         \textbf{FineGym}             & 63.73         & \textbf{80.95}       &   72.00         & 71.21         & 63.92                & 65.02   & 68.49           & 54.62        & \textbf{75.87}       & 69.62        & 68.79       & 47.60                & 58.94              \\ 
         \textbf{MOD20}               & \textbf{98.30}         & 96.75       &  96.61      &  96.18        & 94.06                & 92.64       &  92.08        & 91.23        & 92.08       & 91.94        & 92.08       & 90.81               & \textbf{92.36}             \\ \hline
         \textbf{COIN}                & 81.15         & 78.49       & 73.79        & 74.30        & \textbf{82.99}                & 76.27       & 61.29       & 61.48        & 64.53       & 71.57        & \textbf{72.78}       & 67.64                  & 68.78              \\ 
         \textbf{MECCANO}             & \textbf{41.06}         & 39.28       & 36.88        & 36.13          & 40.95                & 38.89      & 30.78         & 32.34         & 35.10        & 34.86        & 33.62        & 33.30              & \textbf{37.80}              \\ 
         \textbf{InHARD}              & 84.39         & \textbf{88.08}       &  82.06      & 86.31       & 85.16                & 87.60          & 84.86      & 75.63          & \textbf{87.66}         & 82.54        & 80.81         & 71.28           & 80.10              \\ 
         \textbf{PETRAW}              & 94.30         & 95.72       & 94.84        & 94.54      & 94.30                & \textbf{96.43}          & 95.46    & 93.18        & \textbf{95.51}       & 95.02        & 94.38       & 85.56                  &  91.46             \\ 
         \textbf{MISAW}               & 61.44         & \textbf{75.16}       &  68.19     & 64.27        & 71.46                & 69.06         & 69.06     & 59.04        & \textbf{73.64}       & 70.37        & 64.27       & 60.78                  &  68.85             \\ \hline
         Macro Avg.                 & 62.70     & \textbf{68.10}        & 64.92       & 64.75        & 64.27       & 66.48               &  61.39      & 57.09         & \textbf{63.35}       &  60.50     & 59.39        & 57.23                & 62.33               \\ 
         Micro Avg.              & 64.92        & \textbf{70.82}       & 67.81        & 67.83       & 67.66                  &  69.73         & 65.87      & 59.13         & \textbf{68.11}       &  64.35     & 66.21        & 62.19               & 65.71         \\ \hline
    \end{tabular}}
    \vspace{-1em}
\end{table*}

% \subsection{Supervised pre-training}
% For FineGym in self-supervised pre-training, TSM outperforms VideoSwin for 16.93$\%$.

% One of the most important targets of our benchmark is to investigate whether there are some specific models which are versatile across data domains or at least capable of taking care of a typical domain, and provide model suggestions for practitioners in real-world applications. In addition, we would also like to analyze the potential reasons that lead to unsatisfying performance on specific datasets.

\vspace{-1 em}
\paragraph{Model comparison.} 
In previous studies, transformer-based video models~\cite{bertasius2021space,liu2022video} have been demonstrated to be more effective than CNNs on several representative datasets. This conclusion leads the trend of model design toward more sophisticated transformers, which makes CNNs less appealing compared with the pre-transformer era. However, we argue that the current conclusion could be biased since the comparison between transformers and CNNs is obviously unfair. Basically, it is a widely accepted notion that the selection of different backbones can inherently yield significant differences, let alone the overall model design. To this end, as aforementioned, we carefully select ConvNeXt~\cite{liu2022convnet} as the CNN backbone, which is comparable with ViT~\cite{dosovitskiy2020image} and Swin Transformer~\cite{liu2021swin} w.r.t. both model size and ImageNet classification performance. We believe such a fair comparison could lead to more convincing and compelling conclusions. As shown in Table~\ref{tab:finetune}, we notice that there is no absolute winner among all the models, but surprisingly, 2D CNNs perform better on most datasets, especially TSM, which outperforms other models in 8 out of 18 datasets. This indicates that 2D video models are still competitive with transformers when equipped with strong backbones.
% This indicates that apart from model design, better spatial feature from better backbone also plays an important role in spatiotemporal models. \textcolor{red}{up to here, we conclude 2D models are still competitive when equipped with strong backbones}
Likewise,  the two composite metrics also provide evidence that TSM outperforms other models, and transformer-based models do not exhibit clear advantages over CNN-based models. 

Inspecting further, we can see that VideoSwin excels in mini-HACS and mini-Sports1M. However, as aforementioned, these datasets, along with other popular datasets such as UCF-101 and HMDB-51, share high similarities with Kinetics-400 in terms of actions and viewpoints. Thus the performance on these datasets may not fully reflect the effectiveness of the evaluated model. Indeed, as shown in Table \ref{tab:finetune}, VideoSwin is only comparable or inferior to TSM in the other three categories (\ie, anomaly, gesture, and instructional). This demonstrates that the impressive performance on Kinetics-400 and other similar datasets may not be consistent with downstream tasks with vastly different actions. To fully probe the effectiveness of a video model, we need to evaluate it on datasets with very different distributions. Besides, we also consider the NAS-based X3D~\cite{feichtenhofer2020x3d}, which achieves good performance on Kinetics-400, to reveal the overfitting problem of tuning on the test set.

% Moreover, we claim that previous X3D~\cite{feichtenhofer2020x3d} based on neural architecture search overfits on Kinetics400, which can be less transferable on other target datasets. Thus, we also finetune an X3D-M\footnote{https://github.com/open-mmlab/mmaction2/tree/master/configs/\\recognition/x3d} and compare its performance with other supervised results. 

% Inspecting further, on UCF-Crime and FineGym, the accuracy of TSM surpasses the best transformers by a large margin (6.25$\%$ and 15.93$\%$, respectively). \textcolor{red}{inspecting further, we find transformer behaves well on datasets such as Mini-HACS and Mini-Sports1M, which share similar properties with K400. blah blah. here we discuss the second takeway, that previous evaluation protocol/dataset is biased. BEAR is more comprehensive. And we need such benchmark to better evaluate the effectiveness of spatialtemporal representation learning methods.}
% UCF-Crime could be very challenging due to its long temporal duration for each video, and the videos are captured by CCTV cameras, which makes them less recognizable than normal videos. 
% And FineGym is a fine-grained dataset containing element-level gym action and some of the categories may be confusing due to similar dynamics. 
% Besides, in existing evaluation paradigm, popular datasets (e.g., UCF101) are usually utilized as the target to finetune the pre-trained models. However, these datasets often share similar distribution as Kinetics-400; thus, the evaluation results based on such paradigm could be biased, since in real-world scenarios the data can vary a lot. Similarly, even VideoSwin wins in both mini-HACS and mini-Sports1M, which are also collected from YouTube as Kinetics-400 is, in both pre-training settings, we cannot yield a conclusion that VideoSwin is the best. However, as aforementioned, TSM is generally the best models among the six. Actually, we hence expect that BEAR could be a better alternative of current evaluation methods in action recognition.


\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
\item \emph{Despite the emergence of recent transformers, 2D video models can still be promising alternatives for action recognition if equipped with powerful backbones.}
\item \emph{Previous evaluation protocols have been limited to target datasets similar to Kinetics-400, which could potentially result in biased evaluations. However, BEAR could address this issue by including target data from five distinct domains, ensuring a more comprehensive and unbiased assessment of model performance.}
\end{itemize}
\vspace{-1 em}
\end{adjustwidth}

% \textit{This phenomenon could be persuasive evidence that CNNs could still be effective in action recognition with powerful backbones equipped, which also indicates that the spatial feature plays an important role in action recognition. In future design of video models, both the temporal modeling and the backbone should be viewed equally important. Meanwhile, it also suggests that there may be no absolute ranking relation between pre-training performance and finetuning performance for model selection in action recognition.}

\paragraph{Impact of viewpoint change} 
We also observe something interesting in terms of the data distribution. Several datasets such as UCF-Crime, UAV-Human, CharadesEGO, MPII-Cooking, and MECCANO exhibit notably low performance. Upon closer inspection of Table~\ref{table:data_info}, it is evident that these datasets involve significant viewpoint changes from Kinetics-400. For instance, UCF-Crime is collected from CCTV footage, UAV-Human contains drone-view videos, CharadesEGO only contains 1st person-view videos, and MECCANO is also egocentric. This indicates that the viewpoint change in downstream tasks could dramatically damage the model performance.
Therefore, leveraging pre-training datasets with rich egocentric visual knowledge, such as EGO4D~\cite{grauman2022ego4d}, may offer a suitable alternative to Kinetics-400 for finetuning on egocentric data.
Besides, in Sec.~\ref{sec:few-shot} and Sec.~\ref{sec:uda}, we will further discuss the challenge caused by the viewpoint change in the target domain. 
% \vspace{-1.5em}

\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
    \item \emph{Prior evaluation protocols, limited in the scope of target data, fail to capture the impact of domain gap, particularly in regard to the viewpoint, 
    on transfer performance. However, we have identified that such a distribution shift can significantly degrade the quality of spatiotemporal representation, which further undermines the transfer performance. Hence, we recommend that future studies should include pre-training datasets beyond Kinetics-400 to provide more robust representations to improve transferability.}
\end{itemize}
\end{adjustwidth}

% In this work, we also investigate domain adaptation between datasets that have overlapped action classes. Due to the space limit, we leave the discussion to Supp. 
% Some interesting observations could be obtained from both of the results of finetuning and few-shot learning. From Table~\ref{tab:finetune}, it is notably that 2D CNNs obtain higher performance on more datasets. There are 11 out of 18 where TSN and TSM outperform 3D CNNs and transformers in supervised pre-training, while the are 9 datasets where they perform better in self-supervised pertraining. 
%  Generally, in finetuning setting, TSM with ConvNeXt could be a better choice on most datasets.
% \paragraph{Self-supervised pre-training}
\noindent\textbf{Self-supervised vs. supervised pre-training} 
% We continue to investigate the questions proposed at the beginning of this section under the setting of self-supervised pre-training. 
% As shown in the right half column of Table~\ref{tab:finetune}, the selected 6 models similarly keeps their strengths and weaknesses  as they have shown under supervised pre-training. Notably, the performance of TSM surpasses other models on 9 datasets, while the two 3D CNNs only dominate on 2 datasets and TimeSFormer surprisingly have not shown any advantages under this setting. TSM continue its absolute superiority on the finegrained FinGym, where its performance is 16.93$\%$ higher than the best transformer and 6.25$\%$ higher than the best 2D CNN. Besides, we can also observe that transformer-based models, compared with TSM, perform a more severe degradation from supervised pre-training to self-supervised pre-training. For instance, on InHARD, PETRAW and MISAW, the performance of the transformers averagely drop 10.69$\%$, 6.86$\%$ and 5.45$\%$, while the decreases are respectively 0.42$\%$, 0.21$\%$ and 1.52$\%$ for TSM. \textit{This phenomenon further consolidate that with comparative backbones, 2D CNNs are still effective and could sometimes largely outperform the latest transformers.}
% Moreover, there are also some interesting observations beyond model comparison. 
As can be seen from Table~\ref{tab:finetune}, it is notable that the overall finetuning performance of the self-supervised pre-training is less competitive than its supervised counterpart even for TSM. The most pronounced accuracy drop can be found in WLASL and FineGym. The performance of 3D Nonlocal network on WLASL drops from 52.31$\%$ to 30.56$\%$ and the performance of TimeSformer also decreases more than 15$\%$. 
% Based on the fact that the finetuning settings are consistent for the supervised and self-supervised pre-training, we can reach a conclusion that there is a certain reason related to data itself, which makes self-supervised pre-training more prone to be affected, that leads to its lower effectiveness than supervised one. 
To reveal the potential reason behind this, we further scrutinize the data distribution gap between the selected 18 target datasets and Kinetics-400. We observe different types of domain shifts, such as UAV-Human containing only drone-view data and the egocentric MECCANO which differs significantly from Kinetics-400. We conclude that self-supervised pre-training is more susceptible to domain shifts between Kinetics-400 and the target datasets than supervised pre-training. In Sec.~\ref{sec:few-shot}, we take a step forward on this topic by investigating few-shot settings, which are more likely to occur in real-world scenarios.
% For instance, the UAV-Human is purely constituted with drone-view videos where the objective only exists in a small part of the frame, which are quite different with the Kinetics400 videos; and the videos in MECCANO are recorded in a assembly workshop that contains different viewpoints. 
% We also notice that both pre-training methods obtain comparable (the performance of self-supervised pre-training is only averagely 2.59$\%$ lower) performance on Jester, we hypothesis that this is because the Jester videos contains clear gestures that are easily to recognize, which reduces the impact brought by the domain shift.

% \vspace{-0.5 em}
\begin{adjustwidth}{-1.5em}{}
\begin{itemize}
    \item \emph{Self-supervised finetuning generally cannot outperform its supervised counterpart and TSM consistently performs well under the self-supervised setting.}
\end{itemize}
\vspace{-1.em}
\end{adjustwidth}

% \textit{Generally, TSM performs consistently well based on self-supervised pre-trained weights and we claim that self-supervised pre-training is more prone to the domain shift between Kinetics400 and the target datasets than supervised pre-training. In the following section, to obtain a thorough analysis, we take one step forward on this topic by investigating more under few-shot settings, which is more likely to happen in real-world scenarios.}
% This observation is similar to what has been discussed in ~\cite{thoker2022severe} that factors such as downstream domain shift could be vital for the performance based on self-supervised pretrained weights. 
% \paragraph{The overfitting of X3D-M} 
% Besides the 6 models, we also show the finetuning performance of X3D-M~\cite{feichtenhofer2020x3d} which is a structure discovered by NAS. X3D-M has a top-1 accuracy of 75.0\% on Kinetics-400, which is competitive as the models in Table \ref{pretrained}. However, as aforementioned, the NAS process is tuned on the test set and the model may not generalize well to other datasets. The results in Table \ref{tab:finetune} validate our hypothesis. We can see that X3D cannot obtain better performance than any of the best models on each dataset, and only surpass 3D CNNs on MUVIM, UAV-Human, Toyota Smarthome, and PETRAW. This demonstrates that extremely small model architecture may not have the capacity for effective transfer. It also indicates the importance of using a held-out test set for fair comparisons.
% has been demonstrated to be effectiveness via NAS, however, we claim that such method may results in overfitting in source dataset. The results of the finetuning of X3D-M validate our hypothesis that X3D-M cannot obtain better performance than any of the best models on each datasets , and only surpass 3D CNNs on MUVIM, UAV-Human, Toyota Smarthome and PETRAW.

