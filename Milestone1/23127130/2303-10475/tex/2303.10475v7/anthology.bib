@inproceedings{mishra-etal-2022-reframing,
  title={{R}eframing {I}nstructional {P}rompts to {GPT}k’s {L}anguage},
    author = "Mishra, Swaroop  and
      Khashabi, Daniel  and
      Baral, Chitta  and
      Choi, Yejin  and
      Hajishirzi, Hannaneh",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    year = "2022",
    pages = "589--612",
}

@article{min2022rethinking,
  title={Rethinking the {R}ole of {D}emonstrations: {W}hat {M}akes {I}n-Context {L}earning {W}ork?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}

@article{parmar2022don,
  title={Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions},
  author={Parmar, Mihir and Mishra, Swaroop and Geva, Mor and Baral, Chitta},
  journal={arXiv preprint arXiv:2205.00415},
  year={2022}
}

@inproceedings{mishra2022cross,
  title={{C}ross-{T}ask {G}eneralization via {N}atural {L}anguage {C}rowdsourcing {I}nstructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3470--3487},
  year={2022}
}

@inproceedings{wang2022benchmarking,
  title={{B}enchmarking {G}eneralization via {I}n-context {I}nstructions on 1,600+ {L}anguage {T}asks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
  year = "2022",
  pages = "5085--5109",
}


@inproceedings{yin2022contintin,
  author    = {Wenpeng Yin and
               Jia Li and
               Caiming Xiong},
  title     = {{C}on{T}in{T}in: {C}ontinual {L}earning from {T}ask {I}nstructions},
  booktitle = {ACL},
  pages     = {3062--3072},
  year      = {2022}
}

@article{DBLP03871,
  author    = {Fatima T. AL{-}Khawaldeh},
  title     = {{A} {S}tudy of the {E}ffect of {R}esolving {N}egation and {S}entiment {A}nalysis
               in {R}ecognizing {T}ext {E}ntailment for {A}rabic},
  journal   = {CoRR},
  volume    = {abs/1907.03871},
  year      = {2019}
}

@article{gupta2022improving,
  title={{I}mproving {Z}ero and {F}ew-shot {G}eneralization in {D}ialogue through {I}nstruction {T}uning},
  author={Gupta, Prakhar and Jiao, Cathy and Yeh, Yi-Ting and Mehri, Shikib and Eskenazi, Maxine and Bigham, Jeffrey P},
  journal={arXiv preprint arXiv:2205.12673},
  year={2022}
}

@article{hu2022context,
  title={In-Context {L}earning for {F}ew-Shot {D}ialogue {S}tate {T}racking},
  author={Hu, Yushi and Lee, Chia-Hsuan and Xie, Tianbao and Yu, Tao and Smith, Noah A and Ostendorf, Mari},
  journal={arXiv preprint arXiv:2203.08568},
  year={2022}
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
  journal={arXiv preprint arXiv:2205.05638},
  year={2022}
}

@article{goldwasser2014learning,
  title={Learning from natural instructions},
  author={Goldwasser, Dan and Roth, Dan},
  journal={Machine learning},
  volume={94},
  number={2},
  pages={205--232},
  year={2014},
}

@article{efrat2020turking,
  title={The Turking Test: Can Language Models Understand Instructions?},
  author={Efrat, Avia and Levy, Omer},
  journal={arXiv preprint arXiv:2010.11982},
  year={2020}
}

@inproceedings{gaddy2019pre,
  title={Pre-Learning Environment Representations for Data-Efficient Neural Instruction Following},
  author={Gaddy, David and Klein, Dan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1946--1956},
  year={2019}
}

@inproceedings{weller2020learning,
  title={Learning from Task Descriptions},
  author={Weller, Orion and Lourie, Nicholas and Gardner, Matt and Peters, Matthew E},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1361--1375},
  year={2020}
}

@inproceedings{wang2020learning,
  title={Learning from Explanations with Neural Execution Tree},
  author={Wang, Ziqi and Qin, Yujia and Zhou, Wenxuan and Yan, Jun and Ye, Qinyuan and Neves, Leonardo and Liu, Zhiyuan and Ren, Xiang},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{hancock2018training,
  title={Training Classifiers with Natural Language Explanations},
  author={Hancock, Braden and Varma, Paroma and Wang, Stephanie and Bringmann, Martin and Liang, Percy and R{\'e}, Christopher},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1884--1895},
  year={2018}
}

@inproceedings{srivastava2017joint,
  title={Joint concept learning and semantic parsing from natural language explanations},
  author={Srivastava, Shashank and Labutov, Igor and Mitchell, Tom},
  booktitle={Proceedings of the 2017 conference on empirical methods in natural language processing},
  pages={1527--1536},
  year={2017}
}

@inproceedings{ye2020teaching,
  title={Teaching Machine Comprehension with Compositional Explanations},
  author={Ye, Qinyuan and Huang, Xiao and Boschee, Elizabeth and Ren, Xiang},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={1599--1615},
  year={2020}
}

@article{murty2020expbert,
  title={Expbert: Representation engineering with natural language explanations},
  author={Murty, Shikhar and Koh, Pang Wei and Liang, Percy},
  journal={arXiv preprint arXiv:2005.01932},
  year={2020}
}

@inproceedings{zhong2021adapting,
  title={Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections},
  author={Zhong, Ruiqi and Lee, Kristy and Zhang, Zheng and Klein, Dan},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={2856--2878},
  year={2021}
}

@inproceedings{yin2019benchmarking,
  title={Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach},
  author={Yin, Wenpeng and Hay, Jamaal and Roth, Dan},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3914--3923},
  year={2019}
}

@article{acquaviva2021communicating,
  title={Communicating natural programs to humans and machines},
  author={Acquaviva, Samuel and Pu, Yewen and Kryven, Marta and Sechopoulos, Theodoros and Wong, Catherine and Ecanow, Gabrielle E and Nye, Maxwell and Tessler, Michael Henry and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2106.07824},
  year={2021}
}

@inproceedings{vogel2010learning,
  title={Learning to follow navigational directions},
  author={Vogel, Adam and Jurafsky, Dan},
  booktitle={Proceedings of the 48th annual meeting of the association for computational linguistics},
  pages={806--814},
  year={2010}
}

@inproceedings{kuhlmann2004guiding,
  title={Guiding a reinforcement learner with natural language advice: Initial results in RoboCup soccer},
  author={Kuhlmann, Gregory and Stone, Peter and Mooney, Raymond and Shavlik, Jude},
  booktitle={The AAAI-2004 workshop on supervisory control of learning and adaptive systems},
  year={2004},
  organization={San Jose, CA}
}

@inproceedings{matuszek2012joint,
  title={A joint model of language and perception for grounded attribute learning},
  author={Matuszek, Cynthia and FitzGerald, Nicholas and Zettlemoyer, Luke and Bo, Liefeng and Fox, Dieter},
  booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages={1435--1442},
  year={2012}
}

@inproceedings{clarke2010driving,
  title={Driving semantic parsing from the world’s response},
  author={Clarke, James and Goldwasser, Dan and Chang, Ming-Wei and Roth, Dan},
  booktitle={Proceedings of the fourteenth conference on computational natural language learning},
  pages={18--27},
  year={2010}
}

@inproceedings{branavan2010reading,
  title={Reading between the lines: Learning to map high-level instructions to commands},
  author={Branavan, SRK and Zettlemoyer, Luke and Barzilay, Regina},
  booktitle={Proceedings of the 48th annual meeting of the association for computational linguistics},
  pages={1268--1277},
  year={2010}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{radford2019language,
  title={{L}anguage {M}odels are {U}nsupervised {M}ultitask {L}earners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  journal={OpenAI blog}
}

@article{brown2020language,
  title={{L}anguage {M}odels are {F}ew-shot {L}earners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{lewis2020bart,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7871--7880},
  year={2020}
}

@inproceedings{eisenstein2009reading,
  title={Reading to learn: Constructing features from semantic abstracts},
  author={Eisenstein, Jacob and Clarke, James and Goldwasser, Dan and Roth, Dan},
  booktitle={Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
  pages={958--967},
  year={2009}
}

@inproceedings{branavan2011learning,
  title={Learning to Win by Reading Manuals in a Monte-Carlo Framework},
  author={Branavan, SRK and Silver, David and Barzilay, Regina},
  booktitle={Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  pages={268--277},
  year={2011}
}

@inproceedings{babecs2012learning,
  title={Learning to interpret natural language instructions},
  author={Babe{\c{s}}-Vroman, Monica and MacGlashan, James and Gao, Ruoyuan and Winner, Kevin and Adjogah, Richard and Littman, Michael and Muresan, Smaranda and others},
  booktitle={Proceedings of the Second Workshop on Semantic Interpretation in an Actionable Context},
  pages={1--6},
  year={2012}
}

@inproceedings{puig2018virtualhome,
  title={Virtualhome: Simulating household activities via programs},
  author={Puig, Xavier and Ra, Kevin and Boben, Marko and Li, Jiaman and Wang, Tingwu and Fidler, Sanja and Torralba, Antonio},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8494--8502},
  year={2018}
}

@inproceedings{chen2008learning,
  title={Learning to sportscast: a test of grounded language acquisition},
  author={Chen, David L and Mooney, Raymond J},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={128--135},
  year={2008}
}

@inproceedings{kim2012unsupervised,
  title={Unsupervised pcfg induction for grounded language learning with highly ambiguous supervision},
  author={Kim, Joohyun and Mooney, Raymond},
  booktitle={Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning},
  pages={433--444},
  year={2012}
}

@article{artzi2013weakly,
  title={Weakly supervised learning of semantic parsers for mapping instructions to actions},
  author={Artzi, Yoav and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={1},
  pages={49--62},
  year={2013},
}

@article{krishnamurthy2013jointly,
  title={Jointly learning to parse and perceive: Connecting natural language to the physical world},
  author={Krishnamurthy, Jayant and Kollar, Thomas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={1},
  pages={193--206},
  year={2013},
}

@inproceedings{chen2012fast,
  title={Fast online lexicon learning for grounded language acquisition},
  author={Chen, David},
  booktitle={Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={430--439},
  year={2012}
}

@article{tellex2011approaching,
  title={Approaching the symbol grounding problem with probabilistic graphical models},
  author={Tellex, Stefanie and Kollar, Thomas and Dickerson, Steven and Walter, Matthew R and Banerjee, Ashis Gopal and Teller, Seth and Roy, Nicholas},
  journal={AI magazine},
  volume={32},
  number={4},
  pages={64--76},
  year={2011}
}

@inproceedings{bisk2016natural,
  title={Natural language communication with robots},
  author={Bisk, Yonatan and Yuret, Deniz and Marcu, Daniel},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={751--761},
  year={2016}
}

@inproceedings{li2020interactive,
  title={Interactive Task Learning from GUI-Grounded Natural Language Instructions and Demonstrations},
  author={Li, Toby Jia-Jun and Mitchell, Tom and Myers, Brad},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={215--223},
  year={2020}
}

@inproceedings{liang2009learning,
  title={Learning semantic correspondences with less supervision},
  author={Liang, Percy and Jordan, Michael I and Klein, Dan},
  booktitle={Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP},
  pages={91--99},
  year={2009}
}

@inproceedings{srivastava2018zero,
  title={Zero-shot learning of classifiers from natural language quantification},
  author={Srivastava, Shashank and Labutov, Igor and Mitchell, Tom},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={306--316},
  year={2018}
}

@inproceedings{bach2022promptsource,
  title={PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts},
  author={Bach, Stephen and Sanh, Victor and Yong, Zheng Xin and Webson, Albert and Raffel, Colin and Nayak, Nihal V and Sharma, Abheesht and Kim, Taewoon and Bari, M Saiful and F{\'e}vry, Thibault and others},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={93--104},
  year={2022}
}

@inproceedings{lhoest2021datasets,
  title={Datasets: A Community Library for Natural Language Processing},
  author={Lhoest, Quentin and del Moral, Albert Villanova and Jernite, Yacine and Thakur, Abhishek and von Platen, Patrick and Patil, Suraj and Chaumond, Julien and Drame, Mariama and Plu, Julien and Tunstall, Lewis and others},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={175--184},
  year={2021}
}

@article{lin2021few,
  title={Few-shot learning with multilingual language models},
  author={Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott, Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and others},
  journal={arXiv preprint arXiv:2112.10668},
  year={2021}
}

@inproceedings{min-etal-2022-metaicl,
    title = "{M}eta{ICL}: {L}earning to {L}earn {I}n {C}ontext",
    author = "Min, Sewon  and
      Lewis, Mike  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2022",
    pages = "2791--2809"
}

@inproceedings{sanh2021multitask,
  title={Multitask {P}rompted {T}raining {E}nables {Z}ero-{S}hot {T}ask {G}eneralization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Raja, Arun and Dey, Manan and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{wei2021finetuned,
  title={Finetuned {L}anguage {M}odels are {Z}ero-{S}hot {L}earners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{zhang2020analogous,
  title={Analogous Process Structure Induction for Sub-event Sequence Prediction},
  author={Zhang, Hongming and Chen, Muhao and Wang, Haoyu and Song, Yangqiu and Roth, Dan},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1541--1550},
  year={2020}
}

@inproceedings{zhang2021learning,
  title={Learning to decompose and organize complex tasks},
  author={Zhang, Yi and Jauhar, Sujay Kumar and Kiseleva, Julia and White, Ryen and Roth, Dan},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2726--2735},
  year={2021}
}

@inproceedings{chen2011learning,
  title={Learning to interpret natural language navigation instructions from observations},
  author={Chen, David and Mooney, Raymond},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={25},
  pages={859--865},
  year={2011}
}

@article{mishra2022help,
  title={HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models},
  author={Mishra, Swaroop and Nouri, Elnaz},
  journal={arXiv preprint arXiv:2208.08232},
  year={2022}
}

@article{chakrabarty2022help,
  title={Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing},
  author={Chakrabarty, Tuhin and Padmakumar, Vishakh and He, He},
  journal={arXiv preprint arXiv:2210.13669},
  year={2022}
}

@inproceedings{webson-pavlick-2022-prompt,
    title = "Do {P}rompt-Based {M}odels {R}eally {U}nderstand the {M}eaning of {T}heir {P}rompts?",
    author = "Webson, Albert  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2022",
    pages = "2300--2344"
}

@inproceedings{zhao2021calibrate,
  title={Calibrate {B}efore {U}se: {I}mproving {F}ew-shot {P}erformance of {L}anguage {M}odels},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International Conference on Machine Learning},
  pages={12697--12706},
  year={2021},
}

@article{dwivedi2022editeval,
  title={EditEval: An Instruction-Based Benchmark for Text Improvements},
  author={Dwivedi-Yu, Jane and Schick, Timo and Jiang, Zhengbao and Lomeli, Maria and Lewis, Patrick and Izacard, Gautier and Grave, Edouard and Riedel, Sebastian and Petroni, Fabio},
  journal={arXiv preprint arXiv:2209.13331},
  year={2022}
}

@inproceedings{sinha2021unnatural,
  title={UnNatural Language Inference},
  author={Sinha, Koustuv and Parthasarathi, Prasanna and Pineau, Joelle and Williams, Adina},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={7329--7346},
  year={2021}
}

@article{o2021context,
  title={What Context Features Can Transformer Language Models Use?},
  author={O'Connor, Joe and Andreas, Jacob},
  journal={arXiv preprint arXiv:2106.08367},
  year={2021}
}

@inproceedings{pham2021out,
  title={Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?},
  author={Pham, Thang and Bui, Trung and Mai, Long and Nguyen, Anh},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={1145--1160},
  year={2021}
}

@inproceedings{gupta2021bert,
  title={Bert \& family eat word salad: Experiments with text understanding},
  author={Gupta, Ashim and Kvernadze, Giorgi and Srikumar, Vivek},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={12946--12954},
  year={2021}
}

@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3045--3059},
  year={2021}
}

@article{liu2021gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10385},
  year={2021}
}

@article{ye2022retrieval,
  title={Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization},
  author={Ye, Seonghyeon and Jang, Joel and Kim, Doyoung and Jo, Yongrae and Seo, Minjoon},
  journal={arXiv preprint arXiv:2210.03029},
  year={2022}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{raffel2020exploring,
  title={{E}xploring the {L}imits of {T}ransfer {L}earning with a {U}nified {T}ext-{T}o-{T}ext {T}ransformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{ma2021gradts,
  title={GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks},
  author={Ma, Weicheng and Lou, Renze and Zhang, Kai and Wang, Lili and Vosoughi, Soroush},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5621--5632},
  year={2021}
}

@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997}
}

@inproceedings{liu2019multi,
  title={Multi-Task Deep Neural Networks for Natural Language Understanding},
  author={Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4487--4496},
  year={2019}
}

@article{lin2022unsupervised,
  title={{U}nsupervised {C}ross-{T}ask {G}eneralization via {R}etrieval {A}ugmentation},
  author={Lin, Bill Yuchen and Tan, Kangmin and Miller, Chris and Tian, Beiwen and Ren, Xiang},
  journal={arXiv preprint arXiv:2204.07937},
  year={2022}
}

@article{vaswani2017attention,
  title={{Attention is All You Need}},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{yu2017seqgan,
  title={{S}eq{GAN}: {S}equence {G}enerative {A}dversarial {N}ets with {P}olicy {G}radient},
  author={Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  year={2017}
}

@inproceedings{lin2004rouge,
  title={Rouge: A {P}ackage for {A}utomatic {E}valuation of {S}ummaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{rajpurkar2016squad,
  title={{SQuAD}: 100,000+ {Q}uestions for {M}achine {C}omprehension of {T}ext},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={2383--2392},
  year={2016}
}

@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020}
}

@INPROCEEDINGS{9413437,
  author={Wang, Yutong and Lou, Renze and Zhang, Kai and Chen, Mao Yan and Yang, Yujiu},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={More: A {M}etric {L}earning {B}ased {F}ramework for {O}pen-{D}omain {R}elation {E}xtraction}, 
  year={2021},
  volume={},
  number={},
  pages={7698-7702}}

@inproceedings{wang2019ranked,
  title={{Ranked List Loss for Deep Metric Learning}},
  author={Wang, Xinshao and Hua, Yang and Kodirov, Elyor and Hu, Guosheng and Garnier, Romain and Robertson, Neil M},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5207--5216},
  year={2019}
}

@article{kingma2014adam,
  title={{Adam}: {A} {M}ethod for {S}tochastic {O}ptimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{vinyals2015pointer,
  title={Pointer {N}etworks},
  author={Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{10.1162/neco.1997.9.8.1735,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long {S}hort-{T}erm {M}emory},
year = {1997},
volume = {9},
number = {8},
journal = {Neural Comput.},
month = {nov},
pages = {1735–1780},
numpages = {46}
}

@article{sutton1999policy,
  title={Policy {G}radient {M}ethods for {R}einforcement {L}earning with {F}unction {A}pproximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{liu2019roberta,
  title={{R}o{BERT}a: A {R}obustly {O}ptimized {B}ert {P}retraining {A}pproach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{zhu2021topic,
  title={{T}opic-{D}riven and {K}nowledge-{A}ware {T}ransformer for {D}ialogue {E}motion {D}etection},
  author={Zhu, Lixing and Pergola, Gabriele and Gui, Lin and Zhou, Deyu and He, Yulan},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1571--1582},
  year={2021}
}

@inproceedings{gao2021improving,
  title={{I}mproving {E}mpathetic {R}esponse {G}eneration by {R}ecognizing {E}motion {C}ause in {C}onversations},
  author={Gao, Jun and Liu, Yuhan and Deng, Haolin and Wang, Wei and Cao, Yu and Du, Jiachen and Xu, Ruifeng},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={807--819},
  year={2021}
}

@article{maddison2016concrete,
  title={The {C}oncrete {D}istribution: {A} {C}ontinuous {R}elaxation of {D}iscrete {R}andom {V}ariables},
  author={Maddison, Chris J and Mnih, Andriy and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1611.00712},
  year={2016}
}

@article{jang2016categorical,
  title={{C}ategorical {R}eparameterization with {G}umbel-{S}oftmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

@inproceedings{see2017get,
  title={{G}et {T}o {T}he {P}oint: {S}ummarization with {P}ointer-{G}enerator {N}etworks},
  author={See, Abigail and Liu, Peter J and Manning, Christopher D},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1073--1083},
  year={2017}
}

@inproceedings{gu2016incorporating,
  title={Incorporating Copying Mechanism in Sequence-to-Sequence Learning},
  author={Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor OK},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1631--1640},
  year={2016}
}

@inproceedings{schick2021few,
  title={{Few-shot Text Generation with Natural Language Instructions}},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={390--402},
  year={2021}
}

@inproceedings{chen2022knowprompt,
  title={{K}nowprompt: {K}nowledge-aware {P}rompt-tuning with {S}ynergistic {O}ptimization for {R}elation {E}xtraction},
  author={Chen, Xiang and Zhang, Ningyu and Xie, Xin and Deng, Shumin and Yao, Yunzhi and Tan, Chuanqi and Huang, Fei and Si, Luo and Chen, Huajun},
  booktitle={Proceedings of the ACM Web Conference 2022},
  pages={2778--2788},
  year={2022}
}

@article{rubin2021learning,
  title={{Learning to Retrieve Prompts for In-context Learning}},
  author={Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  journal={arXiv preprint arXiv:2112.08633},
  year={2021}
}

@inproceedings{lu2022fantastically,
  title={{F}antastically {O}rdered {P}rompts and {W}here to {F}ind {T}hem: {O}vercoming {F}ew-Shot {P}rompt {O}rder {S}ensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8086--8098},
  year={2022}
}

@inproceedings{min2022noisy,
  title={Noisy {C}hannel {L}anguage {M}odel {P}rompting for {F}ew-Shot {T}ext {C}lassification},
  author={Min, Sewon and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5316--5330},
  year={2022}
}

@inproceedings{chen2022meta,
  title={Meta-learning via Language Model In-context Tuning},
  author={Chen, Yanda and Zhong, Ruiqi and Zha, Sheng and Karypis, George and He, He},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={719--730},
  year={2022}
}

@article{jang2022can,
  title={Can large language models truly understand prompts? a case study with negated prompts},
  author={Jang, Joel and Ye, Seonghyeon and Seo, Minjoon},
  journal={arXiv preprint arXiv:2209.12711},
  year={2022}
}

@inproceedings{sun-lu-2022-implicit,
    title = "Implicit n-grams Induced by Recurrence",
    author = "Sun, Xiaobing  and
      Lu, Wei",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2022",
    pages = "1624--1639",
}

@article{samin2022arguments,
  title={Arguments to Key Points Mapping with Prompt-based Learning},
  author={Samin, Ahnaf Mozib and Nikandish, Behrooz and Chen, Jingyan},
  journal={arXiv preprint arXiv:2211.14995},
  year={2022}
}

@article{burns2022discovering,
  title={Discovering Latent Knowledge in Language Models Without Supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@inproceedings{shen2021towards,
  title={{T}owards {D}omain-{G}eneralizable {P}araphrase {I}dentification by {A}voiding the {S}hortcut {L}earning},
  author={Shen, Xin and Lam, Wai},
  booktitle={Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)},
  pages={1318--1325},
  year={2021}
}

@inproceedings{du2021towards,
  title={{T}owards {I}nterpreting and {M}itigating {S}hortcut {L}earning {B}ehavior of {NLU} {M}odels},
  author={Du, Mengnan and Manjunatha, Varun and Jain, Rajiv and Deshpande, Ruchi and Dernoncourt, Franck and Gu, Jiuxiang and Sun, Tong and Hu, Xia},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={915--929},
  year={2021}
}

@article{radford2018improving,
  title={Improving {L}anguage {U}nderstanding by {G}enerative {P}re-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  journal={arXiv preprint}
}

@article{goertzel2014artificial,
  title={Artificial {G}eneral {I}ntelligence: {C}oncept, {S}tate of {T}he {A}rt, and {F}uture {P}rospects},
  author={Goertzel, Ben},
  journal={Journal of Artificial General Intelligence},
  volume={5},
  number={1},
  pages={1},
  year={2014},
  publisher={De Gruyter Poland}
}

@article{liu2023pre,
  title={Pre-train, {P}rompt, and {P}redict: {A} {S}ystematic {S}urvey of {P}rompting {M}ethods in {N}atural {L}anguage {P}rocessing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@inproceedings{ye2021learning,
  title={Learning to {G}enerate {T}ask-{S}pecific {A}dapters from {T}ask {D}escription},
  author={Ye, Qinyuan and Ren, Xiang},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  pages={646--653},
  year={2021}
}

@article{ivison2022hint,
  title={{HINT}: {H}ypernetwork {I}nstruction {T}uning for {E}fficient {Z}ero-{S}hot {G}eneralisation},
  author={Ivison, Hamish and Bhagia, Akshita and Wang, Yizhong and Hajishirzi, Hannaneh and Peters, Matthew},
  journal={arXiv preprint arXiv:2212.10315},
  year={2022}
}

@article{deb2022boosting,
  title={Boosting {N}atural {L}anguage {G}eneration from {I}nstructions with {M}eta-{L}earning},
  author={Deb, Budhaditya and Zheng, Guoqing and Awadallah, Ahmed Hassan},
  journal={arXiv preprint arXiv:2210.11617},
  year={2022}
}

@article{chung2022scaling,
  title={Scaling {I}nstruction-{F}inetuned {L}anguage {M}odels},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{prasad2022grips,
  title={{GRIPS}: {G}radient-{F}ree, {E}dit-{B}ased {I}nstruction {S}earch for {P}rompting {L}arge {L}anguage {M}odels},
  author={Prasad, Archiki and Hase, Peter and Zhou, Xiang and Bansal, Mohit},
  journal={arXiv preprint arXiv:2203.07281},
  year={2022}
}

@inproceedings{khashabi2022prompt,
  title={{P}rompt {W}aywardness: {T}he {C}urious {C}ase of {D}iscretized {I}nterpretation of {C}ontinuous {P}rompts},
  author={Khashabi, Daniel and Lyu, Xinxi and Min, Sewon and Qin, Lianhui and Richardson, Kyle and Welleck, Sean and Hajishirzi, Hannaneh and Khot, Tushar and Sabharwal, Ashish and Singh, Sameer and others},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={3631--3643},
  year={2022}
}

@article{gu2022robustness,
  title={{R}obustness of {L}earning from {T}ask {I}nstructions},
  author={Gu, Jiasheng and Xu, Hanzi and Nie, Liangyu and Yin, Wenpeng},
  journal={arXiv preprint arXiv:2212.03813},
  year={2022}
}

@article{dong2022survey,
  title={A {S}urvey on {I}n-{C}ontext {L}earning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and others},
  journal={arXiv preprint arXiv:2301.00234},
  year={2023}
}

@article{huang2022towards,
  title={Towards {R}easoning in {L}arge {L}anguage {M}odels: {A} {S}urvey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{qiao2022reasoning,
  title={Reasoning with {L}anguage {M}odel {P}rompting: {A} {S}urvey},
  author={Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  journal={arXiv preprint arXiv:2212.09597},
  year={2022}
}

@inproceedings{gupta-etal-2022-instructdial,
    title = "{I}nstruct{D}ial: {I}mproving {Z}ero and {F}ew-shot {G}eneralization in {D}ialogue through {I}nstruction {T}uning",
    author = "Gupta, Prakhar  and
      Jiao, Cathy  and
      Yeh, Yi-Ting  and
      Mehri, Shikib  and
      Eskenazi, Maxine  and
      Bigham, Jeffrey",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    pages = "505--525",
}

@article{iyer2022opt,
  title={{OPT}-{IML}: {S}caling {L}anguage {M}odel {I}nstruction {M}eta {L}earning through the {L}ens of {G}eneralization},
  author={Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, D{\'a}niel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
  journal={arXiv preprint arXiv:2212.12017},
  year={2022}
}

@article{ye2022guess,
  title={Guess the {I}nstruction! {M}aking {L}anguage {M}odels {S}tronger {Z}ero-{S}hot {L}earners},
  author={Ye, Seonghyeon and Kim, Doyoung and Jang, Joel and Shin, Joongbo and Seo, Minjoon},
  journal={arXiv preprint arXiv:2210.02969},
  year={2022}
}

@article{su2022one,
  title={One {E}mbedder, {A}ny {T}ask: {I}nstruction-{F}inetuned {T}ext {E}mbeddings},
  author={Su, Hongjin and Kasai, Jungo and Wang, Yizhong and Hu, Yushi and Ostendorf, Mari and Yih, Wen-tau and Smith, Noah A and Zettlemoyer, Luke and Yu, Tao and others},
  journal={arXiv preprint arXiv:2212.09741},
  year={2022}
}

@article{wang2022self,
  title={{S}elf-{I}nstruct: {A}ligning {L}anguage {M}odel with {S}elf {G}enerated {I}nstructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{longpre2023flan,
  title={The {F}lan {C}ollection: {D}esigning {D}ata and {M}ethods for {E}ffective {I}nstruction {T}uning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@article{xu2022zeroprompt,
  title={Zero{P}rompt: {S}caling {P}rompt-{B}ased {P}retraining to 1,000 {T}asks {I}mproves {Z}ero-{S}hot {G}eneralization},
  author={Xu, Hanwei and Chen, Yujun and Du, Yulun and Shao, Nan and Wang, Yanggang and Li, Haiyu and Yang, Zhilin},
  journal={arXiv preprint arXiv:2201.06910},
  year={2022}
}

@inproceedings{min2022metaicl,
  title={Meta{ICL}: {L}earning to {L}earn {I}n {C}ontext},
  author={Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2791--2809},
  year={2022}
}

@article{srivastava2022beyond,
  title={Beyond the {I}mitation {G}ame: {Q}uantifying and {E}xtrapolating the {C}apabilities of {L}anguage {M}odels},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{huynh2021survey,
  title={A {S}urvey of {NLP}-related {C}rowdsourcing {H}its: {W}hat {W}orks and {W}hat {D}oes {N}ot},
  author={Huynh, Jessica and Bigham, Jeffrey and Eskenazi, Maxine},
  journal={arXiv preprint arXiv:2111.05241},
  year={2021}
}

@inproceedings{schick2021exploiting,
  title={Exploiting {C}loze-{Q}uestions for {F}ew-{S}hot {T}ext {C}lassification and {N}atural {L}anguage {I}nference},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={255--269},
  year={2021}
}

@article{gonen2022demystifying,
  title={Demystifying prompts in language models via perplexity estimation},
  author={Gonen, Hila and Iyer, Srini and Blevins, Terra and Smith, Noah A and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2212.04037},
  year={2022}
}

@article{tay2022unifying,
  title={Unifying {L}anguage {L}earning {P}aradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022}
}

@article{jang2023exploring,
  title={Exploring the {B}enefits of {T}raining {E}xpert {L}anguage {M}odels over {I}nstruction {T}uning},
  author={Jang, Joel and Kim, Seungone and Ye, Seonghyeon and Kim, Doyoung and Logeswaran, Lajanugen and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
  journal={arXiv preprint arXiv:2302.03202},
  year={2023}
}

@inproceedings{aghajanyan-etal-2021-muppet,
    title = "Muppet: Massive Multi-task Representations with Pre-Finetuning",
    author = "Aghajanyan, Armen  and
      Gupta, Anchit  and
      Shrivastava, Akshat  and
      Chen, Xilun  and
      Zettlemoyer, Luke  and
      Gupta, Sonal",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    pages = "5799--5811",
}

@article{kojima2022large,
  title={Large {L}anguage {M}odels are {Z}ero-{S}hot {R}easoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{ding2022delta,
  title={Delta {T}uning: {A} {C}omprehensive {S}tudy of {P}arameter {E}fficient {M}ethods for {P}re-trained {L}anguage {M}odels},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={arXiv preprint arXiv:2203.06904},
  year={2022}
}

@article{singhal2022large,
  title={Large {L}anguage {M}odels {E}ncode {C}linical {K}nowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={arXiv preprint arXiv:2212.13138},
  year={2022}
}

@article{Sun2023HowDI,
  title={How {D}oes {I}n-{C}ontext {L}earning {H}elp {P}rompt {T}uning?},
  author={Simeng Sun and Yang Liu and Dan Iter and Chenguang Zhu and Mohit Iyyer},
  journal={arXiv preprint arXiv:2302.11521},
  year={2023}
}

@inproceedings{schick-schutze-2021-just,
    title = "It{'}s {N}ot {J}ust {S}ize {T}hat {M}atters: {S}mall {L}anguage {M}odels {A}re {A}lso {F}ew-{S}hot {L}earners",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2021",
    pages = "2339--2352",
}

@inproceedings{strubell-etal-2019-energy,
    title = "Energy and {P}olicy {C}onsiderations for {D}eep {L}earning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    year = "2019",
    pages = "3645--3650",
}

@inproceedings{deng-etal-2022-rlprompt,
    title = "{RLP}rompt: {O}ptimizing {D}iscrete {T}ext {P}rompts with {R}einforcement {L}earning",
    author = "Deng, Mingkai  and
      Wang, Jianyu  and
      Hsieh, Cheng-Ping  and
      Wang, Yihan  and
      Guo, Han  and
      Shu, Tianmin  and
      Song, Meng  and
      Xing, Eric  and
      Hu, Zhiting",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    pages = "3369--3391",
}

@article{honovich2022instruction,
  title={Instruction {I}nduction: {F}rom {F}ew {E}xamples to {N}atural {L}anguage {T}ask {D}escriptions},
  author={Honovich, Or and Shaham, Uri and Bowman, Samuel R and Levy, Omer},
  journal={arXiv preprint arXiv:2205.10782},
  year={2022}
}

@article{wang2019superglue,
  title={Super{GLUE}: {A} {S}tickier {B}enchmark for {G}eneral-{P}urpose {L}anguage {U}nderstanding {S}ystems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{kitaev2018constituency,
  title={Constituency {P}arsing with a {S}elf-{A}ttentive {E}ncoder},
  author={Kitaev, Nikita and Klein, Dan},
  journal={arXiv preprint arXiv:1805.01052},
  year={2018}
}

@inproceedings{ye-etal-2021-crossfit,
    title = "{C}ross{F}it: {A} {F}ew-{S}hot {L}earning {C}hallenge for {C}ross-{T}ask {G}eneralization in {NLP}",
    author = "Ye, Qinyuan  and
      Lin, Bill Yuchen  and
      Ren, Xiang",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    pages = "7163--7189",
}

@inproceedings{khashabi-etal-2020-unifiedqa,
    title = "{UNIFIEDQA}: {C}rossing {F}ormat {B}oundaries with a {S}ingle {QA} {S}ystem",
    author = "Khashabi, Daniel  and
      Min, Sewon  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Tafjord, Oyvind  and
      Clark, Peter  and
      Hajishirzi, Hannaneh",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    year = "2020",
    pages = "1896--1907",
}

@inproceedings{hendrycksmeasuring,
  title={Measuring {M}assive {M}ultitask {L}anguage {U}nderstanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022}
}

@article{li2022explanations,
  title={Explanations from large language models make small reasoners better},
  author={Li, Shiyang and Chen, Jianshu and Shen, Yelong and Chen, Zhiyu and Zhang, Xinlu and Li, Zekun and Wang, Hong and Qian, Jing and Peng, Baolin and Mao, Yi and others},
  journal={arXiv preprint arXiv:2210.06726},
  year={2022}
}

@inproceedings{xu-etal-2022-openstance,
    title = "{O}pen{S}tance: {R}eal-{W}orld {Z}ero-{S}hot {S}tance {D}etection",
    author = "Xu, Hanzi  and
      Vucetic, Slobodan  and
      Yin, Wenpeng",
    booktitle = "Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL)",
    year = "2022",
    pages = "314--324",
}

@article{li2022ultra,
  title={Ultra-{F}ine {E}ntity {T}yping with {I}ndirect {S}upervision from {N}atural {L}anguage {I}nference},
  author={Li, Bangzheng and Yin, Wenpeng and Chen, Muhao},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={607--622},
  year={2022}
}

@inproceedings{xia2021incremental,
  title={Incremental {F}ew-{S}hot {T}ext {C}lassification with {M}ulti-{R}ound {N}ew {C}lasses: {F}ormulation, {D}ataset and {S}ystem},
  author={Xia, Congying and Yin, Wenpeng and Feng, Yihao and Philip, S Yu},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1351--1360},
  year={2021}
}

@inproceedings{sainz-etal-2021-label,
    title = "Label {V}erbalization and {E}ntailment for {E}ffective {Z}ero and {F}ew-{S}hot {R}elation {E}xtraction",
    author = "Sainz, Oscar  and
      Lopez de Lacalle, Oier  and
      Labaka, Gorka  and
      Barrena, Ander  and
      Agirre, Eneko",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    pages = "1199--1212",
}

@inproceedings{sainz-etal-2022-textual,
    title = "Textual {E}ntailment for {E}vent {A}rgument {E}xtraction: {Z}ero- and {F}ew-{S}hot with {M}ulti-{S}ource {L}earning",
    author = "Sainz, Oscar  and
      Gonzalez-Dios, Itziar  and
      Lopez de Lacalle, Oier  and
      Min, Bonan  and
      Agirre, Eneko",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    year = "2022",
    pages = "2439--2455",
}

@inproceedings{kenton2019bert,
  title={{BERT}: {P}re-{T}raining of {D}eep {B}idirectional {T}ransformers for {L}anguage {U}nderstanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}

@article{wang2022instructionner,
  title={Instruction{NER}: {A} {M}ulti-{T}ask {I}nstruction-{B}ased {G}enerative {F}ramework for {F}ew-{S}hot {NER}},
  author={Wang, Liwen and Li, Rumei and Yan, Yang and Yan, Yuanmeng and Wang, Sirui and Wu, Wei and Xu, Weiran},
  journal={arXiv preprint arXiv:2203.03903},
  year={2022}
}

@inproceedings{li-etal-2022-prompt,
    title = "Prompt-{D}riven {N}eural {M}achine {T}ranslation",
    author = "Li, Yafu  and
      Yin, Yongjing  and
      Li, Jing  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    year = "2022",
}

@inproceedings{wu-shi-2022-adversarial,
    title = "Adversarial {S}oft {P}rompt {T}uning for {C}ross-{D}omain {S}entiment {A}nalysis",
    author = "Wu, Hui  and
      Shi, Xiaodong",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2022",
    pages = "2438--2447",
}

@inproceedings{li-liang-2021-prefix,
    title = "Prefix-{T}uning: {O}ptimizing {C}ontinuous {P}rompts for {G}eneration",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    year = "2021",
    pages = "4582--4597",
}

@inproceedings{qin-eisner-2021-learning,
    title = "Learning {H}ow to {A}sk: {Q}uerying {LM}s with {M}ixtures of {S}oft {P}rompts",
    author = "Qin, Guanghui  and
      Eisner, Jason",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2021",
    pages = "5203--5212",
}

@inproceedings{gao-etal-2021-making,
    title = "Making {P}re-{T}rained {L}anguage {M}odels {B}etter {F}ew-{S}hot {L}earners",
    author = "Gao, Tianyu  and
      Fisch, Adam  and
      Chen, Danqi",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    year = "2021",
    pages = "3816--3830",
}

@inproceedings{aribandiext5,
  title={Ex{T}5: {T}owards {E}xtreme {M}ulti-{T}ask {S}caling for {T}ransfer {L}earning},
  author={Aribandi, Vamsi and Tay, Yi and Schuster, Tal and Rao, Jinfeng and Zheng, Huaixiu Steven and Mehta, Sanket Vaibhav and Zhuang, Honglei and Tran, Vinh Q and Bahri, Dara and Ni, Jianmo and others},
  booktitle={International Conference on Learning Representations},
  year = "2022",
}

@article{mccann2018natural,
  title={The {N}atural {L}anguage {D}ecathlon: {M}ultitask {L}earning as {Q}uestion {A}nswering},
  author={McCann, Bryan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1806.08730},
  year={2018}
}

@article{wolf2019huggingface,
  title={Hugging{F}ace's {T}ransformers: {S}tate-of-the-{A}rt {N}atural {L}anguage {P}rocessing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{bai2022constitutional,
  title={Constitutional {AI}: {H}armlessness from {AI} {F}eedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{honovich2022unnatural,
  title={Unnatural {I}nstructions: {T}uning {L}anguage {M}odels with ({A}lmost) {N}o {H}uman {L}abor},
  author={Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  journal={arXiv preprint arXiv:2212.09689},
  year={2022}
}

@inproceedings{wang2007introduction,
  title={Introduction: {A}spects of {A}rtificial {G}eneral {I}ntelligence},
  author={Wang, Pei and Goertzel, Ben},
  booktitle={Proceedings of the 2007 conference on Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms: Proceedings of the AGI Workshop 2006},
  pages={1--16},
  year={2007}
}

@article{arivazhagan2019massively,
  title={Massively {M}ultilingual {N}eural {M}achine {T}ranslation in the {W}ild: {F}indings and {C}hallenges},
  author={Arivazhagan, Naveen and Bapna, Ankur and Firat, Orhan and Lepikhin, Dmitry and Johnson, Melvin and Krikun, Maxim and Chen, Mia Xu and Cao, Yuan and Foster, George and Cherry, Colin and others},
  journal={arXiv preprint arXiv:1907.05019},
  year={2019}
}

@article{pratap2020massively,
  title={Massively {M}ultilingual {ASR}: 50 {L}anguages, 1 {M}odel, 1 {B}illion {P}arameters},
  author={Pratap, Vineel and Sriram, Anuroop and Tomasello, Paden and Hannun, Awni and Liptchinsky, Vitaliy and Synnaeve, Gabriel and Collobert, Ronan},
  journal={arXiv preprint arXiv:2007.03001},
  year={2020}
}

@article{chowdhery2022palm,
  title={Pa{LM}: {S}caling {L}anguage {M}odeling with {P}athways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{wang2023robustness,
  title={On the {R}obustness of {C}hat{GPT}: {A}n {A}dversarial and {O}ut-of-{D}istribution {P}erspective},
  author={Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Huang, Haojun and Ye, Wei and Geng, Xiubo and others},
  journal={arXiv preprint arXiv:2302.12095},
  year={2023}
}

@article{qin2023chatgpt,
  title={Is {C}hat{GPT} a {G}eneral-{P}urpose {N}atural {L}anguage {P}rocessing {T}ask {S}olver?},
  author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  journal={arXiv preprint arXiv:2302.06476},
  year={2023}
}

@article{Kocon2023ChatGPTJO,
  title={Chat{GPT}: {J}ack of {A}ll {T}rades, {M}aster of {N}one},
  author={Jan Koco'n and Igor Cichecki and Oliwier Kaszyca and Mateusz Kochanek and Dominika Szydlo and Joanna Baran and Julita Bielaniewicz and Marcin Gruza and Arkadiusz Janz and Kamil Kanclerz and Anna Koco'n and Bartlomiej Koptyra and Wiktoria Mieleszczenko-Kowszewicz and P. Milkowski and Marcin Oleksy and Maciej Piasecki and Lukasz Radli'nski and Konrad Wojtasik and Stanislaw Wo'zniak and Przemyslaw Kazienko},
  journal={arXiv preprint arXiv:2302.10724},
  year={2023}
}

@article{guo2023close,
  title={How {C}lose is {C}hat{GPT} to {H}uman {E}xperts? {C}omparison {C}orpus, {E}valuation, and {D}etection},
  author={Guo, Biyang and Zhang, Xin and Wang, Ziyuan and Jiang, Minqi and Nie, Jinran and Ding, Yuxuan and Yue, Jianwei and Wu, Yupeng},
  journal={arXiv preprint arXiv:2301.07597},
  year={2023}
}

@article{lan2019albert,
  title={{ALBERT}: {A} {L}ite {B}ert for {S}elf-{S}upervised {L}earning of {L}anguage {R}epresentations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@inproceedings{petroni-etal-2019-language,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
    pages = "2463--2473",
}

@inproceedings{cui2021template,
  title={Template-Based Named Entity Recognition Using BART},
  author={Cui, Leyang and Wu, Yu and Liu, Jian and Yang, Sen and Zhang, Yue},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={1835--1845},
  year={2021}
}

@article{jiang2020can,
  title={How Can We Know What Language Models Know},
  author={Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={423--438},
  year={2020}
}

@inproceedings{haviv-etal-2021-bertese,
    title = "{BERT}ese: Learning to Speak to {BERT}",
    author = "Haviv, Adi  and
      Berant, Jonathan  and
      Globerson, Amir",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    year = "2021",
    pages = "3618--3623",
}

@article{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={200--212},
  year={2021}
}

@article{han2021ptr,
  title={PTR: Prompt Tuning with Rules for Text Classification},
  author={Han, Xu and Zhao, Weilin and Ding, Ning and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2105.11259},
  year={2021}
}

@article{ha2016hypernetworks,
  title={Hypernetworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V},
  journal={arXiv preprint arXiv:1609.09106},
  year={2016}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@inproceedings{jin2020language,
  title={Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions},
  author={Jin, Tian and Liu, Zhun and Yan, Shengjia and Eichenberger, Alexandre and Morency, Louis-Philippe},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={6994--7007},
  year={2020}
}

@inproceedings{li2022maqa,
  title={MAQA: A Multimodal QA Benchmark for Negation},
  author={Li, Judith Yue and Jansen, Aren and Huang, Qingqing and Ganti, Ravi and Lee, Joonseok and Kuzmin, Dima},
  booktitle={NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research},
  year={2022}
}

@inproceedings{sun2022implicit,
  title={Implicit n-grams Induced by Recurrence},
  author={Sun, Xiaobing and Lu, Wei},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1624--1639},
  year={2022}
}

@inproceedings{hosseini2021understanding,
  title={Understanding by Understanding Not: Modeling Negation in Language Models},
  author={Hosseini, Arian and Reddy, Siva and Bahdanau, Dzmitry and Hjelm, R Devon and Sordoni, Alessandro and Courville, Aaron},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1301--1312},
  year={2021}
}

@inproceedings{kassner2020negated,
  title={Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly},
  author={Kassner, Nora and Sch{\"u}tze, Hinrich},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7811--7818},
  year={2020}
}

@article{dudschig2018does,
  title={How does “not left” become “right”? Electrophysiological evidence for a dynamic conflict-bound negation processing account.},
  author={Dudschig, Carolin and Kaup, Barbara},
  journal={Journal of Experimental Psychology: Human Perception and Performance},
  volume={44},
  number={5},
  pages={716},
  year={2018},
  publisher={American Psychological Association}
}

@inproceedings{naik2018stress,
  title={Stress Test Evaluation for Natural Language Inference},
  author={Naik, Aakanksha and Ravichander, Abhilasha and Sadeh, Norman and Rose, Carolyn and Neubig, Graham},
  booktitle={Proceedings of the 27th International Conference on Computational Linguistics},
  pages={2340--2353},
  year={2018}
}

@article{bowman2022measuring,
  title={Measuring progress on scalable oversight for large language models},
  author={Bowman, Samuel R and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Lukosuite, Kamile and Askell, Amanda and Jones, Andy and Chen, Anna and others},
  journal={arXiv preprint arXiv:2211.03540},
  year={2022}
}

@article{hendrycks2020aligning,
  title={Aligning ai with shared human values},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2008.02275},
  year={2020}
}

@inproceedings{cotra2021case,
  title={The case for aligning narrowly superhuman models},
  author={Cotra, Ajeya},
  booktitle={AI Alignment Forum},
  year={2021}
}

@book{tomlinson2006integrating,
  title={Integrating differentiated instruction \& understanding by design: Connecting content and kids},
  author={Tomlinson, Carol A and McTighe, Jay},
  year={2006},
  publisher={ASCD},
  Journal={Association for Supervision and Curriculum Development},
}

@article{musthafa1996learning,
  title={Learning from Texts and Reading Instruction.},
  author={Musthafa, Bachrudin},
  year={1996},
  publisher={ERIC},
  Journal={Association for Supervision and Curriculum Development},
}

@article{greer2008kids,
  Title = {How kids learn to say the darnedest things: the effect of multiple exemplar instruction on the emergence of novel verb usage},
  Author = {Greer, R Douglas and Yuan, Lynn},
  Year = {2008},
  Journal = {The Analysis of verbal behavior},
  Pages = {103—121},
}

@article{fennema1996longitudinal,
  title={A longitudinal study of learning to use children's thinking in mathematics instruction},
  author={Fennema, Elizabeth and Carpenter, Thomas P and Franke, Megan L and Levi, Linda and Jacobs, Victoria R and Empson, Susan B},
  journal={Journal for research in mathematics education},
  year={1996},
  publisher={National Council of Teachers of Mathematics}
}

@article{carpenter1996cognitively,
  title={Cognitively guided instruction: A knowledge base for reform in primary mathematics instruction},
  author={Carpenter, Thomas P and Fennema, Elizabeth and Franke, Megan L},
  journal={The elementary school journal},
  year={1996},
  publisher={University of Chicago Press}
}


@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{ye2023context,
  title={In-Context Instruction Learning},
  author={Ye, Seonghyeon and Hwang, Hyeonbin and Yang, Sohee and Yun, Hyeongu and Kim, Yireun and Seo, Minjoon},
  journal={arXiv preprint arXiv:2302.14691},
  year={2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{peng2023gpt4llm,
    title={Instruction Tuning with GPT-4},
    author={Baolin Peng and Chunyuan Li and Pengcheng He and Michel Galley and Jianfeng Gao},
    journal={arXiv preprint arXiv:2304.03277},
    year={2023}
}

@article{he2023annollm,
  title={AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators},
  author={He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu and others},
  journal={arXiv preprint arXiv:2303.16854},
  year={2023}
}

@article{pan2023gpt4reward,
    title={Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark},
    author={Alexander Pan and Chan Jun Shern and Andy Zou and Nathaniel Li and Steven Basart and Thomas Woodside and Jonathan Ng and Hanlin Zhang and Scott Emmons and Dan Hendrycks},
    journal={arXiv preprint arXiv:2304.03279},
    year={2023}
}

@article{OpenAI2023GPT4TR,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774}
}

@article{yu2023nature,
  title={Nature Language Reasoning, A Survey},
  author={Yu, Fei and Zhang, Hongbo and Wang, Benyou},
  journal={arXiv preprint arXiv:2303.14725},
  year={2023}
}

@article{koksal2023longform,
  title={LongForm: Optimizing Instruction Tuning for Long Text Generation with Corpus Extraction},
  author={K{\"o}ksal, Abdullatif and Schick, Timo and Korhonen, Anna and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2304.08460},
  year={2023}
}

@article{lialin2023scaling,
  title={Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{puri2022many,
  title={How Many Data Samples is an Additional Instruction Worth?},
  author={Puri, Ravsehaj Singh and Mishra, Swaroop and Parmar, Mihir and Baral, Chitta},
  journal={arXiv preprint arXiv:2203.09161},
  year={2022}
}

@article{zhang2023aligning,
  title={Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors},
  author={Zhang, Kai and Guti{\'e}rrez, Bernal Jim{\'e}nez and Su, Yu},
  journal={arXiv preprint arXiv:2305.11159},
  year={2023}
}

@article{wei2023symbol,
  title={Symbol tuning improves in-context learning in language models},
  author={Wei, Jerry and Hou, Le and Lampinen, Andrew and Chen, Xiangning and Huang, Da and Tay, Yi and Chen, Xinyun and Lu, Yifeng and Zhou, Denny and Ma, Tengyu and others},
  journal={arXiv preprint arXiv:2305.08298},
  year={2023}
}

@article{Brock2017SMASHOM,
  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},
  author={Andrew Brock and Theodore Lim and James M. Ritchie and Nick Weston},
  journal={ArXiv},
  year={2017},
  volume={abs/1708.05344}
}

@article{ortiz2023non,
  title={Non-Proportional Parametrizations for Stable Hypernetwork Learning},
  author={Ortiz, Jose Javier Gonzalez and Guttag, John and Dalca, Adrian},
  journal={arXiv preprint arXiv:2304.07645},
  year={2023}
}

@misc{openai2022chatgpt,
    title={ChatGPT},
    author={OpenAI},
    year={2022},
    url={https://openai.com/blog/chatgpt}
}

@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}

@article{wang2023far,
  title={How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources},
  author={Wang, Yizhong and Ivison, Hamish and Dasigi, Pradeep and Hessel, Jack and Khot, Tushar and Chandu, Khyathi Raghavi and Wadden, David and MacMillan, Kelsey and Smith, Noah A and Beltagy, Iz and others},
  journal={arXiv preprint arXiv:2306.04751},
  year={2023}
}

@misc{deng2023mind2web,
  title={Mind2Web: Towards a Generalist Agent for the Web},
  author={Xiang Deng and Yu Gu and Boyuan Zheng and Shijie Chen and Samuel Stevens and Boshi Wang and Huan Sun and Yu Su},
  year={2023},
  eprint={2306.06070},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{wu2023lamini,
  title={Lamini-lm: A diverse herd of distilled models from large-scale instructions},
  author={Wu, Minghao and Waheed, Abdul and Zhang, Chiyu and Abdul-Mageed, Muhammad and Aji, Alham Fikri},
  journal={arXiv preprint arXiv:2304.14402},
  year={2023}
}

@article{xu2022universal,
  title={A Universal Discriminator for Zero-Shot Generalization},
  author={Xu, Haike and Lin, Zongyu and Zhou, Jing and Zheng, Yanan and Yang, Zhilin},
  journal={arXiv preprint arXiv:2211.08099},
  year={2022}
}

@misc{Zhang2023MagicBrush,
      title={MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing}, 
      author={Kai Zhang and Lingbo Mo and Wenhu Chen and Huan Sun and Yu Su},
      year={2023},
      eprint={2306.10012},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{xu2022multiinstruct,
  title={MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning},
  author={Xu, Zhiyang and Shen, Ying and Huang, Lifu},
  journal={arXiv preprint arXiv:2212.10773},
  year={2022}
}

@article{li2023mimic,
  title={MIMIC-IT: Multi-Modal In-Context Instruction Tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2306.05425},
  year={2023}
}

@article{li2023m3,
  title={M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning},
  author={Li, Lei and Yin, Yuwei and Li, Shicheng and Chen, Liang and Wang, Peiyi and Ren, Shuhuai and Li, Mukai and Yang, Yazheng and Xu, Jingjing and Sun, Xu and others},
  journal={arXiv preprint arXiv:2306.04387},
  year={2023}
}

@article{gupta2023instruction,
  title={Instruction Tuned Models are Quick Learners},
  author={Gupta, Himanshu and Sawant, Saurabh Arjun and Mishra, Swaroop and Nakamura, Mutsumi and Mitra, Arindam and Mashetty, Santosh and Baral, Chitta},
  journal={arXiv preprint arXiv:2306.05539},
  year={2023}
}

@article{xu2023baize,
  title={Baize: An open-source chat model with parameter-efficient tuning on self-chat data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={arXiv preprint arXiv:2304.01196},
  year={2023}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{kopf2023openassistant,
  title={OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={arXiv preprint arXiv:2304.07327},
  year={2023}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@article{kim2023cot,
  title={The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning},
  author={Kim, Seungone and Joo, Se June and Kim, Doyoung and Jang, Joel and Ye, Seonghyeon and Shin, Jamin and Seo, Minjoon},
  journal={arXiv preprint arXiv:2305.14045},
  year={2023}
}

@article{ding2023enhancing,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}

@misc{dolly2023,
    title = {Hello dolly: Democratizing the magic of chatgpt with open models.},
    url = {https://github.com/databrickslabs/dolly},
    author = {Conover, Mike and Hayes, Matt and Mathur, Matt and Meng, Xiangrui and Xie, Jianwei and Wan, Jun and Ghodsi, Ali and Wendell, Patrick and Zaharia, Patrick},
    month = {April},
    year = {2023}
}

@article{yin2023dynosaur,
  title={Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation},
  author={Yin, Da and Liu, Xiao and Yin, Fan and Zhong, Ming and Bansal, Hritik and Han, Jiawei and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2305.14327},
  year={2023}
}

@misc{koala_blogpost_2023,
  author = {Xinyang Geng and Arnav Gudibande and Hao Liu and Eric Wallace and Pieter Abbeel and Sergey Levine and Dawn Song},
  title = {Koala: A Dialogue Model for Academic Research},
  howpublished = {Blog post},
  month = {April},
  year = {2023},
  url = {https://bair.berkeley.edu/blog/2023/04/03/koala/},
  urldate = {2023-04-03}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, Jurgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{akmajian,
author = {Akmajian, Adrian and Ray Jackendoff},
year   = {1970},
title  = {Coreferentiality and Stress},
journal = {Linguistic Inquiry}, 
volume = {1},
number = {1},
pages = {124--126},
}

@article{lou2023forget,
  title={Forget demonstrations, focus on learning from textual instructions},
  author={Lou, Renze and Yin, Wenpeng},
  journal={arXiv preprint arXiv:2308.03795},
  year={2023}
}

@inproceedings{liu2022makes,
  title={What Makes Good In-Context Examples for GPT-3?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, William B and Carin, Lawrence and Chen, Weizhu},
  booktitle={Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures},
  pages={100--114},
  year={2022}
}

@inproceedings{sorensen2022information,
  title={An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels},
  author={Sorensen, Taylor and Robinson, Joshua and Rytting, Christopher and Shaw, Alexander and Rogers, Kyle and Delorey, Alexia and Khalil, Mahmoud and Fulda, Nancy and Wingate, David},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={819--862},
  year={2022}
}

@article{wu2022self,
  title={Self-adaptive in-context learning},
  author={Wu, Zhiyong and Wang, Yaoxiang and Ye, Jiacheng and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2212.10375},
  year={2022}
}

@article{li2023finding,
  title={Finding supporting examples for in-context learning},
  author={Li, Xiaonan and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2302.13539},
  year={2023}
}

@article{nguyen2023context,
  title={In-context Example Selection with Influences},
  author={Nguyen, Tai and Wong, Eric},
  journal={arXiv preprint arXiv:2302.11042},
  year={2023}
}

@article{wang2023large,
  title={Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning},
  author={Wang, Xinyi and Zhu, Wanrong and Wang, William Yang},
  journal={arXiv preprint arXiv:2301.11916},
  year={2023}
}

@inproceedings{zhang2022active,
  title={Active Example Selection for In-Context Learning},
  author={Zhang, Yiming and Feng, Shi and Tan, Chenhao},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={9134--9148},
  year={2022}
}

@article{bellman1957markovian,
  title={A Markovian decision process},
  author={Bellman, Richard},
  journal={Journal of mathematics and mechanics},
  pages={679--684},
  year={1957},
  publisher={JSTOR}
}

@article{jang2019q,
  title={Q-learning algorithms: A comprehensive classification and applications},
  author={Jang, Beakcheol and Kim, Myeonghwi and Harerimana, Gaspard and Kim, Jong Wook},
  journal={IEEE access},
  volume={7},
  pages={133653--133667},
  year={2019},
  publisher={IEEE}
}



@misc{Lou2023MUFFIN,
      title={MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following}, 
      author={Renze Lou and Kai Zhang and Jian Xie and Yuxuan Sun and Janice Ahn and Hanzi Xu and Yu su and Wenpeng Yin},
      year={2023},
      eprint={2312.02436},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{zhang2022automatic,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{press2022measuring,
  title={Measuring and Narrowing the Compositionality Gap in Language Models},
  author={Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A and Lewis, Mike},
  year={2022}
}

@inproceedings{zhou2022least,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc V and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{xu2023small,
  title={Small models are valuable plug-ins for large language models},
  author={Xu, Canwen and Xu, Yichong and Wang, Shuohang and Liu, Yang and Zhu, Chenguang and McAuley, Julian},
  journal={arXiv preprint arXiv:2305.08848},
  year={2023}
}

@inproceedings{zeng2022glm,
  title={GLM-130B: An Open Bilingual Pre-trained Model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{muennighoff2022crosslingual,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}

@article{liu2023zero,
  title={From zero to hero: Examining the power of symbolic tasks in instruction tuning},
  author={Liu, Qian and Zhou, Fan and Jiang, Zhengbao and Dou, Longxu and Lin, Min},
  journal={arXiv preprint arXiv:2304.07995},
  year={2023}
}

@article{raheja2023coedit,
  title={CoEdIT: Text Editing by Task-Specific Instruction Tuning},
  author={Raheja, Vipul and Kumar, Dhruv and Koo, Ryan and Kang, Dongyeop},
  journal={arXiv preprint arXiv:2305.09857},
  year={2023}
}

@article{anand2023gpt4all,
  title={Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo},
  author={Anand, Yuvanesh and Nussbaum, Zach and Duderstadt, Brandon and Schmidt, Benjamin and Mulyar, Andriy},
  journal={GitHub},
  year={2023}
}


@inproceedings{xu-etal-2023-instructscore,
    title = "{INSTRUCTSCORE}: Towards Explainable Text Generation Evaluation with Automatic Feedback",
    author = "Xu, Wenda  and
      Wang, Danqing  and
      Pan, Liangming  and
      Song, Zhenqiao  and
      Freitag, Markus  and
      Wang, William  and
      Li, Lei",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    publisher = "Association for Computational Linguistics",
    pages = "5967--5994",
}

@inproceedings{fernandes2023devil,
  title={The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation},
  author={Fernandes, Patrick and Deutsch, Daniel and Finkelstein, Mara and Riley, Parker and Martins, Andr{\'e} FT and Neubig, Graham and Garg, Ankush and Clark, Jonathan H and Freitag, Markus and Firat, Orhan},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={1066--1083},
  year={2023}
}

@article{zhou2023instruction,
  title={Instruction-Following Evaluation for Large Language Models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}

@article{liu2023benchmarking,
  title={Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization},
  author={Liu, Yixin and Fabbri, Alexander R and Chen, Jiawen and Zhao, Yilun and Han, Simeng and Joty, Shafiq and Liu, Pengfei and Radev, Dragomir and Wu, Chien-Sheng and Cohan, Arman},
  journal={arXiv preprint arXiv:2311.09184},
  year={2023}
}

@inproceedings{hendrycks2020measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@article{suzgun2022challenging,
  title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and and Wei, Jason},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{chia2023instructeval,
  title={INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models},
  author={Chia, Yew Ken and Hong, Pengfei and Bing, Lidong and Poria, Soujanya},
  journal={arXiv preprint arXiv:2306.04757},
  year={2023}
}

@article{dubois2023alpacafarm,
  title={Alpacafarm: A simulation framework for methods that learn from human feedback},
  author={Dubois, Yann and Li, Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2305.14387},
  year={2023}
}

@article{saha2023branch,
  title={Branch-solve-merge improves large language model evaluation and generation},
  author={Saha, Swarnadeep and Levy, Omer and Celikyilmaz, Asli and Bansal, Mohit and Weston, Jason and Li, Xian},
  journal={arXiv preprint arXiv:2310.15123},
  year={2023}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{kang2023exploiting,
  title={Exploiting programmatic behavior of llms: Dual-use through standard security attacks},
  author={Kang, Daniel and Li, Xuechen and Stoica, Ion and Guestrin, Carlos and Zaharia, Matei and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2302.05733},
  year={2023}
}

@article{li2023you,
  title={Do you really follow me? adversarial instructions for evaluating the robustness of large language models},
  author={Li, Zekun and Peng, Baolin and He, Pengcheng and Yan, Xifeng},
  journal={arXiv preprint arXiv:2308.10819},
  year={2023}
}

@article{wan2023poisoning,
  title={Poisoning Language Models During Instruction Tuning},
  author={Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein, Dan},
  journal={arXiv preprint arXiv:2305.00944},
  year={2023}
}

@article{xie2023adaptive,
  title={Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Conflicts},
  author={Xie, Jian and Zhang, Kai and Chen, Jiangjie and Lou, Renze and Su, Yu},
  journal={arXiv preprint arXiv:2305.13300},
  year={2023}
}


@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{ivison2023camels,
  title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2},
  author={Ivison, Hamish and Wang, Yizhong and Pyatkin, Valentina and Lambert, Nathan and Peters, Matthew and Dasigi, Pradeep and Jang, Joel and Wadden, David and Smith, Noah A and Beltagy, Iz and others},
  journal={arXiv preprint arXiv:2311.10702},
  year={2023}
}

@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}

@article{besta2023graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  journal={arXiv preprint arXiv:2308.09687},
  year={2023}
}

@article{wang2022selfcons,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{yu2023wavecoder,
  title={WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation},
  author={Yu, Zhaojian and Zhang, Xin and Shang, Ning and Huang, Yangyu and Xu, Can and Zhao, Yishujie and Hu, Wenxiang and Yin, Qiufeng},
  journal={arXiv preprint arXiv:2312.14187},
  year={2023}
}

@article{liu2023makes,
  title={What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning},
  author={Liu, Wei and Zeng, Weihao and He, Keqing and Jiang, Yong and He, Junxian},
  journal={arXiv preprint arXiv:2312.15685},
  year={2023}
}

@article{liu2023automatic,
  title={Automatic Instruction Optimization for Open-source LLM Instruction Tuning},
  author={Liu, Yilun and Tao, Shimin and Zhao, Xiaofeng and Zhu, Ming and Ma, Wenbing and Zhu, Junhao and Su, Chang and Hou, Yutai and Zhang, Miao and Zhang, Min and others},
  journal={arXiv preprint arXiv:2311.13246},
  year={2023}
}

@article{song2023dynamics,
  title={Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace},
  author={Song, Chiyu and Zhou, Zhanchao and Yan, Jianhao and Fei, Yuejiao and Lan, Zhenzhong and Zhang, Yue},
  journal={arXiv preprint arXiv:2310.19651},
  year={2023}
}