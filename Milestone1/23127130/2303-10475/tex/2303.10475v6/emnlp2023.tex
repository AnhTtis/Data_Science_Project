% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{EMNLP2023}
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% File tacl2021v1.tex
% Dec. 15, 2021

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}

\usepackage{ulem}
\usepackage{cleveref}
\crefname{section}{§}{§§}
\usepackage{CJKutf8}
\usepackage{xcolor}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{hhline}
% \usepackage{subfigure} 
\usepackage{graphicx}
\usepackage[caption=false]{subfig}
\usepackage{tabularray}
\usepackage{enumitem}
\usepackage{hyperref}
\UseTblrLibrary{counter}
% \UseTblrLibrary{functional}
\UseTblrLibrary{booktabs}
% \usepackage[table]{xcolor}


%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\usepackage{microtype}
\usepackage{soul}
\usepackage{varwidth} 
\sethlcolor{gray!30}
\newcommand{\grayhl}[1]{%
    \begingroup
    \sethlcolor{gray!30}%
    \hl{#1}%
    \endgroup
}
\newcommand{\coloredbox}[2]{%
    \fcolorbox{#1}{#1}{%
        \begin{varwidth}{\linewidth}
            #2
        \end{varwidth}%
    }
}
% \usepackage[scaled]{beramono}
\newenvironment{hilight}{\par\color{blue}}{\par}
\newcommand{\compacttt}[1]{%
    {\ttfamily\SetTracking{encoding=*}{-30}#1}%
}
% \usetikzlibrary{fadings,decorations.text}
\newcommand{\dateOfLastUpdate}{Dec. 15, 2021}
\newcommand{\styleFileVersion}{tacl2021v1}
\newcommand{\ex}[1]{{\sf #1}}

\newcommand{\citeiter}{\citep[][\textit{inter alia}]}
\newcommand{\blank}{\underline{\hspace{0.9em}}}
\newcommand{\linenotation}{\begin{verbatim} \n \end{verbatim}}
\newcommand{\footnotemarkNL}{\begin{NoHyper}\footnotemark\end{NoHyper}}
\definecolor{deeppink}{RGB}{255, 105, 180}
\definecolor{mycolor}{RGB}{240,240,240}
\definecolor{MyGrey}{HTML}{838383}
\definecolor{MyBlue}{HTML}{1F4E79}
\definecolor{MyRed}{HTML}{A80000}
\definecolor{MyYellow}{HTML}{FFCC00}
% \definecolor{MyPink}{HTML}{7F007F}
% \definecolor{MyGreen}{HTML}{02856B}
% \definecolor{MyOrange}{HTML}{FF6600}
\definecolor{MyPink}{HTML}{83639f}
\definecolor{MyGreen}{HTML}{449945}
\definecolor{MyOrange}{HTML}{ea7827}
\definecolor{LinkPink}{HTML}{df1a7d}

\definecolor{ForestGreen}{HTML}{009B55}
\definecolor{OrangeRed}{HTML}{c22f2f}
\definecolor{Dandelion}{HTML}{e9963e}

\definecolor{InstanceBlue}{HTML}{6bb2e7}
\definecolor{TaskRed}{HTML}{fe4544}

\newcommand{\MyYes}{\textcolor{ForestGreen}{\textrm{yes}}}
\newcommand{\MyNo}{\textcolor{OrangeRed}{\textrm{no}}}
\newcommand{\MyMaybe}{\textcolor{Dandelion}{\textrm{maybe}}}
\newenvironment{compactitemize}{
  \begin{itemize}[nosep, topsep=0pt, partopsep=0pt, parsep=0pt, itemsep=0pt] % Customize itemize spacing
}{
  \end{itemize}
}



\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi


\usepackage{dingbat}
\usepackage{bbding}
\newcommand{\textualentailment}{\textsc{TE}} 
\newcommand{\entailinstruction}{\textsl{NLI-oriented Instructions}}
\newcommand{\plminstruction}{\textsl{PLM-oriented Instructions}} 
\newcommand{\humaninstruction}{\textsl{Human-oriented Instructions}} 
\newcommand{\TaskDef}{{\color{MyPink}{\em {\normalsize \textrm{Task Definition}}}}}
\newcommand{\Demonstrations}{{\color{MyOrange}{\em {\normalsize \textrm{Demonstrations}}}}}
\newcommand{\TestInput}{{\color{MyGreen}{\em {\normalsize \textrm{Test Instance}}}}}

\title{Are Prompts All the Story? No. \\
A Comprehensive and Broader View of Instruction Learning}
% \title{Is Prompt All You Need? \\ No. A Comprehensive and Broader View of Instruction Learning}
% \title{Formatting Instructions for TACL \TaclPapers \\
% (Base files: \styleFileVersion-template.tex \& \styleFileVersion.sty, dated \dateOfLastUpdate)}

% Author information does not appear in the pdf unless the "acceptedWithA" option is given

% The author block may be formatted in one of two ways:

% Option 1. Author’s address is underneath each name, centered.

% \author{
%   Template Author1\Thanks{The {\em actual} contributors to this instruction
%     document and corresponding template file are given in Section
%     \ref{sec:contributors}.} 
%   \\
%   Template Affiliation1/Address Line 1
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   \texttt{template.email1example.com}
%   \And
%   Template Author2 
%   \\
%   Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
% }

% % Option 2.  Author’s address is linked with superscript
% % characters to its name, author names are grouped, centered.

% \author{
%   Template Author1\Thanks{The {\em actual} contributors to this instruction
%     document and corresponding template file are given in Section
%     \ref{sec:contributors}.}$^\diamond$ 
%   \and
%   Template Author2$^\dagger$
%   \\
%   \ \\
%   $^\diamond$Template Affiliation1/Address Line 1
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   \texttt{template.email1example.com}
%   \\
%   \ \\
%   \\
%   $^\dagger$Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
% }

% self-modified 
\author{
  Renze Lou\textsuperscript{\rm $\spadesuit$} \quad
  Kai Zhang\textsuperscript{\rm $\diamondsuit$} \quad
  \and
  Wenpeng Yin\textsuperscript{\rm $\spadesuit$}
  \\
  \textsuperscript{\rm $\spadesuit$}The Pennsylvania State University
  % $^\dagger$The Pennsylvania State University;
  \ 
  \textsuperscript{\rm $\diamondsuit$}The Ohio State University
  \\
  {\small \texttt{\{renze.lou, wenpeng\}@psu.edu};}
  \ 
  {\small \texttt{zhang.13253@osu.edu}}
}

\date{}


\begin{document}
\maketitle
\begin{abstract}
  % Despite the impressive results of the fully supervised fine-tuning, the laborious task-specific human annotation and large-scale pre-trained language models (PLM) make the traditional fine-tuning procedure unaffordable. Therefore, a growing body of research resorted to a new paradigm, namely \textit{textual instruction learning}, to perform various NLP tasks. By learning to follow the task instructions, the language models (LMs) are further evolved into generalist task assistants that can perform multiple tasks in a zero/few-shot manner, indicating a big step toward the idea of Artificial General Intelligence (AGI). In this paper, we survey numerous existing pieces of literature about instruction learning. Specifically, we first introduce the systematic taxonomies of instructions, and then summarize different common-chosen modeling strategies. Next, we provide several critical factors in using instructions in real-world NLP tasks, as well as display some popular applications. Finally, we highlight a few key challenges in the current instruction-learning paradigm, along with the corresponding future trends. To our knowledge, this is the first comprehensive survey about textual instructions.

  Task semantics can be expressed by a set of input-to-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. 
  Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: \textit{learning from task instructions}. Despite its impressive progress, there are some common issues that the community struggles with. This survey paper tries to \textit{summarize} and \textit{provide insights} to the current research on instruction learning, particularly, by answering the following questions: (i) What is task instruction, and what instruction types exist? (ii) How to model instructions? (iii) What factors influence and explain the instructions' performance? (iv) What challenges remain in instruction learning? To our knowledge, this is the first comprehensive survey about textual instructions.\footnote{The curated paper list can be found at:~{
\hypersetup{urlcolor=LinkPink}\url{https://github.com/RenzeLou/awesome-instruction-learning}}}
  % \footnote{To benefit future research, we will release a curated reading list of instruction learning on GitHub, including comprehensive related papers and popular datasets.}
%   \footnote{\texttt{Preliminary release}. We also released a curated reading list that will be maintained continuously to benefit future research, including related papers and popular datasets of instruction learning:~{
% \hypersetup{urlcolor=LinkPink}\url{https://github.com/RenzeLou/awesome-instruction-learning}}.}
\end{abstract}


\begin{figure}[!ht]
 \setlength{\belowcaptionskip}{-12pt}
 \setlength{\abovecaptionskip}{0.6pt}
	\begin{center}
		\centering
		\includegraphics[width=1.02\linewidth]{ picture/two_paradigm_v6.5.pdf}
	\end{center}
	\caption{Two supervised learning paradigms: (a) \textit{example-driven} learning uses extensive labeled examples to represent the task semantics. The resulting system can only generalize to \textcolor{InstanceBlue}{unseen instances} of the same task; (b) \textit{instruction-driven} learning tames model to follow various task instructions. Besides unseen instances, the final system can also generalize to \textcolor{TaskRed}{unseen tasks}.}
    % by leveraging the task semantics of instructions (indirect supervision)
    % \vspace{-0.8em}
	\label{fig:two_paradigms}
\end{figure}





\section{Introduction}
% === background ===
% There were mainly two waves in the previous deep learning paradigms in NLP: (1). \textit{Fully supervised learning} that trains a task-specific model from the random initialization; (2). \textit{Pre-train and fine-tune} that pre-trains a general model on the large-scale corpus and then adapts it on specific downstream tasks by fine-tuning with labeled examples~\cite{radford2018improving}.

% With the help of advanced pre-training strategies, current large-scale language models (LLMs) achieve remarkable results on various natural language processing (NLP) tasks via fine-tuning on the labeled examples. This paradigm, so called \textit{fully supervised fine-tuning} (SFT), suffers from two issues: (1). Laborious human annotations in constructing task-specific corpus; (2). The resulting model is a \textbf{close-set specialist model}, which usually performs well on the seen training tasks but fails at generalizing to the unseen tasks. Therefore, how to build a \textbf{cross-task generalist model} that can handle various novel tasks became a long-term question for the research community~\cite{goertzel2014artificial}.

One goal of AI is to build a system that can universally understand and solve new tasks.
Labeled examples (Figure \ref{fig:two_paradigms} (a)), as the mainstream task representation, are costly to obtain at scale or even do not exist in some cases.
Then, is there any other task representation that can contribute to task comprehension?
% One goal of AI is to build a system that can universally understand and solve new tasks.
% In the past few years, as shown in Figure~\ref{fig:two_paradigms}, example-driven learning was the dominating paradigm. 
% However, such a paradigm requires labeled examples, which are costly to obtain at scale.
Textual instructions provide another dimension of supervision for expressing the task semantics, which often contains more abstract and comprehensive knowledge of the target task than individual labeled examples. 
As shown in Figure~\ref{fig:two_paradigms} (b), with the availability of task instructions, systems can be quickly built to handle new tasks.
% By contrast, instruction-driven learning provides another dimension of supervision with regard to tasks: more abstract and comprehensive knowledge of the target task than individual labeled examples. 
% With the availability of task instructions, systems can be quickly built to handle new tasks. 
Such efficiency is highly desirable in real-world applications, especially when task-specific annotations are scarce.
% Instruction Learning is inspired by typical human learning for new tasks;
More importantly, instruction learning leans toward human intelligence in terms of learning new tasks---a little child can well solve a new mathematical task by learning from its instruction and a few examples~\cite{fennema1996longitudinal, carpenter1996cognitively}.
As a result, this new learning paradigm has recently attracted the main attention of the machine learning and NLP communities~\cite{wang2022benchmarking, longpre2023flan}. 

% When talking about task instructions, most of us will first connect this concept with prompts---using a brief template to reformat an input into a language modeling problem so as to prime a pretrained language model (PLM) for a response~\cite{brown2020language}.
When talking about ``instruction'', most of us will first think of ``prompt''---using a brief template to convert a task input into a new format (e.g., cloze question) that caters to the language modeling objective of pretrained language models (PLMs)~\cite{brown2020language}.
% 
Despite the prevalence of prompts in text classification, machine translation, etc., we argue that prompts are merely a special case of instructions.
This paper takes a comprehensive and broader view of instruction-driven NLP research. Particularly, we try to answer the following questions: (i) what is task instruction, and what instruction types exist? (\cref{sec:categories}) (ii) given a task instruction, how to encode it to assist the model generalization on the target task? (\cref{sec:modeling}) (iii) what factors (e.g., model size, task numbers) impact the instruction-driven systems' performance? (\cref{sec:analysis}) (iv) what challenges exist in instruction learning, and what are future directions? (\cref{sec:challenges})


% === what is instruction and instruction learning (key difference with supervised learning). ===
% To this end, a novel paradigm is emerged to fill this gap, namely \textit{instruction learning}, which utilizes in-context natural language instructions to guide the LMs to perform various tasks. Unlike traditional supervised learning, where the model learns the task-specific skills by gradient-based parameter optimization, i.e., \textit{learn to complete tasks}, the essential objective of instruction learning is driving the model to \textit{learn to follow the instructions}, which provide a new way to address various NLP tasks in a completely zero/few-shot manner. As a result, instruction learning has attracted a lot of attention in recent years, which has led to diverse instructions with distinct formats. Therefore, in this paper, we aim to survey existing works about instruction learning systematically, including instruction taxonomies, modeling strategies, applications, and important aspects that impact zero-shot performance.
% critical challenges, and potential future trends.

% === a brief description of taxonomies ===
% === a brief description of modeling strategies ===
% === the advantages of instruction learning. === 



% === some challenges ===
% Although the instruction learning paradigm has been proven to be effective in handling NLP tasks, there are still several distinct problems in the current paradigm. First, the LMs could find it hard to grasp the semantic meaning of negated instructions truly; Second, the poor explainability of the high-performance instructions; Third, the reliance on the massive multi-task labeled examples due to the implicit learning objective; What's more, the problem of current automatic evaluation paradigm. To help further research address the above issues, we provide several corresponding hints by summarizing the current progress.




% === The purpose and position of this paper ===
To our knowledge, this is the first paper that surveys textual instruction learning. In contrast to some existing surveys that focused on a specific in-context instruction, such as prompts~\cite{liu2023pre}, input-by-output demonstrations~\cite{dong2022survey}, or reasoning~\cite{huang2022towards,qiao2022reasoning,yu2023nature}, this work provides a more comprehensive overview of textual instruction learning.
% \footnote{Please refer to the ``Related Work'' section in Appendix~\ref{appendix:related} for the detailed relation between this paper and other works.} 
Our contributions are three-fold:

\textbullet\enspace Going beyond prompts, we analyze prompt constraints via a user-centric lens, with a focus on discerning the disparity between current instruction learning research and real-world needs.

\textbullet\enspace We \textit{interpret different task instructions from the unified perspective of \textbf{indirect supervision}}, and summarize their advantages, limitations, and scope of applications;

\textbullet\enspace We regard current ever-growing PLMs and instruction datasets as an effort of dual-track scaling; additionally, we point out current notable research issues and promising directions in the future.
% We regard ever-growing PLMs and ever-extension instruction datasets as an effort of dual-track scaling of the current community; accordingly, we point out current research issues and promising directions in the future.
% We combine the ever-growing PLMs with ever-growing instruction datasets as a dual-track mechanism of efforts, and point out current research issues and promising directions in the future.


% we provide a broader perspective to connect distinct researches in this area in an organized way.
% Finally, we release the corresponding paper list for the beginner and future research~\footnote{\url{https://github.com/RenzeLou/awesome-instruction-learning}}, which will be continuously updated to keep track of the latest progress in this field. 
% We hope this paper can present a better story of instruction learning and attract more peers to work on this challenging AI problem.
%
% The corresponding reading list will be released in the future to help beginners.
% We also release the corresponding reading list of this survey.~\footnote{\url{https://github.com/RenzeLou/awesome-instruction-learning}}

\begin{figure*}[!t]
 \setlength{\belowcaptionskip}{-10pt}
 \setlength{\abovecaptionskip}{5pt}
	\begin{center}
		\centering
		% \includegraphics[width=0.9\linewidth]{ picture/Taxonomies_v4.pdf}
        \includegraphics[width=0.95\linewidth]{ picture/Taxonomies_v5.pdf}
	\end{center}
	\caption{An illustration of three distinct categories of textual instructions. 
 % (a) \textbf{\textsl{NLI-oriented}}: regarding the original input as a premise and converting each predefined label into a hypothesis (i.e., instruction). (b) \textbf{\textsl{PLM-oriented}}: using a template to construct the original task input into a cloze question. (c) \textbf{\textsl{Human-oriented}}: utilizing sufficient task information as instruction, such as definitions and optional few-shot demonstrations, etc.
 }
	\label{fig:overview}
\end{figure*}



\section{Related Work}
\label{sec:related}


There are basically two topics that highly relate to this paper, namely \textit{instruction tuning} (\ref{subsec:instruction_tuning}) and \textit{Surveys on In-context Instructions} (\ref{subsec:related_survey}).


\subsection{Instruction Tuning}
\label{subsec:instruction_tuning}

When talking about instruction learning, most of us will first think of \textit{instruction tuning}. As illustrated in Figure~\ref{fig:two_paradigms}, unlike traditional example-driven supervised learning, the essence of instruction tuning is to train the PLMs to understand various instructions and produce the corresponding responses.\footnote{Though some studies also investigated reinforcement learning (RL) based instruction tuning~\cite{ouyang2022training}, we mainly focus on supervised learning setup due to relatively rare resources in the RL-based scheme.} Since this capacity can be extended to any unseen downstream tasks, instruction tuning has become an efficient learning paradigm for solving few/zero-shot tasks~\citeiter{radford2019language,schick-schutze-2021-just,yin2022contintin,li2023mimic,gupta2023instruction}. However, the performance of instruction tuning highly relies on both model and task scale: a larger PLM (or pretraining with more tokens) tuned on more diverse tasks can achieve significantly better few/zero-shot performances on the downstream tasks~\citeiter{chung2022scaling,iyer2022opt,wang2023far}. As scaling model size is unrealistic for most of us, numerous recent studies worked on collecting high-quality instruction-tuning datasets, either employing human labors~\cite{khashabi-etal-2020-unifiedqa,ye-etal-2021-crossfit,sanh2021multitask,wang2022benchmarking,longpre2023flan,dolly2023,kopf2023openassistant} or distilling supervision from the powerful LLMs~\cite{wang2022self,honovich2022unnatural,alpaca,peng2023gpt4llm,xu2023baize,koala_blogpost_2023,vicuna2023,xu2023wizardlm,koksal2023longform,kim2023cot,ding2023enhancing,yin2023dynosaur}, e.g., utilizing ChatGPT or GPT-4 to develop creative task instructions~\cite{openai2022chatgpt,OpenAI2023GPT4TR}.
% , which basically updates PLMs' parameters in a standard supervised learning setup

Despite the popularity, instruction tuning is just a specific stage in the evolution of textual instruction learning. This work not only surveys the extensive existing literature on instruction tuning but also goes beyond: we trace the development of instruction learning back to the early days of feature-engineering-based machine learning, and formulate our story from an indirect supervision perspective. We hope this survey can systematically introduce this popular yet challenging area.



\subsection{Surveys on In-context Instructions}
\label{subsec:related_survey}

Several existing works share similar motivations with us that also survey instruction learning~\cite{dong2022survey,huang2022towards,qiao2022reasoning,yu2023nature}. However, they only focus on some sub-areas of in-context instruction (prompt, few-shot demonstrations, Chain-of-Thought reasoning, etc.). For example, \citet{liu2023pre} provides a comprehensive overview of prompt learning and LLMs, where the prompt can be regarded as one specific type of textual instruction (as categorized in~\cref{sec:categories}). Some other studies surveying ``soft instruction'', namely parameter-efficient fine-tuning methods~\cite{lialin2023scaling}, also differ from our scope of ``textual instruction''. To the best of our knowledge, this is the first work that provides a comprehensive and high-level story of textual instruction learning.




\section{Preliminary}
\label{sec:pre}
 
For instruction learning, we target driving the systems to reach the corresponding output of the input by following the instruction. Thus, we assume that a dataset usually consists of three items:
% In other words, the final input string of LMs can be divided into several distinct components:
% the requirements mentioned in the task instructions



\textbullet\enspace \textbf{Input} (\textsc{X}): the input of an instance; it can be a single piece of text (e.g., sentiment classification) or a group of text pieces (e.g., textual entailment, question answering, etc.). 
% It is widely used in human-oriented instructions (e.g., task title, category, and definition) but is usually optional for PLM-oriented and entailment-oriented instructions.

\textbullet\enspace \textbf{Output} (\textsc{Y}): the output of an instance; in classification problems, it can be one or multiple predefined labels; in text generation tasks, it can be any open-form text. 

\textbullet\enspace \textbf{Template} (\textsc{T}): a textual template that either tries to express task intent or is used for bridging \textsc{X} and \textsc{Y}.\footnote{A plain template connecting \textsc{X} and \textsc{Y}, e.g., ``\texttt{The input is [$\ldots$] The output is [$\ldots$]}'', no task-specific semantics.} \textsc{T} may not be an instruction yet.

In \cref{sec:categories}, we will elaborate that a task instruction \textsc{I} is actually a combination of \textsc{T} with \textsc{X} or \textsc{Y}, or the \textsc{T} on its own in some cases.




% Generally, instruction learning aims to use the template language $f(\cdot)$ to combine the task input $x$ and the task-specific information $I$. The converted result $x^{'} = f(x,I)$ is considered as the final input of LMs~\footnote{Both task-specific and task-agnostic textual information can be regarded as ``instructions''.}. In the following text, we will first summarize different instruction categories (\cref{sec:categories}); We then introduce current popular instruction modeling strategies (\cref{sec:modeling}), that is, how the model uses these different pieces of information, i.e., $x$, $I$, $f(\cdot)$; We further discuss some important factors in using instructions for cross-task generalization (\cref{sec:analysis}) and the applications of instructions (\cref{sec:app}). Finally, we emphasize several challenges in the current instruction learning paradigm and provide potential future directions accordingly (\cref{sec:challenges}).


\section{What is Task Instruction?---A Unified Perspective from Indirect Supervision}
\label{sec:categories}
% The Taxonomies of Textual Instructions
% \textit{What} is Instruction?

% When it comes to instruction, individuals mainly refer to the prevalent prompts.
This section first summarizes three main instruction types constructed by different combinations of \textsc{T}, \textsc{X}, and \textsc{Y}
% (\entailinstruction,~\plminstruction, and~\humaninstruction), 
(as illustrated in Figure~\ref{fig:overview}), then presents our interpretation of them via an \textit{indirect supervision} perspective.

% Various types of textual instructions have been  used in  previous zero- and few-shot NLP tasks, such as \uline{prompts}~\citeiter{hendrycksmeasuring,srivastava2022beyond,bach2022promptsource},
% %
% \uline{Amazon Mechanical Turk instructions}~\citeiter{mishra2022cross,wang2022benchmarking,yin2022contintin},
% %
% \uline{instructions augmented with demonstrations}~\citeiter{khashabi-etal-2020-unifiedqa,ye-etal-2021-crossfit,min-etal-2022-metaicl}, 
% % \footnote{We consider the templates used in the in-context learning as the instructions rather than the few-shot demonstrations.}
% and \uline{Chain-of-Thought explanations}~\citeiter{wei2022chain,lampinen2022can,li2022explanations}, etc.
% % (Chain-of-Thought reasoning, etc.)
% Different instructions are initially designed for distinct objectives (e.g., Mturk instructions are originally created for human annotators to understand, and prompts are to steer PLMs). In this section, as illustrated in Figure~\ref{fig:overview}, we first summarize these instructions into three categories that perform different combinations of \textsc{T}, \textsc{X}, and \textsc{Y} (  \textsc{Entailment-oriented},  \textsc{PLM-oriented}, and  \textsc{Human-oriented}), then  compare them and provide the formal definition of instructions.




\begin{table*}[!ht]
    \centering
    \scriptsize
    \resizebox{0.99\textwidth}{!}{
    \begin{tblr}
    % set tablr args, including linewidth, v/h alignment
    {
    width=\linewidth, 
    colspec = {X[l,0.09\linewidth] || X[l,0.36\linewidth] | X[l,0.44\linewidth]},
    rowspec = {Q[b]Q[m]Q[m]Q[m]Q[m]Q[m]},
    % row{1} = {,blue!15}, % font=\bfseries, head in blue
    row{1} = {bg=azure6, fg=white, font=\bfseries},
    row{even} = {gray!15}, % even rows in gray
    rowhead = 1,
    hspan = minimal,
    }

    Task  & 
    \textsf{\textualentailment~premise (i.e., input text)} &
    \textsf{\textualentailment~hypothesis (i.e., instructions \textsc{Y})} \\ \hline

    \textit{Entity \newline Typing} & [Donald  Trump]$_{ent}$ served as the 45th president of the United States from 2017 to 2021. & (\textcolor{blue}{\checkmark}) Donald  Trump is a \textbf{politician} \newline (\textcolor{red}{\XSolidBrush}) Donald  Trump is a \textbf{journalist}\\\hline

    \textit{Entity \newline Relation}  & [Donald  Trump]$_{ent1}$ served as the 45th president of the [United States]$_{ent2}$ from 2017 to 2021. & (\textcolor{blue}{\checkmark}) Donald  Trump  \textbf{is citizen of} United States \newline (\textcolor{red}{\XSolidBrush}) Donald  Trump  \textbf{is the CEO of} United States \\\hline  

    \textit{Event \newline Argument \newline Extraction} & In [1997]$_{time}$, the [company]$_{sub}$ [hired]$_{trigger}$ [John D. Idol]$_{obj}$ to take over Bill Thang as the new chief executive. & (\textcolor{blue}{\checkmark}) \textbf{John D. Idol}   was hired. \newline (\textcolor{blue}{\checkmark}) \textbf{John D. Idol}   was hired in 1997. \newline (\textcolor{red}{\XSolidBrush}) \textbf{Bill Thang}  was hired. \\\hline

    \textit{\enspace\newline  Event \newline Relation} &  Salesforce  and Slack Technologies  have [entered]$_{event1}$ into a definitive agreement] under which Salesforce will [acquire]$_{event2}$ Slack. & (\textcolor{blue}{\checkmark}) Salesforce acquires Slack \textbf{after} it enters into the agreement with Slack Tech. \newline (\textcolor{red}{\XSolidBrush}) Salesforce acquires Slack \textbf{because} it enters into the agreement with Slack Tech. \\\hline  

    \textit{Stance \newline Detection} & Last Tuesday, Bill said ``animals are equal to human beings'' in his speech. & (\textcolor{blue}{\checkmark}) Bill \textbf{supports} that animals should have lawful rights. \newline (\textcolor{red}{\XSolidBrush}) Bill \textbf{opposes} that animals should have lawful rights.\\\hline

    \end{tblr}
    }
    \vspace{-1em}
    \caption{{\em{\small \entailinstruction}}~construct hypotheses to explain the labels (in \textbf{bold}). ``\textcolor{blue}{\checkmark}'':  correct; ``\textcolor{red}{\XSolidBrush}'': incorrect.}
    \label{tab:nlptonli}
\end{table*}


\begin{table*}[!ht]
    \centering
    \scriptsize
    \resizebox{1.0\textwidth}{!}{
    \begin{tblr}
    % set tablr args, including linewidth, v/h alignment
    {
    width=\linewidth, 
    colspec = {X[l,0.08\linewidth] || X[l,0.36\linewidth] | X[l,0.23\linewidth] | X[l,0.07\linewidth] | X[l,0.11\linewidth]},
    rowspec = {Q[b]Q[m]Q[m]Q[m]Q[m]Q[m]},
    % row{1} = {,blue!15}, % font=\bfseries, head in blue
    row{1} = {bg=azure6, fg=white, font=\bfseries},
    row{even} = {gray!15}, % even rows in gray
    rowhead = 1,
    hspan = minimal,
    }
    \SetCell[c=1]{c} Task & \SetCell[c=1]{c} Input~\textsc{X} & \SetCell[c=1]{c} Template~\textsc{T} (cloze question) & \SetCell[c=1]{c} Answer & \SetCell[c=1]{c} Output~\textsc{Y} \\\hline

    % \SetCell[r=3]{} Sentiment & \SetCell[r=3]{} I would like to buy it again. & \SetCell[r=3]{} [\textsc{X}] The product is \blank & Great & \SetCell[r=3]{} Positive \\ 
    % Classification & & & Wonderful & \\
    %  & & & $\ldots$ & \\\hline
    
    \textit{Sentiment Analysis} & I would like to buy it again. & \texttt{[\textsc{X}]} The product is \blank. & Great \newline Wonderful \newline $\ldots$ & Positive \\\hline

    \textit{Entity Tagging} & [Donald Trump]$_{ent}$ served as the 45th president of the United States from 2017 to 2021. & The entity in \texttt{[\textsc{X}]} is a \blank class? & Politician \newline President \newline $\ldots$ & People \\\hline

    \textit{Relation Tagging} & [Donald  Trump]$_{ent1}$ served as the 45th president of the [United States]$_{ent2}$ from 2017 to 2021. & \texttt{[\textsc{X}]} entity$_1$ is the \blank of entity$_2$? &  Executive \newline Leader \newline $\ldots$ & President \\\hline

    \textit{Textual Entailment} & \texttt{[\textsc{X$_1$}]}: Donald Trump served as the 45th president of the United States from 2017 to 2021. \newline \texttt{[\textsc{X$_2$}]}: Donald Trump is a citizen of United States. & \texttt{[\textsc{X$_2$}]}? \blank, because \texttt{[\textsc{X$_1$}]} & Indeed \newline Sure \newline $\ldots$ & Yes \\\hline

    \textit{Translation} & Donald Trump served as the 45th president of the United States from 2017 to 2021. & Translate \texttt{[\textsc{X}]} to French:~\blank &  / & été président~{...} \\\hline
    % full translation: Donald Trump a été président des États-Unis de la 45ème législature de 2017 à 2021
    
    \end{tblr}
    }
    \vspace{-1em}
    \caption{{\em \plminstruction}~utilize templates to convert the origin inputs into fill-in-blank questions. In most classification tasks, the intermediate answers may require further mapping (i.e., verbalizer).}
    % should be further mapped into the predefined label
    \label{tab:PLM-orentied}
\end{table*}


\subsection{Three Types of Instructions}

% \subsection{\textsc{I}=\textsc{T}+\textsc{Y}: Entailment-oriented Instruction}
\paragraph{ \entailinstruction~(i.e., \textsc{I}=\textsc{T}+\textsc{Y}).}

A conventional scheme to handle the classification tasks is to convert the target labels into indices and let models decide which indices the inputs belong to. This paradigm only encodes the input semantics while losing the label semantics. To let systems recognize new labels without relying on massive labeled examples, \newcite{yin2019benchmarking} proposed converting the target classification tasks into natural language inference (NLI) by building a hypothesis for each label---deriving the truth value of a label is then converted into determining the truth value of the hypothesis.  As exemplified in Figure~\ref{fig:overview}~(a), this approach builds instructions (\textsc{I}) by combining a template (\textsc{T}) with a label (\textsc{Y}) to explain the task semantics. Table~\ref{tab:nlptonli} further provides more detailed examples for~\entailinstruction.
% Since this paradigm naturally satisfies the format of textual entailment (\textualentailment, where the task inputs and the instructions can be treated as premises and hypotheses, respectively), these kinds of instructions are termed ``Entailment-oriented Instructions'' (\entailinstruction).

The advantages of {\entailinstruction} learning are four-fold: (i) it keeps the label semantics and makes it possible to encode the input-output relations; (ii) it unifies various classification problems into an NLI task; (iii) by making use of the indirect supervision from existing NLI datasets, a model trained on NLI tasks is expected to work on other tasks in a zero-shot manner; (iv) it extends the original close-set indices classification problem into an open-domain label recognition paradigm. Therefore, it has been widely used in a variety of few/zero-shot classification tasks~\cite{xu2022universal}, such as classifying topics~\cite{yin2019benchmarking}, sentiments~\cite{zhong2021adapting}, stances~\cite{xu-etal-2022-openstance}, entity types~\cite{li2022ultra}, entity relations~\cite{murty2020expbert,xia2021incremental,sainz-etal-2021-label,sainz-etal-2022-textual}, etc.



\begin{table*}[!ht]
    \centering
    \footnotesize
    \resizebox{1.0\textwidth}{!}{
    \begin{tblr}
    % set tablr args, including linewidth, v/h alignment
    {
    width=\linewidth, 
    colspec = {X[l,0.08\linewidth] || X[l,0.23\linewidth] | X[l,0.64\linewidth] | X[l,0.08\linewidth]},
    rowspec = {Q[b]Q[m]Q[m]Q[m]},
    % row{1} = {,blue!15}, % font=\bfseries, head in blue
    row{1} = {bg=azure6, fg=white, font=\bfseries},
    row{even} = {gray!15}, % even rows in gray
    rowhead = 1,
    hspan = minimal,
    }

    %+ Optional Demonstrations
    % Template \textsc{T} + $\{\textsc{X}_i,\textsc{Y}_i\}_{i=1}^k$
    % \textsc{I} = \textsc{T}+ optional $\{\textsc{X}_i,\textsc{Y}_i\}_{i=1}^k$
    % Instruction (i.e., Template + Few-shot Demonstrations)
    \SetCell[c=1]{c} Task  & \SetCell[c=1]{c} Input~\textsc{X} & \SetCell[c=1]{c} Template~\textsc{T} + Few-shot Demonstrations & \SetCell[c=1]{c} Output~\textsc{Y}  \\\hline

    \textit{Sentiment \newline Analysis} & I am extremely impressed with its good performance. I would like to buy it again! 
     & 
    \TaskDef: \newline
    \colorbox{MyPink!20}{\quad In this task, you are given a product review, and you need to identify~$\dots$} \newline
    \Demonstrations~\textsf{(optional)}: \newline
    \colorbox{MyOrange!15}{\quad Input: \textsl{These are junks, I am really regret...} \quad Output: \textsl{Negative}} \newline
    \colorbox{MyOrange!15}{\quad Input: \textsl{Wonderful bulb with good duration...} \quad Output: \textsl{Positive}} \newline
    \TestInput: \newline
    \colorbox{MyGreen!20}{\quad Input: \texttt{[\textsc{X}]} \quad Output:~\blank}
     & 
     Positive\\\hline

    \textit{Named \newline Entity \newline Extraction} & Donald Trump served as the 45th president of the United States from 2017 to 2021. 
    &
    \TaskDef: \newline
    % {\begin{tcolorbox}[colback=yellow!30]
    % {Your task is to recognize the name of a person in the given sentence~$\dots$}
    % \end{tcolorbox}} 
    \colorbox{MyPink!15}{\quad Your task is to recognize the name of a person in the given sentence~$\dots$} \newline
    \Demonstrations~\textsf{(optional)}: \newline
    \colorbox{MyOrange!15}{\quad Input: \textsl{Ousted WeWork founder Adam Neuman...} \quad Output: \textsl{Adam Neuman}} \newline
    \colorbox{MyOrange!15}{\quad Input: \textsl{Tim Cook became the CEO of Apple Inc since...} \quad Output: \textsl{Tim Cook}} \newline
    % \colorbox{MyOrange!15}{\quad $\ldots$} \newline
    \TestInput: \newline
    \colorbox{MyGreen!20}{\quad Input: \texttt{[\textsc{X}]} \quad Output:~\blank}
    % \colorbox{gray!15}{\quad Input: \texttt{[\textsc{X}]} \quad Output:~\blank}
    & 
    Donald Trump \\\hline
    
    \end{tblr}
    }
    \vspace{-1em}
    \caption{Two examples that illustrate the {\em \humaninstruction}~(w/ 2-shot demonstrations). Similar to the \plminstruction, \humaninstruction~use task-level templates to convert the origin inputs into blank questions. However, the templates here have sufficient task semantics (i.e., {\color{MyPink}{\em \textrm{Task Definition}}}) and are sometimes equipped with~{\color{MyOrange}{\em \textrm{Demonstrations}}}, while those in \plminstruction~usually do not.}
    % The human-oriented instruction is similar to the PLM-oriented instruction, which also utilizes a template to convert origin input (\textcolor{red}{in red}) into a cloze question. However, the task template itself contains informative task semantics, i.e., the formal task definition. Meanwhile, few-shot alternative task demonstrations are also provided (\textcolor{blue}{in blue})
    \label{tab:human-orentied}
\end{table*}




% \subsection{\textsc{I}=\textsc{T}+\textsc{X}: PLM-oriented Instruction (e.g., prompts)}
\paragraph{ \plminstruction~(i.e., prompts; \textsc{I}=\textsc{T}+\textsc{X}).}


As shown in Figure~\ref{fig:overview}~(b) and Table~\ref{tab:PLM-orentied}, the prompt is a representative of the~\plminstruction, which is usually a brief utterance prepended with the task input (prefix prompt), or a cloze-question template (cloze prompt). It is basically designed for querying the intermedia responses (that can be further converted into the final outputs) from the PLM.
% \footnote{Please refer to Section 2.2.1 of~\citet{liu2023pre} for a detailed definition of the prompt.}
Since the prompted input conforms to the pre-training objectives of PLM (e.g., the cloze-style input satisfies the masked language modeling objective~\cite{kenton2019bert}), it helps get rid of the reliance on the traditional supervised fine-tuning and greatly alleviates the cost of human annotations. Thus, prompt learning achieved impressive results on a multitude of previous few/zero-shot NLP tasks, like question answering~\cite{radford2019language,lin2021few}, machine translation~\cite{li-etal-2022-prompt}, sentiment analysis~\cite{wu-shi-2022-adversarial}, textual entailment~\cite{schick2021exploiting,schick2021few}, entity recognition~\cite{cui2021template,wang2022instructionner}, etc.

% Nevertheless, the performance 
% in using this sort of PLM-oriented instruction
Despite the excellent performance of prompt techniques, there are still two obvious shortcomings with \plminstruction~in real-world applications. (i) \textit{Not User-Friendly}. As the prompt is crafted for serving PLMs, it is encouraged to design the prompt in a ``model's language'' (e.g., model-preferred incoherent words or internal embedding). However, this PLM-oriented style is hard to be understood by users and often violates human intuitions~\cite{gao-etal-2021-making,li-liang-2021-prefix,qin-eisner-2021-learning,khashabi2022prompt}. Meanwhile, the performance of prompts highly depends on the laborious prompt engineering~\cite{bach2022promptsource}, but most end-users are not PLM experts and usually lack sufficient knowledge to tune an effective prompt. (ii) \textit{Applications Constraints}. The prompt is usually short and simplistic, whereas many tasks cannot be effectively formulated with solely a brief prompt, making prompt hard to deal with the diverse formats of real-world NLP tasks~\cite{chen2022knowprompt,zhang2023aligning}.
% While we hope 














% \subsection{\textsc{I}=\textsc{T}+ optional $\{\textsc{X}_i,\textsc{Y}_i\}_{i=1}^k$: Human-oriented Instruction}
\paragraph{\humaninstruction~(i.e., \textsc{I}=\textsc{T}+ optional $\{\textsc{X}_i,\textsc{Y}_i\}_{i=1}^k$).}



\humaninstruction~essentially denotes the instructions used for crowd-sourcing on the human-annotation platforms (e.g., Amazon MTurk). Unlike \plminstruction,  \humaninstruction~ (Figure~\ref{fig:overview}~(c)) are usually some human-readable, descriptive, and paragraph-style information consisting of various components, such as ``\texttt{task title}'', ``\texttt{category}'', ``\texttt{definition}'', and ``\texttt{things to avoid}'', etc.~\citep[cf.][]{mishra2022cross} Thus, \humaninstruction~are more user-friendly and can be ideally applied to almost any complex NLP task. Table~\ref{tab:human-orentied} further shows some representative task examples.

% in Appendix~\ref{appendix:example_table}
% More and more works began to employ the human-oriented instructions
% For example, \citet{efrat2020turking} tried to test whether GPT-2~\cite{radford2019language} can follow the MTurk instructions to annotate some popular NLP datasets. Their results showed that the vanilla GPT-2~\cite{wolf2019huggingface} worked poorly on following these \humaninstruction. 
%
Accordingly, \humaninstruction~have attracted much more attention in recent years~\citeiter{hu2022context,gupta-etal-2022-instructdial,yin2022contintin}. However, due to the complex nature, \humaninstruction~are more challenging to encode by vanilla PLMs. For example, off-the-shelf GPT-2 was found to work poorly on following MTurk instructions~\cite{wolf2019huggingface,efrat2020turking}. 
To tame the PLMs better understand the \humaninstruction, follow-up works began to collect large-scale instruction datasets~\cite{mishra2022cross,wang2022benchmarking}. The result was that, after fine-tuning with various task instructions, the text-to-text PLMs, like BART~\cite{lewis2020bart} and T5~\cite{raffel2020exploring}, achieved remarkable few/zero-shot generalizations by following these complex instructions.
% While recent works found that multi-task instruction fine-tuned PLMs could get more positive results. For instance, \citet{mishra2022cross} collected more than 60 NLP tasks with the corresponding MTurk instructions; \citet{wang2022benchmarking} further extended this collection into a 1.6k cross-lingual tasks scale. They all concluded that, after the large-scale instruction tuning, the text-to-text PLMs, like BART~\cite{lewis2020bart} and T5~\cite{raffel2020exploring} could generalize to the challenging unseen tasks by benefiting from the \humaninstruction.


\begin{table*}[t]
 \setlength{\belowcaptionskip}{-10pt}
 \setlength{\abovecaptionskip}{5pt}
    \centering
    \scriptsize
    \resizebox{0.99\textwidth}{!}{
    \begin{tblr}
    % set tablr args, including linewidth, v/h alignment
    {
        width=\linewidth, 
        colspec = 
            {
            X[l,0.34\linewidth]
            X[c,0.18\linewidth]
            X[c,0.17\linewidth]
            X[c,0.20\linewidth]
            },
        rowspec = 
            {
            |[2pt,MyGrey]Q[b]
            |[1pt,MyGrey,solid]Q[m]Q[m]Q[m]Q[m]
            |[0.8pt,MyGrey,dashed]Q[m]Q[m]Q[m]
            Q[m]Q[m]|[2pt,MyGrey]
            },
        row{1} = {font=\bfseries},
        rowhead = 1,
        hspan = minimal,
    }
    {\footnotesize Trait} & {\footnotesize \texttt{NLI-oriented}} & {\footnotesize \texttt{PLM-oriented}} & {\footnotesize \texttt{Human-oriented}} \\
    % \hline
    Update PLM parameter? & \MyYes & \MyMaybe & \MyYes \\ 
    % \hline
    Require  super large PLMs? & \MyNo & \MyYes & \MyNo \\
    % \hline
    Require further label mapping (e.g., verbalizer)? & \MyYes & \MyYes & \MyNo \\
    % \hline
    % Does instruction contain label semantics? & \MyYes & \MyMaybe & \MyYes \\
    % \hline
    % Can it be without additional context (input \textsc{X})? & \MyNo & \MyNo & \MyYes \\
    % \hline
    % Sufficiently describe task semantics? & \MyNo & \MyNo & \MyYes \\
    % \hline
    End-user friendly? & \MyNo & \MyNo & \MyYes \\
    % \hline
    Instruction granularity & sentence-level (brief) & sentence-level (brief) & paragraph-level (complex) \\
    % \hline
    Instruction scope & output-wise & input-wise & task-wise \\
    Task scope & classification & classification \& generation & classification \& generation \\
    % \hline
    Modeling objective  & NLI  & language modeling & follow instructions \\
    % \hline
    Source of indirect supervision & NLI  & language modeling  & various Text-to-Text tasks \\
    % \hline

    \end{tblr}
    }
    % \vspace{-1em}
    \caption{Comparison of the three different instruction types in~\cref{sec:categories}.}
    \label{tab:comparison}
\end{table*}


\subsection{An Indirect Supervision Perspective}
Although the three types of instructions are very different from each other, they are essentially seeking the same thing---\textit{indirect supervision}---to cope with target tasks that have limited annotations. 

Specifically, \entailinstruction~transform target NLP problems into a source task---NLI---so that the rich supervision from existing NLI datasets can act as indirect supervision for those target problems. \plminstruction~reformat target problems into the source task---language modeling, so that the rich generic-purpose knowledge in those PLMs can be directly utilized to get the output. Whether it is \entailinstruction~or \plminstruction, both try to solve unseen tasks with a generalizable system. However, both of them have limited application scope, e.g., they cannot efficiently deal with some structured prediction tasks~\cite{chen2022knowprompt,zhang2023aligning}. Instead of seeking supervision from a single source task (NLI or language modeling), \humaninstruction~learn indirect supervision from a large set of training tasks, the resulting system, therefore, can ideally generalize to any unseen textual tasks.
% As a result, in order to completely handle any new tasks that are expressed by \humaninstruction, a large set of source tasks with an instruction for each has to be prepared, hoping that these source tasks can provide indirect supervision.
\Cref{tab:comparison} further compares them from different dimensions.






\section{How to Model Instructions?}
\label{sec:modeling}

% Instructions are versatile, but how do the current machine systems utilize instructions?
% but how does the model learn to correctly \textit{understand} and \textit{follow} instructions?
% The Modeling Strategies for Instruction Learning
% \textit{How} to Use Instructions?
Since both \entailinstruction~and \plminstruction~are associated with either the input $\textsc{X}$ or the output $\textsc{Y}$, these types of instructions do not require specific system design to encode them. \entailinstruction~can be handled by regular systems for the NLI task, and \plminstruction~are mostly fed to auto-regressive PLMs. In contrast, \humaninstruction~are the most challenging type since it is independent of any labeled instances. 

Therefore, this section mainly presents several mainstream modeling strategies for the \humaninstruction, as illustrated in Figure~\ref{fig:modeling_strategies}.
% , including (i) \textbf{Semantic Parser}, (ii) \textbf{Flatten-and-Concatenation}, and (iii) \textbf{HyperNetwork}.

% Therefore, this section summarizes several popular modeling strategies for instruction learning. Overall, we introduce three different modeling schemes. For the earlier rule-based machine learning systems, the (i) \textbf{Semantic Parser} was the commonly chosen method for encoding instructions. With the rapid evolutions of neural networks, instructions can be encoded into rich neural representations directly. As a result, a naive modeling strategy is (ii) \textbf{Flatten-and-Concatenation} approach, which simply utilizes models to encode the concatenation of instruction and task input. Recently, (iii) \textbf{HyperNetwork} also garnered greater interest.
% (ii). \textbf{Prompting Template-based} and the (iii). \textbf{Prefix Instruction-based}




% \subsection{Semantic Parser-based}
\subsection{Semantic Parser}

At the early stage of machine learning, to help the systems understand natural language instructions, a great number of works employed semantic parsing to convert the instruction into the formal language (logical formula), which can be more easily executed by the systems~\citeiter{goldwasser2014learning}. As exemplified in Figure~\ref{fig:modeling_strategies}~(a), a game instruction ``\texttt{Move any top card to an empty free cell}'' can be processed into an executable formula: ``\texttt{card(x) $\wedge$ freecell(y)}''.
% ``\texttt{card(x) $\wedge$ freecell(y) $\wedge$ empty(y)}''

Previous research spent extensive efforts on this strategy, among which most are used for human-computer interaction tasks, e.g., playing soccer games~\cite{kuhlmann2004guiding}. To alleviate laborious human annotations, the follow-up works leveraged indirect or weak supervision from the grounded environments (e.g., knowledge base) to train the semantic parser~\cite{kim2012unsupervised}.

% \footnote{For instance, a game instruction ``\texttt{Move any top card to an empty free cell}'' can be converted into an executable formula: ``\texttt{card(x) $\wedge$ freecell(y) $\wedge$ empty(y)}''.}
%\cite{liang2009learning,branavan2010reading,vogel2010learning,clarke2010driving,chen2011learning,matuszek2012joint,babecs2012learning,chen2012fast,goldwasser2014learning}
% Formally, the semantic parser-based approach can be written as:
% \[I^{'} = \textrm{Semantic Parser}(I)\]
% where the \(I^{'}\) is the converted logical formula.

% For example, \citet{kuhlmann2004guiding} first tried to utilize natural language instructions to guide the systems to play soccer games, where they trained an individual semantic parser in advance, and then mapped the textual instructions into formal languages that can be used to influence the policy learned by the reinforcement learner. Since constructing a fully-supervised semantic parser requires laborious human annotations, the follow-up works also used indirect or weak supervision coming from the grounded environments (e.g., knowledge base) to train the semantic parser~\cite{eisenstein2009reading,chen2008learning,kim2012unsupervised,artzi2013weakly,krishnamurthy2013jointly}.
% Besides using the converted formal languages to guide the systems to complete specific tasks, some works also utilized the logical formulae of the instructions to perform data and feature augmentations~\cite{srivastava2017joint,hancock2018training,wang2020learning,ye2020teaching}. We will further introduce this sort of application in~\cref{subsec:HCI}.

\paragraph{Limitations.} Semantic parser-based approaches mainly apply to individual tasks rather than universal cross-task generalization, because building a versatile semantic parser for all NLP tasks is over challenging. By contrast, the approach introduced in the next subsection aims at cross-task generalization with limited supervision for the target tasks.


% \begin{table*}[t]
%     \centering
%     \resizebox{0.98\textwidth}{!}{
%     \begin{tabular}{c|c|c|c}
%          & Entailment-oriented & PLM-oriented & human-oriented \\\hline
%        indirect supervision  & textual entailment & language modeling & other text-to-text tasks\\\hline
%        instruction scope & instance-wise instruction & instance-wise instruction & task-wise instruction\\\hline
%        update PLM? & yes & usually no & yes\\\hline
%        need super large PLM? & no & yes & no\\\hline
%        training tasks & textual entailment & no & many text-to-text tasks\\\hline
%        end-user friendly? & no & no & yes\\\hline
%        need additional label mapping? & yes & usually yes & no\\\hline
%        has label information/output constraints? & yes & usually no & usually yes\\\hline
%        can be without task input $\textsc{X}$? & no & no & yes\\\hline
%     \end{tabular}
%     }
%     \caption{Comparison of three types of instructions. \textcolor{red}{@wenpeng: I have added three rows}}
%     \label{tab:my_label}
% \end{table*}




\begin{figure}[!t]
 \setlength{\belowcaptionskip}{-9pt}
 \setlength{\abovecaptionskip}{5pt}
	\begin{center}
		\centering
		\includegraphics[width=1.01\linewidth]{ picture/modeling_strategy_v1.pdf}
	\end{center}
	\caption{Three modeling strategies for instructions.
 % : (a) \textit{semantic parser} based modeling uses an additional semantic parser to process human instruction into a machine-executable logical formula; (b) \textit{flatten-and-concatenation} simply concatenates all the textual pieces (instruction, task input, etc.) into a flattened long sequence, which is further encoded by neural model directly; (c) \textit{HyperNetwork} converts textual instruction into a block of neural parameters (model plugin) that can be inserted into the base model.
 }
    \vspace{-0.5em}
	\label{fig:modeling_strategies}
\end{figure}





% \subsection{Tuning-based}
\subsection{Flatten-and-Concatenation}
% \label{subsec:tuning-based}
\label{subsec:flatten-concatenation}

% With the rapid evolutions of neural networks, instructions can be further encoded into rich neural representations directly. As a result, a naive modeling strategy is
In contrast to the semantic parser approach, which considers the instructions' structure and the target problems, methods based on the neural networks take more brutal treatment: as illustrated in Figure~\ref{fig:modeling_strategies}~(b)---instructions, regardless of their length, structure, task types, etc., are flattened as a long token sequence and concatenated with the input $\textsc{X}$ as a new input sequence for the models, which has been widely adopted by the prior research~\citeiter{wang2022benchmarking,wei2023symbol}. However, this naive strategy constantly results in unsatisfactory performances when using vanilla models~\cite{weller2020learning}, leading to its reliance on large-scale instruction fine-tuning, well-known as ``instruction tuning''.

\paragraph{Limitations.} (i) Flattening and concatenating everything into a long sequence tends to ignore some key information that humans can often capture in the instruction~\cite{mishra-etal-2022-reframing,jang2022can}, such as negation (e.g., ``{{\fontfamily{lmtt}\selectfont do not generate outputs longer than 5 tokens}}''), warning (e.g., ``{\fontfamily{lmtt}\selectfont generate `D' if the question is not answerable or you're not sure}''), output constraints (e.g., ``{\fontfamily{lmtt}\selectfont your answer should be in one of `A', `B', `C', and `D'}''), and so on. (ii) To let models understand the instruction, a large number of training tasks have to be prepared. This is similar to what happened in the early years of deep learning in NLP: to improve the performance of deep neural networks for a particular task, we collect more labeled examples; back to the instruction learning, the system's comprehension of the instruction, unfortunately, still exhibits a high degree of dependence on  the scale of training tasks~\cite{chung2022scaling}.

% =============== all about instruction tuning ============

% As for the neural network-based systems, we can directly encode the natural language instructions into the model's embedding without the help of an additional semantic parser. What's more, benefiting from the rich prior knowledge, off-the-shelf PLMs could recover the task semantics conveyed in the instructions and perform zero-/few-short learning~\cite{radford2019language,brown2020language}. However, PLMs can not always successfully recognize and understand the instructions, especially for those complex human-oriented instructions~\cite{weller2020learning}. Therefore, more and more works try to fine-tune the models with instructions, i.e., instruction tuning~\footnote{Also known as ``instruction fine-tuning'', which means fine-tuning the model parameters instead of instruction optimization (prompt engineering).}.
% , e.g., tuning soft prompts~\cite{li-liang-2021-prefix,tsimpoukelli2021multimodal,han2021ptr}.

% Multi-task instruction tuning is a representative strategy for the tuning-based method~\cite{wei2021finetuned,sanh2021multitask,wang2022benchmarking}. By converting the original task inputs into an instruction format (either prompted questions or prefix instructions), it fine-tunes the models on the massive multi-task datasets. Besides multi-task learning, recent works also conduct instruction tuning in a reinforcement learning manner~\cite{ouyang2022training}. Although instruction tuning still relies on training (i.e., gradient backpropagation), different from traditional supervised learning, it targets training models to follow instructions rather than completing specific tasks.  

% Existing works have demonstrated the effectiveness of instruction tuning, where the fine-tuned models show better instruction-following ability than the off-the-shelf PLMs~\cite{mishra2022cross,chung2022scaling}. We will further discuss the instruction tuning in~\cref{subsec:instruction_tuning}.

% =============== all about instruction tuning ============


% \subsection{Prompting Template-based}
% % to convert the task instructions with the original task input into a cloze question by applying a template.
% As for the neural network-based systems, we can directly encode the natural language instructions into the model's embedding without the help of a semantic parser. One of the prominent modeling strategies is using the prompting template. The essence of the prompting template-based approach is to use a template to convert the task input into a prompted format (i.e., cloze question~\footnote{Unlike the definition in~\citet{liu2023pre}, we regard any fill-in-blank question as the cloze, no matter whether the blank is in the middle or at the end of the question.}). According to the terminologies in~\cref{sec:categories}, the final input of LMs can be described as $x^{'}=f(x)$ or $f(x, I)$, i.e., the task-agnostic template $f(\cdot)$ is required but the task-specific information $I$ is optional. For example, $x=$~``\textit{I love this movie.}'' and $x^{'}=$~``\textit{I love this movie. It was~\blank}'', where there is no any task-specific information provided.
% ; Or $x=$~``\textit{I love this movie.}'' and $x^{'}=$~``\textit{I love this movie. It was~\blank}''

% The prompting template-based approach is particularly useful for modeling PLM-oriented and entailment-oriented instructions. Therefore, a lot of previous works employed this strategy to modeling prompts~\citeiter{petroni-etal-2019-language,jiang2020can,cui2021template,haviv-etal-2021-bertese,schick-schutze-2021-just}. Besides using the discrete prompting template, recent works also tried to tune the continuous templates and achieved incredible results~\citeiter{li-liang-2021-prefix,tsimpoukelli2021multimodal,han2021ptr}.



% \subsection{Prefix Instruction-based}

% Distinct from the prompting template-based approach, the prefix instruction-based method is mainly used for modeling human-oriented instructions, where sufficient task-specific information is provided~\citeiter{mishra2022cross,wang2022benchmarking,yin2022contintin,gu2022robustness}. Formally, in this case, the final input of LMs can be written as $x^{'}=I \oplus x$ or $f(x, I)$, which means the $I$ is required and has to be the prefix of the task input $x$, but the template language $f(\cdot)$ is optional.
% (we can simply consider $I$ as a prefix of $x$).
% task-agnostic
% when using prefix instruction-based modeling strategy
% For example, \citet{wang2022benchmarking} utilized the following template ``\textit{Definition:}~$\cdots$~\textit{Input:}~$\cdots$~\textit{Output:}'' as the $f(\cdot)$, where all the task-specific information $I$ is prepended with the task input $x$.
% to combine the prefix instruction $I$ and the task input $x$

\subsection{HyperNetwork}

% There are two obvious problems in the instruction tuning. First, it usually concatenates the task-level instruction with every instance-level input, the repeating procedure significantly slowing down the processing/inference speed and the lengthy input also increasing the burden of computational cost~\cite{liu2022few}. Second, it can potentially impact the optimization because the model can not explicitly distinguish the task input ($\textsc{X}$) from the whole instructions ($\textsc{I}$); thus, the model can simply learn to complete the task and ignore the instructions~\cite{webson-pavlick-2022-prompt,deb2022boosting}.  

% To address the above issues, recent works began to employ the hypernetwork~\cite{ha2016hypernetworks} to encode the task instructions~\cite{jin2020language,deb2022boosting,ivison2022hint}. The essences of using hypernetwork-based approach are (i) encoding the task instruction ($\textsc{I}$) and the task input ($\textsc{X}$) separately, and (ii) converting the instruction into task-specific model parameters. For example, \citet{ye2021learning} used the hypernetwork to convert the task instruction into several parameter-efficiency adaptors~\cite{houlsby2019parameter}. Then they inserted these adaptors behind the multi-head attention layers of the underlying LMs to perform cross-task generalization.
% % used the hyper network to convert the task instruction into several parameter-efficiency adaptors for the underlying LMs and then inserted these adaptors behind the multi-head attention layers of LMs.


Unlike the conventional modeling strategy that encodes the input sequence into the dense representation (i.e., language-to-representation), hypernetwork follows a language-to-parameter paradigm: as shown in Figure~\ref{fig:modeling_strategies}~(c), this scheme converts textual instruction into a block of model parameters that can be further plugged into the underlying models~\cite{ha2016hypernetworks,houlsby2019parameter,jin2020language}. As a result, hypernetwork-based instruction modeling can better leverage the structured input sequence by encoding the instruction and task input separately (i.e., instruction-to-parameter, input-to-representation), achieving stronger generalization compared with the flatten-and-concatenation approach~\cite{ye2021learning,deb2022boosting}. It can also significantly improve inference efficiency, as concluded by recent works~\cite{ivison2022hint}.
% To better leverage the structured input sequence and improve the modeling efficiency, the hypernetwork emerged accordingly. Unlike simply encoding the whole concatenated sequence, hypernetwork-based strategy tries to encode instructions (\textsc{I}) and the task input ($\textsc{X}$) separately. Specifically, it follows a language-to-model paradigm: converting instructions into several lightweight adaptors that can be further inserted into the underlying models.

\paragraph{Limitations.} 
Despite the attractive attributes of hypernetwork, its training instability and the reliance on architecture design (suiting the underlying models) are the stumbling blocks in real-world applications~\cite{Brock2017SMASHOM,ortiz2023non}. 



\section{Factors that Influence Instruction Learning Performance}
\label{sec:analysis}

Instruction learning is proven to be effective in a lot of few/zero-shot NLP tasks, but how to explain the impressive performance of instruction? And which aspects make a successful instruction learning procedure? We categorize the factors affecting instruction learning performance into four dimensions: \textit{model}, \textit{instruction},  \textit{model-instruction interaction}, and \textit{dataset}. Table~\ref{tab:takeways} displays a roadmap for this section, where we also conclude the takeaways to make it easy to refer to.
% Prior work at times introduced some empirical rules in using instructions and explaining the behaviors of instruction learning. Among various clues,



\begin{table}[t]
 \setlength{\belowcaptionskip}{-10pt}
 \setlength{\abovecaptionskip}{5pt}
    \centering
    \small
    \resizebox{0.99\linewidth}{!}{
    \begin{tblr}
    % set tablr args, including linewidth, v/h alignment
    {
        width=\linewidth, 
        colspec = 
            {
            X[l]
            },
        rowspec = 
            {
            |[2pt,MyBlue]Q[b]
            |[1pt,MyBlue]
            Q[m]Q[m]
            |[0.8pt,MyBlue,dashed]
            Q[m]Q[m]
            |[0.8pt,MyBlue,dashed]
            Q[m]Q[m]
            |[0.8pt,MyBlue,dashed]
            Q[m]Q[m]
            |[2pt,MyBlue]
            },
        row{1} = {bg=MyGrey!15, font=\bfseries},,
        rowhead = 1,
        hspan = minimal,
    }
    \SetCell{c} Recipes for Instruction Learning \\ 
    % \hline
    \colorbox{MyPink!13}{\textrm{{\em Model-related Factors}}} (\cref{subsec:modelfactors}) \\
    \begin{varwidth}[t]{\linewidth}
    \begin{itemize}[topsep=0pt,parsep=0pt]
        \item Instruction-tuned PLMs $>$ Vanilla PLMs.
        \item Instruction tuning tames PLMs to be more safe, robust, and user-friendly. 
        \item Larger PLMs benefit more from instruction tuning. \strut
    \end{itemize}
    \end{varwidth} \\
    
    \colorbox{MyPink!13}{\textrm{{\em Instruction-related Factors}}} (\cref{subsec:instruct_facor}) \\
    \begin{varwidth}[t]{\linewidth}
    \begin{itemize}[topsep=0pt,parsep=0pt]
        \item Keep instruction paradigm consistent during training and testing (e.g., abstractiveness).
        \item Design multiple instructions for one task in different wordings and perspectives. 
        \item Feeling exhausted about promoting diversity? Resort to the LLMs! 
        \item Few-shot demonstrations are useful in most cases. \strut
    \end{itemize}
    \end{varwidth} \\

    \colorbox{MyPink!13}{\textrm{{\em Model-Instruction Alignment}}} (\cref{subsec:model_instruct_align}) \\
    \begin{varwidth}[t]{\linewidth}
    \begin{itemize}[topsep=0pt,parsep=0pt]
        \item Better design your instructions in a model's language (e.g., conforming to the pertaining objectives). \strut
    \end{itemize}
    \end{varwidth} \\

    \colorbox{MyPink!13}{\textrm{{\em Data-wise Factors}}} (\cref{subsec:task_scale}) \\
    \begin{varwidth}[t]{\linewidth}
    \begin{itemize}[topsep=0pt,parsep=0pt]
        \item Try to tune PLMs on more diverse tasks. \strut
    \end{itemize}
    \end{varwidth} \\
    
    % \colorbox{MyPink!13}{\textrm{{\em Instruction Consistency}}} (\cref{subsec:consistency}) \\
    % \begin{varwidth}[t]{\linewidth}
    % \begin{itemize}[topsep=0pt,parsep=0pt]
    %     \item Keep instruction paradigm consistent during training and testing (e.g., conciseness). \strut
    % \end{itemize}
    % \end{varwidth} \\
    % \colorbox{MyPink!13}{\textrm{{\em Model and Task Scale}}} (\cref{subsec:scale}) \\
    % \begin{varwidth}[t]{\linewidth}
    % \begin{itemize}[topsep=0pt,parsep=0pt]
    %     \item Larger LMs benefit more from instructions. 
    %     \item Try to tune LMs on more diverse tasks.
    %     \item Model scale seems to outweigh task scale. \strut
    % \end{itemize}
    % \end{varwidth} \\
    % \colorbox{MyPink!13}{\textrm{{\em Instruction Diversity}}} (\cref{subsec:diversity}) \\
    % \begin{varwidth}[t]{\linewidth}
    % \begin{itemize}[topsep=0pt,parsep=0pt]
    %     \item Design multiple instructions for one task in different wordings and perspectives. 
    %     \item Feeling exhausted about promoting diversity? Resort to the LLMs! \strut
    % \end{itemize}
    % \end{varwidth} \\
    % \colorbox{MyPink!13}{\textrm{{\em Taxonomies and Situations}}} (\cref{subsec:situation}) \\
    % \begin{varwidth}[t]{\linewidth}
    % \begin{itemize}[topsep=0pt,parsep=0pt]
    %     \item Identify the traits of the target problem and choose instructions accordingly.
    %     \item Have no ideas? Just mix them!  \strut
    % \end{itemize}
    % \end{varwidth} \\
    % \colorbox{MyPink!13}{\textrm{{\em Model Preference}}} (\cref{subsec:preference}) \\
    % \begin{varwidth}[t]{\linewidth}
    % \begin{itemize}[topsep=0pt,parsep=0pt]
    %     \item Better design your instructions in a model's language (e.g., conforming to the pertaining objectives). \strut
    % \end{itemize}
    % \end{varwidth} \\

    \end{tblr}
    }
    \caption{The takeaways. We summarize some high-level suggestions for successful instruction learning.}
    \label{tab:takeways}
\end{table}



\subsection{Model-related Factors}
\label{subsec:modelfactors}

\paragraph{Update model or not.} 
As shown in Figure~\ref{fig:two_paradigms}~(b), to drive PLMs to understand and follow task instructions more smoothly, a widely-adopted practice is fine-tuning PLMs on multi-task datasets, where each task input is equipped with a task instruction. This procedure is also well-known as ``instruction tuning''. A lot of works demonstrated that instruction-tuned PLMs could better follow the instructions of unseen tasks compared with frozen PLMs~\cite{wei2021finetuned,sanh2021multitask}.


Besides the performance gains on unseen tasks, instruction tuning has many other benefits, such as learning faster on the downstream tasks~\cite{longpre2023flan,gupta2023instruction}, being more robust to the tiny instruction perturbations (e.g., paraphrasing)~\cite{weller2020learning,sanh2021multitask,gu2022robustness}, becoming more user-friendly~\cite{chung2022scaling}, and being better at following soft instructions~\cite{wei2021finetuned}, etc. 

% For example, \citet{wei2021finetuned} showed that instruction-tuned PLMs performed better on following the soft instructions. Meanwhile, some works discovered that instruction-tuned PLMs could learn faster on the downstream tasks than vanilla PLMs~\cite{longpre2023flan,gupta2023instruction}. What's more, instruction tuning also makes PLMs robust to some tiny perturbations in the instructions, such as the wordings~\cite{sanh2021multitask} and the paraphrasing~\cite{gu2022robustness}. While off-the-shelf PLMs are usually sensitive to the small instruction perturbations~\cite{efrat2020turking,weller2020learning}, thus they require laborious prompt engineering~\cite{bach2022promptsource}. All in all, instruction tuning tames the PLMs to become much more user-friendly~\cite{chung2022scaling}.
% compared the convergences of Flan-T5 and T5 on single-task fine-tuning, and they found that instruction fine-tuned Flan-T5 learned faster than T5 on the downstream single-task fine-tuning

% Despite these attractive attributes, instruction tuning still relies heavily on massive task-specific training, which is costly and could be sub-optimal for instruction learning. We will further discuss this point in~\cref{subsec:explicit_objective}.


% \begin{figure}[t!]
% \centering
% \includegraphics[width=0.8\columnwidth, trim=10 25 0 0]{picture/Flan-PaLM.pdf}
% \caption{.}
% \label{fig:scale_a}
% \end{figure}

% \begin{figure}[t!]
% \centering
% \includegraphics[width=0.8\columnwidth, trim=10 25 0 0]{picture/Flan-T5.pdf}
% \caption{.}
% \label{fig:scale_b}
% \end{figure}

\paragraph{Model scale.}
Recent works demonstrated that the model scale significantly impacts the generalization performance of instruction learning~\citeiter{chung2022scaling,longpre2023flan,wang2023far}. As shown in Figure~\ref{fig:scaling}, the generalization performances of each model consistently increase when scaling up the model size. More interestingly, when the model scale is large enough, even vanilla PLMs can significantly outperform smaller PLMs tuned on extensive tasks (see Flan-PaLM; vanilla 540B $>$ 8B + 1836 tasks), which probably implies that the benefits of scaling up the model size can outweigh dataset scaling.

However, the super-large model scale is usually unaffordable for most research groups, and it also leads to huge carbon emissions, making it unrealistic in most real-world scenarios~\cite{strubell-etal-2019-energy,schick-schutze-2021-just}.
%
Accordingly, recent works began to investigate a more efficient way to address the model scale problem, e.g., the parameter-efficient fine-tuning~\cite{hu2021lora,liu2022few,lialin2023scaling,jang2023exploring}.
%
% For example, \citet{jang2023exploring} only fine-tuned partial parameters of the whole LMs (instruction-level experts), which outperformed the full-model-tuning on the unseen tasks, with less compute cost.





\subsection{Instruction-related Factors}
\label{subsec:instruct_facor}


\paragraph{Instruction consistency.} 
This factor considers the \textit{instructions across the training tasks and test tasks}. Keeping the instruction paradigm (e.g., abstractiveness) consistent is  crucial in instruction learning. \citet{wei2021finetuned} first investigated the performance impact of changing the instruction paradigm. They found that  PLMs tuned on short instructions (i.e., task names) cannot generalize to  longer sentence-style instructions (\texttt{short $\not\Rightarrow$ long}). Similarly, \citet{gu2022robustness} observed the performance dropping when changing paragraph-style instructions to shorter sentence-style instructions at the test phase (\texttt{long $\not\Rightarrow$ short}), further indicating the importance of instruction consistency. 
% during training and testing.


Besides discrete instruction, maintaining the instruction paradigm is also critical for soft instruction, i.e., keeping the same-size prefix embedding when testing on unseen tasks~\cite{xu2022zeroprompt}. 
% For example, \citet{xu2022zeroprompt} showed that the LMs fine-tuned with continuous instruction also require a \textit{same-size} prefix embedding when testing on unseen tasks, even if this embedding is randomly initialized.
% 
Interestingly, similar results were also found in the few-shot demonstrations (i.e., in-context learning), where the combination of input-output pairs or the number of demonstrations cannot be changed during training and evaluation~\cite{min-etal-2022-metaicl,min2022rethinking,iyer2022opt}.
% For instance, \citet{min2022rethinking} concluded that breaking the demonstration paradigm (i.e., removing the \textsc{Y}) significantly harms the performance of MetaICL~\cite{min-etal-2022-metaicl}, which tuned with (\textsc{X}, \textsc{Y}) pairs. Furthermore, \citet{iyer2022opt} found that the number of demonstrations should also not be changed during training and evaluation. 
These phenomena raise a concern: although instruction-tuned PLMs are robust to tiny perturbations of instructions, \textit{they are vulnerable when facing more significant alterations, which is far behind human-level generalization}.
% (e.g., using two demonstrations in tuning and three in testing would result in lower performance, compared with using two demonstrations in testing).
% \cite{chung2022scaling} Instruction-tuned LM is user-friendly and does not need prompt engineering or few-shot exemplars required by the conventional off-the-shelf LM.
% for in-context learning, keeping the (x,y) pair format is the most important factor, compared with removing y~\cite{min2022rethinking}. \citet{iyer2022opt} also found 
% \cite{wei2021finetuned} The experiments also illustrates that the \textbf{instruction paradigm} is really important for instruction-based tuning (my personal observation, cf. Figure 8 in page 7).

\paragraph{Instruction diversity.} To further improve the robustness of PLMs, especially when facing significant alterations of instruction paradigms, people try to promote instruction diversity during the \textit{training phase}---for the same training task, writing multiple instructions in different textual expressions (e.g., different wordings and lengths), then training PLMs on the mixture of diverse instructions. Notably, \citet{sanh2021multitask} showed that adopting instructions with diverse writing styles not only improved the model generalization but also compensated for the limited model scale to some extent.
% ~\cite{longpre2023flan}
% at the training phase also affects the cross-task performance and robustness of PLMs. 

% Notably, \citet{sanh2021multitask} fine-tuned PLMs on the multi-task datasets, where each task dataset is associated with various prompts~\cite{bach2022promptsource}. These prompts are written by different people with distinct perspectives. By varying the number of instructions per dataset used in fine-tuning, \citet{sanh2021multitask} found that the model fine-tuned with more diverse instructions achieved better and more robust performance on the unseen tasks. What's more, they also found that the instruction diversity could compensate for the limited model scale, i.e., relatively small PLMs (T0-3B) could also benefit from multi-task instruction tuning.\footnote{While \citet{wei2021finetuned} only used a fixed number of instructions and found that instruction tuning harmed the performance of smaller PLMs (Flan-8B).}

Nevertheless, manually crafting instructions with diversity is expensive and usually hard to achieve due to the human annotation bias~\cite{huynh2021survey,parmar2022don}. Owing to the excellent annotation quality of LLMs~\cite{he2023annollm,pan2023gpt4reward}, a considerable number of works began to employ models to compose innovative instructions~\cite{zhang2020analogous,zhang2021learning,honovich2022instruction}. Although the model-generated instructions have been proven to contain more noise, benefiting from the diverse syntax structures~\cite{kitaev2018constituency}, these instructions could still show complementary effects with the human-written instructions~\cite{wang2022self}. It may imply the profitability of instruction diversity, \textit{even at the expense of the correctness of instructions}. 
% diverse formats, i.e., various verb-noun structures~\cite{kitaev2018constituency} and lengths,
% \cite{wang2022self} self-generate instructions are quantitative and have good diversity.
%as mentioned in~\cite{wang2022self})

\paragraph{Add demonstrations or not.}
Demonstrations, i.e., a couple of input-output examples, have been shown to be critical for the expressiveness of task instructions. 
For example, existing works found that adding a few positive demonstrations in the textual instructions could result in a significant performance improvement on the unseen tasks~\cite{yin2022contintin,deb2022boosting}, especially for the tasks occupying complex output space~\cite{mishra2022cross,wang2022benchmarking}. Surprisingly, \citet{gu2022robustness} further found that 
models highly relied on few-shot demonstrations and even abandoned other useful resources (e.g., detailed task definition) when demonstrations were available.
% combining incorrect instructions with correct demonstrations could outperform using correct instruction without demonstrations, indicating the key role of demonstrations in instruction learning.
% 
This prominence is perhaps because the PLMs prefer to exploit the more superficial patterns of the demonstrations rather than the other complex textual expressions~\cite{min2022rethinking}. In other words, at present, a comprehensive framework for accurately encoding pure instructions in the absence of demonstrations or task scaling remains elusive.


\subsection{Model-Instruction Alignment}
\label{subsec:model_instruct_align}

This factor refers to making the procedure of instruction learning better conform to the \textit{preference} of PLMs.
% 
One aspect is the training objective. Since the current instruction learning paradigm mainly employs the PLMs as the system backbone, one of the potential explanations for why \plminstruction~(i.e., prompt) can work is that prompt aligns well with the pretraining objective---language modeling---and activates the task-specific knowledge of the PLMs. Some existing works demonstrated the importance of conforming to the pretraining objective of PLMs when doing instruction tuning~\cite{schick-schutze-2021-just,tay2022unifying}, such as recalling language modeling objectives in fine-tuning phase~\cite{iyer2022opt}. 
% For example, \citet{schick2021exploiting,schick-schutze-2021-just} proposed the idea of \textit{pattern exploit training} (PET), which used a prompt to convert the original task inputs into cloze-style questions and then fine-tuned the PLM on instruction datasets with the masked language modeling objective. They found that taking advantage of recalling the pre-training objective, relatively small LMs, such as ALBERT~\cite{lan2019albert}, can outperform GPT-3 on the SuperGLUE benchmark~\cite{wang2019superglue}. Furthermore, \citet{iyer2022opt} found that the PLM could perform better on the unseen tasks after mixing a small proportion of pretraining-style data in the instruction tuning dataset. Some works also found that the PLM was more likely to fail at the tasks whose objective differs from the language modeling but improved by adopting cloze-style instructions~\cite{sanh2021multitask,wei2021finetuned}. 
% 
Another aspect of model preference alignment is the way of designing instructions: that is, converting the instructions into model-oriented styles~\cite{deng-etal-2022-rlprompt}. For example, using soft instructions (i.e., continuous embedding) instead of human-understandable discrete instructions~\cite{lester2021power,liu2021gpt,ye2022retrieval}. It is consistent with the empirical guidelines established in the field of prompt engineering, which emphasize the significance of model-oriented prompt design.\footnote{Using prefix prompts for the auto-regressive LMs, while using cloze prompts for the masked LMs~\cite{liu2023pre}.} Despite the performance profits, it is still controversial whether it is worthwhile to convert the original human-oriented instructions into a PLM-oriented style, because it always impairs the interpretability of instructions and is highly contrary to human intuition~\cite{khashabi2022prompt,webson-pavlick-2022-prompt,prasad2022grips}.
% To better cater to the model's preference, recent works began employing continuous embedding (i.e., soft instructions) instead of human-understandable discrete instructions.
% choosing the appropriate PLM and 
% recent works also tried to design the instructions by following the model's preference
% Similar conclusions are also found in the human-oriented instructions, where the PLM constantly fails at following the human-oriented instructions but gains significant improvements after reframing the instructions to cater to the model's preference~\cite{mishra-etal-2022-reframing,prasad2022grips,gonen2022demystifying,deng-etal-2022-rlprompt,wang2022self}.  



\subsection{Data-wise Factor: Task Scale}
\label{subsec:task_scale}

The task scale often refers to the number of different training task categories in the dataset. Since ``data-wise factor'' also includes the scale of training instances, \citet{wang2022benchmarking} investigated the impact of both task and instance scales. They found that instance scale (fixed task number, increasing the number of instances per task) can only bring a limited performance boost, while task scale is the key factor for instruction tuning, in line with the observations of other works~\cite{wei2021finetuned,chung2022scaling}. As illustrated in Figure~\ref{fig:scaling}, the same-size model with more tuning tasks usually gains better performance. However, the performance improvement of scaling up tasks is unstable, especially when the model size is too small (e.g., 0.08B Flan-T5). This phenomenon aligns with the discussion in \cref{subsec:modelfactors}, we can draw a similar conclusion here: \textit{the profits of the task scale are highly governed by the model scale.}
% (Appendix~\ref{appendix:scaling})
% scaling up the model size should be a prerequisite for scaling up tasks.
%  must be supported by the large-scale model size.
% where we concluded that the benefits of the model scale potentially outweigh the task scale. Here,
% \citet{yin2022contintin} conducted continual learning with task instructions and concluded that the more training tasks, the better performance on test tasks.
% \citet{chung2022scaling} conducted extensive experiments with 1,836 tuning tasks and 540B models. The results illustrated that the cross-task performance takes advantage of both the task scale and the model scale.
% \footnote{Worth noting that the benefits of the model scale seem to outweigh the task scale~\cite{longpre2023flan}.}


\subsection{Main Takeaway: Dual-Track Scaling}

Among all the factors discussed in this section, scaling is arguably the core factor that leads to the success of instruction learning. Prior to instruction learning, scaling was mainly for deep learning models: from single-layer neural nets to multi-layer perceptions, from convolutional/recurrent neural networks to deep-layer transformers~\cite{hochreiter1997long,lecun1998gradient,vaswani2017attention,devlin2018bert}. Along with the pretraining of massive raw text data, the ever-increasing models are expected to have encoded a vast amount of generic-purpose knowledge~\cite{zhou2023lima}. In the era of instruction learning, where the community is more interested in cross-task generalization, merely scaling PLMs seems not enough. Thus, researchers take a parallel scaling: to collect more and more training tasks and labeled examples for each. We interpret this as a \texttt{dual-track scaling}. Overall, this dual-track scaling jointly seeks supervision to solve new tasks---the supervision either comes from PLMs' pretraining or substantial training tasks. Despite its progress, some notable challenges remain in this area, which we will discuss in the next section.

%
% As for extending the task scale, a feasible solution is data augmentation, e.g., \citet{longpre2023flan} adapted the idea of ``noisy channel''~\cite{min2022noisy} that extended the tuning task scale by simply inverting the input-output of instance and achieved a presentable performance improvement. Besides, to alleviate the expensive human labor in creating diverse training tasks, a bunch of recent works also tried to employ LLMs to scale up the tasks of instruction-tuning datasets~\cite{wang2022self,alpaca,peng2023gpt4llm}.




\section{Challenges and Future Directions}
\label{sec:challenges}


\paragraph{Learning negated information.} Negation is the common linguistic property and has been found to be crucial for various NLP tasks, e.g.,  NLI \cite{naik2018stress,kassner2020negated}. Specific to instruction learning, negation denotes any \textit{things-to-avoid} information of in-context instructions, including negated requirements (e.g., ``\texttt{avoid using stop words}'') and negative demonstrations (i.e., some wrong examples). Although humans can learn a lot from the negation~\cite{dudschig2018does}, existing works found PLMs often fail to follow the negated instructions; some negations can even drop models' performance~\cite{li2022maqa,jang2022can,mishra-etal-2022-reframing}.   
% For example, conducted error analyses on GPT-3 and found GPT-3 constantly unable to understand the negated task constraints in the MTurk instructions. \citet{wang2022benchmarking} further found that adding negative demonstrations and explanations to the instructions could even harm the cross-task generalization performance of the PLMs. 

Since negation has increasingly become a challenge in instruction learning, we provide several hints to inspire future work. One potential solution is unlikelihood training~\cite{hosseini2021understanding,ye2022guess}, which trains the PLMs to minimize the ground truth probability when negated instructions are conditioned. Besides, \citet{yin2022contintin} proposed pretraining the LMs on the negative demonstrations with maximizing likelihood objective to exploit the useful information in the negation. Some other methods, such as contrast-consistent projection~\cite{burns2022discovering} and n-gram representations~\cite{sun-lu-2022-implicit}, also provided insights into tackling this problem.

\paragraph{Explainability of instruction learning.}

As we have mentioned in~\cref{sec:analysis}, to achieve a promising cross-task performance, one of the critical factors is to convert the \humaninstruction~into \plminstruction, i.e., making the instructions conform to the model's preference.
% one of the critical factors in achieving promising cross-task performance
Numerous previous works have verified the effectiveness of catering to the model's preference in designing instructions, e.g., using the model's perplexity in choosing appropriate instructions~\cite{gonen2022demystifying}.
%  the prediction entropy or  
Despite the performance gains, the resulting instructions consistently violate human intuitions and show worrying reliability, such as some semantically incoherent, task-irrelevant, or even misleading instructions~\cite{khashabi2022prompt,prasad2022grips}.
% and become far more difficult for humans to understand
% For example, \citet{prasad2022grips} tried to rephrase the \humaninstruction~by using performance rewards as the criterion. Surprisingly, the resulting instructions that yield better performance are constantly semantically incoherent, task-irrelevant, or even misleading instructions. Similar results are also found in~\cite{khashabi2022prompt}, which mapped the continuous instructions back into the discrete space and found those effective instructions are usually associated with semantic-irrelevant utterances. 
\textit{These results prove the conflict between performance profits and the human interpretability of instructions, which is tricky to trade-off.}

Although \citet{mishra-etal-2022-reframing} demonstrated that it is possible to maintain both the faithfulness and effectiveness of instructions, manual rewriting requires laborious human efforts. Therefore, one of the future trends is to investigate how to automatically rephrase the instructions, in a way that matches both human and model preferences.
% such as setting an additional criterion during the instruction optimization.




\paragraph{Learning to follow instruction rather than merely generating $\textsc{Y}$.}


% As we have discussed in~\cref{subsec:modelfactors},
Multi-task instruction tuning is becoming a fundamental practice in the current instruction learning paradigm. However, there are two issues in such a learning paradigm: (i) it relies on training on massive labeled examples to learn the instructions, which is still expensive and unrealistic for using large-scale PLMs; (ii) although the ultimate goal of instruction tuning is learning to follow instructions by observing various training tasks, the current training objective is still the conventional maximum likelihood of reference outputs. This implicit instruction learning objective can lead to sub-optimal optimization (i.e., PLMs can  learn to generate $\textsc{Y}$ for $\textsc{X}$ without really understanding the meaning of instructions $\textsc{I}$).

To this end, one desired future direction is to evolve a new learning objective to help PLMs explicitly learn to follow instructions, which might alleviate the reliance on large-scale labeled instances. 
% ~\citep[cf.][]{tay2022unifying}
Moreover, a more ambitious and challenging idea is to drive the system to follow instructions without additional tuning on the labeled examples of any specific tasks~\cite{ye2023context}, which is somehow similar to semantic parser-based paradigm (\cref{sec:modeling}).
%
% Recent work on in-context instruction learning can be considered an initial step toward this goal~\cite{ye2023context}. However, it is still built from an in-context learning perspective.
% ~\footnote{In-context instruction learning provides PLMs with fixed instruction-learning demonstrations. Although it doesn't require training, the lengthy and complex input makes the inference more costly and requires large-scale models.}.



% \subsection{Scalable Oversight and Alignment}
% \paragraph{Scalable oversight: a new evaluation paradigm for generalist AI systems.} The evaluation procedure of the current research paradigm follows two steps: First, driving the systems to complete specific tasks. Second, using some automatic metrics to evaluate the systems.
% %
% While in the context of evaluating advanced instruction learning systems (i.e., generalist language models), this traditional paradigm suffers from two issues: (i) the automatic metrics are usually insufficient to measure the progress of the system, especially when the system has already been more capable than non-expert humans on those well-known tasks; (ii) we have no idea how good it is for the system to assist non-expert humans in dealing with various daily tasks. 

% Accordingly, recent works proposed the idea of \textit{scalable oversight}~\cite{cotra2021case,bowman2022measuring}, which denoted a new research paradigm for appraising the generalist language models. It includes the following steps: (i) \textit{Task Choices}. Choosing the tasks where the PLMs can outperform the non-experts but underperform the experts; (ii) \textit{Non-experts Annotation}. Instead of driving the model to complete the tasks, ask the non-experts to annotate the challenging tasks with assistance from LMs, i.e., the LMs need to follow some general instructions of non-experts to help solve the tasks. This kind of procedure simulates the real-world scenarios of non-experts in using LMs; (iii) \textit{Experts Evaluation}. At the end of the experiments, ask the experts to evaluate the annotation correctness of non-experts. In doing so, we can continue to promote the progress of generalist LMs by aligning the highly capable LMs with domain experts. Meanwhile, we simulate a real-world application scenery for most non-expert users, where the generalist LMs play the role of an assistant w/o any domain-specific knowledge aided.
% %
% By adopting this paradigm, \citet{bowman2022measuring} found that the non-experts can outperform both LMs-alone or human-alone results by benefiting from the assistance of LMs.

% Overall, the scalable oversight paradigm can help future research to test whether current PLMs (e.g., ChatGPT) can effectively assist non-expert users in solving challenging tasks.


\section{Conclusion}

% In this survey, we comprehensively summarize numerous existing pieces of literature about instruction learning and provide a systematic overview of this field, including different instruction taxonomies, modeling strategies, and some crucial aspects of using instructions at a high level. We also emphasize some distinct challenges and the corresponding hints for future research. Unlike most existing works, we consider textual instruction as a source of indirect supervision for the LLMs. To our knowledge, this is the first extensive survey about instruction learning. In summary, we hope this survey can offer insights and inspiration for further in-depth research on instruction learning. 

This survey summarizes the existing literature on instruction learning, providing a comprehensive overview of the field, including instruction taxonomies, modeling strategies, and key aspects of instruction utilization. It also addresses unique challenges and offers hints for future research. Unlike previous works, we go beyond the limited scope of modern instruction tuning---we trace the studies of instruction learning back to the early stage of machine learning, and explore textual instruction as an indirect supervision for LLMs. To our knowledge, this is the first extensive survey on instruction learning. Overall, we aim to offer valuable insights and inspire further in-depth research in this area.

\section*{Limitations}
We discuss the limitations of this work as follows:
\begin{itemize}
    \item We provide a comprehensive exploration of the subject of instruction learning by concluding three types of instructions and explaining their characteristics. However, this taxonomy is skewed by the broad definition of ``instruction''. To the best of our knowledge, any signal that communicates between humans and machines can be considered ``instructions'', namely interacting human-described actions, e.g., how to use mobile apps~\cite{li2020interactive}, browsing the Internet~\cite{deng2023mind2web}, or some multi-modal related instructions~\cite{xu2022multiinstruct,li2023m3,Zhang2023MagicBrush}. Therefore, our scope focuses only on the NLP-specific ``textual instructions''.
    
    \item Besides, our study lays particular emphasis on the ``task instruction'', which is mainly used as an indirect supervision for cross-task generalization; while the ``task'' here denotes the conventional input-by-output language processing task. As a result, we don't spend too much time on those input-free ``daily-task instructions'' (e.g., ``\texttt{help me plan a 3-day travel to Hawaii}''), which is recently a popular topic in the community~\cite{wang2022self,alpaca,wu2023lamini}. However, from our insight view, these daily-task instructions are essentially special cases of human-oriented instructions (introduced in Section~\ref{sec:categories}) but crafted for daily-life scenarios.
\end{itemize}

% \section*{Ethical Statement}
% This paper surveys the existing literature on instruction learning and large language models; all of our discussions target the future development of NLP techniques, for a safer and more human-centered purpose. Therefore, we do not anticipate any ethical issues, particularly the topics of this research.


\bibliography{anthology}
\bibliographystyle{acl_natbib}




% ================ Appendix ======================
\appendix
\clearpage
\section*{Appendices}

\setcounter{table}{0}
\renewcommand\thetable{\Alph{section}.\arabic{table}}
% \renewcommand\thetable{\Alph{table}}
\setcounter{figure}{0}
\renewcommand\thefigure{\Alph{section}.\arabic{figure}}

Within this supplementary material, we elaborate on the following aspects:
\begin{itemize}
% \item Appendix \ref{appendix:related}: Related Work
% \item Appendix \ref{appendix:example_table}: Examples of Three Instruction Types
\item Appendix \ref{appendix:scaling}: Scaling Trends of Instruction Learning
\item Appendix \ref{sec:app}: Instruction-related Applications
\end{itemize}

% \section{Related Work}
% \label{appendix:related}

% Due to the limited space, we put this section in Appendix. However, in the final version of this paper, we will move this section back to the main body to present a more coherent story. There are basically two topics that highly relate to this paper, namely \textit{instruction tuning} (\ref{subsec:instruction_tuning}) and \textit{Surveys on In-context Instructions} (\ref{subsec:related_survey}).







% \subsection{Surveys on In-context Instructions}
% \label{subsec:related_survey}

% Several existing works share similar motivations with us that also survey instruction learning~\cite{dong2022survey,huang2022towards,qiao2022reasoning,yu2023nature}. However, they only focus on some sub-areas of in-context instruction (prompt, few-shot demonstrations, Chain-of-Thought reasoning, etc.). For example, \citet{liu2023pre} provides a comprehensive overview of prompt learning and LLMs, where the prompt can be regarded as one specific type of textual instruction (as categorized in~\cref{sec:categories}). Some other studies surveying ``soft instruction'', namely parameter-efficient fine-tuning methods~\cite{lialin2023scaling}, also differ from our scope of ``textual instruction''. To the best of our knowledge, this is the first work that provides a comprehensive and high-level story of textual instruction learning.
% categorized by us (discussed in~\cref{sec:categories})


% \section{Examples of Three Instruction Types}
% \label{appendix:example_table}

% To better present the differences between the three types of instructions mentioned in \cref{sec:categories}, we choose some representative task examples for each instruction type in Table~\ref{tab:nlptonli},~\ref{tab:PLM-orentied},~\ref{tab:human-orentied}.




% \begin{table*}[t]
%     \centering
%     % \begin{tabular}{c|l|l}
%     \scriptsize
%     \resizebox{0.99\textwidth}{!}{
%     \begin{tabular}{p{0.12\linewidth}||p{0.36\linewidth}|p{0.44\linewidth}}
%     % \rowcolor{blue!15} , fg=, font=\sffamily
%     \rowcolor{azure6}
%     \textcolor{white}{\textsf{Task}} & 
%     \textcolor{white}{\textsf{\textualentailment~premise (i.e., input text)}} &
%     \textcolor{white}{\textsf{\textualentailment~hypothesis (i.e., instructions \textsc{Y})}} 
%     \\\hline
    
%        \rowcolor{gray!15} 
%        \textit{Entity \newline Typing} & [Donald  Trump]$_{ent}$ served as the 45th president of the United States from 2017 to 2021. & (\textcolor{blue}{\checkmark}) Donald  Trump is a \textbf{politician} \newline (\textcolor{red}{\XSolidBrush}) Donald  Trump is a \textbf{journalist}\\\hline

%         \textit{Entity \newline Relation}  & [Donald  Trump]$_{ent1}$ served as the 45th president of the [United States]$_{ent2}$ from 2017 to 2021. & (\textcolor{blue}{\checkmark}) Donald  Trump  \textbf{is citizen of} United States \newline (\textcolor{red}{\XSolidBrush}) Donald  Trump  \textbf{is the CEO of} United States \\\hline   

%         \rowcolor{gray!15} 
%         \textit{Event \newline Argument \newline Extraction} & In [1997]$_{time}$, the [company]$_{sub}$ [hired]$_{trigger}$ [John D. Idol]$_{obj}$ to take over Bill Thang as the new chief executive. & (\textcolor{blue}{\checkmark}) \textbf{John D. Idol}   was hired. \newline (\textcolor{blue}{\checkmark}) \textbf{John D. Idol}   was hired in 1997. \newline (\textcolor{red}{\XSolidBrush}) \textbf{Bill Thang}  was hired. \\\hline
%         % (\textcolor{red}{\XSolidBrush}) Bill Thang hired \textbf{John D. Idol}. \\\hline

%         \textit{\enspace\newline  Event \newline Relation} &  Salesforce  and Slack Technologies  have [entered]$_{event1}$ into a definitive agreement] under which Salesforce will [acquire]$_{event2}$ Slack. & (\textcolor{blue}{\checkmark}) Salesforce acquires Slack \textbf{after} it enters into the agreement with Slack Tech. \newline (\textcolor{red}{\XSolidBrush}) Salesforce acquires Slack \textbf{because} it enters into the agreement with Slack Tech. \\\hline   

%         \rowcolor{gray!15} 
%         \textit{Stance \newline Detection} & Last Tuesday, Bill said ``animals are equal to human beings'' in his speech. & (\textcolor{blue}{\checkmark}) Bill \textbf{supports} that animals should have lawful rights. \newline (\textcolor{red}{\XSolidBrush}) Bill \textbf{opposes} that animals should have lawful rights.\\\hline
%     \end{tabular}
%     }
%     \vspace{-1em}
%     \caption{Entailment-oriented instructions construct hypotheses to explain the labels (in bold). ``\textcolor{blue}{\checkmark}'':  correct; ``\textcolor{red}{\XSolidBrush}'': incorrect.}
%     \label{tab:nlptonli}
% \end{table*}


% \begin{table*}[t]
%     \centering
%     % \begin{tabular}{c|l|l}
%     \scriptsize
%     \resizebox{0.98\textwidth}{!}{
%     \begin{tabular}{p{0.09\linewidth}|p{0.35\linewidth}|p{0.20\linewidth}|p{0.09\linewidth}|p{0.15\linewidth}}
%     Task & Input \textsc{X} & Template \textsc{T} (cloze question) & Answer & Output \textsc{Y} \\\hline


%     Sentiment \newline Classification & I would like to buy it again. & [\textsc{X}] The product is \blank & Great \newline Wonderful \newline $\ldots$ & Positive \\\hline

%     Entity \newline Tagging & [\textsc{X1}]: Donald Trump served as the 45th president of the United States from 2017 to 2021. \newline [\textsc{X2}]: Donald Trump & [\textsc{X1}] [\textsc{X2}] is a \blank entity? & Politician \newline President \newline $\ldots$ & People \\\hline

%     Relation \newline Tagging & [\textsc{X1}]: Donald Trump served as the 45th president of the United States from 2017 to 2021. \newline [\textsc{X2}]: Donald Trump \newline [\textsc{X3}]: United States & [\textsc{X1}] [\textsc{X2}] is the \blank of [\textsc{X3}]? &  President \newline Leader \newline $\ldots$ & The\_president\_of \\\hline

%     Textual \newline Entailment & [\textsc{X1}]: Donald Trump served as the 45th president of the United States from 2017 to 2021. \newline [\textsc{X2}]: Donald Trump is a citizen of United States. & [\textsc{X2}]? \blank, because [X1] & Indeed \newline Sure \newline $\ldots$ & Yes \\\hline

%     Translation & Donald Trump served as the 45th president of the United States from 2017 to 2021. & Translate [\textsc{X}] to French:~\blank &  / &  $\ldots$~été président des États-Unis~$\ldots$ \\\hline
%     % full translation: Donald Trump a été président des États-Unis de la 45ème législature de 2017 à 2021
    
%     \end{tabular}
%     }
%     \vspace{-1em}
%     \caption{PLM-oriented instruction utilizes a task-specific template to convert the origin input into a fill-in-blank question. In most classification tasks, the intermediate answer should be further mapped into the predefined label.}
%     \label{tab:PLM-orentied}
% \end{table*}




% \begin{tikzfadingfrompicture}[name=tikz]
% \node [text=transparent!20]
% {\fontfamily{ptm}\fontsize{12}{12}\bfseries\selectfont Ti\emph{k}Z};
% \end{tikzfadingfrompicture}

% \begin{tikzpicture}
% \shade[path fading=tikz,fit fading=false,
% left color=blue,right color=black]
% (-2,-1) rectangle (2,1);
% \end{tikzpicture}


% \begin{table*}[t]
%     \centering
%     % \begin{tabular}{c|l|l}
%     \scriptsize
%     \resizebox{0.98\textwidth}{!}{
%     \begin{tabular}{p{0.10\linewidth}|p{0.15\linewidth}|p{0.30\linewidth}|p{0.27\linewidth}|p{0.12\linewidth}}





\section{Scaling Trends of Instruction Learning}
\label{appendix:scaling}

We plot the scaling trends of instruction tuning (as discussed in~\cref{sec:analysis}) in Figure~\ref{fig:scaling}.

\begin{figure*}[ht!]
  \centering
  % \subfloat[Flan-T5]{\includegraphics[width=0.45\textwidth]{ picture/Flan-T5.pdf}}
  \subfloat[Flan-T5]{\includegraphics[width=0.45\textwidth]{ picture/Flan-T5-legend-L.pdf}}
  \hfill
  % \subfloat[Flan-PaLM]{\includegraphics[width=0.45\textwidth]{ picture/Flan-PaLM.pdf}}
  \subfloat[Flan-PaLM]{\includegraphics[width=0.45\textwidth]{ picture/Flan-PaLM-legend-L.pdf}}
  \caption{The scaling trends of instruction tuning, including scaling model size and task numbers. We report the cross-task generalization performances of two widely-adopted instruction-tuned PLMs, namely Flan-T5 and Flan-PaLM~\cite{chung2022scaling}, where the source scores mainly come from~\cite{wei2021finetuned,chung2022scaling,longpre2023flan}. It is worth noting that different papers may utilize distinct evaluation benchmarks with various metrics. To clearly summarize the scaling trends, instead of simply copying the original scores, we report the \textit{normalized performances} in each figure (that's why the highest performance of each figure can reach 100\%).}
  \label{fig:scaling}
\end{figure*}






\section{Instruction-related Applications}
\label{sec:app}

In addition to the main body of our paper, we also survey some popular instruction-related application directions and put this section in the appendix to save space. If the final space allows, we will try to compress these discussions into the main body.

\subsection{Human-Computer Interaction}
\label{subsec:HCI}

Textual instructions can be naturally regarded as a human-computer interaction method. Numerous previous works employed natural language instructions to guide the computer to perform various real-world tasks. 
%

For the non-NLP (multi-modal) tasks, most focused on environment-grounded language learning, i.e., driving the agent to associate natural language instructions with the environments and make corresponding reactions, such as selecting mentioned objects from an image/video~\cite{matuszek2012joint,krishnamurthy2013jointly,puig2018virtualhome}, following navigational instructions to move the agent~\cite{tellex2011approaching,kim2012unsupervised,chen2012fast,artzi2013weakly,bisk2016natural}, plotting corresponding traces on a map~\cite{vogel2010learning,chen2011learning}, playing soccer/card games based on given rules~\cite{kuhlmann2004guiding,eisenstein2009reading,branavan2011learning,babecs2012learning,goldwasser2014learning}, generating real-time sports broadcast~\cite{chen2008learning,liang2009learning}, controlling software~\cite{branavan2010reading}, and querying external databases~\cite{clarke2010driving}, etc.
% generate sportscasting~\cite{}, 
Meanwhile, instructions are also widely adapted to help communicate with the system in solving NLP tasks, e.g., following instructions to manipulate strings~\cite{gaddy2019pre}, classifying emails based on the given explanations~\cite{srivastava2017joint,srivastava2018zero}, and text-to-code generation~\cite{acquaviva2021communicating}.

Recently, a growing body of research tended to design the human-computer communication procedure in an \textit{iterative} and \textit{modular} manner~\cite{dwivedi2022editeval,chakrabarty2022help}. For example, \citet{li2020interactive} built a system to help the users tackle daily missions (e.g., ordering coffee or requesting Uber). Benefiting from a user-friendly graphical interface, the system can iteratively ask questions about the tasks, and users can continually refine their instructions to avoid unclear descriptions or vague concepts. 
% Similarly, \citet{dwivedi2022editeval} proposed a benchmark to iteratively instruct the PLM to improve the text, where each iteration only used a small piece of instruction with a precise purpose (e.g., ``\textit{Simplify the text}'' or ``\textit{Make the text neutral}''). Besides, \citet{chakrabarty2022help} constructed a collaborative poem-writing system, where the user could initially provide an ambiguous instruction (e.g., ``\textit{Write a poem about cake}'') and then incrementally refine the instruction with more details (e.g., ``\textit{Contain the word -- `chocolate'} '') by observing the model's intermediate outputs. Meanwhile, \citet{mishra2022help} proposed a biography generation system\footnote{\citet{mishra2022help} actually experimented with more than 60 text generation tasks.} that progressively collected the necessary personal information from the users (by asking questions in a dialogue scene to guide the users) and generated a paragraph-style bio finally.
%
As it is usually hard for non-expert users to write sufficient instructions in one shot, adapting an iterative and modular paradigm in designing instruction-based AI systems can help guide the users to enrich the task instruction step by step. Thus, this paradigm efficiently relieves the thinking demands of users and leads to a more user-oriented system~\cite{mishra2022help}. Due to its practical values, we emphasize the importance of this branch of work in this paper.




\subsection{Data and Feature Augmentation}

Task instructions are regarded as indirect supervision resources where sometimes superficial and assertive rules are embedded. These rules are also known as \textit{labeling functions} that can be directly applied for annotations.\footnote{For example, if ``a very fair price'' is sentiment-positive, every sentence with a similar adj-noun collocation as ``fair price'' will be positive as well.}
% This kind of instruction (usually some explanations). 
% Some task instructions (i.e., explanations) also contain superficial and assertive rules for annotations, which are usually known as ``\textit{labeling functions}''.
Therefore, some existing works also employed the instruction as a distant supervision to perform data or feature augmentation~\cite{srivastava2018zero,hancock2018training,ye2020teaching}.
% , which can be used as the labeling function to annotate corpus to used as data augmentation automatically.
For instance, \citet{srivastava2017joint} used a semantic parser to convert natural language explanations into logical forms, and applied them on all instances in the dataset to generate additional binary features. While \citet{wang2020learning} utilized the label explanations to annotate the raw corpus automatically and trained the classifier on the resulting noisy data. 

Besides the straightforward augmentation, \citet{su2022one} further used the task instruction to enrich the model representation and achieved strong cross-task generalization. Specifically, they trained an embedding model (a single encoder) on the diverse instruction datasets with contrastive learning, and then used this model to produce task-specific representations based on the instruction for the downstream unseen tasks.
% extended this kind of feature augmentation to a cross-task generalization scene

% All of them achieved admirable results on various NLP tasks.
% \citet{hancock2018training,wang2020learning,ye2020teaching} Authors use semantic parser to convert the labeling explanation of each sample in the training set into formal language (labeling function), and then annotate the raw corpus automatically by using the labeling functions, thus generating some noisy data to train the classifier (thus it is a semi-supervision strategy); Similarly, \citet{srivastava2017joint} converts explanations to logical forms, and then use them to create binary features to enhance the original inputs. - What instruction: natural language \textbf{explanations} (labeling function, explain the reason of annotation), e.g., ``Positive, because the words "very nice" is within 3 words after the TERM''. - What method: Annotate the explanations on the original CLS datasets, use these human-written instructions as the rule of distance supervision, to generate noisy samples (data argument). - What task (input,output): Mostly CLS tasks. Use explanation as an additional supervision.

\subsection{Generalist Language Models}

% Another high-profile application of instruction learning is to build generalist language models.
% \cite{su2022one} use instructions to build a single embedding model to construct a task-level semantic space, which can be used for various downstream tasks.
According to the definition of Artificial General Intelligence (AGI), the ``generalist model'' is usually a system that can be competent for different tasks and scalable in changeable contexts, which shall go far beyond the initial anticipations of its creators~\cite{wang2007introduction,goertzel2014artificial}.
%
While specific to the NLP domain, a generalist language model is supposed to be an excellent multi-task assistant, that is skilled in handling a variety of real-world NLP tasks and different languages, in a completely zero/few-shot manner~\cite{arivazhagan2019massively,pratap2020massively,wei2021finetuned}.
%
As numerous existing works demonstrated the incredible power of using instructions in cross-task generalization~\citeiter{wei2021finetuned,sanh2021multitask,mishra2022cross,wang2022benchmarking,chung2022scaling}, the instruction is likely to become a breakthrough in achieving this ultimate goal. 

Notably, the recent remarkable applications of instructions, namely InstructGPT, ChatGPT, and GPT-4~\cite{ouyang2022training,openai2022chatgpt,OpenAI2023GPT4TR}, also indicated a big step towards building generalist language models. However, unlike the other works that mainly employ instruction learning, ChatGPT also adopts some other components, e.g., reinforcement learning with human feedback (RLHF).
% ~\footnote{At the time of writing, there is no published paper about ChatGPT. Thus, our discussion is mainly based on the underlying techniques of InstructGPT because they share similar philosophies. See OpenAI's blog for more details: \url{https://openai.com/blog/chatgpt}}
Although the answer to ``\textit{which component contributes more to the dramatic results of ChatGPT}'' remains ambiguous and needs further investigation, we introduce some recent works highlighting the critical role of instruction learning.
%
For example, \citet{chung2022scaling} conducted extensive experiments to evaluate the human-preference alignments of PaLM~\cite{chowdhery2022palm}. They found that, even without any human feedback, the instruction tuning significantly reduced the toxicity in the open-ended generations of PaLM, such as gender and occupation bias. In addition, some other works also solely employed creative instructions instead of human feedback and achieved notable cross-task results~\cite{bai2022constitutional,honovich2022unnatural,wang2022self}.
% 

% Although ChatGPT still suffers from many unsatisfactory aspects and is far from the generalist language model~\cite{qin2023chatgpt,guo2023close,Kocon2023ChatGPTJO,wang2023robustness}, we hope the goal of AGI can continue to be promoted by adopting and evolving more powerful techniques, including instruction learning.



\end{document}
