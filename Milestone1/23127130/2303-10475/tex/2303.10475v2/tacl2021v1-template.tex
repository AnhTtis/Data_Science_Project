% File tacl2021v1.tex
% Dec. 15, 2021

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}

%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2021v1}
%% Most compact command to produce a "camera-ready" version
%%    \usepackage[acceptedWithA]{tacl2021v1}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2021v1}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

% \usepackage{tacl2021v1}
\usepackage[acceptedWithA]{tacl2021v1} % cancel anonymous
% \setlength\titlebox{10cm} % <- for Option 2 below

\usepackage{ulem}
\usepackage{cleveref}
\crefname{section}{§}{§§}
\usepackage{CJKutf8}

\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{subfigure} 

%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Dec. 15, 2021}
\newcommand{\styleFileVersion}{tacl2021v1}
\newcommand{\ex}[1]{{\sf #1}}

\newcommand{\citeiter}{\citep[][\textit{inter alia}]}
\newcommand{\blank}{\underline{\hspace{0.9em}}}
\newcommand{\linenotation}{\begin{verbatim} \n \end{verbatim}}

\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi

%%%% End TACL-instructions-specific macro block
%%%%

% \title{A Survey on Textual Instruction Learning: \\
%        Evolving Language Models to Generalist Cross-Task Assistants}

% \title{Learning from Task Instructions: \\
% What Happened, What is Happening, and Where Should We Go}

\usepackage{dingbat}
\usepackage{bbding}
\newcommand{\textualentailment}{\textsc{TE}} 

\title{Is Prompt All You Need? No. \\
A Comprehensive and Broader View of Instruction Learning}
% \title{Formatting Instructions for TACL \TaclPapers \\
% (Base files: \styleFileVersion-template.tex \& \styleFileVersion.sty, dated \dateOfLastUpdate)}

% Author information does not appear in the pdf unless the "acceptedWithA" option is given

% The author block may be formatted in one of two ways:

% Option 1. Author’s address is underneath each name, centered.

% \author{
%   Template Author1\Thanks{The {\em actual} contributors to this instruction
%     document and corresponding template file are given in Section
%     \ref{sec:contributors}.} 
%   \\
%   Template Affiliation1/Address Line 1
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   \texttt{template.email1example.com}
%   \And
%   Template Author2 
%   \\
%   Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
% }

% % Option 2.  Author’s address is linked with superscript
% % characters to its name, author names are grouped, centered.

% \author{
%   Template Author1\Thanks{The {\em actual} contributors to this instruction
%     document and corresponding template file are given in Section
%     \ref{sec:contributors}.}$^\diamond$ 
%   \and
%   Template Author2$^\dagger$
%   \\
%   \ \\
%   $^\diamond$Template Affiliation1/Address Line 1
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   \texttt{template.email1example.com}
%   \\
%   \ \\
%   \\
%   $^\dagger$Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
% }

% self-modified 
\author{
  Renze Lou$^\dagger$,
  Kai Zhang$^\diamond$
  \and
  Wenpeng Yin$^\dagger$
  \\
  $^\dagger$The Pennsylvania State University;
  \ 
  $^\diamond$The Ohio State University
  \\
  \texttt{\{renze.lou, wenpeng\}@psu.edu};
  \ 
  \texttt{zhang.13253@osu.edu}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  % Despite the impressive results of the fully supervised fine-tuning, the laborious task-specific human annotation and large-scale pre-trained language models (PLM) make the traditional fine-tuning procedure unaffordable. Therefore, a growing body of research resorted to a new paradigm, namely \textit{textual instruction learning}, to perform various NLP tasks. By learning to follow the task instructions, the language models (LMs) are further evolved into generalist task assistants that can perform multiple tasks in a zero/few-shot manner, indicating a big step toward the idea of Artificial General Intelligence (AGI). In this paper, we survey numerous existing pieces of literature about instruction learning. Specifically, we first introduce the systematic taxonomies of instructions, and then summarize different common-chosen modeling strategies. Next, we provide several critical factors in using instructions in real-world NLP tasks, as well as display some popular applications. Finally, we highlight a few key challenges in the current instruction-learning paradigm, along with the corresponding future trends. To our knowledge, this is the first comprehensive survey about textual instructions.

  Task semantics can be expressed by a set of input-to-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. 
  Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning from task instructions. Despite its impressive progress, there are some common issues that  the community struggles with. This survey paper tries to summarize the current research on instruction learning, particularly, by answering the following questions: (i) what is task instruction, and what  instruction types exist? (ii) how to model instructions? (iii) what factors influence and explain the instructions' performance? (iv) what challenges remain in instruction learning? To our knowledge, this is the first comprehensive survey about textual instructions.
\end{abstract}


\begin{figure}[!ht]
	\begin{center}
		\centering
		\includegraphics[width=1.02\linewidth, trim=0 55 0 0]{picture/two_paradigms_v3.pdf}
	\end{center}
	\caption{Two machine learning paradigms: (a) traditional fully supervised learning uses extensive labeled examples to represent the task semantics. It is expensive, and the resulting system hardly generalizes to new tasks; (b) instruction learning utilizes task instruction to guide the system quickly adapts to various new tasks.}
    % , to perform binary classification on each label.
    % unify conventional classification problems into an entailment-based paradigm
    % that is initially designed for human annotations
	\label{fig:two_paradigms}
\end{figure}



\begin{figure*}[!ht]
	\begin{center}
		\centering
		\includegraphics[width=0.98\linewidth, trim=0 55 0 0]{picture/taxonomiesv3.pdf}
	\end{center}
	\caption{An illustration of three distinct categories of textual instructions. (a) \textbf{Entailment-oriented}: regarding the original input as a premise and converting each predefined label into a hypothesis (i.e., instruction). (b) \textbf{PLM-oriented}: using a template to construct the original task input into a cloze question. (c) \textbf{Human-oriented}: utilizing sufficient task information as instruction, such as definitions and optional few-shot demonstrations, etc.}
    % , to perform binary classification on each label.
    % unify conventional classification problems into an entailment-based paradigm
    % that is initially designed for human annotations
	\label{fig:overview}
\end{figure*}


\section{Introduction}
% === background ===
% There were mainly two waves in the previous deep learning paradigms in NLP: (1). \textit{Fully supervised learning} that trains a task-specific model from the random initialization; (2). \textit{Pre-train and fine-tune} that pre-trains a general model on the large-scale corpus and then adapts it on specific downstream tasks by fine-tuning with labeled examples~\cite{radford2018improving}.

% With the help of advanced pre-training strategies, current large-scale language models (LLMs) achieve remarkable results on various natural language processing (NLP) tasks via fine-tuning on the labeled examples. This paradigm, so called \textit{fully supervised fine-tuning} (SFT), suffers from two issues: (1). Laborious human annotations in constructing task-specific corpus; (2). The resulting model is a \textbf{close-set specialist model}, which usually performs well on the seen training tasks but fails at generalizing to the unseen tasks. Therefore, how to build a \textbf{cross-task generalist model} that can handle various novel tasks became a long-term question for the research community~\cite{goertzel2014artificial}.

One goal of AI is
to build a system that can universally understand and solve new tasks. Labeled examples, as the
mainstream task representation, are unlikely to be available in large numbers or even do not exist.
Then, is there any other task representation that can contribute to task comprehension? Task instructions provide another dimension
of supervision for expressing the task semantics. Instructions often contain more abstract and
 comprehensive knowledge of the target task than individual labeled examples. As shown in Figure~\ref{fig:two_paradigms}, with the
availability of task instructions, systems can be quickly built to handle new tasks, especially when
task-specific annotations are scarce.
Instruction
Learning is inspired by the typical human learning for new tasks, e.g., a little kid can well solve a new mathematical task by learning from its instruction and a few examples~\cite{fennema1996longitudinal,carpenter1996cognitively}.
This new learning paradigm has recently attracted the main attention of the machine learning and NLP communities~\citeiter{radford2019language,efrat2020turking,brown2020language}. 

When talking about task instructions, most of us will first connect this concept with prompts--using a brief template to reformat new input into a language modeling problem so as to prime a PLM for a response~\cite{liu2023pre}. Despite prompts' prevalence in text classification, machine translation, etc., we claim that prompts are merely a special case of instructions. This paper takes a comprehensive and broader view of instruction-driven NLP research. Particularly, we try to answer the following questions:
\begin{itemize}
    \item What is task instruction, and what instruction types exist? (\cref{sec:categories})
    \item Given a task instruction, how to encode it to help the target task? (\cref{sec:modeling})
    \item What factors (e.g., model size, task numbers) impact the instruction-driven systems' performance, and how to design better instructions? (\cref{sec:analysis})
    \item What applications can instruction learning contribute? (\cref{sec:app})
    \item What challenges exist in instruction learning and what are future directions? (\cref{sec:challenges})
\end{itemize}


% === what is instruction and instruction learning (key difference with supervised learning). ===
% To this end, a novel paradigm is emerged to fill this gap, namely \textit{instruction learning}, which utilizes in-context natural language instructions to guide the LMs to perform various tasks. Unlike traditional supervised learning, where the model learns the task-specific skills by gradient-based parameter optimization, i.e., \textit{learn to complete tasks}, the essential objective of instruction learning is driving the model to \textit{learn to follow the instructions}, which provide a new way to address various NLP tasks in a completely zero/few-shot manner. As a result, instruction learning has attracted a lot of attention in recent years, which has led to diverse instructions with distinct formats. Therefore, in this paper, we aim to survey existing works about instruction learning systematically, including instruction taxonomies, modeling strategies, applications, and important aspects that impact zero-shot performance.
% critical challenges, and potential future trends.

% === a brief description of taxonomies ===
% === a brief description of modeling strategies ===
% === the advantages of instruction learning. === 



% === some challenges ===
% Although the instruction learning paradigm has been proven to be effective in handling NLP tasks, there are still several distinct problems in the current paradigm. First, the LMs could find it hard to truly grasp the semantic meaning of negated instructions; Second, the poor explainability of the high-performance instructions; Third, the reliance on the massive multi-task labeled examples due to the implicit learning objective; What's more, the problem of current automatic evaluation paradigm. To help further research address the above issues, we provide several corresponding hints by summarizing the current progress.


% === the purpose and position of this paper ===
To our knowledge, this is the first paper that surveys textual instruction learning. In contrast to some existing surveys that focused on a specific in-context instruction, such as prompts~\cite{liu2023pre}, input-by-output demonstrations~\cite{dong2022survey}, or reasoning~\cite{huang2022towards,qiao2022reasoning}, we provide a broader perspective to connect distinct researches in this area in an organized way.
% Finally, we release the corresponding paper list for the beginner and future research~\footnote{\url{https://github.com/RenzeLou/awesome-instruction-learning}}, which will be continuously updated to keep track of the latest progress in this field. 
We hope this paper can present a better story of instruction learning and attract more peers to work on this challenging AI problem.
%
% The corresponding reading list will be released in the future to help beginners.
We also release the corresponding reading list of this survey.~\footnote{\url{https://github.com/RenzeLou/awesome-instruction-learning}}


\begin{table*}[t]
    \centering
    % \begin{tabular}{c|l|l}
    \scriptsize
    \begin{tabular}{p{0.12\linewidth}|p{0.36\linewidth}|p{0.44\linewidth}}
    task & \textualentailment~premise (i.e., input text) & \textualentailment~hypothesis (i.e., instructions \textsc{Y})\\\hline
    
       Entity \newline Typing & [Donald  Trump]$_{ent}$ served as the 45th president of the United States from 2017 to 2021. & (\textcolor{blue}{\checkmark}) Donald  Trump is a \textbf{politician} \newline (\textcolor{red}{\XSolidBrush}) Donald  Trump is a \textbf{journalist}\\\hline

        Entity \newline Relation  & [Donald  Trump]$_{ent1}$ served as the 45th president of the [United States]$_{ent2}$ from 2017 to 2021. & (\textcolor{blue}{\checkmark}) Donald  Trump  \textbf{is citizen of} United States \newline (\textcolor{red}{\XSolidBrush}) Donald  Trump  \textbf{is the CEO of} United States \\\hline   
        
        Event \newline Argument \newline Extraction & In [1997]$_{time}$, the [company]$_{sub}$ [hired]$_{trigger}$ [John D. Idol]$_{obj}$ to take over Bill Thang as the new chief executive. & (\textcolor{blue}{\checkmark}) \textbf{John D. Idol}   was hired. \newline (\textcolor{blue}{\checkmark}) \textbf{John D. Idol}   was hired in 1997. \newline (\textcolor{red}{\XSolidBrush}) \textbf{Bill Thang}  was hired. \\\hline
        % (\textcolor{red}{\XSolidBrush}) Bill Thang hired \textbf{John D. Idol}. \\\hline

        \enspace\newline  Event \newline Relation &  Salesforce  and Slack Technologies  have [entered]$_{event1}$ into a definitive agreement] under which Salesforce will [acquire]$_{event2}$ Slack. & (\textcolor{blue}{\checkmark}) Salesforce acquires Slack \textbf{after} it enters into the agreement with Slack Tech. \newline (\textcolor{red}{\XSolidBrush}) Salesforce acquires Slack \textbf{because} it enters into the agreement with Slack Tech. \\\hline   
        
        Stance \newline Detection & Last Tuesday, Bill said ``animals are equal to human beings'' in his speech. & (\textcolor{blue}{\checkmark}) Bill \textbf{supports} that animals should have lawful rights. \newline (\textcolor{red}{\XSolidBrush}) Bill \textbf{opposes} that animals should have lawful rights.\\\hline
    \end{tabular}
    \vspace{-1em}
    \caption{Entailment-oriented instructions construct hypotheses to explain the labels (in bold). ``\textcolor{blue}{\checkmark}'':  correct; ``\textcolor{red}{\XSolidBrush}'': incorrect.}
    \label{tab:nlptonli}
\end{table*}



\section{Preliminary}
\label{sec:pre}
 
For task instruction learning, we aim to drive the systems to reach the output given the input by following the instructions. Thus, a dataset consists of three items:
% In other words, the final input string of LMs can be divided into several distinct components:
% the requirements mentioned in the task instructions



\textbullet\enspace \textbf{Input} (\textsc{X}): the input of an instance; it can be a single piece of text (e.g., sentiment classification) or a group of text pieces (e.g., textual entailment, question answering, etc.). 
% It is widely used in human-oriented instructions (e.g., task title, category, and definition) but is usually optional for PLM-oriented and entailment-oriented instructions.

\textbullet\enspace \textbf{Output} (\textsc{Y}): the output of an instance; in classification problems, it can be one or multiple predefined labels; in text generation tasks, it can be any open-form text. 

\textbullet\enspace \textbf{Template} ($\hat{\textsc{T}}$): a textual template that tries to express the task meaning on its own or acts as a bridge between \textsc{X} and \textsc{Y}.\footnote{A plain template connecting \textsc{X} and \textsc{Y}, e.g., ``\textit{The input is $\ldots$ The output is $\ldots$}'', no task-specific semantics.} $\hat{\textsc{T}}$ may not be an instruction yet.

In \cref{sec:categories}, we will elaborate that a task instruction \textsc{I} is actually a combination of $\hat{\textsc{T}}$ with \textsc{X} or \textsc{Y}, or the $\hat{\textsc{T}}$ on its own in some cases.




% Generally, instruction learning aims to use the template language $f(\cdot)$ to combine the task input $x$ and the task-specific information $I$. The converted result $x^{'} = f(x,I)$ is considered as the final input of LMs~\footnote{Both task-specific and task-agnostic textual information can be regarded as ``instructions''.}. In the following text, we will first summarize different instruction categories (\cref{sec:categories}); We then introduce current popular instruction modeling strategies (\cref{sec:modeling}), that is, how the model uses these different pieces of information, i.e., $x$, $I$, $f(\cdot)$; We further discuss some important factors in using instructions for cross-task generalization (\cref{sec:analysis}) and the applications of instructions (\cref{sec:app}). Finally, we emphasize several challenges in the current instruction learning paradigm and provide potential future directions accordingly (\cref{sec:challenges}).


\section{What is Task Instruction?}
\label{sec:categories}
% The Taxonomies of Textual Instructions
% \textit{What} is Instruction?

Various types of textual instructions have been  used in  previous zero- and few-shot NLP tasks, such as \uline{prompts}~\citeiter{hendrycksmeasuring,srivastava2022beyond,bach2022promptsource},
%
\uline{Amazon Mechanical Turk instructions}~\citeiter{mishra2022cross,wang2022benchmarking,yin2022contintin},
%
\uline{instructions augmented with demonstrations}\footnote{We consider the templates used in the in-context learning as the instructions rather than the few-shot demonstrations.}~\citeiter{khashabi-etal-2020-unifiedqa,ye-etal-2021-crossfit,min-etal-2022-metaicl}, 
%
and \uline{Chain-of-Thought explanations}~\citeiter{wei2022chain,lampinen2022can,li2022explanations}, etc.
% (Chain-of-Thought reasoning, etc.)
Different instructions are initially designed for distinct objectives (e.g., Mturk instructions are originally created for human annotators to understand, and prompts are to steer PLMs). In this section, as illustrated in Figure~\ref{fig:overview}, we first summarize these instructions into three categories that perform different combinations of $\hat{\textsc{T}}$, \textsc{X}, and \textsc{Y} (  \textsc{Entailment-oriented},  \textsc{PLM-oriented}, and  \textsc{Human-oriented}), then  compare them and provide the formal definition of instructions.
% According to the different groups in which the instructions are originally designed for, 、】
% groups and 


\subsection{\textsc{I}=$\hat{\textsc{T}}$+\textsc{Y}: Entailment-oriented Instruction}

One conventional scheme to handle the classification tasks is to convert the target labels into indices and let models decide which indices the inputs belong to. This paradigm focuses on encoding the input semantics while losing the label semantics. To let systems recognize new labels without relying on massive labeled examples, \newcite{yin2019benchmarking} proposed to build a hypothesis for each label---deriving the truth value of a label is then converted into determining the truth value of the hypothesis.  As exemplified in Table \ref{tab:nlptonli}, this approach builds instructions (\textsc{I}) combining a template ($\hat{\textsc{T}}$) with a label \textsc{Y} to explain each target label (\textsc{Y}). Since this paradigm naturally satisfies the format of textual entailment (\textualentailment, where the task inputs and the instructions can be treated as premises and hypotheses, respectively), these kinds of instructions are termed ``Entailment-oriented Instructions''.

The advantages of entailment-oriented instruction learning are four-fold: (i) it keeps the label semantics so that input encoding and output encoding both get equal attention in modeling the input-output relations; (ii) it results in a unified reasoning process---textual entailment---to handle various NLP problems; (iii) it creates the opportunity of making use of indirect supervision from existing \textualentailment~datasets so that a pretrained \textualentailment~model is expected to work on those target tasks without task-specific fine-tuning; (iv) it extends the original close-set labels classification problem into a  recognization problem of open-domain open-form labels with few or even zero label-specific examples. Therefore, it has been widely used in a variety of few/zero-shot classification tasks, such as classifying topics~\cite{yin2019benchmarking}, sentiments~\cite{zhong2021adapting}, stances~\cite{xu-etal-2022-openstance}, entity types~\cite{li2022ultra}, and entity relations~\cite{murty2020expbert,xia2021incremental,sainz-etal-2021-label,sainz-etal-2022-textual}.


\begin{table*}[t]
    \centering
    % \begin{tabular}{c|l|l}
    \scriptsize
    \begin{tabular}{p{0.09\linewidth}|p{0.35\linewidth}|p{0.20\linewidth}|p{0.09\linewidth}|p{0.15\linewidth}}
    Task & Input \textsc{X} & Template $\hat{\textsc{T}}$ (cloze question) & Answer & Output \textsc{Y} \\\hline


    Sentiment \newline Classification & I would like to buy it again. & [\textsc{X}] The product is \blank & Great \newline Wonderful \newline $\ldots$ & Positive \\\hline

    Entity \newline Tagging & [\textsc{X1}]: Donald Trump served as the 45th president of the United States from 2017 to 2021. \newline [\textsc{X2}]: Donald Trump & [\textsc{X1}] [\textsc{X2}] is a \blank entity? & Politician \newline President \newline $\ldots$ & People \\\hline

    Relation \newline Tagging & [\textsc{X1}]: Donald Trump served as the 45th president of the United States from 2017 to 2021. \newline [\textsc{X2}]: Donald Trump \newline [\textsc{X3}]: United States & [\textsc{X1}] [\textsc{X2}] is the \blank of [\textsc{X3}]? &  President \newline Leader \newline $\ldots$ & The\_president\_of \\\hline

    Textual \newline Entailment & [\textsc{X1}]: Donald Trump served as the 45th president of the United States from 2017 to 2021. \newline [\textsc{X2}]: Donald Trump is a citizen of United States. & [\textsc{X2}]? \blank, because [X1] & Indeed \newline Sure \newline $\ldots$ & Yes \\\hline

    Translation & Donald Trump served as the 45th president of the United States from 2017 to 2021. & Translate [\textsc{X}] to French:~\blank &  / &  $\ldots$~été président des États-Unis~$\ldots$ \\\hline
    % full translation: Donald Trump a été président des États-Unis de la 45ème législature de 2017 à 2021
    
    \end{tabular}
    \vspace{-1em}
    \caption{PLM-oriented instruction utilizes a task-specific template to convert the origin input into a fill-in-blank question. In most classification tasks, the intermediate answer should be further mapped into the predefined label.}
    \label{tab:PLM-orentied}
\end{table*}


\subsection{\textsc{I}=$\hat{\textsc{T}}$+\textsc{X}: PLM-oriented Instruction (e.g., prompts)}

% \textcolor{red}{@Renze: pls add a table like Table 1 to show some prompt examples for some typical NLP tasks}

The prompt is a representative of the PLM-oriented instructions, which is usually a brief utterance prepended with the task input (\textit{prefix prompt}), or a cloze-question template (\textit{cloze prompt})~\footnote{Please refer to Section 2.2.1 of~\citet{liu2023pre} for a detailed definition of prompt.}. It is basically designed for querying the intermedia responses (that can be further converted into the final answers) from the pre-trained LMs (PLM).
%
Since the prompted input conforms to the pre-training objectives of PLM (e.g., the cloze-style input satisfies the masked language modeling objective~\cite{kenton2019bert}), it help get rid of the reliance on the traditional supervised fine-tuning and greatly alleviates the cost of human annotations. Consequentially, prompt learning achieved impressive results on a multitude of previous few/zero-shot NLP tasks, e.g., question answering~\cite{radford2019language,lin2021few}, machine translation~\cite{li-etal-2022-prompt}, sentiment analysis~\cite{wu-shi-2022-adversarial}, textual entailment~\cite{schick2021exploiting,schick2021few}, and named entity recognition~\cite{cui2021template,wang2022instructionner}.

% Nevertheless, the performance 
% in using this sort of PLM-oriented instruction
Despite the excellent performance of prompt techniques, there are still two obvious issues with PLM-oriented instructions in real-world applications: (i). \textbf{\textit{Not User-Friendly}}. As the prompt is crafted for service PLM, it is encouraged to design prompts in a ``model's language'' (e.g., model-preferred incoherent words or internal embedding). However, this PLM-oriented instruction is hard to understand and often violates human intuitions.~\cite{gao-etal-2021-making,li-liang-2021-prefix,qin-eisner-2021-learning,khashabi2022prompt}. Meanwhile, the performance of prompts highly depends on the laborious prompt engineering~\cite{bach2022promptsource}, while most end-users are not PLM experts and usually lack sufficient knowledge to tune an effective prompt. (ii). \textbf{\textit{Applications Constraints}}. The prompt is usually short and simplistic, whereas many tasks can not be effectively formulated with solely a brief prompt, making prompt hard to deal with the diverse formats of real-world NLP tasks~\cite{chen2022knowprompt}.
% While we hope 


\begin{table*}[t]
    \centering
    % \begin{tabular}{c|l|l}
    \scriptsize
    \begin{tabular}{p{0.10\linewidth}|p{0.15\linewidth}|p{0.30\linewidth}|p{0.27\linewidth}|p{0.12\linewidth}}
    Task & Input \textsc{X} & Demonstrations \newline $\{\textsc{X}_i,\textsc{Y}_i\}_{i=1}^k$  & Template $\hat{\textsc{T}}$ & Output \textsc{Y}  \\\hline

    Named Entity \newline Extraction & The last three days have been incredible. Eric Lauren sqiddly and diddly all happy. & 
    [\textsc{X1}]: Let us wait for Tal August for a while. \newline
    [\textsc{Y1}]: Tal August \newline
    [\textsc{X2}]: I would like to make a face-to-face interaction with Sachin in August. \newline
    [\textsc{Y2}]: Sachin \newline
    $\ldots$ & Definition: In this task, you will be given sentences~$\ldots$~to recognize the name of a person~$\ldots$~\newline Examples: \newline Input: \textcolor{blue}{[\textsc{X1}]}; Output: \textcolor{blue}{[\textsc{Y1}]} \newline Input: \textcolor{blue}{[\textsc{X2}]}; Output: \textcolor{blue}{[\textsc{Y2}]}  \newline Now complete the following example: \newline Input: \textcolor{red}{[\textsc{X}]}; Output:~\blank  & Eric Lauren \\\hline
    % task1453, full definition: In this task, you will be given sentences in which your task is to recognize the name of a person. Note that Twitter names shouldn't be considered as people's names. Although there might be several correct answers, you need to write one of them.

    Summarization &  A plane carrying U.S. Rep. Gabrielle Giffords departed Houston~$\ldots$ & 
    [\textsc{X1}]: The goals just keep flowing for Lionel Messi~$\ldots$ \newline
    [\textsc{Y1}]: Lionel Messi scores twice as Barcelona beat Mallorca~$\ldots$ \newline
    [\textsc{X2}]: U.S. officials say they have specific reasons~$\ldots$ \newline
    [\textsc{Y2}]: Chairman of the House Intelligence Committee complains about~$\ldots$ \newline
    $\ldots$ & Definition: In this task, you are given news articles, and you need to generate a highlight~$\ldots$~\newline Examples: \newline Input: \textcolor{blue}{[\textsc{X1}]}; Output: \textcolor{blue}{[\textsc{Y1}]} \newline Input: \textcolor{blue}{[\textsc{X2}]}; Output: \textcolor{blue}{[\textsc{Y2}]} \newline Now complete the following example: \newline Input: \textcolor{red}{[\textsc{X}]}; Output:~\blank & James McBride wins for The Good Lord Bird~$\ldots$ \\\hline
    % task1553, full definition: In this task, you are given news articles, and you need to generate a highlight, i.e., a short summary, with a maximum length of 10 lines

    Question \newline Generation & Piedmont, or mountain, glaciers are found in many parts of the world~$\ldots$ & [\textsc{X1}]: There are a million times more viruses on the planet than stars in the universe~$\ldots$ \newline
    [\textsc{Y1}]: How many segments of virus DNA does the human genome contain? \newline
    [\textsc{X2}]: When mice are kept at high population densities~$\ldots$ \newline
    [\textsc{Y2}]: When does the mice behavior change? \newline
    $\ldots$ &  Definition: Given a paragraph, your job is to generate a question that can be answered from the passage~$\ldots$~\newline Examples: \newline Input: \textcolor{blue}{[\textsc{X1}]}; Output: \textcolor{blue}{[\textsc{Y1}]} \newline Input: \textcolor{blue}{[\textsc{X2}]}; Output: \textcolor{blue}{[\textsc{Y2}]} \newline Now complete the following example: \newline Input: \textcolor{red}{[\textsc{X}]}; Output:~\blank & What mountain ranges are present in South America? \\\hline
    % task1609, full definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.
    
    \end{tabular}
    \vspace{-1em}
    \caption{Examples coming from~\citet{wang2022benchmarking}. The human-oriented instruction is similar to the PLM-oriented instruction, which also utilizes a template to convert origin input (\textcolor{red}{in red}) into a cloze question. However, the task template itself contains informative task semantics, i.e., the formal task definition. Meanwhile, few-shot alternative task demonstrations are also provided (\textcolor{blue}{in blue}).}
    \label{tab:human-orentied}
\end{table*}


\subsection{\textsc{I}=$\hat{\textsc{T}}$+ optional $\{\textsc{X}_i,\textsc{Y}_i\}_{i=1}^k$: Human-oriented Instruction}

% \textcolor{red}{@Renze: pls add a table   to show what a human-oriented instruction looks like, specify which part is $\hat{\textsc{T}}$ and $\{\textsc{X}_i,\textsc{Y}_i\}_{i=1}^k$}

Human-oriented instructions basically mean the instructions used for crowd-sourcing works on the human-annotation platforms (e.g., Amazon MTurk instructions). Different from the PLM-oriented instructions, human-oriented instructions are usually some human-readable, descriptive, and paragraph-style task-specific textual information, consisting of \texttt{task title}, \texttt{category}, \texttt{definition}, and \texttt{things to avoid}, etc. Therefore, human-oriented instructions are more user-friendly and can be ideally applied to almost any complex NLP task. 

% More and more works began to employ the human-oriented instructions
Accordingly, human-oriented instructions have attracted much more attention in recent years~\citeiter{yin2022contintin,hu2022context,gupta-etal-2022-instructdial,longpre2023flan}. For example, \citet{efrat2020turking} tried to test whether GPT-2~\cite{radford2019language} can follow the MTurk instructions to annotate some popular NLP datasets. Their results showed that HuggingFace's off-the-shelf GPT-2~\cite{wolf2019huggingface} worked poorly on following these human-oriented instructions. 
%
While recent works found that multi-task instruction fine-tuned LMs could get more positive results. For instance, \citet{mishra2022cross} collected more than 60 NLP tasks with the corresponding MTurk instructions; \citet{wang2022benchmarking} further extended this collection into a 1.6k cross-lingual tasks scale. They all concluded that, after the large-scale instruction fine-tuning, the text-to-text PLM, like BART~\cite{lewis2020bart} and T5~\cite{raffel2020exploring} can generalize to the challenging unseen tasks by benefiting from the MTurk instructions.

Common: make use of indirect supervision


Difference: instance-wise instruction vs. task-wise instruction
\section{How to Model Instructions?}
\label{sec:modeling}
% The Modeling Strategies for Instruction Learning
% \textit{How} to Use Instructions?
In this section, we summarize several most popular modeling strategies for instruction learning. Overall, we introduce four different modeling schemes: For the earlier machine learning-based systems, the (i). \textbf{Semantic Parser-based} strategy was the commonly chosen method for encoding instructions; As the neural networks and the pre-trained language models emerged, the (ii). \textbf{Prompting Template-based} and the (iii). \textbf{Prefix Instruction-based} instruction learning became two highly favored paradigms; Recently, (iv). \textbf{HyperNetwork-based} method also garnered greater interest.

\subsection{Semantic Parser-based}
At the early stage of machine learning, to help the systems understand natural language instructions, a great number of works employed semantic parsing to convert the instruction into the formal language (logical formula), which can be easier executed by the systems (e.g., ``\textit{You can move any top card to an empty free cell}'' $\rightarrow$ ``\textit{card(x) $\wedge$ freecell(y) $\wedge$ empty(y)}'')~\citeiter{matuszek2012joint,babecs2012learning,chen2012fast,goldwasser2014learning}.
%\cite{liang2009learning,branavan2010reading,vogel2010learning,clarke2010driving,chen2011learning,matuszek2012joint,babecs2012learning,chen2012fast,goldwasser2014learning}
% Formally, the semantic parser-based approach can be written as:
% \[I^{'} = \textrm{Semantic Parser}(I)\]
% where the \(I^{'}\) is the converted logical formula.

For example, \citet{kuhlmann2004guiding} first tried to utilize natural language instructions to guide the systems to play soccer games, where they trained an individual semantic parser in advance, and then mapped the textual instructions into formal languages that can be used to influence the policy learned by the reinforcement learner. Since constructing a fully-supervised semantic parser requires laborious human annotations, the follow-up works also used indirect or weak supervision coming from the grounded environments (e.g., knowledge base) to train the semantic parser~\cite{eisenstein2009reading,chen2008learning,kim2012unsupervised,artzi2013weakly,krishnamurthy2013jointly}.
Besides using the converted formal languages to guide the systems to complete specific tasks, some works also utilized the logical formulae of the instructions to perform data and feature augmentations~\cite{srivastava2017joint,hancock2018training,wang2020learning,ye2020teaching}.


% \subsection{Prompt Template-based}
% \subsection{Cloze Question-based}
% \subsection{Fill-in-blank Question}
\subsection{Prompting Template-based}
% to convert the task instructions with the original task input into a cloze question by applying a template.
As for the neural network-based systems, we can directly encode the natural language instructions into the model's embedding without the help of a semantic parser. One of the prominent modeling strategies is using the prompting template. The essence of the prompting template-based approach is to use a template to convert the task input into a prompted format (i.e., cloze question~\footnote{Unlike the definition in~\citet{liu2023pre}, we regard any fill-in-blank question as the cloze, no matter whether the blank is in the middle or at the end of the question.}). According to the terminologies in~\cref{sec:categories}, the final input of LMs can be described as $x^{'}=f(x)$ or $f(x, I)$, i.e., the task-agnostic template $f(\cdot)$ is required but the task-specific information $I$ is optional. For example, $x=$~``\textit{I love this movie.}'' and $x^{'}=$~``\textit{I love this movie. It was~\blank}'', where there is no any task-specific information provided.
% ; Or $x=$~``\textit{I love this movie.}'' and $x^{'}=$~``\textit{I love this movie. It was~\blank}''

The prompting template-based approach is particularly useful for modeling PLM-oriented and entailment-oriented instructions. Therefore, a lot of previous works employed this strategy to modeling prompts~\citeiter{petroni-etal-2019-language,jiang2020can,cui2021template,haviv-etal-2021-bertese,schick-schutze-2021-just}. Besides using the discrete prompting template, recent works also tried to tune the continuous templates and achieved incredible results~\citeiter{li-liang-2021-prefix,tsimpoukelli2021multimodal,han2021ptr}.


% \subsection{Prefix Input-based}
% \subsection{Prefix Task-specific Information}
\subsection{Prefix Instruction-based}

Distinct from the prompting template-based approach, the prefix instruction-based method is mainly used for modeling human-oriented instructions, where sufficient task-specific information is provided~\citeiter{mishra2022cross,wang2022benchmarking,yin2022contintin,gu2022robustness}. Formally, in this case, the final input of LMs can be written as $x^{'}=I \oplus x$ or $f(x, I)$, which means the $I$ is required and has to be the prefix of the task input $x$, but the template language $f(\cdot)$ is optional.
% (we can simply consider $I$ as a prefix of $x$).
% task-agnostic
% when using prefix instruction-based modeling strategy
For example, \citet{wang2022benchmarking} utilized the following template ``\textit{Definition:}~$\cdots$~\textit{Input:}~$\cdots$~\textit{Output:}'' as the $f(\cdot)$, where all the task-specific information $I$ is prepended with the task input $x$.
% to combine the prefix instruction $I$ and the task input $x$

\subsection{HyperNetwork-based}
There are two obvious problems in using the prefix instruction-based modeling strategy. First, it concatenates the task-level instruction with every instance-level input, the repeating procedure significantly slowing down the processing/inference speed and the lengthy input also increasing the burden of computational cost~\cite{liu2022few}. Second, it can potentially impact the optimization because the model can not explicitly distinguish the task input $x$ from the prefix instructions $I$, thus the model can simply learn to complete the task and ignore the instructions~\cite{webson-pavlick-2022-prompt,deb2022boosting}.  
% One obvious problem in using the prefix instruction is the resulting lengthy input, which can lead to the expensive computational cost and potential .
% Since the prefix instructions make the final inputs become lengthy

To address the above issues, recent works began to employ the hypernetwork~\cite{ha2016hypernetworks} to encode the task instructions~\cite{jin2020language,deb2022boosting,ivison2022hint}. The essences of using hypernetwork-based approach are (i). encoding the task instruction $I$ and the task input $x$ separately, and (ii). converting the instruction into task-specific model parameters. For example, \citet{ye2021learning} used the hypernetwork to convert the task instruction into several parameter-efficiency adaptors~\cite{houlsby2019parameter}. Then they inserted these adaptors behind the multi-head attention layers of the underlying LMs to perform cross-task generalization.
% used the hypernetwork to convert the task instruction into several parameter-efficiency adaptors for the underlying LMs, and then inserted these adaptors behind the multi-head attention layers of LMs.



\section{Analyses}
\label{sec:analysis}

Instruction learning is proven to be effective in a lot of zero- and few-shot NLP tasks, but how to explain the impressive performance of instruction? And which aspects make a successful instruction learning procedure? To figure out the empirical rules in using instructions and better understand instruction learning, in this section, we summarize some insights from existing works for further research, i.e., some important factors that contribute to cross-task generalization.
% including (i).\textbf{ \textit{What} makes it works} (i.e., some important factors that contribute to the cross-task generalization); (ii). \textbf{\textit{Why} it works.}

% In this section, we discuss this question in three aspects by displaying some relevant works.

% Recent works conducted extensive experiments to understand the instruction learning. We summarize two aspects from these works: 
% Despite the promising zero- and few-shot performances of instruction learning, 
% \subsection{\textit{What} makes it works?}

\subsection{Instruction-based Fine-tuning} 

We first emphasize the importance of instruction-based fine-tuning (a.k.a. instruction fine-tuning)~\footnote{We use the term ``instruction fine-tuning'' to distinguish it from ``instruction tuning'', e.g., tuning soft prompts.}.
%
Different from the traditional fine-tuning, which aims at training model to complete specific tasks (i.e., $x \rightarrow y$), instruction fine-tuning trains the LMs on various instruction datasets where each input is converted into an instruction style by using the prompting template $f(\cdot)$ (i.e., either prefix instruction or cloze instruction~\cite{liu2023pre}), to drive the models to learn to follow the instruction (i.e., $f(x,I) \rightarrow y$). Existing works demonstrate that multi-task instruction-tuned LMs could better follow the instructions of the unseen tasks compared with no-tuned LMs~\citeiter{wei2021finetuned,sanh2021multitask,yin2022contintin,chung2022scaling,prasad2022grips}.

However, since previous works demonstrated that a massive multi-task training procedure also benefits the downstream tasks learning of LMs~\cite{mccann2018natural,aghajanyan-etal-2021-muppet,aribandiext5}, there is always a question that ``\textit{Whether instruction fine-tuning or multi-task learning plays a key role in cross-task generalization}''.
%
To answer this question, we first introduce the work of \citet{weller2020learning}, who solely tuned LMs with a multi-task learning paradigm and discovered that the LMs could find it hard to follow the instructions of the unseen tasks.
% first conducted experiments to investigate whether LMs can follow the 
\citet{wei2021finetuned,sanh2021multitask} further conducted in-depth comparison between multi-task instruction fine-tuning and multi-task learning on cross-task generalization. They found that instruction fine-tuning is the key for cross-task generalization rather than the multi-task learning itself.
% found that LMs can gain promising performance on the challenging unseen tasks with the help of instruction fine-tuning. They also 
%
% \cite{sanh2021multitask} even shown that relative small LMs can also learn from instructions

Besides the performance gains on the unseen tasks, there are also many other benefits of instruction fine-tuning. For example, \citet{wei2021finetuned} showed that instruction fine-tuned LMs performed better on following the soft instructions. Meanwhile, \citet{longpre2023flan} compared the convergences of Flan-T5 and T5 on single-task fine-tuning, and they found that instruction fine-tuned Flan-T5 learnt faster than T5 on the downstream single-task fine-tuning. What's more, some works also found that instruction fine-tuning makes LMs robust to some tiny perturbations in the instructions, such as the wordings~\cite{sanh2021multitask} and the paraphrasing~\cite{gu2022robustness}. While off-the-shelf LMs are usually sensitive to the small instruction perturbations~\cite{efrat2020turking,weller2020learning}, thus they require laborious prompt engineering~\cite{bach2022promptsource}. All in all, instruction fine-tuning tames the LMs to become much more user-friendly~\cite{chung2022scaling}.
% claimed that multi-task instruction fine-tuning bring benefits to the down-stream traditional fine-tuning on the unseen single task; 
% Instruction tuning makes model robust to some tiny perturbation (except the instruction-level huge perturbation), which can potentially well address the problem of the previous work~\cite{efrat2020turking}; And also, instruction tuning makes the model robust to the paraphrasing.
 % Multi-task instruction-based tuning bring benefits to the large LMs, but hurts the performance of small LMs. \textcolor{red}{This point is conflict with~\cite{sanh2021multitask}, see the comments under~\cite{sanh2021multitask} for details}; 
% \cite{wei2021finetuned} The instruction seems to be the key for multi-task prompted tuning, rather than simply the multi-task learning (i.e., multi-task prompted tuning $>$ multi-task learning);
% \cite{wei2021finetuned} Instruction-tuned models respond better to continuous inputs from prompt tuning (i.e., the model can benefit more from soft prompt, because I think the model have more explicit awareness on the prefix information);
% Instruction-tuned model can easier benefit from the informative task-specific instructions, compared with non-tuned model, indicating \textbf{the importance of instruction-based tuning for instruction following}~\cite{prasad2022grips}
% \cite{sanh2021multitask} Multi-task prompted training make model robust to the wordings of prompt, while the number of dataset doesn't matter the robustness.. 

\subsection{Instruction Paradigm} 

Keeping the instruction paradigm is also a crucial factor in instruction learning, which mainly means retaining the instruction conciseness of tuning and testing. \citet{wei2021finetuned} first investigated the performance impact of changing instruction paradigm. And they found that, the performances of the LMs tuned with short task names dropped when evaluating with more extended sentence-style instructions (i.e., task names $\rightarrow$ sentence-style instructions), compared with the results of keeping the instruction paradigm (i.e., task names $\rightarrow$ task names, or sentence-style instructions $\rightarrow$ sentence-style instructions). 
%
Meanwhile, \citet{gu2022robustness} also observed the performance dropping when changing paragraph-style instructions to sentence-style instructions in testing, further enhancing this conclusion.
% \textbf{Instruction paradigm} is the key for instruction tuning, e.g., the model pre-tuning on paragraph-level instructions produces huge performance drop when evaluating on the concise prompt.

Besides discrete instruction, the instruction paradigm is also critical in soft instruction learning. For example, \citet{xu2022zeroprompt} showed that the LMs fine-tuned with a continuous instruction also require a same-size prefix embedding when testing on unseen tasks, even if this embedding is randomly initialized.
% , which means keeping the formats of instructions consistent during evaluation
%\cite{xu2022zeroprompt} soft prompt of evaluation tasks is randomly initialized due to no labels. but even random initialization, just keep the paradigm, can achieve a promising results.
Interestingly, similar results were also found in the few-shot demonstrations (i.e., in-context learning). For instance, \citet{min2022rethinking} concluded that breaking the demonstration paradigm (i.e., removing the $y$) significantly harm the performance of MetaICL~\cite{min-etal-2022-metaicl}, which tuned with ($x,y$) pairs. Furthermore, \citet{iyer2022opt} found that the number of demonstrations should also not be changed during evaluation (e.g., using 2 demonstrations in tuning and 3 in testing would result in the lower performance, compared with using 2 demonstrations in testing).
% \cite{chung2022scaling} Instruction-tuned LM is user-friendly, and does not need prompt engineering or few-shot exemplars required by the conventional off-the-shelf LM.
% for in-context learning, keep the (x,y) pair format is the most important factor, compared with removing y~\cite{min2022rethinking}. \citet{iyer2022opt} also found 
% \cite{wei2021finetuned} The experiments also illustrates that the \textbf{instruction paradigm} is really important for instruction-based tuning (my personal observation, cf. Figure 8 in page 7).
\subsection{Model and Task Scale}

A series of recent works demonstrated that the scale also matters in instruction learning, including both model parameters and tuning tasks~\footnote{Task scale includes numbers and categories of tasks.} ~\citeiter{wei2021finetuned,sanh2021multitask,mishra2022cross,wang2022benchmarking,xu2022zeroprompt,deb2022boosting,prasad2022grips,chung2022scaling,iyer2022opt,longpre2023flan}.
% 
A representative work among them is~\citet{chung2022scaling}, who conducted extensive experiments with 1,836 tuning tasks and 540B models. The results illustrated that the cross-task performance takes advantages from the both factors~\footnote{Worth noting that the benefits of the model scale seem to outweigh the task scale. Please refer to the Fig.~4 of~\citet{chung2022scaling} and the follow-up work of~\citet{longpre2023flan}.}, suggesting the research community to continue scaling the instruction learning. 
%
However, the scale is usually unaffordable for most groups, and it also leads huge carbon emissions~\cite{strubell-etal-2019-energy,schick-schutze-2021-just}, making it unrealistic in the real-world scenarios.
%
Accordingly, some recent works began to investigate a more efficient way to address the scale problem, such as the parameter-efficient fine-tuning~\cite{schick2021exploiting,liu2022few}.
%
For example, \citet{jang2023exploring} employed the instruction-level experts (adaptors) to fine-tuning LMs on a single task, which outperformed the multi-task fine-tuned LMs on the unseen tasks.
%
It is also noteworthy that \citet{longpre2023flan} adapted the idea of ``noisy channel''~\cite{min2022noisy} that extended the tuning task scale by simply inverting the input-output of instance and achieved a reasonable performance improvement.
% be used as a good data augmentation to create ``new'' tasks, thus increasing task diversity;
% \paragraph{The Scale of Model} 
% \cite{longpre2023flan} \textbf{Large} Language model can benefit more from the scaling of tasks.
% \cite{chung2022scaling} (1). The scaling of task number and model parameters is important for instruction learning (1,800+ tasks, 540B parameters), which can help better generalise to unseen tasks.
% \cite{wei2021finetuned} Multi-task instruction-based tuning bring benefits to the large LMs, but hurts the performance of small LMs. \textcolor{red}{This point is conflict with~\cite{sanh2021multitask}, see the comments under~\cite{sanh2021multitask} for details};
% \paragraph{The Scale of Tasks}
% \cite{longpre2023flan} task quantity is very important; even small models (T5-base) can benefit from it
% \cite{iyer2022opt} The scale of task number and category is overall beneficial
% \cite{chung2022scaling} The positive return of task number diminishes when it reaches a specific upper bound.
% \cite{wei2021finetuned} The number of tasks and categories is important.
% \cite{sanh2021multitask}  Number of training datasets is important when evaluating on the challenging BIG-bench~\cite{srivastava2022beyond}, which requires novel skills.
% \cite{xu2022zeroprompt} The task quantity is important.
% \citet{jang2023exploring} even claims merely one task for tuning is enough.


\subsection{Instruction Diversity} 

Instruction diversity (e.g., various creativity) at fine-tuning phase also affects the cross-task performance and robustness of LMs~\cite{chung2022scaling,longpre2023flan}. 
%
Notably, \citet{sanh2021multitask} fine-tuned T5~\cite{raffel2020exploring} model on the multi-task datasets, where each dataset is associated with various instructions collected from ``Public Pool of Prompts'' (P3)~\cite{bach2022promptsource}. These instructions are written in different ways and perspectives (but in a similar conciseness)~\footnote{See Appendix G of~\citet{sanh2021multitask} for more details.}. By varying the number of instructions per dataset used in fine-tuning, \citet{sanh2021multitask} found that the model fine-tuned with more diverse instructions achieved better and more robust performance on the unseen tasks. What's more, \citet{sanh2021multitask} also found that the instruction diversity could compensate the limited model scale, i.e., a relative small LMs (T0-3B) could still benefit from multi-task fine-tuning due to the mixture of diverse instructions~\footnote{While \citet{wei2021finetuned} only used a fixed number of instructions and found that instruction fine-tuning harmed the performance of smaller LMs (Flan-8B).}.
%
% \cite{sanh2021multitask} Prompt diversity (e.g., length, creativity) is very important, prompted tuning LMs with fixed number of dataset, but with \textbf{more diverse prompt templates can lead to a better performance}; Owing to the diverse prompt, \textbf{\textcolor{red}{smaller LMs (T0-3B) can also benefit from multi-task prompted tuning}} (this is conflict with~\citet{wei2021finetuned}, who find that only large LMs can benefit from instruction-tuning, the held-out performance of smaller LM (Flan-8B) decreases after tuning, inter alia);
% \cite{sanh2021multitask} Training on more prompts per dataset leads to better and more robust generalization to held-out task; (that's reasonable, the mixture of diverse instruction resources make strong held-out generalization~\cite{chung2022scaling,longpre2023flan});

% Since human-written instructions are found to inevitably contain the bias that could impair the annotation creativity
Since manually crafting instructions with diversity is expensive and usually hard to achieve~\cite{huynh2021survey,parmar2022don}, recent works also resorted to model-generated instructions~\cite{zhang2020analogous,zhang2021learning,honovich2022unnatural,honovich2022instruction,ye2022guess,bai2022constitutional}. For instance, \citet{wang2022self} tried to drive GPT-3~\cite{brown2020language} to generate quantitative instructions from scratch iteratively. Although the self-generated instructions contain more noise, owing to their diverse verb-noun structures~\cite{kitaev2018constituency} and lengths, they could still bring benefits to tuning the GPT-3 and show complementary effects with the human-written instructions. These results imply the profitability of instruction diversity, even at the expense of the correctness of instructions. We will further discuss it in~\cref{subsec:Explainability}.
% diverse formats, i.e., various verb-noun structures~\cite{kitaev2018constituency} and lengths,
% \cite{wang2022self} self-generate instructions are quantitative and have good diversity.
%as mentioned in~\cite{wang2022self})

\subsection{Instruction Taxonomies and Situations}

As we have introduced in~\cref{sec:categories}, there are several kinds of textual instructions. Although they were all widely adapted by the previous works, different taxonomies show various-degree effects.
% 
For example, existing works found that adding positive few-shot demonstrations in the textual instructions could lead to a significant performance improvement on the unseen tasks~\cite{mishra2022cross,wang2022benchmarking,yin2022contintin,deb2022boosting,gu2022robustness}, especially for the tasks occupying complex output space~\cite{wei2021finetuned}. Surprisingly, \citet{gu2022robustness} further found that combining incorrect instructions with correct demonstrations could outperform using correct instruction without demonstrations, indicating the key role of demonstrations in instruction learning.
% To take a closer look, \citet{gu2022robustness} concatenated the gold-standard demonstrations with the random task instructions to tune and evaluate the LMs. Surprisingly, 
This prominence is perhaps because the LMs prefer to exploit the more superficial aspects of the demonstrations rather than the other complex contents~\citep[cf.][]{min2022rethinking}.
% It is also noteworthy that the 
% \textcolor{red}{TODO: paragraph-style vs. sentence-style?}

In addition, the effectiveness of different instruction taxonomies also highly depends on the target evaluation tasks. For example, the concise cloze-style instructions are useful on tasks that can be naturally expressed as instructions (e.g., QA), while it seems to be redundant when facing with language modeling tasks~\citep[cf.][]{wei2021finetuned}. What's more, CoT explanations seem to be necessary only for tasks that require multi-steps reasoning~\citep[cf.][]{kojima2022large}. To this end, a practical suggestion is to mix different instruction taxonomies when tuning the LMs, which has been proved to be efficient in tackling the various target evaluation tasks~\cite{chung2022scaling,iyer2022opt,longpre2023flan}.
% \cite{gu2022robustness} Demonstration has a dominant impact on the performance compared with task definitions or prompts, even random task definition conditioned, which perhaps show the importance of demonstration-free instruction learning.
% \cite{chung2022scaling} \textbf{Prompting paradigm} is key for instruction learning. Instruction-tuning has to combine much more diverse instruction styles to meet the different evaluation benchmarks. That's why the mixture of CoT can significantly improve the model generalization on the reasoning tasks (this point is in line with~\citet{gu2022robustness}).
% \cite{longpre2023flan} The effect of different instruction sources is different, and the effect of Flan 2021 is the best; SuperNI may cause the low return of scaling due to the unique instruction format;
% \cite{iyer2022opt} Different benchmarks complementing each other. That's why~\cite{longpre2023flan} choose to mix and balance various sources;
% \cite{iyer2022opt} Reasoning data is obviously useful (i.e., CoT) for reasoning evaluation and also bring benefits to some other task types
% \cite{wei2021finetuned,kojima2022large} CoT helps over direct answer-only prompting the most when tasks require multiple steps of reasoning and when used with a large-enough language model. Such tasks include challenging math problems or symbolic manipulation problems that require multiple steps.
% \cite{iyer2022opt} Dialogue and in-context learning objective~\cite{min2022metaicl} can harm the performance, that is, weakened model's instruction-following ability to conform to the required answer format, especially for those tasks whose label space is a special set of decision words (this potentially indicate the reliance on demonstrations); MetaICL particularly harm the generation tasks while benefiting the classification tasks, because it makes model tend to generate short answers and ignore the output pattern in the presence of in-context exemplars. MetaICL with the suffix loss outperforms regular MetaICL.
% \cite{deb2022boosting} When facing the weak generalization (cross-dataset), \textbf{demonstrations seems sufficient}; but when testing on strong generalization evaluation set (cross-category), \textbf{detailed instruction are more necessary}. the diversity of tasks is important. 
% \cite{wei2021finetuned} The mix of demonstrations is good, it is especially effective for tasks with large/complex output spaces. And also, demonstrations make model robust to the wordings of the prompt templates when testing (because I think the model tends to prefer demonstrations instead of prompt~\cite{gu2022robustness});


% \subsection{\textit{Why} it works?}
% \subsection{Pre-training Objective and Knowledge}
\subsection{Model Preference}

Another factor that can enhance the cross-task performance is making instructions conform to the \textit{preference} of LMs, that is, converting the instructions into model-oriented styles.
% Specifically, for model-oriented instruction, it should accord with the pre-training objective of PLM. 

Since the current instruction learning paradigm mainly employs the PLM as the backbone of the system, one of the potential explanations for why PLM-oriented instruction (i.e., prompt) can work is that prompt recalls the pre-training objective and activates the task-specific knowledge of the PLM.
% One of the potential explanations for why the instruction can work is that
% This conjecture is drawn from existing works
% especially for the PLM-oriented instructions (i.e., prompts).
Some of the existing works demonstrated the importance of conforming to the pre-training objective of PLM when doing instruction fine-tuning~\cite{tay2022unifying}. For example, \citet{schick2021exploiting,schick-schutze-2021-just} proposed the idea of \textit{pattern exploit training} (PET), which used a prompt to convert the original task inputs into cloze-style questions and then fine-tuned the PLM on instruction datasets with the masked language modelling objective. They found that, taking advantage of recalling the pre-training objective, relatively small LMs, such as ALBERT~\cite{lan2019albert}, can outperform GPT-3 on the SuperGLUE benchmark~\cite{wang2019superglue}. Furthermore, \citet{iyer2022opt} found that the PLM could perform better on the unseen tasks after mixing a small proportion of pretraining-style data in the instruction fine-tuning dataset. \citet{sanh2021multitask,wei2021finetuned} also found that the PLM was more likely to fail at the tasks whose objective is different from the language modelling but improved by adopting cloze-style instructions. 
% These works all implied the reason of 
All these results are in line with the empirical rules of prompt engineering~\cite{liu2023pre}, which highlights the importance of aligning the prompts with the PLM~\footnote{Using prefix prompts for the auto-regressive LMs, while using cloze prompts for the masked LMs. Please refer to ~\citet{liu2023pre} for more details.}.

Besides the objective of instruction fine-tuning, the way of designing instructions is also found critical. To better cater to the model's preference, recent works began employing continuous embedding (i.e., soft instructions) instead of human-understandable discrete instructions.~\citeiter{lester2021power,liu2021gpt,ye2022retrieval}.
% choosing the appropriate PLM and 
% recent works also tried to design the instructions by following the model's preference
Similar conclusions are also found in the human-oriented instructions, where the PLM constantly fails at following the human-oriented instructions but gains significant improvements after reframing the instructions to cater to the model's preference~\cite{mishra-etal-2022-reframing,prasad2022grips,gonen2022demystifying,deng-etal-2022-rlprompt,wang2022self}. Despite the performance profits, it is still controversial whether it is worthwhile to convert the original human-oriented instructions into PLM-oriented style, because it impairs the interpretability of instructions and is highly contrary to human intuition~\cite{khashabi2022prompt,webson-pavlick-2022-prompt,prasad2022grips}. We will further discuss it in~\cref{subsec:Explainability}.

% on using PLM-oriented instructions 
% \cite{sanh2021multitask} As a byproduct of learning to predict the next word, a language model is forced to learn from a mixture of implicit tasks included in their pretraining corpus. For example, by training on generic text from a web forum, a model might implicitly learn the format and structure of question answering. This gives large language models the ability to generalize to held-out tasks presented with natural language prompts, going beyond prior multitask studies on generalization to the held-out datasets.
% \cite{iyer2022opt} Adding a certain-proportion pre-training data can improve the results, indicating that recalling the pre-training objective during the instruction-tuning can have some benefits;
% \cite{wei2021finetuned} (1). instruction tuning is \textbf{very effective} on tasks naturally verbalized as instructions (e.g., NLI, QA, translation, struct-to-text) and is \textbf{less effective} on tasks directly formulated as language modeling, where instructions would be largely redundant (because the task input itself aligns with the pre-training objective, e.g., cloze). This point is also enhanced by~\citet{sanh2021multitask};
% \cite{schick-schutze-2021-just} Convert SuperGLUE~ into cloze style, and fine-tune the small language model. What instruction: Prompts (cloze-style template). What method: Prompted fine-tuning.

% \paragraph{Pre-training Knowledge}
% \cite{wang2022self} Although there is a lot of noise in the generated instructions, it can help the model get competitive zero-shot results on the SuperNI test set~\cite{wang2022benchmarking} compared with the other LLM trained on manually-written instruction dataset. (3). The results are consistent with using additional training data from the SuperNI, proving its complementary effect with the training set of SuperNI.
% \cite{zhao2021calibrate} \textbf{Common Token Bias}. The model often predicts common entities such as “America” when the ground-truth answer is instead a rare entity. That's probably why the target word matters (similar to the observation of~\citet{webson-pavlick-2022-prompt}).

% \paragraph{shallow pattern copy} 
% \cite{min2022rethinking}
% \cite{zhao2021calibrate} (1). \textbf{Majority Label Bias}, the model tends to copy the most frequent label in the examples, that's probably why the choice of example matters (similar to the observation of~\cite{min2022rethinking}); (2). \textbf{Recency Bias}. The tendency to repeat answers that appear towards the end of the prompt. That's probably why the order matters. And this recency bias can outweigh the majority label bias.




\section{Applications}
\label{sec:app}

\subsection{Human-Computer Interaction}

Textual instructions can be naturally regarded as a human-computer interaction method. Numerous previous works employed natural language instructions to ``guide'' the computer to perform various real-world tasks. 
%

For the non-NLP (multi-modal) tasks, most focused on environment-grounded language learning, i.e., driving the agent to associate natural language instructions with the environments and make corresponding reactions, such as selecting mentioned objects from an image/video~\cite{matuszek2012joint,krishnamurthy2013jointly,puig2018virtualhome}, following navigational instructions to move the agent~\cite{tellex2011approaching,kim2012unsupervised,chen2012fast,artzi2013weakly,bisk2016natural}, plotting corresponding traces on a map~\cite{vogel2010learning,chen2011learning}, playing soccer/card games based on given rules~\cite{kuhlmann2004guiding,eisenstein2009reading,branavan2011learning,babecs2012learning,goldwasser2014learning}, generating real-time sports broadcast~\cite{chen2008learning,liang2009learning}, controlling software~\cite{branavan2010reading}, and querying external databases~\cite{clarke2010driving}, etc.
% generate sportscasting~\cite{}, 
Meanwhile, instructions are also widely adapted to help communicate with the system in solving NLP tasks, e.g., following instructions to manipulate strings~\cite{gaddy2019pre}, classifying emails based on the given explanations~\cite{srivastava2017joint,srivastava2018zero}, and text-to-code generation~\cite{acquaviva2021communicating}.

Recently, a growing body of research tended to design the human-computer communication procedure in an \textbf{iterative} and \textbf{modular} manner. For example, \citet{li2020interactive} built a system to help the users tackle daily missions (e.g., ordering coffee or requesting Uber). Benefiting from a user-friendly graphical interface, the system can iteratively ask questions about the tasks, and users can continually refine their instructions to avoid unclear descriptions or vague concepts. Similarly, \citet{dwivedi2022editeval} proposed a benchmark to iteratively instruct the PLM to improve the text, where each iteration only used a small piece of instruction with a precise purpose (e.g., ``\textit{Simplify the text}'' or ``\textit{Make the text neutral}''). Besides, \citet{chakrabarty2022help} constructed a collaborative poem-writing system, where the user could initially provide an ambiguous instruction (e.g., ``\textit{Write a poem about cake}'') and then incrementally refine the instruction with more details (e.g., ``\textit{Contain the word -- `chocolate'} '') by observing the model's intermediate outputs. Meanwhile, \citet{mishra2022help} proposed a biography generation system\footnote{\citet{mishra2022help} actually experimented with more than 60 text generation tasks.} that progressively collected the necessary personal information from the users (by asking questions in a dialogue scene to guide the users) and generated a paragraph-style bio finally.
%
% Since the non-experts users usually lacks necessary knowledge to write model-preferred instructions, and the current SOTA AI models are also sensitive to the instructions 
As it is usually hard for non-expert users to write a sufficient instruction in one shot, while adapting an iterative and modular paradigm in designing instruction-based AI systems can help guide the users to enrich the task instruction step by step, thus this paradigm efficiently relieves the thinking demands of users and leads a more user-oriented system. Due to its practical values, we emphasize the importance of this branch of work in this paper.

\subsection{Data and Feature Augmentation}

Task instructions are regarded as indirect supervision resources where sometimes superficial and assertive rules are embedded. These rules are also known as \textit{labeling functions} that can be directly applied for annotations (e.g., the sentence ``\textit{a very fair price}'' is sentiment positive because \textit{``the word `price' is directly preceded by `fair' ''}).
% This kind of instruction (usually some explanations). 
% Some task instructions (i.e., explanations) also contain superficial and assertive rules for annotations, which are usually known as ``\textit{labeling functions}''.
Therefore, some existing works also employed the instruction as a distant supervision to perform data or feature augmentation~\cite{srivastava2018zero,hancock2018training,ye2020teaching}.
% , which can be used as the labeling function to annotate corpus to used as data augmentation automatically.
For instance, \citet{srivastava2017joint} used a semantic parser to convert natural language explanations into logical forms, and applied them on all instances in the dataset to generate additional binary features. While \citet{wang2020learning} utilized the label explanations to annotate the raw corpus automatically and trained the classifier on the resulting noisy data. 

Besides the straightforward augmentation, \citet{su2022one} further used the task instruction to enrich the model representation and achieved strong cross-task generalization. Specifically, they trained an embedding model (a single encoder) on the diverse instruction datasets with contrastive learning, and then used this model to produce task-specific representations based on the instruction for the downstream unseen tasks.
% extended this kind of feature augmentation to a cross-task generalization scene

% All of them achieved admirable results on various NLP tasks.
% \citet{hancock2018training,wang2020learning,ye2020teaching} Authors use semantic parser to convert the labeling explanation of each sample in the training set into formal language (labeling function), and then annotate the raw corpus automatically by using the labeling functions, thus generating some noisy data to train the classifier (thus it is a semi-supervision strategy); Similarly, \citet{srivastava2017joint} converts explanations to logical forms, and then use them to create binary features to enhance the original inputs. - What instruction: natural language \textbf{explanations} (labeling function, explain the reason of annotation), e.g., ``Positive, because the words "very nice" is within 3 words after the TERM''. - What method: Annotate the explanations on the original CLS datasets, use these human-written instructions as the rule of distance supervision, to generate noisy samples (data argument). - What task (input,output): Mostly CLS tasks. Use explanation as an additional supervision.

\subsection{Generalist Language Models}

% Another high-profile application of instruction learning is to build generalist language models.
% \cite{su2022one} use instructions to build a single embedding model to construct a task-level semantic space, which can be used for various downstream tasks.
According to the definition of Artificial General Intelligence (AGI), the ``generalist model'' is usually a system that can be competent for different tasks and scalable in changeable contexts, which shall go far beyond the initial anticipations of its creators~\cite{wang2007introduction,goertzel2014artificial}.
%
While specific to the NLP domain, a generalist language model is supposed to be an excellent multi-task assistant, that is skilled in handling a variety of real-world NLP tasks and different languages, in a completely zero/few-shot manner~\cite{arivazhagan2019massively,pratap2020massively,wei2021finetuned}.
%
As numerous existing works demonstrated the incredible power of using instructions in cross-task generalization~\citeiter{wei2021finetuned,sanh2021multitask,mishra2022cross,wang2022benchmarking,chung2022scaling}, the instruction is likely to become a breakthrough in achieving this ultimate goal. 

Notably, the recent two remarkable applications of instructions, namely InstructGPT~\cite{ouyang2022training} and ChatGPT~\footnote{\url{https://chat.openai.com/}}, also indicated a big step towards building generalist language models. However, unlike the other works that mainly employ instruction learning, ChatGPT also adopts some other components, e.g., reinforcement learning with human feedback (RLHF)~\footnote{At the time of writing, there is no published paper about ChatGPT. Thus, our discussion is mainly based on the underlying techniques of InstructGPT because they share similar philosophies. See OpenAI's blog for more details: \url{https://openai.com/blog/chatgpt}}.
% open-ended generation and 
Although the answer to ``{which component contributes more to the dramatic results of ChatGPT}'' remains ambiguous and needs further investigation, we introduce some recent works to highlight the critical role of instruction learning.
%
For example, \citet{chung2022scaling} conducted extensive experiments to evaluate the human-preference alignments of PaLM~\cite{chowdhery2022palm}. They found that, even without any human feedback, the instruction fine-tuning significantly reduced the toxicity in the open-ended generations of PaLM, such as gender and occupation bias. In addition, some other works also solely employed creative instructions instead of human feedback and achieved notable cross-task results~\cite{bai2022constitutional,honovich2022unnatural,wang2022self}.
% instruction fine-tuned language model. Besides the decent cross-task generalization performance, instruction fine-tuning also significantly reduced the toxicity in the open-ended generation of LMs, such as gender and occupation bias, achieving human-preference alignment even without any human feedback. 
% (e.g., Flan-PaLM)

Although ChatGPT still suffers from many unsatisfactory aspects and is far from the generalist language model~\cite{qin2023chatgpt,guo2023close,Kocon2023ChatGPTJO,wang2023robustness}, we hope the goal of AGI can continue to be promoted by adopting and evolving more powerful techniques, including instruction learning.




% \section{Challenges}
% \subsection{Explainability of Instructions}
% \subsection{Transferability of Instructions}

\section{Challenges and Future Directions}
\label{sec:challenges}

% \subsection{Modeling Strategies}
% Soft instruction is unstable and difficult to optimize~\cite{ding2022delta}, and the tuned embedding is hard to be transferred to new domains and tasks~\citeiter{xu2022zeroprompt}. Recent works also combine soft prompt with discrete instructions that tune soft prompt, which is concatenated with target domain-specific (task-specific) instructions and exemplars~\cite{singhal2022large}. the resulting soft instruction achieves more stable performance compared with the direct prompt tuning, and outperforms both the individual prompt tuning and in-context learning on the unseen tasks~\cite{Sun2023HowDI}.

% \subsection{Instruction Bias}
% \citet{huynh2021survey} found that the MTurk instructions 
% Existing works found that there are biases in the human-written instructions, such as the over-represented patterns and task-unrelated information.
% evaluation metrics,
% gu，wang，zhao
% \cite{zhao2021calibrate,shi2023large} tried to counteract the in-context bias and irrelevant information.

\subsection{Negated Instruction Learning}

Negation is a common linguistic property and has been found to be crucial for various NLP tasks, e.g., textual entailment~\cite{naik2018stress,kassner2020negated}. Specific to instruction learning, negation denotes any \textit{things-to-avoid} information of in-context instructions, such as negated task descriptions and negative demonstrations. Although human can benefit a lot from the negation~\cite{dudschig2018does}, existing works found LMs often fails at following the negated instructions~\cite{mishra2022cross,li2022maqa,jang2022can}. For example, \citet{mishra-etal-2022-reframing} conducted error analyses on GPT-3 and found GPT-3 constantly unable to understand the negated task constraints in the MTurk instructions. \citet{wang2022benchmarking} further found that adding negative demonstrations and explanations to the instructions could even harm the cross-task generalization performance of PLM. 

Since negation has increasingly become a challenge in instruction learning, we provide several hints to inspire future work. One potential solution to utilize the negated instruction is unlikelihood training~\cite{hosseini2021understanding,ye2022guess}, which trains the LMs to minimize the ground truth probability when negated instructions are conditioned. In contrast, \citet{yin2022contintin} proposed to pre-train the LMs on the negative demonstrations with maximizing likelihood objective to exploit the useful information in the negation. Some other methods, such as contrast-consistent projection~\cite{burns2022discovering} and n-gram representations~\cite{sun-lu-2022-implicit}, also provided insights into tackling this problem.

\subsection{Explainable Instruction Learning}
\label{subsec:Explainability}

As we have mentioned in~\cref{sec:analysis}, in order to achieve a promising cross-task performance, one of the critical factors is to convert the human-oriented instructions into a much more PLM-oriented format, i.e., making the instructions conform to the model's preference.
% one of the critical factors in achieving promising cross-task performance
Numerous previous works have verified the effectiveness of catering to the model's preference in designing instructions, such as using the model's perplexity in choosing appropriate instructions~\cite{gonen2022demystifying}.
%  the prediction entropy or  
Despite the performance gains of the PLM-oriented instruction selection, the resulting instructions consistently violate human intuitions, questioning the reliability of PLM-oriented instruction~\cite{webson-pavlick-2022-prompt}.
% and become far more difficult for humans to understand
For example, \citet{prasad2022grips} tried to rephrase the human-oriented instructions by using performance rewards as the criterion. Surprisingly, the resulting instructions that yield better performance are constantly semantically incoherent, task-irrelevant, or even misleading instructions. Similar results are also found in~\citet{khashabi2022prompt}, which mapped the continuous instructions back into the discrete space and found those effective instructions are usually associated with semantic-irrelevant utterances. These results prove the conflict between performance profits and the human interpretability of instructions, which is tricky to trade off.
% \citet{webson-pavlick-2022-prompt} observed that misleading prompts can still lead to comparable performance. Furthermore, \cite{prasad2022grips} shows this semantic-incoherent can even improve the results, which is totally different from human intuitions. \textbf{However, these two papers all experimented with only relatively small LMs (about 300M in~\citet{webson-pavlick-2022-prompt}, and size of InstructGPT in this paper), maybe a larger model can prefer more human-understandable instructions}.

Although \citet{mishra-etal-2022-reframing} demonstrated that it is possible to maintain both the faithfulness and effectiveness of instructions, manual rewriting requires laborious human efforts. Therefore, one of the future trends is to investigate how to automatically rephrase the instructions, in a way that matches both human and model preferences, such as setting an additional criterion during the instruction optimization.




\subsection{Explicit Instruction Learning}

As we have introduced in~\cref{sec:analysis}, multi-task instruction fine-tuning is a fundamental factor in the current instruction learning paradigm. Obviously, there are two issues in such a learning paradigm. (i). It relies on training on the massive labeled examples to learn the instructions, which is still expensive and unrealistic for using large-scale LMs; (ii). Although the ultimate goal of instruction-based fine-tuning is learning to follow instructions by observing various training tasks, the current training objective is still the maximum likelihood of traditional generation tasks. This implicit instruction learning objective can lead to sub-optimal optimization (i.e., LMs can easily learn to complete specific training tasks).

To this end, one desired future direction is to evolve a new training objective that can help LMs explicitly learn from instructions, which can alleviate the reliance on large-scale training instances. 
% ~\citep[cf.][]{tay2022unifying}
Moreover, a more ambitious and challenging idea is to drive the system to follow instructions without additional training on any labeled example of any specific task, which is similar to the conventional semantic parser-based paradigm (\cref{sec:modeling}).



% \subsection{Scalable Oversight and Alignment}
\subsection{Scalable Oversight: A New Evaluation Paradigm for Generalist AI Systems}

The evaluation procedure of the current research paradigm basically follows two steps: First, driving the systems to complete specific tasks; Second, using some automatic metrics to evaluate the systems.
%
While in the context of evaluating advanced instruction learning systems (i.e., generalist language models), this traditional paradigm suffers from two issues: (i). Limited by the automatic metrics, we will find it hard to measure the progress of the system if the system has already been more capable than non-expert humans on these tasks; (ii). We could have no idea whether the system is a qualified generalist ``assistant'' for non-expert humans.

Accordingly, recent works proposed the idea of \textit{scalable oversight}~\cite{cotra2021case,bowman2022measuring}, which denoted a new research paradigm for appraising the generalist language models, including the following steps: (i). \textbf{Task Choices}. Choosing the tasks where the LMs can outperform the non-experts but underperform the experts; (ii). \textbf{Non-experts Annotation}. Instead of driving the model to complete the tasks, ask the non-experts to annotate the challenging tasks by acquiring useful information from the untrustworthy LMs, i.e., the LMs need to follow some general instructions (w/o any domain-specific prompts) of non-experts to provide assistance; (iii). \textbf{Experts Evaluation}. At the end of the experiments, ask the experts to evaluate the annotation correctness of non-experts. In doing so, we can continue to promote the progress of generalist LMs by aligning the highly capable LMs with domain experts. Meanwhile, we simulate a real-world application scenery for most non-expert users, where the generalist LMs play the role of an assistant w/o any domain-specific knowledge aided.
%
By adopting this paradigm, \citet{bowman2022measuring} found that the non-experts can outperform both LMs-alone or human-alone results by benefiting from the assistance of LMs.

Overall, the scalable oversight paradigm can help future research to test whether current LMs (e.g., ChatGPT) can effectively assist non-expert users in solving challenging tasks.


\section{Conclusion}

In this survey, we comprehensively summarize numerous existing pieces of literature about instruction learning and provide a systematic overview of this field, including different instruction taxonomies, modeling strategies, some critical aspects of using instructions in engineering, and several popular applications. We also emphasize some distinct challenges and the corresponding hints for future research. To our knowledge, this is the first extensive survey about instruction learning. In summary, we hope this survey can offer insights and inspiration for further in-depth research on instruction learning. 

\bibliography{tacl2021}
\bibliographystyle{acl_natbib}

\end{document}


