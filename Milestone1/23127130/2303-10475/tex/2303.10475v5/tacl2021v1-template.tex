% File tacl2021v1.tex
% Dec. 15, 2021

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}

%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2021v1}
%% Most compact command to produce a "camera-ready" version
%%    \usepackage[acceptedWithA]{tacl2021v1}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2021v1}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

% \usepackage{tacl2021v1}
\usepackage[acceptedWithA]{tacl2021v1} % cancel anonymous
% \setlength\titlebox{10cm} % <- for Option 2 below
% \usepackage{amsmath}
\usepackage{ulem}
\usepackage{cleveref}
\crefname{section}{§}{§§}
\usepackage{CJKutf8}
\usepackage{xcolor}

\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{hhline}
% \usepackage{subfigure} 
\usepackage{graphicx}
\usepackage[caption=false]{subfig}
\usepackage{tabularray}
\usepackage{enumitem}
\UseTblrLibrary{counter}
% \UseTblrLibrary{functional}
\UseTblrLibrary{booktabs}
% \usepackage[table]{xcolor}


%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
% \usetikzlibrary{fadings,decorations.text}
\newcommand{\dateOfLastUpdate}{Dec. 15, 2021}
\newcommand{\styleFileVersion}{tacl2021v1}
\newcommand{\ex}[1]{{\sf #1}}

\newcommand{\citeiter}{\citep[][\textit{inter alia}]}
\newcommand{\blank}{\underline{\hspace{0.9em}}}
\newcommand{\linenotation}{\begin{verbatim} \n \end{verbatim}}
\newcommand{\footnotemarkNL}{\begin{NoHyper}\footnotemark\end{NoHyper}}
\definecolor{deeppink}{RGB}{255, 105, 180}
\definecolor{mycolor}{RGB}{240,240,240}
\definecolor{MyGrey}{HTML}{838383}
\definecolor{MyBlue}{HTML}{1F4E79}
\definecolor{MyRed}{HTML}{A80000}
\definecolor{MyYellow}{HTML}{FFCC00}
% \definecolor{MyPink}{HTML}{7F007F}
% \definecolor{MyGreen}{HTML}{02856B}
% \definecolor{MyOrange}{HTML}{FF6600}
\definecolor{MyPink}{HTML}{83639f}
\definecolor{MyGreen}{HTML}{449945}
\definecolor{MyOrange}{HTML}{ea7827}

\definecolor{ForestGreen}{HTML}{009B55}
\definecolor{OrangeRed}{HTML}{c22f2f}
\definecolor{Dandelion}{HTML}{e9963e}

\newcommand{\MyYes}{\textcolor{ForestGreen}{\textrm{yes}}}
\newcommand{\MyNo}{\textcolor{OrangeRed}{\textrm{no}}}
\newcommand{\MyMaybe}{\textcolor{Dandelion}{\textrm{maybe}}}

% \makeatletter
% \g@addto@macro{\endtabular}{\rowfont{}}% Clear row font
% \makeatother
% \newcommand{\rowfonttype}{}% Current row font
% \newcommand{\rowfont}[1]{% Set current row font
% \gdef\rowfonttype{#1}#1\ignorespaces%
% }
% \makeatother
% \makeatletter
% \IgnoreSpacesOn
% \tlNew \gFootNoteTl
% \intNew \gFootNoteInt
% \prgNewFunction \footNote {m}
%   {
%     \tlPutRight \gFootNoteTl
%       {
%         \stepcounter{footnote}
%         \footnotetext{#1}
%       }
%     \prgReturn {\footnotemark{}}
%   }
% \AddToHook{env/tblr/before}{
%   \intSetEq \gFootNoteInt \c@footnote
%   \tlClear \gFootNoteTl
% }
% \AddToHook{env/tblr/after}{
%   \intSetEq \c@footnote \gFootNoteInt
%   \tlUse \gFootNoteTl
% }
% \IgnoreSpacesOff
% \makeatother
% \tcbset{tab2/.style={enhanced,fonttitle=\bfseries,fontupper=\normalsize\sffamily,
% colback=yellow!10!white,colframe=red!50!black,colbacktitle=red!40!white,
% coltitle=black,center title}}

\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi

%%%% End TACL-instructions-specific macro block
%%%%

% \title{A Survey on Textual Instruction Learning: \\
%        Evolving Language Models to Generalist Cross-Task Assistants}

% \title{Learning from Task Instructions: \\
% What Happened, What is Happening, and Where Should We Go}

\usepackage{dingbat}
\usepackage{bbding}
\newcommand{\textualentailment}{\textsc{TE}} 
\newcommand{\TaskDef}{{\color{MyPink}{\em {\normalsize \textrm{Task Definition}}}}}
\newcommand{\Demonstrations}{{\color{MyOrange}{\em {\normalsize \textrm{Demonstrations}}}}}
\newcommand{\TestInput}{{\color{MyGreen}{\em {\normalsize \textrm{Test Instance}}}}}

% \title{Are Prompts All the Story? No. \\
% A Comprehensive and Broader View of Instruction Learning}
\title{Is Prompt All You Need? \\ No. A Comprehensive and Broader View of Instruction Learning}
% \title{Formatting Instructions for TACL \TaclPapers \\
% (Base files: \styleFileVersion-template.tex \& \styleFileVersion.sty, dated \dateOfLastUpdate)}

% Author information does not appear in the pdf unless the "acceptedWithA" option is given

% The author block may be formatted in one of two ways:

% Option 1. Author’s address is underneath each name, centered.

% \author{
%   Template Author1\Thanks{The {\em actual} contributors to this instruction
%     document and corresponding template file are given in Section
%     \ref{sec:contributors}.} 
%   \\
%   Template Affiliation1/Address Line 1
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   \texttt{template.email1example.com}
%   \And
%   Template Author2 
%   \\
%   Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
% }

% % Option 2.  Author’s address is linked with superscript
% % characters to its name, author names are grouped, centered.

% \author{
%   Template Author1\Thanks{The {\em actual} contributors to this instruction
%     document and corresponding template file are given in Section
%     \ref{sec:contributors}.}$^\diamond$ 
%   \and
%   Template Author2$^\dagger$
%   \\
%   \ \\
%   $^\diamond$Template Affiliation1/Address Line 1
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   \texttt{template.email1example.com}
%   \\
%   \ \\
%   \\
%   $^\dagger$Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
% }

% self-modified 
\author{
  Renze Lou$^\dagger$,
  Kai Zhang$^\diamond$
  \and
  Wenpeng Yin$^\dagger$
  \\
  $^\dagger$The Pennsylvania State University;
  \ 
  $^\diamond$The Ohio State University
  \\
  \texttt{\{renze.lou, wenpeng\}@psu.edu};
  \ 
  \texttt{zhang.13253@osu.edu}
}

\date{}


\begin{document}
\maketitle
\begin{abstract}
  % Despite the impressive results of the fully supervised fine-tuning, the laborious task-specific human annotation and large-scale pre-trained language models (PLM) make the traditional fine-tuning procedure unaffordable. Therefore, a growing body of research resorted to a new paradigm, namely \textit{textual instruction learning}, to perform various NLP tasks. By learning to follow the task instructions, the language models (LMs) are further evolved into generalist task assistants that can perform multiple tasks in a zero/few-shot manner, indicating a big step toward the idea of Artificial General Intelligence (AGI). In this paper, we survey numerous existing pieces of literature about instruction learning. Specifically, we first introduce the systematic taxonomies of instructions, and then summarize different common-chosen modeling strategies. Next, we provide several critical factors in using instructions in real-world NLP tasks, as well as display some popular applications. Finally, we highlight a few key challenges in the current instruction-learning paradigm, along with the corresponding future trends. To our knowledge, this is the first comprehensive survey about textual instructions.

  Task semantics can be expressed by a set of input-to-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. 
  Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: \textit{learning from task instructions}. Despite its impressive progress, there are some common issues that  the community struggles with. This survey paper tries to \textit{summarize} and \textit{provide insights} to the current research on instruction learning, particularly, by answering the following questions: (i) What is task instruction, and what  instruction types exist? (ii) How to model instructions? (iii) What factors influence and explain the instructions' performance? (iv) What challenges remain in instruction learning? To our knowledge, this is the first comprehensive survey about textual instructions.
  \footnote{We also release \textbf{a curated reading list} that will be maintained continuously to benefit future research, including related papers and popular datasets of instruction learning:~\url{https://github.com/RenzeLou/awesome-instruction-learning}}
  % ~\footnote{We release the corresponding reading list of this survey to benefit the future research:~\url{https://github.com/RenzeLou/awesome-instruction-learning}}
  \footnote{\textcolor{red!70}{Preliminary release. We will continue improving this work to ensure its quality and keep it up-to-date.}}
\end{abstract}


\begin{figure}[!ht]
	\begin{center}
		\centering
		\includegraphics[width=1.02\linewidth, trim=0 55 0 0]{picture/two_paradigms_v4.5.pdf}
	\end{center}
	\caption{Two machine learning paradigms: (a) traditional fully supervised learning uses extensive labeled examples to represent the task semantics. It is expensive, and the resulting system hardly generalizes to new tasks; (b) instruction learning utilizes task instruction to guide the system quickly adapts to various new tasks.}
    % , to perform binary classification on each label.
    % unify conventional classification problems into an entailment-based paradigm
    % that is initially designed for human annotations
    \vspace{-0.8em}
	\label{fig:two_paradigms}
\end{figure}



\begin{figure*}[!ht]
	\begin{center}
		\centering
		\includegraphics[width=0.98\linewidth, trim=0 55 0 0]{picture/taxonomiesv3.pdf}
	\end{center}
	\caption{An illustration of three distinct categories of textual instructions. (a) \textbf{Entailment-oriented}: regarding the original input as a premise and converting each predefined label into a hypothesis (i.e., instruction). (b) \textbf{PLM-oriented}: using a template to construct the original task input into a cloze question. (c) \textbf{Human-oriented}: utilizing sufficient task information as instruction, such as definitions and optional few-shot demonstrations, etc.}
    % , to perform binary classification on each label.
    % unify conventional classification problems into an entailment-based paradigm
    % that is initially designed for human annotations
	\label{fig:overview}
\end{figure*}


\section{Introduction}
% === background ===
% There were mainly two waves in the previous deep learning paradigms in NLP: (1). \textit{Fully supervised learning} that trains a task-specific model from the random initialization; (2). \textit{Pre-train and fine-tune} that pre-trains a general model on the large-scale corpus and then adapts it on specific downstream tasks by fine-tuning with labeled examples~\cite{radford2018improving}.

% With the help of advanced pre-training strategies, current large-scale language models (LLMs) achieve remarkable results on various natural language processing (NLP) tasks via fine-tuning on the labeled examples. This paradigm, so called \textit{fully supervised fine-tuning} (SFT), suffers from two issues: (1). Laborious human annotations in constructing task-specific corpus; (2). The resulting model is a \textbf{close-set specialist model}, which usually performs well on the seen training tasks but fails at generalizing to the unseen tasks. Therefore, how to build a \textbf{cross-task generalist model} that can handle various novel tasks became a long-term question for the research community~\cite{goertzel2014artificial}.

One goal of AI is
to build a system that can universally understand and solve new tasks. Labeled examples, as the
mainstream task representation, are unlikely to be available in large numbers or even do not exist.
Then, is there any other task representation that can contribute to task comprehension? Task instructions provide another dimension
of supervision for expressing the task semantics. Instructions often contain more abstract and
 comprehensive knowledge of the target task than individual labeled examples. As shown in Figure~\ref{fig:two_paradigms}, with the
availability of task instructions, systems can be quickly built to handle new tasks, especially when
task-specific annotations are scarce.
Instruction
Learning is inspired by the typical human learning for new tasks, e.g., a little kid can well solve a new mathematical task by learning from its instruction and a few examples~\cite{fennema1996longitudinal,carpenter1996cognitively}.
This new learning paradigm has recently attracted the main attention of the machine learning and NLP communities~\citeiter{radford2019language,efrat2020turking,brown2020language}. 

When talking about task instructions, most of us will first connect this concept with prompts--using a brief template to reformat new input into a language modeling problem so as to prime a PLM for a response~\cite{liu2023pre}. Despite prompts' prevalence in text classification, machine translation, etc., we claim that prompts are merely a special case of instructions. This paper takes a comprehensive and broader view of instruction-driven NLP research. Particularly, we try to answer the following questions:
\begin{itemize}
    \item What is task instruction, and what instruction types exist? (\cref{sec:categories})
    \item Given a task instruction, how to encode it to help the target task? (\cref{sec:modeling})
    \item What factors (e.g., model size, task numbers) impact the instruction-driven systems' performance, and how to design better instructions? (\cref{sec:analysis})
    \item What applications can instruction learning contribute? (\cref{sec:app})
    \item What challenges exist in instruction learning and what are future directions? (\cref{sec:challenges})
\end{itemize}


% === what is instruction and instruction learning (key difference with supervised learning). ===
% To this end, a novel paradigm is emerged to fill this gap, namely \textit{instruction learning}, which utilizes in-context natural language instructions to guide the LMs to perform various tasks. Unlike traditional supervised learning, where the model learns the task-specific skills by gradient-based parameter optimization, i.e., \textit{learn to complete tasks}, the essential objective of instruction learning is driving the model to \textit{learn to follow the instructions}, which provide a new way to address various NLP tasks in a completely zero/few-shot manner. As a result, instruction learning has attracted a lot of attention in recent years, which has led to diverse instructions with distinct formats. Therefore, in this paper, we aim to survey existing works about instruction learning systematically, including instruction taxonomies, modeling strategies, applications, and important aspects that impact zero-shot performance.
% critical challenges, and potential future trends.

% === a brief description of taxonomies ===
% === a brief description of modeling strategies ===
% === the advantages of instruction learning. === 



% === some challenges ===
% Although the instruction learning paradigm has been proven to be effective in handling NLP tasks, there are still several distinct problems in the current paradigm. First, the LMs could find it hard to grasp the semantic meaning of negated instructions truly; Second, the poor explainability of the high-performance instructions; Third, the reliance on the massive multi-task labeled examples due to the implicit learning objective; What's more, the problem of current automatic evaluation paradigm. To help further research address the above issues, we provide several corresponding hints by summarizing the current progress.


% === The purpose and position of this paper ===
To our knowledge, this is the first paper that surveys textual instruction learning. In contrast to some existing surveys that focused on a specific in-context instruction, such as prompts~\cite{liu2023pre}, input-by-output demonstrations~\cite{dong2022survey}, or reasoning~\cite{huang2022towards,qiao2022reasoning,yu2023nature}, we provide a broader perspective to connect distinct researches in this area in an organized way.
% Finally, we release the corresponding paper list for the beginner and future research~\footnote{\url{https://github.com/RenzeLou/awesome-instruction-learning}}, which will be continuously updated to keep track of the latest progress in this field. 
We hope this paper can present a better story of instruction learning and attract more peers to work on this challenging AI problem.
%
% The corresponding reading list will be released in the future to help beginners.
% We also release the corresponding reading list of this survey.~\footnote{\url{https://github.com/RenzeLou/awesome-instruction-learning}}

\begin{table*}[t]
    \centering
    \scriptsize
    \resizebox{0.99\textwidth}{!}{
    \begin{tblr}
    % set tablr args, including linewidth, v/h alignment
    {
    width=\linewidth, 
    colspec = {X[l,0.09\linewidth] || X[l,0.36\linewidth] | X[l,0.44\linewidth]},
    rowspec = {Q[b]Q[m]Q[m]Q[m]Q[m]Q[m]},
    % row{1} = {,blue!15}, % font=\bfseries, head in blue
    row{1} = {bg=azure6, fg=white, font=\bfseries},
    row{even} = {gray!15}, % even rows in gray
    rowhead = 1,
    hspan = minimal,
    }

    Task  & 
    \textsf{\textualentailment~premise (i.e., input text)} &
    \textsf{\textualentailment~hypothesis (i.e., instructions \textsc{Y})} \\ \hline

    \textit{Entity \newline Typing} & [Donald  Trump]$_{ent}$ served as the 45th president of the United States from 2017 to 2021. & (\textcolor{blue}{\checkmark}) Donald  Trump is a \textbf{politician} \newline (\textcolor{red}{\XSolidBrush}) Donald  Trump is a \textbf{journalist}\\\hline

    \textit{Entity \newline Relation}  & [Donald  Trump]$_{ent1}$ served as the 45th president of the [United States]$_{ent2}$ from 2017 to 2021. & (\textcolor{blue}{\checkmark}) Donald  Trump  \textbf{is citizen of} United States \newline (\textcolor{red}{\XSolidBrush}) Donald  Trump  \textbf{is the CEO of} United States \\\hline  

    \textit{Event \newline Argument \newline Extraction} & In [1997]$_{time}$, the [company]$_{sub}$ [hired]$_{trigger}$ [John D. Idol]$_{obj}$ to take over Bill Thang as the new chief executive. & (\textcolor{blue}{\checkmark}) \textbf{John D. Idol}   was hired. \newline (\textcolor{blue}{\checkmark}) \textbf{John D. Idol}   was hired in 1997. \newline (\textcolor{red}{\XSolidBrush}) \textbf{Bill Thang}  was hired. \\\hline

    \textit{\enspace\newline  Event \newline Relation} &  Salesforce  and Slack Technologies  have [entered]$_{event1}$ into a definitive agreement] under which Salesforce will [acquire]$_{event2}$ Slack. & (\textcolor{blue}{\checkmark}) Salesforce acquires Slack \textbf{after} it enters into the agreement with Slack Tech. \newline (\textcolor{red}{\XSolidBrush}) Salesforce acquires Slack \textbf{because} it enters into the agreement with Slack Tech. \\\hline  

    \textit{Stance \newline Detection} & Last Tuesday, Bill said ``animals are equal to human beings'' in his speech. & (\textcolor{blue}{\checkmark}) Bill \textbf{supports} that animals should have lawful rights. \newline (\textcolor{red}{\XSolidBrush}) Bill \textbf{opposes} that animals should have lawful rights.\\\hline

    \end{tblr}
    }
    \vspace{-1em}
    \caption{Entailment-oriented instructions construct hypotheses to explain the labels (in bold). ``\textcolor{blue}{\checkmark}'':  correct; ``\textcolor{red}{\XSolidBrush}'': incorrect.}
    \label{tab:nlptonli}
\end{table*}



% \begin{table*}[t]
%     \centering
%     % \begin{tabular}{c|l|l}
%     \scriptsize
%     \resizebox{0.99\textwidth}{!}{
%     \begin{tabular}{p{0.12\linewidth}||p{0.36\linewidth}|p{0.44\linewidth}}
%     % \rowcolor{blue!15} , fg=, font=\sffamily
%     \rowcolor{azure6}
%     \textcolor{white}{\textsf{Task}} & 
%     \textcolor{white}{\textsf{\textualentailment~premise (i.e., input text)}} &
%     \textcolor{white}{\textsf{\textualentailment~hypothesis (i.e., instructions \textsc{Y})}} 
%     \\\hline
    
%        \rowcolor{gray!15} 
%        \textit{Entity \newline Typing} & [Donald  Trump]$_{ent}$ served as the 45th president of the United States from 2017 to 2021. & (\textcolor{blue}{\checkmark}) Donald  Trump is a \textbf{politician} \newline (\textcolor{red}{\XSolidBrush}) Donald  Trump is a \textbf{journalist}\\\hline

%         \textit{Entity \newline Relation}  & [Donald  Trump]$_{ent1}$ served as the 45th president of the [United States]$_{ent2}$ from 2017 to 2021. & (\textcolor{blue}{\checkmark}) Donald  Trump  \textbf{is citizen of} United States \newline (\textcolor{red}{\XSolidBrush}) Donald  Trump  \textbf{is the CEO of} United States \\\hline   

%         \rowcolor{gray!15} 
%         \textit{Event \newline Argument \newline Extraction} & In [1997]$_{time}$, the [company]$_{sub}$ [hired]$_{trigger}$ [John D. Idol]$_{obj}$ to take over Bill Thang as the new chief executive. & (\textcolor{blue}{\checkmark}) \textbf{John D. Idol}   was hired. \newline (\textcolor{blue}{\checkmark}) \textbf{John D. Idol}   was hired in 1997. \newline (\textcolor{red}{\XSolidBrush}) \textbf{Bill Thang}  was hired. \\\hline
%         % (\textcolor{red}{\XSolidBrush}) Bill Thang hired \textbf{John D. Idol}. \\\hline

%         \textit{\enspace\newline  Event \newline Relation} &  Salesforce  and Slack Technologies  have [entered]$_{event1}$ into a definitive agreement] under which Salesforce will [acquire]$_{event2}$ Slack. & (\textcolor{blue}{\checkmark}) Salesforce acquires Slack \textbf{after} it enters into the agreement with Slack Tech. \newline (\textcolor{red}{\XSolidBrush}) Salesforce acquires Slack \textbf{because} it enters into the agreement with Slack Tech. \\\hline   

%         \rowcolor{gray!15} 
%         \textit{Stance \newline Detection} & Last Tuesday, Bill said ``animals are equal to human beings'' in his speech. & (\textcolor{blue}{\checkmark}) Bill \textbf{supports} that animals should have lawful rights. \newline (\textcolor{red}{\XSolidBrush}) Bill \textbf{opposes} that animals should have lawful rights.\\\hline
%     \end{tabular}
%     }
%     \vspace{-1em}
%     \caption{Entailment-oriented instructions construct hypotheses to explain the labels (in bold). ``\textcolor{blue}{\checkmark}'':  correct; ``\textcolor{red}{\XSolidBrush}'': incorrect.}
%     \label{tab:nlptonli}
% \end{table*}



\section{Preliminary}
\label{sec:pre}
 
For task instruction learning, we aim to drive the systems to reach the output given the input by following the instructions. Thus, a dataset consists of three items:
% In other words, the final input string of LMs can be divided into several distinct components:
% the requirements mentioned in the task instructions



\textbullet\enspace \textbf{Input} (\textsc{X}): the input of an instance; it can be a single piece of text (e.g., sentiment classification) or a group of text pieces (e.g., textual entailment, question answering, etc.). 
% It is widely used in human-oriented instructions (e.g., task title, category, and definition) but is usually optional for PLM-oriented and entailment-oriented instructions.

\textbullet\enspace \textbf{Output} (\textsc{Y}): the output of an instance; in classification problems, it can be one or multiple predefined labels; in text generation tasks, it can be any open-form text. 

\textbullet\enspace \textbf{Template} ($\hat{\textsc{T}}$): a textual template that tries to express the task meaning on its own or acts as a bridge between \textsc{X} and \textsc{Y}.\footnote{A plain template connecting \textsc{X} and \textsc{Y}, e.g., ``\textit{The input is $\ldots$ The output is $\ldots$}'', no task-specific semantics.} $\hat{\textsc{T}}$ may not be an instruction yet.

In \cref{sec:categories}, we will elaborate that a task instruction \textsc{I} is actually a combination of $\hat{\textsc{T}}$ with \textsc{X} or \textsc{Y}, or the $\hat{\textsc{T}}$ on its own in some cases.




% Generally, instruction learning aims to use the template language $f(\cdot)$ to combine the task input $x$ and the task-specific information $I$. The converted result $x^{'} = f(x,I)$ is considered as the final input of LMs~\footnote{Both task-specific and task-agnostic textual information can be regarded as ``instructions''.}. In the following text, we will first summarize different instruction categories (\cref{sec:categories}); We then introduce current popular instruction modeling strategies (\cref{sec:modeling}), that is, how the model uses these different pieces of information, i.e., $x$, $I$, $f(\cdot)$; We further discuss some important factors in using instructions for cross-task generalization (\cref{sec:analysis}) and the applications of instructions (\cref{sec:app}). Finally, we emphasize several challenges in the current instruction learning paradigm and provide potential future directions accordingly (\cref{sec:challenges}).


\section{What is Task Instruction?}
\label{sec:categories}
% The Taxonomies of Textual Instructions
% \textit{What} is Instruction?

Various types of textual instructions have been  used in  previous zero- and few-shot NLP tasks, such as \uline{prompts}~\citeiter{hendrycksmeasuring,srivastava2022beyond,bach2022promptsource},
%
\uline{Amazon Mechanical Turk instructions}~\citeiter{mishra2022cross,wang2022benchmarking,yin2022contintin},
%
\uline{instructions augmented with demonstrations}~\citeiter{khashabi-etal-2020-unifiedqa,ye-etal-2021-crossfit,min-etal-2022-metaicl}, 
% \footnote{We consider the templates used in the in-context learning as the instructions rather than the few-shot demonstrations.}
and \uline{Chain-of-Thought explanations}~\citeiter{wei2022chain,lampinen2022can,li2022explanations}, etc.
% (Chain-of-Thought reasoning, etc.)
Different instructions are initially designed for distinct objectives (e.g., Mturk instructions are originally created for human annotators to understand, and prompts are to steer PLMs). In this section, as illustrated in Figure~\ref{fig:overview}, we first summarize these instructions into three categories that perform different combinations of $\hat{\textsc{T}}$, \textsc{X}, and \textsc{Y} (  \textsc{Entailment-oriented},  \textsc{PLM-oriented}, and  \textsc{Human-oriented}), then  compare them and provide the formal definition of instructions.
% According to the different groups in which the instructions are originally designed for, 、】
% groups and 


\subsection{\textsc{I}=$\hat{\textsc{T}}$+\textsc{Y}: Entailment-oriented Instruction}

One conventional scheme to handle the classification tasks is to convert the target labels into indices and let models decide which indices the inputs belong to. This paradigm focuses on encoding the input semantics while losing the label semantics. To let systems recognize new labels without relying on massive labeled examples, \newcite{yin2019benchmarking} proposed to build a hypothesis for each label---deriving the truth value of a label is then converted into determining the truth value of the hypothesis.  As exemplified in Table \ref{tab:nlptonli}, this approach builds instructions (\textsc{I}) combining a template ($\hat{\textsc{T}}$) with a label \textsc{Y} to explain each target label (\textsc{Y}). Since this paradigm naturally satisfies the format of textual entailment (\textualentailment, where the task inputs and the instructions can be treated as premises and hypotheses, respectively), these kinds of instructions are termed ``Entailment-oriented Instructions''.

The advantages of entailment-oriented instruction learning are four-fold: (i) it keeps the label semantics so that input encoding and output encoding both get equal attention in modeling the input-output relations; (ii) it results in a unified reasoning process---textual entailment---to handle various NLP problems; (iii) it creates the opportunity of making use of indirect supervision from existing \textualentailment~datasets so that a pretrained \textualentailment~model is expected to work on those target tasks without task-specific fine-tuning; (iv) it extends the original close-set labels classification problem into a  recognization problem of open-domain open-form labels with few or even zero label-specific examples. Therefore, it has been widely used in a variety of few/zero-shot classification tasks, such as classifying topics~\cite{yin2019benchmarking}, sentiments~\cite{zhong2021adapting}, stances~\cite{xu-etal-2022-openstance}, entity types~\cite{li2022ultra}, and entity relations~\cite{murty2020expbert,xia2021incremental,sainz-etal-2021-label,sainz-etal-2022-textual}.


% \begin{table*}[t]
%     \centering
%     % \begin{tabular}{c|l|l}
%     \scriptsize
%     \resizebox{0.98\textwidth}{!}{
%     \begin{tabular}{p{0.09\linewidth}|p{0.35\linewidth}|p{0.20\linewidth}|p{0.09\linewidth}|p{0.15\linewidth}}
%     Task & Input \textsc{X} & Template $\hat{\textsc{T}}$ (cloze question) & Answer & Output \textsc{Y} \\\hline


%     Sentiment \newline Classification & I would like to buy it again. & [\textsc{X}] The product is \blank & Great \newline Wonderful \newline $\ldots$ & Positive \\\hline

%     Entity \newline Tagging & [\textsc{X1}]: Donald Trump served as the 45th president of the United States from 2017 to 2021. \newline [\textsc{X2}]: Donald Trump & [\textsc{X1}] [\textsc{X2}] is a \blank entity? & Politician \newline President \newline $\ldots$ & People \\\hline

%     Relation \newline Tagging & [\textsc{X1}]: Donald Trump served as the 45th president of the United States from 2017 to 2021. \newline [\textsc{X2}]: Donald Trump \newline [\textsc{X3}]: United States & [\textsc{X1}] [\textsc{X2}] is the \blank of [\textsc{X3}]? &  President \newline Leader \newline $\ldots$ & The\_president\_of \\\hline

%     Textual \newline Entailment & [\textsc{X1}]: Donald Trump served as the 45th president of the United States from 2017 to 2021. \newline [\textsc{X2}]: Donald Trump is a citizen of United States. & [\textsc{X2}]? \blank, because [X1] & Indeed \newline Sure \newline $\ldots$ & Yes \\\hline

%     Translation & Donald Trump served as the 45th president of the United States from 2017 to 2021. & Translate [\textsc{X}] to French:~\blank &  / &  $\ldots$~été président des États-Unis~$\ldots$ \\\hline
%     % full translation: Donald Trump a été président des États-Unis de la 45ème législature de 2017 à 2021
    
%     \end{tabular}
%     }
%     \vspace{-1em}
%     \caption{PLM-oriented instruction utilizes a task-specific template to convert the origin input into a fill-in-blank question. In most classification tasks, the intermediate answer should be further mapped into the predefined label.}
%     \label{tab:PLM-orentied}
% \end{table*}


\begin{table*}[t]
    \centering
    \scriptsize
    \resizebox{1.0\textwidth}{!}{
    \begin{tblr}
    % set tablr args, including linewidth, v/h alignment
    {
    width=\linewidth, 
    colspec = {X[l,0.08\linewidth] || X[l,0.36\linewidth] | X[l,0.23\linewidth] | X[l,0.07\linewidth] | X[l,0.11\linewidth]},
    rowspec = {Q[b]Q[m]Q[m]Q[m]Q[m]Q[m]},
    % row{1} = {,blue!15}, % font=\bfseries, head in blue
    row{1} = {bg=azure6, fg=white, font=\bfseries},
    row{even} = {gray!15}, % even rows in gray
    rowhead = 1,
    hspan = minimal,
    }
    \SetCell[c=1]{c} Task & \SetCell[c=1]{c} Input~\textsc{X} & \SetCell[c=1]{c} Template~$\hat{\textsc{T}}$ (cloze question) & \SetCell[c=1]{c} Answer & \SetCell[c=1]{c} Output~\textsc{Y} \\\hline

    % \SetCell[r=3]{} Sentiment & \SetCell[r=3]{} I would like to buy it again. & \SetCell[r=3]{} [\textsc{X}] The product is \blank & Great & \SetCell[r=3]{} Positive \\ 
    % Classification & & & Wonderful & \\
    %  & & & $\ldots$ & \\\hline
    
    \textit{Sentiment Analysis} & I would like to buy it again. & \texttt{[\textsc{X}]} The product is \blank. & Great \newline Wonderful \newline $\ldots$ & Positive \\\hline

    \textit{Entity Tagging} & [Donald Trump]$_{ent}$ served as the 45th president of the United States from 2017 to 2021. & The entity in \texttt{[\textsc{X}]} is a \blank class? & Politician \newline President \newline $\ldots$ & People \\\hline

    \textit{Relation Tagging} & [Donald  Trump]$_{ent1}$ served as the 45th president of the [United States]$_{ent2}$ from 2017 to 2021. & \texttt{[\textsc{X}]} entity$_1$ is the \blank of entity$_2$? &  Executive \newline Leader \newline $\ldots$ & President \\\hline

    \textit{Textual Entailment} & \texttt{[\textsc{X$_1$}]}: Donald Trump served as the 45th president of the United States from 2017 to 2021. \newline \texttt{[\textsc{X$_2$}]}: Donald Trump is a citizen of United States. & \texttt{[\textsc{X$_2$}]}? \blank, because \texttt{[\textsc{X$_1$}]} & Indeed \newline Sure \newline $\ldots$ & Yes \\\hline

    \textit{Translation} & Donald Trump served as the 45th president of the United States from 2017 to 2021. & Translate \texttt{[\textsc{X}]} to French:~\blank &  / & été président~{...} \\\hline
    % full translation: Donald Trump a été président des États-Unis de la 45ème législature de 2017 à 2021
    
    \end{tblr}
    }
    \vspace{-1em}
    \caption{PLM-oriented instructions utilize templates to convert the origin inputs into fill-in-blank questions. In most classification tasks, the intermediate answers may require further mapping (i.e., verbalizer).}
    % should be further mapped into the predefined label
    \label{tab:PLM-orentied}
\end{table*}


\subsection{\textsc{I}=$\hat{\textsc{T}}$+\textsc{X}: PLM-oriented Instruction (e.g., prompts)}

% \textcolor{red}{@Renze: pls add a table like Table 1 to show some prompt examples for some typical NLP tasks}

As shown in Table~\ref{tab:PLM-orentied}, the prompt is a representative of the PLM-oriented instruction, which is usually a brief utterance prepended with the task input (prefix prompt), or a cloze-question template (cloze prompt)~\footnote{Please refer to Section 2.2.1 of~\citet{liu2023pre} for a detailed definition of the prompt.}. It is basically designed for querying the intermedia answers (that can be further converted into the final outputs) from the pre-trained LMs (PLM).
%
Since the prompted input conforms to the pre-training objectives of PLM (e.g., the cloze-style input satisfies the masked language modeling objective~\cite{kenton2019bert}), it helps get rid of the reliance on the traditional supervised fine-tuning and greatly alleviates the cost of human annotations. Consequentially, prompt learning achieved impressive results on a multitude of previous few/zero-shot NLP tasks, e.g., question answering~\cite{radford2019language,lin2021few}, machine translation~\cite{li-etal-2022-prompt}, sentiment analysis~\cite{wu-shi-2022-adversarial}, textual entailment~\cite{schick2021exploiting,schick2021few}, and named entity recognition~\cite{cui2021template,wang2022instructionner}.

% Nevertheless, the performance 
% in using this sort of PLM-oriented instruction
Despite the excellent performance of prompt techniques, there are still two obvious shortcomings with PLM-oriented instructions in real-world applications. (i) \textbf{Not User-Friendly}. As the prompt is crafted for service PLM, it is encouraged to design prompts in a ``model's language'' (e.g., model-preferred incoherent words or internal embedding). However, this PLM-oriented instruction is hard to understand and often violates human intuitions~\cite{gao-etal-2021-making,li-liang-2021-prefix,qin-eisner-2021-learning,khashabi2022prompt}. Meanwhile, the performance of prompts highly depends on the laborious prompt engineering~\cite{bach2022promptsource}, but most end-users are not PLM experts and usually lack sufficient knowledge to tune an effective prompt. (ii) \textbf{Applications Constraints}. The prompt is usually short and simplistic, whereas many tasks can not be effectively formulated with solely a brief prompt, making prompt hard to deal with the diverse formats of real-world NLP tasks~\cite{chen2022knowprompt}.
% While we hope 


% \begin{tikzfadingfrompicture}[name=tikz]
% \node [text=transparent!20]
% {\fontfamily{ptm}\fontsize{12}{12}\bfseries\selectfont Ti\emph{k}Z};
% \end{tikzfadingfrompicture}

% \begin{tikzpicture}
% \shade[path fading=tikz,fit fading=false,
% left color=blue,right color=black]
% (-2,-1) rectangle (2,1);
% \end{tikzpicture}


% \begin{table*}[t]
%     \centering
%     % \begin{tabular}{c|l|l}
%     \scriptsize
%     \resizebox{0.98\textwidth}{!}{
%     \begin{tabular}{p{0.10\linewidth}|p{0.15\linewidth}|p{0.30\linewidth}|p{0.27\linewidth}|p{0.12\linewidth}}
\begin{table*}[t]
    \centering
    \footnotesize
    \resizebox{1.0\textwidth}{!}{
    \begin{tblr}
    % set tablr args, including linewidth, v/h alignment
    {
    width=\linewidth, 
    colspec = {X[l,0.08\linewidth] || X[l,0.23\linewidth] | X[l,0.64\linewidth] | X[l,0.08\linewidth]},
    rowspec = {Q[b]Q[m]Q[m]Q[m]},
    % row{1} = {,blue!15}, % font=\bfseries, head in blue
    row{1} = {bg=azure6, fg=white, font=\bfseries},
    row{even} = {gray!15}, % even rows in gray
    rowhead = 1,
    hspan = minimal,
    }

    %+ Optional Demonstrations
    % Template $\hat{\textsc{T}}$ + $\{\textsc{X}_i,\textsc{Y}_i\}_{i=1}^k$
    % \textsc{I} = $\hat{\textsc{T}}$+ optional $\{\textsc{X}_i,\textsc{Y}_i\}_{i=1}^k$
    % Instruction (i.e., Template + Few-shot Demonstrations)
    \SetCell[c=1]{c} Task  & \SetCell[c=1]{c} Input~\textsc{X} & \SetCell[c=1]{c} Template~$\hat{\textsc{T}}$ + Few-shot Demonstrations & \SetCell[c=1]{c} Output~\textsc{Y}  \\\hline

    \textit{Sentiment \newline Analysis} & I am extremely impressed with its good performance. I would like to buy it again! 
     & 
    \TaskDef: \newline
    \colorbox{MyPink!20}{\quad In this task, you are given a product review, and you need to identify~$\dots$} \newline
    \Demonstrations~\textsf{(optional)}: \newline
    \colorbox{MyOrange!15}{\quad Input: \textsl{These are junks, I am really regret...} \quad Output: \textsl{Negative}} \newline
    \colorbox{MyOrange!15}{\quad Input: \textsl{Wonderful bulb with good duration...} \quad Output: \textsl{Positive}} \newline
    \TestInput: \newline
    \colorbox{MyGreen!20}{\quad Input: \texttt{[\textsc{X}]} \quad Output:~\blank}
     & 
     Positive\\\hline

    \textit{Named \newline Entity \newline Extraction} & Donald Trump served as the 45th president of the United States from 2017 to 2021. 
    &
    \TaskDef: \newline
    % {\begin{tcolorbox}[colback=yellow!30]
    % {Your task is to recognize the name of a person in the given sentence~$\dots$}
    % \end{tcolorbox}} 
    \colorbox{MyPink!15}{\quad Your task is to recognize the name of a person in the given sentence~$\dots$} \newline
    \Demonstrations~\textsf{(optional)}: \newline
    \colorbox{MyOrange!15}{\quad Input: \textsl{Ousted WeWork founder Adam Neuman...} \quad Output: \textsl{Adam Neuman}} \newline
    \colorbox{MyOrange!15}{\quad Input: \textsl{Tim Cook became the CEO of Apple Inc since...} \quad Output: \textsl{Tim Cook}} \newline
    % \colorbox{MyOrange!15}{\quad $\ldots$} \newline
    \TestInput: \newline
    \colorbox{MyGreen!20}{\quad Input: \texttt{[\textsc{X}]} \quad Output:~\blank}
    % \colorbox{gray!15}{\quad Input: \texttt{[\textsc{X}]} \quad Output:~\blank}
    & 
    Donald Trump \\\hline
    
    \end{tblr}
    }
    \vspace{-1em}
    \caption{Two examples that illustrate the human-oriented instructions (w/ 2-shot demonstrations). Similarly, human-oriented instructions use task-level templates to convert the origin inputs into blank questions. However, the templates here have sufficient task semantics (i.e., {\color{MyPink}{\em \textrm{Task Definition}}}) and are sometimes equipped with~{\color{MyOrange}{\em \textrm{Demonstrations}}}, while those in PLM-oriented instructions usually do not.}
    % The human-oriented instruction is similar to the PLM-oriented instruction, which also utilizes a template to convert origin input (\textcolor{red}{in red}) into a cloze question. However, the task template itself contains informative task semantics, i.e., the formal task definition. Meanwhile, few-shot alternative task demonstrations are also provided (\textcolor{blue}{in blue})
    \label{tab:human-orentied}
\end{table*}


\subsection{\textsc{I}=$\hat{\textsc{T}}$+ optional $\{\textsc{X}_i,\textsc{Y}_i\}_{i=1}^k$: Human-oriented Instruction}

% \textcolor{red}{@Renze: pls add a table   to show what a human-oriented instruction looks like, specify which part is $\hat{\textsc{T}}$ and $\{\textsc{X}_i,\textsc{Y}_i\}_{i=1}^k$}

Human-oriented instructions basically mean the instructions used for crowd-sourcing works on the human-annotation platforms (e.g., Amazon MTurk)\footnote{\url{https://www.mturk.com/}}. Table~\ref{tab:human-orentied} provides some examples of human-oriented instructions. Unlike PLM-oriented instructions, human-oriented instructions are usually some human-readable, descriptive, and paragraph-style information consisting of various components, such as \texttt{task title}, \texttt{category}, \texttt{definition}, and \texttt{things to avoid}~\citep[cf.][]{mishra2022cross}, etc. Therefore, human-oriented instructions are more user-friendly and can be ideally applied to almost any complex NLP task. 

% More and more works began to employ the human-oriented instructions
Accordingly, human-oriented instructions have attracted much more attention in recent years~\citeiter{yin2022contintin,hu2022context,gupta-etal-2022-instructdial,longpre2023flan}. For example, \citet{efrat2020turking} tried to test whether GPT-2~\cite{radford2019language} can follow the MTurk instructions to annotate some popular NLP datasets. Their results showed that HuggingFace's off-the-shelf GPT-2~\cite{wolf2019huggingface} worked poorly on following these human-oriented instructions. 
%
While recent works found that multi-task instruction fine-tuned LMs could get more positive results. For instance, \citet{mishra2022cross} collected more than 60 NLP tasks with the corresponding MTurk instructions; \citet{wang2022benchmarking} further extended this collection into a 1.6k cross-lingual tasks scale. They all concluded that, after the large-scale instruction tuning, the text-to-text PLM, like BART~\cite{lewis2020bart} and T5~\cite{raffel2020exploring} can generalize to the challenging unseen tasks by benefiting from the MTurk instructions.


\begin{table*}[t]
    \centering
    \scriptsize
    \resizebox{0.99\textwidth}{!}{
    \begin{tblr}
    % set tablr args, including linewidth, v/h alignment
    {
        width=\linewidth, 
        colspec = 
            {
            X[l,0.34\linewidth]
            X[c,0.18\linewidth]
            X[c,0.17\linewidth]
            X[c,0.20\linewidth]
            },
        rowspec = 
            {
            |[2pt,MyGrey]Q[b]
            |[1pt,MyGrey,solid]Q[m]Q[m]Q[m]Q[m]Q[m]Q[m]Q[m]
            |[0.8pt,MyGrey,dashed]Q[m]Q[m]
            Q[m]Q[m]|[2pt,MyGrey]
            },
        row{1} = {font=\bfseries},
        rowhead = 1,
        hspan = minimal,
    }
    {\footnotesize Trait} & {\footnotesize Entailment-oriented} & {\footnotesize PLM-oriented} & {\footnotesize Human-oriented} \\
    % \hline
    Update PLM parameter? & \MyYes & \MyMaybe~\footnotemarkNL & \MyYes \\ 
    % \hline
    Require the super large PLM? & \MyNo & \MyYes & \MyNo \\
    % \hline
    Require further label mapping (verbalizer)? & \MyYes & \MyYes & \MyNo \\
    % \hline
    Embodies label information/output constraints? & \MyYes & \MyMaybe & \MyYes \\
    % \hline
    Can it be without additional context (input \textsc{X})? & \MyNo & \MyNo & \MyYes \\
    % \hline
    Sufficiently describe task semantics? & \MyNo & \MyNo & \MyYes \\
    % \hline
    End-user friendly? & \MyNo & \MyNo & \MyYes \\
    % \hline
    Instruction conciseness & Sentence-level (brief) & Sentence-level (brief) & Paragraph-level (complex) \\
    % \hline
    Instruction scope & Instance-wise & Instance-wise & Task-wise \\
    % \hline
    Modeling objective  & Textual Entailment  & Language Modeling~\footnotemark & Causal Language Modeling \\
    % \hline
    Source of indirect supervision & Textual Entailment Tasks & Pretraining Tasks & Various Text-to-Text Tasks \\
    % \hline

    \end{tblr}
    }
    \vspace{-1em}
    \caption{Comparison of three types of instructions.}
    \label{tab:comparison}
\end{table*}
\addtocounter{footnote}{-1}
\footnotetext{Usually not, but better performance after updating (see the importance of instruction tuning in \cref{subsec:instruction_tuning}).}
% \addtocounter{footnote}{1}
% \footnotetext{Usually not.}
\addtocounter{footnote}{1}
\footnotetext{Either \textit{masked language modeling} or \textit{causal language modeling}, depending on the PLM and prompt used.}
%



\section{How to Model Instructions?}
\label{sec:modeling}

Instructions are versatile, but how does the model learn to correctly \textit{understand} and \textit{follow} instructions?
% The Modeling Strategies for Instruction Learning
% \textit{How} to Use Instructions?
This section summarizes several popular modeling strategies for instruction learning. Overall, we introduce three different modeling schemes: for the earlier rule-based machine learning systems, the (i) \textbf{Semantic Parser-based} strategy was the commonly chosen method for encoding instructions; as the neural networks and the pre-trained language models emerged, the (ii) \textbf{Tuning-based} approach, which fine-tunes the models to follow the instructions, became a highly favored paradigm; recently, (iii) \textbf{HyperNetwork-based} method also garnered greater interest.
% (ii). \textbf{Prompting Template-based} and the (iii). \textbf{Prefix Instruction-based}


\subsection{Semantic Parser-based}
At the early stage of machine learning, to help the systems understand natural language instructions, a great number of works employed semantic parsing to convert the instruction into the formal language (logical formula), which can be easier executed by the systems (e.g., ``\textit{You can move any top card to an empty free cell}'' $\rightarrow$ ``\textit{card(x) $\wedge$ freecell(y) $\wedge$ empty(y)}'')~\citeiter{matuszek2012joint,babecs2012learning,chen2012fast,goldwasser2014learning}.
%\cite{liang2009learning,branavan2010reading,vogel2010learning,clarke2010driving,chen2011learning,matuszek2012joint,babecs2012learning,chen2012fast,goldwasser2014learning}
% Formally, the semantic parser-based approach can be written as:
% \[I^{'} = \textrm{Semantic Parser}(I)\]
% where the \(I^{'}\) is the converted logical formula.

For example, \citet{kuhlmann2004guiding} first tried to utilize natural language instructions to guide the systems to play soccer games, where they trained an individual semantic parser in advance, and then mapped the textual instructions into formal languages that can be used to influence the policy learned by the reinforcement learner. Since constructing a fully-supervised semantic parser requires laborious human annotations, the follow-up works also used indirect or weak supervision coming from the grounded environments (e.g., knowledge base) to train the semantic parser~\cite{eisenstein2009reading,chen2008learning,kim2012unsupervised,artzi2013weakly,krishnamurthy2013jointly}.
Besides using the converted formal languages to guide the systems to complete specific tasks, some works also utilized the logical formulae of the instructions to perform data and feature augmentations~\cite{srivastava2017joint,hancock2018training,wang2020learning,ye2020teaching}. We will further introduce this sort of application in~\cref{subsec:HCI}.




% \begin{table*}[t]
%     \centering
%     \resizebox{0.98\textwidth}{!}{
%     \begin{tabular}{c|c|c|c}
%          & Entailment-oriented & PLM-oriented & human-oriented \\\hline
%        indirect supervision  & textual entailment & language modeling & other text-to-text tasks\\\hline
%        instruction scope & instance-wise instruction & instance-wise instruction & task-wise instruction\\\hline
%        update PLM? & yes & usually no & yes\\\hline
%        need super large PLM? & no & yes & no\\\hline
%        training tasks & textual entailment & no & many text-to-text tasks\\\hline
%        end-user friendly? & no & no & yes\\\hline
%        need additional label mapping? & yes & usually yes & no\\\hline
%        has label information/output constraints? & yes & usually no & usually yes\\\hline
%        can be without task input $\textsc{X}$? & no & no & yes\\\hline
%     \end{tabular}
%     }
%     \caption{Comparison of three types of instructions. \textcolor{red}{@wenpeng: I have added three rows}}
%     \label{tab:my_label}
% \end{table*}


\subsection{Tuning-based}
\label{subsec:tuning-based}

As for the neural network-based systems, we can directly encode the natural language instructions into the model's embedding without the help of an additional semantic parser. What's more, benefiting from the rich prior knowledge, off-the-shelf PLMs could recover the task semantics conveyed in the instructions and perform zero-/few-short learning~\cite{radford2019language,brown2020language}. However, PLMs can not always successfully recognize and understand the instructions, especially for those complex human-oriented instructions~\cite{weller2020learning}. Therefore, more and more works try to fine-tune the models with instructions, i.e., instruction tuning~\footnote{Also known as ``instruction fine-tuning'', which means fine-tuning the model parameters instead of instruction optimization (prompt engineering).}.
% , e.g., tuning soft prompts~\cite{li-liang-2021-prefix,tsimpoukelli2021multimodal,han2021ptr}.

Multi-task instruction tuning is a representative strategy for the tuning-based method~\cite{wei2021finetuned,sanh2021multitask,wang2022benchmarking}. By converting the original task inputs into an instruction format (either prompted questions or prefix instructions), it fine-tunes the models on the massive multi-task datasets. Besides multi-task learning, recent works also conduct instruction tuning in a reinforcement learning manner~\cite{ouyang2022training}. Although instruction tuning still relies on training (i.e., gradient backpropagation), different from traditional supervised learning, it targets training models to follow instructions rather than completing specific tasks.  

Existing works have demonstrated the effectiveness of instruction tuning, where the fine-tuned models show better instruction-following ability than the off-the-shelf PLMs~\cite{mishra2022cross,chung2022scaling}. We will further discuss the instruction tuning in~\cref{subsec:instruction_tuning}.


% \subsection{Prompting Template-based}
% % to convert the task instructions with the original task input into a cloze question by applying a template.
% As for the neural network-based systems, we can directly encode the natural language instructions into the model's embedding without the help of a semantic parser. One of the prominent modeling strategies is using the prompting template. The essence of the prompting template-based approach is to use a template to convert the task input into a prompted format (i.e., cloze question~\footnote{Unlike the definition in~\citet{liu2023pre}, we regard any fill-in-blank question as the cloze, no matter whether the blank is in the middle or at the end of the question.}). According to the terminologies in~\cref{sec:categories}, the final input of LMs can be described as $x^{'}=f(x)$ or $f(x, I)$, i.e., the task-agnostic template $f(\cdot)$ is required but the task-specific information $I$ is optional. For example, $x=$~``\textit{I love this movie.}'' and $x^{'}=$~``\textit{I love this movie. It was~\blank}'', where there is no any task-specific information provided.
% ; Or $x=$~``\textit{I love this movie.}'' and $x^{'}=$~``\textit{I love this movie. It was~\blank}''

% The prompting template-based approach is particularly useful for modeling PLM-oriented and entailment-oriented instructions. Therefore, a lot of previous works employed this strategy to modeling prompts~\citeiter{petroni-etal-2019-language,jiang2020can,cui2021template,haviv-etal-2021-bertese,schick-schutze-2021-just}. Besides using the discrete prompting template, recent works also tried to tune the continuous templates and achieved incredible results~\citeiter{li-liang-2021-prefix,tsimpoukelli2021multimodal,han2021ptr}.



% \subsection{Prefix Instruction-based}

% Distinct from the prompting template-based approach, the prefix instruction-based method is mainly used for modeling human-oriented instructions, where sufficient task-specific information is provided~\citeiter{mishra2022cross,wang2022benchmarking,yin2022contintin,gu2022robustness}. Formally, in this case, the final input of LMs can be written as $x^{'}=I \oplus x$ or $f(x, I)$, which means the $I$ is required and has to be the prefix of the task input $x$, but the template language $f(\cdot)$ is optional.
% (we can simply consider $I$ as a prefix of $x$).
% task-agnostic
% when using prefix instruction-based modeling strategy
% For example, \citet{wang2022benchmarking} utilized the following template ``\textit{Definition:}~$\cdots$~\textit{Input:}~$\cdots$~\textit{Output:}'' as the $f(\cdot)$, where all the task-specific information $I$ is prepended with the task input $x$.
% to combine the prefix instruction $I$ and the task input $x$

\subsection{HyperNetwork-based}
There are two obvious problems in the instruction tuning. First, it usually concatenates the task-level instruction with every instance-level input, the repeating procedure significantly slowing down the processing/inference speed and the lengthy input also increasing the burden of computational cost~\cite{liu2022few}. Second, it can potentially impact the optimization because the model can not explicitly distinguish the task input ($\textsc{X}$) from the whole instructions ($\textsc{I}$); thus, the model can simply learn to complete the task and ignore the instructions~\cite{webson-pavlick-2022-prompt,deb2022boosting}.  
% One obvious problem in using the prefix instruction is the resulting lengthy input, which can lead to expensive computational costs and potential.
% Since the prefix instructions make the final inputs become lengthy

To address the above issues, recent works began to employ the hypernetwork~\cite{ha2016hypernetworks} to encode the task instructions~\cite{jin2020language,deb2022boosting,ivison2022hint}. The essences of using hypernetwork-based approach are (i) encoding the task instruction ($\textsc{I}$) and the task input ($\textsc{X}$) separately, and (ii) converting the instruction into task-specific model parameters. For example, \citet{ye2021learning} used the hypernetwork to convert the task instruction into several parameter-efficiency adaptors~\cite{houlsby2019parameter}. Then they inserted these adaptors behind the multi-head attention layers of the underlying LMs to perform cross-task generalization.
% used the hyper network to convert the task instruction into several parameter-efficiency adaptors for the underlying LMs and then inserted these adaptors behind the multi-head attention layers of LMs.



\section{Analyses}
\label{sec:analysis}

Instruction learning is proven to be effective in a lot of zero- and few-shot NLP tasks, but how to explain the impressive performance of instruction? And which aspects make a successful instruction learning procedure? To figure out the empirical rules in using instructions and better understand instruction learning, in this section, we summarize some insights from existing works for further research, i.e., some important factors that contribute to cross-task generalization. We also provide a roadmap (Table~\ref{tab:takeways}) for this section and conclude the takeaways to make it easy to refer to.
% including (i).\textbf{ \textit{What} makes it works} (i.e., some important factors that contribute to the cross-task generalization); (ii). \textbf{\textit{Why} it works.}

% In this section, we discuss this question in three aspects by displaying some relevant works.

% Recent works conducted extensive experiments to understand instruction learning. We summarize two aspects of these works: 
% Despite the promising zero- and few-shot performances of instruction learning, 
% \subsection{\textit{What} makes it works?}


% \begin{table}[t]
%     \centering
%     \scriptsize
%     \resizebox{0.99\width}{!}{
%     \begin{tcolorbox}
%     [tab2,tabularx={X|X}]
%     Recipe &  \\ \hline

%     \end{tcolorbox}
%     }
%     \vspace{-1em}
%     \caption{.}
%     \label{tab:takeways}
% \end{table}

\begin{table}[t]
    \centering
    \small
    \resizebox{0.99\linewidth}{!}{
    \begin{tblr}
    % set tablr args, including linewidth, v/h alignment
    {
        width=\linewidth, 
        colspec = 
            {
            X[l]
            },
        rowspec = 
            {
            |[2pt,MyBlue]Q[b]
            |[1pt,MyBlue]
            Q[m]Q[m]
            |[0.8pt,MyBlue,dashed]
            Q[m]Q[m]
            |[0.8pt,MyBlue,dashed]
            Q[m]Q[m]
            |[0.8pt,MyBlue,dashed]
            Q[m]Q[m]
            |[0.8pt,MyBlue,dashed]
            Q[m]Q[m]
            |[0.8pt,MyBlue,dashed]
            Q[m]Q[m]
            |[2pt,MyBlue]
            },
        row{1} = {bg=MyGrey!15, font=\bfseries},,
        rowhead = 1,
        hspan = minimal,
    }
    \SetCell{c} Recipes for Instruction Learning \\ 
    % \hline
    \colorbox{MyPink!13}{\textrm{{\em Instruction Tuning}}} (\cref{subsec:instruction_tuning}) \\
    \begin{varwidth}[t]{\linewidth}
    \begin{itemize}[topsep=0pt,parsep=0pt]
        \item Instruction-tuned LMs $>$ Vanilla LMs.
        \item Instruction tuning tames LMs to be more safe, robust, and user-friendly. \strut
    \end{itemize}
    \end{varwidth} \\
    \colorbox{MyPink!13}{\textrm{{\em Instruction Consistency}}} (\cref{subsec:consistency}) \\
    \begin{varwidth}[t]{\linewidth}
    \begin{itemize}[topsep=0pt,parsep=0pt]
        \item Keep instruction paradigm consistent during training and testing (e.g., conciseness). \strut
    \end{itemize}
    \end{varwidth} \\
    \colorbox{MyPink!13}{\textrm{{\em Model and Task Scale}}} (\cref{subsec:scale}) \\
    \begin{varwidth}[t]{\linewidth}
    \begin{itemize}[topsep=0pt,parsep=0pt]
        \item Larger LMs benefit more from instructions. 
        \item Try to tune LMs on more diverse tasks.
        \item Model scale seems to outweigh task scale. \strut
    \end{itemize}
    \end{varwidth} \\
    \colorbox{MyPink!13}{\textrm{{\em Instruction Diversity}}} (\cref{subsec:diversity}) \\
    \begin{varwidth}[t]{\linewidth}
    \begin{itemize}[topsep=0pt,parsep=0pt]
        \item Design multiple instructions for one task in different wordings and perspectives. 
        \item Feeling exhausted about promoting diversity? Resort to the LLMs! \strut
    \end{itemize}
    \end{varwidth} \\
    \colorbox{MyPink!13}{\textrm{{\em Taxonomies and Situations}}} (\cref{subsec:situation}) \\
    \begin{varwidth}[t]{\linewidth}
    \begin{itemize}[topsep=0pt,parsep=0pt]
        \item Identify the traits of the target problem and choose instructions accordingly.
        \item Have no ideas? Just mix them!  \strut
    \end{itemize}
    \end{varwidth} \\
    \colorbox{MyPink!13}{\textrm{{\em Model Preference}}} (\cref{subsec:preference}) \\
    \begin{varwidth}[t]{\linewidth}
    \begin{itemize}[topsep=0pt,parsep=0pt]
        \item Better design your instructions in a model's language (e.g., conforming to the pertaining objectives). \strut
    \end{itemize}
    \end{varwidth} \\

    \end{tblr}
    }
    \vspace{-1em}
    \caption{The takeaways. We summarize some high-level suggestions for successful instruction learning.}
    \label{tab:takeways}
\end{table}


\subsection{Instruction Tuning} 
\label{subsec:instruction_tuning}

We first emphasize the importance of instruction tuning.
% which trains models on instruction datasets to update the parameters.
As we have introduced in~\cref{subsec:tuning-based}, instruction tuning is one of the most popular modeling strategies of instruction learning.
It usually trains the LMs on various instruction datasets, where each input is equipped with task instruction, to drive the models to learn to follow the instructions. A lot of works demonstrated that multi-task instruction-tuned LMs could better follow the instructions of the unseen tasks compared with no-tuned LMs~\citeiter{wei2021finetuned,sanh2021multitask,yin2022contintin,chung2022scaling,prasad2022grips}.

% Different from the traditional fine-tuning, which aims at training model to complete specific tasks (i.e., $x \rightarrow y$), instruction tuning trains the LMs on various instruction datasets where each input is converted into an instruction style by using the prompting template $f(\cdot)$ (i.e., either prefix instruction or cloze instruction~\cite{liu2023pre}), to drive the models to learn to follow the instruction (i.e., $f(x,I) \rightarrow y$). Existing works demonstrate that multi-task instruction-tuned LMs could better follow the instructions of the unseen tasks compared with no-tuned LMs~\citeiter{wei2021finetuned,sanh2021multitask,yin2022contintin,chung2022scaling,prasad2022grips}.

However, since previous works have also shown that a massive multi-task training procedure also benefits the downstream tasks learning of LMs~\cite{mccann2018natural,aghajanyan-etal-2021-muppet,aribandiext5}, there is always a question that ``\textit{Whether instruction tuning or multi-task learning plays a key role in cross-task generalization}''.
%
To answer this question, we first introduce the work of \citet{weller2020learning}, who solely tuned LMs with a multi-task learning paradigm and discovered that the LMs could find it hard to follow the instructions of the unseen tasks.
% first conducted experiments to investigate whether LMs can follow the 
\citet{wei2021finetuned,sanh2021multitask} further conducted in-depth comparison between multi-task instruction tuning and multi-task learning on cross-task generalization. They found that instruction tuning is the key to cross-task generalization rather than multi-task learning itself.
% found that LMs can gain promising performance on the challenging unseen tasks with the help of instruction tuning. They also 
%
% \cite{sanh2021multitask} even shown that relative small LMs can also learn from instructions

Besides the performance gains on unseen tasks, instruction tuning has many other benefits. For example, \citet{wei2021finetuned} showed that instruction fine-tuned LMs performed better on following the soft instructions. Meanwhile, \citet{longpre2023flan} compared the convergences of Flan-T5 and T5 on single-task fine-tuning, and they found that instruction fine-tuned Flan-T5 learned faster than T5 on the downstream single-task fine-tuning. What's more, some works also found that instruction tuning makes LMs robust to some tiny perturbations in the instructions, such as the wordings~\cite{sanh2021multitask} and the paraphrasing~\cite{gu2022robustness}. While off-the-shelf LMs are usually sensitive to the small instruction perturbations~\cite{efrat2020turking,weller2020learning}, thus they require laborious prompt engineering~\cite{bach2022promptsource}. All in all, instruction tuning tames the LMs to become much more user-friendly~\cite{chung2022scaling}.

Despite these attractive attributes, instruction tuning still relies heavily on massive task-specific training, which is costly and could be sub-optimal for instruction learning. We will further discuss this point in~\cref{subsec:explicit_objective}.

% claimed that multi-task instruction tuning bring benefits to the down-stream traditional fine-tuning on the unseen single task; 
% Instruction tuning makes model robust to some tiny perturbation (except the instruction-level huge perturbation), which can potentially well address the problem of the previous work~\cite{efrat2020turking}; And also, instruction tuning makes the model robust to the paraphrasing.
 % Multi-task instruction-based tuning bring benefits to the large LMs, but hurts the performance of small LMs. \textcolor{red}{This point is conflict with~\cite{sanh2021multitask}, see the comments under~\cite{sanh2021multitask} for details}; 
% \cite{wei2021finetuned} The instruction seems to be the key for multi-task prompted tuning, rather than simply the multi-task learning (i.e., multi-task prompted tuning $>$ multi-task learning);
% \cite{wei2021finetuned} Instruction-tuned models respond better to continuous inputs from prompt tuning (i.e., the model can benefit more from soft prompt, because I think the model have more explicit awareness on the prefix information);
% Instruction-tuned model can easier benefit from the informative task-specific instructions, compared with non-tuned model, indicating \textbf{the importance of instruction-based tuning for instruction following}~\cite{prasad2022grips}
% \cite{sanh2021multitask} Multi-task prompted training make model robust to the wordings of prompt, while the number of dataset doesn't matter the robustness.. 

\subsection{Instruction Consistency} 
\label{subsec:consistency}

Keeping the instruction paradigm (e.g., conciseness) consistent is also crucial in instruction learning, e.g., if you tune the LMs with short instructions, you will also need to keep the instructions short when testing, and vice versa. \citet{wei2021finetuned} first investigated the performance impact of changing the instruction paradigm. They found that, the LMs tuned on the short instructions (i.e., task name) can not generalize to those longer sentence-style instructions (\texttt{short $\rightarrow$ long}).
% the performances of the LMs tuned with short task names dropped when evaluating with more extended sentence-style instructions (i.e., task names $\rightarrow$ sentence-style instructions), compared with the results of keeping the paradigm (i.e., task names $\rightarrow$ task names, or sentence-style instructions $\rightarrow$ sentence-style instructions). 
%
Similarly, \citet{gu2022robustness} observed the performance dropping when changing paragraph-style instructions to shorter sentence-style instructions at the test phase (\texttt{long $\rightarrow$ short}), further indicating the importance of instruction consistency during training and testing.
% \textbf{Instruction paradigm} is the key for instruction tuning, e.g., the model pre-tuning on paragraph-level instructions produces huge performance drop when evaluating on the concise prompt.

Besides discrete instruction, keeping the instruction paradigm is also critical for soft instruction. For example, \citet{xu2022zeroprompt} showed that the LMs fine-tuned with continuous instruction also require a \textbf{same-size} prefix embedding when testing on unseen tasks, even if this embedding is randomly initialized.
%, which means keeping the formats of instructions consistent during the evaluation
%\cite{xu2022zeroprompt} soft prompt of evaluation tasks is randomly initialized due to no labels. but even random initialization, keeping the paradigm, can achieve promising results.
Interestingly, similar results were also found in the few-shot demonstrations (i.e., in-context learning). For instance, \citet{min2022rethinking} concluded that breaking the demonstration paradigm (i.e., removing the \textsc{Y}) significantly harms the performance of MetaICL~\cite{min-etal-2022-metaicl}, which tuned with (\textsc{X}, \textsc{Y}) pairs. Furthermore, \citet{iyer2022opt} found that the number of demonstrations should also not be changed during evaluation (e.g., using two demonstrations in tuning and three in testing would result in lower performance, compared with using two demonstrations in testing).
% \cite{chung2022scaling} Instruction-tuned LM is user-friendly and does not need prompt engineering or few-shot exemplars required by the conventional off-the-shelf LM.
% for in-context learning, keeping the (x,y) pair format is the most important factor, compared with removing y~\cite{min2022rethinking}. \citet{iyer2022opt} also found 
% \cite{wei2021finetuned} The experiments also illustrates that the \textbf{instruction paradigm} is really important for instruction-based tuning (my personal observation, cf. Figure 8 in page 7).


% \begin{figure}[htbp]  
% \centering
% \subfigcapskip=12pt  
% \vspace{-0.5cm}  
% \subfigure[a]{
% \includegraphics[width=0.5\columnwidth]{picture/Flan-PaLM.pdf}
% }
% \hspace{12.6mm}  
% \subfigure[b]{
% \includegraphics[width=0.5\columnwidth]{picture/Flan-T5.pdf}
% }
% \vspace{-0.3cm} 
% \caption{We}
% \label{fig:scale}
% \end{figure}

% \begin{figure}[htp]
% \subfloat[Flan-PaLM]{%
%   \includegraphics[clip,width=\columnwidth]{picture/Flan-PaLM.pdf}%
% }

% \subfloat[Flan-T5]{%
%   \includegraphics[clip,width=\columnwidth]{picture/Flan-T5.pdf}%
% }
% \caption{We}
% \end{figure}

% \begin{figure}[htp]

% \begin{subfigure}{1.0\textwidth}
% \includegraphics[trim=70 30 90 60,clip,width=\textwidth,height=8cm]{picture/Flan-PaLM.pdf}
% \caption{Figure A}
% \end{subfigure}

% \bigskip

% \begin{subfigure}{1.0\textwidth}
% \includegraphics[trim=70 30 90 60,clip,width=\textwidth,height=8cm]{picture/Flan-T5.pdf}
% \caption{Figure B}
% \end{subfigure}

% \caption{Main caption}

% \end{figure}

% \begin{figure}
% \centering     %%% not \center
% \subfigure[Figure A]{\label{fig:a}\includegraphics[width=40mm]{picture/Flan-PaLM.pdf}}
% \subfigure[Figure B]{\label{fig:b}\includegraphics[width=40mm]{picture/Flan-T5.pdf}}
% \caption{my caption}
% \end{figure}


% =============== subfigure ====================
% \begin{figure}[htp]
% \centering 

% \subfloat[The scale of Flan-PaLM.]{%
%   \includegraphics[trim=0 10 0 0,width=0.8\columnwidth]{picture/Flan-PaLM.pdf}%
% }

% \vspace{-1em}

% \subfloat[The scale of Flan-T5.]{%
%   \includegraphics[trim=0 10 0 0,width=0.8\columnwidth]{picture/Flan-T5.pdf}%
% }

% \vspace{-2em}
% \end{figure}


% ==================== single figure * 2 ====================
% \begin{figure}[htp]
% 	\begin{center}
% 		\centering
% 		\includegraphics[width=1.0\columnwidth, trim=10 25 0 0]{picture/Flan-PaLM.pdf}
% 	\end{center}
% 	\caption{.}
%     % , to perform binary classification on each label.
%     % unify conventional classification problems into an entailment-based paradigm
%     % that is initially designed for human annotations
%     % \vspace{-0.8em}
% 	\label{fig:scale_a}
% \end{figure}



\subsection{Model and Task Scale}
\label{subsec:scale}

Recent works demonstrated that the scale significantly impacts the generalization performance of instruction tuning, including the scale of model parameters and tuning tasks~\footnote{Task scale refers to the numbers and categories of tasks.} ~\citeiter{wei2021finetuned,sanh2021multitask,mishra2022cross,wang2022benchmarking,xu2022zeroprompt,deb2022boosting,prasad2022grips,chung2022scaling,iyer2022opt,longpre2023flan}.
% 
A representative work among them is~\citet{chung2022scaling}, who conducted extensive experiments with 1,836 tuning tasks and 540B models. The results illustrated that the cross-task performance takes advantage of both factors~\footnote{Worth noting that the benefits of the model scale seem to outweigh the task scale. Please refer to the Fig.~4 of~\citet{chung2022scaling} and the follow-up work of~\citet{longpre2023flan}.}, suggesting the research community continue scaling the instruction learning. 
%
However, the super large model scale is usually unaffordable for most groups, and it also leads to huge carbon emissions~\cite{strubell-etal-2019-energy,schick-schutze-2021-just}, making it unrealistic in most real-world scenarios.
%
Accordingly, recent works began to investigate a more efficient way to address the model scale problem, such as the parameter-efficient fine-tuning~\cite{hu2021lora,liu2022few,lialin2023scaling}.
%
% For example, \citet{jang2023exploring} employed the instruction-level experts (adaptors) to fine-tune LMs on a single task, which outperformed the multi-task fine-tuned LMs on the unseen tasks.
For example, \citet{jang2023exploring} only fine-tuned partial parameters of the whole LMs (instruction-level experts), which outperformed the full-model-tuning on the unseen tasks, with less compute cost.
%
As for extending the task scale, a feasible solution is data augmentation, e.g., \citet{longpre2023flan} adapted the idea of ``noisy channel''~\cite{min2022noisy} that extended the tuning task scale by simply inverting the input-output of instance and achieved a presentable performance improvement.
% be used as a good data augmentation to create ``new'' tasks, thus increasing task diversity;
% \paragraph{The Scale of Model} 
% \cite{longpre2023flan} \textbf{Large} Language model can benefit more from scaling tasks.
% \cite{chung2022scaling} (1). The scaling of task number and model parameters is important for instruction learning (1,800+ tasks, 540B parameters), which can help better generalize to unseen tasks.
% \cite{wei2021finetuned} Multi-task instruction-based tuning bring benefits to the large LMs but hurts the performance of small LMs. \textcolor{red}{This point is conflict with~\cite{sanh2021multitask}, see the comments under~\cite{sanh2021multitask} for details};
% \paragraph{The Scale of Tasks}
% \cite{longpre2023flan} task quantity is very important; even small models (T5-base) can benefit from it
% \cite{iyer2022opt} The scale of task number and category is overall beneficial
% \cite{chung2022scaling} The positive return of task number diminishes when it reaches a specific upper bound.
% \cite{wei2021finetuned} The number of tasks and categories is important.
% \cite{sanh2021multitask}  Number of training datasets is important when evaluating on the challenging BIG-bench~\cite{srivastava2022beyond}, which requires novel skills.
% \cite{xu2022zeroprompt} The task quantity is important.
% \citet{jang2023exploring} even claims merely one task for tuning is enough.


\subsection{Instruction Diversity} 
\label{subsec:diversity}

Instruction diversity (i.e., for the same task, write multiple instructions in various ways) at fine-tuning phase also affects the cross-task performance and robustness of LMs~\cite{chung2022scaling,longpre2023flan}. 
%
Notably, \citet{sanh2021multitask} fine-tuned T5~\cite{raffel2020exploring} model on the multi-task datasets, where each task dataset is associated with various instructions collected from ``Public Pool of Prompts'' (P3)~\cite{bach2022promptsource}. These instructions are written by different people with distinct perspectives (but in similar conciseness)~\footnote{See Appendix G of~\citet{sanh2021multitask} for more details.}. By varying the number of instructions per dataset used in fine-tuning, \citet{sanh2021multitask} found that the model fine-tuned with more diverse instructions achieved better and more robust performance on the unseen tasks. What's more, \citet{sanh2021multitask} also found that the instruction diversity could compensate for the limited model scale, i.e., a relatively small LMs (T0-3B) could still benefit from multi-task fine-tuning due to the mixture of diverse instructions~\footnote{While \citet{wei2021finetuned} only used a fixed number of instructions and found that instruction tuning harmed the performance of smaller LMs (Flan-8B).}.
%
% \cite{sanh2021multitask} Prompt diversity (e.g., length, creativity) is very important, prompted tuning LMs with fixed number of dataset, but with \textbf{more diverse prompt templates can lead to a better performance}; Owing to the diverse prompt, \textbf{\textcolor{red}{smaller LMs (T0-3B) can also benefit from multi-task prompted tuning}} (this is conflict with~\citet{wei2021finetuned}, who find that only large LMs can benefit from instruction-tuning, the held-out performance of smaller LM (Flan-8B) decreases after tuning, inter alia);
% \cite{sanh2021multitask} Training on more prompts per dataset leads to better and more robust generalization to held-out task; (that's reasonable, the mixture of diverse instruction resources make strong held-out generalization~\cite{chung2022scaling,longpre2023flan});

% Since human-written instructions are found to contain the bias that could impair the annotation creativity inevitably
Nevertheless, manually crafting instructions with diversity is expensive and usually hard to achieve~\cite{huynh2021survey,parmar2022don}.  Since large-scale PLMs, such as ChatGPT and GPT-4, exhibited excellence in automatic annotations with lower charges than human annotators~\cite{OpenAI2023GPT4TR,he2023annollm,pan2023gpt4reward}, recent works began to employ models to compose innovative instructions~\cite{zhang2020analogous,zhang2021learning,honovich2022unnatural,honovich2022instruction,ye2022guess,alpaca,peng2023gpt4llm,koksal2023longform}. For instance, \citet{wang2022self} tried to drive GPT-3~\cite{brown2020language} to generate quantitative instructions from scratch iteratively. Although the self-generated instructions contain more noise, owing to the creative task types and diverse verb-noun structures~\cite{kitaev2018constituency}, these instructions could still bring benefits to tuning the GPT-3 and show complementary effects with the human-written instructions. These results imply the profitability of instruction diversity, even at the expense of the correctness of instructions. We will further discuss it in~\cref{subsec:Explainability}.
% diverse formats, i.e., various verb-noun structures~\cite{kitaev2018constituency} and lengths,
% \cite{wang2022self} self-generate instructions are quantitative and have good diversity.
%as mentioned in~\cite{wang2022self})

\subsection{Instruction Taxonomies and Situations}
\label{subsec:situation}

As we have introduced in~\cref{sec:categories}, there are several kinds of textual instructions. Although they were all widely adapted by the previous works, different taxonomies show various-degree effects.
% 
For example, existing works found that adding positive few-shot demonstrations in the textual instructions could lead to a significant performance improvement on the unseen tasks~\cite{mishra2022cross,wang2022benchmarking,yin2022contintin,deb2022boosting,gu2022robustness}, especially for the tasks occupying complex output space~\cite{wei2021finetuned}. Surprisingly, \citet{gu2022robustness} further found that combining incorrect instructions with correct demonstrations could outperform using correct instruction without demonstrations, indicating the key role of demonstrations in instruction learning.
% To take a closer look, \citet{gu2022robustness} concatenated the gold-standard demonstrations with the random task instructions to tune and evaluate the LMs. Surprisingly, 
This prominence is perhaps because the LMs prefer to exploit the more superficial aspects of the demonstrations rather than the other complex contents~\citep[cf.][]{min2022rethinking}.
% It is also noteworthy that the 
% \textcolor{red}{TODO: paragraph-style vs. sentence-style?}

In addition, the effectiveness of different instruction taxonomies also highly depends on the target evaluation tasks. For example, the concise cloze-style instructions are useful on tasks that can be naturally expressed as instructions (e.g., QA), while it seems to be redundant when facing language modeling tasks~\citep[cf.][]{wei2021finetuned}. What's more, CoT explanations seem to be necessary only for tasks that require multi-steps reasoning~\citep[cf.][]{kojima2022large}. To this end, a practical suggestion is to mix different instruction taxonomies when tuning the LMs, which has been proven to be efficient in tackling the various target evaluation tasks~\cite{chung2022scaling,iyer2022opt,longpre2023flan}.
% \cite{gu2022robustness} Demonstration has a dominant impact on the performance compared with task definitions or prompts, even random task definition conditioned, which perhaps show the importance of demonstration-free instruction learning.
% \cite{chung2022scaling} \textbf{Prompting paradigm} is key for instruction learning. Instruction-tuning has to combine much more diverse instruction styles to meet the different evaluation benchmarks. That's why the mixture of CoT can significantly improve the model generalization on the reasoning tasks (this point is in line with~\citet{gu2022robustness}).
% \cite{longpre2023flan} The effect of different instruction sources is different, and the effect of Flan 2021 is the best; SuperNI may cause the low return of scaling due to the unique instruction format;
% \cite{iyer2022opt} Different benchmarks complementing each other. That's why~\cite{longpre2023flan} choose to mix and balance various sources;
% \cite{iyer2022opt} Reasoning data is obviously useful (i.e., CoT) for reasoning evaluation and also bring benefits to some other task types
% \cite{wei2021finetuned,kojima2022large} CoT helps over direct answer-only prompting the most when tasks require multiple steps of reasoning and when used with a large-enough language model. Such tasks include challenging math problems or symbolic manipulation problems that require multiple steps.
% \cite{iyer2022opt} Dialogue and in-context learning objective~\cite{min2022metaicl} can harm the performance, that is, weakened model's instruction-following ability to conform to the required answer format, especially for those tasks whose label space is a special set of decision words (this potentially indicate the reliance on demonstrations); MetaICL particularly harm the generation tasks while benefiting the classification tasks, because it makes model tend to generate short answers and ignore the output pattern in the presence of in-context exemplars. MetaICL with the suffix loss outperforms regular MetaICL.
% \cite{deb2022boosting} When facing the weak generalization (cross-dataset), \textbf{demonstrations seems sufficient}; but when testing on strong generalization evaluation set (cross-category), \textbf{detailed instruction are more necessary}. the diversity of tasks is important. 
% \cite{wei2021finetuned} The mix of demonstrations is good, it is especially effective for tasks with large/complex output spaces. And also, demonstrations make model robust to the wordings of the prompt templates when testing (because I think the model tends to prefer demonstrations instead of prompt~\cite{gu2022robustness});


% \subsection{\textit{Why} it works?}
% \subsection{Pre-training Objective and Knowledge}
\subsection{Model Preference}
\label{subsec:preference}

Another factor that can enhance the cross-task performance is making instructions conform to the \textit{preference} of LMs, that is, converting the instructions into model-oriented styles.
% Specifically, for model-oriented instruction, it should accord with the pre-training objective of PLM. 

Since the current instruction learning paradigm mainly employs the PLM as the backbone of the system, one of the potential explanations for why PLM-oriented instruction (i.e., prompt) can work is that prompt recalls the pre-training objective and activates the task-specific knowledge of the PLM.
% One of the potential explanations for why the instruction can work is that
% This conjecture is drawn from existing works
% especially for the PLM-oriented instructions (i.e., prompts).
Some of the existing works demonstrated the importance of conforming to the pre-training objective of PLM when doing instruction tuning~\cite{tay2022unifying}. For example, \citet{schick2021exploiting,schick-schutze-2021-just} proposed the idea of \textit{pattern exploit training} (PET), which used a prompt to convert the original task inputs into cloze-style questions and then fine-tuned the PLM on instruction datasets with the masked language modeling objective. They found that taking advantage of recalling the pre-training objective, relatively small LMs, such as ALBERT~\cite{lan2019albert}, can outperform GPT-3 on the SuperGLUE benchmark~\cite{wang2019superglue}. Furthermore, \citet{iyer2022opt} found that the PLM could perform better on the unseen tasks after mixing a small proportion of pretraining-style data in the instruction tuning dataset. \citet{sanh2021multitask,wei2021finetuned} also found that the PLM was more likely to fail at the tasks whose objective differs from the language modeling but improved by adopting cloze-style instructions. 
% These works all implied the reason of 
All these results align with the empirical rules of prompt engineering~\cite{liu2023pre}, which highlights the importance of aligning the prompts with the PLM~\footnote{Using prefix prompts for the auto-regressive LMs, while using cloze prompts for the masked LMs. Please refer to ~\citet{liu2023pre} for more details.}.

Besides the objective of instruction tuning, the way of designing instructions is also found critical. To better cater to the model's preference, recent works began employing continuous embedding (i.e., soft instructions) instead of human-understandable discrete instructions.~\citeiter{lester2021power,liu2021gpt,ye2022retrieval}.
% choosing the appropriate PLM and 
% recent works also tried to design the instructions by following the model's preference
Similar conclusions are also found in the human-oriented instructions, where the PLM constantly fails at following the human-oriented instructions but gains significant improvements after reframing the instructions to cater to the model's preference~\cite{mishra-etal-2022-reframing,prasad2022grips,gonen2022demystifying,deng-etal-2022-rlprompt,wang2022self}. Despite the performance profits, it is still controversial whether it is worthwhile to convert the original human-oriented instructions into PLM-oriented style, because it impairs the interpretability of instructions and is highly contrary to human intuition~\cite{khashabi2022prompt,webson-pavlick-2022-prompt,prasad2022grips}. We will further discuss it in~\cref{subsec:Explainability}.

% on using PLM-oriented instructions 
% \cite{sanh2021multitask} As a byproduct of learning to predict the next word, a language model is forced to learn from a mixture of implicit tasks included in their pretraining corpus. For example, by training on generic text from a web forum, a model might implicitly learn the format and structure of question answering. This gives large language models the ability to generalize to held-out tasks presented with natural language prompts, going beyond prior multitask studies on generalization to the held-out datasets.
% \cite{iyer2022opt} Adding a certain-proportion pre-training data can improve the results, indicating that recalling the pre-training objective during the instruction-tuning can have some benefits;
% \cite{wei2021finetuned} (1). instruction tuning is \textbf{very effective} on tasks naturally verbalized as instructions (e.g., NLI, QA, translation, struct-to-text) and is \textbf{less effective} on tasks directly formulated as language modeling, where instructions would be largely redundant (because the task input itself aligns with the pre-training objective, e.g., cloze). This point is also enhanced by~\citet{sanh2021multitask};
% \cite{schick-schutze-2021-just} Convert SuperGLUE~ into cloze style, and fine-tune the small language model. What instruction: Prompts (cloze-style template). What method: Prompted fine-tuning.

% \paragraph{Pre-training Knowledge}
% \cite{wang2022self} Although there is a lot of noise in the generated instructions, it can help the model get competitive zero-shot results on the SuperNI test set~\cite{wang2022benchmarking} compared with the other LLM trained on manually-written instruction dataset. (3). The results are consistent with using additional training data from the SuperNI, proving its complementary effect with the training set of SuperNI.
% \cite{zhao2021calibrate} \textbf{Common Token Bias}. The model often predicts common entities such as “America” when the ground-truth answer is instead a rare entity. That's probably why the target word matters (similar to the observation of~\citet{webson-pavlick-2022-prompt}).

% \paragraph{shallow pattern copy} 
% \cite{min2022rethinking}
% \cite{zhao2021calibrate} (1). \textbf{Majority Label Bias}, the model tends to copy the most frequent label in the examples, that's probably why the choice of example matters (similar to the observation of~\cite{min2022rethinking}); (2). \textbf{Recency Bias}. The tendency to repeat answers that appear towards the end of the prompt. That's probably why the order matters. And this recency bias can outweigh the majority label bias.




\section{Applications}
\label{sec:app}

\subsection{Human-Computer Interaction}
\label{subsec:HCI}

Textual instructions can be naturally regarded as a human-computer interaction method. Numerous previous works employed natural language instructions to ``guide'' the computer to perform various real-world tasks. 
%

For the non-NLP (multi-modal) tasks, most focused on environment-grounded language learning, i.e., driving the agent to associate natural language instructions with the environments and make corresponding reactions, such as selecting mentioned objects from an image/video~\cite{matuszek2012joint,krishnamurthy2013jointly,puig2018virtualhome}, following navigational instructions to move the agent~\cite{tellex2011approaching,kim2012unsupervised,chen2012fast,artzi2013weakly,bisk2016natural}, plotting corresponding traces on a map~\cite{vogel2010learning,chen2011learning}, playing soccer/card games based on given rules~\cite{kuhlmann2004guiding,eisenstein2009reading,branavan2011learning,babecs2012learning,goldwasser2014learning}, generating real-time sports broadcast~\cite{chen2008learning,liang2009learning}, controlling software~\cite{branavan2010reading}, and querying external databases~\cite{clarke2010driving}, etc.
% generate sportscasting~\cite{}, 
Meanwhile, instructions are also widely adapted to help communicate with the system in solving NLP tasks, e.g., following instructions to manipulate strings~\cite{gaddy2019pre}, classifying emails based on the given explanations~\cite{srivastava2017joint,srivastava2018zero}, and text-to-code generation~\cite{acquaviva2021communicating}.

Recently, a growing body of research tended to design the human-computer communication procedure in an \textbf{iterative} and \textbf{modular} manner. For example, \citet{li2020interactive} built a system to help the users tackle daily missions (e.g., ordering coffee or requesting Uber). Benefiting from a user-friendly graphical interface, the system can iteratively ask questions about the tasks, and users can continually refine their instructions to avoid unclear descriptions or vague concepts. Similarly, \citet{dwivedi2022editeval} proposed a benchmark to iteratively instruct the PLM to improve the text, where each iteration only used a small piece of instruction with a precise purpose (e.g., ``\textit{Simplify the text}'' or ``\textit{Make the text neutral}''). Besides, \citet{chakrabarty2022help} constructed a collaborative poem-writing system, where the user could initially provide an ambiguous instruction (e.g., ``\textit{Write a poem about cake}'') and then incrementally refine the instruction with more details (e.g., ``\textit{Contain the word -- `chocolate'} '') by observing the model's intermediate outputs. Meanwhile, \citet{mishra2022help} proposed a biography generation system\footnote{\citet{mishra2022help} actually experimented with more than 60 text generation tasks.} that progressively collected the necessary personal information from the users (by asking questions in a dialogue scene to guide the users) and generated a paragraph-style bio finally.
%
% Since the non-experts users usually lacks necessary knowledge to write model-preferred instructions, and the current SOTA AI models are also sensitive to the instructions 
As it is usually hard for non-expert users to write sufficient instructions in one shot, adapting an iterative and modular paradigm in designing instruction-based AI systems can help guide the users to enrich the task instruction step by step. Thus, this paradigm efficiently relieves the thinking demands of users and leads to a more user-oriented system. Due to its practical values, we emphasize the importance of this branch of work in this paper.

\subsection{Data and Feature Augmentation}

Task instructions are regarded as indirect supervision resources where sometimes superficial and assertive rules are embedded. These rules are also known as \textit{labeling functions} that can be directly applied for annotations (e.g., the sentence ``\textit{a very fair price}'' is sentiment positive because \textit{``the word `price' is directly preceded by `fair' ''}).
% This kind of instruction (usually some explanations). 
% Some task instructions (i.e., explanations) also contain superficial and assertive rules for annotations, which are usually known as ``\textit{labeling functions}''.
Therefore, some existing works also employed the instruction as a distant supervision to perform data or feature augmentation~\cite{srivastava2018zero,hancock2018training,ye2020teaching}.
% , which can be used as the labeling function to annotate corpus to used as data augmentation automatically.
For instance, \citet{srivastava2017joint} used a semantic parser to convert natural language explanations into logical forms, and applied them on all instances in the dataset to generate additional binary features. While \citet{wang2020learning} utilized the label explanations to annotate the raw corpus automatically and trained the classifier on the resulting noisy data. 

Besides the straightforward augmentation, \citet{su2022one} further used the task instruction to enrich the model representation and achieved strong cross-task generalization. Specifically, they trained an embedding model (a single encoder) on the diverse instruction datasets with contrastive learning, and then used this model to produce task-specific representations based on the instruction for the downstream unseen tasks.
% extended this kind of feature augmentation to a cross-task generalization scene

% All of them achieved admirable results on various NLP tasks.
% \citet{hancock2018training,wang2020learning,ye2020teaching} Authors use semantic parser to convert the labeling explanation of each sample in the training set into formal language (labeling function), and then annotate the raw corpus automatically by using the labeling functions, thus generating some noisy data to train the classifier (thus it is a semi-supervision strategy); Similarly, \citet{srivastava2017joint} converts explanations to logical forms, and then use them to create binary features to enhance the original inputs. - What instruction: natural language \textbf{explanations} (labeling function, explain the reason of annotation), e.g., ``Positive, because the words "very nice" is within 3 words after the TERM''. - What method: Annotate the explanations on the original CLS datasets, use these human-written instructions as the rule of distance supervision, to generate noisy samples (data argument). - What task (input,output): Mostly CLS tasks. Use explanation as an additional supervision.

\subsection{Generalist Language Models}

% Another high-profile application of instruction learning is to build generalist language models.
% \cite{su2022one} use instructions to build a single embedding model to construct a task-level semantic space, which can be used for various downstream tasks.
According to the definition of Artificial General Intelligence (AGI), the ``generalist model'' is usually a system that can be competent for different tasks and scalable in changeable contexts, which shall go far beyond the initial anticipations of its creators~\cite{wang2007introduction,goertzel2014artificial}.
%
While specific to the NLP domain, a generalist language model is supposed to be an excellent multi-task assistant, that is skilled in handling a variety of real-world NLP tasks and different languages, in a completely zero/few-shot manner~\cite{arivazhagan2019massively,pratap2020massively,wei2021finetuned}.
%
As numerous existing works demonstrated the incredible power of using instructions in cross-task generalization~\citeiter{wei2021finetuned,sanh2021multitask,mishra2022cross,wang2022benchmarking,chung2022scaling}, the instruction is likely to become a breakthrough in achieving this ultimate goal. 

Notably, the recent two remarkable applications of instructions, namely InstructGPT~\cite{ouyang2022training} and ChatGPT~\footnote{\url{https://chat.openai.com/}}, also indicated a big step towards building generalist language models. However, unlike the other works that mainly employ instruction learning, ChatGPT also adopts some other components, e.g., reinforcement learning with human feedback (RLHF)~\footnote{At the time of writing, there is no published paper about ChatGPT. Thus, our discussion is mainly based on the underlying techniques of InstructGPT because they share similar philosophies. See OpenAI's blog for more details: \url{https://openai.com/blog/chatgpt}}.
% open-ended generation and 
Although the answer to ``{which component contributes more to the dramatic results of ChatGPT}'' remains ambiguous and needs further investigation, we introduce some recent works highlighting the critical role of instruction learning.
%
For example, \citet{chung2022scaling} conducted extensive experiments to evaluate the human-preference alignments of PaLM~\cite{chowdhery2022palm}. They found that, even without any human feedback, the instruction tuning significantly reduced the toxicity in the open-ended generations of PaLM, such as gender and occupation bias. In addition, some other works also solely employed creative instructions instead of human feedback and achieved notable cross-task results~\cite{bai2022constitutional,honovich2022unnatural,wang2022self}.
% instruction fine-tuned language model. Besides the decent cross-task generalization performance, instruction tuning also significantly reduced the toxicity in the open-ended generation of LMs, such as gender and occupation bias, achieving human-preference alignment even without any human feedback. 
% (e.g., Flan-PaLM)

Although ChatGPT still suffers from many unsatisfactory aspects and is far from the generalist language model~\cite{qin2023chatgpt,guo2023close,Kocon2023ChatGPTJO,wang2023robustness}, we hope the goal of AGI can continue to be promoted by adopting and evolving more powerful techniques, including instruction learning.




% \section{Challenges}
% \subsection{Explainability of Instructions}
% \subsection{Transferability of Instructions}

\section{Challenges and Future Directions}
\label{sec:challenges}

% \subsection{Modeling Strategies}
% Soft instruction is unstable and difficult to optimize~\cite{ding2022delta}, and the tuned embedding is hard to be transferred to new domains and tasks~\citeiter{xu2022zeroprompt}. Recent works also combine soft prompt with discrete instructions that tune soft prompt, which is concatenated with target domain-specific (task-specific) instructions and exemplars~\cite{singhal2022large}. the resulting soft instruction achieves more stable performance compared with the direct prompt tuning, and outperforms both the individual prompt tuning and in-context learning on the unseen tasks~\cite{Sun2023HowDI}.

% \subsection{Instruction Bias}
% \citet{huynh2021survey} found that the MTurk instructions 
% Existing works found that there are biases in the human-written instructions, such as the over-represented patterns and task-unrelated information.
% evaluation metrics,
% gu，wang，zhao
% \cite{zhao2021calibrate,shi2023large} tried to counteract the in-context bias and irrelevant information.

\subsection{Negated Instruction Learning}

Negation is a common linguistic property and has been found to be crucial for various NLP tasks, e.g., textual entailment~\cite{naik2018stress,kassner2020negated}. Specific to instruction learning, negation denotes any \textit{things-to-avoid} information of in-context instructions, such as negated task descriptions and negative demonstrations. Although humans can learn a lot from the negation~\cite{dudschig2018does}, existing works found LMs often fail to follow the negated instructions~\cite{mishra2022cross,li2022maqa,jang2022can}. For example, \citet{mishra-etal-2022-reframing} conducted error analyses on GPT-3 and found GPT-3 constantly unable to understand the negated task constraints in the MTurk instructions. \citet{wang2022benchmarking} further found that adding negative demonstrations and explanations to the instructions could even harm the cross-task generalization performance of PLM. 

Since negation has increasingly become a challenge in instruction learning, we provide several hints to inspire future work. One potential solution to handle the negated instruction is unlikelihood training~\cite{hosseini2021understanding,ye2022guess}, which trains the LMs to minimize the ground truth probability when negated instructions are conditioned. In contrast, \citet{yin2022contintin} proposed to pre-train the LMs on the negative demonstrations with maximizing likelihood objective to exploit the useful information in the negation. Some other methods, such as contrast-consistent projection~\cite{burns2022discovering} and n-gram representations~\cite{sun-lu-2022-implicit}, also provided insights into tackling this problem.

\subsection{Explainable Instruction Learning}
\label{subsec:Explainability}

As we have mentioned in~\cref{sec:analysis}, in order to achieve a promising cross-task performance, one of the critical factors is to convert the human-oriented instructions into a much more PLM-oriented format, i.e., making the instructions conform to the model's preference.
% one of the critical factors in achieving promising cross-task performance
Numerous previous works have verified the effectiveness of catering to the model's preference in designing instructions, such as using the model's perplexity in choosing appropriate instructions~\cite{gonen2022demystifying}.
%  the prediction entropy or  
Despite the performance gains of the PLM-oriented instruction selection, the resulting instructions consistently violate human intuitions, questioning the reliability of PLM-oriented instruction~\cite{webson-pavlick-2022-prompt}.
% and become far more difficult for humans to understand
For example, \citet{prasad2022grips} tried to rephrase the human-oriented instructions by using performance rewards as the criterion. Surprisingly, the resulting instructions that yield better performance are constantly semantically incoherent, task-irrelevant, or even misleading instructions. Similar results are also found in~\citet{khashabi2022prompt}, which mapped the continuous instructions back into the discrete space and found those effective instructions are usually associated with semantic-irrelevant utterances. These results prove the conflict between performance profits and the human interpretability of instructions, which is tricky to trade-off.
% \citet{webson-pavlick-2022-prompt} observed that misleading prompts can still lead to comparable performance. Furthermore, \cite{prasad2022grips} shows this semantic-incoherent can even improve the results, which is totally different from human intuitions. \textbf{However, these two papers all experimented with only relatively small LMs (about 300M in~\citet{webson-pavlick-2022-prompt}, and size of InstructGPT in this paper), maybe a larger model can prefer more human-understandable instructions}.

Although \citet{mishra-etal-2022-reframing} demonstrated that it is possible to maintain both the faithfulness and effectiveness of instructions, manual rewriting requires laborious human efforts. Therefore, one of the future trends is to investigate how to automatically rephrase the instructions, in a way that matches both human and model preferences, such as setting an additional criterion during the instruction optimization.




\subsection{Explicit Instruction Learning}
\label{subsec:explicit_objective}

As we have discussed in~\cref{subsec:instruction_tuning}, multi-task instruction tuning is becoming a fundamental factor in the current instruction learning paradigm. Obviously, there are two issues in such a learning paradigm: (i) it relies on training on the massive labeled examples to learn the instructions, which is still expensive and unrealistic for using large-scale LMs; (ii) although the ultimate goal of instruction-based fine-tuning is learning to follow instructions by observing various training tasks, the current training objective is still the maximum likelihood of traditional generation tasks. This implicit instruction learning objective can lead to sub-optimal optimization (i.e., LMs can easily learn to complete specific training tasks).

To this end, one desired future direction is to evolve a new learning objective to help LMs explicitly learn to follow instructions, which might alleviate the reliance on large-scale tuning instances. 
% ~\citep[cf.][]{tay2022unifying}
Moreover, a more ambitious and challenging idea is to drive the system to follow instructions without additional tuning on the labeled examples of any specific tasks, which is somehow similar to the conventional semantic parser-based paradigm (\cref{sec:modeling}).
%
Recent work on in-context instruction learning can be considered an initial step toward this goal~\cite{ye2023context}. However, it is still built from an in-context learning perspective.
% ~\footnote{In-context instruction learning provides PLMs with fixed instruction-learning demonstrations. Although it doesn't require training, the lengthy and complex input makes the inference more costly and requires large-scale models.}.


% \subsection{Scalable Oversight and Alignment}
\subsection{Scalable Oversight: A New Evaluation Paradigm for Generalist AI Systems}

The evaluation procedure of the current research paradigm basically follows two steps: First, driving the systems to complete specific tasks. Second, using some automatic metrics to evaluate the systems.
%
While in the context of evaluating advanced instruction learning systems (i.e., generalist language models), this traditional paradigm suffers from two issues: (i) the automatic metrics are usually insufficient to measure the progress of the system, especially when the system has already been more capable than non-expert humans on those well-known tasks; (ii) we have no idea how good it is for the system to assist non-expert humans in dealing with various daily tasks. 
% whether the system is a qualified generalist ``assistant'' for.

Accordingly, recent works proposed the idea of \textit{scalable oversight}~\cite{cotra2021case,bowman2022measuring}, which denoted a new research paradigm for appraising the generalist language models. It includes the following steps: (i) \textbf{Task Choices}. Choosing the tasks where the LMs can outperform the non-experts but underperform the experts; (ii) \textbf{Non-experts Annotation}. Instead of driving the model to complete the tasks, ask the non-experts to annotate the challenging tasks with assistance from LMs, i.e., the LMs need to follow some general instructions of non-experts to help solve the tasks. This kind of procedure simulates the real-world scenarios of non-experts in using LMs; (iii) \textbf{Experts Evaluation}. At the end of the experiments, ask the experts to evaluate the annotation correctness of non-experts. In doing so, we can continue to promote the progress of generalist LMs by aligning the highly capable LMs with domain experts. Meanwhile, we simulate a real-world application scenery for most non-expert users, where the generalist LMs play the role of an assistant w/o any domain-specific knowledge aided.
%
By adopting this paradigm, \citet{bowman2022measuring} found that the non-experts can outperform both LMs-alone or human-alone results by benefiting from the assistance of LMs.

Overall, the scalable oversight paradigm can help future research to test whether current LMs (e.g., ChatGPT) can effectively assist non-expert users in solving challenging tasks.


\section{Conclusion}

In this survey, we comprehensively summarize numerous existing pieces of literature about instruction learning and provide a systematic overview of this field, including different instruction taxonomies, modeling strategies, some critical aspects of using instructions in engineering, and several popular applications. We also emphasize some distinct challenges and the corresponding hints for future research. To our knowledge, this is the first extensive survey about instruction learning. In summary, we hope this survey can offer insights and inspiration for further in-depth research on instruction learning. 

\bibliography{tacl2021}
\bibliographystyle{acl_natbib}

\end{document}


