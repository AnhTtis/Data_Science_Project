

\section{Cooperative Tracking and Jamming} \label{sec:proposed_approach}

The \textit{tracking-and-jamming} control module depicted in Fig. \ref{fig:sys_arch} seeks to find the joint mobility and power-level control actions that result in uninterrupted target tracking and target radio-jamming. This section discusses the details of the proposed control approach. 


\subsection{Target State Estimation} \label{ssec:state_estimation}
Each agent $j$ estimates the probability of target existence $e^j_{t}$ and the target spatial density $s^j_{t}$ by maintaining and propagating in time the multi-object probability distribution $F^j_{t}(X^j_t|Y^j_{1:t})$ of the RFS target state $X^j_t$ given measurements $Y^j_{1:t} = \{Y^j_1,..,Y^j_t\}$, using multi-object stochastic filtering \cite{Mahler2014book} as shown below (as a note, the index on the agent is dropped for notational clarity):
\begin{align} 
& \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! F_{t|t-1}(X_t|Y_{1:t-1}) =  \label{eq:predict}\\
& \!\!\!\!\!\!\!\!\!\! \int  \psi_{t|t-1}(X_t|X_{t-1})  F_{t-1}(X_{t-1}|Y_{1:t-1}) d X_{t-1} \notag \\
\!\!\!\!\!\!\!\!\!\! F_{t}(X_t|Y_{1:t}) &= \frac{\phi_t(Y_t|X_t)  F_{t|t-1}(X_t|Y_{1:t-1})}{\int \phi_t(Y_t|X_t) F_{t|t-1}(X_t|Y_{1:t-1}) dX_t} \label{eq:update}
\end{align}

\noindent where $\psi_{t|t-1}(X_t|X_{t-1})$ is the target RFS transitional density as discussed in Section \ref{ssec:target_dynamics}, $\phi_t(Y_t|X_t)$ is the RFS measurement likelihood function according to the measurement model discussed in Section \ref{ssec:agent_sensing}, $F_{t|t-1}(X_t|Y_{1:t-1})$ is the multi-object predictive density, and finally $F_{t}(X_t|Y_{1:t})$ is the multi-object posterior filtering density. The recursion shown here is a generalization of the Bayes filter \cite{Sarkka2013} on random finite sets \cite{Mahler2007book}. The solution of the above recursion  (i.e., see \cite{Ristic2013}) for the modeling assumptions discussed in Section \ref{sec:system_model} allows each agent to compute recursively (over time) $e_{t}$ and $s_{t}$ via the Bernoulli filter recursion \cite{Ristic2013} as:
\begin{align}
    e_{t|t-1} &= p_b(1-e_{t-1}) + p_s e_{t-1} \notag  \\
    s_{t|t-1}\!(x_t)\! &= \!q_{t|t-1}\!\! +\! \frac{p_s e_{t-1} \!\int\!\! \pi_{t|t-1}\!(x_t|x_{t-1})s_{t-1}\!(x_{t-1}) dx_{t-1} }{e_{t|t-1}} \notag
\end{align}
for the prediction step and
\begin{align}
    e_{t} &= \frac{1 - q_t}{1 - e_{t|t-1} q_t} e_{t|t-1} \notag \\
    s_{t}(x_t) &= \frac{(1-p_D) + p_D \sum_{y\in Y_t} \frac{g_t(y|x_t,u_t)}{\lambda_c p_c(y)}}{1-q_t} s_{t|t-1}(x_t) \notag
\end{align}
for the update step, where $p_D(x_t,u_t,l_t)$ is abbreviated as $p_D$, $q_{t|t-1}$ and $q_t$ are defined as $q_{t|t-1} = p_b (1-e_{t-1})b_t(x_t) e_{t|t-1}^{-1}$, $q_t = \int p_D s_{t|t-1}(x) dx - \sum_{y \in Y_t} \int p_D g_t(y|x,u_t) s_{t|t-1}(x) dx \left(\lambda_c p_c(y) \right)^{-1}$, and finally $g_t(y|x_t,u_t) = \mathcal{N}(y; h(x_t,u_t),\Sigma_w)$ is the measurement likelihood function for the target generated measurement.

That said, each agent $j$ computes at each time-step $e^j_{t}$ and $s^j_{t}$ using the recursion shown above. The final target state is then fused as follows: The target existence probability is first exchanged among all agents and the fused estimate is computed as  $\hat{e}_{t} = \text{mean} ~\{ e^j_{t}~ |~ j=1,..,N\}$. Then, the set of agents that sense the presence of the target i.e., $\tilde{N}=\{j ~|~ e^j_{t} > 0.5\}$ first extract the estimated target state $\hat{x}^j_t$ from $s^j_{t}$ and its covariance matrix $C^j$. Then $[ \hat{x}^j_t, C^j],~\forall j \in \tilde{N} $ is exchanged among all $\tilde{N}$ agents and combined using covariance intersection \cite{Julier2017,Deng2012}. Finally, the fused results are communicated to all $N$ agents which they use to sample from and update their filtering densities. 




\subsection{Single Agent Tracking-and-Jamming Control} \label{ssec:single_control}
To achieve tracking-and-jamming control first observe that the posterior target existence probability $e_{t}$, the posterior filtering density $s_{t}$, and the target received power $R_t$ are all influenced by the target detection events. Thus, optimizing $u_t$ and $l_t$ for maximum target detection performance, results in optimized tracking-and-jamming performance. Optimizing the target detection, results in more accurate estimation of $e_{t}$ and $s_{t}$, from which the target state can be extracted and thus target radio jamming can be maximized. Hence, the optimal control actions $(\hat{u}^j_t,\hat{l}^j_t)$ for agent $j$ can be computed as: 
\begin{equation}
    \hat{u}_{t}^{j},\hat{l}^j_t = \underset{u^j_{t} \in \mathbb{U}^j_{t},l^j_{t} \in \mathbb{L}^j_{t}}{\arg\max}~ p^j_D(\hat{x}_t,u^j_t,l^j_t)
\end{equation}
where $\hat{x}_t$ is the estimated state of the target. It should be noted, however, that $\hat{x}_t$ is not available until the control actions $\hat{u}^j_t$ and $\hat{l}^j_t$  have already been chosen and applied. In order to bypass this problem, each agent $j$ approximates $\hat{x}^j_t \sim \tilde{x}^j_t$ as:
\begin{equation}
    \tilde{x}^j_{t} = \int x_t s^j_{t|t-1}(x_t) d x_t, ~\text{iff}~ e^j_{t|t-1} > 0.5
\end{equation}
where $s^j_{t|t-1}(x_t)$ is the predictive spatial density of the target and $e^j_{t|t-1}$ is the predicted probability of target existence.

% When the target existence probability is less than 0.5, then the target is not present in the scene and thus tracking-and-jamming does not takes place.


\subsection{Centralized Tracking-and-Jamming Control}
To tackle the cooperative tracking-and-jamming control problem, the induced jamming interference among the agents must be kept below the critical value so that the agents can remain operational at all times. The joint target detection probability for which exactly $m$ out of $N$ agents jointly track-and-jam the target is given by:
\begin{equation}
\xi^N_m = \underset{1 \le i_1 < i_2 < ,..., < i_m \le N }{\sum}~ \prod_{j=1}^{N}~ \frac{(1-p^j_D) ~p^{i_j}_D}{1-p^{i_j}_D}
\end{equation}

\noindent where $p^j_D(\bar{x}_t,u^j_t,l^j_t)$ is abbreviated as $p^j_D$ for notational clarity, and $\bar{x}_t = \text{mean}\{\tilde{x}^j_{t}: \forall j\}$ denotes the mean predicted target state from all agents $j$ with $e^j_{t|t-1} > 0.5$. The operator $\sum_{1 \le i_1 < i_2 < ,..., < i_m \le N }(.)$ computes the combinations ${N \choose m}$ among the agents. To ensure that at least $n$ out of $N$ agents effectively track-and-jam the target, while at the same time respect the interference constraints amongst them, the cooperative tracking-and-jamming control objective is defined as $\Xi^N_n = \sum_{m=n}^N \xi^N_m$ and the problem becomes:

\begin{subequations}
\begin{align} 
     (\hat{u}^j_t,\hat{l}^j_t) &= \underset{u^j_{t} \in \mathbb{U}^j_{t},l^j_{t} \in \mathbb{L}^j_{t}}{\arg\max} ~\Xi^N_n, ~ \forall j \in [1,..,N] \label{eq:joint_control1}\\
    \text{s.t.} &~ \sum_{j=1}^N R^j_t(u^{i \ne j}_t,u^j_t,l^j_t) < \delta^i,~\forall~ i \in [1,..,N] \label{eq:joint_control2}
\end{align}
\end{subequations}

\noindent where $\delta^i$ denotes the interference tolerance of agent $i$, i.e., the radio-jamming interference received by agent $i$ from all other agents $j$ should be less than $\delta^i$ in order for agent $i$ to remain operational. 

\subsection{Distributed Tracking-and-Jamming Control}
The joint optimization problem of Eqs. (\ref{eq:joint_control1})-(\ref{eq:joint_control2}) is a hard combinatorial problem which quickly becomes computationally intractable as the number of agents and the number of control actions increases. 

For this reason, in order to tackle this problem and achieve the desired system behavior in real-time, a distributed sub-optimal approach is proposed based on the greedy randomized adaptive search procedure or GRASP \cite{Feo1995,Resende2019}. In essence, GRASP is an iterative randomized sampling technique which operates in two steps. In the first step the algorithm constructs an \textit{initial greedy randomized solution} which is further refined through \textit{local search} in the second step. The algorithm alternates between these two steps, while keeping track of the best solution, until the stopping criteria are met (e.g., usually the number of iterations). The two steps of GRASP are implemented as follows:

\subsubsection{Greedy Randomized Solution}
An initial randomized solution is achieved via random sampling to find the joint control actions $(\hat{u}^j_t,\hat{l}^j_t), \forall j$ with the following steps:
\begin{itemize}
    \item  (a) agent $j,~ \forall j \in [1,..,N]$ receives  $ u^{i \ne j}_{t-1}, \tilde{x}^{i \ne j}_{t} ~\forall i \in [1,..,N]$ at time $t$ and using Eq. (\ref{eq:controlVectors}) computes the set of admissible mobility control actions for all agents $i \ne j \in [1,..,N]$ to create $N$ sets  $ \mathbb{U}^{1}_{t},...,\mathbb{U}^{N}_{t}$, including its own $ \mathbb{U}^{j}_{t}$.
    \item (b) agent $j,~ \forall j \in [1,..,N]$ then locally computes $N$ sets of hypothesized joint mobility and transmit power-level control actions one for each agent $i \ne j \in [1,..,N]$ plus its own i.e., $\left[ \{\mathbb{U}^{1}_{t} \times \mathbb{L}^{1}_{t}\}^1,...,\{\mathbb{U}^{N}_{t} \times \mathbb{L}^{N}_{t}\}^N \right]^j$ denoted as $\left[ \mathbb{Q}^{1}_{t},...,\mathbb{Q}^{N}_{t}\right]^j$ hereafter. The assumption here is that the agents have the same capabilities, i.e., the agents exhibit the same number of mobility control actions and transmit power-levels. This assumption however, is not strict and can be relaxed depending on the application scenario. 
    \item (c) agent $j,~ \forall j \in [1,..,N]$ samples uniformly at random with replacement $n_s$ samples from their local set $\left[ \mathbb{Q}^{1}_{t},...,\mathbb{Q}^{N}_{t}\right]^j$ which are used to create $n_s$ joint control vectors $\omega^j_i = \left[(u^1_t,l^1_t)^i,...(u^j_t,l^j_t)^i,...,(u^N_t,l^N_t)^i   \right]^j, i \in [1,..n_s]$.
    \item (d) Finally, each agent $j$ then solves the optimization problem in Eqs. (\ref{eq:joint_control1})-(\ref{eq:joint_control2}) by searching through $\omega^j_i$ to find an initial global solution to the problem denoted as $\left[(\tilde{u}^1_t,\tilde{l}^1_t),...,(\tilde{u}^j_t,\tilde{l}^j_t)...,(\tilde{u}^N_t,\tilde{l}^N_t)  \right]^j$.
\end{itemize}


\subsubsection{Local Search}
Once an initial global solution is found, a local search is performed trying to find an improvement. To do so, each agent $j,~ \forall j \in [1,..,N]$ performs locally an exhaustive search around the neighborhood of $\left[(\tilde{u}^1_t,\tilde{l}^1_t),...,(\tilde{u}^j_t,\tilde{l}^j_t)...,(\tilde{u}^N_t,\tilde{l}^N_t)  \right]^j$ which is defined as follows: Let $\kappa^{j}$ = $\left[ (\tilde{u}^{j-}_t,\tilde{u}^j_t,\tilde{u}^{j+}_t) \times (\tilde{l}^{j-}_t,\tilde{l}^j_t,\tilde{l}^{j+}_t) \right]$ be the joint combinations of mobility and power-level controls of agent $j$, where notation $x^{+},x^{-}$ denotes two different control actions which are close to $x$, and which are chosen at random from a list of close proximity control actions.


 The neighborhood of $\left[(\tilde{u}^1_t,\tilde{l}^1_t),...,(\tilde{u}^j_t,\tilde{l}^j_t)...,(\tilde{u}^N_t,\tilde{l}^N_t)  \right]^j$ is then defined as the joint combinations of the control vector $[(\tilde{u}^i_t,\tilde{l}^i_t), \forall i \ne j]$ with $\kappa^j$. In essence, agent $j$ performs a local search by changing only its own controls and keeping constant the controls assigned to the other agents. Thus, agent $j$ searches the joint control combinations given by $\left[(\tilde{u}^1_t,\tilde{l}^1_t)\times....\times\kappa^{j}\times...\times(\tilde{u}^N_t,\tilde{l}^N_t) \right]^j$ (where the single pair $(\tilde{u}^j_t,\tilde{l}^j_t)$ is replaced with the vector of pairs $\kappa^j$). Each agent $j$ exhaustively searches the joint combinations in its neighborhood to find the best solution by solving the problem in Eqs. (\ref{eq:joint_control1})-(\ref{eq:joint_control2}).

\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{res1.pdf}
	\caption{A simulated tracking-and-jamming scenario, where a team of 3 pursuer agents track-and-jam a target.}	
	\label{fig:res1}
	\vspace{-5mm}
\end{figure*}

\subsubsection{Fusion}
Each agent alternates between steps 1) and 2) and the best known feasible solution over all $n_I$ iterations is kept as the final solution. The agents then exchange their local best solutions to converge to the best global solution $(\hat{u}^j_t,\hat{l}^j_t), \forall j$. The best global mobility and transmit power-level control actions are then applied to the agents, which move to their new states, receive target measurements, and apply the update step of the filtering algorithm discussed in Section \ref{ssec:state_estimation} to compute the posterior existence probability $\epsilon_{t}$ and spatial density $s_t$ of the target. 

In this work, a distributed optimization approach based on GRASP is proposed, for finding the joint control actions for the team of agents. Nevertheless, the reader should note that a solution to this problem can also be obtained with other distributed combinatorial optimization techniques including methods based on swarm particle optimization, genetic algorithms, and others. A comparison between different optimization techniques for this problem is left as future work.
















