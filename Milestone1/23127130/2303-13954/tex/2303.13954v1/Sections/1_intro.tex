%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% File: 1_intro.tex
% Self-contained tex file for the introduction
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:1_intro}
Implementing parallel software that can effectively utilize distributed-memory systems poses many challenges for programmers.
Specifically, modifying an existing serial or shared-memory parallelized application to run in a distributed setting often requires significant programmer effort to orchestrate data distribution and communication.
The Partitioned Global Address Space (PGAS) model attempts to address these challenges by providing programmers with a view of a distributed-memory system that resembles a single shared address space.
The PGAS model has been implemented in various languages and libraries, such as UPC~\cite{el2005upc}, GlobalArrays~\cite{nieplocha1996global} and Chapel~\cite{chamberlain2007parallel}.
Within PGAS languages, details regarding data distribution and communication are often abstracted from the programmer.
For example, in Chapel simply specifying an array as ``distributed'' automatically maps the data across the system and remote communication is performed implicitly.
The PGAS model therefore encourages programmers to write code in a shared-memory manner but aims to provide good performance on distributed-memory systems without requiring the program to be rewritten.

Irregular memory access patterns are commonly found in applications that perform graph analytics~\cite{lumsdaine2007challenges}, sparse linear algebra~\cite{williams2007optimization} and scientific computing operations~\cite{dongarra2016high}.
Such access patterns pose significant challenges for user productivity on distributed-memory systems because the access patterns are not known until runtime, making it difficult to orchestrate communication amongst remote processes.
However, with the PGAS model coding irregular communication patterns becomes more straightforward, as they can be implemented via one-sided communication.
Unfortunately, the performance of such codes will be significantly hindered due to the fine-grained remote communication that arises from the irregular memory accesses.
While the abstractions provided by the PGAS model can be manually bypassed to achieve better performance, such an effort would significantly degrade user productivity.

In this paper, we present the design and implementation of a compiler optimization that automatically applies the \emph{inspector-executor} technique~\cite{SALTZ91B} to parallel loops in PGAS programs written in the Chapel language.
The inspector performs memory access analysis at runtime to determine remote communication to an array of interest within a loop. 
The executor replicates the remote data and runs the original loop, but redirects remote accesses to replicated local copies to avoid repeated remote communication.
Our compiler optimization automatically identifies candidate loops and array accesses, and then performs code transformations to construct the inspector and executor routines.
As a result, the user is not required to change their original code in order to achieve significant performance gains.
The contributions of our work are as follows:
\begin{itemize}
    \item Design and implementation of an inspector-executor based compiler optimization for Chapel programs that specifically targets irregular memory accesses to distributed arrays. 
    To the best of our knowledge, this work presents the first such optimization within the Chapel compiler.
    \item Discussion on the unique features of Chapel, such as implicit processor affinity, as they relate to the compiler optimization.
    While the inspector-executor technique has been employed for a long time~\cite{SALTZ91B,IELoopParallelize,strout2003compile}, and applied to other PGAS languages~\cite{TitaniumOpt,UPCStaticDynamicCoal}, our design within Chapel requires a different approach due to Chapel's high-level features.
    \item Performance evaluation of our optimization across two irregular applications and two different distributed-memory systems.
    Our results show that the optimization can improve performance by as much as 52x on a Cray XC system with a low-latency interconnect and 364x on a standard Linux cluster with an Infiniband interconnect.
\end{itemize}

The rest of the paper is organized as follows. 
Section~\ref{sec:2_chapel} presents an overview of Chapel.
We present our compiler optimization and discuss its details in Section~\ref{sec:3_optimization}.
Section~\ref{sec:4_perfEval} presents a performance evaluation of the optimization across two irregular applications, and the Appendix contains additional performance results.
Prior work as it relates to our paper is described in Section~\ref{sec:5_related}.
Finally, Section~\ref{sec:6_concl} provides concluding remarks and discusses future work.