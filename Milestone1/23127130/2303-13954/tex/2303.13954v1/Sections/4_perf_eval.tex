%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% File: 4_perf_eval.tex
% Self-contained tex file for the performance evaluation
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance Evaluation}
\label{sec:4_perfEval}
To demonstrate the performance benefits of the compiler optimization described in Section~\ref{sec:3_optimization}, we performed an evaluation of two irregular applications running on two different distributed-memory systems.
The applications we evaluate are implemented in a high-level manner, consistent with Chapel's design philosophy, which is to separate data distribution/communication details from the algorithm design.
As a result, our exemplar applications closely match standard shared-memory implementations and are representative of direct use of the PGAS model.
Our goal is to show that the performance of these Chapel programs suffer from implicit fine-grained remote communication, but can be significantly improved via automatic optimization without requiring the user to modify the program.
Therefore, users can take advantage of the productivity benefits that Chapel provides while also achieving good performance.

%######################################################################################
\subsection{Experimental Setup}
\label{sec:4-1_setup}
For our evaluation, we run experiments on a Cray XC cluster and an Infiniband-based cluster.
For the Cray XC, we utilize up to 64 nodes connected over an Aries interconnect, where each node has two 22-core Intel Xeon Broadwell CPUs and 128~GB of DDR4 memory.
For the Infiniband cluster, we utilize up to 32 nodes connected over an FDR Infiniband interconnect, where each node has two 10-core Intel Xeon Haswell CPUs and 512~GB of DDR4 memory.
On the Cray XC, Chapel is built using the ugni communication layer and the \texttt{aries} communication substrate.
On the Infiniband system, Chapel is built using the GASNet communication layer and the \texttt{ibv} communication substrate.
All applications on both systems are compiled using the \texttt{---fast} flag.
For each experiment, we execute the given application multiple times and measure the total runtime, including the inspector overhead.
The results represent the average of these trials.
We observed that the runtime variation between trials did not exceed 4\%.

%######################################################################################
\subsection{Application: NAS-CG}
\label{sec:4-2_cg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code listing for NAS-CG kernel
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mdframed}[backgroundcolor=black!5,hidealllines=true,%
innerbottommargin=-0.75cm,innertopmargin=-0.10cm]
\noindent\begin{minipage}{\linewidth}
\begin{lstlisting}[style=ChplStyle,label={lst:cg}, caption={\texttt{forall} loop for NAS-CG},columns=flexible]
forall row in Rows {
  var accum : real = 0;
  for k in row.offsets {
    accum += values[k] * x[col_idx[k]];
  }
  b[row.id] = accum;
}
\end{lstlisting}
\end{minipage}
\end{mdframed}
The conjugate gradient (CG) method solves the equation $Ax=b$ for $x$, where $A$ is a symmetric positive-definite matrix and is typically large and sparse, and $x$ and $b$ are vectors.
Unstructured optimization problems and partial differential equations can be solved using iterative CG methods.
For the evaluation, we use the NAS-CG benchmark specification and datasets~\cite{bailey1995parallel}.
Table~\ref{tab:cg} describes the problem sizes evaluated, where each problem size corresponds to the size of the $A$ matrix.
Each iteration of NAS-CG performs a total of 26 sparse matrix-vector multiplies (SpMVs), which is the kernel of interest for the optimization and is shown in Listing~\ref{lst:cg}.
The implementation uses a standard Compressed Sparse Row (CSR) format to represent $A$, where \texttt{Rows} is a block distributed array of records that contains the offsets into the distributed array(s) containing the non-zero data values.
This approach closely resembles the Fortan+OpenMP implementation of NAS-CG~\cite{bailey1995parallel}.
The irregular access of interest is on line 4, \texttt{x[col\_idx[k]]}.
For the optimization, the inspector only needs to be executed once since the memory access pattern remains the same across all SpMV operations.
In regards to memory storage overhead due to replication, we observed an average increase in memory usage of 6\%.

%
% Table of datasets for NAS-CG
%
\vspace{-10mm}
\begin{table}
\renewcommand{\arraystretch}{1.0}
\caption{Datasets for NAS-CG}
\label{tab:cg}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Name} & \textbf{Rows} & \textbf{Non-zeros} & \textbf{Density (\%)} & \textbf{\# of SpMVs} \\
\hline
C           & 150k & 39M  & 0.17  & 1950  \\
D           & 150k & 73M  & 0.32  & 2600 \\ 
E           & 9M   & 6.6B & 0.008 & 2600 \\
F           & 54M  & 55B  & 0.002 & 2600\\
\hline
\end{tabular}
%\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	NAS-CG Speed-ups Table
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{table}
\caption{Runtime speed-ups achieved by the optimization on the NAS-CG problem sizes from Table~\ref{tab:cg} relative to the unoptimized code in Listing \ref{lst:cg}. Missing values indicate that the problem size required too much memory to execute and ``NA'' values indicate that the system did not support the specified number of nodes/locales.}
\label{tab:nascg_speedups}
\centering
\begin{tabular}{|c|cccc|cccc|}
\hline
& \multicolumn{4}{c|}{\textbf{Cray XC}} & \multicolumn{4}{c|}{\textbf{Infiniband}} \\
\textbf{Locales (Nodes)} & C   & D   & E    & F    & C    & D    & E   & F   \\ \hline
2                & 3.2 & 2.8 & ---  & ---  & 8.9  & 6    & 357 & --- \\
4                & 3.6 & 3.4 & 17.5 & ---  & 15.8 & 10.4 & 345 & --- \\
8                & 5.7 & 6.2 & 36.7 & ---  & 115  & 127  & 364 & --- \\
16               & 8.6 & 11  & 22.5 & ---  & 238  & 330  & 258 & 270 \\
32               & 6.4 & 8.4 & 34   & 52.3 & 160  & 240  & 195 & 165 \\
64               & 4.1 & 4.9 & 16.7 & 25.4 & NA   & NA   & NA  & NA  \\
\hline
\textbf{geomean} & 5   & 5.5 & 24.1 & 36.4 & 57.3 & 57.5 & 296 & 211 \\
\hline
\end{tabular}
\end{table}
\vspace{-5mm}

Table~\ref{tab:nascg_speedups} presents the NAS-CG runtime speed-ups achieved by the optimization on each system relative to the unoptimized code shown in Listing~\ref{lst:cg}.
We observe large speed-ups on both systems, but most notably on the Infiniband system because fine-grained remote communication exhibits higher latency compared to the Aries interconnect on the Cray XC.
On both systems, such speed-ups are obtained because of a large amount of remote data reuse in the SpMV kernel, which is due to the sparsity pattern of the matrices generated by the benchmark.
The optimization incurs the cost of reading the remote element once, but can then access the element locally throughout the rest of the \texttt{forall} loop.
Without the optimization, each access to the remote element likely pays the full latency cost of a remote access since the access pattern is sparse and irregular.
Additionally, the SpMV kernel is executed many times during the NAS-CG benchmark, which allows for the inspector overhead to be amortized.
We observe that the percentage of runtime devoted to the inspector is 3\% on average across all locale counts for the Cray XC and 2\% for the Infiniband system.

Beyond relative speed-ups, the optimization significantly improves the overall runtime, as can be see in Tables~\ref{tab:nascg_base_times} and~\ref{tab:nascg_opt_times} in the Appendix.
We observe very poor runtime performance and scalability for the unoptimized code, which is due to the implicit fine-grained remote communication required in the straightforward implementation shown in Listing~\ref{lst:cg}.
On the other hand, the optimized version of the code generally gets faster with more locales, since the optimization can take advantage of remote data reuse.
Overall, these results demonstrate the usefulness of our optimization in automatically providing faster runtimes without sacrificing user productivity.


%######################################################################################
\subsection{Application: PageRank}
\label{sec:4-3_pr}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code listing for PageRank kernel
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mdframed}[backgroundcolor=black!5,hidealllines=true,%
innerbottommargin=-0.75cm,innertopmargin=-0.10cm]
\noindent\begin{minipage}{\linewidth}
\begin{lstlisting}[style=ChplStyle,label={lst:pr}, caption={\texttt{forall} loop for PageRank},columns=flexible]
forall v in Graph {
  var val = 0.0;
  for i in v.offsets {
    ref t = Graph[neighbors[i]];
    val += t.pr_read / t.out_degree;
  }
  v.pr_write = (val * d) + ((1.0-d)/num_vertices) + sink_val;
}
\end{lstlisting}
\end{minipage}
\end{mdframed}

PageRank~\cite{bianchini2005inside} is an iterative graph algorithm that provides an importance measurement for each vertex in a graph.
Listing~\ref{lst:pr} presents the PageRank kernel and is the target of the optimization, where the irregular memory access of interest is on line 4.
Similar to NAS-CG, we use a CSR data structure to represent the graph.
The distributed array \texttt{Graph} stores records that correspond to vertices, where each vertex has two importance measurements: \texttt{pr\_write} and \texttt{pr\_read}.
This allows for a straightforward parallel implementation of the kernel by treating one value as read-only during an iteration, and closely matches the GAP Benchmark Suite implementation~\cite{GAPBS}.
Unlike NAS-CG, PageRank adds the complication of storing records in the array of interest rather than base type data (i.e., \texttt{int}, \texttt{real}, etc.).
The optimization recognizes this feature and will only replicate the fields that are accessed in the \texttt{forall} loop, namely, \texttt{pr\_read} and \texttt{out\_degree}.

We evaluate PageRank on two real web graphs obtained from the SuiteSparse Matrix Collection~\cite{MatrixMarket}, which are described in Table~\ref{tab:pr}.
The right-most column denotes the number of iterations that are required to converge with a tolerance value of $1e{\text -}7$ and a damping factor of 0.85 (\texttt{d} on line 7 in Listing \ref{lst:pr}).
Our choice for these values matches what is used in Neo4j~\cite{neo4j}, an open source graph database.
Each PageRank iteration performs one execution of the entire \texttt{forall} in Listing~\ref{lst:pr} and the inspector is only executed once since the graph does not change throughout the execution.
The memory storage overhead of the optimization for PageRank is 40--80\%, which is much larger than what we observed for NAS-CG.
For NAS-CG, the array of interest that is replicated constitutes a small portion of the total memory required.
But for PageRank, the array of interest is much larger by comparison, resulting in a larger relative increase in memory storage.
However, the memory storage overhead incurred by the optimization is significantly less than what full replication would incur.

%
% Table of datasets for PageRank
%
\vspace{-10mm}
\begin{table}
\renewcommand{\arraystretch}{1.0}
\caption{Datasets for PageRank}
\label{tab:pr}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Name} & \textbf{Vertices} & \textbf{Edges} & \textbf{Density (\%)} & \textbf{Iterations}\\
\hline
webbase-2001    & 118M & 992M & \num{7.1e-6} & 33 \\
sk-2005         & 51M  & 1.9B & \num{7.5e-5} & 40 \\
\hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	PageRank Speed-ups Table
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{table}[h]
\renewcommand{\arraystretch}{1}
\caption{Runtime speed-ups achieved by the optimization on PageRank for the graphs from Table~\ref{tab:pr} relative to the unoptimized implementation. ``NA'' values indicate that the system did not support the specified number of nodes/locales.}
\label{tab:pr_speedups}
\centering
\begin{tabular}{|c|cc|cc|}
\hline
& \multicolumn{2}{c|}{\textbf{Cray XC}} & \multicolumn{2}{c|}{\textbf{Infiniband}} \\
\textbf{Locales (Nodes)} & webbase-2001  & sk-2005 & webbase-2001 & sk-2005 \\ \hline
2                & 0.88          & 1.2     & 5.2          & 2   \\
4                & 0.98          & 1.6     & 8.6          & 7.1 \\
8                & 0.97          & 1.3     & 12           & 6   \\
16               & 0.94          & 1.7     & 9.6          & 5.4 \\
32               & 1.3           & 1.4     & 4.5          & 4.2 \\
64               & 1.2           & 2.1     & NA           & NA  \\
\hline
\textbf{geomean} & 1.04          & 1.5     & 7.3          & 4.5 \\
\hline
\end{tabular}
\end{table}
\vspace{-10pt}

Table~\ref{tab:pr_speedups} presents the runtime speed-ups for the optimization on both graphs from Table~\ref{tab:pr}, and Tables~\ref{tab:pr_base_times} and~\ref{tab:pr_opt_times} in the Appendix presents the runtimes without and with the optimization.
As we observed for NAS-CG, the speed-ups on the Infiniband system are larger than those on the Cray XC due to the Aries interconnect.
However, we observe significantly smaller speed-ups overall when compared to NAS-CG.
This is largely because the PageRank kernel is executed fewer times than the NAS-CG kernel, and the graphs exhibit significantly less data reuse when compared to NAS-CG.
As a result, there is a larger inspector overhead and smaller performance gains from the executor.
For these reasons, speed-ups are not achieved on the webbase-2001 graph on the Cray XC until 32 locales, which is when the remote data reuse reaches its peak.
Furthermore, due to the highly irregular nature of the graphs (whose degree distributions follow a power law), the runtime performance fluctuates as the number of locales increase due to the partitioning of the graph across the system.
This can change where the elements are located, which may lead to a once heavily accessed remote element now being local.
This is more significant for the unoptimized code, as the optimization would have only incurred the cost of the remote access once due to replication.
Nevertheless, the scalability of the optimization generally tracks the scalability of the code without the optimization applied.