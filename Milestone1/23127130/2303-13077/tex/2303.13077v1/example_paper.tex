%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}
\pdfoutput=1
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2022}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2022}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{makecell}
%\usepackage{wrapfig}  % can not be used  officially
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{bbding}
% \usepackage{algorithm}
% \usepackage{algorithmicx}
% \usepackage{algpseudocode}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\icmltitlerunning{Improving the Performance of Spiking Neural Networks on Event-based Datasets with Knowledge Transfer}
\begin{document}

\twocolumn[
\icmltitle{Improving the Performance of Spiking Neural Networks \\
on Event-based Datasets with Knowledge Transfer}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Xiang He}{equal,a,b,c}
\icmlauthor{Dongcheng Zhao}{equal,a,c}
\icmlauthor{Yang Li}{a,b,c}
\icmlauthor{Guobin Shen}{a,c,d}
\icmlauthor{Qingqun Kong}{a,b,c}
\icmlauthor{Yi Zeng}{a,b,c,d,e}
\end{icmlauthorlist}



\icmlaffiliation{a}{Institute of Automation, Chinese Academy of Sciences (CAS)}
\icmlaffiliation{b}{School of Artificial Intelligence, University of Chinese Academy of Sciences}
\icmlaffiliation{c}{Brain-inspired Cognitive Intelligence Lab, Institute of Automation, Chinese Academy of Sciences}
\icmlaffiliation{d}{School of Future Technology, University of Chinese Academy of Sciences}
\icmlaffiliation{e}{Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences}

\icmlcorrespondingauthor{Qingqun Kong}{qingqun.kong@ia.ac.cn}
\icmlcorrespondingauthor{Yi Zeng}{yi.zeng@ia.ac.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}
  Spiking neural networks (SNNs) have rich spatial-temporal dynamics, which are suitable for processing neuromorphic, event-based data. However, event-based datasets are usually less annotated than static datasets used in traditional deep learning. Small data scale makes SNNs prone to overfitting and limits the performance of the SNN.
  To enhance the generalizability of SNNs on event-based datasets, we propose a knowledge-transfer framework that leverages static images to assist in the training on neuromorphic datasets. 
  Our method proposes domain loss and semantic loss to exploit both domain-invariant and unique features of these two domains, providing SNNs with more generalized knowledge for subsequent targeted training on neuromorphic data. Specifically, domain loss aligns the feature space and aims to capture common features between static and event-based images, while semantic loss emphasizes that the differences between samples from different categories should be as large as possible.
  Experimental results demonstrate that our method outperforms existing methods on all mainstream neuromorphic vision datasets. In particular, we achieve significant performance improvement of 2.7\% and 9.8\% when using only 10\% training data of CIFAR10-DVS and N-Caltech 101 datasets, respectively.
\end{abstract}

\section{Introduction}
As the third generation of neural networks, spiking neural networks (SNNs)~\cite{maass1997networks} are known for their rich neurodynamic properties in the spatial-temporal domain and event-driven advantages \cite{roy2019towards}. 
Due to the non-differentiable properties of spiking neurons, the training of SNNs has also been a popular topic of extensive research by researchers.
The training of SNNs is mainly divided into the following three categories, surrogate gradient backpropagation-based~\cite{wu2018spatio, wu2019direct, zheng2021going, shen2022backpropagation}, spiking time-dependent plasticity-based~\cite{diehl2015unsupervised, hao2020biologically, zhao2020glsnn}, and conversion-based~\cite{han2020rmp, bu2021optimal, li2022efficient, liu2022spikeconverter, li2022spike}. With the introduction
of these algorithms, SNNs show excellent performance in various complex scenarios~\cite{sun2021quantum, cheni2021reducing, godet2021starflow, stagsted2020event}. 
In particular, SNNs have shown promising results in the processing of neuromorphic, event-based data due to the ability to process information in the time dimension\cite{xing2020new, chen2020event, viale2021carsnn}.

\begin{figure*}[t]
	\centering
		\includegraphics[width=0.9\linewidth]{gradcam_plusplus_refined.pdf}
	\caption{Class Activation Mapping of  CIFAR10 and CIFAR10-DVS. Three categories are selected for display, the first two rows under each category represent static pictures, and the last row represents neuromorphic data integrated into frames; the three columns from left to right represent the results of original picture, baseline and our method, respectively.}
	\label{fig1}
\end{figure*}

Event-based data can be obtained by event cameras. Dynamic Vision Sensor (DVS) \cite{serrano2013128} is a bio-inspired visual sensor that operates differently from conventional cameras. 
Instead of capturing images at a fixed rate, the DVS measures the intensity changes at each pixel asynchronously. Consequently, DVS generates a stream of events that encode the timing, location, and polarity of the intensity changes, thereby facilitating the processing of event-based data.
DVS has been gaining popularity in various applications due to their high dynamic range, high temporal resolution, and low latency \cite{lagorce2016hots, gallego2017event, stoffregen2019event, gallego2020event, zhu2018ev}.
Despite these advantages, a major challenge associated with event cameras is the long and costly shooting process. As a result, the DVS dataset is difficult to acquire and small in scale. Event cameras capture data only when there is a significant change in the scene, resulting in a sparse and noisy data stream, which increases the demand for labeled data. The lack of labeled datasets makes it difficult to train SNNs effectively. Therefore, the development of new methods that can exploit the unique properties of event cameras and overcome the lack of labeled data is essential for their widespread adoption in various applications.

Compared to DVS dataset, it is relatively easy to obtain RGB data. When exploring a new environment, humans use their learned knowledge to compare it with new environment to better adapt to it. 
This inspires us leveraging static images to assist in the SNN training on neuromorphic datasets.

In this paper, we propose the knowledge transfer framework for SNN. Since DVS focuses on the variation of light intensity, we convert the widely available RGB static images into HSV space and take the luminance channel to help SNN learning on DVS data. To extract information useful for DVS, we propose domain loss and semantic loss. Specifically, domain loss learns domain-invariant features by reducing the joint distribution distance between the source domain (static images) and the target domain (DVS data), while semantic loss learns specific features by separating the distances between different classes of samples. 
As shown in Fig.~\ref{fig1}, by introducing these two types of losses, the locations that are important for producing target are shifted to features of objects, such as the body of a bird, or the head of a dog and cat. Fig.~\ref{fig1} shows that our method learns the common features and obtains DVS features that facilitate correct classification. 

The main contributions of this paper can be summarized as follows:
\begin{enumerate}[1)]
   \item We propose a single-stage knowledge transfer framework based on annotated static datasets. To our best knowledge, we are the first to transfer the knowledge of spiking neural networks on static datasets to the DVS datasets, which enhances the generalization of SNNs.   
   \item We propose domain loss and semantic loss to enhance the learning of invariant and unique features between static and event data. Our results show that the learned features facilitate the training of SNN on DVS data, and the models trained with domain loss and semantic loss are flatter around local minima.
   \item We conduct experiments on commonly used neuromorphic datasets CIFAR10-DVS, NCALTECH101, and N-Omniglot to verify the effectiveness of our method. The experimental results show that the proposed method outperforms the state-of-the-art methods on all datasets. Significantly when gradually reducing the number of training samples in the neuromorphic datasets, our method brings more significant performance improvements to SNNs.
\end{enumerate}

\section{Related Work}
In order to solve the problem of limited labeled DVS data, previous works have tried domain adaptation, data augmentation and efficient training methods. \medskip

\noindent
{\bf Domain adaptation using static data.} \quad
\citet{messikommer2022bridging} use a generative event model to classify event features into content and motion features, enabling efficient matching between the latent space of events and images, and training the model directly with labeled images and unlabeled event data.
\citet{li2018deep} combine temporal coding and deep representation. The deep representation learned by the initially optimized CNN is effectively transferred to the event stream classiÔ¨Åcation task, and the performance is further improved by fine-tuning the CNN with a certain number of training samples.
\citet{zhao2022transformer} train a convolutional transformer network for event-based classification tasks using large-scale labeled image data via a passive unsupervised domain adaptation (UDA) algorithm.
These works are related to ours; differently,
we exploit the invariant and unique features between static and event data by cross-domain loss.
Such features can provide a generalized prior knowledge for the SNN, thus facilitating further training of the network. This can enhance the original SNN structure instead of pre-training a new network model with more parameters.
\medskip

\noindent
{\bf Event-based augmentation.} \quad
\citet{li2022neuromorphic} propose neuromorphic data augmentation to stabilize the training of SNNs and improve generalization.
\citet{shen2022eventmix} design an enhancement strategy for event stream data, and perform the mixing of different event streams by Gaussian mixing model, while assigning labels to the mixed samples by calculating
the relative distance of event streams.
\medskip

\noindent
{\bf SNN efficient training} \quad
\citet{zhan2021effective} analyzes the plausibility of central kernel alignment (CKA) as a domain distance measure relative to maximum mean difference (MMD) in deep SNNs.
\citet{deng2022temporal} introduce the time-efficient training (TET) method, which allows the surrogate gradient method to SNN to converge to a flatter minimum.

\section{Preliminaries}
\noindent
{\bf Neuron model.} \quad 
We choose Leaky Integrate-and-Fire (LIF) model~\cite{dayan2005theoretical}, the most commonly used neuron model. Equationally, the update of the membrane potential $\boldsymbol{u}$ can be written in the following discrete form
\begin{equation}
\boldsymbol{u}^{t+1, l} = \tau \boldsymbol{u}^{t, l} + \boldsymbol{W}^l \boldsymbol{s}^{t, l-1},
\end{equation}
where $\tau$ is leaky factor and $\boldsymbol{u}^{t, l}$ denotes membrane potential of the neurons in layer $l$ at time step $t$. 
$\boldsymbol{W}^l$ and $\boldsymbol{s}^l$ represent the weight parameters of the layer $l$ and the fired spikes in layer $l$, respectively.

The membrane potential accumulates with the input until a given threshold $V_{th}$ is exceeded, then the neuron delivers a spike and the membrane potential $\boldsymbol{u}^{t, l}$ is reset to zero. The equation can be expressed as
\begin{gather}
\boldsymbol{s}^{t, l} = H\left(\boldsymbol{u}^{t, l} - V_{th}\right) \\
\boldsymbol{u}^{t+1, l} = \tau\boldsymbol{u}^{t, l} \cdot \left( 1- \boldsymbol{s}^{t, l}\right) + \boldsymbol{W}^{l} \boldsymbol{s}^{t+1, l-1},
\label{eq3}
\end{gather}
where $H$ denotes Heaviside step function. In this paper, leaky factor $\tau$ is set to 0.5 and threshold $V_{th}$ to 0.5. \medskip

\noindent
{\bf Processing of neuromorphic data.} \quad 
The Dynamic Vision Sensor (DVS) triggers an event at a specific pixel point when it detects a significant change in brightness. Formulaically, it can be expressed as
\begin{equation}
  L(x, y, t) - L(x, y, t-\Delta t) \geq pC,
\end{equation}
where $x $ and $y$ denote pixel location and $\Delta t$ means the time since last triggered event at $(x, y)$. $p$ is polarity of brightness change and $C$ is a constant contrast threshold.
In this way, DVS triggers a number of events $ \varepsilon$ during a time interval in the form $\varepsilon = \{ (x_i, y_i, t_i, p_i)\}_{i=1}^N $. 
Due to the large number of events, we integrate them into frames in order to facilitate processing as previous works \cite{wu2019direct, he2020comparing, fang2021incorporating, shen2022eventmix}. Specifically, the events are divided into T slices, and all events in each slice are accumulated. The $j$-th $\left(0 \leq j \leq T-1\right)$ slice event after integration, $E(j, x, y, p)$, can be defined as
\begin{gather}
  E(j, x, y, p) = \sum_{j_s}^{j_e - 1} \mathbf{1}_{x, y, p}\left(x_i, y_i, p_i\right) \\
  j_s=\lfloor \frac{N}{T} \rfloor \cdot j, \quad j_e=\lfloor \frac{N}{T} \rfloor \cdot (j + 1),
\end{gather}
where $\mathbf{1}_{x, y, p}\left(x_i, y_i, p_i\right)$ is an indictor function. $j_s$ and $j_e$ are the start and end index of event in $j$-th slice. \medskip

\noindent
{\bf Feature Similarity Measurements.} \quad
In order to measure the difference between static image and DVS data, we need to calculate the distance and similarity between them. The Hilbert-Schmidt Independence Criterion (HSIC)~\cite{gretton2005measuring} can be used to measure whether two sets of data are independent. For a Reproducing Kernel Hilbert Space $\mathcal{H}$, all eigenfunctions of the kernel function constitute a set of orthogonal bases for the space. So we obtain the matrix form of HSIC under finite samples as follows
\begin{equation}
  \operatorname{HSIC}(K, L)=\frac{1}{(n-1)^2} \operatorname{tr}(K J L J),
\end{equation}
where $J$ is the centering matrix $J_n=I_n-\frac{1}{n} 11^{\mathrm{T}}$, here $I_n$ is an $n$ order unit matrix. $\operatorname{tr}$ means trace of matrix. 
A normalized approach based on HSIC, i.e., central kernel alignment (CKA)~\cite{cortes2012algorithms, cristianini2001kernel}, can make HSIC invariant to isotropic scaling. The formula can be written as:
\begin{equation}
  \operatorname{CKA}(K, L)=\frac{\operatorname{HSIC}(K, L)}{\sqrt{\operatorname{HSIC}(K, K) \operatorname{HSIC}(L, L)}}.
\label{Eq8}
\end{equation}

\citet{kornblith2019similarity} introduced CKA as a similarity index to better measure neural network representation similarity. 
Let $K_{i j}=k\left(\mathbf{x}_i, \mathbf{x}_j\right)$ and $L_{i j}=l\left(\mathbf{y}_i, \mathbf{y}_j\right)$, where  k and l are kernels and can be simply chosen as linear kernel.
\citet{zhan2021effective} demonstrates the feasibility of using CKA as a distance metric in deep SNN.
And in this paper, we use CKA as the distance criterion between static image domain and DVS data domain.


\begin{figure*}[tbp]
	\centering
		\includegraphics[width=0.95\linewidth]{method.pdf}
	\caption{Proposed knowledge transfer framework for spiking neural network. Static image and DVS data are input simultaneously and share the network weights except for the last layer. The membrane potential of the neurons in the second-last layer is used to calculate the domain loss and semantic loss. Static data is replaced with DVS in certain probability. }
	\label{fig2}
\end{figure*}

\section{Methods}

We expect to use the source domain (static images) data to help learn better SNN model for the target domain (DVS data).
This is a supervised domain adaptation problem since we can make use of labeled target domain data despite the fact that they are fewer. 
In this section, we first formalize the method in Sec. \ref{s1}, and then introduce the color space transformation and dimension alignment in Sec. \ref{s2}. The design of the loss function in Sec. \ref{s3}. Sec. \ref{s4} and \ref{s5} elaborate the pipeline and training strategy of proposed method.  

\subsection{Methodology formulation}
\label{s1}
A labeled source domain $\mathcal{D}_s=\left\{x_s^i, y_s^i\right\}_{i=1}^N$ and small labeled target domain $\mathcal{D}_t= \left\{x_t^i, y_t^i\right\}_{i=1}^{M}$ with feature space $\mathcal{X}_s$ and $\mathcal{X}_t$ respectively. The feature $x_s^i \in \mathcal{X}_s$ and $x_t^i \in \mathcal{X}_t$. 
In our task, two domain have the same category space and conditional probability distribution, i.e., $\mathcal{Y}_s=\mathcal{Y}_t$ and $Q_s\left(y_s \mid \mathbf{x}_s\right)=Q_t\left(y_t \mid \mathbf{x}_t\right)$. 
Let us use $\mathbf{x}$ as a vector representation of the domain data and $\mathbf{X}$ as the whole domain data. 
For static image domain and DVS data domain, the feature space and marginal distribution of these two domains is different due to the difference in sensor type, i.e., $\mathcal{X}_s \neq \mathcal{X}_t$ and $P_s\left(X_s\right) \neq P_t\left(X_t\right)$.

We aim to leverages $\mathcal{D}_s$ to assist in learning a better classifer $f: \mathbf{x}_t \mapsto \mathbf{y}_t$ to predict $\mathcal{D}_t$ label $\mathbf{y}_t \in \mathcal{Y}_t$.
The model for function $f$ involves a composition of two functions, i.e., $f=h \circ g$. Here $g: \mathcal{X} \rightarrow \mathcal{Z}$ represents an embedding of the input space $\mathcal{X}$ into a feature space $\mathcal{Z}$, and $h: \mathcal{Z} \rightarrow \mathcal{Y}$ is a function that predicts outputs from the feature space. With this notation we would have $f_s=h_s \circ g_s$ and $f_t=h_t \circ g_t$.
In this paper we utilize the final classification head of the original model as $h_t$. This function is learned solely through supervised signal update gradients.
There are a large number of labeled image that can be studied directly for static image so $h_s$ are not the object of this study.
Critically, we want to provide a generalization of $g_t$ which can pave the way for learning of $h_t$ to improve the generalizability of SNN. 

As mentioned before, due to the different sensory devices, the two data domains have different feature spaces and they have different dimensions. This type of domain adaptation (DA) is heterogeneous and
unlike homogeneous DA approaches, there is not much work focused on heterogeneous DA as far as depth approaches are concerned, and the solution for heterogeneous deep DA is still similar to some homogeneous DA approaches~\cite{wang2018deep}.

In this paper we use a discrepancy-based approach.
The embedding function $g$ would be modeled by network sharing between the source and target domains using all layers before the last classification layer.
 At this point the shared $g_t=g_s=g$, the objective of optimization is finding the satisfied $g$ in its hypothetical space $\mathcal{G}$:
\begin{small}
 \begin{equation}
  \mathop{\arg\min}\limits_{g \in \mathcal{G}} \left(d\left(p\left(g\left(X_s^a\right)\right), p\left(g\left(X_t^a\right)\right)\right) - d\left(p\left(g\left(X_s^a\right)\right), p\left(g\left(X_t^c\right)\right)\right) \right),
  \label{dtarget}
\end{equation}
\end{small}
where $X_s^a$ and $X_t^a$ refer to the same classes of data in the source and target domains while $X_s^a$ and $X_t^c$ mean the data from different classes. The $d$ is a metric for judging similarity between two domain, we choose CKA here.
One of the purpose of Eq. \ref{dtarget} is to align the distributions of the features in the embedding space $\mathcal{Z}$, whose features are assumed to be domain invariant. The other is learning unique features of DVS data. These two aspects facilitate finding a more generalized $g$.

\subsection{HSV color space}
\label{s2}
The event data can be generated by rich local intensity changes in continuous time. One way is to use the relative motion of the image and the camera, e.g., \citet{orchard2015converting} employed the camera movement of saccade, \citet{li2017cifar10} employed image movement instead of camera movement; \citet{li2022n} reconstruct the written record of strokes into a video of writing tracks. 
All these approaches use DVS to sense the change of brightness of each pixel point in the image in an asynchronous manner and output a stream of events. The static images we use directly are in RGB color space, and the three components (red, green, blue) are sensitive to luminance, i.e., whenever the luminance changes, all three components change accordingly. Therefore, using RGB to reflect light intensity is not intuitive enough.
For the above reason, we convert the still RGB image to HSV color space with three components (hue, saturation, value), as expressed in the following equation.

\begin{equation}
  H=\begin{cases}
    0^{\circ} , & \text{if} \enspace \Delta=0 \\
    60^{\circ} \times \frac{G^{\prime}-B^{\prime}}{\Delta}+0^{\circ} , & \text{if} \enspace C_{\text{max}} =R^{\prime} \\
    60^{\circ} \times \frac{B^{\prime}-R^{\prime}}{\Delta}+120^{\circ} , & \text{if} \enspace C_{\text{max}} =G^{\prime} \\
    60^{\circ} \times \frac{R^{\prime}-G^{\prime}}{\Delta}+240^{\circ} , & \text{if} \enspace C_{\text{max}} =B^{\prime}
  \end{cases}
\end{equation}

\begin{equation}
  S=\begin{cases}
    0, & \text{if} \enspace C_{\text{max}} = 0 \\
    \frac{\Delta}{C_{\text{max}}}, & \text{if} \enspace C_{\text{max}} \neq 0
  \end{cases}
\end{equation}

\begin{equation}
  V=C_{\text{max}}
\end{equation}
where $R^{\prime}, G^{\prime}, B^{\prime}$ here are the normalized values of $R, G, B$,
$C_{\text{max}}=\max \{R^{\prime}, G^{\prime}, B^{\prime}\}, C_{\text{min}}=\min \{R^{\prime}, G^{\prime}, B^{\prime}\}$ and $\Delta=C_{\text{max}}-C_{\text{min}}$. 
$V$ indicates the degree of brightness of the color, for the light source color, the brightness value $V$ and the luminosity of the luminous body are related.
To match with 2-channel, i.e., positive and negative polarity DVS data, we duplicated V channel and then input to the network simultaneously with DVS data.
\subsection{Loss function}
\label{s3}
Our loss contains three parts, classification loss for optimizing $h_t$, domain loss and semantic loss for optimizing shared $g$. 
We describe them in detail as following. \medskip

\noindent
{\bf Classification loss.}
For the classification loss, we choose the TET loss~\cite{deng2022temporal}, which is proven to compensate the momentum loss of surrogate gradient and make SNN have better generalizability. The formula is described as follows
\begin{equation}
  \mathcal{L}_{TET}=\frac{1}{T}\sum_{t=1}^T \left( (1-\lambda) \ell_{ce}\left(\mathbf{O}(t), \mathbf{y}\right) + \lambda \operatorname{MSE}\left(\mathbf{O}(t), V_{th}\right) \right),
\end{equation}
where $\mathbf{O}(t)$ represents the presynaptic input current to the output layer. The two items before and after the plus symbol are the cross-entropy loss and the mean-squared error, with the combination of coefficients of $1-\lambda$ and $\lambda$ respectively.\medskip

\noindent
{\bf Domain loss.}
The inputs to dual streams come from static image and DVS data respectively. 
The closer the value of CKA is to 1 indicates that the two vectors are more correlated. For this reason, we subtract the CKA from 1, minimizing the loss i.e., maximizing the correlation of the two inputs. We express the samples $\mathbf{x}_s, \mathbf{x}_t$ drawn from the whole data $\mathbf{X}_s, \mathbf{X}_t$, in this way, domain loss can be expressed as
\begin{equation}
  \mathcal{L}_{d}(g)=1 - \frac{1}{T}\sum_{t=1}^T \mathop{CKA}\limits_{y_i = y_j, y \in \mathcal{Y}}\left(g\left(\mathbf{x}_s^i, t\right), g\left(\mathbf{x}_t^j, t\right)\right) .
\label{Eq14}
\end{equation}
Here we use $g\left(\mathbf{x}_s^i, t\right)$ indicates the value of the input after the shared parameter function $g$, $t$ is brought in to emphasize that here is the output of $g$ at time $t$.
Two samples $\mathbf{x}_s^i, \mathbf{x}_t^j$ are sampled from the same class, expressed by the formula $y_i = y_j$.\medskip

\noindent
{\bf Semantic loss.}
The representation of semantic loss is the opposite of domain loss, which is equal to the similarity between samples belonging to different categories in two domains, minimizing semantic loss means maximizing the difference between samples of different classes. The formula is expressed as 
\begin{small}
\begin{equation}
  \mathcal{L}_{s}(g)=\mathop{max}\left(0, \frac{1}{T}\sum_{t=1}^T\mathop{CKA}\limits_{y_i \neq y_j, y \in \mathcal{Y}}\left(g\left(\mathbf{x}_s^i, t\right), g\left(\mathbf{x}_t^j, t\right)\right) - m \right),
\label{Eq15}
\end{equation}
\end{small}
Here $m$ is the margin of similarity, that is, the loss is considered zero when the similarity is below $m$, and there is no further optimization. The meaning of $m$ is that the similarity of two inputs from different domains can be controlled arbitrarily. For example, when the differences are well characterized, i.e., they are far enough apart to have low similarity, there is no need to waste time reducing similarity in that sample pair, so further training will focus on other sample pairs that are more difficult to separate. \medskip

\noindent
{\bf Total loss.} With the above statement, the total training loss is the sum of the classification loss, domain loss and semantic loss.
That is:
\begin{equation}
  \mathcal{L}_{all}= \mathcal{L}_{ce} + \lambda_d \mathcal{L}_{d} + \lambda_s \mathcal{L}_{s}.
   \label{totalloss}
\end{equation}

One thing to mention is that domain loss and semantic loss work only within pre-set epochs while classification loss works throughout the training process. 

\begin{table*}[tbp]
   \centering
   \begin{threeparttable}
     \resizebox{\linewidth}{!}{
       \begin{tabular}{cccccc}
         \toprule
         \textbf{Dataset} & \textbf{Model} & \textbf{Methods} & \textbf{Architecture} & \textbf{Simulation Length} & \textbf{Accuracy}\\
         \midrule
         \multirow{12}{*}{\text { CIFAR10-DVS }} & \citet{zheng2021going} & \text { STBP-tdBN } & \text { ResNet-19 } & 10 & 67.80 \\
         & \citet{kugele2020efficient} & \text { Streaming Rollout } & \text { DenseNet } & 10 & 66.80 \\
         & \citet{wu2021liaf} & \text { LIAF } & \text { LIAF-Net } & 10 & 70.40 \\
         & \citet{li2021differentiable} & \text{Dspike} & \text{ResNet-18} & 10 & 75.4\\
         & \citet{fang2021incorporating} & \text{PLIF} & \text{SNN4} &20 & 74.8\\
         & \citet{yao2021temporal} & \text{TA-SNN} & SNN5 & 10 & 72.0\\
         & \citet{li2022neuromorphic} & \text { NDA } & \text { VGGSNN } & 10 & 81.70\\
         & \citet{deng2022temporal} & \text { TET } & \text { VGGSNN } & 10 & $8 3 . 1 7 \pm 0.15$ \\
         & \citet{zhu2022tcja} & \text{TJCA-TET} & \text{VGGSNN} & 10 & 83.3\\
         & \citet{duantemporal} & \text{TEBN} & \text{VGGSNN} & 10 & 84.9\\
         \cline { 2 - 6 }
         & \textbf { Our model } & \text { Transfer } & \text { VGGSNN } & 10 & $84.60 \pm 0.14 \; \mathbf{(84.8)}$\\
         & \textbf { Our model } & \text { Transfer } & \text { VGGSNN } & 10 & $84.67 \pm 0.39 \; \mathbf{(85.2)}$ \tnote{2}\\
         \midrule
         \multirow{6}{*}{\text { N-Caltech101 }} & Li \citet{li2022neuromorphic} & \text { NDA } & \text { VGGSNN} & 10 & 78.2 \\
         & \citet{deng2022temporal} & \text { TET } & \text { VGGSNN } & 10 & $79.77 \pm 0.77 $\tnote{1} \\
         & \citet{shen2022eventmix} & \text {EventMixer} & \text{ResNet-18} & 10 & 79.5\\
         & \citet{zhu2022tcja} & \text{TJCA-TET} & \text{CombinedSNN} & 14 & 82.5\\
         \cline { 2 - 6 }
         &\textbf { Our model } & \text { Transfer } & \text { VGGSNN } & 10 & $81.46 \pm 0.82 \; \mathbf{(82.30)}$ \\
         &\textbf { Our model } & \text { Transfer } & \text { VGGSNN } & 10 & $81.57 \pm 0.71 \; \mathbf{(82.52)}$\tnote{2} \\
         \midrule
         \multirow{3}{*}{\text { N-Omniglot }} & \citet{li2022n}  & \text { plain } & \text { SCNN } & 12 & 60.0 \\
         & \citet{li2022n} & \text { plain } & \text { SCNN } & 12 & $61.73 \pm 0.41 \; (62.23)$  \tnote{1} \\
         \cline { 2 - 6 }
         & \textbf { Our model } & \text { Transfer } & \text { SCNN } & 12 & $63.03 \pm 0.14 \; \mathbf{(63.22)}$  \\
         \bottomrule
     \end{tabular}
     }
     \begin{tablenotes}
       \item[1] Our implement
       \item[2] Using label smoothing
     \end{tablenotes}
   \end{threeparttable}
   \caption{Experimental results compared with existing works. The best accuracy is shown in parentheses.}
   \label{sota}
 \end{table*}

\subsection{Single-stage training pipeline}
\label{s4}
During the training process, we gradually replace a portion of the static input with DVS data probabilistically, so that the role of domain loss is smoothly decreased and the semantic loss gradually moves from distinguishing different classes of sample distances across domains to different classes of distances within the DVS domain.
Separately, with $b_{i}$ denoting index of training batch, $b_{l}$ denoting total length of training batch; $e_c$ means current epoch and $e_m$ means maximum training epoch,
then probability of making a substitution $P_{replacement}$ could be expressed by the following equation
\begin{equation}
  P_{replacement} = \left(\frac{b_i + e_c * b_l}{e_s * b_l}\right) ^ 3,
\end{equation}
where $e_s$ is a manual settings epoch for the end of the domain loss and semantic loss effects. The value of $e_s$ if usually set to half of the total training epoch.

In summary, our framework first converts the static images from RGB to HSV and aligns the dimensions. And then for dual-stream inputs, the model shares the network parameters of all layers before the final classification layer to learn domain-invariant features by domain loss and unique features by semantic loss.  Static image inputs are gradually replaced with DVS data inputs by increasing probability with the number of training epochs until the set epochs is reached, all static images are replaced with DVS data, and the feature extraction period ends. Finally, we fine-tune the SNN on the DVS data to get better performance.  The whole process is shown in Fig. \ref{fig2}.

\subsection{Training strategy}
\label{s5}
We use gradient descent to directly train the SNN. 
For classification loss, the last layer of neurons only receives and accumulates presynaptic input currents and does not fire spikes, so the derivative of $\mathcal{L}_{\mathrm{TET}}$ with respect to $W$ is
\begin{small}
  \begin{gather}
    \frac{\partial \mathcal{L}_{\mathrm{TET}}}{\partial W^l}=\sum_{t=1}^T \frac{\partial \mathcal{L}_{\mathrm{TET}}}{\partial O(t)} \frac{\partial O(t)}{\partial W^l} \\
      \frac{\partial \mathcal{L}_{\mathrm{TET}}}{\partial O(t)}=\frac{1}{T} \left( (1-\lambda)\left(S(O(t))-\hat{y}\right) + \lambda(O(t) - V_{th}) \right)
  \end{gather}
\end{small}
where $\hat{y}$ means one-hot and $S$ means softmax function.

The partial derivative of the loss $\mathcal{L}_d$ with respect to $W$ can be expressed as
\begin{equation}
  \frac{\partial \mathcal{L}_{d}}{\partial W^l}=\sum_{t=1}^T \frac{\partial \mathcal{L}_{d}}{\partial s^{t, l}} \frac{\partial s^{t, l}}{\partial u^{t, l}} \frac{\partial u^{t, l}}{\partial W^l}.
\label{Eq20}
\end{equation}

Denote the kernel matrix of source and target domains by $K_S$, $K_T$, and the membrane potential inputs from the source and target domains by $u_S$, $u_T$, respectively. The first term can be expanded as
\begin{equation}
  \begin{aligned}
  \frac{\partial L_d}{\partial s_S^{t, l}}= & -\frac{1}{T} \sum_{t=1}^T\frac{\partial \mathrm{CKA}\left(K_S, K_T\right)}{\partial s_S^{t, l}} \\
  = & -\frac{1}{T} \sum_{t=1}^T\frac{\partial \operatorname{CKA}\left(K_S, K_T\right)}{\partial \operatorname{HSIC}\left(K_S, K_T\right)} \frac{\partial \operatorname{HSIC}\left(K_S, K_T\right)}{\partial s_S^{t, l}} \\
  & -\frac{1}{T} \sum_{t=1}^T\frac{\partial \operatorname{CKA}\left(K_S, K_T\right)}{\partial \operatorname{HSIC}\left(K_S, K_S\right)} \frac{\partial \operatorname{HSIC}\left(K_S, K_S\right)}{\partial s_S^{t, l}}
  \end{aligned}
  \label{Eq21}
\end{equation}
Calculating $\frac{\partial \mathrm{CKA}\left(K_S, K_T\right)}{\partial \operatorname{HSIC}\left(K_S, K_T\right)}$  and $\frac{\operatorname{HSIC}\left(K_S, K_T\right)}{\partial s_S^{t, l}}$ as an example, from Eq. \ref{Eq8}
\begin{equation}
  \frac{\partial \mathrm{CKA}\left(K_S, K_T\right)}{\partial \operatorname{HSIC}\left(K_S, K_T\right)}=\frac{1}{\left[\operatorname{HSIC}\left(K_S, K_S\right) \operatorname{HSIC}\left(K_T, K_T\right)\right]^{1 / 2}}
\label{Eq22}
\end{equation}

 $\frac{\operatorname{HSIC}\left(K_S, K_T\right)}{\partial s_S^{t, l}}$ in Eq. \ref{Eq21} can be expressed as
\begin{equation}
\begin{aligned}
    & \left( \sum_{j=1}^n\frac{\partial k_S\left(u_{S_i}^{t,l}, u_{S_j}^{t,l}\right)}{\partial u_{S_i}^{t,l}} A (k_T)_{ji}+ \sum_{j=1}^n \frac{\partial k_S\left(u_{S_j}^{t,l}, u_{S_i}^{t,l} \right)}{\partial u_{S_i}^{t,l}} A (k_T)_{ij} \right. \\
    &-\left. \frac{\partial k_S\left(u_{S_i}^{t,l}, u_{S_i}^{t,l} \right)}{\partial u_{S_i}^{t,l}}  A (k_T)_{ii}\right) \frac{\partial u_{S_i}^{t,l}}{(n-1)^2 \partial s_{S_i}^{t, l}}
\end{aligned}
\label{Eq23}
\end{equation}
where $A(k_T)_{ij}$ should be 
\begin{equation}
  k_T\left(u_{T_i}^{t,l}, u_{T_j}^{t,l}\right)+\sum_{k=1}^n \sum_{m=1}^n k_T\left(u_{T_k}^{t,l}, u_{T_m}^{t,l}\right)-2 \sum_{m=1}^n k_T\left(z_{T_i}^l, z_{T_m}^l\right)
  \label{Eq24}
\end{equation}

For second term $\frac{\partial s^{t, l}}{\partial u^{t, l}}$, we use Piece-wise linear funciton~\cite{xu2022securing} surrogate gradient, the formula as follows
\begin{equation}
  \frac{\partial s^{t, l}}{\partial u^{t, l}}= max (0, 1 - \vert u^{t,l} \vert)
\end{equation}

As for third term in Eq. \ref{Eq20}, from Eq. \ref{eq3}, It can be expressed cyclically as
\begin{equation}
  \begin{aligned}
  \frac{\partial u^{t, l}}{\partial W^l}= & \tau\left(1-s^{t-1, l}\right) \frac{\partial u^{t-1, l}}{\partial W^l} \\
  & -\tau u^{t-1, l} \frac{\partial s^{t-1, l}}{\partial u^{t-1, l}} \frac{\partial u^{t-1, l}}{\partial W^l}+s^{t, l-1} .
  \end{aligned}
\end{equation}

The derivation of $\partial\mathcal{L}_s / \partial W^l$ is similar to Eq. \ref{Eq21}, \ref{Eq22}, \ref{Eq23}. More detail please refer to supplementary materials.

\begin{figure}[tbp]
  \centering
  \subfigure[10\% training data]{
    \includegraphics[width=0.8\linewidth]{training_acc_1.pdf}
    \label{fig:3a}
  }
  \subfigure[100\% training data]{
    \includegraphics[width=0.8\linewidth]{training_acc_2.pdf}
    \label{fig:3b}
  }
  \caption{Classification accuracy with epoch for CIFAR10-DVS data on the whole test data with different amounts of training data. (a) training with 10\% training data on CIFAR10-DVS, (b) training with 100\% training data on CIFAR10-DVS}
  \label{fig3}
\end{figure}

\section{Experiments}
In this section, we conduct experiments on three mainstream neuromorphic datasets: CIFAR10-DVS \cite{li2017cifar10}, N-Caltech 101 \cite{orchard2015converting} and N-Omniglot \cite{li2022n} to evaluate the effectiveness of the proposed method. We choose AdamW \cite{loshchilov2017decoupled} as the optimizer with an initial learning rate of 1e-3. The total training epoch is 600 and the warmup epoch is 5. 
For a fair comparison, for the first two datasets, the network model is chosen as VGGSNN (64C3-128C3-AP2-AP2-256C3-256C3-AP2-512C3-512C3-AP2-512C3-512C3-AP2-FC) with step 10. For the N-omniglot, we use the network structure SCNN (15C5-AP2-40C5-AP2-FC-FC) from the original paper with step 12. More detail please refer to supplementary materials. 
All experiments are based on the Pytorch framework.


\subsection{Comparsion with the State-of-the-Art}
We first evaluate the proposed method on the CIFAR10-DVS dataset and compare the proposed method with TdBN \cite{zheng2021going}, Streaming Rollour \cite{kugele2020efficient}, LIAF \cite{wu2021liaf}, Dspike \cite{li2021differentiable}, PLIF \cite{fang2021incorporating}, TA-SNN \cite{yao2021temporal}, NDA \cite{li2022neuromorphic}, TET \cite{deng2022temporal}, TJCA-TET \cite{zhu2022tcja}, TEBN \cite{duantemporal}. The results are presented in Tab. \ref{sota}.
In fairness, we perform three independent experiments using different random seeds and report the mean and standard deviation of the results. The best accuracy is indicated in parentheses. 
The experimental results demonstrate that the proposed method can achieve state-of-the-art performance on CIFAR10-DVS dataset compared with existing methods. In addition, we observe that the trick of label smoothing is beneficial to alleviate the overfitting, leading to a 0.4\% accuracy improvement.

As for N-Caltech 101 dataset, there are fewer available results. We use TET, NDA, EventMixer \cite{shen2022eventmix}, and TJCA-TET as baseline. 
The results in Tab. \ref{sota} show that our method outperforms all baselines and similarly, label smoothing brings a modest improvement in accuracy.
For N-omniglot, we reproduce the original result and compare it with the proposed method. Experimental results show that our proposed method improves 1\% accuracy over the original method.

\begin{table}[tbp]
  \centering
  \begin{threeparttable}
    \resizebox{\linewidth}{!}{
      \begin{tabular}{cccc}
        \toprule
        NetWork & Dataset &Methods & Accuracy\\
        \midrule
        \multicolumn{4}{c}{\textbf{Data with no Noise}}\\
        \midrule
        \multirow{6}{*}{VGG\_SNN} &\multirow{3}{*}{CIFAR10-DVS}& origin &83.60\%\\
        & & domainLoss & 84.10\%\\
        & & domainLoss + semanticLoss & \textbf{84.50\%}\\
        \cline{2-4}
         &\multirow{3}{*}{N-Caltech101}& origin &79.54\%\\
        & & domainLoss & 80.46\%\\
        & & domainLoss + semanticLoss & \textbf{81.72\%}\\
        \midrule
        \multicolumn{4}{c}{\textbf{70\% Training Data}}\\
        \midrule
        \multirow{6}{*}{VGG\_SNN} &\multirow{3}{*}{CIFAR10-DVS}& origin &80.70\%\\
        & & domainLoss & 81.50\%\\
        & & domainLoss + semanticLoss & \textbf{82.30\%}\\
        \cline{2-4}
         &\multirow{3}{*}{N-Caltech101}& origin &76.78\%\\
        & & domainLoss & 78.51\%\\
        & & domainLoss + semanticLoss & \textbf{79.20\%}\\
        \midrule
        \multicolumn{4}{c}{\textbf{40\% Training Data}}\\
        \midrule
        \multirow{6}{*}{VGG\_SNN} &\multirow{3}{*}{CIFAR10-DVS}& origin &76.50\%\\
        & & domainLoss & 76.80\%\\
        & & domainLoss + semanticLoss & \textbf{77.90\%}\\
        \cline{2-4}
         &\multirow{3}{*}{N-Caltech101}& origin &67.93\%\\
        & & domainLoss & \textbf{71.84\%}\\
        & & domainLoss + semanticLoss & 71.49\%\\
        \midrule
        \multicolumn{4}{c}{\textbf{10\% Training Data}}\\
        \midrule
        \multirow{6}{*}{VGG\_SNN} &\multirow{3}{*}{CIFAR10-DVS}& origin &58.60\%\\
        & & domainLoss & 60.50\%\\
        & & domainLoss + semanticLoss & \textbf{61.30\%}\\
        \cline{2-4}
         &\multirow{2}{*}{N-Caltech101}& origin &45.40\%\\
        & & domainLoss & 54.71\%\\
        & & domainLoss + semanticLoss & \textbf{55.17\%}\\
        \bottomrule
    \end{tabular}
    }
  \end{threeparttable}
  \caption{Ablation experimental results with VGGSNN.}
  \label{Ablation}
\end{table}

\subsection{Ablation study} 

In our approach, the first step is to extract domain-invariant features using domain loss, followed by adding semantic loss to better extract features from the DVS data. Thus, the ablation experimental section shows the results of baseline, with domain loss, with domain loss and semantic loss.
As shown in Fig. \ref{fig3}, baseline has overfitted at about 100-200 epochs earlier, i.e., the accuracy on the test set does not increase in subsequent training rounds. In contrast, the introduction of domain loss and semantic loss provides a good model at the end of the action period, and the green line is always at the top in the later fine-tuning phase, indicating that the best results can be achieved with these two losses.

Moreover, we conduct a detailed evaluation of our proposed approach on CIFAR10-DVS and N-Caltech101 datasets using varying amounts of training data, as presented in Tab. \ref{Ablation}. 
Our results show that regardless of training data amount, with domain loss results in higher performance than the baseline method, while the combination of both domain and semantic losses outperforms the method with only domain loss. 
Importantly, our proposed approach achieves a remarkable performance improvement of nearly 10\% on N-Caltech101 when using only 10\% training data. 
This indicates that the fewer data available, the easier it is to overfit existing methods. Providing generalizability knowledge helps to alleviate this problem.

\begin{figure}[tbp]
  \centering
  \subfigure[Baseline, CIFAR10-DVS]{
    \includegraphics[width=0.46\linewidth]{dvsc10_baseline.pdf}
    \label{fig:4a}
  }
  \subfigure[Our method, CIFAR10-DVS]{
    \includegraphics[width=0.46\linewidth]{dvsc10_transfer.pdf}
    \label{fig:4b}
  }
  \subfigure[Baseline, N-Caltech101]{
    \includegraphics[width=0.46\linewidth]{ncaltech101_baseline.pdf}
    \label{fig:4c}
  }
  \subfigure[Our method, N-Caltech101]{
    \includegraphics[width=0.46\linewidth]{ncaltech101_transfer.pdf}
    \label{fig:4d}
  }
  \caption{The loss landscape of VGGSNN on CIFAR10-DVS and N-Caltech101 at the 300th epoch.}
  \label{fig4}
\end{figure}

\subsection{Analysis}

\noindent
{\bf Loss losslandscape} \quad
To evaluate the impact of domain and semantic loss in providing generalization weight for SNN fine-tuning on DVS data, we utilize 2D lossscape visualization~\cite{li2018visualizing}.
For this purpose, we selected the model weights at the 300th epoch, which is the moment when the effect of loss ends, for CIFAR10-DVS and N-Caltech101 at 10\% of the training data. 
As depicted in Fig.~\ref{fig:4b} and Fig.~\ref{fig:4d}, the maximum loss decreases compared to Fig.~\ref{fig:4a} and Fig.~\ref{fig:4c} and the lowest loss area becomes flatter, which indicates that the SNN obtains better weights with domain loss and semantic loss. \medskip

\noindent
{\bf Visual Explanations from Deep Networks} \quad
To assess whether domain loss and semantic loss effectively learn common and unique features between static images and neuromorphic data, we employ grad-cam++~\cite{chattopadhay2018grad}. This method enables the visualization of the degree of similarity between each location in an image and a category by generating a heat map. Such a visualization allows us to understand which local locations of an original image contributed most significantly to the model's final classification decision. 
Ideally, static pictures and event data that are integrated into frames have similarity in object contour features when they are in the same class, while they should be as distinct as possible between different classes. This is well illustrated in Fig. \ref{fig1}, where by introducing domain and semantic loss, for both static pictures and DVS data, the network pays attention to the contour features of the category. In particular, the results on DVS show that our approach facilitates the correct classification of DVS data.


\subsection{The effect and selection of hyperparameters}
\label{subs}
In this section, we show the effect of hyperparameters in the proposed method and the principles of their selection.

The coefficient of domain loss for learning common features, $\lambda_d$, is set fixed at 1. 
Our method is insensitive to the semantic loss coefficient and performs well within its reasonable range. 
As shown in Tab.~\ref{hyperparameters}, $\lambda_s=0.0$ means only domain loss works, with the addition of semantic loss there is a performance degradation only when its coefficients $\lambda_s$ are particularly large. 
This means that semantic loss should not dominate, and that the learning of common features should be satisfied before the learning of unique features.

As for the marginal coefficient $m$ , it should not be chosen too small, resulting in not working, or too large, limiting the role of semantic loss. As shown in Tab.~\ref{hyperparameters}, the choice of the m = 0.1 is appropriate.

\begin{table}[tbp]
  \centering
  \begin{threeparttable}
    \resizebox{\linewidth}{!}{
      \begin{tabular}{cccccc}
        \toprule
        \multicolumn{6}{c}{\textbf{$m$ is fixed at 0.1}}\\
        \midrule
        hyperparameter & $\lambda_s= 0.0 $  & $\lambda_s= 0.1 $ & $\lambda_s= 0.5 $ & $\lambda_s= 1.0 $ & $\lambda_s = 5.0$\\
        Accuracy & 84.0 & 84.3 & \textbf{84.5} & 84.2 & 83.3\\
        \midrule
        \multicolumn{6}{c}{\textbf{$\lambda_s$ is fix at 0.5}}\\
        \midrule
        hyperparameter & $m = 0.0 $ & $m = 0.05$  & $m= 0.1 $ & $m= 0.5 $ & $m= 1.0 $\\
        Accuracy & 84.1 &84.1& \textbf{84.5} & 83.6 & 84.0\\
        \bottomrule
    \end{tabular}
    }
  \end{threeparttable}
  \caption{Different hyperparameter settings with VGGSNN.}
  \label{hyperparameters}
\end{table}


\section{Conclusion}
In this paper, we propose a single-stage transfer learning framework based on a large number of annotated static datasets to improve the generalizability of SNN networks on DVS data. We propose domain loss and semantic loss to learn domain-invariant and unique features between static image and DVS data. 
Visualizations on loss-landscape and Class Activation Mapping show that the learned features benefit the performance of SNN on DVS data.
Experimental results show that our method achieves the best performance on mainstream DVS datasets. 

\bibliography{example_paper}
\bibliographystyle{icml2022}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thealgorithm}{S\arabic{algorithm}}
\renewcommand{\theequation}{S\arabic{equation}}
\setcounter{table}{0}
\setcounter{figure}{0}
\section{Introduction of datasets and their processing}
\noindent
An overview of the datasets used in our experiments is shown in Tab. \ref{datasets}.
\medskip

\noindent
{\bf CIFAR10.} \quad
The CIFAR10 dataset~\cite{krizhevsky2009learning} has 60,000 color images, which are 32*32 and divided into 10 classes with 6,000 images in each class. There are 50,000 images for training, forming 5 training batches of 10,000 images each, and another 10,000 for testing, forming a separate batch. \medskip


\noindent
{\bf CIFAR10-DVS.} \quad
The CIFAR10-DVS dataset~\cite{li2017cifar10} is the neuromorphic version of CIFAR10. It converts 10,000 frame-based images from the CIFAR-10 dataset, displayed on an LCD display by repetitive closed-loop smoothing (RCLS) motion, into a stream of 10,000 event-based sensors with a resolution of 128 √ó 128 pixels.
\medskip

\noindent
{\bf Caltech101.} \quad
The Caltech101 dataset~\cite{li_andreeto_ranzato_perona_2022} contains images from 101 object categories and one background category, for a total of 9,145 images, and for each object category, approximately 40 to 800 images.
\medskip


\noindent
{\bf N-Caltech 101.} \quad
The N-Caltech101 dataset~\cite{orchard2015converting} is a neuromorphic version of the original Caltech101 dataset. The original data is displayed on an LCD monitor while being captured by using the saccade method of camera movement. N-Caltech 101 removes the "faces" class from the original dataset to avoid confusion with "simple faces". N-Caltech 101 has 100 object classes and one background class, with a total of 8709 samples. 
\medskip


\noindent
{\bf Omniglot.} \quad
The Omniglot dataset~\cite{lake2015human} consists of 1,623 handwritten characters from 50 different languages, each with 20 different handwritings, and is a class of small sample handwritten character datasets. 1,200 characters are usually selected as the training set, and the remaining 423 characters as the validation set. 
\medskip

\noindent
{\bf N-Omniglot.} \quad
The N-Omniglot dataset~\cite{li2022n} is the first neuromorphic dataset for few-shot learning using SNNs.  The written record of strokes are reconstructed into a video of writing tracks and then DVS is used to obtain the event records to get the neuromorphic version of Omniglot. Its number of samples is consistent with Omniglot. 
\medskip

\noindent
{\bf Processing of static datasets.} \quad
For all static datasets, we merge the training set with the validation set and randomly select samples from the same category as the input DVS data for the paired input of static images and DVS data. We resize them in a bilinear interpolation manner to be consistent with the DVS data and use data augmentation. Our method uses HSV space for better characterization and therefore excludes grayscale images from the Caltech101 dataset. For the Omniglot dataset, the original images are all grayscale images. Therefore, we simply replicate the single-channel images as two-channel to align with the DVS data dimensions.
\medskip

\noindent
{\bf Processing of DVS datasets.} \quad
The CIFAR10-DVS and N-Caltech 101 datasets are resized to 48 x 48, and the training and validation sets are divided according to 9:1. We used tonic~\cite{lenz_gregor_2021_5079802} package to integrate them into 10 frames per sample. For N-Omniglot dataset, its size and the way of dividing training and validation sets are kept the same as the original dataset Omniglot, i.e., 28 x 28 pixel size and 1200 class characters as training set and 423 class characters as validation set.
The event stream is integrated into 12 frames per sample.
We use data augmentation for CIFAR10-DVS following random cropping, random horizontal flipping, and random rotation, and do not use data augmentation on the other two datasets.

\begin{table}[htbp]
  \centering
  \begin{threeparttable}
    \resizebox{0.7\linewidth}{!}{
      \begin{tabular}{cccc}
      \toprule
        \textbf{Datasets} & \textbf{Type}  & \textbf{Categories} & \textbf{Annotated samples}\\
        \midrule
        CIFAR10 & RGB images & 10 & 60000 \\
        CIFAR10-DVS & DVS event data & 10 & 10000 \\
        Caltech101 & Grayscale and RGB images & 101 & 9145 \\
        N-Caltech 101 & DVS event data & 101 & 8709 \\
        Omniglot & Grayscale images & 1623 & 32460\\
        N-Omniglot & DVS event data & 1623 & 32460 \\
        \bottomrule
    \end{tabular}
    }
  \end{threeparttable}
  \caption{Overview of the datasets used in our experiments.}
  \label{datasets}
\end{table}

\section{Network structure and hyperparameters}

\noindent
{\bf Network structure.} \quad
The network structure used on the CIFAR10-DVS and N-Caltech 101 datasets is VGGSNN: 64C3-128C3-AP2-AP2-256C3-256C3-AP2-512C3-512C3-AP2-512C3-512C3-AP2-FC, where 64C3 means output channel is 64, kernel size is 3, padding is 1 and stride is 1, AP2 is the average-pooling layer with kernel size equals to 2 and stride equals to 2, FC is the fully-connected layer for classification.
The network structure used on the N-Omniglot datasets is SCNN: 15C5-AP2-40C5-AP2-FC-FC.
\medskip

\noindent
{\bf Hyperparameter settings.} \quad
The following parameters need to be set manually in our method: 
TET~\cite{deng2022temporal} coefficient $\lambda$, domain loss coefficient $\lambda_d$, semantic loss coefficient $\lambda_s$ and similarity margin $m$. 
We set $\lambda$ value to 1e-3 and $\lambda_d$ to 1.0 in all cases.
The principles for setting the values of $\lambda_s$ and $m$ have already been discussed in Sec. \ref{subs}.
The values of hyperparameters set for different datasets in this experiment are shown in Tab. \ref{s-hyperparameters}.


\begin{table}[htbp]
    \centering
    \begin{threeparttable}
      \resizebox{0.4\linewidth}{!}{
        \begin{tabular}{ccccc}
        \toprule
          Datasets & $\lambda$  & $\lambda_d$ & $\lambda_s$ & $m$\\
          \midrule
          CIFAR10-DVS & 1e-3 & 1.0 & 0.5 & 0.1 \\
          N-Caltech 101 & 1e-3 & 1.0 & 1e-3 & 0.3 \\
          N-Omniglot & 1e-3 & 1.0 & 0.5 & 0.2\\
          \bottomrule
      \end{tabular}
      }
    \end{threeparttable}
    \caption{Different hyperparameter settings.}
    \label{s-hyperparameters}
  \end{table}



\section{Training strategy}

For Eq. \ref{Eq21}, the first term on the right-hand side of the equation, $\frac{\partial \operatorname{CKA}\left(K_S, K_T\right)}{\partial \operatorname{HSIC}\left(K_S, K_T\right)}$ and $\frac{\partial \operatorname{HSIC}\left(K_S, K_T\right)}{\partial s_S^{t, l}}$, is already given by Eqs. \ref{Eq22}, \ref{Eq23} and \ref{Eq24}.
For the second term:
\begin{equation}
  \frac{\partial \mathrm{CKA}\left(K_S, K_T\right)}{\partial \operatorname{HSIC}\left(K_S, K_S\right)}=-\frac{\operatorname{HSIC}\left(K_S, K_T\right) \operatorname{HSIC}\left(K_T, K_T\right)}{2\left[\operatorname{HSIC}\left(K_S, K_S\right) \operatorname{HSIC}\left(K_T, K_T\right)\right]^{3 / 2}}
\end{equation}
and $\frac{\partial \operatorname{HSIC}\left(K_S, K_S\right)}{\partial s_S^{t, l}}$ can be expressed as
\begin{equation}
  \begin{aligned}
      & \left( \sum_{j=1}^n\frac{\partial k_S\left(u_{S_i}^{t,l}, u_{S_j}^{t,l}\right)}{\partial u_{S_i}^{t,l}} A (k_S)_{ji}+ \sum_{j=1}^n \frac{\partial k_S\left(u_{S_j}^{t,l}, u_{S_i}^{t,l} \right)}{\partial u_{S_i}^{t,l}} A (k_S)_{ij} \right. \\
      &-\left. \frac{\partial k_S\left(u_{S_i}^{t,l}, u_{S_i}^{t,l} \right)}{\partial u_{S_i}^{t,l}}  A (k_S)_{ii}\right) \frac{2\partial u_{S_i}^{t,l}}{(n-1)^2 \partial s_{S_i}^{t, l}},
  \end{aligned}
  \end{equation}
where $A(k_S)_{ij}$ should be 
\begin{equation}
    k_S\left(u_{S_i}^{t,l}, u_{S_j}^{t,l}\right)+\sum_{k=1}^n \sum_{m=1}^n k_T\left(u_{S_k}^{t,l}, u_{S_m}^{t,l}\right)-2 \sum_{m=1}^n k_T\left(z_{S_i}^l, z_{S_m}^l\right)
\end{equation}

As for $\partial\mathcal{L}_s / \partial W^l$, by comparing Eq. \ref{Eq14} and Eq. \ref{Eq15}, it can be obtained that $\partial\mathcal{L}_s / \partial W^l$ is zero when the similarity of different samples in different categories is less than the margin $m$; when it is larger than margin $m$, $\partial\mathcal{L}_s / \partial W^l$ is calculated similarly to $\partial \mathcal{L}_{d}/\partial W^l$ with a negative sign difference.

Our approach can be expressed as algorithm \ref{alg:1}.

\begin{algorithm}[t] 
  \caption{Knowledge transfer for Spiking Neural Networks on event-based datasets} 
  \begin{algorithmic}[1] 
      \STATE {\bfseries input:} network's parameter $\mathbf{\theta}$, SNN time step T, learning rate $\alpha$, training epoch $E$, loss $L$, shared model $g$, classification head $h_t$ and a training batch sample $\mathbf{x}_s^i$ and $\mathbf{x}_t^j$ from static images and DVS data.
      \STATE {\bfseries Ensure:} Different domain samples belong to the same category, i.e., $y_i = y_j, y \in \mathcal{Y}$
  \FOR {$e \gets 1,2, \cdots E$}
  \STATE Replace $\mathbf{x}_{s,m}^i$ with $\mathbf{x}_{t,m}^j$ in the probability of P as Eq. \ref{p-replacement}, where $m$ is the subscript index of a batch.
  \STATE Define empty list $V_{mem,s}, V_{mem,t}, V_{mem,to}$
      \FOR {$t \gets 0, 1, \cdots T$}
      \STATE $V_{mem,s}[t] \gets g(\mathbf{x}_s^i) $
      \ENDFOR
      \STATE Reset the membrane potentials and spikes
      \FOR {$t \gets 0, 1, \cdots T$}
      \STATE $V_{mem,t}[t] \gets g(\mathbf{x}_t^i) $
      \STATE $V_{mem,to}[t] \gets h_t(V_{mem,t}[t]) $
      
      \ENDFOR
  
      \STATE $L_{TET} = \mathcal{L}_{TET}(V_{mem,to}, y)$
      \STATE $L_{d} = \mathcal{L}_d(V_{mem,s}, V_{mem,t})$
      \STATE Rearrange $V_{mem,s}$ according to $ y_i \neq y_j $ subscripts
      \STATE $L_{s} = \mathcal{L}_s(V_{mem,s}, V_{mem,t})$
      \IF{ $e \leq E /2$}
      \STATE $L = L_{TET} + \lambda_dL_{d} + \lambda_sL_{s}$
      \ELSE
      \STATE $L = L_{TET}$
      \ENDIF
  \STATE update parameter $\mathbf{\theta} = \mathbf{\theta} - \alpha \bigtriangledown_{\mathbf{\theta}}L$
  \STATE Reset the membrane potentials and spikes
  \ENDFOR
  \end{algorithmic} 
  \label{alg:1}
  \end{algorithm}


\section{Reproducibility}

All experiments were implemented by BrainCog~\cite{zeng2022braincog}, and the visualization part was done using loss-landscape~\cite{li2018visualizing} and gradcam++~\cite{chattopadhay2018grad}. All source code and training scripts are provided in the supplemental material.


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
