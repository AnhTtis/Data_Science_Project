{
    "arxiv_id": "2303.13077",
    "paper_title": "Improving the Performance of Spiking Neural Networks on Event-based Datasets with Knowledge Transfer",
    "authors": [
        "Xiang He",
        "Dongcheng Zhao",
        "Yang Li",
        "Guobin Shen",
        "Qingqun Kong",
        "Yi Zeng"
    ],
    "submission_date": "2023-03-23",
    "revised_dates": [
        "2023-03-24"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Spiking neural networks (SNNs) have rich spatial-temporal dynamics, which are suitable for processing neuromorphic, event-based data. However, event-based datasets are usually less annotated than static datasets used in traditional deep learning. Small data scale makes SNNs prone to overfitting and limits the performance of the SNN. To enhance the generalizability of SNNs on event-based datasets, we propose a knowledge-transfer framework that leverages static images to assist in the training on neuromorphic datasets. Our method proposes domain loss and semantic loss to exploit both domain-invariant and unique features of these two domains, providing SNNs with more generalized knowledge for subsequent targeted training on neuromorphic data. Specifically, domain loss aligns the feature space and aims to capture common features between static and event-based images, while semantic loss emphasizes that the differences between samples from different categories should be as large as possible. Experimental results demonstrate that our method outperforms existing methods on all mainstream neuromorphic vision datasets. In particular, we achieve significant performance improvement of 2.7\\% and 9.8\\% when using only 10\\% training data of CIFAR10-DVS and N-Caltech 101 datasets, respectively.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13077v1"
    ],
    "publication_venue": null
}