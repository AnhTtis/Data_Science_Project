\documentclass[lettersize,journal,onecolumn]{IEEEtran}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{color}
\usepackage{ragged2e}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{float}
\usepackage{caption}
\captionsetup{justification=raggedright,singlelinecheck=false}
\begin{document}

\title{FixFit: deep learning for optimal model reduction}


\author{
  \IEEEauthorblockN{Botond B. Antal\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}},
  \IEEEauthorblockN{Anthony G. Chesebro\IEEEauthorrefmark{1}\IEEEauthorrefmark{3}},
  \IEEEauthorblockN{Helmut H. Strey\IEEEauthorrefmark{1}\IEEEauthorrefmark{4}},
  \IEEEauthorblockN{Lilianne R. Mujica-Parodi\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}\IEEEauthorrefmark{3}\IEEEauthorrefmark{4}},
  \IEEEauthorblockN{Corey Weistuch\IEEEauthorrefmark{5}}\\
  
  \IEEEauthorblockA{\IEEEauthorrefmark{1}Department of Biomedical Engineering, Stony Brook University, Stony Brook, NY, USA}\\
  \IEEEauthorblockA{\IEEEauthorrefmark{2}Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA}\\
  \IEEEauthorblockA{\IEEEauthorrefmark{3}Renaissance School of Medicine, Stony Brook University, Stony Brook, NY, USA}\\
  \IEEEauthorblockA{\IEEEauthorrefmark{4}Laufer Center for Physical and Quantitative Biology, Stony Brook University, Stony Brook, NY, USA}\\
  \IEEEauthorblockA{\IEEEauthorrefmark{5}Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, USA}\\

\thanks{Email: weistucc@mskcc.org}
}

\date{\today}
\maketitle



\justifying
\section{Supplementary information}
%%
\begin{figure}[htb]
    \begin{center}
	    \includegraphics[scale=1]{
	    figs_cr/FigS5.pdf}
    \end{center}
	\caption*{Fig. S1:
 \textbf{Kepler orbit model: distributions of input parameters in the training dataset.}
 The Kepler model had four input parameters. These were sampled for the training dataset using a Sobol sequence algorithm in four dimensions. The sampled parameter sets were then filtered based on eccentricity criteria. The above histograms show the resultant distributions for the four input parameters from 2,276 samples that passed these criteria.
 }
	\label{fig:kep_inp}
\end{figure}

\clearpage
\begin{figure}%[htb]
    \begin{center}
	    \includegraphics[scale=1]{
	    figs_cr/FigS6.pdf}
    \end{center}
	\caption*{Fig. S2:
 \textbf{Kepler orbit model: examples of model outputs from the training dataset.}
 Output data for the Kepler model entailed polar coordinate representation of planetary orbits. For each sample, we first computed $r(\theta)$ at 100 $\theta$ increments, given the input parameters, and then we log-transformed the computed $r(\theta)$ values. Finally, the full set of outputs was scaled to [0, 1]. The above-shown curves are 20 randomly chosen examples of output data from the training dataset.
 }
	\label{fig:kep_out}
\end{figure}

%%
\clearpage
\begin{figure}[htb]
    \begin{center}
	    \includegraphics[scale=1]{
	    figs_cr/FigS7.pdf}
    \end{center}
	\caption*{Fig. S3:
 \textbf{Brain network model: distributions of input parameters in the training dataset.}
 The Larter-Breakspear brain network model had eleven input parameters. These were sampled for the training dataset using a Sobol sequence algorithm in eleven dimensions given predefined ranges. The sampled parameter sets were then filtered based on criteria that tested for oscillatory behavior and constrained the average functional connectivity. Finally, the input parameters were scaled to [0, 1] before training. The above histograms show the resultant distributions for the eleven input parameters from 4,730 samples that passed the described criteria.
 }
	\label{fig:lb_inp}
\end{figure}

%%
\clearpage
\begin{figure}[htb]
    \begin{center}
	    \includegraphics[scale=1]{
	    figs_cr/FigS8.pdf}
    \end{center}
	\caption*{Fig. S4:
 \textbf{Brain network model: examples of model outputs from the training dataset.}
 Six examples of functional connectivity matrices from the training dataset are shown above for the Larter-Breakspear brain network model. Functional connectivity was quantified among 78 brain regions using Pearson correlations. Since the resultant values were between -1 and 1, no additional scaling was implemented for the neural network training.}
	\label{fig:lb_out}
\end{figure}

%%
\clearpage
\begin{figure}[htb]
    \begin{center}
	    \includegraphics[scale=1]{
	    figs_cr/FigS9.pdf}
    \end{center}
	\caption*{Fig. S5:
 \textbf{Kepler orbit model: neural network architecture.}
 The above diagram describes the architecture of the neural network we applied to the Kepler orbit model. The neural network involved fully connected layers, among which one of the intermediate layers was a bottleneck layer. We retrained this architecture while varying the dimensionality of the bottleneck layer to identify the underlying complexity of the model.
 }
	\label{fig:kep_nn}
\end{figure}

%%
\clearpage
\begin{figure}[htb]
    \begin{center}
	    \includegraphics[scale=1]{
	    figs_cr/FigS10.pdf}
    \end{center}
	\caption*{Fig. S6:
 \textbf{Brain network model: neural network architecture.}
 The above diagram describes the architecture of the neural network we applied to the brain network model. The neural network involved fully connected layers, among which one of the intermediate layers was a bottleneck layer. We retrained this architecture while varying the dimensionality of the bottleneck layer to identify the underlying complexity of the model.
 }
	\label{fig:lb_nn}
\end{figure}

\clearpage
\begin{figure}[htb]
    \begin{center}
	    \includegraphics[scale=1]{
	    figs_cr/FigS1.pdf}
    \end{center}
	\caption*{Fig. S7:
 \textbf{Kepler orbit model: evolution of error during neural network training.}
 The above curves show how error on a validation dataset progressed throughout the training of the applied neural network. The error was quantified as mean-squared error and is displayed for a single replicate from each tested bottleneck dimension. Early stopping was implemented to avoid overfitted models: the training was stopped if there had not been an improvement for the 200 most recent epochs. The final model was selected from the epoch with minimal error.
 }
	\label{fig:kep_train}
\end{figure}

%%
\clearpage
\begin{figure}[htb]
    \begin{center}
	    \includegraphics[scale=1]{
	    figs_cr/FigS2.pdf}
    \end{center}
	\caption*{Fig. S8:
 \textbf{Kepler orbit model: distribution of latent parameters following training.}
 After we had trained the neural networks and selected the minimal dimensionality for the bottleneck, we computed the values of the latent representation for the training dataset. We acquired the values of the two latent parameters from the bottleneck layer, and we display their distributions above. These distributions were considered when choosing bounds for latent parameters for the global fitting.
 }
	\label{fig:kep_latent}
\end{figure}

%%
\clearpage
\begin{figure}[htb]
    \begin{center}
	    \includegraphics[scale=1]{
	    figs_cr/FigS3.pdf}
    \end{center}
	\caption*{Fig. S9:
 \textbf{Brain network model:  evolution of error during neural network training.}
 The above curves show how error on a validation dataset progressed throughout the training of the applied neural network. The error was quantified using mean-squared error and is displayed for a single replicate from each tested bottleneck dimension. Early stopping was implemented to avoid overfitted models: the training was stopped if there had not been an improvement for the 200 most recent epochs. The final model was selected from the epoch with minimal error.
 }
	\label{fig:lb_train}
\end{figure}

%%
\clearpage
\begin{figure}[htb]
    \begin{center}
	    \includegraphics[scale=1]{
	    figs_cr/FigS4.pdf}
    \end{center}
	\caption*{Fig. S10: \textbf{Brain network model:  distribution of latent parameters following training.}
  After we had trained the neural networks and selected the minimal dimensionality for the bottleneck, we computed the values of the latent representation for the training dataset. We acquired the values of the four latent parameters from the bottleneck layer, and we display their distributions above. These distributions were considered when choosing bounds for latent parameters for the global fitting.
 }
	\label{fig:lb_latent}
\end{figure}

%%


\end{document}