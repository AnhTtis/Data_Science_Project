% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% \usepackage{caption}
% \usepackage{subcaption}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{graphicx}
% \usepackage[toc,page]{appendix}
\usepackage{amsmath,amssymb}
\usepackage{verbatim}
%\usepackage{cite}
\usepackage{multirow}
\usepackage{marvosym}
\usepackage{makecell}
\usepackage{bm}
% \usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{caption}
% \usepackage{floatrow}
% \usepackage{algorithm}
\usepackage{accents}
% \usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{setspace}

 \newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{s}}}{~}}
\newcommand{\commandname}[1]{\underaccent{\sim}{#1}}
\newcommand{\highlightChange}{\color{red}}
\def\HC{\highlightChange}
\newcommand{\Note}[1]{{\color{blue} \bf \small [NOTE: #1]}}
\newcommand{\NoteSC}[1]{{\color{red} \bf \small [NOTE: #1]}}
\newcommand{\yiqi}[1]{\textcolor{blue}{#1}}
\newcommand{\Sout}[1]{{\HC\sout{#1}}}
\def\delete{\mbox{\color{blue} \bf \small DELETE:} \qquad}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
%\DeclareMathOperator{\Id}{\mathbf{I}}
\DeclareMathOperator{\Id}{I}

\newcommand{\argmin}{\mathbf{arg min}}
\DeclareMathOperator{\ZeroMatrix}{\mathbf{0}}

\newcommand{\mypar}[1]{{\bf #1.}}


\def\a{\mathbf{a}}
\def\bb{\mathbf{b}}
\def\c{\mathbf{c}}
\def\x{\mathbf{x}}
\def\y{\mathbf{y}}
\def\h{\mathbf{h}}
\def\d{\mathbf{d}}
\def\p{\mathbf{p}}
\def\f{\mathbf{f}}
\def\ee{\mathbf{e}}
\def\s{\mathbf{s}}
\def\t{\mathbf{t}}
\def\vv{\mathbf{v}}
\def\u{\mathbf{u}}
\def\w{\mathbf{w}}
\def\z{\mathbf{z}}
\def\S{\mathcal{S} }
\def\shat{\hat{s}}
\def\N{\mathcal{N}}
\def\V{\mathcal{V}}
\def\E{\mathcal{E}}
\def\M{\mathcal{M}}
\def\U{\mathcal{U}}

\def\Pj{\mathbf{P}}
\def\X{\mathbf{X}}


\def\Aa{\mathbb{A}}
\def\Bb{\mathbb{B}}
\def\Cc{\mathbb{C}}
\def\Dd{\mathbb{D}}
\def\Ee{\mathbb{E}}
\def\Ff{\mathbb{F}}
\def\Gg{\mathbb{G}}
\def\Hh{\mathbb{H}}



\def\xww{\widetilde{\x}}
\def\sw{\widehat{\s}}
\def\sww{\widetilde{\s}}  
\def\tw{\widehat{\t}}
\def\yw{\widehat{\y}}

\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\LL}{L}
\DeclareMathOperator{\LP}{LP}
\DeclareMathOperator{\TV}{TV}
\DeclareMathOperator{\DFT}{F}
\DeclareMathOperator{\BL}{BL}
\DeclareMathOperator{\PL}{PL}
\DeclareMathOperator{\PC}{PC}
\DeclareMathOperator{\PPL}{PPL}
\DeclareMathOperator{\PBL}{PBL}
\DeclareMathOperator{\STV}{S}
\DeclareMathOperator{\Adj}{A}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\CC}{C}
\DeclareMathOperator{\D}{D}
\DeclareMathOperator{\F}{F}
\DeclareMathOperator{\HH}{H}
\DeclareMathOperator{\Eig}{\Lambda}
\DeclareMathOperator{\Q}{Q}
\DeclareMathOperator{\G}{G}
\DeclareMathOperator{\J}{J}
\DeclareMathOperator{\RR}{R}
\DeclareMathOperator{\Sr}{S}
\DeclareMathOperator{\Vm}{V}
\DeclareMathOperator{\Mm}{M}
\DeclareMathOperator{\EE}{E}
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\Y}{Y}
\def\Xw{\widehat{\X}}
\DeclareMathOperator{\Um}{U}
\DeclareMathOperator{\Z}{Z}
\DeclareMathOperator{\W}{W}
\DeclareMathOperator{\Ss}{S}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}} %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{4073} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Truncated Diffusion Model for Real-Time Human Trajectory Prediction}
\title{Supplementary Material for Leapfrog \\Diffusion Model for Stochastic Trajectory Prediction}
% \title{Good Start Values: Truncated Diffusion Model for Real-Time Human Trajectory Prediction}

\author{
Chenxin Xu\textsuperscript{1\footnotemark[1]}, Weibo Mao\textsuperscript{1\footnotemark[1]}, Wenjun Zhang\textsuperscript{1}, Siheng Chen\textsuperscript{1,2\footnotemark[2]},
\\\textsuperscript{1}Shanghai Jiao Tong University,  \textsuperscript{2}Shanghai AI Laboratory
\\
\authorcr{\tt\small \{xcxwakaka,kirino.mao,zhangwenjun,sihengc\}@sjtu.edu.cn}
}

\maketitle


\section{Detailed Derivations}

\subsection{Derivation of Standard Diffusion Models}

In the paper submission, we present a standard diffusion model for trajectory prediction following the diffusion-denoising process. Here we elaborate on the details of the diffusion-denoising process.

In standard diffusion models, the diffusion process is operated on the future trajectory $\mathbf{Y}$, while the past trajectories $\mathbf{X}$ and $\mathbb{X}$ serve as a condition for the denoising process. Mathematically, let $\mathbf{Y}^{\gamma}$ be the diffused future trajectory at step $\gamma$, being a basic state in the bidirectional Markov chain of the diffusion-denoising process. We have the start state $\mathbf{Y}^{0} = \mathbf{Y}$ and the end state $\mathbf{Y}^{\Gamma}\sim \mathcal{N}(\mathbf{Y}^{\Gamma}; \mathrm{0}, \mathrm{I})$. We restate the overall procedure of diffusion models for trajectory prediction here, following
\begin{subequations}
\setlength{\abovedisplayskip}{2pt}
   \setlength{\belowdisplayskip}{2pt}
\label{eq:diffusion_model}
    \begin{align}
        \label{eq:initialization_step}
        &\mathbf{Y}^{0}  =  \mathbf{Y},
        \\ 
        \label{eq:diffusion_step}
        &\mathbf{Y}^{\gamma}  =  f_{\operatorname{diffuse}}(\mathbf{Y}^{\gamma-1}),  \ \gamma=1, \cdots, \Gamma,
        \\
        \label{eq:sampling_step}
        &\widehat{\mathbf{Y}}^\Gamma_k  \stackrel{i.i.d}{\sim}   \mathcal{P}(\widehat{\mathbf{Y}}^\Gamma)=\mathcal{N}(\widehat{\mathbf{Y}}^\Gamma; \textbf{0}, \mathbf{I}), {\rm sample}~K~\operatorname{times},
        \\
        \label{eq:denoising_step}
        &\widehat{\mathbf{Y}}^{\gamma}_k  =  f_{\operatorname{denoise}}(\widehat{\mathbf{Y}}^{\gamma+1}_k, \mathbf{X}, \mathbb{X}_{\mathcal{N}}),  \;\gamma\!=\!\Gamma\!-\!1,\!\cdots\!,\!0,
    \end{align}
\end{subequations}
where we use the $f_{\operatorname{diffuse}}(\cdot)$ to represent the diffusion process and $f_{\operatorname{denoise}}(\cdot)$ to represent the conditional denoising process. Here we present the details of these two processes.


\textbf{Forward diffusion process}. Let $(\mathbf{Y}^0, \mathbf{Y}^1, \dots, \mathbf{Y}^\Gamma)$ be the forward $\Gamma$-steps Markov chain constructed by the diffusion model where $\mathbf{Y}^\gamma$ is the diffused future trajectory at step $\gamma$. The forward diffusion process between two steps is defined as
\begin{equation}
\nonumber
    \begin{aligned}
        q(\mathbf{Y}^\gamma|\mathbf{Y}^{\gamma-1}) &= \mathcal{N}(\mathbf{Y}^{\gamma}; \sqrt{1-\beta_\gamma}\mathbf{Y}^{\gamma-1}, \beta_\gamma \mathbf{I}), \\
        \Rightarrow \mathbf{Y}^{\gamma} &= \sqrt{1-\beta_\gamma}\mathbf{Y}^{\gamma-1} + \sqrt{\beta_\gamma} \mathbf{z},
    \end{aligned}
\end{equation}
where $\mathbf{z}\sim \mathcal{N}(\mathbf{z}; 0, \mathbf{I})$ and $\beta_1, \beta_2, \dots, \beta_\Gamma$ are the diffusion parameters controlling the distortion between two steps. In the forward diffusion process, we can directly sample $\gamma$th step diffused trajectory $\mathbf{Y}^{\gamma}$ directly using 
\begin{equation}
\nonumber
    \begin{aligned}
        \mathbf{Y}^{\gamma} &= \sqrt{1-\beta_\gamma}\mathbf{Y}^{\gamma-1} + \sqrt{\beta_\gamma} \mathbf{z}\\
        &\overset{\alpha_\gamma := 1 - \beta_\gamma}{=\!=\!=\!=\!=\!=} \sqrt{\alpha_\gamma} ~\mathbf{Y}^{\gamma-1} + \sqrt{1-\alpha_\gamma} \mathbf{z}\\
        &= \sqrt{\alpha_\gamma} ~(\sqrt{\alpha_{\gamma-1}} ~\mathbf{Y}^{\gamma-2} + \sqrt{1-\alpha_{\gamma-1}} \mathbf{z'}) + \sqrt{1-\alpha_\gamma} \mathbf{z}\\
        &\overset{\operatorname{reparam.}}{=\!=\!=\!=\!=\!=} \sqrt{\alpha_\gamma}\sqrt{\alpha_{\gamma-1}} ~\mathbf{Y}^{\gamma-2} + \sqrt{1-\alpha_\gamma\alpha_{\gamma-1}} \mathbf{z}''\\
        &= \cdots \\
        &= \sqrt{\bar{\alpha}_\gamma}\mathbf{Y}^{0} + \sqrt{1-\bar{\alpha}_\gamma}\mathbf{z},
    \end{aligned}
\end{equation}
where we set the diffusion parameter $\alpha_\gamma := 1 - \beta_\gamma$ and use the reparameterization to merge two Gaussian distributions.
Note that the forward process is non-trainable and with sufficient steps, the final state $\mathbf{Y}^\Gamma \sim q(\mathbf{Y}^\Gamma)$ will be approximate to sample in a normal distribution, i.e., $\mathbf{Y}^\Gamma\sim \mathcal{N}(\mathbf{Y}^\Gamma; \textbf{0}, \textbf{I})$.


\textbf{Conditional denoising process}. Conversely, denote $(\widehat{\mathbf{Y}}^\Gamma, \widehat{\mathbf{Y}}^{\Gamma-1}, \dots, \widehat{\mathbf{Y}}^0)$ as the reverse denoising process conditioned on context information extracting from past trajectories, i.e. $\mathbf{C}=f_{\operatorname{condition}}(\mathbf{X}, \mathbb{X})$. We formulate the conditional denoising process as follows:
\begin{equation}
\label{eq:denoising}
    \begin{aligned}
        & p_\theta(\mathbf{Y}^{\gamma-1} | \mathbf{Y}^{\gamma}, \mathbf{C}) = \mathcal{N}(\mathbf{Y}^{\gamma-1}; \bm{\mu}_\theta^\gamma (\mathbf{Y}^{\gamma}, \mathbf{C}), \beta_\gamma \mathbf{I}), \\
        & \Rightarrow \widehat{\mathbf{Y}}^{\gamma-1} = \bm{\mu}_\theta^\gamma (\widehat{\mathbf{Y}}^{\gamma}, \mathbf{C}) + \sqrt{\beta_\gamma} \mathbf{z},\\
        &\bm{\mu}_\theta^\gamma(\widehat{\mathbf{Y}}^\gamma, \mathbf{C}) = \dfrac{1}{\sqrt{\alpha_\gamma}}(\widehat{\mathbf{Y}}^\gamma - \dfrac{\beta_\gamma}{\sqrt{1-\bar{\alpha}_\gamma}} \bm{\epsilon}_\theta^\gamma(\widehat{\mathbf{Y}}^\gamma, \mathbf{C}))
    \end{aligned}
\end{equation}
where $\mathbf{z}\sim \mathcal{N}(\mathbf{z}; \textbf{0}, \mathbf{I})$, $\alpha_\gamma = 1 - \beta_\gamma$ and $\bar{\alpha}_\gamma = \prod_{\tau=1}^{\gamma} \alpha_\gamma$ are the diffusion parameters at step $\gamma$, and $\bm{\mu}_\theta(\cdot)\in \mathbb{R}^{T_{\rm f}\times 2}$ is the core denoising module with the learnable parameters $\theta$. Note that we have specified the mean term and simplified the variance term in Eq.(\ref{eq:denoising}) following DDPM~\cite{jonathan2020ddpm} so that we can derive the noise estimation loss.


\subsection{Derivation of Noise Estimation Loss}

Here we elaborate on the derivation of our noise estimation loss, the overall target of diffusion models is to maximize the $p_\theta(\mathbf{Y}^{0}|\mathbf{C})$.


\begin{equation}
\nonumber
    \begin{aligned}
     & - \log p_\theta (\mathbf{Y}^{0}|\mathbf{C}) \\
     & \leq -\log p_\theta(\mathbf{Y}^{0}|\mathbf{C}) +\operatorname{D_{KL}} (q(\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0}) || p_\theta (\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0}, \mathbf{C})) \\ 
     & = -\log p_\theta(\mathbf{Y}^{0}|\mathbf{C}) \\
     & \quad + \int_{\mathbf{Y}^{1:\Gamma}}[\log \dfrac{q(\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0})}{p_\theta (\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0}, \mathbf{C})}]q(\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0})~d\mathbf{Y}^{1:\Gamma} \\
     & = -\log p_\theta(\mathbf{Y}^{0}|\mathbf{C}) + \mathbb{E}_{\mathbf{Y}^{1:\Gamma}\sim q(\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0})}[\log \dfrac{q(\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0})}{p_\theta (\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0}, \mathbf{C})}] \\
     & = -\log p_\theta(\mathbf{Y}^{0}|\mathbf{C}) + \mathbb{E}_{q}[\log \dfrac{q(\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0})}{p_\theta (\mathbf{Y}^{0:\Gamma}|\mathbf{C})} + \log p_\theta (\mathbf{Y}^{0}|\mathbf{C})] \\
     & = \mathbb{E}_{q(\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0})}[\log \dfrac{q(\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0})}{p_\theta (\mathbf{Y}^{0:\Gamma}|\mathbf{C})}] \\
     \end{aligned}
\end{equation}
where we derive the variational lower bound (VLB) to minimize the negative log-likelihood.

\begin{equation}
\nonumber
    \begin{aligned}
     & \Rightarrow  \mathbb{E}_{q(\mathbf{Y}^{0})}- \log p_\theta (\mathbf{Y}^{0}|\mathbf{C}) \leq \mathbb{E}_{q(\mathbf{Y}^{0:\Gamma})}[\log \dfrac{q(\mathbf{Y}^{1:\Gamma}|\mathbf{Y}^{0})}{p_\theta (\mathbf{Y}^{0:\Gamma}|\mathbf{C})}] \\
     &=\mathbb{E}_{q}[\sum_{\gamma=1}^\Gamma - \log \dfrac{p_\theta (\mathbf{Y}^{\gamma-1} | \mathbf{Y}^{\gamma}, \mathbf{C})}{q(\mathbf{Y}^{\gamma}|\mathbf{Y}^{\gamma-1})} - \log p_\theta(\mathbf{Y}^{\Gamma})] \\
     &= - \mathbb{E}_{q}[\sum_{\gamma=2}^\Gamma  \log \dfrac{p_\theta (\mathbf{Y}^{\gamma-1} | \mathbf{Y}^{\gamma}, \mathbf{C})}{q(\mathbf{Y}^{\gamma}|\mathbf{Y}^{\gamma-1})} \\
     & \quad + \log \dfrac{p_\theta (\mathbf{Y}^{0} | \mathbf{Y}^{1}, \mathbf{C})}{q(\mathbf{Y}^{1}|\mathbf{Y}^{0})} + \log p_\theta(\mathbf{Y}^{\Gamma})]\\
     & = - \mathbb{E}_{q}[\sum_{\gamma=2}^\Gamma  \log\Big( \dfrac{p_\theta (\mathbf{Y}^{\gamma-1} | \mathbf{Y}^{\gamma}, \mathbf{C})}{q(\mathbf{Y}^{\gamma-1}|\mathbf{Y}^{\gamma}, \mathbf{Y}^{0})} 
     \cdot \dfrac{q(\mathbf{Y}^{\gamma-1}| \mathbf{Y}^{0})}{q(\mathbf{Y}^{\gamma}| \mathbf{Y}^{0})}\Big) \\
     & \quad +  \log \dfrac{p_\theta (\mathbf{Y}^{0} | \mathbf{Y}^{1}, \mathbf{C})}{q(\mathbf{Y}^{1}|\mathbf{Y}^{0})} + \log p_\theta(\mathbf{Y}^{\Gamma})]\\
     & = - \mathbb{E}_{q}[\sum_{\gamma=2}^\Gamma  \log \dfrac{p_\theta (\mathbf{Y}^{\gamma-1} | \mathbf{Y}^{\gamma}, \mathbf{C})}{q(\mathbf{Y}^{\gamma-1}|\mathbf{Y}^{\gamma}, \mathbf{Y}^{0})} 
     + \sum_{\gamma=2}^\Gamma \log \dfrac{q(\mathbf{Y}^{\gamma-1}| \mathbf{Y}^{0})}{q(\mathbf{Y}^{\gamma}| \mathbf{Y}^{0})} \\
     & \quad + \log  \dfrac{p_\theta (\mathbf{Y}^{0} | \mathbf{Y}^{1}, \mathbf{C})}{q(\mathbf{Y}^{1}|\mathbf{Y}^{0})} + \log p_\theta(\mathbf{Y}^{\Gamma})]\\
     & = -\mathbb{E}_{q}[\dfrac{p_\theta (\mathbf{Y}^{\Gamma})}{q(\mathbf{Y}^{\Gamma}|\mathbf{Y}^{0})} + \sum_{\gamma=2}^\Gamma  \log \dfrac{p_\theta (\mathbf{Y}^{\gamma-1} | \mathbf{Y}^{\gamma}, \mathbf{C})}{q(\mathbf{Y}^{\gamma-1}|\mathbf{Y}^{\gamma}, \mathbf{Y}^{0})}  \\
     & \quad + \log p_\theta (\mathbf{Y}^{0} | \mathbf{Y}^{1}, \mathbf{C})]
     \end{aligned}
\end{equation}
where the first term can be ignored since there are no trainable parameters in $p_\theta (\mathbf{Y}^{\Gamma})$. Then, we only need to focus on the second term $\mathbb{E}_{q}[\log \dfrac{q(\mathbf{Y}^{\gamma-1}|\mathbf{Y}^{\gamma}, \mathbf{Y}^{0})}{p_\theta (\mathbf{Y}^{\gamma-1} | \mathbf{Y}^{\gamma}, \mathbf{C})}]$ where $p_\theta(\cdot)$ is given in Equation~\eqref{eq:denoising}. We can derive the close form for $q(\mathbf{Y}^{\gamma-1}|\mathbf{Y}^{\gamma}, \mathbf{Y}^{0})$ with the Bayes' rule,
\begin{equation}
\nonumber
    \begin{aligned}
        q(\mathbf{Y}^{\gamma-1}|\mathbf{Y}^{\gamma}, \mathbf{Y}^{0}) & = \dfrac{q(\mathbf{Y}^{\gamma-1}, \mathbf{Y}^{\gamma} | \mathbf{Y}^{0})}{q(\mathbf{Y}^{\gamma}| \mathbf{Y}^{0})} \\
        & = \dfrac{q(\mathbf{Y}^{\gamma-1}| \mathbf{Y}^{0})q(\mathbf{Y}^{\gamma} | \mathbf{Y}^{\gamma-1} , \mathbf{Y}^{0})}{q(\mathbf{Y}^{\gamma}| \mathbf{Y}^{0})}\\
        & = \dfrac{q(\mathbf{Y}^{\gamma-1}| \mathbf{Y}^{0})q(\mathbf{Y}^{\gamma} | \mathbf{Y}^{\gamma-1})}{q(\mathbf{Y}^{\gamma}| \mathbf{Y}^{0})}
    \end{aligned}
\end{equation}
where $q(\mathbf{Y}^{\gamma-1}| \mathbf{Y}^{0}), q(\mathbf{Y}^{\gamma} | \mathbf{Y}^{\gamma-1})$, and $q(\mathbf{Y}^{\gamma}| \mathbf{Y}^{0})$ are all Gaussian distributions, which indicates the target distribution $q(\mathbf{Y}^{\gamma-1}|\mathbf{Y}^{\gamma}, \mathbf{Y}^{0})$ also has the Gaussian form. Follow~\cite{jonathan2020ddpm}, suppose $q(\mathbf{Y}^{\gamma-1}|\mathbf{Y}^{\gamma}, \mathbf{Y}^{0}) = \mathcal{N}(\mathbf{Y}^{\gamma-1}; \bm{\mu}^\gamma, \beta_\gamma \mathbf{I})$, where 
\begin{equation}
\nonumber
    \begin{aligned}
        \bm{\mu}^\gamma & = \dfrac{\sqrt{\alpha_\gamma}(1-\bar{\alpha}_{\gamma-1})}{1-\bar{\alpha}_\gamma}\mathbf{Y}^{\gamma} + \dfrac{\sqrt{\bar{\alpha}_{\gamma-1}}\beta_\gamma}{1-\bar{\alpha}_\gamma}\mathbf{Y}^{0} \\
        & = \dfrac{1}{\sqrt{\alpha}_\gamma}(\mathbf{Y}^{\gamma} - \dfrac{\beta_\gamma}{\sqrt{1-\bar{\alpha}_\gamma}}\bm{\epsilon})
    \end{aligned}
\end{equation}
where $\bm{\epsilon}\sim \mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{I})$. Then, we only need to minimize the means between two distributions and get the noise estimation loss:
\begin{eqnarray}
\nonumber
\label{eq:loss_diffusion}
    \mathcal{L_{\operatorname{NE}}} = \Vert \bm{\epsilon} - \bm{\epsilon}_\theta^\gamma(\widehat{\mathbf{Y}}^\gamma, \mathbf{C})\Vert_2,
\end{eqnarray}
% \subsection{Derivation of Uncertainty Loss}

% In the paper submission, we use the uncertainty loss $\mathcal{L}_{\operatorname{uncertainty}}$ to bridge the scenery complexity and prediction diversity. Here we present the detailed derivation of this loss. In the proposed leapfrog initializer, we use the reparameterization $\widehat{\mathbf{Y}}^\tau_k = \mu_\theta + \sigma_\theta \cdot \widehat{\mathbf{S}}_{\theta, k}$ with three trainable modules to erasing the learning burden, where the mean estimation $\mu_\theta$ predicts the mean trajectory and the variance estimation $\sigma_\theta$ predicts the global standard deviation. In this case, suppose the normalized sample prediction $\mathbf{S}_\theta \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$. Then, we have $\widehat{\mathbf{Y}}^\tau \sim \mathcal{N}(\mu_\theta , \sigma_\theta \mathbf{I})$, which simplifies the denoised distribution to Gaussian distribution for the derivation only.

% \begin{equation}
% \label{eq:uncertainty}
%     \begin{aligned}
        
%     \end{aligned}
% \end{equation}

\section{Experiment Details}

We apply a standard diffusion model with the diffusion step $\Gamma=100$, the start value $\beta_1=$1e-4, and the end value $\beta_{100}=$5e-2. We use the linear schedule to interpolate the intermediate values $\beta_2, \beta_3, \cdots, \beta_{99}$.

On SDD, following previous destination prediction strategies~\cite{mangalam2020pecnet, chenxin2022memonet}, we first predict the destination of a pedestrian using the proposed leapfrog diffusion model. And then, we fulfill the trajectory using the multi-layer perceptron.

\section{Supplementary Experiments}

\begin{table}[!t]
\footnotesize
\centering
\setlength{\tabcolsep}{1mm}{
\caption{Influence of different parameters in the standard diffusion models on SDD. We run 5 times for each setting with $K$=20 and report the average and best performance.}
% \vspace{-10pt}
\fontsize{8}{11.5}\selectfont
% \resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|l|ll|ll}
\toprule[1pt]
\multicolumn{4}{c|}{Diffusion Parameters}                                                            & \multicolumn{2}{c|}{AVG}                                 & \multicolumn{2}{c}{Best}                                \\ \hline
$\beta_1$           & $\beta_\Gamma$            & $\Gamma$             & \multicolumn{1}{c|}{schedule} & \multicolumn{1}{c}{minADE} & \multicolumn{1}{c|}{minFDE} & \multicolumn{1}{c}{minADE} & \multicolumn{1}{c}{minFDE} \\ \midrule[0.7pt]
\multirow{8}{*}{1e-4} & \multirow{8}{*}{5e-2} & 20                   & linear                        & 19.27                      & 32.77                      & 10.42                      & 19.19                      \\ \cline{3-8} 
                      &                       & 50                   & linear                        & 11.04                      & 17.75                      & 9.94                      & 15.95                      \\ \cline{3-8} 
                      &                       & \multirow{3}{*}{100} & linear                        & \textbf{10.36 }                     & 16.92                      & \textbf{9.73}                       & \textbf{15.32}                      \\
                      &                       &                      & sigmoid                       & 10.65                      & \textbf{16.87}                      & 9.76                      & 15.52                      \\
                      &                       &                      & quadratic                     & 10.55                      & 17.87                      & 9.84                       & 15.77                      \\ \cline{3-8} 
                      &                       & 200                  & linear                        & 10.70                      & 18.03                      & 10.24                      & 16.98                      \\ \cline{3-8} 
                      &                       & 500                  & linear                        & 10.94                      & 18.68                      & 10.45                      & 17.68                      \\ \cline{3-8} 
                      &                       & 1000                 & linear                        & 11.27                      & 19.01                      & 10.91                      & 18.26                      \\ 
\hline
1e-5                  & 5e-2                  & 100                  & linear                        & 10.43                      & 17.45                      & 9.92                       & 16.03                      \\
\hline
1e-4                  & 1e-2                  & 100                  & linear                        & 25.80                      & 48.29                      & 12.70                      & 21.37                      \\
\hline
1e-5                  & 1e-2                  & 100                  & linear                        & 26.91                      & 44.85                      &  12.52                     & 21.06                      \\
\bottomrule[1pt]
\end{tabular}
\label{table:ablation_parameters}}
% \vspace{-5mm}
\end{table}

\subsection{Influence of Diffusion Parameters}

We explore the influence of different parameters in the diffusion model, including the denoising steps $\Gamma$, the start value of $\beta_1$, the end value of $\beta_\Gamma$, and the schedule to generate $\beta$'s; see Table~\ref{table:ablation_parameters}. We see that i) $\beta_1=1e-4, \beta_\Gamma=5e-2, \Gamma=100$ provides the best performance for the standard diffusion model; ii) with the fixed $\beta_1$ and $\beta_\Gamma$, the schedule to generate the intermediate parameters will not influence the performance lot, also the linear schedule provides the best performance; and iii) when the diffusion step $\Gamma$ is too small, the denoising step is not equivalent to estimating the Gaussian noise, deteriorating the performance.




\begin{table}[!t]
\footnotesize
\centering
\setlength{\tabcolsep}{1mm}{
\caption{Influence of different social-temporal structures in the leapfrog initializer on SDD. We run 5 times for each setting with $K$=20 and report the average and best performance.}
\fontsize{8}{11.5}\selectfont
\begin{tabular}{l|cc|cc}
\toprule[1pt]
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Encoder\\ Structure\end{tabular}} & \multicolumn{2}{c|}{AVG} & \multicolumn{2}{c}{Best} \\ \cline{2-5} 
                                                                             & minADE      & minFDE     & minADE      & minFDE     \\ \midrule[0.7pt]
without social                                                               & 8.69        & 12.07      & 8.64        & 11.93      \\
sequential                                                                   & 8.65        & 11.94      & 8.60        & 11.81      \\
parallel                                                                     & \textbf{8.47}        & \textbf{11.54}      & \textbf{8.46}        & \textbf{11.47}      \\ \bottomrule[1pt]
\end{tabular}
     
\label{table:ablation_encoders}}
% \vspace{-5mm}
\end{table}

\subsection{Influence of Different Encoders}

In the leapfrog initializer, we use a social encoder to capture social influence, a temporal encoder to learn temporal embedding, and an aggregation layer to fuse both social and temporal information. Here we explore the influence of different encoders including without considering the social information (without social), sequential structure to fuse the social-temporal information (sequential), and the parallel structure used in the paper submission (parallel); see Table~\ref{table:ablation_encoders}. We see that i) the parallel structure provides the best performance since the social-temporal information is decoupled without influencing each other; and ii) the social force will influence the agent's movement since considering the social embedding outperforms the without social embedding structure.

% \clearpage
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{supp}
}

\end{document}
