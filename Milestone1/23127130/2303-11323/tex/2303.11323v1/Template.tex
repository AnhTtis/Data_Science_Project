% Template for ICIP-2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass[journal]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
%\usepackage{graphicx}
\usepackage{epstopdf}
% \usepackage{titling}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath,graphicx}
\usepackage{amsbsy}
\usepackage{epsfig}
\usepackage{float}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url}
\usepackage{cite}
\usepackage{amssymb}
\usepackage{pgf, tikz, pgfplots, epstopdf}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{float}
\usepackage{cuted}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{calc}  
\usepackage{mathtools}
\usepackage{enumitem}  
\usepackage{multirow}
%\usepackage{algorithmic}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
% #1 = text to left, #2 = text to right
%\usepackage{algorithm2e}
\usepackage{subcaption}
\input{mysymbol.sty}
% Example definitions.
% --------------------

\makeatletter
\newcommand*\dotp{\mathpalette\dotp@{.5}}
\newcommand*\dotp@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\newcommand\svdeq{\stackrel{\mathclap{\scriptsize\mbox{ SVD }}}{=}}

\makeatother
\def\x{{\mathbf x}}
\def\f{{\mathbf f}}
\def\g{{\mathbf g}}
\def\z{{\mathbf z}}
\def\F{{\mathbf F}}
\def\U{{\mathbf U}}
\def\G{{\mathbf G}}
\def\D{{\mathbf D}}
\def\S{{\mathbf S}}
\def\B{{\mathbf B}}
\def\R{{\mathbf R}}
\def\h{{\mathbf h}}
\def\L{{\cal L}}
\def\tmx{{\mathcal{T}_x\mathcal{M}}}
\def\tmxrp{{\mathcal{T}_x\mathbb{R}^p}}
\def\tmxrd{{\mathcal{T}_x\mathbb{R}^d}}
\def\tmy{{\mathcal{T}_y\mathcal{M}}}
\def\tm{{\mathcal{T}\mathcal{M}}}
\def\M{{\mathcal{M}}}
\def\i{\iota}
\def\rp{{\mathbb{R}^p}}
% \def\inc{{\underline{\triangleleft}}}
\makeatletter
\newcommand{\inc}{%
  \mathrel{\mathpalette\inc@\relax}%
}
\newcommand{\inc@}[2]{%
  \sbox\z@{$#1\lhd$}%
  \sbox\tw@{$#1\leqslant$}%
  \dimen@=\ht\tw@
  \advance\dimen@-\ht\z@
  \ifx#1\displaystyle
    \advance\dimen@ .2pt
  \else
    \ifx#1\textstyle
      \advance\dimen@ .2pt
    \fi
  \fi
  \ooalign{\raisebox{\dimen@}{$\m@th#1\lhd$}\cr$\m@th#1\leqslant$\cr}%
}
\makeatother
\makeatletter
\newcommand{\coinc}{%
  \mathrel{\mathpalette\coinc@\relax}%
}
\newcommand{\coinc@}[2]{%
  \sbox\z@{$#1\rhd$}%
  \sbox\tw@{$#1\geqslant$}%
  \dimen@=\ht\tw@
  \advance\dimen@-\ht\z@
  \ifx#1\displaystyle
    \advance\dimen@ .2pt
  \else
    \ifx#1\textstyle
      \advance\dimen@ .2pt
    \fi
  \fi
  \ooalign{\raisebox{\dimen@}{$\m@th#1\rhd$}\cr$\m@th#1\geqslant$\cr}%
}
\makeatother

\def\ltm{{\mathcal{L}^2(\tm)}}
\def\ltmn{{\mathcal{L}^2(\tm_n)}}
\def\lmn{{\mathcal{L}^2(\mathcal{M}_n)}}
\def\lx{{\mathcal{L}^2(\mathcal{X})}}
\def\pxy{{\mathcal{P}_x^y}}
\def \Phii{{\boldsymbol{\phi}_i}}
\def \th{{\widetilde{h}}}
\def \bPsi{{\boldsymbol{\Psi}}}
\def\Oi{{\mathbf{O}_i}}
\def\Oj{{\mathbf{O}_j}}
\def\Oij{{\mathbf{O}_{i,j}}}
\def\hd{{\hat{d}}}
\def \samp{\boldsymbol{\Omega}}
\def \tGamma{\widetilde{\Gamma}}
\def \eigGammai{\widetilde{\boldsymbol{\phi}}^n_{i}}
\def \eivGammai{\widetilde{\lambda}^n_{i}}
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

%THEOREM ENVIRONMENTS

\usepackage{amsthm}
\newtheoremstyle{claudio}% name of the style to be used
  {-0.1em}% measure of space to leave above the theorem. E.g.: 3pt
  {-0.1em}% measure of space to leave below the theorem. E.g.: 3pt
  {}% name of font to use in the body of the theorem
  {}% measure of space to indent
  {\itshape\bfseries}% name of head font
  {.}% punctuation between head and body
  { }% space after theorem head; " " = normal interword space
  {\thmname{#1}\thmnumber{ #2 }\thmnote{(#3)}}% Manually specify head

\theoremstyle{claudio}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

% Title.
% ------
\title{Tangent Bundle Convolutional Learning: \\ from Manifolds to Cellular Sheaves and Back}
%
% Single address.
% ---------------
\author{Claudio Battiloro \quad  Zhiyang Wang \quad Hans Riess \quad  P. Di Lorenzo \quad Alejandro Ribeiro
\thanks{CB is with the DIET Department, Sapienza University of Rome, Rome, Italy, and with with the ESE Department, University of Pennsylvania, Philadelphia, USA, email: claudio.battiloro@uniroma1.it. ZW and AR are with the ESE Department, University of Pennsylvania, Philadelphia, USA, email: zhiyangw@seas.upenn.edu. HR is with  the ECE Department, Duke University, Durham, USA. PDL is with the DIET Department, Sapienza University of Rome, Rome, Italy.  This work was funded by NSF CCF 1934960.  Preliminary results presented in \cite{battiloro2022tnnicassp}.
}
}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
\black{In this work we introduce a convolution operation over the tangent bundle of Riemann manifolds in terms of exponentials of the Connection Laplacian operator.} We define  tangent bundle filters and tangent bundle neural networks (TNNs) based on this convolution operation, which are novel continuous architectures operating on tangent bundle signals, i.e. vector fields over the manifolds. Tangent bundle filters admit a spectral representation that generalizes the ones of scalar manifold filters, graph filters and standard convolutional filters in continuous time. We then introduce a discretization procedure, both in the space and time domains, to make TNNs implementable, showing that their discrete counterpart is a novel principled variant of the very recently introduced sheaf neural networks. We formally prove that this discretized architecture converges to the underlying continuous TNN. Finally, we numerically evaluate the effectiveness of the proposed architecture on various learning tasks, both on synthetic and real data.
\end{abstract}
%
\begin{IEEEkeywords}
Tangent Bundle Signal Processing, Tangent Bundle Neural Networks, Cellular Sheaves, Graph Signal Processing
\end{IEEEkeywords}
%
\vspace{-.5cm}
\section{Introduction}\label{sec:intro}
During the last few years, the development of deep learning techniques have led to  state-of-the-art results in various fields. More and more sophisticated architectures have promoted significant improvements from  both theoretical and practical perspectives. Although it is not the only reason, the success of deep learning is \textcolor{black}{in part} due to Convolutional Neural Networks (CNNs) \cite{lecun1998gradient}. CNNs have achieved excellent performances in a wide range of applications, spanning from image recognition \cite{alexnet2012} to speech analysis \cite{hamid2012cnnspeech} while, at the same time, \textcolor{black}{lightening the computational load of feedforward fully-connected neural networks and integrating features in different spatial resolutions with pooling operators}. CNNs are based on shift operators in the space domain that induce desirable properties in the convolutional filters, among which the most relevant one is the property of shift equivariance. CNNs naturally leverage the regular (often metric) structure of the signals they process, such as spatial or temporal structure. However, data defined on irregular (non-Euclidean) domains are pervasive, with applications ranging from detection and recommendation in social networks \cite{aggarwal2020machine}, to resource allocations over wireless networks \cite{wang2022learning}, and point clouds for shape segmentation \cite{xie2020linking}, just to name a few. The structured data are modeled via the more varied mathematical objects, among which graphs and manifolds are notable examples.  For this reason, the notions of shifts in CNNs have been adapted to convolutional architectures on graphs (GNNs) \cite{gama2018convolutional,scarselli2008graph} as well as a plethora of other structures,~e.g.~simplicial complexes \cite{battiloro2022san,bodnar2021weisfeiler,barbarossa2020topological, edgenetsIsufi}, cell complexes \cite{battiloro2022can,bodnarcwnet}, \textcolor{black}{order lattices \cite{riessmultidimensional}}, and manifolds \cite{wang2021stability}. In \cite{parada2020algebraic}, a framework for algebraic neural networks has been proposed exploiting commutative algebras. \textcolor{black}{However, none of these studies consider convolutional filtering of vector fields over manifolds.} Therefore, in this work we focus on tangent bundles, \textcolor{black}{manifolds constructed from the tangent spaces of a domain manifold. Tangent bundles are a specialization of vector bundles which are a specialization of sheaves, all three of which, in increasing levels of generality, mathematically characterizes both (1) when local data extends globally and (2) topological obstructions thereof. Our present focus is on tangent bundles as they are a tool for describing and processing vector fields, ubiquitous data structures critical} in tasks such as robot navigation and flocking modeling, as well as in climate science \cite{bermejo2009climate} and astrophysics \cite{collier2018magnet}. Moreover, to make the proposed procedures implementable, we formally describe and leverage the link between tangent bundles and orthogonal cellular sheaves (also called discrete vector bundles), a mathematical structure that generalizes connection graphs and matrix weighted graphs.
\vspace{-.1cm}
\subsection{Related Works}  
\textcolor{black}{The well-known manifold hypothesis states that high dimensional data examples are sampled from one (or more) low-dimensional (Riemann) manifolds.} This assumption is the fundamental block of manifold learning, a class of methods for non-linear dimensionality reduction. \textcolor{black}{The Laplacian Eigenmap framework is based on the approximation of manifolds by weighted undirected graphs constructed with $k$-nearest neighbors or proximity radius heuristics}, with the key assumption being that a set of sampled points of the manifold is available \cite{belkin2008towards,chung1997spectral,dunson2021spectral}. \textcolor{black}{Formal connections between GNNs and Manifold Neural Networks (MNNs) are established in \cite{wang2022convolution,levie2019transferability}.}  Most of the previous works focused on scalar signals, e.g. one or more scalar values attached to each node of  graphs or point of  manifolds; however, recent developments \cite{Sharp2019vhm,hansen2019sheafsp,hansen2020opinion, bodnar2022sheafdiff} showed that processing vector data defined on tangent bundles of manifolds or discrete vector bundles comes with a series of benefits. \black{The work in \cite{Sharp2019vhm} introduced a method for computing parallel transport
of vector-valued data on a curved manifold by extending a vector field defined over any region to the rest of the manifold via \textcolor{black}{geodesic curves}. The work in \cite{collier2018magnet} presented an algorithm to reconstruct the magnetopause surfaces from tangent vector observations. \textcolor{black}{In \cite{hansen2019sheafsp}, the authors studied the problem of learning cellular sheaves from (assumed) smooth graph signals.} The work in \cite{hansen2020opinion} introduced a novel class of diffusion dynamics on cellular sheaves as a model for network dynamics. }In \cite{bodnar2022sheafdiff,hansen2020sheafnn,barbero2022sheafnnconn}, neural networks operating on discrete vector bundles are presented, generalizing GNNs: additionally, the work in \cite{bodnar2022sheafdiff} exploited cellular sheaf theory to show that the underlying geometry of the graph \textcolor{black}{gives rise to} oversmoothing behavior of GNNs. Finally, the most important works for us are  \cite{singer2012vdm, singer2013spectral}. In particular, in \cite{singer2012vdm}, \black{the authors introduced an algorithmic generalization of non-linear dimensionality reduction methods based on the Connection Laplacian operator and proved that both manifolds and their tangent bundles can be approximated with certain cellular sheaves constructed from sampled points of the manifolds.} The work in \cite{singer2013spectral} further generalized the result of \cite{singer2012vdm} by presenting a framework for approximating  Connection Laplacians over manifolds via their principle bundle structure, and by proving \black{the spectral convergence of the approximating sheaf Laplacians.}
\vspace{-.5cm}
\subsection{Contributions.} In this work, we firstly define a \textit{convolution operation over the tangent bundle} of Riemann manifolds via the Connection Laplacian operator. Our definition is derived from the vector diffusion equation over manifolds; this choice is crucial to make the convolution operation consistent. \textcolor{black}{Convolution on the tangent bundle reduces to manifold convolution \cite{wang2022convolution} in the scalar bundle case ($\mathbb{R}$-valued signals), and standard convolution if the manifold is the real line.} Leveraging this operation, we introduce \textit{Tangent Bundle Convolutional Filters} to process tangent bundle signals \textcolor{black}{(vector fields)}. We define the \textit{frequency representation} of tangent bundle signals and the \textit{frequency response} of tangent bundle filters using the spectral properties of the Connection Laplacian. By cascading  layers consisting of tangent bundle filter banks and pointwise nonlinearities, we introduce \textit{Tangent Bundle Neural Networks} (TNNs).

However, tangent bundle filters and tangent bundle neural networks are continuous architectures that cannot be directly implemented in practice. \textcolor{black}{For this reason, we provide a principled way of discretizing them, both in time and space domains, making convolutions on them computable}. In particular, we discretize the TNNs in the space domain by sampling points on the manifold and building a cellular sheaf \cite{Hansen2019towardspecsheaf} that represents a \textcolor{black}{legitimate} approximation of both the manifold and its tangent bundle \cite{singer2012vdm}. We \textit{prove that the space discretized architecture over the cellular sheaf converges to the underlying TNN} as the number of sampled points increases. Moreover, we further discretize the architecture in the time domain by sampling the filter impulse function in discrete and finite time steps, notably showing that space-time discretized TNNs (DD-TNNs) are a novel principled variant of the very recently introduced Sheaf Neural Networks \cite{bodnar2022sheafdiff,hansen2020sheafnn,barbero2022sheafnnconn}, and thus shedding further light, from a theoretical point of view, on the deep connection between algebraic topology and differential geometry. Finally, we evaluate the performance of TNNs on both synthetic and real data; in particular, we design a denoising task of a synthetic tangent vector field on the torus, a reconstruction task and a forecasting task of the daily Earth wind field, tackled via a recurrent version of our architecture. \textcolor{black}{We empirically demonstrate the advantage of incorporating the tangent bundle structure into our model by comparing TNNs against Manifold Neural Networks from \cite{wang2022convolution}, architectures taking into account the manifold structure, but not the tangent spaces.} %\footnote{Part of this work was presented in the preliminary conference paper \cite{battiloro2022tnnicassp}}
\vspace{-.1cm}
\subsection{Paper Outline} The paper is organized as follows. We introduce some preliminary concepts in Section \ref{subsec:prelim_def}.  We define tangent bundle convolution, filters and neural networks in Section \ref{sec:filters}. In Section \ref{sec:disc}, we illustrate the proposed discretization procedure for TNNs and we prove the convergence result. Numerical results are in Section \ref{sec:num_res}, and conclusions in Section \ref{sec:conlcusion}. 
\vspace{-.1cm}
\section{Preliminary Definitions}\label{subsec:prelim_def}
In this section, we review some concepts from Riemann geometry  that will be useful to introduce the convolution operation over tangent bundles.
\begin{figure}[t]
    \centering
    \includegraphics[scale = .37]{tangent.pdf}
    \caption{An example of tangent vector}
    \label{fig:tangent_space}
\end{figure}
\vspace{-.1cm}
\subsection{Manifolds and Tangent Bundles} 
In this paper, we consider a compact and smooth $d$-dimensional manifold $\mathcal{M}$ isometrically embedded in $\mathbb{R}^p$. Each point $x \in \mathcal{M}$ is endowed with a $d$-dimensional tangent  space \textcolor{black}{$\tmx$ isomorphic to  $\mathbb{R}^d$}, whose elements $\mathbf{v} \in \tmx$ are said to be tangent vectors at $x$. \textcolor{black}{For explicit construction of tangent spaces on a manifold, consult an introductory textbook on differential topology \cite{lee2006Riemannian}. Informally, tangent spaces can be seen as a generalization of the velocity vector of a curve constrained to $\mathcal{M}$ passing through the point $x$. An example of tangent vector is depicted in Fig. \ref{fig:tangent_space}}.
\textcolor{black}{
\begin{definition}[Tangent Bundle]
    The tangent bundle is the  disjoint union of the tangent spaces $\tm = \bigsqcup_{x \in \M} \tmx$ together with the projection map $\pi: \tm \to \M$ given by $\pi(x,\mathbf{v}) = x$.
\end{definition}}
\textcolor{black}{In abuse of language, we often refer to the tangent bundle as simply the space $\tm$}. The embedding induces a Riemann structure on $\mathcal{M}$ which allows to equip each tangent space $\tmx$ with an inner product.
\textcolor{black}{\begin{definition}[Riemann Metric]
    A Riemann Metric on a compact and smooth $d$-dimensional manifold $\mathcal{M}$ isometrically embedded in $\mathbb{R}^p$ is a (smoothly chosen) inner product $\langle~,~\rangle_{\tmx} : \tmx \times \tmx \rightarrow \mathbb{R}$ on each of the tangent
spaces $\tmx$ of $\mathcal{M}$ given, for each $\mathbf{v}$,$\mathbf{w} \in \tmx$, by
\begin{equation}
\label{riemann_metric}
\langle\mathbf{v},\mathbf{w}\rangle_{\tmx} = \langle d\i \mathbf{v},  d\i\mathbf{w} \rangle_{\mathbb{R}^p},
\end{equation}
where $d\i\mathbf{v} \in \tmxrp$ is \textcolor{black}{called the} differential of $\mathbf{v} \in \tmx$ in $\tmxrp \subset \rp$, $\tmxrp$ is the $d$-dimensional subspace of $\rp$ being the embedding of $\tmx$ in $\rp$,  the differential $d\i:\tm\rightarrow\tmxrp$ is an injective linear mapping  \textcolor{black}{(also referred to as pushforward, as it pushes tangent vectors on $\mathcal{M}$ forward to tangent vectors on $\mathbb{R}^p$)} \cite{lee2006Riemannian}, and $\langle,\rangle_{\mathbb{R}^p}$ is the usual dot product.
\end{definition}}
The  Riemann metric induces also a uniform probability measure $\mu$ over the manifold, simply given by the considered region scaled by the volume of the manifold.
\vspace{-.1cm}
\subsection{Tangent Bundle Signals} 
A tangent bundle signal is a vector field over the manifold, thus a mapping $\F: \M \rightarrow \tm$ that associates to each point of the manifold a vector in the corresponding tangent space. \textcolor{black}{In the theory of vector bundles, a bundle signal is a section.} An example of a (sparse) tangent vector field over the unit $2$-sphere is depicted in Fig. \ref{fig:sphere}\cite{battiloro2022tnnicassp}. We can define an inner product for tangent bundle signals in the following way.
\textcolor{black}{\begin{definition}[Tangent Bundle Inner Product]
    Given tangent bundle signals $\F$ and $\mathbf{G}$, their inner product is given by
\begin{align}
\label{inn_prod}
\langle \F, \mathbf{G} \rangle_{\tm}
&= \int_{\M} \langle \F(x), \mathbf{G}(x) \rangle_{\tmx} \textrm{d}\mu(x),
\end{align}
and the induced norm is $||\F||^2_{\tm} = \langle \F, \F \rangle_{\tm}$.
\end{definition}}
\textcolor{black}{We denote with $\ltm$ the collection (Hilbert Space) of tangent bundle signals with finite energy with respect to $|| \cdot ||_{\tm}$.} In the following, we denote $\langle \cdot, \cdot \rangle_{\tm}$ with $\langle \cdot, \cdot \rangle$  when there is no risk of confusion. %An example of tangent bundle signal over the unit 2-sphere is depicted in Fig. \ref{fig:sphere}.
\vspace{-.1cm}
\section{Tangent Bundle Convolutional Filters}\label{sec:filters}
\textcolor{black}{Linear filtering operations are historically synonymous (under appropriate assumptions) with convolution.} Time signals are filtered by computing the
continuous time convolution of the input signal and the filter
impulse response [17]; images  are filtered by computing multidimensional
convolutions [34]; graph signals are filtered by computing
graph convolutions [5]; scalar manifold signals are filtered by computing manifold convolutions \cite{wang2022convolution}. In this paper, we define a tangent bundle filter as the convolution of the filter impulse response $\th$ and the tangent bundle signal $\F$. To do so, we exploit the Connection Laplacian Operator.
\vspace{-.1cm}
\subsection{Connection Laplacian}
 The Connection Laplacian is a (second-order) operator $\Delta: \ltm \rightarrow \ltm$, given by the trace of the second covariant derivative defined (for this work) via the Levi-Cita connection \cite{singer2012vdm} \textcolor{black}{(the unique connection compatible with the Riemann metric).} The connection Laplacian $\Delta$ has some desirable properties: it is negative semidefinite, self-adjoint and elliptic.
The Connection Laplacian $\Delta$ has a negative  spectrum $\{-\lambda_i, \Phii\}_{i=1}^{\infty}$ with eigenvalues $\lambda_i$ and  corresponding eigenvector fields $\Phii$ satisfying
\begin{equation}
\label{eigen}
\Delta\Phii = -\lambda_i\Phii,
\end{equation}
with $0<\lambda_1\leq\lambda_2\leq\dots$. The only possible accumulation \textcolor{black}{(limit)} point is $-\infty$ \black{\cite{singer2012vdm}}. The $\lambda_i$s and the  $\Phii$s can be interpreted as the canonical frequencies and oscillation modes of $\tm$.

 We can use the Connection Laplacian \textcolor{black}{to fathom a heat equation for vector diffusion:}
\begin{equation}
\label{diff_eq}
\frac{\partial \U(x,t)}{\partial t} - \Delta\U(x,t) = 0,
\end{equation}
where $\U: \M \times \mathbb{R}_0^+ \rightarrow \tm$ and $\U( \cdot, t) \in \ltm \, \forall t \in \mathbb{R}_0^+$; we denote the initial condition condition with $\U( x, 0) = \F(x)$. As reported in \cite{Sharp2019vhm}, an intuitive interpretation of  \eqref{diff_eq} is imagining the evolution of the vector field $\U(x,t)$ over time as a "smearing out" of the initial vector field $\F(x)$. In this interpretation, the role of the Connection Laplacian can be understood as a means to  diffuse vectors from one tangent space to another \textcolor{black}{(indeed, in the ``flat'' case it is sufficient to independently diffuse each scalar component, however, this approach fails for curved space).} The solution of \eqref{diff_eq} is 
\begin{equation}
\label{exp_sol}
\U( x, t) = e^{t\Delta}\F(x),
\end{equation}
which provides a way to construct tangent bundle convolution, as explained in the following section. 
\begin{figure}[t]
    \centering
    \includegraphics[scale = .4]{sphere_ex_cropped.pdf}
    \caption{An example of tangent bundle signal}
    \label{fig:sphere}
\end{figure}
\vspace{-.1cm}
\subsection{Tangent Bundle Filters}
We are now in the condition of defining a convolution operation and tangent bundle convolutional filters leveraging the heat diffusion dynamics in \eqref{diff_eq}. 
\textcolor{black}{\begin{definition}[Tangent Bundle Filter] \label{def:tangent-bundle-filter}
Let $\th:\mathbb{R}^+ \rightarrow \mathbb{R}$ and let $\F \in \ltm$ be a tangent bundle signal. The manifold filter with impulse response $\th$, denoted with $\h$, is given by
\begin{equation}
    \label{convolution}
    \G(x) = \big(\th \star_{\tm} \F\big)= \int_0^{\infty}\th(t)\U(x,t)\textrm{d}t,
\end{equation}
where $\star_{\tm}$ is the tangent bundle convolution, and $\U(x,t)$ is the solution of the heat equation in \eqref{diff_eq} with $\U(x,0) = \F(x)$.
\end{definition}}
In the following we will use the terms tangent bundle filter and tangent bundle convolution interchangeably. \textcolor{black}{One cannot explicity compute the output $\G$ directly from the input $\F$ in  Definition \ref{def:tangent-bundle-filter}. However,
this is remedied by injecting the solution of the heat equation \eqref{exp_sol} into
\eqref{convolution}.} In this way, we can derive a closed-form expression for $\h$ that is parametric on the Connection Laplacian, as shown in the following proposition.
\begin{proposition}[Parametric Filter] \label{prop:parametric-filter}
    Any tangent bundle filter $\h$ defined as in \eqref{convolution} is a parametric map $\h(\Delta)$ of the Connection Laplacian operator $\Delta$, given by
\begin{equation}
    \label{param_conv}
    \G(x) = \h\F(x) =  \int_0^{\infty}\th(t)e^{t\Delta}\F(x)\textrm{d}t = \h(\Delta)\F(x).
\end{equation}
\end{proposition}
We can make several considerations starting from Proposition \ref{prop:parametric-filter}: we can state that tangent bundle filters are spatial operators,
since they operate directly on points $x \in \mathcal{M}$; moreover,  they are local operators, because they are parametrized by $\Delta$ which is itself a local operator.
\begin{remark}
The exponential term $e^{t\Delta}$ can be seen as a diffusion or shift operator similiar to a time delay in a linear time-invariant (LTI) filter \cite{oppenheim1997signals}, or to a graph shift operator in a linear shift-invariant (LSI) graph filter \cite{gama2020gcnmagazine}, or to a manifold shift operator based on the Laplace-Beltrami operator \cite{wang2022convolution}. \black{The reason for
this resemblance is that tangent bundle filters are linear combinations of the elements of the tangent bundle diffusion sequence, such as graph filters are linear combinations of
the elements of the graph diffusion sequence and manifold filters are linear combinations of the elements of the manifold diffusion sequence. Tangent bundle filters
are also generalizations of standard time-convolutions, \textcolor{black}{which may be obtained by considering the one-sided wave equation on the real line and the derivative  operator.} The previous considerations are further useful to validate the consistency of the proposed convolution operation; a brief formal discussion can be found in Appendix \ref{ap:consistency} of this work and \cite[Appendix A]{wang2022stability}}.
\begin{figure*}[t!]
    \centering
    \input{alpha_FDT} 
    \caption{Illustration of an $\alpha$-FDT filter. The $x$-axis stands for the spectrum with each sample representing an eigenvalue. The gray shaded areas show the grouping of the eigenvalues according to Definition 6. The red lines show a set of $\alpha$-FDT filters that can discriminate each eigenvalue group.}
    \label{fig:alpha_fdt}
\end{figure*}
\end{remark}
\vspace{-.1cm}
\subsection{Frequency Representation of Tangent Bundles Filters}
The spectral properties of the Connection Laplacian $\Delta$ allow us to introduce the \textcolor{black}{notion of a frequency domain. Following the approach historically common to many signal processing frameworks, we define the frequency representation of a tangent bundle signal $\F \in \mathcal{L}^2(\tm)$ as its projection onto the eigenbasis of the Connection Laplacian}
\begin{equation}
\label{freq_resp}
    \big[\hat{F}\big]_i = \langle \F, \Phii \rangle = \int_{\M}\langle \F(x), \Phii(x) \rangle_{\tmx} \textrm{d}\mu(x).
\end{equation}
\begin{proposition}[Frequency Representation] \label{prop:frequencey-rep}
    Given a tangent bundle signal $\F$ and a tangent bundle filter $\h(\Delta)$ as in Definition \ref{def:tangent-bundle-filter}, the frequency representation of the filtered signal $\G = \h(\Delta)\F$ is given by
\begin{equation}
    \big[\hat{G}\big]_i = \int_0^{\infty} \th(t)e^{-t\lambda_i}\textrm{d}t\big[\hat{F}\big].
\end{equation}
\end{proposition}
\begin{proof}
    See Appendix \ref{ap:prop1}.
\end{proof}
Therefore, we can characterize the frequency response of a tangent bundle filter in the following way.
\begin{definition}[Frequency Response] \label{def:frequency}
    The frequency response $\hat{h}(\lambda)$ of the filter $\h(\Delta)$ is defined as
\begin{equation}
    \label{frequency_filt}
    \hat{h}(\lambda) = \int_0^{\infty} \th(t)e^{-t\lambda}\textrm{d}t.
\end{equation}
\end{definition}
This leads to $\big[\hat{G}\big]_i = \hat{h}(\lambda_i)\big[\hat{F}\big]_i$, meaning that the  tangent bundle filter is point-wise in the frequency domain. We can finally write the frequency representation of the filter as
\begin{equation}
\label{filtered_series}
\G = \h(\Delta)\F = \sum_{i=1}^{\infty} \hat{h}(\lambda_i) \langle \F, \Phii \rangle \Phii.
\end{equation}
\begin{remark}
    Definition \ref{def:frequency} can be seen a Laplace transform, that reduces to a Fourier transform when restricted to $\lambda = j\omega$. For this reasons, the frequency response of tangent bundle filters generalizes also the frequency response of standard time filters \cite{oppenheim1997signals} (which is a Fourier transform), as well as the one of graph filters \cite{shuman2013emerging} (which is a $z$-transform, thus a discretization of a Laplace transform), and the one of manifold filters \cite{wang2022convolution} (which is a Laplace transform).
\end{remark}
\vspace{-.1cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{$\alpha$-FDT Filters}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The spectrum of the Connection Laplacian $\Delta$ is infinite-dimensional, i.e., there is an infinite (though countable) number of eigenvalues that need to be taken into account. However, under the mild assumption of having an accumulation point at $-\infty$ for the eigenvalues of $\Delta$, we can design filters to tackle this problem. This design strategy will be also crucial in proving the convergence result of the discretized filters and TNNs to the underlying continuous filters and TNNs stated in Theorem 1 (Section V.B). 
\begin{proposition}[$\alpha$-Separated Spectrum \cite{wang2022stability}]
    Let us denote the set of the eigenvalues of the Connection Laplacian with $\Lambda=\{-\lambda_i\}_i$. If $\Lambda$ has an accumulation point at $-\infty$, then there exist $\alpha > 0$ and a finite partition $\Lambda = \Lambda_1(\alpha) \cup \ldots\cup \Lambda_N(\alpha)$ such that, for all $\lambda_i \in \Lambda_k(\alpha)$ and $\lambda_j \in \Lambda_l(\alpha)$, $k \neq l$, it holds:
\begin{align}\label{eqn:alpha-spectrum}
|\lambda_i - \lambda_j| > \alpha \text{.}
\end{align}
\end{proposition}
\begin{proof}
    The proof is a direct consequence of the definition of accumulation point.
\end{proof}
\begin{definition}[$\alpha$-FDT Filters \cite{wang2022stability}]
      The $\alpha$-frequency difference threshold ($\alpha$-FDT) filter is defined as a filter $\mathbf{h}(\Delta)$ whose frequency response satisfies:
\begin{equation} \label{eq:fdt-filter}
    |\hhath(\lambda_i)-\hhath(\lambda_j)|\leq \delta_k \mbox{ for all } \lambda_i, \lambda_j \in \Lambda_k(\alpha) 
\end{equation}
\end{definition}
It is easy to see that the family of subsets of eigenvalues in the $\alpha-$separated spectrum are eigenvalue groups (of any size, even singletons) spaced by at least $\alpha$. The $\alpha$-FDT filter assigns similar frequency responses to eigenvalues of the same group. In other words, the $\alpha$-FDT filter does not discriminate between eigenvalues belonging to the same group. An example of an $\alpha$-FDT is depicted in Fig.~\ref{fig:alpha_fdt}. Finally, we  define Lipshitz continous tangent bundle filters and non-amplifying tangent bundle filters.
\begin{definition}[Tangent Bundle Filters with Lipschitz Continuity]
    A tangent bundle filter is $C$-Lispchitz if its frequency response is Lipschitz continuous with constant $C$, i.e,
    \begin{equation}
     |\hat{h}(a)-\hat{h}(b)| \leq C |a-b|\text{ for all } a,b\in (0,\infty)\text{.}
    \end{equation}
\end{definition}
\begin{definition}[Non-Amplifying Tangent Bundle Filters]
    A tangent bundle filter is non-amplifying if for all $\lambda\in(0,\infty)$, its frequency response $\hat{h}$ satisfies $|\hat{h}(\lambda)|\leq 1$.
\end{definition}
The Lipschitz continuity is a standard assumption, while the non-amplifying assumption is perfectly reasonable, as any (finite-energy) filter function $\hhath(\lambda)$ can be normalized.
\vspace{-.1cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Tangent Bundle Neural Networks}\label{sec:tnn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We define a layer of a Tangent Bundle Neural Network (TNN) as a bank of tangent bundle filters followed by a pointwise non-linearity. In this setting,  pointwise informally means ``pointwise in the ambient space''. We introduce the notion of differential-preserving non-linearity to formalize this concept in a consistent way.
\begin{definition}[Differential-preserving Non-Linearity] \label{def:pointwise}
    \textcolor{black}{Denote with $U_x \subset \tmxrp$ the image of the injective differential $d\i$ in $\tmxrp$. A mapping $\sigma:\ltm \rightarrow \ltm$ is a differential-preserving non-linearity if it can be written as $\sigma(\F(x)) = d\i^{-1} \widetilde{\sigma}_x  (d\i  \F(x))$, where $\widetilde{\sigma}_x: U _x\rightarrow U_x$ is a point-wise non-linearity in the usual (Euclidean) sense.}
\end{definition}
Furthermore, we assume that $\widetilde{\sigma}_x = \widetilde{\sigma}$ for all $x \in \M$. 
\begin{definition}[Tangent Bundle Neural Networks]
    The $l$-th layer of a TNN with $F_l$ input signals $\{\F_l^q\}_{q = 1}^{F_l}$, $F_{l+1}$ output signals $\{\F_{l+1}^u\}_{u = 1}^{F_{l+1}}$, and non-linearity $\sigma(\cdot)$ is defined as
\begin{equation}
    \label{tnn_layer}
    \F_{l+1}^u(x) = \sigma\Bigg(\sum_{q=1}^{F_l}\h(\Delta)_l^{u,q}\F_l^q(x)\Bigg), \; u = 1,...,F_{l+1}.
\end{equation}
\end{definition}
Therefore, a  TNN of depth $L$ with input signals $\{\mathbf{F}^q\}_{q=1}^{F_0}$ is built as the stack of $L$ layers defined as in \eqref{tnn_layer}, where $\F_0^q = \F^q$. An additional task-dependent readout layer \textcolor{black}{(e.g~sum for classification) can be appended to the final layer.}

To globally represent the TNN, we collect all the filter impulse responses in a function set $\mathcal{H} = \big\{\widetilde{h}_l^{u,q}\big\}_{l,u,q}$ and we describe the TNN $u$-th output as a mapping $\F^u_{L}=\bPsi_u\big(\mathcal{H}, \Delta, \{\mathbf{F}^q\}_{q=1}^{F_0}\big)$ to emphasize that at TNN is parameterized by both $\mathcal{H}$ and the Connection Laplacian $\Delta$.
\vspace{-.2cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discretization in Space and Time} \label{sec:disc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-.1cm}
Tangent Bundle Filters and Tangent Bundle Neural Networks operate on tangent bundle signals, thus they are continuous architectures that cannot be directly implemented in practice. Here we provide a \textcolor{black}{procedure for discretizing tangent bundle signals, both in time and spatial domains; the discretized counterpart of TNNs is a novel principled variant of the very recently introduced Sheaf Neural Networks \cite{hansen2020sheafnn}}. For this reason, in this section we firstly provide a brief review of cellular sheaves over undirected graphs, and then we explain the proposed discretization procedure. 
% The reader will notice that there are some slight abuses and overridings of notation in the following subsections, but we believe that  they are useful to better understand the discretization procedure and to make clearer the link between tangent bundles and orthogonal cellular sheaves.
\vspace{-.1cm}
\subsection{Cellular Sheaves}
\textcolor{black}{A cellular sheaf over a (undirected) graph consists of the data of a vector space for each node and edge and a collection of linear transformations indexed by node-edge incidence pairs of the graph. We introduce the following non-standard notation to place emphasis on the role that sheaves play in approximating tangent bundles as the number of nodes increases.
\begin{definition}[Cellular Sheaf over a Graph]
    Suppose $\mathcal{M}_n = (\mathcal{V}_n, \mathcal{E}_n)$ is an undirected graph with $n = |\mathcal{V}_n|$ nodes. A cellular sheaf over $\mathcal{M}_n$ is the tuple $\tm_n = (\mathcal{M}_n, \mathcal{F})$, i.e.:
\begin{itemize}
    \item A vector space $\mathcal{F}(v)$ for each $v \in \mathcal{V}_n$. We refer to these vector spaces as nodes stalks.
    \item A vector space $\mathcal{F}(e)$ for each $e \in \mathcal{E}_n$. We refer to these vector spaces as edges stalks.
    \item A linear mapping $V_{v,e} : \mathcal{F}(v) \rightarrow \mathcal{F}(e)$ represented by a matrix $\mathbf{V}_{v,e}$ for each pair $(v,e) \in \mathcal{V}_n \times \mathcal{E}_n$ with incidence $v~\inc~e$. These mappings are called restriction maps.
\end{itemize}
The space $\mathcal{L}^2(\tm_n) = \bigoplus_{v \in \mathcal{V}} \mathcal{F}(v)$ formed by the direct sum of vector spaces associated with the nodes of the graph is commonly called the space of $0$-cochains, which we refer to as sheaf signals on $\tm_n$. We write a sheaf signal on $\mathcal{M}_n$ as $\mathbf{f}_n \in \mathcal{L}^2(\tm_n)$.
\end{definition}
\begin{definition}[Sheaf Laplacian]
The (non-normalized) Sheaf Laplacian of a sheaf $\tm_n$ is a linear mapping $\Delta_n:\mathcal{L}^2(\tm_n) \rightarrow \mathcal{L}^2(\tm_n)$ defined node-wise
\begin{equation}
    (\Delta_n\mathbf{f}_n)(v) = \sum_{v \inc e \coinc u}\mathbf{V}_{v,e}^T(\mathbf{V}_{v,e}\mathbf{f}_n(v) - \mathbf{V}_{v \inc e}\mathbf{f}_n(u)).
\end{equation}
\end{definition}
While in general, the dimensions of the stalks may be arbitrary, this work focuses on discrete $\mathcal{O}(d)$-bundles, or orthogonal sheaves. In an orthogonal sheaf, we have $\mathbf{V}_{v,e}^{-1} = \mathbf{V}_{v,e}^{T}$ for all $v \inc e$ and $\mathcal{F}(v) \cong \mathbb{R}^d$ for all $v$.} \textcolor{black}{An intuitive interpretation of cellular sheaves is given in \cite{hansen2020opinion} in terms of opinion dynamics. In this setting, the component $\mathbf{f}_n(v)$ of the sheaf signal $\mathbf{f}_n$ is the "private opinion" of node $v$, while $\mathbf{V}_{v,e}\mathbf{f}_n(v)$ describes how that private opinion publicly manifests  in the "discourse space"  $\mathcal{F}(e)$: in this sense, the Sheaf Laplacian applied to a  sheaf signal measures the aggregated "disagreement of opinions" at each node \cite{bodnar2022sheafdiff}.}
\begin{figure*}
    \centering
    \includegraphics[scale = .36]{transport.pdf} 
    \caption{Pictorial view of discrete parallel transport.}
    \label{fig:transport}
\end{figure*}
\subsection{Discretization in the Space Domain}\label{sec:space_disc}
The manifold $\M$, the tangent bundle $\tm$, and the Connection Laplacian $\Delta$ can be approximated from a set of sampled points $\mathcal{X} \subset \mathbb{R}^p$. Knowing the coordinates of the sampled points, we construct an orthogonal cellular sheaf over an undirected geometric graph such that its  normalized Sheaf Laplacian converges to the manifold Connection Laplacian as the number of sampled points (nodes) increases \cite{singer2013spectral}. Formally,  we assume that a set of $n$  points $\mathcal{X}=\{x_1,\dots,x_n\}\subset \mathbb{R}^p$ are sampled i.i.d. from measure $\mu$ over $\mathcal{M}$. We  build a cellular sheaf $\tm_n$ via the Vector Diffusion Maps procedure whose details are listed in \cite{singer2012vdm} and which we briefly review here.

\textcolor{black}{We start by building a weighted (geometric) graph $\M_n = (\mathcal{V}_n, \mathcal{E}_n$) nodes $\mathcal{V}_n = \{1,2,\dots, n \}$ and weights $w_{ij}$ for nodes $i$ and $j$ as follows. Set a scale $\epsilon>0$. For each pair $i,j \in \mathcal{V}_n \times \mathcal{V}_n$, if $\|x_i - x_j \|_2^2 \leq \epsilon$, then let $ij \in \mathcal{E}_n$ with weight
\begin{equation}
\label{graph_weights} 
    w_{i,j} = \exp\Bigg(\frac{||x_i-x_j||_{2}}{\sqrt{\epsilon}}\Bigg);
\end{equation}
otherwise, $ij \notin \mathcal{E}_n$ and $w_{i,j} = 0$.}
The neighborhood $\mathcal{N}_i$ of each point $x_i$ contains the points $x_j \in \mathcal{X}$ lying in a ball of radius $\sqrt{\epsilon}$ centered at $x_i$. Using a local PCA procedure, we assign to each node $i$ an orthogonal transformation $\Oi \in \mathbb{R}^{p\times\hd}$, that is an approximation of a basis of the tangent space $\mathcal{T}_{x_i}\mathcal{M}$, with $\hd$ being an estimate of $d$ obtained from the same procedure (or $d$ itself, if known). In particular, we fix another scale parameter $\epsilon_{\textrm{PCA}}$ (different from the graph kernel scale parameter $\epsilon$) and we define the PCA neighborhood $\mathcal{N}^{\textrm{P}}_i$ of each point $x_i$ as the points $x_j \in \mathcal{X}$ lying in a ball of radius $\sqrt{\epsilon_{\textrm{PCA}}}$ centered at $x_i$. We define $\mathbf{X}_i \in \mathbb{R}^{p \times |\mathcal{N}^{\mathrm{P}}_i|}$ for each point to be a matrix whose $j$-th column is the vector $x_j - x_i$, with $x_j \in \mathcal{N}^{\mathrm{P}}_i$; equivalently, it is possible to shift each neighbor by the mean $1/|\mathcal{N}^{\textrm{P}}_i|\sum_{x_j \in \mathcal{N}^{\mathrm{P}}_i} x_j$. 
At this point, we compute for each point a matrix $\mathbf{B}_i = \mathbf{X}_i\mathbf{C}_i$, where $\mathbf{C}_i$ is a diagonal matrix whose entry are defined as $\mathbf{C}(j,j) = \sqrt{K(||x_i -x_j||_{2}/\sqrt{\epsilon_{\textrm{PCA}}})}$, with $K(\cdot)$ being any twice differentiable positive monotone function supported on $[0,1]$ \textcolor{black}{(this scaling is useful to emphasize nearby points over far away points). We now perform the actual Local PCA by computing, per each point, the following covariance matrix and its eigendecomposition
\begin{equation}\label{cov_mat}
\mathbf{R}_i=\B_i^T\B_i=\mathbf{M}_i\Sigma_i\mathbf{M}_i^T.
\end{equation}}
\textcolor{black}{\begin{definition}[Approximated Tangent Space \cite{singer2012vdm}] \label{def:approxtangent}
    For each point $x_i \in \mathcal{X}\subset \mathcal{M}$, the approximated basis $\mathbf{O}_i$ of its tangent space $\mathcal{T}_{x_i}\mathcal{M}$ is given by the $\hd$ largest left eigenvectors of its corresponding covariance matrix $\mathbf{R}_i$ from \eqref{cov_mat}, where $\hd$ is an estimate of $\dim(\M)$ or $\dim(\M)$, if known.
\end{definition}}
\textcolor{black}{An efficient way to compute an estimate $\hd$ if the true manifold dimension is not known can be found in \cite{singer2012vdm}. Definition \ref{def:approxtangent} is equivalent to say that $\mathbf{O}_i$ is built with the first $\hd$ columns of $\mathbf{M}_i$ from \eqref{cov_mat}. Morover, as usual, $\mathbf{O}_i$ can be equivalently (and efficiently) computed as the first $\hd$ left singular vectors of $\B_i$, without explicitly computing the covariance matrix $\R_i$}. The local PCA procedure is summarized in Algorithm 1 in Appendix \ref{ap:algos}. Now, an approximation of the parallel transport operator \cite{lee2006Riemannian}, that is a linear transformation from $\mathcal{T}_{x_i}\mathcal{M}$ to $\mathcal{T}_{x_j}\mathcal{M}$, is needed. In the discrete domain, this translates in associating a matrix to each edge of the above graph. For $\epsilon$  small enough, $\mathcal{T}_{x_i}\mathcal{M}$ and $\mathcal{T}_{x_j}\mathcal{M}$ are close, meaning that the column spaces of $\Oi$ and $\Oj$ are similiar. If the column spaces coincide, then the matrices $\Oi$ and $\Oj$ are related by an orthogonal transformation $\widetilde{\mathbf{O}}_{i,j}$: $\widetilde{\mathbf{O}}_{i,j} = \Oi^T\Oj$. However, if $\M$ is curved, the column spaces of $\Oi$ and $\Oj$ will not coincide. For this reason, the transport operator approximation $\Oij$ is defined as the closest orthogonal matrix \cite{singer2012vdm} to $\widetilde{\mathbf{O}}_{i,j}$, and it is computed as $\Oij = \mathbf{M}_{i,j}\mathbf{V}^T_{i,j} \in \mathbb{R}^{\hd\times\hd}$, where $\mathbf{M}_{i,j}$ and $\mathbf{V}_{i,j}$ are the SVD of $\widetilde{\mathbf{O}}_{i,j} = \mathbf{M}_{i,j}\boldsymbol{\Sigma}_{i,j}\mathbf{V}^T_{i,j}$ (and the restriction maps of the approximating sheaf); a pictorial view of this discrete approximating  transport is presented in Fig. \ref{fig:transport}. We now build a block matrix $\S \in \mathbb{R}^{n\hd\times n\hd}$ and a diagonal block matrix $\D \in \mathbb{R}^{n\hd\times n\hd}$ with $\hd \times \hd $ blocks defined as
\begin{align}
\label{DS}
&\S_{i,j} = w_{i,j}\widetilde{\mathbf{D}}_i^{-1}\Oij\widetilde{\mathbf{D}}_j^{-1}, \quad\D_{i,i} = \textrm{ndeg}(i) \mathbf{I}_{\hd} ,
\end{align}
where  $\widetilde{\mathbf{D}}_i = \textrm{deg}(i)\mathbf{I}_{\hd}$, $\textrm{deg}(i) = \sum_{j}w_{i,j}$ is the degree of node $i$, and $\textrm{ndeg}(i) = \sum_{j}w_{i,j}/(\textrm{deg}(i)\textrm{deg}(j))$ is the normalized degree of node $i$. Finally, we define the (normalized) Sheaf Laplacian as the following matrix
\begin{equation}
\label{sheaf_laplacian}
    \Delta_n = \epsilon^{-1}\big(\D^{-1}\S - \mathbf{I}\big) \in \mathbb{R}^{n\hd\times n\hd},
\end{equation}
which is the approximated Connection Laplacian of the underlying manifold \black{$\ccalM$}. The procedure to build the Sheaf Laplacian is summarized in Algorithm 2 in Appendix \ref{ap:algos}. A sheaf $\tm_n$ with this (orthogonal) structure represents a discretized version of  $\tm$. For further details, the reader can refer to \cite{singer2012vdm}.
At this point, we introduce a linear sampling operator $\samp_n^{\mathcal{X}}:\ltm \rightarrow \ltmn$ to discretize a  tangent bundle signal $\F$ as a sheaf signal $\f_n \in \mathbb{R}^{n\hd}$ such that:
\begin{align}
\label{sampler}
    &\f_n = \samp_n^{\mathcal{X}}\F, \\
    &\f_{n}(x_i):=[\f_n]_{((i-1)\hd+1):(i+1)\hd} = \Oi^T  d\i\F(x_i) \in \mathbb{R}^\hd,
\end{align}
where $((i-1)\hd+1):(i+1)\hd$ indicates all the components of $\f_{n}$ from the $((i-1)\hd+1)$-th to the $(i+1)\hd$-th component. In words, the sampling operator $\samp_n^{\mathcal{X}}$ in \eqref{sampler} takes  the embedded tangent signal $d\i\F$ as input, evaluates it on each point $x_i$ in the sampling set $\mathcal{X}$, projects the evaluated signals $d\i_{x_i}\left(\F(x_i)\right) \in \mathbb{R}^p$ over the $d$-dimensional subspaces spanned by the $\Oi$s from Definition \ref{def:approxtangent} and, finally, sequentially collects the $n$ projections $\Oi^Td\i\F(x_i) \in \mathbb{R}^\hd$ in the vector $\f_n \in \mathbb{R}^{n\hd}$, representing the discretized tangent bundle signal. We are now in the condition of plugging the discretized operator from \eqref{sheaf_laplacian} and signal from \eqref{sampler} in the definition of tangent bundle filter from \eqref{param_conv}, obtaining:
\begin{equation}
    \label{discr_param_conv}
    \g_n = \int_0^{\infty}\th(t)e^{t\Delta_n}\f_n\textrm{d}t = \h(\Delta_n)\f_n \in \mathbb{R}^{n\hd}.
\end{equation}
Following the same considerations of Section \ref{sec:tnn}, we can define a  discretized space tangent bundle neural network (D-TNN) as the stack of $L$ layers of the form
\begin{equation}
    \label{dt_tnn_layer}
    \f_{n,l+1}^u = \sigma\Bigg(\sum_{q=1}^{F_l}\h(\Delta_n)_l^{u,q}\f_{n,l}^q\Bigg), \; u = 1,...,F_{l+1},
\end{equation}
where (with a slight abuse of notation) $\sigma$ has the same point-wise law of $\widetilde{\sigma}$ in Definition \ref{def:pointwise}.  As in the continuous case, we describe the $u$th output of a D-TNN as a mapping $\bPsi_u\big(\mathcal{H}, \Delta_n, \{\x_n^q\}_{q=1}^{F_0}\big)$ to emphasize that it is parameterized by filters $\mathcal{H}$ and the Sheaf Laplacian $\Delta_n$. The D-TNN architecture comes with desirable theoretical properties. As the number of sampling points goes to infinity, the Sheaf Laplacian $\Delta_n$ converges to the Connection Laplacian $\Delta$ \cite{singer2012vdm} and the  sheaf signal $\x_n$ consequently converges to the tangent bundle signal $\F$. Combining these results, we prove in the next theorem that the output of a D-TNN converges to the output of the corresponding underlying TNN as the sample size increases, validating the approximation fitness of a D-TNN. At the best of our knowledge, this is the first result to \textit{formally} connect Sheaf Neural Networks to tangent bundles of Riemann manifolds. 

\begin{theorem} \label{theorem-main}
    Let $\mathcal{X}=\{x_1,\dots,x_n\}\subset \mathbb{R}^p$ be a set of $n$ i.i.d. sampled points from measure $\mu$ over $\M \subset \mathbb{R}^p$ and $\F$ a tangent bundle signal. Let $\tm_n$ be the cellular sheaf built from $\mathcal{X}$ as explained above, and let $\epsilon = n^{-2/(\hd+4)}$. Let  $\bPsi_u\big(\mathcal{H}, \cdot, \cdot \big)$ be the $u$th output of a neural network with $L$ layers parameterized by the operator $\Delta$ of  $\tm$  or by the discrete operator $\Delta_n$ of  $\tm_n$. If:
\begin{itemize}
    \item $\Delta$ has an accumulation point at $-\infty$;
    \item  the filters in $\mathcal{H}$ are $\alpha$-FDT filters
    \item the frequency response of filters in $\mathcal{H}$ are non-amplifying Lipschitz continuous;
    \item $\widetilde{\sigma}$ from Definition \ref{def:pointwise} is point-wise normalized Lipschitz continuous,
\end{itemize}
then it holds for each $u = 1, 2, \dots, F_L$ that:
\begin{equation}
\label{convergence_main}
\lim_{n \rightarrow \infty} ||\bPsi_u\big(\mathcal{H}, \Delta_n, \samp_n^{\mathcal{X}}\F\big) - \samp_n^{\mathcal{X}}\bPsi_u\big(\mathcal{H}, \Delta,\F\big)||_{\tm_n} = 0,
\end{equation}
with the limit taken in probability.
\end{theorem}
\begin{proof}
    See Supplemental Materials.
\end{proof}


\begin{table*}[t!]
\centering\begin{tabular}{@{}lcccc@{}}
\cmidrule(l){3-5}
                                               &                             & $\tau = 7 \cdot 10^{-2}$                                                    & $\tau = 10^{-1}$                                              & $\tau = 3 \cdot 10^{-1}$                                                 \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=100$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1.18\cdot 10^{-2}} \pm 1.38\cdot 10^{-3}$}   & \multicolumn{1}{c|}{$\mathbf{2.03\cdot 10^{-2}} \pm 2.22 \cdot 10^{-3}$}  & \multicolumn{1}{c|}{$\mathbf{1.93\cdot 10^{-1}} \pm 3.26 \cdot 10^{-2}$}    \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$1.38\cdot 10^{-2} \pm 3 \cdot 10^{-3}$}         & \multicolumn{1}{c|}{$4\cdot 10^{-2} \pm 3.32 \cdot 10^{-2}$}         & \multicolumn{1}{c|}{$2.11\cdot 10^{-2} \pm 3.27 \cdot 10^{-2}$}           \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=200$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1.1\cdot 10^{-2}} \pm 1.07\cdot 10^{-3}$}   & \multicolumn{1}{c|}{$\mathbf{2.13\cdot 10^{-2}} \pm 3.39 \cdot 10^{-3}$}  & \multicolumn{1}{c|}{$\mathbf{1.71\cdot 10^{-1}} \pm 1.77 \cdot 10^{-2}$}    \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$1.33\cdot 10^{-2} \pm 2.69 \cdot 10^{-2}$}         & \multicolumn{1}{c|}{$2.69\cdot 10^{-2} \pm 4.67 \cdot 10^{-2}$}         & \multicolumn{1}{c|}{$2.12\cdot 10^{-1} \pm 3.67 \cdot 10^{-2}$}           \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=300$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1\cdot 10^{-2}} \pm 1\cdot 10^{-3}$}   & \multicolumn{1}{c|}{$\mathbf{2.02\cdot 10^{-2}} \pm 1.28 \cdot 10^{-3}$}  & \multicolumn{1}{c|}{$\mathbf{1.64\cdot 10^{-1}} \pm 1.31 \cdot 10^{-2}$}    \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$1.36\cdot 10^{-2} \pm 2.7 \cdot 10^{-3}$}         & \multicolumn{1}{c|}{$2.65\cdot 10^{-2} \pm 4.2 \cdot 10^{-3}$}         & \multicolumn{1}{c|}{$1.99\cdot 10^{-1} \pm 2.8 \cdot 10^{-2}$}           \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=400$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1.06\cdot 10^{-2} \pm 6.84 \cdot 10^{-4}}$}   & \multicolumn{1}{c|}{$\mathbf{2.07\cdot 10^{-2}} \pm 1.05  \cdot 10^{-3}$} & \multicolumn{1}{c|}{$\mathbf{1.64 \cdot 10^{-1}} \pm 1.5 \cdot 10^{-2}$} \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$8.3\cdot 10^{-2} \pm 2.22 \cdot 10^{-1}$} & \multicolumn{1}{c|}{$1.49\cdot 10^{-1} \pm 3 \cdot 10^{-1}$}         & \multicolumn{1}{c|}{$2.4 \cdot 10^{-1} \pm 1.25 \cdot 10^{-1}$}           \\ \bottomrule
\end{tabular}
\caption{MSE on the torus denoising task}
     \label{table:results_torus}
\end{table*}
\vspace{-.1cm}
\subsection{Discretization in the Time Domain} 
The discretization in space introduced in the previous section is still not enough for implementing TNNs in practice. Indeed, learning the continuous time function $\tilde{h}(t)$ is in general infeasible. For this reason, we discretize $\tilde{h}(t)$ in the continuous time domain by fixing a sampling interval $T_s>0$. In this way, we can replace the filter response function with a series of coefficients $h_k = \tilde{h}(k T_s)$, $k =0 ,1, 2\dots$. Fixing $T_s=1$ and taking $K$ samples over the time horizon, the discrete-time version of the convolution in \eqref{convolution} is given by
\begin{equation}
\label{eqn:manifold_convolution_discrete}
    \bbh(\Delta_n) \F(x)= \sum_{k=0}^{\infty} h_k e^{k\Delta_n}\F(x), 
\end{equation}
which can be seen as a finite impulse response (FIR) filter with shift operator $e^{\Delta_n}$. We are now in the condition of injecting the space discretization from Section \ref{sec:disc} in the finite-time architecture in \eqref{eqn:manifold_convolution_discrete}, thus finally obtaining an implementable tangent bundle filter that exploits the approximating cellular sheaf $\tm_n$ as
\begin{equation}
\label{eqn:discrete_manifold_convolution_discrete}
  \mathbf{g}_n=  \bbh(\Delta_n) \f_n = \sum_{k=0}^{K-1} h_k e^{k\Delta_n}\f_n.
\end{equation}
The discretized manifold filter of order $K$ can be seen as a generalization of graph convolution  to the orthogonal cellular sheaf domain. Thus, we refer $e^{\Delta_n}$ as a sheaf shift operator. At this point, by replacing the filter $\bbh_l^{pq}(\Delta_n)$ in \eqref{dt_tnn_layer} with \eqref{eqn:discrete_manifold_convolution_discrete}, we obtain the following architecture:
\begin{equation}
    \label{sheaf_nn_layer}
    \f_{n,l+1}^u = \sigma\Bigg(\sum_{q=1}^{F_l}\sum_{k=1}^{K}h_{k,l}^{u,q}\big(e^{\Delta_n}\big)^k\f_{n,l}^q\Bigg), \; u = 1,...,F_{l+1},
\end{equation}
that we refer to as  discretized space-time tangent bundle neural network (DD-TNN). DD-TNNs are a novel principled variant of the recently proposed Sheaf Neural Networks \cite{hansen2020sheafnn,bodnar2022sheafdiff,barbero2022sheafnnconn}, with $e^{\Delta_n}$  as (sheaf) shift operator and order $K$ diffusion. To better enhance this similarity, we rewrite the layer in \eqref{sheaf_nn_layer} in  matrix form by introducing the matrices $\mathbf{X}_{n,l}=\{\f_{n,l}^u\}_{u=1}^{F_{l}}\in \mathbb{R}^{n\hd \times F_{l}}$, and $\mathbf{H}_{l,k} = \{h_{k,l}^{u,q}\}_{q=1,u=1}^{F_l,F_{l+1}}\in \mathbb{R}^{F_l \times F_{l+1}}$, as
\begin{equation}\label{tnn_matrix}
    \mathbf{X}_{n,l+1} = \sigma\Bigg(\sum_{k=1}^{K}\big(e^{\Delta_n}\big)^k\mathbf{X}_{n,l}\mathbf{H}_{l,k}\Bigg) \; \in \mathbb{R}^{n\hd \times F_{l+1}},
\end{equation}
where the filter weights $\{\mathbf{H}_{l,k}\}_{l,k}$ are learnable parameters. Finaly, we have completed the process of building TNNs from (orthogonal) cellular sheaves and back.  The proposed methodology also shows that manifolds and their tangent bundles can be seen as the limits of graphs and  (orthogonal) cellular sheaves on top of them.
\vspace{-.1cm}
\section{Numerical Results}\label{sec:num_res}
In this section, we assess the performance of Tangent Bundle Neural Networks on three tasks: denoising of a tangent vector field on the torus (synthetic data), reconstruction from partial observations of the Earth wind field (real data) and forecasting of the Earth wind field (real data) obtained via a recurrent version of the proposed architecture. In this work, we are interested in showing the advantage of including information about the tangent bundle structure for processing tangent bundle signals. For this reason, in the following experiments we always use the vanilla DD-TNN architecture in \eqref{tnn_matrix} without any additional component, and we compare our architectures against vanilla Manifold Neural Networks (MNNs) from \cite{wang2022convolution}, convolutional architectures built in a similiar way to our but taking into account only the manifold structure. MNNs are implemented as GNNs with the exponential of minus the normalized cloud Laplacian \cite{belkin2001laplacian, wang2022convolution}. Therefore, from a discrete point of view, we present a comparison between a specific (novel and principled) Sheaf Neural Networks class (DD-TNNs) and a specific Graph Neural Networks class (MNNs), both obtained by space and time discretizations of continous manifold operators and signals.  It is clear that both classes of architectures could be enriched with many additional components (biases, layer normalization, dropout, gating, just to name a few), and it is also clear that a huge number of other neural architectures could be tailored to the proposed tasks, but testing this variants is beyond the scope of this paper.\footnote{Our TNN implementation \& datasets are available at \url{https://github.com/clabat9/Tangent-Bundle-Neural-Networks}}
\begin{figure}[t!]
\label{torus}
\centering
\includegraphics[width=0.5\textwidth]{torus_crop.pdf}
\caption{A ring torus} 
\end{figure} 
\begin{table*}[]
\centering\begin{tabular}{@{}lcccc@{}}
\cmidrule(l){3-5}
                                               &                             & $ \textrm{E}\{\widetilde{n}\} = 0.5n$                                                    & $\textrm{E}\{\widetilde{n}\} = 0.3n$                                            & $\textrm{E}\{\widetilde{n}\} = 0.1n$                                               \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=100$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1.93\cdot 10^{-2}} \pm 3.58\cdot 10^{-3}$}   & \multicolumn{1}{c|}{$\mathbf{1.16\cdot 10^{-2}} \pm 2.76 \cdot 10^{-3}$}  & \multicolumn{1}{c|}{$\mathbf{3.39\cdot 10^{-3}} \pm 1.58 \cdot 10^{-3}$}    \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$4.96\cdot 10^{-2} \pm 3.31 \cdot 10^{-2}$}         & \multicolumn{1}{c|}{$4\cdot 10^{-2} \pm 3.32 \cdot 10^{-2}$}         & \multicolumn{1}{c|}{$3.29\cdot 10^{-2} \pm 2.61 \cdot 10^{-2}$}           \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=200$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1.91\cdot 10^{-2}} \pm 2\cdot 10^{-3}$}   & \multicolumn{1}{c|}{$\mathbf{1.18\cdot 10^{-2}} \pm 1.8 \cdot 10^{-3}$}  & \multicolumn{1}{c|}{$\mathbf{3.86\cdot 10^{-3}} \pm 1.24 \cdot 10^{-3}$}    \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$3.23\cdot 10^{-2} \pm 1.19 \cdot 10^{-2}$}         & \multicolumn{1}{c|}{$2.79\cdot 10^{-2} \pm 1.35 \cdot 10^{-2}$}         & \multicolumn{1}{c|}{$2.54\cdot 10^{-2} \pm 1.92 \cdot 10^{-2}$}           \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=300$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1.92\cdot 10^{-2}} \pm 1.79\cdot 10^{-3}$}   & \multicolumn{1}{c|}{$\mathbf{1\cdot 10^{-2}} \pm 1.4 \cdot 10^{-3}$}  & \multicolumn{1}{c|}{$\mathbf{3.83\cdot 10^{-3}} \pm 9.7 \cdot 10^{-4}$}    \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$2.72\cdot 10^{-2} \pm 8.18 \cdot 10^{-3}$}         & \multicolumn{1}{c|}{$2.47\cdot 10^{-2} \pm 1.5 \cdot 10^{-2}$}         & \multicolumn{1}{c|}{$3\cdot 10^{-2} \pm 2.1 \cdot 10^{-2}$}           \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=400$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1.94\cdot 10^{-2} \pm 1.14 \cdot 10^{-3}}$}   & \multicolumn{1}{c|}{$\mathbf{1.16\cdot 10^{-2}} \pm 1.2  \cdot 10^{-3}$} & \multicolumn{1}{c|}{$\mathbf{4.1 \cdot 10^{-3}} \pm 7.5 \cdot 10^{-4}$} \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$2.87\cdot 10^{-2} \pm 8.9 \cdot 10^{-3}$} & \multicolumn{1}{c|}{$3.22\cdot 10^{-2} \pm 1.7 \cdot 10^{-2}$}         & \multicolumn{1}{c|}{$2 \cdot 10^{-2} \pm 1.72 \cdot 10^{-2}$}           \\ \bottomrule
\end{tabular}
\caption{MSE on the wind field reconstruction task}
     \label{table:results_recon}
\end{table*}
\begin{figure*}[t]
     \centering
    \hspace{-.5cm}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[scale = 0.17]{zonal_wind.jpeg}
         \caption{Zonal Wind}
         \label{fig:synth}
     \end{subfigure}
     \hspace{0.5cm}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[scale = 0.17]{meridional_wind.jpeg}
         \caption{Meridional Wind}
         \label{fig:ocean}
     \end{subfigure}
        \caption{Visualization of Earth wind field on 1st of January 2016 (a) Zonal component. (b) Meridional component.}
        \label{fig:maps}
\end{figure*}
\vspace{-.1cm}
\subsection{Torus Denoising}\label{torus_den}
We  design a denoising task on a 2-dimensional torus ($\ccalM=\mathcal{T}_2$) and its tangent bundle. A 2-dimensional torus is a surface obtained by revolving a circle in three-dimensional space about an axis that is coplanar with the circle. It is parametrized in the following way:
\begin{equation}\label{torus_param}
    [x,y,z] = [(b+a\cos{\theta})\cos{\phi},(b+a\cos{\theta})\sin{\phi},r\sin{\theta}],
\end{equation}
where $\phi,\theta \in [0, 2\pi)$, $a$ is the radius of the tube, and  $b$ is the distance from the center of the tube to the center of the torus; $b/a$ is called the aspect ratio. In this experiment, we work on a ring torus, thus a torus with aspect ratio greater than one (in particular, we choose $b= 0.3$, $a = 0.1$), depicted in Fig. 2. We uniformly sample the torus on $n$ points $\mathcal{X}=\{x_1,\dots,x_n\}$, and we compute the corresponding cellular sheaf $\tm_n$, Sheaf Laplacian $\Delta_n$ and signal sampler $\samp_n^{\mathcal{X}}$ as explained in Section \ref{sec:space_disc}, with $\epsilon_{\textrm{PCA}}=0.8$ and $\epsilon = 0.5$. We consider the tangent vector field over the torus given by
\begin{equation}
 d\i\F(x,y,z)=(-\sin{\theta},\cos{\theta},0) \in 
\mathbb{R}^3.
\end{equation}
At this point, we add AWGN with variance $\tau^2$ to $ d\i\F$ obtaining a noisy field $\widetilde{ d\i\F}$, then we use $\samp_n^{\mathcal{X}}$ to sample it, obtaining $\widetilde{\mathbf{f}}_n \in \mathbb{R}^{2n}$. We test the perfomance of the TNN architecture by evaluating its ability of denoising $\widetilde{\mathbf{f}}_n$. We exploit a 3 layers architecture with $8$ and $4$ hidden features, and $1$ output feature (the denoised signal),  using $K=2$ in each layer, with $\textrm{Tanh()}$ non linearities in the hidden layers and a linear activation on the output layer. We train the architecture to minimize the square error
\begin{equation}
\|\widetilde{\mathbf{f}}_n - \mathbf{f}_{n}^o\|^2
\end{equation}
between the noisy signal $\widetilde{\mathbf{f}}_n$ and the output of the network $\mathbf{f}_{n}^o$ via the ADAM optimizer \cite{kingma2014adam} and a patience of 5 epochs, with hyperparameters set to obtain the best results. We compare our architecture with a 3 layers MNN (implemented via a GNN as explained in \cite{wang2022convolution}) with same hyperparameters; to make the comparison fair, $\widetilde{ d\i\F}$ evaluated on $\mathcal{X}$ is given as input to the MNN, organized in a  matrix $\widetilde{\F}_n \in \mathbb{R}^{n \times 3}$. We train the MNN to minimize the square error 
\begin{equation}
\|\widetilde{\F}_n - \mathbf{F}_{n}^o\|_F^2,
\end{equation}
where $\|\|_F$ is the Frobenius Norm and $\mathbf{F}_{n}^o$ is the network output. It is trivial to see that the "two" MSEs used for TNN and MNN are completely equivalent due to the orthogonality of the projection matrices $\Oi$. In Table \ref{table:results_torus}, we evaluate TNNs and MNNs for three different expected sample sizes ($\textrm{E}\{n\} = 200$ and $\textrm{E}\{n\}=800$), for three different noise standard deviation ($\tau = 10^{-2}$,$\tau = 5\cdot10^{-2}$ and $\tau = 10^{-1}$), showing the MSEs $\frac{1}{n}\|\mathbf{f}_n - \mathbf{f}_{n}^o\|^2$ and $\frac{1}{n}\|\F_n - \mathbf{F}_{n}^o\|_F^2$, where $\mathbf{f}_n$ is the sampling via $\samp_n^{\mathcal{X}}$ of the clean field and $\mathbf{F}_n$ is the matrix collecting the clean field evaluated on $\mathcal{X}$. 8 sampling realizations and 8 mask realizations per each of them are tested; to make the results consistent, divergent or badly trained runs are discarded if present, and then the results are averaged. As the reader can notice from Table 1, TNNs always perform better than MNNs, due to their "bundle-awareness". 
\begin{table*}[]
\centering\begin{tabular}{@{}lcccc@{}}
\cmidrule(l){3-5}
                                               &                             & $T_f = 20$                                                    & $T_f = 50$                                           & $T_f = 80$                                               \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=100$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1.25\cdot 10^{-1}} \pm 1.63\cdot 10^{-2}$}   & \multicolumn{1}{c|}{$\mathbf{1.34\cdot 10^{-1}} \pm 3.98 \cdot 10^{-2}$}  & \multicolumn{1}{c|}{$\mathbf{2.01\cdot 10^{-1}} \pm 4.45 \cdot 10^{-2}$}    \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$3.93\cdot 10^{-1} \pm 3.01 \cdot 10^{-1}$}         & \multicolumn{1}{c|}{$7.02\cdot 10^{-1} \pm 2.78 \cdot 10^{-1}$}         & \multicolumn{1}{c|}{$8.98\cdot 10^{-1} \pm 2.35 \cdot 10^{-2}$}           \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=200$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1.59\cdot 10^{-1}} \pm 4.47\cdot 10^{-2}$}   & \multicolumn{1}{c|}{$\mathbf{1.86\cdot 10^{-1}} \pm 3.23 \cdot 10^{-2}$}  & \multicolumn{1}{c|}{$\mathbf{1.63\cdot 10^{-1}} \pm 4.05 \cdot 10^{-2}$}    \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$5.22\cdot 10^{-1} \pm 3.61 \cdot 10^{-1}$}         & \multicolumn{1}{c|}{$8.29\cdot 10^{-1} \pm 1.47 \cdot 10^{-1}$}         & \multicolumn{1}{c|}{$7.24\cdot 10^{-1} \pm 3.1 \cdot 10^{-1}$}           \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=300$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1.10\cdot 10^{-1}} \pm 7.02\cdot 10^{-3}$}   & \multicolumn{1}{c|}{$\mathbf{1.95\cdot 10^{-1}} \pm 3.71 \cdot 10^{-1}$}  & \multicolumn{1}{c|}{$1.59\cdot 10^{-1} \pm 2.87 \cdot 10^{-2}$}    \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$6.28\cdot 10^{-1} \pm 4.08 \cdot 10^{-1}$}         & \multicolumn{1}{c|}{$5.54\cdot 10^{-1} \pm 3.13 \cdot 10^{-1}$}         & \multicolumn{1}{c|}{$6.69\cdot 10^{-1} \pm 1.09 \cdot 10^{-1}$}           \\ \midrule
\multicolumn{1}{|l|}{\multirow{2}{*}{$\textrm{E}\{n\}=400$}} & \multicolumn{1}{c|}{DD-TNN} & \multicolumn{1}{c|}{$\mathbf{1.18\cdot 10^{-1} \pm 2.69 \cdot 10^{-2}}$}   & \multicolumn{1}{c|}{$\mathbf{1.79\cdot 10^{-1} \pm 4.45 \cdot 10^{-2}}$} & \multicolumn{1}{c|}{$\mathbf{1.61 \cdot 10^{-1}} \pm 4.08 \cdot 10^{-2}$} \\
\multicolumn{1}{|l|}{}                         & \multicolumn{1}{c|}{MNN}    & \multicolumn{1}{c|}{$2.21\cdot 10^{-1} \pm 3.68 \cdot 10^{-2}$} & \multicolumn{1}{c|}{$4.71\cdot 10^{-1} \pm 2.3 \cdot 10^{-1}$}         & \multicolumn{1}{c|}{$8.73 \cdot 10^{-1} \pm 9.49 \cdot 10^{-2}$}           \\ \bottomrule
\end{tabular}
\caption{MSE on the wind field forecasting task}
     \label{table:results_forec}
\end{table*}
\vspace{-.1cm}
\subsection{Wind Field Reconstruction}\label{sec:wind_rec}
We design a reconstruction task on real-world data. We use daily average measurements (the tangent bundle signal) of Earth surface wind field collected by NCEP/NCAR\footnote{https://psl.noaa.gov/data/gridded/data.ncep.reanalysis.html}; in particular, we use the data corresponding to the wind field of the 1st of January 2016, consisting in regularly spaced observations covering the whole Earth surface. The observations are localized in terms of latitude and longitude, thus we convert them in 3-dimensional coordinates by using the canonical spherical approximation for the Earth with nominal radius $R=6356.8$. The wind field is a 2-dimensional tangent vector field made of a zonal component, following the local parallel of latitude, and a meridional component, following the local meridian of longitude. A visualization of the wind field is shown in Fig. \ref{fig:maps} (figures taken from the official data repository). We preprocess the data by scaling the observations to be in the range $[-1,1]$. We first randomly sample $n$ points to obtain the sampling set $\mathcal{X}$, the cellular sheaf $\tm_n$, and the Sheaf Laplacian $\Delta_n$ again with $\epsilon_{\textrm{PCA}}=0.8$ and $\epsilon = 0.5$; at this point, we mask $\widetilde{n}<n$ of these points, we collect them in a set $\widetilde{\mathcal{X}}^C \subset \mathcal{X}$, and we aim to infer their corresponding measurements exploiting the remaining available $n-\widetilde{n}$ measurements, collected in the set $\widetilde{\mathcal{X}} \subset \mathcal{X}$. This reconstruction problem can be equivalently seen as a semi-supervised regression problem. To tackle it, we first organize the data corresponding to the point in $\mathcal{X}$ in a matrix $\mathbf{F}_n \in \mathbb{R}^{n \times 2}$, where the first column collects the zonal components and the second column collects the meridional components. At this point, we build the matrix $\widetilde{\mathbf{F}}_n\in \mathbb{R}^{n \times 2}$, that is a copy of $\mathbf{F}$ except for the rows of $\mathbf{F}$ corresponding to the masked points in $\widetilde{\mathcal{X}}^C$, replaced with the mean of the measurements of the available points in  $\widetilde{\mathcal{X}}$. We then vectorize $\widetilde{\mathbf{F}}_n$ to obtain $\widetilde{\mathbf{f}}_{n} \in \mathbb{R}^{2n}$, the input tangent bundle signal. We now exploit the same DD-TNN architecture from Section \ref{torus_den}, with the same hyperparameters, to perform the reconstruction task by training it to minimize the reconstruction square error
\begin{equation}
\sum_{i \in \widetilde{\mathcal{X}}}\|\widetilde{\mathbf{f}}_n(i) - \mathbf{f}_{n}^o(i)\|^2
\end{equation}
between the available measurements $\mathbf{f}_n(i)$ and the output of the network corresponding to them $\mathbf{f}_{n}^o(i)$, $i \in \widetilde{\mathcal{X}}$. Again, we compare our architecture with the same MNN from Section \ref{torus_den}, to which we give as input the matrix $\widetilde{\mathbf{F}}$ and we train it to minimize
\begin{equation}
\sum_{i \in \widetilde{\mathcal{X}}}\|\widetilde{\F}_n(i) - \mathbf{F}_{n}^o(i)\|^2,
\end{equation}
where  $\mathbf{F}_{n}^o$ is the network output and $\widetilde{\F}_n(i)$ indicates the $i-$th row of $\widetilde{\F}_n(i)$; being $\widetilde{\f}_n$ the vectorization of $\widetilde{\F}_n$, also in this case it is trivial to check the equivalence of the two MSEs. As evaluation metric we use the reconstruction MSE on the measurements corresponding to the masked nodes $\frac{1}{n}\sum_{i \in \widetilde{\mathcal{X}}^C}\|\mathbf{f}_n(i) - \mathbf{f}_{n}^o(i)\|^2$.
In Table \ref{table:results_recon} we evaluate TNNs and MNNs for four different expected sample sizes ($\textrm{E}\{n\} = 100$, $\textrm{E}\{n\} = 200$, $\textrm{E}\{n\} = 300$, and $\textrm{E}\{n\}=400$), for three different masking probabilities ($\textrm{E}\{\widetilde{n}\} = 0.5n$, $\textrm{E}\{\widetilde{n}\} = 0.3n$, and $\textrm{E}\{\widetilde{n}\} = 0.1n$) per each of them (the probability of a node to being masked). As the reader can notice, TNNs are always able to perform better than MNNs, keeping the perfomance stable with the number of samples and, of course, improving with more observations available.
\vspace{-.1cm}
\subsection{Wind Field Forecasting with Recurrent TNNs}
We design a forecasting task on the same wind field data from Section \ref{sec:wind_rec}. In particular, we use daily observation corresponding to the wind field from the 1st of January 2016 to 7 September 2016 to train the model and we use observations from the 1st of January 2017 to 7 September 2017 to test it. We, again,  randomly sample $n$ points to obtain the sampling set $\mathcal{X}$, the cellular sheaf $\tm_n$, and the Sheaf Laplacian $\Delta_n$; at this point, we organize the data corresponding to the sampled point in $\mathcal{X}$ in a sequence $\{\mathbf{F}_{n,t}\}_t$ indexed by time $t$ (daily interval), with each $\mathbf{F}_{n,t} \in \mathbb{R}^{n \times 2}$. As in Section \ref{sec:wind_rec}, we  vectorize $\{\mathbf{F}_{n,t}\}_t$  to obtain $\{\mathbf{f}_{n,t}\}_t$, the input tangent bundle signals, with each   $\mathbf{f}_{n,t}\in \mathbb{R}^{2n}$. We now introduce an hyperparamter $T_f>0$ representing the length of the predictive time window of the model, i.e., given in input a subsequence $\{\mathbf{f}_{n,t}\}_{t=T_s}^{t =T_s+T_f}$ starting at time $T_s$ of length $T_f$ , the model outputs a sequence $\{\mathbf{f}^{o}_{n,t}\}_{t=1}^{t =T_f}$ of length $T_f$  aiming at estimating the next $T_f$ element $\{\mathbf{f}_{n,t}\}_{t=T_s+T_f+1}^{t =T_s+2T_f+1}$ of the input sequence. To do so, we introduce a recurrent version of the proposed DD-TNNs, that, at the best of our knowledge, is also the first recurrent architecture working on cellular sheaves. The building block of the proposed recurrent architecture is a layer made of three components: a tangent bundle filter processing the current sequence element $\mathbf{f}_{n,t}$, a tangent bundle filter processing the current hidden state $\mathbf{z}_{t-1}$, i.e., the output of the layer computed on the previous sequence element, and a pointwise non-linearity. Formally, the layer reads as:
\begin{equation}\label{rtnn}
    \mathbf{z}_{t} = \sigma\Bigg(\sum_{k=1}^{K}h_{k}\big(e^{\Delta_n}\big)^k\mathbf{f}_{n,t} + \sum_{k=1}^{K}w_{k}\big(e^{\Delta_n}\big)^k\mathbf{z}_{t-1}\Bigg),
\end{equation}
with $t = T_s,..., T_s + T_f$, and $\mathbf{z}_{0}=\mathbf{0}$. To obtain the required estimates,  we can set $\{\mathbf{f}^{o}_{n,t}\}_{t=1}^{t =T_f}=\{\mathbf{z}_t\}_{t=1}^{t =T_f}$. This architecture can be used also in a multilayer fashion: in this case, at layer $l$ and at time $t$, the first filter takes $\mathbf{z}_{l-1,t}$ (the current time $t$ hidden state of the previous layer $l-1$) as input, and the second filter takes $\mathbf{z}_{l,t-1}$ (the previous time $t-1$ hidden state of the current layer $l$) as input. Therefore, the resulting $L-$layers architecture is:
\begin{equation}\label{rtnn_ml}
    \mathbf{z}_{l,t} = \sigma\Bigg(\sum_{k=1}^{K}h_{k,l}\big(e^{\Delta_n}\big)^k\mathbf{z}_{l-1,t} + \sum_{k=1}^{K}w_{k,l}\big(e^{\Delta_n}\big)^k\mathbf{z}_{l,t-1}\Bigg),
\end{equation}
with $l=1,...,L$, $t = T_s,..., T_s + T_f$, and $\mathbf{z}_{0,t}=\mathbf{f}_{n,t}$. In this case, to obtain the required estimates,  we can set $\{\mathbf{f}^{o}_{n,t}\}_{t=1}^{t =T_f}=\{\mathbf{z}_{L,t}\}_{t=1}^{t =T_f}$. For the wind field forecasting task, the training set is made of all the possible $m=250-2T_f$ subsequences of lenght $2T_f$ of the 2016 data, we use a 3-layers Recurrent DD-TNN with $K=2$ and $\textrm{Tanh}$ non-linearities, and we train it to minimize the square error
\begin{equation}\label{mse_rec}
\sum_{t =1}^m\sum_{\widetilde{t} =t}^{t+T_f}\|\mathbf{f}_{n,\widetilde{t}} - \mathbf{f}^{o}_{n,\widetilde{t}-t+1}\|_2^2,
\end{equation}
To have a fair comparison, we set up the corresponding recurrent version of MNNs (RMNNs, a recurrent graph neural network) with the same structure, same hyperparameters, same loss but with inputs $\{\mathbf{F}_{n,t}\}_t$. As evaluation metric, we compute the MSE on the 2017 data after training. In Table \ref{table:results_forec} we evaluate RTNNs and RMNNs for four different expected sample sizes ($\textrm{E}\{n\} = 100$, $\textrm{E}\{n\} = 200$, $\textrm{E}\{n\} = 300$, and $\textrm{E}\{n\}=400$), and for three different time window lenghts ($T_f = 20$, $T_f = 50$, and $T_f = 80$) per each of them. . Also in this case, the bundle "awareness" of RTNNs allow to reach significantly better results in all the tested scenarios. Please notice that, in all the presented experiments, TNNs have always less parameters of the corresponding MNNs, due to the different organization/processing of the data in the input layer. Finally, especially in the wind forecasting task, we also found MNNs harder to train.
\vspace{-.1cm}
\section{Conclusions}\label{sec:conlcusion}
In this work we introduced Tangent Bundle Filters and Tangent Bundle Neural Networks (TNNs), novel continuous architectures operating on tangent bundle signals, i.e. manifold vector fields. We made TNNs implementable by discretization in space and time domains, showing that their discrete counterpart is a principled variant of Sheaf Neural Networks. We proved that discretized TNNs asymptotically converge to their continous counterparts, and  we assessed the performance of TNNs on both synthetic and real dat{}a. This work gives a multifaceted contribution: on the methodological side, it is the first work to introduce a signal processing framework for signals defined on tangent bundles of Riemann manifolds via the Connection Laplacian; on the theoretical side, the presented discretization procedure and convergencence result explicitly link the manifold domain with cellular sheaves, formalizing intuitions presented in works like \cite{barbero2022sheafnnconn}. \textcolor{black}{In future work, we will investigate more general classes of cellular sheaves that approximate unions of manifolds (perhaps representing multiple classes) or, more generally, stratified spaces \cite{stolz2020geometric,nanda2020local}. We believe our perspective on tangent bundle neural networks could shed further light on challenging problems in graph neural networks such as heterophily \cite{bodnar2022sheafdiff}, over-squashing \cite{digiovanni2023oversquashing}, or transferability \cite{ruiz2020transf, levie2021transferability, topping2022understanding}. Finally,we plan to tackle more sophisticated tasks such as robot coordination with our proposed architectures.}
\vspace{-.1cm}
\appendix
\section{Appendix}
\vspace{-.1cm}
\subsection{Proof of Proposition 1} \label{ap:prop1}
\begin{proof}[Proof of Proposition  \ref{prop:parametric-filter}]
By definition of frequency representation in \eqref{freq_resp} we have:
\begin{align}
\label{g_freq}
    &\big[\hat{G}\big]_i = \langle \G, \Phii \rangle = \int_{\M}\langle \G(x), \Phii(x) \rangle_{\tmx} \textrm{d}\mu(x)
\end{align}
Injecting \eqref{param_conv} in \eqref{g_freq}, we get:
\begin{align}
\label{g_freq_f}
    &\big[\hat{G}\big]_i = \langle \int_0^{\infty}\th(t)e^{t\Delta}\F(x)\textrm{d}t, \Phii \rangle 
\end{align}
For the linearity of integrals and inner products, we can write:
\begin{align}
\label{g_freq_int_out}
    &\big[\hat{G}\big]_i = \int_0^{\infty}\th(t)\langle e^{t\Delta}\F(x), \Phii \rangle \textrm{d}t
\end{align}
Finally, exploiting first the self-adjointness of $\Delta$ and then the eigenvector fields definition in \eqref{eigen}, we can write:
\begin{align}
\label{g_freq_final}
    \big[\hat{G}\big]_i &= \int_0^{\infty}\th(t)\langle e^{t\Delta}\F(x), \Phii \rangle \textrm{d}t \nonumber\\
    &=\int_0^{\infty}\th(t)\langle \F(x), e^{t\Delta}\Phii \rangle \textrm{d}t \nonumber \\  
    &=\int_0^{\infty}\th(t)\langle \F(x), e^{-t\lambda_i}\Phii \rangle \textrm{d}t \nonumber \\
    &=\int_0^{\infty}\th(t)e^{-t\lambda_i}\langle \F(x), \Phii \rangle \textrm{d}t,
\end{align}
which concludes the proof.
\end{proof}
\vspace{-.1cm}
\subsection{Consistency of Tangent Bundle Convolution}\label{ap:consistency}
The tangent bundle convolution in Definition 1 is a generalization of the manifold convolution from \cite{wang2022convolution} and of the standard convolution on the real line. For the former case, the result is trivial, because manifold convolution is just the tangent bundle convolution in the case of scalar vector fields. For the latter case, consider the differential equation:
\begin{equation} \label{eqn:wave}
   \frac{\partial u(x,t)}{\partial t} = \frac{\partial}{\partial x} u(x,t) \text{,}
\end{equation}
which is a one-sided wave equation, thus it is not the exact
analogous of the diffusion equation in \eqref{diff_eq} for which we would require the second derivative to be used in the right side of \eqref{eqn:wave}. However, the important observation to make here is that the exponential of the derivative operator is a time shift operator so that we can write $u(x,t) = e^{-t\partial/\partial x}f(x) = f(x-t)$, where $f(x) = u(x,0)$; this is a known result and it holds because the operator $e^{-t\partial/\partial x}$ applied to $f$ evaluated in $x$ is equivalent to the Taylor Expansion of $f(x-t)$ around $x$. Another way of proving it is noticing that both $e^{t\partial/\partial x}f(x)$ and $f(x-t)$ are  solutions of \eqref{eqn:wave}. It then follows that Definition 1 particularized to \eqref{eqn:wave} yields the convolution definition:
\begin{equation}\label{eqn:conv-1d-1}
    g(x) = \int_{0}^\infty \tdh(t) e^{-t\partial/\partial x}f(x)\, \text{d}t.
         = \int_{0}^\infty \tdh(t) f(x-t) \,\text{d}t,
\end{equation}
that is the standard definition of time convolutions. 
\subsection{Sheaf Laplacian Algorithms}\label{ap:algos}
\begin{algorithm}[H]
   \caption{: Local PCA \cite{singer2012vdm}}    \hspace*{\algorithmicindent} \textbf{Inputs}: \\
    \hspace*{\algorithmicindent} \quad $\mathcal{X} \subset \mathbb{R}^p$: Manifold samples. \vspace{.05cm}\\
    \hspace*{\algorithmicindent} \quad $\epsilon_{\textrm{PCA}} > 0$: Scale parameter \vspace{.05cm}\\
    \hspace*{\algorithmicindent} \quad $K(\cdot) \in C^2(\mathbb{R})$: positive monotonic  supported on $[0, 1]$ \vspace{.05cm}\\
    \hspace*{\algorithmicindent} \textbf{Outputs}: \vspace{.05cm}\\
    \hspace*{\algorithmicindent} \quad $\{\mathbf{O}_i\}_{x_i \in \mathcal{X}}$: Orthogonal transformation
    \begin{algorithmic}[1]
        \Function{LOCAL PCA\,} {\textbf{Inputs}}
                \For{$x_i \in \mathcal{X}$}
                    \State Compute $\mathcal{N}^{\textrm{P}}_i = \{x_j: 0 < \|x_i - x_j\|_{\mathbb{R}^p}\leq \sqrt{\epsilon_{\textrm{PCA}}}\}$\vspace{.05cm}
                    \State Compute $\mathbf{X}_i = [\dots, x_i - x_j, \dots]$, $x_j \in \mathcal{N}^{\textrm{P}}_i$ \vspace{.05cm}
                    \State Compute $\mathbf{C}_i$ with $[\mathbf{C}_i]_{j,j}=\sqrt{K\big(\frac{||x_i -x_j||}{\sqrt{\epsilon_{\textrm{PCA}}}}\big)}$ \vspace{.05cm}
                    \State Compute $\mathbf{B}_i=\mathbf{X}_i\mathbf{C}_i$ and $\mathbf{R}_i=\mathbf{B}_i^T\mathbf{B}_i$  \vspace{.05cm}
                    \State Eigendecompose $\mathbf{R}_i=\mathbf{M}_i\Sigma_i\mathbf{J}_i^T$ \vspace{.05cm}
                \EndFor
        \State \textbf{end}
        \State Compute $\hd$ as in \cite{singer2012vdm} \vspace{.05cm}
        \State Set $\mathbf{O}_i$ to be the first $\hd$ columns of $\mathbf{M}_i$ \vspace{.05cm}\\
        \Return: \\
        \hspace*{\algorithmicindent} \quad $\{\mathbf{O}_i\}_{x_i \in \mathcal{X}}$ 
       \EndFunction
\end{algorithmic}
\end{algorithm}\label{algo:locpca}

\begin{algorithm}[H]
   \caption{: Sheaf Laplacian \cite{singer2012vdm}}   \hspace*{\algorithmicindent} \textbf{Inputs}: \\
    \hspace*{\algorithmicindent} \quad $\mathcal{X} \subset \mathbb{R}^p$: Manifold samples.  \vspace{.05cm}\\
    \hspace*{\algorithmicindent} \quad $\epsilon > 0$: Scale parameter for geometric graph \vspace{.05cm}\\
    \hspace*{\algorithmicindent} \quad $\epsilon_{\textrm{PCA}} > 0$: Scale parameter for local PCA \vspace{.05cm}\\
    \hspace*{\algorithmicindent} \quad $K(\cdot) \in C^2(\mathbb{R})$: positive monotonic  supported on $[0, 1]$\\
    \hspace*{\algorithmicindent} \textbf{Outputs}: \vspace{.05cm}\\
    \hspace*{\algorithmicindent} \quad $\{\mathbf{O}_i\}_{x_i \in \mathcal{X}}$: Orthogonal transformation \vspace{.05cm}\\
    \hspace*{\algorithmicindent} \quad $\Delta_n$: Normalized Sheaf Laplacian
    \begin{algorithmic}[1]
        \Function{SHEAF LAPLACIAN\,} {\textbf{Inputs}}
        \State Compute graph $\mathcal{M}_n$ with edge weights as in \eqref{graph_weights}
                \For{$x_i \in \mathcal{X}$}
                    \State Compute $\{\mathbf{O}_i\}$ with Algorithm 2
                \EndFor
                \State \textbf{end}
                \For{$x_i \in \mathcal{X}$}
                     \For{$x_j \in \mathcal{N}^{\textrm{P}}_i$}
                        \State Compute $\widetilde{\mathbf{O}}_{i,j}=\mathbf{O}_i^T\mathbf{O}_j\svdeq \mathbf{M}_{i,j}\Sigma_i\mathbf{V}_{i,j}^T$ \vspace{.05cm}
                        \State Compute $\mathbf{O}_{i,j}=\mathbf{M}_{i,j}\mathbf{V}_{i,j}^T$ \vspace{.05cm}
                        \State Compute $\textrm{deg}(i)=\sum_j w_{i,j}$ \vspace{.05cm}
                        \State Compute $\textrm{ndeg}(i)=\sum_j \frac{w_{i,j}}{\textrm{deg}(i)\textrm{deg}(j)}$ \vspace{.05cm}
                        \State Compute $\widetilde{\mathbf{D}}_i=\textrm{deg}(i)\mathbf{I}_{\hd}$ and  $\mathbf{D}_i=\textrm{ndeg}(i)\mathbf{I}_{\hd}$ \vspace{.05cm}
                        \State Compute $\mathbf{S}_{i,j}=w_{i,j}\widetilde{\mathbf{D}}^{-1}_i\mathbf{O}_{i,j}\widetilde{\mathbf{D}}^{-1}_i$ \vspace{.05cm}
                    \EndFor
                    \State \textbf{end}
                \EndFor
        \State \textbf{end}
        \State Compute block matrix $\mathbf{S}$ with $\mathbf{S}_{i,j}$s as blocks \vspace{.05cm}
        \State Compute block diagonal matrix $\mathbf{D}$ with $\mathbf{D}_{i,i}$ as blocks 
        \State Compute $\Delta_n = \epsilon^{-1}\big(\D^{-1}\S - \mathbf{I}\big)$ \vspace{.05cm}\\
        \Return: \\
        \hspace*{\algorithmicindent} \quad $\{\mathbf{O}_i\}_{x_i \in \mathcal{X}}$, $\Delta_n$
       \EndFunction
\end{algorithmic}
\end{algorithm}\label{algo:sheaflap}

\bibliographystyle{IEEEtran}
\bibliography{refs}
%\end{document}
\clearpage
\setcounter{page}{1}
\twocolumn[%
%  Title and authors
   \begin{center}
     {\huge Supplemental Materials }\\
     \end{center}\vspace{0.5cm}
]
% \centering
% \textbf{\large Supplemental Materials}
\vspace{1cm}
% \section*{Supplementary Material}
\setcounter{subsection}{0}
\subsection{Proof of Theorem 1}
\noindent\textbf{\textit{Theorem 1.}} Let $\mathcal{X}=\{x_1,\dots,x_n\}\subset \mathbb{R}^p$ be a set of $n$ i.i.d. sampled points from measure $\mu$ over $\M \subset \mathbb{R}^p$ and $\F$ a tangent bundle signal. Let $\tm_n$ be a cellular sheaf built from $\mathcal{X}$ as explained above, with $\epsilon = n^{-2/(\hd+4)}$. Let  $\bPsi_u\big(\mathcal{H}, \cdot, \cdot \big)$ be the $u-th$ output of a neural network with $L$ layers parameterized
by the operator $\Delta$ of  $\tm$  or by the discrete operator $\Delta_n$ of  $\tm_n$. If:
\begin{itemize}
    \item $\Delta$ has an accumulation point at $-\infty$;
    \item  the filters in $\mathcal{H}$ are $\alpha-$FDT filters
    \item the frequency response of filters in $\mathcal{H}$ are non-amplifying Lipschitz continuous;
    \item $\widetilde{\sigma}$ from Definition 4 is point-wise normalized Lipschitz continuous,
\end{itemize}
then it holds for each $u = 1, 2, \dots, F_L$ that:
\begin{equation}
\label{convergence}
\lim_{n \rightarrow \infty} ||\bPsi_u\big(\mathcal{H}, \Delta_n, \samp_n^{\mathcal{X}}\F\big) - \samp_n^{\mathcal{X}}\bPsi_u\big(\mathcal{H}, \Delta,\F\big)||_{\tm_n} = 0,
\end{equation}
with the limit taken in probability.

\noindent\textbf{\textit{Proof.}}  We define an inner product for sheaf signals $\mathbf{f}$ and $\mathbf{u}$ on a general cellular sheaf $\tm_n$ as
\begin{align}
\label{emp_metr_sheaf}
\langle \mathbf{f}, \mathbf{u} \rangle_{\tm_n}   & = \frac{1}{n}\sum_{i = 1}^n  \f_{n}(x_i) \dotp \mathbf{u}_{n}(x_i) ,
\end{align}
and the induced norm $||\mathbf{f}||^2_{\tm_n} = \langle \mathbf{f}, \mathbf{f} \rangle_{\tm_n}$.
Under the assumption that the points in $\mathcal{X}$ are sampled i.i.d. from the uniform probability measure $\mu$ given by the induced metric on $\M$ and that $\tm_n$ is built as in Section 5, the inner product in \eqref{emp_metr_sheaf} is equivalent to the following inner product for tangent bundle signals $\F$ and $\U$:
\begin{align}
\label{emp_metr}
\langle \F, \U \rangle_{\tm_n}   &= \int_{\M} \langle \F(x), \U(x) \rangle_{\tmx} \textrm{d}\mu_n(x) \nonumber \\
&= \frac{1}{n}\sum_{i = 1}^n \langle \F(x_i), \mathbf{U}(x_i) \rangle_{\mathcal{T}_{x_i}\M},
\end{align}
and the induced norm $||\F||^2_{\tm_n} = \langle \F, \F \rangle_{\tm_n}$, where $\mu_n = \frac{1}{n}\sum_{i=1}^n\delta_{x_i}$ is the empirical measure corresponding to $\mu$. Indeed, from \eqref{riemann_metric} and due to the orthogonality of the transformations $\Oi$ in Section 5, \eqref{emp_metr} can be  rewritten as
\begin{align}
\label{emp_metr_versions}
\langle \F, \U \rangle_{\tm_n}&=\frac{1}{n}\sum_{i = 1}^n \langle \F(x_i), \mathbf{U}(x_i) \rangle_{\mathcal{T}_{x_i}\M} \nonumber \\
&= \frac{1}{n}\sum_{i = 1}^n  d\i\F(x_i) \dotp   d\i\mathbf{U}(x_i) \nonumber \\
&= \frac{1}{n}\sum_{i = 1}^n \Oi^T d\i\F(x_i) \dotp  \Oi^T d\i\mathbf{U}(x_i) \nonumber \\
&= \frac{1}{n}\sum_{i = 1}^n \f_{n}(x_i) \dotp \mathbf{u}_{n}(x_i)  = \langle \mathbf{f}_n, \mathbf{u}_n \rangle_{\tm_n}
\end{align}
where $\f_n = \samp_n^{\mathcal{X}}\F$ and $\mathbf{u}_n = \samp_n^{\mathcal{X}}\U$, respectively. We denote with $\ltmn$ the Hilbert Space of finite energy tangent bundle signals w.r.t. the empirical measure $\mu_n$ (or, equivalently, the Hilbert Space of finite energy sheaf signals w.r.t the norm induced by \eqref{emp_metr_sheaf}). In the following, we will denote the norm $||\cdot||_{\tm_n}$ with $||\cdot||$  when there is no risk of confusion. In \cite{singer2013spectral}, the spectral convergence of the constructed Sheaf Laplacian in \eqref{sheaf_laplacian} based on the discretized manifold to the Connection Laplacian of the underlying manifold has been proved, and we will exploit that result for proving the following proposition.

\noindent\textbf{\textit{Proposition 3. (Consequence of Theorem 6.3 \cite{singer2013spectral})}}
 Let $\mathcal{X}=\{x_1,\dots,x_n\}\subset \mathbb{R}^p$ be a set of $n$ i.i.d. sampled points from measure $\mu$ over $\M \subset \mathbb{R}^p$. Let $\tm_n$ be a cellular sheaf built from $\mathcal{X}$ as explained in Section 5, with $\epsilon = n^{-2/(\hd+4)}$. Let $\Delta_n$ be the Sheaf Laplacian of $\tm_n$ and $\Delta$ be the Connection Laplacian operator of $\M$. Let $\lambda_{i}^n$ be the $i$-th eigenvalue of $\Delta_n$ and $\Phii^n$ the corresponding eigenvector. Let $\lambda_i$ be the $i$-th eigenvalue of $\Delta$ and $\Phii$  the corresponding eigenvector field of $\Delta$, respectively. Then it holds:
\begin{equation}
\label{eqn:convergence_spectrum}
    \lim_{n\rightarrow \infty } \lambda_i^n = \lambda_i, \quad\lim_{n\rightarrow \infty} \|\Phii^{n} -  \samp_n^{\mathcal{X}}\Phii\|_{\tm_n}=0,
\end{equation}
where the limits are taken in probability.

\noindent\textbf{\textit{Proof.}} These proposition is a consequence of Theorem 6.3 in \cite{chung1997spectral}. Indeed, we  rely on the operator introduced in Definition 6.1 in \cite{singer2013spectral} with $\alpha=1$ and $h_n = n^{-2/(\hd+4)}$ (our $\epsilon$), here denoted as $\Gamma:\ltm \rightarrow \ltm$, and on the operator $\widetilde{\Gamma} = \epsilon^{-1}\big(\Gamma - \textrm{id}\big)$, where $\textrm{id}$ is the identity mapping. It is straightforward to check that:
\begin{equation}
    \label{omega_delta_equiv}
    \widetilde{\Gamma}\F(x_j) =  d\i^{-1}\Oj\big(\Delta_n\samp_n^{\mathcal{X}}\F\big)(x_j),
\end{equation}
for $j = 1,\dots,n$. We now show that the eigenvectors sampled on $\mathcal{X}$ and eigenvalues of $\widetilde{\Gamma}$  correspond to the eigenvectors and eigenvalues of $\Delta_n$. Let us denote the the $i-th$ eigenvector and eigenvalue of $\widetilde{\Gamma}$ with $\widetilde{\boldsymbol{\phi}}^n_{i}$ and $-\widetilde{\lambda}^n_{i}$, respectively. We have:
\begin{align}
    \label{eigen_conv}
    \tGamma\eigGammai(x_j) = -\eivGammai\eigGammai(x_j)=d\i^{-1}\Oj\big(\Delta_n\samp_n^{\mathcal{X}}\eigGammai\big)(x_j)
\end{align}
If we apply the mapping $i$ to the last two equalities of \eqref{eigen_conv} and we exploit the orthoghonality of $\Oj$, we obtain:
\begin{align}
\label{eig_gamma_delt}
    \big(\Delta_n\samp_n^{\mathcal{X}}\eigGammai\big)(x_j) = -\eivGammai\Oj^T d\i\eigGammai &= -\eivGammai\samp_n^{\mathcal{X}}\eigGammai(x_j)
\end{align}
where the second equality applies the definition of $\samp_n^{\mathcal{X}}$ in \eqref{sampler}. Therefore, we have:
\begin{align}
\label{eig_gamma_delta}
    \lambda_i^n = \eivGammai, \quad \Phii^n(x_j) = \samp_n^{\mathcal{X}}\eigGammai(x_j),
\end{align}
$j= 1,\dots,n$. At this point, we can recall Theorem 6.3 in \cite{singer2013spectral}, that, in the setting of our Theorem 1, states:
\begin{equation}
    \label{spect_VDM}
    \lim_{n\rightarrow \infty } \widetilde{\lambda}_i^n = \lambda_i, \quad\lim_{n\rightarrow \infty} \|\eigGammai -  \Phii\|_{\tm}=0,
\end{equation}
with the limit taken in probability, $j= 1,\dots,n$. 
Injecting the empirical measure in \eqref{spect_VDM} and exploiting the results in \eqref{emp_metr_versions} and \eqref{eig_gamma_delta}, we obtain:
\begin{align}
\label{norm_tm}
    &\|\eigGammai-  \Phii\|_{\tm_n} = \|\Phii^n- \samp_n^{\mathcal{X}}\Phii\|_{\tm_n}
\end{align}
The results in \eqref{spect_VDM} and \eqref{norm_tm}
combined with the a.s. convergence of the empirical measure $\mu_n$ to the measure $\mu$ conclude the proof.

For the sake of clarity, in the following we will drop the dependence on the NNs output index $u$; from the definitions of TNNs in \eqref{tnn_layer} and D-TNNS in \eqref{dt_tnn_layer}, we can thus write:
 \begin{align}
    \nonumber  \|\bPsi\big(\mathcal{H}, \Delta_n, \samp_n^{\mathcal{X}}\F\big) - \samp_n^{\mathcal{X}}\bPsi\big(\mathcal{H}, \Delta,\F\big)\|&= \left\| \bbx_{n,L}-\samp_n^{\mathcal{X}}\F_L \right\|.
 \end{align}
 Further explicating the layers definitions, at layer $l$ we have: 
 \begin{align}
   \nonumber  &\left\| \bbx_{n,l}- \samp_n^{\mathcal{X}} \F_l \right\|\\
     &=\left\| \sigma\left(\sum_{q=1}^{F_{l-1}} \bbh_l^{q}(\Delta_n) \bbx_{n,l-1}^q \right) -\samp_n^{\mathcal{X}} \sigma\left(\sum_{q=1}^{F_{l-1}} \bbh_l^{q}(\Delta) \F_{l-1}^q\right) \right\|
 \end{align}
 with $\bbx_{n,0}^q=\samp_n^{\mathcal{X}} \F^q$ for $q=1,\dots,F_0$. Exploiting the normalized point-wise Lipschitz continuity of the non-linearities and the linearity of the sampling operator $\samp_n^{\mathcal{X}}$, we have:
  \begin{align}
  \label{proof_1}
    \| \bbx_{n,l} - \samp_n^{\mathcal{X}} \F_l  \| &\leq \Bigg\|  \sum_{q=1}^{F_{l-1}} \bbh_l^{q}(\Delta_n) \bbx_{n,l-1}^q    \nonumber \\
    &- \samp_n^{\mathcal{X}} \sum_{q=1}^{F_{l-1}} \bbh_l^{q}(\Delta)  \F_{l-1}^q\Bigg\| \nonumber\\
    & \leq \sum_{q=1}^{F_{l-1}} \left\|    \bbh_l^{q}(\Delta_n) \bbx_{n,l-1}^q    - \samp_n^{\mathcal{X}}   \bbh_l^{q}(\Delta)  \F_{l-1}^q\right\|
 \end{align}
 The difference term in the last LHS of \eqref{proof_1} can be further decomposed for every $q=1,\dots,F_{l-1}$ as
\begin{align}\label{proof_2}
   \nonumber   \|    \bbh_l^{q}(\Delta_n) & \bbx_{n,l-1}^q    - \samp_n^{\mathcal{X}}   \bbh_l^{q}(\Delta)  \F_{l-1}^q \| 
   \\ \nonumber&\leq \|
\bbh_l^{q}(\Delta_n) \bbx_{n,l-1}^q  - \bbh_l^{q}(\Delta_n) \samp_n^{\mathcal{X}} \F_{l-1}^q \\ &\qquad +\bbh_l^{q}(\Delta_n) \samp_n^{\mathcal{X}} \F_{l-1}^q  - \samp_n^{\mathcal{X}}   \bbh_l^{q}(\Delta)  \F_{l-1}^q
    \|\nonumber \\ \nonumber
   & \leq \left\|
    \bbh_l^{q}(\Delta_n) \bbx_{n,l-1}^q  - \bbh_l^{q}(\Delta_n) \samp_n^{\mathcal{X}} \F_{l-1}^q
    \right\|
  \\ &\qquad +
    \left\|
    \bbh_l^{q}(\Delta_n) \samp_n^{\mathcal{X}} \F_{l-1}^q  - \samp_n^{\mathcal{X}}   \bbh_l^{q}(\Delta)  \F_{l-1}^q
    \right\|
\end{align}
The first term of the last inequality in \eqref{proof_2} can be bounded as $\| \bbx_{n,l-1}^q - \samp_n^{\mathcal{X}}\F_{l-1}^q\|$ with the initial condition $\|\bbx_{n,0}^q - \samp_n^{\mathcal{X}} \F_0^q\|=0$ for $q = 1,\dots,F_0$. Denoting the second term with $D_{l-1}^n$,  and iterating the bounds derived above through layers and features, we obtain:
\begin{align}
 \nonumber \|\bPsi(\mathcal{H},\Delta_n,\samp_n^{\mathcal{X}} \F) - \samp_n^{\mathcal{X}} \bPsi(\mathcal{H},\Delta,\F)\|
 \leq
 \sum_{l=0}^L \prod\limits_{l'=l}^L F_{l'} D_l^n.
 \end{align}
 Therefore, we can focus on each difference term $D_l^n$ and omit the feature and layer indices to simplify the notations. 
 We can write the convolution operation in the spectral domain as
 \begin{align}
    & \nonumber\|\bbh(\Delta_n)\samp_n^{\mathcal{X}} \F - \samp_n^{\mathcal{X}}\bbh(\Delta) \F\| \nonumber \\
   & = \Bigg\| \sum_{i=1}^n \hat{h}(\lambda_i^n) \langle \samp_n^{\mathcal{X}}\F,\Phii^n \rangle_{\tm_n}\Phii^n \nonumber \\
    & \qquad \qquad \qquad \quad- \sum_{i=1}^\infty \hat{h}(\lambda_i)\langle \F,\Phii\rangle_{\tm} \samp_n^{\mathcal{X}} \Phii  \Bigg\| 
   %   \\ 
   %   &\nonumber \leq  \Bigg\| \sum_{i=1}^M \hat{h}(\lambda_i^n) \langle \samp_n^{\mathcal{X}}\F,\Phii^n \rangle_{\tm_n}\Phii^n \nonumber \\
   %   & \qquad \quad - \sum_{i=1}^M \hat{h}(\lambda_i) \langle \samp_n^{\mathcal{X}}\F,\Phii^n \rangle_{\tm_n}\Phii^n\Bigg\| \nonumber \\
   %   & \quad +\Bigg\| \sum_{i=1}^M \hat{h}(\lambda_i) \langle \samp_n^{\mathcal{X}} \F,\Phii^n \rangle_{\tm_n} \Phii^n \nonumber\\
   %   & \qquad \quad - \sum_{i=1}^M \hat{h}(\lambda_i) \langle \F,\Phii \rangle_{\ccalM} \samp_n^{\mathcal{X}} \Phii \Bigg\|,
   \label{eqn:conv-1}
 \end{align}
We can separately bound the difference in \eqref{eqn:conv-1}; indeed, due to the fact that the filter involved is $\alpha-$FDT by assumption, we can decompose its frequency response as follows:
\begin{align}
\label{eqn:h0-gamma}& h^{(0)}(\lambda) = \left\{ 
\begin{array}{cc} 
                \hat{h}(\lambda)-\sum\limits_{l\in\ccalK_m}\hat{h}(C_l)  &  \lambda\in[\Lambda_k(\gamma)]_{k\in\ccalK_s} \\
                0& \text{otherwise}  \\
                \end{array} \right.  \\
\label{eqn:hl-gamma}& h^{(l)}(\lambda) = \left\{ 
\begin{array}{cc} 
                \hat{h}(C_l) &  \lambda\in[\Lambda_k(\gamma)]_{k\in\ccalK_s} \\
                \hat{h}(\lambda) & 
                \lambda\in\Lambda_l(\gamma)\\
                0 &
                \text{otherwise}  \\
                \end{array} \right.             
\end{align}
where now $\hat{h}(\lambda)=h^{(0)}(\lambda)+\sum_{l\in\ccalK_m}h^{(l)}(\lambda)$ with $\ccalK_s$ defined as the group index set of singletons and $\ccalK_m$ the set of partitions that contain multiple eigenvalues. With the triangle inequality and $n > N_\alpha=\max_{i}\{\lambda_i\in[\Lambda_k(\alpha)]_{k\in\ccalK_s}\}$, we start by analyzing the output difference of $h^{(0)}(\lambda)$ as

\begin{align}
    & \nonumber \Bigg\| \sum_{i=1}^{N_\alpha} {h}^{(0)}(\lambda_i^n) \langle \samp_n^{\mathcal{X}}\F,\Phii^n \rangle_{\tm_n}\Phii^n \nonumber \\
    &- \sum_{i=1}^{N_\alpha} {h}^{(0)}(\lambda_i)\langle \F,\Phii\rangle_{\tm} \samp_n^{\mathcal{X}} \Phii   \Bigg\| \nonumber
     \\ 
     &\nonumber \leq  \left\| \sum_{i=1}^{N_\alpha} \left({h}^{(0)}(\lambda_i^n)- {h}^{(0)}(\lambda_i) \right) \langle \samp_n^{\mathcal{X}}\F,\Phii^n \rangle_{\tm_n}\Phii^n \right\| \\
     &  +\left\| \sum_{i=1}^{N_\alpha} {h}^{(0)}(\lambda_i)\left( \langle \samp_n^{\mathcal{X}}\F,\Phii^n \rangle_{\tm_n}\Phii^n - \langle \F,\Phii\rangle_{\tm} \samp_n^{\mathcal{X}} \Phii  \right)  \right\|.\label{eqn:conv-2}
 \end{align}

The first term of the last bound in \eqref{eqn:conv-2} can be further bounded by exploiting the $C$-Lipschitz continuity of the frequency response function and the convergence in probability stated in \eqref{eqn:convergence_spectrum}: indeed, we can claim that for each eigenvalue $\lambda_i \leq \lambda_{N_\alpha}$, for all $\epsilon_i>0$ and all $\delta_i>0$, there exists some $N_i$ such that for all $n>N_i$, we have:
\begin{gather}
 \label{eqn:eigenvalue}   \mathbb{P}(|\lambda_i^n-\lambda_i|\leq \epsilon_i)\geq 1-\delta_i,
 \end{gather}
Letting $\epsilon_i < \epsilon$ with $\epsilon > 0$, with probability at least $\prod_{i=1}^M(1-\delta_i) := 1-\delta$, we obtain:
\begin{align}
   &\left\| \sum_{i=1}^{N_\alpha} ({h}^{(0)}(\lambda_i^n) - {h}^{(0)}(\lambda_i))\langle \samp_n^{\mathcal{X}} \F,\Phii^n \rangle_{\tm_n} \Phii^n  \right\| \nonumber
   \\
   &\qquad \leq \sum_{i=1}^{N_\alpha} |{h}^{(0)}(\lambda_i^n)-{h}^{(0)}(\lambda_i)| |\langle \samp_n^{\mathcal{X}} \F,\Phii^n \rangle_{\tm_n}| \|\Phii^n\| \nonumber\\
   &\qquad \leq \sum_{i=1}^{N_\alpha}  C |\lambda_i^n-\lambda_i| \|\samp_n^{\mathcal{X}} \F\| \|\Phii^n \|^2\leq N_s C\epsilon,
\end{align} 
for all $n>\max\{\max_i N_i, N_\alpha \}:= N$, where the first inequality is obtained applying the triangle inequality, the second inequality exploits the $C-$Lipschitz continuity of the frequency response, and the last inequality exploits \eqref{eqn:eigenvalue}.
The second term of the last bound in \eqref{eqn:conv-1} can be bounded eploiting the convergence of eigenvectors in \eqref{eqn:convergence_spectrum}. We start with
\begin{align}\label{eqn:term1}
   &\nonumber \left\|\sum_{i=1}^{N_\alpha}{h}^{(0)}(\lambda_i)( \langle \samp_n^{\mathcal{X}} \F,\Phii^n \rangle_{\tm_n}\Phii^n -  \langle f,\Phii \rangle_{\tm} \samp_n^{\mathcal{X}} \Phii )\right\|\\
   &\leq \nonumber \Bigg\|  \sum_{i=1}^{N_\alpha} {h}^{(0)}(\lambda_i)\Big(\langle \samp_n^{\mathcal{X}} \F,\Phii^n\rangle_{\tm_n}\Phii^n  \nonumber \\
   &- \langle \samp_n^{\mathcal{X}} \F,\Phii^n \rangle_{\tm_n} \samp_n^{\mathcal{X}}\Phii\Big)\Bigg\| \nonumber \\
   &+ \Bigg\| \sum_{i=1}^{N_\alpha} {h}^{(0)}(\lambda_i)\Big(\langle \samp_n^{\mathcal{X}} \F,\Phii^n\rangle_{\tm_n} \samp_n^{\mathcal{X}}\Phii \nonumber \\
   &-\langle \F,\Phii\rangle_\tm \samp_n^{\mathcal{X}}\Phii \Big) \Bigg\|
\end{align}
From the convergence in probability stated in \eqref{eqn:convergence_spectrum}, we can claim that for some fixed eigenvector field $\Phii$,  for all $\epsilon_i>0$ and all $\delta_i>0$, there exists some $N_i$ such that for all $n>N_i$, we have
\begin{gather}
 \label{eqn:eigenfunction}    \mathbb{P}(\|\Phii^n - \samp_n^{\mathcal{X}}\Phii\|\leq \epsilon_i)\geq 1-\delta_i.
 \end{gather}
 Therefore, letting $\epsilon_i < \epsilon$ with $\epsilon > 0$, with probability at least $\prod_{i=1}^M(1-\delta_i) := 1-\delta$, for all $n>\max\{ \max_i N_i, N_\alpha\} := N$, the first term in \eqref{eqn:term1} can be bounded as
\begin{align}
&\nonumber \Bigg\| \sum_{i=1}^{N_\alpha} {h}^{(0)}(\lambda_i) \Big(\langle \samp_n^{\mathcal{X}} f,\Phii^n\rangle_{\tm_n}\Phii^n  \nonumber \\
&\qquad \qquad \qquad- \langle \samp_n^{\mathcal{X}} \F,\Phii^n \rangle_{\tm_n} \samp_n^{\mathcal{X}}\Phii\Big)\Bigg\|\\
&\qquad \qquad \qquad \leq \sum_{i=1}^{N_\alpha} \|\samp_n^{\mathcal{X}} \F\|\|\Phii^n - \samp_n^{\mathcal{X}}\Phii\|\leq N_s \epsilon,
\end{align}
considering the boundedness of frequency response function.
The second term in \eqref{eqn:term1} can be written as
\begin{align}
  \nonumber &\left\| \sum_{i=1}^{N_\alpha} {h}^{(0)}(\lambda_i^n) \left(\langle \samp_n^{\mathcal{X}} \F,\Phii^n\rangle_{\tm_n} \samp_n^{\mathcal{X}}\Phii -\langle \F,\Phii\rangle_\ccalM \samp_n^{\mathcal{X}}\Phii \right) \right\| \\
   &\leq \sum_{i=1}^{N_\alpha}|{h}^{(0)}(\lambda_i^n)| \left|\langle \samp_n^{\mathcal{X}} \F,\Phii^n\rangle_{\tm_n}  -\langle \F,\Phii\rangle_\ccalM\right|\|\samp_n^{\mathcal{X}}\Phii\|.
\end{align}
Because $\{x_1, x_2,\cdots,x_n\}$ is a set of uniform sampled points from $\ccalM$, based on Proposition 11 in \cite{von2008consistency}, we can claim that 
\begin{equation}
    \lim_{n\to \infty} \mathbb{P}\left(\left|\langle \samp_n^{\mathcal{X}} \F,\Phii^n\rangle_{\tm_n}  -\langle \F,\Phii\rangle_\tm\right|\leq\epsilon \right)\geq 1-\delta,
\end{equation}
for all $\epsilon>0$ and $\delta>0$. Consider the boundedness of frequency response $|{h}^{(0)}(\lambda)|\leq 1$ and the bounded energy of $\|\samp_n^{\mathcal{X}}\Phii\|$, we have for all $\epsilon>0$ and $\delta>0$:
\begin{align}
\lim_{n\to \infty}\mathbb{P}\Bigg(\Bigg\| & \sum_{i=1}^{N_\alpha}|{h}^{(0)}(\lambda_i^n) |\bigg(\langle \samp_n^{\mathcal{X}} \F,\Phii^n\rangle_{\tm_n}  \nonumber \\
 &-\langle \F,\Phii\rangle_\tm\bigg)\samp_n^{\mathcal{X}}\Phii  \Bigg\|\leq N_s \epsilon\Bigg) \geq 1-\delta.
\end{align}

Combining the above results, we can bound the output difference of $h^{(0)}(\lambda)$. Then we need to analyze the output difference of $h^{(l)}(\lambda)$ and bound this as
\begin{align}
    \nonumber &\left\| \samp_n^{\mathcal{X}}\bbh^{(l)}(\Delta) \F -\bbh^{(l)} (\Delta_n)\samp_n^{\mathcal{X}} \F \right\| 
    \\& \leq \left\| (\hat{h}(C_l)+\delta) \samp_n^{\mathcal{X}} \F - (\hat{h}(C_l)-\delta)\samp_n^{\mathcal{X}} \F \right\| \leq 2\delta\|\samp_n^{\mathcal{X}} \F\|,
\end{align}
where $\bbh^{(l)}(\Delta)$ and $\bbh^{(l)}(\Delta_n)$ are filters with filter function $h^{(l)}(\lambda)$ on the connection Laplacian $\Delta$ and Sheaf Laplacian $\Delta_n$ respectively.
Combining the filter functions, we can write
\begin{align}
   \nonumber &\|\samp_n^{\mathcal{X}} \bbh(\Delta)\F-\bbh(\Delta_n)\samp_n^{\mathcal{X}} \F\|\\\nonumber &=
    \Bigg\|\samp_n^{\mathcal{X}}\bbh^{(0)}(\Delta)\F +\samp_n^{\mathcal{X}}\sum_{l\in\ccalK_m}\bbh^{(l)}(\Delta)\F -\\& \qquad \qquad \qquad \bbh^{(0)}(\Delta_n)\samp_n^{\mathcal{X}} \F - \sum_{l\in\ccalK_m} \bbh^{(l)}(\Delta_n)\samp_n^{\mathcal{X}} \F  \Bigg\|\\
    &\nonumber \leq \|\samp_n^{\mathcal{X}} \bbh^{(0)}(\Delta)\F-\bbh^{(0)}(\Delta_n)\samp_n^{\mathcal{X}} \F\|+\\
    &\qquad \qquad \qquad \sum_{l\in\ccalK_m}\|\samp_n^{\mathcal{X}} \bbh^{(l)}(\Delta) \F-\bbh^{(l)}\samp_n^{\mathcal{X}}\F\|.
\end{align}


Combining all these results, we can claim that for all $\epsilon'>0$ and $\delta>0$, there exists some $N$, such that for all $n>N$ we have
\begin{equation}
    \mathbb{P}(\|\bbh(\Delta_n)\samp_n^{\mathcal{X}} \F - \samp_n^{\mathcal{X}}\bbh(\Delta) \F\|\leq \epsilon')\geq 1-\delta.
\end{equation}

With $\lim\limits_{n\rightarrow \infty}D_l^n=0$ in high probability, this concludes the proof.
\end{document}

The frequency representation result in Proposition 1 holds for \eqref{eqn:conv-1d-1} and it implies that standard convolutional filters in continuous time are completely characterized by the frequency response in Definition 3. The more standard definition of a filter's frequency response as the Fourier transform of the impulse response $\tdh(t)$  (as opposed to the  Laplace transform we use in Definition 3) suffices because complex exponentials $e^{jw}$ are an orthonormal basis of eigenfunctions of the derivative operator with associated eigenvalues $j\omega$.

\gray{The work in \cite{belkin2008towards} presented an important result stating that, for a fine enough sampling resolution, the graph Laplacian of the approximating geometric graph converges to the Laplace-Beltrami operator of the manifold in the spectral sense, meaning that the eigenvalues and eigenvectors of the graph Laplacian \cite{chung1997spectral} converge, in norm and with high probability, to the eigenvalues and eigenfunctions of the Laplace-Beltrami operator, respectively. These techniques gave rise to a novel perspective on manifold learning. Indeed, the above approximations lead to the important transferability results of graph neural networks (GNNs) \cite{ruiz2021transferability, levie2019transferability}, as well as to the introduction of Graphon and Manifold Neural Networks, continuous architectures shown to be limit objects of GNNs \cite{ruiz2021graphon, wang2022convolution}.}

\gray{the authors introduced an algorithmic generalization of Laplacian eigenmaps and other non-linear dimensionality reduction methods, based on the Connection Laplacian operator and the vector heat kernel; moreover, they proved that it is possible to approximate both manifolds and their tangent bundles with certain cellular sheaves obtained from a point cloud via k-NN and Local PCA, such that, for a fine enough sampling resolution, the Sheaf Laplacian of the approximating sheaf converges pointwise to the Connection Laplacian operator.}

\section{Discussion and Future Developments AFTER NUMERICAL}
In our opinion, this work gives a multifaceted contribution. On the methodological side, it is the first work to introduce and formalize a consistent signal processing framework for processing signals defined on tangent bundles of Riemann manifolds via the Connection Laplacian operator obtained from the Levi-Cita connection; this methodology could be exploited to define frameworks on more general spaces by working with different connections, vector bundles and corresponding Connection Laplacians. On the theoretical side, the presented discretization procedure and convergence results explicitly link the manifold domain with cellular sheaves, formalizing intuitions presented in works like \cite{bodnar2022sheafdiff}; this formal connection could be exploited to derive results about problems of interest in the graph signal processing and machine learning communities such as heterophily \cite{bodnar2022sheafdiff}, transferability \cite{ruiz2020transf}, and/or oversquashing, building on intuitions as the one in \cite{levie2021transferability,topping2022understanding}; on the practical side, as we show in the next section, the proposed filters and neural networks can be exploited to address tasks involving (tangent) vector fields on curved domains; we believe that also problems as flocking, robot navigation or molecular-related tasks could be addressed by tailoring our tools. Possible future works include the aforementioned directions, as well as deriving quantitative finite-sample results on the convergence of DD-TNN  to TNN under general sampling disitributions.

% % Weyl's law \cite[Chapter~1]{arendt2009mathematical} states that if $(\ccalM, g)$ is a compact Riemann manifold of dimension $d$, then 
% % \begin{equation}\label{eqn:weylslaw}
% % \lambda_k = \frac{C_1}{C_d^{2/d}}\left(\frac{k}{Vol(\ccalM)} \right)^{2/d},
% % \end{equation}
% % where $C_d$ denotes the volume of the unit ball of $\reals^d$ and $C_1$ is an arbitrary constant. Therefore, if $\lambda_{k+1}-\lambda_k\leq \alpha$, we have
% % \begin{align}
% %     (k+1)^{2/d}-k^{2/d}\leq \frac{\alpha}{C_1} (Vol(\ccalM) C_d)^{2/d}
% % \end{align}
% % while the left side can be scaled down to $(k+1)^{2/d}-k^{2/d}\geq \frac{2}{d}k^{2/d-1}$. This implies that 
% % \begin{align}
% %     k^{\frac{2-d}{d}}\leq \left( \frac{\alpha d (Vol(\ccalM) C_d)^{2/d}}{2C_1} \right)^{\frac{d}{2-d}},
% % \end{align}
% % with $d>2$, we can claim that for 
% % $$k> \lceil (\alpha d/C_1)^{d/(2-d)}(C_d \text{Vol}(\ccalM))^{2/(2-d)} \rceil,$$ it holds that $\lambda_{k+1}-\lambda_k\leq \alpha$. Proof of Proposition \ref{prop:finite_num_rela} is similar and is also based on \eqref{eqn:weylslaw}.

% % \subsection{Proof of Theorem \ref{thm:stability_rela_filter}}
% % \label{app:stability_rela_filter}
% % The decomposition follows the same routine as \eqref{eqn:diff} shows. 
% By decomposing the filter function as \eqref{eqn:h0-gamma} and \eqref{eqn:hl-gamma}, the norm difference can also be bounded separately. 
% \begin{align}
% \label{eqn:h0-gamma}& h^{(0)}(\lambda) = \left\{ 
% \begin{array}{cc} 
%                 \hat{h}(\lambda)-\sum\limits_{l\in\ccalK_m}\hat{h}(C_l)  &  \lambda\in[\Lambda_k(\gamma)]_{k\in\ccalK_s} \\
%                 0& \text{otherwise}  \\
%                 \end{array} \right.  \\
% \label{eqn:hl-gamma}& h^{(l)}(\lambda) = \left\{ 
% \begin{array}{cc} 
%                 \hat{h}(C_l) &  \lambda\in[\Lambda_k(\gamma)]_{k\in\ccalK_s} \\
%                 \hat{h}(\lambda) & 
%                 \lambda\in\Lambda_l(\gamma)\\
%                 0 &
%                 \text{otherwise}  \\
%                 \end{array} \right.             
% \end{align}
% where now $\hat{h}(\lambda)=h^{(0)}(\lambda)+\sum_{l\in\ccalK_m}h^{(l)}(\lambda)$ with $\ccalK_s$ defined as the group index set of singletons and $\ccalK_m$ the set of partitions that contain multiple eigenvalues. For manifold filter $\bbh^{(0)}(\ccalL)$ with filter function $h^{(0)}(\lambda)$, the norm difference can also be written as
% \begin{align}
%  \label{eqn:rela-h0}  & \nonumber \left\| \sum_{i=1}^\infty h^{(0)}(\lambda_{i}) \langle f, \bm\phi_i \rangle \bm\phi_i  -  h^{(0)}(\lambda'_i )  \langle f, \bm\phi'_i \rangle \bm\phi'_i \right\| \\
%   & \nonumber \leq \left\| \sum_{i=1}^\infty h^{(0)}(\lambda_i )\langle f, \bm\phi_i \rangle (\bm\phi_i - \bm\phi'_i ) \right\| \\ \nonumber&\qquad\qquad  + \Bigg\|  \sum_{i =1}^\infty  h^{(0)}(\lambda_i )\langle f, \bm\phi_i - \bm\phi'_i  \rangle \bm\phi'_i \Bigg\|\\& \qquad\qquad \quad+ \left\|\sum_{i=1}^\infty  (h^{(0)}(\lambda_i ) -h^{(0)}(\lambda'_i) ) \langle f, \bm\phi'_i \rangle \bm\phi'_i  \right\| . 
% \end{align}
% The difference of the eigenvalues due to relative perturbations can be similarly addressed by Lemma \ref{lem:eigenvalue_relative}.


% The first two terms of \eqref{eqn:rela-h0} rely on the differences of eigenfunctions, which can be derived with Davis-Kahan Theorem in Lemma \ref{lem:davis-kahan}, the difference of eigenfunctions can be written as
% \begin{align}
% \| \ccalE\ccalL \bm\phi_i \| =\| \ccalE\lambda_i\bm\phi_i \|=\lambda_i \|\ccalE \bm\phi_i\|\leq\lambda_i\|\ccalE\|\|\bm\phi_i\|\leq \lambda_i \epsilon.
% \end{align}
% The first term in \eqref{eqn:rela-h0} then can be bounded as
% \begin{align}
% &\nonumber\left\| \sum_{i=1}^\infty h^{(0)}(\lambda_i )\langle f, \bm\phi_i \rangle (\bm\phi_i - \bm\phi'_i ) \right\|\\
% & \leq \sum_{i=1}^\infty |h^{(0)}(\lambda_i)| | \langle f, \bm\phi_i \rangle | \left\|\bm\phi_i-\bm\phi'_i \right\| \leq \sum_{i\in\ccalK_s} \frac{\pi\lambda_i \epsilon}{2d_i}  \|f\|.
% \end{align} 
% Because $d_i=\min\{ |\lambda_i-\lambda'_{i-1}|, |\lambda'_i-\lambda_{i-1}|, |\lambda'_{i+1}-\lambda_i| , | \lambda_{i+1}-\lambda'_i|\}$, with Lemma \ref{lem:eigenvalue_relative} implied, we have
% \begin{gather}
% |\lambda_i-\lambda'_{i-1}|\geq | \lambda_i - (1+\epsilon)\lambda_{i-1}|,\\
%  |\lambda'_i-\lambda_{i-1}|\geq |(1-\epsilon)\lambda_i-\lambda_{i-1}|,\\ |\lambda'_{i+1}-\lambda_i|\geq | (1-\epsilon)\lambda_{i+1}-\lambda_i|,\\| \lambda_{i+1}-\lambda'_i|\geq |\lambda_{i+1}-(1+\epsilon)\lambda_i|.
% \end{gather}
% Combine with Lemma \ref{lem:eigenvalue_relative} and Definition \ref{def:frt-spectrum}, $d_i\geq \epsilon\gamma +\gamma-\epsilon$:
% \begin{align}
%  |(1-\epsilon)\lambda_{i+1}-\lambda_i|
%  &\geq |\gamma \lambda_i-\epsilon \lambda_{i+1}|\\
%  &=\epsilon \lambda_i\left|1-\frac{\lambda_{i+1}}{\lambda_i}+\frac{\gamma}{\epsilon}-1\right|\\&\geq \lambda_i(\gamma-\epsilon+\gamma\epsilon)
% \end{align}
% This leads to the bound as
% \begin{align}
% \left\| \sum_{i=1}^\infty h^{(0)}(\lambda_i )\langle f, \bm\phi_i \rangle (\bm\phi_i - \bm\phi'_i ) \right\| \leq   \frac{M_s \pi \epsilon}{2(\gamma-\epsilon+\gamma\epsilon)} \|f\|.
% \end{align}

% The second term in \eqref{eqn:rela-h0} can also be bounded as
% \begin{align}
%     &\nonumber \left\|  \sum_{i =1}^\infty  h^{(0)}(\lambda_i )\langle f, \bm\phi_i - \bm\phi'_i  \rangle \bm\phi'_i \right\| \\
%  &\leq   \sum_{i =1}^\infty |h^{(0)}(\lambda_i)| \|\bm\phi_i - \bm\phi'_i \| \|f\|  \leq   \frac{M_s \pi \epsilon}{2(\gamma-\epsilon+\gamma\epsilon)} \|f\|,
% \end{align}
% which similarly results from the fact that $|h^{(0)}(\lambda)|<1$ and $h^{(0)}(\lambda)=0$ for $\lambda\in[\Lambda_k(\gamma)]_{k\in\ccalK_m}$. The number of eigenvalues within $[\Lambda_k(\gamma)]_{k\in\ccalK_s}$ is denoted as $M_s$.

% The third term in \eqref{eqn:rela-h0} is:
% \begin{align}
%    &\nonumber \Bigg\|\sum_{i=1}^\infty  (h^{(0)}(\lambda_i ) -h^{(0)}(\lambda'_i) ) \langle f, \bm\phi'_i \rangle \bm\phi'_i  \Bigg\|^2 \\
%     &\leq  \sum_{i=1}^\infty\left( \frac{B_h \epsilon|\lambda_i|}{(\lambda_i+\lambda_i')/2}\right)^2   \langle f,\bm\phi'_i \rangle^2 \leq \left( \frac{2B_h\epsilon}{2-\epsilon}\right)^2\|f\|^2,
% \end{align}
% with the use of Lemma \ref{lem:eigenvalue_relative} and Definition \ref{def:int-lipschitz}.

% Then we need to analyze the output difference of $h^{(l)}(\lambda)$.
% \begin{align}
%      \nonumber &\left\| \bbh^{(l)}(\ccalL)f -\bbh^{(l)}(\ccalL')f \right\| 
%     \\& \leq \left\| (\hat{h}(C_l)+\delta)f -(\hat{h}(C_l)-\delta)f\right\| \leq 2\delta\|f\|.
% \end{align}

% Combine the filter function, we could get 
% \begin{align}
% \label{eqn:sta-filter-gamma}
%     \nonumber &\|\bbh(\ccalL)f-\bbh(\ccalL')f\|=\\&
%     \left\|\bbh^{(0)}(\ccalL)f +\sum_{l\in\ccalK_m}\bbh^{(l)}(\ccalL)f - \bbh^{(0)}(\ccalL')f - \sum_{l\in\ccalK_m} \bbh^{(l)}(\ccalL')f \right\|\\
%     &\leq \|\bbh^{(0)}(\ccalL)f-\bbh^{(0)}(\ccalL')f\|+\sum_{l\in\ccalK_m}\|\bbh^{(l)}(\ccalL)f-\bbh^{(l)}(\ccalL')f\|\\
%     &\leq \frac{M_s\pi\epsilon}{\gamma-\epsilon+\gamma\epsilon}\|f\| + \frac{2B_h\epsilon}{2-\epsilon}\|f\| +2(M-M_s)\delta\|f\|,
% \end{align}
% which concludes the proof.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%% SUBSECTION %%%%%%%%%%%%%%%%%% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Proof of Theorem \ref{thm:stability_nn}}
% \label{app:stability_nn}
% To bound the output difference of MNNs, we need to write in the form of features of the final layer
%  \begin{equation}
%  \|\bm\Phi(\bbH,\ccalL,f)-\bm\Phi(\bbH,\ccalL',f)\| =  \left\| \sum_{q=1}^{F_L} f_L^q - \sum_{q=1}^{F_L} f_L^{'q}\right\|.
%  \end{equation}
% The output signal of layer $l$ of MNN $\bbPhi(\bbH,\ccalL, f)$ can be written as
% \begin{equation}
%  f_l^p = \sigma\left( \sum_{q=1}^{F_{l-1}} \bbh_l^{pq}(\ccalL) f_{l-1}^q\right).
% \end{equation}
% Similarly, for the perturbed $\ccalL'$ the corresponding MNN is $\bbPhi(\bbH,\ccalL',f)$ the output signal can be written as
%  \begin{equation}
%  f_l^{'p} = \sigma\left( \sum_{q=1}^{F_{l-1}} \bbh_l^{pq}(\ccalL') f_{l-1}^{'q}\right).
%  \end{equation}
% The difference therefore becomes
%  \begin{align}
%  &\nonumber\| f_l^p - f_l^{'p} \| \\& =\left\|  \sigma\left( \sum_{q=1}^{F_{l-1}} \bbh_l^{pq}(\ccalL) f_{l-1}^q\right) -  \sigma\left( \sum_{q=1}^{F_{l-1}} \bbh_l^{pq}(\ccalL') f_{l-1}^{'q}\right) \right\|.   
%  \end{align}
% With the assumption that $\sigma$ is normalized Lipschitz, we have
%  \begin{align}
%   \| f_l^p - f_l^{'p} \| &\leq \left\| \sum_{q=1}^{F_{l-1}}  \bbh_l^{pq}(\ccalL) f_{l-1}^q - \bbh_l^{pq}(\ccalL') f_{l-1}^{'q}  \right\| \\&\leq \sum_{q=1}^{F_{l-1}} \left\|  \bbh_l^{pq}(\ccalL) f_{l-1}^q - \bbh_l^{pq}(\ccalL') f_{l-1}^{'q} \right\|.
%  \end{align}
% By adding and subtracting $\bbh_l^{pq}(\ccalL') f_{l-1}^{q}$ from each term, combined with the triangle inequality we can get
%  \begin{align}
%  & \nonumber \left\|  \bbh_l^{pq}(\ccalL) f_{l-1}^q - \bbh_l^{pq}(\ccalL') f_{l-1}^{'q} \right\| \\\nonumber &\quad \leq \left\|  \bbh_l^{pq}(\ccalL) f_{l-1}^q - \bbh_l^{pq}(\ccalL') f_{l-1}^{q} \right\| \\&\qquad \qquad \qquad + \left\| \bbh_l^{pq}(\ccalL') f_{l-1}^q - \bbh_l^{pq}(\ccalL') f_{l-1}^{'q} \right\|
%  \end{align}
% The first term can be bounded with \eqref{eqn:sta-filter-alpha} for absolute perturbations. The second term can be decomposed by Cauchy-Schwartz inequality and non-amplifying of the filter functions as
%  \begin{align}
%  \left\| f_{l}^p - f_l^{'p} \right\| \leq \sum_{q=1}^{F_{l-1}} C_{per} \epsilon \| f_{l-1}^q\| + \sum_{q=1}^{F_{l-1}} \| f_{l-1}^q - f_{l-1}^{'q} \|,
%  \end{align}
% where $C_{per}$ representing the constant in the stability bound of manifold filters. To solve this recursion, we need to compute the bound for $\|f_l^p\|$. By normalized Lipschitz continuity of $\sigma$ and the fact that $\sigma(0)=0$, we can get
%  \begin{align}
%  \nonumber &\| f_l^p \|\leq \left\| \sum_{q=1}^{F_{l-1}} \bbh_l^{pq}(\ccalL) f_{l-1}^{q}  \right\| \leq  \sum_{q=1}^{F_{l-1}}  \left\| \bbh_l^{pq}(\ccalL)\right\|  \|f_{l-1}^{q}  \| \\
%  &\qquad \leq   \sum_{q=1}^{F_{l-1}}   \|f_{l-1}^{q}  \| \leq \prod\limits_{l'=1}^{l-1} F_{l'} \sum_{q=1}^{F_0}\| f^q \|.
%  \end{align}
%  Insert this conclusion back to solve the recursion, we can get
%  \begin{align}
%  \left\| f_{l}^p - f_l^{'p} \right\| \leq l C_{per}\epsilon \left( \prod\limits_{l'=1}^{l-1} F_{l'} \right) \sum_{q=1}^{F_0} \|f^q\|.
%  \end{align}
%  Replace $l$ with $L$ we can obtain
%  \begin{align}
%  &\nonumber \|\bm\Phi(\bbH,\ccalL,f) - \bm\Phi(\bbH,\ccalL',f)\| \\
%  &\qquad \qquad \leq \sum_{q=1}^{F_L} \left( L C_{per}\epsilon \left( \prod\limits_{l'=1}^{L-1} F_{l'} \right) \sum_{q=1}^{F_0} \|f^q\| \right).
%  \end{align}
%  With $F_0=F_L=1$ and $F_l=F$ for $1\leq l\leq L-1$, then we have
%   \begin{align}
%  \|\bm\Phi(\bbH,\ccalL,f) - \bm\Phi(\bbH,\ccalL',f)\| \leq LF^{L-1} C_{per}\epsilon \|f\|,
%  \end{align}
% which concludes the proof.
\end{document}
\begin{figure}[t!]
\label{sphere_vf}
\centering
\includegraphics[width=0.42\textwidth]{tangent.pdf}
\caption{An example of a tangent bundle signal} 
\end{figure}
\begin{figure}[t!]
\label{sphere_vf}
\centering
\includegraphics[width=0.42\textwidth]{sphere_ex_cropped.pdf}
\caption{An example of a tangent bundle signal} 
\end{figure}