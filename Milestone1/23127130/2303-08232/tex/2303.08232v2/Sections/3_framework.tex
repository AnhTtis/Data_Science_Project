\section{TELEOPERATION FRAMEWORK}

An outline of our framework is shown in Fig. \ref{fig:control_flow}. A \textit{keyframe} represents a whole-body configuration of the robot. At a high level, teleoperation is performed by iteratively generating a keyframe and dispatching it to the controller. Keyframes are generated in VR by configuring a set of kinematic tasks, such as desired contact state, taskspace posture, preferred joint angles and CoM position. An inverse kinematics (IK) solver continuously processes these inputs and computes the candidate keyframe along with an interpolated trajectory. The operator receives real-time feedback based on 3 feasibility metrics: (1) the set of contacts that are currently removable, (2) the static stability and collision status of the keyframe and (3) the static stability and collision status of the interpolated trajectory to the keyframe. The feasibility estimation is then displayed to the user along with relevant data such as CoM stability margin and joint actuation saturation. When a user is finished configuring the candidate keyframe and the feasibility estimation is valid, it can be dispatched to the robot. 

% We use this framework in two modes: \textit{online} if the controller is running on hardware or \textit{offline} if the controller is running in simulation, in which case keyframes can be exported and replayed on hardware.

\subsection{Kinematic Task Generation} \label{sec:task_generation}

The VR interface is designed to enable an operator to pose the robot by creating, removing or modifying constraints on-the-fly. To do this, four types of virtual interactable ``anchors'' are available, each corresponding to a type of kinematic task (Fig \ref{fig:teleop_combo}(a)):

\begin{itemize}

    \item \textbf{Taskspace Posture}: Matches a reference frame $F_p$ that is rigidly attached to link $l$ with a desired frame $F_d$ expressed in world coordinates.
    \item \textbf{Center of Mass}: Matches the robot's CoM with a desired position expressed in world coordinates.
    \item \textbf{Joint Position}: Matches to a preferred joint angle.
    \item \textbf{Contact Point}: Matches a contact point $p_c$ that is rigidly attached to link $l$ with a desired position $p_d$ in world coordinates.
 
\end{itemize}

% Taskspace posture anchors are configured by placing the VR controller inside the desired link $l$, clicking to create the anchor, and dragging or rotating it to the desired pose. At the moment the user clicks, the frame $F_p$ is initialized at the VR controller's position. A convex approximation of the robot's mesh (Fig. \ref{fig:teleop_combo}(b)) is used to aid the link-VR controller association. The user can toggle which of the three linear and angular constraint axes are enabled, which are reflected visually with highlights on the anchor (Fig. \ref{fig:teleop_combo}(a)).

An approximate convex decomposition of the robot's mesh (Fig. \ref{fig:teleop_combo}(b)) is used to aid the link-VR controller association. When the VR controller enters a convex shape of the preview robot, the corresponding link $l$ is highlighted and the user can click to generate a taskspace control frame $F_p$ at the VR controller's pose, which is initially coincident with $F_d$. The virtual anchor, which represents $F_d$, can then be dragged and rotated to the desired pose. The user can toggle which of the three linear and angular constraint axes are enabled, reflected visually with highlights on the anchor (Fig. \ref{fig:teleop_combo}(a)).

The CoM anchor is created by selecting a marker designating the candidate keyframe's current CoM. Similar to the taskspace posture anchor, the user can enable which linear axes are constrained. Joint position anchors can be enabled for any joint and are primarily used to bias the IK solver within a nullspace. All kinematic tasks are assigned a relative priority level by specifying one of soft, mid or hard weights, which are set through anchor menus (Fig. \ref{fig:teleop_combo}(e,f)). The option to ``Snap anchors to ghost'' (Fig. \ref{fig:teleop_combo}(d)) will move the setpoint of all non-contact kinematic tasks to the currently achieved IK configuration.

\subsection{Contact Points}
Contact point anchors are created in a specific mode which projects the VR controller positions to the surface of the nearest convex shape of the preview robot (Fig. \ref{fig:teleop_combo}(b)), with an arrow showing the surface normal. Environment shapes are also modelled as convex shapes. Contact points can be snapped to the environment using a similar mode that projects the VR controller to the surface of the nearest environment shape and shows the environment surface normal. Fig. \ref{fig:teleop_combo}(c)-top shows a contact point being created on the robot's forearm and Fig. \ref{fig:teleop_combo}(c)-bottom shows the contact point being snapped to a wall. Line and plane contacts are created by configuring the set of contact points accordingly.

% To represent this workflow as a motion planner, the user is manually generating nodes and traversing a graph of whole-body configurations. The state of the graph consists of all previous keyframes and the current keyframe

% A node is validated if the IK converges, the posture is quasi-statically stable and is collision-free. Graph edges are primarily validated by simulating the motion. <Mentioned/add a threshold for tracking error?> However, contact removals that cause the robot to become quasi-statically unstable are prohibited. <Mention that if a graph edge succeeds, it's considered the parent, otherwise the simulation resets and the user modifies the posture>, <make the point that it's kind of like single-shot planning> <discuss the design decision to keep the high branching factor, reference the image and mention that a later section compares branching factors>. Explain the terminology of controller/puppet robot. Maybe outline the rest of the paper

% AM: should probably mention the implementation details separately, maybe as the first paragraph under results? that would include hardware specs, tools/libraries/solvers used, any additional low-level abstractions (like Val's impedance controller), and specific values of any tunable weights/constants of importance mentioned in the math 

% The diagram in Fig. \ref{fig:script_pipeline} shows the control flow of our framework. At a high level, the operator creates whole-body trajectories by incrementally generating statically stable, collision-free keyframes. These keyframes can either be created offline using a simulation or online with hardware and are logged for future editing and reuse. At a software level, there are three processes involved in generating trajectories. The first is the user-interface (UI) which enables the operator to specify kinematic objectives and validate keyframes. Both a VR app in Java Monkey Engine \cite{JME} and a desktop app in JavaFX \cite{JavaFX} are implemented with similar functionality. In practice, the VR UI is preferred for generating trajectories from scratch and the desktop for editing existing trajectories. The VR app is developed for the Valve Index \cite{Index} and also has bindings for the HTC Vive \cite{Vive}. The second process is an optimization-based inverse kinematics solver (Sec. \ref{sec:ik}) which solves for whole-body configurations given a set of kinematic objectives. The third process is a robot controller which executes the commanded motion in either a physics simulator or on hardware. Both the UI and robot controller have message-layer abstraction to allow any permutation of active modules (Fig. \ref{fig:script_pipeline}). All inter-process communication is done through ROS 2 messages \cite{ros2}.

% The architecture of the framework is shown in Fig. 2
% It consists of four main components. 
%   - The first is an optimization-based inverse kinematics solver. This solver accepts kinematic tasks and outputs whole-body configurations, next section has deets
%   - The second component is a GUI for the operator to interface with the kinematics solver. Both a VR and desktop app are implemented with identical functionality. The idea is to create trajectories from scratch in VR and have the desktop app for offline editing. 
%   - The third component represents the robot's onboard controller, which executes motions commanded by the operator
%   - The fourth is the sequence of keyframes being assembled

% We develop trajectories by prototyping in simulation, assembling a library of trajectories which can be dispatched online.

% Messaging is all done in ROS 2. 
