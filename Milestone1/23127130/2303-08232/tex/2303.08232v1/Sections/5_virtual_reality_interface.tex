\section{VIRTUAL REALITY USER INTERFACE} \label{sec:vrui}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Figures/ComboFigure6.PNG}
    \caption{(a) Operator view while using the VR interface to pose Valkyrie in a squatting position. The robots and anchors are fixed in the scene and the operator can walk or teleport to move around. (b) A spatial pose anchor and its corresponding menu panel. (c) A joint position anchor and its corresponding menu panel. (d) The main menu is used to export or load scripts, toggle visibility of the controller robot and joint position anchors, configure the IK solver behavior, and change anchor behavior  by toggling contact mode and dragging. (e) Valkyrie braced against a wall with the left arm and reaching with the right arm. Visual cues for actuation feasibility include force polytopes, actuation and friction consistent support region, and alerts for joints at risk of torque saturation.}
    \label{fig:vr_combo}
\end{figure*}

The VR user-interface is designed to enable an operator to pose the robot by creating and modifying constraints on-the-fly. To do this, we implemented virtual interactable ``anchors'' which correspond to kinematic tasks. Fig. \ref{fig:vr_combo}a shows the robot in a squatting configuration which has anchors on the chest, legs and arms. The interface displays two robots: a transparent puppet robot which the operator manipulates and (optionally) an opaque controller robot.

\subsection{Anchor Configuration}
 
The interface contains three types of motion control anchors: spatial pose, Center of Mass, and joint position. Spatial pose anchors are created by simply clicking on the desired rigid body and dragging it to the desired pose. Individual axes can also be activated, cleared or modified. A ``ring-and-arrows'' \cite{leeper2012strategies} indicates the anchor setpoint, with actively constrained axes highlighted (Fig. \ref{fig:vr_combo}b). Spatial pose anchors can be designated as a ``contact anchor'', shown as smaller, blue anchors, which increases the weight and can be used to construct the support region (see \ref{sec:contact_mode}). The CoM anchor is represented by only arrows since there is not an orientation setpoint but is modified in the same way as the spatial pose. Joint position anchors are a single arrow centered at the joint. When dragging the joint, vertical motion in world frame is mapped to a desired joint position.

Each anchor has a menu with additional customization options. Figures \ref{fig:vr_combo}(b) and \ref{fig:vr_combo}(c) show the spatial pose and joint position menus, respectively. Each menu allows switching between low, medium and high weight for the corresponding kinematic task, which corresponds to entries of $\mathbf{C_J}$ in the IK. Each anchor can optionally be set to match the controller robot's pose. Two options are provided: a tracking option which continuously updates to the controller's pose and a snapping option which performs the update once. The tracking feature is beneficial for any contact that does not need precise placement in world frame but needs to remain stationary. Each anchor can also be flagged as ``persistent,'' such that the user can quickly clear all anchors which are not persistent. We find that often the operator would keep certain anchors static while iterating through configurations of other anchors and a notion of persistence aids the operator in this process. The joint position anchor has a ``mirroring'' option in which dragging or activating a joint will perform the same action on the opposite-side joint, if one exists.

Spatial pose contact anchors are created in a specific way to ensure the robot and environment mesh polytopes are in contact and tangent, shown in Fig. \ref{fig:contact_creation}. The operator first enables ``contact mode'', in which the controller is then projected to the surface of the nearest robot mesh as the operator moves (Fig. \ref{fig:contact_creation}(a)). The surface normal of the robot mesh is also shown during this step. When the operator chooses a contact point, the controller then switches to projecting to the environment mesh, with that surface normal also being shown. When the operator chooses a setpoint, the contact point then snaps to this position. We note that the orientation is only constrained in the two orientations required to keep the polytopes tangent. This is achieved by first having the $\mathbf{z}$ axis of the task frame aligned with the robot mesh surface normal. The $\mathbf{x}$ and $\mathbf{y}$ orientation setpoints are then computed to align the two surface normals.

\begin{figure}
    \centering
    \includegraphics[width=0.7\columnwidth]{Figures/ContactCreationPipeline_scaled.PNG}
    \caption{Contact points are projected to the surface of the collision mesh while creating and placing a contact anchor. The arrow indicates the surface normal of the robot or environment mesh.}
    \label{fig:contact_creation}
\end{figure}

% This is done by ensuring the anchor is on the surface of the robot's collision mesh and setpoint on the surface of the enviroment's collision mesh. Fig. \ref{fig:contact_creation} shows this process from an operator perspective. First the anchor is created (Fig. \ref{fig:contact_creation}a), and the closest robot collision mesh is queried and the anchor is projected to the surface of the mesh. The anchor position follows the VR controller and is continuously recomputed until the operator confirms the anchor position or switches modes. The operator can then place the contact anchor onto an environment mesh, in which the anchor setpoint is moved to the closest environment mesh.

\subsection{Actuation Feasibility}\label{sec:contact_mode}

The interface is intended for multi-contact scenarios which may saturate the robot's actuation limits. Visual cues are included to assist the operator in maintaining actuation feasibility, shown in Fig. \ref{fig:vr_combo}(e). The force polytope \cite{chiacchio1997force} of any contact point can be visualized and is updated as the robot's posture is adjusted. This can be useful when the ``major axis'' of the polytope has an intuitive orientation, such as vertical when contacting the ground or horizontal when bracing against a wall. A multi-contact support polygon can also be visualized for cases where the feasibility of the CoM position needs to be checked. This region is a convex set of feasible CoM XY positions based on the contact friction constraints \cite{bretl2008testing} and actuation constraints \cite{orsolino2020feasible} of the robot. The operator can toggle between this generalized support region and the nominal convex hull of contact points. The active support polygon is also sent to the inverse kinematics module to compute $\mathbf{H}$, to constrain the robot's CoM. A visualization of the feasibility of individual joint torques for the target configuration is also provided. Given the robot's contact state, we compute the torques required for static equilibrium through inverse dynamics. The color of each joint anchor from this to provide a visual cue for joints which are near saturation. For example, the elbow joint in Fig. \ref{fig:vr_combo}(e) is red, indicating it is near a torque limit for this configuration.

\subsection{Additional Operator Tools}\label{sec:op_tools}

% Additional tools are included to facilitate the operator's workflow and control of the kinematic solver's behavior. 
An option to ``snap anchors to puppet'' is provided which replaces each anchor's setpoint with the achieved value of the IK solver. For tasks with numerous anchors this is a tool for ensuring the operator is requesting a feasible set of objectives. The operator can also ``snap puppet'' when a preferred configuration is reached, which sets the kinematic solver's nominal configuration (Sec. \ref{sec:ik}) to the current solver solution. This was found to be helpful during manipulation tasks where the arm's nullspace may not be sufficiently constrained.

\subsection{Interface Workflow}

The trajectory design workflow has three phases: prototype, edit and deploy. We primarily prototype in simulation but the framework does allow for online trajectory creation (Fig. \ref{fig:script_pipeline}). When prototyping, the puppet robot is initialized to the controller robot's configuration and operator begins placing anchors. When the desired pose is reached, the operator dispatches the pose to the controller which executes the motion. The key frame is also stored which includes the controller robot's pose, the puppet robot's pose and all anchor data. If the operator is not satisfied with the motion or the robot falls down, an ``undo'' button is available which rewinds the simulator to the start of the motion and removes the stored key frame. This process is iteratively performed until the trajectory is complete, which usually is comprised of 5-20 key frames. The list of key frames as well as environment model is logged as a JSON file. Trajectory scripts can be edited and replayed in both the VR or desktop applications. Often it is useful to prototype a ``template'' script from which other scripts can be created. For example, a push-up template can be copied and push-ups of varied heights can be achieved by only modifying the $\mathbf{z}$ setpoint of the CoM objective. In the following section we present results from deploying trajectories in simulation and on hardware.

% At a high level, the goal is designing a library of trajectories which can be saved.
% We have two main uses for these trajectories:
%   one is to prototype specific scenarios in simulation which could in turn inform robot specs
%   the other is operational, in which a trajectory could be re-played on hardware
% The workflow has three main phases: prototyping, editting and deploying.
%   the prototyping phase involves the setup in fig., in which the simulation is used as the robot. during this phase the operator generates a keyframe, executes it, and checks for stability. the option to undo execution is provided which removes the keyframe, etc.
%   when the operator is satisfied with the script, the desktop editor can be used to copy or modify scripts. for example a pushup position can be duplicated at various heights by simply modifying the z setpoint on the CoM
%   finally the deployment step as mentioned is either in simulation or on hardware

% \subsection{Script Sequence Creation}

% We use the VR interface to build trajectories by incrementally generating keyframes. Each keyframe corresponds to a static wholebody configuration which is the IK solver's solution. When a keyframe is finalized by the operator, it is simultaneously saved and dispatched to a simulation which executes the motion as an additional validation step. If the keyframe causes the simulated robot to fall or become unstable, the operator can press a reset button which rewinds the simulation to before the latest keyframe and removes the saved keyframe. The operator can then make adjustments to the anchors and try dispatching the keyfrarme again. This process continues incrementally until the desired trajectory is complete, which usually consists of 5-20 keyframes depending on the motion's complexity. We use this workflow to build trajectory libraries so that motions can be saved for future use. Figure \ref{fig:script_pipeline} shows a simple schematic of the control flow for building a trajectory. This was implemented using a distributed setup in which the IK, VR and simulation were running on separate processes and coordinated using ROS 2 messages \cite{ros2}.
