@article{eskimez2022real,
  title={Real-Time Joint Personalized Speech Enhancement and Acoustic Echo Cancellation with E3Net},
  author={Eskimez, Sefik Emre and Yoshioka, Takuya and Ju, Alex and Tang, Min and Parnamaa, Tanel and Wang, Huaming},
  journal={arXiv preprint arXiv:2211.02773},
  year={2022}
}

@article{allen1979image,
  title={Image method for efficiently simulating small-room acoustics},
  author={Allen, Jont B and Berkley, David A},
  journal={The Journal of the Acoustical Society of America},
  volume={65},
  number={4},
  pages={943--950},
  year={1979},
  publisher={Acoustical Society of America}
}

@article{thakker2022fast,
  title={Fast Real-time Personalized Speech Enhancement: End-to-End Enhancement Network (E3Net) and Knowledge Distillation},
  author={Thakker, Manthan and Eskimez, Sefik Emre and Yoshioka, Takuya and Wang, Huaming},
  journal={arXiv preprint arXiv:2204.00771},
  year={2022}
}


@incollection{chickering_learning_1996,
	address = {New York, NY, USA},
	title = {Learning {Bayesian} {Networks} is {NP}-{Complete}},
	volume = {112},
	booktitle = {Learning from {Data}. {Lecture} {Notes} in {Statistics}},
	publisher = {Springer},
	author = {Chickering, D. M.},
	year = {1996},
}
@InProceedings{C2,
	author = 	 "Jones, C.D. and Smith, A.B. and Roberts, E.F.",
	title =        "Article Title",
	booktitle =        "Proceedings Title",
	organization = "IEEE",
	year = 	 "2003",
	volume = 	 "II",
	pages = 	 "803-806"
}


@inproceedings{fonseca2017freesound,
	title={Freesound datasets: a platform for the creation of open audio datasets},
	author={Fonseca, Eduardo and Pons Puig, Jordi and Favory, Xavier and Font Corbera, Frederic and Bogdanov, Dmitry and Ferraro, Andres and Oramas, Sergio and Porter, Alastair and Serra, Xavier},
	booktitle={Hu X, Cunningham SJ, Turnbull D, Duan Z, editors. Proceedings of the 18th ISMIR Conference; 2017 oct 23-27; Suzhou, China.[Canada]: International Society for Music Information Retrieval; 2017. p. 486-93.},
	year={2017},
	organization={International Society for Music Information Retrieval (ISMIR)}
}

@inproceedings{gemmeke2017audioset,
	title={Audio set: An ontology and human-labeled dataset for audio events},
	author={Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},
	booktitle={2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
	pages={776--780},
	year={2017},
	organization={IEEE}
}

@misc{paralanguage,
	title={{Paralanguage}},
	howpublished = "\url{https://en.wikipedia.org/wiki/Paralanguage}",
	note = "[Online; accessed 2022-09-01]"
}

@misc{git_dnsmos,
	title={{DNSMOS Git Repo}},
	howpublished = "\url{https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS}",
	note = "[Online; accessed 2022-09-01]"
}

@misc{model_ecapa,
	title={{DNSMOS Git Repo}},
	howpublished ="\url{https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb}", note = "[Online; accessed 2022-09-01]"
}

@inproceedings{dubey2022icasspdns,
	title={{ICASSP 2022} Deep Noise Suppression Challenge},
	author={Dubey, Harishchandra and Gopal, Vishak and Cutler, Ross and Aazami, Ashkan and Matusevych, Sergiy and Braun, Sebastian and Eskimez, Sefik Emre and Thakker, Manthan and Yoshioka, Takuya and Gamper, Hannes and others},
	booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={9271--9275},
	year={2022},
	organization={IEEE}
}

@inproceedings{reddy2021icasspdns,
	title={ICASSP 2021 deep noise suppression challenge},
	author={Reddy, Chandan KA and Dubey, Harishchandra and Gopal, Vishak and Cutler, Ross and Braun, Sebastian and Gamper, Hannes and Aichner, Robert and Srinivasan, Sriram},
	booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={6623--6627},
	year={2021},
	organization={IEEE}
}

@article{reddy2020interspeechdns,
	title={The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results},
	author={Reddy, Chandan KA and Gopal, Vishak and Cutler, Ross and Beyrami, Ebrahim and Cheng, Roger and Dubey, Harishchandra and Matusevych, Sergiy and Aichner, Robert and Aazami, Ashkan and Braun, Sebastian and others},
	journal={arXiv preprint arXiv:2005.13981},
	year={2020}
}

@inproceedings{xia2020weighted,
	title={Weighted speech distortion losses for neural-network-based real-time speech enhancement},
	author={Xia, Yangyang and Braun, Sebastian and Reddy, Chandan KA and Dubey, Harishchandra and Cutler, Ross and Tashev, Ivan},
	booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={871--875},
	year={2020},
	organization={IEEE}
}

@article{recommendation2003subjective,
	title={Subjective test methodology for evaluating speech communication systems that include noise suppression algorithm},
	author={Recommendation, ITUT},
	journal={ITU-T recommendation},
	pages={835},
	year={2003}
}

@inproceedings{reddy2022dnsmos,
	title={DNSMOS P. 835: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors},
	author={Reddy, Chandan KA and Gopal, Vishak and Cutler, Ross},
	booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={886--890},
	year={2022},
	organization={IEEE}
}

@article{dawalatabad2021ecapa,
	title={ECAPA-TDNN embeddings for speaker diarization},
	author={Dawalatabad, Nauman and Ravanelli, Mirco and Grondin, Fran{\c{c}}ois and Thienpondt, Jenthe and Desplanques, Brecht and Na, Hwidong},
	journal={arXiv preprint arXiv:2104.01466},
	year={2021}
}

@article{desplanques2020ecapa,
	title={Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification},
	author={Desplanques, Brecht and Thienpondt, Jenthe and Demuynck, Kris},
	journal={arXiv preprint arXiv:2005.07143},
	year={2020}
}


@inproceedings{Team2_baidu,
	title={MULTI-SCALE TEMPORAL FREQUENCY CONVOLUTIONAL NETWORK WITH AXIAL
	ATTENTION FOR SPEECH ENHANCEMENT},
	author={Zhang, Guochang and Yu,Libiao and Wang, Chunliang and Wei, Jianqiang},
	booktitle={IEEE ICASSP},
	year={2022}
}

@inproceedings{Team42_Meet_TEA,
	title={{TEA-PSE}: TENCENT-ETHEREAL-AUDIO-LAB PERSONALIZED SPEECH ENHANCEMENT
	SYSTEM FOR {ICASSP 2022 DNS CHALLENGE}},
	author={Ju,Yukai and Rao, Wei and Yan, Xiaopeng and Fu, Yihui and Lv, Shubo  and Cheng, Luyao and Wang, Yannan and Xie, Lei  and Shang, Shidong},
	booktitle={IEEE ICASSP},
	year={2022}
}

@inproceedings{Team29_Kuaishou,
	title={MULTI-STAGE AND MULTI-LOSS TRAINING FOR FULLBAND NON-PERSONALIZED AND PERSONALIZED SPEECH ENHANCEMENT},
	author={Chen, Lianwu and Xu, Chenglin and Zhang, Xu and Ren, Xinlei and Zheng, Xiguang and Zhang, Chen and Guo, Liang and Yu, Bing},
	booktitle={IEEE ICASSP},
	year={2022}
}

@inproceedings{Team25_CMRI_BJTU,
	title={HARMONIC GATED COMPENSATION NETWORK PLUS FOR {ICASSP} 2022 DNS CHALLENGE},
	author={Wang, Tianrui and Zhu, Weibin and Gao, Yingying and Chen, Yanan and Feng, Junlan and Zhang, Shilei},
	booktitle={IEEE ICASSP},
	year={2022}
}

@inproceedings{Team14_Alibaba_NTU,
	title={{FRCRN}: BOOSTING FEATURE REPRESENTATION USING FREQUENCY RECURRENCE FOR MONAURAL
	SPEECH ENHANCEMENT},
	author={Zhao, Shengkui and Ma, Bin and Watcharasupat, Karn N. and Gan, Woon-Seng},
	booktitle={IEEE ICASSP},
	year={2022}
}

@inproceedings{Team41_Harbin,
	title={{FB-MSTCN}: A FULL-BAND SINGLE-CHANNEL SPEECH ENHANCEMENT METHOD
	BASED ON MULTI-SCALE TEMPORAL CONVOLUTIONAL NETWORK},
	author={Zhang, Zehua and Zhang, Lu and Zhuang, Xuyi and Qian, Yukun and Li, Heng and Wang, Mingjiang},
	booktitle={IEEE ICASSP},
	year={2022}
}

@inproceedings{naderi2020transformation,
	title={Transformation of mean opinion scores to avoid misleading of ranked based statistical techniques},
	author={Naderi, Babak and M{\"o}ller, Sebastian},
	booktitle={2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX)},
	pages={1--4},
	year={2020},
	organization={IEEE}
}

@inproceedings{braun2020data,
	title={Data augmentation and loss normalization for deep noise suppression},
	author={Braun, Sebastian and Tashev, Ivan},
	booktitle={International Conference on Speech and Computer},
	pages={79--86},
	year={2020},
	organization={Springer}
}


@article{jung2020improved,
	title={Improved rawnet with feature map scaling for text-independent speaker verification using raw waveforms},
	author={Jung, Jee-weon and Kim, Seung-bin and Shim, Hye-jin and Kim, Ju-ho and Yu, Ha-Jin},
	journal={ISCA INTERSPEECH},
	year={2020}
}
@inproceedings{naderi2021,
	title={Subjective evaluation of noise suppression algorithms in crowdsourcing},
	author={Naderi, Babak and Cutler, Ross},
	booktitle={ISCA INTERSPEECH},
	year={2021}
}
@article{jung2019rawnet,
	title={Rawnet: Advanced end-to-end deep neural network using raw waveforms for text-independent speaker verification},
	author={Jung, Jee-weon and Heo, Hee-Soo and Kim, Ju-ho and Shim, Hye-jin and Yu, Ha-Jin},
	journal={ISCA INTERSPEECH},
	year={2019}
}
@inproceedings{reddy2021dnsmos,
	title={{DNSMOS P.835: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors}},
	author={Reddy, Chandan KA and Gopal, Vishak and Cutler, Ross},
	booktitle={IEEE ICASSP},
	year={2021}
}

@INPROCEEDINGS{icassp2021challenge,
	author={Reddy, Chandan K. A. and Dubey, Harishchandra and Gopal, Vishak and Cutler, Ross and Braun, Sebastian and Gamper, Hannes and Aichner, Robert and Srinivasan, Sriram},
	booktitle={IEEE ICASSP}, 
	title={{ICASSP 2021 Deep Noise Suppression Challenge}}, 
	year={2021},
	
	pages={6623-6627},
	doi={10.1109/ICASSP39728.2021.9415105}}

@inproceedings{snyder2018x,
	title={X-vectors: Robust dnn embeddings for speaker recognition},
	author={Snyder, David and Garcia-Romero, Daniel and Sell, Gregory and Povey, Daniel and Khudanpur, Sanjeev},
	booktitle={IEEE ICASSP},
	pages={5329--5333},
	year={2018},
}


@inproceedings{wan2018generalized,
	title={Generalized end-to-end loss for speaker verification},
	author={Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
	booktitle={IEEE ICASSP},
	pages={4879--4883},
	year={2018},
	
}


@misc{mailabs,
	title={{M-AILABS Speech Multi-lingual Dataset}},
	howpublished = "\url{https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/}",
	note = "[Online; accessed 2020-09-01]"
}

@misc{swc,
	title={{The Spoken Wikipedia Corpora}},
	howpublished = "\url{https://nats.gitlab.io/swc/}",
	note = "[Online; accessed 2020-09-01]",
}

@misc{kinect,
	title={{Telecooperation German Corpus for Kinect}},
	howpublished = "\url{http://www.repository.voxforge1.org/downloads/de/german-speechdata-TUDa-2015.tar.gz}",
	note = "[Online; accessed 2020-09-01]",
}

@article{chung2018voxceleb2,
	title={Voxceleb2: Deep speaker recognition},
	author={Chung, Joon Son and Nagrani, Arsha and Zisserman, Andrew},
	journal={ISCA INTERSPEECH},
	year={2018}
}

@article{hansen2015speaker,
	title={Speaker recognition by machines and humans: A tutorial review},
	author={Hansen, John HL and Hasan, Taufiq},
	journal={IEEE Signal processing magazine},
	volume={32},
	number={6},
	pages={74--99},
	year={2015},
}

@article{yamagishi2019cstr,
	title={{CSTR VCTK} Corpus: English Multi-speaker Corpus for {CSTR} Voice Cloning Toolkit (version 0.92)},
	author={Yamagishi, Junichi and Veaux, Christophe and MacDonald, Kirsten and others},
	year={2019},
	publisher={University of Edinburgh}
}

@inproceedings{aishell_2017,
	title={{Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline}},
	author={Bu, Hui and Du, Jiayu and Na, Xingyu and Wu, Bengu and Zheng, Hao},
	booktitle={2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA)},
	organization={IEEE}
}

@misc{THCHS30_2015,
	title={{THCHS-30} : A Free Chinese Speech Corpus},
	author={Dong Wang, Xuewei Zhang, Zhiyong Zhang},
	year={2015},
	url={http://arxiv.org/abs/1512.01882}
}

@inproceedings{antsalo2001estimation,
	title={Estimation of modal decay parameters from noisy response measurements},
	author={Antsalo, Poju and others},
	booktitle={Audio Engineering Society Convention 110},
	year={2001},
}

@inproceedings{gamper2020blind,
	title={Blind {C50} estimation from single-channel speech using a convolutional neural network},
	author={Gamper, Hannes},
	booktitle={Proc. IEEE MMSP},
	pages={136--140},
	year={2020},
}

@inproceedings{ko2017study,
	title={A study on data augmentation of reverberant speech for robust speech recognition},
	author={Ko, Tom and Peddinti, Vijayaditya and Povey, Daniel and Seltzer, Michael L and Khudanpur, Sanjeev},
	booktitle={IEEE ICASSP},
	year={2017},
}

@inproceedings{wilkins2018vocalset,
	title={VocalSet: A Singing Voice Dataset.},
	author={Wilkins, Julia and Seetharaman, Prem and Wahl, Alison and Pardo, Bryan},
	booktitle={ISMIR},
	year={2018}
}

@article{cao2014crema,
	title={{CREMA-D}: Crowd-sourced emotional multimodal actors dataset},
	author={Cao, Houwei and Cooper, David G and Keutmann, Michael K and Gur, Ruben C and Nenkova, Ani and Verma, Ragini},
	journal={IEEE Trans. on Affective Computing},
	volume={5},
	number={4},
	pages={377--390},
	year={2014},
}

@article{polqa,
	author = {Beerends, John and others},
	year = {2013},
	month = {06},
	pages = {385-402},
	title = {Perceptual Objective Listening Quality Assessment ({POLQA}), The Third Generation {ITU-T} Standard for End-to-End Speech Quality Measurement Part {II}-Perceptual Model},
	volume = {61},
	journal = {AES: Journal of the Audio Engineering Society}
}

@article{p800,
	title={{ITU-T} Recommendation {P.800}: Methods for subjective determination of transmission quality, ITU-T Recommendation P.800}, 
	month = {Feb}, 
	year={1998}
}

@article{p862,
	title={{ITU-T} Recommendation {P.862}: Perceptual evaluation of speech quality ({PESQ}): An objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs}, 
	month = {Feb}, 
	year={2001}
}

@inproceedings{reddy2022dnsmos,
	title={DNSMOS P. 835: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors},
	author={Reddy, Chandan KA and Gopal, Vishak and Cutler, Ross},
	booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={886--890},
	year={2022},
	organization={IEEE}
}

@article{dns2021interspeech,
	title={{INTERSPEECH 2021 Deep Noise Suppression Challenge}},
	author={Reddy, Chandan KA and Dubey, Harishchandra and Koishida, Kazuhito and Nair, Arun and Gopal, Vishak and Cutler, Ross and Braun, Sebastian and Gamper, Hannes and Aichner, Robert and Srinivasan, Sriram},
	journal={ISCA INTERSPEECH},
	year={2021}
}

@article{g168,
	title={{ITU-T} Recommendation {G.168}: Digital network echo cancellers}, 
	month = {Feb}, 
	year={2012}
}

@INPROCEEDINGS{Avila2019,
	author={A. R. {Avila} and H. {Gamper} and C. {Reddy} and R. {Cutler} and I. {Tashev} and J. {Gehrke}},
	booktitle={IEEE ICASSP}, 
	title={Non-intrusive Speech Quality Assessment Using Neural Networks}, 
	year={2019}
}

@inproceedings{gamper2019intrusive,
	title={Intrusive and non-intrusive perceptual speech quality assessment using a convolutional neural network},
	author={Gamper, Hannes and Reddy, Chandan KA and Cutler, Ross and Tashev, Ivan J and Gehrke, Johannes},
	booktitle={2019 IEEE WASPAA},
	pages={85--89},
	year={2019},
}

@inproceedings{naderi2020open,
	title={An Open source Implementation of {ITU-T} Recommendation {P.808} with Validation},
	booktitle={ISCA INTERSPEECH},
	author={Babak Naderi and Ross Cutler},
	year={2020},
}

@inproceedings{reddy2020interspeech,
	title={The {INTERSPEECH} 2020 Deep Noise Suppression Challenge: Datasets, Subjective Testing Framework, and Challenge Results},
	author={Reddy, Chandan KA and others},
	year={2020}
}

@INPROCEEDINGS{Xia2020,
	author={Y. {Xia} and S. {Braun} and C. K. A. {Reddy} and H. {Dubey} and R. {Cutler} and I. {Tashev}},
	booktitle={IEEE ICASSP}, 
	title={Weighted Speech Distortion Losses for Neural-Network-Based Real-Time Speech Enhancement}, 
	year={2020},
	pages={871-875},}

@INPROCEEDINGS{7178964,
	author={V. {Panayotov} and G. {Chen} and D. {Povey} and S. {Khudanpur}},
	booktitle={IEEE ICASSP}, 
	title={{Librispeech: An ASR corpus based on public domain audio books}}, 
	year={2015}
}

@INPROCEEDINGS{7952261,
	author={J. F. {Gemmeke} and D. P. W. {Ellis} and D. {Freedman} and A. {Jansen} and W. {Lawrence} and R. C. {Moore} and M. {Plakal} and M. {Ritter}},
	booktitle={IEEE ICASSP}, 
	title={Audio Set: An ontology and human-labeled dataset for audio events}, 
	year={2017},
}

@article{demand,
	author = {Thiemann, Joachim and Ito, Nobutaka and Vincent, Emmanuel},
	year = {2013},
	month = {05},
	pages = {3591},
	title = {The Diverse Environments Multi-Channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings},
	journal = {The Journal of the Acoustical Society of America},
}

@ARTICLE{8031044,
	author={C. {Karadagur Ananda Reddy} and N. {Shankar} and G. {Shreedhar Bhat} and R. {Charan} and I. {Panahi}},
	journal={IEEE Signal Processing Letters}, 
	title={An Individualized Super-Gaussian Single Microphone Speech Enhancement for Hearing Aid Users With Smartphone as an Assistive Device}, 
	year={2017},
	volume={24},
	number={11},
	pages={1601-1605},}

@ARTICLE{malah,
	author={Y. {Ephraim} and D. {Malah}},
	journal={IEEE TASLP}, 
	title={Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator}, 
	year={1984},
}

@INPROCEEDINGS{8281993,
	author={S. {Fu} and Y. {Tsao} and X. {Lu} and H. {Kawai}},
	booktitle={2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}, 
	title={Raw waveform-based speech enhancement by fully convolutional networks}, 
}

@article{choi2020phase,
	title={Phase-aware Single-stage Speech Denoising and Dereverberation with {U}-Net},
	author={Choi, Hyeong-Seok and Heo, Hoon and Lee, Jie Hwan and Lee, Kyogu},
	journal={arXiv preprint arXiv:2006.00687},
	year={2020}
}

@article{koyama2020exploring,
	title={Exploring the Best Loss Function for {DNN}-Based Low-latency Speech Enhancement with Temporal Convolutional Networks},
	author={Koyama, Yuichiro and Vuong, Tyler and Uhlich, Stefan and Raj, Bhiksha},
	journal={arXiv preprint arXiv:2005.11611},
	year={2020}
}

@inproceedings{isik2020poconet,
	title={{PoCoNet}: Better Speech Enhancement with Frequency-Positional Embeddings, Semi-Supervised Conversational Data, and Biased Loss},
	author={Isik, Umut and others},
	booktitle={ISCA INTERSPEECH},
	year={2020}
}

@article{valin2020perceptually,
	title={A Perceptually-Motivated Approach for Low-Complexity, Real-Time Enhancement of Fullband Speech},
	author={Valin, Jean-Marc and others},
	journal={arXiv preprint arXiv:2008.04259},
	year={2020}
}

@article{reddy2019scalable,
	title={A scalable noisy speech dataset and online subjective test framework},
	author={Reddy, Chandan KA and others},
	journal={arXiv preprint arXiv:1909.08050},
	year={2019}
}

@article{eskimez2021personalized,
	title={Personalized Speech Enhancement: New Models and Comprehensive Evaluation},
	author={Eskimez, Sefik Emre and Yoshioka, Takuya and Wang, Huaming and Wang, Xiaofei and Chen, Zhuo and Huang, Xuedong},
	journal={arXiv preprint arXiv:2110.09625},
	year={2021}
}

@book{hastie_elements_2001,
	address = {New York, NY, USA},
	title = {The {Elements} of {Statistical} {Learning}},
	publisher = {Springer},
	author = {Hastie, T and Tibshirani, R and Friedman, J},
	year = {2001},
}

@incollection{wainwright_high-dimensional_2007,
	title = {High-{Dimensional} {Graphical} {Model} {Selection} {Using} {\textbackslash}textbackslash ell\_1-{Regularized} {Logistic} {Regression}},
	url = {http://papers.nips.cc/paper/3138-high-dimensional-graphical-model-selection-using-ell_1-regularized-logistic-regression.pdf},
	urldate = {2019-10-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {MIT Press},
	author = {Wainwright, Martin J and Lafferty, John D. and Ravikumar, Pradeep K.},
	editor = {Schölkopf, B. and Platt, J. C. and Hoffman, T.},
	year = {2007},
	pages = {1465--1472},
	file = {NIPS Full Text PDF:/Users/rosscutler/Zotero/storage/TKL32E5T/Wainwright et al. - 2007 - High-Dimensional Graphical Model Selection Using .pdf:application/pdf;NIPS Snapshot:/Users/rosscutler/Zotero/storage/6IYGLQX7/3138-high-dimensional-graphical-model-selection-using-ell_1-regularized-logistic-regression.html:text/html},
}

@inproceedings{li_deep_nodate,
	title = {Deep {Learning} {IP} {Network} {Representations}},
	url = {https://sites.cs.ucsb.edu/~bzong/doc/bigdama-18.pdf},
	author = {Li, Mingda},
}

@article{sellen_remote_1995,
	title = {Remote {Conversations}: {The} {Effects} of {Mediating} {Talk} {With} {Technology}},
	volume = {10},
	issn = {0737-0024},
	shorttitle = {Remote {Conversations}},
	url = {http://www.tandfonline.com/doi/abs/10.1207/s15327051hci1004_2},
	doi = {10.1207/s15327051hci1004_2},
	abstract = {Three different videoconferencing systems for supporting multiparty, remote conversations are described and evaluated experimentally. The three systems differed by how many participants were visible at once, their spatial arrangement, and control over who was seen. Conversations using these systems were compared to same-room (Experiment 1) and audio-only (Experiment 2) conversations. Specialized speech-tracking equipment recorded the on-off patterns of speech that allowed objective measurement of structural aspects of the conversations, such as turn length, pauses, and interruptions. Questionnaires and interviews also docuented participants' opinions and perceptions in the various settings.},
	language = {en},
	number = {4},
	urldate = {2019-10-14},
	journal = {Human-Computer Interaction},
	author = {Sellen, Abigail},
	month = dec,
	year = {1995},
	pages = {401--444},
	file = {Sellen - 1995 - Remote Conversations The Effects of Mediating Tal.pdf:/Users/rosscutler/Zotero/storage/NDUCQJJV/Sellen - 1995 - Remote Conversations The Effects of Mediating Tal.pdf:application/pdf},
}

@article{rogelberg_science_nodate,
	title = {The science and fiction of meetings},
	url = {https://www.researchgate.net/profile/Cliff_Scott/publication/265508855_The_Science_and_Fiction_of_Meetings/links/54107b050cf2df04e75d5eaa.pdf},
	urldate = {2019-06-28},
	author = {Rogelberg, Steven and Scott, Cliff and Kello, John},
}

@inproceedings{wainwright_high-dimensional_2006,
	title = {High-dimensional graphical model selection using ℓ1-regularized logistic regression},
	abstract = {We focus on the problem of estimating the graph structure associated with a discrete Markov random field. We describe a method based on ℓ1regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an ℓ1-constraint. Our framework applies to the high-dimensional setting, in which both the number of nodes p and maximum neighborhood sizes d are allowed to grow as a function of the number of observations n. Our main result is to establish sufficient conditions on the triple (n, p, d) for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. Under certain mutual incoherence conditions analogous to those imposed in previous work on linear regression, we prove that consistent neighborhood selection can be obtained as long as the number of observations n grows more quickly than 6d 6 log d + 2d 5 log p, thereby establishing that logarithmic growth in the number of samples n relative to graph size p is sufficient to achieve neighborhood consistency.},
	booktitle = {In {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Wainwright, Martin J. and Ravikumar, Pradeep and Lafferty, John D.},
	year = {2006},
	file = {Citeseer - Full Text PDF:/Users/rosscutler/Zotero/storage/2989R7VY/Wainwright et al. - 2006 - High-dimensional graphical model selection using ℓ.pdf:application/pdf;Citeseer - Snapshot:/Users/rosscutler/Zotero/storage/5U5MTCQF/summary.html:text/html},
}

@inproceedings{veinott_video_1999,
	address = {Pittsburgh, Pennsylvania, United States},
	title = {Video helps remote work: speakers who need to negotiate common ground benefit from seeing each other},
	isbn = {978-0-201-48559-2},
	shorttitle = {Video helps remote work},
	url = {http://portal.acm.org/citation.cfm?doid=302979.303067},
	doi = {10.1145/302979.303067},
	abstract = {More and more organizations are forming teamsthat are not co-located. Theseteamscommunicatevia email, fax, telephone and audio conferences,and sometimesvideo. The question often arises whether the cost of video is worth it. Previous researchhas shown that video makes people more satisfied with the work, but it doesn’t help the quality of the work itself. There is one exception; negotiation tasks are measurably better with video. In this study, we show that the same effect holds for a more subtle form of negotiation, when people have to negotiate meaning in a conversation. We compared the performance and communication of people explaining a map route to each other. Half the pairs have video and audio connections, half only audio. Half of the pairs were native speakersof English; the other half were non-native speakers, those presumably who have to negotiate meaning more. The results showed that non-native speakerpairs did benefit from the video; native speakers did not. Detailed analysis of the conversational strategies showed that with video, the non-native speaker pairs spent proportionately more effort negotiating common ground.},
	language = {en},
	urldate = {2019-10-10},
	booktitle = {Proceedings of the {SIGCHI} conference on {Human} factors in computing systems the {CHI} is the limit - {CHI} '99},
	publisher = {ACM Press},
	author = {Veinott, Elizabeth S. and Olson, Judith and Olson, Gary M. and Fu, Xiaolan},
	year = {1999},
	pages = {302--309},
	file = {Veinott et al. - 1999 - Video helps remote work speakers who need to nego.pdf:/Users/rosscutler/Zotero/storage/CYJLNDYB/Veinott et al. - 1999 - Video helps remote work speakers who need to nego.pdf:application/pdf},
}

@article{habash_impact_1999,
	title = {The impact of audio- or video-conferencing and group decision tools on group perception and satisfaction in distributed meetings},
	volume = {3},
	issn = {1550-3461(Electronic),1088-7156(Print)},
	doi = {10.1037/h0095872},
	abstract = {Investigated the impact of communication media (audio-conferencing [AC] vs video-conferencing [VC]) and the use/non-use of group decision support tools (GDST) on group perception and goal satisfaction in synchronous distributed group decision tasks. A factorial design with repeated measures on the communication medium factor was employed. 72 professionals from a large non-profit organization solved real-world tasks in small groups. Perception of social presence, of communication interface, and of communication effectiveness was assessed. Individual personal satisfaction with the meeting process and outcome, and perception of the final decision quality were also measured. The study found that organizations can get all the benefits of using GDST in distributed meetings without losing much in terms of perception of communication interface. If GDST are used, AC is sufficient, since adding VC provided little or no additional benefit. Implications for managers are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {The Psychologist-Manager Journal},
	author = {Habash, Tony F.},
	year = {1999},
	keywords = {Audiovisual Communications Media, Decision Support Systems, Group Decision Making, Perception, Professional Personnel, Satisfaction, Teleconferencing},
	pages = {211--230},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/C5TUVLXV/2000-15667-003.html:text/html},
}

@article{boutaba_comprehensive_2018,
	title = {A comprehensive survey on machine learning for networking: evolution, applications and research opportunities},
	volume = {9},
	issn = {1869-0238},
	shorttitle = {A comprehensive survey on machine learning for networking},
	url = {https://doi.org/10.1186/s13174-018-0087-2},
	doi = {10.1186/s13174-018-0087-2},
	abstract = {Machine Learning (ML) has been enjoying an unprecedented surge in applications that solve problems and enable automation in diverse domains. Primarily, this is due to the explosion in the availability of data, significant improvements in ML techniques, and advancement in computing capabilities. Undoubtedly, ML has been applied to various mundane and complex problems arising in network operation and management. There are various surveys on ML for specific areas in networking or for specific network technologies. This survey is original, since it jointly presents the application of diverse ML techniques in various key areas of networking across different network technologies. In this way, readers will benefit from a comprehensive discussion on the different learning paradigms and ML techniques applied to fundamental problems in networking, including traffic prediction, routing and classification, congestion control, resource and fault management, QoS and QoE management, and network security. Furthermore, this survey delineates the limitations, give insights, research challenges and future opportunities to advance ML in networking. Therefore, this is a timely contribution of the implications of ML for networking, that is pushing the barriers of autonomic network operation and management.},
	number = {1},
	urldate = {2019-10-09},
	journal = {Journal of Internet Services and Applications},
	author = {Boutaba, Raouf and Salahuddin, Mohammad A. and Limam, Noura and Ayoubi, Sara and Shahriar, Nashid and Estrada-Solano, Felipe and Caicedo, Oscar M.},
	month = jun,
	year = {2018},
	pages = {16},
	file = {Full Text:/Users/rosscutler/Zotero/storage/M96D3CAQ/Boutaba et al. - 2018 - A comprehensive survey on machine learning for net.pdf:application/pdf;Snapshot:/Users/rosscutler/Zotero/storage/F45LJWPH/s13174-018-0087-2.html:text/html},
}

@inproceedings{ruth_large-scale_2017,
	address = {London, United Kingdom},
	title = {Large-scale scanning of {TCP}'s initial window},
	isbn = {978-1-4503-5118-8},
	url = {http://dl.acm.org/citation.cfm?doid=3131365.3131370},
	doi = {10.1145/3131365.3131370},
	abstract = {Improving web performance is fueling the debate of sizing TCP’s initial congestion window (IW), which is a critical performance parameter especially for short-lived flows. This debate yielded several RFC updates to recommended IW sizes, e.g., an increase to IW10 in 2010. The current adoption of IW recommendations is, however, unknown. In this paper, we therefore conduct large-scale measurements covering the entire IPv4 space inferring the IW distribution size by probing HTTP and HTTPS servers. We present an HTTP and TLS scanning method implemented in ZMap, enabling quick estimations of IW sizes at Internet scale. For the first time since the standardization and implementation of IW 10, we shed light on the rugged landscape of IW configurations on the Internet.},
	language = {en},
	urldate = {2019-10-08},
	booktitle = {Proceedings of the 2017 {Internet} {Measurement} {Conference} on - {IMC} '17},
	publisher = {ACM Press},
	author = {Rüth, Jan and Bormann, Christian and Hohlfeld, Oliver},
	year = {2017},
	pages = {304--310},
	file = {Rüth et al. - 2017 - Large-scale scanning of TCP's initial window.pdf:/Users/rosscutler/Zotero/storage/2S5XISUZ/Rüth et al. - 2017 - Large-scale scanning of TCP's initial window.pdf:application/pdf},
}

@inproceedings{sundaresan_tcp_2017,
	address = {London, United Kingdom},
	title = {{TCP} congestion signatures},
	isbn = {978-1-4503-5118-8},
	url = {http://dl.acm.org/citation.cfm?doid=3131365.3131381},
	doi = {10.1145/3131365.3131381},
	abstract = {We develop and validate Internet path measurement techniques to distinguish congestion experienced when a ﬂow self-induces congestion in the path from when a ﬂow is affected by an already congested path. One application of this technique is for speed tests, when the user is affected by congestion either in the last mile or in an interconnect link. This difference is important because in the latter case, the user is constrained by their service plan (i.e., what they are paying for), and in the former case, they are constrained by forces outside of their control. We exploit TCP congestion control dynamics to distinguish these cases for Internet paths that are predominantly TCP trafﬁc. In TCP terms, we re-articulate the question: was a TCP ﬂow bottlenecked by an already congested (possibly interconnect) link, or did it induce congestion in an otherwise idle (possibly a last-mile) link? TCP congestion control affects the round-trip time (RTT) of packets within the ﬂow (i.e., the ﬂow RTT): an endpoint sends packets at higher throughput, increasing the occupancy of the bottleneck buffer, thereby increasing the RTT of packets in the ﬂow. We show that two simple, statistical metrics derived from the ﬂow RTT during the slow start period—its coefﬁcient of variation, and the normalized difference between the maximum and minimum RTT—can robustly identify which type of congestion the ﬂow encounters. We use extensive controlled experiments to demonstrate that our technique works with up to 90\% accuracy. We also evaluate our techniques using two unique real-world datasets of TCP throughput measurements using Measurement Lab data and the Ark platform. We ﬁnd up to 99\% accuracy in detecting self-induced congestion, and up to 85\% accuracy in detecting external congestion. Our results can beneﬁt regulators of interconnection markets, content providers trying to improve customer service, and users trying to understand whether poor performance is something they can ﬁx by upgrading their service tier.},
	language = {en},
	urldate = {2019-10-08},
	booktitle = {Proceedings of the 2017 {Internet} {Measurement} {Conference} on - {IMC} '17},
	publisher = {ACM Press},
	author = {Sundaresan, Srikanth and Allman, Mark and Dhamdhere, Amogh and Claffy, Kc},
	year = {2017},
	pages = {64--77},
	file = {Sundaresan et al. - 2017 - TCP congestion signatures.pdf:/Users/rosscutler/Zotero/storage/LNFQ39V8/Sundaresan et al. - 2017 - TCP congestion signatures.pdf:application/pdf},
}

@article{koo_guideline_2016,
	title = {A {Guideline} of {Selecting} and {Reporting} {Intraclass} {Correlation} {Coefficients} for {Reliability} {Research}},
	volume = {15},
	issn = {1556-3707},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4913118/},
	doi = {10.1016/j.jcm.2016.02.012},
	abstract = {Objective
Intraclass correlation coefficient (ICC) is a widely used reliability index in test-retest, intrarater, and interrater reliability analyses. This article introduces the basic concept of ICC in the content of reliability analysis.

Discussion for Researchers
There are 10 forms of ICCs. Because each form involves distinct assumptions in their calculation and will lead to different interpretations, researchers should explicitly specify the ICC form they used in their calculation. A thorough review of the research design is needed in selecting the appropriate form of ICC to evaluate reliability. The best practice of reporting ICC should include software information, “model,” “type,” and “definition” selections.

Discussion for Readers
When coming across an article that includes ICC, readers should first check whether information about the ICC form has been reported and if an appropriate ICC form was used. Based on the 95\% confident interval of the ICC estimate, values less than 0.5, between 0.5 and 0.75, between 0.75 and 0.9, and greater than 0.90 are indicative of poor, moderate, good, and excellent reliability, respectively.

Conclusion
This article provides a practical guideline for clinical researchers to choose the correct form of ICC and suggests the best practice of reporting ICC parameters in scientific publications. This article also gives readers an appreciation for what to look for when coming across ICC while reading an article.},
	number = {2},
	urldate = {2019-10-04},
	journal = {Journal of Chiropractic Medicine},
	author = {Koo, Terry K. and Li, Mae Y.},
	month = jun,
	year = {2016},
	pmid = {27330520},
	pmcid = {PMC4913118},
	pages = {155--163},
	file = {PubMed Central Full Text PDF:/Users/rosscutler/Zotero/storage/IYL7LPWZ/Koo and Li - 2016 - A Guideline of Selecting and Reporting Intraclass .pdf:application/pdf},
}

@inproceedings{heidemann_census_2008,
	address = {Vouliagmeni, Greece},
	title = {Census and survey of the visible internet},
	isbn = {978-1-60558-334-1},
	url = {http://portal.acm.org/citation.cfm?doid=1452520.1452542},
	doi = {10.1145/1452520.1452542},
	language = {en},
	urldate = {2019-10-03},
	booktitle = {Proceedings of the 8th {ACM} {SIGCOMM} conference on {Internet} measurement conference - {IMC} '08},
	publisher = {ACM Press},
	author = {Heidemann, John and Pradkin, Yuri and Govindan, Ramesh and Papadopoulos, Christos and Bartlett, Genevieve and Bannister, Joseph},
	year = {2008},
	pages = {169},
	file = {Heidemann et al. - 2008 - Census and survey of the visible internet.pdf:/Users/rosscutler/Zotero/storage/G6G8GY5J/Heidemann et al. - 2008 - Census and survey of the visible internet.pdf:application/pdf},
}

@article{yang_tcp_2014,
	title = {{TCP} {Congestion} {Avoidance} {Algorithm} {Identification}},
	volume = {22},
	doi = {10.1109/TNET.2013.2278271},
	abstract = {The Internet has recently been evolving from homogeneous congestion control to heterogeneous congestion control. Several years ago, Internet traffic was mainly controlled by the traditional RENO, whereas it is now controlled by multiple different TCP algorithms, such as RENO, CUBIC, and Compound TCP (CTCP). However, there is very little work on the performance and stability study of the Internet with heterogeneous congestion control. One fundamental reason is the lack of the deployment information of different TCP algorithms. In this paper, we first propose a tool called TCP Congestion Avoidance Algorithm Identification (CAAI) for actively identifying the TCP algorithm of a remote Web server. CAAI can identify all default TCP algorithms (e.g., RENO, CUBIC, and CTCP) and most non-default TCP algorithms of major operating system families. We then present the CAAI measurement result of about 30 000 Web servers. We found that only 3.31 \% 14.47 \% of the Web servers still use RENO, 46.92\% of the Web servers use BIC or CUBIC, and 14.5 \% 25.66 \% of the Web servers use CTCP. Our measurement results show a strong sign that the majority of TCP flows are not controlled by RENO anymore, and a strong sign that the Internet congestion control has changed from homogeneous to heterogeneous.},
	number = {4},
	journal = {IEEE/ACM Transactions on Networking},
	author = {Yang, P. and Shao, J. and Luo, W. and Xu, L. and Deogun, J. and Lu, Y.},
	month = aug,
	year = {2014},
	keywords = {Feature extraction, Algorithm design and analysis, CAAI, CAAI measurement, compound TCP, CTCP, CUBIC, default TCP algorithms, file servers, heterogeneous congestion control, Heterogeneous congestion control, homogeneous congestion control, Internet, Internet measurement, Internet traffic, Linux, nondefault TCP algorithms, operating system, Operating systems, operating systems (computers), remote Web server, RENO, stability study, TCP congestion avoidance algorithm identification, TCP congestion control, telecommunication congestion control, telecommunication traffic, transport protocols, Web servers},
	pages = {1311--1324},
	file = {Full Text:/Users/rosscutler/Zotero/storage/Z8637B7N/Yang et al. - 2014 - TCP Congestion Avoidance Algorithm Identification.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/YVIM6FKF/6594906.html:text/html;IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/XDETRKHF/6594906.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/5VUNUKFP/Yang et al. - 2014 - TCP Congestion Avoidance Algorithm Identification.pdf:application/pdf},
}

@article{al-saadi_survey_2019,
	title = {A {Survey} of {Delay}-{Based} and {Hybrid} {TCP} {Congestion} {Control} {Algorithms}},
	doi = {10.1109/COMST.2019.2904994},
	abstract = {Congestion Control (CC) has a significant influence on the performance of Transmission Control Protocol (TCP) connections. Over the last three decades, many researchers have extensively studied and proposed a multitude of enhancements to standard TCP CC. However, this topic still inspires both academic and industrial research communities due to the change in Internet application requirements and the evolution of Internet technologies. The standard TCP CC infers network congestion based on packet loss events which leads to long queuing delay when bottleneck buffer size is large. A promising solution to this problem is to use the delay signal (RTT or one-way delay measurements) to infer congestion earlier and react to the congestion before the queuing delay reaches a high value. In this survey paper, we describe the delay signal and the algorithms that completely or partially utilise this type of signal. Additionally, we illustrate standard CC and modern Active Queue Management (AQM) principles and discus the interaction between AQM and the delay signal.},
	journal = {IEEE Communications Surveys Tutorials},
	author = {Al-Saadi, R. and Armitage, G. and But, J. and Branch, P.},
	year = {2019},
	keywords = {Internet, Active Queue Management., Delay-based congestion control, Delays, Packet loss, Protocols, Receivers, Standards, TCP},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/787GFAGI/8668433.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/4YCSMK99/Al-Saadi et al. - 2019 - A Survey of Delay-Based and Hybrid TCP Congestion .pdf:application/pdf},
}

@inproceedings{hakkinen_qualitative_2010,
	title = {Qualitative analysis of mediated communication experience},
	doi = {10.1109/QOMEX.2010.5516195},
	abstract = {We studied the experiences of participants collaborating in video and teleconference situations. We utilized a qualitative experience measurement methodology where participants freely described their experiences by giving attributes that best described their experience. The most frequent descriptions were combined to attributes which were used to discriminate the experimental conditions. The results indicated that qualitative methodology can discriminate the experience differences between mediated communication technologies. Videoconferencing setting was described as the most natural and exciting, while teleconferencing was described as confusing and frustrating. The results also showed that utilization of face-to-face situation as a comparison condition to mediated communication situations can be problematic, because the meaning of experience attributes changes.},
	booktitle = {2010 {Second} {International} {Workshop} on {Quality} of {Multimedia} {Experience} ({QoMEX})},
	author = {Häkkinen, J. and Aaltonen, V. and Schrader, M. and Nyman, G. and Lehtonen, M. and Takatalo, J.},
	month = jun,
	year = {2010},
	keywords = {Teleconferencing, Cameras, Communication channels, communication experience, Communication systems, Communications technology, computer mediated communication, Context, Displays, mediated communication experience, Mobile communication, qualitative experience measurement methodology, Signal analysis, teleconferencing, Teleworking, video communication, Videoconference, videoconferencing},
	pages = {147--151},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/3L6S3JZ5/5516195.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/6BDKKUI7/Häkkinen et al. - 2010 - Qualitative analysis of mediated communication exp.pdf:application/pdf},
}

@article{groen_improving_2012,
	title = {Improving video-mediated communication with orchestration},
	volume = {28},
	issn = {07475632},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0747563212000854},
	doi = {10.1016/j.chb.2012.03.019},
	abstract = {Video-mediated communication (VMC) has become a popular communication medium. However, research to date suggests that the inherent constraints of VMC impair effective and efﬁcient communication and task performance. We propose that these negative ﬁndings could be attributed to how the technology was used and propose the novel concept of communication orchestration aimed at mitigating some of the signaled limitations. Orchestration is a selection process for displaying information that is deemed relevant for accomplishing an effective and efﬁcient task performance and communicative experience. We report an experiment that conﬁrmed this suggestion. The results indicate that orchestration could be an important novel feature to aid humans when communicating via VMC, but also suggest that there is potential for further improvements in orchestration.},
	language = {en},
	number = {5},
	urldate = {2019-09-29},
	journal = {Computers in Human Behavior},
	author = {Groen, Martin and Ursu, Marian and Michalakopoulos, Spiros and Falelakis, Manolis and Gasparis, Epameinondas},
	month = sep,
	year = {2012},
	pages = {1575--1579},
	file = {Groen et al. - 2012 - Improving video-mediated communication with orches.pdf:/Users/rosscutler/Zotero/storage/E8VNA84F/Groen et al. - 2012 - Improving video-mediated communication with orches.pdf:application/pdf},
}

@inproceedings{kirk_home_2010,
	address = {Savannah, Georgia, USA},
	title = {Home video communication: mediating 'closeness'},
	isbn = {978-1-60558-795-0},
	shorttitle = {Home video communication},
	url = {http://portal.acm.org/citation.cfm?doid=1718918.1718945},
	doi = {10.1145/1718918.1718945},
	abstract = {Video-mediated communication (VMC) technologies are becoming rapidly adopted by home users. Little research has previously been conducted into why home users would choose to use VMC or their practices surrounding its use. We present the results of an interview and diary-based study of 17 people about their uses of, and attitudes towards, VMC. We highlight the artful ways in which users appropriate VMC to reconcile a desire for closeness with those with whom they communicate, and we explore the rich ways in which VMC supports different expressions of this desire. We conclude with discussions of how nextgeneration VMC technologies might be designed to take advantage of this understanding of human values in communicative practice.},
	language = {en},
	urldate = {2019-09-29},
	booktitle = {Proceedings of the 2010 {ACM} conference on {Computer} supported cooperative work - {CSCW} '10},
	publisher = {ACM Press},
	author = {Kirk, David S. and Sellen, Abigail and Cao, Xiang},
	year = {2010},
	pages = {135},
	file = {Kirk et al. - 2010 - Home video communication mediating 'closeness'.pdf:/Users/rosscutler/Zotero/storage/NMEXXY4Z/Kirk et al. - 2010 - Home video communication mediating 'closeness'.pdf:application/pdf},
}

@article{daly-jones_advantages_1998,
	title = {Some advantages of video conferencing over high-quality audio conferencing: fluency and awareness of attentional focus},
	volume = {49},
	issn = {10715819},
	shorttitle = {Some advantages of video conferencing over high-quality audio conferencing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581998901951},
	doi = {10.1006/ijhc.1998.0195},
	language = {en},
	number = {1},
	urldate = {2019-09-29},
	journal = {International Journal of Human-Computer Studies},
	author = {Daly-Jones, Owen and Monk, Andrew and Watts, Leon},
	month = jul,
	year = {1998},
	pages = {21--58},
	file = {Daly-Jones et al. - 1998 - Some advantages of video conferencing over high-qu.pdf:/Users/rosscutler/Zotero/storage/HTLDCLGF/Daly-Jones et al. - 1998 - Some advantages of video conferencing over high-qu.pdf:application/pdf;Daly-Jones et al. - 1998 - Some advantages of video conferencing over high-qu.pdf:/Users/rosscutler/Zotero/storage/XH75UZ63/Daly-Jones et al. - 1998 - Some advantages of video conferencing over high-qu.pdf:application/pdf},
}

@inproceedings{cutler_study_2019,
	address = {Redmond, WA, USA},
	title = {A {Study} of {Meeting} {Effectiveness} and {Inclusion} in {Teams}},
	author = {Cutler, Ross and Hosseinkashi, Yasaman and Filipi, Senja and Pool,, Jamie and Aichner, Robert and Gehrke, Johannes},
	month = nov,
	year = {2019},
}

@article{you_imagenet_2017,
	title = {{ImageNet} {Training} in {Minutes}},
	url = {http://arxiv.org/abs/1709.05011},
	abstract = {Since its creation, the ImageNet-1k benchmark set has played a signiﬁcant role as a benchmark for ascertaining the accuracy of different deep neural net (DNN) models on the classiﬁcation problem. Moreover, in recent years it has also served as the principal benchmark for assessing different approaches to DNN training. Finishing a 90-epoch ImageNet-1k training with ResNet-50 on a NVIDIA M40 GPU takes 14 days. This training requires 1018 single precision operations in total. On the other hand, the world’s current fastest supercomputer can ﬁnish 2 × 1017 single precision operations per second. If we can make full use of the computing capability of the fastest supercomputer for DNN training, we should be able to ﬁnish the 90-epoch ResNet-50 training in ﬁve seconds. Over the last two years, a number of researchers have focused on closing this signiﬁcant performance gap through scaling DNN training to larger numbers of processors. Most successful approaches to scaling ImageNet training have used the synchronous stochastic gradient descent. However, to scale synchronous stochastic gradient descent one must also increase the batch size used in each iteration.},
	language = {en},
	urldate = {2019-09-26},
	journal = {arXiv:1709.05011 [cs]},
	author = {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.05011},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {You et al. - 2017 - ImageNet Training in Minutes.pdf:/Users/rosscutler/Zotero/storage/8SCAUKU3/You et al. - 2017 - ImageNet Training in Minutes.pdf:application/pdf},
}

@article{rieffel_quantum_2019,
	title = {Quantum {Supremacy} {Using} a {Programmable} {Superconducting} {Processor}},
	language = {en},
	author = {Rieffel, Eleanor G},
	year = {2019},
	pages = {12},
	file = {Rieffel - 2019 - Quantum Supremacy Using a Programmable Superconduc.pdf:/Users/rosscutler/Zotero/storage/4ESGHWR9/Rieffel - 2019 - Quantum Supremacy Using a Programmable Superconduc.pdf:application/pdf},
}

@book{keelan_brian_handbook_2002,
	title = {Handbook of {Image} {Quality}: {Characterization} and {Prediction} ({Optical} {Science} and {Engineering})},
	publisher = {CRC Press},
	author = {Keelan, Brian},
	year = {2002},
}

@article{kiesler_group_1992,
	title = {Group decision making and communication technology},
	volume = {52},
	issn = {07495978},
	url = {https://linkinghub.elsevier.com/retrieve/pii/074959789290047B},
	doi = {10.1016/0749-5978(92)90047-B},
	language = {en},
	number = {1},
	urldate = {2019-09-24},
	journal = {Organizational Behavior and Human Decision Processes},
	author = {Kiesler, Sara and Sproull, Lee},
	month = jun,
	year = {1992},
	pages = {96--123},
	file = {Kiesler and Sproull - 1992 - Group decision making and communication technology.pdf:/Users/rosscutler/Zotero/storage/UTLYQC3X/Kiesler and Sproull - 1992 - Group decision making and communication technology.pdf:application/pdf},
}

@article{rogelberg_employee_2010,
	title = {Employee satisfaction with meetings: {A} contemporary facet of job satisfaction},
	volume = {49},
	issn = {00904848, 1099050X},
	shorttitle = {Employee satisfaction with meetings},
	url = {http://doi.wiley.com/10.1002/hrm.20339},
	doi = {10.1002/hrm.20339},
	abstract = {Given the ubiquity, time investment, and theoretical relevance of meetings to work attitudes, this study explored whether organizational science should consider employee satisfaction with meetings as a contemporary, important, and discrete facet of job satisfaction. Using affective events theory, we postulated that meetings are affect-generating events that meaningfully contribute to overall job satisfaction. Two surveys queried working adults: Study 1 used a paper-based survey (n = 201), while Study 2 used an Internet-based survey (n = 785). Satisfaction with meetings was positively related to and signiﬁcantly predicted overall job satisfaction (p {\textless} .05) after controlling for individual difference variables (e.g., participant background variables, negative affect), traditional job satisfaction facets (e.g., work, supervision, pay), and other conceptually relevant constructs (e.g., satisfaction with communication, organizational commitment). Exploratory (Study 1) and conﬁrmatory (Study 2) factor analyses provided evidence that meeting satisfaction is a distinct facet of job satisfaction. Finally, as hypothesized, the relationship between meeting satisfaction and job satisfaction depends in part upon the number of meetings typically attended. The relationship was stronger (more positive) when meeting demands were higher and weaker when meeting demands were lower. Implications for assessment, leadership development, on-boarding, and high potential initiatives are discussed. © 2010 Wiley Periodicals, Inc.},
	language = {en},
	number = {2},
	urldate = {2019-09-24},
	journal = {Human Resource Management},
	author = {Rogelberg, Steven G. and Allen, Joseph A. and Shanock, Linda and Scott, Cliff and Shuffler, Marissa},
	month = mar,
	year = {2010},
	pages = {149--172},
	file = {Rogelberg et al. - 2010 - Employee satisfaction with meetings A contemporar.pdf:/Users/rosscutler/Zotero/storage/8EBPNWMR/Rogelberg et al. - 2010 - Employee satisfaction with meetings A contemporar.pdf:application/pdf},
}

@article{mroz_we_2018,
	title = {Do {We} {Really} {Need} {Another} {Meeting}? {The} {Science} of {Workplace} {Meetings}},
	volume = {27},
	issn = {0963-7214, 1467-8721},
	shorttitle = {Do {We} {Really} {Need} {Another} {Meeting}?},
	url = {http://journals.sagepub.com/doi/10.1177/0963721418776307},
	doi = {10.1177/0963721418776307},
	abstract = {Meetings are routine in organizations, but their value is often questioned by the employees who must sit through them daily. The science of meetings that has emerged as of late provides necessary direction toward improving meetings, but an evaluation of the current state of the science is much needed. In this review, we examine current directions for the psychological science of workplace meetings, with a focus on applying scientific findings about the activities that occur before, during, and after meetings that facilitate success. We conclude with concrete recommendations and a checklist for promoting good meetings, as well as some thoughts on the future of the science of workplace meetings.},
	language = {en},
	number = {6},
	urldate = {2019-09-24},
	journal = {Current Directions in Psychological Science},
	author = {Mroz, Joseph E. and Allen, Joseph A. and Verhoeven, Dana C. and Shuffler, Marissa L.},
	month = dec,
	year = {2018},
	pages = {484--491},
	file = {Mroz et al. - 2018 - Do We Really Need Another Meeting The Science of .pdf:/Users/rosscutler/Zotero/storage/G2X9I5EZ/Mroz et al. - 2018 - Do We Really Need Another Meeting The Science of .pdf:application/pdf},
}

@incollection{montavon_forecasting_2012,
	address = {Berlin, Heidelberg},
	title = {Forecasting with {Recurrent} {Neural} {Networks}: 12 {Tricks}},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	shorttitle = {Forecasting with {Recurrent} {Neural} {Networks}},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8_37},
	abstract = {Recurrent neural networks (RNNs) are typically considered as relatively simple architectures, which come along with complicated learning algorithms. This paper has a diﬀerent view: We start from the fact that RNNs can model any high dimensional, nonlinear dynamical system. Rather than focusing on learning algorithms, we concentrate on the design of network architectures. Unfolding in time is a well-known example of this modeling philosophy. Here a temporal algorithm is transferred into an architectural framework such that the learning can be performed by an extension of standard error backpropagation.},
	language = {en},
	urldate = {2019-09-24},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer Berlin Heidelberg},
	author = {Zimmermann, Hans-Georg and Tietz, Christoph and Grothmann, Ralph},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_37},
	pages = {687--707},
	file = {Zimmermann et al. - 2012 - Forecasting with Recurrent Neural Networks 12 Tri.pdf:/Users/rosscutler/Zotero/storage/WNA59CU2/Zimmermann et al. - 2012 - Forecasting with Recurrent Neural Networks 12 Tri.pdf:application/pdf},
}

@article{zaremba_recurrent_2014,
	title = {Recurrent {Neural} {Network} {Regularization}},
	url = {http://arxiv.org/abs/1409.2329},
	abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overﬁtting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
	language = {en},
	urldate = {2019-09-24},
	journal = {arXiv:1409.2329 [cs]},
	author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.2329},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Zaremba et al. - 2014 - Recurrent Neural Network Regularization.pdf:/Users/rosscutler/Zotero/storage/B7BX6XJC/Zaremba et al. - 2014 - Recurrent Neural Network Regularization.pdf:application/pdf},
}

@article{hinton_lecture_nodate,
	title = {Lecture 10 {Recurrent} neural networks},
	language = {en},
	author = {Hinton, Geoffrey},
	pages = {57},
	file = {Hinton - Lecture 10 Recurrent neural networks.pdf:/Users/rosscutler/Zotero/storage/4V9PDZVZ/Hinton - Lecture 10 Recurrent neural networks.pdf:application/pdf},
}

@article{forcada_learning_1995,
	title = {Learning the {Initial} {State} of a {Second}-{Order} {Recurrent} {Neural} {Network} during {Regular}-{Language} {Inference}},
	volume = {7},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1995.7.5.923},
	doi = {10.1162/neco.1995.7.5.923},
	language = {en},
	number = {5},
	urldate = {2019-09-24},
	journal = {Neural Computation},
	author = {Forcada, Mikel L. and Carrasco, Rafael C.},
	month = sep,
	year = {1995},
	pages = {923--930},
}

@inproceedings{zaki_adaptive_2015,
	address = {London, United Kingdom},
	title = {Adaptive {Congestion} {Control} for {Unpredictable} {Cellular} {Networks}},
	isbn = {978-1-4503-3542-3},
	url = {http://dl.acm.org/citation.cfm?doid=2785956.2787498},
	doi = {10.1145/2785956.2787498},
	abstract = {Legacy congestion controls including TCP and its variants are known to perform poorly over cellular networks due to highly variable capacities over short time scales, self-inﬂicted packet delays, and packet losses unrelated to congestion. To cope with these challenges, we present Verus, an end-to-end congestion control protocol that uses delay measurements to react quickly to the capacity changes in cellular networks without explicitly attempting to predict the cellular channel dynamics. The key idea of Verus is to continuously learn a delay proﬁle that captures the relationship between end-to-end packet delay and outstanding window size over short epochs and uses this relationship to increment or decrement the window size based on the observed short-term packet delay variations. While the delay-based control is primarily for congestion avoidance, Verus uses standard TCP features including multiplicative decrease upon packet loss and slow start.},
	language = {en},
	urldate = {2019-09-20},
	booktitle = {Proceedings of the 2015 {ACM} {Conference} on {Special} {Interest} {Group} on {Data} {Communication} - {SIGCOMM} '15},
	publisher = {ACM Press},
	author = {Zaki, Yasir and Pötsch, Thomas and Chen, Jay and Subramanian, Lakshminarayanan and Görg, Carmelita},
	year = {2015},
	pages = {509--522},
	file = {Zaki et al. - 2015 - Adaptive Congestion Control for Unpredictable Cell.pdf:/Users/rosscutler/Zotero/storage/FF3JUZY5/Zaki et al. - 2015 - Adaptive Congestion Control for Unpredictable Cell.pdf:application/pdf},
}

@inproceedings{reddy_supervised_2019,
	title = {Supervised {Classifiers} for {Audio} {Impairments} with {Noisy} {Labels}},
	url = {http://arxiv.org/abs/1907.01742},
	abstract = {Voice-over-Internet-Protocol (VoIP) calls are prone to various speech impairments due to environmental and network conditions resulting in bad user experience. A reliable audio impairment classifier helps to identify the cause for bad audio quality. The user feedback after the call can act as the ground truth labels for training a supervised classifier on a large audio dataset. However, the labels are noisy as most of the users lack the expertise to precisely articulate the impairment in the perceived speech. In this paper, we analyze the effects of massive noise in labels in training dense networks and Convolutional Neural Networks (CNN) using engineered features, spectrograms and raw audio samples as inputs. We demonstrate that CNN can generalize better on the training data with a large number of noisy labels and gives remarkably higher test performance. The classifiers were trained both on randomly generated label noise and the label noise introduced by human errors. We also show that training with noisy labels requires a significant increase in the training dataset size, which is in proportion to the amount of noise in the labels.},
	urldate = {2019-08-09},
	booktitle = {{ICASSP}},
	author = {Reddy, Chandan K. A. and Cutler, Ross and Gehrke, Johannes},
	month = may,
	year = {2019},
	note = {arXiv: 1907.01742},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {arXiv\:1907.01742 PDF:/Users/rosscutler/Zotero/storage/5YBESSW6/Reddy et al. - 2019 - Supervised Classifiers for Audio Impairments with .pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/GGIRV4D4/1907.html:text/html;Reddy et al. - 2019 - Supervised Classifiers for Audio Impairments with .pdf:/Users/rosscutler/Zotero/storage/DSNZMCGT/Reddy et al. - 2019 - Supervised Classifiers for Audio Impairments with .pdf:application/pdf},
}

@inproceedings{reddy_scalable_2019,
	title = {A {Scalable} {Noisy} {Speech} {Dataset} and {Online} {Subjective} {Test} {Framework}},
	url = {http://www.isca-speech.org/archive/Interspeech_2019/abstracts/3087.html},
	doi = {10.21437/Interspeech.2019-3087},
	abstract = {Background noise is a major source of quality impairments in Voice over Internet Protocol (VoIP) and Public Switched Telephone Network (PSTN) calls. Recent work shows the efficacy of deep learning for noise suppression, but the datasets have been relatively small compared to those used in other domains (e.g., ImageNet) and the associated evaluations have been more focused. In order to better facilitate deep learning research in Speech Enhancement, we present a noisy speech dataset (MS-SNSD) that can scale to arbitrary sizes depending on the number of speakers, noise types, and Speech to Noise Ratio (SNR) levels desired. We show that increasing dataset sizes increases noise suppression performance as expected. In addition, we provide an open-source evaluation methodology to evaluate the results subjectively at scale using crowdsourcing, with a reference algorithm to normalize the results. To demonstrate the dataset and evaluation framework we apply it to several noise suppressors and compare the subjective Mean Opinion Score (MOS) with objective quality measures such as SNR, PESQ, POLQA, and VISQOL and show why MOS is still required. Our subjective MOS evaluation is the first large scale evaluation of Speech Enhancement algorithms that we are aware of.},
	language = {en},
	urldate = {2019-09-20},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Reddy, Chandan K.A. and Beyrami, Ebrahim and Pool, Jamie and Cutler, Ross and Srinivasan, Sriram and Gehrke, Johannes},
	month = sep,
	year = {2019},
	pages = {1816--1820},
	file = {Reddy et al. - 2019 - A Scalable Noisy Speech Dataset and Online Subject.pdf:/Users/rosscutler/Zotero/storage/TAHMERRX/Reddy et al. - 2019 - A Scalable Noisy Speech Dataset and Online Subject.pdf:application/pdf},
}

@article{luong_applications_2019,
	title = {Applications of {Deep} {Reinforcement} {Learning} in {Communications} and {Networking}: {A} {Survey}},
	shorttitle = {Applications of {Deep} {Reinforcement} {Learning} in {Communications} and {Networking}},
	doi = {10.1109/COMST.2019.2916583},
	abstract = {This paper presents a comprehensive literature review on applications of deep reinforcement learning in communications and networking. Modern networks, e.g., Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become more decentralized and autonomous. In such networks, network entities need to make decisions locally to maximize the network performance under uncertainty of network environment. Reinforcement learning has been efficiently used to enable the network entities to obtain the optimal policy including, e.g., decisions or actions, given their states when the state and action spaces are small. However, in complex and large-scale networks, the state and action spaces are usually large, and the reinforcement learning may not be able to find the optimal policy in reasonable time. Therefore, deep reinforcement learning, a combination of reinforcement learning with deep learning, has been developed to overcome the shortcomings. In this survey, we first give a tutorial of deep reinforcement learning from fundamental concepts to advanced models. Then, we review deep reinforcement learning approaches proposed to address emerging issues in communications and networking. The issues include dynamic network access, data rate control, wireless caching, data offloading, network security, and connectivity preservation which are all important to next generation networks such as 5G and beyond. Furthermore, we present applications of deep reinforcement learning for traffic routing, resource sharing, and data collection. Finally, we highlight important challenges, open issues, and future research directions of applying deep reinforcement learning.},
	journal = {IEEE Communications Surveys Tutorials},
	author = {Luong, N. C. and Hoang, D. T. and Gong, S. and Niyato, D. and Wang, P. and Liang, Y. and Kim, D. I.},
	year = {2019},
	keywords = {Ad hoc networks, caching, communications, data collection., data offloading, Deep learning, deep Q-learning, Deep reinforcement learning, Markov processes, networking, rate control, Reinforcement learning, security, Security, spectrum access, Tutorials, Unmanned aerial vehicles},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/2PIL4ZNV/8714026.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/J43YVBEK/Luong et al. - 2019 - Applications of Deep Reinforcement Learning in Com.pdf:application/pdf},
}

@article{banks_gender_nodate,
	title = {{GENDER} {BIAS} {IN} {THE} {CLASSROOM}},
	volume = {14},
	language = {en},
	author = {Banks, Taunya Lovell},
	pages = {17},
	file = {Banks - GENDER BIAS IN THE CLASSROOM.pdf:/Users/rosscutler/Zotero/storage/S4IY9F8C/Banks - GENDER BIAS IN THE CLASSROOM.pdf:application/pdf},
}

@article{standaert_effectiveness_nodate,
	title = {Effectiveness of {Communication} {Technologies} for {Distributed} {Business} {Meetings}},
	language = {en},
	author = {Standaert, Willem},
	pages = {224},
	file = {Standaert - Effectiveness of Communication Technologies for Di.pdf:/Users/rosscutler/Zotero/storage/9WYKAE7V/Standaert - Effectiveness of Communication Technologies for Di.pdf:application/pdf},
}

@article{cohen_meeting_2011,
	title = {Meeting design characteristics and attendee perceptions of staff/team meeting quality.},
	volume = {15},
	issn = {1930-7802, 1089-2699},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0021549},
	doi = {10.1037/a0021549},
	language = {en},
	number = {1},
	urldate = {2019-09-09},
	journal = {Group Dynamics: Theory, Research, and Practice},
	author = {Cohen, Melissa A. and Rogelberg, Steven G. and Allen, Joseph A. and Luong, Alexandra},
	year = {2011},
	pages = {90--104},
	file = {Cohen et al. - 2011 - Meeting design characteristics and attendee percep.pdf:/Users/rosscutler/Zotero/storage/MNX7EPGI/Cohen et al. - 2011 - Meeting design characteristics and attendee percep.pdf:application/pdf},
}

@article{allen_meetings_2016,
	title = {Meetings as a positive boost? {How} and when meeting satisfaction impacts employee empowerment},
	volume = {69},
	issn = {01482963},
	shorttitle = {Meetings as a positive boost?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S014829631630131X},
	doi = {10.1016/j.jbusres.2016.04.011},
	abstract = {Meetings constitute an important context for understanding organizational behavior and employee attitudes. Employees spend ever-increasing time in meetings and often complain about their meetings. In contrast, we explore the positive side of meetings and argue that satisfying meetings can empower rather than deplete individual employees. We gathered time-lagged data from an online sample of working adults in the U.S. As hypothesized, meeting satisfaction predicted employee empowerment, and information availability partially mediated this effect. Moreover, we found that these effects were stronger when employees participated in more meetings: Meeting demands moderated the link between meeting satisfaction and information availability as well as the positive, indirect effect of meeting satisfaction (through information availability) on psychological empowerment. Our ﬁndings underscore the relevance of workplace meetings for managing and promoting positive employee attitudes. We discuss implications for meeting science and the value of satisfying meetings as a managerial tool for promoting empowerment.},
	language = {en},
	number = {10},
	urldate = {2019-09-08},
	journal = {Journal of Business Research},
	author = {Allen, Joseph A. and Lehmann-Willenbrock, Nale and Sands, Stephanie J.},
	month = oct,
	year = {2016},
	pages = {4340--4347},
	file = {Allen et al. - 2016 - Meetings as a positive boost How and when meeting.pdf:/Users/rosscutler/Zotero/storage/XXJT6CKJ/Allen et al. - 2016 - Meetings as a positive boost How and when meeting.pdf:application/pdf},
}

@incollection{lowe_multi-agent_2017,
	title = {Multi-{Agent} {Actor}-{Critic} for {Mixed} {Cooperative}-{Competitive} {Environments}},
	url = {http://papers.nips.cc/paper/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.pdf},
	urldate = {2019-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Lowe, Ryan and WU, YI and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {6379--6390},
	file = {NIPS Full Text PDF:/Users/rosscutler/Zotero/storage/NXZUQ6HC/Lowe et al. - 2017 - Multi-Agent Actor-Critic for Mixed Cooperative-Com.pdf:application/pdf;NIPS Snapshot:/Users/rosscutler/Zotero/storage/28WHX5US/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.html:text/html},
}

@article{awan_scalable_2018,
	title = {Scalable {Distributed} {DNN} {Training} using {TensorFlow} and {CUDA}-{Aware} {MPI}: {Characterization}, {Designs}, and {Performance} {Evaluation}},
	shorttitle = {Scalable {Distributed} {DNN} {Training} using {TensorFlow} and {CUDA}-{Aware} {MPI}},
	url = {http://arxiv.org/abs/1810.11112},
	abstract = {The current wave of advances in Machine Learning (ML) and Deep Learning (DL) have been triggered by the availability of large-scale datasets, efﬁcient CPU and GPU hardware, and development of easy-to-use software frameworks like TensorFlow (TF), Caffe and Torch. TensorFlow has been, by far, the most widely adopted ML/DL framework. However, little exists in the literature that provides a thorough understanding of the capabilities which TensorFlow offers for the distributed training of large ML/DL models that need computation and communication at scale. Most commonly used distributed training approaches for TF can be categorized as follows: 1) Google Remote Procedure Call (gRPC), 2) gRPC+‘X’: X = (InﬁniBand Verbs, Message Passing Interface (MPI), and GPUDirect RDMA), and 3) No-gRPC: Baidu Allreduce with MPI, Horovod with MPI, and Horovod with NVIDIA NCCL. In this paper, we provide an in-depth performance characterization and analysis of these distributed training approaches on various GPU clusters including the Piz Daint system (\#6 on Top500). We perform experiments to gain novel insights along the following vectors: 1) Application-level scalability of DNN training, 2) Effect of Batch Size on scaling efﬁciency, 3) Impact of the MPI library used for no-gRPC approaches, and 4) Type and size of DNN architectures (e.g ResNet vs. MobileNet). Based on these experiments, we present two key insights: 1) Overall, No-gRPC designs achieve better performance compared to gRPC-based approaches for most conﬁgurations, and 2) The performance of No-gRPC is heavily inﬂuenced by the gradient aggregation using the Allreduce communication pattern. Finally, we propose a truly CUDAAware MPI Allreduce design that exploits 1) CUDA kernels to perform large reductions on the GPU and 2) A pointer cache to avoid overheads involved in queries to the CUDA driver. Our proposed designs have been implemented in MVAPICH2-GDR and offer 5-17× better performance than NCCL2 for small and medium messages, and reduces latency by 29\% for large messages on 16 GPUs (nodes). The proposed optimizations help HorovodMPI to achieve approximately 90\% scaling efﬁciency for ResNet50 training on 64 GPUs. Further, Horovod-MPI achieves 1.8× and 3.2× higher throughput than the native gRPC method for ResNet-50 and MobileNet, respectively, on the Piz Daint cluster.},
	language = {en},
	urldate = {2019-09-04},
	journal = {arXiv:1810.11112 [cs]},
	author = {Awan, Ammar Ahmad and Bedorf, Jeroen and Chu, Ching-Hsiang and Subramoni, Hari and Panda, Dhabaleswar K.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.11112},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Awan et al. - 2018 - Scalable Distributed DNN Training using TensorFlow.pdf:/Users/rosscutler/Zotero/storage/6KAKQTKJ/Awan et al. - 2018 - Scalable Distributed DNN Training using TensorFlow.pdf:application/pdf},
}

@article{mayer_scalable_2019,
	title = {Scalable {Deep} {Learning} on {Distributed} {Infrastructures}: {Challenges}, {Techniques} and {Tools}},
	shorttitle = {Scalable {Deep} {Learning} on {Distributed} {Infrastructures}},
	url = {http://arxiv.org/abs/1903.11314},
	abstract = {Deep Learning (DL) has had an immense success in the recent past, leading to state-of-the-art results in various domains such as image recognition and natural language processing. One of the reasons for this success is the increasing size of DL models and the proliferation of vast amounts of training data being available. To keep on improving the performance of DL, increasing the scalability of DL systems is necessary. In this survey, we perform a broad and thorough investigation on challenges, techniques and tools for scalable DL on distributed infrastructures. This incorporates infrastructures for DL, methods for parallel DL training, multi-tenant resource scheduling and the management of training and model data. Further, we analyze and compare 11 current open-source DL frameworks and tools and investigate which of the techniques are commonly implemented in practice. Finally, we highlight future research trends in DL systems that deserve further research.},
	language = {en},
	urldate = {2019-09-04},
	journal = {arXiv:1903.11314 [cs]},
	author = {Mayer, Ruben and Jacobsen, Hans-Arno},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.11314},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Mayer and Jacobsen - 2019 - Scalable Deep Learning on Distributed Infrastructu.pdf:/Users/rosscutler/Zotero/storage/Z7NKH4D9/Mayer and Jacobsen - 2019 - Scalable Deep Learning on Distributed Infrastructu.pdf:application/pdf},
}

@misc{noauthor_realtime_nodate,
	title = {Realtime {Mobile} {Bandwidth} {Prediction} using {LSTM} {Neural} {Network}},
	url = {https://github.com/NYU-METS/Main},
	abstract = {NYU Metropolitan Mobile Bandwidth Trace. Contribute to NYU-METS/Main development by creating an account on GitHub.},
	language = {en},
	urldate = {2019-09-04},
	journal = {GitHub},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/E62LABGJ/Realtime_Mobile_Bandwidth_Prediction_using_LSTM_Neural_Network.html:text/html},
}

@article{ren_time-series_2019,
	title = {Time-{Series} {Anomaly} {Detection} {Service} at {Microsoft}},
	url = {http://arxiv.org/abs/1906.03821},
	doi = {10.1145/3292500.3330680},
	abstract = {Large companies need to monitor various metrics (for example, Page Views and Revenue) of their applications and services in real time. At Microsoft, we develop a time-series anomaly detection service which helps customers to monitor the time-series continuously and alert for potential incidents on time. In this paper, we introduce the pipeline and algorithm of our anomaly detection service, which is designed to be accurate, efficient and general. The pipeline consists of three major modules, including data ingestion, experimentation platform and online compute. To tackle the problem of time-series anomaly detection, we propose a novel algorithm based on Spectral Residual (SR) and Convolutional Neural Network (CNN). Our work is the first attempt to borrow the SR model from visual saliency detection domain to time-series anomaly detection. Moreover, we innovatively combine SR and CNN together to improve the performance of SR model. Our approach achieves superior experimental results compared with state-of-the-art baselines on both public datasets and Microsoft production data.},
	urldate = {2019-08-29},
	journal = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining  - KDD '19},
	author = {Ren, Hansheng and Xu, Bixiong and Wang, Yujing and Yi, Chao and Huang, Congrui and Kou, Xiaoyu and Xing, Tony and Yang, Mao and Tong, Jie and Zhang, Qi},
	year = {2019},
	note = {arXiv: 1906.03821},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3009--3017},
	file = {arXiv\:1906.03821 PDF:/Users/rosscutler/Zotero/storage/NPUU54RZ/Ren et al. - 2019 - Time-Series Anomaly Detection Service at Microsoft.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/MYT8HT3R/1906.html:text/html},
}

@article{gu_q-prop:_2016,
	title = {Q-{Prop}: {Sample}-{Efficient} {Policy} {Gradient} with {An} {Off}-{Policy} {Critic}},
	shorttitle = {Q-{Prop}},
	url = {http://arxiv.org/abs/1611.02247},
	abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
	urldate = {2019-08-28},
	journal = {arXiv:1611.02247 [cs]},
	author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02247},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1611.02247 PDF:/Users/rosscutler/Zotero/storage/YTB3DCCB/Gu et al. - 2016 - Q-Prop Sample-Efficient Policy Gradient with An O.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/TICLP6D6/1611.html:text/html},
}

@article{schulman_high-dimensional_2015,
	title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
	url = {http://arxiv.org/abs/1506.02438},
	abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
	urldate = {2019-08-28},
	journal = {arXiv:1506.02438 [cs]},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02438},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv\:1506.02438 PDF:/Users/rosscutler/Zotero/storage/8FRZBSV5/Schulman et al. - 2015 - High-Dimensional Continuous Control Using Generali.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/9NJLJP4T/1506.html:text/html},
}

@article{lillicrap_continuous_2015,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2019-08-28},
	journal = {arXiv:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1509.02971 PDF:/Users/rosscutler/Zotero/storage/WEMKM2UT/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/BREA2RY8/1509.html:text/html},
}

@misc{noauthor_ddpg_nodate,
	title = {ddpg paper - {Google} {Search}},
	url = {https://www.google.com/search?q=ddpg+paper&rlz=1C1CHBF_enUS812US812&oq=DDPG+paper&aqs=chrome.0.35i39j0l3j69i65.2980j1j8&sourceid=chrome&ie=UTF-8},
	urldate = {2019-08-28},
	file = {ddpg paper - Google Search:/Users/rosscutler/Zotero/storage/5BRMRGQS/search.html:text/html},
}

@inproceedings{chen_quantifying_2006,
	address = {New York, NY, USA},
	series = {{SIGCOMM} '06},
	title = {Quantifying {Skype} {User} {Satisfaction}},
	isbn = {978-1-59593-308-9},
	url = {http://doi.acm.org/10.1145/1159913.1159959},
	doi = {10.1145/1159913.1159959},
	abstract = {The success of Skype has inspired a generation of peer-to-peer-based solutions for satisfactory real-time multimedia services over the Internet. However, fundamental questions, such as whether VoIP services like Skype are good enough in terms of user satisfaction,have not been formally addressed. One of the major challenges lies in the lack of an easily accessible and objective index to quantify the degree of user satisfaction.In this work, we propose a model, geared to Skype, but generalizable to other VoIP services, to quantify VoIP user satisfaction based on a rigorous analysis of the call duration from actual Skype traces. The User Satisfaction Index (USI) derived from the model is unique in that 1) it is composed by objective source-and network-level metrics, such as the bit rate, bit rate jitter, and round-trip time, 2) unlike speech quality measures based on voice signals, such as the PESQ model standardized by ITU-T, the metrics are easily accessible and computable for real-time adaptation, and 3) the model development only requires network measurements, i.e., no user surveys or voice signals are necessary. Our model is validated by an independent set of metrics that quantifies the degree of user interaction from the actual traces.},
	urldate = {2019-08-27},
	booktitle = {Proceedings of the 2006 {Conference} on {Applications}, {Technologies}, {Architectures}, and {Protocols} for {Computer} {Communications}},
	publisher = {ACM},
	author = {Chen, Kuan-Ta and Huang, Chun-Ying and Huang, Polly and Lei, Chin-Laung},
	year = {2006},
	note = {event-place: Pisa, Italy},
	keywords = {human perception, internet measurement, quality of service, survival analysis, VoIP, wavelet denoising},
	pages = {399--410},
	file = {ACM Full Text PDF:/Users/rosscutler/Zotero/storage/Q5TEI9LQ/Chen et al. - 2006 - Quantifying Skype User Satisfaction.pdf:application/pdf},
}

@article{goyal_accurate_2017,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	shorttitle = {Accurate, {Large} {Minibatch} {SGD}},
	url = {http://arxiv.org/abs/1706.02677},
	abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efﬁcient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difﬁculties, but when these are addressed the trained networks exhibit good generalization. Speciﬁcally, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyperparameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ∼90\% scaling efﬁciency when moving from 8 to 256 GPUs. Our ﬁndings enable training visual recognition models on internet-scale data with high efﬁciency.},
	language = {en},
	urldate = {2019-08-26},
	journal = {arXiv:1706.02677 [cs]},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02677},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Goyal et al. - 2017 - Accurate, Large Minibatch SGD Training ImageNet i.pdf:/Users/rosscutler/Zotero/storage/NTKPRJ7G/Goyal et al. - 2017 - Accurate, Large Minibatch SGD Training ImageNet i.pdf:application/pdf;Goyal et al. - 2017 - Accurate, Large Minibatch SGD Training ImageNet i.pdf:/Users/rosscutler/Zotero/storage/CWH2NCFI/Goyal et al. - 2017 - Accurate, Large Minibatch SGD Training ImageNet i.pdf:application/pdf},
}

@article{loizou_speech_nodate,
	title = {Speech {Quality} {Assessment}},
	abstract = {This chapter provides an overview of the various methods and techniques used for assessment of speech quality. A summary is given of some of the most commonly used listening tests designed to obtain reliable ratings of the quality of processed speech from human listeners. Considerations for conducting successful subjective listening tests are given along with cautions that need to be exercised. While the listening tests are considered the gold standard in terms of assessment of speech quality, they can be costly and time consuming. For that reason, much research effort has been placed on devising objective measures that correlate highly with subjective rating scores. An overview of some of the most commonly used objective measures is provided along with a discussion on how well they correlate with subjective listening tests.},
	language = {en},
	author = {Loizou, Philipos C},
	pages = {32},
	file = {Loizou - Speech Quality Assessment.pdf:/Users/rosscutler/Zotero/storage/C7ZWJFBW/Loizou - Speech Quality Assessment.pdf:application/pdf},
}

@article{hands_basic_2004,
	title = {A basic multimedia quality model},
	volume = {6},
	issn = {1520-9210},
	doi = {10.1109/TMM.2004.837233},
	abstract = {This paper describes two experiments designed to develop a basic multimedia predictive quality metric. In Experiment 1, two head and shoulder audio-video sequences were used for test material. Experiment 2 used one of the head and shoulder sequences from Experiment 1 together with a different, high-motion sequence. In both experiments, subjects assessed the audio quality first, followed by the video quality and finally a third test evaluated multimedia quality. The results of these studies found that human subjects integrate audio and video quality together using a multiplicative rule. A regression analysis using the subjective quality test data from each experiment found that: 1) for head and shoulder content, both modalities contribute significantly to the predictive power of the resultant model, although audio quality is weighted slightly higher than video quality and 2) for high-motion content, video quality is weighted significantly higher than audio quality.},
	number = {6},
	journal = {IEEE Transactions on Multimedia},
	author = {Hands, D. S.},
	month = dec,
	year = {2004},
	keywords = {quality of service, audio quality, audio-video sequence, basic multimedia predictive quality metric, Bit rate, Communication industry, Context-aware services, high-motion sequence, Humans, Materials testing, Multimedia, multimedia communication, Multimedia communication, objective measurement, perceptual quality, Performance evaluation, Power system modeling, Predictive models, regression analysis, Regression analysis, shoulder sequence, video quality, video signal processing, visual perception},
	pages = {806--816},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/XZLGR2K9/1359861.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/SIYP8JGD/Hands - 2004 - A basic multimedia quality model.pdf:application/pdf},
}

@article{leach_perceived_2009,
	title = {Perceived {Meeting} {Effectiveness}: {The} {Role} of {Design} {Characteristics}},
	volume = {24},
	issn = {0889-3268, 1573-353X},
	shorttitle = {Perceived {Meeting} {Effectiveness}},
	url = {http://link.springer.com/10.1007/s10869-009-9092-6},
	doi = {10.1007/s10869-009-9092-6},
	abstract = {Purpose The aim of this investigation was to test hypotheses about meeting design characteristics (punctuality, chairperson, etc.) in relation to attendees’ perceptions of meeting effectiveness.},
	language = {en},
	number = {1},
	urldate = {2019-08-21},
	journal = {Journal of Business and Psychology},
	author = {Leach, Desmond J. and Rogelberg, Steven G. and Warr, Peter B. and Burnfield, Jennifer L.},
	month = mar,
	year = {2009},
	pages = {65--76},
	file = {Leach et al. - 2009 - Perceived Meeting Effectiveness The Role of Desig.pdf:/Users/rosscutler/Zotero/storage/6F9QMKV5/Leach et al. - 2009 - Perceived Meeting Effectiveness The Role of Desig.pdf:application/pdf},
}

@inproceedings{eecke_influence_2016,
	title = {On the {Influence} of {Gender} on {Interruptions} in {Multiparty} {Dialogue}},
	url = {http://www.isca-speech.org/archive/Interspeech_2016/abstracts/0951.html},
	doi = {10.21437/Interspeech.2016-951},
	abstract = {During conversations, participants do not always alternate turns smoothly. One cause of disturbance particularly prominent in multiparty dialogue is the presence of interruptions: interventions that prevent current speakers from ﬁnishing their turns. Previous work, mostly within the ﬁeld of sociolinguistics, has suggested that the gender of the dialogue participants plays an important role in their interruptive behaviour. We investigate existing hypotheses in this respect by systematically analysing interruptions in a corpus of spoken multiparty meetings that include a minimum of two male and two female participants. We ﬁnd a number of signiﬁcant differences, including the fact that women are more often interrupted overall and that men interrupt more often women than other men, in particular using speech overlap to grab the ﬂoor. We do not ﬁnd evidence for the hypothesis that women interrupt other women more frequently than they interrupt men.},
	language = {en},
	urldate = {2019-08-21},
	author = {Eecke, Paul Van and Fernández, Raquel},
	month = sep,
	year = {2016},
	pages = {2070--2074},
	file = {Eecke and Fernández - 2016 - On the Influence of Gender on Interruptions in Mul.pdf:/Users/rosscutler/Zotero/storage/IXHA8Q8C/Eecke and Fernández - 2016 - On the Influence of Gender on Interruptions in Mul.pdf:application/pdf},
}

@inproceedings{bicharra_garcia_cutting_2004,
	address = {Chicago, Illinois, USA},
	title = {Cutting to the chase: improving meeting effectiveness by focusing on the agenda},
	isbn = {978-1-58113-810-8},
	shorttitle = {Cutting to the chase},
	url = {http://portal.acm.org/citation.cfm?doid=1031607.1031664},
	doi = {10.1145/1031607.1031664},
	abstract = {We propose an agenda planning technique with a built-in incentive mechanism, based on the VCG (Vickrey-ClarkeGroves) method from game theory, to help project managers in the engineering construction industry create a more effective agenda. Preliminary results have shown an improvement in both instrumented and perceived meeting quality.},
	language = {en},
	urldate = {2019-08-21},
	booktitle = {Proceedings of the 2004 {ACM} conference on {Computer} supported cooperative work  - {CSCW} '04},
	publisher = {ACM Press},
	author = {Bicharra Garcia, Ana Cristina and Kunz, John and Fischer, Martin},
	year = {2004},
	pages = {346},
	file = {Bicharra Garcia et al. - 2004 - Cutting to the chase improving meeting effectiven.pdf:/Users/rosscutler/Zotero/storage/CW475QU3/Bicharra Garcia et al. - 2004 - Cutting to the chase improving meeting effectiven.pdf:application/pdf},
}

@article{geimer_meetings_2015,
	title = {Meetings at work: {Perceived} effectiveness and recommended improvements},
	volume = {68},
	issn = {01482963},
	shorttitle = {Meetings at work},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0148296315000879},
	doi = {10.1016/j.jbusres.2015.02.015},
	abstract = {This study investigates why a large proportion of meetings continue to be regarded as a poor use of time, despite a substantial body of literature on how to make improvements. Employees from 41 countries provide comments on the effectiveness of their typical meetings and how to improve effectiveness. Less than half the respondents describe meetings as an effective use of time. The results suggest that employees are often invited to meetings of little personal relevance and many meeting organizers fail to apply fundamental meeting design practices. The ﬁndings show differences in response patterns for country of origin, job status (part- or full-time), and organizational type, but not for gender, supervisory status, and organizational tenure. The study provides illustrative comments about forms of effectiveness/ineffectiveness and forms of improvement, and discusses the implications with respect to theory development, future research, and practice.},
	language = {en},
	number = {9},
	urldate = {2019-08-21},
	journal = {Journal of Business Research},
	author = {Geimer, Jennifer L. and Leach, Desmond J. and DeSimone, Justin A. and Rogelberg, Steven G. and Warr, Peter B.},
	month = sep,
	year = {2015},
	pages = {2015--2026},
	file = {Geimer et al. - 2015 - Meetings at work Perceived effectiveness and reco.pdf:/Users/rosscutler/Zotero/storage/8Q2QL9M3/Geimer et al. - 2015 - Meetings at work Perceived effectiveness and reco.pdf:application/pdf},
}

@article{luo_conv-tasnet:_2019,
	title = {Conv-{TasNet}: {Surpassing} {Ideal} {Time}-{Frequency} {Magnitude} {Masking} for {Speech} {Separation}},
	volume = {27},
	issn = {2329-9290, 2329-9304},
	shorttitle = {Conv-{TasNet}},
	url = {http://arxiv.org/abs/1809.07454},
	doi = {10.1109/TASLP.2019.2915167},
	abstract = {Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufﬁcient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully-convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modiﬁed encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network (TCN) consisting of stacked 1-D dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system signiﬁcantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, ConvTasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a signiﬁcantly smaller model size and a shorter minimum latency, making it a suitable solution for both ofﬂine and real-time speech separation applications. This study therefore represents a major step toward the realization of speech separation systems for real-world speech processing technologies.},
	language = {en},
	number = {8},
	urldate = {2019-08-20},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Luo, Yi and Mesgarani, Nima},
	month = aug,
	year = {2019},
	note = {arXiv: 1809.07454},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	pages = {1256--1266},
	file = {Luo and Mesgarani - 2019 - Conv-TasNet Surpassing Ideal Time-Frequency Magni.pdf:/Users/rosscutler/Zotero/storage/YZDJF5LG/Luo and Mesgarani - 2019 - Conv-TasNet Surpassing Ideal Time-Frequency Magni.pdf:application/pdf},
}

@article{xu_components_2019,
	title = {Components {Loss} for {Neural} {Networks} in {Mask}-{Based} {Speech} {Enhancement}},
	url = {http://arxiv.org/abs/1908.05087},
	abstract = {Estimating time-frequency domain masks for single-channel speech enhancement using deep learning methods has recently become a popular research field with promising results. In this paper, we propose a novel components loss (CL) for the training of neural networks for mask-based speech enhancement. During the training process, the proposed CL offers separate control over preservation of the speech component quality, suppression of the residual noise component, and preservation of a naturally sounding residual noise component. We illustrate the potential of the proposed CL by evaluating a standard convolutional neural network (CNN) for mask-based speech enhancement. The new CL obtains a better and more balanced performance in almost all employed instrumental quality metrics over the baseline losses, the latter comprising the conventional mean squared error (MSE) loss and also auditory-related loss functions, such as the perceptual evaluation of speech quality (PESQ) loss and the recently proposed perceptual weighting filter loss. Particularly, applying the CL offers better speech component quality, better overall enhanced speech perceptual quality, as well as a more naturally sounding residual noise. On average, an at least 0.1 points higher PESQ score on the enhanced speech is obtained while also obtaining a higher SNR improvement by more than 0.5 dB, for seen noise types. This improvement is stronger for unseen noise types, where an about 0.2 points higher PESQ score on the enhanced speech is obtained, while also the output SNR is ahead by more than 0.5 dB. The new proposed CL is easy to implement and code is provided at https://github.com/ifnspaml/Components-Loss.},
	urldate = {2019-08-19},
	journal = {arXiv:1908.05087 [cs, eess]},
	author = {Xu, Ziyi and Elshamy, Samy and Zhao, Ziyue and Fingscheidt, Tim},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.05087},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv\:1908.05087 PDF:/Users/rosscutler/Zotero/storage/Q75UARFG/Xu et al. - 2019 - Components Loss for Neural Networks in Mask-Based .pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/QRI7DPFE/1908.html:text/html;Xu et al. - 2019 - Components Loss for Neural Networks in Mask-Based .pdf:/Users/rosscutler/Zotero/storage/6X83Y3EX/Xu et al. - 2019 - Components Loss for Neural Networks in Mask-Based .pdf:application/pdf},
}

@inproceedings{noauthor_smartchoices:_nodate,
	title = {{SmartChoices}: {Hybridizing} {Programming} and {Machine} {Learning}},
	url = {https://arxiv.org/abs/1810.00619},
}

@inproceedings{noauthor_distillation_nodate,
	title = {A {Distillation} {Approach} to {Data} {Efficient} {Individual} {Treatment} {Effect} {Estimation}},
	url = {https://www.microsoft.com/en-us/research/uploads/prod/2019/01/deitee-camera-ready.pdf},
}

@inproceedings{fu_complex_2017,
	address = {Tokyo},
	title = {Complex spectrogram enhancement by convolutional neural network with multi-metrics learning},
	isbn = {978-1-5090-6341-3},
	url = {http://ieeexplore.ieee.org/document/8168119/},
	doi = {10.1109/MLSP.2017.8168119},
	abstract = {This paper aims to address two issues existing in the current speech enhancement methods: 1) the difficulty of phase estimations; 2) a single objective function cannot consider multiple metrics simultaneously. To solve the first problem, we propose a novel convolutional neural network (CNN) model for complex spectrogram enhancement, namely estimating clean real and imaginary (RI) spectrograms from noisy ones. The reconstructed RI spectrograms are directly used to synthesize enhanced speech waveforms. In addition, since log-power spectrogram (LPS) can be represented as a function of RI spectrograms, its reconstruction is also considered as another target. Thus a unified objective function, which combines these two targets (reconstruction of RI spectrograms and LPS), is equivalent to simultaneously optimizing two commonly used objective metrics: segmental signal-to-noise ratio (SSNR) and logspectral distortion (LSD). Therefore, the learning process is called multi-metrics learning (MML). Experimental results confirm the effectiveness of the proposed CNN with RI spectrograms and MML in terms of improved standardized evaluation metrics on a speech enhancement task.},
	language = {en},
	urldate = {2019-08-13},
	booktitle = {2017 {IEEE} 27th {International} {Workshop} on {Machine} {Learning} for {Signal} {Processing} ({MLSP})},
	publisher = {IEEE},
	author = {Fu, Szu-Wei and Hu, Ting-yao and Tsao, Yu and Lu, Xugang},
	month = sep,
	year = {2017},
	pages = {1--6},
	file = {Fu et al. - 2017 - Complex spectrogram enhancement by convolutional n.pdf:/Users/rosscutler/Zotero/storage/N9LQJ9IY/Fu et al. - 2017 - Complex spectrogram enhancement by convolutional n.pdf:application/pdf},
}

@incollection{luo_multivariate_2018,
	title = {Multivariate {Time} {Series} {Imputation} with {Generative} {Adversarial} {Networks}},
	url = {http://papers.nips.cc/paper/7432-multivariate-time-series-imputation-with-generative-adversarial-networks.pdf},
	urldate = {2019-08-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Luo, Yonghong and Cai, Xiangrui and ZHANG, Ying and Xu, Jun and xiaojie, Yuan},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {1596--1607},
	file = {NIPS Full Text PDF:/Users/rosscutler/Zotero/storage/HWWXR4BY/Luo et al. - 2018 - Multivariate Time Series Imputation with Generativ.pdf:application/pdf;NIPS Snapshot:/Users/rosscutler/Zotero/storage/NMECV3M7/7432-multivariate-time-series-imputation-with-generative-adversarial-networks.html:text/html},
}

@misc{noauthor_wu_ejmr_paper_09_2017.pdf_nodate,
	title = {Wu\_EJMR\_paper\_09\_2017.pdf},
	url = {https://drive.google.com/file/d/0BwjFN4HbBrDBbnFqZzdLWThDb0U/view?usp=embed_facebook},
	urldate = {2019-08-12},
	journal = {Google Docs},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/BKHTBNIL/view.html:text/html},
}

@inproceedings{avila_non-intrusive_2019,
	address = {Brighton, United Kingdom},
	title = {Non-intrusive {Speech} {Quality} {Assessment} {Using} {Neural} {Networks}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683175/},
	doi = {10.1109/ICASSP.2019.8683175},
	urldate = {2019-08-09},
	booktitle = {{ICASSP}},
	publisher = {IEEE},
	author = {Avila, Anderson R. and Gamper, Hannes and Reddy, Chandan and Cutler, Ross and Tashev, Ivan and Gehrke, Johannes},
	month = may,
	year = {2019},
	pages = {631--635},
}

@article{kumar_knowledge_2017,
	title = {Knowledge {Transfer} from {Weakly} {Labeled} {Audio} using {Convolutional} {Neural} {Network} for {Sound} {Events} and {Scenes}},
	url = {http://arxiv.org/abs/1711.01369},
	abstract = {In this work we propose approaches to effectively transfer knowledge from weakly labeled web audio data. We first describe a convolutional neural network (CNN) based framework for sound event detection and classification using weakly labeled audio data. Our model trains efficiently from audios of variable lengths; hence, it is well suited for transfer learning. We then propose methods to learn representations using this model which can be effectively used for solving the target task. We study both transductive and inductive transfer learning tasks, showing the effectiveness of our methods for both domain and task adaptation. We show that the learned representations using the proposed CNN model generalizes well enough to reach human level accuracy on ESC-50 sound events dataset and set state of art results on this dataset. We further use them for acoustic scene classification task and once again show that our proposed approaches suit well for this task as well. We also show that our methods are helpful in capturing semantic meanings and relations as well. Moreover, in this process we also set state-of-art results on Audioset dataset, relying on balanced training set.},
	urldate = {2019-08-09},
	journal = {arXiv:1711.01369 [cs, eess]},
	author = {Kumar, Anurag and Khadkevich, Maksim and Fugen, Christian},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.01369},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Multimedia},
	file = {arXiv\:1711.01369 PDF:/Users/rosscutler/Zotero/storage/LIN3RAL9/Kumar et al. - 2017 - Knowledge Transfer from Weakly Labeled Audio using.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/NGCMDCVD/1711.html:text/html},
}

@article{mccowan_delta-phase_2011,
	title = {The {Delta}-{Phase} {Spectrum} {With} {Application} to {Voice} {Activity} {Detection} and {Speaker} {Recognition}},
	volume = {19},
	issn = {1558-7916},
	doi = {10.1109/TASL.2011.2109379},
	abstract = {For several reasons, the Fourier phase domain is less favored than the magnitude domain in signal processing and modeling of speech. To correctly analyze the phase, several factors must be considered and compensated, including the effect of the step size, windowing function and other processing parameters. Building on a review of these factors, this paper investigates a spectral representation based on the Instantaneous Frequency Deviation, but in which the step size between processing frames is used in calculating phase changes, rather than the traditional single sample interval. Reflecting these longer intervals, the term delta-phase spectrum is used to distinguish this from instantaneous derivatives. Experiments show that mel-frequency cepstral coefficients features derived from the delta-phase spectrum (termed Mel-Frequency delta-phase features) can produce broadly similar performance to equivalent magnitude domain features for both voice activity detection and speaker recognition tasks. Further, it is shown that the fusion of the magnitude and phase representations yields performance benefits over either in isolation.},
	number = {7},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {McCowan, I. and Dean, D. and McLaren, M. and Vogt, R. and Sridharan, S.},
	month = sep,
	year = {2011},
	keywords = {Feature extraction, speaker recognition, cepstral analysis, delta-phase spectrum, equivalent magnitude domain, Fourier phase domain, Fourier transforms, Instantaneous frequency (IF), instantaneous frequency deviation, Mel frequency cepstral coefficient, mel-frequency cepstral coefficients, mel-frequency delta-phase features, phase, Phase measurement, sample interval, signal processing, spectral representation, Speech, speech analysis, step size, Time frequency analysis, voice activity detection, voice activity detection (VAD), windowing function},
	pages = {2026--2038},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/XTGV9G2H/5704566.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/RBCLZX78/McCowan et al. - 2011 - The Delta-Phase Spectrum With Application to Voice.pdf:application/pdf},
}

@article{esteban_real-valued_2017,
	title = {Real-valued ({Medical}) {Time} {Series} {Generation} with {Recurrent} {Conditional} {GANs}},
	url = {http://arxiv.org/abs/1706.02633},
	abstract = {Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from 'serialised' MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data.},
	urldate = {2019-08-08},
	journal = {arXiv:1706.02633 [cs, stat]},
	author = {Esteban, Cristóbal and Hyland, Stephanie L. and Rätsch, Gunnar},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02633},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1706.02633 PDF:/Users/rosscutler/Zotero/storage/U8HR8UUX/Esteban et al. - 2017 - Real-valued (Medical) Time Series Generation with .pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/C96A4ETS/1706.html:text/html},
}

@inproceedings{tan_complex_2019,
	title = {Complex {Spectral} {Mapping} with a {Convolutional} {Recurrent} {Network} for {Monaural} {Speech} {Enhancement}},
	doi = {10.1109/ICASSP.2019.8682834},
	abstract = {Phase is important for perceptual quality in speech enhancement. However, it seems intractable to directly estimate phase spectrogram through supervised learning due to lack of clear structure in phase spectrogram. Complex spectral mapping aims to estimate the real and imaginary spectrograms of clean speech from those of noisy speech, which simultaneously enhances magnitude and phase responses of noisy speech. In this paper, we propose a new convolutional recurrent network (CRN) for complex spectral mapping, which leads to a causal system for noise- and speaker-independent speech enhancement. In terms of objective intelligibility and perceptual quality, the proposed CRN significantly outperforms an existing convolutional neural network (CNN) for complex spectral mapping, as well as a strong CRN for magnitude spectral mapping. We additionally incorporate a newly-developed group strategy to substantially reduce the number of trainable parameters and the computational cost without sacrificing performance.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Tan, K. and Wang, D.},
	month = may,
	year = {2019},
	keywords = {Speech enhancement, learning (artificial intelligence), Spectrogram, Training, perceptual quality, causal system, clean speech, complex spectral mapping, Convolution, convolutional neural nets, convolutional neural network, convolutional recurrent network, Decoding, Estimation, group strategy, imaginary spectrogram estimation, magnitude spectral mapping, monaural speech enhancement, Noise measurement, noise-independent speech enhancement, noisy speech, objective intelligibility, phase response, phase spectrogram estimation, real spectrogram estimation, recurrent neural nets, speaker-independent speech enhancement, spectral analysis, speech enhancement, speech intelligibility, supervised learning},
	pages = {6865--6869},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/8STB5JEV/8682834.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/YVVQF2JT/Tan and Wang - 2019 - Complex Spectral Mapping with a Convolutional Recu.pdf:application/pdf},
}

@inproceedings{deng_imagenet:_2009,
	address = {Miami, FL},
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	isbn = {978-1-4244-3992-8},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848/},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	language = {en},
	urldate = {2019-08-07},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	pages = {248--255},
	file = {Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:/Users/rosscutler/Zotero/storage/6ZTPVYM8/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:application/pdf},
}

@article{wang_unsupervised_2016,
	title = {Unsupervised {Category} {Discovery} via {Looped} {Deep} {Pseudo}-{Task} {Optimization} {Using} a {Large} {Scale} {Radiology} {Image} {Database}},
	url = {http://arxiv.org/abs/1603.07965},
	abstract = {Obtaining semantic labels on a large scale radiology image database (215,786 key images from 61,845 unique patients) is a prerequisite yet bottleneck to train highly effective deep convolutional neural network (CNN) models for image recognition. Nevertheless, conventional methods for collecting image labels (e.g., Google search followed by crowd-sourcing) are not applicable due to the formidable difficulties of medical annotation tasks for those who are not clinically trained. This type of image labeling task remains non-trivial even for radiologists due to uncertainty and possible drastic inter-observer variation or inconsistency. In this paper, we present a looped deep pseudo-task optimization procedure for automatic category discovery of visually coherent and clinically semantic (concept) clusters. Our system can be initialized by domain-specific (CNN trained on radiology images and text report derived labels) or generic (ImageNet based) CNN models. Afterwards, a sequence of pseudo-tasks are exploited by the looped deep image feature clustering (to refine image labels) and deep CNN training/classification using new labels (to obtain more task representative deep features). Our method is conceptually simple and based on the hypothesized "convergence" of better labels leading to better trained CNN models which in turn feed more effective deep image features to facilitate more meaningful clustering/labels. We have empirically validated the convergence and demonstrated promising quantitative and qualitative results. Category labels of significantly higher quality than those in previous work are discovered. This allows for further investigation of the hierarchical semantic nature of the given large-scale radiology image database.},
	urldate = {2019-08-07},
	journal = {arXiv:1603.07965 [cs]},
	author = {Wang, Xiaosong and Lu, Le and Shin, Hoo-chang and Kim, Lauren and Nogues, Isabella and Yao, Jianhua and Summers, Ronald},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.07965},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1603.07965 PDF:/Users/rosscutler/Zotero/storage/YTHNTNK4/Wang et al. - 2016 - Unsupervised Category Discovery via Looped Deep Ps.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/SNLIVL2J/1603.html:text/html},
}

@article{krause_lessons_nodate,
	title = {Lessons {Learned} from {Large}‑{Scale} {Crowdsourced} {Data} {Collection} for {ILSVRC}},
	language = {en},
	author = {Krause, Jonathan},
	pages = {79},
	file = {Krause - Lessons Learned from Large‑Scale Crowdsourced Data.pdf:/Users/rosscutler/Zotero/storage/SSJQKQR2/Krause - Lessons Learned from Large‑Scale Crowdsourced Data.pdf:application/pdf},
}

@article{kavalerov_universal_2019,
	title = {Universal {Sound} {Separation}},
	url = {http://arxiv.org/abs/1905.03330},
	abstract = {Recent deep learning approaches have achieved impressive performance on speech enhancement and separation tasks. However, these approaches have not been investigated for separating mixtures of arbitrary sounds of different types, a task we refer to as universal sound separation, and it is unknown how performance on speech tasks carries over to non-speech tasks. To study this question, we develop a dataset of mixtures containing arbitrary sounds, and use it to investigate the space of mask-based separation architectures, varying both the overall network architecture and the framewise analysis-synthesis basis for signal transformations. These network architectures include convolutional long short-term memory networks and time-dilated convolution stacks inspired by the recent success of time-domain enhancement networks like ConvTasNet. For the latter architecture, we also propose novel modiﬁcations that further improve separation performance. In terms of the framewise analysis-synthesis basis, we explore both a short-time Fourier transform (STFT) and a learnable basis, as used in ConvTasNet. For both of these bases, we also examine the effect of window size. In particular, for STFTs, we ﬁnd that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal sound separation, STFTs outperform learnable bases. Our best methods produce an improvement in scale-invariant signal-todistortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation.},
	language = {en},
	urldate = {2019-08-06},
	journal = {arXiv:1905.03330 [cs, eess, stat]},
	author = {Kavalerov, Ilya and Wisdom, Scott and Erdogan, Hakan and Patton, Brian and Wilson, Kevin and Roux, Jonathan Le and Hershey, John R.},
	month = may,
	year = {2019},
	note = {arXiv: 1905.03330},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Kavalerov et al. - 2019 - Universal Sound Separation.pdf:/Users/rosscutler/Zotero/storage/MSPWQ8Y5/Kavalerov et al. - 2019 - Universal Sound Separation.pdf:application/pdf},
}

@article{ribas_deep_2019,
	title = {Deep {Speech} {Enhancement} for {Reverberated} and {Noisy} {Signals} using {Wide} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1901.00660},
	abstract = {This paper proposes a deep speech enhancement method which exploits the high potential of residual connections in a wide neural network architecture, a topology known as Wide Residual Network. This is supported on single dimensional convolutions computed alongside the time domain, which is a powerful approach to process contextually correlated representations through the temporal domain, such as speech feature sequences. We ﬁnd the residual mechanism extremely useful for the enhancement task since the signal always has a linear shortcut and the non-linear path enhances it in several steps by adding or subtracting corrections. The enhancement capacity of the proposal is assessed by objective quality metrics and the performance of a speech recognition system. This was evaluated in the framework of the REVERB Challenge dataset, including simulated and real samples of reverberated and noisy speech signals. Results showed that enhanced speech from the proposed method succeeded for both, the enhancement task with intelligibility purposes and the speech recognition system. The DNN model, trained with artiﬁcial synthesized reverberation data, was able to deal with far-ﬁeld reverberated speech from real scenarios. Furthermore, the method was able to take advantage of the residual connection achieving to enhance signals with low noise level, which is usually a strong handicap of traditional enhancement methods.},
	language = {en},
	urldate = {2019-08-05},
	journal = {arXiv:1901.00660 [cs, stat]},
	author = {Ribas, Dayana and Llombart, Jorge and Miguel, Antonio and Vicente, Luis},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.00660},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Ribas et al. - 2019 - Deep Speech Enhancement for Reverberated and Noisy.pdf:/Users/rosscutler/Zotero/storage/N7MH9FGW/Ribas et al. - 2019 - Deep Speech Enhancement for Reverberated and Noisy.pdf:application/pdf},
}

@article{wang_speech_nodate,
	title = {{SPEECH} {SEPARATION} {USING} {SPEAKER} {INVENTORY}},
	abstract = {Overlapped speech is one of the main challenges in conversational speech applications such as meeting transcription. Blind speech separation and speech extraction are two common approaches to this problem. Both of them, however, suffer from limitations resulting from the lack of abilities to either leverage additional information or process multiple speakers simultaneously. In this work, we propose a novel method called speech separation using speaker inventory (SSUSI), which combines the advantages of both approaches and thus solves their problems. SSUSI makes use of a speaker inventory, i.e. a pool of pre-enrolled speaker signals, and jointly separates all participating speakers. This is achieved by a specially designed attention mechanism, eliminating the need for accurate speaker identities. Experimental results show that SSUSI outperforms permutation invariant training based blind speech separation by up to 48\% relatively in word error rate (WER). Compared with speech extraction, SSUSI reduces computation time by up to 70\% and improves the WER by more than 13\% relatively.},
	language = {en},
	author = {Wang, Peidong and Chen, Zhuo and Xiao, Xiong and Meng, Zhong and Yoshioka, Takuya and Zhou, Tianyan and Lu, Liang and Li, Jinyu},
	pages = {7},
	file = {Wang et al. - SPEECH SEPARATION USING SPEAKER INVENTORY.pdf:/Users/rosscutler/Zotero/storage/P99AE9JF/Wang et al. - SPEECH SEPARATION USING SPEAKER INVENTORY.pdf:application/pdf},
}

@inproceedings{chung_voxceleb2:_2018,
	title = {{VoxCeleb2}: {Deep} {Speaker} {Recognition}},
	shorttitle = {{VoxCeleb2}},
	url = {http://www.isca-speech.org/archive/Interspeech_2018/abstracts/1929.html},
	doi = {10.21437/Interspeech.2018-1929},
	abstract = {The objective of this paper is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset.},
	language = {en},
	urldate = {2019-08-05},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Chung, Joon Son and Nagrani, Arsha and Zisserman, Andrew},
	month = sep,
	year = {2018},
	pages = {1086--1090},
	file = {Chung et al. - 2018 - VoxCeleb2 Deep Speaker Recognition.pdf:/Users/rosscutler/Zotero/storage/23KCCT2S/Chung et al. - 2018 - VoxCeleb2 Deep Speaker Recognition.pdf:application/pdf},
}

@article{wu_improved_2019,
	title = {Improved {Speaker}-{Dependent} {Separation} for {CHiME}-5 {Challenge}},
	url = {http://arxiv.org/abs/1904.03792},
	abstract = {This paper summarizes several follow-up contributions for improving our submitted NWPU speaker-dependent system for CHiME-5 challenge, which aims to solve the problem of multi-channel, highly-overlapped conversational speech recognition in a dinner party scenario with reverberations and nonstationary noises. We adopt a speaker-aware training method by using i-vector as the target speaker information for multi-talker speech separation. With only one uniﬁed separation model for all speakers, we achieve a 10\% absolute improvement in terms of word error rate (WER) over the previous baseline of 80.28\% on the development set by leveraging our newly proposed data processing techniques and beamforming approach. With our improved back-end acoustic model, we further reduce WER to 60.15\% which surpasses the result of our submitted CHiME-5 challenge system without applying any fusion techniques.},
	language = {en},
	urldate = {2019-08-02},
	journal = {arXiv:1904.03792 [cs, eess]},
	author = {Wu, Jian and Xu, Yong and Zhang, Shi-Xiong and Chen, Lian-Wu and Yu, Meng and Xie, Lei and Yu, Dong},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.03792},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Wu et al. - 2019 - Improved Speaker-Dependent Separation for CHiME-5 .pdf:/Users/rosscutler/Zotero/storage/HQF97KB2/Wu et al. - 2019 - Improved Speaker-Dependent Separation for CHiME-5 .pdf:application/pdf},
}

@article{chuang_speaker-aware_nodate,
	title = {Speaker-aware {Deep} {Denoising} {Autoencoder} with {Embedded} {Speaker} {Identity} for {Speech} {Enhancement}},
	abstract = {Previous studies indicate that noise and speaker variations can degrade the performance of deep-learning–based speech–enhancement systems. To increase the system performance over environmental variations, we propose a novel speaker-aware system that integrates a deep denoising autoencoder (DDAE) with an embedded speaker identity. The overall system ﬁrst extracts embedded speaker identity features using a neural network model; then the DDAE takes the augmented features as input to generate enhanced spectra. With the additional embedded features, the speech-enhancement system can be guided to generate the optimal output corresponding to the speaker identity. We tested the proposed speech-enhancement system on the TIMIT dataset. Experimental results showed that the proposed speech-enhancement system could improve the sound quality and intelligibility of speech signals from additive noisecorrupted utterances. In addition, the results suggested system robustness for unseen speakers when combined with speaker features.},
	language = {en},
	author = {Chuang, Fu-Kai and Wang, Syu-Siang and Hung, Jeih-weih and Tsao, Yu and Fang, Shih-Hau},
	pages = {5},
	file = {Chuang et al. - Speaker-aware Deep Denoising Autoencoder with Embe.pdf:/Users/rosscutler/Zotero/storage/77TBD94N/Chuang et al. - Speaker-aware Deep Denoising Autoencoder with Embe.pdf:application/pdf},
}

@article{wu_time_2019,
	title = {Time {Domain} {Audio} {Visual} {Speech} {Separation}},
	url = {http://arxiv.org/abs/1904.03760},
	abstract = {Audio-visual multi-modal modeling has been demonstrated to be effective in many speech related tasks, such as speech recognition and speech enhancement. This paper introduces a new time-domain audio-visual architecture for target speaker extraction from monaural mixtures. The architecture generalizes the previous TasNet (time-domain speech separation network) to enable multi-modal learning and at meanwhile it extends the classical audio-visual speech separation from frequencydomain to time-domain. The main components of proposed architecture include an audio encoder, a video encoder which can extract lip embedding from video steams, a multi-modal separation network and an audio decoder. Experiments on simulated mixtures based on recently released LRS2 dataset show that our method can bring 3dB+ and 4dB+ Si-SNR improvements on 2 and 3 speakers cases respectively, compared to audio-only TasNet and frequency domain audio-visual networks.},
	language = {en},
	urldate = {2019-08-02},
	journal = {arXiv:1904.03760 [cs, eess]},
	author = {Wu, Jian and Xu, Yong and Zhang, Shi-Xiong and Chen, Lian-Wu and Yu, Meng and Xie, Lei and Yu, Dong},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.03760},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Wu et al. - 2019 - Time Domain Audio Visual Speech Separation.pdf:/Users/rosscutler/Zotero/storage/EQYRRVJC/Wu et al. - 2019 - Time Domain Audio Visual Speech Separation.pdf:application/pdf},
}

@article{mobin_auditory_2019,
	title = {Auditory {Separation} of a {Conversation} from {Background} via {Attentional} {Gating}},
	url = {http://arxiv.org/abs/1905.10751},
	abstract = {We present a model for separating a set of voices out of a sound mixture containing an unknown number of sources. Our Attentional Gating Network (AGN) uses a variable attentional context to specify which speakers in the mixture are of interest. The attentional context is speciﬁed by an embedding vector which modiﬁes the processing of a neural network through an additive bias. Individual speaker embeddings are learned to separate a single speaker while superpositions of the individual speaker embeddings are used to separate sets of speakers. We ﬁrst evaluate AGN on a traditional single speaker separation task and show an improvement of 9\% with respect to comparable models. Then, we introduce a new task to separate an arbitrary subset of voices from a mixture of an unknown-sized set of voices, inspired by the human ability to separate a conversation of interest from background chatter at a cafeteria. We show that AGN is the only model capable of solving this task, performing only 7\% worse than on the single speaker separation task.},
	language = {en},
	urldate = {2019-08-02},
	journal = {arXiv:1905.10751 [cs, eess]},
	author = {Mobin, Shariq and Olshausen, Bruno},
	month = may,
	year = {2019},
	note = {arXiv: 1905.10751},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Mobin and Olshausen - 2019 - Auditory Separation of a Conversation from Backgro.pdf:/Users/rosscutler/Zotero/storage/PAVD5VY3/Mobin and Olshausen - 2019 - Auditory Separation of a Conversation from Backgro.pdf:application/pdf},
}

@article{shon_voiceid_2019,
	title = {{VoiceID} {Loss}: {Speech} {Enhancement} for {Speaker} {Verification}},
	shorttitle = {{VoiceID} {Loss}},
	url = {http://arxiv.org/abs/1904.03601},
	abstract = {In this paper, we propose VoiceID loss, a novel loss function for training a speech enhancement model to improve the robustness of speaker veriﬁcation. In contrast to the commonly used loss functions for speech enhancement such as the L2 loss, the VoiceID loss is based on the feedback from a speaker veriﬁcation model to generate a ratio mask. The generated ratio mask is multiplied pointwise with the original spectrogram to ﬁlter out unnecessary components for speaker veriﬁcation. In the experiments, we observed that the enhancement network, after training with the VoiceID loss, is able to ignore a substantial amount of time-frequency bins, such as those dominated by noise, for veriﬁcation. The resulting model consistently improves the speaker veriﬁcation system on both clean and noisy conditions.},
	language = {en},
	urldate = {2019-08-02},
	journal = {arXiv:1904.03601 [cs, eess]},
	author = {Shon, Suwon and Tang, Hao and Glass, James},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.03601},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Shon et al. - 2019 - VoiceID Loss Speech Enhancement for Speaker Verif.pdf:/Users/rosscutler/Zotero/storage/DFDM7MLQ/Shon et al. - 2019 - VoiceID Loss Speech Enhancement for Speaker Verif.pdf:application/pdf},
}

@article{zmolikova_speakerbeam:_2019,
	title = {{SpeakerBeam}: {Speaker} {Aware} {Neural} {Network} for {Target} {Speaker} {Extraction} in {Speech} {Mixtures}},
	volume = {13},
	issn = {1932-4553},
	shorttitle = {{SpeakerBeam}},
	doi = {10.1109/JSTSP.2019.2922820},
	abstract = {The processing of speech corrupted by interfering overlapping speakers is one of the challenging problems with regards to today's automatic speech recognition systems. Recently, approaches based on deep learning have made great progress toward solving this problem. Most of these approaches tackle the problem as speech separation, i.e., they blindly recover all the speakers from the mixture. In some scenarios, such as smart personal devices, we may however be interested in recovering one target speaker from a mixture. In this paper, we introduce SpeakerBeam, a method for extracting a target speaker from the mixture based on an adaptation utterance spoken by the target speaker. Formulating the problem as speaker extraction avoids certain issues such as label permutation and the need to determine the number of speakers in the mixture. With SpeakerBeam, we jointly learn to extract a representation from the adaptation utterance characterizing the target speaker and to use this representation to extract the speaker. We explore several ways to do this, mostly inspired by speaker adaptation in acoustic models for automatic speech recognition. We evaluate the performance on the widely used WSJ0-2mix and WSJ0-3mix datasets, and these datasets modified with more noise or more realistic overlapping patterns. We further analyze the learned behavior by exploring the speaker representations and assessing the effect of the length of the adaptation data. The results show the benefit of including speaker information in the processing and the effectiveness of the proposed method.},
	number = {4},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Žmolíková, K. and Delcroix, M. and Kinoshita, K. and Ochiai, T. and Nakatani, T. and Burget, L. and Černocký, J.},
	month = aug,
	year = {2019},
	keywords = {Neural networks, Training, Data mining, Speech processing, Automatic speech recognition, Hidden Markov models, multi-speaker speech recognition, Speaker extraction, speaker-aware neural network},
	pages = {800--814},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/R62IVXRW/8736286.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/GCSR9CGC/Žmolíková et al. - 2019 - SpeakerBeam Speaker Aware Neural Network for Targ.pdf:application/pdf},
}

@article{afouras_my_2019,
	title = {My lips are concealed: {Audio}-visual speech enhancement through obstructions},
	shorttitle = {My lips are concealed},
	url = {http://arxiv.org/abs/1907.04975},
	abstract = {Our objective is an audio-visual model for separating a single speaker from a mixture of sounds such as other speakers and background noise. Moreover, we wish to hear the speaker even when the visual cues are temporarily absent due to occlusion.},
	language = {en},
	urldate = {2019-08-02},
	journal = {arXiv:1907.04975 [cs, eess]},
	author = {Afouras, Triantafyllos and Chung, Joon Son and Zisserman, Andrew},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.04975},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Afouras et al. - 2019 - My lips are concealed Audio-visual speech enhance.pdf:/Users/rosscutler/Zotero/storage/BBG6USQ2/Afouras et al. - 2019 - My lips are concealed Audio-visual speech enhance.pdf:application/pdf;arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/4TRKIWDC/Afouras et al. - 2019 - My lips are concealed Audio-visual speech enhance.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/6H7NGN9U/1907.html:text/html},
}

@inproceedings{delcroix_compact_2019,
	title = {Compact {Network} for {Speakerbeam} {Target} {Speaker} {Extraction}},
	doi = {10.1109/ICASSP.2019.8683087},
	abstract = {Speech separation that separates a mixture of speech signals into each of its sources has been an active research topic for a long time and has seen recent progress with the advent of deep learning. A related problem is target speaker extraction, i.e. extraction of only speech of a target speaker out of a mixture, given characteristics of his/her voice. We have recently proposed SpeakerBeam, which is a neural network-based target speaker extraction method. Speaker-Beam uses a speech extraction network that is adapted to the target speaker using auxiliary features derived from an adaptation utterance of that speaker. Initially, we implemented SpeakerBeam with a factorized adaptation layer, which consists of several parallel linear transformations weighted by weights derived from the auxiliary features. The factorized layer is effective for target speech extraction, but it requires a large number of parameters. In this paper, we propose to simply scale the activations of a hidden layer of the speech extraction network with weights derived from the auxiliary features. This simpler approach greatly reduces the number of model parameters by up to 60\%, making it much more practical, while maintaining a similar level of performance. We tested our approach on simulated and real noisy and reverberant mixtures, showing the potential of SpeakerBeam for real-life applications. Moreover, we showed that speech extraction performance of SpeakerBeam compares favorably with that of a state-of-the-art speech separation method with a similar network configuration.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Delcroix, M. and Zmolikova, K. and Ochiai, T. and Kinoshita, K. and Araki, S. and Nakatani, T.},
	month = may,
	year = {2019},
	keywords = {Speech enhancement, learning (artificial intelligence), speaker recognition, feature extraction, neural nets, acoustic signal processing, Adaptation, adaptation utterance, Auxiliary feature, compact network, factorized adaptation layer, neural network, Neural network, parallel linear transformations, real noisy mixtures, reverberant mixtures, reverberation, source separation, speakerbeam target speaker extraction, speech extraction network, speech recognition, speech separation method, speech signals, Target speech extraction},
	pages = {6965--6969},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/UNLIKIMB/8683087.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/P7Y4C593/Delcroix et al. - 2019 - Compact Network for Speakerbeam Target Speaker Ext.pdf:application/pdf},
}

@inproceedings{xiao_single-channel_2019,
	title = {Single-channel {Speech} {Extraction} {Using} {Speaker} {Inventory} and {Attention} {Network}},
	doi = {10.1109/ICASSP.2019.8682245},
	abstract = {Neural network-based speech separation has received a surge of interest in recent years. Previously proposed methods either are speaker independent or extract a target speaker's voice by using his or her voice snippet. In applications such as home devices or office meeting transcriptions, a possible speaker list is available, which can be leveraged for speech separation. This paper proposes a novel speech extraction method that utilizes an inventory of voice snippets of possible interfering speakers, or speaker enrollment data, in addition to that of the target speaker. Furthermore, an attention-based network architecture is proposed to form time-varying masks for both the target and other speakers during the separation process. This architecture does not reduce the enrollment audio of each speaker into a single vector, thereby allowing each short time frame of the input mixture signal to be aligned and accurately compared with the enrollment signals. We evaluate the proposed system on a speaker extraction task derived from the Libri corpus and show the effectiveness of the method.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Xiao, X. and Chen, Z. and Yoshioka, T. and Erdogan, H. and Liu, C. and Dimitriadis, D. and Droppo, J. and Gong, Y.},
	month = may,
	year = {2019},
	keywords = {Feature extraction, speaker recognition, Training, attention, attention network, attention-based network architecture, Context modeling, Data mining, feature extraction, interfering speakers, neural nets, neural network-based speech separation, office meeting transcriptions, Periodic structures, possible speaker list, separation process, single-channel speech extraction, speaker enrollment data, speaker extraction, speaker extraction task, speaker inventory, speaker profile, speech extraction method, Speech processing, speech separation, target speaker, Task analysis, voice snippet},
	pages = {86--90},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/F2KNK5TV/8682245.html:text/html;IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/7MN5QBME/8682245.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/ZCEQZLZS/Xiao et al. - 2019 - Single-channel Speech Extraction Using Speaker Inv.pdf:application/pdf;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/NM5XJLCK/Xiao et al. - 2019 - Single-channel Speech Extraction Using Speaker Inv.pdf:application/pdf},
}

@article{wilson_exploring_2018,
	title = {Exploring {Tradeoffs} in {Models} for {Low}-latency {Speech} {Enhancement}},
	url = {http://arxiv.org/abs/1811.07030},
	abstract = {We explore a variety of neural networks conﬁgurations for one- and two-channel spectrogram-mask-based speech enhancement. Our best model improves on previous state-ofthe-art performance on the CHiME2 speech enhancement task by 0.4 decibels in signal-to-distortion ratio (SDR). We examine trade-offs such as non-causal look-ahead, computation, and parameter count versus enhancement performance and ﬁnd that zero-look-ahead models can achieve, on average, within 0.03 dB SDR of our best bidirectional model. Further, we ﬁnd that 200 milliseconds of look-ahead is sufﬁcient to achieve equivalent performance to our best bidirectional model.},
	language = {en},
	urldate = {2019-08-01},
	journal = {arXiv:1811.07030 [cs, eess]},
	author = {Wilson, Kevin and Chinen, Michael and Thorpe, Jeremy and Patton, Brian and Hershey, John and Saurous, Rif A. and Skoglund, Jan and Lyon, Richard F.},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.07030},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Wilson et al. - 2018 - Exploring Tradeoffs in Models for Low-latency Spee.pdf:/Users/rosscutler/Zotero/storage/UG932U7X/Wilson et al. - 2018 - Exploring Tradeoffs in Models for Low-latency Spee.pdf:application/pdf},
}

@article{wang_voicefilter:_2018,
	title = {{VoiceFilter}: {Targeted} {Voice} {Separation} by {Speaker}-{Conditioned} {Spectrogram} {Masking}},
	shorttitle = {{VoiceFilter}},
	url = {http://arxiv.org/abs/1810.04826},
	abstract = {In this paper, we present a novel system that separates the voice of a target speaker from multi-speaker signals, by making use of a reference signal from the target speaker. We achieve this by training two separate neural networks: (1) A speaker recognition network that produces speaker-discriminative embeddings; (2) A spectrogram masking network that takes both noisy spectrogram and speaker embedding as input, and produces a mask. Our system signiﬁcantly reduces the speech recognition WER on multi-speaker signals, with minimal WER degradation on single-speaker signals.},
	language = {en},
	urldate = {2019-08-01},
	journal = {arXiv:1810.04826 [cs, eess, stat]},
	author = {Wang, Quan and Muckenhirn, Hannah and Wilson, Kevin and Sridhar, Prashant and Wu, Zelin and Hershey, John and Saurous, Rif A. and Weiss, Ron J. and Jia, Ye and Moreno, Ignacio Lopez},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.04826},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	file = {Wang et al. - 2018 - VoiceFilter Targeted Voice Separation by Speaker-.pdf:/Users/rosscutler/Zotero/storage/5SXGMUGJ/Wang et al. - 2018 - VoiceFilter Targeted Voice Separation by Speaker-.pdf:application/pdf},
}

@article{grzywalski_using_2018,
	title = {Using recurrences in time and frequency within {U}-net architecture for speech enhancement},
	url = {http://arxiv.org/abs/1811.06805},
	abstract = {When designing fully-convolutional neural network, there is a trade-off between receptive ﬁeld size, number of parameters and spatial resolution of features in deeper layers of the network. In this work we present a novel network design based on combination of many convolutional and recurrent layers that solves these dilemmas. We compare our solution with Unets based models known from the literature and other baseline models on speech enhancement task. We test our solution on TIMIT speech utterances combined with noise segments extracted from NOISEX-92 database and show clear advantage of proposed solution in terms of SDR (signal-todistortion ratio), SIR (signal-to-interference ratio) and STOI (spectro-temporal objective intelligibility) metrics compared to the current state-of-the-art.},
	language = {en},
	urldate = {2019-08-01},
	journal = {arXiv:1811.06805 [cs, eess, stat]},
	author = {Grzywalski, Tomasz and Drgas, Szymon},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.06805},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Grzywalski and Drgas - 2018 - Using recurrences in time and frequency within U-n.pdf:/Users/rosscutler/Zotero/storage/2PT7MGXM/Grzywalski and Drgas - 2018 - Using recurrences in time and frequency within U-n.pdf:application/pdf},
}

@article{wang_supervised_2018,
	title = {Supervised {Speech} {Separation} {Based} on {Deep} {Learning}: {An} {Overview}},
	volume = {26},
	issn = {2329-9290, 2329-9304},
	shorttitle = {Supervised {Speech} {Separation} {Based} on {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8369155/},
	doi = {10.1109/TASLP.2018.2842159},
	abstract = {Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This article provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multitalker separation), and speech dereverberation, as well as multi-microphone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.},
	language = {en},
	number = {10},
	urldate = {2019-08-01},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wang, DeLiang and Chen, Jitong},
	month = oct,
	year = {2018},
	pages = {1702--1726},
	file = {Wang and Chen - 2018 - Supervised Speech Separation Based on Deep Learnin.pdf:/Users/rosscutler/Zotero/storage/MR25J6MP/Wang and Chen - 2018 - Supervised Speech Separation Based on Deep Learnin.pdf:application/pdf;Wang and Chen - 2018 - Supervised Speech Separation Based on Deep Learnin.pdf:/Users/rosscutler/Zotero/storage/H57A5X2Q/Wang and Chen - 2018 - Supervised Speech Separation Based on Deep Learnin.pdf:application/pdf},
}

@article{ruffy_iroko:_2018,
	title = {Iroko: {A} {Framework} to {Prototype} {Reinforcement} {Learning} for {Data} {Center} {Trafﬁc} {Control}},
	url = {https://drive.google.com/file/d/1uA7VfAMsEUZftr8aP-nb-gJBax30nkyk/view},
	abstract = {Recent networking research has identiﬁed that data-driven congestion control (CC) can be more efﬁcient than traditional CC in TCP. Deep reinforcement learning (RL), in particular, has the potential to learn optimal network policies. However, RL suffers from instability and over-ﬁtting, deﬁciencies which so far render it unacceptable for use in datacenter networks. In this paper, we analyze the requirements for RL to succeed in the datacenter context. We present a new emulator, Iroko, which we developed to support different network topologies, congestion control algorithms, and deployment scenarios. Iroko interfaces with the OpenAI gym toolkit, which allows for fast and fair evaluation of different RL and traditional CC algorithms under the same conditions. We present initial benchmarks on three deep RL algorithms compared to TCP New Vegas and DCTCP. Our results show that these algorithms are able to learn a CC policy which exceeds the performance of TCP New Vegas on a dumbbell and fat-tree topology. We make our emulator open-source and publicly available: https://github.com/dcgym/iroko.},
	language = {en},
	author = {Ruffy, Fabian and Przystupa, Michael and Beschastnikh, Ivan},
	year = {2018},
	pages = {11},
	file = {Ruffy et al. - Iroko A Framework to Prototype Reinforcement Lear.pdf:/Users/rosscutler/Zotero/storage/B7XSXNWI/Ruffy et al. - Iroko A Framework to Prototype Reinforcement Lear.pdf:application/pdf},
}

@article{polese_survey_2018,
	title = {A {Survey} on {Recent} {Advances} in {Transport} {Layer} {Protocols}},
	url = {http://arxiv.org/abs/1810.03884},
	abstract = {Over the years, the Internet has been enriched with new available communication technologies, for both ﬁxed and mobile networks and devices, exhibiting an impressive growth in terms of performance, with steadily increasing available data rates. The Internet research community has kept trying to evolve the transport layer protocols to match the capabilities of modern networks, in order to fully reap the beneﬁts of the new communication technologies. This paper surveys the main novelties related to transport protocols that have been recently proposed, identifying three main research trends: (i) the evolution of congestion control algorithms, to target optimal performance in challenging scenarios, possibly with the application of machine learning techniques; (ii) the proposal of brand new transport protocols, alternative to TCP and implemented in the user-space; and (iii) the introduction of multi-path capabilities at the transport layer.},
	language = {en},
	urldate = {2019-01-05},
	journal = {arXiv:1810.03884 [cs]},
	author = {Polese, Michele and Chiariotti, Federico and Bonetto, Elia and Rigotto, Filippo and Zanella, Andrea and Zorzi, Michele},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.03884},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {Polese et al. - 2018 - A Survey on Recent Advances in Transport Layer Pro.pdf:/Users/rosscutler/Zotero/storage/DNLWBAS2/Polese et al. - 2018 - A Survey on Recent Advances in Transport Layer Pro.pdf:application/pdf},
}

@inproceedings{arun_copa:_2018,
	address = {Montreal, QC, Canada},
	title = {Copa: {Practical} {Delay}-{Based} {Congestion} {Control} for the {Internet}},
	isbn = {978-1-4503-5585-8},
	shorttitle = {Copa},
	url = {http://dl.acm.org/citation.cfm?doid=3232755.3232783},
	doi = {10.1145/3232755.3232783},
	abstract = {This paper introduces Copa, an end-to-end congestion control algorithm that uses three ideas. First, it shows that a target rate equal to 1/(δdq), where dq is the (measured) queueing delay, optimizes a natural function of throughput and delay under a Markovian packet arrival model. Second, it adjusts its congestion window in the direction of this target rate, converging quickly to the correct fair rates even in the face of significant flow churn. These two ideas enable a group of Copa flows to maintain high utilization with low queuing delay. However, when the bottleneck is shared with loss-based congestion-controlled flows that fill up buffers, Copa, like other delay-sensitive schemes, achieves low throughput. To combat this problem, Copa uses a third idea: detect the presence of buffer-fillers by observing the delay evolution, and respond with additive-increase/multiplicative decrease on the δ parameter. Experimental results show that Copa outperforms Cubic (similar throughput, much lower delay, fairer with diverse RTTs), BBR and PCC (significantly fairer, lower delay), and co-exists well with Cubic unlike BBR and PCC. Copa is also robust to non-congestive loss and large bottleneck buffers, and outperforms other schemes on long-RTT paths.},
	language = {en},
	urldate = {2019-01-04},
	booktitle = {Proceedings of the {Applied} {Networking} {Research} {Workshop} on   - {ANRW} '18},
	publisher = {ACM Press},
	author = {Arun, Venkat and Balakrishnan, Hari},
	year = {2018},
	pages = {19--19},
	file = {Arun and Balakrishnan - 2018 - Copa Practical Delay-Based Congestion Control for.pdf:/Users/rosscutler/Zotero/storage/VEXQ3JZC/Arun and Balakrishnan - 2018 - Copa Practical Delay-Based Congestion Control for.pdf:application/pdf},
}

@article{cardwell_bbr:_2017,
	title = {{BBR}: congestion-based congestion control},
	volume = {60},
	issn = {00010782},
	shorttitle = {{BBR}},
	url = {http://dl.acm.org/citation.cfm?doid=3042068.3009824},
	doi = {10.1145/3009824},
	language = {en},
	number = {2},
	urldate = {2019-01-04},
	journal = {Communications of the ACM},
	author = {Cardwell, Neal and Cheng, Yuchung and Gunn, C. Stephen and Yeganeh, Soheil Hassas and {Van Jacobson}},
	month = jan,
	year = {2017},
	pages = {58--66},
	file = {Cardwell et al. - 2017 - BBR congestion-based congestion control.pdf:/Users/rosscutler/Zotero/storage/EAJ25WFC/Cardwell et al. - 2017 - BBR congestion-based congestion control.pdf:application/pdf},
}

@inproceedings{calder_odin:_2018,
	title = {Odin: {Microsoft}’s {Scalable} {Fault}-{Tolerant} \{{CDN}\} {Measurement} {System}},
	shorttitle = {Odin},
	url = {https://www.usenix.org/conference/nsdi18/presentation/calder},
	language = {en},
	urldate = {2019-01-04},
	author = {Calder, Matt and Gao, Ryan and Schröder, Manuel and Stewart, Ryan and Padhye, Jitendra and Mahajan, Ratul and Ananthanarayanan, Ganesh and Katz-Bassett, Ethan},
	year = {2018},
	pages = {501--517},
	file = {Full Text PDF:/Users/rosscutler/Zotero/storage/3LZ2ZY9Z/Calder et al. - 2018 - Odin Microsoft’s Scalable Fault-Tolerant CDN Me.pdf:application/pdf;Snapshot:/Users/rosscutler/Zotero/storage/8ZDSGD4K/calder.html:text/html},
}

@article{dong_pcc_nodate,
	title = {{PCC} {Vivace}: {Online}-{Learning} {Congestion} {Control}},
	abstract = {TCP’s congestion control architecture suffers from notoriously bad performance. Consequently, recent years have witnessed a surge of interest in both academia and industry in novel approaches to congestion control. We show, however, that past approaches fall short of attaining ideal performance. We leverage ideas from the rich literature on online (convex) optimization in machine learning to design Vivace, a novel rate-control protocol, designed within the recently proposed PCC framework. Our theoretical and experimental analyses establish that Vivace signiﬁcantly outperforms traditional TCP variants, the previous realization of the PCC framework, and BBR in terms of performance (throughput, latency, loss), convergence speed, alleviating bufferbloat, reactivity to changing network conditions, and friendliness towards legacy TCP in a range of scenarios. Vivace requires only sender-side changes and is thus readily deployable.},
	language = {en},
	author = {Dong, Mo and Meng, Tong and Zarchy, Doron and Arslan, Engin and Gilad, Yossi and Godfrey, P Brighten and Schapira, Michael},
	pages = {15},
	file = {Dong et al. - PCC Vivace Online-Learning Congestion Control.pdf:/Users/rosscutler/Zotero/storage/XNG7726A/Dong et al. - PCC Vivace Online-Learning Congestion Control.pdf:application/pdf},
}

@inproceedings{kazer_fast_2018,
	address = {Redmond, WA, USA},
	title = {Fast {Network} {Simulation} {Through} {Approximation} or: {How} {Blind} {Men} {Can} {Describe} {Elephants}},
	isbn = {978-1-4503-6120-0},
	shorttitle = {Fast {Network} {Simulation} {Through} {Approximation} or},
	url = {http://dl.acm.org/citation.cfm?doid=3286062.3286083},
	doi = {10.1145/3286062.3286083},
	abstract = {Network researchers today are unable to test their new ideas at scale before deployment due to the prohibitive costs of custom testbeds and the slow speed of large-scale network simulators. Data center simulation is particularly slow because of the massive amount of bandwidth and high degree of redundant computation incurred in simulating the network stacks of thousands of commodity machines. By using approximation to replace redundant portions of the simulation, we improve computation time while retaining high accuracy.},
	language = {en},
	urldate = {2019-01-03},
	booktitle = {Proceedings of the 17th {ACM} {Workshop} on {Hot} {Topics} in {Networks}  - {HotNets} '18},
	publisher = {ACM Press},
	author = {Kazer, Charles W. and Sedoc, João and Ng, Kelvin K.W. and Liu, Vincent and Ungar, Lyle H.},
	year = {2018},
	keywords = {To Read},
	pages = {141--147},
	file = {Kazer et al. - 2018 - Fast Network Simulation Through Approximation or .pdf:/Users/rosscutler/Zotero/storage/JGAI2XSS/Kazer et al. - 2018 - Fast Network Simulation Through Approximation or .pdf:application/pdf},
}

@inproceedings{yan_pantheon:_2018,
	title = {Pantheon: the training ground for {Internet} congestion-control research},
	shorttitle = {Pantheon},
	url = {https://www.usenix.org/conference/atc18/presentation/yan-francis},
	language = {en},
	urldate = {2019-01-03},
	author = {Yan, Francis Y. and Ma, Jestin and Hill, Greg D. and Raghavan, Deepti and Wahby, Riad S. and Levis, Philip and Winstein, Keith},
	year = {2018},
	pages = {731--743},
	file = {Full Text PDF:/Users/rosscutler/Zotero/storage/TDAC99Z7/Yan et al. - 2018 - Pantheon the training ground for Internet congest.pdf:application/pdf;Snapshot:/Users/rosscutler/Zotero/storage/NVHVIVHA/yan-francis.html:text/html},
}

@inproceedings{jiang_via:_2016,
	address = {Florianopolis, Brazil},
	title = {Via: {Improving} {Internet} {Telephony} {Call} {Quality} {Using} {Predictive} {Relay} {Selection}},
	isbn = {978-1-4503-4193-6},
	shorttitle = {Via},
	url = {http://dl.acm.org/citation.cfm?doid=2934872.2934907},
	doi = {10.1145/2934872.2934907},
	abstract = {The use of the Internet for voice calls is here to stay. In spite of the volume and importance of Internet telephony, we have little understanding of (1) how network performance impacts user-perceived call quality, and (2) why and where such quality problems occur in the wild. To bridge this gap, we analyze a data set of 430 million calls from Skype, with clients across 1900 ASes and 126 countries. We observe that call quality problems are quite pervasive. More importantly, these problems are signiﬁcantly spread out geographically and over time, thereby making simple ﬁxes targeted at speciﬁc "pockets" of poor performance largely ineffective.},
	language = {en},
	urldate = {2018-11-06},
	booktitle = {Proceedings of the 2016 conference on {ACM} {SIGCOMM} 2016 {Conference} - {SIGCOMM} '16},
	publisher = {ACM Press},
	author = {Jiang, Junchen and Vafin, Renat and Zhang, Hui and Das, Rajdeep and Ananthanarayanan, Ganesh and Chou, Philip A. and Padmanabhan, Venkata and Sekar, Vyas and Dominique, Esbjorn and Goliszewski, Marcin and Kukoleca, Dalibor},
	year = {2016},
	pages = {286--299},
	file = {Jiang et al. - 2016 - Via Improving Internet Telephony Call Quality Usi.pdf:/Users/rosscutler/Zotero/storage/33FK3NN8/Jiang et al. - 2016 - Via Improving Internet Telephony Call Quality Usi.pdf:application/pdf},
}

@inproceedings{gupchup_trustworthy_2018,
	address = {New York, NY, USA},
	series = {{CIKM} '18},
	title = {Trustworthy {Experimentation} {Under} {Telemetry} {Loss}},
	copyright = {All rights reserved},
	isbn = {978-1-4503-6014-2},
	url = {http://doi.acm.org/10.1145/3269206.3271747},
	doi = {10.1145/3269206.3271747},
	abstract = {Failure to accurately measure the outcomes of an experiment can lead to bias and incorrect conclusions. Online controlled experiments (aka AB tests) are increasingly being used to make decisions to improve websites as well as mobile and desktop applications. We argue that loss of telemetry data (during upload or post-processing) can skew the results of experiments, leading to loss of statistical power and inaccurate or erroneous conclusions. By systematically investigating the causes of telemetry loss, we argue that it is not practical to entirely eliminate it. Consequently, experimentation systems need to be robust to its effects. Furthermore, we note that it is nontrivial to measure the absolute level of telemetry loss in an experimentation system. In this paper, we take a top-down approach towards solving this problem. We motivate the impact of loss qualitatively using experiments in real applications deployed at scale, and formalize the problem by presenting a theoretical breakdown of the bias introduced by loss. Based on this foundation, we present a general framework for quantitatively evaluating the impact of telemetry loss, and present two solutions to measure the absolute levels of loss. This framework is used by well-known applications at Microsoft, with millions of users and billions of sessions. These general principles can be adopted by any application to improve the overall trustworthiness of experimentation and data-driven decision making.},
	urldate = {2019-01-03},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Gupchup, Jayant and Hosseinkashi, Yasaman and Dmitriev, Pavel and Schneider, Daniel and Cutler, Ross and Jefremov, Andrei and Ellis, Martin},
	year = {2018},
	keywords = {ab testing, client experimentation, data loss, experimentation trustworthiness, online controlled experiments, telemetry loss},
	pages = {387--396},
}

@inproceedings{gupchup_analysis_2017,
	title = {Analysis of problem tokens to rank factors impacting quality in {VoIP} applications},
	copyright = {All rights reserved},
	doi = {10.1109/QoMEX.2017.7965648},
	abstract = {User-perceived quality-of-experience (QoE) in internet telephony systems is commonly evaluated using subjective ratings computed as a Mean Opinion Score (MOS). In such systems, while user MOS can be tracked on an ongoing basis, it does not give insight into which factors of a call induced any perceived degradation in QoE - it does not tell us what caused a user to have a sub-optimal experience. For effective planning of product improvements, we are interested in understanding the impact of each of these degrading factors, allowing the estimation of the return (i.e., the improvement in user QoE) for a given investment. To obtain such insights, we advocate the use of an end-of-call “problem token questionnaire” (PTQ) which probes the user about common call quality issues (e.g., distorted audio or frozen video) which they may have experienced. In this paper, we show the efficacy of this questionnaire using data gathered from over 700,000 end-of-call surveys gathered from Skype (a large commercial VoIP application). We present a method to rank call quality and reliability issues and address the challenge of isolating independent factors impacting the QoE. Finally, we present representative examples of how these problem tokens have proven to be useful in practice.},
	booktitle = {2017 {Ninth} {International} {Conference} on {Quality} of {Multimedia} {Experience} ({QoMEX})},
	author = {Gupchup, J. and Hosseinkashi, Y. and Ellis, M. and Johnson, S. and Cutler, R.},
	month = may,
	year = {2017},
	keywords = {VoIP, call quality issues, data analysis, Degradation, distorted audio, end-of-call surveys, frozen video, Internet telephony, internet telephony systems, mean opinion score, Measurement, MOS, problem token questionnaire, PTQ, QoE, Quality assessment, quality of experience, rank factors, Reactive power, reliability issues, Skype, Sociology, Statistics, telecommunication network reliability, user-perceived quality-of-experience, VoIP applications},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/7S4ZWKXR/7965648.html:text/html;Submitted Version:/Users/rosscutler/Zotero/storage/R8QFJ77E/Gupchup et al. - 2017 - Analysis of problem tokens to rank factors impacti.pdf:application/pdf},
}

@inproceedings{gupchup_design_2018,
	title = {On {Design} of {Problem} {Token} {Questions} in {Quality} of {Experience} {Surveys}},
	copyright = {All rights reserved},
	doi = {10.1109/QoMEX.2018.8463424},
	abstract = {User surveys for Quality of Experience (QoE) are a critical source of information for application developers. In addition to the common “star rating” used to estimate Mean Opinion Score (MOS), more detailed survey questions (problem tokens) about specific areas provide valuable insight into the factors impacting QoE. This paper explores two aspects of problem token questionnaire design. First, we study the bias introduced by fixed question order, and second, we provide a methodology to manage the size of the survey while keeping it informative. Based on 900,000 calls gathered using a randomized controlled experiment from Skype, we find that token selections can be strongly biased due to token positions and display design. This selection bias can be significantly reduced by randomizing the display order of tokens. It is worth noting that users respond to the randomized-order variant at levels that are comparable to the fixed-order variant. The effective selection of a subset of tokens is achieved by extracting tokens that provide the highest information gain over user ratings. This selection is known to be in the class of NP-hard problems. We apply a well-known greedy submodular maximization method on our dataset to capture 94\% of the information using just 30\% of thequestions.},
	booktitle = {2018 {Tenth} {International} {Conference} on {Quality} of {Multimedia} {Experience} ({QoMEX})},
	author = {Gupchup, J. and Beyrami, E. and Ellis, M. and Hosseinkashi, Y. and Johnson, S. and Cutler, R.},
	month = may,
	year = {2018},
	keywords = {VoIP, data analysis, mean opinion score, QoE, quality of experience, Sociology, application developers, common star rating, computational complexity, Correlation, detailed survey questions, Distortion, fixed question order, fixed-order variant, greedy algorithms, highest information gain, NP-hard problem, NP-hard problems, optimisation, problem token questionnaire design, problem token questions, Quality of experience, Random variables, randomized controlled experiment, randomized-order variant, social networking (online), Survey design, token selections, user ratings, user surveys},
	pages = {1--3},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/Q9E5AQSD/8463424.html:text/html;Submitted Version:/Users/rosscutler/Zotero/storage/WVP3Z5RJ/Gupchup et al. - 2018 - On Design of Problem Token Questions in Quality of.pdf:application/pdf},
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2019-07-31},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1503.02531 PDF:/Users/rosscutler/Zotero/storage/RVQGFC7F/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/HJUMPGK2/1503.html:text/html},
}

@article{lei_simple_2017,
	title = {Simple {Recurrent} {Units} for {Highly} {Parallelizable} {Recurrence}},
	url = {http://arxiv.org/abs/1709.02755},
	abstract = {Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model on translation by incorporating SRU into the architecture.},
	urldate = {2019-07-31},
	journal = {arXiv:1709.02755 [cs]},
	author = {Lei, Tao and Zhang, Yu and Wang, Sida I. and Dai, Hui and Artzi, Yoav},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.02755},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv\:1709.02755 PDF:/Users/rosscutler/Zotero/storage/QZ5XSQ77/Lei et al. - 2017 - Simple Recurrent Units for Highly Parallelizable R.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/K7XJVNII/1709.html:text/html},
}

@article{lehmann-willenbrock_our_2016,
	title = {Our love/hate relationship with meetings: {Relating} good and bad meeting behaviors to meeting outcomes, engagement, and exhaustion},
	volume = {39},
	issn = {2040-8269},
	shorttitle = {Our love/hate relationship with meetings},
	url = {http://www.emeraldinsight.com/doi/10.1108/MRR-08-2015-0195},
	doi = {10.1108/MRR-08-2015-0195},
	abstract = {Purpose – Employees at all organizational levels spend large portions of their work lives in meetings, many of which are not effective. Previous process-analytical research has identified counterproductive communication patterns to help explain why many meetings go wrong. This study aims to illustrate the ways in which counterproductive – and productive – meeting behaviors are related to individual work engagement and emotional exhaustion.},
	language = {en},
	number = {10},
	urldate = {2019-07-26},
	journal = {Management Research Review},
	author = {Lehmann-Willenbrock, Nale and Allen, Joseph A. and Belyeu, Dain},
	month = oct,
	year = {2016},
	pages = {1293--1312},
	file = {Lehmann-Willenbrock et al. - 2016 - Our lovehate relationship with meetings Relating.pdf:/Users/rosscutler/Zotero/storage/6E87DP62/Lehmann-Willenbrock et al. - 2016 - Our lovehate relationship with meetings Relating.pdf:application/pdf},
}

@article{a._allen_linking_2014,
	title = {Linking pre-meeting communication to meeting effectiveness},
	volume = {29},
	issn = {0268-3946},
	url = {http://www.emeraldinsight.com/doi/10.1108/JMP-09-2012-0265},
	doi = {10.1108/JMP-09-2012-0265},
	language = {en},
	number = {8},
	urldate = {2019-07-26},
	journal = {Journal of Managerial Psychology},
	author = {A. Allen, Joseph and Lehmann-Willenbrock, Nale and Landowski, Nicole},
	month = nov,
	year = {2014},
	pages = {1064--1081},
	file = {A. Allen et al. - 2014 - Linking pre-meeting communication to meeting effec.pdf:/Users/rosscutler/Zotero/storage/4GBDNYAB/A. Allen et al. - 2014 - Linking pre-meeting communication to meeting effec.pdf:application/pdf},
}

@inproceedings{bachl_rax:_2019,
	title = {Rax: {Deep} {Reinforcement} {Learning} for {Congestion} {Control}},
	shorttitle = {Rax},
	doi = {10.1109/ICC.2019.8761187},
	abstract = {This paper proposes Reactive Adaptive eXperience based congestion control (Rax), a new method of congestion control (CC) that uses online reinforcement learning (RL) to maintain an optimum congestion window with respect to a given reward function and based on current network conditions. We use a neural network based approach that can be initialized either with random weights or with a previously trained neural network to improve stability and convergence time. As the processing of rewards in CC depends on the arrival of acknowledgements, which are delayed and received one by one, the problem is not suitable for current implementations of Deep RL. As a remedy we propose Partial Action Learning, a formulation of Deep RL that supports delayed and partial rewards. We show that our method converges to a stable, close-to-optimum solution within minutes and outperforms existing CC algorithms in typical networks. Thus, this paper demonstrates that Deep RL can be done online and can compete with classic CC schemes such as Cubic.},
	booktitle = {{ICC} 2019 - 2019 {IEEE} {International} {Conference} on {Communications} ({ICC})},
	author = {Bachl, M. and Zseby, T. and Fabini, J.},
	month = may,
	year = {2019},
	keywords = {Reinforcement learning, Measurement, Biological neural networks, Microsoft Windows, Telecommunications, TV},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/V8YWDD79/8761187.html:text/html},
}

@article{gao_unified_2017,
	title = {A unified {DNN} approach to speaker-dependent simultaneous speech enhancement and speech separation in low {SNR} environments},
	volume = {95},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639316303843},
	doi = {10.1016/j.specom.2017.10.003},
	abstract = {We propose a uniﬁed speech enhancement framework to jointly handle both background noise and interfering speech in a speaker-dependent scenario based on deep neural networks (DNNs). We ﬁrst explore speaker-dependent speech enhancement that can signiﬁcantly improve system performance over speaker-independent systems. Next, we consider interfering speech as one noise type, thus a speaker-dependent DNN system can be adopted for both speech enhancement and separation. Experimental results demonstrate that the proposed uniﬁed system can achieve comparable performances to speciﬁc systems where only noise or speech interference is present. Furthermore, much better results can be obtained over individual enhancement or separation systems in mixed background noise and interfering speech scenarios. The training data for the two speciﬁc tasks are also found to be complementary. Finally, an ensemble learning-based framework is employed to further improve the system performance in low signal-to-noise ratio (SNR) environments. A voice activity detection (VAD) DNN and an ideal ratio mask (IRM) DNN are investigated to provide prior information to integrate two sub-modules at frame level and time-frequency level, respectively. The results demonstrate the eﬀectiveness of the ensemble method in low SNR environments.},
	language = {en},
	urldate = {2019-07-19},
	journal = {Speech Communication},
	author = {Gao, Tian and Du, Jun and Dai, Li-Rong and Lee, Chin-Hui},
	month = dec,
	year = {2017},
	pages = {28--39},
	file = {Gao et al. - 2017 - A unified DNN approach to speaker-dependent simult.pdf:/Users/rosscutler/Zotero/storage/6XC5DK5P/Gao et al. - 2017 - A unified DNN approach to speaker-dependent simult.pdf:application/pdf},
}

@article{wan_generalized_2017,
	title = {Generalized {End}-to-{End} {Loss} for {Speaker} {Verification}},
	url = {http://arxiv.org/abs/1710.10467},
	abstract = {In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10\%, while reducing the training time by 60\% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e. "OK Google" and "Hey Google") as well as multiple dialects.},
	urldate = {2019-07-19},
	journal = {arXiv:1710.10467 [cs, eess, stat]},
	author = {Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.10467},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv\:1710.10467 PDF:/Users/rosscutler/Zotero/storage/9RENUZBG/Wan et al. - 2017 - Generalized End-to-End Loss for Speaker Verificati.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/M89FPK9Y/1710.html:text/html;Wan et al. - 2017 - Generalized End-to-End Loss for Speaker Verificati.pdf:/Users/rosscutler/Zotero/storage/5WUSYX5P/Wan et al. - 2017 - Generalized End-to-End Loss for Speaker Verificati.pdf:application/pdf},
}

@inproceedings{bengio_curriculum_2009,
	address = {Montreal, Quebec, Canada},
	title = {Curriculum learning},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553380},
	doi = {10.1145/1553374.1553380},
	abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them “curriculum learning”. In the context of recent research studying the diﬃculty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that signiﬁcant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an eﬀect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
	language = {en},
	urldate = {2019-07-17},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
	year = {2009},
	pages = {1--8},
	file = {Bengio et al. - 2009 - Curriculum learning.pdf:/Users/rosscutler/Zotero/storage/5D7FFPNX/Bengio et al. - 2009 - Curriculum learning.pdf:application/pdf},
}

@inproceedings{varga_deeprn:_2018,
	address = {San Diego, CA},
	title = {Deeprn: {A} {Content} {Preserving} {Deep} {Architecture} for {Blind} {Image} {Quality} {Assessment}},
	isbn = {978-1-5386-1737-3},
	shorttitle = {Deeprn},
	url = {https://ieeexplore.ieee.org/document/8486528/},
	doi = {10.1109/ICME.2018.8486528},
	abstract = {This paper presents a blind image quality assessment (BIQA) method based on deep learning with convolutional neural networks (CNN). Our method is trained on full and arbitrarily sized images rather than small image patches or resized input images as usually done in CNNs for image classiﬁcation and quality assessment. The resolution independence is achieved by pyramid pooling. This work is the ﬁrst that applies a ﬁnetuned residual deep learning network (ResNet-101) to BIQA. The training is carried out on a new and very large, labeled dataset of 10,073 images (KonIQ-10k) that contains quality rating histograms besides the mean opinion scores (MOS). In contrast to previous methods we do not train to approximate the MOS directly, but rather use the distributions of scores. Experiments were carried out on three benchmark image quality databases. The results showed clear improvements of the accuracy of the estimated MOS values, compared to current state-of-the-art algorithms. We also report on the quality of the estimation of the score distributions.},
	language = {en},
	urldate = {2019-07-17},
	booktitle = {2018 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	publisher = {IEEE},
	author = {Varga, Domonkos and Saupe, Dietmar and Sziranyi, Tamas},
	month = jul,
	year = {2018},
	pages = {1--6},
	file = {Varga et al. - 2018 - Deeprn A Content Preserving Deep Architecture for.pdf:/Users/rosscutler/Zotero/storage/KP9S3Q88/Varga et al. - 2018 - Deeprn A Content Preserving Deep Architecture for.pdf:application/pdf},
}

@article{bianco_use_2018,
	title = {On the {Use} of {Deep} {Learning} for {Blind} {Image} {Quality} {Assessment}},
	volume = {12},
	issn = {1863-1703, 1863-1711},
	url = {http://arxiv.org/abs/1602.05531},
	doi = {10.1007/s11760-017-1166-8},
	abstract = {In this work we investigate the use of deep learning for distortion-generic blind image quality assessment. We report on diﬀerent design choices, ranging from the use of features extracted from pre-trained Convolutional Neural Networks (CNNs) as a generic image description, to the use of features extracted from a CNN ﬁne-tuned for the image quality task. Our best proposal, named DeepBIQ, estimates the image quality by average-pooling the scores predicted on multiple sub-regions of the original image. Experimental results on the LIVE In the Wild Image Quality Challenge Database show that DeepBIQ outperforms the state-ofthe-art methods compared, having a Linear Correlation Coeﬃcient (LCC) with human subjective scores of almost 0.91. These results are further conﬁrmed also on four benchmark databases of synthetically distorted images: LIVE, CSIQ, TID2008 and TID2013.},
	language = {en},
	number = {2},
	urldate = {2019-07-17},
	journal = {Signal, Image and Video Processing},
	author = {Bianco, Simone and Celona, Luigi and Napoletano, Paolo and Schettini, Raimondo},
	month = feb,
	year = {2018},
	note = {arXiv: 1602.05531},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {355--362},
	file = {Bianco et al. - 2018 - On the Use of Deep Learning for Blind Image Qualit.pdf:/Users/rosscutler/Zotero/storage/UUS77U3U/Bianco et al. - 2018 - On the Use of Deep Learning for Blind Image Qualit.pdf:application/pdf},
}

@article{nagrani_voxceleb:_2017,
	title = {{VoxCeleb}: a large-scale speaker identification dataset},
	shorttitle = {{VoxCeleb}},
	url = {http://arxiv.org/abs/1706.08612},
	abstract = {Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification.},
	urldate = {2019-07-17},
	journal = {arXiv:1706.08612 [cs]},
	author = {Nagrani, Arsha and Chung, Joon Son and Zisserman, Andrew},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.08612},
	keywords = {Computer Science - Sound},
	file = {arXiv\:1706.08612 PDF:/Users/rosscutler/Zotero/storage/QXUPGMI3/Nagrani et al. - 2017 - VoxCeleb a large-scale speaker identification dat.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/8CI439DF/1706.html:text/html},
}

@inproceedings{panayotov_librispeech:_2015,
	title = {Librispeech: {An} {ASR} corpus based on public domain audio books},
	shorttitle = {Librispeech},
	doi = {10.1109/ICASSP.2015.7178964},
	abstract = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Panayotov, V. and Chen, G. and Povey, D. and Khudanpur, S.},
	month = apr,
	year = {2015},
	keywords = {speech recognition, acoustic models, ASR corpus, Bioinformatics, Blogs, Corpus, Electronic publishing, evaluating speech recognition systems, frequency 16 kHz, Genomics, Information services, Kaldi scripts, language-model training data, LibriSpeech corpus, LibriVox, LibriVox project, natural language processing, pre-built language models, public domain audio books, read english speech, Resource description framework, Speech Recognition, training speech recognition systems, Wall Street Journal, WSJ},
	pages = {5206--5210},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/JHETWMQJ/7178964.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/BQMAPNVH/Panayotov et al. - 2015 - Librispeech An ASR corpus based on public domain .pdf:application/pdf},
}

@inproceedings{pandey_tcnn:_2019,
	address = {Brighton, United Kingdom},
	title = {{TCNN}: {Temporal} {Convolutional} {Neural} {Network} for {Real}-time {Speech} {Enhancement} in the {Time} {Domain}},
	isbn = {978-1-4799-8131-1},
	shorttitle = {{TCNN}},
	url = {https://ieeexplore.ieee.org/document/8683634/},
	doi = {10.1109/ICASSP.2019.8683634},
	urldate = {2019-07-16},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Pandey, Ashutosh and Wang, DeLiang},
	month = may,
	year = {2019},
	pages = {6875--6879},
}

@inproceedings{wang_oracle_2016,
	address = {Xi'an, China},
	title = {Oracle performance investigation of the ideal masks},
	isbn = {978-1-5090-2007-2},
	url = {http://ieeexplore.ieee.org/document/7602888/},
	doi = {10.1109/IWAENC.2016.7602888},
	urldate = {2019-07-16},
	booktitle = {2016 {IEEE} {International} {Workshop} on {Acoustic} {Signal} {Enhancement} ({IWAENC})},
	publisher = {IEEE},
	author = {Wang, Ziteng and Wang, Xiaofei and Li, Xu and Fu, Qiang and Yan, Yonghong},
	month = sep,
	year = {2016},
	pages = {1--5},
}

@article{lee_phase-sensitive_2018,
	title = {Phase-{Sensitive} {Joint} {Learning} {Algorithms} for {Deep} {Learning}-{Based} {Speech} {Enhancement}},
	volume = {25},
	issn = {1070-9908, 1558-2361},
	url = {https://ieeexplore.ieee.org/document/8392411/},
	doi = {10.1109/LSP.2018.2849578},
	number = {8},
	urldate = {2019-07-16},
	journal = {IEEE Signal Processing Letters},
	author = {Lee, Jinkyu and Skoglund, Jan and Shabestary, Turaj and Kang, Hong-Goo},
	month = aug,
	year = {2018},
	pages = {1276--1280},
}

@article{choi_phase-aware_2019,
	title = {Phase-aware {Speech} {Enhancement} with {Deep} {Complex} {U}-{Net}},
	url = {http://arxiv.org/abs/1903.03107},
	abstract = {Most deep learning-based models for speech enhancement have mainly focused on estimating the magnitude of spectrogram while reusing the phase from noisy speech for reconstruction. This is due to the difficulty of estimating the phase of clean speech. To improve speech enhancement performance, we tackle the phase estimation problem in three ways. First, we propose Deep Complex U-Net, an advanced U-Net structured model incorporating well-defined complex-valued building blocks to deal with complex-valued spectrograms. Second, we propose a polar coordinate-wise complex-valued masking method to reflect the distribution of complex ideal ratio masks. Third, we define a novel loss function, weighted source-to-distortion ratio (wSDR) loss, which is designed to directly correlate with a quantitative evaluation measure. Our model was evaluated on a mixture of the Voice Bank corpus and DEMAND database, which has been widely used by many deep learning models for speech enhancement. Ablation experiments were conducted on the mixed dataset showing that all three proposed approaches are empirically valid. Experimental results show that the proposed method achieves state-of-the-art performance in all metrics, outperforming previous approaches by a large margin.},
	urldate = {2019-07-16},
	journal = {arXiv:1903.03107 [cs, eess, stat]},
	author = {Choi, Hyeong-Seok and Kim, Jang-Hyun and Huh, Jaesung and Kim, Adrian and Ha, Jung-Woo and Lee, Kyogu},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.03107},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/GXR7VVV2/1903.html:text/html},
}

@article{qian_autovc:_2019,
	title = {{AUTOVC}: {Zero}-{Shot} {Voice} {Style} {Transfer} with {Only} {Autoencoder} {Loss}},
	shorttitle = {{AUTOVC}},
	url = {http://arxiv.org/abs/1905.05879},
	abstract = {Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain under-explored areas. Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this field. However, GAN training is sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. On the other hand, CVAE training is simple but does not come with the distribution-matching property of a GAN. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on a self-reconstruction loss. Based on this scheme, we proposed AUTOVC, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.},
	urldate = {2019-07-16},
	journal = {arXiv:1905.05879 [cs, eess, stat]},
	author = {Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and Hasegawa-Johnson, Mark},
	month = may,
	year = {2019},
	note = {arXiv: 1905.05879},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1905.05879 PDF:/Users/rosscutler/Zotero/storage/RX3L3WER/Qian et al. - 2019 - AUTOVC Zero-Shot Voice Style Transfer with Only A.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/KB45YKCL/1905.html:text/html},
}

@article{yan_continual_2019,
	title = {Continual learning improves {Internet} video streaming},
	url = {http://arxiv.org/abs/1906.01113},
	abstract = {We describe Fugu, a continual learning algorithm for bitrate selection in streaming video. Each day, Fugu retrains a neural network from its experience in deployment over the prior week. The neural network predicts how long it would take to transfer each available version of the upcoming video chunks, given recent history and internal TCP statistics. We evaluate Fugu with Puffer, a public website we built that streams live TV using Fugu and existing algorithms. Over a nine-day period in January 2019, Puffer streamed 8,131 hours of video to 3,719 unique users. Compared with buffer-based control, MPC, RobustMPC, and Pensieve, Fugu performs better on several metrics: it stalls 5-13x less and has better and more stable video quality, and users who were randomly assigned to Fugu streamed for longer on average before quitting or reloading. We find that TCP statistics can aid bitrate selection, that congestion-control and bitrate-selection algorithms are best selected jointly, that probabilistic transfer-time estimates that consider chunk size outperform point estimates of throughput, and that both stalls and video quality have an influence on how long users choose to keep a video stream playing. Fugu's results suggest that continual learning of network algorithms in situ is a promising area of research. To support further investigation, we plan to operate Puffer for several years and will open it to researchers for evaluating new congestion-control and bitrate-selection approaches.},
	urldate = {2019-07-16},
	journal = {arXiv:1906.01113 [cs]},
	author = {Yan, Francis Y. and Ayers, Hudson and Zhu, Chenzhi and Fouladi, Sadjad and Hong, James and Zhang, Keyi and Levis, Philip and Winstein, Keith},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.01113},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {arXiv\:1906.01113 PDF:/Users/rosscutler/Zotero/storage/WAYZR8ZV/Yan et al. - 2019 - Continual learning improves Internet video streami.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/NQJZXZ63/1906.html:text/html},
}

@article{espeholt_impala:_2018,
	title = {{IMPALA}: {Scalable} {Distributed} {Deep}-{RL} with {Importance} {Weighted} {Actor}-{Learner} {Architectures}},
	shorttitle = {{IMPALA}},
	url = {http://arxiv.org/abs/1802.01561},
	abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
	urldate = {2019-07-16},
	journal = {arXiv:1802.01561 [cs]},
	author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.01561},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1802.01561 PDF:/Users/rosscutler/Zotero/storage/5M4X6354/Espeholt et al. - 2018 - IMPALA Scalable Distributed Deep-RL with Importan.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/2VPC2CXT/1802.html:text/html},
}

@article{hessel_rainbow:_2017,
	title = {Rainbow: {Combining} {Improvements} in {Deep} {Reinforcement} {Learning}},
	shorttitle = {Rainbow},
	url = {http://arxiv.org/abs/1710.02298},
	abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
	urldate = {2019-07-15},
	journal = {arXiv:1710.02298 [cs]},
	author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.02298},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1710.02298 PDF:/Users/rosscutler/Zotero/storage/MW8CVFBP/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/D3MKE7KJ/1710.html:text/html;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/UULZXQ2M/1710.html:text/html},
}

@article{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2019-07-15},
	journal = {arXiv:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.06347},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1707.06347 PDF:/Users/rosscutler/Zotero/storage/2FY2N3P7/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/2C7VUTHU/1707.html:text/html},
}

@article{kazak_verifying_2019,
	title = {Verifying {Deep}-{RL}-{Driven} {Systems}},
	abstract = {Deep reinforcement learning (RL) has recently been successfully applied to networking contexts including routing, flow scheduling, congestion control, packet classification, cloud resource management, and video streaming. Deep-RL-driven systems automate decision making, and have been shown to outperform state-of-the-art handcrafted systems in important domains. However, the (typical) non-explainability of decisions induced by the deep learning machinery employed by these systems renders reasoning about crucial system properties, including correctness and security, extremely difficult. We show that despite the obscurity of decision making in these contexts, verifying that deep-RL-driven systems adhere to desired, designer-specified behavior, is achievable. To this end, we initiate the study of formal verification of deep RL and present Verily, a system for verifying deep-RL-based systems that leverages recent advances in verification of deep neural networks. We employ Verily to verify recently-introduced deep-RL-driven systems for adaptive video streaming, cloud resource management, and Internet congestion control. Our results expose scenarios in which deep-RL-driven decision making yields undesirable behavior. We discuss guidelines for building deep-RL-driven systems that are both safer and easier to verify.},
	language = {en},
	author = {Kazak, Yafim and Barrett, Clark and Katz, Guy and Schapira, Michael},
	year = {2019},
	pages = {7},
	file = {Kazak et al. - 2019 - Verifying Deep-RL-Driven Systems.pdf:/Users/rosscutler/Zotero/storage/XVANU2VU/Kazak et al. - 2019 - Verifying Deep-RL-Driven Systems.pdf:application/pdf},
}

@article{dethise_cracking_2019,
	title = {Cracking {Open} the {Black} {Box}: {What} {Observations} {Can} {Tell} {Us} {About} {Reinforcement} {Learning} {Agents}},
	abstract = {Machine learning (ML) solutions to challenging networking problems, while promising, are hard to interpret; the uncertainty about how they would behave in untested scenarios has hindered adoption. Using a case study of an ML-based video rate adaptation model, we show that carefully applying interpretability tools and systematically exploring the model inputs can identify unwanted or anomalous behaviors of the model; hinting at a potential path towards increasing trust in ML-based solutions.},
	language = {en},
	author = {Dethise, Arnaud and Canini, Marco and Kandula, Srikanth},
	year = {2019},
	pages = {8},
	file = {cotbb.netai19.pdf:/Users/rosscutler/Zotero/storage/PARH6AXL/cotbb.netai19.pdf:application/pdf},
}

@article{ephrat_looking_2018,
	title = {Looking to {Listen} at the {Cocktail} {Party}: {A} {Speaker}-{Independent} {Audio}-{Visual} {Model} for {Speech} {Separation}},
	volume = {37},
	issn = {07300301},
	shorttitle = {Looking to {Listen} at the {Cocktail} {Party}},
	url = {http://arxiv.org/abs/1804.03619},
	doi = {10.1145/3197517.3201357},
	abstract = {We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to "focus" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).},
	language = {en},
	number = {4},
	urldate = {2019-07-15},
	journal = {ACM Transactions on Graphics},
	author = {Ephrat, Ariel and Mosseri, Inbar and Lang, Oran and Dekel, Tali and Wilson, Kevin and Hassidim, Avinatan and Freeman, William T. and Rubinstein, Michael},
	month = jul,
	year = {2018},
	note = {arXiv: 1804.03619},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {1--11},
	file = {arXiv\:1804.03619 PDF:/Users/rosscutler/Zotero/storage/RARVLGEP/Ephrat et al. - 2018 - Looking to Listen at the Cocktail Party A Speaker.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/WQIAHHDZ/1804.html:text/html;Ephrat et al. - 2018 - Looking to Listen at the Cocktail Party A Speaker.pdf:/Users/rosscutler/Zotero/storage/CKRUCRL5/Ephrat et al. - 2018 - Looking to Listen at the Cocktail Party A Speaker.pdf:application/pdf;Ephrat et al. - 2018 - Looking to Listen at the Cocktail Party A Speaker.pdf:/Users/rosscutler/Zotero/storage/9W47R5YA/Ephrat et al. - 2018 - Looking to Listen at the Cocktail Party A Speaker.pdf:application/pdf;Ephrat et al. - 2018 - Looking to Listen at the Cocktail Party A Speaker.pdf:/Users/rosscutler/Zotero/storage/ULUZWACX/ULUZWACX.pdf:application/pdf},
}

@article{zhang_ensemble_2019,
	title = {Ensemble {Adaptive} {Streaming} - {A} {New} {Paradigm} to {Generate} {Streaming} {Algorithms} via {Specializations}},
	issn = {1536-1233},
	doi = {10.1109/TMC.2019.2909202},
	abstract = {Video streaming is now ubiquitous in the mobile Internet. This motivated intense research in adaptive streaming algorithms to tackle mobile networks' fluctuating conditions. Our investigations revealed that while existing algorithms can perform well in their intended operating environments, their performance can degrade substantially in other environments. This work tackles this challenge by developing a novel Ensemble Adaptive Streaming (EAS) paradigm to mobile video streaming. As opposed to designing a single streaming algorithm for all network conditions, we argue that different network conditions require different algorithms. We introduce the notion of network differentiators to segregate network conditions into different classes where each class has its own adaptation algorithm designed and optimized specifically for it. An EAS mobile streaming client then selects at runtime the matching adaptation algorithm using the same network differentiators on a per session basis for streaming. We show how EAS can be applied to existing machine-learning approaches to improve their performances. Moreover, to fully exploit EAS's potential we developed the first Genetic Programming approach to evolve adaptive streaming algorithms. The resultant EAS-GP algorithms not only outperformed state-of-the-art algorithms substantially, but also exhibited remarkable robustness over time, location, mobile operators, as well as quality-of-experience metrics.},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Zhang, G. and Lee, J. Y. B.},
	year = {2019},
	keywords = {Bit rate, Quality of experience, Adaptive systems, Genetic Programming, Mobile computing, Mobile Network, Quality-of-experience, Streaming media, Throughput, Video Streaming},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/2ENFQRDK/8681142.html:text/html;PDF:/Users/rosscutler/Zotero/storage/K6W6V666/Zhang, Lee - 2019 - Ensemble Adaptive Streaming - A New Paradigm to Generate Streaming Algorithms via Specializations.pdf:application/pdf},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2019-07-14},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
	file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:/Users/rosscutler/Zotero/storage/N77N4AR8/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf;Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:/Users/rosscutler/Zotero/storage/GLS6VFMY/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf},
}

@article{seo_jnd-salcar:_nodate,
	title = {{JND}-{SalCAR}: {A} {Novel} {JND}-based {Saliency}-{Channel} {Attention} {Residual} {Network} for {Image} {Quality} {Prediction}},
	abstract = {In image quality enhancement processing, it is the most important to predict how humans perceive processed images since human observers are the ultimate receivers of the images. Thus, objective image quality assessment (IQA) methods based on human visual sensitivity from psychophysical experiments have been extensively studied. Thanks to the powerfulness of deep convolutional neural networks (CNN), many CNN based IQA models have been studied. However, previous CNN-based IQA models have not fully utilized the characteristics of human visual systems (HVS) for IQA problems by simply entrusting everything to CNN where the CNN-based models are often trained as a regressor to predict the scores of subjective quality assessment obtained from IQA datasets. In this paper, we propose a novel JND-based saliency-channel attention residual network for image quality assessment, called JND-SalCAR, where the human psychophysical characteristics such as visual saliency and just noticeable difference (JND) are effectively incorporated. We newly propose a SalCAR block so that perceptually important features can be extracted by using a saliency-based spatial attention and a channel attention. In addition, the visual saliency map is further used as a guideline for predicting the patch weight map in order to afford a stable training of end-to-end optimization for the JND-SalCAR. To our best knowledge, our work is the first HVS-inspired trainable IQA network that considers both the visual saliency and JND characteristics of HVS. We evaluate the proposed JND-SalCAR on large IQA datasets where it outperforms all the recent state-of-the-art IQA methods.},
	language = {en},
	author = {Seo, Soomin and Ki, Sehwan and Kim, Munchurl},
	pages = {12},
	file = {Seo et al. - JND-SalCAR A Novel JND-based Saliency-Channel Att.pdf:/Users/rosscutler/Zotero/storage/8DT2IT9T/Seo et al. - JND-SalCAR A Novel JND-based Saliency-Channel Att.pdf:application/pdf},
}

@article{jiang_deep_2019,
	title = {Deep {Optimization} model for {Screen} {Content} {Image} {Quality} {Assessment} using {Neural} {Networks}},
	url = {http://arxiv.org/abs/1903.00705},
	abstract = {In this paper, we propose a novel quadratic optimized model based on the deep convolutional neural network (QODCNN) for full-reference and no-reference screen content image (SCI) quality assessment. Unlike traditional CNN methods taking all image patches as training data and using average quality pooling, our model is optimized to obtain a more effective model including three steps. In the first step, an end-to-end deep CNN is trained to preliminarily predict the image visual quality, and batch normalized (BN) layers and l2 regularization are employed to improve the speed and performance of network fitting. For second step, the pretrained model is fine-tuned to achieve better performance under analysis of the raw training data. An adaptive weighting method is proposed in the third step to fuse local quality inspired by the perceptual property of the human visual system (HVS) that the HVS is sensitive to image patches containing texture and edge information. The novelty of our algorithm can be concluded as follows: 1) with the consideration of correlation between local quality and subjective differential mean opinion score (DMOS), the Euclidean distance is utilized to measure effectiveness of image patches, and the pretrained model is fine-tuned with more effective training data; 2) an adaptive pooling approach is employed to fuse patch quality of textual and pictorial regions, whose feature only extracted from distorted images owns strong noise robust and effects on both FR and NR IQA; 3) Considering the characteristics of SCIs, a deep and valid network architecture is designed for both NR and FR visual quality evaluation of SCIs. Experimental results verify that our model outperforms both current no-reference and full-reference image quality assessment methods on the benchmark screen content image quality assessment database (SIQAD).},
	urldate = {2019-07-12},
	journal = {arXiv:1903.00705 [cs]},
	author = {Jiang, Xuhao and Shen, Liquan and Feng, Guorui and Yu, Liangwei and An, Ping},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.00705},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1903.00705 PDF:/Users/rosscutler/Zotero/storage/EH4S8LN9/Jiang et al. - 2019 - Deep Optimization model for Screen Content Image Q.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/CKL9DCEH/1903.html:text/html},
}

@techreport{aytar_soundnet:_nodate,
	title = {{SoundNet}: {Learning} {Sound} {Representations} from {Unlabeled} {Video}},
	abstract = {We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.},
	urldate = {2019-06-05},
	author = {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
	note = {arXiv: 1610.09001v1},
	file = {PDF:/Users/rosscutler/Zotero/storage/DTGZL8GZ/Aytar, Vondrick, Torralba - Unknown - SoundNet Learning Sound Representations from Unlabeled Video.pdf:},
}

@techreport{wang_trainable_nodate,
	title = {Trainable {Frontend} {For} {Robust} and {Far}-{Field} {Keyword} {Spotting}},
	abstract = {Robust and far-field speech recognition is critical to enable true hands-free communication. In far-field conditions, signals are attenuated due to distance. To improve robustness to loudness variation, we introduce a novel frontend called per-channel energy normalization (PCEN). The key ingredient of PCEN is the use of an automatic gain control based dynamic compression to replace the widely used static (such as log or root) compression. We evaluate PCEN on the keyword spotting task. On our large rerecorded noisy and far-field eval sets, we show that PCEN significantly improves recognition performance. Furthermore, we model PCEN as neural network layers and optimize high-dimensional PCEN parameters jointly with the keyword spotting acoustic model. The trained PCEN fron-tend demonstrates significant further improvements without increasing model complexity or inference-time cost.},
	urldate = {2019-05-24},
	author = {Wang, Yuxuan and Getreuer, Pascal and Hughes, Thad and Lyon, Richard F and Saurous, Rif A},
	note = {arXiv: 1607.05666v1},
	keywords = {automatic gain control, deep neural networks, Index Terms: Keyword spotting, robust and far-field speech recognition},
	file = {PDF:/Users/rosscutler/Zotero/storage/PWCKJNQZ/Wang et al. - Unknown - Trainable Frontend For Robust and Far-Field Keyword Spotting.pdf:},
}

@techreport{prabhavalkar_automatic_nodate,
	title = {{AUTOMATIC} {GAIN} {CONTROL} {AND} {MULTI}-{STYLE} {TRAINING} {FOR} {ROBUST} {SMALL}-{FOOTPRINT} {KEYWORD} {SPOTTING} {WITH} {DEEP} {NEURAL} {NETWORKS}},
	abstract = {We explore techniques to improve the robustness of small-footprint keyword spotting models based on deep neural networks (DNNs) in the presence of background noise and in far-field conditions. We find that system performance can be improved significantly, with relative improvements up to 75\% in far-field conditions, by employing a combination of multi-style training and a proposed novel formulation of automatic gain control (AGC) that estimates the levels of both speech and background noise. Further, we find that these techniques allow us to achieve competitive performance, even when applied to DNNs with an order of magnitude fewer parameters than our base-line.},
	urldate = {2019-06-28},
	author = {Prabhavalkar, Rohit and Alvarez, Raziel and Parada, Carolina and Nakkiran, Preetum and Sainath, Tara N},
	keywords = {automatic gain control, Index Terms-keyword spotting, multi-style training, small-footprint models},
	file = {PDF:/Users/rosscutler/Zotero/storage/895XDEZT/Prabhavalkar et al. - Unknown - AUTOMATIC GAIN CONTROL AND MULTI-STYLE TRAINING FOR ROBUST SMALL-FOOTPRINT KEYWORD SPOTTING WITH DEEP NE.pdf:},
}

@techreport{hershey_cnn_nodate,
	title = {{CNN} {ARCHITECTURES} {FOR} {LARGE}-{SCALE} {AUDIO} {CLASSIFICATION}},
	abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
	urldate = {2019-05-09},
	author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P W and Gemmeke, Jort F and Jansen, Aren and Channing Moore, R and Plakal, Manoj and Platt, Devin and Saurous, Rif A and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J and Wilson, Kevin},
	note = {arXiv: 1609.09430v2},
	keywords = {Acoustic Scene Clas-sification, Convolutional Neural Networks, Deep Neural Networks, Index Terms-Acoustic Event Detection, Video Classification},
	file = {PDF:/Users/rosscutler/Zotero/storage/IM6ZL9HU/Hershey et al. - Unknown - CNN ARCHITECTURES FOR LARGE-SCALE AUDIO CLASSIFICATION.pdf:},
}

@techreport{jansen_large-scale_nodate,
	title = {{LARGE}-{SCALE} {AUDIO} {EVENT} {DISCOVERY} {IN} {ONE} {MILLION} {YOUTUBE} {VIDEOS}},
	abstract = {Internet videos provide a virtually boundless source of audio with a conspicuous lack of localized annotations, presenting an ideal setting for unsupervised methods. With this motivation, we perform an unprecedented exploration into the large-scale discovery of recurring audio events in a diverse corpus of one million YouTube videos (45K hours of audio). Our approach is to apply a streaming, non-parametric clustering algorithm to both spectral features and out-of-domain neural audio embeddings. We use a small portion of manually annotated audio events to quantitatively estimate the intrinsic clustering performance. In addition to providing a useful mechanism for unsupervised active learning, we demonstrate the effectiveness of the discovered audio event clusters in two downstream applications. The first is weakly-supervised learning, where we exploit the association of video-level metadata and cluster occurrences to temporally localize audio events. The second is informative activity detection, an unsupervised method for semantic saliency based on the corpus statistics of the discovered event clusters.},
	urldate = {2019-05-09},
	author = {Jansen, Aren and Gemmeke, Jort F and Ellis, Daniel P W and Liu, Xiaofeng and Lawrence, Wade and Freedman, Dylan},
	keywords = {Index Terms-Audio event discovery, streaming clustering algorithms, unsupervised learning, weakly-supervised learning},
	file = {PDF:/Users/rosscutler/Zotero/storage/665D8G2V/Jansen et al. - Unknown - LARGE-SCALE AUDIO EVENT DISCOVERY IN ONE MILLION YOUTUBE VIDEOS.pdf:},
}

@techreport{klejsa_per_hedelin_cong_zhou_high-quality_nodate,
	title = {{HIGH}-{QUALITY} {SPEECH} {CODING} {WITH} {SAMPLE} {RNN}},
	abstract = {We provide a speech coding scheme employing a generative model based on SampleRNN that, while operating at significantly lower bitrates, matches or surpasses the perceptual quality of state-of-the-art classic wide-band codecs. Moreover, it is demonstrated that the proposed scheme can provide a meaningful rate-distortion trade-off without retraining. We evaluate the proposed scheme in a series of listening tests and discuss limitations of the approach.},
	urldate = {2019-05-24},
	author = {Klejsa Per Hedelin Cong Zhou, Janusz and Fejgin, Roy and Villemoes, Lars},
	note = {arXiv: 1811.03021v1
ISBN: 1811.03021v1},
	keywords = {deep neural networks, Index Terms-speech coding, SampleRNN, vocoder},
	file = {PDF:/Users/rosscutler/Zotero/storage/IQSML697/Klejsa Per Hedelin Cong Zhou, Fejgin, Villemoes - Unknown - HIGH-QUALITY SPEECH CODING WITH SAMPLE RNN.pdf:},
}

@article{purwins_deep_2019,
	title = {Deep {Learning} for {Audio} {Signal} {Processing}},
	volume = {14},
	url = {http://doi.org/10.1109/JSTSP.2019.2908700},
	doi = {10.1109/JSTSP.2019.2908700},
	abstract = {Given the recent surge in developments of deep learning, this article provides a review of the state-of-the-art deep learning techniques for audio signal processing. Speech, music, and environmental sound processing are considered side-by-side, in order to point out similarities and differences between the domains, highlighting general methods, problems, key references , and potential for cross-fertilization between areas. The dominant feature representations (in particular, log-mel spectra and raw waveform) and deep learning models are reviewed, including convolutional neural networks, variants of the long short-term memory architecture, as well as more audio-specific neural network models. Subsequently, prominent deep learning application areas are covered, i.e. audio recognition (automatic speech recognition, music information retrieval, environmental sound detection, localization and tracking) and synthesis and transformation (source separation, audio enhancement, genera-tive models for speech, sound, and music synthesis). Finally, key issues and future questions regarding deep learning applied to audio signal processing are identified.},
	number = {8},
	urldate = {2019-05-19},
	journal = {In Journal of Selected Topics of Signal Processing},
	author = {Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Schlüter, Jan and Chang, Shuo-Yiin and Sainath, Tara},
	year = {2019},
	note = {arXiv: 1905.00078v1},
	keywords = {source separation, audio enhancement, automatic speech recognition, connectionist temporal memory, environmental sounds, Index Terms-deep learning, music information retrieval},
	file = {PDF:/Users/rosscutler/Zotero/storage/LMSDJGFY/Purwins et al. - 2019 - Deep Learning for Audio Signal Processing.pdf:application/pdf},
}

@article{kim_anique+:_2007,
	title = {{ANIQUE}+: {A} new {American} national standard for non-intrusive estimation of narrowband speech quality},
	volume = {12},
	issn = {10897089},
	doi = {10.1002/bltj.20228},
	abstract = {Spiess, J., Joens, Y. T., Dragnea, R., \& Spencer, P. (2014). Using Big Data to Improve Customer Experience and Business Performance. Bell Labs Technical Journal, 18(4), 3–17. doi:10.1002/bltj},
	number = {1},
	journal = {Bell Labs Technical Journal},
	author = {Kim, Doh Suk and Tarraf, Ahmed},
	month = mar,
	year = {2007},
	pages = {221--236},
	file = {PDF:/Users/rosscutler/Zotero/storage/QKEKUY3X/Kim, Tarraf - 2007 - ANIQUE A new American national standard for non-intrusive estimation of narrowband speech quality.pdf:application/pdf},
}

@article{noauthor_subjective_2003,
	title = {Subjective test methodology for evaluating speech communication systems that include noise suppression algorithm},
	abstract = {SERIES P: TELEPHONE TRANSMISSION QUALITY, TELEPHONE INSTALLATIONS, LOCAL LINE NETWORKS Methods for objective and subjective assessment of quality},
	journal = {ITU-T Recommendation P.835},
	year = {2003},
	note = {Place: Geneva, Switzerland
Publisher: ITU},
	file = {PDF:/Users/rosscutler/Zotero/storage/FARTQD8Y/Unknown - 2003 - Subjective test methodology for evaluating speech communication systems that include noise suppression algorithm.pdf:application/pdf},
}

@article{hu_evaluation_2008,
	title = {Evaluation of {Objective} {Quality} {Measures} for {Speech} {Enhancement}},
	volume = {16},
	url = {http://www.utdallas.edu/loizou/speech/noizeus/},
	doi = {10.1109/TASL.2007.911054},
	abstract = {In this paper, we evaluate the performance of several objective measures in terms of predicting the quality of noisy speech enhanced by noise suppression algorithms. The objective measures considered a wide range of distortions introduced by four types of real-world noise at two signal-to-noise ratio levels by four classes of speech enhancement algorithms: spectral subtrac-tive, subspace, statistical-model based, and Wiener algorithms. The subjective quality ratings were obtained using the ITU-T P.835 methodology designed to evaluate the quality of enhanced speech along three dimensions: signal distortion, noise distortion, and overall quality. This paper reports on the evaluation of correlations of several objective measures with these three subjective rating scales. Several new composite objective measures are also proposed by combining the individual objective measures using nonparametric and parametric regression analysis techniques. Index Terms-Objective measures, speech enhancement, speech quality assessment, subjective listening tests.},
	number = {1},
	urldate = {2019-07-01},
	journal = {IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
	author = {Hu, Yi and Loizou, Philipos C and Member, Senior},
	year = {2008},
	pages = {229},
	file = {PDF:/Users/rosscutler/Zotero/storage/VIEC9FDK/Hu, Loizou, Member - 2008 - Evaluation of Objective Quality Measures for Speech Enhancement.pdf:application/pdf},
}

@article{hines_visqol:_2015,
	title = {{ViSQOL}: an objective speech quality model},
	url = {http://arrow.dit.ie/scschcomart},
	doi = {10.1186/s13636-015-0054-9},
	abstract = {© 2015, Hines et al.; licensee Springer.This paper presents an objective speech quality model, ViSQOL, the Virtual Speech Quality Objective Listener. It is a signal-based, full-reference, intrusive metric that models human speech quality perception using a spectro-temporal measure of similarity between a reference and a test speech signal. The metric has been particularly designed to be robust for quality issues associated with Voice over IP (VoIP) transmission. This paper describes the algorithm and compares the quality predictions with the ITU-T standard metrics PESQ and POLQA for common problems in VoIP: clock drift, associated time warping, and playout delays. The results indicate that ViSQOL and POLQA significantly outperform PESQ, with ViSQOL competing well with POLQA. An extensive benchmarking against PESQ, POLQA, and simpler distance metrics using three speech corpora (NOIZEUS and E4 and the ITU-T P.Sup. 23 database) is also presented. These experiments benchmark the performance for a wide range of quality impairments, including VoIP degradations, a variety of background noise types, speech enhancement methods, and SNR levels. The results and subsequent analysis show that both ViSQOL and POLQA have some performance weaknesses and under-predict perceived quality in certain VoIP conditions. Both have a wider application and robustness to conditions than PESQ or more trivial distance metrics. ViSQOL is shown to offer a useful alternative to POLQA in predicting speech quality in VoIP scenarios.},
	number = {1},
	urldate = {2019-03-28},
	journal = {Eurasip Journal on Audio, Speech, and Music Processing},
	author = {Hines, Andrew and Skoglund, Jan and Kokaram, Anil C and Harte, Naomi},
	year = {2015},
	keywords = {NSIM, Objective speech quality, P.853, PESQ, POLQA, ViSQOL},
	pages = {13},
	file = {PDF:/Users/rosscutler/Zotero/storage/SIN4RE29/Hines et al. - 2015 - ViSQOL an objective speech quality model.pdf:application/pdf},
}

@techreport{lo_mosnet:_nodate,
	title = {{MOSNet}: {Deep} {Learning}-based {Objective} {Assessment} for {Voice} {Conversion}},
	abstract = {Existing objective evaluation metrics for voice conversion (VC) are not always correlated with human perception. Therefore , training VC models with such criteria may not effectively improve naturalness and similarity of converted speech. In this paper, we propose deep learning-based assessment models to predict human ratings of converted speech. We adopt the con-volutional and recurrent neural network models to build a mean opinion score (MOS) predictor, termed as MOSNet. The proposed models are tested on large-scale listening test results of the Voice Conversion Challenge (VCC) 2018. Experimental results show that the predicted scores of the proposed MOSNet are highly correlated with human MOS ratings at the system level while being fairly correlated with human MOS ratings at the utterance level. Meanwhile, we have modified MOSNet to predict the similarity scores, and the preliminary results show that the predicted scores are also fairly correlated with human ratings. These results confirm that the proposed models could be used as a computational evaluator to measure the MOS of VC systems to reduce the need for expensive human rating.},
	urldate = {2019-05-01},
	author = {Lo, Chen-Chou and Fu, Szu-Wei and Huang, Wen-Chin and Wang, Xin and Yamagishi, Junichi and Tsao, Yu and Wang, Hsin-Min},
	note = {arXiv: 1904.08352v2},
	keywords = {MOS, Index Terms: speech naturalness assessment, non-intrusive, speech quality assessment, voice conversion},
	file = {PDF:/Users/rosscutler/Zotero/storage/4XEMTZFH/Lo et al. - Unknown - MOSNet Deep Learning-based Objective Assessment for Voice Conversion.pdf:},
}

@article{ooster_prediction_nodate,
	title = {Prediction of {Perceived} {Speech} {Quality} {Using} {Deep} {Machine} {Listening}},
	url = {http://www.bitsandpieces.co.uk/},
	doi = {10.21437/Interspeech.2018-1374},
	abstract = {Subjective ratings of speech quality (SQ) are essential for evaluating algorithms for speech transmission and enhancement. In this paper we explore a non-intrusive model for SQ prediction based on the output of a deep neural net (DNN) from a regular automatic speech recognizer. The degradation of phoneme probabilities obtained from the net is quantified with the mean temporal distance proposed earlier for multi-stream ASR. The SQ predicted with this method is compared with average subject ratings from the TCD-VoIP speech quality database that covers several effects of SQ degradation that can occur in VoIP applications such as clipping, packet loss, echo effects, background noise, and competing speakers. Our approach is tailored to speech and therefore not applicable when quality is degraded by a competing speaker, which is reflected by an insignificant correlation between model output and subjective SQ. In all other conditions mentioned above, the model reaches an average correlation of r = 0.87, which is higher than the correlation achieved with the baseline ITU-T P.563 (r = 0.71) and the American National Standard ANIQUE+ (r = 0.75). Since the most robust ASR system is not necessarily the best model to predict SQ, we investigate the effect of the amount of training data on quality prediction.},
	urldate = {2019-05-16},
	author = {Ooster, Jasper and Huber, Rainer and Meyer, Bernd T},
	keywords = {deep learning, Index Terms: single-ended speech-quality prediction, mean temporal distance},
	file = {PDF:/Users/rosscutler/Zotero/storage/X4L6EE9K/Ooster, Huber, Meyer - Unknown - Prediction of Perceived Speech Quality Using Deep Machine Listening.pdf:application/pdf},
}

@article{sloan_objective_2017,
	title = {Objective {Assessment} of {Perceptual} {Audio} {Quality} {Using} {ViSQOLAudio}},
	volume = {63},
	issn = {00189316},
	doi = {10.1109/TBC.2017.2704421},
	abstract = {Digital audio broadcasting services transmit substantial amounts of data that is encoded to minimize bandwidth whilst maximizing user quality of experience. Many large service providers continually alter codecs to improve the encoding process. Performing subjective tests to validate each codec alteration would be impractical, necessitating the use of objective perceptual audio quality models. This paper evaluates the quality scores from ViSQOLAudio, an objective perceptual audio quality model, against the quality scores of PEAQ, POLQA, and PEMO-Q on three datasets containing fullband audio encoded with a variety of codecs and bitrates. The results show that ViSQOLAudio was more accurate than all other models on two of the datasets and performed well on the third, demonstrating the utility of ViSQOLAudio for predicting the perceptual audio quality for encoded music.},
	number = {4},
	journal = {IEEE Transactions on Broadcasting},
	author = {Sloan, Colm and Harte, Naomi and Kelly, Damien and Kokaram, Anil C. and Hines, Andrew},
	month = dec,
	year = {2017},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {POLQA, ViSQOL, objective audio quality assessment, PEAQ, PEMO-Q, Perceived audio quality, subjective audio quality assessment, ViSQOLAudio},
	pages = {693--705},
	file = {PDF:/Users/rosscutler/Zotero/storage/RZL8R7CK/Sloan et al. - 2017 - Objective Assessment of Perceptual Audio Quality Using ViSQOLAudio.pdf:application/pdf;PDF:/Users/rosscutler/Zotero/storage/P8WXQIPE/Sloan et al. - 2017 - Objective Assessment of Perceptual Audio Quality Using ViSQOLAudio.pdf:application/pdf},
}

@techreport{fu_metricgan:_nodate,
	title = {{MetricGAN}: {Generative} {Adversarial} {Networks} based {Black}-box {Metric} {Scores} {Optimization} for {Speech} {Enhancement}},
	abstract = {Adversarial loss in a conditional generative ad-versarial network (GAN) is not designed to directly optimize evaluation metrics of a target task, and thus, may not always guide the generator in a GAN to generate data with improved metric scores. To overcome this issue, we propose a novel MetricGAN approach with an aim to optimize the generator with respect to one or multiple evaluation metrics. Moreover, based on Metric-GAN, the metric scores of the generated data can also be arbitrarily specified by users. We tested the proposed MetricGAN on a speech enhancement task, which is particularly suitable to verify the proposed approach because there are multiple metrics measuring different aspects of speech signals. Moreover, these metrics are generally complex and could not be fully optimized by L p or conventional adversarial losses.},
	urldate = {2019-06-12},
	author = {Fu, Szu-Wei and Liao, Chien-Feng and Tsao, Yu and Lin, Shou-De},
	file = {PDF:/Users/rosscutler/Zotero/storage/CMMEJKF5/Fu et al. - Unknown - MetricGAN Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement.pdf:},
}

@inproceedings{ooster_improving_2019,
	title = {Improving {Deep} {Models} of {Speech} {Quality} {Prediction} through {Voice} {Activity} {Detection} and {Entropy}-based {Measures}},
	doi = {10.1109/icassp.2019.8682754},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	author = {Ooster, Jasper and Meyer, Bernd T.},
	month = apr,
	year = {2019},
	pages = {636--640},
	file = {PDF:/Users/rosscutler/Zotero/storage/B2EZSA34/Ooster, Meyer - 2019 - Improving Deep Models of Speech Quality Prediction through Voice Activity Detection and Entropy-based Measures.pdf:application/pdf},
}

@techreport{fu_quality-net:_nodate,
	title = {Quality-{Net}: {An} {End}-to-{End} {Non}-intrusive {Speech} {Quality} {Assessment} {Model} based on {BLSTM}},
	abstract = {Nowadays, most of the objective speech quality assessment tools (e.g., perceptual evaluation of speech quality (PESQ)) are based on the comparison of the degraded/processed speech with its clean counterpart. The need of a "golden" reference considerably restricts the practicality of such assessment tools in real-world scenarios since the clean reference usually cannot be accessed. On the other hand, human beings can readily evaluate the speech quality without any reference (e.g., mean opinion score (MOS) tests), implying the existence of an objective and non-intrusive (no clean reference needed) quality assessment mechanism. In this study, we propose a novel end-to-end, non-intrusive speech quality evaluation model, termed Quality-Net, based on bidirectional long short-term memory. The evaluation of utterance-level quality in Quality-Net is based on the frame-level assessment. Frame constraints and sensible initializations of forget gate biases are applied to learn meaningful frame-level quality assessment from the utterance-level quality label. Experimental results show that Quality-Net can yield high correlation to PESQ (0.9 for the noisy speech and 0.84 for the speech processed by speech enhancement). We believe that Quality-Net has potential to be used in a wide variety of applications of speech signal processing. Index Terms: speech quality assessment, PESQ, BLSTM, end-to-end model, non-intrusive quality assessment.},
	urldate = {2019-05-16},
	author = {Fu, Szu-Wei and Tsao, Yu and Hwang, Hsin-Te and Wang, Hsin-Min},
	file = {PDF:/Users/rosscutler/Zotero/storage/U7MIEB98/Fu et al. - Unknown - Quality-Net An End-to-End Non-intrusive Speech Quality Assessment Model based on BLSTM.pdf:},
}

@techreport{van_den_oord_wavenet:_nodate,
	title = {{WAVENET}: {A} {GENERATIVE} {MODEL} {FOR} {RAW} {AUDIO}},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predic-tive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Chinese. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2019-05-10},
	author = {Van Den Oord, Aäron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	file = {PDF:/Users/rosscutler/Zotero/storage/8JPZFRUU/Van Den Oord et al. - Unknown - WAVENET A GENERATIVE MODEL FOR RAW AUDIO.pdf:},
}

@techreport{jay_deep_nodate,
	title = {A {Deep} {Reinforcement} {Learning} {Perspective} on {Internet} {Congestion} {Control}},
	url = {https://github.com/PCCproject/PCC-RL},
	abstract = {We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources' data-transmission rates to efficiently utilize network capacity, and is the subject of extensive attention in light of the advent of Internet services such as live video, virtual reality , Internet-of-Things, and more. We show that casting congestion control as RL enables training deep network policies that capture intricate patterns in data traffic and network conditions, and leverage this to outperform the state-of-the-art. We also highlight significant challenges facing real-world adoption of RL-based congestion control , including fairness, safety, and generalization, which are not trivial to address within conventional RL formalism. To facilitate further research and reproducibility of our results, we present a test suite for RL-guided congestion control based on the OpenAI Gym interface.},
	urldate = {2019-06-25},
	author = {Jay, Nathan and Rotman, Noga H and Godfrey, P Brighten and Schapira, Michael and Tamar, Aviv},
	note = {arXiv: 1810.03259v3},
	file = {PDF:/Users/rosscutler/Zotero/storage/IBVY47W9/Jay et al. - Unknown - A Deep Reinforcement Learning Perspective on Internet Congestion Control.pdf:application/pdf},
}

@incollection{mei_realtime_2019,
	title = {Realtime {Mobile} {Bandwidth} {Prediction} {Using} {LSTM} {Neural} {Network}},
	url = {http://link.springer.com/10.1007/978-3-030-15986-3_3},
	urldate = {2019-06-10},
	publisher = {Springer, Cham},
	author = {Mei, Lifan and Hu, Runchen and Cao, Houwei and Liu, Yong and Han, Zifa and Li, Feng and Li, Jin},
	month = mar,
	year = {2019},
	doi = {10.1007/978-3-030-15986-3_3},
	pages = {34--47},
}

@inproceedings{kan_deep_2019,
	title = {Deep {Reinforcement} {Learning}-based {Rate} {Adaptation} for {Adaptive} 360-{Degree} {Video} {Streaming}},
	doi = {10.1109/icassp.2019.8683779},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	author = {Kan, Nuowen and Zou, Junni and Tang, Kexin and Li, Chenglin and Liu, Ning and Xiong, Hongkai},
	month = apr,
	year = {2019},
	pages = {4030--4034},
	file = {PDF:/Users/rosscutler/Zotero/storage/8W4MFA9X/Kan et al. - 2019 - Deep Reinforcement Learning-based Rate Adaptation for Adaptive 360-Degree Video Streaming.pdf:application/pdf},
}

@techreport{yan_continual_nodate,
	title = {Continual learning improves {Internet} video streaming},
	url = {https://puffer.stanford.edu},
	abstract = {We describe Fugu, a continual learning algorithm for bitrate selection in streaming video. Each day, Fugu retrains a neural network from its experience in deployment over the prior week. The neural network predicts how long it would take to transfer each available version of the upcoming video chunks, given recent history and internal TCP statistics. We evaluate Fugu with Puffer 1 , a public website we built that streams live TV using Fugu and existing algorithms. Over a nine-day period in January 2019, Puffer streamed 8,131 hours of video to 3,719 unique users. Compared with buffer-based control, MPC, RobustMPC, and Pensieve, Fugu performs better on several metrics: it stalls 5-13× less and has better and more stable video quality, and users who were randomly assigned to Fugu streamed for longer on average before quitting or reloading. We find that TCP statistics can aid bitrate selection, that congestion-control and bitrate-selection algorithms are best selected jointly, that probabilistic transfer-time estimates that consider chunk size outperform point estimates of throughput, and that both stalls and video quality have an influence on how long users choose to keep a video stream playing. Fugu's results suggest that continual learning of network algorithms in situ is a promising area of research. To support further investigation, we plan to operate Puffer for several years and will open it to researchers for evaluating new congestion-control and bitrate-selection approaches.},
	urldate = {2019-06-10},
	author = {Yan, Francis Y and Ayers, Hudson and Zhu, Chenzhi and Fouladi, Sadjad and Hong, James and Zhang, Keyi and Levis, Philip and Winstein, Keith},
	note = {arXiv: 1906.01113v1},
	file = {PDF:/Users/rosscutler/Zotero/storage/HMIULNWV/Yan et al. - Unknown - Continual learning improves Internet video streaming.pdf:application/pdf},
}

@article{carlucci_analysis_nodate,
	title = {Analysis and {Design} of the {Google} {Congestion} {Control} for {Web} {Real}-time {Communication} ({WebRTC})},
	url = {http://dx.doi.org/10.1145/2910017.2910605},
	doi = {10.1145/2910017.2910605},
	abstract = {Video conferencing applications require low latency and high bandwidth. Standard TCP is not suitable for video conferencing since its reliability and in order delivery mechanisms induce large latency. Recently the idea of using the delay gradient to infer congestion is appearing again and is gaining momentum. In this paper we present an algorithm that is based on estimating through a Kalman filter the end-to-end one way delay variation which is experienced by packets traveling from a sender to a destination. This estimate is compared to an adaptive threshold to dynamically throttle the sending rate. The control algorithm has been implemented over the RTP/RTCP protocol and is currently used in Google Hangouts and in the Chrome WebRTC stack. Experiments have been carried out to evaluate the algorithm performance in the case of variable link capacity, presence of heterogeneous or homogeneous concurrent traffic, and backward path traffic.},
	urldate = {2019-04-24},
	author = {Carlucci, Gaetano and De Cicco, Luca and Holmer Google, Stefan and Mascolo, Saverio},
	note = {ISBN: 9781450342971},
	keywords = {•Information systems → Web conferencing, CCS Concepts •Networks → Network protocol design, Keywords Real-time communication, congestion control, WebRTC},
	file = {PDF:/Users/rosscutler/Zotero/storage/5WIYMFKC/Carlucci et al. - Unknown - Analysis and Design of the Google Congestion Control for Web Real-time Communication (WebRTC).pdf:application/pdf},
}

@techreport{lillicrap_continuous_nodate,
	title = {{CONTINUOUS} {CONTROL} {WITH} {DEEP} {REINFORCEMENT} {LEARNING}},
	url = {https://goo.gl/J4PIAz},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the de-terministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies "end-to-end": directly from raw pixel inputs .},
	urldate = {2019-05-02},
	author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	note = {arXiv: 1509.02971v5},
	file = {PDF:/Users/rosscutler/Zotero/storage/NMR6XPTY/Lillicrap et al. - Unknown - CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING.pdf:application/pdf},
}

@techreport{carlson_sensor_nodate,
	title = {Sensor {Transfer}: {Learning} {Optimal} {Sensor} {Effect} {Image} {Augmentation} for {Sim}-to-{Real} {Domain} {Adaptation}},
	abstract = {Performance on benchmark datasets has drastically improved with advances in deep learning. Still, cross-dataset generalization performance remains relatively low due to the domain shift that can occur between two different datasets. This domain shift is especially exaggerated between synthetic and real datasets. Significant research has been done to reduce this gap, specifically via modeling variation in the spatial layout of a scene, such as occlusions, and scene environmental factors, such as time of day and weather effects. However, few works have addressed modeling the variation in the sensor domain as a means of reducing the synthetic to real domain gap. The camera or sensor used to capture a dataset introduces artifacts into the image data that are unique to the sensor model, suggesting that sensor effects may also contribute to domain shift. To address this, we propose a learned augmentation network composed of physically-based augmentation functions. Our proposed augmentation pipeline transfers specific effects of the sensor model-chromatic aberration, blur, exposure, noise, and color temperature-from a real dataset to a synthetic dataset. We provide experiments that demonstrate that augmenting synthetic training datasets with the proposed learned augmentation framework reduces the domain gap between synthetic and real domains for object detection in urban driving scenes.},
	urldate = {2019-04-07},
	author = {Carlson, Alexandra and Skinner, Katherine A and Vasudevan, Ram and Johnson-Roberson, Matthew},
	note = {arXiv: 1809.06256v2},
	file = {PDF:/Users/rosscutler/Zotero/storage/V3DV5ACN/Carlson et al. - Unknown - Sensor Transfer Learning Optimal Sensor Effect Image Augmentation for Sim-to-Real Domain Adaptation.pdf:},
}

@article{ponomarenko_image_2015,
	title = {Image database {TID2013}: {Peculiarities}, results and perspectives},
	volume = {30},
	issn = {09235965},
	doi = {10.1016/j.image.2014.10.009},
	abstract = {This paper describes a recently created image database, TID2013, intended for evaluation of full-reference visual quality assessment metrics. With respect to TID2008, the new database contains a larger number (3000) of test images obtained from 25 reference images, 24 types of distortions for each reference image, and 5 levels for each type of distortion. Motivations for introducing 7 new types of distortions and one additional level of distortions are given; examples of distorted images are presented. Mean opinion scores (MOS) for the new database have been collected by performing 985 subjective experiments with volunteers (observers) from five countries (Finland, France, Italy, Ukraine, and USA). The availability of MOS allows the use of the designed database as a fundamental tool for assessing the effectiveness of visual quality. Furthermore, existing visual quality metrics have been tested with the proposed database and the collected results have been analyzed using rank order correlation coefficients between MOS and considered metrics. These correlation indices have been obtained both considering the full set of distorted images and specific image subsets, for highlighting advantages and drawbacks of existing, state of the art, quality metrics. Approaches to thorough performance analysis for a given metric are presented to detect practical situations or distortion types for which this metric is not adequate enough to human perception. The created image database and the collected MOS values are freely available for downloading and utilization for scientific purposes.},
	journal = {Signal Processing: Image Communication},
	author = {Ponomarenko, Nikolay and Jin, Lina and Ieremeiev, Oleg and Lukin, Vladimir and Egiazarian, Karen and Astola, Jaakko and Vozel, Benoit and Chehdi, Kacem and Carli, Marco and Battisti, Federica and Jay Kuo, C. C.},
	month = jan,
	year = {2015},
	note = {Publisher: Elsevier},
	keywords = {Image denoising, Image lossy compression, Image visual quality metrics},
	pages = {57--77},
	file = {PDF:/Users/rosscutler/Zotero/storage/Y8AQGLMH/Ponomarenko et al. - 2015 - Image database TID2013 Peculiarities, results and perspectives.pdf:application/pdf},
}

@article{virtanen_cid2013:_2015,
	title = {{CID2013}: {A} database for evaluating no-reference image quality assessment algorithms},
	volume = {24},
	issn = {10577149},
	doi = {10.1109/TIP.2014.2378061},
	abstract = {© 2014 IEEE. This paper presents a new database, CID2013, to address the issue of using no-reference (NR) image quality assessment algorithms on images with multiple distortions. Current NR algorithms struggle to handle images with many concurrent distortion types, such as real photographic images captured by different digital cameras. The database consists of six image sets; on average, 30 subjects have evaluated 12-14 devices depicting eight different scenes for a total of 79 different cameras, 480 images, and 188 subjects (67\% female). The subjective evaluation method was a hybrid absolute category rating-pair comparison developed for the study and presented in this paper. This method utilizes a slideshow of all images within a scene to allow the test images to work as references to each other. In addition to mean opinion score value, the images are also rated using sharpness, graininess, lightness, and color saturation scales. The CID2013 database contains images used in the experiments with the full subjective data plus extensive background information from the subjects. The database is made freely available for the research community.},
	number = {1},
	journal = {IEEE Transactions on Image Processing},
	author = {Virtanen, Toni and Nuutinen, Mikko and Vaahteranoksa, Mikko and Oittinen, Pirkko and Häkkinen, Jukka},
	month = jan,
	year = {2015},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {no-reference image quality assessment algorithms, subjective image quality evaluation, test image databases},
	pages = {390--402},
	file = {PDF:/Users/rosscutler/Zotero/storage/YG2KHTSD/Virtanen et al. - 2015 - CID2013 A database for evaluating no-reference image quality assessment algorithms.pdf:application/pdf},
}

@article{seshadrinathan_study_2010,
	title = {Study of {Subjective} and {Objective} {Quality} {Assessment} of {Video}},
	volume = {19},
	url = {http://ieeexplore.ieee.org.},
	doi = {10.1109/TIP.2010.2042111},
	abstract = {We present the results of a recent large-scale subjective study of video quality on a collection of videos distorted by a variety of application-relevant processes. Methods to assess the visual quality of digital videos as perceived by human observers are becoming increasingly important, due to the large number of applications that target humans as the end users of video. Owing to the many approaches to video quality assessment (VQA) that are being developed, there is a need for a diverse independent public database of distorted videos and subjective scores that is freely available. The resulting Laboratory for Image and Video Engineering (LIVE) Video Quality Database contains 150 distorted videos (obtained from ten uncompressed reference videos of natural scenes) that were created using four different commonly encountered distortion types. Each video was assessed by 38 human subjects, and the difference mean opinion scores (DMOS) were recorded. We also evaluated the performance of several state-of-the-art, publicly available full-reference VQA algorithms on the new database. A statistical evaluation of the relative performance of these algorithms is also presented. The database has a dedicated web presence that will be maintained as long as it remains relevant and the data is available online. Index Terms-Full reference, human visual system, LIVE video quality database, perceptual quality assessment, video quality, visual perception.},
	number = {6},
	urldate = {2019-05-20},
	journal = {IEEE TRANSACTIONS ON IMAGE PROCESSING},
	author = {Seshadrinathan, Kalpana and Soundararajan, Rajiv and Bovik, Alan Conrad and Cormack, Lawrence K},
	year = {2010},
	file = {PDF:/Users/rosscutler/Zotero/storage/GK45NQYG/Seshadrinathan et al. - 2010 - Study of Subjective and Objective Quality Assessment of Video.pdf:application/pdf},
}

@article{krishna_moorthy_video_2012,
	title = {Video {Quality} {Assessment} on {Mobile} {Devices}: {Subjective}, {Behavioral} and {Objective} {Studies}},
	volume = {6},
	url = {http://ieeexplore.ieee.org.},
	doi = {10.1109/JSTSP.2012.2212417},
	abstract = {We introduce a new video quality database that models video distortions in heavily-trafficked wireless networks and that contains measurements of human subjective impressions of the quality of videos. The new LIVE Mobile Video Quality Assessment (VQA) database consists of 200 distorted videos created from 10 RAW HD reference videos, obtained using a RED ONE digital cinematographic camera. While the LIVE Mobile VQA database includes distortions that have been previously studied such as compression and wireless packet-loss, it also incorporates dynamically varying distortions that change as a function of time, such as frame-freezes and temporally varying compression rates. In this article, we describe the construction of the database and detail the human study that was performed on mobile phones and tablets in order to gauge the human perception of quality on mobile devices. The subjective study portion of the database includes both the differential mean opinion scores (DMOS) computed from the ratings that the subjects provided at the end of each video clip, as well as the continuous temporal scores that the subjects recorded as they viewed the video. The study involved over 50 subjects and resulted in 5,300 summary subjective scores and time-sampled subjective traces of quality. In the behavioral portion of the article we analyze human opinion using statistical techniques, and also study a variety of models of temporal pooling that may reflect strategies that the subjects used to make the final decision on video quality. Further, we compare the quality ratings obtained from the tablet and the mobile phone studies in order to study the impact of these different display modes on quality. We also evaluate several objective image and video quality assessment (IQA/VQA) algorithms with regards to their efficacy in predicting visual quality. A detailed correlation analysis and statistical hypothesis testing is carried out. Our general conclusion is that existing VQA algorithms are not well-equipped to handle distortions that vary over time. The LIVE Mobile VQA database, along with the subject DMOS and the continuous temporal scores is being made available to researchers in the field of VQA at no cost in order to further research in the area of video quality assessment. Index Terms-Mobile video quality, objective algorithm evaluations , subjective quality, video quality assessment, video quality database.},
	number = {6},
	urldate = {2019-05-20},
	journal = {IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING},
	author = {Krishna Moorthy, Anush and Kwon Choi, Lark and Conrad Bovik, Alan and de Veciana, Gustavo and Choi, L K and Bovik, A C},
	year = {2012},
	file = {PDF:/Users/rosscutler/Zotero/storage/AVR7M592/Krishna Moorthy et al. - 2012 - Video Quality Assessment on Mobile Devices Subjective, Behavioral and Objective Studies.pdf:application/pdf},
}

@article{vu_vis3:_2014,
	title = {{ViS3}: an algorithm for video quality assessment via analysis of spatial and spatiotemporal slices},
	volume = {23},
	issn = {1017-9909},
	url = {http://electronicimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JEI.23.1.013016},
	doi = {10.1117/1.JEI.23.1.013016},
	abstract = {Algorithms for video quality assessment (VQA) aim to estimate the qualities of videos in a manner that agrees with human judgments of quality. Modern VQA algorithms often estimate video quality by comparing localized space-time regions or groups of frames from the reference and distorted videos, using comparisons based on visual features, statistics, and/or perceptual models. We present a VQA algorithm that estimates quality via separate estimates of perceived degradation due to (1) spatial distortion and (2) joint spatial and temporal distortion. The first stage of the algorithm estimates perceived quality degradation due to spatial distortion; this stage operates by adaptively applying to groups of spatial video frames the two strategies from the most apparent distortion algorithm with an extension to account for temporal masking. The second stage of the algorithm estimates perceived quality degradation due to joint spatial and temporal distortion; this stage operates by measuring the dissimilarity between the reference and distorted videos represented in terms of two-dimensional spatiotemporal slices. Finally, the estimates obtained from the two stages are combined to yield an overall estimate of perceived quality degradation. Testing on various video-quality databases demonstrates that our algorithm performs well in predicting video quality and is competitive with current state-of-the-art VQA algorithms.},
	number = {1},
	journal = {Journal of Electronic Imaging},
	author = {Vu, Phong V. and Chandler, Damon M.},
	month = feb,
	year = {2014},
	pages = {013016},
	file = {PDF:/Users/rosscutler/Zotero/storage/6V9HQEAQ/Vu, Chandler - 2014 - ViS3 an algorithm for video quality assessment via analysis of spatial and spatiotemporal slices.pdf:application/pdf},
}

@article{nuutinen_cvd2014_2016,
	title = {{CVD2014} - {A} {Database} for {Evaluating} {No}-{Reference} {Video} {Quality} {Assessment} {Algorithms}},
	volume = {25},
	issn = {10577149},
	doi = {10.1109/TIP.2016.2562513},
	abstract = {© 1992-2012 IEEE. In this paper, we present a new video database: CVD2014 - Camera Video Database. In contrast to previous video databases, this database uses real cameras rather than introducing distortions via post-processing, which results in a complex distortion space in regard to the video acquisition process. CVD2014 contains a total of 234 videos that are recorded using 78 different cameras. Moreover, this database contains the observer-specific quality evaluation scores rather than only providing mean opinion scores. We have also collected open-ended quality descriptions that are provided by the observers. These descriptions were used to define the quality d imensions for the videos in CVD2014. The dimensions included sharpness, graininess, color balance, darkness, and jerkiness. At the end of this paper, a performance study of image and video quality algorithms for predicting the subjective video quality is reported. For this performance study, we proposed a new performance measure that accounts for observer variance. The performance study revealed that there is room for improvement regarding the video quality assessment algorithms. The CVD2014 video database has been made publicly available for the research community. All video sequences and corresponding subjective ratings can be obtained from the CVD2014 project page (http://www.helsinki.fi/psychology/groups/visualcognition/).},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Nuutinen, Mikko and Virtanen, Toni and Vaahteranoksa, Mikko and Vuori, Tero and Oittinen, Pirkko and Hakkinen, Jukka},
	month = jul,
	year = {2016},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {quality attribute, subjective evaluation, Video camera, video quality algorithm},
	pages = {3073--3086},
	file = {PDF:/Users/rosscutler/Zotero/storage/UGZ47E86/Nuutinen et al. - 2016 - CVD2014 - A Database for Evaluating No-Reference Video Quality Assessment Algorithms.pdf:application/pdf},
}

@techreport{ghadiyaram_ieee_nodate,
	title = {{IEEE} {TRANSACTIONS} {ON} {IMAGE} {PROCESSING} 1 {Massive} {Online} {Crowdsourced} {Study} of {Subjective} and {Objective} {Picture} {Quality}},
	url = {https://www.cs.utexas.},
	abstract = {Most publicly available image quality databases have been created under highly controlled conditions by introducing graded simulated distortions onto high-quality photographs. However, images captured using typical real-world mobile camera devices are usually afflicted by complex mixtures of multiple distortions, which are not necessarily well-modeled by the synthetic distortions found in existing databases. The originators of existing legacy databases usually conducted human psychometric studies to obtain statistically meaningful sets of human opinion scores on images in a stringently controlled visual environment, resulting in small data collections relative to other kinds of image analysis databases. Towards overcoming these limitations, we designed and created a new database that we call the LIVE In the Wild Image Quality Challenge Database, which contains widely diverse authentic image distortions on a large number of images captured using a representative variety of modern mobile devices. We also designed and implemented a new online crowdsourcing system, which we have used to conduct a very large-scale, multi-month image quality assessment subjective study. Our database consists of over 350,000 opinion scores on 1,162 images evaluated by over 8100 unique human observers. Despite the lack of control over the experimental environments of the numerous study participants, we demonstrate excellent internal consistency of the subjective dataset. We also evaluate several top-performing blind IQA algorithms on it and present insights on how mixtures of distortions challenge both end users as well as automatic perceptual quality prediction models. The new database is available for public use at https://www.cs.utexas. edu/ ∼ deepti/ChallengeDB.zip. Index Terms-Perceptual image quality, subjective image quality assessment, crowdsourcing, authentic distortions.},
	urldate = {2019-05-20},
	author = {Ghadiyaram, Deepti and Bovik, Alan C},
	note = {arXiv: 1511.02919v1},
	file = {PDF:/Users/rosscutler/Zotero/storage/H2J9SVNI/Ghadiyaram, Bovik - Unknown - IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Massive Online Crowdsourced Study of Subjective and Objective Pict.pdf:application/pdf},
}

@article{ghadiyaram_-capture_2018,
	title = {In-{Capture} {Mobile} {Video} {Distortions}: {A} {Study} of {Subjective} {Behavior} and {Objective} {Algorithms}},
	volume = {28},
	issn = {10518215},
	doi = {10.1109/TCSVT.2017.2707479},
	abstract = {Digital videos often contain visual distortions that are introduced by the camera hardware or processing software during the capture process. These distortions often detract from a viewer’s quality of experience. Understanding how human observers perceive the visual quality of digital videos is of great importance to camera designers. Thus, the development of automatic objective methods that accurately quantify the impact of visual distortions on perception has greatly accelerated. Video quality algorithm design and verification requires realistic databases of distorted videos and human judgments of them. However, most current publicly available video quality databases have been created under highly controlled conditions using graded, simulated, post-capture distortions (such as jitter and compression artifacts) on high-quality videos. The commercial plethora of hand-held mobile video capture devices produces videos often afflicted by a variety of complex distortions generated during the capturing process. These in-capture distortions are not well-modeled by the synthetic, post-capture distortions found in existing VQA databases. Towards overcoming this limitation, we designed and created a new database that we call the LIVE-Qualcomm Mobile In-Capture Video Quality Database, comprising a total of 208 videos, which model six common incapture distortions. We also conducted a subjective quality assessment study using this database, where each video was assessed by 39 unique subjects. Furthermore, we evaluated several top performing No-Reference IQA and VQA algorithms on the new database and studied how real-world in-capture distortions challenge both human viewers as well as automatic perceptual quality prediction models. The new database is freely available at: http: //live.ece.utexas.edu/research/incaptureDatabase/index.html.},
	number = {9},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Ghadiyaram, Deepti and Pan, Janice and Bovik, Alan C. and Moorthy, Anush Krishna and Panda, Prasanjit and Yang, Kai Chieh},
	month = sep,
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {in-capture distortions, Mobile videos, smart-phone cameras, subjective quality assessment},
	pages = {2061--2077},
	file = {PDF:/Users/rosscutler/Zotero/storage/Y42BTCM7/Ghadiyaram et al. - 2018 - In-Capture Mobile Video Distortions A Study of Subjective Behavior and Objective Algorithms.pdf:application/pdf},
}

@techreport{sinno_large-scale_nodate,
	title = {Large-{Scale} {Study} of {Perceptual} {Video} {Quality}},
	url = {http://live.ece.utexas.edu/research/LIVEVQC/index.html},
	abstract = {The great variations of videographic skills, camera designs, compression and processing protocols, communication and bandwidth environments, and displays lead to an enormous variety of video impairments. Current no-reference (NR) video quality models are unable to handle this diversity of distortions. This is true in part because available video quality assessment databases contain very limited content, fixed resolutions, were captured using a small number of camera devices by a few videographers and have been subjected to a modest number of distortions. As such, these databases fail to adequately represent real world videos, which contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, complex and often commingled distortions that are difficult or impossible to simulate. As a result, NR video quality predictors tested on real-world video data often perform poorly. Towards advancing NR video quality prediction, we have constructed a large-scale video quality assessment database containing 585 videos of unique content, captured by a large number of users, with wide ranges of levels of complex, authentic distortions. We collected a large number of subjective video quality scores via crowdsourcing. A total of 4776 unique participants took part in the study, yielding more than 205000 opinion scores, resulting in an average of 240 recorded human opinions per video. We demonstrate the value of the new resource, which we call the LIVE Video Quality Challenge Database (LIVE-VQC for short), by conducting a comparison of leading NR video quality predictors on it. This study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores. The database is available for download on this link: http://live.ece.utexas.edu/research/LIVEVQC/index.html .},
	urldate = {2019-05-20},
	author = {Sinno, Zeina and Bovik, Alan Conrad},
	note = {arXiv: 1803.01761v2},
	keywords = {Multimedia, Crowd-sourcing, Database, Index Terms-Video Quality Assessment},
	file = {PDF:/Users/rosscutler/Zotero/storage/M4VVQWXE/Sinno, Bovik - Unknown - Large-Scale Study of Perceptual Video Quality.pdf:application/pdf},
}

@article{hines_perceived_nodate,
	title = {Perceived {Audio} {Quality} for {Streaming} {Stereo} {Music}},
	url = {http://dx.doi.org/10.1145/2647868.2655025.},
	doi = {10.1145/2647868.2655025},
	abstract = {Users of audiovisual streaming services expect an ever increasing quality of experience. Channel bandwidth remains a bottleneck commonly addressed with lossy compression schemes for both the video and audio streams. Anecdotal evidence suggests a strongly perceived link between bit rate and quality. This paper presents three audio quality listening experiments using the ITU MUSHRA methodology to assess a number of audio codecs typically used by streaming services. They were assessed for a range of bit rates using three presentation modes: consumer and studio quality headphones and loudspeakers. Our results indicate that with consumer quality headphones, listeners were not differentiating between codecs with bit rates greater than 48 kb/s (p{\textgreater}=0.228). For studio quality headphones and loudspeakers aac-lc at 128 kb/s and higher was differentiated over other codecs (p{\textless}=0.001). The results provide insights into quality of experience that will guide future development of objective audio quality metrics.},
	urldate = {2019-05-24},
	author = {Hines, Andrew and Gillen, Eoin and Kelly, Damien and Skoglund, Jan and Kokaram, Anil and Harte, Naomi},
	note = {ISBN: 9781450330633},
	keywords = {Audio Codec, H55 [Information Interfaces and Presentation]: Sound and music Computing Keywords Audio Quality, MUSHRA, YouTube},
	file = {PDF:/Users/rosscutler/Zotero/storage/PPRUGACX/Hines et al. - Unknown - Perceived Audio Quality for Streaming Stereo Music.pdf:application/pdf},
}

@book{hosu_konstanz_nodate,
	title = {The {Konstanz} {Natural} {Video} {Database} ({KoNViD}-1k)},
	isbn = {978-1-5386-4024-1},
	abstract = {Subjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. Currently, all existing VQA databases include only a small number of video sequences with artificial distortions. The development and evaluation of objective quality assessment methods would benefit from having larger datasets of real-world video sequences with corresponding subjective mean opinion scores (MOS), in particular for deep learning purposes. In addition, the training and validation of any VQA method intended to be 'general purpose' requires a large dataset of video sequences that are representative of the whole spectrum of available video content and all types of distortions. We report our work on KoNViD-1k, a subjectively annotated VQA database consisting of 1,200 public-domain video sequences, fairly sampled from a large public video dataset, YFCC100m. We present the challenges and choices we have made in creating such a database aimed at 'in the wild' authentic distortions, depicting a wide variety of content.},
	urldate = {2019-05-11},
	author = {Hosu, Vlad and Hahn, Franz and Jenadeleh, Mohsen and Lin, Hanhe and Men, Hui and Szirányi, Tamás and Li, Shujun and Saupe, Dietmar},
	keywords = {authentic video, crowdsourcing, fair sampling, Video database, video quality as-sessment},
	file = {PDF:/Users/rosscutler/Zotero/storage/5IBGHTBT/Hosu et al. - Unknown - The Konstanz Natural Video Database (KoNViD-1k).pdf:},
}

@article{wang_videoset:_2017,
	title = {{VideoSet}: {A} large-scale compressed video quality dataset based on {JND} measurement},
	volume = {46},
	issn = {10473203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047320317300950},
	doi = {10.1016/j.jvcir.2017.04.009},
	urldate = {2019-05-19},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Wang, Haiqiang and Katsavounidis, Ioannis and Zhou, Jiantong and Park, Jeonghoon and Lei, Shawmin and Zhou, Xin and Pun, Man-On and Jin, Xin and Wang, Ronggang and Wang, Xu and Zhang, Yun and Huang, Jiwu and Kwong, Sam and Kuo, C.-C. Jay},
	month = jul,
	year = {2017},
	pages = {292--302},
	file = {PDF:/Users/rosscutler/Zotero/storage/C38DZHE5/Wang et al. - 2017 - VideoSet A large-scale compressed video quality dataset based on JND measurement.pdf:application/pdf},
}

@techreport{wang_mcl-jcv:_nodate,
	title = {{MCL}-{JCV}: {A} {JND}-{BASED} {H}.264/{AVC} {VIDEO} {QUALITY} {ASSESSMENT} {DATASET}},
	abstract = {A compressed video quality assessment dataset based on the just noticeable difference (JND) model, called MCL-JCV, is recently constructed and released. In this work, we explain its design objectives, selected video content and subject test procedures. Then, we conduct statistical analysis on collected JND data. We compute the difference between every two adjacent JND points and propose an outlier detection algorithm to remove unreliable data. We also show that each JND difference group can be well approximated by a normal distribution so that we can adopt the Gaussian mixture model (GMM) to characterize the distribution of multiple JND points. Finally, it is demonstrated by experimental results that the proposed JND analysis performed in the difference domain, called the D-method, achieves a lower BIC (Bayesian information criteria) value than the previously proposed G-method.},
	urldate = {2019-05-19},
	author = {Wang, Haiqiang and Gan, Weihao and Hu, Sudeng and Lin, Joe Yuchieh and Jin, Lina and Song, Longguang and Wang, Ping and Katsavounidis, Ioannis and Aaron, Anne and Kuo, C.-C Jay},
	keywords = {Index Terms-Video Quality Assessment, Gaussian Mixture Model, Just Noticeable Dif-ference, Outlier Detection},
	file = {PDF:/Users/rosscutler/Zotero/storage/97ZUVND3/Wang et al. - Unknown - MCL-JCV A JND-BASED H.264AVC VIDEO QUALITY ASSESSMENT DATASET.pdf:},
}

@techreport{roth_ava-activespeaker:_nodate,
	title = {{AVA}-{ActiveSpeaker}: {An} {Audio}-{Visual} {Dataset} for {Active} {Speaker} {Detection}},
	abstract = {Active speaker detection is an important component in video analysis algorithms for applications such as speaker diarization, video re-targeting for meetings, speech enhancement , and human-robot interaction. The absence of a large, carefully labeled audiovisual dataset for this task has constrained algorithm evaluations with respect to data diversity, environments, and accuracy. This has made comparisons and improvements difficult. In this paper , we present the AVA Active Speaker detection dataset (AVA-ActiveSpeaker) that will be released publicly to facilitate algorithm development and enable comparisons. The dataset contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio. We also present a new audiovisual approach for active speaker detection , and analyze its performance, demonstrating both its strength and the contributions of the dataset.},
	urldate = {2019-05-19},
	author = {Roth, Joseph and Chaudhuri, Sourish and Klejch, Ondrej and Marvin, Radhika and Gallagher, Andrew and Kaver, Liat and Ramaswamy, Sharadh and Stopczynski, Arkadiusz and Schmid, Cordelia and Xi, Zhonghua and Pantofaru, Caroline},
	note = {arXiv: 1901.01342v1
ISBN: 1901.01342v1},
	file = {PDF:/Users/rosscutler/Zotero/storage/7N5XVH32/Roth et al. - Unknown - AVA-ActiveSpeaker An Audio-Visual Dataset for Active Speaker Detection.pdf:},
}

@article{chandler_most_2010,
	title = {Most apparent distortion: full-reference image quality assessment and the role of strategy},
	volume = {19},
	issn = {1017-9909},
	doi = {10.1117/1.3267105},
	abstract = {The mainstream approach to image quality assessment has centered around accurately modeling the single most relevant strategy employed by the human visual system (HVS) when judging image quality (e.g., detecting visible differences, and extracting image structure/information). In this work, we suggest that a single strategy may not be sufficient; rather, we advocate that the HVS uses multiple strategies to determine image quality. For images containing near-threshold distortions, the image is most apparent, and thus the HVS attempts to look past the image and look for the distortions (a detection-based strategy). For images containing clearly visible distortions, the distortions are most apparent, and thus the HVS attempts to look past the distortion and look for the image’s subject matter (an appearance-based strategy). Here, we present a quality assessment method [most apparent distortion (MAD)], which attempts to explicitly model these two separate strategies. Local luminance and contrast masking are used to estimate detection-based perceived distortion in high-quality images, whereas changes in the local statistics of spatial-frequency components are used to estimate appearance-based perceived distortion in low-quality images. We show that a combination of these two measures can perform well in predicting subjective ratings of image quality.},
	number = {1},
	journal = {Journal of Electronic Imaging},
	author = {Chandler, Damon M.},
	month = jan,
	year = {2010},
	note = {Publisher: SPIE-Intl Soc Optical Eng},
	pages = {011006},
	file = {PDF:/Users/rosscutler/Zotero/storage/RPKHKV5N/Chandler - 2010 - Most apparent distortion full-reference image quality assessment and the role of strategy.pdf:application/pdf},
}

@techreport{hu_subjective_nodate,
	title = {Subjective comparison and evaluation of speech enhancement algorithms},
	url = {http://www.utdallas.edu/},
	abstract = {Making meaningful comparisons between the performance of the various speech enhancement algorithms proposed over the years, has been elusive due to lack of a common speech database, differences in the types of noise used and differences in the testing methodology. To facilitate such comparisons, we report on the development of a noisy speech corpus suitable for evaluation of speech enhancement algorithms. This corpus is subsequently used for the subjective evaluation of 13 speech enhancement methods encompassing four classes of algorithms: spectral subtractive, subspace, statistical-model based and Wiener-type algorithms. The subjective evaluation was performed by Dynastat, Inc. using the ITU-T P.835 methodology designed to evaluate the speech quality along three dimensions: signal distortion, noise distortion and overall quality. This paper reports the results of the subjective tests.},
	urldate = {2019-06-02},
	author = {Hu, Yi and Loizou, Philipos C},
	keywords = {Speech enhancement, ITU-T P835, Noise reduction, Subjective evaluation},
	file = {PDF:/Users/rosscutler/Zotero/storage/PWZ276FQ/Hu, Loizou - Unknown - Subjective comparison and evaluation of speech enhancement algorithms.pdf:application/pdf},
}

@article{demirbilek_inrs_2016,
	title = {{INRS} {Audiovisual} {Quality} {Dataset}},
	url = {http://dx.doi.org/10.1145/2964284.2967204},
	doi = {10.1145/2964284.2967204},
	abstract = {We present the INRS audiovisual quality dataset made of 160 unique configurations for audiovisual content including various media compression and network distortion parameters such as video frame rate, quantization and noise reduction parameters, and packet loss rate. The compression and network distortion parameter range values are selected to match real-time communications use cases. The H.264 video codec in 720p resolution and the AMR-WB audio codec are used for encoding video and audio streams. Thirty observers have rated the overall audiovisual quality on the Absolute Category Rating (ACR) 5-level quality scale in a controlled environment. The dataset includes MOS values, packet loss rates measured at bit stream level for both video and audio streams, compression parameters and various packet header information. We have used open source software for producing source audiovisual sequences, end-to-end streaming and a custom video player. These tools and the dataset are free to public access for research and development purposes.},
	urldate = {2019-06-14},
	author = {Demirbilek, Edip and Grégoire, Jean-Charles},
	year = {2016},
	note = {ISBN: 9781450336031},
	keywords = {MOS, •Networks → Network measure-ment, Audiovisual Quality Dataset, CCS Concepts •Information systems → Multimedia databases, Keywords Perceived Quality, Mul-timedia streaming, Network performance analysis},
	file = {PDF:/Users/rosscutler/Zotero/storage/TTEZH59H/Demirbilek, Grégoire - 2016 - INRS Audiovisual Quality Dataset.pdf:application/pdf},
}

@techreport{lin_koniq-10k:_nodate,
	title = {{KonIQ}-{10K}: {TOWARDS} {AN} {ECOLOGICALLY} {VALID} {AND} {LARGE}-{SCALE} {IQA} {DATABASE}},
	url = {http://database.mmsp-kn.de.},
	abstract = {The main challenge in applying state-of-the-art deep learning methods to predict image quality in-the-wild is the relatively small size of existing quality scored datasets. The reason for the lack of larger datasets is the massive resources required in generating diverse and publishable content. We present a new systematic and scalable approach to create large-scale, authentic and diverse image datasets for Image Quality Assessment (IQA). We show how we built an IQA database, KonIQ-10k 1 , consisting of 10,073 images, on which we performed very large scale crowdsourcing experiments in order to obtain reliable quality ratings from 1,467 crowd workers (1.2 million ratings). We argue for its ecological validity by analyzing the diversity of the dataset, by comparing it to state-of-the-art IQA databases, and by checking the reliability of our user studies.},
	urldate = {2019-05-11},
	author = {Lin, Hanhe and Hosu, Vlad and Saupe, Dietmar},
	note = {arXiv: 1803.08489v1},
	keywords = {crowdsourcing, diversity sampling, image quality assess-ment, Index Terms-Image database},
	file = {PDF:/Users/rosscutler/Zotero/storage/TFAIYE95/Lin, Hosu, Saupe - Unknown - KonIQ-10K TOWARDS AN ECOLOGICALLY VALID AND LARGE-SCALE IQA DATABASE.pdf:application/pdf},
}

@article{sheikh_statistical_2006,
	title = {A statistical evaluation of recent full reference image quality assessment algorithms},
	volume = {15},
	issn = {10577149},
	doi = {10.1109/TIP.2006.881959},
	abstract = {Measurement of visual quality is of fundamental importance for numerous image and video processing applications, where the goal of quality assessment (QA) algorithms is to automatically assess the quality of images or videos in agreement with human quality judgments. Over the years, many researchers have taken different approaches to the problem and have contributed significant research in this area and claim to have made progress in their respective domains. It is important to evaluate the performance of these algorithms in a comparative setting and analyze the strengths and weaknesses of these methods. In this paper, we present results of an extensive subjective quality assessment study in which a total of 779 distorted images were evaluated by about two dozen human subjects. The "ground truth" image quality data obtained from about 25 000 individual human quality judgments is used to evaluate the performance of several prominent full-reference image quality assessment algorithms. To the best of our knowledge, apart from video quality studies conducted by the Video Quality Experts Group, the study presented in this paper is the largest subjective image quality study in the literature in terms of number of images, distortion types, and number of human judgments per image. Moreover, we have made the data from the study freely available to the research community . This would allow other researchers to easily report comparative results in the future},
	number = {11},
	journal = {IEEE Transactions on Image Processing},
	author = {Sheikh, Hamid R. and Sabir, Muhammad F. and Bovik, Alan Conrad},
	month = nov,
	year = {2006},
	keywords = {Image quality assessment performance, Image quality study, Subjective quality assessment},
	pages = {3440--3451},
	file = {PDF:/Users/rosscutler/Zotero/storage/P4DMRVMP/Sheikh, Sabir, Bovik - 2006 - A statistical evaluation of recent full reference image quality assessment algorithms.pdf:application/pdf},
}

@techreport{uber_deconstructing_nodate,
	title = {Deconstructing {Lottery} {Tickets}: {Zeros}, {Signs}, and the {Supermask}},
	abstract = {The recent "Lottery Ticket Hypothesis" paper by Frankle \& Carbin showed that a simple approach to creating sparse networks (keep the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied significantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the re-initialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, or masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86\% on MNIST, 41\% on CIFAR-10).},
	urldate = {2019-07-01},
	author = {Uber, Hattie Zhou and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
	note = {arXiv: 1905.01067v1},
	file = {PDF:/Users/rosscutler/Zotero/storage/QZZETMLX/Uber et al. - Unknown - Deconstructing Lottery Tickets Zeros, Signs, and the Supermask.pdf:},
}

@techreport{mehta_sparse_nodate,
	title = {Sparse {Transfer} {Learning} via {Winning} {Lottery} {Tickets}},
	url = {https://github.com/rahulsmehta/sparsity-experiments},
	abstract = {The recently proposed Lottery Ticket Hypothesis of Frankle and Carbin (2019) suggests that the performance of over-parameterized deep networks is due to the random initialization seeding the network with a small fraction of favorable weights. These weights retain their dominant status throughout training-in a very real sense, this sub-network "won the lottery" during initialization. The authors find sub-networks via unstructured magnitude pruning with 85-95\% of parameters removed that train to the same accuracy as the original network at a similar speed, which they call winning tickets. In this paper, we extend the Lottery Ticket Hypothesis to a variety of transfer learning tasks. We show that sparse sub-networks with approximately 90-95\% of weights removed achieve (and often exceed) the accuracy of the original dense network in several realistic settings. We experimentally validate this by transferring the sparse representation found via pruning on CIFAR-10 to SmallNORB and FashionMNIST for object recognition tasks.},
	urldate = {2019-07-01},
	author = {Mehta, Rahul S},
	note = {arXiv: 1905.07785v1},
	file = {PDF:/Users/rosscutler/Zotero/storage/6TDJHR2X/Mehta - Unknown - Sparse Transfer Learning via Winning Lottery Tickets.pdf:application/pdf},
}

@techreport{geirhos_imagenet-trained_2019,
	title = {{IMAGENET}-{TRAINED} {CNNS} {ARE} {BIASED} {TOWARDS} {TEXTURE}; {INCREASING} {SHAPE} {BIAS} {IMPROVES} {ACCURACY} {AND} {ROBUSTNESS}},
	url = {https://github.com/rgeirhos/texture-vs-shape},
	abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation. (a) Texture image 81.4\% Indian elephant 10.3\% indri 8.2\% black swan (b) Content image 71.1\% tabby cat 17.3\% grey fox 3.3\% Siamese cat (c) Texture-shape cue conflict 63.9\% Indian elephant 26.4\% indri 9.6\% black swan Figure 1: Classification of a standard ResNet-50 of (a) a texture image (elephant skin: only texture cues); (b) a normal image of a cat (with both shape and texture cues), and (c) an image with a texture-shape cue conflict, generated by style transfer between the first two images.},
	urldate = {2019-04-29},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A and Brendel, Wieland},
	year = {2019},
	note = {arXiv: 1811.12231v2},
	file = {PDF:/Users/rosscutler/Zotero/storage/KQL9B2W4/Geirhos et al. - 2019 - IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS.pdf:application/pdf},
}

@article{cheng_polynomial_2019,
	title = {Polynomial {Regression} {As} an {Alternative} to {Neural} {Nets}},
	url = {http://arxiv.org/abs/1806.06850v3},
	urldate = {2019-05-28},
	journal = {arXiv:1806.06850v3 [cs]},
	author = {Cheng, Xi and Khomtchouk, Bohdan and Matloff, Norman and Mohanty, Pete},
	year = {2019},
}

@techreport{dey_gate-variants_nodate,
	title = {Gate-{Variants} of {Gated} {Recurrent} {Unit} ({GRU}) {Neural} {Networks}},
	abstract = {The paper evaluates three variants of the Gated Recurrent Unit (GRU) in recurrent neural networks (RNN) by reducing parameters in the update and reset gates. We evaluate the three variant GRU models on MNIST and IMDB datasets and show that these GRU-RNN variant models perform as well as the original GRU RNN model while reducing the computational expense.},
	urldate = {2019-06-13},
	author = {Dey, Rahul and Salem, Fathi M},
	file = {PDF:/Users/rosscutler/Zotero/storage/SFP2CP9B/Dey, Salem - Unknown - Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks.pdf:},
}

@techreport{frankle_lottery_nodate,
	title = {{THE} {LOTTERY} {TICKET} {HYPOTHESIS}: {FINDING} {SPARSE}, {TRAINABLE} {NEURAL} {NETWORKS}},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	urldate = {2019-05-06},
	author = {Frankle, Jonathan and Carbin, Michael},
	note = {arXiv: 1803.03635v5},
	file = {PDF:/Users/rosscutler/Zotero/storage/6VZZHQ9L/Frankle, Carbin - Unknown - THE LOTTERY TICKET HYPOTHESIS FINDING SPARSE, TRAINABLE NEURAL NETWORKS.pdf:},
}

@inproceedings{petkov_unsupervised_2019,
	title = {An {Unsupervised} {Learning} {Approach} to {Neural}-net-supported {Wpe} {Dereverberation}},
	doi = {10.1109/icassp.2019.8683542},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	author = {Petkov, Petko N. and Tsiaras, Vasileios and Doddipatla, Rama and Stylianou, Yannis},
	month = apr,
	year = {2019},
	pages = {5761--5765},
	file = {PDF:/Users/rosscutler/Zotero/storage/PB3KCTC8/Petkov et al. - 2019 - An Unsupervised Learning Approach to Neural-net-supported Wpe Dereverberation.pdf:application/pdf},
}

@techreport{ernst_speech_nodate,
	title = {Speech {Dereverberation} {Using} {Fully} {Convolutional} {Networks}},
	abstract = {Speech derverberation using a single microphone is addressed in this paper. Motivated by the recent success of the fully convolutional networks (FCN) in many image processing applications, we investigate their applicability to enhance the speech signal represented by short-time Fourier transform (STFT) images. We present two variations: a "U-Net" which is an encoder-decoder network with skip connections and a generative adversarial network (GAN) with U-Net as generator, which yields a more intuitive cost function for training. To evaluate our method we used the data from the REVERB challenge, and compared our results to other methods under the same conditions. We have found that our method outperforms the competing methods in most cases.},
	urldate = {2019-05-16},
	author = {Ernst, Ori and Chazan, Shlomo E and Gannot, Sharon and Goldberger, Jacob},
	note = {arXiv: 1803.08243v2},
	file = {PDF:/Users/rosscutler/Zotero/storage/B9AWRK8N/Ernst et al. - Unknown - Speech Dereverberation Using Fully Convolutional Networks.pdf:},
}

@techreport{cauchi_joint_nodate,
	title = {{JOINT} {DEREVERBERATION} {AND} {NOISE} {REDUCTION} {USING} {BEAMFORMING} {AND} {A} {SINGLE}-{CHANNEL} {SPEECH} {ENHANCEMENT} {SCHEME}},
	abstract = {The REVERB challenge provides a common framework for the evaluation of speech enhancement algorithms in the presence of both reverberation and noise. This contribution proposes a system consisting of a commonly used combination of a beamformer with a single-channel speech enhancement scheme aiming at joint dereverberation and noise reduction. First, a minimum variance distortionless response beam-former with an on-line estimated noise coherence matrix is used to suppress the noise and possibly some reflections. The beamformer output is then processed by a single-channel speech enhancement scheme, incorporating temporal cep-strum smoothing which suppresses both reverberation and residual noise. Experimental results show that improvements are particularly significant in conditions with high reverberation times.},
	urldate = {2019-05-16},
	author = {Cauchi, Benjamin and Kodrasi, Ina and Rehr, Robert and Gerlach, Stephan and Juki´c, Ante Juki´c and Gerkmann, Timo and Doclo, Simon and Goetze, Stefan},
	keywords = {dereverberation, Index Terms-REVERB challenge, noise reduction},
	file = {PDF:/Users/rosscutler/Zotero/storage/MIZERT6W/Cauchi et al. - Unknown - JOINT DEREVERBERATION AND NOISE REDUCTION USING BEAMFORMING AND A SINGLE-CHANNEL SPEECH ENHANCEMENT SCHEME.pdf:},
}

@article{zhang_deep_nodate,
	title = {Deep {Learning} for {Acoustic} {Echo} {Cancellation} in {Noisy} and {Double}-{Talk} {Scenarios}},
	doi = {10.21437/Interspeech.2018-1484},
	abstract = {Traditional acoustic echo cancellation (AEC) works by identifying an acoustic impulse response using adaptive algorithms. We formulate AEC as a supervised speech separation problem, which separates the loudspeaker signal and the near-end signal so that only the latter is transmitted to the far end. A recurrent neural network with bidirectional long short-term memory (BLSTM) is trained to estimate the ideal ratio mask from features extracted from the mixtures of near-end and far-end signals. A BLSTM estimated mask is then applied to separate and suppress the far-end signal, hence removing the echo. Experimental results show the effectiveness of the proposed method for echo removal in double-talk, background noise, and nonlinear distortion scenarios. In addition, the proposed method can be generalized to untrained speakers.},
	urldate = {2019-05-19},
	author = {Zhang, Hao and Wang, Deliang},
	keywords = {double-talk, ideal ratio mask, Index Terms: Acoustic echo cancellation, long short-term memory, nonlin-ear distortion, supervised speech separation},
	file = {PDF:/Users/rosscutler/Zotero/storage/HKPS9TY8/Zhang, Wang - Unknown - Deep Learning for Acoustic Echo Cancellation in Noisy and Double-Talk Scenarios.pdf:},
}

@techreport{he_photo-realistic_nodate,
	title = {Photo-realistic {Monocular} {Gaze} {Redirection} using {Generative} {Adversarial} {Networks}},
	abstract = {Gaze redirection is the task of changing the gaze to a desired direction for a given monocular eye patch image. Many applications such as videoconferencing, films and games, and generation of training data for gaze estimation require redirecting the gaze, without distorting the appearance of the area surrounding the eye and while producing photo-realistic images. Existing methods lack the ability to generate perceptually plausible images. In this work, we present a novel method to alleviate this problem by lever-aging generative adversarial training to synthesize an eye image conditioned on a target gaze direction. Our method ensures perceptual similarity and consistency of synthesized images to the real images. Furthermore, a gaze estimation loss is used to control the gaze direction accurately. To attain high-quality images, we incorporate perceptual and cycle consistency losses into our architecture. In extensive evaluations we show that the proposed method outperforms state-of-the-art approaches in terms of both image quality and redirection precision. Finally, we show that generated images can bring significant improvement for the gaze estimation task if used to augment real training data.},
	urldate = {2019-05-24},
	author = {He, Zhe and Spurr, Adrian and Zhang, Xucong and Hilliges, Otmar},
	note = {arXiv: 1903.12530v1},
	file = {PDF:/Users/rosscutler/Zotero/storage/PA2CJ6ME/He et al. - Unknown - Photo-realistic Monocular Gaze Redirection using Generative Adversarial Networks.pdf:},
}

@techreport{krull_noise2void-learning_nodate,
	title = {{Noise2Void}-{Learning} {Denoising} from {Single} {Noisy} {Images}},
	url = {http://celltrackingchallenge.net/},
	abstract = {The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as NOISE2NOISE (N2N). Here, we introduce NOISE2VOID (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denois-ing performance of NOISE2VOID drops in moderation and compares favorably to training-free denoising methods.},
	urldate = {2019-04-07},
	author = {Krull, Alexander and Buchholz, Tim-Oliver and Jug, Florian},
	note = {arXiv: 1811.10980v2
ISBN: 1811.10980v2},
	file = {PDF:/Users/rosscutler/Zotero/storage/EHAG6YH6/Krull, Buchholz, Jug - Unknown - Noise2Void-Learning Denoising from Single Noisy Images.pdf:application/pdf},
}

@inproceedings{chen_image_2018,
	title = {Image {Blind} {Denoising} {With} {Generative} {Adversarial} {Network} {Based} {Noise} {Modeling}},
	abstract = {In this paper, we consider a typical image blind denois-ing problem, which is to remove unknown noise from noisy images. As we all know, discriminative learning based methods, such as DnCNN, can achieve state-of-the-art de-noising results, but they are not applicable to this problem due to the lack of paired training data. To tackle the barrier, we propose a novel two-step framework. First, a Generative Adversarial Network (GAN) is trained to estimate the noise distribution over the input noisy images and to generate noise samples. Second, the noise patches sampled from the first step are utilized to construct a paired training dataset, which is used, in turn, to train a deep Convolutional Neural Network (CNN) for denoising. Extensive experiments have been done to demonstrate the superiority of our approach in image blind denoising.},
	urldate = {2019-04-06},
	booktitle = {{CVPR}},
	author = {Chen, Jingwen and Chen, Jiawei and Chao, Hongyang and Yang, Ming},
	year = {2018},
	file = {PDF:/Users/rosscutler/Zotero/storage/KTFI7PU4/Chen et al. - 2018 - Image Blind Denoising With Generative Adversarial Network Based Noise Modeling.pdf:},
}

@techreport{jiang_deep_nodate,
	title = {Deep {Optimization} {Model} for {Screen} {Content} {Image} {Quality} {Assessment} using {Neural} {Networks}},
	abstract = {In this paper, we propose a novel quadratic optimized model based on the deep convolutional neural network (QODCNN) for full-reference and no-reference screen content image (SCI) quality assessment. Unlike traditional CNN methods taking all image patches as training data and using average quality pooling, our model is optimized to obtain a more effective model including three steps. In the first step, an end-to-end deep CNN is trained to preliminarily predict the image visual quality, and batch normalized (BN) layers and l2 regularization are employed to improve the speed and performance of network fitting. For second step, the pretrained model is fine-tuned to achieve better performance under analysis of the raw training data. An adaptive weighting method is proposed in the third step to fuse local quality inspired by the perceptual property of the human visual system (HVS) that the HVS is sensitive to image patches containing texture and edge information. The contribution of our algorithm can be concluded as follows: 1) Considering the characteristics of SCIs, a deep and valid network architecture is designed for both NR and FR visual quality evaluation of SCIs; 2) with the consideration of correlation between local quality and subjective differential mean opinion score (DMOS), a training data selection method based on effectiveness is proposed to fine-tune the pretrained model; 3) an adaptive pooling approach is employed to fuse patch quality of textual and pictorial regions, whose feature only extracted from distorted images owns strong noise robust and effects on both FR and NR IQA. Experimental results verify that our model outperforms both current no-reference and full-reference image quality assessment methods on the benchmark screen content image quality assessment database (SIQAD). Cross-database evaluation shows high generalization ability and high effectiveness of our model.},
	urldate = {2019-07-06},
	author = {Jiang, Xuhao and Shen, Liquan and Feng, Guorui and Yu, Liangwei and An, Ping},
	note = {arXiv: 1903.00705v1},
	keywords = {convolutional neural network, data selection, full-reference, Index Terms-Image quality assessment, no-reference, quality pooling, screen content im-age},
	file = {PDF:/Users/rosscutler/Zotero/storage/ZR4MRIBF/Jiang et al. - Unknown - Deep Optimization Model for Screen Content Image Quality Assessment using Neural Networks.pdf:},
}

@article{noauthor_full-text_nodate,
	title = {full-text},
	file = {PDF:/Users/rosscutler/Zotero/storage/TB7P7WT4/Unknown - Unknown - full-text(2).pdf:application/pdf;PDF:/Users/rosscutler/Zotero/storage/IHMT9UBT/Unknown - Unknown - full-text(3).pdf:application/pdf},
}

@techreport{zhang_unreasonable_nodate,
	title = {The {Unreasonable} {Effectiveness} of {Deep} {Features} as a {Perceptual} {Metric}},
	url = {https://www.github.com/richzhang/PerceptualSimilarity.},
	abstract = {Reference Patch 1 Patch 0 Reference Patch 1 Patch 0 Reference Patch 1 Patch 0 L2/PSNR, SSIM, FSIM Random Networks Unsupervised Networks Self-Supervised Networks Supervised Networks Humans Figure 1: Which patch (left or right) is "closer" to the middle patch in these examples? In each case, the traditional metrics (L2/PSNR, SSIM, FSIM) disagree with human judgments. But deep networks, even across architectures (Squeezenet [20], AlexNet [27], VGG [51]) and supervision type (supervised [46], self-supervised [13, 40, 42, 63], and even unsupervised [26]), provide an emergent embedding which agrees surprisingly well with humans. We further calibrate existing deep embeddings on a large-scale database of perceptual judgments; models and data can be found at https://www.github.com/richzhang/PerceptualSimilarity. Abstract While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently , the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "percep-tual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (su-pervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
	urldate = {2019-04-24},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
	file = {PDF:/Users/rosscutler/Zotero/storage/NF2H2KTT/Zhang et al. - Unknown - The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.pdf:application/pdf},
}

@techreport{blau_rethinking_nodate,
	title = {Rethinking {Lossy} {Compression}: {The} {Rate}-{Distortion}-{Perception} {Tradeoff}},
	abstract = {Lossy compression algorithms are typically designed and analyzed through the lens of Shan-non's rate-distortion theory, where the goal is to achieve the lowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate. However , in recent years, it has become increasingly accepted that "low distortion" is not a synonym for "high perceptual quality", and in fact optimization of one often comes at the expense of the other. In light of this understanding, it is natural to seek for a generalization of rate-distortion theory which takes perceptual quality into account. In this paper, we adopt the mathematical definition of perceptual quality recently proposed by Blau \& Michaeli (2018), and use it to study the three-way tradeoff between rate, distortion, and perception. We show that restricting the perceptual quality to be high, generally leads to an elevation of the rate-distortion curve, thus necessitating a sacrifice in either rate or distortion. We prove several fundamental properties of this triple-tradeoff, calculate it in closed form for a Bernoulli source, and illustrate it visually on a toy MNIST example.},
	urldate = {2019-04-25},
	author = {Blau, Yochai and Michaeli, Tomer},
	note = {arXiv: 1901.07821v1},
	file = {PDF:/Users/rosscutler/Zotero/storage/6E3YMZCE/Blau, Michaeli - Unknown - Rethinking Lossy Compression The Rate-Distortion-Perception Tradeoff(2).pdf:;PDF:/Users/rosscutler/Zotero/storage/DZPPNRKZ/Blau, Michaeli - Unknown - Rethinking Lossy Compression The Rate-Distortion-Perception Tradeoff.pdf:},
}

@techreport{kede_dipiq:_nodate,
	title = {{dipIQ}: {Blind} {Image} {Quality} {Assessment} by {Learning}-to-{Rank} {Discriminable} {Image} {Pairs}},
	abstract = {Objective assessment of image quality is fundamentally important in many image processing tasks. In this work, we focus on learning blind image quality assessment (BIQA) models which predict the quality of a digital image with no access to its original pristine-quality counterpart as reference. One of the biggest challenges in learning BIQA models is the conflict between the gigantic image space (which is in the dimension of the number of image pixels) and the extremely limited reliable ground truth data for training. Such data are typically collected via subjective testing, which is cumbersome, slow, and expensive. Here we first show that a vast amount of reliable training data in the form of quality-discriminable image pairs (DIP) can be obtained automatically at low cost by exploiting large-scale databases with diverse image content. We then learn an opinion-unaware BIQA (OU-BIQA, meaning that no subjective opinions are used for training) model using RankNet, a pairwise learning-to-rank (L2R) algorithm, from millions of DIPs, each associated with a perceptual uncertainty level, leading to a DIP inferred quality (dipIQ) index. Extensive experiments on four benchmark IQA databases demonstrate that dipIQ outperforms state-of-the-art OU-BIQA models. The robustness of dipIQ is also significantly improved as confirmed by the group MAximum Differentiation (gMAD) competition method. Furthermore, we extend the proposed framework by learning models with ListNet (a listwise L2R algorithm) on quality-discriminable image lists (DIL). The resulting DIL Inferred Quality (dilIQ) index achieves an additional performance gain. Index Terms-Blind image quality assessment (BIQA), learning-to-rank (L2R), dipIQ, RankNet, quality-discriminable image pair (DIP), gMAD.},
	urldate = {2019-07-06},
	author = {Kede, Ma and Liu, Wentao and Liu, Tongliang and Wang, Zhou and Tao, Dacheng},
	note = {arXiv: 1904.06505v1},
	file = {PDF:/Users/rosscutler/Zotero/storage/GQDPWCUL/Kede et al. - Unknown - dipIQ Blind Image Quality Assessment by Learning-to-Rank Discriminable Image Pairs.pdf:},
}

@techreport{gao_learning_2015,
	title = {Learning to {Rank} for {Blind} {Image} {Quality} {Assessment}},
	abstract = {Blind image quality assessment (BIQA) aims to predict perceptual image quality scores without access to reference images. State-of-the-art BIQA methods typically require subjects to score a large number of images to train a robust model. However, subjective quality scores are imprecise, biased, and inconsistent, and it is challenging to obtain a large scale database, or to extend existing databases, because of the inconvenience of collecting images, training the subjects, conducting subjective experiments, and realigning human quality evaluations. To combat these limitations, this paper explores and exploits preference image pairs (PIPs) such as "the quality of image Ia is better than that of image I b " for training a robust BIQA model. The preference label, representing the relative quality of two images, is generally precise and consistent, and is not sensitive to image content, distortion type, or subject identity; such PIPs can be generated at very low cost. The proposed BIQA method is one of learning to rank. We first formulate the problem of learning the mapping from the image features to the preference label as one of classification. In particular, we investigate the utilization of a multiple kernel learning algorithm based on group lasso (MKLGL) to provide a solution. A simple but effective strategy to estimate perceptual image quality scores is then presented. Experiments show that the proposed BIQA method is highly effective and achieves comparable performance to state-of-the-art BIQA algorithms. Moreover, the proposed method can be easily extended to new distortion categories. Index Terms-Image quality assessment, learning to rank, multiple kernel learning, learning preferences, universal blind image quality assessment.},
	urldate = {2019-07-06},
	author = {Gao, Fei and Tao, Dacheng and Member, Senior and Gao, Xinbo and Li, Xuelong},
	year = {2015},
	note = {arXiv: 1309.0213v3
Publication Title: IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
ISBN: 2988201838},
	file = {PDF:/Users/rosscutler/Zotero/storage/ATV5PGP4/Gao et al. - 2015 - Learning to Rank for Blind Image Quality Assessment.pdf:},
}

@techreport{liu_rankiqa:_nodate,
	title = {{RankIQA}: {Learning} from {Rankings} for {No}-reference {Image} {Quality} {Assessment}},
	abstract = {We propose a no-reference image quality assessment (NR-IQA) approach that learns from rankings (RankIQA). To address the problem of limited IQA dataset size, we train a Siamese Network to rank images in terms of image quality by using synthetically generated distortions for which relative image quality is known. These ranked image sets can be automatically generated without laborious human labeling. We then use fine-tuning to transfer the knowledge represented in the trained Siamese Network to a traditional CNN that estimates absolute image quality from single images. We demonstrate how our approach can be made significantly more efficient than traditional Siamese Networks by forward propagating a batch of images through a single network and backpropagating gradients derived from all pairs of images in the batch. Experiments on the TID2013 benchmark show that we improve the state-of-the-art by over 5\%. Furthermore, on the LIVE benchmark we show that our approach is superior to existing NR-IQA techniques and that we even outperform the state-of-the-art in full-reference IQA (FR-IQA) methods without having to resort to high-quality reference images to infer IQA.},
	urldate = {2019-07-07},
	author = {Liu, Xialei and Bagdanov, Andrew D},
	file = {PDF:/Users/rosscutler/Zotero/storage/SPJB3WI2/Liu, Bagdanov - Unknown - RankIQA Learning from Rankings for No-reference Image Quality Assessment.pdf:},
}

@techreport{zhang_learning_nodate,
	title = {Learning to {Blindly} {Assess} {Image} {Quality} in the {Laboratory} and {Wild}},
	abstract = {Previous models for blind image quality assessment (BIQA) can only be trained (or fine-tuned) on one subject-rated database due to the difficulty of combining multiple databases with different perceptual scales. As a result, models trained in a well-controlled laboratory environment with synthetic distortions fail to generalize to realistic distortions, whose data distribution is different. Similarly, models optimized for images captured in the wild do not account for images simulated in the laboratory. Here we describe a simple technique of training BIQA models on multiple databases simultaneously without additional subjective testing for scale realignment. Specifically, we first create and combine image pairs within individual databases, whose ground-truth binary labels are computed from the corresponding mean opinion scores, indicating which of the two images is of higher quality. We then train a deep neural network for BIQA by learning-to-rank massive such image pairs. Extensive experiments on six databases demonstrate that our BIQA method based on the proposed learning technique works well for both synthetic and realistic distortions, outperforming existing BIQA models with a single set of model parameters. The generalizability of our method is further verified by group maximum differentiation (gMAD) competition.},
	urldate = {2019-07-06},
	author = {Zhang, Weixia and Ma, Kede and Yang, Xiaokang},
	note = {arXiv: 1907.00516v1
ISBN: 1907.00516v1},
	keywords = {deep neural networks, gMAD competition, Index Terms-Blind image quality assessment, learning-to-rank},
	file = {PDF:/Users/rosscutler/Zotero/storage/T52I38VU/Zhang, Ma, Yang - Unknown - Learning to Blindly Assess Image Quality in the Laboratory and Wild.pdf:},
}

@article{ma_waterloo_2017,
	title = {Waterloo {Exploration} {Database}: {New} {Challenges} for {Image} {Quality} {Assessment} {Models}},
	volume = {26},
	url = {http://www.ieee.org/publications_standards/publications/rights/index.html},
	doi = {10.1109/TIP.2016.2631888},
	abstract = {The great content diversity of real-world digital images poses a grand challenge to image quality assessment (IQA) models, which are traditionally designed and validated on a handful of commonly used IQA databases with very limited content variation. To test the generalization capability and to facilitate the wide usage of IQA techniques in real-world applications, we establish a large-scale database named the Waterloo Exploration Database, which in its current state contains 4744 pristine natural images and 94 880 distorted images created from them. Instead of collecting the mean opinion score for each image via subjective testing, which is extremely difficult if not impossible, we present three alternative test criteria to evaluate the performance of IQA models, namely, the pristine/distorted image discriminability test, the listwise ranking consistency test, and the pairwise preference consistency test (P-test). We compare 20 well-known IQA models using the proposed criteria, which not only provide a stronger test in a more challenging testing environment for existing models, but also demonstrate the additional benefits of using the proposed database. For example, in the P-test, even for the best performing no-reference IQA model, more than 6 million failure cases against the model are "discovered" automatically out of over 1 billion test pairs. Furthermore, we discuss how the new database may be exploited using innovative approaches in the future, to reveal the weaknesses of existing IQA models, to provide insights on how to improve the models, and to shed light on how the next-generation IQA models may be developed. The database and codes are made publicly available at: https://ece.uwaterloo.ca/{\textasciitilde}k29ma/exploration/. Index Terms-Image quality assessment, image database, discriminable image pair, listwise ranking consistency, pairwise preference consistency, mean opinion score.},
	number = {2},
	urldate = {2019-07-07},
	journal = {IEEE TRANSACTIONS ON IMAGE PROCESSING},
	author = {Ma, Kede and Member, Student and Duanmu, Zhengfang and Wu, Qingbo and Wang, Zhou and Yong, Hongwei and Li, Hongliang and Member, Senior and Zhang, Lei},
	year = {2017},
	file = {PDF:/Users/rosscutler/Zotero/storage/NKGYGIHI/Ma et al. - 2017 - Waterloo Exploration Database New Challenges for Image Quality Assessment Models.pdf:application/pdf},
}

@inproceedings{de_stoutz_fast_2019,
	title = {Fast perceptual image enhancement},
	volume = {11133 LNCS},
	isbn = {978-3-030-11020-8},
	doi = {10.1007/978-3-030-11021-5_17},
	abstract = {The vast majority of photos taken today are by mobile phones. While their quality is rapidly growing, due to physical limitations and cost constraints, mobile phone cameras struggle to compare in quality with DSLR cameras. This motivates us to computationally enhance these images. We extend upon the results of Ignatov et al., where they are able to translate images from compact mobile cameras into images with comparable quality to high-resolution photos taken by DSLR cameras. However, the neural models employed require large amounts of computational resources and are not lightweight enough to run on mobile devices. We build upon the prior work and explore different network architectures targeting an increase in image quality and speed. With an efficient network architecture which does most of its processing in a lower spatial resolution, we achieve a significantly higher mean opinion score (MOS) than the baseline while speeding up the computation by 6.3 times on a consumer-grade CPU. This suggests a promising direction for neural-network-based photo enhancement using the phone hardware of the future.},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	publisher = {Springer Verlag},
	author = {de Stoutz, Etienne and Ignatov, Andrey and Kobyshev, Nikolay and Timofte, Radu and Van Gool, Luc},
	year = {2019},
	note = {ISSN: 16113349},
	pages = {260--275},
	file = {PDF:/Users/rosscutler/Zotero/storage/V8PHLJMD/de Stoutz et al. - 2019 - Fast perceptual image enhancement.pdf:application/pdf},
}

@techreport{lehtinen_noise2noise:_2018,
	title = {{Noise2Noise}: {Learning} {Image} {Restoration} without {Clean} {Data}},
	url = {http://r0k.us/graphics/kodak/},
	abstract = {We apply basic statistical reasoning to signal reconstruction by machine learning-learning to map corrupted observations to clean signals-with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denois-ing synthetic Monte Carlo images, and reconstruction of undersampled MRI scans-all corrupted by different processes-based on noisy data only.},
	urldate = {2019-04-07},
	author = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
	year = {2018},
	note = {arXiv: 1803.04189v3},
	file = {PDF:/Users/rosscutler/Zotero/storage/UB8GMZS9/Lehtinen et al. - 2018 - Noise2Noise Learning Image Restoration without Clean Data.pdf:application/pdf},
}

@techreport{lu_unsupervised_nodate,
	title = {Unsupervised {Domain}-{Specific} {Deblurring} via {Disentangled} {Representations}},
	abstract = {Image deblurring aims to restore the latent sharp images from the corresponding blurred ones. In this paper, we present an unsupervised method for domain-specific single-image deblurring based on disentangled representations. The disentanglement is achieved by splitting the content and blur features in a blurred image using content encoders and blur encoders. We enforce a KL divergence loss to regular-ize the distribution range of extracted blur attributes such that little content information is contained. Meanwhile, to handle the unpaired training data, a blurring branch and the cycle-consistency loss are added to guarantee that the content structures of the deblurred results match the original images. We also add an adversarial loss on deblurred results to generate visually realistic images and a perceptual loss to further mitigate the artifacts. We perform extensive experiments on the tasks of face and text deblurring using both synthetic datasets and real images, and achieve improved results compared to recent state-of-the-art deblur-ring methods.},
	urldate = {2019-04-24},
	author = {Lu, Boyu and Chen, Jun-Cheng and Chellappa, Rama},
	note = {arXiv: 1903.01594v1},
	file = {PDF:/Users/rosscutler/Zotero/storage/AAXBMLXN/Lu, Chen, Chellappa - Unknown - Unsupervised Domain-Specific Deblurring via Disentangled Representations.pdf:},
}

@article{nembhard_making_2006,
	title = {Making it safe: the effects of leader inclusiveness and professional status on psychological safety and improvement efforts in health care teams},
	volume = {27},
	issn = {0894-3796},
	url = {http://doi.wiley.com/10.1002/job.413},
	doi = {10.1002/job.413},
	number = {7},
	urldate = {2019-06-27},
	journal = {Journal of Organizational Behavior},
	author = {Nembhard, Ingrid M. and Edmondson, Amy C.},
	month = nov,
	year = {2006},
	pages = {941--966},
}

@article{hancock_influence_2014,
	title = {Influence of {Communication} {Partner}'s {Gender} on {Language}},
	doi = {10.1177/0261927X14533197},
	abstract = {Forty participants (20 male) had 3-minute conversations with trained male and female communication partners in a repeated-measures, within-subject design. Eighty 3-minute conversations were transcribed and coded for dependent clauses, fillers, tag questions, intensive adverbs, negations, hedges, personal pronouns, self-references, justifiers, and interruptions. Results suggest no significant changes in language based on speaker gender. However, when speaking with a female, participants interrupted more and used more dependent clauses than when speaking with a male. There was no significant interaction to suggest that the language differences based on communication partner was specific to one gender group. These results are discussed in context of previous research, communication accommodation theory, and general process model for gendered language.},
	urldate = {2019-06-26},
	journal = {Journal of Language and Social Psychology},
	author = {Hancock, Adrienne B and Rubin, Benjamin A},
	year = {2014},
	keywords = {code-switching, communication accommodation, discourse analysis, dyads, gender},
	pages = {1--19},
	file = {PDF:/Users/rosscutler/Zotero/storage/PN3AMJDQ/Hancock, Rubin - 2014 - Influence of Communication Partner's Gender on Language.pdf:},
}

@article{mendelberg_gender_2014,
	title = {Gender {Inequality} in {Deliberation}: {Unpacking} the {Black} {Box} of {Interaction}},
	doi = {10.1017/S1537592713003691},
	abstract = {When and why do women gain from increased descriptive representation in deliberating bodies? Using a large randomized experiment, and linking individual-level speech with assessments of speaker authority, we find that decision rules interact with the number of women in the group to shape the conversation dynamics and deliberative authority, an important form of influence. With majority rule and few women, women experience a negative balance of interruptions when speaking, and these women then lose influence in their own eyes and in others'. But when the group is assigned to unanimous rule, or when women are many, women experience a positive balance of interruptions, mitigating the deleterious effect of small numbers. Men do not experience this pattern. We draw implications for a type of representation that we call authoritative representation, and for democratic deliberation.},
	urldate = {2019-06-26},
	author = {Mendelberg, Tali and Karpowitz, Christopher F and Oliphant, J Baxter},
	year = {2014},
	file = {PDF:/Users/rosscutler/Zotero/storage/T9PN45MK/Mendelberg, Karpowitz, Oliphant - 2014 - Gender Inequality in Deliberation Unpacking the Black Box of Interaction.pdf:},
}

@phdthesis{nicol_l._davidson_trust_2013,
	title = {Trust and {Member} {Inclusion} as {Communication} {Factors} to {Foster} {Collaboration} in {Globally} {Distributed} {Teams}},
	urldate = {2019-06-27},
	author = {{Nicol L. Davidson}},
	year = {2013},
	file = {PDF:/Users/rosscutler/Zotero/storage/G8HJ9MP4/Nicol L. Davidson - 2013 - Trust and Member Inclusion as Communication Factors to Foster Collaboration in Globally Distributed Teams.pdf:},
}

@article{lisak_positive_2016,
	title = {The positive role of global leaders in enhancing multicultural team innovation},
	url = {https://www.researchgate.net/publication/304455341},
	doi = {10.1057/s41267-016-0002-7},
	urldate = {2019-06-27},
	journal = {Journal of International Business Studies},
	author = {Lisak, Alon and Erez, Miriam and Sui, Yang and Lee, Cynthia},
	year = {2016},
	file = {PDF:/Users/rosscutler/Zotero/storage/RTLJUA3U/Lisak et al. - 2016 - The positive role of global leaders in enhancing multicultural team innovation.pdf:application/pdf},
}

@techreport{kumatani_multi-geometry_nodate,
	title = {{MULTI}-{GEOMETRY} {SPATIAL} {ACOUSTIC} {MODELING} {FOR} {DISTANT} {SPEECH} {RECOGNITION}},
	abstract = {The use of spatial information with multiple microphones can improve far-field automatic speech recognition (ASR) accuracy. However, conventional microphone array techniques degrade speech enhancement performance when there is an array geometry mismatch between design and test conditions. Moreover, such speech enhancement techniques do not always yield ASR accuracy improvement due to the difference between speech enhancement and ASR optimization objectives. In this work, we propose to unify an acoustic model framework by optimizing spatial filtering and long short-term memory (LSTM) layers from multi-channel (MC) input. Our acoustic model subsumes beamformers with multiple types of array geometry. In contrast to deep clustering methods that treat a neural network as a black box tool, the network encoding the spatial filters can process streaming audio data in real time without the accumulation of target signal statistics. We demonstrate the effectiveness of such MC neural networks through ASR experiments on the real-world far-field data. We show that our two-channel acoustic model can on average reduce word error rates (WERs) by 13.4 and 12.7\% compared to a single channel ASR system with the log-mel filter bank energy (LFBE) feature under the matched and mismatched microphone placement conditions, respectively. Our result also shows that our two-channel network achieves a relative WER reduction of over 7.0\% compared to conventional beamforming with seven microphones overall.},
	urldate = {2019-04-03},
	author = {Kumatani, Kenichi and Minhua, Wu and Sundaram, Shiva and Ström, Nikko and Hoffmeister, Björn},
	note = {arXiv: 1903.06539v1},
	keywords = {Index Terms-Far-field speech recognition, microphone arrays},
	file = {PDF:/Users/rosscutler/Zotero/storage/6F5AM7UV/Kumatani et al. - Unknown - MULTI-GEOMETRY SPATIAL ACOUSTIC MODELING FOR DISTANT SPEECH RECOGNITION.pdf:},
}

@techreport{minhua_frequency_nodate,
	title = {{FREQUENCY} {DOMAIN} {MULTI}-{CHANNEL} {ACOUSTIC} {MODELING} {FOR} {DISTANT} {SPEECH} {RECOGNITION}},
	abstract = {Conventional far-field automatic speech recognition (ASR) systems typically employ microphone array techniques for speech enhancement in order to improve robustness against noise or reverberation. However, such speech enhancement techniques do not always yield ASR accuracy improvement because the optimization criterion for speech enhancement is not directly relevant to the ASR objective. In this work, we develop new acoustic modeling techniques that optimize spatial filtering and long short-term memory (LSTM) layers from multi-channel (MC) input based on an ASR criterion directly. In contrast to conventional methods, we incorporate array processing knowledge into the acoustic model. Moreover, we initialize the network with beamformers' coefficients. We investigate effects of such MC neural networks through ASR experiments on the real-world far-field data where users are interacting with an ASR system in uncontrolled acoustic environments. We show that our MC acoustic model can reduce a word error rate (WER) by 16.5\% compared to a single channel ASR system with the traditional log-mel filter bank energy (LFBE) feature on average. Our result also shows that our network with the spatial filtering layer on two-channel input achieves a relative WER reduction of 9.5\% compared to conventional beamforming with seven microphones.},
	urldate = {2019-04-03},
	author = {Minhua, Wu and Kumatani, Kenichi and Sundaram, Shiva and Ström, Nikko and Hoffmeister, Björn},
	note = {arXiv: 1903.05299v1},
	keywords = {Index Terms-Far-field speech recognition, microphone arrays},
	file = {PDF:/Users/rosscutler/Zotero/storage/9IUXS3YL/Minhua et al. - Unknown - FREQUENCY DOMAIN MULTI-CHANNEL ACOUSTIC MODELING FOR DISTANT SPEECH RECOGNITION.pdf:},
}

@techreport{coleman_dawnbench:_nodate,
	title = {{DAWNBench}: {An} {End}-to-{End} {Deep} {Learning} {Benchmark} and {Competition}},
	url = {http://dawn.cs.stanford.edu/benchmark},
	abstract = {Despite considerable research on systems, algorithms and hardware to speed up deep learning workloads, there is no standard means of evaluating end-to-end deep learning performance. Existing benchmarks measure proxy metrics, such as time to process one minibatch of data, that do not indicate whether the system as a whole will produce a high-quality result. In this work, we introduce DAWNBench, a benchmark and competition focused on end-to-end training time to achieve a state-of-the-art accuracy level, as well as inference time with that accuracy. Using time to accuracy as a target metric, we explore how different optimizations, including choice of optimizer, stochastic depth, and multi-GPU training, affect end-to-end training performance. Our results demonstrate that optimizations can interact in non-trivial ways when used in conjunction, producing lower speed-ups and less accurate models. We believe DAWNBench will provide a useful, reproducible means of evaluating the many trade-offs in deep learning systems.},
	urldate = {2019-03-20},
	author = {Coleman, Cody and Narayanan, Deepak and Kang, Daniel and Zhao, Tian and Zhang, Jian and Nardi, Luigi and Bailis, Peter and Olukotun, Kunle and Ré, Chris and Zaharia, Matei and Dawn, Stanford},
	file = {PDF:/Users/rosscutler/Zotero/storage/UF67GLG8/Coleman et al. - Unknown - DAWNBench An End-to-End Deep Learning Benchmark and Competition.pdf:application/pdf},
}

@inproceedings{liu_benchmarking_2018,
	title = {Benchmarking deep learning frameworks: {Design} considerations, metrics and beyond},
	isbn = {978-1-5386-6871-9},
	doi = {10.1109/ICDCS.2018.00125},
	abstract = {With increasing number of open-source deep learning (DL) software tools made available, benchmarking DL software frameworks and systems is in high demand. This paper presents design considerations, metrics and challenges towards developing an effective benchmark for DL software frameworks and illustrate our observations through a comparative study of three popular DL frameworks: TensorFlow, Caffe, and Torch. First, we show that these deep learning frameworks are optimized with their default configurations settings. However, the default configuration optimized on one specific dataset may not work effectively for other datasets with respect to runtime performance and learning accuracy. Second, the default configuration optimized on a dataset by one DL framework does not work well for another DL framework on the same dataset. Third, experiments show that different DL frameworks exhibit different levels of robustness against adversarial examples. Through this study, we envision that unlike traditional performance-driven benchmarks, benchmarking deep learning software frameworks should take into account of both runtime and accuracy and their latent interaction with hyper-parameters and data-dependent configurations of DL frameworks.},
	booktitle = {Proceedings - {International} {Conference} on {Distributed} {Computing} {Systems}},
	author = {Liu, Ling and Wu, Yanzhao and Wei, Wenqi and Cao, Wenqi and Sahin, Semih and Zhang, Qi},
	year = {2018},
	keywords = {Accuracy, Adversarial Examples, Benchmarking, Deep Learning Frameworks, Performance, Robustness},
	file = {PDF:/Users/rosscutler/Zotero/storage/8WHPEA4U/Liu et al. - 2018 - Benchmarking deep learning frameworks Design considerations, metrics and beyond.pdf:application/pdf},
}

@article{noauthor_guidelines_nodate,
	title = {Guidelines and {Benchmarks} for {Deployment} of {Deep} {Learning} {Models} on {Smartphones} as {Real}-{Time} {Apps}},
	file = {PDF:/Users/rosscutler/Zotero/storage/LMEKV2ZP/Unknown - Unknown - full-text.pdf:application/pdf},
}

@techreport{ignatov_eth_zurich_ai_nodate,
	title = {{AI} {Benchmark}: {Running} {Deep} {Neural} {Networks} on {Android} {Smartphones} {Radu} {Timofte}},
	url = {http://ai-benchmark.com},
	abstract = {Over the last years, the computational power of mobile devices such as smartphones and tablets has grown dramatically, reaching the level of desktop computers available not long ago. While standard smartphone apps are no longer a problem for them, there is still a group of tasks that can easily challenge even high-end devices, namely running artificial intelligence algorithms. In this paper, we present a study of the current state of deep learning in the Android ecosystem and describe available frameworks, programming models and the limitations of running AI on smartphones. We give an overview of the hardware acceleration resources available on four main mobile chipset platforms: Qualcomm, HiSilicon, MediaTek and Samsung. Additionally, we present the real-world performance results of different mobile SoCs collected with AI Benchmark 1 that are covering all main existing hardware configurations.},
	urldate = {2019-03-20},
	author = {Ignatov ETH Zurich, Andrey and Zurich, Eth and Chou, William and Wang, Ke and Wu, Max and Hartley, Tim and Van Gool, Luc},
	note = {arXiv: 1810.01109v2},
	file = {PDF:/Users/rosscutler/Zotero/storage/QQQHN2GV/Ignatov ETH Zurich et al. - Unknown - AI Benchmark Running Deep Neural Networks on Android Smartphones Radu Timofte.pdf:application/pdf},
}

@techreport{carbune_smartchoices:_nodate,
	title = {{SmartChoices}: {Hybridizing} {Programming} and {Machine} {Learning}},
	abstract = {We present SmartChoices, an approach to making machine learning (ML) a first class citizen in programming languages. There is a growing divide in approaches to building systems: on the one hand, programming leverages human experts to define a system while on the other hand behavior is learned from data in machine learning. We propose to hybridize these two by leveraging the existing concept of a variable and creating a new type called SmartChoice. SmartChoices are akin to native variables with one important distinction: they determine their value using ML when evaluated. We describe the SmartChoices-interface, how it can be used in programming with minimal code changes, and demonstrate that it is an easy to use but still powerful tool by demonstrating improvements over not using ML at all on three algo-rithmic problems: binary search, QuickSort, and caches. In these three examples, we replace the commonly used heuristics with an ML model entirely encapsulated within a SmartChoice and thus requiring minimal code changes. As opposed to previous work applying ML to algorithmic problems , our proposed approach does not require to drop existing implementations but seamlessly integrates into the standard software development workflow and gives full control to the software developer over how ML methods are applied. Our implementation currently relies on standard Reinforcement Learning (RL) methods. To learn faster, we use the heuristic function, which they are replacing, as an initial function. We show how this initial function can be used to speed up and stabilize learning while providing a safety net that prevents performance to become substantially worse-allowing for a safe deployment in critical applications.},
	urldate = {2019-06-04},
	author = {Carbune, Victor and Coppey, Thierry and Daryin, Alexander and Deselaers, Thomas and Sarda, Nikhil and Yagnik, Jay},
	note = {arXiv: 1810.00619v2},
	file = {PDF:/Users/rosscutler/Zotero/storage/RLNZFKDF/Carbune et al. - Unknown - SmartChoices Hybridizing Programming and Machine Learning.pdf:},
}

@article{pascual_segan:_2017,
	title = {{SEGAN}: {Speech} {Enhancement} {Generative} {Adversarial} {Network}},
	url = {http://dx.doi.org/10.7488/ds/1356},
	doi = {10.7488/ds/1356},
	abstract = {Current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature. The majority of them tackle a limited number of noise conditions and rely on first-order statistics. To circumvent these issues, deep networks are being increasingly used, thanks to their ability to learn complex functions from large example sets. In this work, we propose the use of generative adversarial networks for speech enhancement. In contrast to current techniques, we operate at the waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same model, such that model parameters are shared across them. We evaluate the proposed model using an independent, unseen test set with two speakers and 20 alternative noise conditions. The enhanced samples confirm the viability of the proposed model, and both objective and subjective evaluations confirm the effectiveness of it. With that, we open the exploration of gen-erative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to improve their performance.},
	urldate = {2019-03-23},
	journal = {Interspeech},
	author = {Pascual, Santiago and Bonafonte, Antonio and Serrà, Joan},
	year = {2017},
	note = {arXiv: 1703.09452v3},
	keywords = {deep learning, convolutional neural networks, generative adversarial networks, Index Terms: speech enhancement},
	pages = {3642--3646},
	file = {PDF:/Users/rosscutler/Zotero/storage/XCDXRQN6/Pascual, Bonafonte, Serrà - 2017 - SEGAN Speech Enhancement Generative Adversarial Network.pdf:application/pdf},
}

@inproceedings{hu_subjective_2006,
	title = {Subjective comparison of speech enhancement algorithms},
	isbn = {1-4244-0469-X},
	url = {http://www.utdallas.edu/},
	abstract = {We report on the development of a noisy speech corpus suitable for evaluation of speech enhancement algorithms. This corpus is used for the subjective evaluation of 13 speech enhancement methods encompassing four classes of algorithms: spectral subtractive, subspace, statistical-model based and Wi-ener algorithms. The subjective evaluation was performed by Dynastat, Inc. using the ITU-T P.835 methodology designed to evaluate the speech quality along three dimensions: signal distortion, noise distortion and overall quality. This paper reports the results of the subjective tests.},
	booktitle = {{ICASSP}},
	author = {Hu, Yi and Loizou, Philipos C},
	year = {2006},
	pages = {153--156},
	file = {PDF:/Users/rosscutler/Zotero/storage/KWY4URPV/Hu, Loizou - 2006 - Subjective comparison of speech enhancement algorithms.pdf:application/pdf},
}

@inproceedings{valin_hybrid_2018,
	title = {A hybrid {DSP}/{Deep} learning approach to real-time full-band speech enhancement},
	isbn = {978-1-5386-6070-6},
	url = {http://www-mmsp.ece.mcgill.ca/Documents/Data/},
	doi = {10.1109/MMSP.2018.8547084},
	abstract = {Despite noise suppression being a mature area in signal processing, it remains highly dependent on fine tuning of estimator algorithms and parameters. In this paper, we demonstrate a hybrid DSP/deep learning approach to noise suppression. A deep neural network with four hidden layers is used to estimate ideal critical band gains, while a more traditional pitch filter attenuates noise between pitch harmonics. The approach achieves significantly higher quality than a traditional minimum mean squared error spectral estimator, while keeping the complexity low enough for real-time operation at 48 kHz on a low-power processor.},
	booktitle = {{IEEE} {International} {Workshop} on {Multimedia} {Signal} {Processing},},
	author = {Valin, Jean Marc},
	year = {2018},
	note = {arXiv: 1709.08243},
	keywords = {noise suppression, recurrent neural network},
	file = {PDF:/Users/rosscutler/Zotero/storage/JEMAMGUG/Valin - 2018 - A hybrid DSPDeep learning approach to real-time full-band speech enhancement.pdf:application/pdf},
}

@techreport{luo_tasnet:_nodate,
	title = {{TasNet}: {Surpassing} {Ideal} {Time}-{Frequency} {Masking} for {Speech} {Separation}},
	abstract = {Robust speech processing in multitalker acoustic environments requires automatic speech separation. While single-channel, speaker-independent speech separation methods have recently seen great progress, the accuracy, latency, and computational cost of speech separation remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of spectro-gram representations for speech separation, and the long latency in calculating the spectrogram. To address these shortcomings, we propose the time-domain audio separation network (TasNet), which is a deep learning autoencoder framework for time-domain speech separation. TasNet uses a convolutional encoder to create a representation of the signal that is optimized for extracting individual speakers. Speaker extraction is achieved by applying a weighting function (mask) to the encoder output. The modified encoder representation is then inverted to the sound waveform using a linear decoder. The masks are found using a temporal convolutional network consisting of dilated convolutions, which allow the network to model the long-term dependencies of the speech signal. This end-to-end speech separation algorithm significantly outperforms previous time-frequency methods in terms of separating speakers in mixed audio, even when compared to the separation accuracy achieved with the ideal time-frequency mask of the speakers. In addition, TasNet has a smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications. This study therefore represents a major step toward actualizing speech separation for real-world speech processing technologies.},
	urldate = {2019-04-09},
	author = {Luo, Yi and Mesgarani, Nima},
	note = {arXiv: 1809.07454v2},
	keywords = {deep learning, Index Terms-Source separation, real-time, single-channel, time-domain},
	file = {PDF:/Users/rosscutler/Zotero/storage/2IL78VH8/Luo, Mesgarani - Unknown - TasNet Surpassing Ideal Time-Frequency Masking for Speech Separation.pdf:},
}

@techreport{germain_speech_nodate,
	title = {Speech {Denoising} with {Deep} {Feature} {Losses}},
	url = {http://ieeexplore.ieee.org.},
	abstract = {We present an end-to-end deep learning approach to denoising speech signals by processing the raw waveform directly. Given input audio containing speech corrupted by an additive background signal, the system aims to produce a processed signal that contains only the speech content. Recent approaches have shown promising results using various deep network architec-tures. In this paper, we propose to train a fully-convolutional context aggregation network using a deep feature loss. That loss is based on comparing the internal feature activations in a different network, trained for acoustic environment detection and domestic audio tagging. Our approach outperforms the state-of-the-art in objective speech quality metrics and in large-scale perceptual experiments with human listeners. It also outperforms an identical network trained using traditional regression losses. The advantage of the new approach is particularly pronounced for the hardest data with the most intrusive background noise, for which denoising is most needed and most challenging.},
	urldate = {2019-05-19},
	author = {Germain, François G and Chen, Qifeng and Koltun, Vladlen},
	note = {arXiv: 1806.10522v2},
	keywords = {deep learning, speech enhancement, context aggregation network, deep feature loss, Index Terms-Speech denoising},
	file = {PDF:/Users/rosscutler/Zotero/storage/888TGH6A/Germain, Chen, Koltun - Unknown - Speech Denoising with Deep Feature Losses.pdf:application/pdf},
}

@inproceedings{rethage_wavenet_2018,
	title = {A {Wavenet} for {Speech} {Denoising} {Outline} {Motivation}},
	isbn = {978-1-5386-4658-8},
	url = {www.jordipons.me-@jordiponsme},
	abstract = {Most speech processing techniques use magnitude spectro-grams as front-end and are therefore by default discarding part of the signal: the phase. In order to overcome this limitation , we propose an end-to-end learning method for speech denoising based on Wavenet. The proposed model adaptation retains Wavenet's powerful acoustic modeling capabilities, while significantly reducing its time-complexity by eliminating its autoregressive nature. Specifically, the model makes use of non-causal, dilated convolutions and predicts target fields instead of a single target sample. The discriminative adaptation of the model we propose, learns in a supervised fashion via minimizing a regression loss. These modifications make the model highly parallelizable during both training and inference. Both quantitative and qualitative evaluations indicate that the proposed method is preferred over Wiener filtering, a common method based on processing the magnitude spectrogram.},
	booktitle = {{ICASSP}},
	author = {Rethage, Dario and Pons, Jordi and Serra, Xavier},
	year = {2018},
	keywords = {deep learning, convolutional neural networks, Index Terms-Speech denoising, audio, end-to-end learning},
	pages = {5069--5073},
	file = {PDF:/Users/rosscutler/Zotero/storage/QKRR8ISE/Rethage, Pons, Serra - 2018 - A Wavenet for Speech Denoising Outline Motivation.pdf:application/pdf},
}

@techreport{qian_speech_nodate,
	title = {Speech {Enhancement} {Using} {Bayesian} {Wavenet}},
	url = {http://tiny.cc/7t5dly.},
	abstract = {In recent years, deep learning has achieved great success in speech enhancement. However, there are two major limitations regarding existing works. First, the Bayesian framework is not adopted in many such deep-learning-based algorithms. In particular , the prior distribution for speech in the Bayesian framework has been shown useful by regularizing the output to be in the speech space, and thus improving the performance. Second, the majority of the existing methods operate on the frequency domain of the noisy speech, such as spectrogram and its variations. The clean speech is then reconstructed using the approach of overlap-add, which is limited by its inherent performance upper bound. This paper presents a Bayesian speech enhancement framework, called BaWN (Bayesian WaveNet), which directly operates on raw audio samples. It adopts the recently announced WaveNet, which is shown to be effective in mod-eling conditional distributions of speech samples while generating natural speech. Experiments show that BaWN is able to recover clean and natural speech.},
	urldate = {2019-03-23},
	author = {Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and Florêncio, Dinei and Hasegawa-Johnson, Mark},
	keywords = {convolutional neural network, Index Terms: speech enhancement, Bayesian framework, model-based, WaveNet},
	file = {PDF:/Users/rosscutler/Zotero/storage/LRA5K9U8/Qian et al. - Unknown - Speech Enhancement Using Bayesian Wavenet.pdf:application/pdf},
}

@inproceedings{nie_deep_2018,
	title = {Deep noise tracking network: {A} hybrid signal processing/deep learning approach to speech enhancement},
	volume = {2018-Septe},
	doi = {10.21437/Interspeech.2018-1020},
	abstract = {Noise statistics and speech spectrum characteristics are the essential information for the single channel speech enhancement. The signal processing-based methods mainly rely on noise statistics estimation. They perform very well for stationary noise, but have remained difficult to cope with non-stationary noise. While the deep learning-based methods mainly focus on the perception on the spectrum characteristics of speech and have a capacity in dealing with non-stationary noise. However, the performance would degrade dramatically for the unseen noise types, which could be due to the over-reliance on data and the ignorance to domain knowledge of signal process. Obviously, the hybrid signal processing/deep learning scheme may be a smart alternative. In this paper, we incorporate the powerful perceptual capabilities of deep learning in the conventional speech enhancement framework. Deep learning is used to estimate the speech presence probability and the update factor of noise statistics, which are then integrated into the Wiener filter-based speech enhancement structure to enhance the desired speech. All components are jointly optimized by a spectrum approximation objective. Systematic experiments on CHiME-4 and NOISEX-92 demonstrate the proposed hybrid signal processing/deep learning approach to noise suppression in noise-unmatched and noise-matched conditions .},
	urldate = {2019-03-23},
	booktitle = {Proceedings of the {Annual} {Conference} of the {International} {Speech} {Communication} {Association}, {INTERSPEECH}},
	author = {Nie, Shuai and Liang, Shan and Liu, Bin and Zhang, Yaping and Liu, Wenju and Tao, Jianhua},
	year = {2018},
	note = {ISSN: 19909772},
	keywords = {Speech enhancement, Deep learning, Noise tracking, Signal processing},
	pages = {3219--3223},
	file = {PDF:/Users/rosscutler/Zotero/storage/S24DR8DU/Nie et al. - 2018 - Deep noise tracking network A hybrid signal processingdeep learning approach to speech enhancement.pdf:},
}

@inproceedings{valentini-botinhao_speech_2016,
	title = {Speech enhancement for a noise-robust text-to-speech synthesis system using deep recurrent neural networks},
	volume = {08-12-Sept},
	url = {http://dx.doi.org/10.21437/Interspeech.2016-159},
	doi = {10.21437/Interspeech.2016-159},
	abstract = {Quality of text-to-speech voices built from noisy recordings is diminished. In order to improve it we propose the use of a re-current neural network to enhance acoustic parameters prior to training. We trained a deep recurrent neural network using a parallel database of noisy and clean acoustics parameters as in-put and output of the network. The database consisted of mul-tiple speakers and diverse noise conditions. We investigated using text-derived features as an additional input of the net-work. We processed a noisy database of two other speakers using this network and used its output to train an HMM acous-tic text-to-synthesis model for each voice. Listening experiment results showed that the voice built with enhanced parameters was ranked significantly higher than the ones trained with noisy speech and speech that has been enhanced using a conventional enhancement system. The text-derived features improved re-sults only for the female voice, where it was ranked as highly as a voice trained with clean speech.},
	urldate = {2019-03-23},
	booktitle = {{INTERSPEECH}},
	author = {Valentini-Botinhao, Cassia and Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
	year = {2016},
	note = {ISSN: 19909772},
	keywords = {Speech enhancement, RNN, Speech synthesis},
	pages = {352--356},
	file = {PDF:/Users/rosscutler/Zotero/storage/XV52FXSU/Valentini-Botinhao et al. - 2016 - Speech enhancement for a noise-robust text-to-speech synthesis system using deep recurrent neural net.pdf:application/pdf},
}

@article{chirtmay_speech_1997,
	title = {Speech {Enhancement} using {Wiener} filtering},
	volume = {21},
	abstract = {The problem of reducing the disturbing effects of additive white noise on a speech signal is considered when a "noise-reference" is not available. Wiener filtering with all-pole modeling built built upon line spectral pair (LSP) frequencies is considered. The filter parameters have been optimized to achieve the highest reduction of noise. The noise is filtered using an iterative LSP-based estimations of LPC parameters. The speech model filter uses an accurate updated estimate of the current noise power spectral density with the aid of a voice activity decoder.},
	number = {6},
	urldate = {2019-03-28},
	journal = {Acoustic Letters},
	author = {Chirtmay, S and Tahernezhadi, M},
	year = {1997},
	pages = {110--115},
	file = {PDF:/Users/rosscutler/Zotero/storage/T5AQCY8U/Chirtmay, Tahernezhadi - 1997 - Speech Enhancement using Wiener filtering.pdf:application/pdf},
}

@article{martin-donas_deep_2018,
	title = {A deep learning loss function based on the perceptual evaluation of the speech quality},
	volume = {25},
	issn = {10709908},
	doi = {10.1109/LSP.2018.2871419},
	number = {11},
	journal = {IEEE Signal Processing Letters},
	author = {Martin-Donas, Juan Manuel and Gomez, Angel Manuel and Gonzalez, Jose A. and Peinado, Antonio M.},
	month = nov,
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Deep learning, speech enhancement, PESQ, DNN, loss function},
	pages = {1680--1684},
	file = {PDF:/Users/rosscutler/Zotero/storage/PY9HVI29/Martin-Donas et al. - 2018 - A deep learning loss function based on the perceptual evaluation of the speech quality.pdf:application/pdf},
}

@techreport{michelashvili_audio_nodate,
	title = {Audio {Denoising} with {Deep} {Network} {Priors}},
	url = {https://github.com/mosheman5/DNP.},
	abstract = {We present a method for audio denoising that combines processing done in both the time domain and the time-frequency domain. Given a noisy audio clip, the method trains a deep neural network to fit this signal. Since the fitting is only partly successful and is able to better capture the underlying clean signal than the noise, the output of the network helps to disentangle the clean audio from the rest of the signal. The method is completely unsupervised and only trains on the specific audio clip that is being denoised. Our experiments demonstrate favorable performance in comparison to the literature methods , and our code and audio samples are available at https: //github.com/mosheman5/DNP.},
	urldate = {2019-05-19},
	author = {Michelashvili, Michael and Wolf, Lior},
	note = {arXiv: 1904.07612v1},
	keywords = {Audio denoising, Index Terms, Unsupervised learning},
	file = {PDF:/Users/rosscutler/Zotero/storage/YJUE2G3C/Michelashvili, Wolf - Unknown - Audio Denoising with Deep Network Priors.pdf:application/pdf},
}

@techreport{guo_curriculumnet:_nodate,
	title = {{CurriculumNet}: {Weakly} {Supervised} {Learning} from {Large}-{Scale} {Web} {Images}},
	url = {https://github.com/guoshengcv/CurriculumNet.},
	abstract = {We present a simple yet efficient approach capable of training deep neural networks on large-scale weakly-supervised web images, which are crawled rawly from the Internet by using text queries, without any human annotation. We develop a principled learning strategy by leveraging curriculum learning, with the goal of handling massive amount of noisy labels and data imbalance effectively. We design a new learning curriculum by measuring the complexity of data using its distribution density in a feature space, and rank the complexity in an unsuper-vised manner. This allows for an efficient implementation of curriculum learning on large-scale web images, resulting in a high-performance CNN model, where the negative impact of noisy labels is reduced substantially. Importantly, we show by experiments that those images with highly noisy labels can surprisingly improve the generalization capability of model, by serving as a manner of regularization. Our approaches obtain the state-of-the-art performance on four benchmarks, including Webvision, Ima-geNet, Clothing-1M and Food-101. With an ensemble of multiple models, we achieve a top-5 error rate of 5.2\% on the Webvision challenge [18] for 1000-category classification, which is the top performance that surpasses other results by a large margin of about 50\% relative error rate. Codes and models are available at: https://github.com/guoshengcv/CurriculumNet.},
	urldate = {2019-03-12},
	author = {Guo, Sheng and Huang, Weilin and Zhang, Haozhi and Zhuang, Chenfan and Dong, Dengke and Scott, Matthew R and Huang, Dinglong},
	file = {PDF:/Users/rosscutler/Zotero/storage/FGELY35H/Guo et al. - Unknown - CurriculumNet Weakly Supervised Learning from Large-Scale Web Images.pdf:application/pdf},
}

@techreport{wang_iterative_nodate,
	title = {Iterative {Learning} with {Open}-set {Noisy} {Labels}},
	abstract = {Large-scale datasets possessing clean label annotations are crucial for training Convolutional Neural Networks (CNNs). However, labeling large-scale data can be very costly and error-prone, and even high-quality datasets are likely to contain noisy (incorrect) labels. Existing works usually employ a closed-set assumption, whereby the samples associated with noisy labels possess a true class contained within the set of known classes in the training data. However, such an assumption is too restrictive for many applications , since samples associated with noisy labels might in fact possess a true class that is not present in the training data. We refer to this more complex scenario as the open-set noisy label problem and show that it is nontrivial in order to make accurate predictions. To address this problem, we propose a novel iterative learning framework for training CNNs on datasets with open-set noisy labels. Our approach detects noisy labels and learns deep discriminative features in an iterative fashion. To benefit from the noisy label detection , we design a Siamese network to encourage clean labels and noisy labels to be dissimilar. A reweighting module is also applied to simultaneously emphasize the learning from clean labels and reduce the effect caused by noisy labels. Experiments on CIFAR-10, ImageNet and real-world noisy (web-search) datasets demonstrate that our proposed model can robustly train CNNs in the presence of a high proportion of open-set as well as closed-set noisy labels.},
	urldate = {2019-03-12},
	author = {Wang, Yisen and Liu, Weiyang and Ma, Xingjun and Bailey, James and Zha, Hongyuan and Song, Le and Xia, Shu-Tao},
	file = {PDF:/Users/rosscutler/Zotero/storage/ZCZKCD6A/Wang et al. - Unknown - Iterative Learning with Open-set Noisy Labels.pdf:},
}

@article{cauchi_non-intrusive_2019,
	title = {Non-{Intrusive} {Speech} {Quality} {Prediction} {Using} {Modulation} {Energies} and {LSTM}-{Network}},
	volume = {27},
	url = {http://www.ieee.org/publications_standards/publications/rights/index.html},
	doi = {10.1109/TASLP.2019.2912123},
	abstract = {Many signal processing algorithms have been proposed to improve the quality of speech recorded in the presence of noise and reverberation. Perceptual measures, i.e., listening tests, are usually considered the most reliable way to evaluate the quality of speech processed by such algorithms but are costly and time-consuming. Consequently, speech enhancement algorithms are often evaluated using signal-based measures, which can be either intrusive or non-intrusive. As the computation of intrusive measures requires a reference signal, only non-intrusive measures can be used in applications for which the clean speech signal is not available. However, many existing non-intrusive measures correlate poorly with the perceived speech quality, particularly when applied over a wide range of algorithms or acoustic conditions. In this paper, we propose a novel non-intrusive measure of the quality of processed speech that combines modulation energy features and a recurrent neural network using long short-term memory cells. We collected a dataset of perceptually evaluated signals representing several acoustic conditions and algorithms and used this dataset to train and evaluate the proposed measure. Results show that the proposed measure yields higher correlation with perceptual speech quality than that of benchmark intrusive and non-intrusive measures when considering various categories of algorithms. Although the proposed measure is sensitive to mismatch between training and testing, results show that it is a useful approach to evaluate specific algorithms over a wide range of acoustic conditions and may, thus, become particularly useful for real-time selection of speech enhancement algorithm settings.},
	number = {7},
	urldate = {2019-06-10},
	journal = {IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
	author = {Cauchi, Benjamin and Siedenburg, Kai and Santos, João F and Falk, Tiago H and Doclo, Simon and Goetze, Stefan and Siedenburg, K and Doclo, S and Santos, J F and Falk, T H},
	year = {2019},
	keywords = {Index Terms-Speech quality, LSTM-network, modu-lation energy, non-intrusive prediction},
	pages = {1151},
	file = {PDF:/Users/rosscutler/Zotero/storage/RRAC3KFA/Cauchi et al. - 2019 - Non-Intrusive Speech Quality Prediction Using Modulation Energies and LSTM-Network.pdf:application/pdf},
}

@techreport{cartwright_fast_nodate,
	title = {{FAST} {AND} {EASY} {CROWDSOURCED} {PERCEPTUAL} {AUDIO} {EVALUATION}},
	abstract = {Automated objective methods of audio evaluation are fast, cheap, and require little effort by the investigator. However, objective evaluation methods do not exist for the output of all audio processing algorithms, often have output that correlates poorly with human quality assessments, and require ground truth data in their calculation. Subjective human ratings of audio quality are the gold standard for many tasks, but are expensive, slow, and require a great deal of effort to recruit subjects and run listening tests. Moving listening tests from the lab to the micro-task labor market of Amazon Mechanical Turk speeds data collection and reduces investigator effort. However, it also reduces the amount of control investigators have over the testing environment, adding new variability and potential biases to the data. In this work, we compare multiple stimulus listening tests performed in a lab environment to multiple stimulus listening tests performed in web environment on a population drawn from Mechanical Turk.},
	urldate = {2019-05-28},
	author = {Cartwright, Mark and Pardo, Bryan and Mysore, Gautham J and Hoffman, Matt},
	keywords = {crowdsourcing, Index Terms-audio quality evaluation},
	file = {PDF:/Users/rosscutler/Zotero/storage/GMTEUHUD/Cartwright et al. - Unknown - FAST AND EASY CROWDSOURCED PERCEPTUAL AUDIO EVALUATION.pdf:},
}

@article{zequeira_jimenez_outliers_2018,
	title = {Outliers {Detection} vs. {Control} {Questions} to {Ensure} {Reliable} {Results} in {Crowdsourcing}.{A} {Speech} {Quality} {Assessment} {Case} {Study}},
	url = {https://doi.org/10.1145/3184558.3191545},
	doi = {10.1145/3184558.3191545},
	abstract = {Crowdsourcing provides an exceptional opportunity for the rapid collection of human input for data acquisition and labelling. This approach have been adopted in multiple domains and researchers are now able to reach a demographically diverse audience at low cost. However, it remains the question of whether the results are still valid and reliable. Previous work have introduced different mechanisms to ensure data reliability in crowdsourcing. This work examines to which extend, "trapping question" or "outliers detec-tion" assure reliable results to the detriment of, overloading task content with stimuli that are not of interest for the researcher, or by discarding data points that might be the true opinion of a worker. To this end, a speech quality assessment study have been conducted in a web crowdsourcing platform, following the ITU-T Rec. P.800. Workers assessed the speech stimuli of the database 501 from the ITU-T Rec. P.863. We examine results' validity in terms of correlations to previous ratings collected in laboratory. Our outcomes shows that neither of the techniques under investigation improve results accuracy by itself, but a combination of both. Our goal is to provide empirical guidance for designing experiments in crowdsourcing while ensuring data reliability.},
	urldate = {2019-05-28},
	author = {Zequeira Jiménez, Rafael and Fernández Gallardo, Laura and Möller, Sebastian and Sartor, Jennifer B and De, Wolfgang},
	year = {2018},
	note = {Publisher: WWW '18 Companion
ISBN: 9781450356404},
	keywords = {speech quality assessment, crowdsourcing, data validity, gold-strandard questions, outliers detection, trapping questions, users reliability},
	file = {PDF:/Users/rosscutler/Zotero/storage/EMY6NQG5/Zequeira Jiménez et al. - 2018 - Outliers Detection vs. Control Questions to Ensure Reliable Results in Crowdsourcing.A Speech Quality A.pdf:application/pdf},
}

@article{de_leeuw_jspsych:_nodate,
	title = {{jsPsych}: {A} {JavaScript} library for creating behavioral experiments in a {Web} browser},
	url = {https://github.com/jodeleeuw/jsPsych/wiki/List-of-Plugins},
	doi = {10.3758/s13428-014-0458-y},
	abstract = {Online experiments are growing in popularity, and the increasing sophistication of Web technology has made it possible to run complex behavioral experiments online using only a Web browser. Unlike with offline laboratory experiments, however, few tools exist to aid in the development of browser-based experiments. This makes the process of creating an experiment slow and challenging, particularly for researchers who lack a Web development background. This article introduces jsPsych, a JavaScript library for the development of Web-based experiments. jsPsych formalizes a way of describing experiments that is much simpler than writing the entire experiment from scratch. jsPsych then executes these descriptions automatically, handling the flow from one task to another. The jsPsych library is open-source and designed to be expanded by the research community. The project is available online at www.jspsych.org.},
	urldate = {2019-05-31},
	author = {De Leeuw, Joshua R},
	file = {PDF:/Users/rosscutler/Zotero/storage/NRSMRJ6C/De Leeuw - Unknown - jsPsych A JavaScript library for creating behavioral experiments in a Web browser.pdf:application/pdf},
}

@techreport{navarro_flexible_nodate,
	title = {Flexible experiments in the browser: {A} tutorial in {jsPsych} and {Google} app engine},
	urldate = {2019-05-31},
	author = {Navarro, Dani},
	file = {PDF:/Users/rosscutler/Zotero/storage/RRXHJD3L/Navarro - Unknown - Flexible experiments in the browser A tutorial in jsPsych and Google app engine.pdf:},
}

@article{salamon_seeing_2017,
	title = {Seeing {Sound}: {Investigating} the {Effects} of {Visualizations} and {Complexity} on {Crowdsourced} {Audio} {Annotations}},
	volume = {29},
	doi = {10.1145/3134664},
	abstract = {Audio annotation is key to developing machine-listening systems; yet, effective ways to accurately and rapidly obtain crowdsourced audio annotations is understudied. In this work, we seek to quantify the relia-bility/redundancy trade-off in crowdsourced soundscape annotation, investigate how visualizations affect accuracy and efficiency, and characterize how performance varies as a function of audio characteristics. Using a controlled experiment, we varied sound visualizations and the complexity of soundscapes presented to human annotators. Results show that more complex audio scenes result in lower annotator agreement, and spectrogram visualizations are superior in producing higher quality annotations at lower cost of time and human labor. We also found recall is more affected than precision by soundscape complexity, and mistakes can be often attributed to certain sound event characteristics. These findings have implications not only for how we should design annotation tasks and interfaces for audio data, but also how we train and evaluate machine-listening systems.},
	number = {2},
	urldate = {2019-05-28},
	journal = {Proc. ACM Hum.-Comput. Interact. 1, CSCW, Article},
	author = {Salamon, Justin and Williams, Alex and Mikloska, Stefanie and Macconnell, Duncan and Bello, Juan P and Cartwright, Mark and Seals, Ayanna and Law, Edith and Nov, Oded and Law, ; Edith},
	year = {2017},
	note = {Publisher: CSCW},
	keywords = {Annotation, Sound Event Detection},
	pages = {29},
	file = {PDF:/Users/rosscutler/Zotero/storage/FF4HDFKI/Salamon et al. - 2017 - Seeing Sound Investigating the Effects of Visualizations and Complexity on Crowdsourced Audio Annotations.pdf:},
}

@techreport{cartwright_crowdsourced_nodate,
	title = {Crowdsourced {Pairwise}-{Comparison} {For} {Source} {Separation} {Evaluation}},
	url = {https://github.com/interactiveaudiolab/CAQE},
	abstract = {Automated objective methods of audio source separation evaluation are fast, cheap, and require little effort by the investigator. However, their output often correlates poorly with human quality assessments and typically require ground-truth (perfectly separated) signals to evaluate algorithm performance. Subjective multi-stimulus human ratings (e.g. MUSHRA) of audio quality are the gold standard for many tasks, but they are slow and require a great deal of effort to recruit participants and run listening tests. Recent work has shown that a crowdsourced multi-stimulus listening test can have results comparable to lab-based multi-stimulus tests. While these results are encouraging, MUSHRA multi-stimulus tests are limited to evaluating 12 or fewer stimuli, and they require ground-truth stimuli for reference. In this work, we evaluate a web-based pairwise-comparison listening approach that promises to speed and facilitate conducting listening tests, while also addressing some of the shortcomings of multi-stimulus tests. Using audio source separation quality as our evaluation task, we compare our web-based pairwise-comparison listening test to both web-based and lab-based multi-stimulus tests. We find that pairwise-comparison listening tests perform comparably to multi-stimulus tests, but without many of their shortcomings.},
	urldate = {2019-05-28},
	author = {Cartwright, Mark and Pardo, Bryan and Mysore, Gautham J},
	keywords = {source separation, crowdsourcing, Index Terms-audio quality evaluation},
	file = {PDF:/Users/rosscutler/Zotero/storage/IPHUL9AI/Cartwright, Pardo, Mysore - Unknown - Crowdsourced Pairwise-Comparison For Source Separation Evaluation.pdf:application/pdf},
}

@techreport{noauthor_understanding_nodate,
	title = {Understanding {MOS}, {JND}, and {PSNR}},
	url = {www.videoclarity.com},
	urldate = {2019-05-31},
	institution = {Video Clarity},
	note = {ISBN: 4083796952},
	file = {PDF:/Users/rosscutler/Zotero/storage/F3QARVAN/Unknown - Unknown - Understanding MOS, JND, and PSNR.pdf:application/pdf},
}

@book{ribeiro_crowdmos:_nodate,
	title = {{CROWDMOS}: {AN} {APPROACH} {FOR} {CROWDSOURCING} {MEAN} {OPINION} {SCORE} {STUDIES}},
	isbn = {978-1-4577-0539-7},
	url = {http://research.microsoft.com/crowdmos/.},
	abstract = {MOS (mean opinion score) subjective quality studies are used to evaluate many signal processing methods. Since laboratory quality studies are time consuming and expensive, researchers often run small studies with less statistical significance or use objective measures which only approximate human perception. We propose a cost-effective and convenient measure called crowdMOS, obtained by having internet users participate in a MOS-like listening study. Workers listen and rate sentences at their leisure, using their own hardware, in an environment of their choice. Since these individuals cannot be supervised, we propose methods for detecting and discarding inaccurate scores. To automate crowdMOS testing, we offer a set of freely distributable, open-source tools for Amazon Mechanical Turk, a platform designed to facilitate crowdsourcing. These tools implement the MOS testing methodology described in this paper , providing researchers with a user-friendly means of performing subjective quality evaluations without the overhead associated with laboratory studies. Finally, we demonstrate the use of crowdMOS using data from the Blizzard text-to-speech competition, showing that it delivers accurate and repeatable results.},
	urldate = {2019-06-01},
	author = {Ribeiro, Flávio and Florêncio, Dinei and Zhang, Cha and Seltzer, Michael},
	keywords = {mean opinion score, MOS, MUSHRA, crowdMOS, Index Terms-crowdsourcing, mechanical turk, subjective quality},
	file = {PDF:/Users/rosscutler/Zotero/storage/ICSDNLLY/Ribeiro et al. - Unknown - CROWDMOS AN APPROACH FOR CROWDSOURCING MEAN OPINION SCORE STUDIES.pdf:application/pdf},
}

@article{keimel_qualitycrowd-framework_2012,
	title = {{QualityCrowd}-{A} {Framework} for {Crowd}-based {Quality} {Evaluation}},
	doi = {10.1109/PCS.2012.6213338},
	abstract = {Video quality assessment with subjective testing is both time consuming and expensive. An interesting new approach to traditional testing is the so-called crowdsourcing, moving the testing effort into the internet. We therefore propose in this contribution the QualityCrowd framework to effortlessly perform subjective quality assessment with crowdsourcing. QualityCrowd allows codec independent quality assessment with a simple web interface, usable with common web browsers. We compared the results from an online subjective test using this framework with the results from a test in a standardized environment. This comparison shows that QualityCrowd delivers equivalent results within the acceptable inter-lab correlation. While we only consider video quality in this contribution, QualityCrowd can also be used for multimodal quality assessment.},
	urldate = {2019-06-01},
	author = {Keimel, Christian and Habigt, Julian and Horch, Clemens and Diepold, Klaus},
	year = {2012},
	file = {PDF:/Users/rosscutler/Zotero/storage/EHGUZLEH/Keimel et al. - 2012 - QualityCrowd-A Framework for Crowd-based Quality Evaluation.pdf:},
}

@techreport{gardlo_crowdsourcing_nodate,
	title = {Crowdsourcing 2.0: {Enhancing} {Execution} {Speed} and {Reliability} of {Web}-based {QoE} {Testing}},
	abstract = {Since its introduction a few years ago, the concept of 'Crowdsourcing' has been heralded as highly attractive alternative approach towards evaluating the Quality of Experience (QoE) of networked multimedia services. The main reason is that, in comparison to traditional laboratory-based subjective quality testing, crowd-based QoE assessment over the Internet promises to be not only much more cost-effective (no lab facilities required, less cost per subject) but also much faster in terms of shorter campaign setup and turnaround times. However, the reliability of remote test subjects and consequently , the trustworthiness of study results is still an issue that prevents the widespread adoption of crowd-based QoE testing. Various ideas for improving user rating reliability and test efficiency have been proposed, with the majority of them relying on a posteriori analysis of results. However, such methods introduce a major lag that significantly affects efficiency of campaign execution. In this paper we address these shortcomings by introducing in momento methods for crowdsourced video QoE assessment which yield improvements of results reliability by factor two and campaign execution efficiency by factor ten. The proposed in momento methods are applicable to existing crowd-based QoE testing approaches and suitable for a variety of service scenarios.},
	urldate = {2019-06-01},
	author = {Gardlo, Bruno and Egger, Sebastian and Seufert, Michael and Schatz, Raimund},
	keywords = {Crowdsourcing, Index Terms: Quality of Experience, Quality Evaluation, Reliability verification},
	file = {PDF:/Users/rosscutler/Zotero/storage/JPLFEXY2/Gardlo et al. - Unknown - Crowdsourcing 2.0 Enhancing Execution Speed and Reliability of Web-based QoE Testing.pdf:},
}

@article{egger-lampl_crowdsourcing_nodate,
	title = {Crowdsourcing {Quality} of {Experience} {Experiments}},
	url = {https://dx.doi.org/10.1007/978-3-319-66435-4_7},
	doi = {10.1007/978-3-319-66435-4_7},
	urldate = {2019-06-01},
	author = {Egger-Lampl, Sebastian and Redi, Judith and Hoßfeld, Tobias and Hirth, Matthias and Möller, Sebastian and Naderi, Babak and Keimel, Christian and Saupe, Dietmar},
	note = {ISBN: 9783319664347},
	file = {PDF:/Users/rosscutler/Zotero/storage/I3E4UCUN/Egger-Lampl et al. - Unknown - Crowdsourcing Quality of Experience Experiments.pdf:application/pdf},
}

@techreport{naderi_crowdee:_2014,
	title = {Crowdee: {Mobile} {Crowdsourcing} {Micro}-task {Platform} for {Celebrating} the {Diversity} of {Languages}},
	url = {http://www.piwik.org},
	abstract = {This paper introduces a novel crowdsourcing platform provided to the community. The platform operates on mobile devices and makes data generation and labeling scenarios available for many related research tracks potentially covering also small and underrepresented languages. Besides the versatile ways for commencing studies using the platform, also active research on crowdsourcing itself becomes feasible. With special focus on speech-and video recordings, the mobility and scalability of the platform is expected to stimulate and foster data-driven studies and insights throughout the community.},
	urldate = {2019-06-01},
	author = {Naderi, Babak and Polzehl, Tim and Beyer, André and Pilz, Tibor and Möller, Sebastian},
	year = {2014},
	keywords = {crowdsourcing, field tests, Index Terms: mobile phone app, labeling, recording, scalable studies, tools},
	file = {PDF:/Users/rosscutler/Zotero/storage/88F567DE/Naderi et al. - 2014 - Crowdee Mobile Crowdsourcing Micro-task Platform for Celebrating the Diversity of Languages.pdf:application/pdf},
}

@article{gadiraju_improving_2017,
	title = {Improving {Reliability} of {Crowdsourced} {Results} by {Detecting} {Crowd} {Workers} with {Multiple} {Identities}},
	url = {http://afel-project.eu/},
	doi = {10.1007/978-3-319-60131-1_11},
	abstract = {Quality control in crowdsourcing marketplaces plays a vital role in ensuring useful outcomes. In this paper, we focus on tackling the issue of crowd workers participating in tasks multiple times using different worker-ids to maximize their earnings. Workers attempting to complete the same task repeatedly may not be harmful in cases where the aim of a requester is to gather data or annotations, wherein more contributions from a single worker are fruitful. However, in several cases where the outcomes are subjective, requesters prefer the participation of distinct crowd workers. We show that traditional means to identify unique crowd workers such as worker-ids and ip-addresses are not sufficient. To overcome this problem, we propose the use of browser fingerprinting in order to ascertain the unique identities of crowd workers in paid crowdsourcing microtasks. By using browser fingerprinting across 8 different crowdsourced tasks with varying task difficulty, we found that 6.18\% of crowd workers participate in the same task more than once, using different worker-ids to avoid detection. Moreover, nearly 95\% of such workers in our experiments pass gold-standard questions and are deemed to be trustworthy, significantly biasing the results thus produced.},
	urldate = {2019-06-01},
	author = {Gadiraju, Ujwal and Kawase, Ricardo},
	year = {2017},
	keywords = {Crowdsourcing, Microtasks, Multiple Identities, Quality Control, Reliability},
	file = {PDF:/Users/rosscutler/Zotero/storage/DMK36KIN/Gadiraju, Kawase - 2017 - Improving Reliability of Crowdsourced Results by Detecting Crowd Workers with Multiple Identities.pdf:application/pdf},
}

@techreport{jillings_web_nodate,
	title = {{WEB} {AUDIO} {EVALUATION} {TOOL}: {A} {BROWSER}-{BASED} {LISTENING} {TEST} {ENVIRONMENT}},
	url = {https://developer.mozilla.org/en-US/docs/Web/HTML/},
	abstract = {Perceptual evaluation tests where subjects assess certain qualities of different audio fragments are an integral part of audio and music research. These require specialised software, usually custom-made, to collect large amounts of data using meticulously designed interfaces with carefully formulated questions, and play back audio with rapid switching between different samples. New functionality in HTML5 included in the Web Audio API allows for increasingly powerful media applications in a platform independent environment. The advantage of a web application is easy deployment on any platform, without requiring any other application, enabling multiple tests to be easily conducted across locations. In this paper we propose a tool supporting a wide variety of easily configurable, multi-stimulus perceptual audio evaluation tests over the web with multiple test interfaces, pre-and post-test surveys , custom configuration, collection of test metrics and other features. Test design and setup doesn't require programming background, and results are gathered automatically using web friendly formats for easy storing of results on a server.},
	urldate = {2019-06-02},
	author = {Jillings, Nicholas and De Man, Brecht and Moffat djmoffat, David and Reiss, Joshua D},
	file = {PDF:/Users/rosscutler/Zotero/storage/YUVIVV2E/Jillings et al. - Unknown - WEB AUDIO EVALUATION TOOL A BROWSER-BASED LISTENING TEST ENVIRONMENT.pdf:application/pdf},
}

@article{wu_crowdsourcing_2013,
	title = {Crowdsourcing multimedia {QoE} evaluation: {A} trusted framework},
	volume = {15},
	issn = {15209210},
	doi = {10.1109/TMM.2013.2241043},
	abstract = {Crowdsourcing has emerged in recent years as a potential strategy to enlist the general public to solve a wide variety of tasks. With the advent of ubiquitous Internet access, it is now feasible to ask an Internet crowd to conduct QoE (Quality of Experience) experiments on their personal computers in their own residences rather than in a laboratory. The considerable size of the Internet crowd allows researchers to crowdsource their experiments to a more diverse set of participant pool at a relatively low economic cost. However, as participants carry out experiments without supervision, the uncertainty of the quality of their experiment results is a challenging problem. In this paper, we propose a crowdsourceable framework to quantify the QoE of multimedia content. To overcome the aforementioned quality problem, we employ a paired comparison method in our framework. The advantages of our framework are: 1) trustworthiness due to the support for cheat detection; 2) a simpler rating procedure than that of the commonly-used but more difficult mean opinion score (MOS), which places less burden on participants; 3) economic feasibility since reliable QoE measures can be acquired with less effort compared with MOS; and 4) generalizability across a variety of multimedia content. We demonstrate the effectiveness and efficiency of the proposed framework by a comparison with MOS. Moreover, the results of four case studies support our assertion that the framework can provide reliable QoE evaluation at a lower cost.},
	number = {5},
	journal = {IEEE Transactions on Multimedia},
	author = {Wu, Chen Chi and Chen, Kuan Ta and Chang, Yu Chun and Lei, Chin Laung},
	year = {2013},
	keywords = {Quality of experience, Crowdsourcing, Mean opinion score, Paired comparison, Probabilistic choice model, Subjective test},
	pages = {1121--1137},
	file = {PDF:/Users/rosscutler/Zotero/storage/LLD9Z8G3/Wu et al. - 2013 - Crowdsourcing multimedia QoE evaluation A trusted framework.pdf:application/pdf},
}

@techreport{wang_jnd-based_nodate,
	title = {A {JND}-based {Video} {Quality} {Assessment} {Model} and {Its} {Application}},
	abstract = {Based on the Just-Noticeable-Difference (JND) criterion, a subjective video quality assessment (VQA) dataset, called the VideoSet, was constructed recently. In this work, we propose a JND-based VQA model using a probabilistic framework to analyze and clean collected subjective test data. While most traditional VQA models focus on content variability, our proposed VQA model takes both subject and content variabilities into account. The model parameters used to describe subject and content variabilities are jointly optimized by solving a maximum likelihood estimation (MLE) problem. As an application, the new subjective VQA model is used to filter out unreliable video quality scores collected in the VideoSet. Experiments are conducted to demonstrate the effectiveness of the proposed model.},
	urldate = {2019-05-18},
	author = {Wang, Haiqiang and Zhang, Xinfeng and Yang, Chao and Kuo, C.-C Jay},
	note = {arXiv: 1807.00920v1},
	keywords = {Index Terms Video Quality Assessment, Just Noticeable Difference, Subjective Viewing Model},
	file = {PDF:/Users/rosscutler/Zotero/storage/DK74FEY5/Wang et al. - Unknown - A JND-based Video Quality Assessment Model and Its Application.pdf:},
}

@techreport{streijl_multimedia_nodate,
	title = {Multimedia {Systems} {Mean} {Opinion} {Score} ({MOS}) revisited: {Methods} and applications, limitations and alternatives},
	abstract = {Mean Opinion Score (MOS) has become a very popular indicator of perceived media quality. While there is a clear benefit to such a "reference quality indicator" and its widespread acceptance, MOS is often applied without sufficient consideration of its scope or limitations. In this paper, we critically examine MOS and the various ways it is being used today. We highlight common issues with both subjective and objective MOS and discuss a variety of alternative approaches that have been proposed for media quality measurement.},
	urldate = {2019-05-19},
	author = {Streijl, R C and Winkler, S and Hands, D S and Streijl, Robert C and Winkler, Stefan and Hands, David S},
	keywords = {Customer opinion ·, Fault isolation ·, Objective MOS ·, Psychophysics ·, Quality monitoring ·, Service level agreements, Subjective MOS ·},
	file = {PDF:/Users/rosscutler/Zotero/storage/54NJADXZ/Streijl et al. - Unknown - Multimedia Systems Mean Opinion Score (MOS) revisited Methods and applications, limitations and alternatives.pdf:},
}

@article{hossfeld_survey_2014,
	title = {Survey of {Web}-based {Crowdsourcing} {Frameworks} for {Subjective} {Quality} {Assessment}},
	url = {http://www.crowdsource.com},
	doi = {10.1109/MMSP.2014.6958831},
	abstract = {The popularity of the crowdsourcing for performing various tasks online increased significantly in the past few years. The low cost and flexibility of crowdsourcing, in particular, attracted researchers in the field of subjective multimedia evaluations and Quality of Experience (QoE). Since online assessment of multimedia content is challenging, several dedicated frameworks were created to aid in the designing of the tests, including the support of the testing methodologies like ACR, DCR, and PC, setting up the tasks, training sessions, screening of the subjects, and storage of the resulted data. In this paper, we focus on the web-based frameworks for multimedia quality assessments that support commonly used crowdsourcing platforms such as Amazon Mechanical Turk and Microworkers. We provide a detailed overview of the crowdsourcing frameworks and evaluate them to aid researchers in the field of QoE assessment in the selection of frameworks and crowdsourcing platforms that are adequate for their experiments.},
	urldate = {2019-06-01},
	author = {Hossfeld, Tobias and Hirth, Matthias and Timmerer, Christian and Hoßfeld, Tobias and Korshunov, Pavel and Hanhart, Philippe and Gardlo, Bruno and Keimel, Christian},
	year = {2014},
	note = {ISBN: 9781479958962},
	file = {PDF:/Users/rosscutler/Zotero/storage/3U3DTQWI/Hossfeld et al. - 2014 - Survey of Web-based Crowdsourcing Frameworks for Subjective Quality Assessment.pdf:application/pdf},
}

@book{rainer_web_nodate,
	title = {A {WEB} {BASED} {SUBJECTIVE} {EVALUATION} {PLATFORM}},
	isbn = {978-1-4799-0738-0},
	url = {http://selab.itec.aau.at.},
	abstract = {Preparing and conducting subjective quality assessments is a time consuming and expensive task. Therefore, we present a Web-based evaluation framework which aims on reducing the time needed for planning and designing a subjective quality assessment. The presented framework can be used for both crowdsourced and laboratory experiments. It should ease the task of designing a subjective quality assessment by providing a flexible framework. The framework has proven its applicability and flexibility to design and conduct assessments in the past and is available as open source.},
	urldate = {2019-06-01},
	author = {Rainer, Benjamin and Waltl, Markus and Timmerer, Christian},
	keywords = {Crowdsourced Quality Evaluation, Index Terms-Evaluation Platform, Laboratory Quality Evaluation, Quality Assessment Framework},
	file = {PDF:/Users/rosscutler/Zotero/storage/JU3EYLLU/Rainer, Waltl, Timmerer - Unknown - A WEB BASED SUBJECTIVE EVALUATION PLATFORM.pdf:application/pdf},
}

@techreport{kraft_beaqlejs:_nodate,
	title = {{BeaqleJS}: {HTML5} and {JavaScript} based {Framework} for the {Subjective} {Evaluation} of {Audio} {Quality}},
	url = {https://github.com/audiocogs/aurora.js},
	abstract = {Subjective listening tests are an essential tool for the evaluation and comparison of audio processing algorithms. In this paper we introduce BeaqleJS, a framework based on HTML5 and JavaScript to run listening tests in any modern web browser. This allows an easy distribution of the test environment to a significant amount of participants in combination with simple configuration and good expandability.},
	urldate = {2019-06-01},
	author = {Kraft, Sebastian and Zölzer, Udo},
	keywords = {HTML5, JavaScript, listening test, subjective audio evaluation},
	file = {PDF:/Users/rosscutler/Zotero/storage/XDC3TXF4/Kraft, Zölzer - Unknown - BeaqleJS HTML5 and JavaScript based Framework for the Subjective Evaluation of Audio Quality.pdf:application/pdf},
}

@article{vuong_versustool_2018,
	title = {Versus—{A} tool for evaluating visualizations and image quality using a {2AFC} methodology},
	volume = {2},
	issn = {2468502X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2468502X18300433},
	doi = {10.1016/j.visinf.2018.12.003},
	number = {4},
	urldate = {2019-06-01},
	journal = {Visual Informatics},
	author = {Vuong, Jenny and Kaur, Sandeep and Heinrich, Julian and Ho, Bosco K. and Hammang, Christopher J. and Baldi, Benedetta F. and O’Donoghue, Seán I.},
	month = dec,
	year = {2018},
	pages = {225--234},
	file = {PDF:/Users/rosscutler/Zotero/storage/N4FNNW7T/Vuong et al. - 2018 - Versus—A tool for evaluating visualizations and image quality using a 2AFC methodology.pdf:application/pdf},
}

@article{schoeffler_webmushra_2018,
	title = {{webMUSHRA} — {A} {Comprehensive} {Framework} for {Web}-based {Listening} {Tests}},
	volume = {6},
	doi = {10.5334/jors.187},
	abstract = {For a long time, many popular listening test methods, such as ITU-R BS.1534 (MUSHRA), could not be carried out as web-based listening tests, since established web standards did not support all required audio processing features. With the standardization of the Web Audio API, the required features became available and, therefore, also the possibility to implement a wide range of established methods as web-based listening tests. In order to simplify the implementation of MUSHRA listening tests, the development of webMUSHRA was started. By utilizing webMUSHRA, experimenters can configure web-based MUSHRA listening tests without the need of web programming expertise. Today, webMUSHRA supports many more listening test methods, such as ITU-R BS.1116 and forced-choice procedures. Moreover, webMUSHRA is highly customizable and has been used in many auditory studies for different purposes.},
	journal = {Journal of Open Research Software},
	author = {Schoeffler, Michael and Bartoschek, Sarah and Stöter, Fabian-Robert and Roess, Marlene and Westphal, Susanne and Edler, Bernd and Herre, Jürgen},
	month = feb,
	year = {2018},
	note = {Publisher: Ubiquity Press, Ltd.},
	file = {PDF:/Users/rosscutler/Zotero/storage/VE6KB39N/Schoeffler et al. - 2018 - webMUSHRA — A Comprehensive Framework for Web-based Listening Tests.pdf:application/pdf},
}

@techreport{hosfeld_best_2013,
	title = {Best {Practices} for {QoE} {Crowdtesting}: {QoE} {Assessment} with {Crowdsourcing}},
	abstract = {Quality of Experience (QoE) in multimedia applications is closely linked to the end users' perception and therefore its assessment requires subjective user studies in order to evaluate the degree of delight or annoyance as experienced by the users. QoE crowdtesting refers to QoE assessment using crowdsourcing, where anonymous test subjects conduct subjective tests remotely in their preferred environment. The advantages of QoE crowdtesting lie not only in the reduced time and costs for the tests, but also in a large and diverse panel of international, geographically distributed users in realistic user settings. However, conceptual and technical challenges emerge due to the remote test settings. Key issues arising from QoE crowdtesting include the reliability of user ratings, the influence of incentives, payment schemes and the unknown environmental context of the tests on the results. In order to counter these issues, strategies and methods need to be developed, included in the test design, and also implemented in the actual test campaign, while statistical methods are required to identify reliable user ratings and to ensure high data quality. This contribution provides a collection of best practices addressing these issues based on our experience gained in a large set of conducted QoE crowdtesting studies. The focus of this article is in particular on the issue of reliability and we use video quality assessment as an example for the proposed best practices, showing that our recommended two-stage QoE crowdtesting design leads to more reliable results.},
	urldate = {2019-06-10},
	author = {Hoßfeld, Tobias and Keimel, Christian and Hirth, Matthias and Gardlo, Bruno and Habigt, Julian and Diepold, Klaus and Tran-Gia, Phuoc},
	year = {2013},
	file = {PDF:/Users/rosscutler/Zotero/storage/DZNHY3PY/Hoßfeld et al. - 2013 - Best Practices for QoE Crowdtesting QoE Assessment with Crowdsourcing.pdf:},
}

@book{freitas_video_nodate,
	title = {Video {Quality} {Ruler}: {A} {New} {Experimental} {Methodology} for {Assessing} {Video} {Quality}},
	isbn = {978-1-4799-8958-4},
	abstract = {In this paper, we propose a subjective video quality assessment method called video quality ruler (VQR) that can be employed to determine the perceived quality of video sequences. The described method is an extension of the ISO 20462, which is a method to assess image quality. The VQR method provides an interface with a set of pictures. The subjects assess the video using these pictures as a scale and compare the subjective perceived video quality with their perceived quality. The pictures are calibrated to form a numerical scale in units of just noticeable differences (JNDs), which allows to analyze and compare both subjective video and image stimuli. To evaluate the effectiveness of the proposed method, we compare the VQR method with a well-used single stimulus (SS) method. The results show that proposed method can be used to quantify the overall video quality with higher efficiency and with a less biased results than the SS method.},
	urldate = {2019-06-14},
	author = {Freitas, Pedro Garcia and Redi, Judith A and Farias, Mylène C Q and Silva, Alexandre F},
	file = {PDF:/Users/rosscutler/Zotero/storage/T4FVEFK9/Freitas et al. - Unknown - Video Quality Ruler A New Experimental Methodology for Assessing Video Quality.pdf:},
}

@techreport{guse_thefragebogen:_nodate,
	title = {{TheFragebogen}: {A} {Web} {Browser}-based {Questionnaire} {Framework} for {Scientific} {Research} 2019 {Eleventh} {International} {Conference} on {Quality} of {Multimedia} {Experience} ({QoMEX})},
	url = {http://thefragebogen.de/releases},
	abstract = {Quality of Experience (QoE) typically involves conducting experiments in which stimuli are presented to participants and their judgments as well as behavioral data are collected. Nowadays, many experiments require software for the presentation of stimuli and the data collection from participants. While different software solutions exist, these are not tailored to conduct experiments on QoE. Moreover, replicating experiments or repeating the same experiment in different settings (e. g., laboratory vs. crowdsourcing) can further increase the software complexity. TheFragebogen is an open-source, versatile, extendable software framework for the implementation of questionnaires-especially for research on QoE. Implemented questionnaires can be presented with a state-of-the-art web browser to support a broad range of devices while the use of a web server being optional. Out-of-the-box, TheFragebogen provides graphical exact scales as well as free-hand input, the ability to collect behavioral data, and playback multimedia content.},
	urldate = {2019-07-10},
	author = {Guse, Dennis and Orefice, Henrique R and Reimers, Gabriel and Hohlfeld, Oliver},
	keywords = {data collection, experiments, Index Terms-survey software},
	file = {PDF:/Users/rosscutler/Zotero/storage/ZKYWDGXT/Guse et al. - Unknown - TheFragebogen A Web Browser-based Questionnaire Framework for Scientific Research 2019 Eleventh International Co.pdf:application/pdf},
}

@misc{noauthor_autosys.pdf_nodate,
	title = {autosys.pdf - {OneDrive}},
	urldate = {2019-03-13},
}

@techreport{gebru_audio-visual_nodate,
	title = {Audio-{Visual} {Speaker} {Diarization} {Based} on {Spatiotemporal} {Bayesian} {Fusion}},
	url = {https://team.inria.fr/perception/avdiarization/},
	abstract = {Speaker diarization consists of assigning speech signals to people engaged in a dialogue. An audiovisual spatiotem-poral diarization model is proposed. The model is well suited for challenging scenarios that consist of several participants engaged in multi-party interaction while they move around and turn their heads towards the other participants rather than facing the cameras and the microphones. Multiple-person visual tracking is combined with multiple speech-source localization in order to tackle the speech-to-person association problem. The latter is solved within a novel audiovisual fusion method on the following grounds: binaural spectral features are first extracted from a microphone pair, then a supervised audiovisual alignment technique maps these features onto an image, and finally a semi-supervised clustering method assigns binaural spectral features to visible persons. The main advantage of this method over previous work is that it processes in a principled way speech signals uttered simultaneously by multiple persons. The diarization itself is cast into a latent-variable temporal graphical model that infers speaker identities and speech turns, based on the output of an audiovisual association process, executed at each time slice, and on the dynamics of the diarization variable itself. The proposed formulation yields an efficient exact inference procedure. A novel dataset, that contains audiovisual training data as well as a number of scenarios involving several participants engaged in formal and informal dialogue, is introduced. The proposed method is thoroughly tested and benchmarked with respect to several state-of-the art diarization algorithms.},
	urldate = {2019-05-01},
	author = {Gebru, Israel D and Eye Ba, Siì and Li, Xiaofei and Horaud, Radu},
	note = {arXiv: 1603.09725v2
ISBN: 1603.09725v2},
	keywords = {audio-visual tracking, dy-namic Bayesian network, Index Terms-speaker diarization, sound source localization},
	file = {PDF:/Users/rosscutler/Zotero/storage/9B7FAFHC/Gebru et al. - Unknown - Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion.pdf:application/pdf},
}

@article{cabanas-molero_multimodal_2018,
	title = {Multimodal speaker diarization for meetings using volume-evaluated {SRP}-{PHAT} and video analysis},
	volume = {77},
	issn = {1380-7501},
	url = {http://link.springer.com/10.1007/s11042-018-5944-2},
	doi = {10.1007/s11042-018-5944-2},
	number = {20},
	urldate = {2019-05-01},
	journal = {Multimedia Tools and Applications},
	author = {Cabañas-Molero, P. and Lucena, M. and Fuertes, J. M. and Vera-Candeas, P. and Ruiz-Reyes, N.},
	month = oct,
	year = {2018},
	note = {Publisher: Springer US},
	pages = {27685--27707},
}

@techreport{anguera_speaker_2010,
	title = {Speaker diarization : {A} review of recent research {Speaker} {Diarization}: {A} {Review} of {Recent} {Research}},
	url = {https://hal.archives-ouvertes.fr/hal-00733397},
	abstract = {Speaker diarization is the task of determining "who spoke when?" in an audio or video recording that contains an unknown amount of speech and also an unknown number of speakers. Initially, it was proposed as a research topic related to automatic speech recognition, where speaker diarization serves as an upstream processing step. Over recent years, however, speaker diarization has become an important key technology for many tasks, such as navigation, retrieval, or higher-level inference on audio data. Accordingly, many important improvements in accuracy and robustness have been reported in journals and conferences in the area. The application domains, from broadcast news, to lectures and meetings, vary greatly and pose different problems, such as having access to multiple microphones and multimodal information or overlapping speech. The most recent review of existing technology dates back to 2006 and focuses on the broadcast news domain. In this paper we review the current state-of-the-art, focusing on research developed since 2006 that relates predominantly to speaker diarization for conference meetings. Finally, we present an analysis of speaker diarization performance as reported through the NIST Rich Transcription evaluations on meeting data and identify important areas for future research.},
	urldate = {2019-05-01},
	author = {Anguera, Xavier and Bozonnet, Simon and Evans, Nicholas and Fredouille, Corinne and Friedland, Gerald and Vinyals, Oriol},
	year = {2010},
	keywords = {()},
	file = {PDF:/Users/rosscutler/Zotero/storage/PJX2I3V2/Anguera et al. - 2010 - Speaker diarization A review of recent research Speaker Diarization A Review of Recent Research.pdf:application/pdf},
}

@techreport{ma_image_nodate,
	title = {Image and {Video} {Compression} with {Neural} {Networks}: {A} {Review}},
	abstract = {In recent years, the image and video coding technologies have advanced by leaps and bounds. However, due to the popularization of image and video acquisition devices, the growth rate of image and video data is far beyond the improvement of the compression ratio. In particular, it has been widely recognized that there are increasing challenges of pursuing further coding performance improvement within the traditional hybrid coding framework. Deep convolution neural network (CNN) which makes the neural network resurge in recent years and has achieved great success in both artificial intelligent and signal processing fields, also provides a novel and promising solution for image and video compression. In this paper, we provide a systematic, comprehensive and up-to-date review of neural network based image and video compression techniques. The evolution and development of neural network based compression methodologies are introduced for images and video respectively. More specifically, the cutting-edge video coding techniques by leveraging deep learning and HEVC framework are presented and discussed, which promote the state-of-the-art video coding performance substantially. Moreover, the end-to-end image and video coding frameworks based on neural networks are also reviewed, revealing interesting explorations on next generation image and video coding frameworks/standards. The most significant research works on the image and video coding related topics using neural networks are highlighted, and future trends are also envisioned. In particular, the joint compression on semantic and visual information is tentatively explored to formulate high efficiency signal representation structure for both human vision and machine vision, which are the two dominant signal receptor in the age of artificial intelligence.},
	urldate = {2019-05-24},
	author = {Ma, Siwei and Zhang, Xinfeng and Jia, Chuanmin and Zhao, Zhenghui and Wang, Shiqi and Wang, Shanshe},
	note = {arXiv: 1904.03567v2},
	keywords = {deep learning, CNN, image compression, Index Terms-Neural network, video coding},
	file = {PDF:/Users/rosscutler/Zotero/storage/VY5VZ67K/Ma et al. - Unknown - Image and Video Compression with Neural Networks A Review.pdf:},
}

@article{noauthor_08662024_nodate,
	title = {08662024},
	file = {PDF:/Users/rosscutler/Zotero/storage/UHH7BU4E/Unknown - Unknown - 08662024.pdf:application/pdf},
}

@book{ieee_international_conference_on_acoustics_2018_nodate,
	title = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing} proceedings : {April} 15-20, 2018, {Calgary} {Telus} {Convention} {Center}, {Calgary}, {Alberta}, {Canada}},
	isbn = {978-1-5386-4658-8},
	abstract = {"The theme of this year's ICASSP conference is: Signal processing and artificial intelligence: changing the world." - Vorwort (Seite xxxii) Literaturangaben},
	author = {IEEE International Conference on Acoustics, Speech and {Institute of Electrical and Electronics Engineers} and {IEEE Signal Processing Society} and IEEE International Conference on Acoustics, Speech and {ICASSP 2018.04.15-20 Calgary}},
	keywords = {deep learning, Index Terms-statistical speech enhancement, recurrent networks, speech recogni-tion},
	file = {PDF:/Users/rosscutler/Zotero/storage/SNB3UJRG/Tu et al. - Unknown - A hybrid approach to combining conventional and deep learning techniques for single-channel speech enhancement and.pdf:application/pdf;PDF:/Users/rosscutler/Zotero/storage/JAST3RZ6/IEEE International Conference on Acoustics et al. - Unknown - 2018 IEEE International Conference on Acoustics, Speech, and Signal Proces.pdf:application/pdf},
}

@inproceedings{fan_auc_2019,
	address = {Brighton, United Kingdom},
	title = {{AUC} {Optimization} for {Deep} {Learning} {Based} {Voice} {Activity} {Detection}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8682803/},
	doi = {10.1109/ICASSP.2019.8682803},
	abstract = {Voice activity detection (VAD) based on deep neural networks (DNN) has demonstrated good performance in adverse acoustic environments. Current DNN based VAD optimizes a surrogate function, e.g. minimum cross-entropy or minimum squared error, at a given decision threshold. However, VAD usually works on-the-ﬂy with a dynamic decision threshold; and ROC curve is a global evaluation metric of VAD that reﬂects the performance of VAD at all possible decision thresholds. In this paper, we propose to optimize the area under ROC curve (AUC) by DNN, which can maximize the performance of VAD in terms of the ROC curve. Experimental results show that optimizing AUC by DNN results in higher performance than the common method of optimizing the minimum squared error by DNN.},
	language = {en},
	urldate = {2019-07-12},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Fan, Zi-Chen and Bai, Zhongxin and Zhang, Xiao-Lei and Rahardja, Susanto and Chen, Jingdong},
	month = may,
	year = {2019},
	pages = {6760--6764},
	file = {Fan et al. - 2019 - AUC Optimization for Deep Learning Based Voice Act.pdf:/Users/rosscutler/Zotero/storage/IT5MIRB9/Fan et al. - 2019 - AUC Optimization for Deep Learning Based Voice Act.pdf:application/pdf;PDF:/Users/rosscutler/Zotero/storage/JUDZZI3X/Fan et al. - 2019 - AUC Optimization for Deep Learning Based Voice Activity Detection.pdf:application/pdf},
}

@inproceedings{handigol_reproducible_2012,
	address = {Nice, France},
	title = {Reproducible network experiments using container-based emulation},
	isbn = {978-1-4503-1775-7},
	url = {http://dl.acm.org/citation.cfm?doid=2413176.2413206},
	doi = {10.1145/2413176.2413206},
	abstract = {In an ideal world, all research papers would be runnable: simply click to replicate all results, using the same setup as the authors. One approach to enable runnable network systems papers is Container-Based Emulation (CBE), where an environment of virtual hosts, switches, and links runs on a modern multicore server, using real application and kernel code with software-emulated network elements. CBE combines many of the best features of software simulators and hardware testbeds, but its performance ﬁdelity is unproven.},
	language = {en},
	urldate = {2019-06-27},
	booktitle = {Proceedings of the 8th international conference on {Emerging} networking experiments and technologies - {CoNEXT} '12},
	publisher = {ACM Press},
	author = {Handigol, Nikhil and Heller, Brandon and Jeyakumar, Vimalkumar and Lantz, Bob and McKeown, Nick},
	year = {2012},
	pages = {253},
	file = {Handigol et al. - 2012 - Reproducible network experiments using container-b.pdf:/Users/rosscutler/Zotero/storage/ZD98Q588/Handigol et al. - 2012 - Reproducible network experiments using container-b.pdf:application/pdf},
}

@inproceedings{hosfeld_unexpected_2015,
	title = {The unexpected {QoE} killer: {When} the network emulator misshapes traffic and {QoE}},
	shorttitle = {The unexpected {QoE} killer},
	doi = {10.1109/QoMEX.2015.7148093},
	abstract = {Packet streams observed in today's networks show a high degree of autocorrelation as successive packets tend to exhibit similar delay and loss characteristics. Network emulators used to conduct performance evaluations therefore have to support the generation of such autocorrelated streams. Typically, this is done using first order autoregressive processes. In this paper, we show that a common implementation of this approach leads to problems when emulating correlated packet loss, and therefore possibly produces erroneous results and conclusions when evaluating objectively application quality or QoE in subjective studies. Further typical network emulator implementations make even low packet jitter become an unexpected QoE killer.},
	booktitle = {2015 {Seventh} {International} {Workshop} on {Quality} of {Multimedia} {Experience} ({QoMEX})},
	author = {Hoßfeld, T. and Fiedler, M.},
	month = may,
	year = {2015},
	keywords = {telecommunication traffic, Delays, Packet loss, quality of service, multimedia communication, QoE, quality of experience, Correlation, autoregressive process, autoregressive processes, computer network performance evaluation, Emulation, Jitter, multimedia system, network emulator, network performance evaluation, packet loss emulation, packet stream, QoS, Quality of service, traffic pattern},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/HT7M3XF9/7148093.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/2D94UA85/Hoßfeld and Fiedler - 2015 - The unexpected QoE killer When the network emulat.pdf:application/pdf},
}

@article{nussbaum_comparative_nodate,
	title = {A {Comparative} {Study} of {Network} {Link} {Emulators}},
	abstract = {Between discrete event simulation and evaluation within real networks, network emulation is a useful tool to study and evaluate the behaviour of applications. Using a real network as a basis to simulate another network’s characteristics, it enables researchers to perform experiments in a wide range of conditions. After an overview of the various available network emulators, this paper focuses on three freely available and widely used network link emulators: Dummynet, NISTNet, and the Linux Trafﬁc Control subsystem. We start by comparing their features, then focus on the accuracy of their latency and bandwidth emulation, and discuss the way they are affected by the time source of the system. We expose several problems that cannot be ignored when using such tools. We also outline differences in their user interfaces, such as the interception point, and discuss possible solutions. This work aims at providing a complete overview of the different solutions for network emulation.},
	language = {en},
	author = {Nussbaum, Lucas and Richard, Olivier},
	pages = {9},
	file = {Nussbaum and Richard - A Comparative Study of Network Link Emulators.pdf:/Users/rosscutler/Zotero/storage/ZAWX79W3/Nussbaum and Richard - A Comparative Study of Network Link Emulators.pdf:application/pdf},
}

@article{jay_deep_2018,
	title = {A {Deep} {Reinforcement} {Learning} {Perspective} on {Internet} {Congestion} {Control}},
	url = {http://arxiv.org/abs/1810.03259},
	abstract = {We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating trafﬁc sources’ data-transmission rates to efﬁciently utilize network capacity, and is the subject of extensive attention in light of the advent of Internet services such as live video, virtual reality, Internet-of-Things, and more. We show that casting congestion control as RL enables training deep network policies that capture intricate patterns in data trafﬁc and network conditions, and leverage this to outperform the state-of-the-art. We also highlight signiﬁcant challenges facing real-world adoption of RL-based congestion control, including fairness, safety, and generalization, which are not trivial to address within conventional RL formalism. To facilitate further research and reproducibility of our results, we present a test suite for RL-guided congestion control based on the OpenAI Gym interface.},
	language = {en},
	urldate = {2019-06-24},
	journal = {arXiv:1810.03259 [cs]},
	author = {Jay, Nathan and Rotman, Noga H. and Godfrey, P. Brighten and Schapira, Michael and Tamar, Aviv},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.03259},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {Jay et al. - 2018 - A Deep Reinforcement Learning Perspective on Inter.pdf:/Users/rosscutler/Zotero/storage/W79V5UX6/Jay et al. - 2018 - A Deep Reinforcement Learning Perspective on Inter.pdf:application/pdf},
}

@article{yan_continual_2019-1,
	title = {Continual learning improves {Internet} video streaming},
	url = {http://arxiv.org/abs/1906.01113},
	abstract = {We describe Fugu, a continual learning algorithm for bitrate selection in streaming video. Each day, Fugu retrains a neural network from its experience in deployment over the prior week. The neural network predicts how long it would take to transfer each available version of the upcoming video chunks, given recent history and internal TCP statistics. We evaluate Fugu with Puffer, a public website we built that streams live TV using Fugu and existing algorithms. Over a nine-day period in January 2019, Puffer streamed 8,131 hours of video to 3,719 unique users. Compared with buffer-based control, MPC, RobustMPC, and Pensieve, Fugu performs better on several metrics: it stalls 5-13x less and has better and more stable video quality, and users who were randomly assigned to Fugu streamed for longer on average before quitting or reloading. We find that TCP statistics can aid bitrate selection, that congestion-control and bitrate-selection algorithms are best selected jointly, that probabilistic transfer-time estimates that consider chunk size outperform point estimates of throughput, and that both stalls and video quality have an influence on how long users choose to keep a video stream playing. Fugu's results suggest that continual learning of network algorithms in situ is a promising area of research. To support further investigation, we plan to operate Puffer for several years and will open it to researchers for evaluating new congestion-control and bitrate-selection approaches.},
	language = {en},
	urldate = {2019-06-11},
	journal = {arXiv:1906.01113 [cs]},
	author = {Yan, Francis Y. and Ayers, Hudson and Zhu, Chenzhi and Fouladi, Sadjad and Hong, James and Zhang, Keyi and Levis, Philip and Winstein, Keith},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.01113},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {Yan et al. - 2019 - Continual learning improves Internet video streami.pdf:/Users/rosscutler/Zotero/storage/96V46AXW/Yan et al. - 2019 - Continual learning improves Internet video streami.pdf:application/pdf},
}

@article{bhattacharyya_qflow:_2019,
	title = {{QFlow}: {A} {Reinforcement} {Learning} {Approach} to {High} {QoE} {Video} {Streaming} over {Wireless} {Networks}},
	shorttitle = {{QFlow}},
	url = {http://arxiv.org/abs/1901.00959},
	abstract = {Wireless Internet access has brought legions of heterogeneous applications all sharing the same resources. However, current wireless edge networks that cater to worst or average case performance lack the agility to best serve these diverse sessions. Simultaneously, software reconfigurable infrastructure has become increasingly mainstream to the point that dynamic per packet and per flow decisions are possible at multiple layers of the communications stack. Exploiting such reconfigurability requires the design of a system that can enable a configuration, measure the impact on the application performance (Quality of Experience), and adaptively select a new configuration. Effectively, this feedback loop is a Markov Decision Process whose parameters are unknown. The goal of this work is to design, develop and demonstrate QFlow that instantiates this feedback loop as an application of reinforcement learning (RL). Our context is that of reconfigurable (priority) queueing, and we use the popular application of video streaming as our use case. We develop both model-free and model-based RL approaches that are tailored to the problem of determining which clients should be assigned to which queue at each decision period. Through experimental validation, we show how the RL-based control policies on QFlow are able to schedule the right clients for prioritization in a high-load scenario to outperform the status quo, as well as the best known solutions with over 25\% improvement in QoE, and a perfect QoE score of 5 over 85\% of the time.},
	urldate = {2019-06-11},
	journal = {arXiv:1901.00959 [cs, eess, stat]},
	author = {Bhattacharyya, Rajarshi and Bura, Archana and Rengarajan, Desik and Rumuly, Mason and Shakkottai, Srinivas and Kalathil, Dileep and Mok, Ricky K. P. and Dhamdhere, Amogh},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.00959},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv\:1901.00959 PDF:/Users/rosscutler/Zotero/storage/6DAINFDG/Bhattacharyya et al. - 2019 - QFlow A Reinforcement Learning Approach to High Q.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/T56B3IBD/1901.html:text/html},
}

@article{jiang_pytheas:_nodate,
	title = {Pytheas: {Enabling} {Data}-{Driven} {Quality} of {Experience} {Optimization} {Using} {Group}-{Based} {Exploration}-{Exploitation}},
	abstract = {Content providers are increasingly using data-driven mechanisms to optimize quality of experience (QoE). Many existing approaches formulate this process as a prediction problem of learning optimal decisions (e.g., server, bitrate, relay) based on observed QoE of recent sessions. While prediction-based mechanisms have shown promising QoE improvements, they are necessarily incomplete as they: (1) suffer from many known biases (e.g., incomplete visibility) and (2) cannot respond to sudden changes (e.g., load changes). Drawing a parallel from machine learning, we argue that data-driven QoE optimization should instead be cast as a real-time exploration and exploitation (E2) process rather than as a prediction problem. Adopting E2 in network applications, however, introduces key architectural (e.g., how to update decisions in real time with fresh data) and algorithmic (e.g., capturing complex interactions between session features vs. QoE) challenges. We present Pytheas, a framework which addresses these challenges using a group-based E2 mechanism. The insight is that application sessions sharing the same features (e.g., IP preﬁx, location) can be grouped so that we can run E2 algorithms at a per-group granularity. This naturally captures the complex interactions and is amenable to realtime control with fresh measurements. Using an endto-end implementation and a proof-of-concept deployment in CloudLab, we show that Pytheas improves video QoE over a state-of-the-art prediction-based system by up to 31\% on average and 78\% on 90th percentile of persession QoE.},
	language = {en},
	author = {Jiang, Junchen and Sun, Shijie and Sekar, Vyas and Zhang, Hui},
	pages = {15},
	file = {Jiang et al. - Pytheas Enabling Data-Driven Quality of Experienc.pdf:/Users/rosscutler/Zotero/storage/HAAZQ25H/Jiang et al. - Pytheas Enabling Data-Driven Quality of Experienc.pdf:application/pdf},
}

@inproceedings{zheng_demystifying_2018,
	address = {Beijing, China},
	title = {Demystifying {Deep} {Learning} in {Networking}},
	isbn = {978-1-4503-6395-2},
	url = {http://dl.acm.org/citation.cfm?doid=3232565.3232569},
	doi = {10.1145/3232565.3232569},
	abstract = {We are witnessing a surge of efforts in networking community to develop deep neural networks (DNNs) based approaches to networking problems. Most results so far have been remarkably promising, which is arguably surprising given how intensively these problems have been studied before. Despite these promises, there has not been much systematic work to understand the inner workings of these DNNs trained in networking settings, their generalizability in different workloads, and their potential synergy with domain-specific knowledge. The problem of model opacity would eventually impede the adoption of DNN-based solutions in practice. This position paper marks the first attempt to shed light on the interpretability of DNNs used in networking problems. Inspired by recent research in ML towards interpretable ML models, we call upon this community to similarly develop techniques and leverage domain-specific insights to demystify the DNNs trained in networking settings, and ultimately unleash the potential of DNNs in an explainable and reliable way.},
	language = {en},
	urldate = {2019-06-11},
	booktitle = {Proceedings of the 2nd {Asia}-{Pacific} {Workshop} on {Networking}  - {APNet} '18},
	publisher = {ACM Press},
	author = {Zheng, Ying and Liu, Ziyu and You, Xinyu and Xu, Yuedong and Jiang, Junchen},
	year = {2018},
	pages = {1--7},
	file = {Zheng et al. - 2018 - Demystifying Deep Learning in Networking.pdf:/Users/rosscutler/Zotero/storage/M2KMNWQ5/Zheng et al. - 2018 - Demystifying Deep Learning in Networking.pdf:application/pdf},
}

@inproceedings{flach_internet-wide_2016,
	address = {Florianopolis, Brazil},
	title = {An {Internet}-{Wide} {Analysis} of {Traffic} {Policing}},
	isbn = {978-1-4503-4193-6},
	url = {http://dl.acm.org/citation.cfm?doid=2934872.2934873},
	doi = {10.1145/2934872.2934873},
	abstract = {Large ﬂows like video streams consume signiﬁcant bandwidth. Some ISPs actively manage these high volume ﬂows with techniques like policing, which enforces a ﬂow rate by dropping excess trafﬁc. While the existence of policing is well known, our contribution is an Internet-wide study quantifying its prevalence and impact on transportlevel and video-quality metrics. We developed a heuristic to identify policing from server-side traces and built a pipeline to process traces at scale collected from hundreds of Google servers worldwide. Using a dataset of 270 billion packets served to 28,400 client ASes, we ﬁnd that, depending on region, up to 7\% of connections are identiﬁed to be policed. Loss rates are on average 6× higher when a trace is policed, and it impacts video playback quality. We show that alternatives to policing, like pacing and shaping, can achieve trafﬁc management goals while avoiding the deleterious effects of policing.},
	language = {en},
	urldate = {2019-04-12},
	booktitle = {Proceedings of the 2016 conference on {ACM} {SIGCOMM} 2016 {Conference} - {SIGCOMM} '16},
	publisher = {ACM Press},
	author = {Flach, Tobias and Papageorge, Pavlos and Terzis, Andreas and Pedrosa, Luis and Cheng, Yuchung and Karim, Tayeb and Katz-Bassett, Ethan and Govindan, Ramesh},
	year = {2016},
	pages = {468--482},
	file = {Flach et al. - 2016 - An Internet-Wide Analysis of Traffic Policing.pdf:/Users/rosscutler/Zotero/storage/RTLXRGFV/Flach et al. - 2016 - An Internet-Wide Analysis of Traffic Policing.pdf:application/pdf},
}

@article{huang_qarc:_2018,
	title = {{QARC}: {Video} {Quality} {Aware} {Rate} {Control} for {Real}-{Time} {Video} {Streaming} via {Deep} {Reinforcement} {Learning}},
	shorttitle = {{QARC}},
	url = {http://arxiv.org/abs/1805.02482},
	doi = {10.1145/3240508.3240545},
	abstract = {Real-time video streaming is now one of the main applications in all network environments. Due to the fluctuation of throughput under various network conditions, how to choose a proper bitrate adaptively has become an upcoming and interesting issue. To tackle this problem, most proposed rate control methods work for providing high video bitrates instead of video qualities. Nevertheless, we notice that there exists a trade-off between sending bitrate and video quality, which motivates us to focus on how to reach a balance between them.},
	language = {en},
	urldate = {2019-04-10},
	journal = {2018 ACM Multimedia Conference on Multimedia Conference  - MM '18},
	author = {Huang, Tianchi and Zhang, Rui-Xiao and Zhou, Chao and Sun, Lifeng},
	year = {2018},
	note = {arXiv: 1805.02482},
	keywords = {Computer Science - Multimedia},
	pages = {1208--1216},
	file = {Huang et al. - 2018 - QARC Video Quality Aware Rate Control for Real-Ti.pdf:/Users/rosscutler/Zotero/storage/PWP8P48J/Huang et al. - 2018 - QARC Video Quality Aware Rate Control for Real-Ti.pdf:application/pdf},
}

@article{gawlowicz_ns3-gym:_2018,
	title = {ns3-gym: {Extending} {OpenAI} {Gym} for {Networking} {Research}},
	shorttitle = {ns3-gym},
	url = {http://arxiv.org/abs/1810.03943},
	abstract = {OpenAI Gym is a toolkit for reinforcement learning (RL) research. It includes a large number of well-known problems that expose a common interface allowing to directly compare the performance results of diﬀerent RL algorithms. Since many years, the ns–3 network simulation tool is the de–facto standard for academic and industry research into networking protocols and communications technology. Numerous scientiﬁc papers were written reporting results obtained using ns–3, and hundreds of models and modules were written and contributed to the ns–3 code base. Today as a major trend in network research we see the use of machine learning tools like RL. What is missing is the integration of a RL framework like OpenAI Gym into the network simulator ns-3. This paper presents the ns3-gym framework. First, we discuss design decisions that went into the software. Second, two illustrative examples implemented using ns3-gym are presented. Our software package is provided to the community as open source under a GPL license and hence can be easily extended.},
	language = {en},
	urldate = {2019-04-08},
	journal = {CoRR},
	author = {Gawłowicz, Piotr and Zubow, Anatolij},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.03943},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {Gawłowicz and Zubow - 2018 - ns3-gym Extending OpenAI Gym for Networking Resea.pdf:/Users/rosscutler/Zotero/storage/C3J9B8RQ/Gawłowicz and Zubow - 2018 - ns3-gym Extending OpenAI Gym for Networking Resea.pdf:application/pdf},
}

@inproceedings{johansson_self-clocked_2014,
	address = {Chicago, Illinois, USA},
	title = {Self-clocked rate adaptation for conversational video in {LTE}},
	isbn = {978-1-4503-2991-0},
	url = {http://dl.acm.org/citation.cfm?doid=2630088.2631976},
	doi = {10.1145/2630088.2631976},
	abstract = {This paper describes a rate adaptation framework for conversational video services. The solution conforms to the packet conservation principle and uses a hybrid loss and delay based congestion control algorithm. The framework is evaluated over both simulated bottleneck scenarios as well as in a LTE system simulator and is shown to achieve both low latency and high video throughput in these scenarios, something that improves the end user experience.},
	language = {en},
	urldate = {2019-04-06},
	booktitle = {Proceedings of the 2014 {ACM} {SIGCOMM} workshop on {Capacity} sharing workshop - {CSWS} '14},
	publisher = {ACM Press},
	author = {Johansson, Ingemar},
	year = {2014},
	pages = {51--56},
	file = {Johansson - 2014 - Self-clocked rate adaptation for conversational vi.pdf:/Users/rosscutler/Zotero/storage/S6MEZUTQ/Johansson - 2014 - Self-clocked rate adaptation for conversational vi.pdf:application/pdf},
}

@inproceedings{carlucci_analysis_2016,
	address = {Klagenfurt, Austria},
	title = {Analysis and design of the google congestion control for web real-time communication ({WebRTC})},
	isbn = {978-1-4503-4297-1},
	url = {http://dl.acm.org/citation.cfm?doid=2910017.2910605},
	doi = {10.1145/2910017.2910605},
	abstract = {Video conferencing applications require low latency and high bandwidth. Standard TCP is not suitable for video conferencing since its reliability and in order delivery mechanisms induce large latency. Recently the idea of using the delay gradient to infer congestion is appearing again and is gaining momentum. In this paper we present an algorithm that is based on estimating through a Kalman ﬁlter the end-to-end one way delay variation which is experienced by packets traveling from a sender to a destination. This estimate is compared to an adaptive threshold to dynamically throttle the sending rate. The control algorithm has been implemented over the RTP/RTCP protocol and is currently used in Google Hangouts and in the Chrome WebRTC stack. Experiments have been carried out to evaluate the algorithm performance in the case of variable link capacity, presence of heterogeneous or homogeneous concurrent trafﬁc, and backward path trafﬁc.},
	language = {en},
	urldate = {2019-04-06},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Multimedia} {Systems} - {MMSys} '16},
	publisher = {ACM Press},
	author = {Carlucci, Gaetano and De Cicco, Luca and Holmer, Stefan and Mascolo, Saverio},
	year = {2016},
	pages = {1--12},
	file = {Carlucci et al. - 2016 - Analysis and design of the google congestion contr.pdf:/Users/rosscutler/Zotero/storage/UKCPXNK5/Carlucci et al. - 2016 - Analysis and design of the google congestion contr.pdf:application/pdf},
}

@inproceedings{akhtar_oboe:_2018,
	address = {Budapest, Hungary},
	title = {Oboe: auto-tuning video {ABR} algorithms to network conditions},
	isbn = {978-1-4503-5567-4},
	shorttitle = {Oboe},
	url = {http://dl.acm.org/citation.cfm?doid=3230543.3230558},
	doi = {10.1145/3230543.3230558},
	abstract = {Most content providers are interested in providing good video delivery QoE for all users, not just on average. State-of-the-art ABR algorithms like BOLA and MPC rely on parameters that are sensitive to network conditions, so may perform poorly for some users and/or videos. In this paper, we propose a technique called Oboe to auto-tune these parameters to different network conditions. Oboe pre-computes, for a given ABR algorithm, the best possible parameters for different network conditions, then dynamically adapts the parameters at run-time for the current network conditions. Using testbed experiments, we show that Oboe significantly improves BOLA, MPC, and a commercially deployed ABR. Oboe also betters a recently proposed reinforcement learning based ABR, Pensieve, by 24\% on average on a composite QoE metric, in part because it is able to better specialize ABR behavior across different network states.},
	language = {en},
	urldate = {2019-03-27},
	booktitle = {Proceedings of the 2018 {Conference} of the {ACM} {Special} {Interest} {Group} on {Data} {Communication}  - {SIGCOMM} '18},
	publisher = {ACM Press},
	author = {Akhtar, Zahaib and Nam, Yun Seong and Govindan, Ramesh and Rao, Sanjay and Chen, Jessica and Katz-Bassett, Ethan and Ribeiro, Bruno and Zhan, Jibin and Zhang, Hui},
	year = {2018},
	pages = {44--58},
	file = {Akhtar et al. - 2018 - Oboe auto-tuning video ABR algorithms to network .pdf:/Users/rosscutler/Zotero/storage/27PUP9AE/Akhtar et al. - 2018 - Oboe auto-tuning video ABR algorithms to network .pdf:application/pdf},
}

@article{yeo_neural_nodate,
	title = {Neural {Adaptive} {Content}-aware {Internet} {Video} {Delivery}},
	abstract = {Internet video streaming has experienced tremendous growth over the last few decades. However, the quality of existing video delivery critically depends on the bandwidth resource. Consequently, user quality of experience (QoE) suffers inevitably when network conditions become unfavorable. We present a new video delivery framework that utilizes client computation and recent advances in deep neural networks (DNNs) to reduce the dependency for delivering high-quality video. The use of DNNs enables us to enhance the video quality independent to the available bandwidth. We design a practical system that addresses several challenges, such as client heterogeneity, interaction with bitrate adaptation, and DNN transfer, in enabling the idea. Our evaluation using 3G and broadband network traces shows the proposed system outperforms the current state of the art, enhancing the average QoE by 43.08\% using the same bandwidth budget or saving 17.13\% of bandwidth while providing the same user QoE.},
	language = {en},
	author = {Yeo, Hyunho and Jung, Youngmok and Kim, Jaehong and Shin, Jinwoo and Han, Dongsu},
	pages = {19},
	file = {Yeo et al. - Neural Adaptive Content-aware Internet Video Deliv.pdf:/Users/rosscutler/Zotero/storage/L4X566BV/Yeo et al. - Neural Adaptive Content-aware Internet Video Deliv.pdf:application/pdf},
}

@inproceedings{hagos_recurrent_2018,
	title = {Recurrent {Neural} {Network}-{Based} {Prediction} of {TCP} {Transmission} {States} from {Passive} {Measurements}},
	doi = {10.1109/NCA.2018.8548064},
	abstract = {Long Short-Term Memory (LSTM) neural networks are a state-of-the-art techniques when it comes to sequence learning and time series prediction models. In this paper, we have used LSTM-based Recurrent Neural Networks (RNN) for building a generic prediction model for Transmission Control Protocol (TCP) connection characteristics from passive measurements. To the best of our knowledge, this is the first work that attempts to apply LSTM for demonstrating how a network operator can identify the most important system-wide TCP per-connection states of a TCP client that determine a network condition (e.g., cwnd) from passive traffic measured at an intermediate node of the network without having access to the sender. We found out that LSTM learners outperform the state-of-the-art classical machine learning prediction models. Through an extensive experimental evaluation on multiple scenarios, we demonstrate the scalability and robustness of our approach and its potential for monitoring TCP transmission states related to network congestion from passive measurements. Our results based on emulated and realistic settings suggest that Deep Learning is a promising tool for monitoring system-wide TCP states from passive measurements and we believe that the methodology presented in our paper may strengthen future research work in the computer networking community.},
	booktitle = {2018 {IEEE} 17th {International} {Symposium} on {Network} {Computing} and {Applications} ({NCA})},
	author = {Hagos, D. H. and Engelstad, P. E. and Yazidi, A. and Kure, Ø},
	month = nov,
	year = {2018},
	keywords = {learning (artificial intelligence), telecommunication congestion control, telecommunication traffic, transport protocols, Predictive models, recurrent neural nets, Computer architecture, computer networks, Logic gates, Long Short-Term Memory, Long Short-Term Memory neural networks, LSTM-based Recurrent Neural Networks, Machine learning, machine learning prediction models, Microprocessors, Monitoring, network congestion, Passive Measurement, Recurrent Neural network-based prediction, Recurrent neural networks, Recurrent Neural Networks, sequence learning, system-wide TCP states, TCP Congestion Control, TCP transmission states, TCP Transmission states, time series prediction models, Transmission Control Protocol connection},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/K4VLM4MV/8548064.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/JDCT669Q/Hagos et al. - 2018 - Recurrent Neural Network-Based Prediction of TCP T.pdf:application/pdf},
}

@article{zhang_deep_2018,
	title = {Deep {Learning} in {Mobile} and {Wireless} {Networking}: {A} {Survey}},
	shorttitle = {Deep {Learning} in {Mobile} and {Wireless} {Networking}},
	url = {http://arxiv.org/abs/1803.04311},
	abstract = {The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile trafﬁc volumes, real-time extraction of ﬁne-grained analytics, and agile management of network resources, so as to maximize user experience. Fulﬁlling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques, in order to help manage the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space.},
	language = {en},
	urldate = {2019-03-08},
	journal = {arXiv:1803.04311 [cs]},
	author = {Zhang, Chaoyun and Patras, Paul and Haddadi, Hamed},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.04311},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, To Read},
	file = {Zhang et al. - 2018 - Deep Learning in Mobile and Wireless Networking A.pdf:/Users/rosscutler/Zotero/storage/WFC6EK2S/Zhang et al. - 2018 - Deep Learning in Mobile and Wireless Networking A.pdf:application/pdf},
}

@article{jay_internet_2018,
	title = {Internet {Congestion} {Control} via {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1810.03259},
	abstract = {We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating trafﬁc sources’ data-transmission rates so as to efﬁciently utilize network capacity. Congestion control is fundamental to computer networking research and practice, and has recently been the subject of extensive attention in light of the advent of Internet services such as live video, augmented and virtual reality, Internet-of-Things, and more.},
	language = {en},
	urldate = {2019-01-14},
	journal = {arXiv:1810.03259 [cs]},
	author = {Jay, Nathan and Rotman, Noga H. and Godfrey, P. Brighten and Schapira, Michael and Tamar, Aviv},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.03259},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {arXiv\:1810.03259 PDF:/Users/rosscutler/Zotero/storage/7DZDY6JB/Jay et al. - 2018 - Internet Congestion Control via Deep Reinforcement.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/5YRXNWJR/1810.html:text/html;Jay et al. - 2018 - Internet Congestion Control via Deep Reinforcement.pdf:/Users/rosscutler/Zotero/storage/X6ERV7DE/Jay et al. - 2018 - Internet Congestion Control via Deep Reinforcement.pdf:application/pdf;Jay et al. - 2018 - Internet Congestion Control via Deep Reinforcement.pdf:/Users/rosscutler/Zotero/storage/VPX2VLT3/Jay et al. - 2018 - Internet Congestion Control via Deep Reinforcement.pdf:application/pdf;Jay et al. - 2018 - Internet Congestion Control via Deep Reinforcement.pdf:/Users/rosscutler/Zotero/storage/A4IRF8VA/Jay et al. - 2018 - Internet Congestion Control via Deep Reinforcement.pdf:application/pdf},
}

@inproceedings{mao_neural_2017,
	address = {Los Angeles, CA, USA},
	title = {Neural {Adaptive} {Video} {Streaming} with {Pensieve}},
	isbn = {978-1-4503-4653-5},
	url = {http://dl.acm.org/citation.cfm?doid=3098822.3098843},
	doi = {10.1145/3098822.3098843},
	abstract = {Client-side video players employ adaptive bitrate (ABR) algorithms to optimize user quality of experience (QoE). Despite the abundance of recently proposed schemes, state-of-the-art ABR algorithms suffer from a key limitation: they use ﬁxed control rules based on simpliﬁed or inaccurate models of the deployment environment. As a result, existing schemes inevitably fail to achieve optimal performance across a broad set of network conditions and QoE objectives. We propose Pensieve, a system that generates ABR algorithms using reinforcement learning (RL). Pensieve trains a neural network model that selects bitrates for future video chunks based on observations collected by client video players. Pensieve does not rely on pre-programmed models or assumptions about the environment. Instead, it learns to make ABR decisions solely through observations of the resulting performance of past decisions. As a result, Pensieve automatically learns ABR algorithms that adapt to a wide range of environments and QoE metrics. We compare Pensieve to state-of-theart ABR algorithms using trace-driven and real world experiments spanning a wide variety of network conditions, QoE metrics, and video properties. In all considered scenarios, Pensieve outperforms the best state-of-the-art scheme, with improvements in average QoE of 12\%–25\%. Pensieve also generalizes well, outperforming existing schemes even on networks for which it was not explicitly trained.},
	language = {en},
	urldate = {2019-01-09},
	booktitle = {Proceedings of the {Conference} of the {ACM} {Special} {Interest} {Group} on {Data} {Communication}  - {SIGCOMM} '17},
	publisher = {ACM Press},
	author = {Mao, Hongzi and Netravali, Ravi and Alizadeh, Mohammad},
	year = {2017},
	pages = {197--210},
	file = {Mao et al. - 2017 - Neural Adaptive Video Streaming with Pensieve.pdf:/Users/rosscutler/Zotero/storage/5HKMD4D5/Mao et al. - 2017 - Neural Adaptive Video Streaming with Pensieve.pdf:application/pdf},
}

@inproceedings{mao_resource_2016,
	address = {Atlanta, GA, USA},
	title = {Resource {Management} with {Deep} {Reinforcement} {Learning}},
	isbn = {978-1-4503-4661-0},
	url = {http://dl.acm.org/citation.cfm?doid=3005745.3005750},
	doi = {10.1145/3005745.3005750},
	abstract = {Resource management problems in systems and networking often manifest as difﬁcult online decision making tasks where appropriate solutions depend on understanding the workload and environment. Inspired by recent advances in deep reinforcement learning for AI problems, we consider building systems that learn to manage resources directly from experience. We present DeepRM, an example solution that translates the problem of packing tasks with multiple resource demands into a learning problem. Our initial results show that DeepRM performs comparably to state-ofthe-art heuristics, adapts to different conditions, converges quickly, and learns strategies that are sensible in hindsight.},
	language = {en},
	urldate = {2019-01-09},
	booktitle = {Proceedings of the 15th {ACM} {Workshop} on {Hot} {Topics} in {Networks}  - {HotNets} '16},
	publisher = {ACM Press},
	author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
	year = {2016},
	pages = {50--56},
	file = {Mao et al. - 2016 - Resource Management with Deep Reinforcement Learni.pdf:/Users/rosscutler/Zotero/storage/WUSY3HYY/Mao et al. - 2016 - Resource Management with Deep Reinforcement Learni.pdf:application/pdf},
}

@inproceedings{fouladi_salsify:_2018,
	title = {Salsify: {Low}-{Latency} {Network} {Video} through {Tighter} {Integration} between a {Video} {Codec} and a {Transport} {Protocol}},
	shorttitle = {Salsify},
	url = {https://www.usenix.org/conference/nsdi18/presentation/fouladi},
	language = {en},
	urldate = {2019-01-09},
	author = {Fouladi, Sadjad and Emmons, John and Orbay, Emre and Wu, Catherine and Wahby, Riad S. and Winstein, Keith},
	year = {2018},
	pages = {267--282},
	file = {Full Text PDF:/Users/rosscutler/Zotero/storage/BA38ICSI/Fouladi et al. - 2018 - Salsify Low-Latency Network Video through Tighter.pdf:application/pdf;Snapshot:/Users/rosscutler/Zotero/storage/5UQWBGUK/fouladi.html:text/html},
}

@article{winstein_tcp_nodate,
	title = {{TCP} ex {Machina}: {Computer}-{Generated} {Congestion} {Control}},
	abstract = {This paper describes a new approach to end-to-end congestion control on a multi-user network. Rather than manually formulate each endpoint’s reaction to congestion signals, as in traditional protocols, we developed a program called Remy that generates congestioncontrol algorithms to run at the endpoints.},
	language = {en},
	author = {Winstein, Keith and Balakrishnan, Hari},
	pages = {12},
	file = {Winstein and Balakrishnan - TCP ex Machina Computer-Generated Congestion Cont.pdf:/Users/rosscutler/Zotero/storage/WWEVECDY/Winstein and Balakrishnan - TCP ex Machina Computer-Generated Congestion Cont.pdf:application/pdf},
}

@inproceedings{chen_auto:_2018,
	address = {Budapest, Hungary},
	title = {{AuTO}: scaling deep reinforcement learning for datacenter-scale automatic traffic optimization},
	isbn = {978-1-4503-5567-4},
	shorttitle = {{AuTO}},
	url = {http://dl.acm.org/citation.cfm?doid=3230543.3230551},
	doi = {10.1145/3230543.3230551},
	abstract = {Current DRL systems’ performance is not enough to make online decisions for datacenter-scale tra c. They su er from long processing delays even for simple algorithms and low tra c load.},
	language = {en},
	urldate = {2019-01-09},
	booktitle = {Proceedings of the 2018 {Conference} of the {ACM} {Special} {Interest} {Group} on {Data} {Communication}  - {SIGCOMM} '18},
	publisher = {ACM Press},
	author = {Chen, Li and Lingys, Justinas and Chen, Kai and Liu, Feng},
	year = {2018},
	pages = {191--205},
	file = {Chen et al. - 2018 - AuTO scaling deep reinforcement learning for data.pdf:/Users/rosscutler/Zotero/storage/USIWPW98/Chen et al. - 2018 - AuTO scaling deep reinforcement learning for data.pdf:application/pdf},
}

@article{joskowicz_towards_2013,
	title = {Towards a {General} {Parametric} {Model} for {Perceptual} {Video} {Quality} {Estimation}},
	volume = {59},
	issn = {1557-9611},
	doi = {10.1109/TBC.2013.2277951},
	abstract = {During the last few years, different parametric models were proposed for video quality estimation. Each model uses different parameters as inputs, such as bit rate, frame rate and percentage of packet loss, and each model was designed and tested by their authors for a particular codec, display resolution and/or application. This paper presents a review of the parametric models published by ten different groups of authors. Each model is briefly described, and the relevant parametric formulas are presented. The performance of each model is evaluated and contrasted to the other models, using a common video clips set, in different coding and transmission scenarios. Based on the results, a new and more general parametric model is presented, which takes into account bit rate, frame rate, display resolution, video content and the percentage of packet loss.},
	number = {4},
	journal = {IEEE Transactions on Broadcasting},
	author = {Joskowicz, José and Sotelo, Rafael and Ardao, J. Carlos Lopez},
	month = dec,
	year = {2013},
	keywords = {Mathematical model, Packet loss, Bit rate, video signal processing, Quality assessment, account bit rate, common video clips set, display resolution, frame rate, general parametric model, Integrated circuit modeling, perceptual video quality estimation, video content, Video perceptual quality, video quality parametric models, Video recording},
	pages = {569--579},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/QWV7WR4V/6587314.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/23G84EE2/Joskowicz et al. - 2013 - Towards a General Parametric Model for Perceptual .pdf:application/pdf},
}

@inproceedings{abbeel_using_2006,
	address = {Pittsburgh, Pennsylvania},
	title = {Using inaccurate models in reinforcement learning},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143845},
	doi = {10.1145/1143844.1143845},
	abstract = {In the model-based policy search approach to reinforcement learning (RL), policies are found using a model (or “simulator”) of the Markov decision process. However, for highdimensional continuous-state tasks, it can be extremely diﬃcult to build an accurate model, and thus often the algorithm returns a policy that works in simulation but not in real-life. The other extreme, model-free RL, tends to require infeasibly large numbers of real-life trials. In this paper, we present a hybrid algorithm that requires only an approximate model, and only a small number of real-life trials. The key idea is to successively “ground” the policy evaluations using real-life trials, but to rely on the approximate model to suggest local changes. Our theoretical results show that this algorithm achieves near-optimal performance in the real system, even when the model is only approximate. Empirical results also demonstrate that—when given only a crude model and a small number of real-life trials—our algorithm can obtain near-optimal performance in the real system.},
	language = {en},
	urldate = {2020-05-08},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning  - {ICML} '06},
	publisher = {ACM Press},
	author = {Abbeel, Pieter and Quigley, Morgan and Ng, Andrew Y.},
	year = {2006},
	pages = {1--8},
	file = {icml06-usinginaccuratemodelsinrl.pdf:/Users/rosscutler/Zotero/storage/WBEIPQ37/icml06-usinginaccuratemodelsinrl.pdf:application/pdf},
}

@inproceedings{xia_weighted_2020,
	address = {Barcelona, Spain},
	title = {Weighted {Speech} {Distortion} {Losses} for {Neural}-{Network}-{Based} {Real}-{Time} {Speech} {Enhancement}},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9054254/},
	doi = {10.1109/ICASSP40776.2020.9054254},
	urldate = {2020-04-19},
	booktitle = {{ICASSP}},
	publisher = {IEEE},
	author = {Xia, Yangyang and Braun, Sebastian and Reddy, Chandan K. A. and Dubey, Harishchandra and Cutler, Ross and Tashev, Ivan},
	month = may,
	year = {2020},
	pages = {871--875},
	file = {Submitted Version:/Users/rosscutler/Zotero/storage/4W7CCPG9/Xia et al. - 2020 - Weighted Speech Distortion Losses for Neural-Netwo.pdf:application/pdf},
}

@article{li_time-domain_2020,
	title = {A {Time}-domain {Monaural} {Speech} {Enhancement} with {Recursive} {Learning}},
	url = {http://arxiv.org/abs/2003.09815},
	abstract = {In this paper, we propose a type of neural network with recursive learning in the time domain called RTNet for monaural speech enhancement, where the proposed network consists of three principal components. The first part is called stage recurrent neural network, which is introduced to effectively aggregate the deep feature dependencies across different stages with a memory mechanism and also remove the interference stage by stage. The second part is the convolutional auto-encoder. The third part consists of a series of concatenated gated linear units, which are capable of facilitating the information flow and gradually increasing the receptive fields. Recursive learning is adopted to improve the parameter efficiency and therefore, the number of trainable parameters is effectively reduced without sacrificing its performance. Numerous experiments are conducted on TIMIT corpus and experimental results demonstrate that the proposed network can achieve consistently better performance in terms of both PESQ and STOI scores than two state-of-the-art time domain-based baselines in different conditions. The code is provided at https://github.com/ Andong-Li-speech/RTNet.},
	urldate = {2020-04-11},
	journal = {arXiv:2003.09815 [cs, eess]},
	author = {Li, Andong and Zheng, Chengshi and Cheng, Linjuan and Peng, Renhua and Li, Xiaodong},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.09815},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/442DUMME/Li et al. - 2020 - A Time-domain Monaural Speech Enhancement with Rec.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/JUZFJ63Q/2003.html:text/html},
}

@inproceedings{strake_fully_2020,
	title = {Fully {Convolutional} {Recurrent} {Networks} for {Speech} {Enhancement}},
	doi = {10.1109/ICASSP40776.2020.9054230},
	abstract = {Convolutional recurrent neural networks (CRNs) using convolutional encoder-decoder (CED) structures have shown promising performance for single-channel speech enhancement. These CRNs handle temporal modeling through integrating long short-term memory (LSTM) layers in between convolutional encoder and decoder. However, in such a CRN, the organization of internal representations in feature maps and the focus on local structure of the convolutional mappings has to be discarded for fully-connected LSTM processing. Furthermore, CRNs can be quite restricted concerning the feature space dimension at the input of the LSTM, which, through its fully-connected nature, requires a large amount of trainable parameters. As first novelty, we propose to replace the fully-connected LSTM by a convolutional LSTM (ConvLSTM) and call the resulting network a fully convolutional recurrent network (FCRN). Secondly, since the ConvLSTM retains the structured organization of its input feature maps, we can show that this helps to internally represent the harmonic structure of speech, allowing us to handle high-dimensional input features using less trainable parameters than an LSTM. The proposed FCRN clearly outperforms CRN reference models with similar amounts of trainable parameters in terms of PESQ, STOI, and segmental ∆SNR.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Strake, Maximilian and Defraene, Bruno and Fluyt, Kristoff and Tirry, Wouter and Fingscheidt, Tim},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Speech enhancement, convolutional LSTM, convolutional recurrent neural networks},
	pages = {6674--6678},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/CS84ZBB6/9054230.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/BCNZL5TB/Strake et al. - 2020 - Fully Convolutional Recurrent Networks for Speech .pdf:application/pdf},
}

@inproceedings{strake_separated_2019,
	title = {Separated {Noise} {Suppression} and {Speech} {Restoration}: {Lstm}-{Based} {Speech} {Enhancement} in {Two} {Stages}},
	shorttitle = {Separated {Noise} {Suppression} and {Speech} {Restoration}},
	doi = {10.1109/WASPAA.2019.8937222},
	abstract = {Regression based on neural networks (NNs) has led to considerable advances in speech enhancement under non-stationary noise conditions. Nonetheless, speech distortions can be introduced when employing NNs trained to provide strong noise suppression. We propose to address this problem by first suppressing noise and subsequently restoring speech with specifically chosen NN topologies for each of these distinct tasks. A mask-estimating long short-term memory (LSTM) network is employed for noise suppression, while the speech restoration is performed by a fully convo-lutional encoder-decoder (CED) network, where we introduce temporal modeling capabilities by using a convolutional LSTM layer in the bottleneck. We show considerable performance gains over reference methods of up to 0.26 MOS points (PESQ) and the ability to significantly improve intelligibility in terms of STOI for low-SNR conditions.},
	booktitle = {2019 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	author = {Strake, Maximilian and Defraene, Bruno and Fluyt, Kristoff and Tirry, Wouter and Fingscheidt, Tim},
	month = oct,
	year = {2019},
	note = {ISSN: 1947-1629},
	keywords = {regression analysis, speech enhancement, long short-term memory, convolutional codes, convolutional encoder-decoder, convolutional LSTM layer, fully convolutional encoder-decoder network, LSTM-based speech enhancement, mask-estimating long short-term memory network, neural networks, separated noise suppression, speech distortions, speech restoration, temporal modeling capabilities, two-stage processing},
	pages = {239--243},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/SP56PUB8/8937222.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/CTXN3XSV/Strake et al. - 2019 - Separated Noise Suppression and Speech Restoration.pdf:application/pdf},
}

@article{belmudez_call_2013,
	title = {Call {Quality} {Prediction} for {Audiovisual} {Time}-{Varying} {Impairments} {Using} {Simulated} {Conversational} {Structures}},
	volume = {99},
	issn = {16101928},
	url = {http://openurl.ingenta.com/content/xref?genre=article&issn=1610-1928&volume=99&issue=5&spage=792},
	doi = {10.3813/AAA.918657},
	abstract = {In this study we present an evaluation and improvement of time integration speech quality models applied to assess ﬂuctuating quality of audiovisual transmission. We ﬁrst introduce a subjective test methodology to evaluate the user perception of time-varying quality of 90 seconds long sequences that are organized in a simulated conversational structure. We conducted a two-fold user test where in the ﬁrst part, the quality of simulated video-telephony conversations was assessed. Audiovisual impairments were temporally distributed to follow predeﬁned quality proﬁles in this study. In the second part of the experiment, subjective ratings of short audiovisual samples (9 seconds) constituent of the simulated conversations are gathered. The results of both experiments show that the enddialog judgments are closely correlated to the plain average of the short samples. The modeling results for call quality models that predict the quality at the end of a (simulated) conversation are described. These models proved to enhance the prediction accuracy in comparison to the plain average, and an optimization of the model parameters further reﬁnes the correlation of the estimates with the subjective data. The optimized models also showed a higher correlation and a lower prediction error on independent test data.},
	language = {en},
	number = {5},
	urldate = {2020-03-04},
	journal = {Acta Acustica united with Acustica},
	author = {Belmudez, Benjamin and Lewcio, Blazej and Möller, Sebastian},
	month = sep,
	year = {2013},
	pages = {792--805},
	file = {Belmudez et al. - 2013 - Call Quality Prediction for Audiovisual Time-Varyi.pdf:/Users/rosscutler/Zotero/storage/ERW5BZQ8/Belmudez et al. - 2013 - Call Quality Prediction for Audiovisual Time-Varyi.pdf:application/pdf},
}

@article{weiss_modeling_2009,
	title = {Modeling {Call} {Quality} for {Time}-{Varying} {Transmission} {Characteristics} {Using} {Simulated} {Conversational} {Structures}},
	volume = {95},
	issn = {16101928},
	url = {http://openurl.ingenta.com/content/xref?genre=article&issn=1610-1928&volume=95&issue=6&spage=1140},
	doi = {10.3813/AAA.918245},
	abstract = {This study investigates the perception of speech quality over telephone channels with time-varying transmission characteristics for simulated conversational structures. The aim is to establish a relationship between subjective quality associated with short speech samples (5–6 seconds) and quality associated with overall conversations (1–2 minutes). Two two-part experiments were conducted. In the ﬁrst part of each experiment, dialog-ﬁnal ratings within the temporal structure of a telephone conversation were assessed. Varying transmission characteristics were realized with ten different degradation proﬁles of preprocessed speech samples obtained mainly from real mobile channels to ensure authentic types of degradation. The second part was carried out to obtain separate short-term ratings of the speech samples used in the ﬁrst part. Experiments 1 and 2 tested different conversation durations (1 and 2 minutes). The results demonstrate that dialog-ﬁnal ratings vary with respect to the degradation proﬁle, revealing a recency effect and a strong impact of individual bad samples. Two related models which implement these ﬁndings are presented. With these models, dialog-ﬁnal quality ratings can be estimated signiﬁcantly better than by plain averaging of short sample ratings (about 10\% absolute improvement). They also perform better than two algorithms taken from literature. Both models can be applied to the instrumental method described in ITU-T Rec. P.862 [1], resulting in about 13\% absolute improvement. They were evaluated with the results of two different experiments, which were performed independently but on the basis of our test procedure. In these experiments similar proﬁles but a different type of quality degradation, different sample durations, and different speech material were used. The models proved to be valid and reliable for the time span investigated (1–2 minutes) and for the proﬁles used. One of them is now being recommended by the ETSI STQ mobile group.},
	language = {en},
	number = {6},
	urldate = {2020-03-04},
	journal = {Acta Acustica united with Acustica},
	author = {Weiss, Benjamin and Möller, Sebastian and Raake, Alexander and Berger, Jens and Ullmann, Raphael},
	month = nov,
	year = {2009},
	pages = {1140--1151},
	file = {weiss_2009_modeling-call-quality-for-time.-.varying-transmission-characteristics-using-simulated-conversational-structures.pdf:/Users/rosscutler/Zotero/storage/W7N5W3QL/weiss_2009_modeling-call-quality-for-time.-.varying-transmission-characteristics-using-simulated-conversational-structures.pdf:application/pdf},
}

@article{krishnamoorthi_quantizing_2018,
	title = {Quantizing deep convolutional networks for efficient inference: {A} whitepaper},
	shorttitle = {Quantizing deep convolutional networks for efficient inference},
	url = {http://arxiv.org/abs/1806.08342},
	abstract = {We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2\% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. Quantization-aware training can provide further improvements, reducing the gap to floating point to 1\% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2\% to 10\%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.},
	language = {en},
	urldate = {2020-03-02},
	journal = {arXiv:1806.08342 [cs, stat]},
	author = {Krishnamoorthi, Raghuraman},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.08342},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Krishnamoorthi - 2018 - Quantizing deep convolutional networks for efficie.pdf:/Users/rosscutler/Zotero/storage/K85IBYAR/Krishnamoorthi - 2018 - Quantizing deep convolutional networks for efficie.pdf:application/pdf},
}

@inproceedings{cutler_look_2000,
	title = {Look {Who}’s {Talking}: {Speaker} {Detection} {Using} {Video} and {Audio} {Correlation}},
	shorttitle = {Look {Who}’s {Talking}},
	abstract = {The visual motion of the mouth and the corresponding audio data generated when a person speaks are highly correlated. This fact has been exploited for lip/speechreading and for improving speech recognition. We describe a method of automatically detecting a talking person (both spatially and temporally) using video and audio data from a single microphone. The audio-visual correlation is learned using a TDNN, which is then used to perform a spatio-temporal search for a speaking person. Applications include video conferencing, video indexing, and improving human computer interaction (HCI). An example HCI application is provided. 1.},
	author = {Cutler, Ross and Davis, Larry},
	year = {2000},
	file = {Citeseer - Full Text PDF:/Users/rosscutler/Zotero/storage/MS6HHLG5/Look Who’s Talking Speaker Detection Using Video .pdf:application/pdf;Citeseer - Snapshot:/Users/rosscutler/Zotero/storage/HHKXYVLN/summary.html:text/html},
}

@inproceedings{shahid_voice_2019,
	title = {Voice {Activity} {Detection} by {Upper} {Body} {Motion} {Analysis} and {Unsupervised} {Domain} {Adaptation}},
	abstract = {We present a novel vision-based voice activity detection (VAD) method that relies only on automatic upper body motion (UBM) analysis. Traditionally, VAD is performed using audio features only, but the use of visual cues instead of audio can be desirable especially when audio is not available such as due to technical, ethical or legal issues. Psychology literature conﬁrms that the way people move while speaking is different from while they are not speaking. This motivates us to claim that an effective representation of UBM can be used to detect “Who is Speaking and When”. On the other hand, the way people move during their speech varies a lot from culture to culture, and even person to person in the same culture. This results in unrelated UBM representations, such that the distribution of training and test data becomes disparate. To overcome this, we combine stacked sparse autoencoders and simple subspace alignment methods while a classiﬁer is jointly learned using the VAD labels of the training data only. This yields new domain invariant feature representations for training and test data, showing improved VAD results. Our approach is applicable to any person without requiring re-training. The tests applied on a publicly available real-life VAD dataset show better results as compared to the state-of-the-art video-only VAD methods. Moreover, the ablation study justiﬁes the superiority of the proposed method and demonstrates the positive contribution of each component.},
	language = {en},
	author = {Shahid, Muhammad and Beyan, Cigdem and Murino, Vittorio},
	year = {2019},
	file = {Shahid et al. - Voice Activity Detection by Upper Body Motion Anal.pdf:/Users/rosscutler/Zotero/storage/DW7GBCC2/Shahid et al. - Voice Activity Detection by Upper Body Motion Anal.pdf:application/pdf},
}

@inproceedings{sergi_smart_2005,
	title = {Smart {Room}: {Participant} {And} {Speaker} {Localization} {And} {Identification}},
	shorttitle = {Smart {Room}},
	abstract = {Our long-term objective is to create Smart Room Technologies that are aware of the users presence and their behavior and can become an active, but not an intrusive, part of the interaction. In this work, we present a multimodal approach for estimating and tracking the location and identity of the participants including the active speaker. Our smart room design contains three user-monitoring systems: four CCD cameras, an omnidirectional camera and a 16 channel microphone array. The various sensory modalities are processed both individually and jointly and it is shown that the multimodal approach results in significantly improved performance in spatial localization, identification and speech activity detection of the participants.},
	booktitle = {Proc. {IEEE} {Int}. {Conf}. on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Sergi, Carlos Busso and Hernanz, Sergi and Chu, Chi-wei and Kwon, Soon-il and Lee, Sung and Georgiou, Panayiotis G. and Cohen, Isaac and Narayanan, Shrikanth},
	year = {2005},
	file = {Citeseer - Full Text PDF:/Users/rosscutler/Zotero/storage/Z8L3NLEI/Sergi et al. - 2005 - Smart Room Participant And Speaker Localization A.pdf:application/pdf;Citeseer - Snapshot:/Users/rosscutler/Zotero/storage/L4YM4QEZ/summary.html:text/html},
}

@techreport{noauthor_itu-t_1996,
	title = {{ITU}-{T} {P}.800: {Methods} for subjective determination of transmission quality},
	year = {1996},
}

@article{pirker_pitch_nodate,
	title = {A {Pitch} {Tracking} {Corpus} with {Evaluation} on {Multipitch} {Tracking} {Scenario}},
	language = {en},
	author = {Pirker, Gregor and Wohlmayr, Michael and Petrik, Stefan and Pernkopf, Franz},
	pages = {4},
	file = {Pirker et al. - A Pitch Tracking Corpus with Evaluation on Multipi.pdf:/Users/rosscutler/Zotero/storage/HFMSWEY8/Pirker et al. - A Pitch Tracking Corpus with Evaluation on Multipi.pdf:application/pdf},
}

@inproceedings{thiemann_diverse_2013,
	address = {Montreal, Canada},
	title = {The {Diverse} {Environments} {Multi}-channel {Acoustic} {Noise} {Database} ({DEMAND}): {A} database of multichannel environmental noise recordings},
	shorttitle = {The {Diverse} {Environments} {Multi}-channel {Acoustic} {Noise} {Database} ({DEMAND})},
	url = {http://asa.scitation.org/doi/abs/10.1121/1.4799597},
	doi = {10.1121/1.4799597},
	language = {en},
	urldate = {2020-01-17},
	author = {Thiemann, Joachim and Ito, Nobutaka and Vincent, Emmanuel},
	year = {2013},
	pages = {035081--035081},
	file = {Thiemann et al. - 2013 - The Diverse Environments Multi-channel Acoustic No.pdf:/Users/rosscutler/Zotero/storage/AQT42DL8/Thiemann et al. - 2013 - The Diverse Environments Multi-channel Acoustic No.pdf:application/pdf},
}

@article{beerends_perceptual_2013,
	title = {Perceptual {Objective} {Listening} {Quality} {Assessment} ({POLQA}), {The} {Third} {Generation} {ITU}-{T} {Standard} for {End}-to-{End} {Speech} {Quality} {Measurement} {Part} {I}–{Temporal} {Alignment}},
	volume = {61},
	language = {en},
	number = {6},
	journal = {J. Audio Eng. Soc.},
	author = {Beerends, John G and Obermann, Matthias and Ullmann, Raphael and Pomy, Joachim and Keyhl, Michael},
	year = {2013},
	pages = {19},
	file = {Beerends et al. - 2013 - Perceptual Objective Listening Quality Assessment .pdf:/Users/rosscutler/Zotero/storage/IARRT9Y3/Beerends et al. - 2013 - Perceptual Objective Listening Quality Assessment .pdf:application/pdf},
}

@article{xu_regression_2015,
	title = {A {Regression} {Approach} to {Speech} {Enhancement} {Based} on {Deep} {Neural} {Networks}},
	volume = {23},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/6932438/},
	doi = {10.1109/TASLP.2014.2364452},
	number = {1},
	urldate = {2020-01-17},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Xu, Yong and Du, Jun and Dai, Li-Rong and Lee, Chin-Hui},
	month = jan,
	year = {2015},
	pages = {7--19},
}

@article{srinivasan_codebook-based_2007,
	title = {Codebook-{Based} {Bayesian} {Speech} {Enhancement} for {Nonstationary} {Environments}},
	volume = {15},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4067050/},
	doi = {10.1109/TASL.2006.881696},
	number = {2},
	urldate = {2020-01-17},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Srinivasan, Sriram and Samuelsson, Jonas and Kleijn, W. Bastiaan},
	month = feb,
	year = {2007},
	pages = {441--452},
	file = {Full Text:/Users/rosscutler/Zotero/storage/I7ZGZPMK/Srinivasan et al. - 2007 - Codebook-Based Bayesian Speech Enhancement for Non.pdf:application/pdf},
}

@techreport{noauthor_itu-t_2018,
	title = {{ITU}-{T} {P}.808: {Subjective} evaluation of speech quality with a crowdsourcing approach},
	year = {2018},
}

@article{lotter_speech_2005,
	title = {Speech {Enhancement} by {MAP} {Spectral} {Amplitude} {Estimation} {Using} a {Super}-{Gaussian} {Speech} {Model}},
	volume = {2005},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.1110},
	doi = {10.1155/ASP.2005.1110},
	language = {en},
	number = {7},
	urldate = {2020-01-17},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Lotter, Thomas and Vary, Peter},
	month = dec,
	year = {2005},
	pages = {354850},
	file = {Full Text:/Users/rosscutler/Zotero/storage/YT4DIWF9/Lotter and Vary - 2005 - Speech Enhancement by MAP Spectral Amplitude Estim.pdf:application/pdf},
}

@inproceedings{wolfe_simple_2001,
	address = {Singapore},
	title = {Simple alternatives to the {Ephraim} and {Malah} suppression rule for speech enhancement},
	isbn = {978-0-7803-7011-1},
	url = {http://ieeexplore.ieee.org/document/955331/},
	doi = {10.1109/SSP.2001.955331},
	urldate = {2020-01-17},
	booktitle = {Proceedings of the 11th {IEEE} {Signal} {Processing} {Workshop} on {Statistical} {Signal} {Processing} ({Cat}. {No}.{01TH8563})},
	publisher = {IEEE},
	author = {Wolfe, P.J. and Godsill, S.J.},
	year = {2001},
	pages = {496--499},
}

@techreport{noauthor_itu-t_2003,
	title = {{ITU}-{T} {P}.835 : {Subjective} test methodology for evaluating speech communication systems that include noise suppression algorithm},
	year = {2003},
}

@techreport{noauthor_itu-t_1998,
	title = {{ITU}-{T} {Supplement} 23 {ITU}-{T} coded-speech database {Supplement} 23 to {ITU}-{T} {P}-series {Recommendations} ({Previously} {CCITT} {Recommendations})},
	year = {1998},
	file = {PDF:/Users/rosscutler/Zotero/storage/PGTV783P/Unknown - Unknown - ITU-T Supplement 23 ITU-T coded-speech database Supplement 23 to ITU-T P-series Recommendations (Previously CCITT Re.pdf:application/pdf},
}

@article{karadagur_ananda_reddy_individualized_2017,
	title = {An {Individualized} {Super}-{Gaussian} {Single} {Microphone} {Speech} {Enhancement} for {Hearing} {Aid} {Users} {With} {Smartphone} as an {Assistive} {Device}},
	volume = {24},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/8031044/},
	doi = {10.1109/LSP.2017.2750979},
	number = {11},
	urldate = {2020-01-17},
	journal = {IEEE Signal Processing Letters},
	author = {Karadagur Ananda Reddy, Chandan and Shankar, Nikhil and Shreedhar Bhat, Gautam and Charan, Ram and Panahi, Issa},
	month = nov,
	year = {2017},
	pages = {1601--1605},
	file = {Accepted Version:/Users/rosscutler/Zotero/storage/K7RAQ6SV/Karadagur Ananda Reddy et al. - 2017 - An Individualized Super-Gaussian Single Microphone.pdf:application/pdf},
}

@article{ephraim_speech_1985,
	title = {Speech enhancement using a minimum mean-square error log-spectral amplitude estimator},
	volume = {33},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1164550/},
	doi = {10.1109/TASSP.1985.1164550},
	language = {en},
	number = {2},
	urldate = {2020-01-16},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Ephraim, Y. and Malah, D.},
	month = apr,
	year = {1985},
	pages = {443--445},
}

@article{ismail_studies_1975,
	title = {Studies on malaria and responses of {Anopheles} balabacensis balabacensis and {Anopheles} minimus to {DDT} residual spraying in {Thailand}},
	volume = {32},
	issn = {0001-706X},
	abstract = {Studies on malaria and on A. b. balabacensis and A. minimus responses to DDT spraying were conducted in a forested hilly area in northern Thailand. In a first phase, base-line data were collected from July 1970 to March 1972. In a second phase, the study area received five round of DDT spraying over a period of two years and at the same time all malaria infections received radical treatment. During this two-year period of field operations, entomological and epidemiological observations were continued. The studies carried out in the second phase, showed that malaria transmission decreased under the applied optimum anti-malarial measures but was not interrupted. Human ecology and population movement inside the forest, especially during the dry season, contributed to a great extent to this result. The transmission occurring in the early part of the monsoon season clearly indicates the importance of the timing of DDT spraying. A. b. balabacensis appeared to be transmitting malaria all the year round in the deep forest but only in the monsoon season in the forest fringe. The vectorial capacity of both vectors was estimated separately for indoor and outdoor populations. The pre-spraying values obtained for A. b. balabacensis were much higher thaan for A. minimus. After DDT spraying A. b. balabacensis showed a decrease in vectorial capacity estimated at 31.5 times for the indoor population and 18 times for the outdoor population. A. minimus, on the other hand, showed a much smaller decrease, estimated at 6.8 and 1.9 times for the indoor and outdoor populations respectively.},
	language = {eng},
	number = {3},
	journal = {Acta Tropica},
	author = {Ismail, I. A. and Notananda, V. and Schepens, J.},
	year = {1975},
	pmid = {1984},
	keywords = {Humans, Animals, Anopheles, Blood, DDT, Feeding Behavior, Female, Housing, Insect Vectors, Malaria, Male, Mosquito Control, Rain, Thailand},
	pages = {206--231},
}

@article{chow_studies_1975,
	title = {Studies of oxygen binding energy to hemoglobin molecule},
	volume = {66},
	issn = {0006-291X},
	doi = {10.1016/0006-291x(75)90518-5},
	language = {eng},
	number = {4},
	journal = {Biochemical and Biophysical Research Communications},
	author = {Chow, Y. W. and Pietranico, R. and Mukerji, A.},
	month = oct,
	year = {1975},
	pmid = {6},
	keywords = {Humans, Binding Sites, Cobalt, Hemoglobins, Hydrogen-Ion Concentration, Iron, Ligands, Mathematics, Oxygen, Oxyhemoglobins, Protein Binding, Spectrum Analysis},
	pages = {1424--1431},
}

@inproceedings{giri_attention_2019,
	title = {Attention {Wave}-{U}-{Net} for {Speech} {Enhancement}},
	doi = {10.1109/WASPAA.2019.8937186},
	abstract = {We propose a novel application of an attention mechanism in neural speech enhancement, by presenting a U-Net architecture with attention mechanism, which processes the raw waveform directly, and is trained end-to-end. We find that the inclusion of the attention mechanism significantly improves the performance of the model in terms of the objective speech quality metrics, and outperforms all other published speech enhancement approaches on the Voice Bank Corpus (VCTK) dataset. We observe that the final layer attention mask has an interpretation as a soft Voice Activity Detector (VAD). We also present some initial results to show the efficacy of the proposed system as a pre-processing step to speech recognition systems.},
	booktitle = {2019 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	author = {Giri, Ritwik and Isik, Umut and Krishnaswamy, Arvindh},
	month = oct,
	year = {2019},
	note = {ISSN: 1931-1168},
	keywords = {deep learning, speech enhancement, attention, Speech denoising, U-Net},
	pages = {249--253},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/UM9GNWRQ/8937186.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/HRSQTCS2/Giri et al. - 2019 - Attention Wave-U-Net for Speech Enhancement.pdf:application/pdf},
}

@inproceedings{dubey_cure_2019,
	title = {{CURE} {Dataset}: {Ladder} {Networks} for {Audio} {Event} {Classification}},
	url = {https://www.microsoft.com/en-us/research/publication/cure-dataset-ladder-networks-for-audio-event/},
	abstract = {Audio event classiﬁcation is an important task for several applications such as surveillance, audio, video and multimedia retrieval etc. There are approximately 340 million people with hearing loss who can’t perceive events happening around them. This paper establishes the CURE dataset which contains curated set of speciﬁc audio events most relevant for people with hearing loss. It is formatted as 5 sec sound recordings derived from the Freesound project. We propose a ladder network based audio event classiﬁer. We adopted the state-of-the-art convolutional neural network (CNN) embeddings as audio features for this task. We start with signal and feature normalization that aims to reduce the mismatch between different recordings scenarios. Initially, a CNN is trained on weakly labeled Audioset data. Next, the pre-trained model is adopted as feature extractor for proposed CURE corpus. We also explore the performance of extreme learning machine (ELM) and use support vector machine (SVM) as baseline classiﬁer. As a second evaluation set we incorporate ESC-50. Results and discussions validate the superiority of Ladder network over ELM and SVM classiﬁer in terms of robustness and increased classiﬁcation accuracy.},
	booktitle = {2019 {IEEE} {Pacific} {Rim} {Conference} on {Communications}, {Computers} and {Signal} {Processing} ({PacRim})},
	publisher = {IEEE},
	author = {Dubey, Harishchandra and Emmanouilidou, Dimitra and Tashev, Ivan},
	month = aug,
	year = {2019},
}

@article{wu_increasing_2019,
	title = {Increasing {Compactness} {Of} {Deep} {Learning} {Based} {Speech} {Enhancement} {Models} {With} {Parameter} {Pruning} {And} {Quantization} {Techniques}},
	volume = {26},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/1906.01078},
	doi = {10.1109/LSP.2019.2951950},
	abstract = {Most recent studies on deep learning based speech enhancement (SE) focused on improving denoising performance. However, successful SE applications require striking a desirable balance between denoising performance and computational cost in real scenarios. In this study, we propose a novel parameter pruning (PP) technique, which removes redundant channels in a neural network. In addition, a parameter quantization (PQ) technique was applied to reduce the size of a neural network by representing weights with fewer cluster centroids. Because the techniques are derived based on different concepts, the PP and PQ can be integrated to provide even more compact SE models. The experimental results show that the PP and PQ techniques produce a compacted SE model with a size of only 10.03\% compared to that of the original model, resulting in minor performance losses of 1.43\% (from 0.70 to 0.69) for STOI and 3.24\% (from 1.85 to 1.79) for PESQ. The promising results suggest that the PP and PQ techniques can be used in a SE system in devices with limited storage and computation resources.},
	language = {en},
	number = {12},
	urldate = {2020-01-12},
	journal = {IEEE Signal Processing Letters},
	author = {Wu, Jyun-Yi and Yu, Cheng and Fu, Szu-Wei and Liu, Chih-Ting and Chien, Shao-Yi and Tsao, Yu},
	month = dec,
	year = {2019},
	note = {arXiv: 1906.01078},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1887--1891},
	file = {Wu et al. - 2019 - Increasing Compactness Of Deep Learning Based Spee.pdf:/Users/rosscutler/Zotero/storage/EY6C97DI/Wu et al. - 2019 - Increasing Compactness Of Deep Learning Based Spee.pdf:application/pdf;Wu et al. - 2019 - Increasing Compactness Of Deep Learning Based Spee.pdf:/Users/rosscutler/Zotero/storage/8QIPQZUD/Wu et al. - 2019 - Increasing Compactness Of Deep Learning Based Spee.pdf:application/pdf},
}

@article{fu_learning_2019,
	title = {Learning with {Learned} {Loss} {Function}: {Speech} {Enhancement} with {Quality}-{Net} to {Improve} {Perceptual} {Evaluation} of {Speech} {Quality}},
	issn = {1070-9908, 1558-2361},
	shorttitle = {Learning with {Learned} {Loss} {Function}},
	url = {https://ieeexplore.ieee.org/document/8902088/},
	doi = {10.1109/LSP.2019.2953810},
	abstract = {Utilizing a human-perception-related objective function to train a speech enhancement model has become a popular topic recently. The main reason is that the conventional mean squared error (MSE) loss cannot represent auditory perception well. One of the typical human-perception-related metrics, which is the perceptual evaluation of speech quality (PESQ), has been proven to provide a high correlation to the quality scores rated by humans. Owing to its complex and non-differentiable properties, however, the PESQ function may not be used to optimize speech enhancement models directly. In this study, we propose optimizing the enhancement model with an approximated PESQ function, which is differentiable and learned from the training data. The experimental results show that the learned surrogate function can guide the enhancement model to further boost the PESQ score (increase of 0.18 points compared to the results trained with MSE loss) and maintain the speech intelligibility.},
	language = {en},
	urldate = {2020-01-12},
	journal = {IEEE Signal Processing Letters},
	author = {Fu, Szu-Wei and Liao, Chien-Feng and Tsao, Yu},
	year = {2019},
	pages = {1--1},
	file = {Fu et al. - 2019 - Learning with Learned Loss Function Speech Enhanc.pdf:/Users/rosscutler/Zotero/storage/A47F6JTF/Fu et al. - 2019 - Learning with Learned Loss Function Speech Enhanc.pdf:application/pdf},
}

@article{kolbaek_loss_2019,
	title = {On {Loss} {Functions} for {Supervised} {Monaural} {Time}-{Domain} {Speech} {Enhancement}},
	url = {http://arxiv.org/abs/1909.01019},
	abstract = {Many deep learning-based speech enhancement algorithms are designed to minimize the mean-square error (MSE) in some transform domain between a predicted and a target speech signal. However, optimizing for MSE does not necessarily guarantee high speech quality or intelligibility, which is the ultimate goal of many speech enhancement algorithms. Additionally, only little is known about the impact of the loss function on the emerging class of time-domain deep learning-based speech enhancement systems.},
	language = {en},
	urldate = {2020-01-12},
	journal = {arXiv:1909.01019 [cs, eess]},
	author = {Kolbæk, Morten and Tan, Zheng-Hua and Jensen, Søren Holdt and Jensen, Jesper},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.01019},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Kolbæk et al. - 2019 - On Loss Functions for Supervised Monaural Time-Dom.pdf:/Users/rosscutler/Zotero/storage/XIJTYJ5N/Kolbæk et al. - 2019 - On Loss Functions for Supervised Monaural Time-Dom.pdf:application/pdf},
}

@article{yin_phasen_2019,
	title = {{PHASEN}: {A} {Phase}-and-{Harmonics}-{Aware} {Speech} {Enhancement} {Network}},
	shorttitle = {{PHASEN}},
	url = {http://arxiv.org/abs/1911.04697},
	abstract = {Time-frequency (T-F) domain masking is a mainstream approach for single-channel speech enhancement. Recently, focuses have been put to phase prediction in addition to amplitude prediction. In this paper, we propose a phase-and-harmonics-aware deep neural network (DNN), named PHASEN, for this task. Unlike previous methods that directly use a complex ideal ratio mask to supervise the DNN learning, we design a two-stream network, where amplitude stream and phase stream are dedicated to amplitude and phase prediction. We discover that the two streams should communicate with each other, and this is crucial to phase prediction. In addition, we propose frequency transformation blocks to catch long-range correlations along the frequency axis. The visualization shows that the learned transformation matrix spontaneously captures the harmonic correlation, which has been proven to be helpful for T-F spectrogram reconstruction. With these two innovations, PHASEN acquires the ability to handle detailed phase patterns and to utilize harmonic patterns, getting 1.76dB SDR improvement on AVSpeech + AudioSet dataset. It also achieves significant gains over Google's network on this dataset. On Voice Bank + DEMAND dataset, PHASEN outperforms previous methods by a large margin on four metrics.},
	urldate = {2020-01-06},
	journal = {arXiv:1911.04697 [cs, eess]},
	author = {Yin, Dacheng and Luo, Chong and Xiong, Zhiwei and Zeng, Wenjun},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.04697},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/B2RIXU2Q/Yin et al. - 2019 - PHASEN A Phase-and-Harmonics-Aware Speech Enhance.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/HW7TSSP7/1911.html:text/html},
}

@article{chen_quadrant_2010,
	title = {Quadrant of euphoria: a crowdsourcing platform for {QoE} assessment},
	volume = {24},
	issn = {0890-8044},
	shorttitle = {Quadrant of euphoria},
	url = {http://ieeexplore.ieee.org/document/5430141/},
	doi = {10.1109/MNET.2010.5430141},
	abstract = {Existing QoE (Quality of Experience) assessment methods, subjective or objective, suffer from either or both problems of inaccurate experiment tools and expensive personnel cost. The panacea for them, as we have come to realize, lies in the joint application of paired comparison and crowdsourcing, the latter being a Web 2.0 practice of organizations asking ordinary, unspeciﬁc Internet users to carry out internal tasks. We present in this article Quadrant of Euphoria, a user-friendly, web-based platform facilitating QoE assessments in network and multimedia studies, with features low cost, participant diversity, meaningful and interpretable QoE scores, subject consistency assurance, and burdenless experiment process.},
	language = {en},
	number = {2},
	urldate = {2020-01-02},
	journal = {IEEE Network},
	author = {Chen, Kuan-Ta and Chang, Chi-Jui and Wu, Chen-Chi and Chang, Yu-Chun and Lei, Chin-Laung},
	month = mar,
	year = {2010},
	pages = {28--35},
	file = {Chen et al. - 2010 - Quadrant of euphoria a crowdsourcing platform for.pdf:/Users/rosscutler/Zotero/storage/JFZLSBQE/Chen et al. - 2010 - Quadrant of euphoria a crowdsourcing platform for.pdf:application/pdf},
}

@article{afouras_conversation:_2018,
	title = {The {Conversation}: {Deep} {Audio}-{Visual} {Speech} {Enhancement}},
	shorttitle = {The {Conversation}},
	url = {http://arxiv.org/abs/1804.04121},
	abstract = {Our goal is to isolate individual speakers from multi-talker simultaneous speech in videos. Existing works in this area have focussed on trying to separate utterances from known speakers in controlled environments. In this paper, we propose a deep audio-visual speech enhancement network that is able to separate a speaker's voice given lip regions in the corresponding video, by predicting both the magnitude and the phase of the target signal. The method is applicable to speakers unheard and unseen during training, and for unconstrained environments. We demonstrate strong quantitative and qualitative results, isolating extremely challenging real-world examples.},
	urldate = {2019-12-26},
	journal = {arXiv:1804.04121 [cs]},
	author = {Afouras, Triantafyllos and Chung, Joon Son and Zisserman, Andrew},
	month = jun,
	year = {2018},
	note = {arXiv: 1804.04121},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound},
	file = {30014e936d153db4f2d4555d411c35f986cc.pdf:/Users/rosscutler/Zotero/storage/TEZPGXXP/30014e936d153db4f2d4555d411c35f986cc.pdf:application/pdf;arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/3PAXDHHK/Afouras et al. - 2018 - The Conversation Deep Audio-Visual Speech Enhance.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/FTNDLSG7/1804.html:text/html},
}

@article{jiang_predicting_2019,
	title = {Predicting the {Generalization} {Gap} in {Deep} {Networks} with {Margin} {Distributions}},
	url = {http://arxiv.org/abs/1810.00113},
	abstract = {As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum). Our measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.},
	urldate = {2019-12-26},
	journal = {arXiv:1810.00113 [cs, stat]},
	author = {Jiang, Yiding and Krishnan, Dilip and Mobahi, Hossein and Bengio, Samy},
	month = jun,
	year = {2019},
	note = {arXiv: 1810.00113},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/YLWYXCUJ/Jiang et al. - 2019 - Predicting the Generalization Gap in Deep Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/886S6PP9/1810.html:text/html},
}

@article{jiang_fantastic_2019,
	title = {Fantastic {Generalization} {Measures} and {Where} to {Find} {Them}},
	url = {http://arxiv.org/abs/1912.02178},
	abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
	urldate = {2019-12-26},
	journal = {arXiv:1912.02178 [cs, stat]},
	author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.02178},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/WM6YS3BL/Jiang et al. - 2019 - Fantastic Generalization Measures and Where to Fin.pdf:application/pdf;arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/68G2MHCF/Jiang et al. - 2019 - Fantastic Generalization Measures and Where to Fin.pdf:application/pdf;arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/XC6IEFFA/Jiang et al. - 2019 - Fantastic Generalization Measures and Where to Fin.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/8MXN3Y2Z/1912.html:text/html;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/R5B6XALK/1912.html:text/html;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/XSRHU3CR/1912.html:text/html},
}

@article{harris_youre_nodate,
	title = {You’re {Hired}! {An} {Examination} of {Crowdsourcing} {Incentive} {Models} in {Human} {Resource} {Tasks}},
	abstract = {Many human resource tasks, such as screening a large number of job candidates, are labor-intensive and rely on subjective evaluation, making them excellent candidates for crowdsourcing. We conduct several experiments using the Amazon Mechanical Turk platform to conduct resume reviews. We then apply several incentive-based models and examine their effects. Next, we assess the accuracy measures of our incentive models against a gold standard and ascertain which incentives provide the best results. We find that some incentives actually encourage quality if the task is designed appropriately.},
	language = {en},
	author = {Harris, Christopher G},
	pages = {4},
	file = {Harris - You’re Hired! An Examination of Crowdsourcing Ince.pdf:/Users/rosscutler/Zotero/storage/2DPUMYN8/Harris - You’re Hired! An Examination of Crowdsourcing Ince.pdf:application/pdf},
}

@article{raghavendran_implementation_nodate,
	title = {Implementation of an {Acoustic} {Echo} {Canceller} {Using} {Matlab}},
	language = {en},
	author = {Raghavendran, Srinivasaprasath},
	pages = {67},
	file = {Raghavendran - Implementation of an Acoustic Echo Canceller Using.pdf:/Users/rosscutler/Zotero/storage/SD97PGPC/Raghavendran - Implementation of an Acoustic Echo Canceller Using.pdf:application/pdf},
}

@inproceedings{zhang_deep_2019,
	title = {Deep {Learning} for {Joint} {Acoustic} {Echo} and {Noise} {Cancellation} with {Nonlinear} {Distortions}},
	url = {http://www.isca-speech.org/archive/Interspeech_2019/abstracts/2651.html},
	doi = {10.21437/Interspeech.2019-2651},
	abstract = {We formulate acoustic echo and noise cancellation jointly as deep learning based speech separation, where near-end speech is separated from a single microphone recording and sent to the far end. We propose a causal system to address this problem, which incorporates a convolutional recurrent network (CRN) and a recurrent network with long short-term memory (LSTM). The system is trained to estimate the real and imaginary spectrograms of near-end speech and detect the activity of near-end speech from the microphone signal and far-end signal. Subsequently, the estimated real and imaginary spectrograms are used to separate the near-end signal, hence removing echo and noise. The trained near-end speech detector is employed to further suppress residual echo and noise. Evaluation results show that the proposed method effectively removes acoustic echo and background noise in the presence of nonlinear distortions for both simulated and measured room impulse responses (RIRs). Additionally, the proposed method generalizes well to untrained noises, RIRs and speakers.},
	language = {en},
	urldate = {2019-12-04},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Zhang, Hao and Tan, Ke and Wang, DeLiang},
	month = sep,
	year = {2019},
	pages = {4255--4259},
	file = {Zhang et al. - 2019 - Deep Learning for Joint Acoustic Echo and Noise Ca.pdf:/Users/rosscutler/Zotero/storage/T8BN35ZT/Zhang et al. - 2019 - Deep Learning for Joint Acoustic Echo and Noise Ca.pdf:application/pdf},
}

@inproceedings{chen_crowdsourceable_2009,
	address = {Beijing, China},
	title = {A crowdsourceable {QoE} evaluation framework for multimedia content},
	isbn = {978-1-60558-608-3},
	url = {http://portal.acm.org/citation.cfm?doid=1631272.1631339},
	doi = {10.1145/1631272.1631339},
	abstract = {Until recently, QoE (Quality of Experience) experiments had to be conducted in academic laboratories; however, with the advent of ubiquitous Internet access, it is now possible to ask an Internet crowd to conduct experiments on their personal computers. Since such a crowd can be quite large, crowdsourcing enables researchers to conduct experiments with a more diverse set of participants at a lower economic cost than would be possible under laboratory conditions. However, because participants carry out experiments without supervision, they may give erroneous feedback perfunctorily, carelessly, or dishonestly, even if they receive a reward for each experiment.},
	language = {en},
	urldate = {2019-11-26},
	booktitle = {Proceedings of the seventeen {ACM} international conference on {Multimedia} - {MM} '09},
	publisher = {ACM Press},
	author = {Chen, Kuan-Ta and Wu, Chen-Chi and Chang, Yu-Chun and Lei, Chin-Laung},
	year = {2009},
	pages = {491},
	file = {chen09_crowdsourcing.pdf:/Users/rosscutler/Zotero/storage/HGXCJGJ4/chen09_crowdsourcing.pdf:application/pdf},
}

@inproceedings{chen_pairwise_2013,
	address = {Rome, Italy},
	title = {Pairwise ranking aggregation in a crowdsourced setting},
	isbn = {978-1-4503-1869-3},
	url = {http://dl.acm.org/citation.cfm?doid=2433396.2433420},
	doi = {10.1145/2433396.2433420},
	language = {en},
	urldate = {2019-11-26},
	booktitle = {Proceedings of the sixth {ACM} international conference on {Web} search and data mining - {WSDM} '13},
	publisher = {ACM Press},
	author = {Chen, Xi and Bennett, Paul N. and Collins-Thompson, Kevyn and Horvitz, Eric},
	year = {2013},
	pages = {193},
	file = {Chen et al. - 2013 - Pairwise ranking aggregation in a crowdsourced set.pdf:/Users/rosscutler/Zotero/storage/DR9FZ36M/Chen et al. - 2013 - Pairwise ranking aggregation in a crowdsourced set.pdf:application/pdf},
}

@article{grais_referenceless_2018,
	title = {Referenceless {Performance} {Evaluation} of {Audio} {Source} {Separation} using {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1811.00454},
	abstract = {Current performance evaluation for audio source separation depends on comparing the processed or separated signals with reference signals. Therefore, common performance evaluation toolkits are not applicable to real-world situations where the ground truth audio is unavailable. In this paper, we propose a performance evaluation technique that does not require reference signals in order to assess separation quality. The proposed technique uses a deep neural network (DNN) to map the processed audio into its quality score. Our experiment results show that the DNN is capable of predicting the sources-to-artifacts ratio from the blind source separation evaluation toolkit without the need for reference signals.},
	urldate = {2019-11-26},
	journal = {arXiv:1811.00454 [cs, eess]},
	author = {Grais, Emad M. and Wierstorf, Hagen and Ward, Dominic and Mason, Russell and Plumbley, Mark D.},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.00454},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Computer Science - Multimedia, 68T01, 68T10, 68T45, 62H25, H.5.5, I.2, I.2.6, I.4, I.4.3, I.5},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/ZGINEYZT/Grais et al. - 2018 - Referenceless Performance Evaluation of Audio Sour.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/UYXWLVCQ/1811.html:text/html},
}

@inproceedings{santos_towards_2019,
	title = {Towards the development of a non-intrusive objective quality measure for {DNN}-enhanced speech},
	doi = {10.1109/QoMEX.2019.8743156},
	abstract = {Recently, several works have focused on leveraging the advances of deep neural networks (DNN) to a variety of domains, including speech enhancement. While advances in instrumental quality metrics have been made, particularly for enhanced speech, there is still relatively little research assessing how useful such metrics are for DNN-enhanced speech. This work aims to fill this gap. We performed online listening tests using the outputs of three different DNN-based speech enhancement models for both denoising and dereverberation. When assessing the predictive power of several objective metrics, we found that existing non-intrusive methods fail at monitoring signal quality. To overcome this limitation, we propose a new metric based on a combination of a handful of relevant acoustic features. Results inline with those obtained with intrusive measures are then attained. In a leave-one-model-out test, the proposed non-intrusive metric is also shown to outperform two non-intrusive benchmarks for all three DNN enhancement methods, showing the proposed method is capable of generalizing to unseen models.},
	booktitle = {2019 {Eleventh} {International} {Conference} on {Quality} of {Multimedia} {Experience} ({QoMEX})},
	author = {Santos, João Felipe and Falk, Tiago H.},
	month = jun,
	year = {2019},
	note = {ISSN: 2372-7179},
	keywords = {speech enhancement, neural nets, deep neural networks, DNN-based speech enhancement models, DNN-enhanced speech, instrumental quality metrics, intrusive measures, leave-one-model-out test, monitoring signal quality, non-intrusive metrics, nonintrusive objective quality measure, objective metrics, online listening tests, signal denoising, speech quality, statistical testing},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/ER46M8A7/8743156.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/5FYILF2Z/Santos and Falk - 2019 - Towards the development of a non-intrusive objecti.pdf:application/pdf},
}

@article{cheng_predictor-corrector_2019,
	title = {Predictor-{Corrector} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1810.06509},
	abstract = {We present a predictor-corrector framework, called PicCoLO, that can transform a first-order model-free reinforcement or imitation learning algorithm into a new hybrid method that leverages predictive models to accelerate policy learning. The new "PicCoLOed" algorithm optimizes a policy by recursively repeating two steps: In the Prediction Step, the learner uses a model to predict the unseen future gradient and then applies the predicted estimate to update the policy; in the Correction Step, the learner runs the updated policy in the environment, receives the true gradient, and then corrects the policy using the gradient error. Unlike previous algorithms, PicCoLO corrects for the mistakes of using imperfect predicted gradients and hence does not suffer from model bias. The development of PicCoLO is made possible by a novel reduction from predictable online learning to adversarial online learning, which provides a systematic way to modify existing first-order algorithms to achieve the optimal regret with respect to predictable information. We show, in both theory and simulation, that the convergence rate of several first-order model-free algorithms can be improved by PicCoLO.},
	urldate = {2019-11-13},
	journal = {arXiv:1810.06509 [cs, stat]},
	author = {Cheng, Ching-An and Yan, Xinyan and Ratliff, Nathan and Boots, Byron},
	month = may,
	year = {2019},
	note = {arXiv: 1810.06509},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/I8Z63I44/Cheng et al. - 2019 - Predictor-Corrector Policy Optimization.pdf:application/pdf;arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/G96RPH3H/Cheng et al. - 2019 - Predictor-Corrector Policy Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/NDNTS753/1810.html:text/html;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/FX3RRVT7/1810.html:text/html},
}

@inproceedings{fazel_deep_2019,
	title = {Deep {Multitask} {Acoustic} {Echo} {Cancellation}},
	url = {http://www.isca-speech.org/archive/Interspeech_2019/abstracts/2908.html},
	doi = {10.21437/Interspeech.2019-2908},
	abstract = {Acoustic echo cancellation or suppression methods aim to suppress the echo originated from acoustic coupling between loudspeakers and microphones. Conventional approaches estimate echo using adaptive filtering. Due to the nonlinearities in the acoustic path of far-end signal, further post-processing is needed to attenuate these nonlinear components. In this paper, we propose a novel architecture based on deep gated recurrent neural networks to estimate the near-end signal from the microphone signal. The proposed architecture is trained using multitask learning to learn the auxiliary task of estimating the echo in order to improve the main task of estimating the clean near-end speech signal. Experimental results show that our proposed deep learning based method outperforms the existing methods for unseen speakers in terms of the echo return loss enhancement (ERLE) for single-talk periods and the perceptual evaluation of speech quality (PESQ) score for double-talk periods.},
	language = {en},
	urldate = {2019-11-07},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Fazel, Amin and El-Khamy, Mostafa and Lee, Jungwon},
	month = sep,
	year = {2019},
	pages = {4250--4254},
	file = {Fazel et al. - 2019 - Deep Multitask Acoustic Echo Cancellation.pdf:/Users/rosscutler/Zotero/storage/LCUAC7Y6/Fazel et al. - 2019 - Deep Multitask Acoustic Echo Cancellation.pdf:application/pdf},
}

@article{lostanlen_per-channel_2019,
	title = {Per-{Channel} {Energy} {Normalization}: {Why} and {How}},
	volume = {26},
	issn = {1070-9908, 1558-2361},
	shorttitle = {Per-{Channel} {Energy} {Normalization}},
	url = {https://ieeexplore.ieee.org/document/8514023/},
	doi = {10.1109/LSP.2018.2878620},
	abstract = {In the context of automatic speech recognition and acoustic event detection, an adaptive procedure named perchannel energy normalization (PCEN) has recently shown to outperform the pointwise logarithm of mel-frequency spectrogram (logmelspec) as an acoustic frontend. This article investigates the adequacy of PCEN for spectrogram-based pattern recognition in far-ﬁeld noisy recordings, both from theoretical and practical standpoints. First, we apply PCEN on various datasets of natural acoustic environments and ﬁnd empirically that it Gaussianizes distributions of magnitudes while decorrelating frequency bands. Secondly, we describe the asymptotic regimes of each component in PCEN: temporal integration, gain control, and dynamic range compression. Thirdly, we give practical advice for adapting PCEN parameters to the temporal properties of the noise to be mitigated, the signal to be enhanced, and the choice of time-frequency representation. As it converts a large class of real-world soundscapes into additive white Gaussian noise (AWGN), PCEN is a computationally efﬁcient frontend for robust detection and classiﬁcation of acoustic events in heterogeneous environments.},
	language = {en},
	number = {1},
	urldate = {2019-11-05},
	journal = {IEEE Signal Processing Letters},
	author = {Lostanlen, Vincent and Salamon, Justin and Cartwright, Mark and McFee, Brian and Farnsworth, Andrew and Kelling, Steve and Bello, Juan Pablo},
	month = jan,
	year = {2019},
	pages = {39--43},
	file = {Lostanlen et al. - 2019 - Per-Channel Energy Normalization Why and How.pdf:/Users/rosscutler/Zotero/storage/3M4UE9RR/Lostanlen et al. - 2019 - Per-Channel Energy Normalization Why and How.pdf:application/pdf},
}

@article{wisdom_differentiable_2018,
	title = {Differentiable {Consistency} {Constraints} for {Improved} {Deep} {Speech} {Enhancement}},
	url = {http://arxiv.org/abs/1811.08521},
	abstract = {In recent years, deep networks have led to dramatic improvements in speech enhancement by framing it as a data-driven pattern recognition problem. In many modern enhancement systems, large amounts of data are used to train a deep network to estimate masks for complex-valued short-time Fourier transforms (STFTs) to suppress noise and preserve speech. However, current masking approaches often neglect two important constraints: STFT consistency and mixture consistency. Without STFT consistency, the system's output is not necessarily the STFT of a time-domain signal, and without mixture consistency, the sum of the estimated sources does not necessarily equal the input mixture. Furthermore, the only previous approaches that apply mixture consistency use real-valued masks; mixture consistency has been ignored for complex-valued masks. In this paper, we show that STFT consistency and mixture consistency can be jointly imposed by adding simple differentiable projection layers to the enhancement network. These layers are compatible with real or complex-valued masks. Using both of these constraints with complex-valued masks provides a 0.7 dB increase in scale-invariant signal-to-distortion ratio (SI-SDR) on a large dataset of speech corrupted by a wide variety of nonstationary noise across a range of input SNRs.},
	urldate = {2019-11-05},
	journal = {arXiv:1811.08521 [cs, eess]},
	author = {Wisdom, Scott and Hershey, John R. and Wilson, Kevin and Thorpe, Jeremy and Chinen, Michael and Patton, Brian and Saurous, Rif A.},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.08521},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/GTTF3PUM/Wisdom et al. - 2018 - Differentiable Consistency Constraints for Improve.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/3TWFYUBL/1811.html:text/html;Wisdom et al. - 2018 - Differentiable Consistency Constraints for Improve.pdf:/Users/rosscutler/Zotero/storage/TZ2GH8NE/Wisdom et al. - 2018 - Differentiable Consistency Constraints for Improve.pdf:application/pdf},
}

@article{ni_onssen:_2019,
	title = {Onssen: an open-source speech separation and enhancement library},
	shorttitle = {Onssen},
	url = {http://arxiv.org/abs/1911.00982},
	abstract = {Speech separation is an essential task for multi-talker speech recognition. Recently many deep learning approaches are proposed and have been constantly refreshing the state-of-the-art performances. The lack of algorithm implementations limits researchers to use the same dataset for comparison. Building a generic platform can benefit researchers by easily implementing novel separation algorithms and comparing them with the existing ones on customized datasets. We introduce "onssen": an open-source speech separation and enhancement library. onssen is a library mainly for deep learning separation and enhancement algorithms. It uses LibRosa and NumPy libraries for the feature extraction and PyTorch as the back-end for model training. onssen supports most of the Time-Frequency mask-based separation algorithms (e.g. deep clustering, chimera net, chimera++, and so on) and also supports customized datasets. In this paper, we describe the functionality of modules in onssen and show the algorithms implemented by onssen achieve the same performances as reported in the original papers.},
	urldate = {2019-11-05},
	journal = {arXiv:1911.00982 [cs, eess]},
	author = {Ni, Zhaoheng and Mandel, Michael I.},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.00982},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/LEYINFDI/Ni and Mandel - 2019 - Onssen an open-source speech separation and enhan.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/53C2824E/1911.html:text/html;Ni and Mandel - 2019 - Onssen an open-source speech separation and enhan.pdf:/Users/rosscutler/Zotero/storage/ZUI2J44B/Ni and Mandel - 2019 - Onssen an open-source speech separation and enhan.pdf:application/pdf},
}

@article{maciejewski_whamr!:_2019,
	title = {{WHAMR}!: {Noisy} and {Reverberant} {Single}-{Channel} {Speech} {Separation}},
	shorttitle = {{WHAMR}!},
	url = {http://arxiv.org/abs/1910.10279},
	abstract = {While significant advances have been made in recent years in the separation of overlapping speech signals, studies have been largely constrained to mixtures of clean, near-field speech, not representative of many real-world scenarios. Although the WHAM! dataset introduced noise to the ubiquitous wsj0-2mix dataset, it did not include the addition of reverberation, generally present in indoor recordings outside of recording studios. The spectral smearing caused by reverberation can result in significant performance degradation for standard deep learning-based speech separation systems, which rely on spectral structure and the sparsity of speech signals to tease apart sources. To address this, we introduce WHAMR!, an augmented version of WHAM! with synthetic reverberated sources, and provide a thorough baseline analysis of current techniques as well as novel cascaded architectures on the newly introduced conditions.},
	urldate = {2019-11-05},
	journal = {arXiv:1910.10279 [cs, eess]},
	author = {Maciejewski, Matthew and Wichern, Gordon and McQuinn, Emmett and Roux, Jonathan Le},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.10279},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/7BXVGDH6/Maciejewski et al. - 2019 - WHAMR! Noisy and Reverberant Single-Channel Speec.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/8QSEXUBN/1910.html:text/html},
}

@article{sivakumar_mvfst-rl:_2019,
	title = {{MVFST}-{RL}: {An} {Asynchronous} {RL} {Framework} for {Congestion} {Control} with {Delayed} {Actions}},
	shorttitle = {{MVFST}-{RL}},
	url = {http://arxiv.org/abs/1910.04054},
	abstract = {Effective network congestion control strategies are key to keeping the Internet (or any large computer network) operational. Network congestion control has been dominated by hand-crafted heuristics for decades. Recently, Reinforcement Learning (RL) has emerged as an alternative to automatically optimize such control strategies. Research so far has primarily considered RL interfaces which block the sender while an agent considers its next action. This is largely an artifact of building on top of frameworks designed for RL in games (e.g. OpenAI Gym). However, this does not translate to real-world networking environments, where a network sender waiting on a policy without sending data leads to under-utilization of bandwidth. We instead propose to formulate congestion control with an asynchronous RL agent that handles delayed actions. We present MVFST-RL, a scalable framework for congestion control in the QUIC transport protocol that leverages state-of-the-art in asynchronous RL training with off-policy correction. We analyze modeling improvements to mitigate the deviation from Markovian dynamics, and evaluate our method on emulated networks from the Pantheon benchmark platform. The source code is publicly available at https://github.com/facebookresearch/mvfst-rl.},
	language = {en},
	urldate = {2019-11-05},
	journal = {arXiv:1910.04054 [cs, stat]},
	author = {Sivakumar, Viswanath and Rocktäschel, Tim and Miller, Alexander H. and Küttler, Heinrich and Nardelli, Nantas and Rabbat, Mike and Pineau, Joelle and Riedel, Sebastian},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.04054},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Networking and Internet Architecture, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Sivakumar et al. - 2019 - MVFST-RL An Asynchronous RL Framework for Congest.pdf:/Users/rosscutler/Zotero/storage/EUMVBCQZ/Sivakumar et al. - 2019 - MVFST-RL An Asynchronous RL Framework for Congest.pdf:application/pdf},
}

@inproceedings{thermos_audio-visual_2016,
	address = {San Diego, CA},
	title = {Audio-visual speech activity detection in a two-speaker scenario incorporating depth information from a profile or frontal view},
	isbn = {978-1-5090-4903-5},
	url = {http://ieeexplore.ieee.org/document/7846321/},
	doi = {10.1109/SLT.2016.7846321},
	abstract = {Motivated by increasing popularity of depth visual sensors, such as the Kinect device, we investigate the utility of depth information in audio-visual speech activity detection. A two-subject scenario is assumed, allowing to also consider speech overlap. Two sensory setups are employed, where depth video captures either a frontal or proﬁle view of the subjects, and is subsequently combined with the corresponding planar video and audio streams. Further, multi-view fusion is regarded, using audio and planar video from a sensor at the complementary view setup. Support vector machines provide temporal speech activity classiﬁcation for each visually detected subject, fusing the available modality streams. Classiﬁcation results are further combined to yield speaker diarization. Experiments are reported on a suitable audio-visual corpus recorded by two Kinects. Results demonstrate the beneﬁts of depth information, particularly in the frontal depth view setup, reducing speech activity detection and speaker diarization errors over systems that ignore it.},
	language = {en},
	urldate = {2019-11-02},
	booktitle = {2016 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	publisher = {IEEE},
	author = {Thermos, Spyridon and Potamianos, Gerasimos},
	month = dec,
	year = {2016},
	pages = {579--584},
	file = {Thermos and Potamianos - 2016 - Audio-visual speech activity detection in a two-sp.pdf:/Users/rosscutler/Zotero/storage/7VF8EXK9/Thermos and Potamianos - 2016 - Audio-visual speech activity detection in a two-sp.pdf:application/pdf},
}

@article{ariav_deep_2018,
	title = {A deep architecture for audio-visual voice activity detection in the presence of transients},
	volume = {142},
	issn = {01651684},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165168417302529},
	doi = {10.1016/j.sigpro.2017.07.006},
	abstract = {We address the problem of voice activity detection in diﬃcult acoustic environments including high levels of noise and transients, which are common in real life scenarios. We consider a multimodal setting, in which the speech signal is captured by a microphone, and a video camera is pointed at the face of the desired speaker. Accordingly, speech detection translates to the question of how to properly fuse the audio and video signals, which we address within the framework of deep learning. Speciﬁcally, we present a neural network architecture based on a variant of auto-encoders, which combines the two modalities, and provides a new representation of the signal, in which the effect of interferences is reduced. To further encode differences between the dynamics of speech and interfering transients, the signal, in this new representation, is fed into a recurrent neural network, which is trained in a supervised manner for speech detection. Experimental results demonstrate improved performance of the proposed deep architecture compared to competing multimodal detectors.},
	language = {en},
	urldate = {2019-11-02},
	journal = {Signal Processing},
	author = {Ariav, Ido and Dov, David and Cohen, Israel},
	month = jan,
	year = {2018},
	pages = {69--74},
	file = {Ariav et al. - 2018 - A deep architecture for audio-visual voice activit.pdf:/Users/rosscutler/Zotero/storage/7K2S2C57/Ariav et al. - 2018 - A deep architecture for audio-visual voice activit.pdf:application/pdf},
}

@article{he_deep_2018,
	title = {Deep {Neural} {Networks} for {Multiple} {Speaker} {Detection} and {Localization}},
	url = {http://arxiv.org/abs/1711.11565},
	doi = {10.1109/ICRA.2018.8461267},
	abstract = {We propose to use neural networks for simultaneous detection and localization of multiple sound sources in human-robot interaction. In contrast to conventional signal processing techniques, neural network-based sound source localization methods require fewer strong assumptions about the environment. Previous neural network-based methods have been focusing on localizing a single sound source, which do not extend to multiple sources in terms of detection and localization. In this paper, we thus propose a likelihood-based encoding of the network output, which naturally allows the detection of an arbitrary number of sources. In addition, we investigate the use of sub-band cross-correlation information as features for better localization in sound mixtures, as well as three different network architectures based on different motivations. Experiments on real data recorded from a robot show that our proposed methods significantly outperform the popular spatial spectrum-based approaches.},
	urldate = {2019-11-02},
	journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
	author = {He, Weipeng and Motlicek, Petr and Odobez, Jean-Marc},
	month = may,
	year = {2018},
	note = {arXiv: 1711.11565},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Multimedia},
	pages = {74--79},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/LQC7S9K3/He et al. - 2018 - Deep Neural Networks for Multiple Speaker Detectio.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/4EV6NPDF/1711.html:text/html},
}

@article{tapu_deep-hear:_2019,
	title = {{DEEP}-{HEAR}: {A} {Multimodal} {Subtitle} {Positioning} {System} {Dedicated} to {Deaf} and {Hearing}-{Impaired} {People}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {{DEEP}-{HEAR}},
	doi = {10.1109/ACCESS.2019.2925806},
	abstract = {In this paper, we introduce the DEEP-HEAR framework, a multimodal dynamic subtitle positioning system designed to increase the accessibility of deaf and hearing impaired people (HIP) to multimedia documents. The proposed system exploits both computer vision algorithms and deep convolutional neural networks specifically designed and tuned in order to detect and recognize the identity of the active speaker. The main contributions of the paper concern: a novel method dedicated to recognizing various characters existent in the video stream. A video temporal segmentation algorithm that divides the video sequence into semantic units, based on face tracks and visual consistency. Finally, the core of our approach concerns a novel active speaker recognition method relying on the multimodal information fusion from the text, audio, and video streams. The experimental results carried out on a large scale dataset of more than 30 videos, validate the proposed methodology with average accuracy and recognition rates superior to 90\%. Moreover, the method shows robustness to important object/camera motion and face pose variation, yielding gains of more than 8\% in precision and recall rates when compared with state-of-the-art techniques. The subjective evaluation of the proposed dynamic subtitle positioning system demonstrates the effectiveness of our approach.},
	journal = {IEEE Access},
	author = {Tapu, Ruxandra and Mocanu, Bogdan and Zaharia, Titus},
	year = {2019},
	keywords = {speaker recognition, Cameras, multimedia communication, video signal processing, convolutional neural nets, Streaming media, convolutional neural networks, Active speaker recognition, active speaker recognition method, assistive framework for deaf and hearing impaired people, Auditory system, computer vision, computer vision algorithms, deep convolutional neural networks, DEEP-HEAR framework, dynamic subtitle positioning, Face, face recognition, Face recognition, face tracks, handicapped aids, hearing-impaired people, image fusion, image motion analysis, image segmentation, image sequences, multimedia documents, multimodal information fusion, multimodal subtitle positioning system, object/camera motion, Speaker recognition, video sequence, video stream, video streaming, video temporal segmentation algorithm, Visualization},
	pages = {88150--88162},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/U6BIXYB5/8751956.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/MW93T8F9/Tapu et al. - 2019 - DEEP-HEAR A Multimodal Subtitle Positioning Syste.pdf:application/pdf},
}

@article{cutler_distributed_2002,
	title = {Distributed {Meetings}: {A} {Meeting} {Capture} and {Broadcasting} {System}},
	abstract = {The common meeting is an integral part of everyday life for most workgroups. However, due to travel, time, or other constraints, people are often not able to attend all the meetings they need to. Teleconferencing and recording of meetings can address this problem. In this paper we describe a system that provides these features, as well as a user study evaluation of the system. The system uses a variety of capture devices (a novel 360º camera, a whiteboard camera, an overview camera, and a microphone array) to provide a rich experience for people who want to participate in a meeting from a distance. The system is also combined with speaker clustering, spatial indexing, and time compression to provide a rich experience for people who miss a meeting and want to watch it afterward.},
	language = {en},
	journal = {ACM international conference on Multimedia},
	author = {Cutler, Ross and Rui, Yong and Gupta, Anoop and Cadiz, JJ and Tashev, Ivan and He, Li-wei and Colburn, Alex and Zhang, Zhengyou and Liu, Zicheng and Silverberg, Steve},
	month = dec,
	year = {2002},
	pages = {503--512},
	file = {Cutler et al. - Distributed Meetings A Meeting Capture and Broadc.pdf:/Users/rosscutler/Zotero/storage/L6MDRT84/Cutler et al. - Distributed Meetings A Meeting Capture and Broadc.pdf:application/pdf},
}

@misc{noauthor_cisco_nodate,
	title = {Cisco {TelePresence} {SpeakerTrack} 60 - {Cisco}},
	url = {https://www.cisco.com/c/en/us/products/collaboration-endpoints/telepresence-speaker-track-60/index.html},
	file = {Cisco TelePresence SpeakerTrack 60 - Cisco:/Users/rosscutler/Zotero/storage/W94R2MUC/index.html:text/html},
}

@misc{noauthor_poly_nodate,
	title = {Poly {Studio} {X} {Series}},
	url = {https://www.polycom.com/hd-video-conferencing/room-video-systems/poly-studio-x-series.html},
	abstract = {Poly Studio X Series video bars deliver radical simplicity in a single sleek device.},
	language = {en},
	journal = {en},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/XSI2ZSXG/poly-studio-x-series.html:text/html},
}

@misc{noauthor_eagleeye_nodate,
	title = {{EagleEye} {Director} {II}},
	url = {https://www.polycom.com/hd-video-conferencing/peripherals/eagleeye-director-ii.html},
	abstract = {EagleEye Director II takes video conferencing to the next level by automatically zooming in on an active speaker without the use of a remote control or camera presets.},
	language = {en},
	journal = {en},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/ZFFYU55S/eagleeye-director-ii.html:text/html},
}

@article{chakravarty_cross-modal_2016,
	title = {Cross-modal {Supervision} for {Learning} {Active} {Speaker} {Detection} in {Video}},
	url = {http://arxiv.org/abs/1603.08907},
	abstract = {In this paper, we show how to use audio to supervise the learning of active speaker detection in video. Voice Activity Detection (VAD) guides the learning of the vision-based classifier in a weakly supervised manner. The classifier uses spatio-temporal features to encode upper body motion - facial expressions and gesticulations associated with speaking. We further improve a generic model for active speaker detection by learning person specific models. Finally, we demonstrate the online adaptation of generic models learnt on one dataset, to previously unseen people in a new dataset, again using audio (VAD) for weak supervision. The use of temporal continuity overcomes the lack of clean training data. We are the first to present an active speaker detection system that learns on one audio-visual dataset and automatically adapts to speakers in a new dataset. This work can be seen as an example of how the availability of multi-modal data allows us to learn a model without the need for supervision, by transferring knowledge from one modality to another.},
	urldate = {2019-10-22},
	journal = {arXiv:1603.08907 [cs]},
	author = {Chakravarty, Punarjay and Tuytelaars, Tinne},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.08907},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1603.08907 PDF:/Users/rosscutler/Zotero/storage/JYBDKC4K/Chakravarty and Tuytelaars - 2016 - Cross-modal Supervision for Learning Active Speake.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/PPZHRGYE/1603.html:text/html},
}

@misc{noauthor_logitech_nodate,
	title = {Logitech {MeetUp} {Video} {Conference} {Camera} for {Huddle} {Rooms}},
	url = {https://www.logitech.com/en-us/product/meetup-conferencecam},
	abstract = {MeetUp, Logitech’s premier ConferenceCam, is perfect for huddle rooms video conferences.  A super-wide field of vision ensures everyone in the room is seen.},
	language = {EN},
	urldate = {2019-10-22},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/NHUFHTLT/meetup-conferencecam.html:text/html},
}

@article{chung_naver_2019,
	title = {Naver at {ActivityNet} {Challenge} 2019 -- {Task} {B} {Active} {Speaker} {Detection} ({AVA})},
	url = {http://arxiv.org/abs/1906.10555},
	abstract = {This report describes our submission to the ActivityNet Challenge at CVPR 2019. We use a 3D convolutional neural network (CNN) based front-end and an ensemble of temporal convolution and LSTM classifiers to predict whether a visible person is speaking or not. Our results show significant improvements over the baseline on the AVA-ActiveSpeaker dataset.},
	urldate = {2019-10-22},
	journal = {arXiv:1906.10555 [cs, eess]},
	author = {Chung, Joon Son},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.10555},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv\:1906.10555 PDF:/Users/rosscutler/Zotero/storage/5TL7QPKX/Chung - 2019 - Naver at ActivityNet Challenge 2019 -- Task B Acti.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/QTY69H5Q/1906.html:text/html},
}

@article{roth_ava-activespeaker:_2019,
	title = {{AVA}-{ActiveSpeaker}: {An} {Audio}-{Visual} {Dataset} for {Active} {Speaker} {Detection}},
	shorttitle = {{AVA}-{ActiveSpeaker}},
	url = {http://arxiv.org/abs/1901.01342},
	abstract = {Active speaker detection is an important component in video analysis algorithms for applications such as speaker diarization, video re-targeting for meetings, speech enhancement, and human-robot interaction. The absence of a large, carefully labeled audio-visual dataset for this task has constrained algorithm evaluations with respect to data diversity, environments, and accuracy. This has made comparisons and improvements difficult. In this paper, we present the AVA Active Speaker detection dataset (AVA-ActiveSpeaker) that will be released publicly to facilitate algorithm development and enable comparisons. The dataset contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio. We also present a new audio-visual approach for active speaker detection, and analyze its performance, demonstrating both its strength and the contributions of the dataset.},
	urldate = {2019-10-22},
	journal = {arXiv:1901.01342 [cs, eess]},
	author = {Roth, Joseph and Chaudhuri, Sourish and Klejch, Ondrej and Marvin, Radhika and Gallagher, Andrew and Kaver, Liat and Ramaswamy, Sharadh and Stopczynski, Arkadiusz and Schmid, Cordelia and Xi, Zhonghua and Pantofaru, Caroline},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.01342},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Multimedia},
	file = {arXiv\:1901.01342 PDF:/Users/rosscutler/Zotero/storage/MT98CSIJ/Roth et al. - 2019 - AVA-ActiveSpeaker An Audio-Visual Dataset for Act.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/YG948UH7/1901.html:text/html},
}

@incollection{chen_out_2017,
	address = {Cham},
	title = {Out of {Time}: {Automated} {Lip} {Sync} in the {Wild}},
	volume = {10117},
	isbn = {978-3-319-54426-7 978-3-319-54427-4},
	shorttitle = {Out of {Time}},
	url = {http://link.springer.com/10.1007/978-3-319-54427-4_19},
	abstract = {The goal of this work is to determine the audio-video synchronisation between mouth motion and speech in a video. We propose a two-stream ConvNet architecture that enables the mapping between the sound and the mouth images to be trained end-to-end from unlabelled data. The trained network is used to determine the lip-sync error in a video.},
	language = {en},
	urldate = {2019-10-22},
	booktitle = {Computer {Vision} – {ACCV} 2016 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Chung, Joon Son and Zisserman, Andrew},
	editor = {Chen, Chu-Song and Lu, Jiwen and Ma, Kai-Kuang},
	year = {2017},
	doi = {10.1007/978-3-319-54427-4_19},
	pages = {251--263},
	file = {Chung and Zisserman - 2017 - Out of Time Automated Lip Sync in the Wild.pdf:/Users/rosscutler/Zotero/storage/8RPABQ6C/Chung and Zisserman - 2017 - Out of Time Automated Lip Sync in the Wild.pdf:application/pdf},
}

@book{noauthor_audiovisual_nodate,
	title = {Audiovisual {Fusion}: {Challenges} and {New} {Approaches}},
	shorttitle = {Audiovisual {Fusion}},
	abstract = {This paper reviews recent results in audiovisual fusion and discusses main challenges in the area with a focus on desynchronization of the two modalities and the issue of training and testing where one of the modalities might be absent from testing. By Aggelos K. Katsaggelos, Fellow IEEE, Sara Bahaadini, and Rafael Molina ABSTRACT {\textbar} In this paper, we review recent results on audio-visual (AV) fusion. We also discuss some of the challenges and report on approaches to address them. One important issue in AV fusion is how the modalities interact and influence each other. This review will address this question in the context of AV speech processing, and especially speech recognition, where one of the issues is that the modalities both interact but also sometimes appear to desynchronize from each other. An additional issue that sometimes arises is that one of the modalities may be missing at test time, although it is available at training time; for example, it may be possible to collect AV training data while only having access to audio at test time. We will review approaches to address this issue from the area of multiview learning, where the goal is to learn a model or re-presentation for each of the modalities separately while taking advantage of the rich multimodal training data. In addition to multiview learning, we also discuss the recent application of deep learning (DL) toward AV fusion. We finally draw con-clusions and offer our assessment of the future in the area of AV fusion.},
	file = {Citeseer - Full Text PDF:/Users/rosscutler/Zotero/storage/VHTVIYVU/Audiovisual Fusion Challenges and New Approaches.pdf:application/pdf;Citeseer - Snapshot:/Users/rosscutler/Zotero/storage/XU8HMDIH/summary.html:text/html},
}

@inproceedings{chakravarty_active_2016,
	address = {Tokyo, Japan},
	title = {Active speaker detection with audio-visual co-training},
	isbn = {978-1-4503-4556-9},
	url = {http://dl.acm.org/citation.cfm?doid=2993148.2993172},
	doi = {10.1145/2993148.2993172},
	abstract = {In this work, we show how to co-train a classiﬁer for active speaker detection using audio-visual data. First, audio Voice Activity Detection (VAD) is used to train a personalized video-based active speaker classiﬁer in a weakly supervised fashion. The video classiﬁer is in turn used to train a voice model for each person. The individual voice models are then used to detect active speakers. There is no manual supervision - audio weakly supervises video classiﬁcation, and the co-training loop is completed by using the trained video classiﬁer to supervise the training of a personalized audio voice classiﬁer.},
	language = {en},
	urldate = {2019-10-22},
	booktitle = {Proceedings of the 18th {ACM} {International} {Conference} on {Multimodal} {Interaction} - {ICMI} 2016},
	publisher = {ACM Press},
	author = {Chakravarty, Punarjay and Zegers, Jeroen and Tuytelaars, Tinne and Van hamme, Hugo},
	year = {2016},
	pages = {312--316},
	file = {Chakravarty et al. - 2016 - Active speaker detection with audio-visual co-trai.pdf:/Users/rosscutler/Zotero/storage/WQMIVJ4R/Chakravarty et al. - 2016 - Active speaker detection with audio-visual co-trai.pdf:application/pdf},
}

@inproceedings{yong_rui_sound_2005,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Sound {Source} {Localization} for {Circular} {Arrays} of {Directional} {Microphones}},
	volume = {3},
	isbn = {978-0-7803-8874-1},
	url = {http://ieeexplore.ieee.org/document/1415654/},
	doi = {10.1109/ICASSP.2005.1415654},
	abstract = {Previous research in sound source localization has helped increase the robustness of estimates to noise and reverberation. Circular arrays are of particular interest for a number of scenarios, particularly because they can be placed in the center of the sources. First, that improves the sound capture due to the reduced distance. Second, it helps on the direction estimation, not only because of the reduced distance, but also because it increases the angle differences. Nevertheless, most research on circular arrays focused on the case of omni-directional microphones. In this paper we present a new algorithm for sound source localization developed specifically for directional microphones. Results obtained from real meeting room setups show a typical error of less than 3 degrees.},
	language = {en},
	urldate = {2019-10-22},
	booktitle = {Proceedings. ({ICASSP} '05). {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}, 2005.},
	publisher = {IEEE},
	author = {{Yong Rui} and Florencio, D. and Lam, W. and {Jinyan Su}},
	year = {2005},
	pages = {93--96},
	file = {Yong Rui et al. - 2005 - Sound Source Localization for Circular Arrays of D.pdf:/Users/rosscutler/Zotero/storage/HK854XJ2/Yong Rui et al. - 2005 - Sound Source Localization for Circular Arrays of D.pdf:application/pdf},
}

@inproceedings{zhang_maximum_2007,
	address = {Honolulu, HI},
	title = {Maximum {Likelihood} {Sound} {Source} {Localization} for {Multiple} {Directional} {Microphones}},
	isbn = {978-1-4244-0727-9},
	url = {https://ieeexplore.ieee.org/document/4217032/},
	doi = {10.1109/ICASSP.2007.366632},
	abstract = {This paper presents a maximum likelihood (ML) framework for multimicrophone sound source localization (SSL). Besides deriving the framework, we focus on making the connection and contrast between the ML-based algorithm and popular steered response power (SRP) SSL algorithms such as phase transform (SRP-PHAT). We also show under our ML framework how challenging conditions such as directional microphone arrays and reverberations can be handled. The computational cost of our method is low – similar to SRPPHAT. The effectiveness of the proposed method is shown on a large dataset with 99 real-world audio sequences recorded by directional circular microphone arrays in over 50 different meeting rooms.},
	language = {en},
	urldate = {2019-10-22},
	booktitle = {2007 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} - {ICASSP} '07},
	publisher = {IEEE},
	author = {Zhang, Cha and Zhang, Zhengyou and Florencio, Dinei},
	month = apr,
	year = {2007},
	pages = {I--125--I--128},
	file = {Zhang et al. - 2007 - Maximum Likelihood Sound Source Localization for M.pdf:/Users/rosscutler/Zotero/storage/PIW4VY94/Zhang et al. - 2007 - Maximum Likelihood Sound Source Localization for M.pdf:application/pdf},
}

@article{zhang_boosting-based_2008,
	title = {Boosting-{Based} {Multimodal} {Speaker} {Detection} for {Distributed} {Meeting} {Videos}},
	volume = {10},
	issn = {1520-9210},
	url = {http://ieeexplore.ieee.org/document/4694847/},
	doi = {10.1109/TMM.2008.2007344},
	abstract = {Identifying the active speaker in a video of a distributed meeting can be very helpful for remote participants to understand the dynamics of the meeting. A straightforward application of such analysis is to stream a high resolution video of the speaker to the remote participants. In this paper, we present the challenges we met while designing a speaker detector for the Microsoft RoundTable distributed meeting device, and propose a novel boosting-based multimodal speaker detection (BMSD) algorithm. Instead of separately performing sound source localization (SSL) and multiperson detection (MPD) and subsequently fusing their individual results, the proposed algorithm fuses audio and visual information at feature level by using boosting to select features from a combined pool of both audio and visual features simultaneously. The result is a very accurate speaker detector with extremely high efﬁciency. In experiments that includes hundreds of real-world meetings, the proposed BMSD algorithm reduces the error rate of SSL-only approach by 24.6\%, and the SSL and MPD fusion approach by 20.9\%. To the best of our knowledge, this is the ﬁrst real-time multimodal speaker detection algorithm that is deployed in commercial products.},
	language = {en},
	number = {8},
	urldate = {2019-10-22},
	journal = {IEEE Transactions on Multimedia},
	author = {Zhang, Cha and Yin, Pei and Rui, Yong and Cutler, Ross and Viola, Paul and Sun, Xinding and Pinto, Nelson and Zhang, Zhengyou},
	month = dec,
	year = {2008},
	pages = {1541--1552},
	file = {Zhang et al. - 2008 - Boosting-Based Multimodal Speaker Detection for Di.pdf:/Users/rosscutler/Zotero/storage/IWHPXA2N/Zhang et al. - 2008 - Boosting-Based Multimodal Speaker Detection for Di.pdf:application/pdf},
}

@inproceedings{yoshimi_multimodal_2002,
	address = {New York, NY, USA},
	series = {{MULTIMEDIA} '02},
	title = {A {Multimodal} {Speaker} {Detection} and {Tracking} {System} for {Teleconferencing}},
	isbn = {978-1-58113-620-3},
	url = {http://doi.acm.org/10.1145/641007.641100},
	doi = {10.1145/641007.641100},
	abstract = {A serious problem in both audio and video conferencing facilities available today is the difficulty in determining who is speaking among a large number of participants. There is a strong need for developing meeting room infrastructure and teleconference facilities that improve the sense of presence and participation experienced in remote meetings. We present a distributed multimodal tracking system that uses multiple cameras and microphones to automatically select the current speaker among multiple meeting participants. The system actively obtains and transmits video showing a good view of the selected speaker. The tracking system is integrated into a web-based video conferencing application that connects seven meeting rooms around the globe. An important part of designing such a system is to determine sensor placement and configuration through systematic experiments in the actual rooms where the system is deployed.},
	urldate = {2019-10-22},
	booktitle = {Proceedings of the {Tenth} {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Yoshimi, Billibon H. and Pingali, Gopal S.},
	year = {2002},
	note = {event-place: Juan-les-Pins, France},
	keywords = {audio, collaboration, face, IP, localization, telepresence, video},
	pages = {427--428},
}

@article{kapralos_audiovisual_2003,
	title = {Audiovisual localization of multiple speakers in a video teleconferencing setting},
	volume = {13},
	copyright = {Copyright © 2003 Wiley Periodicals, Inc.},
	issn = {1098-1098},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ima.10045},
	doi = {10.1002/ima.10045},
	abstract = {Attending to multiple speakers in a video teleconferencing setting is a complex task. From a visual point of view, multiple speakers can occur at different locations and present radically different appearances. From an audio point of view, multiple speakers may be speaking at the same time, and background noise may make it difficult to localize sound sources without some a priori estimate of the sound source locations. This article presents a novel sensor and corresponding sensing algorithms to address the task of attending, simultaneously, to multiple speakers for video teleconferencing. A panoramic visual sensor is used to capture a 360° view of the speakers in the environment and from this view potential speakers are identified via a color histogram approach. A directional audio system based on beamforming is then used to confirm potential speakers and attend to them. Experimental evaluation of the sensor and its algorithms are presented including sample performance of the entire system in a teleconferencing setting. © 2003 Wiley Periodicals, Inc. Int J Imaging Syst Technol 13: 95–105, 2003; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/ima.10045},
	language = {en},
	number = {1},
	urldate = {2019-10-22},
	journal = {International Journal of Imaging Systems and Technology},
	author = {Kapralos, Bill and Jenkin, Michael R. M. and Milios, Evangelos},
	year = {2003},
	keywords = {teleconferencing, active vision sensor, face detection system},
	pages = {95--105},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/5K9ZH6XU/ima.html:text/html},
}

@article{chow_cognitive_2018,
	title = {Cognitive diversity and creativity in teams: the mediating roles of team learning and inclusion},
	issn = {1750-614X},
	shorttitle = {Cognitive diversity and creativity in teams},
	url = {https://www.emerald.com/insight/content/doi/10.1108/CMS-09-2017-0262/full/html},
	doi = {10.1108/CMS-09-2017-0262},
	abstract = {This paper aims to identify the mechanisms through which cognitive diversity affects creativity. It explores how and in what ways cognitive diversity affects team members by examining the mediating roles of team learning and inclusion.,Questionnaire survey data were collected from matched supervisor and employee pairs from a direct sales company in the health-care industry in China. The final sample consisted of 216 employees from 48 teams, with a response rate of 90 per cent. Each employee’s immediate supervisor rated his or her creativity and in-role performance.,The empirical results indicate that team learning and inclusion mediate the effect of cognitive diversity on creativity.,This study was conducted in a single organisation in China and used subjective self-reported measures.,The results suggest that diversity training reduces the negative consequences of team diversity and offer practical insights into the effectiveness of diversity management and the ways to create a diverse and inclusive workplace. The study should help human resource professionals to identify human resources strategies that stimulate an inclusive environment and leverage the benefits associated with higher levels of diversity.,The findings have significant implications for developing and maintaining social harmony.,The uniqueness of this study is its simultaneous investigation of diversity and inclusion and how they lead to creativity.},
	language = {en},
	urldate = {2019-10-21},
	journal = {Chinese Management Studies},
	author = {Chow, Irene Hau-Siu},
	month = jun,
	year = {2018},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/VVJCAIAW/html.html:text/html},
}

@article{shore_inclusion_2011,
	title = {Inclusion and {Diversity} in {Work} {Groups}: {A} {Review} and {Model} for {Future} {Research}},
	volume = {37},
	issn = {0149-2063, 1557-1211},
	shorttitle = {Inclusion and {Diversity} in {Work} {Groups}},
	url = {http://journals.sagepub.com/doi/10.1177/0149206310385943},
	doi = {10.1177/0149206310385943},
	abstract = {A great deal of research has focused on work group diversity, but management scholars have only recently focused on inclusion. As a result, the inclusion literature is still under development with limited agreement on the conceptual underpinnings of this construct. In this article, we first use Brewer’s optimal distinctiveness theory (ODT; 1991) to develop a definition of employee inclusion in the work group as involving the satisfaction of both belongingness and uniqueness needs. Building on our definition, we then present a framework of inclusion. Our framework is subsequently used as a basis for reviewing the inclusion and diversity literature. Following, we suggest potential contextual factors and outcomes associated with inclusion in order to guide future research.},
	language = {en},
	number = {4},
	urldate = {2019-10-21},
	journal = {Journal of Management},
	author = {Shore, Lynn M. and Randel, Amy E. and Chung, Beth G. and Dean, Michelle A. and Holcombe Ehrhart, Karen and Singh, Gangaram},
	month = jul,
	year = {2011},
	pages = {1262--1289},
	file = {Shore et al. - 2011 - Inclusion and Diversity in Work Groups A Review a.pdf:/Users/rosscutler/Zotero/storage/VRYZPJH9/Shore et al. - 2011 - Inclusion and Diversity in Work Groups A Review a.pdf:application/pdf},
}

@inproceedings{sommers_self-configuring_2004,
	address = {Taormina, Sicily, Italy},
	title = {Self-configuring network traffic generation},
	isbn = {978-1-58113-821-4},
	url = {http://portal.acm.org/citation.cfm?doid=1028788.1028798},
	doi = {10.1145/1028788.1028798},
	abstract = {The ability to generate repeatable, realistic network traﬃc is critical in both simulation and testbed environments. Trafﬁc generation capabilities to date have been limited to either simple sequenced packet streams typically aimed at throughput testing, or to application-speciﬁc tools focused on, for example, recreating representative HTTP requests. In this paper we describe Harpoon, a new application-independent tool for generating representative packet traﬃc at the IP ﬂow level. Harpoon generates TCP and UDP packet ﬂows that have the same byte, packet, temporal and spatial characteristics as measured at routers in live environments. Harpoon is distinguished from other tools that generate statistically representative traﬃc in that it can self-conﬁgure by automatically extracting parameters from standard Netﬂow logs or packet traces. We provide details on Harpoon’s architecture and implementation, and validate its capabilities in controlled laboratory experiments using conﬁgurations derived from ﬂow and packet traces gathered in live environments. We then demonstrate Harpoon’s capabilities in a router benchmarking experiment that compares Harpoon with commonly used throughput test methods. Our results show that the router subsystem load generated by Harpoon is signiﬁcantly diﬀerent, suggesting that this kind of test can provide important insights into how routers might behave under actual operating conditions.},
	language = {en},
	urldate = {2019-10-17},
	booktitle = {Proceedings of the 4th {ACM} {SIGCOMM} conference on {Internet} measurement  - {IMC} '04},
	publisher = {ACM Press},
	author = {Sommers, Joel and Barford, Paul},
	year = {2004},
	pages = {68},
	file = {Sommers and Barford - 2004 - Self-configuring network traffic generation.pdf:/Users/rosscutler/Zotero/storage/48CFLKF2/Sommers and Barford - 2004 - Self-configuring network traffic generation.pdf:application/pdf},
}

@article{ihm_towards_2006,
	title = {Towards understanding modern web traffic},
	abstract = {As Web sites move from relatively static displays of simple pages to rich media applications with heavy client-side interaction, the nature of the resulting Web trafﬁc changes as well. Understanding this change is necessary in order to improve response time, evaluate caching effectiveness, and design intermediary systems, such as ﬁrewalls, security analyzers, and reporting/management systems. Unfortunately, we have little understanding of the underlying nature of today’s Web trafﬁc.},
	language = {en},
	author = {Ihm, Sunghwan and Pai, Vivek S},
	year = {2006},
	pages = {18},
	file = {Ihm and Pai - 2006 - Towards understanding modern web traffic.pdf:/Users/rosscutler/Zotero/storage/4D3M2KL4/Ihm and Pai - 2006 - Towards understanding modern web traffic.pdf:application/pdf;Ihm and Pai - 2006 - Towards understanding modern web traffic.pdf:/Users/rosscutler/Zotero/storage/JX2SHL5D/Ihm and Pai - 2006 - Towards understanding modern web traffic.pdf:application/pdf},
}

@incollection{james_understanding_1993,
	address = {New York, NY, US},
	series = {Oxford studies in sociolinguistics},
	title = {Understanding gender differences in amount of talk: {A} critical review of research},
	isbn = {978-0-19-508193-0 978-0-19-508194-7},
	shorttitle = {Understanding gender differences in amount of talk},
	abstract = {examine the inconsistent research findings [from studies which appeared from 1951–1991] and attempt to demonstrate that they are, in fact, more consistent than they might initially appear / argue that in order to make sense of these findings, it is necessary to consider carefully the context and structure of social interaction within which gender differences are observed (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	booktitle = {Gender and conversational interaction},
	publisher = {Oxford University Press},
	author = {James, Deborah and Drakich, Janice},
	year = {1993},
	keywords = {Human Sex Differences, Literature Review, Oral Communication},
	pages = {281--312},
}

@article{leaper_meta-analytic_2007,
	title = {A {Meta}-{Analytic} {Review} of {Gender} {Variations} in {Adults}' {Language} {Use}: {Talkativeness}, {Affiliative} {Speech}, and {Assertive} {Speech}},
	volume = {11},
	issn = {1088-8683},
	shorttitle = {A {Meta}-{Analytic} {Review} of {Gender} {Variations} in {Adults}' {Language} {Use}},
	url = {https://doi.org/10.1177/1088868307302221},
	doi = {10.1177/1088868307302221},
	abstract = {Three separate sets of meta-analyses were conducted of studies testing for gender differences in adults' talkativeness, affiliative speech, and assertive speech. Across independent samples, statistically significant but negligible average effects sizes were obtained with all three language constructs: Contrary to the prediction, men were more talkative (d = —.14) than were women. As expected, men used more assertive speech (d = .09), whereas women used more affiliative speech (d = .12). In addition, 17 moderator variables were tested that included aspects of the interactive context (e.g., familiarity, gender composition, activity), measurement qualities (e.g., operational definition, observation length), and publication characteristics (e.g., author gender, publication source). Depending on particular moderators, more meaningful effect sizes (d {\textgreater} .2) occurred for each language construct. In addition, the direction of some gender differences was significantly reversed under particular conditions. The results are interpreted in relation to social-constructionist, socialization, and biological interpretations of gender-related variations in social behavior.},
	language = {en},
	number = {4},
	urldate = {2019-10-16},
	journal = {Personality and Social Psychology Review},
	author = {Leaper, Campbell and Ayres, Melanie M.},
	month = nov,
	year = {2007},
	pages = {328--363},
}

@misc{noauthor_glmnet:_nodate,
	title = {glmnet: {Lasso} and {Elastic}-{Net} {Regularized} {Generalized} {Linear} {Models}. https://cran.r-project.org/web/packages/glmnet/index.html},
	url = {https://cran.r-project.org/web/packages/glmnet/index.html},
}

@article{hofling_estimation_2009,
	title = {Estimation of {Sparse} {Binary} {Pairwise} {Markov} {Networks} using {Pseudo}-likelihoods. {Journal} of machine learning research},
	volume = {10},
	author = {Höfling, Holger and Tibshirani, Robert},
	year = {2009},
	pages = {883--906},
}

@inproceedings{zhao_monaural_2021,
	title = {Monaural {Speech} {Enhancement} with {Complex} {Convolutional} {Block} {Attention} {Module} and {Joint} {Time} {Frequency} {Losses}},
	doi = {10.1109/ICASSP39728.2021.9414569},
	abstract = {Deep complex U-Net structure and convolutional recurrent network (CRN) structure achieve state-of-the-art performance for monaural speech enhancement. Both deep complex U-Net and CRN are encoder and decoder structures with skip connections, which heavily rely on the representation power of the complex-valued convolutional layers. In this paper, we propose a complex convolutional block attention module (CCBAM) to boost the representation power of the complex-valued convolutional layers by constructing more informative features. The CCBAM is a lightweight and general module which can be easily integrated into any complex-valued convolutional layers. We integrate CCBAM with the deep complex U-Net and CRN to enhance their performance for speech enhancement. We further propose a mixed loss function to jointly optimize the complex models in both time-frequency (TF) domain and time domain. By integrating CCBAM and the mixed loss, we form a new end-to-end (E2E) complex speech enhancement framework. Ablation experiments and objective evaluations show the superior performance of the proposed approaches.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Zhao, Shengkui and Nguyen, Trung Hieu and Ma, Bin},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {deep learning, Speech enhancement, Convolution, Decoding, speech enhancement, Acoustics, attention mechanism, complex network, Conferences, DNSV2, Time-domain analysis, Time-frequency analysis},
	pages = {6648--6652},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/GJJST2KD/9414569.html:text/html;Submitted Version:/Users/rosscutler/Zotero/storage/UU6Q2YKD/Zhao et al. - 2021 - Monaural Speech Enhancement with Complex Convoluti.pdf:application/pdf},
}

@inproceedings{vuong_modulation-domain_2021,
	title = {A {Modulation}-{Domain} {Loss} for {Neural}-{Network}-{Based} {Real}-{Time} {Speech} {Enhancement}},
	doi = {10.1109/ICASSP39728.2021.9414965},
	abstract = {We describe a modulation-domain loss function for deep-learning-based speech enhancement systems. Learnable spectro-temporal receptive fields (STRFs) were adapted to optimize for a speaker identification task. The learned STRFs were then used to calculate a weighted mean-squared error (MSE) in the modulation domain for training a speech enhancement system. Experiments showed that adding the modulation-domain MSE to the MSE in the spectro-temporal domain substantially improved the objective prediction of speech quality and intelligibility for real-time speech enhancement systems without incurring additional computation during inference.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Vuong, Tyler and Xia, Yangyang and Stern, Richard M.},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Speech enhancement, Training, Measurement, Conferences, DNSV2, Time-frequency analysis, loss functions, Modulation, Real-time speech enhancement, Real-time systems, spectro-temporal receptive field},
	pages = {6643--6647},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/CTY7QGKQ/9414965.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/4MR8IJHH/Vuong et al. - 2021 - A Modulation-Domain Loss for Neural-Network-Based .pdf:application/pdf},
}

@inproceedings{li_densely_2021,
	title = {Densely {Connected} {Multi}-{Stage} {Model} with {Channel} {Wise} {Subband} {Feature} for {Real}-{Time} {Speech} {Enhancement}},
	doi = {10.1109/ICASSP39728.2021.9413967},
	abstract = {Research on single channel speech enhancement (SE) has a long tradition, but two main practical problems still remain unsolved. Firstly, it’s hard to balance between enhancement quality and computational efficiency, and low-latency always brings loss of quality. Secondly, enhancement in specific scenarios, such as singing and emotional speech, is also an intricate problem of conventional methods. In this paper, we propose a computationally efficient real-time speech enhancement network with densely connected multi-stage structures, which progressively enhances the channel-wise subband speech. The enhanced speech from earlier stage is used to guide the processing of deeper stage in order to obtain coarse to fine estimations. Besides, supervision is applied to all intermediate results in order to stabilize training and accelerate convergence. Moreover, an adaptive fine-tune step is utilized with some small datasets of specific scenarios, which achieves superb improvement under corresponding scenes. As a result, the proposed method achieves promising performance improvements in terms of speech quality and demonstrates robustness in complex scenarios. We submitt the proposed method to the deep noise suppression (DNS) challenge 2021, real-time denoising track, which was held by Microsoft. In the subjective evaluation, our system outperforms DNS-Challenge baseline by 0.14 points in terms of mean opinion score (MOS).},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Li, Jingdong and Luo, Dawei and Liu, Yun and Zhu, Yuanyuan and Li, Zhaoxia and Cui, Guohui and Tang, Wenqi and Chen, Wei},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Speech enhancement, Training, Estimation, speech enhancement, supervised learning, Noise reduction, Robustness, noise suppression, Conferences, DNSV2, Real-time systems, speech perceptual quality},
	pages = {6638--6642},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/DFG3X6K2/9413967.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/WV3UGWVR/Li et al. - 2021 - Densely Connected Multi-Stage Model with Channel W.pdf:application/pdf},
}

@inproceedings{hao_fullsubnet_2021,
	title = {Fullsubnet: {A} {Full}-{Band} and {Sub}-{Band} {Fusion} {Model} for {Real}-{Time} {Single}-{Channel} {Speech} {Enhancement}},
	shorttitle = {Fullsubnet},
	doi = {10.1109/ICASSP39728.2021.9414177},
	abstract = {This paper proposes a full-band and sub-band fusion model, named as FullSubNet, for single-channel real-time speech enhancement. Full-band and sub-band refer to the models that input full-band and sub-band noisy spectral feature, output full-band and sub-band speech target, respectively. The sub-band model processes each frequency independently. Its input consists of one frequency and several context frequencies. The output is the prediction of the clean speech target for the corresponding frequency. These two types of models have distinct characteristics. The full-band model can capture the global spectral context and the long-distance cross-band dependencies. However, it lacks the ability to modeling signal stationarity and attending the local spectral pattern. The sub-band model is just the opposite. In our proposed FullSubNet, we connect a pure full-band model and a pure sub-band model sequentially and use practical joint training to integrate these two types of models’ advantages. We conducted experiments on the DNS challenge (INTERSPEECH 2020) dataset to evaluate the proposed method. Experimental results show that full-band and sub-band information are complementary, and the FullSubNet can effectively integrate them. Besides, the performance of the FullSubNet also exceeds that of the top-ranked methods in the DNS Challenge (INTERSPEECH 2020).},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Hao, Xiang and Su, Xiangdong and Horaud, Radu and Li, Xiaofei},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Speech enhancement, Training, Noise measurement, Signal processing, Acoustics, Conferences, DNSV2, Real-time systems, Full-band and Sub-band Fusion, FullSubNet, Speech Enhancement, Sub-band},
	pages = {6633--6637},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/R6JMQVGN/9414177.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/SMRXNBXN/Hao et al. - 2021 - Fullsubnet A Full-Band and Sub-Band Fusion Model .pdf:application/pdf},
}

@inproceedings{li_icassp_2021,
	title = {{ICASSP} 2021 {Deep} {Noise} {Suppression} {Challenge}: {Decoupling} {Magnitude} and {Phase} {Optimization} with a {Two}-{Stage} {Deep} {Network}},
	shorttitle = {{ICASSP} 2021 {Deep} {Noise} {Suppression} {Challenge}},
	doi = {10.1109/ICASSP39728.2021.9414062},
	abstract = {It remains a tough challenge to recover the speech signals contaminated by various noises under real acoustic environments. To this end, we propose a novel system for denoising in the complicated applications, which is mainly comprised of two pipelines, namely a two-stage network and a post-processing module. The first pipeline is proposed to decouple the optimization problem w.r.t. magnitude and phase, i.e., only the magnitude is estimated in the first stage and both of them are further refined in the second stage. The second pipeline aims to further suppress the remaining unnatural distorted noise, which is demonstrated to sufficiently improve the subjective quality. In the ICASSP 2021 Deep Noise Suppression (DNS) Challenge, our submitted system ranked top-1 for the real-time track 1 in terms of Mean Opinion Score (MOS) with ITU-T P.808 framework.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Li, Andong and Liu, Wenzhe and Luo, Xiaoxue and Zheng, Chengshi and Li, Xiaodong},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Speech enhancement, Training, Noise reduction, real-time, Conferences, DNSV2, Acoustic distortion, Pipelines, post-processing, Signal processing algorithms, two-stage},
	pages = {6628--6632},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/R7ZLZ2YN/9414062.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/R3F9BCCD/Li et al. - 2021 - ICASSP 2021 Deep Noise Suppression Challenge Deco.pdf:application/pdf},
}

@inproceedings{reddy_icassp_2021,
	title = {{ICASSP} 2021 {Deep} {Noise} {Suppression} {Challenge}},
	doi = {10.1109/ICASSP39728.2021.9415105},
	abstract = {The Deep Noise Suppression (DNS) challenge is designed to foster innovation in the area of noise suppression to achieve superior perceptual speech quality. We recently organized a DNS challenge special session at INTERSPEECH 2020 where we open-sourced training and test datasets for researchers to train their noise suppression models. We also open-sourced a subjective evaluation framework and used the tool to evaluate and select the final winners. Many researchers from academia and industry made significant contributions to push the field forward. We also learned that as a research community, we still have a long way to go in achieving excellent speech quality in challenging noisy real-time conditions. In this challenge, we expanded both our training and test datasets. Clean speech in the training set has increased by 200\% with the addition of singing voice, emotion data, and non-English languages. The test set has increased by 100\% with the addition of singing, emotional, non-English (tonal and non-tonal) languages, and, personalized DNS test clips. There are two tracks with focus on (i) real-time denoising, and (ii) real-time personalized DNS. We present the challenge results at the end.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Reddy, Chandan K. A. and Dubey, Harishchandra and Gopal, Vishak and Cutler, Ross and Braun, Sebastian and Gamper, Hannes and Aichner, Robert and Srinivasan, Sriram},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Speech enhancement, Training, Noise reduction, Conferences, DNSV2, Speech Enhancement, Deep Noise Suppressor, DNS, Industries, Machine Learning, P.808, Perceptual Speech Quality, Technological innovation, Tools},
	pages = {6623--6627},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/8GAC25ME/9415105.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/IQ2AF96H/Reddy et al. - 2021 - ICASSP 2021 Deep Noise Suppression Challenge.pdf:application/pdf},
}

@inproceedings{strake_interspeech_2020,
	title = {{INTERSPEECH} 2020 {Deep} {Noise} {Suppression} {Challenge}: {A} {Fully} {Convolutional} {Recurrent} {Network} ({FCRN}) for {Joint} {Dereverberation} and {Denoising}},
	shorttitle = {{INTERSPEECH} 2020 {Deep} {Noise} {Suppression} {Challenge}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/strake20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2439},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Strake, Maximilian and Defraene, Bruno and Fluyt, Kristoff and Tirry, Wouter and Fingscheidt, Tim},
	month = oct,
	year = {2020},
	keywords = {DNSV1},
	pages = {2467--2471},
	file = {Strake et al. - 2020 - INTERSPEECH 2020 Deep Noise Suppression Challenge.pdf:/Users/rosscutler/Zotero/storage/LZGC7RY2/Strake et al. - 2020 - INTERSPEECH 2020 Deep Noise Suppression Challenge.pdf:application/pdf},
}

@inproceedings{isik_poconet_2020,
	title = {{PoCoNet}: {Better} {Speech} {Enhancement} with {Frequency}-{Positional} {Embeddings}, {Semi}-{Supervised} {Conversational} {Data}, and {Biased} {Loss}},
	shorttitle = {{PoCoNet}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/isik20_interspeech.html},
	doi = {10.21437/Interspeech.2020-3027},
	abstract = {Neural network applications generally beneﬁt from larger-sized models, but for current speech enhancement models, larger scale networks often suffer from decreased robustness to the variety of real-world use cases beyond what is encountered in training data. We introduce several innovations that lead to better large neural networks for speech enhancement. The novel PoCoNet architecture is a convolutional neural network that, with the use of frequency-positional embeddings, is able to more efﬁciently build frequency-dependent features in the early layers. A semi-supervised method helps increase the amount of conversational training data by pre-enhancing noisy datasets, improving performance on real recordings. A new loss function biased towards preserving speech quality helps the optimization better match human perceptual opinions on speech quality. Ablation experiments and objective and human opinion metrics show the beneﬁts of the proposed improvements.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Isik, Umut and Giri, Ritwik and Phansalkar, Neerad and Valin, Jean-Marc and Helwani, Karim and Krishnaswamy, Arvindh},
	month = oct,
	year = {2020},
	keywords = {DNSV1},
	pages = {2487--2491},
	file = {Isik et al. - 2020 - PoCoNet Better Speech Enhancement with Frequency-.pdf:/Users/rosscutler/Zotero/storage/XV4ZL2ZP/Isik et al. - 2020 - PoCoNet Better Speech Enhancement with Frequency-.pdf:application/pdf},
}

@inproceedings{valin_perceptually-motivated_2020,
	title = {A {Perceptually}-{Motivated} {Approach} for {Low}-{Complexity}, {Real}-{Time} {Enhancement} of {Fullband} {Speech}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/valin20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2730},
	abstract = {Over the past few years, speech enhancement methods based on deep learning have greatly surpassed traditional methods based on spectral subtraction and spectral estimation. Many of these new techniques operate directly in the the short-time Fourier transform (STFT) domain, resulting in a high computational complexity. In this work, we propose PercepNet, an efﬁcient approach that relies on human perception of speech by focusing on the spectral envelope and on the periodicity of the speech. We demonstrate high-quality, real-time enhancement of fullband (48 kHz) speech with less than 5\% of a CPU core.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Valin, Jean-Marc and Isik, Umut and Phansalkar, Neerad and Giri, Ritwik and Helwani, Karim and Krishnaswamy, Arvindh},
	month = oct,
	year = {2020},
	keywords = {DNSV1},
	pages = {2482--2486},
	file = {Valin et al. - 2020 - A Perceptually-Motivated Approach for Low-Complexi.pdf:/Users/rosscutler/Zotero/storage/5HK6VU9P/Valin et al. - 2020 - A Perceptually-Motivated Approach for Low-Complexi.pdf:application/pdf},
}

@inproceedings{westhausen_dual-signal_2020,
	title = {Dual-{Signal} {Transformation} {LSTM} {Network} for {Real}-{Time} {Noise} {Suppression}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/westhausen20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2631},
	abstract = {This paper introduces a dual-signal transformation LSTM network (DTLN) for real-time speech enhancement as part of the Deep Noise Suppression Challenge (DNS-Challenge). This approach combines a short-time Fourier transform (STFT) and a learned analysis and synthesis basis in a stacked-network approach with less than one million parameters. The model was trained on 500 h of noisy speech provided by the challenge organizers. The network is capable of real-time processing (one frame in, one frame out) and reaches competitive results. Combining these two types of signal transformations enables the DTLN to robustly extract information from magnitude spectra and incorporate phase information from the learned feature basis. The method shows state-of-the-art performance and outperforms the DNS-Challenge baseline by 0.24 points absolute in terms of the mean opinion score (MOS).},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Westhausen, Nils L. and Meyer, Bernd T.},
	month = oct,
	year = {2020},
	keywords = {DNSV1},
	pages = {2477--2481},
	file = {Westhausen and Meyer - 2020 - Dual-Signal Transformation LSTM Network for Real-T.pdf:/Users/rosscutler/Zotero/storage/FPZPGGRA/Westhausen and Meyer - 2020 - Dual-Signal Transformation LSTM Network for Real-T.pdf:application/pdf},
}

@inproceedings{hu_dccrn_2020,
	title = {{DCCRN}: {Deep} {Complex} {Convolution} {Recurrent} {Network} for {Phase}-{Aware} {Speech} {Enhancement}},
	shorttitle = {{DCCRN}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/hu20g_interspeech.html},
	doi = {10.21437/Interspeech.2020-2537},
	abstract = {Speech enhancement has beneﬁted from the success of deep learning in terms of intelligibility and perceptual quality. Conventional time-frequency (TF) domain methods focus on predicting TF-masks or speech spectrum, via a naive convolution neural network (CNN) or recurrent neural network (RNN). Some recent studies use complex-valued spectrogram as a training target but train in a real-valued network, predicting the magnitude and phase component or real and imaginary part, respectively. Particularly, convolution recurrent network (CRN) integrates a convolutional encoder-decoder (CED) structure and long short-term memory (LSTM), which has been proven to be helpful for complex targets. In order to train the complex target more effectively, in this paper, we design a new network structure simulating the complex-valued operation, called Deep Complex Convolution Recurrent Network (DCCRN), where both CNN and RNN structures can handle complex-valued operation. The proposed DCCRN models are very competitive over other previous networks, either on objective or subjective metric. With only 3.7M parameters, our DCCRN models submitted to the Interspeech 2020 Deep Noise Suppression (DNS) challenge ranked ﬁrst for the real-time-track and second for the non-real-time track in terms of Mean Opinion Score (MOS).},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Hu, Yanxin and Liu, Yun and Lv, Shubo and Xing, Mengtao and Zhang, Shimin and Fu, Yihui and Wu, Jian and Zhang, Bihong and Xie, Lei},
	month = oct,
	year = {2020},
	keywords = {DNSV1},
	pages = {2472--2476},
	file = {Hu et al. - 2020 - DCCRN Deep Complex Convolution Recurrent Network .pdf:/Users/rosscutler/Zotero/storage/NB6MZDHD/Hu et al. - 2020 - DCCRN Deep Complex Convolution Recurrent Network .pdf:application/pdf},
}

@inproceedings{reddy_interspeech_2020,
	title = {The {INTERSPEECH} 2020 {Deep} {Noise} {Suppression} {Challenge}: {Datasets}, {Subjective} {Testing} {Framework}, and {Challenge} {Results}},
	shorttitle = {The {INTERSPEECH} 2020 {Deep} {Noise} {Suppression} {Challenge}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/reddy20_interspeech.html},
	doi = {10.21437/Interspeech.2020-3038},
	abstract = {The INTERSPEECH 2020 Deep Noise Suppression (DNS) Challenge is intended to promote collaborative research in realtime single-channel Speech Enhancement aimed to maximize the subjective (perceptual) quality of the enhanced speech. A typical approach to evaluate the noise suppression methods is to use objective metrics on the test set obtained by splitting the original dataset. While the performance is good on the synthetic test set, often the model performance degrades significantly on real recordings. Also, most of the conventional objective metrics do not correlate well with subjective tests and lab subjective tests are not scalable for a large test set. In this challenge, we open-sourced a large clean speech and noise corpus for training the noise suppression models and a representative test set to real-world scenarios consisting of both synthetic and real recordings. We also open-sourced an online subjective test framework based on ITU-T P.808 for researchers to reliably test their developments. We evaluated the results using P.808 on a blind test set. The results and the key learnings from the challenge are discussed.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Reddy, Chandan K.A. and Gopal, Vishak and Cutler, Ross and Beyrami, Ebrahim and Cheng, Roger and Dubey, Harishchandra and Matusevych, Sergiy and Aichner, Robert and Aazami, Ashkan and Braun, Sebastian and Rana, Puneet and Srinivasan, Sriram and Gehrke, Johannes},
	month = oct,
	year = {2020},
	keywords = {DNSV1},
	pages = {2492--2496},
	file = {Reddy et al. - 2020 - The INTERSPEECH 2020 Deep Noise Suppression Challe.pdf:/Users/rosscutler/Zotero/storage/H6AHM24F/Reddy et al. - 2020 - The INTERSPEECH 2020 Deep Noise Suppression Challe.pdf:application/pdf},
}

@inproceedings{reddy_interspeech_2021,
	title = {{INTERSPEECH} 2021 {Deep} {Noise} {Suppression} {Challenge}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/reddy21_interspeech.html},
	doi = {10.21437/Interspeech.2021-1609},
	abstract = {The Deep Noise Suppression (DNS) challenge was designed to unify the research efforts in the area of noise suppression targeted for human perception. We recently organized a DNS challenge special session at INTERSPEECH 2020 and ICASSP 2021. We open-sourced training and test datasets for the wideband scenario along with a subjective evaluation framework based on ITU-T standard P.808, which was used to evaluate participants of the challenge. Many researchers from academia and industry made signiﬁcant contributions to push the ﬁeld forward, yet even the best noise suppressor was far from achieving superior speech quality in challenging scenarios. In this version of the challenge organized at INTERSPEECH 2021, we expanded our training and test datasets to accommodate fullband scenarios and challenging test conditions. We used ITU-T P.835 to evaluate the challenge winners as it gives additional information about the quality of processed speech and residual noise. The two tracks in this challenge focused on real-time denoising for (i) wideband, and (ii) fullband scenarios. We also made available a reliable non-intrusive objective speech quality metric for wideband called DNSMOS for the participants to use during their development phase.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Reddy, Chandan K.A. and Dubey, Harishchandra and Koishida, Kazuhito and Nair, Arun and Gopal, Vishak and Cutler, Ross and Braun, Sebastian and Gamper, Hannes and Aichner, Robert and Srinivasan, Sriram},
	month = aug,
	year = {2021},
	keywords = {DNSV3},
	pages = {2796--2800},
	file = {Reddy et al. - 2021 - INTERSPEECH 2021 Deep Noise Suppression Challenge.pdf:/Users/rosscutler/Zotero/storage/TBLU2GZ4/Reddy et al. - 2021 - INTERSPEECH 2021 Deep Noise Suppression Challenge.pdf:application/pdf},
}

@inproceedings{li_simultaneous_2021,
	title = {A {Simultaneous} {Denoising} and {Dereverberation} {Framework} with {Target} {Decoupling}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/li21g_interspeech.html},
	doi = {10.21437/Interspeech.2021-1137},
	abstract = {Background noise and room reverberation are regarded as two major factors to degrade the subjective speech quality. In this paper, we propose an integrated framework to address simultaneous denoising and dereverberation under complicated scenario environments. It adopts a chain optimization strategy and designs four sub-stages accordingly. In the ﬁrst two stages, we decouple the multi-task learning w.r.t. complex spectrum into magnitude and phase, and only implement noise and reverberation removal in the magnitude domain. Based on the estimated priors above, we further polish the spectrum in the third stage, where both magnitude and phase information are explicitly repaired with the residual learning. Due to the data mismatch and nonlinear effect of DNNs, the residual noise often exists in the DNN-processed spectrum. To resolve the problem, we adopt a light-weight algorithm as the post-processing module to capture and suppress the residual noise in the non-active regions. In the Interspeech 2021 Deep Noise Suppression (DNS) Challenge, our submitted system ranked top-1 for the real-time track in terms of Mean Opinion Score (MOS) with ITU-T P.835 framework.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Li, Andong and Liu, Wenzhe and Luo, Xiaoxue and Yu, Guochen and Zheng, Chengshi and Li, Xiaodong},
	month = aug,
	year = {2021},
	keywords = {DNSV3},
	pages = {2801--2805},
	file = {Li et al. - 2021 - A Simultaneous Denoising and Dereverberation Frame.pdf:/Users/rosscutler/Zotero/storage/D28KQWRG/Li et al. - 2021 - A Simultaneous Denoising and Dereverberation Frame.pdf:application/pdf},
}

@inproceedings{lv_dccrn_2021,
	title = {{DCCRN}+: {Channel}-{Wise} {Subband} {DCCRN} with {SNR} {Estimation} for {Speech} {Enhancement}},
	shorttitle = {{DCCRN}+},
	url = {https://www.isca-speech.org/archive/interspeech_2021/lv21_interspeech.html},
	doi = {10.21437/Interspeech.2021-1482},
	abstract = {Deep complex convolution recurrent network (DCCRN), which extends CRN with complex structure, has achieved superior performance in MOS evaluation in Interspeech 2020 deep noise suppression challenge (DNS2020). This paper further extends DCCRN with the following signiﬁcant revisions. We ﬁrst extend the model to sub-band processing where the bands are split and merged by learnable neural network ﬁlters instead of engineered FIR ﬁlters, leading to a faster noise suppressor trained in an end-to-end manner. Then the LSTM is further substituted with a complex TF-LSTM to better model temporal dependencies along both time and frequency axes. Moreover, instead of simply concatenating the output of each encoder layer to the input of the corresponding decoder layer, we use convolution blocks to ﬁrst aggregate essential information from the encoder output before feeding it to the decoder layers. We speciﬁcally formulate the decoder with an extra a priori SNR estimation module to maintain good speech quality while removing noise. Finally a post-processing module is adopted to further suppress the unnatural residual noise. The new model, named DCCRN+, has surpassed the original DCCRN as well as several competitive models in terms of PESQ and DNSMOS, and has achieved superior performance in the new Interspeech 2021 DNS challenge.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Lv, Shubo and Hu, Yanxin and Zhang, Shimin and Xie, Lei},
	month = aug,
	year = {2021},
	keywords = {DNSV3},
	pages = {2816--2820},
	file = {Lv et al. - 2021 - DCCRN+ Channel-Wise Subband DCCRN with SNR Estima.pdf:/Users/rosscutler/Zotero/storage/CXN6DIXC/Lv et al. - 2021 - DCCRN+ Channel-Wise Subband DCCRN with SNR Estima.pdf:application/pdf},
}

@inproceedings{le_dpcrn_2021,
	title = {{DPCRN}: {Dual}-{Path} {Convolution} {Recurrent} {Network} for {Single} {Channel} {Speech} {Enhancement}},
	shorttitle = {{DPCRN}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/le21b_interspeech.html},
	doi = {10.21437/Interspeech.2021-296},
	abstract = {The dual-path RNN (DPRNN) was proposed to more effectively model extremely long sequences for speech separation in the time domain. By splitting long sequences to smaller chunks and applying intra-chunk and inter-chunk RNNs, the DPRNN reached promising performance in speech separation with a limited model size. In this paper, we combine the DPRNN module with Convolution Recurrent Network (CRN) and design a model called Dual-Path Convolution Recurrent Network (DPCRN) for speech enhancement in the time-frequency domain. We replace the RNNs in the CRN with DPRNN modules, where the intra-chunk RNNs are used to model the spectrum pattern in a single frame and the inter-chunk RNNs are used to model the dependence between consecutive frames. With only 0.8M parameters, the submitted DPCRN model achieves an overall mean opinion score (MOS) of 3.57 in the wide band scenario track of the Interspeech 2021 Deep Noise Suppression (DNS) challenge. Evaluations on some other test sets also show the efﬁcacy of our model.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Le, Xiaohuai and Chen, Hongsheng and Chen, Kai and Lu, Jing},
	month = aug,
	year = {2021},
	keywords = {DNSV3},
	pages = {2811--2815},
	file = {Le et al. - 2021 - DPCRN Dual-Path Convolution Recurrent Network for.pdf:/Users/rosscutler/Zotero/storage/E6C2WPLT/Le et al. - 2021 - DPCRN Dual-Path Convolution Recurrent Network for.pdf:application/pdf},
}

@inproceedings{xu_deep_2021,
	title = {Deep {Noise} {Suppression} with {Non}-{Intrusive} {PESQNet} {Supervision} {Enabling} the {Use} of {Real} {Training} {Data}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/xu21h_interspeech.html},
	doi = {10.21437/Interspeech.2021-936},
	abstract = {Data-driven speech enhancement employing deep neural networks (DNNs) can provide state-of-the-art performance even in the presence of non-stationary noise. During the training process, most of the speech enhancement neural networks are trained in a fully supervised way with losses requiring noisy speech to be synthesized by clean speech and additive noise. However, in a real implementation, only the noisy speech mixture is available, which leads to the question, how such data could be advantageously employed in training. In this work, we propose an end-to-end non-intrusive PESQNet DNN which estimates perceptual evaluation of speech quality (PESQ) scores, allowing a reference-free loss for real data. As a further novelty, we combine the PESQNet loss with denoising and dereverberation loss terms, and train a complex mask-based fully convolutional recurrent neural network (FCRN) in a “weakly” supervised way, each training cycle employing some synthetic data, some real data, and again synthetic data to keep the PESQNet up-to-date. In a subjective listening test, our proposed framework outperforms the Interspeech 2021 Deep Noise Suppression (DNS) Challenge baseline overall by 0.09 MOS points and in particular by 0.45 background noise MOS points.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Xu, Ziyi and Strake, Maximilian and Fingscheidt, Tim},
	month = aug,
	year = {2021},
	keywords = {DNSV3},
	pages = {2806--2810},
	file = {Xu et al. - 2021 - Deep Noise Suppression with Non-Intrusive PESQNet .pdf:/Users/rosscutler/Zotero/storage/ZVGZHN5N/Xu et al. - 2021 - Deep Noise Suppression with Non-Intrusive PESQNet .pdf:application/pdf},
}

@inproceedings{zhang_low-delay_2021,
	title = {Low-{Delay} {Speech} {Enhancement} {Using} {Perceptually} {Motivated} {Target} and {Loss}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/zhang21t_interspeech.html},
	doi = {10.21437/Interspeech.2021-1410},
	abstract = {Speech enhancement approaches based on deep neural network have outperformed the traditional signal processing methods. This paper presents a low-delay speech enhancement method that employs a new perceptually motivated training target and loss function. The proposed approach can achieve similar speech enhancement performance compared to the state-of-theart approaches, but with significantly less latency and computational complexities. Judged by the MOS tests conducted by the INTERSPEECH 2021 Deep Noise Suppression Challenge organizer, the proposed method is ranked the 2nd place for Background Noise MOS, and the 6th place for overall MOS.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Zhang, Xu and Ren, Xinlei and Zheng, Xiguang and Chen, Lianwu and Zhang, Chen and Guo, Liang and Yu, Bing},
	month = aug,
	year = {2021},
	keywords = {DNSV3},
	pages = {2826--2830},
	file = {Zhang et al. - 2021 - Low-Delay Speech Enhancement Using Perceptually Mo.pdf:/Users/rosscutler/Zotero/storage/PKD58VSI/Zhang et al. - 2021 - Low-Delay Speech Enhancement Using Perceptually Mo.pdf:application/pdf},
}

@inproceedings{oostermeijer_lightweight_2021,
	title = {Lightweight {Causal} {Transformer} with {Local} {Self}-{Attention} for {Real}-{Time} {Speech} {Enhancement}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/oostermeijer21_interspeech.html},
	doi = {10.21437/Interspeech.2021-668},
	abstract = {In this paper, we describe a novel speech enhancement transformer architecture. The model uses local causal selfattention, which makes it lightweight and therefore particularly well-suited for real-time speech enhancement in computation resource-limited environments. In addition, we provide several ablation studies that focus on different parts of the model and the loss function to ﬁgure out which modiﬁcations yield best improvements. Using this knowledge, we propose a ﬁnal version of our architecture, that we sent in to the INTERSPEECH 2021 DNS Challenge, where it achieved competitive results, despite using only 2\% of the maximally allowed computation. Furthermore, we performed experiments to compare it with with LSTM and CNN models, that had 127\% and 257\% more parameters, respectively. Despite this difference in model size, we achieved signiﬁcant improvements on the considered speech quality and intelligibility measures.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Oostermeijer, Koen and Wang, Qing and Du, Jun},
	month = aug,
	year = {2021},
	keywords = {DNSV3},
	pages = {2831--2835},
	file = {Oostermeijer et al. - 2021 - Lightweight Causal Transformer with Local Self-Att.pdf:/Users/rosscutler/Zotero/storage/QHHBMI6R/Oostermeijer et al. - 2021 - Lightweight Causal Transformer with Local Self-Att.pdf:application/pdf},
}

@inproceedings{zhang_dbnet_2021,
	title = {{DBNet}: {A} {Dual}-{Branch} {Network} {Architecture} {Processing} on {Spectrum} and {Waveform} for {Single}-{Channel} {Speech} {Enhancement}},
	shorttitle = {{DBNet}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/zhang21s_interspeech.html},
	doi = {10.21437/Interspeech.2021-1042},
	abstract = {In real acoustic environment, speech enhancement is an arduous task to improve the quality and intelligibility of speech interfered by background noise and reverberation. Over the past years, deep learning has shown great potential on speech enhancement. In this paper, we propose a novel real-time framework called DBNet which is a dual-branch structure with alternate interconnection. Each branch incorporates an encoderdecoder architecture with skip connections. The two branches are responsible for spectrum and waveform modeling, respectively. A bridge layer is adopted to exchange information between the two branches. Systematic evaluation and comparison show that the proposed system substantially outperforms related algorithms under very challenging environments. And in INTERSPEECH 2021 Deep Noise Suppression (DNS) challenge, the proposed system ranks the top 8 in real-time track 1 in terms of the Mean Opinion Score (MOS) of the ITU-T P.835 framework.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Zhang, Kanghao and He, Shulin and Li, Hao and Zhang, Xueliang},
	month = aug,
	year = {2021},
	keywords = {DNSV3},
	pages = {2821--2825},
	file = {Zhang et al. - 2021 - DBNet A Dual-Branch Network Architecture Processi.pdf:/Users/rosscutler/Zotero/storage/7NJBKKVW/Zhang et al. - 2021 - DBNet A Dual-Branch Network Architecture Processi.pdf:application/pdf},
}

@inproceedings{li_online_2020,
	title = {Online {Monaural} {Speech} {Enhancement} {Using} {Delayed} {Subband} {LSTM}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/li20z_interspeech.html},
	doi = {10.21437/Interspeech.2020-2091},
	abstract = {This paper proposes a delayed subband LSTM network for online monaural (single-channel) speech enhancement. The proposed method is developed in the short time Fourier transform (STFT) domain. Online processing requires frame-byframe signal reception and processing. A paramount feature of the proposed method is that the same LSTM is used across frequencies, which drastically reduces the number of network parameters, the amount of training data and the computational burden. Training is performed in a subband manner: the input consists of a frequency together with a few context frequencies. The network learns a speech-to-noise discriminative function relying on the signal stationarity and on the local spectral pattern, based on which it predicts a clean-speech mask at each frequency. To exploit future information, i.e. a look-ahead strategy, we propose an output-delayed subband LSTM network, which allows the unidirectional forward network to use a few future frames to process the current frame. We leverage the proposed method to participate to the DNS real-time speech enhancement challenge. Experiments with the DNS dataset show that the proposed method achieves better performancemeasuring scores than the DNS baseline method, which learns the full-band spectra using a gated recurrent unit network.},
	language = {en},
	urldate = {2021-10-24},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Li, Xiaofei and Horaud, Radu},
	month = oct,
	year = {2020},
	keywords = {DNSV1},
	pages = {2462--2466},
	file = {Li and Horaud - 2020 - Online Monaural Speech Enhancement Using Delayed S.pdf:/Users/rosscutler/Zotero/storage/47TPYQUP/Li and Horaud - 2020 - Online Monaural Speech Enhancement Using Delayed S.pdf:application/pdf},
}

@article{bhattacherjee_principles_nodate,
	title = {Principles of {Dataset} {Versioning}: {Exploring} the {Recreation}/{Storage} {Tradeoff}},
	abstract = {The relative ease of collaborative data science and analysis has led to a proliferation of many thousands or millions of versions of the same datasets in many scientiﬁc and commercial domains, acquired or constructed at various stages of data analysis across many users, and often over long periods of time. Managing, storing, and recreating these dataset versions is a non-trivial task. The fundamental challenge here is the storage-recreation trade-off: the more storage we use, the faster it is to recreate or retrieve versions, while the less storage we use, the slower it is to recreate or retrieve versions. Despite the fundamental nature of this problem, there has been a surprisingly little amount of work on it. In this paper, we study this trade-off in a principled manner: we formulate six problems under various settings, trading off these quantities in various ways, demonstrate that most of the problems are intractable, and propose a suite of inexpensive heuristics drawing from techniques in delayconstrained scheduling, and spanning tree literature, to solve these problems. We have built a prototype version management system, that aims to serve as a foundation to our DATAHUB system for facilitating collaborative data science. We demonstrate, via extensive experiments, that our proposed heuristics provide efﬁcient solutions in practical dataset versioning scenarios.},
	language = {en},
	author = {Bhattacherjee, Souvik and Chavan, Amit and Huang, Silu and Deshpande, Amol and Parameswaran, Aditya},
	pages = {12},
	file = {Bhattacherjee et al. - Principles of Dataset Versioning Exploring the Re.pdf:/Users/rosscutler/Zotero/storage/L4V4WYYG/Bhattacherjee et al. - Principles of Dataset Versioning Exploring the Re.pdf:application/pdf},
}

@misc{noauthor_data_nodate,
	title = {Data {Version} {Control} · {DVC}},
	url = {https://dvc.org/},
	abstract = {Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.},
	language = {en},
	urldate = {2021-10-02},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/NZRQXJA2/dvc.org.html:text/html},
}

@misc{noauthor_pachyderm_nodate,
	title = {Pachyderm},
	url = {https://www.pachyderm.com/},
	abstract = {Data Lineage with End-to-End Pipelines on Kubernetes, engineered for the enterprise. And… It’s open source!},
	language = {en-us},
	urldate = {2021-10-04},
}

@misc{noauthor_comparing_2020,
	title = {Comparing {Data} {Version} {Control} {Tools} - 2020},
	url = {https://dagshub.com/blog/data-version-control-tools/},
	abstract = {Data versioning is one of the keys to automating a team's machine learning model development. While it can be very complicated if your team attempts to develop its own system to manage the process, this doesn’t need to be the case.},
	language = {en},
	urldate = {2021-10-04},
	journal = {DAGsHub Blog},
	month = oct,
	year = {2020},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/BXP8GTV2/data-version-control-tools.html:text/html},
}

@misc{noauthor_azure_nodate,
	title = {Azure {Machine} {Learning} announces output dataset ({Preview}) {\textbar} {Azure} updates {\textbar} {Microsoft} {Azure}},
	url = {https://azure.microsoft.com/en-us/updates/azure-machine-learning-announces-output-dataset-preview/},
	abstract = {With the new output datasets capability, write back to cloud storage and configure and register output data.},
	language = {en},
	urldate = {2021-10-04},
}

@misc{noauthor_weights_nodate,
	title = {Weights \& {Biases}},
	url = {https://docs.wandb.ai/},
	abstract = {Machine learning experiment tracking, dataset versioning, and model evaluation},
	urldate = {2021-10-04},
}

@misc{hu_public_2021,
	title = {[{Public} {Preview}] output-dataset},
	url = {https://github.com/MayMSFT/output-dataset},
	urldate = {2021-10-04},
	author = {Hu, May},
	month = apr,
	year = {2021},
	note = {original-date: 2020-06-09T18:54:39Z},
}

@misc{noauthor_git_nodate,
	title = {Git {Large} {File} {Storage}},
	url = {https://git-lfs.github.com/},
	abstract = {Git Large File Storage (LFS) replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server like GitHub.com or GitHub Enterprise.},
	urldate = {2021-10-04},
	journal = {Git Large File Storage},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/QNB3AH73/git-lfs.github.com.html:text/html},
}

@misc{noauthor_first_nodate,
	title = {First {Impressions} of {Data} {Science} {Version} {Control} ({DVC})},
	url = {https://christophergs.com/machine%20learning/2019/05/13/first-impressions-of-dvc/},
	urldate = {2021-10-04},
	file = {First Impressions of Data Science Version Control (DVC):/Users/rosscutler/Zotero/storage/SUSP53S5/first-impressions-of-dvc.html:text/html},
}

@misc{noauthor_dolt_nodate,
	title = {Dolt},
	url = {https://docs.dolthub.com/},
	urldate = {2021-10-04},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/HBUVR8AZ/docs.dolthub.com.html:text/html},
}

@misc{noauthor_delta_nodate,
	title = {Delta {Lake} - {Reliable} {Data} {Lakes} at {Scale}},
	url = {https://delta.io/},
	abstract = {Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.},
	language = {en-US},
	urldate = {2021-10-04},
	journal = {Delta Lake},
}

@misc{noauthor_dataset_nodate,
	title = {Dataset versioning - {Azure} {Machine} {Learning}},
	url = {https://docs.microsoft.com/en-us/azure/machine-learning/how-to-version-track-datasets},
	abstract = {Learn how to version machine learning datasets and how versioning works with machine learning pipelines.},
	language = {en-us},
	urldate = {2021-10-04},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/YU86MHEC/how-to-version-track-datasets.html:text/html},
}

@misc{noauthor_atomic_nodate,
	title = {Atomic {Versioned} {Data} {Lake}},
	url = {https://lakefs.io/},
	abstract = {lakeFS is an open-source tool that transforms your object storage to Git-like repositories. Start managing data the way you manage your code.},
	language = {en-US},
	urldate = {2021-10-04},
	journal = {LakeFS},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/IT2E4QDC/lakefs.io.html:text/html},
}

@inproceedings{naderi_subjective_2021,
	title = {Subjective {Evaluation} of {Noise} {Suppression} {Algorithms} in {Crowdsourcing}},
	abstract = {The quality of the speech communication systems, which include noise suppression algorithms, are typically evaluated in laboratory experiments according to the ITU-T Rec. P.835, in which participants rate background noise, speech signal, and overall quality separately. This paper introduces an open-source toolkit for conducting subjective quality evaluation of noise suppressed speech in crowdsourcing. We followed the ITU-T Rec. P.835, and P.808 and highly automate the process to prevent moderator's error. To assess the validity of our evaluation method, we compared the Mean Opinion Scores (MOS), calculate using ratings collected with our implementation, and the MOS values from a standard laboratory experiment conducted according to the ITU-T Rec P.835. Results show a high validity in all three scales namely background noise, speech signal and overall quality (average PCC = 0.961). Results of a round-robin test (N=5) showed that our implementation is also a highly reproducible evaluation method (PCC=0.99). Finally, we used our implementation in the INTERSPEECH 2021 Deep Noise Suppression Challenge as the primary evaluation metric, which demonstrates it is practical to use at scale. The results are analyzed to determine why the overall performance was the best in terms of background noise and speech quality.},
	urldate = {2021-07-02},
	booktitle = {{INTERSPEECH}},
	author = {Naderi, Babak and Cutler, Ross},
	year = {2021},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/Z6G6XMTL/Naderi and Cutler - 2021 - Subjective Evaluation of Noise Suppression Algorit.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/W2BGXY2Y/2010.html:text/html},
}

@inproceedings{cutler_crowdsourcing_2020,
	title = {Crowdsourcing approach for subjective evaluation of echo impairment},
	abstract = {The quality of acoustic echo cancellers (AECs) in real-time communication systems is typically evaluated using objective metrics like ERLE and PESQ, and less commonly with lab-based subjective tests like ITU-T Rec. P.831. We will show that these objective measures are not well correlated to subjective measures. We then introduce an open-source crowdsourcing approach for subjective evaluation of echo impairment which can be used to evaluate the performance of AECs. We provide a study that shows this tool is highly reproducible. This new tool has been recently used in the ICASSP 2021 AEC Challenge which made the challenge possible to do quickly and cost effectively.},
	urldate = {2020-12-08},
	booktitle = {{ICASSP}},
	author = {Cutler, Ross and Naderi, Babak and Loide, Markus and Sootla, Sten and Saabas, Ando},
	year = {2020},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/BN2FR679/Cutler et al. - 2020 - Crowdsourcing approach for subjective evaluation o.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/D2WGGKAT/2010.html:text/html},
}

@article{noauthor_itu-t_2008,
	title = {{ITU}-{T} {Recommendation} {P}.910 {Subjective} video quality assessment methods for multimedia applications},
	journal = {International Telecommunication Union},
	year = {2008},
}

@article{naderi_open_2020,
	title = {An {Open} {Source} {Implementation} of {ITU}-{T} {Recommendation} {P}.808 with {Validation}},
	doi = {10.21437/Interspeech.2020-2665},
	abstract = {The ITU-T Recommendation P.808 provides a crowdsourcing approach for conducting a subjective assessment of speech quality using the Absolute Category Rating (ACR) method. We provide an open-source implementation of the ITU-T Rec. P.808 that runs on the Amazon Mechanical Turk platform. We extended our implementation to include Degradation Category Ratings (DCR) and Comparison Category Ratings (CCR) test methods. We also significantly speed up the test process by integrating the participant qualification step into the main rating task compared to a two-stage qualification and rating solution. We provide program scripts for creating and executing the subjective test, and data cleansing and analyzing the answers to avoid operational errors. To validate the implementation, we compare the Mean Opinion Scores (MOS) collected through our implementation with MOS values from a standard laboratory experiment conducted based on the ITU-T Rec. P.800. We also evaluate the reproducibility of the result of the subjective speech quality assessment through crowdsourcing using our implementation. Finally, we quantify the impact of parts of the system designed to improve the reliability: environmental tests, gold and trapping questions, rating patterns, and a headset usage test.},
	journal = {INTERSPEECH},
	author = {Naderi, Babak and Cutler, Ross},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {2862--2866},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/CP6GKFHH/Naderi and Cutler - 2020 - An Open source Implementation of ITU-T Recommendat.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/FGU4376X/2005.html:text/html},
}

@article{pascual_adversarial_2021,
	title = {Adversarial {Auto}-{Encoding} for {Packet} {Loss} {Concealment}},
	url = {http://arxiv.org/abs/2107.03100},
	abstract = {Communication technologies like voice over IP operate under constrained real-time conditions, with voice packets being subject to delays and losses from the network. In such cases, the packet loss concealment (PLC) algorithm reconstructs missing frames until a new real packet is received. Recently, autoregressive deep neural networks have been shown to surpass the quality of signal processing methods for PLC, specially for long-term predictions beyond 60 ms. In this work, we propose a non-autoregressive adversarial auto-encoder, named PLAAE, to perform real-time PLC in the waveform domain. PLAAE has a causal convolutional structure, and it learns in an auto-encoder fashion to reconstruct signals with gaps, with the help of an adversarial loss. During inference, it is able to predict smooth and coherent continuations of such gaps in a single feed-forward step, as opposed to autoregressive models. Our evaluation highlights the superiority of PLAAE over two classic PLCs and two deep autoregressive models in terms of spectral and intonation reconstruction, perceptual quality, and intelligibility.},
	urldate = {2021-10-01},
	journal = {arXiv:2107.03100 [cs, eess]},
	author = {Pascual, Santiago and Serrà, Joan and Pons, Jordi},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.03100},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/X94VLCYE/Pascual et al. - 2021 - Adversarial Auto-Encoding for Packet Loss Concealm.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/Q4STGIL2/2107.html:text/html},
}

@inproceedings{nair_cascaded_2021,
	title = {Cascaded {Time} + {Time}-{Frequency} {Unet} {For} {Speech} {Enhancement}: {Jointly} {Addressing} {Clipping}, {Codec} {Distortions}, {And} {Gaps}},
	shorttitle = {Cascaded {Time} + {Time}-{Frequency} {Unet} {For} {Speech} {Enhancement}},
	doi = {10.1109/ICASSP39728.2021.9414721},
	abstract = {Speech enhancement aims to improve speech quality by eliminating noise and distortions. While most speech enhancement methods address signal independent additive sources of noise, several degradations to speech signals are signal dependent and non-additive, like speech clipping, codec distortions, and gaps in speech. In this work, we first systematically study and achieve state of the art results on each of these three distortions individually. Next, we demonstrate a neural network pipeline that cascades a time domain convolutional neural network with a time-frequency domain convolutional neural network to address all three distortions jointly. We observe that such a cascade achieves good performance while also keeping the action of each neural network component interpretable.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Nair, Arun Asokan and Koishida, Kazuhito},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Speech enhancement, Neural networks, Degradation, convolutional neural networks, Time-frequency analysis, Acoustic distortion, Pipelines, codec distortion removal, Codecs, declipping, gap filling, Speech coding},
	pages = {7153--7157},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/D573T4L4/9414721.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/NRILBKN3/Nair and Koishida - 2021 - Cascaded Time + Time-Frequency Unet For Speech Enh.pdf:application/pdf},
}

@article{mohamed_deep_2020,
	title = {On {Deep} {Speech} {Packet} {Loss} {Concealment}: {A} {Mini}-{Survey}},
	shorttitle = {On {Deep} {Speech} {Packet} {Loss} {Concealment}},
	url = {http://arxiv.org/abs/2005.07794},
	abstract = {Packet-loss is a common problem in data transmission, using Voice over IP. The problem is an old problem, and there has been a variety of classical approaches that were developed to overcome this problem. However, with the rise of deep learning and generative models like Generative Adversarial Networks and Autoencoders, a new avenue has emerged for attempting to solve packet-loss using deep learning, by generating replacements for lost packets. In this mini-survey, we review all the literature we found to date, that attempt to solve the packet-loss in speech using deep learning methods. Additionally, we briefly review how the problem of packet-loss in a realistic setting is modelled, and how to evaluate Packet Loss Concealment techniques. Moreover, we review a few modern deep learning techniques in related domains that have shown promising results. These techniques shed light on future potentially better solutions for PLC and additional challenges that need to be considered simultaneously with packet-loss.},
	urldate = {2021-10-01},
	journal = {arXiv:2005.07794 [cs, eess]},
	author = {Mohamed, Mostafa M. and Nessiem, Mina A. and Schuller, Björn W.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.07794},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/7EN267AY/Mohamed et al. - 2020 - On Deep Speech Packet Loss Concealment A Mini-Sur.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/AAUC7CRH/2005.html:text/html},
}

@article{thirunavukkarasu_survey_2015,
	title = {A survey on {VoIP} packet loss techniques},
	volume = {14},
	issn = {1754-3916},
	url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJCNDS.2015.066029},
	doi = {10.1504/IJCNDS.2015.066029},
	abstract = {Voice over internet protocol (VoIP) is a group of hardware and software that facilitates people to utilise the internet as the transmission medium for telephone calls by transmitting voice data in packets using IP instead of using conventional circuit transmissions of the public switched telephone network (PSTN). VoIP is also considered as internet telephony, IP telephony or voice over the internet (VOI). VoIP communication practically needs a high-speed internet connection for consistent functionality. There are several internet telephony applications existing at present. VoIP has established necessities for data packets to arrive at their target in a more controlled time. The techniques are used to quantifying the survey of VoIP. The packet loss techniques are proposed to service the VoIP.},
	number = {1},
	urldate = {2021-10-01},
	journal = {International Journal of Communication Networks and Distributed Systems},
	author = {Thirunavukkarasu, E.s. and Karthikeyan, E.},
	month = jan,
	year = {2015},
	note = {Publisher: Inderscience Publishers},
	keywords = {VoIP, VOI, voice over internet protocol, voice over the internet, VoIP packet loss},
	pages = {106--116},
	file = {Thirunavukkarasu and Karthikeyan - 2015 - A survey on VoIP packet loss techniques.pdf:/Users/rosscutler/Zotero/storage/NG7HS8D6/Thirunavukkarasu and Karthikeyan - 2015 - A survey on VoIP packet loss techniques.pdf:application/pdf},
}

@inproceedings{lin_time-domain_2021,
	title = {A {Time}-{Domain} {Convolutional} {Recurrent} {Network} for {Packet} {Loss} {Concealment}},
	doi = {10.1109/ICASSP39728.2021.9413595},
	abstract = {Packet loss may affect a wide range of applications that use voice over IP (VoIP), e.g. video conferencing. In this paper, we investigate a time-domain convolutional recurrent network (CRN) for online packet loss concealment. The CRN comprises a convolutional encoder-decoder structure and long short-term memory (LSTM) layers, which have been shown to be suitable for real-time speech enhancement applications. Moreover, we propose lookahead and masked training to further improve the performance of the CRN framework. Experimental results show that the proposed system outperforms a baseline system using only LSTM layers in terms of two objective metrics – perceptual evaluation of speech quality (PESQ) and short-term objective intelligibility (STOI); it also reduces the word error rate (WER) more than the baseline when used as a frontend for speech recognition. The advantage of the proposed system is also verified in a subjective evaluation by the mean opinion score (MOS).},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Lin, Ju and Wang, Yun and Kalgaonkar, Kaustubh and Keren, Gil and Zhang, Didi and Fuegen, Christian},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Speech enhancement, Training, Packet loss, Convolution, Streaming media, long short-term memory, neural networks, Error analysis, packet Loss concealment, Speech recognition, Voice over IP},
	pages = {7148--7152},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/5E5XWUEE/9413595.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/R3FMWGQM/Lin et al. - 2021 - A Time-Domain Convolutional Recurrent Network for .pdf:application/pdf},
}

@inproceedings{stimberg_waveneteq_2020,
	title = {{WaveNetEQ} — {Packet} {Loss} {Concealment} with {WaveRNN}},
	doi = {10.1109/IEEECONF51394.2020.9443419},
	abstract = {We present WaveNetEQ, a novel packet loss concealment method based on a WaveRNN architecture. The model is conditioned on a log-mel spectrogram of the past signal to extract slow moving features, like voice characteristics and prosody and achieves significantly better quality than pattern based methods for medium and long term packet loss. Through aggressive sparsification the model is efficient enough to run on a phone.},
	booktitle = {2020 54th {Asilomar} {Conference} on {Signals}, {Systems}, and {Computers}},
	author = {Stimberg, Florian and Narest, Alex and Bazzica, Alessio and Kolmodin, Lennart and Barrera González, Pablo and Sharonova, Olga and Lundin, Henrik and Walters, Thomas C.},
	month = nov,
	year = {2020},
	note = {ISSN: 2576-2303},
	keywords = {Feature extraction, Neural networks, Training, Packet loss, Streaming media, Computer architecture, Production systems},
	pages = {672--676},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/I4QREJY6/9443419.html:text/html;Stimberg et al. - 2020 - WaveNetEQ — Packet Loss Concealment with WaveRNN.pdf:/Users/rosscutler/Zotero/storage/W3DPUE6C/Stimberg et al. - 2020 - WaveNetEQ — Packet Loss Concealment with WaveRNN.pdf:application/pdf},
}

@article{bhardwaj_datahub_nodate,
	title = {{DataHub}: {Collaborative} {Data} {Science} \& {Dataset} {Version} {Management} at {Scale}},
	abstract = {Relational databases have limited support for data collaboration, where teams collaboratively curate and analyze large datasets. Inspired by software version control systems like git, we propose (a) a dataset version control system, giving users the ability to create, branch, merge, difference and search large, divergent collections of datasets, and (b) a platform, DATAHUB, that gives users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale.},
	language = {en},
	author = {Bhardwaj, Anant and Bhattacherjee, Souvik and Chavan, Amit and Deshpande, Amol and Elmore, Aaron J and Madden, Samuel and Parameswaran, Aditya},
	pages = {7},
	file = {Bhardwaj et al. - DataHub Collaborative Data Science & Dataset Vers.pdf:/Users/rosscutler/Zotero/storage/CECKSH5L/Bhardwaj et al. - DataHub Collaborative Data Science & Dataset Vers.pdf:application/pdf},
}

@article{hans_kinesics_2015,
	title = {Kinesics, {Haptics} and {Proxemics}: {Aspects} of {Non} -{Verbal} {Communication}},
	volume = {20},
	number = {2},
	journal = {IOSR Journal Of Humanities And Social Science},
	author = {Hans, Anjali and Hans, Emmanuel},
	year = {2015},
}

@inproceedings{cutler_interspeech_2021,
	title = {{INTERSPEECH} 2021 {Acoustic} {Echo} {Cancellation} {Challenge}},
	abstract = {The INTERSPEECH 2021 Acoustic Echo Cancellation Challenge is intended to stimulate research in the area of acoustic echo cancellation (AEC), which is an important part of speech enhancement and still a top issue in audio communication. Many recent AEC studies report good performance on synthetic datasets where the training and testing data may come from the same underlying distribution. However , AEC performance often degrades significantly on real recordings. Also, most of the conventional objective metrics such as echo return loss enhancement and perceptual evaluation of speech quality do not correlate well with subjective speech quality tests in the presence of background noise and reverberation found in realistic environments. In this challenge, we open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 5,000 real audio devices and human speakers in real environments, as well as a synthetic dataset. We also open source an online subjective test framework and provide an online objective metric service for researchers to quickly test their results. The winners of this challenge are selected based on the average Mean Opinion Score achieved across all different single talk and double talk scenarios.},
	booktitle = {{INTERSPEECH}},
	author = {Cutler, Ross and Saabas, Ando and Parnamaa, Tanel and Loide, Markus and Sootla, Sten and Purin, Marju and Gamper, Hannes and Braun, Sebastian and Sorensen, Karsten and Aichner, Robert and Srinivasan, Sriram},
	month = jun,
	year = {2021},
}

@inproceedings{tang_understanding_2021,
	title = {Understanding the {Telework} {Experience} of {People} with {Disabilities}},
	url = {https://www.microsoft.com/en-us/research/publication/understanding-the-telework-experience-of-people-with-disabilities-2/},
	abstract = {To understand the lived experience of how people with disabilities telework in the United States, 25 people were interviewed. The participants included people who are blind or low vision, deaf or hard of hearing, neurodiverse, have limited mobility/dexterity, and have chronic health issues. The interviews focused on how they used video calling, screen sharing, and collaborative editing technologies to accomplish their telework. The interviews found ways in which design choices made in telework technologies interact with people’s abilities, especially those who are blind or low vision, since the tools rely heavily on the visual channel to enable remote collaboration. A central theme emerged around how design choices made in telework technologies affect the digital representation of people’s online activities in the video call interface: those who turn off their video (because they are blind or do not want to expend the cognitive effort to present themselves over video) are relegated to a static icon on a blank video frame with their name while those who are deaf and speak silently through a sign language interpreter never show up in interfaces that use active speaker detection to choose which video streams to display. Users with disabilities may avoid using screen sharing and collaborative editing tools which “leak” cues that disclose their disabilities. Because the interviews were conducted during the first month of the COVID-19 pandemic response, they also provided a preview of how the sudden shift to pervasive teleworking affected their telework experience.},
	booktitle = {{CSCW} 2021},
	publisher = {ACM},
	author = {Tang, John},
	month = apr,
	year = {2021},
	note = {Backup Publisher: ACM},
}

@article{kitawaki_pure_1991,
	title = {Pure delay effects on speech quality in telecommunications},
	volume = {9},
	issn = {1558-0008},
	doi = {10.1109/49.81952},
	abstract = {The effect of transmission delay on speech quality in telecommunications is described, with human factors such as conversational mode and the talker's knowledge of the cause of delay taken into account. Objective quality estimation methods for delay effects are proposed, and these methods are applied in an actual communications network. In connection with delay perception in a telephone conversation, the assumption was verified that a talker expects a particular response time from a partner, and that delay that is outside this expectation time window is noticed. Taking this information into account, a subjective conversational experiment is controlled by six kinds of tasks by varying the temporal characteristics. Thus, a subjective assessment of delay effects is obtained by laboratory tests in relation to the detectability threshold, opinion rating, and conversational efficiency. Objective quality measures for each test were defined as a linear combination of temporal parameters that correspond closely to subjective qualities.{\textless}{\textgreater}},
	number = {4},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Kitawaki, N. and Itoh, K.},
	month = may,
	year = {1991},
	note = {Conference Name: IEEE Journal on Selected Areas in Communications},
	keywords = {Speech, Artificial satellites, Asynchronous transfer mode, Delay effects, Delay estimation, Human factors, Laboratories, Propagation delay, Telephony, Testing},
	pages = {586--593},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/APYHELGD/81952.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/2RA2NUQK/Kitawaki and Itoh - 1991 - Pure delay effects on speech quality in telecommun.pdf:application/pdf},
}

@inproceedings{milton_chen_leveraging_2002,
	title = {Leveraging the {Asymmetric} {Sensitivity} of {Eye} {Contact} for {Videoconferencing}},
	abstract = {Eye contact is a natural and often essential element in the language of visual communication. Unfortunately, perceiving eye contact is difficult in most videoconferencing systems and hence limits their effectiveness. We conducted experiments to determine how accurately people perceive eye contact. We discovered that the sensitivity to eye contact is asymmetric, in that we are an order of magnitude less sensitive to eye contact when people look below our eyes than when they look to the left, right, or above our eyes. Additional experiments support a theory that people are prone to perceive eye contact, that is, we will think that someone is making eye contact with us unless we are certain that the person is not looking into our eyes. These experimental results suggest parameters for the design of videoconferencing systems. As a demonstration, we were able to construct from commodity components a simple dyadic videoconferencing prototype that supports eye contact.},
	booktitle = {{CHI}},
	author = {Milton Chen},
	year = {2002},
	file = {Citeseer - Full Text PDF:/Users/rosscutler/Zotero/storage/6YMRGUYY/Videoconferencing - Leveraging the Asymmetric Sensitivity of.pdf:application/pdf;Citeseer - Snapshot:/Users/rosscutler/Zotero/storage/2DVLW4CJ/download.html:text/html},
}

@article{stokes_human_1969,
	title = {Human {Factors} and {Appearance} {Design} {Considerations} of the {Mod} {II} {PICTUREPHONE}® {Station} {Set}},
	volume = {17},
	issn = {2162-2175},
	doi = {10.1109/TCOM.1969.1090060},
	abstract = {This paper treats the human factor considerations involved in the Mod II PICTUREPHONE®set development to maximize customer safety and usage efficiency, to provide a functional circuit environment, and to produce a pleasing appearance design. The set was designed to accommodate 99 percent of the American adult population in environments having illuminations varying from 3 to 660 fL. Particular attention was given to the optical problems of juxtaposing video receiving and transmitting facilities. The design incorporates a number of new features including electronic zoom capability for changing the field of view, electronic height control, graphics mode capability, and a silicon target camera tube which is considerably more resistant to damage than previous tubes.},
	number = {2},
	journal = {IEEE Transactions on Communication Technology},
	author = {Stokes, R.},
	month = apr,
	year = {1969},
	note = {Conference Name: IEEE Transactions on Communication Technology},
	keywords = {Communications technology, Displays, Human factors, Laboratories, Telephony, Circuits, Communications Committee, Electron tubes, Power generation economics, Wire},
	pages = {318--323},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/VAFTE8ZZ/1090060.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/TGH4IPFF/Stokes - 1969 - Human Factors and Appearance Design Considerations.pdf:application/pdf},
}

@article{vertegaal_explaining_nodate,
	title = {Explaining {Effects} of {Eye} {Gaze} on {Mediated} {Group} {Conversations}: {Amount} or {Synchronization}?},
	abstract = {We present an experiment examining effects of gaze on speech during three-person conversations. Understanding such effects is crucial for the design of teleconferencing systems and Collaborative Virtual Environments (CVEs). Previous findings suggest subjects take more turns when they experience more gaze. We evaluated whether this is because more gaze allowed them to better observe whether they were being addressed. We compared speaking behavior between two conditions: (1) in which subjects experienced gaze synchronized with conversational attention, and (2) in which subjects experienced random gaze. The amount of gaze experienced by subjects was a covariate. Results show subjects were 22\% more likely to speak when gaze behavior was synchronized with conversational attention. However, covariance analysis showed these results were due to differences in amount of gaze rather than synchronization of gaze, with correlations of .62 between amount of gaze and amount of subject speech. Task performance was 46\% higher when gaze was synchronized. We conclude it is commendable to use synchronized gaze models when designing CVEs, but depending on task situation, random models generating sufficient amounts of gaze may suffice.},
	language = {en},
	author = {Vertegaal, Roel and Ding, Yaping},
	pages = {8},
	file = {Vertegaal and Ding - Explaining Effects of Eye Gaze on Mediated Group C.pdf:/Users/rosscutler/Zotero/storage/ZLMVHVLG/Vertegaal and Ding - Explaining Effects of Eye Gaze on Mediated Group C.pdf:application/pdf},
}

@book{wandell_brian_foundations_1997,
	title = {Foundations of vision},
	publisher = {Psyccritiques},
	author = {Wandell, Brian},
	year = {1997},
}

@article{kourkounakis_fluentnet_2021,
	title = {{FluentNet}: {End}-to-{End} {Detection} of {Stuttered} {Speech} {Disfluencies} with {Deep} {Learning}},
	issn = {2329-9304},
	shorttitle = {{FluentNet}},
	doi = {10.1109/TASLP.2021.3110146},
	abstract = {Strong presentation skills are valuable and sought-after in workplace and classroom environments alike. Of the possible improvements to vocal presentations, disfluencies and stutters in particular remain one of the most common and prominent factors of someone's demonstration. Millions of people are affected by stuttering and other speech disfluencies, with the majority of the world having experienced mild stutters while communicating under stressful conditions. While there has been much research in the field of automatic speech recognition and language models, stutter detection and recognition has not received as much attention. To this end, we propose an end-to-end deep neural network, FluentNet, capable of detecting a number of different stutter types. FluentNet consists of a Squeeze-and-Excitation Residual convolutional neural network which facilitate the learning of strong spectral frame-level representations, followed by a set of bidirectional long short-term memory layers that aid in learning effective temporal relationships. Lastly, FluentNet uses an attention mechanism to focus on the important parts of speech to obtain a better performance. We perform a number of different experiments, comparisons, and ablation studies to evaluate our model. Our model achieves state-of-the-art results by outperforming other solutions in the field on the publicly available UCLASS dataset. Additionally, we present LibriStutter: a stuttered speech dataset based on the public LibriSpeech dataset with synthesized stutters. We also evaluate FluentNet on this dataset, showing the strong performance of our model versus a number of benchmark techniques.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Kourkounakis, Tedd and Hajavi, Amirhossein and Etemad, Ali},
	year = {2021},
	note = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	keywords = {deep learning, Training, Deep learning, Speech, attention, Speech processing, Speaker recognition, Tools, Benchmark testing, BLSTM, disfluency, Residual neural networks, squeeze-and-excitation, stutter},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/ICQFUHBF/9528931.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/84GMLMCC/Kourkounakis et al. - 2021 - FluentNet End-to-End Detection of Stuttered Speec.pdf:application/pdf},
}

@inproceedings{liu_end--end_2020,
	address = {Barcelona, Spain},
	title = {End-{To}-{End} {Accent} {Conversion} {Without} {Using} {Native} {Utterances}},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9053797/},
	doi = {10.1109/ICASSP40776.2020.9053797},
	abstract = {Techniques for accent conversion (AC) aim to convert non-native to native accented speech. Conventional AC methods try to convert only the speaker identity of a native speaker’s voice to that of the non-native accented target speaker, leaving the underlying content and pronunciations unchanged. This hinders their practical use in real-world applications, because native-accented utterances are required at conversion stage. In this paper, we present an end-to-end framework, which is able to conduct AC from non-native-accented utterances without using any native-accented utterances during online conversion. We achieve this by independently extracting linguistic and speaker representations from non-native accented speech and condition a speech synthesis model on these representations to generate native-accented speech. Experiments on open-source data corpora show that the proposed system can convert Hindi-accented English speech into native American English speech with high naturalness, which is indistinguishable from native-accented recordings in terms of accent.},
	language = {en},
	urldate = {2021-09-14},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Liu, Songxiang and Wang, Disong and Cao, Yuewen and Sun, Lifa and Wu, Xixin and Kang, Shiyin and Wu, Zhiyong and Liu, Xunying and Su, Dan and Yu, Dong and Meng, Helen},
	month = may,
	year = {2020},
	pages = {6289--6293},
	file = {Liu et al. - 2020 - End-To-End Accent Conversion Without Using Native .pdf:/Users/rosscutler/Zotero/storage/4LQEKRP7/Liu et al. - 2020 - End-To-End Accent Conversion Without Using Native .pdf:application/pdf},
}

@article{zhao_converting_2021,
	title = {Converting {Foreign} {Accent} {Speech} {Without} a {Reference}},
	volume = {29},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/9477581/},
	doi = {10.1109/TASLP.2021.3060813},
	abstract = {Foreign accent conversion (FAC) is the problem of generating a synthetic voice that has the voice identity of a secondlanguage (L2) learner and the pronunciation patterns of a native (L1) speaker. This synthetic voice has been referred to as a “golden-speaker” in the pronunciation-training literature. FAC is generally achieved by building a voice-conversion model that maps utterances from a source (L1) speaker onto the target (L2) speaker. As such, FAC requires that a reference utterance from the L1 speaker be available at synthesis time. This greatly restricts the application scope of the FAC system. In this work, we propose a “reference-free” FAC system that eliminates the need for reference L1 utterances at synthesis time, and transforms L2 utterances directly. The system is trained in two steps. First, a conventional FAC procedure is used to create a golden-speaker using utterances from a reference L1 speaker (which are then discarded) and the L2 speaker. Second, a pronunciation-correction model is trained to convert L2 utterances to match the goldenspeaker utterances obtained in the ﬁrst step. At synthesis time, the pronunciation-correction model directly transforms a novel L2 utterance into its golden-speaker counterpart. Our results show that the system reduces foreign accents in novel L2 utterances, achieving a 20.5\% relative reduction in word-error-rate of an American English automatic speech recognizer and a 19\% reduction in perceptual ratings of foreign accentedness obtained through listening tests. Over 73\% of the listeners also rated golden-speaker utterances as having the same voice identity as the original L2 utterances.},
	language = {en},
	urldate = {2021-09-14},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Zhao, Guanlong and Ding, Shaojin and Gutierrez-Osuna, Ricardo},
	year = {2021},
	pages = {2367--2381},
	file = {Zhao et al. - 2021 - Converting Foreign Accent Speech Without a Referen.pdf:/Users/rosscutler/Zotero/storage/Y69KQ7YG/Zhao et al. - 2021 - Converting Foreign Accent Speech Without a Referen.pdf:application/pdf},
}

@misc{noauthor_itu-r_1998,
	title = {{ITU}-{R} {BT}.1359-1},
	url = {https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.1359-1-199811-I!!PDF-E.pdf},
	publisher = {ITU-T},
	year = {1998},
}

@misc{noauthor_itu-t_2016,
	title = {{ITU}-{T} {P}.1305: {Effect} of delays on telemeeting quality},
	url = {https://www.itu.int/rec/T-REC-P.1305-201607-I/en},
	publisher = {ITU-T},
	month = jul,
	year = {2016},
}

@misc{noauthor_itu-t_2017,
	title = {{ITU}-{T} {P}.1310: {Spatial} audio meetings quality evaluation},
	url = {https://www.itu.int/rec/T-REC-P.1310-201703-I/en},
	publisher = {ITU-T},
	month = mar,
	year = {2017},
}

@article{cutler_meeting_2021,
	title = {Meeting {Effectiveness} and {Inclusiveness} in {Remote} {Collaboration}},
	volume = {5},
	issn = {2573-0142},
	url = {https://dl.acm.org/doi/10.1145/3449247},
	doi = {10.1145/3449247},
	language = {en},
	number = {CSCW1},
	urldate = {2021-09-13},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Cutler, Ross and Hosseinkashi, Yasaman and Pool, Jamie and Filipi, Senja and Aichner, Robert and Tu, Yuan and Gehrke, Johannes},
	month = apr,
	year = {2021},
	pages = {1--29},
	file = {Cutler et al. - 2021 - Meeting Effectiveness and Inclusiveness in Remote .pdf:/Users/rosscutler/Zotero/storage/WHL54IZV/Cutler et al. - 2021 - Meeting Effectiveness and Inclusiveness in Remote .pdf:application/pdf},
}

@article{wu_video_2018,
	title = {Video {Compression} through {Image} {Interpolation}},
	url = {http://arxiv.org/abs/1804.06919},
	abstract = {An ever increasing amount of our digital communication, media consumption, and content creation revolves around videos. We share, watch, and archive many aspects of our lives through them, all of which are powered by strong video compression. Traditional video compression is laboriously hand designed and hand optimized. This paper presents an alternative in an end-to-end deep learning codec. Our codec builds on one simple idea: Video compression is repeated image interpolation. It thus benefits from recent advances in deep image interpolation and generation. Our deep video codec outperforms today's prevailing codecs, such as H.261, MPEG-4 Part 2, and performs on par with H.264.},
	urldate = {2021-07-03},
	journal = {arXiv:1804.06919 [cs]},
	author = {Wu, Chao-Yuan and Singhal, Nayan and Krähenbühl, Philipp},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.06919},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/MEK9EIHP/Wu et al. - 2018 - Video Compression through Image Interpolation.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/6EH72JLW/1804.html:text/html},
}

@inproceedings{rippel_learned_2019,
	address = {Seoul, Korea (South)},
	title = {Learned {Video} {Compression}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010297/},
	doi = {10.1109/ICCV.2019.00355},
	abstract = {We present a new algorithm for video coding, learned end-to-end for the low-latency mode. In this setting, our approach outperforms all existing video codecs across nearly the entire bitrate range. To our knowledge, this is the ﬁrst ML-based method to do so.},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Rippel, Oren and Nair, Sanjay and Lew, Carissa and Branson, Steve and Anderson, Alexander and Bourdev, Lubomir},
	month = oct,
	year = {2019},
	pages = {3453--3462},
	file = {Rippel et al. - 2019 - Learned Video Compression.pdf:/Users/rosscutler/Zotero/storage/2WCLEDDE/Rippel et al. - 2019 - Learned Video Compression.pdf:application/pdf},
}

@inproceedings{lin_m-lvc_2020,
	address = {Seattle, WA, USA},
	title = {M-{LVC}: {Multiple} {Frames} {Prediction} for {Learned} {Video} {Compression}},
	isbn = {978-1-72817-168-5},
	shorttitle = {M-{LVC}},
	url = {https://ieeexplore.ieee.org/document/9156714/},
	doi = {10.1109/CVPR42600.2020.00360},
	abstract = {We propose an end-to-end learned video compression scheme for low-latency scenarios. Previous methods are limited in using the previous one frame as reference. Our method introduces the usage of the previous multiple frames as references. In our scheme, the motion vector (MV) ﬁeld is calculated between the current frame and the previous one. With multiple reference frames and associated multiple MV ﬁelds, our designed network can generate more accurate prediction of the current frame, yielding less residual. Multiple reference frames also help generate MV prediction, which reduces the coding cost of MV ﬁeld. We use two deep auto-encoders to compress the residual and the MV, respectively. To compensate for the compression error of the autoencoders, we further design a MV reﬁnement network and a residual reﬁnement network, taking use of the multiple reference frames as well. All the modules in our scheme are jointly optimized through a single rate-distortion loss function. We use a step-by-step training strategy to optimize the entire scheme. Experimental results show that the proposed method outperforms the existing learned video compression methods for low-latency mode. Our method also performs better than H.265 in both PSNR and MS-SSIM. Our code and models are publicly available.},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lin, Jianping and Liu, Dong and Li, Houqiang and Wu, Feng},
	month = jun,
	year = {2020},
	pages = {3543--3551},
	file = {Lin et al. - 2020 - M-LVC Multiple Frames Prediction for Learned Vide.pdf:/Users/rosscutler/Zotero/storage/LK7UELLT/Lin et al. - 2020 - M-LVC Multiple Frames Prediction for Learned Vide.pdf:application/pdf},
}

@article{hu_improving_2020,
	title = {Improving {Deep} {Video} {Compression} by {Resolution}-adaptive {Flow} {Coding}},
	url = {http://arxiv.org/abs/2009.05982},
	abstract = {In the learning based video compression approaches, it is an essential issue to compress pixel-level optical flow maps by developing new motion vector (MV) encoders. In this work, we propose a new framework called Resolution-adaptive Flow Coding (RaFC) to effectively compress the flow maps globally and locally, in which we use multi-resolution representations instead of single-resolution representations for both the input flow maps and the output motion features of the MV encoder. To handle complex or simple motion patterns globally, our frame-level scheme RaFC-frame automatically decides the optimal flow map resolution for each video frame. To cope different types of motion patterns locally, our block-level scheme called RaFC-block can also select the optimal resolution for each local block of motion features. In addition, the rate-distortion criterion is applied to both RaFC-frame and RaFC-block and select the optimal motion coding mode for effective flow coding. Comprehensive experiments on four benchmark datasets HEVC, VTL, UVG and MCL-JCV clearly demonstrate the effectiveness of our overall RaFC framework after combing RaFC-frame and RaFC-block for video compression.},
	urldate = {2021-07-03},
	journal = {arXiv:2009.05982 [cs]},
	author = {Hu, Zhihao and Chen, Zhenghao and Xu, Dong and Lu, Guo and Ouyang, Wanli and Gu, Shuhang},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.05982},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/QAFCYS8M/Hu et al. - 2020 - Improving Deep Video Compression by Resolution-ada.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/69YAW9GL/2009.html:text/html},
}

@inproceedings{agustsson_scale-space_2020,
	address = {Seattle, WA, USA},
	title = {Scale-{Space} {Flow} for {End}-to-{End} {Optimized} {Video} {Compression}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157366/},
	doi = {10.1109/CVPR42600.2020.00853},
	abstract = {Despite considerable progress on end-to-end optimized deep networks for image compression, video coding remains a challenging task. Recently proposed methods for learned video compression use optical ﬂow and bilinear warping for motion compensation and show competitive rate–distortion performance relative to hand-engineered codecs like H.264 and HEVC. However, these learningbased methods rely on complex architectures and training schemes including the use of pre-trained optical ﬂow networks, sequential training of sub-networks, adaptive rate control, and buffering intermediate reconstructions to disk during training. In this paper, we show that a generalized warping operator that better handles common failure cases, e.g. disocclusions and fast motion, can provide competitive compression results with a greatly simpliﬁed model and training procedure. Speciﬁcally, we propose scale-space ﬂow, an intuitive generalization of optical ﬂow that adds a scale parameter to allow the network to better model uncertainty. Our experiments show that a low-latency video compression model (no B-frames) using scale-space ﬂow for motion compensation can outperform analogous stateof-the art learned video compression models while being trained using a much simpler procedure and without any pre-trained optical ﬂow networks.},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Agustsson, Eirikur and Minnen, David and Johnston, Nick and Balle, Johannes and Hwang, Sung Jin and Toderici, George},
	month = jun,
	year = {2020},
	pages = {8500--8509},
	file = {Agustsson et al. - 2020 - Scale-Space Flow for End-to-End Optimized Video Co.pdf:/Users/rosscutler/Zotero/storage/AFQTSPI8/Agustsson et al. - 2020 - Scale-Space Flow for End-to-End Optimized Video Co.pdf:application/pdf},
}

@article{lu_end--end_2020,
	title = {An {End}-to-{End} {Learning} {Framework} for {Video} {Compression}},
	volume = {PP},
	doi = {10.1109/TPAMI.2020.2988453},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Lu, Guo and Zhang, Xiaoyun and Ouyang, Wanli and Chen, Li and Gao, Zhiyong and Xu, Dong},
	month = apr,
	year = {2020},
	pages = {1--1},
}

@article{reddy_dnsmos_2021,
	title = {{DNSMOS}: {A} {Non}-{Intrusive} {Perceptual} {Objective} {Speech} {Quality} metric to evaluate {Noise} {Suppressors}},
	shorttitle = {{DNSMOS}},
	url = {http://arxiv.org/abs/2010.15258},
	abstract = {Human subjective evaluation is the gold standard to evaluate speech quality optimized for human perception. Perceptual objective metrics serve as a proxy for subjective scores. The conventional and widely used metrics require a reference clean speech signal, which is unavailable in real recordings. The no-reference approaches correlate poorly with human ratings and are not widely adopted in the research community. One of the biggest use cases of these perceptual objective metrics is to evaluate noise suppression algorithms. This paper introduces a multi-stage self-teaching based perceptual objective metric that is designed to evaluate noise suppressors. The proposed method generalizes well in challenging test conditions with a high correlation to human ratings.},
	urldate = {2021-07-02},
	journal = {arXiv:2010.15258 [cs, eess]},
	author = {Reddy, Chandan K. A. and Gopal, Vishak and Cutler, Ross},
	month = feb,
	year = {2021},
	note = {arXiv: 2010.15258},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/YZPZJU4M/Reddy et al. - 2021 - DNSMOS A Non-Intrusive Perceptual Objective Speec.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/4D7UXQ3P/2010.html:text/html},
}

@article{ladune_coding_2021,
	title = {Coding {Standards} as {Anchors} for the {CVPR} {CLIC} video track},
	url = {http://arxiv.org/abs/2105.09833},
	abstract = {In 2021, a new track has been initiated in the Challenge for Learned Image Compression{\textasciitilde}: the video track. This category proposes to explore technologies for the compression of short video clips at 1 Mbit/s. This paper proposes to generate coded videos using the latest standardized video coders, especially Versatile Video Coding (VVC). The objective is not only to measure the progress made by learning techniques compared to the state of the art video coders, but also to quantify their progress from years to years. With this in mind, this paper documents how to generate the video sequences fulfilling the requirements of this challenge, in a reproducible way, targeting the maximum performance for VVC.},
	urldate = {2021-07-02},
	journal = {arXiv:2105.09833 [eess]},
	author = {Ladune, Théo and Philippe, Pierrick},
	month = may,
	year = {2021},
	note = {arXiv: 2105.09833},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/SUTLH7IL/Ladune and Philippe - 2021 - Coding Standards as Anchors for the CVPR CLIC vide.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/8B5KMF56/2105.html:text/html},
}

@inproceedings{tan_connectboard_2009,
	address = {Rio De Janeiro, Brazil},
	title = {{ConnectBoard}: {A} remote collaboration system that supports gaze-aware interaction and sharing},
	isbn = {978-1-4244-4463-2},
	shorttitle = {{ConnectBoard}},
	url = {http://ieeexplore.ieee.org/document/5293268/},
	doi = {10.1109/MMSP.2009.5293268},
	abstract = {We present ConnectBoard, a new system for remote collaboration where users experience natural interaction with one another, seemingly separated only by a vertical, transparent sheet of glass. It overcomes two key shortcomings of conventional video communication systems: the inability to seamlessly capture natural user interactions, like using hands to point and gesture at parts of shared documents, and the inability of users to look into the camera lens without taking their eyes off the display. We solve these problems by placing the camera behind the screen, where the remote user is virtually located. The camera sees through the display to capture images of the user. As a result, our setup captures natural, frontal views of users as they point and gesture at shared media displayed on the screen between them. Users also never have to take their eyes off their screens to look into the camera lens. Our novel optical solution based on wavelength multiplexing can be easily built with off-the-shelf components and does not require custom electronics for projector-camera synchronization.},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {2009 {IEEE} {International} {Workshop} on {Multimedia} {Signal} {Processing}},
	publisher = {IEEE},
	author = {Tan, Kar-Han and Robinson, Ian and Samadani, Ramin and Lee, Bowon and Gelb, Dan and Vorbau, Alex and Culbertson, Bruce and Apostolopoulos, John},
	month = oct,
	year = {2009},
	pages = {1--6},
	file = {Tan et al. - 2009 - ConnectBoard A remote collaboration system that s.pdf:/Users/rosscutler/Zotero/storage/KVQW8HMN/Tan et al. - 2009 - ConnectBoard A remote collaboration system that s.pdf:application/pdf},
}

@article{chen_assessment_2014,
	title = {Assessment visual fatigue of watching {3DTV} using {EEG} power spectral parameters},
	volume = {35},
	issn = {0141-9382},
	url = {https://www.sciencedirect.com/science/article/pii/S0141938214000699},
	doi = {10.1016/j.displa.2014.10.001},
	abstract = {Although the three-dimensional television is popular for its stereoscopy, the fatigue caused by the prolonged watching of 3DTV should not be underestimated. Electroencephalogram (EEG) has been widely used for monitoring the brain’s functional activities. Based on our previous research of 3DTV fatigue, one more objective and effective 3DTV fatigue evaluation model is proposed on gravity frequency of power spectrum and power spectral entropy. As the fatigue changes, the gravity frequency reflects the transition of EEG power spectrum and the power spectral entropy describes the level of chaos of EEG. 16 channels of EEG data of twenty-five subjects watching 2DTV and 3DTV were collected, and gravity frequency of power spectrum and power spectral entropy were then calculated and analyzed. These two parameters of the 3D group changed more significantly comparing with that of the 2D group on several electrodes. There are significant decreases in gravity frequency and power spectral entropy in several brain regions after long time of watching 3DTV, which indicates the decline of subjects’ alertness level. Based on the subjective evaluation and two significant parameters, gravity frequency and power spectral entropy, an accurate evaluation model for 3DTV fatigue was established using the regression equation.},
	language = {en},
	number = {5},
	urldate = {2021-05-25},
	journal = {Displays},
	author = {Chen, Chunxiao and Wang, Jing and Li, Kun and Wu, Qiuyi and Wang, Haowen and Qian, Zhiyu and Gu, Ning},
	month = dec,
	year = {2014},
	keywords = {3DTV, EEG, Fatigue evaluation, Gravity frequency, Power spectral entropy},
	pages = {266--272},
	file = {ScienceDirect Snapshot:/Users/rosscutler/Zotero/storage/CR5PKM8Z/S0141938214000699.html:text/html},
}

@article{noauthor_itu-t_2016-1,
	title = {{ITU}-{T} {P}.1305 {Effect} of delays on telemeeting quality},
	year = {2016},
}

@article{gale_eeg_1975,
	title = {{EEG} correlates of eye contact and interpersonal distance},
	volume = {3},
	issn = {0301-0511},
	url = {https://www.sciencedirect.com/science/article/pii/030105117590023X},
	doi = {10.1016/0301-0511(75)90023-X},
	abstract = {The EEG of 18 male subjects was monitored while the subject gazed at the eyes of a male experimenter located 2, 4, 8, 16 or 32 ft from the subject. The experimenter either gazed directly at the subject or averted his eyes. EEG arousal was highest when the experimenter was at 2 ft and gazing into the subject's eyes. EEG arousal diminished as a function of distance, while arousal for direct gaze was always higher than for averted gaze, whatever the distance.},
	language = {en},
	number = {4},
	urldate = {2021-05-25},
	journal = {Biological Psychology},
	author = {Gale, Anthony and Spratt, Graham and Chapman, Antony J. and Smallbone, Adrian},
	month = dec,
	year = {1975},
	pages = {237--245},
	file = {ScienceDirect Snapshot:/Users/rosscutler/Zotero/storage/VKGILR36/030105117590023X.html:text/html},
}

@inproceedings{antons_too_2012,
	address = {Melbourne, Australia},
	title = {Too tired for calling? {A} physiological measure of fatigue caused by bandwidth limitations},
	isbn = {978-1-4673-0726-0 978-1-4673-0724-6 978-1-4673-0725-3},
	shorttitle = {Too tired for calling?},
	url = {http://ieeexplore.ieee.org/document/6263840/},
	doi = {10.1109/QoMEX.2012.6263840},
	abstract = {Common methods to determine the quality of media rely on conscious ratings of a subject’s opinion about the quality of presented stimuli. While such methods provide a reliable and valid means of determining quality, they provide little insight into the physiological processes preceding the quality judgment, which, however, may affect the subjective behavior, e.g., in terms of alertness or media usage duration. In this paper we used a non-intrusive physiological method, electroencephalography, to assess the cognitive state of subjects related to the quality of auditory speech stimuli. We show that users listening to degraded audio rated the quality lower in comparison to an undisturbed stimulus as expected and, in addition, got more fatigued during the 20 minute presentation. Indicators of the increased fatigue were Theta and Alpha frequencies of the electroencephalogram data. The results show that the perception of degraded media has long-term inﬂuences on physiological processes at the time scale of minutes which may immediately inﬂuence customer behavior.},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {2012 {Fourth} {International} {Workshop} on {Quality} of {Multimedia} {Experience}},
	publisher = {IEEE},
	author = {Antons, Jan-Niklas and Schleicher, Robert and Arndt, Sebastian and Moller, Sebastian and Curio, Gabriel},
	month = jul,
	year = {2012},
	pages = {63--67},
	file = {Antons et al. - 2012 - Too tired for calling A physiological measure of .pdf:/Users/rosscutler/Zotero/storage/ZNT523N4/Antons et al. - 2012 - Too tired for calling A physiological measure of .pdf:application/pdf},
}

@article{hinds_cognitive_1999,
	title = {The {Cognitive} and {Interpersonal} {Costs} of {Video}},
	volume = {1},
	issn = {1521-3269, 1532-785X},
	url = {http://www.tandfonline.com/doi/abs/10.1207/s1532785xmep0104_1},
	doi = {10.1207/s1532785xmep0104_1},
	language = {en},
	number = {4},
	urldate = {2021-05-25},
	journal = {Media Psychology},
	author = {Hinds, Pamela J.},
	month = dec,
	year = {1999},
	pages = {283--311},
}

@misc{hinds_pamela_cognitive_1999,
	title = {The {Cognitive} and {Interpersonal} {Costs} of {Video}: {Media} {Psychology}: {Vol} 1, {No} 4},
	url = {https://www.tandfonline.com/doi/abs/10.1207/s1532785xmep0104_1},
	urldate = {2021-05-25},
	author = {Hinds, Pamela},
	year = {1999},
	file = {The Cognitive and Interpersonal Costs of Video\: Media Psychology\: Vol 1, No 4:/Users/rosscutler/Zotero/storage/UT886CC3/s1532785xmep0104_1.html:text/html},
}

@article{fauville_nonverbal_2021,
	title = {Nonverbal {Mechanisms} {Predict} {Zoom} {Fatigue} and {Explain} {Why} {Women} {Experience} {Higher} {Levels} than {Men}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3820035},
	doi = {10.2139/ssrn.3820035},
	abstract = {There is little data on Zoom Fatigue, the exhaustion that follows video conference meetings. This paper tests associations between Zoom Fatigue and five theoretical nonverbal mechanisms (mirror anxiety, being physically trapped, hyper gaze from a grid of staring faces, and the cognitive load from producing and interpreting nonverbal cues) with 10,591 participants from a convenience sample. We show that daily usage predicts the amount of fatigue, and that women have longer meetings and shorter breaks between meetings than men. Moreover, women report greater fatigue than men, and we replicate this effect with an online sample. The five nonverbal mechanisms predict Zoom fatigue, and we confirm that mirror anxiety, measured both by self-report and by linguistic analysis of open-ended responses, mediates the gender difference in fatigue. Exploratory research shows that race, age, and personality relate to fatigue. We discuss avenues for future research and strategies to decrease Zoom fatigue.},
	language = {en},
	urldate = {2021-05-25},
	journal = {SSRN Electronic Journal},
	author = {Fauville, Geraldine and Luo, Mufan and Queiroz, Anna C. M. and Bailenson, Jeremy N. and Hancock, Jeff},
	year = {2021},
	file = {Fauville et al. - 2021 - Nonverbal Mechanisms Predict Zoom Fatigue and Expl.pdf:/Users/rosscutler/Zotero/storage/V3BDT47M/Fauville et al. - 2021 - Nonverbal Mechanisms Predict Zoom Fatigue and Expl.pdf:application/pdf},
}

@unpublished{noauthor_quantizing_nodate,
	title = {Quantizing {Deep} {Convolutional} {Networks} for {Efficient} {Inference}},
	file = {_.pdf:/Users/rosscutler/Zotero/storage/E556XGW5/_.pdf:application/pdf},
}

@article{serra_sesqa_2021,
	title = {{SESQA}: semi-supervised learning for speech quality assessment},
	shorttitle = {{SESQA}},
	url = {http://arxiv.org/abs/2010.00368},
	abstract = {Automatic speech quality assessment is an important, transversal task whose progress is hampered by the scarcity of human annotations, poor generalization to unseen recording conditions, and a lack of flexibility of existing approaches. In this work, we tackle these problems with a semi-supervised learning approach, combining available annotations with programmatically generated data, and using 3 different optimization criteria together with 5 complementary auxiliary tasks. Our results show that such a semi-supervised approach can cut the error of existing methods by more than 36\%, while providing additional benefits in terms of reusable features or auxiliary outputs. Improvement is further corroborated with an out-of-sample test showing promising generalization capabilities.},
	urldate = {2021-03-09},
	journal = {arXiv:2010.00368 [cs, eess]},
	author = {Serrà, Joan and Pons, Jordi and Pascual, Santiago},
	month = feb,
	year = {2021},
	note = {arXiv: 2010.00368},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/787M3QP4/Serrà et al. - 2021 - SESQA semi-supervised learning for speech quality.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/QTDZBA56/2010.html:text/html},
}

@article{manocha_cdpam_2021,
	title = {{CDPAM}: {Contrastive} learning for perceptual audio similarity},
	shorttitle = {{CDPAM}},
	url = {http://arxiv.org/abs/2102.05109},
	abstract = {Many speech processing methods based on deep learning require an automatic and differentiable audio metric for the loss function. The DPAM approach of Manocha et al. learns a full-reference metric trained directly on human judgments, and thus correlates well with human perception. However, it requires a large number of human annotations and does not generalize well outside the range of perturbations on which it was trained. This paper introduces CDPAM, a metric that builds on and advances DPAM. The primary improvement is to combine contrastive learning and multi-dimensional representations to build robust models from limited data. In addition, we collect human judgments on triplet comparisons to improve generalization to a broader range of audio perturbations. CDPAM correlates well with human responses across nine varied datasets. We also show that adding this metric to existing speech synthesis and enhancement methods yields significant improvement, as measured by objective and subjective tests.},
	urldate = {2021-03-08},
	journal = {arXiv:2102.05109 [cs, eess]},
	author = {Manocha, Pranay and Jin, Zeyu and Zhang, Richard and Finkelstein, Adam},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.05109},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/UQ6IN4YD/Manocha et al. - 2021 - CDPAM Contrastive learning for perceptual audio s.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/S5NYU6X4/2102.html:text/html},
}

@book{gamper_intrusive_2019,
	title = {Intrusive and {Non}-{Intrusive} {Perceptual} {Speech} {Quality} {Assessment} {Using} a {Convolutional} {Neural} {Network}},
	abstract = {Speech quality, as perceived by humans, is an important performance metric for telephony and voice services. It is typically measured through subjective listening tests, which can be tedious and expensive. Algorithms such as PESQ and POLQA serve as a computational proxy for subjective listening tests. Here we propose using a convolutional neural network to predict the perceived quality of speech with noise, reverberation, and distortions, both intrusively and non-intrusively, i.e., with and without a clean reference signal. The network model is trained and evaluated on a corpus of about ten thousand utterances labeled by human listeners to derive a Mean Opinion Score (MOS) for each utterance. It is shown to provide more accurate MOS estimates than existing speech quality metrics, including PESQ and POLQA. The proposed method reduces the root mean squared error from 0.48 to 0.35 MOS points and increases the Pearson correlation from 0.78 to 0.89 compared to the state-of-the-art POLQA algorithm.},
	author = {Gamper, Hannes and Reddy, Chandan and Cutler, Ross and Tashev, Ivan and Gehrke, Johannes},
	month = oct,
	year = {2019},
	doi = {10.1109/WASPAA.2019.8937202},
}

@article{ramlall_review_2004,
	title = {A {Review} of {Employee} {Motivation} {Theories} and their {Implications} for {Employee} {Retention} within {Organizations}},
	abstract = {The article provides a synthesis of employee motivation theories and offers an explanation of how employee motivation affects employee retention and other behaviors within organizations. In addition to explaining why it is important to retain critical employees, the author described the relevant motivation theories and explained the implications of employee motivation theories on developing and implementing employee retention practices. The final segment of the paper provides an illustration with explanation on how effective employee retention practices can be explained through motivation theories and how these efforts serve as a strategy to increasing organizational performance.},
	language = {en},
	author = {Ramlall, Sunil},
	year = {2004},
	pages = {13},
	file = {Ramlall - 2004 - A Review of Employee Motivation Theories and their.pdf:/Users/rosscutler/Zotero/storage/2UY8WLZ2/Ramlall - 2004 - A Review of Employee Motivation Theories and their.pdf:application/pdf},
}

@article{woolley_evidence_2010,
	title = {Evidence for a {Collective} {Intelligence} {Factor} in the {Performance} of {Human} {Groups}},
	volume = {330},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.1193147},
	doi = {10.1126/science.1193147},
	language = {en},
	number = {6004},
	urldate = {2021-01-15},
	journal = {Science},
	author = {Woolley, Anita Williams and Chabris, Christopher F. and Pentland, Alex and Hashmi, Nada and Malone, Thomas W.},
	month = oct,
	year = {2010},
	pages = {686--688},
	file = {Woolley et al. - 2010 - Evidence for a Collective Intelligence Factor in t.pdf:/Users/rosscutler/Zotero/storage/3RDYE9VM/Woolley et al. - 2010 - Evidence for a Collective Intelligence Factor in t.pdf:application/pdf},
}

@article{olson_working_2013,
	title = {Working {Together} {Apart}: {Collaboration} over the {Internet}},
	volume = {6},
	issn = {1946-7680},
	shorttitle = {Working {Together} {Apart}},
	url = {https://www.morganclaypool.com/doi/abs/10.2200/S00542ED1V01Y201310HCI020},
	doi = {10.2200/S00542ED1V01Y201310HCI020},
	number = {5},
	urldate = {2021-01-15},
	journal = {Synthesis Lectures on Human-Centered Informatics},
	author = {Olson, Judith S. and Olson, Gary M.},
	month = nov,
	year = {2013},
	note = {Publisher: Morgan \& Claypool Publishers},
	pages = {1--151},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/KCPPL5US/S00542ED1V01Y201310HCI020.html:text/html},
}

@article{sproull_reducing_1988,
	title = {Reducing {Social} {Context} {Cues}: {Electronic} {Mail} in {Organizational} {Communication}},
	volume = {32},
	shorttitle = {Reducing {Social} {Context} {Cues}},
	doi = {10.1287/mnsc.32.11.1492},
	abstract = {This paper examines electronic mail in organizational communication. Based on ideas about how social context cues within a communication setting affect information exchange, it argues that electronic mail does not simply speed up the exchange of information but leads to the exchange of new information as well. In a field study in a Fortune 500 company, we used questionnaire data and actual messages to examine electronic mail communication at all levels of the organization. Based on hypotheses from research on social communication, we explored effects of electronic communication related to self-absorption, status equalization, and uninhibited behavior. Consistent with experimental studies, we found that decreasing social context cues has substantial deregulating effects on communication. And we found that much of the information conveyed through electronic mail was information that would not have been conveyed through another medium.},
	journal = {Management Science},
	author = {Sproull, Lee and Kiesler, Sara},
	month = jan,
	year = {1988},
	pages = {1492--1512},
	file = {Full Text PDF:/Users/rosscutler/Zotero/storage/KNY69FT2/Sproull and Kiesler - 1988 - Reducing Social Context Cues Electronic Mail in O.pdf:application/pdf},
}

@inproceedings{kulkarni_talkabout_2015,
	address = {Vancouver BC Canada},
	title = {Talkabout: {Making} {Distance} {Matter} with {Small} {Groups} in {Massive} {Classes}},
	isbn = {978-1-4503-2922-4},
	shorttitle = {Talkabout},
	url = {https://dl.acm.org/doi/10.1145/2675133.2675166},
	doi = {10.1145/2675133.2675166},
	abstract = {Massive online classes are global and diverse. How can we harness this diversity to improve engagement and learning? Currently, though enrollments are high, students’ interactions with each other are minimal: most are alone together. This isolation is particularly disappointing given that a global community is a major draw of online classes. This paper illustrates the potential of leveraging geographic diversity in massive online classes. We connect students from around the world through small-group video discussions. Our peer discussion system, Talkabout, has connected over 5,000 students in fourteen online classes. Three studies with 2,670 students from two classes found that globally diverse discussions boost student performance and engagement: the more geographically diverse the discussion group, the better the students performed on later quizzes. Through this work, we challenge the view that online classes are useful only when in-person classes are unavailable. Instead, we demonstrate how diverse online classrooms can create benefits that are largely unavailable in a traditional classroom.},
	language = {en},
	urldate = {2021-01-15},
	booktitle = {Proceedings of the 18th {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work} \& {Social} {Computing}},
	publisher = {ACM},
	author = {Kulkarni, Chinmay and Cambre, Julia and Kotturi, Yasmine and Bernstein, Michael S. and Klemmer, Scott R.},
	month = feb,
	year = {2015},
	pages = {1116--1128},
	file = {Kulkarni et al. - 2015 - Talkabout Making Distance Matter with Small Group.pdf:/Users/rosscutler/Zotero/storage/6XS3G3AZ/Kulkarni et al. - 2015 - Talkabout Making Distance Matter with Small Group.pdf:application/pdf},
}

@inproceedings{lykourentzou_personality_2016,
	address = {New York, NY, USA},
	series = {{CSCW} '16},
	title = {Personality {Matters}: {Balancing} for {Personality} {Types} {Leads} to {Better} {Outcomes} for {Crowd} {Teams}},
	isbn = {978-1-4503-3592-8},
	shorttitle = {Personality {Matters}},
	url = {https://doi.org/10.1145/2818048.2819979},
	doi = {10.1145/2818048.2819979},
	abstract = {When personalities clash, teams operate less effectively. Personality differences affect face-to-face collaboration and may lower trust in virtual teams. For relatively short-lived assignments, like those of online crowdsourcing, personality matching could provide a simple, scalable strategy for effective team formation. However, it is not clear how (or if) personality differences affect teamwork in this novel context where the workforce is more transient and diverse. This study examines how personality compatibility in crowd teams affects performance and individual perceptions. Using the DISC personality test, we composed 14 five-person teams (N=70) with either a harmonious coverage of personalities (balanced) or a surplus of leader-type personalities (imbalanced). Results show that balancing for personality leads to significantly better performance on a collaborative task. Balanced teams exhibited less conflict and their members reported higher levels of satisfaction and acceptance. This work demonstrates a simple personality matching strategy for forming more effective teams in crowdsourcing contexts.},
	urldate = {2021-01-15},
	booktitle = {Proceedings of the 19th {ACM} {Conference} on {Computer}-{Supported} {Cooperative} {Work} \& {Social} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Lykourentzou, Ioanna and Antoniou, Angeliki and Naudet, Yannick and Dow, Steven P.},
	month = feb,
	year = {2016},
	keywords = {Crowsourcing, personality-based balancing, team formation},
	pages = {260--273},
}

@article{bear_role_2011,
	title = {The {Role} of {Gender} in {Team} {Collaboration} and {Performance}},
	volume = {36},
	doi = {10.1179/030801811X13013181961473},
	abstract = {Given that women continue to be underrepresented in STEM (Science, Technology, Engineering and Math) and that scientific innovations are increasingly produced by team collaborations, we reviewed the existing literature regarding the effects of gender diversity on team processes and performance. Recent evidence strongly suggests that team collaboration is greatly improved by the presence of women in the group, and this effect is primarily explained by benefits to group processes. The evidence concerning the effect of gender diversity on team performance is more equivocal and contingent upon a variety of contextual factors. In light of the importance of collaboration in science, promoting the role of women in the field can have positive practical consequences for science and technology.},
	journal = {Interdisciplinary Science Reviews},
	author = {Bear, Julia and Woolley, Anita},
	month = jun,
	year = {2011},
}

@incollection{enzner_acoustic_2014,
	title = {Acoustic {Echo} {Control}},
	volume = {4},
	isbn = {978-0-12-396501-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780123965011000303},
	language = {en},
	urldate = {2021-01-15},
	booktitle = {Academic {Press} {Library} in {Signal} {Processing}},
	publisher = {Elsevier},
	author = {Enzner, Gerald and Buchner, Herbert and Favrot, Alexis and Kuech, Fabian},
	year = {2014},
	doi = {10.1016/B978-0-12-396501-1.00030-3},
	pages = {807--877},
	file = {Enzner et al. - 2014 - Acoustic Echo Control.pdf:/Users/rosscutler/Zotero/storage/I8G9XF3P/Enzner et al. - 2014 - Acoustic Echo Control.pdf:application/pdf},
}

@article{pons_musicnn_2019,
	title = {{MUSICNN}: {PRE}-{TRAINED} {CONVOLUTIONAL} {NEURAL} {NETWORKS} {FOR} {MUSIC} {AUDIO} {TAGGING}},
	language = {en},
	author = {Pons, Jordi and Serra, Xavier},
	year = {2019},
	pages = {2},
	file = {Pons and Serra - 2019 - MUSICNN PRE-TRAINED CONVOLUTIONAL NEURAL NETWORKS.pdf:/Users/rosscutler/Zotero/storage/BY7ULPDK/Pons and Serra - 2019 - MUSICNN PRE-TRAINED CONVOLUTIONAL NEURAL NETWORKS.pdf:application/pdf},
}

@article{mesaros_sound_2019,
	title = {Sound {Event} {Detection} in the {DCASE} 2017 {Challenge}},
	volume = {27},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/8673582/},
	doi = {10.1109/TASLP.2019.2907016},
	abstract = {Each edition of the challenge on Detection and Classiﬁcation of Acoustic Scenes and Events (DCASE) contained several tasks involving sound event detection in different setups. DCASE 2017 presented participants with three such tasks, each having speciﬁc datasets and detection requirements: Task 2, in which target sound events were very rare in both training and testing data, Task 3 having overlapping events annotated in real-life audio, and Task 4, in which only weakly-labeled data was available for training. In this paper, we present the three tasks, including the datasets and baseline systems, and analyze the challenge entries for each task. We observe the popularity of methods using deep neural networks, and the still widely used mel frequency based representations, with only few approaches standing out as radically different. Analysis of the systems behavior reveals that task-speciﬁc optimization has a big role in producing good performance; however, often this optimization closely follows the ranking metric, and its maximization/minimization does not result in universally good performance. We also introduce the calculation of conﬁdence intervals based on a jackknife resampling procedure, to perform statistical analysis of the challenge results. The analysis indicates that while the 95\% conﬁdence intervals for many systems overlap, there are signiﬁcant difference in performance between the top systems and the baseline for all tasks.},
	language = {en},
	number = {6},
	urldate = {2021-01-13},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Mesaros, Annamaria and Diment, Aleksandr and Elizalde, Benjamin and Heittola, Toni and Vincent, Emmanuel and Raj, Bhiksha and Virtanen, Tuomas},
	month = jun,
	year = {2019},
	pages = {992--1006},
	file = {Mesaros et al. - 2019 - Sound Event Detection in the DCASE 2017 Challenge.pdf:/Users/rosscutler/Zotero/storage/FRAHPA7Z/Mesaros et al. - 2019 - Sound Event Detection in the DCASE 2017 Challenge.pdf:application/pdf},
}

@article{haidar-ahmad_music_nodate,
	title = {Music and {Instrument} {Classiﬁcation} using {Deep} {Learning} {Technics}},
	abstract = {This paper presents our implementation of a multi-class classiﬁer that identiﬁes instruments in music streams. Our model consists of a CNN which’s input is an audio stream that we pre-process to extract the mel-spectogram, and outputs the dominance or non-dominance of pre-selected instruments. We focus our study on 3 instruments, and thus classify audio streams into one of 4 classes: “Piano”, “Drums”, “Flute” or “Other”. We obtained a precision of 70\%, a recall of 65\%, and a F1-score of 64\%. As future work, we aim to implement and compare the results of more deep learning model architectures such as RNN, RCNN, CRNN, in addition to adding more instruments.},
	language = {en},
	author = {Haidar-Ahmad, Lara},
	pages = {6},
	file = {Haidar-Ahmad - Music and Instrument Classiﬁcation using Deep Lear.pdf:/Users/rosscutler/Zotero/storage/38T755EL/Haidar-Ahmad - Music and Instrument Classiﬁcation using Deep Lear.pdf:application/pdf},
}

@inproceedings{qin_query_2013,
	address = {Portland, OR, USA},
	title = {Query {Adaptive} {Similarity} for {Large} {Scale} {Object} {Retrieval}},
	isbn = {978-0-7695-4989-7},
	url = {http://ieeexplore.ieee.org/document/6619055/},
	doi = {10.1109/CVPR.2013.211},
	abstract = {Existing music recognition applications require a connection to a server that performs the actual recognition. In this paper we present a low-power music recognizer that runs entirely on a mobile device and automatically recognizes music without user interaction. To reduce battery consumption, a small music detector runs continuously on the mobile device’s digital signal processor (DSP) chip and wakes up the main application processor only when it is conﬁdent that music is present. Once woken, the recognizer on the application processor is provided with a few seconds of audio which is ﬁngerprinted and compared to the stored ﬁngerprints in the on-device ﬁngerprint database of tens of thousands of songs. Our presented system, Now Playing, has a daily battery usage of less than 1 \% on average, respects user privacy by running entirely on-device and can passively recognize a wide range of music.},
	language = {en},
	urldate = {2021-01-13},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Qin, Danfeng and Wengert, Christian and Van Gool, Luc},
	month = jun,
	year = {2013},
	pages = {1610--1617},
	file = {Qin et al. - 2013 - Query Adaptive Similarity for Large Scale Object R.pdf:/Users/rosscutler/Zotero/storage/QUGYTHRX/Qin et al. - 2013 - Query Adaptive Similarity for Large Scale Object R.pdf:application/pdf},
}

@inproceedings{gemmeke_audio_2017,
	address = {New Orleans, LA},
	title = {Audio {Set}: {An} ontology and human-labeled dataset for audio events},
	isbn = {978-1-5090-4117-6},
	shorttitle = {Audio {Set}},
	url = {http://ieeexplore.ieee.org/document/7952261/},
	doi = {10.1109/ICASSP.2017.7952261},
	abstract = {Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous beneﬁts from comprehensive datasets – principally ImageNet. This paper describes the creation of Audio Set, a largescale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of speciﬁc audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.},
	language = {en},
	urldate = {2021-01-13},
	booktitle = {{ICASSP}},
	publisher = {IEEE},
	author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
	month = mar,
	year = {2017},
	pages = {776--780},
	file = {Gemmeke et al. - 2017 - Audio Set An ontology and human-labeled dataset f.pdf:/Users/rosscutler/Zotero/storage/UD37MANW/Gemmeke et al. - 2017 - Audio Set An ontology and human-labeled dataset f.pdf:application/pdf},
}

@inproceedings{hu_phase-aware_2020,
	title = {Phase-{Aware} {Music} {Super}-{Resolution} {Using} {Generative} {Adversarial} {Networks}},
	url = {http://www.isca-speech.org/archive/Interspeech_2020/abstracts/2605.html},
	doi = {10.21437/Interspeech.2020-2605},
	abstract = {Audio super-resolution is a challenging task of recovering the missing high-resolution features from a low-resolution signal. To address this, generative adversarial networks (GAN) have been used to achieve promising results by training the mappings between magnitudes of the low and high-frequency components. However, phase information is not well-considered for waveform reconstruction in conventional methods. In this paper, we tackle the problem of music super-resolution and conduct a thorough investigation on the importance of phase for this task. We use GAN to predict the magnitudes of the highfrequency components. The corresponding phase information can be extracted using either a GAN-based waveform synthesis system or a modiﬁed Grifﬁn-Lim algorithm. Experimental results show that phase information plays an important role in the improvement of the reconstructed music quality. Moreover, our proposed method signiﬁcantly outperforms other state-ofthe-art methods in terms of objective evaluations.},
	language = {en},
	urldate = {2021-01-13},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Hu, Shichao and Zhang, Bin and Liang, Beici and Zhao, Ethan and Lui, Simon},
	month = oct,
	year = {2020},
	pages = {4074--4078},
	file = {Hu et al. - 2020 - Phase-Aware Music Super-Resolution Using Generativ.pdf:/Users/rosscutler/Zotero/storage/2CK4JAM2/Hu et al. - 2020 - Phase-Aware Music Super-Resolution Using Generativ.pdf:application/pdf},
}

@misc{fonseca_fsd50k_2020,
	title = {{FSD50K}},
	copyright = {Open Access},
	url = {https://zenodo.org/record/4060432},
	abstract = {Most existing datasets for sound event recognition (SER) are relatively small and/or domain-speciﬁc, with the exception of AudioSet, based on a massive amount of audio tracks from YouTube videos and encompassing over 500 classes of everyday sounds. However, AudioSet is not an open dataset—its release consists of pre-computed audio features (instead of waveforms), which limits the adoption of some SER methods. Downloading the original audio tracks is also problematic due to constituent YouTube videos gradually disappearing and usage rights issues, which casts doubts over the suitability of this resource for systems’ benchmarking. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classiﬁcation experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research.},
	language = {en},
	urldate = {2021-01-13},
	publisher = {Zenodo},
	author = {Fonseca, Eduardo and Favory, Xavier and Pons, Jordi and Font, Frederic and Serra, Xavier},
	month = oct,
	year = {2020},
	doi = {10.5281/ZENODO.4060432},
	note = {type: dataset},
	keywords = {data collection, audio dataset, environmental sound, everyday sounds, sound event classification, sound event recognition, sound event tagging},
	file = {Fonseca et al. - 2020 - FSD50K.pdf:/Users/rosscutler/Zotero/storage/EJBXNWRA/Fonseca et al. - 2020 - FSD50K.pdf:application/pdf},
}

@article{kong_weakly_2019,
	title = {Weakly {Labelled} {AudioSet} {Tagging} {With} {Attention} {Neural} {Networks}},
	volume = {27},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/8777125/},
	doi = {10.1109/TASLP.2019.2930913},
	abstract = {Audio tagging is the task of predicting the presence or absence of sound classes within an audio clip. Previous work in audio tagging focused on relatively small datasets limited to recognising a small number of sound classes. We investigate audio tagging on AudioSet, which is a dataset consisting of over 2 million audio clips and 527 classes. AudioSet is weakly labelled, in that only the presence or absence of sound classes is known for each clip, while the onset and offset times are unknown. To address the weakly-labelled audio tagging problem, we propose attention neural networks as a way to attend the most salient parts of an audio clip. We bridge the connection between attention neural networks and multiple instance learning (MIL) methods, and propose decision-level and feature-level attention neural networks for audio tagging. We investigate attention neural networks modelled by different functions, depths and widths. Experiments on AudioSet show that the feature-level attention neural network achieves a state-of-the-art mean average precision (mAP) of 0.369, outperforming the best multiple instance learning (MIL) method of 0.317 and Google’s deep neural network baseline of 0.314. In addition, we discover that the audio tagging performance on AudioSet embedding features has a weak correlation with the number of training samples and the quality of labels of each sound class.},
	language = {en},
	number = {11},
	urldate = {2021-01-13},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Kong, Qiuqiang and Yu, Changsong and Xu, Yong and Iqbal, Turab and Wang, Wenwu and Plumbley, Mark D.},
	month = nov,
	year = {2019},
	pages = {1791--1802},
	file = {Kong et al. - 2019 - Weakly Labelled AudioSet Tagging With Attention Ne.pdf:/Users/rosscutler/Zotero/storage/YT76IQIU/Kong et al. - 2019 - Weakly Labelled AudioSet Tagging With Attention Ne.pdf:application/pdf},
}

@article{snyder_musan_nodate,
	title = {{MUSAN}: {A} {Music}, {Speech}, and {Noise} {Corpus}},
	abstract = {This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a ﬂexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identiﬁcation.},
	language = {en},
	author = {Snyder, David and Chen, Guoguo and Povey, Daniel},
	pages = {4},
	file = {Snyder et al. - MUSAN A Music, Speech, and Noise Corpus.pdf:/Users/rosscutler/Zotero/storage/2ZX7IKJQ/Snyder et al. - MUSAN A Music, Speech, and Noise Corpus.pdf:application/pdf},
}

@article{hussain_swishnet_nodate,
	title = {{SwishNet}: {A} {Fast} {Convolutional} {Neural} {Network} for {Speech}, {Music} and {Noise} {Classiﬁcation} and {Segmentation}},
	abstract = {Speech, Music and Noise classiﬁcation/segmentation is an important preprocessing step for audio processing/indexing. To this end, we propose a novel 1D Convolutional Neural Network (CNN) - SwishNet. It is a fast and lightweight architecture that operates on MFCC features which is suitable to be added to the front-end of an audio processing pipeline. We showed that the performance of our network can be improved by distilling knowledge from a 2D CNN, pretrained on ImageNet. We investigated the performance of our network on the MUSAN corpus - an openly available comprehensive collection of noise, music and speech samples, suitable for deep learning. The proposed network achieved high overall accuracy in clip (length of 0.52s) classiﬁcation ({\textgreater}97\% accuracy) and frame-wise segmentation ({\textgreater}93\% accuracy) tasks with even higher accuracy ({\textgreater}99\%) in speech/non-speech discrimination task. To verify the robustness of our model, we trained it on MUSAN and evaluated it on a different corpus - GTZAN and found good accuracy with very little ﬁne-tuning. We also demonstrated that our model is fast on both CPU and GPU, consumes a low amount of memory and is suitable for implementation in embedded systems.},
	language = {en},
	author = {Hussain, Shamim and Haque, Mohammad Ariful},
	pages = {7},
	file = {Hussain and Haque - SwishNet A Fast Convolutional Neural Network for .pdf:/Users/rosscutler/Zotero/storage/TDSD466B/Hussain and Haque - SwishNet A Fast Convolutional Neural Network for .pdf:application/pdf},
}

@inproceedings{sheng_quantization-friendly_2018,
	address = {Williamsburg, VA},
	title = {A {Quantization}-{Friendly} {Separable} {Convolution} for {MobileNets}},
	isbn = {978-1-5386-7367-6},
	url = {https://ieeexplore.ieee.org/document/8524017/},
	doi = {10.1109/EMC2.2018.00011},
	abstract = {As deep learning (DL) is being rapidly pushed to edge computing, researchers invented various ways to make inference computation more efficient on mobile/IoT devices, such as network pruning, parameter compression, and etc. Quantization, as one of the key approaches, can effectively offload GPU, and make it possible to deploy DL on fixed-point pipeline. Unfortunately, not all existing networks design are friendly to quantization. For example, the popular lightweight MobileNetV1 [1], while it successfully reduces parameter size and computation latency with separable convolution, our experiment shows its quantized models have large accuracy gap against its float point models. To resolve this, we analyzed the root cause of quantization loss and proposed a quantization-friendly separable convolution architecture. By evaluating the image classification task on ImageNet2012 dataset, our modified MobileNetV1 model can archive 8-bit inference top-1 accuracy in 68.03\%, almost closed the gap to the float pipeline.},
	language = {en},
	urldate = {2021-01-13},
	booktitle = {2018 1st {Workshop} on {Energy} {Efficient} {Machine} {Learning} and {Cognitive} {Computing} for {Embedded} {Applications} ({EMC2})},
	publisher = {IEEE},
	author = {Sheng, Tao and Feng, Chen and Zhuo, Shaojie and Zhang, Xiaopeng and Shen, Liang and Aleksic, Mickey},
	month = mar,
	year = {2018},
	pages = {14--18},
	file = {Sheng et al. - 2018 - A Quantization-Friendly Separable Convolution for .pdf:/Users/rosscutler/Zotero/storage/3SA4RAE3/Sheng et al. - 2018 - A Quantization-Friendly Separable Convolution for .pdf:application/pdf;Sheng et al. - 2018 - A Quantization-Friendly Separable Convolution for .pdf:/Users/rosscutler/Zotero/storage/TTRYSULL/Sheng et al. - 2018 - A Quantization-Friendly Separable Convolution for .pdf:application/pdf},
}

@article{kong_panns_2020,
	title = {{PANNs}: {Large}-{Scale} {Pretrained} {Audio} {Neural} {Networks} for {Audio} {Pattern} {Recognition}},
	volume = {28},
	issn = {2329-9290, 2329-9304},
	shorttitle = {{PANNs}},
	url = {https://ieeexplore.ieee.org/document/9229505/},
	doi = {10.1109/TASLP.2020.3030497},
	abstract = {Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classiﬁcation, music classiﬁcation, speech emotion classiﬁcation and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on speciﬁc datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset. These PANNs are transferred to other audio related tasks. We investigate the performance and computational complexity of PANNs modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-CNN using both log-mel spectrogram and waveform as input feature. Our best PANN system achieves a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the best previous system of 0.392. We transfer PANNs to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of PANNs: https://github.com/qiuqiangkong/ audioset\_tagging\_cnn.},
	language = {en},
	urldate = {2021-01-13},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D.},
	year = {2020},
	pages = {2880--2894},
	file = {Kong et al. - 2020 - PANNs Large-Scale Pretrained Audio Neural Network.pdf:/Users/rosscutler/Zotero/storage/WIDR9EWK/Kong et al. - 2020 - PANNs Large-Scale Pretrained Audio Neural Network.pdf:application/pdf},
}

@inproceedings{iqbal_capsule_2018,
	address = {Rome},
	title = {Capsule {Routing} for {Sound} {Event} {Detection}},
	isbn = {978-90-827970-1-5},
	url = {https://ieeexplore.ieee.org/document/8553198/},
	doi = {10.23919/EUSIPCO.2018.8553198},
	abstract = {The detection of acoustic scenes is a challenging problem in which environmental sound events must be detected from a given audio signal. This includes classifying the events as well as estimating their onset and offset times. We approach this problem with a neural network architecture that uses the recentlyproposed capsule routing mechanism. A capsule is a group of activation units representing a set of properties for an entity of interest, and the purpose of routing is to identify part-whole relationships between capsules. That is, a capsule in one layer is assumed to belong to a capsule in the layer above in terms of the entity being represented. Using capsule routing, we wish to train a network that can learn global coherence implicitly, thereby improving generalization performance. Our proposed method is evaluated on Task 4 of the DCASE 2017 challenge. Results show that classiﬁcation performance is state-of-the-art, achieving an Fscore of 58.6\%. In addition, overﬁtting is reduced considerably compared to other architectures.},
	language = {en},
	urldate = {2021-01-13},
	booktitle = {2018 26th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {IEEE},
	author = {Iqbal, Turab and Xu, Yong and Kong, Qiuqiang and Wang, Wenwu},
	month = sep,
	year = {2018},
	pages = {2255--2259},
	file = {Iqbal et al. - 2018 - Capsule Routing for Sound Event Detection.pdf:/Users/rosscutler/Zotero/storage/73YP5LAH/Iqbal et al. - 2018 - Capsule Routing for Sound Event Detection.pdf:application/pdf},
}

@inproceedings{kong_audio_2018,
	address = {Calgary, AB},
	title = {Audio {Set} {Classification} with {Attention} {Model}: {A} {Probabilistic} {Perspective}},
	isbn = {978-1-5386-4658-8},
	shorttitle = {Audio {Set} {Classification} with {Attention} {Model}},
	url = {https://ieeexplore.ieee.org/document/8461392/},
	doi = {10.1109/ICASSP.2018.8461392},
	abstract = {This paper investigates the Audio Set classiﬁcation. Audio Set is a large scale weakly labelled dataset (WLD) of audio clips. In WLD only the presence of a label is known, without knowing the happening time of the labels. We propose an attention model to solve this WLD problem and explain the attention model from a novel probabilistic perspective. Each audio clip in Audio Set consists of a collection of features. We call each feature as an instance and the collection as a bag following the terminology in multiple instance learning. In the attention model, each instance in the bag has a trainable probability measure for each class. The classiﬁcation of the bag is the expectation of the classiﬁcation output of the instances in the bag with respect to the learned probability measure. Experiments show that the proposed attention model achieves a mAP of 0.327 on Audio Set, outperforming the Google’s baseline of 0.314.},
	language = {en},
	urldate = {2021-01-13},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Kong, Qiuqiang and Xu, Yong and Wang, Wenwu and Plumbley, Mark D.},
	month = apr,
	year = {2018},
	pages = {316--320},
	file = {Kong et al. - 2018 - Audio Set Classification with Attention Model A P.pdf:/Users/rosscutler/Zotero/storage/5STVBH8G/Kong et al. - 2018 - Audio Set Classification with Attention Model A P.pdf:application/pdf},
}

@inproceedings{xu_large-scale_2018,
	address = {Calgary, AB},
	title = {Large-{Scale} {Weakly} {Supervised} {Audio} {Classification} {Using} {Gated} {Convolutional} {Neural} {Network}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8461975/},
	doi = {10.1109/ICASSP.2018.8461975},
	abstract = {In this paper, we present a gated convolutional neural network and a temporal attention-based localization method for audio classiﬁcation, which won the 1st place in the large-scale weakly supervised sound event detection task of Detection and Classiﬁcation of Acoustic Scenes and Events (DCASE) 2017 challenge. The audio clips in this task, which are extracted from YouTube videos, are manually labelled with one or more audio tags, but without time stamps of the audio events, hence referred to as weakly labelled data. Two subtasks are deﬁned in this challenge including audio tagging and sound event detection using this weakly labelled data. We propose a convolutional recurrent neural network (CRNN) with learnable gated linear units (GLUs) non-linearity applied on the log Mel spectrogram. In addition, we propose a temporal attention method along the frames to predict the locations of each audio event in a chunk from the weakly labelled data. The performances of our systems were ranked the 1st and the 2nd as a team in these two sub-tasks of DCASE 2017 challenge with F value 55.6\% and Equal error 0.73, respectively.},
	language = {en},
	urldate = {2021-01-13},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Xu, Yong and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D.},
	month = apr,
	year = {2018},
	pages = {121--125},
	file = {Xu et al. - 2018 - Large-Scale Weakly Supervised Audio Classification.pdf:/Users/rosscutler/Zotero/storage/R4EUULEP/Xu et al. - 2018 - Large-Scale Weakly Supervised Audio Classification.pdf:application/pdf;Xu et al. - 2018 - Large-Scale Weakly Supervised Audio Classification.pdf:/Users/rosscutler/Zotero/storage/2RSWHSBK/Xu et al. - 2018 - Large-Scale Weakly Supervised Audio Classification.pdf:application/pdf},
}

@inproceedings{sridhar_icassp_2021,
	title = {{ICASSP} 2021 {Acoustic} {Echo} {Cancellation} {Challenge}: {Datasets}, {Testing} {Framework}, and {Results}},
	shorttitle = {{ICASSP} 2021 {Acoustic} {Echo} {Cancellation} {Challenge}},
	abstract = {The ICASSP 2021 Acoustic Echo Cancellation Challenge is intended to stimulate research in the area of acoustic echo cancellation (AEC), which is an important part of speech enhancement and still a top issue in audio communication and conferencing systems. Many recent AEC studies report good performance on synthetic datasets where the train and test samples come from the same underlying distribution. However, the AEC performance often degrades significantly on real recordings. Also, most of the conventional objective metrics such as echo return loss enhancement (ERLE) and perceptual evaluation of speech quality (PESQ) do not correlate well with subjective speech quality tests in the presence of background noise and reverberation found in realistic environments. In this challenge, we open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 2,500 real audio devices and human speakers in real environments, as well as a synthetic dataset. We open source two large test sets, and we open source an online subjective test framework for researchers to quickly test their results. The winners of this challenge will be selected based on the average Mean Opinion Score (MOS) achieved across all different single talk and double talk scenarios.},
	booktitle = {{ICASSP}},
	author = {Sridhar, Kusha and Cutler, Ross and Saabas, Ando and Parnamaa, Tanel and Loide, Markus and Gamper, Hannes and Braun, Sebastian and Aichner, Robert and Srinivasan, Sriram},
	year = {2021},
	note = {arXiv: 2009.04972},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/QUWE7SQI/Sridhar et al. - 2020 - ICASSP 2021 Acoustic Echo Cancellation Challenge .pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/J7AB4HWY/2009.html:text/html},
}

@article{lawanyashri_energy-aware_2017,
	title = {Energy-aware hybrid fruitfly optimization for load balancing in cloud environments for {EHR} applications},
	volume = {8},
	issn = {23529148},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352914817300187},
	doi = {10.1016/j.imu.2017.02.005},
	abstract = {Cloud computing has gained precise attention from the research community and management of IT, due to its scalable and dynamic capabilities. It is evolving as a vibrant technology to modernize and restructure healthcare organization to provide best services to the consumers. The rising demand for healthcare services and applications in cloud computing leads to the imbalance in resource usage and drastically increases the power consumption resulting in high operating cost. To achieve fast execution time and optimum utilization of the virtual machines, we propose a multi-objective hybrid fruitﬂy optimization technique based on simulated annealing to improve the convergence rate and optimization accuracy. The proposed approach is used to achieve the optimal resource utilization and reduces the energy consumption and cost in cloud computing environment. The result attained in our proposed technique provides an improved solution. The experimental results show that the proposed algorithm eﬃciently outperforms compared to the existing load balancing algorithms.},
	language = {en},
	urldate = {2020-11-13},
	journal = {Informatics in Medicine Unlocked},
	author = {Lawanyashri, M. and Balusamy, Balamurugan and Subha, S.},
	year = {2017},
	pages = {42--50},
	file = {Lawanyashri et al. - 2017 - Energy-aware hybrid fruitfly optimization for load.pdf:/Users/rosscutler/Zotero/storage/3DHYCKC4/Lawanyashri et al. - 2017 - Energy-aware hybrid fruitfly optimization for load.pdf:application/pdf},
}

@article{patel_enhanced_2015,
	title = {Enhanced {Load} {Balanced} {Min}-min {Algorithm} for {Static} {Meta} {Task} {Scheduling} in {Cloud} {Computing}},
	volume = {57},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050915019146},
	doi = {10.1016/j.procs.2015.07.385},
	language = {en},
	urldate = {2020-11-13},
	journal = {Procedia Computer Science},
	author = {Patel, Gaurang and Mehta, Rutvik and Bhoi, Upendra},
	year = {2015},
	pages = {545--553},
	file = {Patel et al. - 2015 - Enhanced Load Balanced Min-min Algorithm for Stati.pdf:/Users/rosscutler/Zotero/storage/CY7JTVFU/Patel et al. - 2015 - Enhanced Load Balanced Min-min Algorithm for Stati.pdf:application/pdf},
}

@article{mondal_load_2012,
	title = {Load {Balancing} in {Cloud} {Computing} using {Stochastic} {Hill} {Climbing}-{A} {Soft} {Computing} {Approach}},
	volume = {4},
	issn = {22120173},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212017312004070},
	doi = {10.1016/j.protcy.2012.05.128},
	abstract = {Cloud Computing, a new concept is a pool of virtualized computer resources. An Internet-based development where dynamically scalable and often virtualized resources are provided as a service over the Internet has become a signiﬁcant issue. Cloud computing describes both a platform and type of application. A cloud computing platform dynamically provisions, conﬁgures, reconﬁgures, and deprovisions servers as needed. Servers in the cloud can be physical machines or virtual machines spanned across the network. Thus it utilizes the computing resources (service nodes) on the network to facilitate the execution of complicated tasks that require large-scale computation. Selecting nodes (load balancing) for executing a task in the cloud computing must be considered, and to exploit the eﬀectiveness of the resources, they have to be properly selected according to the properties of the task. In this paper, a soft computing based load balancing approach has been proposed. A local optimization approach Stochastic Hill climbing is used for allocation of incoming jobs to the servers or virtual machines(VMs). Performance of the algorithm is analyzed both qualitatively and quantitatively using CloudAnalyst. CloudAnalyst is a CloudSim-based Visual Modeller for analyzing cloud computing environments and applications. A comparison is also made with Round Robin and First Come First Serve (FCFS) algorithms.},
	language = {en},
	urldate = {2020-11-13},
	journal = {Procedia Technology},
	author = {Mondal, Brototi and Dasgupta, Kousik and Dutta, Paramartha},
	year = {2012},
	pages = {783--789},
	file = {Mondal et al. - 2012 - Load Balancing in Cloud Computing using Stochastic.pdf:/Users/rosscutler/Zotero/storage/CY3PX7RY/Mondal et al. - 2012 - Load Balancing in Cloud Computing using Stochastic.pdf:application/pdf},
}

@inproceedings{yao_dynamic_2009,
	address = {Beijing, China},
	title = {On {Dynamic} {Pricing} in {Market}-{Based} {Resource} {Management} for {Clusters}},
	isbn = {978-1-4244-4638-4},
	url = {http://ieeexplore.ieee.org/document/5303551/},
	doi = {10.1109/ICMSS.2009.5303551},
	language = {en},
	urldate = {2020-11-13},
	booktitle = {2009 {International} {Conference} on {Management} and {Service} {Science}},
	publisher = {IEEE},
	author = {Yao, Yong-Lei and Liu, Jing-Fa},
	month = sep,
	year = {2009},
	pages = {1--4},
	file = {Yao and Liu - 2009 - On Dynamic Pricing in Market-Based Resource Manage.pdf:/Users/rosscutler/Zotero/storage/XHLZ87JH/Yao and Liu - 2009 - On Dynamic Pricing in Market-Based Resource Manage.pdf:application/pdf},
}

@article{samal_analysis_2013,
	title = {Analysis of variants in {Round} {Robin} {Algorithms} for load balancing in {Cloud} {Computing}},
	volume = {4},
	abstract = {Cloud computing is the emerging internet based technology which emphasizes commercial computing. Cloud is a platform providing dynamic pool resources and virtualization. Based on a pay-as-you-go model, it enables hosting of pervasive applications from consumer, scientific, and business domains. To properly manage the resources of the service provider, load balancing is required for the jobs that are submitted to the service provider. Load balancing also helps in improving the performance of the centralized server. In the present work, various policies in relation to the algorithms developed are analyzed using an analysis tool, namely, cloud analyst. Comparison is also made for variants of Round Robin (RR) algorithms.},
	language = {en},
	author = {Samal, Pooja and Mishra, Pranati},
	year = {2013},
	pages = {4},
	file = {Samal and Mishra - 2013 - Analysis of variants in Round Robin Algorithms for.pdf:/Users/rosscutler/Zotero/storage/UPVF9D5D/Samal and Mishra - 2013 - Analysis of variants in Round Robin Algorithms for.pdf:application/pdf},
}

@inproceedings{chen_rial_2014,
	address = {Toronto, ON, Canada},
	title = {{RIAL}: {Resource} {Intensity} {Aware} {Load} balancing in clouds},
	isbn = {978-1-4799-3360-0},
	shorttitle = {{RIAL}},
	url = {http://ieeexplore.ieee.org/document/6848062/},
	doi = {10.1109/INFOCOM.2014.6848062},
	abstract = {To provide robust infrastructure as a service (IaaS), clouds currently perform load balancing by migrating virtual machines (VMs) from heavily loaded physical machines (PMs) to lightly loaded PMs. The unique features of clouds pose formidable challenges to achieving effective and efﬁcient load balancing. First, VMs in clouds use different resources (e.g., CPU, bandwidth, memory) to serve a variety of services (e.g., high performance computing, web services, ﬁle services), resulting in different overutilized resources in different PMs. Also, the overutilized resources in a PM may vary over time due to the time-varying heterogenous service requests. Second, there is intensive network communication between VMs. However, previous load balancing methods statically assign equal or predeﬁned weights to different resources, which leads to degraded performance in terms of speed and cost to achieve load balance. Also, they do not strive to minimize the VM communications between PMs. We propose a Resource Intensity Aware Load balancing method (RIAL). For each PM, RIAL dynamically assigns different weights to different resources according to their usage intensity in the PM, which signiﬁcantly reduces the time and cost to achieve load balance and avoids future load imbalance. It also tries to keep frequently communicating VMs in the same PM to reduce bandwidth cost, and migrate VMs to PMs with minimum VM performance degradation. Our extensive trace-driven simulation results and real-world experimental results show the superior performance of RIAL compared to other load balancing methods.},
	language = {en},
	urldate = {2020-11-12},
	booktitle = {{IEEE} {INFOCOM} 2014 - {IEEE} {Conference} on {Computer} {Communications}},
	publisher = {IEEE},
	author = {Chen, Liuhua and Shen, Haiying and Sapra, Karan},
	month = apr,
	year = {2014},
	pages = {1294--1302},
	file = {Chen et al. - 2014 - RIAL Resource Intensity Aware Load balancing in c.pdf:/Users/rosscutler/Zotero/storage/F6CZVMPY/Chen et al. - 2014 - RIAL Resource Intensity Aware Load balancing in c.pdf:application/pdf},
}

@inproceedings{huankai_chen_user-priority_2013,
	address = {Bangalore, India},
	title = {User-priority guided {Min}-{Min} scheduling algorithm for load balancing in cloud computing},
	isbn = {978-1-4799-1591-0 978-1-4799-1589-7},
	url = {http://ieeexplore.ieee.org/document/6621389/},
	doi = {10.1109/ParCompTech.2013.6621389},
	abstract = {Cloud computing is emerging as a new paradigm of large-scale distributed computing. In order to utilize the power of cloud computing completely, we need an efficient task scheduling algorithm. The traditional Min-Min algorithm is a simple, efficient algorithm that produces a better schedule that minimizes the total completion time of tasks than other algorithms in the literature [7]. However the biggest drawback of it is load imbalanced, which is one of the central issues for cloud providers. In this paper, an improved load balanced algorithm is introduced on the ground of Min-Min algorithm in order to reduce the makespan and increase the resource utilization (LBIMM). At the same time, Cloud providers offer computer resources to users on a pay-per-use base. In order to accommodate the demands of different users, they may offer different levels of quality for services. Then the cost per resource unit depends on the services selected by the user. In return, the user receives guarantees regarding the provided resources. To observe the promised guarantees, user-priority was considered in our proposed PA-LBIMM so that user’s demand could be satisfied more completely. At last, the introduced algorithm is simulated using Matlab toolbox. The simulation results show that the improved algorithm can lead to significant performance gain and achieve over 20\% improvement on both VIP user satisfaction and resource utilization ratio.},
	language = {en},
	urldate = {2020-11-12},
	booktitle = {2013 {National} {Conference} on {Parallel} {Computing} {Technologies} ({PARCOMPTECH})},
	publisher = {IEEE},
	author = {{Huankai Chen} and Wang, Frank and Helian, Na and Akanmu, Gbola},
	month = feb,
	year = {2013},
	pages = {1--8},
	file = {Huankai Chen et al. - 2013 - User-priority guided Min-Min scheduling algorithm .pdf:/Users/rosscutler/Zotero/storage/PDECEGPW/Huankai Chen et al. - 2013 - User-priority guided Min-Min scheduling algorithm .pdf:application/pdf},
}

@inproceedings{nuaimi_survey_2012,
	address = {London, United Kingdom},
	title = {A {Survey} of {Load} {Balancing} in {Cloud} {Computing}: {Challenges} and {Algorithms}},
	isbn = {978-1-4673-5581-0 978-0-7695-4943-9},
	shorttitle = {A {Survey} of {Load} {Balancing} in {Cloud} {Computing}},
	url = {http://ieeexplore.ieee.org/document/6472470/},
	doi = {10.1109/NCCA.2012.29},
	abstract = {Load Balancing is essential for efficient operations in distributed environments. As Cloud Computing is growing rapidly and clients are demanding more services and better results, load balancing for the Cloud has become a very interesting and important research area. Many algorithms were suggested to provide efficient mechanisms and algorithms for assigning the client’s requests to available Cloud nodes. These approaches aim to enhance the overall performance of the Cloud and provide the user more satisfying and efficient services. In this paper, we investigate the different algorithms proposed to resolve the issue of load balancing and task scheduling in Cloud Computing. We discuss and compare these algorithms to provide an overview of the latest approaches in the field.},
	language = {en},
	urldate = {2020-11-12},
	booktitle = {2012 {Second} {Symposium} on {Network} {Cloud} {Computing} and {Applications}},
	publisher = {IEEE},
	author = {Nuaimi, Klaithem Al and Mohamed, Nader and Nuaimi, Mariam Al and Al-Jaroodi, Jameela},
	month = dec,
	year = {2012},
	pages = {137--142},
	file = {Nuaimi et al. - 2012 - A Survey of Load Balancing in Cloud Computing Cha.pdf:/Users/rosscutler/Zotero/storage/3RS6JW7B/Nuaimi et al. - 2012 - A Survey of Load Balancing in Cloud Computing Cha.pdf:application/pdf},
}

@article{kaur_load_nodate,
	title = {Load {Balancing} in {Cloud} {Computing}},
	abstract = {Cloud computing helps to share data and provide many resources to users. Users pay only for those resources as much they used. Cloud computing stores the data and distributed resources in the open environment. The amount of data storage increases quickly in open environment. So, load balancing is a main challenge in cloud environment. Load balancing is helped to distribute the dynamic workload across multiple nodes to ensure that no single node is overloaded. It helps in proper utilization of resources .It also improve the performance of the system. Many existing algorithms provide load balancing and better resource utilization. There are various types load are possible in cloud computing like memory, CPU and network load. Load balancing is the process of finding overloaded nodes and then transferring the extra load to other nodes.},
	language = {en},
	author = {Kaur, Rajwinder and Luthra, Pawan},
	pages = {8},
	file = {Kaur and Luthra - Load Balancing in Cloud Computing.pdf:/Users/rosscutler/Zotero/storage/5Q4YYHWK/Kaur and Luthra - Load Balancing in Cloud Computing.pdf:application/pdf},
}

@article{_study_2013,
	title = {A {STUDY} {OF} {LOAD} {DISTRIBUTION} {ALGORITHMS} {IN} {DISTRIBUTED} {SCHEDULING}},
	volume = {02},
	issn = {23217308, 23191163},
	url = {https://ijret.org/volumes/2013v02/i14/IJRET20130214007.pdf},
	doi = {10.15623/ijret.2013.0214007},
	abstract = {As we know that in distributed systems several autonomous computers are interconnected to provide a single coherent view of a powerful system and these autonomous computers work independently in a team-like fashion such that the domain of tasks is shared between all of them. Now to take full advantage of this distributed scenario, we need good resource allocation schemes. Load distribution algorithm’s work is to deliverately distributed and re-distributes the tasks (loads) among all the participating nodes so that the overall performance of the entire system is maximized. in this paper we study the details of the load distribution algorithms and their suitableness in various load scenerios.},
	language = {en},
	number = {14},
	urldate = {2020-11-12},
	journal = {International Journal of Research in Engineering and Technology},
	author = {., Shounak Chakraborty},
	month = dec,
	year = {2013},
	pages = {37--40},
	file = {- 2013 - A STUDY OF LOAD DISTRIBUTION ALGORITHMS IN DISTRIB.pdf:/Users/rosscutler/Zotero/storage/KBBB3D7X/ - 2013 - A STUDY OF LOAD DISTRIBUTION ALGORITHMS IN DISTRIB.pdf:application/pdf},
}

@misc{noauthor_results_nodate,
	title = {Results {\textbar} {Manage} {\textbar} {Requester} {\textbar} {Amazon} {Mechanical} {Turk}},
	url = {https://requester.mturk.com/batches/4216726?openid.pape.max_auth_age=43200&openid.identity=https%3A%2F%2Fwww.amazon.com%2Fap%2Fid%2Famzn1.account.AHMTOH2CSWNXCGP2KGYAZAVHZNRQ&stashKey=812ce2f0-a337-44dd-b2d6-7b421f9896eb&openid.claimed_id=https%3A%2F%2Fwww.amazon.com%2Fap%2Fid%2Famzn1.account.AHMTOH2CSWNXCGP2KGYAZAVHZNRQ},
	urldate = {2020-10-19},
	file = {Results | Manage | Requester | Amazon Mechanical Turk:/Users/rosscutler/Zotero/storage/VAPVPBT3/4216726.html:text/html},
}

@article{rogelberg_not_2006,
	title = {"{Not} {Another} {Meeting}!" {Are} {Meeting} {Time} {Demands} {Related} to {Employee} {Well}-{Being}?},
	volume = {91},
	doi = {10.1037/0021-9010.91.1.83},
	abstract = {Using an interruptions framework, this article proposes and tests a set of hypotheses concerning the relationship of meeting time demands with job attitudes and well-being (JAWB). Two Internet surveys were administered to employees who worked 35 hr or more per week. Study 1 examined prescheduled meetings attended in a typical week (N=676), whereas Study 2 investigated prescheduled meetings attended during the current day (N=304). As proposed, the relationship between meeting time demands and JAWB was moderated by task interdependence, meeting experience quality, and accomplishment striving. However, results were somewhat dependent on the time frame of a study and the operational definition used for meeting time demands. Furthermore, perceived meeting effectiveness was found to have a strong, direct relationship with JAWB.},
	journal = {The Journal of applied psychology},
	author = {Rogelberg, Steven and Leach, Desmond and Warr, Peter and Burnfield, Jennifer},
	month = feb,
	year = {2006},
	pages = {83--96},
	file = {Submitted Version:/Users/rosscutler/Zotero/storage/8X37J3HA/Rogelberg et al. - 2006 - Not Another Meeting! Are Meeting Time Demands Re.pdf:application/pdf},
}

@article{pearce_expectations_2004,
	title = {Expectations of organizational mobility, workplace social inclusion, and employee job performance},
	volume = {25},
	issn = {0894-3796, 1099-1379},
	url = {http://doi.wiley.com/10.1002/job.232},
	doi = {10.1002/job.232},
	language = {en},
	number = {1},
	urldate = {2020-10-16},
	journal = {Journal of Organizational Behavior},
	author = {Pearce, Jone L. and Randel, Amy E.},
	month = feb,
	year = {2004},
	pages = {81--98},
	file = {Pearce and Randel - 2004 - Expectations of organizational mobility, workplace.pdf:/Users/rosscutler/Zotero/storage/U33JWYYY/Pearce and Randel - 2004 - Expectations of organizational mobility, workplace.pdf:application/pdf},
}

@book{kline_principles_2015,
	title = {Principles and {Practice} of {Structural} {Equation} {Modeling}, {Fourth} {Edition}},
	isbn = {978-1-4625-2335-1},
	abstract = {Emphasizing concepts and rationale over mathematical minutiae, this is the most widely used, complete, and accessible structural equation modeling (SEM) text. Continuing the tradition of using real data examples from a variety of disciplines, the significantly revised fourth edition incorporates recent developments such as Pearl\&\#39;s graphing theory and the structural causal model (SCM), measurement invariance, and more. Readers gain a comprehensive understanding of all phases of SEM, from data collection and screening to the interpretation and reporting of the results. Learning is enhanced by exercises with answers, rules to remember, and topic boxes. The companion website supplies data, syntax, and output for the book\&\#39;s examples--now including files for Amos, EQS, LISREL, Mplus, Stata, and R (lavaan). New to This Edition *Extensively revised to cover important new topics: Pearl\&\#39;s graphing theory and the SCM, causal inference frameworks, conditional process modeling, path models for longitudinal data, item response theory, and more. *Chapters on best practices in all stages of SEM, measurement invariance in confirmatory factor analysis, and significance testing issues and bootstrapping. *Expanded coverage of psychometrics. *Additional computer tools: online files for all detailed examples, previously provided in EQS, LISREL, and Mplus, are now also given in Amos, Stata, and R (lavaan). *Reorganized to cover the specification, identification, and analysis of observed variable models separately from latent variable models. Pedagogical Features *Exercises with answers, plus end-of-chapter annotated lists of further reading. *Real examples of troublesome data, demonstrating how to handle typical problems in analyses. *Topic boxes on specialized issues, such as causes of nonpositive definite correlations. *Boxed rules to remember. *Website promoting a learn-by-doing approach, including syntax and data files for six widely used SEM computer tools.},
	language = {en},
	publisher = {Guilford Publications},
	author = {Kline, Rex B.},
	month = nov,
	year = {2015},
	note = {Google-Books-ID: Q61ECgAAQBAJ},
	keywords = {Business \& Economics / Statistics, Education / Statistics, Medical / Psychiatry / General, Psychology / Statistics, Social Science / Research},
}

@book{ferdman_diversity_2014,
	address = {San Francisco, CA},
	series = {The professional practice series},
	title = {Diversity at work: the practice of inclusion},
	isbn = {978-1-118-41515-3 978-1-118-41782-9},
	shorttitle = {Diversity at work},
	language = {en},
	publisher = {Jossey-Bass, A Wiley Brand},
	editor = {Ferdman, Bernardo M. and Deane, Barbara},
	year = {2014},
	keywords = {Corporate culture, Diversity in the workplace},
	file = {Ferdman and Deane - 2014 - Diversity at work the practice of inclusion.pdf:/Users/rosscutler/Zotero/storage/M4IIWGFJ/Ferdman and Deane - 2014 - Diversity at work the practice of inclusion.pdf:application/pdf},
}

@article{rice_improving_2020,
	title = {Improving employee emotional and behavioral investments through the trickle-down effect of organizational inclusiveness and the role of moral supervisors},
	issn = {1573-353X},
	url = {https://doi.org/10.1007/s10869-019-09675-2},
	doi = {10.1007/s10869-019-09675-2},
	abstract = {Over two (i.e., a 2 × 2 experiment and a multi-source field study) studies, we propose and demonstrate how employees increase their emotional (i.e., affective commitment) and behavioral (i.e., citizenship behavior) investments in the workplace as a valuable outcome of the trickle-down effect of organizational inclusiveness. We also explain how supervisory moral identity impacts the trickle-down effect. Notably, the research integrates social cognitive theory with the diversity and inclusion literature to enhance our understanding as to how organizations can create a welcoming environment for all organizational members. Theoretical and practical implications are discussed.},
	language = {en},
	urldate = {2020-10-11},
	journal = {Journal of Business and Psychology},
	author = {Rice, Darryl B. and Young, Nicole C. J. and Sheridan, Sharon},
	month = jan,
	year = {2020},
}

@article{ashikali_diversity_2013,
	title = {Diversity {Management} in {Public} {Organizations} and {Its} {Effect} on {Employees}’ {Affective} {Commitment}},
	doi = {10.1177/0734371X13511088},
	abstract = {The purpose of this article is to explore the link between diversity management in public organizations and employees’ affective commitment by testing hypotheses on the mediating roles of transformational leadership and inclusive organizational culture. By combining theories on human resource management and performance with theories on diversity and inclusiveness, a theoretical model is built explaining when and why diversity management should positively affect employees’ affective commitment. Survey data from a representative sample of 10,976 Dutch public sector employees were used in testing our hypotheses using structural equation modeling techniques. Results show that the effect of diversity management on employees’ affective commitment can partially be explained by its impact on the inclusiveness of the organizational culture. In addition, the impact is influenced through the transformational leadership shown by supervisors who can be considered as the implementers of diversity management and as agents in creating inclusiveness. The implications for future research and management practice are further discussed.},
	journal = {Review of Public Personnel Administration},
	author = {Ashikali, Tanachia and Groeneveld, Sandra},
	month = nov,
	year = {2013},
}

@article{noauthor_joint_nodate,
	title = {Joint {Speaker} {Counting}, {Speech} {Recognition}, and {Speaker} {Identification} for {Overlapped} {Speech} of {Any} {Number} of {Speakers}},
	journal = {arXiv},
}

@article{noauthor_detecting_nodate,
	title = {Detecting {Laughter} and {Filled} {Pauses} {Using} {Syllable}-{Based} {Features}},
	journal = {Interspeech 2013},
}

@article{sainath_convolutional_nodate,
	title = {Convolutional {Neural} {Networks} for {Small}-footprint {Keyword} {Spotting}},
	journal = {Interspeech 2015},
	author = {Sainath, Tara and Parada, Carolina},
}

@article{noauthor_memory_nodate,
	title = {A {MEMORY} {AUGMENTED} {ARCHITECTURE} {FOR} {CONTINUOUS} {SPEAKER} {IDENTIFICATION} {IN} {MEETINGS}},
}

@article{noauthor_combining_nodate,
	title = {Combining {Acoustics}, {Content} and {Interaction} {Features} to {Find} {Hot} {Spots} in {Meetings}},
	url = {https://ieeexplore.ieee.org/abstract/document/9053167},
	journal = {020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
}

@article{wann_what_1996,
	title = {What does virtual reality {NEED}?: human factors issues in the design of three-dimensional computer environments},
	volume = {44},
	issn = {10715819},
	shorttitle = {What does virtual reality {NEED}?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S107158199690035X},
	doi = {10.1006/ijhc.1996.0035},
	language = {en},
	number = {6},
	urldate = {2020-10-05},
	journal = {International Journal of Human-Computer Studies},
	author = {Wann, John and Mon-Williams, Mark},
	month = jun,
	year = {1996},
	pages = {829--847},
	file = {Wann and Mon-Williams - 1996 - What does virtual reality NEED human factors iss.pdf:/Users/rosscutler/Zotero/storage/I3MCR4UV/Wann and Mon-Williams - 1996 - What does virtual reality NEED human factors iss.pdf:application/pdf},
}

@article{hoffman_vergenceaccommodation_2008,
	title = {Vergence–accommodation conflicts hinder visual performance and cause visual fatigue},
	volume = {8},
	issn = {1534-7362},
	url = {https://jov.arvojournals.org/article.aspx?articleid=2122611},
	doi = {10.1167/8.3.33},
	language = {en},
	number = {3},
	urldate = {2020-10-05},
	journal = {Journal of Vision},
	author = {Hoffman, David M. and Girshick, Ahna R. and Akeley, Kurt and Banks, Martin S.},
	month = mar,
	year = {2008},
	note = {Publisher: The Association for Research in Vision and Ophthalmology},
	pages = {33--33},
	file = {Full Text PDF:/Users/rosscutler/Zotero/storage/6WF977L9/Hoffman et al. - 2008 - Vergence–accommodation conflicts hinder visual per.pdf:application/pdf;Snapshot:/Users/rosscutler/Zotero/storage/FPZI6Y7Y/article.html:text/html},
}

@misc{noauthor_zoom_2020,
	title = {‘{Zoom} fatigue’ is taxing the brain. {Here}'s why that happens.},
	url = {https://www.nationalgeographic.com/science/2020/04/coronavirus-zoom-fatigue-is-taxing-the-brain-here-is-why-that-happens/},
	abstract = {Video calls seemed an elegant solution to remote work, but they wear on the psyche in complicated ways.},
	language = {en},
	urldate = {2020-10-04},
	journal = {Science},
	month = apr,
	year = {2020},
	note = {Section: Science},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/XRFLJ9CI/coronavirus-zoom-fatigue-is-taxing-the-brain-here-is-why-that-happens.html:text/html},
}

@misc{moyer_eye_nodate,
	title = {Eye {Contact}: {How} {Long} {Is} {Too} {Long}?},
	shorttitle = {Eye {Contact}},
	url = {https://www.scientificamerican.com/article/eye-contact-how-long-is-too-long/},
	abstract = {Research explores the factors that influence our tolerance for long mutual gazes},
	language = {en},
	urldate = {2020-10-04},
	journal = {Scientific American},
	author = {Moyer, Melinda Wenner},
	doi = {10.1038/scientificamericanmind0116-8},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/ELJX2FQW/eye-contact-how-long-is-too-long.html:text/html},
}

@article{behrens_interplay_2019,
	title = {The {Interplay} {Between} {Face}-to-{Face} {Contact} and {Feedback} on {Cooperation} {During} {Real}-{Life} {Interactions}},
	volume = {43},
	issn = {1573-3653},
	url = {https://doi.org/10.1007/s10919-019-00314-1},
	doi = {10.1007/s10919-019-00314-1},
	abstract = {Cooperation forms the basis of our society and becomes increasingly essential during times of globalization. However, despite technological developments people still prefer to meet face-to-face, which has been shown to foster cooperation. However, what is still unclear is how this beneficial effect depends on what people know about their interaction partner. To examine this question, 58 dyads played an iterated Prisoner’s Dilemma game, sometimes facing each other, sometimes without face contact. Additionally, explicit feedback regarding their decisions was manipulated between dyads. The results revealed that participants were more cooperative when they saw each other compared to when they could not, and when receiving reliable compared to unreliable or no feedback. Contradicting our hypothesis that participants would rely more on nonverbal communication in the absence of explicit information, we observed that the two sources of information operated independently on cooperative behavior. Interestingly, although individuals mostly relied on explicit information if available, participants still cooperated more after their partner defected with face-to-face contact compared to no face-to-face contact. The results of our study have implications for real-life interactions, suggesting that face-to-face contact has beneficial effects on prosocial behavior even if people cannot verify whether their selfless acts are being reciprocated.},
	language = {en},
	number = {4},
	urldate = {2020-10-04},
	journal = {Journal of Nonverbal Behavior},
	author = {Behrens, Friederike and Kret, Mariska E.},
	month = dec,
	year = {2019},
	pages = {513--528},
	file = {Springer Full Text PDF:/Users/rosscutler/Zotero/storage/M994YBSD/Behrens and Kret - 2019 - The Interplay Between Face-to-Face Contact and Fee.pdf:application/pdf},
}

@article{arndt_using_2016,
	title = {Using electroencephalography to analyze sleepiness due to low-quality audiovisual stimuli},
	volume = {42},
	issn = {0923-5965},
	url = {http://www.sciencedirect.com/science/article/pii/S092359651600014X},
	doi = {10.1016/j.image.2016.01.011},
	abstract = {Standardized methods to assess the quality of experience (QoE) of multimedia focus typically on short sequences of approximately 10s and a subjective judgment of the test participant. Two main problems occur when using this methodology: On the one hand these short sequences do not represent the typical media usage, and on the other hand it is still not completely understood how these subjective ratings are formed within the test participant. To overcome the second issue and to gain insight into the internal processing, electroencephalography (EEG) has been introduced successfully to the domain of QoE. As it is possible to use EEG to assess the quality perception continuously, we present two studies using standard documentary audiovisual clips with a length of 40min and 60min as stimuli. During the presentation of these quality-wise manipulated test sequences, we record an electroencephalogram and other physiological measures. We show that features of the EEG recordings indicate a change in the cognitive state of the test participant during the exposure to low-quality compared to high-quality sequences.},
	language = {en},
	urldate = {2020-10-04},
	journal = {Signal Processing: Image Communication},
	author = {Arndt, Sebastian and Antons, Jan-Niklas and Schleicher, Robert and Möller, Sebastian},
	month = mar,
	year = {2016},
	keywords = {Quality of experience, Alpha frequency band power, Audiovisual, Cognitive state, Physiology},
	pages = {120--129},
	file = {ScienceDirect Snapshot:/Users/rosscutler/Zotero/storage/C26Q8MDH/S092359651600014X.html:text/html},
}

@article{arndt_using_2014,
	title = {Using {Electroencephalography} to {Measure} {Perceived} {Video} {Quality}},
	volume = {8},
	issn = {1932-4553, 1941-0484},
	url = {http://ieeexplore.ieee.org/document/6777327/},
	doi = {10.1109/JSTSP.2014.2313026},
	abstract = {Using less bandwidth on the one hand and delivering high quality content on the other hand is one of the big goals for video service providers. Standardized rating tests are commonly used to quantify audiovisual quality. In order to better understand the neuronal processes underlying these quality ratings physiological measures may provide insights. This paper shows results of a series of studies using a physiological measurement, namely electroencephalography (EEG), combined with standard rating methods. The experiments follow an incremental approach in stimuli selection, ranging from purely visual to audiovisual stimuli. Results show that EEG is a feasible complement measurement technique to assess audiovisual quality as we achieve high correlation values between subjective and physiological data consistently through all experiments.},
	language = {en},
	number = {3},
	urldate = {2020-10-04},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Arndt, Sebastian and Antons, Jan-Niklas and Schleicher, Robert and Moller, Sebastian and Curio, Gabriel},
	month = jun,
	year = {2014},
	pages = {366--376},
	file = {Arndt et al. - 2014 - Using Electroencephalography to Measure Perceived .pdf:/Users/rosscutler/Zotero/storage/VEECXCKG/Arndt et al. - 2014 - Using Electroencephalography to Measure Perceived .pdf:application/pdf},
}

@article{wiederhold_connecting_2020,
	title = {Connecting {Through} {Technology} {During} the {Coronavirus} {Disease} 2019 {Pandemic}: {Avoiding} “{Zoom} {Fatigue}”},
	volume = {23},
	issn = {2152-2715, 2152-2723},
	shorttitle = {Connecting {Through} {Technology} {During} the {Coronavirus} {Disease} 2019 {Pandemic}},
	url = {https://www.liebertpub.com/doi/10.1089/cyber.2020.29188.bkw},
	doi = {10.1089/cyber.2020.29188.bkw},
	language = {en},
	number = {7},
	urldate = {2020-10-04},
	journal = {Cyberpsychology, Behavior, and Social Networking},
	author = {Wiederhold, Brenda K.},
	month = jul,
	year = {2020},
	pages = {437--438},
	file = {Wiederhold - 2020 - Connecting Through Technology During the Coronavir.pdf:/Users/rosscutler/Zotero/storage/VNFL59QH/Wiederhold - 2020 - Connecting Through Technology During the Coronavir.pdf:application/pdf},
}

@article{reeves_effects_1999,
	title = {The {Effects} of {Screen} {Size} and {Message} {Content} on {Attention} and {Arousal}},
	volume = {1},
	issn = {1521-3269, 1532-785X},
	url = {http://www.tandfonline.com/doi/abs/10.1207/s1532785xmep0101_4},
	doi = {10.1207/s1532785xmep0101_4},
	language = {en},
	number = {1},
	urldate = {2020-10-04},
	journal = {Media Psychology},
	author = {Reeves, Byron and Lang, Annie and Kim, Eun Young and Tatar, Deborah},
	month = mar,
	year = {1999},
	pages = {49--67},
	file = {Reeves et al. - 1999 - The Effects of Screen Size and Message Content on .pdf:/Users/rosscutler/Zotero/storage/3KBAS33W/Reeves et al. - 1999 - The Effects of Screen Size and Message Content on .pdf:application/pdf},
}

@inproceedings{nguyen_multiview_2007,
	address = {San Jose, California, USA},
	title = {Multiview: improving trust in group video conferencing through spatial faithfulness},
	isbn = {978-1-59593-593-9},
	shorttitle = {Multiview},
	url = {http://dl.acm.org/citation.cfm?doid=1240624.1240846},
	doi = {10.1145/1240624.1240846},
	abstract = {Video conferencing is still considered a poor alternative to face-to-face meetings. In the business setting, where these systems are most prevalent, the misuse of video conferencing systems can have detrimental results, especially in high-stakes communications. Prior work suggests that spatial distortions of nonverbal cues, particularly gaze and deixis, negatively impact many aspects of effective communication in dyadic communications. However, video conferencing systems are often used for group-to-group meetings where spatial distortions are exacerbated. Meanwhile, its effects on the group dynamic are not well understood. In this study, we examine the effects that spatial distortions of nonverbal cues have on inter-group trust formation. We conducted a large (169 participant) study of group conferencing under various conditions. We found that the use of systems that introduce spatial distortions negatively affect trust formation patterns. On the other hand, these effects are essentially eliminated by using a spatially faithful video conferencing system.},
	language = {en},
	urldate = {2020-10-04},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '07},
	publisher = {ACM Press},
	author = {Nguyen, David T. and Canny, John},
	year = {2007},
	pages = {1465--1474},
	file = {Nguyen and Canny - 2007 - Multiview improving trust in group video conferen.pdf:/Users/rosscutler/Zotero/storage/LPJVDV8I/Nguyen and Canny - 2007 - Multiview improving trust in group video conferen.pdf:application/pdf},
}

@book{nguyen_more_2009,
	title = {More than {Face}-to-{Face}: {Empathy} {Effects} of {Video} {Framing}},
	shorttitle = {More than {Face}-to-{Face}},
	abstract = {Video conferencing attempts to convey subtle cues of face-to-face interaction (F2F), but it is generally believed to be less effective than F2F. We argue that careful design based on an understanding of non-verbal communication can mitigate these differences. In this paper, we study the effects of video image framing in one-on-one meetings on empathy formation. We alter the video image by framing the display such that, in one condition, only the head is visible while, in the other condition, the entire upper body is visible. We include a F2F control case. We used two measures of dyad empathy and found a significant difference between head-only framing and both upper-body framing and F2F, but no significant difference between upper-body framing and F2F. Based on these and earlier results, we present some design heuristics for video conferencing systems. We revisit earlier negative experimental results on video systems in the light of these new experiments. We conclude that for systems that preserve both gaze and upper-body cues, there is no evidence of deficit in communication effectiveness compared to face-to-face meetings.},
	author = {Nguyen, David T. and Canny, John},
	year = {2009},
	file = {Citeseer - Full Text PDF:/Users/rosscutler/Zotero/storage/LTKUN5Q9/Nguyen and Canny - 2009 - More than Face-to-Face Empathy Effects of Video F.pdf:application/pdf;Citeseer - Snapshot:/Users/rosscutler/Zotero/storage/SB3I9WPY/summary.html:text/html},
}

@inproceedings{rix_perceptual_2001,
	address = {Salt Lake City, UT, USA},
	title = {Perceptual evaluation of speech quality ({PESQ})-a new method for speech quality assessment of telephone networks and codecs},
	volume = {2},
	isbn = {978-0-7803-7041-8},
	url = {http://ieeexplore.ieee.org/document/941023/},
	doi = {10.1109/ICASSP.2001.941023},
	urldate = {2020-08-08},
	booktitle = {2001 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}. {Proceedings} ({Cat}. {No}.{01CH37221})},
	publisher = {IEEE},
	author = {Rix, A.W. and Beerends, J.G. and Hollier, M.P. and Hekstra, A.P.},
	year = {2001},
	pages = {749--752},
}

@inproceedings{hammer_well-tempered_2005,
	title = {The {Well}-{Tempered} {Conversation}: {Interactivity}, {Delay} and {Perceptual} {VoIP} {Quality}},
	shorttitle = {The {Well}-{Tempered} {Conversation}},
	abstract = {The factors causing perceptual quality impairment on Voice-over-IP (VoIP) connections include traditional network Quality-of-Service (QoS) parameters like packet loss rate, delay or jitter as well as parameters characterizing the conversation itself. Among the latter ones, we focus on the impact of "conversational interactivity" on the perceptual quality of a phone conversation. We introduce "Parametric Conversation Analysis" as a formal framework for the instrumental investigation of conversational parameters at different transmission delay conditions, we further present the notion of "conversational temperature" as an intuitive scalar metric for the interactivity of conversations, and we demonstrate the application of our methods to a set of conversation recordings performed under various delay conditions, also with respect to results of subjective quality ratings.},
	booktitle = {Proc. {IEEE} {Int}. {Conf}. on {Communications} ({ICC}), {Seoul}, {Korea}},
	author = {Hammer, Florian and Reichl, Peter and Raake, Alexander},
	year = {2005},
	file = {Citeseer - Full Text PDF:/Users/rosscutler/Zotero/storage/WL53J9TI/Hammer et al. - 2005 - The Well-Tempered Conversation Interactivity, Del.pdf:application/pdf;Citeseer - Snapshot:/Users/rosscutler/Zotero/storage/AX2USQ65/summary.html:text/html},
}

@article{mcshefferty_just-noticeable_2015,
	title = {The {Just}-{Noticeable} {Difference} in {Speech}-to-{Noise} {Ratio}},
	volume = {19},
	issn = {2331-2165},
	url = {https://doi.org/10.1177/2331216515572316},
	doi = {10.1177/2331216515572316},
	abstract = {Just-noticeable differences (JNDs) have been measured for various features of sounds, but despite its importance to communication, there is no benchmark for what is a just-noticeable?and possibly meaningful?difference in speech-to-noise ratio (SNR). SNR plays a crucial role in speech communication for normal-hearing and hearing-impaired listeners. Difficulty hearing speech in background noise?a poor SNR?often leads to dissatisfaction with hearing-assistance devices. While such devices attempt through various strategies to address this problem, it is not currently known how much improvement in SNR is needed to provide a noticeable benefit. To investigate what is a noticeable benefit, we measured the JND in SNR for both normal-hearing and hearing-impaired listeners. Here, we report the SNR JNDs of 69 participants of varying hearing ability, estimated using either an adaptive or fixed-level procedure. The task was to judge which of the two intervals containing a sentence in speech-spectrum noise presented over headphones was clearer. The level of each interval was roved to reduce the influence of absolute level cues. The results of both procedures showed an average SNR JND of 3?dB that was independent of hearing ability. Further experiments using a subset of normal-hearing listeners showed that level roving does elevate threshold. These results suggest that noise reduction schemes may need to achieve a benefit greater than 3?dB to be reliably discriminable.},
	urldate = {2020-06-19},
	journal = {Trends in Hearing},
	author = {McShefferty, David and Whitmer, William M. and Akeroyd, Michael A.},
	month = dec,
	year = {2015},
	note = {Publisher: SAGE Publications Inc},
	pages = {2331216515572316},
	file = {SAGE PDF Full Text:/Users/rosscutler/Zotero/storage/X3UH5J5S/McShefferty et al. - 2015 - The Just-Noticeable Difference in Speech-to-Noise .pdf:application/pdf},
}

@article{garcia_understanding_2019,
	title = {Understanding and estimating quality of experience in {WebRTC} applications},
	volume = {101},
	issn = {0010-485X, 1436-5057},
	url = {http://link.springer.com/10.1007/s00607-018-0669-7},
	doi = {10.1007/s00607-018-0669-7},
	abstract = {WebRTC comprises a set of technologies and standards that provide real-time communication with web browsers, simplifying the embedding of voice and video communication in web applications and mobile devices. The perceived quality of WebRTC communication can be measured using quality of experience (QoE) indicators. QoE is deﬁned as the degree of delight or annoyance of the user with an application or service. This paper is focused on the QoE assessment of WebRTC-based applications and its contribution is threefold. First, an analysis of how WebRTC topologies affect the quality perceived by users is provided. Second, a group of Key Performance Indicators for estimating the QoE of WebRTC users is proposed. Finally, a systematic survey of the literature on QoE assessment in the WebRTC arena is presented.},
	language = {en},
	number = {11},
	urldate = {2020-06-12},
	journal = {Computing},
	author = {García, Boni and Gallego, Micael and Gortázar, Francisco and Bertolino, Antonia},
	month = nov,
	year = {2019},
	pages = {1585--1607},
	file = {García et al. - 2019 - Understanding and estimating quality of experience.pdf:/Users/rosscutler/Zotero/storage/HUN98EBW/García et al. - 2019 - Understanding and estimating quality of experience.pdf:application/pdf},
}

@article{simundic_measures_2009,
	title = {Measures of {Diagnostic} {Accuracy}: {Basic} {Definitions}},
	volume = {19},
	issn = {1650-3414},
	shorttitle = {Measures of {Diagnostic} {Accuracy}},
	abstract = {Diagnostic accuracy relates to the ability of a test to discriminate between the target condition and health. This discriminative potential can be quantified by the measures of diagnostic accuracy such as sensitivity and specificity, predictive values, likelihood ratios, the area under the ROC curve, Youden's index and diagnostic odds ratio. Different measures of diagnostic accuracy relate to the different aspects of diagnostic procedure: while some measures are used to assess the discriminative property of the test, others are used to assess its predictive ability. Measures of diagnostic accuracy are not fixed indicators of a test performance, some are very sensitive to the disease prevalence, while others to the spectrum and definition of the disease. Furthermore, measures of diagnostic accuracy are extremely sensitive to the design of the study. Studies not meeting strict methodological standards usually over- or under-estimate the indicators of test performance as well as they limit the applicability of the results of the study. STARD initiative was a very important step toward the improvement the quality of reporting of studies of diagnostic accuracy. STARD statement should be included into the Instructions to authors by scientific journals and authors should be encouraged to use the checklist whenever reporting their studies on diagnostic accuracy. Such efforts could make a substantial difference in the quality of reporting of studies of diagnostic accuracy and serve to provide the best possible evidence to the best for the patient care. This brief review outlines some basic definitions and characteristics of the measures of diagnostic accuracy.},
	language = {eng},
	number = {4},
	journal = {EJIFCC},
	author = {Šimundić, Ana-Maria},
	month = jan,
	year = {2009},
	pmid = {27683318},
	pmcid = {PMC4975285},
	keywords = {AUC, diagnostic accuracy, DOR, likelihood ratio, NPV, PPV, predictive values, sensitivity, specificity},
	pages = {203--211},
}

@article{ho_generative_2016,
	title = {Generative {Adversarial} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/1606.03476},
	abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert’s cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
	language = {en},
	urldate = {2020-06-01},
	journal = {arXiv:1606.03476 [cs]},
	author = {Ho, Jonathan and Ermon, Stefano},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03476},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Ho and Ermon - 2016 - Generative Adversarial Imitation Learning.pdf:/Users/rosscutler/Zotero/storage/8N22GUX2/Ho and Ermon - 2016 - Generative Adversarial Imitation Learning.pdf:application/pdf},
}

@misc{pendergrass_inclusive_2019,
	title = {Inclusive scientific meetings: {Where} to start},
	url = {https://opensky.ucar.edu/islandora/object/manuscripts%3A983/datastream/PDF/view},
	author = {Pendergrass, Angie},
	month = mar,
	year = {2019},
}

@book{dobson_introduction_2002,
	address = {Boca Raton},
	edition = {2nd ed},
	series = {Chapman \& {Hall}/{CRC} texts in statistical science series},
	title = {An introduction to generalized linear models},
	isbn = {978-1-58488-165-0},
	publisher = {Chapman \& Hall/CRC},
	author = {Dobson, Annette J.},
	year = {2002},
	keywords = {Linear models (Statistics)},
}

@article{odermatt_incivility_2018,
	title = {Incivility in {Meetings}: {Predictors} and {Outcomes}},
	volume = {33},
	issn = {0889-3268, 1573-353X},
	shorttitle = {Incivility in {Meetings}},
	url = {http://link.springer.com/10.1007/s10869-017-9490-0},
	doi = {10.1007/s10869-017-9490-0},
	language = {en},
	number = {2},
	urldate = {2020-05-30},
	journal = {Journal of Business and Psychology},
	author = {Odermatt, Isabelle and König, Cornelius J. and Kleinmann, Martin and Bachmann, Maria and Röder, Heiko and Schmitz, Patricia},
	month = apr,
	year = {2018},
	pages = {263--282},
}

@inproceedings{yankelovich_meeting_2004,
	address = {Chicago, Illinois, USA},
	title = {Meeting central: making distributed meetings more effective},
	isbn = {978-1-58113-810-8},
	shorttitle = {Meeting central},
	url = {http://portal.acm.org/citation.cfm?doid=1031607.1031678},
	doi = {10.1145/1031607.1031678},
	abstract = {The Meeting Central prototype is a suite of collaboration tools designed to support distributed meetings. The tools' minimalist design provides only those features that have the most impact on distributed meeting effectiveness. The collaboration suite is built on top of a distributed, extensible, and scalable framework.},
	language = {en},
	urldate = {2020-05-30},
	booktitle = {Proceedings of the 2004 {ACM} conference on {Computer} supported cooperative work  - {CSCW} '04},
	publisher = {ACM Press},
	author = {Yankelovich, Nicole and Walker, William and Roberts, Patricia and Wessler, Mike and Kaplan, Jonathan and Provino, Joe},
	year = {2004},
	pages = {419},
	file = {Yankelovich et al. - 2004 - Meeting central making distributed meetings more .pdf:/Users/rosscutler/Zotero/storage/BATS3Y6D/Yankelovich et al. - 2004 - Meeting central making distributed meetings more .pdf:application/pdf},
}

@article{kauffeld_meetings_2012,
	title = {Meetings {Matter}: {Effects} of {Team} {Meetings} on {Team} and {Organizational} {Success}},
	volume = {43},
	issn = {1046-4964, 1552-8278},
	shorttitle = {Meetings {Matter}},
	url = {http://journals.sagepub.com/doi/10.1177/1046496411429599},
	doi = {10.1177/1046496411429599},
	abstract = {This study follows the idea that the key to understanding team meeting effectiveness lies in uncovering the microlevel interaction processes throughout the meeting. Ninety-two regular team meetings were videotaped. Interaction data were coded and evaluated with the act4teams coding scheme and INTERACT software. Team and organizational success variables were gathered via questionnaires and telephone interviews. The results support the central function of interaction processes as posited in the traditional input-process-output model. Teams that showed more functional interaction, such as problem-solving interaction and action planning, were significantly more satisfied with their meetings. Better meetings were associated with higher team productivity. Moreover, constructive meeting interaction processes were related to organizational success 2.5 years after the meeting. Dysfunctional communication, such as criticizing others or complaining, showed significant negative relationships with these outcomes. These negative effects were even more pronounced than the positive effects of functional team meeting interaction.The results suggest that team meeting processes shape both team and organizational outcomes.The critical meeting behaviors identified here provide hints for group researchers and practitioners alike who aim to improve meeting success.},
	language = {en},
	number = {2},
	urldate = {2020-05-30},
	journal = {Small Group Research},
	author = {Kauffeld, Simone and Lehmann-Willenbrock, Nale},
	month = apr,
	year = {2012},
	pages = {130--158},
	file = {Kauffeld and Lehmann-Willenbrock - 2012 - Meetings Matter Effects of Team Meetings on Team .pdf:/Users/rosscutler/Zotero/storage/JQWG32RZ/Kauffeld and Lehmann-Willenbrock - 2012 - Meetings Matter Effects of Team Meetings on Team .pdf:application/pdf},
}

@article{guo_effectiveness_2006,
	title = {Effectiveness of {Meeting} {Outcomes} in {Virtual} vs. {Face}-to-{Face} {Teams}: {A} {Comparison} {Study} in {China}},
	abstract = {As virtual teams become more and more important in organizations, understanding how to improve virtual team outcomes is vital to project success. This study examines how virtual teams interacting via videoconferencing systems may enhance their team outcomes in a Chinese cultural context. The results reveal that traditional face-to-face interaction outperformed videoconferencing teams when both teams had same team-building experience. However, a dialogue-based framework can be employed to help virtual teams to perform as effectively as traditional face-to-face teams that had no such shared basis of effective communication. Implications of these findings are discussed.},
	language = {en},
	author = {Guo, Zixiu and D'Ambra, John and Turner, Tim and Zhang, Huiying and Zhang, Tong},
	year = {2006},
	pages = {13},
	file = {Guo et al. - 2006 - Effectiveness of Meeting Outcomes in Virtual vs. F.pdf:/Users/rosscutler/Zotero/storage/F58W4ZTZ/Guo et al. - 2006 - Effectiveness of Meeting Outcomes in Virtual vs. F.pdf:application/pdf},
}

@article{rice_improving_2007,
	title = {Improving the {Effectiveness} of {Virtual} {Teams} by {Adapting} {Team} {Processes}},
	volume = {16},
	issn = {0925-9724, 1573-7551},
	url = {http://link.springer.com/10.1007/s10606-007-9070-3},
	doi = {10.1007/s10606-007-9070-3},
	language = {en},
	number = {6},
	urldate = {2020-05-30},
	journal = {Computer Supported Cooperative Work (CSCW)},
	author = {Rice, Daniel J. and Davidson, Barry D. and Dannenhoffer, John F. and Gay, Geri K.},
	month = oct,
	year = {2007},
	pages = {567--594},
}

@article{nixon_impact_1992,
	title = {Impact of meeting procedures on meeting effectiveness},
	volume = {6},
	issn = {0889-3268, 1573-353X},
	url = {http://link.springer.com/10.1007/BF01126771},
	doi = {10.1007/BF01126771},
	language = {en},
	number = {3},
	urldate = {2020-05-30},
	journal = {Journal of Business and Psychology},
	author = {Nixon, Carol T. and Littlepage, Glenn E.},
	year = {1992},
	pages = {361--369},
}

@inproceedings{kim_meeting_2008,
	address = {San Diego, CA, USA},
	title = {Meeting mediator: enhancing group collaborationusing sociometric feedback},
	isbn = {978-1-60558-007-4},
	shorttitle = {Meeting mediator},
	url = {http://portal.acm.org/citation.cfm?doid=1460563.1460636},
	doi = {10.1145/1460563.1460636},
	abstract = {We present the Meeting Mediator (MM), a real-time portable system that detects social interactions and provides persuasive feedback to enhance group collaboration. Social interactions is captured using Sociometric badges [17] and are visualized on mobile phones to promote behavioral change. Particularly in distributed collaborations, MM attempts to bridge the gap among the distributed groups by detecting and communicating social signals. In a study on brainstorming and problem solving meetings, MM had a signiﬁcant effect on overlapping speaking time and interactivity level without distracting the subjects. The Sociometric badges were also able to detect dominant players in the group and measure their inﬂuence on other participants. Most interestingly, in groups with one or more dominant people, MM effectively reduced the dynamical difference between co-located and distributed collaboration as well as the behavioral difference between dominant and non-dominant people. Our system encourages change in group dynamics that may lead to higher performance and satisfaction. We envision that MM will be deployed in real-world organizations to improve interactions across various group collaboration contexts.},
	language = {en},
	urldate = {2020-05-30},
	booktitle = {Proceedings of the {ACM} 2008 conference on {Computer} supported cooperative work - {CSCW} '08},
	publisher = {ACM Press},
	author = {Kim, Taemie and Chang, Agnes and Holland, Lindsey and Pentland, Alex Sandy},
	year = {2008},
	pages = {457},
	file = {Kim et al. - 2008 - Meeting mediator enhancing group collaborationusi.pdf:/Users/rosscutler/Zotero/storage/3ZMUHL7F/Kim et al. - 2008 - Meeting mediator enhancing group collaborationusi.pdf:application/pdf},
}

@article{greven_influence_nodate,
	title = {The {Influence} of {Non}-{Verbal} {Behaviour} on {Meeting} {Effectiveness} and {Pro}-{Active} {Behaviour}: {A} {Video} {Observational} {Study}},
	abstract = {Based on the nonverbal leadership literature, it has been hypothesized that hand gestures and body gestures have an influence on both perceived meeting effectiveness and pro-active behaviour of their followers. The research is focused on video-observations of team meetings, consisting of fine-grained codings of nonverbal behaviour displayed during the meetings, as well as several surveys that have been filled-out by team-members within the teams that have been recorded. The data consisted of 20 leaders and 192 followers which are employed in a large public organization. As a result, one correlation has been found, this correlation implies that upward palms gestures have a negative influence on the level of proactivity of the followers. In the discussion section the outcomes of the analysis were discussed and suggestions will be given for future research.},
	language = {en},
	author = {Greven, Tim},
	pages = {14},
	file = {Greven - The Influence of Non-Verbal Behaviour on Meeting E.pdf:/Users/rosscutler/Zotero/storage/C337BF9Y/Greven - The Influence of Non-Verbal Behaviour on Meeting E.pdf:application/pdf},
}

@article{tang_why_1992,
	title = {Why do users like video?: {Studies} of multimedia-supported collaboration},
	volume = {1},
	issn = {0925-9724, 1573-7551},
	shorttitle = {Why do users like video?},
	url = {http://link.springer.com/10.1007/BF00752437},
	doi = {10.1007/BF00752437},
	language = {en},
	number = {3},
	urldate = {2020-05-29},
	journal = {Computer Supported Cooperative Work (CSCW)},
	author = {Tang, John C. and Isaacs, Ellen},
	month = sep,
	year = {1992},
	pages = {163--196},
	file = {Tang and Isaacs - 1992 - Why do users like video Studies of multimedia-su.pdf:/Users/rosscutler/Zotero/storage/CFW4E4ZU/Tang and Isaacs - 1992 - Why do users like video Studies of multimedia-su.pdf:application/pdf},
}

@article{triana_does_2012,
	title = {Does the {Order} of {Face}-to-{Face} and {Computer}-{Mediated} {Communication} {Matter} in {Diverse} {Project} {Teams}? {An} {Investigation} of {Communication} {Order} {Effects} on {Minority} {Inclusion} and {Participation}},
	volume = {27},
	issn = {0889-3268, 1573-353X},
	shorttitle = {Does the {Order} of {Face}-to-{Face} and {Computer}-{Mediated} {Communication} {Matter} in {Diverse} {Project} {Teams}?},
	url = {http://link.springer.com/10.1007/s10869-011-9232-7},
	doi = {10.1007/s10869-011-9232-7},
	language = {en},
	number = {1},
	urldate = {2020-05-29},
	journal = {Journal of Business and Psychology},
	author = {Triana, María del Carmen and Kirkman, Bradley L. and Wagstaff, María Fernanda},
	month = mar,
	year = {2012},
	pages = {57--70},
}

@misc{noauthor_does_nodate,
	title = {Does the {Order} of {Face}-to-{Face} and {Computer}-{Mediated} {Communication} {Matter} in {Diverse} {Project} {Teams}? {An} {Investigation} of {Communication} {Order} {Effects} on {Minority} {Inclusion} and {Participation} {\textbar} {Paper} {\textbar} {Microsoft} {Academic}},
	url = {https://academic.microsoft.com/paper/1978916137/reference/search?q=Does%20the%20Order%20of%20Face-to-Face%20and%20Computer-Mediated%20Communication%20Matter%20in%20Diverse%20Project%20Teams%3F%20An%20Investigation%20of%20Communication%20Order%20Effects%20on%20Minority%20Inclusion%20and%20Participation&qe=%2540%2540%2540REFERENCES%253D1978916137&f=&orderBy=0},
	urldate = {2020-05-29},
	file = {Does the Order of Face-to-Face and Computer-Mediated Communication Matter in Diverse Project Teams? An Investigation of Communication Order Effects on Minority Inclusion and Participation | Paper | Microsoft Academic:/Users/rosscutler/Zotero/storage/VH4CESJW/search.html:text/html},
}

@misc{noauthor_meeting_nodate,
	title = {Meeting effectiveness and task performance: meeting size matters {\textbar} {Paper} {\textbar} {Microsoft} {Academic}},
	url = {https://academic.microsoft.com/paper/3010124859/reference/search?q=Meeting%20effectiveness%20and%20task%20performance%3A%20meeting%20size%20matters&qe=Or(Id%253D2106096361%252CId%253D3013892365%252CId%253D2162437324%252CId%253D2153610778%252CId%253D1989763926%252CId%253D2137819727%252CId%253D1982135085%252CId%253D2132199097%252CId%253D2111859378%252CId%253D2138597880%252CId%253D2090711836%252CId%253D2036763306%252CId%253D2132472780%252CId%253D2060725634%252CId%253D1984908356%252CId%253D1985050249%252CId%253D2254231552%252CId%253D2064228468%252CId%253D2096484863%252CId%253D1974607066%252CId%253D2808143924%252CId%253D2100068618%252CId%253D1508608232%252CId%253D2147129372%252CId%253D2476821336%252CId%253D2015303057%252CId%253D2032053084%252CId%253D2006436443%252CId%253D1922732933%252CId%253D1696264554%252CId%253D1978157683%252CId%253D2034660663%252CId%253D2032858903%252CId%253D2762965668%252CId%253D2328468864%252CId%253D2331447120%252CId%253D2077882850%252CId%253D2147041405%252CId%253D2797538882)&f=&orderBy=0&showAllAuthors=1},
	urldate = {2020-05-29},
	file = {Meeting effectiveness and task performance\: meeting size matters | Paper | Microsoft Academic:/Users/rosscutler/Zotero/storage/3YEKYH2P/search.html:text/html},
}

@article{standaert_empirical_2016,
	title = {An empirical study of the effectiveness of telepresence as a business meeting mode},
	volume = {17},
	issn = {1385-951X, 1573-7667},
	url = {http://link.springer.com/10.1007/s10799-015-0221-9},
	doi = {10.1007/s10799-015-0221-9},
	language = {en},
	number = {4},
	urldate = {2020-05-29},
	journal = {Information Technology and Management},
	author = {Standaert, Willem and Muylle, Steve and Basu, Amit},
	month = dec,
	year = {2016},
	pages = {323--339},
	file = {H020244752.pdf:/Users/rosscutler/Zotero/storage/IJAKEEGI/H020244752.pdf:application/pdf;Standaert et al. - 2016 - An empirical study of the effectiveness of telepre.pdf:/Users/rosscutler/Zotero/storage/WMMW9K5B/Standaert et al. - 2016 - An empirical study of the effectiveness of telepre.pdf:application/pdf},
}

@article{allen_meeting_2020,
	title = {Meeting effectiveness and task performance: meeting size matters},
	volume = {ahead-of-print},
	issn = {0262-1711},
	shorttitle = {Meeting effectiveness and task performance},
	url = {https://www.emerald.com/insight/content/doi/10.1108/JMD-12-2019-0510/full/html},
	doi = {10.1108/JMD-12-2019-0510},
	abstract = {Purpose
              The purpose of this study was to investigate how a key meeting design characteristic, meeting size, affects the relationship between meeting effectiveness and task performance through employee engagement.
            
            
              Design/methodology/approach
              A three-wave time-lagged survey design was used to gather data concerning meeting experiences from employees for statistical model testing.
            
            
              Findings
              Using a moderated mediated path analysis, we found that effective meetings only translated into end-of-the-day task performance through engagement when the meeting size was small.
            
            
              Research limitations/implications
              Although much research supports the current findings related to group size and meetings, meeting science has not investigated meeting design characteristics as levers to be pulled to enhance or detract from both meeting outcomes and organizationally desired outcomes. The findings, though are limited, due to potential common method bias, which was limited using methodological and statistical processes.
            
            
              Practical implications
              Managers and meeting attendees should consider how to maintain relatively small meeting size when possible so as to maximize both engagement and performance.
            
            
              Originality/value
              The current study is one of the few to look at meeting size directly as a moderator and helps demonstrate, once again, the importance of effectively designing meetings for success.},
	language = {en},
	number = {ahead-of-print},
	urldate = {2020-05-29},
	journal = {Journal of Management Development},
	author = {Allen, Joseph A. and Tong, Jiajin and Landowski, Nicole},
	month = mar,
	year = {2020},
}

@article{noauthor_meeting_nodate-1,
	title = {Meeting effectiveness and task performance: meeting size matters},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2020-05-26},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:/Users/rosscutler/Zotero/storage/Y5FTRI35/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:/Users/rosscutler/Zotero/storage/GPEDBN7E/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@article{manocha_differentiable_2020,
	title = {A {Differentiable} {Perceptual} {Audio} {Metric} {Learned} from {Just} {Noticeable} {Differences}},
	url = {http://arxiv.org/abs/2001.04460},
	abstract = {Many audio processing tasks require perceptual assessment. The ``gold standard`` of obtaining human judgments is time-consuming, expensive, and cannot be used as an optimization criterion. On the other hand, automated metrics are efficient to compute but often correlate poorly with human judgment, particularly for audio differences at the threshold of human detection. In this work, we construct a metric by fitting a deep neural network to a new large dataset of crowdsourced human judgments. Subjects are prompted to answer a straightforward, objective question: are two recordings identical or not? These pairs are algorithmically generated under a variety of perturbations, including noise, reverb, and compression artifacts; the perturbation space is probed with the goal of efficiently identifying the just-noticeable difference (JND) level of the subject. We show that the resulting learned metric is well-calibrated with human judgments, outperforming baseline methods. Since it is a deep network, the metric is differentiable, making it suitable as a loss function for other tasks. Thus, simply replacing an existing loss (e.g., deep feature loss) with our metric yields significant improvement in a denoising network, as measured by subjective pairwise comparison.},
	urldate = {2020-05-26},
	journal = {arXiv:2001.04460 [cs, eess]},
	author = {Manocha, Pranay and Finkelstein, Adam and Zhang, Richard and Bryan, Nicholas J. and Mysore, Gautham J. and Jin, Zeyu},
	month = may,
	year = {2020},
	note = {arXiv: 2001.04460},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/VSSH9MMQ/Manocha et al. - 2020 - A Differentiable Perceptual Audio Metric Learned f.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/H4ZNWD93/2001.html:text/html},
}

@article{westhausen_dual-signal_2020-1,
	title = {Dual-{Signal} {Transformation} {LSTM} {Network} for {Real}-{Time} {Noise} {Suppression}},
	url = {http://arxiv.org/abs/2005.07551},
	abstract = {This paper introduces a dual-signal transformation LSTM network (DTLN) for real-time speech enhancement as part of the Deep Noise Suppression Challenge (DNS-Challenge). This approach combines a short-time Fourier transform (STFT) and a learned analysis and synthesis basis in a stacked-network approach with less than one million parameters. The model was trained on 500 h of noisy speech provided by the challenge organizers. The network is capable of real-time processing (one frame in, one frame out) and reaches competitive results. Combining these two types of signal transformations enables the DTLN to robustly extract information from magnitude spectra and incorporate phase information from the learned feature basis. The method shows state-of-the-art performance and outperforms the DNS-Challenge baseline by 0.24 points absolute in terms of the mean opinion score (MOS).},
	urldate = {2020-05-20},
	journal = {arXiv:2005.07551 [cs, eess]},
	author = {Westhausen, Nils L. and Meyer, Bernd T.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.07551},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/28MHELC2/Westhausen and Meyer - 2020 - Dual-Signal Transformation LSTM Network for Real-T.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/NPENZKW5/2005.html:text/html},
}

@article{kinoshita_summary_2016,
	title = {A summary of the {REVERB} challenge: state-of-the-art and remaining challenges in reverberant speech processing research},
	volume = {2016},
	issn = {1687-6180},
	shorttitle = {A summary of the {REVERB} challenge},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-016-0306-6},
	doi = {10.1186/s13634-016-0306-6},
	abstract = {In recent years, substantial progress has been made in the field of reverberant speech signal processing, including both single- and multichannel dereverberation techniques and automatic speech recognition (ASR) techniques that are robust to reverberation. In this paper, we describe the REVERB challenge, which is an evaluation campaign that was designed to evaluate such speech enhancement (SE) and ASR techniques to reveal the state-of-the-art techniques and obtain new insights regarding potential future research directions. Even though most existing benchmark tasks and challenges for distant speech processing focus on the noise robustness issue and sometimes only on a single-channel scenario, a particular novelty of the REVERB challenge is that it is carefully designed to test robustness against reverberation, based on both real, single-channel, and multichannel recordings. This challenge attracted 27 papers, which represent 25 systems specifically designed for SE purposes and 49 systems specifically designed for ASR purposes. This paper describes the problems dealt within the challenge, provides an overview of the submitted systems, and scrutinizes them to clarify what current processing strategies appear effective in reverberant speech processing.},
	language = {en},
	number = {1},
	urldate = {2021-10-31},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Kinoshita, Keisuke and Delcroix, Marc and Gannot, Sharon and P. Habets, Emanuël A. and Haeb-Umbach, Reinhold and Kellermann, Walter and Leutnant, Volker and Maas, Roland and Nakatani, Tomohiro and Raj, Bhiksha and Sehr, Armin and Yoshioka, Takuya},
	month = dec,
	year = {2016},
	pages = {7},
	file = {Kinoshita et al. - 2016 - A summary of the REVERB challenge state-of-the-ar.pdf:/Users/rosscutler/Zotero/storage/JA6VI9Q4/Kinoshita et al. - 2016 - A summary of the REVERB challenge state-of-the-ar.pdf:application/pdf},
}

@misc{noauthor_search_nodate,
	title = {Search},
	url = {https://forums.zotero.org/search},
	abstract = {Zotero is a powerful, easy-to-use research tool that helps you gather, organize, and analyze sources and then share the results of your research.},
	language = {en},
	urldate = {2021-10-31},
	journal = {Zotero Forums},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/7DZXFQ3R/search.html:text/html},
}

@inproceedings{hadad_multichannel_2014,
	address = {Juan-les-Pins, France},
	title = {Multichannel audio database in various acoustic environments},
	isbn = {978-1-4799-6808-4},
	url = {http://ieeexplore.ieee.org/document/6954309/},
	doi = {10.1109/IWAENC.2014.6954309},
	abstract = {In this paper we describe a new multichannel room impulse responses database. The impulse responses are measured in a room with conﬁgurable reverberation level resulting in three different acoustic scenarios with reverberation times RT60 equals to 160 ms, 360 ms and 610 ms. The measurements were carried out in recording sessions of several source positions on a spatial grid (angle range of −90o to 90o in 15o steps with 1 m and 2 m distance from the microphone array). The signals in all sessions were captured by three microphone array conﬁgurations. The database is accompanied with software utilities to easily access and manipulate the data. Besides the description of the database we demonstrate its use in spatial source separation task.},
	language = {en},
	urldate = {2021-10-31},
	booktitle = {2014 14th {International} {Workshop} on {Acoustic} {Signal} {Enhancement} ({IWAENC})},
	publisher = {IEEE},
	author = {Hadad, Elior and Heese, Florian and Vary, Peter and Gannot, Sharon},
	month = sep,
	year = {2014},
	pages = {313--317},
	file = {Hadad et al. - 2014 - Multichannel audio database in various acoustic en.pdf:/Users/rosscutler/Zotero/storage/E6JZYYLL/Hadad et al. - 2014 - Multichannel audio database in various acoustic en.pdf:application/pdf},
}

@misc{garofolo_csr-i_1993,
	title = {{CSR}-{I} ({WSJ0}) {Complete} {LDC93S6A}. {Web} {Download}.},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John and Graff, David and Paul, Doug and Pallett, David},
	year = {1993},
}

@article{guo_didispeech_2021,
	title = {{DiDiSpeech}: {A} {Large} {Scale} {Mandarin} {Speech} {Corpus}},
	shorttitle = {{DiDiSpeech}},
	url = {http://arxiv.org/abs/2010.09275},
	abstract = {This paper introduces a new open-sourced Mandarin speech corpus, called DiDiSpeech. It consists of about 800 hours of speech data at 48kHz sampling rate from 6000 speakers and the corresponding texts. All speech data in the corpus is recorded in quiet environment and is suitable for various speech processing tasks, such as voice conversion, multi-speaker text-to-speech and automatic speech recognition. We conduct experiments with multiple speech tasks and evaluate the performance, showing that it is promising to use the corpus for both academic research and practical application. The corpus is available at https://outreach.didichuxing.com/research/opendata/.},
	urldate = {2021-10-31},
	journal = {arXiv:2010.09275 [eess]},
	author = {Guo, Tingwei and Wen, Cheng and Jiang, Dongwei and Luo, Ne and Zhang, Ruixiong and Zhao, Shuaijiang and Li, Wubo and Gong, Cheng and Zou, Wei and Han, Kun and Li, Xiangang},
	month = feb,
	year = {2021},
	note = {arXiv: 2010.09275},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/JSGNPQL8/Guo et al. - 2021 - DiDiSpeech A Large Scale Mandarin Speech Corpus.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/7XAGF3AE/2010.html:text/html},
}

@inproceedings{choi_real-time_2021,
	title = {Real-{Time} {Denoising} and {Dereverberation} wtih {Tiny} {Recurrent} {U}-{Net}},
	doi = {10.1109/ICASSP39728.2021.9414852},
	abstract = {Modern deep learning-based models have seen outstanding performance improvement with speech enhancement tasks. The number of parameters of state-of-the-art models, however, is often too large to be deployed on devices for real-world applications. To this end, we propose Tiny Recurrent U-Net (TRU-Net), a lightweight online inference model that matches the performance of current state-of- the-art models. The size of the quantized version of TRU-Net is 362 kilobytes, which is small enough to be deployed on edge devices. In addition, we combine the small-sized model with a new masking method called phase-aware ß-sigmoid mask, which enables simultaneous denoising and dereverberation. Results of both objective and subjective evaluations have shown that our model can achieve competitive performance with the current state-of-the-art models on benchmark datasets using fewer parameters by orders of magnitude.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Choi, Hyeong-Seok and Park, Sungjin and Lee, Jie Hwan and Heo, Hoon and Jeon, Dongsuk and Lee, Kyogu},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Speech enhancement, Neural networks, Performance evaluation, Noise reduction, dereverberation, Signal processing, Conferences, Benchmark testing, denoising, lightweight network, real-time speech enhancement},
	pages = {5789--5793},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/MX2C4FBL/9414852.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/QCIGTVHT/Choi et al. - 2021 - Real-Time Denoising and Dereverberation wtih Tiny .pdf:application/pdf},
}

@article{lim_183_2021,
	title = {18‐3: {Free} {Viewpoint} {Teleconferencing} {Using} {Cameras} {Behind} {Screen}},
	volume = {52},
	issn = {0097-966X, 2168-0159},
	shorttitle = {18‐3},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sdtp.14651},
	doi = {10.1002/sdtp.14651},
	abstract = {Natural interaction in videoconferences can be made by correcting gaze/perspective, scale, and position by using cameras placed behind a partially transparent front emitting OLED panel and frame interpolation of deep neural networks. The diffraction artifacts from through-screen imaging are removed by a deconvolution method.},
	language = {en},
	number = {1},
	urldate = {2021-11-03},
	journal = {SID Symposium Digest of Technical Papers},
	author = {Lim, Sehoon and Liang, Luming and Zhong, Yatao and Emerton, Neil and Large, Tim and Bathiche, Steven},
	month = may,
	year = {2021},
	pages = {218--221},
	file = {Lim et al. - 2021 - 18‐3 Free Viewpoint Teleconferencing Using Camera.pdf:/Users/rosscutler/Zotero/storage/JNPI7E32/Lim et al. - 2021 - 18‐3 Free Viewpoint Teleconferencing Using Camera.pdf:application/pdf},
}

@incollection{3rd_generation_partnership_project_adaptive_2004,
	title = {Adaptive {Multi}-{Rate} ({AMR}) speech codec; {Error} concealment of lost frames},
	shorttitle = {Error concealment of lost frames},
	url = {https://www.atis.org/wp-content/uploads/3gpp-documents/Rel99-14/ATIS.3GPP.26.091V600-2006.pdf},
	urldate = {2021-11-25},
	booktitle = {Adaptive {Multi}-{Rate} ({AMR}) speech codec},
	author = {3rd Generation Partnership Project},
	year = {2004},
	file = {ATIS.3GPP.26.091V600-2006.pdf:/Users/rosscutler/Zotero/storage/SE6DB4DV/ATIS.3GPP.26.091V600-2006.pdf:application/pdf},
}

@article{mohamed_concealnet_2020,
	title = {{ConcealNet}: {An} {End}-to-end {Neural} {Network} for {Packet} {Loss} {Concealment} in {Deep} {Speech} {Emotion} {Recognition}},
	shorttitle = {{ConcealNet}},
	url = {http://arxiv.org/abs/2005.07777},
	abstract = {Packet loss is a common problem in data transmission, including speech data transmission. This may affect a wide range of applications that stream audio data, like streaming applications or speech emotion recognition (SER). Packet Loss Concealment (PLC) is any technique of facing packet loss. Simple PLC baselines are 0-substitution or linear interpolation. In this paper, we present a concealment wrapper, which can be used with stacked recurrent neural cells. The concealment cell can provide a recurrent neural network (ConcealNet), that performs real-time step-wise end-to-end PLC at inference time. Additionally, extending this with an end-to-end emotion prediction neural network provides a network that performs SER from audio with lost frames, end-to-end. The proposed model is compared against the fore-mentioned baselines. Additionally, a bidirectional variant with better performance is utilised. For evaluation, we chose the public RECOLA dataset given its long audio tracks with continuous emotion labels. ConcealNet is evaluated on the reconstruction of the audio and the quality of corresponding emotions predicted after that. The proposed ConcealNet model has shown considerable improvement, for both audio reconstruction and the corresponding emotion prediction, in environments that do not have losses with long duration, even when the losses occur frequently.},
	language = {en},
	urldate = {2021-11-25},
	journal = {arXiv:2005.07777 [cs, eess]},
	author = {Mohamed, Mostafa M. and Schuller, Björn W.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.07777},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Mohamed and Schuller - 2020 - ConcealNet An End-to-end Neural Network for Packe.pdf:/Users/rosscutler/Zotero/storage/SEQ2CZ3R/Mohamed and Schuller - 2020 - ConcealNet An End-to-end Neural Network for Packe.pdf:application/pdf},
}

@inproceedings{shi_speech_2019,
	title = {Speech {Loss} {Compensation} by {Generative} {Adversarial} {Networks}},
	doi = {10.1109/APSIPAASC47483.2019.9023132},
	abstract = {Speech loss, including frequency loss and packet loss, can lead to significant speech distortion in many Internet-based speech communication services. In this study, a generative adversarial networks (GANs) structure, which takes deep convolutional neural networks (CNN) as the generator and discriminator components, is adopted as a general framework for speech loss compensation. Network settings are modified for real-time communications. A set of experiments are conducted to evaluate the performance of the GANs-based framework for both bandwidth expansion (BWE) and packet loss concealment (PLC) at several simulated loss conditions. Experimental results demonstrate that the proposed system achieves better performance, with respective to 4 objective metrics, in both BWE and PLC compared to the baseline systems.},
	booktitle = {2019 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Shi, Yupeng and Zheng, Nengheng and Kang, Yuyong and Rong, Weicong},
	month = nov,
	year = {2019},
	note = {ISSN: 2640-0103},
	keywords = {Speech enhancement, Training, Packet loss, generative adversarial networks, Bandwidth, bandwidth extension, Gallium nitride, Generative adversarial networks, packet loss concealment, Speech loss compensation},
	pages = {347--351},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/6HIEKTAT/9023132.html:text/html;IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/R52S2Z3B/9023132.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/IGV66HCD/Shi et al. - 2019 - Speech Loss Compensation by Generative Adversarial.pdf:application/pdf;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/PTWLUZGT/Shi et al. - 2019 - Speech Loss Compensation by Generative Adversarial.pdf:application/pdf},
}

@inproceedings{sun_impact_2001,
	title = {Impact of {Packet} {Loss} {Location} on {Perceived} {Speech} {Quality}},
	abstract = {Abstract – In VoIP applications, packet loss can have a major impact on perceived speech quality. The impact is affected by factors such as packet loss size, loss pattern and loss locations. In this paper, we report an investigation into the impact of loss location on perceived speech quality and the relationships between convergence time and loss location for three different codecs (G.729, G.723.1 and AMR) using perceptual-based objective measurement methods (PSQM+, MNB and EMBSD). Our results show that loss location has a severe effect on perceived speech quality. The loss at unvoiced speech segments has little impact on perceived speech quality for all codecs. However, the loss at the beginning of voiced segments has the most severe impact on perceived speech quality. The convergence time depends on the speech content (voiced/unvoiced). For unvoiced segments, the convergence time is stable whereas for voiced segments it varies but has an upper bound at the end of the segment. Our method allows a more accurate measurement of the exact effect of packet loss on perceived speech quality. This could help in the development of a perceptually relevant packet loss metric, which could be valuable in non-intrusive VoIP measurements.},
	booktitle = {In 2nd {IP}-{Telephony} {Workshop}},
	author = {Sun, L. F. and Wade, G. and Lines, B. M. and Ifeachor, E. C.},
	year = {2001},
	pages = {114--122},
	file = {Citeseer - Full Text PDF:/Users/rosscutler/Zotero/storage/W9JXYAK9/Sun et al. - 2001 - Impact of Packet Loss Location on Perceived Speech.pdf:application/pdf;Citeseer - Snapshot:/Users/rosscutler/Zotero/storage/TF5BUUCS/download.html:text/html},
}

@inproceedings{kegler_deep_2020,
	title = {Deep {Speech} {Inpainting} of {Time}-{Frequency} {Masks}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/kegler20_interspeech.html},
	doi = {10.21437/Interspeech.2020-1532},
	language = {en},
	urldate = {2021-11-25},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Kegler, Mikolaj and Beckmann, Pierre and Cernak, Milos},
	month = oct,
	year = {2020},
	pages = {3276--3280},
	file = {Kegler et al. - 2020 - Deep Speech Inpainting of Time-Frequency Masks.pdf:/Users/rosscutler/Zotero/storage/97EHBJ25/Kegler et al. - 2020 - Deep Speech Inpainting of Time-Frequency Masks.pdf:application/pdf},
}

@article{kalchbrenner_efficient_2018,
	title = {Efficient {Neural} {Audio} {Synthesis}},
	url = {http://arxiv.org/abs/1802.08435},
	abstract = {Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating high-quality samples. Efficient sampling for this class of models has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24kHz 16-bit audio 4x faster than real time on a GPU. Second, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds for sparsity levels beyond 96\%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile CPU in real time. Finally, we propose a new generation scheme based on subscaling that folds a long sequence into a batch of shorter sequences and allows one to generate multiple samples at once. The Subscale WaveRNN produces 16 samples per step without loss of quality and offers an orthogonal method for increasing sampling efficiency.},
	urldate = {2021-11-25},
	journal = {arXiv:1802.08435 [cs, eess]},
	author = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and Oord, Aaron van den and Dieleman, Sander and Kavukcuoglu, Koray},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.08435
version: 1},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/NFGV3P8W/Kalchbrenner et al. - 2018 - Efficient Neural Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/8FSD2TLA/1802.html:text/html},
}

@inproceedings{hellwig_speech_1989,
	title = {Speech codec for the {European} mobile radio system},
	doi = {10.1109/GLOCOM.1989.64121},
	abstract = {The speech coding scheme which will be used as the standard for the European mobile radio system has been selected by the CEPT Groupe Special-Mobile (GSM) as a result of formal subjective listening tests. It is based on the regular-pulse excitation linear predictive coding technique (RPE-LPC) combined with long-term prediction (LTP). The solution is called the RPE-LTP codec. The codec algorithm and the error protection scheme are presented. The net bit rate is 13.0 kb/s, and the gross bit rate, including error protection, is 22.8 kb/s. The experimental implementation based on VLSI signal processors is described. The speech quality obtained with the technique considered is far superior to that obtainable with present-day analog mobile radio systems. A duplex speech codec including error protection can be implemented with two VLSI sign processors with external data memories of about 1 K*16 b.{\textless}{\textgreater}},
	booktitle = {1989 {IEEE} {Global} {Telecommunications} {Conference} and {Exhibition} '{Communications} {Technology} for the 1990s and {Beyond}'},
	author = {Hellwig, K. and Vary, P. and Massaloux, D. and Petit, J.P. and Galand, C. and Rosso, M.},
	month = nov,
	year = {1989},
	keywords = {Bit rate, Signal processing algorithms, Speech coding, GSM, Land mobile radio, Linear predictive coding, Protection, Speech codecs, System testing, Very large scale integration},
	pages = {1065--1069 vol.2},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/8NG2TTTU/64121.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/DRHSA75V/Hellwig et al. - 1989 - Speech codec for the European mobile radio system.pdf:application/pdf},
}

@article{rodbro_hidden_2006,
	title = {Hidden {Markov} model-based packet loss concealment for voice over {IP}},
	volume = {14},
	issn = {1558-7924},
	doi = {10.1109/TSA.2005.858561},
	abstract = {As voice over IP proliferates, packet loss concealment (PLC) at the receiver has emerged as an important factor in determining voice quality of service. Through the use of heuristic variations of signal and parameter repetition and overlap-add interpolation to handle packet loss, conventional PLC systems largely ignore the dynamics of the statistical evolution of the speech signal, possibly leading to perceptually annoying artifacts. To address this problem, we propose the use of hidden Markov models for PLC. With a hidden Markov model (HMM) tracking the evolution of speech signal parameters, we demonstrate how PLC is performed within a statistical signal processing framework. Moreover, we show how the HMM is used to index a specially designed PLC module for the particular signal context, leading to signal-contingent PLC. Simulation examples, objective tests, and subjective listening tests are provided showing the ability of an HMM-based PLC built with a sinusoidal analysis/synthesis model to provide better loss concealment than a conventional PLC based on the same sinusoidal model for all types of speech signals, including onsets and signal transitions},
	number = {5},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Rodbro, C.A. and Murthi, M.N. and Andersen, S.V. and Jensen, S.H.},
	month = sep,
	year = {2006},
	note = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
	keywords = {Speech processing, Hidden Markov models, Internet telephony, Signal processing, Speech synthesis, Quality of service, Testing, packet loss concealment, Hidden Markov models (HMMs), internet telephony, Interpolation, packet loss, packet switching, Programmable control, Speech analysis, speech coding, speech communication, voice over IP},
	pages = {1609--1623},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/9VY23QSZ/1677981.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/EVIYPWCF/Rodbro et al. - 2006 - Hidden Markov model-based packet loss concealment .pdf:application/pdf},
}

@inproceedings{morrone_audio-visual_2021,
	title = {Audio-{Visual} {Speech} {Inpainting} with {Deep} {Learning}},
	doi = {10.1109/ICASSP39728.2021.9413488},
	abstract = {In this paper, we present a deep-learning-based framework for audio-visual speech inpainting, i.e., the task of restoring the missing parts of an acoustic speech signal from reliable audio context and uncorrupted visual information. Recent work focuses solely on audio-only methods and generally aims at inpainting music signals, which show highly different structure than speech. Instead, we inpaint speech signals with gaps ranging from 100 ms to 1600 ms to investigate the contribution that vision can provide for gaps of different duration. We also experiment with a multi-task learning approach where a phone recognition task is learned together with speech inpainting. Results show that the performance of audio-only speech inpainting approaches degrades rapidly when gaps get large, while the proposed audio-visual approach is able to plausibly restore missing information. In addition, we show that multi-task learning is effective, although the largest contribution to performance comes from vision.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Morrone, Giovanni and Michelsanti, Daniel and Tan, Zheng-Hua and Jensen, Jesper},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {deep learning, multi-task learning, Deep learning, Signal processing, Visualization, Acoustics, Speech recognition, audio-visual, Distance measurement, face-landmarks, Multiple signal classification, speech inpainting},
	pages = {6653--6657},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/JL66R5IS/9413488.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/CPKJ72U9/Morrone et al. - 2021 - Audio-Visual Speech Inpainting with Deep Learning.pdf:application/pdf},
}

@inproceedings{mokry_introducing_2019,
	title = {Introducing {SPAIN} ({SParse} {Audio} {INpainter})},
	doi = {10.23919/EUSIPCO.2019.8902560},
	abstract = {A novel sparsity-based algorithm for audio inpainting is proposed. It is an adaptation of the SPADE algorithm by Kitić et al., originally developed for audio declipping, to the task of audio inpainting. The new SPAIN (SParse Audio INpainter) comes in synthesis and analysis variants. Experiments show that both A-SPAIN and S-SPAIN outperform other sparsity-based inpainting algorithms. Moreover, A-SPAIN performs on a par with the state-of-the-art method based on linear prediction in terms of the SNR, and, for larger gaps, SPAIN is even slightly better in terms of the PEMO-Q psychoacoustic criterion.},
	booktitle = {2019 27th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Mokrý, Ondřej and Záviška, Pavel and Rajmic, Pavel and Veselý, Vítězslav},
	month = sep,
	year = {2019},
	note = {ISSN: 2076-1465},
	keywords = {Task analysis, Reliability, Time-domain analysis, Signal processing algorithms, Analysis, Approximation algorithms, Cosparse, Inpainting, Signal to noise ratio, Sparse, Synthesis},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/5DESQ8TC/8902560.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/6CJV3JTT/Mokrý et al. - 2019 - Introducing SPAIN (SParse Audio INpainter).pdf:application/pdf},
}

@inproceedings{chantas_sparse_2018,
	title = {Sparse audio inpainting with variational {Bayesian} inference},
	doi = {10.1109/ICCE.2018.8326160},
	abstract = {Audio inpainting is defined as the process of restoring the damaged segments of an audio signal, based on the known signal values and prior information about the signal. In this paper, we formulate the problem in a Bayesian framework and adopt an efficient sparsity inducing Students-t prior distribution, assumed for the discrete cosine transform coefficients, applied on the signal. We also propose a variational Bayesian algorithm for inpainting, that performs approximate, though tractable, inference. Lastly, experiments demonstrate the efficiency of the proposed methodology when used for declipping audio signals, by comparing with the state-of-the-art.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Consumer} {Electronics} ({ICCE})},
	author = {Chantas, Giannis and Nikolopoulos, Spiros and Kompatsiaris, Ioannis},
	month = jan,
	year = {2018},
	note = {ISSN: 2158-4001},
	keywords = {Mathematical model, Distortion, Signal processing algorithms, Audio inpainting, Bayes methods, Dictionaries, Discrete cosine transforms, Inference algorithms, Sparsity, Student's-t, Variational Inference},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/UML3G5FU/8326160.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/7CSSVDLA/Chantas et al. - 2018 - Sparse audio inpainting with variational Bayesian .pdf:application/pdf},
}

@inproceedings{rajmic_acceleration_2015,
	title = {Acceleration of audio inpainting by support restriction},
	doi = {10.1109/ICUMT.2015.7382451},
	abstract = {We present a simple algorithm which accelerates the sparsity-based audio inpainting. The algorithm optimally restricts the signal support around the missing data region. This way, increased computational efficiency is achieved by avoiding inclusion of unnecessary values in the optimization process. For testing purposes, we use the discrete Gabor transform as the sparsity promoting representation, but the method can be easily translated to other systems.},
	booktitle = {2015 7th {International} {Congress} on {Ultra} {Modern} {Telecommunications} and {Control} {Systems} and {Workshops} ({ICUMT})},
	author = {Rajmic, Pavel and Bartlová, Hana and Průša, Zdeněk and Holighaus, Nicki},
	month = oct,
	year = {2015},
	note = {ISSN: 2157-0221},
	keywords = {Time-frequency analysis, Dictionaries, Acceleration, Image restoration, Indexes, Optimization, Transforms},
	pages = {325--329},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/CYIBXYXE/7382451.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/BRHIT7IH/Rajmic et al. - 2015 - Acceleration of audio inpainting by support restri.pdf:application/pdf},
}

@article{bahat_self-content-based_2015,
	title = {Self-content-based audio inpainting},
	volume = {111},
	issn = {0165-1684},
	url = {https://www.sciencedirect.com/science/article/pii/S0165168414005623},
	doi = {10.1016/j.sigpro.2014.11.023},
	abstract = {The popularity of voice over Internet protocol (VoIP) systems is continuously growing. Such systems depend on unreliable Internet communication, in which chunks of data often get lost during transmission. Various solutions to this problem were proposed, most of which are better suited to small rates of lost data. This work addresses this problem by filling in missing data using examples taken from prior recorded audio of the same user. Our approach also harnesses statistical priors and data inpainting smoothing techniques. The effectiveness of the proposed solution is demonstrated experimentally, even in large data-gaps, which cannot be handled by the standard packet loss concealment techniques.},
	language = {en},
	urldate = {2021-11-25},
	journal = {Signal Processing},
	author = {Bahat, Yuval and Y. Schechner, Yoav and Elad, Michael},
	month = jun,
	year = {2015},
	keywords = {Example-based inpainting, Packet loss concealment},
	pages = {61--72},
	file = {Bahat et al. - 2015 - Self-content-based audio inpainting.pdf:/Users/rosscutler/Zotero/storage/6ZM6SIUV/Bahat et al. - 2015 - Self-content-based audio inpainting.pdf:application/pdf;ScienceDirect Snapshot:/Users/rosscutler/Zotero/storage/Z4ZXBRU2/S0165168414005623.html:text/html},
}

@inproceedings{zhou_vision-infused_2019,
	address = {Seoul, Korea (South)},
	title = {Vision-{Infused} {Deep} {Audio} {Inpainting}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9008233/},
	doi = {10.1109/ICCV.2019.00037},
	abstract = {Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, i.e. synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset [51]. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI). Code, models, dataset and video results are available at https://github.com/Hangz-nju-cuhk/ Vision-Infused-Audio-Inpainter-VIAI.},
	language = {en},
	urldate = {2021-11-25},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhou, Hang and Liu, Ziwei and Xu, Xudong and Luo, Ping and Wang, Xiaogang},
	month = oct,
	year = {2019},
	pages = {283--292},
	file = {Zhou et al. - 2019 - Vision-Infused Deep Audio Inpainting.pdf:/Users/rosscutler/Zotero/storage/8KEDT7MF/Zhou et al. - 2019 - Vision-Infused Deep Audio Inpainting.pdf:application/pdf;Zhou et al. - 2019 - Vision-Infused Deep Audio Inpainting.pdf:/Users/rosscutler/Zotero/storage/57LUYDQP/Zhou et al. - 2019 - Vision-Infused Deep Audio Inpainting.pdf:application/pdf},
}

@article{wang_temporal-spectral_2021,
	title = {A temporal-spectral generative adversarial network based end-to-end packet loss concealment for wideband speech transmission},
	volume = {150},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/10.1121/10.0006528},
	doi = {10.1121/10.0006528},
	abstract = {Packet loss concealment (PLC) aims to mitigate speech impairments caused by packet losses so as to improve speech perceptual quality. This paper proposes an end-to-end PLC algorithm with a time-frequency hybrid generative adversarial network, which incorporates a dilated residual convolution and the integration of a time-domain discriminator and frequency-domain discriminator into a convolutional encoder-decoder architecture. The dilated residual convolution is employed to aggregate the short-term and long-term context information of lost speech frames through two network receptive fields with different dilation rates, and the integrated time-frequency discriminators are proposed to learn multi-resolution time-frequency features from correctly received speech frames with both time-domain waveform and frequency-domain complex spectrums. Both causal and noncausal strategies are proposed for the packet-loss problem, which can effectively reduce the transitional distortion caused by lost speech frames with a significantly reduced number of training parameters and computational complexity. The experimental results show that the proposed method can achieve better performance in terms of three objective measurements, including the signal-to-noise ratio, perceptual evaluation of speech quality, and short-time objective intelligibility. The results of the subjective listening test further confirm a better performance in the speech perceptual quality.},
	number = {4},
	urldate = {2021-11-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Wang, Jie and Guan, Yuansheng and Zheng, Chengshi and Peng, Renhua and Li, Xiaodong},
	month = oct,
	year = {2021},
	note = {Publisher: Acoustical Society of America},
	pages = {2577--2588},
	file = {Full Text PDF:/Users/rosscutler/Zotero/storage/7TQFWVNT/Wang et al. - 2021 - A temporal-spectral generative adversarial network.pdf:application/pdf},
}

@article{mokry_audio_2020,
	title = {Audio {Inpainting}: {Revisited} and {Reweighted}},
	volume = {28},
	issn = {2329-9290, 2329-9304},
	shorttitle = {Audio {Inpainting}},
	url = {http://arxiv.org/abs/2001.02480},
	doi = {10.1109/TASLP.2020.3030486},
	abstract = {We deal with the problem of sparsity-based audio inpainting, i.e. filling in the missing segments of audio. A consequence of the approaches based on mathematical optimization is the insufficient amplitude of the signal in the filled gaps. Remaining in the framework based on sparsity and convex optimization, we propose improvements to audio inpainting, aiming at compensating for such an energy loss. The new ideas are based on different types of weighting, both in the coefficient and the time domains. We show that our propositions improve the inpainting performance in terms of both the SNR and ODG.},
	urldate = {2021-11-25},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Mokrý, Ondřej and Rajmic, Pavel},
	year = {2020},
	note = {arXiv: 2001.02480},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {2906--2918},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/EVLCVHS2/Mokrý and Rajmic - 2020 - Audio Inpainting Revisited and Reweighted.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/2A33DH5X/2001.html:text/html},
}

@article{kong_hifi-gan_2020,
	title = {{HiFi}-{GAN}: {Generative} {Adversarial} {Networks} for {Efficient} and {High} {Fidelity} {Speech} {Synthesis}},
	shorttitle = {{HiFi}-{GAN}},
	url = {http://arxiv.org/abs/2010.05646},
	abstract = {Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.},
	urldate = {2021-11-25},
	journal = {arXiv:2010.05646 [cs, eess]},
	author = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.05646},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/73L9YJYY/Kong et al. - 2020 - HiFi-GAN Generative Adversarial Networks for Effi.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/IW9BI7EQ/2010.html:text/html},
}

@misc{jung_isoiec_2021,
	title = {{ISO}/{IEC} {JTC} 1/{SC} 29/{AG} 5: {Guidelines} for remote experts viewing sessions},
	author = {Jung, J. and Wien, M. and Baroncini, V.},
	month = nov,
	year = {2021},
}

@article{gotz-hahn_konvid-150k_2021,
	title = {{KonVid}-150k: {A} {Dataset} for {No}-{Reference} {Video} {Quality} {Assessment} of {Videos} in-the-{Wild}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {{KonVid}-150k},
	doi = {10.1109/ACCESS.2021.3077642},
	abstract = {Video quality assessment (VQA) methods focus on particular degradation types, usually artificially induced on a small set of reference videos. Hence, most traditional VQA methods under-perform in-the-wild. Deep learning approaches have had limited success due to the small size and diversity of existing VQA datasets, either artificial or authentically distorted. We introduce a new in-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k. It consists of a coarsely annotated set of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally, we propose new efficient VQA approaches (MLSP-VQA) relying on multi-level spatially pooled deep-features (MLSP). They are exceptionally well suited for training at scale, compared to deep transfer learning approaches. Our best method, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient (SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark dataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC) and hand-crafted feature-based method (0.78 SRCC). We further investigate how alternative approaches perform under different levels of label noise, and dataset size, showing that MLSP-VQA-FF is the overall best method for videos in-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k sets the new state-of-the-art for cross-test performance on KoNViD-1k and LIVE-Qualcomm with a 0.83 and 0.64 SRCC, respectively. For KoNViD-1k this inter-dataset testing outperforms intra-dataset experiments, showing excellent generalization.},
	journal = {IEEE Access},
	author = {Götz-Hahn, Franz and Hosu, Vlad and Lin, Hanhe and Saupe, Dietmar},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Feature extraction, Training, Cameras, Quality assessment, Distortion, Streaming media, Video recording, Datasets, deep transfer learning, multi-level spatially-pooled features, video quality assessment, video quality dataset},
	pages = {72139--72160},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/55NM3IGS/9423997.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/25A7ERPE/Götz-Hahn et al. - 2021 - KonVid-150k A Dataset for No-Reference Video Qual.pdf:application/pdf},
}

@article{upenik_large-scale_2021,
	title = {Large-{Scale} {Crowdsourcing} {Subjective} {Quality} {Evaluation} of {Learning}-{Based} {Image} {Coding}},
	abstract = {Learning-based image codecs produce different compression artifacts, when compared to the blocking and blurring degradation introduced by conventional image codecs, such as JPEG, JPEG{\textasciitilde}2000 and HEIC. In this paper, a crowdsourcing based subjective quality evaluation procedure was used to benchmark a representative set of end-to-end deep learning-based image codecs submitted to the MMSP'2020 Grand Challenge on Learning-Based Image Coding and the JPEG AI Call for Evidence. For the first time, a double stimulus methodology with a continuous quality scale was applied to evaluate this type of image codecs. The subjective experiment is one of the largest ever reported including more than 240 pair-comparisons evaluated by 118 naïve subjects. The results of the benchmarking of learning-based image coding solutions against conventional codecs are organized in a dataset of differential mean opinion scores along with the stimuli and made publicly available},
	journal = {IEEE Visual Communications and Image Processing (VCIP 2021)},
	editor = {Upenik, Evgeniy and Testolina, Michela and Ascenso, Joao and Pereira, Fernando and Ebrahimi, Touradj},
	year = {2021},
	note = {Meeting Name: IEEE Visual Communications and Image Processing (VCIP 2021)
Publisher: IEEE},
	keywords = {deep learning, subjective evaluation, crowdsourcing, image coding, learning-based compression, visual quality},
}

@inproceedings{rao_towards_2021,
	title = {Towards {High} {Resolution} {Video} {Quality} {Assessment} in the {Crowd}},
	doi = {10.1109/QoMEX51781.2021.9465425},
	abstract = {Assessing high resolution video quality is usually performed using controlled, defined, and standardized lab tests. This method of acquiring human ratings in a lab environment is time-consuming and may also not reflect the typical viewing conditions. To overcome these disadvantages, crowd testing paradigms have been used for assessing video quality in general. Crowdsourcing-based tests enable a more diverse set of participants and also use a realistic hardware setup and viewing environment of typical users. However, obtaining valid ratings for high-resolution video quality poses several problems. Example issues are that streaming of such high-bandwidth content may not be feasible for some users, or that crowd participants lack an appropriate, high-resolution display device. In this paper, we propose a method to overcome such problems and conduct a crowd test using for higher resolution content by using a 540 p cutout from the center of the original 2160p video. To this aim, we use the videos from Test\#1 of the publicly available dataset AVT-VQDB-UHD-1, which contains videos up to a resolution of UHD-1. The quality-labels available from that lab test allow us to compare the results with the crowd test presented in this paper. It is shown that there is a Pearson correlation of 0.96 between the lab and crowd tests and hence such crowd tests can reliably be used for video assessment of higher resolution content. The overall implementation of the crowd test framework and the results are made publicly available for further research and reproducibility1.},
	booktitle = {2021 13th {International} {Conference} on {Quality} of {Multimedia} {Experience} ({QoMEX})},
	author = {Rao, Rakesh Rao Ramachandra and Göring, Steve and Raake, Alexander},
	month = jun,
	year = {2021},
	note = {ISSN: 2472-7814},
	keywords = {Quality assessment, Correlation, Streaming media, Crowdsourcing, Reliability, Video recording, video quality assessment, adaptive streaming, Hardware, Reproducibility of results, subjective testing},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/59IZSVDW/9465425.html:text/html;IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/JVLKJPAR/9465425.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/BW44YYRU/Rao et al. - 2021 - Towards High Resolution Video Quality Assessment i.pdf:application/pdf},
}

@inproceedings{testolina_review_2021,
	title = {Review of subjective quality assessment methodologies and standards for compressed images evaluation},
	volume = {11842},
	doi = {10.1117/12.2597813},
	abstract = {Lossy image compression algorithms are usually employed to reduce the storage space required by the large number of digital pictures that are acquired and stored daily on digital devices. Despite the gain in storage space, these algorithms might introduce visible distortions on the images. However, users typically value the visual quality of digital media and do not tolerate any distortion. Objective image quality assessment metrics propose to predict the amount of such distortions as perceived by human subjects, but a limited number of studies have been devoted to the objective assessment of the visibility of artifacts on images as seen by human subjects. In other words, most objective quality metrics do not indicate when the artifacts become imperceptible to human observers. An objective image quality metric that assesses the visibility of artifacts could, in fact, drive the compression methods toward a visually lossless approach. In this paper, we present a subjective image quality assessment dataset, designed for the problem of visually lossless quality evaluation for image compression. The distorted images have been labeled, after a subjective experiment held with crowdsourcing, with the probability of the artifact to be visible to human observers. In contrast to other datasets in the state of the art, the proposed dataset contains a big number of images along with multiple distortions, making it suitable as a training set for a learning-based approach to objective quality assessment.},
	urldate = {2021-11-29},
	booktitle = {Applications of {Digital} {Image} {Processing} {XLIV}},
	publisher = {SPIE},
	author = {Testolina, Michela and Ebrahimi, Touradj},
	month = aug,
	year = {2021},
	pages = {302--315},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/M54VJ6NK/12.2597813.html:text/html;Submitted Version:/Users/rosscutler/Zotero/storage/WXI6U4ZL/Testolina and Ebrahimi - 2021 - Review of subjective quality assessment methodolog.pdf:application/pdf},
}

@inproceedings{rainer_web_2013,
	title = {A web based subjective evaluation platform},
	doi = {10.1109/QoMEX.2013.6603196},
	abstract = {Preparing and conducting subjective quality assessments is a time consuming and expensive task. Therefore, we present a Web-based evaluation framework which aims on reducing the time needed for planning and designing a subjective quality assessment. The presented framework can be used for both crowdsourced and laboratory experiments. It should ease the task of designing a subjective quality assessment by providing a flexible framework. The framework has proven its applicability and flexibility to design and conduct assessments in the past and is available as open source.},
	booktitle = {2013 {Fifth} {International} {Workshop} on {Quality} of {Multimedia} {Experience} ({QoMEX})},
	author = {Rainer, Benjamin and Waltl, Markus and Timmerer, Christian},
	month = jul,
	year = {2013},
	keywords = {Multimedia communication, Quality assessment, TV, Streaming media, Crowdsourced Quality Evaluation, Laboratory Quality Evaluation, Quality Assessment Framework, Conferences, Laboratories, Browsers, Evaluation Platform},
	pages = {24--25},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/H67NYFW6/6603196.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/4F8MJSBR/Rainer et al. - 2013 - A web based subjective evaluation platform.pdf:application/pdf},
}

@article{noauthor_tutorial_nodate,
	title = {Tutorial - {Objective} perceptual assessment of video quality: {Full} reference television},
	language = {en},
	pages = {218},
	file = {Tutorial - Objective perceptual assessment of vide.pdf:/Users/rosscutler/Zotero/storage/NULJNUWH/Tutorial - Objective perceptual assessment of vide.pdf:application/pdf},
}

@misc{noauthor_objective_2004,
	title = {Objective {Perceptual} {Assessment} of {Video} {Quality}: {Full} {Reference} {Television}, {Tutorial}},
	publisher = {ITU-T Telecommunication Standardization Bureau},
	year = {2004},
}

@article{taal_algorithm_2011,
	title = {An {Algorithm} for {Intelligibility} {Prediction} of {Time}–{Frequency} {Weighted} {Noisy} {Speech}},
	volume = {19},
	issn = {1558-7924},
	doi = {10.1109/TASL.2011.2114881},
	abstract = {In the development process of noise-reduction algorithms, an objective machine-driven intelligibility measure which shows high correlation with speech intelligibility is of great interest. Besides reducing time and costs compared to real listening experiments, an objective intelligibility measure could also help provide answers on how to improve the intelligibility of noisy unprocessed speech. In this paper, a short-time objective intelligibility measure (STOI) is presented, which shows high correlation with the intelligibility of noisy and time-frequency weighted noisy speech (e.g., resulting from noise reduction) of three different listening experiments. In general, STOI showed better correlation with speech intelligibility compared to five other reference objective intelligibility models. In contrast to other conventional intelligibility models which tend to rely on global statistics across entire sentences, STOI is based on shorter time segments (386 ms). Experiments indeed show that it is beneficial to take segment lengths of this order into account. In addition, a free Matlab implementation is provided.},
	number = {7},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Taal, Cees H. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},
	month = sep,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
	keywords = {Speech, Time frequency analysis, Noise measurement, speech enhancement, Speech processing, Correlation, Noise reduction, Signal to noise ratio, objective measure, speech intelligibility prediction},
	pages = {2125--2136},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/D7SN7DI9/5713237.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/EEQQ7MYH/Taal et al. - 2011 - An Algorithm for Intelligibility Prediction of Tim.pdf:application/pdf},
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2021-12-01},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/QIMRQAET/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/QGN4UCTT/1505.html:text/html},
}

@inproceedings{kubichek_mel-cepstral_1993,
	title = {Mel-cepstral distance measure for objective speech quality assessment},
	volume = {1},
	doi = {10.1109/PACRIM.1993.407206},
	abstract = {The author proposes a perceptually motivated modification to the cepstral distance measure (CD) based on the mel frequency scale and critical-band filtering. The new objective parameter is referred to as the mel cepstral distance (MCD). The author measures and compares the performance of the CD and MCD algorithms by applying them to a dataset representing low-bit-rate code-excited linear prediction (CELP)-coded speech with simulated channel conditions. The improvement in correlation with subjective DAM scores indicates that critical band filtering (and frequency warping) allows better modeling of perceived quality.{\textless}{\textgreater}},
	booktitle = {Proceedings of {IEEE} {Pacific} {Rim} {Conference} on {Communications} {Computers} and {Signal} {Processing}},
	author = {Kubichek, R.},
	month = may,
	year = {1993},
	keywords = {Humans, Speech, Quality assessment, Cepstral analysis, Distortion measurement, Ear, Filters, Frequency measurement, Nonlinear distortion, Psychoacoustic models},
	pages = {125--128 vol.1},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/473L3CB7/407206.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/MTPVZINZ/Kubichek - 1993 - Mel-cepstral distance measure for objective speech.pdf:application/pdf},
}

@misc{noauthor_improving_nodate,
	title = {Improving {Audio} {Quality} in {Duo} with {WaveNetEQ}},
	url = {http://ai.googleblog.com/2020/04/improving-audio-quality-in-duo-with.html},
	abstract = {Posted by Pablo Barrera, Software Engineer, Google Research and Florian Stimberg, Research Engineer, DeepMind     Online calls have become a...},
	language = {en},
	urldate = {2021-11-30},
	journal = {Google AI Blog},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/29GLV9KA/improving-audio-quality-in-duo-with.html:text/html},
}

@inproceedings{lecomte_packet-loss_2015,
	title = {Packet-loss concealment technology advances in {EVS}},
	doi = {10.1109/ICASSP.2015.7179065},
	abstract = {EVS, the newly standardized 3GPP Codec for Enhanced Voice Services (EVS) was developed for mobile services such as VoLTE, where error resilience is highly essential. The presented paper outlines all aspects of the advances brought during the EVS development on packet loss concealment, by presenting a high level description of all technical features present in the final standardized codec. Coupled with jitter buffer management, the EVS codec provides robustness against late or lost packets. The advantages of the new EVS codec over reference codecs are further discussed based on listening test results.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Lecomte, Jérémie and Vaillancourt, Tommy and Bruhn, Stefan and Sung, Hosang and Peng, Ke and Kikuiri, Kei and Wang, Bin and Subasingha, Shaminda and Faure, Julien},
	month = apr,
	year = {2015},
	note = {ISSN: 2379-190X},
	keywords = {Packet loss, Speech, Decoding, Time-domain analysis, Codecs, Speech coding, Interpolation, speech coding, audio coding, Concealment, EVS, VoLTE},
	pages = {5708--5712},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/HINKJTUF/7179065.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/P4PBK888/Lecomte et al. - 2015 - Packet-loss concealment technology advances in EVS.pdf:application/pdf},
}

@inproceedings{tominaga_performance_2010,
	title = {Performance comparisons of subjective quality assessment methods for mobile video},
	doi = {10.1109/QOMEX.2010.5517948},
	abstract = {There are many subjective assessment methods, depending on the purpose of the video quality evaluation, provided by ITU-T and ITU-R recommendations. We compared eight subjective assessment methods, Double-Stimulus Continuous Quality-Scale (DSCQS), Double-Stimulus Impairment Scale (DSIS), ACR 5-grade scale (ACR5), ACR5 with Hidden Reference (ACR5-HR), ACR 11-grade scale (ACR11), ACR11-HR, Subjective Assessment of Multimedia Video Quality (SAMVIQ), and SAMVIQ-HR, to clarify their applicability to a variety of video quality assessment. To do this, we evaluated the total assessment time and difficult/ease in evaluation for the participants, as well as the characteristics of different rating scales and their statistical reliability. As a result, we clarified that the correlation coefficients and rank correlation coefficients of the mean opinion scores between pairs of the eight methods were high. As for statistical reliability of each rating method, DSIS, ACR5, and SAMVIQ outperformed the other methods. Moreover, participant questionnaire results showed that ACR5 was a suitable method from the viewpoint of total assessment time and ease of evaluation.},
	booktitle = {2010 {Second} {International} {Workshop} on {Quality} of {Multimedia} {Experience} ({QoMEX})},
	author = {Tominaga, Toshiko and Hayashi, Takanori and Okamoto, Jun and Takahashi, Akira},
	month = jun,
	year = {2010},
	keywords = {Multimedia communication, QoE, Quality assessment, Mobile computing, Streaming media, Laboratories, Testing, Broadcasting, IPTV, Microcomputers, Performance comparison, QVGA, Standardization, Subjective assessment method, Video quality},
	pages = {82--87},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/FZ8JFU2E/5517948.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/EF7NK9EG/Tominaga et al. - 2010 - Performance comparisons of subjective quality asse.pdf:application/pdf},
}

@article{mercer_moss_optimal_2016,
	title = {On the {Optimal} {Presentation} {Duration} for {Subjective} {Video} {Quality} {Assessment}},
	volume = {26},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2015.2461971},
	abstract = {Subjective quality assessment is an essential component of modern image and video processing for both the validation of objective metrics and the comparison of coding methods. However, the standard procedures used to collect data can be prohibitively time consuming. One way of increasing the efficiency of data collection is to reduce the duration of test sequences from a 10-s length currently used in most subjective video quality assessment (VQA) experiments. Here, we explore the impact of reducing sequence length upon perceptual accuracy when identifying compression artifacts. A group of four reference sequences, together with five levels of distortion, are used to compare the subjective ratings of viewers watching videos between 1.5 and 10 s long. We identify a smooth function indicating that accuracy increases linearly as the length of the sequences increases from 1.5 to 7 s. The accuracy of observers viewing 1.5-s sequences was significantly inferior to those viewing sequences of 5, 7, and 10 s. We argue that sequences between 5 and 10 s produce satisfactory levels of accuracy but the practical benefits of acquiring more data lead us to recommend the use of 5-s sequences for future VQA studies that use the double stimulus continuous quality scale methodology.},
	number = {11},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Mercer Moss, Felix and Wang, Ke and Zhang, Fan and Baddeley, Roland and Bull, David R.},
	month = nov,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Context, Quality assessment, Distortion, Video recording, Testing, subjective testing, Databases, DSCQS, HEVC, mean opinion scores, methodology, Observers, quality assessment, video databases, video presentation},
	pages = {1977--1987},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/4G3GWNJR/7172512.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/GC5E6XHU/Mercer Moss et al. - 2016 - On the Optimal Presentation Duration for Subjectiv.pdf:application/pdf},
}

@article{cote_influence_2007,
	title = {Influence of loudness level on the overall quality of transmitted speech},
	volume = {2},
	abstract = {This paper consists of a study on the influence of the loudness on the perceived quality of transmitted speech. This quality is based on judgments of particular quality features, one of which is loudness. In order to determine the influence of loudness on perceived speech quality, we designed a two-step auditory experiment. We varied the speech level of selected speech samples and degraded them by coding and packet-loss. Results show that loudness has an effect on the overall speech quality, but that effect depends on the other impairments involved in the transmission path, and especially on the bandwidth of the transmitted speech. We tried to predict the auditory judgments with two quality prediction models. The signal-based WB-PESQ model, which normalizes the speech signals to a constant speech level, does not succeed in predicting the speech quality for speech signals with only impairments due to a non-optimum speech level. However, the parametric E-model, which includes a measure of the listening level, provides a good estimation of the speech quality.},
	journal = {Audio Engineering Society - 123rd Audio Engineering Society Convention 2007},
	author = {Côté, Nicolas and Gautier-Turbin, V. and Möller, S.},
	month = jan,
	year = {2007},
	pages = {677--684},
}

@inproceedings{purin_aecmos_2022,
	title = {{AECMOS}: {A} speech quality assessment metric for echo impairment},
	shorttitle = {{AECMOS}},
	abstract = {Traditionally, the quality of acoustic echo cancellers is evaluated using intrusive speech quality assessment measures such as ERLE {\textbackslash}cite\{g168\} and PESQ {\textbackslash}cite\{p862\}, or by carrying out subjective laboratory tests. Unfortunately, the former are not well correlated with human subjective measures, while the latter are time and resource consuming to carry out. We provide a new tool for speech quality assessment for echo impairment which can be used to evaluate the performance of acoustic echo cancellers. More precisely, we develop a neural network model to evaluate call quality degradations in two separate categories: echo and degradations from other sources. We show that our model is accurate as measured by correlation with human subjective quality ratings. Our tool can be used effectively to stack rank echo cancellation models. AECMOS is being made publicly available as an Azure service.},
	urldate = {2022-02-15},
	booktitle = {{ICASSP}},
	author = {Purin, Marju and Sootla, Sten and Sponza, Mateja and Saabas, Ando and Cutler, Ross},
	year = {2022},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{karjalainen_estimation_2001,
	title = {Estimation of {Modal} {Decay} {Parameters} from {Noisy} {Response} {Measurements}},
	volume = {50},
	abstract = {Estimation of modal decay parameters from noisy measurements of reverberant and resonating systems is a common problem in audio and acoustics, e.g., in room and concert hall measurements or musical instrument modeling. In this paper, reliable methods to estimate the initial response level, decay rate, and noise floor level from noisy measurement data are studied and compared. A new method, based on nonlinear optimization of a model for exponential decay plus stationary noise floor, is presented. Comparison with traditional decay parameter estimation techniques using simulated measurement data shows that the proposed method outperforms in accuracy and robustness, especially in extreme SNR conditions. Three cases of practical applications of the method are demonstrated.},
	journal = {Journal of the Audio Engineering Society},
	author = {Karjalainen, Matti and Antsalo, Poju and Mäkivirta, Aki and Peltonen, Timo and Välimäki, Vesa},
	month = may,
	year = {2001},
}

@inproceedings{halimeh_efficient_2020,
	title = {Efficient {Multichannel} {Nonlinear} {Acoustic} {Echo} {Cancellation} {Based} on a {Cooperative} {Strategy}},
	booktitle = {{ICASSP}},
	author = {Halimeh, Modar and Kellermann, Walter},
	year = {2020},
}

@inproceedings{fazel_cad-aec_2020,
	title = {{CAD}-{AEC}: {Context}-{Aware} {Deep} {Acoustic} {Echo} {Cancellation}},
	shorttitle = {{CAD}-{AEC}},
	doi = {10.1109/ICASSP40776.2020.9053508},
	abstract = {Deep-leaming based acoustic echo cancellation (AEC) methods have been shown to outperform the classical techniques. The main drawback of the learning-based AEC is its dependency on the training set, which limits its practical deployment in mobile devices and unconstrained environments. This paper proposes a context- aware deep AEC (CAD-AEC) by introducing two main components. The first component of the CAD-AEC borrows ideas from the classical AEC and performs frequency domain adaptive filtering of the microphone signal, to provide the deep AEC network with features that have less dependency on the development context. The second component is a deep contextual-attention module (CAM), inserted between the recurrent encoder and decoder architectures. The deep CAM adaptively scales the encoder output during inference with calculated attention weights that depend on the context. Experiments in both matched and mismatched training and testing environments, show that the proposed CAD-AEC can robustly achieve better echo return loss enhancement (ERLE) and perceptual speech quality compared to the previous classical and deep learning techniques.},
	booktitle = {{ICASSP}},
	author = {Fazel, Amin and El-Khamy, Mostafa and Lee, Jungwon},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {acoustic echo cancellation, context awareness, deep learning, gated recurrent unit, recurrent neural networks, Training, Acoustics, Testing, Echo cancellers, Filtering, Frequency-domain analysis, Microphones},
	pages = {6919--6923},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/JFH398UX/9053508.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/TW98WN4E/Fazel et al. - 2020 - CAD-AEC Context-Aware Deep Acoustic Echo Cancella.pdf:application/pdf},
}

@inproceedings{ma_acoustic_2020,
	title = {Acoustic {Echo} {Cancellation} by {Combining} {Adaptive} {Digital} {Filter} and {Recurrent} {Neural} {Network}},
	url = {http://arxiv.org/abs/2005.09237},
	abstract = {Acoustic Echo Cancellation (AEC) plays a key role in voice interaction. Due to the explicit mathematical principle and intelligent nature to accommodate conditions, adaptive filters with different types of implementations are always used for AEC, giving considerable performance. However, there would be some kinds of residual echo in the results, including linear residue introduced by mismatching between estimation and the reality and non-linear residue mostly caused by non-linear components on the audio devices. The linear residue can be reduced with elaborate structure and methods, leaving the non-linear residue intractable for suppression. Though, some non-linear processing methods have already be raised, they are complicated and inefficient for suppression, and would bring damage to the speech audio. In this paper, a fusion scheme by combining adaptive filter and neural network is proposed for AEC. The echo could be reduced in a large scale by adaptive filtering, resulting in little residual echo. Though it is much smaller than speech audio, it could also be perceived by human ear and would make communication annoy. The neural network is elaborately designed and trained for suppressing such residual echo. Experiments compared with prevailing methods are conducted, validating the effectiveness and superiority of the proposed combination scheme.},
	urldate = {2022-02-15},
	booktitle = {{arXiv}:2005.09237},
	author = {Ma, Lu and Huang, Hua and Zhao, Pei and Su, Tengrong},
	month = may,
	year = {2020},
	note = {arXiv: 2005.09237},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/NJMHBFFL/Ma et al. - 2020 - Acoustic Echo Cancellation by Combining Adaptive D.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/X6HTA4LZ/2005.html:text/html},
}

@article{noauthor_itu-t_2001,
	title = {{ITU}-{T} {Recommendation} {P}.862: {Perceptual} evaluation of speech quality ({PESQ}): {An} objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codec},
	year = {2001},
}

@article{noauthor_itu-t_2012,
	title = {{ITU}-{T} {Recommendation} {G}.168: {Digital} network echo cancellers},
	year = {2012},
}

@article{noauthor_itu-t_2012-1,
	title = {{ITU}-{T} {Recommendation} {G}.126: {Listener} {Echo} in {Telephone} {Networks}},
	year = {2012},
}

@article{noauthor_itu-t_2012-2,
	title = {{ITU}-{T} {Recommendation} {G}.122: {Influence} of {National} {Systems} on {Stability} and {Talker} {Echo} in {International} {Connections}},
	year = {2012},
}

@article{noauthor_itu-t_2012-3,
	title = {{ITU}-{T} {Recommendation} {G}.108.2: {Transmission} {Planning} {Aspects} of {Eco} {Cancellers}},
	year = {2012},
}

@inproceedings{hari_krishna_acoustic_2010,
	title = {Acoustic echo cancellation using a computationally efficient transform domain {LMS} adaptive filter},
	doi = {10.1109/ISSPA.2010.5605458},
	abstract = {Applications such as hands-free telephony, tele-classing and video-conferencing require the use of an acoustic echo canceller (AEC) to eliminate acoustic feedback from the loudspeaker to the microphone. Room acoustic echo cancellation typically requires adaptive filters with thousands of coefficients. Transform domain adaptive filter finds best solution for echo cancellation as it results in a significant reduction in the computational burden. Literature finds different orthogonal transform based adaptive filters for echo cancellation. In this paper, we present Hirschman Optimal Transform (HOT) based adaptive filter for elimination of echo from audio signals. Simulations and analysis show that HOT based LMS adaptive filter is computationally efficient and has fast convergence compared to LMS, NLMS and DFT-LMS. The computed Echo Return Loss Enhancement (ERLE), the general evaluation measure of echo cancellation, established the efficacy of proposed HOT based adaptive algorithm. In addition, the spectral flatness measure showed a significant improvement in cancelling the acoustic echo.},
	booktitle = {10th {International} {Conference} on {Information} {Science}, {Signal} {Processing} and their {Applications} ({ISSPA} 2010)},
	author = {Hari Krishna, E. and Raghuram, M. and Venu Madhav, K. and Ashoka Reddy, K.},
	month = may,
	year = {2010},
	keywords = {Mathematical model, Transforms, Adaptation model, Adaptive filters, Echo Cancellation, Equations, Filtering algorithms, HOT, Least squares approximation, LMS},
	pages = {409--412},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/JQLKBU72/5605458.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/HWKPJDU2/Hari Krishna et al. - 2010 - Acoustic echo cancellation using a computationally.pdf:application/pdf},
}

@article{noauthor_itu-t_1998-1,
	title = {{ITU}-{T} {P}.831: {Subjective}\vphantom{\{}\} performance evaluation of network echo cancellers {ITU}-{T} {P}-series {Recommendations}},
	year = {1998},
}

@article{noauthor_itu-t_2000,
	title = {{ITU}-{T} {Recommendation} {P}.832: {Subjective} performance evaluation of hands-free terminals},
	journal = {International Telecommunication Union},
	year = {2000},
}

@article{noauthor_ieee_2010,
	title = {{IEEE} 1329-2010 {Standard}: {Method} for {Measuring} {Transmission} {Performance} of {Handsfree} {Telephone} {Sets}},
	journal = {IEEE},
	year = {2010},
}

@article{garofolo_darpa_1993,
	title = {Darpa {Timit} {Acoustic}-{Phonetic} {Continuous} {Speech} {Corpus} {CD}-{ROM} {TIMIT}},
	url = {https://www.nist.gov/publications/darpa-timit-acoustic-phonetic-continuous-speech-corpus-cd-rom-timit},
	language = {en},
	urldate = {2022-02-15},
	author = {Garofolo, John S. and Lamel, L. F. and Fisher, W. M. and Fiscus, Jonathan G. and Pallett, D. S. and Dahlgren, Nancy L.},
	month = feb,
	year = {1993},
	note = {Last Modified: 2017-02-19T20:02-05:00},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/JX59L59Y/darpa-timit-acoustic-phonetic-continuous-speech-corpus-cd-rom-timit.html:text/html},
}

@misc{angeles_governors_2002,
	title = {Governors},
	abstract = {null},
	author = {Angeles, Los and {Bluetooth} and Nunn, John P. and Sohma, Yoshizo and Furness, Roger K. and Director, Executive and President, Kees A. Immink and President-elect, Ronald Streicher and Kaiser, James A. and President, Vice and Thiele, Neville and Bäder, Karl-otto and Hoyt, Curtis and Pritts, Roy and Puluse, Don and Robinson, David and Staepelaere, Annemarie and Tan, Roland},
	year = {2002},
	file = {Citeseer - Full Text PDF:/Users/rosscutler/Zotero/storage/KUSRHJJA/Angeles et al. - 2002 - Governors.pdf:application/pdf;Citeseer - Snapshot:/Users/rosscutler/Zotero/storage/YNYY3UNY/summary.html:text/html},
}

@misc{lee_dnn-based_2015,
	title = {{DNN}-{Based} {Residual} {Echo} {Suppression}},
	abstract = {Due to the limitations of power ampliﬁers or loudspeakers, the echo signals captured in the microphones are not in a linear relationship with the far-end signals even when the echo path is perfectly linear. The nonlinear components of the echo cannot be successfully removed by a linear acoustic echo canceller. Residual echo suppression (RES) is a technique to suppress the remained echo after acoustic echo suppression (AES). Conventional approaches compute RES gain using Wiener ﬁlter or spectral subtraction method based on the estimated statistics on related signals. In this paper, we propose a deep neural network (DNN)-based RES gain estimation based on both the far-end and the AES output signals in all frequency bins. A DNN architecture, which is suitable to model a complicated nonlinear mapping between high-dimensional vectors, is employed as a regression function from these signals to the optimal RES gain. The proposed method can suppress the residual components without any explicit double-talk detectors. The experimental results show that our proposed approach outperforms a conventional method in terms of the echo return loss enhancement (ERLE) for single-talk periods and the perceptual evaluation of speech quality (PESQ) score for double-talk periods.},
	language = {en},
	journal = {Sixteenth Annual Conference of the International Speech Communication Association},
	author = {Lee, Chul Min and Shin, Jong Won and Kim, Nam Soo},
	year = {2015},
	pages = {5},
	file = {Lee et al. - DNN-Based Residual Echo Suppression.pdf:/Users/rosscutler/Zotero/storage/6DXKCQ58/Lee et al. - DNN-Based Residual Echo Suppression.pdf:application/pdf},
}

@article{gitiaux_aura_2021,
	title = {Aura: {Privacy}-preserving augmentation to improve test set diversity in noise suppression applications},
	shorttitle = {Aura},
	url = {http://arxiv.org/abs/2110.04391},
	abstract = {Noise suppression models running in production environments are commonly trained on publicly available datasets. However, this approach leads to regressions in production environments due to the lack of training/testing on representative customer data. Moreover, due to privacy reasons, developers cannot listen to customer content. This `ears-off' situation motivates augmenting existing datasets in a privacy-preserving manner. In this paper, we present Aura, a solution to make existing noise suppression test sets more challenging and diverse while limiting the sampling budget. Aura is `ears-off' because it relies on a feature extractor and a metric of speech quality, DNSMOS P.835, both pre-trained on data obtained from public sources. As an application of {\textbackslash}aura, we augment a current benchmark test set in noise suppression by sampling audio files from a new batch of data of 20K clean speech clips from Librivox mixed with noise clips obtained from AudioSet. Aura makes the existing benchmark test set harder by 100\% in DNSMOS P.835, a 26 improvement in Spearman's rank correlation coefficient (SRCC) compared to random sampling and, identifies 73\% out-of-distribution samples to augment the test set.},
	urldate = {2022-02-15},
	journal = {arXiv:2110.04391 [cs, eess]},
	author = {Gitiaux, Xavier and Khant, Aditya and Reddy, Chandan and Gupchup, Jayant and Cutler, Ross},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.04391},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/KVWILT43/Gitiaux et al. - 2021 - Aura Privacy-preserving augmentation to improve t.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/LC3EGBTA/2110.html:text/html},
}

@inproceedings{sun_explore_2022,
	title = {Explore relative and context information with transformer for joint acoustic echo cancellation and speech enhancement},
	language = {en},
	booktitle = {{ICASSP}},
	author = {Sun, Xingwei and Cao, Chenbin and Li, Qinglong and Wang, Linzhang and Xiang, Fei},
	year = {2022},
	file = {Sun et al. - ACOUSTIC ECHO CANCELLATION AND SPEECH ENHANCEMENT.pdf:/Users/rosscutler/Zotero/storage/9CDGQKIQ/Sun et al. - ACOUSTIC ECHO CANCELLATION AND SPEECH ENHANCEMENT.pdf:application/pdf},
}

@article{zhang_multi-scale_2022,
	title = {Multi-scale temporal frequency convolutional network with axial attention for speech enhancement},
	language = {en},
	journal = {ICASSP},
	author = {Zhang, Guochang and Yu, Libiao and Wang, Chunliang and Wei, Jianqiang},
	year = {2022},
	file = {Zhang et al. - ATTENTION FOR SPEECH ENHANCEMENT.pdf:/Users/rosscutler/Zotero/storage/YWWGUND4/Zhang et al. - ATTENTION FOR SPEECH ENHANCEMENT.pdf:application/pdf},
}

@inproceedings{zhao_deep_2022,
	title = {A deep hierarchical fusion network for fullband acoustic echo cancellation},
	language = {en},
	booktitle = {{ICASSP}},
	author = {Zhao, Haoran and Li, Nan and Han, Runqiang and Chen, Lianwu and Zheng, Xiguang and Zhang, Chen and Guo, Liang and Yu, Bing},
	year = {2022},
	file = {Zhao et al. - Kuaishou Technology, Beijing, China.pdf:/Users/rosscutler/Zotero/storage/ZGGKU98T/Zhao et al. - Kuaishou Technology, Beijing, China.pdf:application/pdf},
}

@inproceedings{cui_multi-scale_2022,
	title = {Multi-scale refinement network based acoustic echo cancellation},
	language = {en},
	booktitle = {{ICASSP}},
	author = {Cui, Fan and Guo, Liyong and Li, Wenfeng and Gao, Peng and Wang, Yujun},
	year = {2022},
	file = {Cui et al. - FOR REVIEW ONLY MULTI-SCALE REFINEMENT NETWORK BAS.pdf:/Users/rosscutler/Zotero/storage/BUNL3KT9/Cui et al. - FOR REVIEW ONLY MULTI-SCALE REFINEMENT NETWORK BAS.pdf:application/pdf},
}

@inproceedings{zhang_multi-task_2022,
	title = {Multi-task deep residual echo suppression with echo-aware loss},
	language = {en},
	booktitle = {{ICASSP}},
	author = {Zhang, Shimin and Wang, Ziteng and Sun, Jiayao and Fu, Yihui and Tian, Biao and Fu, Qiang and Xie, Lei},
	year = {2022},
	file = {Zhang et al. - FOR REVIEW ONLY MULTI-TASK DEEP RESIDUAL ECHO SUPP.pdf:/Users/rosscutler/Zotero/storage/AF7U5TEL/Zhang et al. - FOR REVIEW ONLY MULTI-TASK DEEP RESIDUAL ECHO SUPP.pdf:application/pdf},
}

@inproceedings{hao_attention-based_2019,
	title = {An {Attention}-based {Neural} {Network} {Approach} for {Single} {Channel} {Speech} {Enhancement}},
	doi = {10.1109/ICASSP.2019.8683169},
	abstract = {This paper proposes an attention-based neural network approach for single channel speech enhancement. Our work is inspired by the recent success of attention models in sequence-to-sequence learning. It is intuitive to use attention mechanism in speech enhancement as humans are able to focus on the important speech components in an audio stream with "high attention" while perceiving the unimportant region (e.g., noise or interference) in "low attention", and thus adjust the focal point over time. Specifically, taking noisy spectrum as input, our model is composed of an LSTM based encoder, an attention mechanism and a speech generator, resulting in enhanced spectrum. Experiments show that, as compared with OM-LSA and the LSTM baseline, the proposed attention approach can consistently achieve better performance in terms of speech quality (PESQ) and intelligibility (STOI). More promisingly, the attention-based approach has better generalization ability to unseen noise conditions.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Hao, Xiang and Shan, Changhao and Xu, Yong and Sun, Sining and Xie, Lei},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {Speech enhancement, Feature extraction, Neural networks, Training, Noise measurement, speech enhancement, neural networks, attention mechanism, Signal to noise ratio, Generators},
	pages = {6895--6899},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/U2L8EE39/8683169.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/ZDFNV6WA/Hao et al. - 2019 - An Attention-based Neural Network Approach for Sin.pdf:application/pdf},
}

@article{ebner_audio_2020,
	title = {Audio inpainting with generative adversarial network},
	url = {http://arxiv.org/abs/2003.07704},
	abstract = {We study the ability of Wasserstein Generative Adversarial Network (WGAN) to generate missing audio content which is, in context, (statistically similar) to the sound and the neighboring borders. We deal with the challenge of audio inpainting long range gaps (500 ms) using WGAN models. We improved the quality of the inpainting part using a new proposed WGAN architecture that uses a short-range and a long-range neighboring borders compared to the classical WGAN model. The performance was compared with two different audio instruments (piano and guitar) and on virtuoso pianists together with a string orchestra. The objective difference grading (ODG) was used to evaluate the performance of both architectures. The proposed model outperforms the classical WGAN model and improves the reconstruction of high-frequency content. Further, we got better results for instruments where the frequency spectrum is mainly in the lower range where small noises are less annoying for human ear and the inpainting part is more perceptible. Finally, we could show that better test results for audio dataset were reached where a particular instrument is accompanist by other instruments if we train the network only on this particular instrument neglecting the other instruments.},
	urldate = {2022-02-14},
	journal = {arXiv:2003.07704 [cs, eess, stat]},
	author = {Ebner, P. P. and Eltelt, A.},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.07704},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/P3TQAI6D/Ebner and Eltelt - 2020 - Audio inpainting with generative adversarial netwo.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/QR2KLZ9T/2003.html:text/html},
}

@article{kegler_deep_2020-1,
	title = {Deep speech inpainting of time-frequency masks},
	url = {http://arxiv.org/abs/1910.09058},
	doi = {10.21437/Interspeech.2020-1532},
	abstract = {Transient loud intrusions, often occurring in noisy environments, can completely overpower speech signal and lead to an inevitable loss of information. While existing algorithms for noise suppression can yield impressive results, their efficacy remains limited for very low signal-to-noise ratios or when parts of the signal are missing. To address these limitations, here we propose an end-to-end framework for speech inpainting, the context-based retrieval of missing or severely distorted parts of time-frequency representation of speech. The framework is based on a convolutional U-Net trained via deep feature losses, obtained using speechVGG, a deep speech feature extractor pre-trained on an auxiliary word classification task. Our evaluation results demonstrate that the proposed framework can recover large portions of missing or distorted time-frequency representation of speech, up to 400 ms and 3.2 kHz in bandwidth. In particular, our approach provided a substantial increase in STOI \& PESQ objective metrics of the initially corrupted speech samples. Notably, using deep feature losses to train the framework led to the best results, as compared to conventional approaches.},
	urldate = {2022-02-14},
	journal = {Interspeech 2020},
	author = {Kegler, Mikolaj and Beckmann, Pierre and Cernak, Milos},
	month = oct,
	year = {2020},
	note = {arXiv: 1910.09058},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
	pages = {3276--3280},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/KC778TUP/Kegler et al. - 2020 - Deep speech inpainting of time-frequency masks.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/KFW4RHTT/1910.html:text/html},
}

@article{chang_deep_2019,
	title = {Deep {Long} {Audio} {Inpainting}},
	url = {http://arxiv.org/abs/1911.06476},
	abstract = {Long ({\textgreater} 200 ms) audio inpainting, to recover a long missing part in an audio segment, could be widely applied to audio editing tasks and transmission loss recovery. It is a very challenging problem due to the high dimensional, complex and non-correlated audio features. While deep learning models have made tremendous progress in image and video inpainting, audio inpainting did not attract much attention. In this work, we take a pioneering step, exploring the possibility of adapting deep learning frameworks from various domains inclusive of audio synthesis and image inpainting for audio inpainting. Also, as the first to systematically analyze factors affecting audio inpainting performance, we explore how factors ranging from mask size, receptive field and audio representation could affect the performance. We also set up a benchmark for long audio inpainting. The code will be available on GitHub upon accepted.},
	urldate = {2022-02-14},
	journal = {arXiv:1911.06476 [cs, eess]},
	author = {Chang, Ya-Liang and Lee, Kuan-Ying and Wu, Po-Yu and Lee, Hung-yi and Hsu, Winston},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.06476},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/UVWSQQEP/Chang et al. - 2019 - Deep Long Audio Inpainting.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/L87DE6VT/1911.html:text/html},
}

@article{marafioti_context_2019,
	title = {A {Context} {Encoder} {For} {Audio} {Inpainting}},
	volume = {27},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2019.2947232},
	abstract = {In this article, we study the ability of deep neural networks (DNNs) to restore missing audio content based on its context, i.e., inpaint audio gaps. We focus on a condition which has not received much attention yet: gaps in the range of tens of milliseconds. We propose a DNN structure that is provided with the signal surrounding the gap in the form of time-frequency (TF) coefficients. Two DNNs with either complex-valued TF coefficient output or magnitude TF coefficient output were studied by separately training them on inpainting two types of audio signals (music and musical instruments) having 64-ms long gaps. The magnitude DNN outperformed the complex-valued DNN in terms of signal-to-noise ratios and objective difference grades. Although, for instruments, a reference inpainting obtained through linear predictive coding performed better in both metrics, it performed worse than the magnitude DNN for music. This demonstrates the potential of the magnitude DNN, in particular for inpainting signals that are more complex than single instrument sounds.},
	number = {12},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Marafioti, Andrés and Perraudin, Nathanaël and Holighaus, Nicki and Majdak, Piotr},
	month = dec,
	year = {2019},
	note = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	keywords = {Reliability, Time-domain analysis, Psychoacoustic models, frequency-domain analysis, Image reconstruction, Instruments, machine learning, Music, Prediction algorithms, signal processing algorithms},
	pages = {2362--2372},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/QTZW2GPC/8867915.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/T72CK9JS/Marafioti et al. - 2019 - A Context Encoder For Audio Inpainting.pdf:application/pdf},
}

@inproceedings{lotfidereshgi_speech_2018,
	title = {Speech {Prediction} {Using} an {Adaptive} {Recurrent} {Neural} {Network} with {Application} to {Packet} {Loss} {Concealment}},
	doi = {10.1109/ICASSP.2018.8462185},
	abstract = {This paper proposes a novel approach for speech signal prediction based on a recurrent neural network (RNN). Unlike existing RNN-based predictors, which operate on parametric features and are trained offline on a large collection of such features, the proposed predictor operates directly on speech samples and is trained online on the recent past of the speech signal. Optionally, the network can be pre-trained offline to speed-up convergence at start-up. The proposed predictor is a single end-to-end network that captures all sorts of dependencies between samples, and therefore has the potential to outperform classicallinear/non-linear and short-termllong-term speech predictor structures. We apply it to the packet loss concealment (PLC) problem and show that it outperforms the standard ITU G.711 Appendix I PLC technique.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Lotfidereshgi, Reza and Gournay, Philippe},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {Training, Packet loss, Speech processing, Correlation, long short-term memory, recurrent neural network, Recurrent neural networks, Speech recognition, packet loss concealment, Speech prediction},
	pages = {5394--5398},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/WVZ76CM2/8462185.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/8WWVBR4G/Lotfidereshgi and Gournay - 2018 - Speech Prediction Using an Adaptive Recurrent Neur.pdf:application/pdf},
}

@inproceedings{sreeraj_hybrid_2016,
	title = {Hybrid {Adaptive} {Loss} {Recovery} and {Enhanced} {Retransmission} {Technique} for {VoIP}},
	doi = {10.1109/WiSPNET.2016.7566415},
	abstract = {VoIP is an emerging technology, where the voice information is transmitted through the Internet. This technology is very useful but at the same time very challenging due to its susceptibility to various network variations. The VoIP packet transmission can get degraded easily and thus resulting in the packet loss, which in turn degrades network performance. In this paper, we propose to develop a hybrid adaptive loss recovery and enhanced retransmission technique for VoIP. In this technique, the sender and the receiver separately work on error correction of the damaged packets to enhance the network performance. The sender employs the FEC codes to retrieve the damaged packets. The receiver deploys the Packet Loss Concealment (PLC) technique and the FD retransmission technique dynamically according to the network requirements, to enhance the network performance.},
	booktitle = {2016 {International} {Conference} on {Wireless} {Communications}, {Signal} {Processing} and {Networking} ({WiSPNET})},
	author = {Sreeraj, M.V. and Gangadhar, M.},
	month = mar,
	year = {2016},
	keywords = {Delays, Packet loss, Receivers, Internet telephony, Forward error correction, Forward Error Correction (FEC), IP networks, Loss Recovery, Packet Loss Concealment (PLC), Retransmission, Voice Over IP (VoIP)},
	pages = {1631--1634},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/DNT83N4V/7566415.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/TQ4HY3YQ/Sreeraj and Gangadhar - 2016 - Hybrid Adaptive Loss Recovery and Enhanced Retrans.pdf:application/pdf},
}

@article{kim_robust_2018,
	title = {Robust speech quality enhancement method against background noise and packet loss at voice-over-{IP} receiver},
	volume = {37},
	url = {https://doi.org/10.7776/ASK.2018.37.6.512},
	doi = {10.7776/ASK.2018.37.6.512},
	abstract = {Improving voice quality is a major concern in telecommunications. In this paper, we propose a robust speech quality enhancement against background noise and packet loss at VoIP (Voice-over-IP) receiver. The proposed method combines network jitter estimation based on hybrid Markov chain, adaptive playout scheduling using the estimated jitter, and speech enhancement based on restoration of amplitude and phase to enhance the quality of the speech signal arriving at the VoIP receiver over IP network. The experimental results show that the proposed method removes the background noise added to the speech signal before encoding at the sender side and provides the enhanced speech quality in an unstable network environment.},
	language = {ko},
	number = {6},
	urldate = {2022-02-14},
	journal = {The Journal of the Acoustical Society of Korea},
	author = {Kim, Gee Yeun and Kim, Hyoung-Gook},
	month = nov,
	year = {2018},
	pages = {512--517},
	file = {Kim and Kim - 2018 - Robust speech quality enhancement method against b.pdf:/Users/rosscutler/Zotero/storage/Y82SMHBD/Kim and Kim - 2018 - Robust speech quality enhancement method against b.pdf:application/pdf},
}

@article{affonso_speech_2018,
	title = {Speech {Quality} {Assessment} in {Wireless} {VoIP} {Communication} {Using} {Deep} {Belief} {Network}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2871072},
	abstract = {Nowadays, the voice over Internet protocol (VoIP) communication service is widely adopted, and it counts with many users across the world. However, the users' quality of experience is not guaranteed because the voice signal quality can be affected by several degradations that happen in the network infrastructure. Thus, it is relevant to have a global speech quality assessment method that considers both wired and wireless networks to provide reliable results. In this paper, several network scenarios that consider different packet loss rates (PLRs) and wireless channel models are implemented in which the impaired signals are evaluated using the algorithm described in ITU-T Recommendation P.862. Preliminary results showed a relationship between both fading and PLR parameters and the global speech quality index. However, the P.862 algorithm is not viable in real VoIP scenarios. The ITU-T Recommendation P.563 describes a non-intrusive speech quality assessment method; nevertheless, its results are not confident. In this context, the main objective of this paper is to propose a non-intrusive speech quality classification model based on a deep belief network (DBN) that considers the wired and wireless impairments on the speech signal. Experimental results demonstrated a high correlation between the proposed model based on the DBN and P.862 algorithm, reaching a F-measure of 97.01\%. For validation, the non-intrusive P.563 algorithm is used; the proposed model and P.563 reached an average accuracy of 96.14\% and 72.12\%, respectively. Furthermore, subjective tests were carried out, and the proposed DBN model reached an accuracy of 94\%.},
	journal = {IEEE Access},
	author = {Affonso, Emmanuel T. and Nunes, Rodrigo D. and Rosa, Renata L. and Pivaro, Gabriel F. and Rodríguez, Demóstenes Z.},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Packet loss, Degradation, MOS, Quality assessment, deep neural networks, Indexes, Prediction algorithms, degradation, ITU-T P.862, Speech quality, wired network, Wireless communication, wireless network},
	pages = {77022--77032},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/7GZBIA7K/8513822.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/GI9QE97V/Affonso et al. - 2018 - Speech Quality Assessment in Wireless VoIP Communi.pdf:application/pdf},
}

@inproceedings{mugisha_double-side_2019,
	title = {A double-side {WSOLA} with gain prediction based on {GRU} for packet loss concealment},
	doi = {10.1109/ICSIDP47821.2019.9172927},
	abstract = {The perceived speech quality is one of the main issues in VoIP communications. Packet loss concealment (PLC) is among the best techniques to recover the lost packets over IP networks in order to improve the speech quality. In this paper, a double-side waveform similarity overlap and add(WSOLA) technique has been proposed in which both previously received packets and future packets involve in recovering the lost packets by using WSOLA technique. To overcome energy decrease that occurs during the waveform reconstruction, a gated recurrent unit (GRU) based gain control approach has been introduced to maintain the energy of a reconstructing waveform with the prediction of the lost packets gain by learning the previously received packets gain. The proposed algorithm has shown the outperformance to the standard WSOLA and double-side WSOLA without gain control at both random and burst packet loss according to the obtained perceptual evaluation of speech quality (PESQ) results.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Signal}, {Information} and {Data} {Processing} ({ICSIDP})},
	author = {Mugisha, Patrick and Wang, Jing and Zhao, Xiaohan and Li, Zhuoran and Qian, Kai},
	month = dec,
	year = {2019},
	keywords = {VoIP, packet loss concealment, gain prediction, GRU, WSOLA},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/QG3BSPZL/9172927.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/MQGRD74F/Mugisha et al. - 2019 - A double-side WSOLA with gain prediction based on .pdf:application/pdf},
}

@inproceedings{bakri_improved_2018,
	title = {An improved packet loss concealment technique for speech transmission in {VOIP}},
	doi = {10.1109/ICNLSP.2018.8374381},
	abstract = {Packet loss is a major source of voice distortion in VoIP (Voice over IP). Packet loss may be due to several reasons: routing problems, transmission errors, and network congestion in VoIP communications. To mitigate the effect of these losses on voice quality, PLC (Packet Loss Concealment) mechanisms are introduced in the decoders to reconstruct the lost frames. Many decoders have PLC algorithms as G729.A and G723.1. This decoders generally have bad correcting losses when there are many consecutive lost packets. For reinforce the PLC algorithm performance of decoders, several PLC techniques used both at the receiver level such as PLC technique based on ITU-T G711 Appendix I, then is longer used in VoIP. In this paper, we proposed the method of polynomial interpolation integrate at PLC technique based on ITU-T G711 Appendix I, for finding the synthetic signal approximation as an original signal by interpolating samples of pitch. Evaluation results of subjective PESQ confirmed that method proposed can effectively evaluate the speech quality in the IP environment.},
	booktitle = {2018 2nd {International} {Conference} on {Natural} {Language} and {Speech} {Processing} ({ICNLSP})},
	author = {Bakri, Adil and Amrouche, Abderrahmane and Abbas, Mourad},
	month = apr,
	year = {2018},
	keywords = {Delays, Packet loss, Receivers, VoIP, PESQ, Real-time systems, Interpolation, IP networks, ITU-IG7II Appendix I, OLA, PLC},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/34MHB67T/8374381.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/U2SDINNQ/Bakri et al. - 2018 - An improved packet loss concealment technique for .pdf:application/pdf},
}

@inproceedings{gueham_enhanced_2017,
	title = {An enhanced insertion packet loss concealment method for voice over {IP} network services},
	doi = {10.1109/TSP.2017.8076009},
	abstract = {In this paper, a Packet Loss Concealment (PLC) algorithm for ITU-G.722.2 speech coder is proposed in order to improve the decoded speech quality under burst packet loss conditions over packet-switched networks. First, we evaluated the repetition of the last received frame method in order to improve the concealment of lost frames. Then, we made a statistical study to evaluate the correlation between parameters values of adjacent frames. Finally, we present a new PLC method for ITU-G.722.2 by proposing an efficient approach to sending the side information to improve concealment of the lost frames and to resynchronize the encoder and decoder. The proposed method significantly improves the robustness of the ITU-G.722.2 codec against packet losses for voice over networks services. Tests in terms of Wideband Perceptual Evaluation of Speech Quality Mean Opinion Score (WB-PESQ MOS) quality measure and MUlti-Stimulus test with Hidden Reference and Anchor (MUSHRA) evaluation confirm the superiority of our proposed method against the PLC method embedded in G.722.2.},
	booktitle = {2017 40th {International} {Conference} on {Telecommunications} and {Signal} {Processing} ({TSP})},
	author = {Gueham, Tarek and Merazka, Fatiha},
	month = jul,
	year = {2017},
	keywords = {Packet loss, Standards, VoIP, Speech, MUSHRA, Codecs, Speech coding, Indexes, Frame Loss Concealment, ITU-G.722.2 Codec, WB-PESQ},
	pages = {377--382},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/J6SGTNIS/8076009.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/YZGYZHAN/Gueham and Merazka - 2017 - An enhanced insertion packet loss concealment meth.pdf:application/pdf},
}

@misc{noauthor_elsevier_nodate,
	title = {Elsevier {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S1877050918302230?token=EE4E1C60BFFCCA6E0FC2C3D63DA6C2891C5F3270FEC29ED8D2BEFFC53BD53A21FF8C7F44FE153E1F6F693567BBEEEBDF&originRegion=eu-west-1&originCreation=20220214151324},
	language = {en},
	urldate = {2022-02-14},
	doi = {10.1016/j.procs.2018.03.010},
	file = {Snapshot:/Users/rosscutler/Zotero/storage/C4AVCCQN/S1877050918302230.html:text/html},
}

@article{zhang_f-t-lstm_2021,
	title = {F-{T}-{LSTM} based {Complex} {Network} for {Joint} {Acoustic} {Echo} {Cancellation} and {Speech} {Enhancement}},
	url = {http://arxiv.org/abs/2106.07577},
	abstract = {With the increasing demand for audio communication and online conference, ensuring the robustness of Acoustic Echo Cancellation (AEC) under the complicated acoustic scenario including noise, reverberation and nonlinear distortion has become a top issue. Although there have been some traditional methods that consider nonlinear distortion, they are still inefficient for echo suppression and the performance will be attenuated when noise is present. In this paper, we present a real-time AEC approach using complex neural network to better modeling the important phase information and frequency-time-LSTMs (F-T-LSTM), which scan both frequency and time axis, for better temporal modeling. Moreover, we utilize modified SI-SNR as cost function to make the model to have better echo cancellation and noise suppression (NS) performance. With only 1.4M parameters, the proposed approach outperforms the AEC-challenge baseline by 0.27 in terms of Mean Opinion Score (MOS).},
	urldate = {2022-02-12},
	journal = {arXiv:2106.07577 [cs, eess]},
	author = {Zhang, Shimin and Kong, Yuxiang and Lv, Shubo and Hu, Yanxin and Xie, Lei},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.07577},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/THC6LA6E/Zhang et al. - 2021 - F-T-LSTM based Complex Network for Joint Acoustic .pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/SE36IPF7/2106.html:text/html},
}

@article{seidel_y2-net_2021,
	title = {Y\${\textasciicircum}2\$-{Net} {FCRN} for {Acoustic} {Echo} and {Noise} {Suppression}},
	url = {http://arxiv.org/abs/2103.17189},
	abstract = {In recent years, deep neural networks (DNNs) were studied as an alternative to traditional acoustic echo cancellation (AEC) algorithms. The proposed models achieved remarkable performance for the separate tasks of AEC and residual echo suppression (RES). A promising network topology is a fully convolutional recurrent network (FCRN) structure, which has already proven its performance on both noise suppression and AEC tasks, individually. However, the combination of AEC, postfiltering, and noise suppression to a single network typically leads to a noticeable decline in the quality of the near-end speech component due to the lack of a separate loss for echo estimation. In this paper, we propose a two-stage model (Y\${\textasciicircum}2\$-Net) which consists of two FCRNs, each with two inputs and one output (Y-Net). The first stage (AEC) yields an echo estimate, which - as a novelty for a DNN AEC model - is further used by the second stage to perform RES and noise suppression. While the subjective listening test of the Interspeech 2021 AEC Challenge mostly yielded results close to the baseline, the proposed method scored an average improvement of 0.46 points over the baseline on the blind testset in double-talk on the instrumental metric DECMOS, provided by the challenge organizers.},
	urldate = {2022-02-12},
	journal = {arXiv:2103.17189 [cs, eess]},
	author = {Seidel, Ernst and Franzen, Jan and Strake, Maximilian and Fingscheidt, Tim},
	month = jul,
	year = {2021},
	note = {arXiv: 2103.17189},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/6MH8V5EW/Seidel et al. - 2021 - Y\$^2\$-Net FCRN for Acoustic Echo and Noise Suppres.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/E5H6WHHI/2103.html:text/html},
}

@article{ivry_nonlinear_2021,
	title = {Nonlinear {Acoustic} {Echo} {Cancellation} with {Deep} {Learning}},
	url = {http://arxiv.org/abs/2106.13754},
	abstract = {We propose a nonlinear acoustic echo cancellation system, which aims to model the echo path from the far-end signal to the near-end microphone in two parts. Inspired by the physical behavior of modern hands-free devices, we first introduce a novel neural network architecture that is specifically designed to model the nonlinear distortions these devices induce between receiving and playing the far-end signal. To account for variations between devices, we construct this network with trainable memory length and nonlinear activation functions that are not parameterized in advance, but are rather optimized during the training stage using the training data. Second, the network is succeeded by a standard adaptive linear filter that constantly tracks the echo path between the loudspeaker output and the microphone. During training, the network and filter are jointly optimized to learn the network parameters. This system requires 17 thousand parameters that consume 500 Million floating-point operations per second and 40 Kilo-bytes of memory. It also satisfies hands-free communication timing requirements on a standard neural processor, which renders it adequate for embedding on hands-free communication devices. Using 280 hours of real and synthetic data, experiments show advantageous performance compared to competing methods.},
	urldate = {2022-02-12},
	journal = {arXiv:2106.13754 [cs, eess]},
	author = {Ivry, Amir and Cohen, Israel and Berdugo, Baruch},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.13754},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/YX53XUMW/Ivry et al. - 2021 - Nonlinear Acoustic Echo Cancellation with Deep Lea.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/XCCUMBK2/2106.html:text/html},
}

@article{wang_weighted_2021,
	title = {Weighted {Recursive} {Least} {Square} {Filter} and {Neural} {Network} based {Residual} {Echo} {Suppression} for the {AEC}-{Challenge}},
	url = {http://arxiv.org/abs/2102.08551},
	abstract = {This paper presents a real-time Acoustic Echo Cancellation (AEC) algorithm submitted to the AEC-Challenge. The algorithm consists of three modules: Generalized Cross-Correlation with PHAse Transform (GCC-PHAT) based time delay compensation, weighted Recursive Least Square (wRLS) based linear adaptive filtering and neural network based residual echo suppression. The wRLS filter is derived from a novel semi-blind source separation perspective. The neural network model predicts a Phase-Sensitive Mask (PSM) based on the aligned reference and the linear filter output. The algorithm achieved a mean subjective score of 4.00 and ranked 2nd in the AEC-Challenge.},
	urldate = {2022-02-12},
	journal = {arXiv:2102.08551 [cs, eess]},
	author = {Wang, Ziteng and Na, Yueyue and Liu, Zhang and Tian, Biao and Fu, Qiang},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.08551},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/NQJEV4D3/Wang et al. - 2021 - Weighted Recursive Least Square Filter and Neural .pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/ACTHANKP/2102.html:text/html},
}

@article{westhausen_acoustic_2020,
	title = {Acoustic echo cancellation with the dual-signal transformation {LSTM} network},
	url = {http://arxiv.org/abs/2010.14337},
	abstract = {This paper applies the dual-signal transformation LSTM network (DTLN) to the task of real-time acoustic echo cancellation (AEC). The DTLN combines a short-time Fourier transformation and a learned feature representation in a stacked network approach, which enables robust information processing in the time-frequency and in the time domain, which also includes phase information. The model is only trained on 60 h of real and synthetic echo scenarios. The training setup includes multi-lingual speech, data augmentation, additional noise and reverberation to create a model that should generalize well to a large variety of real-world conditions. The DTLN approach produces state-of-the-art performance on clean and noisy echo conditions reducing acoustic echo and additional noise robustly. The method outperforms the AEC-Challenge baseline by 0.30 in terms of Mean Opinion Score (MOS).},
	language = {en},
	urldate = {2022-02-12},
	journal = {arXiv:2010.14337 [eess]},
	author = {Westhausen, Nils L. and Meyer, Bernd T.},
	month = nov,
	year = {2020},
	note = {arXiv: 2010.14337},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Westhausen and Meyer - 2020 - Acoustic echo cancellation with the dual-signal tr.pdf:/Users/rosscutler/Zotero/storage/P5D2NYAJ/Westhausen and Meyer - 2020 - Acoustic echo cancellation with the dual-signal tr.pdf:application/pdf},
}

@inproceedings{peng_icassp_2021,
	title = {{ICASSP} 2021 {Acoustic} {Echo} {Cancellation} {Challenge}: {Integrated} {Adaptive} {Echo} {Cancellation} with {Time} {Alignment} and {Deep} {Learning}-{Based} {Residual} {Echo} {Plus} {Noise} {Suppression}},
	shorttitle = {{ICASSP} 2021 {Acoustic} {Echo} {Cancellation} {Challenge}},
	doi = {10.1109/ICASSP39728.2021.9414462},
	abstract = {This paper describes a three-stage acoustic echo cancellation (AEC) and suppression framework for the ICASSP 2021 AEC Challenge. In the first stage, a partitioned block frequency domain adaptive filtering is implemented to cancel the linear echo components without introducing the near-end speech distortion, where we compensate the time delay between the far-end reference signal and the micro-phone signal beforehand. In the second stage, a deep complex U-Net integrated with gated recurrent unit is proposed to further suppress the residual echo components. In the last stage, an extremely tiny deep complex U-Net is trained to suppress non-speech residual components that have not been suppressed completely in the second stage, which can also further increase the echo return loss enhancement (ERLE) without increasing the computational complexity dramatically. Experimental results show that the proposed three-stage framework can get the ERLE higher than 50 dB in both single-talk and double-talk scenarios, and perceptual evaluation of speech quality can be improved about 0.75 in double-talk scenarios. The proposed framework outperforms the AEC-Challenge baseline ResRNN by 0.12 points in terms of the MOS.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Peng, Renhua and Cheng, Linjuan and Zheng, Chengshi and Li, Xiaodong},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {acoustic echo cancellation, deep learning, Deep learning, speech enhancement, Noise reduction, Conferences, Delay effects, Echo cancellers, Filtering, Frequency-domain analysis, adaptive filtering, AEC Challenge},
	pages = {146--150},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/CHBQKIYD/9414462.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/PBHPUTPL/Peng et al. - 2021 - ICASSP 2021 Acoustic Echo Cancellation Challenge .pdf:application/pdf},
}

@book{halimeh_combining_2020,
	title = {Combining {Adaptive} {Filtering} and {Complex}-valued {Deep} {Postfiltering} for {Acoustic} {Echo} {Cancellation}},
	abstract = {In this contribution, we introduce a novel approach to noise-robust acoustic echo cancellation employing a complex-valued Deep Neu-ral Network (DNN) for postfiltering. In a first step, early linear echo components are removed using a double-talk robust adaptive filter. The residual signal is subsequently processed by the proposed post-filter (PF). Due to its complex-valued nature, the PF allows to suppress unwanted signal components without introducing distortions to the near-end speaker. For training and evaluation, we exclusively use data from the ICASSP 2021 AEC challenge. Exploiting only a moderate amount of training data, we demonstrate the efficacy of the proposed method. Specifically, we show that the PF (i) benefits significantly from a preceding linear adaptive filter and (ii) significantly outperforms a conventional real-valued DNN-based PF.},
	author = {Halimeh, Modar and Haubner, Thomas and Briegleb, Annika and Schmidt, Alexander and Kellermann, Walter},
	month = oct,
	year = {2020},
	doi = {10.13140/RG.2.2.14083.94241},
}

@article{ivry_deep_2021,
	title = {Deep {Residual} {Echo} {Suppression} with {A} {Tunable} {Tradeoff} {Between} {Signal} {Distortion} and {Echo} {Suppression}},
	url = {http://arxiv.org/abs/2106.13531},
	doi = {10.1109/ICASSP39728.2021.9414958},
	abstract = {In this paper, we propose a residual echo suppression method using a UNet neural network that directly maps the outputs of a linear acoustic echo canceler to the desired signal in the spectral domain. This system embeds a design parameter that allows a tunable tradeoff between the desired-signal distortion and residual echo suppression in double-talk scenarios. The system employs 136 thousand parameters, and requires 1.6 Giga floating-point operations per second and 10 Mega-bytes of memory. The implementation satisfies both the timing requirements of the AEC challenge and the computational and memory limitations of on-device applications. Experiments are conducted with 161{\textasciitilde}h of data from the AEC challenge database and from real independent recordings. We demonstrate the performance of the proposed system in real-life conditions and compare it with two competing methods regarding echo suppression and desired-signal distortion, generalization to various environments, and robustness to high echo levels.},
	urldate = {2022-02-12},
	journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	author = {Ivry, Amir and Cohen, Israel and Berdugo, Baruch},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.13531},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	pages = {126--130},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/V6IQ5Y2H/Ivry et al. - 2021 - Deep Residual Echo Suppression with A Tunable Trad.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/CNL32DJF/2106.html:text/html},
}

@article{valin_low-complexity_2021,
	title = {Low-{Complexity}, {Real}-{Time} {Joint} {Neural} {Echo} {Control} and {Speech} {Enhancement} {Based} {On} {PercepNet}},
	url = {http://arxiv.org/abs/2102.05245},
	abstract = {Speech enhancement algorithms based on deep learning have greatly surpassed their traditional counterparts and are now being considered for the task of removing acoustic echo from hands-free communication systems. This is a challenging problem due to both real-world constraints like loudspeaker non-linearities, and to limited compute capabilities in some communication systems. In this work, we propose a system combining a traditional acoustic echo canceller, and a low-complexity joint residual echo and noise suppressor based on a hybrid signal processing/deep neural network (DSP/DNN) approach. We show that the proposed system outperforms both traditional and other neural approaches, while requiring only 5.5\% CPU for real-time operation. We further show that the system can scale to even lower complexity levels.},
	urldate = {2022-02-12},
	journal = {arXiv:2102.05245 [eess]},
	author = {Valin, Jean-Marc and Tenneti, Srikanth and Helwani, Karim and Isik, Umut and Krishnaswamy, Arvindh},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.05245},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/rosscutler/Zotero/storage/SWZPDBLS/Valin et al. - 2021 - Low-Complexity, Real-Time Joint Neural Echo Contro.pdf:application/pdf;arXiv.org Snapshot:/Users/rosscutler/Zotero/storage/BE8RD9Q8/2102.html:text/html},
}

@article{duvall_exploring_nodate,
	title = {Exploring {Filler} {Words} and {Their} {Impact}},
	language = {en},
	author = {Duvall, Emily and Robbins, Aimee and Graham, Thomas and Divett, Scott},
	pages = {16},
	file = {Duvall et al. - Exploring Filler Words and Their Impact.pdf:/Users/rosscutler/Zotero/storage/6R3EJE37/Duvall et al. - Exploring Filler Words and Their Impact.pdf:application/pdf},
}

@inproceedings{dubey_icassp_2022,
	title = {{ICASSP} 2022 {Deep} {Noise} {Suppression} {Challenge}},
	booktitle = {{ICASSP}},
	author = {Dubey, Harishchandra and Gopal, Vishak and Cutler, Ross and Aazami, Ashkan and Matusevych, Sergiy and Braun, Sebastian and Eskimez, Sefik Emre and Thakker, Manthan and Yoshioka, Takuya and Gamper, Hannes and Aichner, Robert},
	year = {2022},
}

@inproceedings{wang_one-shot_2021,
	address = {Nashville, TN, USA},
	title = {One-{Shot} {Free}-{View} {Neural} {Talking}-{Head} {Synthesis} for {Video} {Conferencing}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578110/},
	doi = {10.1109/CVPR46437.2021.00991},
	abstract = {We propose a neural talking-head video synthesis model and demonstrate its application to video conferencing. Our model learns to synthesize a talking-head video using a source image containing the target person’s appearance and a driving video that dictates the motion in the output. Our motion is encoded based on a novel keypoint representation, where the identity-speciﬁc and motion-related information is decomposed unsupervisedly. Extensive experimental validation shows that our model outperforms competing methods on benchmark datasets. Moreover, our compact keypoint representation enables a video conferencing system that achieves the same visual quality as the commercial H.264 standard while only using one-tenth of the bandwidth. Besides, we show our keypoint representation allows the user to rotate the head during synthesis, which is useful for simulating face-to-face video conferencing experiences.},
	language = {en},
	urldate = {2022-02-20},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Ting-Chun and Mallya, Arun and Liu, Ming-Yu},
	month = jun,
	year = {2021},
	pages = {10034--10044},
	file = {Wang et al. - 2021 - One-Shot Free-View Neural Talking-Head Synthesis f.pdf:/Users/rosscutler/Zotero/storage/ZRZA9KR4/Wang et al. - 2021 - One-Shot Free-View Neural Talking-Head Synthesis f.pdf:application/pdf},
}

@inproceedings{saito_scanimate_2021,
	address = {Nashville, TN, USA},
	title = {{SCANimate}: {Weakly} {Supervised} {Learning} of {Skinned} {Clothed} {Avatar} {Networks}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{SCANimate}},
	url = {https://ieeexplore.ieee.org/document/9578182/},
	doi = {10.1109/CVPR46437.2021.00291},
	language = {en},
	urldate = {2022-02-20},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Saito, Shunsuke and Yang, Jinlong and Ma, Qianli and Black, Michael J.},
	month = jun,
	year = {2021},
	pages = {2885--2896},
	file = {Saito et al. - 2021 - SCANimate Weakly Supervised Learning of Skinned C.pdf:/Users/rosscutler/Zotero/storage/PIQ9YKXI/Saito et al. - 2021 - SCANimate Weakly Supervised Learning of Skinned C.pdf:application/pdf},
}

@inproceedings{ma_scale_2021,
	address = {Nashville, TN, USA},
	title = {{SCALE}: {Modeling} {Clothed} {Humans} with a {Surface} {Codec} of {Articulated} {Local} {Elements}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{SCALE}},
	url = {https://ieeexplore.ieee.org/document/9577479/},
	doi = {10.1109/CVPR46437.2021.01582},
	urldate = {2022-02-20},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ma, Qianli and Saito, Shunsuke and Yang, Jinlong and Tang, Siyu and Black, Michael J.},
	month = jun,
	year = {2021},
	pages = {16077--16088},
}

@inproceedings{richard_audio-_2021,
	address = {Waikoloa, HI, USA},
	title = {Audio- and {Gaze}-driven {Facial} {Animation} of {Codec} {Avatars}},
	isbn = {978-1-66540-477-8},
	url = {https://ieeexplore.ieee.org/document/9423270/},
	doi = {10.1109/WACV48630.2021.00009},
	language = {en},
	urldate = {2022-02-20},
	booktitle = {2021 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Richard, Alexander and Lea, Colin and Ma, Shugao and Gall, Juergen and la Torre, Fernando de and Sheikh, Yaser},
	month = jan,
	year = {2021},
	pages = {41--50},
	file = {Richard et al. - 2021 - Audio- and Gaze-driven Facial Animation of Codec A.pdf:/Users/rosscutler/Zotero/storage/NZ3F2PJ9/Richard et al. - 2021 - Audio- and Gaze-driven Facial Animation of Codec A.pdf:application/pdf},
}

@inproceedings{ma_pixel_2021,
	address = {Nashville, TN, USA},
	title = {Pixel {Codec} {Avatars}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577690/},
	doi = {10.1109/CVPR46437.2021.00013},
	abstract = {Telecommunication with photorealistic avatars in virtual or augmented reality is a promising path for achieving authentic face-to-face communication in 3D over remote physical distances. In this work, we present the Pixel Codec Avatars (PiCA): a deep generative model of 3D human faces that achieves state of the art reconstruction performance while being computationally efﬁcient and adaptive to the rendering conditions during execution. Our model combines two core ideas: (1) a fully convolutional architecture for decoding spatially varying features, and (2) a renderingadaptive per-pixel decoder. Both techniques are integrated via a dense surface representation that is learned in a weakly-supervised manner from low-topology mesh tracking over training images. We demonstrate that PiCA improves reconstruction over existing techniques across testing expressions and views on persons of different gender and skin tone. Importantly, we show that the PiCA model is much smaller than the state-of-art baseline model, and makes multi-person telecommunicaiton possible: on a single Oculus Quest 2 mobile VR headset, 5 avatars are rendered in realtime in the same scene.},
	language = {en},
	urldate = {2022-02-20},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ma, Shugao and Simon, Tomas and Saragih, Jason and Wang, Dawei and Li, Yuecheng and La Torre, Fernando De and Sheikh, Yaser},
	month = jun,
	year = {2021},
	pages = {64--73},
	file = {Ma et al. - 2021 - Pixel Codec Avatars.pdf:/Users/rosscutler/Zotero/storage/5ZEN3LDC/Ma et al. - 2021 - Pixel Codec Avatars.pdf:application/pdf},
}

@article{xiang_modeling_2021,
	title = {Modeling clothing as a separate layer for an animatable human avatar},
	volume = {40},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3478513.3480545},
	doi = {10.1145/3478513.3480545},
	language = {en},
	number = {6},
	urldate = {2022-02-20},
	journal = {ACM Transactions on Graphics},
	author = {Xiang, Donglai and Prada, Fabian and Bagautdinov, Timur and Xu, Weipeng and Dong, Yuan and Wen, He and Hodgins, Jessica and Wu, Chenglei},
	month = dec,
	year = {2021},
	pages = {1--15},
	file = {Xiang et al. - 2021 - Modeling clothing as a separate layer for an anima.pdf:/Users/rosscutler/Zotero/storage/LNKHB3JY/Xiang et al. - 2021 - Modeling clothing as a separate layer for an anima.pdf:application/pdf},
}

@inproceedings{gamper_blind_2018,
	title = {Blind {Reverberation} {Time} {Estimation} {Using} a {Convolutional} {Neural} {Network}},
	doi = {10.1109/IWAENC.2018.8521241},
	abstract = {The reverberation time of an acoustic environment is a useful parameter for applications including source localisation, speech recognition and mixed reality. However, estimating the reverberation time blindly and on the fly remains a challenge. Here we propose formulating the estimation as a regression problem and using a convolutional neural network (CNN) to estimate the reverberation time directly from a four second long single-channel recording of reverberant speech in noise. Evaluation on the ACE Challenge data corpus suggests that the proposed method is computationally efficient and outperforms state-of-the-art methods.},
	booktitle = {2018 16th {International} {Workshop} on {Acoustic} {Signal} {Enhancement} ({IWAENC})},
	author = {Gamper, Hannes and Tashev, Ivan J.},
	month = sep,
	year = {2018},
	keywords = {Neural networks, Training, Estimation, Noise measurement, deep neural networks, energy decay rate, Reverberation, T60, Training data},
	pages = {136--140},
	file = {IEEE Xplore Abstract Record:/Users/rosscutler/Zotero/storage/5ZGKLJEZ/8521241.html:text/html;IEEE Xplore Full Text PDF:/Users/rosscutler/Zotero/storage/LPGCVM6C/Gamper and Tashev - 2018 - Blind Reverberation Time Estimation Using a Convol.pdf:application/pdf},
}

@inproceedings{cutler_icassp_2022,
	title = {{ICASSP} 2022 {Acoustic} {Echo} {Cancellation} {Challenge}},
	abstract = {The ICASSP 2022 Acoustic Echo Cancellation Challenge is intended to stimulate research in acoustic echo cancellation (AEC), which is an important area of speech enhancement and still a top issue in audio communication. This is the third AEC challenge and it is enhanced by including mobile scenarios, adding speech recognition word accuracy rate as a metric, and making the audio 48 kHz. We open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 10,000 real audio devices and human speakers in real environments, as well as a synthetic dataset. We open source an online subjective test framework and provide an online objective metric service for researchers to quickly test their results. The winners of this challenge were selected based on the average Mean Opinion Score (MOS) achieved across all scenarios and the word accuracy rate.},
	language = {en},
	booktitle = {{ICASSP}},
	author = {Cutler, Ross and Saabas, Ando and Parnamaa, Tanel and Purin, Marju and Gamper, Hannes and Braun, Sebastian and Aichner, Robert},
	year = {2022},
	file = {Cutler et al. - ICASSP 2022 ACOUSTIC ECHO CANCELLATION CHALLENGE.pdf:/Users/rosscutler/Zotero/storage/9UUKDADV/Cutler et al. - ICASSP 2022 ACOUSTIC ECHO CANCELLATION CHALLENGE.pdf:application/pdf},
}
