\section{Transformer Models for EHR}
\label{sec:method}
In this section, we describe how EHR data can be modeled as medical event sequences and our usage of transformer models for learning from these sequences. 

% Present how hospitalizations as seen as a sequence of event pertaining to a patient
\subsection{Hospitalizations as Event Sequences}\label{subsec:event_sequences}
Patient hospitalizations can be naturally modeled as sequences of medical procedures for determining, measuring, or diagnosing the patient's condition. Other medical procedures are therapeutic and intend to treat or cure the patient. To standardize how medical procedures are described, medical facilities code procedure concepts using accepted medical taxonomies often used in ML applications~\cite{HansenSHLLS22}, such as the Anatomical Therapeutic Classification (ATC)~\cite{ronning2002historical} for medication administration and the International Classification of Diseases and Related Health Problems (ICD)~\cite{cartwright2013icd} for condition diagnosis. Hence, a patient hospitalization can be described as the sequence of concept tokens detailing medical procedures pertaining to a patient coded using medical concepts from accepted medical taxonomies. An example patient hospitalization sequence is illustrated in Figure \ref{fig:patient_sequence}.       

Furthermore, as a patient's medical history is crucial for correct management and treatment, we pre-pend the patient's medical history to the hospitalization sequence as a tokenized vector as illustrated in Figure~\ref{fig:patient_sequence}. The vector consists of 38 tokens describing the patient's medical history, including comorbidities from the Charlson Index~\cite{sundararajan2004new}, five years of medications prescription history grouped by the first level of the ATC hierarchy~\cite{ronning2002historical}, and the mode, time, and initial triage category~\cite{wireklint2021updated} of hospitalization. The historical information included in the vector is summarized in Table~\ref{tab:medical_history}.    

\begin{figure}[tb]
    \centering
    \includegraphics[trim={1cm 11.5cm 8cm 0cm}, width=0.9\linewidth]{figures/patient_sequence.pdf}
    \caption{Illustration of a patient event sequence. Starting with the patient's medical history as summarized in Table \ref{tab:medical_history}, the patient is initially diagnosed with the ICD-10 code \emph{Z039}. Subsequent vital measurements and laboratory tests are performed to monitor the patient's state and determine the patient's underlying condition. Consequently, the patient is diagnosed with acute cystitis without hematuria (ICD-10 code \emph{N300}), and antibiotic treatment is initiated with nitrofurantoin (ATC code \emph{J01XE01}). After additional procedure and therapeutic medical events, the patient is released from the hospital.} 
    \label{fig:patient_sequence}
\end{figure}

\begin{wraptable}{r}{5.5cm}
\vspace{-2em}
    \centering
    \caption{Patient medical history.}
    \label{tab:medical_history}
	\begin{tabular}{p{3.5cm}p{1.7cm}}
        \textbf{Data}  & \textbf{\#Tokens} \\
        \hline
        Comorbidities & 18   \\
        Prescription history & 14  \\
        Mode, time, \& triage &  6    
	\end{tabular}
    \vspace{-2em}
\end{wraptable}

% Present how we integrate measurement values into the task
\subsection{Hospital Measurement Events}\label{subsec:event_measurements}

For some medical procedures, such as vital measurements and laboratory tests, a numerical measurement value accompanies the procedure. While other works in transformer models for EHR data disregard the numerical values of measurements~\cite{meng2021bidirectional,rasmy2021med,li2020behrt}, we instead integrate this information as part of the patient input sequences because numerical measurement values add important information regarding the state of a patient. For example, the knowledge that a temperature measurement was performed is naturally important information. Still, from the measurement value of $40.1^{\circ}C$, we learn that the patient has a fever. Using patient-specific threshold values for measurements based on age, gender, and pregnancy status, we map measurement values into tokens representing either normal, abnormal-low, or abnormal-high findings. For example, given that we measure an albumin level of $56$ g/L for a $31$-year-old male patient, we would create the token \emph{albumin-high} to reflect that the value of the measurement was above what is expected ($36$-$48$ g/L) for a patient with the given demography.  

%\subsection{Comparison Between Text and EHR.}
%Even though text and EHR data can be seen as sequences of tokens, they exhibit separate characteristics. Whereas time intervals between words in sentences are constant, the time between EHR concepts 

\begin{figure}[bt]
    \centering
    \includegraphics[trim={1.7cm 12cm 11.7cm 0cm}, width=0.9\linewidth]{figures/sequence_model.pdf}
    \caption{Medical event sequence pre-pended with the patient's medical history.} 
    \label{fig:sequence_model}
\end{figure}

% How we aim to use machine learning with transformer models to solve the problem
\vspace{-1em}
\subsection{Transformer Models for EHR Data}
To investigate the challenge of LOS, we examine a modified version, henceforth termed Medic-BERT (M-BERT), of the Bidirectional Encoder Representations from Transformers (BERT)~\cite{bert19} model for EHR data. BERT is an NLP model based on a stack of encoder layers from the transformer architecture introduced by Vaswani et al.~\cite{vaswani2017attention}. We argue that sequence models, such as BERT exhibit properties beneficial for solving medical tasks based on EHR data. The transformer encoder naturally handles the complex long-term dependencies that occur between medical concepts through its utilization of multi-head self-attention. BERT can naturally integrate disparate modalities, such as diagnostic end therapeutic events, as each event is encoded as an n-dimensional vector token. Furthermore, BERT naturally operates in domains with irregular intervals between events, as is the case with EHR data. We, therefore, investigate our modified version of BERT for LOS prediction for patient event sequences. The M-BERT architecture is illustrated in Figure \ref{fig:sequence_model}.

M-BERT learns an embedding for each medical event token while trained toward LOS prediction. The position embedding enables the model to learn from the temporal dependencies within a sequence. We use a static position embedding as described in Waswani et al.~\cite{vaswani2017attention}; however, modified for the usage on medical event sequences as described in Section~\ref{subsec:position_embedding}. As patient demographics are a vital part of any medical prediction model, we pay special attention to this information by adding a trainable age and sex embedding at each medical event~\cite{li2020behrt}. Furthermore, as in the original BERT model, we use the special classification (CLS) token as a final aggregate representation of the sequence. Hence, as illustrated in Figure~\ref{fig:sequence_model}, the CLS representation is fed to a linear output layer for LOS classification and regression tasks.          

\subsection{Position Embedding}\label{subsec:position_embedding}
Due to the nature of patient care and hospital administration, some measurement events tend to chunk together. Clinicians will often conduct various measurements, such as blood pressure, temperature, and heart rate, over a short time and later persist the information into the patient's EHR. Hence, we are sometimes prevented from knowing the specific times and order of medical events. This effect is most frequent for vital measurements and laboratory tests. Multiple laboratory tests are often conducted on the same patient sample, such as a single blood sample, making it impossible to know the chronological order for such measurement events. An example sequence of patient events chunking together is illustrated in Figure \ref{fig:event_chunks}. To enable the model to understand that these events have no fixed ordering, we assign the same position embedding to events co-located in time as illustrated in the position embedding of Figure \ref{fig:sequence_model}. For example, the $L_1$ and $L_2$ events are both mapped to the $E_{P_3}$ position embedding.

\begin{figure}[tb]
    \centering
    \includegraphics[trim={1cm 14cm 14cm 0cm}, width=0.9\linewidth]{figures/event_chunks.pdf}
    \caption{Patient event sequence illustrating events grouping together.} 
    \label{fig:event_chunks}
\end{figure}
