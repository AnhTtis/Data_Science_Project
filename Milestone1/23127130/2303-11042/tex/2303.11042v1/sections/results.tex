
\section{Empirical Evaluation and Results}\label{sec:eval}
In this section, we explain the experimental settings and results of the empirical evaluation.

\subsection{Experimental Setting}
% Pre-train of bert
Transformer models are trained, as a rule, using unsupervised pre-training for learning general token embeddings, followed by supervised fine-tuning targeting a specific downstream task. As we are only focusing on a single prediction task, we directly train model parameters and token embeddings toward the downstream task of LOS prediction without pre-training. The experimental code is available online\footnote{\label{footnote:online_appendix}\url{https://github.com/dkw-aau/medic\_transformer}}.
%we directly train the model  For the original BERT~\cite{bert19} model, pre-training is done using Masked Label Modeling (MLM) for learning latent representations of event tokens and model parameters. Subsequently, pre-trained token embeddings and model parameters can be used as a basis for model fine-tuning on specific downstream tasks such as sentence classification and question answering. Although it could be interesting to investigate MLM as a way of learning general-purpose embeddings for EHR sequences, currently, we omit the pre-training stage and directly train model parameters and token embeddings toward the single downstream task of LOS prediction. 

% Describe the experimental settings
%Following related work in LOS prediction, 
We use the admission data gathered within the first 24 hours of admissions for LOS predictions. Hence, we remove sequence events happening after 24 hours of admission. We evaluate our approach on three LOS experiments of increasing complexity. The first two experiments are a \textbf{Binary} classification of LOS $> 2$ days, and a three-class \textbf{Category} task 
%$<2$, $2-7$, and $>7$ days of admission
of LOS $> 2$, $2 \leq$ LOS $\leq 7$, and LOS $> 7$ days 
with class balances as illustrated in Figure \ref{fig:los_cat}. The last experiment, termed \textbf{Real}, is a regression task with the objective of predicting the LOS as the real number, with a histogram of admission times as illustrated in Figure~\ref{fig:los_hist}.

% Describe the methods we benchmark against
We evaluate our approach against three ML models: RF, ANN, and SVM. We use implementations of the models from the Sklearn library~\cite{scikit-learn} with standard model hyperparameters. In preparing samples for these models, we use the latest measured value for each event type (within 24 hours of admission) as input features~\cite{iwase2022prediction}. Subsequently, the mean of variables is used for imputation of missing data values, and variables are scaled to values between 0 and 1. Lastly, a chi$^2$ test is used in feature selection for the selection of the 50 most relevant features.     

% Describe our model parameters 
Our model is trained on a random split distributed as 80/10/10 of all patient samples for training, validation, and testing. We use the loss of the evaluation data for early stopping training if the loss does not go down within ten training epochs. The model architecture has six hidden layers with an intermediate layer size of 288, eight attention heads, and input token embeddings have a size of 288. We truncate sequences to 256 tokens, as most sequences adhere to this limit. To counter overfitting, we add a dropout layer with a probability of 10\% after the output of the final encoder layer, attention dropout at every layer with a probability of 10\%, and weight decay of $0,003$. Furthermore, experiments were performed with a learning rate of 1e-5.
 
%We use MLM as in the original work on the BERT paper~\cite{bert19}. In our training scheme randomly select 15\% of the event codes for modification. For each randomly chosen code, 85\% of the time we replace the token with a [MASK] token, and 15\% of the time we replace it with a random code from the vocabulary. The task is now to try and predict the correct label for the masked labels. The reason for replacing 15\% of the tokens with random codes is to mitigate overfitting by not allowing the model to learn the exact contextual sequences for the training data, and can thus be seen as a way of introducing noise into the model.
%Explain how much data we use for pre-training and how we stop pre-training using accuracy as in \cite{meng2021bidirectional}.


%How we split data into three groups: Train and validataion from the year 2014-2020 and test data is from the year 2021. This way we do not learn from future patient events. 

\begin{figure}[tb]
  \centering
  \subfloat[AUROC curves for the \textbf{Binary} LOS experiment over all methods.]{\includegraphics[width=0.47\textwidth]{figures/roc_binary.pdf}\label{fig:roc_all}}
  \hfill
  \subfloat[\textbf{Binary} experiment AUROC curves for different age groups over M-BERT.]{\includegraphics[width=0.47\textwidth]{figures/roc_binary_ages.pdf}\label{fig:roc_ages}}
  \caption{AUROC plots for experimental results on the binary prediction task.}
  \label{fig:auroc_plots}
\end{figure}

\vspace{-1em}
\subsection{Results}
% Make sure to tell that the important metrics are the Binary and Categorical experiments for a clinical setting. 

Table \ref{tab:results} presents the Area Under the Receiver Operating Characteristics \mbox{(AUROC)} and harmonic mean of precision and recall (F1) values for the \textbf{Binary} and \textbf{Category} experimental settings and Mean Absolute Error (MAE) and Mean Squared Error (MSE) for the LOS regression task. Furthermore, Figure~\ref{fig:roc_all} presents the AUROC curves for the \textbf{Binary} experimental setting. M-BERT outperforms the traditional ML techniques in all experimental settings. The results indicate that transformer models may be able to leverage the temporal dependencies inherent to patient EHR data for increased predictive accuracy. 
\begin{wraptable}{r}{9.2cm}
\caption{Experimental results.}
    \label{tab:results}
    \begin{tabular}{lccccccccc}
    \hline
    \textbf{} & \multicolumn{2}{c}{\textbf{Binary}}  & \textbf{} & \multicolumn{2}{c}{\textbf{Category}} & \textbf{} & \multicolumn{2}{c}{\textbf{Real}}     \\ 
    \cline{2-3} \cline{5-6} \cline{8-9}
    \textbf{} & \textbf{AUROC} & \textbf{F1} &           & \textbf{AUROC} & \textbf{F1}  &           & \textbf{MAE} & \textbf{MSE} \\ 
    \hline
    RF        & $0.72$ & $0.70$ &    & $0.66$ & $0.45$ &    & $4.18$ & $39.08$ & \\ 
    ANN       & $0.67$ & $0.68$ &    & $0.63$ & $0.43$ &    & $4.09$ & $38.10$ & \\
    SVM       & $0.70$ & $0.70$ &    & $0.65$ & $0.38$ &    & $3.56$ & $43.36$ & \\
    \hline
    M-BERT  & \textbf{0.78} & \textbf{0.77} &    & \textbf{0.74} & \textbf{0.54} &    & \textbf{3.42} & \textbf{37.48} & \\ 
    \hline
    \end{tabular}
\end{wraptable}
Furthermore, being a transformer-based model, M-BERT overcomes the challenge of missing data and imputation, as patient sequences are not required to contain the same medical events, nor the same sequence length. 
Figure~\ref{fig:roc_ages} presents the AUROC for binary LOS prediction stratified by age groups for those with more than 100 patient samples. Interestingly, the model performance is stable for different age groups, with an AUROC of 0.78 for the age groups 60-70 and 70-80 years.
Furthermore, stratification based on the sex of patients yielded similar results, pointing to the robustness of M-BERT for LOS classification.
% Tell the reader again that this model does not require imputation
% Furthermore, modeling EHR data as sequences of medical events overcomes the challenge of missing data.


