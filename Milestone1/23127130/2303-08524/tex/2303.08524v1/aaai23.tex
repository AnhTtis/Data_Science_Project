%File: formatting-instructions-latex-2023.tex
%release 2023.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai23}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\let\Bbbk\relax
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{color}
\usepackage{url}
% \renewcommand\footnotetextcopyrightpermission[1]{} % removes 
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2023.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai23.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% % directly follow a colon or long dash
% \title{CoordFill: Efficient High-Resolution Image Inpainting via \\ Parameterized Coordinate Querying}
% \author{
%     %Authors
%     % All authors must be in the same font size and format.
%     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
%     AAAI Style Contributions by Pater Patel Schneider,
%     Sunil Issar,\\
%     J. Scott Penberthy,
%     George Ferguson,
%     Hans Guesgen,
%     Francisco Cruz\equalcontrib,
%     Marc Pujol-Gonzalez\equalcontrib
% }
% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar, \textsuperscript{\rm 2}
%     % J. Scott Penberthy, \textsuperscript{\rm 3}
%     % George Ferguson,\textsuperscript{\rm 4}
%     % Hans Guesgen, \textsuperscript{\rm 5}.
%     % Note that the comma should be placed BEFORE the superscript for optimum readability

%     1900 Embarcadero Road, Suite 101\\
%     Palo Alto, California 94303-3310 USA\\
%     % email address must be in roman text type, not monospace or sans serif
%     publications23@aaai.org
% %
% See more examples next
% }
\newcommand{\xiaodong}[1]{{\color{blue}{[xiaodong: #1]}}}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

% \iffalse
% %Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
% \author {
%     % Authors
%     First Author Name,\textsuperscript{\rm 1}
%     Second Author Name, \textsuperscript{\rm 2}
%     Third Author Name \textsuperscript{\rm 1}
% }
% \affiliations {
%     % Affiliations
%     \textsuperscript{\rm 1} Affiliation 1\\
%     \textsuperscript{\rm 2} Affiliation 2\\
%     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
% }
% \fi

% \iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{CoordFill: Efficient High-Resolution Image Inpainting via \\ Parameterized Coordinate Querying}
\author {
    % Authors
    Weihuang Liu,\textsuperscript{\rm 1}
    Xiaodong Cun,\textsuperscript{\rm 2}$^{*}$
    Chi-Man Pun,\textsuperscript{\rm 1}$^{*}$
    Menghan Xia,\textsuperscript{\rm 2}
    Yong Zhang,\textsuperscript{\rm 2}
    Jue Wang\textsuperscript{\rm 2}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} University of Macau,
    \textsuperscript{\rm 2} Tencent AI Lab\\
    mc05379@umac.mo, 
    vinthony@gmail.com, 
    cmpun@umac.mo,\\
    menghanxyz@gmail.com,
    zhangyong201303@gmail.com,
    arphid@gmail.com
}

% \fi
% \cortext[mycorrespondingauthor]{Corresponding author}

% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

% \maketitle
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \centering
    \vspace{-2em}
    \includegraphics[width=\textwidth,height=5cm]{images/teaser.pdf}
    \vspace{-2em}
    \captionof{figure}{The proposed method only requires \underline{\textit{24ms}}~(25$\times$ faster than state-of-the-art method LaMa~\cite{lama} in the same resolution) to remove the red masked regions on the resolution of a 2048$\times$2048 image. Compared with most of the previous methods that synthesize the full image $Y$ by convolutional neural networks, CoordFill queries the coordinates $(x,y)$ in the missing mask $M$ of the input $X$ only and generates the pixel-value by the implicit representation. Thanks to the proposed decoder, CoordFill runs faster than previous methods on the high-resolution image inpainting task.  }
\end{center}%
}]

% \begin{figure*}
% \centering
% \includegraphics[width=\textwidth]{images/coord_fill_teaster.pdf}
% \vspace{-1em}
% \caption{}
% \end{figure*}

% \newenvironment{alphafootnotes}
%   {\par\edef\savedfootnotenumber{\number\value{footnote}}
%   \renewcommand{\thefootnote}{\alph{footnote}}
%   \setcounter{footnote}{0}}
%   {\par\setcounter{footnote}{\savedfootnotenumber}}

\newenvironment{alphafootnotes}
  {\par\edef\savedfootnotenumber{\number\value{footnote}}
  \renewcommand{\thefootnote}{\alph{footnote}}
  \setcounter{footnote}{0}}
  {\par\setcounter{footnote}{\savedfootnotenumber}}

\begin{alphafootnotes}
\let\thefootnote\relax\footnotetext{* Corresponding author}
\let\thefootnote\relax\footnotetext{Copyright \copyright\space 2023,
Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.}
\end{alphafootnotes}



\input{submit/abs}


\input{submit/intro}
\input{submit/related}
\input{submit/method}
\input{submit/exp}
\input{submit/conclusion}
\input{submit/acknowledgments}


% \bibliography{submit/egbib}
\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Barnes et~al.(2009)Barnes, Shechtman, Finkelstein, and
  Goldman}]{patchmatch}
Barnes, C.; Shechtman, E.; Finkelstein, A.; and Goldman, D.~B. 2009.
\newblock {PatchMatch}: A Randomized Correspondence Algorithm for Structural
  Image Editing.
\newblock \emph{ACM Transactions on Graphics (Proc. SIGGRAPH)}, 28(3).

\bibitem[{Chen, Liu, and Wang(2021)}]{llif}
Chen, Y.; Liu, S.; and Wang, X. 2021.
\newblock Learning continuous image representation with local implicit image
  function.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 8628--8638.

\bibitem[{Chi, Jiang, and Mu(2020)}]{ffc}
Chi, L.; Jiang, B.; and Mu, Y. 2020.
\newblock Fast fourier convolution.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:
  4479--4488.

\bibitem[{Dong, Cao, and Fu(2022)}]{zits}
Dong, Q.; Cao, C.; and Fu, Y. 2022.
\newblock Incremental Transformer Structure Enhanced Image Inpainting with
  Masking Positional Encoding.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}.

\bibitem[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly et~al.}]{vit}
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.;
  Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et~al.
  2020.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}.

\bibitem[{Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio}]{gan}
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair,
  S.; Courville, A.; and Bengio, Y. 2014.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27.

\bibitem[{Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam}]{mobilenet}
Howard, A.~G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang, W.; Weyand, T.;
  Andreetto, M.; and Adam, H. 2017.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}.

\bibitem[{Iandola et~al.(2016)Iandola, Han, Moskewicz, Ashraf, Dally, and
  Keutzer}]{squeezenet}
Iandola, F.~N.; Han, S.; Moskewicz, M.~W.; Ashraf, K.; Dally, W.~J.; and
  Keutzer, K. 2016.
\newblock SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5
  MB model size.
\newblock \emph{arXiv preprint arXiv:1602.07360}.

\bibitem[{Iizuka, Simo-Serra, and Ishikawa(2017)}]{global_and_local}
Iizuka, S.; Simo-Serra, E.; and Ishikawa, H. 2017.
\newblock Globally and locally consistent image completion.
\newblock \emph{ACM Transactions on Graphics (ToG)}, 36(4): 1--14.

\bibitem[{Isola et~al.(2017)Isola, Zhu, Zhou, and Efros}]{pix2pix}
Isola, P.; Zhu, J.-Y.; Zhou, T.; and Efros, A.~A. 2017.
\newblock Image-to-image translation with conditional adversarial networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 1125--1134.

\bibitem[{Jo, Yang, and Kim(2020)}]{jo2020investigating}
Jo, Y.; Yang, S.; and Kim, S.~J. 2020.
\newblock Investigating loss functions for extreme super-resolution.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition workshops}, 424--425.

\bibitem[{Karras et~al.(2018)Karras, Aila, Laine, and Lehtinen}]{progan}
Karras, T.; Aila, T.; Laine, S.; and Lehtinen, J. 2018.
\newblock Progressive Growing of GANs for Improved Quality, Stability, and
  Variation.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Karras et~al.(2021)Karras, Aittala, Laine, H{\"a}rk{\"o}nen,
  Hellsten, Lehtinen, and Aila}]{styleganv3}
Karras, T.; Aittala, M.; Laine, S.; H{\"a}rk{\"o}nen, E.; Hellsten, J.;
  Lehtinen, J.; and Aila, T. 2021.
\newblock Alias-free generative adversarial networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34.

\bibitem[{Li et~al.(2020)Li, Lin, Ding, Liu, Zhu, and Han}]{gancompression}
Li, M.; Lin, J.; Ding, Y.; Liu, Z.; Zhu, J.-Y.; and Han, S. 2020.
\newblock GAN Compression: Efficient Architectures for Interactive Conditional
  GANs.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}.

\bibitem[{Li et~al.(2022)Li, Lin, Zhou, Qi, Wang, and Jia}]{mat}
Li, W.; Lin, Z.; Zhou, K.; Qi, L.; Wang, Y.; and Jia, J. 2022.
\newblock MAT: Mask-Aware Transformer for Large Hole Image Inpainting.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}.

\bibitem[{Liang, Cun, and Pun(2021)}]{s2crnet}
Liang, J.; Cun, X.; and Pun, C.-M. 2021.
\newblock Spatial-Separated Curve Rendering Network for Efficient and
  High-Resolution Image Harmonization.
\newblock \emph{arXiv preprint arXiv:2109.05750}.

\bibitem[{Liao et~al.(2021)Liao, Xiao, Wang, Lin, and Satoh}]{semantic_guide}
Liao, L.; Xiao, J.; Wang, Z.; Lin, C.-W.; and Satoh, S. 2021.
\newblock Image inpainting guided by coherence priors of semantics and
  textures.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 6539--6548.

\bibitem[{Lin et~al.(2021)Lin, Lee, Cheng, Tulyakov, and Yang}]{infinitygan}
Lin, C.~H.; Lee, H.-Y.; Cheng, Y.-C.; Tulyakov, S.; and Yang, M.-H. 2021.
\newblock InfinityGAN: Towards Infinite-Pixel Image Synthesis.
\newblock \emph{arXiv preprint arXiv:2104.03963}.

\bibitem[{Liu et~al.(2018)Liu, Reda, Shih, Wang, Tao, and Catanzaro}]{pconv}
Liu, G.; Reda, F.~A.; Shih, K.~J.; Wang, T.-C.; Tao, A.; and Catanzaro, B.
  2018.
\newblock Image inpainting for irregular holes using partial convolutions.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, 85--100.

\bibitem[{Liu et~al.(2020)Liu, Jiang, Song, Huang, and
  Yang}]{liu2020rethinking}
Liu, H.; Jiang, B.; Song, Y.; Huang, W.; and Yang, C. 2020.
\newblock Rethinking image inpainting via a mutual encoder-decoder with feature
  equalizations.
\newblock In \emph{European Conference on Computer Vision}, 725--741. Springer.

\bibitem[{Mildenhall et~al.(2020)Mildenhall, Srinivasan, Tancik, Barron,
  Ramamoorthi, and Ng}]{nerf}
Mildenhall, B.; Srinivasan, P.~P.; Tancik, M.; Barron, J.~T.; Ramamoorthi, R.;
  and Ng, R. 2020.
\newblock Nerf: Representing scenes as neural radiance fields for view
  synthesis.
\newblock In \emph{European conference on computer vision}, 405--421. Springer.

\bibitem[{Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida}]{spectralnorm}
Miyato, T.; Kataoka, T.; Koyama, M.; and Yoshida, Y. 2018.
\newblock Spectral normalization for generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1802.05957}.

\bibitem[{Nazeri et~al.(2019)Nazeri, Ng, Joseph, Qureshi, and
  Ebrahimi}]{edgeconnect}
Nazeri, K.; Ng, E.; Joseph, T.; Qureshi, F.~Z.; and Ebrahimi, M. 2019.
\newblock Edgeconnect: Generative image inpainting with adversarial edge
  learning.
\newblock \emph{arXiv preprint arXiv:1901.00212}.

\bibitem[{Saito et~al.(2019)Saito, Huang, Natsume, Morishima, Kanazawa, and
  Li}]{pifu}
Saito, S.; Huang, Z.; Natsume, R.; Morishima, S.; Kanazawa, A.; and Li, H.
  2019.
\newblock Pifu: Pixel-aligned implicit function for high-resolution clothed
  human digitization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, 2304--2314.

\bibitem[{Saito et~al.(2020)Saito, Simon, Saragih, and Joo}]{pifuhd}
Saito, S.; Simon, T.; Saragih, J.; and Joo, H. 2020.
\newblock Pifuhd: Multi-level pixel-aligned implicit function for
  high-resolution 3d human digitization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 84--93.

\bibitem[{Shaham et~al.(2021)Shaham, Gharbi, Zhang, Shechtman, and
  Michaeli}]{asapnet}
Shaham, T.~R.; Gharbi, M.; Zhang, R.; Shechtman, E.; and Michaeli, T. 2021.
\newblock Spatially-adaptive pixelwise networks for fast image translation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 14882--14891.

\bibitem[{Sitzmann et~al.(2020)Sitzmann, Martel, Bergman, Lindell, and
  Wetzstein}]{siren}
Sitzmann, V.; Martel, J.; Bergman, A.; Lindell, D.; and Wetzstein, G. 2020.
\newblock Implicit neural representations with periodic activation functions.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:
  7462--7473.

\bibitem[{Suvorov et~al.(2022)Suvorov, Logacheva, Mashikhin, Remizova, Ashukha,
  Silvestrov, Kong, Goka, Park, and Lempitsky}]{lama}
Suvorov, R.; Logacheva, E.; Mashikhin, A.; Remizova, A.; Ashukha, A.;
  Silvestrov, A.; Kong, N.; Goka, H.; Park, K.; and Lempitsky, V. 2022.
\newblock Resolution-robust Large Mask Inpainting with Fourier Convolutions.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, 2149--2159.

\bibitem[{{Unsplash}(2021)}]{unsplash}
{Unsplash}. 2021.
\newblock Unsplash Lite Dataset 1.2.0.
\newblock [Online; accessed 4-March-2021].

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{transformer}
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.~N.;
  Kaiser, {\L}.; and Polosukhin, I. 2017.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30.

\bibitem[{Vo, Duong, and P{\'e}rez(2018)}]{structural_inpainting}
Vo, H.~V.; Duong, N.~Q.; and P{\'e}rez, P. 2018.
\newblock Structural inpainting.
\newblock In \emph{Proceedings of the 26th ACM international conference on
  Multimedia}, 1948--1956.

\bibitem[{Wan et~al.(2021)Wan, Zhang, Chen, and Liao}]{Wan_2021_ICCV}
Wan, Z.; Zhang, J.; Chen, D.; and Liao, J. 2021.
\newblock High-Fidelity Pluralistic Image Completion With Transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, 4692--4701.

\bibitem[{Wang et~al.(2018)Wang, Liu, Zhu, Tao, Kautz, and
  Catanzaro}]{pix2pixhd}
Wang, T.-C.; Liu, M.-Y.; Zhu, J.-Y.; Tao, A.; Kautz, J.; and Catanzaro, B.
  2018.
\newblock High-Resolution Image Synthesis and Semantic Manipulation with
  Conditional GANs.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}.

\bibitem[{Yi et~al.(2020)Yi, Tang, Azizi, Jang, and Xu}]{hifill}
Yi, Z.; Tang, Q.; Azizi, S.; Jang, D.; and Xu, Z. 2020.
\newblock Contextual residual aggregation for ultra high-resolution image
  inpainting.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 7508--7517.

\bibitem[{Yu et~al.(2018)Yu, Lin, Yang, Shen, Lu, and Huang}]{deepfill}
Yu, J.; Lin, Z.; Yang, J.; Shen, X.; Lu, X.; and Huang, T.~S. 2018.
\newblock Generative image inpainting with contextual attention.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 5505--5514.

\bibitem[{Yu et~al.(2019)Yu, Lin, Yang, Shen, Lu, and Huang}]{deepfillv2}
Yu, J.; Lin, Z.; Yang, J.; Shen, X.; Lu, X.; and Huang, T.~S. 2019.
\newblock Free-form image inpainting with gated convolution.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, 4471--4480.

\bibitem[{Yu et~al.(2020)Yu, Guo, Jin, Wu, Chen, Li, Zhang, and Liu}]{rn}
Yu, T.; Guo, Z.; Jin, X.; Wu, S.; Chen, Z.; Li, W.; Zhang, Z.; and Liu, S.
  2020.
\newblock Region normalization for image inpainting.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, 12733--12740.

\bibitem[{Zeng et~al.(2020{\natexlab{a}})Zeng, Cai, Li, Cao, and Zhang}]{3dlut}
Zeng, H.; Cai, J.; Li, L.; Cao, Z.; and Zhang, L. 2020{\natexlab{a}}.
\newblock Learning Image-adaptive 3D Lookup Tables for High Performance Photo
  Enhancement in Real-time.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}.

\bibitem[{Zeng et~al.(2022)Zeng, Fu, Chao, and Guo}]{aotgan}
Zeng, Y.; Fu, J.; Chao, H.; and Guo, B. 2022.
\newblock Aggregated contextual transformations for high-resolution image
  inpainting.
\newblock \emph{IEEE Transactions on Visualization and Computer Graphics}.

\bibitem[{Zeng et~al.(2021)Zeng, Lin, Lu, and Patel}]{crfill}
Zeng, Y.; Lin, Z.; Lu, H.; and Patel, V.~M. 2021.
\newblock Cr-fill: Generative image inpainting with auxiliary contextual
  reconstruction.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, 14164--14173.

\bibitem[{Zeng et~al.(2020{\natexlab{b}})Zeng, Lin, Yang, Zhang, Shechtman, and
  Lu}]{zeng2020high}
Zeng, Y.; Lin, Z.; Yang, J.; Zhang, J.; Shechtman, E.; and Lu, H.
  2020{\natexlab{b}}.
\newblock High-resolution image inpainting with iterative confidence feedback
  and guided upsampling.
\newblock In \emph{European conference on computer vision}, 1--17. Springer.

\bibitem[{Zhang et~al.(2018{\natexlab{a}})Zhang, Hu, Luo, Zuo, and
  Wang}]{semantic_inpainting}
Zhang, H.; Hu, Z.; Luo, C.; Zuo, W.; and Wang, M. 2018{\natexlab{a}}.
\newblock Semantic image inpainting with progressive generative networks.
\newblock In \emph{Proceedings of the 26th ACM international conference on
  Multimedia}, 1939--1947.

\bibitem[{Zhang et~al.(2018{\natexlab{b}})Zhang, Isola, Efros, Shechtman, and
  Wang}]{lpips}
Zhang, R.; Isola, P.; Efros, A.~A.; Shechtman, E.; and Wang, O.
  2018{\natexlab{b}}.
\newblock The unreasonable effectiveness of deep features as a perceptual
  metric.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 586--595.

\bibitem[{Zheng et~al.(2019)Zheng, Qiao, Cao, and Lau}]{distraction-aware}
Zheng, Q.; Qiao, X.; Cao, Y.; and Lau, R.~W. 2019.
\newblock Distraction-aware shadow detection.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 5167--5176.

\bibitem[{Zhou et~al.(2017)Zhou, Lapedriza, Khosla, Oliva, and
  Torralba}]{place_dataset}
Zhou, B.; Lapedriza, A.; Khosla, A.; Oliva, A.; and Torralba, A. 2017.
\newblock Places: A 10 million image database for scene recognition.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40(6): 1452--1464.

\end{thebibliography}


\end{document}
