
\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{images/sota_vis.pdf}
\vspace{-2em}
\caption{Qualitative comparisons of the proposed CoordFill and other methods. The top two examples are 512$\times$512 images from the Places2 dataset while the bottom two images are 1024$\times$1024 images from the Unsplash dataset.}
\label{fig:sota_places2}
\vspace{-1em}
\end{figure*}


\input{images/sota_table}

\section{Experiments}
\label{sec:exp}

\subsection{Implementation Details}
\label{sec:Implementation Details}
We evaluate our method on the widely-used Places2~\cite{place_dataset} dataset, as well as two high-resolution image datasets, CelebA-HQ~\cite{progan} and Unsplash~\cite{unsplash}. We use the irregular mask from the previous method~\cite{pconv} for mask generation with holes up to 25\% following HiFill~\cite{hifill}.  
Our model is optimized by the Adam optimizer with a learning rate of 1e-4 and trained with 100 epochs. All the training experiments are conducted on 8 Tesla A100 GPUs with a batch size of 128. All the images are randomly resized from 256$\times$256 to 512$\times$512 during training, then, we test them in the range of 512, 1024, 2048, and 4096. Following previous methods~\cite{hifill,aotgan}, we use the PSNR, SSIM, and LPIPS~\cite{lpips} for performance comparison.



\subsection{Comparisons with Prior Arts}
\label{sec:State-of-the-art Comparisons}
Our method is compared against the existing state-of-the-art image inpainting algorithms including the general image inpainting models~(DeepFillv2~\cite{deepfillv2},  RN~\cite{rn} and CR-Fill~\cite{crfill}), and the high-resolution image inpainting models~(HiFill~\cite{hifill}, LaMa~\cite{lama}, MAT~\cite{mat}, and ZITS~\cite{zits}) using their official pre-trained models.
% \xiaodong{more methods should be compared}
As shown in Table~\ref{tab:compare}, our CoordFill achieves competitive performance at all resolutions on the Places2 validation set. Compared with the common image inpainting methods~(DeepFillv2, RN, and CR-Fill) on the 512$\times$512 images, the proposed method achieves the best performance. Compared with the existing high-resolution image inpainting methods~(HiFill, LaMa, MAT, and ZITS) on various resolutions, the proposed method also shows more stable results when the resolution increases thanks to the robust resolution and continuous pixel-wise query. For visual comparison, we also compare our methods with other state-of-the-art methods in Figure~\ref{fig:sota_places2}. According to the figure, the proposed method shows more visual-please results than the previous methods with stable reception fields and pixel-wise querying.

Since the previous high-resolution image inpainting method~\cite{hifill} generates the high-resolution samples~($>$512) by the bilinear upsampling on Places2, we use another two high-resolution image datasets to evaluate the performance of each method more accurately, including CelebA-HQ and Unsplash. CelebA-HQ is a commonly used high-resolution face dataset with a resolution of 1024$\times$1024, and the model is retrained with its train set and evaluated on its test set. Unsplash is a real-world high-resolution dataset collected from Unsplash~\cite{unsplash} in which the original images are cropped to 1024$\times$1024, and we use the pre-trained model on Place2 for evaluation. Table~\ref{tab:sota_highres} reports the performance of each method on these datasets, where CoordFill shows competitive performance with the fastest inference speed. As for the visual comparison, the proposed CoordFill also generates clear details as in Figure~\ref{fig:sota_places2}.




\subsection{Efficiency Analysis}
We give a detailed efficiency analysis. Firstly, we show the speed comparison of our method and others in Table~\ref{tab:compare}. The speed is calculated using the average inference time on an NVIDIA GTX 2080 Ti GPU. Specifically, the proposed method runs much faster than all the methods under all the resolutions.
On the same experiment platform, nearly all the methods cause out-of-memory memory issues when the resolution increases to 4096$\times$4096 except ours and HiFill. Also, as shown in Table~\ref{tab:compare}, the proposed method shows a much better performance. 
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{images/component_speed.png}
\vspace{-2.5em}
\caption{The speed comparison of the proposed method~(three different mask ratios) and the baseline~(DownSample + AttFFC + $D_{Conv}$) on the three different resolutions.}
\vspace{-1em}
\label{component_speed}
\end{figure}


Furthermore, as shown in Figure~\ref{component_speed}, we give a detailed explanation of what exactly influences the speed when the resolution changes. As in Figure~\ref{component_speed}, our pixel-wise querying network always runs faster than the normal convolutional decoder~($D_{Conv}$), On the other hand, the speedup comes from the stable reception fields where larger feature maps will speed more time. It is also interesting to see that the running time of our method is flexible when the hole changes. It is because our method only needs to predict the masked region using the pixel-query strategy.



\begin{table}[t]
\caption{Ablation study on the Places2 dataset.}
\vspace{-1em}
\resizebox{\columnwidth}{!}{
\centering
% \begin{tabular}{l|c|l|l|ccc}
\begin{tabular}{c|c|c|c|ccc}
\toprule
    % & Model
    % & 
    Masked
    & \multirow{2}{*}{Decoder} 
    & \multirow{2}{*}{Block}  
    & Resolution
    &  \multirow{2}{*}{PSNR$\uparrow$} 
    & \multirow{2}{*}{SSIM$\uparrow$}   
    & \multirow{2}{*}{LPIPS$\downarrow$}   \\ 
    Prediction
    & 
    & 
    & Injection
    & 
    \\ \hline
    
-
&  $D_{Conv}$
&  ResFFC & -
&  25.5420 &  0.9074 &  0.0688
\\
% & 
$\checkmark$
&  $D_{Conv}$
&  ResFFC & -
&  25.9557 &  0.9097 &  0.0679
\\
% &  
$\checkmark$
&  $D_{MLP}$
&  ResFFC & -
&  24.6480 &  0.8995 &  0.1186  
\\
% & 
-
&  Pixel-Query
&  ResFFC & -
&  26.0434  &  0.9091 &  0.0683
\\
% &  
$\checkmark$
&  Pixel-Query
&  ResFFC & -
&  26.2160 & 0.9109 & 0.0679
\\
% & 
$\checkmark$
&  Pixel-Query
&  AttFFC
& -
&  26.3177 &  0.9116 &  0.0679
\\
% & 
\textbf{$\checkmark$}
&  \textbf{Pixel-Query}
&  \textbf{AttFFC}
& \textbf{$\checkmark$}
&  \textbf{26.3653} &  \textbf{0.9119} &  \textbf{0.0677}
\\ \hline

% \bottomrule
\end{tabular}}
\label{tab:ablation}
\vspace{-1em}
\end{table}
%  
% \vspace{-1em}

\subsection{Ablation Study}
\label{sec:Ablation Study}
We conduct a detailed ablation study to verify the efficiency of each component in the proposed framework on the 512$\times$512 Places2 images, and the quantitative results are shown in Table~\ref{tab:ablation}.

\paragraph{\textbf{Supervision strategy}}
We design a novel framework for image inpainting, \emph{i.e.} selective region synthesis based on the coordinate query. Interestingly, we find that existing methods still calculate the loss function on the whole image, which might be biased supervision because we only care about the hole region accuracy. To verify this speculation, we calculate the loss function in the masked region only, called \textit{Masked Prediction}. As shown in Table~\ref{tab:ablation}, calculating the loss function in the masked region improves the performance in both $D_{Conv}$ and the proposed pixel-wise querying network structure on both two datasets.  It might be because the original supervision strategy for the full image synthesis will be influenced by the extra reception fields and result in color bleeding artifacts as shown in Figure~\ref{fig:mask_predict}.


\paragraph{\textbf{Importance of the coordinate-query scheme}}
The proposed pixel-wise querying network helps our method to obtain high-resolution and high-quality results, which is the key to our method. Thus, we compare it with $D_{Conv}$ and $D_{MLP}$ with the same encoder, where $D_{Conv}$ uses the original convolutional decoder for up-sampling and $D_{MLP}$ feed the pixel-wise feature to a shared MLP for pixel value decoding.
% For a more detailed illustration of $D_{Conv}$ and $D_{MLP}$, we give a simple plot in Figure~\ref{fig:abs}. 
As shown in Table~\ref{tab:ablation} and Figure~\ref{fig:ablation}, the proposed pixel-wise querying framework gets much better performance and runs faster than the previous.

\input{images/ablation}

\paragraph{\textbf{Advantage of our attentional FFC}}

The proposed attentional FFC~(AttFFC) is also an important component. As we have claimed, in AttFFC, we suppress the spatial noise by filtering out the distracting features. Compared with the original FFC blocks~(ResFFC) in LaMa~\cite{lama}, AttFFC achieves a better result than ResFFC as shown in Table~\ref{tab:ablation}.


\paragraph{\textbf{Impact of resolution injection}}
To extract features under a unified receptive field, the parameter generation network accepts a fixed low-resolution input. Hence the pixel-wise querying network generates the same results for different resolutions, and the high-frequency information can only be synthesized with the help of the input positional encoding. By condition on the resolution information, our method got a better performance as shown in Table~\ref{tab:ablation}.

\paragraph{\textbf{Comparisons with inpainting then super-resolution}}
Our coordinate query-based decoding network supports synthesizing the content in arbitrary resolution, and it is also implicitly required to consider the super-resolution as post-processing during testing. To check the effectiveness, we construct a baseline: the decoding network only generates the output of the same resolution as the down-sampled input~(256$\times$256) and then gets high-resolution image~(1024$\times$1024 in our case) by an extra up-sampling or super-resolution network.
We plot the visual results to support our claim in Figure~\ref{fig:super-resolution}, where the proposed method shows sharper and visual-friendly results.


\input{images/super_resolution}


\newcommand\wwfailed{0.23\columnwidth}
\newcommand\hhfailed{0.23\columnwidth}
% Figure ablation
\begin{figure}[!h]
\centering  

% \addtocounter{subfigure}{-4}
% \par\bigskip\vspace*{-2em}
\subfigure[\scriptsize{Input}]{\centering\includegraphics[width=\wwfailed, height=\hhfailed]{images/fail_cases/input/Places365_val_00000384.png}}
\subfigure[\scriptsize{GT}]{\centering\includegraphics[width=\wwfailed, height=\hhfailed]{images/fail_cases/gt/Places365_val_00000384.png}}
\subfigure[\scriptsize{LaMa}]{\centering\includegraphics[width=\wwfailed, height=\hhfailed]{images/fail_cases/lama/Places365_val_00000384.png}}
\subfigure[\scriptsize{CoordFill}]{\centering\includegraphics[width=\wwfailed, height=\hhfailed]{images/fail_cases/ours/Places365_val_00000384.png}}

\vspace{-1em}
\caption{Failure case of the proposed CoordFill on Places2 512$\times$512 images. CoordFill gets dazzling artifacts when the background is complex. 
% In this case, 
Although LaMa cannot achieve satisfactory results either, their artifacts are more natural.}
% \vspace{-1em}
\label{fig:failed_case}
\end{figure}


\subsection{Limitation}
Although notable advantages are demonstrated by our method, there are still some limitations. 
To achieve stable and efficient performance, only a 256$\times$256 image will be used to infer the missing pixels, where the original high-frequency details are hard to preserve.
Besides, as shown in Figure~\ref{fig:failed_case}, in some cases, our method struggles to synthesize the photo-realistic textures when the background is complex, which is a common challenge of existing image inpainting methods. 