\section{Related Work}
\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{images/overview2.pdf}
\vspace{-2em}
\caption{Overview of the proposed framework.}
\label{overview}
\end{figure*}

\subsubsection{Image Inpainting}
Traditional image inpainting methods rely on strong low-level assumptions, for example, local patches~\cite{patchmatch} can be used to fill the missing region. Several recent CNN-based methods~\cite{liu2020rethinking,lama,global_and_local,deepfill,crfill,deepfillv2,Wan_2021_ICCV,pconv,rn, semantic_inpainting, structural_inpainting,mat,zits} have a similar or a stack of encoder-decoder architectures. More specifically, Iizuka~\emph{et.~al.}~\cite{global_and_local} introduce a GAN-based framework by the global and local discriminator. Based on the GAN framework, more novel blocks are also introduced through Attentions~\cite{deepfill,deepfillv2, liu2020rethinking,crfill}, Regional Normalization~\cite{rn} and Regional Convolutions~\cite{pconv}. Novel structures, like Vision Transformer~\cite{vit}, also draw the attention of the image editing community~\cite{Wan_2021_ICCV}. Meaningful priors, such as edge~\cite{edgeconnect} and semantic label~\cite{semantic_guide} also play an important role. For high-resolution image inpainting, a multi-stage network is a common choice. For example, Yi~\emph{et.~al.}~\cite{hifill} refine the low-resolution output via the contextual residual aggregation. Zeng~\emph{et.~al.}~\cite{zeng2020high} design a multi-scale network structure with guided up-sampling. MAT~\cite{mat} and ZITS~\cite{zits} are transformer-based image inpainting systems for high-resolution images. However, the multi-stage network structure is slow for real-world applications. In contrast, LaMa~\cite{lama} designs a one-stage network by the fusion of the multi-scale reception fields. However, their speed is also restricted by the normal decoder.

\subsubsection{Continuous Image Representation}
Learning the continuous image representation, \emph{i.e}, the implicit representation, are popular recently, due to the success in 3D human digitization~\cite{pifu, pifuhd} and novel view synthesis~\cite{nerf}. Generally, these methods query the color of the specific localization using the positional encoding in the frequency domain, which can generate more detailed high-frequency details than other methods. However, implicit representation is rarely developed in the 2D image domain. StyleGANv3~\cite{styleganv3} is built on implicit representation for alias-free settings of the image generation in the specific domain. InfinityGAN~\cite{infinitygan} also learns for image synthesis. ASAPNet~\cite{asapnet} learns the pixels from the semantic layout. LIIF~\cite{llif} learns a continuous representation for image super-solution. However, how to utilize this technique in image inpainting is still unclear and we make the first step.


\subsubsection{Efficient Network Structure}
Designing an efficient network structure for the high-resolution image is a more practical consideration for any deep learning-based method. Several attempts have also been made to make the convolutional network work in mobile applications. For example, the efficient network backbone has been widely explored in image classification~\cite{mobilenet,squeezenet}. In the image editing and synthesis fields, the models can be distilled from a trained larger network using knowledge distillation~\cite{gancompression}. For the image retouching tasks~\cite{3dlut, s2crnet}, learning the global-aware representation is helpful to keep the efficiency in high-resolution images. As for the high-resolution inpainting, efficiency is not the concern of the community yet.