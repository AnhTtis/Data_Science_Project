% \clearpage


%-------------------------------------
\section{Method}
\label{sec:method}
%-------------------------------------

%-------------------------------------
\subsubsection{Overview}
\label{subsec:overview}
%-------------------------------------

Our goal is to synthesize the missing content for high-resolution images in an efficient manner and hold a stable performance to diverse resolutions. To this end, our proposed model, dubbed as CoordFill, solves the inpainting problem with two phases: (i) restorative feature synthesis, as represented by spatially-adaptive parameters of the reconstruction function; (ii) individual pixel reconstruction based on the per-pixel coordinate query.
The advantage of this design is that we can synthesize high-resolution content efficiently for any specified regions thanks to the coordinate query-based reconstruction scheme. Therefore, on the one hand, it is allowed to selectively reconstruct the hole region of the input image. On the other hand, we can resample the input image with a fixed resolution before being fed to the network, which enables the contextual feature extraction under a unified receptive field, yet the image still could be restored with the original resolution (i.e. by sub-pixel coordinate query).


Specifically, given a high-resolution masked image $\mathbf{I}*\mathbf{M} \in \mathbb{R}^{H \times W \times 3}$ and its mask $\mathbf{M} \in [0, 1]^{H \times W}$, we first downsample them by $S$ times to $256 \times256$. 
Then, we build a Parameter Generation Network $G$ to generate the full image representation as a parameter map $\mathbf{\Phi}$. Finally, a Pixel-wise Querying Network $Q$, parameterized by $\mathbf{\Phi}_p$, generates pixel values according to the coordinate query $\mathbf{p}$ at the corresponding patches and finally generates the restored image $\mathbf{I}_o$ by pasting back the non-hole content.
The overall architecture is illustrated in Figure~\ref{overview}. Below, we give the details of each component.


%-------------------------------------
\subsubsection{Parameter Generation Network}
\label{subsec:parameter_network}
%-------------------------------------

Existing inpainting models are typically formulated to generate a full image with hole regions filled but only the hole region pixels are used in the final results. Obviously, the non-hole pixels are not necessary to be synthesized, as they contribute nothing to the final visualization. To avoid such unnecessary computation, we propose to first generate a parameter map from the input and then reconstruct the pixel values at hole regions individually.
As the inpainting task relies more on the surrounding context interpretation than texture details, we propose to resample the input with a fixed resolution before feeding it to our parameter generation network $G$. This operation not only promotes computation efficiency, especially for high-resolution images but also guarantees our network with good generalization to various input resolutions. Note that, we can still reconstruct the hole region with the original resolution, due to the coordinate query-based reconstruction scheme.


\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{images/attffc.pdf}
\vspace{-2em}
\caption{The details of the proposed AttFFC Block.}
\label{efb}
\end{figure}

We employ a fully convolutional structure for the parameter generation network, composed of the encoder, bottleneck blocks, and the final layers of our parameter generation network.
In detail, we down-sample the input feature via the convolution layer three times to favor the process of calculation.  

In bottleneck blocks, inspired by recently popular Fast Fourier Convolution~(FFC)~\cite{ffc,lama} blocks, we propose an FFC-based framework also. This block captures the global reception fields of the input in the frequency domain, yet a better performance in image inpainting which needs larger reception fields. Please refer to the previous works~\cite{ffc,lama} for more details.


However, the global receptive field in FFC is only limited in the frequency domain, which lacks representation in the spatial space.
To alleviate this problem, we propose a spatial-aware attention block based on FFC~(AttFFC). 
As shown in Figure~\ref{efb}, the proposed block contains $-/+$ operations to remove the spatial noise and enhance the features~\cite{distraction-aware}.
Given a feature map $\mathbf{F} \in \mathbb{R}^{H \times W \times C}$, AttFFC first computes the spatial attention map $\mathbf{F}_{dm}$ through a FFC Block and a Sigmoid function $\sigma$:
\begin{equation}
\mathbf{F}_{dm} = \sigma(\operatorname{FFC}(\mathbf{F})).
\end{equation}
where $F_{dm}$ are the one channel spatial confidential map automatically. Next, we use this spatial prior and the original feature to get the spatial noise:
% , we can calculate the distracting features by:
\begin{equation}
\mathbf{F}_{d} = \operatorname{FFC}\left(\operatorname{concat}\left(\mathbf{F}, \mathbf{F}_{dm}\right)\right).
\end{equation}
Then, we remove the spatial noise, and enhance the features by learning the residual with an FFC block:
\begin{equation}
\mathbf{F}'= \mathbf{F} - F_{d} + \operatorname{FFC}(\mathbf{F} - \mathbf{F}_{d}).
\end{equation}


We stack six AttFFCs in our parameter generation network as the bottleneck.

Notice that, since we use the encoder to reduce the feature space, the learned spatial feature is also in a small resolution. To this end, we build the pixel-wise parameters via selection, mapping, and upsampling. In detail, we first select the masked region features by the corresponding coordinates, and then, for each spatial feature, we generate the required number of the parameters by the linear mapping function $f$. In addition, to make the parameter generation network sensitive to the target resolution $r$, we inject the target resolution as a conditional input to $f$:
\begin{equation}
\phi_{m} = f(cat[F_{m}, r]),
\end{equation}
where $F_m$ is the features in the masked region only, and $cat[\cdots]$ denotes the concatenation operation.
Finally, the spatial-adaptive patch $\phi_{m}$ will be upsampled to the required resolution with nearest-neighbor interpolation. Hence, we obtain a series of parameter vectors with channels corresponding to the weights~(and the biases) of the MLPs, and the heavy computation is performed at low resolution.


%-------------------------------------
\subsubsection{Pixel-wise Querying Network}
\label{subsec:query_network}
%-------------------------------------

As mentioned above, we adopt a coordinate query-based pixel reconstruction scheme by utilizing the spatially-adaptive parameters. In particular, inspired by the efficiency of implicit representation~\cite{asapnet}, we employ a Multi-Layer Perception~(MLP) for per-pixel reconstruction, \emph{i.e.} the Pixel-wise Querying Network $Q$, which is parameterized by the spatial-adaptive contextual information embedded parameters, as extracted by $G$. Note that, $Q$ is just a function form and its expressive capability mainly inherits from those deeply extracted parameters $\mathbf{\Phi}$. Formally, $Q$ takes the pixel's coordinate $\mathbf{p}$ as input and outputs the color values:
\begin{equation}
\mathbf{y}_{p} = Q(\mathbf{p};\mathbf{\Phi}_{p}),
% = Q(p ; G(I, M)),
\end{equation}
To boost high-frequency details, we encode each component of the 2D pixel position $p = (P_x, P_y)$ as a vector of sinusoids~\cite{transformer}.
Additionally, we normalize the positional encoding system and sample positional encoding within a fixed range. Thus, the pixels in the high-resolution images can be synthesized by changing the interval of the positional encoding. For example, given an input image $\mathbf{I}$ and its hole mask $\mathbf{M}$ of resolution $H \times W$, our parameter generation network generates the spatially-adaptive parameter map $\mathbf{\Phi}$ of resolution $h \times w$ from the downsampled input $\{\tilde{\mathbf{I}}, \tilde{\mathbf{M}}\}$. Then, we feed the coordinate query:
\begin{equation}
\begin{aligned}
\mathbf{p} = (\sin \left(2 \pi p_{x} / \mathbf{E}_{x}\right), \cos \left(2 \pi p_{x} / \mathbf{E}_{x}\right), \\
\sin \left(2 \pi p_{y} / \mathbf{E}_{y}\right), \cos \left(2 \pi p_{y} / \mathbf{E}_{y}\right))
\end{aligned}
\end{equation}
at expected interval $\mathbf{E}_{x}$, $\mathbf{E}_{y}$, where $\mathbf{E}_{x} = \frac{H}{h}$ and $\mathbf{E}_{y} = \frac{W}{w}$
, and then synthesize the hole-region content of the original resolution.


\subsubsection{Loss Function}
\label{sec:loss}
Since image inpainting only needs to measure the realism of the content, we train our model with different perception losses, including the differences in the pre-trained AlexNet domain~\cite{lpips,jo2020investigating}, adversarial loss~\cite{gan}, and feature matching loss~\cite{lama,pix2pixhd}.

For the perceptual loss on the pre-trained ImageNet classification domain, the loss function is defined as:
\begin{equation}
L_{per}=\sum_{k} \tau_{k}\left(E_{k}\left(\mathbf{I}_o\right)-E_{k}\left(\mathbf{I}_{gt}\right)\right),
\end{equation}
where $E$ is an AlexNet feature extractor, $\tau$ calculates the differences in the feature domain via $L_1$ loss, and then, we compute and average the losses from $k$ layers.

Then, the adversarial loss is used to encourage the model to generate more realistic details, which can be written as:
\begin{equation}
\begin{aligned}
L_{adv} = \mathbb{E}[\log (1-D(\mathbf{I}_o)] + \mathbb{E}[\log D(\mathbf{I}_{gt})].
\end{aligned}
\end{equation}
where $\mathbb{E}$ denotes expectation values over the training batch. 


Next, the feature matching loss is adopted for stabilizing the GAN training~\cite{pix2pixhd,lama}:
\begin{equation}
L_{fm} = \sum_{i}\left(D^{i}\left(\mathbf{I}_{gt}\right), D^{i}\left(\mathbf{I}_o\right)\right),
\end{equation}
where $D^{i}$ denotes the activations from the $i$-th layer of the discriminator D.

Finally, the total loss of our model can be written as:
\begin{equation}
L_{total} = \lambda_{per} L_{per} + \lambda_{adv} L_{adv} + \lambda_{fm} L_{fm}.
\end{equation}
where we empirically set $\lambda_{per}=10$, $\lambda_{adv}=1$ and $\lambda_{fm}=100$ respectively.
