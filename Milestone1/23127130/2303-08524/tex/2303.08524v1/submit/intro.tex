\section{Introduction}

With the rapid development of digital media, the demand for image editing, for example, filling the given hole according to the remaining background pixels or removing the unnecessary object, also increases a lot. These tasks belong to image inpainting, which is one of the fundamental image synthesis tasks, which requires both semantic understanding and conditional generation. 



Convolutional neural network~(CNN)-based methods dominate this field in recent years. For example, coarse-to-fine network structures and generative adversarial networks~\cite{gan} based methods~\cite{deepfill, deepfillv2, global_and_local, pconv} are used to synthesize the realistic textures. Other types of image inpainting are inspired by the typical hints~(\emph{e.g.} edges~\cite{edgeconnect}, semantic maps~\cite{semantic_guide}) from the original background, these methods guess the hints in the hole region firstly, and then, they synthesize the texture by the guidance of the filled hints. However, current methods still suffer from many limitations for real-world applications since casual images are in high resolution and on the mobile platform. We raise a question: \textit{What makes the inpainting hard in efficiently handling the high-resolution images?} and we attempt to answer this question from the following perspectives:

$(i)$ Learning the larger reception fields is time-consuming for high-resolution images. A larger reception field is essential in image inpainting~\cite{lama}, which means that we need to feed the full-size images to the inpainting network. In general, there are two straightforward ways to increase speed. We can crop or resize the images to fit the resolution of the input. However, direct cropping will influence the semantic guidance from the background region and hugely reduce the ability of scene understanding. Directly resizing is another option, it requires preserving the detailed high-frequency details for upsampling.

$(ii)$ The decoders will also synthesize a lot of unnecessary pixels. Image inpainting aims to remove a relatively small hole from the input. Restricted by the form of the image matrix, small irregular holes also need to be synthesized using a stack of convolutional layers in full image size. 



We try to break the above limitations via the implicit representation from neural rendering community~\cite{siren,nerf,asapnet}. A typical implicit representation overfits the scene to small multi-layer perceptrons~(MLPs) network, where the input is the coordinate of the image matrix and the output is the queried pixels. It is a continuous representation that can keep the high-frequency details well and can achieve pixel-wise reconstruction. Inspired by these wonderful features, we propose a novel framework for efficient high-resolution image inpainting using implicit representation for the first time. To make the network related to the specific input, our method utilize a meta-learning strategy where the parameters of the pixel-wise query network are spatial-adaptive~\cite{asapnet}. In detail, our framework contains two sub-networks for parameter generation and pixel-wise query. 
In the parameter generation network, we resize the high-resolution image and generate the parameters for each local image patch. This network is built via a series of the proposed attentional FFC blocks. 
Then, the pixel-wise query network reforms the predicted parameters as a series of the MLP, where the input is the positional embeddings in the frequency domain and the output is the predicted color value in the given coordinate. Thanks to the proposed structure, our method can run faster in the challenge of high-resolution image inpainting from two aspects. On the one hand, most of the costly operations are in the low-resolution parameter generation network, where we can learn larger reception fields. On the other hand, we can decode the masked regions only since the coordinate can be queried one-by-one in parallel and the larger size image can be got via the coordinate re-sampling. The detailed experiments show the efficiency and the power of the proposed method  compared with previous state-of-the-art on several benchmarks.

The contributions of this paper are summarized as follows:


\begin{itemize}
    \item We propose CoordFill, a novel framework for efficient high-resolution image inpainting via parameterized coordinate querying.
    \item We design an attentional FFC-based block as the basic block in our parameter generation network. It learns to focus on the masked region automatically.
    \item Our method runs faster than previous baselines and achieves state-of-the-art performance in image inpainting on multiple datasets.
\end{itemize}


