\newpage
\section{Research Approach}
\label{sec:approach}

%\changeda{R1: The description of the research approach has improved. Still, I find that more details
%are needed. I would advise the authors to delete the paragraphs about all the things it was
%not relevant to do (grounded theory, experiments, statistically significant data, etc.). This is
%irrelevant and it is (in my opinion) obvious that this is not something that would match the
%research objectives. 

%I would rather have the authors spend time on explaining how their work fits within a
%design science approach. As of now, it is merely stated that it does fit. I also suggest that the
%authors include in this section a description of the method used for evaluating the developed
%artifact (see comments related to section 6).
%}

% \abnote{Aldo adds one short sentence to summarize why this fits DSR}\bdsnote{I don't think this is necessary.}
%\changeda{R1: Still, I think it lacks an explanation of what DSR is, and why what you are doing fits into this research approach. You should also add a reference to show what you mean by DSR, as there are several approaches to DSR that could be applied.}\bdsnote{I think my novel text answers this already.}

\changed{Beyond their quotes in the previous section, Hevner et al.\ describe \dsr as ``achieving knowledge and understanding of a problem domain and its solution by the building and application of designed artifacts''~\cite{hevner2004design}. For doing so in the \softprot risk management problem domain, in particular for answering the three RQs, we collected, structured, designed, built, and applied a large set of related artifacts in the three research steps shown in Figure~\ref{fig:approach}.}

First, in the collaborative European ASPIRE research project, we designed, developed, and evaluated a largely automated \poc called \esp. This bottom-up research was driven by the industrial partners' business needs and \sdlc requirements. Section~\ref{sec:workflow} presents the \poc, of which Section~\ref{sec:evaluation} presents the evaluation.

Second, we studied the adoption of a standardized IT risk management approach, the NIST SP800-39 standard, in the domain of \softprot. This was driven by our observations of the state of the domain of \softprot as discussed in Section~\ref{sec:background} and the motivation presented in Section~\ref{sec:motivation}. The result of this study is the approach presented in Section~\ref{sec:requirements}. 

Third, we analyzed which of the constructs, models, and methods required in the adopted approach are actually covered by the automated tool support in the \esp. The result is a mapping between the artifacts introduced in Section~\ref{sec:requirements} and those discussed in Section~\ref{sec:workflow}.

We now discuss our approach in these steps in more detail.

\begin{figure}
\centering
\includegraphics[width=12cm]{images/approach.pdf}
\caption{Three research steps leading to the results presented in this paper.}
\label{fig:approach}
\end{figure}


\subsection{Step 1: Bottom-up Development and Evaluation of Proof-of-Concept Decision Support Tools}
\label{sec:step1:bottom-up}
Our research into \softprot decision support intensified in the 2013--2016 European ASPIRE FP7 research project\footnote{ \url{https://aspire-fp7.eu/}} in which we collaborated with three \softprot companies: Nagravision with a focus on DRM, Gemalto (now Thales) with a focus on software trusted execution environments and SafeNet (now Thales) with a focus on software license management. 
The project researched a layered \softprot toolchain for mobile apps and corresponding (semi-)automated decision support methods and tools. The companies identified the lack of such automated support tools as a critical, foundational gap in the \softprot knowledge base that hampered the effective and efficient deployment of \softprot in practice. In the traditions of \dsr, we endeavoured to close this gap by researching the design and development of novel artifacts, including proof-of-concept tools.
The companies and their technical and commercial \softprot business needs drove the project's requirements analysis and scope determination, as well as the considered attack model. In technical meetings, we engaged with their stakeholders and experts in \softprot, including software developers, \softprot tool developers and users, security architects, and penetration testers. We engaged with higher management in advisory board meetings. The insights obtained there drove the development of decision support techniques in a bottom-up fashion during the ASPIRE project, i.e., starting from concrete \softprot problems and business requirements and solutions, as well as existing, mostly informally described best practices\footnote{Unfortunately, many documents that formalized and structured those insights in the ASPIRE project are confidential.}. As we were, to the best of our knowledge, the first project to research largely automated end-to-end decision support tools, conforming to existing standards was at that time not at all a requirement or concern. 

Through the \softprot tool flow and decision support developed in the project, we provide an answer to RQ1, demonstrating that automated decision support may be within reach. That evidence in the form of artifacts and their evaluation is presented later in this paper. 

We performed and present our evaluation using the \feds~\cite{FEDS-evaluation}, the taxonomy of evaluation methods for IS artifacts by Prat\etal~\cite{Prat-taxonomy}, and the evaluation criteria and terminology by Cleven\etal~\cite{DSRA-evaluation-alternatives}. 

Our \emph{ex-post} evaluation from an \emph{engineering perspective} focused mainly on the \emph{human risk and effectiveness} strategy, as the aim was to determine whether the artifacts consisting of our \poc decision support tool and all the data it generates are accepted by the involved users and whether it benefits their work. We focused on properties compliant with the ISO/IEC 9126-1:2001 criteria\footnote{\url{https://www.iso.org/standard/22749.html}}, namely usability, efficiency, correctness, and comprehensibility and acceptability by the users.
%\changeda{R1: I still feel that it would be good to have the method part only one place in the article. However, if you are convinced that it is too early to explain the details here in Section 2 (as you state in your response), could you at least point to the appropriate subsection in section 7 for more details. I also think it would be a good idea to add a figure showing an overview of the overall steps of the evaluation, and e.g. include a few details on how many experts were involved in each step so that the reader can get an easier overview of how the evaluation happened and the strength of the evaluation}\bdsnote{I agree with the idea of presenting an overview figure. But isn't it better to include a figure in Section 7 then? Maybe we can simply mention how many experts were involved here, by simply adding "2" or "3" before "experts"?}
\changed{As Section~\ref{sec:design_qualitative_eval} will describe in much more detail,} we organized the evaluation in multiple steps to gather early feedback and gradually involve more external experts.
Initially, a qualitative assessment of the automatic decision support prototype (and of the artifacts it used) was performed with \changed{three} industrial experts working on the ASPIRE project. The objective was to improve the early versions and components and to release the final prototype. %Three mobile applications provided by the industrial partners have been used as reference scenarios: a media streaming app, a licence checker, and one-time password generator.
A second qualitative assessment was performed on the final \poc, for which we involved \changed{two} additional industrial experts that had not participated in the development.
Moreover, they evaluated the performance of the algorithms and techniques used in the PoC on both the reference use cases and artificial applications, with measurements and with a complexity analysis.
%
In addition, we performed a \emph{purely technical artifact} assessment to verify that the tool provides solutions in a useful time with reasonable use of resources. %To that extent, we have evaluated its performance to determine computational feasibility and the scalability of its tasks. \bdsnote{This last paragraph seems to be a pure repetition of the last sentence above it.}





% \begin{table}[tb]
%     \centering
%     \begin{tabularx}{\textwidth}{ p{2cm}  X }
%     \toprule
%     \textbf{Category} & \textbf{Description} \\
%     \midrule
%     \textit{Time} & Ex-Post, \ie after the implementation of a specific version or after the implementation of individual components\\
    
%     \textit{Perspective} &  the evaluation has been performed according to an Engineering perspective\\
    
%     \textit{Artifact Focus} & \textit{Technical} focus: impact on the users; \textit{Strategic} focus: suitability of interpreting \softprot  as a risk management process\\
    
%     \textit{Artifact Type} & prototype.  Indirectly constructs, methods, and models have been assessed, as the correctness of the artifact provides evidence that the models, data, and algorithms properly model the decision task\\
    
%     \textit{Function} &  we assessed the artifact's correctness, efficiency, usability, and comprehensibility and acceptability by \softprot users. Moreover, we used later the results of the experiment for 
%     \textit{Legitimation} purposes, \ie to prove that \softprot can be approached as a risk management process and that it can be mostly automated. Nonetheless, we have acquired Knowledge about the SP process and the individual parts of the artifact focusing on individual risk management phases.\\

    
%     \textit{Position} & the involved experts were not part of the team that implemented the artifact \\

%     % \textit{Epistemology} & Positivist and Realist\\
%     \bottomrule
%     \end{tabularx}
%     \caption{Categorization of evaluation criteria according to criteria proposed by Cleven\etal~\cite{DSRA-evaluation-alternatives}.}
%     % \abnote{Should we move this table in section STEP1???}
% \end{table}

% \abnote{merge the table content into the text}




\subsection{Step 2: Top-Down Adoption of a Standardized IT Risk Management Approach}
After the ASPIRE project had formally finished, we continued our collaboration and gradually developed our vision that the best way to approach decision support is from the perspective of information risk management approaches. This vision first manifested itself in the July 2019 Ph.D. thesis of Leonardo Regano~\cite{ReganoPhd} that presents the components of the \esp and that is structured according to the phases of information risk management standards. 

We reached out to other researchers and practitioners in the \softprot domain to gather their opinions and insights, as well as doubts on decision support for \softprot. This happened in informal discussions but also in structured ones, including the August 2019 Dagstuhl seminar on Software Protection Decision Support and Evaluation Methodologies~\cite{Dagstuhl}, of which B.\ De Sutter was the main organizer. In this one-week seminar, the three senior authors of this paper engaged again with a range of experts, including, amongst others, \softprot researchers, security economists, reverse engineering practitioners, software analysis experts, and commercial \softprot developers. %Several sessions at the seminar were devoted to discussion on decision support, as well as on presentations and demonstrations of the artifact presented in Section~\ref{sec:workflow} and evaluated in Section~\ref{sec:esp:results}.
During the seminar, the need for standardization came to the forefront, if not formalized, then at least in terms of best practices and guidelines for conducting research into SP and evaluating the strength of proposed SPs and attacks thereon. 

Following that seminar, we invested in a top-down approach, studying the adoption of existing information and system security standards in the domain of \softprot. We investigated how the generic concepts that make up these standards are specialized and adopted in specific security domains such as network security. We then extensively brainstormed about how they can also be specialized and adopted in the domain of \softprot. For example, the \nist SP800-39 IT systems risk management standard~\cite{nistSP800-39} prescribes a top-level method consisting of four generic risk management phases, each corresponding to their own, conceptually formulated, abstract method. We studied the domain-specific organizational problems that need to be solved in the different phases, and which more concrete domain-specific concepts those phases need to encompass for \softprot. 

We started this research by collecting our combined insights, then structuring them, and then iteratively coming to the text of Section~\ref{sec:requirements} that, in essence, presents a top-level \softprot risk assessment method and the necessary artifacts for using that method, thus providing our answer to RQ2. We want to thank the reviewers of earlier versions of this text for their valuable insights that helped us produce the final result. 

While this research step was rooted in our previous experience with \softprot, we tried to perform this study as independently as possible from the \poc results of the ASPIRE project. This shows, amongst others, in the fact that in Section~\ref{sec:requirements}, we put forward about 40\% more concepts to be included in the proposed standard approach adoption than are covered in the \poc results we present in Section~\ref{sec:workflow}. As a concrete example, we discuss the organizational problem of \softprot tool vendors and their customers not giving each other white-box access because they do not trust each other in Section~\ref{sec:framing_sdlc}. That problem was out of scope in the ASPIRE project and is hence not tackled by the presented \poc tools.



\subsection{Step 3: Coverage Analysis of the Adopted Approach in the \dsr Framework}

An important consideration in our study in step~2 was the need for automation, as reflected in the last part of RQ2 and in RQ3. In later sections, we argue in more detail why we consider automation of many of the adopted and specialized methods beneficial, if not crucial.

Our answer to RQ3 is not based on theoretical analysis and abstract reasoning but on tangible evidence, i.e., the existence of the concrete artifacts that form the \poc developed in step 1. To provide this answer to RQ3, we organized numerous internal discussions in which we analyzed which of the concepts from the different phases of the proposed approach are instantiated by the automated components of our \poc. 

To do this more methodologically, we adopted the \dsr framework by Hevner et al.~\cite{hevner2004design}. First, we rephrased the adopted approach such that all essential concepts of the approach's four phases are clearly identified as either constructs (vocabulary to define and communicate concrete \softprot cases), models (abstractions and representations to aid case understanding and to link case features and solution components to enable exploration), and methods (algorithms, practices, and processes, as well as guidance on how to tackle concrete cases). These are the three abstract types of artifacts that Hevner et al.\ identify as foundational elements of an information systems knowledge base, in this case the \softprot knowledge base. 

Next, we identified which of these abstract artifacts are instantiated by means of components of the \poc \esp. Such implementations are called instantiation artifacts by Hevner et al. They form the fourth type of foundational element in a knowledge base. 

The mapping from more abstracts \dsr artifacts onto concrete \poc instantiation artifacts is documented in this paper by means of recurring tags. The tags are introduced in Section~\ref{sec:requirements} when the constructs, models, and method artifacts are first introduced, and they recur in Section~\ref{sec:workflow} where the corresponding instantiations are discussed.


