% \subsection{Challenges towards Standardization, Formalization, and Automation}
% \label{sec:challenges}

%\changeda{R1: This section seems to me to be mainly about the difference between network security and SP. It is not always clear to my why this difference constitutes a challenge toward standardisation, etc. Consider to rewrite this section so that the different challenges and their implications for standardisation, etc., comes to the forefront. The comparison with network security can then be seen as examples illustrating the challenges, showing why SP is more difficult (if that is the case)}

Despite the many benefits a standardized, formalized, and automated approach would bring, such an approach is a long way off, and adopting a \nist-style risk management faces several challenges.
%: we currently lack even the most basic common \mate vocabulary, the assessment of exposure to threats needs to be tailored on the application to protect, and the deployment of \mate\ {\softprot}s is in some regards more complex than securing networks.

%
% ASSETS
%
\changed{
A first challenge relates to the definition of asset categories and their relation with security properties. These are lacking today, which is problematic for the framing of risks. %
{\softprot} relies on the \mate attacker model that has never been defined clearly. The abilities of \mate attackers are unclear, not in the least because of the complexity of modelling human code comprehension and software tampering capabilities.

A second challenge is the definition of threat and risk assessment models that allow enough precision and objectivity. Estimating the feasibility of \mate attacks requires a white-box analysis of the assets and of the entire application.
The complexity of mounting static, dynamic, symbolic, and concolic attacks heavily depends on the structure and artifacts of the software, such as the occurrence of all kinds of patterns or observable invariants. 

Thirdly, moving towards a more precise categorization of protections and risk in the \mate scenario is another challenge that needs to be overcome for the risk mitigation phase. In practice, \softprot provides only fuzzy forms of protection. {\softprot}s have only been categorized coarsely (e.g., obfuscation vs.\ anti-tampering). In general, it is not clear what security level they offer where, and there yet exists no well-defined set of categories of security controls to mitigate \mate risks. This contrasts with, e.g., the field of cryptography, in which algorithms are characterized in terms of well-defined properties such as ciphertext indistinguishability or second pre-image resistance~\cite{intro_crypto}. Also in network security, it is clear what firewalls and VPNs do and how to use them to mitigate network security risks. There are accepted measures and guidelines to estimate the effectiveness of categories of network security mitigations and in some cases categorization of tools and vendors that help in estimating their efficacy~\cite{ISO27004}. The \mate domain lacks such well-definedness. 

Fourthly, today it remains a huge challenge to simply measure or estimate the efficacy of {\softprot}s. This is obviously necessary to assess the residual risks of deployed {\softprot}s. However, no metrics are currently available to quantify \softprot efficacy. Potency, resilience, and stealth are commonly accepted criteria~\cite{collberg1997taxonomy}, but no standardized metrics are available for measuring them. Complexity metrics originating from the field of software engineering have been proposed~\cite{D4.06}, and ad-hoc metrics are used in academic papers~\cite{JENS,linn2003branchFunctions}. However, none have been empirically validated for use in {\softprot}, and practitioners most often do not see the metrics used in academic papers as reliable proxies of real-world potency, resilience, or stealth. Using those metrics is hence not yet considered a viable replacement for human expertise and manual pen testing. In many cases, there are no hard proofs that {\softprot}s are effective in delaying attackers. Rather than encouraging checks by external parties, {\softprot} vendors often contractually prevent the analysis of protected code, instead relying on \sto. As a result, there is neither an objective nor a measurable assurance of protection, nor an objective evaluation of the companies' work.
%
In academic {research}, the situation is not much better. For example, the seminal obfuscation versus analysis survey from Schrittwieser et al.\ never refers to a risk analysis framework~\cite{schrittwieser2016}. Their results, although widely acknowledged, are hence not readily usable in a decision process.

The aforementioned challenges are particularly hard because in \softprot, determining the boundaries between assets and protections is no easy task. \softprot{}s are often processes that transform assets to hinder analysis and comprehension of their logic~\cite{schrittwieser2016}.
For instance, most forms of obfuscation transform code fragments. Since {\softprot}s need to be layered for stronger and mutual protection and to exploit synergies, obfuscation can transform code that results from previous transformations, such as code guards injected for anti-tampering purposes. Some obfuscations even aim for eliminating recognizable boundaries between different components~\cite{JENS}, and others aim for re-using application code for obfuscations~\cite{Jens2}. 
As a result, the code of multiple {\softprot}s and of the assets they protect becomes highly interwoven. We hence need to talk of protected assets, certainly not of separated protection and asset entities. 

Furthermore, software internals must be known to the tools. This includes the types of instructions, structure and semantics of the code, and the presence of any artifacts that might benefit attackers. This information is needed to decide whether some (layered) {\softprot} can be effective or not and to tune its parameters.
%
In addition, it is generally accepted that in order to deploy {\softprot}s effectively, an application's architecture needs to be designed with the protection of the sensitive assets in mind.
If it is not designed well, {\softprot}s will only provide superficial mitigation.  For theoretical definitions of \softprot, such as virtual black-box obfuscation, Barak already proved this for contrived programs~\cite{barakImpossibility}, but this statement also holds for real-world software and practical \softprot. Examples of design problems that are hard, if not impossible, to fix with {\softprot}s are bad external or internal APIs, missing authorization, and improper or missing crypto key ladders to protect various assets. Such ladders require complex key management, key storage, and crypto functionality, which are easy to get wrong for non-experts. Risk assessment methods must hence recognize software whose design prevents proper protection and report that risks cannot be reduced to the desired level solely with {\softprot}s.
This again stresses that \mate risk analysis requires insights into software internals to identify weaknesses that may turn into vulnerabilities that cannot be protected with \softprot.

% \abonote{not clear why this is relevant}
\softprot thus poses challenges that, to the best of our knowledge, have not emerged and have thus not been addressed when standardizing other fields such as the network security field in which de-facto standards are used and referred to. These challenges impact the standardization of risk management and, in particular, the definition of objective criteria for assessing the mitigations.
%

In conclusion, despite their obvious appeal, risk management standardization and a functioning open market as they exist in other areas of ICT security are in our opinion missing in {\softprot} not only because the community is late in developing them, but also because managing the risks in \softprot is really challenging. 
}