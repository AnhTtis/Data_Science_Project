
\section{Experiment}

\input{tbls/spider}
\input{tbls/spider-dk}
\subsection{Experiment Setup}

\noindent\textbf{Datasets.} 
 We conduct extensive experiments on twelve public benchmark datasets as follows: (1) \textbf{Spider} \cite{yu2018spider} is a large-scale cross-domain Text-to-SQL benchmark. It contains 8659 training samples across 146 databases and 1034 evaluation samples across 20 databases.  (2) \textbf{Spider-SYN} \cite{gan2021towards} is a challenging variant of the Spider evaluation dataset. Spider-SYN is constructed by manually modifying natural language questions with synonym substitutions.  (3) \textbf{Spider-DK} \cite{gan2021exploring}  is a human-curated dataset based on Spider, which samples 535 question-SQL pairs across 10 databases from the Spider development set and modifies them to incorporate the domain knowledge. (4) \textbf{Spider-Realistic} \cite{deng2020structure} is a new evaluation set based on the Spider dev set with explicit mentions of column names removed, which contains 508 samples. (5) \textbf{Spider-CG(SUB)} and \textbf{Spider-CG(APP)} \cite{gan-etal-2022-measuring-and} are two evaluation datasets to measure the compositional generalization of models, which is constructed by sub-sentence substitution between different
examples and appending a sub-sentence into another sentence separately. (6) \textbf{ADVETA(rpl)} and \textbf{ADVETA(add)} \cite{pi-etal-2022-towards}  are two challenging test datasets for the Spider dataset which are composed of adversarial replacements of column names and the addition of new column names, respectively. (7) 
 \textbf{CSpider} \cite{min2019pilot} dataset is constructed by translating Spider into Chinese, which is the same size as the origin Spider dataset (8) \textbf{DuSQL} \cite{wang2020dusql} is a larger scale Chinese Text-to-SQL dataset with 23,797 question/SQL pairs. (9) \textbf{SParC} \cite{yu2019sparc} and \textbf{CoSQL} \cite{yu2019cosql} are two multi-turn Text-to-SQL dataset with 1625 and 1007 questions in the dev set separately. \\

\noindent\textbf{Evaluation Metrics.}  We mainly adopt three evaluation metrics which are valid SQL (VA), execution accuracy(EX), and test-suite accuracy (TS). Valid SQL (VA) is the proportion of SQL statements that can be executed successfully. Execution accuracy (EX) is the proportion of data where the execution results match the standard SQL statements. Test-suite accuracy (TS) \cite{zhong2020semantic} could achieve high code coverage from a distilled test suite of the database, which is also based on execution. Note that we do not use  the main-stream exact match accuracy, as SQL queries that achieve the same goal can often be expressed in different ways, making it difficult for zero-shot ChatGPT models to achieve high exact match accuracy. \\

 \input{tbls/spider-cg}

\noindent\textbf{Baselines.}  Due to our exclusive reliance on execution-based evaluation, we did not employ baselines such as RatSQL \cite{wang2019rat} and LGESQL \cite{cao2021lgesql}, which generate only SQL skeletons without generating values. Instead, we primarily utilized three baselines: (1) PICARD \cite{scholak2021picard} is a method for constraining auto-regressive decoders of language models through incremental parsing. (2) RASAT \cite{qi2022rasat} introduces relation-aware self-attention into transformer models and also utilizes constrained auto-regressive decoders. (3) RESDSQL \cite{li2023decoupling} proposes a ranking-enhanced encoding and skeleton-aware decoding framework to decouple the schema linking and the skeleton parsing.  Among those, PICARD and RASAT are based on T5-3B \cite{raffel2020exploring} model.
 
\subsection{Main Experiment}

\noindent\textbf{Evaluation on Spider Dataset.} In Table \ref{tab:spider-performance}, we present a comparison between ChatGPT and the current state-of-the-art (SOTA) models. Overall, ChatGPT exhibits a strong Text-to-SQL ability.Despite the 14\% gap in execution accuracy compared to the current SOTA models and a 13.4\% gap in test suite accuracy, it is remarkable that ChatGPT achieved such results in a zero-shot scenario considering that it was not fine-tuned on the Spider training set.\\



\noindent\textbf{Evaluation on Spider-SYN and Spider-Realistic Datasets.} Table \ref{tab:spider-performance} also includes a comparison of ChatGPT's performance on the Spider-SYN and Spider-Realistic datasets.  The main difference between these datasets and the Spider dev set is that they eliminate the explicit appearance of the database schema in the questions. Overall, although ChatGPT still performs well on these two settings, the performance gap between ChatGPT and the original SOTA models becomes slightly larger than that on the Spider dataset. This suggests that the current models have already achieved sufficient robustness in these two scenarios. \\

\noindent\textbf{Evaluation on Spider-DK and ADVETA Datasets.} In Table \ref{tab:spider-dk}, we further compare and analyze ChatGPT's performance on Spider-DK, ADVETA (RPL), and ADVETA (ADD). We find that ChatGPT performs exceptionally well on these datasets, with very small performance gaps compared to the current SOTA models. In fact, ChatGPT outperforms all current SOTA models on ADVETA (RPL). For the Spider-DK dataset, we speculate that ChatGPT's excellent performance is due to its additional knowledge provided by the large-scale pretraining. As for scenarios such as ADVETA, where the dataset's column names undergo adversarial modifications, the poor generalization performance of current models may be due to the significant distribution difference from the original dataset. Overall, ChatGPT exhibits strong robustness in scenarios that require additional knowledge or adversarial modifications are applied to the database column names.  \\

 \input{tbls/multi-turn}

\noindent\textbf{Evaluation on Spider-CG Dataset.} In Table \ref{tab:spider-cg}, we further analyze ChatGPT's ability in the compositional generalization scenario. We found that in Spider-CG (SUB), SQL substructures are replaced to form combinations that do not exist in the training set. In this scenario, ChatGPT even outperforms the original Spider dev set. Even on the more challenging Spider-CG (APP) dataset, ChatGPT achieves strong performance, and the performance gap with SOTA models is relatively smaller than that on the original Spider dataset. Overall, since ChatGPT is a zero-shot model, it is not as affected by compositional generalization as the SOTA models. Overall, zero-shot models have greater advantages in the compositional generalization setting. \\

\input{tbls/chinese}

\noindent\textbf{Evaluation on multi-turn Text-to-SQL scenarios.} Given ChatGPT's strong contextual modeling ability, we further evaluate its performance on multi-turn Text-to-SQL scenarios: \textsc{SParC} and \textsc{CoSQL}. As shown in Table \ref{tab:multi}, ChatGPT exhibits strong multi-turn Text-to-SQL ability. Although there is still a gap compared to the current SOTA models, the gap is relatively smaller compared to the single-turn Spider dataset. Meanwhile, ChatGPT also performs better on CoSQL datasets with more average interactions, which also indicates that ChatGPT's strong contextual modeling ability is very helpful for multi-turn Text-to-SQL. \\

\noindent\textbf{Evaluation on Chinese Text-to-SQL scenarios.} We further evaluate ChatGPT's Text-to-SQL ability on other languages in Table \ref{tab:chinese}. The experiments are mainly conducted on two datasets, CSpider and DuSQL, where only the questions are in Chinese for CSpider and both the schema names and questions are in Chinese for DuSQL. The results show that while ChatGPT performs well in the Chinese Text-to-SQL scenario, there is still a performance gap compared to the English Text-to-SQL scenario. Moreover, the performance is even worse when the table names and column names are also in Chinese, with a large number of generated SQL queries being non-executable and a lower execution accuracy. This suggests the cross-lingual generalization ability of ChatGPT requires further improvement.

\subsection{Case Study}

\input{tbls/case}

In Table 6, we present four typical prediction errors made by ChatGPT on the Spider dev dataset. The first error case shows that ChatGPT tends to design JOIN statements more finely by using LEFT JOIN, but this level of granularity is not present in the original Spider dev dataset. The second error case arises from ChatGPT's confusion regarding the database structure, and it is not clear which column the term "full name" specifically refers to. The third example's error was due to the generated SQL statement lacking correct semantic interpretation, resulting in incorrect output for the "where" clauses with nested SQL statements.  The fourth case of error is due to errors in copying specific values, where the case sensitivity of the original value was not preserved when regenerating the value.

In summary, ChatGPT's errors mostly occur in small details, and some of these issues can be addressed and improved in later stages of development, such as in the first, third, and fourth cases. However, for errors like the second case, which indicate a lack of understanding of the database schema, further improvements to the model's ability may be necessary to resolve them.














