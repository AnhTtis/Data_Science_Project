\section{Discussion and Conclusions}

The retrospectively collected dataset in our study consisted of wound images taken by medical personnel for the purpose of medical documentation. Such documentation does not impose strict requirements on imaging conditions such as lighting and image capturing devices. Although not as perfect as prospectively collected images with standardized conditions, these images were much closer to images captured in real-world scenarios. The deep learning model derived from these real-world images showed a significantly better or non-inferior performance compared with other medical personnel on all five tasks. Our model should be suitable for further application in primary medical facilities with no strict limitations on imaging devices and conditions.

The proposed deep learning model performed well in classifying arterial, venous, and pressure wounds. It exhibited an accuracy and AUC of well above 85\%. The performance is not as profound in the tasks of classifying deep wound and infected wound, with an accuracy of less than 75\% and AUC between 75\% and 80\% for both tasks. It should be noted that in the task of venous wound, our model exhibited a low sensitivity value of 63.2\% and yielded a wide ROC confidence band, as shown in Figure~\ref{fig:Figure4}. This is due to the highly imbalanced nature of our venous wound dataset. There were only 52 positive cases of venous wound in our entire dataset. Aside from the low sensitivity in venous ulcer, we can also observe that the accuracy, specificity, and AUC of the model is usually higher than the sensitivity in all five tasks. This is due to the relatively smaller number of positive cases compared to negative cases in the training data for all tasks. The model learns from the relatively larger negative cases and can identify the negative cases more successfully.



The seven human subjects exhibited an accuracy of less than 75\% in the tasks of deep wound and infected wound. It can be observed that it is difficult for a human to perform these two tasks using only an image. The same trend is observed in the performance of the proposed model in the tasks of deep wound and infected wound. Among all of the medical personnel, resident B, who is the most junior among doctors, has the worst performance on average and performs a large performance gap compared to others on the infected wound and venous ulcer. It shows the difficulty of identifying those wound problems using only visual clues and the importance of long time-training for experienced wound experts. A comparison of the proposed model and human medical personnel using Cohenâ€™s kappa difference indicates the promising performance of the proposed model. The proposed model performed better than three out of seven human medical personnel in the tasks of deep wound; four out of seven in the tasks of infected wound; two out of seven in the tasks of arterial and venous; one out of seven in the tasks of pressure wounds. In summary, when compared with human medical personnel, our model performed significantly better than or is non-inferior to all the medical personnel in all the five wound classification tasks.

Our model not only performs well in wound classification but does not require manual feature selection, as it is based on an end-to-end deep learning architecture~\cite{ref5_wang2015unified,ref6_shenoy2018deepwound,ref7_goyal2018dfunet,ref8_zahia2018tissue,ref22_nilsson2018classification,ref23_ootadeep}. A comparison of our proposed model with other deep learning-based methods is presented in Electronic Supplementary Material. Our model either clearly exhibited the best performance (with p-value < 0.05) compared with the other methods or is among the best methods. The proposed model size is very compact, which is advantageous in terms of the computation resource requirement.  

\begin{figure*}[t!]
\centerline{
	\hspace{3mm}\includegraphics[width=0.9\textwidth]{figures/Figure4.pdf}
 	% \vspace{-2mm}
}  
    \caption{ROC curve of our model, and sensitivity and specificity characteristic points of medical personnel.}
    \vspace{-2mm}
	\label{fig:Figure4}
\end{figure*}

\begin{figure}[t!]
\centerline{
	\hspace{3mm}\includegraphics[width=1.0\columnwidth]{figures/Figure5.pdf}
 	% \vspace{-2mm}
}  
    \caption{Potential future clinical application. A user can take a photograph and our
wound assessment model will display the analysis results.}
    \vspace{-2mm}
	\label{fig:Figure5}
\end{figure}

In addition to the deep learning-based wound classification model, we plan to deploy such a core model to a web service prototype. The web service consists of a back-end computer server and a front-end smartphone app, as shown in Figure~\ref{fig:Figure5}. We plan to use such a prototype to study the applicability of the wound classification model in telemedicine. To perform wound assessment, a user can take a photograph of the wound using a smartphone. The wound image is then uploaded via a web service and fed to our deep learning-based wound classification model. The classification results are sent back to the mobile end and presented on the user interface of the mobile app. This mobile app can be extremely helpful and could be widely adopted by non-wound care medical personnel. In the future, we plan to extend our model to include more wound classification tasks to extend its scope. Patient information, such as vital signs and laboratory test results, will also be included to further improve our model performance.

The main limitation of our study is the generalizability of the proposed model. Although our various non-strict imaging conditions in capturing retrospective data already provide some generalizability for the proposed model in terms of coping with different imaging conditions, its prospective use may require some fine tuning with more data, particularly data captured under more diverse imaging conditions in different settings. Another limitation is the model uses only the wound images to classify the wound tasks which might omit some other non-visual information. We plan to incorporate additional information from the patient such as pain degree and temperature around the wound into the model to improve the model capacity.

In conclusion, we investigated a compact multi-task deep learning framework to classify deep wound, infected wound, arterial wound, venous wound, and pressure wound. The performance of the proposed model is good and is significantly better or non-inferior to all of the human study participants.

