\section{Proposed Method}\label{sec:method}
\subsection{Patients and datasets}\label{ssec:patients_datasets}
Our dataset consists of 2149 wound images collected from 1429 patients randomly selected from the database of the corresponding author’s affiliated hospital. The 1429 patients consist of 900 men and 529 women with a mean age of 62.6 years and a standard deviation of 18.5 years. This study was approved by the Institutional Review Board (approval number: 2020-08-008A). The wound images in our dataset were taken by medical personnel under various imaging conditions such as lighting, capturing devices, shooting angles, resolutions, and backgrounds (some with irrelevant surrounding objects). Five classification tasks were included (Figure~\ref{fig:Figure1}): deep wound, infected wound, arterial wound, venous wound, and pressure wound. All the wound images were reviewed by a medical wound specialist. Each task is a binary classification with either a negative or a positive label. A negative label in deep wound indicates no wound or shallower than deep fascia; a positive label in deep wound indicates that the wound is deeper than deep fascia. For the other four tasks, a negative label indicates the absence of such symptoms. In designing the deep learning model as well as in the experiments, we partitioned the dataset by patients, and not images, because a patient may have more than one wound image in the dataset. That is, some patients were used as training subjects, whereas the rest were used for testing. The dataset was randomly shuffled and divided into training (80\% of all patients) and testing (20\% of all patients) datasets. Table~\ref{table:datasets} presents the data distributions of all the images and for each task.


\begin{figure}[t!]
\centerline{
	\hspace{3mm}\includegraphics[width=1.0\columnwidth]{figures/Figure1_no_title.pdf}
 	% \vspace{-2mm}
}  
    \caption{Some sample images from our dataset. A: superficial wound, B: deep wound, C: infected wound, D: arterial wound, E: venous wound, F: pressure wound.}
    \vspace{-2mm}
	\label{fig:Figure1}
\end{figure}




\subsection{Model architecture and implementation details}\label{ssec:model architectur}
A conventional machine learning-based method requires two steps to classify an image. The first step is to find the relevant segment from the image or perform feature extraction. Next, a classifier, such as a support vector machine, performs classification on the extracted image segment or features. Alternatively, deep learning-based approaches learn features and construct classifiers within their deep learning model without explicitly separating the steps. Our proposed method belongs to the latter category. 

In this study, we build a DL-based CNN model to perform five wound classification tasks. A CNN-based classification model performs steps of feature extraction from the original input image to gradually extract lower-level features such as shapes and texture to higher-level features such as semantic information, all automatically learned from the training data. The extracted features are then used to classify the input image.

It is possible to design five separate deep learning models to tackle the task separately. However, this could be expensive in terms of computation and memory requirements. We observed that these tasks may share meaningful features, such as skin texture or localization information of the wound. To leverage the intrinsic relationships among them, we designed a CNN-based model that learns these five tasks simultaneously and also provides the results simultaneously. There are many different ways to share features among these tasks~\cite{ref9_ruder2017overview,ref10_misra2016cross,ref11_liu2019end,ref12_gao2019nddr,ref13_meyerson2017beyond,ref14_sener2018multi,ref15_zhang2021survey,ref16_he2016deep,ref17_deng2009imagenet}. Figure~\ref{fig:Figure2} shows an overview of our wound classification model. Following~\cite{ref11_liu2019end}, all five tasks in our model shared the same backbone model with their task-specific branches. Those task-specific branches are composed of attention modules that imitate the human attention mechanism. They help each task branch extract its critical features. In addition, we aggregate different levels of information to improve the final classification performance.

\input{tables/Table1_datasets}


To address the class imbalance problem in our dataset, we used the weighted cross-entropy loss as the loss function to optimize our model. The smaller the data size of a class, the larger the class weight, so that the model would not be biased to a class because of its data size. To increase the robustness of the model and reduce overfitting to certain imaging conditions, we applied training image data augmentation by performing realistic affine transformations, vertical or horizontal flipping, brightness altering, and contrast adjustments. This augmentation strategy is to mimic the different imaging conditions which are beneficial to model learning. The augmentation is carefully chosen and will not produce unrealistic data. For example, real images are sometimes upside-down or taken from a different angle. A human can rotate the image manually or can identify the image regardless of the upside-down. To imitate this ability, we apply vertical flipping to the original image and use both versions of images to train the model during the training process. The model will learn from both images to become more robust and knows that the upside-down is irrelevant to the disease. Note that we do not perform augmentations on testing data. Given the varying original image sizes, we manually cropped them to a square shape and resized them to the minimal size of all cropped images, which is 300 × 300 pixels. 

\begin{figure}[t!]
\centerline{
	\hspace{3mm}\includegraphics[width=1.0\columnwidth]{figures/Figure2.pdf}
 	% \vspace{-2mm}
}  
    \caption{Overview of our wound assessment model framework. The grey blocks include the shared backbone blocks. The purple regions contain task-specific attention blocks. Each task has one branch; the model has a total of five individual task branches. We only showed two branches here for clarity. Each task branch uses task-specific attention blocks to extract shared features from shared backbone blocks. The task branch outputs feature maps from different levels and merge them into fused task-specific features. The classifier then accepts the features and outputs the prediction.}
    \vspace{-2mm}
	\label{fig:Figure2}
\end{figure}

\subsection{Model evaluation and statistical analysis}\label{ssec:Model evaluation}

To evaluate the effectiveness of our proposed deep learning model, we used some performance evaluation metrics from descriptive statistics, including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC). AUC is particularly effective for assessing classifiers trained with highly imbalanced data. Given the nature of our data, we included this metric in addition to the rest of the performance evaluation metrics. 

Furthermore, it is worthwhile to compare the clinical performance of the proposed deep learning model with that of medical personnel.  Seven human subjects participated in this study, including two surgical attendings specialized in wound care, one senior surgical resident, one junior resident, one nurse practitioner with wound specialty, and two registered nurses of the surgical ward. In the experiments, we labeled them as attending A, attending B, resident A, resident B, nurse A, nurse B, and nurse C, respectively. We randomly selected 350 images from the test dataset with a similar distribution as the test set for all the experiments, including the deep learning model and seven human medical personnel. This is referred to as a smaller test dataset “human test dataset”. The medical personnel were provided with images in the human test dataset and were asked to answer if each image is positive for all five wound conditions. To compare the performance of the proposed deep learning model with those of human subjects, we used the accuracy, sensitivity, specificity, and Cohen’s kappa coefficient~\cite{ref18_mchugh2012interrater}. Cohen’s kappa coefficient is commonly used to measure the rater reliability for categorical items. Cohen’s kappa coefficient was calculated with respect to the ground truth labeled by the medical wound specialist. The higher the Cohen’s kappa coefficient, the better the performance.

We used the difference in the Cohen’s kappa coefficient values (the proposed deep learning model minus medical personnel) with their 95\% confidence interval (CI) as the metric for the comparison. The 95\% CI is calculated using bootstrapping~\cite{ref19_efron1994introduction}. The proposed model is significantly better if such a difference is positive and its CI does not cross zero. It is non-inferior if the CI crosses zero, and inferior otherwise. 
We plotted the receiver operating characteristic (ROC) curve~\cite{ref20_mcclish1989analyzing} of our model and calculated the 95\% confidence band around the ROC curve using vertical averaging~\cite{ref21_provost1998case}. The characteristic points of sensitivity and specificity for all medical personnel were calculated and plotted together with the model ROC curve in the same figure for comparison. A side-by-side comparison of the characteristic points and the ROC curve provides another illustrative method for comparison.

