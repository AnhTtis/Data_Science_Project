\documentclass[accepted]{uai2023} % for initial submission
% \documentclass[accepted]{uai2023} % after acceptance, for a revised
                                    % version; also before submission to
                                    % see how the non-anonymous paper
                                    % would look like
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2023} % ptmx math instead of Computer
                                         % Modern (has noticable issues)
% \documentclass[mathfont=newtx]{uai2023} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath,amsfonts}
\usepackage{array}
\usepackage{multirow}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{booktabs} % for
\usepackage{wrapfig}
%professional tables
\newtheorem{definition}{Definition}[section]
\newtheorem{propositions}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{observation}{Observation}
\newtheorem{assumption}{Assumption}[section]
\newtheorem{usecase}{Use Case}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\N}{\mathcal{N}}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{color, colortbl}
\usepackage{natbib}
\defcitealias{sundararajan2017axiomatic}{S 2017}
\defcitealias{causalattributions}{AC 2019}
\defcitealias{kancheti2021matching}{SK 2022}
\newcommand{\CC}[1]{\cellcolor{gray!#1}}

\definecolor{Gray}{gray}{0.9}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example

%\title{An Ante-hoc Approach to Learning Causal Attributions in Neural Networks}
\title{Learning Causal Attributions in Neural Networks: Beyond Direct Effects}

% The standard author block has changed for UAI 2023 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
\author[1]{\href{mailto:cs19resch11002@iith.ac.in}{Abbavaram Gowtham Reddy}{}}
\author[1]{Saketh Bachu}
\author[1]{Harsharaj Pathak}
\author[1]{Benin L Godfrey}
\author[1]{\\Vineeth N. Balasubramanian}
\author[2]{Varshaneya V}
\author[2]{Satya Narayanan Kar}
% Add affiliations after the authors
\affil[1]{%
    Indian Institute of Technology Hyderabad, India
}
\affil[2]{%
       Honeywell, Bengaluru, India 
}
  
  \begin{document}
\maketitle

\begin{abstract}
\vspace{-13pt}
There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. 
We also propose effective approximation strategies to quantify causal attributions in high-dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
%We study the attributions in Neural Networks (NNs) from a causal perspective. Existing causal attribution methods for NNs suffer from two critical problems: (1) the impossibility of learning and quantifying indirect causal attributions of inputs on output and (2) the impracticality of quantifying causal attributions when input features are real-valued and high dimensional. In this paper, we provide practical solutions to both problems. Viewing an NN as a structural causal model (SCM), we highlight the need for learning indirect causal attributions. Assuming access to the partial causal graph, we propose an ante-hoc attributions method to learn both direct and indirect causal attributions. 
\end{abstract}

\vspace{-13pt}
\section{Introduction}
\vspace{-10pt}
\label{introduction}

There has been a significantly growing interest in integrating perspective of causality into neural network (NN) models in recent years. Causal perspectives provide a possible pathway to achieve robustness, invariance and meaningful explanations in NN models \citep{scholkopf2021toward,icm, rim}, especially in risk-sensitive and safety-critical applications. One specific dimension that has received attention is viewing NN model explanations/attributions with a causal lens. These include efforts to understand causal attributions in NN models~\citep{alvarez2017causal,causalattributions, acausalproblem,causalexplanationblackbox}, as well as regularization methods that maintain causal attributions~\citep{bahadori2017causal,janzing2019causal,kyono2020castle,kancheti2021matching}.

%Using the ideas of causal inference in deep learning is useful in various applications, such as understanding causal attributions in neural networks (NNs)~\citep{alvarez2017causal,causalattributions, acausalproblem,causalexplanationblackbox}, regularizing NNs to use prior knowledge~\citep{bahadori2017causal,janzing2019causal,kyono2020castle,kancheti2021matching}, learning independent causal mechanisms~\citep{icm, rim,scholkopf2021toward}, etc. In this paper, we study attributions in NNs from a causal perspective using a first-principles approach. 

Interpretability and explainability of NN models has seen a wide variety of methods in the last few years~\citep{dovsilovic2018explainable,molnar2022,Samek2019TowardsEA}; however, most such methods have focused on model gradients or input perturbations to compute input-output attributions. Recently, explanations based on the notion of \textit{causal attribution/causal effect} have gained more attention as causal explanations are more reliable~\citep{causalattributions,goyal2019counterfactual,Hendricks_2018,causalexplanationblackbox,wachter}, and can also help in debugging~\citep{lmdebugger}, or improving an NN model's performance~\citep{kyono2020castle, kancheti2021matching}.

%Due to its \textit{black box}~\citep{lipton2018mythos} nature, there is no clear way to know the reasoning behind a given NN model's predictions, which is crucial for its applicability in safety-critical applications. Several algorithms have been proposed for explaining NNs ranging from occlusion experiments~\citep{zeiler2014visualizing} to attribution-based methods~\citep{sundararajan2017axiomatic,causalattributions,dovsilovic2018explainable,molnar2022,Samek2019TowardsEA}. Recently, explanations based on the notion of \textit{causal attribution/causal effect} have gained more attention as causal explanations are more reliable in understanding~\citep{causalattributions,goyal2019counterfactual,Hendricks_2018,causalexplanationblackbox,wachter}, debugging~\citep{lmdebugger}, and improving an NN performance~\citep{kyono2020castle, kancheti2021matching}.

Almost all existing efforts towards causal explanations (except for \citep{kancheti2021matching}) are post-hoc in nature i.e., they take an already trained NN model and explain the causal attributions of input features (neurons in the first layer)\footnote{We use \textit{input features} and \textit{input neurons} interchangeably} on the output (neurons in the last layer). Apart from being a post-hoc assessment, these methods suffer from another significant limitation -- \textit{due to the lack of connections between input neurons in NN models, input features are assumed to be independent}. That is, when we view a standard NN model as a causal graph that captures causal relationships between input features and the output via a sequence of connections through the hidden layers, it is evident that input features are not causally related~\citep{causalattributions,acausalproblem}. Hence, causal attributions thus obtained are able to only capture direct effects (i.e. direct input-output attributions that do not flow via other input features). Indirect effects (attributions that flow via other input features -- see Appendix~\ref{preliminaries} for preliminaries on causality) in this case are simply zero for all features, which may not represent the true indirect causal effects in the underlying data. In other words, existing post-hoc causal explanation methods therefore cannot explain causal relationships among input features in the real-world~\citep{acausalproblem}. While \citep{kancheti2021matching} is an ante-hoc approach, this work too makes the assumption of causally independent inputs, and hence does not model indirect effects. % (We note that real-world causal graph is different from the causal graph induced by an NN)

In order to understand the need to go beyond direct effect and model indirect effects, as a motivating example, consider the task of predicting an individual's \textit{income} given features such as \textit{education}, \textit{socio-economic status}, etc. \textit{Socio-economic status} often causally influences \textit{education} in the real world. However, \textit{socio-economic status} does not causally influence \textit{education} in an NN model because there is no edge from \textit{socio-economic status} to \textit{education} in the first layer of the NN. Hence, we can learn and explain only \textit{direct causal attributions} but not the \textit{indirect causal attributions} of \textit{socio-economic} on the \textit{income}~\citep{acausalproblem}, although it exists in the data. In this work, We propose an approach to learn indirect causal attributions in ante-hoc manner while training an NN model.

To obtain causal attributions,~\citep{kocaoglu2017causalgan,causalattributions,acausalproblem} propose to view an NN as a Structural Causal Model (SCM). An SCM represents the causal relationships among a set of variables in a functional form, which can also define the causal graph~\citep{pearl2009causality}. Since our objective is to study input-output attributions, following ~\citep{kocaoglu2017causalgan,causalattributions,kancheti2021matching}, we marginalize the hidden layers of an NN and view the output of an NN solely as a function of its inputs (see Fig~\ref{fig:example} (a)).
%To study the causal attributions of input neurons on output neurons in an NN model, we do not need to consider the interactions between neurons in hidden layers. Hence, similar to earlier work~\citep{kocaoglu2017causalgan,causalattributions,kancheti2021matching}, we marginalize the hidden layers of an NN and view the output of an NN solely as a function of its inputs (see Fig~\ref{fig:example} (a)).
\begin{figure}
    \centering
    \includegraphics[scale=0.09]{images/example.png}
     \vspace{-7pt}
    \caption{\footnotesize (a) NN SCM whose inputs $x_1, \dots, x_n$ are not causally related(b) We introduce connections (e.g., $x_1 \rightarrow x_i$) among input features to capture underlying causal relationships (e.g., $x_1$ causes $x_i$) to learn the indirect causal attributions of inputs on $y$. Note that hidden layers are marginalized to estimate the causal attributions of inputs on output.}
    \label{fig:example}
    \vspace{-18pt}
\end{figure}
Evidently, as stated earlier, the SCM captured by a traditional NN model does not have connections among the input features, and hence cannot model indirect effects. In this work, we focus on advancing methods for causal attributions in NN models by also modeling indirect effect, by including relationships between input features (Fig~\ref{fig:example}(b)). To the best of our knowledge, this is the first such effort to capture indirect causal effects in NN models. In this process, we also provide efficient implementation strategies that can speed up the computation of causal attributions significantly.

%(e.g., a multi-layer perceptron) has causally unrelated input neurons. This makes studying indirect causal attributions impossible with such models~\citep{acausalproblem} (in other words, indirect attributions don't exist). For example, consider Fig~\ref{fig:example}(a) in which inputs $x_1,\dots,x_n$ of a marginalized NN are not causally related to each other i.e., no causal arrows among $x_1,\dots,x_n$. Hence, it is impossible to learn and explain the indirect causal attributions of input features on output (indirect attributions will trivially be zero for all input features). To overcome this problem, our ante-hoc causal explanation method is trained to include such relationships among input features (Fig~\ref{fig:example}(b)).

%p6
%Another limitation of existing causal attribution methods is that they are prohibitively time-consuming. To understand further, consider the following equation for evaluating the causal attributions (see Def~\ref{def: ACE}) of an input feature $x_i$ on the output $y$ of an NN: 
% \begin{equation*}
% \vspace{-3pt}
% ACE^y_{do(x_i = \alpha)} = \E[y|do(x_i = \alpha)] - \E[y|do(x_i = b)]
% \vspace{-3pt}
% \end{equation*}
% where $do(x_i=\alpha)$ denotes the intervention on feature $x_i$ with the value $\alpha$. $b$ is the baseline intervention value relative to which causal attribution is computed. To evaluate the expression $\E[y|do(x_i = \alpha)]$ from observational data, we need to marginalize over a sufficient adjustment set $W$~\citep{pearl2009causality}. That is, $\E[y|do(x_i = \alpha)] = \E_{w\sim W}\E[y|x_i = \alpha,W=w]$. If $W$ has many features and they are real-valued, evaluating $ACE^y_{do(x_i = \alpha)}$ is prohibitively time-consuming. A second-order approximation to the expression $\E[y|do(x_i = \alpha)]$ has been considered for providing causal explanations in~\citep{causalattributions}. However, such an approximation requires computing interventional means and interventional covariances, which again suffers from time complexity issues if the input is high-dimensional (see Sec~\ref{Section: causal effects in NNs}). We propose practical solutions that are viable for data with high dimensions and to efficiently store and retrieve interventional data while computing attributions.  

%In this work, we seek to address these limitations by providing an ante-hoc (inherently interpretable during training) approach to learning causal attributions in NN models. Our work also addresses the growing call for intrinsically interpretable ante-hoc methods over post-hoc explanation methods in recent years~\citep{rudin2019stopexplaining,rudin2021interpretableml}. 
Our key contributions can be summarized as follows.
\begin{itemize}[leftmargin=*]
\vspace{-3pt}
 \setlength\itemsep{-0.2em}
    \item We highlight the need to model indirect effects while capturing causal attributions in NN models, and provide a methodology to capture such effects in an NN. Importantly, our work is an ante-hoc explainability method, and addresses the growing call for intrinsically interpretable methods over post-hoc explanation methods ~\citep{rudin2019stopexplaining,rudin2021interpretableml}%emphasize the need for learning and explaining indirect causal attributions of inputs on output in an NN and hypothesize that NNs should contain lateral edges across input features to learn and quantify indirect causal attributions. 
    \item We provide a training methodology that introduces lateral connections between input neurons in an NN model to learn both direct and indirect causal attributions of inputs on the output. This is the first such effort to the best of our knowledge. %We also propose an algorithm to quantify the causal attributions in the proposed ante-hoc explanation model.
    \item We also present effective implementation strategies to scale causal attribution methods to high-dimensional data w.r.t. time and space complexity.
    \item We present a wide range of empirical results on both synthetic and real-world datasets to showcase the usefulness of the proposed method.
\end{itemize}  

\vspace{-11pt}
\section{Related Work}
\vspace{-7pt}

%\noindent \textbf{Explainability:}
Beyond providing transparency of decision-making, explaining NN models has been shown to be helpful in various tasks including identifying hidden biases in data~\citep{survey, alvarez2017causal}, uncovering its fairness~\citep{survey}, debugging machine learning models~\citep{survey,lmdebugger} and model improvement through explanation-based regularizers~\citep{survey,ross2017right, rieger2019interpretations,kancheti2021matching}. Most existing NN model explanation methods quantify effect of input features on model output using saliency maps~\citep{zeiler2014visualizing,simonyan2013deep,selvaraju2016grad}, local model approximations~\citep{ribeiro2016should}, approximations of gradients of output w.r.t. inputs~\citep{sundararajan2017axiomatic,smilkov2017smoothgrad},Shapley values~\citep{shapley,causalshapley}, etc. In this work, we focus on causal attributions of input features on  output in an NN model, which can be very useful in safety-critical domains such as healthcare, aerospace, law, and defense.

\noindent \textbf{Causal Explanations:}
Viewing an NN as an SCM, and assuming that the treatment feature (feature w.r.t. which causal attribution is calculated) is $d$-separated from other input features upon intervention,~\citep{causalattributions} proposed a post-hoc causal explanation method to find the average causal attributions of a trained NN. However, assuming independence across inputs hinders us from considering indirect causal effects. Subsequent work such as~\citep{khademi2020causal,yadu2021icip,wang2021contrastive,cxplain,goyal2019explaining} follows the definition of ACE defined therein to quantify the learned average causal effect of an NN. These methods divide causal explanations broadly into global and local explanations, where global explanations estimate average causal effect and local explanations estimate individual causal effect. Gradient-based methods can also be viewed as individual causal attribution methods where intervention to an input neuron is similar to finding direct causal attribution of the input~\citep{kancheti2021matching}. For example, given a function $f$, $f(x_1, x_2, ..., x_i=\alpha, .., x_n) - f(x_1, x_2, ..., x_i=\alpha - \epsilon, .., x_n)$ is equal to individual causal attribution of the feature $x_i$ on the output, provided that $x_i$ doesn't cause any other feature~\citep{causalattributions,kancheti2021matching}.% in global explanations and individual causal attributions in local explanations. 

Other causality-based explanation methods use counterfactuals~\citep{pearl2009causality} to study model behavior under semantically meaningful changes to inputs~\citep{surveyrecourse,goyal2019counterfactual,verma2020counterfactual,wachter,multiobjective,concepts,mothilal2021towards,mahajan2019preserving}. Counterfactuals, which are computed for individual instances, allow us to study the behavior of a model when a specific aspect of input (e.g., the color of an object) is changed while keeping all other aspects to be naturally followed from the respective causal mechanisms~\citep{pearl2009causality}. Counterfactual explanation methods often optimize an objective to find the minimum change to an input that alters the model's decision. However, such methods are often used for qualitative analysis of a model than computing causal input-output attributions.%This way of modifying inputs allows us to know the discriminative features used by a model to arrive at a decision. 


% It should be "Another popular causality-based explanation method is to use counterfactuals"

% We should write "(e.g., changing the color ... to black)" after "is changed"
% We may say "keeping all other aspects as they naturally follow from the ..."

\noindent \textbf{Beyond Independence Assumption on Input Features:}
Considering this work's focus is on going beyond direct effect, we observe existing literature in computing NN model attributions that explicitly study input variable interactions. The most well-known such efforts include  explanations via Shapley values~\citep{shapley,causalshapley}. However, these methods do not consider causal attributions learned by the NN. %the true causal graph or its relationship to the causal graph of the SCM corresponding to the NN. 
For e.g., when handling missing features in Shapley explanations, sampling from the conditional distribution (rather than the marginal distribution) is discouraged as the inputs are independent w.r.t. the causal graph of the NN~\citep{acausalproblem}. We address input feature interactions while computing causal attributions in this work. %However, to capture the relationships among input features, our explanation model has feedforward connections among input features. 
Such an approach is essential for estimating and maintaining indirect effects in an NN model. While ~\citep{kancheti2021matching} discuss the different effects that are important for NN model attributions, it focuses on estimating direct effects (such as natural and direct).
%Also, when NNs are regularized to incorporate different causal effects~\citep{kancheti2021matching}, it is crucial to quantify the indirect causal attributions to validate the learned effects. We propose an algorithm to quantify the learned direct and indirect causal attributions by extending very little work that quantifies only direct causal attributions~\citep{causalattributions}.

The work closest to ours is in ~\citep{genderbias}, which studied direct and indirect causal attributions in Transformer~\citep{transformer}-based language models for capturing gender bias. This was a post-hoc analysis of such models for a different objective, while our proposed method is an ante-hoc approach to learn direct and indirect effects as causal attributions while training an NN model. %are considered in~\citep{genderbias}. However, similar to earlier work,~\citep{genderbias} is also a post-hoc explanation method. Our method, on the other hand, can learn both direct and indirect attributions during training.

\noindent \textbf{Learning Structural Causal Models:}
The other dimension of existing efforts that can be considered related to our work are those that learn structural equations of an underlying causal model, which have been proposed for causal discovery~\citep{goudet2018learning}, causal effect identification and estimation~\citep{xia2021causal}, or counterfactual instance generation~\citep{deepscm}. However, the objective of each of these efforts is different from ours. We learn ad leverage the structural equations among input features to estimate direct and indirect causal attributions of input features on NN outputs.% by learning the structural equations among input features. 
 
Additional discussions on related topics such as concept-based explanations are in the Appendix.

% Concept-based explanation models attempt to explain an NN behavior in terms of semantically meaningful concepts in the input when the input is not readily expressed as a discrete set of concepts. Concept bottleneck models~\citep{cbmodels}, and concept embedding models~\citep{cemodels} learn the relationship between manually annotated concepts in the input samples and the model predictions.
% % so that apart from having an interpretation for the prediction the predicted concept probabilities at test time can be manually intervened upon to improve test accuracy. 
% Linguistic concept-based explanation models such as~\citep{flexmodel,ccnngc} try to automatically discover semantically meaningful concepts in the input space by leveraging text annotations corresponding to the inputs and using them to explain model predictions. Other methods like TCAV~\citep{tcav} can be used as a post hoc method to explain model predictions in terms of arbitrary concepts that may exist in the input samples but not known at the time of training. ~\citep{eecv18} is a method to decompose the input samples into semantically interpretable concepts derived from a pre-trained concept corpus. However, as discussed earlier, when the number of concepts is more, evaluating the causal attributions is time-consuming.
\vspace{-9pt}
\section{Causal Attributions in Neural Networks: Preliminaries}
\label{Section: causal effects in NNs}
\vspace{-8pt}
Let $X=\{x_1,x_2\dots,x_n\}$ be the input to an NN $f$, and $y$ be any output neuron in the final layer of $f$. $f$ can be viewed as a directed acyclic graph (DAG) with directed edges from one layer of neurons to the next layer of neurons. The output of $y$ of $f$ can thus be viewed as a result of the sequence of interactions from the first layer to the final layer. Moving onto a lower level of abstraction, we view $f$ as an SCM $\mathcal{S}$ that encodes the interactions among inputs and outputs of $f$, which is formalized as follows.

\vspace{-2pt}
\begin{definition}({Structural Causal Models}~\citep{pearl2009causality}). A Structural Causal Model $\mathcal{S}$ is a 4-tuple $(X, U, F, P_u)$ where: (i) $X$ is a finite set of endogenous variables (analogous to the input $X$ of an NN); (ii) $U$ is a finite set of exogenous variables; (iii) $F$ is a set of functions $[f_1, f_2,....f_n]$, where n is the cardinality of the set $X$, that define causal mechanisms such that $\forall  x_i \in X, x_i = f_i(Pa(x_i),u_i)$. The set $Pa(x_i)$ is a subset of $X\setminus x_i$ denoting the parents of $x_i$ in the underlying causal graph and $u_i \in U$; (iv) $P_u$ defines a probability distribution over exogenous variables $U$.
\label{definition:SCM}
\end{definition}

\vspace{-7pt}
\begin{restatable}[\citep{causalattributions}]{propositions}{propositionone}
An $n$-layer feedforward neural network $\mathcal{N} (l_1, l_2, ...l_n)$ where $l_i$ is the set of neurons in layer $i$, has a corresponding SCM $\mathcal{S}([l_1, l_2, ...., l_n], U, [F_1, F_2, ... F_n], P_U)$, where $l_1$ is the input layer and $l_n$ is the output layer. Corresponding to every $l_i$, $F_i$ refers to the set of causal mechanisms for neurons in layer $i$. $U$ refers to a set of exogenous variables that act as causal factors for the input neurons $l_1$. 
\label{prop: NN as SCM}
\end{restatable}
\vspace{-6pt}

When considering the causal effects of inputs on the output of an NN, only the neurons in the first layer and the final layer are considered. W.l.o.g., we reduce the causal structure to an SCM $\mathcal{S}'([l_1, l_n], U, F', P_U)$ by marginalizing the hidden layers.

\vspace{-2pt}
\begin{restatable}[\citep{causalattributions}]{corollary}{corollaryone}
Every $n$-layer feedforward neural network $\mathcal{N} (l_1, l_2, ...l_n)$, with $l_i$ denoting the set of neurons in layer $i$, has a corresponding SCM $\mathcal{S}([l_1, l_2, ...., l_n], U, [F_1, F_2, ... F_n], P_u)$ which can be reduced to an SCM $\mathcal{S}'([l_1, l_n], U, F', P_U)$. 
\label{corr: recursive_subs}
\end{restatable}
\vspace{-4pt}

We refer the interested reader to \citep{causalattributions} for detailed proofs (we also present these in the Appendix for completeness). 
%We defer all the proofs to Appendix~\ref{proofs}. 
We note that the SCM $\mathcal{S}$ of an NN $f$ may not be equal to the underlying SCM $\mathcal{S}^*$ that generated the training data. We also note that the exogenous variables $U$ in the SCM $\mathcal{S}$ of an NN $f$ are not part of the NN $f$ and indicate potential correlations among input variables, thus helping to formalize the notion of the SCM of an NN. %~\citep{causalattributions} also proposed a similar construction technique to view Recurrent Neural Networks (RNN) as SCMs. 
Having defined the SCM of an NN, we now define the causal attributions of input features on the output learned by a trained NN.

\vspace{-2pt}
\begin{definition}(Average Causal Effect and Causal Attribution).
The \textit{Average Causal Effect (ACE)} of a binary input feature $x_i$ on an output neuron $y$ of an NN is defined as $ACE^y_{x_i} = \E[y|do(x_i = 1)] - \E[y|do(x_i = 0)]$. $ACE_{x_i}^y$ is defined as the causal attribution of $x_i$ on $y$.
\label{def: ACE}
\end{definition}
\vspace{-7pt}
While Defn~\ref{def: ACE} pertains to binary-valued features, inputs of an NN usually are continuous and real-valued. Following~\citep{causalattributions}, for an NN with input layer $l_1$ and output layer $l_n$, we measure the $ACE_{x_i}^y$ at the intervention $do(x_i=\alpha)$ relative to a baseline intervention $do(x_i=b)$ as: 
\vspace{-5pt}
\begin{equation}
ACE^y_{do(x_i = \alpha)} = \E[y|do(x_i = \alpha)] - \E[y|do(x_i = b)]
\label{eq: ACE}
%\vspace{-3pt}
\end{equation}

\vspace{-12pt}
\section{Learning Direct and Indirect Causal Attributions in Neural Networks}
\vspace{-7pt}

\label{sec:dependent_features}
In this section, we introduce our methodology to learn causal attributions in neural networks, with a focus on learning indirect effects. This is an ante-hoc methodology where we seek to imbue NN models with the capability to maintain known causal attributions, including indirect effects. To this end, we present the formal definitions of direct and indirect causal effects, and then present our key hypothesis for learning indirect causal attributions. %We discuss our hypothesis in relation to recent literature on causal attributions. 
We begin with one assumption we make w.r.t. the underlying causal model.
\vspace{-2pt}
\begin{assumption}
There are no `latent' (unobserved) confounders in the underlying causal graph $\mathcal{G}$. 
\end{assumption}
\vspace{-4pt}
We note that this is not a restrictive assumption and is fairly general~\cite{kancheti2021matching,tarnet,disent}, since we do allow for observed confounders in our framework. 
To quantify direct and indirect causal attributions, it is required to find the values of the children of the treatment variable at a specific intervention (as formally stated in Defns~\ref{def:direct_effect} and \ref{def:indirect_effects}). Without the \textit{`no latent confounding'} assumption, it is not possible to find the values of the descendants of the treatment variable, thus necessitating this assumption. We now proceed with the definitions.
\begin{definition}
\label{def:direct_effect}
(Average Direct Causal Effect~\citep{pearl2001direct}). The Average Direct Causal Effect (ADCE) measures the causal effect of $x_i$ on $y$ when descendants $Z$ of $x_i$ that are in a directed path to $y$ in $\mathcal{G}$ are intervened to their known natural values under the intervention $do(x_i=x_i^*)$ (denoted by $Z_{x_i^*}$). ADCE is defined as: $ADCE^{y}_{x_i} = \mathbb{E}[y|do(x_i,Z_{x_i^*})]-\mathbb{E}[y|do(x_i^*, Z_{x_i^*})]$.
\end{definition}
\begin{definition}
\label{def:indirect_effects}
(Average Indirect Causal Effect~\citep{pearl2001direct}). The Average Indirect Causal Effect (AICE) measures the causal effect of $x_i$ on $y$ when descendants $Z$ of $x_i$ that are in a directed path to $y$ $\mathcal{G}$ are fixed to their interventional values under $x_i$, given by $Z_{x_i}$ while keeping the $x_i$ value fixed at the baseline value $x_i^*$. AICE is defined as: $AICE^{y}_{x_i} = \mathbb{E}[y|do(x_i^*,Z_{x_i})]-\mathbb{E}[y|do(x_i^*, Z_{x_i^*})]$.
\end{definition}
\vspace{-3pt}
% ### In the above two definitions, should we mention that we are referring to descendants in the reduced SCM of the NN ?
% Since definitions have to be general, we do not need to mention anything about our paper. If we provide our own definition, we can define to include our construction
Our key idea towards learning direct and indirect causal effects while training NN models is formalized in the following proposition.
\vspace{-2pt}
\begin{restatable}{propositions}{propositionindirectidentifiability}
\label{indirectidentifiability} In a neural network $\mathcal{N}$,
the $AICE_{x_i}^y$ of an input feature $x_i$ on an output neuron $y$ is identifiable if and only if $x_i$ has outgoing edges towards its children capturing the causal relationships in the real world and the weights parametrizing the edges from $x_i$ to its children should be learned by $\mathcal{N}$ along with other weights in the model while optimizing $\mathcal{N}$'s objective.
\end{restatable}
\vspace{-3pt}
While Propn~\ref{indirectidentifiability} seems obvious, this is not considered in recent methods that attempt to provide causal explanations in NN models.  %many recent methods try to provide explanations that are not direct causal explanations even though the causal links among inputs are missing. 
For example, ~\citep{acausalproblem} argue that  \textit{Shapley} explanations in a simple feedforward NN should treat all input features to be independent as the causal graph of a simple feedforward NN has no causal connections among input features. A similar discussion can be seen in~\citep{Datta2016AlgorithmicTV} with a focus on only \textit{direct} effects using quantitative input influence. Any explanation method that computes input-output attributions using statistical relationships in the observed data distribution (e.g., using conditional expectation instead of marginal expectation for missing features while calculating Shapley values) may generate incorrect explanations that are not representative of the explanations of the model being explained, if indirect effects are not accounted for using such lateral connections.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{images/erm_ante.png}
    \vspace{-8pt}
    \caption{\footnotesize Comparison of the proposed architecture $\mathcal{N}^{AH}$ with existing simple neural network architecture $\mathcal{N}$. $\mathcal{G}$ is the ground truth causal graph. $\mathcal{N}$ and $\mathcal{N}^{AH}$ differ in input layer such that the inputs in $\mathcal{N}^{AH}$ are connected (shown in blue color) according to the causal edges in $\mathcal{G}$. In contrast, the inputs in $\mathcal{N}$ are independent.}
    \label{fig:intvserm}
    \vspace{-10pt}
\end{figure}

\vspace{-13pt}
%\subsection{Training and Explaining Ante-hoc Explanation Model}
\subsection{Methodology}
\vspace{-9pt}

Based on our abovementioned hypothesis, we design an NN architecture $\mathcal{N}^{AH}$, which is an augmented version of a simple feedforward NN, denoted by $\mathcal{N}$ such that $\mathcal{N}^{AH}$ contains lateral directed connections among input nodes that are added using knowledge of the true causal graph $\mathcal{G}$ (see Fig~\ref{fig:intvserm}). We note that our method will work even with a partial causal graph as input, but may be able to capture indirect effects only on the available connections. These connections among input features have learnable parameters, just like other weight parameters of the NN.
% NNs usually contain feedforward connections from all input features to output, even if some edges are not present in $\mathcal{G}$ (e.g., $W\rightarrow Y$ in $\mathcal{N}, \mathcal{N}^{AH}$). Studying the intricacies involved with such edges in NNs but not in $\mathcal{G}$ will be interesting for future work.

In order to train $\mathcal{N}^{AH}$, we adopt a methodology consisting of two phases, each of which is invoked sequentially in each epoch.
%We follow a two-phase training methodology, where each phase is alternatingly performed in each epoch.  Training of $\mathcal{N}^{AH}$ happens in two phases that are carried out simultaneously. 
In the first phase, we freeze the parameters of the NN edges introduced among input features and train the remaining part of the NN. In the second phase, we train the parameters of the NN edges among inputs along with the remaining part of the NN. In the second phase, for input variables that do not have parents in $\mathcal{G}$, we sample only these variables from training data, and the remaining variable values are obtained by feeding the sampled variable values to the NN edges among input variables. These two steps are carried out sequentially in every epoch until we reach the desired minimum loss value (or appropriate stopping condition). The NN is trained using Stochastic Gradient Descent, as with any other NN model. Algorithm~\ref{algo:nn_edges_input_layer} summarizes this training procedure.

\vspace{-3pt}
\begin{algorithm}
\footnotesize
\caption{Training Algorithm for Proposed $\mathcal{N}^{AH}$}
\label{algo:nn_edges_input_layer}
\begin{algorithmic}
   \STATE {\bfseries Input:} Causal graph $\mathcal{G}$, $\mathcal{D} = \{(x^{i}_{1},\dots,x^{i}_{n},y^i)\}_{i=1}^{m}$, $l_0 =$ edges among $\{x_1, \dots, x_n\}$
   \STATE {\bfseries Output:} Trained $\mathcal{N}^{AH}$   
\FOR{each epoch}
\FOR{phase in [freeze, full]}
\IF{phase = freeze}
\STATE {Freeze $l_0$, train $l_1,\dots,l_n$ of $\mathcal{N}^{AH}$ using $\mathcal{D}$}
\ELSE 
\STATE{$X^r = \{x_i: pa(x_i)=\emptyset\}$}
\STATE{Sample $X\setminus X^r$ using $l_0, X^r$}
\STATE {Train $l_0,\dots,l_n$ of $\mathcal{N}^{AH}$ using $(X, y)$.}
\ENDIF
\ENDFOR
\ENDFOR
\STATE{return trained $\mathcal{N}^{AH}$}
\end{algorithmic}
\end{algorithm}
\vspace{-6pt}

The identifiability of direct causal effects~\citep{pearl2009causality} is trivial in $\mathcal{N}$ because interventional distribution is the same as conditional distribution when inputs are independent in the SCM~\citep{pearl2001direct}. However, in $\mathcal{N}^{AH}$, before quantifying the causal attributions, we need to ensure that the direct and indirect causal effects of input neurons on the output neurons are identifiable. To this end, we present the following proposition. 

\begin{restatable}{propositions}{propositiontwo}
\vspace{-3pt}
\label{proposition:exp_identification}
(Experimental Identification). In $\mathcal{N}^{AH}$, the direct and indirect causal effects of inputs on outputs are experimentally identifiable. Indirect causal attributions of inputs on the output are zero in the base model $\mathcal{N}$.
\vspace{-7pt}
\end{restatable}

Please see Appendix \ref{proofs} for a proof. Now, to quantify the causal attributions of input $x_i$ on output $y$ in $\mathcal{N}^{AH}$, it is required to evaluate $ACE_{x_i}^{y}$ (Defn~\ref{def: ACE}) and hence the expression $\E[y|do(x_i = \alpha)]$. However, often evaluating $\E[y|do(x_i = \alpha)]$ requires us to marginalize over other input features $X\setminus x_i$:
\vspace{-4pt}
\begin{equation}
\E[y|do(x_i = \alpha)] = \E_{X\setminus x_i}[\E[y|x_i=\alpha, X\setminus x_i]]
\label{interventional eq}
\end{equation}
\vspace{-20pt}

The time required to evaluate Eqn~\ref{interventional eq}, known as the \textit{backdoor adjustment formula}~\citep{pearl2009causality}, grows exponentially with the number of features in $X\setminus x_i$, especially when they are continuous and real-valued. To avoid such  prohibitive computation requirements, following earlier work ~\citep{montavon2017explaining,causalattributions}, we consider the second-order Taylor's expansion to the NN output around the mean vector $\mu$, where $\mu_j = \mathbb{E}[x_j|do(x_i=\alpha)]$ as follows (recall from Sec \ref{Section: causal effects in NNs} that $l_1$ is layer 1):
\vspace{-6pt}
\begin{equation*}
\small
f(l_1) \approx f(\mu)  + \nabla^T f(\mu)(l_1 - \mu) + \frac{1}{2}(l_1 - \mu)^T\nabla^2f(\mu)(l_1 - \mu)
\label{taylor expansion eq}
\end{equation*}
\vspace{-20pt}

Now, for an intervention $do(x_i=\alpha)$, taking expectation on both sides gives:
\begin{equation}
\begin{aligned}
&\E[f(l_1)| do(x_i = \alpha)] \approx f(\mu) + \\
&\frac{1}{2}Tr(\nabla^2f(\mu)\E[(l_1 - \mu)(l_1 - \mu)^T| do(x_i = \alpha)])
\end{aligned}
\label{expected taylor expansion eq}
\end{equation}
The first-order terms vanish because $\E[l_1|x_i = \alpha] = \mu$. To compute Eqn~\ref{interventional eq}, we now only need to calculate the individual interventional means $\mu$ and the interventional covariance between input features $\E[(l_1 - \mu)(l_1 - \mu)^T| do(x_i = \alpha)]$.
For $\mathcal{N}$, the intervention $do(x_i=\alpha)$ does not change the 
probability distribution of the other inputs $X\setminus x_i$, i.e., $\forall x_j \in l_1$ and $x_j \neq x_i$, $p(x_j|do(x_i = \alpha)) = p(x_j)$. Thus, the interventional means and covariances are equal to the observational means and covariances respectively. Also it is easy to see that $\E[x_i|do(x_i = \alpha)] = \alpha$ and $Cov(x_i, x_j|do(x_i = \alpha)) = 0$ $\forall x_j \in l_1$. Since the intervention on any feature $x_i$ does not alter other variables in the input layer (although they can be causally related in the real world), 
we can only find direct causal attributions using the above approach.

\vspace{-6pt}
\begin{algorithm}
\footnotesize
\caption{Computing $\mathbb{E}[y|do(x_i=\alpha)]\ \forall \alpha$ in $\mathcal{N}^{AH}$}
   \label{algo:ace}
\begin{algorithmic}
% \SetAlgoLined
% \KwResult{$\E(y|do(x_i))$}
   \STATE {\bfseries Input:} NN output $y$, Input feature $x_i$,  $Z=\{x_p\in \mathcal{G}:x_i\rightarrow \dots x_p\rightarrow \dots y\}$, interventions range [$l, h$], number of interventions $n$, functions $f_0^p$ learned in $l_0$ for each $x_p$
    \STATE {\bfseries Output:} $\E(y|do(x_i=\alpha)) \forall \alpha$
   \STATE {\bfseries Initialize:} $Cov[x_i,:] := 0$; $Cov[:,x_i] := 0$; $IE := []$; $\alpha = low^i; \mu := [\mathbb{E}(x_1), \dots, \mathbb{E}(x_n)] $ %$counter := 0$; 

   \WHILE{$\alpha \leq h$}
    \STATE {\textbf{case 1: To compute $\mathbb{E}[y|do(x_i^*, Z_{x_i^*})]$} }
    \STATE{\hspace{1cm} $x_i = b$ \hspace{1cm} \textbackslash * Defns~\ref{def:direct_effect} and~\ref{def:indirect_effects} * \textbackslash}
    \STATE{\hspace{1cm}  $\mu[i] = b$}

    \STATE {\textbf{case 2: To compute $\mathbb{E}[y|do(x_i, Z_{x_i^*})]$}}
    \STATE{\hspace{1cm} $x_i = b$ \hspace{1cm} \textbackslash * Defn~\ref{def:direct_effect} * \textbackslash}
    \STATE{\hspace{1cm}  $\mu[i] = \alpha$}

    \STATE {\textbf{case 3: To compute $\mathbb{E}[y|do(x_i^*, Z_{x_i})]$}}
    \STATE{\hspace{1cm} $x_i = \alpha$ \hspace{1cm} \textbackslash * Defn~\ref{def:indirect_effects} * \textbackslash }
    \STATE{\hspace{1cm}  $\mu[i] = b$}
    
    \FOR{ $x_p\in Z$}
       \STATE{$x_p = f_0^p(x_p^{pa})$}
\STATE{\textbackslash * Interventional Statistics * \textbackslash}
       
        \STATE{$\mu[p] = \mathbb{E}_{x_p^{pa}}\big[\mathbb{E}[x_p|x_i,x_p^{pa}]\big]$}
        \STATE{Compute $Cov[x_j,x_p]\ \forall j\neq i,p$}
    \ENDFOR
    
       \STATE add $\mathcal{N}^{AH}(\mu)$+ $\frac{1}{2}$Trace($\nabla^2\mathcal{N}^{AH}(\mu)\times Cov$) to $IE$\;
       \STATE $\alpha := \alpha + \frac{h - l}{n}$ \;
   \ENDWHILE
 \STATE{return $IE$}
\end{algorithmic}
\end{algorithm}
To quantify indirect causal attributions in $\mathcal{N}^{AH}$, we need to perform the following steps to account for the fact that intervention on $x_i$ will change its descendants' values (unlike in $\mathcal{N}$ where intervention to $x_i$ does not change the values of $X\setminus x_i$). As can be seen from the Defns~\ref{def:direct_effect} and~\ref{def:indirect_effects}, to calculate direct and indirect effects, we need to intervene on $Z$ (descendants of $x_i$ that are in a directed path to $y$ in $\mathcal{G}$) with the values that $Z$ would attain at a specific intervention (e.g., $do(Z=Z_{x_i^*})$). To this end, for all nodes $x_p$ that belong to a path $x_i\rightarrow y$ in $\mathcal{G}$ (taken in topological order), we calculate interventional means of $x_p$ as $\mathbb{E}[x_p|do(x_i=\alpha)] = \mathbb{E}_{x_p^{pa}}\big[\mathbb{E}[x_p|x_i=\alpha,x_p^{pa}]\big]$ where $x_p^{pa}$ is the set of parents of $x_p$ except $x_i$ in $\mathcal{G}$, and interventional covariances involving $x_p$ are calculated after evaluating the values of all features under the intervention in topological order. We summarize the procedure to calculate $\E[y|do(x_i=\alpha)]$ for $\mathcal{N}^{AH}$ in Algorithm~\ref{algo:ace}, which helps us compute Eqn \ref{interventional eq}.

\vspace{-15pt}
\subsection{Efficient Implementation Strategies}
\label{sec:improve_time_and_space_complexity}
\vspace{-9pt}

Computation of causal attributions can, in general, be compute-intensive \citep{causalattributions}. We now study a few efficient implementation strategies for such computations. % practical limitations of causal attribution methods. 
Let the number of inputs to an NN $f$ be $n$ and let each input $x_i\in X$ assume one of $k$ possible values ($k=2$ in the binary case). Evaluating $ACE_{x_i}^y$ (as in Defn~\ref{def: ACE}) takes roughly $\mathcal{O}(n^k)$ time because of the marginalization step in Eqn~\ref{interventional eq}. On the other hand, evaluating the approximation in Eqn~\ref{expected taylor expansion eq} also scales in the order of $\mathcal{O}(n^2)$ as input after the intervention may affect all of its children (Defns~\ref{def:direct_effect},~\ref{def:indirect_effects}). These limitations get accentuated in architectures such as Recurrent Neural Networks (RNNs) (please see Appendix~\ref{implementation} for complexity analysis in RNN). To address these issues, we propose the following implementation strategies. %the following techniques. 

\noindent \textbf{Run Time Improvements using Binning:}
Computing ACE as given in Eqn~\ref{expected taylor expansion eq} requires computing the interventional mean and interventional covariance (interventional statistics).  To speed up this calculation, we divide the computation of ACE into offline and online phases. In the Offline step (which can be done independent of the NN training phase), for every data point $X$ in the training set, we generate and store the interventional statistics for all features $x_i\in X$ for all interventional values. In the Online phase, to find the ACE for feature $x_i$ with intervention value of $\alpha$ in a test data point $X_{te}$, we first find the data point $X_{tr}$ in the training set that is most similar to the given test data point. Let $\beta$ be the value taken by feature $x_i$ in $X_{tr}$, closest to $\alpha$. We  access the interventional statistics stored for $X_{tr}$ corresponding to feature $x_i$ with intervention $\beta$ (computed in the offline phase). %We now pick the interventional statistics for the $\alpha$ value, which is nearest to $\beta$. 
This retrieved \textit{nearest} interventional statistics is used for ACE computation. This process is outlined in Algorithm~\ref{algo:improvements}. This procedure reduces significant runtime leveraging offline computations. We refer to this approach as \textit{binning} since a training sample or a feature invention value captures a bin, and acts as a proxy for other samples/values in its neighborhood. To further speed up ACE computation, we exploit the fact that the Hessian term $\nabla^2f$ in Eqn~\ref{expected taylor expansion eq} can be approximated using $J^TJ$ where $J$ is the Jacobian of the NN model function (using Gauss-Newton Hessian approximation). %That is, $\nabla^2f \approx J^TJ$ resulted in a lot of reduction in time.

%By splitting ACE computation into the offline and online phases, we convert the problem of generating interventional statistics into the problem of retrieving \textit{nearest} interventional statistics, thereby reducing online computation time. 

\noindent \textbf{Memory Requirements:}
Storing offline interventional statistics for every sample on the dataset (and corresponding intervention values) quickly becomes impractical, especially for high-dimensional data. %This is owing to the multiple dimensions of data required for every point, such as $\alpha$ values and the number of features. 
In order to reduce this memory overhead, we use clustering/hashing techniques (KD Tree, DBSCAN) to cluster training data samples, and store interventional statistics for only cluster centers (please see Appendix~\ref{implementation} for additional details).

%So we seek to reduce the number of points, number of samples of each point and amount of data stored per point while maintaining accuracy of online results of incoming points. For this purpose, we use clustering techniques such as KD Tree, DBSCAN etc. to get clusters of data points and take cluster centers and store interventional statistics for only those points (see Appendix~\ref{implementation} for additional details).

\begin{algorithm}
\footnotesize
\caption{Algorithm to Efficiently Evaluate $ACE_{x_i}^y$}
\label{algo:improvements}
\begin{algorithmic}
\STATE {\bfseries Input:} Test sample $X_{te}$; Feature $x_i$, Intervention value $\beta$, Database $DB$, Training data $\mathcal{D}$, Nearest neighbor function $NB$, Intervention values $\alpha_1,\dots,\alpha_m$ to $x_i$ in $\mathcal{D}$.
\STATE {\bfseries Output:} $\E(y|do(x_i=\beta))$
\STATE{\textbackslash * Offline Phase * \textbackslash}
\FOR{$X_{tr} \in \mathcal{D}$}
\STATE {$DB[X_{tr};E;\alpha]=\mathbb{E}[x_j|do(x_i=\alpha)]\ \forall j, \alpha$ }
\STATE {$DB[X_{tr};C;\alpha]=Cov(x_j, x_l|do(x_i=\alpha))\ \forall j,l,\alpha$}
\ENDFOR

\STATE{\textbackslash * Online Phase * \textbackslash}
\STATE {\textbf{step 1:} $X_{tr}=NB(X_{te}, \mathcal{D})$}

\STATE {\textbf{step 2:} $\alpha = NB(\beta, [\alpha_1,\dots,\alpha_m])$}

\STATE {\textbf{step 3:} $\mathbb{E}[x_j|do(x_i=\alpha)]\ \forall j = DB[X_{tr};E;\alpha]$
}

\STATE {\textbf{step 4:} $Cov(x_j, x_l|do(x_i=\alpha))\ \forall j,l = DB[X_{tr};C;\alpha]$}

\STATE {\textbf{step 5:} Evaluate $ACE_{x_i}^y$ using $\mathbb{E}[x_j|do(x_i=\alpha)]\ \forall j, Cov(x_j,x_l|do(x_i=\alpha)))\ \forall j,l$}

\STATE {return $ACE_{x_i}^y$}
\end{algorithmic}
\end{algorithm}

\vspace{-15pt}
\section{Experiments and Results}
\vspace{-9pt}
We conduct a wide range of experiments across three kinds of datasets -- synthetic datasets, well-known public real-world benchmark datasets, and industry-based simulated datasets -- to study the usefulness of the proposed approach. We compare the causal attributions of our method with a well-known post-hoc gradient-based attributions method: Integrated Gradients (IG)~\citep{sundararajan2017axiomatic}, a post-hoc causal attributions (CA) method~\citep{causalattributions}, and the more recent causal regularization method in ~\citep{kancheti2021matching}. (To the best of our knowledge, none of these methods model indirect effect, a key focus of our method.) We use Integrated Gradients since it satisfies the axioms of attributions established in~\citep{sundararajan2017axiomatic}. Following ~\citep{kancheti2021matching}, we use the Root Mean Squared Error (RMSE) and Frechet scores between true causal effects and the learned causal effects, for fairness of comparison. %as  to showcase the usefulness of our proposed method in capturing the true causal attributions.
For each experiment, if an input feature is real-valued, we consider 1000 interventions (i.e., 1000 $\alpha$ values in Eqn~\ref{expected taylor expansion eq}) in its input range to calculate interventional expectation values. We present our results on total and indirect effect estimation in this section, and additional results on direct effect estimation in the appendix (owing to space constraints). Our ground truth causal attributions are computed using the adjustment formula~\citep{pearl2009causality}. Our code is provided in the supplementary material for reproducibility. %to reproduce the results of the experiments is provided in the supplementary material.
%In this section, we calculate the \textit{total} causal attributions (sum of direct and indirect causal attributions) and compare them with the ground truth causal attributions calculated using adjustment formula~\citep{pearl2009causality}. We present the results w.r.t. direct, indirect, and total causal attributions in the supplementary material. We then provide experiments showcasing the promising results of our approximation strategies for causal attributions for high dimensional and real-valued inputs. Code to reproduce the results of the experiments is provided in the supplementary material.

\noindent \textbf{Synthetic Data:}
\label{synthetic_tablular_1_expts}
We create three synthetic datasets using structural equations of the form shown in Tab~\ref{tab:synthetic_data_1}
(structural equations for other two datasets are provided in the Appendix~\ref{setup}). From Tab~\ref{tab:synthetic_data_1}, $W$ has only indirect effect on $Y$ via the paths: $W\rightarrow X\rightarrow Y, W\rightarrow Z\rightarrow X \rightarrow Y$, and $W\rightarrow X \rightarrow Y$. $Z$ has direct effect on $Y$ via the path $Z\rightarrow Y$ and indirect effect on $Y$ via the path $Z\rightarrow X\rightarrow Y$, and $X$ has only direct effect on $Y$ via the path $X\rightarrow Y$. These datasets have linear relationships with additive Gaussian noise for input features $W,Z,X$, and a non-linear function of its inputs with additive Gaussian noise for output $Y$.
\begin{table}
\footnotesize
\begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{images/synthetic1.png}
        \vspace{-5pt}
	\end{minipage}
	\begin{minipage}{0.45\linewidth}
    \begin{align*}
     W &\leftarrow Uniform(0, 1)\\
     Z &\leftarrow 2W + \mathcal{N}(0, 0.1)\\
     X &\leftarrow 2W - Z + \mathcal{N}(0, 0.1)\\
     Y &\leftarrow 3X + e^{3Z} + \mathcal{N}(0, 0.1)
    \end{align*}
     \vspace{-.7cm}
    \captionof{table}{Synthetic Data 1}
    \label{tab:synthetic_data_1}
	\end{minipage}
\end{table}

% \begin{wraptable}[7]{r}{0.2\textwidth}
% \vspace{-24pt}
%     \footnotesize
%     \begin{align*}
%      W &\leftarrow Uniform(0, 1)\\
%      Z &\leftarrow 2W + \mathcal{N}(0, 0.1)\\
%      X &\leftarrow 2W - Z + \mathcal{N}(0, 0.1)\\
%      Y &\leftarrow 3X + e^{3Z} + \mathcal{N}(0, 0.1)
%     \end{align*}
%      \vspace{-25pt}
%     \captionof{table}{\footnotesize SCM for synthetic data 1}
%     \vspace{-.2cm}
%     \label{tab:synthetic_data_1}
% \end{wraptable}
Hence, for purposes of computing ACE, the lateral connections among inputs in $\mathcal{N}^{AH}$ are obtained using simple linear regressors that take a set of real numbers as input and produce a real number as output (in case of real-world datasets, we replace simple linear regressors with a multi-layer perceptron). 
\begin{table}%[18]{r}{0.3\textwidth}
      \vspace{-7pt}
      \footnotesize
      \centering
     \scalebox{0.92}{
             \begin{tabular}{ll|ccc|c}
                \toprule
                % \multicolumn{5}{c}{\textbf{Indirect causal effects}}\\
                % \midrule
                 &\textbf{Feature}&\textbf{IG}& \textbf{CA} & \textbf{CREDO}& \textbf{Ours}\\
&&\citepalias{sundararajan2017axiomatic}&\citepalias{causalattributions}&\citepalias{kancheti2021matching}&\\
                 \midrule
                 \multicolumn{6}{c}{\textbf{Synthetic Data 1}}\\
                \midrule
                &W &0.869&0.869&0.835&1.114\\
                &Z &0.569&0.569&0.804&0.373\\
                &X &0.000&0.000&0.229&0.314\\
                \rowcolor{Gray}\parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{RMSE ($\downarrow$)}}}&Average &0.479&\textbf{0.326}&0.622&0.618\\
                \midrule
                &W &1.000&1.000&1.000&1.000\\
                &Z &1.000&1.000&1.883&0.883\\
                &X &0.000&0.000&0.397&0.352\\
                \rowcolor{Gray}\parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{Frechet ($\downarrow$)}}}&Average &\textbf{0.667}&\textbf{0.667}&1.109&0.745\\
                \midrule
                 \multicolumn{6}{c}{\textbf{Synthetic Data 2}}\\
                \midrule
                &W &0.571&0.571&0.856&0.534\\
                &Z &0.577&0.577&0.678&0.576\\
                &X &0.000&0.000&0.333&0.000\\
                \rowcolor{Gray}\parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{RMSE ($\downarrow$)}}}&Average &0.382&0.383&0.622&\textbf{0.370}\\
                \midrule
                &W &1.000&1.000&1.000&0.922\\
                &Z &1.000&1.000&0.999&0.992\\
                &X &0.000&1.000&0.602&0.000\\
                \rowcolor{Gray}\parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{Frechet ($\downarrow$)}}}&
                Average &0.667&0.667&0.867&\textbf{0.638}\\
                \midrule
                 \multicolumn{6}{c}{\textbf{Synthetic Data 3}}\\
                \midrule
                &W &0.574&0.573&0.847&0.006\\
                &Z &0.467&0.467&0.782&0.105\\
                &X &0.000&0.000&0.202&0.451\\

                \rowcolor{Gray}\parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{RMSE ($\downarrow$)}}}&Average &0.347&0.347&0.610&\textbf{0.187}\\
                \midrule
                &W &1.000&1.000&0.870&0.011\\
                &Z &1.000&1.000&1.991&0.003\\
                &X &0.000&0.000&0.385&1.000\\

                \rowcolor{Gray}\parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{Frechet ($\downarrow$)}}}&Average &0.667&0.667&1.082&\textbf{0.338}\\
                \bottomrule
             \end{tabular}
             }
             \vspace{-5pt}
             \captionof{table}{Indirect causal effects results on synthetic datasets.}
             \label{tab:synthetic_results_indirect}
             \vspace{-5pt}
    \end{table}

Tabs~\ref{tab:synthetic_results_indirect},~\ref{tab:synthetic_results_total}show the results. The indirect and total causal attributions given by our method are closer to ground truth causal attributions compared to baselines. These results show that the training algorithm provided to train our ante-hoc explanation model can learn both direct and indirect causal attributions and hence match the total causal attributions better. We achieve these results without affecting the performance of the models. The Mean Square Error (MSE) losses on the test set for our method for three datasets are 0.879, 0.089, and 0.576, and for CA, are 0.0589, 0.016, and 0.057, respectively.
\begin{table}%[18]{r}{0.3\textwidth}
      \vspace{-7pt}
      \footnotesize
      \centering
     \scalebox{0.92}{
             \begin{tabular}{ll|ccc|c}
                \toprule
                % \multicolumn{5}{c}{\textbf{Total causal effects}}\\
                % \midrule
                 &\textbf{Feature}&\textbf{IG}& \textbf{CA} & \textbf{CREDO}& \textbf{Ours}\\
&&\citepalias{sundararajan2017axiomatic}&\citepalias{causalattributions}&\citepalias{kancheti2021matching}&\\
                 \midrule
                 \multicolumn{6}{c}{\textbf{Synthetic Data 1}}\\
                \midrule
                &W &0.264&0.447&0.226&0.207\\
                &Z &0.146&0.462&0.532&0.357\\
                &X &0.759&0.279&0.547&0.277\\
                \rowcolor{Gray}
                \parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{RMSE ($\downarrow$)}}}&Average &0.389&0.326&0.435&\textbf{0.280}\\
                \midrule
                &W &0.350&0.038&0.001&0.001\\
                &Z &0.386&0.386&0.431&0.386\\
                &X &0.336&1.000&0.158&0.584\\
                \rowcolor{Gray}
                \parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{Frechet ($\downarrow$)}}}&Average &0.357&0.474&\textbf{0.196}&0.323\\
                \midrule
                 \multicolumn{6}{c}{\textbf{Synthetic Data 2}}\\
                \midrule
                &W &0.149&0.618&0.510&0.104\\
                &Z &0.159&0.268&0.119&0.214\\
                &X &0.584&0.361&0.353&0.359\\
                \rowcolor{Gray}
                \parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{RMSE ($\downarrow$)}}}&Average &0.297&0.431&0.327&\textbf{0.226}\\
                \midrule
                &W &0.323&1.000&1.000&0.001\\
                &Z &0.080&0.001&0.111&0.001\\
                &X &1.000&0.592&0.566&0.592\\
                \rowcolor{Gray}
                \parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{Frechet ($\downarrow$)}}}&Average &0.467&0.531&0.559&\textbf{0.198}\\
                \midrule
                 \multicolumn{6}{c}{\textbf{Synthetic Data 3}}\\
                \midrule
                &W &0.309&0.450&0.189&0.410\\
                &Z &0.225&0.385&0.555&0.222\\
                &X &0.273&0.164&0.253&0.102\\
                \rowcolor{Gray}
                \parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{RMSE ($\downarrow$)}}}&Average &0.269&0.333&0.332&\textbf{0.244}\\
                \midrule
                &W &0.860&1.000&0.283&0.863\\
                &Z &0.573&0.576&0.708&0.576\\
                &X &0.685&0.079&0.073&0.079\\
                \rowcolor{Gray}
                \parbox[t]{2mm}{\multirow{-4}{*}{\rotatebox[origin=c]{90}{Frechet ($\downarrow$)}}}&Average &0.706&0.551&\textbf{0.354}&0.504\\
                \bottomrule
             \end{tabular}
             }
             \vspace{-5pt}
             \captionof{table}{Total causal effects results on synthetic datasets.}
             \label{tab:synthetic_results_total}
             \vspace{-12pt}
    \end{table} 
    
\noindent \textbf{Lung Cancer and AUTOMPG:}
We conduct experiments on the Lung Cancer dataset~\citep{scutari2014bayesian} whose causal graph is known (provided in Appendix~\ref{setup}). \textit{Dyspnea} is the output variable with the remaining seven as input features (see Tab~\ref{tab:cancerandautompg}). All methods, including CA, CREDO, and our model, achieve a test accuracy of $\approx 85\%$. Table~\ref{tab:cancerandautompg} shows the results. We observe that our model is better at learning the true total causal effects when compared to the baselines. The lateral connections among input features are implemented using simple multi-layer perceptrons with non-linear activation functions for use in ACE computation. Since the underlying causal graph of the Lung Cancer dataset is a discrete Bayesian network with binary-valued features, Frechet score may not be relevant, and so we report only RMSE values. 

To further study the scenario when the true causal graph is not known, we study the AUTOMPG dataset~\citep{Dua2019}. Herein, we first run the LiNGAM causal discovery method~\citep{directlingam,directlingamestimation} to estimate the true causal graph before training our ante-hoc causal explanation algorithm. The causal graph thus estimated is shown in Appendix~\ref{setup}. Table~\ref{tab:cancerandautompg} shows these results. From the results, our method outperforms baselines in capturing total causal attributions.
\begin{table}%[14]{r}{0.27\textwidth}
    \centering
    \footnotesize
    \vspace{-3pt}
    \scalebox{0.90}{
    \begin{tabular}{ll|ccc|c}
         \toprule
    &\textbf{Feature}&\textbf{IG}&\textbf{CA}&\textbf{CREDO}&\textbf{Ours}\\
&&\citepalias{sundararajan2017axiomatic}&\citepalias{causalattributions}&\citepalias{kancheti2021matching}&\\
        \midrule
        \multicolumn{6}{c}{\textbf{Lung Cancer}}  \\
        \midrule
        &Asia &0.172& 0.961&0.804&0.003\\
        &Tub. &0.472&0.426&0.339&0.982\\
        &Smoking &1.003& 1.275&1.679&0.932\\
        &L.Cancer &0.530&0.678&0.147&0.440\\
        &Bronch. &2.351&2.433&3.249&0.773\\
        &Either &0.823& 0.443&0.619&0.492\\
        &X-ray &0.001&0.138&0.131&0.031\\
        \rowcolor{Gray}
        \parbox[t]{2mm}{\multirow{-8}{*}{\rotatebox[origin=c]{90}{RMSE ($\downarrow$)}}}&Average&0.764&0.908&0.995&\textbf{0.522}\\
        \midrule
        \multicolumn{6}{c}{\textbf{AUTOMPG}}  \\
        \midrule
         &Cylinders&0.155&0.580&0.120&0.120\\
         &Disp.&0.545&0.580&0.000&0.060\\
         &Horsepow.&0.216&0.580&0.580&0.060\\
         &Weight&0.344&0.010&0.000&0.070\\
         &Acceler.&0.175&0.580&0.620&0.540\\
     \rowcolor{Gray}\parbox[t]{2mm}{\multirow{-6}{*}{\rotatebox[origin=c]{90}{RMSE ($\downarrow$)}}}&Average&0.286&0.466&0.264&\textbf{0.172}\\
     \midrule
        &Cylinders&0.236&1.000&0.460&0.210\\
         &Disp.&0.985&1.000&0.000&0.000\\
         &Horsepow.&0.376&1.000&1.000&0.000\\
         &Weight&0.506&0.000&0.000&0.000\\
         &Acceler.&0.269&1.000&1.000&1.000\\
     \rowcolor{Gray}\parbox[t]{2mm}{\multirow{-6}{*}{\rotatebox[origin=c]{90}{Frechet ($\downarrow$)}}}&Average&0.474&0.800&0.492&\textbf{0.242}\\
    \bottomrule
    \end{tabular}
    }
    \caption{Total causal effects results on Lung Cancer and AUTOMPG datasets}
    \label{tab:cancerandautompg}
\end{table}

\noindent \textbf{Ablation Study on Synthetic Datasets:} To empirically verify our hypothesis that adding edges among input features helps in learning causal attributions (Propn~\ref{indirectidentifiability}, Algorithm~\ref{algo:nn_edges_input_layer}), we perform an ablation study on synthetic datasets. In this study, we check the causal attributions of each input feature $\{W,Z,X\}$ with and without outgoing edges in $\mathcal{N}^{AH}$ (denoted by $\mathcal{N}^{AH}_{W}$, $\mathcal{N}^{AH}_{WO}$ respectively in Tab~\ref{tab:synthetic_ablation}). This setting also resembles the real world scenarios where we often have access to only partial causal graph. From the results in Tab~\ref{tab:synthetic_ablation}, We observe that the total causal attributions of $\mathcal{N}^{AH}_{W}$ are closer to the ground truth total causal attributions compared to $\mathcal{N}^{AH}_{WO}$.

\begin{table}%[18]{r}{0.3\textwidth}
      \footnotesize
      \centering
     \scalebox{1.0}{
             \begin{tabular}{l|cc|cc}
                \toprule
                Feature& \multicolumn{2}{c|}{RMSE ($\downarrow$)}  & \multicolumn{2}{c}{Frechet ($\downarrow$)}\\
                \midrule
                &$\mathcal{N}^{AH}_{W}$& $\mathcal{N}^{AH}_{WO}$& $\mathcal{N}^{AH}_{W}$&$\mathcal{N}^{AH}_{WO}$\\
                \midrule
                \multicolumn{5}{c}{\textbf{Synthetic Data 1}}\\
                \midrule
                W&0.207&0.457&0.001&1.000\\
                Z&0.357&0.429&0.386&0.380\\
                X&0.277&0.277&0.584&0.584\\
        \rowcolor{Gray}Average&\textbf{0.280}&0.387&\textbf{0.323}&0.654\\
        \midrule
                \multicolumn{5}{c}{\textbf{Synthetic Data 2}}\\
                \midrule
                W&0.104&0.217&0.001&0.001\\
                Z&0.214&0.320&0.001&0.002\\
                X&0.359&0.359&0.592&0.592\\
        \rowcolor{Gray}Average&\textbf{0.225}&0.298&\textbf{0.198}&\textbf{0.198}\\
        \midrule
                \multicolumn{5}{c}{\textbf{Synthetic Data 3}}\\
                \midrule
                W&0.410&0.430&0.863&1.000\\
                Z&0.222&0.212&0.576&0.576\\
                X&0.102&0.102&0.076&0.076\\
        \rowcolor{Gray}Average&\textbf{0.280}&0.387&\textbf{0.323}&0.654\\
            \bottomrule
             \end{tabular}
             }
             \vspace{-5pt}
             \captionof{table}{
             Ablation study on synthetic datasets.}
             \label{tab:synthetic_ablation}
             \vspace{-12pt}
    \end{table}

\noindent \textbf{Flight Simulation Datasets:}
\label{exp:improvements}
In order to study the value of our efficient implementation strategies discussed in Sec.~\ref{sec:improve_time_and_space_complexity}, we consider flight simulation datasets that benefit from such strategies. We consider three different time series-based datasets that require RNNs for modeling: \textit{Parking Brake Dataset (PBD)}, \textit{Flap Dataset (FD)} and \textit{Multiple Anomaly Dataset (MAD)} which simulate the application of parking brakes during the takeoff, the deployment of a wrong flap during takeoff and the multiple brake anomalies (\textit{left-brake, right-brake, and auto-brake}) respectively. These datasets are captured on an industry-grade flight simulator. In all these 3 cases, we train an RNN to predict whether a given sequence is anomalous or not. We compare our method with CA and an approximation to the second-order term in Eqn~\ref{expected taylor expansion eq} CA. Table~\ref{tab:total_results} shows the results, highlighting the improvements in time needed to compute ACE in our method. Appendix~\ref{implementation} contains qualitative results showing the performance of these methods in terms of ACE plots.

\begin{table}
    \centering
    \scalebox{0.99}{
    \begin{tabular}{lcccc}%c}
    \toprule
   \textbf{Method} &\textbf{PBD}&\textbf{FD}&\textbf{MAD}\\
    \midrule
        CA (Eqn~\ref{expected taylor expansion eq})&  64.50&69.33&176\\
        CA (Hess.$\times$Cov.) & 20&41&91\\
        $\nabla^2f \approx J^TJ$ &18.5 &24&48.50\\
        Ours + CA  & 21.50&25.5&73\\
        Ours + CA (Hess.$\times$Cov.) & 0.13 &20& 25\\
        Ours + ($\nabla^2f \approx J^TJ$) & 0.004&1&3 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Time taken in minutes by different methods of finding ACE for flight simulation datasets.}
    \label{tab:total_results}
    \vspace{-9pt}
\end{table}

\vspace{-12pt}
\section{Conclusions}
\label{sec_conclusions}
\vspace{-8pt}
This work presents a new perspective to quantify causal attributions in neural networks. Using available prior causal knowledge, we design an ante-hoc causal explanation method to study both direct and indirect causal attributions of inputs on the output of an NN. The work also presents effective approximation strategies to compute causal effects for high-dimensional data efficiently. Our wide range of experiments on synthetic and real-world data show significant promise of the methodology to elicit direct and indirect causal effect of input on output in an NN model.

\bibliography{bibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\include{supplementary}

\end{document}