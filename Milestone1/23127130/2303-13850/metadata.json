{
    "arxiv_id": "2303.13850",
    "paper_title": "Towards Learning and Explaining Indirect Causal Effects in Neural Networks",
    "authors": [
        "Abbaavaram Gowtham Reddy",
        "Saketh Bachu",
        "Harsharaj Pathak",
        "Benin L Godfrey",
        "Vineeth N. Balasubramanian",
        "Varshaneya V",
        "Satya Narayanan Kar"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2023-08-28"
    ],
    "latest_version": 2,
    "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
    ],
    "abstract": "Recently, there has been a growing interest in learning and explaining causal effects within Neural Network (NN) models. By virtue of NN architectures, previous approaches consider only direct and total causal effects assuming independence among input variables. We view an NN as a structural causal model (SCM) and extend our focus to include indirect causal effects by introducing feedforward connections among input neurons. We propose an ante-hoc method that captures and maintains direct, indirect, and total causal effects during NN model training. We also propose an algorithm for quantifying learned causal effects in an NN model and efficient approximation strategies for quantifying causal effects in high-dimensional data. Extensive experiments conducted on synthetic and real-world datasets demonstrate that the causal effects learned by our ante-hoc method better approximate the ground truth effects compared to existing methods.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13850v1",
        "http://arxiv.org/pdf/2303.13850v2"
    ],
    "publication_venue": null
}