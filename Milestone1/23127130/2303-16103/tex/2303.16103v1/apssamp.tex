% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
 reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math




\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{color}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{subfigure}

\usepackage[justification=centering]{caption}









%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

\begin{document}

\preprint{APS/123-QED}

\title{Modularity-Guided Graph Topology Optimization And Self-Boosting Clustering}% Force line breaks with \\
%\thanks{A footnote to the article title}%

\author{Yongyu Wang}
 %\altaffiliation[Also at ]{JD, XYZ University.}%Lines break automatically or can be forced with \\
 %\email{wangyongyu1@jd.com}
\author{Shiqi Hao}%
 %\email{haoshiqi@jd.com}
 \author{Zhangxun Liu}%
 %\email{liuzhangxun1@jd.com}
 \author{Xiaotian Zhuang}%
 %\email{zhuangxiaotian@jd.com}
\affiliation{%
 JD Logistics\\
 %This line break forced with \textbackslash\textbackslash
}%


\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
%Due to the superior capability of graph for representing and exploiting intrinsic data characteristics, graph-based machine learning methods have received great attention in recent years. The performance of graph-based methods highly depends on the quality of the underlying graph. However, existing graph learning (construction) methods are often not suitable for providing satisfactory solution quality. 
Existing modularity-based community detection methods attempt to find community memberships which can lead to the maximum of modularity in a fixed graph topology. In this work, we propose to optimize the graph topology through the modularity maximization process. We introduce a modularity-guided graph optimization approach for learning sparse high modularity graph from algorithmically generated clustering results
by iterative pruning edges between two distant clusters.  To the best of our knowledge, this represents a first attempt for using modularity to guide graph topology learning. Extensive experiments conducted on various real-world data sets show that our method outperforms the state-of-the-art graph construction methods by a large margin. Our experiments show that with increasing modularity, the accuracy of graph-based clustering algorithm is simultaneously increased, demonstrating the validity of modularity theory through numerical experimental results of real-world data sets. From clustering perspective, our method can also be seen as a self-boosting clustering method.
%\begin{description}
%\item[Usage]
%Secondary publications and information retrieval purposes.
%\item[Structure]
%You may use the \texttt{description} environment to structure your abstract;
%use the optional argument of the \verb+\item+ command to give the category of each item. 
%\end{description}
\end{abstract}

%\keywords{Suggested keywords}%Use showkeys class option if keyword
                              %display desired
\maketitle

%\tableofcontents

\section{Introduction}


Graph-based methods play an important role in many machine learning and data mining applications, due to the superior capability of graph in representing the underlying structure of the data set and the relationship among data points. The graph is at the heart of graph-based method. The quality of graph has significantly effect on the solution quality of the algorithm. In the past decades, many graph construction (learning) methods have been proposed: The $k$-nearest neighbor ($k$-NN) graph is most commonly used in practice. In $k$-NN graph, each node is connected to its $k$ nearest neighbors. $k$-NN graph is good at capturing the local manifold structure. It also has good robustness to outliers. However, The fixed-size neighborhood limits its capability in representing the global manifold structure. In {$\epsilon$-neighborhood graph}, each node is connected to all the neighbors within a range of distance {$\epsilon$}. But the proper $\epsilon$  value can be very difficult to find or does not exist when different clusters have different radii. \cite{premachandran2013consensus} proposed to extract consensus information from a provided $k$-NN graph for pruning noisy edges and the edges with a consensus value less than a threshold are dropped. However, it relies on the consensus information only. Some useful structural information can also be discarded. In recent studies, several graph learning methods based on the emerging graph signal processing (GSP) techniques have been proposed for estimating sparse graph Laplacians. For instance, \cite{egilmez2017graph} learns graph by restricting the precision matrix to be a graph Laplacian and maximizing a posterior estimation of Gaussian Markov Random Field (GMRF), while an $l1$-regularization term is used for preserving graph sparsity;  \cite{kalofolias2018large} proposed to use an approximate nearest-neighbor method to reduce the number of variables in the optimization procedure. However, their involved Laplacian estimation procedure requires at least \textbf{$O(n^2)$} time in each iteration, restricting their application to real-world large-scale problems.


In the past decade, modularity has became the standard measure of graph division. It measures the density of connections inside communities as compared to connections between communities. Many methods have been proposed to solve the community detection problem by maximizing the modularity
\cite{newman2006modularity,blondel2008fast,traag2019louvain}. They attempt to find a community membership assignment which can lead to the maximum of modularity in the graph.

In contrast to existing modularity methods which use modularity to optimize the membership of node, this paper propose to use modularity to optimize the graph topology. We introduce a modularity-guided graph optimizing approach for learning sparse high modularity graph from  algorithmically generated clustering results by iterative pruning edges between two distant clusters. Compared with prior graph construction methods that target capturing local manifold or guaranteeing smoothness of the graph, our method aims at iteratively improve the graph modularity. Comparing with state-of-the-art modularity maximization methods, our method is the first attempt to optimize the modularity from graph topology perspective. We summarize the contribution of this work as follows:


\begin{enumerate}
  \item We propose a modularity-guided graph optimization approach that allows efficient build a high modularity sparse graph for the downstream graph-based algorithm. The key to achieving high quality is a novel redundant and misleading edge identification scheme for finding edges that can be removed for improving modularity while will not lead to loss of structural information of the data set. To the best of our knowledge, this represents a first attempt for employing modularity to guide graph topology learning. \\


   \item We demonstrate the validity of modularity theory through numerical experimental results of clustering with real-world data sets. Our experiments shows that with increasing modularity, the accuracy of graph-based clustering algorithm is simultaneously increased. \\

  
  \item Experiments show that by using our method, we can have a better solution quality of graph-based algorithm and a more efficient graph construction step, outperforming previous state-of-the-art graph learning (construction) methods by a large margin. \\

\item Our method can also be seen as a self-boosting clustering method. Based on the initial clustering result, we upgrade the graph. Then, based on the upgraded graph, we can obtain more precise clustering.  \\



  
\end{enumerate} 


\section{Preliminaries}\label{sect:preliminaries}

\subsection{Modularity}

Modularity is the fraction of the connections inside a cluster minus the expected fraction if connections were randomly distributed among nodes. \cite{newman2006modularity} proposed to calculate the modularity as follows:

\begin{equation}\label{eqn:scale}
Q= \frac{\sum\limits_{i,j} \left[ A_{ij}-\frac{k_i k_j}{2m} \right] {\delta {(c_i,c_j)}}}{{2m}},
\end{equation}
where $A_{ij}$ is the weight of the edge between node $i$ and node $j$; $k_i$ and $k_i$ are the degrees of node $i$ and node $j$, respectively; $c_i$ and $c_j$ are the cluster-memberships of node $i$ and node $j$, respectively. $\delta (x,y)$ is a delta function defined as: $\delta (x,y)$=1 for $x=y$, and $\delta (x,y)$=0, otherwise. $m$ is the total number of edges in the graph. A higher value of Q indicates better graph division.


\subsection{The Importance Of Graph Topology}

Graph topology is very useful in detecting non-convex and linearly non-separable patterns. $k$-means is the most popular non-graphical clustering method. However, it only cares about the distance, aiming to minimize the total distances between each data point and its corresponding cluster centroid, so it cannot provide satisfactory clustering results for complicated data sets such as the well-known two moons and two circles data sets. In the two moons data set, the ground truth clusters are corresponding to the two slightly entangled moons. In the two circles data set, the ground truth clusters are corresponding to the two concentric circles. $k$-means generates wrong clustering results for both of them. In contrast, graph-based algorithms such as spectral clustering makes use of the connectivity information in the graph topology so that the correct clustering results can be generated.




 
\section{Methods}


\subsection{Algorithmic Framework}
At high level, our method is inspired by recent modularity-based community detection methods but use the modularity in different way to achieve different goal. We integrate the modularity measure into graph construction step to guide graph topology learning. Our method aims to iteratively optimize the graph topology from the modularity perspective until the modularity can no longer be improved via the proposed strategy, which consists of the following key steps:\\


Step (1): Given a graph $G$, our method first perform clustering algorithm on $G$ to divide it into several clusters. \\

Step (2): Theoretically, all the edges between clusters should be removed in order to maximize the modularity. However, in practice, this is difficult if not impossible because the ground-truth number of clusters is often unknown and  clusters generated by algorithm are not precise. So some edges between two algorithmically generated clusters maybe are the ``true'' edges inside a ground-truth cluster. To remove the redundant and misleading edges while avoiding removing possible ``true''  edges, we only remove edges that connect nodes between two distant clusters. Because these edges have  high probabilities to be redundant and misleading edges and can be safely removed to increase the graph modularity. The distance between two clusters is measured by the distance between their centroids.\\

Step (3): After increasing the modularity in Step (2), we perform clustering algorithm in the upgraded graph to gain more precise clustering result to guide further optimization of the graph topology. \\


Step (4): After repeating the Steps (2)-(3) multiple iterations, our method will return the final graph once the graph modularity becomes stable (the modularity will not be significantly improved by changing graph topology).\\









\section{Experiment}\label{sect:experiments}

In this paper, we use spectral clustering, a classical graph-based clustering method to divide and evaluate graph. Our graph optimization method is applied on a $k$-NN graph. The value of $k$ is set to 10. Experiments are performed using MATLAB running on the Laptop. 



\subsection{Data Sets}
We have conducted experiments on three real-world data sets. \textbf{COIL-20} includes 1,440 gray scale images of 20 different objects and each image is represented by $1,024$ attributes; \textbf{PenDigits} includes  $7,474$ handwritten digits and each digit is represented by $16$ attributes;
\textbf{USPS} includes   $9,298$ images of USPS hand written digits with $256$ attributes. 





\subsection{Comparison Methods}
We compare the proposed method against both the baseline and the state-of-the-art graph learning (construction) methods, including:\\

1) $k$-NN graph: the most widely used graph construction method. Each node is connected to its $k$ nearest neighbors.\\

2) Consensus $k$-NN graph: the state-of-the-art graph edge selection methods for improving the performance of $k$-NN graph. It improves the robustness of the $k$-NN graph by using the consensus information from different neighborhoods of a given $k$-NN graph.\\

3) LGSS: the most recent progress in graph learning field from the GSP perspective. It can automatically select the parameters of the model for achieving the desired graph properties. 

\subsection{Evaluation Metrics}

Clustering accuracy (ACC) is the most widely used measurement of clustering quality. It is defined as follows \cite{chen2011parallel}:
\begin{equation}\label{eqn:scale}
ACC= \frac{\sum\limits_{i = 1}^n  {\delta {(y_i,map(c_i))}}}{{n}},
\end{equation}
where $n$ is the number of samples in the data set, $y_i$ is the ground-truth cluster membership of the $i$-th sample, and its cluster membership generated by the clustering algorithm is denoted by $C_i$. $map(\bullet)$ is a permutation function that maps each cluster index to a ground truth label based on the Hungarian algorithm \cite{papadimitriou1982combinatorial}. A higher value of ACC indicates a more precise clustering result.\\







\vspace{-0.1in}
\subsection{Experimental Results}
We perform spectral clustering algorithm on the graphs generated by the four graph construction methods. The clustering accuracy results are shown in Table~\ref{table:compare3}. The graph construction time of the methods are shown in Table~\ref{table:compare5}.



\begin{table*}[!htbp]\label{compare3}
\begin{center}

\caption{Clustering Accuracy (\%)}
\scalebox{1.4}{
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c|c|c }
\hline
 
 \hline   Data Set &$k$-NN &Consensus&LGSS&Our method\\
 \hline   COIL20  &75.72&81.60 &85.49&82.22\\
 \hline   PenDigits &74.36&71.08 &74.53&86.42 \\
 \hline   USPS     &64.31&68.54 &81.50&80.55 \\
 
 \hline
\end{tabular}}\label{table:compare3}
\end{center}
\end{table*}





\begin{table*}[!htbp]\label{compare5}
\begin{center}

\caption{Graph learning (construction) time (Seconds)}
\scalebox{1.57}{
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c|c|c }

 \hline   Data Set &Consensus&LGSS&Our method\\
 \hline   COIL20  &2.43 &13.56&6.32 \\
 \hline   PenDigits &172.51 &1085.43&7.87 \\
 \hline   USPS     &574.28 &2074.78&5.01 \\
 \hline
\end{tabular}}\label{table:compare5}
\end{center}
\end{table*}






\begin{figure*}[!htbp]
\centering\includegraphics[scale=0.35]{accvsiter.png}
\caption{Clustering accuracy through iterations of the Pendigits data set.\protect\label{fig:accvsiter.png}}
\end{figure*}



\begin{figure*}[!htbp]
\centering\includegraphics[scale=0.35]{modularityvsiter.png}
\caption{Modularity through iterations of the Pendigits data set.\protect\label{fig:modularityvsiter.png}}
\end{figure*}




As shown in Table~\ref{table:compare3}, the proposed method can consistently lead to dramatic performance improvement over the given graph. By applying our optimization method on $k$-NN graph, it achieves more than $6\%$, $12\%$ and $16\%$ clustering accuracy gains on the three data sets, respectively. For the Pendigits dataset, our method has $12\%$ clustering accuracy gain over the second-best method. The superior clustering results clearly demonstrate the effectiveness of the proposed method. As shown in Table~\ref{table:compare5}, our method is also much more efficient compared to other methods. For the Pendigits data set, our method achieved \textbf{22X} and \textbf{138X} times speedup over the Consensus method and the LGSS method, respectively. For the USPS data set, our method achieved \textbf{115X} and \textbf{415X} times speedup over the Consensus method and the LGSS method, respectively.

The clustering accuracy and modularity results through iterations have been shown in Figure \ref{fig:accvsiter.png} and Figure \ref{fig:modularityvsiter.png}, respectively. As observed, with only a few iterations, the optimized graph will suffice for achieving the very good clustering results. The curves shown in Figure \ref{fig:accvsiter.png} and Figure \ref{fig:modularityvsiter.png} show that with increasing modularity, the accuracy of graph-based clustering algorithm is simultaneously increased, demonstrating the validity of the modularity theory. From this point of view, the proposed graph topoloy optimization method can also be seen as a self-boosting clustering method.


\section{Conclusion}\label{sect:conclusions}
In this work, we present a modularity-guided graph topology optimization method. We show that the graph topology learning problem can be solved by iteratively identifying and removing redundant and misleading edges to increase the modularity of the graph. Unlike traditional modularity-based  approaches which focus on updating cluster-membership of samples to maximize the modularity, our method aims to optimize the graph topology through the modularity maximization process. When comparing with state-of-the-art graph
construction (learning) approaches, our approach is more efficient and leads to substantially improved solution quality of graph-based machine learning methods. From clustering perspective, our method can also be used as a self-boosting clustering method.












\nocite{*}

\bibliography{apssamp}% Produces the bibliography via BibTeX.

\end{document}
%
% ****** End of file apssamp.tex ******
