\documentclass{article}



\usepackage{arxiv}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
%\usepackage{natbib}
\bibliographystyle{abbrv}
\usepackage{doi}



\title{Contrastive Transformer: Contrastive Learning Scheme with Transformer innate Patches}

%A new learning scheme using Transformer innate patches for contrastive learning}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{ \href{https://orcid.org/0000-0000-0000-0000}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Sander Riisøen Jyhne}\\
	Department of ICT\\
	University of Agder\\
	Grimstad, Norway \\
	\texttt{sander.jyhne@uia.no} \\
	%% examples of more authors
	\And
	\href{https://orcid.org/0000-0000-0000-0000}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Per-Arne Andersen} \\
	Department of ICT\\
	University of Agder\\
	Grimstad, Norway \\
	\texttt{per.andersen@uia.no} \\
        \And
	\href{https://orcid.org/0000-0000-0000-0000}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Morten Goodwin} \\
	Department of ICT\\
	University of Agder\\
	Grimstad, Norway \\
	\texttt{morten.goodwin@uia.no} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={remote sensing, contrastive learning, deep learning},
}

\begin{document}
\maketitle

\begin{abstract}
This paper presents Contrastive Transformer, a contrastive learning scheme using the Transformer innate patches. Contrastive Transformer enables existing contrastive learning techniques, often used for image classification, to benefit dense downstream prediction tasks such as semantic segmentation. The scheme performs supervised patch-level contrastive learning, selecting the patches based on the ground truth mask, subsequently used for hard-negative and hard-positive sampling. The scheme applies to all vision-transformer architectures, is easy to implement, and introduces minimal additional memory footprint. Additionally, the scheme removes the need for huge batch sizes, as each patch is treated as an image.

We apply and test Contrastive Transformer for the case of aerial image segmentation, known for low-resolution data, large class imbalance, and similar semantic classes. We perform extensive experiments to show the efficacy of the Contrastive Transformer scheme on the ISPRS Potsdam aerial image segmentation dataset. Additionally, we show the generalizability of our scheme by applying it to multiple inherently different Transformer architectures. Ultimately, the results show a consistent increase in mean IoU across all classes.
\end{abstract}


% keywords can be removed
\keywords{Remote sensing \and Contrastive Learning \and Deep Learning}


\section{Introduction}
Building segmentation in aerial images is a crucial task that has garnered much attention due to its importance in various fields, such as urban planning, disaster management, and environmental monitoring. Building segmentation provides valuable information for urban planning, such as population density estimation and urban development mapping. In the event of natural disasters, building segmentation can aid in damage assessment and relief efforts. Environmental monitoring also benefits from building segmentation, as it allows for the analysis of land use and urbanization patterns. Furthermore, accurate building segmentation is the first step towards automating map creation performed by entities such as the Norwegian Mapping Authority.

Building segmentation in aerial images is challenging due to the complex and diverse nature of real-world building structures. Aerial images can contain various types of buildings, such as skyscrapers, residential buildings, and industrial buildings, which vary in shape, size, and texture. Factors such as shadows, occlusions, and variations in lighting affect aerial images, complicating the segmentation process. Furthermore, building structures in aerial images can overlap or be partially obscured by other objects, making it difficult to segment individual buildings accurately.

Previous research on building segmentation has primarily focused on applications such as urban planning, disaster damage assessment, and change detection. However, these applications often have different precision requirements than map production, which creates a research gap when considering the level of accuracy needed for segmentation masks. Also, models must be able to effectively segment various types of buildings and environments, including those in urban and rural settings.

In this study, we propose Contrastive Transformer, a supervised contrastive learning scheme based on the transformer architecture. This model aims to improve the accuracy of existing segmentation models by utilizing innate patches in the Transformer architecture to perform both intra- and inter-image contrastive learning using existing contrastive loss functions. Depending on the patch and image size, each image can contain over 20,000 patches collected from four stages. The collection of patches is guided by the ground truth mask, which constrains the anchor and positive samples to patches with a homogenous class distribution. Negative samples, on the other hand, can have a heterogeneous class distribution as long as they do not contain the class from the anchor and positive samples. To further enhance contrastive learning, naive hard-sample mining is applied to select the best samples from the patches.

As a summary of the above, this paper presents the following key contributions to computer vision segmentation tasks:
\begin{enumerate}
\item The paper proposes a supervised contrastive learning scheme called Contrastive Transformer that utilizes innate patches in the Transformer architecture for dense downstream prediction tasks such as semantic segmentation.
\item The scheme performs supervised patch-level contrastive learning, selecting the patches based on the ground truth mask, subsequently used for hard-negative and hard-positive sampling.
\item The scheme applies to all vision-transformer architectures is easy to implement, and introduces minimal additional memory footprint.
\item The proposed Contrastive Transformer removes the need for huge batch sizes, as each patch is treated as an image.
\item The paper tests the Contrastive Transformer on the ISPRS Potsdam aerial image segmentation dataset, known for low-resolution data, large class imbalance, and similar semantic classes.
\item The scheme is shown to be effective in improving the accuracy of existing segmentation models and increases mean IoU across all classes consistently.
\item The proposed scheme has potential applications in dense prediction fields where accurate semantic representations are crucial, such as map production.
\end{enumerate}

This paper is structured as follows. Section \ref{sec:related_work} provides an overview of prior work, highlighting gaps in the current literature that motivate our contributions. In Section \ref{sec:ct}, we introduce our primary contribution, the Contrastive Transformer learning scheme. We evaluate the proposed method and compare it to previous state-of-the-art methods in aerial building segmentation in Section \ref{sec:experiments}. Finally, in Section \ref{sec:conclusion}, we summarize our findings and discuss the implications of our work. We also explore potential future directions for the Contrastive Transformer in Section \ref{sec:future_work}.

\section{Related Work}
\label{sec:related_work}

Advances in semantic segmentation often depend on developing more powerful deep neural networks. Initially, these networks are variations of deep convolutional neural networks, such as the U-Net model \cite{Ronneberger2015U-net:Segmentation}. However, since the release of \cite{Dosovitskiy2021AnScale}, the Transformer architecture is the most prominent method for large-scale semantic segmentation. The main issue with Transformers was the scalability, as the self-attention mechanism has quadratic time and space complexity \cite{DumanKeles2023OnSelf-Attention}. In the preceding years, several works have reduced the computational complexity while maintaining or increasing the accuracy. The authors of \cite{Liu2022SwinResolution} introduce a hierarchical transformer model using shifted windows, effectively reducing self-attention computation. Other works, such as \cite{Yu2022MetaFormerVision}, replace the attention-based module with a simple spatial pooling operator performing basic token mixing. Furthermore, in \cite{Li2022UniFormer:Recognition}, the authors propose a more generalizable model that uses convolutions and self-attention for vision tasks. Lastly, current state-of-the-art models for aerial image segmentation is dominated by transformer-based architectures. The top 2 performing models on the ISPRS Potsdam Dataset \cite{isprs.orgISPRSDataset} use the Swin Transformer \cite{Liu2022SwinResolution} as the backbone, a strong argument for advancing the performance of transformers.

A different approach is creating optimization strategies that increase accuracy without changing the network architecture, such as new loss functions. Contrastive Transformer is a transformer-based approach that utilizes the inherent patch representations from the backbone to perform contrastive learning, improving the internal representations of the classes. Contrastive learning is a technique that aims to reduce the distance between the representations of the same class while increasing the distance between representations of different classes. Contrastive learning uses a similarity metric to evaluate two representations and then gives feedback to the network based on the measure and class. The latest contrastive frameworks determine similarity scores using the global representation of the data, often used in image classification \cite{Chen2020ARepresentations, Dao2021Multi-LabelLearning, vandenOordDeepMind2018RepresentationCoding, He2020MomentumLearning}. A different approach is to use dense representations for contrastive learning, where the representation of a group of pixels of a specific class is used instead of the global representation of the image. Using dense representations shows better performance for dense prediction tasks such as object detection \cite{Liu2021BootstrappingContrast}.

Existing literature on contrastive learning for semantic segmentation explores several different approaches. The most common is to operate using a two-stage training process, where the backbone is pre-trained using contrastive learning and then finetuned for semantic segmentation. Both \cite{Zhang2021LookingLearning, Zhao2021ContrastiveSegmentation} use generated auxiliary labels in combination with the ground truth labels to perform contrastive learning with substantial memory consumption. In contrast, our work performs end-to-end training without requiring a two-stage process. Other works, such as \cite{Wang2021ExploringSegmentation, Alonso2021Semi-SupervisedBank}, are also trained end-to-end. However, they use a memory bank to store features during training, while our approach chooses features on the fly. Furthermore, the active sampling in \cite{Alonso2021Semi-SupervisedBank} uses class-specific attention modules, whereas our approach uses ground truth labels to choose the patches used for contrastive learning. In \cite{Liu2021BootstrappingContrast}, the authors introduce a contrastive learning framework for regional learning to assist semantic segmentation with end-to-end learning and active sampling. Unlike our approach, they sample key pixels using a class relationship graph, while hard queries are sampled using the predicted confidence map.

\section{Contrastive Transformer (CT)}
\label{sec:ct}
Contrastive Transformer (CT) is a patch-based contrastive learning scheme that exploits the innate patch-based architecture in vision transformers, resulting in enhanced semantic representation of classes in a Transformer backbone. Figure \ref{fig:overview} presents a high-level overview of the CT approach. The procedure involves processing an input image using the transformer backbone. A contrastive loss is computed between positive and negative samples at each backbone stage. This generates contrastive feedback at multiple abstraction levels, allowing for learning from more robust data signals. The contrastive loss is computed at both the initial and final encoding stages. In addition, a normal segmentation loss is employed, which jointly optimizes the model. The CT methodology is straightforward yet highly effective and can be applied to all Transformer models. Furthermore, it can be paired with most image-based contrastive learning methods, improving performance for dense prediction tasks.

\begin{figure*}[ht]
    \centering\includegraphics[width=0.8\linewidth]{figures/method_model_overview.png}
    \caption{CT uses the innate patch representations from each encoder stage and calculates the contrastive loss between positive and negative samples using the ground truth mask.}
    \label{fig:overview}
\end{figure*}

Let $(X, Y)$ represent the training dataset, where $x \in X$ represents the training images, and $y \in Y$ represents the pixel-level classes in the dataset. A segmentation network $g$ is trained to learn the mapping $g_\theta : X \rightarrow Y$, where $\theta$ represents the network's parameters. The segmentation network $g$ consists of two components, the backbone $\phi$, which maps $\phi : X \rightarrow Z$, and the decoder $\omega$, which maps $\omega : Z \rightarrow Y$. To perform patch-level contrastive learning, we attach a projection head $\psi$ in parallel to the decoder network on the $\phi$ mapping, where $\psi : Z \rightarrow F$ and $F$ is an $n$-dimensional representation of $Z$. The projection head only incurs additional computational costs during training and is removed during inference.

CT collects all encoder stage patch representations during training and couples them with the corresponding ground truth. For each encoder stage $s$, we have feature patches $F_s$ and corresponding ground truth patches $G_s$ constructed with the same patch size as the feature patches. For each class $c$ in $G$, we sample positive patches $P_s$ and negative patches $N_s$ from $F_s$. $P_s$ are sampled from $F_s$ where $G_s$ have a homogenous class distribution of class $c$. Similarly, $N_s$ are sampled from $F_s$ where class $c$ is not in $G_s$. Figure \ref{fig:contrastive} visualizes the sampling process for positive and negative samples. The positive and negative samples are then used in a contrastive loss function.

\begin{figure*}[ht]
    \centering\includegraphics[width=0.8\linewidth]{figures/method_model_contrastive.png}
    \caption{CT contrastive loss module takes the ground truth and constructs patches the same size as the feature patches for the particular stage. Furthermore, it uses the ground truth patches to create a mask that defines the positive and negative patches. Positive patches contain a homogenous class distribution of the target class. Negative patches can be heterogenous or homogenous if they do not contain the target class. Finally, the selected positive and negative patches are used in a contrastive loss function, pulling the representation of positive patches closer and pushing the representation of negative patches further away.}
    \label{fig:contrastive}
\end{figure*}

\subsection{Sampling and Loss Functions}
Sampling strategies and loss functions are two critical components of contrastive learning. Firstly, we use a sampling strategy to filter positive and negative samples for the contrastive loss function. Our experiments use a naive sampling strategy that randomizes all patches in the batch for both positive and negative samples. For negative contrastive learning, we calculate the cosine similarity for a fixed number of the positive and negative pairs, sort them in descending order, and take the top 50\%. For positive contrastive learning, we split the positive samples into two halves, compute the cosine similarity, sort them in ascending order, and take the top 50\%.

Once the sampling process is complete, we calculate the loss using a contrastive loss function. Treating each patch as an image allows us to utilize contrastive loss functions from the image classification literature, giving us various options. Additionally, each image has many patches split among multiple stages. Therefore we're not required to use a large batch size to collect a sufficient amount of samples for contrastive learning. We use the InfoNCE \cite{vandenOordDeepMind2018RepresentationCoding} and a contrastive loss function in our experiments. 

The contrastive loss function (CL) we use takes the positive and negative samples, calculates the cosine similarity, and normalizes them to be in a range between 0 and 1, where 0 is dissimilar, and 1 is similar. Then, using the soft cross-entropy loss function with a smoothing set to 0.1, we calculate the loss, where the target is 0 for samples from different classes and 1 for samples from the same class. 

\section{Experiments}
\label{sec:experiments}
This section reveals that CT is able to improve mean IoU on the ISPRS Potsdam Dataset using image-based contrastive loss functions. Our experimental evaluation aims to answer these questions:

\begin{itemize}
    \item How well does a simple approach for transformer-based contrastive learning compare to state-of-the-art segmentation models?
    \item What conclusions can we draw about the ability to handle semantic classes smaller than the smallest patch in the Transformer?
    \item How does the learning scheme perform with different types of transformer backbones?
\end{itemize}

\subsection{Experimental Setup}
We test the effectiveness of our proposed learning scheme on the International Society for Photogrammetry, and Remote Sensing (ISPRS) Potsdam semantic labeling dataset \cite{isprs.orgISPRSDataset}. It contains 38 tiles, each of size 6000x6000. Similar to previous research \cite{Liu2020DenseClassification, Audebert2018BeyondNetworks, Liu2018SemanticNetwork}, we use 24 tiles for training and 14 tiles for testing. We create image tiles of size 500x500 from the original tile and resize them to 512 for training and testing purposes. We do not use the DSMs in our experiments, mainly because we want to advance image-based research.

We evaluate CT using DCSwin \cite{wang2022aimages}, UnetFormer \cite{Wang2022UNetFormer:Imagery}, and the PoolFormer \cite{Yu2022MetaFormerVision} backbone with the DCSwin \cite{wang2022aimages} decoder. All experiments use a learning rate of $8e-5$, batch size of $32$, and the AdamW optimizer. We clip both the gradients and the contrastive loss to $1.0$ to stop the contrastive learning from being the dominant optimization factor. All experiments use a joint loss function of soft cross-entropy loss and dice loss. Each experiment is trained using 50 epochs and averaged across 3 separate runs.

\subsection{Results}
\label{sec:results}

We compare our results with baselines for all models, where the only difference is the addition of the CT learning scheme. We evaluate CT with InfoNCE and CL, two popular loss functions for image-based contrastive learning. Table \ref{tab:results} presents the results for the experiments, displaying consistent improvements compared to the baselines for at least one of the contrastive loss functions for each model.

\begin{table*}
\scriptsize
  \begin{center}
    \caption{Evaluation results on the ISPRS Potsdam dataset for all models. Values in the mIoU column display the average mIoU across all runs with the standard deviation behind in parentheses. The values in the class columns are the average IoU for the class across each run. The results show that CT achieves comparable or better results for all models.}
    \label{tab:results}
    \begin{tabular}{ccccccccccc}
      \toprule
      \emph{Model} & \emph{Backbone} & \emph{Contrastive Loss} & \emph{mIoU $(\sigma)$} & \emph{Surface} & \emph{Building} & \emph{Vegetation} & \emph{Tree} & \emph{Car} & \emph{Clutter} & \emph{Source}\\
      \midrule
      DCSwin (Baseline) & Swin & - & 0.7728 (0.0016) & 0.8284 & 0.8852 & \textit{0.7309} & \textit{0.7292} & 0.6906 & 0.3182 &\cite{wang2022aimages}\\
      \hline
      \textbf{DCSwin} & \textbf{Swin} & \textbf{CL} & \textbf{0.7740 (0.0019)} & \textit{0.8303} & \textit{0.8900} & 0.7284 & 0.7267 & \textit{0.6945} & 0.3073 & Ours\\
      \hline
      DCSwin & Swin & InfoNCE & 0.7730 (0.0013) & 0.8263 & 0.8855 & 0.7257 & 0.7311 & 0.6924 & \textit{0.3200} & Ours\\
      \midrule
      UnetFormer (Baseline) & Swin & - & 0.7687 (0.0011) & 0.8293 & 0.8891 & 0.7218 & 0.7191 & 0.6842 & 0.3153 & \cite{Wang2022UNetFormer:Imagery}\\
      \hline
      UnetFormer & Swin & CL & 0.7674 (0.0017) & 0.8275 & 0.8874 & 0.7204 & 0.7145 & 0.6874 & 0.3181 & Ours\\
      \hline
      \textbf{UnetFormer} & \textbf{Swin} & \textbf{InfoNCE} & \textbf{0.7715 (0.0006)} & \textit{0.8300} & \textit{0.8896} & \textit{0.7266} & \textit{0.7217} & \textit{0.6896} & \textit{0.3337} & Ours\\
      \midrule
      DCPoolFormer (Baseline) & PoolFormer & - & 0.7559 (0.0021) & 0.8113 & 0.8694 & \textit{0.7084} & 0.7053 & 0.6853 & 0.2752 & \cite{wang2022aimages, Yu2022MetaFormerVision}\\
      \hline
      DCPoolFormer & PoolFormer & CL & 0.7620 (0.0055) & \textit{0.8155} & 0.8732 & 0.7068 & \textit{0.7108} & \textit{0.6887} & \textit{0.2897} & Ours\\
      \hline
      \textbf{DCPoolFormer} & \textbf{PoolFormer} & \textbf{InfoNCE} & \textbf{0.7625 (0.0068)} & 0.8126 & \textit{0.8735} & 0.7076 & 0.7080 & 0.6855 & 0.2701 & Ours\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table*}

Figure \ref{poolformer} presents the graph for max validation mIoU for the DCPoolFormer model for each experiment. It visualizes the improvements in mIoU over the training period, displaying the effectiveness of our approach.

Furthermore, it is worth noting the results for the car class. The approach outperforms the baselines on the car class, despite the situation that the car class does not fit into the smallest patch during training. CT is still able to perform better than the baseline for this class, indicating that it is sufficient for the class to be present in negative samples to create a better semantic representation than the baseline. 

\begin{figure}[ht]
    \centering\includegraphics[width=0.7\linewidth]{figures/dcpoolformer.png}
    \caption{The validation mIoU graph for DCPoolFormer clearly demonstrate the performance improvement throughout the training time.}
    \label{poolformer}
\end{figure}

It is also worth mentioning that CT performs well with both the Swin Transformer and PoolFormer backbone. However, it seems to pair better with the PoolFormer backbone, which might indicate that the Swin Transformer is better at representation learning than PoolFormer, but this needs further research. Thereby we conclude:
\begin{itemize}
    \item CT performs well with the backbone and improves upon the baselines in all but one experiment.
    \item Even though instances of cars are too small to be used as a positive sample during contrastive learning, the results indicate that it is sufficient to be part of the negative samples to improve the representation of a class. 
    \item In our experiments, we have used two different transformer-based backbones, and CT outperforms the baseline with both. However, the results indicate that it pairs better with the PoolFormer backbone. Further research is necessary to explore which types of Transformer-based backbones are best fit, and why.
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}
In this work, we introduce Contrastive Transformer (CT), a novel patch-based contrastive learning scheme for transformers. We show that CT improves performance for segmentation transformers when trained end-to-end with an on-the-fly naive sampling strategy. In particular, we want to highlight the simplicity of CT and the large potential for future work it introduces. In our experiments, we show consistent improvements in segmentation results for all models on the ISPRS Potsdam dataset.

\section{Future Work}
\label{sec:future_work}
The Contrastive Transformer is a novel mechanism that leverages the inherent patch characteristics of vision-based transformers. The results demonstrate the potential to improve the performance and robustness of transformers for dense prediction tasks. The idea of using innate Transformer patches for contrastive learning is simple yet novel and warrants further exploration. This research highlights several avenues for future investigation:

\begin{itemize}
    \item Investigating the impact of patch size and number of patches used in contrastive learning on CT's performance for dense prediction tasks.
    \item Exploring the use of various augmentation techniques on patches to improve the quality of learned representations.
    \item Developing new architectures that can better leverage the representations learned by contrastive learning.
    \item Studying the effects of different objective functions on the quality of learned representations.
    \item Investigating the use of unsupervised pre-training techniques to initialize the model before fine-tuning it for specific dense prediction tasks.
    \item Evaluating the generalization capability of learned representations across different datasets and domains.
\end{itemize}

Another interesting avenue for study is sampling techniques in CT. Preliminary studies indicate that attention could be useful for selecting the most influential patches for each stage in a batch, with work from \cite{Xia2022VisionAttention} serving as a key accelerator. Another promising approach is to explore how patches containing a heterogeneous class distribution of positive and negative classes can be utilized. Hard samples are often present around the edges of two different semantic classes, and including them in the learning scheme can have a positive impact on the results.

% in overleaf you prress the document besides "Recompile" then lower right corner "other logs and files". then download .bbl and paste here. 
\begin{thebibliography}{10}

\bibitem{Alonso2021Semi-SupervisedBank}
I.~Alonso, A.~Sabater, D.~Ferstl, L.~Montesano, and A.~C. Murillo.
\newblock {Semi-Supervised Semantic Segmentation With Pixel-Level Contrastive
  Learning From a Class-Wise Memory Bank}, 2021.

\bibitem{Audebert2018BeyondNetworks}
N.~Audebert, B.~Le~Saux, and S.~Lef{\`{e}}vre.
\newblock {Beyond RGB: Very high resolution urban remote sensing with
  multimodal deep networks}.
\newblock {\em ISPRS Journal of Photogrammetry and Remote Sensing}, 140:20--32,
  6 2018.

\bibitem{Chen2020ARepresentations}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~E. Hinton.
\newblock {A Simple Framework for Contrastive Learning of Visual
  Representations}.
\newblock {\em ArXiv}, 2020.

\bibitem{Dao2021Multi-LabelLearning}
S.~D. Dao, E.~Zhao, D.~Phung, and J.~Cai.
\newblock {Multi-Label Image Classification with Contrastive Learning}.
\newblock {\em ArXiv}, 2021.

\bibitem{Dosovitskiy2021AnScale}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby.
\newblock {An Image is Worth 16x16 Words: Transformers for Image Recognition at
  Scale}.
\newblock In {\em Proceedings of the 9th International Conference on Learning
  Representations (ICLR)}, pages 1--21, 10 2021.

\bibitem{DumanKeles2023OnSelf-Attention}
F.~Duman~Keles, P.~Mahesakya~Wijewardena, C.~Hegde, S.~Agrawal, and F.~Orabona.
\newblock {On The Computational Complexity of Self-Attention}, 2 2023.

\bibitem{He2020MomentumLearning}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick.
\newblock {Momentum Contrast for Unsupervised Visual Representation Learning},
  2020.

\bibitem{isprs.orgISPRSDataset}
{isprs.org}.
\newblock {ISPRS Potsdam Dataset}.

\bibitem{Li2022UniFormer:Recognition}
K.~Li, Y.~Wang, J.~Zhang, P.~Gao, G.~Song, Y.~Liu, H.~Li, and Y.~Qiao.
\newblock {UniFormer: Unifying Convolution and Self-attention for Visual
  Recognition}.
\newblock {\em ArXiv}, 1 2022.

\bibitem{Liu2020DenseClassification}
Q.~Liu, M.~Kampffmeyer, R.~Jenssen, and A.~B. Salberg.
\newblock {Dense dilated convolutions merging network for land cover
  classification}.
\newblock {\em IEEE Transactions on Geoscience and Remote Sensing},
  58(9):6309--6320, 9 2020.

\bibitem{Liu2021BootstrappingContrast}
S.~Liu, S.~Zhi, E.~Johns, and A.~J. Davison.
\newblock {Bootstrapping Semantic Segmentation with Regional Contrast}.
\newblock {\em ArXiv}, 4 2021.

\bibitem{Liu2018SemanticNetwork}
Y.~Liu, B.~Fan, L.~Wang, J.~Bai, S.~Xiang, and C.~Pan.
\newblock {Semantic labeling in very high resolution images via a self-cascaded
  convolutional neural network}.
\newblock {\em ISPRS Journal of Photogrammetry and Remote Sensing}, 145:78--95,
  11 2018.

\bibitem{Liu2022SwinResolution}
Z.~Liu, H.~Hu, Y.~Lin, Z.~Yao, Z.~Xie, Y.~Wei, J.~Ning, Y.~Cao, Z.~Zhang,
  L.~Dong, F.~Wei, and B.~Guo.
\newblock {Swin Transformer V2: Scaling Up Capacity and Resolution}, 2022.

\bibitem{Ronneberger2015U-net:Segmentation}
O.~Ronneberger, P.~Fischer, and T.~Brox.
\newblock {U-net: Convolutional networks for biomedical image segmentation}.
\newblock {\em Lecture Notes in Computer Science (including subseries Lecture
  Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  9351:234--241, 2015.

\bibitem{vandenOordDeepMind2018RepresentationCoding}
A.~van~den Oord~DeepMind, Y.~Li~DeepMind, and O.~Vinyals~DeepMind.
\newblock {Representation Learning with Contrastive Predictive Coding}.
\newblock {\em ArXiv}, 7 2018.

\bibitem{wang2022aimages}
L.~Wang, R.~Li, C.~Duan, C.~Zhang, X.~Meng, and S.~Fang.
\newblock {A Novel Transformer Based Semantic Segmentation Scheme for
  Fine-Resolution Remote Sensing Images}.
\newblock {\em IEEE Geoscience and Remote Sensing Letters}, 19, 2022.

\bibitem{Wang2022UNetFormer:Imagery}
L.~Wang, R.~Li, C.~Zhang, S.~Fang, C.~Duan, X.~Meng, and P.~M. Atkinson.
\newblock {UNetFormer: A UNet-like transformer for efficient semantic
  segmentation of remote sensing urban scene imagery}.
\newblock {\em ISPRS Journal of Photogrammetry and Remote Sensing},
  190:196--214, 8 2022.

\bibitem{Wang2021ExploringSegmentation}
W.~Wang, T.~Zhou, F.~Yu, J.~Dai, E.~Konukoglu, and L.~Van~Gool.
\newblock {Exploring Cross-Image Pixel Contrast for Semantic Segmentation},
  2021.

\bibitem{Xia2022VisionAttention}
Z.~Xia, X.~Pan, S.~Song, L.~E. Li, and G.~Huang.
\newblock {Vision Transformer With Deformable Attention}, 2022.

\bibitem{Yu2022MetaFormerVision}
W.~Yu, M.~Luo, P.~Zhou, C.~Si, Y.~Zhou, X.~Wang, J.~Feng, and S.~Yan.
\newblock {MetaFormer Is Actually What You Need for Vision}, 2022.

\bibitem{Zhang2021LookingLearning}
F.~Zhang, P.~Torr, R.~Ranftl, and S.~R. Richter.
\newblock {Looking Beyond Single Images for Contrastive Semantic Segmentation
  Learning}.
\newblock {\em Advances in Neural Information Processing Systems},
  34:3285--3297, 12 2021.

\bibitem{Zhao2021ContrastiveSegmentation}
X.~Zhao, R.~Vemulapalli, P.~A. Mansfield, B.~Gong, B.~Green, L.~Shapira, and
  Y.~Wu.
\newblock {Contrastive Learning for Label Efficient Semantic Segmentation},
  2021.

\end{thebibliography}

\end{document}
