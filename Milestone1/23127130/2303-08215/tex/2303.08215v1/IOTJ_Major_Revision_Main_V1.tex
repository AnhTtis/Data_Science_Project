% \documentclass[conference]{IEEEtran}
\documentclass[journal]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage[flushleft]{threeparttable}
\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage{balance}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\usepackage[a4paper, total={184mm,239mm}]{geometry}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% \renewcommand{\baselinestretch}{.96}


\begin{document}

% \title{Context-Aware Stress Detection using Adaptive Fusion of Heterogeneous Physiological Signals\\
% \title{SELF-CARE: Stress Detection Using Selective Late Fusion on Context-Aware Early Fusion of Heterogeneous Physiological Signals\\
% \title{SELF-CARE: Stress Detection Using \underline{Se}lective \underline{L}ate \underline{F}usion on \underline{C}ontext-\underline{A}wa\underline{r}e \underline{E}arly Fusion of Sensors\\
% \title{SELF-CARE: \underline{Sel}ective \underline{F}usion with \underline{C}ontext-\underline{A}wa\underline{re} Low-Power Edge Computing for Stress Detection\\

% \title{Stress Detection using Context-Aware Sensor Selection from Wearable Devices\\

\title{Stress Detection using Context-Aware\\ Sensor Fusion from Wearable Devices}
% \title{SELF-CARE: \underline{Sel}ective \underline{F}usion with \underline{C}ontext-\underline{A}wa\underline{re} Efficient Edge Computing for Stress Detection\\
% \thanks{$^*$Both authors contributed equally to this research.}


%\pagestyle{plain}

% \author{\IEEEauthorblockN{Nafiul Rashid$^{1}$, Trier Mortlock$^{2}$, Mohammad Abdullah Al Faruque$^{1,2}$}
% 		\IEEEauthorblockA{\textit{$^1$Department of Electrical Engineering and Computer Science}\\
% 			\textit{$^2$Department of Mechanical and Aerospace Engineering} \\
% 			\textit{University of California, Irvine, California, United States }\\
% 			\{nafiulr, tmortloc, alfaruqu\}@uci.edu}

% \author{\IEEEauthorblockN{Nafiul Rashid}
% \IEEEauthorblockA{\textit{Department of Electrical Engineering}\\
% \textit{and Computer Science} \\
% \textit{University of California, Irvine}\\
% Irvine, California, United States \\
% nafiulr@uci.edu}
% \and
% \IEEEauthorblockN{Trier Mortlock}
% \IEEEauthorblockA{\textit{Department of Mechanical and}\\
% \textit{Aerospace Engineering} \\
% \textit{University of California, Irvine}\\
% Irvine, California, United States \\
% tmortloc@uci.edu}
% \and
% \IEEEauthorblockN{Mohammad Abdullah Al Faruque}
% \IEEEauthorblockA{\textit{Department of Electrical Engineering}\\
% \textit{and Computer Science} \\
% \textit{University of California, Irvine}\\
% Irvine, California, United States \\
% alfaruqu@uci.edu}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% }

\author{Nafiul~Rashid,~\IEEEmembership{Student Member,~IEEE,}
    Trier Mortlock,
    and~Mohammad~Abdullah~Al~Faruque,~\IEEEmembership{Senior~Member,~IEEE}% <-this % stops a space
    \thanks{ Nafiul Rashid is a PhD candidate in the Department of Electrical Engineering
        and Computer Science; Trier Mortlock is a PhD candidate in the Department of Mechanical and Aerospace Engineering, and Mohammad Abdullah Al Faruque is a professor in both Electrical Engineering
        and Computer Science and Mechanical and Aerospace Engineering at the University of California, Irvine, CA
        92697, USA, e-mail: (nafiulr@uci.edu)}% <-this % stops a space
    %\thanks{Manuscript received April 19, 2020; revised August 26, 2015.}
    \thanks{ Copyright (c) 2023 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org.}
}

\markboth{IEEE Internet of Things Journal,~Vol.~XX, No.~XX, XXXX~2023}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}


\maketitle

% \begin{abstract}
% %One intro sentence on background of problem and focus of the paper.
% Human stress detection is a key application of mobile health that has important implications for physical well-being and health. Detecting stress can also significantly improve affective computing---that is, the ability for machines to autonomously process and understand human interactions while displaying empathy or emotions. State-of-the-art methods of stress detection rely on training machine learning models with data from physiological body-worn sensors. Robustness remains a challenging problem for wearable sensors that are often susceptible to measurement noise. Methods of sensor fusion are commonly used to combine sensor data to improve stress classification performance. However, very little work has been done on context-aware sensor fusion that takes the human physical state into consideration. 
% %In our previous work, we proposed \textbf{SELF-CARE}---a fully wrist-based method for stress detection that employs context-aware selective sensor fusion that dynamically adapts based on the sensor inputs. 
% %Our analysis, however, only focused on wrist-worn devices and context-aware sensor selection for chest-worn devices have yet to be examined.  
% Some initial work has shown that stress detection using wrist-based wearable devices can be improved by modeling context, however, the differences in using chest-based wearable devices have yet to be examined. 
% Moreover, contextual modeling may vary based on the location of the wearable devices (e.g., chest or wrist) which will in turn impact the sensor fusion decisions.
% In this paper, we propose SELF-CARE, a generalized selective sensor fusion method of stress detection that utilizes wearable devices, and we explicitly study the difference between sensing modalities. SELF-CARE dynamically adjusts the sensors selected for fusion based on sensor inputs, attempting to maximize classification performance while also conforming to energy constraints in the wearable device. 
% %SELF-CARE employs a context-aware selective sensor fusion approach that dynamically adapts based on the sensor inputs.
% %In this work, we extend our framework to include analysis of chest-worn sensor devices that can identify the context of the system through multiple sensing modalities (electromyography or motion).
% %to determine the context of the system based on where it is worn (chest or wrist) and adjusts the fused sensors accordingly. \textbf{SELF-CARE} is a stress detection method that employs context-aware selective sensor fusion that dynamically adapts based on the sensor inputs. 
% This results in significant performance improvement compared to state-of-the-art works. \textbf{SELF-CARE} obtains state-of-the-art performance across the publicly available WESAD dataset. Using wrist-based sensors, our methodology achieves 86.34\% and 94.12\% accuracy for the 3-class and 2-class classification problems, respectively. Similarly, for chest-based wearable sensors, our methodology achieves 86.19\% (3-class) and 93.68\% (2-class) classification accuracy while outperforming state-of-the-art works on stress detection. Moreover, we empirically demonstrate that while motion sensors are well suited for contextual understanding in wrist-wearable devices, electromyography sensors enable higher levels of performance in chest-wearable devices.

% % One sentence introducing our framework and what its about. 1-2 sentence to stress our main contributions. Final concluding sentence with claims on how we present state of the art results and consider adding numbers. 
% % Make sure to cover big key words throughout.
% \end{abstract}

% \begin{abstract}
% %One intro sentence on background of problem and focus of the paper.
% Human stress detection is one of the key applications of mobile health that has direct implications for physical well-being and health. Stress detection can also significantly improve affective computing.
% %---that is, the ability for machines to autonomously process and understand human interactions while displaying empathy or emotions. 
% State-of-the-art methods of stress detection rely on training machine learning models with data from physiological body-worn sensors. Robustness remains a challenging problem for wearable sensors that are often susceptible to measurement noise that in turn affects stress classification performance. Methods of sensor fusion are commonly used to combine sensor data to improve stress classification performance. However, very little work has been done on sensor fusion based on the noise context that takes the location of the wearable devices into consideration. The noise context of wearable health sensors is referred to as the group of external factors that can influence the variation in measurements and noise levels of the sensors.
% %In our previous work, we proposed \textbf{SELF-CARE}---a fully wrist-based method for stress detection that employs context-aware selective sensor fusion that dynamically adapts based on the sensor inputs. 
% %Our analysis, however, only focused on wrist-worn devices and context-aware sensor selection for chest-worn devices have yet to be examined.  
% % Some initial work has shown that stress detection using wrist-based wearable devices can be improved by modeling context, however, the differences in using chest-based wearable devices have yet to be examined. 
% To the best of our knowledge, we are the first to demonstrate that noise context varies  based on the location of the wearable devices (e.g., chest or wrist) which in turn impacts the sensor fusion decisions.
% In this paper, we propose SELF-CARE, a generalized selective sensor fusion method of stress detection that determines the noise context based on wearable device location. Findings in this paper suggest that, while motion is most suitable to understand the noise context in wrist-worn devices, muscle contraction is more suitable for determining noise context in chest-worn devices. SELF-CARE dynamically adjusts the sensors selected for fusion based on noise context, attempting to maximize classification performance. 
% %while also conforming to energy constraints in the wearable device. 
% %SELF-CARE employs a context-aware selective sensor fusion approach that dynamically adapts based on the sensor inputs.
% %In this work, we extend our framework to include analysis of chest-worn sensor devices that can identify the context of the system through multiple sensing modalities (electromyography or motion).
% %to determine the context of the system based on where it is worn (chest or wrist) and adjusts the fused sensors accordingly. \textbf{SELF-CARE} is a stress detection method that employs context-aware selective sensor fusion that dynamically adapts based on the sensor inputs. 
% This results in significant performance improvement compared to state-of-the-art works. \textbf{SELF-CARE} obtains state-of-the-art performance across the publicly available WESAD dataset. Using wrist-based sensors, our methodology achieves 86.34\% and 94.12\% accuracy for the 3-class and 2-class classification problems, respectively. Similarly, for chest-based wearable sensors, our methodology achieves 86.19\% (3-class) and 93.68\% (2-class) classification accuracy while outperforming state-of-the-art works on stress detection. 
% %Moreover, we empirically demonstrate that while motion sensors are well suited for contextual understanding in wrist-wearable devices, electromyography sensors enable higher levels of performance in chest-wearable devices.

% % One sentence introducing our framework and what its about. 1-2 sentence to stress our main contributions. Final concluding sentence with claims on how we present state of the art results and consider adding numbers. 
% % Make sure to cover big key words throughout.
% \end{abstract}

\begin{abstract}
Wearable medical technology has become increasingly popular in recent years. One function of wearable health devices is stress detection, which relies on sensor inputs to determine a patient’s mental state. This continuous, real-time monitoring can provide healthcare professionals with vital physiological data and enhance the quality of patient care.
Current methods of stress detection lack: (i) robustness---wearable health sensors contain high levels of measurement noise that degrades performance, and (ii) adaptation---static architectures fail to adapt to changing contexts in sensing conditions.
We propose to address these deficiencies with SELF-CARE, a generalized selective sensor fusion method of stress detection that employs novel techniques of context identification and ensemble machine learning. SELF-CARE uses a learning-based classifier to process sensor features and model the environmental variations in sensing conditions known as the noise context. SELF-CARE uses noise context to selectively fuse different sensor combinations across an ensemble of models to perform robust stress classification. Our findings suggest that for wrist-worn devices, sensors that measure motion are most suitable to understand noise context, while for chest-worn devices, the most suitable sensors are those that detect muscle contraction.
We demonstrate SELF-CARE’s state-of-the-art performance on the WESAD dataset. Using wrist-based sensors, SELF-CARE achieves 86.34\% and 94.12\% accuracy for the 3-class and 2-class stress classification problems, respectively. For chest-based wearable sensors, SELF-CARE achieves 86.19\% (3-class) and 93.68\% (2-class) classification accuracy. This work demonstrates the benefits of utilizing selective, context-aware sensor fusion in mobile health sensing that can be applied broadly to Internet of Things applications.
%{Wearable health devices can be leveraged for emotional and stress state detection, allowing for increased levels of health monitoring and physical well-being. Current methods of stress detection rely on training machine learning models with data from wearable devices, however, robustness remains a key challenge as wearable sensors are susceptible to measurement noise that degrades stress detection performance. Sensor fusion across a wearable device’s sensors is commonly performed to account for variations and gaps in individual sensor coverage, but most current works employ static architectures that fail to adapt to changing contexts in sensing conditions. In this paper, we propose SELF-CARE, a generalized selective sensor fusion method of stress detection that employs novel techniques of context identification and ensemble machine learning. SELF-CARE uses a learning-based classifier to process sensor features and model the environmental variations in sensing conditions known as the noise context. SELF-CARE uses this noise context to selectively fuse different sensor combinations across an ensemble of models to perform robust stress classification. Findings in this paper suggest that, while motion is most suitable to understand the noise context in wrist-worn devices, muscle contraction works best for determining noise context in chest-worn devices. We demonstrate SELF-CARE’s state-of-the-art performance through extensive evaluation on the public WESAD dataset. Using wrist-based sensors, SELF-CARE achieves 86.34\% and 94.12\% accuracy for the 3-class and 2-class stress classification problems, respectively. Similarly, for chest-based wearable sensors, SELF-CARE achieves 86.19\% (3-class) and 93.68\% (2-class) classification accuracy. Additionally, we provide a benchmark for selective sensor fusion performance for stress detection. This work demonstrates the benefits of utilizing selective, context-aware sensor fusion in mobile health sensing that can be applied broadly to Internet of Things applications.} 
%Human stress detection is one of the key applications of mobile health that has direct implications for physical well-being. State-of-the-art methods of stress detection rely on training machine learning models with data from wearable devices. However, wearable sensors are often susceptible to measurement noise that in turn affects stress detection performance. Methods of sensor fusion are commonly used to combine sensor data to improve stress classification performance. However, very little work has been done on sensor fusion based on the noise context that takes the location of the wearable devices into consideration. 
%In this paper, we propose SELF-CARE, a generalized selective sensor fusion method of stress detection that determines the noise context based on wearable device location. Findings in this paper suggest that, while motion is most suitable to understand the noise context in wrist-worn devices, muscle contraction works best for determining noise context in chest-worn devices. SELF-CARE dynamically adjusts the sensors selected for fusion based on noise context. This results in significant performance improvement compared to state-of-the-art works. SELF-CARE obtains state-of-the-art performance across the publicly available WESAD dataset. Using wrist-based sensors, SELF-CARE achieves 86.34\% and 94.12\% accuracy for the 3-class and 2-class classification problems, respectively. Similarly, for chest-based wearable sensors, SELF-CARE achieves 86.19\% (3-class) and 93.68\% (2-class) classification accuracy while outperforming state-of-the-art works.
\end{abstract}

\begin{IEEEkeywords}
Stress Detection, Context-Aware Models, Wearable Health Sensor Fusion, Ensemble Learning.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}

% Points to cover: affective computing, health importance and benefits, other domains (military + first-responders), sensor fusion in physiological sensors and stress recognition, context-aware topics, adaptive frameworks, low-power/energy-efficient.

% Low power/Energy efficient approaches to cite:  \cite{naeini2021amser}
% \cite{ragav2019scalable}

% \textcolor{blue}{One paragraph here introducing the problem: background, relevance, importance to other applications, why it matters. Need some more background on what dcoss is all about, or what track we are submitting to (dependable autonomy in smart healthcare sensor systems, integration of sensing and machine intelligence, wearable sensor and low power while being efficient and robust).} 

{Advancement in technology and the prevalence of Internet of Things (IoT) has led to the wide adoption of wearable medical devices in recent years.} 
{Wearable medical devices have shaped the study and practice of healthcare by allowing continuous, remote monitoring of vital physiological signs.}
{Wearable health devices can also be used for stress detection, which uses inputs from body-worn sensors to analyze a patient's mental state.}
%been redefining the future of healthcare by allowing continuous monitoring of vital signs and physiological signals on a daily basis. Among various applications of wearable devices \cite{aharrashid2022, rashid2021hear,rashid2022template,rashid2022self}, stress detection has been one such application that has enormous significance in determining human health and well-being in general \cite{american2020stress,goldstein2010adrenal,lai2021intelligent}.
{Stress detection is of growing interest as recently the American Psychological Association issued a warning about long-term physical and mental health impacts due to stresses from the COVID-19 Pandemic, deeming it a \textit{`a national mental health crisis'} \cite{american2020stress}.}
%Especially in recent years, the COVID-19 pandemic has exacerbated stress levels worldwide.
%The American Psychological Association issued a recent warning about long-term physical and mental health impacts due to stresses from the pandemic, deeming it a \textit{`a national mental health crisis'} \cite{american2020stress}.

% The future of smart healthcare requires dependable sensor systems that can operate at increased levels of autonomy while providing valuable health-related information. Integrating machine intelligence with physiological sensor systems helps to push this vision of autonomy, where energy-efficient wearable technology must also be considered. One area gaining significant attention is \textit{affective computing}, or the ability for machines to understand human emotional states. Stress detection is one of example of affective computing that allows machines to detect stress levels within humans, which has a myriad of implications for healthcare science \cite{american2020stress,goldstein2010adrenal,lai2021intelligent}.


%Stress detection is one of the examples of affective computing that not only allows machines to more effectively display empathy in their interactions, but also has many important health consequences \cite{lai2021intelligent}.
%where the task is to determine the stress levels of a human using only wearable health sensors.
%Detecting stress levels within humans 
{Medically, stress is a physiological state that can be triggered by hormonal surges during moments of physical, cognitive, or emotional challenges \cite{goldstein2010adrenal}.}
%Stress can be interpreted as a physiological state that is triggered by hormonal surges during moments of physical, cognitive, or emotional challenges such as \textit{``fight or flight"} situations\cite{goldstein2010adrenal}. 
Stress detection falls under the umbrella of \textit{affective computing}---the area of computing that allows machines to recognize and interpret human emotions \cite{picard2000affective}. 
{Affective computing using wearable devices is a rapidly developing industry, the value of which is projected to expand from \$29 billion to \$140 billion---an increase of nearly five times---by 2025 \cite{affective_computing_market}.}
%According to market research, affective computing using wearable devices is expected to grow from \$29 billion to \$140 billion by 2025 with a compound annual growth rate (CAGR) of 37.40\% \cite{affective_computing_market} since the COVID-19 pandemic. 

% In recent years, the COVID-19 pandemic has only exacerbated stress levels worldwide.
% The American Psychological Association issued a recent warning about long-term physical and mental health impacts due to the stresses from the pandemic, deeming it a \textit{`a national mental health crisis'} \cite{american2020stress}.

%[Need to add importance of stress detection in terms of COVID-19] 
% Stress detection is similarly important in many other fields such as stress monitoring for first-responders \cite{lai2021intelligent}, add more related fields here.

% \textcolor{blue}{One paragraph here introducing how it is currently done and all the challenges of the problem. Include relevant background information for the rest of the paper. Answer how previous works have done feature engineering and time-series alignment with sensors that have differing frequencies.}

\subsection{{Research Challenges}}
%Mention state-of-the-art work in general and why machine learning is preferred over deep learning.
%Stress detection via physiological sensor data has been widely studied \cite{kim2008emotion,gjoreski2016continuous,hovsepian2015cstress,healey2005detecting,picard2001toward,koelstra2011deap,schmidt2018introducing}.
{The increasing prevalence of wearable health technology---and the data that can be gleaned from this technology---has given rise to a body of academic literature focusing on stress detection \cite{kim2008emotion,gjoreski2016continuous,hovsepian2015cstress,healey2005detecting,picard2001toward,koelstra2011deap,schmidt2018introducing}.}
%The growing usage of physiological sensors and the increasing availability of their data has led to many studies focusing on stress detection \cite{kim2008emotion,gjoreski2016continuous,hovsepian2015cstress,healey2005detecting,picard2001toward,koelstra2011deap,schmidt2018introducing}.  
%As no known physics-based model can exactly relate this sensor data to stress states, classical machine learning models (random forests, decision trees, etc.) or deep learning models (convolutional neural networks, long short-term memory, etc.) are often used to perform the stress classification as models learn over labeled datasets \cite{samyoun2020stress, huynh2021stressnas, rashid2021feature, ragav2019scalable}. %\cite{}.
The relationship between this sensor data and stress states is not governed by known physical equations. {As a result, researchers have used classical machine learning models (e.g., random forests, decision trees) or deep learning models (e.g., convolutional neural networks, long short-term memory) to perform stress classification via supervised learning over labeled datasets with annotated stress states \cite{samyoun2020stress, huynh2021stressnas, rashid2021feature, ragav2019scalable}.}
% [sentence here explaining the dominance of statistical machine learning models in this field - can mention no direct physics model between the sensors and human emotions].
% Effective methods of preprocessing, feature engineering, and time-series alignment of unsynchronized sensors have all been shown [insert citations]. 
%Machine learning models have been proven more popular than deep learning models due to their ability to detect stress from heterogeneous physiological signals with much lower complexity
% \cite{schmidt2019multi}. 
%However, machine learning models often lack the ability to incorporate temporal aspects of the data, whereas some deep learning models can include these temporal dependencies.
Deep learning models have benefits in their ability to incorporate temporal modeling from the sensor data into the stress detection problem.
Despite this, in stress detection, classical machine learning models have been more widely adopted compared to deep learning models due to the classical models' lower complexity levels, important for wearable on-device deployment \cite{schmidt2019multi}.  
{However, both of these types of learning-based methods lack robustness when using single sensor modalities, since the coverage area of each sensing modality is limited by the domain in which the sensors operate \cite{naeini2021amser}.}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{Figures/Motivation_Human_Body.png}
%     \caption{Wearable sensors used for stress detection. Impact of motion on sensors depends on their respective locations on the human body.} 
%     \label{fig:mot_human_body}
%     % \vspace{-5mm}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{Figures/Wrist_Mot_1.png}
%     \caption{Physiological Signals from Wrist Sensors. A baseline segment where BVP and EDA is affected due to motion. Data taken from wrist sensors on one subject from WESAD dataset\cite{schmidt2018introducing}.}
%     \label{fig:mot_wrist}
%     % \vspace{-5mm}
% \end{figure}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/wrist_chest_motivation.png}
    \caption{The context of noise from sensor measurements depends on the respective sensor locations on the human body. a) Physiological signals from chest sensors. A baseline segment where EMG affects ECG and RESP even with no motion, whereas ECG remains unaffected even during motion. This shows that EMG is more suitable than ACC to understand the noise context from chest wearable devices. b) Physiological signals from wrist sensors. A baseline segment where BVP and EDA is affected due to motion. Hence, ACC is more suitable  to understand the noise context in wrist wearable devices. Both sets of data in a) and b) are taken from wrist and chest sensors on one subject from the WESAD dataset\cite{schmidt2018introducing}.}
    \label{fig:mot_wrist_chest}
    % \vspace{-5mm}
\end{figure*}



% Justify the importance of context
%Methods using sensor fusion across multi-modal physiological data have been commonly used to increase performance of emotion recognition \cite{bota2020emotion,naeini2021amser}.
Researchers commonly use sensor fusion across multi-modal physiological data to increase the performance of emotion recognition from wearable devices \cite{bota2020emotion}. {\textit{Early} fusion (also known as feature-level fusion) focuses on combining data at the raw-data level. Alternatively, \textit{late} fusion (also known as decision-level fusion) combines the final outputs of a system.} 
%Even methods that employ both early and late fusion are noticeably limited, since they have static architectures that cannot adapt to changing contexts \cite{malawade2022hydrafusion}. 
Current methods of sensor fusion that employ combinations of early and late fusion still have limited efficacy due to the use of static architectures that cannot adapt to changing sensing conditions within the environment \cite{malawade2022hydrafusion}. 

Another notable challenge in using data from these physiological signals for affective computing is that the data may be susceptible to substantial amounts of sensor noise due to physical motion or muscle contraction.
%\blue{Fusing noisy sensor data has even been shown to reduce model performance \cite{schmidt2018introducing,malawade2022hydrafusion}.}
% Fig. \ref{fig:mot_wrist_chest} shows that motion has different impact on other sensors based on the location of wearable device.
Throughout the remainder of this paper, we define the noise context of wearable health sensors as the group of external factors that can influence the variation in measurements and noise levels of the sensors. 
This context can be interpreted through intra-sensor relationships in the device as well as through sensing conditions surrounding the device {(\textit{e.g.}, the location of a wearable sensor on the body)}. {And fusing data from multiple sensors without understanding the noise context may lead to performance degradation as found in \cite{schmidt2018introducing}.} 

{The main research challenges we address in this work include: (i) how to effectively fuse multi-modal sensor data from wearable devices; (ii) how to develop an adaptive architecture to account for variations in sensing conditions; and (iii) how to model noise context in wearable sensors to improve stress classification performance.}

%the variation in sensor performance due to external factors influencing the human body as a system ... relationship between ...}

\subsection{{Motivation}}
{In this subsection, we provide motivation and qualitative analysis regarding the challenges our approach addresses.}  
Fig. \ref{fig:mot_wrist_chest} shows that the context of noise on sensors varies depending on the location of the wearable device.
Fusing such noisy measurements can subsequently degrade the classification performance \cite{schmidt2019multi}. {For example, Fig. \ref{fig:mot_wrist_chest} b) represents a baseline segment of data from four wrist sensing modalities: tri-axis accelerometer (ACC), blood volume pulse (BVP), electrodermal activity(EDA), skin temperature (TEMP). At several times during the segment, significant motion causes two of the sensors (BVP, EDA) to vary in their readings, which could cause a model to classify this segment \textit{incorrectly} as stress.} {Therefore, it is important to understand the noise context when making sensor fusion decisions. Moreover, it also shows that motion sensors (ACC) have benefits for modeling the noise context in wrist-worn devices.}

On the other hand, Fig. \ref{fig:mot_wrist_chest} a) shows data from six sensing modalities from the chest (ACC, electromyography: EMG, electrocardiogram: ECG, EDA, TEMP, respiration: RESP) for a baseline segment of the subject. While chest motion may affect EMG and EDA, it does not affect ECG. {However, EMG may be affected even without any motion when the subject makes muscle contractions without moving.} This may in turn affect ECG and RESP as shown in Fig. \ref{fig:mot_wrist_chest}. Thus, for chest wearable devices, motion is not the best modality to understand the noise context for sensor fusion decisions. Rather, EMG is more suitable for chest-worn devices which is empirically validated later in Section \ref{sec:results}. 

% Justify the importance of wrist-based solution

%Therefore, stress detection using only wrist-based wearable devices will facilitate the continuous monitoring of stress. Keeping that in mind, a number of works have attempted to compare wrist vs chest stress detection performance to understand feasibility of using wrist-based wearable devices \cite{schmidt2018introducing,samyoun2020stress}. 
% However, as shown in Figure \ref{fig:mot}, one of the key challenges of using wrist-based solution is that they are more prone to movement noise which leads to poor performance compared to the chest-based solution. 
% Justify the importance of edge computing and energy-efficiency

% The type of sensors used for stress detection (e.g., chest-worn or wrist-worn) has important consequences on the energy constraints of real-time wearable health devices. None of the state-of-the-art works evaluate feasibility of their solution for edge (on-device) computing --- an important component of a cloud computing platform with issues such as user data privacy, communication latency, and communication energy \cite{aharrashid2022}. Edge computing solutions should be energy-efficient and capable of running on resource-constrained wearable devices.
% Furthermore, the growing popularity of smart watches supports the conclusion that wrist-worn devices are more user-friendly and widely used than chest-worn devices \cite{samyoun2020stress}. 

% However, theirs along with other works, have limitations such as still needing chest data during training. Moreover, there is a lack of focus on energy efficiency for stress detection \cite{ragav2019scalable,naeini2021amser}

% \textcolor{red}{The flow will be -
% 1. Importance of using wrist based sensors (user friendly and widely adopted)
% 2. Wrist sensors are prone to movements and show less performance
% 3.DCOSS-2020 paper tried to address the issue but they need the chest data for training. Moreover, it is not energy-efficient. (We can show the energy required to transmit 60 S of data to for different communication energy if possible)}
% Sentences about noise from motion.



% \textcolor{blue}{***Motivational example can come here. Paragraph on motiv figure here.}


% Key points the figure should convey: (i) benefits of modeling context through motion - showing the spikes in accelerometer that are correlated with high motion and its affect on decreasing the accuracy on standard methods, (ii) benefits of an adaptive framework - shortcoming of static framework and the ability of the fusion framework to adjust which branches to run depending on the context of the system, (iii) benefits of diverse branches (e.g. temperature is less sensitive to motion that degrades other modalities), (iv) possibly add the benefits of temporal modeling - for our Kalman filter we can model the classification problem as an estimation problem where previous time provides valuable information for the state of the system, (v) performance and energy trade-off for differing fusion methods and need for low-power. 

% Naturally, key research challenges arise from the current methods for stress detection, notably: (i) how to develop an adaptive architecture that alters the fusion schema depending on the current context; (ii) how to utilize measurements from the accelerometers to model the context; (iii) how to achieve comparable results to chest-worn devices with only wrist wearable sensors while being energy-efficient; and (iv) how to incorporate temporal aspects into the stress classification problem that can further improve accuracy.

% \textcolor{blue}{Sentence summarizing the \textbf{open} research challenges here ().}

% \textcolor{red}{Add the research challenges derived from the previous flow of problems which our SELF-CARE addresses}

% \textcolor{blue}{One paragraph here introducing Self-Care. Blend this into the contributions...}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{Figures/Chest_Mot_1.png}
%     \caption{Physiological Signals from Chest Sensors. A baseline segment where EMG affects ECG and RESP even with no motion whereas, ECG remains unaffected even due to motion. This shows that EMG is more suitable than ACC to understand the context from chest wearable devices. Data taken from wrist sensors on one subject from WESAD dataset\cite{schmidt2018introducing}.}
%     \label{fig:mot_chest}
%     % \vspace{-5mm}
% \end{figure}

%This motivates us to extend the \textsc{SELF-CARE} method briefly introduced in our previous work \cite{rashid2022self} and generalize it to both chest and wrist wearable devices. 
% This motivates us to develop a method that is generalizable to both chest and wrist wearable devices. 
% Prior work has shown that stress detection using wrist-based wearable devices can be improved by modeling noise context \cite{rashid2022self}, however, the differences in using chest-based wearable devices have yet to be examined. 

{The aforementioned examples motivate us to develop a context-aware sensor fusion technique that utilizes the noise context of wearable devices to make sensor fusion decisions, which will help us to  maintain performance while avoiding misclassification. Moreover, the developed method should be generalizable to both chest and wrist wearable devices as the noise context varies based on the location of wearable devices.} 
Prior work has shown that stress detection using wrist-based wearable devices can be improved by modeling noise context \cite{rashid2022self}, however, the differences in using chest-based wearable devices have yet to be examined. 

\subsection{{Contributions}}
In this paper, we propose SELF-CARE, a generalized stress detection method that utilizes the noise context of wearable devices to perform sensor fusion.
%Initially, \textsc{SELF-CARE} was applied for stress detection using sensor fusion based on the noise context (motion) of wrist-based wearable devices only. In this paper, we extend SELF-CARE method for chest-based wearable devices.
%and demonstrate that it works for both chest and wrist-based wearable devices. 
We show that while motion-based noise context understanding works best for wrist-based wearable devices, muscle contraction works best for chest-based wearable devices. Through experimental evaluation, we demonstrate that EMG is better than ACC in understanding the noise context of chest-based wearable devices.
%``care'' for the wearable device by conserving energy and prolonging device lifetime.
%energy ``stresses'' of wearable devices.
%for its own internal model structure by limiting unnecessary computations, or ``stresses'', that could otherwise decrease performance.
%for the wearable devices while being energy-efficient and improving the performance at the same.
% SELF-CARE not only improves personal health blah blah stuff but also improves the performance of the learning algorithm. 


\noindent
The key contributions of this paper are as follows:

\begin{enumerate}
% \item \textcolor{red}{To demonstrate that, doing late fusion on the various combination of sensors improves the performance.}
% \textcolor{red}{Propose two new late fusion technique - Confidence, Kalman filtering.}
% \item We extend the SELF-CARE method and generalize it for stress detection using wrist or chest-based wearable devices. 

\item {We introduce a generalized selective sensor fusion method, SELF-CARE, for stress detection from wearable health sensors. SELF-CARE implements a novel context identification method that models noise context based on the location of wearable devices (chest or wrist), and utilizes the noise context to dynamically adjust the sensor fusion performed across an ensemble of machine learning classifiers to improve classification performance.}

% \item We propose a novel late fusion technique for classification over an ensemble of learners using a Kalman filter that incorporates temporal dynamics.

% \item We perform contextual identification for sensor selection through modeling the effects of motion on various sensing modalities from both chest and wrist-worn devices.

%\item To the best of our knowledge, we are the first to empirically demonstrate that noise context varies based on the location of wearable devices. Our findings suggest that, while motion (ACC) is most suitable to understand the noise context in wrist-worn devices, muscle contraction (EMG) is more suitable to determine noise context in chest-worn devices.

\item {We empirically demonstrate that noise context varies based on the location of wearable devices through experimentation across nine different wearable sensors.} Our findings suggest that while motion (ACC) is most suitable to understand the noise context in wrist-worn devices, muscle contraction (EMG) is more suitable to determine noise context in chest-worn devices.

\item {We propose a novel late fusion technique for classification over an ensemble of learners using a Kalman filter that incorporates temporal dynamics.}


% We identify and demonstrate that chest and wrist wearable devices are affected differently due to motion. It requires different signals (EMG or ACC) for better understanding of the context depending on the location of wearable devices (chest or wrist) 
% { allowing for different  Use Context-aware gating based on motion to enable selective late fusion which not only improves the performance rather provides energy efficiency compared to late fusion.}
\item We perform an extensive performance evaluation of the different combinations of sensors from chest and wrist wearable devices for stress detection. This may serve as the benchmark for the research community to understand, evaluate, and compare the impact of sensor fusion in stress detection.  % methods that use only wrist-worn sensing modalities, while achieving competitive, and at times even better, performance than works that use both wrist and chest modalities.
% {Validation on WESAD dataset containing multiple wrist modalities (ACC, BVP, EDA, TEMP) shows that, our SELF-CARE methodology outperforms other works using wrist only modalities. It also provides competitive or even better performance compared works using both chest and wrist modality.}

\item We validate our methodology on the WESAD dataset, showing that SELF-CARE is suitable for wrist-based and chest-based wearable devices and achieves state-of-the-art performance for the 3-class and 2-class stress detection problems.

% \item Experimental evaluation on real hardware shows that our SELF-CARE methodology is feasible for on-device, energy-efficient computing.

\end{enumerate}

% Related works are covered in Section \ref{sec:rw}. The problem of stress classification using context-aware sensor selection is defined in Section \ref{sec:prob}. In Section \ref{sec:meth}, we provide the details of SELF-CARE method, and in Section \ref{sec:results} we provide results and discussion for the experiments conducted. Concluding remarks are given in Section \ref{sec:con}. 


% \subsection{Motivational Example}
% One idea would be to show a segment with a high movement and show how early, late and adaptive fusion performance differs.
\subsection{{Paper Organization}}
{The remainder of this paper is structured as follows. In Section \ref{sec:rw}, we discuss related works in stress and emotion detection and sensor fusion. In Section \ref{sec:prob}, we describe the stress classification problem formulation. In Section \ref{sec:meth}, we introduce the methodology of our context-aware, selective sensor fusion approach. In Section \ref{sec:results} we show the results of our approach on a publicly available stress classification dataset. In Section \ref{sec:disc}, we highlight future directions and limitations, and in Section \ref{sec:con}, we provide concluding remarks.}



% For each describe what they did and how some of their findings support our arguments. Then explicitly say how we are different. \textcolor{red}{Key points this section should illustrate: (i) modeling motion through context can be helpful (ii) benefits and drawbacks of sensor fusion, (iii) the need for using wrist over chest data, (iv) downsides of deep learning, (v) important of energy efficiency.}

% Main papers to cite behind basics:
% \cite{schmidt2018introducing}
% \cite{samyoun2020stress}
% The problem of stress classification using physiological sensors requires models to learn from data as no known physical equations relate stress with the sensors' data. Statistical machine learning methods for classification such as Random Forests, Support Vector Machines, Decision Trees, Adaboost, Linear Discriminant Analysis, K-Nearest Neighbor, have dominated the field [add citation]. 

% Recent advances in technology \cite{EMBC_2020,IoTJ_2020} have enabled the collection of stress and emotion-related physiological signals through various modalities like video, audio, and physiological sensors. Besides the advances in data analysis techniques have enabled the use of various machine or deep learning algorithms to classify or detect those states. Authors in \cite{audio_1, audio_2} used audio and/or visual data to classify different emotional states. However, such modalities are intrusive in nature and raise privacy concerns for the users. Therefore, the use of physiological signals collected through various wearable devices has been gaining momentum in stress and emotion detection.
	
% %Authors in \cite{chest_worn,cStress} 
% Authors in \cite{cStress} use chest-worn device that captures physiological signals from Electrocardiogram (ECG), respiration (RESP) and 3-axis Accelerometer (ACC) to detect stress. Another group in \cite{wrist_worn} used a wrist-worn device recording BVP, Electrodermal Activity (EDA), Skin Temperature (TEMP), and ACC to detect stress. Researchers in \cite{music_stress} used ECG, RESP, EDA, and Electromyogram (EMG) data to detect emotions in response to music. 

% % 	Authors in \cite{chest_worn,cStress,wrist_worn,music_stress} use either chest-worn or wrist-worn device that captures physiological signals from multiple sensors. They use multiple physiological signals like- Electrocardiogram (ECG), Respiration (RESP) and 3-axis Accelerometer (ACC), BVP, Electrodermal Activity (EDA), Skin Temperature (TEMP), and Electromyogram (EMG) data to detect stress or emotional states.

% The datasets used in the above works are collected in-house and are not publicly available. On the other hand, authors in \cite{driver_stress} published a dataset that has ECG, EDA, RESP, and EMG data for drivers' stress detection. Another group in \cite{picard_emotion} published a dataset containing EMG, BVP, EDA, RESP signals for 8 different emotional stimuli from a single subject. Authors in \cite{DEAP} published a dataset for emotion analysis using Electroencephalogram (EEG), facial videos, and physiological signals. The aforementioned works either focused on detecting stress or emotion using wearable devices. Authors in \cite{WESAD} tried to bridge that gap by creating WESAD (Wearable Stress and Affect Detection) dataset that contains stress and emotion data using chest-worn and wrist-worn devices. They also provided a comparative analysis of individual physiological signals from chest and wrist in detecting stress using classical machine learning algorithms - Decision Tree (DT), Random Forest (RF), etc. The same authors also used the wrist-worn device in \cite{CNN_pitfall} to detect stress and emotion in the wild. Researchers in \cite{sirat_DCOSS} used the WESAD dataset to propose a sensor translation mechanism to create chest-based features from the wrist data to detect stress using classical machine learning algorithms.

\section{Related Works}
\label{sec:rw}
% \red{Need to add the rational of why we provide the related works in those two subsections.}
{As this paper presents a context-aware sensor fusion technique for stress detection, we consider the related works from stress detection and sensor fusion. Therefore, we categorize the related works into two parts. In Section \ref{sec:stress_emotion_rw}, we present some related works that consider stress and emotion detection using various sensor modalities. We also discuss the availability of the dataset used in the corresponding works. In Section \ref{sec:sensor_fusion_rw}, we present and compare against the works that mainly focus on sensor fusion techniques for stress detection.}

\subsection{Stress and Emotion Detection}
\label{sec:stress_emotion_rw}
{A number of studies \cite{kim2008emotion,gjoreski2016continuous,hovsepian2015cstress} focus on detecting stress or emotion from physiological signals such as electrocardiograms (ECGs), electromyograms (EMGs), blood volume pulse (BVP), respiration (RESP), electrodermal activity (EDA), and skin temperature (TEMP).} However, these datasets are not publicly available. {Among works with publicly available datasets, authors in \cite{healey2005detecting} detect stress while driving a vehicle, while \cite{picard2001toward} and \cite{koelstra2011deap} perform a more complex analysis on subjects' general emotional states. However, these datasets are limited in that they do not include data on both stress and additional emotions simultaneously.}

{Authors in \cite{schmidt2018introducing} created the WESAD (Wearable Stress and Affect Detection) dataset, which includes data on both stress and amusement states from chest- and wrist-worn devices.} Moreover, the authors compare the classification performance of multiple common machine learning methods using chest-worn sensors, wrist-worn sensors, and their combinations. They conclude that: (i) chest sensors perform better, and wrist sensors become redundant and sometimes even decrease performance, (ii) fusing multiple sensor modalities together can improve results, and (iii) the accelerometer can negatively impact classification performance. The third finding supports our claims that modeling the context as a learned abstraction of motion can be beneficial for wearable devices, and that sometimes fusing all available sensors together reduces performance. {Authors in \cite{samyoun2020stress} use the WESAD dataset to present a translation method using a Generative Adversarial Network (GAN) to generate chest sensor features using the wrist sensors. However, the higher computational complexity of GANs, along with the requirement of chest data during training, limits the application for computing on a wrist-worn device. Authors in \cite{rashid2021feature} propose a hybrid convolutional neural network (CNN) architecture that uses both manually extracted and CNN features for classification, but only uses one sensing modality.} 
%They use only the wrist BVP signal from WESAD dataset and improve performance compared to traditional machine and deep learning models.
 {The authors of \cite{lin2019explainable,fouladgar2021cn} and \cite{huynh2021stressnas} explore the feasibility of deep learning models for stress and emotion detection using the WESAD dataset. However, traditional machine learning models are currently favored over deep learning approaches due to deep learning's increased computational complexity and lack of explainability \cite{schmidt2019multi,bota2020emotion}.} 
%traditional machine learning models are more efficient for edge computing than deep learning approaches. 

%based to low-power, wearable devices that rely on energy efficiency.

% Deep learning papers to cite:
% \cite{lin2019explainable}
% \cite{fouladgar2021cn}
% \cite{schmidt2019multi}
% \cite{huynh2021stressnas}
% \cite{bota2020emotion}

%Furthermore, many of these approaches fail to apply the leave-one-subject-out validation strategy which is [talk about importance here].
%notable issues of small dataset sizes
% Fusion papers to cite:
% \cite{gravina2017multi}
% \cite{lai2021intelligent}
% \cite{oviatt2018handbook}
% \cite{bota2020emotion}
% \textcolor{red}{HydraFusion}

\subsection{Sensor Fusion}
\label{sec:sensor_fusion_rw}
Sensor fusion has many benefits when applied to both physiological signals and stress recognition \cite{gravina2017multi,bota2020emotion}. {By combining raw-sensor data or features (early fusion), more information can be extracted from sensor measurements than would otherwise be available.} Likewise, using an ensemble method of multiple learners (late fusion) can increase robustness to sensor/classifier errors. {Performing late fusion on the outputs of multiple classifiers can improve performance, as each classifier can be specialized for its particular set of input data \cite{oviatt2018handbook}.}
Traditional late fusion approaches typically use a voting method over the outputs of the classifiers to make a final decision. Other works have also proposed a learned late fusion method, such as the method discussed in \cite{yan2022emotion}. The authors propose an adaptive fusion method, detailing the benefits of using event-related feature extraction techniques along with an adaptive framework. {However, their approach does not consider the noise context of data for the sensor fusion decision as we do in our approach. Additionally, we also show that the noise context varies based on the location of wearable devices, which has not been addressed in their work. Furthermore, although their late fusion is adaptive, their method is static in that it requires a set number of classifiers. Our model, on the other hand, can dynamically adjust the number and type of classifiers used based on the performance-computation trade-off.}

% \begin{table}[htb]
%     \centering
%     \begin{tabular}{c c c c c}
%     \hline
%         Modalities & Robust & Efficient & Adaptive & Temporal\\\hline
%         {Single Sensor} & {X} & {\checkmark} & {X} & {X}\\\hline
%         {Wrist} & {X} & {\checkmark} & {X} & {X}\\\hline
%         {Chest} & {\checkmark} & {X} & {X} & {X}\\\hline
%         {Wrist+Chest} & {\checkmark} & {X} & {X} & {X}\\\hline
%         {SELF-CARE} & {\checkmark} & {\checkmark} & {\checkmark} & {\checkmark}\\\hline
%         % {Night} & {\ding{53}} & {\checkmark} & {\checkmark} & {\checkmark}\\\hline
%         \hline \\
%     \end{tabular}
%     \caption{\textcolor{red}{This table will likely be removed}.}
%     \label{tab:mot_tab}
% \end{table}


%Late fusion section:
% Utilizing an ensemble of base classifiers together has the ability to improve performance as each classifier is specifically specialized for its set of input data \cite{oviatt2018handbook}. Bagging, boosting, and stacking are common approaches that attempt to effectively learn over an ensemble of classifiers. However, in our dynamic architecture, various components of our model are not run depending on the context, which limits these ensemble learning approaches. For this reason we implement a number of late fusion methods that are compatible with our methodology.


%use either soft-voting or hard-voting on the classifiers output to make a final decision. 
%This kind of late fusion is static and applies the voting method on the available set of classifiers which is not energy-efficient for wearable devices multiple classifiers need to be run to make a decision. 
%To address this, our SELF-CARE architecture dynamically selects a subset of classifier(s) from the available ones based on the context of input data.
% More sentences here on \cite{oviatt2018handbook}.
% Difference between us and bagging(trains several base classifiers on bootstrap samples of a training dataset and combines the outputs of these base classifiers using simple aggregation such as majority voting), boosting(approaches iteratively learn component classifiers such that each one specializes on specific types of training examples), and stacking(two stage process in which the outputs of a collection of first stage base classifiers are combined by a second stage classifier to produce a final output). Explain how with a varying number of branches activated, learning and other methods are not capable etc.
% Kalman filter Papers to cite:
% \cite{siswantoro2016linear,pakrashi2019kalman}
%Moreover, we also propose  
{Lastly, sensor fusion presents additional benefits when fusing time-series data with temporal correlations, like the data present in physiological sensors.} Kalman filters are tools for estimating unknown quantities by iteratively predicting and updating the estimated state of interest \cite{kalman1960new}, which in our case is the predicted class. {Some works propose using Kalman filters to solve classification problems, \cite{pakrashi2019kalman}, while other works do not consider temporal aspects within their formulation. In this work, we present a novel late fusion method using a Kalman filter to take advantage of the temporal dynamics in the stress classification problem.} %\cite{siswantoro2016linear} proposed the use of a Kalman filter to improve neural network classification but they use their filter to estimate parameters of their model. 

% \textcolor{red}{need to figure out where and how to cite HydraFusion}
% Past DCOSS Papers to cite:
% \cite{samyoun2020stress}

% \subsection{Sensor Fusion in Physiological Signals}

% \subsection{Sensor Fusion in Stress Recognition}
% \label{sec:Sensor_Fusion_in_Stress_Recognition}

% \subsection{Multi Branch Architectures}

\section{Problem Formulation}
\label{sec:prob}
As discussed in Section \ref{sec:rw}, fusing multiple heterogeneous physiological signals has benefits for stress detection. The main sources of these physiological signals are generally either chest- or wrist-worn wearable devices. Between the two, wrist-worn wearable devices are more prone to noise induced by random movements of hands, and, as shown in Fig. \ref{fig:mot_wrist_chest}, movements create varying impacts on different physiological signals. Fusing such noisy signals often deteriorates the classification performance \cite{schmidt2018introducing}. {On the other hand, chest-worn wearable devices are less prone to random movements due to their location, but signals may be affected or become noisy for other reasons, such as muscle contraction.} Therefore, it is important to understand the context of the noise which varies based on the location of the wearable  devices. Understanding the noise context can help to dynamically select the less impacted signals to be fused, which will  eventually improve the classification performance.
% Moreover, this selective fusion approach will also promote energy efficiency, compared to the static (early/late) fusion approaches where \textit{all} the available signals are constantly used to perform a classification task. This approach provides an opportunity for performance-energy tuning depending on the application setting. 
 The problem formulation for stress detection in a selective approach is provided as follows.

%\subsection{Adaptive Sensor Fusion for Stress Recognition}
For each input segment of sensor data, the goal of a classifier $\phi$ is to utilize the measurements from available sensors, $\mathbf{X}$, to classify the segment, $\mathbf{Y}$:
\begin{equation}
    \mathbf{Y} = \phi(\mathbf{X}) = [p_1, p_2 , \dots p_c]
    \label{eq:Y}
\end{equation}    
    \begin{equation}
    \mathbf{X} = \{\mathbf{X}_i\}_{i=1 \dots s}
 \end{equation}   
% \begin{equation}
%     \mathbf{Y} = [p_1, p_2 , \dots p_c]
% \end{equation}
% \{\mathbf{Y}_{class}^i\}_{i=1 \dots d} 
%where $\mathbf{Y}_{class}$ and $\mathbf{Y}_{reg}$ represent the class and location, respectively, of object $i$ in the scene. 
where $s$ is the number of available sensors; ${\mathbf{X}_i}$ represents the measurements from sensor $i$; and ${\mathbf{Y}}$ represents the classifier output which is comprised of the probabilities $p$ of the $c$ classes, (e.g., $c=1$: baseline, $c=2$: stress, $c=3$: amusement).
$\phi$ can be implemented via traditional sensor fusion techniques, a machine learning (ML) or deep learning (DL) model, or an ensemble of ML/DL models. 
% The targets for object $i$ in the sample are defined as follows:
% \begin{equation}
%     \mathbf{Y}_{class}^i \in \{c_1, c_2, c_3, \dots\}, \;   
%     \mathbf{Y}_{reg}^i = [\mu_1 , \nu_1 , \mu_2, \nu_2 ] \in \mathbb{R}^2
% \end{equation}
% where $\mathbf{Y}_{class}^i$ represents the class of the object (e.g., $c_1$: car, $c_2$: truck, $c_3$: pedestrian) from a set of defined object classes, and $\mathbf{Y}_{reg}^i$ represents the 2D bounding box coordinates of the object in reference to the coordinate frame of the sample. We denote the model's estimate of $\mathbf{Y}$ as $\mathbf{\hat{Y}}$.

Since $\mathbf{X}$ represents data from multiple heterogeneous sensing modalities, sensor fusion can be used to fuse the data to provide a better estimate of $\mathbf{Y}$. In early fusion, the raw sensor inputs are fused before being passed through the classifier as follows:
\begin{equation}
    \mathbf{\hat{Y}}  = \phi(\psi(\mathbf{X}_1, \mathbf{X}_2, \dots , \mathbf{X}_s ))
\end{equation}
where $\psi$ represents the function for fusing the different inputs. In contrast, \textit{late fusion}, involves fusing the outputs of an ensemble of sensor-specific classifiers as follows:
\begin{equation}
    \mathbf{\hat{Y}}_1,\, \mathbf{\hat{Y}}_2,\, \dots, \mathbf{\hat{Y}}_s = \phi_1(\mathbf{X}_1),\,  \phi_2(\mathbf{X}_2),\, \dots , \, \phi_s(\mathbf{X}_s)
\end{equation}
% \vspace{-3mm}
\begin{equation}
    \mathbf{\hat{Y}} = \phi(\mathbf{\hat{Y}}_1, \mathbf{\hat{Y}}_2, \dots , \mathbf{\hat{Y}}_s)
\end{equation}
% \vspace{-3mm}

The context of noise can vary dramatically based  on the wearable device location  and may have a range of impacts on different sensor modalities. This variance calls for the use of an adaptive $\phi$ that selects the sensor modalities to be fused based on the noise context---for example, movements of hands in wrist-worn wearable devices or muscle contractions in chest-worn wearable devices. In this case, $\phi$ represents an ensemble of classification models, and $\phi^*$ represents the selected best subset of models in the ensemble for a given input $\mathbf{X}$.
The context of the noise (either learned and modeled from the inputs or provided externally) is denoted as $\Omega$. We introduce the context identification problem formulation as:
% \begin{equation}
%     \Omega = \pi(\mathbf{X}) ,\quad \phi^* = \rho(\Omega) ,\quad \mathbf{Y} = \phi^*(\mathbf{X})
% \end{equation}
\begin{equation}
    \Omega = \pi(\mathbf{X}) ,
\end{equation}
\begin{equation}
     \phi^* = \rho(\Omega) ,
\end{equation}
% \begin{equation}
%     \mathbf{Y} = \phi^*(\mathbf{X}) ,
% \end{equation}
where $\pi$ represents a gating model that performs context identification, and $\rho$ represents the mechanism for selecting $\phi^*$ given the identified context $\Omega$.
%In this paper we attempt to find the optimal models for $\phi^*$ that can accurately predict $Y$ while internally estimating $\Omega$ across different contexts given only sensor measurements, $\mathbf{X}$, as inputs. 
The goal of $\pi$ and $\rho$ is to select the optimal subset of branch models $\phi^*$ for the inferred context $\Omega$ to maximize stress classification performance for a given $\mathbf{X}$. In our specific case, context is defined as motion for wrist-worn wearable devices or as muscle contraction for chest-worn wearable devices. The inputs to $\pi$ typically consist of measurements from the accelerometer  (wrist-worn) or EMG (chest-worn) based on the wearable device location. %We posit that this general problem formulation can be extended to other sensor fusion problems in CPS.  

% \subsection{Context Identification through Motion Modeling}
% \textcolor{blue}{Potential Section here if we have good findings.}

% \textcolor{blue}{
% In the scope of this work, we define context as the relative motion of the subject based on their accelerometer readings. There are various ways to formulate a motion model using measurements from a tri-axis accelerometer: (i) passing raw measurements directly to a learning-based model; (ii) using statistical methods to combine the signal measurements into set \textit{contextual bins}; (iii) or extracting local acceleration measurements from the sensor and developing a physics-based model. In general, accelerometer measurements measure \textit{specific} force, \textit{i.e.}, where the local gravity factor must be accounted for, in the body frame of the sensor as in the following:}
% \begin{equation}
%     a[k] = C^b_g * ( a_b[k] - g[k] ) + b_a[k] + n_a[k]
% \end{equation}
% \textcolor{blue}{
% where explain all the variables here further.}

% \textcolor{blue}{
% There are two challenges that limit exact motion modeling. Firstly, without a gyroscope, we cannot calculate the relative orientation of the sensor to the global frame to account for the gravity. Secondly, for a wrist-watch application, like we consider, this body frame is not fixed relative to any global frame, presenting similar problems for an accurate physics model.} 

% \begin{equation}
%     statistical modeling equation here 
% \end{equation}


% \textcolor{blue}{
% Note: we may also trying using some features from the other sensors in a deep learning context id module as another option if using just the accelerometer does not work.
% }


%\subsection{Joint Performance-Energy Optimization}

\section{Methodology}
\label{sec:meth}
In this Section we detail our method, SELF-CARE, depicted in Fig. \ref{fig:arch}. Our method performs stress classification given input sensor measurements from a specified time segment using four main blocks: (i) preprocessing, (ii) context identification, (iii) branch classifiers, and (iv) late fusion. SELF-CARE takes the form of a multi-branched architecture in which different ``branches'' represent stress detection classifiers using different combinations of sensors. {Context identification selects which branches to execute, while late fusion is used to fuse the stress classification predictions if multiple branches are selected.}
The following subsections provide further details on the proposed method.
%The preprocessing block is explained in details later in Section \ref{SELF-CARE_implementation}. The following subsections provide details of other three blocks.
%The sensing modalities considered include accelerometers (ACC), blood-volume pressure sensors (BVP), electrodermal activity sensors (EDA), and temperature (TEMP) sensors. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/Methodology_3.png}
    % \includegraphics[width=\linewidth]{Figures/archi_3.png}
    \caption{Proposed SELF-CARE Architecture. In this depiction different types of chest/wrist-worn sensors are used, the gating model selects the branches given the context, a Random Forest/AdaBoost classifier is used for the branch models, and a Kalman filter is used for the late fusion over the selected branches.}
    \label{fig:arch}
    % \vspace{-5mm}
\end{figure}

\subsection{Preprocessing Step}
{SELF-CARE can take in data from varying numbers of heterogeneous or homogeneous physiological sensors as inputs.} Preprocessing is a common step when dealing with raw, unfiltered sensor data. By applying various filters (e.g., band-pass filters or lowpass filters) to the input data, random noises are reduced, and important features are more easily extracted. The preprocessing performed over each sensing modality is detailed in Section \ref{sec:results}. %The exact implementation details for the preprocessing we performed over each sensing modality are detailed in Section \ref{sec:results}.

% \subsection{Preprocessing Step}
% \subsubsection{Filtering} The preprocessing step involves raw data processing to filter out typical noises. The ACC data is passed through a Finite Impulse Response (FIR) filter with a length of 64 with a cut-off frequency of 0.4 Hz. Following the work in \cite{rashid2021feature}, the raw BVP signal is filtered by a Butterworth band-pass filter of order 3 with cutoff frequencies ($f_1$=.7 Hz and $f_2$=3.7 Hz) which takes into account the heart rate at rest ($\approx$40 BPM) or high heart rate due to exercise scenarios or tachycardia ($\approx$220 BPM) \cite{salehizadeh2016novel}. The raw EDA signals are filtered using a Butterworth lowpass filter of order 6 with cut-off frequency of 1 Hz. Finally, the raw TEMP signals are smoothed by passing them through a Savitzky-Golay filter (window size=11, order=3).

% \subsubsection{Segmentation}: The filtered signals are segmented by a window of 60 seconds of data with a sliding length of 5 seconds following the work \cite{samyoun2020stress}. It gives us a total of 6458 segments for each signal across all subjects of the WESAD datset.

\subsection{Context Identification}
\subsubsection{Feature Extraction}
% This step performs the feature extraction of a segment from wrist signals. We extract the same wrist features as used in \cite{schmidt2018introducing}. Due to limitation of space we skip to list them in the paper. However, features from all the signals are not calculated at once. 
The purpose of the context identification block is to predict which branch classifier(s) will perform the best given an input set of sensor features that are used to model the context of the system. Contextual modeling can help illuminate the performance of various sensors in terms of their levels of noise under different situations and the locations of the wearable device on the human body. 
%select the branch classifier(s) based on the context of the noise which varies based on the location of wearable devices. 
For wrist wearable devices, we use motion to model the context. Therefore, for wrist devices, we first extract only ACC features as they are directly related to the relative motion of the test subject. {For chest wearable devices, on the other hand, the context is best modeled by muscle contraction, which is captured by EMG signal. We then extract EMG features for chest-worn devices for contextual modeling. Next, these features are processed by the gating model to select the best performing branch.} The feature extraction of the other modalities takes place after the gating model has selected which branch(es) will be used for classification. 
%Once the classifiers to be used are selected after adding delta value to the gating model decision, then rest of the required features for the selected branch(es) are calculated in the early fusion block.
\subsubsection{Gating Model ($\pi$)}
The gating model trains a classifier that uses the ACC/EMG features as inputs to select one of the available branch classifiers according to wrist/chest-worn devices. For wrist-worn device, we shortlist these three branches:$WB_{1}$=\{BVP, EDA, TEMP\}; $WB_{2}$=\{ACC, BVP, EDA\}; $WB_{3}$=\{BVP, EDA\} using Random Forest classifier for both 3-class and 2-class classification. Similarly, for chest-worn devices, we shortlist five branches for 3-class and 2-class classification using AdaBoost classifiers. For 3-class classification, the shortlisted branches are: $CB_{1}$=\{ECG, RESP, EMG, EDA, TEMP\}; $CB_{12}$=\{ECG, EMG, EDA, TEMP\}; $CB_{14}$=\{RESP, EMG, EDA, TEMP\}; $CB_{24}$=\{ECG, EMG, EDA\}; $CB_{27}$=\{EMG, EDA, TEMP\}. For 2-class classification, the shortlisted branches are: $CB_{5}$=\{ACC, ECG, RESP, EDA\}; $CB_{7}$=\{ACC, ECG, EMG, EDA\}; $CB_{9}$=\{ACC, ECG, EDA, TEMP\}; $CB_{13}$=\{ECG, RESP, EDA, TEMP\}; $CB_{20}$=\{ACC, ECG, EDA\}. {The process for choosing these branches is discussed further in Section \ref{SELF-CARE_implementation}. We employ a Decision Tree (DT) classifier for our gating model because it is lightweight and adds minimum overhead to our architecture.}
\subsubsection{Performance-Computation Trade-off ($\delta$)}
\label{sec:delta}
An important feature of SELF-CARE is its ability to balance constraints between performance and computation. We introduce the term $\delta$ that aids the gating decision in considering this trade-off. The gating model outputs prediction probabilities for the available branches with $\bar{b}$ representing the maximum probability branch. $\delta$ has a range between 0 and 1, representing the range in which non-maximum branches are selected by allowing branches with probabilities greater than $\bar{b}-\delta$ to be also selected. Lower $\delta$ values indicate tighter computation constraints, with $\delta=0$ indicating that only the highest probability branch from the gating classifier is selected, while higher $\delta$ values allow more branches to be selected, with $\delta=1$ indicating that all possible branches are selected. 
% For example, the prediction probability of the gating model for a particular segment on the three branches is: [$B_{1}$, $B_{2}$, $B_{3}$] = [0.1, 0.6, 0.3]. By the rule of maximum probability the classifier for $B_{2}$ would be selected to classify that segment. Now for example, if $\delta$=0.4, then any other prediction probability that falls within the $max([0.1, 0.6, 0.3 ])-0.4$ range will also be considered. As 0.3 falls within that range, the classifier for $B_{3}$ also be selected to be used for classification of the segment. Thus, two out of the three available branches will be used for classification of the segment.
\subsubsection{Early Fusion ($\psi$)}
Once the branches are selected after applying $\delta$ on the gating model decision, the features for those branches will be extracted and concatenated together to be passed to the corresponding classifiers. For example, while using wrist modalities, if $WB_1$ and $WB_3$ are the selected branches by the gating model for either 3-class  or 2-class classification, the features from BVP, EDA, and TEMP signals are concatenated together using early fusion for $WB_1$, while features from BVP and EDA are fused for branch $WB_{3}$. {Similarly, for 3-class classification using chest modalities, the features from ECG, RESP, EMG, EDA, and TEMP are fused together if the gating model selects the $CB_1$ branch.}%  \textcolor{red}{One sentence here on how early fusion is done: For example, do we just concatenate features together? Or is there a special function we run the sensor modalities features through to do the early fusion?}
%As shown in the previous example --- the final branches are $B_{2}$=\{ACC, BVP, EDA\} and $B_{3}$=\{BVP, EDA\}. Therefore, we need to additionally extract the features for BVP and EDA, as ACC features were already extracted for the gating model. After that, ACC, BVP and EDA features are fused together for branch $B_{2}$ whereas BVP and EDA are fused for branch $B_{3}$.

\subsection{Branch Classifiers}
Next, the corresponding branch classifier(s) are used to classify the segment. For our approach, we use a Random Forest (RF) classifier for all three branches of wrist modalities for 3-class and 2-class classification. For chest modalities, we use the AdaBoost classifier for all five branches for 3-class and 2-class classification. The details of the classifier training and selection are provided below in Section \ref{SELF-CARE_implementation}. {Currently, SELF-CARE operates using either only wrist sensors or only chest sensors, however, our method is capable of integrating both sets of branches with modifications to the context identification module.
Each selected branch produces a classification prediction to serve as input for the late fusion method.}

%without loss of generality, more branches could be added or considered, whether it be from compositions of different sensors or using different classification models

\subsection{Late Fusion Method}
\label{sec:lfb}
%General info paragraph about what the block does:
The late fusion method is tasked with fusing the class predictions from the various selected branches, $\{\mathbf{\hat{Y}}_1, \mathbf{\hat{Y}}_2, \dots , \mathbf{\hat{Y}}_s\}$, with the goal of producing higher accuracy predictions than any one individual branch by itself. 
{Here we present our \textit{Kalman filter-based} method for classification over an ensemble of classifiers.}
%, although we claim that any applicable late fusion method is supported within SELF-CARE.
%Late fusion generally achieves the most productive results when the branches fused are independent from each other, \textit{i.e.}, large ensemble methods benefit from diversity in results as opposed to highly correlated branches.   
%We implement and evaluate the performance of the following late fusion methods: (i) \textit{hard-voting}, (ii) \textit{soft-voting}, (iii) \textit{confidence-based}, and (iv) \textit{Kalman filter-based}. 

% Voting refers to the case of late fusion where the predictions from each branch are combined, to form the final prediction in a statistical manner. Recall from Eq. \ref{eq:Y} that each branch outputs a confidence value per class such that $\sum_{j=1}^c{p_j}=1$. The method of \textit{hard-voting} refers to assigning the class based on which class was the most commonly voted by each classifier. The fused output from \textit{hard-voting}, $\mathbf{\hat{Y}}_{hv}$, is illustrated with the following equations:
% \begin{equation}
%     \mathbf{\hat{Y}}_{hv} = \text{mode}{[y_1, y_2, \dots , y_s]}
% \end{equation}
% \begin{equation}
%     y_j =  \operatorname*{arg\,max}_c \mathbf{\hat{Y}}_j ,\quad j=1 \dots s
% \end{equation}
% \begin{equation}
%     \mathbf{\hat{Y}}_j = [p_1,p_2,\dots,p_c]
% \end{equation}
% where mode represents the most common class picked by the $s$ classifiers for each $y_j \in \{0,1,...,c\}$. Another variation of voting, \textit{soft-voting}, selects the class with the highest average value across all the classifiers. The fused output from \textit{soft-voting}, $\mathbf{\hat{Y}}_{sv}$, is described by:
% \begin{equation}
%     \mathbf{\hat{Y}}_{sv} = \operatorname*{arg\,max}_c [a_1 , a_2 , \dots a_c] 
% \end{equation}
% \begin{equation}
%     a_c = \frac{1}{s}\sum_{j=1}^s{\mathbf{\hat{Y}}_j}[c] %,\quad k=1 \dots c
% \end{equation}
% In the \textit{confidence-based} voting method, we propose assigning the fused class based on the single highest confidence class by any branch. We define $\mathbf{\hat{Y}}_{co}$ as:
% \begin{equation}
%     \mathbf{\hat{Y}}_{co} = \operatorname*{arg\,max}_c ({\mathbf{\bar{Y}}})
% \end{equation}
% \begin{equation}
%     \mathbf{\bar{Y}} =[\mathbf{\hat{Y}}_1 , \mathbf{\hat{Y}}_2 , \dots, \mathbf{\hat{Y}}_s ]^\top
% \end{equation}
%Lastly, we present our novel \textit{Kalman filter-based} method for classification over an ensemble of classifiers.
Kalman filters are powerful and commonly used tools for sensor fusion and the broader field of estimation. They are designed to estimate the unknown state of a system along with the state's uncertainty by performing a series of recursive predictions and measurement updates. In the context of our problem, we consider a Kalman filter approach towards the multi-class classification problem like in \cite{pakrashi2019kalman}, and we additionally model the temporal dynamics in the stress classification problem for each sample at time $k$. The general form of the discretized linear dynamics of a system with state $\mathbf{x}$ and measurements $\mathbf{z}$ is given as:
\begin{equation}
\mathbf{x}(k) = \mathbf{F}x(k-1) + \mathbf{v}(k)
\end{equation}
\begin{equation}
\mathbf{z}(k) = \mathbf{H}x(k) + \mathbf{w}(k) 
\end{equation}
where $\mathbf{F}$ is the state transition matrix; $\mathbf{v}$ is the process noise vector, which is modeled as zero-mean, normally distributed random variable with covariance, $\mathbf{Q}$; $\mathbf{H}$ is the measurement matrix relating the state to the measurements; and $\mathbf{w}$ is the measurement noise vector, which also is zero-mean with a normal distribution and covariance, $\mathbf{R}$.

During the prediction step of the Kalman filter, the state estimate and its estimation error covariance matrix, $\mathbf{P}(k)$, are propagated forward through the dynamics model with the added process noise. This step enforces the temporal dependency that the stress class probabilities at the current time step have on the future time step. The prediction equations are:
% \begin{equation}
% \mathbf{x}(k+1|k) = \mathbf{F}\,\mathbf{x}(k|k) , 
% \end{equation}
% \begin{equation}
% \mathbf{P}(k+1|k) = \mathbf{F}\,\mathbf{P}(k|k)\,\mathbf{F}^\top + \mathbf{Q}(k) 
% \end{equation}
\begin{equation}
\mathbf{x}(k|k-1) = \mathbf{F}\,\mathbf{x}(k-1|k-1) , 
\end{equation}
\begin{equation}
\mathbf{P}(k|k-1) = \mathbf{F}\,\mathbf{P}(k-1|k-1)\,\mathbf{F}^\top + \mathbf{Q}(k-1) 
\end{equation}
where the notation $(k+1|k)$ indicates the next time step given the current time step. Next, during the update step, measurements are processed and updated estimates of the states and their covariance are corrected according to the measurements. The measurement update equations are as follows:
% \begin{multline}
% \mathbf{x}(k+1|k+1) = \mathbf{x}(k+1|k) \, - \\ \mathbf{K}(k+1)
% \,[\mathbf{H}\,(\mathbf{x}(k+1|k) - \mathbf{z}(k+1)] 
% \end{multline}
% \begin{multline}
% \mathbf{P}(k+1|k+1) = \mathbf{P}(k+1|k) - \mathbf{K}(k+1)\, \mathbf{H}\, \mathbf{P}(k+1|k)  
% \end{multline}
% \begin{multline}
% \mathbf{K}(k+1) = \mathbf{P}(k+1|k) \, \mathbf{H}^\top \, [\mathbf{H}\,\mathbf{P}(k+1|k)\, \mathbf{H}^\top \\+ \mathbf{R}(k+1)]^{-1} 
% \end{multline}
\begin{equation}
\mathbf{x}(k|k) = \mathbf{x}(k|k-1) -  \mathbf{K}(k)
[\mathbf{H}(\mathbf{x}(k|k-1) - \mathbf{z}(k)] 
\end{equation}
\begin{equation}
\mathbf{P}(k|k) = \mathbf{P}(k|k-1) - \mathbf{K}(k) \mathbf{H} \mathbf{P}(k|k-1)  
\end{equation}
\begin{equation}
\mathbf{K}(k) = \mathbf{P}(k|k-1)  \mathbf{H}^\top  [\mathbf{H}\mathbf{P}(k|k-1) \mathbf{H}^\top + \mathbf{R}(k)]^{-1} 
\end{equation}
with $\mathbf{K}$ representing the Kalman gain. The prediction and update step are iterated to produce an estimate of the state, $\mathbf{x}$, and its associated estimation error covariance, $\mathbf{P}$, representing the uncertainty involved with the state estimate.

For our case, we abstract the multi-class classification problem as follows. The unknown state our filter is attempting to estimate is the probability of each class during each segment. Thus, $\mathbf{x}$ is a $c$ dimensional vector of estimated class probabilities. Additionally, the predictions from each separate classifier are the measurements $\mathbf{z}$, which are processed sequentially per time step. This allows for $s^*$ measurement updates per iteration where $s^*$ is adaptively selected per sample by the gating model. We additionally provide some measurement thresholding during the filter updates that are detailed in Section \ref{sec:kf_tune}. Finally, we arrive at our late fusion output using the Kalman filter-based method:
\begin{equation}
    \mathbf{\hat{Y}}_{kf} = \operatorname*{arg\,max}_c \mathbf{x} 
\end{equation}
where $\mathbf{x}$ is the state vector from the Kalman filter. 
%In our previous work  \cite{rashid2022self}, we validated that our Kalman filter-based method outperformed commonly used voting mechanisms for late fusion.
To validate our Kalman-filter based method, we benchmark its performance against commonly used voting mechanisms for late fusion: \textit{hard-voting} and \textit{soft-voting} \cite{oviatt2018handbook}. The method of hard-voting assigns the final class based on the class most commonly voted by each classifier, whereas soft-voting selects the class with the highest average value across all the classifiers. Our results comparing different late fusion approaches are presented in Figures \ref{fig:overall_performance_wrist_3_class}, \ref{fig:overall_performance_wrist_2_class}, \ref{fig:overall_performance_chest_3_class}, and  \ref{fig:overall_performance_chest_2_class} of Section \ref{sec:results}.

%Our results in the following section compare the performance between the four proposed selective late fusion approaches ($\mathbf{\hat{Y}}_{hv}$, $\mathbf{\hat{Y}}_{sv}$, $\mathbf{\hat{Y}}_{co}$, $\mathbf{\hat{Y}}_{kf}$) within SELF-CARE.% framework. 


% \textbf{Learned Fusion}, on the other hand, uses \textit{learning} to form the final predictions. This further abstracts the model's decision on the class by implementing a model that is not trained on. Detail the challenges in training something like this here. A general representation of this method is as follows:
% \begin{equation}
%     y = f(g_1(x|\theta_1),g_2(x|\theta_2),...g_n(x|\theta_n) | \theta_f)
% \end{equation}
% where .... describe variables here. We implement X types of learned fusion blocks, add more details on which we implement.
% Note: we could not do this with an adaptive framework.

%Sentence on basics of KF: goal, theory background, how it is derived. Another sentence on how it arrives at the minimal estimate etc. Details on what we consider the state and what we consider as measurements. Sentence explaining prediction dynamics model. Sentence explaining sequential measurement updates. Explain in words the identity matrices used to avoid drawing attention to simplicity of the model

% Comparison: To compare our Kalman filter late fusion approach, we implement three separate baseline late fusion ensemble methods that are compatible with our framework. Soft voting, hard voting, confidence-based voting.

%Different options to cover:

\section{Experimental Analysis}
\label{sec:results}
This section presents the experimental findings of SELF-CARE on a wearable health device stress detection dataset. First, we describe the dataset used for evaluation. Second, we explain the training and implementation of our models. Third, we describe our evaluation metrics and analyze experimental results.
%the details of the dataset utilized are presented, followed by the training and implementation details for our models. Next, evaluation metrics are detailed and analysis of experimental results is conducted.} 

\subsection{Dataset}
SELF-CARE is validated on the publicly available WESAD dataset \cite{schmidt2018introducing}. This dataset was selected because it contains data from both wrist- and chest-worn wearable devices, which makes it an ideal dataset for understanding the noise context devices worn on different parts of the body. The dataset contains data for a total of 15 subjects, from both chest- (RespiBAN) and wrist- (Empatica E4) worn sensors. The chest sensors used in RespiBAN  are ACC, ECG, RESP, EMG, EDA, TEMP. The wrist sensors from the Empatica E4 are ACC BVP, EDA, TEMP. %accelerometer (ACC), blood volume  pressure (BVP),  electrodermal  activity (EDA), temperature (TEMP). 
The dataset has three types of classes related to emotional states: (i) baseline (neutral), (ii) amusement, and (iii) stress. For the 2-class problem, baseline and amusement are grouped together in the non-stress class.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{Figures/Training_Test_Self-care_3.png}
%     \caption{SELF-CARE Training Procedure - \textcolor{red}{Option A.}}
%     \label{fig:self-care_training}
%     % \vspace{-5mm}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/training-diagram.png}
    \caption{SELF-CARE training and implementation procedure. We follow the steps sequentially as numbered in the figure. To train the gating model, we first generate the gating labels from step 5 and then use extracted features from step 2 to train it.}
    \label{fig:self-care_training}
    % \vspace{-5mm}
\end{figure}

\subsection{SELF-CARE Training and Implementation}
\label{SELF-CARE_implementation}
%\textcolor{red}{Specific Implementation details for each block here without being repetitive of the previous section.}
This section describes the training and implementation details for the SELF-CARE architecture, shown in Fig. \ref{fig:self-care_training}. 

\subsubsection{Preprocessing Step} The preprocessing step involves raw data processing to filter out typical noises.

\textbf{Wrist Modalities}: The ACC data is passed through a Finite Impulse Response (FIR) filter with a length of 64 with a cut-off frequency of 0.4 Hz. Following the work in \cite{rashid2021feature}, the raw BVP signal is filtered by a Butterworth band-pass filter of order 3 with cutoff frequencies ($f_1$=0.7 Hz and $f_2$=3.7 Hz), which takes into account the heart rate at rest ($\approx$40 BPM) or high heart rate due to exercise scenarios or tachycardia ($\approx$220 BPM) \cite{salehizadeh2016novel}. The raw EDA signals are filtered using a Butterworth lowpass filter of order 6 with cut-off frequency of 1 Hz. Finally, we use a Savitzky-Golay filter (window size=11, order=3) to smooth the raw TEMP signals.

\textbf{Chest Modalities}: Because the chest data is collected at a very high sampling rate (700 Hz), the signals are first smoothed using  a Savitzky-Golay filter. The ACC data is smoothed using a window size of 31 with an order of 5. The other signals (ECG, EMG, EDA, RESP, and  TEMP) are smoothed using a window size of 11 and an order of 3. Similar to wrist BVP,  the ECG signal is further filtered by a Butterworth band-pass filter of order 3 with cutoff frequencies ($f_1$=0.7 Hz and $f_2$=3.7 Hz) that takes into account the heart rate at rest ($\approx$40 BPM) or high heart rate due to exercise scenarios or tachycardia ($\approx$220 BPM) \cite{salehizadeh2016novel}. The EDA signals are filtered using a Butterworth lowpass filter of order 2 with a cutoff frequency of 5 Hz. To extract some of the peak features (number of peaks, peak amplitude), the EMG signal is passed through a Butterworth lowpass filter of order 3 and a cutoff frequency of 0.5 Hz. We extract other EMG features from the smoothed EMG signal. The RESP signal is filtered by a Butterworth bandpass filter  of order 3 with cutoff frequencies $f_1$=0.1 Hz and $f_2$=0.35 Hz.

The filtered signals from both the wrist and chest  are segmented by a window of 60 seconds of data with a sliding length of 5 seconds following \cite{samyoun2020stress}. This process produces a total of 6458 segments for each signal across all subjects of the WESAD dataset.


  	\begin{table}[t]
   	\centering
   	\begin{threeparttable}
   		\caption{List of Extracted Features}
   		\label{tab:extracted_features}
   		\begin{tabular}{|c|c|}
   				\hline
   				\textbf{Feature Symbol} & \textbf{Feature Names}  \\
   				\hline\hline
       \multicolumn{2}{|c|}{\textbf{ACC Features}}\\
                \hline
   				$\mu_{ACC,i}$, $\sigma_{ACC,i}$ & Mean and STD of each\\
       $i\in\{x, y, z, 3D\}$ & axis and summed over  all axes\\
   				\hline
       $|\int_{ACC,i}|,~i\in\{x, y, z, 3D\}$ & Absolute integral for each/all axes\\
   				\hline
   				$f^{peak}_{ACC,j},~j\in\{x, y, z\}$ & Peak  frequency of each axis\\
   			\hline\hline
                \multicolumn{2}{|c|}{\textbf{ECG/BVP Features}}\\
                \hline
   				$\mu_{HR}$, $\sigma_{HR}$ & Mean and STD of HR \\
   				\hline
   				$\mu_{HRV}$, $\sigma_{HRV}$ & Mean and STD of HRV \\
   				\hline
   				\multirow{2}{*}{$NN50$, $pNN50$} & Number and percentage of HRV \\
   				&intervals differing more than 50 ms  \\
   				\hline
   				$rms_{HRV}$ & Root mean square of the HRV\\
   				\hline
   				$f^{x}_{HRV}$& Energy in ultra-low,  low, high,\\
   				$x\in{ULF, LF, HF, UHF}$& ultra-high frequency band of the HRV\\
   				\hline
   				$f^{LF/HF}_{HRV}$ & Ratio of LF and HF component\\
   				\hline
   				$\sum_{x}^{f}$ & $\sum$ of the frequency components\\
   				$x\in{ULF, LF, HF, UHF}$& in ULF-HF\\
   				\hline
   				$rel_{x}^{f}$ & Relative power of freq. components\\
   				\hline
   				$LF_{norm}$, $HF_{norm}$ & Normalised LF and HF
component \\
\hline\hline
       \multicolumn{2}{|c|}{\textbf{EMG Features}}\\
                \hline
   				$\mu_{EDA}$, $\sigma_{EDA}$ & Mean and STD of EMG\\
       
   				 \hline
       $range_{EDA}$ & Dynamic range  of EMG\\
       \hline
       $|\int_{EMG}|$ & Absolute integral\\
       \hline
       $\Bar{\pi}_{EMG}$ & Median of EMG\\
       \hline
       $P_{EMG}^{10},~P_{EMG}^{90}$ & $10^{th}$ and $90^{th}$ percentile\\
       \hline
       $\mu_{EMG}^{f}$, $\Bar{f}_{EMG}$, $f_{EMG}^{peak}$  & Mean, median, and peak frequency\\
       \hline
       PSD($f_{EMG}$)  & Energy in seven bands\\
       \hline
       $\#^{peaks}_{EMG}$ & Number of peaks\\
       \hline
       $\mu_{EMG}^{amp},~\sigma_{EMG}^{amp}$  & Mean and STD of peak amplitude \\
       \hline
       $\sum_{EMG}^{amp},~\Bar{\sum}_{EMG}^{amp}$  & $\sum$ and norm. $\sum$ of peak amplitude \\
   				\hline\hline
       \multicolumn{2}{|c|}{\textbf{EDA Features}}\\
                \hline
   				$\mu_{EDA}$, $\sigma_{EDA}$ & Mean and STD of EDA\\
       \hline
       $min_{EDA},~max_{EDA}$ & Min and max  value of EDA\\
   				 \hline
       $\delta_{EDA},~range_{EDA}$ & Slope and dynamic range  of EDA\\
       \hline
       $\mu_{SCL}$, $\sigma_{SCL}$, $\sigma_{SCR}$ & Mean and STD of SCL/SCR\\
       \hline
       $Corr_{SCL,t}$ & Correlation between SCL and time\\
       \hline
       $\#~SCR$ & Number of SCR segments\\
       \hline
       $\sum_{SCR}^{amp},~\sum_{SCR}^{t}$  & $\sum$ of SCR magnitudes and duration \\
       \hline
       $\int_{SCR}$  & Area under SCR segments \\
       \hline\hline
       \multicolumn{2}{|c|}{\textbf{RESP Features}}\\
                \hline
   				$\mu_{x}$, $\sigma_{x}$ & Mean and STD of inhalation (I)\\
       $x\in{I,E}$ & exhalation (E) duration\\
       \hline
       I/E & Inhalation/exhalation ratio\\
       \hline
       $vol_{insp},~range_{RESP}$ & Volume and range  of RESP\\
       \hline
       $rate_{RESP},~ \sum_{RESP}$ & Respiration rate and duration\\
       \hline\hline
       \multicolumn{2}{|c|}{\textbf{TEMP Features}}\\
                \hline
   				$\mu_{TEMP}$, $\sigma_{TEMP}$ & Mean and STD of TEMP\\
       \hline
    
       $min_{TEMP},~max_{TEMP}$ & Min and max  of TEMP\\
       \hline
       $\delta_{TEMP},~range_{TEMP}$ & Slope and dynamic range  of TEMP\\
       \hline
    
   			\end{tabular}
  			\begin{tablenotes}
			\item \textit{Standard Deviation (STD), Skin Conductance Response (SCR), Skin Conductance Level  (SCL), Heart Rate (HR), Heart Rate Variability (HRV)}
			\end{tablenotes}
   		\end{threeparttable}
   	% \vspace{-5mm}
   	\end{table}


\subsubsection{Feature Extraction}
%This step performs the feature extraction of a segment from wrist signals. 
We extract the same wrist and chest sensor features as used in \cite{schmidt2018introducing}, some of which include mean/standard deviations, correlations, slope, and dynamic ranges, peak and power frequencies, and absolute integrals.
%mean/standard deviation, absolute integral, peak accuracy for ACC; ... for BVP; ... for EDA; and mean/standard deviation, min/max, dynamic range, and slope for TEMP. 
We note that this feature extraction is only performed across the sensors that are selected to run by the gate for a given input sample.  Table \ref{tab:extracted_features}  contains the list  of extracted features. We refer readers to \cite{schmidt2018introducing} for further details of extracted features per sensor. % Due to limitation of space we skip to list them in the paper. However, features from all the signals are not calculated at once. 

\subsubsection{Train Branch Classifiers}
To train the individual branch classifiers within SELF-CARE, we train using different combinations of input sensor data.

For Wrist Modalities, we use five different early fusion combinations of wrist sensors as input branches: $WB_{1}$=\{BVP, EDA, TEMP\}; $WB_{2}$=\{ACC, BVP, EDA\}; $WB_{3}$=\{BVP, EDA\}; $WB_{4}$=\{ACC, BVP\}; $WB_{5}$=\{ACC, EDA\} as shown in  Tables \ref{tab:wrist_3_class_analysis} and \ref{tab:wrist_2_class_analysis}. For chest modalities, we tried forty-two different combinations of chest sensors as input branches as shown in Tables \ref{tab:chest_3_class_analysis} and \ref{tab:chest_2_class_analysis}.

We evaluate each branch on five different machine learning classifiers---Decision Tree (DT), Random Forest (RF), AdaBoost (AB), Linear Discriminant Analysis (LDA), K-Nearest Neighbor (KNN). We selected these classifiers to ensure a fair comparison with the original WESAD work \cite{schmidt2018introducing}. %Additionally, the low complexity of the classifiers makes SELF-CARE suitable for wearable devices. 
Following the work in \cite{schmidt2018introducing}, we use the same configurations for the classifiers. We use DT as the base estimator for the RF and AB ensemble classifiers, and use 100 base estimators for both RF and AB. In order to measure the splitting quality of the decision nodes, we used information gain and set the minimum number of samples to split a node to 20. For KNN, the K value is set to 9. All classifiers are trained using leave-one-subject-out (LOSO) validation.% is used to train all the classifiers.

\subsubsection{Select Branch Classifiers}
\label{sec:branch_classifier_selection}
We select the branches with the least amount of training loss to be used. The training loss is calculated from the classification confidence of the trained classifiers on the training samples using the categorical cross-entropy, $CE = - \sum_{i}^{n_c} y_{i} \log \hat{y}_{i}$, where $y$ is the one hot encoded true label of a sample, $\hat{y}$ is the corresponding classification output for that sample, and ${n_c}$ is the number of classes. $CE$ is then calculated for all the training samples across all rounds of LOSO validation.
% and the classifier with minimum training loss is selected.

Next, out of the 25 (5 branches x 5 classifiers per branch) possible branch classifiers for wrist modalities, RF classifiers for input branches $WB_{1}$, $WB_{2}$, and $WB_{3}$ are selected as the branch classifiers for both 3-class and 2-class classification. Similarly, for chest modalities, out of 210 (42 branches x 5 classifiers per branch) possible branches, AB classifiers for input branches $CB_{1}$, $CB_{12}$, $CB_{14}$, $CB_{24}$, and $CB_{27}$ are selected for 3-class classification. And for 2-class classification, we select AB classifiers for input branches $CB_{5}$, $CB_{7}$, $CB_{9}$, $CB_{13}$, and $CB_{20}$ for use within our SELF-CARE methodology. These classifier selections are informed by the extensive experiments we performed across the classifiers variations, which we benchmark in Tables \ref{tab:wrist_3_class_analysis}, \ref{tab:wrist_2_class_analysis},  \ref{tab:chest_3_class_analysis}, and \ref{tab:chest_2_class_analysis}.
%The fact that they show minimum training loss, is justified by their performance (shown later in Tables \ref{tab:wrist_3_class_analysis}, \ref{tab:wrist_2_class_analysis},  \ref{tab:chest_3_class_analysis}, \ref{tab:chest_2_class_analysis}).
%The reason for selecting these three branches is that it covers all the wrist sensors. The performance analysis of all the classifiers for all input branches is shown later in Table \ref{2_class_analysis}.


\subsubsection{Generate Gating Labels}
\label{sec:generate_gating_label} 
% In our SELF-CARE framework, the gating model will choose from the three final branch classifiers to be used for final classification of a sample.
%The gating model chooses which combination of the three branch classifiers to be chosen given each sample.
% We want the gating model to select the branch classifier that has the minimum loss for that particular training sample. 
The objective of the gating model is to predict one or a subset of branch classifiers from the classifiers listed in Section \ref{sec:branch_classifier_selection} to be used in our  SELF-CARE methodology.
For each of the training samples, we generate gating labels representing the branch that has the least amount of training loss. These gating labels will be used to train the gating model. For each round of LOSO validation, gating labels are generated based only on the training data, and no test data is used to ensure the validity of our approach.% of our gating model training.


\begin{table*}[!ht]
	\centering
	\caption{Early Fusion Performance of Wrist Modalities in WESAD Dataset for 3-Class (Baseline vs. Stress vs. Amusement)}
	\label{tab:wrist_3_class_analysis}
	\begin{tabular}{|c||c|c||c|c||c|c||c|c||c|c|}
		\hline
		\multirow{2}{*}{\textbf{Modality Used}} &
		\multicolumn{2}{c||}{\textbf{DT}} &\multicolumn{2}{c||}{\cellcolor{gray!20}\textbf{RF}} & \multicolumn{2}{c||}{\textbf{AB}}  & \multicolumn{2}{c||}{\textbf{LDA}}  & \multicolumn{2}{c|}{\textbf{KNN}} \\
		\cline{2-3}\cline{4-5}\cline{6-7}\cline{8-9}\cline{10-11}
		& \textbf{Mac. F1} & \textbf{Acc.} & \textbf{Mac. F1} & \textbf{Acc.} & \textbf{Mac. F1} & \textbf{Acc.}& \textbf{Mac. F1} & \textbf{Acc.} & \textbf{Mac. F1} & \textbf{Acc.}\\
		\hline\hline
% 		\multicolumn{11}{|c|}{\cellcolor{blue!10}{\textbf{WESAD \cite{schmidt2018introducing}}}}\\
% 		\hline
% 		\textbf{All (Wrist+Chest)} & 58.05 & 63.56 & 64.08 & 74.97 &	68.85 &	79.57 & 71.56 &	75.8 &	48.7 &	56.14\\
% 		\hline
% 		\textbf{All (Wrist+Chest) W/O ACC} & 55.71 & 62.57 & 64.23 & 73.33 & 71.1 & 79.86 &	72.48 &	78.19 &	52.94 &	59.61\\
% 		\hline
% 		\textbf{All Chest} & 53.06 & 57.68 & 60.8 & 68.76 & 64.89 & 74.74 & 72.49 & 76.5 & 38.39 & 46.18\\
% 		\hline
% 		\textbf{All Chest W/O ACC} & 55.1 & 58.62 & 64.6 & 71.37 & 72.51 & 80.34 & 74.43 & 79.35 & 51.09	& 57.31\\
% 		\hline
% 		\textbf{All Wrist \cite{schmidt2018introducing}} & 43.62 & 53.98 & 62.86 & 74.85 & 64.12 & \textbf{75.21} & 63.24 & 70.74 & 37.2 & 45.54\\
% 		\hline
% 		\cellcolor{gray!20} {\textbf{[B1] BVP, EDA, TEMP (Wrist)}} & 57.13 & 63.34 & 66.33 & \textbf{76.17} &	64.24 &	73.62 &	58.18 &	68.85 &	50.85 &	58.54\\

        \cellcolor{gray!20} {\textbf{$WB_{1}$=\{BVP, EDA, TEMP\}}} & 56.23 & 62.32 & 	\cellcolor{gray!20}\textbf{62.73} & \cellcolor{gray!20}\textbf{76.62} & 63.78 & 75.78	  &	52.62 &	61.79 &	58.3 & 69.04\\


% 		\hline\hline
% 		\multicolumn{11}{|c|}{\cellcolor{blue!10}\textbf{Additional Analysis [Ours]}}\\
		\hline
		\cellcolor{gray!20}{\textbf{$WB_{2}$=\{ACC, BVP, EDA\}}} & 58.46 &	48.27 &	\cellcolor{gray!20}\textbf{62.88} &	\cellcolor{gray!20}\textbf{77.71} &	62.39 &	76.63 &	60.23 &	69.63 &	58.9 &	68.55\\
		\hline
		\cellcolor{gray!20}{\textbf{$WB_{3}$=\{BVP, EDA\}}} & 55.14 &	59.02 &	\cellcolor{gray!20}\textbf{61.02} &	\cellcolor{gray!20}\textbf{73.96} &	60.67 &	72.54 &	56.55 &	69.8 &	65.73 &	53.44\\
		\hline
		\textbf{$WB_{4}$=\{ACC, BVP\}} & 51.54 & 60.66 & 56.86 & 71.38 & 57.83 & 71.96 &	58.67 &	68.36 &	55.51 &	67.05\\
		\hline
		\textbf{$WB_{5}$=\{ACC, EDA\}} & 47.98 & 54.5 & 52.97 & 70.15 & 56.47 &	71.31 & 57.71 &	68.6 &	58.75 &	64.87\\
		\hline
\end{tabular}
	% \vspace{-2ex}
\end{table*}

\begin{table*}[t]
	\centering
	\caption{Early Fusion Performance of Wrist Modalities in WESAD Dataset for 2-Class (Stress vs. Non-stress)}
	\label{tab:wrist_2_class_analysis}
	\begin{tabular}{|c||c|c||c|c||c|c||c|c||c|c|}
		\hline
		\multirow{2}{*}{\textbf{Modality Used}} &
		\multicolumn{2}{c||}{\textbf{DT}} &\multicolumn{2}{c||}{\cellcolor{gray!20}\textbf{RF}} & \multicolumn{2}{c||}{\textbf{AB}}  & \multicolumn{2}{c||}{\textbf{LDA}}  & \multicolumn{2}{c|}{\textbf{KNN}} \\
		\cline{2-3}\cline{4-5}\cline{6-7}\cline{8-9}\cline{10-11}
		& \textbf{Mac. F1} & \textbf{Acc.} & \textbf{Mac. F1} & \textbf{Acc.} & \textbf{Mac. F1} & \textbf{Acc.}& \textbf{Mac. F1} & \textbf{Acc.} & \textbf{Mac. F1} & \textbf{Acc.}\\
		\hline\hline
% 		\multicolumn{11}{|c|}{\cellcolor{blue!10}{\textbf{WESAD \cite{schmidt2018introducing}}}}\\
% 		\hline
% 		\textbf{All (Wrist+Chest)} & 80.83 & 83.60 & 85.71 & 87.74 & 83.88 & 87.00 & 90.74 & 92.28 &	69.14 &	74.2\\
% 		\hline
% 		\textbf{All (Wrist+Chest) W/O ACC} & 83.03 & 85.16 & 86.02 & 87.91 & 87.78 & 89.77 &	90.93 &	92.51 &	79.44 &	83.16\\
% 		\hline
% 		\textbf{All Chest} & 78.26	& 81.29	& 90.04	 & 91.7	& 89.57	& 91.58 & 91.07	& 92.83	& 64.2 & 69.7\\
% 		\hline
% 		\textbf{All Chest W/O ACC} & 81.29	& 84.18	& 90.44	& 92.01	& 87.11	& 89.76	& 91.47	& 93.12 &	77.27 &	81.05\\
% 		\hline
% 		\textbf{All Wrist\cite{schmidt2018introducing}} & 78.71	& 82.19	& 84.11	& \textbf{87.12} & 80.11	& 83.98	& 84.05	& 86.88 & 52.72 &	63.8\\
% 		\hline
% 		\cellcolor{gray!20} {\textbf{[B1] BVP, EDA, TEMP (Wrist)}} & 82.37 & 84.88 & 86.1 & \textbf{88.33} &	85.86 &	88.05 &	83.77 &	86.46 &	78.93 &	81.96\\
		
		\cellcolor{gray!20} {\textbf{$WB_{1}$=\{BVP, EDA, TEMP\}}} & 74.1	& 84.27 &	\cellcolor{gray!20}\textbf{84.66} &	\cellcolor{gray!20}\textbf{89.01} &	85.29 &	88.96 &	71.46 &	77.32 &	83.74 &	86.56\\
% 		\hline\hline
% 		\multicolumn{11}{|c|}{\cellcolor{blue!10}\textbf{Additional Analysis [Ours]}}\\
		\hline
		\cellcolor{gray!20}{\textbf{$WB_{2}$=\{ACC, BVP, EDA\}}} & 69.44 & 77.06 & \cellcolor{gray!20}\textbf{85.08} & \cellcolor{gray!20}\textbf{88.76} & 85.44 & 88.45	& 85.66	& 87.92	& 80.25	& 83.62\\
		\hline
		\cellcolor{gray!20}{\textbf{$WB_{3}$=\{BVP, EDA\}}} & 80.8 & 84.48 & \cellcolor{gray!20}\textbf{86.37} & \cellcolor{gray!20}\textbf{89.33} & 86.13 & 89.26 & 83.77 & 86.55 & 79.7 & 83.66\\
		\hline
		\textbf{$WB_{4}$=\{ACC, BVP\}} & 74.97 & 79.94 & 76.43 & 82.45 & 79.77 & 84.21 & 82.37 & 85.07 & 76.49 & 80.13\\
		\hline
		\textbf{$WB_{5}$=\{ACC, EDA\}} & 65.65 & 76.1 & 72.77 & 82.42 & 75.39 & 83.52 & 78.66 & 84.19 & 73.72 & 77.55\\
		\hline
\end{tabular}
	% \vspace{-2ex}
\end{table*}

\begin{table*}[!ht]
	\centering
	\caption{Early Fusion Performance of Chest Modalities in WESAD Dataset for 3-Class (Baseline vs. Stress vs. Amusement)}
	\label{tab:chest_3_class_analysis}
	\begin{tabular}{|c||c|c||c|c||c|c||c|c||c|c|}
		\hline
		\multirow{2}{*}{\textbf{Modality Used}} &
		\multicolumn{2}{c||}{\textbf{DT}} &\multicolumn{2}{c||}{\textbf{RF}} & \multicolumn{2}{c||}{\cellcolor{gray!20}\textbf{AB}}  & \multicolumn{2}{c||}{\textbf{LDA}}  & \multicolumn{2}{c|}{\textbf{KNN}} \\
		\cline{2-3}\cline{4-5}\cline{6-7}\cline{8-9}\cline{10-11}
		& \textbf{M. F1} & \textbf{Acc.} & \textbf{M. F1} & \textbf{Acc.} & \textbf{M. F1} & \textbf{Acc.}& \textbf{M. F1} & \textbf{Acc.} & \textbf{M. F1} & \textbf{Acc.}\\
		\hline\hline
\cellcolor{gray!20}{\textbf{$CB_{1}$=\{ECG,RESP,EMG,EDA,TEMP\}}} & 54.26 & 62.07 &	58.68 & 71.39 & \cellcolor{gray!20}\textbf{65.63} & \cellcolor{gray!20}\textbf{76.53} & 30.14 & 38.2 & 53.87 &	65.1\\
\hline
\textbf{$CB_{2}$=\{ACC,ECG,RESP,EMG,TEMP\}} & 49.41 & 57.01 &	53.55 & 69.1 & 60.18 & 71.88 & 55.71 & 72.0 & 43.15 &	51.5\\
\hline\textbf{$CB_{3}$=\{ACC,RESP,EMG,TEMP\}} & 45.8 & 55.14 &	53.46 & 68.58 & 57.11 & 69.14 & 54.93 & 66.07 & 44.79 &	55.49\\
\hline\textbf{$CB_{4}$=\{ACC,ECG,RESP,EMG\}} & 48.29 & 55.55 &	50.37 & 63.36 & 54.52 & 65.55 & 51.8 & 62.8 & 43.04 &	51.56\\
\hline\textbf{$CB_{5}$=\{ACC,ECG,RESP,EDA\}} & 44.33 & 53.75 &	52.32 & 69.15 & 55.02 & 73.09 & 44.05 & 54.93 & 43.73 &	52.43\\
\hline\textbf{$CB_{6}$=\{ACC,ECG,EMG,TEMP\}} & 48.96 & 56.55 &	53.98 & 69.58 & 58.98 & 70.88 & 55.31 & 71.41 & 42.33 &	49.93\\
\hline\textbf{$CB_{7}$=\{ACC,ECG,EMG,EDA\}} & 51.22 & 61.09 &	57.0 & 72.64 & 59.54 & 73.35 & 45.79 & 56.21 & 46.82 &	56.17\\
\hline\textbf{$CB_{8}$=\{ACC,RESP,EDA,TEMP\}} & 42.79 & 51.74 &	54.6 & 71.06 & 52.34 & 67.32 & 22.66 & 24.62 & 40.91 &	48.78\\
\hline\textbf{$CB_{9}$=\{ACC,ECG,EDA,TEMP\}} & 41.94 & 51.1 &	54.75 & 72.57 & 57.36 & 75.99 & 38.59 & 48.37 & 45.53 &	54.2\\
\hline
\textbf{$CB_{10}$=\{ECG,RESP,EMG,TEMP\}} & 47.17 & 52.8 &	58.11 & 70.71 & 61.8 & 71.92 & 54.51 & 68.6 & 51.17 &	59.94\\
\hline
{\textbf{$CB_{11}$=\{ECG,RESP,EMG,EDA\}}} & 51.14 & 59.24 &	57.7 & 68.9 & 60.47 & 70.68 & 50.34 & 60.93 & 51.54 &	62.99\\
\hline
\cellcolor{gray!20}{\textbf{$CB_{12}$=\{ECG,EMG,EDA,TEMP\}}} & 53.95 & 61.41 &	57.95 & 71.12 & \cellcolor{gray!20}\textbf{62.09} & \cellcolor{gray!20}\textbf{74.51} & 31.52 & 39.52 & 53.31 &	63.83\\
\hline
\textbf{$CB_{13}$=\{ECG,RESP,EDA,TEMP\}} & 51.75 & 60.09 &	55.02 & 72.85 & 57.73 & 73.45 & 31.98 & 38.97 & 57.14 &	70.26\\
\hline
\cellcolor{gray!20}{\textbf{$CB_{14}$=\{RESP,EMG,EDA,TEMP\}}} & 48.93 & 54.44 &	60.87 & 71.41 & \cellcolor{gray!20}\textbf{63.68} & \cellcolor{gray!20}\textbf{74.16} & 31.59 & 37.16 & 51.69 &	64.38\\
\hline\textbf{$CB_{15}$=\{ACC,EDA,TEMP\}} & 41.23 & 49.34 &	53.69 & 69.18 & 52.91 & 68.95 & 24.18 & 25.84 & 42.29 &	50.57\\
\hline\textbf{$CB_{16}$=\{ACC,EMG,EDA\}} & 48.81 & 58.07 &	54.59 & 69.32 & 54.7 & 69.25 & 35.84 & 45.83 & 44.99 &	54.87\\
\hline\textbf{$CB_{17}$=\{ACC,RESP,EDA\}} & 43.23 & 51.69 &	51.5 & 67.36 & 49.91 & 66.85 & 35.15 & 45.74 & 39.96 &	49.74\\
\hline\textbf{$CB_{18}$=\{ACC,ECG,RESP\}} & 40.4 & 50.19 &	48.55 & 61.61 & 50.11 & 65.04 & 51.39 & 61.91 & 39.85 &	48.3\\
\hline\textbf{$CB_{19}$=\{ACC,RESP,EMG\}} & 45.19 & 52.93 &	48.03 & 61.65 & 50.84 & 62.98 & 43.41 & 54.94 & 42.54 &	54.79\\
\hline\textbf{$CB_{20}$=\{ACC,ECG,EDA\}} & 44.66 & 53.48 &	53.36 & 69.41 & 53.78 & 72.28 & 43.15 & 53.74 & 42.98 &	50.43\\
\hline\textbf{$CB_{21}$=\{ECG,RESP,EMG\}} & 43.25 & 49.07 &	51.13 & 58.51 & 52.17 & 59.63 & 54.68 & 65.37 & 47.82 &	57.02\\
\hline\textbf{$CB_{22}$=\{ECG,EDA,TEMP\}} & 52.6 & 62.66 &	55.06 & 72.59 & 57.63 & 72.9 & 33.23 & 39.41 & 56.58 &	68.1\\
\hline\textbf{$CB_{23}$=\{ECG,RESP,EDA\}} & 49.02 & 55.73 &	51.74 & 65.97 & 50.34 & 65.1 & 46.24 & 59.24 & 52.38 &	64.95\\
\hline
\cellcolor{gray!20}{\textbf{$CB_{24}$=\{ECG,EMG,EDA\}}} & 52.58 & 60.19 &	57.89 & 68.58 & \cellcolor{gray!20}\textbf{61.69} & \cellcolor{gray!20}\textbf{71.25} & 49.41 & 59.77 & 50.23 &	60.75\\
\hline\textbf{$CB_{25}$=\{RESP,EMG,EDA\}} & 42.09 & 49.83 &	56.13 & 64.04 & 61.46 & 69.09 & 39.07 & 49.63 & 48.25 &	61.33\\
\hline\textbf{$CB_{26}$=\{RESP,EDA,TEMP\}} & 45.95 & 54.85 &	56.76 & 74.1 & 54.56 & 71.05 & 22.48 & 23.51 & 50.27 &	64.98\\
\hline
\cellcolor{gray!20}{\textbf{$CB_{27}$=\{EMG,EDA,TEMP\}}} & 49.94 & 55.45 &	61.32 & 71.51 & \cellcolor{gray!20}\textbf{64.72} & \cellcolor{gray!20}\textbf{74.36} & 32.73 & 40.29 & 51.23 &	62.91\\
\hline
\textbf{$CB_{28}$=\{ACC,RESP\}} & 42.27 & 51.12 &	48.39 & 60.41 & 45.06 & 58.18 & 43.3 & 56.92 & 40.93 &	52.46\\
\hline\textbf{$CB_{29}$=\{ACC,EMG\}} & 44.36 & 52.12 &	47.96 & 61.39 & 50.41 & 63.09 & 42.04 & 53.55 & 42.18 &	53.4\\
\hline\textbf{$CB_{30}$=\{ACC,ECG\}} & 39.41 & 48.76 &	49.48 & 63.37 & 48.79 & 62.9 & 51.14 & 61.68 & 37.45 &	45.01\\
\hline\textbf{$CB_{31}$=\{ACC,EDA\}} & 42.26 & 49.04 &	51.56 & 67.42 & 47.84 & 65.89 & 32.8 & 42.47 & 40.46 &	50.72\\
\hline\textbf{$CB_{32}$=\{ACC,TEMP\}} & 39.86 & 48.43 &	49.58 & 62.29 & 46.81 & 59.66 & 52.05 & 63.16 & 42.78 &	52.02\\
\hline\textbf{$CB_{33}$=\{ECG,RESP\}} & 42.24 & 47.3 &	44.13 & 54.5 & 45.72 & 55.92 & 52.31 & 66.59 & 45.34 &	56.68\\
\hline\textbf{$CB_{34}$=\{ECG,EMG\}} & 40.99 & 46.91 &	51.12 & 57.73 & 50.89 & 58.93 & 54.25 & 64.96 & 48.46 &	55.93\\
\hline\textbf{$CB_{35}$=\{ECG,EDA\}} & 51.3 & 57.86 &	50.4 & 64.85 & 50.02 & 64.46 & 44.76 & 57.57 & 50.21 &	61.31\\
\hline\textbf{$CB_{36}$=\{ECG,TEMP\}} & 41.34 & 48.0 &	48.82 & 63.43 & 51.94 & 63.68 & 53.47 & 69.58 & 50.07 &	59.97\\
\hline\textbf{$CB_{37}$=\{EDA,TEMP\}} & 47.52 & 56.72 &	55.39 & 72.88 & 53.17 & 69.89 & 23.22 & 24.12 & 47.97 &	59.91\\
\hline\textbf{$CB_{38}$=\{RESP,EDA\}} & 39.88 & 47.63 &	50.19 & 60.34 & 45.98 & 54.46 & 30.8 & 42.41 & 48.9 &	64.12\\
\hline\textbf{$CB_{39}$=\{RESP,EMG\}} & 42.73 & 50.93 &	47.94 & 57.68 & 47.65 & 57.34 & 44.9 & 57.34 & 45.69 &	56.73\\
\hline\textbf{$CB_{40}$=\{RESP,TEMP\}} & 41.78 & 53.36 &	51.85 & 69.44 & 49.91 & 59.96 & 56.65 & 71.94 & 48.43 &	62.27\\
\hline\textbf{$CB_{41}$=\{EMG,EDA\}} & 42.9 & 51.26 &	55.5 & 63.64 & 60.54 & 68.68 & 36.4 & 45.9 & 49.92 &	60.11\\
\hline\textbf{$CB_{42}$=\{EMG,TEMP\}} & 40.63 & 45.37 &	62.12 & 71.95 & 63.16 & 70.91 & 58.63 & 67.99 & 50.41 &	59.67\\
\hline
\end{tabular}
	% \vspace{-2ex}
\end{table*}

\begin{table*}[t]
	\centering
	\caption{Early Fusion Performance of Chest Modalities in WESAD Dataset for 2-Class (Stress vs. Non-stress)}
	\label{tab:chest_2_class_analysis}
	\begin{tabular}{|c||c|c||c|c||c|c||c|c||c|c|}
		\hline
		\multirow{2}{*}{\textbf{Modality Used}} &
		\multicolumn{2}{c||}{\textbf{DT}} &\multicolumn{2}{c||}{\textbf{RF}} & \multicolumn{2}{c||}{\cellcolor{gray!20}\textbf{AB}}  & \multicolumn{2}{c||}{\textbf{LDA}}  & \multicolumn{2}{c|}{\textbf{KNN}} \\
		\cline{2-3}\cline{4-5}\cline{6-7}\cline{8-9}\cline{10-11}
		& \textbf{M. F1} & \textbf{Acc.} & \textbf{M. F1} & \textbf{Acc.} & \textbf{M. F1} & \textbf{Acc.}& \textbf{M. F1} & \textbf{Acc.} & \textbf{M. F1} & \textbf{Acc.}\\
		\hline\hline
        \textbf{$CB_{1}$=\{ECG,RESP,EMG,EDA,TEMP\}} & 73.17 & 75.85 &	82.02 & 83.24 & 81.45 & 84.14 & 46.32 & 48.77 & 74.68 &	78.81\\
\hline\textbf{$CB_{2}$=\{ACC,ECG,RESP,EMG,TEMP\}} & 67.25 & 73.11 &	80.12 & 83.86 & 77.07 & 82.75 & 77.53 & 79.98 & 63.81 &	69.48\\
\hline\textbf{$CB_{3}$=\{ACC,RESP,EMG,TEMP\}} & 68.85 & 76.61 &	78.15 & 83.44 & 73.16 & 81.56 & 74.03 & 77.17 & 61.97 &	70.88\\
\hline\textbf{$CB_{4}$=\{ACC,ECG,RESP,EMG\}} & 66.93 & 72.16 &	70.57 & 78.06 & 72.98 & 80.57 & 75.66 & 79.83 & 64.12 &	69.86\\
\hline
\cellcolor{gray!20}\textbf{$CB_{5}$=\{ACC,ECG,RESP,EDA\}} & 70.39 & 75.81 &	82.41 & 84.21 & \cellcolor{gray!20}\textbf{83.21} & \cellcolor{gray!20}\textbf{85.64} & 68.46 & 75.02 & 74.6 &	77.75\\
\hline\textbf{$CB_{6}$=\{ACC,ECG,EMG,TEMP\}} & 66.06 & 72.22 &	80.13 & 83.88 & 75.77 & 82.36 & 77.13 & 79.64 & 62.97 &	68.0\\
\hline
\cellcolor{gray!20}\textbf{$CB_{7}$=\{ACC,ECG,EMG,EDA\}} & 72.45 & 77.72 &	83.49 & 85.64 & \cellcolor{gray!20}\textbf{82.29} & \cellcolor{gray!20}\textbf{85.72} & 69.6 & 75.2 & 72.05 &	75.86\\
\hline\textbf{$CB_{8}$=\{ACC,RESP,EDA,TEMP\}} & 68.71 & 73.26 &	81.03 & 84.16 & 72.19 & 79.34 & 29.71 & 32.23 & 66.42 &	74.28\\
\hline
\cellcolor{gray!20}\textbf{$CB_{9}$=\{ACC,ECG,EDA,TEMP\}} & 65.13 & 70.79 &	84.47 & 86.12 & \cellcolor{gray!20}\textbf{82.15} & \cellcolor{gray!20}\textbf{85.2} & 59.5 & 61.72 & 75.02 &	78.4\\
\hline\textbf{$CB_{10}$=\{ECG,RESP,EMG,TEMP\}} & 52.55 & 54.77 &	76.87 & 80.08 & 76.02 & 80.39 & 76.89 & 79.17 & 68.32 &	73.94\\
\hline\textbf{$CB_{11}$=\{ECG,RESP,EMG,EDA\}} & 74.89 & 77.51 &	81.23 & 82.93 & 82.25 & 84.79 & 70.15 & 75.69 & 73.76 &	77.89\\
\hline\textbf{$CB_{12}$=\{ECG,EMG,EDA,TEMP\}} & 72.35 & 75.25 &	82.05 & 83.39 & 79.64 & 82.7 & 47.71 & 50.14 & 73.03 &	77.14\\
\hline
\cellcolor{gray!20}\textbf{$CB_{13}$=\{ECG,RESP,EDA,TEMP\}} & 73.62 & 75.58 &	79.2 & 80.24 & \cellcolor{gray!20}\textbf{83.31} & \cellcolor{gray!20}\textbf{84.78} & 49.66 & 51.9 & 79.22 &	82.04\\
\hline\textbf{$CB_{14}$=\{RESP,EMG,EDA,TEMP\}} & 70.82 & 73.11 &	81.8 & 84.15 & 77.41 & 81.44 & 47.13 & 50.71 & 67.67 &	76.19\\
\hline\textbf{$CB_{15}$=\{ACC,EDA,TEMP\}} & 66.82 & 72.0 &	80.51 & 83.08 & 71.5 & 79.53 & 32.61 & 35.96 & 66.95 &	73.5\\
\hline\textbf{$CB_{16}$=\{ACC,EMG,EDA\}} & 69.7 & 76.11 &	78.93 & 83.67 & 74.92 & 82.07 & 54.66 & 61.85 & 64.89 &	71.65\\
\hline\textbf{$CB_{17}$=\{ACC,RESP,EDA\}} & 66.46 & 73.42 &	77.17 & 81.81 & 70.91 & 79.16 & 53.28 & 62.59 & 63.82 &	71.44\\
\hline\textbf{$CB_{18}$=\{ACC,ECG,RESP\}} & 62.03 & 69.17 &	72.22 & 78.51 & 75.78 & 81.16 & 74.68 & 79.53 & 65.87 &	69.49\\
\hline\textbf{$CB_{19}$=\{ACC,RESP,EMG\}} & 63.76 & 71.48 &	66.44 & 77.19 & 64.21 & 75.5 & 63.29 & 71.03 & 60.84 &	70.04\\
\hline
\cellcolor{gray!20}\textbf{$CB_{20}$=\{ACC,ECG,EDA\}} & 69.46 & 74.81 &	84.11 & 85.62 & \cellcolor{gray!20}\textbf{84.0} & \cellcolor{gray!20}\textbf{86.37} & 66.96 & 73.53 & 73.19 &	76.18\\
\hline\textbf{$CB_{21}$=\{ECG,RESP,EMG\}} & 61.32 & 65.25 &	68.82 & 73.78 & 67.5 & 74.26 & 75.91 & 80.2 & 65.96 &	72.18\\
\hline\textbf{$CB_{22}$=\{ECG,EDA,TEMP\}} & 73.09 & 75.06 &	78.33 & 79.4 & 81.01 & 82.4 & 52.84 & 55.01 & 77.92 &	80.7\\
\hline\textbf{$CB_{23}$=\{ECG,RESP,EDA\}} & 70.15 & 72.45 &	78.83 & 80.05 & 79.93 & 81.62 & 67.9 & 74.41 & 76.45 &	79.31\\
\hline\textbf{$CB_{24}$=\{ECG,EMG,EDA\}} & 74.75 & 77.26 &	80.69 & 82.39 & 82.03 & 84.58 & 69.03 & 74.81 & 71.75 &	75.88\\
\hline\textbf{$CB_{25}$=\{RESP,EMG,EDA\}} & 60.6 & 65.44 &	74.27 & 78.99 & 73.3 & 79.62 & 54.6 & 62.95 & 67.22 &	76.13\\
\hline\textbf{$CB_{26}$=\{RESP,EDA,TEMP\}} & 69.55 & 71.23 &	77.57 & 78.93 & 77.44 & 79.83 & 35.86 & 39.07 & 70.25 &	75.78\\
\hline\textbf{$CB_{27}$=\{EMG,EDA,TEMP\}} & 70.92 & 73.2 &	80.65 & 83.39 & 77.88 & 82.14 & 50.88 & 54.17 & 66.11 &	73.9\\
\hline\textbf{$CB_{28}$=\{ACC,RESP\}} & 64.32 & 70.8 &	68.92 & 77.16 & 66.29 & 75.53 & 62.96 & 75.13 & 65.24 &	72.01\\
\hline\textbf{$CB_{29}$=\{ACC,EMG\}} & 61.76 & 69.37 &	66.36 & 76.85 & 64.19 & 75.86 & 60.91 & 69.19 & 60.51 &	68.4\\
\hline\textbf{$CB_{30}$=\{ACC,ECG\}} & 63.41 & 70.31 &	71.71 & 78.03 & 75.3 & 80.99 & 74.32 & 79.37 & 62.87 &	66.02\\
\hline\textbf{$CB_{31}$=\{ACC,EDA\}} & 65.63 & 73.35 &	77.51 & 81.63 & 69.97 & 78.57 & 50.03 & 59.14 & 64.28 &	70.83\\
\hline\textbf{$CB_{32}$=\{ACC,TEMP\}} & 65.69 & 72.52 &	76.55 & 82.0 & 68.5 & 78.26 & 73.3 & 76.77 & 67.49 &	74.41\\
\hline\textbf{$CB_{33}$=\{ECG,RESP\}} & 65.21 & 69.46 &	72.23 & 77.74 & 73.98 & 78.63 & 77.57 & 82.62 & 68.9 &	72.35\\
\hline\textbf{$CB_{34}$=\{ECG,EMG\}} & 59.41 & 64.16 &	68.24 & 73.16 & 66.69 & 74.49 & 75.07 & 79.67 & 65.89 &	71.05\\
\hline\textbf{$CB_{35}$=\{ECG,EDA\}} & 73.98 & 75.83 &	79.27 & 80.54 & 79.02 & 80.46 & 66.44 & 73.23 & 74.62 &	77.1\\
\hline\textbf{$CB_{36}$=\{ECG,TEMP\}} & 60.01 & 62.86 &	74.62 & 77.53 & 76.0 & 79.02 & 77.6 & 79.43 & 69.8 &	72.64\\
\hline\textbf{$CB_{37}$=\{EDA,TEMP\}} & 67.64 & 69.64 &	76.97 & 78.79 & 73.82 & 76.4 & 42.8 & 45.87 & 68.43 &	72.55\\
\hline\textbf{$CB_{38}$=\{RESP,EDA\}} & 56.46 & 59.02 &	74.86 & 77.79 & 66.92 & 71.09 & 48.47 & 59.08 & 71.71 &	78.38\\
\hline\textbf{$CB_{39}$=\{RESP,EMG\}} & 57.13 & 66.44 &	53.69 & 69.12 & 54.17 & 69.16 & 61.04 & 70.26 & 57.0 &	69.98\\
\hline\textbf{$CB_{40}$=\{RESP,TEMP\}} & 54.42 & 56.14 &	73.9 & 77.39 & 73.96 & 77.51 & 73.24 & 76.48 & 70.44 &	74.84\\
\hline\textbf{$CB_{41}$=\{EMG,EDA\}} & 61.9 & 67.19 &	76.18 & 80.4 & 72.54 & 79.26 & 51.34 & 59.27 & 66.06 &	74.2\\
\hline\textbf{$CB_{42}$=\{EMG,TEMP\}} & 56.15 & 60.49 &	77.06 & 81.6 & 72.14 & 78.9 & 73.38 & 76.76 & 62.5 &	71.6\\
\hline
\end{tabular}
	% \vspace{-2ex}
\end{table*}


\begin{figure*} [!ht]
    \centering
    \includegraphics[width=16cm]{Figures/wrist_3_class.png}
    \caption{Overall performance comparison of related works using LOSO validation on wrist data 3-class. Results show that SELF-CARE outperforms the related works, branch classifiers, and other traditional late fusion methods in terms of macro F1 and accuracy except for the macro F1 score of \cite{samyoun2020stress} which uses both wrist and chest data.}
    \label{fig:overall_performance_wrist_3_class}
    % \vspace{-5mm}
\end{figure*}

\subsubsection{Train Gating Model}
% The purpose of the gating model is to understand the context of the movement and generate the probability of using the 3 final branch classifiers based on that context. 
The gating model interprets the context of a sample by modeling the movement (for wrist-worn devices) or muscle contraction (for chest-worn devices) that occurred during that segment.
Therefore, we use the ACC (wrist) or EMG (chest) features as input data to train the gating model with the labels generated from the previous Section \ref{sec:generate_gating_label}.
% And the generated label from previous Section \ref{generate_gating_label} is used as the true labels for training the gating model. 
We use a DT classifier as the gating model where the minimum number of samples to split a node is set to 20. The DT classifier is very lightweight and helps to minimize the overhead of SELF-CARE. Once the gating model is trained, the test subject data is used to test our architecture as shown in Fig. \ref{fig:arch}. For wrist-worn devices, the gating model outputs the probability of using the three final branch classifiers based on the test subject's ACC features. Similarly, for chest-worn devices, EMG features are  used by the gating model to determine the probability of using the five final branch classifiers as mentioned in Section \ref{sec:branch_classifier_selection}. One, two, or all of the final classifiers may be selected for final classification depending on the value of $\delta$, as discussed earlier in Section \ref{sec:delta}. For our 3-class (2-class) classification using wrist-worn devices, we set $\delta=0.40$ ($\delta=0.10$). And for the chest-worn devices, we set $\delta=0.20$ for 3-class and $\delta=0.15$ for 2-class classification. The model extracts additional features based on the required input of the selected branch classifiers, and applies a late fusion method to the classification output of the selected branches to generate the final result.

\subsubsection{Kalman Filter Tuning}
\label{sec:kf_tune}
The Kalman filter-based method is the only late fusion method in our implementation that requires tuning. As described in Section \ref{sec:lfb}, Kalman filters require an initial state ($\mathbf{x}_0$), state covariance ($\mathbf{P}_0$) and process noise and measurement noise vectors, $\mathbf{v}$ and $\mathbf{w}$, respectively. For the 3-class (2-class) classification using wrist-worn devices, we initialize $x_0 = [0.8,0.1,0.1]^\top$ ($x_0 = [0.8,0.2]^\top$). Similarly, for the 3-class and 2-class classification using chest-worn devices, $x_0$ is initialized to $[0.93,0.21,0.01]^\top$  $[1.0,0.55]^\top$. For 3-class (2-class) classification, we initialize $P_0 = 0.01\cdot\mathbf{I}_{3x3}$ ($P_0 = 0.01\cdot\mathbf{I}_{2x2}$) for both  wrist-worn and chest-worn devices. The state transition matrix $\mathbf{F}$ and measurement matrix $\mathbf{H}$ are identity matrices for the respective problems. The $\mathbf{Q}$ for both problems is modeled as a discrete time white process noise with variance set at 5e-4. The measurement noise is modeled as a function of each measurement to allow the filter to adjust the confidence of the measurements according to each reported class probability: $\mathbf{R} = ((\mathbf{1}-\mathbf{z})\cdot2\cdot\mathbf{I}_{3x3})^2$ ($\mathbf{R} = ((\mathbf{1}-\mathbf{z})/2\cdot\mathbf{I}_{2x2})^2$).
Lastly, a tunable threshold technique was used to process the measurements which involved (i) an $\epsilon$ parameter to select measurements which had a maximum predicted probability above the threshold and (ii) a $\gamma$ factor to scale the measurements to account for the imbalanced class distribution in the dataset. This thresholding process allows the filter to weigh each measurement it receives with a different degree of noise while also attempting to resolve issues that arise from imbalanced datasets. For the 3-class (2-class) classification using wrist-worn devices, we set $\epsilon = 0.4$ ($\epsilon = 0.7$) and $\gamma = [0.278,1,1]^\top $ ($\gamma = [0.667,1.1]^\top$). 
For the 3-class (2-class)  classification using chest-worn  devices, we set $\epsilon = 0.5$ ($\epsilon = 0.5$) and $\gamma = [1.35,1.5,1.6]^\top $ ($\gamma = [0.915,0.995]^\top$).
 %During 3-class classification, the prediction probabilities are generally lower as they are distributed across an additional class when compared to 2-class classification, thus calling for a lower $\epsilon$ threshold. 
%These parameters can be considered similar to hyperparameters in a machine learning model where tuning in the current implementation can improve results.
%\textcolor{red}{Add sentence}

% Implementation Paragraph for Kalman block: Explain settings for initial uncertainty covariance, process noise generation, measurement noise covariance. Another sentence here on our algorithm only using measurements that are above a certain threshold (abstract to an epsilon parameter). 
% Implementation details for other late fusion methods - not necessary with our current implementation.


\newcommand{\bad}[0]{\cellcolor{red!10}}
\newcommand{\good}[0]{\cellcolor{green!10}}
% \newcommand{\best}[0]{\cellcolor{green!25}}

	
% \begin{table*}[t]
% 	\centering
% 	\caption{Overall Performance Comparison of Related Works using LOSO Validation}
% 	\label{overall_performance}
% 	\begin{tabular}{|c||c|c|c||c|c|c||c|c|}
% 		\hline
% 		\multirow{2}{*}{\textbf{Modality Used}} &
% 		\multicolumn{3}{c||}{\textbf{3-Class}} &\multicolumn{3}{c||}{\textbf{2-Class}} & \textbf{Wrist} & \textbf{Wrist} \\%& \textbf{Loso}\\
% 		\cline{2-4}\cline{5-7}
% 		& \textbf{Best Model} & \textbf{Macro F1} & \textbf{Accuracy} & \textbf{Best Model} & \textbf{Macro F1} & \textbf{Accuracy} & \textbf{Only} & \textbf{Computing} \\%& \textbf{Validation}\\
% 		\hline\hline
% 		\multicolumn{9}{|c|}{\cellcolor{blue!5}\textbf{Related Works}}\\
% 		\hline
% 		All (Wrist+Chest) \cite{schmidt2018introducing} & AB & 68.85 & 79.57 & LDA & 90.74 & 92.28 & \bad No & \bad No \\%& Yes\\
% 		\hline
% 		{All (Wrist+Chest) W/O ACC \cite{schmidt2018introducing}} & AB & 71.10 & 79.86 & LDA & 90.93	& 92.51 & \bad No & \bad No \\%& Yes\\
% 		\hline
% 		{All Chest \cite{schmidt2018introducing}} & LDA & 72.49 & 76.50 & LDA & 91.07 & 92.83 & \bad No & \bad No \\%& Yes\\
% 		\hline
% 		{All Chest W/O ACC \cite{schmidt2018introducing}} & AB & 72.51 & 80.34 & LDA & 91.47 & 93.12 & \bad No & \bad No \\%& Yes\\
% 		\hline
% 		{All Wrist \cite{schmidt2018introducing}} & AB & 64.12 & 75.21 & RF & 84.11 & 87.12 & \good Yes & \good Yes \\%& Yes\\
% 		\hline
% 		{All Wrist W/O ACC \cite{schmidt2018introducing}} & RF & 66.33 & 76.17 & RF & 72.51 & 80.34 & \good Yes & \good Yes \\%& Yes\\
% 		\hline
% 		{All Wrist + Trans. Chest\cite{samyoun2020stress}} & GAN-RF	& \textbf{74.5} & 81.4 & GAN-RF & 89.7 & 92.1 & \bad No & \bad No \\%& Yes\\
% 		\hline
% 		{All Wrist \cite{huynh2021stressnas}}  & DNN & - & 83.43 & DNN & - & 93.14 & \good Yes & \bad No \\%& Yes\\
% 		\hline
% 		{BVP (Wrist) \cite{rashid2021feature}}  & HCNN & 64.15 & 75.21 & HCNN & 86.18 & 88.56 & \good Yes & \bad No \\%& Yes\\
% 		%\hline
% 		%\textbf{All (Chest+Wrist) \cite{lin2019explainable}} & CNN - RF &- & 85 & - & - & - & No & No \\%& No\\
% 		\hline\hline
% 		\multicolumn{9}{|c|}{\cellcolor{blue!5}\textbf{Selected Branch Classifiers [Ours]}}\\
% 		\hline
% 		{\textbf{$B_{1}$=\{BVP, EDA, TEMP\}(Wrist)}} & RF	 & 62.73 & 76.62 & RF & 84.66 & 89.01 & \good Yes & \good Yes \\%& Yes\\
% 		\hline
% 		{\textbf{$B_{2}$=\{ACC, BVP, EDA\}(Wrist)}} & RF & 62.88 & 77.71 & RF & 85.08 & 88.76 & \good Yes & \good Yes \\%& Yes\\
% 		\hline
% 		{\textbf{$B_{3}$=\{BVP, EDA\}(Wrist)}} & RF & 61.02 & 73.96 & RF & 86.37 & 89.33 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% % 		\textbf{[B4] ACC, BVP (Wrist)} & AB	& 57.83	& 71.96 & LDA & 82.37 &	85.07 & Yes & Yes & Yes\\
% % 		\hline
% % 		\textbf{[B5] ACC, EDA (Wrist)} & AB	& 56.47	& 71.31 & LDA &	78.66 & 84.19 & Yes & Yes & Yes\\
% % 		\hline\hline
% % 		\multicolumn{10}{|c|}{\cellcolor{blue!10}\textbf{Related Works}}\\
% % 		\hline
		
% 		\hline\hline
% 		\multicolumn{9}{|c|}{\cellcolor{blue!5}\textbf{Traditional Late Fusion [Ours]}}\\
% 		\hline
% 		\textbf{Soft-voting ($B_{1}$, $B_{2}$, $B_{3}$)} & RF	& 63.75	& 78.79 & RF & 87.09 & 90.00 & \good Yes & \good Yes \\%& Yes\\
% 		\hline
% 		\textbf{Hard-voting ($B_{1}$, $B_{2}$, $B_{3}$)}  & RF & 64.02	& 78.70 & RF & 87.17 & 89.89 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% % 		\textbf{Confidence (B1, B2, B3)} & RF & 63.01 & 78.05 & RF & 86.55 & 89.87 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% % 		\textbf{Kalman (B1, B2, B3)} & RF & 67.30 & 77.57 & RF & 91.05 & 92.41 & \good Yes & \good Yes \\%& Yes\\
% 		\hline\hline
% 		\multicolumn{9}{|c|}{\cellcolor{blue!5}\textbf{SELF-CARE [Ours]}}\\
% 		\hline
% % 		\textbf{Soft (B1, B2, B3)} & RF	& 63.93 & 77.68 & RF & 85.89 & 89.42 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% % 		\textbf{Hard (B1, B2, B3)}  & RF & 62.79 & 77.05 & RF & 85.50 & 89.11 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% % 		\textbf{Confidence (B1, B2, B3)} & RF & 64.03 & 77.63 & RF & 85.85 & 89.34 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% 		\textbf{Kalman ($B_{1}$, $B_{2}$, $B_{3}$)} & RF & 71.97 & \textbf{86.34} & RF & \textbf{92.93} & \textbf{94.12} & \good Yes & \good Yes \\%& Yes\\
% 		\hline
		
% 	\end{tabular}
% 	\vspace{-3ex}
% \end{table*}

% \begin{table*}[t]
% 	\centering
% 	\caption{Overall Performance Comparison of Related Works using LOSO Validation on Wrist Data}
% 	\label{overall_performance}
% 	\begin{tabular}{|c||c|c|c||c|c|c||c|c|}
% 		\hline
% 		\multirow{2}{*}{\textbf{Modality Used}} &
% 		\multicolumn{3}{c||}{\textbf{3-Class}} &\multicolumn{3}{c||}{\textbf{2-Class}} & \textbf{Wrist} & \textbf{Wrist} \\%& \textbf{Loso}\\
% 		\cline{2-4}\cline{5-7}
% 		& \textbf{Best Model} & \textbf{Macro F1} & \textbf{Accuracy} & \textbf{Best Model} & \textbf{Macro F1} & \textbf{Accuracy} & \textbf{Only} & \textbf{Computing} \\%& \textbf{Validation}\\
% 		\hline\hline
% 		\multicolumn{9}{|c|}{\cellcolor{blue!5}\textbf{Related Works}}\\
% 		\hline
% 		All (Wrist+Chest) \cite{schmidt2018introducing} & AB & 68.85 & 79.57 & LDA & 90.74 & 92.28 & \bad No & \bad No \\%& Yes\\
% 		\hline
% % 		{All (Wrist+Chest) W/O ACC \cite{schmidt2018introducing}} & AB & 71.10 & 79.86 & LDA & 90.93	& 92.51 & \bad No & \bad No \\%& Yes\\
% % 		\hline
% % 		{All Chest \cite{schmidt2018introducing}} & LDA & 72.49 & 76.50 & LDA & 91.07 & 92.83 & \bad No & \bad No \\%& Yes\\
% % 		\hline
% % 		{All Chest W/O ACC \cite{schmidt2018introducing}} & AB & 72.51 & 80.34 & LDA & 91.47 & 93.12 & \bad No & \bad No \\%& Yes\\
% % 		\hline
% 		{All Wrist \cite{schmidt2018introducing}} & AB & 64.12 & 75.21 & RF & 84.11 & 87.12 & \good Yes & \good Yes \\%& Yes\\
% 		\hline
% % 		{All Wrist W/O ACC \cite{schmidt2018introducing}} & RF & 66.33 & 76.17 & RF & 72.51 & 80.34 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% 		{All Wrist + Trans. Chest\cite{samyoun2020stress}} & GAN-RF	& \textbf{74.5} & 81.4 & GAN-RF & 89.7 & 92.1 & \bad No & \bad No \\%& Yes\\
% 		\hline
% 		{All Wrist \cite{huynh2021stressnas}}  & DNN & - & 83.43 & DNN & - & 93.14 & \good Yes & \bad No \\%& Yes\\
% 		\hline
% 		{BVP (Wrist) \cite{rashid2021feature}}  & HCNN & 64.15 & 75.21 & HCNN & 86.18 & 88.56 & \good Yes & \bad No \\%& Yes\\
% 		%\hline
% 		%\textbf{All (Chest+Wrist) \cite{lin2019explainable}} & CNN - RF &- & 85 & - & - & - & No & No \\%& No\\
% 		\hline\hline
% 		\multicolumn{9}{|c|}{\cellcolor{blue!5}\textbf{Selected Branch Classifiers [Ours]}}\\
% 		\hline
% 		{\textbf{$B_{1}$=\{BVP, EDA, TEMP\}(Wrist)}} & RF	 & 62.73 & 76.62 & RF & 84.66 & 89.01 & \good Yes & \good Yes \\%& Yes\\
% 		\hline
% 		{\textbf{$B_{2}$=\{ACC, BVP, EDA\}(Wrist)}} & RF & 62.88 & 77.71 & RF & 85.08 & 88.76 & \good Yes & \good Yes \\%& Yes\\
% 		\hline
% 		{\textbf{$B_{3}$=\{BVP, EDA\}(Wrist)}} & RF & 61.02 & 73.96 & RF & 86.37 & 89.33 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% % 		\textbf{[B4] ACC, BVP (Wrist)} & AB	& 57.83	& 71.96 & LDA & 82.37 &	85.07 & Yes & Yes & Yes\\
% % 		\hline
% % 		\textbf{[B5] ACC, EDA (Wrist)} & AB	& 56.47	& 71.31 & LDA &	78.66 & 84.19 & Yes & Yes & Yes\\
% % 		\hline\hline
% % 		\multicolumn{10}{|c|}{\cellcolor{blue!10}\textbf{Related Works}}\\
% % 		\hline
		
% 		\hline\hline
% 		\multicolumn{9}{|c|}{\cellcolor{blue!5}\textbf{Traditional Late Fusion [Ours]}}\\
% 		\hline
% 		\textbf{Soft-voting ($B_{1}$, $B_{2}$, $B_{3}$)} & RF	& 63.75	& 78.79 & RF & 87.09 & 90.00 & \good Yes & \good Yes \\%& Yes\\
% 		\hline
% 		\textbf{Hard-voting ($B_{1}$, $B_{2}$, $B_{3}$)}  & RF & 64.02	& 78.70 & RF & 87.17 & 89.89 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% % 		\textbf{Confidence (B1, B2, B3)} & RF & 63.01 & 78.05 & RF & 86.55 & 89.87 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% % 		\textbf{Kalman (B1, B2, B3)} & RF & 67.30 & 77.57 & RF & 91.05 & 92.41 & \good Yes & \good Yes \\%& Yes\\
% 		\hline\hline
% 		\multicolumn{9}{|c|}{\cellcolor{blue!5}\textbf{SELF-CARE [Ours]}}\\
% 		\hline
% % 		\textbf{Soft (B1, B2, B3)} & RF	& 63.93 & 77.68 & RF & 85.89 & 89.42 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% % 		\textbf{Hard (B1, B2, B3)}  & RF & 62.79 & 77.05 & RF & 85.50 & 89.11 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% % 		\textbf{Confidence (B1, B2, B3)} & RF & 64.03 & 77.63 & RF & 85.85 & 89.34 & \good Yes & \good Yes \\%& Yes\\
% % 		\hline
% 		\textbf{Kalman ($B_{1}$, $B_{2}$, $B_{3}$)} & RF & 71.97 & \textbf{86.34} & RF & \textbf{92.93} & \textbf{94.12} & \good Yes & \good Yes \\%& Yes\\
% 		\hline
		
% 	\end{tabular}
% 	\vspace{-3ex}
% \end{table*}



% \begin{table}[t]
% 	\centering
% 	\caption{Overall Performance Comparison of Related Works using LOSO Validation on Wrist Data 3-Class}
% 	\label{overall_performance_wrist_3_class}
% 	\begin{tabular}{|c||c|c|c|}
% 		\hline
% 	    \textbf{Modality Used}	& \textbf{Model} & \textbf{M. F1} & \textbf{Acc.}\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Related Works}}\\
% 		\hline
% 		All (Wrist+Chest) \cite{schmidt2018introducing} & AB & 68.85 & 79.57 \\
% 		\hline
% 		{All Wrist \cite{schmidt2018introducing}} & AB & 64.12 & 75.21\\
% 		\hline
% 		{All Wrist + Trans. Chest\cite{samyoun2020stress}} & GAN-RF	& \textbf{74.5} & 81.4\\
% 		\hline
% 		{All Wrist \cite{huynh2021stressnas}}  & DNN & - & 83.43\\
% 		\hline
% 		{BVP (Wrist) \cite{rashid2021feature}}  & HCNN & 64.15 & 75.21\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Selected Branch Classifiers [Ours]}}\\
% 		\hline
% 		{\textbf{$B_{1}$=\{BVP, EDA, TEMP\}}} & RF & 62.73 & 76.62 \\
% 		\hline
% 		{\textbf{$B_{2}$=\{ACC, BVP, EDA\}}} & RF & 62.88 & 77.71\\
% 		\hline
% 		{\textbf{$B_{3}$=\{BVP, EDA\}}} & RF & 61.02 & 73.96\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Traditional Late Fusion [Ours]}}\\
% 		\hline
% 		\textbf{Soft-voting ($B_{1}$, $B_{2}$, $B_{3}$)} & RF & 63.75 & 78.79\\
% 		\hline
% 		\textbf{Hard-voting ($B_{1}$, $B_{2}$, $B_{3}$)}  & RF & 64.02	& 78.70\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{SELF-CARE [Ours]}}\\
% 		\hline
% 		\textbf{Kalman ($B_{1}$, $B_{2}$, $B_{3}$)} & RF & 71.97 & \textbf{86.34}\\
% 		\hline
		
% 	\end{tabular}
% % 	\vspace{-3ex}
% \end{table}




% \begin{table}[t]
% 	\centering
% 	\caption{Overall Performance Comparison of Related Works using LOSO Validation on Wrist Data 2-Class}
% 	\label{overall_performance}
% 	\begin{tabular}{|c||c|c|c|}
% 		\hline
% 	\textbf{Modality Used}	& \textbf{Model} & \textbf{M. F1} & \textbf{Acc.}\\%& 
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Related Works}}\\
% 		\hline
% 		All (Wrist+Chest) \cite{schmidt2018introducing} & LDA & 90.74 & 92.28\\
% 		\hline
% 		{All Wrist \cite{schmidt2018introducing}} & RF & 84.11 & 87.12\\
% 		\hline
% 		{All Wrist + Trans. Chest\cite{samyoun2020stress}} & GAN-RF & 89.7 & 92.1\\
% 		\hline
% 		{All Wrist \cite{huynh2021stressnas}}  & DNN & - & 93.14\\
% 		\hline
% 		{BVP (Wrist) \cite{rashid2021feature}}  & HCNN & 86.18 & 88.56 \\%& Yes\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Selected Branch Classifiers [Ours]}}\\
% 		\hline
% 		{\textbf{$B_{1}$=\{BVP, EDA, TEMP\}}} & RF & 84.66 & 89.01 \\%& Yes\\
% 		\hline
% 		{\textbf{$B_{2}$=\{ACC, BVP, EDA\}}} & RF & 85.08 & 88.76 \\%& Yes\\
% 		\hline
% 		{\textbf{$B_{3}$=\{BVP, EDA\}}} & RF & 86.37 & 89.33 \\%& Yes\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Traditional Late Fusion [Ours]}}\\
% 		\hline
% 		\textbf{Soft-voting ($B_{1}$, $B_{2}$, $B_{3}$)} & RF & 87.09 & 90.00 \\%& Yes\\
% 		\hline
% 		\textbf{Hard-voting ($B_{1}$, $B_{2}$, $B_{3}$)}  & RF & 87.17 & 89.89 \\%& Yes\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{SELF-CARE [Ours]}}\\
% 		\hline
% 		\textbf{Kalman ($B_{1}$, $B_{2}$, $B_{3}$)} & RF & \textbf{92.93} & \textbf{94.12}\\%& Yes\\
% 		\hline
% 	\end{tabular}
% % 	\vspace{-3ex}
% \end{table}






% \begin{table}[t]
% 	\centering
% 	\caption{Overall Performance Comparison of Related Works using LOSO Validation on Chest Data 3-Class}
% 	\label{overall_performance}
% 	\begin{tabular}{|c||c|c|c|}
% 		\hline
% 	    \textbf{Modality Used}	& \textbf{Model} & \textbf{M. F1} & \textbf{Acc.}\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Related Works}}\\
% 		\hline
% 		All (Wrist+Chest) \cite{schmidt2018introducing} & AB & 68.85 & 79.57 \\
% 		\hline
% 		{All Chest \cite{schmidt2018introducing}} & AB & 64.12 & 75.21\\
% 		\hline
% 		{All Wrist + Trans. Chest\cite{samyoun2020stress}} & GAN-RF	& 74.5 & 81.4\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Selected Branch Classifiers [Ours]}}\\
% 		\hline
% 		{\textbf{$B_{1}$=\{ECG,RESP,EMG,EDA,TEMP\}}} & AB & 65.63 & 76.53 \\
% 		\hline
% 		{\textbf{$B_{12}$=\{ECG,EMG,EDA,TEMP\}}} & AB & 62.09 & 74.51\\
% 		\hline
% 		{\textbf{$B_{14}$=\{RESP,EMG,EDA,TEMP\}}} & AB & 63.68 & 74.16\\
% 		\hline
% 		{\textbf{$B_{24}$=\{ECG,EMG,EDA\}}} & AB & 61.69 & 71.25\\
% 		\hline
% 		{\textbf{$B_{27}$=\{EMG,EDA,TEMP\}}} & AB & 64.72 & 74.36\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Traditional Late Fusion [Ours]}}\\
% 		\hline
% 		\textbf{Soft-voting ($B_{1}$, $B_{12}$, $B_{14}$, $B_{24}$, $B_{27}$)} & AB & 63.13 & 74.08\\
% 		\hline
% 		\textbf{Hard-voting ($B_{1}$, $B_{12}$, $B_{14}$, $B_{24}$, $B_{27}$)}  & AB & 62.85 & 74.50\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{SELF-CARE [Ours]}}\\
% 		\hline
% 		\textbf{Kalman (EMG) $B_{1}$, $B_{12}$, $B_{14}$, $B_{24}$, $B_{27}$)} & AB & \textbf{79.31} & \textbf{86.19}\\
% 		\hline
% 		\textbf{Kalman (ACC) $B_{1}$, $B_{12}$, $B_{14}$, $B_{24}$, $B_{27}$)} & AB & \textbf{76.28} & \textbf{84.59}\\
% 		\hline
		
% 	\end{tabular}
% % 	\vspace{-3ex}
% \end{table}



% \begin{table}[t]
% 	\centering
% 	\caption{Overall Performance Comparison of Related Works using LOSO Validation on Wrist Data 2-Class}
% 	\label{overall_performance}
% 	\begin{tabular}{|c||c|c|c|}
% 		\hline
% 	\textbf{Modality Used}	& \textbf{Model} & \textbf{M. F1} & \textbf{Acc.}\\%& 
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Related Works}}\\
% 		\hline
% 		All (Wrist+Chest) \cite{schmidt2018introducing} & LDA & 90.74 & 92.28\\
% 		\hline
% 		{All Wrist \cite{schmidt2018introducing}} & RF & 84.11 & 87.12\\
% 		\hline
% 		{All Wrist + Trans. Chest\cite{samyoun2020stress}} & GAN-RF & 89.7 & 92.1\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Selected Branch Classifiers [Ours]}}\\
% 		\hline
% 		{\textbf{$B_{5}$=\{ACC,ECG,RESP,EDA\}}} & AB & 83.21 & 85.64 \\%& Yes\\
% 		\hline
% 		{\textbf{$B_{7}$=\{ACC,ECG,EMG,EDA\}}} & AB & 82.29 & 85.72 \\%& Yes\\
% 		\hline
% 		{\textbf{$B_{9}$=\{ACC,ECG,EDA,TEMP\}}} & AB & 82.15 & 85.20 \\%& Yes\\
% 		\hline
% 		{\textbf{$B_{13}$=\{ECG,RESP,EDA,TEMP\}}} & AB & 83.31 & 84.78 \\%& Yes\\
% 		\hline
% 		{\textbf{$B_{20}$=\{ACC,ECG,EDA\}}} & AB & 84.00 & 86.37 \\%& Yes\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{Traditional Late Fusion [Ours]}}\\
% 		\hline
% 		\textbf{Soft-voting ($B_{5}$, $B_{7}$, $B_{9}$, $B_{13}$, $B_{20}$)} & AB & 84.61 & 86.82 \\%& Yes\\
% 		\hline
% 		\textbf{Hard-voting ($B_{5}$, $B_{7}$, $B_{9}$, $B_{13}$, $B_{20}$)}  & AB & 78.12 & 81.57 \\%& Yes\\
% 		\hline\hline
% 		\multicolumn{4}{|c|}{\cellcolor{blue!5}\textbf{SELF-CARE [Ours]}}\\
% 		\hline
% 		\textbf{Kalman (EMG) ($B_{5}$, $B_{7}$, $B_{9}$, $B_{13}$, $B_{20}$)} & AB & \textbf{92.61} & \textbf{93.68}\\%& Yes\\
% 		\hline
% 		\textbf{Kalman (ACC) ($B_{5}$, $B_{7}$, $B_{9}$, $B_{13}$, $B_{20}$)} & AB & \textbf{90.72} & \textbf{92.17}\\%& Yes\\
% 		\hline
% 	\end{tabular}
% % 	\vspace{-3ex}
% \end{table}

\begin{figure*} [t]
    \centering
    % \captionsetup{justification=centering}
    \includegraphics[width=16cm]{Figures/wrist_2_class.png}
    \caption{Overall performance comparison of related works using LOSO validation on wrist data 2-Class. Results show that SELF-CARE outperforms the related works, branch classifiers, and other traditional late fusion methods in terms of both macro F1 and accuracy.}
    \label{fig:overall_performance_wrist_2_class}
    % \vspace{-5mm}
\end{figure*}
\subsection{Evaluation Metrics}
As stated previously, the WESAD dataset is highly imbalanced in terms of the number of segments per class. For this reason, we rely on both the F1 score and the accuracy to measure the classification performance. To ensure a fair comparison with other works, we use the macro F1 score. The metrics used for evaluation:
\begin{equation}
Accuracy = (TP+TN)/(TP+FP+TN+FN)
\end{equation}
\vspace{-5mm}
\begin{equation}
P = TP/(TP+FP), \; R = TP/(TP+FN)
\end{equation}
\vspace{-5mm}
\begin{equation}
Macro~F_1 = \frac {1} {{n_c}} \sum_{i}^{n_c} 2* \frac {P_i.R_i}{P_i+R_i}
\end{equation}
%\vspace{-3mm}
where TP, TN, FP, FN represents True Positives, True Negatives, False Positives, and False Negatives, respectively; and P and R represent Precision and Recall, respectively. The classes are indexed by \textit{i}, and $n_c$ is the number of output classes.

\begin{figure*} [t]
    \centering
    \includegraphics[width=16cm]{Figures/chest_3_class.png}
    \caption{Overall performance comparison of related works using LOSO validation on chest data 3-class. Results show that the proposed SELF-CARE method (using either ACC or EMG to determine the noise context) outperforms other related works, branch classifiers, and traditional late fusion methods in terms of macro F1 and accuracy. Moreover, SELF-CARE using EMG (for the noise context understanding) outperforms the one using ACC in both metrics. This justifies the fact that EMG is more suitable than ACC to understand the noise context in chest wearable devices.}
    \label{fig:overall_performance_chest_3_class}
    % \vspace{-5mm}
\end{figure*}

\begin{figure*} [!ht]
    \centering
    \includegraphics[width=16cm]{Figures/chest_2_class.png}
    \caption{Overall performance comparison of related works using LOSO validation on chest data 2-class. Results show that the proposed SELF-CARE method outperforms other related works, branch classifiers, and traditional late fusion methods in terms of macro F1 and accuracy. Also, SELF-CARE using EMG outperforms the one using ACC in both metrics which further justifies the fact that EMG is more suitable than ACC to understand the noise context in chest wearable devices.}
    \label{fig:overall_performance_chest_2_class}
    % \vspace{-5mm}
\end{figure*}




\subsection{Experimental Results}
This section presents the performance of SELF-CARE for stress detection in 3-class and 2-class classification using wrist and chest modalities. %We also demonstrate the energy efficiency of our approach in a ultra-low-power 32-bit microcontroller EFM32 Giant Gecko (EFM32GG-STK3700A) \cite{EFMGG} representing a wearable device. The microcontroller has an ARM Cortex-M3 processor with a maximum clock rate of 48 MHz. It has 128 KB of RAM and 1 MB of flash memory. 

% \textcolor{blue}{List specific tables we would like to show.}
% \textcolor{teal}{
% \begin{itemize}
% \item Modality Analysis table for 3 class (One of these table may go into motivation section for different combination of modalities)
% \item Modality Analysis table for 2 class (May remove if not enough space)
% \item Overall table comparing performance for 2 class and 3 class (Must)
% \end{itemize}
% }


% \subsubsection{Branch Classifiers Using Wrist Modalities}
\subsubsection{Performance Evaluation of Wrist Modalities}
\label{wrist_performance_evaluation}

Tables \ref{tab:wrist_3_class_analysis} and \ref{tab:wrist_2_class_analysis} show the performance analysis of different classifiers for various input branches for the 3-class and 2-class problems, with each branch representing different combinations of input sensors. The RF classifier for branches $WB_1$, $WB_2$, and $WB_3$ shows better or competitive performance compared to the other classifiers for both 3-class and 2-class. The RF classifiers also achieved minimum training loss for these input branches during training, leading to our selection of these three branches with the RF classifier for our approach.

As shown in Fig. \ref{fig:overall_performance_wrist_3_class}, for 3-class classification, the SELF-CARE method outperforms other related works \cite{schmidt2018introducing,huynh2021stressnas,rashid2021feature}, the branch  classifiers, and the traditional late  fusion methods in terms of both accuracy and macro F1 score achieving a performance of 86.34\% and 71.97\%, respectively. Compared to \cite{samyoun2020stress}, SELF-CARE achieves better accuracy---though \cite{samyoun2020stress} achieves a better macro F1 score, as this work uses both wrist and chest sensors for stress classification. For 2-class classification, the SELF-CARE method achieves an accuracy of 94.12\% and macro F1 score of 92.93\%, outperforming  the related works \cite{schmidt2018introducing,huynh2021stressnas,rashid2021feature,samyoun2020stress}, the  branch  classifiers, and the traditional late fusion methods in terms of both accuracy and macro F1 score (as shown in Fig. \ref{fig:overall_performance_wrist_2_class}). For the three selected branch classifiers, we apply soft- and hard-voting methods, showing performance improvements compared to the individual branch classifiers for both 3-class and 2-class classifications. SELF-CARE also uses Kalman filter-based late fusion to further improve the performance for 3-class and 2-class classification compared to these traditional late fusion methods.

% \subsubsection{Branch Classifiers Using Chest Modalities}
\subsubsection{Performance Evaluation of Chest Modalities}
\label{chest_performance_evaluation}

Tables \ref{tab:chest_3_class_analysis} and \ref{tab:chest_2_class_analysis} show the performance analysis of different classifiers for various input branches for the 3-class and 2-class problems, with each branch representing different combinations of input sensors. The AB classifier for branches $CB_1$, $CB_{12}$, $CB_{14}$, $CB_{24}$, and $CB_{27}$ shows better or competitive performance compared to the other classifiers 3-class classification. Similarly, for 2-class classification, the branches $CB_5$, $CB_7$, $CB_9$, $CB_{13}$, and $CB_{20}$ showed better performance that other classifiers. The AB classifiers also achieved minimum training loss for these input branches during training, which led to the selection of five branches for the SELF-CARE framework. The soft- and hard-voting methods applied to the five selected branch classifiers do not show performance improvements compared to the individual branch classifiers for both 3-class and 2-class classifications. However, incorporating Kalman filter-based late fusion significantly improves the performance for 3-class and 2-class classification compared to these traditional late fusion methods. 

As shown in Fig. \ref{fig:overall_performance_chest_3_class} and \ref{fig:overall_performance_chest_2_class}, for both 3-class and 2-class classification, the SELF-CARE method, using either muscle contraction (EMG) or motion (ACC) for context understanding, outperforms other related works \cite{schmidt2018introducing,huynh2021stressnas,rashid2021feature, samyoun2020stress}, the  branch  classifiers, and the traditional late  fusion methods in terms of both accuracy and macro F1 score. This study also demonstrates that even with motion-based context understanding, SELF-CARE outperforms other works. However, the model's performance improves by 2-3\% while  using  muscle contraction for context understanding compared to motion. This illustrates that the impact of movement on other sensors depends on the location of wearable devices. Therefore, movement is not always the best choice for contextual understanding as we observe the results while using chest modalities for stress detection.

% \subsubsection{Overall Performance Comparison}
% \label{overall_performance_evaluation}
% Fig. \ref{fig:overall_performance_wrist_3_class} and \ref{fig:overall_performance_wrist_2_class} show the overall performance comparison of the related works against our proposed method. Authors in \cite{schmidt2018introducing} explored different combinations of chest and wrist sensors, with and without the ACC sensor. The results for three deep learning methods are also shown \cite{samyoun2020stress,huynh2021stressnas,rashid2021feature}. 
%the combination of all wrist and all chest sensors together, all chest together, all wrist together, and the same combinations but without the ACC sensor. 
%As demonstrated in Table \ref{overall_performance}, the sensor combinations without the ACC input show better performance than combinations with the ACC input. This proves that the ACC input often leads to reduced classification performance due to the movement, and that the context of the movement is important to model before fusing the different sensor modalities. 

%And compared to traditional late fusion methods our proposed SELF-CARE architecture using \textit{Kalman filter-based} late fusion improves both the accuracy and F1 score by ~8\% for 3 class classification, and by ~4\% accuracy and ~6\% F1 score for 2 class classification. 

% Despite using only wrist signals, SELF-CARE outperforms all other state-of-the-art works that use either wrist, chest, or both sensors for 3-class and 2-class classification. Only \cite{samyoun2020stress} achieve a better macro F1 score than SELF-CARE for 3-class classification. However, they use both wrist and translated chest features, and employ a computationally expensive GAN model, which is not suitable for wrist computing.



% \subsubsection{Energy Evaluation}
% % \textcolor{blue}{List specific figures we would like to show.}
% % \begin{itemize}
% % \item \textcolor{teal}{(Must) Bar plot figure for 3 class performance and energy between late fusion and SELF-CARE (May add individual branches as well)}
% % \item \textcolor{teal}{(Must) Bar plot figure for 2 class performance and energy between late fusion and SELF-CARE (May add individual branches as well)}
% % \item \textcolor{red}{(Optional) Figure to show the change in performance and Energy for SELF-CARE for different values of delta}
% % \end{itemize}

% % \textcolor{red}{For the hardware results, we could add a picture to the board you are using if it looks nice.}
% As shown in Table \ref{overall_performance}, traditional late fusion improves the performance compared to individual branch classifiers. However, it is not energy-efficient, as multiple classifiers need to be used simultaneously --- unlike SELF-CARE that minimizes the number of classifiers selected for a given segment.
% %SELF-CARE address this problem by minimizing the number of classifiers used based on the context while improving the performance. 
% We benchmark SELF-CARE with hard-voting late fusion, which is relatively more energy-efficient than soft-voting, and shows similar performance to soft-voting. 
% %The RF branch classifiers are implemented with maxsplit of 100 for each decision trees. %As our proposed \textit{Kalman filter-based} late fusion outperforms other late fusion methods and other related works, we benchmark the performance of SELF-CARE with this late fusion method against the individual branch classifiers. 
% As shown in Fig. \ref{3_class_benchmark} for 3-class classification, SELF-CARE with $\delta=0.4$ improves up to $\sim$8\% accuracy and $\sim$8\% F-1 score, while being 2.2$\times$ energy-efficient compared to hard-voting. Similarly, for 2-class classification, SELF-CARE with $\delta=0.1$ outperforms hard-voting by up to $\sim$4\% accuracy and $\sim$6\% F-1 score while being 2.7$\times$ energy-efficient. The higher energy efficiency for 2-class can be partially attributed to the lower $\delta=0.1$, which reduces the use of multiple branches compared to $\delta=0.4$ for 3-class.
% The higher $\delta$ for 3-class is chosen to prioritize performance over energy, as the 3-class problem is inherently more challenging than the 2-class problem.
% %enfores less multi-branch selections 

%compared to late fusion approach.


% \begin{figure}[t]
% 		\centering
% 		\includegraphics[trim={9cm 15.74cm 11.5cm 5.7cm},clip, width=\linewidth]{Figures/3_Class_ACC_F1_Energy_7.pdf}
% 		\caption{Benchmarking on 3-Class Classification}
% 		\vspace{-3mm}
% 		\label{3_class_benchmark}
% \end{figure}

% % \begin{figure}[t]
% % 		\centering
% % 		\includegraphics[, width=\linewidth]{Figures/3_Class_ACC_F1_Energy_5.png}
% % 		\caption{Benchmarking on 3 Class Classification}
% % 		% \vspace{-5mm}
% % 		\label{3_class_benchmark}
% % \end{figure}

% \begin{figure}[t]
% 		\centering
% 		\includegraphics[trim={9cm 15.74cm 11.5cm 5.7cm},clip, width=\linewidth]{Figures/2_Class_ACC_F1_Energy_7.pdf}
% 		\caption{Benchmarking on 2-Class Classification}
% 		\vspace{-5mm}
% 		\label{2_class_benchmark}
% \end{figure}

	
% \subsection{Discussion}
% \vspace{-1mm}

\section{Limitations and Future Directions}
\label{sec:disc}
One of the main goals of this paper is to explore how the context of noise varies depending on the location of  wearable devices. % such as  chest-worn and wrist-worn as used in the paper. 
For this reason, we modeled the noise context of sensor modalities from stand-alone devices, choosing not to combine the wrist and chest sensor modalities.
%That is why we used the sensor modalities from stand alone devices for  stress detection. 
%We intentionally did not combine the wrist and chest sensor modalities for stress detection as it is contradictory to  our goal of understanding the noise context based on the device location. 
%However, this is definitely one of the future directions that we would like to pursue and encourage the research community to ponder upon.
However, future research could explore this issue further.
%this is a future research direction that warrants further exploration
Understanding the relation between the noise context of multiple wearable devices from physically different locations and fusing cross modal sensors based on that relation may produce interesting scientific findings that can be leveraged for methods of affective computing. %be an interesting idea to explore.
Modeling the noise context of wearable health sensors can lead to further levels of human emotion understanding as information from the health sensors becomes increasingly useful when interpreted on a contextual basis.

Further, SELF-CARE is limited by the manual design of sensor fusion branch configurations. Though domain knowledge is required to determine which sensor data to fuse together, future works could explore using machine learning to make these determinations instead. Additionally, the energy efficiency of wearable health devices is an important constraint that could be examined in future works. SELF-CARE implements a configurable parameter for balancing computation and performance, but future works could examine the efficiency trade-offs between chest and wrist sensing modalities. This paper did not focus on deep learning models, but SELF-CARE's modular design allows for the implementation of any learning-based classifier, including deep learning branches. Further, SELF-CARE could be applied more broadly in the domain of affective computing to include additional tasks beyond stress detection and emotion recognition.
Moreover, it can also be applied to other wearable healthcare applications like human activity recognition \cite{rashid2022ahar, odema2021eexnas, rashid2021hear}, myocardial infarction detection \cite{rashid2020energy, odema2021energy, rashid2022template} etc., that involves data from multiple wearable sensors. Lastly, SELF-CARE's use of a specialized set of ensemble classifiers has broad applicability to IoT sensing, including the domains of sensor networks \cite{yasaei2020iot}, and transportation \cite{malawade2021sage}.

%\blue{Add more research directions here. Possible topics include: focus on energy efficiency and edge computing performance (can cite some deep learning works here also), the inclusion of tasks in addition to stress detection/emotion recognition: multi-task problems because we have a number of sensors to leverage data from, some sentences on how to extend our approach to increase affective computing in general, limitations of domain knowledge in defining and designing the branches - could explore learned approaches.}

\section{Conclusion}
\label{sec:con}
% In this paper we proposed SELF-CARE, a selective sensor fusion approach that uses context-aware, energy-efficient edge computing to perform stress detection. SELF-CARE models context as the motion of a subject and performs an intelligent gating mechanism to select which sensor fusion schema to use given a certain input. 
%SELF-CARE with our novel Kalman-filter-based late fusion achieving the highest results. 
% To the best of our knowledge, SELF-CARE achieves state-of-the-art performance on the WESAD dataset in terms of 3-class classification (\textbf{86.34\%}) and 2-class classification (\textbf{94.12\%}) in approaches that use LOSO validation. Furthermore, SELF-CARE achieves upto \textbf{2.2$\times$} (3-class) and \textbf{2.7$\times$} (2-class) energy efficiency with respect to comparable late fusion methods. 

% In this paper, we extend the \textsc{SELF-CARE} method for stress detection proposed in our previous work \cite{rashid2022self} and generalize it for both chest and wrist wearable devices. SELF-CARE is a selective sensor fusion approach for stress detection that models context using muscle contractions (EMG) or motion (ACC) of a subject and performs an intelligent gating mechanism to select which sensor fusion schema to use given a certain input. Moreover, we also show that while motion-based context selection works best for wrist-based wearable devices, it is not the best for chest-based wearable devices. Through experimental evaluation, we demonstrate that EMG is better than ACC in understanding the context of chest-based wearable devices. To the best of our knowledge, SELF-CARE achieves state-of-the-art performance on the WESAD dataset for both chest and wrist-based sensors. Using wrist-based sensors our methodology achieves 86.34\% (3-class) and 94.12\% (2-class) classification accuracy while outperforming state-of-the-art works. Similarly, for chest-based wearable sensors, our methodology outperforms  state-of-the-art works with 86.19\% (3-class) and 93.68\% (2-class) classification accuracy.

In this paper, we propose SELF-CARE, a generalized selective sensor fusion method for stress detection that utilizes the noise context in the chest- and wrist-worn devices to dynamically adjust the sensor fusion performed to maximize classification performance. SELF-CARE determines the noise context using muscle contractions (EMG) or motion (ACC) of a subject, and performs an intelligent gating mechanism to select which sensor fusion schema to use depending on the location of the sensor.%given a certain input. 
 We also show that, while determining the noise context based on motion works best for wrist-based wearable devices, it is not the best for chest-based wearable devices. Through experimental evaluation, we conclude that EMG is better than ACC in understanding the noise context of chest-based wearable devices. To the best of our knowledge, SELF-CARE achieves state-of-the-art performance on the WESAD dataset for both chest and wrist-based sensors among methods that use LOSO validation. Using wrist-based sensors our methodology achieves 86.34\% (3-class) and 94.12\% (2-class) classification accuracy while outperforming current state-of-the-art works. Similarly, for chest-based wearable sensors, our methodology outperforms existing models with 86.19\% (3-class) and 93.68\% (2-class) classification accuracy. 

% \vspace{-1mm}
% \section*{Acknowledgment}
% % \vspace{-1mm}
% This work was partially supported by the National Science Foundation (NSF) under awards CMMI-1739503 and CCF-2140154. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.
% \vspace{-1mm}
% \section{References}

\balance
\bibliographystyle{IEEEtran}
\bibliography{bibliography}


\end{document}
