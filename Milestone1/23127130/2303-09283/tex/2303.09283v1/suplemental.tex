\onecolumn

\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thetable}{\roman{table}}
\renewcommand{\thefigure}{\Alph{figure}}

\setcounter{section}{0}
\setcounter{table}{0}
\setcounter{figure}{0}


\section{Supplemental material}

\subsection{Training parameters for heterogeneous architectures}

Table~\ref{tab:tParams} shows the architecture and optimization hyper-parameters used for training the  models used in our experiments.

\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
id & Architecture & Optimizer & Parameters & Scheduler & Epochs &
BatchSize\\
\hline
ResNext50\_32\_2&ResNext50 cardinality=32, blockWidth=2 layers=[3;4;6;3], dropout=0.2 & SGD & lr=0.1 decay=0.0001, momentum=0.9 & Step: $\gamma$=0.1, epochs=30 & 100 & 32\\
\hline
ResNext50\_32\_4l&ResNext50 cardinality=32, blockWidth=4 layers=[2;2;3;2], dropout=0.2 & SGD & lr=0.1 decay=0.0001, momentum=0.9 & Step: $\gamma$=0.1, epochs=30 & 100 & 32\\
\hline
ResNext50\_16\_4&ResNext50 cardinality=16, blockWidth=4 layers=[3;4;6;3], dropout=0.2 & SGD & lr=0.1 decay=0.0001, momentum=0.9& Step: $\gamma$=0.1, epochs=30& 100 & 32\\
\hline
MNASNET\_1&MNASNET ratio=1, dropout=0.2 & SGD & lr=0.1 decay=0.0001, momentum=0.9& Step: $\gamma$=0.1, epochs=30& 100 & 32\\
\hline
MNASNET\_1p&MNASNET ratio=1, dropout=0.2 & RMSprop & lr=0.256, decay=0.9, momentum=0.9& Step: $\gamma$=0.97, epochs=2.4& 100 & 32\\
\hline
Squeezenet\_512 & Squeezenet version=1.1 & SGD & lr=0.01 decay=0.0002, momentum=0.9& Step: $\gamma$=0.1, epochs=30& 100 & 512\\
\hline
Squeezenet & Squeezenet version=1.1 & SGD & lr=0.01 decay=0.0002, momentum=0.9& Step: $\gamma$=0.1, epochs=30& 100 & 128\\
\hline
\multirow{2}{*}{bootstrapNAS-B\_0} & ResNet50 depth=[0, 0, 0, 0, 1], width=[0, 0, 0, 2, 2, 2]&\multirow{2}{*}{SGD} &\multirow{2}{*}{See Listing~\ref{lst:progShr}} &\multirow{2}{*}- &\multirow{2}{*}-&\multirow{2}{*}-\\
& expansion=[0.2, 0.2, 0.2, 0.25, 0.2, 0.25, 0.25, 0.25,0.2, 0.25, 0.25, 0.25] & & & & &\\
\hline
\multirow{2}{*}{bootstrapNAS-B\_1} & ResNet50 d=[0, 0, 0, 0, 1], w=[0, 0, 0, 2, 2, 2]&\multirow{2}{*}{SGD} &\multirow{2}{*}{See Listing~\ref{lst:progShr}} &\multirow{2}{*}- &\multirow{2}{*}- & \multirow{2}{*}-\\
& expansion=[0.25, 0.2, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25] & & & & &\\
\hline
deit\_tiny\_p16\_224 & DeiT size=tiny(3 heads), patchSize=16, embedding=192& AdamW & lr=0.0005 & Cosine & 300 & 256\\
\hline
\multirow{2}{*}{dino\_deit\_tiny1} & DINO DeiT size=tiny, patchSize=16, embedding=192, & \multirow{2}{*}{AdamW} & \multirow{2}{*}{lr=0.0005} & \multirow{2}{*}{Cosine} & Backbone=300 & \multirow{2}{*}{256}\\
&seed=0, localCropsScale=\{0.05,0.2\}, globalCropsScale=\{0.4, 1.\}&&&& Classifier=100&\\
\hline
\multirow{2}{*}{dino\_deit\_tiny2} & DINO DeiT size=tiny, patchSize=16, embedding=192, & \multirow{2}{*}{AdamW} & \multirow{2}{*}{lr=0.0005} & \multirow{2}{*}{Cosine} & Backbone=300 & \multirow{2}{*}{256}\\
&seed=7, localCropsScale=\{0.05,0.2\}, globalCropsScale=\{0.4, 1.\}&&&& Classifier=100&\\
\hline
\multirow{2}{*}{dino\_resnet1} & DINO ResNet50 backboneSeed=7 localCropsScale=\{0.05,0.14\} & \multirow{2}{*}{SGD} & \multirow{2}{*}{decay=0.0001, lr=0.03} & \multirow{2}{*}{-} & Backbone=300 & \multirow{2}{*}{256}\\
&globalCropsScale=\{0.14, 1.\}, classifierSeed=5&&&& Classifier=100&\\
\hline
\multirow{2}{*}{dino\_resnet2} & DINO ResNet50 backboneSeed=7 localCropsScale=\{0.05,0.14\} & \multirow{2}{*}{SGD} & \multirow{2}{*}{decay=0.0001, lr=0.03} & \multirow{2}{*}{-} & Backbone=300 & \multirow{2}{*}{256}\\
&globalCropsScale=\{0.14, 1.\}, classifierSeed=15&&&& Classifier=100&\\
\hline
\end{tabular}
            }
\caption{Training architectures and parameters used to create ensembles of heterogeneous architectures}
\label{tab:tParams}
\end{table}

Listing~\ref{lst:progShr} shows an example configuration file to create a super-network using a pre-trained model from Torchvision.
The configuration parameters are used by BootstrapNAS in the Neural Network Compression Framework (NNCF) and specify the size and elasticity of the super-network.  Once the super-network has been trained, the user can extract models of different sizes and performances.

{\setstretch{0.5}\tiny
\begin{lstlisting}[label=lst:progShr, linewidth=\columnwidth, language=json, caption=Super-network configuration example for NNCF's BootstrapNAS]
# Insert here model and dataset fields
# Insert here optimizer fields
"bootstrapNAS":{
    "training": {
        "algorithm":"progressive_shrinking",   
        "progressivity_of_elasticity": ["depth", "width"], 
        "batchnorm_adaptation": {
            "num_bn_adaptation_samples": 1500},
    "schedule": { 
        "list_stage_descriptions": [
        {"train_dims": ["depth"], "epochs": 25, 
        "depth_indicator": 1, "init_lr": 2.5e-6, 
        "epochs_lr": 25},
        {"train_dims": ["depth"], "epochs": 40, "depth_indicator": 2, "init_lr": 2.5e-6, "epochs_lr": 40},
        {"train_dims": ["depth", "width"], "epochs": 50, "depth_indicator": 2, "reorg_weights": true, "width_indicator": 2, "bn_adapt": true, "init_lr": 2.5e-6, "epochs_lr": 50},
        {"train_dims": ["depth", "width"], "epochs": 50, "depth_indicator": 2, "reorg_weights": true, "width_indicator": 3, "bn_adapt": true, "init_lr": 2.5e-6, "epochs_lr": 50}
        ]
    }, 
    "elasticity": {            
        "available_elasticity_dims": ["width", "depth"],
        "width": {
        "max_num_widths": 3,
        "min_width": 32,
        "width_step": 32, 
        "width_multipliers": [1, 0.80, 0.60]
        }
    }    
},
    "search": {
        "algorithm": "NSGA2",
        "num_evals": 1000,
        "population": 50,
        "ref_acc": 93.65
    }
}
\end{lstlisting}
}

The parameters shown in Table~\ref{tab:nasParam} indicate the configuration of the subnetworks extracted from the super-network in our
experiments. We used a previous version of BootstrapNAS in our experiments, which extends Once-for-all (OFA) super-networks from Cai et al. [4] and follows its conventions to describe the search space. In newer versions of BootstrapNAS, expansion ratios are handled by an elastic width handler, and the search space description follows a different convention. 

\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|}
\hline
id & Subnetwork Configurations \\\hline
B\_0 & depth: [0, 0, 0, 0, 1], expansion: [0.2, 0.2, 0.2, 0.25, 0.2, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25], width: [0, 0, 0, 2, 2, 2], \\\hline
nB\_0 & depth: [0, 0, 0, 0, 1], expansion: [0.2, 0.2, 0.2, 0.25, 0.2, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25], width : [0, 0, 0, 2, 2, 2]\\\hline
nB\_1 & depth: [0, 0, 0, 0, 1], expansion:  [0.25, 0.2, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25], width : [0, 0, 0, 2, 2, 2]\\\hline
nB\_2 & depth: [0, 0, 0, 0, 1], expansion:  [0.25, 0.2, 0.25, 0.2, 0.2, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25], width : [0, 0, 0, 2, 2, 2]\\\hline
nB\_3 & depth: [0, 0, 0, 0, 1], expansion:  [0.25, 0.2, 0.25, 0.25, 0.2, 0.2, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25], width : [0, 0, 0, 2, 2, 2]\\\hline
dB\_1a & depth: [0, 0, 1, 0, 0], expansion: [0.25, 0.2, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25], width : [0, 0, 0, 2, 2, 2]\\\hline
dB\_1b & depth: [1, 0, 0, 0, 0], expansion: [0.25, 0.2, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25], width : [0, 0, 0, 2, 2, 2],\\\hline
wB\_1a & depth: [0, 0, 0, 0, 1], expansion: [0.25, 0.2, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25], width : [0, 0, 1, 1, 2, 2],\\\hline
wB\_1b & depth: [0, 0, 0, 0, 1], expansion: [0.25, 0.2, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25], width : [0, 1, 1, 1, 1, 2],\\\hline
wB\_1c & depth: [0, 0, 0, 0, 1], expansion: [0.25, 0.2, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25, 0.2, 0.25, 0.25, 0.25], width : [1, 1, 1, 1, 1, 1],\\\hline
    \end{tabular}
    }
    \caption{Configuration of subnetworks extracted for the creation of ensembles of heterogeneous architectures}
    \label{tab:nasParam}
\end{table}


Table~\ref{tab:trainingTrans} shows the transformations used during training for all individual models.

    \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|}
            \hline
            Transform & Parameters\\
             \hline
             RandomResizedCrop & size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear \\
             \hline
             RandomHorizontalFlip & p=0.5 \\
             \hline
             Normalize & mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\\
             \hline
        \end{tabular}
        \caption{Training data set transforms used for the training of all models}
        \label{tab:trainingTrans}
    \end{table}

Figure~\ref{tab:indAccNAS} shows the final top1 accuracy scores of each model defined by Table~\ref{tab:tParams}.

\begin{figure}[htbp]
    \begin{subtable}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{|c|c|}
            \hline
            Model id & Top 1\% accuracy \\
            \hline
            ResNext50\_32\_2 & 75.198  \\
            ResNext50\_32\_4l & 75.072 \\
            ResNext50\_16\_4 & 75.432 \\
            MNASNET\_1 & 54.408 \\
            MNASNET\_1p & 54.0 \\
            Squeezenet\_512 & 41.918 \\
            Squeezenet & 56.558 \\
            bootstrapNAS-B\_0 & 76.342 \\
            bootstrapNAS-B\_1 & 76.282 \\
            deit\_tiny\_p16\_224 & 71.654 \\
            dino\_deit\_tiny1 & 67.932 \\
            dino\_deit\_tiny2 & 67.52 \\
            dino\_resnet1 & 67.236 \\
            dino\_resnet2 & 67.216 \\
            \hline
        \end{tabular}
    \end{subtable}
    \quad
    \begin{subtable}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{|c|c|}
            \hline
            Model id & Top 1\% accuracy \\
            \hline
            B\_0 & 76.342 \\
            B\_1 & 76.282 \\
            nB\_0 & 76.301\\
            nB\_1 & 76.318\\
            nB\_2 & 76.191\\
            nB\_3 & 76.142\\
            dB\_1a & 76.138\\
            dB\_1b & 76.084\\
            wB\_1a & 76.170\\
            wB\_1b & 75.804\\
            wB\_1c & 75.852\\
            \hline
        \end{tabular}
        
    \end{subtable}
    \caption{Individual accuracies of individual trained models on ImageNet validation dataset}
        \label{tab:indAccNAS}
\end{figure}

\newpage

\subsection{Comparison of prediction-based disagreement, input attribution diversity and average accuracy metrics in ensembles of heterogeneous architectures}

    \begin{figure}[htp]
        \subcaptionbox{\centering Attr. ImageNetv2\label{figB:odda}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_linear_combination_DIFF_imagenetv2.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Attr. Waterdrop\label{figB:oddb}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_linear_combination_DIFF_waterdrop_7.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Attr. Lines\label{figB:oddc}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_linear_combination_DIFF_lines.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Attr. Plasma\label{figB:oddd}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_linear_combination_DIFF_plasma_noise.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Attr. Checkerb.\label{figB:odde}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_linear_combination_DIFF_checkboard.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Disagr. ImageNetv2\label{figB:oddf}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_linear_combination_OUT_DIS_imagenetv2.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Disagr. Waterdrop\label{figB:oddg}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_linear_combination_OUT_DIS_waterdrop_7.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Disagr. Lines\label{figB:oddh}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_linear_combination_OUT_DIS_lines.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Disagr. Plasma\label{figB:oddi}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_linear_combination_OUT_DIS_plasma_noise.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Disagr. Checkerb.\label{figB:oddj}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_linear_combination_OUT_DIS_checkboard.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Acc. ImageNetv2\label{figB:oddk}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_justAcc_imagenetv2.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Acc. Waterdrop\label{figB:oddl}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_justAcc_waterdrop_7.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Acc. Lines\label{figB:oddm}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_justAcc_lines.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Acc. Plasma\label{figB:oddn}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_justAcc_plasma_noise.eps}}%
        \hspace{0em}%
        \subcaptionbox{\centering Acc. Checkerb.\label{figB:oddo}}{\includegraphics[width=0.2\linewidth]{figures/fig_hetero3_exp_justAcc_checkboard.eps}}
        \caption{Comparison of \textbf{improvement correlation of three different metrics on five validation datasets} using averaging as consensus mechanism. Columns: Datasets. Rows: Metrics.}
        \label{figB:supHeteroODD}
        \end{figure}

