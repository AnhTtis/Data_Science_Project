% Encoding: UTF-8

@Article{Davies1982,
  author    = {Mark Davies and Joseph L. Fleiss},
  title     = {Measuring Agreement for Multinomial Data},
  journal   = {Biometrics},
  year      = {1982},
  volume    = {38},
  number    = {4},
  pages     = {1047--1051},
  month     = {dec},
  issn      = {0006341X, 15410420},
  abstract  = {A kappa-like statistic is proposed as a measure of agreement for a set of multinomial random variables arrayed in a two-way laytout. This statistic is shown to arise either via the chance-correction of a certain proportion of agreement or via an analysis of variance for a two-way layout. The asymptotic standard error is derived under the assymption of complete independence among the random variables.},
  doi       = {10.2307/2529886},
  publisher = {[Wiley, International Biometric Society]},
  urldate   = {2022-07-19},
}

@Article{cohen1,
  author  = {Jacob Cohen},
  title   = {A Coefficient of Agreement for Nominal Scales},
  journal = {Educational and Psychological Measurement},
  year    = {1960},
  volume  = {20},
  number  = {1},
  pages   = {37-46},
  doi     = {10.1177/001316446002000104},
  eprint  = {https://doi.org/10.1177/001316446002000104},
}

@Article{Conger1980,
  author    = {Anthony J. Conger},
  title     = {Integration and generalization of kappas for multiple raters.},
  journal   = {Psychological Bulletin},
  year      = {1980},
  volume    = {88},
  number    = {2},
  pages     = {322--328},
  month     = {sep},
  doi       = {10.1037/0033-2909.88.2.322},
  publisher = {American Psychological Association ({APA})},
}

@Article{fleiss1971measuring,
  author    = {Fleiss, Joseph L},
  title     = {Measuring nominal scale agreement among many raters.},
  journal   = {Psychological bulletin},
  year      = {1971},
  volume    = {76},
  number    = {5},
  pages     = {378},
  doi       = {10.1037/h0031619},
  publisher = {American Psychological Association},
}

@Book{gwet,
  title     = {Handbook of inter-rater reliability: The definitive guide to measuring the extent of agreement among raters},
  publisher = {Advanced Analytics, LLC},
  year      = {2012},
  author    = {Gwet, Kilem Li},
  month     = {01},
}

@InCollection{Osgood1959,
  author    = {Charles Egerton Osgood},
  title     = {The Representational Model and Relevant Research Methods},
  booktitle = {Trends in content analysis},
  publisher = {Urbana: University of Illinois Press},
  year      = {1959},
  editor    = {I. Pool},
  pages     = {33,38},
}

@Article{BENNETT1954,
  author   = {Bennett, E. and Alpert, R. and Goldstein, A.C.},
  title    = {{Communications Through Limited-Response Questioning*}},
  journal  = {Public Opinion Quarterly},
  year     = {1954},
  volume   = {18},
  number   = {3},
  pages    = {303,308},
  issn     = {0033-362X},
  abstract = {{The extent of consistency between information from two methods of communication, the interview and the limited-response question, was investigated. Thirty questions showed consistencies greater than could be expected on the basis of chance. The questions were classified into four general categories, and the mean coefficients of consistency for these categories ranged from 0.46 to 1.00.}},
  doi      = {10.1086/266520},
  eprint   = {https://academic.oup.com/poq/article-pdf/18/3/303/5384778/18-3-303.pdf},
}

@Article{Gwet2008,
  author             = {Gwet, Kilem Li},
  title              = {Computing inter-rater reliability and its variance in the presence of high agreement.},
  journal            = {The British journal of mathematical and statistical psychology},
  year               = {2008},
  volume             = {61},
  pages              = {29-48},
  abstract           = {Pi (pi) and kappa (kappa) statistics are widely used in the areas of psychiatry and psychological testing to compute the extent of agreement between raters on nominally scaled data. It is a fact that these coefficients occasionally yield unexpected results in situations known as the paradoxes of kappa. This paper explores the origin of these limitations, and introduces an alternative and more stable agreement coefficient referred to as the AC1 coefficient. Also proposed are new variance estimators for the multiple-rater generalized pi and AC1 statistics, whose validity does not depend upon the hypothesis of independence between raters. This is an improvement over existing alternative variances, which depend on the independence assumption. A Monte-Carlo simulation study demonstrates the validity of these variance estimators for confidence interval construction, and confirms the value of AC1 as an improved alternative to existing inter-rater reliability statistics.},
  address            = {England},
  article-doi        = {10.1348/000711006X126600},
  completed          = {20080822},
  doi                = {10.1348/000711006X126600},
  history            = {2008/05/17 09:00 [entrez]},
  issue              = {Pt 1},
  keywords           = {Analysis of Variance, Humans, Monte Carlo Method, *Observer Variation, Probability, Psychological Tests/*statistics & numerical data, Psychometrics/*statistics & numerical data, Reproducibility of Results},
  language           = {eng},
  linking-issn       = {0007-1102},
  location-id        = {10.1348/000711006X126600 [doi]},
  nlm-unique-id      = {0004047},
  owner              = {NLM},
  print-issn         = {0007-1102},
  publication-status = {ppublish},
  revised            = {20220409},
  source             = {Br J Math Stat Psychol. 2008 May;61(Pt 1):29-48. doi: 10.1348/000711006X126600.},
  status             = {MEDLINE},
  subset             = {IM},
  title-abbreviation = {Br J Math Stat Psychol},
}

@Article{FEINSTEIN1990543,
  author   = {Alvan R. Feinstein and Domenic V. Cicchetti},
  title    = {High agreement but low Kappa: I. the problems of two paradoxes},
  journal  = {Journal of Clinical Epidemiology},
  year     = {1990},
  volume   = {43},
  number   = {6},
  pages    = {543-549},
  issn     = {0895-4356},
  abstract = {In a fourfold table showing binary agreement of two observers, the observed proportion of agreement, P0 can be paradoxically altered by the chance-corrected ratio that creates κ as an index of concordance. In one paradox, a high value of P0 can be drastically lowered by a substantial imbalance in the table's marginal totals either vertically or horizontally. In the second pardox, κ will be higher with an asymmetrical rather than symmetrical imbalance in marginal totals, and with imperfect rather than perfect symmetry in the imbalance. An adjustment that substitutes Kmax for κ does not repair either problem, and seems to make the second one worse.},
  doi      = {10.1016/0895-4356(90)90158-L},
  keywords = {Kappa, Concordance, Agreement, Paradox},
}

@Article{Kraemer2002,
  author    = {Helena Chmura Kraemer and Vyjeyanthi S. Periyakoil and Art Noda},
  title     = {Kappa coefficients in medical research},
  journal   = {Statistics in Medicine},
  year      = {2002},
  volume    = {21},
  number    = {14},
  pages     = {2109--2129},
  doi       = {10.1002/sim.1180},
  publisher = {Wiley},
}

@Article{Little,
  author    = {Roderick J. A. Little},
  title     = {A Test of Missing Completely at Random for Multivariate Data with Missing Values},
  journal   = {Journal of the American Statistical Association},
  year      = {1988},
  volume    = {83},
  number    = {404},
  pages     = {1198-1202},
  doi       = {10.1080/01621459.1988.10478722},
  eprint    = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478722},
  publisher = {Taylor \& Francis},
}

@Unpublished{moons1,
  author  = {Filip Moons and Jozef Colpaert and Ellen Vandervieren},
  title   = {Checkbox grading: a semi-automated way assess and give atomic feedback to handwritten mathematics exams with multiple teachers? A study into time investment, reliability and teachers’ use and perceivement.},
  note    = {Will be send to Educational Studies in Mathematics},
  year    = {in preparation},
  journal = {Educational Studies in Mathematics},
}

@Article{MEZZICH198129,
  author  = {Juan E. Mezzich and Helena C. Kraemer and David R.L. Worthington and Gerald A. Coffman},
  title   = {Assessment of agreement among several raters formulating multiple diagnoses},
  journal = {Journal of Psychiatric Research},
  year    = {1981},
  volume  = {16},
  number  = {1},
  pages   = {29-39},
  issn    = {0022-3956},
  doi     = {10.1016/0022-3956(81)90011-X},
}

@Book{Association2022,
  title     = {Diagnostic and Statistical Manual of Mental Disorders},
  publisher = {American Psychiatric Association Publishing},
  year      = {2022},
  author    = {{American Psychiatric Association}},
  month     = {mar},
  doi       = {10.1176/appi.books.9780890425787},
}

@Article{kraemer_extend,
  author    = {Helena Chmura Kraemer},
  title     = {Extension of the Kappa Coefficient},
  journal   = {Biometrics},
  year      = {1980},
  volume    = {36},
  number    = {2},
  pages     = {207--216},
  issn      = {0006341X, 15410420},
  abstract  = {An extension of the kappa coefficient is proposed which is appropriate for use with multiple observations per subject (not necessarily an equal number) and for multiple response choices per observation. Computational methods and nonasymptotic, nonnull distribution theory are discussed. The proposed method is applied to previously published data, not only to compare results with those in earlier methods, but to illustrate new approaches to difficult problems in evaluation of reliability. The effect of using an 'Other' response category is examined. Strategies to enhance reliability are evaluated, including empirical investigation of the Spearman-Brown formula as applied to nominal response measures.},
  doi       = {10.2307/2529972},
  publisher = {[Wiley, International Biometric Society]},
  urldate   = {2022-11-02},
}

@TechReport{nvivo,
  author      = {NVivo},
  title       = {Run a Coding Comparison query},
  institution = {NVivo 11},
  year        = {2022},
  url         = {https://help-nv11.qsrinternational.com/desktop/procedures/run_a_coding_comparison_query.htm},
}

@Article{Vries2008,
  author    = {De Vries, Han and Marc N. Elliott and David E. Kanouse and Stephanie S. Teleki},
  title     = {Using Pooled Kappa to Summarize Interrater Agreement across Many Items},
  journal   = {Field Methods},
  year      = {2008},
  volume    = {20},
  number    = {3},
  pages     = {272--282},
  month     = {mar},
  doi       = {10.1177/1525822x08317166},
  publisher = {{SAGE} Publications},
}

@Article{Koo2016,
  author    = {Terry K. Koo and Mae Y. Li},
  title     = {A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research},
  journal   = {Journal of Chiropractic Medicine},
  year      = {2016},
  volume    = {15},
  number    = {2},
  pages     = {155--163},
  month     = {jun},
  doi       = {10.1016/j.jcm.2016.02.012},
  publisher = {Elsevier {BV}},
}

@Article{Warrens2010,
  author    = {Matthijs J. Warrens},
  title     = {A Formal Proof of a Paradox Associated with Cohen's Kappa},
  journal   = {Journal of Classification},
  year      = {2010},
  volume    = {27},
  number    = {3},
  pages     = {322--332},
  month     = {oct},
  doi       = {10.1007/s00357-010-9060-x},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{subjective,
  author   = {Vanacore, Amalia and Pellegrino, Maria Sole},
  title    = {Benchmarking procedures for characterizing the extent of rater agreement: a comparative study},
  journal  = {Quality and Reliability Engineering International},
  year     = {2022},
  volume   = {38},
  number   = {3},
  pages    = {1404-1415},
  abstract = {Abstract Decision making processes often rely on subjective evaluations provided by human raters. In the absence of a gold standard against which check evaluation trueness, rater's evaluative performance is generally measured through rater agreement coefficients. In this study some parametric and non-parametric inferential benchmarking procedures for characterizing the extent of rater agreement—assessed via kappa-type agreement coefficients—are illustrated. A Monte Carlo simulation study has been conducted to compare the performance of each procedure in terms of weighted misclassification rate computed for all agreement categories. Moreover, in order to investigate whether the procedures overestimate or underestimate the level of agreement, misclassifications have been computed also for each specific category alone. The practical application of coefficients and inferential benchmarking procedures has been illustrated via two real data sets exemplifying different experimental conditions so as to highlight performance differences due to sample size.},
  doi      = {10.1002/qre.2982},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.2982},
  keywords = {benchmarking procedures, κ$\kappa$-type agreement coefficients, misclassification rate, Monte Carlo simulation},
}

@Article{scott1955reliability,
  author    = {Scott, William A},
  title     = {Reliability of content analysis: The case of nominal scale coding},
  journal   = {Public opinion quarterly},
  year      = {1955},
  pages     = {321--325},
  doi       = {10.1086/266577},
  publisher = {JSTOR},
}

@Article{McHugh2012,
  author             = {McHugh, Mary L.},
  title              = {Interrater reliability: the kappa statistic.},
  journal            = {Biochemia medica},
  year               = {2012},
  volume             = {22},
  pages              = {276-82},
  __markedentry      = {[mfili:]},
  abstract           = {The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen's kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from -1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen's suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.},
  address            = {Croatia},
  article-pii        = {biochem_med-22-3-276-4},
  completed          = {20121127},
  doi                = {10.11613/BM.2012.031},
  electronic-issn    = {1846-7482},
  history            = {2012/12/10 06:00 [medline]},
  issue              = {3},
  keywords           = {*Data Interpretation, Statistical, Observer Variation, Reproducibility of Results},
  language           = {eng},
  linking-issn       = {1330-0962},
  nlm-unique-id      = {9610305},
  owner              = {NLM},
  print-issn         = {1330-0962},
  publication-status = {ppublish},
  revised            = {20221017},
  source             = {Biochem Med (Zagreb). 2012;22(3):276-82.},
  status             = {MEDLINE},
  subset             = {IM},
  title-abbreviation = {Biochem Med (Zagreb)},
}

@Article{McHugh_2012,
  author    = {Marry L. McHugh},
  title     = {Interrater reliability: the kappa statistic},
  journal   = {Biochemia Medica},
  year      = {2012},
  pages     = {276--282},
  doi       = {10.11613/bm.2012.031},
  publisher = {Croatian Society for Medical Biochemistry and Laboratory Medicine},
}

@InProceedings{moons:hal-03753446,
  author      = {Moons, Filip and Vandervieren, Ellen},
  title       = {{Handwritten math exams with multiple assessors: researching the added value of semi-automated assessment with atomic feedback}},
  booktitle   = {{Twelfth Congress of the European Society for Research in Mathematics Education (CERME12)}},
  year        = {2022},
  volume      = {TWG21},
  number      = {14},
  series      = {Twelfth Congress of the European Society for Research in Mathematics Education (CERME12)},
  address     = {Bozen-Bolzano, Italy},
  hal_id      = {hal-03753446},
  hal_version = {v1},
  keywords    = {Assessment ; computer-assisted assessment ; state examinations ; feedback ; inter-rater reliability ; Assessment},
  pdf         = {https://hal.science/hal-03753446/file/TWG21_14_Moons.pdf},
  url         = {https://hal.science/hal-03753446},
}

@Article{doi:10.1177/0013164420973080,
  author   = {Gwet, Kilem Li},
  title    = {Large-Sample Variance of Fleiss Generalized Kappa},
  journal  = {Educational and Psychological Measurement},
  year     = {2021},
  volume   = {81},
  number   = {4},
  pages    = {781-790},
  abstract = { Cohen’s kappa coefficient was originally proposed for two raters only, and it later extended to an arbitrarily large number of raters to become what is known as Fleiss’ generalized kappa. Fleiss’ generalized kappa and its large-sample variance are still widely used by researchers and were implemented in several software packages, including, among others, SPSS and the R package “rel.” The purpose of this article is to show that the large-sample variance of Fleiss’ generalized kappa is systematically being misused, is invalid as a precision measure for kappa, and cannot be used for constructing confidence intervals. A general-purpose variance expression is proposed, which can be used in any statistical inference procedure. A Monte-Carlo experiment is presented, showing the validity of the new variance estimation procedure. },
  doi      = {10.1177/0013164420973080},
  eprint   = {https://doi.org/10.1177/0013164420973080},
}

@Article{Fleiss1979,
  author    = {Joseph L. Fleiss and John C. Nee and J. Richard Landis},
  title     = {Large sample variance of kappa in the case of different sets of raters.},
  journal   = {Psychological Bulletin},
  year      = {1979},
  volume    = {86},
  number    = {5},
  pages     = {974--977},
  month     = {sep},
  doi       = {10.1037/0033-2909.86.5.974},
  publisher = {American Psychological Association ({APA})},
}

@Article{MOONS2022100086,
  author   = {Filip Moons and Ellen Vandervieren and Jozef Colpaert},
  title    = {Atomic, reusable feedback: a semi-automated solution for assessing handwritten tasks? A crossover experiment with mathematics teachers.},
  journal  = {Computers and Education Open},
  year     = {2022a},
  volume   = {3},
  pages    = {100086},
  issn     = {2666-5573},
  abstract = {Feedback has been recognized as a crucial element in the learning and teaching process. Although teachers know and accept this, they are not always eager to engage in this tedious and time-consuming activity. This study investigates how computers can work together with teachers to make the process of giving feedback more efficient by introducing a semi-automated approach (SA) with reusable feedback: when a teacher writes feedback for a student, the computer saves it, so it can be reused when following students make similar mistakes. We devised the concept of atomic feedback, a set of form requirements that could enhance feedback's reusability. To write atomic feedback, teachers have to identify the independent errors and write brief feedback items for each separate error. Our SA approach with reusable feedback was implemented in Moodle. During a crossover experiment with math teachers (n = 36 + 9 in pilot study), we examined (1) whether SA saves time or changes the amount of feedback, as compared to traditional, paper-based correction work, (2) the extent to which the feedback was atomic, (3) whether atomic feedback enhances the reusability of feedback and (4) how teachers used and perceived the SA system. In light of the results, which suggest that atomic feedback is indeed reusable, we propose formal requirements for writing reusable feedback. Nevertheless, teachers did not save time using the SA system, but they provided significantly more feedback.},
  doi      = {10.1016/j.caeo.2022.100086},
  keywords = {Human-computer interface, Architectures for educational technology system, Improving classroom teaching, Evaluation methodologies, Distributed learning environments},
}

@Article{Landis1977,
  author    = {J. Richard Landis and Gary G. Koch},
  title     = {The Measurement of Observer Agreement for Categorical Data},
  journal   = {Biometrics},
  year      = {1977},
  volume    = {33},
  number    = {1},
  pages     = {159},
  doi       = {10.2307/2529310},
  publisher = {{JSTOR}},
}

@Article{Sim2005,
  author        = {Sim, Julius and Wright, Chris C},
  title         = {{The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements}},
  journal       = {Physical Therapy},
  year          = {2005},
  volume        = {85},
  number        = {3},
  pages         = {257-268},
  month         = {03},
  issn          = {0031-9023},
  __markedentry = {[mfili:6]},
  abstract      = {{Purpose. This article examines and illustrates the use and interpretation of the kappa statistic in musculoskeletal research. Summary of Key Points. The reliability of clinicians' ratings is an important consideration in areas such as diagnosis and the interpretation of examination findings. Often, these ratings lie on a nominal or an ordinal scale. For such data, the kappa coefficient is an appropriate measure of reliability. Kappa is defined, in both weighted and unweighted forms, and its use is illustrated with examples from musculoskeletal research. Factors that can influence the magnitude of kappa (prevalence, bias, and nonindependent ratings) are discussed, and ways of evaluating the magnitude of an obtained kappa are considered. The issue of statistical testing of kappa is considered, including the use of confidence intervals, and appropriate sample sizes for reliability studies using kappa are tabulated. Conclusions. The article concludes with recommendations for the use and interpretation of kappa.}},
  doi           = {10.1093/ptj/85.3.257},
  eprint        = {https://academic.oup.com/ptj/article-pdf/85/3/257/31670498/ptj0257.pdf},
}

@Article{dawson,
  author    = {Phillip Dawson},
  title     = {Assessment rubrics: towards clearer and more replicable design, research and practice},
  journal   = {Assessment \& Evaluation in Higher Education},
  year      = {2017},
  volume    = {42},
  number    = {3},
  pages     = {347-360},
  doi       = {10.1080/02602938.2015.1111294},
  publisher = {Routledge},
}

@Article{boas,
  author    = {Jo‐Anne Baird and Jackie Greatorex and John F. Bell},
  title     = {What makes marking reliable? Experiments with UK examinations},
  journal   = {Assessment in Education: Principles, Policy \& Practice},
  year      = {2004},
  volume    = {11},
  number    = {3},
  pages     = {331-348},
  doi       = {10.1080/0969594042000304627},
  publisher = {Routledge},
}

@Comment{jabref-meta: databaseType:bibtex;}
