% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%

\usepackage{listings}
\lstset{language=Pascal}

\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{algpseudocode}
\newtheorem{define}{Definition}
\usepackage{amsmath,amssymb}
\usepackage{cleveref}
\usepackage{subcaption}
\usepackage{stmaryrd}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{dirtytalk}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{bm}
\usepackage{emoji}

\usepackage[acronym]{glossaries}
\newacronym{us}{US}{Ultrasound}
\newacronym{fps}{fps}{frames per second}
\newacronym{ef}{LVEF}{Left Ventricular Ejection Fraction}
\newacronym{es}{ES}{End-Systolic}
\newacronym{ed}{ED}{End-Diastolic}
\newacronym{sv}{SV}{Systolic Volume}
\newacronym{cdm}{CDM}{Cascaded Diffusion Model}
\newacronym{edm}{EDM}{Elucidated Diffusion Model}
\newacronym{ml}{ML}{Machine Learning}
\newacronym{mri}{MRI}{Magnetic Resonance Imaging}
\newacronym{ct}{CT}{Computed Tomography}

\newcommand{\ua}{\uparrow}
\newcommand{\da}{\downarrow}

% For referencing inline equations
\makeatletter
\newcommand*{\inlineequation}[2][]{%
  \begingroup
    % Put \refstepcounter at the beginning, because
    % package `hyperref' sets the anchor here.
    \refstepcounter{equation}%
    \ifx\\#1\\%
    \else
      \label{#1}%
    \fi
    % prevent line breaks inside equation
    \relpenalty=10000 %
    \binoppenalty=10000 %
    \ensuremath{%
      % \displaystyle % larger fractions, ...
      #2%
    }%
    ~\@eqnnum
  \endgroup
}
\makeatother




\begin{document}
% From Noise to Speckle: Causaly Generating Echocardiograms with Diffusion Models
% \title{Counterfactual Video Generation with Precise Conditional Latent Diffusion Models%\thanks{Supported by organization x.}}
% \title{From Noise to Speckle: Cascaded Diffusion Models for Precise Video Synthesis}
\title{Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis}
\titlerunning{Feature-Contioned Echocardiogram Synthesis}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Paper ID: 905}
%
\authorrunning{Paper ID: 905}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{***}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}

Image synthesis is expected to provide value for the translation of machine learning methods into clinical practice. Fundamental problems like model robustness, domain transfer, causal modelling, and operator training become approachable through synthetic data. Especially, heavily operator-dependant modalities like Ultrasound imaging require robust frameworks for image and video generation. So far, video generation has only been possible by providing input data that is as rich as the output data, \emph{e.g.}, image sequence plus conditioning in~$\rightarrow$~video out. However, clinical documentation is usually scarce and only single images are reported and stored, thus retrospective patient-specific analysis or the generation of rich training data becomes impossible with current approaches. 
In this paper, we extend elucidated diffusion models for video modelling to generate plausible video sequences from single images and arbitrary conditioning with clinical parameters. 
We explore this idea within the context of echocardiograms by looking into the variation of the Left Ventricle Ejection Fraction, the most essential clinical metric gained from these examinations. We use the publicly available EchoNet-Dynamic dataset for all our experiments.
Our image to sequence approach achieves an $R^2$ score of 93\%, which is 38 points higher than recently proposed sequence to sequence generation methods. A public (and anonymous) demo is available as soon as the reviewing period starts: \url{bit.ly/3HTskPF}. All the code, experiments, and weight files will be released by the time of the conference.

%\keywords{First keyword  \and Second keyword \and Another keyword.} -- not necessary for submission
\end{abstract}
%
%
%
\section{Introduction}
%
\gls{us} is widely used in clinical practice because of its availability, real-time imaging capabilities, lack of side effects for the patient and flexibility. \gls{us} is a dynamic modality that heavily relies on operator experience and on-the-fly interpretation, which requires many years of training and/or \gls{ml} support that can handle image sequences. 
However, clinical reporting is conventionally done via single, selected images that rarely suffice for clinical audit or as training data for \gls{ml}. 
Simulating \gls{us} from anatomical information, \emph{e.g.} \gls{ct} \cite{shams_real-time_2008}, \gls{mri} \cite{salehi_patient-specific_2015} or computational phantoms \cite{jensen_simulation_2004,segars_4d_2010}, has been considered as a possible avenue to provide more \gls{us} data for both operator and \gls{ml} training. However, simulations are usually very computationally expensive due to complex scattering, reflection and refraction of sound waves at tissue boundaries during image generation. 
% Therefore, \gls{us} simulations did not provide the required image quality yet to support tasks like cross-modality registration during intervention, multi-modal learning or robust decision support for image analysis during \gls{us} examinations. 
Therefore, the image quality of \gls{us} simulations has not yet met the necessary quality to support tasks such as cross-modality registration, multi-modal learning, and robust decision support for image analysis during \gls{us} examinations. More recently, generative deep learning methods have been proposed to address this issue. While early approaches show promising results, they either focus on generating individual images \cite{liang_sketch_2022} or require video input data and further conditioning to provide useful results \cite{liang_weakly-supervised_2022,reynaud_dartagnan_2022}. Research in the field of image-conditioned video generation is very scarce \cite{song_talking_2019} and, to the best of our knowledge, we are the first to apply it to medical imaging.

\noindent\textbf{Contribution: } In this paper, we propose a new method for video diffusion \cite{ho_imagen_2022,singer_make--video_2022} based on the \gls{edm} \cite{karras_elucidating_2022} that allows to synthesise plausible video data from single frames together with precise conditioning on interpretable clinical parameters, \emph{e.g.}, \gls{ef} in echocardiography. 
%This is, to the best of our knowledge, the first time diffusion models have been extended for \gls{us} image and video synthesis.
This is the first time diffusion models have been extended for \gls{us} image and video synthesis. Our contributions are three-fold:
(1) We show that discarding the conventional text-embeddings \cite{ho_imagen_2022,ramesh_zero-shot_2021,rombach_high-resolution_2022,saharia_photorealistic_2022,singer_make--video_2022} to control the reverse diffusion process is desirable for medical use cases where very specific elements must be precisely controlled; 
(2) We quantitatively improve upon existing methods \cite{reynaud_dartagnan_2022} for counterfactual modelling, \emph{e.g.}, when doctors try to answer questions like ``how would the scan of this patient look like if we would change a given clinical parameter?'';  
(3) We show that fine-grained control of the conditioning leads to precise data generation with specific properties and outperforms the state-of-the-art when using such data, for example, for the estimation of \gls{ef} in patients that are not commonly represented in training databases. 

\noindent\textbf{Related work}:
\noindent\textbf{Video Generation} has been a research area within computer vision for many years now. Prior works can be organized in three categories: (1) pixel-level autoregressive models \cite{babaeizadeh_fitvid_2021,finn_unsupervised_2016,kalchbrenner_video_2017}, (2) latent-level autoregressive model coupled with generators or up-samplers \cite{babaeizadeh_stochastic_2018,kumar_videoflow_2020} and (3) latent-variable transformer-based models with up-samplers \cite{gupta_maskvit_2022,villegas_phenaki_2022}. Diffusion models have recently shown reasonable performance on low temporal and spatial resolutions \cite{ho_video_2022} as well as on longer samples with high definition image quality \cite{ho_imagen_2022,singer_make--video_2022} conditioned on text inputs. Recently, \cite{yang_diffusion_2022} combined an autoregressive pixel-level model with a diffusion-based pipeline that predicts a correction of the frame, while \cite{esser_structure_2023} presents an autoregressive latent diffusion model. 
% 

\noindent\textbf{Ultrasound simulation} has been attempted with three major approaches: (1) physics-based simulators \cite{jensen_simulation_2004,shams_real-time_2008}, (2) cross-modality registration-based methods \cite{ledesma-carbayo_spatio-temporal_2005} and (3) deep-learning based methods, usually conditioned on \gls{us}, \gls{mri} or \gls{ct} image priors~\cite{salehi_patient-specific_2015,teng_interactive_2020,tomar_content-preserving_2021} to condition the anatomy of the generated \gls{us} images. 
Cine-ultrasound has also attracted some interest. \cite{liang_weakly-supervised_2022} presents a motion-transfer-based method for pelvic \gls{us} video generation, while \cite{reynaud_dartagnan_2022} proposes a causal model for generating echocardiograms conditioned on arbitrary \gls{ef}. 
% 

\noindent\textbf{\gls{ef}}\label{sec:reg_ef}
is a major metric in the assessment of cardiac function and diagnosis of cardiomyopathy. The EchoNet-dynamic dataset \cite{ouyang_video-based_2020} is used as the go-to benchmark for \gls{ef}-regression methods. Various works \cite{mokhtari_echognn_2022,reynaud_ultrasound_2021} have attempted to improve on \cite{ouyang_video-based_2020} but the most reproducible method remains the use of an R2+1D model trained over fixed-length videos. The \texttt{R2+1D\_18} trained for this work achieves an $R^2$ score of 0.81 on samples of 64 frames spanning 2 seconds.
%
%
\section{Method}

\begin{figure}
    \centering
    \vspace{-0.5cm}
    \includegraphics[trim={0 0.35cm 0 0},clip,width=\linewidth]{figures/model_narrow.pdf}
    \caption{Summarized view of our Model. 
    Inputs (blue): a noised sample $x_i$, a diffusion step $t_i$, one anatomy image $I_c$, and one LVEF $\lambda_c$. 
    %Inputs are transformed before being used in the 3D-Unet with Temporal Attention layers model, which 
    Output (red): a slightly denoised version of $x_i$ named $x_{i+1}$. 
    See Appendix Fig.~1 for more details.}
    \label{fig:narrow_model}
    \vspace{-0.5cm}
\end{figure}
%
Diffusion probabilistic models \cite{ho_denoising_2020,sohl-dickstein_deep_2015,song_denoising_2022} are the most recent family of generative models. In this work, we follow the definition of the \gls{edm} from \cite{karras_elucidating_2022}. 
Let $q(\bm{x})$ represent the real distribution of our data, with a standard deviation of $\sigma_q$. A family of distributions $p(\bm{x}; \sigma)$ can be obtained by adding i.i.d Gaussian noise with a standard deviation of $\sigma$ to the data. When $\sigma_{\text{max}} \gg \sigma_q$, the distribution $p(\bm{x}; \sigma_{\text{max}})$ is essentially the same as pure Gaussian noise. The core idea of diffusion models is to sample a pure noise data point $\bm{x}_0 \sim \mathcal{N}(\bm{0}, \sigma_{\text{max}}^{2}\textbf{I})$ and then progressively remove the noise, generating images $\bm{x}_i$ with standard deviation $\sigma_i$ such that
$\sigma_{\text{max}} = \sigma_0 > \sigma_1 > ... > \sigma_N = 0 $, and $\bm{x}_i \sim p(\bm{x}; \sigma_i)$. The final image $\bm{x}_N$ produced by this process is thus distributed according to $q(\bm{x})$, the true distribution of the data. 
To perform the reverse diffusion process, we define a denoising function $D(\bm{x}, \sigma)$ trained to minimize the $L_2$ denoising error for all samples drawn from $q$ for every $\sigma$ such that: 
\inlineequation[eq:loss]{\mathcal{L}~=~\mathbb{E}_{\bm{y}\sim q}\mathbb{E}_{\bm{n} \sim \mathcal{N}(\bm{0}, \sigma^{2}\textbf{I})} ||D(\bm{y}+\bm{n};\sigma) - \bm{y}||^2_2}
where $\bm{y}$ is a training data point and $\bm{n}$ is noise. 
%
By following the definition of ordinary differential equations (ODE) we can continuously increase or decrease the noise level of our data point by moving it forward or backward in the diffusion process, respectively. To define the ODE we need a schedule $\sigma(t)$ that sets the noise level given the time step $t$, which we set to $\sigma(t) = t$ . The probability flow ODE's characteristic property is that moving a sample $\bm{x}_a \sim p(\bm{x}_a;\sigma(t_a)) $ from the diffusion step $t_a$ to $t_b$ with $t_a > t_b$ or $t_a < t_b$ should result in a sample $\bm{x}_b \sim p(\bm{x}_b;\sigma(t_b)) $ and this requirement is satisfied by setting $\text{d}\bm{x} = -\dot{\sigma}(t)\sigma(t)\nabla_x\text{log}~p(\bm{x};\sigma(t))\text{d}t $ where $\dot{\sigma}$ denotes the time derivative and $\nabla_x\text{log}~p(\bm{x};\sigma)$ is the score function. 
%
From the score function, we can thus write $\nabla_x\text{log}~p(\bm{x};\sigma) = (D(\bm{x};\sigma) - \bm{x})/\sigma^2$ in the case of our denoising function, such that the score function isolates the noise from the signal $\bm{x}$ and can either amplify it or diminish it depending on the direction we take in the diffusion process. 
We define $D(\bm{x};\sigma)$ to transform a neural network $F$, which can be trained inside $D$ by following the loss described in \Cref{eq:loss}. The \gls{edm} also defines a list of four important preconditionings which are defined as 
% \begin{align*}
%     c_{\text{skip}}(\sigma) = (\sigma_q^2)/(\sigma^2+\sigma_q^2)
%     &~\text{, }~
%     c_{\text{out}}(\sigma) = \sigma * \sigma_q * 1/(\sqrt{\sigma_q^2 * \sigma^2})\\
%     c_{\text{in}}(\sigma) = 1/(\sqrt{\sigma_q^2 * \sigma^2})
%     &~\text{, }~
%     c_{\text{noise}}(\sigma) = \text{log}(\sigma_t)/4
% \end{align*}
$c_{\text{skip}}(\sigma) = (\sigma_q^2)/(\sigma^2+\sigma_q^2)$, $c_{\text{out}}(\sigma) = \sigma * \sigma_q * 1/(\sigma_q^2 * \sigma^2)^{0.5}$, $c_{\text{in}}(\sigma) = 1/(\sigma_q^2 * \sigma^2)^{0.5}$ and  $c_{\text{noise}}(\sigma) = \text{log}(\sigma_t)/4$
where $\sigma_q$ is the standard deviation of the real data distribution.
%
In this paper, we focus on generating temporally coherent and realistic-looking echocardiograms.
We start by generating a low resolution, low-frame rate video $\bm{v}_0$ from noise and condition on arbitrary clinical parameters and an anatomy instead of the commonly used text-prompt embeddings~\cite{ho_imagen_2022,singer_make--video_2022}.
Then, the video is used as conditioning for the following diffusion model, which generates a temporally and/or spatially upsampled video $\bm{v}_1$ resembling $\bm{v}_0$, following the \gls{cdm} \cite{ho_cascaded_2022} idea. Compared to image diffusion models, the major change to the Unet-based architecture is to add time-aware layers, through attention, at various levels as well as 3D convolutions (see Fig.~\ref{fig:narrow_model} and Appendix Fig.~1). 
For the purpose of this research, we extend \cite{ho_imagen_2022} to handle our own set of conditioning inputs, which are a single image $\bm{I}_c$ and a scalar value $\lambda_c$, while following the \gls{edm} setup, which we apply to video generation.
%
We formally define the denoising models in the cascade as $D_{\theta_s}$ where $s$ defines the rank (stage) of the model in the cascade, and where $D_{\theta_0}$ is the base model. The base model is defined as: 
\begin{align*}
    D_{\theta_0}(\bm{x};\sigma,\bm{I}_c,\lambda_c) = c_{\text{skip}}(\sigma)\bm{x} + c_{\text{out}}(\sigma) F_{\theta_0}(c_{\text{in}}(\sigma)\bm{x};c_{\text{noise}}(\sigma), \bm{I}_c,\lambda_c)), 
\end{align*}
where $F_{\theta_0}$ is the neural network transformed by $D_{\theta_0}$ and $D_{\theta_0}$ outputs $\bm{v}_0$.
%
For all subsequent models in the cascade, the conditioning remains similar, but the models also receive the output from the preceding model, such that:
\begin{align*}
    D_{\theta_s}(\bm{x};\sigma,\bm{I}_c,\lambda_c, \bm{v}_{s-1}) = c_{\text{skip}}(\sigma)\bm{x} + c_{\text{out}}(\sigma) F_{\theta_s}(c_{\text{in}}(\sigma)\bm{x};c_{\text{noise}}(\sigma), \bm{I}_c,\lambda_c, \bm{v}_{s-1})).
\end{align*}
This holds $\forall{s} > 0$ and inputs $\bm{I}_c, \bm{v}_{s-1}$ are rescaled to the spatial and temporal resolutions expected by the neural network $F_{\theta_s}$ as a pre-processing. We apply the robustness trick from \cite{ho_cascaded_2022}, \emph{i.e}, we add a small amount of noise to real videos $\bm{v}_{s-1}$ during training, when using them as conditioning, in order to mitigate domain gaps with the generated samples $\bm{v}_{s-1}$ during inference.


Sampling from the \gls{edm} is done through a stochastic sampling method. We start by sampling a noise sample $\bm{x}_0 \sim \mathcal{N}(\bm{0}, t^2_0\textbf{I})$, where $t$ comes from our previously defined $\sigma(t_i)=t_i$ and sets the noise level. 
We follow \cite{karras_elucidating_2022} and set constants
$S_{\text{noise}}=1.003, S_{t_\text{min}}=0.05, S_{t_\text{max}}=50$ and one constant $S_{\text{churn}}$ dependent on the model. These are used to compute $\gamma_i(t_i) = \min(S_{\text{churn}}/N, \sqrt{2}-1)$ $\forall t_i \in [S_{t_\text{min}}, S_{t_\text{max}}]$ and 0 otherwise, where $N$ is the number of sampling steps. Then $\forall i \in \{0,...,N-1\}$, we sample $\bm{\epsilon}_i \sim \mathcal{N}(\bm{0}, S_{\text{noise}}\textbf{I})$
and compute a slightly increased noise level $\hat{t}_i = (\gamma_i(t_i)+1)t_i$, 
which is added to the previous sample $\hat{\bm{x}}_i = \bm{x}_i + (\hat{t}^2_i - t^2_i)^{0.5}\bm{\epsilon}_i$. 
We then execute the denoising model $D_{\theta}$ on that sample and compute the local slope 
$\bm{d}_i = (\hat{\bm{x}}_i - D_{\theta}(\hat{\bm{x}}_i;\hat{t}_i))/\hat{t}_i $ 
which is used to predict the next sample 
$ \bm{x}_{i+1}~=~\hat{\bm{x}}_i~+~(\hat{t}_{i+1}~-~\hat{t}_i)\bm{d}_i $. 
At every step but the last (\emph{i.e:} $\forall i \neq N-1$), we apply a correction to $\bm{x}_{i+1}$ such that: 
$\bm{d}_i' = (\bm{x}_{i+1} - D_{\theta}(\bm{x}_{i+1};t_{i+1}))/t_{i+1} $ and 
$\bm{x}_{i+1} = \hat{\bm{x}}_i + (t_{i+1} - \hat{t}_i) (\bm{d}_i + \bm{d}_i') / 2$. The correction step doubles the number of executions of the model, and thus the sampling time per step, compared to DDPM\cite{ho_denoising_2020} or DDIM\cite{song_denoising_2022}.
The whole sampling process is repeated sequentially for all models in the cascaded \gls{edm}. Models are conditioned on the previous output video $\bm{v}_{s-1}$ inputted at each step of the sampling process, with the frame conditioning $\bm{I}_c$ as well as the scalar value $\lambda_c$. 

\noindent\textbf{Conditioning}: Our diffusion models are conditioned on two components. First, an \textit{anatomy}, which is represented by a randomly sampled frame $I_c$. It defines the patient's anatomy, but also all the information regarding the visual style and quality of the target video. These parameters cannot be explicitly disentangled, and we therefore limit ourselves to this approach. Second, we condition the model on clinical parameters $\lambda_c$. This is done by discarding the text-encoders that are used in \cite{ho_video_2022,singer_make--video_2022} and directly inputting normalized clinical parameters into the conditional inputs of the Unets. By doing so, we give the model fine-grained control over the generated videos, which we evaluate using task-specific metrics.

\noindent\textbf{Parameters}: As video diffusion models are still in their early stage, there is no consensus on which are the best methods to train them. In our case, we define, depending on our experiment, 1-, 2- or 4-stages \gls{cdm}s. We also experiment with various schedulers and parametrizations of the model. \cite{salimans_progressive_2022,song_denoising_2022} show relatively fast sampling techniques which work fine for image sampling. However, in the case of video, we reach larger sampling times as we sample 64 frames at once. 
We therefore settled for the \gls{edm} \cite{karras_elucidating_2022}, which presents a method to sample from the model in much fewer steps, largely reducing sampling times. We do not observe any particular speed-up in training and would argue, from our experience, that the v-parametrization \cite{song_denoising_2022} converges faster. We experimentally find our models to behave well with parameters close to those suggested in~\cite{karras_elucidating_2022}.
%
\section{Experiments}
%
\noindent\textbf{Data: } % Try CAMUS ?
We use the EchoNet-Dynamic~\cite{ouyang_video-based_2020} dataset, a publicly available dataset that consists of 10,030 4-chamber cardiac ultrasound sequences, with a spatial resolution of $112 \times 112$ pixels. Videos range from 0.7 to 20.0 seconds long, with frame rates between 18 and 138 \gls{fps}. Each video has 3 channels, although most of them are greyscale. 
We keep the original data split of EchoNet-Dynamic which has 7465 training, 1288 validation and 1277 testing videos.We only train on the training data, and validate on the validation data.
In terms of labels, each video comes with an \gls{ef} score $\lambda \in [0,100]$, estimated by a trained clinician. 
At every step of our training process, we pull a batch of videos, which are resampled to 32 \gls{fps}. For each video, we retrieve its corresponding ground truth \gls{ef} as well as a random frame. After that, the video is truncated or padded to 64 frames, in order to last 2 seconds, which is enough to cover any human heartbeat. 
The randomly sampled frame is sampled from the same original video as the 64-frames sample, but may not be contained in those 64 frames, as it may come from before or after that sub-sample.

\noindent\textbf{Architectural variants: } 
We define three sets of models, and present them in details in Table 1 of the Appendix. We call the models \textit{X}-Stage Cascaded Models (\textit{X}SCM) and present the models' parameters at every stage. Every \gls{cdm} starts with a \textit{Base} diffusion model that is conditioned on the \gls{ef} and one conditional frame. The subsequent models perform either temporal super resolution (TSR), spatial super resolution (SSR) or temporal and spatial super resolution (TSSR). TSR, SSR and TSSR models receive the same conditioning inputs as the Base model, along with the output of the previous-stage model. Note that \cite{ho_imagen_2022} does not mention TSSR models and \cite{singer_make--video_2022} states that extending an SSR model to perform simultaneous temporal and spatial up-sampling is too challenging.

\noindent\textbf{Training: }
All our models are trained from scratch on individual cluster nodes, each with 8 $\times$ NVIDIA A100. We use a per-GPU batch size of 4 to 8, resulting in batches of 32 to 64 elements after gradient accumulation. 
% We tried to maximize memory usage and would have used bigger batch sizes if possible. 
The distributed training is handled by the \texttt{accelerate} library from HuggingFace \emoji{hugging-face}. We did not see any speed-up or memory usage reduction when enabling mixed precision and thus used full precision. As pointed out by \cite{ho_cascaded_2022} all models in a \gls{cdm} can be trained in parallel which significantly speeds up experimentation. We empirically find that training with a learning rate up to $5*10^{-4}$ is stable and reaches good image quality. We use an Exponential Moving Average (EMA) copy of our model to smooth out the training. We train all our models' stages for 48h, \emph{i.e.}, the 2SCM and 4SCM \gls{cdm}s are proportionally more costly to train than the 1SCM. As noted by \cite{ho_imagen_2022,singer_make--video_2022} training on images and videos improves the overall image quality. As our dataset only consists of videos, we simply deactivate the time attention layers in the Unet with a 25\% chance during training, for all models.

\noindent\textbf{Results: }
We evaluate our models' video synthesis capabilities on two objectives: \gls{ef} accuracy ($R^2$, MAE, RMSE) and image quality (SSIM, LPIPS, FID, FVD). We formulate the task as counterfactual modelling, where we set 
(1) a random conditioning frame as confounder, 
(2) the ground-truth \gls{ef} as a factual conditioning, and 
(3) a random \gls{ef} in the physiologically plausible range from 15\% to 85\% as counterfactual conditioning.
%We iterate through the entire validation set three times. Thus, 
For each ground truth video, we sample three random starting noise samples and conditioning frames. We use the \gls{ef} regression model to create a feedback loop, following what \cite{reynaud_dartagnan_2022} did, even though their model was run $100\times$ per sample instead of $3\times$. For each ground truth video, we keep the sample with the best \gls{ef} accuracy to compute all our scores over 1288 videos for each model.
%
\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{lllccXXcXcXc}
        \toprule
        Model & Task & Res. & Frames       & S. time       & $R^2\ua$ & MAE$\da$ & RMSE$\da$ & SSIM$\ua$ & LPIPS$\da$ & FID$\da$ & FVD$\da$ \\
        \midrule
        %1SCMv2
        1SCM  & Gen. & 112  & 16$^{\ddag}$ & 62s$^{\ddag}$ & 0.64     & 9.65     & 12.2      & 0.53      & 0.21       & 12.3     & 60.5     \\
        % 2SCMv6
        2SCM  & Gen. & 112  & 64           & 146s          & 0.89     & 4.81     & 6.69      & 0.53      & 0.24       & 31.7     & 141      \\
        % 4SCMv2
        4SCM  & Gen. & 112  & 64           & 279s          & 0.93     & 3.77     & 5.26      & 0.48      & 0.25       & 24.6     & 230      \\
        \midrule
        1SCM  & Rec. & 112  & 16$^{\ddag}$ & 62s $^{\ddag}$& 0.76     & 4.51     & 6.07      & 0.53      & 0.21       & 13.6     & 89.7     \\
        2SCM  & Rec. & 112  & 64           & 146s          & 0.93     & 2.22     & 3.35      & 0.54      & 0.24       & 31.4     & 147      \\
        4SCM  & Rec. & 112  & 64           & 279s          & 0.90     & 2.42     & 3.87      & 0.48      & 0.25       & 24.0     & 228      \\
        \bottomrule
        
        \end{tabularx}
    \caption{\label{tab:all_models}Metrics for all \gls{cdm}s. The \emph{Gen.} task is the counterfactual generation comparable to~\cite{reynaud_dartagnan_2022}, the \emph{Rec.} task is the factual reconstruction task. \emph{Frames} is the number of frames generated by the model, always spanning 2 seconds. $^{\ddag}$Videos are temporally upsampled to 64 frames for metric computation. \emph{S. time} is the sampling time for one video on an RTX A5000. $R^2$, MAE and RMSE are computed between the conditional \gls{ef} $\lambda_c$ and the regressed \gls{ef} using the model described in \Cref{sec:reg_ef}. SSIM, LPIPS, FID and FVD are used to quantify the image quality. LPIPS is computed with VGG \cite{simonyan_very_2015}, FID\cite{heusel_gans_2018} and FVD\cite{unterthiner_fvd_2019} with I3D. FID and FVD are computed over padded frames of $128 \times 128$ pixels.}
\end{table}

The results in Table~\ref{tab:all_models} show that increasing the frame rate improves model fidelity to the given \gls{ef}, while adding more models to the cascade decreases image quality. This is due to a distribution gap between true low-resolution samples and sequentially generated samples during inference. This issue is partially addressed by adding noise to real low-resolution samples during training, but the 1SCM model with only one stage still achieves better image quality metrics. However, the 2SCM and 4SCM models perform equally well on \gls{ef} metrics and outperform the 1SCM model thanks to their higher temporal resolution that precisely captures key frames of the heartbeat. The TSSR model, used in the 2SCM, yields the best compromise between image quality, \gls{ef} accuracy, and sampling times, and is compared to previous literature.
%
\begin{figure}
    % \centering
    \center
    % \includegraphics[width=0.5\linewidth]{figures/wide_us_with_legend.jpg}
    % \includegraphics[width=\linewidth]{figures/narrow_us_no_legend.jpg}
    \includegraphics[width=\linewidth]{figures/us_v6.pdf}
    \caption{
    Top: Ground truth frames with 29.3\% \gls{ef}. 
    Middle: Generated factual frames, with estimated 27.9\% \gls{ef}.
    Bottom: Generated counterfactual frames, with estimated 64.0\% \gls{ef}.
    (Counter-)Factual frames are generated with the 1SCM, conditioned on the ground-truth anatomy.
    %Each row spans 2s.
    }
    \label{fig:us_example}
\end{figure}
%
\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{llXXXXXX}
        \toprule
        Method                                   & Conditioning & Task   & S. time   & $R^2 \ua$ & MAE $\da$ & RMSE $\da$ & SSIM $\ua$ \\
        \midrule
        Dartagnan \cite{reynaud_dartagnan_2022}  & Video+EF     & Gen.   & $\sim$1s       & 0.51      & 15.7      & 18.4       & \textbf{0.79} \\
        2SCM                                     & Image+EF     & Gen.   & 146s      & \textbf{0.89}      & \textbf{4.81}      & \textbf{6.69}       & 0.53 \\
        \midrule
        Dartagnan \cite{reynaud_dartagnan_2022}  & Video+EF     & Rec.   & $\sim$1s       & 0.87      & 2.79      & 4.45       & \textbf{0.82}  \\
        2SCM                                     & Image+EF     & Rec.   & 146s      & \textbf{0.93}      & \textbf{2.22}      & \textbf{3.35}       & 0.54  \\
        \bottomrule
    \end{tabularx}
    \caption{\label{tab:comparison} Comparison of our 2SCM model to previous work. We try to reconstruct a ground truth video or to generate a new one. Our model is conditioned on a single frame and an \gls{ef}, while \cite{reynaud_dartagnan_2022} conditions on the entire video and an \gls{ef}. In both cases the LVEF is either the ground truth \gls{ef} (\emph{Rec.}) or a randomly sampled \gls{ef} (\emph{Gen.}).}
\end{table}

We outperform previous work for \gls{ef} regression: counterfactual video generation improves with our method by a large margin of 38 points for the $R^2$ score as shown in Table~\ref{tab:comparison}. The similarity between our factual and counterfactual results show that our time-agnostic confounding factor (\emph{i.e.} an image instead of a video) prevents entanglement, as opposed to the approach taken in \cite{reynaud_dartagnan_2022}. Our method does not score as high for SSIM as global image similarity metric, which is expected because of the stochasticity of the speckle noise. In \cite{reynaud_dartagnan_2022} this was mitigated by their data-rich confounder. Our results also match other video diffusion models \cite{esser_structure_2023,ho_imagen_2022,ho_video_2022,singer_make--video_2022} as structure is excellent, while texture tends to be more noisy as shown in Fig.~\ref{fig:us_example}.

\noindent\textbf{Qualitative study: }
% Expert 1 is Consultant cardiologist
% Expert 2 is Senior trainee in cardiology
% Expert 6 is Chief cardiac physiologist
We asked three trained clinicians (Consultant cardiologist $>$ 10 years experience, Senior trainee in cardiology $>$ 5 years experience, Chief cardiac physiologist $>$ 15 years experience) to classify 100 samples, each, as \emph{real} or \emph{fake}. Experts were not given feedback on their performance during the evaluation process and were not shown fake samples beforehand. All samples were generated with the 1SCM model or were true samples from the EchoNet-Dynamic dataset, resampled to 32fps and 2s. The samples were picked by alphabetical order from the validation set. Among the 300 samples evaluated, 130 (43.33\%) were real videos, 89 (29.67\%) were factual generated videos, and 81 (27.0\%) were counterfactual generated videos. The average expert accuracy was 54.33\%, with an inter-expert agreement of 50.0\%. More precisely, experts detected real samples with an accuracy of 63.85\%, 50.56\% for factual samples and 43.21\% for the counterfactual samples. The average time taken to evaluate each sample was 16.2s. 
We believe that these numbers show the video quality that our model reaches, and can put in perspective the SSIM scores from \Cref{tab:all_models}.%, which are lowered by the noise present in the echocardiograms.

\noindent\textbf{Downstream task: } We train our \gls{ef} regression model on rebalanced datasets and resampled datasets. 
%The datasets are rebalanced by generating samples with our 4SCM model for \gls{ef} values lacking sufficient samples. 
We rebalance the datasets by using our 4SCM model to generate samples for \gls{ef} values that have insufficient data. 
The resampled datasets are smaller datasets randomly sampled from the real training set. We show that, in small data regimes, using generated data to rebalance the dataset improves the overall performance. Training on 790 real data samples yields an $R^2$ score of 56\% while the rebalanced datasets with 790 samples, $\sim$50\% of which are real, reaches a better 59\% on a balanced validation set. This observation is mitigated when more data is available. See Appendix Table 2 for all our results.

\noindent\textbf{Discussion:}
Generating echocardiograms is a challenging task that differs from traditional computer vision due to the noisy nature of US images and videos. However, restricting the training domain simplifies certain aspects, such as not requiring a long series of \gls{cdm}s to reach the target resolution of $112 \times 112$ pixels and limiting samples to 2 seconds, which covers any human heartbeat. The limited pixel-intensity space of the data also allows for models with fewer parameters. In the future, we plan to explore other organs and views within the US domain, with different clinical conditionings and segmentation maps.% and other clinical features.
\section{Conclusion}
Our application of \gls{edm}s to \gls{us} video generation achieves state-of-the-art performance on a counterfactual generation task, a data augmentation task, and a qualitative study by experts. This significant advancement provides a valuable solution for downstream tasks that could benefit from representative foundation models for medical imaging and precise medical video generation.

\newpage
\bibliographystyle{splncs04}
\bibliography{bibliography.bib}

\end{document}


% \begin{algorithm}
%     \caption{Distillation for v-parametrized diffusion models}\label{alg:cap}
%     \begin{algorithmic}
%         \Require Trained teacher model $\hat{\textbf{v}}_{\eta}(\textbf{z}_{t})$
%         \Require Data set $\mathcal{D}$
%         \Require Loss weight function $w$()
%         \Require Student sampling steps $N$
%         \For{$K$ \text{iterations} }
%             \State $\theta \leftarrow \eta $ \Comment{Init student from teacher}
%             \While{ \text{not converged} }
%                 \State $\textbf{x} \sim \mathcal{D}$
%                 \State $t = i/N, ~ i \sim Cat[1,2,...,N]$ \Comment{Set discrete sampling steps}
%                 \State $\epsilon \sim N(0, I)$
%                 \State $\textbf{z}_t = \alpha_t\textbf{x} + \sigma_t\epsilon$

%                 \State $t^{\prime} = t - 0.5/N, ~ t^{\prime\prime} = t - 1/N$ \Comment{Generate 2 previous teacher steps}

%                 \State $\hat{\textbf{x}}_{\eta}(\textbf{z}_t) = 
%                 \alpha_t \textbf{z}_t - \sigma_t \hat{\textbf{v}}_{\eta}(\textbf{z}_{t})  $  \Comment{Sample from teacher}
%                 \State $ \textbf{z}_{t^{\prime}} = 
%                 \alpha_{t^{\prime}} \hat{\textbf{x}}_{\eta}(\textbf{z}_t) + 
%                 \frac{\sigma_{t^{\prime}}}{\sigma_{t}} ( \textbf{z}_t -\alpha_t \hat{\textbf{x}}_{\eta}(\textbf{z}_t) ) $ \Comment{Perturb}
                
%                 \State $\hat{\textbf{x}}_{\eta}(\textbf{z}_{t^{\prime}}) = 
%                 \alpha_{t^{\prime}} \textbf{z}_{t^{\prime}} - 
%                 \sigma_{t^{\prime}} \hat{\textbf{v}}_{\eta}(\textbf{z}_{t^{\prime}})  $  \Comment{Sample from teacher}
%                 \State $ \textbf{z}_{t^{\prime\prime}} = 
%                 \alpha_{t^{\prime\prime}} \hat{\textbf{x}}_{\eta}(\textbf{z}_{t^{\prime}}) + 
%                 \frac{\sigma_{t^{\prime\prime}}}{\sigma_{t^{\prime}}} 
%                 ( \textbf{z}_{t^{\prime}} -\alpha_{t^{\prime}} \hat{\textbf{x}}_{\eta}(\textbf{z}_{t^{\prime}}) ) $ \Comment{Perturb}

%                 \State $ \Tilde{\textbf{x}} =  
%                 \frac
%                 { \textbf{z}_{t^{\prime\prime}} -  (\sigma_{t^{\prime\prime}}/\sigma_t) \textbf{z}_t }
%                 { \alpha_{t^{\prime\prime}}     -  (\sigma_{t^{\prime\prime}}/\sigma_t) \alpha_t     }  
%                 $

%                 \State $ \Tilde{\textbf{v}} = \alpha_t * \epsilon - \sigma_t * \Tilde{\textbf{x}} $ \Comment{Apply v-parametrization}

%                 \State $ \lambda_t = \text{log}[\alpha_{t}^{2} / \sigma_{t}^{2}]    $
%                 \State $ L_{\theta} = w(\lambda_t) ||\Tilde{\textbf{v}} - \hat{\textbf{v}}_{\theta}(\textbf{z}_t)||^2_2 $
%                 \State $ \theta \leftarrow \theta -\gamma\nabla_{\theta}L_{\theta} $
                                
%             \EndWhile
%             \State $ \eta \leftarrow \theta $
%             \State $ N \leftarrow N/2 $
            
%         \EndFor
%     \end{algorithmic}
% \end{algorithm}



% \subsection{Best results}
% Compare R2 and SSIM to DARTAGNAN \cite{reynaud_dartagnan_2022} (focus on counterfactual results as DARTAGNAN factual branch was basically reconstruction of confounder)
% Add lpips score
% may be FID, FVD, IS - need to check backend models to match "video diffusion model" (ho et al.) - probably not worth it.

% \begin{table}
%     \centering
%     \begin{tabular}{|c|cc|c|ccc|cc|}
%         \hline
%         Method             & Conditioning & Task           & Samp. time & R$^2$ $\uparrow$ & MAE $\downarrow$ & RMSE $\downarrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\
%         \hline
%         Dartagnan [cite]  & Video+EF   & Rec. & $\sim$10s      & 0.87  & 2.79& 4.45 & 0.82 & -   \\
%         Ours SRv5         & Image+EF   & Rec. & $\sim$4min    & 0.91  & 2.43 & 3.6  & 0.54  & 0.22   \\
%         \hline
%         Dartagnan [cite]  & Video+EF   & Gen.     & $\sim$10s      & 0.51  & 15.7& 18.4 & 0.79 & -   \\
%         Ours SRv5         & Image+EF   & Gen.     & $\sim$4min     & 0.81  & 6.74 & 9.13 & 0.54 & 0.21  \\

%         \hline
%     \end{tabular}
%     \caption{\label{tab:comparison}}
% \end{table}

% \subsection{Ablation study} 
% look into importance of:
% training time
% size of model
% resolution spatial/temporal

% Check results of R2, SSIM, LPIPS on base model
% Check results with other conditioning methods (seg + style embeddings, seg image conditioning)? (time hungry)
% Check results with different diffusion pipelines:
% 1 model  : image+lvef -> 112x112x16 (8fps)
% 2 models : image+lvef -> 56x56x16 (8fps) -> 112x112x64 (32fps)
% 4 models : image+lvef -> 56x56x16 (8fps) -> 56x56x32 (16fps) -> 112x112x32 (16fps) -> 112x112x64 (32fps)

% Explore impact of sampling steps â€“ compute time increases linearly with number of sampling steps

% \begin{table}
%     \centering
%     \begin{tabular}{|cccc|c|ccc|cc|}
%         \hline
%         Conditionning & Dim & Blocks & Stage & Samp. time & R$^2$ & MAE & RMSE & SSIM & LPIPS \\
%         \hline
%         LVEF + image  & 128 & 2      & Base  & 30 s.      & X.X   & X.X & X.X  & X.X  & X.X   \\
%         LVEF + image  & 64  & 2      & TSR   & 10 min.    & X.X   & X.X & X.X  & X.X  & X.X   \\
%         Seg/Style emb.& 128 & 2      & Base  & 30 s.      & X.X   & X.X & X.X  & X.X  & X.X   \\
%         Seg/Style emb.& 64  & 2      & TSR   & 10 min.    & X.X   & X.X & X.X  & X.X  & X.X   \\
%         \hline
%     \end{tabular}
%     \caption{ \label{tab:ablation}}
% \end{table}

% Metrics: FID, IS, SSIM, LPIPS, FVD (https://arxiv.org/pdf/1812.01717.pdf) - cite respective papers 


% \noindent\textbf{Acknowledgements:} This work was supported by Ultromics Ltd., the UKRI Centre for Doctoral Training in Artificial Intelligence for Healthcare (EP/S023283/1) and FAU cluster . We thank Phil Wang (https://github.com/lucidrains) for his open source implementation of \cite{} (imagen-video ho et al)







% SR_V5_lr - scores_factual_20230207_091934 - noise x3
% R2        | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - Base    | 0.7828    0.6934    0.6940    | 0.8389    0.8808    
% - TSR     | 0.7427    0.8105    0.6096    | 0.9142    0.8490    

% MAE       | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - Base    | 4.3355    4.9131    6.5509    | 3.5212    2.6974    
% - TSR     | 4.7348    4.0616    5.1357    | 2.4317    3.4517    

% RMSE      | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - Base    | 5.7343    6.8133    9.0580    | 4.9392    4.2491    
% - TSR     | 6.2420    5.3561    6.8472    | 3.6037    4.7817    

% SSIM      | GT/GEN    | BEST GEN  
% - Base    | 0.5766    | 0.6067    
% - TSR     | 0.5249    | 0.5429    
% LPIPS     | GT/GEN    | BEST GEN  
% - Base    | 0.1716    | 0.1593    
% - TSR     | 0.2271    | 0.2165    


% SR_V5_lr - scores_counterfactual_20230207_062716 - noise x3
% R2        | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - Base    | -0.3118   0.8022    -0.3681   | -0.2315   0.9087    
% - TSR     | 0.6455    -0.3250   -1.2510   | 0.8055    -0.2628   

% MAE       | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - Base    | 19.8136   7.1918    18.2485   | 18.9386   4.4798    
% - TSR     | 9.8433    19.9670   12.9607   | 6.7496    19.2947   

% RMSE      | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - Base    | 23.7087   9.2064    22.5060   | 22.9716   6.2560    
% - TSR     | 12.3248   23.8276   16.5034   | 9.1292    23.2615   

% SSIM      | GT/GEN    | BEST GEN  
% - Base    | 0.5766    | 0.6041    
% - TSR     | 0.5257    | 0.5426    
% LPIPS     | GT/GEN    | BEST GEN  
% - Base    | 0.1728    | 0.1610    
% - TSR     | 0.2267    | 0.2167  



% 2SCM_v5 
% Loaded file scores_factual_20230223_062519.csv with 3864 rows.
%                FileName      GT EF     Gen EF     Ref EF      SSIM     LPIPS
% 0    0X100009310A3BD7FC  78.498405  75.743120  66.434610  0.512411  0.247761
% 322  0X100009310A3BD7FC  78.498405  68.332535  66.037930  0.502559  0.253615
% 161  0X100009310A3BD7FC  78.498405  64.473020  65.931950  0.522537  0.245003
% 1    0X10094BA0A028EAC3  24.887743  26.769073  26.643589  0.478847  0.256780
% 323  0X10094BA0A028EAC3  24.887743  23.219376  28.509110  0.484286  0.259081
% Found 3 repeats per filename.
% Separated values.
% Found best values.
% Computed R2 scores.
% Computed MAE scores.
% Computed RMSE scores.
% Computed scores between ref and gen.
% Computed SSIM scores.
% Computed LPIPS scores.
% R2        | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - XSCM    | 0.8552    0.8116    0.6538    | 0.9543    0.8498    

% MAE       | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - XSCM    | 3.4659    4.0658    4.8622    | 1.7101    3.4589    

% RMSE      | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - XSCM    | 4.6819    5.3410    6.4630    | 2.6318    4.7696    

% SSIM      | GT/GEN    | BEST GEN  
% - XSCM    | 0.5043    | 0.5216    
% LPIPS     | GT/GEN    | BEST GEN  
% - XSCM    | 0.2558    | 0.2455    
% Done.


% Loaded file scores_counterfactual_20230223_063154.csv with 3864 rows.
%                FileName      GT EF     Gen EF     Ref EF      SSIM     LPIPS
% 0    0X100009310A3BD7FC  39.705463  42.686020  66.591280  0.517658  0.252465
% 322  0X100009310A3BD7FC  39.705463  36.846600  66.434610  0.500139  0.248238
% 161  0X100009310A3BD7FC  39.705463  52.978413  67.022736  0.563501  0.233374
% 1    0X10094BA0A028EAC3  68.524445  68.329300  27.171555  0.454702  0.283452
% 323  0X10094BA0A028EAC3  68.524445  67.628860  26.798595  0.447061  0.287214
% Found 3 repeats per filename.
% Separated values.
% Found best values.
% Computed R2 scores.
% Computed MAE scores.
% Computed RMSE scores.
% Computed scores between ref and gen.
% Computed SSIM scores.
% Computed LPIPS scores.
% R2        | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - XSCM    | 0.8211    -0.3996   -2.1724   | 0.9146    -0.3370   

% MAE       | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - XSCM    | 6.4576    19.6723   15.8958   | 4.0185    19.0260   

% RMSE      | GT/GEN    GT/REF    GEN/REF   | BEST GEN  BEST REF  
% - XSCM    | 8.4893    23.7414   19.5746   | 5.8644    23.2051   

% SSIM      | GT/GEN    | BEST GEN  
% - XSCM    | 0.4991    | 0.5156    
% LPIPS     | GT/GEN    | BEST GEN  
% - XSCM    | 0.2596    | 0.2495    
% Done.