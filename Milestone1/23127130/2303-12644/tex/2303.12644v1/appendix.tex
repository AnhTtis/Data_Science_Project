% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%

\usepackage{listings}
\lstset{language=Pascal}

\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{algpseudocode}
\newtheorem{define}{Definition}
\usepackage{amsmath,amssymb}
\usepackage{cleveref}
\usepackage{subcaption}
\usepackage{stmaryrd}
\usepackage{booktabs}
\usepackage{dirtytalk}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{bm}
\usepackage{emoji}
\usepackage{tabularx}


\usepackage[acronym]{glossaries}
\newacronym{us}{US}{Ultrasound}
\newacronym{fps}{fps}{frames per second}
\newacronym{ef}{LVEF}{Left Ventricular Ejection Fraction}
\newacronym{es}{ES}{End-Systolic}
\newacronym{ed}{ED}{End-Diastolic}
\newacronym{sv}{SV}{Systolic Volume}
\newacronym{cdm}{CDM}{Cascaded Diffusion Model}
\newacronym{edm}{EDM}{Elucidated Diffusion Model}
\newacronym{ml}{ML}{Machine Learning}

\newcommand{\ua}{\uparrow}
\newcommand{\da}{\downarrow}




\begin{document}


\section{Appendix}
\subsection{Models architecture details}

\begin{table}[h!]
    \centering
    \begin{tabularx}{\textwidth}{lclcccccccX}
        \toprule
        Model & Stage & Mode & Res.            & Frames & FPS & Dims. & Layers & Steps &$S_{\text{churn}}$& Comments\\
        \midrule
        1SCM  & 1     & Base & $112\times 112$ & 16     & 8   & 64    & 2,2,2  & 64    & 160              & No bot. att. \\
        \midrule
        2SCM  & 1     & Base & $56\times 56$   & 16     & 8   & 64    & 2,2,2  & 32    &  80              &  \\
        2SCM  & 2     & TSSR & $112\times 112$ & 64     & 32  & 64    & 2,2,2  & 64    & 160              & Mem. Opti. \\
        \midrule
        4SCM  & 1     & Base & $56\times 56$  & 16      & 8   & 64    & 2,2,2  & 32    &  40              & - \\
        4SCM  & 2     & TSR  & $56\times 56$  & 32      & 16  & 64    & 2,2,2  & 32    &  80              & Mem. Opti. \\
        4SCM  & 3     & TSR  & $56\times 56$  & 64      & 32  & 64    & 2,2,2  & 32    & 160              & Mem. Opti. \\
        4SCM  & 4     & SSR  & $112\times112$ & 64      & 32  & 64    & 2,2,2  & 64    & 160              & No bot. att.\\
        \bottomrule
    \end{tabularx}
    \caption{\label{tab:scms} Overview of our \textit{X}SCM architectures. The resolution, frames and fps values are those of the outputs of each model, from which the input dimensions can be inferred. The Dims. (dimensions) values define the base number of channels in the model, which is doubled at each layer, while Layers states the number of ResNet blocks at each layer. The layer count defines the depth of the UNet model. The steps refer to the number of sampling steps. The comments indicate if we deactivated bottleneck attention (\textit{No bot. att.}) or used modified downsampling and upsampling blocks to reduce memory usage (\textit{Mem. Opti.}).}
    \centering
    \begin{tabularx}{0.65\textwidth}{Xcccccc}
        \toprule
        Name          & Bin Size & Fake \% & Total Size & $R^2$ & MAE  & RMSE \\
        \midrule
        % Balanced      & 5        & 49.64\% & 395        & 0.00  & 0.00 & 0.00 \\
        Balanced      & 10       & 52.28\% & 790        & 0.59  & 8.73 & 11.19 \\
        Balanced      & 20       & 50.51\% & 1580       & 0.69  & 7.46 & 9.79 \\
        Balanced      & 50       & 50.18\% & 3950       & 0.75  & 6.61 & 8.72 \\
        % Balanced      & 100      & 50.86\% & 7900       & 0.79  & 6.22 & 8.08 \\
        \midrule
        % Baseline      & -        & 0\%     & 395        & 0.00  & 0.00 & 0.00 \\
        Baseline      & -        & 0\%     & 790        & 0.56  & 8.62 & 11.62 \\
        Baseline      & -        & 0\%     & 1580       & 0.73  & 4.12 & 9.07 \\
        Baseline      & -        & 0\%     & 3950       & 0.77  & 6.32 & 8.46 \\
        % Baseline      & -        & 0\%     & 7465       & 0.80  & 5.90 & 7.83 \\
        % \hline
        % All Samples   & -        & 35.71\% & 11611       & 0.79  & 6.09 & 7.97 \\
        \bottomrule
    \end{tabularx}
    \caption{Results for LVEF regression downstream task. Balanced models are trained on a rebalanced training set where we use our 4SCM to equalize the number of samples for each LVEF value with a 1\% step size, from 10\% to 89\%. The baseline models are trained by sampling randomly over the training data, following it's unbalanced distribution. We evaluate on 588 samples from a resampled dataset built from the validation and test sets where the LVEF are balanced as much as possible to be representative of the performance over the whole spectrum of plausible LVEF values.}
\end{table}

% ef_reg_20230301_120515_balance_5pb (bin10): R2: 0.46, MAE: 10.01, RMSE: 12.81
% ef_reg_20230301_120515_balance_5pb (bin20): R2: 0.45, MAE: 9.22, RMSE: 11.93
% ef_reg_20230301_120515_balance_5pb (bin50): R2: 0.48, MAE: 7.77, RMSE: 10.29
% ef_reg_20230301_120515_balance_5pb (val):   R2: 0.51, MAE: 6.40, RMSE: 8.60

% ef_reg_20230228_143352_balance_10pb (bin10): R2: 0.59, MAE: 8.73, RMSE: 11.19
% ef_reg_20230228_143352_balance_10pb (bin20): R2: 0.56, MAE: 8.39, RMSE: 10.74
% ef_reg_20230228_143352_balance_10pb (bin50): R2: 0.56, MAE: 7.26, RMSE: 9.52
% ef_reg_20230228_143352_balance_10pb (val):   R2: 0.56, MAE: 6.15, RMSE: 8.12

% ef_reg_20230228_143356_balance_20pb (bin10): R2: 0.69, MAE: 7.46, RMSE: 9.79
% ef_reg_20230228_143356_balance_20pb (bin20): R2: 0.66, MAE: 7.16, RMSE: 9.33
% ef_reg_20230228_143356_balance_20pb (bin50): R2: 0.67, MAE: 6.25, RMSE: 8.25
% ef_reg_20230228_143356_balance_20pb (val):   R2: 0.65, MAE: 5.48, RMSE: 7.33

% ef_reg_20230228_143408_balance_50pb (bin10): R2: 0.75, MAE: 6.61, RMSE: 8.72
% ef_reg_20230228_143408_balance_50pb (bin20): R2: 0.73, MAE: 6.37, RMSE: 8.34
% ef_reg_20230228_143408_balance_50pb (bin50): R2: 0.74, MAE: 5.47, RMSE: 7.32
% ef_reg_20230228_143408_balance_50pb (val):   R2: 0.72, MAE: 4.94, RMSE: 6.53

% ef_reg_20230228_143428_balance_100pb (bin10): R2: 0.79, MAE: 6.22, RMSE: 8.08
% ef_reg_20230228_143428_balance_100pb (bin20): R2: 0.77, MAE: 5.93, RMSE: 7.73
% ef_reg_20230228_143428_balance_100pb (bin50): R2: 0.78, MAE: 5.09, RMSE: 6.78
% ef_reg_20230228_143428_balance_100pb (val):   R2: 0.77, MAE: 4.46, RMSE: 5.88

% ef_reg_20230301_120515_resample_395 (bin10): R2: 0.49, MAE: 9.79, RMSE: 12.52
% ef_reg_20230301_120515_resample_395 (bin20): R2: 0.47, MAE: 9.21, RMSE: 11.77
% ef_reg_20230301_120515_resample_395 (bin50): R2: 0.50, MAE: 7.78, RMSE: 10.16
% ef_reg_20230301_120515_resample_395 (val):   R2: 0.50, MAE: 6.52, RMSE: 8.74

% ef_reg_20230228_143558_resample_790 (bin10): R2: 0.56, MAE: 8.62, RMSE: 11.62
% ef_reg_20230228_143558_resample_790 (bin20): R2: 0.54, MAE: 8.23, RMSE: 10.93
% ef_reg_20230228_143558_resample_790 (bin50): R2: 0.56, MAE: 6.99, RMSE: 9.50
% ef_reg_20230228_143558_resample_790 (val):   R2: 0.56, MAE: 5.89, RMSE: 8.21

% ef_reg_20230228_143612_resample_1580 (bin10): R2: 0.73, MAE: 6.79, RMSE: 9.07
% ef_reg_20230228_143612_resample_1580 (bin20): R2: 0.71, MAE: 6.61, RMSE: 8.70
% ef_reg_20230228_143612_resample_1580 (bin50): R2: 0.72, MAE: 5.62, RMSE: 7.59
% ef_reg_20230228_143612_resample_1580 (val):   R2: 0.71, MAE: 4.90, RMSE: 6.65

% ef_reg_20230228_143635_resample_3950 (bin10): R2: 0.77, MAE: 6.32, RMSE: 8.46
% ef_reg_20230228_143635_resample_3950 (bin20): R2: 0.75, MAE: 5.94, RMSE: 7.99
% ef_reg_20230228_143635_resample_3950 (bin50): R2: 0.76, MAE: 5.12, RMSE: 7.00
% ef_reg_20230228_143635_resample_3950 (val):   R2: 0.77, MAE: 4.30, RMSE: 5.86

% ef_reg_20230228_143512_baseline (bin10): R2: 0.80, MAE: 5.90, RMSE: 7.83
% ef_reg_20230228_143512_baseline (bin20): R2: 0.79, MAE: 5.58, RMSE: 7.43
% ef_reg_20230228_143512_baseline (bin50): R2: 0.79, MAE: 4.77, RMSE: 6.50
% ef_reg_20230228_143512_baseline (val):   R2: 0.81, MAE: 3.98, RMSE: 5.42

% ef_reg_20230228_142400_all_samples (bin10): R2: 0.79, MAE: 6.09, RMSE: 7.97
% ef_reg_20230228_142400_all_samples (bin20): R2: 0.77, MAE: 5.92, RMSE: 7.70
% ef_reg_20230228_142400_all_samples (bin50): R2: 0.78, MAE: 5.03, RMSE: 6.71
% ef_reg_20230228_142400_all_samples (val):   R2: 0.79, MAE: 4.22, RMSE: 5.63


\newpage

\subsection{Illustration of Model Architecture}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.55\linewidth]{figures/model_full.pdf}
    \caption{Model architecture. Ellipsoids and circles are variables, blue indicates an input, green an internal value and red an output. The figure is split into named groups of layers, reused layers are signalized with a bold name. We do not set values for the dimensions, as this depends on model parametrization. The top-level inputs are the noised video input $x_i$, the corresponding time conditioning $t_i$, the step-independent arbitrary conditioning $\lambda_c$, the reference frame $I_c$ and, when applicable, the previous stage output $v_{s-1}$. The final output $x_{i+1}$ is a slightly denoised version of $x_i$.}
    \label{fig:my_label}
\end{figure}

\end{document}
