\vspace{-5pt}
\section{Related Work}
\vspace{-5pt}
\paragraph{Unified image segmentation.}
Image segmentation targets grouping coherent pixels. Classical model architectures for semantic~\cite{fcn,deeplabv2,pspnet,psanet,ocrnet}, instance~\cite{maskrcnn,panet,hybridtaskcascade,yolact,tian2020conditional}, and panoptic~\cite{panopticseg,upsnet,Panoptic-deeplab,panopticfpn,panopticfcn} segmentation differ greatly. Recently, some works~\cite{maxdeeplab,knet,maskformer,mask2former} propose unified frameworks for image segmentation. With the help of vision transformers~\cite{vit,liu2021swin,DETR}, they retain a set of learnable queries, use these queries as convolutional kernels to produce multiple binary masks, and add an MLP head on the updated queries to predict the categories of the binary masks. This kind of simple pipeline is suitable for different segmentation tasks, and is called unified image segmentation. Nevertheless, although they design a universal structure, they are developed on specific datasets with predefined categories. Once trained on a dataset, these models could only conduct segmentation within the predefined categories in a closed vocabulary, resulting in inevitable failures in the real open vocabulary. 
We extend their scope to open vocabulary. Our model provides not only an omnipotent structure for different segmentation tasks, but also an omnipotent recognition ability for diverse scenarios in open vocabulary.

\vspace{-12pt}
\paragraph{Class-agnostic detection and segmentation.}
To generalize the localization ability of the existing detection and segmentation models, some works~\cite{entityseg,OLN,withoutclassify,uvo,konan2022extending} remove the classification head of a detection or segmentation model and treat all categories as entities. It is proven that the class-agnostic models can detect more objects since they focus on learning the generalizable knowledge of `what makes an object' rather than distinguishing visually similar classes like `house' or `building', and `cow' or `sheep', etc.  Although they give better mask predictions for general categories, recognizing the detected objects is not touched. 

\vspace{-12pt}
\paragraph{Open-vocabulary detection and segmentation.}

Some recent works try to tackle open-vocabulary detection and segmentation using language embeddings. \cite{GLIP,GLIPv2} leverage the large-scale image-text pairs to pre-train the detection network. ViLD~\cite{ViLD} distills the knowledge of ALIGN~\cite{ALIGN} to improve the detector's generalization ability. Detic~\cite{Detic} utilizes the ImageNet-21K\cite{imagenet} data to expand the detection categories. For segmentation, \cite{simplebaselinezsg} proposes a two-stage pipeline, where generalizable mask proposals are extracted and then fed into CLIP~\cite{clip} for classification. DenseCLIP~\cite{denseclipccl} adopts text embedding as a classifier to conduct convolution on feature maps produced by CLIP image encoder, and extends the architecture of the image encoder to semantic segmentation models~\cite{pspnet,deeplab}. OpenSeg~\cite{openseg} predicts general mask proposals and aligns the mask pooled features to the language space of ALIGN~\cite{ALIGN} with large-scale caption data~\cite{localizednarratives} for training. They reach great zero-shot performance for a large range of categories. However, all these works~\cite{denseclipccl,openseg,simplebaselinezsg} only deal with semantic segmentation. OpenSeg~\cite{openseg} and \cite{simplebaselinezsg} predict general masks that are noisy and overlapped, which could not accomplish instance-level distinction. MaskCLIP~\cite{MaskCLIP} is the only existing work for panoptic segmentation, which trains to gather the feature from a pre-trained CLIP image encoder. However, although it reaches great cross-dataset ability, its performance on COCO is far from satisfactory.  
