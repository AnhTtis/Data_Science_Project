\begin{figure*}[t]
\centering 
\includegraphics[width=1.0\linewidth]{figure/pipeline_v15.pdf} 
\caption{The overall pipeline of OPSNet, our novel blocks are marked in red. For an input image, the segmentation model predicts updated query embeddings, binary masks, and IoU scores. Meanwhile, we leverage a Spatial Adapter to extract CLIP visual features. We use these CLIP features to enhance the query embeddings and use binary masks to pool them into CLIP embeddings. Afterward, the CLIP Embed, Query Embeds, and Concept Embeds are fed into the Embedding Modulation module to produce the  modulated embeddings.  Next, we use Mask Filtering to remove low-quality proposals thus getting masks and embeddings for each object. Finally, 
we use the modulated embeddings to match the text embeddings extracted by the CLIP text encoder and assign a category label for each mask.
}
\label{fig:pipeline}
\vspace{-10pt}
\end{figure*}

\section{Method}

We introduce our OPSNet, an omnipotent and data-efficient framework for open-vocabulary panoptic segmentation. The overall pipeline is demonstrated in Fig.~\ref{fig:pipeline}. 
 We introduce our roadmap towards open-vocabulary  from the vanilla version to our exquisite designs.


\subsection{Vanilla Open-vocabulary Segmentation}
\label{sec:baseline}

Inspired by DETR~\cite{DETR}, recently, unified image segmentation models~\cite{entityseg,knet,maskformer,mask2former} reformulate image segmentation as binary mask extraction and mask classification problems. They typically update a series of learnable queries to represent all things and stuff in the input image. Then, the updated queries are utilized to conduct convolution on a feature map produced by the backbone and pixel-decoder to get binary masks for each object. At the same time, a classification head with fixed FC layers is added after each updated query to predict a class label from a predefined category set.


We pick Mask2Former~\cite{mask2former} as the base model. To make it compatible with the open-vocabulary setting, we remove its classification layer and project each initial query to a query embedding to match the text embeddings extracted by the CLIP text encoder. Thus, after normalization, we could get the logits for each category by calculating the cosine similarity. Since the values of cosine similarity are small, it is crucial to make the distribution sharper when utilizing the softmax function during training. Hence, we add a temperature parameter $\tau$ as 0.01 to amplify the logits.


We train our vanilla model on COCO~\cite{coco} using panoptic annotations. Following unified segmentation methods~\cite{mask2former,maskformer,knet}, we apply bipartite matching to assign one-on-one targets for each predicted query embedding, binary mask, and IOU score. We apply cross-entropy loss on the softmax normalized cosine similarity matrix to train the mask-text alignment. For the binary masks, we apply dice loss~\cite{vnet} and binary cross-entropy loss. More details refer to ~\cite{mask2former}.


\subsection{Leveraging  CLIP Visual Features}
\label{sec:CLIP embeddings}

Instead of training the query embeddings with large amount of data like \cite{openseg,Detic}, 
we investigate introducing the pretrained CLIP visual embeddings for better object recognition. Similarly, some works~\cite{simplebaselinezsg,zegformer} pass each masked proposal into the CLIP image encoder to extract the visual embedding. However, this strategy has the following drawbacks: first, it is extremely inefficient, especially when the object number is big; second, the masked region lacks context information, which is harmful for recognition.   

Conducting mask-pooling on the CLIP features seems a straightforward solution. However, CLIP image encoder uses an attention-pooling layer to reduce the spatial dimension and makes image-text alignment simultaneously. We use a Spatial Adapter to maintain its resolution. Concretely, we re-parameterize the linear transform layer in attention-pooling as $1\times1$ convolution to project the feature map into language space.

Getting the CLIP visual features, on the one hand, we make information exchange with the segmentation model via using the CLIP features to enhance the query embeddings through cross attention. On the other hand, we adopt Mask Pooling which utilizes the binary masks to pool them into CLIP embeddings. These embeddings contain the generalizable representation for each proposal.

\subsection{Embedding Modulation} 
\label{sec:ecm}
Both the query embeddings and the CLIP embeddings could be utilized for recognition. We analyze that, as the query embeddings are trained, they have advantages in predicting in-domain categories, whereas the CLIP embeddings have priorities for unfamiliar novel categories. Therefore, we develop Embedding Modulation that takes advantage of those two embeddings and enables adequate embedding enhancement and information exchange for them, thus advancing the recognition ability and making OPSNet omnipotent for generalized domains and data-efficient for training. The Embedding Modulation contains two steps.

\vspace{-4mm}
\paragraph{Embedding Fusion.}
We first use the CLIP text encoder to extract text embeddings for the $\text N$ category names of the training data, and for the $\text M$ names of the predicting concept set. Then, we calculate a cosine similarity matrix $\mathbf{H}^{\text{M} \times \text{N}}$ between the two embeddings. Afterward, we calculate a domain similarity coefficient $\text s$ for the target concept set as $\text{s} = \frac{1}{\text{M}} \sum_{i}  \emph{max}_{j}(\mathbf{H}_{i,j})$, which means that for each category in the predicting set, we find its nearest neighbor in the training set by calculating the cosine similarity, and then they are averaged to calculate the domain similarity.

With this domain similarity, we fuse the query embeddings $\mathbf{E}_{q}$ and the CLIP embeddings $\mathbf{E}_{c}$ to get the modulated embeddings 
$\mathbf{E}_{m} = \mathbf{E}_{q} + \alpha\cdot(1-\text{s})\cdot\mathbf{E}_{c}$. 
The principle is, the fusion ratio between the two embeddings is controlled by the domain similarity $\text s$, as well as a $\alpha$ which is 10 as default.


\vspace{-3mm}
\paragraph{Logits Debiasing.}
With the modulated embeddings, we get the category logits by computing the cosine similarity between the  modulated embeddings and the text embeddings of category names. We denote the logits of the \textit{i}-th category as $\ve{z}_i$.
Inspired by \cite{logitadjustment}, which uses frequency statistics to adjust the logits for long-tail recognition, in this work, we use the concept similarity  to debias the logits, thus balancing seen and unseen categories as $\hat{\ve{z}}_i = \ve{z}_i ~/~ ( \emph{max}_{j}( \mathbf{H}_{i,j})_i)^\beta $, where $\beta$ is a coefficient controls the adjustment intensity.
The equation means that, for the \textit{i}-th category, we find the most similar category in the training set and use this class similarity to adjust the logits. In this way, the bias towards seen categories could be alleviated smoothly. The default value of $\beta$ is 0.5.




\subsection{Additional Improvements}
\label{sec:additional}
The framework above is already able to make open-vocabulary predictions. In this section, we propose two additional improvements to push the boundary of OPSNet.      

\vspace{-5mm}
\paragraph{Mask Filtering.}
Leveraging the CLIP embeddings for modulation is crucial for improving the generalization ability, but it also raises a problem: the query-based segmentation methods~\cite{maskformer,mask2former} rely on the classification predictions to filter invalid proposals to get the panoptic results. Concretely, they add an additional background class and assign all unmatched proposals as background in Hungarian matching. Thus, they could filter invalid proposals during inference without NMS. Without this filtering process, there would be multiple duplicate or low-quality masks. 

However, the CLIP embeddings are not trained with this intention. Thus, we should either adapt the CLIP embeddings for background filtering or seek other solutions. To address the issue, we design Mask Filtering to filter invalid proposals according to the estimated mask quality. We add an IoU head with one linear layer to the segmentation model after the updated queries. It learns to regress the mask IoU between each predicted binary mask and the corresponding ground truth. For unmatched or duplicated proposals, it learns to regress to zero. We use an $L_2$-loss to train the IoU head and utilize the predicted IoU scores to rank and filter segmentation masks during testing. As the IoU is not relevant to the category label, it could naturally be generalized to unseen classes. This modification enables our model the ability to detect and segment more novel objects, which serves as the essential step towards open vocabulary.


\vspace{-4mm}
\paragraph{Decoupled Supervision.} 
Common segmentation datasets like \cite{coco,ade20k,cityscape,mapillaryvistas} contain less than 200 classes, 
but image classification datasets cover far more categories. Therefore, it is natural to explore the potential of classification datasets. Some previous works~\cite{Detic,openseg} attempt to use of image-level supervision. However, the strategy of Detic~\cite{Detic} is not extendable for multi-label supervision;  OpenSeg~\cite{openseg} designs a contrastive loss requiring a very large batch size and memory, which is hard to follow. Besides, they only supervise the classification but give no constraints for segmentation.   
In this situation, we develop Decoupled Supervision, a superior paradigm that utilizes image-level labels to improve the generalization ability, and digging supervisions from the predictions themselves for training masks. We denote this advanced version as OPSNet$^+$.


 


For a classification dataset with C categories, we extract the text embeddings $\mathbf{T}^{\text{C} \times \text{D}}$ with $\text{D}$ dimensions.
For a specific image with $\text c$ annotated object labels, assuming that OPSNet gives K predicted binary masks $\mathbf{M}^{\text{K} \times \text{H} \times \text{W}}$ with spatial dimension $\text{H}\times \text{W}$, modulated embeddings $\mathbf{E}_m^{\text{K} \times \text{D}}$, and IoU scores $\mathbf{U}^{\text{K} \times 1}$.     
We first remove the invalid predictions if their IoU scores are lower than a threshold, resulting in $\text J$ valid predictions. At the same time, we pick the embeddings $\mathbf{E}_m^{\text{J} \times \text{D}}$ for each valid prediction. 
We compute the cosine similarity of this selected  embeddings and the text embeddings $\mathbf{T}^{\text{C} \times \text{D}}$  and obtain a similarity matrix $\mathbf{S}^{\text{J}\times \text{C}}$.

We normalize each row~(the first dimension) of  $\mathbf{S}^{\text{J}\times\text{C}}$ using a softmax function $\delta$. Afterwards, we select the max value along the first dimension of $\delta(\mathbf{S}^{\text{J}\times \text{C}} )$, and select the columns~(the second dimension) for the  c annotated categories. We note this column selection operations as $\mathbbm{1}_{j\in{\mathbbm{R}^c}}$. The matching loss could be formulated as:

\begin{equation}
    \mathcal{L}_{match} = 1 - \frac{1}{ \text{c} } \sum\limits_{j=1}^\text{c} \emph{max}_i  (\delta( \mathbf{S}_{i,j} )) \mathbbm{1}_{j\in{\mathbbm{R}^c}} 
    \label{eqn:matching loss}
\end{equation}

This loss encourages the model to predict at least one matched embeddings for each image-level label. The model will not be penalized if it exists multiple masks for one category, or if there exists missing GT labels.


Although without mask annotations, the layout of panoptic masks could be regarded as supervision. As we expect the predicted masks to fill the full image, and do not overlap with each other, the summation of all predicted masks could be formulated as a constraint.
Concretely, we normalize all the K predicted masks using the Sigmoid function $\sigma$ and add all K masks to one channel. We encourage each pixel of the mask to get close to one, and propose a sum loss as: 
\begin{equation}
    \mathcal{L}_{sum} = || 1 - \sum_{k=1}^{\text{K}} (\sigma(\mathbf{M}_{k,i,j}))||_2
    \label{eqn:sum loss}
\end{equation}

When introducing ImageNet for training,  we  add $\mathcal{L}_{match}$ and $\mathcal{L}_{sum}$ with weights of $1.0$ and $0.4$.