\section{Experiments} \label{sec:exp}
\paragraph{Implementation details.} We adopt Mask2Former~\cite{mask2former} as our segmentation model, 
and choose the ResNet-50~\cite{resnet} version CLIP~\cite{clip} for visual-language alignment, where the image and text are encoded as 1024-dimension feature vectors. Compared with Mask2Former, the additional computation burden of CLIP is acceptable as we choose the smallest version of CLIP and do not compute the gradient.  When using the Swin-L backbone for Mask2Former, with an input size of 640, the FLOPs and Params of Mask2Former and OPSNet are 403G/485G and 215M/242M. As we pass CLIP only once, our FLOPs is significantly smaller than \cite{zegformer,simplebaselinezsg}, which feed each proposal into CLIP.


\vspace{-2mm}
\paragraph{Training configurations.} In the basic setting, we train on the COCO~\cite{coco} panoptic segmentation training set. The hyper-parameters follow Mask2Former. The training procedure lasts 50 epochs with AdamW~\cite{adamw} optimizer. The initial learning rate (LR) is 0.0001, and it is decayed with the ratio of 0.1 at the 0.9 and 0.95 fractions of the total steps.

For the advanced version with extra image-level labels, we mix the classification data with COCO panoptic segmentation data. The re-annotated ImageNet~\cite{imagenetpp} is utilized where correct multi-label annotations are included. We use the validation split for simplicity, which covers 1K categories and contains 50 images for each category. When calculating the losses, the category names from COCO and ImageNet are treated separately.
We finetune OPSNet for 80K iterations ($\sim$ 5 epochs). The initial LR is 0.0001 and multiplied by 0.1 at the 50K iteration.

\vspace{-2mm}
\paragraph{Evaluation and metrics.} We evaluate OPSNet for both open-vocabulary and closed-world settings.
We evaluate the open-vocabulary ability by conducting cross-dataset validation for panoptic segmentation on ADE20K~\cite{ade20k}, and Cityscapes~\cite{cityscape}. To evaluate the closed-world ability, we also compare OPSNet with SOTAs on COCO panoptic segmentation. 
We report the overall PQ~(Panoptic Quality), the PQ for things and stuff, the SQ~(Segmentation Quality), and the RQ~(Recognition Quality).
Then, we report the mIoU~(mean Intersection over Union) for semantic segmentation on ADE20K~\cite{ade20k} and Pascal Context~\cite{pascalcontext} to compare with previous works. Afterward, we use the large concept set of ImageNet-21K~\cite{imagenet} and give qualitative results for open-vocabulary prediction and hierarchical prediction. 



\subsection{Roadmap to Open-vocabulary Segmentation}
\input{table/ade20k}
\input{table/panopticall}
\input{table/imagesup}
\input{table/fusion}
\input{table/clip_embed}
\input{table/grid}

We introduce our roadmap for building an open-vocabulary segmentation model. We first describe the overall procedure for how to equip our vanilla solution to OPSNet step by step. Then, we dive into the details to analyze each of our exquisite components. Following CLIP and OpenSeg~\cite{openseg}, we report the cross-dataset results for evaluating the generalization ability of our model. 

Besides, we claim that keeping the performance on the training domain is also important. Therefore, we report the performance of both  ADE20K and COCO~(training domain) for the ablation studies.  


\vspace{-2mm}
\paragraph{From vanilla solutions to OPSNet.}
In Table~\ref{tab:crossade}, we conduct experiments on COCO and ADE20K panoptic data step by step from vanilla solutions to OPSNet.

The closed-vocabulary method Mask2Former cannot directly evaluate other datasets due to the category conflicts. 
In row 2, we remove its classification head to make it predict  class-agnostic masks. Then, as introduced in Sec.~\ref{sec:CLIP embeddings}, we use these masks to pool the CLIP features to get CLIP embeddings, and use them for recognition. However, as explained in Sec.~\ref{sec:additional}, this modification would not be suitable if we still adopt the classification results to filter the proposals. Therefore, in row 3, we add Mask Filtering and observe significant performance improvements.
In rows 4 and 5, we show the performance of only using the query embeddings for recognition. Then, in row 6, we demonstrate that adding a cross-attention layer to gather the CLIP features would be helpful for learning query embeddings. Finally, in row 7, we add the Embedding Modulation for the full-version OPSNet, which shows a great gain in generalization.

The experimental results show that with the adequate embedding enhancement and the
information exchange between CLIP and the segmentation model.  Even only trained on COCO, OPSNet archives great performance on both COCO and ADE20K datasets. 


%\vspace{-2mm}
\paragraph{More data with Decoupled Supervision.} As introduced in Sec.~\ref{sec:additional}, we develop a superior training paradigm that utilizes image-level labels. In Table~\ref{tab:imagesup}, beside using COCO annotations, we further improve the generalization ability of OPSNet by introducing 50,000 images from the relabeled version of ImageNet-Val~\cite{imagenetpp}.
We first verify the effectiveness of each decoupled supervision. Then we report the performance of different backbones. When more training categories are introduced, the cross-dataset ability of OPSNet improves significantly, as indicated by the exceptional performance of OPSNet$^+$.

\begin{figure*}[t]
\newcommand{\image}{\includegraphics[width=0.97\linewidth]}
\centering 
\image{figure/demo1_v5.pdf}
%\vspace{-2mm}
\caption{Illustrations of open-vocabulary image segmentation. We choose the 21K categories of ImageNet as our prediction set. We display five proposals  with the highest confidence. OPSNet could make predictions for categories that are not included in COCO.}
\vspace{-3mm}
\label{fig:demo1}
\end{figure*}

\vspace{-2mm}
\paragraph{Analysis for CLIP embedding extraction.} In Table~\ref{tab:clipembed}, we verify the priority of our Spatial-Adapter and Mask Pooling using pure CLIP embeddings. This design shows better recognition ability and efficiency.

\vspace{-2mm}
\paragraph{Analysis for Embedding Modulation.} We give an in-depth analysis of the modulation mechanism. In Table~\ref{tab:modulation}, we report the results of the naive ensemble strategies between the query embeddings and the CLIP embeddings. We simply add these two embeddings with different ratios, and surprisingly find this straightforward method quite effective. However, we observe that the best ratio is different for each target dataset, a specific ratio would be beneficial for certain datasets but harmful for others. Our modulation strategy controls this ratio according to the domain similarity between the training and target sets and debias the final logits using the categorical similarity, which shows a strong balance across different domains.

In Table~\ref{tab:grid}, we carry out grid search for the coefficient $\alpha$ and $\beta$ which control the modulation intensity. The results show the robustness of proposed method.  

\input{table/iouhead}
\input{table/openseg}

\vspace{-4mm}
\paragraph{Analysis for Mask Filtering.} First, to demonstrate the gap between closed-vocabulary  and open-vocabulary settings, in Fig.~\ref{fig:sim}, we compare the cosine similarity distribution between the trained class prototypes~(weights of the last FC layer) of Mask2Former and the CLIP text embeddings that used by OPSNet. We find the text embeddings are much less discriminative than the trained class prototypes, and the similarity distribution text embeddings vary for different datasets. Thus, the classification score of OPSNet would not be as indicative as the original Mask2Former to rank the predicted masks, which supports the claims in Sec.~\ref{sec:additional}.

In Table~\ref{tab:iou}, we conduct ablation studies with different visual embeddings. The three blocks correspond to the CLIP, query, and modulated embeddings respectively. The results show that an IoU score could notably improve performance especially when CLIP embeddings are introduced. 

\begin{figure}[t]
\newcommand{\image}{\includegraphics[width=0.28\columnwidth]}
\centering 
\tabcolsep=0.05cm
\renewcommand{\arraystretch}{0.06}
\hspace{-3mm}
\begin{tabular}{ccc}
\vspace{1mm}
\image{figure/distribution_v2/coco.png} &
\image{figure/distribution_v2/ade.png} &
\image{figure/distribution_v2/city.png} \\
\vspace{1mm}
{ \tiny (a)~COCO Protos } & {\tiny (b)~ADE20K Protos} & {\tiny (c)~CityScapes Protos} \\
\vspace{1mm}
\image{figure/distribution_v2/coco_clip.png} &
\image{figure/distribution_v2/ade_clip.png} &
\image{figure/distribution_v2/city_clip.png} \\
{ \tiny (d)~COCO CLIP embeds } & {\tiny (e)~ADE20K CLIP embeds} & {\tiny (f)~CityScapes CLIP embeds} \\
\end{tabular}
\caption{The distributions of the pairwise cosine similarities among categories for the trained prototypes and  the CLIP text embeddings on different datasets. The mean value is noted as $\mu$. }
\vspace{-2mm}
\label{fig:sim}
\end{figure}

\begin{figure*}[t]
\newcommand{\image}{\includegraphics[width=1.98\columnwidth]}
\centering 
\image{figure/hie_v4.pdf} 
\vspace{-3mm}
\caption{Demonstrations for open-vocabulary image segmentation with hierarchical categories.}
\vspace{-4mm}
\label{fig:hie}
\end{figure*}


\vspace{-4mm}
\subsection{Cross-dataset Validation }
To evaluate the generalization ability of the proposed OPSNet, we conduct cross-dataset validation.

\vspace{-4mm}
\paragraph{Open-vocabulary panoptic segmentation.} In Table~\ref{tab:panoptic}, we report the results on three different panoptic segmentation datasets. 
OPSNet shows significant superiority over MaskCLIP~\cite{MaskCLIP} on both COCO and ADE20K, which verifies our omnipotent for general domains.  


\vspace{-5mm}
\paragraph{Open-vocabulary semantic segmentation.} 
Some previous works~\cite{openseg,lseg,zegformer,simplebaselinezsg} explore open-vocabulary semantic segmentation. In Table~\ref{tab:openseg}, we make comparisons with them by merging our panoptic predictions into semantic results according to the predicted categories. 

Among previous methods, OpenSeg~\cite{openseg} is the most representative one. Here we emphasize our differences with OpenSeg: 1)~OpenSeg could only conduct semantic segmentation, as it does not deal with duplicated or overlapped masks. However, we develop Mask Filtering to remove the invalid predictions, thus maintaining the instance-level information. 2)~OpenSeg completely retrains the mask-text alignment, thus requiring a vast amount of training data. In contrast, Embedding Modulation efficiently utilizes features extracted by the CLIP image encoder, which makes our model data-efficient but effective.

OPSNet demonstrates superior results on all these datasets. Compared with OpenSeg, our model shows superiority using much fewer training samples. Besides, although OpenSeg reaches great cross-dataset ability, its performance on COCO is poor. In contrast, OPSNet keeps a strong performance in the training domain~(COCO), which is also important for a universal solution.

\input{table/coco_sota}

\vspace{-1mm}
\subsection{Closed-vocabulary Performance}
\vspace{-1mm}
We consider maintaining a competitive performance on the classical closed-world datasets is also important for a omnipotent solution. Therefore, in Table~\ref{tab:sotacocopanoptic}, we compare the proposed OPSNet with the current best methods for COCO panoptic segmentation. OPSNet gets better performance than our base model Mask2Former, and shows competitive results compared with SOTA methods. 


\subsection{Generation to Broader Object Category}
\paragraph{Prediction with 21K concepts.} We use the categories of ImageNet-21K~\cite{imagenet} to describe the segmented targets. This large scope of words could roughly cover all common objects in everyday life. As illustrated in Fig.~\ref{fig:demo1}, we display the top-5 category predictions for several segmented masks.

The first row shows examples in COCO. The ground truth annotations ignore the objects that are not in the 133 categories. However, OPSNet could extract their masks and give reasonable category proposals, like `mantle, gown, robe' for the `clothes'.
In row~2, we test on Berkeley dataset~\cite{berkeley}, OPSNet successfully predicts the `penguin' and the `leopard', which are not included in COCO. However, the prediction inevitably contains some noise. For example, in case (2) of Fig.~\ref{fig:demo1}, our model predicts the background as `rock' and `stone', but the `iceberg' is still within the top-5 predictions. 

\vspace{-4mm}
\paragraph{Hierarchical category prediction.} \label{hie}
WordNet~\cite{wordnet} gives the hierarchy for large amounts of vocabulary, which provides a better way to understand the world. Inspired by this, we explore building a hierarchical concept set.

In Fig.~\ref{fig:hie}, we make predictions with hierarchy via building a category tree.  
For example, when dealing with `Target 1' in the first row. We first make classification among coarse-grained categories like `thing' and `stuff', and gradually dive into some fine-grained categories like the specific types of cats. Finally, for this cat, we predict different levels of category [thing, animal, cat, siamese cat].
