
\section{Introduction}
\vspace{-3pt}
The real world is diverse and contains numerous distinct objects. In practical scenarios, we inevitably encounter various objects with different shapes, colors, and categories. Although some of them are unfamiliar or rarely seen, to better understand the world, we still need to figure out the region and shape of each object and what it is. The ability to perceive and segment both known and unknown objects is natural and essential for many real-world applications like autonomous driving, robot sensing and navigation, human-object interaction, augmented reality, healthcare, \etc.

\begin{figure}[t]
\centering 
\includegraphics[width=1.0\linewidth]{figure/f1_v7.pdf} 
\vspace{-10pt}
\caption{Visual comparisons of classical closed-vocabulary segmentation and our open-vocabulary segmentation. Models are trained on the COCO panoptic dataset. Categories like `printer', `card index', `dongle', and `kangaroo' are not presented in the COCO concept set. Closed-vocabulary segmentation algorithms like Mask2Former~\cite{mask2former} are not able to detect and segment new objects (top middle) or fail to recognize object categories (bottom middle). In contrast, our approach is able to segment and recognize novel objects (top right, bottom right) for the open vocabulary.}
\vspace{-10pt}
\label{fig:1}
\end{figure}

Lots of works have explored image segmentation and achieved great success~\cite{pspnet,maskrcnn,knet,mask2former}. However, they are typically designed and developed on specific datasets (\eg, COCO~\cite{coco}, ADE20K~\cite{ade20k}) with predefined categories in a closed vocabulary, which assume the data distribution and category space remain unchanged during algorithm development and deployment procedures, resulting in noticeable and unsatisfactory failures when handling new environments in the complex real world, as shown in Fig.~\ref{fig:1} (b).



To address this problem, open-vocabulary perception is densely explored for semantic segmentation and object detection. Some methods~\cite{openseg,Detic,ViLD,GLIP,GLIPv2} use the visual-linguistic well-aligned CLIP~\cite{clip} text encoder to extract the language embeddings of category names to represent each category, and train the classification head to match these language embeddings. However, training the text-image alignment from scratch often requires a large amount of data and a heavy training burden. 
Other works~\cite{MaskCLIP,simplebaselinezsg} use both of the pre-trained CLIP image/text encoders to transfer the open-vocabulary ability from CLIP. However, as CLIP is not a cure-all for all domains and categories, although they are data-efficient, they struggle to balance the generalization ability and the performance in the training domain. \cite{simplebaselinezsg, zegformer} demonstrate suboptimal cross-datasset results, \cite{MaskCLIP} shows unsatifactory performance on the training domain. 
Besides, their methods for leveraging CLIP visual features are inefficient. Specifically, they need to pass each of the proposals into the CLIP image encoder to extract the visual features. 

Considering the characteristics and challenges of the previous methods, we propose \textbf{OPSNet} for \textit{\textbf{O}pen-vocabulary \textbf{P}anoptic \textbf{S}egmentation}, which is omnipotent and data-efficient for both open- and closed-vocabulary settings. 
Given an image, OPSNet first predicts class-agnostic masks for all objects and learns a series of in-domain query embeddings. 
For classification, a Spatial Adapter is added after the CLIP image encoder to maintain the spatial resolution. Then Mask Pooling uses the class-agnostic masks to pool the visual feature into CLIP embeddings, thus the visual embedding for each object can be extracted in one pass.

Afterward, we propose the key module named Embedding Modulation to produce the modulated embeddings for classification according to the query embeddings, CLIP embeddings, and the concept semantics.
This modulated final embedding could be used to match the text embeddings of category names extracted by the CLIP text encoder. 
Embedding Modulation combines the advantages of query and CLIP embeddings, and enables adequate embedding enhancement and information exchange for them, thus making OPSNet omnipotent for generalized domains and data-efficient for training.
To further push the boundary of our framework, we propose Mask Filtering to improve the quality of mask proposals, and Decoupled Supervision to scale up the training concepts using image-level labels to train classification and the self-constrains to supervise masks.

With these exquisite designs, OPSNet archives superior performance on COCO~\cite{coco}, shows exceptional cross-dataset performance on ADE20K~\cite{ade20k}, Cityscapes~\cite{cityscape}, PascalContext~\cite{pascalcontext}, and generalizes well to novel objects in the open vocabulary, as shown in Fig.~\ref{fig:1} (c).

In general, our contributions could be summarized as:

\begin{comment}
\begin{compactitem}
    \item We propose OPSNet, a simple yet effective framework for open-vocabulary panoptic segmentation, which achieves competitive performance on COCO and generalizes well to novel datasets.
    \item With spatial-adapter and mask-pooling, we efficiently leverage the CLIP visual embeddings in one pass, making our pipeline efficient and effective.
    \item We propose Embedding Modulation, enabling OPSNet to get promising results on both the training domain and general domains.
    \item We design Mask Filtering, and Decoupled Supervision to further improve OPSNet, which has proven effective for open-vocabulary segmentation.  
\end{compactitem}
\end{comment}

\begin{compactitem}
    \item We address the challenging open-vocabulary panoptic segmentation task and propose a novel framework named OPSNet, which is omnipotent and data-efficient, with the assistance of the exquisitely designed Embedding Modulation module.
    \item We propose several meticulous components like Spatial Adapter, Mask Pooling, Mask Filtering, and Decoupled Supervision, which are proven to be of great benefit for open-vocabulary segmentation.
    \item We conduct extensive experimental evaluations across multiple datasets under various circumstances, and the harvested state-of-the-art results demonstrate the effectiveness and generality of the proposed approach.
\end{compactitem}