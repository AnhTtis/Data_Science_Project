
\documentclass{article}

\usepackage[bbgreekl]{mathbbol}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{indentfirst}
\usepackage{amssymb}
\usepackage{algorithm, algorithmic}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{appendix}
\usepackage{bm}
\usepackage{url}
\usepackage{setspace}
\usepackage{subfigure}
\usepackage{verbatim}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{lineno}
\usepackage{float}
\usepackage{array}
\usepackage{diagbox}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage{cleveref}
\usepackage[affil-it]{authblk}
\numberwithin{equation}{section}

\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\newcommand\parameter{\bm{\theta}}
\newcommand\bmx{\bm{x}}

%
\newcommand{\dm}{displaymath}
\newcommand{\p}{\partial}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}
\newcommand{\la}{\left\langle}
\newcommand{\ra}{\right\rangle}

% combination
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrb}[1]{\left[#1\right]}
\newcommand{\lrbb}[1]{\left\{#1\right\}}
\newcommand{\lra}[1]{\left\langle #1\right\rangle}
\newcommand{\norm}[1]{\left\| #1\right\|}
\newcommand{\lbb}[1]{\left\{\begin{aligned} #1\end{aligned}\right.}
\newcommand{\ldr}[1]{\left. #1\right|}
\newcommand{\bmbm}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\pmpm}[1]{\begin{pmatrix} #1 \end{pmatrix}}
\newcommand{\bra}[1]{\left\langle #1\right|}
\newcommand{\ket}[1]{\left| #1\right\rangle}
\newcommand{\flr}[1]{\lfloor #1\rfloor}

% 上标
\newcommand{\T}{\mathrm{T}}


% text names in equation
\newcommand{\ac}{\operatorname{ac}}
\newcommand{\co}{\operatorname{co}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\tdist}{\operatorname{dist}}
\newcommand{\dist}{\operatorname{dis}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\epi}{\operatorname{epi}}
\newcommand{\gl}{\operatorname{gl}}
\newcommand{\grad}{\operatorname{grad}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\prox}{\operatorname{prox}}
\newcommand{\tsign}{\operatorname{sign}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\vvec}{\operatorname{vec}}
\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\Dens}{\operatorname{Dens}}
\newcommand{\Diff}{\operatorname{Diff}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\Hess}{\operatorname{Hess}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\Hor}{\operatorname{Hor}}
\newcommand{\Id}{\operatorname{Id}}
\newcommand{\KL}{\operatorname{KL}}
\newcommand{\MMD}{\operatorname{MMD}}
\newcommand{\OB}{\mathcal{OB}}
\newcommand{\Prox}{\operatorname{Prox}}
\newcommand{\Proj}{\operatorname{Proj}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Ver}{\operatorname{Ver}}
\newcommand{\der}{\operatorname{d}}
% mathbf
\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfb}{\mathbf{b}}
\newcommand{\bfc}{\mathbf{c}}
\newcommand{\bfd}{\mathbf{d}}
\newcommand{\bfe}{\mathbf{e}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bfg}{\mathbf{g}}
\newcommand{\bfh}{\mathbf{h}}
\newcommand{\bfi}{\mathbf{i}}
\newcommand{\bfj}{\mathbf{j}}
\newcommand{\bfk}{\mathbf{k}}
\newcommand{\bfl}{\mathbf{l}}
\newcommand{\bfm}{\mathbf{m}}
\newcommand{\bfn}{\mathbf{n}}
\newcommand{\bfo}{\mathbf{o}}
\newcommand{\bfp}{\mathbf{p}}
\newcommand{\bfq}{\mathbf{q}}
\newcommand{\bfr}{\mathbf{r}}
\newcommand{\bfs}{\mathbf{s}}
\newcommand{\bft}{\mathbf{t}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfv}{\mathbf{v}}
\newcommand{\bfw}{\mathbf{w}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfC}{\mathbf{C}}
\newcommand{\bfD}{\mathbf{D}}
\newcommand{\bfE}{\mathbf{E}}
\newcommand{\bfF}{\mathbf{F}}
\newcommand{\bfG}{\mathbf{G}}
\newcommand{\bfH}{\mathbf{H}}
\newcommand{\bfI}{\mathbf{I}}
\newcommand{\bfJ}{\mathbf{J}}
\newcommand{\bfK}{\mathbf{K}}
\newcommand{\bfL}{\mathbf{L}}
\newcommand{\bfM}{\mathbf{M}}
\newcommand{\bfN}{\mathbf{N}}
\newcommand{\bfO}{\mathbf{O}}
\newcommand{\bfP}{\mathbf{P}}
\newcommand{\bfQ}{\mathbf{Q}}
\newcommand{\bfR}{\mathbf{R}}
\newcommand{\bfS}{\mathbf{S}}
\newcommand{\bfT}{\mathbf{T}}
\newcommand{\bfU}{\mathbf{U}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfW}{\mathbf{W}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bfY}{\mathbf{Y}}
\newcommand{\bfZ}{\mathbf{Z}}
% mathbb
\newcommand{\Abb}{\mathbb{A}}
%\newcommand{\Bbb}{\mathbb{B}}
\newcommand{\Cbb}{\mathbb{C}}
\newcommand{\Dbb}{\mathbb{D}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Fbb}{\mathbb{F}}
\newcommand{\Gbb}{\mathbb{G}}
\newcommand{\Hbb}{\mathbb{H}}
\newcommand{\Ibb}{\mathbb{I}}
\newcommand{\Jbb}{\mathbb{J}}
\newcommand{\Kbb}{\mathbb{K}}
\newcommand{\Lbb}{\mathbb{L}}
\newcommand{\Mbb}{\mathbb{M}}
\newcommand{\Nbb}{\mathbb{N}}
\newcommand{\Obb}{\mathbb{O}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Sbb}{\mathbb{S}}
\newcommand{\Tbb}{\mathbb{T}}
\newcommand{\Ubb}{\mathbb{U}}
\newcommand{\Vbb}{\mathbb{V}}
\newcommand{\Wbb}{\mathbb{W}}
\newcommand{\Xbb}{\mathbb{X}}
\newcommand{\Ybb}{\mathbb{Y}}
\newcommand{\Zbb}{\mathbb{Z}}
% mathcal
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}
\newcommand{\Hscr}{\mathscr{H}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\leqsim}{\lesssim}
\newcommand{\geqsim}{\gtrsim}
\newcommand{\minimize}{\mathop{\textrm{minimize}}}
\newcommand{\maximize}{\mathop{\textrm{maximize}}}
\newcommand{\iprod}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\nrm}[1]{\left\|#1\right\|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\minop}[1]{\min\left\{#1\right\}}
\newcommand{\sqr}[1]{\left\|#1\right\|^2}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\epct}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\cond}[2]{\mathbb{E}\left[\left.#1\right|#2\right]}
\newcommand{\condP}[2]{\mathbb{P}\left(\left.#1\right|#2\right)}
\newcommand{\condV}[2]{\mathbb{V}\left(\left.#1\right|#2\right)}
\newcommand{\bigO}[1]{\mathcal{O}\left(#1\right)}
\newcommand{\tbO}[1]{\tilde{\mathcal{O}}\left(#1\right)}
\newcommand{\tbOm}[1]{\tilde{\Omega}\left(#1\right)}
\newcommand{\Om}[1]{\Omega\left(#1\right)}
\newcommand{\ThO}[1]{\Theta\left(#1\right)}
\newcommand{\ceil}[1]{\left\lceil #1\right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1\right\rfloor}
\newcommand{\Log}[1]{\log\left(#1\right)}
\newcommand{\bmy}{\bm{y}}
% \newcommand{\minop}[1]{\min\left\{#1\right\}}
\newcommand{\pos}[1]{\left[#1\right]_{+}}

\newcommand{\Lipt}{L_{\Theta}}
\newcommand{\Lipe}{L_e}
\newcommand{\oE}{\bar{E}}
\newcommand{\oP}{\operatorname{P}}
\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}}
\newcommand{\Lthre}{\mathcal{D}}
\newcommand{\tparameter}{\tilde{\parameter}}
\newcommand{\thre}{\mathrm{T}}


\newcommand{\cfn}[1]{{(\color{blue} cf:\,\,#1)}}
\title{Provable Convergence of Variational Monte Carlo Methods%for Solving Many-body Quantum Problems
}

\author{Tianyou Li\thanks{School of Mathematical Sciences, Peking University, CHINA (tianyouli@stu.pku.edu.cn).}
,\quad Fan Chen\thanks{School of Mathematical Sciences, Peking University, CHINA (chern@pku.edu.cn).}
,\quad  Huajie Chen\thanks{School of Mathematical Sciences, Beijing Normal University, CHINA  (chen.huajie@bnu.edu.cn).}
,\quad  Zaiwen Wen\thanks{Beijing International Center for Mathematical Research, Peking University, CHINA (wenzw@pku.edu.cn).}
% 
}


\begin{document}

\maketitle

\begin{abstract}
    The Variational Monte Carlo (VMC) is a promising approach for computing the ground state energy of many-body quantum problems and attracts more and more interests due to the development of machine learning.  The recent paradigms in VMC construct neural networks as trial wave functions, sample quantum configurations using Markov chain Monte Carlo (MCMC) and train neural networks with stochastic gradient descent (SGD) method.  However,  the theoretical convergence of VMC is still unknown when SGD interacts with MCMC sampling given a well-designed trial wave function.  Since MCMC reduces the difficulty of estimating gradients, it has inevitable bias in practice.  Moreover, the local energy may be unbounded, which makes it harder to analyze the error of MCMC sampling.  Therefore, we assume that the local energy is sub-exponential and use the Bernstein inequality for non-stationary Markov chains to derive error bounds of the MCMC estimator.  Consequently, VMC is proven to have a first order convergence rate $O(\log K/\sqrt{n K})$ with $K$ iterations and a sample size $n$.  It partially explains how MCMC influences the behavior of SGD.  Furthermore, we verify the so-called correlated negative curvature condition  and relate it to the zero-variance phenomena in solving eigenvalue functions.  It is shown that VMC escapes from saddle points and reaches $(\epsilon,\epsilon^{1/4})$ approximate second order stationary points or $\epsilon^{1/2}$-variance points in at least $O(\epsilon^{-11/2}\log^{2}(1/\epsilon) )$ steps with high probability. Our analysis enriches the understanding of how VMC converges efficiently and can be applied to general variational methods in physics and statistics. 
\end{abstract}


% \input{introduction.tex}
% \input{model.tex}
% \input{vmc.tex}
% \input{convergence.tex}
% \input{saddle.tex}
% \input{conclusion.tex}
\section{Introduction}
\label{sec:introduction}
\setcounter{equation}{0}

Solving many-body quantum problems at a manageable cost is a central aspect in many fields of physics and chemistry.
A fundamental difficulty is the complexity of the problem grows exponentially fast with respect to the number of particles in the system.
An efficient numerical algorithm is the key to reducing the computational cost and promoting accuracy.
%
The variational Monte Carlo (VMC) methods in particular can produce highly accurate predictions provided that (i) a sufficiently flexible trial wavefunction is available and (ii) the variational parameters of this wavefunction can be optimized.

A rapidly developing approach for trial wavefunctions is based on machine learning techniques.
These techniques typically rely on the ability of artificial neural networks (ANNs) to represent complex high-dimensional functions, which have already been explored in many fields of physics and chemistry.
The restricted Boltzmann machine (RBM) is first proposed  by Carleo and Troyer \cite{carleo17} as a variational ansatz for many-body quantum problems. Furthermore, a large number of deep neural networks, such as feed forward neural networks \cite{saito17,cai18}, deep RBMs \cite{deng17,glasser18,nomura17,kaubruegger2018}, convolutional neural networks \cite{liang2018,choo2018}, variational autoencoders \cite{rocchetto18}, have been applied to capture the physical features and improve the accuracy of the ground state. Motivated by the traditional Slater-Jastrow-backflow ansatzes, PauliNet \cite{hermann2020deep} and FermiNet \cite{pfau2020ab} use the permutation equivariant and invariant construction of networks and many determinants to approximate a general antisymmetric wavefunction. Both of them achieve a great success in electronic structure calculations. With an efficient ansatz that has strong ability to represent high-dimensional quantum states, the VMC method provides the route to obtain the best possible solution to the problem based on the variational principle.
%
% Particularly, ANNs have proven to be a flexible tool to approximate quantum many-body states that allow people to consider a wide range of different quantum problems.
% In a series of recent works \cite{cai18,carleo17,deng17,glasser18,kaubruegger18,nomura17,saito17,rocchetto18}, deep neural networks have been developed to tackle {\it ab initio} problems within VMC, showing accuracy improvements over traditional methods used to describe correlated systems. 
% With an efficient ansatz that has good ability to represent high-dimensional quantum states, the VMC method provides the route to obtain the best possible solution to the problem based on the variational principle.
% %
% By performing an optimization of the parameters, we can reach the lowest energy state and capture the correct ground state behavior. 

A main issue in optimizing parameterized quantum states is that the total scale of configurations grows exponentially with the system size. It is prohibitive to perform direct sampling for exceptionally large configuration spaces. 
However, the VMC approach recasts the variational eigenvalue problem into a stochastic optimization problem that is solvable under the curse of dimensionality. To compute the expectation, Markov chain Monte Carlo (MCMC) is employed to sample from the unnormalized probability. In contrast to the ordinary unbiased Monte Carlo methods, the MCMC method requires some time for mixing and produces the desired samples biasedly and non-independently. There are several common MCMC algorithms, such as  Gibbs sampling, Metropolis-Hastings (MH) algorithm, Hamiltonian Monte Carlo (HMC) etc. The efficiency of the VMC algorithm relies on the error of the stochastic sampling, which has not been well investigated in the VMC literatures.

%\hc{[add the literature review for analysis of (1) stochastic optimization and (2) MCMC sampling?]}
% VMC method !
The VMC framework is solved by stochastic optimization algorithms. With the MCMC samples, we minimize the objective function using the stochastic gradient descent (SGD) method. The vanilla SGD method is to sample independently from a uniform distribution on the finite sum, and has been extensively studied. Moulines \& Bach \cite{moulines2011non} first show linear convergence of SGD non-asymptotically for strongly convex functions. Needell et al. \cite{needell2014stochastic} improve these results by removing the quadratic dependency on the condition number in the iteration complexity results. Among these convergence results, the gradient noise assumptions for i.i.d samples are of vital importance to establish an $O(1/\sqrt{nK})$ convergence rate for general non-convex cost functions, where $n$ is the sample size per iteration and $K$ is the total number of iterations. However, stochastic sampling is not always independent or unbiased.
The convergence of SGD with Markovian gradients has been studied in \cite{duchi2012ergodic, sun2018markov}, and SGD with biased gradient estimators is considered in \cite{ajalloeian2020convergence}. But the VMC method, which produces MCMC samples from varying distributions, has been seldomly considered. More practical settings of many-body quantum systems make it harder to deal with theoretical properties in sampling and optimization.

The goal of this paper is to provide a rigorous analysis of the VMC method from an aspect of stochastic optimization, which has not been studied in pervious literatures according to our limited knowledge. %The VMC method, whose gradient estimator is biased and non-independent, cannot be covered by the vanilla SGD framework. Our assumptions on the ansatzes and Markov chains are standard or verifiable for the VMC method in practice. 
MCMC constructs a Markov chain to sample quantum configurations from an unnormalized distribution. Since it reduces the computational cost of sampling, the biasness and dependency  lead to a distinct analysis of the vanilla SGD method. 
Moreover, the local energy may be unbound due to the unbound operator and untrained trial wave functions. In our settings, the local energy is assumed sub-exponential and differentiable with respect to parameters. 
By a uniform spectral gap of Markov operators, we give a Bernstein inequality for Markov chains
and  present non-asymptotic error bounds of the MCMC estimator with unbound functions.
We establish the first-order convergence of VMC depending on a few related factors, such as step sizes, sample sizes and the length of burn-in time. 
It may help us to set suitable hyperparameters and promote algorithm efficiency in practice.  
Due to the particularity of eigenvalue problems, we further discuss how VMC escapes from saddle points. A positive lower bound of MCMC variance is constructed to satisfy a correlated negative curvature condition. It is shown that the intrinsic noise contributes to make a slight perturbation to get rid of saddle points. We make a convergence analysis of VMC escaping from saddle points. The VMC method returns  $(\epsilon,\epsilon^{1/4})$ approximate second order stationary points or $\epsilon^{1/2}$-variance points after at least $O(\epsilon^{-11/2}\log^{2}(1/\epsilon) )$ steps in high probability. Our analysis explains how VMC escapes from saddle points and why it may converge to other excited states. 

%
%\hc{[shall we show a sketch of the main contribution and techniques of this paper?]}

The rest of this paper is organized as follows. In Section \ref{sec:model}, we describe the many-body quantum system and the variational optimization problem to obtain the ground state energy. In Section \ref{sec:vmc}, the MH algorithm and the VMC method are introduced. In Section \ref{sec:conv}, we first give our assumptions for the Hamiltonian and the ansatz. Then, the sampling error is analyzed asymptotically by the concentration inequality for Markov chains. We prove that the VMC method converges to stationary points and estimate the convergence rates. In Section \ref{sec:saddle}, we provide the convergence guarantee to avoid saddle points with high probability by the stochastic error of MCMC.


\section{Many-body quantum problems}
\label{sec:model}
\setcounter{equation}{0}
Consider a many-body system with $N$ particles.
We denote the $N$-particle configuration by $\bmx:=(x_1,\cdots,x_N)\in \Xcal := \Acal^N$, with $\Acal$ being the one-particle configuration space, which can be continuous or discrete. The wavefunction $\Psi:\Xcal\rightarrow\Cbb~({\rm or}~\Rbb)$ describes the quantum state of the many-body system, and is often required to satisfy some symmetric/anti-symmetric conditions.
We denote the Hilbert space for the wavefunction by $\Hscr$. The Hamiltonian $\Hcal$ is a self-adjoint operator on the Hilbert space $\Hscr$, which determines the dynamics and the ground state of the quantum system.
$\Hcal$ can always be written as the sum of local operators acting only on one or two coordinates.

\vskip 0.2cm

% The total energy of the quantum system can be written as the Rayleigh-quotient 
% \begin{eqnarray}
% \label{eq:energy}
% \Lcal(\Psi) = \frac{\lra{\Psi,\Hcal\Psi}}{\lra{\Psi,\Psi}} .
% \end{eqnarray}
% The central task of this work is to find the ground state of the system, which can be obtained by minimizing the energy \eqref{eq:energy}
% \begin{eqnarray}
% \label{pb:min}
% \min_{\Psi\in\Hscr} \Lcal(\Psi)
% \end{eqnarray}
% or solving the corresponding eigenvalue problem $\Hcal\Psi = E_{0}\Psi$ for the lowest lying eigenvalue $E_{0}$.
Our goal is to compute the ground state energy and wavefunction of the system, which corresponds to the lowest eigenvalue $E_0$ and its corresponding eigenfunction of 
\begin{eqnarray}
\label{eigen}
\Hcal\Psi_0 = E_0\Psi_0 .
\end{eqnarray}
%
To obtain the ground state solution,  one can either solve the eigenvalue problem \eqref{eigen} directly, or alternatively minimize the following Rayleigh quotient
\begin{align}
\label{min:Rayleigh}
E_0 = \min\limits_{\Psi\in \Hscr,~\Psi\neq\bm{0}} \frac{\left \langle\Psi,\Hcal\Psi\right \rangle}{\left \langle\Psi,\Psi\right \rangle} ,
\end{align}
where the bracket means
\begin{align}
\label{sum_sigma}
\left \langle\Psi,\Hcal\Psi\right \rangle = \int_{\bmx\in \Xcal} \Psi^{*}(\bmx)\cdot \big(\Hcal\Psi\big)(\bmx)d\bmx
,\quad
\left \langle\Psi,\Psi\right \rangle = \int_{\bmx\in \Xcal} \big|\Psi(\bmx)\big|^2 d\bmx .
\end{align}
When $\Xcal$ is a discrete configuration space (see Example 1), the above integrals in \eqref{sum_sigma} is regarded as summations over all configurations.

We will then give two examples of the most popular many-body problems in the following. 

\vskip 0.2cm

{\bf Example 1.} (Heisenberg model)
%
The Heisenberg model is a spin model with $\Acal=\Zbb_2:=\{-1,1\}$.
The $N$-body spin configuration is denoted by $\bmx=(x_1,\cdots,x_N)$ with $x_i\in\Zbb_2$.
The corresponding $N$-body wavefunction 
$\Psi:\big(\Zbb_2\big)^N\rightarrow \Cbb$ belongs to the discrete space $\Hscr=\big(\Cbb^2\big)^{\otimes N}$. The Hamiltonian of the spin system is more conveniently expressed as in the linear space $\big(\Cbb^2\big)^{\otimes N}$ as %$H:\big(\Cbb^2\big)^{\otimes N} \rightarrow \big(\Cbb^2\big)^{\otimes N}$ with
\begin{align}
\label{ham:tensor}
\Hcal=\sum_{(j,k)\in G}\bigg( J_x\cdot H_{jk}^{x} + J_y\cdot H_{jk}^{y} + J_z\cdot H_{jk}^{z} \bigg) ~.
\end{align}
In \eqref{ham:tensor}, $G$ denotes the given interacting pairs within the $N$ particles, $J_x,J_y,J_z\in\Rbb$ are the coupling coefficients and the local operator are defined by
% the $x,y,z$ components represents different physical interactions,
\begin{align}
\label{ham:xyz}
& H_{jk}^{\alpha} := I_2^{\otimes j-1}\otimes \sigma^\alpha \otimes I_2^{\otimes k-j-1} \otimes \sigma^\alpha \otimes I_2^{\otimes N-k-1} 
\quad\quad {\rm for} ~~ \alpha=x,y,z,
\end{align}
with $\otimes$ denoting the kronecker product, $I_2 \in \Cbb^{2\times 2}$ being an identity matrix and $\sigma^x,~\sigma^y,~\sigma^z\in \Cbb^{2\times 2}$ representing the Pauli matrices
\begin{eqnarray}
\label{pauliM}
	\sigma^x=
	\begin{pmatrix}
	0 & 1 \\
	1 & 0
	\end{pmatrix}
	, \qquad
	\sigma^y=
	\begin{pmatrix}
	0 & -i \\
	i & 0
	\end{pmatrix}
	,\qquad
	\sigma^z=
	\begin{pmatrix}
	1 & 0 \\
	0 & -1
	\end{pmatrix}. 
\end{eqnarray} 



% We can further give a more explicit form of the Hamiltonian computation for this Heisenberg model.
% For a given wavefunction $\Psi$, we only concern the value of $\big(\mathcal{H}\Psi\big)(\bmx)$ at the given spin configuration $\bmx=(x_1,\cdots,x_N)$.
% %
% Let $\delta_{jk}:\big(\Zbb_2\big)^N\rightarrow\Rbb$ be functions defined by
% \begin{align*}
% 	\delta_{jk} (\bmx) = \left\{
% \begin{array}{ll}
% 1 & {\rm if}~x_j=x_k
% \\[1ex]
% -1 & ~{\rm if}~x_j\neq x_k
% \end{array} .
% \right.
% \end{align*}
% Let $\tau_j:\big(\Zbb_2\big)^N\rightarrow\big(\Zbb_2\big)^N$ be flipping operators, defined by
% \begin{align*}
% 	\tau_j\bmx= \big(x_1,\cdots,x_{j-1},\bar{x}_j,x_{j+1},\cdots,x_N\big) 
% \end{align*}
% with $\bar{x} = 1$ if $x=-1$ and $\bar{x} = -1$ if $x=1$.
% %
% We observe from the definitions \eqref{ham:xyz} that
% % and the relation between tensor and wavefunction formulas 
% \begin{align*}
% & \big(\Hcal_{jk}^{x}\Psi\big)(\bmx) ~=~  \Psi( \tau_k\tau_j\bmx) ,
% \\[1ex]
% & \big(\Hcal_{jk}^{y}\Psi\big)(\bmx) ~=~ - \delta_{jk}(\bmx) \cdot \Psi( \tau_k\tau_j\bmx) ,
% \\[1ex]
% & \big(\Hcal_{jk}^{z}\Psi\big)(\bmx) ~=~  \delta_{jk}(\bmx) \cdot \Psi(\bmx) ,
% \end{align*}
% where $\Hcal_{jk}^{\alpha}$ corresponds to $H_{jk}^{\alpha}$ in \eqref{ham:xyz}.
% %
% Then we have the following total Hamiltonian $\Hcal$ as
% \begin{multline}
% \label{ham:Hphi}
% \quad
% \big(\Hcal\Psi\big)(\bmx) ~=~  \sum_{(j,k)\in G} 
% \Big( J_x\cdot \Psi( \tau_k\tau_j\bmx) 
% \\[1ex]
% - J_y\cdot \delta_{jk}(\bmx) \cdot  \Psi( \tau_k\tau_j\bmx)
% + J_z\cdot \delta_{jk}(\bmx) \cdot  \Psi(\bmx) \Big),
% \qquad\forall~\bmx\in\big(\Zbb_2\big)^N.
% \qquad
% \end{multline}

\vskip 0.2cm

{\bf Example 2.} (Schr\"{o}dinger equation)
%
For a many-electron system in 3 dimension, $\Acal=\Rbb^3\times \Zbb_2$, and the $N$-electron configuration is $\bmx=(x_1,\cdots,x_N)$ with $x_i=(r_i,\sigma_i)\in \Acal$.
Here $r_i$ represents the spatial coordinate and $\sigma_i$ is the spin coordinate.
%
Noting that the $N$-electron wavefunction is required to be anti-symmetric, we have the space
\begin{eqnarray*}
\Hscr = \bigwedge_{i=1}^N L^2(\Rbb^3\times\Zbb_2,\Cbb) ,
\end{eqnarray*}
where the symbol $\bigwedge$ means the usual tensorial product $\otimes$ with the additional requirement that one  keeps only the permutational anti-symmetrized products. 

The Hamiltonian of the electron system is given by
\begin{eqnarray}
\label{hamiltonian:SE}
\Hcal = -\frac{1}{2}\sum_{i=1}^N\Delta_{r_i} + \sum_{i=1}^{N} v_\mathrm{ext}(r_i) + \sum_{1\leq i<j\leq N}v_\mathrm{ee}(r_i,r_j),
\end{eqnarray}
where $v_\mathrm{ext}:\Rbb^3\rightarrow\Rbb$ is the ionic potential and $v_\mathrm{ee}:\Rbb^3\times \Rbb^3\rightarrow\Rbb$ represents the interaction between electrons
\begin{equation}
	\begin{aligned}
		v_\mathrm{ext}(r)=-\sum_{I}\frac{Z_I}{|r-R_I|},~~ v_\mathrm{ee}(r,r^{\prime})=\frac{1}{|r-r^{\prime}|},\quad r,r^{\prime}\in \Rbb^3,
	\end{aligned}
\end{equation}
with $R_I\in \Rbb^3$ and $Z_I\in \Nbb $ being the position and atomic number of the $I$-th nuclear.


\section{Variational Monte Carlo}
\label{sec:vmc}

\subsection{Problem formulation}
% By using this matrix-vector representation, we see that the Rayleigh quotient can be exactly rewritten as
% \begin{align}
% \frac{\left \langle\Psi|\Hcal|\Psi\right \rangle}{\left \langle\Psi|\Psi\right \rangle} 
% = \frac{\Psi^{\rm T}H\Psi}{\Psi^{\rm T}\Psi}
% = \frac{\sum_{j=1}^{2^N}\Psi_j \cdot (H\Psi)_j}{\sum_{j=1}^{2^N}|\Psi_j|^2} ,
% \end{align}
% where the summation over spin configurations $\{(\sigma_1,\cdots,\sigma_N)\}$ is nothing but the summation over the vector entry indices, say, $\displaystyle \sum_{\sigma_1,\cdots,\sigma_N\in\Z_2} \rightarrow \sum_{j=1}^{2^N}$~.

 The VMC method is well known in electronic structure calculations \cite{foulkes01} by evaluating the Rayleigh quotient \eqref{min:Rayleigh}. However, the complexity of many-body quantum systems grows exponentially with respect to $N$. To search for the optimum in an infinite dimensional function space, VMC approximates the wavefunction by a suitable ansatz $\Psi_{\parameter},~ \parameter\in\Rbb^d$ within a finite parameter space. There are plenty of common variational ansatzes in VMC, including Jastrow ansatz, restricted Boltzmann machine (RBM), feed forward neural networks (FFNN) and a variety of other neural networks. We solve the optimization problem:
\begin{equation}
	\min\limits_{\parameter}\Lcal(\parameter)=\frac{\left \langle\Psi_{\parameter},\Hcal\Psi_{\parameter}\right \rangle}{\left \langle\Psi_{\parameter},\Psi_{\parameter}\right \rangle}=\frac{\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot \big(\Hcal\Psi_{\parameter}\big)(\bmx)d\bmx}{\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot\Psi_{\parameter}(\bmx) d\bmx}.
\end{equation}
In the VMC framework, the Rayleigh quotient in \eqref{min:Rayleigh} can be rewritten in terms of a statistical expectation:
\begin{equation}
	\label{loss:vmc}
	\begin{aligned}
		\Lcal(\parameter)
		&= \int_{\bmx\in\Xcal}\underbrace{\frac{\Psi_{\parameter}^*(\bmx)\cdot\Psi_{\parameter}(\bmx)}{\int_{\bmx\in\Xcal}\Psi_{\parameter}^*(\bmx)\cdot\Psi_{\parameter}(\bmx)d\bmx}}_{\pi_{\parameter}(\bmx)}\cdot\underbrace{\frac{\Hcal\Psi_{\parameter}(\bmx)}{\Psi_{\parameter}(\bmx)}}_{E_{\parameter}(\bmx)}d\bmx
		\\
		&=\mathbb{E}_{\bmx\sim\pi_{\parameter}}[E_{\parameter}(\bmx)] ,
		\end{aligned}
\end{equation}
% \[
% 	\frac{\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot \big(\Hcal\Psi_{\parameter}\big)(\bmx)d\bmx}{\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot\Psi_{\parameter}(\bmx) d\bmx}=\int_{\bmx\in\Xcal}\underbrace{\frac{\Psi_{\parameter}^*(\bmx)\cdot\Psi_{\parameter}(\bmx)}{\int_{\bmx\in\Xcal}\Psi_{\parameter}^*(\bmx)\cdot\Psi_{\parameter}(\bmx)d\bmx}}_{\pi_{\parameter}(\bmx)}\cdot\underbrace{\frac{\Hcal\Psi_{\parameter}(\bmx)}{\Psi_{\parameter}(\bmx)}}_{E_{\parameter}(\bmx)}d\bmx =\mathbb{E}_{\bmx\sim\pi_{\parameter}}[E_{\parameter}(\bmx)]
% \]
where $\pi_{\parameter}(\bmx)$ and $E_{\parameter}(\bmx)$ represent the probability and the local energy at configuration $\bmx$.
%
% 

We derive the gradient and the Hessian of \eqref{loss:vmc} in the real settings for convenience. A similar statement holds true for the complex settings, according to Appendix E in \cite{lin2021explicitly}.
\begin{theorem}
	Let the map 
	$\parameter\rightarrow \Psi_{\parameter}$ from $\Rbb^{d}$ to $ L^2(\Xcal;\Rbb)$ be smooth. The gradient of $\Lcal(\parameter)$ has the following form
	\begin{equation}
		\label{eq:grad}
		g(\parameter):=\nabla_{\parameter}\Lcal(\parameter)=2\Ebb_{ \pi_{\parameter}}\lb\oE_{\parameter}(\bmx)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)\rb,
	\end{equation}
	with $\oE_{\parameter}(\bmx)=E_{\parameter}(\bmx)-\Ebb_{\bmx\sim\pi_{\parameter}}\left[E_{\parameter}(\bmx)\right]$. The Hessian of $\Lcal(\parameter)$ takes the form of 
	\begin{equation}
		\label{eq:Hess}
		\begin{aligned}
			&H(\parameter):=\nabla_{\parameter}^2\Lcal(\parameter)
		=H_1(\parameter)+H_2(\parameter)+H_3(\parameter)+H_{4}(\parameter)+H_{4}(\parameter)^{\T},
		\end{aligned}
	\end{equation}
	where 
	\begin{equation*}
		\begin{aligned}
		H_1(\parameter)&=2\Ebb_{\pi_{\parameter}}\lrb{\nabla_{\parameter}E_{\parameter}\nabla_{\parameter}\log \Psi_{\parameter}^{\T}},~~
		H_2(\parameter)=4\Ebb_{\pi_{\parameter}}\lrb{\oE_{\parameter}\nabla_{\parameter}\log \Psi_{\parameter}\nabla_{\parameter}\log \Psi_{\parameter}^{\T}},\\
		H_3(\parameter)&=2\Ebb_{\pi_{\parameter}}\lrb{\oE_{\parameter}\nabla^{2}_{\parameter}\log \Psi_{\parameter}},~~
		H_{4}(\parameter)=-4\Ebb_{\pi_{\parameter}}\lrb{\oE_{\parameter}\nabla_{\parameter}\log \Psi_{\parameter}}\Ebb_{\pi_{\parameter}}\lrb{\nabla_{\parameter}\log \Psi_{\parameter}^{\T}}.
		\end{aligned}
	\end{equation*}
	
\end{theorem}
\begin{proof}
	Using the definition of $E_{\parameter}$ and $\pi_{\parameter}$ in \eqref{loss:vmc}, we have
	\begin{equation*}
		\begin{aligned}
			g(\parameter)
					=&\frac{\int_{\bmx\in \Xcal} 2\nabla_{\parameter}\Psi_{\parameter}(\bmx)\cdot \big(\Hcal\Psi_{\parameter}\big)(\bmx)d\bmx}{\int_{\bmx\in \Xcal} \big|\Psi_{\parameter}(\bmx)\big|^2 d\bmx}
					-\frac{\int_{\bmx\in \Xcal} \Psi_{\parameter}(\bmx)\cdot \big(\Hcal\Psi_{\parameter}\big)(\bmx)d\bmx}{\int_{\bmx\in \Xcal} \big|\Psi_{\parameter}(\bmx)\big|^2 d\bmx} \frac{\int_{\bmx\in \Xcal} 2\Psi_{\parameter}(\bmx)\cdot\nabla_{\parameter}\Psi_{\parameter}(\bmx) d\bmx}{\int_{\bmx\in \Xcal} \big|\Psi_{\parameter}(\bmx)\big|^2 d\bmx}\\
					=&2\int_{\bmx\in \Xcal} \pi_{\parameter}(\bmx)E_{\parameter}(\bmx)\nabla_{\parameter}\log\Psi_{\parameter}(\bmx) d\bmx -2\int_{\bmx\in\Xcal}\pi_{\parameter}(\bmx)E_{\parameter}(\bmx) d\bmx \int_{\bmx\in \Xcal} \pi_{\parameter}(\bmx)\nabla_{\parameter}\log\Psi_{\parameter}(\bmx) d\bmx \\
					=&2\Ebb_{ \pi_{\parameter}}\lb \oE_{\parameter}(\bmx)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)\rb.
		\end{aligned}
	\end{equation*}
	Moreover, by differentiating the gradient \eqref{eq:grad}, the Hessian is calculated as follows:
	\begin{equation*}
		\begin{aligned}
			H(\parameter)=&2\nabla_{\parameter}\Ebb_{ \pi_{\parameter}}\lb \oE_{\parameter}(\bmx)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)\rb\\
			=&2\Ebb_{ \pi_{\parameter}}\lb \oE_{\parameter}(\bmx)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)\nabla_{\parameter}\log \pi_{\parameter}(\bmx)^{\T}\rb+2\Ebb_{ \pi_{\parameter}}\lb \nabla_{\parameter}\log \Psi_{\parameter}(\bmx)\nabla_{\parameter}\oE_{\parameter}(\bmx)^{\T}\rb\\
			&+2\Ebb_{ \pi_{\parameter}}\lb \oE_{\parameter}(\bmx)\nabla^2_{\parameter}\log \Psi_{\parameter}(\bmx)\rb\\
			=&4\Ebb_{\pi_{\parameter}}\lrb{\oE_{\parameter}\nabla_{\parameter}\log \Psi_{\parameter}\nabla_{\parameter}\log \Psi_{\parameter}^{\T}}-4\Ebb_{\pi_{\parameter}}\lrb{\oE_{\parameter}\nabla_{\parameter}\log \Psi_{\parameter}}\Ebb_{\pi_{\parameter}}\lrb{\nabla_{\parameter}\log \Psi_{\parameter}^{\T}}\\
			&+2\Ebb_{ \pi_{\parameter}}\lb \nabla_{\parameter}E_{\parameter} \nabla_{\parameter}\log \Psi_{\parameter}^{\T}\rb-4\Ebb_{\pi_{\parameter}}\lrb{\nabla_{\parameter}\log \Psi_{\parameter}}\Ebb_{\pi_{\parameter}}\lrb{\oE_{\parameter}\nabla_{\parameter}\log \Psi_{\parameter}^{\T}}+2\Ebb_{ \pi_{\parameter}}\lb \oE_{\parameter}\nabla^2_{\parameter}\log \Psi_{\parameter}\rb.
		\end{aligned}
	\end{equation*}
	This completes the derivation.
\end{proof}
% VMC operates an optimization algorithm to update the parameters $\parameter$, by which the energy approaches minimization within the parameter space. With the complex wavefunction, the gradient of \eqref{loss:vmc} can be obtained by the following formula. Due to the essential self-adjointness of the Hamiltonian $\Hcal$ in the Hilbert space, it holds that
% \begin{equation}
% 	\begin{aligned}
% 		\label{eq:grad}
% 		\partial_{\parameter}\Lcal(\parameter)=&\partial_{\parameter}\frac{\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot \big(\Hcal\Psi_{\parameter}\big)(\bmx)d\bmx}{\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot\Psi_{\parameter}(\bmx) d\bmx}\\
% 		=&\frac{\int_{\bmx\in \Xcal} \partial_{\parameter}\Psi_{\parameter}^{*}(\bmx)\cdot \big(\Hcal\Psi_{\parameter}\big)(\bmx)d\bmx+\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot \big(\Hcal\partial_{\parameter}\Psi_{\parameter}\big)(\bmx)d\bmx}{\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot\Psi_{\parameter}(\bmx) d\bmx}\\
% 		&-\frac{\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot \big(\Hcal\Psi_{\parameter}\big)(\bmx)d\bmx}{\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot\Psi_{\parameter}(\bmx) d\bmx} \frac{\int_{\bmx\in \Xcal} \partial_{\parameter}\Psi_{\parameter}^{*}(\bmx)\cdot\Psi_{\parameter}(\bmx) d\bmx+\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot\partial_{\parameter}\Psi_{\parameter}(\bmx) d\bmx}{\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot\Psi_{\parameter}(\bmx) d\bmx}\\
% 		=&\int_{\bmx\in \Xcal} \pi_{\parameter}(\bmx)E_{\parameter}(\bmx)\partial_{\parameter}\log\Psi_{\parameter}^{*}(\bmx) d\bmx+\int_{\bmx\in \Xcal} \pi_{\parameter}(\bmx)E^{*}_{\parameter}(\bmx)\partial_{\parameter}\log\Psi_{\parameter}(\bmx) d\bmx\\
% 		&-\int_{\bmx\in\Xcal}\pi_{\parameter}(\bmx)E_{\parameter}(\bmx) d\bmx \cdot \lrp{\int_{\bmx\in \Xcal} \pi_{\parameter}(\bmx)\partial_{\parameter}\log\Psi_{\parameter}^{*}(\bmx) d\bmx+\int_{\bmx\in \Xcal} \pi_{\parameter}(\bmx)\partial_{\parameter}\log\Psi_{\parameter}(\bmx) d\bmx }\\
% 		=&\Ebb_{ \pi_{\parameter}}\lb \lp E_{\parameter}(\bmx)-\Lcal(\parameter)\rp\partial_{\parameter}\log \Psi^{*}_{\parameter}(\bmx)\rb+\Ebb_{\pi_{\parameter}}\lb\lp E_{\parameter}^{*}(\bmx)-\Lcal(\parameter)\rp\partial_{\parameter}\log \Psi_{\parameter}(\bmx)\rb.
% 	\end{aligned}
% \end{equation}
% \begin{equation}
% 	\label{eq:grad}
% 	\begin{aligned}
% 		\nabla_{\parameter}\Lcal(\parameter)&=\sum_{\bmx,\bmx^{\prime}}\left\{\frac{\nabla_{\parameter}\left(\Psi_{\parameter}^{*}(\bmx)H_{\bmx,\bmx^{\prime}}\Psi_{\parameter}(\bmx^{\prime})\right)}{\sum_{\bmx}\Psi_{\parameter}^{*}(\bmx)\Psi_{\parameter}(\bmx)}-\frac{\nabla_{\parameter}\left(\sum_{\bmx}\Psi_{\parameter}^{*}(\bmx)\Psi_{\parameter}(\bmx)\right)\Psi_{\parameter}^{*}(\bmx)H_{\bmx,\bmx^{\prime}}\Psi_{\parameter}(\bmx^{\prime})}{(\sum_{\bmx}\Psi_{\parameter}^{*}(\bmx)\Psi_{\parameter}(\bmx))^2}\right\}\\
% 				&\overset{a)}{=}\sum_{\bmx,\bmx^{\prime}}\left\{\frac{\Psi_{\parameter}^{*}(\bmx)H_{\bmx,\bmx^{\prime}}\nabla_{\parameter}\Psi_{\parameter}(\bmx^{\prime})}{\sum_{\bmx}\Psi_{\parameter}^{*}(\bmx)\Psi_{\parameter}(\bmx)}-\frac{\sum_{\bmx}\Psi_{\parameter}^{*}(\bmx)\nabla_{\parameter}\Psi_{\parameter}(\bmx)}{\sum_{\bmx}\Psi_{\parameter}^{*}(\bmx)\Psi_{\parameter}(\bmx)}\cdot\frac{\Psi_{\parameter}^{*}(\bmx)H_{\bmx,\bmx^{\prime}}\Psi_{\parameter}(\bmx^{\prime})}{\sum_{\bmx}\Psi_{\parameter}^{*}(\bmx)\Psi_{\parameter}(\bmx)}\right\}\\
% 				&=\sum_{\bmx,\bmx^{\prime}}\left\{\frac{\Psi_{\parameter}^{*}(\bmx)H_{\bmx,\bmx^{\prime}}\Psi_{\parameter}(\bmx^{\prime})}{\sum_{\bmx}\Psi_{\parameter}^{*}(\bmx)\Psi_{\parameter}(\bmx)}\nabla_{\parameter}\log\Psi_{\parameter}(\bmx^{\prime})-\Ebb_{\bmx\sim\pi_{\parameter}}[\nabla_{\parameter}\log\Psi_{\parameter}(\bmx)]\cdot\frac{\Psi_{\parameter}^{*}(\bmx)H_{\bmx,\bmx^{\prime}}\Psi_{\parameter}(\bmx^{\prime})}{\sum_{\bmx}\Psi_{\parameter}^{*}(\bmx)\Psi_{\parameter}(\bmx)}\right\}
% 				\\
% 				&\overset{b)}{=}\sum_{\bmx,\bmx^{\prime}}\left\{\frac{\Psi_{\parameter}^{*}(\bmx^{\prime})H_{\bmx,\bmx^{\prime}}\Psi_{\parameter}(\bmx)}{\sum_{\bmx}\Psi_{\parameter}^{*}(\bmx)\Psi_{\parameter}(\bmx)}\nabla_{\parameter}\log\Psi_{\parameter}(\bmx)-\Ebb_{\bmx\sim\pi_{\parameter}}[\nabla_{\parameter}\log\Psi_{\parameter}(\bmx)]\cdot\frac{\Psi_{\parameter}^{*}(\bmx^{\prime})H_{\bmx,\bmx^{\prime}}\Psi_{\parameter}(\bmx)}{\sum_{\bmx}\Psi_{\parameter}^{*}(\bmx)\Psi_{\parameter}(\bmx)}\right\}
% 				\\
% 				&=\sum_{\bmx}\left\{\pi_{\parameter}(\bmx)E^{*}_{\parameter}(\bmx)\nabla_{\parameter}\log\Psi_{\parameter}(\bmx)-\Ebb_{\bmx\sim\pi_{\parameter}}[\nabla_{\parameter}\log\Psi_{\parameter}(\bmx)]\cdot \pi_{\parameter}(\bmx)E^{*}_{\parameter}(\bmx)\right\}\\
% 				&=\Ebb_{\bmx\sim\pi_{\parameter}}\left[E^{*}_{\parameter}(\bmx)\nabla_{\parameter}\log\Psi_{\parameter}(\bmx)\right]-\Ebb_{\bmx\sim\pi_{\parameter}}\left[E^{*}_{\parameter}(\bmx)\right]\Ebb_{\bmx\sim\pi_{\parameter}}\left[\nabla_{\parameter}\log\Psi_{\parameter}(\bmx)\right]\\
% 				&=\Ebb_{\bmx\sim\pi_{\parameter}}\left[(E^{*}_{\parameter}(\bmx)-\Ebb_{\bmx\sim\pi_{\parameter}}\left[E^{*}_{\parameter}(\bmx)\right])\nabla_{\parameter}\log\Psi_{\parameter}(\bmx)\right],
% 	\end{aligned}
% \end{equation}
% where a) holds since $\Psi_{\parameter}$ is complex analytic, and b) switches $\bmx$ and $\bmx^{\prime}$ in summation order.
% When the wavefunction is real, we may simplify this to 
% \begin{equation}
% 	\begin{aligned}
% 		\label{eq:realgrad}
% 		\nabla_{\parameter}\Lcal(\parameter)=2\Ebb_{ \pi_{\parameter}}\lb\lp E_{\parameter}(\bmx)-\Ebb_{ \pi_{\parameter}}\lb E_{\parameter}(\bmx)\rb\rp\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)\rb.
% 	\end{aligned}
% \end{equation}
\subsection{Sampling algorithm}
The exact loss function and the gradient can be hardly calculated in practice as the dimension of the Hilbert space grows exponentially on the number of particles. We can hardly compute the high-dimensional integral $\int_{\bmx\in \Xcal} \Psi_{\parameter}^{*}(\bmx)\cdot\Psi_{\parameter}(\bmx) d\bmx $ to sample directly. However, the VMC method uses the MCMC sampling to estimate the expectation with a unnormalized probability. The MH algorithm is a common MCMC algorithm which constructs a Markov chain with proposal and acceptance steps.

The MH algorithm generates samples from a Markov chain whose stationary distribution is desired. As more and more samples are produced, the Markov chain converges to some equilibrium, which means that it becomes stationary over time. We can construct a Markov chain with a transition probability $Q(\bmx^{\prime}|\bmx)$, which is also called proposal distribution. For the Schr\"{o}dinger equation, the simplest proposal distribution is a normal distribution with a mean of zero. At the $i$-th step, the MH algorithm produces a Markov chain with the stationary distribution $\pi$ as follows. We first draw a sample $\bmx^{\prime}$ from $Q(\bmx^{\prime}|\bmx_{i})$, where $\bmx_{i}$ is the previous sample. Then, the acceptance probability is computed by
\begin{equation}
	\begin{aligned}
		\label{eq: accept}
		a(\bmx^{\prime}|\bmx_{i})=\min\left\{1,\frac{\pi(\bmx^{\prime})Q(\bmx_{i}|\bmx^{\prime})}{\pi(\bmx_{i})Q(\bmx^{\prime}|\bmx_{i})}\right\}=\min\left\{1,\frac{|\Psi_{\parameter}(\bmx^{\prime})|^2Q(\bmx_i|\bmx^{\prime})}{|\Psi_{\parameter}(\bmx_i)|^2Q(\bmx^{\prime}|\bmx_i)}\right\}.
	\end{aligned}
\end{equation}
We accept the new sample $\bmx_{i+1}=\bmx^{\prime}$ with probability $a(\bmx^{\prime}|\bmx_i)$ or remain $\bmx_{i+1}=\bmx_{i}$. After $n_0+n$ iterations, we discard first $n_0$ samples for a better estimate, which is called a burn-in period. Finally, $n$ samples $\mathbf{S}=\{\bmx_{n_0+1},\bmx_{n_0+2},\dots,\bmx_{n_0+n}\}$ is obtained by to estimate the expectation. We avoid computing the high-dimensional integral by the cancellation of terms in the numerator and denominator in \eqref{eq: accept}.
The MH algorithm is summarized as Algorithm \ref{alg:MH}.
\begin{algorithm} %算法开始 
	
	\caption{Metropolis-Hasting algorithm}
	\begin{algorithmic}[1]
	\label{alg:MH}
		\REQUIRE Any initial configuration $\bmx_0$, the proposal distribution $Q(\bmx^{\prime}|\bmx)$.
		\FOR{ $i=0,1,2,\dots$}
		\STATE Draw a sample $\bmx^{\prime}$ from $Q(\bmx^{\prime}|\bmx_i)$.
		\STATE Generate $u\sim U[0,1]$ and compute the acceptance probability $a(\bmx^{\prime}|\bmx_i)$ defined by \eqref{eq: accept}.
		\IF{ $u\leq a(\bmx^{\prime}|\bmx_i)$ }
		\STATE Accept the proposal sample and set $\bmx_{i+1}=\bmx^{\prime}$.
		\ELSE 
		\STATE Reject it and set $\bmx_{i+1}=\bmx_{i}$.
		\ENDIF 
		\ENDFOR
	\end{algorithmic}
\end{algorithm}


\subsection{Stochastic gradient descent using MCMC estimator}

The objective function and its gradient is estimated by the MCMC samples $\mathbf{S}$ generated by the MH Algorithm \ref{alg:MH}:
\begin{align}
	\hat{\Lcal}(\parameter;\mathbf{S})&=\frac{1}{|\mathbf{S}|}\sum_{\bmx\in \mathbf{S}}E_{\parameter}(\bmx),\\
	\hat{g}(\parameter;\mathbf{S})&=\frac{2}{|\mathbf{S}|}\sum_{\bmx\in \mathbf{S}}\big(E_{\parameter}(\bmx)-\frac{1}{|\mathbf{S}|}\sum_{\bmx\in \mathbf{S}}E_{\parameter}(\bmx)\big)\nabla_{\parameter} \log \Psi_{\parameter}(\bmx).\label{eq:approxgrad}
\end{align}
% \begin{align}
% 	\hat{\Lcal}(\parameter;\mathbf{S})&=\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}E_{\parameter}(\bmx_i),\\
% 	\hat{g}(\parameter;\mathbf{S})&=\frac{2}{n}\sum_{i=n_0+1}^{n_0+n}\big(E_{\parameter}(\bmx_i)-\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}E_{\parameter}(\bmx_i)\big)\nabla \log \Psi_{\parameter}(\bmx_i).\label{eq:approxgrad}
% \end{align}
 The MCMC methods provide a tractable and efficient way of approximating expectation in the objective function and its gradient. Nevertheless, MCMC estimators are not generally unbiased for a finite sample size and the samples generated by MCMC are non-independent. The existence of bias implies that the expectation of our stochastic gradient is not equal to the exact gradient. A sufficiently large sample size should be applied for reducing the bias, while the size should not be too large to lose randomness and computability. It suggests that we should pay attention to the influence of sampling methods in optimization. The MCMC method enables us to apply the SGD to update the parameters:
\begin{equation}
	\label{eq:iter}
	\parameter_{k+1}= \parameter_{k} - \alpha_k\hat{g}(\parameter_k,\mathbf{S}_{k}),
\end{equation}
where $\alpha_k$ are chosen stepsizes and $\mathbf{S}_k$ can be obtained by the MH algorithm with $\pi_{\parameter_{k}}$. 

% Additionally and not necessary, for noise reduction, we clip those outliers in local energies calculated by MCMC algorithm to get better estimate of the gradient. When we already have a batch of local energies $E_{\parameter_k}(\bmx_{i}^{(k)})$, replace those with an extreme deviation 
% \begin{equation}
% 	\label{ineq:clip}
% 	\begin{aligned}
% 		\hat{g}^{clip}(\parameter,\mathbf{S})&=\frac{2}{n}\sum_{i=n_0+1}^{n_0+n}\hat{E}_{\parameter}\nabla \log \Psi_{\parameter}(\bmx_i)\\
% 		\hat{E}_{\parameter}&=\begin{cases}~E_{\parameter_k}(\bmx_i)-\hat{\Lcal}(\parameter,\mathbf{S})&\mathrm{if}~\left|E_{\parameter_k}(\bmx_i)-\hat{\Lcal}(\parameter,\mathbf{S})\right|\leq R,\\
% 		~0 &\mathrm{else}.
% 		\end{cases}
% 	\end{aligned}
% \end{equation}
% by the mean energy $\hat{\Lcal}(\parameter,\mathbf{S})$, where $R$ is a hyperparameter. In practice, it is verified that this trick reduces the risk of abnormal gradients and improve stability.
\begin{algorithm} %算法开始 

	\setstretch{1.15}
	\caption{Variational Monte Carlo}
	\begin{algorithmic}[1]
		\label{alg:VMC}
		\REQUIRE The Hamiltonian $\Hcal$, the initialized parameter $\parameter_0$, the sample size $n$ and the length of the burn-in period $n_0$.
		\FOR{ $k=0,1,2,\dots$}
		\STATE Draw $n$ samples $\mathbf{S}_k=\{\bmx_{n_0+1}^{(k)},\dots,\bmx^{(k)}_{n_0+n}\}$ by MH algorithm after a burn-in period of $n_0$. 
		\STATE Compute the estimated gradient $\hat{g}(\parameter_k,\mathbf{S}_{k})$ by \eqref{eq:approxgrad}.
		\STATE Update the parameter by \eqref{eq:iter} with the stepsize $\alpha_k$.
		\ENDFOR
	\end{algorithmic}
\end{algorithm} 


\section{Convergence analysis of VMC}
\label{sec:conv}

In this section, we study the convergence of optimization in the VMC method. Some assumptions are imposed to ensure indispensable properties in the objective function and sampling. Then, we analyze the MCMC estimator by the concentration inequality for Markov chains, which is essentially different from the ordinary SGD. Finally, our convergence theorems for VMC are presented through the gradient norm in expectation.
\subsection{Assumptions}
\label{subsec:asm}
We first give the definition of sub-exponential random variables to introduce our assumptions.
\begin{definition}
    The sub-exponential norm of a random variable $X$ is 
    \begin{equation}
        \|X\|_{\psi_1}=\inf \left\{t > 0 : \epct{\exp\lrp{\frac{|X-\Ebb[X]|}{t}}}\leq 2\right\}.
    \end{equation}
    If $\|X\|_{\psi_1}$ is finite, we say that $X$ is sub-exponential with the parameter $\|X\|_{\psi_1}$.

    % (2) The sub-gaussian norm of a random variable $X$ is 
    % \begin{equation}
    %     \|X\|_{\psi_2}=\inf \left\{t > 0 : \epct{\exp\lrp{\frac{|X|^2}{t}}}\leq 2\right\}.
    % \end{equation}
    % If $\|X\|_{\psi_2}$ is finite, we say that $X$ is  sub-gaussian with the parameter $\|X\|_{\psi_2}$.
\end{definition}

We introduce some regularity conditions on the trial wavefunction $\Psi_{\parameter}$, in order to guarantee Lipschitz continuity of the gradient $g(\parameter)$. In practice, we usually substitute $\Psi_{\parameter}(\bmx)$ by $\log \Psi_{\parameter}(\bmx)$ to avoid potential numerical instability. The following assumptions can be satisfied in most of ansatzes.
\begin{assumption}
    \label{asm:wavefun}
    Let $\log\Psi_{\parameter}(\bmx)$ be differentiable with respect to the parameters $\parameter\in \Rbb^d$ for any $\bmx\in \Xcal$. There exist constants $B,L_1>0$ such that
    \begin{enumerate}
        \setlength{\itemsep}{2pt}
        \item[(1)] $\sup\limits_{\parameter\in \Rbb^d}\sup\limits_{\bmx \in \Xcal}\norm{\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)}\leq B$,
        \item[(2)] $\sup\limits_{\parameter\in \Rbb^d}\Ebb_{\pi_{\parameter}}\lb \norm{\nabla_{\parameter}^2\log \Psi_{\parameter}(\bmx)}^2\rb\leq L_1^2$.
    \end{enumerate}
\end{assumption}


There are also some assumptions on the local energy $E_{\parameter}$. Noticing that the local energy may be unbounded for trial function, we give the sub-exponential assumption of the local energy under the distribution $\pi_{\parameter}$.
\begin{assumption}
    \label{asm:local}
    Let the local energy $E_{\parameter}(\bmx)$ defined in \eqref{loss:vmc} satisfy the following conditions. There exist constants $M,L_2>0$ such that
    \begin{enumerate}
        \setlength{\itemsep}{2pt}
        \item[(1)] $\sup\limits_{\parameter\in \Rbb^d}\norm{\oE_{\parameter}(\bmx)}_{\psi_1}\leq M$,
        \item[(2)] $\sup\limits_{\parameter\in \Rbb^d}\Ebb_{\pi_{\parameter}}\lb \norm{\nabla_{\parameter}\oE_{\parameter}(\bmx)}^2\rb\leq L_2^2$.
    \end{enumerate}
    
    % (3) The local energy is locally Lipschitz continuous, namely for $\ell_{\parameter}(\bmx):=\nrm{\nabla_{\parameter}E_{\parameter}(\bmx)}$, its expectation over $\pi_{\parameter}(\bmx)$ can be bounded by some $\Lipe$. More specifically, we assume
    % \[\begin{aligned}
    % \EE_{\bmx\sim\pi_{\parameter}}\left[\ell_{\parameter}(\bmx)\right]=
    % \EE_{\bmx\sim\pi_{\parameter}}\left[\nrm{\nabla_{\parameter}E_{\parameter}(\bmx)}\right]
    % \leq L_e.
    % \end{aligned}\]
\end{assumption}

% \begin{assumption}
%     \label{asm:wavefun}
%     Let $\Psi_{\parameter}$ be a real wavefunction parameterized by $\parameter\in\Rbb^{d}$. We assume there exist constants $B_g,L_g >0$ such that 
%     \begin{enumerate}
%         \setlength{\itemsep}{2pt}
%         % \item[(1)] $|\log \Psi_{\parameter}(\bmx)|\leq B,~\forall \parameter \in \Rbb^{d},$
%         \item[(1)] $\Ebb_{\pi_{\parameter}}[\|\nabla_{\parameter} \log \Psi_{\parameter}(\bmx)\|^4]\leq B_g^4,~\forall \parameter \in \Rbb^{d},$
%         \item[(2)] $\Ebb_{\pi_{\parameter}}[\|\nabla_{\parameter}^2 \log \Psi_{\parameter}(\bmx)\|^2]\leq L_{g}^2,~\forall \parameter \in \Rbb^{d}.$
%     \end{enumerate}
% \end{assumption}

% The regularity assumptions on the distribution are similar to those in the convergence analysis of policy gradient in reinforcement learning, according to the literature \cite{wu2020finite}. It is reasonable for neural networks to achieve the assumption.
% Obviously, $\log \Psi_{\parameter}(\bmx)$ is also Lipschitz continuous with the constant $B_g$ due to its bounded gradient.

% Based on this assumption, we show the Lipschitz continuity of the probability and the local energy. Before the lemmas, there are some facts of Lipschitz continuous functions:
% \begin{lemma}
%     \label{lem:Lips}
%     Let $f,g$ be any Lipschitz continuous functions of $\parameter$ with constants $L_1,L_2 > 0$, bounded by $B_1,B_2 > 0$, then it holds that 
%     \begin{enumerate}
%         \item  $e^{f}$ is Lipschitz continuous with the constant $e^{B_1}L_1$;
%         \item  $f+g$ is Lipschitz continuous with the constant $L_1+L_2$;
%         \item  $fg$ is Lipschitz continuous with the constant $B_1L_2+B_2L_1$.
%     \end{enumerate}
% \end{lemma}
% \begin{proof}
%     \begin{enumerate}
%         \item Using Lagrange's mean theorem, the inequality holds that 
%         \begin{align*}
%             |e^{f(\parameter_1)}-e^{f(\parameter_1)}|\leq \max_{\parameter}\{e^{f(\parameter)}\}\left|f(\parameter_1)-f(\parameter_2)\right|\leq e^{B_1}L_1\|\parameter_1-\parameter_2\|.
%         \end{align*}
%         \item Obviously.
%         \item It holds that
%         \begin{align*}
%             |f(\parameter_1)g(\parameter_1)-f(\parameter_2)g(\parameter_2)|&\leq |f(\parameter_1)g(\parameter_1)-f(\parameter_1)g(\parameter_2)|+|f(\parameter_1)g(\parameter_2)-f(\parameter_2)g(\parameter_2)|\\
%             &\leq B_1 L_1\|\parameter_1-\parameter_2\|+B_2 L_2\|\parameter_1-\parameter_2\|.
%         \end{align*}
%     \end{enumerate} 
% \end{proof}


% \begin{proof} By Lemma \ref{lem:Lips}, we have
% \begin{align*}
%     \|\pi_{\parameter_1}(\bmx)-\pi_{\parameter_2}(\bmx)\|&=\left|\frac{e^{2\log \Psi_{\parameter_1}(\bmx)}}{\sum_{\bmx^{\prime}}e^{2\log \Psi_{\parameter_1}(\bmx^{\prime})}}-\frac{e^{2\log \Psi_{\parameter_2}(\bmx)}}{\sum_{\bmx^{\prime}}e^{2\log \Psi_{\parameter_2}(\bmx^{\prime})}}\right|\\
%     &\leq \left|\frac{e^{2\log \Psi_{\parameter_1}(\bmx)}-e^{2\log \Psi_{\parameter_2}(\bmx)}}{\sum_{\bmx^{\prime}}e^{2\log \Psi_{\parameter_1}(\bmx^{\prime})}}\right|+\left|e^{2\log \Psi_{\parameter_2}(\bmx)}\right|\left|\frac{1}{\sum_{\bmx^{\prime}}e^{2\log \Psi_{\parameter_1}(\bmx^{\prime})}}-\frac{1}{\sum_{\bmx^{\prime}}e^{2\log \Psi_{\parameter_2}(\bmx^{\prime})}}\right|\\
%     &\leq \frac{1}{2^{N}e^{-2B}}e^{2B}2B_g\|\parameter_1-\parameter_2\|+e^{2B}\frac{1}{(2^{N}e^{-2B})^2}2^{N}e^{2B}2B_g\|\parameter_1-\parameter_2\|\\
%     &=C_{\pi}\|\parameter_1-\parameter_2\|.
% \end{align*}
% where we denote that $C_{\pi}=\frac{B_g}{2^{N-1}}(e^{4B}+e^{8B})$.
% \end{proof}

% In practice, the local energy is computed in the form \begin{equation}
%     \label{eq:Eloc}
%         E_{\parameter}(\bmx)=\sum_{\bmx^{\prime}}H_{\bmx,\bmx^{\prime}}e^{2\log \Psi_{\parameter}(\bmx^{\prime})-2\log \Psi_{\parameter}(\bmx)},
%     \end{equation}
%     where $H_{\bmx,\bmx^{\prime}}$ be the element of the Hamiltonian matrix defined in \eqref{ham:tensor}. If $\bmx$ is not relevant to $\bmx$ in Heisenberg model, which means that $\bmx$ can not transfer to $\bmx^{\prime}$ through flips on two neighboring spins, then $H_{\bmx,\bmx^{\prime}}=0$. It suggests that $H$ is a sparse matrix since relevant spin configurations are rare in a huge space of $2^{N}$.
% \begin{assumption}
%     \label{asm:Eloc}
%     For the local energy $E_{\parameter}(\bmx)$ defined in \eqref{loss:vmc}, there exist a bound $B_e$ and a Lipschitz constant $C_{\pi}>0$ such that for all $\bmx$, it holds that
%     \begin{enumerate}
%         \item $|E_{\parameter}(\bmx)|\leq B_e,~\forall \parameter \in \Rbb^{d},$
%         \item $|E_{\parameter_1}(\bmx)-E_{\parameter_2}(\bmx)|\leq L_e\|\parameter_1-\parameter_2\|,~\forall \parameter_1,\parameter_2 \in \Rbb^{d}.$
%     \end{enumerate}
% \end{assumption}
% \begin{proof}\begin{enumerate}
%         \item Let $H_{\max}=\max_{\bmx,\bmx^{\prime}}|H_{\bmx,\bmx^{\prime}}|$, then it holds that 
%         \begin{align*}
%             |E_{\parameter}(\bmx)|=\left|\sum_{\bmx^{\prime}}H_{\bmx,\bmx^{\prime}}e^{2\log \Psi_{\parameter}(\bmx^{\prime})-2\log \Psi_{\parameter}(\bmx)}\right|\leq \sum_{\bmx^{\prime}}H_{\max}e^{4B}=2^{N}H_{\max}e^{4B}=:B_e.    \end{align*}
%         \item By Lemma \ref{lem:Lips}, we have
%         \begin{align*}
%             |E_{\parameter_1}(\bmx)-E_{\parameter_2}(\bmx)|&=\left|\sum_{\sigma^{\prime}}H_{\bmx,\bmx^{\prime}}\left(e^{2\log \Psi_{\parameter_1}(\bmx^{\prime})-2\log \Psi_{\parameter_1}(\bmx)}-e^{2\log \Psi_{\parameter_2}(\bmx^{\prime})-2\log \Psi_{\parameter_2}(\bmx)}\right)\right|\\
%             &\leq 2^{N} H_{\max} e^{4B}\cdot 4B_g\|\parameter_1-\parameter_2\|=L_e\|\parameter_1-\parameter_2\|,
%         \end{align*}
%         since $2\log \Psi_{\parameter}(\bmx^{\prime})-2\log \Psi_{\parameter}(\bmx)$ be bounded by $4B$ and has the Lipschitz constant $4B_g$.
%     \end{enumerate}
% \end{proof}
% Beside the regularity conditions on the wavefunction, there is a general assumption on the local energy for different types of Hamiltonian. For example, the local energy in Heisenberg model has a uniform bound while that in Schr\"odinger equation \eqref{hamiltonian:SE} is unbounded. Compared to the bounded condition, there are some weaker assumptions on the local energy $E_{\parameter}$ under the distribution $\pi_{\parameter}$.

% Besides, we may have an alternative assumption that claims $\|\nabla_{\parameter} \log \Psi_{\parameter}(\bmx)\|$ and $\bar{E}_{\parameter}$ are all sub-gaussian with respect to $\pi_{\parameter}$.
% \begin{assumption}
%     \label{asm:wavefun2}
%     Let $\Psi_{\parameter}$ be the real parameterized wavefunction and $\bar{E}_{\parameter} $ is the local energy function. We assume there exist constants $M_g,M_e >0$ such that 
%     \begin{enumerate}
%         \setlength{\itemsep}{2pt}
%         \item[(1)] $ \|\nabla_{\parameter} \log \Psi_{\parameter}\|$ is sub-gaussian with the parameter $M_g$, that is 
%         \begin{equation*}
%             \Ebb_{\pi_{\parameter}}\left[\exp\lrp{\frac{\|\nabla_{\parameter} \log \Psi_{\parameter}(\bmx)\|^2}{M_g}}\right]\leq 2,~\forall \parameter \in \Rbb^{d},
%         \end{equation*}

%         \item[(2)] $ \bar{E}_{\parameter}$ is sub-gaussian with the parameter $M_e$, that is 
%         \begin{equation*}
%             \Ebb_{\pi_{\parameter}}\left[\exp\lrp{\frac{|\bar{E}_{\parameter}(\bmx)|^2}{M_e}}\right]\leq 2,~\forall \parameter \in \Rbb^{d}.
%         \end{equation*}
%     \end{enumerate}
% \end{assumption}



Under Assumptions \ref{asm:wavefun} and \ref{asm:local}, we are able to analyze the objective function $\Lcal(\parameter)$. Since the wavefunction $\Psi_{\parameter}$ appears in the denominator of the local energy expression \eqref{loss:vmc}, and there are unbounded potentials in the many-electron Hamiltonian \eqref{hamiltonian:SE}, the above two assumptions seem not easy to be satisfied.
%\color{blue} Nevertheless, the choice of the most suited ansatz for the wavefunction, as well as the optimization of its parameters, is a prerequisite in the VMC calculations.}
For instances, when solving the many-electron Schr\"{o}dinger equations, people often use the so-called Jastrow factor \cite{gubernatis16} to enable the network to efficiently capture the  cusps and decay of the wavefunction.
By exploiting physical knowledge in the construction of the wavefunction ansatz can make the local energy $E_{\parameter}$ smooth with respect to the parameters $\parameter$, such that Assumption \ref{asm:local} is satisfied in practical VMC calculations.

% The sub-exponential random variables have the following properties. 
% \begin{lemma}\label{lemma:sub-exp}
%     Assume the sub-exponential norm of a random variable $X$ is bounded by $M$. Then the following holds.

%     (1) For any $t\geq 0$, we have the tail bound of $X$
%     \[\begin{aligned}
%         \Pbb(\left|X\right|\geq t) \leq 2 \exp \left(-\frac{t}{M}\right).
%     \end{aligned}\]

%     (2) For $p=1,2,\dots$, the absolute $p$-th moment of $X$ satisfies
%     \begin{equation*}
%         \epct{\left|X\right|^p}\leq 2 \cdot p! M^{p}.
%     \end{equation*}
% \end{lemma}
% \begin{proof}
%     (1) Using Markov's inequality, it holds for any $t>0$,
%     \begin{align*}
%         \Pbb(|X|\geq t)=\Pbb(e^{\frac{|X-\epct{X}|}{M}}\geq e^{ \frac{t}{M}})\leq e^{-\frac{t}{M}}\epct{e^{\frac{|X-\epct{X}|}{M}}}\leq  2\exp \left(-\frac{t}{M}\right).
%     \end{align*}

%     (2) Due to the tail bound, the following holds for the absolute $p$-th moment of $X$,
%     \begin{equation*}
%         \begin{aligned}
%             \epct{\left|X\right|^p}&=\int_{0}^{+\infty}p s^{p-1}\Pbb(\left|X \right|\geq s)d s\\
%             &\leq 2 p \int_{0}^{+\infty}  s^{p-1}\exp \left(-\frac{t}{M}\right)d s\\
%             &=2\cdot p! M^{p}.
%         \end{aligned} 
%     \end{equation*}

% \end{proof}




% \begin{assumption}
%     {\color{blue} (This assumption makes Assumption 4.1 redundent.)} Either one of the following statements holds true:
    
%     (1) Both $\bar{E}_{\parameter}(\bmx)$ and $\nabla_{\parameter} \log \Psi_{\parameter}(\bmx)$ are subgaussian random variables under $\bmx\sim\pi_{\parameter}$.

%     (2) $\nabla_{\parameter} \log \Psi_{\parameter}(\bmx)$ is uniformly bounded, and Assumption 4.4(2) holds.
% \end{assumption}





% \begin{proof} We can obtain the gradient $g(\parameter)$ by \eqref{eq:grad}. We rewrite its real-valued version in the same way, then the gradient is 
% \begin{align*}
%     \nabla l(\parameter)&=2\Ebb_{\bmx\sim\pi_{\parameter}}\left[(E_{\parameter}(\bmx)-\Ebb_{\bmx\sim\pi_{\parameter}}\left[E_{\parameter}(\bmx)\right])\nabla_{\parameter}\log\Psi_{\parameter}(\bmx)\right],\\
%     &=2\sum_{\bmx}\pi_{\parameter}(\bmx)E_{\parameter}(\bmx)\log\Psi_{\parameter}(\bmx)-2\left(\sum_{\bmx}\pi_{\parameter}(\bmx)E_{\parameter}(\bmx)\right)\left(\sum_{\bmx}\pi_{\parameter}(\bmx)\log\Psi_{\parameter}(\bmx)\right).
% \end{align*}
% Lemma \ref{lem:Lips} suggests that the addition and multiplication of two bounded Lipschitz function is also Lipschitz continuous. Lemma \ref{lem:prob} and Lemma \ref{lem:Eloc} show that $\pi_{\parameter}(\bmx)$ and $E_{\parameter}(\bmx)$ is bounded and Lipschitz continuous for all $\bmx$. Meanwhile, $\nabla \log \Psi_{\parameter}(\bmx)$ has also boundness and Lipschitz continuity under Assumption \ref{asm:wavefun}. Hence, it implies the gradient is also Lipschitz continuous with constant $L=2^{N+1}(3C_{\pi}B_eB_g+2L_eB_g+2L_gB_e)$.
% \end{proof}

\subsection{Analysis of the MCMC error}
\label{subsec:error}
%% Markov chain definition

We introduce following notations which are used frequently throughout this paper. For any function $f:\Xcal\rightarrow \Rbb$ and any distribution $\pi$ on $\Xcal$, we write its expectation $\Ebb_\pi[f]:=\int f(x)\pi(dx)$ and $p$-th central moment $\sigma^p_p[f]:=\Ebb[(f-\Ebb_\pi[f])^p]$. The second central moment, called the variance, is denoted by $\mathrm{Var}_{\pi}[f]=\sigma_2^2[f]:=\Ebb[(f-\Ebb_\pi[f])^2]$. 

Before the analysis of MCMC methods, we provide several general definitions about Markov chains. 
% and $L_{0}^2(\pi)=\{f\in L^{2}(\pi):\Ebb_\pi[f]=0 \}$ be its subspace of $\pi$-measure zero functions. 
Within our consideration, the state space $\Xcal$ is Polish and equipped with its $\sigma$-algebra $\Bcal$. Let $\{X_i\}_{i=1}^{n}$ be a time-homogeneous Markov chain defined on $\Xcal$. The distribution of the Markov chain is uniquely determined by its initial distribution $\nu$ and its transition kernel $P$. For any Borel set $A\in \Bcal$, let 
\begin{equation*}
    \begin{aligned}
        \nu(A)=\Pbb(X_1\in A),\quad P(X_i,A)=\Pbb(X_{i+1}\in A|X_i).
    \end{aligned}
\end{equation*}
A distribution $\pi$ is called stationary with respect to a transition kernel $P$ if 
\begin{equation*}
    \pi(A)=\int P(x,A)\pi(dx), ~\forall A\in \Bcal.
\end{equation*}
When the initial distribution $\nu=\pi$, we call the Markov chain stationary.

Our analysis starts from the perspective of operator theory on Hilbert spaces. Let $\pi$ be the stationary distribution of a Markov chain and $L^{2}(\pi)=\{f:\Ebb_\pi[f^2]<\infty \}$ be the Hilbert space equipped with the norm $\norm{f}_{\pi}=(\Ebb_\pi[f^2])^{1/2}$. And for any $f\in L^2(\pi)$, we define $\norm{f}_{\psi_1}$ by $\norm{f(X)}_{\psi_1} $ with $X\sim\pi$. Each transition kernel can be viewed as a Markov operator on the Hilbert space $L^2(\pi)$. The Markov operator $\operatorname{P}:L^{2}(\pi)\rightarrow L^{2}(\pi)$ is defined by 
\begin{equation*}
    \begin{aligned}
        \operatorname{P}f(x)=\int f(y)P(x,dy),~\forall x\in \Xcal,~\forall f \in L^{2}(\pi).
    \end{aligned}
\end{equation*}
It is easy to show that $\operatorname{P}$ has the largest eigenvalue $1$. Intuitively but not strictly, the gap between $1$ and other eigenvalues matters to the Markov chain from non-stationarity towards stationarity. Hence, we introduce the definition of the absolute spectral gap.
\begin{definition}[absolute spectral gap]
    \label{def:absgap}
     A Markov operator $\operatorname{P}:L^{2}(\pi)\rightarrow L^{2}(\pi)$ admits an absolute spectral gap $\gamma$ if 
    \begin{equation*}
        \gamma(\operatorname{P})=1-\lambda(\operatorname{P}):=1-\interleave \operatorname{P}-\Pi\interleave _{\pi}>0,
    \end{equation*}
    where  $\Pi:f\in L^2(\pi)\rightarrow\Ebb_\pi[f]\mathit{1}$ is the projection operator with $\mathit{1}$ denoting the identity operator and $\interleave\cdot\interleave_{\pi}$ is the operator norm induced by $\norm{\cdot}_{\pi}$ on $L^2(\pi)$.
\end{definition}
% \begin{definition}[right spectral gap]
%     \label{def:rsgap}
%     A Markov operator $P$ admits a right spectral gap $1-\gamma$ if 
%     \begin{equation*}
%         \gamma=\gamma(R):=\sup\{\langle Rf,f\rangle:f\in \Lcal^2_{0}(\pi)\},~~ where ~ R=(P+P^*)/2.
%     \end{equation*}
% \end{definition}
% By these definitions, we have
% \begin{equation}
%     \label{eq:gap}
%     |\gamma(R)|\leq \gamma(R)\leq \frac{\gamma(P)+\gamma(P^{*})}{2}=\gamma(P).
% \end{equation}


% \begin{remark}
%     For the finite-state time-homogeneous Markov chain, $\gamma(P)=\frac{\max\{|\gamma_2|,|\gamma_{min}|\}+1}{2}$ where $\gamma_2$ is actually the second largest eigenvalue of the transition matrix $P$ and $\gamma_{min}$ is the minimum eigenvalue. 
% \end{remark}

We consider the MCMC samples generated by the MH algorithm $\mathbf{S}=\{\bmx_{i}\}_{i=n_0+1}^{n_0+n}$ from the desired distribution $\pi_{\parameter}$. The Markov operator in the Metropolis way, denoted by $\operatorname{P}_{\parameter}$, determines the convergence rate of the Markov chain. We assume that there exists a uniform lower bound of the spectral gap.  
\begin{assumption}
    \label{asm:unigap}
    Let $\operatorname{P}_{\parameter}$ be the Markov operator induced by the MH algorithm with the absolute spectral gap $\gamma(\operatorname{P}_{\parameter})$. For any $\parameter\in \Rbb^d$, there is a positive lower bound of absolute spectral gaps, that is,
    \begin{equation*}
        \gamma := \inf_{\parameter\in\Rbb^d}\gamma(\operatorname{P}_{\parameter}) >0.
    \end{equation*}
\end{assumption}

This lemma excludes the situation that $\inf_{\parameter\in\Rbb^d}\gamma(\operatorname{P}_{\parameter})=0$, which means the spectral gap might converge to zero in the iteration. The spectral gap $\gamma$ ensures the Markov chains to mix well enough, by which the gradient is approximated correctly.

The spectral gap of the MH algorithm has been studied for some specific examples. On finite-state spaces, $\operatorname{P}$ becomes a transition matrix while $1-\gamma(\operatorname{P})$ relates to its eigenvalue.  A survey of spectrums for Markov chains on discrete state-spaces is in \cite{saloff1997lectures}. This develops amounts of analytic techniques and \cite{diaconis1998we} has further applications to the MH algorithm. For the continuous spaces, there are few examples of sharp rates of convergence for the MH algorithm. In \cite{kienitz2000convergence,miclo2000trous}, it is claimed that the spectral gap $\gamma\sim O(h^2)$ for the Gaussian proposal $Q(x^{\prime}|x)\sim \exp\left(-\tfrac{1}{2h^2}\norm{x^{\prime}-x}^2\right)$.

MCMC provides an approach to estimate the expectation in VMC by averaging Markov chain samples after a burn-in period. A Bernstein inequality for general Markov chains is proposed by Jiang and Fan \cite{jiang2018bernstein} and beneficial to our analysis. The following lemma is a direct corollary of \cite[Theorem 2]{jiang2018bernstein}
%Then, Fan et al. (2021) derive a non-asymptotic error bound for MCMC estimation using Hoeffding's inequality \cite{fan2021hoeffding}. A corollary is obtained by these two references without much effort.

\begin{lemma}%[Jiang and Fan, 2018]
    \label{lem:Bern}
    Let $\{X_i\}_{i= 1}^{n}$ be a Markov chain with stationary distribution $\pi$ and absolute spectral gap $\gamma$. Suppose the initial distribution $\nu$ is absolute continuous with respect to the stationary distribution $\pi$ and its derivative $\tfrac{d\nu}{d\pi}\in L^2(\pi)$. Consider a bounded function $f:\mathcal{X}\rightarrow [-c,c]$ with $\Ebb_\pi[f]=0$ and variance $\sigma_2^2[f]$. Then, when $\nu=\pi$ , that is, $\{X_i\}_{i= 1}^{n}$ is stationary, it holds that
    \begin{equation}
        \label{eq:Bern-stat}
        \mathbb{P}_{\pi}\left(\frac{1}{n} \sum_{i=1}^{n} f\left(X_{i}\right)\geq s\right) 
    \leq \exp\left(-\frac{\gamma ns^2}{4\sigma^2+5cs}\right), \qquad \forall s\geq 0.
    \end{equation}
    % Therefore, for the general case, it holds that
    % \begin{equation}
    %     \label{eq:Bern}
    %     \mathbb{P}_{\nu}\left(\frac{1}{n} \sum_{i=1}^{n} f\left(X_{i}\right)\geq s\right) 
    % \leq (1+\rchi^2(\nu,\pi))^{\frac{1}{2}} \exp\left(-\frac{\gamma ns^2}{8\sigma^2+10cs}\right), \qquad \forall s\geq 0.
    % \end{equation}
    % where $\rchi^2(\nu,\pi):=\norm{\tfrac{d\nu}{d\pi}-1}^{2}_{\pi}$ represents the chi-squared divergence between $\nu$ and $\pi$.
\end{lemma}

% \begin{lemma}[Jiang and Fan, 2018]
%     \label{lem:Bern}
%     Let $\{X_i\}_{i= 1}^{n}$ be a non-stationary Markov chain with stationary distribution $\pi$ and absolute spectral gap $\gamma$. Suppose the initial distribution $\nu$ is absolute continuous with respect to the stationary distribution $\pi$ and its derivative $\tfrac{d\nu}{d\pi}\in L^2(\pi)$. Consider a bounded function $f:\mathcal{X}\rightarrow [-c,c]$ with variance $\sigma_2^2[f]$. Then, for any $ |t|<\frac{\gamma}{ 10 c}$, it holds that
%     % \begin{equation}
%     %     \mathbb{E}\left[e^{t \left(\sum_{i=n_0+1}^{n_0+n} f\left(X_{i}\right)-n\pi(f)\right)}\right] 
%     %     \leq C\exp \left(\frac{n \sigma^{2}}{c^{2}}\left(e^{t qc}-1-t qc\right)+\frac{n \sigma^{2} \max \left\{\gamma, 0\right\} q^2t^{2}}{1-\max \left\{\gamma, 0\right\}-5 qc t}\right). 
%     % \end{equation}
%     % Moreover, for any $\epsilon>0$,
%     % \begin{equation}
%     %     \mathbb{P}\left(\frac{1}{n} \sum_{i=n_0+1}^{n_0+n} f\left(X_{i}\right)-n\pi(f)>\epsilon\right) 
%     % \leq C\exp \left(-\frac{n \epsilon^{2} / 2}{\alpha_{1}\left(\max \left\{\gamma, 0\right\}\right) \cdot q\sigma^{2}+\alpha_{2}\left(\max \left\{\gamma, 0\right\}\right) \cdot qc \epsilon}\right),
%     % \end{equation}
%     % where $\alpha_1,\alpha_2$ are defined as
%     % \[\begin{aligned}
%     %     \alpha_{1}(\gamma)=\frac{1+\gamma}{1-\gamma}, \quad \alpha_{2}(\gamma)= \begin{cases}\frac{1}{3} & \text { if } \gamma=0 \\ \frac{5}{1-\gamma} & \text { if } \gamma \in(0,1)\end{cases}
%     % \end{aligned}\]

%     % Especially, we can simplify for $0 \leq t<\left(1-\max \left\{\gamma, 0\right\}\right) / 10 qc$
%     \begin{equation}
%         \label{eq:Bern}
%         \mathbb{E}_{\nu}\left[e^{t \left(\sum_{i=1}^{n} f\left(X_{i}\right)-n\Ebb_\pi[f]\right)}\right] 
%     \leq (1+\rchi^2(\nu,\pi))^{\frac{1}{2}}\exp\left(\frac{16n\sigma_2^2[f]  t^2}{\gamma}\right),
%     \end{equation}
%     where $\rchi^2(\nu,\pi):=\norm{\tfrac{d\nu}{d\pi}-1}^{2}_{\pi}$ represents the chi-squared divergence between $\nu$ and $\pi$.
% \end{lemma}

The Bernstein inequality \eqref{eq:Bern-stat} shows how the average of MCMC samples concentrates at the expectation. However, the Markov chain is not always non-stationary. We define $\rchi^2(\nu,\pi):=\norm{\tfrac{d\nu}{d\pi}-1}^{2}_{\pi}$ represents the chi-squared divergence between $\nu$ and $\pi$, by which we can extend the Bernstein inequality into a non-stationary one. We abbreviate $\rchi=\rchi(\nu,\pi)$ and $C=(1+\rchi^2)^{\frac{1}{2}}$. Then we give the tail bound of the MCMC estimator for sub-exponential functions in the following lemma.

\begin{lemma}
    \label{lem:Bern-exp}
    Let $\{X_i\}_{i= 1}^{n}$ be a non-stationary Markov chain with stationary distribution $\pi$ and absolute spectral gap $\gamma$. Suppose the initial distribution $\nu$ is absolute continuous with respect to the stationary distribution $\pi$ and its derivative $\tfrac{d\nu}{d\pi}\in L^2(\pi)$.  We consider a function $f\in L^2(\pi)$ satisfying $ \norm{f}_{\psi_1}\leq M$. If $s\geq \frac{20M(\log n)^2}{n}$, the following tail bound holds,  
    \begin{equation}
        \mathbb{P}_{\nu}\left(\abs{ \frac{1}{n} \sum_{i=1}^{n} f\left(X_{i}\right)-\Ebb_\pi[f] }\geq s\right) 
    \leq 2C\exp\left(-\frac{\gamma ns^2}{64\sigma^2_2[f]}\right) + 2C\exp\left(-\sqrt\frac{\gamma ns}{80M}\right).
    \end{equation}
    In other words, for $\delta>0$, with probability at least $1-\delta$, it holds that
    \begin{equation}
        \label{eq:highprobbound}
        \left|\frac{1}{n}\sum_{i=1}^{n}f(X_i)-\Ebb_\pi[f]\right|\leq 8\sigma_2[f]\sqrt{\frac{\log(4C/\delta)}{n\gamma}}+ 80M\frac{[\log(4Cn/\delta)]^2}{n\gamma}.
    \end{equation}
\end{lemma}

\begin{proof}
    Without loss of generality, we assume that $\Ebb_\pi[f]=0$ in the following proof.
    To deal with a possibly unbounded $f$, we firstly fix a $M'>0$ and consider the truncation function $\bar{f}=\max\{\min\{f,M^{\prime}\},-M^{\prime}\}$ and $\hat{f}=f-\bar{f}$. The basic property of $\hat{f}$ is that, as $\norm{f(X)}_{\psi_1}\leq M$, the Markov's inequality implies 
    \begin{equation}
        \label{eq:subexp-tail}
        \Pbb_\pi(\abs{f}>s)\leq \exp\left(-\frac{s}{M}\right)\Ebb_\pi\left[\exp\left(\frac{|f|}{M}\right)\right] \leq 2\exp\left(-\frac{s}{M}\right).
    \end{equation}
    % Then, for any $p\geq 1$, we have
    % \begin{equation}
    %     \label{eq:pmonent-hatf}
    %     \begin{aligned}
    %         \Ebb_\pi\big[|\hat{f}|^p\big]&=\int_{0}^{+\infty} ps^{p-1}\Pbb_\pi(|\hat{f}|>s)ds\leq \int_{0}^{+\infty} ps^{p-1}\Pbb_\pi(|f|>s+M^{\prime})ds\\
    %         &\leq 2p \int_{0}^{+\infty} s^{p-1}\exp\left(-\frac{s+M^{\prime}}{M}\right)ds=2\cdot p! M^{p}\exp\left(-\frac{M^{\prime}}{M}\right).
    %     \end{aligned}
    % \end{equation}
    
    % By introducing the truncation $\bar{f},\hat{f}$, we make the corresponding decomposition,
    % \begin{align*}
    %     &\Delta:=\frac{1}{n}\sum_{i=1}^{n}f(X_i)=\bar{\Delta}+\hat{\Delta},\\
    %     &\bar{\Delta}:=\frac{1}{n}\sum_{i=1}^{n}\bar{f}(X_i)-\Ebb_\pi[\bar{f}],~~
    %     \hat{\Delta}:=\frac{1}{n}\sum_{i=1}^{n}\hat{f}(X_i)-\Ebb_\pi[\hat{f}].
    % \end{align*}
    % We analyze the error from two aspects.
    % \paragraph{High probability bound} 
    Notice that for any event $A\in\sigma(X_1,\cdots,X_n)$, it holds from the Cauchy-Schwarz inequality
    \begin{align*}
        \Pbb_{\nu}(A)
        =&~ \int_{\mathcal{X}} \Pbb(A|X_1=x)\nu(dx) 
        = \int_{A} \Pbb(A|X_1=x)\frac{d\nu}{d\pi}\pi(dx)\\
        \leq&~ \sqrt{\int_{\mathcal{X}} \left(\frac{d\nu}{d\pi}\right)^2\Pbb(A|X_1=x)\pi(dx) \int_{\mathcal{X}} \Pbb(A|X_1=x) \pi(dx) } \\
        \leq&~ C\sqrt{\Pbb_{\pi}(A)}.
    \end{align*}
    Hence, we only consider the case $\nu=\pi$, i.e., the case $\{X_i\}_{i=1}^{n}$ is stationary.
    We first fix a large $M^{\prime}>0$. For any $1\leq i\leq n$, we have 
    \begin{equation}
        \label{eq:highprob1}
        \begin{aligned}
            \Pbb\left(\abs{f(X_i)}>M^{\prime} \right) \leq 2\exp\left(-\frac{M^{\prime}}{M}\right).
        \end{aligned}
    \end{equation} 
    It follows from the inclusion of events that
    \begin{equation}
        \begin{aligned}
            \mathbb{P}\left(\abs{ \frac{1}{n} \sum_{i=1}^{n} f\left(X_{i}\right) }\geq s\right)
            \leq&~\mathbb{P}\left(\abs{ \frac{1}{n} \sum_{i=1}^{n} \bar{f}\left(X_{i}\right) }\geq s\right) + \Pbb\left( \exists ~ 1\leq i\leq n,~ \abs{f(X_i)}>M^{\prime}\right)\\
            \leq&~\mathbb{P}\left(\abs{ \frac{1}{n} \sum_{i=1}^{n} \bar{f}\left(X_{i}\right) -\EE_{\pi}[\bar{f}]}\geq s-\abs{\EE_{\pi}[\bar{f}]}\right) + 2n\exp\left(-\frac{M^{\prime}}{M}\right).
        \end{aligned}
    \end{equation}
    Notice that $\abs{\EE_{\pi}[\bar{f}]}=\abs{\EE_{\pi}[\hat{f}]}\leq 2M\exp\left(-\frac{M^{\prime}}{M}\right)$. Therefore, when $s\geq 2\abs{\EE_{\pi}[\hat{f}]}$, it holds that
    \begin{equation}
        \begin{aligned}
            \mathbb{P}\left(\abs{ \frac{1}{n} \sum_{i=1}^{n} \bar{f}\left(X_{i}\right) }\geq s\right) 
            \leq&~\mathbb{P}\left(\abs{ \frac{1}{n} \sum_{i=1}^{n} \bar{f}\left(X_{i}\right) -\EE_{\pi}[\bar{f}]}\geq s-\abs{\EE_{\pi}[\bar{f}]}\right) \\
            \leq&~\mathbb{P}\left(\abs{ \frac{1}{n} \sum_{i=1}^{n} \bar{f}\left(X_{i}\right) -\EE_{\pi}[\bar{f}]}\geq \frac{s}{2}\right) \\
            \leq&~ 2\exp\left(-\frac{\gamma ns^2}{16\sigma^2_2[\bar{f}]+10M^{\prime}s}\right) 
            \leq 2\exp\left(-\frac{\gamma ns^2}{16\sigma^2_2[f]+10M^{\prime}s}\right),
        \end{aligned}
    \end{equation} 
    where the third inequality is due to \eqref{eq:Bern-stat}, and  the last inequality uses $\sigma^2_2[\bar{f}]\leq \sigma^2_2[f]$. Finally, for any fixed $s\geq \frac{20M(\log n)^2}{n}$, we take $M^{\prime}=\sqrt{\gamma nMs/5}$ and then obtain
    \begin{equation}
        \begin{aligned}
            \mathbb{P}\left(\abs{ \frac{1}{n} \sum_{i=1}^{n} f\left(X_{i}\right) }\geq s\right)
            \leq&~2\exp\left(-\frac{\gamma ns^2}{16\sigma^2_2[f]+10M^{\prime}s}\right) + 2n\exp\left(-\frac{M^{\prime}}{M}\right)\\
            =&~ 2\exp\left(-\frac{\gamma ns^2}{16\sigma^2_2[f]+10s\sqrt{\gamma nMs/5}}\right) + 2n\exp\left(-\sqrt\frac{\gamma ns}{5M}\right)\\
            \leq&~ 2\exp\left(-\frac{\gamma ns^2}{2\max\{16\sigma^2_2[f],10s\sqrt{\gamma nMs/5}\}}\right) + 2n\exp\left(-\sqrt\frac{\gamma ns}{5M}\right)\\
            \leq&~ 2\exp\left(-\frac{\gamma ns^2}{32\sigma^2_2[f]}\right) + 2\exp\left(-\sqrt\frac{\gamma ns}{20M}\right) + 2n\exp\left(-\sqrt\frac{\gamma ns}{5M}\right)\\
            \leq&~ 2\exp\left(-\frac{\gamma ns^2}{32\sigma^2_2[f]}\right) + 4\exp\left(-\sqrt\frac{\gamma ns}{20M}\right),
        \end{aligned}
    \end{equation}
    where the last inequality holds from $n\exp\left(-\sqrt\frac{\gamma ns}{5M}\right)\leq \exp\left(-\sqrt\frac{\gamma ns}{20M}\right)$ as long as $s\geq \frac{20M(\log n)^2}{n}$.
    This completes the proof.
\end{proof}

In the above lemma, we establish the tail bound of the MCMC estimator for unbounded Markov chains. To the best of our knowledge, this result has not appeared in the literatures of Bernstein inequality for Markov chains. 

\begin{lemma}
    \label{lem:BernBiasVar}
    Suppose that the condition of Lemma \ref{lem:Bern-exp} holds. Then, error bounds for the MCMC estimator are given as follows. 
    
    (1) The bias satisfies that
    \begin{equation}
        \label{eq:generalbias}
        \left|\Ebb_{\nu}\lrb{\frac{1}{n}\sum_{i=1}^{n}f(X_i)}-\Ebb_\pi[f]\right|
        \leq\frac{c_1}{n\gamma},
    \end{equation}
    where $c_1=\sigma_2[f]\minop{1,\rchi}+4M\pos{\log\rchi}^2+4M\pos{\log\rchi}$ and $\rchi=\rchi(\nu,\pi)$. In particular, when $\rchi\leq 1$, $c_1=\sigma_2[f]\rchi$.

    (2) The variance satisfies that
    \begin{equation}
        \label{eq:generalvariance}
        \Ebb_{\nu}\lrb{\left|\frac{1}{n}\sum_{i=1}^{n}f(X_i)-\Ebb_\pi[f]\right|^2} \leq\frac{c_2}{n\gamma}\sigma_2^2[f]+\frac{c_3+c_4\log^4 n}{n^2\gamma^2}M^2,
    \end{equation}
    where $c_2=64(1+\log 2C)$, $c_3=6400(4+\log 2C)^4$, $c_4=800$, and $C=(1+\rchi^2)^{\frac{1}{2}}$. 
\end{lemma}

\begin{proof}
    % Without loss of generality, we assume that $\Ebb_\pi[f]=0$ in the following proof.
    % To deal with a possibly unbounded $f$, we firstly fix a $M'>0$ and consider the truncation function $\bar{f}=\max\{\min\{f,M^{\prime}\},-M^{\prime}\}$ and $\hat{f}=f-\bar{f}$. The basic property of $\hat{f}$ is that, as $\norm{f(X)}_{\psi_1}\leq M$, the Markov's inequality implies 
    % \begin{equation}
    %     \label{eq:subexp-tail}
    %     \Pbb_\pi(\abs{f}>s)\leq \exp\left(-\frac{s}{M}\right)\Ebb_\pi\left[\exp\left(\frac{|f|}{M}\right)\right] \leq 2\exp\left(-\frac{s}{M}\right).
    % \end{equation}
    % Then, for any $p\geq 1$, we have
    % \begin{equation}
    %     \label{eq:pmonent-hatf}
    %     \begin{aligned}
    %         \Ebb_\pi\big[|\hat{f}|^p\big]&=\int_{0}^{+\infty} ps^{p-1}\Pbb_\pi(|\hat{f}|>s)ds\leq \int_{0}^{+\infty} ps^{p-1}\Pbb_\pi(|f|>s+M^{\prime})ds\\
    %         &\leq 2p \int_{0}^{+\infty} s^{p-1}\exp\left(-\frac{s+M^{\prime}}{M}\right)ds=2\cdot p! M^{p}\exp\left(-\frac{M^{\prime}}{M}\right).
    %     \end{aligned}
    % \end{equation}
    
    % By introducing the truncation $\bar{f},\hat{f}$, we make the corresponding decomposition,
    % \begin{align*}
    %     &\Delta:=\frac{1}{n}\sum_{i=1}^{n}f(X_i)=\bar{\Delta}+\hat{\Delta},\\
    %     &\bar{\Delta}:=\frac{1}{n}\sum_{i=1}^{n}\bar{f}(X_i)-\Ebb_\pi[\bar{f}],~~
    %     \hat{\Delta}:=\frac{1}{n}\sum_{i=1}^{n}\hat{f}(X_i)-\Ebb_\pi[\hat{f}].
    % \end{align*}
    % We analyze the error from two aspects.
    We follow the notations in the proof of Lemma \ref{lem:Bern-exp}.
    \paragraph{Bias bound} 
    Clearly, we only need to bound $\frac{1}{n}\sum_{i=1}^{n}\abs{\Ebb_{\nu_i}[f]-\Ebb_{\pi}[f]}$, where $\nu_i= \nu \oP^{i-1}$ is the distribution of $X_i$. 
    For the distribution $\pi^{\prime}$ whose derivative $\frac{d\pi^{\prime}}{d\pi}\in L^{2}(\pi)$, it holds from the Cauchy-Schwarz inequality that 
    \begin{align}\label{eq:change-measure}
        \abs{\Ebb_{\pi^{\prime}}[f]-\Ebb_\pi[f]}
        =\abs{ \EE_{\pi}\left[\left(\frac{d\pi^{\prime}}{d\pi}-1\right)(f-\Ebb_\pi[f])\right] }
        \leq  \left[\EE_{\pi}\left(\frac{d\pi^{\prime}}{d\pi}-1\right)^2\right]^{1/2}\sigma_2[f]
        = \rchi(\pi^{\prime},\pi)\sigma_2[f].
    \end{align}
    Besides, noticing that $\left(\Ebb_\pi \abs{\hat{f}}^2\right)^{\frac12}\leq 2M\exp\left(-\frac{M'}{2M}\right)$, we have
    \begin{align}
        \abs{\Ebb_{\pi^{\prime}}[\hat{f}]-\Ebb_{\pi}[\hat{f}]}
        \leq \rchi(\pi^{\prime},\pi) \left(\Ebb_\pi \abs{\hat{f}}^2\right)^{\frac12}
        \leq 2M\rchi(\pi^{\prime},\pi)\exp\left(-\frac{M'}{2M}\right).
    \end{align}
    % \begin{align}\label{eq:change-measure}
    %     \abs{\nu(f)-\pi(f)}
    %     =\abs{ \EE_{\pi}\left[\left(\frac{d\nu}{d\pi}-1\right)f\right] }
    %     \leq \sigma_f \left[\EE_{\pi}\left(\frac{d\nu}{d\pi}-1\right)^2\right]^{1/2}
    %     =\sigma_f\rchi(\nu,\pi).
    % \end{align}
    Then \eqref{eq:change-measure} implies that
    \begin{equation}
        \label{eq:change-function}
        \abs{\Ebb_{\nu_i}[f]-\Ebb_{\pi}[f]}
        \leq \abs{\Ebb_{\nu_i}[\bar{f}]-\Ebb_{\pi}[\bar{f}]}+\abs{\Ebb_{\nu_i}[\hat{f}]-\Ebb_{\pi}[\hat{f}]}
        \leq 2M'+2M\rchi(\nu_i,\pi)\exp\left(-\frac{M'}{2M}\right),
    \end{equation}
    and as long as $\rchi(\nu_i,\pi)\geq 1$, we can choose $M'=2M\log \rchi(\nu_i,\pi)$ in \eqref{eq:change-function} to derive $\abs{\Ebb_{\nu_i}[f]-\Ebb_{\pi}[f]}\leq 4M(1+\log \rchi(\nu_i,\pi))$.
    Combining \eqref{eq:change-measure} and \eqref{eq:change-function} yields that
    \begin{equation}
        \begin{aligned}
            \label{eq:nu-pi}
            \abs{\Ebb_{\nu_i}[f]-\Ebb_{\pi}[f]}
        \leq \begin{cases}
             \rchi(\nu_i,\pi)\sigma_2[f], & \text{always},\\
            4M\left(1+\log\rchi(\nu_i,\pi)\right), &\text{if }\rchi(\nu_i,\pi)\geq 1.
        \end{cases}
        \end{aligned}
    \end{equation}
    Now, by the definition of $\gamma$, it holds that $\rchi(\nu_i,\pi)\leq (1-\gamma)^{i-1} \rchi(\nu_1,\pi)=(1-\gamma)^{i-1} \rchi$. 
        Let us consider the smallest $k\geq 1$ such that $\rchi(\nu_{k},\pi)\leq 1$. Then clearly $k\leq 1+\ceil{\frac{[\log\rchi]_+}{\gamma}}$, and for $i\geq k$, $\rchi(\nu_{i},\pi)\leq (1-\gamma)^{i-1}\min\{1,\rchi\}$. Hence,
        \begin{align*}
            \sum_{i=1}^{n}\abs{\Ebb_{\nu_i}[f]-\Ebb_{\pi}[f]}
            \leq& \sum_{i=1}^{k-1}\abs{\Ebb_{\nu_i}[f]-\Ebb_{\pi}[f]}+\sum_{i=k}^{n}\abs{\Ebb_{\nu_i}[f]-\Ebb_{\pi}[f]}\\
            \leq& \sum_{i=1}^{k-1}4M\left(1+\log\rchi(\nu_i,\pi)\right)+\sum_{i=k}^{n} \rchi(\nu_i,\pi)\sigma_2[f]\\
            \leq& 4M(k-1)\left(1+\log\rchi\right)+ \sigma_2[f] \min\{1,\rchi\}\cdot \frac1{\gamma},
        \end{align*}
    where the first inequality is the triangle inequality and the second uses \eqref{eq:nu-pi}. Therefore, it holds that
    \begin{equation}
        \frac{1}{n}\sum_{i=1}^{n}\abs{\Ebb_{\nu_i}[f]-\Ebb_{\pi}[f]}
        \leq \frac{1}{n\gamma}\left(\sigma_2[f] \minop{
            1, \rchi}+4M\pos{\log\rchi}^2+4M\pos{\log\rchi}\right).
    \end{equation}

    \paragraph{Variance bound}
    In the following proof, we provide an upper bound on all higher-order moments of $\Delta$ simultaneously.
    Denote $A=\log(2C), R_1=8\sqrt{\frac{\sigma_2^2[f]}{\gamma n}}, R_2=\frac{80M}{\gamma n}, s_0=\frac{20M(\log n)^2}{n}$. Then, we only need to bound $\Ebb[\abs{\Delta}^m]$ for even $m$ under the condition
    \begin{align*}
        \Pbb\left(\abs{\Delta}\geq s\right)\leq \exp(A-s^2/R_1^2)+\exp(A-\sqrt{s/R_2}), \qquad \forall s\geq s_0.
    \end{align*}
    Notice that
    \begin{align*}
        \frac1m\Ebb[\abs{\Delta}^m]
        =&~\int_{0}^\infty s^{m-1}\Pbb(\abs{\Delta}\geq s)ds\\
        \leq&~\int_{0}^\infty s^{m-1}\min\{1,\exp(A-s^2/R_1^2)+\exp(A-\sqrt{s/R_2})\}ds\\
        \leq&~\underbrace{\int_{0}^\infty s^{m-1}\min\{1,\exp(A-s^2/R_1^2)\}ds}_{I_1}+\underbrace{\int_{0}^\infty s^{m-1}\min\{1,\exp(A-\sqrt{s/R_2})\}ds}_{I_2}.
    \end{align*}
    Thus, we further denote $s_1=\max\{s_0,R_1\sqrt{A}\}, s_2=\max\{s_0,R_2A^2\}$. Then
    \begin{align*}
        I_1=&~\int_{0}^{s_1} s^{m-1}ds+\int_{s_1}^\infty s^{m-1}\exp(A-s^2/R_1^2)ds\\
        =&~\frac{s_1^m}{m}+\frac{R_1^{m}}{2}\int_{s_1}^\infty \left(\frac{s}{R_1}\right)^{m-2}\exp(A-s^2/R_1^2)d\left(\frac{s^2}{R_1^2}\right)\\
        % =&~\frac{s_1^m}{m}+\frac{R_1^{m}}{2}\cdot\exp(A-s_1^2/R_1^2)\cdot \left(\frac{m-2}{2}\right)!,
        =&~\frac{s_1^m}{m}+\frac{R_1^{m}}{2}\int_{s_1^2/R_1^2}^\infty t^{(m-2)/2}\exp(A-t)dt\\
        \leq&~\frac{s_1^m}{m}+\frac{R_1^{m}}{m}\left((A+m/2)^{m/2}-A^{m/2}\right),
    \end{align*}
    where the last inequality is due to the fact $\int_{A}^{\infty} t^{k-1}\exp(A-t)dt\leq (A+k-1)^{k-1}\leq ((A+k)^k-A^k)/k$ for any integer $k\geq 1$. Similarly,
    \begin{align*}
        I_2=&~\int_{0}^\infty s^{m-1}\min\{1,\exp(A-\sqrt{s/R_2})\}ds\\
        =&~\int_{0}^{s_2} s^{m-1}ds+\int_{s_2}^\infty s^{m-1}\exp(A-\sqrt{s/R_2})ds\\
        =&~\frac{s_2^m}{m}+\int_{\sqrt{s_2/R_2}}^\infty 2R_2^{m}t^{2m-1}\exp(A-t)dt\\
        \leq&~ \frac{s_2^m}{m}+\frac{R_2^{m}}{m}\left((A+2m)^{2m}-A^{2m}\right).
    \end{align*}
    Combining these two cases, we obtain
    \begin{align*}
        \Ebb[\abs{\Delta}^m]\leq&~ mI_1+mI_2
        \leq s_0^m+s_1^m+R_1^m\left((A+m/2)^{m/2}-A^{m/2}\right) + R_2^m \left((A+2m)^{2m}-A^{2m}\right)\\
        \leq&~ 2s_0^m+R_1^mA^{m/2}+R_2^mA^{2m}+R_1^m\left((A+m/2)^{m/2}-A^{m/2}\right) + R_2^m \left((A+2m)^{2m}-A^{2m}\right)\\
        =&~2s_0^m+R_1^m(A+m/2)^{m/2} + R_2^m (A+2m)^{2m}.
    \end{align*}
    This completes the proof.
\end{proof}

% We can also consider the MCMC error of a bounded function that can be regarded as the special case of a sub-exponential variable. If $X$ is a random variable bounded by $M$, then the exponential norm of $X$ is less than $3M$.

% \begin{proof}
%     Use the Jensen inequality and Hoeffding inequality \eqref{eq:HoeffdingExp}, then for any $t>0$, it holds that 
%     \begin{equation}
%         \begin{aligned}
%             \left|\Ebb\left[\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}f(X_i)\right]-\pi(f)\right|&=\frac{1}{t}\left|\Ebb\left[t\left(\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}f(X_i)-\pi(f)\right)\right]\right|\\
%             &\leq \frac{1}{t} \left|\log \Ebb\left[e^{\frac{t}{n}(\sum_{i=n_0+1}^{n_0+n}f(X_i)-n\pi(f))}\right]\right|\\
%             &\leq \frac{\log C}{t} +\frac{\sigma^2 t}{2n}.
%         \end{aligned}
%     \end{equation}
%     Let $t=\sqrt{\frac{2n \log C}{\sigma}}$, the RHS is equal to $\sqrt{\frac{2\sigma^2 \log C}{n}}$. 
% \end{proof}

Through Lemma \ref{lem:BernBiasVar}, we are able to characterize the sampling error in VMC. For any fixed $\parameter\in \Rbb^d$, let the MH algorithm generate $n$ samples $\mathbf{S}=\{\bmx_i\}_{i=n_0+1}^{n_0+n}$, and the stochastic gradient calculated by \eqref{eq:approxgrad}. Then, we give the error bounds of the stochastic gradient with the MCMC estimator in the following lemma.
\begin{lemma}
    \label{lem:gbiasVar}
    Let Assumption \ref{asm:wavefun}, \ref{asm:local} and \ref{asm:unigap} hold. For a fixed parameter $\parameter\in \Rbb^d$, we generate the MCMC samples $\mathbf{S}=\{\bmx_i\}_{i=n_0+1}^{n_0+n}$ with the stationary distribution $\pi_{\parameter}$ by the MH algorithm \ref{alg:MH}. Suppose  we start the MH algorithm from the initial distribution $\nu$ and let $\rchi=\rchi(\nu,\pi_{\parameter})<+\infty$. The stochastic gradient $\hat{g}(\parameter;\mathbf{S})$, defined by \eqref{eq:approxgrad} has the following error bounds that
    \begin{equation}
        \label{eq:BV}
        \begin{aligned}
            \norm{\Ebb[\hat{g}(\parameter;\mathbf{S})]-g(\parameter)}&\leq B_{n,n_0}:=\frac{4c_1B}{n\gamma},\\
            \epct{\norm{\hat{g}(\parameter;\mathbf{S})-g(\parameter)}^2}
            &\leq V_{n,n_0}:=\frac{16c_2B^2\sigma^2_{2}[E_{\parameter}]}{n\gamma}+\frac{40(c_3+c_4\log^4 n)B^2M^2}{n^2\gamma^2},
        \end{aligned}
    \end{equation}
    where with $\rchi_{n_0}= (1-\gamma)^{n_0}\rchi$ and $C=(1+\rchi_{n_0})^{\frac{1}{2}}$, these factors are defined by $c_1=\rchi_{n_0}\sigma_{2}[E_{\parameter}] +4M[\log \rchi_{n_0}]^{2}_{+}+4M[\log \rchi_{n_0}]_{+}$, $c_2=64(1+\log 2C)$ and $c_3=6400(4+\log 2C)^4$, $c_4=800$.
    % where $c_1=(1-\gamma)^{n_0}\rchi\sigma_{2}[E_{\parameter}] +4M[\log \rchi + n_0\log( 1-\gamma)]^{2}_{+}+4M[\log \rchi + n_0\log( 1-\gamma)]_{+}$, $c_2=128(3+\log C)$, $ c_3=100(8+4\log C)^4$ and $C=(1+(1-\gamma)^{2n_0}\rchi^2)^{\frac{1}{2}}$.
\end{lemma}
\begin{proof}
    The error of the stochastic gradient can be rewritten as 
    \begin{equation*}
        \begin{aligned}
            \norm{\Ebb[\hat{g}(\parameter;\mathbf{S})]-g(\parameter)}&=\sup_{\norm{v}=1}\abs{\Ebb[v^{\T}\hat{g}(\parameter;\mathbf{S})]-v^{\T}g(\parameter)},\\
            \epct{\norm{\hat{g}(\parameter;\mathbf{S})-g(\parameter)}^2}&=\tr\left(\epct{\left(\hat{g}(\parameter;\mathbf{S})-g(\parameter)\right)\left(\hat{g}(\parameter;\mathbf{S})-g(\parameter)\right)^{\T}}\right),\\
            &\leq d \sup_{\norm{v}=1}\Ebb[\left(v^{\T}\hat{g}(\parameter;\mathbf{S})-v^{\T}g(\parameter)\right)^2].
        \end{aligned}
    \end{equation*}
    For any given $v$ such that $\norm{v}=1$, $v^{\T}g(\parameter)$ is approximated by $v^{\T}\hat{g}(\parameter;\mathbf{S})$. We denote the stationary variables $E = E_{\parameter}(\bmx)$, $Y = v^{\T}\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)$, $\bar{E}= E_{\parameter}(\bmx)-\Ebb_{\bmx\sim \pi_{\parameter}} [E_{\parameter}(\bmx)]$ with $\bmx\sim \pi_{\parameter}$ and the empirical variables  $E_i = E_{\parameter}(\bmx_i)$, $Y_i = v^{\T}\nabla_{\parameter}\log \Psi_{\parameter}(\bmx_i)$, $\bar{E}_i=E_i-\Ebb_{\bmx\sim \pi_{\parameter}} [E_{\parameter}(\bmx)]$. Then it holds 
    \begin{equation}
        \label{eq:gradsplit}
        \begin{aligned}
            \frac{1}{2}\left(v^{\T}\hat{g}(\parameter;\mathbf{S})-v^{\T}g(\parameter)\right)&=\frac{1}{n}\sum_{i=n_0+1}^{n+n_0}E_i Y_i-\left(\frac{1}{n}\sum_{j=n_0+1}^{n+n_0}E_j \right)\left(\frac{1}{n}\sum_{i=n_0+1}^{n+n_0}Y_i\right) - \Ebb_{\pi_{\parameter}} [\bar{E}Y]\\
            &=\underbrace{\frac{1}{n}\sum_{i=n_0+1}^{n+n_0}\bar{E}_i Y_i-\Ebb_{\pi_{\parameter}}[\bar{E}Y]}_{I_1}-\underbrace{\left(\frac{1}{n}\sum_{j=n_0+1}^{n+n_0}\bar{E}_i\right)\left(\frac{1}{n}\sum_{i=n_0+1}^{n+n_0}Y_i\right)}_{I_2}.
        \end{aligned}
    \end{equation}
    With Assumptions \ref{asm:wavefun} and \ref{asm:local}, we have $\norm{\bar{E}}_{\psi_1}\leq M$ and $\norm{Y}\leq\norm{v}\norm{\nabla_{\parameter}\log\Psi_{\parameter}(\bmx)} \leq B$. Then the variance is bounded by $\sigma_{2}^{2}[\bar{E}Y]\leq \Ebb[(\bar{E}Y)^2]\leq \sigma_2^{2}[E]B^2$. It also holds 
    \begin{equation}
        \begin{aligned}
            \Ebb\left[\exp\left( \frac{\abs{\bar{E}Y-\Ebb[\bar{E}Y]}}{2MB}\right) \right]\leq  \Ebb\left[\exp\left( \frac{\abs{\bar{E}}B+MB }{2MB}\right) \right]\leq 2 ,
        \end{aligned}
    \end{equation}
    which implies $\norm{\bar{E}Y}_{\psi_1}\leq 2MB$. 
    Applying Lemma \ref{lem:BernBiasVar} to $\{\bar{E}_i Y_{i}\}_{ i= n_0+1}^{n_0+n}$, we have %\cfn{more details? The factor $\sigma_{\parameter}$ in $c_1$ seems to be $\sqrt{d}\sigma_{\parameter}$?}
    \begin{equation}
        \begin{aligned}
            \label{eq:Bias}
            \norm{\Ebb[I_1]}\leq \frac{c_1B}{n\gamma},\quad
            \Ebb\left[ \norm{I_1}^2\right] \leq \frac{c_2dB^2\sigma^2_{2}[E_{\parameter}]}{n\gamma}+\frac{4(c_3d+c_4d\log^4 n)M^2B^2}{n^2\gamma^2}.
        \end{aligned}
    \end{equation} 
    % where $c_1=(1-\gamma)^{n_0}\rchi\sigma_{2}[E_{\parameter}] +4M[\log \rchi + n_0\log( 1-\gamma)]^{2}_{+}+4M[\log \rchi + n_0\log( 1-\gamma)]_{+}$, $c_2=128(2+\log C)$, $ c_3=100(8+4\log C)^4$ and $C=(1+(1-\gamma)^{2n_0}\rchi^2)^{\frac{1}{2}}$.

    As $\norm{\bar{E}}_{\psi_1}\leq M$, $\Ebb_{\pi_{\parameter}}[\bar{E}]=0$ and $\norm{\frac{1}{n}\sum_{i=n_0+1}^{n+n_0}Y_i}\leq B$, Lemma \ref{lem:BernBiasVar} implies that
    \begin{equation}
        \begin{aligned}
            \label{eq:Var}
            \norm{\Ebb[I_2]}\leq \frac{c_1B}{n\gamma},\quad \Ebb\left[ \norm{I_2}^2\right] \leq \frac{c_2dB^2\sigma^2_{2}[E_{\parameter}]}{n\gamma}+\frac{(c_3d+c_4d\log^4 n)M^2B^2}{n^2\gamma^2}.
        \end{aligned}
    \end{equation}
    Combining \eqref{eq:Bias} and \eqref{eq:Var}, we finally obtain
    \begin{equation}
        \begin{aligned}
            \norm{\Ebb[\hat{g}(\parameter;\mathbf{S})]-g(\parameter)}&=2\norm{\Ebb[I_1-I_2]}\leq 2\norm{\Ebb[I_1]}+2\norm{\Ebb[I_2]}\leq B_{n,n_0},\\
            \epct{\norm{\hat{g}(\parameter;\mathbf{S})-g(\parameter)}^2}
            &=4\Ebb[\norm{I_1-I_2}^2]\leq 8\Ebb[\norm{I_1}^2]+8\Ebb[\norm{I_2}^2]\leq V_{n,n_0},
        \end{aligned}
    \end{equation}
    where $B_{n,n_0},V_{n,n_0}$ are given in \eqref{eq:BV}.
\end{proof}
%     Then the bias can be divided into two portions,
%     \begin{equation*}
%         \norm{\epct{\hat{g}(\parameter;\mathbf{S})}-\nabla_{\parameter} \Lcal(\parameter)}\leq \underbrace{\norm{\epct{\hat{g}(\parameter;\mathbf{S})-\tilde{g}(\parameter;\mathbf{S})}}}_{\mathrm{I}}+\underbrace{\norm{\epct{\tilde{g}(\parameter;\mathbf{S})}-\nabla_{\parameter} \Lcal(\parameter)}}_{\mathrm{II}}.
%     \end{equation*}

%     As $E_{\parameter}(\bmx)$ has a sub-exponential tail with the parameter $M$, using Lemma \ref{lem:BernBiasVar}, we have
%     \begin{equation*}
%         \begin{aligned}
%             \mathrm{I}&=\left\|\Ebb\left[\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}\left(\frac{1}{n}\sum_{j=n_0+1}^{n_0+n}E_{\parameter}(\bmx_j)-\Ebb_{\pi_{\parameter}}[E_{\parameter}(\bmx)]\right)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx_i)\right]\right\|\\
%             &\leq \frac{8\sigma_{\parameter}B\sqrt{\log C}}{\sqrt{n(1-\gamma)}}+\frac{2MB(1+C)\log C}{n(1-\gamma)}.
%         \end{aligned}
%     \end{equation*}  
%    Besides, each component of $\bar{E}_{\parameter}(\bmx)\log \Psi_{\parameter}(\bmx)$ has a sub-exponential norm less than $\frac{MB}{\sqrt{d}}$ and variance less that $\frac{4B^2\sigma^2_{\parameter}}{d}$. It follows that
%     \begin{equation*}
%         \begin{aligned}
%             \mathrm{II}&= \left\|\Ebb\left[\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}\bar{E}_{\parameter}(\bmx_i)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx_i)-\Ebb_{\pi_{\parameter}}\left[ \bar{E}_{\parameter}(\bmx)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)\right]\right]\right\|\\
%             &\leq \frac{16\sigma_{\parameter}B\sqrt{\log C}}{\sqrt{n(1-\gamma)}}+\frac{2MB(1+C)\log C}{n(1-\gamma)} .
%         \end{aligned}
%     \end{equation*}
%     Above all, we obtain the bias bound
%     \begin{equation*}
%         B_{n,n_0}=  \frac{24\sigma_{\parameter}B\sqrt{\log C}}{\sqrt{n(1-\gamma)}}+\frac{4MB(1+C)\log C}{n(1-\gamma)}.
%     \end{equation*}

%     Similarly, we estimate the variance through Lemma \ref{lem:BernBiasVar},
%     \begin{equation*}
%         \begin{aligned}
%             % \epct{\|\hat{g}(\parameter;\mathbf{S})-\tilde{g}(\parameter;\mathbf{S})\|^2}\leq & B^2 \cdot\Ebb\left[\left(\frac{1}{n}\sum_{j=n_0+1}^{n_0+n}E_{\parameter}(\bmx_j)-\Ebb_{\pi_{\parameter}}[E_{\parameter}(\bmx)]\right)^2\right]\\
%             % \leq& \frac{256C\sigma_{\parameter}^2B^2}{n(1-\gamma)}+\frac{8\sqrt{3}(1+C)M^2B^2}{n^2(1-\gamma)^2},\\
%             % \epct{\|\tilde{g}(\parameter;\mathbf{S})-\nabla \Lcal(\parameter)\|^2}\leq & \Ebb\left[\left\|\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}\bar{E}_{\parameter}(\bmx_i)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx_i)-\Ebb_{\pi_{\parameter}}\left[\bar{E}_{\parameter}(\bmx)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)\right]\right\|^2\right]\\
%             % \leq & \frac{1024C\sigma_{\parameter}^2B^2}{n(1-\gamma)}+\frac{8\sqrt{3}(1+C)M^2B^2}{n^2(1-\gamma)^2}.
%             \epct{\|\hat{g}(\parameter;\mathbf{S})-\tilde{g}(\parameter;\mathbf{S})\|^2}\leq& \frac{256C\sigma_{\parameter}^2B^2}{n(1-\gamma)}+\frac{8\sqrt{3}(1+C)M^2B^2}{n^2(1-\gamma)^2},\\
%             \epct{\|\tilde{g}(\parameter;\mathbf{S})-\nabla \Lcal(\parameter)\|^2}
%             \leq & \frac{1024C\sigma_{\parameter}^2B^2}{n(1-\gamma)}+\frac{8\sqrt{3}(1+C)M^2B^2}{n^2(1-\gamma)^2}.
%         \end{aligned}
%     \end{equation*}
%     Hence, the variance bound is shown as follows,
%     \begin{equation*}
%         \begin{aligned}
%             V_{n,n_0}= \frac{2560C\sigma_{\parameter}^2B^2}{n(1-\gamma)}+\frac{32\sqrt{3}(1+C)M^2B^2}{n^2(1-\gamma)^2}.
%         \end{aligned}
%     \end{equation*}
%     Here we complete the proof.


% \begin{lemma}
%     \label{lem:ghighprob}
%     Let Assumption \ref{asm:wavefun} and \ref{asm:unigap} hold. For a fixed parameter $\parameter$, we obtain $n$ MCMC samples $\mathbf{S}=\{\bmx_i\}_{i=n_0+1}^{n_0+n}$ with the stationary distribution $\pi_{\parameter}$ after the burn-in period of length $n_0$. For any $\delta \in (0,1)$, with probability at least $1-\delta$,
%     \begin{equation}
%         \left\|\hat{g}(\parameter,\mathbf{S})-\nabla \Lcal(\parameter)\right\|\leq O\left(\sqrt{\frac{\log(4C/\delta)}{n} }\right).
%     \end{equation}
% \end{lemma}
% \begin{proof}
%     Let the auxiliary gradient $\tilde{g}(\parameter,\mathbf{S})$ be defined by \eqref{eq:auxigrad}. Corollary \ref{crl:highprob} implies that with probability at least $1-\frac{\delta}{2}$,
%     \begin{equation*}
%         \begin{aligned}
%             \|\hat{g}(\parameter,\mathbf{S})-\tilde{g}(\parameter,\mathbf{S})\|&=\left\|\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}\left(\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}E_{\parameter}(\bmx_i)-\Ebb_{\pi_{\parameter}}[E_{\parameter}(\sigma)]\right)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx_i)\right\|\\
%             &\leq 2B_gB_e\sqrt{\frac{1+\gamma}{1-\gamma}\cdot \frac{1}{n}\cdot \log\left(\frac{4C}{\delta}\right) }.
%         \end{aligned}
%     \end{equation*}
%     Similarly, with probability at least $1-\frac{\delta}{2}$,
%     \begin{equation*}
%         \begin{aligned}
%             \|\tilde{g}(\parameter,\mathbf{S})-\nabla \Lcal(\parameter)\|&\leq\left\|\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}E_{\parameter}(\bmx_i)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx_i)-\Ebb_{\pi_{\parameter}}\left[E_{\parameter}(\bmx)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)\right]\right\|\\
%             &+\left|\Ebb_{\pi_{\parameter}}[E_{\parameter}(\bmx)]\right|\left\|\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}\nabla_{\parameter}\log \Psi_{\parameter}(\bmx_i)-\Ebb_{\pi_{\parameter}}[\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)]\right\| \\
%             &\leq 4B_gB_e\sqrt{\frac{1+\gamma}{1-\gamma}\cdot \frac{1}{n}\cdot \log\left(\frac{4C}{\delta}\right) }.
%         \end{aligned}
%     \end{equation*}
%     Hence, we obtain high probability error bound of the gradient. With probability at least $1-\delta\leq(1-\frac{\delta}{2})^2$, it holds
%     \begin{equation*}
%         \begin{aligned}
%             \left\|\hat{g}(\parameter,\mathbf{S})-\nabla\Lcal(\parameter)\right\|&\leq  \|\hat{g}(\parameter,\mathbf{S})-\tilde{g}(\parameter,\mathbf{S})\|+\|\tilde{g}(\parameter,\mathbf{S})-\nabla \Lcal(\parameter)\|\\
%             &\leq 6B_gB_e\sqrt{\frac{1+\gamma}{1-\gamma}\cdot \frac{1}{n}\cdot \log\left(\frac{4C}{\delta}\right) } \\
%             &= O\left(\sqrt{\frac{\log(4C/\delta)}{n} }\right).
%         \end{aligned}
%     \end{equation*}
% \end{proof}

\subsection{First order convergence}
In this subsection, we will first analyze the descent for one iteration based on the error bounds. Then, our main result shows the first order convergence of the VMC method in expectation.

Under our assumptions in subsection \ref{subsec:asm}, we can establish the $L$-smoothness of the objective function $\Lcal(\parameter)$ to perform a standardized analysis in optimization. This is a common condition for functions to achieve effective descent. We prove the following lemma by giving the bound of the Hessian.
\begin{lemma}
    \label{prop:gLip}
    Let Assumption \ref{asm:wavefun} and \ref{asm:local} hold.  There exists a constant $L>0$ such that $\Lcal(\parameter)$ is $L$-smooth, that is
    \begin{equation}
        \label{eq:gLip}
        \|g(\parameter_1)-g(\parameter_2)\|\leq L\|\parameter_1-\parameter_2\|,~\forall \parameter_1,\parameter_2 \in \Rbb^{d}.
    \end{equation} 
\end{lemma}




% \begin{proof}
%     A random variable $Y$ is of sub-Gaussian type if there exists $\sigma,c>0$ such that
%     \[\begin{aligned}
%         \Ebb\left[e^{tY}\right]\leq c\exp\left(\frac{\sigma^2t^2}{2}\right).
%     \end{aligned}\]
%     The definition above implies that $\Pbb\left(|Y|>t\right)\leq 2c\exp\left(-\frac{t^2}{2\sigma^2}\right)$, and thus
%     \[\begin{aligned}
%         \Ebb\left[Y^2\right]
%         &=\int_{0}^{\infty} \Pbb\left(Y^2\geq s\right)ds\\
%         &\leq \int_{0}^{\infty}\min\left\{1,2c\exp\left(-\frac{s}{2\sigma^2}\right)\right\}ds\\
%         &=2\sigma^2(\log 2c +1).
%     \end{aligned}\]
%     Combined with \eqref{eq:HoeffdingExp}, this fact implies that
%     \[\begin{aligned}
%         \Ebb\left[\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}f(X_i)-\pi(f)\right]^2
%         \leq \frac{2\sigma^2(\log 2C +1)}{n} .
%     \end{aligned}\]
% \end{proof}
% Besides, a high probability upper bound is given by the following corollary.
% \begin{corollary}
%     \label{crl:highprob}
%     Let the conditions in Lemma \ref{lem:hoeffding} be satisfied. For any $\delta \in (0,1)$, with probability at least $1-\delta$,
%     \begin{equation}
%         \left|\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}f(X_i)-\pi(f)\right|\leq \sqrt{\frac{2\sigma^2\log (2C/\delta)}{n} }.
%     \end{equation}
% \end{corollary}
% \begin{proof}
%     Let the right of the inequality \eqref{eq:HoeffdingProb} be $\delta$, then $\epsilon$ become the high probability upper bound. 
% \end{proof}

\begin{proof}
    As Assumptions \ref{asm:wavefun} and \ref{asm:local} hold, we have 
    \begin{equation*}
        \Ebb_{\pi_{\parameter}}\lrb{|\oE_{\parameter}|}\leq M. 
    \end{equation*}
    The Hessian  $H(\parameter)$ defined in \eqref{eq:Hess} can be bounded by
        \begin{equation*}
    \begin{aligned}
    \norm{H(\parameter)}
    \leq &\norm{H_1}+\norm{H_2}+\norm{H_3}+2 \norm{H_{12}}\\
    \leq &2\Ebb_{\pi_{\parameter}}\lrb{\norm{\nabla_{\parameter}E_{\parameter} \nabla_{\parameter}\log \Psi_{\parameter}^{\T}}}+4\Ebb_{\pi_{\parameter}}\lrb{|\oE_{\parameter}|\norm{\nabla_{\parameter}\log \Psi_{\parameter}\nabla_{\parameter}\log \Psi_{\parameter}^{\T}}}\\
    &+2\Ebb_{\pi_{\parameter}}\lrb{|\oE_{\parameter}|\norm{\nabla^{2}_{\parameter}\log \Psi_{\parameter}}}+8\Ebb_{\pi_{\parameter}}\lrb{|\oE_{\parameter}|\norm{\nabla_{\parameter}\log \Psi_{\parameter}}}\Ebb_{\pi_{\parameter}}\lrb{\norm{\nabla_{\parameter}\log \Psi_{\parameter}^{\T}}}\\
    \leq & 2 B L_1+4M B^2+2M L_2+8 M B^2,
    \end{aligned}
    \end{equation*}
    where the last inequality is due to Assumptions \ref{asm:wavefun} and \ref{asm:local}.
    Let $L =2 B L_1+2M L_2+12 M B^2$, then $\Lcal(\parameter)$ is $L$-smooth. 
\end{proof}

According to \cite{wright1999numerical}, the $L$-smoothness \eqref{eq:gLip} is also equivalent to 
\begin{equation}
    \Lcal(\parameter_2)\leq \Lcal(\parameter_1)+\langle g(\parameter_1),\parameter_2-\parameter_1\rangle+\frac{L}{2}\|\parameter_1-\parameter_2\|^2.
\end{equation}

It is already known that the stochastic gradient $\hat{g}$ is biased, unlike the unbiased estimator in the classical SGD. However, the bias decays with increasing burn-in time $n_0$ or the sample size $n$,  which can be regarded sufficiently small. We discuss the expected descent of the function value within each iteration in the following lemma. 

\begin{lemma}
    \label{lem:decrease}
    If the stepsize $\alpha_k\leq \frac{1}{2L}$, for any given $\parameter_k$ the loss function $\Lcal(\parameter_{k+1}) $ decreases in expectation as
    \begin{equation}
        \label{eq:descent}
        \Lcal(\parameter_{k})-\epct{\Lcal(\parameter_{k+1})|\parameter_k}\geq \frac{\alpha_k}{2}\|\nabla\Lcal(\parameter_{k})\|^2-\frac{\alpha_k}{2}\cdot B_{n,n_0}^2
        -\frac{\alpha_k^2L}{2} \cdot V_{n,n_0}.
    \end{equation}
    where  $B_{n,n_0}$ and $V_{n,n_0}$ are defined in Lemma \ref{lem:gbiasVar}.
\end{lemma}
\begin{proof}
    For convenience, we simplify the notations with $\Lcal_k:= \Lcal(\parameter_{k})$, $g_k:=g(\parameter_k)$ and $\hat{g}_k:=\hat{g}(\parameter_k,\mathbf{S}_k)$ for any $k\geq 1$.
    By Lemma \ref{prop:gLip}, we perform a gradient descent analysis of the iteration \eqref{eq:iter}, 
    \begin{equation}\label{eqn:sgd-s1}
        \begin{aligned}
            \Lcal_{k+1}\leq & \Lcal_k+\langle g_k,\parameter_{k+1}-\parameter_k\rangle+\frac{L}{2}\|\parameter_{k+1}-\parameter_k\|^2\\
            =&\Lcal_k-\alpha_k\langle g_k,\hat{g}_k-g_k\rangle-\alpha_k\|g_k\|^2+\frac{\alpha_k^2 L}{2}\|\hat{g}_k\|^2\\
            =&\Lcal_k-(\alpha_k-\alpha_k^2L)\langle g_k,\hat{g}_k-g_k\rangle-\left(\alpha_k-\frac{\alpha_k^2L}{2}\right)\|g_k\|^2+\frac{\alpha_k^2 L}{2}\|\hat{g}_k-g_k\|^2.
        \end{aligned}
    \end{equation}
Taking the conditional expectation of \eqref{eqn:sgd-s1} on $\parameter_k$ gives
\begin{equation}\label{eqn:sgd-s2}
    \begin{aligned}
        \cond{\Lcal_{k+1}}{\parameter_{k}}\leq
        &\Lcal_k-\left(\alpha_k-\frac{\alpha_k^2L}{2}\right)\|g_k\|^2\\
        &-(\alpha_k-\alpha_k^2L)\langle g_k,\cond{\hat{g}_k}{\parameter_{k}}-g_k\rangle+\frac{\alpha_k^2 L}{2}\cond{\nrm{\hat{g}_k-g_k}^2}{\parameter_k}.
    \end{aligned}
\end{equation}
Through the fact $|\langle x,y\rangle|\leq (\nrm{x}^2+\nrm{y}^2)/2$, it holds
\begin{equation}\label{eqn:sgd-s3}
\begin{aligned}
    -(\alpha_k-\alpha_k^2L)\iprod{g_k}{\cond{\hat{g}_k}{\parameter_{k}}-g_k}
    \leq& \frac{\alpha_k-\alpha_k^2L}{2}\nrm{g_k}^2
    +\frac{\alpha_k}{2}\nrm{\cond{\hat{g}_k}{\parameter_{k}}-g_k}^2.
\end{aligned}
\end{equation}
Therefore, we can plug in \eqref{eqn:sgd-s3} to \eqref{eqn:sgd-s2} and rearrange to get
\begin{equation}
    \label{eq:descent2}
    2\left(\Lcal_k-\cond{\Lcal_{k+1}}{\parameter_{k}}
    \right)
    \geq \alpha_k\nrm{g_k}^2
    -\alpha_k \nrm{\cond{\hat{g}_k}{\parameter_{k}}-g_k}^2-L\alpha_k^2 \cond{\nrm{\hat{g}_k-g_k}^2}{\parameter_k}.
\end{equation}
Thus, \eqref{eq:descent} holds when we substitute the error bounds in Lemma \ref{lem:gbiasVar} into \eqref{eq:descent2}.
\end{proof}

As $n$ goes towards positive infinity, the objective function decreases as an exact gradient descent. However, $n$ and $n_0$ are not too large because of the high sampling cost in practice. The error bounds studied in Section \ref{subsec:error} are valuable for our analysis of the stochastic optimization. Since $\Lcal(\parameter)$ is non-convex, we consider the convergence rate in terms of the expected norm of the gradient $\Ebb\norm{\nabla_{\parameter}\Lcal(\parameter)}^2$. Finally, we establish our first-order convergence results as follows.  
\begin{theorem}
    \label{thm:expt}
    Let Assumptions \ref{asm:wavefun},\ref{asm:local} and \ref{asm:unigap} hold and $\{\parameter_k\}$ be generated by Algorithm \ref{alg:VMC}. If the stepsize satisfies $\alpha_k\leq \frac{1}{2L}$, 
        then for any $K$, we have
        \begin{equation}
            \label{eq:converge1}
            \min_{1\leq k\leq K}\Ebb \|g(\parameter_k)\|^2 \leq O\left(\frac{1}{\sum_{k=1}^{K}\alpha_k}\right)+O\left(\frac{\sum_{k=1}^{K}\alpha_k^2}{n\sum_{k=1}^{K}\alpha_k}\right)+O\left(\frac{(1-\gamma)^{2n_0}}{n^2}\right),
        \end{equation}
        where $O(\cdot)$ hides constants $\gamma,M,B,C$ and retains $n,n_0$ and $K$. In particular, if the stepsize is chosen as  $\alpha_k=\frac{c\sqrt{n}}{\sqrt{k}}$ where $c\leq\frac{1}{2L} $, then we have
        \begin{equation}
            \label{eq:converge2}
            \min_{1\leq k\leq K}\Ebb \|g(\parameter_k)\|^2 \leq O\left(\frac{\log K}{\sqrt{nK}}\right)+O\left(\frac{(1-\gamma)^{2n_0}}{n^2}\right).
        \end{equation}
\end{theorem}
\begin{proof}
    


Lemma \ref{lem:decrease} suggests that, for any $k\geq 1$, 
\begin{equation*}
    \begin{aligned}
        \alpha_k\nrm{g(\parameter_k)}^2
        \leq & 2\left(\Lcal(\parameter_k)-\cond{\Lcal(\parameter_{k+1})}{\parameter_{k}}\right)+\alpha_k\cdot B_{n,n_0}^2+\alpha_k^2\cdot L V_{n,n_0}.%\\
        %B_{n,n_0}\leq & O\left(\frac{\rchi^{\frac{1}{2}}\gamma^{\frac{n_0}{2}}\sigma}{\sqrt{n}}\right)+O\left(\frac{\rchi\gamma^{n_0}}{n}\right) ,\\
        %V_{n,n_0}\leq & O\left(\frac{(1+\rchi\gamma^{n_0})\sigma^2}{n}\right)+O\left(\frac{1}{n^2}\right).
    \end{aligned}
\end{equation*}
Thus, taking total expectation and summing over $k=1,\cdots,K$ yields
\begin{align*}
    \left(\sum_{k=1}^{K}\alpha_k \right)\min_{1\leq k\leq K} \epct{\nrm{g(\parameter_k)}^2} &\leq \sum_{k=1}^{K}\alpha_k \epct{\nrm{g(\parameter_k)}^2}
    \\
    &\leq 2\left(\Lcal(\parameter_1)-\epct{\Lcal(\parameter_{K+1})}
    \right)
    +\sum_{k=1}^{K}\alpha_k^2 \cdot LV_{n,n_0}
    + \sum_{k=1}^{K}\alpha_k\cdot B_{n,n_0}^2\\
    &=O(1)+\sum_{k=1}^{K}\alpha_k^2 \cdot O\left(\frac{1}{n}\right)+\sum_{k=1}^{K}\alpha_k \cdot O\left(\frac{(1-\gamma)^{2n_0}  }{n^2}\right).
\end{align*}
Divide both sides by $\sum_{k=1}^{K}\alpha_k$, then \eqref{eq:converge1} holds. If we take $\alpha_k=\frac{c\sqrt{n}}{\sqrt{k}}$, \eqref{eq:converge2} is implied by
\begin{equation*}
        \sum_{k=1}^{K}\alpha_k=\sum_{k=1}^{K}\frac{c\sqrt{n}}{\sqrt{k}}=O(\sqrt{nK}),\quad \sum_{k=1}^{K}\alpha_k^2=\sum_{k=1}^{K}\frac{c^2n}{k}=O(n\log K).
\end{equation*}
This completes the proof.
\end{proof}

Theorem \ref{thm:expt} shows the convergence rate of the VMC method with certain choices of stepsizes. The convergence rate is related to $K$ and the sample size $n$ with an additional term from the bias. The burn-in period $n_0$ influences the bias. Theoretically, when $n_0$ is sufficiently large, the Markov chain becomes stationary and the bias is reduced to zero, regardless of the sample size $n$.

\section{Escaping from saddle points}
\label{sec:saddle}
In this section, we discuss how VMC escapes from saddle points. 
First, a second moment lower-bound for non-stationary Markov chains is developed to guarantee the efficient noise. Then, we verify that the correlated negative curvature condition is satisfied by VMC. Eventually, we demonstrate our convergence analysis of Algorithm \ref{alg:VMC} to reach approximate second-order stationary points in high probability.

% We first give the definition of kurtosis. Let $\sigma_2^{2}[X]:=\Ebb[\left(X-\Ebb[X]\right)^2]$ be the variance and $\sigma_4^4[X]:=\Ebb[\left(X-\Ebb[X]\right)^4]$ be the fourth central moment for any random variable $X$. The kurtosis of $X$ is the fourth standardized moment, defined as
% \begin{equation*}
%     \kappa[X]:=\frac{\sigma_4^4[X]}{\sigma_2^{4}[X]}=\frac{\Ebb[\left(X-\Ebb[X]\right)^4]}{\big(\Ebb[\left(X-\Ebb[X]\right)^2]\big)^2}.
% \end{equation*}
% Kurtosis means the "tailedness" of the probability distribution of a random variable. This number is related to the tails of the distribution.

To deal with the second-order structure, the following assumption is needed. 
\begin{assumption}
    \label{asm:sec} 
    (1) The Hessian matrix $H(\parameter)$ defined in \eqref{eq:Hess} is $\rho$-Lipschitz continuous, i.e.,
    \begin{equation*}
       \norm{H(\parameter_1)-H(\parameter_2)}\leq \rho\norm{\parameter_1-\parameter_2}, ~\forall \parameter_1,\parameter_2\in\Rbb^{d}.
   \end{equation*}
    
   (2) Let $v$ be the unit eigenvector with respect to the minimum eigenvalue of the Hessian matrix $H(\parameter)$. Let $\sigma_2^2(\parameter)$ be the variance of the local energy in the quantum system. There exists a constant $\eta>0$ such that 
   \begin{equation*}
    \label{eq:glowerbound}
    \Ebb_{\pi_{\parameter}}\left[\left(\oE_{\parameter}(\bmx)v^{T}\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)\right)^2 \right]\geq \eta \sigma_{2}^2[E_{\parameter}],~\forall \parameter \in \Rbb^{d}.
   \end{equation*}

   (3) For any $\parameter\in \Rbb^d$, the ratio of the sub-exponential norm of the local energy to its variance has an upper bound $\kappa>0$, that is,
   \begin{equation*}
    \sup_{\parameter\in \Rbb^d}\frac{\norm{E_{\parameter}}_{\psi_1}}{\sigma_2[E_{\parameter}]}\leq \kappa.
   \end{equation*}
\end{assumption}

The first assumption is common to preform a second-order convergence analysis. The second assumption shows the relationship between the gradient $g(\parameter)=2\Ebb_{ \pi_{\parameter}}\lb\oE_{\parameter}(\bmx)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)\rb$ and a negative curvature direction $v$. It guarantees the variance of the stochastic gradient will not disappear along a negative curvature direction unless the local energy has a zero variance. We assume that the absolute angle between the gradient and the eigenvector of Hessian is expected to have a positive lower bound. 
The third one guarantees that the local energy will have a respectively light-tailed distribution. It holds for most of distributions and especially near the ground state of quantum many-body problems.

Besides, over Assumption \ref{asm:wavefun} and \ref{asm:local}, we can assume an upper bound of the stochastic gradient to simply our analysis:
\begin{equation}
    \label{eq:gradientbound}
    \begin{aligned}
        l_g:=\sup_{\parameter\in \Rbb^{d}}\sup_{\bmx\in \Xcal}\norm{\oE_{\parameter}(\bmx)\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)}<+\infty.
        %\Ebb\norm{\hat{g}(\parameter;\mathbf{S})}^2\leq 2\norm{g(\parameter)}^2+2\Ebb\norm{\hat{g}(\parameter;\mathbf{S})-g(\parameter)}^2\leq 2l^2+2V_{n,n_0}=:l_g^2,
    \end{aligned}
\end{equation}
This treatment is legal because $\oE_{\parameter}$ is assumed to sub-exponential. Otherwise, the proof can be also established with similar techniques in section \ref{subsec:error}. The upper bound will simplify our proof in Lemma \ref{lem:R2}.

\subsection{The Correlated Negative Curvature condition}
\label{subsec:cnc}
We first prove a general lemma about the second moment for non-stationary Markov chains. The lemma shows that the second moment of the MCMC estimation has a positive lower-bound relying on the sample size $n$, the spectral gap $\gamma$ and $\Ebb_{\pi}[f^2]$. 
\begin{lemma}
    \label{lem:lowerbound}
    Let $\{X_i\}_{i= 1}^{n_0+n}$ be the Markov chain with the stationary distribution $\pi$ and the initial distribution $\nu$. Suppose it admits a absolute spectral gap $\gamma$ and $\rchi=\rchi(\nu,\pi)<+\infty$. The function $f$ has a finite fourth central moment $\sigma_4^4[f]:=\Ebb_{\pi}[\left(f-\Ebb_{\pi}[f]\right)^4]$. If $n\geq \frac{32}{\gamma^3}$ and $n_0\geq\frac{2}{\gamma}(\log \rchi+\log(\sigma_4[f]/\sigma_2[f])+\log n)$, it holds that  
\begin{equation}
    \label{eq:lowerbound}
    \Ebb_{\nu}\left[\left(\frac{1}{n}\sum_{i=n_0+1}^{n+n_0}f(X_i)\right)^2\right]\geq \frac{\gamma}{4n}\Ebb_{\pi}[f^2].
\end{equation}

\end{lemma}
\begin{proof}
    We denote $Z=\frac{1}{n}\sum_{i=n_0+1}^{n+n_0}f(X_i)$ and $c=\Ebb_\pi[f]$. Then it holds
    \begin{equation}
        \label{eq:lb1}
        \Ebb_\nu[Z^2]=c^2+\Ebb_\nu[(Z-c)^2]+2c(\Ebb_\nu[Z]-c)
        \geq \frac{c^2}{2}+\Ebb_\nu[(Z-c)^2]-2(\Ebb_\nu[Z]-c)^2.
    \end{equation}
    Notice the difference 
    \begin{equation}
        \label{eq:diff}
        \begin{aligned}
            \left| \Ebb_{\nu}\left[(Z-c)^2\right] - \Ebb_{\pi}\left[(Z-c)^2\right]\right|
             \leq (1-\gamma)^{n_0}\rchi \cdot \left(  \Ebb_{\pi}\left[(Z-c)^4\right] \right)^{\frac{1}{2}}
        \end{aligned}
    \end{equation}
    where the inequality changes the measure as in \eqref{eq:change-measure}. It follows from the Cauchy-Schwarz inequality that
    \begin{align*}
        \Ebb_{\pi}\left[(Z-c)^4\right]
        =&~\Ebb_{\pi}\left[\left(\frac{1}{n}\sum_{i=n_0+1}^{n+n_0} f(X_i)-\Ebb_\pi[f]\right)^4\right] \\
        \leq&~ \Ebb_{\pi}\left[\frac{1}{n}\sum_{i=n_0+1}^{n+n_0} \left(f(X_i)-\Ebb_\pi[f]\right)^4\right] = \Ebb_{X\sim \pi}\left[\left(f(X)-\Ebb_\pi[f]\right)^4\right] = \sigma_4^4[f],
    \end{align*}
    where the second equality is because $\pi$ is the stationary distribution of the Markov chain.
    Therefore, we derive that
    \begin{equation}
        \label{eq:lb2}
        \Ebb_{\nu}\left[(Z-c)^2\right] \geq \Ebb_{\pi}\left[\left(\frac{1}{n}\sum_{i=n_0+1}^{n+n_0} f(X_i)-\Ebb_\pi[f]\right)^2\right] - (1-\gamma)^{n_0}\rchi\sigma_4^2[f].
    \end{equation}
    It holds from Theorem 3.1 in \cite{paulin2015concentration} that
    \begin{equation}
        \label{eq:lb3}
        \begin{aligned}
            \left|\Ebb_{\pi}\left[\left(\frac{1}{n}\sum_{i=n_0+1}^{n+n_0} f(X_i)-\Ebb_\pi[f]\right)^2 \right]-\frac{\sigma^2_{asy}[f]}{n}\right|\leq \frac{16 \sigma^2_2[f]}{\gamma^2n^2},
        \end{aligned}
    \end{equation}
    where $\sigma^2_{asy}[f]:=\langle f,[2(I-(\oP-\pi))^{-1}-I]f\rangle_{\pi}$ on page 24 of \cite{paulin2015concentration}. Using the spectral method therein, we obtain that 
    \begin{equation}
        \label{eq:lb4}
        \begin{aligned}
        \sigma^2_{asy}[f]&=\langle f,[2(I-(\oP-\pi))^{-1}-I]f\rangle_{\pi}=\norm{(I-\oP)^{-1}f}_{\pi}^2-\norm{P(I-P)^{-1}f}_{\pi}^2\\
        &\geq (1-(1-\gamma)^2)\norm{(I-\oP)^{-1}f}_{\pi}^2\geq \frac{\gamma\sigma^2_2[f]}{2}.
    \end{aligned}
    \end{equation}
    Next, we bound the term $(\Ebb_\nu[Z]-c)^2$ in \eqref{eq:lb1}. By Lemma \ref{lem:BernBiasVar}, it holds that
    \begin{equation}
        \label{eq:lb5}
        \abs{ \Ebb_\nu[Z]-c } = \abs{ \frac{1}{n}\sum_{i=n_0+1}^{n+n_0} \Ebb_{\pi} \left[f(X_i)\right]-\Ebb_\pi[f] } \leq \frac{(1-\gamma)^{n_0} \rchi \sigma_2[f] }{ \gamma n}.
    \end{equation}
    Finally, combining \eqref{eq:lb1}, \eqref{eq:lb2}, \eqref{eq:lb3}, \eqref{eq:lb4} and \eqref{eq:lb5} yields
    \begin{align*}
        \Ebb_{\nu}\left[\left(\frac{1}{n}\sum_{i=n_0+1}^{n+n_0}f(X_i)\right)^2\right]
        &\geq \frac{1}{2}\left(\Ebb_\pi[f]\right)^2 + \frac{\gamma\sigma^2_2[f]}{2n}-\frac{16\sigma^2_2[f]}{\gamma^2n^2}- (1-\gamma)^{n_0}\rchi\sigma_4^2[f] -\frac{2(1-\gamma)^{2n_0}\rchi^2\sigma^2_2[f]}{\gamma^2 n^2}\\
        &\geq \sigma^2_2[f]\left(\frac{\gamma}{n}-\frac{16}{\gamma^2n^2}-\frac{2(1-\gamma)^{2n_0}\rchi^2}{\gamma^2 n^2}-(1-\gamma)^{n_0}\rchi\cdot \frac{\sigma^2_4[f]}{\sigma^2_2[f]}\right)+ \frac{1}{2}\left(\Ebb_\pi[f]\right)^2
        \\
        &\geq \frac{\gamma}{4n}\sigma^2_2[f] + \frac{1}{2}\left(\Ebb_\pi[f]\right)^2
        \geq \frac{\gamma}{4n}\Ebb_{\pi}[f^2],
    \end{align*}
    where the third inequality holds when $n\geq \frac{32}{\gamma^3}$ and $n_0\geq\frac{2}{\gamma}(\log \rchi+\log(\sigma_4[f]/\sigma_2[f]) + \log n)$.
\end{proof}

The CNC condition \eqref{eq:cnc} in the following is of vital importance to analyze the saddle escaping property in VMC. That is one of the reason why stochastic optimization algorithms can escape from saddle points. If the CNC condition does not hold, the noise may not be strong enough to escape along the descent direction. The following lemma shows that the stochastic gradient $\hat{g}(\parameter;\mathbf{S})$ defined in \eqref{eq:approxgrad} satisfies CNC condition.
\begin{lemma}
    \label{lem:cnc}
    Let Assumptions \ref{asm:wavefun},\ref{asm:local}, \ref{asm:unigap} and  \ref{asm:sec} hold. Then, for the unit eigenvector $v$ with respect to the minimum eigenvalue of the Hessian, there exists $\mu=\frac{\eta \gamma}{16n}$ such that it holds 
    \begin{equation}
        \label{eq:cnc}
        \Ebb_{\nu}\left[\left(v^{\T}\hat{g}(\parameter;\mathbf{S})\right)^2\right]\geq \mu\sigma_2^2[E_{\parameter}],\quad \forall \parameter\in \Rbb^d,
    \end{equation}
    as long as $n\geq \Omega\left(\frac{1}{\eta\gamma^3}\right)+\tilde{\Omega}\left( \frac{\kappa B }{\sqrt{\eta}\gamma^2}\right)$  and $n_0\geq\frac{2}{\gamma}(\log \rchi+\log(2\kappa) + \log n)$ where $\Omega(\cdot)$ and $\tilde{\Omega}(\cdot)$ hide constants $M,B,C,\rchi$. 
\end{lemma}
\begin{proof}
   The MCMC estimate of the gradient $\hat{g}(\parameter,\mathbf{S})$ is computed by \eqref{eq:approxgrad}. For convenience, fix a unit vector $v$ and we denote empirical variables by $E_i=E_{\parameter}(\bmx_i),Y_i=v^{\T}\nabla_{\parameter}\log \Psi_{\parameter}(\bmx_i)$ and $\overline{E},\overline{Y}$ represents the average of $E_i,Y_i$. Let stationary variables $E=E_{\parameter}(\bmx)$ and $Y=v^{\T}\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)$ with a dependent variable $\bmx\sim \pi_{\parameter}$, then it holds that
    \begin{align*}
        v^{\T}\hat{g}(\parameter,\mathbf{S})&=\frac{2}{n}\sum_{i=n_0+1}^{n_0+n}\big(E_{\parameter}(\bmx_i)-\frac{1}{n}\sum_{i=n_0+1}^{n_0+n}E_{\parameter}(\bmx_i)\big)v^{\T}\nabla \log \Psi_{\parameter}(\bmx_i)\\
        &=\frac{2}{n}\sum_{i=n_0+1}^{n_0+n}E_iY_i-2\left(\overline{E}-\Ebb E+\Ebb E\right)\left(\overline{Y}-\Ebb Y+\Ebb Y\right)\\
        &=\underbrace{\frac{2}{n}\sum_{i=n_0+1}^{n_0+n}(E_i-\Ebb E)(Y_i-\Ebb Y) }_{Z_1}-\underbrace{2\left(\overline{E}-\Ebb E\right)\left(\overline{Y}-\Ebb Y\right)}_{Z_2}.
    \end{align*}
To obtain the lower bound, we have
\begin{align*}
    \Ebb_{\nu}\left[(v^{\T}\hat{g}(\parameter,\mathbf{S}))^2\right]=\Ebb_{\nu} \left[(Z_1-Z_2)^2\right]\geq \frac{1}{2}\Ebb_{\nu} \left[Z_1^2\right]-\Ebb_{\nu} \left[Z_2^2\right].
\end{align*}
Since $e^{x}> x^4/8$ when $x\geq 0$, it holds that $\sigma_{4}[E]\leq 2 \norm{E}_{\psi_1}$. It implies the kurtosis of the local energy $\sigma_4[E]/\sigma_2[E]$ is less than $2\kappa$.
Then, when $n\geq \frac{32}{\gamma^3}$ and $n_0\geq\frac{2}{\gamma}(\log \rchi+\log(2\kappa) + \log n)$, it follows from Lemma \ref{lem:lowerbound} that 
\begin{equation}
    \label{eq:Z1}
    \begin{aligned}
        \Ebb_{\nu}\left[ Z_1^2\right]& \geq \frac{\gamma}{4n}\Ebb_{\pi}[(E-\Ebb E)^2 (Y-\Ebb Y)^2]{\geq}\frac{\eta\gamma \sigma_2^2[E_{\parameter}]}{4n}.
    \end{aligned}
\end{equation}

% We introduce the Cramer-Rao's inequality in the multivariate case. Suppose $\parameter\in \Rbb^{d}$ is a parameter with probability density function $p_{\parameter}(x)$ and its Fisher information matrix $F(\parameter)$. Let $T(X)$ be an estimator of any function of parameters and denote its expectation $h(\parameter)=\Ebb_{X\sim p_{\parameter}}[T(X)]$.Then, the Cramer-Rao's inequality states that the variance of $T(X)$ satisfies
% \begin{equation*}
%     \mathrm{Var}_{p_{\parameter}}[T(X)]\geq (\nabla_{\parameter}h(\parameter))^{\T} F(\parameter) (\nabla_{\parameter}h(\parameter)).
% \end{equation*}
% As $(X-\Ebb X)(Y-\Ebb Y)$ is an estimator of $v^{\T}\nabla_{\parameter}\Lcal(\parameter)$ whose gradient is $H(\parameter)v$, it follows from the Cramer-Rao's inequality that
% \begin{equation}
%     \label{eq:CRineq}
%     \mathrm{Var}\left(XY- \Ebb Y \cdot X-\Ebb X \cdot Y\right)\geq v^{\T}H(\parameter) F^{-1}(\parameter) H(\parameter) v.
% \end{equation}
% When we take $v$ as the unit eigenvector corresponding to the minimum eigenvalue of the Hessian $\nabla_{\parameter}^2\Lcal(\parameter)$, together with \eqref{eq:Z1}, \eqref{eq:CRineq} implies that
% \begin{equation}
%     \Ebb\left[ Z_1^2\right]\geq \frac{4\zeta\lambda_0^2}{n M_F}.
% \end{equation}

Besides, the upper bound of $ \Ebb_{\nu} \left[Z_2^2\right]$ can be obtained by the Cauchy-Schwarz inequality that
\begin{equation}
    \label{eq:z2}
    \begin{aligned}
        \Ebb_{\nu} [Z_2^2]&=4\Ebb_{\nu}[(\bar{E}-\Ebb E)^2(\bar{Y}-\Ebb Y)^2]\leq 4\sqrt{\Ebb_{\nu}[(\bar{E}-\Ebb E)^4]\cdot \Ebb_{\nu}[(\bar{Y}-\Ebb Y)^4]}.
    \end{aligned}
\end{equation}
By the result in the proof of Lemma \ref{lem:BernBiasVar}, we have
\begin{equation}
    \label{eq:Xsquare}
    \Ebb_\nu[\abs{\bar{E}-\Ebb E}^4]\leq \bigO{ \frac{\sigma_2^4[E_{\parameter}]}{n^2\gamma^2}+\frac{M^4(\log n)^8}{n^{4}\gamma^4} }.
\end{equation}
% To estimate fourth central moment, we use the high probability bound in \eqref{eq:highprobbound}. Reusing notations in Lemma \ref{lem:BernBiasVar}, we write that for any $\delta>0$, it holds that
% \begin{equation*}
%     \Pbb\left(\abs{\Delta}\leq s\right)\geq 1-\delta,\quad s\geq\max\left\{\frac{8\sigma_2[f]\sqrt{\log(4C/\delta)}}{\sqrt{n \gamma}},\frac{40M\left(\log(4Cn/\delta)\right)^2 }{n\gamma}\right\}.
% \end{equation*}
% By computing the following integral, we obtain the fourth moment bound
% \begin{equation}
%     \label{eq:fourthmoment}
%     \Ebb_\nu[\abs{\Delta}^4]=\int_{0}^{+\infty}4 s^3 \Pbb(\abs{\Delta}>s)ds\leq \frac{8^5C\sigma_2^4[f]}{n^2\gamma^2}+\frac{10080 C(80M)^4}{n^{3}\gamma^4}.
% \end{equation}
% Under Assumption \ref{asm:sec}, it follows from \eqref{eq:fourthmoment} and $n\geq \frac{4096}{\gamma^3}$ that
% \begin{equation}
%     \label{eq:Xsquare}
%     \Ebb(\bar{X}-\Ebb X)^2\leq \frac{8^5C\sigma_2^4[E_{\parameter}]}{n^2\gamma^2}+\frac{10080 C(80M)^4}{n^{3}\gamma^4}\leq \frac{40^5\kappa^4C\sigma_2^4[E_{\parameter}]}{n^2\gamma^2}.
% \end{equation}
Similarly, notice that $\abs{Y}=\abs{v^{\T}\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)}\leq \norm{v}\norm{\nabla_{\parameter}\log \Psi_{\parameter}(\bmx)}\leq B$, we can apply \cite[Theorem 12]{fan2021hoeffding} to obtain 
\begin{equation*}
    \Pbb\left(|\bar{Y}-\Ebb Y|\geq t \right)\leq 2C \exp\left(-\frac{n\gamma t^2}{4B^2} \right).
\end{equation*}  
Using the similar technique in the proof of Lemma \ref{lem:Bern-exp}, we derive
\begin{equation}
    \label{eq:Ysquare}
    \Ebb_{\nu}[|\bar{Y}-\Ebb Y|^4]\leq \bigO{ \frac{B^4}{n^2\gamma^2} }.
\end{equation}
% \begin{equation*}
%     \Pbb\left(|\bar{Y}-\Ebb Y|\geq t \right)\leq 2C \exp\left(-\frac{n\gamma t^2}{4B^2} \right).
% \end{equation*}  
% It implies that
% \begin{equation}
%     \label{eq:Ysquare}
%     \Ebb_{\nu}[|\bar{Y}-\Ebb Y|^4]=\int_{0}^{+\infty}4t^3\Pbb\left(|\bar{Y}-\Ebb Y|\geq t \right)dt\leq \frac{64C B^4}{n^2\gamma^2}.
% \end{equation}
Plugging \eqref{eq:Xsquare} and \eqref{eq:Ysquare} into \eqref{eq:z2} yields
\begin{equation*}
    \Ebb_{\nu} [Z_2^2]\leq 4\sqrt{\Ebb_{\nu}[(\bar{E}-\Ebb E)^4]\cdot \Ebb_{\nu}[(\bar{Y}-\Ebb Y)^4]}
    \leq \bigO{ \frac{\sigma_2^{2}[E_{\parameter}]B^2}{n^2\gamma^2} + \frac{(\log n)^4M^2B^2}{n^3\gamma^3} }.
    % \leq \frac{409600\kappa^2CB^2\sigma_2^{2}[E_{\parameter}]}{n^2\gamma^2}\leq \frac{\eta\gamma \sigma_2^2[E_{\parameter}]}{32n},
\end{equation*}
Therefore, $\Ebb_{\nu} [Z_2^2]\leq \frac{\eta\gamma \sigma_2^2[E_{\parameter}]}{32n}$ as long as $n\geq \Omega\left(\frac{1}{\eta\gamma^3}\right)+\tilde{\Omega}\left( \frac{\kappa B }{\sqrt{\eta}\gamma^2}\right)$.
% when $n\geq \frac{c\kappa^2CB^2}{\eta\gamma^3}$ with a constant $c$. 
Finally, combining our discussion of $Z_1$ and $Z_2$, we derive that
\begin{equation}
    \Ebb_{\nu}\left[(v^{\T}\hat{g}(\parameter,\mathbf{S}))^2\right]\geq \frac{1}{2}\Ebb_{\nu} \left[Z_1^2\right]-\Ebb_{\nu} \left[Z_2^2\right]\geq \frac{\eta\gamma \sigma_2^2[E_{\parameter}]}{32n}.
\end{equation}
This completes the proof.
\end{proof}

\subsection{Second order stationary}

As discussed above, VMC performs a special SGD with the biased gradient estimator. While the convergence of SGD is well-understood for convex functions, the existence of saddle points and local minimum poses challenges for non-convex optimization. Since the ordinary GD  often stucks near the saddle points, the additional noise within SGD allows it to escape from saddle points. By this way, the simple SGD without explicit improvement is proven to have second-order convergence. 

We now give the definition of approximate second-order stationary point.
\begin{definition}[Approximate second-order stationary point]
    Given a function $\Lcal$, an $(\epsilon_g,\epsilon_h)$ approximate second-order stationary point $\parameter$ of $\Lcal$ is defined as 
    \begin{equation*}
        \norm{g(\parameter)}\leq \epsilon_g,\quad \lambda_{\min}\left(H(\parameter)\right)\leq -\epsilon_h,
    \end{equation*}
    where $g$ and $H$ denote the gradient and Hessian of $\Lcal$ respectively.
\end{definition}
    
If $\epsilon_g=\epsilon_h=0$, the point $\parameter$ is a second-order stationary point. The second order analysis contributes to understand how VMC performs well in solving eigenvalue problems.

We simultaneously consider the convergence of the VMC method from the perspective of optimization and  eigenvalue equations. When the exact wavefunction $\Psi$ satisfies $\Hcal \Psi=\lambda \Psi$, the variance of the local energy $\sigma_2^2[E]$ is equal to zero. Naturally, we define $\epsilon$-variance points as a criteria for the trial wavefunction approximation.
\begin{definition}[$\epsilon$-variance point]
    For the optimization problem \eqref{loss:vmc}, we call $\parameter$ an $\epsilon$-variance point if the local energy satisfies $\sigma_2^2[E_{\parameter}]< \epsilon$.
\end{definition}
For notational simplification, we take an abbreviation $\sigma_2^2=\sigma_2^2[E_{\parameter}]$ in this subsection.

To escape from saddle points, a new stepsize schedule is established for Algorithm \ref{alg:VMC}. Given a period $T$, note that $\alpha$ and $\beta$ are the constant stepsizes with $\beta>\alpha>0$, with values given in Table \ref{table:1}. Within one iteration period of $T$ steps, we adopt a large stepsize $\beta$ at the beginning of the period and a small one $\alpha$ at the other $T-1$ iterations, that is,
\begin{equation}
    \label{eq:schedule}
    \alpha_k=\begin{cases}
        \alpha, \quad k (\mathrm{mod}T)\not =0,\\
        \beta,  \quad k (\mathrm{mod}T) =0.
    \end{cases}
\end{equation}
It will be shown that the schedule can be suitably designed to achieve sufficient descent in one period.

Suppose the total number of iterations $K$ is a multiple of $T$ and there are $K/T$ periods. We denote $\tparameter_m=\parameter_{m\cdot T}$ for $m=0,1,\dots,K/T$ in each period. For some given $\epsilon>0$, we consider four regimes of the iterates $\{\tparameter_m\}$ as follow,
\begin{align}
    \Rcal_1&:=\left\{\parameter\Big| \norm{g(\parameter)}\geq \epsilon\right\},\label{eq:R1}\\
    \Rcal_2&:=\left\{\parameter\Big| \norm{g(\parameter)}< \epsilon,~\lambda_{\min}\left(H(\parameter)\right)\leq -\epsilon^{\frac{1}{4}} ,~~\sigma_2^2[E_{\parameter}]\geq \epsilon^{\frac{1}{2}} \right\},\label{eq:R2}\\
    \Rcal_3&:=\left\{\parameter\Big| \norm{g(\parameter)}< \epsilon,~\lambda_{\min}\left(H(\parameter)\right)>-\epsilon^{\frac{1}{4}} ,~\mathrm{or}~\sigma_2^2[E_{\parameter}]<\epsilon^{\frac{1}{2}} \right\},\label{eq:R3}
\end{align}
$\Rcal_1$ stands for the regime with a large gradient, where the stochastic gradient works effectively. When the iterate lies in $\Rcal_2$, despite being close to a first-order stationary point, the CNC condition mentioned in section \ref{subsec:cnc} guarantees a decrease after $T$ iterations under our schedule. $\Rcal_3$ is a regime of $(\epsilon,\epsilon^{1/4})$ approximate second order stationary points or $\epsilon^{1/2}$ variance points. We need to show Algorithm \ref{alg:VMC} will reach $\Rcal_3$ with high probability, that is, converge to approximate second order stationary points.

The analysis below relies on a particular choice of parameters, whose values satisfy the following lemmas and the main theorem. For ease of verification, the choice of parameters is collected in Table \ref{table:1}. 
\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.8}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
     Parameter& Value & Order & Conditions  \\ \hline
     $\beta $ & $\frac{ \delta\epsilon^2}{192 l_g\rho L V_{n,n_0}}$ & $O(\epsilon)$ &     \eqref{eq:R1conditions}, \eqref{eq:conditions_1} \\ \hline
     $\alpha$& $\frac{\beta}{\sqrt{T}}$ & $  O\left(\epsilon^{9/4}\log^{-1}\left(\frac{1}{\epsilon}\right)\right)$&  \eqref{eq:conditions_1}  \\ \hline
     $\Lthre$&$\frac{\beta \epsilon^2 }{192 \rho l_g}$ & $ O(\epsilon^{3})$ &  \eqref{eq:R1conditions},\eqref{eq:conditions_1}  \\ \hline
     $n$&  $\frac{\eta \gamma }{64\epsilon}$ & $O\left(\epsilon^{-1}\right)$ &  Lemma \ref{lem:cnc}  \\ \hline
     $n_0$&  - & $O\left(\log\left(\frac{1}{\epsilon}\right)\right)$ &  \eqref{eq:BV}, Lemma \ref{lem:cnc}  \\ \hline
     $B_{n,n_0}$ & $\sqrt{\frac{\epsilon^{2} }{96 T^{3/2} l_g\rho}} $  & $ O(\epsilon^{3})$ &  \eqref{eq:R1conditions}, \eqref{eq:conditions_1}  \\ \hline
     $T$& $\frac{1}{\beta^2\epsilon^{1/2}}
     \log^2\left(\frac{\rho l_g L V_{n,n_0}}{\mu \delta \epsilon} \right)$ &  $ O\left(\epsilon^{-5/2}\log^2\epsilon\right)$& \eqref{eq:Tconst}    \\ \hline
     $K$& $\frac{2[\Lcal(\parameter_0)-\Lcal^{*}]T}{\delta \Lthre}$ & $ O\left(\epsilon^{-11/2}\log^2\epsilon\right)$ &   \eqref{eq:Kconst}  \\ \hline
    \end{tabular}
    \caption{List of parameter values used in the convergence analysis.}
    \label{table:1}
\end{table}

With a large gradient, it is easy to show a sufficient decrease of the objective function value. We have analyzed the decrease for each iteration in Lemma \ref{lem:decrease}, and it follows a similar argument for the biased SGD in VMC.
\begin{lemma}
    \label{lem:R1}
    Suppose that $\tparameter_m$ lies in $\Rcal_1$ defined in \eqref{eq:R1} and Algorithm \ref{alg:VMC} updates with the schedule \eqref{eq:schedule} and parameters in Table \ref{table:1}, then the expected value of $\Lcal(\tparameter_{m+1})$ taken over the randomness of $\{\parameter_{k}\}_{k=m\cdot T+1}^{(m+1)\cdot T}$ decreases as 
    \begin{equation}
        \Lcal(\tparameter_{m})-\Ebb[\Lcal(\tparameter_{m+1})|\tparameter_{m}]\geq \Lthre.
    \end{equation}
\end{lemma}
\begin{proof}
    We first decompose the difference of the expected function value into each iteration,
    \begin{equation}
        \label{eq:period-decrease}
        \Lcal(\tparameter_m)-\Ebb[\Lcal(\tparameter_{m+1})|\tparameter_{m}]=\sum_{p=0}^{T -1}\epct{\Lcal(\parameter_{m\cdot T +p})-\epct{\Lcal(\parameter_{m\cdot T +p+1})}\bigg|\parameter_{m\cdot T +p}},
    \end{equation}
    where $\epct{\Lcal(\parameter_{m\cdot T})|\parameter_{m\cdot T }}=\Lcal(\tparameter_{m})$ due to the definition $\tparameter_{m}=\parameter_{m\cdot T}$. Using the choice $\beta^2= T\alpha^2$ in Table \ref{table:1}, it follows from \eqref{eq:period-decrease} that
    \begin{equation}
        \label{eq:R1ineq}
        \begin{aligned}
            2\left(\Lcal(\tparameter_m)-\Ebb[\Lcal(\tparameter_{m+1})|\tparameter_{m}]\right)\geq &\beta\norm{g(\tparameter_m)}^2-\beta B_{n,n_0}^2-\beta^2LV_{n,n_0}\\
            &\quad -(T-1)\left(\alpha B_{n,n_0}^2+\alpha^2LV_{n,n_0} \right)\\
            \geq &\beta\norm{g(\tparameter_m)}^2-2T\alpha B_{n,n_0}^2-2\beta^2LV_{n,n_0},
        \end{aligned}
    \end{equation}
    where the first inequality is by Lemma \ref{lem:decrease} for $p=0,\dots,T-1$ and the second is by the direct substitution.
    Then, by the choice of $\beta,n_0,\Lthre$ in Table \ref{table:1}, these conditions hold that
    \begin{equation}
        \label{eq:R1conditions}
        \beta\leq \frac{\epsilon^2}{8LV_{n,n_0}},\quad B_{n,n_0}^2\leq \frac{\epsilon^2}{8\sqrt{T}},\quad \Lthre \leq \frac{\beta\epsilon^2}{4}.
    \end{equation}
    As $\tparameter_m$ lies in $\Rcal_1$, which means $\norm{\nabla_{\parameter}\Lcal(\tparameter_m)}\geq \epsilon$, we plug \eqref{eq:R1conditions} into \eqref{eq:R1ineq} and obtain that
    \begin{equation}
        \begin{aligned}
            2\left(\Lcal(\tparameter_m)-\Ebb[\Lcal(\tparameter_{m+1})|\tparameter_{m}]\right)\geq &\beta \epsilon^2-2\beta\sqrt{T}B_{n,n_0}^2-2\beta LV_{n,n_0}\cdot \frac{\epsilon^2}{8LV_{n,n_0}}\\
            \geq& \beta \epsilon^2-\frac{\beta \epsilon^2}{4}-\frac{\beta \epsilon^2}{4}= \frac{\beta\epsilon^2}{2}\geq 2\Lthre.
        \end{aligned}
    \end{equation}
    This completes the proof.
\end{proof}

Near the saddle points, the classical GD gets stuck if the gradient is orthogonal to the negative curvature direction. However, the stochastic gradient with the CNC condition has inherent noise along the negative curvature direction. Under our stepsize schedule \eqref{eq:schedule}, the objective function value can have sufficient decrease after a period.
\begin{lemma}
    \label{lem:R2}
    Suppose $\tparameter_m$ lies in $\Rcal_2$ defined in \eqref{eq:R2} and Algorithm \ref{alg:VMC} updates with the schedule \eqref{eq:schedule} and parameters in Table \ref{table:1}, then the expected value of $\Lcal(\tparameter_{m+1})$ taken over the randomness of $\{\parameter_{k}\}_{k=m\cdot T+1}^{(m+1)\cdot T}$ decreases as 
    \begin{equation}
        \Lcal(\tparameter_{m})-\Ebb[\Lcal(\tparameter_{m+1})|\tparameter_{m}]\geq \Lthre.
    \end{equation}
\end{lemma}
\begin{proof}
\newcommand{\asumgh}{A_1}
    
    The proof is by contradiction. Without loss of generality, we suppose that $m=0$ in this proof and denote
    \begin{equation*}
        \Lcal_p=\Lcal\left( \parameter_p\right),~~g_{p}=\nabla_{\parameter}\Lcal\left( \parameter_p\right),~~
            \hat{g}_{p}=\hat{g}\left( \parameter_p,\mathbf{S}_{p}\right),~~ H_p=\nabla^2_{\parameter}\Lcal\left( \parameter_p\right)
    \end{equation*}
    for $p=0,\dots,T-1$. Every expectation in this proof is taken over all existed $\parameter_{p}$ in every formula. We assume the expected function value decreases by no more than $\Lthre$, i.e.,
    \begin{equation}
        \label{eq:fakedecrease}
        \Lcal_0-\epct{\Lcal_{T }}< \Lthre.
    \end{equation}
    We proceed to show that the assumption \eqref{eq:fakedecrease} is invalid.

    We start with estimating the expected distance between $\parameter_0$ and $\parameter_p$ for $p=1,\dots,T-1$. By Lemma \ref{lem:decrease}, it holds
    \begin{equation}
        \label{eq:R2dec1}
        \begin{aligned}
            2\left(\Lcal_0-\epct{\Lcal_{T }}\right)\geq & \beta\norm{g_0}^2-\beta B^2_{n,n_0}-\beta^2 L V_{n,n_0}
            +\sum_{h=1}^{T -1}\left(\alpha \Ebb\norm{g_h}^2-\alpha B^2_{n,n_0}-\alpha^2 L V_{n,n_0}\right)\\
            \geq & \beta\norm{g_0}^2 + \alpha \sum_{h=1}^{T-1}\Ebb\norm{g_h}^2- 2T\alpha B^2_{n,n_0}-2\beta^2LV_{n,n_0},
        \end{aligned}
    \end{equation}
    where the first inequality is derived similarly to \eqref{eq:R1ineq}, and the second inequality is due to $\beta=\sqrt{T}\alpha$.
    Together with the assumption \eqref{eq:fakedecrease}, it follows that
    \begin{equation}
        \label{eq:R2dec2}
        \beta\norm{g_0}^2+\alpha\sum_{h=1}^{T-1}\Ebb\norm{g_h}^2\leq 2\Lthre+2T\alpha B^2_{n,n_0}+2\beta^2 L V_{n,n_0}=:\asumgh.
    \end{equation}
    A direct implication of \eqref{eq:R2dec2} is that
    \begin{equation}
        \label{eq:R2dec3}
        \beta\norm{g_0}^2\leq \asumgh, \qquad
        \alpha\Ebb\norm{\sum_{h=1}^{p}g_h}^2\leq p\alpha\sum_{h=1}^{p}\Ebb\norm{g_h}^2\leq p\asumgh.
    \end{equation}
    
    We proceed to bound $\parameter_{p+1}-\parameter_0=\beta \hat{g}_0+\alpha\sum_{h=1}^{p}\hat{g}_h$ as follows. Firstly,
    \begin{equation}
        \label{eq:distdecomp2}
        \begin{aligned}
            \Ebb\norm{\sum_{h=1}^{p}(\hat{g}_h-g_h)}^2
            &= \sum_{h=1}^{p}\Ebb\norm{\hat{g}_h-g_h}^2+2\sum_{1\leq h<l\leq p}\Ebb\left\langle \hat{g}_h-g_h,\hat{g}_l-g_l\right\rangle\\
            &= \sum_{h=1}^{p}\Ebb\norm{\hat{g}_h-g_h}^2+2\sum_{1\leq h<l\leq p}\Ebb\left\langle \hat{g}_h-g_h,\Ebb[\hat{g}_l|\parameter_l]-g_l\right\rangle\\
            &\leq pV_{n,n_0}+p(p-1)B_{n,n_0}\sqrt{V_{n,n_0}}
            \leq 2pV_{n,n_0}+p^3B_{n,n_0}^2,
        \end{aligned}
    \end{equation}
    where the second equality is because we can take conditional expectation on $\parameter_l$ first, the following inequality holds from the result $\Ebb\norm{\hat{g}_h-g_h}^2\leq V_{n,n_0}$ and $\norm{\Ebb[\hat{g}_l|\parameter_l]-g_l}\leq B_{n,n_0}$ in Lemma \ref{lem:gbiasVar}, and the last inequality is due to AM-GM inequality. Therefore, we can bound
    \begin{equation}
        \label{eq:distbound}
        \begin{aligned}
            \Ebb\norm{\parameter_{p+1}-\parameter_0}^2
            &=\Ebb\norm{\beta\hat{g}_0+ \alpha\sum_{h=1}^{p}\hat{g}_h}^2\\
            &=\Ebb\norm{\beta g_0+\beta(\hat{g}_0-g_0)+ \alpha\sum_{h=1}^{p}g_h+\alpha\sum_{h=1}^{p}(\hat{g}_h-g_h)}^2\\
            &\leq \underbrace{ 4\beta^2\norm{g_0}^2 }_{\eqref{eq:R2dec3}}
            +\underbrace{ 4\beta^2\Ebb\norm{\hat{g}_0-g_0}^2 }_{\text{\cref{lem:gbiasVar}}}
            +\underbrace{ 4\alpha^2\Ebb\norm{\sum_{h=1}^{p}g_h}^2 }_{\eqref{eq:R2dec3}}
            +\underbrace{ 4\alpha^2\Ebb\norm{\sum_{h=1}^{p}(\hat{g}_h-g_h)}^2 }_{ \eqref{eq:distdecomp2} }\\
            &\leq 4\beta \asumgh + 4\beta^2 V_{n,n_0} + 4\alpha p \asumgh + 2\alpha^2(2pV_{n,n_0}+p^3B_{n,n_0}^2)\\
            &\leq 4\beta \asumgh + 8\beta^2 V_{n,n_0} + 4 p (\alpha\asumgh+ \alpha^2 T^2 B_{n,n_0}^2 ),
        \end{aligned}
    \end{equation}
    where the second inequality is due to \eqref{eq:R2dec3}, Lemma \ref{lem:gbiasVar} and \eqref{eq:distdecomp2}, and the final follows from $p\alpha^2\leq T\alpha^2 = \beta^2$. Thus, we can take 
    \begin{align*}
        A_2&:=8\alpha\Lthre+16\alpha^2 T^2B_{n,n_0}^2+ 8\alpha\beta^2 LV_{n,n_0},\\
        A_3&:=8\beta\Lthre+8\alpha\beta T B_{n,n_0}^2+ 16\beta^2 V_{n,n_0},
    \end{align*}
    and then
    \begin{align}\label{eq:dtheta-linear}
        \Ebb\norm{\parameter_{p}-\parameter_0}^2 \leq (p-1)A_2+A_3.
    \end{align}
    When we take $p=T-1$, \eqref{eq:distbound} shows that the expected distance between $\parameter_0$ and $\parameter_T$ is bounded by a quadratic function of $T$.

    We further prove the expected distance between $\parameter_0$ and $\parameter_T$ grows at least exponentially for $T$, leading to a contradiction. Since $\parameter_{p}$ stays close to $\parameter_0$, the quadratic Taylor approximation of the function $\Lcal$ at $\parameter_0$ is introduced as
    \begin{equation*}
        Q(\parameter):=\Lcal_0+g_0^{\T}(\parameter-\parameter_0)+\frac{1}{2}(\parameter-\parameter_0)^{\T}H_0(\parameter-\parameter_0).
    \end{equation*}
    We denote $Q_p=Q(\parameter_p)$ and $q_p=\nabla_{\parameter}Q(\parameter_p)=g_0+H_0(\parameter_p-\parameter_0)$ for $p=0,\dots,T-1$. Using the Taylor approximation is firstly proposed in \cite{ge2015escaping}. As $\Lcal$ is twice-differentiable with a $\rho$-Lipschitz Hessian, \cite[Lemma 1.2.4]{nesterov2003introductory} gives that 
    \begin{equation}
        \label{eq:Taylorapprox}
        \norm{\nabla_{\parameter}\Lcal(\parameter)-\nabla_{\parameter}Q(\parameter)}\leq \frac{\rho}{2} \norm{\parameter-\parameter_0}^2.
    \end{equation}
    Thus, $\norm{q_h-g_h}^2\leq \frac\rho2\norm{\parameter_h-\parameter_0}^2$.
    To derive the lower bound, $\parameter_{p+1}-\parameter_0$ is decomposed as
    \begin{equation*}
        \label{eq:decompTaylor}
        \begin{aligned}
            \parameter_{p+1}-\parameter_0&=\parameter_{p}-\parameter_0-\alpha\hat{g}_{p}\\
            &=\parameter_{p}-\parameter_0-\alpha q_p-\alpha(\hat{g}_p-g_p+g_p-q_p)\\
            &=(I-\alpha H_0)(\parameter_{p}-\parameter_0)-\alpha g_0-\alpha(\hat{g}_p-g_p+g_p-q_p).
        \end{aligned}
    \end{equation*}
    Let $-\lambda_0<0$ be the minimum eigenvalue of the Hessian $H_0=H(\parameter_0)$, and let $v$ be the unit eigenvector with respect to $-\lambda_0$ (which is deterministic conditional on $\parameter_0$). Then $(I-\alpha H_0)v=(1+\alpha \lambda_0) v=\kappa v$, and hence
    \begin{align*}
        \iprod{v}{\parameter_{p+1}-\parameter_0}= \kappa \iprod{v}{\parameter_{p+1}-\parameter_0} - \alpha \iprod{v}{g_0} - \alpha \iprod{v}{\hat{g}_p-q_p}.
    \end{align*}
    Recursively expanding this equality out, we finally obtain 
    \begin{align}
        &\iprod{v}{\parameter_{p+1}-\parameter_0}
        = \kappa^p \iprod{v}{\parameter_{1}-\parameter_0} - \alpha \iprod{v}{g_0}\sum_{h=1}^p \kappa^{p-h} - \alpha \sum_{h=1}^p \kappa^{p-h}\iprod{v}{\hat{g}_h-q_h}\notag\\
        &\qquad = \kappa^p \bigg[\beta  \underbrace{ \iprod{v}{-\hat{g}_0} }_{u} - \alpha \underbrace{ \frac{1-\kappa^{-p}}{\kappa-1} \iprod{v}{g_0} }_{d_p} - \alpha \underbrace{ \sum_{h=1}^p \kappa^{-h}\iprod{v}{\hat{g}_h-g_h} }_{\xi_p}
        - \alpha \underbrace{ \sum_{h=1}^p \kappa^{-h}\iprod{v}{g_h-q_h} }_{\delta_p} \bigg]. \label{eq:decompTaylor1}
    \end{align} 
    Therefore,
    \begin{equation}
        \label{eq:R2decp}
        \begin{aligned}
            \Ebb\norm{\parameter_{p+1}-\parameter_0}^2
            \geq& \Ebb\iprod{v}{\parameter_{p+1}-\parameter_0}^2
            =\kappa^{2p}\Ebb\left(\beta u-\alpha d_p-\alpha \xi_p-\alpha \delta_p\right)^2\\
            \geq& \kappa^{2p} \left(\beta^2 \Ebb [u^2] - 2\alpha\beta \epct{ud_p} - 2\alpha\beta\epct{u\xi_p} - 2\alpha\beta\epct{u\delta_p} \right).
        \end{aligned}
    \end{equation}
    
    % Therefore, the lower bound can be estimated by 
    % \begin{equation}
    %     \label{eq:R2decp}
    %     \Ebb\norm{\parameter_{p+1}-\parameter_0}^2\geq \Ebb\norm{u_p}^2-2\alpha \epct{ \iprod{u_p}{\delta_p}}-2\alpha \epct{ \iprod{u_p}{d_p}}-2\alpha  \epct{ \iprod{u_p}{\xi_p}},
    % \end{equation}
    % where we use the fact that $\norm{a+b}^2\geq \norm{a}^2+2\iprod{a}{b}$. 
    
    % Now we bound each term on the RHS of \eqref{eq:R2decp}. We introduce the notation
    % \begin{equation*}
    %     \kappa :=1+\alpha \lambda_{0}, ~~\lambda_{0}=\abs{ \lambda_{\min}\left(H_0\right)}.
    % \end{equation*}
    % Since $0<\alpha<\frac{1}{L}$, the eigenvalue of $I-\alpha H_0$ lies in $(0,\kappa]$. As $1<\kappa<2$, it holds that %\cfn{easy formula, no need to state here (instead, we can state it inline after inequalities if needed)} 
    % \begin{equation}
    %     \label{eq:summation}
    %     \sum_{h=1}^{p}\kappa^{p-h}\leq \frac{2\kappa^p}{\kappa-1},\quad \sum_{h=1}^{p}\kappa^{p-h}h\leq \frac{2\kappa^p}{(\kappa-1)^2}.
    % \end{equation}
    By the Cauchy-Schwarz inequality, Lemma \ref{lem:cnc} implies that 
    \begin{equation}
        \label{eq:right1}
        \begin{aligned}
            \Ebb u^2&= \epct{(v^{\T}\hat{g}_0)^2}\geq \mu\sigma_2^2.
        \end{aligned}
    \end{equation}
    where $\mu=\frac{\eta \gamma}{16n}$.
    Next, because $d_p$ is deterministic, the term $\epct{ud_p}$ can be bounded as
    \begin{equation}
        \label{eq:d_p}
        \begin{aligned}
            \epct{ud_p}
            =& -d_p\Ebb \iprod{v}{\hat{g}_0}
            =-d_p\iprod{v}{g_0}+d_p\Ebb \iprod{v}{g_0-\hat{g}_0}
            \leq d_p\Ebb \iprod{v}{g_0-\hat{g}_0} \leq \frac{l_gB_{n,n_0}}{\kappa-1},
        \end{aligned}
    \end{equation}
    where the first inequality is due to $-d_p\iprod{v}{g_0}=-\frac{1-\kappa^{-p}}{\kappa-1}\iprod{v}{g_0}^2\leq 0$, and the second inequality uses Lemma \ref{lem:gbiasVar}.
    
    
    We next upper bound the term $\epct{u\xi_p}$ as follows.
    \begin{equation}
        \label{eq:xi_p}
        \begin{aligned}
            \epct{u\xi_p}
            =& \Ebb u\sum_{h=1}^p \kappa^{-h}\iprod{v}{\hat{g}_h-g_h} 
            = \epct{ u\sum_{h=1}^p \kappa^{-h}\iprod{v}{\Ebb[\hat{g}_h|\parameter_h]-g_h} }\\
            \leq& \epct{ |u|\sum_{h=1}^p \kappa^{-h}\norm{\Ebb[\hat{g}_h|\parameter_h]-g_h} }
            \leq \epct{ l_g\sum_{h=1}^p \kappa^{-h}B_{n,n_0} }
            \leq \frac{l_g B_{n,n_0}}{\kappa-1}.
            % \leq& \Ebb \left(\sum_{h=1}^p \kappa^{p-h}\norm{\hat{g}_h-g_h}\right)^2 \\
            % \leq& \left(\Ebb \sum_{l=1}^p \kappa^{p-l}\right) \cdot \left(\sum_{h=1}^p \kappa^{p-h}\norm{\hat{g}_h-g_h}^2\right) \\
            % \leq& \frac{\kappa^p}{\kappa-1} \sum_{h=1}^p \kappa^{p-h} \Ebb\norm{\hat{g}_h-g_h}^2
            % \leq \frac{\kappa^{2p}}{(\kappa-1)^2} V_{n,n_0}
        \end{aligned}
    \end{equation}
    where the second inequality is due to the Cauchy-Schwarz inequality, and in the last inequality we use Lemma \ref{lem:gbiasVar}. 

    Finally, we bound the term $\epct{u\delta_p}$:
    \begin{equation}
        \label{eq:delta_p}
        \begin{aligned}
            \epct{u\delta_p}
            =& \epct{ u\sum_{h=1}^p \kappa^{-h}\iprod{v}{g_h-q_h} }
            \leq \epct{ l_g \sum_{h=1}^p \kappa^{-h}\norm{g_h-q_h} }\\
            \leq& \frac{l_g\rho}{2} \epct{ \sum_{h=1}^p \kappa^{-h}\norm{\parameter_h-\parameter_0}^2 }
            \leq \frac{l_g\rho}{2} \sum_{h=1}^p \kappa^{-h}\left(A_2(h-1)+A_3\right) \\
            \leq& \frac{l_g\rho}{2} \left[ \frac{A_2}{(\kappa-1)^2}+\frac{A_3}{\kappa-1} \right],
        \end{aligned}
    \end{equation}
    where the second inequality is due to \eqref{eq:gradientbound}, the third inequality uses \eqref{eq:dtheta-linear}, and the last inequality is because $\sum_{h=1}^p \kappa^{-h}(h-1)\leq \frac{1}{(\kappa-1)^2}$.
    
    % Finally for the last term on the RHS of \eqref{eq:R2decp}, it is derived similarly that
    % \begin{equation}
    %     \label{eq:right4}
    %     \begin{aligned}
    %         \epct{\iprod{u_p}{\xi_p}}&=-\beta \epct{\hat{g}_0^{\T}\sum_{h=1}^{p}(I-\alpha H_0)^{2p-h}(g_h-\hat{g}_h)}\\
    %         &\leq -\beta \Ebb\left[\norm{\hat{g}_0}\sum_{h=1}^{p}\kappa^{2p-h}\norm{g_h-\epct{\hat{g}_h}}\right]\\
    %         &\leq 2\beta l_g B_{n,n_0}\cdot \sum_{h=1}^{p}\kappa^{2p-h}\overset{\eqref{eq:summation}}{\leq}2\beta l_g B_{n,n_0}\cdot \frac{\kappa^{2p}}{\kappa-1},
    %     \end{aligned}
    % \end{equation}
    % where the first inequality is also due to $\norm{I-\alpha H_0}\leq \kappa$ and the second follows from Lemma \ref{lem:gbiasVar} and \eqref{eq:gradientbound}.
    % By the choice of parameters in Table \ref{table:1}, there hold that
    % \begin{equation}
    %     \label{eq:conditions}
    %     \begin{aligned}
    %         B_{n,n_0}\leq \frac{\eta \beta \lambda_0^2}{64 l_g n},~ B_{n,n_0}^{2}\leq \frac{\eta\lambda_0^4}{64(\sqrt{T}+1) \rho l_g n}&\Rightarrow\frac{8l_g B_{n,n_0}}{\lambda_0}\leq \frac{\eta \beta \lambda_0^2}{8n},~ \frac{8(\beta+2T\alpha) l_g\rho B_{n,n_0}^2}{\lambda_0^2}\leq \frac{\eta \beta \lambda_0^2}{8n},\\
    %      \beta \leq \frac{\eta\lambda_0^3}{32\rho l_g^3n},~ \beta\leq \frac{\eta \lambda_0^4}{128\rho l_g LV_{n,n_0}n}&\Rightarrow\frac{4\beta^2l_g^3\rho}{\lambda_0}\leq \frac{\eta \beta \lambda_0^2}{8n},~\frac{16\beta^2 l_g\rho LV_{n,n_0}}{\lambda_0^2}\leq \frac{\eta \beta \lambda_0^2}{8n}\\
    %      \alpha\leq \frac{\eta \beta \lambda_0^4}{64\rho l_gV_{n,n_0}n},~\Lthre \leq \frac{\eta \beta\lambda_0^4}{128\rho l_g n}&\Rightarrow\frac{8\alpha l_g\rho V_{n,n_0}}{\lambda_0^2}\leq \frac{\eta \beta \lambda_0^2}{8n},~\frac{16 l_g\rho \Lthre}{\lambda_0^2}\leq \frac{\eta \beta \lambda_0^2}{8n}.
    %     \end{aligned}
    % \end{equation}

    Substituting the four inequalities \eqref{eq:right1}, \eqref{eq:d_p}, \eqref{eq:xi_p} and \eqref{eq:delta_p} into \eqref{eq:R2decp}, we obtain the lower bound as
    \begin{equation*}
        % \label{eq:total}
        \begin{aligned}
           \Ebb\norm{\parameter_{p+1}-\parameter_0}^2&\geq \kappa^{2p} \left(\beta^2 \mu\sigma_2^2 - 2\alpha\beta \frac{l_g B_{n,n_0}}{\kappa-1}- 2\alpha\beta\frac{l_g B_{n,n_0}}{\kappa-1} - 2\alpha\beta\frac{l_g\rho}{2} \left[ \frac{A_2}{(\kappa-1)^2}+\frac{A_3}{\kappa-1}\right] \right).
        \end{aligned}
    \end{equation*}
    According to our settings in Table \ref{table:1}, these conditions are satisfied:
    \begin{align}
        \Lthre\leq \frac{\mu\sigma_2^2\lambda_0\cdot\min\{\beta\lambda_0,1\}}{192l_g\rho},~ B_{n,n_0}^2\leq\frac{\mu\beta\sigma_2^2\lambda_0\cdot\min\{\beta T\lambda_0,1\}}{384\alpha T^2l_g\rho},~ \beta\leq \frac{\mu\sigma_2^2\lambda_0^2L}{384l_g\rho LV_{n,n_0}}.\label{eq:conditions_1}
    \end{align}
    It follows that 
    \begin{equation}
        \label{eq:A2A3}
        A_2\leq \frac{\mu \alpha \beta \sigma_2^2\lambda_0^2}{8l_g\rho},\quad A_3\leq \frac{\mu \beta \sigma_2^2\lambda_0}{8l_g\rho}.
    \end{equation}
    Therefore, we can conclude that
    \begin{equation}
        \label{eq:total-2}
        \begin{aligned}
           \Ebb\norm{\parameter_{p+1}-\parameter_0}^2&\geq 
           \kappa^{2p} \left(\beta^2 \mu\sigma_2^2-\frac{1}{4}\beta^2 \mu\sigma_2^2 -\frac{1}{4}\beta^2 \mu\sigma_2^2-\frac{1}{4}\beta^2 \mu\sigma_2^2\right)=\frac{1}{4}\beta^2 \kappa^{2p}\mu\sigma_2^2.
            % &~~ -8\alpha^2 \beta  l_g \rho \cdot \frac{\kappa^{2p} }{(\kappa-1)^2}\cdot (\alpha V_{n,n_0} +2\Lthre+(\beta+2T \alpha)B^2_{n,n_0}+2\beta^2LV_{n,n_0}),\\
            % &\overset{\eqref{eq:conditions}}{\geq} \frac{\eta\beta^2\kappa^{2p}\sigma_{2}^{2}}{n}-6\cdot \frac{\eta\beta^2\kappa^{2p}\sigma_{2}^{2}}{8n}=\frac{\eta\beta^2\kappa^{2p}\sigma_{2}^{2}}{4n}.
        \end{aligned}
    \end{equation}
    In other word, \eqref{eq:total-2} shows that the expected distance between $\parameter_0$ and $\parameter_{p+1}$ grows at least exponentially for $p$. Substituting $p=T-1$, it leads to a contradiction if the lower bound of $\Ebb\norm{\parameter_T-\parameter_0}^2$ is greater than the upper bound, i.e.,
    \begin{equation}
        \label{eq:contradiction}
         \frac{1}{4}\beta^2 \kappa^{2T}\mu\sigma_2^2\geq(T-1)A_2+A_3.
        % \frac{\eta\beta^2\kappa^{2T }\sigma_{2}^{2}}{4n}> 4 \alpha T\left(\alpha V_{n,n_0}+ 2\Lthre+(\beta+2T\alpha) B^2_{n,n_0}+2\beta^2LV_{n,n_0}\right)+2\beta^2 l_g^2.
    \end{equation}
    %\cfn{the constraints spell as $\beta\leq c_0 \frac{\underline{V}_{n,n_0}}{LV_{n,n_0}}\lambda^2, B_{n,n_0}<<1, \Lthre\leq \lambda^2\beta\underline{V}_{n,n_0}$, and the $n_0,n$ can simply be taken to be $\tbO{1}$.}
    We choose 
    \begin{equation}
        \label{eq:Tconst}
        T=\frac{c}{\alpha \lambda_0}\log \left(\frac{LV_{n,n_0}n}{\eta \alpha \beta \sigma_2} \right)
    \end{equation}
     in Table \ref{table:1} with taking a sufficiently large $c$ and then \eqref{eq:contradiction} implies a contradiction. Hence, the proof is completed.
\end{proof}


Finally, we establish the main theorem in this section. It is shown that VMC returns approximate second-order points or $\epsilon$-variance points in high probability. Since an $\epsilon$-variance point is desired, we should design better neural network architectures to reduce those meaningless second order stability points. 

\begin{theorem}
    \label{thm:efsp}
    Let Assumptions \ref{asm:wavefun} ,\ref{asm:local} ,\ref{asm:unigap} and \ref{asm:sec} hold. For any $\delta\in (0,1)$, with the stepsizes \eqref{eq:schedule} and parameters in Table \ref{table:1}, Algorithm \ref{alg:VMC} returns an $(\epsilon,\epsilon^{1/4})$ approximate second-order stationary point or an $\epsilon^{1/2}$-variance point with probability at least $1-\delta$ after the following steps

    \begin{equation}
        O\left(\delta^{-4}\epsilon^{-11/2}\log^2\left(\frac{1}{\epsilon\delta}\right)\right).
    \end{equation}
    
\end{theorem}
\begin{proof}
    Suppose $\Ecal_m$ is the event
    \begin{equation*}
        \Ecal_m:=\left\{\tparameter_m\in \Rcal_1\cup \Rcal_2\right\},
    \end{equation*}
    and its complement is $\Ecal_m^c=\left\{\tparameter_m\in \Rcal_3 
   \right\}$. Let $\Pcal_m$ denote the  probability of the occurrence of the event $\Ecal_m$. 
    
    When $\Ecal_m$ occurs, by Lemmas \ref{lem:R1} and \ref{lem:R2}, we have 
    \begin{equation}
        \label{eq:Edescent}
        \epct{\Lcal(\tparameter_{m})-\Lcal(\tparameter_{m+1})\Big|\Ecal_m}\geq \Lthre.
    \end{equation} On the other hand, when $\Ecal_m^c$ occurs, it follows from \eqref{eq:R1ineq} that
    \begin{equation}
        \label{eq:Ecdescent}
        2\epct{\Lcal(\tparameter_m)-\Lcal(\tparameter_{m+1})\Big|\Ecal_m^c}\geq -2T\alpha B_{n,n_0}^2-2\beta^2LV_{n,n_0}\geq -\delta \Lthre,
    \end{equation}
    where the first inequality is by discarding positive terms in \eqref{eq:R1ineq} and the second inequality is due to the choice $\Lthre\geq \left(2T\alpha B_{n,n_0}^2+2\beta^2LV_{n,n_0}\right)/\delta$ in Table \ref{table:1}. It means that the function value may increase by no more than $\delta \Lthre/2$. When the expectation is taken overall, \eqref{eq:Edescent} and \eqref{eq:Ecdescent} imply that
    \begin{equation}
        \label{eq:totaldescent}
        \epct{\Lcal(\tparameter_{m})-\Lcal(\tparameter_{m+1})} \geq (1-\Pcal_m)\cdot \left(-\frac{\delta \Lthre}{2}\right)+\Pcal_m\cdot \Lthre.
    \end{equation}
    
    Suppose Algorithm \ref{alg:VMC} runs for $K$ steps starting from $\parameter_0$ and there are $M=K/T $ of $\tparameter_{m}$. Let $\Lcal^*$ be the global minimum of $\Lcal(\parameter)$. Summing \eqref{eq:totaldescent} for $m=1,\dots, M$ yields that
    \begin{equation*}
        \Lcal(\parameter_0)-\Lcal^{*}\geq  -\frac{\delta \Lthre M}{2}+\sum_{m=1}^{M}\Pcal_m\cdot \Lthre\Rightarrow\frac{1}{M}\sum_{m=1}^{M}\Pcal_m \leq \frac{\Lcal(\parameter_0)-\Lcal^{*}}{M\Lthre}+\frac{\delta}{2}\leq \delta,
    \end{equation*}
    where the last inequality holds if $K$ satisfies 
    \begin{equation}
        \label{eq:Kconst}
        K\geq \frac{2[\Lcal(\parameter_0)-\Lcal^{*}]T}{\delta \Lthre}=O\left(\delta^{-4}\epsilon^{-11/2}\log^2\left(\frac{1}{\epsilon\delta}\right)\right).
    \end{equation} Hence, the probability of the event $\Ecal_m^c$ occurs can be bounded by 
    \begin{equation*}
        1-\frac{1}{M}\sum_{m=1}^{M}\Pcal_m\geq 1-\delta.
    \end{equation*}
    This proves the statement in Theorem \ref{thm:efsp}.
\end{proof}
\section{Conclusions}
\label{sec:conclusion}

We explore the theoretical convergence of the VMC algorithm for solving the ground state of many-body quantum systems.
The upper bound of the bias and variance corresponding to the MCMC  is estimated without assuming that the local energy in a quantum system is bounded.
Then, we show that VMC with the biased stochastic gradient achieves first order stationary convergence. It has $O\left(\frac{\log K}{\sqrt{n K}}\right)$ convergence rate with a sufficiently large sample size $n$ after $K$ iterations. Moreover, by verifying the correlated negative curvature condition, we discuss how VMC escapes from saddle points and establish the second order convergence guarantee of $O\left(\epsilon^{-11/2}\log^{2} \left(\frac{1}{\epsilon}\right)\right)$. Our result explains the observation that VMC usually converges to eigenvalues of the quantum system.

There are some potential directions to be concerned for future works. (1) Other variational methods, such as variational inference, variational Bayesian matrix factorization, and variational annealing,  can be studied in our analytical framework. (2) Our convergence analysis suggests that improving the efficiency of sampling methods is of vital importance for the better performance of stochastic algorithms. (3) The convergence of the natural gradient method and the KFAC method for VMC is also interesting.

\bibliographystyle{plain}
\bibliography{bib.bib}

\end{document}
