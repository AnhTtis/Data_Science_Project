\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{xcolor,colortbl}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{subfigure}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\newcommand{\R}[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
\newcommand{\B}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\definecolor{shadecolor}{rgb}{0.92,0.92,0.92}
\newcommand{\MC}[1]{\multicolumn{2}{c} {#1}}
\newcommand{\et}{\textit{et al}.}
\newcommand{\cmark}{\text{\ding{51}}}%
\newcommand{\xmark}{\text{\ding{55}}}%
\def\comp{\ensuremath\mathop{\scalebox{.6}{$\circ$}}}
\newcommand{\RT}[1]{\textcolor{blue}{\textbf{[RT:} \textit{#1}\textbf{]}}}
\makeatletter %
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz}
\newcommand{\widthscale}{0.10}
\newcommand{\widthscalefive}{0.105}
\newcommand{\name}{0}
\newcommand{\h}{0}
\newcommand{\w}{0.15}
\newcommand{\wa}{0.15}
\newlength \g
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[symbol]{footmisc}
\usepackage{pifont}%


% \crefname{section}{Sec.}{Secs.}
% \Crefname{section}{Section}{Sections}
% \Crefname{table}{Table}{Tables}
% \crefname{table}{Tab.}{Tabs.}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage[capitalize]{cleveref} % add

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{2817} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
% \title{MSFA-Frequency guided Half Swin-Transformer for Hyperspectral Image Demosaicing}
% \title{MSFA-Frequency Guided Half-Swin Transformer for Enhanced Hyperspectral Image Demosaicing in Challenging Cases}
% \title{Specializing in Demosaicing Hard Cases: MSFA-Frequency Guided Half-Swin Transformer for Hyperspectral Images}
\title{MSFA-Frequency-Aware Transformer for Hyperspectral Images Demosaicing}



\author{$\text{Haijin Zeng}^1$, $\text{Kai Feng}^2$, $\text{Shaoguang Huang}^3$, $\text{Jiezhang Cao}^4$, $\text{Yongyong Chen}^5$, \\ $\text{Hongyan Zhang}^6$, $\text{Hiep Luong}^1$, and 
	$\text{Wilfried Philips}^1$\\
$^1\text{IMEC-IPI-UGent}$, $^2\text{NWPU}$, $^3\text{CUG}$, $^4\text{ETH Zurich}$, $^5\text{HIT}$, $^6\text{WHU}$ \\

{\tt\small haijin.zeng@ugent.be}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
% The utilization of spectral information and correlated spatial patches in multispectral filter array (MSFA) sampled mosaic images is vital for hyperspectral demosaicing. 
% However, current Convolutional Neural Network (CNN)-based methods present limitations in capturing the periodic MSFA pattern and modeling non-local self-similarity. 
% In addition, we observe that while low-frequency structural information can be efficiently reconstructed by most demosaicing techniques, the main challenge lies in the recovery of high-frequency detail and texture information.
% However, previous methods have not sufficiently differentiated between these two components and instead use a single integrated model to reconstruct both high and low frequencies simultaneously.
% To address this, a novel framework, MSFA-frequency guided Half Swin Transformer (MFG-HST), is proposed in this paper. 
% MFG-HST features a high-frequency and low-frequency demosaicing framwork, MSFA-frequency-guided attention window based shifted multi-head self-attention, which leverages the global periodic information and non-local dependencies correspond to the spatial-spectral dimensions of hyperspectral image. 
% Additionally, a joint spatial and frequency loss function is presented to transfer information from MSFA and enhance the training on challenging frequencies. Experimental results demonstrate that MSAT outperforms state-of-the-art methods on both ARAD and Cave datasets and produces visually appealing results in real hyperspectral image demosaicing.
% The framework can also be applied to reconstruction tasks in other imaging systems with known sample pattern or mask, such as snapshot compressive sensing, Bayer, and Quad Bayer demosaicing.
% Hyperspectral imaging systems using multispectral filter arrays (MSFA) employ hyperspectral image (HSI) demosaicing methods to recover the spatial-spectral signal from mosaiced measurements. While deep learning methods have shown promise, they suffer from several issues, 
Hyperspectral imaging systems that use multispectral filter arrays (MSFA) capture only one spectral component in each pixel. 
Hyperspectral demosaicing is used to recover the non-measured components. 
While deep learning methods have shown promise in this area, they still suffer from several challenges,
including limited modeling of non-local dependencies, lack of consideration of the periodic MSFA pattern that could be linked to periodic artifacts, and difficulty in recovering high-frequency details. To address these challenges, this paper proposes a novel demosaicing framework, the MSFA-frequency-aware Transformer network (FDM-Net). FDM-Net integrates a novel MSFA-frequency-aware multi-head self-attention mechanism (MaFormer) and a filter-based Fourier zero-padding method to reconstruct high pass components with greater difficulty and low pass components with relative ease, separately. The advantage of Maformer is that it can leverage the MSFA information and non-local dependencies present in the data. Additionally, we introduce a joint spatial and frequency loss to transfer MSFA information and enhance training on frequency components that are hard to recover. Our experimental results demonstrate that FDM-Net outperforms state-of-the-art methods with 6dB PSNR, and reconstructs high-fidelity details successfully.
% Additionally, the framework can be applied to other imaging systems with known sample pattern or mask directly, such as snapshot compressive sensing, Bayer, and Quad Bayer demosaicing.



\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
Hyperspectral imaging (HI) captures light across a broad range of spectral bands, including those within the visible and beyond near-infrared spectrum. This provides much higher spectral resolution than the 3 spectra, leading to more accurate material characterization than is achievable through RGB imaging. This capability makes HI a valuable tool in numerous fields, including medical imaging, astronomy, food quality control, remote sensing, precision agriculture and pharmaceuticals. \cite{2021ICCV_HSIDenoising,zhang2019hyperspectral,chang2007hyperspectral,cao2016computational}.

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{figures/Demo_1st_page.pdf}
\caption{Overview of previous methods and our frequency-aware HSI demosaicing framework. In contrast to current demosaicing methods that do not differentiate between the facile low-pass components and arduous high-pass details, we propose a frequency-aware demosaicing framework, which employs a customized transformer to reconstruct the hard high-pass components and data-independent but stable traditional interpolation-filtering to recover low-pass parts expeditiously. The proposed approach yields a significant improvement in the reconstruction of details.}
\label{fig_demo_1stpage}
\end{figure}

% The extra information that HI systems (HISs) provide in comparison to RGB cameras may outweigh the downsides of their bulkier size, costlier price, and reduced resolution. However, the slow acquisition times in HISs that result from spatial or temporal scanning is a major hindrance for their widespread use in computer vision applications. Snapshot HISs, however, eliminate this challenge by quickly capturing both spectral and spatial information \cite{Spectral_Imaging_review,arad2022ntire}.
% Among new techniques for snapshot hyperspectral imaging, such as computed tomography \cite{dalton2021monte,koundinyan2018material} and light-field imaging \cite{light_field_2,light_field_1}, snapshot mosaic HISs or multi-spectral filter array (MSFA) cameras are gaining prominence. MSFA cameras are snapshot HISs that use an MSFA to swiftly acquire spectral information through a single exposure of a 2D image sensor \cite{lapray2014multispectral}. MSFA cameras sample the imaged scene in a manner similar to Bayer-filter-based RGB cameras. While RGB cameras employ a repeating 2 × 2 color filter array (CFA) or "mosaic," MSFA cameras typically use larger CFAs, such as 3 × 3, 4 × 4, or even 5 × 5 \cite{detailed_HIS}. Figure 1 displays a traditional RGB CFA and a 4 × 4 MSFA.

% The spectral-spatial information provided by HI systems is more comprehensive than that of RGB cameras. 
However, their employment in computer vision is limited due to slow acquisition times attributed to spatial or spectral scanning. To address this issue, snapshot HI systems \cite{Spectral_Imaging_review,arad2022ntire}, such as computed tomography \cite{dalton2021monte,koundinyan2018material} and light-field imaging \cite{light_field_2,light_field_1}, have been introduced recently, which capture both spectral and spatial information rapidly. These snapshot HI systems can be realized by snapshot mosaic HI systems or Multi-Spectral Filter Array (MSFA) cameras \cite{lapray2014multispectral}. The latter uses an MSFA to acquire spectral information in a single 2D image sensor exposure, similar to RGB cameras. However, MSFA cameras employ larger Color Filter Arrays (CFAs), such as 3 × 3, 4 × 4, or 5 × 5 \cite{detailed_HIS}.


% The availability of MSFA cameras, designed with tiny Fabry-Pérot interferometry filters on top of CMOS or InGaAs sensors to obtain wavelength selectivity via a multiple-beam interference process, has been increasing for researchers and professionals at more accessible prices. Examples of such cameras include the IMEC SNAPSHOT, XIMEA Snapshot USB3, and silios CMS series. However, to fully utilize the spatial and spectral information provided by MSFA cameras, it is essential to apply efficient spectral demosaicing methods to estimate a fully defined hyperspectral image (HSI). Demosaicing large MSFAs poses a challenge due to the larger mosaic pattern and weaker inter-channel correlation compared to Bayer filter cameras. While several demosaicing methods have been proposed, they demonstrate limited demosaicing capability for high frequency details, with periodic artifacts remained. This maybe due to current CNN-based methods do not to adequately account for long-range dependencies \cite{cai2022degradation} and MSFA periodic information.

% The availability of MSFA cameras, which are equipped with small Fabry-Pérot interferometry filters on top of CMOS or InGaAs sensors to obtain wavelength selectivity through a multiple-beam interference process, has increased in recent times, providing researchers and professionals with more accessible options at reasonable prices. 
The availability of MSFA cameras, designed with tiny Fabry-Pérot interferometry filters on top of CMOS or InGaAs sensors to obtain wavelength selectivity via a multiple-beam interference process, has been increasing for researchers and professionals at more accessible prices. 
Prominent examples of such cameras include the IMEC SNAPSHOT, XIMEA Snapshot USB3, and silios CMS series \cite{arad2022ntire}. However, to make optimal use of the spatial and spectral information provided by MSFA cameras, it is necessary to apply effective spectral demosaicing methods that can estimate a fully defined hyperspectral image (HSI).
Demosaicing large MSFAs presents a challenge due to the larger mosaic pattern and weaker inter-channel correlation in comparison to Bayer filter cameras. Although several demosaicing methods have been proposed, they exhibit limited demosaicing capability for high frequency details, resulting in the persistence of periodic artifacts. This may be due to the current CNN-based methods inadequately accounting for long-range dependencies \cite{cai2022degradation} and MSFA periodic information that is also critical for HSI demosaicing.


% including interpolation-based methods \cite{WB,BTES}, matrix-factorization/recovery-based methods \cite{GRMR,PPID}, and deep learning approaches \cite{InNet,DesNet,MCAN,rathi2021convolution,SpNet}. 
\emph{Motivation of Using Transformer:}
Specifically, during the MSFA imaging process, the entire spectral domain information is sampled and compressed into a single band, resulting in spatial-spectral confusion. Specifically, the nearest neighbors with similar spectral information are stored in a periodic MSFA pattern, causing them to be several pixels away in memory compared to RGB. This confusion occurs both across adjacent bands and throughout the entire spectral domain, and current CNN-based methods are unable to eliminate it. Non-local similarity has been identified as a critical factor in addressing spatial-spectral confusion  \cite{wang2022spatial,liu2018rank}. However, the receptive field of convolution limits its ability to leverage non-local information. In contrast, the Transformer architecture can exploit long-range non-local similarity and significantly improve reconstruction outcomes \cite{devlin2018bert,rao2021msa,transformer_pretrained,transformer_2,transformer_pyramid,UTransformer,transformer_swinir,liu2021swin}. Additionally, current methods struggle with detail recovery, while the Transformer has demonstrated exceptional capability in detecting subtle spatial differences \cite{dstransformer_HSI_res,he2022transfg,Restormer}.

% The Transformer architecture has recently emerged as a dominant model in natural language processing (NLP) and computer vision tasks, exhibiting superior performance as demonstrated by various studies \cite{devlin2018bert,rao2021msa,transformer_pretrained,transformer_2,transformer_pyramid,UTransformer,transformer_swinir,Restormer,dstransformer_HSI_res}. This architecture utilizes a self-attention mechanism to capture long-range dependencies \cite{liu2021swin}, and has the potential to alleviate periodic artifacts in CNN-based HSI demosaicing methods by incorporating non-local information. Additionally, Transformer has shown exceptional capability in detecting subtle spatial differences, which could be beneficial in reconstructing high-frequency details of HSI that are difficult to recover using conventional methods.

% Inspired by these observations and the fact that hard details always be high frequency, we introduce a novel demosaicing network, the Frequency-aware Demosaicing Network (\emph{FDM-Net}), with the aim of reconstructing high-frequency details and reducing artifacts in current CNN-based methods. Our approach reconstructs the high-pass and low-pass components of latent HSI separately. To achieve this, we first employ classical Fourier zero-padding based low-pass filter to quickly reconstruct the low-pass components. Next, we develop a MSFA-Frequency-aware transformer called \emph{MaFormer} that models non-local dependencies and MSFA information simultaneously to recover the hard high-pass details with fewer artifacts, as illustrated in Fig.~\ref{fig_demo_1stpage}. Finally, we integrate a joint spatial-frequency regularization term into the network, leveraging the MSFA pattern and frequency information to enhance the reconstruction of details while preserving the fidelity of the low-pass components.

Motivated by these observations and the inherently high-frequency nature of details in HSI, we propose an efficient HSI demosaicing network that employs a Transformer model and models MSFA information. Our method reconstructs the high-pass and low-pass components of HSI separately. Firstly, we utilize a Fourier zero-padding-based low-pass filter to quickly reconstruct the low-pass components that are easier to recover. Secondly, we introduce a novel MSFA-Frequency-aware Transformer, named \emph{MaFormer}, which focuses on the hard high-frequency details by concurrently modeling non-local dependencies and MSFA information. This enables us to recover the high-frequency details with reduced artifacts, as illustrated in Fig.~\ref{fig_demo_1stpage}. Finally, we integrate a joint spatial-frequency regularization term into the network, which utilizes both the MSFA pattern and frequency information to improve the reconstruction of details while preserving the fidelity of the low-pass components.
In summary, our contributions are three-fold:
\begin{enumerate}
    \item We propose a novel MSFA-frequency-aware HSI demosaicing framework that amalgamates the benefits of traditional methods with transformer to reconstruct HSI with precise details and fewer artifacts.
    \item By simultaneously incorporating non-local and MSFA periodic modeling, we present MaFormer, a tailored transformer designed specifically to demosaic the challenging high-pass HSI.
    % To the best of our knowledge, this is the first instance of exploring the potential of Transformer for this task.
    % \item We customize a novel MSFA-Conv based Multi-head Self-Attention mechanism, STMC, and implement it using parallel group convolution. This mechanism enables our MaFormer to perform non-local and MSFA periodic modeling simultaneously.
    \item Our FDM-Net outperforms state-of-the-art methods by a large margin, and produces highly accurate details.
\end{enumerate}
% Unlike previous approaches that employed Convolutional Neural Networks (CNNs), the MFST uses a customized MSFA-Conv guided Half Swin Transformer architecture, which has the advantage of capturing long-range spatial dependencies, periodic MSFA pattern, and non-local self-similarity.
% The MFST is constructed by integrating the customized transformer block and MSFA-Conv in an elegant UNet-style, where the down-sample layer is retained, and the up-sample layer is replaced with a transpose convolution. Additionally, residual connections are added to compensate for the information loss incurred during sampling.


% To overcome the limitations of traditional Transformer models, the method customizes MSFA and frequency Guided Multi-head Self-Attention (MFG-MSA) with shifted windows.  
% Firstly, 
% the MFG-MSA method splits each feature map into two sub-cubes, one of which is fed into the periodic branch of the MFG-MSA. The periodic branch adopts a different convolution approach, referred to as MSFA-Conv, that diverges from the standard convolution operation commonly used in previous demosaicing methods.
% Instead of using a standard kernel to extract features from neighboring elements, the MSFA-Conv employs an MSFA-based convolution operation that assigns the same weights to elements with the same relative positions within the MSFA pattern, preserving the prior information of the original MSFA pattern.
% As illustrated in Fig. \ref{fig_sketch_2} (e), the MSFA-Conv computes the relative position of the input based on the MSFA pattern and then uses a weight prediction block, consisting of two 1×1 convolutions and a ReLu activation function, to learn the convolution weight of each element. The weights are then used to perform the 8×8 convolution on the input feature map.
% This convolution process provides self-similar spectral distribution within each wavelength and global spectral continuity information, which is crucial for maintaining spatial-spectral consistency.


% Furthermore, to effectively represent non-local dependencies, the other split feature sub-cube is processed by the Swin Transformer block. The shifted window partitioning approach \cite{liu2021swin}, alternating between regular window partitioning and shifted window partitioning, is employed to introduce cross-window connections while preserving the efficiency of non-overlapping windows and forming local attention windows. Additionally, a fully-connected layer is utilized to effectively merge information between the two sub-modules of the MFG-MSA.



% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol


\section{Related Work}

\subsection{HSI Demosaicing}
% \textbf{Interpolation-based methodologies:}

% \textbf{Matrix-factorization/recovery-based methods:}

% \textbf{Deep learning approaches:}
 
% The topic of multispectral image demosaicking has been explored in a number of research studies \cite{WB,BTES,PPID,GRMR,Demosaic_matrix_completion,SpNet,DesNet,InNet,MCAN}. However, despite these efforts, research in this area remains limited, likely due to the inherent complexity of the problem and the underutilization of multispectral imaging systems across various applications. 
% Current methods for hyperspectral image (HSI) demosaicking can be broadly classified into three categories: interpolation-based methodologies, matrix-factorization/recovery-based methods, and deep learning approaches. Interpolation and matrix-based methods \cite{WB,BTES,GRMR,PPID,Demosaic_matrix_completion} mainly rely on spectral-spatial priors to reconstruct the missing spectral and spatial information. This paper will focus on the latest learning-based approaches.

Research on multispectral image demosaicing has been conducted in various studies \cite{WB,BTES,PPID,GRMR,Demosaic_matrix_completion,SpNet,DesNet,InNet,MCAN}. 
% However, due to the inherent complexity of the problem and underutilization of multispectral imaging systems, research in this area is limited. 
Current methods for demosaicing can be categorized into interpolation-based, matrix-factorization/recovery-based, and deep learning approaches. Interpolation and matrix-based methods \cite{WB,BTES,GRMR,PPID,Demosaic_matrix_completion} rely on spectral-spatial priors to reconstruct missing spectral and spatial information. This paper will focus on the latest learning-based approaches for HSI demosaicing.


CNNs has gained widespread popularity in various low-level image processing tasks, including image deblurring \cite{kupyn2019deblurgan, Restormer, cho2021rethinking}, denoising \cite{chen2021hinet, zeng_DPLRTA}, and super-resolution \cite{cao2022towards, wei2021unsupervised}. Although CNNs have been effectively employed in demosaicing \cite{ehret2019joint, liu2020joint, xing2021end}, their application has been predominantly limited to the Bayer pattern, which has a predominant green band. In contrast, spectral demosaicing necessitates the representation of multispectral correlations to enable CNN utilization. Consequently, researchers have introduced four distinct methods, namely DsNet \cite{DesNet}, SpNet \cite{SpNet}, In-Net \cite{InNet}, and MCAN \cite{MCAN}.
In particular, InNet \cite{InNet} applies a deep network employing a bilinear interpolated MSI cube as input, MCAN \cite{MCAN} proposes an end-to-end network that models joint spatial-spectral correlations in mosaic images. 
% These methods signify an advancement in CNN-based approaches for demosaicing. 
However, their capability to restore high frequency details is still restricted. Additionally, these methods do not account for long-range dependencies.

% The application of convolutional neural networks (CNNs) has become increasingly popular in various low-level image processing tasks, including image deblurring \cite{kupyn2019deblurgan,Restormer,cho2021rethinking}, denoising \cite{chen2021hinet,zeng_DPLRTA}, and super-resolution \cite{cao2022towards,wei2021unsupervised}. While CNNs have been successfully applied to demosaicing \cite{ehret2019joint,liu2020joint,xing2021end}, they have primarily been used with the Bayer pattern, which has a dominant green band. In contrast, spectral demosaicing require representation of long-range spectral correlation to enable the use of CNNs. To this end, researchers have proposed four different methods: DsNet\cite{DesNet}, SpNet \cite{SpNet}, In-Net \cite{InNet} and MCAN \cite{MCAN}. 
% % DsNet \cite{DesNet} applies strided convolutions to softly rearrange the spectral mosaic pattern and reduce computation. SpNet \cite{SpNet}, on the other hand, uses the split sparse band image as the input, which preserves the full spatial information of the raw image. 
% Specifically, InNet \cite{InNet} applies a deep network using a bilinear interpolated MSI cube as input. 
% MCAN \cite{MCAN} proposes an end-to-end network that models joint spatial-spectral correlations in mosaic images. 
% These methods presented herein denote remarkable progress in the advancement of CNN-based approaches for demosaicing. However, their ability to reconstruct high frequency details is still constrained. Moreover, the methods do not take into consideration non-local dependencies.


\subsection{Vision Transformer}
As an dominated architecture in NLP, the Transformer \cite{vaswani2017attention} is designed for sequence modeling, by incorporating a self-attention mechanism. 
It also has demonstrated remarkable performance in various vision-related tasks \cite{transformer_1,transformer_2,liu2021swin,transformer_swinir}. 
However, the use of transformer in image restoration \cite{transformer_pretrained,transformer_video} often involves dividing the input image into small, fixed-sized patches and processing each patch independently, which leads to two main issues \cite{liu2021swin,cao2023swin_unet}. Firstly, the restored image may exhibit border artifacts around the edges of each patch, and secondly, border pixels in each patch lose information that could have otherwise contributed to better restoration. 
% Although patch overlapping can alleviate these issues, it results in added computational complexity.
Recently, the Swin Transformer \cite{liu2021swin} has emerged as a promising solution, incorporating the benefits of both CNNs and Transformers: on the one hand, it inherits the advantage of CNNs in processing large images due to its local attention mechanism; on the other hand, it retains the capability of Transformers in modeling long-range dependencies using the shifted window scheme \cite{transformer_swinir,cao2023swin_unet,liu2021swin}.


% \subsection{Attention-based Multispectral Image Restoration}

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{figures/demosaic_High_Low_fre_11.pdf}
% \includegraphics[width=3.2in]{figures/demosaic_High_Low_fre_1.pdf}
\vspace{-2mm}
\caption{Overview of the imaging process of MSFA camera and our frequency-aware demosaicing framework: FDM-Net.}
\label{fig_sketch_1}
\end{figure}

\section{Method}
% We first brief overview of our FDM-Net (\ref{model:overview}), and its keystone block: MaFormer (\ref{maformer}).
% Subsequently, we present MaFormer's sub-blocks MSFA-Conv and MSFA-guided Half Swin Transformer (\ref{MSFA_SW}). 
% Then, we introduce the proposed joint frequency and spatial loss in \ref{joint_loss}.


% \subsection{Mathematical Model of HSI Demosaicing} \label{math:DM_HSI}

% MSFA cameras effectively address the challenges of long acquisition times and high cost present in traditional Hyperspectral Imaging Systems. However, the use of larger Color Filter Arrays (CFAs), i.e., MSFA, results in a decreased sample rate, as demonstrated in Figure \ref{fig_sketch_1}. This presents a significant challenge in the demosaicing of hyperspectral images.

% ---------------------------------------------
% Mathematically, reconstruct a hyperspectral image $\hat{\mathbf{x}}$ by referencing the observed mosaic image $\mathbf{y}$, as demonstrated in Fig \ref{fig_sketch_1} (a), can be modeled as an ill-posed inverse problem. 
% It can be solved by maximizing a posterior (MAP) as follows:
% $$
% \hat{\mathbf{x}}=\arg \min _{\mathbf{x}} F(\mathbf{x}, \mathbf{y})+\lambda R(\mathbf{x}),
% $$
% where $F(\mathbf{x}, \mathbf{y})$ denotes image fidelity term, $R(\mathbf{x})$ is the regularization used to represent the prior of HSI, and $\lambda$ is a constant trade-off the two terms.
% % To solve it, the prior knowledge of desired signals is employed as a regularizer, and a typical framework \cite{zengCVIU} can be formulated as:
% % \begin{equation}
% %   \arg\min_{\mathbf{x}} \left\|\boldsymbol{y}-\mathbf{\Phi} \boldsymbol{x} \right\|_{2}^2 + \beta R(\boldsymbol{x}),  
% % \end{equation}
% % where $F(\cdot)$ is the fidelity term, 
% % and $\mathbf{\Phi}$ represents the MSFA imaging operator. 
% % $R(\cdot)$ encodes prior information and $\beta$ is used to trade-off the two terms.
% The performance a demosaicing algorithm is contingent upon the adequate representation of $R(\boldsymbol{x})$ and the intricacies of the imaging process. To tackle this challenge, both supervised and unsupervised approaches have been proposed. In this paper, our focus is on the supervised deep model, which has been demonstrated to possess a remarkable ability to capture complex image priors \cite{scunet}. More specifically, the deep neural network is considered a succinct unrolled inference algorithm:
% $$
% \left\{\begin{array}{l}
% \min _W \sum_i L\left(\hat{\mathbf{x}}_i\left(\mathbf{y}_i, W\right), \mathbf{x}_i\right) \\
% \text { s.t. } \quad \hat{\mathbf{x}}_i=\arg \min _{\mathbf{x}} F\left(\mathbf{x}, \mathbf{y}_i\right)+\lambda R(\mathbf{x}),
% \end{array}\right.
% $$
% where $W$ is the weighs inside the network, $L(\cdot)$ denotes the loss function.
%------------------------------------------------

% Hyperspectral imaging systems (HIS) capture light distribution across many narrow spectral bands, providing more information than RGB cameras. 
% However, traditional HISs have long acquisition times and high cost. 
% Snapshot HISs, including MSFA cameras, overcome this by rapidly acquiring spectral and spatial information. 
% As shwon in Fig. \ref{fig_sketch_1}, MSFA cameras use a large mosaic to sub-sample the scene, which is done in a single exposure of a 2D image sensor.
% However, it requires efficient and accurate demosaicing methods to reconstruct 3-D HSI cube from the observed 2-D mosaic image, due to the utilization of much larger CFAs. 
% Previous methods, such as interpolation-based, matrix-factorization/recovery-based, and deep learning-based, have limitations, such as small training and testing data sets.


\subsection{Frequency-Aware Demosaicing Network} \label{model:overview}

The proposed MSFA-frequency-aware demosaicing network (FDM-Net) for HSI is depicted in Fig.~\ref{fig_sketch_1} (b).
The method was inspired by the recognition that while low pass structural information can be efficiently reconstructed by most demosaicing techniques, the main challenge lies in the recovery of high pass detail and texture information.
However, previous methods have not sufficiently differentiated between these two components and instead use a single integrated model to reconstruct both high and low pass components simultaneously.
To address this issue, we first decompose the HSI cube into its high pass and low pass components. Then, we customise a MSFA-Conv based Swin Transformer network (MaFormer) by performing non-local and MSFA periodic modeling simultaneously, and a sinc-interpolation block to reconstruct the high-low pass components.
Finally, we merge the reconstructed high and low pass components to obtain the final demosaiced HSI.

Specifically, given a mosaiced image $\mathcal{Y} \in \mathbb{R}^{M \times N}$, where $M$, $N$ are the image height and width, respectively.
Firstly, the low pass components $\hat{\mathcal{X}}_{LF}$ are reconstructed using Fourier
zero-padding (Lanczos windowed sinc \cite{duchon1979lanczos}) with guided-filter, which is a low-pass filter very accurate on smooth data, 
\begin{equation}
    \hat{\mathcal{X}}_{LF} = \operatorname{Filter}(\operatorname{Sinc}(\mathcal{Y}), \operatorname{Sinc}(\mathcal{Y})(:,:,0))\in \mathbb{R}^{M \times N \times C},
\end{equation}
where $u(x,y) = \sum_{m,n}v_{m,n}\emph{sinc}(x-m),\emph{sinc}(y-n)$ is the Sinc interpolation of $v(x,y)$, and $\emph{sinc}(t):=\omega_t\emph{sin}(\pi t)/(\pi t)$ for $t \neq 0$, $\emph{sinc}(0):=1$, $\omega_t=\frac{n}{\pi t}\emph{sin}(\pi t/n)$, if $\|t\|<n$.
Subsequently, we propose a customized transformer for reconstructing the high pass details $\hat{\mathcal{X}}_{HF}$. 
% \begin{equation}
%     \hat{\mathcal{X}}_{HF} = \operatorname{MaFormer}(\mathcal{Y}) \in \mathbb{R}^{M \times N \times C},
% \end{equation}
% where $C$ is the total bands of latent HSI. 
% The final demosaiced HSI is generated as $\hat{\mathcal{X}} = \hat{\mathcal{X}}_{HF} + \hat{\mathcal{X}}_{LF}$.

Our primary objective is to develop an effective and efficient module for the recovery of high pass details and textures, which presents a significant challenge. To address this, we select the transformer network, as it has demonstrated outstanding performance in distinguishing even subtle spatial differences by characterizing sequential spatial data \cite{he2022transfg,da2022transformer}. Fig.~\ref{fig_HL_demo} shows that the reconstructed low pass component contains clear smoothed structural information, while the high pass component learned by our MaFormer effectively captures the details and textures of the HSI. The demosaiced HSI, which is a result of the aggregation of the high pass and low pass components produced by our FDM-Net, is shown to be highly similar to the ground truth.
% The framework of proposed Multi-Scale Feature Aggregation (MSFA) frequency guided Swin Transformer for hyperspectral image demosaicing is presented in Fig. \ref{fig_sketch_1}. 
% Driven by the observation that low frequency structure information can be easily recovered by most demosaicing methods, whereas the challenge lies in reconstructing high frequency detail and texture information. The HSI cube is decomposed into these two components and reconstructed using a MSFA-frequency guided Swin Transformer network (STMC-Net) and a Sinc block, respectively. The final HSI cube is obtained by aggregating the reconstructed high and low frequency parts.


\begin{figure}[!t]
\scriptsize
\centering
\rotatebox{90}{\vspace{9mm}  \centering \small Image}
% \includegraphics[width=1.6in]{figures/h_L_demo_110.png}
% \includegraphics[width=1.6in]{figures/h_L_demo_112.png}
% \includegraphics[width=1.6in]{figures/frequency_demo.png}
\includegraphics[width=0.74in]{figures/HL_pass/arad_917_OurRGB.png}
\hspace{-0.5mm}
\includegraphics[width=0.735in]{figures/HL_pass/GT.png}
\hspace{-0.5mm}
\includegraphics[width=0.74in]{figures/HL_pass/ARAD_1K_0917_16_sinc_guide.png}
\hspace{-0.5mm}
\includegraphics[width=0.74in]{figures/HL_pass/H_RGB.png}\\
\vspace{-1.5mm}
\rotatebox{90}{\vspace{9mm}  \centering \small Spectrum}
\subfigure[FDM-Net]{\includegraphics[width=0.76in]{figures/HL_pass/Ourpass.jpg}}
\hspace{-1mm}
\subfigure[GT]{\includegraphics[width=0.76in]{figures/HL_pass/GTpass.jpg}}
\hspace{-1mm}
\subfigure[Low-Pass]{\includegraphics[width=0.765in]{figures/HL_pass/Lpass.jpg}}
\hspace{-1mm}
\subfigure[High-Pass]{\includegraphics[width=0.765in]{figures/HL_pass/Hpass.jpg}}
\vspace{-1.5mm}
\caption{Illustration of the low pass part and high pass component reconstructed by our FDM-Net. The output of our FDM-Net is generated by adding the low pass and high pass it reconstructed.}
\label{fig_HL_demo}
\end{figure}


\begin{figure*}[!t]
\centering
\includegraphics[width=6.5in]{figures/MCM_HSI_demosaicing.pdf}
\vspace{-3mm}
\caption{The architecture of our MaFormer. (a) MaFormer consists of an encoder, a bottleneck, and a decoder. MaFormer is built up by STMCs. (b) STMC
is composed of a parallel group convolution, which includes a periodic branch and a non-local branch. (c) The swin transformer used in non-local branch. (d) The weights prediction block of MSFA-Conv. (e) The MSFA-driven convolution: MSFA-Conv.}
\label{fig_sketch_2}
\end{figure*}


\subsection{Customized High Frequency Transformer} \label{maformer}
The proposed \emph{MaFormer} is the cornerstone of our FDM-Net, which adopts an overall architecture resembling a U-Net, as illustrated in Fig. \ref{fig_sketch_2} (a). Comprising an encoder, a bottleneck, and a decoder, \emph{MaFormer} employs downsampling and upsampling techniques through transpose convolutions. This architectural choice, which differs from stacking modules layer by layer without scaling, has been shown to enhance the performance of the algorithm and increase the receptive field of the basic CNN and the proposed MSFA convolution, as detailed in Sec. \ref{MSFA_SW}. However, downsampling inevitably leads to a loss of information, which we address by incorporating residual connections between the encoder and decoder stages.

% More specifically, as shown in Fig. \ref{fig_sketch_2} (a), the first module of MaFormer is MSFA-Conv, its input is the raw mosaic data $\mathcal{Y} \in \mathbb{R}^{M \times N}$ that sampled from latent HSI $\mathcal{X} \in \mathbb{R}^{M \times N \times C}$, where the $M, N$ are the height and width of observed raw data, respectively. 
% \emph{Firstly}, MSFA-Conv projects $\mathcal{Y}$ into $\mathcal{X}_0 \in \mathbb{R}^{M \times N \times 2C}$, where $2C$ denotes the number of channel.
% \emph{Secondly}, $\mathcal{X}_0$ is fed into 3 paired STMC and Downsample blocks, and results in hierarchical feature maps. Downsample layer is implemented using a $2 \times 2$ convolution without bias, it generates a downsampled feature map with double channels while half spatial resolution. Here, we denote the three outputs of these three paired STMC and Downsample groups as $\mathcal{X}_i, i=1, 2, 3$,  respectively.
% \emph{Thirdly}, $\mathcal{X}_3$ is processed by the bottleneck: a pure STMC without any sample. 
% \emph{Subsequently}, a symmetric decoder is designed as a classical U-Net. It also consists of three STMC blocks and a MSFA Conv, but the downsample layers are replaced with Transpose convolution, which is used to upsample the spatial dimensions of intermediate feature maps. \emph{Finally}, the $\hat{\mathcal{X}}_{HF} \in \mathbb{R}^{M \times N \times C}$, i.e., the learned high frequency information, e.g., details, textures of latent hyperpsectral image is reconstructed by a MSFA-Conv block. 

Specifically, the first module of MaFormer is the MSFA-Conv, as illustrated in Fig. \ref{fig_sketch_2} (a). Its input is the raw mosaic data $\mathcal{Y} \in \mathbb{R}^{M \times N}$ that is sampled from the latent HSI $\mathcal{X} \in \mathbb{R}^{M \times N \times C}$. Here, $M$ and $N$ denote the height and width of the observed raw data, respectively, and $C$ denotes the number of channels.
Firstly, the MSFA-Conv extracts feature maps $\mathcal{X}_0 \in \mathbb{R}^{M \times N \times 2C}$ from $\mathcal{Y}$.
Secondly, the $\mathcal{X}_0$ is fed into three paired STMC and Downsample blocks, resulting in hierarchical feature maps. The Downsample layer is implemented using a $2 \times 2$ convolution without bias, which generates a downsampled feature map with double channels while half spatial resolution. We denote the outputs of these three paired STMC and Downsample groups as $\mathcal{X}_i, i=1, 2, 3$, respectively.
Thirdly, the bottleneck processes $\mathcal{X}_3$ using a pure STMC without any sampling.
Subsequently, a symmetric decoder is designed as a classical U-Net. It also consists of three STMC blocks and an MSFA Conv, but the downsample layers are replaced with transpose convolutions, which are used to upsample the spatial dimensions of intermediate feature maps.
Finally, the high-frequency information, such as details and textures of the latent hyperspectral image, i.e., $\hat{\mathcal{X}}_{HF} \in \mathbb{R}^{M \times N \times C}$, is learned and reconstructed by an MSFA-Conv block.

% \subsection{MSFA Convolution Block}
% Inspired by promising performance of the attention strategy in image restoration, we 

\subsection{MSFA-based Half Swin Transformer} \label{MSFA_SW}
% \subsubsection{MSFA-guided Multi-head Self-Attention}
% \noindent \emph{MSFA-guided Multi-head Self-Attention:} 
% The key block of the proposed MaFormer is the proposed STMC module. Fig. \ref{fig_sketch_2} (b) shows the STMC used to process $\mathcal{X}_0 \in \mathbb{R} ^{M \times N \times 2C}$. Firstly, $\mathcal{X}_0$ is linearly projected via a $1 \times 1$ convolution layer, then it is split half into two sub-feature maps along the channel orientation, 
The crucial component of the proposed \emph{MaFormer} is the proposed integrated Swin-Tansformer \& MSFA-Conv (STMC) module. Fig.~\ref{fig_sketch_2} (b) illustrates the STMC module utilized to process the input tensor $\mathcal{X}_0 \in \mathbb{R} ^{M \times N \times 2C}$. Firstly, $\mathcal{X}_0$ is linearly projected via a $1 \times 1$ convolution layer, after which it is split into two sub-feature maps along the channel orientation,
\begin{equation}
    \mathcal{X}_0 = [ \mathcal{X}_0^{p}\in \mathbb{R}^{M \times N \times C}, \mathcal{X}_0^{nl} \in \mathbb{R}^{M \times N \times C} ].
\end{equation}
Then, $\mathcal{X}_0^{p}$ passes through the \emph{Periodic Branch} to model periodic MSFA information, while $\mathcal{X}_0^{nl}$ passes through the \emph{Non-local Branch} to model non-local dependencies. 


\subsubsection{Periodic Branch}
% \noindent \emph{Periodic Branch:} 
% \emph{Periodic Branch} passes through \emph{MSFA-Conv}, which computes the relative positions of the elements of input firstly, according to MSFA pattern, e.g.,
% $\left(m, n\right)=(i \bmod p, j \bmod p)$ is the relative position of element $\mathcal{X}_0^{p}(i, j)$ with $p \times p$ MSFA pattern. 
% Then, relative position matrix $R$ with element $(m, n)$, is fed into a MSFA attention weights prediction block (MAWP), which consists of two $1 \times 1$ convolution layers and one $\operatorname{ReLu}$ as shown in the Fig. \ref{fig_sketch_2} (d), to generate a MSFA-driven convolution kernel with weight $W$, i.e., 
% \begin{equation}
%    W= \operatorname{Conv}1\times1(\operatorname{ReLU}(\operatorname{Conv}1\times1(R))), 
% \end{equation}
% and then we can get $W$, in which the weights are changed periodically according to the period of MSFA.
% With the kernel $W$, one can assign the same weights to elements with the same relative positions, i.e., 
% $$F_0^{p} = \operatorname{Conv}8\times8(\mathcal{X}, W) \in \mathbb{R}^{M \times N \times 2C}$$
% In this way, our MSFA-Conv can effectively and efficiently ensure that neighboring elements sampled with the same wavelength share similar spectral distributions.
The \emph{Periodic Branch} employs the \emph{MSFA-Conv} block to apply a MSFA-driven convolution operator, as shown in Fig. \ref{fig_sketch_2} (e). This operator refines the features based on the relative positions of the elements in input, which are determined by MSFA pattern. Specifically, for an element with index $(i, j)$, $p \times p$ MSFA pattern, its relative position is denoted as $\left(m, n\right)=(i \bmod p, j \bmod p)$. The relative position matrix $R$, with element $(m, n)$, is then fed into a MSFA attention weights prediction block (MWP), which consists of two $1 \times 1$ convolution layers and one $\operatorname{ReLU}$ activation function, as shown in Fig. \ref{fig_sketch_2} (d). The MWP generates a MSFA-driven convolution kernel with weight $W$ as follows:
\begin{equation}
W= \operatorname{Conv}1\times1(\operatorname{ReLU}(\operatorname{Conv}1\times1(R))).
\end{equation}
The kernel $W$ assigns the same weights to elements with the same relative positions, allowing neighboring elements sampled with the same wavelength to share similar spectral distributions. The resulting feature map is
\begin{equation}
    F_0^{p} = \operatorname{Conv}8\times8(\mathcal{X}, W) \in \mathbb{R}^{M \times N \times 2C},
\end{equation}
is obtained by convolving the input $\mathcal{X}$ with the kernel $W$ using an $8\times8$ convolution operation. This MSFA-Conv block is designed to effectively and efficiently model periodic information in the input.
Then, feature $F_0^{p}$ is aggregated by MSFA pooling, instead of using normal pooling, e.g., maximum or average pooling. 
Specifically, MSFA pooling aggregates the feature points of $F_0^{mc}$ with the same relative position to get $F_1^{p} \in \mathbb{R}^{\frac{M}{2} \times \frac{N}{2} \times 2C}$, where 
\begin{equation}
    F_1^{p}\left(i, j, k\right)=c \sum_{s=0}^{\frac{m}{4}-1} \sum_{t=0}^{\frac{n}{4}-1} F_0^{p}\left(i+4s, j+4t, k \right),
\end{equation}
$c=\frac{1}{m / 4 \times n / 4}$. After that, two $3 \times 3$ Residual Convolutional (RConv) are used to refine the feature map $F_1^{p}$,
\begin{equation}
   F_2^{p} = \operatorname{RConv}(\operatorname{RConv}(F_1^{p})).
   \label{eq:periodic_branch}
\end{equation}

\subsubsection{Non-local Branch}
% \noindent \emph{Non-local Branch:} 
The \emph{non-local branch} computes MSA \cite{vaswani2017attention} within position-specific shifted windows  and cross-window connection, by using swin transformer \cite{liu2021swin}.
Given an input $\mathcal{X}_0^{nl} \in \mathbb{R}^{M \times N \times C}$ split from $\mathcal{X}_0$, the \emph{non-local} branch partitions it into $K \times K$ local windows without overlapping, it reshapes it into $\mathcal{X}_0^{nl} \in \mathbb{R}^{\frac{MN}{K^2} \times K^2  \times C}$, where $K$ denotes the window size. 
To take into account the cross window connection, regular and shifted window partitioning are used alternately here \cite{liu2021swin}, as shown in Fig. \ref{fig_sketch_2} (b).
Then, the self-attention of each local windows $X_{nl} \in \mathbb{R}^{K^2 \times C}$ is computed, i.e., 
\begin{equation}
    Q_{nl} = X_{nl}W_Q, K_{nl} = X_{nl}W_K, V_{nl} = X_{nl}W_V, 
\end{equation}
where $Q\in \mathbb{R}^{K^2 \times d}, K\in \mathbb{R}^{K^2 \times d}, V\in \mathbb{R}^{K^2 \times d}$ are the \emph{query, key} and \emph{value}. 
$W_Q, W_K, W_V$ are projection matrices, which are shared across different partitioned local windows.
Then, the local self-attention matrix of local windows is computed as follows:
\begin{equation}
\text{Attention}(Q_{nl}, K_{nl}, V_{nl}) = \operatorname{SoftMax}(\frac{Q_{nl}K_{nl}^T}{\sqrt{d}}+B)V_{nl},  
\label{eq:nonlocal_branch}
\end{equation}
where $d$ is the \emph{query / key} dimension, $B \in \mathbb{R}^{K^2 \times K^2}$ is the learnable relative parameters depicts positional encoding.
Subsequently, the attention feature maps are fed into a LayerNorm(LN) layer, and then pass through two fully connected layers with GELU, i.e., MLP. 
In addition, the residual connection is also added to each input of LN, as shown in Fig. \ref{fig_sketch_2} (c), and the output is reshaped back to size $M \times N \times C$.
Finally, the outputs of \emph{periodic branch} in Eq. (\ref{eq:periodic_branch}) and \emph{non-local} branch Eq. (\ref{eq:nonlocal_branch}) are concatenated and then a fully connected layer is used to fuse the information between \emph{periodic branch}  and \emph{non-local branch}, generate the final output $X_1 \in \mathbb{R}^{M \times N \times 2C}$ of STMC. 
Following STMC, the $\operatorname{Downsample}$ operator samples $X_1$ with half spatial resolution and double channels.

Overall, our MaFormer combines the non-local modeling ability of Swin Transformer block and 
MSFA periodic modeling ability of MSFA-Conv. 
Furthermore, we enhance the integrated periodic and non-local modeling ability by stacking our STMC in a down-sample \& up-sample U-Net style, together with transpose convolution. 
In addition to take into account extra MSFA information, this is also computationally cheaper than global standard MSA, due to the split and concatenation operations within STMC can act as the group convolution with two groups.

\subsection{Joint Spatial and Frequency Loss} \label{joint_loss}

The demosaicing of HSI is an ill-posed inverse problem, where a single observed mosaic image can correspond to multiple HSIs. To mitigate this difficulty, as regularization-based optimization methods \cite{l12SSTV, zeng_DPLRTA}, we propose the incorporation of a joint spatial and frequency loss as a constraint in the optimization procedure, to decrease the potential solution space. 
\emph{Firstly}, consider that in low frequency reconstruction phase, our method leverages the use of classical interpolation to reconstruct the low frequency components quickly, and a guided filter to remove noise and refine the reconstructed low frequency parts.
To preserve the accuracy of the known high-frequency information, we introduce an MSFA-based $L_1$ loss to regularize the sampled pixels,
% This is performed firstly to ensure that the known high frequency information is kept intact,
\begin{equation}
    L_1^s = \| x - \hat{x}\odot M\|_1.
\end{equation}
where $M$ is the MSFA sample mask, $x, \hat{x}$ are the ground truth and demosaiced HSI, respectively. 
$\odot$ denotes element-wise multiplication. 


\emph{Secondly,} based on our observations, the lower frequency component within the high frequency part of the HSI is relatively easier to reconstruct, while the main challenge lies in the reconstruction of the higher frequency component, which contains complex details and textures.
To enhance the network's ability to model these challenging cases, we introduce the Focal Frequency Loss (FFL) \cite{focal_loss} instead of using frequency loss directly. The FFL focuses the network on the most challenging frequencies during its training,
\begin{equation}
    L_\mathrm{FFL}=\frac{1}{M N} \sum_{u=0}^{M-1} \sum_{v=0}^{N-1} w(u, v)\left|F_{\hat{x}}(u, v)-F_x(u, v)\right|^2 .
\end{equation}
where $M \times N$ is the image size, $(u, v)$ denotes the coordinate of a spatial
frequency on the frequency spectrum, $w(u, v)$ is the matrix element, i.e.,
the weight for the spatial frequency at $(u, v)$, it is defined as:
$$
w(u, v)=\left|F_{\hat{x}}(u, v)-F_x(u, v)\right|^\alpha,
$$
where $\alpha$ is the scaling factor for flexibility ($\alpha=1$), 
\begin{equation}
    F(u, v) = \frac{1}{M N} \sum_{u=0}^{M-1} \sum_{v=0}^{N-1} f(x, y) \cdot e^{-i2\pi(\frac{ux}{M}+\frac{vy}{N})}.
\end{equation}
% As detailed in \cite{focal_loss}, the matrix values are normalized into the range [0,1] with a weight of 1 assigned to the most challenging frequency and lower weights to easier frequencies. 
The gradient through the spectrum weight matrix only serves as a weight for each frequency.
The Focal Frequency Loss (FFL) can be understood as a weighted average of the frequency differences between the reference and demosaiced images.
By using the FFL, the loss function is re-weighted to give priority to the reconstruction of the most complex details of textures with challenging frequencies, and to down-weight easier cases. In addition, the focus region is dynamically updated, which improves the immediate challenging frequencies and results in a gradual refinement of the generated images.

\emph{Subsequently,} $L_1$ loss is also added to the high frequency part, to ensure complete and global texture information in image domain can be learned by our network, i.e., 
\begin{equation}
    L_1^c = \| x - \hat{x}_l- \hat{x}_h\|_1
\end{equation}
where $\hat{x}_l$ and $\hat{x}_h$ are the predicted high frequency cube and low frequency part. 
\emph{Finally,} our joint frequency and spatial loss function for hyperspectral image demosaicing is formulated as follows:
\begin{equation}
    L = \alpha_1 L_1^s + \alpha_2 L_\mathrm{FFL} + \alpha_3 L_1^c 
\end{equation}
where $\alpha_1 = 0.1, \alpha_2 = 1,$ and $\alpha_3 = 1$.


\definecolor{Gray}{gray}{0.90}
\begin{table*}[!htbp]
% \scriptsize
\centering
\setlength{\tabcolsep}{1.2pt}
\renewcommand{\arraystretch}{1.2}%
% \setlength\tabcolsep{1.1}
\caption{Demosaicing results and running time compared with other methods. FDM-Net achieves SOTA results.}
\label{Table:PQI_1}
% \begin{adjustbox}{width=\columnwidth,center}
\scalebox{0.9}{
\begin{tabular}{cccccccccc}
% \Xcline{1-10}{0.3pt}
\toprule[0.05em]
\rowcolor{Gray}
Datasets & Method &	WB \cite{WB} 	&	BTES \cite{BTES}	&	PPID \cite{PPID} & GRMR \cite{GRMR}  & In-Net \cite{InNet}& MCAN \cite{MCAN} & \textbf{FDM-Net} & \textbf{FDM-Net-L}\\
\midrule[0.05em]
\multirow{4}{*}{ARAD 901}  & PSNR $\uparrow$ &  29.27 & 29.52& 36.87 & 29.35 & 44.98& 41.60& \textbf{52.41} & 51.65\\
& SSIM $\uparrow$ & 0.956 & 0.947& 0.969 &0.960 &0.993 &0.988& \textbf{0.998} & 0.997\\
& SAM $\downarrow$  & 0.093&   0.089&   0.090&   0.096   &  0.011   &  0.020   &\textbf{ 0.004} & 0.004\\
& MRAE $\downarrow$ &-&-&-&-& 0.014 &0.022 & \textbf{0.005} & 0.005\\
\Xcline{1-10}{0.1pt}
% \end{tabular}}
% % \end{adjustbox}
% \end{table*}
% \definecolor{Gray}{gray}{0.95}
% \begin{table*}
% \centering
% \renewcommand{\arraystretch}{1.1}%
% \caption{Quantitative evaluation on testing datasets.}
% \label{Table:PQI_2}
% % \begin{adjustbox}{width=\columnwidth,center}
% \scalebox{0.83}{
% \begin{tabular}{ccccccccc}
% \Xcline{1-9}{0.7pt}
% \rowcolor{Gray}
% Dataset & Method &	WB \cite{WB} 	&	BTES \cite{BTES}	&	PPID \cite{PPID} & GRMR \cite{GRMR} & In-Net \cite{InNet} & MCAN \cite{MCAN}  & \textbf{FDM-Net (Ours)}\\
% \Xcline{1-9}{0.4pt}
\multirow{4}{*}{ARAD 903}  & PSNR $\uparrow$ &  30.95 & 31.06& 39.10 & 30.99& 41.09&36.92 & \textbf{48.78} & 48.63\\
& SSIM $\uparrow$ & 0.965 & 0.955& 0.977 &0.967 &0.985 &0.936 & \textbf{0.993} & 0.993\\
& SAM $\downarrow$ & 0.089 & 0.078 & 0.063 & 0.085 & 0.041 & 0.065& \textbf{ 0.011} & 0.011\\
& MRAE $\downarrow$ &-&-&-&-& 0.037& 0.071 & \textbf{0.012} & 0.012\\
\Xcline{1-10}{0.1pt}
% \end{tabular}}
% % \end{adjustbox}
% \end{table*}
% \definecolor{Gray}{gray}{0.95}
% \begin{table*}
% \centering
% \renewcommand{\arraystretch}{1.1}%
% \caption{Quantitative evaluation on testing datasets.}
% \label{Table:PQI_3}
% % \begin{adjustbox}{width=\columnwidth,center}
% \scalebox{0.83}{
% \begin{tabular}{ccccccccc}
% \Xcline{1-9}{0.7pt}
% \rowcolor{Gray}
% Dataset & Method &	WB \cite{WB} 	&	BTES \cite{BTES}	&	PPID \cite{PPID} & GRMR \cite{GRMR}  & In-Net \cite{InNet} & MCAN \cite{MCAN} & \textbf{FDM-Net (Ours)}\\
% \Xcline{1-9}{0.4pt}
\multirow{4}{*}{ARAD 907}  & PSNR $\uparrow$ &  33.85 & 33.49& 38.81 & 33.96& 43.50& 45.29 & \textbf{51.81} & 50.84\\
& SSIM $\uparrow$ & 0.943 & 0.929& 0.959 &0.944 &0.984 &0.991 & \textbf{0.997} & 0.997\\
& SAM $\downarrow$ & 0.113 & 0.134 & 0.079 & 0.113 & 0.033& 0.030& \textbf{0.012} & 0.013\\
& MRAE $\downarrow$ &-&-&-&-& 0.043 & 0.038 & \textbf{ 0.015} & 0.017\\
\Xcline{1-10}{0.1pt}
% \end{tabular}}
% % \end{adjustbox}
% \end{table*}
% \definecolor{Gray}{gray}{0.95}
% \begin{table*}[!t]
% \centering
% \renewcommand{\arraystretch}{1.1}%
% \caption{HSIs demosaicing results compared with other methods on the 50 HSI cubes \cite{arad2022ntire}. FDM-Net achieves SOTA results.}
% \label{Table:PQI_4}
% % \begin{adjustbox}{width=\columnwidth,center}
% \scalebox{0.83}{
% \begin{tabular}{ccccccccc}
% \Xcline{1-9}{0.7pt}
% \rowcolor{Gray}
% Datasets & Method &	WB \cite{WB} 	&	BTES \cite{BTES}	&	PPID \cite{PPID} & GRMR \cite{GRMR} & In-Net \cite{InNet} & MCAN \cite{MCAN}  & \textbf{FDM-Net (Ours)}\\
% \Xcline{1-9}{0.4pt}
\multirow{5}{*}{50 HSIs} 
& PSNR $\uparrow$ & 31.17 & 30.94& 35.98 &31.38 &42.88  &43.22 & \textbf{49.23} & 48.60\\
& SSIM $\uparrow$ & 0.912 & 0.892 & 0.937& 0.922 & 0.981 &  0.986& \textbf{0.996} & 0.995\\
& SAM $\downarrow$ &0.158 & 0.176 & 0.121 & 0.150 & 0.034  &  0.034& \textbf{0.013} & 0.014\\
& MRAE  $\downarrow$ & - & - & - & - & 0.043  &0.044 & \textbf{0.017} & 0.018\\
\Xcline{1-10}{0.1pt}
Average & TIME (s)  & 0.11 & 0.12 & 0.75 & 15.18 &  0.014 & 0.015 & 0.054 & 0.029 \\
\bottomrule[0.05em]
\end{tabular}}
% \end{adjustbox}
\end{table*}



% trim={<left> <lower> <right> <upper>}
\begin{figure*}[!htbp]
	% \captionsetup{font=small}
	\centering
	\scriptsize
	\renewcommand{\h}{0.105}
	\renewcommand{\wa}{0.12}
	\newcommand{\wb}{0.16}
	\renewcommand{\g}{-0.7mm}
	\renewcommand{\tabcolsep}{1.8pt}
	\renewcommand{\arraystretch}{1}
        \resizebox{0.9\linewidth}{!} {
		\begin{tabular}{cc}			
			\renewcommand{\name}{figures/arad_929/ARAD_1K_0929_16_}
			\renewcommand{\h}{0.12}
			\renewcommand{\w}{0.2}
			\begin{tabular}{cc}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{c}%
		         	\includegraphics[trim={74 40 0 40 },clip, width=0.275\textwidth]{\name mosaic.jpg}
						\\
						ARAD 929: Mosaic 
					\end{tabular}
				\end{adjustbox}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{cccccc}
						\includegraphics[trim={125 215 115 40 },clip,height=\h \textwidth, width=\w \textwidth]{\name GT.png} \hspace{\g} &
						\includegraphics[trim={125 215 115 40 },clip,height=\h \textwidth, width=\w \textwidth]{\name BTES.jpg} \hspace{\g} &
						\includegraphics[trim={125 215 115 40 },clip,height=\h \textwidth, width=\w \textwidth]{\name WB.jpg} &
						\includegraphics[trim={125 215 115 40 },clip,height=\h \textwidth, width=\w \textwidth]{\name PPID.jpg} \hspace{\g} 
						\\
						GT  &
						BTES\cite{BTES} & WB\cite{WB} &
						PPID~\cite{PPID} 
						\\
						\vspace{-1.5mm}
						\\
						\includegraphics[trim={125 215 115 40 },clip,height=\h \textwidth, width=\w \textwidth]{\name GRMR.jpg} \hspace{\g} &
						\includegraphics[trim={125 215 115 40 },clip,height=\h \textwidth, width=\w \textwidth]{\name MCAN.png} \hspace{\g} &
						\includegraphics[trim={125 215 115 40 },clip,height=\h \textwidth, width=\w \textwidth]{\name InNet.png}
						\hspace{\g} &		
						\includegraphics[trim={125 215 115 40},clip,height=\h \textwidth, width=\w \textwidth]{\name our.png} 
						\\ 
						GRMR\cite{GRMR}  \hspace{\g} &	MCAN \cite{MCAN}  \hspace{\g} & InNet~\cite{InNet}
						&
						\textbf{FDM-Net} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular}	
		\end{tabular}
	}
% 	\vspace{0.5mm}
% 	\caption{Visual comparison of \textbf{HSI demosaicing} methods (False color, R: 2, G: 11, B:16).} %
% 	  \vspace{-2mm}
% 	\label{fig_ARAD_929}
% \end{figure*}
% % 145 235 125 45 big P

% \begin{figure*}[!htbp]
% 	% \captionsetup{font=small}
% 	\centering
% 	\scriptsize
% 	\renewcommand{\h}{0.105}
% 	\renewcommand{\wa}{0.12}
% 	\newcommand{\wb}{0.16}
% 	\renewcommand{\g}{-0.7mm}
% 	\renewcommand{\tabcolsep}{1.8pt}
% 	\renewcommand{\arraystretch}{1}
        \resizebox{0.9\linewidth}{!} {
		\begin{tabular}{cc}			
			\renewcommand{\name}{figures/arad_907/ARAD_1K_0907_16_}
			\renewcommand{\h}{0.12}
			\renewcommand{\w}{0.2}
			\begin{tabular}{cc}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{c}%
		         	\includegraphics[trim={74 0 0 80 },clip, width=0.275\textwidth]{\name mosaic.jpg}
						\\
						ARAD 907: Mosaic 
					\end{tabular}
				\end{adjustbox}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{cccccc}
						\includegraphics[trim={160 45 40 90 },clip,height=\h \textwidth, width=\w \textwidth]{\name GT.jpg} \hspace{\g} &
						\includegraphics[trim={160 45 40 90  },clip,height=\h \textwidth, width=\w \textwidth]{\name BTES.jpg} \hspace{\g} &
						\includegraphics[trim={160 45 40 90  },clip,height=\h \textwidth, width=\w \textwidth]{\name WB.jpg} &
						\includegraphics[trim={160 45 40 90  },clip,height=\h \textwidth, width=\w \textwidth]{\name PPID.jpg} \hspace{\g} 
						\\
						GT  &
						BTES\cite{BTES} & WB\cite{WB} &
						PPID~\cite{PPID} 
						\\
						\vspace{-1.5mm}
						\\
						\includegraphics[trim={160 45 40 90  },clip,height=\h \textwidth, width=\w \textwidth]{\name GRMR.jpg} \hspace{\g} &
						\includegraphics[trim={160 45 40 90  },clip,height=\h \textwidth, width=\w \textwidth]{\name MCAN.png} \hspace{\g} &
						\includegraphics[trim={160 45 40 90   },clip,height=\h \textwidth, width=\w \textwidth]{\name InNet.png}
						\hspace{\g} &		
						\includegraphics[trim={160 45 40 90    },clip,height=\h \textwidth, width=\w \textwidth]{\name our.png} 
						\\ 
						GRMR\cite{GRMR}  \hspace{\g} &	MCAN \cite{MCAN}  \hspace{\g} & InNet~\cite{InNet}
						&
						\textbf{FDM-Net} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular}	
		\end{tabular}
	}
	\vspace{0.5mm}
	\caption{Visual comparison of \textbf{HSI demosaicing} methods (False color, R: 2, G: 11, B:16).} %
	  \vspace{-2mm}
	\label{fig_ARAD_907}
\end{figure*}



\section{DATASETS AND TRAINING}

\subsection{Datasets and Processing}

% \begin{figure}[h]
% \centering
% \includegraphics[width=1.4in]{figures/thumbnail_ARAD2022.png}
% \includegraphics[width=1.8in]{figures/response_ARAD.png}
% \caption{Sample images and response function of the prototype MSFA camera sensor from the ARAD 1K hyperspectral image dataset \cite{arad2022ntire}. Note images modified for optimal display.}
% \label{fig_arad_thumbnail}
% \end{figure}

% \begin{figure}
% \centering
% \includegraphics[width=2.8in]{figures/response_ARAD.png}
% \caption{Response curve of the prototype MSFA sensor used to capture ARAD dataset \cite{arad2022ntire}. 
% The sensor provides 16 channels of spectral information over the 400-1000nm range, spatially sub-sampled using a 4 by 4 multi-spectral filter array. The approximate peaks of each channel are denoted in the
% legend. Line colors are roughly correlated to the perceived “color” of each filter to a human observer. }
% \label{fig_response_curve}
% \end{figure}

Attaining high accuracy in supervised MSFA demosaicing requires access to ground truth hyperspectral information for both training and evaluation purposes. We utilized the ARAD 1K dataset \cite{arad2022ntire,arad2022ntire_RGB2HSI} to meet this requirement, which comprises 384 images ranging from 400-1000nm and includes 16-channel hyperspectral images spanning a wide range of wavelengths. The ground truth hyperspectral information is presented as 480 x 512 spatial resolution images over the 16 spectral bands.
% We validated our model on 50 HSI cubes, which features various scenes and also served as the validation dataset for the NTIRE 2022 spectral demosaicing challenge \cite{arad2022ntire}.
Previous methods for spectral demosaicing have employed diverse techniques, but they have often relied on limited datasets for evaluation, with some recent works testing on fewer than 10 images. In contrast, we validated our model on 50 HSI cubes, which features various scenes and also served as the validation dataset for the NTIRE 2022 spectral demosaicing challenge \cite{arad2022ntire,arad2022ntire_RGB2HSI}. 
% All datasets were normalized to the [0, 1] range before usage in training and testing.

\subsection{Training Details}

The proposed demosaicing model was trained from scratch using randomly initialized weights drawn from a normal distribution. We employed the Adam optimizer \cite{kingma2014adam} with a learning rate of 0.0001 that was halved every 1000 epochs. 
The weights of our loss are: $\alpha_1 = \alpha_2 = \alpha_3 = 1$.
% The number of STMC blocks used was $N_1 = N_2 = N_3 = N_4 = 2$, and a batch size of 1 was utilized. 
For all compared methods and our FDM-Net, the patch size was set to $128 \times 128$, and the MSFA pattern was $4 \times 4$, resulting in 16 bands in the latent HSI. All networks were trained using one NVIDIA RTX 2080Ti-11GB GPU running Ubuntu 22.04.1. The FDM-Net was trained for 1600 epochs until convergence. In addition, for fair comparison, all the SOTA methods are retrained on ARAD1K \cite{arad2022ntire}.



% \begin{figure*}[h]
% \centering
% \includegraphics[width=6in]{figures/ARAD_901_comparison.jpg}
% \caption{Visual results of FDM-Net and SOTA methods on NTIRE 2022 spectral demosaicing challenge validation datasets.}
% \label{fig_ARAD_901}
% \end{figure*}
%  trim={<left> <lower> <right> <upper>}
\begin{figure*}[!htbp]
	% \captionsetup{font=small}
	\centering
	\scriptsize
	\renewcommand{\h}{0.105}
	\renewcommand{\wa}{0.12}
	\newcommand{\wb}{0.16}
	\renewcommand{\g}{-0.7mm}
	\renewcommand{\tabcolsep}{1.8pt}
	\renewcommand{\arraystretch}{1}
        \resizebox{0.9\linewidth}{!} {
		\begin{tabular}{cc}			
			\renewcommand{\name}{figures/arad_901/ARAD_1K_0901_16_}
			\renewcommand{\h}{0.12}
			\renewcommand{\w}{0.2}
			\begin{tabular}{cc}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{c}%
		         	\includegraphics[trim={74 10 0 70 },clip, width=0.275\textwidth]{\name mosaic.jpg}
						\\
						ARAD 901: Mosaic 
					\end{tabular}
				\end{adjustbox}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{cccccc}
						\includegraphics[trim={110 30 150 260 },clip,height=\h \textwidth, width=\w \textwidth]{\name gt.png} \hspace{\g} &
						\includegraphics[trim={110 30 150 260  },clip,height=\h \textwidth, width=\w \textwidth]{\name BTES.jpg} \hspace{\g} &
						\includegraphics[trim={110 30 150 260  },clip,height=\h \textwidth, width=\w \textwidth]{\name WB.jpg} &
						\includegraphics[trim={110 30 150 260  },clip,height=\h \textwidth, width=\w \textwidth]{\name PPID.jpg} \hspace{\g} 
						\\
						GT  &
						BTES\cite{BTES} & WB\cite{WB} &
						PPID~\cite{PPID} 
						\\
						\vspace{-1.5mm}
						\\
						\includegraphics[trim={110 30 150 260  },clip,height=\h \textwidth, width=\w \textwidth]{\name GRMR.jpg} \hspace{\g} &
						\includegraphics[trim={110 30 150 260  },clip,height=\h \textwidth, width=\w \textwidth]{\name MCAN.png} \hspace{\g} &
						\includegraphics[trim={110 30 150 260  },clip,height=\h \textwidth, width=\w \textwidth]{\name InNet.png}
						\hspace{\g} &		
						\includegraphics[trim={110 30 150 260  },clip,height=\h \textwidth, width=\w \textwidth]{\name our.png} 
						\\ 
						GRMR\cite{GRMR}  \hspace{\g} & MCAN \cite{MCAN}  \hspace{\g} &	InNet~\cite{InNet}
						&
						\textbf{FDM-Net} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular}	
		\end{tabular}
	}
	\vspace{0.5mm}
	\caption{Visual comparison of \textbf{HSI demosaicing} methods (False color, R: 2, G: 11, B:16).} %
	  \vspace{-2mm}
	\label{fig_ARAD_901}
\end{figure*}






% \begin{figure*}
% \centering
% \includegraphics[width=6in]{figures/ARAD_903_comparison_1.jpg}
% \caption{Visual results of FDM-Net and SOTA methods on NTIRE 2022 spectral demosaicing challenge datasets.}
% \label{fig_ARAD_903}
% \end{figure*}
% \begin{figure*}[!htbp]
% 	% \captionsetup{font=small}
% 	\centering
% 	\scriptsize
% 	\renewcommand{\h}{0.105}
% 	\renewcommand{\wa}{0.12}
% 	\newcommand{\wb}{0.16}
% 	\renewcommand{\g}{-0.7mm}
% 	\renewcommand{\tabcolsep}{1.8pt}
% 	\renewcommand{\arraystretch}{1}
%         \resizebox{0.9\linewidth}{!} {
% 		\begin{tabular}{cc}			
% 			\renewcommand{\name}{figures/arad_903/ARAD_1K_0903_16_}
% 			\renewcommand{\h}{0.12}
% 			\renewcommand{\w}{0.2}
% 			\begin{tabular}{cc}
% 				\begin{adjustbox}{valign=t}
% 					\begin{tabular}{c}%
% 		         	\includegraphics[trim={74 0 0 80 },clip, width=0.275\textwidth]{\name mosaic.jpg}
% 						\\
% 						ARAD 903: Mosaic 
% 					\end{tabular}
% 				\end{adjustbox}
% 				\begin{adjustbox}{valign=t}
% 					\begin{tabular}{cccccc}
% 						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name gt.png} \hspace{\g} &
% 						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name BTES.jpg} \hspace{\g} &
% 						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name WB.jpg} &
% 						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name PPID.jpg} \hspace{\g} 
% 						\\
% 						GT &
% 						BTES\cite{BTES} & WB\cite{WB} &
% 						PPID~\cite{PPID} 
% 						\\
% 						\vspace{-1.5mm}
% 						\\
% 						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name GRMR.jpg} \hspace{\g} &
% 						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name MCAN.png} \hspace{\g} &
% 						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name InNet.png}
% 						\hspace{\g} &		
% 						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name our.png} 
% 						\\ 
% 						GRMR\cite{GRMR}  \hspace{\g} &	MCAN \cite{MCAN}  \hspace{\g} & InNet~\cite{InNet}
% 						&
% 						\textbf{FDM-Net} (ours)
% 						\\
% 					\end{tabular}
% 				\end{adjustbox}
% 			\end{tabular}	
% 		\end{tabular}
% 	}
% 	\vspace{0.5mm}
% 	\caption{Visual comparison of \textbf{HSI demosaicing} methods (False color, R: 2, G: 11, B:16).} %
% 	  \vspace{-2mm}
% 	\label{fig_ARAD_903}
% \end{figure*}
% \begin{figure*}
% \centering
% \includegraphics[width=6in]{figures/ARAD_907_comparison_1.jpg}
% \caption{Visual results of FDM-Net and SOTA methods on testing datasets. Please zoom in for a better view.}
% \label{fig_ARAD_907}
% \end{figure*}

\begin{figure}[!htbp]
\centering
% \rotatebox{90}{\vspace{7mm}  \centering Image}
\includegraphics[width=3.3in]{figures/imec/imec_001784.png}
\includegraphics[width=3.3in]{figures/imec/imec_004000.png}
\vspace{-4.8mm}
\caption{Illustration of the results on real data captured by our-self with IMEC $4 \times 4$ hyperspectral camera. }
\label{fig_imec}
\end{figure}

 % trim={<left> <lower> <right> <upper>}
% \begin{figure*}[!htbp]
% 	% \captionsetup{font=small}
% 	\centering
% 	\scriptsize
% 	\renewcommand{\h}{0.105}
% 	\renewcommand{\wa}{0.12}
% 	\newcommand{\wb}{0.16}
% 	\renewcommand{\g}{-0.7mm}
% 	\renewcommand{\tabcolsep}{1.8pt}
% 	\renewcommand{\arraystretch}{1}
%         \resizebox{0.9\linewidth}{!} {
% 		\begin{tabular}{cc}			
% 			\renewcommand{\name}{figures/arad_940/ARAD_1K_0940_16_}
% 			\renewcommand{\h}{0.12}
% 			\renewcommand{\w}{0.2}
% 			\begin{tabular}{cc}
% 				\begin{adjustbox}{valign=t}
% 					\begin{tabular}{c}%
% 		         	\includegraphics[trim={0 10 70 65 },clip, width=0.275\textwidth]{\name GT.png}
% 						\\
% 						ARAD 940 
% 					\end{tabular}
% 				\end{adjustbox}
% 				\begin{adjustbox}{valign=t}
% 					\begin{tabular}{cccccc}
% 						\includegraphics[trim={165 205 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name GT.png} \hspace{\g} &
% 						\includegraphics[trim={165 205 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name BTES.jpg} \hspace{\g} &
% 						\includegraphics[trim={165 205 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name WB.jpg} &
% 						\includegraphics[trim={165 205 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name PPID.jpg} \hspace{\g} 
% 						\\
% 						GT  &
% 						BTES\cite{BTES} & WB\cite{WB} &
% 						PPID~\cite{PPID} 
% 						\\
% 						\vspace{-1.5mm}
% 						\\
% 						\includegraphics[trim={165 205 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name GRMR.jpg} \hspace{\g} &
% 						\includegraphics[trim={165 205 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name MCAN.png} \hspace{\g} &
% 						\includegraphics[trim={165 205 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name InNet.png}
% 						\hspace{\g} &		
% 						\includegraphics[trim={165 205 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name our.png} 
% 						\\ 
% 						GRMR\cite{GRMR}  \hspace{\g} &	MCAN \cite{MCAN}  \hspace{\g} & InNet~\cite{InNet}
% 						&
% 						\textbf{FDM-Net} (ours)
% 						\\
% 					\end{tabular}
% 				\end{adjustbox}
% 			\end{tabular}	
% 		\end{tabular}
% 	}
% 	\vspace{0.5mm}
% 	\caption{Visual comparison of \textbf{HSI demosaicing} methods (False color, R: 2, G: 11, B:16).} %
% 	  \vspace{-2mm}
% 	\label{fig_ARAD_940}
% \end{figure*}

\begin{figure}[!htbp]
	% \captionsetup{font=small}
	\centering
	\scriptsize
	\renewcommand{\h}{0.105}
	\renewcommand{\wa}{0.12}
	\newcommand{\wb}{0.16}
	\renewcommand{\g}{-0.7mm}
	\renewcommand{\tabcolsep}{1.8pt}
	\renewcommand{\arraystretch}{1}
        \resizebox{1\linewidth}{!} {
		\begin{tabular}{c}	
                \normalsize
			\renewcommand{\name}{figures/ablation/ARAD_1K_0933_16_}
			\renewcommand{\h}{0.15}
			\renewcommand{\w}{0.09}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{cccccc}
						\includegraphics[trim={130 50 85 140 },clip,height=\h \textwidth, width=\w \textwidth]{\name gt.png}  &
						\includegraphics[trim={130 50 85 140 },clip,height=\h \textwidth, width=\w \textwidth]{\name our_blind.png}  &
						\includegraphics[trim={130 50 85 140 },clip,height=\h \textwidth, width=\w \textwidth]{\name our.png}  &
                        % \\ 
					% GT  \hspace{\g} &	\textbf{W/O} Frequency  \hspace{\g} & \textbf{W/} Frequency
					% \\
					% \vspace{-1.5mm}
					% \\
					\includegraphics[trim={0 80 210 110 },clip,height=\h \textwidth, width=\w \textwidth]{\name gt.png}  &
						\includegraphics[trim={0 80 210 110 },clip,height=\h \textwidth, width=\w \textwidth]{\name our_blind.png}  &
						\includegraphics[trim={0 80 210 110 },clip,height=\h \textwidth, width=\w \textwidth]{\name our.png} 
                        \\ 
					GT  &	\textbf{W/O} & \textbf{W/} & GT  &	\textbf{W/O} & \textbf{W/}
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular}
	}
	\vspace{-2mm}
	\caption{Visual comparison of \textbf{ablation studies}, \textbf{w/} means High+Low frequency based demosaicing using MaFormer, \textbf{w/o} is blind demosaicing using MaFormer. Zoom in for better view.} %
	  \vspace{-2mm}
	\label{fig_ablation}
\end{figure}

\section{EXPERIMENTAL RESULTS}

\subsection{Quantitative Experiments Metrics}
We quantitatively evaluate the performance of our proposed method by measuring the peak signal-to-noise ratio (PSNR), structure similarity (SSIM), Mean Relative Absolute Error (MRAE), and spectral angle mapper (SAM). PSNR and SSIM are conventional PQIs in image processing and computer vision that measure the similarity between the target and reference images based on mean squared error (MSE) and structural consistency, respectively. 
% The higher the values of PSNR, SSIM, the closer the target MSI is to the reference image. 
Finally, SAM is used to compute the spectral fidelity.
% between the reference and demosaiced multi-spectral images.

\subsection{Quantitative Comparison}
We compare the proposed approach with the strong baselines: MCAN \cite{MCAN}, In-Net \cite{InNet}, PPID \cite{PPID}, GRMR \cite{GRMR}, and also classical WB \cite{WB}, BTES \cite{BTES}. 
% Specifically, we compare the performance of these methods on a $4 \times 4$ MSFA pattern with 16 spectral channels. 
The comparisons between FDM-Net, its light version: FDM-Net-L, and other SOTA methods are listed in Tab.~\ref{Table:PQI_1}. As can be observed: Our FDM-Net outperforms SOTA methods by a large margin. Specifically, FDM-Net surpasses the recent best algorithm MCAN by 6.01 dB, the classic PPID by 13.25dB, on the averaged results of 50 HSIs. 
In addition, our method exceeds all the counterparts with 7dB on three sub-datasets.
These results demonstrate the effectiveness of our method.
Furthermore, Tab.~\ref{Table:PQI_1} and Fig.~\ref{fig_spectral_curve} also present the spectral angle mapper (SAM) and curve of all tested methods. Our proposed FDM-Net demonstrates the highest correlation and coincidence with the reference, indicating its superiority in achieving spectral-dimension consistency reconstruction.
\begin{figure}[!htbp]
\centering
% \rotatebox{90}{\vspace{7mm}  \centering Image}
\includegraphics[width=1.6in]{figures/arad_907/Spectral_arad907_300_255.jpg}
\includegraphics[width=1.6in]{figures/arad_907/Spectral_arad907_300_250.jpg}
\vspace{-1mm}
\caption{Illustration of the Spectral Density Curves on ARAD 907 HSI cube.  The curve for FDM-Net appears to be the most similar to the curve for GT, indicating that the spectral properties of FDM-Net are more closely aligned with the ground truth than the others.}
\label{fig_spectral_curve}
\end{figure}

\subsection{Visual Comparison}
In this subsection, we present a comparison of our FDM-Net with other state-of-the-art (SOTA) methods for MSFA demosaicing, as depicted in Fig.~\ref{fig_ARAD_907}, \ref{fig_ARAD_901}. The left side of each image represents the observation data, which is the raw mosaicked image, while the right side displays zoomed-in patches of the red boxes in the entire hyperspectral images (HSIs).
Our FDM-Net exhibits superior performance compared to the other methods by producing visually pleasing HSIs with more detailed content, cleaner textures, and fewer artifacts, while simultaneously preserving the spatial smoothness of homogeneous regions. In contrast, previous methods have either yielded over-smooth results, which compromise fine-grained structures, or have introduced undesired chromatic artifacts and blotchy textures that are not present in the ground truth. Furthermore, Fig. \ref{fig_imec} demonstrates the efficacy of FDM-Net in processing real data captured with an IMEC HSI camera. More detailed results and discussion please refer to the \textbf{supplementary material}.
% \begin{figure}
% \centering
% % \subfigure[GT]{\includegraphics[width=1in]{figures/ARAD_1K_0913_16_gt_blind_p1.png}}
% % \subfigure[w/o frequency]{\includegraphics[width=1in]{figures/ARAD_1K_0913_16_our_blind_p1.png}}
% % \subfigure[with frequency]{\includegraphics[width=1in]{figures/ARAD_1K_0913_16_our_p1.png}}
% \includegraphics[width=3in]{figures/ablation.jpg}
% \caption{Visual results of ablation study.}
% \label{fig_ablation}
% \end{figure}
\subsection{Ablation Study on Pipeline Components}
In order to examine the necessity of each component within our pipeline, we performed an ablation study on two distinct configurations of our method. These configurations included: 1) blind demosaicing using the MaFormer model, 2) utilizing the FDM-Net model with $N_i$ STMC blocks, with $N_i$ being fine-tuned for $i=1,2,3,4$, and then we evaluate the performance of these pipelines. The qualitative results of our ablation study are depicted in Fig.~\ref{fig_ablation} and \ref{fig_ablation_light}, while the quantitative results are presented in Tab.~\ref{Table:PQI_ablation} and \ref{Table:PQI_ablation_light}.
By comparing the blind demosaicing approach to the frequency-driven demosaicing method, we found that the latter demonstrated improvement in all metrics. This provides evidence that the method described in Sec.~\ref{model:overview}, which separates high and low frequency components, is superior to methods that do not. Additionally, we observed that increasing the value of $N_i$ from 1 to 2 resulted in a performance improvement of 1.59dB, but it also increased the running time by 1.86 times.

\begin{figure}
	% \captionsetup{font=small}
	\centering
	\scriptsize
	\renewcommand{\h}{0.105}
	\renewcommand{\wa}{0.12}
	\newcommand{\wb}{0.16}
	\renewcommand{\g}{-0.7mm}
	\renewcommand{\tabcolsep}{1.8pt}
	\renewcommand{\arraystretch}{1}
        \resizebox{1\linewidth}{!} {
		\begin{tabular}{c}	
                \normalsize
			\renewcommand{\name}{figures/FDMNet_light/}
			\renewcommand{\h}{0.15}
			\renewcommand{\w}{0.09}
				\begin{adjustbox}{valign=t} %  % trim={<left> <lower> <right> <upper>}
					\begin{tabular}{cccccc}
						\includegraphics[trim={50 120 210 130 },clip,height=\h \textwidth, width=\w \textwidth]{ \name ARAD_1K_0905_16_gt.png}  &
						\includegraphics[trim={50 120 210 130 },clip,height=\h \textwidth, width=\w \textwidth]{\name ARAD_1K_0905_16_our_sc1.png}  &
						\includegraphics[trim={50 120 210 130 },clip,height=\h \textwidth, width=\w \textwidth]{\name ARAD_1K_0905_16_our.png}  &
                        % \\ 
					% GT  \hspace{\g} &	\textbf{W/O} Frequency  \hspace{\g} & \textbf{W/} Frequency
					% \\
					% \vspace{-1.5mm}
					% \\
					\includegraphics[trim={50 120 210 130 },clip,height=\h \textwidth, width=\w \textwidth]{\name ARAD_1K_0908_16_gt.png}  &
						\includegraphics[trim={50 120 210 130 },clip,height=\h \textwidth, width=\w \textwidth]{\name ARAD_1K_0908_16_our_sc1.png}  &
						\includegraphics[trim={50 120 210 130 },clip,height=\h \textwidth, width=\w \textwidth]{\name ARAD_1K_0908_16_our.png} 
                        \\ 
					GT  & FDM-Net-L & FDM-Net & GT  &	FDM-Net-L & FDM-Net
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular}
	}
	\vspace{-2mm}
	\caption{Visual comparison of \textbf{ablation studies}, FDM-Net in normal size with $N_i$=2, FDM-Net-L is a light one with $N_i$=1.} %
	  \vspace{-2mm}
	\label{fig_ablation_light}
\end{figure}
\begin{table}
\centering
\renewcommand{\arraystretch}{1.3}%
\caption{The PSNR, SSIM, SAM(lower is better), MRAE(lower is better) scores for the \textbf{ablation studies} of frequency.}
\label{Table:PQI_ablation}
% \begin{adjustbox}{width=\columnwidth,center}
\scalebox{0.67}{
\begin{tabular}{ccccccc}
\toprule[0.05em]
% \rowcolor{Gray}
% \rowcolor{Gray}
High& Low &  \multirow{2}{*}{Method} &	\multirow{2}{*}{PSNR $\uparrow$} & \multirow{2}{*}{SSIM $\uparrow$}  & \multirow{2}{*}{SAM $\downarrow$} & \multirow{2}{*}{MRAE $\downarrow$}\\
Frequency &  Frequency &  &  &  &  &\\
\Xcline{1-7}{0.05pt}
\xmark  & \xmark & MaFormer  & 47.64 & 0.994 & 0.014 & 0.019\\
\cmark  & \cmark & FDM-Net  & \textbf{49.23}  & \textbf{0.996} & \textbf{0.013} & \textbf{0.017}\\
\Xcline{1-7}{0.05pt}
\vspace{-1mm}
\end{tabular}}
% \end{adjustbox}
\end{table}

\begin{table}
\centering
\renewcommand{\arraystretch}{1.3}%
\caption{The PSNR, SSIM, SAM, MRAE of the \textbf{ablation studies} of model size for the proposed FDM-Net, i.e., FDM-Net with normal size, $N_i=2, i-1,2,3,4$, and light FDM-Net with $N_i=1$.}
\label{Table:PQI_ablation_light}
% \begin{adjustbox}{width=\columnwidth,center}
\scalebox{0.68}{
\begin{tabular}{ccccccc}
\toprule[0.05em]
% \rowcolor{Gray}
% \rowcolor{Gray}
Model Size & STMC Block &	PSNR $\uparrow$ & SSIM $\uparrow$  & SAM $\downarrow$ & MRAE $\downarrow$ & Time(s)\\
\midrule
Light & $N_i$=1  & 48.60 & 0.995 & 0.014 & 0.018 & \textbf{0.029}\\
Normal & $N_i$=2 & \textbf{49.23}  & \textbf{0.996} & \textbf{0.013} & \textbf{0.017} & 0.053\\
\bottomrule
\vspace{-1mm}
\end{tabular}}
% \end{adjustbox}
\end{table}

% The SAM scores (in the third column) confirms that the spectral feature of the target HSI
% are best reconstructed when the aggregate high frequency and low frequency parts.
% Finally, as demonstrated by the last three columns, the number of STMC blocks of our MaFormer....



% \textbf{Real Raw Hyperspectral Image.}

% \begin{figure*}[h]
% \centering
% \includegraphics[width=7.0in]{figures/visual_comparison_1.png}
% \caption{visual comparison.}
% \label{fig_response_curve}
% \end{figure*}

% \begin{figure*}[h]
% \centering
% \includegraphics[width=7.0in]{figures/Visual_comparison.png}
% \caption{visual comparison.}
% \label{fig_c2}
% \end{figure*}

% \begin{figure*}[h]
% \centering
% \includegraphics[width=7.0in]{figures/visual_comparison_real.png}
% \caption{visual comparison.}
% \label{fig_real}
% \end{figure*}


% \begin{figure*}[h]
% \centering
% \includegraphics[width=7.0in]{ARAD_901_comparison_small.png}
% \includegraphics[width=7.0in]{ARAD_901_comparison_small.png}
% \includegraphics[width=7.0in]{ARAD_901_comparison_small.png}
% \caption{visual comparison.}
% \label{fig_ARAD_901}
% \end{figure*}

% \begin{figure}[h]
% \centering
% \includegraphics[width=3.3in]{figures/ARAD_901_comparison.png}
% \caption{visual comparison.}
% \label{fig_ARAD_901}
% \end{figure}




% \definecolor{Gray}{gray}{0.95}
% \begin{table}[h]
% \centering
% \renewcommand{\arraystretch}{1.5}%
% \caption{Quantitative evaluation on testing datasets.}
% \label{Table:PQI_2}
% \begin{adjustbox}{width=\columnwidth,center}
% \begin{tabular}{cccccccc}
% \Xcline{1-8}{0.9pt}
% \rowcolor{Gray}
% Dataset & Method &	WB \cite{WB} 	&	BTES \cite{BTES}	&	PPID \cite{PPID} & GRMR \cite{GRMR} & MCAN \cite{MCAN} & In-Net \cite{InNet} & \textbf{FDM-Net (Ours)}\\
% \Xcline{1-8}{0.4pt}
% PSNR $\uparrow$ &  0.0037 & 0.0096& 0.0237 & -& -&- & \textbf{53.12}\\
% % & Urban100  & 0.2105 & -& -\\
% % & MIT MOIRE  & 0.4107 & -& -\\
% SSIM $\downarrow$ & 51.51 & 45.31& 40.45 &40.58 &36.19 &32.43 & \textbf{0.998}\\
% & SAM $\downarrow$ & 0.4107 & -& - &&&& \textbf{0.012}\\
% & MRAE $\downarrow$ &&&&&&& \textbf{ 0.015}\\
% \Xcline{1-8}{0.4pt}
% \end{tabular}
% \end{adjustbox}
% \end{table}





% \subsubsection{Subsubsection Heading Here}
% Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.





% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
% \begin{figure*}[!t]
% \centering
% \subfloat[Case I]{\includegraphics[width=2.5in]{box}%
% \label{fig_first_case}}
% \hfil
% \subfloat[Case II]{\includegraphics[width=2.5in]{box}%
% \label{fig_second_case}}
% \caption{Simulation results for the network.}
% \label{fig_sim}
% \end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{Conclusion}
% In this paper, we have presented a novel HSI demosaicing method that is driven by both high and low frequencies. Specifically, we have proposed a customized transformer architecture that is designed to effectively handle the challenging task of high frequency HSI demosaicing. To further improve the high frequency modeling ability of our approach, we have introduced a joint spatial and frequency loss that ensures stable low frequency reconstruction while enhancing high frequency modeling.
% Our transformer-based model is particularly effective at reconstructing high frequency details by separating different frequency components. 
% To better capture the non-local dependencies, critical spectral correlation, and periodic MSFA pattern of HSI, we have guided our customized transformer using an MSFA convolution block.
% The proposed approach has been extensively evaluated on a large testing dataset comprising 50 HSI cubes. The experimental results demonstrate that our approach achieves state-of-the-art performance, outperforming other existing methods. 
% Overall, our proposed method can significantly improve the quality of HSI demosaicing, which is crucial for many real-world applications.


This paper has proposed a novel HSI demosaicing method that is driven by both high and low frequencies. The proposed method leverages Fourier zero-padding to quickly reconstruct the easy low frequency part, while a customized transformer architecture effectively handles the challenging task of high pass HSI demosaicing. By introducing a joint spatial and frequency loss, our approach enhances high frequency modeling while ensuring stable low frequency reconstruction.
Extensive evaluations of the proposed approach on a large testing dataset comprising 50 HSI cubes demonstrate that it achieves SOTA performance, outperforming SOTA 6.01dB.
Overall, the results indicate that focusing on the hard high frequency components has the potential to improve the accuracy and reliability of HSI demosaicing in various applications. 
Further research could explore the possibility of incorporating additional prior.
% to enhance the performance of the proposed approach.


\definecolor{Gray}{gray}{0.90}
\begin{table*}[!htbp]
	% \scriptsize
	\centering
	\setlength{\tabcolsep}{2.4pt}
	\renewcommand{\arraystretch}{1.2}%
	% \setlength\tabcolsep{1.1}
	\caption{\textbf{Detailed Demosaicing results compared with other methods. Our FDM-Net significantly surpass other competitors}}
	\vspace{1mm}
	\label{Table:PQI_1}
	% \begin{adjustbox}{width=\columnwidth,center}
	\scalebox{0.85}{
		\begin{tabular}{|c|>{\bfseries}c>{\bfseries}c>{\bfseries}c>{\bfseries}c|cccc|cccc|}
			% \Xcline{1-10}{0.3pt}
			\toprule[0.05em]
			\rowcolor{Gray}
			& \multicolumn{4}{c|}{\textbf{FDM-Net (Ours)}} & \multicolumn{4}{c|}{\textbf{MCAN}} & \multicolumn{4}{c|}{\textbf{InNet}} \\
			\rowcolor{Gray}
			HSIs	&	PSNR	&	SSIM	&	SAM	&	MRAE	&	\textbf{PSNR}	&	\textbf{SSIM}	&	\textbf{SAM}	&	\textbf{MRAE}	&	\textbf{PSNR}	&	\textbf{SSIM}	&	\textbf{SAM}	&	\textbf{MRAE}	\\	
			\midrule
			ARAD1K090116 	&	51.648	&	0.997	&	0.004	&	0.005	&	41.596	&	0.988	&	0.020	&	0.022	&	44.983	&	0.991	&	0.011	&	0.014	\\
			ARAD1K090216 	&	48.082	&	0.997	&	0.024	&	0.028	&	42.850	&	0.991	&	0.045	&	0.053	&	42.208	&	0.987	&	0.061	&	0.072	\\
			ARAD1K090316 	&	48.629	&	0.993	&	0.011	&	0.012	&	36.920	&	0.936	&	0.064	&	0.071	&	41.094	&	0.985	&	0.041	&	0.037	\\
			ARAD1K090416 	&	47.682	&	0.997	&	0.015	&	0.018	&	42.604	&	0.990	&	0.040	&	0.044	&	42.109	&	0.986	&	0.039	&	0.046	\\
			ARAD1K090516 	&	43.056	&	0.984	&	0.015	&	0.017	&	37.452	&	0.954	&	0.040	&	0.042	&	37.940	&	0.950	&	0.032	&	0.036	\\
			ARAD1K090616 	&	47.648	&	0.993	&	0.009	&	0.012	&	43.079	&	0.986	&	0.020	&	0.023	&	41.708	&	0.972	&	0.020	&	0.027	\\
			ARAD1K090716 	&	50.842	&	0.997	&	0.014	&	0.017	&	45.293	&	0.991	&	0.030	&	0.038	&	43.498	&	0.984	&	0.033	&	0.043	\\
			ARAD1K090816 	&	45.274	&	0.995	&	0.029	&	0.036	&	40.079	&	0.988	&	0.048	&	0.064	&	39.914	&	0.972	&	0.060	&	0.078	\\
			ARAD1K090916 	&	49.755	&	0.996	&	0.008	&	0.011	&	43.756	&	0.990	&	0.020	&	0.026	&	41.926	&	0.977	&	0.026	&	0.034	\\
			ARAD1K091016 	&	49.294	&	0.996	&	0.011	&	0.015	&	44.458	&	0.990	&	0.026	&	0.034	&	43.912	&	0.987	&	0.027	&	0.036	\\
			ARAD1K091116 	&	53.020	&	0.998	&	0.010	&	0.012	&	46.716	&	0.992	&	0.026	&	0.029	&	45.638	&	0.987	&	0.027	&	0.034	\\
			ARAD1K091216 	&	39.662	&	0.993	&	0.024	&	0.027	&	35.010	&	0.975	&	0.041	&	0.053	&	33.140	&	0.944	&	0.055	&	0.074	\\
			ARAD1K091316 	&	48.726	&	0.996	&	0.008	&	0.009	&	41.912	&	0.988	&	0.021	&	0.026	&	41.695	&	0.983	&	0.019	&	0.023	\\
			ARAD1K091416 	&	49.943	&	0.995	&	0.016	&	0.019	&	46.012	&	0.989	&	0.035	&	0.043	&	44.501	&	0.984	&	0.035	&	0.044	\\
			ARAD1K091516 	&	47.226	&	0.995	&	0.015	&	0.020	&	43.193	&	0.987	&	0.031	&	0.039	&	40.915	&	0.975	&	0.038	&	0.049	\\
			ARAD1K091616 	&	53.799	&	0.998	&	0.015	&	0.018	&	46.397	&	0.993	&	0.042	&	0.047	&	47.605	&	0.989	&	0.037	&	0.044	\\
			ARAD1K091716 	&	48.404	&	0.996	&	0.007	&	0.008	&	40.293	&	0.986	&	0.025	&	0.028	&	42.113	&	0.988	&	0.017	&	0.020	\\
			ARAD1K091816 	&	53.342	&	0.997	&	0.004	&	0.005	&	45.133	&	0.989	&	0.016	&	0.024	&	48.285	&	0.992	&	0.009	&	0.012	\\
			ARAD1K091916 	&	48.149	&	0.998	&	0.008	&	0.009	&	43.597	&	0.994	&	0.020	&	0.023	&	42.299	&	0.992	&	0.018	&	0.021	\\
			ARAD1K092016 	&	43.598	&	0.989	&	0.028	&	0.037	&	40.435	&	0.982	&	0.054	&	0.065	&	39.952	&	0.970	&	0.057	&	0.079	\\
			ARAD1K092116 	&	53.018	&	0.997	&	0.012	&	0.015	&	47.913	&	0.993	&	0.029	&	0.038	&	46.338	&	0.987	&	0.032	&	0.041	\\
			ARAD1K092216 	&	47.805	&	0.997	&	0.009	&	0.010	&	42.061	&	0.992	&	0.023	&	0.028	&	41.129	&	0.986	&	0.025	&	0.029	\\
			ARAD1K092316 	&	45.168	&	0.992	&	0.017	&	0.022	&	40.907	&	0.982	&	0.032	&	0.041	&	39.132	&	0.968	&	0.038	&	0.052	\\
			ARAD1K092416 	&	49.216	&	0.996	&	0.009	&	0.011	&	42.483	&	0.988	&	0.021	&	0.028	&	42.557	&	0.982	&	0.022	&	0.028	\\
			ARAD1K092516 	&	50.128	&	0.997	&	0.009	&	0.012	&	45.396	&	0.994	&	0.022	&	0.029	&	45.149	&	0.992	&	0.019	&	0.025	\\
			ARAD1K092616 	&	50.816	&	0.996	&	0.023	&	0.032	&	47.260	&	0.990	&	0.049	&	0.069	&	44.727	&	0.981	&	0.063	&	0.086	\\
			ARAD1K092716 	&	48.117	&	0.996	&	0.013	&	0.014	&	41.822	&	0.987	&	0.031	&	0.035	&	41.693	&	0.986	&	0.029	&	0.033	\\
			ARAD1K092816 	&	41.490	&	0.985	&	0.044	&	0.052	&	38.339	&	0.966	&	0.066	&	0.086	&	36.442	&	0.938	&	0.082	&	0.113	\\
			ARAD1K092916 	&	46.020	&	0.992	&	0.030	&	0.037	&	39.259	&	0.974	&	0.082	&	0.101	&	40.664	&	0.957	&	0.064	&	0.076	\\
			ARAD1K093016 	&	48.675	&	0.996	&	0.010	&	0.013	&	42.521	&	0.990	&	0.024	&	0.034	&	43.001	&	0.987	&	0.022	&	0.028	\\
			ARAD1K093116 	&	51.699	&	0.997	&	0.004	&	0.007	&	45.832	&	0.993	&	0.017	&	0.024	&	46.247	&	0.991	&	0.011	&	0.016	\\
			ARAD1K093216 	&	45.773	&	0.993	&	0.009	&	0.011	&	35.575	&	0.958	&	0.054	&	0.081	&	40.879	&	0.987	&	0.025	&	0.027	\\
			ARAD1K093316 	&	43.582	&	0.991	&	0.017	&	0.022	&	37.880	&	0.965	&	0.066	&	0.084	&	39.716	&	0.977	&	0.050	&	0.056	\\
			ARAD1K093416 	&	51.068	&	0.997	&	0.010	&	0.016	&	47.002	&	0.991	&	0.026	&	0.063	&	45.602	&	0.990	&	0.025	&	0.042	\\
			ARAD1K093516 	&	44.234	&	0.991	&	0.016	&	0.021	&	40.954	&	0.983	&	0.028	&	0.037	&	38.755	&	0.969	&	0.033	&	0.045	\\
			ARAD1K093616 	&	44.797	&	0.994	&	0.013	&	0.018	&	40.823	&	0.986	&	0.027	&	0.041	&	40.041	&	0.980	&	0.028	&	0.038	\\
			ARAD1K093716 	&	52.013	&	0.997	&	0.014	&	0.021	&	47.991	&	0.993	&	0.037	&	0.059	&	46.512	&	0.992	&	0.034	&	0.050	\\
			ARAD1K093816 	&	43.117	&	0.992	&	0.017	&	0.023	&	39.648	&	0.982	&	0.032	&	0.048	&	38.722	&	0.974	&	0.036	&	0.049	\\
			ARAD1K093916 	&	52.190	&	0.996	&	0.013	&	0.017	&	48.554	&	0.993	&	0.028	&	0.035	&	47.207	&	0.989	&	0.029	&	0.040	\\
			ARAD1K094016 	&	44.691	&	0.994	&	0.016	&	0.021	&	39.697	&	0.986	&	0.031	&	0.044	&	39.901	&	0.982	&	0.034	&	0.046	\\
			ARAD1K094116 	&	48.969	&	0.995	&	0.018	&	0.024	&	45.662	&	0.991	&	0.032	&	0.046	&	43.960	&	0.986	&	0.039	&	0.052	\\
			ARAD1K094216 	&	47.487	&	0.996	&	0.008	&	0.012	&	42.336	&	0.990	&	0.020	&	0.033	&	41.663	&	0.985	&	0.019	&	0.027	\\
			ARAD1K094316 	&	52.551	&	0.998	&	0.006	&	0.007	&	43.046	&	0.994	&	0.018	&	0.021	&	47.079	&	0.994	&	0.016	&	0.019	\\
			ARAD1K094416 	&	48.019	&	0.996	&	0.015	&	0.019	&	43.765	&	0.990	&	0.037	&	0.044	&	43.480	&	0.986	&	0.041	&	0.051	\\
			ARAD1K094516 	&	52.938	&	0.998	&	0.011	&	0.014	&	48.726	&	0.995	&	0.030	&	0.038	&	47.715	&	0.992	&	0.031	&	0.039	\\
			ARAD1K094616 	&	51.571	&	0.997	&	0.013	&	0.016	&	47.802	&	0.994	&	0.029	&	0.033	&	45.627	&	0.990	&	0.030	&	0.038	\\
			ARAD1K094716 	&	57.270	&	0.998	&	0.016	&	0.021	&	53.460	&	0.996	&	0.042	&	0.050	&	51.726	&	0.993	&	0.046	&	0.062	\\
			ARAD1K094816 	&	52.415	&	0.998	&	0.006	&	0.009	&	47.046	&	0.994	&	0.020	&	0.025	&	46.392	&	0.992	&	0.015	&	0.022	\\
			ARAD1K094916 	&	45.846	&	0.994	&	0.018	&	0.023	&	42.039	&	0.987	&	0.036	&	0.048	&	40.264	&	0.978	&	0.044	&	0.058	\\
			ARAD1K095016 	&	51.319	&	0.019	&	0.02	&	0.008	&	48.538	&	0.992	&	0.036	&	0.047	&	46.117	&	0.986	&	0.044	&	0.057	\\
			\hline
	\end{tabular}}
\end{table*}



% \begin{table}[h]
% \centering
% \begin{tabular}{|c|c|c|c|c|c|}
% \hline
% \textbf{File Name} & \textbf{Time (s)} & \textbf{PSNR\_DM} & \textbf{SSIM\_DM} & \textbf{SAM\_DM} & \textbf{MRAE\_DM} \\
% \hline
% $ARAD 1K_0901_16.mat$ & 0.034 & 51.65 & 0.997 & 0.004 & 0.005\\
% \hline
% $ARAD 1K_0902_16.mat$ & 0.029 & 48.08 & 0.997 & 0.024 & 0.028\\
% \hline
% $ARAD 1K_0903_16.mat$ & 0.029 & 48.63 & 0.993 & 0.011 & 0.012\\
% \hline
% $ARAD 1K_0904_16.mat$ & 0.029 & 47.68 & 0.997 & 0.015 & 0.018\\
% \hline
% $ARAD 1K_0905_16.mat$ & 0.029 & 43.06 & 0.984 & 0.015 & 0.017\\
% \hline
% $ARAD 1K_0906_16.mat$ & 0.028 & 47.65 & 0.993 & 0.009 & 0.012\\
% \hline
% $ARAD 1K_0907_16.mat$ & 0.028 & 50.84 & 0.997 & 0.014 & 0.017\\
% \hline
% $ARAD 1K_0908_16.mat$ & 0.029 & 45.27 & 0.995 & 0.029 & 0.036\\
% \hline
% \end{tabular}
% \caption{Results for each file}
% \end{table}


% \definecolor{Gray}{gray}{0.90}
% \begin{table*}[!htbp]
% % \scriptsize
% \centering
% \setlength{\tabcolsep}{1.4pt}
% \renewcommand{\arraystretch}{1.1}%
% % \setlength\tabcolsep{1.1}
% \caption{Demosaicing results and running time compared with other methods. FDM-Net achieves SOTA results.}
% \label{Table:PQI_1}
% % \begin{adjustbox}{width=\columnwidth,center}
% \scalebox{0.95}{
% \begin{tabular}{|cccccccccc|}
% % \Xcline{1-10}{0.3pt}
% \toprule[0.05em]
% \rowcolor{Gray}
% Datasets & Method &	WB \cite{WB} 	&	BTES \cite{BTES}	&	PPID \cite{PPID} & GRMR \cite{GRMR}  & In-Net \cite{InNet}& MCAN \cite{MCAN} & \textbf{FDM-Net} & \textbf{FDM-Net-L}\\
% \midrule[0.05em]
% \multirow{4}{*}{ARAD 902}  & PSNR $\uparrow$ &  29.27 & 29.52& 36.87 & 29.35 & 44.98& 41.60& \textbf{52.41} & 51.65\\
% & SSIM $\downarrow$ & 0.956 & 0.947& 0.969 &0.960 &0.993 &0.988& \textbf{0.998} & 0.997\\
% & SAM $\downarrow$  & 0.093&   0.089&   0.090&   0.096   &  0.011   &  0.020   &\textbf{ 0.004} & 0.004\\
% & MRAE $\downarrow$ &-&-&-&-& 0.014 &0.022 & \textbf{0.005} & 0.005\\
% \Xcline{1-10}{0.1pt}
% \multirow{4}{*}{ARAD 904}  & PSNR $\uparrow$ &  30.95 & 31.06& 39.10 & 30.99& 41.09&36.92 & \textbf{48.78} & 48.63\\
% & SSIM $\downarrow$ & 0.965 & 0.955& 0.977 &0.967 &0.985 &0.936 & \textbf{0.993} & 0.993\\
% & SAM $\downarrow$ & 0.089 & 0.078 & 0.063 & 0.085 & 0.041 & 0.065& \textbf{ 0.011} & 0.011\\
% & MRAE $\downarrow$ &-&-&-&-& 0.037& 0.071 & \textbf{0.012} & 0.012\\
% \Xcline{1-10}{0.1pt}
% \multirow{4}{*}{ARAD 905}  & PSNR $\uparrow$ &  33.85 & 33.49& 38.81 & 33.96& 43.50& 45.29 & \textbf{51.81} & 50.84\\
% & SSIM $\downarrow$ & 0.943 & 0.929& 0.959 &0.944 &0.984 &0.991 & \textbf{0.997} & 0.997\\
% & SAM $\downarrow$ & 0.113 & 0.134 & 0.079 & 0.113 & 0.033& 0.030& \textbf{0.012} & 0.013\\
% & MRAE $\downarrow$ &-&-&-&-& 0.043 & 0.038 & \textbf{ 0.015} & 0.017\\
% \Xcline{1-10}{0.1pt}
% \multirow{5}{*}{ARAD 906} 
% & PSNR $\uparrow$ & 31.17 & 30.94& 35.98 &31.38 &42.88  &43.22 & \textbf{49.23} & 48.60\\
% & SSIM $\downarrow$ & 0.912 & 0.892 & 0.937& 0.922 & 0.981 &  0.986& \textbf{0.996} & 0.995\\
% & SAM $\downarrow$ &0.158 & 0.176 & 0.121 & 0.150 & 0.034  &  0.034& \textbf{0.013} & 0.014\\
% & MRAE  $\downarrow$ & - & - & - & - & 0.043  &0.044 & \textbf{0.017} & 0.018\\
% \Xcline{1-10}{0.1pt}
% \multirow{4}{*}{ARAD 908}  & PSNR $\uparrow$ &  29.27 & 29.52& 36.87 & 29.35 & 44.98& 41.60& \textbf{52.41} & 51.65\\
% & SSIM $\downarrow$ & 0.956 & 0.947& 0.969 &0.960 &0.993 &0.988& \textbf{0.998} & 0.997\\
% & SAM $\downarrow$  & 0.093&   0.089&   0.090&   0.096   &  0.011   &  0.020   &\textbf{ 0.004} & 0.004\\
% & MRAE $\downarrow$ &-&-&-&-& 0.014 &0.022 & \textbf{0.005} & 0.005\\
% \Xcline{1-10}{0.1pt}
% \multirow{4}{*}{ARAD 908}  & PSNR $\uparrow$ &  30.95 & 31.06& 39.10 & 30.99& 41.09&36.92 & \textbf{48.78} & 48.63\\
% & SSIM $\downarrow$ & 0.965 & 0.955& 0.977 &0.967 &0.985 &0.936 & \textbf{0.993} & 0.993\\
% & SAM $\downarrow$ & 0.089 & 0.078 & 0.063 & 0.085 & 0.041 & 0.065& \textbf{ 0.011} & 0.011\\
% & MRAE $\downarrow$ &-&-&-&-& 0.037& 0.071 & \textbf{0.012} & 0.012\\
% \Xcline{1-10}{0.1pt}
% \multirow{4}{*}{ARAD 910}  & PSNR $\uparrow$ &  33.85 & 33.49& 38.81 & 33.96& 43.50& 45.29 & \textbf{51.81} & 50.84\\
% & SSIM $\downarrow$ & 0.943 & 0.929& 0.959 &0.944 &0.984 &0.991 & \textbf{0.997} & 0.997\\
% & SAM $\downarrow$ & 0.113 & 0.134 & 0.079 & 0.113 & 0.033& 0.030& \textbf{0.012} & 0.013\\
% & MRAE $\downarrow$ &-&-&-&-& 0.043 & 0.038 & \textbf{ 0.015} & 0.017\\
% \Xcline{1-10}{0.1pt}
% \multirow{5}{*}{ARAD 911} 
% & PSNR $\uparrow$ & 31.17 & 30.94& 35.98 &31.38 &42.88  &43.22 & \textbf{49.23} & 48.60\\
% & SSIM $\downarrow$ & 0.912 & 0.892 & 0.937& 0.922 & 0.981 &  0.986& \textbf{0.996} & 0.995\\
% & SAM $\downarrow$ &0.158 & 0.176 & 0.121 & 0.150 & 0.034  &  0.034& \textbf{0.013} & 0.014\\
% & MRAE  $\downarrow$ & - & - & - & - & 0.043  &0.044 & \textbf{0.017} & 0.018\\
% \Xcline{1-10}{0.1pt}
% \multirow{4}{*}{ARAD 912}  & PSNR $\uparrow$ &  29.27 & 29.52& 36.87 & 29.35 & 44.98& 41.60& \textbf{52.41} & 51.65\\
% & SSIM $\downarrow$ & 0.956 & 0.947& 0.969 &0.960 &0.993 &0.988& \textbf{0.998} & 0.997\\
% & SAM $\downarrow$  & 0.093&   0.089&   0.090&   0.096   &  0.011   &  0.020   &\textbf{ 0.004} & 0.004\\
% & MRAE $\downarrow$ &-&-&-&-& 0.014 &0.022 & \textbf{0.005} & 0.005\\
% \Xcline{1-10}{0.1pt}
% \multirow{4}{*}{ARAD 913}  & PSNR $\uparrow$ &  30.95 & 31.06& 39.10 & 30.99& 41.09&36.92 & \textbf{48.78} & 48.63\\
% & SSIM $\downarrow$ & 0.965 & 0.955& 0.977 &0.967 &0.985 &0.936 & \textbf{0.993} & 0.993\\
% & SAM $\downarrow$ & 0.089 & 0.078 & 0.063 & 0.085 & 0.041 & 0.065& \textbf{ 0.011} & 0.011\\
% & MRAE $\downarrow$ &-&-&-&-& 0.037& 0.071 & \textbf{0.012} & 0.012\\
% \Xcline{1-10}{0.1pt}
% \multirow{4}{*}{ARAD 914}  & PSNR $\uparrow$ &  33.85 & 33.49& 38.81 & 33.96& 43.50& 45.29 & \textbf{51.81} & 50.84\\
% & SSIM $\downarrow$ & 0.943 & 0.929& 0.959 &0.944 &0.984 &0.991 & \textbf{0.997} & 0.997\\
% & SAM $\downarrow$ & 0.113 & 0.134 & 0.079 & 0.113 & 0.033& 0.030& \textbf{0.012} & 0.013\\
% & MRAE $\downarrow$ &-&-&-&-& 0.043 & 0.038 & \textbf{ 0.015} & 0.017\\
% \Xcline{1-10}{0.1pt}
% \multirow{5}{*}{ARAD 915} 
% & PSNR $\uparrow$ & 31.17 & 30.94& 35.98 &31.38 &42.88  &43.22 & \textbf{49.23} & 48.60\\
% & SSIM $\downarrow$ & 0.912 & 0.892 & 0.937& 0.922 & 0.981 &  0.986& \textbf{0.996} & 0.995\\
% & SAM $\downarrow$ &0.158 & 0.176 & 0.121 & 0.150 & 0.034  &  0.034& \textbf{0.013} & 0.014\\
% & MRAE  $\downarrow$ & - & - & - & - & 0.043  &0.044 & \textbf{0.017} & 0.018\\
% % \Xcline{1-10}{0.1pt}
% % Average & TIME (s)  & 0.11 & 0.12 & 0.75 & 15.18 &  0.014 & 0.015 & 0.054 & 0.029 \\
% \bottomrule[0.05em]
% \end{tabular}}
% % \end{adjustbox}
% \end{table*}






% \begin{figure*}
% \centering
% \includegraphics[width=6in]{figures/ARAD_903_comparison_1.jpg}
% \caption{Visual results of FDM-Net and SOTA methods on NTIRE 2022 spectral demosaicing challenge datasets.}
% \label{fig_ARAD_903}
% \end{figure*}


\begin{figure*}[!htbp]
	% \captionsetup{font=small}
	\centering
	\scriptsize
	\renewcommand{\h}{0.105}
	\renewcommand{\wa}{0.12}
	\newcommand{\wb}{0.16}
	\renewcommand{\g}{-0.7mm}
	\renewcommand{\tabcolsep}{1.8pt}
	\renewcommand{\arraystretch}{1}
	\resizebox{0.85\linewidth}{!} {
		\begin{tabular}{cc}			
			\renewcommand{\name}{figures/arad_905/ARAD_1K_0905_16_}
			\renewcommand{\h}{0.12}
			\renewcommand{\w}{0.2}
			\begin{tabular}{cc}
				% \begin{adjustbox}{valign=t}
				% 	\begin{tabular}{c}%
				%        	\includegraphics[trim={74 0 0 80 },clip, width=0.275\textwidth]{\name mosaic.jpg}
				% 		\\
				% 		ARAD 903: Mosaic 
				% 	\end{tabular}
				% \end{adjustbox}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{cccccc}
						\includegraphics[trim={175 185 55 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name gt.png} \hspace{\g} &
						\includegraphics[trim={175 185 55 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name BTES.jpg} \hspace{\g} &
						\includegraphics[trim={175 185 55 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name WB.jpg} &
						\includegraphics[trim={175 185 55 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name PPID.jpg} \hspace{\g} 
						\\
						GT &
						BTES & WB &
						PPID 
						\\
						\includegraphics[trim={175 185 55 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name GRMR.jpg} \hspace{\g} &
						\includegraphics[trim={175 185 55 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name InNet.png} \hspace{\g} &
						\includegraphics[trim={175 185 55 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name MCAN.png}
						\hspace{\g} &		
						\includegraphics[trim={175 185 55 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name our.png} 
						\\ 
						GRMR  \hspace{\g} &	InNet  \hspace{\g} & MCAN
						&
						\textbf{FDM-Net} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular}	
		\end{tabular}
	}
	\resizebox{0.85\linewidth}{!} {
		\begin{tabular}{cc}			
			\renewcommand{\name}{figures/arad_903/ARAD_1K_0903_16_}
			\renewcommand{\h}{0.12}
			\renewcommand{\w}{0.2}
			\begin{tabular}{cc}
				% \begin{adjustbox}{valign=t}
				% 	\begin{tabular}{c}%
				%        	\includegraphics[trim={74 0 0 80 },clip, width=0.275\textwidth]{\name mosaic.jpg}
				% 		\\
				% 		ARAD 903: Mosaic 
				% 	\end{tabular}
				% \end{adjustbox}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{cccccc}
						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name gt.png} \hspace{\g} &
						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name BTES.jpg} \hspace{\g} &
						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name WB.jpg} &
						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name PPID.jpg} \hspace{\g} 
						\\
						GT &
						BTES & WB &
						PPID 
						\\
						\vspace{-2mm}
						\\
						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name GRMR.jpg} \hspace{\g} &
						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name InNet.png} \hspace{\g} &
						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name MCAN.png}
						\hspace{\g} &		
						\includegraphics[trim={265 45 5 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name our.png} 
						\\ 
						GRMR  \hspace{\g} &	InNet \hspace{\g} & MCAN
						&
						\textbf{FDM-Net} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular}	
		\end{tabular}
	}
	\resizebox{0.85\linewidth}{!} {
		\begin{tabular}{cc}			
			\renewcommand{\name}{figures/arad_911/ARAD_1K_0911_16_}
			\renewcommand{\h}{0.12}
			\renewcommand{\w}{0.2}
			\begin{tabular}{cc}
				% \begin{adjustbox}{valign=t}
				% 	\begin{tabular}{c}%
				%        	\includegraphics[trim={74 0 0 80 },clip, width=0.275\textwidth]{\name mosaic.jpg}
				% 		\\
				% 		ARAD 911: Mosaic 
				% 	\end{tabular}
				% \end{adjustbox}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{cccccc}
						\includegraphics[trim={65 45 205 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name gt.png} \hspace{\g} &
						\includegraphics[trim={65 45 205 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name BTES.jpg} \hspace{\g} &
						\includegraphics[trim={65 45 205 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name WB.jpg} &
						\includegraphics[trim={65 45 205 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name PPID.jpg} \hspace{\g} 
						\\
						GT &
						BTES & WB &
						PPID 
						\\
						\vspace{-2mm}
						\\
						\includegraphics[trim={65 45 205 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name GRMR.jpg} \hspace{\g} &
						\includegraphics[trim={65 45 205 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name InNet.png} \hspace{\g} &
						\includegraphics[trim={65 45 205 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name MCAN.png}
						\hspace{\g} &		
						\includegraphics[trim={65 45 205 235 },clip,height=\h \textwidth, width=\w \textwidth]{\name our.png} 
						\\ 
						GRMR  \hspace{\g} &	InNet \hspace{\g} & MCAN
						&
						\textbf{FDM-Net} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular}	
		\end{tabular}
	}
	\resizebox{0.85\linewidth}{!} {
		\begin{tabular}{cc}			
			\renewcommand{\name}{figures/arad_917/ARAD_1K_0917_16_}
			\renewcommand{\h}{0.12}
			\renewcommand{\w}{0.2}
			\begin{tabular}{cc}
				% \begin{adjustbox}{valign=t}
				% 	\begin{tabular}{c}%
				%        	\includegraphics[trim={74 0 0 80 },clip, width=0.275\textwidth]{\name mosaic.jpg}
				% 		\\
				% 		ARAD 917: Mosaic 
				% 	\end{tabular}
				% \end{adjustbox}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{cccccc}
						\includegraphics[trim={75 200 155 90 },clip,height=\h \textwidth, width=\w \textwidth]{\name gt.png} \hspace{\g} &
						\includegraphics[trim={75 200 155 90 },clip,height=\h \textwidth, width=\w \textwidth]{\name BTES.jpg} \hspace{\g} &
						\includegraphics[trim={75 200 155 90 },clip,height=\h \textwidth, width=\w \textwidth]{\name WB.jpg} &
						\includegraphics[trim={75 200 155 90 },clip,height=\h \textwidth, width=\w \textwidth]{\name PPID.jpg} \hspace{\g} 
						\\
						GT &
						BTES & WB &
						PPID 
						\\
						\vspace{-2mm}
						\\
						\includegraphics[trim={75 200 155 90 },clip,height=\h \textwidth, width=\w \textwidth]{\name GRMR.jpg} \hspace{\g} &
						\includegraphics[trim={75 200 155 90 },clip,height=\h \textwidth, width=\w \textwidth]{\name InNet.png} \hspace{\g} &
						\includegraphics[trim={75 200 155 90 },clip,height=\h \textwidth, width=\w \textwidth]{\name MCAN.png}
						\hspace{\g} &		
						\includegraphics[trim={75 200 155 90 },clip,height=\h \textwidth, width=\w \textwidth]{\name our.png} 
						\\ 
						GRMR  \hspace{\g} &	InNet  \hspace{\g} & MCAN
						&
						\textbf{FDM-Net} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular}	
		\end{tabular}
	}
	\resizebox{0.85\linewidth}{!} {
		\begin{tabular}{cc}			
			\renewcommand{\name}{figures/arad_940/ARAD_1K_0940_16_}
			\renewcommand{\h}{0.12}
			\renewcommand{\w}{0.2}
			\begin{tabular}{cc}
				% \begin{adjustbox}{valign=t}
				% 	\begin{tabular}{c}%
				%        	\includegraphics[trim={74 0 0 80 },clip, width=0.275\textwidth]{\name mosaic.jpg}
				% 		\\
				% 		ARAD 940: Mosaic 
				% 	\end{tabular}
				% \end{adjustbox}
				\begin{adjustbox}{valign=t}
					\begin{tabular}{cccccc}
						\includegraphics[trim={105 185 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name gt.png} \hspace{\g} &
						\includegraphics[trim={105 185 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name BTES.jpg} \hspace{\g} &
						\includegraphics[trim={105 185 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name WB.jpg} &
						\includegraphics[trim={105 185 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name PPID.jpg} \hspace{\g} 
						\\
						GT &
						BTES & WB &
						PPID 
						\\
						\vspace{-2mm}
						\\
						\includegraphics[trim={105 185 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name GRMR.jpg} \hspace{\g} &
						\includegraphics[trim={105 185 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name InNet.png} \hspace{\g} &
						\includegraphics[trim={105 185 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name MCAN.png}
						\hspace{\g} &		
						\includegraphics[trim={105 185 105 75 },clip,height=\h \textwidth, width=\w \textwidth]{\name our.png} 
						\\ 
						GRMR  \hspace{\g} &	InNet  \hspace{\g} & MCAN
						&
						\textbf{FDM-Net} (ours)
						\\
					\end{tabular}
				\end{adjustbox}
			\end{tabular}	
		\end{tabular}
	}
	\caption{Visual comparison of \textbf{HSI demosaicing} methods (False color, R: 2, G: 11, B:16).} %
	\label{fig_ARAD_903}
\end{figure*}


\newpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{reference}
}

\end{document}