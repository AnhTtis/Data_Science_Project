\section{Introduction}

GK: THE INTRODUCTION IS OFF THE MARK. THE FRST PART IS NOT NEEDED.
TALK ABOUT THE HISTORY OF NEURAL OPERATORS and GIVE REFERENCES. THEN WHY NEURAL OPERATORS AND WHAT IS MISSING OR CAN BE IMPROVED. THEN, WHAT IS THE PURPOSE OF THIS PAPER. Geneal blah blas are useless, go directly to the point!
MERGE SECTIONS 1 and 2.
\panos{I'll work on merging 1 and 2 and condensing}

The field of scientific machine learnig (SciML) has been growing rapidly in recent years. Many problems in physics \cite{}, biology \cite{}, chemistry \cite{}, geophysics \cite{}, etc., can be formulated as data driven problems. The accelerated research of artificial intelligence has produced many new techniques for achieving highly accurate classification and regression models. Many of these state-of-the-art (SOTA) methods can be applied for scientific computations. Authors study innovative ways to incorporate the scientific knowledge of the process (mathematical formulation of physics processes, models of biological components, chemical reaction, etc.), into a machine learning algorithm that trains on observations (input and output data samples). By combining the two methodologies, more accurate results are achieved. Another methodology is to completely replace the scientific models with learning machines, that can operate solely on data and approximate the solution of the scientific problem. In this paper we propose such a method. \panos{This sounds a bit contradicting with the previous statement that people are trying to combine scientific models with data. It's as if we are throwing away scientific models when we have said that they are a good thing. I think we should rephrase.}

When modeling a physical experiment we usually find one of two scenarios interesting. The first scenario is the \textit{forward problem}, when given a set of conditions, one attempts to simulate the physical process. For example, given an initial source, find the acoustic wave amplitude in the ocean after some time. The second scenario is the opposite one, called the \textit{inverse problem}. Given the state of the physical experiment, find the causal condition that led to that state. For example, given measurements of the acoustic pressures in the ocean at some time instant, find the source that started emitting the acoustic sounds. The inverse problem is often considered more challenging since one has access to limited data (for example, recordings at a small set of sensors only), the data may be noisy, some recordings may be missing, etc. Inverse problems are often ill-posed, meaning they do not necessarily have a solution and if so, it may not necessarily be unique. In this paper we focus on inverse problems. The main applications involve recovering the original state of a physical system given partial information from future states. There are many different physical experiments that can be mathematically formulated as inverse problems, such as inverse scattering \cite{}, source refocusing \cite{}, material identification \cite{}, non-destructive testing \cite{}, and many more which serve as motivation for this work.

Most latest advances of artificial intelligence involve deep neural networks \cite{}. Due to their ability to fit non-linear problems with high accuracy and generalize, as well as the availability of diverse architectures to fit a variety of tasks, they have been used in many applications. One drawback is the training time and resources it takes to train the networks, hence more efficient architectures are highly sought after. A recent innovation in the field are the so-called transformers \cite{}, which refer to deep neural networks that have an attention mechanism \cite{}. The attention module attempts to understand context from the given input. To learn the context, the attention mechanism operates on a discrete embedding of the data that is composed of tokens. An immediate example is Natural Language Programming (NLP) using transformers, where a sentence is embedded using tokens according to a specific vocabulary \cite{}.

A challenge that arises when using transformers is considering non-discrete data. For example, instead of sentences (sequence of words), we would like to embed a continuous function or signal. The literature offers methods to achieve that, as introduced in the following section. One prominent method, explored in this work, is called Vision Transformers (ViT) \cite{}. The vision transformers receive an image as input, for example the initial condition of a system (in the forward problem), or a future state of a system (in the inverse problem). Then, the image is split into small regions, and each region acts as a token. The vision transformer extracts the context from the tokens (regions of the image), and thus is able to utilize the attention mechanism for the continuous signal and make accurate predictions.

Another key component of this work is operator learning. Machine learning models aim to find a function that connects input data to output data. Operators train to represent mappings from a family of functions to another family of functions. For example, if one wants to infer the acoustic wave pressure in the ocean for each and every initial source, they can define the operator as the mapping from the initial source (a function, initial pressure in every point of the domain) to the pressures at a later time (also a function, future pressures in every point of the domain). The main advantage of operator learning is that, after the operator has been learned, no further training is needed and the solution e.g. for a partial differential equation (PDE), which can be very expensive using classic methods, can be simply inferred (estimated) by the network with low computational cost.

In this paper we combine vision transformers with operator learning to solve a variety of inverse problems. We propose a U-net \cite{} based architecture, infused with a ViT, to do operator learning. The operators in this case solve inverse problems that are of high interest in the scientific community. We show that the results using the proposed method exceed expectations, beating the leading operator network benchmarks in terms of accuracy. In addition, the transformers require much less data and the architecture is lighter (less parameters to train), achieving up to 5.5 times faster computation time.
