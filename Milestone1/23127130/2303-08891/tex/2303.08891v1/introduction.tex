\section{Introduction}

Operator learning refers to training neural networks to represent mappings between families of functions. For example, if we want to infer the acoustic wave pressure in the ocean for each and every initial source, we can define the operator as the mapping from the initial source (a function, initial pressure in every point of the domain) to the pressures at a later time (also a function, future pressures in every point of the domain). The main advantage of operator learning is that, after the operator has been learned, no further training is needed and the solution, e.g., for a partial differential equation (PDE), which can be very expensive using classic methods, can be simply inferred (estimated) by the network with negligible computational cost in real-time. The first operator learning method was introduced by Lu \textit{et al.}, named the Deep Operator Network (DeepONet) \cite{deeponet}. DeepONet is composed of a branch and a trunk; the branch learns the input function space of the operator, while the trunk learns the space of functions onto which the output is projected. By multiplying the branch with the trunk, the projection provides a representation of the output function space. Another popular invention is the Fourier Neural Operator \cite{FNO, FNO2}, which is based on replacing the kernel integral operator with a convolution operator defined in Fourier space by employing a fast Fourier transform on the input space. It uses a ResNet but does not have a trunk net.

When modeling a physical system/experiment we usually find one of two scenarios. The first scenario is the \textit{forward problem}, when given a set of conditions, one attempts to simulate the physical process. For example, given an initial source, find the acoustic wave amplitude in the ocean after some time. The second scenario is the opposite one, called the \textit{inverse problem}. Given the state of the physical experiment, find the causal condition that led to that state. For example, given measurements of the acoustic pressures in the ocean at some time instant, find the source that started emitting the acoustic sounds. The inverse problem is often considered more challenging since one has access to limited data (for example, recordings at a small set of sensors only), the data may be noisy and of low resolution, some recordings may be missing, etc. Inverse problems are often ill-posed, meaning they do not necessarily have a solution and if so, it may not necessarily be unique. In this work, we focus on inverse problems given {\em relatively sparse} data sets. 

% The main applications involve recovering the original state of a physical system given partial information from future states. There are many different physical experiments that can be mathematically formulated as inverse problems, such as inverse scattering \cite{}, source refocusing \cite{}, material identification \cite{}, non-destructive testing \cite{}, and many more which serve as motivation for this work.




A recent innovation in the field of deep learning is the so-called transformers \cite{transformer}, which refer to deep neural networks that have an attention mechanism. The attention module attempts to understand context from the given input. To learn the context, the attention mechanism operates on a discrete embedding of the data that is composed of tokens. An immediate example is Natural Language Processing (NLP) using transformers, where a sentence is embedded using tokens according to a specific vocabulary. A challenge that arises when using transformers is considering non-discrete data. For example, instead of sentences (sequence of words), we would like to embed a continuous function or signal. The literature offers methods to achieve that, as introduced in the following section. One prominent method, explored in the current work, is called Vision Transformers (ViT) \cite{vit}. The vision transformers receive an image as input, for example the initial condition of a system (in the forward problem), or a future state of a system (in the inverse problem). Then, the image is split into small regions, often referred to as patches, and each region acts as a token. The vision transformer extracts the context from the tokens (regions of the image), and thus is able to utilize the attention mechanism for the continuous signal and make accurate predictions. In the original ViT paper \cite{vit}, the authors demonstrated how ViT outperforms the state-of-the-art (SOTA) methods for image classification, including ImageNet \cite{imagenet}, CIFAR \cite{cifar}, Oxford pets and flowers \cite{oxford_pets}, and VTAB \cite{VTAB}. In addition, the benchmark ViT model is up to four times more efficient than the SOTA methods used as reference. In \cite{okolo2022ievit}, Okolo \textit{et. al.} used a ViT for X-ray image classification, while in \cite{chen2021vit} it was used for unsupervised volumetric medical image registration. ViTs are  currently being adopted in various areas of research.

In the current work, we are interested in using transformers for operator learning. There have been some recent attempts to use various types of transformers for operator learning \cite{transformerPDE1, transformerPDE2, transformerPDE3, transformer_cao_choose, transformer_cao2}. For example, in \cite{transformerPDE1}, Li \textit{et. al.} use transformers to approximate \emph{forward} solutions of PDEs with operator learning. They propose an innovative way for choosing the collocation points and iterate through time to find the solution at those points. The attention mechanism is split in such a way that the query, key and value are split into different parts of the forward pass and is implemented as a multi-layer perceptron (MLP).  The latent encoding, which is the outcome of the combination of the three components, is the embedding of the coordinates used for the spatio-temporal input of the PDE. In \cite{transformer_cao_choose}, Cao presents a method to combine FNO with attention to improve the performance for PDE solutions, by replacing the softmax (often used in transformers especially in classification) with a linear variant of it that does not involve normalization.

Here, we introduce a novel way to perform operator learning using vision transformers combined with a U-net \cite{ronneberger2015u, chen2021transunet} based architecture to design the Vision Transformer-Operator or ViTO. We apply ViTO to solve inverse problems of increasing complexity, obtaining the solution at high resolution using only sparse and low resolution data. Compared to SOTA results for operator learning, our current results exceed the leading operator network benchmarks in terms of accuracy, and they are also obtained at a significant speedup.

The paper is organized as follows. Section \ref{method} presents the proposed methodology. Section \ref{results} presents numerical results for a collection of inverse problems of increasing complexity. Section \ref{discussion} offers a discussion of the results and directions for future work.