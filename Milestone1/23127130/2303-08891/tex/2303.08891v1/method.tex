\section{Methodology}\label{method}

\subsection{Mathematical formulation of operator learning}

We first present the general problem formulation of operator learning for PDEs before focusing on the particular case of inverse problems. We follow the DeepONet theory and notation, as given by \cite{deeponet, deeponet-fno}.

\subsubsection{PDE operators}\label{PDE_operators}
Typically, when tackling a forward PDE problem, our objective is to determine the solution to the PDE. Thus, we aim to approximate the PDE solution by utilizing a set of parameters that describe the PDE problem setup. Such parameters include initial and boundary conditions, forcing terms, and other physical characteristics that may vary between different PDEs. Hence, forward PDE problems can be formulated as a mapping between an input function that corresponds to these parameters to an output function representing the solution. 

Mathematically, let $v$ denote the input function defined on some physical domain $D \in \mathbb{R}^d$ and $u$ denote the corresponding output function defined on the physical domain $D' \in \mathbb{R}^{d'}$:
 \begin{align*}
     v: D \ni x \longmapsto v(x) \in \mathbb{R} , \\
     u: D' \ni \xi \longmapsto u(\xi) \in \mathbb{R} .
 \end{align*}

Let $\mathcal{V}$ and $\mathcal{U}$ be the spaces of the functions $v$ and $u$, respectively. Then, the mapping from $v$ to $u$ is defined by an operator $\mathcal{G}$:

\begin{equation*}
     \mathcal{G}: \mathcal{V} \ni v \longmapsto u \in \mathcal{U} .
\end{equation*}

This operator describes the forward problem. For example, in many applications $v$ is the initial condition of the PDE and $u$ is its solution at some final time. However, in this work we are interested in the inverse problem. So, the operator corresponding to the inverse problem is of the form:

 \begin{equation}\label{inverse_operator}
     \Tilde{\mathcal{G}}: \mathcal{U} \ni u \longmapsto v \in \mathcal{V} .
\end{equation}

Continuing the previous example, the relevant inverse problem would be to retrieve the initial condition of the PDE given a snapshot of its solution.

We note that in many cases, the inverse operator relates to an ill-posed problem \cite{hadamard1902problemes}. This type of problem is generally considered more challenging, particularly when dealing with incomplete or noisy data.

\subsubsection{Super-resolution}\label{SR}

For most applications, it is impossible to get the full analytical solution of a PDE. In some cases, it is even hard to get a discrete approximation of the solution on a fine mesh due to computational, physical, or experimental difficulties. This is especially common in the domain of inverse problems, where the input function is often derived from sensor measurements of physical phenomena. These considerations often lead to a low-resolution mesh for the discrete approximation.

Low-resolution data is challenging to use. The main goal of Super-Resolution (SR) methods is to produce high-resolution accurate results given low-resolution input data. In this work, we do not treat the SR aspect as a separate problem. Instead, we combine it with the inverse operator defined in the previous section \ref{PDE_operators} to form a unified inverse-SR framework. Using the same notation as before, instead of getting functions $u \in \mathcal{U}, v \in \mathcal{V}$, we get discrete approximations of these functions on meshes. However, $u \in \mathcal{U}$ is discretized using a much coarser mesh in comparison to $v \in \mathcal{V}$. For example, the input might be a low-resolution snapshot of a PDE solution, while the desired output would be a high resolution discretization of the initial condition.

\subsection{Data driven formulation of operator learning}\label{data_driven}

The goal is to approximate the operator $\Tilde{\mathcal{G}}$ in \eqref{inverse_operator}, when the input is in low-resolution and the output is in high-resolution, using a ViT-based neural network. We define a dataset, where each sample is composed of pairs of discretized functions: 
$\mathcal{T} = \{(\mathrm{u}^{(1)}, \mathrm{v}^{(1)}), (\mathrm{u}^{(2)}, \mathrm{v}^{(2)}), \hdots, (\mathrm{u}^{(N)}, \mathrm{v}^{(N)})\}$ such that $\forall n,  \mathrm{u}^{(n)}$ and $\mathrm{v}^{(n)}$ are the projections of functions  ${u} ^{(n)} \in \mathcal{U}$ and ${v} ^{(n)}\in \mathcal{V}$ onto discrete meshes $\mathcal{M}_{u}$ and $\mathcal{M}_{v}$, respectively. We let $\mathcal{M}_{u}$ and $\mathcal{M}_{v}$ remain constant for all samples, and assume that the discretization is equispaced. For ease of notation and without loss of generality, we assume that the domain coordinates are positive. Then, in the two-dimensional case, which is used for all of the numerical experiments, these meshes are written as:

\begin{align*}
    \mathcal{M}_{u} = \{ (i \Delta_{x, u}, \: \: j \Delta_{y, u}) \: \: | \: \: i,j \in \mathbb{N}, i \leq N_{x, u}, j \leq N_{y, u}  \} \\ 
    \mathcal{M}_{v} = \{ (i \Delta_{x, v}, \: \: j \Delta_{y, v}) \: \: | \: \: i,j \in \mathbb{N}, i \leq N_{x, v}, j \leq N_{y, v}  \},
\end{align*}

where $\Delta_{x, \cdot}, \Delta_{y, \cdot}, N_{x, \cdot}, N_{y, \cdot}$ determine the resolution of the discretization. We use $\# \mathcal{M}_{u} = N_{x, u} \cdot N_{y, u}$ to note the number of points in the set $\mathcal{M}_{u}$.

If $\mathcal{M}_{u}$ and $\mathcal{M}_{v}$ have the same number of points ($\# \mathcal{M}_{u} = \# \mathcal{M}_{v}$.), SR is not performed, and instead, we are describing a standard inverse problem. However, as highlighted in the previous section \ref{SR}, the focus is on scenarios where the input grid is significantly coarser than the output grid, i.e., $\# \mathcal{M}_{u} \ll \# \mathcal{M}_{v}$. To achieve this, we can choose a super-resolution factor $s > 1$ that specifies the relationship between the discretization. We choose $N_{\cdot, v}$ such that $N_{x, v} = s N_{x, u}$ and $ N_{y, v} = s N_{y, u}$ to attain a SR factor of $s$. This results in the output grid size becoming $\# \mathcal{M}_{v} = s^2 N_{x, u} N_{y, u}$. For large values of $s$, this renders the problem considerably more challenging.

\subsection{Network architecture}\label{network_arch}

A modified version of the TransUNet \cite{chen2021transunet} architecture is employed in this study, wherein a U-Net \cite{ronneberger2015u} backbone is integrated with a ViT (see Figure \ref{fig:architecture}). The U-Net model comprises an encoder-decoder structure with interconnecting skip connections. The U-Net architecture has emerged as a powerful technique in the computer vision field, particularly in the realm of segmentation problems. Given the image-to-image nature of the data-driven problem outlined in \ref{data_driven}, utilizing segmentation tools is a natural choice.

The network has two inputs: the observed solution of the PDE $u$, and its corresponding numerical grid $\mathcal{M}_{u}$. In the two-dimensional case, $u$ is represented by a two-dimensional matrix, where all elements are values of the $u$ at points on the grid  $\mathcal{M}_{u}$. We want the network to be exposed to the grid itself, so it can learn some relation between the $(x,y)$ values of a grid point and their corresponding solution value $u(x,y)$. We achieve this by a simple encoding of the grid using two matrices representing discretizations in the $x$ and $y$ directions, as seen in Figure \ref{fig:grids}. The $i$-th row of the $x$-matrix is defined as a vector consisting of $x_i = i \Delta_{x, u}$ repeated $N_{y, u}$ times. Similarly, the $j$-th column of the $y$-matrix is defined as a column vector consisting of $y_j = j \Delta_{y, u}$ repeated $N_{x, u}$ times. We also use bilinear interpolation to modify the sizes of the inputs to the desired output shape. We make sure that the results of the interpolation operation is divisible by $16$, so it could be compatible with the downsampling of the U-Net.

The U-Net architecture we employ is composed of three convolutional blocks for both the encoder and decoder (see Figure \ref{fig:architecture}). Each block is comprised of three convolutional layers, equipped with a residual skip connection \cite{ResNet}. All convolutions are followed by a batch normalization \cite{batch_norm} layer and a GELU activation function \cite{hendrycks2016gaussian}. The final layer of each block performs either a downsampling operation for the encoder or an upsampling operation for the decoder.

In our approach, we utilize a ViT within the latent space of the U-Net by taking the encoded values as input. As the encoding is significantly smaller than the original inputs due to the U-Net's downsampling nature, we can use a patch size of $1 \times 1$ without any computational difficulties.

The original ViT utilizes absolute positional embedding via a linear projection layer. However, this can be problematic in PDE applications as it requires all inputs to have the same shape. In operator learning, we are often interested in models that can handle inputs of various sizes. To address this issue, we employ a form of relative conditional embedding \cite{shaw2018self, wu2021rethinking}, where we use convolutions to learn the relationships between tokens instead of linearly projecting their absolute positions within the representation in the latent space. Specifically, we use a separable convolutional layer \cite{chollet2017xception} followed by a standard convolutional layer, similar to the approach proposed in \cite{chu2021conditional}. 

\begin{figure}[htb]
\includegraphics[scale=0.45]{media/architecture.jpg}
\centering
\caption{The architecture of the ViTO deep neural network. The inputs are the discretized function $u(x,y)$ and the grid points, concatenated and inserted into U-net convolutional blocks. In the lowest level of the U-net the ViT is employed.}
\label{fig:architecture}
\end{figure}

\begin{figure}[htb]
\includegraphics[scale=0.45]{media/grids.jpg}
\centering
\caption{An example of discrete $X, Y$ grid encoded inputs for a problem defined in $[0, 1]^2$.}\label{fig:grids}
\end{figure}

\subsection{Training loss function}

The loss function is defined as the mean relative $\textit{L}^2$ error:

\begin{align}
    \label{eqn:loss}
    \mathcal{L}=\frac{1}{N} \sum_{j=1}^N\frac{||\mathrm{\hat{v}}^{(j)}-\mathrm{v}^{(j)}||_2}{\varepsilon+||\mathrm{v}^{(j)}||_2},
\end{align}

% \begin{align*}
%     \label{eqn:loss}
%     \mathcal{L}=\frac{1}{N} \sum_{j=1}^N\frac{(\mathrm{\hat{v}}^{(j)}-\mathrm{v}^{(j)})^2}{\varepsilon+|\mathrm{v}^{(j)}|^2},
% \end{align*}
where $N$ is the size of training data, $\mathrm{v}^{(j)}$ is the $j$-th ground-truth sample of the training data, $\mathrm{\hat{v}}^{(j)}$ is the $j$-th sample prediction, and $\varepsilon$ is a small number to  prevent a zero denominator and stabilize the loss. Note that the inputs and outputs of the model are two-dimensional, so they are flattened inside the loss function. 