\section{Numerical results}\label{results}

We apply the ViTO method on various ill-posed two-dimensional inverse problems. The conducted tests include three PDEs: the acoustic wave equation, time-dependent incompressible Navier-Stokes, and a steady-state Darcy flow equation. In all cases we compare the results of the ViTO to three other popular methods in the scientific machine learning literature: 1) DeepONet \cite{deeponet}, 2) FNO \cite{FNO}, and 3) a standard ResNet \cite{ResNet}. For each method we compute the relative $\textit{L}^2$ error compared to the ground truth data.  We also measure relevant information regarding the training time and the number of parameters of used by each method. Details regarding the data generation process can be found in each of the following sections.

An important consideration in the domain of inverse problems is robustness to noise. Since inverse problems are often ill-posed, even a small amount of noise in the observed data can greatly amplify the numerical error \cite{ill_posed}. To test how well ViTO can handle noise, we run all experiments twice: with noise and without noise. In both cases we train the model with the relevant amount of noise. We used zero-mean Gaussian additive noise, which is a  common choice. Since different PDEs can behave quite differently, we make sure that the variance of the Gaussian noise is dependent on the input data. The operation of adding noise is given by:
$\mathrm{D} \ni x^{(n)} \longmapsto x^{(n)} + \gamma \mathcal{N}(0, \sigma^2_{\mathrm{D}})$
where $x^{(n)}$ is an input sample in the dataset $\mathrm{D}$, $\sigma^2_{\mathrm{D}}$ is the variance of the entire dataset, and $\gamma$ is the desired noise level, e.g. $\gamma = 0.1$ is equivalent to $10 \%$ noise.

For all our evaluations, we utilized a super-resolution scale factor of 8, which means, for example, that an image of size $16 \times 16$ would be mapped to an image of size $128 \times 128$. It should be noted that a magnification of $\times 8$ is considerably high. Although lower SR factors are utilized in several benchmarking datasets, the choice of $\times 8$ is still prevalent for certain datasets such as Urban100 \cite{urban100} and the Berkeley Segmentation Dataset \cite{BSD}.

For the FNO we used the same architecture as described in \cite{FNO} for the FNO-2D network: four Fourier layers with width of 32 and 12 modes. Each layer was followed by a GeLU \cite{hendrycks2016gaussian} activation function. For the ResNet we used 3 residual blocks with 16, 32, and 64 filters for each convolution within each block of depth 3. In the DeepONet case we used the ResNet described before as the branch network, 4 hidden fully connected layers for the trunk network, and 256 neurons for the latent dimension.

Following standard machine learning practice, we split all datasets into train, test, and validation sets. During training, we monitor the relative $\textit{L}^2$ losses and save the model with the lowest validation loss. Unless stated otherwise, all models are trained with a batch size of 100 for 500 epochs, subject to an early stopping criterion of 50 consecutive epochs with no validation loss improvement. The optimizer of choice is the Adam/AdamW optimizer \cite{kingma2014adam, adamw} with an initial learning rate $10^{-3}$ and weight decay $10^{-4}$. The learning rate is updated throughout the training process using cosine annealing \cite{loshchilov2016sgdr}. The main code was implemented in PyTorch \cite{pytorch}, and the DeepONet was implemented using DeepXDE \cite{deepxde} with a PyTorch backend. All computations were conducted using a single RTX-4090 GPU. 




\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Method       & $\#$ of parameters (M) & Memory (GB) & Iterations per second \\ \hline \hline
FNO          &  2.376             & 12.74           &  3.45              \\ \hline
DeepONet     &  0.297             &  -              &  -                 \\ \hline
ResNet       &  \textbf{0.148}             & 4.66            &  9.25               \\ \hline
ViTO         &  0.150             & \textbf{0.85}            &  \textbf{59.98}             \\ \hline
\end{tabular}
\caption{Computational performance of the different models.}
\label{details}
\end{table}

In Table \ref{details} we present a computational comparison of the four models mentioned above. These results are given for a Darcy problem (see \ref{darcy_problem}) experiment with a grid size of $128 \times 128$ and batch size 100. Memory was calculated as the peak GPU memory usage from the beginning of the training process to its end. The iterations per second metric was calculated by measuring the time it took the model to train for a single batch, averaged over 200 batches to increase consistency. Note that we do not report these two metrics for the DeepONet, since the training process was quite different from the other three models, so any direct comparison would have been misleading.

ViTO was the most efficient model by a substantial margin, both in terms of memory consumption and training time. ViTO was able to produce results on-par with SOTA methods, using a surprisingly small number of trainable parameters. The full details of the ViT-related weights in ViTO are shown in Table \ref{Vit_architecture}. Despite having a similar number of parameters to the ResNet, ViTO employs downsampling due to its U-Net architecture. Consequently, many convolutional operations occur on smaller feature maps compared to the ResNet which explains the lower memory usage and running time for ViTO. 

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Problem       & Transformer blocks & Attention heads & Embedding dimension & ViT MLP size \\ \hline \hline
Wave equation &   2                &    2            &      16             &      128      \\ \hline
Navier-Stokes &   4                &    8            &      16             &      64      \\ \hline
Darcy Flow    &   2                &    2            &      16             &      128     \\ \hline
\end{tabular}
\caption{ViT parameters for each scenario.}
\label{Vit_architecture}
\end{table}

\subsection{Wave equation}\label{wave_problem}

The formulation of the acoustic wave equation in two dimensions is given by \cite{evans2022partial, jost2012partial}:

\begin{equation}
	\begin{cases}
		\ddot{u}(x, y, t)=c^2(x,y) (u_{xx}(x, y, t) + u_{yy}(x, y, t)) + f(x, y, t) & (x,y) \in (0,L)^2; 0\leq t \leq T, \\
		u(x, y, 0)=u_0(x, y) & (x,y) \in (0,L)^2, \\
		\dot{u}(x, y, 0)=v_0(x,y) & (x,y) \in (0,L)^2, \\
		u(0, y, t)=u(L, y, t)=0 & y \in (0,L),\; 0\leq t \leq T, \\
		u(x, 0, t)=u(x, L, t)=0 & x \in (0,L),\; 0\leq t \leq T. \\
	\end{cases}
\end{equation}

where $u(x,y,t)$ is the wave amplitude or acoustic pressure, $c(x,y)$ is the wave propagation speed, $f(x,y,t)$ is the source term, $T$ is the final propagation time, $L$ is the size of the physical domain, and $u_0(x, y), v_0(x, y)$ are the initial pressures and velocities, respectively. The boundary condition is a homogeneous Dirichlet boundary (fully-reflective). The inverse problem is to learn the following mapping:

\begin{equation*}
    u(x,y,T) \longmapsto u_0(x,y).
\end{equation*}
 
We chose a physical domain with $L = \pi$ and propagation time $T = 0.001$. We set the initial pressure and velocity to be 0, and randomly created Gaussian-shaped sources at different locations. For each sample, we created two such Gaussian sources with random amplitudes and locations. The locations are selected using a discrete random uniform distribution on the indices of the grid. The amplitudes are sampled uniformly using $\mathcal{U}(-1, 1)$ for each source in each initial condition. The wave velocity was taken as $c(x,y) = c_0 sin(x) sin(y)$, where $c_0$ was randomly sampled for each initial condition using a uniform distribution set between 1,300 and 1,600, which is centred around the average acoustic wave propagation speed of 1,484 in the Mediterranean sea.  

The dataset was generated using a standard explicit second-order finite-difference scheme \cite{hyper1, hyper2}. We generated 20,000 samples, of which 16,000 were used as the training set, while the remaining 4,000 were evenly split to form the testing and validation sets.

The results are shown in Table \ref{tab:wave_results} and Figure \ref{fig:wave}. ViTO obtains the lowest error compared to the other methods, both with and without noise. It is worth noting that ViTO is able to reconstruct the initial condition even in difficult scenarios where there is a large difference between the amplitudes of the different sources (such as the first row in Figure \ref{fig:wave}).  

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Method       & $0 \%$ noise       & $10 \%$ noise   \\ \hline \hline
FNO          & 0.3260             & 0.4383          \\ \hline
DeepONet     & 0.7128             & 0.7131          \\ \hline
ResNet       & 0.7892             & 0.8154          \\ \hline
ViTO         & \textbf{0.2678}             & \textbf{0.2942}         \\ \hline
\end{tabular}
\caption{Test relative $\textit{L}^2$ errors for the wave problem.}
\label{tab:wave_results}
\end{table}

\begin{figure}[htb]
\includegraphics[scale=0.35]{media/comparison_wave.jpg}
\centering
\caption{Predictions for the wave problem \ref{wave_problem} for 5 random samples. In the section labeled "Data" is the propagated wave at the final time using fine and coarse discretizations, alongside a high-resolution image of the ground truth sources. In the section labeled "Predictions" are the initial condition reconstructions by the various methods.}
\label{fig:wave}
\end{figure}

\subsection{Navier-Stokes equations}\label{ns_problem}

The time-dependent two-dimensional Navier-Stokes equation for the viscous, incompressible fluid in vorticity form is given by:

\begin{equation}\label{NS_equation}
\begin{cases}
    \partial_t \omega(x,y,t) + u(x,y,t) \cdot \nabla  \omega(x,y,t) = \nu \Delta \omega(x,y,t) + f(x,y), & x,y \in (0,1)^2, t \in (0, T] \\
    \nabla \cdot u(x,y,t) = 0, & (x,y) \in (0,1)^2, t \in (0, T] \\
    \omega(x,y,0) = \omega_0, & (x,y) \in (0,1)^2 \\
\end{cases}
\end{equation}
where $\omega$ is the vorticity, $u$ is the velocity field,  $\nu = 10^{-3}$ is the viscosity, and $\Delta$ is the two-dimensional Laplacian. We consider periodic boundary conditions. The source term $f$ is set as: $f(x,y) = 0.1 (sin(2\pi (x + y)) + cos(2\pi(x + y)))$, and the initial condition $\omega_0(x)$ is sampled from a Gaussian random field according to the following distribution: $\mathcal{N}(0, 7^{3/2} (-\Delta + 49 I)^{-5/2})$. The inverse problem is to learn the following mapping:

\begin{equation*}
    \omega(x,y,T) \longmapsto \omega_0(x,y).
\end{equation*}

 We used the publicly available Python solver given in \cite{FNO} to create two separate datasets with different final simulation times $T=1$ and $T=5$. Each dataset was composed of 10,000 samples, which we then split into train, test, and validation. The error analysis for $T=1,5$ are shown in Table \ref{tab:ns}. ViTO and FNO obtain very similar accuracy in both cases,  ViTO is slightly more accurate without noise, while FNO has a minor advantage with noise. 
 
 A visualization of the results is shown in Figure \ref{fig:ns_results_1} and Figure \ref{fig:ns_results_5}. In the case $T=1$ (Figure \ref{fig:ns_results_1}), the vorticities for different initial conditions are still very different from one another, and so the reconstructions are able to capture fine details. However, for $T=5$ (Figure \ref{fig:ns_results_5}), the behavior of the vorticity becomes very similar, regardless of the choice of initial condition. In that case, some fine details are lost in all reconstructions, which explains the larger error compared to the $T=1$ case. 
  


\begin{table}[ht]\caption{Test relative $\textit{L}^2$ errors for the errors for the Navier-Stokes problem with different final simulation times ($T$) and noise levels ($\gamma$).}
\centering 
\begin{tabular}{ccccccc} %  
\toprule
\multirow{2}{*}{
\parbox[c]{.2\linewidth}{}}
  & \multicolumn{2}{c}{$T=1$} &&
\multicolumn{2}{c}{$T=5$} \\ 
\cmidrule{2-3} \cmidrule{5-6}

 & {\centering $\gamma = 0$} & {$\gamma = 0.1$} && {$\gamma = 0$} & {$\gamma = 0.1$} \\
\midrule
FNO      & 0.06449 & \textbf{0.1587} &&   0.1881  & \textbf{0.3582} \\
DeepONet & 0.09424 & 0.1684 &&   0.2007  & 0.4528 \\
ResNet   & 0.1271 & 0.4471 &&   0.4520  & 0.5745 \\
ViTO     & \textbf{0.06348} & 0.1635 &&   \textbf{0.1757} & 0.3757 \\
\bottomrule
\end{tabular}
\label{tab:ns}
\end{table}

% \begin{table}[!ht]
% \centering
% \begin{tabular}{|c|c|c|c|}
% \hline
% Method       & No noise           & $10 \%$ noise   \\ \hline \hline
% FNO          &   0.06449          &  \textbf{0.1587}         \\ \hline
% DeepONet     &   0.09424          &  0.1684         \\ \hline
% ResNet       &   0.1271           &  0.4471         \\ \hline
% ViTO         &   \textbf{0.06348}          &  0.1635         \\ \hline
% \end{tabular}
% \caption{Test relative $\textit{L}^2$ errors for the Navier-Stokes problem with $T=1$.}
% \label{tab:ns_results_1}
% \end{table}

\begin{figure}[htb]
\includegraphics[scale=0.35]{media/comparison_navier_stokes_T_1.jpg}
\centering
\caption{Predictions for the Navier-Stokes problem \ref{ns_problem} with $T=1$ for 5 random samples. In the section labeled "Data" is the vorticity at the final time using fine and coarse discretizations, alongside a high-resolution image of the initial vorticity. In the section labeled "Predictions" are the initial condition reconstructions by the various methods.}
\label{fig:ns_results_1}
\end{figure}

% \begin{table}[!ht]
% \centering
% \begin{tabular}{|c|c|c|c|}
% \hline
% Method       & No noise           & $10 \%$ noise   \\ \hline \hline
% FNO          &   0.1881           & \textbf{0.3582}          \\ \hline
% DeepONet     &   0.2007           & 0.4528          \\ \hline
% ResNet       &   0.4520           & 0.5745          \\ \hline
% ViTO         &   \textbf{0.1757 }          & 0.3757          \\ \hline
% \end{tabular}
% \caption{Test relative $\textit{L}^2$ errors for the Navier-Stokes problem with $T=5$.}
% \label{tab:ns_results_5}
% \end{table}


\begin{figure}[htb]
\includegraphics[scale=0.35]{media/comparison_navier_stokes_T_5.jpg}
\centering
\caption{Predictions for the Navier-Stokes problem \ref{ns_problem} with $T=5$ for 5 random samples. In the section labeled "Data" is the vorticity at the final time using fine and coarse discretizations, alongside a high-resolution image of the initial vorticity. In the section labeled "Predictions" are the initial condition reconstructions by the various methods.}
\label{fig:ns_results_5}
\end{figure}


\subsection{Darcy equation}\label{darcy_problem}
The steady-state two-dimensional Darcy flow for a porous medium is given by the following equation:
\begin{equation}\label{darcy_equation}
\begin{cases}
    -\nabla \cdot (K(x,y) \nabla h(x,y))= f(x,y), & (x,y) \in (0,1)^2  \\
    h(x,y) = 0, & (x,y) \in \partial (0,1)^2 \\
\end{cases}
\end{equation}

where $\partial (0,1)^2$ is the domain boundary, $K(x, y)$ is the permeability coefficient field, $h(x, y)$ is the pressure, and $f$ is a forcing function. The boundary condition used here is a homogeneous Dirichlet boundary (fully-reflective). The inverse problem is to learn the following mapping:

\begin{equation*}
    h(x,y) \longmapsto K(x,y).
\end{equation*}

We used the publicly available finite difference solver (written in MATLAB \cite{MATLAB}, given in \cite{FNO}) to create data with piecewise smooth coefficients $K$, with a constant forcing function $f \equiv 1$. The coefficient was selected using a Gaussian random field according to the following distribution: $\mathcal{N}(0, (-\Delta + 9 I)^{-2})$. This is followed by a binarization operation that mapped positive values to 12 and negative values to 3. We created 3 such datasets for the following resolutions: $n=128, 256, 512$ with a SR scale factor of 8. Hence, the super-resolution mappings were of the following dimensions:  $16 \times 16 \longmapsto 128 \times 128$, $32 \times 32 \longmapsto 256 \times 256$, and $64 \times 64 \longmapsto 512 \times 512$. Each dataset contained $1,000$ samples which were split into $800, 100, 100$ training, validation, and testing samples, respectively. Despite being a binary problem, we still used the $\textit{L}^2$ loss function \eqref{eqn:loss}, and not a binary loss function like negative log likelihood, since $K(x,y)$ does not have to be binary in many applications. For the two datasets with finer grids we had to use a smaller batch size of 10 to fit the models into memory. We note that ViTO was the only model we were able to run with a batch size of 100, but we kept it at 10 to make the comparison more accurate. 

The full error analysis is shown in Table \ref{tab:darcy}. In all 6 cases ViTO obtained the best accuracy compared to the benchmark methods. Note that the results for the Darcy problem and the wave problem \ref{wave_problem} are more decisive in comparison to the Navier-Stokes problem \ref{ns_problem}. This could potentially be explained by the shift from smooth functions to functions consisting of irregular interfaces and sharp features. Recall that FNOs rely on Fourier transforms, which can be very accurate for smooth functions, but face severe difficulties with discontinuities. Furthermore, as we refined the grid in the Darcy case we saw a noticeable improvement in the ViTO results, which was not observed in the FNO case. This can also be explained by Fourier analysis, since FNOs are learning a global base of functions, which renders them grid-invariant. 

Visualizations of the results for $n=128$ and $n=512$ are shown in Figure \ref{fig:darcy}, respectively. Note that ViTO was able to capture sharp features of the coefficient $K(x,y)$. This is especially noticeable in cases where there are very small discontinuities (cavities) in the data; while ViTO was mostly able to capture them, FNO tended to smooth them.

% Due to the larger grid, the coefficient $K(x,y)$ generally has sharper features c 

\begin{table}[ht]\caption{Test relative $\textit{L}^2$ errors for the Darcy flow problem with different grid sizes ($n$) and noise levels ($\gamma$).}
\centering 
\begin{tabular}{ccccccccc} %  
\toprule
\multirow{2}{*}{
\parbox[c]{.2\linewidth}{}}
  & \multicolumn{2}{c}{$n=128$} &&
\multicolumn{2}{c}{$n=256$} &&
\multicolumn{2}{c}{$n=512$} \\ 
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} 

 & {\centering $\gamma = 0$} & {$\gamma = 0.1$} && {$\gamma = 0$} & {$\gamma = 0.1$}  && {$\gamma = 0$} & {$\gamma = 0.1$} \\
\midrule
FNO      & 0.1422 & 0.4502 &&   0.1272  & 0.1915  &&  0.1235  & 0.1683 \\
DeepONet & 0.1463 & 0.4502 &&   0.1422  & 0.2090  &&  0.1608  & 0.2174 \\
ResNet   & 0.1603 & 0.2760 &&   0.1287  & 0.3078  &&  0.1416  & 0.3702 \\
ViTO     & \textbf{0.1184} & \textbf{0.1943} &&   \textbf{0.08216} & \textbf{0.1799}  &&  \textbf{0.05197} & \textbf{0.1623} \\
\bottomrule
\end{tabular}
\label{tab:darcy}
\end{table}


\begin{figure}
\centering
\begin{subfigure}[b]{0.9\textwidth}
   \includegraphics[scale=0.36]{media/comparison_darcy.jpg}
   \caption{Results for $n=128$.}
   \label{fig:darcy128} 
\end{subfigure}
\begin{subfigure}[b]{0.9\textwidth}
   \includegraphics[scale=0.36]{media/comparison_darcy_512.jpg}
   \caption{Results for $n=512$.}
   \label{fig:darcy512}
\end{subfigure}
\caption{Predictions for the Darcy problem \ref{darcy_problem} for 5 random samples using for $n=128$ and $n=512$. In the section labeled "Data" is the PDE solution $u(x,y)$ using fine and coarse discretizations, alongside a high-resolution image of the permeability  coefficient field  $K(x,y)$. In the section labeled "Predictions" are the permeability coefficient reconstructions by the various methods.}
\label{fig:darcy}
\end{figure}



% \begin{figure}[hbt!]
% \includegraphics[scale=0.35]{media/comparison_darcy.jpg}
% \centering
% \caption{Predictions for the Darcy problem \ref{darcy_problem} for 5 random samples using a resolution of $128 \times 128$. In the section labeled "Data" is the PDE solution $u(x,y)$ using fine and coarse discretizations, alongside a high-resolution image of the permeability  coefficient field  $K(x,y)$. In the section labeled "Predictions" are the permeability coefficient reconstructions by the various methods.}
% \label{fig:darcy_128}
% \end{figure}

% \begin{figure}[hbt!]
% \includegraphics[scale=0.35]{media/comparison_darcy_512.jpg}
% \centering
% \caption{Predictions for the Darcy problem \ref{darcy_problem} for 5 random samples using a resolution of $512 \times 512$. In the section labeled "Data" is the PDE solution $u(x,y)$ using fine and coarse discretizations, alongside a high-resolution image of the permeability coefficient field $K(x,y)$. In the section labeled "Predictions" are the permeability coefficient reconstructions by the various methods.}
% \label{fig:darcy_512}
% \end{figure}


\subsection{Varying input size}\label{var_input}

Finally, we assessed the ability of ViTO to handle inputs of various sizes without requiring retraining. Typically, transformers are capable of handling such inputs, which are prevalent in NLP contexts. As mentioned in \ref{network_arch}, we employed relative positional encoding, which was shown to be effective for computer vision problems of this sort. Additionally, the U-Net architecture is fully convolutional \cite{long2015fully}, allowing it to handle such inputs.

To evaluate this capability, we used the Darcy example \ref{darcy_problem} with $n=512$. We followed the same steps as in all other experiments, with one addition to the training process. During each training batch, we randomly selected a subsampling parameter $r \in \{1, 2, 3, \hdots , 9 \}$ and applied it to the input image (rounding the number of grid points to the nearest integer). For instance, taking $r =4$, an input of size $512 \times 512$ was downsampled to size $128 \times 128$. We used this process to allow ViTO to generalize better for new discretizations. This procedure can be considered a type of data augmentation. We also dropped the super-resolution part of the inverse problem (i.e. $s=1$ in \ref{data_driven}) to enable us to run tests for large grids.

Finally, we tested the model twice. First, we evaluated it on samples from the test set with grid sizes it had encountered during training, which were: $\{ \frac{512}{r}: r = 1, \hdots , 9 \}$, rounded to the nearest integer. Next, we created a zero-shot scenario, where the model was presented with samples having random discretizations that it had not seen during training. We created these discretizations by resizing the original samples accordingly. The results are presented in Figure \ref{fig:variable_input}. The results show that ViTO is capable of handling different grids without retraining, even in a zero-shot scenario. The error maps show us that larger grids generally yield better results, and that the error is mostly concentrated around the discontinuities (due to the binarization of the permeability field). 


\begin{figure}[!htb]
\centering
\begin{subfigure}[b]{0.9\textwidth}
   \includegraphics[scale=0.35]{media/variable_grid.jpeg}
   \caption{Previously seen discretizations.}
   \label{fig:variable_grid} 
\end{subfigure}
\begin{subfigure}[b]{0.9\textwidth}
   \includegraphics[scale=0.35]{media/zero_shot_variable_grid.jpeg}
   \caption{Zero-shot discretizations.}
   \label{fig:zero_shot_variable_grid}
\end{subfigure}
\caption{ViTO predictions with varying input sizes. The results are presented in columns corresponding to different discretizations. The first two rows show the ground-truth values of $u(x,y)$ and $K(x,y)$ as given by \eqref{darcy_equation}. The third row presents the predictions of ViTO. The last row shows the point-wise relative $\textit{L}^2$ error between $K(x,y)$ and the ViTO prediction.}
\label{fig:variable_input}
\end{figure}