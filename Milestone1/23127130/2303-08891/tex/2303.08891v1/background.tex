\section{Related work}

Several studies have been conducted by the SciML community for both forward and inverse problems. A notable advancement are the Physics-Informed Neural Networks (PINNs), invented by Raissi \textit{et al.} \cite{}. In PINNs, one designs two loss terms that are used simultaneously as the unified target function of the network. The first is the data loss, which operates on the known data, often called the ground truth. For the forward problems this data is usually gathered from the initial and boundary conditions, while for the inverse problem it is gathered from either the sensor measurement, the final state, the medium properties, a combination of the aforementioned, etc. The second loss term enforces the known physics about the problem. One can compute the gradients of the network, hence the derivatives in the different dimensions, and create a loss term by implicitly summing the partial derivatives that compose the equation for the physical model. By using the combination of the two loss terms, PINNs are able to correctly infer the solution of a certain PDE with high accuracy. In addition, they receive a point in time and space and produce the solution at that point, so they are not bound to a mesh. The drawbacks of PINNs are that they are problem specific, meaning that any change of the problem conditions requires training a new PINN which is a costly procedure.

The literature is offers many implementations of PINNs in different forward scenarios, such as mechanics \cite{}, acoustics \cite{}, tomography \cite{}, etc. However, not many PINN papers exist for inverse problems. In \cite{} the authors show how a PINN can be used to recover the wave propagation speed (medium property) using a few measurement of the acoustic scattered wave. In \cite{}, a method that combines Time Reversal \cite{} with a PINN is proposed for finding the source that started the emission of acoustic waves, and similar concepts have been used for identifying and locating a scatterer \cite{}.

Another important factor when dealing with SciML computations in general ,and specifically with inverse problems, is that we often approximate a solution to a problem on a discrete grid. That leads to issues such as the \textit{inverse crime} \cite{}, where one uses the same setup (usually a mesh) for both the forward and the inverse problems solvers. In addition, when dealing with an inverse problem, one does not have data recorded in every point of the domain (especially since the real-world scenario is assumed continuous). Therefore, one tries to reconstruct the solution of the inverse problem using a much more coarse discretization than the theoretical one, a situation which is often referred to as \textit{super resolution} (SR). Methods for SR \cite{} have been investigated throughout the years \cite{}, including ML based methods \cite{}. To emphasize the challenge of SR, an example could be finding an underground movement  e.g. of tectonic plates (geophysics) using recordings from few seismic sensors spread at multiple cities in the country. If one is interested in a solution that is accurate up to, say a few meters, a solution may not be possible.

Vision transformers with examples from classical use cases like CIFAR and medical imaging.

Following the revolution in NLP after the invention of transformers, the ViT, invented by Dosovitskiy \textit{et al.} \cite{}, is the latest advancement for imaging tasks. The proposed ViT architecture splits the image into small patches and translates each patch into a token using linear projections. The tokens that represent the image are then pushed into a transformer. In their paper, the authors demonstrate how the ViT outperforms the SOTA methods for image classifications, including ImageNet \cite{}, CIFAR \cite{}, Oxford pets and flowers \cite{}, and VTAB \cite{}. In addition, the benchmark ViT model is up to four times more efficient than the SOTA methods used as reference. The ViT can also be used in many different fields. In \cite{} the authors used a ViT for X-ray image classification, while in \cite{} it was used for unsupervised volumetric medical image registration. The capabilities of the ViT are vast and it is being currently adopted in various areas of research.

Operator learning has been a accelerating in recent years. The first operator learning method was invented by Lu \textit{et al.}, named the Deep Operator Network (DeepONet) \cite{}. The DeepONet is composed from a branch and a trunk. The branch learns the input function space of the operator, while the trunk learns the space of functions onto which the output is projected. By multiplying the branch with the trunk, the projection provides a representation of the output function space. Another popular invention is the Fourier Neural Operator \cite{}, which is based on a Fourier space kernel. The operator networks do not need to know the physics (unlike their PINN counterparts), and when changing the problem conditions no further training is needed. Using operator networks for inverse problems is natural, since one can train an operator network for many different problems and conditions and exploit the generalization capabilities for finding the solution for a new case. In \cite{}, the authors  demonstrate how to use downstream measurements and DeepONet for the inverse problem of determining the original perturbation of instability waves in high-speed boundary layers. The main drawback of operator networks is that they require a large amount of data for training, which may not be available especially when dealing with inverse problems.


There have been attempts to use transformers for operator learning. In \cite{} the authors use transformers to approximate \emph{forward} solutions of PDEs with operator learning. They propose an innovative way for choosing the collocation points and iterate through time to find the solution at those points. The attention mechanism is split in such a way that the query, key and value are split into different parts of the forward pass and is implemented as a multi-layer perceptron (MLP). \panos{I'm not sure about the previous sentence, something is missing} The latent encoding, which is the outcome of the combination of the three components is the embedding of the coordinates used for the spatio-temporal input of the PDE. In \cite{} the author presents a method to combine FNO with attention to improve the performance for PDE solutions, ny replacing the softmax (often used in transformers especially in classification) with a linear variant of it that does not involve normalization.