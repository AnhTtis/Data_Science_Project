\section{Experiments}
\label{sec:experiments}
\vspace{-.20cm}
\subsection{Setup and Dataset Specifications}
\vspace{-.05cm}

We generated a video dataset that consists of 10-frame clips of videos from the AV2 Common Test Conditions \cite{AV2CTC}
% an internal video folder)\footnote{Due to confidentiality concerns, we are not able to provide more information about the videos, rather than some samples given in Figures~\ref{fig:subjective_YUV} and~\ref{fig:subjective_LRHR}.} 
and their associated motion flows, calculated using UFlow \cite{lodha1996uflow}. We used a batch size of 8, i.e., 8 video clips in each batch. Each clip is processed during the dataset generation step such that it has 10 frames of size 256x256, selected from video of fps 
%and frame per second (fps) 
20-40. HEVC is implemented using x265 (IPP.., single reference frame, rdoq and loop filter on.). We compare the sandwich model with the standard codec, HEVC, under three settings: (1) YUV 4:0:0 format with single-channel (grayscale) bottlenecks in Figure~\ref{fig:YUV400_comp}, (2) YUV 4:4:4 format in low-resolution (LR) bottlenecks in Figure~\ref{fig:LRHR_comp}, and (3) YUV 4:4:4 format using the Learned Perceptual Image Patch Similarity (LPIPS) -- a common perceptual similarity metric in the literature \cite{zhang2018unreasonable}. In each setting, the model is trained for 1000 epochs, with a learning rate 1e-4, 
%on ... training video clips
and tested on 120 test video clips. All R-D plots are over the entire test set.
We report results in terms of YUV PSNR when using the $\ell_2$ norm and ``LPIPS (RGB) PSNR'' when using LPIPS. Because LPIPS is intended for RGB, for the latter the final decoded YUV video is converted into RGB and then LPIPS computed. In order to report results on an approximately similar scale we derived a fixed linear scaler for LPIPS so that for image vectors $x, y$,
\begin{eqnarray}
\vspace{-.1cm}
s LPIPS(x, y) \sim ||x-y||^2, \mbox{if } ||x-y||^2 < \tau
\vspace{-.1cm}
\end{eqnarray}
where $\tau$ is a small threshold and $s$ is the LPIPS linear scaler.

\begin{figure}[h]
\vspace{-.4cm}
    \centering
    \includegraphics[width=.8\linewidth]{figures/new_figures/YUV400.png}
    \vspace{-.2cm}
    \caption{RD performance of the YUV 4:0:0 sandwich.}
    \label{fig:YUV400_comp}
    \vspace{-.5cm}
\end{figure}
\vspace{-.20cm}
\subsection{Results}
\textbf{YUV 4:0:0.} Figures~\ref{fig:subjective_YUV} and~\ref{fig:YUV400_comp} show the subjective and quantitative results with sandwiched HEVC YUV 4:0:0 codec. As can be seen from the bottleneck frames in Figure~\ref{fig:subjective_YUV}, sandwich model is able to preserve the temporal consistency between the frames, which enables the standard codec HEVC to take advantage of the modulation patterns to achieve a better rate-distortion point. Quantitatively, Figure~\ref{fig:YUV400_comp} shows the substantial improvements (by 8 dB) of the sandwich model over 
%the standard codec 
HEVC YUV400.

\begin{figure}[h]
\vspace{-.4cm}
    \centering
    \includegraphics[width=.8\linewidth]{figures/new_figures/LRHR.png}
    \vspace{-.2cm}
    \caption{RD performance of YUV 4:4:4 low-resolution (LR) sandwich. (HEVC 444 HR stands for HEVC YUV 4:4:4 high-resolution.)}
    \label{fig:LRHR_comp}
\end{figure}

\vspace{-.15cm}
\textbf{Low-Resolution (LR).} 
Figures~\ref{fig:subjective_LRHR} and~\ref{fig:LRHR_comp} show the subjective and quantitative results with sandwiched HEVC YUV 4:4:4 LR codec. The standard LR codec transports LR video that is a linearly downsampled (bicubic) version of the original. Decoded video is linearly upsampled  (lanczos3). In the case of the sandwich, the exact same system is used but with generated bottlenecks. The 6.5 dB improvement of the sandwich model with HEVC YUV 4:4:4 LR  over the standard HEVC YUV 4:4:4 LR in Figure~\ref{fig:LRHR_comp} is also visible in the subjective comparison provided in Figure~\ref{fig:subjective_LRHR} which demonstrates the substantially detailed output of the sandwich in places where the standard HEVC LR produces blurred results.


\textbf{Perceptual Similarity Metric - LPIPS.} Finally, we train sandwich HEVC YUV 4:4:4 with LPIPS using a TensorFlow implementation of \cite{zhang2018unreasonable} and compare it with the standard HEVC YUV 4:4:4 under LPIPS. The results are provided in Figures~\ref{fig:subjective_LPIPS} and~\ref{fig:LPIPS_comp}. Notice that the sandwich model provides $\sim 30 \%$ improvements in rate over the standard HEVC across a broad range.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/new_figures/LPIPS_subjective_2_errors.jpg}
    \vspace{-0.2in}
    \caption{HEVC with LPIPS: Inter-frames from reconstructions by sandwich, original source videos, and reconstructions from HEVC (errors magnified 10x in the bottom-right). Note the rate reduction in the sandwich result. We were not able to observe perceptually meaningful differences despite larger absolute errors.}
    \label{fig:subjective_LPIPS}
    \vspace{-0.2in}
\end{figure}
Viewing the decoded video we observed consistent reductions in rate obtained by the sandwich system with visually imperceptible differences (e.g., Figure~\ref{fig:subjective_LPIPS}.)
\begin{figure}[h]
\vspace{-.4cm}
    \centering
    \includegraphics[width=.8\linewidth]{figures/new_figures/LPIPS.png}
    \caption{RD performance of the HEVC YUV 4:4:4 sandwich trained and tested with LPIPS. Neural-enhanced HEVC YUV 4:4:4 performs $\sim 30 \%$ better than the standard HEVC over a broad range of rates.}
    \label{fig:LPIPS_comp}
    \vspace{-.5cm}
\end{figure}




%\begin{figure}[t]
%    \centering
%    \includegraphics[width=0.35\linewidth]{ICIP/figures/original.png}
%    \includegraphics[width=0.35\linewidth]{ICIP/figures/h264_prediction.png}
%    \caption{(left) Original frame. (right) Prediction of H264.}
%    \label{fig:subjective_quality_original_h264}
    %\vspace{-0.2in}
%\end{figure}

%begin{figure}[t]
%    \centering
%    \includegraphics[width=0.35\linewidth]{ICIP/figures/sandw%ich_video_on_video_proxy_bottleneck.png}
%    \includegraphics[width=0.35\linewidth]{ICIP/figures/sandwich_video_on_video_proxy_prediction.png}
%    \caption{Extracted from the sandwiched video model tested with video codec proxy. (left) Bottleneck and (right) Prediction. }
%    \label{fig:subjective_quality_video_with_video_proxy}
    %\vspace{-0.2in}
%\end{figure}

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=0.35\linewidth]{ICIP/figures/sandwich_image_on_video_proxy_bottleneck.png}
%    \includegraphics[width=0.35\linewidth]{ICIP/figures/sanwich_image_on_video_proxy_prediction.png}
%    \caption{Extracted from the sandwiched image model tested with video codec proxy. (left) Bottleneck and (right) Prediction. }
%    \label{fig:subjective_quality_image_with_video_proxy}
    %\vspace{-0.2in}
%\end{figure}


%\begin{figure}[t]
%    \centering
%    \includegraphics[width=0.35\linewidth]{ICIP/figures/sandwich_video_on_h264_bottleneck.png}
%    \includegraphics[width=0.35\linewidth]{ICIP/figures/sandwich_video_on_h264_prediction.png}
%    \caption{Extracted from the sandwiched video model tested with H264. (left) Bottleneck and (right) Prediction. }
%    \label{fig:subjective_quality_video_with_h264}
    %\vspace{-0.2in}
%\end{figure}


%\begin{figure}[t]
%    \centering
%    \includegraphics[width=0.35\linewidth]{ICIP/figures/sandwich_image_on_h264_bottleneck.png}
%    \includegraphics[width=0.35\linewidth]{ICIP/figures/sandwich_image_on_h264_prediction.png}
%    \caption{Extracted from the sandwiched image model tested with H264. (left) Bottleneck and (right) Prediction. }
%    \label{fig:subjective_quality_image_with_h264}
    %\vspace{-0.2in}
%\end{figure}