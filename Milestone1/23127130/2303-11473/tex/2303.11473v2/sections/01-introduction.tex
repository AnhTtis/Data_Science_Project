\section{Introduction}
\label{sec:introduction}
%\vspace{-.25cm}
%Over 
The last decade 
%we have 
has witnessed neural image compression methods \cite{balle2016end_pcs} gradually outperforming traditional image compressors such as JPEG2000 and HEVC \cite{sullivan2012overview} and replacing them as state-of-the-art image compressors. The success of neural image compressors is due to their ability to perform complex non-linear transforms, which are learned end-to-end through back-propagation \cite{balle2020nonlinear}. More recently, the first learned video compressors have been proposed \cite{rippel2019learned} and research on improving neural video compressors has accelerated \cite{agustsson2020scale, hu2021fvc, rippel2021elf, yang2020hierarchical}. The existing strategies focus mainly on learning how to predict flows, 
%warp the predicted flows with the previous reconstructions, 
warp the previous reconstructions with the predicted flows,
and do residual compensation, all with neural networks trained end-to-end. 
This has lead to very complex networks with tens of millions of parameters \cite{hu2021fvc, rippel2021elf}, tens of millions of floating point operations per pixel, and weeks of training on high end GPUs \cite{lu2019dvc}.


\begin{figure}[t]
    \centering
    \includegraphics[width=.95\linewidth]{figures/new_figures/yuv_subjective_6_D=7.63_R=0.50.drawio.png}
    %\vspace{-0.15in}
    \caption{Toy example with gray-scale codec (HEVC4:0:0). The sandwich is used to transport full color video over a codec that can only transport gray-scale. Frames of neural codes decoded by the standard codec (compressed bottlenecks), final reconstructions by the post-processor, and original source videos, at time $t$, $t+1$, and $t+2$ are shown. Rate=0.50 bpp, PSNR=39.3dB, fps=30. Note that the sandwich establishes temporally consistent modulation-like patterns on the bottlenecks through which the pre-processor encodes color that is then demodulated by the post-processor for a full-color result. }
    \label{fig:subjective_YUV}
    %\vspace{-0.6cm}
\end{figure}

In this work, we propose a computationally more efficient way of improving upon any standard video codec by using neural networks.
In particular, we sandwich a standard video codec between lightweight neural pre- and post-processors, and optimize them end-to-end.
Our approach leverages existing highly optimized computational implementations of the standard codec and the seamless network-transport of standard bit-streams over network nodes already familiar with such traffic.  The proposed neural pre- and post-processors learn how to perform non-linear transformations on the input and the output of the standard codec for the best rate-distortion performance while the standard video codec does the heavy lifting by performing the usual video coding operations on the output of the neural pre-processor. Our work is primarily geared toward adapting the standard codec to compression scenarios  that are outside the immediate scope of its design. In doing so, we are interested not only in significant improvements but also in
understanding how the neural networks accomplish those improvements.
% the resulting designs way of accomplishing said improvements.
As the networks have to generate viewable 8-bit video that is related to the original visual scenes, it is typically possible to observe their operation even if at a coarse level (see for instance the toy example of Figure \ref{fig:subjective_YUV}). 



\begin{figure*}[t]
    \centering
     %\vspace{-0.2in}
    \includegraphics[width=\linewidth]{figures/sandwiched_video_diagram.jpg}
       % \vspace{-0.25in}
    \caption{Sandwich architecture during training. The shaded box (video codec proxy) is replaced with a standard video codec at inference.}
    \label{fig:diagram}
    %\vspace{-0.25in}
\end{figure*}


Figure~\ref{fig:diagram} summarizes our approach.
For the $i$th video clip with frames $t=0,1,2,\ldots$, source images $\{I_i(t)\}$ are  processed by a neural pre-processor in a temporally independent fashion to obtain ``bottleneck'' images $\{L_i(t)\}$.  The bottleneck images contain neural latent codes representing the source images, but are nevertheless in 4:4:4 (or 4:2:0) format ready for compression by the standard video codec.  The standard video codec compresses the bottleneck images into reconstructed bottleneck images $\{\hat L_i(t)\}$, again in 4:4:4 format.  Finally, the reconstructed bottleneck images are processed by a neural post-processor into reconstructed source images $\{\hat I_i(t)\}$.  To train the neural pre- and post-processors, we replace the standard video codec by a differentiable proxy (shown shaded in yellow) and we minimize the total rate-distortion Lagrangian $\sum_{i,t}D_i(t)+\lambda R_i(t)$, where $D_i(t)$ and $R_i(t)$ are the distortion and rate of frame $t$ in clip $i$.  At evaluation, the standard video codec is used instead of the proxy.
% Throughout the manuscript, we refer to the output of the pre-processor network $L_i(\cdot)$ as the \emph{bottleneck} and input to the post-processor network $\hat{L}_i(\cdot)$ as the \emph{reconstructed bottleneck}.

 Our framework is inspired by previous work on sandwiched image compression \cite{guleryuz2021sandwiched, guleryuz2022sandwiched}, which shares the motivation to improve over standard codecs in rate-distortion performance, while leveraging their implementation-related conveniences. The end-to-end learned image codecs that outperform the standard image codecs in rate-distortion performance \cite{balle2016end_pcs, balle2018variational, hu2021learning, toderici2017full} require over-parameterized networks and entropy models, which have significant time, power, and resource costs. \cite{guleryuz2021sandwiched,guleryuz2022sandwiched} proposed a solution to this by wrapping %lightweight 
 neural networks around a standard image codec. Training the sandwich end-to-end requires a differentiable proxy for the standard image codec. %In \cite{guleryuz2021sandwiched,guleryuz2022sandwiched}, 
 The authors trained the sandwiched model by replacing the standard image codec with a differentiable JPEG proxy that uses $8\times8$ discrete cosine transforms (DCTs) and straight-through quantization\footnote{A straight-through quantizer is a software-implemented function $y=Q(x)$ that implements ordinary quantization in the forward direction but pretends to have a gradient $dy/dx$ of 1 instead of 0 in the backward direction.} as a proxy for the quantization step, and a differentiable function as an approximation of the bit rate from the entropy coding step. 
 They showed that with training, the pre-processor and post-processor would learn to communicate with each other by sending neural codes in the form of spatial modulation patterns that represent details of the original input. The patterns are easy to compress with a standard codec while being robust to its degradations.  Gains over the standard codec alone were shown to be especially noteworthy when the input has different characteristics than intended for the standard codec.  In particular, \cite{guleryuz2021sandwiched} showed high gains when coding multispectral or normal map images, while \cite{guleryuz2022sandwiched} showed high gains when coding high dynamic range or high resolution images with a low dynamic range (LDR) or low resolution (LR) codec.
 
 Our motivation and approach are similar to \cite{guleryuz2021sandwiched,guleryuz2022sandwiched} but in this paper, we make the fundamental leap of transferring the sandwich approach from images to video. This leap is challenging for a number of reasons.  First, independently pre- and post-processing the video frames may be problematic, as the neural codes in the bottleneck images need to be temporally coherent in order for the video codec to take advantage of motion compensation. It is not at all clear that modulation-like high frequency patterns will be well-represented through motion compensation. Second, standard video codecs \cite{han2021technical,mukherjee2013latest,sullivan2004h,sze2014high, tudor1995mpeg,zhang2019recent}
 % such as ISO/IEC MPEG series \cite{tudor1995mpeg, sikora1997mpeg}, ITU-T H.26x series \cite{sze2014high, sullivan2004h}, AVS series \cite{yu2009overview, ma2013overview, zhang2019recent}, VP9 \cite{mukherjee2013latest}, and AV1 \cite{chen2018overview, han2021technical},
 are substantially more complicated than image codecs, involving not only motion estimation and compensation, but also mode selection, loop filtering, and many other aspects. In this paper, we address these issues, while showing results for sandwiched video compression that parallel the earlier results for sandwiched image compression, including multispectral (color video compressed by grayscale video codecs) and super-resolution (high resolution video compressed by standard resolution video codecs) applications.  Moreover, we get gains over standard codecs with networks on the order of 100K parameters 
 %or 100K flops per pixel 
 (pre and post combined), a reduction in complexity by two orders of magnitude over state-of-the-art neural video compressors.  Few works prior to ours sandwich a standard video codec between neural processors. Those that do, e.g. \cite{Andreopoulos22,EusebioAP20,QiuLD21}, do so in such a way that the pre- and post-processors may be used independently --- thus do not take full advantage of the communication available between pre- and post-processors. Our contributions can be summarized as:
 


(1) We propose a sandwiched video compression framework that provides more efficient training and inference due to the simplicity of the neural networks but still enjoys the flexibility of neural networks in learning non-linear transforms.
     
(2) We propose a differentiable video codec proxy that can be used effectively in the training loop.
     
(3) We demonstrate that the sandwiched HEVC obtains 6.5 dB improvement over the standard HEVC 4:4:4 in low-resolution transport (8 dB improvement over 4:0:0 when coding color over gray-scale.) Moreover, when optimized for and tested with a perceptual similarity metric, Learned Perceptual Image Patch Similarity (LPIPS), we observe $\sim 30 \%$ improvements in rate at the same quality over the standard HEVC 4:4:4.
 


 
