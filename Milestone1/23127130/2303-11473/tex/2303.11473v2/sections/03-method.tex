\section{Method}
\label{sec:method}
%\vspace{-.20cm}
\subsection{Video Codec Proxy}
%\vspace{-.05cm}

Our video codec proxy, shown in Figure~\ref{fig:diagram}, maps an input group of images (like a group of pictures, or GOP) to an output group of images, plus a bit rate for each image.  The details of our video codec proxy can be summarized as follows.

{\em Intra-Frame Coding}.  The video codec proxy simulates coding the first ($t=0$) frame of the group, or the I-frame, using the image codec proxy from \cite{guleryuz2021sandwiched,guleryuz2022sandwiched}.

{\em Motion Compensation}.  The video codec proxy simulates predicting each subsequent ($t>0$) frame of the group, or P-frame, by motion-compensating the previous frame, whether the previous frame is an I-frame or a P-frame.  Motion compensation is performed using a ``ground truth'' dense motion flow field obtained by running a state-of-the-art optical flow estimator, UFlow \cite{lodha1996uflow}, between the original source images $I_i(t)$ and $I_i(t-1)$.  The video proxy simply {\em applies} this ground truth motion flow to the previous reconstructed bottleneck image $\hat L_i(t-1)$ to obtain an inter-frame prediction $\tilde L_i(t)$ for bottleneck image $L_i(t)$.  This simulates motion compensation in a standard codec operating on the bottleneck images.  Note that our motion compensation proxy does not actually depend on $L_i(t)$, so although it is a warping, it is a linear map from $\hat L_i(t-1)$ to $\tilde L_i(t)$, with a constant Jacobian.  This makes optimization much easier than if we had tried to use a differentiable function of both $\hat L_i(t-1)$ and $L_i(t)$, like UFlow itself, that {\em finds} as well as {\em applies} a warping from $\hat L_i(t-1)$ to $L_i(t)$.  Such functions have notoriously fluctuating Jacobians that make training difficult.

{\em Prediction Mode Selection}.  Our video codec proxy also simulates Inter/Intra prediction mode decisions. This ensures better handling of temporally occluded/uncovered regions in video. First, Intra-prediction is simulated by rudimentarily compressing the current-frame and low-pass filtering it. This simulates filtering, albeit not the usual directional filtering, to predict each block from its neighboring blocks.  Initial rudimentary compression ensures that the Intra-prediction proxy is not unduly preferred at very low rates. For each block, the Intra prediction (from spatial filtering) is compared to the Inter prediction (from motion compensation), and the one closest to the input block determines the mode of the prediction.

{\em Residual Coding}.  The predicted image, comprising a combination of Intra- and Inter-predicted blocks, is subtracted from the bottleneck image, to form a prediction residual.  The residual image is then 
%``compressed'' 
compressed using the image codec proxy of \cite{guleryuz2021sandwiched,guleryuz2022sandwiched}.  The compressed residual is added back to the prediction to obtain a ``pre-filtered'' reconstruction of the bottleneck image $\hat L_i(t)$.

{\em Loop Filtering}.  The ``pre-filtered'' reconstruction is then filtered by a loop filter to obtain the final reconstructed bottleneck image $\hat L_i(t)$.  The loop filter is implemented with a small U-Net((8);(8, 8)) \cite{UNet} that processes one channel at a time and is trained once for four rate points on natural video using only the video codec proxy with rate-distortion ($\ell_2$) training loss in order to mimic common loop filters. The resulting tandem of filters are kept fixed for all of our simulations, i.e., once independently trained, the loop filters for four rate points are not further trained.

{\em Rate Proxy}. Similar to \cite{guleryuz2021sandwiched,guleryuz2022sandwiched}, the differentiable rate for each frame is obtained as $    R(E_i(j)) = a\sum_{k,l}\log( 1+|e_l^{(k)}|/\Delta ),$ where $e_l^{(k)}$ is the $l$th coefficient of the $k$th block of DCT coefficients, and $a$ is chosen such that $R(E_i(j))$ is the actual rate at which JPEG codes $E_i(j)$ with uniform stepsize $\Delta$.
An alternative proxy is given in \cite{SaidSP22}.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/new_figures/LRHR_subjective_5.drawio.png}
    %\vspace{-0.2in}
    \caption{Sandwich of HEVC LR: Frames from compressed bottlenecks, reconstructions by sandwich, original source videos, and %reconstructions from 
    HEVC LR.
    %, at time $t$, $t+2$, and $t+4$. 
    Note the substantially improved definition on the crowd and the building. While not shown here the sandwich system also obtains significant improvements over neural-post-processed HEVC LR thanks to the information embedded by the neural pre-processor. }
    \label{fig:subjective_LRHR}
    %\vspace{-0.2in}
\end{figure}

%\vspace{-.20cm}
\subsection{Pre- and Post-Processors}
%\vspace{-.05cm}

The pre- and post-processors are implemented as a $1\times1$ conv-network (a multilayer perceptron acting pixelwise) in parallel with a 
%slimmed-down 
U-Net.  The intuition is that the $1\times1$ conv-network learns the appropriate color space conversion and tone mapping, while U-Net adds or removes spatial modulation patterns, or watermarks, which serve as the neural codes. We report results with U-Nets of two different complexities. As U-Net encoder/decoder filters, the high complexity network uses U-Net((32, 64, 128, 256); (512, 256, 128, 64, 32)) ($\sim 8$M parameters). Our low complexity, {\em slim} network uses U-Net((32); (32, 32)) ($\sim 57$ thousand parameters), with a $\sim \boldsymbol{ 99\%}$ reduction in parameters.

%\vspace{-.20cm}
\subsection{Loss Function}
%\vspace{-.05cm}

As mentioned in Section~\ref{sec:introduction}, the pre- and post-processors, and the loop filter in the video codec proxy are trained to minimize the total rate-distortion Lagrangian $\sum_{i,t}D_i(t)+\lambda R_i(t)$, where $D_i(t)$ and $R_i(t)$ are the distortion and rate of frame $t$ in clip $i$. The rate term in particular serves to encourage the pre- and post-processors to produce temporally consistent neural codes, since neural codes that move according to our ``ground truth'' motion flow field will be well predicted, leading to smaller residuals and rate terms.  We do indeed observe that the resulting neural codes seem to have some degree of temporal consistency, as shown in Figure~\ref{fig:subjective_YUV}. Note that the overall mapping from the input images through the pre-processor, video codec proxy, post-processor, and loss function is differentiable.
