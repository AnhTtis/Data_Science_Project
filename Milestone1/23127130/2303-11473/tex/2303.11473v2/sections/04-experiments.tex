\section{Experiments}
\label{sec:experiments}
%\vspace{-.20cm}
\subsection{Setup and Dataset Specifications}
%\vspace{-.05cm}

We generate a video dataset that consists of 10-frame clips of videos from the AV2 Common Test Conditions \cite{AV2CTC} and their associated motion flows, calculated using UFlow \cite{lodha1996uflow}. We use a batch size of 8, i.e., 8 video clips in each batch. Each clip is processed during the dataset generation step such that it has 10 frames of size 256x256, selected from video of fps 
%and frame per second (fps) 
20-40. HEVC is implemented using x265 (IPP.., single reference frame, rdoq and loop filter on.). We compare the sandwich model with the standard codec, HEVC, under three settings: (1) YUV 4:0:0 format with single-channel (grayscale) bottlenecks in Figure~\ref{fig:YUV400_comp}, (2) YUV 4:4:4 format in low-resolution (LR) bottlenecks in Figure~\ref{fig:LRHR_comp}, and (3) YUV 4:4:4 format using the Learned Perceptual Image Patch Similarity (LPIPS) -- a common perceptual similarity metric in the literature \cite{zhang2018unreasonable, elpips}. In each setting, the model is trained for 1000 epochs, with a learning rate 1e-4, and tested on 120 test video clips. {\em All RD plots are over the entire test set.}
We report results in terms of YUV PSNR when using the $\ell_2$ norm and ``LPIPS (RGB) PSNR'' when using LPIPS. Because LPIPS is intended for RGB, for the latter the final decoded YUV video is converted into RGB and then LPIPS is computed. In order to report results on an approximately similar scale, we derived a fixed linear scaler for LPIPS so that for image vectors $x, y$,
\begin{eqnarray}
%\vspace{-.3cm}
s LPIPS(x, y) \sim ||x-y||^2, \mbox{if } ||x-y||^2 < \tau
%\vspace{-.2cm}
\end{eqnarray}
where $\tau$ is a small threshold and $s$ is the LPIPS linear scaler. The LPIPS loss of a clip is the average of the LPIPS losses over 10 frames. 
 

\begin{figure}[h]
%\vspace{-.45cm}
    \centering
    \includegraphics[width=\linewidth]{figures/final_figures/plot_grayscale_video.pdf}
    %\vspace{-.4cm}
    \caption{RD performance of the YUV 4:0:0 sandwich.}
    \label{fig:YUV400_comp}
    %\vspace{-.5cm}
\end{figure}
%\vspace{-.20cm}
\subsection{Results}
\textbf{YUV 4:0:0.} Figures~\ref{fig:subjective_YUV} and~\ref{fig:YUV400_comp} show the subjective and quantitative results with sandwiched HEVC YUV 4:0:0 codec. As can be seen from the bottleneck frames in Figure~\ref{fig:subjective_YUV}, sandwich model is able to preserve the temporal consistency between the frames, which enables the standard codec HEVC to take advantage of the modulation patterns to achieve a better rate-distortion point. Quantitatively, Figure~\ref{fig:YUV400_comp} shows the substantial improvements (by 8 dB) of the sandwich model over 
%the standard codec 
HEVC YUV400. For reference HEVC 444 and its neural-enhanced version are also included. The latter  obtains compression-wise meaningful but relatively minor $5\%$ improvements in rate over the former. %HEVC 444.

\begin{figure}[h]
%\vspace{-.4cm}
    \centering
    \includegraphics[width=\linewidth]{figures/final_figures/plot_lrhr_video.pdf}
    %\vspace{-.4cm}
    \caption{RD performance of YUV 4:4:4 low-resolution (LR) sandwich. (HEVC 444 HR stands for HEVC YUV 4:4:4 high-resolution.)}
    \label{fig:LRHR_comp}
\end{figure}

%\vspace{-.2cm}
\textbf{Low-Resolution (LR).} 
Figures~\ref{fig:subjective_LRHR} and~\ref{fig:LRHR_comp} show the subjective and quantitative results with sandwiched HEVC YUV 4:4:4 LR codec. The standard LR codec transports LR video that is a linearly downsampled (bicubic) version of the original. Decoded video is linearly upsampled  (lanczos3). In the case of the sandwich, the exact same system is used but with generated bottlenecks. The 6.5 dB improvement of the sandwich model with HEVC YUV 4:4:4 LR  over the standard HEVC YUV 4:4:4 LR in Figure~\ref{fig:LRHR_comp} is also visible in the subjective comparison provided in Figure~\ref{fig:subjective_LRHR} which demonstrates the substantially detailed output of the sandwich in places where the standard HEVC LR produces blurred results.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/LPIPS_subjective_updated.jpg}
    %\vspace{-0.2in}
    \caption{HEVC with LPIPS: Inter-frames from reconstructions by sandwich, original source videos, and reconstructions from HEVC (errors magnified 10x in the bottom-right). Note the rate reduction in the sandwich result. We were not able to observe perceptually meaningful differences despite larger absolute errors.}
    \label{fig:subjective_LPIPS}
    %\vspace{-0.2in}
\end{figure}
\textbf{Perceptual Similarity Metric - LPIPS.} Finally, we train sandwich HEVC YUV 4:4:4 with LPIPS using a TensorFlow~\cite{tensorflow2015-whitepaper} implementation of \cite{zhang2018unreasonable} and compare it with the standard HEVC YUV 4:4:4 under LPIPS. The results are provided in Figures~\ref{fig:subjective_LPIPS} and~\ref{fig:LPIPS_comp}. Notice that the sandwich model provides $\sim 30 \%$ improvements in rate over the standard HEVC across a broad range.
Viewing the decoded video, we observe consistent reductions in rate obtained by the sandwich system with visually imperceptible differences (e.g., Figure~\ref{fig:subjective_LPIPS}.)
Concerns about the networks over-optimizing or ``hacking'' LPIPS are further alleviated by noting that because a clip typically depicts the same scene under slight geometric and color transformations, the utilized clip-averaged LPIPS loss  is expected to be robust for the scene--akin to the robustified ensemble LPIPS loss of \cite{elpips}.
\begin{figure}[h]
%\vspace{-.45cm}
    \centering
    \includegraphics[width=\linewidth]{figures/final_figures/plot_lpips_video.pdf}
    %\vspace{-0.4cm}
    \caption{RD performance of the HEVC YUV 4:4:4 sandwich trained and tested with LPIPS. Neural-enhanced HEVC YUV 4:4:4 performs $\sim 30 \%$ better than the standard HEVC over a broad range of rates.}
    \label{fig:LPIPS_comp}
    %\vspace{-.45cm}
\end{figure}


