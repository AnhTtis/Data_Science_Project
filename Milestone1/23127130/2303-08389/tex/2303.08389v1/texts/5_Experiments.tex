Our framework seeks to identify whether a given metric can detect lexical noise in a generated caption. Through exhaustive experiments, we evaluate whether the PR-MCS developed as described in this paper successfully distinguishes the perturbed caption from the original caption.
%Our framework aims to identify whether a given metric can detect lexical noise in the generated caption. Through exhaustive experiments, we evaluate whether the PR-MCS developed through our methodology distinguishes the perturbed caption from the original caption.

\input{resource/table2}

\subsection{Experimental Setup}
\noindent \textbf{Fine-tuning set} We use MSCOCO, the dataset most widely used for image captioning, as the fine-tuning set to enhance the perturbation robustness of MCS. We use the training and validation split of the MSCOCO dataset described by \citet{chen2020uniter}. The number of elements in the training set is 414k. Since only English captions exist in MSCOCO, captions are translated into four other languages using the MBART-50~\cite{liu2020multilingual} MT model.\\
%We use COCO, the most used dataset in image captioning, as the finetuning set to give perturbation robustness to MCS. We use the train and validation split of the COCO dataset in Chen-et-al. The number of the training set is 414k. Since only English captions exist in COCO, captions are translated into four other languages using the MBART-50 MT model.\\

\noindent \textbf{Evaluation set} To comprehensively evaluate the perturbation robustness of PR-MCS, we choose four evaluation sets: MSCOCO, VizWiz~\cite{gurari2020captioning}, Flickr8k~\cite{anitha2019automated}, and M-FineCapEval. As in the fine-tuning, MSCOCO, VizWiz, and Flickr8k, which have only English captions, are translated using the MBART-50 MT model. The evaluation dataset sizes are unified to 3k.\\
%To comprehensively evaluate the perturbation robustness of PR-MCS, we choose four evaluation sets: COCO, VizWiz, Flickr8k, and M-FineCapEval. As in finetuning, COCO, VizWiz, and Flickr8k, which have only English captions, are translated using the MBART-50 MT model, and the evalset sizes are unified to 3k.\\

\noindent \textbf{Baseline metric} As the baseline of the experiment, we use two MCS metrics. As mentioned above, the MCS metric is configured using CLIP’s visual and multilingual text encoder. The first baseline is the MCS metric constructed using the multilingual CLIP text encoder implemented by \citet{multilingualCLIP2022} as the backbone. The second baseline is the MCS metric constructed using the multilingual text encoder trained by the teacher learning method described in Section~\ref{MCS}.
%As the baseline of the experiment, we use two MCS metrics. As mentioned above, the MCS metric is configured using CLIP's visual and multi-lingual text encoder. The first baseline is the MCS metric constructed using the multi-lingual CLIP text encoder implemented by Fredrik as the backbone. The second baseline is the MCS metric constructed using the multi-lingual text encoder trained by the teacher learning method of section 3.1.

\subsection{Perturbation configuration} 
We select the following five criteria to perturb the sentences in the fine-tuning and evaluation sets. The criteria below are error types commonly found in model-generated captions. These criteria are part of the checklist proposed by \citet{sai2021perturbation}. We select five orthogonal criteria. Each perturbation example is shown in Figure~\ref{fig5}.\\
%We select the following five criteria in the finetuning and evaluation sets to perturb the sentences. The criteria below are error types commonly found in model-generated captions. These criteria are part of the checklist of the sai-et-al, and we select five orthogonal criteria. Each perturbation example is shown in figure 5.

\noindent \textbf{Repetition} Repeated words are found in several model-generated captions. A well-known problem is that the transformer model is vulnerable because it does not capture repetitive perturbation well at the embedding level. We give each word token a repeating perturbation with a probability of 0.4.\\
%Repeated words are found in several model-generated captions. The well-known problem is that transformer model are vulnerable because they do not capture the repetitive perturbation well at the embedding level. We give each word-token a repeating perturbation with a probability of 0.4.\\

\noindent \textbf{Removal} Among the sentences given a low score in the evaluation dataset for the image captioning metric, such as Composite~\cite{aditya2015images} or Pascal50s~\cite{vedantam2015cider}, some word tokens are removed, and incomplete sentences are found. We configure perturbation by removing some tokens to reflect this noise. Each word token is drawn with a probability of 0.4.\\
%Among the sentences given a low score in the eval set for the image captioning metric, such as Composite or Pascal50s, some word tokens are removed, and incomplete sentences are found. We configure perturbation by removing some tokens to reflect this noise. We implement that each word-token is drawn with a 0.4 probability.\\

\noindent \textbf{Masking} Masking is a perturbation in which randomly selected tokens in the caption are replaced with [Mask] tokens. When lexical noise is given in units of tokens, the meaning of the corresponding token disappears, but unlike in the Removal case, the position is maintained. Position information can be critical in a reference-free metric based on a transformer model such as CLIPScore~\cite{dai2019transformer, devlin2018bert, ramachandran2019stand, wu2021rethinking}. Therefore, even if the [Mask] token does not appear in the generated caption, we select Masking perturbation as the criterion, separate from Removal, to address the above case. Each word token is replaced with a [Mask] token with a probability of 0.4.\\
%Masking is a perturbation in which randomly selected tokens in the caption are replaced with [Mask] tokens. When lexical noise is given in units of tokens, the meaning of the corresponding token disappears, but unlike in the Removal case, the position is maintained. In the case of position information, it can be very critical in a reference-free metric based on a transformer model such as CLIPScore. Therefore, even if the [Mask] token does not appear in the generated caption, we select Masking perturbation as the criteria separately from Removal to tackle the above case. Similarly, each word-token is replaced with a [Mask] token with a probability of 0.4.\\

\noindent \textbf{Jumble} We generate perturbed samples using random-order permutation at the token level in the original reference caption. The model composing the metric can see all tokens of the sentence, including visual content, but considerable noise is introduced into the position information.\\
%We generate perturbed samples using random order permutation at the token level in the original reference caption. The model composing the metric can see all tokens of the sentence, including visual content, but considerable noise is given to the position information.\\

\noindent \textbf{Substitution} Substitution involves changing the positions of key elements in a sentence. In the case of \dataset, substitution is performed using critical objects annotated by human experts. In the remaining three datasets, nouns in the caption are extracted, and their positions are changed. The perturbed caption includes all elements that exist in the original caption, but unlike in the Jumble case, it does not deform the grammatical structure at all.
Detecting substitution noise well is the most challenging task because it requires judging semantic correspondence to visual content perfectly.
%Substitution is changing the position of key elements in a sentence. In the case of M-FineCapEval, perturbation is constructed using critical objects annotated by human experts directly. In the remaining three datasets, nouns in the caption are extracted, and their positions are changed. The perturbed caption includes all elements that exist in the original caption, but unlike the jumble case, it does not destroy the grammatical structure at all. Therefore, for the metric to detect substitution noise well, it is the most challenging task because it must be able to judge semantic correspondence to visual content perfectly.

\subsection{Perturbation robustness evaluation}
\label{results}
We report the main results for all datasets and languages in Table~\ref{table2} and Figure~\ref{fig6}. The robust evaluation metric is expected to give lower scores to perturbed captions than to original captions.
%We report the main results for all datasets and languages in Table 2 and Figure 6. The robust evaluation metric is expected to give a lower score to the perturbed one compared to the original caption.

Each graph of Figure~\ref{fig6} shows the experimental results for MSCOCO, VizWiz, Flickr8k, and M-FineCapEval by perturbation. Each point represents the average results for five languages for one perturbation. It shows how much score drop the perturbed caption has from the original caption. The green line indicates PR-MCS, and the blue and red lines refer to the two baseline multilingual CLIPScores. The scores of the perturbed caption by baseline metrics do not differ much from those of the original caption for any perturbation methods. In some cases, the scores for the perturbed captions are higher than those for the original captions.
%Each row of Figure 6 is a graph showing the experimental results of COCO, VizWiz, Flickr8k, and M-FineCapEval by perturbation from the top. In Figure 6, the green line indicates PR-MCS, and the blue and red lines refer to the two baselines multi-lingual CLIPScores. Both baseline metrics do not show much difference from the original caption for all perturbation methods, or even in some cases, the scores of the perturbed captions are higher than those of the original captions. 

However, our metric exhibits a significant score decrease for all perturbations compared to the original captions, which means that the metric can clearly distinguish when the perturbation is applied. In other words, our metric exhibits robustness for all perturbations in the evaluation dataset. 
In particular, even in the cases of Repetition and Substitution, which are known to be challenging perturbations, PR-MCS detects perturbations very well, while baseline metrics do not capture perturbations at all.
%However, our metric shows a significant score drop in all perturbations compared to the original captions, which means the metric can clearly distinguish to which perturbation is applied. In other words, our metric has robustness for all perturbations constituting the eval set. Also, the result shows the same tendency in English and the other four languages. In particular, in the case of Repetition and Substitution, which are known as challenging perturbations, PR-MCS detects very well, while baseline metrics do not catch perturbation at all.

Table~\ref{table2} shows the score of the original caption and the average score of the perturbed caption given by the metrics for each of the four evaluation sets for each language. The result shows how much the percentage of the score decreased for each perturbation in comparison to the original caption. The results for the baseline metrics show that the score decrease for the average perturbation is very small, i.e., approximately 3\%, relative to the original caption. %The 3\% score decrease can be interpreted as 1.0 point given to the original caption corresponding to 0.97 of a point given to the perturbed caption. 
It is difficult to say that the metric can distinguish the perturbed caption from the original caption based on such a slight difference.
%Table 2 shows the score of the original caption and the average score of the perturbed caption given by the metrics for each of the four evaluation sets for each language. The result shows how much the percentage of the score has dropped for each perturbation compared to the original caption. From the results of the baseline metrics, it can be seen that the score drop for the average perturbation score is very small, around 3\%, compared to the original caption. For the 3\% score drop, it can be interpreted that when 1 point is given to the original caption, 0.97 point is given to the perturbed caption. It is difficult to say that the metric can distinguish the perturbed one from the original caption with such a slight difference.
In contrast, in the case of PR-MCS, the percentage decrease for the perturbed caption ranges from 50\% to 70\%. Clearly, our proposed method exhibits perturbation robustness in the metric score and can identify perturbed captions through anomaly detection only with a performance drop. In the cases of Vizwiz, Flickr8k, and M-FineCapEval, the performance is outstanding even though the distributions are not trained in fine-tuning.
%On the other hand, in the case of PR-MCS, the percentage drop for the perturbed caption is as low as 50\% to as high as 70\%. Clearly, it can be said that our proposed methodology contains perturbation robustness in the metric score and can find perturbed captions through anomaly detection only with a performance drop. In particular, in the case of Vizwiz, Flickr8k, and M-FineCapEval, the performance was outstanding even though the distributions were not trained in finetuning.

\subsection{Few-shot setting for M-FineCapEval}
As the results summarized in section~\ref{results} show, PR-MCS is much more robust to perturbation than the baselines in \dataset. However, the performance degradation for perturbed captions in \dataset is lower than for MSCOCO, VizWiz, and Flickr8k (e.g., -55.66\% to -39.89\% in average from MSCOCO). We attribute this to the distribution shift from the sequence length difference between the MSCOCO fine-tuning set and the \dataset test set. VizWiz and Flickr8k are composed of short captions, so there is not much difference in caption length between them and MSCOCO. Therefore, to check whether the distribution can be learned when some information about the \dataset 3K test set is provided, we perform additional experiments on \dataset with a few-shot setting. We split \dataset into subsets proportioned 1:9 in size and use only 300 perturbed captions as the few-shot input.
%As seen from the results of 5.3, the results show that PR-MCS is much more robust to perturbation than the baselines also in M-FineCapEval. However, it can be seen that the performance degradation for perturbed caption in M-FineCapEval is lower compared to the results of the COCO, VizWiz, and Flickr8k (e.g., -62.99\% to -40.548\% in DE from COCO). We consider the distribution shift from the sequence length difference between COCO fine-tuning set and the M-FineCapEval test set as a reason. In the case of VizWiz and Flickr8k, they are composed of short captions, so there is not much difference in caption length with COCO. Therefore, to check whether the distribution can be learned when some information of the M-FineCapEval 3K test set is provided, we perform additional experiments on M-FineCapEval with a few-shot setting. We split the M-FineCapEval into 1:9, and only 300 perturbed captions are used as a few-shot input. 

The experimental results for the few-shot setting are shown in Table~\ref{table2} and Figure~\ref{fig6} (the yellow line in rightmost graph). When the distribution for the fine-grained caption is given, the overall performance in perturbation detection, in terms of the average score, increases for all five languages. These results show that lexical noise in long sentences is more reliably captured by learning a small number of samples with a few-shot setting. The experimental results for all languages and all perturbations for each dataset are provided in the Appendix~\ref{appendix:all_results_table}.
%The experimental result for a few-shot setting is shown in Table 2 and Figure 6. When the distribution for the fine-grained caption is given, the overall performance of detecting perturbation increased for all five languages in terms of the average score. Therefore, it can be seen that lexical noise in long sentences is more reliably caught even by learning a small number of samples with a few-shot setting. Experimental results for all languages and all perturbations for each dataset can be seen in the appendix.


\input{resource/table3.tex}

\subsection{Correlations with human judgement}
Table~\ref{table3} shows that PR-MCS is an useful image captioning metric with high correlation with human judgment. 
Flickr8k\_Expert \cite{hodosh2013framing} and CapEval1k \cite{lee2021umic} are evaluation sets for measuring the performance of image captioning metric, and the higher the Kendall tau-c ($\tau_{c}$) value \cite{kendall1938new} and the Pearson correlation coefficient ($\rho$)~\cite{benesty2009pearson}, indicators for viewing the correlation with human judgment, the better. The Kendall tau-c value is the similarity between the two variables based on ranking, and the Pearson correlation coefficient is a measure of linear correlation between two sets of data.

We conduct experiments on the CLIPScore only in English, and for MCS and PR-MCS, we translate the two datasets into four languages and then report the average value. Due to the incompleteness and bias of the MT model, MCS shows slightly lower correlations with human evaluation compared to the CLIPScore. On the other hand, PR-MCS shows a higher correlations than MCS, or even similar to the CLIPScore, even though it has been granted perturbation robustness as shown in Section~\ref{results}. From these results, it can be seen that PR-MCS is a metric highly correlated with human evaluation while capturing perturbation well.




\iffalse
The experiment is conducted on two evaluation datasets : MSCOCO and \dataset. 
We first test our methodology on the existing short-sentence dataset using MSCOCO, and then on our fine-grained dataset.
The both evalsets' perturbed captions are configured by applying seven types of perturbation: Repetition, Removal, Masking, Repetition \& Removal, Random order word permutation (Jumble), Substitution in-sentence, and Substitution out-of-sentence.

\input{resource/table2}
\color{blue}
\subsection{Experiment details}
\subsubsection{Pre-training Details}
Like \citet{multilingualCLIP2022}'s method, our multilingual CLIP is learned by pre-training through teacher learning using MSE Loss as shown on the left of the Figure \ref{fig2}. Datasets used for pre-Training are GCC, VizWiz, and MSCOCO with total 2.2M sentences. For translation, each English sentence is translated into German, Spanish, French, and Japanese using MBART-50. The pre-trained CLIP model used as the teacher model is the RN50X4 model, and the Distill-Multilingual BERT is used as the trained text encoder.
\label{appendix:pretraining_details}

\subsubsection{Reproductability checklist}
The dataset used for finetuning is MSCOCO. We use the train and validation split of COCO dataset in \cite{chen2020uniter}. The number of the training set is 414k, and the validation set is 25k. To learn perturbation for each language, it is translated into four target languages  using MBART-50 model. The MSCOCO optimizer uses AdamW with a learning rate of 5e-5. Lambda values are set to 0.1 and 0.1, respectively, and 600K iteration is performed. The batch size is 32 for training. 

\color{cyan}
\subsection{Fine-tuning set configuration}
\label{appendix_fine-tuning_set_configuration}

\subsubsection{Fine-tuning set perturbation implementation}
Fine-tuning consists of 5 types of perturbation methods : Repetition, Removal, Masking, Random Order Permutation, Substitution in-sentence.
Basically, the implementation method of the five perturbation methods is the same as the MSCOCO Eval set configuration.
When performing fine-tuning, one perturbation method is selected for one batch. At this time, the probability is basically even and the probability is 20\%, and the experimental result when the ratio of substitution is increased is dealt with in the discussion section.


% \section{\dataset fine-tuning set configuration}

\subsubsection{\dataset~fine-tuning set perturbation implementation}
A fine tuning dataset is constructed using 300, which is 10\% of the~\dataset~eval set.
Each language of 300 data has 7 perturbed captions by eval set configuration.
At this time, since one perturbed caption is used for each language in one batch, one of the seven perturbed captions is randomly extracted.
Depending on which perturbation method is extracted, different combinations of batches can be created even with the same data.
Therefore, since several types of batches can be created from one data, we use the bootstrapping technique to create 10,000 finetuning datasets from 300 data sets.

\color{blue}

\subsection{Eval set configuration}
\label{appendix_eval_set_configuration}

\subsubsection{Eval set perturbation implementation}
There are 7 types of perturbation in MSCOCO and~\dataset~Eval Sets: Repetition, Removal, Masking, Repetition \& Removal, Random Order Permutation(Jumble), Substitution in-sentence, Substitution out-of-sentence.
The perturbation method is implemented in a way that changes the word token in the sentence.
In the case of Japanese, there is no spacing, so after separating the POS using the Spacy tool, each separated text is used as a word token.
First, in the case of Repetition, Removal, Masking, Repetition \& Removal, perturbation is performed with a 40\% probability for each word token.
In the case of Jumble, each word token is implemented by random permutation.
The implementation method of the two substitutions is different depending on the Eval set.
\dataset~is implemented using the critical objects given in the dataset.
In the case of substitution in-sentence, the critical objects in each sentence are randomly permuted.
In the case of substitution out-of-sentence, after making the critical object set of the entire eval set, the critical object of each sentence is randomly extracted from the set and replaced.
Since MSCOCO does not have a given critical object in the dataset, only nouns are selected using the Spacy Tool and used as critical objects.
Following step of substitution method is the same as \dataset.
The code will be released later on github.

\subsubsection{Eval set perturbed caption examples}
Perturbed captions examples of~\dataset~are shown in Figure \ref{fig6}-\ref{fig9}

\color{black}

\subsection{Baseline Metrics}
As the baseline of the experiment, we use two MCS metrics.
We create the first baseline MCS metric using multi-lingual CLIP model in \cite{multilingualCLIP2022} as a backbone and create the second baseline MCS using multi-lingual CLIP model implemented by ourselves.
Afterward, PR-MCS, which has been trained through the proposed finetuning loss, is compared with baseline metrics to verify the superiority of our methodology.

%As the baseline of the experiment, we use CLIPScore for English and two MultiLingual CLIPScore for other languages. First MultiLingual CLIPScore model is created by using MultiLingual CLIP in Fredrik's demo as a backbone and the second one uses MultiLingual CLIP implemented by ourselves. Afterwards, the metric trained with proposed finetuning method in this study was compared with baseline metrics to verify the superiority of our methodology.


\subsection{Experiment Results}
\noindent \textbf{MSCOCO 3k Eval set Results}
% MSCOCO 의 결과는 Figure3(Upper) 과 Table 2 에서 볼 수 있다. Figure 에는 모든 언어의 7 가지 perturbation 전부에 대한 결과를 graph 로 나타나고, table 에는 언어들마다 petrubation 의 average 값을 볼 수 있다. Figure2를 보면 Fredirk baseline은 파랑색으로, 우리의 베이스라인은 주황색으로 나타난다. 두 베이스라인 모든 언어의 모든 퍼터베이션에서 오리지널 캡션에 비해 큰 스코어 하락이 없는 것을 확인할 수 있다. 
The experimental results for the MSCOCO Eval set can be seen in the Figure \ref{fig4} (Upper) and Table \ref{table2}. In the Figure \ref{fig4}, the green line indicates PR-MCS and the blue and red lines refer to the two baseline Multi-Lingual CLIPScores.
%the blue and red lines indicate the two baseline MultiLingual CLIPScores 
%PR-MCS, which has been trained through the proposed finetuning loss, is indicated by a green line.
%Our metric, which has been trained through PR-MCS, is indicated by a green line.

%The graph of the figure shows how much difference in score each perturbation shows as a percentage drop compared to the original caption. 
The result shows how much percentage of score has dropped for each perturbation compared to the original caption.
The greater the negative absolute value of the percentage change, the stronger the degree of perturbation robustness. 
%As can be seen from the figure, for all perturbation methods, PR-MCS shows overwhelming performance.
Both baseline metrics do not show much difference from the original caption for all perturbation methods, or even in some cases, the scores of the perturbed captions are higher than those of original captions. 
%However, compared to the original caption, our metric shows an overwhelming score drop in all perturbations, so it can clearly distinguish sentences to which perturbation is applied.
However, our metric shows an significant score drop in all perturbations compared to the original captions, which means the metric can clearly distinguish to which perturbation is applied.
%Thus it can be seen that we have robustness for perturbations constituting the Eval set. 
In other words, our metric has robustness for all perturbations constituting the eval set.
%This results shows that our finetuning methodology is effective not only in English, but also in other four languages.
Also, the result shows the same tendency not only in English, but also in other four languages. 
%In other words, it shows that our finetuning methodology is effective in providing perturbation robustness in all the presented languages. 
%A full table of results of the experiment can be found in the appendix.
A full table of experiment results can be found in the appendix~\ref{appendix:all_results_table}. 

\noindent \textbf{\dataset~3k Eval set Results}
The experimental results for the~\dataset~3k Evaluation set introduced in this study can be checked in Figure \ref{fig4}(Lower) and Table \ref{table2}. 
%The $\dagger$ mark in the table is Fredrik Demo's MultiLingaul CLIP backbone score, and the * mark is the baseline metric we implemented.
%The result of Ours (Fine-tuned), which is located in the middle of each language in the Table \ref{table2}, is the result of the metric learned through the finetuning method we propose. 
%위에 문장은 PR-MCS로 바꿨으니까 빼도 될거같고
%The results of~\dataset~also show that our methodology is much more robust to perturbation than the baseline.
The results show that PR-MCS is much more robust to perturbation than the baselines also in \dataset.

However, it can be seen that the performance degradation for perturbed caption in~\dataset~is lower compared to the results of the MSCOCO(e.g. -62.99\% to -40.548\% in DE). % In the case of finetuning, the result of the validation after 50K steps shows that the degree of degradation for perturbation for each language is much higher than 50\%, whereas the result for the~\dataset~Test Set is not so.
%We thought that the reason was because of the distribution difference that comes from the sequence length difference between MSCOCO finetuning set and~\dataset~test set. 
We consider the distribution shift that comes from the sequence length difference between MSCOCO finetuning set and~\dataset~test set as a reason.
Therefore, in order to check whether the distribution can be learned when some information of the~\dataset~3K test set is provided, the~\dataset~3k test set is split into 1:9 and 0.3k set is used as an additional Finetuning dataset. 
At this time, the 0.3k training set of~\dataset~fine-tuning are bootstrapped to 10K datasize. 
A detailed implementation method for this can be found in the appendix~\ref{appendix_fine-tuning_set_configuration} and~\ref{appendix_eval_set_configuration}.

In the Table \ref{table2} and Figure \ref{fig4}, it shows the experimental results for our metric that learned additional finetuning. 
%When the distribution for fine-grained caption is given, the overall performance of detecting perturbation goes up. 
When the distribution for fine-grained caption is given, the overall performance of detecting perturbation increased for all five languages in terms of the average score. 
In particular, in case of English, the finedtuned metric overwhelms performance for all perturbation methods.
%In particular, in case of English, the metric shows much better results for all perturbations after finetuning.
%In the case of English, when~\dataset~finetuning is performed, it shows much better results for all perturbations, and if you check the average results of perturbations in other languages, it is more robust to perturbation after~\dataset~fine-tuning.
%In the case of English, when~\dataset~finetuning is performed, it shows much better results for all perturbations, and if you check the average results of perturbations in other languages, it is more robust to perturbation after~\dataset~fine-tuning.




\fi