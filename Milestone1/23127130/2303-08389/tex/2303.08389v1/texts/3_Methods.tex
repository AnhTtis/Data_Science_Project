
\subsection{MultiLingual CLIPscore}
\label{MCS}
We propose a new multilingual image captioning metric, \textbf{M}ultilingual \textbf{C}LIP\textbf{S}core (MCS), to overcome the limitation of CLIPScore that it can only be applied to English.
%We propose a new multi-lingual image captioning metric, Multi-lingual CLIPScore (MCS), to overcome the limitation of CLIPScore that it can be applied only to English. 
Recently, \citet{multilingualCLIP2022} proposed a multilingual CLIP model applicable to various languages by learning the expressive power of CLIP’s text encoder through teacher learning.
%Recently, Fredrik proposed a multi-lingual CLIP model applicable to various languages by learning the expressive power of CLIP’s text encoder through teacher learning. 
Similarly, in this study, we pre-train the multilingual text encoder with teacher learning (Figure~\ref{fig2} left).
%In this study, we pre-train the multi-lingual text encoder with teacher learning likewise (Figure 2 left). 
In teacher learning, the multilingual text encoder learns the pre-trained CLIP text embedding of an English sentence so that the embeddings of the sentence translated through the machine translation model are similar.
%In teacher learning, the multi-lingual text encoder learns the pre-trained CLIP text embedding of an English sentence so that the embeddings of the sentence translated through the machine translation model are similar. 
We use MSE loss as the teacher learning loss, and the formula is as follows:
%MSE loss is used as the teacher learning loss, and the formula is as follows.
$$ L(t,s) = \frac{1}{N}\sum_{i=0}^{n}(t-s)^{2}, $$

\noindent where $t$ is the teacher embedding and $s$ is the student embedding. More details on multilingual textual encoder pre-training can be found in the appendix~\ref{pretraining_details}.

We present a new multilingual image captioning metric, MCS, using this model as a backbone.
%Afterwards, we present a new multi-lingual image captioning metric MCS using this model as a backbone. 
MCS uses image–caption pairs with weight given to the cosine similarity of embeddings created through visual and text encoders, respectively.
%MCS uses image and caption pair by giving weight to cosine similarity of embeddings created through visual encoder and text encoder, respectively. 
The formula for an image–caption input pair $(I, c)$ in MCS is as follows:
%The formula for the input pair $(I, c)$ of the image and caption of MCS is as follows.
$$MCS {(I, c) = w * max( 0, cos(V(I), T(c))},$$

\noindent where $V(I)$ is the visual embedding where the image is passed through the visual encoder and $T(c)$ is the text embedding where the caption is passed through the multilingual text encoder. 
The value of $w$ is set to 2.5, as in the original CLIPScore paper.
%w is set to 2.5, as in the original CLIPScore paper.

\input{resource/Figure3.tex}

\subsection{Vulnerability to lexical perturbation}
We employed some of the perturbation criteria identified by \citet{sai2021perturbation} and checked the perturbation sensitivity of CLIPScore and MCS.
%We established some of the perturbation criteria defined in sai-et-al., and checked the perturbation sensitivity for CLIPScore and MCS. 
One of the criteria, \textit{"Repetition"}, is a perturbation in which words appear repeatedly at the token level in the original caption (e.g., “\textit{I am a boy.}” $\rightarrow$ “\textit{I am am a boy boy.}”).
%One of the criteria, \textit{Repetition}, is perturbation in which each word appears repeatedly at the token level in the original caption. (i.e “\textit{I am a boy.}” $\rightarrow$ “\textit{I am am a boy boy.}”)
The figure~\ref{fig3} shows what score is given by baseline metrics when repetitive lexical noise is introduced.
%The figure~\ref{tmp_figure} shows what score is given to each language by baseline metrics when repetitive lexical noise is given. 
We randomly selected 3,000 samples from the MSCOCO~\cite{lin2014microsoft} dataset, translated them into four languages, and gave repetitive lexical noise to the captions.
The blue bars are the score for the original captions, and the red bars are the score for the perturbed captions.
%The blue bar is the score for the original caption, and the red bar is the score for the perturbed caption. 
For English, CLIPScore is used as a metric, and for other languages, MCS is used for score extraction.
The caption to which lexical noise is added is expected to have a lower matching score with the image than the original caption.
%It is expected that the caption to which lexical noise is added has a lower matching score with the image compared to the original caption.
However, for all languages, the scores for the perturbed captions are not lower than those for the original captions. There are even cases in which the perturbed caption is given a higher score.
%However, for all languages, the perturbed captions are not given a different score compared to the original score, and there are even cases where the perturbed caption got a higher score. 
Similar tendencies can be observed for other perturbation criteria as well as \textit{"Repetition"}.
These results confirm that CLIPScore and MCS are limited in that they are vulnerable to lexical perturbation and that a metric that is robust to perturbation is needed.
%Through this, the limitation that CLIPScore and MCS are very vulnerable to lexical perturbation can be confirmed, and the need for a metric robust to perturbation arises.


\subsection{Perturbation-Robust Multilingual CLIPScore (PR-MCS)}
%\subsection{Perturbation Robustness Fine-tuning}
We introduce a novel language-agnostic perturbation method that increases the robustness of MCS.
%We introduce a novel language-agnostic perturbation methodology which aims to increase the robustness of MCS.  
This method of fine-tuning the multilingual text encoder is to add three losses to original CLIP loss.
%This is a method of finetuning the multi-lingual text encoder by adding contrastive losses that keeps the relationship between the image embedding and the original caption close while moving away from the perturbed caption.
The CLIPScore is constructed through embeddings of pre-trained CLIP without additional training. 
The CLIP loss $\mathcal{L}_{CLIP}$ consists of in-batch contrastive loss using cross-entropy loss, and the implementation is the same as the pre-training loss of CLIP.
We construct a loss based on the contrastive loss of CLIP to maintain the high correlation with the human judgment of CLIPScore.

Then, we train the text encoder by adding three additional losses for perturbation robustness. 
These losses aim to maintain the close relationship between the image embedding and the original caption while increasing the distance from the perturbed caption.
%First, the training set is constructed using five types of perturbation: Repetition, Removal, Masking, Random order permutation, and Substitution in-sentence. 
An (image, original caption, perturbed caption) triplet is then used as input to fine-tune the text encoder through three losses, as shown in Figure~\ref{fig2} (right). The losses are as follows:
%Afterward, using an (Image, Original Caption, Perturbed Caption) triplet as input, text encoder is finetuned through three losses, as shown in Figure 2(right). The losses are as follows.
%\vspace{-5pt}
\begin{align} 
    \mathcal{L}_{1}& = 1 - cos(V(I), T(o)) ,\label{equation:finetuning_loss1}\\
    \mathcal{L}_{2}& = max(0, cos(V(I), T(p)) ,\label{equation:finetuning_loss2}\\
    \mathcal{L}_{3}& = max(0, cos(T(o), T(p)), \label{equation:finetuning_loss3}
\end{align}
\noindent where $(I,o,p)$ is the (image, original caption, perturbed caption) triplet, $V$ is a visual encoder, and $T$ is a text encoder. 

Equation (\ref{equation:finetuning_loss1}) is composed of the cosine embedding loss of the two representations needed to increase the similarity between the image embedding and the original caption.
%Eq.~(\ref{equation:finetuning_loss1}) is composed of the cosine embedding loss of the two representations to increase the similarity between image embedding and the original caption. 
Since MCS is based on cosine similarity, the purpose of Eq. (\ref{equation:finetuning_loss1}) is to obtain a higher score for the original caption.
%Since MCS is based on cosine similarity, the purpose of the Eq.~(\ref{equation:finetuning_loss1}) is to make it show a higher score for the original caption. 
Equation (\ref{equation:finetuning_loss2}) reduces the cosine similarity of image embedding and perturbed caption embedding.
%Eq.~(\ref{equation:finetuning_loss2}) is designed to reduce the cosine similarity of image embedding and perturbed caption embedding. 
Equation (\ref{equation:finetuning_loss3}) reduces the similarity between the original and perturbed caption embeddings.
%Eq.~(\ref{equation:finetuning_loss3}) is to reduce the similarity between original and perturbed caption embeddings. 
These three losses are combined to obtain the final objective function as follows:
%by mixing these losses, the final objective function is as follows:
$$\mathcal{L} = \mathcal{L}_{CLIP} + \lambda_{1} * \mathcal{L}_{1} +  \lambda_{2} * \mathcal{L}_{2} + \lambda_{3} * \mathcal{L}_{3}.$$

\noindent We develop PR-MCS by fine-tuning Multilingual CLIP using the proposed loss function.
%\noindent Finally, we develop PR-MCS, perturbation robust MCS, by finetuning Multi-Lingual CLIP using the proposed loss function.
$$PR {\text -} MCS {(I, c) = w * max( 0, cos(V(I), T^{*}(c))},$$
\noindent where $T^{*}(c)$ is the text embedding from the fine-tuned multi-lingual text encoder. 
$w$ is also set to 2.5, as in the original CLIPScore and MCS.


\iffalse
%After learning the text encoder with good expressive power aligned with image embedding for four languages other than English through pre-training, fine-tuning is performed as shown on the right of the Figure \ref{fig2}.
% We introduce a novel language-agnostic methodology which aims to give perturbation robustness to Multi-Lingual CLIPScore.
We introduce a novel language-agnostic perturbation methodology which aims to increase the robustness of Multi-Lingual CLIPScore.
First, the training set is constructed using five types of perturbation : Repetition, Removal, Masking, Random order permutation, and Substitution in-sentence.
%Detailed implementation methods and examples of Perturbation Method for fine-tuning set configuration can be seen in the appendix.
%First, through the fine-tuning set configuration process, the original caption is changed to perturbed caption using one of the perturbation methods described above. 
Afterwards, using a \textit{(Image, Original Caption, Perturbed Caption)} triplet as input, fine-tuning is performed through three losses as shown in Figure \ref{fig2}(right).
%With these perturbed samples, three losses are constructed for fine-tuning as show in Figure \ref{fig2}.
The first loss is composed of the cosine embedding loss of the two representations to increase the similarity between image embedding and original caption:

$$Loss = Loss_{1} + \lambda_{1} * Loss_{2} + \lambda_{2} * Loss_{3}.$$
We develop PR-MCS by fine-tuning Multi-Lingual CLIP using the proposed loss function.


\noindent where $(I,o,p)$ is the (Image, original caption, perturbed caption) triplet, $V$ is a visual encoder, and $T$ is a text encoder. 
Since MCS is based on cosine similarity, the purpose of the first loss is to make it show a higher score for the original caption.
%The second loss function composes the objective function in the direction of reducing the cosine similarity of image embedding and perturbed caption embedding. The loss consists of cosine embedding loss between two representations.
The second loss is designed to reduce the cosine similarity of image embedding and perturbed caption embedding, and the third one is to reduce the similarity between original and perturbed caption embeddings.
Finally, mixing these three losses, the final objective function is as follows:
$$Loss = Loss_{1} + \lambda_{1} * Loss_{2} + \lambda_{2} * Loss_{3}.$$
We develop PR-MCS by finetuning Multi-Lingual CLIP using the proposed loss function.
%We then construct PR-MCS by fine-tuning MultiLingual-CLIP with this loss function.
\fi