\textbf{Image captioning metrics}
As with other natural language generation tasks, image captioning can be evaluated using various proposed metrics.
%As with other natural language generation tasks, various metrics have been proposed to evaluate the image captioning model. 
BLEU~\cite{papineni2002bleu}, ROUGE~\cite{lin2004rouge}, and METEOR~\cite{banerjee2005meteor} are representative image captioning metrics based on n-gram similarity with reference captions.
%BLEU, ROUGE, and METEOR, based on N-gram similarity with reference caption, are representative metrics of the image captioning task. 
Other widely used reference-based metrics include CIDEr~\cite{vedantam2015cider}, which weights n-gram similarity~\cite{kondrak2005n} through TF-IDF~\cite{aizawa2003information}, and SPICE~\cite{anderson2016spice}, which evaluates captioning based on scene graphs.
%Afterward, CIDEr, which weights N-gram similarity through TF-IDF, and SPICE, which evaluates captioning based on scene graphs, are proposed and widely used. 
%\textcolor{blue}{Recently, reference-based metrics such as BERTScore~\cite{zhang2019bertscore}, BERT-TBR~\cite{yi2020improving}, and VilBERTScore~\cite{lee2020vilbertscore}, which use embedding similarity with reference captions based on a model, have been introduced.}\\
Recently, reference-based metrics using embedding similarity with reference captions based on a model, such as BERTScore~\cite{zhang2019bertscore}, BERT-TBR~\cite{yi2020improving}, and VilBERTScore~\cite{lee2020vilbertscore}, have been introduced.

%Recently, metrics such as BERTScore, BERT-TBR, and VilBERTScore using the embedding similarity with reference based on the model have been introduced.\\
\indent Researchers have also proposed unreferenced image captioning metrics that evaluate generated captions by comparing them with original images.
%\indent Unlike these reference-based metrics, researchers have proposed reference-free metrics that sorely evaluate generated captions by comparing them with original images without reference caption. 
For instance, VIFIDEL~\cite{madhyastha2019vifidel} uses the word mover distance~\cite{kusner2015word} between the image and candidate caption, and UMIC~\cite{lee2021umic}, which fine-tunes UNITER~\cite{chen2020uniter} using contrastive loss from augmented captions, directly evaluates captions generated from vision-and-language embedding spaces.
%For instance, VIFIDEL uses the word mover distance between the image and candidate caption, and UMIC, fine-tunes UNITER using contrastive loss from augmented captions, directly evaluates generated caption from the vision-and-language embedding spaces. 
\\
\\
\noindent \textbf{CLIPScore} 
CLIPScore~\cite{hessel2021clipscore} is a reference-free metric that does not require ground-truth captions.
%CLIPScore is a reference-free metric that does not require a ground-truth caption. 
CLIPScore relies heavily on the CLIP~\cite{radford2021learning} model, trained with 400 million image caption pairs using a contrastive objective function that distinguishes original image–caption pairs from unmatched captions.
%CLIPScore heavily relies on the CLIP model trained with 400M image caption pairs using a contrastive objective function that distinguishes original image-caption pairs from unmatched captions. 
The calculated CLIPScore is the weighted value of cosine similarity between image embedding and text embedding encoded by the CLIP model.
%CLIPScore is the weighted value of cosine similarity between image embedding and text embedding encoded by the CLIP model. 
Thanks to the power of the massive training dataset and CLIP’s objective function, CLIPScore exhibits a high correlation with human evaluation.
%Thanks to the power of the massive training dataset and CLIP’s objective function, CLIPScore shows a high correlation with human evaluation. 
However, CLIPScore is limited in that it is an image captioning metric that applies only to English.
%However, CLIPScore has a limitation in that it is an image captioning metric that applies only to English. 
In this study, we propose a new multilingual image captioning metric developed by extending CLIPScore to a multilingual setting.\\
%In this study, we propose a new multilingual image captioning metric by extending CLIPScore to multi-lingual setting.\\

\noindent \textbf{Perturbation Robustness}
In a recent study,  \citet{sai2021perturbation} selected various criteria for use in assessing how various NLG evaluation metrics perform.
%In a recent study, Sai-et-al. selected various criteria for NLG evaluation metrics to investigate whether each metric reflects the criteria well. 
In addition, perturbation was applied to multiple image captioning factors to assess the perturbation robustness of the image captioning metrics.
%Also, for the image captioning, perturbation was given to multiple factors constituting the criteria of correctness and thoroughness to measure the degree of perturbation robustness of image captioning metrics. 
 \citet{sai2021perturbation} provided a perturbation checklist of metrics for NLG tasks; we go further and present a novel metric that overcomes the limitations of other metrics.
%Sai-et-al. has only provided a perturbation checklist of metrics for the NLG tasks, but we go beyond that and present a novel metric that overcomes that limitations. 
We select some perturbation criteria from among those suggested by  \citet{sai2021perturbation}, designate them as target perturbations, and show that the CLIPScore cannot detect these perturbations in multiple languages.
%We select some perturbation criteria among them and designate them as target perturbation, and show that these perturbations cannot be detected by the CLIPScore in multiple languages. 
Even if the generated captions are corrupted, CLIPScore outputs similar results for the original and corrupted sentences.
%Even if the generated captions are corrupted, CLIPScore outputs similar results for the original and corrupted sentences. 
This study proposes a novel metric with perturbation robustness based on CLIPScore to address its weaknesses in multiple languages.
%In this study, we propose a novel metric with perturbation robustness based on CLIPScore to solve weaknesses for multiple languages.



\iffalse
\textbf{Reference-based Metric.}
% As with other NLG tasks, various metrics have been proposed to evaluate Image Captioning. 
% BLEU\cite{papineni2002bleu}, ROUGE\cite{lin2004rouge}, and METEOR\cite{banerjee2005meteor}, which are based on n-gram similarity with reference caption, are representative metrics of the Image Captioning task. 
% Afterwards, CIDEr\cite{vedantam2015cider}, which weights n-gram similarity through TF-IDF, and SPICE\cite{anderson2016spice}, which evaluates captioning based on scene graph, are also well-known metrics.
% After the study of BERTScore\cite{zhang2019bertscore}, many studies on metrics based on the model were conducted.
% BERTScore uses BERT\cite{devlin2018bert} to evaluate caption by comparing contextualized embedding of generated caption and reference. The BERT-TBR\cite{yi2020improving} study based on variance in multiple hypotheses and the VilBERTScore\cite{lee2020vilbertscore} based on the VilBERT\cite{lu2019vilbert} model are also well-known model-based Image Captioning Metrics. \\
As with other NLG tasks, various metrics have been proposed to evaluate the Image Captioning model. 
BLEU~\cite{papineni2002bleu}, ROUGE~\cite{lin2004rouge}, and METEOR~\cite{banerjee2005meteor}, which are based on n-gram similarity with reference caption, are representative metrics of the Image Captioning task. 
Afterward, CIDEr~\cite{vedantam2015cider}, which weights n-gram similarity through TF-IDF, and SPICE~\cite{anderson2016spice}, which evaluates captioning based on scene graph, are proposed and widely used.
Recently, embedding-based metrics, which compare generated captions with ground captions on the vector space, show higher human correlation than previous n-gram-based metrics. One of the main advantages of these metrics lies in their robustness, free from lexical matching.
BERTScore~\cite{zhang2019bertscore} uses BERT~\cite{devlin2018bert} to evaluate caption by comparing contextualized embedding of generated caption and reference. The BERT-TBR~\cite{yi2020improving} consider variance in multiple hypotheses and the VilBERTScore~\cite{lee2020vilbertscore} uses multimodal information (i.e., image and caption) to compute its corresponding vector representation for comparision. \\


\noindent \textbf{Reference-free Metric.}
% All of the Image Captioning Metrics above require reference captions for performance evaluation.
% Unlike these, in the case of VIFIDEL\cite{madhyastha2019vifidel}, a metric using the word mover distance\cite{kusner2015word} of image and candidate caption, reference caption is not required. UMIC, based on the UNITER\cite{chen2020uniter} model, trained by contrastive loss presentes a model-based reference-free metric.
% Similar to UMIC\cite{lee2021umic}, our study proposes a metric with good performance that meets the purpose through fine-tuning of the Vision-and-Language transformer model. 
% However, unlike UMIC, our study proposes a new methodology to create a perturbation robust metric by giving perturbation robustness to the existing metric.
Researchers have proposed unreferenced Image Captioning metrics sorely evaluate generated captions by comparing them with original images.
For instance, VIFIDEL~\cite{madhyastha2019vifidel} uses the word mover distance~\cite{kusner2015word} between image and candidate caption and UMIC~\cite{lee2021umic}, finetunes UNITER~\cite{chen2020uniter} using contrastive loss from augmented captions, directly evaluate generated caption from the vision-and-language embedding spaces.
Similar to UMIC, our study proposes a metric with good performance that meets the purpose through fine-tuning the Vision-and-Language transformer model. 
However, unlike UMIC, our study proposes a new methodology to create a perturbation robust metric by training a model to distinguish differences in key information. We further extend its applicabilities to a multilingual domain.

\subsection{CLIPScore}
% CLIPScore is a reference-free metric, like UMIC, which does not require a reference and measures the score using the given image and caption. UMIC is a metric based on UNITER, and CLIPScore is a metric based on the CLIP model. For the CLIP model trained with a 400M image-caption pairs as a contrastive loss, the cosine similarity for the contrastive loss used during training is itself used as an evaluation metric for the similarity between the image and generated caption. CLIPScore is the weighted value for this cosine similarity between image embedding and text embedding, and in itself, it is a great image captioning metric that has a very high correlation with human evaluation.
% However, even if the generated captions are corrupted (for example, repetitive or attribute replacement occurs), CLIPScore outputs similar results for the original and corrupted sentences, showing weaknesses. In this study, we propose a novel method to solve this weakness by using the CLIPscore as the baseline but giving perturbation robustness. Additionally, by extending CLIPScore to multilingual, we propose a new multiLingual image captioning metric, and present a metric with robustness for perturbation in multilingual settings. 
CLIPScore~\cite{hessel2021clipscore} is a reference-free metric that does not require a ground-truth caption. CLIPScore heavily relies on CLIP~\cite{radford2021learning} model trained with 400M image-caption pairs using a contrastive objective function that distinguishes original image-caption pairs from unmatched captions.
CLIPScore is the weighted value of cosine similarity between image embedding and text embedding encoded by CLIP model. Thanks to the power of the massive training dataset and CLIP's objective function, CLIPScore shows a high correlation with human evaluation.\\
However, even if the generated captions are corrupted (for example, repetitive or attribute replacement occurs), CLIPScore outputs similar results for the original and corrupted sentences, showing weaknesses. In this study, we propose a novel method to solve this weakness by using the CLIPscore as the baseline but giving perturbation robustness. Additionally, by extending CLIPScore to multilingual, we propose a new multiLingual image captioning metric, and present a metric with robustness for perturbation in multilingual settings. 

\subsection{Perturbation Robustness}
In recent study,  \citet{sai2021perturbation} select various criteria for NLG Evaluation Metrics to investigate whether each metric reflects the criteria well. Also for Image Captioning, perturbation was given to various factors constituting the Criteria of Correctness and Thoroughness, and the degree of perturbation robustness of Image Captioning Metrics was measured. We select some perturbation methods among them and designate them as target perturbation, and show that this perturbation cannot be reflected in the existing metrics. In addition, we propose a new learning methodology and a metric with robustness for the selected perturbations.
\fi