To evaluate the performance of PR-MCS, we introduce a new evaluation set, \dataset. Most existing image captioning datasets are limited to English~\cite{young2014image, krishna2017visual}. Therefore, a model for machine translation (MT) from English to other languages is essential to evaluate image captioning in various languages. However, translated evaluation set is highly dependent on the performance of the MT model and is highly likely to have an English-language bias. In addition, translation is inarticulate in the target language because it is difficult to reflect the unique characteristics of each language, such as word order and lexical choice~\cite{zhang2019effect, cao2020multilingual}. Therefore, the results obtained using a translated evaluation set achieve poorer agreement with human evaluation than those obtained using the English evaluation set. For these reasons, a human-annotated image captioning evaluation set with a wide variety of languages is needed.\\
%To evaluate PR-MCS performance, we introduce a new evaluation set, M-FineCapEval. Most existing image captioning datasets are limited to English. Therefore, in order to evaluate in various languages, a translation process using the Machine Translation(MT) model from English to other languages is essential. However, this evaluation set is highly dependent on the performance of the MT model and is highly likely to have English-language bias. In addition, translation is inarticulate in the target language because it is difficult to reflect the unique characteristics of each language, such as word order or lexical choice. Therefore, the evaluation result using the translated evaluation set has a poorer agreement with the human evaluation compared to the English case because the performance is severely reduced. For these reasons, a human annotated image captioning evaluation set with a wide variety of languages is needed.\\

\noindent \textbf{Multilingual image captioning evaluation set} 
We introduce a new human-annotated multilingual evaluation dataset, \dataset. We extended FineCapEval~\cite{cho2022fine}, which has only English captions, to five languages (English, German, French, Spanish, and Japanese).
Human experts viewed the images for each language and added captions directly.
Each sentence generated directly by native speakers is more fluent than translated versions.
Moreover, \dataset can capture various cultural aspects that MT models cannot~\cite{liu2021visually}.
Therefore, the reliability of evaluation in multilingual settings increases. An example of the dataset is shown in Figure~\ref{fig4}. Human annotators for each language created a caption, critical objects, backgrounds, and relationships for a given image.\\
%We introduce a new human-annotated multi-lingual evaluation dataset, M-FineCapEval. We extended FineCapEval, which had only English captions, to five languages (English, German, French, Spanish and Japanese), and human experts viewed the images for each language and added captions directly. For sentences directly generated by native speakers, the variety of cultural aspects can be captured for which MT models cannot capture, and are more fluent than translated one. Therefore, the reliability of evaluation in multilingual setting increases. An example of the dataset can be seen in Figure 4. For a given Image, human annotators for each language created caption, critical object, background and relationship.\\

\input{resource/table1.tex}

\noindent \textbf{Fine-grained caption with critical objects}
To generate various perturbed captions for evaluation, \dataset is constructed with long fine-grained captions of 20 words or more. In addition, we had human experts point to critical objects, backgrounds, and relationships to create perturbed captions effectively. As described above, the image captioning metric should also reflect the semantic correspondence of whether the caption captures information contained in visual content well. The critical object of the caption should point to the most important object of this visual content, so it plays a key role in the comparison between embeddings. When perturbation is applied to this critical object, a more powerful and effective perturbation is achieved.
%In order to generate various perturbed captions for evaluation, M-FineCapEval is constructed with long fine-grained captions of 20 words or more. In addition, we had human experts point to critical objects, backgrounds, and relationships to effectively create perturbed captions. As described above, the image captioning metric should also reflect the semantic correspondence of whether caption captures information contained in visual content well. The critical object of caption should point to the most important object of this visual content, so it plays a key role in the comparison between embeddings. When perturbation is given to this critical object, it can be said that more powerful and effective perturbation is given.

However, the well-known weakness of CLIP is that it does not produce different results when the positions of critical objects in the sentence are changed. For example, the CLIP text embeddings of the two sentences “\textit{A \textbf{blue} car in front of the \textbf{white} church}” and “\textit{A \textbf{white} car in front of the \textbf{blue} church}” are almost identical. To evaluate the robustness to this perturbation, we construct perturbation criteria using critical object information.\\
%However, the well-known representative weakness of CLIP is that it doesn’t show any difference even when the positions of critical objects in the sentence are changed and the relationship to be modified is changed. For example, the CLIP text embeddings of two sentences “\textit{A \textbf{blue} car in front of the \textbf{white} church}” and “\textit{A \textbf{white} car in front of the \textbf{blue} church}” are almost similar. In order to evaluate the robustness of this perturbation, we construct perturbation criteria using critical objects information.\\

\noindent \textbf{Statistics}
Table~\ref{table1} provides detailed statistics for \dataset, including the dataset size for each language, the average sentence length, and the average numbers of critical objects, backgrounds, and relationships. M-FineCapEval consists of lengthy sentences of approximately 20 word tokens on average. In the case of Japanese, since there is no spacing in a sentence, sentence length is calculated using a tokenizer based on word extractor, and the sentence length is almost the same as the character level. In addition, there are three to four critical objects in all languages, so each sentence describes the visual content of an image in great detail.
%Table1 provides detailed statistics of M-FineCapEval, including dataset size per language, average length of sentence, the average number of critical objects, backgrounds, and relationships. On average, M-FineCapEval consists of long sentences of approximately 20 word-tokens. In the case of Japanese, since there is no spacing in sentence, it is calculated using a tokenizer using Word Extractor, and the length of the sentence is almost similar to the character level. There are three to four critical objects in all languages, so it can be seen that the sentence describes the visual content of the image in great detail.








\iffalse
To evaluate our methodology, we introduce the evaluation dataset, \dataset, that measures how robust the image captioning metric is with respect to perturbation. 
%If the metric is robust for perturbation, a high score will be given to the original caption that explains the image well, and a low score will be given to the perturbed caption, just as humans feel. 
%This confirms whether our methodology can measure perturbation robustness well.
Also, to prove the language-general characteristics of our methodology, we configure the dataset in five languages: English, German, Spanish, French, and Japanese. 
%For each language, CrowdWorker composes the caption as fine-grained.
%Additionally, in order to alleviate the issue of lack of dataset for multilingual image captioning, the dataset is configured by extending it to four languages other than English: German, Spanish, French, and Japanese. For each language, CrowdWorker directly observes the image and writes and composes the caption as fine-grained.
An example of the dataset can be seen in Figure \ref{fig3}.
\dataset~uses the images of GCC dataset, and crowdworkers compose the fine-grained captions, backgrounds, critical objects, and the relationships between the objects.
%\noindent \textbf{Eval set Statistics}
\input{resource/table1.tex}
\dataset~statistics are shown in table \ref{table1}.
On average,~\dataset~consists of over 20 words and each sentence has 3 to 4 critical objects.
%Fine-grained captions consist of, on average, over 20 words.
%Each sentence has 3 to 4 critical objects on average, so it is a high-quality dataset with fine captions that describe images.
In the case of Japanese, since there is no spacing, the length of a sentence is expressed as the number of characters that constitute a sentence.

We apply several types of perturbation to~\dataset~to figure out whether our metric detects them well.
Specifically, if perturbation is given using the critical objects of \dataset, the robust metric should yield a very low score for perturbed caption, since critical objects are  indispensable elements in describing the image.
%In this study, the evaluation set for perturbation robustness evaluation is constructed by generating various perturbed captions using the given critical objects.
%Unlike general perturbation, critical objects are literally indispensable elements in describing image, so the metric should yield a very low score for perturbed caption, where the critical objects are replaced or deleted with other words.

\color{blue}
\subsection{Why is Dataset Configured as Perturbation Robustness instead of Likert scale?}
The evaluation method for the existing metrics usually looks at the correlation between the metric score of the model and the score evaluated by a human. That is, the model-bias for configuring the eval set is too severe. 
Several studies have pointed it out~\cite{lee2021umic}, and we propose a new eval set for metric evaluation that evaluates perturbation robustness away from the problem of configuring the eval set for metric evaluation.\\


\subsection{Fine-grained Caption~(\dataset)}
%\noindent \textbf{

Our dataset,~\dataset,~consists of fine-grained captions with multiple critical objects and relationships (see the examples in Figure~\ref{fig5}). These fine-grained captions have the advantage of being able to generate various perturbed captions for evaluation by using many critical objects.
In addition, for follow-up study for multilingual image captioning metric, in the end, a high-quality dataset made by humans rather than a model for the candidate caption of the evaluation dataset is needed. Since the existing image captioning metric study only deals with short sentences like MSCOCO, the need for fine-grained captions is increasing. We collect captions in fine-grained form not only for measurement of perturbation robustness, but also for composing high-quality datasets that can be helpful in the future multilingual image captioning research. 


\subsection{Broader impact of~\dataset}
%\noindent \textbf{}

Our dataset presented as fine-grained proposed in this study can be used for generation performance evaluation in the multilingual image captioning model in the future. In addition to the fine-grained caption to confirm the perturbation, the backgrounds, critical objects, and the relationships between the objects are provided so that it can be used as a dataset for evaluating the factual consistency performance for the generated caption.

\color{black}
\fi