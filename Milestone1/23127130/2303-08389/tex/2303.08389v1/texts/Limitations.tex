\noindent \textbf{Model bias of machine translation in training}

\noindent In our study, an evaluation set is created by directly annotating languages other than English to remove the bias of the machine translation (MT) model in the evaluation phase.
However, in the training phase, the dataset size is too large to annotate directly in multiple languages other than English.
Therefore, the pre-training set and the fine-tuning set are translated into other languages by utilizing the MT model, so we have no choice but to depend on the performance of the MT model and avoid model bias.

A better model can be obtained if training is carried out using a large corpus with captions annotated in multiple languages corresponding to one image.
So far, such a multilingual multimodal large corpus is insufficient.
Recently, several large corpora having image-text pairs in various languages, such as LAION\_5B \cite{schuhmann2022laion}, have been introduced, but such sets have a structure in which text is matched in one language among several languages on one image.
In future work, we plan to extend our research by using these datasets to consider images as universal language representations.
