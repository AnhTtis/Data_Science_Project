%\section{Related works}
%\input{resource/Relatedworks}

\section{Pre-training Details}
Like \citet{multilingualCLIP2022}'s method, our multilingual CLIP is trained by pre-training through teacher learning using MSE loss as shown on the left of the Figure \ref{fig2}. The datasets used for pre-training are GCC \cite{wang2019learning}, VizWiz, and MSCOCO with total 2.2M sentences. Each English sentence is translated into German, Spanish, French, and Japanese using MBART-50. The pre-trained CLIP model used as the teacher model is the RN50X4 model, and the Distill-Multilingual BERT \cite{sanh2019distilbert} is used as the student text encoder. The model is trained in a total of 5 epochs, and it takes about 20 hours per epoch with our computing power . 
\label{pretraining_details}

\section{Experimental Details}
\subsection{Reproductabilty checklists} 
\noindent \textbf{Dataset and Source code} We provide our pre-training, fine-tuning, and evaluation source code along with configuration code for perturbations as supplementary materials.
We will publicly release our dataset~\dataset, and the full codes with weight parameters. \\

\noindent \textbf{Computing Resources} AMD Ryzen Threadripper 2950X (3.50 GHz) with GeForce GTX 2080 Ti is used for the experiments. All codes are implemented on Python 3.6.15 and PyTorch 1.7.1. The fine-tuning of each model trains 5 epochs, and takes about 6 hours per epoch.\\

\noindent \textbf{Number of Parameters} The number of parameter of our multilingual CLIP is about 66M as like as Distill-Multilingual BERT. \\

\noindent \textbf{Train-Valid-Test split} MSCOCO used for fine-tuning consists of 414k training set and 25k validation set. We split the training set by 9:1 and used it for fine-tuning and validation. We also randomly extracted 3k samples from the existing validation set and used it as a test set.

\subsection{Hyper-parameters}
\noindent \textbf{Hyper-parameters for fine-tuning} In order to find the best-performing model, we conducted an experiment on 16 hyper-parameter combinations ($\lambda_{1}:0\sim 0.5, \lambda_{2}:0\sim0.1, \lambda_{3}:0\sim0.1$). 
The hyper-parameter was manually tuned based on the effective detection of lexical noise while maintaining high human correlation, and finally, the best-performing $\lambda$ values of the objective function for fine-tuning are as follows:
$\lambda_{1}=0.1, \lambda_{2}=0.05, \lambda_{3}=0.05$. \\ 

\noindent \textbf{Hyper-parameters for optimizer} We use AdamW \cite{loshchilov2017decoupled} optimizer with $\beta_{1}=0.9,\beta_{2}=0.999, \epsilon=1e-8$. The initial learning rate is $5e-5$.

\section{\dataset eval set examples}
The examples of the \dataset eval set for languages other than English can be seen in Figure~\ref{fig7}.
\begin{figure}[h]
\centering
\includegraphics[width=1.0\columnwidth]{resource/ex1-4.pdf} 
\caption{\dataset~eval set examples for each languages.}
\label{fig7}
\end{figure}

% \section{Eval set Examples}

% \begin{figure}[h]
% \centering
% \includegraphics[width=1.0\columnwidth]{resource/ex1-4.pdf} 
% \caption{\dataset~eval set examples for each languages.}
% \label{fig5}
% \end{figure}
% \clearpage

\section{Perturbed caption examples}
The examples of the perturbed captions for languages other than English can be seen in Figure~\ref{fig8}-Figure~\ref{fig11}.
The critical objects shuffled for in-sentence substitution perturbation are displayed using each color.

\section{Implementation Details}
In Alg.~\ref{alg1}, we show the Python implementation of each perturbation criterion: \textit{"Repetition"}, \textit{"Removal"}, \textit{"Masking"}, \textit{"Jumble"}, and \textit{"Substitution"}.

\section{All results tables}
\label{appendix:all_results_table}

\noindent \textbf{MSCOCO}
The results for all perturbation of all languages for MSCOCO 3k eval set can be found in Table \ref{table4}.\\
\noindent \textbf{Flickr8k}
The results for all perturbation of all languages for Flickr8k eval set can be found in Table \ref{table5}.\\
\noindent \textbf{VizWiz}
The results for all perturbation of all languages for Vizwiz eval set can be found in Table \ref{table6}.\\
\noindent \textbf{\dataset}
The results for all perturbation of all languages for~\dataset eval set can be found in Table \ref{table7}.

\input{resource/alg1.tex}
\clearpage

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{resource/evalset1.pdf} 
\caption{Eval set perturbed captions example (FR).}
\label{fig8}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{resource/evalset2.pdf} 
\caption{Eval set perturbed captions example (DE).}
\label{fig9}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{resource/evalset3.pdf} 
\caption{Eval set perturbed captions example (JA).}
\label{fig10}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{resource/evalset4.pdf} 
\caption{Eval set perturbed captions example (ES).}
\label{fig11}
\end{figure}

\clearpage
\input{resource/table4}
\input{resource/table5}
\input{resource/table6}
\input{resource/table7}
