Image captioning~\cite{xu2015show, vinyals2015show, vinyals2016show, lu2017knowing} is a multimodal task that automatically generates captions that describe the visual content of an image and integrates multiple disciplines of visual and textual modality.
%Image captioning is a multimodal task that automatically generates captions that describe the visual content of an image, and integrates multiple disciplines of visual modality and textual modality. 
Image captioning is a natural language generation (NLG) task~\cite{gatt2018survey}, but the evaluation metric has different characteristics from other NLG metrics~\cite{sai2022survey}.
%Image captioning is a kind of natural language generation (NLG) task, but evaluation metric has different characteristics from other NLG metrics. 
Image captioning metrics should be able to evaluate not only linguistic fluency and syntactic thoroughness but also semantic correspondence to visual content\cite{bai2018survey}.
%Image captioning metric should be able to evaluate not only linguistic fluency and syntactic thoroughness, but also semantic correspondence to visual content.
\input{resource/figure1.tex}
Evaluation criteria for image captioning have evolved from N-gram-based metrics \cite{papineni2002bleu, lin2004rouge, banerjee2005meteor, vedantam2015cider} to reference-free metrics~\cite{lee2020vilbertscore, lee2021umic, hessel2021clipscore}.
%Evaluation criteria for image captioning have been evolving from N-gram-based metrics to reference-free metrics. 
Recently, CLIPScore \cite{hessel2021clipscore} has been proposed to leverage the large-scale pretrained vision and language model CLIP~\cite{radford2021learning}.
%Recently, CLIPScore has been proposed to leverage large-scale pretrained vision and language model CLIP.
In evaluating generated captions by computing cosine similarity between embedded vectors (i.e., image and text) using CLIP, CLIPScore achieves a higher correlation with human judgments than traditional metrics.
%In evaluating the generated caption by computing cosine similarity between embedded vectors (i.e., image and text) using CLIP, CLIPScore achieves a higher correlation with human judgments compared to the traditional metrics. 

However, \citet{sai2021perturbation} have revealed that current metrics are prone to failure in capturing lexical noise in generated captions.
%However, Sai-et-al. have revealed that current metrics are prone to fail at capturing lexical noise in the generated captions. 
For example, when a perturbation is applied to an original caption (e.g., a removal or swap at the token level), existing image captioning metrics do not recognize the change and compute a score similar to that for the original caption case.
%For example, when a perturbation is applied to the original caption (e.g., removal or swap on token level), existing image captioning metrics do not recognize the change and compute a similar score to the original caption case. 
This failure to capture lexical noise raises a critical question concerning the reliability of the metric, as shown in the example in Figure~\ref{fig1}.
%It raises a critical question on the reliability of the metric, as shown in the example in Figure 1. 
CLIPScore exhibits the same tendency in our analysis, which reflects its vulnerability to perturbed texts.
%CLIPScore also shows the same tendency in our analysis,  which stresses its vulnerability to perturbed texts. 
By extending CLIPScore to a multilingual setting, we observe that a multilingual CLIPScore exhibits the same limitations in multiple languages other than English, i.e., French, German, Spanish, and Japanese.
%By extending CLIPScore to multilingual setting, we could also demonstrate that multi-lingual CLIPScore exposes the same limitations in multiple languages other than English: French, German, Spanish and Japanese. 

In this paper, we address this problem by proposing a novel method for enhancing the perturbation robustness of CLIPScore.
%In this paper, we tackle this problem by proposing a novel methodology that allows perturbation robustness to CLIPScore. 
Our method is to fine-tune the text encoder of CLIP with perturbed captions so that the text encoder can distinguish the perturbed text embeddings from the original text embeddings.
%Our method is to fine-tune the text encoder of CLIP with the perturbed captions so that the text encoder can distinguish the perturbed text embeddings from the original text embeddings. 
The simplicity and effectiveness of our method enable us to apply it to multiple languages without relying on human annotations.
%The simplicity and effectiveness of our method enables us to apply it into multiple languages without the engagement of human annotations. 
Using our method, we develop Perturbation-Robust Multilingual CLIPScore (PR-MCS), a perturbation-robust and language-agnostic metric for image captioning.
%By using our methodology, we develop a perturbation robust and language-agnostic metric for image captioning: Perturbation Robust Multi-lingual CLIPScore (PR-MCS). 
Experimental results show that PR-MCS outperforms baseline metrics in capturing lexical noise in captions in all five languages considered.
%Experimental results show that PR-MCS vastly outperforms in capturing lexical noise of captions compared to the baseline metrics in all five languages.


Furthermore, we create a new multilingual fine-grained caption evaluation dataset, M-FineCapEval, to validate the perturbation robustness of PR-MCS.
%In addition, we create a new multi-lingual fine-grained caption evaluation dataset, M-FineCapEval, to validate the perturbation robustness of PR-MCS. 
Currently, most of the image captioning evaluation datasets are limited to  English, so a translation process using machine translation~\cite{bahdanau2014neural, johnson2017google} is essential for multilinguality experiments.
%Currently, most of the image captioning evaluation datasets are limited to English, so a translation process using machine translation is essential for multi-linguality experiments. 
However, this process depends on the performance of the translation model, so there is a disadvantage in that the evaluation reliability is low compared to human-annotated labels.
%However, this process has to depend on the performance of the translation model, so there is a disadvantage in that the reliability of evaluation is low compared to human annotated labels.
Accordingly, we ask human experts to describe captions in the five languages mentioned above. In addition, to provide various types of effective lexical noise to the caption, the caption was configured as fine-grained, and the experts were instructed to point out the critical objects in the caption.
%Accordingly, we ask human experts to describe captions in the five languages mentioned above. In addition, to provide various and effective lexical noise to the caption, the caption was configured as fine-grained, and the experts were instructed to point out the critical objects in the caption. 
%We will release this dataset along with our code.
We have made our code publicly available\footnote{https://github.com/yong1-kim/PR-MCS}.

Our contributions are as follows:
%Namely, our contributions are as follows :
\begin{itemize}
\item We develop a language-agnostic method for fine-tuning the CLIP text encoder to capture lexical perturbations.
%\item We develop a language-agnostic methodology that fine-tunes the text encoder of CLIP to capture lexical perturbations.
\item We propose PR-MCS, a perturbation-robust metric for image captioning in multiple languages.
%\item We propose PR-MCS, which can serve as a perturbation robust metric for image captioning in multiple languages.
\item We introduce a fine-grained multilingual image captioning evaluation set, M-FineCapEval, to demonstrate the performance and robustness of our method.
%\item We introduce a fine-grained multi-lingual image captioning evaluation set, ~\dataset~, to demonstrate our methodology.
\end{itemize}


\iffalse
% Image Captioning, which aims to automatically describe the contents of an image, has made tremendous progress with a lot of attention. 
% To evaluate the quality of the generated captions, many evaluation metrics have been proposed.
% Among them, the model-based reference-free metrics that measure scores using image and caption embeddings from Vision-and-Language Transformer show better performance than the traditional reference-based metrics with respect to correlation with human evaluation.

%Image captioning task, which aims to generate a description of a given image, has made tremendous progress with a lot of attention.
%To evaluate the performance of image captioning models along with their generating captions, researchers have investigated n-gram-based text overlap matching metric using reference- and generated- captions~\cite{papineni2002bleu, lin2004rouge, banerjee2005meteor, vedantam2015cider, lee2020vilbertscore}. 
%Unlike these methods that heavily rely on lexical matching between texts, recent studies propose reference-free metrics that directly compute similarity scores between the target image and generated caption~\cite{lee2021umic, hessel2021clipscore}. 
%These unreferenced metrics (i.e., CLIPScore) leveraged from vision-and-language pretrained models show better performance than traditional reference-based metrics achieving a higher correlation with human judgements.

Evaluation criteria for Image Captioning have been evolving from N-gram based metrics \cite{papineni2002bleu, lin2004rouge, banerjee2005meteor, vedantam2015cider} to reference-free metrics \cite{lee2020vilbertscore, lee2021umic, hessel2021clipscore}.
Recently, CLIPScore \cite{hessel2021clipscore} is proposed to leverage large-scale pretrained vision and language model CLIP \cite{radford2021learning}.
In evaluating the generated caption by computing cosine similarity between embedded vectors (i.e., image and text) using CLIP, CLIPScore achieves a higher correlation with human judgements compared to the traditional metrics.


However, \citet{sai2021perturbation} have revealed that current metrics are prone to fail at capturing lexical noise in the generated captions.
%For example, when a perturbation is applied to the original caption (\textit{e.g.} removal or swap on token-level), the metrics tend to output a score very close to the score of the original text even when the perturbation happens on the critical phrases as shown in Figure \ref{fig1}.
% 
For example, when a perturbation is applied to the original caption (\textit{e.g.} removal or swap on token level), existing image captioning metrics do not recognize the change and compute a similar score to the original caption case. It raises a critical question on the reliability of the metric, as shown in the example in Figure \ref{fig1} (i.e., critical phrases are perturbed).
%CLIPScore also shows the same tendency in our analysis, hence it risks very poor performance in perturbed texts.
%CLIPScore also shows the same tendency in our analysis, which means it takes risk when perturbed texts are given as an input. 
%CLIPScore also shows the same tendency in our analysis, which means it is vulnerable to perturbed texts.
CLIPScore also shows the same tendency in our analysis (Sec.~\ref{sec:experiments}), which stresses its vulnerability to perturbed texts.
% However, recent study\cite{sai2021perturbation} show that reference-based as well as reference-free metrics are able to evaluate well only for immaculate captions.
% For the main criteria by which a person evaluates captions, the existing metrics are not accurate for captions with criteria-specific changes applied. 
% If perturbation is applied to the original caption, it should be judged that such caption does not describe the image well. 
% In other words, a robust perturbation metric should output a low score for this, but most existing metrics do not capture this perturbation well and produce a score almost similar to the original input as shown in Figure \ref{fig1}.
%A recent study\cite{sai2021perturbation} reveals that current evaluation metrics assume immaculate captions and are not robust to lexical noise. 
%For example, when a perturbation is applied to the original caption, such as removing a salient phrase or swapping the phrase, the differences do not apply to the evaluation outcome producing a similar score to the original caption, as shown in Figure~\ref{fig1}.
% In this paper, we want to tackle this problem by present a novel language-general methodology that gives perturbation robustness to Transformer-based reference-free metric. 
% The metric with perturbation robustness only for English may reflect the specific characteristics of English, so it may be limited to the methodology with language bias. Hence, we present a multilingual methodology that can be effectively applied in various languages.
%To overcome the weakness for such noise, we propose a novel methodology that allows perturbation robustness to CLIPScore. 

In this paper, we tackle this problem by proposing a novel methodology that allows perturbation robustness to CLIPScore.
Our method is to fine-tune the text encoder of CLIP with the perturbed captions in the way that the text encoder can distinguish the perturbed text embeddings from the original text embeddings.
The simplicity and effectiveness of our method enable us to extend CLIPScore into multiple languages without the engagement of human annotations, including English, French, German, Spanish, and Japanese.
Finally, we develop a perturbation robust and language-agnostic metric for Image Captioning: \textbf{P}ertubation \textbf{R}obust \textbf{M}ulti-lingual \textbf{C}LIP\textbf{S}core \textbf{(\ours)}.
Experimental results show that \ours ~greatly outperforms in capturing perturbation of captions compared to the original CLIPScore in all five languages.
%This paper aims to tackle this problem by proposing a novel methodology that gives perturbation robustness to a Transformer-based reference-free metric.
%By this methodology, we develop a new metric : \textbf{P}urtubation \textbf{R}obust \textbf{M}ultilingual \textbf{C}LIP\textbf{S}core \textbf{(\ours)}. % by extending the English CLIPScore to Multilingual.
%Unlike previous research that develops English-only image captioning evaluation metrics, we further present a language-agnostic method that can be effectively applied in various languages.

In addition, we create a new \textbf{M}ultilingual \textbf{Fine}-grained \textbf{cap}tion \textbf{eval}uation dataset,~\textbf{\dataset}~, to validate perturbation robustness of \ours.
To construct~\dataset, we ask human experts to describe fine-grained captions from given 3k images as well as backgrounds, critical objects, and relationships between the objects in the aforementioned five languages. 
Thus,~\dataset~not only performs its role as a challenging test suite for multi-lingual Image Captioning metrics, but it is also likely to be used in various multi-lingual tasks.
%However, it is hard to evaluate our perturbation robustness methodology due to the lack of datasets and metrics in multilingual environments other than English.
%Therefore, we create a new fine-grained multilingual evaluation dataset for measuring perturbation robustness.
%This dataset consists of five languages: English, French, German, Japanese, and Spanish. 
%Also, since captions are fine-grained directly generated by crowd workers, metrics are even more challenging to understand and correctly evaluate captions. 
%In addition to fine-grained captions, the dataset provides backgrounds, critical objects, and relationships between the objects. 
%Therefore, it is expected that it can be used in various ways for evaluating generation performance in multilingual image captioning tasks, which has recently been in the spotlight but lacks datasets and metrics.

%As a result of the experiment, \ours~proved to be very effective for all five languages.
%As shown in Figure \ref{fig1}, our metric automatically detects perturbed captions and give them very low scores compared to the original caption.

Namely, our contributions are as follows :
\begin{itemize}
\item We develop a language-agnostic methodology to fine-tune the text encoder of CLIP to capture lexical perturbations.
%\item We present the concept of extending the English clip score to the multilingual clip score.
\item We propose \ours~ which can serve as a perturbation robust metric for Image Captioning in multiple languages.
%\item We introduce the multilingual perturbation robustness dataset to evaluate our metric.
\item We introduce a fine-grained image caption evaluation set~\dataset~which describes detailed captions, objects, relationships between objects for given images.
\end{itemize}
\fi



\iffalse
Recently, research on the Natural Language Generation(NLG) system that automatically generates natural language sentences using deep learning is being actively conducted.
Image Captioning, one of the tasks of the NLG system, aims to create a caption that describes the image given as input.
Various captioning algorithms~\cite{vinyals2015show, anderson2018bottom, li2020oscar} and datasets~\cite{fang2015captions, sharma2018conceptual} have been proposed with great interest, but as with other NLG systems, research to evaluate the generated captions is still lacking.
Due to this need, recent studies~\cite{madhyastha2019vifidel, lee2021umic, hessel2021clipscore} proposes an automatic reference-free metric based on a powerful transformer model to evaluate image captioning tasks.
These reference-free metrics show better performance than the existing traditional reference-based metrics in correlation with Human Evaluation.
However, these metrics can well evaluate whether it describes the image well only for neatly given captions. 
If the given caption is not immaculate, it will not produce the desired result.
For example, if perturbation is applied to the original caption as in the example in Figure \ref{fig1}, it should be judged that such caption does not describe the image well. 
%\input{resource/figure1.tex}
Therefore, a metric with robustness to perturbation is expected to predict a very low score for the perturbed caption, but most existing metrics do not capture this perturbation well and produce a score almost similar to the original input.   
In this study, we present a novel methodology that gives perturbation robustness to such Transformer-based reference-free metrics. 
Our model-based metric learned through the proposed methodology automatically detects perturbation by giving a very low score compared to the original caption. 
In other words, our methodology is very robust to various perturbations compared to the existing metrics. 
Additionally, to verify this, we present a new fine-grained evaluation dataset for measuring perturbation robustness.
%perturbation 의 정의가 먼저

In addition, we extend our study to a multilingual case and propose a Multilingual Perturbation Robust Metric to evaluate image captioning for four languages other than English. 
Currently, there is no research on Multilingual Image Captioning that directly creates captions in target languages other than English.
Therefore, the method of translating into the target language through Machine Translation after generating captions in English is the most well-performing so far.
However, since this way has to go through two systems, it is less effective to directly perform captioning in target languages other than English in terms of credibility and computational cost.
Hence, research on MultiLingual Captioning should be developed in the future. 
Still, as far as we know, even the Dataset and Evaluation Metric for MultiLingual Image captioning do not exist at all.
As a cornerstone for MultiLingual Image Captioning research, we present MultiLingual CLIPScore, a reference-free metric, by extending CLIPScore~\cite{hessel2021clipscore} to a MultiLingual setting that can be evaluated for four languages other than English. 
In addition, by extending our fine-grained dataset proposed to a multilingual environment, an advanced metric with perturbation robustness for each language is presented.

Through many experiments, it is confirmed that our methodology has perturbation robustness in English as well as in other languages. 
As a result of the experiment, the existing metrics show similar performance for the original caption and the perturbed caption. 
On the other hand, the metric to which our methodology is applied shows clear score degradation in perturbation, confirming that it has the robustness to perturbation. 
We numerically demonstrate that our proposed methodology is a good way to meet the objectives of the study by showing significant numerical differences with baselines.

Moreover, the fine-grained dataset proposed in this study can be used for evaluating generation performance in the MultiLingual Image Captioning model in the future.
In addition to fine-grained captions, it provides critical objects, backgrounds, and relationships between objects so that it also can be used as a dataset evaluating factual consistency for later.

Namely, our contributions are as follows :
\begin{itemize}
\item We present a metric with robustness for perturbation.
\item By extending to the multilingual environment where the data is insufficient, we propose a metric with perturbation robustness characteristics in multiLingual image Captioning as well.
\item We present the multilingual perturbation robustness dataset to evaluate our metric.
\end{itemize}
\fi