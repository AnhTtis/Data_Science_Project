
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} %

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{dsfont}
\usepackage{mathtools}
\usepackage{bm}

\usepackage{multirow}
\usepackage{colortbl}
\usepackage{amssymb}%
\usepackage{pifont}%
\usepackage{colortbl}
\usepackage[font=small]{subcaption}
\usepackage{enumitem}
\usepackage{hhline}
\usepackage{bbold}
\usepackage{algorithmic}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage[accsupp]{axessibility}
 \usepackage{balance}







\newcommand{\xmark}{\ding{55}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\bestresult}[1]{\textbf{\textcolor{red}{#1}}}
\newcommand{\secondbest}[1]{\textcolor{blue}{\underline{#1}}}
\newcommand{\supdagger}{\textsuperscript{\textdagger}}
\newcommand{\supstar}{\textsuperscript{\textasteriskcentered}}

\newcommand{\venueTT}[1]{{$_{\texttt{\text{#1}}}$}}
\newcommand{\venue}[1]{{$_{{\text{#1}}}$}}


\newcommand{\MS}[1]{{\color{blue}  MS: #1}}
\newcommand{\CC}[1]{{\color{green}  CC: #1}}
\newcommand{\NR}[1]{{\color{cyan}  NR: #1}}
\newcommand{\ID}[1]{{\color{red}  ID: #1}}

\newcommand{\supp}[1]{{\color{blue}  #1}}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{1154} %
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition}


\author{Ishan Rajendrakumar Dave, Mamshad Nayeem Rizve,  Chen Chen, Mubarak Shah\\
Center for Research in Computer Vision, University of Central Florida, Orlando, USA\\
{\tt\small \{ishandave, nayeemrizve\}@knights.ucf.edu,  \{chen.chen, shah\}@crcv.ucf.edu}}

\maketitle

\begin{abstract}




Semi-Supervised Learning can be more beneficial for the video domain compared to images because of its higher annotation cost and dimensionality. Besides, any video understanding task requires reasoning over both spatial and temporal dimensions. In order to learn both the static and motion related features for the semi-supervised action recognition task, existing methods rely on hard input inductive biases like using two-modalities (RGB and Optical-flow) or two-stream of different playback rates. Instead of utilizing unlabeled videos through diverse input streams, we rely on self-supervised video representations, particularly, we utilize temporally-invariant and temporally-distinctive representations. We observe that these representations complement each other depending on the nature of the action. Based on this observation, we propose a student-teacher semi-supervised learning framework, TimeBalance, where we distill the knowledge from a temporally-invariant and a temporally-distinctive teacher. Depending on the nature of the unlabeled video, we dynamically combine the knowledge of these two teachers based on a novel temporal similarity-based reweighting scheme. Our method achieves state-of-the-art performance on three action recognition benchmarks: UCF101, HMDB51, and Kinetics400. Code: \url{https://github.com/DAVEISHAN/TimeBalance}.



\end{abstract}

\section{Introduction}


Recent development in action recognition have opened up a wide range of real-world applications: visual security systems~\cite{gabv2, rizve2021gabriella,cmu2020}, behavioral studies~\cite{behaviour}, sports analytics~\cite{li2021multisports}, elderly person fall detection systems~\cite{buzzelli2020vision, zhang2012privacy, liu2020privacy}, etc. Most of these developments are mainly courtesy of large-scale curated datasets like Kinetics~\cite{kinetics}, HVU~\cite{hvu}, and HACS~\cite{hacs}. However, labeling such a massive video dataset requires an enormous amount of annotation time and human effort. 
At the same time, there is a vast amount of unlabeled videos available on the internet. The goal of semi-supervised action recognition is to use such large-scale unlabeled dataset to provide additional supervision along with the labeled supervision of the small-scale dataset.




\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{Figures/semi_motivation_try6.drawio.pdf}
    \caption{\textbf{Motivation for Temporally-Distinctive and Temporally-Invariant  Representations.} In order to leverage the unlabeled videos effectively, we consider two kinds of self-supervised video representation learning techniques with complementary goals: (1) \textit{Temporally Invariant Representations} (\texttt{Bottom Right}) encourage learning the commonalities of the clips, hence it mainly focuses on learning features related to highly frequent repetitions and appearance. (2) \textit{Temporally Distinctive Representations} (\texttt{Bottom Left}) encourage learning the dissimilarities between clips of the same video, hence it encourages learning features for sub-actions within the video. The plot shows the activity-wise UCF101 performance difference of finetuned models which were self-supervised pretrained with temporally-distinctive and temporally-invariant objectives. The plot shows extreme 25-25 classes after sorting the all classwise differences.}
    \label{fig:motivation}
\end{figure*}



Semi-supervised learning for image classification has seen tremendous progress in recent years~\cite{tarvainen2017mean,Shi_2018_ECCV,arazo2020pseudo,ups}. In semi-supervised action recognition, recent approaches have adapted these image-based methods by incorporating motion-related inductive biases into the setup.
For instance, some methods~\cite{semi_tgfixmatch, semi_mvpl} use two different input modalities where the original RGB video promotes learning appearance-based features while optical flow/temporal gradients promotes learning of motion-centric features. Another set of methods uses input streams of different sampling rates to achieve this~\cite{semi_tcl, semi_tacl}. Although these input-level inductive biases are simple-yet-very-effective to provide unlabeled supervision for action recognition, they are not suitable for large-scale datasets due to their multiplicative storage requirement and high preprocessing overhead. 





Contrastive Self-supervised Learning (CSL) has emerged as a powerful technique to learn meaningful representations from unlabeled videos. Existing video CSL methods deal with mainly two different kinds of objectives: (1) Learning similarities across clips of the same video i.e \textit{temporally-invariant} representations~\cite{cvrl, byol, videomoco} (2) Learning differences across clips of the video i.e. \textit{temporally-distinctive} representations~\cite{jenni2021time, tclr, dsm}. 
Each objective has its own advantages, depending on the nature of the unlabeled videos being used. 

Our experiments reveal a clear difference in the classwise performance of both methods, as illustrated in Fig.~\ref{fig:motivation}.
The right half of the figure shows the action classes where the temporally-invariant model is dominant. We can observe that all such action classes are atomic actions with high repetitions, \eg, \texttt{Fencing}, \texttt{Knitting}. Any two clips from such videos are highly similar, hence, increasing agreement between them i.e. learning \textit{temporal-invariant} representation is more meaningful. The left half of the figure shows action classes where temporally-distinctive representations perform better. We can observe that such action classes are slightly more complex i.e. they contain sub-actions, \eg, \texttt{JavelinThrow} first involves running and then throwing. Any two clips from such videos are visually very different, hence if we maximize agreement between them then it results in loss of the temporal dynamics. Therefore, \textit{temporally-distinctive} representation is more suitable in such videos.





Based on our observation, we aim to leverage the strengths of both temporally-invariant and temporally-distinctive representations for semi-supervised action recognition. To achieve this, we propose a semi-supervised framework based on a student-teacher setup. The teacher supervision includes two models pre-trained using CSL with temporally-invariant and temporally-distinctive objectives. After pre-training, the teachers are fine-tuned with the labeled set to adapt to the semi-supervised training of the student. During semi-supervised training, we weigh each teacher model based on the nature of the unlabeled video instance. We determine the nature of the instance by computing its similarity score using the temporal self-similarity matrices of both teachers. This way, the student is trained using the labeled supervision from the labeled set and the unlabeled supervision from the weighted average of the teachers. It is worth noting that our framework doesn't depend on complicated data-augmentation schemes like FixMatch~\cite{fixmatch}.









\noindent{The contributions of this work are summarized as follows:}
\setlist{nolistsep}
\begin{itemize}

\item We propose a student-teacher-based semi-supervised learning framework that consists of two teachers with complementary self-supervised video representations: Temporally-Invariant and Temporally-Distinctive. 

\item In order to leverage the strength of each representation (invariant or distinctive), we weigh a suitable teacher according to an unlabeled video instance. We achieve it by the proposed temporal-similarity-based reweighting scheme. 
\item Our method outperforms existing approaches and achieves state-of-the-art results on popular action recognition benchmarks, including UCF101, HMDB51, and Kinetics400.

\end{itemize}



    
\section{Prior Work}
\noindent \textbf{Semi-supervised learning in images}
Semi-supervised learning is one of the fundamental approaches to learning from limited labeled data~\cite{Gammerman1998Learning,joachims1999transductive,liu2019deep,kingma2014semi,pu2016variational,simclrv2,rizve2022openldn}. There are two common approaches for semi-supervised learning are consistency regularization \cite{NIPS2016_6333,LaineA17,Miyato2018VirtualAT,tarvainen2017mean, xie2020unsupervised} and pseudo-labeling \cite{Lee2013PseudoLabelT,Shi_2018_ECCV,arazo2020pseudo,ups,zhang2021flexmatch,rizve2022towards}.  The consistency regularization methods attempt to achieve perturbation invariant output space. To this end, these methods try to minimize a distance/divergence-based loss as a measure of consistency between two differently perturbed/augmented versions of an image. Pseudo-labeling-based methods on the other hand promote entropy minimization to improve by performing self-training on the confident pseudo-labels generated from the network. Hybrid methods~\cite{NIPS2019_8749_MixMatch,Berthelot2020ReMixMatch:,fixmatch} combine consistency regularization and pseudo-labeling techniques to obtain further improvement. Some of the recent works~\cite{assran2021semi, s4l, simclrv2} have also shown the effectiveness of self-supervised representations in solving this task. Our work is somewhat in the spirit of this last set of works as we also leverage self-supervised representation learning to obtain temporally distinct and invariant features.









\noindent \textbf{Semi-supervised learning in videos}
Although there is a tremendous recent development in action recognition~\cite{actionreco_bulat2021space, actionreco_chen2022mm, spact, actionreco_mvit, actionreco_mvit2, actionreco_long2022stand, actionreco_ryoo2021tokenlearner, actionreco_timesformer, actionreco_vidtr, c3d,kenshohara,actionreco_arnab2021vivit, actionreco_wang2022long}, semi-supervised learning in videos is not explored as in the image domain. 
CMPL~\cite{semi_cmpl} utilizes a FixMatch framework, where they study the effect of model capacity to provide complementary gains of unlabeled videos. They provide empirical evidence that the smaller model is responsible for learning the motion features whereas, the bigger model learns more appearance-biased features. However, defining smaller and bigger models is very relative, and the observation may not hold true for different architectures. Some prior work injects temporal dynamics to their semi-supervised framework by additional input modality to RGB videos like temporal gradients~\cite{semi_tgfixmatch}, optical-flow~\cite{semi_mvpl}, or P-frames~\cite{semi_compressed}. Another set of methods utilizes consistency loss between the slow and fast input streams of the video to leverage the unlabeled videos~\cite{semi_tacl, semi_tcl}. We can see that the prior works rely on hard-inductive biases like model architectures, input modality, or input sampling to learn temporal-invariance and distinctiveness. On the other hand, we do not have such a hard design choice; we leverage the unlabeled videos by the nature of the video instance using two complementary self-supervised teachers. 





\noindent \textbf{Self-supervised learning (SSL)}
In recent years, self-supervised learning has demonstrated learning powerful representations for images~\cite{simclr, moco, swav, zbontar2021barlow, mae, gupta2022higher} and videos~\cite{jenni2021time, tclr, Feichtenhofer_2021_CVPR, cvrl, simon, videomae, stmae, Schiappa_2022}. Although some works~\cite{simclrv2, s4l} have exploited self-supervised representation in semi-supervised image classification, Video SSL is not explored yet for semi-supervised action recognition. 

Video SSL methods can be grouped mainly into two categories: the first set of methods focuses on learning temporal invariance~\cite{cvrl, Feichtenhofer_2021_CVPR, videomoco}.
These methods are a simple-yet-effective extension of instance-discimination-based methods like SimCLR~\cite{simclr}, MoCo~\cite{moco}, etc., where mutual information between two views of an instance is maximized. For videos, two views are two clips from different timestamps, hence it introduces temporal invariance in the learned representations. 
The second set of methods focuses on learning the temporal distinctiveness, where they try to learn different representations for different clips through contrastive loss~\cite{tclr, dsm, seco, iic} or through different temporal pretext transformations~\cite{jenni2021time, simon, rspnet, varpsp, pace_pred, speedNet}.







TCLR~\cite{tclr} introduces temporal contrastive losses for both temporally pooled and unpooled features to learn the temporal distinctiveness. In our method, we utilized these losses to learn the temporal distinctive teacher. 




\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Figures/semi_main_try4.drawio.pdf}
    \caption{\textbf{Our Framework} We use a teacher-student framework where we use two teachers: $f_{I}$ and $f_{D}$. The input clip $\mathbf{x}^{(i)}$ is given to the teachers and student to get their predictions.
    We utilize a reweighting strategy to combine the predictions of two teachers. Regardless of whether the video $\mathbf{v}^{(i)}$ is labeled or unlabeled, we distill the combined knowledge of teachers to the student. For the labeled samples, we also apply standard cross-entropy loss.}
    \label{fig:framework}
\end{figure}

\section{Method}
Let's consider a small labeled set of videos $\mathbb{D}_{l} = \{(\mathbf{v}^{(i)}, \mathbf{y}^{(i)})\}_{i=1}^{N_{l}}$, where $\mathbf{v}^{(i)}$ and $\mathbf{y}^{(i)}$ denote $i$th video instance and its associated action label and $N_{l}$ is number of total instances in the dataset. We also have access to a unlabeled dataset $\mathbb{D}_{u} = \{\mathbf{v}^{(i)}\}_{i=1}^{N_{u}}$, where $N_u$ is the total number of unlabeled videos and $N_u \gg N_l$. The objective of the semi-supervised action recognition is to utilize both labeled and unlabeled sets ($\mathbb{D}_{l}$ and $\mathbb{D}_{u}$) to improve the action recognition performance. 


A high-level schematic diagram of our framework is depicted in Fig.~\ref{fig:framework}. Our semi-supervised learning framework, \textit{TimeBalance}, is a teacher-student framework. To train on the unlabeled samples, we distill the knowledge of two teacher models: temporally-invariant teacher $f_{I}$ and temporally-distinctive teacher $f_{D}$, which are trained in self-supervised manner,  to a student model $f_{S}$.  In the following, we explain the details of \textit{TimeBalance}. To be particular, in Sec.~\ref{sec:sslteachers}, we present the self-supervised pretraining of teacher models. 
In Sec.~\ref{sec:semisup} we explain the semi-supervised training of the student model and our \underline{T}emporal \underline{S}imilarity based \underline{T}eacher \underline{R}eweighting (TSTR) scheme. 







\subsection{Self-supervised pretraining of teachers}
\label{sec:sslteachers}
From a video instance $\mathbf{v}^{(i)}$, we sample $n$ consecutive clips $\mathbb{X}^{(i)} = \{\mathbf{x}^{(i)}_{t}\}_{t=1}^{n}$, where $t$ represents clip location (timestamp). Each of these clips undergoes stochastic transformations (e.g. random crop, random color jittering, etc.). Next, we send these clips
to the teacher model $f$ and a non-linear projection head $g$ respectively. The non-linear projection head, projects the clip-embedding from the teacher model to a lower-dimensional normalized representation $\mathbf{z}$, s.t.  $\mathbf{z} \in \mathbf{R}^{d}$, where $d$ is the dimension of output vector $\mathbf{z}$.
\subsubsection{Pretraining of Temporally-Invariant Teacher}
The goal of temporal-invariant pretraining is to learn the shared information across the $n$ different clips $\{\mathbf{x}^{(i)}_{t}\}_{t=1}^{n}$ of the same video instance $i$. 
To achieve this, we maximize the agreement between the projections, $\mathbf{z}$, of two different clips from the same video instance $\{ (\mathbf{z}^{(i)}_{t_1},\mathbf{z}^{(i)}_{t_2}) \mid t_1,t_2 \in \{1...n\} \;and\; t_1 \neq t_2 \}$, while maximizing the disagreement between the projections of clips from different video instances $\{(\mathbf{z}^{(i)},\mathbf{z}^{(j)}) \mid i, j \in B  \;and\; i\neq j\}$, where $B$ is the batch-size. This contrastive objective can be expressed as the following equation:
\begin{equation}\label{eq:inv}
  \mathcal{L}_{I}^{(i)}=- \sum_{\substack{t_1,t_2 \\ t_2\neq t_1}}^{n} \log \frac{\mathrm{h}\left(\mathbf{z}^{(i)}_{t_1}, \mathbf{z}^{(i)}_{t_2}\right)}{\sum\limits_{j=1}^{B}\mathbb{1}_{[j\neq i]} \mathrm{h}(\mathbf{z}^{(i)}_{t_1}, \mathbf{z}^{(j)}_{t_1}) + \mathrm{h}(\mathbf{z}^{(i)}_{t_1}, \mathbf{z}^{(j)}_{t_2})},
\end{equation}
\normalsize	
\noindent where $\mathrm{h}(\mathbf{u_{1}}, \mathbf{u_{2}})=\exp \left(\mathbf{u_{1}}^{T}\mathbf{u_{2}}/(\|\mathbf{u_{1}}\| \|\mathbf{u_{2}}\| \tau) \right)$ is used to compute the similarity between $\mathbf{u_{1}}$ and $\mathbf{u_{2}}$ vectors with an adjustable temperature parameter, $\tau$. $\mathbb{1}_{[j\neq i]} \in \{0, 1\}$ is an indicator function which equals 1 iff $j \neq i$.


\begin{equation}\label{eq:total}
\mathcal{L}^{(i)}= \mathcal{L}^{(i)}_{sup} + \omega \mathcal{L}^{(i)}_{unsup}, 
\end{equation}
\noindent where $\omega$ is the weight of the unsupervised loss.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Figures/semi_similarity_try6.drawio.pdf}
    \caption{\textbf{Temporal Similarity based Teacher Reweighting} Firstly, a set of clips from video $\mathbf{v}^{(i)}$ are passed through the distinctive and invariant teachers to get representations. Secondly, Temporal Similarity Matrices ($\mathbf{C}^{(i)}_{D}$ and $\mathbf{C}^{(i)}_{I}$) are constructed from the cosine similarity of one timestamp to another timestamp. Finally, from the average matrix $\mathbf{C}^{(i)}$, a temporal similarity score $s^{(i)}$ is computed. $s^{(i)}$ is utilized to combine predictions of teachers for video $\mathbf{v}^{(i)}$ during semi-supervised training.}
    \label{fig:similarity}
\end{figure}

\subsubsection{Pretraining of Temporally-Distinctive Teacher}
Contrary to the goal of $\mathcal{L}_{I}$, temporally-distinctive pretraining deals with learning the differences across the clips of the same video instance. To achieve this, we generate another set of clips $\mathbb{\tilde{X}}^{(i)} = \{\mathbf{\tilde{x}}^{(i)}_{t}\}_{t=1}^{n}$, which are randomly-augmented versions of clips in $\mathbb{X}^{(i)}$. After that, we maximize the agreement between the projections of a pair of clips $\{ (\mathbf{z}^{(i)}_{t_1},\mathbf{\tilde{z}}^{(i)}_{t_1}) \mid t_1 \in \{1..n\} \}$ from the same timestamp and maximize the disagreement between the projections of pair of temporally misaligned clips. A mathematical expression for this contrastive objective can be written as:

\begin{equation}\label{eq:dist}
  \mathcal{L}_{D1}^{(i)}=- \sum_{t_1=1}^{n} \log \frac{\mathrm{h}\left(\mathbf{z}^{(i)}_{t_1}, \mathbf{\tilde{z}}^{(i)}_{t_1}\right)}{\sum\limits_{\substack{t_2=1 \\ t_2\neq t_1}}^{n} \mathrm{h}(\mathbf{z}^{(i)}_{t_1}, \mathbf{z}^{(i)}_{t_2}) + \mathrm{h}(\mathbf{z}^{(i)}_{t_1}, \mathbf{\tilde{z}}^{(i)}_{t_2})},
\end{equation}
\normalsize	

The above contrastive loss imposes temporal distinctiveness at the clip-level i.e. temporally-average-pooled features. Similarly, we can also impose temporal distinctiveness ($\mathcal{L}_{D2}^{(i)}$) on a more fine-grained level i.e. on the unpooled temporal feature slices~\cite{tclr}. More details in \supp{Supp. Sec.~\ref{sec:method_semi_supp}}. We combine these pooled and unpooled temporal-distinctiveness objectives to obtain $\mathcal{L}_{D}^{(i)}=  \mathcal{L}_{D1}^{(i)} + \mathcal{L}_{D2}^{(i)}$.


The primary objective of this work is semi-supervised action recognition. Therefore, even though the self-supervised teacher models lack any explicit notion of category-specific output space, we argue that to solve the downstream action recognition task their knowledge has to be distilled from the action category-specific output space. To this end, we finetune both of the self-supervised teacher models on the labeled set $\mathbb{D}_{l}$ using cross-entropy loss.  



\subsection{Semi-supervised training of student model}
\label{sec:semisup}
We initialize the student model with weights from a video self-supervised model~\cite{tclr} trained on $\mathbb{D}_{u}$. 
During the semi-supervised training, student model $f_{S}$ gets supervision from two sources: (i) ground-truth label (if available), and (ii) teacher supervision (Fig.~\ref{fig:framework}). A visual aid is provided in \supp{Supp. Sec.~\ref{sec:method_semi_supp}} for loss computations over the labeled and unlabeled split in semi-supervised training.  The supervision from the ground-truth label is utilized using standard cross-entropy loss, as depicted in the following equation:

\begin{equation}\label{eq:crossentropy}
\mathcal{L}^{(i)}_{sup} = -\sum_{c=1}^{C} \mathbf{y}^{(i)}_{c}\log \mathbf{p}^{(i)}_{c}
\end{equation}

\noindent For instance $i$, the prediction vectors of the invariant and distinctive teacher are denoted as $\mathbf{p}_{I}$ and $\mathbf{p}_{D}$, respectively.

Next, we will discuss our TSTR scheme to distill knowledge from the temporally invariant and distinct teachers to train the student model on the unlabeled data, $\mathbb{D}_u$ and videos of labeled data $\mathbb{D}_l$.



\paragraph{Temporal Similarity based Teacher Reweighting.}
In order to combine supervision from $f_I$ and $f_D$ for a particular video instance $\mathbf{v}^{(i)}$, we first compute temporal similarity scores. To this end, we compute the cosine similarity between each pair of clips to form a temporal similarity matrix, $\mathbf{C}^{(i)}$, as depicted in Fig.~\ref{fig:similarity}. The temporal similarity matrix computation is described in the following,

\begin{equation}\label{eq:similarity1}
\begin{aligned}
\mathbf{C}^{(i)} = \Big[\mathrm{Sim}(\mathbf{z}_{t_1}^{(i)}, \mathbf{z}_{t_2}^{(i)})\Big]_{t_1, t_2=1}^{n},\\
\end{aligned}
\end{equation}
\noindent where, $\mathrm{Sim}(.)$ is the cosine similarity function.

We compute the temporal similarity matrix for both invariant and distinctive teachers denoted as $\mathbf{C}^{(i)}_{I}$, and $\mathbf{C}^{(i)}_{D}$, respectively. Next, in order to get a similarity score, $s^{(i)}$, for an instance $i$, we take the average of non-diagonal elements of both $\mathbf{C}^{(i)}_{I}$, and $\mathbf{C}^{(i)}_{D}$ matrices. 

\begin{equation}\label{eq:similarity2}
s^{(i)} = \frac{1}{2n(n-1)} \sum \limits_{\substack{t_1,t_2=1 \\ t_2\neq t_1}}^{n} (\mathbf{C}_{I}^{(i)} + \mathbf{C}_{D}^{(i)})
\end{equation}




We use this temporal similarity score, $s^{(i)}$, to aggregate the outputs of the teacher models. Let's assume, for instance $i$, the prediction vectors of the invariant and distinctive teacher are denoted as $\mathbf{p}^{(i)}_{I}$ and $\mathbf{p}^{(i)}_{D}$, respectively. Now, we want to combine these teacher prediction vectors in such a way that the temporally-invariant prediction $\mathbf{p}^{(i)}_{I}$ gets a higher weight if the temporal similarity score is high, and the weight of the temporal-distinctive prediction $\mathbf{p}^{(i)}_{D}$ gets higher in the case of a lower temporal similarity score. This dynamic weighting scheme for obtaining the aggregated teacher prediction is provided below.  

\begin{equation}\label{eq:reweighting}
\mathbf{p}^{(i)}_{T} = s^{(i)}.\mathbf{p}^{(i)}_{I} + (1-s^{(i)}).\mathbf{p}^{(i)}_{D}
\end{equation}

We use the combined teacher prediction $\mathbf{p}^{(i)}_{T}$ to provide supervision to the student model using $\mathcal{L}_2$ loss. We compute this loss only on the unlabeled samples ($\mathbb{D}_u$ and videos of $\mathbb{D}_l$ without assigned labels), hence, we refer to this loss as the unsupervised loss. This loss term is defined below.

\begin{equation}\label{eq:l2}
\mathcal{L}^{(i)}_{unsup} =\left(\mathbf{p}^{(i)}_T-\mathbf{p}^{(i)}_S\right)^2
\end{equation}


Finally, we sum both the supervised and unsupervised losses to train the student model. The overall objective function is defined below.




\subsection{Algorithm}
Let's consider models $f_{I}$, $f_{D}$, and $f_{S}$ are parameterized by $\theta_{I}$, $\theta_{D}$, and $\theta_{S}$.  All steps of our semi-supervised training are put together in Algorithm~\ref{alg:algo1}. 





\input{Tables/algo1}



\input{Tables/sota_table.tex}

\section{Experiments}
\subsection{Datasets}
\noindent\textbf{UCF101}~\cite{ucf101} is a dataset for human action recognition collected from internet videos consisting 101 action classes. We use split-1 for our experiments, which has 9,537 train videos and 3,783 test videos.


\noindent\textbf{HMDB51}~\cite{hmdb} is relatively a smaller dataset collected from movie videos. It has 51 human activity classes and has a high intra-class variance. We use split-1 in this paper, which has 3,570 train videos and 1,530 test videos.

\noindent \textbf{Kinetics400}~\cite{kinetics} is a large-scale dataset collected from YouTube videos. It has a standard split of 240k training videos and 20k validation videos which covers 400 actions. 




\subsection{Implementation Details}
For our default experimental setup, we use an input clip resolution of $224\times224$. We use 3D-ResNet-50~\cite{slowfast} for both student and teacher models. We use most commonly used augmentations: geometry-based augmentations like random cropping, and flipping, and color-based augmentations like random greyscale, and color jittering. 



\noindent \textbf{Self-supervised pretraining} 
The pretraining is performed with clips of $16$ frames for 100 epochs for Kinetics400, 250 epochs for UCF101, and HMDB51 experiments. For contrastive losses, the default temperature is set to 0.1. 


\noindent \textbf{Semi-supervised training} 
Semi-supervised training is performed with clips of $8$ frames for 150 epochs. 

\noindent \textbf{Inference} 
We follow standard protocol~\cite{r2plus1d} of averaging the predictions of 10 uniformly spaced clips and 3 different spatial scales to get a video-level prediction.
\\
More implementation details in \supp{Supp. Sec.~\ref{sec:impl_semi_supp}}. 

\begin{figure*}
    
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[clip, trim=0cm 3.2cm 0cm 0cm, width=0.99\textwidth]{Figures/semi_visualizationL_try2.drawio.pdf}
        \vspace{-0.5mm}
        \caption{\texttt{Fencing}, $s^{(i)}=0.795$}

    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[clip, trim=0cm 3.2cm 0cm 0cm, width=0.99\textwidth]{Figures/semi_visualizationR_try2.drawio.pdf}
        \vspace{-0.5mm}
        \caption{\texttt{JavelinThrow}, $s^{(i)}=0.387$}

    \end{subfigure}

    \caption{Visualization of similarity matrices from temporally-invariant and distinctive teachers and resultant instance-similarity score. (a) Video instances with atomic actions like \texttt{Fencing} result in a high instance similarity score, which will proportionally increase the weightage of temporally-invariant teacher (b) Whereas, complex actions video instances like  \texttt{JavelinThrow} results in a low similarity score, which results into more weightage of temporally-distinctive teacher. Details of the computation of the similarity score in Sec~\ref{sec:semisup} }
    \label{fig:similarity_matrix_visualize}
\end{figure*}



\subsection{Comparison with prior work}
We compare our method with image-based baselines, video self-supervised baselines, and recent state-of-the-art methods for semi-supervised action recognition in Table~\ref{table:big}. We present the results in two sections based on the backbone architecture used: (1) 3D-ResNet18 and (2) 3D-ResNet50. We report Top-1 classification accuracy as the performance measure and follow the standard protocol of reporting average performance over three independent runs. 


\noindent \textbf{Image-based baselines} We consider widely used semi-supervised image classification baselines like Psuedo-Label~\cite{lee2013pseudo}, MeanTeacher~\cite{tarvainen2017mean}, UPS~\cite{ups}, S4L~\cite{s4l} and FixMatch~\cite{fixmatch}. From Table~\ref{table:big}, we observe that the results of these image-based methods are significantly lower than the video-based methods across all benchmarks; which suggests that spatial information is not enough to excel in semi-supervised action recognition. 



\noindent \textbf{Video Self-supervised Learning baselines} Video self-supervised learning methods are first pretrained on the full-training set without using any labels and then finetuned on the labeled set in a supervised manner. We compare with contrastive learning-based methods like TCLR~\cite{tclr}, MemDPC~\cite{memdpc}, multimodal approach like MotionFit~\cite{motionfit}, and pretext-task based approach 3DRotNet~\cite{3drotnet}.  From Table~\ref{table:big}, we notice that the performance of these video self-supervised methods is significantly better than image-based baselines, and in some cases, it even performs favorably against the semi-supervised action recognition methods~\cite{semi_cmpl, semi_mvpl, semi_tgfixmatch}. However, our proposed semi-supervised method outperforms all these video self-supervised baselines by a noticeable margin. 

\noindent \textbf{Semi-supervised action recognition baselines} We also compare with prior semi-supervised action recognition works like TG-Fixmatch~\cite{semi_tgfixmatch}, VideoSemi~\cite{semi_videossl}, MvPL~\cite{semi_mvpl}, TACL~\cite{semi_tacl}, and ActorCM~\cite{semi_actorcutmix}. Our method achieves significant improvement over these methods across all benchmarks. Remarkably, our method also outperforms the methods that use additional modalities like optical flow in MvPL~\cite{semi_mvpl} and additional data (ImageNet) in ActorCM~\cite{semi_actorcutmix} and TACL~\cite{semi_tacl}.






\subsection{Ablations and Analysis}
\label{sec:ablation}
In the default setting of ablations, we consider UCF101 with 3D-ResNet50 as the teacher backbone. More ablations in \supp{Supp. Sec.~\ref{sec:ablation_semi_supp}}. 

\noindent \textbf{Contribution of different training components}
In Table~\ref{table:abl_framework}, we analyze the effect of each model ($f_S$, $f_I$, $f_D$) and our proposed teacher reweighting scheme with 5\% and 20\% labeled data on the UCF101 dataset. \texttt{Row a-c}, demonstrates the performance of teacher and student models individually. Results in \texttt{Row f} demonstrates that the student model performs the best when we average the predictions of both teachers and removing any of them(\texttt{Row d, e}) degrades the performance. This validates our hypothesis that for optimal performance, we need to distill the knowledge from both teachers. Finally, \texttt{Row g}, demonstrates the effectiveness of our proposed teacher reweighting scheme using temporal similarity. We found similar results with $f_S$ from random initialization in Table~\ref{table:abl_scratch_framework}.


\input{Tables/abl_framework.tex}
\input{Tables/abl_scratch_framework}
\noindent \textbf{Visualization of Temporal Similarity Matrix}
We sample four consecutive frames from unlabeled videos and compute the temporal similarity matrix for both $f_I$ and $f_D$. Visualization is shown Fig.~\ref{fig:similarity_matrix_visualize}. Firstly, we observe that $f_I$ gives higher similarity values in both video instances than $f_D$. Secondly, we can observe that the final instance similarity score aligns with our goal i.e. providing more weight to $f_I$ in atomic actions and providing more weight to $f_D$ in complex actions. 






\noindent \textbf{Different types of teacher pairs}
In order to further investigate the importance of having teachers with complementary self-supervised objectives (i.e temporally invariant and distinctive), we study the effect of having various pairs of teachers in our framework (Table~\ref{table:abl_ensemble}). \textit{Inv1} and \textit{Inv2} models are obtained from independent SSL pretrainings with temporal-invariance objective. Similarly, \textit{Dist1} and \textit{Dist2} are trained with temporal-distinctiveness SSL objective. We observe that, compared to combining teachers with the same SSL objective (\texttt{Row a, b}), teachers with different SSL objectives (\texttt{Row c, e}) perform significantly better. In our default setting, we use (\textit{Inv1}, \textit{Dist1}) as teacher pair.

\input{Tables/abl_ensemble.tex}



    









\noindent \textbf{Student initialization}
We also study various video self-supervised methods to initialize the student model and report the results in Fig~\ref{fig:epochwise}.
We conduct these experiments on the UCF101 dataset with 3D-ResNet18 as the student backbone. We observe that even across a diverse set of video SSL-based initialization techniques, our proposed semi-supervised framework can achieve better/competitive performance against the current state-of-the-art. This further validates the effectiveness of our idea of leveraging temporally distinct and invariant teachers and our proposed teacher prediction reweighting scheme. 

\begin{figure}
    \centering
    \includegraphics[trim={5mm 0 2mm 3mm},clip, width=0.8\columnwidth]{Figures/chart_scratch.pdf}
    \caption{Analysis of different student initialization strategies on UCF101 dataset with 5\% labeled data and  3D-ResNet18 student.}
    \label{fig:epochwise}
\end{figure}

\noindent \textbf{Number of clips in Teacher SSL pretraining}
We analyze the effect of using a different number of clips to impose temporal invariance/distinctiveness in teacher pretraining in Table~\ref{table:abl_nclips}. We perform pretraining on UCF101 with the 3D-ResNet50 model and report the performance of the teacher model after finetuning on the labeled set. In the default setting, we use n=4 clips. We observe that while reducing the clips from 4 to 2, the performance drop is significant for temporally-distinctive teachers, whereas, it is not severe for the temporally-invariant teacher. Since the distinctiveness contrastive loss $\mathcal{L}_{D}$ treats other clips as negatives, its contrastive objective becomes very easy to solve if we reduce the number of clips (Eq.~\ref{eq:dist}). Whereas, temporal-invariance contrastive loss $\mathcal{L}_{I}$ takes negatives from the other instances of the batch, and decreasing the number of clips does not change the difficulty of the loss. 


\input{Tables/abl_nclips.tex}







\section{Conclusion and Future Work}
In this work, we have proposed TimeBalance, a teacher-student framework for semi-supervised action recognition. We utilize the complementary strengths of temporally-invariant and temporally-distinctive representations to leverage unlabeled videos. Our extensive experimentation has empirically validated the effectiveness of different components of our framework and results on multiple benchmark datasets established TimeBalance as the new state-of-the-art for semi-supervised action recognition. 

Our findings regarding the complementary strengths of temporally-invariant and temporally-distinctive video representations could be applied to other data-efficient video understanding problems, such as few-shot action recognition and spatio-temporal action detection. It would also be interesting to explore the invariance and distinctiveness properties of video using recent masked-image modeling techniques~\cite{videomae, stmae} and multi-modal settings~\cite{bachmann2022multimae}, such as audio + video and text + video.





\section*{Acknowledgments}
\noindent We thank Tristan de Blegiers for his help with visualization. 








\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}
\clearpage

\appendix
\section{Overview}
\begin{itemize}
    \item Section~\ref{sec:impl_semi_supp}: Implementation details about network architectures and training setup. 
    \item Section~\ref{sec:ablation_semi_supp}: Ablation study for our framework.
    \item Section~\ref{sec:method_semi_supp}: Supportive diagrams and explanation for our method.    


    
\end{itemize}



\section{Implementation Details}
\label{sec:impl_semi_supp}
\subsection{Network Architecture}
\subsubsection{Backbone}
For teacher models $f_I$ and $f_D$, we utilize 3D-ResNet50 model from the implementation of \texttt{Slow-R50}~\cite{slowfast} of official PyTorchVideo\footnote{\href{https://github.com/facebookresearch/pytorchvideo}{https://github.com/facebookresearch/pytorchvideo}}. For experiments with 3D-ResNet18, we utilize its official PyTorch implementation \texttt{r3d\_18}\footnote{\href{https://github.com/pytorch/vision/blob/main/torchvision/models/video/resnet.py}{https://github.com/pytorch/vision/blob/main/torchvision/models/video}}. 
\subsubsection{Non-Linear Projection Head}
We use non-linear projection head $g(\cdot)$ during the self-supervised pretraining of temporally-invariant and temporally-distinctive teachers to reduce the dimensions of the representation. We utilize Multi-layer Perceptron (MLP) as a non-linear projection head to project 2048-dimensional model features to 128-dimensional vectors in normalized representation space. The design of MLP is as follows, where \texttt{nn} indicates \texttt{torch.nn} PyTorch package:
\begin{verbatim}
nn.Linear(2048,512, bias = True)
nn.BatchNorm1d(512)
nn.ReLU(inplace=True)
nn.Linear(512, 128, bias = False)
nn.BatchNorm1d(128)
\end{verbatim}

\subsection{Training Details}
For all weight updates, we utilize Adam Optimizer~\cite{adam} with default parameters $\beta_1=0.9$ and $\beta_2=0.999$ with a base learning rate ($\alpha_I$, $\alpha_D$, $\alpha_S$) of 1e-3. For all training, we utilize a linear warmup of 10 epochs. A patience-based learning rate scheduler is also used, which drops the learning rate to its half value on a loss plateau. 


\section{Additional Ablations}
\label{sec:ablation_semi_supp}

\subsection{Loss function for teacher supervision}
In order to distill teacher knowledge, we study three different loss functions as $\mathcal{L}_{unsup}$ and report the results in Table~\ref{table:abl_teacherloss}. For these experiments, we use 3D-Resnet50 as the student model on the UCF101 dataset~\cite{ucf101}. We observe that all three losses perform reasonably while $\mathcal{L}_2$ performs the best, which we use as the default loss in our method. 
\input{Tables/abl_teacherloss.tex}


\subsection{Student $f_S$ from Scratch}
We perform experiments with student from random initialization and compare them with the prior methods in Table~\ref{table:scratch}.

\input{Tables/scratch.tex}



\begin{figure*}[h]
    
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[height=2.5in]{Figures/semi_labeled.drawio.pdf}
        \caption{Labeled data}

    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[height=2.5in]{Figures/semi_unlabeled.drawio.pdf}
        \caption{Unlabeled data}

    \end{subfigure}

    \caption{Loss computations in labeled and unlabeled data. (a) In case of Labeled data, the student $f_S$ gets supervision from supervised cross-entropy loss from label $\mathbf{y}^{(i)}$ and unsupervised $\mathcal{L}_2$ loss from teachers. (b) For unlabeled set, the student is only trained with the unsupervised loss from teachers. Details are in Sec~\ref{sec:semisup} of the main paper.}
    \label{fig:lab_unlab}
    \vspace{5mm}
\end{figure*}
\section{Method}
\label{sec:method_semi_supp}
\subsection{Loss for Labeled and Unlabeled set} In Fig.~\ref{fig:lab_unlab}, we show the handling of labeled and unlabeled data in the semi-supervised training of student $f_S$. For labeled data $\mathbb{D}_l$, the student model has two sources of supervision: (1) Labeled supervision $\mathcal{L}^{(i)}_{sup}$ in the form of standard cross entropy loss which is computed from the student's prediction and given class label $\mathbf{y}^{(i)}$ (2) Unlabeled supervision $\mathcal{L}^{(i)}_{unsup}$ in the form of $\mathcal{L}_2$ distillation loss computed from the weighted average of predictions of teachers ($f_D$ and $f_I$). For the unlabeled set $\mathbb{D}_u$, the student model gets supervision only in the form of $\mathcal{L}_2$ distillation loss.



\subsection{Temporally-Distinctive pretraining using unpooled features}
Since $\mathcal{L}_{D1}$ deals with temporally-pooled(averaged) features, it promotes temporal-distinctiveness for the \textit{pooled} features. Similar to that, ~\cite{tclr} designs a contrastive objective that promotes temporally-distinctive representation on the \textit{unpooled} features. We call it unpooled temporal-distinctive objective $\mathcal{L}_{D2}$, which is illustrated in Fig.~\ref{fig:global_local}. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=.55\linewidth]{Figures/global_local_try4.drawio.pdf}
    \caption{\textbf{Temporally-Distinctive Contrastive Objective for Temporally-unpooled features $\mathcal{L}_{D2}$}: A time-duration of the video can be represented in two different ways: (1) Pooled features of the short(local) clip (2) Unpooled feature slice of the long(global) clip. In this contrastive objective, we maximize the agreement between \textit{temporally-aligned} pooled and unpooled features.}
    
    \label{fig:global_local}
\end{figure*}













\end{document}
