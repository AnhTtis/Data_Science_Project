%\chapter{The State of Current Trust Measures}\label{Chap:three}
%Our initial goal is to understand how trust is currently measured, how well it is measured, and whether the validated means by which it is measured converge on a shared set of factors or a structure that allow us to better grasp a more complete understanding of human-machine trust.

%Thus, this chapter details how we generated and validated our sample of trust survey instruments, what we discovered about their quality, and what trends we identified among their structures, styles, and factors.  Before we begin, however, we must first discuss our method for assessment of survey quality. %%This will need to be adjusted depending on how we break the chapter down


\section{Methodology for Assessing Instrument Quality}
In order to determine instrument quality, we wanted a method that was flexible enough to accommodate various aspects of trust, multiple styles of instrument and assessment, but still retained reasonable and acceptable standards in the field of psychometric survey creation.   To this end, every survey identified was assessed for various forms of \textit{reliability} and \textit{validity} and their \textit{construction quality}. 


\subsection{Reliability}\label{sec:rel}
Reliability is the consistency of a test, such as between experimenters (inter-rater), between points in time (test-retest), between versions of the same test (parallel forms), and between items that purport to measure the same thing (internal consistency). 
The primary type of reliability we are concerned with here is \textit{internal consistency}; when multiple questions are asked about the same construct, the responses should be consistent with each other.  That being said, if an assessment reported other forms of reliability, it was given credit for having a reliability assessment as other forms of reliability ideally should also be tested and reported.

Within internal consistency, several measures are available, each with its own assumptions that must be acknowledged.  The various methods of establishing internal consistency are based on different ways of measuring the correlations between item scores.  The simplest is the average inter-item correlation, which should generally be between 0.15-0.50.  Correlations less than this indicate that the measure is not homogenous (unidimensional);  correlations with more than this and the measures may have too much redundancy.  A second method is by using split-half reliabilities.  That is, the questions can be divided into two groups, and participants' scores in one group of questions can be correlated with the scores among the other group.  Of course, there are many ways to divide the questions randomly, so one might use the minimum, average, or maximum across all possible splits.  The mean of all split-half reliabilities is better known as Cronbach's $\alpha$.  Additionally, Guttman's $\lambda$-6 (G6) and McDonald's $\omega$ are also available methods to measure internal consistency.

\subsubsection{Cronbach's $\alpha$}
 While Cronbach's $\alpha$ is the standard for reporting internal consistency, it makes a considerable assumption concerning $\tau$-equivalence.  In other words, it assumes that factor loadings are equivalent across all survey items, which is not the case for most of the reported scales \cite{Flora2020}.  Even more critical for avoiding bias in $\alpha$ is that the scale is homogeneous (contains a single factor)\cite{Flora2020}, that the errors for each item are uncorrelated \cite{Flora2020}, and that there are no missing data, outliers, heavy skew, or other substantial deviations from non-normality \cite{Sheng2012,Zhang2016,Flora2020}.  If missing data or non-normality, especially outliers, are concerns, alternative robust calculations for $\alpha$ should be used \cite{Zhang2016}.  Care must also be taken to standardize $\alpha$ if different items are measured on different scales.  Even if all of these assumptions are met, $\alpha$ becomes inflated as the number of items increases.

Taking all this into account, if Cronbach's $\alpha$ is still used, it should \emph{at least} be $>0.7$ for an experimental scale or $>0.8$ for a well-established scale \cite{NajeraCatalan2019}.  It should also remain less than $<0.95$, or even $<0.9$ \cite{tavakol2011making} as too high an alpha indicates redundancy, which can lead to problems with variance inflation if a factor analysis is to be performed.

\subsubsection{Guttman's $\lambda$-6 (G6)}
While Cronbach's $\alpha$ can be understood as the ratio of individual to total variance, Guttman's $\lambda$-6 (G6) compares the total variance to the squared variance of the errors.  It presents a lower bound on the communality of each item.  While including more items generally improves G6, like with $\alpha$, they also positively bias the metric.  Furthermore, G6 is just as sensitive as $\alpha$ when it comes to non-homogeneity.

That being said, $\alpha$ helps assess whether a factor is unidimensional or multi-dimensional when compared to G6 and McDonald's coefficient $\omega$ (see below).  Both will be greater than $\alpha$ in the case of the former and less than $\alpha$ in the case of the latter \cite{RPsych,NajeraCatalan2019}, thus allowing us to check one of the assumptions of both $\alpha$ and G6 and an essential parameter in specifying $\omega$. 

\subsubsection{McDonald's $\omega$}
McDonald's $\omega$ is yet a 3rd way to measure internal consistency.  Compared to $\alpha$ or G6, McDonald's coefficient omega ($\omega$) is a better measure, placing a lower bound on total test reliability while making fewer assumptions.  Unfortunately, $\omega$ is under-utilized in the field of measurement in general, especially in trust measures.  Furthermore, the metric can be confusing because there are at least four types of $\omega$s and many different naming conventions.

\begin{itemize}
    \item $\omega_{t}$ or $\omega_{u}$: unidimensional, congeneric, continuous, sometimes referred to as total
    \item $\omega_{cat}$ or $\omega_{u-cat}$: unidimensional, congeneric, categorical
    \item $\omega_h$: hierarchical, multi-factorial, continuous
    \item $\omega_{gr}$: average $\omega$ across all factors
\end{itemize}

While $\omega_t$ does give a better estimate of reliability than either $\alpha$ or G6, the true power of $\omega$ is that it can better capture the reliability of factors that are better captured by using hierarchical factors.  While $\omega_t$ is the estimate of total test reliability, $\omega_h$ is the estimated variance accounted for by the general (highest-order) factor (similar to Cronbach's $\alpha$).  Since for a unidimensional factor, Cronbach's $\alpha$ will always be lower than  $\omega_{t}$, maintaining a high standard for $\alpha$ can help ensure a sufficiently high $\omega_{t}$ \cite{NajeraCatalan2019}.  $\omega_{t}$ can indicate how much error we may have in an estimate.  For instance, in binary classification at $\omega_{t}\simeq0.7$, classification error is approximately 10\%, whereas at $\omega_{t}>0.8$ error remains below 6\% \cite{NajeraCatalan2019}. 

For multi-dimensional scales, using $\omega$ is even more critical.  In that case, the recommended minimum values for total omega and the average omega overall factors are $\omega_t>0.8$ and $\omega_{gr}>0.6$, respectively.  This bound on $\omega_{gr}$ is especially important in the case of strong multi-dimensionality.  Strong multi-dimensionality is when items do not individually load highly on the general latent variable.  In contrast, in weak multi-dimensionality, each item also loads strongly on a single general latent variable.  Acceptable thresholds for $\omega_h$ can also change depending on the strength of multi-dimensionality, $\omega_h>0.65$ with stronger multi-dimensionality and $\omega_h>0.7$ when its weaker \cite{NajeraCatalan2019}.

While generally, both $\alpha$ and $\omega$ are used even with non-normal distributions, it has been found that both are not robust to samples with skew or positive kurtosis \cite{Sheng2012}.  Furthermore, outliers and leverage can play havoc with results \cite{Zhang2016}.  While more test items can somewhat improve these results, it is a greater sample size that offers more improvement \cite{Zhang2016}.  Additionally, algorithmic solutions offering more robust $\alpha$ and $\omega$ coefficients are now available to help mitigate these limitations \cite{ZhangA2016}.

We utilize these measures and the appropriate ranges to assess instrument reliability in Section \ref{sec:goodness} on page~\pageref{sec:goodness}.

\subsection{Statistical Validation}
When considering the validity of the assessment, three types of vailidity are typically reviewed: Construct Validity, Instrument Validity, and Content Validity.

The \textit{validity}, or to be more exact, \textit{construct validity}, is the degree to which ``the measure of a construct sufficiently measures the intended construct" \cite{o1998empirical}.  However, achieving that in practice is a multifaceted challenge.  First, we need to establish which construct is intended, whether it is well constructed, reflective or formative, and homogeneous or heterogeneous.  Once the construct is clarified and refined, showing that the measure captures the construct is even more complex.  It requires understanding how to select appropriate items, their wording, and how they co-vary.  Additionally, it must be established that the items accurately correlate well with the latent construct, that appropriate sample size and power were used, and that surface, content, statistical conclusion, internal, convergent, and discriminant validity. 

By measuring validity statistically, we are attempting to measure some parameter of a sample to come to a statistically sound conclusion about the population of interest as a whole.  Often we are trying to quantify the size of an effect in how a parameter differs between two groups.  Effect sizes may be of varying magnitudes and may also be more difficult to detect depending on the variability of the population.  The statistical power of a test describes the probability that it will detect an effect if it exists; hence it is the inverse of the false negative rate.  Smaller effects require higher powers to detect.  The significance level is the likelihood that a researcher will reject the population-level null hypothesis, even if true (false positive rate).  Thus, it is a measure of the strength of the evidence within the sample required to conclude that the effect is statistically significant.  Ultimately, the \textit{appropriate sample size} must be determined by the significance level and desired \textit{power}, the \textit{effect size}, and the standard deviation of the parameter within the population.  Thus, researchers need to establish the statistical level of their test and whether they have the appropriate power and sample size for the effect sizes and variability.  

These are just the initial steps to establish statistical conclusion validity, as the researcher must also take care that their interpretation is correct and that the chosen test's specific assumptions are met, such as the errors of the variables being uncorrelated.  Statistical conclusion validity, however, is insufficient to validate a survey fully, as it can leave potential mediators unaccounted for and relevant variables untested and even conceptually ungrounded.  Both instrument and internal validation are also needed.

\subsection{Instrument Validation}
In the first stages of instrument validation, we are mainly concerned with establishing and operationalizing the concept into a viable construct.  It is vital to get a clear definition of the construct, refine it against the established literature and have it reviewed by subject matter experts.  Within this definition process, the goal is to ensure that the construct is neither too broad nor narrow, positively and clearly defined and that the construct's nature, domain, entities, and dimensionality are assessed.  Not only is it crucial to understand if the construct is unidimensional (homogenous) or multi-dimensional (heterogenous) but also whether the construct is reflective or formative.  A reflective construct means that the latent construct is reflected in the items within the instrument.  Alternatively, a formative construct is defined by the items composing it.  However, it is essential to note that constructs are not inherently formative or reflective, and depend on how the construct and indicators are linked, the way the researcher understands them, and one's underlying ontological understanding of the world \cite{MacKenzie2011}.  

Beyond dimensionality and clear language, instrument designers must also carefully consider the item scale (ordinal, nominal, ratio) and types (discrete, continuous).  These will affect not only statistical analysis but also the types of questions that can be asked and the resolution at which they can be answered. 

\subsubsection{Content Validity} \label{sec:val}
In content validation, the primary goal is to ensure coverage.  The items on the survey must be drawn from a universal pool so that the entire construct is covered.  Additionally, each item should represent a facet of the content domain.  Given the difficulty of ensuring sufficient coverage from a universal pool, several techniques have been proposed.  First, this highlights the importance of a clear and positively defined construct.  From there, the initial question pool can be extended using items from previous surveys and top-down conceptual work from the literature review.  Subject matter experts (SMEs) can be leveraged, as in \cite{schaefer2013perception}, to review the quality of coverage and make sure all aspects of the intended construct are included and no others.  More quantitative techniques, such as that of Yao \cite{MacKenzie2011}, may be employed to ensure that individual items are sufficiently unique to maximize coverage while ensuring all construct dimensions are adequately covered.

\subsubsection{Construct Validity}
Returning now to construct validity, the goal is to ensure the construct is being measured and the results do not arise from methodological quirks.  While difficult to assess, many techniques can be used before and after the survey is administered to help mitigate concerns and verify techniques. 


% Items on trust survey instruments can be of many scales (ordinal, nominal, ratio) and types (discrete, continuous).  Instruments may range from asking a single item or two to dozens, and they may group these items into a single trust factor or multiple factors which compose cause or influence trust.  There are specific known challenges to item construction that should be accounted for, mainly depending on the type of analysis that will be used to validate the scale later. 

In addition to clear, unambiguous language, other steps can be taken to remove systemic bias resulting from how questions are presented.  For instance, many surveys will use a mixture of positive and negative items in the same factor to reduce acquiescence bias, if not extreme response bias \cite{schriesheim1981controlling}, both of which artificially inflate reliability metrics such as $\alpha$.  However, the use of negatively worded items introduces its own threat to response validity and construct homogeneity \cite{schriesheim1981controlling}, lowering $\alpha$ by implying multi-dimensionality exists, which is essentially an artifact of the opposed item valences.  This may be illustrative of one of the many trade-offs between types of validity that exist in survey design.  However, this problem can be mitigated to some extent by inverting the direction of the response,% as illustrated in \ref{fig:3_0Prisma}, 
which maintains the protection against acquiescence bias while seemingly serving as less of a threat to response validity \cite{barnette2000effects,merritt2015measuring}.  Factors solely established based on clustering negative vs. positive items should be handled with care.

Factors that share similar wording can present a similar challenge.  They may suffer both from acquiescence bias and the tendency of similarly worded items to cluster together more strongly, have correlated error, and thus yield an inflated $\alpha$ as well as invalid factors \cite{merritt2015measuring}.  Again, care should be taken when assessing such surveys' reported reliability and validity.

Other techniques to validate one particular method are measuring the construct using other modalities and instruments.  Together these can be analyzed through various means, such as multitrait-multimethod (MTMM) techniques and confirmatory factor analysis to verify whether convergent validity is achieved.  Conversely, constructs proposed to differ fundamentally can be tested for discriminant validity.  To better establish the construct experimentally, one can vary conditions or sample populations with known attributes to see that the construct changes appropriately \cite{Straub1989, MacKenzie2011}. 

Validity is a complex topic with many facets and requires care to design, develop, and test for.  This cannot be achieved in a single shot but requires iterative refinement of definitions, dimensions, language, items, survey structures,  statistical tests, and analysis choices, using both top-down conceptual approaches and bottom-up empirical methods.

\section{Surveying the Field: Identification of Trust Instruments}\label{sec: survey}
Now that we have established how we will assess the various trust instruments in the literature, we must now systematically identify which trust instruments are available to researchers.  In this section, we will describe a systematic method for trust assessment identification and the refinement of this list to a tractable number which are sufficiently useful/popular and well documented to perform our in-depth assessment.  

\subsection{Sampling Methodology}\label{sec:sampling}
To build a sufficient sample, we first collected all trust-related works mentioned among the latest and largest literature reviews in HCI, HRI, and HAI \cite{french2018trust, Hoffman2013,hancock2021evolving,brzowski2019trust}, resulting in 436 records.  We added to these 54 other sources that we gathered through our own literature review, especially of more recent works that the reviews may have missed.  We then continued to snowball our sample by reviewing these works, especially their background and 
method sections, and noting all potentially relevant trust studies in these fields.  Each time a paper in either literature review mentioned another method/survey instrument, whether used directly or not, that paper was added to the sample, resulting in 687 total records.  After accounting for duplication, we also excluded works where no experiment was performed (e.g., review works, theory papers), works that did not use a self-reported trust survey instrument (e.g., those that only measured behavior or physiology), and works to which we did not have full-text access.  Other criteria for inclusion were that the work was published in a peer-reviewed venue and accessible in English.  We ended up with a sample of 173 experimental works through June 2021.

Then we worked on identifying the trust instruments themselves.  Here inclusion criteria were further tightened to instruments developed and used in human-subject studies, and the whole instrument had to be accessible.  For instruments published or validated over multiple papers, relevant citations were combined across papers with duplicates removed.  In the end, we identified 62 unique survey instruments.  This process is outlined in Fig. \ref{fig:3_0Prisma}.

\begin{figure}
    \centering
    \includegraphics[width=6in]{HF_Formatted_Version/figures/Fig3_0Prisma.png}
    \caption{PRISMA diagram of literature review for identifying }%%FIX ME
    \label{fig:3_0Prisma}
\end{figure}

While we may have missed some trust instruments using this approach, it is unlikely that there remain undiscovered instruments that are widely used.  What is more likely is that instruments that are newer or less utilized, such as those primarily in use in a single lab or research group,  may have been overlooked.  However, the results produced here can be taken as indicative of the trends in the field. 


% \section{Surveying the Field: Identification of Trust Instruments}\label{sec: survey}
%  \subsection{Sampling Methodology}\label{sec:sampling}
% To identify the broadest range of trust survey instruments in HAI/HRI/HCI, Google Scholar was first used to assess the scope of the field.  Searches using various combinations of the terms \textit{human}, \textit{robot}, \textit{machine}, \textit{computer}, \textit{automation}, and \textit{trust} yielded 43,500 papers. Narrowing our view to experimental work, we found approximately 21,000, or just under half, actually mentioned \textit{subjects} or \textit{participants}.  Even after this narrowing, we recognized that a complete, systematic review of every paper was unfeasible.  Thus, we employed multiple approaches to generate samples to analyze that were more directly relevant to our stated aims of identifying the survey instruments used.   
% First, multiple significant and recent trust-adjacent literature reviews were analyzed \cite{McKnightD2011,french2018trust, Hoffman2013,hancock2021evolving,brzowski2019trust}. 
% Second, for completeness, we conducted our literature review, identifying what appeared to be the most cited, influential, impactful, used, or reworked instruments. 



% \begin{figure}
%     \centering
%     \includegraphics[width=6in]{figures/Fig3_0Prisma.png}
%     \caption{PRISMA diagram of literature review for identifying }
%     \label{fig:3_0Prisma}
% \end{figure}

% Each time a paper in either literature review mentioned another method/survey instrument, whether used directly or not, that paper was added to the set of collected trust instruments, snowballing our data set.  For instruments published or validated over multiple papers, relevant citations were combined across papers with duplicates removed.  Finally, the most relevant papers, as deemed by Google Scholar, Semantic Scholar, SCOPUS, and IEEE, using the exact keywords and search combinations mentioned previously, were used to confirm that the most influential works were indeed identified.  




% It is also worth noting that upon further inspection, the original sample size (indicating 21,000 works of interest) found that the vast majority of papers focused solely on behavioral measures of trust and thus were not relevant to this work.  Domains such as cyber, network, and IOT predominantly measure trust using tests of (simulated) behavioral trust using algorithmic approaches to calculate reputation or Bayesian expectations are the norm, and the actual measurement of the initial trust inputs is assumed (e.g., \cite{meng2018towards}).  Further refinement and estimation of the actual number of direct experimental works using trust survey instruments are discussed in the following section.

% While we may have missed some trust instruments using this approach, it is unlikely that any undiscovered instruments are widely used.  Likely, newer and less well-known instruments utilized primarily in a single lab or research group may need to be noticed.  However, the results here indicate that the general broader field and the most utilized and influential instruments have been captured.  

% %%After this further filtering, 3914 unique papers remained.  See paragraph 203


\subsection{Findings and Down Selection for Analysis}\label{sec:quality}
As shown in Fig. \ref{fig:alluvial}, HMI trust instruments can be divided into two major classes, those that treat trust as a single factor and those that decompose it along multiple factors.  These classes have about equal representation within the sample.  It can also be seen that while HMI trust measurement slowly gained steam starting from the late 1980s, the past decade has seen an explosion in new trust survey instruments, as illustrated in Fig. \ref{fig:hist1}.  


\begin{figure}
\centering
\includegraphics[trim = 0 0 0 0, width=0.7\columnwidth]{Fig3_1_SurvHist.png}
    \caption{Number of unique multi-item ($>2$) HAI/HRI/HCI trust survey instruments by year published, broken down by the quality of empirical reliability and validity reported}
    \label{fig:hist1}
\end{figure}


As illustrated in Table \ref{tab:Maj_Stats}, three very intriguing patterns emerged.  First, the single- and two-item surveys for assessing trust are very popular (12.6\% and 12.0\%, respectively), which is unsurprising given their simplicity and ease of administration and that in many fields, trust is only one of several measures being assessed in a study.  The benefits and pitfalls of such short assessments of trust are given a complete treatment by K\"{o}rber \cite{Korber2018}.  They provide a quick way to probe trust in the moment, making them suitable for inter-task assessments and tasks where time or access to training is limited.   K\"{o}rber further points out that uni-dimensional trust instruments' biggest weakness is their high variance in repeated measures, undermining their reliability.  Furthermore, they cannot explain the factors that underlie or lead to trust or lack of it.

%(herein designated the Top 12).  While it is expected that older instruments would have more citations, impact, and usage, this bias was even more pronounced in the literature reviews, despite only being published over the last three years.

Second, the provenance of the different instruments varied substantially.  Nearly half of the sampled experiments used one of just 12 instruments.  Fully one-third of experiments opted to develop their own survey, though often incorporating individual questions and even entire factors from the other 62.  Over one quarter used one of the other 50 identified instruments wholesale.  Thus, while the field exhibits a high degree of variability, certain instruments have proven more impactful than others. 

Third, severe divisions and terminological mismatches exist between sub-fields within trust research that have contributed to the fractiousness and confusion \cite{lewis1985trust,Gefen2003, french2018trust,krausman2022trust}.  Taken all together, these patterns indicate there is much competition and critical differences in the field, such that newcomers seeking a trust instrument may need help to choose from among the options.  This chapter will address each of these patterns in turn.    

\begin{table}[b]
    \centering
    \normalsize
    \caption{Overall statistics of the surveys used by papers included in the literature review ($n=173$). Because use could be in whole or in part, the numbers do not add up to 100\%\\  * Within which 57\% used (Lee \& Morray, 1994)}
    \begin{tabular}{lc}
      \begin{tabular}{@{}l@{}} \textbf{Instrument}\\ \textbf{Category}\end{tabular}   & \begin{tabular}{@{}c@{}}\textbf{Frequency in}\\ \textbf{Lit. Reviews}\end{tabular}  \\\hline
        Single Question & 12.6\% \\
        Two Question &  12.0\%$^*$\\
        Developed Own & 33.7\% \\
        Used Top 12 & 47.9\% \\
        Used Others & 26.9\% \\
    \end{tabular}
    \label{tab:Maj_Stats}
\end{table}
 

\paragraph{Most Popular Instruments.}
Twelve of the 62 instruments accounted for nearly half (47.9\%) of citations in the identified literature reviews (Table \ref{tab:Maj_Stats}), similar to the findings of Brzowski and Nathan-Roberts \cite{brzowski2019trust}.   The popularity of the majority of these instruments (8) within HRI, in particular, has been confirmed in an independent literature review \cite{kohn2021measurement}.  The pattern of citations in experimentally-focused HMI papers closely reflected the frequency of usage of each instrument in the literature review, which helps validate that our literature sample statistically reflects the experimental literature overall Table \ref{tab:overall_stats}.  Of these most used 12, McKnight's and Gefen's instruments \cite{McKnightD2011,Gefen2003} have had the most influence, with the greatest number of general citations both in background and method sections.  However, their work also goes far beyond the fields covered in our scope, covering studies of trust in management science, e-commerce, purchasing, and branding, including much work on human-human and human-company trust, though often technologically mediated.  Thus, they have a relatively weaker showing in the robotics and automation literature review.  On the other hand, some commonly-cited instruments were barely used in practice (e.g., Fluency \cite{Hoffman2013}, SATI \cite{SATI}, CTI \cite{Chien2014}, Schaefer \cite{schaefer2013perception}, the German TiA \cite{Korber2018}), in part reflecting the deep divisions and lack of overlap between even the seemingly closely related fields that study human-machine trust, such as fluency, automation trust, e-commerce, and human-computer interaction.   %This should not come as a surprise to the practitioners in these fields, who have long bemoaned construct proliferation and the lack of a consensus definition.  


\paragraph{Assessing Instrument Quality}
A common finding in our review was the high number of trust instruments were seemingly ad hoc creations.  On the other hand, several other instruments were carefully crafted and painstakingly validated.  Thus, it became clear that our survey should take on a second, important dimension - to compare the quality of the instruments, as well as their availability and utility.  

% This is the only example in the literature to provide such a complete picture of the state of the field.

%The primary purpose of this work is not only to understand the survey instruments that exist and have been used but, perhaps more importantly, to investigate their quality. 

To assess the current quality of field surveys, we classified them by the quality of their internal reliability and quantitative validity.  As discussed above (Section \ref{sec:rel}), internal reliability can be calculated in many ways.  While the standard is Cronbach's $\alpha$, reporting it alone is insufficient to establish the test's reliability unless its assumptions are met.  This is true of all the internal reliability tests; researchers must account for sample size, missing data, outliers, skew, kurtosis, heterogeneity, and $\tau$-equivalence.  Reporting more than one test also increases confidence that the reliability is accurate.  Thus, we understood a test to have \textit{Complete Reliability} if it reported internal reliability and checked that the data met the test's assumptions. 

For validity, the gold standard is a complete exploratory factor analysis (EFA), including reporting the number of subjects, rotation used, factor loadings, justified choice of factor number, and descriptive statistics - especially including the number of items per factor.  Failing this, however, a confirmatory factor analysis (CFA) then it was considered sufficiently complete if it included Root Means Square Error (RMSEA) and at least two fit statistics such as $\chi^2$, the Standardized Root-Mean Squared Residual (SRMR), the Comparative Fit Index (CFI), or the Tucker-Lewis Index (TLI).  Instruments meeting all appropriate minimally sufficient criteria were classified as reporting \textit{Complete Reliability and Validity}.  If some of these elements were missing, but more effort was made and reported than Cronbach reliability alone, the instrument was categorized as having \textit{Some Reliability and Validity}.  Those that reported no reliability or validity analysis or only Cronbach's internal reliability were classified as \textit{No Reliability or Validity}

It is important to note that carrying out appropriate analyses alone does not mean that the results met any acceptable thresholds, however.  Criteria for assessing the actual quality of the validity and reliability are discussed below in Section \S\ref{sec:goodness}.  Furthermore, while we have done our best to identify the most influential and best-validated instruments currently, this is liable to change. 

%Those that reported no reliability or validity analysis or only Cronbach's internal reliability were straightforward to categorize.  Beyond that, we deemed complete reliability as those which included at least one better assessment of reliability (e.g., McDonald's Omega, test-retest).  For validity, a complete exploratory factor analysis (EFA) meant reporting the number of subjects, rotation used, factor loadings, justified choice of factor number, and descriptive statistics - especially including the number of items per factor.  If, on the other hand, they reported a confirmatory factor analysis (CFA), then it was considered sufficiently complete if it included Root Means Square Error (RMSEA) and at least two fit statistics such as $\chi^2$, the Standardized Root-Mean Squared Residual (SRMR), the Comparative Fit Index (CFI), or the Tucker-Lewis Index (TLI).  Instruments meeting all appropriate minimally sufficient criteria were classified as reporting \textit{Complete Reliability and Validity}.  If some of these elements were missing, but more effort was made and reported than Cronbach reliability alone, the instrument was categorized as having \textit{Some Reliability and Validity}.  However, carrying out appropriate analyses alone does not mean that the results met acceptable thresholds.  Criteria for assessing the actual quality of the validity and reliability are discussed below in \S\ref{sec:goodness}.  Furthermore, while we have done our best to identify the most influential and best-validated instruments currently, this is liable to change. 

Many of the 62 instruments, especially those that measure trust as a single factor, lack reported reliability or validity (Fig. \ref{fig:alluvial}), with 24\% having no validity or reliability analysis of any sort and another 24\% only reporting Cronbach's $\alpha$ for internal reliability.  Only 17\% of studies with a single-factor instrument report any level of both empirical validity and reliability analyses, compared to 54\% of multi-factor instruments.  This is unsurprising, given that multi-factor instruments are subjected to greater scrutiny of dimensional validity.  Even so, 30\% of multi-factor surveys did not report any reliability and validity testing (Fig. \ref{fig:alluvial}).  Despite repeated calls for stronger instrument validation \cite{Gefen2003,Korber2018}, the number of studies utilizing instruments without validation has increased over the last decade.  However, there are currently some promising signs of that trend reversing (see the most recent years in Fig. \ref{fig:hist1}).  

\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{HF_Formatted_Version/figures/Fig3_2_sankeymatic_1200x1200-2.png}
    \caption{The categorization of HAI/HRI/HCI trust survey instruments by the quality of validation and reliability. 
    Instruments were first divided by whether they treated trust as a single factor or composed of multiple factors.  Full analysis means that a proper reliability technique was used as well as either a CFA or EFA - with their loadings, error, and fit statistics reported.}
    \label{fig:alluvial}
    % \Description{An alluvial plot shows that 29 of the 62 trust surveys are single and 33 are multi-factor.  It then shows how many of each type fall into each quality category: 15 had no analysis, 15 with Cronbach reliability only, 3 with alternative analyses, 11 with some reliability and validity, and 18 with complete, or sufficient, reliability and validity.}
\end{figure}

 
% We accomplished our goal by performing a meta-analysis of validated survey instruments and their respective factor analyses.  The surveys were identified by their validation, impact, number of citations, and number of actual uses in experiments.  They total 12,567 citations (Google Scholar).  Narrowing our search to the number of citing papers that mentioned running "subjects" or "participants," we found that these instruments were cited 2089 times.  Of those that reported testing for validation, most did a two-level analysis, averaging 87$\pm$19 participants per round.  

It is worth noting a particular trade-off in types of validity evident throughout our review, lack of survey reuse.  Among those that used or incorporated other surveys, only 17\% used other trust instruments wholesale.  In contrast, the remainder used other previously validated instruments to get at specific individual antecedents or trust-related factors.  Reusing surveys allows for direct comparison and can help validate the reliability and generalizability of an instrument.  On the other hand, creating new surveys can help confirm the nomological/construct validity of both the new survey and the parallel constructs in previous works \cite{boudreau2001validation}.   

The most common of these were instruments that aim to capture \textbf{\emph{Dispositional trust}}, such as either \emph{Faith in Technology} or \emph{Faith in People}.  \emph{Faith in Technology} was most often captured with the Complacency Potential Rating Scale \cite{IdramaniL.SinghRobertMalloy1993} (4 studies) or the Negative Attitudes Towards Robots Scale \cite{nomura2006measurement} (6 studies).  \emph{Faith in People} was measured with instruments like Rotterâ€™s Interpersonal Trust Scale \cite{Rotter_67}, or the instruments of \citeA{moorman1993factors}, \citeA{mcshane2014propensity}, \citeA{wheeless}, and \citeA{anderson1990development}.  The next most common area to try and capture independently are assessments of trust antecedents or correlates such as likeability, bonding, human likeness/anthropomorphism, or affect \cite{Heerink2009,VanDerLaan1997,Merritt2011,kidd2008robots,watson1988development,de2005assessing}.  Other areas measured included \emph{Suspicion} \cite{Lyons2012,selkowitz2017using},\textit{Situation Awareness} \cite{DeVisser2011}, \textit{Mood} \cite{Petersen2018}, \textit{Risk} \cite{Petersen2018,rajaonah2008role,Desai2012}, \textit{Transparency} \cite{Rupp2016}, and \textit{Intention to Work} \cite{Erebak2019}.  Finally, \emph{Workload}, both with the NASA task load index (TLX) and other measures, was the most popular non-trust-related measure (12 studies).


\begin{table*}
    \centering
        \caption{Citation Statistics of the 12 Most Cited, Used, or Impactful Survey Instruments for Capturing Trust across HRI, HAI, HMI, HCI, e-commerce, Technological Acceptance, and Fluency by year, including for citations (cit.) in experimental work (exp.).}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{l|c|c|c|c|c|c|c|c||c}
        &&\multicolumn{4}{c|}{\underline{Google Scholar}}&\multicolumn{3}{c||}{\underline{Semantic Scholar}}& \\
    Survey & Year & \begin{tabular}{@{}c@{}}General\\ Citations\end{tabular} & \begin{tabular}{@{}c@{}}Exp.\\ Citations\end{tabular} & \begin{tabular}{@{}c@{}}Gen.\\ Cit./year\end{tabular} & \begin{tabular}{@{}c@{}}Experimental\\ Cit./Year\end{tabular} & \begin{tabular}{@{}c@{}}Background\\ Section\end{tabular} & \begin{tabular}{@{}c@{}}Method\\ Section\end{tabular} & Highly Influential & \begin{tabular}{@{}c@{}}Frequency in \\ Lit. Reviews\end{tabular}\\\hline
      Muir  & 1996 & \cellcolor{red!27}960 & \cellcolor{red!70}351 & \cellcolor{red!7}36 & \cellcolor{red!52}13 & \cellcolor{red!24}613 & \cellcolor{red!20}92 & \cellcolor{red!16}98 & \cellcolor{red!37}8.0\% \\ \hline
      Jian et al. & 2000 & \cellcolor{red!30}1027 & \cellcolor{red!100}473 & \cellcolor{red!10}49 & \cellcolor{red!88}22 & \cellcolor{red!3}78 & \cellcolor{red!20}93 & \cellcolor{red!4}22 & \cellcolor{red!100}21.7\%\\ \hline
      \begin{tabular}{@{}l@{}}Madsen \&\\ Gregor (HCT)\end{tabular} & 2000 & \cellcolor{red!10}293 & \cellcolor{red!26}131 & \cellcolor{red!3}14 & \cellcolor{red!24}6 & \cellcolor{red!3}79 & \cellcolor{red!10}48 & \cellcolor{red!5}34 & \cellcolor{red!9}4.0\% \\ \hline
      Gefen (TAM) & 2003 & \cellcolor{red!100}8832 & \cellcolor{red!23}118 & \cellcolor{red!100}491 & \cellcolor{red!24}6 & \cellcolor{red!100}2353 & \cellcolor{red!100}549 & \cellcolor{red!100}615 & \cellcolor{red!20}4.4\% \\ \hline
      SHAPE (SATI) & 2003 & \cellcolor{red!3}60 & \cellcolor{red!5}24 & \cellcolor{red!1}3 & \cellcolor{red!4}1 & \cellcolor{red!0}6 & \cellcolor{red!1}6 & \cellcolor{red!0}4 & \cellcolor{red!0}0.6\%\\ \hline
      Heerink/UTUAT & \begin{tabular}{@{}c@{}}2009,\\2011\end{tabular} & \cellcolor{red!20}638 & \cellcolor{red!46}231 & \cellcolor{red!11}58 & \cellcolor{red!84}21 & \cellcolor{red!7}174 & \cellcolor{red!25}119 & \cellcolor{red!9}56& \cellcolor{red!13}2.9\%\\ \hline
        McKnight & 2011 & \cellcolor{red!55}5725 & \cellcolor{red!20}100 & \cellcolor{red!60}301 & \cellcolor{red!20}5 & \cellcolor{red!62}1560 & \cellcolor{red!90}428 & \cellcolor{red!78}478 & \cellcolor{red!24}5.1\% \\ \hline
      \begin{tabular}{@{}l@{}}Merritt \\ (Propensity to Trust)\end{tabular} & \begin{tabular}{@{}c@{}}2008,2011,\\2015,2019\end{tabular} & \cellcolor{red!15}510 & \cellcolor{red!48}239 & \cellcolor{red!9}44 & \cellcolor{red!81}21 & \cellcolor{red!8}204 & \cellcolor{red!10}45 & \cellcolor{red!7}46 & \cellcolor{red!13}2.9\% \\ \hline
      Hoffman (Fluency) & 2013 & \cellcolor{red!8}177 & \cellcolor{red!12}62 & \cellcolor{red!4}22 & \cellcolor{red!32}8 & \cellcolor{red!3}62 & \cellcolor{red!10}45 & \cellcolor{red!2}11 & \cellcolor{red!0}0.6\% \\ \hline
      Schaefer  & 2013 & \cellcolor{red!6}121 & \cellcolor{red!11}56 & \cellcolor{red!3}15 & \cellcolor{red!28}7 &\cellcolor{red!2}50 & \cellcolor{red!8}36 & \cellcolor{red!3}16 & \cellcolor{red!3}0.6\%\\ \hline
      Chien (CTI)  & 2014 & \cellcolor{red!2}38 & \cellcolor{red!3}15 & \cellcolor{red!1}5 & \cellcolor{red!8}2 & \cellcolor{red!0}3 & \cellcolor{red!0}0 & \cellcolor{red!0}0 & \cellcolor{red!3}0.6\%\\ \hline
      K\"{o}rber (German TiA)  & 2018 & \cellcolor{red!7}155 & \cellcolor{red!15}75 & \cellcolor{red!10}52 & \cellcolor{red!100}25 & \cellcolor{red!1}11 & \cellcolor{red!4}17 & \cellcolor{red!0}3& \cellcolor{red!3}0.6\%\\ \hline \hline
      \textbf{Total}&&\textbf{18536}&\textbf{1875}&\textbf{1090}&\textbf{137}&\textbf{5193}&\textbf{1488}&\textbf{1383}&\textbf{47.9\%}%\hline
    \end{tabular}
    }
    \label{tab:overall_stats}
\end{table*}
 
\paragraph{Terminology Challenges}  Upon first inspection, there appears to be little overlap with how various instruments decompose trust and, thus, how they assess it.  Further probing reveals that a conceptual consensus is emerging behind the terminological differences.  The following section in this chapter will work to provide a unifying language and then apply that language to the selection of papers chosen so that a complete understanding and comparison can be made.  

\begin{table}
\centering
    \caption{Multi-factorial HMI trust surveys that reported sufficient reliability and validity. The number of subjects, number of relevant trust antecedent and factors, and number of inter-factor relationships reported are listed. For factor mappings between the surveys see Tables \ref{tab:termMap1} \& \ref{tab:termMap2}, Appendix A.}
    \resizebox{\columnwidth}{!}{%
\begin{tabular}{l|c|c|c}
\textbf{Survey Name or Authors, if scale unnamed} & \# Subjects & \# Rel. Factors & \# Inter-factor Relations Tested \\\hline
Gefen's TAM with Truat \cite{Gefen2003} & 213 & 8 & 12 \\\hline
Benbasat and Komiak (v1; 2005) \cite{Benbasat2005} & 120 & 7 & 9\\\hline
Komiak and Benbasat (v2; 2006) \cite{Komiak2006} & 100 & 7 & 10 \\\hline
Trust in RA \cite{Wang2008} & 120 & 7 & 6\\\hline
Merritt's collected Trust-related Scales \cite{Merritt2008,merritt2019automation,merritt2011affective} & 255 & 4 & 6 \\\hline
UTUAT with Trust \cite{Heerink2009} & 30 & 8 & 19\\\hline
Trust in Specific Technology \cite{McKnightD2011} & 359 & 7 & 15 \\\hline
Trust in IT Artifacts \cite{Sollner2012} & 284 & 4 & 3 \\\hline
German TiA \cite{Korber2018} & 94 & 4 & 6 \\\hline
Attitudes toward Affective Technology\cite{Freude2019}&303&7&9\\\hline
Hegner, Beldad, \& Brunswick \cite{Hegner2019}&369&5&7\\\hline
Tussyadiah, Zach, \& Wang \cite{Tussyadiah2020}&625&6&5\\\hline
Park \cite{Park2020}&406&7&6\\\hline
TOAST \cite{Wojton2020}&331&2&1\\\hline
SSRIT \cite{Chi2021}&326&4&6\\
    \end{tabular}}
    \label{tab:factor_summary}
\end{table}


\begin{table}[b]
    \centering
        \caption{The number of multi-factorial trust surveys that found each identified factor of significant reliability and validity (N=25). Familiarity includes reputation and recommendation.}
    \begin{tabular}{l|c|c}
    \textbf{Factor} &	\textbf{Number of Surveys} & \textbf{Percent Represented} \\
    \textbf{Dispositional}&&\\
     Faith in Technology&	10 & 40\%\\
     &&\\
     \textbf{Situational}&&\\
     Familiarity  &	6 & 24\%\\
     Situation Normality&	6& 24\%\\
    Emotional Response&	11 & 44\%\\

    &&\\
    Shared Mental Model &	12 & 48\%\\
&&\\
 \textbf{Learned}&&\\    
Structural Trust &9 & 36\%\\
Capability-based Trust &	19 & 76\%\\
Affective Trust &	15 &60\%\\
General Trust &	14 &56\%\\
&&\\
Intention to Use &	12&48\% \\
    \end{tabular}
    \label{tab:factor_summary2}
\end{table}

\section{Mapping and Defining the Factors and Antecedents of Human Machine Trust}
Thus, between the bottom-up analysis of individual items and the clustering of commonalities between the reliable and validated multi-factorial survey instruments,  we ultimately grouped each set of identified categories as cleanly as possible into 10 factors.  A mapping of the reliable and validated factors from the sampled surveys is displayed in Appendix A, Tables \ref{tab:termMap1} and \ref{tab:termMap2}.   The purpose of this is to primarily show that despite the vast diversity in terminology and dozens of varying survey instruments coming from a multiplicity of fields and approaches, the measurement of trust across human-system interaction is converging.

In order to exploit the increased cross-discipline understandability this convergence makes possible, and for greater clarity in this paper, we will now proceed to describe each of these critical factors for understanding human-machine trust.  The following factors are the most commonly found to be distinct, reliable, and have multiple forms of validity (i.e., surface, construct, internal, external).  We do not claim these constructs are perfect or that the terms we have chosen are the best.  However, we believe they capture the emergent consensus among the experimental literature and the collected measures.

One point to consider is which factors should be considered antecedents instead of direct or proper trust factors.  This is a complex question, as different approaches treat trust differently.  For some, \textit{Faith in Technology} would be considered a proper trust factor \cite{IdramaniL.SinghRobertMalloy1993,Hegner2019,Merritt2011}.  Others might consider \textit{Shared Mental Model} one instead \cite{Wojton2020,Rupp2016}.  Herein, because many in the field want to differentiate between antecedents of trust and trust itself, the choice was made to name those factors that are more downstream, per the meta-analysis in Section \ref{sec:meta-anal}, as more direct factors of trust and more upstream ones as antecedents.  However, we acknowledge that this distinction is not sharp and may be of limited utility. 

\subsection{Dispositional Level}
The term \textbf{Dispositional} herein refers to beliefs or attitudes that exist before one is considering or in a particular situation or interaction; we still recognize that this level may be composed of multiple distinct factors.  While there may be many components, they generally fall along three lines: faith in persons, faith in institutions, and faith in technology.  These are not natural dispositions in the same sense that Hoff and Bashir employed the term \cite{Hoff2015} but are generic attitudes.  Some of the oldest multi-item trust measurement instruments were those measuring faith in institutions and persons \cite{Rotter_67,smith2019general}.  However, these surveys have rarely been used in conjunction with HMI research.

On the other hand, faith in technology is relatively common, appearing as a factor within 10/25 reliable and validated multi-factor surveys (see Tables \ref{tab:factor_summary},\ref{tab:factor_summary2}).  Just because \textbf{Dispositional} factors are not in the \textbf{learned trust} layer, that does not mean they, or those in the \textbf{situational level}, are static.  They may get updated but at much slower time scales than \textbf{learned trust}, as previous interactions shape future dispositions, sense of situation normality, familiarity, and future emotional responses.


\subsubsection{Faith in Technology}
This factor is defined by how much faith one has in technology, usually generally, though sometimes in a specific technology.  \emph{Faith in technology} is a form of \textbf{dispositional trust}, that is, it is not particular to the given situation or even technology involved but one's general trust, though it may be specific to a given type of technology or socio-technical interaction (e.g., robots, online shopping, automation).  While this can be thought of as a form of trust, in terms of a given trust interaction, this is more of a trust antecedent or \textit{prior} that may indicate how a person will feel and what they may anticipate upon first encountering a specific system.  We also choose to term it \textit{faith}, in alignment with  \cite{McKnightD2011}, to stress that it may not be based on any evidence or experience.

\emph{Faith in Technology} is also called \emph{Propensity to Trust Machines} \cite{IdramaniL.SinghRobertMalloy1993}, \emph{Trusting Stance}, or simply \emph{Faith} \cite{McKnightD2011}, \emph{Attitude} \cite{Heerink2009,Scopelliti2005,nomura2008prediction}, and \emph{Anxiety} \cite{Heerink2009,nomura2006measurement}.  It is often measured with pre-existing \textbf{\emph{Dispositional trust}} survey instruments aimed at capturing attitudes and anxiety toward robots or automation, such as the Complacency Potential Rating Scale (CPRS) \cite{IdramaniL.SinghRobertMalloy1993}.  One validated standalone variant of this factor that specifically looks at \emph{faith in technology} from the view of high expectations of technology and all-or-none thinking is the Perfect Automation Schema  \cite{merritt2015measuring}.  If the questions are left open about a generic "robot," then this factor may also include the Robot Anxiety Scale (RAS) and the Negative Attitude towards Robots Scale (NARS) \cite{nomura2008prediction}.  However, those scales may correspond more closely to \emph{Emotional response}, below, if measured within a specific situation or interaction.  Given the wide range of facets that the dispositional may capture, one outstanding question is whether this factor is homogeneous.

\subsection{Situational Level}
\textbf{Situational} factors are those that are contextually relevant to the specific situation or interaction but exist prior to the current interaction.  These draw upon previous knowledge or experience with elements of the system at hand (\textit{familiarity}), similar systems (\textit{situation normality}), and what the design of the system or the quality of the interaction evoke (\textit{emotional response}).  These all inform the mental model the user formulates of the system, its goals, capabilities, preferences, and procedures.  \textbf{Situational} factors are not normally considered forms of trust in and of themselves but as their antecedents.  

\subsubsection{Familiarity}
At the heart of the \textbf{\emph{situational}} antecedents stands \textbf{Familiarity}, which is not only about indirect knowledge but also includes direct experience with the vendor, product, or brand.  The first part of \textbf{familiarity} includes indirect acquaintance such as \emph{reputation}, \emph{recommendation}, and \emph{Social Influence} \cite{Korber2018}.  These sub-factors are central to \textbf{Familiarity} in human-human trust \cite{yamagishi2001} and marketing \cite{ha2005effects}, though mostly ignored in HAI/HRI.  It is primarily termed \textbf{familiarity} by most surveys that employ it, except for Scopelliti's somewhat confusingly named category: \textit{Capabilities of Robots} \cite{Scopelliti2005}.   This factor may be overlooked when trust is measured for users interacting with newly developed systems, especially in academic settings, given the technologies' lack of branding and novelty.

\subsubsection{Situation Normality}
In such cases, it may be preferable to focus on \textbf{Situation normality}, which is similar to \textbf{familiarity} but more indirect.  Similar experiences with other technologies or machine behaviors and functionalities create recognizable parallels from which the human's expectations are formed.  It is usually referred to as \textbf{Situation(al) Normality} \cite{Gefen2003,McKnightD2011,Freude2019,Park2020} but sometimes construed under \emph{life-likeness} \cite{Powers2007}, in a limited functional/behavioral sense.  

\subsubsection{Emotional Response}
Defining the appropriate scope of this category and naming it required much thought, and its cohesiveness as a single factor requires further substantiation.  The unique name of this category is based on the increasing acceptance of the term \textit{emotive} to describe the response evoked by robots when studying these factors in relation to trust \cite{schaefer2013perception,Khalid2020,SATI,french2018trust,Kim2020}.  This category spans factors such as \emph{warm} \cite{Lee2015}, \emph{sociable} \cite{Powers2007,Heerink2009}, \emph{pleasant, friendly, likeable} \cite{Merritt2011,Rau2009}, \emph{evoking of comfort}, and \emph{attachment} \cite{Chi2021,Madsen2000}, \emph{engagement} \cite{Park2020}, \emph{seriousness} \cite{Hammer2015}, \emph{dominance} \cite{Powers2007}, and \emph{professionalism} \cite{Skarlatidou2013}.  Related but negatively valenced factors exist may be \textit{anxiety} \cite{nomura2006measurement}, and more general \textit{negative attitudes} \cite{nomura2008prediction} when measured once a situation has been initiated. 

The importance of \textbf{emotional response} has been increasingly acknowledged by HRI, especially in affective robotics \cite{Mann2015,schaefer2013perception,Merritt2011}.  Anthropomorphism likely affects trust through this antecedent, as does the user interface more generally \cite{Deligianis2017,hauslschmid2017supportingtrust,Zanatto2020}.  Note that we chose \textbf{Emotional response} as it focuses on the human's internal psychological assessment and reaction as opposed to the physical or functional properties of the system being interacted with, which better encompasses the factors among the survey instruments.


\subsection{Shared Mental Model}
\textbf{Shared Mental Model} was chosen as an existing term that broadly captures one's perception of a system's understanding of their interaction and the (lack of) confidence that brings about. This category has seen the highest variability due to construct proliferation and poor understanding.  Consequently, our naming schema deviates more from those currently used as this concept than the other categories.  It has been variously called: \emph{Perceived Understandability/Technical Competence} \cite{Madsen2000}, \emph{Intention of Developers+Understanding/Predictability} \cite{Korber2018}, \emph{Human-Robot Interaction} \cite{Scopelliti2005}, \emph{Responsiveness} \cite{Powers2007}, \emph{Process Transparency} \cite{Chien2014}, \emph{Intentionality} \cite{Ullman2014}, \emph{Comfort of Use and Transparency} \cite{Hammer2015}, \emph{Process} \cite{Sollner2012,Park2020}, \emph{Understanding} \cite{Wojton2020}, \emph{Wearable Technology Trust} \cite{Rupp2016}, and \emph{Perceived Ease of Use} \cite{Gefen2003,Benbasat2005,Hegner2019}.  

While similar, the \textbf{Shared Mental Model} can be thought of as a subset of a theory of mind system which infers cognitive states. It specifically is the part which models the trustee's understanding of the trustor's goals, methodological preferences, their relationship and interaction, and what the trustor believes the trustee knows of them personally.  The inferred cognitive states that a shared mental model embedded in a theory of mind support can be termed \emph{Situation Awareness} \cite{Andrews2022}.  \textbf{Shared Mental Models} are the infrastructure that allow \emph{Situation Awareness} but match the factors among the survey literature more clearly, and SMMs themselves are not as directly tied to predictability.  The formation of a \textbf{Shared Mental Model} on the current perceived state of the interaction yields the `highest' level of  \emph{Situation Awareness}, the ability to project how the situation will unfold into the future \cite{Andrews2022}.  However, the SMM itself does not include predictability but serves as an antecedent to it \cite{Andrews2022}, which makes sense as the literature supports that in trust measurement predictability itself is correlated with \textbf{Capability-based Trust}.


While here we categorized it as an antecedent of trust, it seems to exist between situational and learned trust.  The \textit{shared mental model} is initially formed by the \textbf{dispositional} and \textbf{situational} levels but, unlike them, gets quickly and continuously updated during the trust interaction as one learns more.  

\subsection{Learned Trust}
Within the \textbf{\textit{Learned}} level, we are now talking about forms of actual trust and no longer antecedents.  As such, all of these factors directly have \textbf{trust} in their names.  These are factors that continuously are re-assessed in the interaction as it dynamically progresses between the system and the human and appears to be composed of four main factors.

\subsubsection{Structural Trust}
This factor is defined as the belief that the trustee will abide by whatever cultural and behavioral norms, morals, agreements, and laws the trustor expects.  \textbf{Structural Trust} includes the motivations of the trustee to follow such norms, including having a sense of integrity, responsibility, honor, and shame.  This factor has also been termed \emph{Reliance} \cite{IdramaniL.SinghRobertMalloy1993}, in addition to \emph{Credibility-Character} \cite{Rau2009}, \emph{Fairness} \cite{Ullman2014}, \emph{Honesty} \cite{Ullman2014}, and \emph{Integrity} \cite{Benbasat2005}.  This type of trust is closely related to perceived moral agency \cite{Banks2019}, though that is more about the capacity for morality than the trust placed in shared norms.  Hence, why the author of that study depended on \cite{Rempel1985} to measure trust separately.  While an instrument designed for human-human interpersonal trust, \cite{Rempel1985} can be understood as specifically focusing on the \textit{shared mental model}, \textit{structural}, and \textit{affective trust}.

We specifically chose the word \textbf{trust} within the factor name to differentiate from \textbf{assurance}, \textbf{integrity}, \textbf{honesty}, and \textbf{fairness}. It is not \textbf{Structural Assurance}, which only focuses on regulatory and institutional mechanisms \cite{McKnightD2011,Gefen2003, Freude2019,Park2020}, and does not also include beliefs about the trustee's motives or likelihood of compliance with structural mechanisms.  This additional aspect of \textbf{Structural Assurance} integrates a normative framing, while remaining open to both internally and externally driven motives.  In additional to allowing for external motivations, we also choose to call this a type of \textit{trust} instead of \textbf{integrity}, \textbf{honesty}, and \textbf{fairness} as these terms are ambiguous as to whether they refer to beliefs of the trustor or properties of the trustee (i.e., their trustworthiness).   

\subsubsection{Capability-based Trust}
This category, whose name is based on \citeA{Freude2019}, broadly captures whether a system has the resources and capabilities to perform a task (\emph{competence}) \cite{Benbasat2005,Komiak2006,Korber2018}, one's belief in that competence, whether based on experience or faith (\emph{confidence}) \cite{Lee_94,Lee2004,josang2016subjective}, the belief that the system performs the task reliably (\emph{reliance}) \cite{Madsen2000,Korber2018,McKnightD2011,Ullman2014,Tussyadiah2020}, and that the system can recover from errors (\emph{robustness}) \cite{SATI,Madsen2000}  Depending on the system, security, privacy, and accessibility may all be perceived as falling under \textbf{Capability-based Trust}, though they are sometimes more accurately framed as elements of \textbf{Structural Trust}.  Other factors that may capture all or part of these concepts are \textit{usefulness} \cite{Gefen2003,Heerink2009,Heerink2011}, \textit{attitude} \cite{Heerink2009}, \textit{functionality} \cite{McKnightD2011,schaefer2013perception,Tussyadiah2020}, \textit{performance} \cite{Sollner2012,schaefer2013perception,Chien2014,Park2020,Wojton2020}, \textit{fluency} \cite{Hoffman2013}, robot-relative contribution \cite{Hoffman2013}, and \textit{capable} \cite{Ullman2014}.  Some surveys take all trust or trustworthiness as capability-based and term the factor accordingly \cite{IdramaniL.SinghRobertMalloy1993,Merritt2008,Chi2021}.  

Another common but confusing term for this factor is \emph{cognitive or cognition-based trust}\cite{johnson2005cognitive,tussyadiah2018,Komiak2006,schaefer2013perception}, as all trust emerges from cognition.  This usage rested on the assumption that \emph{cognitive trust} was evidence-based and rationally derived as opposed to \textbf{affective trust} arising from blind emotions \cite{lewis1985trust}.  This division is based on the philosophical understanding that one form of trust is irrational because it is tied to emotions.  However, the debates  over the rationality of emotions \cite{pham2007emotion,gubka2022there,haselton2006irrational,solomon1973emotions,de1979rationality} and cognition as computation \cite{van1995might,piccinini2020neurocognitive} are long standing.  Thus, despite less historical weight, we chose the term \textbf{capability-based trust} over \textbf{cognition-based or cognitive trust}.  However, we retained the term \textbf{affective trust}, as explained below.  

\subsubsection{Affective Trust}
\textbf{Affective trust} is trusting that the trustee will support one's goals or actions to achieve those goals.  More generally, it may be understood as how cooperative the trustee is expected to be \cite{gillespie2003measuring,McKnight2001,dunning2014trust}.  At its most narrow, this factor has been termed \emph{Calculative-Based Beliefs} \cite{Gefen2003} and at its broadest and most anthropomorphized, \emph{benevolence} \cite{Mayer1995,Wang2005,Chien2014,Benbasat2005}.   Between the two we find \emph{Purpose} \cite{Chien2014,Sollner2012,Park2020}, \emph{Intention of Developers} \cite{Korber2018}, \emph{Safety} \cite{IdramaniL.SinghRobertMalloy1993}, \emph{Goal} \cite{Hoffman2013},  and \textit{Helpfulness} \cite{McKnightD2011,Sollner2013,Wechsung2013,tussyadiah2018}.  Despite the popularity of Mayer's trust definition \cite{Mayer1995}, we caution against framing this factor as benevolence, which may imply that the robot or machine is kind, caring, and capable of goodwill.

Concepts such as honesty and loyalty may sometimes be construed as either \textbf{Affective} or \textbf{Structural}, or a mix of both.  Their categorization is affected by perceived motivation, such as whether the robot is enabling one's goals because it is aware of them specifically or because it is conforming to societal standards.  Depending on the questions asked, these two constructs may overlap so heavily as to be indistinguishable and be captured under the catch-all term \emph{relation-based trust} \cite{law2021trust}.  Conversely, their opposites may be packed together into an amorphous distrust category that includes deception, trickiness, and underhandedness, like the distrust items in \citeA{jian}.  In general, \emph{distrust} is often meant as the antonym of \textbf{Affective Trust}, which may lead to confusion as to whether it is the opposite of `trust' or whether it is a distinct concept that must be measured independently \cite{Lewicki1998}.

Another limitation of our goal-supportive definition is that it can be at odds with a more dyadic, \textbf{affective trust} that the trustee wants what is best for the trustor.  This can lead to a paternalistic tension between the trustor's self-posited goals and intended means and the trustee's belief in what the trustor should want or how they should act.  As we are primarily interested in goal-focused trust, for now, we simply acknowledge that this can lead to different conceptions of affective trust, especially across cultures where paternalism may be received very differently \cite{Shi2020,Pellegrini2008,Kocak2021,clarke2013trust}.

\subsubsection{General Trust} This category probes trust or trustworthiness directly, though questions can range from focusing on a particular function to the whole system.  Questions here also may ask about trust in those that designed, built, or sold the system.   Other monikers for this category include \emph{Trust in (Service) Robots} \cite{Park2020,Hoffman2013}, \emph{Trust in Automation} \cite{Korber2018}, \emph{Trusting Beliefs in Specific Technologies} \cite{McKnightD2011}, \emph{Perceived Machine Accuracy} \cite{Merritt2011}, \emph{Social Service Robot Interaction Trust} \cite{Chi2021}, and simply \emph{Trust} \cite{Sollner2012,Hegner2019,Freude2019}.

\subsubsection{Intent to Use}
Once we get beyond evaluating all our other internal beliefs, perceived risk, and other environmental and task-related factors, we must finally decide whether to trust.  It is usually called some variation of \emph{Intention} \cite{McKnightD2011,Heerink2009,Benbasat2005,Hegner2019,Park2020,Freude2019}, but also sometimes \emph{Reliance}\cite{Merritt2011} or \emph{Confidence} \cite{Chien2014,IdramaniL.SinghRobertMalloy1993}, which some may find particularly confusing. 

\section{Final Selection of Studies to Analyze}
It is simply beyond the scope of this dissertation to thoroughly analyze for validity, reliability, and terminology used in all 62 instruments.  Instead, a down-selection was performed, it quickly became clear that despite the wide number of instruments only around a dozen were cited frequently or re-utilized among the validated reports, as discussed above in Section \ref{sec:quality}.

As many papers cite from these works, totals in Table \ref{tab:overall_stats} should be taken as estimates of upper bounds on the number of relevant papers.  Here the citations among Method sections, according to Semantic Scholar, best reflect unique experiments, so we can likely put the upper bound on experiments that cite these Top 12 at $ \sim1500 $.  Assuming our sampling method was appropriate, this represents approximately half of the total experiments, so there are likely 3000 relevant experiments, of which our literature review analyzed 173 (5.7\%).  At this sample size, we can have 95\% confidence that the Top 12 really do represent approximately $50\pm7.2\%$ of all survey instruments used in experimental research.  

%Coupling that with my understanding of the field, I believe that these 12 represent the range of instruments that are most important out of the 62 based on the objective data available to me.

While there are other contenders for top spots as far as direct experimental usage, these 12 were identified and chosen to analyze herein after assessing their overall impact (as partially illustrated in Table \ref{tab:overall_stats}).  This list is by no means exhaustive, and the choice of 12 was made primarily to scope our study.  However, we believe they represent a substantial sample of the current state-of-the-art, a good range of validated factors, and provide an entry point into assessing how instruments are tested for reliability and validity.

With these definitions of trust and its antecedents in mind, we can now turn to the Top 12 trust instruments and assess their reliability and validity as well as how well they cover these major {\fontfamily{qzc}\selectfont internal} categories that relate to the psychology of trust and its antecedents.  Note that these categories did not include {\fontfamily{qzc}\selectfont external} environmental, or robot-based factors, but were limited to what the human perceives, understands, and expects of the robot.