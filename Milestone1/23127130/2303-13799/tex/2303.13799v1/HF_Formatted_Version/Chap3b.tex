\section{Analysis of the Trust Instruments}\label{sec:goodness}
\subsection{Down Selection to a Representative Dozen}

% \begin{table*}
%     \centering
%         \caption{Citation Statistics of the 12 Most Cited, Used, or Impactful Survey Instruments for Capturing Trust across HRI, HAI, HMI, HCI, e-commerce, Technological Acceptance, and Fluency by year, including for citations (cit.) in experimental work (exp.).}
%     \resizebox{\columnwidth}{!}{%
%     \begin{tabular}{l|c|c|c|c|c|c|c|c||c}
%         &&\multicolumn{4}{c|}{\underline{Google Scholar}}&\multicolumn{3}{c||}{\underline{Semantic Scholar}}& \\
%     Survey & Year & \begin{tabular}{@{}c@{}}General\\ Citations\end{tabular} & \begin{tabular}{@{}c@{}}Exp.\\ Citations\end{tabular} & \begin{tabular}{@{}c@{}}Gen.\\ Cit./year\end{tabular} & \begin{tabular}{@{}c@{}}Experimental\\ Cit./Year\end{tabular} & \begin{tabular}{@{}c@{}}Background\\ Section\end{tabular} & \begin{tabular}{@{}c@{}}Method\\ Section\end{tabular} & Highly Influential & \begin{tabular}{@{}c@{}}Frequency in \\ Lit. Reviews\end{tabular}\\\hline
%       Muir  & 1996 & \cellcolor{red!27}960 & \cellcolor{red!70}351 & \cellcolor{red!7}36 & \cellcolor{red!52}13 & \cellcolor{red!24}613 & \cellcolor{red!20}92 & \cellcolor{red!16}98 & \cellcolor{red!37}8.0\% \\ \hline
%       Jian et al. & 2000 & \cellcolor{red!30}1027 & \cellcolor{red!100}473 & \cellcolor{red!10}49 & \cellcolor{red!88}22 & \cellcolor{red!3}78 & \cellcolor{red!20}93 & \cellcolor{red!4}22 & \cellcolor{red!100}21.7\%\\ \hline
%       \begin{tabular}{@{}l@{}}Madsen \&\\ Gregor (HCT)\end{tabular} & 2000 & \cellcolor{red!10}293 & \cellcolor{red!26}131 & \cellcolor{red!3}14 & \cellcolor{red!24}6 & \cellcolor{red!3}79 & \cellcolor{red!10}48 & \cellcolor{red!5}34 & \cellcolor{red!9}4.0\% \\ \hline
%       Gefen (TAM) & 2003 & \cellcolor{red!100}8832 & \cellcolor{red!23}118 & \cellcolor{red!100}491 & \cellcolor{red!24}6 & \cellcolor{red!100}2353 & \cellcolor{red!100}549 & \cellcolor{red!100}615 & \cellcolor{red!20}4.4\% \\ \hline
%       SHAPE (SATI) & 2003 & \cellcolor{red!3}60 & \cellcolor{red!5}24 & \cellcolor{red!1}3 & \cellcolor{red!4}1 & \cellcolor{red!0}6 & \cellcolor{red!1}6 & \cellcolor{red!0}4 & \cellcolor{red!0}0.6\%\\ \hline
%       Heerink/UTUAT & 2010 & \cellcolor{red!20}638 & \cellcolor{red!46}231 & \cellcolor{red!11}58 & \cellcolor{red!84}21 & \cellcolor{red!7}174 & \cellcolor{red!25}119 & \cellcolor{red!9}56& \cellcolor{red!13}2.9\%\\ \hline
%       \begin{tabular}{@{}l@{}}Merritt \\ (Propensity to Trust)\end{tabular} & \begin{tabular}{@{}c@{}}2008,\\2011\end{tabular} & \cellcolor{red!15}510 & \cellcolor{red!48}239 & \cellcolor{red!9}44 & \cellcolor{red!81}21 & \cellcolor{red!8}204 & \cellcolor{red!10}45 & \cellcolor{red!7}46 & \cellcolor{red!13}2.9\% \\ \hline
%       McKnight & 2011 & \cellcolor{red!55}5725 & \cellcolor{red!20}100 & \cellcolor{red!60}301 & \cellcolor{red!20}5 & \cellcolor{red!62}1560 & \cellcolor{red!90}428 & \cellcolor{red!78}478 & \cellcolor{red!24}5.1\% \\ \hline
%       Hoffman (Fluency) & 2013 & \cellcolor{red!8}177 & \cellcolor{red!12}62 & \cellcolor{red!4}22 & \cellcolor{red!32}8 & \cellcolor{red!3}62 & \cellcolor{red!10}45 & \cellcolor{red!2}11 & \cellcolor{red!0}0.6\% \\ \hline
%       Schaefer  & 2013 & \cellcolor{red!6}121 & \cellcolor{red!11}56 & \cellcolor{red!3}15 & \cellcolor{red!28}7 &\cellcolor{red!2}50 & \cellcolor{red!8}36 & \cellcolor{red!3}16 & \cellcolor{red!3}0.6\%\\ \hline
%       Chien (CTI)  & 2014 & \cellcolor{red!2}38 & \cellcolor{red!3}15 & \cellcolor{red!1}5 & \cellcolor{red!8}2 & \cellcolor{red!0}3 & \cellcolor{red!0}0 & \cellcolor{red!0}0 & \cellcolor{red!3}0.6\%\\ \hline
%       K\"{o}rber (German TiA)  & 2018 & \cellcolor{red!7}155 & \cellcolor{red!15}75 & \cellcolor{red!10}52 & \cellcolor{red!100}25 & \cellcolor{red!1}11 & \cellcolor{red!4}17 & \cellcolor{red!0}3& \cellcolor{red!3}0.6\%\\ \hline \hline
%       \textbf{Total}&&\textbf{18536}&\textbf{1875}&\textbf{1090}&\textbf{137}&\textbf{5193}&\textbf{1488}&\textbf{1383}&\textbf{47.9\%}%\hline
%     \end{tabular}
%     }
%     \label{tab:overall_stats}
% \end{table*}
 
\paragraph{Terminology Challenges.}  Upon first inspection, there appears to be almost no overlap with how the various instruments decompose trust, the terms they use for various factors and antecedents, and how they assess trust.  This may contribute to the continued popularity of the uni- and bi-dimensional trust instruments.  Upon closer inspection, a consensus is emerging, however much it is obscured.  The next section in this chapter will work to provide a unifying language and then apply that language to the selection of papers chosen so that a complete understanding and comparison can be made.  


\paragraph{Final Selection of Studies to Analyze}
It is simply beyond the scope of any paper to thoroughly analyze all 62 instruments for validity, reliability, and terminology.  Instead, a downselect was made to select between 10-20 studies to investigate.  %We utilized XXXXX method to choose.  

 As many papers cite from these works, totals in Table \ref{tab:overall_stats} should be taken as estimates of upper bounds on the number of relevant documents.  Here the citations among Method sections according to Semantic Scholar may best reflect unique experiments, so we can likely put the upper bound on experiments that cite these Top 12 at $\sim1500$.  Assuming our sampling method was appropriate, this represents approximately half of the total experiments, so there are likely 3000 relevant experiments, of which our literature review analyzed 173 (5.7\%).  At this sample size, we can have 95\% confidence that the Top 12 really do represent approximately $50\pm7.2\%$ of all survey instruments used in experimental research.  %Coupling that with my understanding of the field, I believe that these 12 represent the range of instruments that are most important out of the 62 based on the objective data available to me.

While there are other contenders for top spots as far as direct experimental usage, these 12 were identified and chosen to analyze herein after assessing their overall impact (as partially illustrated in Table \ref{tab:overall_stats}).  This list is by no means exhaustive, and the choice of 12 was made to scope our study.  Still, we believe they represent a robust sample of the current state-of-the-art, a good range of validated factors and provide an entry point into assessing how instruments are tested for reliability and validity.



\subsection{Instrument: Human-Machine Interaction (HMI), (Muir, 1987, 1994, 1996)}
%description of scale
\paragraph{Description.}
The earliest Human-Machine Interaction (HMI) trust scale, Muir \cite{Muir1987}, is primarily focused on measuring performance-based \textbf{\emph{Learned Trust}}.  It consists of nine questions assessed after an interaction occurs.  Muir proposed three main sub-constructs of trust: \textit{faith, dependability}, and \textit{predictability} but also included overall trust in the system and trust in specific elements of the system. \textit{Reliability over time} was grouped with \textit{predictability}, and \textit{competence} and \textit{responsibility} with \textit{dependability}. \textit{Faith} here meant faith in future robustness and not \textbf{Faith in Technology}.  Ultimately, Muir's short survey is a measure of \textbf{Capability-Based Trust}.  

%mapping
\paragraph{Construct Mapping.}
Being the first published scale for HMI trust, Muir saw widespread use early on.  Muir picked her scale items based on a combination of more general human-human interaction theories \cite{barber1983logic,Rempel1985}, starting with their trust definitions, developing them, and proposing a basic mathematics of trust and trying to substantiate it.

A few key findings concerning the Muir scale are \textit{1)} that it detected that \textbf{Familiarity} and \textbf{Faith in Technology} play more significant roles earlier in an interaction than later \cite{Muir1987,Desai2012}, \textit{2)} that the scale is less sensitive to overall trust changes during an interaction \cite{Desai2012}, \textit{3)} that the factors relating to trust in Muir's scale correlate more with error rate than uncertainty \cite{uggirala2004measurement} (highlighting the scale's focus on  \textbf{Capability-based Trust} vs. \textbf{General} or \textbf{Affective Trust}), and \textit{4)} that their \textit{competence} factor was the only one that correlated with uncertainty.  This single correlation is surprising as one would have surmised both \textit{Faith over Time} and \textit{Predictability} would have been correlated as well, and brings up some fundamental questions as to the scale's content validity \cite{uggirala2004measurement}.  On the other hand, the scale has been shown to correlate strongly with Jian's survey \cite{jian,Desai2012}, though what this may mean is complicated, as discussed in the next section.

Furthermore, deriving meaning from the internal correlations among Muir's items can be messy, as single questions inherently have higher variance and do poorly at examining sub-factor constructs \cite{Korber2018}.  Adding to the variance was the small numbers of participants in her experiments, ranging from 6-12 subjects each \cite{Lee_94,muir1996trust}.  When used to measure between-group trust measures in a driving task, the scale captured trust magnitude and calibration \cite{xiong2012use}. 

%validity
\paragraph{Validity \& Reliability.}
Despite its history of usage and popularity, this scale has rarely been tested quantitatively for reliability or validity.  The closest is a refined six-item version that showed Cronbach's $\alpha=0.87-0.91$ \cite{Merritt2011}.  Overall, Muir's scale is short, making it attractive for quickly capturing trust at various times in experiments that can handle brief interruptions.  Its widespread use and citation can partly be attributed to age.  High repeated usage implies cross-application reliability.  However, its lack of internal reliability and validation testing, as well as additional concerns that have arisen experimentally, lead us not to recommend the scale any longer.  It has been superseded by better instruments, including single-factor scales, with more extensive validation and reliability.  


\subsection{Instrument: Trust in Automated Systems, (Jian et al., 2000)}
%Description
\paragraph{Description.}
\citeA{jian}'s scale of Trust in Automated Systems (TAS or TASS\footnote{Sometimes referred to as Empirically Derived (ED), especially to differentiate it from the Technological Adoptiveness Scale (TAS)\cite{halpert2008technological}}) presently is the most widely used trust scale, and thus is the most important to fully analyze.% and thus requires additional scrutiny.   
TAS aims to capture general trust, as well as to compare human-human and human-automation trust.  The method it employs to understand the concepts underlying trust did not come from a larger model or theory of trust.  Instead, it used linguistic association and distance of words relating to trust.  Ultimately, the 12-item single-factor trust scale that emerged has only seen use in the HAI/HRI/HCI communities rather than in human-human trust applications. 

%Mapping
\paragraph{Construct Mapping.}
TAS consists of 30 words (reduced from an initial set of 176) clustered and mapped into a 12-item scale based on conceptual distances found during the experiment.  Only the most strongly affective words were retained, leading to the dismissal of many more neutral or mixed-effect words related to \textbf{Capability-based Trust} and \textit{certainty}, such as \textit{competence, cooperation, unerring, certainty, definite, absolute}, and \textit{predictability}. Two other words of note that were also dropped were \textit{commit} and \textit{stable}.

Due to its preference for word effect, TAS is almost entirely focused only on \textbf{Affective Trust} and \textbf{Structural Trust} with nods to \textbf{Familiarity} and \textbf{General Trust}.  However, in practice, TAS is often used instead as a measure of \textbf{Capability-based Trust}, and it correlates well with other \textbf{Capability-based Trust} scales such as \cite{Muir1987, Merritt2011}.  This is surprising as the designers of TAS dropped linguistic terms relating to capability, as they tended to have neutral or mixed valences.  Their exclusion of capability, however, is seemingly obscured by scale items such as `reliability,' 'dependability,' `integrity,' and `confidence.'  These terms may have been misconstrued or misunderstood as these words have multiple meanings, complicating their use in trust measurement.

Thus, while TAS can be said to measure trust, it is harder to be sure whether its underlying factors have much construct validity, though they have concept validity (see Section \ref{sec:val}).  This dissembling and disassembling of trust factors has understandably led to a mismatch between the mental models of the scale designers, researchers, and experiment participants, which reflects a fundamental weakness of this scale.

%Validity

\paragraph{Validity \& Reliability.}
Originally, TAS was developed through three rounds, with the cluster analysis being based on a sample of just 30 students, though now it has been used by hundreds if not thousands in subsequent experiments \cite{gutzwiller2019positive}.  TAS generally has been found to have strong reliability ($\alpha>0.85$), even when the number of questions retained changes dramatically \cite{gutzwiller2019positive,spain2008effect,Verberne2015,Beggiato2013}.  Issues have arisen between the negative and positive affect sides of the scale, leading some to posit that it consists of two sub-scales, one for trust and one for distrust \cite{jian,spain2008towards}.  On the other hand, the appearance of two sub-scales may be an artifact as negatively valenced items often cluster in factor analysis \cite{merritt2012two}.  

%There are two more major issues that must be discussed at this point - the first is that the item questions were chosen based on a single underlying item and its associated words may be less than clear.  For instance, do participants (or researchers) in a human-machine trust experiment suppose that when a participant rates whether "The system has integrity" that this is meant to capture honor, per the linguistic association?  Or that answering whether "The system is reliable" also encompasses friendship and love?  

Another significant issue is that the final version of the trust survey is based on the clusters found for their Human-Human Analysis rather than those identified in the General or Human-Machine Analyses.  Yet, it is the Human-Human-based survey that has become standard for assessing HRI and HMI \cite{Niu2018,Desai2012,Weitz2019}.  As can be seen in Fig. \ref{fig:Jian_Groups}, when comparing the result of the Human-Human to the Human-Machine analysis there, the clusters and the items they support, are not consistent.  Among the items of negative valence, two clusters are not inquired of at all in the current version of TAS (\emph{cheating}, \emph{phoniness}).  Furthermore, the shifting of `mistrust' and `distrust' from the \emph{suspicion} to \emph{deception} cluster may reflect their treatment as the inverses of \textbf{Affective Trust} instead of \textbf{General Trust}.  

Much more impactful are the differences in the items with positive valence.  The clusters behind the items' \emph{reliability}, \emph{dependability}, and \emph{trust} simply fall apart, completely losing any categorical identity with the other terms in their clusters.  Note \emph{dependability} is not even a term retained in the linguistic clustering of TAS but was reintroduced as the cluster name assigned to loyalty and fidelity.   Furthermore, in the human-machine clustering \emph{system integrity} loses its association with honor, suggesting that integrity has a different meaning in the machine context, perhaps relating more to soundness than a moral virtue.  Most importantly, at the item level, \emph{loyalty} replaces \emph{reliability}, and \emph{fidelity} becomes its own standalone item.  Finally, we must note the top-most positive valence human-robot cluster in Fig. \ref{fig:Jian_Groups} is very hard to label (here called \emph{confident}).  It seems to be a combination of \textbf{Affective} and \textbf{Structural Trust} - something closer to one's confidence in love-motivated promises or assurances.

\begin{figure*}[]
    \centering
    \includegraphics[trim=1cm 1cm 1cm 1cm,width=\textwidth]{HF_Formatted_Version/figures/Fig3_3_Jian_Clusters_Added.pdf}
    \caption{Re-Mapping Jian's Trust Items from the Human-Human linguistic trust clusters to the Human-Robot clusters per their own paper (Jian, 2000).  Our proposed items that do not appear on the original scale appear in red.  The bold lines indicate where terms mapped to when the trustee was assumed to be a machine instead of a human.}
    \label{fig:Jian_Groups}
    % \Description{The named clusters from Jian's used trust scale, based on their human-human trust findings, are compared against what the clusters and items would have been if the human-robot trust findings had been used.  An additional column maps the clusters to one of the three types of trust: affective, structural, and capability-based.}
\end{figure*}


The authors of \citeA{jian} posit 8 underlying factors of human-machine trust.  However, they admit to having difficulty distinguishing more than the first three \cite[p.21-23]{jian}: \textit{familiarity, reliability}, and \textit{confidence}.  Naming aside, these 8 factors explain 79\% of the variance in the survey.  The authors also note in the factor analysis that many positive and negative items load on the same factors, but not all of them, highlighting a partial divergence between trust and distrust constructs.  While cluster analysis initially revealed the two primary sub-scales, one of the positive and the other of the negative items, it also showed some finer resolution groupings depending on the correlational cut-off adopted.  Later studies in the field found that the 12-item survey supported the positive and negative sub-scales \cite{Hoffman2013,dolgov2017trust,holthausen2020situational}.  The finding that items group by effect is not surprising but does not speak much to the actual underlying constructs behind trust.

Overall, while TAS is currently the most popular scale, is easy to administer, and has strong internal and test-retest reliability, it could be improved by better construct validity, less terminological confusion, and scale items based on appropriate clusters.

\subsection{Instrument: Human-Computer Trust survey  (HCT or HCTS), (Madsen, 2000)}
%description
\paragraph{Description.}
The Human-Computer Trust survey  (HCT or HCTS \cite{Madsen2000}) was the first validated technological trust survey widely utilized.  The HCT has five factors divided into two larger categories: \textit{affect-based trust}, composed of faith and personal attachment, and \textit{cognitive-based trust}, covering perceived understandability, technical competence, and reliability.  An essential contribution of HCT to trust theory was its surprising finding that \textit{affective-based trust} was stronger than \textit{cognitive-based trust}, though they did not quantify this result in their publication.  Even so, this finding challenged the traditional HRI trust research up to that time, which had almost exclusively focused on \textit{cognitive-based trust}.

%mapping
\paragraph{Construct Mapping.}
We find several direct mappings comparing HCT's factors with the reference set.  Their \textit{personal attachment} maps nicely to \textbf{Emotional Response}, their \textit{understandability} to  \textbf{Shared Mental Model}, and their \textit{faith} to  \textbf{General Trust} (note a decidedly different usage of the term `faith' than in the reference factors).    

%Validity
\paragraph{Validity \& Reliability.}
HCT, while highly cited, has only been tested for internal reliability, but not for less validity.  Madsen and Gregor \cite{Madsen2000} had proposed 5 factors, but factor analysis yielded 5 others that did not map cleanly to those initially proposed.  Even among those identified, the two weakest constructs, surprisingly, were their \textit{perceived reliability} and \textit{technical competence}.  These categories resulted in just two unique questions each, which is not enough to establish either as a validated factor.  One likely cause of their weakness was their sharing of quite a bit of language and sentence structure, which is known to cause a bias toward clustering together during factor analysis \cite{merritt2012two}. 

Beyond this weakness, the authors acknowledge other limitations, such as the low sample size of just 75, which was approximately a 3:1 ratio of subjects to survey items, and that only a single round of testing was performed.  Considering these issues, more validation was called for \cite{adams2003trust}.  A re-mapping to just the three stronger, validated factors with 88 participants in a different experiment was shown to have reliability, but validity analysis was still lacking \cite{Chancey2016TheEO}. 

Recently the HCT was re-tested for validity, as well as against Jian's TAS Scale \cite{dolgov2017trust}.  They confirmed that the HCT had high internal reliability, both overall and per factor ($\alpha>80\%$), and found high explained variance ($<70\%$).  However, the loadings indicated only four factors (not 5).  Unfortunately, these factors displayed low construct validity.  While this recent researcher did not do a full CFA, their work quantitatively confirmed some of HCT's main weaknesses. %On the other hand, the HCT was shown to have a high correlation with TAS \cite{dolgov2017trust}.  Still, this correlation was complicated when looking at individual factors, where the positive valence side of TAS was better correlated with the HCT than its negative valence side, as we might expect given the range of different types of trust on which each side focuses.  Remember that the positive valence items of TAS capture six out of the ten factors of internal trust, while the negative valence side only captures two to three factors (per Fig. \ref{fig:Jian_Groups}).


\subsection{Instrument: Gefen Instrument (Gefen, 2003)}
%Description
\paragraph{Description.}
The Gefen instrument \cite{Gefen2003} was developed by having experienced online shoppers perform a retrospective on the last e-vendor they encountered.  The authors settled on eight factors revised according to best CFA practices.  Their resulting instrument was responsible for introducing and popularizing many factor categories.  Being the first to combine the Technology Acceptance Model (TAM) with McKnight and Chervany's model of trust \cite{McKnight2001}, they can be credited for introducing  \textbf{Intended Use}, \textbf{Structural Assurance}, and \textbf{Situational Normality} to HAI/HRI.   

%Mapping
\paragraph{Construct Mapping.}
As this instrument was developed nearly independently from the HAI/HRI tradition, its terminology differs from previous works in the field.  A few critical nuances are worth special attention.  The closest Gefen comes to \textbf{Affective Trust} is what they termed \textit{Calculative-base Beliefs}.  However, these beliefs are best understood as the most cynical construct of \textbf{Affective Trust}, measuring a bare minimum of non-exploitation and non-malevolence, as opposed to active cooperation.  Gefen also introduced the concept of \textit{Perceived Ease of Use} and \textit{Perceived Usefulness} into the HAI/HRI trust measurement literature (e.g., \cite{Wang2005,Hegner2019}). \textit{Perceived Ease of Use} maps within the \textbf{Shared Mental Model} factor, as it is the main hypothesized result of having a \textbf{Shared Mental Model}, and is taken to indicate the strength of the human's confidence in the Shared Mental Model.  \textit{Perceived Usefulness} is more complicated; some have construed it to mean how useful something has been (\textbf{\emph{Learned Trust}}), whereas others have taken it to be anticipatory (\textbf{\emph{Situational Trust}}).  Depending on the instrument design, \textit{Perceived Usefulness} may alternatively be classified under \textbf{Faith in (Specific) Technology} or \textbf{Capability-based Trust} -- the latter being the case for the Gefen instrument.

%Validity
\paragraph{Validity \& Reliability.}
This instrument achieved good factor reliability ($\alpha > 0.77$), and its creators not only performed tests of convergent and discriminate validity but also did a full structural equation modeling (SEM) to test for the plausibility of directional causation between sub-constructs.   The survey was first pre-tested with 50 participants, and the main validation was on a sample of 213 participants.  Interestingly, the survey was given as a retrospective and not focused on a specific technology.  Each participant answered regarding the last e-vendor with which they had interacted.

The instrument's main weakness is that clustered items exhibit highly overlapped phrasing within each factor, which may have biased their likelihood of clustering, internal reliability, and validity scores.  Additionally, after dropping items to achieve sufficient reliability, both the \textit{Familiarity} and \textit{Situational Normality} factors were left with only two remaining items, insufficient for establishing stable factor validity.  A number of the remaining factors displayed explained variances ranging from 0.44-0.62, where the suggested cut-off is generally $>0.6$, implying significant weaknesses in \textit{Perceived Usefulness} and \textit{Perceived Ease of Use}, as well as borderline acceptability for \textit{Trust and Intended Use}.  The authors note that a large proportion of their fit statistics were marginally below accepted standards, too.

Wang and Benbaset \cite{Wang2005} replicated a large part of Gefen's survey but for new users, as opposed to experienced ones, confirming good reliability of the factors but much weaker and lower explained variances and construct validity (as indicated by significant, high cross-loadings).  They also identified a different SEM structure, which provided a better fit.  Their structure, unlike Gefen's, implied that the various trust factors (\textbf{Affective}, \textbf{General}, \textbf{Capability-based}, etc.) are dependent upon the \textbf{Shared Mental Model}.  Differences in trust constructs between new and experienced users are expected, as trust takes time to initiate, develop, and calibrate \cite{Sollner2013}.  Still, it was surprising that not just the weights between factors but their actual inferred direction of influence changed between these proposed models.

A fuller critique of Gefen and TAM, on which it is based, can be found in the Discussion (Section \ref{Limits}).


 \subsection{Instrument: SHAPE ATM Trust Index (SATI; Goillou et al., 2003)}
  %Description
  \paragraph{Description.}
SATI \cite{SATI} was developed specifically for assessing trust in air-traffic control systems and was founded on three pillars: \textit{trust}, \textit{situation awareness}, and \textit{teamwork}.  The trust aspect was heavily influenced by Madsen \& Gregor's HCT survey \cite{Madsen2000} and SATI, in turn, has influenced many additional surveys \cite{Chien2014,li2010cross,cummings2010supporting,Yagoda}. 

%Some may question the place of SATI among the Top 12, as it has the weakest showing across all metrics (see Table \ref{tab:overall_stats}).  Its inclusion is partly because it has been picked up by influential groups who have then used it to write their surveys \cite{Chien2014,li2010cross,cummings2010supporting,Yagoda}, giving it an out-sized impact despite its relatively few citations. %YSR why did KF cut this line?

%Mapping
\paragraph{Construct Mapping.}
SATI is one of the most challenging scales to classify using our approach.  The main trust questionnaire contains two parts - the first is an expansion of Lee and Moray (\citeyear{Lee_94}) focusing on \textbf{General Trust} and asking about trust in each part of the system and team: self-confidence, trust in a specific technology, overall trust in the technology, as well as trust in colleagues, designers, and operators.  The second part consists of questions rating or ranking trust along 7 sub-scales and antecedents.  They do include \textit{Liking} (e.g., \textbf{Emotional Response}), \textbf{Familiarity}, and \textit{Understanding} (e.g., \textbf{Shared Mental Model}); with the rest of the items falling under \textbf{Capability-based Trust}, such as \textit{Reliability} and \textit{Accuracy}. A third instrument for assessing \textbf{Faith in Technology} is also proposed.

%Validity
\paragraph{Validity \& Reliability.}
While SATI has no reported reliability or empirical validation, it does focus quite a bit on two other aspects of construct validity: face and item validity.  The authors formulated a list of 16 validation and success criteria, of which five were met, two were still being confirmed, and a further eight were being actively worked on.  As far as we could discern, a final report on achieving these criteria was never published.

Beyond validation and reliability, SATI  impacted the trust field in several important ways.  First, it highlighted the importance of ease of use for the instrument itself.  Second, it was an early proponent of the idea that not all sub-scales varied over time, preceding Hoff and Bashir's three-layered trust \cite{Hoff2015}.  Third, it revealed that system operators viewed trust as a binary decision and preferred to rate that decision independently of trust beliefs.  Furthermore, they found that confidence had to meet a certain threshold for the binary decision to trust to be made but that this threshold could be articulated by most.  Fourth, SATI unveiled that the type of failure mattered more than frequency.  Failures leading to accidents were perceived as far worse than false alarms, and a single accident was worse for trust than frequent false alarms \cite{SATI}.  Finally, it qualitatively demonstrated that monitoring and trust are not inverses, aligning it with other critiques of this assertion \cite{Razin2021b,castelfranchi2010trust} in Mayer's classic formulation \cite{Mayer1995}. %, which had insisted that lack of monitoring is fundamental to the definition of trust.


 \subsection{Instrument:  Unified Theory of Acceptance and Use of Technology (UTUAT; Heerink, 2009)}
 %Description
 \paragraph{Description.}
Heerink's Trust extension \cite{Heerink2009,Heerink2011} of the Unified Theory of Acceptance and Use of Technology (UTUAT) is similar to Gefen's extension of TAM\cite{Gefen2003}\footnote{UTUAT is itself built off TAM but started incorporating environmental, attitudinal, and \textbf{Emotional Response} factors much earlier}.  Heerink's survey posits 12 factors, with the primary goal of measuring the acceptance and use of a specific technology.  

%Mapping
\paragraph{Construct Mapping.}
UTUAT includes a factor called \textit{Social Presence} which aims to capture comfort/likeability, though its items are more along the line of \textbf{Situational Normality} than \textbf{Emotional Response}.  Its \textit{Facilitating Conditions}  maps directly to (perceived) \textbf{Familiarity}.  This was the rare survey that asked about \textit{Social Influence}, a factor identified with Reputation/Recommendation that in human-human trust is considered a sub-factor of familiarity \cite{yamagishi2001}.  We have classified \textit{Attitude} under \textbf{Capability-based} since it is very close to \textit{Perceived Usefulness}, but may also map to  \textbf{Faith in Technology}.  Finally, \textit{Perceived Adaptability} is clearly defined as goal-supportive helpfulness, and thus a form of \textbf{Affective Trust}.

%Validity
\paragraph{Validity \& Reliability.}
UTUAT has undergone extensive reliability and validation analysis, although there are some remaining causes for concern, which will be discussed further in Section \ref{Limits}.  While very popular (see Table \ref{tab:overall_stats}), there are also concerns with the validity of Heerink's Trust extension of UTUAT.  Heerink's survey was only tested on two small populations (30 and 66), and only internal reliability data was reported.  The cut-off for Cronbach's $\alpha$ was kept low, at 0.7.  As no validation data is reported, it is unknown how distinct or meaningful many of the factors are.  Two of the factors, including \textit{trust} itself, are only composed of two questions.  A particular area of concern is that the methods used (correlational and regression analyses) to establish the structure between these factors created results that are not in line with any of the other regression or SEM results from other surveys (compare against \cite{McKnightD2011,Gefen2003}.  In particular,  the direction of plausible causality seemed reversed between General Trust and other factors, suggesting its results should be used with caution.


% % Moreover, many factors only proved correlational significantly but not part of the causal structure (Social Influence, Facilitating Conditions, and Anxiety).  Considering UTUAT specifically is preferred to TAM for its social-environmental extensions, this implies that trust and even use are not directly influenced by the environment, which seems unlikely.  

\subsection{Instrument: McKnight Trust Survey (McKnight, 2001)}
%Description
\paragraph{Description.}
A decade after their seminal multi-disciplinary review of the trust literature \cite{McKnight2001}, \citeA{McKnightD2011} created a trust survey instrument for specific technologies .  Through pre-tested card sorting and broad definitional reviews, they identified 12 factors (7 being trust-related), grouped into three main second-order conceptual categories. 

%Mapping
\paragraph{Construct Mapping.}
This survey covered every factor in \textbf{\textit{Learned Trust}}, including a whole range of sub-factor distinctions, 
e.g., \textbf{Faith in (General) Technology} which is about positive expectations of technology in general, and the \textit{Trusting Stance} which looks at lack of suspicion.  Other interesting sub-factor distinctions were \textit{Functionality} vs. \textit{Reliability} within \textbf{Capability-based Trust} and \textit{Intention to Explore} and \textit{Deep Structure Usage} within \textbf{Intention to Use}.  McKnight also proposed a second-order structure with three levels \textit{Propensity to Trust} (\textbf{\textit{Dispositional}}), \textit{Institution-Based Trust}, and  \textit{Trusting Beliefs in a Specific Technology} (\textbf{\textit{Learned}}).  %\textit{Institution-Based Trust} spans both Situation Normality (situational) and Structural Trust (learned) and further clarification is needed, which is where a meta-review such as this work hoped to start to address.   %The former is about the assessment of use vs. confidence in that assessment.  The latter distinguishes between intending to use for known tasks and more open-ended play/exploration to engage and learn more. 

%Validity
\paragraph{Validity \& Reliability.}
McKnight's reliability and validation analyses are some of the most thorough and yield some of the most robust results.  The survey was tested on a single large sample of 359 participants.  Internal reliabilities were all above 0.8, and the average explained variance ranged from 0.56-0.81, with the majority above 0.7.  The factor loadings per item revealed strong construct validity, with high primary loadings and no significant cross-loadings.  All the CFA and SEM statistics indicated a good fit.  They also found moderate to strong predictive validity for usage based on trust.  Further support comes from \citeA{Tussyadiah2020}, which used and re-validated McKnight's survey, finding almost identical correlations, regressions, and second-order structure.  They added the use of NARS \cite{nomura2008prediction} to the \textbf{\textit{Dispositional}} layer, showing it had a significant adverse effect on trust, as might be expected.  


\subsection{Instrument: Merritt's Collective Set of Trust Scales (Merrit, 2008, 2011, 2015, 2019)}
%Description
\paragraph{Description.}
Merritt has been refining and creating multiple scales for measuring trust and its correlates over many years \cite{Merritt2008,Merritt2011,merritt2015measuring,merritt2019automation}.  Her group has produced six separate stand-alone scales such as \textit{Trust} (\textbf{Capability-based Trust}) \cite{Merritt2008}, \textit{Liking} (\textbf{Emotional Response}) \cite{Merritt2011}, \textit{Propensity to Trust} (\textbf{Faith in Technology}) \cite{Merritt2011}, the Perfect Automation Schema (\textbf{Dispositional}) \cite{merritt2015measuring},  All-or-None Thinking scale (\textbf{Dispositional}) \cite{merritt2015measuring}, and Automation-Induced Complacency (\textbf{Dispositional}) \cite{merritt2019automation}.  

%Mapping
\paragraph{Construct Mapping.}
Merritt's collective work has produced vital insights with respect to \textbf{Dispositional Trust} and its effect on \textbf{Capability-based Trust}.  High expectations did not significantly impact \textbf{Capability-based Trust} or its failure, but an all-or-nothing attitude severely affects trust repair.  Furthermore, \textbf{Faith in Technology's} ability to alleviate workload has a more substantial effect on \textbf{Capability-based Trust} than the need for monitoring, but both are significant.  Thus, Merritt's work has begun to tease out the sub-scales within \textbf{Faith in Technology} that provide a path forward but remain limited in their focus on \textbf{Capability-based Trust}.

%Validity
\paragraph{Validity \& Reliability.}
 Each scale was developed in multiple rounds of medium to large sample sizes (69-500) and reported reliability and validity criteria as appropriate.  Merrit's six-item scale (Muir's+1 item) had internal reliabilities ranging from $\alpha \backsim0.87-0.92$.  They also performed a CFA and compared multiple SEMs looking at \textit{Propensity to Trust}, initial trust, and post-task trust as well as \textit{liking}, many of which displayed good fit statistics, though other validity criteria were not reported \cite{Merritt2008,Merritt2011}. Their \textit{Liking} and \textit{Propensity to Trust} scales demonstrated internal reliabilities of $\alpha\backsim0.8-0.89$ and $0.86$, respectively, and both fit well by their CFA and SEM.  The Perfect Automation Schema (\textbf{Dispositional}) had a weaker but still acceptable reliability score of $0.76$, and the All-or-None Thinking scale was at the very edge of acceptability ($\alpha=0.68$) \cite{merritt2015measuring}.  The Automation-Induced Complacency scale had two factors (\emph{Alleviating Workload} and \emph{Monitoring}) that were tested for internal reliability, split-half reliability, and with a confirmatory factor analysis. They achieved fairly good values across the board ($\alpha_{AW}=0.87, \alpha_{M}=0.79, \chi^2=84.33, df=34$, RMSEA = 0.08, CFI = 0.92, TLI = 0.89) though concluded that the two factors should be treated as separate scales and not two factors within a single construct. 

 \subsection{Instrument: Subjective Fluency Metric Scale, (Hoffman, 2013)}
 %Description and Mapping
 \paragraph{Description.}
The Subjective Fluency Metric Scale \cite{Hoffman2013} was designed to assess fluency in human-robot teams.  It was primarily based on the popular Working Alliance Inventory  (WAI) \cite{horvath1989development} that measures the patient-therapist `alliance' and is thus a human-human teamwork/support scale.  


\paragraph{Construct Mapping.}  The WAI is comprised of a \textit{bond} subscale (\textbf{Shared Mental Model}) and a \textit{goal} sub-scale (\textbf{Affective Trust}). To the WAI, \citeA{Hoffman2013} added what they called factors of \textit{human-robot fluency} (\textbf{Capability-based Trust}), \textit{robot relative contribution}, \textit{trust} (\textbf{General Trust}), \textit{positive teammate traits} (\textbf{Emotional Response}), \textit{improvement} (\textbf{Capability-based Trust}), and some individual standalone items regarding commitment and cooperation (\textbf{Affective Trust}) \cite{Hoffman2013}.  

%Validity
\paragraph{Validity \& Reliability}
Despite multiple uses and adaptations in the human-robot fluency community \cite{Hoffman2013}, validation work has yet to be reported, though strong internal reliability scores have been demonstrated \cite{Hoffman2013,Dragan2015}.  This scale is promising because it converges to our identified factors while arising from a sub-field that developed separately from the rest of the HAI trust literature and is heavily influenced by a completely different side of human-human trust. 

 \subsection{Instrument: Trust Perception Scale for HRI, (Schaefer 2013,2016)}
 %Description
 \paragraph{Description.}
Leading one of the major groups working on conceptualizing human-robot trust and human-automation interaction for years, Parasuraman's and Hancock's groups developed and explored numerous ways to parse and categorize previous trust research, after numerous meta-reviews and analyses \cite{Parasuraman2000,Sheridan2005,Parasuraman2008,Hancock2011a}.  Their efforts, as far as trust measurement, were brought to fruition by Schaefer, who used their framework to develop her own trust model and survey instrument, the Trust Perception Scale for HRI (TPS-HRI)\cite{schaefer2013perception}, with further influence by Rotter \cite{Rotter_67}, Muir \cite{Muir1987}, TAS \cite{jian}, and SATI \cite{SATI}.  Schaefer cast a wide net as well, extensively citing not only the HRI literature but also the automation trust literature, providing one of the most valuable appendices of summaries for dozens of trust instruments.

%Mapping
\paragraph{Construct Mapping.}
The TPS-HRI found four factors, which were identified as \textit{performance-based functionality}, \textit{robot behaviors/communication}, \textit{task/mission-specific}, and \textit{robot features}. On careful review of each factor, we map \textit{performance-based functionality} to \textbf{Capability-based Trust} and \textit{robot behaviors/communication} as a combination of \textbf{Emotional Response}, \textbf{Shared Mental Model}, and \textbf{Structural Trust}.  We also suggest that what she termed \textit{task/mission-specific} should be understood as a failure of \textbf{Structural Trust}, whereas \textit{robot features} should be understood as dangers arising from failures of robustness \textbf{Capability-based Trust}.  It is evident, though, that these mappings are some of the least clear to our proposed factors.  However, as our factors generally match the other reliable and validated surveys, it is essential to clarify this mapping to understand how Schaefer's instrument and the oft-cited body of trust research behind it (\cite{Sheridan2005,Parasuraman2008,Hancock2011a}) fit into the rest of the field.

%Validity
\paragraph{Validity \& Reliability.}
Schaefer produced a well-validated, multi-round study that checked and refined scales and tested for sub-constructs of trust.  She tested 630 participants in six rounds, though the scale itself was only validated with the final 102 participants in two rounds, reducing the set of 172 items to both 40-item and 14-item instruments. 

Regarding validation, she began by asking subject matter experts and then used the content validity ratio to identify item importance.  From here, she created both a 40-item and a 14-item scale and compared them against TAS \cite{jian}.  In her factor analysis, she used a Kaiser Criterion of eigenvalues $>1$ for truncation, an orthogonal varimax rotation, and a loading cut-off of 0.3.  The varimax rotation may have hurt her validation, forcing orthogonality on trust factors that are often understood to be heavily correlated.  We can also see this drop in her explained variance, which was 79.63\% overall but fell to 30.64\% after rotation.  The cut-off chosen for cross-loadings under 0.3 should be separately considered from the cut-off for primary factor loadings, which is often said to need to be above 0.7 \cite{McKnightD2011}.  Not a single item had a factor loading over 0.7; only 3 out of the 172 reached over 0.6.  Factor reliability and item communalities were not reported, and given their orthogonal rotation, there are no correlations between factors.  However, there were some cross-loadings, especially between items on the \textit{performance-base} and \textit{coordination/communication-based} factor.  

%None of the loadings on the fourth factor were over 0.5.  
Schaefer's work was a significant effort that brought together a vast number of previous surveys into focus that had not received due attention. The large sample sizes and multi-round testing have lent this survey an air of validity that has led to significantly increasing use in the field.  Thus, it is critical to note that challenges remain with its reliability and validity to assess the factors that makeup trust.


 \subsection{Instrument: Cultural Trust Instrument (CTI), (Chien, 2014)}
 \paragraph{Description.}
The Cultural Trust Instrument (CTI) \cite{Chien2014} was primarily based on TAS \cite{jian}, HCT \cite{Madsen2000}, and SATI \cite{SATI}, with some influence from \citeA{McKnightD2011} among others that were more focused on \textbf{Faith in Technology}. 

\paragraph{Construct Mapping.}
The instrument was refined and tested in two rounds, the first of 45 participants and the second of 65.  The first reduced a set of 110 potential items to 59, and then a further 21 items.  Nineteen of these items mapped to four factors, three of which they termed \textit{performance expectancy} (\textbf{Capability-based Trust}), \textit{process transparency} (\textbf{Shared Mental Models}), and \textit{purpose influence} (\textbf{Faith in Technology}), keeping them closely aligned with the framework in Lee and Moray \cite{Lee2004}.  The fourth factor is left \textit{unnamed} and is solely composed of three negative valence items from TAS \cite{jian} that span \textbf{Affective Trust}, \textbf{Structural Trust}, and perhaps \textbf{General Trust}.  It is unclear whether the negative scale items are truly separate sub-constructs or cluster together solely because they are inversely scored \cite{merritt2012two}.

\paragraph{Validity \& Reliability.}
The EFA showed that these items fell into five internally reliable factors ($\alpha>0.7$), four of which showed good construct validity based on factor loadings from the EFA.  The final reliable but non-validated factor seems to be related to  `unreliability' though its correlation with the \textbf{Capability-based} factor is left unreported.  Their models also captured 50\% of the variance of general trust and 70\% of specific trust.  While less used overall, even as recently as this year, the CTI continues to be used in place of TAS, HCT, and SATI, with researchers citing its higher quality "rigor and empirical validation" \cite{Crawford2021}.  While the CTI is indeed somewhat better validated, this perhaps still does not warrant its use without further validation.

 \subsection{Instrument: Trust in Automation (German TiA) Scale (K\"{o}rber, 2018)}
 \paragraph{Description.}
The final scale worth consideration is the Trust in Automation (German TiA) scale  \cite{Korber2018}, whose popularity is impressive given its relatively recent date of publication.  

\paragraph{Construct Mapping.}
The German TiA followed \cite{Mayer1995,Lee2004} using three primary dimensions of trust, which they call \textit{Understandability/Predictability} (\textbf{Shared Mental Model}), \textit{Reliability/Competence} (\textbf{Capability-based Trust}), and \textit{Intention of Developers} \textbf{(Affective Trust}). They then try to map those three dimensions to Trust in Automation (\textbf{General Trust}).  They also test adding factors for \textbf{Familiarity} and \textit{Propensity to Trust} (\textbf{Faith in Technology}).

\paragraph{Validity \& Reliability}
They iterated their survey over two rounds, totaling 152 participants.  They found that despite the five factors proposed, only four could be validated with an EFA, which combined most of the items in \textit{Understanding/Predictability} with \textit{Intention of Developers}.  This scale had fairly complete validation: an excellent reliability analysis, and a well-done factor analysis, with an eye toward construct, surface, empirical, and predictive validation.  Unfortunately, \textbf{Familiarity} and \textit{Intention of Developers} were initially only composed of two items each, though after analysis \textit{Intention of Developers} was shown to factor with \textit{Understanding/Predictability}, resulted in only \textbf{Familiarity} being left with two items.  The fourth factor, \textit{Propensity to Trust}, had fairly low factor loadings, with all just below 0.6.


% \begin{table*}[]
%     \centering
%     % 
%         \caption{Summary of quality assessment of the Top 12.  \# of items are those specific to trust and its factors. \# of Factors is the number the authors find or confirm, whether they match ours or not.  Quality of empirical reliability and validity is rated in order of None, Limited, Decent, and Good as defined in the text.}
%     \resizebox{\columnwidth}{!}{%
%     \begin{tabular}{l"c|c|c|c}
%         Instrument & \# of items & \# of Factors  &Reliability&Validity
%         \\\hline
%         HMI Trust Scale \cite{Muir1987} &9&1&\cellcolor{yellow!40}Limited&\cellcolor{red!60}None\\\hline
%         TAS \cite{jian}&12&1&\cellcolor{green!15}Good&\cellcolor{yellow!40}Limited\\\hline
%         HCT \cite{Madsen2000}&25&5&\cellcolor{green!15}Good&\cellcolor{yellow!40}Limited\\\hline
%         Gefen's TAM with Trust \cite{Gefen2003}&25&8&\cellcolor{green!15}Good&\cellcolor{blue!15}Decent\\\hline
%         SATI \cite{SATI}&17&?&\cellcolor{red!60}None&\cellcolor{yellow!40}Limited\\\hline
%         UTUAT with Trust \cite{Heerink2009,Heerink2011}&41&12&\cellcolor{green!15}Good&\cellcolor{yellow!40}Limited\\\hline
%         Trust in Specific Technology  \cite{McKnightD2011}&39&10&\cellcolor{green!15}Good&\cellcolor{green!15}Good\\\hline
%         Merritt's collected Trust-related Scales \cite{Merritt2008,Merritt2011,merritt2015measuring,merritt2019automation}&28&4&\cellcolor{green!15}Good&\cellcolor{blue!15}Decent\\\hline
%         Subjective Fluency Metric Scale \cite{Hoffman2013}&30&7&\cellcolor{green!15}Good&\cellcolor{red!60}None\\\hline
%         TPS-HRI \cite{schaefer2013perception}&40 (14)&4&\cellcolor{green!15}Good&\cellcolor{yellow!40}Limited\\\hline
%         CTI \cite{Chien2014}&19&4&\cellcolor{green!15}Good&\cellcolor{blue!15}Decent\\\hline
%         German TiA \cite{Korber2018}&17 &5&\cellcolor{green!15}Good&\cellcolor{green!15}Good\\%\hline
%         % \textbf{Maximum}&&&1&3&1&4&1&& \thickhline
%     \end{tabular}
%      }%
%     \label{tab:top12sum}
% \end{table*}