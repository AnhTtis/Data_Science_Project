@article{nass1997computers,
	title        = {Computers are social actors: A review of current},
	author       = {Nass, Clifford I and Moon, Youngme and Morkes, John},
	year         = 1997,
	journal      = {Human values and the design of computer technology},
	volume       = 72,
	pages        = 137
}
@article{de2016almost,
	title        = {Almost human: Anthropomorphism increases trust resilience in cognitive agents.},
	author       = {De Visser, Ewart J and Monfort, Samuel S and McKendrick, Ryan and Smith, Melissa AB and McKnight, Patrick E and Krueger, Frank and Parasuraman, Raja},
	year         = 2016,
	journal      = {Journal of Experimental Psychology: Applied},
	publisher    = {American Psychological Association},
	volume       = 22,
	number       = 3,
	pages        = 331
}
@article{ha2005effects,
	title        = {Effects of consumer perceptions of brand experience on the web: Brand familiarity, satisfaction and brand trust},
	author       = {Ha, Hong-Youl and Perks, Helen},
	year         = 2005,
	journal      = {Journal of consumer behaviour: An international research review},
	publisher    = {Wiley Online Library},
	volume       = 4,
	number       = 6,
	pages        = {438--452}
}
@article{Carp2004,
	title        = {{Field experiments}},
	author       = {Carpenter, Jeffrey and Gerking, Shelby and Mark, R and Krueger, Alan and Mcmillan, John and Ortmann, Andreas and Plott, Charles and Reiley, David and Wilcox, Nathaniel and Roth, Alvin},
	year         = 2004,
	month        = {December},
	journal      = {{J. of Econ. Lit.}},
	volume       = 42,
	number       = 4,
	pages        = {1009--1055},
	mendeley-groups = {Trust{\_}Game{\_}Theory}
}
@article{Deutsch1958,
	title        = {{Trust and suspicion}},
	author       = {Deutsch, Morton},
	year         = 1958,
	journal      = {J. of Conflict Resolution},
	volume       = 2,
	number       = 4,
	pages        = {265--279},
	abstract     = {Discussion of such problems as the armaments race, mental illness, the "hidden persuaders", and juvenile delinquency frequently employ such terms as trust, suspicion, betrayal, faith. Trust and its related concepts are vital to the understanding both of social life and of personality development. Trust involves the notion of motivational relevance as well as predictability. To trust implies that the trustor will suffer unpleasant consequences when trust is not fulfilled i.e. that he will be worse off if trusting and it not fulfilled than if he never trusted. Risk-taking and trusting are therefore different sides of the same coin.},
	mendeley-groups = {Grant},
	pmid         = 13022
}
@article{deutsch1960effect,
	title        = {{The effect of motivational orientation upon trust and suspicion}},
	author       = {Deutsch, Morton},
	year         = 1960,
	journal      = {Human Relations},
	publisher    = {Sage Publications Sage UK: London, England},
	volume       = 13,
	number       = 2,
	pages        = {123--139},
	mendeley-groups = {NASA}
}
@book{deutsch1977resolution,
	title        = {{The Resolution of Conflict: Constructive and Destructive Processes}},
	author       = {Deutsch, Morton},
	year         = 1977,
	publisher    = {Yale University Press},
	address      = {New Haven, CT},
	mendeley-groups = {NASA}
}
@article{Alexander2018,
	title        = {{Why trust an algorithm? Performance, cognition, and neurophysiology}},
	author       = {Alexander, Veronika and Blinder, Collin and Zak, Paul J.},
	year         = 2018,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier},
	volume       = 89,
	number       = {July},
	pages        = {279--288},
	doi          = {10.1016/j.chb.2018.07.026},
	issn         = {07475632},
	url          = {https://doi.org/10.1016/j.chb.2018.07.026},
	abstract     = {OBJECTIVE: We measured neurophysiologic responses and task performance while participants solved mazes after choosing whether to adopt an imperfect helper algorithm. BACKGROUND: Every day we must decide whether to trust or distrust algorithms. Will an algorithm improve our performance on a task? What if we trust it too much? METHOD: Participants had to pay to use the algorithm and were aware that it offered imperfect help. We varied the information about the algorithm to assess the factors that affected adoption while measuring participants' peripheral neurophysiology. RESULTS: We found that information about previous adoption by others had a larger effect on adoption and resulted in lower cognitive load than did information about algorithm accuracy. The neurophysiologic measurement showed that algorithm adoption without any information resulted in low cognitive engagement during the task and impaired task performance. Conversely, algorithm use after information about others' use improved engagement and performance. CONCLUSION: By objectively measuring cognitive load and task performance, we identified how to increase algorithm adoption while sustaining high performance by human operators. APPLICATION: Algorithm adoption can be increased by sharing previous use information and performance improved by providing a reason to monitor the algorithm. Precis: We collected neurophysiologic data while varying information about an algorithm that assisted participants in solving a timed and incentivized maze and found that information about prior use by others more effectively influenced adoption, reduced cognitive load, and improved performance compared to algorithm accuracy information.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563218303480-main.pdf:pdf},
	keywords     = {Automation,Computers,Decisions,Neurophysiology},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bailey2007,
	title        = {{Automation-induced complacency for monitoring highly reliable systems: The role of task complexity, system experience, and operator trust}},
	author       = {Bailey, N. R. and Scerbo, M. W.},
	year         = 2007,
	journal      = {Theoretical Issues in Ergonomics Science},
	volume       = 8,
	number       = 4,
	pages        = {321--348},
	doi          = {10.1080/14639220500535301},
	issn         = {1464536X},
	abstract     = {The increase in quantity and complexity of advanced automated systems has generated new concerns surrounding automation-induced complacency, or the difficulties operators have monitoring the status of automated systems. The present investigation consists of two studies that assessed the impact of system reliability, monitoring complexity, operator trust, and system experience on automation-induced complacency. In both studies, participants operated a manually controlled flight task while monitoring several simulated aircraft displays for failures. The ability of operators to detect a single automation failure over three experimental sessions was also assessed. Results indicated that realistic levels of system reliability severely impaired an operator's ability to monitor effectively. Further, as system experience increased, operator monitoring performance declined. The results also indicated that the complexity of the monitoring task heavily influenced operator monitoring, with poorer performance associated with more cognitively demanding tasks. Finally, results from both studies indicated that operator trust increased and monitoring performance decreased as a function of increasing system reliability. These results suggest that for highly reliable systems, increasing task complexity and extensive experience may severely impair an operator's ability to monitor for unanticipated system states. {\textcopyright} 2007 Taylor & Francis.},
	file         = {:Users/yosef/Downloads/Automation induced complacency for monitoring highly reliable systems the role of task complexity system experience and operator trust.pdf:pdf},
	keywords     = {Attention,Complacency,Monitoring,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Balfe2018,
	title        = {{Understanding Is Key: An Analysis of Factors Pertaining to Trust in a Real-World Automation System}},
	author       = {Balfe, Nora and Sharples, Sarah and Wilson, John R.},
	year         = 2018,
	journal      = {Human Factors},
	volume       = 60,
	number       = 4,
	pages        = {477--495},
	doi          = {10.1177/0018720818761256},
	issn         = 15478181,
	abstract     = {Objective: This paper aims to explore the role of factors pertaining to trust in real-world automation systems through the application of observational methods in a case study from the railway sector. Background: Trust in automation is widely acknowledged as an important mediator of automation use, but the majority of the research on automation trust is based on laboratory work. In contrast, this work explored trust in a real-world setting. Method: Experienced rail operators in four signaling centers were observed for 90 min, and their activities were coded into five mutually exclusive categories. Their observed activities were analyzed in relation to their reported trust levels, collected via a questionnaire. Results: The results showed clear differences in activity, even when circumstances on the workstations were very similar, and significant differences in some trust dimensions were found between groups exhibiting different levels of intervention and time not involved with signaling. Conclusion: Although the empirical, lab-based studies in the literature have consistently found that reliability and competence of the automation are the most important aspects of trust development, understanding of the automation emerged as the strongest dimension in this study. The implications are that development and maintenance of trust in real-world, safety-critical automation systems may be distinct from artificial laboratory automation. Application: The findings have important implications for emerging automation concepts in diverse industries including highly automated vehicles and Internet of things.},
	file         = {:Users/yosef/Downloads/0018720818761256-2.pdf:pdf},
	keywords     = {ethnographic observations,human-automation interaction,supervisory control,technology acceptance,trust in automation},
	mendeley-groups = {Other_Trust_Metric_Papers},
	pmid         = 29613815
}
@article{Bass2013,
	title        = {{The effect of information analysis automation display content on human judgment performance in noisy environments}},
	author       = {Bass, Ellen J. and Baumgart, Leigh A. and Shepley, Kathryn Klein},
	year         = 2013,
	journal      = {Journal of Cognitive Engineering and Decision Making},
	volume       = 7,
	number       = 1,
	pages        = {49--65},
	doi          = {10.1177/1555343412453461},
	issn         = 21695032,
	abstract     = {Displaying both the strategy that information analysis automation employs to makes its judgments and variability in the task environment may improve human judgment performance, especially in cases where this variability impacts the judgment performance of the information analysis automation. This work investigated the contribution of providing either information analysis automation strategy information, task environment information, or both, on human judgment performance in a domain where noisy sensor data are used by both the human and the information analysis automation to make judgments. In a simplified air traffic conflict prediction experiment, 32 participants made probability of horizontal conflict judgments under different display content conditions. After being exposed to the information analysis automation, judgment achievement significantly improved for all participants as compared to judgments without any of the automation's information. Participants provided with additional display content pertaining to cue variability in the task environment had significantly higher aided judgment achievement compared to those provided with only the automation's judgment of a probability of conflict. When designing information analysis automation for environments where the automation's judgment achievement is impacted by noisy environmental data, it may be beneficial to show additional task environment information to the human judge in order to improve judgment performance. Copyright {\textcopyright} 2012, Human Factors and Ergonomics Society.},
	file         = {:Users/yosef/Downloads/1555343412453461.pdf:pdf},
	keywords     = {Automation display content,Human-automated judge learning,Human-automation interaction,Judgment analysis},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bata2018,
	title        = {{Mobile social networking and salesperson maladaptive dependence behaviors}},
	author       = {Bata, Hatem and Pentina, Iryna and Tarafdar, Monideepa and Pullins, Ellen Bolman},
	year         = 2018,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier Ltd},
	volume       = 81,
	pages        = {235--249},
	doi          = {10.1016/j.chb.2017.12.025},
	issn         = {07475632},
	url          = {https://doi.org/10.1016/j.chb.2017.12.025},
	abstract     = {This study investigates technology dependence associated with the work-related use of mobile social networking (MSN) by salespeople. A scale for maladaptive technology dependence behaviors (MTDB) is developed and empirically validated using survey data from 242 mid-level sales managers in the US. Personal and job-related antecedents, as well as consequences of MTDB for sales outcomes, are also examined. Results suggest that emotional attachment to MSN and perceptions of its greater affordances for task accomplishment may lead to maladaptive behaviors of overreliance on MSN for job completion, blind trust, cognitive absorption and dysfunctional use. These associations increase in organizations with competitive psychological climate. Findings also show that using MSN for prospecting does not lead to maladaptive dependence, as opposed to using it for customer relationship maintenance. Salespeople using MSN for relationship maintenance exhibit more maladaptive behaviors if they experience work-related role stress. Finally, salespeople who exhibit MTDB are less likely to complete their assignments and participate in teamwork. These findings provide tools for organizations to develop technology use policies, design sales training, and enhance the work environment. Future studies can examine dependencies on others types of technologies (CRM, marketing automation, etc.), and in other contexts (online retailing, social media analytics, etc.)},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563217307069-main-2.pdf:pdf},
	keywords     = {Dark side of social networking,Maladaptive technology dependence,Mobile social media,Professional sales,Technology addiction},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Benbasat2005,
	title        = {{Trust In and Adoption of Online Recommendation Agents}},
	author       = {Benbasat, Izak and Wang, Weiquan},
	year         = 2005,
	journal      = {Journal of the Association for Information Systems},
	volume       = 6,
	number       = 3,
	pages        = {72--101},
	doi          = {10.17705/1jais.00065},
	issn         = {1536-9323},
	abstract     = {Online product recommendation agents are becoming increasingly prevalent on a wide range of websites. These agents assist customers in reducing information overload, providing advice to find suitable products, and facilitating online decision-making. Consumer trust in recommendation agents is an integral factor influencing their successful adoption. However, the nature of trust in technological artifacts is still an under-investigated and not well understood topic. Online recommendation agents work on behalf of individual users (principals) by reflecting their specific needs and preferences. Trust issues associated with online recommendation agents are complicated. Users may be concerned about the competence of an agent to satisfy their needs as well as its integrity and benevolence in regard to acting on their behalf rather than on behalf of a web merchant or a manufacture. This study extends the interpersonal trust construct to trust in online recommendation agents and examines the nomological validity of trust in agents by testing an integrated Trust-TAM (Technology Acceptance Model). The results from a laboratory experiment confirm the nomological validity of trust in online recommendation agents. Consumers treat online recommendation agents as " social actors" and perceive human characteristics (e.g., benevolence and integrity) in computerized agents. Furthermore, the results confirm the validity of Trust-TAM to explain online recommendation acceptance and reveal the relative importance of consumers' initial trust vis-¨¤-vis other antecedents addressed by TAM (i.e. perceived usefulness and perceived ease of use). Both the usefulness of the agents as "tools" and consumers' trust in the agents as "virtual assistants" are important in consumers' intentions to adopt online recommendation agents.},
	file         = {:Users/yosef/Downloads/WangBenbasat-TrustTAM-JAIS2005-3.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bisantz2001,
	title        = {{Assessment of operator trust in and utilization of automated decision-aids under different framing conditions}},
	author       = {Bisantz, Ann M and Seong, Younho},
	year         = 2001,
	journal      = {International Journal of Industrial Ergonomics},
	volume       = 28,
	pages        = {85--97},
	url          = {https://ac.els-cdn.com/S0169814101000154/1-s2.0-S0169814101000154-main.pdf?_tid=ab8205ea-c0b0-11e7-8a06-00000aacb35e&acdnat=1509725249_efbd90d4d95baf3ba14220adc6f6bff8},
	abstract     = {Computerized aids may be used to support decision-making and control in a variety of complex, dynamic arenas. For instance, such systems have been introduced into industrial settings as the means to implement automated control or support decision-making activities such as fault detection and recovery. Of interest in these systems is the extent to which operators utilize and trust such systems, in terms of their ability to successfully control systems, or the information or decision support they provide, particularly under conditions of potential failure. A theoretical framework to describe potential factors affecting these issues, and an experiment to investigate the role of failure cause on trust and system utilization, are described. Results provide some support for factors in the theoretical framework, and also demonstrated the use of an empirically developed trust scale. Relevance to industry As manufacturing environments increasingly rely on computerized and automated systems for control and human operator support, it is necessary to understand the situational factors which could impact operators' use of such systems. This paper describes a framework which could be used to investigate trust in industrial automation settings, as well as a rating scale which could be applied. #},
	file         = {:Users/yosef/Downloads/1-s2.0-S0169814101000154-main.pdf:pdf},
	keywords     = {Automation,Decision-aids,Trust},
	mendeley-groups = {Jian_Experiments,Other_Trust_Metric_Papers}
}
@article{Bruijn2013,
	title        = {{The Base of Trust in Human-Robot Interaction}},
	author       = {Bruijn, Marlies De},
	year         = 2013,
	pages        = {1--9},
	file         = {:Users/yosef/Downloads/Bruijn,_M.L.E._de_1.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Charalambous2016,
	title        = {{The Development of a Scale to Evaluate Trust in Industrial Human-robot Collaboration}},
	author       = {Charalambous, George and Fletcher, Sarah and Webb, Philip},
	year         = 2016,
	journal      = {International Journal of Social Robotics},
	publisher    = {Springer Netherlands},
	volume       = 8,
	number       = 2,
	pages        = {193--209},
	doi          = {10.1007/s12369-015-0333-8},
	issn         = 18754805,
	abstract     = {Trust has been identified as a key element for the successful cooperation between humans and robots. However, little research has been directed at understanding trust development in industrial human-robot collaboration (HRC). With industrial robots becoming increasingly integrated into production lines as a means for enhancing productivity and quality, it will not be long before close proximity industrial HRC becomes a viable concept. Since trust is a multidimensional construct and heavily dependent on the context, it is vital to understand how trust develops when shop floor workers interact with industrial robots. To this end, in this study a trust measurement scale suitable for industrial HRC was developed in two phases. In phase one, an exploratory study was conducted to collect participants' opinions qualitatively. This led to the identification of trust related themes relevant to the industrial context and a related pool of questionnaire items was generated. In the second phase, three human-robot trials were carried out in which the questionnaire items were applied to participants using three different types of industrial robots. The results were statistically analysed to identify the key factors impacting trust and from these generate a trust measurement scale for industrial HRC.},
	file         = {:Users/yosef/Downloads/Charalambous2016_Article_TheDevelopmentOfAScaleToEvalua.pdf:pdf},
	keywords     = {Human-robot collaboration,Industrial robot,Trust scale},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{moorman1993factors,
	title        = {Factors affecting trust in market research relationships},
	author       = {Moorman, Christine and Deshpande, Rohit and Zaltman, Gerald},
	year         = 1993,
	journal      = {Journal of marketing},
	publisher    = {SAGE Publications Sage CA: Los Angeles, CA},
	volume       = 57,
	number       = 1,
	pages        = {81--101}
}
@inproceedings{holthausen2020situational,
	title        = {Situational Trust Scale for Automated Driving (STS-AD): Development and Initial Validation},
	author       = {Holthausen, Brittany E and Wintersberger, Philipp and Walker, Bruce N and Riener, Andreas},
	year         = 2020,
	booktitle    = {12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
	pages        = {40--47}
}
@article{helldin2014transparency,
	title        = {Transparency for future semi-automated systems},
	author       = {Helldin, Tove},
	year         = 2014,
	journal      = {PhD diss., Orebro University}
}
@incollection{hart1988development,
	title        = {Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research},
	author       = {Hart, Sandra G and Staveland, Lowell E},
	year         = 1988,
	booktitle    = {Advances in psychology},
	publisher    = {Elsevier},
	volume       = 52,
	pages        = {139--183}
}
@article{morra2019building,
	title        = {Building Trust in Autonomous Vehicles: role of virtual reality driving simulators in HMI design},
	author       = {Morra, Lia and Lamberti, Fabrizio and Prattic{\'o}, F Gabriele and La Rosa, Salvatore and Montuschi, Paolo},
	year         = 2019,
	journal      = {IEEE Transactions on Vehicular Technology},
	publisher    = {IEEE},
	volume       = 68,
	number       = 10,
	pages        = {9438--9450}
}
@inproceedings{xu2015optimo,
	title        = {{Optimo: Online probabilistic trust inference model for asymmetric human-robot collaborations}},
	author       = {Xu, Anqi and Dudek, Gregory},
	year         = 2015,
	booktitle    = {Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction},
	pages        = {221--228},
	file         = {:Users/yosef/Downloads/08520625.pdf:pdf},
	mendeley-groups = {Trust_Game_Theory},
	organization = {ACM}
}
@inproceedings{adams2005human,
	title        = {Human-robot interaction design: Understanding user needs and requirements},
	author       = {Adams, Julie A},
	year         = 2005,
	booktitle    = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	volume       = 49,
	number       = 3,
	pages        = {447--451},
	organization = {SAGE Publications Sage CA: Los Angeles, CA}
}
@article{wang2005trust,
	title        = {Trust in e-commerce: consideration of interface design factors},
	author       = {Wang, Ye Diana and Emurian, Henry H},
	year         = 2005,
	journal      = {Journal of Electronic Commerce in Organizations (JECO)},
	publisher    = {IGI Global},
	volume       = 3,
	number       = 4,
	pages        = {42--60}
}
@article{wang2005overview,
	title        = {An overview of online trust: Concepts, elements, and implications},
	author       = {Wang, Ye Diana and Emurian, Henry H},
	year         = 2005,
	journal      = {Computers in human behavior},
	publisher    = {Elsevier},
	volume       = 21,
	number       = 1,
	pages        = {105--125}
}

@inproceedings{BusTrust,
	title        = {{Supporting trust in autonomous driving}},
	author       = {H{\"{a}}uslschmid, Renate and von Buelow, Max and Pfleging, Bastian and Butz, Andreas},
	year         = 2017,
	booktitle    = {Proceedings of the 22nd international conference on intelligent user interfaces},
	pages        = {319--329},
	file         = {:Users/yosef/Downloads/haeuslschmid2017iui.pdf:pdf},
	mendeley-groups = {ATC,Trust_Metrics_Analysis},
	organization = {ACM}
}
@inproceedings{BusTrust,
	title        = {{Supporting trust in autonomous driving}},
	author       = {H{\"{a}}uslschmid, Renate and von Buelow, Max and Pfleging, Bastian and Butz, Andreas},
	year         = 2017,
	booktitle    = {Proceedings of the 22nd international conference on intelligent user interfaces},
	pages        = {319--329},
	file         = {:Users/yosef/Downloads/haeuslschmid2017iui.pdf:pdf},
	mendeley-groups = {ATC,Trust_Metrics_Analysis},
	organization = {ACM}
}
@article{de2017little,
	title        = {A little anthropomorphism goes a long way: Effects of oxytocin on trust, compliance, and team performance with automated agents},
	author       = {De Visser, Ewart J. and Monfort, Samuel S. and Goodyear, Kimberly and Lu, Li and O’Hara, Martin and Lee, Mary R. and Parasuraman, Raja and Krueger, Frank},
	year         = 2017,
	journal      = {Human factors},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 59,
	number       = 1,
	pages        = {116--133}
}
@article{freitag2016personality,
	title        = {Personality traits and the propensity to trust friends and strangers},
	author       = {Freitag, Markus and Bauer, Paul C},
	year         = 2016,
	journal      = {The social science journal},
	publisher    = {Elsevier},
	volume       = 53,
	number       = 4,
	pages        = {467--476}
}
@article{haidt2007new,
	title        = {The new synthesis in moral psychology},
	author       = {Haidt, Jonathan},
	year         = 2007,
	journal      = {science},
	publisher    = {American Association for the Advancement of Science},
	volume       = 316,
	number       = 5827,
	pages        = {998--1002}
}
@article{van2012sniff,
	title        = {A sniff of trust: meta-analysis of the effects of intranasal oxytocin administration on face recognition, trust to in-group, and trust to out-group},
	author       = {Van IJzendoorn, Marinus H and Bakermans-Kranenburg, Marian J},
	year         = 2012,
	journal      = {Psychoneuroendocrinology},
	publisher    = {Elsevier},
	volume       = 37,
	number       = 3,
	pages        = {438--443}
}
@article{wagner2015robots,
	title        = {Robots that stereotype: creating and using categories of people for human-robot interaction},
	author       = {Wagner, Alan R},
	year         = 2015,
	journal      = {Journal of Human-Robot Interaction},
	publisher    = {Journal of Human-Robot Interaction Steering Committee},
	volume       = 4,
	number       = 2,
	pages        = {97--124}
}
@article{tanis2005social,
	title        = {A social identity approach to trust: Interpersonal perception, group membership and trusting behaviour},
	author       = {Tanis, Martin and Postmes, Tom},
	year         = 2005,
	journal      = {European Journal of Social Psychology},
	publisher    = {Wiley Online Library},
	volume       = 35,
	number       = 3,
	pages        = {413--424}
}
@article{williams2001whom,
	title        = {In whom we trust: Group membership as an affective context for trust development},
	author       = {Williams, Michele},
	year         = 2001,
	journal      = {Academy of management review},
	publisher    = {Academy of Management Briarcliff Manor, NY 10510},
	volume       = 26,
	number       = 3,
	pages        = {377--396}
}
@incollection{alarcon2021role,
	title        = {The role of human personality on trust in human-robot interaction},
	author       = {Alarcon, Gene M and Capiola, August and Pfahler, Marc D},
	year         = 2021,
	booktitle    = {Trust in Human-Robot Interaction},
	publisher    = {Elsevier},
	pages        = {159--178}
}
@article{dohmen2018identifying,
	title        = {Identifying the effect of age on willingness to take risks},
	author       = {Dohmen, Thomas and Falk, Armin and Golsteyn, Bart and Huffman, David and Sunde, Uwe},
	year         = 2018,
	journal      = {VOX CEPR's policy portal}
}
@article{pak2012decision,
	title        = {Decision support aids with anthropomorphic characteristics influence trust and performance in younger and older adults},
	author       = {Pak, Richard and Fink, Nicole and Price, Margaux and Bass, Brock and Sturre, Lindsay},
	year         = 2012,
	journal      = {Ergonomics},
	publisher    = {Taylor \& Francis},
	volume       = 55,
	number       = 9,
	pages        = {1059--1072}
}
@article{zeffane2020gender,
	title        = {Gender, individualism--collectivism and individuals’ propensity to trust: A comparative exploratory study},
	author       = {Zeffane, Rachid},
	year         = 2020,
	journal      = {Journal of Management \& Organization},
	publisher    = {Cambridge University Press},
	volume       = 26,
	number       = 4,
	pages        = {445--459}
}
@article{cummings2010supporting,
	title        = {Supporting intelligent and trustworthy maritime path planning decisions},
	author       = {Cummings, Mary L and Buchin, Mariela and Carrigan, Geoffrey and Donmez, Birsen},
	year         = 2010,
	journal      = {International journal of human-computer studies},
	publisher    = {Elsevier},
	volume       = 68,
	number       = 10,
	pages        = {616--626}
}
@article{zhang2011effect,
	title        = {The effect of vertical--horizontal individualism--collectivism on acculturation and the moderating role of gender},
	author       = {Zhang, Jingyu and Mandl, Heinz and Wang, Erping},
	year         = 2011,
	journal      = {International Journal of Intercultural Relations},
	publisher    = {Elsevier},
	volume       = 35,
	number       = 1,
	pages        = {124--134}
}
@inproceedings{sundar2016hollywood,
	title        = {The Hollywood robot syndrome media effects on older adults' attitudes toward robots and adoption intentions},
	author       = {Sundar, S Shyam and Waddell, T Franklin and Jung, Eun Hwa},
	year         = 2016,
	booktitle    = {2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
	pages        = {343--350},
	organization = {IEEE}
}
@inproceedings{huang2017users,
	title        = {Users’ trust in automation: a cultural perspective},
	author       = {Huang, Hsiao-Ying and Bashir, Masooda},
	year         = 2017,
	booktitle    = {International Conference on Applied Human Factors and Ergonomics},
	pages        = {282--289},
	organization = {Springer}
}
@incollection{yerdon2017investigating,
	title        = {Investigating cross-cultural differences in trust levels of automotive automation},
	author       = {Yerdon, Valarie A and Marlowe, Tiffani A and Volante, William G and Li, Shuling and Hancock, Peter A},
	year         = 2017,
	booktitle    = {Advances in Cross-Cultural Decision Making},
	publisher    = {Springer},
	pages        = {183--194}
}
@article{Robert2009,
	title        = {{Individual swift trust and knowledge-based trust in face-to-face and virtual team members}},
	author       = {Robert, Lionel and Denis, Alan and Hung, Yu Ting},
	year         = 2009,
	journal      = {Journal of Management Information Systems},
	volume       = 26,
	number       = 2,
	pages        = {241--279},
	doi          = {10.2753/MIS0742-1222260210},
	issn         = {07421222},
	abstract     = {Traditionally, trust has been seen as a result of personal knowledge of an individual's past behavior. In this view, trust develops gradually over time based on an individual's cognitive assessment of the other person's behavior. However, high levels of trust have been observed among members of virtual teams, who often have little prior history of working together and may never meet each other in person. To integrate these two seemingly contradictory views of trust, this study manipulated team member characteristics and team member behavior to empirically test a two-stage theoretical model of trust formation and the influence of information and communication technologies (ICT) on trust formation. The results indicate that category-based processing of team member characteristics and an individual's own disposition to trust dominated the initial formation of swift trust. Once individuals accumulated sufficient information to assess a team member's trustworthiness, the effects of swift trust declined and knowledge-based trust formed using team members' behaviors (perceived ability, integrity, and benevolence) became dominant. The use of ICT increased perceived risk of team failure, which reduced the likelihood that team members would engage in future trusting behaviors. {\textcopyright} 2009 M.E. Sharpe, Inc.},
	file         = {:Users/yosef/Downloads/Robert et al., 2009.pdf:pdf},
	keywords     = {Cognitive trust,Computer-mediated communication,Initial trust,Knowledge-based trust,Presumptive trust,Swift trust,Trust,Vignettes,Virtual team},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{kuchenbrandt2014keep,
	title        = {Keep an eye on the task! How gender typicality of tasks influence human--robot interactions},
	author       = {Kuchenbrandt, Dieta and H{\"a}ring, Markus and Eichberg, Jessica and Eyssel, Friederike and Andr{\'e}, Elisabeth},
	year         = 2014,
	journal      = {International Journal of Social Robotics},
	publisher    = {Springer},
	volume       = 6,
	number       = 3,
	pages        = {417--427}
}
@article{merritt2012two,
	title        = {The two-factor solution to Allen and Meyer’s (1990) affective commitment scale: Effects of negatively worded items},
	author       = {Merritt, Stephanie M},
	year         = 2012,
	journal      = {Journal of Business and Psychology},
	publisher    = {Springer},
	volume       = 27,
	number       = 4,
	pages        = {421--436}
}
@article{wiegmann2001automated,
	title        = {Automated diagnostic aids: The effects of aid reliability on users' trust and reliance},
	author       = {Wiegmann, Douglas A and Rich, Aaron and Zhang, Hui},
	year         = 2001,
	journal      = {Theoretical Issues in Ergonomics Science},
	publisher    = {Taylor \& Francis},
	volume       = 2,
	number       = 4,
	pages        = {352--367}
}
@article{Chi2021,
	title        = {{Developing a formative scale to measure consumers' trust toward interaction with artificially intelligent (AI) social robots in service delivery}},
	author       = {Chi, Oscar Hengxuan and Jia, Shizhen and Li, Yafang and Gursoy, Dogan},
	year         = 2021,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier Ltd},
	volume       = 118,
	number       = {January},
	pages        = 106700,
	doi          = {10.1016/j.chb.2021.106700},
	issn         = {07475632},
	abstract     = {This study develops and validates a scale of Social Service Robot Interaction Trust (SSRIT) that measures consumers' trust toward interaction with AI social robots in service delivery. Through a systematic literature review, semi-structured interviews, a focus group study, and rigorous quantitative studies, this study conceptualizes the interaction-based trust and proposes a third-order reflective-formative scale, which suggests that trust in interaction is measured by 3 s-order indicators: propensity to trust in robot, trustworthy robot function and design, and trustworthy service task and context. Propensity to trust in robot is predicted by familiarity, robot use self-efficacy, social influence, technology attachment, and trust stance in technology. Trustworthy robot function and design is formed by anthropomorphism, robot performance, and effort expectancy. Trustworthy service task and context is determined by perceived service risk, robot-service fit, and facilitating robot-use condition. The convergent, discriminant, external, concurrent, and predictive validities of the scale are validated. Theoretical contributions and managerial implications are provided.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563221000224-main.pdf:pdf},
	keywords     = {Artificial intelligence,Interaction,Scale development,Service,Social robot,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{li2010cross,
	title        = {A cross-cultural study: Effect of robot appearance and task},
	author       = {Li, Dingjun and Rau, PL Patrick and Li, Ye},
	year         = 2010,
	journal      = {International Journal of Social Robotics},
	publisher    = {Springer},
	volume       = 2,
	number       = 2,
	pages        = {175--186}
}
@article{dimoka2010does,
	title        = {What does the brain tell us about trust and distrust? Evidence from a functional neuroimaging study},
	author       = {Dimoka, Angelika},
	year         = 2010,
	journal      = {Mis Quarterly},
	publisher    = {JSTOR},
	pages        = {373--396}
}
@article{mcknight2001while,
	title        = {While trust is cool and collected, distrust is fiery and frenzied: A model of distrust concepts},
	author       = {McKnight, D Harrison and Chervany, Norman},
	year         = 2001,
	journal      = {Amcis 2001 proceedings},
	pages        = 171
}
@inproceedings{mcknight2006distrust,
	title        = {Distrust and trust in B2C e-commerce: Do they differ?},
	author       = {McKnight, D Harrison and Choudhury, Vivek},
	year         = 2006,
	booktitle    = {Proc. of the 8th Intl. Conf. on Electronic Commerce: The new e-commerce: innovations for conquering current barriers, obstacles and limitations to conducting successful business on the internet},
	pages        = {482--491}
}
@article{bukowski1994measuring,
	title        = {Measuring friendship quality during pre-and early adolescence: The development and psychometric properties of the Friendship Qualities Scale},
	author       = {Bukowski, William M and Hoza, Betsy and Boivin, Michel},
	year         = 1994,
	journal      = {Journal of social and Personal Relationships},
	publisher    = {Sage Publications Sage CA: Thousand Oaks, CA},
	volume       = 11,
	number       = 3,
	pages        = {471--484}
}
@article{bhattacherjee2002individual,
	title        = {Individual trust in online firms: Scale development and initial test},
	author       = {Bhattacherjee, Anol},
	year         = 2002,
	journal      = {Journal of management information systems},
	publisher    = {Taylor \& Francis},
	volume       = 19,
	number       = 1,
	pages        = {211--241}
}
@article{ermisch2009measuring,
	title        = {Measuring people's trust},
	author       = {Ermisch, John and Gambetta, Diego and Laurie, Heather and Siedler, Thomas and Noah Uhrig, SC},
	year         = 2009,
	journal      = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	publisher    = {Wiley Online Library},
	volume       = 172,
	number       = 4,
	pages        = {749--769}
}
@article{thom2004measuring,
	title        = {Measuring patients’ trust in physicians when assessing quality of care},
	author       = {Thom, David H and Hall, Mark A and Pawlson, L Gregory},
	year         = 2004,
	journal      = {Health affairs},
	publisher    = {Project HOPE-The People-to-People Health Foundation, Inc.},
	volume       = 23,
	number       = 4,
	pages        = {124--132}
}
@article{Costa2011,
	title        = {{Measuring trust in teams : Development and validation of a multifaceted measure of formative and reflective indicators of team trust Measuring trust in teams : Development and validation indicators of team trust}},
	author       = {Costa, Ana Cristina and Anderson, Neil and Costa, Ana Cristina and Anderson, Neil},
	year         = 2011,
	volume       = {0643},
	doi          = {10.1080/13594320903272083},
	file         = {:Users/yosef/Downloads/Measuring trust in teams Development and validation of a multifaceted measure of formative and reflective indicators of team trust.pdf:pdf},
	mendeley-groups = {Trust_Metrics_Analysis,Other_Trust_Metric_Papers}
}
@misc{schoorman2007integrative,
	title        = {An integrative model of organizational trust: Past, present, and future},
	author       = {Schoorman, F David and Mayer, Roger C and Davis, James H},
	year         = 2007,
	publisher    = {Academy of Management Briarcliff Manor, NY 10510}
}
@article{anderson1990development,
	title        = {Development of the Trust in Physician scale: a measure to assess interpersonal trust in patient-physician relationships},
	author       = {Anderson, Lynda A and Dedrick, Robert F},
	year         = 1990,
	journal      = {Psychological reports},
	publisher    = {SAGE Publications Sage CA: Los Angeles, CA},
	volume       = 67,
	number       = {3\_suppl},
	pages        = {1091--1100}
}
@article{johnson2005cognitive,
	title        = {Cognitive and affective trust in service relationships},
	author       = {Johnson, Devon and Grayson, Kent},
	year         = 2005,
	journal      = {Journal of Business research},
	publisher    = {Elsevier},
	volume       = 58,
	number       = 4,
	pages        = {500--507}
}
@article{mcevily2011measuring,
	title        = {Measuring trust in organisational research: Review and recommendations},
	author       = {McEvily, Bill and Tortoriello, Marco},
	year         = 2011,
	journal      = {Journal of Trust Research},
	publisher    = {Taylor \& Francis},
	volume       = 1,
	number       = 1,
	pages        = {23--63}
}


@article{Dadashi2013,
	title        = {{Semi-automated CCTV surveillance: The effects of system confidence, system accuracy and task complexity on operator vigilance, reliance and workload}},
	author       = {Dadashi, N. and Stedmon, A. W. and Pridmore, T. P.},
	year         = 2013,
	journal      = {Applied Ergonomics},
	publisher    = {Elsevier Ltd},
	volume       = 44,
	number       = 5,
	pages        = {730--738},
	doi          = {10.1016/j.apergo.2012.04.012},
	issn         = 18729126,
	abstract     = {Recent advances in computer vision technology have lead to the development of various automatic surveillance systems, however their effectiveness is adversely affected by many factors and they are not completely reliable. This study investigated the potential of a semi-automated surveillance system to reduce CCTV operator workload in both detection and tracking activities. A further focus of interest was the degree of user reliance on the automated system. A simulated prototype was developed which mimicked an automated system that provided different levels of system confidence information. Dependent variable measures were taken for secondary task performance, reliance and subjective workload. When the automatic component of a semi-automatic CCTV surveillance system provided reliable system confidence information to operators, workload significantly decreased and spare mental capacity significantly increased. Providing feedback about system confidence and accuracy appears to be one important way of making the status of the automated component of the surveillance system more 'visible' to users and hence more effective to use. {\textcopyright} 2012 Elsevier Ltd and The Ergonomics Society.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0003687012000695-main.pdf:pdf},
	keywords     = {CCTV,Detection,Security,Semi-automated surveillance,Tracking},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Daniel2013,
	title        = {{Simplified Human-Robot Interaction: Modeling and Evaluation}},
	author       = {Daniel, Balazs and Thomessen, Trygve and Korondi, Peter},
	year         = 2013,
	journal      = {Modeling, Identification and Control: A Norwegian Research Bulletin},
	volume       = 34,
	number       = 4,
	pages        = {199--211},
	doi          = {10.4173/mic.2013.4.4},
	issn         = {0332-7353},
	abstract     = {In this paper a novel concept of human-robot interaction (HRI) modeling is proposed. Including factors like trust in automation, situational awareness, expertise and expectations a new user experience framework is formed for industrial robots. Service Oriented Robot Operation, proposed in a previous paper, creates an abstract level in HRI and it is also included in the framework. This concept is evaluated with exhaustive tests. Results prove that significant improvement in task execution may be achieved and the new system is more usable for operators with less experience with robotics; personnel specific for small and medium enterprises (SMEs).},
	file         = {:Users/yosef/Downloads/MIC-2013-4-4.pdf:pdf},
	keywords     = {graphical user interface,human-robot interaction,industrial robotics,usability},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{DeVisser2011,
	title        = {{Adaptive Aiding of Human-Robot Teaming:Effects of Imperfect Automation on Performance, Trust, and Workload}},
	author       = {{De Visser}, Ewart and Parasuraman, Raja},
	year         = 2011,
	journal      = {Journal of Cognitive Engineering and Decision Making},
	volume       = 5,
	number       = 2,
	pages        = {209--231},
	doi          = {10.1177/1555343411410160},
	issn         = 15553434,
	file         = {:Users/yosef/Downloads/1555343411410160.pdf:pdf},
	keywords     = {adaptive automation,automation,human-robot interaction},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Ekman2019,
	title        = {{Exploring automated vehicle driving styles as a source of trust information}},
	author       = {Ekman, Fredrick and Johansson, Mikael and Blig{\aa}rd, Lars Ola and Karlsson, Mari Anne and Str{\"{o}}mberg, Helena},
	year         = 2019,
	journal      = {Transportation Research Part F: Traffic Psychology and Behaviour},
	publisher    = {Elsevier Ltd},
	volume       = 65,
	pages        = {268--279},
	doi          = {10.1016/j.trf.2019.07.026},
	issn         = 13698478,
	url          = {https://doi.org/10.1016/j.trf.2019.07.026},
	abstract     = {Trust is important for users' acceptance and adoption of automated vehicles (AVs). Previous research has mainly focused on what information affects user trust in AVs and how the user's trust is affected by the way the content is communicated. However, recent studies have shown that trust may also be affected by the AV's driving style. The aim of the study was to further investigate if and how the vehicle's driving style affects user trust in AVs and, in particular, how this is expressed by users. An experiment involving 18 participants, using a Wizard of Oz setup and within a subject design, was conducted comparing two different driving styles, ‘Defensive' and ‘Aggressive'. Trust was measured using a mixed method research design including momentaneous trust ratings and think-aloud procedures while driving, a post-run trust questionnaire as well as trust curve sketching and a personal interview. The results show that driving style had an effect on user trust and that the ‘Defensive' driving style was perceived as more trustworthy, in part because it was deemed more predictable than the ‘Aggressive' driving style. Furthermore, participants expressed trust in using affective, analogical and analytic responses, the two former during the test runs and the latter directly after each test run. The interview after the completion produced a more mixed result. By combining different data collection methods, a nuanced picture of the trust formation process and users' trust in AVs was obtained. The study concludes that it is important to consider the vehicle performance information provided by the vehicle's driving style so as to create user trust in AVs.},
	file         = {:Users/yosef/Downloads/1-s2.0-S1369847818306594-main.pdf:pdf},
	keywords     = {Automated vehicles,Driving style,Multi-methods approach,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Ekman2018,
	title        = {{Creating appropriate trust in automated vehicle systems: A framework for HMI design}},
	author       = {Ekman, Fredrick and Johansson, Mikael and Sochor, Jana},
	year         = 2018,
	journal      = {IEEE Transactions on Human-Machine Systems},
	publisher    = {IEEE},
	volume       = 48,
	number       = 1,
	pages        = {95--101},
	doi          = {10.1109/THMS.2017.2776209},
	issn         = 21682291,
	abstract     = {While automated vehicle technology progresses, potentially leading to a safer and more efficient traffic environment, many challenges remain within the area of human factors, such as user trust for automated driving (AD) vehicle systems. The aim of this paper is to investigate how an appropriate level of user trust for AD vehicle systems can be created via human-machine interaction (HMI). A guiding framework for implementing trust-related factors into the HMI interface is presented. This trust-based framework incorporates usage phases, AD events, trust-Affecting factors, and levels explaining each event from a trust perspective. Based on the research findings, the authors recommend that HMI designers and automated vehicle manufacturers take a more holistic perspective on trust rather than focusing on single, 'isolated' events, for example understanding that trust formation is a dynamic process that starts long before a user's first contact with the system, and continues long thereafter. Furthermore, factors-Affecting trust change, both during user interactions with the system and over time; thus, HMI concepts need to be able to adapt. Future work should be dedicated to understanding how trust-related factors interact, as well as validating and testing the trust-based framework.},
	file         = {:Users/yosef/Downloads/08125704.pdf:pdf},
	keywords     = {Automated vehicle systems,Framework,Human-machine interaction (HMI),Trust,User},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Erebak2019,
	title        = {{Caregivers ' attitudes toward potential robot coworkers in elder care}},
	author       = {Erebak, Serkan and Turgut, T{\"{u}}lay},
	year         = 2019,
	journal      = {Cognition, Technology \& Work},
	publisher    = {Springer London},
	volume       = 21,
	number       = 2,
	pages        = {327--336},
	doi          = {10.1007/s10111-018-0512-0},
	isbn         = {0123456789},
	issn         = {1435-5566},
	url          = {http://dx.doi.org/10.1007/s10111-018-0512-0},
	file         = {:Users/yosef/Downloads/Erebak-Turgut2019_Article_CaregiversAttitudesTowardPoten.pdf:pdf},
	keywords     = {Elder care,Robots,Automation,Trust in robots,Anthr,anthropomorphism,automation,elder care,intention to work,robots,trust in robots},
	mendeley-groups = {Brzowski_All,Other_Trust_Metric_Papers}
}
@article{Freude2019,
	title        = {{Association for Information Systems Association for Information Systems Unveiling Emotions: Attitudes Toward Affective Technology Unveiling Emotions: Attitudes Toward Affective Technology}},
	author       = {Freude, Henrik and Heger, Oliver and Niehaves, Bjoern},
	year         = 2019,
	journal      = {ICIS 2019 Proceedings},
	number       = 20
}
@article{tussyadiah2018,
	title        = {When guests trust hosts for their words: Host description and trust in sharing economy},
	author       = {Tussyadiah, Iis P and Park, Sangwon},
	year         = 2018,
	journal      = {Tourism Management},
	publisher    = {Elsevier},
	volume       = 67,
	pages        = {261--272}
}
@article{meng2018towards,
	title        = {Towards Bayesian-based trust management for insider attacks in healthcare software-defined networks},
	author       = {Meng, Weizhi and Choo, Kim-Kwang Raymond and Furnell, Steven and Vasilakos, Athanasios V and Probst, Christian W},
	year         = 2018,
	journal      = {IEEE Transactions on Network and Service Management},
	publisher    = {IEEE},
	volume       = 15,
	number       = 2,
	pages        = {761--773}
}
@article{coker2017ineffectiveness,
	title        = {The ineffectiveness of counterclaim advertising for increasing consumer sentiment},
	author       = {Coker, Brent},
	year         = 2017,
	journal      = {Journal of Consumer Behaviour},
	publisher    = {Wiley Online Library},
	volume       = 16,
	number       = 1,
	pages        = {34--41}
}
@article{hancock2021evolving,
	title        = {Evolving trust in robots: specification through sequential and comparative meta-analyses},
	author       = {Hancock, Peter A and Kessler, Theresa T and Kaplan, Alexandra D and Brill, John C and Szalma, James L},
	year         = 2021,
	journal      = {Human factors},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 63,
	number       = 7,
	pages        = {1196--1229}
}

@unpublished{Chuttur,
	title        = {{Working Papers on Information Systems Overview of the Technology Acceptance Model : Origins , Developments and Future Directions}},
	author       = {Chuttur, Mohammad},
	year         = 2009,
	journal      = {Sprouts: Working Papers on Information Systems},
	volume       = 9,
	issue        = 37,
	school       = {Indiana University}
}
@article{Heerink2011,
	title        = {{Exploring the influence of age, gender, education and computer experience on robot acceptance by older adults}},
	author       = {Heerink, Marcel},
	year         = 2011,
	journal      = {HRI 2011 - Proceedings of the 6th ACM/IEEE International Conference on Human-Robot Interaction},
	publisher    = {IEEE},
	pages        = {147--148},
	doi          = {10.1145/1957656.1957704},
	isbn         = 9781450305617,
	abstract     = {It is generally recognized that non perceptual factors like age, gender, education and computer experience can have a moderating effect on how perception of a technology leads to acceptance of it. In our present research we are exploring the influence of these factors on the acceptance of assistive social robots by older adults. In this short paper we discuss the results of a user study in which a movie of an elderly person using a social assistive robot was shown to older adults. The analysis of the responses give a first indication on if and how these factors relate to the perceptual processes that lead to acceptance.},
	file         = {:Users/yosef/Downloads/06281268.pdf:pdf},
	keywords     = {Assistive technology,Human-robot interaction,Technology acceptance models},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Ammenwerth2019,
	title        = {{Technology Acceptance Models in ealth nformatics: TAM and UTAUT}},
	author       = {Ammenwerth, Elske},
	year         = 2019,
	journal      = {Studies in Health Technology and Informatics},
	volume       = 263,
	pages        = {64--71},
	doi          = {10.3233/SHTI190111},
	isbn         = 9781614999904,
	issn         = 18798365,
	abstract     = {Both the Technology Acceptance Model (TAM) and the Unified Theory of Acceptance and Use of Technology (UTAUT) aim at understanding better why users accept or reject a given technology, and how user acceptance can be improved through technology design. Two case studies are presented where TAM and UTAUT were successfully used in a health care setting to predict technology adoption. Both models have found popularity in health care. However, recent reviews show that TAM and UTAUT failed to provide stable predictive capabilities for acceptance and use of technologies in health care. Reasons for this may be the specific context of health care, where not only the technology, but also socio-organizational and cultural factors influence technology acceptance.},
	file         = {:Users/yosef/Downloads/SHTI-263-SHTI190111.pdf:pdf},
	keywords     = {Assessment,Attitude to computers,Health informatics,Intention,Models,technology,theoretical},
	pmid         = 31411153
}
@incollection{law2021trust,
	title        = {{Trust: Recent concepts and evaluations in human-robot interaction}},
	author       = {Law, Theresa and Scheutz, Matthias},
	year         = 2021,
	booktitle    = {Trust in Human-Robot Interaction},
	publisher    = {Elsevier},
	pages        = {27--57}
}
@article{matthes2008content,
	title        = {The content analysis of media frames: Toward improving reliability and validity},
	author       = {Matthes, J{\"o}rg and Kohring, Matthias},
	year         = 2008,
	journal      = {Journal of communication},
	publisher    = {Oxford University Press},
	volume       = 58,
	number       = 2,
	pages        = {258--279}
}
@article{Komiak2006,
	title        = {{The effects of personalization and familiarity on trust and adoption of recommendation agents}},
	author       = {Komiak, Sherrie Y.X. and Benbasat, Izak},
	year         = 2006,
	journal      = {MIS Quarterly: Management Information Systems},
	volume       = 30,
	number       = 4,
	pages        = {941--960},
	doi          = {10.2307/25148760},
	issn         = {02767783},
	abstract     = {In the context of personalization technologies, such as Web-based product-brokering recommendation agents (RAs) in electronic commerce, existing technology acceptance theories need to be expanded to take into account not only the cognitive beliefs leading to adoption behavior, but also the affect elicited by the personalized nature of the technology. This study takes a trust-centered, cognitive and emotional balanced perspective to study RA adoption. Grounded on the theory of reasoned action, the IT adoption literature, and the trust literature, this study theoretically articulates and empirically examines the effects of perceived personalization and familiarity on cognitive trust and emotional trust in an RA, and the impact of cognitive trust and emotional trust on the intention to adopt the RA either as a decision aid or as a delegated agent. An experiment was conducted using two commercial RAs. PLS analysis results provide empirical support for the proposed theoretical perspective. Perceived personalization significantly increases customers' intention to adopt by increasing cognitive trust and emotional trust. Emotional trust plays an important role beyond cognitive trust in determining customers ' intention to adopt. Emotional trust fully mediates the impact of cognitive trust on the intention to adopt the RA as a delegated agent, while it only partially mediates the impact of cognitive trust on the intention to adopt the RA as a decision aid. Familiarity increases the intention to adopt through cognitive trust and emotional trust.},
	file         = {:Users/yosef/Downloads/25148760-3.pdf:pdf},
	keywords     = {Adoption,Cognitive trust,Delegation,Electronic commerce,Emotional trust,Familiarity,Personalization,Recommendation agent,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}

@article{yamagishi2001,
	title        = {{Trust as a form of social intelligence.}},
	author       = {Yamagishi, Toshio},
	year         = 2001,
	journal      = {Trust in Society},
	publisher    = {Russell Sage Foundation},
	address      = {New York, New York, USA},
	pages        = {121--147},
	editor       = {Cook, K.S.}
}
@article{McKnight2001,
	title        = {{What trust means in e-commerce customer relationships: An interdisciplinary conceptual typology}},
	author       = {McKnight, D. Harrison and Chervany, Norman L.},
	year         = 2001,
	journal      = {International Journal of Electronic Commerce},
	volume       = 6,
	number       = 2,
	pages        = {35--59},
	doi          = {10.1080/10864415.2001.11044235},
	isbn         = 10864415,
	issn         = 10864415,
	abstract     = {ABSTRACT: Trust is a vital relationship concept that needs clarification because researchers across disciplines have defined it in so many different ways. A typology of trust t ypes would make it easier to compare and communicate results, and would be especially valuable if the types of trust related to one other. The typology should be interdisciplinary because many disciplines research e-commerce. This paper justifies a parsimonious interdisciplinary t ypology and relates trust constructs to e-commerce consumer actions, defining both conceptual-level and operational-level trust constructs. Conceptual-level constructs consist of disposition to trust (primarily from psychology), institution-based trust (from sociology), and trusting beliefs and trusting intentions (primarily from social psychology). Each construct is decomposed into measurable subconstructs, and the typology shows how trust constructs relate to already existing Internet relationship constructs. The effects of Web vendor interventions on consumer behaviors are posited to be partially mediated by consumer trusting beliefs and trusting intentions in the e-vendor. KEY WORDS AND PHRASES: Customer relationships, human issues in e-commerce, Internet consumers, trust. [ABSTRACT FROM AUTHOR]},
	archiveprefix = {arXiv},
	arxivid      = {1104.0942},
	eprint       = {1104.0942},
	file         = {:Users/yosef/Downloads/What_trust_means_in_e-commerce_customer.pdf:pdf},
	keywords     = {Customer relationships,Human issues in e-commerce,Internet consumers,Trust},
	mendeley-groups = {NASA,Main_Trust_Metric_Papers},
	pmid         = 6418522
}
@article{Parasuraman2008,
	title        = {{Situation Awareness,Mental Workload,and Trust in Automation:Viable,Empirically Supported Cognitive Engineering Constructs}},
	author       = {Parasuraman, Raja and Sheridan, Thomas B and Wickens, Christopher D},
	year         = 2008,
	journal      = {Journal of Cognitive Engineering and Decision Making},
	volume       = 2,
	number       = 2,
	pages        = {140--160},
	doi          = {10.1518/155534308X284417.},
	isbn         = {1555-3434},
	issn         = {1555-3434,},
	abstract     = {ABSTRACT: Cognitive engineering needs viable constructs and principles to promote\nbetter understanding and prediction of human performance in complex systems.\nThree human cognition and performance constructs that have been the subjects of\nmuch attention in research and practice over the past three decades are situation\nawareness (SA), mental workload, and trust in automation. Recently, Dekker and\nWoods (2002) and Dekker and Hollnagel (2004; henceforth DWH) argued that these\nconstructs represent “folk models” without strong empirical foundations and lacking\nscientific status. We counter this view by presenting a brief description of the large\nscience base of empirical studies on these constructs. We show that the constructs\ncan be operationalized using behavioral, physiological, and subjective measures,\nsupplemented by computational modeling, but that the constructs are also distinct\nfrom human performance. DWH also caricatured as “abracadabra” a framework\nsuggested by us to address the problem of the design of automated systems\n(Parasuraman, Sheridan, & Wickens, 2000). We point to several factual and conceptual\nerrors in their description of our approach. Finally, we rebut DWH's view that SA,\nmental workload, and trust represent folk concepts that are not falsifiable. We\nconclude that SA, mental workload, and trust are viable constructs that are valuable in\nunderstanding and predicting human-system performance in complex systems.},
	file         = {:Users/yosef/Downloads/Situation awareness mental workload and trust in automation - Viable empirically supported cognitive engineering constructs.pdf:pdf},
	mendeley-groups = {NASA}
}
@article{watkins2018,
	title        = {Exploratory Factor Analysis: A Guide to Best Practice},
	author       = {Marley W. Watkins},
	year         = 2018,
	journal      = {Journal of Black Psychology},
	volume       = 44,
	number       = 3,
	pages        = {219--246},
	doi          = {10.1177/0095798418771807},
	url          = {https://doi.org/10.1177/0095798418771807},
	eprint       = {https://doi.org/10.1177/0095798418771807},
	abstract     = {Exploratory factor analysis (EFA) is a multivariate statistical method that has become a fundamental tool in the development and validation of psychological theories and measurements. However, researchers must make several thoughtful and evidence-based methodological decisions while conducting an EFA, and there are a number of options available at each decision point, some better than others. Reviews of the professional literature have consistently found that many applications of EFA are marked by an injudicious choice of methods and incomplete reports. This article provides a systematic, evidence-based guide to the conduct of EFA studies that can be followed by researchers with modest statistical training, supplemented with an example to illustrate its application.}
}
@incollection{bollen2013eight,
	title        = {Eight myths about causality and structural equation models},
	author       = {Bollen, Kenneth A and Pearl, Judea},
	year         = 2013,
	booktitle    = {Handbook of causal analysis for social research},
	publisher    = {Springer},
	pages        = {301--328}
}
@article{jackson2009reporting,
	title        = {Reporting practices in confirmatory factor analysis: an overview and some recommendations.},
	author       = {Jackson, Dennis L and Gillaspy Jr, J Arthur and Purc-Stephenson, Rebecca},
	year         = 2009,
	journal      = {Psychological methods},
	publisher    = {American Psychological Association},
	volume       = 14,
	number       = 1,
	pages        = 6
}
@incollection{orne1996demand,
	title        = {Demand characteristics},
	author       = {Orne, Martin T},
	year         = 1996,
	booktitle    = {Introducing psychological research},
	publisher    = {Springer},
	pages        = {395--401}
}
@article{merritt2019automation,
	title        = {Automation-induced complacency potential: Development and validation of a new scale},
	author       = {Merritt, Stephanie M and Ako-Brew, Alicia and Bryant, William J and Staley, Amy and McKenna, Michael and Leone, Austin and Shirase, Lei},
	year         = 2019,
	journal      = {Frontiers in psychology},
	publisher    = {Frontiers},
	volume       = 10,
	pages        = 225
}
@article{merritt2015measuring,
	title        = {Measuring individual differences in the perfect automation schema},
	author       = {Merritt, Stephanie M and Unnerstall, Jennifer L and Lee, Deborah and Huber, Kelli},
	year         = 2015,
	journal      = {Human factors},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 57,
	number       = 5,
	pages        = {740--753}
}
@article{watson1988development,
	title        = {Development and validation of brief measures of positive and negative affect: the PANAS scales.},
	author       = {Watson, David and Clark, Lee Anna and Tellegen, Auke},
	year         = 1988,
	journal      = {Journal of personality and social psychology},
	publisher    = {American Psychological Association},
	volume       = 54,
	number       = 6,
	pages        = 1063
}
@article{de2005assessing,
	title        = {Assessing the effects of building social intelligence in a robotic interface for the home},
	author       = {De Ruyter, Boris and Saini, Privender and Markopoulos, Panos and Van Breemen, Albert},
	year         = 2005,
	journal      = {Interacting with computers},
	publisher    = {Oxford University Press Oxford, UK},
	volume       = 17,
	number       = 5,
	pages        = {522--541}
}
@article{Dragan2015,
	title        = {{Effects of Robot Motion on Human-Robot Collaboration}},
	author       = {Dragan, Anca D. and Bauman, Shira and Forlizzi, Jodi and Srinivasa, Siddhartha S.},
	year         = 2015,
	journal      = {ACM/IEEE International Conference on Human-Robot Interaction},
	publisher    = {ACM},
	volume       = {2015-March},
	pages        = {51--58},
	doi          = {10.1145/2696454.2696473},
	isbn         = 9781450328821,
	issn         = 21672148,
	abstract     = {Most motion in robotics is purely functional, planned to achieve the goal and avoid collisions. Such motion is great in isolation, but collaboration affords a human who is watching the motion and making inferences about it, trying to coordinate with the robot to achieve the task. This paper analyzes the benefit of planning motion that explicitly enables the collaborator's inferences on the success of physical collaboration, as measured by both objective and subjective metrics. Results suggest that legible motion, planned to clearly express the robot's intent, leads to more fluent collaborations than predictable motion, planned to match the collaborator's expectations. Furthermore, purely functional motion can harm coordination, which negatively affects both task efficiency, as well as the participants' perception of the collaboration.},
	file         = {:Users/yosef/Downloads/08520620.pdf:pdf},
	keywords     = {coordination,human-robot collaboration,intent,motion},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@inproceedings{kidd2008robots,
	title        = {Robots at home: Understanding long-term human-robot interaction},
	author       = {Kidd, Cory D and Breazeal, Cynthia},
	year         = 2008,
	booktitle    = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
	pages        = {3230--3235},
	organization = {IEEE}
}
@misc{mcshane2014propensity,
	title        = {Propensity to trust scale},
	author       = {McShane, SL and von Glinow, M},
	year         = 2014
}

@incollection{uslaner2015measuring,
	title        = {Measuring generalized trust: In defense of the ‘standard’question},
	author       = {Uslaner, Eric M},
	year         = 2015,
	booktitle    = {Handbook of research methods on trust},
	publisher    = {Edward Elgar Publishing}
}

@article{starke2021fairness,
	title        = {Fairness Perceptions of Algorithmic Decision-Making: A Systematic Review of the Empirical Literature},
	author       = {Starke, Christopher and Baleis, Janine and Keller, Birte and Marcinkowski, Frank},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2103.12016}
}
@article{Merritt2008,
	title        = {{Not All Trust Is Created Equal : Dispositional and History- Based Trust in Human-Automation Interactions}},
	author       = {Merritt, Stephanie M and Ilgen, Daniel R},
	year         = 2008,
	journal      = {Human Factors},
	volume       = 50,
	number       = 2,
	pages        = {194--210},
	doi          = {10.1518/001872008X288574.},
	file         = {:Users/yosef/Downloads/001872008x288574-2.pdf:pdf},
	mendeley-groups = {Merritt}
}
@article{Ajenaghughrure2020,
	title        = {{Measuring trust with psychophysiological signals: A systematic mapping study of approaches used}},
	author       = {Ajenaghughrure, Ighoyota Ben and Sousa, Sonia Da Costa and Lamas, David},
	year         = 2020,
	journal      = {Multimodal Technologies and Interaction},
	volume       = 4,
	number       = 3,
	pages        = {1--29},
	doi          = {10.3390/mti4030063},
	issn         = 24144088,
	abstract     = {Trust plays an essential role in all human relationships. However, measuring trust remains a challenge for researchers exploring psychophysiological signals. Therefore, this article aims to systematically map the approaches used in studies assessing trust with psychophysiological signals. In particular, we examine the numbers and frequency of combined psychophysiological signals, the primary outcomes of previous studies, and the types and most commonly used data analysis techniques for analyzing psychophysiological data to infer a trust state. For this purpose, we employ a systematic mapping review method, through which we analyze 51 carefully selected articles (studies focused on trust using psychophysiology). Two significant findings are as follows: (1) Psychophysiological signals from EEG(electroencephalogram) and ECG(electrocardiogram) for monitoring peripheral and central nervous systems are the most frequently used to measure trust, while audio and EOG(electro-oculography) psychophysiological signals are the least commonly used. Moreover, the maximum number of psychophysiological signals ever combined so far is three (3). Most of which are peripheral nervous system monitoring psychophysiological signals that are low in spatial resolution. (2) Regarding outcomes: there is only one tool proposed for assessing trust in an interpersonal context, excluding trust in a technology context. Moreover, there are no stable and accurate ensemble models that have been developed to assess trust; all prior attempts led to unstable but fairly accurate models or did not satisfy the conditions for combining several algorithms (ensemble). In conclusion, the extent to which trust can be assessed using psychophysiological measures during user interactions (real-time) remains unknown, as there several issues, such as the lack of a stable and accurate ensemble trust classifier model, among others, that require urgent research attention. Although this topic is relatively new, much work has been done. However, more remains to be done to provide clarity on this topic.},
	file         = {:Users/yosef/Downloads/mti-04-00063-v2.pdf:pdf},
	keywords     = {Human-to-human relationship,Human-to-technology relationship,Psychophysiology,Trust}
}
@article{Cohen1998,
	title        = {{Trust in decision aids: A model and its training implications}},
	author       = {Cohen, MS and Parasuraman, Raja and Freeman, JT},
	year         = 1998,
	journal      = {in Proc. Command and Control {\ldots}},
	pages        = {1--37},
	doi          = {10.1.1.90.2591},
	url          = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.90.2591},
	abstract     = {Decision aids have much in common with other types of automation. For example, they vary in the level of automation that they offer — from data integration, through expert systems that generate decision options, to associate systems that take action unless the user overrides. In principle, “autonomous ” systems can be developed that evaluate, choose and act without the user's knowledge. Automation of decision making has not advanced as far along this spectrum as automation in other fields. Explanations for this “failure ” vary. According to some advocates of decision aids, the reason is a sort of irrational technophobia, evidenced by a lack of appropriate trust in the decisions that such aids make. According to skeptics, on the other hand, a good reason for rejecting decision aids altogether is overtrust: the tendency of users to rely on an aid as if it were infallible when they should instead rely on their own judgment. Strangely enough, both camps share a similar concept of trust. In both views, trust is a relatively enduring attitude that a user has toward an aid, akin to love, hate, or faith, rather than a more transient and situation-specific attitude, such as agreement or disagreement. We will propose an alternative, more differentiated conception of trust. It includes the},
	file         = {:Users/yosef/Downloads/PhaseIReportTrust_in_decision_aids_final.pdf:pdf}
}
@article{Cho2015,
	title        = {{A Survey on Trust Modeling}},
	author       = {Cho, Jin Hee and Chan, Kevin and Adali, Sibel},
	year         = 2015,
	journal      = {ACM Computing Surveys},
	volume       = 48,
	number       = 2,
	doi          = {10.1145/2815595},
	issn         = 15577341,
	abstract     = {The concept of trust and/or trust management has received considerable attention in engineering research communities as trust is perceived as the basis for decision making in many contexts and the motivation for maintaining long-term relationships based on cooperation and collaboration. Even if substantial research effort has been dedicated to addressing trust-based mechanisms or trust metrics (or computation) in diverse contexts, prior work has not clearly solved the issue of how to model and quantify trust with sufficient detail and context-based adequateness. The issue of trust quantification has become more complicated as we have the need to derive trust from complex, composite networks that may involve four distinct layers of communication protocols, information exchange, social interactions, and cognitive motivations. In addition, the diverse application domains require different aspects of trust for decision making such as emotional, logical, and relational trust. This survey aims to outline the foundations of trust models for applications in these contexts in terms of the concept of trust, trust assessment, trust constructs, trust scales, trust properties, trust formulation, and applications of trust. We discuss how different components of trust can be mapped to different layers of a complex, composite network; applicability of trust metrics and models; research challenges; and future work directions.},
	annote       = {Critical uni-trust scale, logic. poor review though. Connects to paladyn paper - USE LATER!},
	file         = {:Users/yosef/Downloads/2815595.pdf:pdf},
	keywords     = {Algorithms,Composite trust,Decision making,Human Factors,Modeling,Networks,Trust,Trust modeling,Trustee,Trustor}
}
@book{Abbass,
  title={Foundations of trusted autonomy},
  author={Abbass, Hussein A and Scholz, Jason and Reid, Darryn J},
  year={2018},
  publisher={Springer Nature}
}
@article{Molloy_1996_Monitoringautomatedsystem,
	title        = {{Monitoring an automated system for a single failure: vigilance and task complexity effects}},
	author       = {Molloy, R and Parasuraman, R},
	year         = 1996,
	journal      = {Human Factors},
	volume       = 38,
	pages        = {311--322},
	annote       = {NULL}
}
@article{Crawford2021,
	title        = {{Generalised Anxiety Disorder and Depression on Implicit and Explicit Trust Tendencies Toward Automated Systems}},
	author       = {Crawford, Jordan R. and Goh, Yee Mey and Hubbard, Ella Mae},
	year         = 2021,
	journal      = {IEEE Access},
	volume       = 9,
	pages        = {68081--68092},
	doi          = {10.1109/ACCESS.2021.3076681},
	issn         = 21693536,
	abstract     = {This paper explores whether generalised anxiety disorder (GAD) and depression have any effect on an individual's explicit general propensity to trust automated systems (trust that is unspecific to any one automated system) and whether those that do have these disorders have an implicit bias towards automated systems over other humans. The human-automated system literature to date has discovered that individual differences in humans, such as self-confidence, mood, and personality types, can influence the human-automated system relationship through human trust and reliance attitudes and behaviour. However, whether suffering from a mental disorder influences an individual's attitudes towards automated systems generally is yet to be explored. In this study, 184 UK university students responded to online experiments between December 2019 - January 2020 and were subjected to the cultural trust instrument survey and the implicit association test in a between-subjects design to measure their general propensity to trust and implicit association towards automated systems respectively. A two-way ANOVA was performed to evaluate GAD $\times $ depression interaction effects on the dependent variables. Results suggest there was a significant interaction between GAD and depression regarding propensity to trust automated systems but they have little to no influential effect on mean implicit association test scores. Furthermore, those without depression showed a significantly higher trust score when they also had GAD. It can be concluded that GAD and depression have potential critical influence over human-automated system trust, thus creating potential issues with misuse and disuse and must be accounted for in automated system design.},
	file         = {:Users/yosef/Downloads/09419043.pdf:pdf},
	keywords     = {Automated systems,common mental disorders,design,implicit association,trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@misc{halpert2008technological,
	title        = {Technological Adoptiveness Scale (TAS): Internal Properties and Construct Validity},
	author       = {Halpert, A and Horvath, A and Preston, F and Somerville, K and Semnani-azad, Z},
	year         = 2008
}
@article{Ghazizadeh2012,
	title        = {{Extending the Technology Acceptance Model to assess automation}},
	author       = {Ghazizadeh, Mahtab and Lee, John D. and Boyle, Linda Ng},
	year         = 2012,
	journal      = {Cognition, Technology and Work},
	volume       = 14,
	number       = 1,
	pages        = {39--49},
	doi          = {10.1007/s10111-011-0194-3},
	issn         = 14355558,
	abstract     = {Often joint human-automation performance depends on the factors influencing the operator's tendency to rely on and comply with automation. Although cognitive engineering (CE) researchers have studied automation acceptance as related to task-technology compatibility and human-technology coagency, information system (IS) researchers have evaluated user acceptance of technology, using the Technology Acceptance Model (TAM). The parallels between the two views suggest that the user acceptance perspective from the IS community can complement the human-automation interaction perspective from the CE community. TAM defines constructs that govern acceptance and provides a framework for evaluating a broad range of factors influencing technology acceptance and reliance. TAM is extensively used by IS researchers in various applications and it can be applied to assess the effect of trust and other factors on automation acceptance. Likewise, extensions to the TAM framework use the constructs of task-technology compatibility and past experience to extend its description of the role of human-automation interaction in automation adoption. We propose the Automation Acceptance Model (AAM) to draw upon the IS and CE perspectives and take into account the dynamic and multi-level nature of automation use, highlighting the influence of use on attitudes that complements the more common view that attitudes influence use. {\textcopyright} 2011 Springer-Verlag London Limited.},
	file         = {:Users/yosef/Downloads/Ghazizadeh2012_Article_ExtendingTheTechnologyAcceptan.pdf:pdf},
	keywords     = {Automation Acceptance Model (AAM),Cognitive engineering,Task-technology compatibility,Technology Acceptance Model (TAM),Technology acceptance,Trust in automation},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@inproceedings{dolgov2017trust,
	title        = {Trust in automation inventories: An investigation and comparison of the human-computer trust and trust in automated systems scales},
	author       = {Dolgov, Igor and Kaltenbach, Elizabeth K},
	year         = 2017,
	booktitle    = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	volume       = 61,
	number       = 1,
	pages        = {1271--1275},
	organization = {SAGE Publications Sage CA: Los Angeles, CA}
}
@article{adams2003trust,
	title        = {Trust in automated systems},
	author       = {Adams, Barbara D and Bruyn, Lora E and Houde, S{\'e}bastien and Angelopoulos, Paul and Iwasa-Madge, Kim and McCann, Carol},
	year         = 2003,
	journal      = {Toronto: Ministry of National Defence}
}
@article{Hancock2020,
	title        = {{Evolving Trust in Robots: Specification Through Sequential and Comparative Meta-Analyses}},
	author       = {Hancock, P. A. and Kessler, Theresa T. and Kaplan, Alexandra D. and Brill, John C. and Szalma, James L.},
	year         = 2020,
	journal      = {Human Factors},
	doi          = {10.1177/0018720820922080},
	issn         = 15478181,
	abstract     = {Objective: The objectives of this meta-analysis are to explore the presently available empirical findings on the antecedents of trust in robots and use this information to expand upon a previous meta-analytic review of the area. Background: Human–robot interaction (HRI) represents an increasingly important dimension of our everyday existence. Currently, the most important element of these interactions is proposed to be whether the human trusts the robot or not. We have identified three overarching categories that exert effects on the expression of trust. These consist of factors associated with (a) the human, (b) the robot, and (c) the context in which any specific HRI event occurs. Method: The current body of literature was examined and all qualifying articles pertaining to trust in robots were included in the meta-analysis. A previous meta-analysis on HRI trust was used as the basis for this extended, updated, and evolving analysis. Results: Multiple additional factors, which have now been demonstrated to significantly influence trust, were identified. The present results, expressed as points of difference and points of commonality between the current and previous analyses, are identified, explained, and cast in the setting of the emerging wave of HRI. Conclusion: The present meta-analysis expands upon previous work and validates the overarching categories of trust antecedent (human-related, robot-related, and contextual), as well as identifying the significant individual precursors to trust within each category. A new and updated model of these complex interactions is offered. Application: The identified trust factors can be used in order to promote appropriate levels of trust in robots.},
	file         = {:Users/yosef/Downloads/HRT2020TrustMeta.pdf:pdf},
	keywords     = {human–robot interaction,meta-analysis,robotics,trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Heerink2009,
	title        = {{Measuring acceptance of an assistive social robot: A suggested toolkit}},
	author       = {Heerink, Marcel and Kr{\"{o}}se, Ben and Evers, Vanessa and Wielinga, Bob},
	year         = 2009,
	journal      = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
	number       = {November},
	pages        = {528--533},
	doi          = {10.1109/ROMAN.2009.5326320},
	isbn         = 9781424450817,
	abstract     = {The human robot interaction community is multidisciplinary by nature and has members from social science to engineering backgrounds. In this paper we aim to provide human robot developers with a straightforward toolkit to evaluate users' acceptance of assistive social robots they are designing or developing for elderly care environments. We will explain how we developed the measures for this analysis, provide do's and don'ts in designing the experiments, demonstrate the application of the measures we have developed for this purpose and the analysis and interpretation of the data. As such we hope to engage human robot interaction developers in evaluating the acceptability of their own robot to inform the development process and improve the final robot's design. {\textcopyright} 2009 IEEE.},
	file         = {:Users/yosef/Downloads/HeerinkRo-man09-2.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Hegner2019,
	title        = {{In Automatic We Trust: Investigating the Impact of Trust, Control, Personality Characteristics, and Extrinsic and Intrinsic Motivations on the Acceptance of Autonomous Vehicles}},
	author       = {Hegner, Sabrina M. and Beldad, Ardion D. and Brunswick, Gary J.},
	year         = 2019,
	journal      = {International Journal of Human-Computer Interaction},
	publisher    = {Taylor & Francis},
	volume       = 35,
	number       = 19,
	pages        = {1769--1780},
	doi          = {10.1080/10447318.2019.1572353},
	issn         = 15327590,
	abstract     = {According to industry research, the automation of vehicles promises a revolution in traffic safety, mobility, and quality of life. However, the success of such vehicles depends on their acceptance. This study investigates the influence of trust in technology, concerns of giving up control, perceived usefulness, perceived ease of use, the personality factor innovativeness, and the enjoyment of driving a car on the a priori intention to adopt an autonomous vehicle. By means of an online survey with 369 German participants, our study shows that trust in the technology and the concern about handing over control to a machine go hand in hand as respondents' cognitive and affective perception of this innovation. Moreover, perceived usefulness represents an influential factor, while the enjoyment of driving a car is a barrier to the technology's acceptance. Innovators represent a promising target for campaigns, as they are more likely to adopt an autonomous vehicle.},
	file         = {:Users/yosef/Downloads/In Automatic We Trust Investigating the Impact of Trust Control Personality Characteristics and Extrinsic and Intrinsic Motivations on the Acceptance-2.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Jøsang2007,
	title        = {{A survey of trust and reputation systems for online service provision}},
	author       = {J{\o}sang, Audun and Ismail, Roslan and Boyd, Colin},
	year         = 2007,
	journal      = {Decision Support Systems},
	volume       = 43,
	number       = 2,
	pages        = {618--644},
	doi          = {10.1016/j.dss.2005.05.019},
	issn         = {01679236},
	abstract     = {Trust and reputation systems represent a significant trend in decision support for Internet mediated service provision. The basic idea is to let parties rate each other, for example after the completion of a transaction, and use the aggregated ratings about a given party to derive a trust or reputation score, which can assist other parties in deciding whether or not to transact with that party in the future. A natural side effect is that it also provides an incentive for good behaviour, and therefore tends to have a positive effect on market quality. Reputation systems can be called collaborative sanctioning systems to reflect their collaborative nature, and are related to collaborative filtering systems. Reputation systems are already being used in successful commercial online applications. There is also a rapidly growing literature around trust and reputation systems, but unfortunately this activity is not very coherent. The purpose of this article is to give an overview of existing and proposed systems that can be used to derive measures of trust and reputation for Internet transactions, to analyse the current trends and developments in this area, and to propose a research agenda for trust and reputation systems. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0167923605000849-main.pdf:pdf},
	keywords     = {Collaboration,Decision,E-commerce,Reputation,Security,Transitivity,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Khadangi2016,
	title        = {{Biased sampling from facebook multilayer activity network using learning automata}},
	author       = {Khadangi, Ehsan and Bagheri, Alireza and Shahmohammadi, Amin},
	year         = 2016,
	journal      = {Applied Intelligence},
	publisher    = {Applied Intelligence},
	volume       = 45,
	number       = 3,
	pages        = {829--849},
	doi          = {10.1007/s10489-016-0784-0},
	issn         = 15737497,
	url          = {http://dx.doi.org/10.1007/s10489-016-0784-0},
	abstract     = {Although much research has been devoted to unbiased sampling of various networks, bias is not always disadvantageous, but sometimes useful. Especially for many real-world applications such as detecting influential nodes, spam users, and the most trustful people, it is preferred to sample users with special properties. Since sampling from friendship network alone cannot collect these important nodes appropriately, one may use interactions occurred among users. This paper deals with biased sampling of multilayer activity network. The proposed method initially learns the transition probabilities according to the considered application using learning automata. Then we sample the graph by running an application-based random walk following the learnt probabilities, in order to be guided to suitable nodes and collect their information. At last, the performance of the proposed method in terms of different applications such as fame, spam, and trust is evaluated and compared with those of common sampling algorithms. According to the experiments done, biased sampling method based on learning automata outperforms all other sampling approaches including simple random walk, Metropolis-Hastings random walk, BFS, forest fire, degree, and uniform sampling in terms of all the evaluation measures. To the best of our knowledge, our method is the first and only biased sampling method which can be used in a multilayer activity network.},
	file         = {:Users/yosef/Downloads/Khadangi2016_Article_BiasedSamplingFromFacebookMult.pdf:pdf},
	keywords     = {Activity network,Facebook,Learning automata,Network sampling,Social media marketing,Social network analysis},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Razin2021b,
	title        = {{Committing to Interdependence: Implications from Game Theory for Human–Robot Trust}},
	author       = {Yosef S. Razin and Karen M. Feigh},
	year         = 2021,
	journal      = {Paladyn, Journal of Behavioral Robotics},
	volume       = 12,
	number       = 1,
	pages        = {481--502},
	doi          = {doi:10.1515/pjbr-2021-0031},
	url          = {https://doi.org/10.1515/pjbr-2021-0031}
}
@article{Kiesler2008,
	title        = {{Anthropomorphic interactions with a robot and robot-like agent}},
	author       = {Kiesler, Sara and Powers, Aaron and Fussell, Susan R. and Torrey, Cristen},
	year         = 2008,
	journal      = {Social Cognition},
	volume       = 26,
	number       = 2,
	pages        = {169--181},
	doi          = {10.1521/soco.2008.26.2.169},
	issn         = {0278016X},
	abstract     = {People's physical embodiment and presence increase their salience and importance. We predicted people would anthropomorphize an embodied humanoid robot more than a robot-like agent, and a collocated more than a remote robot. A robot or robot-like agent interviewed participants about their health. Participants were either present with the robot/agent, or interacted remotely with the robot/agent projected life-size on a screen. Participants were more engaged, disclosed less undesirable behavior, and forgot more with the robot versus the agent. They ate less and anthropomorphized most with the collocated robot. Participants interacted socially and attempted conversational grounding with the robot/agent though aware it was a machine. Basic questions remain about how people resolve the ambiguity of interacting with a humanlike nonhuman.},
	file         = {:Users/yosef/Downloads/2008_anthro-interactions-robot.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Lee2015,
	title        = {{Reciprocity in computer-human interaction: Source-based, norm-based, and affect-based explanations}},
	author       = {Lee and Liang},
	year         = 2015,
	journal      = {Cyberpsychology, Behavior, and Social Networking},
	volume       = 18,
	number       = 4,
	pages        = {234--240},
	doi          = {10.1089/cyber.2014.0458},
	issn         = 21522723,
	abstract     = {Individuals often apply social rules when they interact with computers, and this is known as the Computers Are Social Actors (CASA) effect. Following previous work, one approach to understand the mechanism responsible for CASA is to utilize computer agents and have the agents attempt to gain human compliance (e.g., completing a pattern recognition task). The current study focuses on three key factors frequently cited to influence traditional notions of compliance: evaluations toward the source (competence and warmth), normative influence (reciprocity), and affective influence (mood). Structural equation modeling assessed the effects of these factors on human compliance with computer request. The final model shows that norm-based influence (reciprocity) increased the likelihood of compliance, while evaluations toward the computer agent did not significantly influence compliance.},
	file         = {:Users/yosef/Downloads/cyber.2014.0458.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers},
	pmid         = 25803145
}
@article{Madhavan2007,
	title        = {{Similarities and differences between human–human and human–automation trust: An integrative review}},
	author       = {Madhavan, P. and Wiegmann, D. A.},
	year         = 2007,
	journal      = {Theoretical Issues in Ergonomics Science},
	volume       = 8,
	number       = 4,
	pages        = {277--301},
	doi          = {10.1080/14639220500337708},
	issn         = {1464536X},
	keywords     = {Automation,Decision aids, Reliance,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Mann2015,
	title        = {{People respond better to robots than computer tablets delivering healthcare instructions}},
	author       = {Mann, Jordan A. and Macdonald, Bruce A. and Kuo, I. Han and Li, Xingyan and Broadbent, Elizabeth},
	year         = 2015,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier Ltd},
	volume       = 43,
	pages        = {112--117},
	doi          = {10.1016/j.chb.2014.10.029},
	issn         = {07475632},
	abstract     = {The population of the world is ageing, particularly in developed countries. As the population's age increases, the healthcare workforce is becoming progressively unable to meet the high healthcare demands of the elderly population. Increasingly, technology is being used to solve this dilemma. Using a sample from the general population (n = 65), this study examined how people interacted with either a robot or a tablet computer delivering healthcare instructions. During this interaction, the robot/tablet asked them several health-related questions, and to perform limited physical tests and a relaxation exercise. Results showed participants had more positive interactions with the robot compared to the computer tablet, including increased speech and positive emotion (smiling), and participation in the relaxation exercise. Further results showed the robot was rated higher on scales of trust, enjoyment, and desire for future interaction. This suggests that robots may offer benefits over and above computer tablets in delivering healthcare. These results further demonstrate that the physical nature of technology is important in determining responses to healthcare interactions.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563214005524-main.pdf:pdf},
	keywords     = {Attitude,Computer tablet,Healthcare behaviours,Human-robot interaction,Relaxation,Robot},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Mayer1995,
	title        = {{An Integrative Model of Organizational Trust}},
	author       = {Mayer, Roger C. and Davis, James H and Schoorman, David F},
	year         = 1995,
	journal      = {Academy of Management Review},
	volume       = 20,
	number       = 3,
	pages        = {709--734}
}
@article{McCloskey2007,
	title        = {{The importance of ease of use, usefulness, and trust to online consumers: An examination of the technology acceptance model with older consumers}},
	author       = {McCloskey, Donna Weaver},
	year         = 2007,
	journal      = {End User Computing Challenges and Technologies: Emerging Tools and Applications},
	volume       = 65,
	pages        = {259--276},
	doi          = {10.4018/978-1-59904-295-4.ch015},
	isbn         = 9781599042954,
	abstract     = {This research examines electronic commerce participation and attitudes by older Americans. Questionnaires were distributed at a large retirement community and several senior centers located in Pennsylvania. The sample of 110 respondents ranged in age from 52 to 87. Fifty-nine percent reported purchasing an item online in the last 6 months. The Technology Acceptance Model (TAM) was used and modified to examine the impact attitudes concerning ease of use, usefulness and trust had on electronic commerce usage. Usefulness and trust were found to have a positive, direct affect on usage. Ease of use had significant impacts on usefulness and trust had a significant impact on both ease of use and usefulness. The chapter concludes with a discussion of these results, study limitations, and directions for future research. {\textcopyright} 2008, IGI Global.},
	file         = {:Users/yosef/Downloads/The_Importance_of_Ease_of_UseUsefulnessandTrust-2.pdf:pdf},
	keywords     = {elderly and mature market,electronic commerce,online shopping,technology},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Merritt2011,
	title        = {{Affective processes in human-automation interactions}},
	author       = {Merritt, Stephanie M.},
	year         = 2011,
	journal      = {Human Factors},
	volume       = 53,
	number       = 4,
	pages        = {356--370},
	doi          = {10.1177/0018720811411912},
	isbn         = {0018720811},
	issn         = {00187208},
	abstract     = {Objective: This study contributes to the literature on automation reliance by illuminating the influences of user moods and emotions on reliance on automated systems. Background: Past work has focused predominantly on cognitive and attitudinal variables, such as perceived machine reliability and trust. However, recent work on human decision making suggests that affective variables (i.e., moods and emotions) are also important. Drawing from the affect infusion model, significant effects of affect are hypothesized. Furthermore, a new affectively laden attitude termed liking is introduced. Method: Participants watched video clips selected to induce positive or negative moods, then interacted with a fictitious automated system on an X-ray screening task. At five time points, important variables were assessed including trust, liking, perceived machine accuracy, user self-perceived accuracy, and reliance. These variables, along with propensity to trust machines and state affect, were integrated in a structural equation model. Results: Happiness significantly increased trust and liking for the system throughout the task. Liking was the only variable that significantly predicted reliance early in the task. Trust predicted reliance later in the task, whereas perceived machine accuracy and user self-perceived accuracy had no significant direct effects on reliance at any time. Conclusion: Affective influences on automation reliance are demonstrated, suggesting that this decision-making process may be less rational and more emotional than previously acknowledged. Application: Liking for a new system may be key to appropriate reliance, particularly early in the task. Positive affect can be easily induced and may be a lever for increasing liking. {\textcopyright} 2011, Human Factors and Ergonomics Society.},
	file         = {:Users/yosef/Downloads/0018720811411912-4.pdf:pdf},
	keywords     = {emotions,reliance,trust},
	mendeley-groups = {Merritt,Other_Trust_Metric_Papers},
	pmid         = 21901933
}
@article{Nie2012,
	title        = {{Can you hold my hand?: Physical warmth in human-robot interaction}},
	author       = {Nie, Jiaqi and Pak, Michelle and Marin, Angie Lorena and Sundar, S. Shyam},
	year         = 2012,
	journal      = {HRI'12 - Proceedings of the 7th Annual ACM/IEEE International Conference on Human-Robot Interaction},
	publisher    = {IEEE},
	pages        = {201--202},
	doi          = {10.1145/2157689.2157755},
	isbn         = 9781450310635,
	abstract     = {This study investigates whether the temperature of a robot's hand can affect perceptions of the robot as a companion. Our research empirically analyzes the responses of 39 individuals randomly assigned to one of three conditions: (1) holding a warm robot hand or (2) holding a cold robot hand or (3) not holding a robot hand. The effects of this simulated 'human touch' on HRI were examined in the context of viewing a horror film clip. Results suggest that experiences of physical warmth and handholding increase feelings of friendship and trust toward the robot. However, the discrepancy between the expectation of an actual human touch and the mechanical appearance of a robot could result in negative effects. {\textcopyright} 2012 Authors.},
	file         = {:Users/yosef/Downloads/06249527.pdf:pdf},
	keywords     = {hri,human touch,robot handholding,robot warmth},
	mendeley-groups = {Other_Trust_Metric_Papers}
}

@article{Powers2007,
	title        = {{Comparing a computer agent with a humanoid robot}},
	author       = {Powers, Aaron and Kiesler, Sara and Fussell, Susan and Torrey, Cristen},
	year         = 2007,
	journal      = {HRI 2007 - Proceedings of the 2007 ACM/IEEE Conference on Human-Robot Interaction - Robot as Team Member},
	pages        = {145--152},
	doi          = {10.1145/1228716.1228736},
	isbn         = 1595936173,
	abstract     = {HRI researchers interested in social robots have made large investments in humanoid robots. There is still sparse evidence that peoples' responses to robots differ from their responses to computer agents, suggesting that agent studies might serve to test HRI hypotheses. To help us understand the difference between people's social interactions with an agent and a robot, we experimentally compared people's responses in a health interview with (a) a computer agent projected either on a computer monitor or life-size on a screen, (b) a remote robot projected life-size on a screen, or (c) a collocated robot in the same room. We found a few behavioral and large attitude differences across these conditions. Participants forgot more and disclosed least with the collocated robot, next with the projected remote robot, and then with the agent. They spent more time with the collocated robot and their attitudes were most positive toward that robot. We discuss tradeoffs for HRI research of using collocated robots, remote robots, and computer agents as proxies of robots. Copyright 2007 ACM.},
	file         = {:Users/yosef/Downloads/1228716.1228736.pdf:pdf},
	keywords     = {Embodiment,Human-robot interaction,Social robots},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Rau2009,
	title        = {{Effects of communication style and culture on ability to accept recommendations from robots}},
	author       = {Rau, P. L.Patrick and Li, Ye and Li, Dingjun},
	year         = 2009,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier Ltd},
	volume       = 25,
	number       = 2,
	pages        = {587--595},
	doi          = {10.1016/j.chb.2008.12.025},
	issn         = {07475632},
	abstract     = {The objective of this paper is to investigate the effects of communication styles and culture on people's accepting recommendations from robots. The goal was to provide insight for culturally adaptive robot design. The independent variables were communication style (i.e. implicit or explicit), the participants' cultural background (i.e. Chinese or German), and the robot's language (i.e. native language and English for Chinese and German subjects). A laboratory experiment was conducted with 16 Chinese and 16 German college students. Basic descriptive statistics and t-test are used for biographical information analysis; reliability test is used for questionnaire; MANOVA and non-parametric test are used for testing the hypotheses. The results showed that the Chinese participants preferred an implicit communication style than German participants. Chinese participants evaluated the robots as being more likable, trustworthy, and credible, and were more likely to accept the implicit recommendations. The German participants evaluated the robots as being less likable, trustworthy, and credible, and were less inclined to accept implicit recommendations. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563208002367-main.pdf:pdf},
	keywords     = {Communication style,Cultural differences,Human-robot interaction,Robot language},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@book{Rocha2013,
	title        = {{Advances in Intelligent Systems and Computing: Preface}},
	author       = {Rocha, {\'{A}}lvaro and Correia, Ana Maria and Wilson, Tom and Stroetmann, Karl A.},
	year         = 2013,
	booktitle    = {Advances in Intelligent Systems and Computing},
	volume       = {206 AISC},
	doi          = {{10.1007/978-3-642-36981-0}},
	isbn         = 9783642369803,
	issn         = 21945357,
	file         = {:Users/yosef/Downloads/Trends_PAAMS_2014-2.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Rupp2016,
	title        = {{The impact of technological trust and self-determined motivation on intentions to use wearable fitness technology}},
	author       = {Rupp, Michael A. and Michaelis, Jessica R. and McConnell, Daniel S. and Smither, Janan A.},
	year         = 2016,
	journal      = {Proceedings of the Human Factors and Ergonomics Society},
	pages        = {1433--1437},
	doi          = {10.1177/1541931213601329},
	issn         = 10711813,
	abstract     = {Exercise has many health benefits, however, not enough Americans engage in physical activity which may be due to a lack of motivation and knowledge of how to do so. Wearable technologies, such as fitness trackers, can ameliorate this pervasive problem. The aim of this study was to examine trust and motivation as predictors of fitness tracker usage across several devices. Moreover, we developed trust and motivation scales specific to wearable fitness devices to determine the extent to which these factors may relate to individuals' desire for continued use. A Confirmatory factor analysis and Structural equation model analysis was conducted on the data. Our results indicated that: 1) both technological trust and self-determined motivation are significantly related to each other and 2) both users' perceived trust and motivation of the device are predictive of individuals' desire for continued use of wearable fitness devices. Therefore, considerations of how the technology can be transparent to the user and provide motivational support are important design considerations.},
	file         = {:Users/yosef/Downloads/ProceedingsoftheHumanFactorsandErgonomicsSocietyAnnualMeeting-2016-Rupp-1434-8.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Scopelliti2005,
	title        = {{Robots in a domestic setting: A psychological approach}},
	author       = {Scopelliti, Massimiliano and Giuliani, Maria Vittoria and Fornara, Ferdinando},
	year         = 2005,
	journal      = {Universal Access in the Information Society},
	volume       = 4,
	number       = 2,
	pages        = {146--155},
	doi          = {10.1007/s10209-005-0118-1},
	issn         = 16155289,
	abstract     = {The study presented in this paper aims at improving the current understanding of human-robot interaction by adopting a psychological approach. The acceptability of robotic devices in home settings, especially by elderly people, does not depend only on the practical benefits they can provide, but on complex relationships between the cognitive, affective and emotional components of people's images of robots. This study has investigated the main dimensions of these representations, by comparing the attitudes towards technology in general, and domestic robots in particular, held by people at different stages of the lifespan. The results confirm that age is a critical variable. {\textcopyright} Springer-Verlag 2005.},
	file         = {:Users/yosef/Downloads/Scopelliti2005_Article_RobotsInADomesticSettingAPsych.pdf:pdf},
	keywords     = {Affective response,Cognitive representation,Human-robot interaction,Lifespan},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Skarlatidou2013,
	title        = {{Guidelines for trust interface design for public engagement Web GIS}},
	author       = {Skarlatidou, Artemis and Cheng, Tao and Haklay, Muki},
	year         = 2013,
	journal      = {International Journal of Geographical Information Science},
	volume       = 27,
	number       = 8,
	pages        = {1668--1687},
	doi          = {10.1080/13658816.2013.766336},
	issn         = 13658816,
	abstract     = {Attesting to the powerful capabilities and in technology trends, many scholars envisioned the consolidation of geographic information systems (GIS) into vital tools for disseminating spatial information. GIS are presently used to inform, advise and instruct users in several contexts and to further engage citizens in decision-making processes that can impact and sustain policy development. Interaction with these applications incorporates risk and uncertainty, which have been repeatedly identified as preconditions in nurturing trust perceptions and which instigate a user's decision to rely on a system and act on the provided information. Research studies consistently demonstrated that a trust-oriented interface design can facilitate the development of more trustworthy, mainly e-commerce, systems. Trust in the Web GIS context, despite its significance, has only relatively recently received some attention. A set of human-computer interaction (HCI) user-based studies revealed some Web GIS trustee attributes that influence non-experts' trust beliefs and found that when these are problematic or absent from interface design, users form irrational trust perceptions, which amplifies the risk and may impose dangers to the user. These Web GIS trustee attributes that influence non-experts' trust perceptions are formulated here into a set of trust guidelines. These are then evaluated using the PE-Nuclear tool, a Web GIS application, to inform the public about the site selection of a nuclear waste repository in the United Kingdom. Our preliminary results indicate that the proposed trust guidelines not only support the development of rational trust perceptions that protect non-experts from inappropriate use of Web GIS technology but also contribute towards improving interaction with such applications of public interest issue. {\textcopyright} 2013 Taylor and Francis Group, LLC.},
	file         = {:Users/yosef/Downloads/Guidelines for trust interface design for public engagement Web GIS-3.pdf:pdf},
	keywords     = {GIS,human-computer interaction,interface design,public engagement,trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Sollner2013,
	title        = {{Understanding Trust in IT Artifacts - A New Conceptual Approach}},
	author       = {Sollner, M. and Pavlou, P. and Leimeister, J. M.},
	year         = 2013,
	journal      = {Academy of Management Proceedings},
	volume       = 2013,
	number       = 1,
	pages        = {11412--11412},
	doi          = {10.5465/ambpp.2013.11412abstract},
	issn         = {0065-0668},
	abstract     = {To add value to companies, IT artifacts – such as information systems – need to be adopted and\r\nused. Research and practice have shown that designing IT artifacts in a way that they are readily\r\nadopted and used is not trivial. To support designers, research has identified a plethora of factors\r\ndriving the adoption and use of IT artifacts, with trust being one of the most important factors.\r\nDespite this knowledge, research on trust in IT artifact struggles to leverage its potential for IT\r\nartifact design, due to several disagreements among scholars. The goal of our paper is to present\r\nand reconcile the different competing arguments, and to provide a new conceptual approach to\r\nstudy trust in IT artifacts. The core argument of our approach is that trust is a suitable concept for\r\nstudying relationships between humans and IT artifact, but trust in an IT artifact should not be\r\nstudied without examining trust in the provider of the IT artifact. Whereas interpersonal trust\r\ntheory is suitable to assess trust in the provider of the IT artifact, we propose a new\r\nconceptualization for trust in the IT artifact itself. Separately investigating trust in the provider of\r\nthe IT artifact and trust in the IT artifact itself, will allow researchers to gather a deeper\r\nunderstanding of the nature of trust in IT artifacts and how it can be built. This knowledge will\r\nsupport designers in designing IT artifacts that are more readily adopted and used, and thus can\r\nprovide the desired value to companies.},
	file         = {:Users/yosef/Downloads/JML_474.pdf:pdf},
	keywords     = {UNDERSTANDING TRUST IN IT ARTIFACTS – A NEW CONCEP},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{sollner2016longitudinal,
	title        = {{A longitudinal perspective on trust in IT artefacts}},
	author       = {S{\"{o}}llner, Matthias and Pavlou, Paul A},
	year         = 2016,
	journal      = {European Conference on Information Systems (ECIS)},
	address      = {Istanbul}
}
@article{Sollner2012,
	title        = {{U Nderstanding the F Ormation of T Rust in It}},
	author       = {S{\"{o}}llner, Matthias and Hoffmann, Axel and Leimeister, Jan Marco},
	year         = 2012,
	journal      = {33rd Intl. Conf. on Information Systems},
	volume       = 127,
	number       = {June},
	pages        = {1--18},
	file         = {:Users/yosef/Downloads/301364478.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Tussyadiah2020,
	title        = {{Do travelers trust intelligent service robots?}},
	author       = {Tussyadiah, Iis P. and Zach, Florian J. and Wang, Jianxi},
	year         = 2020,
	journal      = {Annals of Tourism Research},
	publisher    = {Elsevier},
	volume       = 81,
	number       = {June 2018},
	pages        = 102886,
	doi          = {10.1016/j.annals.2020.102886},
	issn         = {01607383},
	file         = {:Users/yosef/Downloads/1-s2.0-S016073832030030X-main-2.pdf:pdf},
	keywords     = {Driverless car,Initial trust formation,Intelligent robot,Robot bartender,Self-driving vehicle,Service robot},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Ullman2014,
	title        = {{Proceedings of the Annual Meeting of the Cognitive Science Smart Human , Smarter Robot : How Cheating Affects Perceptions of Social Agency}},
	author       = {Ullman, Daniel and  Leite,Iolanda and Phillips,Jonathan  and Kim-Cohen,Julia  and Scassellati, Brian},
	year         = 2014,
	journal      = {Proceedings of the Annual Meeting of the Cognitive Science Society},
	volume       = 36,
	number       = 36,
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{VanDerLaan1997,
	title        = {{A simple procedure for the assessment of acceptance of advanced transport telematics}},
	author       = {{Van Der Laan}, Jinke D. and Heino, Adriaan and {De Waard}, Dick},
	year         = 1997,
	journal      = {Transportation Research Part C: Emerging Technologies},
	volume       = 5,
	number       = 1,
	pages        = {1--10},
	doi          = {10.1016/S0968-090X(96)00025-3},
	issn         = {0968090X},
	abstract     = {There is no standard way of measuring driver acceptance of new technology. A review of the literature shows that there are almost as many methods of assessment of acceptance as there are acceptance studies. The tool for studying acceptance of new technological equipment that is presented here has a major advantage compared with many other studies in that esoteric knowledge of scaling techniques is not required. The technique is simple and consists of nine 5-point rating-scale items. These items load on two scales, a scale denoting the usefulness of the system, and a scale designating satisfaction. The technique has been applied in six different studies in different test environments and analyses performed over these studies show that it is a reliable instrument for the assessment of acceptance of new technology. The technique was sensitive to differences in opinion to specific aspects of in-vehicle systems, as well as to differences in opinion between driver groups. In a concluding section explicit recommendations for use of the scale are given. {\textcopyright} 1997 Elsevier Science Ltd.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0968090X96000253-main.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bata2018,
	title        = {{Mobile social networking and salesperson maladaptive dependence behaviors}},
	author       = {Bata, Hatem and Pentina, Iryna and Tarafdar, Monideepa and Pullins, Ellen Bolman},
	year         = 2018,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier Ltd},
	volume       = 81,
	pages        = {235--249},
	doi          = {10.1016/j.chb.2017.12.025},
	issn         = {07475632},
	url          = {https://doi.org/10.1016/j.chb.2017.12.025},
	abstract     = {This study investigates technology dependence associated with the work-related use of mobile social networking (MSN) by salespeople. A scale for maladaptive technology dependence behaviors (MTDB) is developed and empirically validated using survey data from 242 mid-level sales managers in the US. Personal and job-related antecedents, as well as consequences of MTDB for sales outcomes, are also examined. Results suggest that emotional attachment to MSN and perceptions of its greater affordances for task accomplishment may lead to maladaptive behaviors of overreliance on MSN for job completion, blind trust, cognitive absorption and dysfunctional use. These associations increase in organizations with competitive psychological climate. Findings also show that using MSN for prospecting does not lead to maladaptive dependence, as opposed to using it for customer relationship maintenance. Salespeople using MSN for relationship maintenance exhibit more maladaptive behaviors if they experience work-related role stress. Finally, salespeople who exhibit MTDB are less likely to complete their assignments and participate in teamwork. These findings provide tools for organizations to develop technology use policies, design sales training, and enhance the work environment. Future studies can examine dependencies on others types of technologies (CRM, marketing automation, etc.), and in other contexts (online retailing, social media analytics, etc.)},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563217307069-main-2.pdf:pdf},
	keywords     = {Dark side of social networking,Maladaptive technology dependence,Mobile social media,Professional sales,Technology addiction},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Alexander2018,
	title        = {{Why trust an algorithm? Performance, cognition, and neurophysiology}},
	author       = {Alexander, Veronika and Blinder, Collin and Zak, Paul J.},
	year         = 2018,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier},
	volume       = 89,
	number       = {July},
	pages        = {279--288},
	doi          = {10.1016/j.chb.2018.07.026},
	issn         = {07475632},
	url          = {https://doi.org/10.1016/j.chb.2018.07.026},
	abstract     = {OBJECTIVE: We measured neurophysiologic responses and task performance while participants solved mazes after choosing whether to adopt an imperfect helper algorithm. BACKGROUND: Every day we must decide whether to trust or distrust algorithms. Will an algorithm improve our performance on a task? What if we trust it too much? METHOD: Participants had to pay to use the algorithm and were aware that it offered imperfect help. We varied the information about the algorithm to assess the factors that affected adoption while measuring participants' peripheral neurophysiology. RESULTS: We found that information about previous adoption by others had a larger effect on adoption and resulted in lower cognitive load than did information about algorithm accuracy. The neurophysiologic measurement showed that algorithm adoption without any information resulted in low cognitive engagement during the task and impaired task performance. Conversely, algorithm use after information about others' use improved engagement and performance. CONCLUSION: By objectively measuring cognitive load and task performance, we identified how to increase algorithm adoption while sustaining high performance by human operators. APPLICATION: Algorithm adoption can be increased by sharing previous use information and performance improved by providing a reason to monitor the algorithm. Precis: We collected neurophysiologic data while varying information about an algorithm that assisted participants in solving a timed and incentivized maze and found that information about prior use by others more effectively influenced adoption, reduced cognitive load, and improved performance compared to algorithm accuracy information.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563218303480-main.pdf:pdf},
	keywords     = {Automation,Computers,Decisions,Neurophysiology},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Charalambous2016,
	title        = {{The Development of a Scale to Evaluate Trust in Industrial Human-robot Collaboration}},
	author       = {Charalambous, George and Fletcher, Sarah and Webb, Philip},
	year         = 2016,
	journal      = {International Journal of Social Robotics},
	publisher    = {Springer Netherlands},
	volume       = 8,
	number       = 2,
	pages        = {193--209},
	doi          = {10.1007/s12369-015-0333-8},
	issn         = 18754805,
	abstract     = {Trust has been identified as a key element for the successful cooperation between humans and robots. However, little research has been directed at understanding trust development in industrial human-robot collaboration (HRC). With industrial robots becoming increasingly integrated into production lines as a means for enhancing productivity and quality, it will not be long before close proximity industrial HRC becomes a viable concept. Since trust is a multidimensional construct and heavily dependent on the context, it is vital to understand how trust develops when shop floor workers interact with industrial robots. To this end, in this study a trust measurement scale suitable for industrial HRC was developed in two phases. In phase one, an exploratory study was conducted to collect participants' opinions qualitatively. This led to the identification of trust related themes relevant to the industrial context and a related pool of questionnaire items was generated. In the second phase, three human-robot trials were carried out in which the questionnaire items were applied to participants using three different types of industrial robots. The results were statistically analysed to identify the key factors impacting trust and from these generate a trust measurement scale for industrial HRC.},
	file         = {:Users/yosef/Downloads/Charalambous2016_Article_TheDevelopmentOfAScaleToEvalua.pdf:pdf},
	keywords     = {Human-robot collaboration,Industrial robot,Trust scale},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Ekman2019,
	title        = {{Exploring automated vehicle driving styles as a source of trust information}},
	author       = {Ekman, Fredrick and Johansson, Mikael and Blig{\aa}rd, Lars Ola and Karlsson, Mari Anne and Str{\"{o}}mberg, Helena},
	year         = 2019,
	journal      = {Transportation Research Part F: Traffic Psychology and Behaviour},
	publisher    = {Elsevier Ltd},
	volume       = 65,
	pages        = {268--279},
	doi          = {10.1016/j.trf.2019.07.026},
	issn         = 13698478,
	url          = {https://doi.org/10.1016/j.trf.2019.07.026},
	abstract     = {Trust is important for users' acceptance and adoption of automated vehicles (AVs). Previous research has mainly focused on what information affects user trust in AVs and how the user's trust is affected by the way the content is communicated. However, recent studies have shown that trust may also be affected by the AV's driving style. The aim of the study was to further investigate if and how the vehicle's driving style affects user trust in AVs and, in particular, how this is expressed by users. An experiment involving 18 participants, using a Wizard of Oz setup and within a subject design, was conducted comparing two different driving styles, ‘Defensive' and ‘Aggressive'. Trust was measured using a mixed method research design including momentaneous trust ratings and think-aloud procedures while driving, a post-run trust questionnaire as well as trust curve sketching and a personal interview. The results show that driving style had an effect on user trust and that the ‘Defensive' driving style was perceived as more trustworthy, in part because it was deemed more predictable than the ‘Aggressive' driving style. Furthermore, participants expressed trust in using affective, analogical and analytic responses, the two former during the test runs and the latter directly after each test run. The interview after the completion produced a more mixed result. By combining different data collection methods, a nuanced picture of the trust formation process and users' trust in AVs was obtained. The study concludes that it is important to consider the vehicle performance information provided by the vehicle's driving style so as to create user trust in AVs.},
	file         = {:Users/yosef/Downloads/1-s2.0-S1369847818306594-main.pdf:pdf},
	keywords     = {Automated vehicles,Driving style,Multi-methods approach,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{McCloskey2007,
	title        = {{The importance of ease of use, usefulness, and trust to online consumers: An examination of the technology acceptance model with older consumers}},
	author       = {McCloskey, Donna Weaver},
	year         = 2007,
	journal      = {End User Computing Challenges and Technologies: Emerging Tools and Applications},
	volume       = 65,
	pages        = {259--276},
	doi          = {10.4018/978-1-59904-295-4.ch015},
	isbn         = 9781599042954,
	abstract     = {This research examines electronic commerce participation and attitudes by older Americans. Questionnaires were distributed at a large retirement community and several senior centers located in Pennsylvania. The sample of 110 respondents ranged in age from 52 to 87. Fifty-nine percent reported purchasing an item online in the last 6 months. The Technology Acceptance Model (TAM) was used and modified to examine the impact attitudes concerning ease of use, usefulness and trust had on electronic commerce usage. Usefulness and trust were found to have a positive, direct affect on usage. Ease of use had significant impacts on usefulness and trust had a significant impact on both ease of use and usefulness. The chapter concludes with a discussion of these results, study limitations, and directions for future research. {\textcopyright} 2008, IGI Global.},
	file         = {:Users/yosef/Downloads/The_Importance_of_Ease_of_UseUsefulnessandTrust-2.pdf:pdf},
	keywords     = {elderly and mature market,electronic commerce,online shopping,technology},
	mendeley-groups = {Other_Trust_Metric_Papers}
}

@inproceedings{Weitz2019,
  title={" Do you trust me?" Increasing user-trust by integrating virtual agents in explainable AI interaction design},
  author={Weitz, Katharina and Schiller, Dominik and Schlagowski, Ruben and Huber, Tobias and Andr{\'e}, Elisabeth},
  booktitle={Proceedings of the 19th ACM international conference on intelligent virtual agents},
  pages={7--9},
  year={2019}
}

@article{deVisser2016almost,
	title        = {{Almost Human: Anthropomorphism Increases Trust Resilience in Cognitive Agents}},
	author       = {de Visser, Ewart and Monfort, Samuel and Mckendrick, Ryan and Smith, Melissa and Mcknight, Patrick and Krueger, Frank and Parasuraman, Raja},
	year         = 2016,
	journal      = {Journal of Experimental Psychology: Applied},
	volume       = 22,
	doi          = {10.1037/xap0000092},
	mendeley-groups = {Trust_Metrics_Analysis}
}
@article{Wojton2020,
	title        = {{Initial validation of the trust of automated systems test (TOAST)}},
	author       = {Wojton, Heather M. and Porter, Daniel and {T. Lane}, Stephanie and Bieber, Chad and Madhavan, Poornima},
	year         = 2020,
	journal      = {Journal of Social Psychology},
	volume       = 160,
	number       = 6,
	pages        = {735--750},
	doi          = {10.1080/00224545.2020.1749020},
	issn         = 19401183,
	abstract     = {Trust is a key determinant of whether people rely on automated systems in the military and the public. However, there is currently no standard for measuring trust in automated systems. In the present studies, we propose a scale to measure trust in automated systems that is grounded in current research and theory on trust formation, which we refer to as the Trust in Automated Systems Test (TOAST). We evaluated both the reliability of the scale structure and criterion validity using independent, military-affiliated and civilian samples. In both studies we found that the TOAST exhibited a two-factor structure, measuring system understanding and performance (respectively), and that factor scores significantly predicted scores on theoretically related constructs demonstrating clear criterion validity. We discuss the implications of our findings for advancing the empirical literature and in improving interface design.},
	file         = {:Users/yosef/Downloads/WojtonPorterLaneBieberMadhavan2020.pdf:pdf},
	keywords     = {Confirmatory Factor Analysis,Human-Machine Trust,Trust Scale,Trust in Automation},
	mendeley-groups = {Other_Trust_Metric_Papers},
	pmid         = 32297844
}
@article{Wang2005,
	title        = {{An overview of online trust: Concepts, elements, and implications}},
	author       = {Wang and Emurian},
	year         = 2005,
	journal      = {Computers in Human Behavior},
	volume       = 21,
	number       = 1,
	pages        = {105--125},
	doi          = {10.1016/j.chb.2003.11.008},
	issn         = {07475632},
	abstract     = {Lack of trust has been repeatedly identified as one of the most formidable barriers to people for engaging in e-commerce, involving transactions in which financial and personal information is submitted to merchants via the Internet. The future of e-commerce is tenuous without a general climate of online trust. Building consumer trust on the Internet presents a challenge for online merchants and is a research topic of increasing interest and importance. This paper provides an overview of the nature and concepts of trust from multi-disciplinary perspectives, and it reviews relevant studies that investigate the elements of online trust. Also, a framework of trust-inducing interface design features articulated from the existing literature is presented. The design features were classified into four dimensions, namely (1) graphic design, (2) structure design, (3) content design, and (4) social-cue design. By applying the design features identified within this framework to e-commerce web site interfaces, online merchants might then anticipate fostering optimal levels of trust in their customers. {\textcopyright} 2003 Elsevier Ltd. All rights reserved.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563203001092-main.pdf:pdf},
	keywords     = {E-commerce,Interface design,Online trust,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Kiesler2008,
	title        = {{Anthropomorphic interactions with a robot and robot-like agent}},
	author       = {Kiesler, Sara and Powers, Aaron and Fussell, Susan R. and Torrey, Cristen},
	year         = 2008,
	journal      = {Social Cognition},
	volume       = 26,
	number       = 2,
	pages        = {169--181},
	doi          = {10.1521/soco.2008.26.2.169},
	issn         = {0278016X},
	abstract     = {People's physical embodiment and presence increase their salience and importance. We predicted people would anthropomorphize an embodied humanoid robot more than a robot-like agent, and a collocated more than a remote robot. A robot or robot-like agent interviewed participants about their health. Participants were either present with the robot/agent, or interacted remotely with the robot/agent projected life-size on a screen. Participants were more engaged, disclosed less undesirable behavior, and forgot more with the robot versus the agent. They ate less and anthropomorphized most with the collocated robot. Participants interacted socially and attempted conversational grounding with the robot/agent though aware it was a machine. Basic questions remain about how people resolve the ambiguity of interacting with a humanlike nonhuman.},
	file         = {:Users/yosef/Downloads/2008_anthro-interactions-robot.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Jøsang2007,
	title        = {{A survey of trust and reputation systems for online service provision}},
	author       = {J{\o}sang, Audun and Ismail, Roslan and Boyd, Colin},
	year         = 2007,
	journal      = {Decision Support Systems},
	volume       = 43,
	number       = 2,
	pages        = {618--644},
	doi          = {10.1016/j.dss.2005.05.019},
	issn         = {01679236},
	abstract     = {Trust and reputation systems represent a significant trend in decision support for Internet mediated service provision. The basic idea is to let parties rate each other, for example after the completion of a transaction, and use the aggregated ratings about a given party to derive a trust or reputation score, which can assist other parties in deciding whether or not to transact with that party in the future. A natural side effect is that it also provides an incentive for good behaviour, and therefore tends to have a positive effect on market quality. Reputation systems can be called collaborative sanctioning systems to reflect their collaborative nature, and are related to collaborative filtering systems. Reputation systems are already being used in successful commercial online applications. There is also a rapidly growing literature around trust and reputation systems, but unfortunately this activity is not very coherent. The purpose of this article is to give an overview of existing and proposed systems that can be used to derive measures of trust and reputation for Internet transactions, to analyse the current trends and developments in this area, and to propose a research agenda for trust and reputation systems. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0167923605000849-main.pdf:pdf},
	keywords     = {Collaboration,Decision,E-commerce,Reputation,Security,Transitivity,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bailey2007,
	title        = {{Automation-induced complacency for monitoring highly reliable systems: The role of task complexity, system experience, and operator trust}},
	author       = {Bailey, N. R. and Scerbo, M. W.},
	year         = 2007,
	journal      = {Theoretical Issues in Ergonomics Science},
	volume       = 8,
	number       = 4,
	pages        = {321--348},
	doi          = {10.1080/14639220500535301},
	issn         = {1464536X},
	abstract     = {The increase in quantity and complexity of advanced automated systems has generated new concerns surrounding automation-induced complacency, or the difficulties operators have monitoring the status of automated systems. The present investigation consists of two studies that assessed the impact of system reliability, monitoring complexity, operator trust, and system experience on automation-induced complacency. In both studies, participants operated a manually controlled flight task while monitoring several simulated aircraft displays for failures. The ability of operators to detect a single automation failure over three experimental sessions was also assessed. Results indicated that realistic levels of system reliability severely impaired an operator's ability to monitor effectively. Further, as system experience increased, operator monitoring performance declined. The results also indicated that the complexity of the monitoring task heavily influenced operator monitoring, with poorer performance associated with more cognitively demanding tasks. Finally, results from both studies indicated that operator trust increased and monitoring performance decreased as a function of increasing system reliability. These results suggest that for highly reliable systems, increasing task complexity and extensive experience may severely impair an operator's ability to monitor for unanticipated system states. {\textcopyright} 2007 Taylor & Francis.},
	file         = {:Users/yosef/Downloads/Automation induced complacency for monitoring highly reliable systems the role of task complexity system experience and operator trust.pdf:pdf},
	keywords     = {Attention,Complacency,Monitoring,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Daniel2013,
	title        = {{Simplified Human-Robot Interaction: Modeling and Evaluation}},
	author       = {Daniel, Balazs and Thomessen, Trygve and Korondi, Peter},
	year         = 2013,
	journal      = {Modeling, Identification and Control: A Norwegian Research Bulletin},
	volume       = 34,
	number       = 4,
	pages        = {199--211},
	doi          = {10.4173/mic.2013.4.4},
	issn         = {0332-7353},
	abstract     = {In this paper a novel concept of human-robot interaction (HRI) modeling is proposed. Including factors like trust in automation, situational awareness, expertise and expectations a new user experience framework is formed for industrial robots. Service Oriented Robot Operation, proposed in a previous paper, creates an abstract level in HRI and it is also included in the framework. This concept is evaluated with exhaustive tests. Results prove that significant improvement in task execution may be achieved and the new system is more usable for operators with less experience with robotics; personnel specific for small and medium enterprises (SMEs).},
	file         = {:Users/yosef/Downloads/MIC-2013-4-4.pdf:pdf},
	keywords     = {graphical user interface,human-robot interaction,industrial robotics,usability},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Nie2012,
	title        = {{Can you hold my hand?: Physical warmth in human-robot interaction}},
	author       = {Nie, Jiaqi and Pak, Michelle and Marin, Angie Lorena and Sundar, S. Shyam},
	year         = 2012,
	journal      = {HRI'12 - Proceedings of the 7th Annual ACM/IEEE International Conference on Human-Robot Interaction},
	publisher    = {IEEE},
	pages        = {201--202},
	doi          = {10.1145/2157689.2157755},
	isbn         = 9781450310635,
	abstract     = {This study investigates whether the temperature of a robot's hand can affect perceptions of the robot as a companion. Our research empirically analyzes the responses of 39 individuals randomly assigned to one of three conditions: (1) holding a warm robot hand or (2) holding a cold robot hand or (3) not holding a robot hand. The effects of this simulated 'human touch' on HRI were examined in the context of viewing a horror film clip. Results suggest that experiences of physical warmth and handholding increase feelings of friendship and trust toward the robot. However, the discrepancy between the expectation of an actual human touch and the mechanical appearance of a robot could result in negative effects. {\textcopyright} 2012 Authors.},
	file         = {:Users/yosef/Downloads/06249527.pdf:pdf},
	keywords     = {hri,human touch,robot handholding,robot warmth},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Ghazizadeh2012,
	title        = {{Extending the Technology Acceptance Model to assess automation}},
	author       = {Ghazizadeh, Mahtab and Lee, John D. and Boyle, Linda Ng},
	year         = 2012,
	journal      = {Cognition, Technology and Work},
	volume       = 14,
	number       = 1,
	pages        = {39--49},
	doi          = {10.1007/s10111-011-0194-3},
	issn         = 14355558,
	abstract     = {Often joint human-automation performance depends on the factors influencing the operator's tendency to rely on and comply with automation. Although cognitive engineering (CE) researchers have studied automation acceptance as related to task-technology compatibility and human-technology coagency, information system (IS) researchers have evaluated user acceptance of technology, using the Technology Acceptance Model (TAM). The parallels between the two views suggest that the user acceptance perspective from the IS community can complement the human-automation interaction perspective from the CE community. TAM defines constructs that govern acceptance and provides a framework for evaluating a broad range of factors influencing technology acceptance and reliance. TAM is extensively used by IS researchers in various applications and it can be applied to assess the effect of trust and other factors on automation acceptance. Likewise, extensions to the TAM framework use the constructs of task-technology compatibility and past experience to extend its description of the role of human-automation interaction in automation adoption. We propose the Automation Acceptance Model (AAM) to draw upon the IS and CE perspectives and take into account the dynamic and multi-level nature of automation use, highlighting the influence of use on attitudes that complements the more common view that attitudes influence use. {\textcopyright} 2011 Springer-Verlag London Limited.},
	file         = {:Users/yosef/Downloads/Ghazizadeh2012_Article_ExtendingTheTechnologyAcceptan.pdf:pdf},
	keywords     = {Automation Acceptance Model (AAM),Cognitive engineering,Task-technology compatibility,Technology Acceptance Model (TAM),Technology acceptance,Trust in automation},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@book{Rocha2013,
	title        = {{Advances in Intelligent Systems and Computing: Preface}},
	author       = {Rocha, {\'{A}}lvaro and Correia, Ana Maria and Wilson, Tom and Stroetmann, Karl A.},
	year         = 2013,
	booktitle    = {Advances in Intelligent Systems and Computing},
	volume       = {206 AISC},
	doi          = {10.1007/978-3-642-36981-0},
	isbn         = 9783642369803,
	issn         = 21945357,
	file         = {:Users/yosef/Downloads/Trends_PAAMS_2014-2.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Wechsung2013,
	title        = {{Development and validation of the conversational agents scale (cas)}},
	author       = {Wechsung, Ina and Weiss, Benjamin and K{\"{u}}hnel, Christine and Ehrenbrink, Patrick and M{\"{o}}ller, Sebastian},
	year         = 2013,
	journal      = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	number       = {August},
	pages        = {1106--1110},
	issn         = 19909772,
	abstract     = {The gulf between user and system can be minimized by adapting the system to the user's natural characteristics. Socalled anthropomorphic interfaces represent one strategy of such an adaption as they are assumed to provide a more human-like and therefore more natural interaction. However, regarding the evaluation of anthropomorphic interfaces, the well-known and empirically tested instruments are limited to educational contexts. Hence, this paper describes the first steps towards the development of an evaluation instrument applicable to a wide range of such interfaces. Copyright {\textcopyright} 2013 ISCA.},
	file         = {:Users/yosef/Downloads/i13_1106.pdf:pdf},
	keywords     = {Anthropomorphic interfaces,Evaluation,Usercentered design},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Skarlatidou2013a,
	title        = {{Guidelines for trust interface design for public engagement Web GIS}},
	author       = {Skarlatidou, Artemis and Cheng, Tao and Haklay, Muki},
	year         = 2013,
	journal      = {International Journal of Geographical Information Science},
	volume       = 27,
	number       = 8,
	pages        = {1668--1687},
	doi          = {10.1080/13658816.2013.766336},
	issn         = 13658816,
	abstract     = {Attesting to the powerful capabilities and in technology trends, many scholars envisioned the consolidation of geographic information systems (GIS) into vital tools for disseminating spatial information. GIS are presently used to inform, advise and instruct users in several contexts and to further engage citizens in decision-making processes that can impact and sustain policy development. Interaction with these applications incorporates risk and uncertainty, which have been repeatedly identified as preconditions in nurturing trust perceptions and which instigate a user's decision to rely on a system and act on the provided information. Research studies consistently demonstrated that a trust-oriented interface design can facilitate the development of more trustworthy, mainly e-commerce, systems. Trust in the Web GIS context, despite its significance, has only relatively recently received some attention. A set of human-computer interaction (HCI) user-based studies revealed some Web GIS trustee attributes that influence non-experts' trust beliefs and found that when these are problematic or absent from interface design, users form irrational trust perceptions, which amplifies the risk and may impose dangers to the user. These Web GIS trustee attributes that influence non-experts' trust perceptions are formulated here into a set of trust guidelines. These are then evaluated using the PE-Nuclear tool, a Web GIS application, to inform the public about the site selection of a nuclear waste repository in the United Kingdom. Our preliminary results indicate that the proposed trust guidelines not only support the development of rational trust perceptions that protect non-experts from inappropriate use of Web GIS technology but also contribute towards improving interaction with such applications of public interest issue. {\textcopyright} 2013 Taylor and Francis Group, LLC.},
	file         = {:Users/yosef/Downloads/Guidelines for trust interface design for public engagement Web GIS-3.pdf:pdf},
	keywords     = {GIS,human-computer interaction,interface design,public engagement,trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bass2013,
	title        = {{The effect of information analysis automation display content on human judgment performance in noisy environments}},
	author       = {Bass, Ellen J. and Baumgart, Leigh A. and Shepley, Kathryn Klein},
	year         = 2013,
	journal      = {Journal of Cognitive Engineering and Decision Making},
	volume       = 7,
	number       = 1,
	pages        = {49--65},
	doi          = {10.1177/1555343412453461},
	issn         = 21695032,
	abstract     = {Displaying both the strategy that information analysis automation employs to makes its judgments and variability in the task environment may improve human judgment performance, especially in cases where this variability impacts the judgment performance of the information analysis automation. This work investigated the contribution of providing either information analysis automation strategy information, task environment information, or both, on human judgment performance in a domain where noisy sensor data are used by both the human and the information analysis automation to make judgments. In a simplified air traffic conflict prediction experiment, 32 participants made probability of horizontal conflict judgments under different display content conditions. After being exposed to the information analysis automation, judgment achievement significantly improved for all participants as compared to judgments without any of the automation's information. Participants provided with additional display content pertaining to cue variability in the task environment had significantly higher aided judgment achievement compared to those provided with only the automation's judgment of a probability of conflict. When designing information analysis automation for environments where the automation's judgment achievement is impacted by noisy environmental data, it may be beneficial to show additional task environment information to the human judge in order to improve judgment performance. Copyright {\textcopyright} 2012, Human Factors and Ergonomics Society.},
	file         = {:Users/yosef/Downloads/1555343412453461.pdf:pdf},
	keywords     = {Automation display content,Human-automated judge learning,Human-automation interaction,Judgment analysis},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@inproceedings{evers2008relational,
	title        = {Relational vs. group self-construal: Untangling the role of national culture in HRI},
	author       = {Evers, Vanessa and Maldonado, Heidy and Brodecki, Talia and Hinds, Pamela},
	year         = 2008,
	booktitle    = {2008 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
	pages        = {255--262},
	organization = {IEEE}
}
@article{lewis1985trust,
	title        = {Trust as a social reality},
	author       = {Lewis, J David and Weigert, Andrew},
	year         = 1985,
	journal      = {Social forces},
	publisher    = {Oxford University Press},
	volume       = 63,
	number       = 4,
	pages        = {967--985}
}
@article{mccomb2017evaluation,
	title        = {An evaluation of shared mental models and mutual trust on general medical units: Implications for collaboration, teamwork, and patient safety},
	author       = {McComb, Sara A and Lemaster, Matthew and Henneman, Elizabeth A and Hinchey, Kevin T},
	year         = 2017,
	journal      = {Journal of patient safety},
	publisher    = {LWW},
	volume       = 13,
	number       = 4,
	pages        = {237--242}
}
@article{terrin2003adjusting,
	title        = {Adjusting for publication bias in the presence of heterogeneity},
	author       = {Terrin, Norma and Schmid, Christopher H and Lau, Joseph and Olkin, Ingram},
	year         = 2003,
	journal      = {Statistics in medicine},
	publisher    = {Wiley Online Library},
	volume       = 22,
	number       = 13,
	pages        = {2113--2126}
}
@article{Wang2008,
	title        = {{Attributions of trust in decision support technologies: A study of recommendation agents for e-commerce}},
	author       = {Wang, Weiquan and Benbasat, Izak},
	year         = 2008,
	journal      = {Journal of Management Information Systems},
	volume       = 24,
	number       = 4,
	pages        = {249--273},
	doi          = {10.2753/MIS0742-1222240410},
	issn         = {07421222},
	abstract     = {As organizations increasingly utilize Web-based technologies to support customers better, trust in decision support technologies has emerged as an important issue in online environments. In this study, we identify six reasons users trust (or do not trust) a technology in the early stages of its use by extending the theories of trust formation in interpersonal and organizational contexts to that of decision support technologies. We study the particular context of decision support technologies for e-commerce: online recommendation agents (RAS), which facilitate users' decision making by providing advice on what to buy based on user-specified needs and preferences. A laboratory experiment is conducted using a multimethod approach to collect data. Both quantitative data about participants' trust in RAS and written protocols that explain the reasons for their levels of trust are collected. A content analysis of the written protocols identifies both positive and negative trust attributions that are then mapped to six trust reasons. A structural equation modeling analysis is employed to test the causal strengths of the trust reasons in explaining participants' trust in RAS. The results reveal that in the early stages of trust formation, four positive reasons (i.e., knowledge-based, interactive, calculative, and dispositional) are associated with higher trust in RAS and two negative reasons (i.e., calculative and interactive) are associated with lower trust in RAS. The results also demonstrate some distinctive features of trust formation with respect to decision support technologies. We discuss the research and practical implications of the findings and describe opportunities for future research. {\textcopyright} 2008 M.E. Sharpe, Inc.},
	file         = {:Users/yosef/Downloads/Attributions of Trust in Decision Support Technologies A Study of Recommendation Agents for E Commerce.pdf:pdf},
	keywords     = {Decision support technology,Reasons to trust,Recommendation agents,Trust attribution,Trust in technology}
}
@article{hak2016interpret,
	title        = {How to interpret results of meta-analysis},
	author       = {Hak, Tony and van Rhee, Henk and Suurmond, Robert},
	year         = 2016,
	journal      = {Available at SSRN 3241367}
}
@article{Costa2011,
	title        = {{Measuring trust in teams : Development and validation of a multifaceted measure of formative and reflective indicators of team trust Measuring trust in teams : Development and validation indicators of team trust}},
	author       = {Costa, Ana Cristina and Anderson, Neil and Costa, Ana Cristina and Anderson, Neil},
	year         = 2011,
	volume       = {0643},
	doi          = {10.1080/13594320903272083},
	file         = {:Users/yosef/Downloads/Measuring trust in teams Development and validation of a multifaceted measure of formative and reflective indicators of team trust.pdf:pdf},
	mendeley-groups = {Trust_Metrics_Analysis,Other_Trust_Metric_Papers}
}
@article{Bisantz2001,
	title        = {{Assessment of operator trust in and utilization of automated decision-aids under different framing conditions}},
	author       = {Bisantz, Ann M and Seong, Younho},
	year         = 2001,
	journal      = {International Journal of Industrial Ergonomics},
	volume       = 28,
	pages        = {85--97},
	url          = {https://ac.els-cdn.com/S0169814101000154/1-s2.0-S0169814101000154-main.pdf?_tid=ab8205ea-c0b0-11e7-8a06-00000aacb35e&acdnat=1509725249_efbd90d4d95baf3ba14220adc6f6bff8},
	abstract     = {Computerized aids may be used to support decision-making and control in a variety of complex, dynamic arenas. For instance, such systems have been introduced into industrial settings as the means to implement automated control or support decision-making activities such as fault detection and recovery. Of interest in these systems is the extent to which operators utilize and trust such systems, in terms of their ability to successfully control systems, or the information or decision support they provide, particularly under conditions of potential failure. A theoretical framework to describe potential factors affecting these issues, and an experiment to investigate the role of failure cause on trust and system utilization, are described. Results provide some support for factors in the theoretical framework, and also demonstrated the use of an empirically developed trust scale. Relevance to industry As manufacturing environments increasingly rely on computerized and automated systems for control and human operator support, it is necessary to understand the situational factors which could impact operators' use of such systems. This paper describes a framework which could be used to investigate trust in industrial automation settings, as well as a rating scale which could be applied. #},
	file         = {:Users/yosef/Downloads/1-s2.0-S0169814101000154-main.pdf:pdf},
	keywords     = {Automation,Decision-aids,Trust},
	mendeley-groups = {Jian_Experiments,Other_Trust_Metric_Papers}
}
@incollection{malle2021multidimensional,
	title        = {A multidimensional conception and measure of human-robot trust},
	author       = {Malle, Bertram F and Ullman, Daniel},
	year         = 2021,
	booktitle    = {Trust in Human-Robot Interaction},
	publisher    = {Elsevier},
	pages        = {3--25}
}
@article{cherif2014impact,
	title        = {The Impact of Recommendation Agents’ Type of Voice on Perceived Social Presence, Trust and Users Intentions on an Insurance Website},
	author       = {Cherif, Emna and Lemoine, Jean-Fran{\c{c}}ois},
	year         = 2014,
	journal      = {Trends in Practical Applications of Heterogeneous Multi-agent Systems. The PAAMS Collection},
	publisher    = {Springer},
	pages        = {139--148}
}
@article{Ross2008,
	title        = {{Moderators of trust and reliance across multiple decision aids.}},
	author       = {Ross, Jennifer Marie},
	year         = 2008,
	journal      = {Dissertation Abstracts International: Section B: The Sciences and Engineering},
	volume       = 69,
	number       = {6-B},
	pages        = 3885,
	isbn         = {0419-4217},
	keywords     = {*Automation,*Decision Making,*Human Machine Systems,*Trust (Social Behavior)}
}
@phdthesis{YAGODA,
	title        = {{You Want Me to Use THAT Robot? Identifying Underlying Factors Affecting Robot Use}},
	author       = {Yagoda, Rosemarie Elaine},
	year         = 2013,
	school       = {North Carolina State University}
}
@article{Wang2018,
	title        = {{Is it my looks? Or something i said? The impact of explanations, embodiment, and expectations on trust and performance in human-robot teams}},
	author       = {Wang, Ning and Pynadath, David V. and Rovira, Ericka and Barnes, Michael J. and Hill, Susan G.},
	year         = 2018,
	journal      = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume       = {10809 LNCS},
	pages        = {56--69},
	doi          = {10.1007/978-3-319-78978-1\_5},
	isbn         = 9783319789774,
	issn         = 16113349,
	abstract     = {Trust is critical to the success of human-robot interaction. Research has shown that people will more accurately trust a robot if they have an accurate understanding of its decision-making process. The Partially Observable Markov Decision Process (POMDP) is one such decision-making process, but its quantitative reasoning is typically opaque to people. This lack of transparency is exacerbated when a robot can learn, making its decision making better, but also less predictable. Recent research has shown promise in calibrating human-robot trust by automatically generating explanations of POMDP-based decisions. In this work, we explore factors that can potentially interact with such explanations in influencing human decision-making in human-robot teams. We focus on explanations with quantitative expressions of uncertainty and experiment with common design factors of a robot: its embodiment and its communication strategy in case of an error. Results help us identify valuable properties and dynamics of the human-robot trust relationship.},
	file         = {:Users/yosef/Downloads/PT-2018-Wang.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{marsh2003role,
	title        = {The role of trust in information science and technology},
	author       = {Marsh, Stephen and Dibben, Mark R},
	year         = 2003,
	journal      = {Annual Review of Information Science and Technology (ARIST)},
	volume       = 37,
	pages        = {465--98}
}
@unpublished{Razin2022up,
	title        = {{A Measure of Trust? A Review of the Current State of Human-Automation Trust Questionnaires}},
	author       = {Razin, Yosef S. and Feigh, Karen M.},
	note         = {Revised and Resubmitted}
}
@article{Flora2020,
	title        = {{Your Coefficient Alpha Is Probably Wrong, but Which Coefficient Omega Is Right? A Tutorial on Using R to Obtain Better Reliability Estimates}},
	author       = {Flora, David B.},
	year         = 2020,
	journal      = {Advances in Methods and Practices in Psychological Science},
	volume       = 3,
	number       = 4,
	pages        = {484--501},
	doi          = {10.1177/2515245920951747},
	issn         = {2515-2459},
	abstract     = {Measurement quality has recently been highlighted as an important concern for advancing a cumulative psychological science. An implication is that researchers should move beyond mechanistically reporting coefficient alpha toward more carefully assessing the internal structure and reliability of multi-item scales. Yet a researcher may be discouraged upon discovering that a prominent alternative to alpha, namely, coefficient omega, can be calculated in a variety of ways. In this Tutorial, I alleviate this potential confusion by describing alternative forms of omega and providing guidelines for choosing an appropriate omega estimate pertaining to the measurement of a target construct represented with a confirmatory factor analysis model. Several applied examples demonstrate how to compute different forms of omega in R.},
	file         = {:Users/yosef/Downloads/2515245920951747.pdf:pdf},
	keywords     = {alpha,assessment,confirmatory factor analysis,measurement,omega,open,open data,psychometrics,r,reliability}
}
@article{dzindolet2003role,
	title        = {The role of trust in automation reliance},
	author       = {Dzindolet, Mary T and Peterson, Scott A and Pomranky, Regina A and Pierce, Linda G and Beck, Hall P},
	year         = 2003,
	journal      = {International journal of human-computer studies},
	publisher    = {Elsevier},
	volume       = 58,
	number       = 6,
	pages        = {697--718}
}
@article{xiong2012use,
	title        = {Use patterns among early adopters of adaptive cruise control},
	author       = {Xiong, Huimin and Boyle, Linda Ng and Moeckli, Jane and Dow, Benjamin R and Brown, Timothy L},
	year         = 2012,
	journal      = {Human factors},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 54,
	number       = 5,
	pages        = {722--733}
}
@article{uggirala2004measurement,
	title        = {Measurement of trust in complex and dynamic systems using a quantitative approach},
	author       = {Uggirala, Ananth and Gramopadhye, Anand K and Melloy, Brain J and Toler, Joe E},
	year         = 2004,
	journal      = {International Journal of Industrial Ergonomics},
	publisher    = {Elsevier},
	volume       = 34,
	number       = 3,
	pages        = {175--186}
}
@article{Khadangi2016,
	title        = {{Biased sampling from facebook multilayer activity network using learning automata}},
	author       = {Khadangi, Ehsan and Bagheri, Alireza and Shahmohammadi, Amin},
	year         = 2016,
	journal      = {Applied Intelligence},
	publisher    = {Applied Intelligence},
	volume       = 45,
	number       = 3,
	pages        = {829--849},
	doi          = {10.1007/s10489-016-0784-0},
	issn         = 15737497,
	url          = {http://dx.doi.org/10.1007/s10489-016-0784-0},
	abstract     = {Although much research has been devoted to unbiased sampling of various networks, bias is not always disadvantageous, but sometimes useful. Especially for many real-world applications such as detecting influential nodes, spam users, and the most trustful people, it is preferred to sample users with special properties. Since sampling from friendship network alone cannot collect these important nodes appropriately, one may use interactions occurred among users. This paper deals with biased sampling of multilayer activity network. The proposed method initially learns the transition probabilities according to the considered application using learning automata. Then we sample the graph by running an application-based random walk following the learnt probabilities, in order to be guided to suitable nodes and collect their information. At last, the performance of the proposed method in terms of different applications such as fame, spam, and trust is evaluated and compared with those of common sampling algorithms. According to the experiments done, biased sampling method based on learning automata outperforms all other sampling approaches including simple random walk, Metropolis-Hastings random walk, BFS, forest fire, degree, and uniform sampling in terms of all the evaluation measures. To the best of our knowledge, our method is the first and only biased sampling method which can be used in a multilayer activity network.},
	file         = {:Users/yosef/Downloads/Khadangi2016_Article_BiasedSamplingFromFacebookMult.pdf:pdf},
	keywords     = {Activity network,Facebook,Learning automata,Network sampling,Social media marketing,Social network analysis},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bruijn2013,
	title        = {{The Base of Trust in Human-Robot Interaction}},
	author       = {Bruijn, Marlies De},
	year         = 2013,
	pages        = {1--9},
	file         = {:Users/yosef/Downloads/Bruijn,_M.L.E._de_1.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Srinivasa2021,
	title        = {{‘Help me please'}},
	author       = {Srinivasa, Siddhartha S.},
	year         = 2021,
	journal      = {The Routledge International Handbook of Penal Abolition},
	pages        = {119--130},
	doi          = {10.4324/9780429425035-18},
	isbn         = 9781450333627,
	file         = {:Users/yosef/Downloads/2858036.2858217.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@phdthesis{Desai2012,
	title        = {{Modeling Trust To Improve Human-Robot Interaction}},
	author       = {Desai},
	year         = 2012,
	school       = {University of Massachusetts, Lowell}
}
@inproceedings{Tegmark,
	title        = {Can You Trust Your Trust Measure?},
	author       = {Chita-Tegmark, Meia and Law, Theresa and Rabb, Nicholas and Scheutz, Matthias},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
	pages        = {92--100}
}
@article{tomlinson2020revisiting,
	title        = {Revisiting the trustworthiness--trust relationship: exploring the differential predictors of cognition-and affect-based trust},
	author       = {Tomlinson, Edward C and Schnackenberg, Andrew K and Dawley, David and Ash, Steven R},
	year         = 2020,
	journal      = {Journal of Organizational Behavior},
	publisher    = {Wiley Online Library},
	volume       = 41,
	number       = 6,
	pages        = {535--550}
}
@book{josang2016subjective,
	title        = {Subjective logic},
	author       = {J{\o}sang, Audun},
	year         = 2016,
	publisher    = {Springer}
}
@book{castelfranchi2010trust,
	title        = {Trust theory: A socio-cognitive and computational model},
	author       = {Castelfranchi, Christiano and Falcone, Rino},
	year         = 2010,
	publisher    = {John Wiley \& Sons},
	volume       = 18
}
@misc{YagodaApp,
	title        = {{Appendix A Final HRI Trust Scale}},
	author       = {YAGODA, ROSEMARIE ELAINE},
	pages        = {1--2}
}
@article{nomura2006measurement,
	title        = {Measurement of negative attitudes toward robots},
	author       = {Nomura, Tatsuya and Suzuki, Tomohiro and Kanda, Takayuki and Kato, Kensuke},
	year         = 2006,
	journal      = {Interaction Studies},
	publisher    = {John Benjamins},
	volume       = 7,
	number       = 3,
	pages        = {437--454}
}
@article{endsley2017here,
	title        = {From here to autonomy: lessons learned from human--automation research},
	author       = {Endsley, Mica R},
	year         = 2017,
	journal      = {Human factors},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 59,
	number       = 1,
	pages        = {5--27}
}
@article{muir1996trust,
	title        = {{Trust in automation. Part II. Experimental studies of trust and human intervention in a process control simulation}},
	author       = {Muir, Bonnie M and Moray, Neville},
	year         = 1996,
	journal      = {Ergonomics},
	publisher    = {Taylor \& Francis},
	volume       = 39,
	number       = 3,
	pages        = {429--460}
}
@inproceedings{brzowski2019trust,
	title        = {Trust Measurement in Human--Automation Interaction: A Systematic Review},
	author       = {Brzowski, Matthew and Nathan-Roberts, Dan},
	year         = 2019,
	booktitle    = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	volume       = 63,
	number       = 1,
	pages        = {1595--1599},
	organization = {SAGE Publications Sage CA: Los Angeles, CA}
}
@article{Rempel1985,
	title        = {{Trust in close relationships scale}},
	author       = {Rempel, J.K. and Holmes, J.G. and Zanna, M.P.},
	year         = 1985,
	journal      = {J. of Personality and Social Psych.},
	volume       = 49,
	number       = 1,
	pages        = {95--112},
	abstract     = {Tested a theoretical model of interpersonal trust in close relationships with 47 dating, cohabiting, or married couples (mean ages were 31 yrs for males and 29 yrs for females). The validity of the model's 3 dimensions of trust—predictability, dependability, and faith—was examined. Ss completed scales designed to measure liking and loving, trust, and motivation for maintaining the relationship. An analysis of the instrument measuring trust was consistent with the notion that the predictability, dependability, and faith components represent distinct and coherent dimensions. The perception of intrinsic motives in a partner emerged as a dimension, as did instrumental and extrinsic motives. As expected, love and happiness were closely tied to feelings of faith and the attribution of intrinsic motivation to both self and partner. Women appeared to have more integrated, complex views of their relationships than men: All 3 forms of trust were strongly related, and attributions of instrumental motives in their partners seemed to be self-affirming. There was a tendency for Ss to view their own motives as less self-centered and more exclusively intrinsic than their partner's motives.},
	mendeley-groups = {Grant}
}
@article{Rotter1980,
	title        = {{Interpersonal trust, trustworthiness, and gullibility.}},
	author       = {Rotter, Julian B.},
	year         = 1980,
	journal      = {American Psychologist},
	volume       = 35,
	number       = 1,
	pages        = {1--7},
	abstract     = {This article reviews the positive and potential negative consequences of being high or low in interpersonal trust in current social life, particularly in interacting with ordinary people. A summary and analysis of many investigations lead to the following conclusions applicable to the population studied: People who trust more are less likely to lie and are possibly less likely to cheat or steal. They are more likely to give others a second chance and to respect the rights of others. The high truster is less likely to be unhappy, conflicted, or maladjusted, and is liked more and sought out as a friend more often, both by low-trusting and high-trusting others. When gullibility is defined as naivet{\'{e}} or foolishness and trust is defined as believe others in the absence of clear-cut reasons to disbelieve, then it can be shown over a series of studies that high trusters are not more gullible than low trusters.},
	mendeley-groups = {Grant,NASA},
	pmid         = 22171949
}


@book{barber1983logic,
	title        = {{The logic and limits of trust}},
	author       = {Barber, Bernard},
	year         = 1983,
	publisher    = {Rutgers Univ. Press},
	address      = {New Brunswick, NJ},
	mendeley-groups = {NASA}
}
@article{Muir1987,
	title        = {{Trust between humans and machines, and the design of decision aids}},
	author       = {Muir, Bonnie M},
	year         = 1987,
	journal      = {Int. J. Man-Machine Studies},
	volume       = 27,
	pages        = {527--539},
	doi          = {10.1016/S0020-7373(87)80013-5},
	isbn         = {0020-7373},
	issn         = {00207373},
	abstract     = {A problem in the design of decision aids is how to design them so that decision makers will trust them and therefore use them appropriately. This problem is approached in this paper by taking models of trust between humans as a starting point, and extending these to the human-machine relationship. A definition and model of human-machine trust are proposed, and the dynamics of trust between humans and machines are examined. Based upon this analysis, recommendations are made for calibrating users' trust in decision aids.},
	annote       = {NULL},
	file         = {:Users/yosef/Library/Application Support/Mendeley Desktop/Downloaded/Muir - 1987 - Trust between humans and machines, and the design of decision aids.pdf:pdf},
	mendeley-groups = {WeRobot,Trust,Grant,NASA}
}

@article{McKnightD2011,
	title        = {Trust in a specific technology: An investigation of its components and measures},
	author       = {McKnight, D Harrison and Carter, Michelle and Thatcher, Jason Bennett and Clay, Paul F},
	year         = 2011,
	journal      = {ACM Trans. on Mgmt. Info. Sys. (TMIS)},
	publisher    = {ACM New York, NY, USA},
	volume       = 2,
	number       = 2,
	pages        = {1--25}
}
@article{Korber2018,
	title        = {Theoretical considerations and development of a questionnaire to measure trust in automation},
	author       = {K{\"o}rber, Moritz},
	year         = 2018,
	journal      = {Cong. of the Intl. Ergonomics Ass.},
	pages        = {13--30},
	organization = {Springer}
}
@article{Chien2014,
	title        = {An empirical model of cultural factors on trust in automation},
	author       = {Chien, ShihYi and Lewis, Michael and Semnani-Azad, Zhaleh and Sycara, Katia},
	year         = 2014,
	journal      = {Proc. of the Human Factors and Ergonomics Soc.},
	volume       = 58,
	number       = 1,
	pages        = {859--863},
	organization = {SAGE Publications Sage CA: Los Angeles, CA}
}
@article{Gefen2003,
	title        = {{Trust and TAM in online shopping: An integrated model}},
	author       = {Gefen, David and Karahanna, Elena and Straub, Detmar W.},
	year         = 2003,
	journal      = {MIS Qtly},
	volume       = 27,
	number       = 1,
	pages        = {51--90},
	doi          = {10.1017/CBO9781107415324.004},
	isbn         = 9788578110796,
	issn         = {1098-6596},
	archiveprefix = {arXiv},
	arxivid      = {arXiv:1011.1669v3},
	eprint       = {arXiv:1011.1669v3},
	keywords     = {icle},
	mendeley-groups = {Grant},
	pmid         = 25246403
}
@article{Madsen2000,
	title        = {{Measuring human-computer trust}},
	author       = {Madsen, Maria and Gregor, Shirley},
	year         = 2000,
	journal      = {Proc. of 11th Australasian Conf. on Information Systems},
	volume       = 53,
	pages        = {6--8},
	file         = {:Users/yosef/Downloads/10.1.1.93.3874.pdf:pdf},
	keywords     = {decision support,expert systems,human-computer trust,instrument development,survey research,systems},
	mendeley-groups = {NASA,Trust{\_}Metrics{\_}Analysis}
}


@article{Balliet2013,
	title        = {{Trust, conflict, and cooperation: A meta-analysis}},
	author       = {Balliet, Daniel and {Van Lange}, Paul A.M.},
	year         = 2013,
	journal      = {Psych. Bulletin},
	volume       = 139,
	number       = 5,
	pages        = {1090--1112},
	doi          = {10.1037/a0030939},
	isbn         = {1939-1455 (Electronic)$\backslash$r0033-2909 (Linking)},
	issn         = {00332909},
	abstract     = {Many theories of trust emphasize that trust is most relevant to behavior in situations involving a conflict of interests. However, it is not clear how trust relates to behavior across situations that differ in the degree of conflicting interest: Does trust matter more when the conflict of interest is small or large? According to an interdependence perspective, trust becomes an especially important deter- minant of behavior in situations involving larger, compared to smaller, degrees of conflicting interests. To examine this perspective, we conducted a meta-analysis involving 212 effect sizes on the relation between trust (both state and dispositional trust in others) and cooperation in social dilemmas—situations that involve varying degrees of conflict between self-interest and collective interest. Results revealed that the positive relation between trust and cooperation is stronger when there is a larger, compared to smaller, degree of conflict. We also examined several other possible moderators of the relation between trust and cooperation. The relation between trust and cooperation was stronger during individual, compared to intergroup, interactions but did not vary as a function of the situation being either a one-shot or repeated interaction. We also find differences across countries in the extent that people condition their own cooperation based on their trust in others. We discuss how the results support an emerging consensus about trust being limited to situations of conflict and address some theoretical and societal implications for our understanding of how and why trust is so important to social interactions and relationships.},
	file         = {:Users/yosef/Downloads/Trust{\_}Conflict{\_}and{\_}Cooperation{\_}A{\_}Meta-An.pdf:pdf},
	keywords     = {Cooperation,Expectations,Meta-analysis,Social dilemmas,Trust},
	mendeley-groups = {NASA},
	pmid         = 23231532
}
@inproceedings{Ermisch2006,
	title        = {People's Trust: The design of a survey-based experiment},
	author       = {Ermisch, John and Gambetta, Diego},
	year         = 2006,
	booktitle    = {ISER Working Paper Series},
	publisher    = {IZA Discussion Paper},
	address      = {Colchester, UK},
	number       = {2006-34},
	institution  = {University of Essex, Institute for Social and Economic Research (ISER)}
}
@article{Bacharach2007,
	title        = {The self-fulfilling property of trust: An experimental study},
	author       = {Bacharach, Michael and Guerra, Gerardo and Zizzo, Daniel John},
	year         = 2007,
	journal      = {Theory and Decision},
	publisher    = {Springer},
	volume       = 63,
	number       = 4,
	pages        = {349--388}
}
@phdthesis{Wagner2009,
	title        = {{The role of trust and relationships in human-robot social interaction}},
	author       = {Wagner, Alan Richard},
	year         = 2009,
	school       = {Georgia Institute of Technology}
}
@article{aloe2014empirical,
	title        = {An empirical investigation of partial effect sizes in meta-analysis of correlational data},
	author       = {Aloe, Ariel M},
	year         = 2014,
	journal      = {The Journal of general psychology},
	publisher    = {Taylor \& Francis},
	volume       = 141,
	number       = 1,
	pages        = {47--64}
}
@article{van2015user,
	title        = {User manual for Meta-Essentials: Workbooks for meta-analysis},
	author       = {van Rhee, Henk and Suurmond, Robert and Hak, Tony},
	year         = 2015,
	journal      = {Available at SSRN 3241355}
}
@article{suurmond2017introduction,
	title        = {Introduction, comparison, and validation of Meta-Essentials: a free and simple tool for meta-analysis},
	author       = {Suurmond, Robert and van Rhee, Henk and Hak, Tony},
	year         = 2017,
	journal      = {Research synthesis methods},
	publisher    = {Wiley Online Library},
	volume       = 8,
	number       = 4,
	pages        = {537--553}
}
@inproceedings{Razin2020,
	title        = {Hitting the Road: Exploring Human-Robot Trust for Self-Driving Vehicles},
	author       = {Razin, Yosef S. and Feigh, Karen M.},
	year         = 2020,
	booktitle    = {2020 IEEE International Conference on Human-Machine Systems (ICHMS)},
	volume       = {},
	number       = {},
	pages        = {1--6},
	doi          = {10.1109/ICHMS49158.2020.9209525}
}
@article{Razin2021a,
	title        = {Watch For Failing Objects: What Inappropriate Compliance Reveals About Shared Mental Models In Autonomous Cars},
	author       = {Yosef S. Razin and Jack Gale and Jiaojiao Fan and Jaznae’ Smith and Karen M. Feigh},
	year         = 2021,
	journal      = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	volume       = 65,
	number       = 1,
	pages        = {643--647},
	doi          = {10.1177/1071181321651081},
	url          = {https://doi.org/10.1177/1071181321651081},
	eprint       = {https://doi.org/10.1177/1071181321651081}
}
@inproceedings{razin2019,
	title        = {{Toward Interactional Trust for Humans and Automation: Extending Interdependence}},
	author       = {Razin, Yosef S. and Feigh, Karen M.},
	year         = 2019,
	booktitle    = {2019 IEEE SmartWorld: Advanced   Trusted Computing},
	pages        = {1348--1355}
}
@book{gottman2011science,
	title        = {{The Science of Trust: Emotional Attunement for Couples}},
	author       = {Gottman, John M},
	year         = 2011,
	publisher    = {W.W. Norton {\&} Company},
	address      = {New York, London},
	mendeley-groups = {ATC}
}
@article{rusbult_93,
	title        = {{Commitment processes in close relationships: An interdependence analysis}},
	author       = {Rusbult, Caryl E and Buunk, Bram P},
	year         = 1993,
	journal      = {J. of Social and Personal Relationships},
	publisher    = {Sage Publications Sage CA: Thousand Oaks, CA},
	volume       = 10,
	number       = 2,
	pages        = {175--204},
	mendeley-groups = {ATC}
}
@article{Ert2011,
	title        = {A choice prediction competition for social preferences in simple extensive form games: An introduction},
	author       = {Ert, Eyal and Erev, Ido and Roth, Alvin E},
	year         = 2011,
	journal      = {Games},
	publisher    = {Molecular Diversity Preservation International},
	volume       = 2,
	number       = 3,
	pages        = {257--276}
}
@article{battigalli2005,
	title        = {Dynamic psychological games},
	author       = {Battigalli, Pierpaolo and Dufwenberg, Martin},
	year         = 2009,
	journal      = {J. of Econ. Theory},
	publisher    = {Elsevier},
	volume       = 144,
	number       = 1,
	pages        = {1--35}
}

@article{dunning2014trust,
	title        = {{Trust at zero acquaintance: More a matter of respect than expectation of reward.}},
	author       = {Dunning, David and Anderson, Joanna E and Schl{\"{o}}sser, Thomas and Ehlebracht, Daniel and Fetchenhauer, Detlef},
	year         = 2014,
	journal      = {J. of Personality and Social Psychology},
	publisher    = {US: American Psychological Association},
	volume       = 107,
	number       = 1,
	pages        = 122,
	file         = {:Users/yosef/Downloads/Trust-at-Zero-Acquaintance-More-a-Matter-of-Respect-Than-Expectation-of-Reward-2.pdf:pdf},
	mendeley-groups = {Trust{\_}Game{\_}Theory}
}
@article{faulkner2017problem,
	title        = {The problem of trust},
	author       = {Faulkner, Paul},
	year         = 2017,
	journal      = {The philosophy of trust},
	publisher    = {Oxford University Press Oxford},
	pages        = {109--28}
}
@book{thibaut_59,
	title        = {{The Social Psychology of Groups}},
	author       = {Thibaut, John W and Kelley, Harold H},
	year         = 1959,
	publisher    = {J. Wiley \& Sons},
	address      = {New York},
	mendeley-groups = {ATC}
}
@article{jian,
	title        = {Foundations for an empirically determined scale of trust in automated systems},
	author       = {Jian, Jiun-Yin and Bisantz, Ann M and Drury, Colin G},
	year         = 2000,
	journal      = {Intl. J. of Cog. Ergonomics},
	publisher    = {Taylor \& Francis},
	volume       = 4,
	number       = 1,
	pages        = {53--71}
}
@phdthesis{zhu2009intentional,
	title        = {Intentional systems and the artificial intelligence (ai) hermeneutic network: Agency and intentionality in expressive computational systems},
	author       = {Zhu, Jichen},
	year         = 2009,
	school       = {Georgia Institute of Technology}
}
@inproceedings{atkinson2014shared,
	title        = {Shared awareness, autonomy and trust in human-robot teamwork},
	author       = {Atkinson, David J and Clancey, William J and Clark, Micah H},
	year         = 2014,
	booktitle    = {2014 AAAI Fall Symposium Series}
}
@inproceedings{alaieri2016ethical,
	title        = {Ethical decision making in robots: Autonomy, trust and responsibility},
	author       = {Alaieri, Fahad and Vellino, Andr{\'e}},
	year         = 2016,
	booktitle    = {Intl. conf. on social robotics},
	pages        = {159--168},
	organization = {Springer}
}
@article{nass1999people,
	title        = {Are people polite to computers? Responses to computer-based interviewing systems 1},
	author       = {Nass, Clifford and Moon, Youngme and Carney, Paul},
	year         = 1999,
	journal      = {J. of Applied Social Psych.},
	publisher    = {Wiley Online Library},
	volume       = 29,
	number       = 5,
	pages        = {1093--1109}
}
@article{Epley2007,
	title        = {{On seeing human: A three-factor theory of anthropomorphism.}},
	author       = {Epley, Nicholas and Waytz, Adam and Cacioppo, John T.},
	year         = 2007,
	journal      = {Psych. Review},
	volume       = 114,
	number       = 4,
	pages        = {864--886},
	doi          = {10.1037/0033-295X.114.4.864},
	isbn         = {0033-295X (Print)$\backslash$r0033-295X (Linking)},
	issn         = {1939-1471},
	abstract     = {Anthropomorphism describes the tendency to imbue the real or imagined behavior of nonhuman agents with humanlike characteristics, motivations, intentions, or emotions. Although surprisingly common, anthropomorphism is not invariant. This article describes a theory to explain when people are likely to anthropomorphize and when they are not, focused on three psychological determinants—the accessibility and applicability of anthropocentric knowledge (elicited agent knowledge), the motivation to explain and understand the behavior of other agents (effectance motivation), and the desire for social contact and affiliation (sociality motivation). This theory predicts that people are more likely to anthropomorphize when anthropocentric knowledge is accessible and applicable, when motivated to be effective social agents, and when lacking a sense of social connection to other humans. These factors help to explain why anthropomorphism is so variable; organize diverse research; and offer testable predictions about dispo- sitional, situational, developmental, and cultural influences on anthropomorphism. Discussion addresses extensions of this theory into the specific psychological processes underlying anthropomorphism, applications of this theory into robotics and human–computer interaction, and the insights offered by this theory into the inverse process of dehumanization.},
	annote       = {NULL},
	archiveprefix = {arXiv},
	arxivid      = {epley2007},
	eprint       = {epley2007},
	file         = {:Users/yosef/Desktop/WeRobot/10.1.1.457.4031.pdf:pdf},
	keywords     = {1995,75 million,agency,animal cognition,anthropomorphism,as cited in heywood,at least,biodiversity assessment,inhabited by approximately 1,known species with unique,mind perception,our planet is currently,phylogenetic characteristics,social cognition,unep},
	mendeley-groups = {Trust},
	pmid         = 17907867
}
@article{Park2020,
	title        = {{Multifaceted trust in tourism service robots}},
	author       = {Park, Sangwon},
	year         = 2020,
	journal      = {Annals of Tourism Research},
	publisher    = {Elsevier},
	volume       = 81,
	number       = {March},
	pages        = 102888,
	issn         = {0160-7383},
	keywords     = {Artificial intelligence,Autonomous robots,Service robots,Trust and service encounters,arti fi cial intelligence},
	mendeley-groups = {Trust{\_}Metrics{\_}Analysis}
}
@article{Bolton1995,
	title        = {{ERC: A theory of equity, reciprocity, and competition}},
	author       = {Bolton, Gary E and Ockenfels, Axel},
	year         = 2000,
	journal      = {The American Econ. Rev.},
	volume       = 90,
	number       = 1,
	pages        = {166--193}
}
@article{Rabin2020,
	title        = {Incorporating fairness into game theory and economics},
	author       = {Rabin, Matthew},
	year         = 1993,
	journal      = {The American Econ. Rev.},
	publisher    = {JSTOR},
	pages        = {1281--1302}
}
@inproceedings{hauslschmid2017supportingtrust,
	title        = {Supportingtrust in autonomous driving},
	author       = {H{\"a}uslschmid, Renate and von Buelow, Max and Pfleging, Bastian and Butz, Andreas},
	year         = 2017,
	booktitle    = {Proc. of the 22nd Intl. Conf. on Intelligent User Interfaces},
	pages        = {319--329}
}
@article{deVisser,
	title        = {Almost human: Anthropomorphism increases trust resilience in cognitive agents},
	author       = {de Visser, Ewart and Monfort, Samuel and Mckendrick, Ryan and Smith, Melissa and Mcknight, Patrick and Krueger, Frank and Parasuraman, Raja},
	year         = 2016,
	month        = {08},
	journal      = {J. of Experimental Psych.: Appl.},
	volume       = 22,
	pages        = {}
}
@mastersthesis{Nordheim2018,
	title        = {Trust in chatbots for customer service--findings from a questionnaire study},
	author       = {Nordheim, Cecilie Bertinussen},
	year         = 2018,
	school       = {U. of Oslo}
}


@phdthesis{muir2002operators,
	title        = {Operators' trust in and use of automatic controllers in a supervisory process control task.},
	author       = {Muir, Bonnie Marlene},
	year         = 2002,
	school       = {Univ. of Toronto}
}


@article{baier1986trust,
	title        = {Trust and antitrust},
	author       = {Baier, Annette},
	year         = 1986,
	journal      = {ethics},
	publisher    = {University of Chicago Press},
	volume       = 96,
	number       = 2,
	pages        = {231--260}
}
@article{Holton1994,
	title        = {{Deciding to Trust, Coming to Believe}},
	author       = {Holton, Richard},
	year         = 1994,
	journal      = {Australasian Journal of Philosophy},
	volume       = 72,
	number       = 1,
	pages        = {63--76},
	doi          = {10.1080/00048409412345881},
	isbn         = {0004840941},
	issn         = 14716828,
	file         = {:Users/yosef/Downloads/Deciding to trust coming to believe.pdf:pdf},
	mendeley-groups = {Thesis_Intro}
}
@article{Wee2011,
	title        = {{"Xin", Trust, And Confucius' Ethics}},
	author       = {Wee, Cicilia},
	year         = 2011,
	journal      = {Philosophy East and West},
	volume       = 61,
	number       = 3,
	pages        = {516--533},
	url          = {https://www.jstor.org/stable/23015356},
	file         = {:Users/yosef/Downloads/23015356.pdf:pdf},
	mendeley-groups = {Thesis_Intro}
}
@article{Hawley2014,
	title        = {{Trust , Distrust and Commitment Author ( s ): Katherine Hawley Published by : Wiley Stable URL : https://www.jstor.org/stable/43828859 Trust , Distrust and Commitment}},
	author       = {Hawley, Katherine},
	year         = 2014,
	volume       = 48,
	number       = 1,
	pages        = {1--20},
	file         = {:Users/yosef/Downloads/43828859.pdf:pdf},
	mendeley-groups = {Thesis_Intro}
}
@incollection{sep-trust,
	title        = {{Trust}},
	author       = {McLeod, Carolyn},
	year         = 2021,
	booktitle    = {The {Stanford} Encyclopedia of Philosophy},
	publisher    = {Metaphysics Research Lab, Stanford University},
	editor       = {Edward N. Zalta},
	howpublished = {\url{https://plato.stanford.edu/archives/fall2021/entries/trust/}},
	edition      = {{F}all 2021}
}
@book{Jøsang2016,
	title        = {{Subjective Logic: A Formalism for Reasoning Under Uncertainty}},
	author       = {J{\o}sang, Audun},
	year         = 2016,
	publisher    = {Springer International Publishing},
	address      = {Switzerland},
	url          = {https://link.springer.com/content/pdf/10.1007%2F978-3-319-42337-1.pdf},
	file         = {:Users/yosef/Library/Application Support/Mendeley Desktop/Downloaded/J{\o}sang - Unknown - Artificial Intelligence Foundations, Theory, and Algorithms Subjective Logic A Formalism for Reasoning Under Uncerta.pdf:pdf},
	mendeley-groups = {Trust,Trust_For_Paper,Thesis_Intro}
}
@article{Lee2004,
	title        = {{Trust in automation: designing for appropriate reliance.}},
	author       = {Lee, John D and See, Katrina A},
	year         = 2004,
	journal      = {Human factors},
	volume       = 46,
	number       = 1,
	pages        = {50--80},
	doi          = {10.1518/hfes.46.1.50.30392},
	isbn         = {00187208 (ISSN)},
	issn         = {0018-7208},
	annote       = {NULL},
	file         = {:Users/yosef/Library/Application Support/Mendeley Desktop/Downloaded/Lee, See - 2004 - Trust in automation designing for appropriate reliance.pdf:pdf},
	keywords     = {agent,automation,complacency,confidence,credibility,reliability,reliance,technology},
	mendeley-groups = {WeRobot,Trust,Grant,NASA,Thesis_Intro},
	pmid         = 15151155
}
@article{Carp2004,
	title        = {{Field experiments}},
	author       = {Carpenter, Jeffrey and Gerking, Shelby and Mark, R and Krueger, Alan and Mcmillan, John and Ortmann, Andreas and Plott, Charles and Reiley, David and Wilcox, Nathaniel and Roth, Alvin},
	year         = 2004,
	month        = {December},
	journal      = {{J. of Econ. Lit.}},
	volume       = 42,
	number       = 4,
	pages        = {1009--1055},
	mendeley-groups = {Trust{\_}Game{\_}Theory}
}
@article{Deutsch1958,
	title        = {{Trust and suspicion}},
	author       = {Deutsch, Morton},
	year         = 1958,
	journal      = {J. of Conflict Resolution},
	volume       = 2,
	number       = 4,
	pages        = {265--279},
	abstract     = {Discussion of such problems as the armaments race, mental illness, the "hidden persuaders", and juvenile delinquency frequently employ such terms as trust, suspicion, betrayal, faith. Trust and its related concepts are vital to the understanding both of social life and of personality development. Trust involves the notion of motivational relevance as well as predictability. To trust implies that the trustor will suffer unpleasant consequences when trust is not fulfilled i.e. that he will be worse off if trusting and it not fulfilled than if he never trusted. Risk-taking and trusting are therefore different sides of the same coin.},
	mendeley-groups = {Grant},
	pmid         = 13022
}
@article{deutsch1960effect,
	title        = {{The effect of motivational orientation upon trust and suspicion}},
	author       = {Deutsch, Morton},
	year         = 1960,
	journal      = {Human Relations},
	publisher    = {Sage Publications Sage UK: London, England},
	volume       = 13,
	number       = 2,
	pages        = {123--139},
	mendeley-groups = {NASA}
}
@book{deutsch1977resolution,
	title        = {{The Resolution of Conflict: Constructive and Destructive Processes}},
	author       = {Deutsch, Morton},
	year         = 1977,
	publisher    = {Yale University Press},
	address      = {New Haven, CT},
	mendeley-groups = {NASA}
}
@article{Alexander2018,
	title        = {{Why trust an algorithm? Performance, cognition, and neurophysiology}},
	author       = {Alexander, Veronika and Blinder, Collin and Zak, Paul J.},
	year         = 2018,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier},
	volume       = 89,
	number       = {July},
	pages        = {279--288},
	doi          = {10.1016/j.chb.2018.07.026},
	issn         = {07475632},
	url          = {https://doi.org/10.1016/j.chb.2018.07.026},
	abstract     = {OBJECTIVE: We measured neurophysiologic responses and task performance while participants solved mazes after choosing whether to adopt an imperfect helper algorithm. BACKGROUND: Every day we must decide whether to trust or distrust algorithms. Will an algorithm improve our performance on a task? What if we trust it too much? METHOD: Participants had to pay to use the algorithm and were aware that it offered imperfect help. We varied the information about the algorithm to assess the factors that affected adoption while measuring participants' peripheral neurophysiology. RESULTS: We found that information about previous adoption by others had a larger effect on adoption and resulted in lower cognitive load than did information about algorithm accuracy. The neurophysiologic measurement showed that algorithm adoption without any information resulted in low cognitive engagement during the task and impaired task performance. Conversely, algorithm use after information about others' use improved engagement and performance. CONCLUSION: By objectively measuring cognitive load and task performance, we identified how to increase algorithm adoption while sustaining high performance by human operators. APPLICATION: Algorithm adoption can be increased by sharing previous use information and performance improved by providing a reason to monitor the algorithm. Precis: We collected neurophysiologic data while varying information about an algorithm that assisted participants in solving a timed and incentivized maze and found that information about prior use by others more effectively influenced adoption, reduced cognitive load, and improved performance compared to algorithm accuracy information.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563218303480-main.pdf:pdf},
	keywords     = {Automation,Computers,Decisions,Neurophysiology},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bailey2007,
	title        = {{Automation-induced complacency for monitoring highly reliable systems: The role of task complexity, system experience, and operator trust}},
	author       = {Bailey, N. R. and Scerbo, M. W.},
	year         = 2007,
	journal      = {Theoretical Issues in Ergonomics Science},
	volume       = 8,
	number       = 4,
	pages        = {321--348},
	doi          = {10.1080/14639220500535301},
	issn         = {1464536X},
	abstract     = {The increase in quantity and complexity of advanced automated systems has generated new concerns surrounding automation-induced complacency, or the difficulties operators have monitoring the status of automated systems. The present investigation consists of two studies that assessed the impact of system reliability, monitoring complexity, operator trust, and system experience on automation-induced complacency. In both studies, participants operated a manually controlled flight task while monitoring several simulated aircraft displays for failures. The ability of operators to detect a single automation failure over three experimental sessions was also assessed. Results indicated that realistic levels of system reliability severely impaired an operator's ability to monitor effectively. Further, as system experience increased, operator monitoring performance declined. The results also indicated that the complexity of the monitoring task heavily influenced operator monitoring, with poorer performance associated with more cognitively demanding tasks. Finally, results from both studies indicated that operator trust increased and monitoring performance decreased as a function of increasing system reliability. These results suggest that for highly reliable systems, increasing task complexity and extensive experience may severely impair an operator's ability to monitor for unanticipated system states. {\textcopyright} 2007 Taylor & Francis.},
	file         = {:Users/yosef/Downloads/Automation induced complacency for monitoring highly reliable systems the role of task complexity system experience and operator trust.pdf:pdf},
	keywords     = {Attention,Complacency,Monitoring,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Balfe2018,
	title        = {{Understanding Is Key: An Analysis of Factors Pertaining to Trust in a Real-World Automation System}},
	author       = {Balfe, Nora and Sharples, Sarah and Wilson, John R.},
	year         = 2018,
	journal      = {Human Factors},
	volume       = 60,
	number       = 4,
	pages        = {477--495},
	doi          = {10.1177/0018720818761256},
	issn         = 15478181,
	abstract     = {Objective: This paper aims to explore the role of factors pertaining to trust in real-world automation systems through the application of observational methods in a case study from the railway sector. Background: Trust in automation is widely acknowledged as an important mediator of automation use, but the majority of the research on automation trust is based on laboratory work. In contrast, this work explored trust in a real-world setting. Method: Experienced rail operators in four signaling centers were observed for 90 min, and their activities were coded into five mutually exclusive categories. Their observed activities were analyzed in relation to their reported trust levels, collected via a questionnaire. Results: The results showed clear differences in activity, even when circumstances on the workstations were very similar, and significant differences in some trust dimensions were found between groups exhibiting different levels of intervention and time not involved with signaling. Conclusion: Although the empirical, lab-based studies in the literature have consistently found that reliability and competence of the automation are the most important aspects of trust development, understanding of the automation emerged as the strongest dimension in this study. The implications are that development and maintenance of trust in real-world, safety-critical automation systems may be distinct from artificial laboratory automation. Application: The findings have important implications for emerging automation concepts in diverse industries including highly automated vehicles and Internet of things.},
	file         = {:Users/yosef/Downloads/0018720818761256-2.pdf:pdf},
	keywords     = {ethnographic observations,human-automation interaction,supervisory control,technology acceptance,trust in automation},
	mendeley-groups = {Other_Trust_Metric_Papers},
	pmid         = 29613815
}
@article{Bass2013,
	title        = {{The effect of information analysis automation display content on human judgment performance in noisy environments}},
	author       = {Bass, Ellen J. and Baumgart, Leigh A. and Shepley, Kathryn Klein},
	year         = 2013,
	journal      = {Journal of Cognitive Engineering and Decision Making},
	volume       = 7,
	number       = 1,
	pages        = {49--65},
	doi          = {10.1177/1555343412453461},
	issn         = 21695032,
	abstract     = {Displaying both the strategy that information analysis automation employs to makes its judgments and variability in the task environment may improve human judgment performance, especially in cases where this variability impacts the judgment performance of the information analysis automation. This work investigated the contribution of providing either information analysis automation strategy information, task environment information, or both, on human judgment performance in a domain where noisy sensor data are used by both the human and the information analysis automation to make judgments. In a simplified air traffic conflict prediction experiment, 32 participants made probability of horizontal conflict judgments under different display content conditions. After being exposed to the information analysis automation, judgment achievement significantly improved for all participants as compared to judgments without any of the automation's information. Participants provided with additional display content pertaining to cue variability in the task environment had significantly higher aided judgment achievement compared to those provided with only the automation's judgment of a probability of conflict. When designing information analysis automation for environments where the automation's judgment achievement is impacted by noisy environmental data, it may be beneficial to show additional task environment information to the human judge in order to improve judgment performance. Copyright {\textcopyright} 2012, Human Factors and Ergonomics Society.},
	file         = {:Users/yosef/Downloads/1555343412453461.pdf:pdf},
	keywords     = {Automation display content,Human-automated judge learning,Human-automation interaction,Judgment analysis},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bata2018,
	title        = {{Mobile social networking and salesperson maladaptive dependence behaviors}},
	author       = {Bata, Hatem and Pentina, Iryna and Tarafdar, Monideepa and Pullins, Ellen Bolman},
	year         = 2018,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier Ltd},
	volume       = 81,
	pages        = {235--249},
	doi          = {10.1016/j.chb.2017.12.025},
	issn         = {07475632},
	url          = {https://doi.org/10.1016/j.chb.2017.12.025},
	abstract     = {This study investigates technology dependence associated with the work-related use of mobile social networking (MSN) by salespeople. A scale for maladaptive technology dependence behaviors (MTDB) is developed and empirically validated using survey data from 242 mid-level sales managers in the US. Personal and job-related antecedents, as well as consequences of MTDB for sales outcomes, are also examined. Results suggest that emotional attachment to MSN and perceptions of its greater affordances for task accomplishment may lead to maladaptive behaviors of overreliance on MSN for job completion, blind trust, cognitive absorption and dysfunctional use. These associations increase in organizations with competitive psychological climate. Findings also show that using MSN for prospecting does not lead to maladaptive dependence, as opposed to using it for customer relationship maintenance. Salespeople using MSN for relationship maintenance exhibit more maladaptive behaviors if they experience work-related role stress. Finally, salespeople who exhibit MTDB are less likely to complete their assignments and participate in teamwork. These findings provide tools for organizations to develop technology use policies, design sales training, and enhance the work environment. Future studies can examine dependencies on others types of technologies (CRM, marketing automation, etc.), and in other contexts (online retailing, social media analytics, etc.)},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563217307069-main-2.pdf:pdf},
	keywords     = {Dark side of social networking,Maladaptive technology dependence,Mobile social media,Professional sales,Technology addiction},
	mendeley-groups = {Other_Trust_Metric_Papers}
}

@article{Bisantz2001,
	title        = {{Assessment of operator trust in and utilization of automated decision-aids under different framing conditions}},
	author       = {Bisantz, Ann M and Seong, Younho},
	year         = 2001,
	journal      = {International Journal of Industrial Ergonomics},
	volume       = 28,
	pages        = {85--97},
	url          = {https://ac.els-cdn.com/S0169814101000154/1-s2.0-S0169814101000154-main.pdf?_tid=ab8205ea-c0b0-11e7-8a06-00000aacb35e&acdnat=1509725249_efbd90d4d95baf3ba14220adc6f6bff8},
	abstract     = {Computerized aids may be used to support decision-making and control in a variety of complex, dynamic arenas. For instance, such systems have been introduced into industrial settings as the means to implement automated control or support decision-making activities such as fault detection and recovery. Of interest in these systems is the extent to which operators utilize and trust such systems, in terms of their ability to successfully control systems, or the information or decision support they provide, particularly under conditions of potential failure. A theoretical framework to describe potential factors affecting these issues, and an experiment to investigate the role of failure cause on trust and system utilization, are described. Results provide some support for factors in the theoretical framework, and also demonstrated the use of an empirically developed trust scale. Relevance to industry As manufacturing environments increasingly rely on computerized and automated systems for control and human operator support, it is necessary to understand the situational factors which could impact operators' use of such systems. This paper describes a framework which could be used to investigate trust in industrial automation settings, as well as a rating scale which could be applied. #},
	file         = {:Users/yosef/Downloads/1-s2.0-S0169814101000154-main.pdf:pdf},
	keywords     = {Automation,Decision-aids,Trust},
	mendeley-groups = {Jian_Experiments,Other_Trust_Metric_Papers}
}
@article{Bruijn2013,
	title        = {{The Base of Trust in Human-Robot Interaction}},
	author       = {Bruijn, Marlies De},
	year         = 2013,
	pages        = {1--9},
	file         = {:Users/yosef/Downloads/Bruijn,_M.L.E._de_1.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Charalambous2016,
	title        = {{The Development of a Scale to Evaluate Trust in Industrial Human-robot Collaboration}},
	author       = {Charalambous, George and Fletcher, Sarah and Webb, Philip},
	year         = 2016,
	journal      = {International Journal of Social Robotics},
	publisher    = {Springer Netherlands},
	volume       = 8,
	number       = 2,
	pages        = {193--209},
	doi          = {10.1007/s12369-015-0333-8},
	issn         = 18754805,
	abstract     = {Trust has been identified as a key element for the successful cooperation between humans and robots. However, little research has been directed at understanding trust development in industrial human-robot collaboration (HRC). With industrial robots becoming increasingly integrated into production lines as a means for enhancing productivity and quality, it will not be long before close proximity industrial HRC becomes a viable concept. Since trust is a multidimensional construct and heavily dependent on the context, it is vital to understand how trust develops when shop floor workers interact with industrial robots. To this end, in this study a trust measurement scale suitable for industrial HRC was developed in two phases. In phase one, an exploratory study was conducted to collect participants' opinions qualitatively. This led to the identification of trust related themes relevant to the industrial context and a related pool of questionnaire items was generated. In the second phase, three human-robot trials were carried out in which the questionnaire items were applied to participants using three different types of industrial robots. The results were statistically analysed to identify the key factors impacting trust and from these generate a trust measurement scale for industrial HRC.},
	file         = {:Users/yosef/Downloads/Charalambous2016_Article_TheDevelopmentOfAScaleToEvalua.pdf:pdf},
	keywords     = {Human-robot collaboration,Industrial robot,Trust scale},
	mendeley-groups = {Other_Trust_Metric_Papers}
}


@article{waytz2014mind,
	title        = {The mind in the machine: Anthropomorphism increases trust in an autonomous vehicle},
	author       = {Waytz, Adam and Heafner, Joy and Epley, Nicholas},
	year         = 2014,
	journal      = {Journal of Experimental Social Psychology},
	publisher    = {Elsevier},
	volume       = 52,
	pages        = {113--117}
}
@inproceedings{BusTrust,
	title        = {{Supporting trust in autonomous driving}},
	author       = {H{\"{a}}uslschmid, Renate and von Buelow, Max and Pfleging, Bastian and Butz, Andreas},
	year         = 2017,
	booktitle    = {Proceedings of the 22nd international conference on intelligent user interfaces},
	pages        = {319--329},
	file         = {:Users/yosef/Downloads/haeuslschmid2017iui.pdf:pdf},
	mendeley-groups = {ATC,Trust_Metrics_Analysis},
	organization = {ACM}
}
@inproceedings{BusTrust,
	title        = {{Supporting trust in autonomous driving}},
	author       = {H{\"{a}}uslschmid, Renate and von Buelow, Max and Pfleging, Bastian and Butz, Andreas},
	year         = 2017,
	booktitle    = {Proceedings of the 22nd international conference on intelligent user interfaces},
	pages        = {319--329},
	file         = {:Users/yosef/Downloads/haeuslschmid2017iui.pdf:pdf},
	mendeley-groups = {ATC,Trust_Metrics_Analysis},
	organization = {ACM}
}


@article{Levin2006,
	title        = {{Fairness and Reciprocity Theories of Fairness : Payo ff -Driven}},
	author       = {Levin, Jonathan},
	year         = 2006,
	number       = {June},
	pages        = {1--7},
	url          = {https://web.stanford.edu/$\sim$jdlevin/Econ 286/Fairness.pdf},
	file         = {:Users/yosef/Library/Application Support/Mendeley Desktop/Downloaded/de8a9ecab71b9e3430bd7298949e73970dbc557a.pdf:pdf},
	mendeley-groups = {Trust_Game_Theory}
}
@article{Dufwenberg2004,
	title        = {{A theory of sequential reciprocity}},
	author       = {Dufwenberg, Martin and Kirchsteiger, Georg},
	year         = 2004,
	volume       = 47,
	pages        = {268--298},
	doi          = {10.1016/j.geb.2003.06.003},
	file         = {:Users/yosef/Downloads/Dufwenberg-Kirchsteiger_GEB-2004_A-theory-of-sequential-reciprocity.pdf:pdf},
	keywords     = {extensive form games,reciprocity},
	mendeley-groups = {Trust_Game_Theory}
}
@book{kelley_03,
	title        = {{An Atlas of Interpersonal Situations}},
	author       = {Kelley, Harold H and Holmes, John G and Kerr, Norbert L and Reis, Harry T and Rusbult, Caryl E and {Van Lange}, Paul A M},
	year         = 2003,
	publisher    = {Cambridge University Press},
	address      = {New York, NY},
	mendeley-groups = {ATC}
}
@article{Rusbult2008,
	title        = {{Why We Need Interdependence Theory}},
	author       = {Rusbult, Caryl E. and {Van Lange}, Paul A. M.},
	year         = 2008,
	journal      = {Social and Personality Psychology Compass},
	volume       = 2,
	number       = 5,
	pages        = {2049--2070},
	doi          = {10.1111/j.1751-9004.2008.00147.x},
	abstract     = {The exceptional sociability of human life colors nearly every phenomenon in the social and behavioral sciences. However, most psychological theories continue to adopt a within-person perspective, analyzing human behavior by reference to individual-level biological processes, personal dispositions, or cognitive experiences. Interdependence theory is an important antidote to this actor-focused bias. Interdependence theory identifies the most important characteristics of interpersonal situations via a comprehensive analysis of situation structure and describes the implications of structure for understanding intrapersonal and interpersonal processes. Situation structure matters because it is the interpersonal reality within which motives are activated, toward which cognition is oriented and around which interaction unfolds. This paper describes key principles of interdependence structure and processes, and illustrates the utility of an interdependence theoretic analysis via a review of four phenomena – regulatory fit, persistence in the face of dissatisfaction, tit-for-tat versus generosity, and the origins and consequences of trust. Most},
	file         = {:Users/yosef/Downloads/Social   Personality Psych - 2008 - Rusbult - Why We Need Interdependence Theory.pdf:pdf}
}
@article{VanLange2014,
	title        = {{Interdependence theory.}},
	author       = {{Van Lange}, Paul A. M. and Balliet, Daniel},
	year         = 2014,
	journal      = {APA handbook of personality and social psychology, Volume 3: Interpersonal relations.},
	pages        = {65--92},
	doi          = {10.1037/14344-003},
	abstract     = {One of the classic theories in the social and behavioral sciences is interdependence theory, originally developed by John Thibaut and Harold Kelley (1959). Over the past decades, this theory has been extended, first by Kelley and Thibaut (1978) and then by others, into a comprehensive theory of social interaction. In this chapter, we provide a history and overview of interdependence theory and discuss the primary features of the theory, including (a) the principle of structure (the situation); (b) the principle of transformation, or what people make of the situation; (c) the principle of interaction, being determined by the interacting people and (objective features) of the situation; and (d) the principle of adaptation, suggesting that repeated social interaction experiences yield adaptations that are reflected in relatively stable orientations to adopt particular transformations in similar situations. These principles are illustrated by research on topics such as power and dependence, cooperation and conflict, trust and distrust, attribution and self-presentation, and stereotyping and information seeking. We conclude by outlining broader implications of interdependence theory as well as issues for future research, such as understanding the intricate relation between material and personal outcomes or articulating how interdependence theory helps us to understand the social mind.},
	file         = {:Users/yosef/Downloads/van-lange_balliet-interdependence-theory-chapter.pdf:pdf}
}
@article{thibaut_59,
	title        = {{The Social Psychology of Groups.}},
	author       = {Thibaut, John W and Kelley, Harold H},
	year         = 1959,
	publisher    = {John Wiley},
	address      = {New York},
	mendeley-groups = {ATC}
}
@misc{kelley_78,
	title        = {{Interpersonal Relations: A theory of interdependence}},
	author       = {Kelley, H H and Thibaut, J W},
	year         = 1978,
	publisher    = {John Wiley & Sons},
	address      = {New York, NY},
	mendeley-groups = {ATC}
}
@article{deortentiis2013cohesion,
	title        = {Cohesion and satisfaction as mediators of the team trust--team effectiveness relationship: An interdependence theory perspective},
	author       = {DeOrtentiis, Philip S and Summers, James K and Ammeter, Anthony P and Douglas, Ceasar and Ferris, Gerald R},
	year         = 2013,
	journal      = {Career Development International},
	publisher    = {Emerald Group Publishing Limited}
}
@article{Battigalli2005,
	title        = {{Dynamic Psychological Games Dynamic Psychological Games ∗}},
	author       = {Battigalli, Pierpaolo and Dufwenberg, Martin},
	year         = 2005,
	number       = {April},
	file         = {:Users/yosef/Downloads/287.pdf:pdf},
	mendeley-groups = {Trust_Game_Theory}
}
@article{Pesendorfer2005,
	title        = {{The Canonical Type Space for}},
	author       = {Pesendorfer, Wolfgang},
	year         = 2005,
	file         = {:Users/yosef/Downloads/14032005.pdf:pdf},
	mendeley-groups = {Trust_Game_Theory}
}
@article{Rabin2020,
	title        = {{Incorporating Fairness into Game Theory and Economics}},
	author       = {Rabin, Matthew},
	year         = 2020,
	volume       = 83,
	number       = 5,
	pages        = {1281--1302},
	file         = {:Users/yosef/Downloads/2117561-2.pdf:pdf},
	mendeley-groups = {Trust_Game_Theory}
}
@article{Battigalli2009,
	title        = {{Dynamic psychological games}},
	author       = {Battigalli, Pierpaolo and Dufwenberg, Martin},
	year         = 2009,
	journal      = {Journal of Economic Theory},
	publisher    = {Elsevier Inc.},
	volume       = 144,
	number       = 1,
	pages        = {1--35},
	doi          = {10.1016/j.jet.2008.01.004},
	issn         = {0022-0531},
	url          = {http://dx.doi.org/10.1016/j.jet.2008.01.004},
	file         = {:Users/yosef/Downloads/1-s2.0-S0022053108000379-main.pdf:pdf},
	keywords     = {belief-dependent motivation,dynamic interactive,extensive-form solution concepts,psychological games},
	mendeley-groups = {Trust_Game_Theory}
}
@book{Bacharach2007,
	title        = {{The self-fulfilling property of trust: an experimental study}},
	author       = {Bacharach, Michael and Guerra, Gerardo and Zizzo, Daniel John},
	year         = 2007,
	pages        = {349--388},
	doi          = {10.1007/s11238-007-9043-5},
	isbn         = 1123800790,
	file         = {:Users/yosef/Downloads/Bacharach2007_Article_TheSelf-FulfillingPropertyOfTr-3.pdf:pdf},
	keywords     = {belief,kindness,need to trust,trust,trust responsiveness},
	mendeley-groups = {Trust_Game_Theory}
}


@article{wiegmann2001automated,
	title        = {Automated diagnostic aids: The effects of aid reliability on users' trust and reliance},
	author       = {Wiegmann, Douglas A and Rich, Aaron and Zhang, Hui},
	year         = 2001,
	journal      = {Theoretical Issues in Ergonomics Science},
	publisher    = {Taylor \& Francis},
	volume       = 2,
	number       = 4,
	pages        = {352--367}
}

@article{Costa2011,
	title        = {{Measuring trust in teams : Development and validation of a multifaceted measure of formative and reflective indicators of team trust Measuring trust in teams : Development and validation indicators of team trust}},
	author       = {Costa, Ana Cristina and Anderson, Neil and Costa, Ana Cristina and Anderson, Neil},
	year         = 2011,
	volume       = {0643},
	doi          = {10.1080/13594320903272083},
	file         = {:Users/yosef/Downloads/Measuring trust in teams Development and validation of a multifaceted measure of formative and reflective indicators of team trust.pdf:pdf},
	mendeley-groups = {Trust_Metrics_Analysis,Other_Trust_Metric_Papers}
}



@article{horvath1989development,
	title        = {Development and validation of the Working Alliance Inventory.},
	author       = {Horvath, Adam O and Greenberg, Leslie S},
	year         = 1989,
	journal      = {Journal of counseling psychology},
	publisher    = {American Psychological Association},
	volume       = 36,
	number       = 2,
	pages        = 223
}
@book{gillespie2003measuring,
	title        = {Measuring trust in working relationships: The behavioral trust inventory},
	author       = {Gillespie, Nicole},
	year         = 2003,
	publisher    = {Melbourne Business School}
}

@article{Daniel2013,
	title        = {{Simplified Human-Robot Interaction: Modeling and Evaluation}},
	author       = {Daniel, Balazs and Thomessen, Trygve and Korondi, Peter},
	year         = 2013,
	journal      = {Modeling, Identification and Control: A Norwegian Research Bulletin},
	volume       = 34,
	number       = 4,
	pages        = {199--211},
	doi          = {10.4173/mic.2013.4.4},
	issn         = {0332-7353},
	abstract     = {In this paper a novel concept of human-robot interaction (HRI) modeling is proposed. Including factors like trust in automation, situational awareness, expertise and expectations a new user experience framework is formed for industrial robots. Service Oriented Robot Operation, proposed in a previous paper, creates an abstract level in HRI and it is also included in the framework. This concept is evaluated with exhaustive tests. Results prove that significant improvement in task execution may be achieved and the new system is more usable for operators with less experience with robotics; personnel specific for small and medium enterprises (SMEs).},
	file         = {:Users/yosef/Downloads/MIC-2013-4-4.pdf:pdf},
	keywords     = {graphical user interface,human-robot interaction,industrial robotics,usability},
	mendeley-groups = {Other_Trust_Metric_Papers}
}

@article{Ekman2019,
	title        = {{Exploring automated vehicle driving styles as a source of trust information}},
	author       = {Ekman, Fredrick and Johansson, Mikael and Blig{\aa}rd, Lars Ola and Karlsson, Mari Anne and Str{\"{o}}mberg, Helena},
	year         = 2019,
	journal      = {Transportation Research Part F: Traffic Psychology and Behaviour},
	publisher    = {Elsevier Ltd},
	volume       = 65,
	pages        = {268--279},
	doi          = {10.1016/j.trf.2019.07.026},
	issn         = 13698478,
	url          = {https://doi.org/10.1016/j.trf.2019.07.026},
	abstract     = {Trust is important for users' acceptance and adoption of automated vehicles (AVs). Previous research has mainly focused on what information affects user trust in AVs and how the user's trust is affected by the way the content is communicated. However, recent studies have shown that trust may also be affected by the AV's driving style. The aim of the study was to further investigate if and how the vehicle's driving style affects user trust in AVs and, in particular, how this is expressed by users. An experiment involving 18 participants, using a Wizard of Oz setup and within a subject design, was conducted comparing two different driving styles, ‘Defensive' and ‘Aggressive'. Trust was measured using a mixed method research design including momentaneous trust ratings and think-aloud procedures while driving, a post-run trust questionnaire as well as trust curve sketching and a personal interview. The results show that driving style had an effect on user trust and that the ‘Defensive' driving style was perceived as more trustworthy, in part because it was deemed more predictable than the ‘Aggressive' driving style. Furthermore, participants expressed trust in using affective, analogical and analytic responses, the two former during the test runs and the latter directly after each test run. The interview after the completion produced a more mixed result. By combining different data collection methods, a nuanced picture of the trust formation process and users' trust in AVs was obtained. The study concludes that it is important to consider the vehicle performance information provided by the vehicle's driving style so as to create user trust in AVs.},
	file         = {:Users/yosef/Downloads/1-s2.0-S1369847818306594-main.pdf:pdf},
	keywords     = {Automated vehicles,Driving style,Multi-methods approach,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}




@article{meng2018towards,
	title        = {Towards Bayesian-based trust management for insider attacks in healthcare software-defined networks},
	author       = {Meng, Weizhi and Choo, Kim-Kwang Raymond and Furnell, Steven and Vasilakos, Athanasios V and Probst, Christian W},
	year         = 2018,
	journal      = {IEEE Transactions on Network and Service Management},
	publisher    = {IEEE},
	volume       = 15,
	number       = 2,
	pages        = {761--773}
}
@article{coker2017ineffectiveness,
	title        = {The ineffectiveness of counterclaim advertising for increasing consumer sentiment},
	author       = {Coker, Brent},
	year         = 2017,
	journal      = {Journal of Consumer Behaviour},
	publisher    = {Wiley Online Library},
	volume       = 16,
	number       = 1,
	pages        = {34--41}
}

@article{Sheridan2005,
	title        = {{Human-automation interaction}},
	author       = {Sheridan, Thomas B and Parasuraman, Raja},
	year         = 2005,
	journal      = {Reviews of human factors and ergonomics},
	publisher    = {Sage Publications},
	volume       = 1,
	number       = 1,
	pages        = {89--129},
	annote       = {NULL}
}



@article{Shachak2019,
	title        = {{Beyond TAM and UTAUT: Future directions for HIT implementation research}},
	author       = {Shachak, Aviv and Kuziemsky, Craig and Petersen, Carolyn},
	year         = 2019,
	journal      = {Journal of Biomedical Informatics},
	publisher    = {Elsevier},
	volume       = 100,
	number       = {October},
	pages        = 103315,
	doi          = {10.1016/j.jbi.2019.103315},
	issn         = 15320464,
	abstract     = {The Technology Acceptance Model (TAM) and Unified Theory of Acceptance and Use of Technology (UTAUT) have been used widely in studies of health information technology (HIT) implementation. However, TAM and UTAUT have also been criticized for being overly simplistic (TAM) and for taking a narrow perspective, which focuses only on individual adopters' beliefs, perceptions and usage intention. Furthermore, with thousands of studies using these theories, their contribution to knowledge has reached a plateau. In this commentary, we discuss some of the criticism of TAM and UTAUT, and argue that biomedical informatics research would benefit from shifting attention from these theories to multi-dimensional approaches that can better capture the complexity of issues surrounding implementation and use of HIT. We propose a number of future undertakings which, in our opinion, are more likely to move the field forward.},
	file         = {:Users/yosef/Downloads/1-s2.0-S1532046419302345-main.pdf:pdf},
	keywords     = {Complexity science,Health information technology implementation,Technology acceptance model,Technology use,Unified theory of acceptance and use of technology},
	pmid         = 31629923
}

@incollection{orne1996demand,
	title        = {Demand characteristics},
	author       = {Orne, Martin T},
	year         = 1996,
	booktitle    = {Introducing psychological research},
	publisher    = {Springer},
	pages        = {395--401}
}


@article{wheeless,
	title        = {{The Measurement of Trust and Its Relationship to Self-Disclosure}},
	author       = {Wheeless, Lawrence R. and Grotz, Janis},
	year         = 2006,
	month        = {03},
	journal      = {Human Communication Research},
	volume       = 3,
	number       = 3,
	pages        = {250--257},
	doi          = {10.1111/j.1468-2958.1977.tb00523.x},
	issn         = {0360-3989},
	abstract     = {{This study examined the relationship of trust to self-disclosure. A measure of individualized trust was developed and used in conjunction with a multidimensional measure of disclosure to reassess the relationship between the two. A modest, linear relationship between individualized trust and various dimensions of self-disclosure was discovered. Moreover, a higher level of trust (as opposed to lesser trust as well as distrust) was found to be associated with more consciously intended disclosure and a greater amount of disclosure.}},
	eprint       = {https://academic.oup.com/hcr/article-pdf/3/3/250/22344414/jhumcom0250.pdf}
}

@article{Hoffman2013,
	title        = {{Trust in automation}},
	author       = {Hoffman, Robert R. and Johnson, Matthew and Bradshaw, Jeffrey M. and Underbrink, Al},
	year         = 2013,
	journal      = {IEEE Intelligent Systems},
	volume       = 28,
	number       = 1,
	pages        = {84--88},
	doi          = {10.1109/MIS.2013.24},
	issn         = 15411672,
	abstract     = {This essay focuses on trust in the automation within macrocognitive work systems. The authors emphasize the dynamics of trust. They consider numerous different meanings or kinds of trust, and different modes of operation in which trust dynamics play a role. Their goal is to contribute to the development of a methodology for designing and analyzing collaborative human-centered work systems, a methodology that might promote both trust 'calibration' and appropriate reliance. The analysis suggests an ontology for what the authors call 'active exploration for trusting' (AET). {\textcopyright} 2001-2011 IEEE.},
	file         = {:Users/yosef/Downloads/465416-2.pdf:pdf},
	keywords     = {active exploration,competence envelopes,dynamics,macrocognitive work systems,trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Molloy_1996_Monitoringautomatedsystem,
	title        = {{Monitoring an automated system for a single failure: vigilance and task complexity effects}},
	author       = {Molloy, R and Parasuraman, R},
	year         = 1996,
	journal      = {Human Factors},
	volume       = 38,
	pages        = {311--322},
	annote       = {NULL}
}
@article{Parasuraman2000,
	title        = {{A model for types and levels of human interaction with automation}},
	author       = {Parasuraman, R and Sheridan, T B and Wickens, C D},
	year         = 2000,
	month        = {may},
	journal      = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	volume       = 30,
	number       = 3,
	pages        = {286--297},
	doi          = {10.1109/3468.844354},
	issn         = {1083-4427},
	annote       = {NULL},
	keywords     = {automation;human factors;man-machine systems;user},
	mendeley-groups = {WeRobot,Grant,NASA}
}
@article{Ghazizadeh2012,
	title        = {{Extending the Technology Acceptance Model to assess automation}},
	author       = {Ghazizadeh, Mahtab and Lee, John D. and Boyle, Linda Ng},
	year         = 2012,
	journal      = {Cognition, Technology and Work},
	volume       = 14,
	number       = 1,
	pages        = {39--49},
	doi          = {10.1007/s10111-011-0194-3},
	issn         = 14355558,
	abstract     = {Often joint human-automation performance depends on the factors influencing the operator's tendency to rely on and comply with automation. Although cognitive engineering (CE) researchers have studied automation acceptance as related to task-technology compatibility and human-technology coagency, information system (IS) researchers have evaluated user acceptance of technology, using the Technology Acceptance Model (TAM). The parallels between the two views suggest that the user acceptance perspective from the IS community can complement the human-automation interaction perspective from the CE community. TAM defines constructs that govern acceptance and provides a framework for evaluating a broad range of factors influencing technology acceptance and reliance. TAM is extensively used by IS researchers in various applications and it can be applied to assess the effect of trust and other factors on automation acceptance. Likewise, extensions to the TAM framework use the constructs of task-technology compatibility and past experience to extend its description of the role of human-automation interaction in automation adoption. We propose the Automation Acceptance Model (AAM) to draw upon the IS and CE perspectives and take into account the dynamic and multi-level nature of automation use, highlighting the influence of use on attitudes that complements the more common view that attitudes influence use. {\textcopyright} 2011 Springer-Verlag London Limited.},
	file         = {:Users/yosef/Downloads/Ghazizadeh2012_Article_ExtendingTheTechnologyAcceptan.pdf:pdf},
	keywords     = {Automation Acceptance Model (AAM),Cognitive engineering,Task-technology compatibility,Technology Acceptance Model (TAM),Technology acceptance,Trust in automation},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@phdthesis{Chancey2016TheEO,
	title        = {The Effects of Alarm System Errors on Dependence: Moderated Mediation of Trust With and Without Risk},
	author       = {Eric T. Chancey},
	year         = 2016,
	school       = {Old Dominion University}
}
@article{Hammer2015,
	title        = {{Trust-based decision-making for smart and adaptive environments}},
	author       = {Hammer, Stephan and Wi{\ss}ner, Michael and Andr{\'{e}}, Elisabeth},
	year         = 2015,
	journal      = {User Modeling and User-Adapted Interaction},
	publisher    = {Springer Netherlands},
	volume       = 25,
	number       = 3,
	pages        = {267--293},
	doi          = {10.1007/s11257-015-9160-8},
	isbn         = 1125701591608,
	issn         = 15731391,
	abstract     = {Smart environments are able to support users during their daily life. For example, smart energy systems can be used to support energy saving by controlling devices, such as lights or displays, depending on context information, such as the brightness in a room or the presence of users. However, proactive decisions should also match the users' preferences to maintain the users' trust in the system. Wrong decisions could negatively influence the users' acceptance of a system and at worst could make them abandon the system. In this paper, a trust-based model, called User Trust Model (UTM), for automatic decision-making is proposed, which is based on Bayesian networks. The UTM's construction, the initialization with empirical data gathered in an online survey, and its integration in an office setting are described. Furthermore, the results of a live study and a live survey analyzing the users' experience and acceptance are presented.},
	file         = {:Users/yosef/Downloads/Hammer2015_Article_Trust-basedDecision-makingForS.pdf:pdf},
	keywords     = {Computational trust,Context awareness,Energy saving,Proactive systems},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@misc{IdramaniL.SinghRobertMalloy1993,
	title        = {{Automation Induced Complacency}},
	author       = {Singh, Idramani L. and Malloy, Robert and Parasuraman, Raja},
	year         = 1993,
	booktitle    = {The International Journal of Aviation Psychology},
	file         = {:Users/yosef/Downloads/ContentServer.asp.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Jøsang2007,
	title        = {{A survey of trust and reputation systems for online service provision}},
	author       = {J{\o}sang, Audun and Ismail, Roslan and Boyd, Colin},
	year         = 2007,
	journal      = {Decision Support Systems},
	volume       = 43,
	number       = 2,
	pages        = {618--644},
	doi          = {10.1016/j.dss.2005.05.019},
	issn         = {01679236},
	abstract     = {Trust and reputation systems represent a significant trend in decision support for Internet mediated service provision. The basic idea is to let parties rate each other, for example after the completion of a transaction, and use the aggregated ratings about a given party to derive a trust or reputation score, which can assist other parties in deciding whether or not to transact with that party in the future. A natural side effect is that it also provides an incentive for good behaviour, and therefore tends to have a positive effect on market quality. Reputation systems can be called collaborative sanctioning systems to reflect their collaborative nature, and are related to collaborative filtering systems. Reputation systems are already being used in successful commercial online applications. There is also a rapidly growing literature around trust and reputation systems, but unfortunately this activity is not very coherent. The purpose of this article is to give an overview of existing and proposed systems that can be used to derive measures of trust and reputation for Internet transactions, to analyse the current trends and developments in this area, and to propose a research agenda for trust and reputation systems. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0167923605000849-main.pdf:pdf},
	keywords     = {Collaboration,Decision,E-commerce,Reputation,Security,Transitivity,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Khadangi2016,
	title        = {{Biased sampling from facebook multilayer activity network using learning automata}},
	author       = {Khadangi, Ehsan and Bagheri, Alireza and Shahmohammadi, Amin},
	year         = 2016,
	journal      = {Applied Intelligence},
	publisher    = {Applied Intelligence},
	volume       = 45,
	number       = 3,
	pages        = {829--849},
	doi          = {10.1007/s10489-016-0784-0},
	issn         = 15737497,
	url          = {http://dx.doi.org/10.1007/s10489-016-0784-0},
	abstract     = {Although much research has been devoted to unbiased sampling of various networks, bias is not always disadvantageous, but sometimes useful. Especially for many real-world applications such as detecting influential nodes, spam users, and the most trustful people, it is preferred to sample users with special properties. Since sampling from friendship network alone cannot collect these important nodes appropriately, one may use interactions occurred among users. This paper deals with biased sampling of multilayer activity network. The proposed method initially learns the transition probabilities according to the considered application using learning automata. Then we sample the graph by running an application-based random walk following the learnt probabilities, in order to be guided to suitable nodes and collect their information. At last, the performance of the proposed method in terms of different applications such as fame, spam, and trust is evaluated and compared with those of common sampling algorithms. According to the experiments done, biased sampling method based on learning automata outperforms all other sampling approaches including simple random walk, Metropolis-Hastings random walk, BFS, forest fire, degree, and uniform sampling in terms of all the evaluation measures. To the best of our knowledge, our method is the first and only biased sampling method which can be used in a multilayer activity network.},
	file         = {:Users/yosef/Downloads/Khadangi2016_Article_BiasedSamplingFromFacebookMult.pdf:pdf},
	keywords     = {Activity network,Facebook,Learning automata,Network sampling,Social media marketing,Social network analysis},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Kiesler2008,
	title        = {{Anthropomorphic interactions with a robot and robot-like agent}},
	author       = {Kiesler, Sara and Powers, Aaron and Fussell, Susan R. and Torrey, Cristen},
	year         = 2008,
	journal      = {Social Cognition},
	volume       = 26,
	number       = 2,
	pages        = {169--181},
	doi          = {10.1521/soco.2008.26.2.169},
	issn         = {0278016X},
	abstract     = {People's physical embodiment and presence increase their salience and importance. We predicted people would anthropomorphize an embodied humanoid robot more than a robot-like agent, and a collocated more than a remote robot. A robot or robot-like agent interviewed participants about their health. Participants were either present with the robot/agent, or interacted remotely with the robot/agent projected life-size on a screen. Participants were more engaged, disclosed less undesirable behavior, and forgot more with the robot versus the agent. They ate less and anthropomorphized most with the collocated robot. Participants interacted socially and attempted conversational grounding with the robot/agent though aware it was a machine. Basic questions remain about how people resolve the ambiguity of interacting with a humanlike nonhuman.},
	file         = {:Users/yosef/Downloads/2008_anthro-interactions-robot.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{McCloskey2007,
	title        = {{The importance of ease of use, usefulness, and trust to online consumers: An examination of the technology acceptance model with older consumers}},
	author       = {McCloskey, Donna Weaver},
	year         = 2007,
	journal      = {End User Computing Challenges and Technologies: Emerging Tools and Applications},
	volume       = 65,
	pages        = {259--276},
	doi          = {10.4018/978-1-59904-295-4.ch015},
	isbn         = 9781599042954,
	abstract     = {This research examines electronic commerce participation and attitudes by older Americans. Questionnaires were distributed at a large retirement community and several senior centers located in Pennsylvania. The sample of 110 respondents ranged in age from 52 to 87. Fifty-nine percent reported purchasing an item online in the last 6 months. The Technology Acceptance Model (TAM) was used and modified to examine the impact attitudes concerning ease of use, usefulness and trust had on electronic commerce usage. Usefulness and trust were found to have a positive, direct affect on usage. Ease of use had significant impacts on usefulness and trust had a significant impact on both ease of use and usefulness. The chapter concludes with a discussion of these results, study limitations, and directions for future research. {\textcopyright} 2008, IGI Global.},
	file         = {:Users/yosef/Downloads/The_Importance_of_Ease_of_UseUsefulnessandTrust-2.pdf:pdf},
	keywords     = {elderly and mature market,electronic commerce,online shopping,technology},
	mendeley-groups = {Other_Trust_Metric_Papers}
}

@article{Nie2012,
	title        = {{Can you hold my hand?: Physical warmth in human-robot interaction}},
	author       = {Nie, Jiaqi and Pak, Michelle and Marin, Angie Lorena and Sundar, S. Shyam},
	year         = 2012,
	journal      = {HRI'12 - Proceedings of the 7th Annual ACM/IEEE International Conference on Human-Robot Interaction},
	publisher    = {IEEE},
	pages        = {201--202},
	doi          = {10.1145/2157689.2157755},
	isbn         = 9781450310635,
	abstract     = {This study investigates whether the temperature of a robot's hand can affect perceptions of the robot as a companion. Our research empirically analyzes the responses of 39 individuals randomly assigned to one of three conditions: (1) holding a warm robot hand or (2) holding a cold robot hand or (3) not holding a robot hand. The effects of this simulated 'human touch' on HRI were examined in the context of viewing a horror film clip. Results suggest that experiences of physical warmth and handholding increase feelings of friendship and trust toward the robot. However, the discrepancy between the expectation of an actual human touch and the mechanical appearance of a robot could result in negative effects. {\textcopyright} 2012 Authors.},
	file         = {:Users/yosef/Downloads/06249527.pdf:pdf},
	keywords     = {hri,human touch,robot handholding,robot warmth},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Petersen2018,
	title        = {{THE INFLUENCE OF RISK ON DRIVER TRUST IN AUTONOMOUS DRIVING SYSTEMS}},
	author       = {Petersen, Luke and Yang, X Jessie and Arbor, Ann and Jr, Lionel P Robert},
	year         = 2018,
	pages        = {1--10},
	file         = {:Users/yosef/Downloads/98_Peterson_Luke-Final.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers},
    journal = {Autonomous Ground Systems Technical Session of the Ground Vehicle Systems Engineering and Technology Symposium}
}

@book{Rocha2013,
	title        = {{Advances in Intelligent Systems and Computing: Preface}},
	author       = {Rocha, {\'{A}}lvaro and Correia, Ana Maria and Wilson, Tom and Stroetmann, Karl A.},
	year         = 2013,
	booktitle    = {Advances in Intelligent Systems and Computing},
	volume       = {206 AISC},
	doi          = {{10.1007/978-3-642-36981-0}},
	isbn         = 9783642369803,
	issn         = 21945357,
	file         = {:Users/yosef/Downloads/Trends_PAAMS_2014-2.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}

@article{Srinivasa2021,
	title        = {{‘Help me please'}},
	author       = {Srinivasa, Siddhartha S.},
	year         = 2021,
	journal      = {The Routledge International Handbook of Penal Abolition},
	pages        = {119--130},
	doi          = {10.4324/9780429425035-18},
	isbn         = 9781450333627,
	abstract     = {Robots that can leverage help from people could accomplish much more than robots that cannot. We present the results of two experiments that examine how robots can more effec- tively request help from people. Study 1 is a video prototype experiment (N=354), investigating the effectiveness of four linguistic politeness strategies as well as the effects of social status (equal, low), size of request (large, small), and robot fa- miliarity (high, low) on people's willingness to help a robot. The results of this study largely support Politeness Theory and the Computers as Social Actors paradigm. Study 2 is a physical human-robot interaction experiment (N=48), exam- ining the impact of source orientation (autonomous, single operator, multiple operators) on people's behavioral willing- ness to help the robot. People were nearly 50% faster to help the robot if they perceived it to be autonomous rather than being teleoperated. Implications for research design, theory, and methods are discussed.},
	file         = {:Users/yosef/Downloads/2858036.2858217.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}


@article{Bata2018,
	title        = {{Mobile social networking and salesperson maladaptive dependence behaviors}},
	author       = {Bata, Hatem and Pentina, Iryna and Tarafdar, Monideepa and Pullins, Ellen Bolman},
	year         = 2018,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier Ltd},
	volume       = 81,
	pages        = {235--249},
	doi          = {10.1016/j.chb.2017.12.025},
	issn         = {07475632},
	url          = {https://doi.org/10.1016/j.chb.2017.12.025},
	abstract     = {This study investigates technology dependence associated with the work-related use of mobile social networking (MSN) by salespeople. A scale for maladaptive technology dependence behaviors (MTDB) is developed and empirically validated using survey data from 242 mid-level sales managers in the US. Personal and job-related antecedents, as well as consequences of MTDB for sales outcomes, are also examined. Results suggest that emotional attachment to MSN and perceptions of its greater affordances for task accomplishment may lead to maladaptive behaviors of overreliance on MSN for job completion, blind trust, cognitive absorption and dysfunctional use. These associations increase in organizations with competitive psychological climate. Findings also show that using MSN for prospecting does not lead to maladaptive dependence, as opposed to using it for customer relationship maintenance. Salespeople using MSN for relationship maintenance exhibit more maladaptive behaviors if they experience work-related role stress. Finally, salespeople who exhibit MTDB are less likely to complete their assignments and participate in teamwork. These findings provide tools for organizations to develop technology use policies, design sales training, and enhance the work environment. Future studies can examine dependencies on others types of technologies (CRM, marketing automation, etc.), and in other contexts (online retailing, social media analytics, etc.)},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563217307069-main-2.pdf:pdf},
	keywords     = {Dark side of social networking,Maladaptive technology dependence,Mobile social media,Professional sales,Technology addiction},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Alexander2018,
	title        = {{Why trust an algorithm? Performance, cognition, and neurophysiology}},
	author       = {Alexander, Veronika and Blinder, Collin and Zak, Paul J.},
	year         = 2018,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier},
	volume       = 89,
	number       = {July},
	pages        = {279--288},
	doi          = {10.1016/j.chb.2018.07.026},
	issn         = {07475632},
	url          = {https://doi.org/10.1016/j.chb.2018.07.026},
	abstract     = {OBJECTIVE: We measured neurophysiologic responses and task performance while participants solved mazes after choosing whether to adopt an imperfect helper algorithm. BACKGROUND: Every day we must decide whether to trust or distrust algorithms. Will an algorithm improve our performance on a task? What if we trust it too much? METHOD: Participants had to pay to use the algorithm and were aware that it offered imperfect help. We varied the information about the algorithm to assess the factors that affected adoption while measuring participants' peripheral neurophysiology. RESULTS: We found that information about previous adoption by others had a larger effect on adoption and resulted in lower cognitive load than did information about algorithm accuracy. The neurophysiologic measurement showed that algorithm adoption without any information resulted in low cognitive engagement during the task and impaired task performance. Conversely, algorithm use after information about others' use improved engagement and performance. CONCLUSION: By objectively measuring cognitive load and task performance, we identified how to increase algorithm adoption while sustaining high performance by human operators. APPLICATION: Algorithm adoption can be increased by sharing previous use information and performance improved by providing a reason to monitor the algorithm. Precis: We collected neurophysiologic data while varying information about an algorithm that assisted participants in solving a timed and incentivized maze and found that information about prior use by others more effectively influenced adoption, reduced cognitive load, and improved performance compared to algorithm accuracy information.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563218303480-main.pdf:pdf},
	keywords     = {Automation,Computers,Decisions,Neurophysiology},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Charalambous2016,
	title        = {{The Development of a Scale to Evaluate Trust in Industrial Human-robot Collaboration}},
	author       = {Charalambous, George and Fletcher, Sarah and Webb, Philip},
	year         = 2016,
	journal      = {International Journal of Social Robotics},
	publisher    = {Springer Netherlands},
	volume       = 8,
	number       = 2,
	pages        = {193--209},
	doi          = {10.1007/s12369-015-0333-8},
	issn         = 18754805,
	file         = {:Users/yosef/Downloads/Charalambous2016_Article_TheDevelopmentOfAScaleToEvalua.pdf:pdf},
	keywords     = {Human-robot collaboration,Industrial robot,Trust scale},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Ekman2019,
	title        = {{Exploring automated vehicle driving styles as a source of trust information}},
	author       = {Ekman, Fredrick and Johansson, Mikael and Blig{\aa}rd, Lars Ola and Karlsson, Mari Anne and Str{\"{o}}mberg, Helena},
	year         = 2019,
	journal      = {Transportation Research Part F: Traffic Psychology and Behaviour},
	publisher    = {Elsevier Ltd},
	volume       = 65,
	pages        = {268--279},
	doi          = {10.1016/j.trf.2019.07.026},
	issn         = 13698478,
	url          = {https://doi.org/10.1016/j.trf.2019.07.026},
	abstract     = {Trust is important for users' acceptance and adoption of automated vehicles (AVs). Previous research has mainly focused on what information affects user trust in AVs and how the user's trust is affected by the way the content is communicated. However, recent studies have shown that trust may also be affected by the AV's driving style. The aim of the study was to further investigate if and how the vehicle's driving style affects user trust in AVs and, in particular, how this is expressed by users. An experiment involving 18 participants, using a Wizard of Oz setup and within a subject design, was conducted comparing two different driving styles, ‘Defensive' and ‘Aggressive'. Trust was measured using a mixed method research design including momentaneous trust ratings and think-aloud procedures while driving, a post-run trust questionnaire as well as trust curve sketching and a personal interview. The results show that driving style had an effect on user trust and that the ‘Defensive' driving style was perceived as more trustworthy, in part because it was deemed more predictable than the ‘Aggressive' driving style. Furthermore, participants expressed trust in using affective, analogical and analytic responses, the two former during the test runs and the latter directly after each test run. The interview after the completion produced a more mixed result. By combining different data collection methods, a nuanced picture of the trust formation process and users' trust in AVs was obtained. The study concludes that it is important to consider the vehicle performance information provided by the vehicle's driving style so as to create user trust in AVs.},
	file         = {:Users/yosef/Downloads/1-s2.0-S1369847818306594-main.pdf:pdf},
	keywords     = {Automated vehicles,Driving style,Multi-methods approach,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{McCloskey2007,
	title        = {{The importance of ease of use, usefulness, and trust to online consumers: An examination of the technology acceptance model with older consumers}},
	author       = {McCloskey, Donna Weaver},
	year         = 2007,
	journal      = {End User Computing Challenges and Technologies: Emerging Tools and Applications},
	volume       = 65,
	pages        = {259--276},
	doi          = {10.4018/978-1-59904-295-4.ch015},
	isbn         = 9781599042954,
	abstract     = {This research examines electronic commerce participation and attitudes by older Americans. Questionnaires were distributed at a large retirement community and several senior centers located in Pennsylvania. The sample of 110 respondents ranged in age from 52 to 87. Fifty-nine percent reported purchasing an item online in the last 6 months. The Technology Acceptance Model (TAM) was used and modified to examine the impact attitudes concerning ease of use, usefulness and trust had on electronic commerce usage. Usefulness and trust were found to have a positive, direct affect on usage. Ease of use had significant impacts on usefulness and trust had a significant impact on both ease of use and usefulness. The chapter concludes with a discussion of these results, study limitations, and directions for future research. {\textcopyright} 2008, IGI Global.},
	file         = {:Users/yosef/Downloads/The_Importance_of_Ease_of_UseUsefulnessandTrust-2.pdf:pdf},
	keywords     = {elderly and mature market,electronic commerce,online shopping,technology},
	mendeley-groups = {Other_Trust_Metric_Papers}
}

@article{Kiesler2008,
	title        = {{Anthropomorphic interactions with a robot and robot-like agent}},
	author       = {Kiesler, Sara and Powers, Aaron and Fussell, Susan R. and Torrey, Cristen},
	year         = 2008,
	journal      = {Social Cognition},
	volume       = 26,
	number       = 2,
	pages        = {169--181},
	doi          = {10.1521/soco.2008.26.2.169},
	issn         = {0278016X},
	abstract     = {People's physical embodiment and presence increase their salience and importance. We predicted people would anthropomorphize an embodied humanoid robot more than a robot-like agent, and a collocated more than a remote robot. A robot or robot-like agent interviewed participants about their health. Participants were either present with the robot/agent, or interacted remotely with the robot/agent projected life-size on a screen. Participants were more engaged, disclosed less undesirable behavior, and forgot more with the robot versus the agent. They ate less and anthropomorphized most with the collocated robot. Participants interacted socially and attempted conversational grounding with the robot/agent though aware it was a machine. Basic questions remain about how people resolve the ambiguity of interacting with a humanlike nonhuman.},
	file         = {:Users/yosef/Downloads/2008_anthro-interactions-robot.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Jøsang2007,
	title        = {{A survey of trust and reputation systems for online service provision}},
	author       = {J{\o}sang, Audun and Ismail, Roslan and Boyd, Colin},
	year         = 2007,
	journal      = {Decision Support Systems},
	volume       = 43,
	number       = 2,
	pages        = {618--644},
	doi          = {10.1016/j.dss.2005.05.019},
	issn         = {01679236},
	abstract     = {Trust and reputation systems represent a significant trend in decision support for Internet mediated service provision. The basic idea is to let parties rate each other, for example after the completion of a transaction, and use the aggregated ratings about a given party to derive a trust or reputation score, which can assist other parties in deciding whether or not to transact with that party in the future. A natural side effect is that it also provides an incentive for good behaviour, and therefore tends to have a positive effect on market quality. Reputation systems can be called collaborative sanctioning systems to reflect their collaborative nature, and are related to collaborative filtering systems. Reputation systems are already being used in successful commercial online applications. There is also a rapidly growing literature around trust and reputation systems, but unfortunately this activity is not very coherent. The purpose of this article is to give an overview of existing and proposed systems that can be used to derive measures of trust and reputation for Internet transactions, to analyse the current trends and developments in this area, and to propose a research agenda for trust and reputation systems. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0167923605000849-main.pdf:pdf},
	keywords     = {Collaboration,Decision,E-commerce,Reputation,Security,Transitivity,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bailey2007,
	title        = {{Automation-induced complacency for monitoring highly reliable systems: The role of task complexity, system experience, and operator trust}},
	author       = {Bailey, N. R. and Scerbo, M. W.},
	year         = 2007,
	journal      = {Theoretical Issues in Ergonomics Science},
	volume       = 8,
	number       = 4,
	pages        = {321--348},
	doi          = {10.1080/14639220500535301},
	issn         = {1464536X},
	abstract     = {The increase in quantity and complexity of advanced automated systems has generated new concerns surrounding automation-induced complacency, or the difficulties operators have monitoring the status of automated systems. The present investigation consists of two studies that assessed the impact of system reliability, monitoring complexity, operator trust, and system experience on automation-induced complacency. In both studies, participants operated a manually controlled flight task while monitoring several simulated aircraft displays for failures. The ability of operators to detect a single automation failure over three experimental sessions was also assessed. Results indicated that realistic levels of system reliability severely impaired an operator's ability to monitor effectively. Further, as system experience increased, operator monitoring performance declined. The results also indicated that the complexity of the monitoring task heavily influenced operator monitoring, with poorer performance associated with more cognitively demanding tasks. Finally, results from both studies indicated that operator trust increased and monitoring performance decreased as a function of increasing system reliability. These results suggest that for highly reliable systems, increasing task complexity and extensive experience may severely impair an operator's ability to monitor for unanticipated system states. {\textcopyright} 2007 Taylor & Francis.},
	file         = {:Users/yosef/Downloads/Automation induced complacency for monitoring highly reliable systems the role of task complexity system experience and operator trust.pdf:pdf},
	keywords     = {Attention,Complacency,Monitoring,Trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Daniel2013,
	title        = {{Simplified Human-Robot Interaction: Modeling and Evaluation}},
	author       = {Daniel, Balazs and Thomessen, Trygve and Korondi, Peter},
	year         = 2013,
	journal      = {Modeling, Identification and Control: A Norwegian Research Bulletin},
	volume       = 34,
	number       = 4,
	pages        = {199--211},
	doi          = {10.4173/mic.2013.4.4},
	issn         = {0332-7353},
	abstract     = {In this paper a novel concept of human-robot interaction (HRI) modeling is proposed. Including factors like trust in automation, situational awareness, expertise and expectations a new user experience framework is formed for industrial robots. Service Oriented Robot Operation, proposed in a previous paper, creates an abstract level in HRI and it is also included in the framework. This concept is evaluated with exhaustive tests. Results prove that significant improvement in task execution may be achieved and the new system is more usable for operators with less experience with robotics; personnel specific for small and medium enterprises (SMEs).},
	file         = {:Users/yosef/Downloads/MIC-2013-4-4.pdf:pdf},
	keywords     = {graphical user interface,human-robot interaction,industrial robotics,usability},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Nie2012,
	title        = {{Can you hold my hand?: Physical warmth in human-robot interaction}},
	author       = {Nie, Jiaqi and Pak, Michelle and Marin, Angie Lorena and Sundar, S. Shyam},
	year         = 2012,
	journal      = {HRI'12 - Proceedings of the 7th Annual ACM/IEEE International Conference on Human-Robot Interaction},
	publisher    = {IEEE},
	pages        = {201--202},
	doi          = {10.1145/2157689.2157755},
	isbn         = 9781450310635,
	abstract     = {This study investigates whether the temperature of a robot's hand can affect perceptions of the robot as a companion. Our research empirically analyzes the responses of 39 individuals randomly assigned to one of three conditions: (1) holding a warm robot hand or (2) holding a cold robot hand or (3) not holding a robot hand. The effects of this simulated 'human touch' on HRI were examined in the context of viewing a horror film clip. Results suggest that experiences of physical warmth and handholding increase feelings of friendship and trust toward the robot. However, the discrepancy between the expectation of an actual human touch and the mechanical appearance of a robot could result in negative effects. {\textcopyright} 2012 Authors.},
	file         = {:Users/yosef/Downloads/06249527.pdf:pdf},
	keywords     = {hri,human touch,robot handholding,robot warmth},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Ghazizadeh2012,
	title        = {{Extending the Technology Acceptance Model to assess automation}},
	author       = {Ghazizadeh, Mahtab and Lee, John D. and Boyle, Linda Ng},
	year         = 2012,
	journal      = {Cognition, Technology and Work},
	volume       = 14,
	number       = 1,
	pages        = {39--49},
	doi          = {10.1007/s10111-011-0194-3},
	issn         = 14355558,
	abstract     = {Often joint human-automation performance depends on the factors influencing the operator's tendency to rely on and comply with automation. Although cognitive engineering (CE) researchers have studied automation acceptance as related to task-technology compatibility and human-technology coagency, information system (IS) researchers have evaluated user acceptance of technology, using the Technology Acceptance Model (TAM). The parallels between the two views suggest that the user acceptance perspective from the IS community can complement the human-automation interaction perspective from the CE community. TAM defines constructs that govern acceptance and provides a framework for evaluating a broad range of factors influencing technology acceptance and reliance. TAM is extensively used by IS researchers in various applications and it can be applied to assess the effect of trust and other factors on automation acceptance. Likewise, extensions to the TAM framework use the constructs of task-technology compatibility and past experience to extend its description of the role of human-automation interaction in automation adoption. We propose the Automation Acceptance Model (AAM) to draw upon the IS and CE perspectives and take into account the dynamic and multi-level nature of automation use, highlighting the influence of use on attitudes that complements the more common view that attitudes influence use. {\textcopyright} 2011 Springer-Verlag London Limited.},
	file         = {:Users/yosef/Downloads/Ghazizadeh2012_Article_ExtendingTheTechnologyAcceptan.pdf:pdf},
	keywords     = {Automation Acceptance Model (AAM),Cognitive engineering,Task-technology compatibility,Technology Acceptance Model (TAM),Technology acceptance,Trust in automation},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@book{Rocha2013,
	title        = {{Advances in Intelligent Systems and Computing: Preface}},
	author       = {Rocha, {\'{A}}lvaro and Correia, Ana Maria and Wilson, Tom and Stroetmann, Karl A.},
	year         = 2013,
	booktitle    = {Advances in Intelligent Systems and Computing},
	volume       = {206 AISC},
	doi          = {10.1007/978-3-642-36981-0},
	isbn         = 9783642369803,
	issn         = 21945357,
	file         = {:Users/yosef/Downloads/Trends_PAAMS_2014-2.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}

@article{Skarlatidou2013a,
	title        = {{Guidelines for trust interface design for public engagement Web GIS}},
	author       = {Skarlatidou, Artemis and Cheng, Tao and Haklay, Muki},
	year         = 2013,
	journal      = {International Journal of Geographical Information Science},
	volume       = 27,
	number       = 8,
	pages        = {1668--1687},
	doi          = {10.1080/13658816.2013.766336},
	issn         = 13658816,
	abstract     = {Attesting to the powerful capabilities and in technology trends, many scholars envisioned the consolidation of geographic information systems (GIS) into vital tools for disseminating spatial information. GIS are presently used to inform, advise and instruct users in several contexts and to further engage citizens in decision-making processes that can impact and sustain policy development. Interaction with these applications incorporates risk and uncertainty, which have been repeatedly identified as preconditions in nurturing trust perceptions and which instigate a user's decision to rely on a system and act on the provided information. Research studies consistently demonstrated that a trust-oriented interface design can facilitate the development of more trustworthy, mainly e-commerce, systems. Trust in the Web GIS context, despite its significance, has only relatively recently received some attention. A set of human-computer interaction (HCI) user-based studies revealed some Web GIS trustee attributes that influence non-experts' trust beliefs and found that when these are problematic or absent from interface design, users form irrational trust perceptions, which amplifies the risk and may impose dangers to the user. These Web GIS trustee attributes that influence non-experts' trust perceptions are formulated here into a set of trust guidelines. These are then evaluated using the PE-Nuclear tool, a Web GIS application, to inform the public about the site selection of a nuclear waste repository in the United Kingdom. Our preliminary results indicate that the proposed trust guidelines not only support the development of rational trust perceptions that protect non-experts from inappropriate use of Web GIS technology but also contribute towards improving interaction with such applications of public interest issue. {\textcopyright} 2013 Taylor and Francis Group, LLC.},
	file         = {:Users/yosef/Downloads/Guidelines for trust interface design for public engagement Web GIS-3.pdf:pdf},
	keywords     = {GIS,human-computer interaction,interface design,public engagement,trust},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bass2013,
	title        = {{The effect of information analysis automation display content on human judgment performance in noisy environments}},
	author       = {Bass, Ellen J. and Baumgart, Leigh A. and Shepley, Kathryn Klein},
	year         = 2013,
	journal      = {Journal of Cognitive Engineering and Decision Making},
	volume       = 7,
	number       = 1,
	pages        = {49--65},
	doi          = {10.1177/1555343412453461},
	issn         = 21695032,
	abstract     = {Displaying both the strategy that information analysis automation employs to makes its judgments and variability in the task environment may improve human judgment performance, especially in cases where this variability impacts the judgment performance of the information analysis automation. This work investigated the contribution of providing either information analysis automation strategy information, task environment information, or both, on human judgment performance in a domain where noisy sensor data are used by both the human and the information analysis automation to make judgments. In a simplified air traffic conflict prediction experiment, 32 participants made probability of horizontal conflict judgments under different display content conditions. After being exposed to the information analysis automation, judgment achievement significantly improved for all participants as compared to judgments without any of the automation's information. Participants provided with additional display content pertaining to cue variability in the task environment had significantly higher aided judgment achievement compared to those provided with only the automation's judgment of a probability of conflict. When designing information analysis automation for environments where the automation's judgment achievement is impacted by noisy environmental data, it may be beneficial to show additional task environment information to the human judge in order to improve judgment performance. Copyright {\textcopyright} 2012, Human Factors and Ergonomics Society.},
	file         = {:Users/yosef/Downloads/1555343412453461.pdf:pdf},
	keywords     = {Automation display content,Human-automated judge learning,Human-automation interaction,Judgment analysis},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@inproceedings{evers2008relational,
	title        = {Relational vs. group self-construal: Untangling the role of national culture in HRI},
	author       = {Evers, Vanessa and Maldonado, Heidy and Brodecki, Talia and Hinds, Pamela},
	year         = 2008,
	booktitle    = {2008 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
	pages        = {255--262},
	organization = {IEEE}
}


@article{terrin2003adjusting,
	title        = {Adjusting for publication bias in the presence of heterogeneity},
	author       = {Terrin, Norma and Schmid, Christopher H and Lau, Joseph and Olkin, Ingram},
	year         = 2003,
	journal      = {Statistics in medicine},
	publisher    = {Wiley Online Library},
	volume       = 22,
	number       = 13,
	pages        = {2113--2126}
}

@article{hak2016interpret,
	title        = {How to interpret results of meta-analysis},
	author       = {Hak, Tony and van Rhee, Henk and Suurmond, Robert},
	year         = 2016,
	journal      = {Available at SSRN 3241367}
}

@article{Costa2011,
	title        = {{Measuring trust in teams : Development and validation of a multifaceted measure of formative and reflective indicators of team trust Measuring trust in teams : Development and validation indicators of team trust}},
	author       = {Costa, Ana Cristina and Anderson, Neil and Costa, Ana Cristina and Anderson, Neil},
	year         = 2011,
	volume       = {0643},
	doi          = {10.1080/13594320903272083},
	file         = {:Users/yosef/Downloads/Measuring trust in teams Development and validation of a multifaceted measure of formative and reflective indicators of team trust.pdf:pdf},
	mendeley-groups = {Trust_Metrics_Analysis,Other_Trust_Metric_Papers}
}
@article{Bisantz2001,
	title        = {{Assessment of operator trust in and utilization of automated decision-aids under different framing conditions}},
	author       = {Bisantz, Ann M and Seong, Younho},
	year         = 2001,
	journal      = {International Journal of Industrial Ergonomics},
	volume       = 28,
	pages        = {85--97},
	url          = {https://ac.els-cdn.com/S0169814101000154/1-s2.0-S0169814101000154-main.pdf?_tid=ab8205ea-c0b0-11e7-8a06-00000aacb35e&acdnat=1509725249_efbd90d4d95baf3ba14220adc6f6bff8},
	abstract     = {Computerized aids may be used to support decision-making and control in a variety of complex, dynamic arenas. For instance, such systems have been introduced into industrial settings as the means to implement automated control or support decision-making activities such as fault detection and recovery. Of interest in these systems is the extent to which operators utilize and trust such systems, in terms of their ability to successfully control systems, or the information or decision support they provide, particularly under conditions of potential failure. A theoretical framework to describe potential factors affecting these issues, and an experiment to investigate the role of failure cause on trust and system utilization, are described. Results provide some support for factors in the theoretical framework, and also demonstrated the use of an empirically developed trust scale. Relevance to industry As manufacturing environments increasingly rely on computerized and automated systems for control and human operator support, it is necessary to understand the situational factors which could impact operators' use of such systems. This paper describes a framework which could be used to investigate trust in industrial automation settings, as well as a rating scale which could be applied. #},
	file         = {:Users/yosef/Downloads/1-s2.0-S0169814101000154-main.pdf:pdf},
	keywords     = {Automation,Decision-aids,Trust},
	mendeley-groups = {Jian_Experiments,Other_Trust_Metric_Papers}
}



@article{marsh2003role,
	title        = {The role of trust in information science and technology},
	author       = {Marsh, Stephen and Dibben, Mark R},
	year         = 2003,
	journal      = {Annual Review of Information Science and Technology (ARIST)},
	volume       = 37,
	pages        = {465--98}
}
@unpublished{Razin2022up,
	title        = {{A Measure of Trust? A Review of the Current State of Human-Automation Trust Questionnaires}},
	author       = {Razin, Yosef S. and Feigh, Karen M.},
	note         = {Revised and Resubmitted}
}

@article{dzindolet2003role,
	title        = {The role of trust in automation reliance},
	author       = {Dzindolet, Mary T and Peterson, Scott A and Pomranky, Regina A and Pierce, Linda G and Beck, Hall P},
	year         = 2003,
	journal      = {International journal of human-computer studies},
	publisher    = {Elsevier},
	volume       = 58,
	number       = 6,
	pages        = {697--718}
}


@article{Khadangi2016,
	title        = {{Biased sampling from facebook multilayer activity network using learning automata}},
	author       = {Khadangi, Ehsan and Bagheri, Alireza and Shahmohammadi, Amin},
	year         = 2016,
	journal      = {Applied Intelligence},
	publisher    = {Applied Intelligence},
	volume       = 45,
	number       = 3,
	pages        = {829--849},
	doi          = {10.1007/s10489-016-0784-0},
	issn         = 15737497,
	url          = {http://dx.doi.org/10.1007/s10489-016-0784-0},
	abstract     = {Although much research has been devoted to unbiased sampling of various networks, bias is not always disadvantageous, but sometimes useful. Especially for many real-world applications such as detecting influential nodes, spam users, and the most trustful people, it is preferred to sample users with special properties. Since sampling from friendship network alone cannot collect these important nodes appropriately, one may use interactions occurred among users. This paper deals with biased sampling of multilayer activity network. The proposed method initially learns the transition probabilities according to the considered application using learning automata. Then we sample the graph by running an application-based random walk following the learnt probabilities, in order to be guided to suitable nodes and collect their information. At last, the performance of the proposed method in terms of different applications such as fame, spam, and trust is evaluated and compared with those of common sampling algorithms. According to the experiments done, biased sampling method based on learning automata outperforms all other sampling approaches including simple random walk, Metropolis-Hastings random walk, BFS, forest fire, degree, and uniform sampling in terms of all the evaluation measures. To the best of our knowledge, our method is the first and only biased sampling method which can be used in a multilayer activity network.},
	file         = {:Users/yosef/Downloads/Khadangi2016_Article_BiasedSamplingFromFacebookMult.pdf:pdf},
	keywords     = {Activity network,Facebook,Learning automata,Network sampling,Social media marketing,Social network analysis},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Bruijn2013,
	title        = {{The Base of Trust in Human-Robot Interaction}},
	author       = {Bruijn, Marlies De},
	year         = 2013,
	pages        = {1--9},
	file         = {:Users/yosef/Downloads/Bruijn,_M.L.E._de_1.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}
@article{Srinivasa2021,
	title        = {{‘Help me please'}},
	author       = {Srinivasa, Siddhartha S.},
	year         = 2021,
	journal      = {The Routledge International Handbook of Penal Abolition},
	pages        = {119--130},
	doi          = {10.4324/9780429425035-18},
	isbn         = 9781450333627,
	abstract     = {Robots that can leverage help from people could accomplish much more than robots that cannot. We present the results of two experiments that examine how robots can more effec- tively request help from people. Study 1 is a video prototype experiment (N=354), investigating the effectiveness of four linguistic politeness strategies as well as the effects of social status (equal, low), size of request (large, small), and robot fa- miliarity (high, low) on people's willingness to help a robot. The results of this study largely support Politeness Theory and the Computers as Social Actors paradigm. Study 2 is a physical human-robot interaction experiment (N=48), exam- ining the impact of source orientation (autonomous, single operator, multiple operators) on people's behavioral willing- ness to help the robot. People were nearly 50% faster to help the robot if they perceived it to be autonomous rather than being teleoperated. Implications for research design, theory, and methods are discussed.},
	file         = {:Users/yosef/Downloads/2858036.2858217.pdf:pdf},
	mendeley-groups = {Other_Trust_Metric_Papers}
}

@article{Lyons2012,
	title        = {{Human-human reliance in the context of automation}},
	author       = {Lyons, Joseph B. and Stokes, Charlene K.},
	year         = 2012,
	journal      = {Human Factors},
	volume       = 54,
	number       = 1,
	pages        = {112--121},
	doi          = {10.1177/0018720811427034},
	isbn         = {0018720811},
	issn         = 15478181,
	abstract     = {Objective: The current study examined human-human reliance during a computer-based scenario where participants interacted with a human aid and an automated tool simultaneously. Background: Reliance on others is complex, and few studies have examined human-human reliance in the context of automation. Past research found that humans are biased in their perceived utility of automated tools such that they view them as more accurate than humans. Prior reviews have postulated differences in human-human versus human-machine reliance, yet few studies have examined such reliance when individuals are presented with divergent information from different sources. Method: Participants (N = 40) engaged in the Convoy Leader experiment. They selected a convoy route based on explicit guidance from a human aid and information from an automated map. Subjective and behavioral human-human reliance indices were assessed. Perceptions of risk were manipulated by creating three scenarios (low, moderate, and high) that varied in the amount of vulnerability (i.e., potential for attack) associated with the convoy routes. Results: Results indicated that participants reduced their behavioral reliance on the human aid when faced with higher risk decisions (suggesting increased reliance on the automation); however, there were no reported differences in intentions to rely on the human aid relative to the automation. Conclusion: The current study demonstrated that when individuals are provided information from both a human aid and automation, their reliance on the human aid decreased during high-risk decisions. Application: This study adds to a growing understanding of the biases and preferences that exist during complex human-human and human-machine interactions. {\textcopyright} 2012 Human Factors and Ergonomics Society.},
	file         = {:Users/yosef/Downloads/0018720811427034.pdf:pdf},
	keywords     = {human-human reliance,risk perceptions,trust in automation,vulnerability},
	mendeley-groups = {Other_Trust_Metric_Papers},
	pmid         = 22409106
}



@misc{YagodaApp,
	title        = {{Appendix A Final HRI Trust Scale}},
	author       = {YAGODA, ROSEMARIE ELAINE},
	pages        = {1--2}
}

@article{smith2019general,
	title        = {General social surveys, 1972--2018: Cumulative codebook},
	author       = {Smith, Tom W and Davern, Michael and Freese, Jeremy and Morgan, Stephen L},
	year         = 2019,
	journal      = {Chicago: NORC}
}
@book{giddens,
	title        = {{The Consequences of Modernity}},
	author       = {Giddens, Anthony},
	year         = 1990,
	publisher    = {Polity Press},
	address      = {Standford, CA},
	mendeley-groups = {ATC}
}
@book{fukuyama1996trust,
	title        = {Trust: The social virtues and the creation of prosperity},
	author       = {Fukuyama, Francis},
	year         = 1996,
	publisher    = {Simon and Schuster}
}
@article{endsley2017here,
	title        = {From here to autonomy: lessons learned from human--automation research},
	author       = {Endsley, Mica R},
	year         = 2017,
	journal      = {Human factors},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 59,
	number       = 1,
	pages        = {5--27}
}

@article{Morgner2018,
	title        = {{Trust and Society: Suggestions for Further Development of Niklas Luhmann's Theory of Trust}},
	author       = {Morgner, Christian},
	year         = 2018,
	journal      = {Canadian Review of Sociology},
	volume       = 55,
	number       = 2,
	pages        = {232--256},
	doi          = {10.1111/cars.12191},
	issn         = {1755618X},
	abstract     = {This paper addresses an apparent gap in the work of Niklas Luhmann. While the issue of trust continues to receive widespread attention in the social sciences, Luhmann's interest in this topic declined following the development of his systems theory. It is argued that this decline does not reflect any diminished relevance of trust for systems theory, but rather that the architectural remodeling of theory cannot easily be applied to the issue of trust. Here, the issue of trust is reconceptualized as a connection medium. This entails a reconstruction of Luhmann's early theory of trust, especially with regard to function and social positioning. In this context, trust can in turn be linked to the concept of medium in Luhmann's late work. As a connection medium, trust mediates between the different levels of sociality—interaction, organization, and society. These theoretical considerations are employed to develop a more applied framework for empirical research, with a brief case study from southern Italy. From this perspective, the idea of trust as society's glue is seen to be overly simplistic. The common ethical understanding that more trust leads to a better society is also questioned on the grounds that social cooperation can also lead to social sclerosis. Finally, risk and trust are shown to accommodate the formation of different cultures of trust. The paper shows how Luhmann's updated version of trust can inspire current research and enhance our understanding of how trust operates in contemporary society.},
	file         = {:Users/yosef/Downloads/cars.12191.pdf:pdf},
	pmid         = 29635830
}

@article{Rotter1980,
	title        = {{Interpersonal trust, trustworthiness, and gullibility.}},
	author       = {Rotter, Julian B.},
	year         = 1980,
	journal      = {American Psychologist},
	volume       = 35,
	number       = 1,
	pages        = {1--7},
	abstract     = {This article reviews the positive and potential negative consequences of being high or low in interpersonal trust in current social life, particularly in interacting with ordinary people. A summary and analysis of many investigations lead to the following conclusions applicable to the population studied: People who trust more are less likely to lie and are possibly less likely to cheat or steal. They are more likely to give others a second chance and to respect the rights of others. The high truster is less likely to be unhappy, conflicted, or maladjusted, and is liked more and sought out as a friend more often, both by low-trusting and high-trusting others. When gullibility is defined as naivet{\'{e}} or foolishness and trust is defined as believe others in the absence of clear-cut reasons to disbelieve, then it can be shown over a series of studies that high trusters are not more gullible than low trusters.},
	mendeley-groups = {Grant,NASA},
	pmid         = 22171949
}
@article{palmqvist2022desiderata,
	title        = {Desiderata for Rational, Non-doxastic Faith},
	author       = {Palmqvist, Carl-Johan},
	year         = 2022,
	journal      = {Sophia},
	publisher    = {Springer},
	pages        = {1--21}
}


@article{Hoff2015,
	title        = {{Trust in automation: Integrating empirical evidence on factors that influence trust}},
	author       = {Hoff, Kevin Anthony and Bashir, Masooda},
	year         = 2015,
	journal      = {Human Factors},
	volume       = 57,
	number       = 3,
	pages        = {407--434},
	doi          = {10.1177/0018720814547570},
	isbn         = 3540348891,
	issn         = 15478181,
	abstract     = {Objective: We systematically review recent empirical research on factors that influence trust in automation to present a three-layered trust model that synthesizes existing knowledge.Background: Much of the existing research on factors that guide human-automation interaction is centered around trust, a variable that often determines the willingness of human operators to rely on automation. Studies have utilized a variety of different automated systems in diverse experimental paradigms to identify factors that impact operators' trust.Method: We performed a systematic review of empirical research on trust in automation from January 2002 to June 2013. Papers were deemed eligible only if they reported the results of a human-subjects experiment in which humans interacted with an automated system in order to achieve a goal. Additionally, a relationship between trust (or a trust-related behavior) and another variable had to be measured. All together, 101 total papers, containing 127 eligible studies, were included in the review.Results: Our analysis revealed three layers of variability in human–automation trust (dispositional trust, situational trust, and learned trust), which we organize into a model. We propose design recommendations for creating trustworthy automation and identify environmental conditions that can affect the strength of the relationship between trust and reliance. Future research directions are also discussed for each layer of trust.Conclusion: Our three-layered trust model provides a new lens for conceptualizing the variability of trust in automation. Its structure can be applied to help guide future research and develop training interventions and design procedures that encourage appropriate trust.},
	file         = {:Users/yosef/Downloads/0018720814547570.pdf:pdf},
	keywords     = {automated system,human-automation interaction,reliance,trust formation,trust in automation},
	mendeley-groups = {NASA},
	pmid         = 25875432
}
@phdthesis{schaefer2013perception,
	title        = {{The perception and measurement of human-robot trust}},
	author       = {Schaefer, Kristin},
	year         = 2013,
	school       = {University of Central Florida}
}
@article{Balliet2013,
	title        = {{Trust, conflict, and cooperation: A meta-analysis}},
	author       = {Balliet, Daniel and {Van Lange}, Paul A.M.},
	year         = 2013,
	journal      = {Psych. Bulletin},
	volume       = 139,
	number       = 5,
	pages        = {1090--1112},
	doi          = {10.1037/a0030939},
	isbn         = {1939-1455 (Electronic)$\backslash$r0033-2909 (Linking)},
	issn         = {00332909},
	abstract     = {Many theories of trust emphasize that trust is most relevant to behavior in situations involving a conflict of interests. However, it is not clear how trust relates to behavior across situations that differ in the degree of conflicting interest: Does trust matter more when the conflict of interest is small or large? According to an interdependence perspective, trust becomes an especially important deter- minant of behavior in situations involving larger, compared to smaller, degrees of conflicting interests. To examine this perspective, we conducted a meta-analysis involving 212 effect sizes on the relation between trust (both state and dispositional trust in others) and cooperation in social dilemmas—situations that involve varying degrees of conflict between self-interest and collective interest. Results revealed that the positive relation between trust and cooperation is stronger when there is a larger, compared to smaller, degree of conflict. We also examined several other possible moderators of the relation between trust and cooperation. The relation between trust and cooperation was stronger during individual, compared to intergroup, interactions but did not vary as a function of the situation being either a one-shot or repeated interaction. We also find differences across countries in the extent that people condition their own cooperation based on their trust in others. We discuss how the results support an emerging consensus about trust being limited to situations of conflict and address some theoretical and societal implications for our understanding of how and why trust is so important to social interactions and relationships.},
	file         = {:Users/yosef/Downloads/Trust{\_}Conflict{\_}and{\_}Cooperation{\_}A{\_}Meta-An.pdf:pdf},
	keywords     = {Cooperation,Expectations,Meta-analysis,Social dilemmas,Trust},
	mendeley-groups = {NASA},
	pmid         = 23231532
}
@inproceedings{Ermisch2006,
	title        = {People's Trust: The design of a survey-based experiment},
	author       = {Ermisch, John and Gambetta, Diego},
	year         = 2006,
	booktitle    = {ISER Working Paper Series},
	publisher    = {IZA Discussion Paper},
	address      = {Colchester, UK},
	number       = {2006-34},
	institution  = {University of Essex, Institute for Social and Economic Research (ISER)}
}
@article{Bacharach2007,
	title        = {The self-fulfilling property of trust: An experimental study},
	author       = {Bacharach, Michael and Guerra, Gerardo and Zizzo, Daniel John},
	year         = 2007,
	journal      = {Theory and Decision},
	publisher    = {Springer},
	volume       = 63,
	number       = 4,
	pages        = {349--388}
}
@phdthesis{Wagner2009,
	title        = {{The role of trust and relationships in human-robot social interaction}},
	author       = {Wagner, Alan Richard},
	year         = 2009,
	school       = {Georgia Institute of Technology}
}
@article{aloe2014empirical,
	title        = {An empirical investigation of partial effect sizes in meta-analysis of correlational data},
	author       = {Aloe, Ariel M},
	year         = 2014,
	journal      = {The Journal of general psychology},
	publisher    = {Taylor \& Francis},
	volume       = 141,
	number       = 1,
	pages        = {47--64}
}
@article{van2015user,
	title        = {User manual for Meta-Essentials: Workbooks for meta-analysis},
	author       = {van Rhee, Henk and Suurmond, Robert and Hak, Tony},
	year         = 2015,
	journal      = {Available at SSRN 3241355}
}
@article{suurmond2017introduction,
	title        = {Introduction, comparison, and validation of Meta-Essentials: a free and simple tool for meta-analysis},
	author       = {Suurmond, Robert and van Rhee, Henk and Hak, Tony},
	year         = 2017,
	journal      = {Research synthesis methods},
	publisher    = {Wiley Online Library},
	volume       = 8,
	number       = 4,
	pages        = {537--553}
}

@article{rusbult_93,
	title        = {{Commitment processes in close relationships: An interdependence analysis}},
	author       = {Rusbult, Caryl E and Buunk, Bram P},
	year         = 1993,
	journal      = {J. of Social and Personal Relationships},
	publisher    = {Sage Publications Sage CA: Thousand Oaks, CA},
	volume       = 10,
	number       = 2,
	pages        = {175--204},
	mendeley-groups = {ATC}
}
@article{Ert2011,
	title        = {A choice prediction competition for social preferences in simple extensive form games: An introduction},
	author       = {Ert, Eyal and Erev, Ido and Roth, Alvin E},
	year         = 2011,
	journal      = {Games},
	publisher    = {Molecular Diversity Preservation International},
	volume       = 2,
	number       = 3,
	pages        = {257--276}
}
@article{battigalli2005,
	title        = {Dynamic psychological games},
	author       = {Battigalli, Pierpaolo and Dufwenberg, Martin},
	year         = 2009,
	journal      = {J. of Econ. Theory},
	publisher    = {Elsevier},
	volume       = 144,
	number       = 1,
	pages        = {1--35}
}

@article{faulkner2017problem,
	title        = {The problem of trust},
	author       = {Faulkner, Paul},
	year         = 2017,
	journal      = {The philosophy of trust},
	publisher    = {Oxford University Press Oxford},
	pages        = {109--28}
}
@book{thibaut_59,
	title        = {{The Social Psychology of Groups}},
	author       = {Thibaut, John W and Kelley, Harold H},
	year         = 1959,
	publisher    = {J. Wiley \& Sons},
	address      = {New York},
	mendeley-groups = {ATC}
}

@phdthesis{zhu2009intentional,
	title        = {Intentional systems and the artificial intelligence (ai) hermeneutic network: Agency and intentionality in expressive computational systems},
	author       = {Zhu, Jichen},
	year         = 2009,
	school       = {Georgia Institute of Technology}
}
@inproceedings{atkinson2014shared,
	title        = {Shared awareness, autonomy and trust in human-robot teamwork},
	author       = {Atkinson, David J and Clancey, William J and Clark, Micah H},
	year         = 2014,
	booktitle    = {2014 AAAI Fall Symposium Series}
}
@inproceedings{alaieri2016ethical,
	title        = {Ethical decision making in robots: Autonomy, trust and responsibility},
	author       = {Alaieri, Fahad and Vellino, Andr{\'e}},
	year         = 2016,
	booktitle    = {Intl. conf. on social robotics},
	pages        = {159--168},
	organization = {Springer}
}
@article{nass1999people,
	title        = {Are people polite to computers? Responses to computer-based interviewing systems 1},
	author       = {Nass, Clifford and Moon, Youngme and Carney, Paul},
	year         = 1999,
	journal      = {J. of Applied Social Psych.},
	publisher    = {Wiley Online Library},
	volume       = 29,
	number       = 5,
	pages        = {1093--1109}
}
@article{Epley2007,
	title        = {{On seeing human: A three-factor theory of anthropomorphism.}},
	author       = {Epley, Nicholas and Waytz, Adam and Cacioppo, John T.},
	year         = 2007,
	journal      = {Psych. Review},
	volume       = 114,
	number       = 4,
	pages        = {864--886},
	doi          = {10.1037/0033-295X.114.4.864},
	isbn         = {0033-295X (Print)$\backslash$r0033-295X (Linking)},
	issn         = {1939-1471},
	abstract     = {Anthropomorphism describes the tendency to imbue the real or imagined behavior of nonhuman agents with humanlike characteristics, motivations, intentions, or emotions. Although surprisingly common, anthropomorphism is not invariant. This article describes a theory to explain when people are likely to anthropomorphize and when they are not, focused on three psychological determinants—the accessibility and applicability of anthropocentric knowledge (elicited agent knowledge), the motivation to explain and understand the behavior of other agents (effectance motivation), and the desire for social contact and affiliation (sociality motivation). This theory predicts that people are more likely to anthropomorphize when anthropocentric knowledge is accessible and applicable, when motivated to be effective social agents, and when lacking a sense of social connection to other humans. These factors help to explain why anthropomorphism is so variable; organize diverse research; and offer testable predictions about dispo- sitional, situational, developmental, and cultural influences on anthropomorphism. Discussion addresses extensions of this theory into the specific psychological processes underlying anthropomorphism, applications of this theory into robotics and human–computer interaction, and the insights offered by this theory into the inverse process of dehumanization.},
	annote       = {NULL},
	archiveprefix = {arXiv},
	arxivid      = {epley2007},
	eprint       = {epley2007},
	file         = {:Users/yosef/Desktop/WeRobot/10.1.1.457.4031.pdf:pdf},
	keywords     = {1995,75 million,agency,animal cognition,anthropomorphism,as cited in heywood,at least,biodiversity assessment,inhabited by approximately 1,known species with unique,mind perception,our planet is currently,phylogenetic characteristics,social cognition,unep},
	mendeley-groups = {Trust},
	pmid         = 17907867
}

@article{Bolton1995,
	title        = {{ERC: A theory of equity, reciprocity, and competition}},
	author       = {Bolton, Gary E and Ockenfels, Axel},
	year         = 2000,
	journal      = {The American Econ. Rev.},
	volume       = 90,
	number       = 1,
	pages        = {166--193}
}
@article{Rabin2020,
	title        = {Incorporating fairness into game theory and economics},
	author       = {Rabin, Matthew},
	year         = 1993,
	journal      = {The American Econ. Rev.},
	publisher    = {JSTOR},
	pages        = {1281--1302}
}

@mastersthesis{Nordheim2018,
	title        = {Trust in chatbots for customer service--findings from a questionnaire study},
	author       = {Nordheim, Cecilie Bertinussen},
	year         = 2018,
	school       = {U. of Oslo}
}
@techreport{SATI,
	title        = {{Guidelines for Trust in Future ATM Systems : Measures}},
	author       = {Goillau, P. and Kelly, C. and Boardman, M. and Jeannot, E.},
	year         = 2003,
	doi          = {HRS/HSP-005-GUI-02},
	isbn         = {HRS/HSP-005-GUI-01},
	abstract     = {The purpose of this document is to describe the development, evaluation, validation and potential use of a measure of Air Traffic Control (ATC) trust. The measure, named ‘SATI' for ‘SHAPE ATM Trust Index', is primarily concerned with human trust of ATC computer-assistance tools and other forms of automation support, which are expected to be major components of future Air Traffic Management (ATM) systems. This deliverable is the second one developed within the ‘Solutions for Human-Automation Partnerships in European ATM (SHAPE)' Project. A related deliverable provides a set of human factors guidelines for facilitating and fostering human trust in ATM systems (see EATMP, 2003a). A subsequent deliverable on the trust issue provides detailed information on trust principles (see EATMP, 2003b).},
	annote       = {NULL},
	file         = {:Users/yosef/Desktop/WeRobot/SATI.pdf:pdf},
	keywords     = {SATI},
	mendeley-groups = {WeRobot,Trust,Trust{\_}Proposal{\_}and{\_}metrics},
	institution  = {European Organisation for the Safety of Air Navigation}
}
@article{Lewicki1998,
	title        = {{Trust and distrust: New relationships and realities}},
	author       = {Lewicki, R O Y J and McAllister, DANIEL J. and Bies, Robert J},
	year         = 1998,
	journal      = {Acad. of Mgmt. Rev.},
	volume       = 23,
	number       = 3,
	pages        = {438--458},
	doi          = {10.5465/AMR.2003.10196729},
	isbn         = {03637425},
	issn         = {03637425},
	file         = {:Users/yosef/Downloads/259288.pdf:pdf},
	mendeley-groups = {NASA},
	pmid         = 10196729
}
@phdthesis{muir2002operators,
	title        = {Operators' trust in and use of automatic controllers in a supervisory process control task.},
	author       = {Muir, Bonnie Marlene},
	year         = 2002,
	school       = {Univ. of Toronto}
}
@article{merritt2011affective,
	title        = {Affective processes in human--automation interactions},
	author       = {Merritt, Stephanie M},
	year         = 2011,
	journal      = {Human Factors},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 53,
	number       = 4,
	pages        = {356--370}
}
@techreport{french2018trust,
	title        = {Trust in Automation},
	author       = {French, Bronwyn and Duenser, Andreas and Heathcote, Andrew},
	year         = 2018,
	address      = {Australia},
	institution  = {CSIRO Report EP184082}
}
@inproceedings{belle2011physiological,
	title        = {A physiological signal processing system for optimal engagement and attention detection},
	author       = {Belle, Ashwin and Hobson, Rosalyn and Najarian, Kayvan},
	year         = 2011,
	booktitle    = {2011 IEEE International Conference on Bioinformatics and Biomedicine Workshops (BIBMW)},
	pages        = {555--561},
	organization = {IEEE}
}
@inproceedings{ma2018modelling,
	title        = {Modelling and evaluating failures in human-robot teaming using simulation},
	author       = {Ma, Lanssie M and IJtsma, Martijn and Feigh, Karen M and Paladugu, Abhinay and Pritchett, Amy R},
	year         = 2018,
	booktitle    = {2018 IEEE Aerospace Conference},
	pages        = {1--16},
	organization = {IEEE}
}
@inproceedings{rojas2017physiological,
	title        = {Physiological fluctuations show frequency-specific networks in fNIRS signals during resting state},
	author       = {Rojas, Raul Fernandez and Huang, Xu and Hernandez-Juarez, Jesus and Ou, Keng-Liang},
	year         = 2017,
	booktitle    = {2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
	pages        = {2550--2553},
	organization = {IEEE}
}
@phdthesis{Oh2018,
	title        = {{An Investigation of Neural Correspondence of Human Trust in Automation}},
	author       = {Oh, Seeung},
	year         = 2018,
	file         = {:Users/yosef/Downloads/An_Investigation_of_Neural_Cor.pdf:pdf},
	mendeley-groups = {Trust_Metrics_Analysis},
	school       = {North Carolina Agricultural and Technical State University}
}
@article{bartneck2009measurement,
	title        = {Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots},
	author       = {Bartneck, Christoph and Kuli{\'c}, Dana and Croft, Elizabeth and Zoghbi, Susana},
	year         = 2009,
	journal      = {International journal of social robotics},
	publisher    = {Springer},
	volume       = 1,
	number       = 1,
	pages        = {71--81}
}
@article{krausman2022trust,
	title        = {Trust Measurement in Human-Autonomy Teams: Development of a Conceptual Toolkit},
	author       = {Krausman, Andrea and Neubauer, Catherine and Forster, Daniel and Lakhmani, Shan and Baker, Anthony L and Fitzhugh, Sean M and Gremillion, Gregory and Wright, Julia L and Metcalfe, Jason S and Schaefer, Kristin E},
	year         = 2022,
	journal      = {ACM Transactions on Human-Robot Interaction},
	publisher    = {ACM New York, NY}
}
@article{NajeraCatalan2019,
	title        = {{Reliability, Population Classification and Weighting in Multidimensional Poverty Measurement: A Monte Carlo Study}},
	author       = {{N{\'{a}}jera Catal{\'{a}}n}, H{\'{e}}ctor E.},
	year         = 2019,
	journal      = {Social Indicators Research},
	publisher    = {Springer Netherlands},
	volume       = 142,
	number       = 3,
	pages        = {887--910},
	doi          = {10.1007/s11205-018-1950-z},
	isbn         = {0123456789},
	issn         = 15730921,
	url          = {https://doi.org/10.1007/s11205-018-1950-z},
	abstract     = {In poverty measurement, differential weighting aims to take into account the unequal importance of the diverse dimensions and aspects of poverty and to add valuable information that improves the classification of the poor and the not-poor. This practice, however, is in contention with both classical test theory and modern measurement theories, which state that high reliability is a necessary condition for consistent population classification, while differential weighting is not so. The literature needs a clear numerical illustration of the relationship between high/low reliability and good/poor population classification to dissolve this tension and assist applied researchers in the assessment of multidimensional poverty indexes, using different reliability statistics. This paper uses a Monte Carlo study based on factor mixture models to draw up a series of uni-and multidimensional poverty measures with different reliabilities and predefined groups. The article shows that low reliability results in a high proportion of the poor group erroneously classified as part of the not poor group. Therefore, reliability inspections should be a systematic practice in poverty measurement. The article provides guidelines for interpreting the effects of unreliability upon adequate population classification and suggest that the classification error of current unreliable multidimensional indexes is above 10\%.},
	file         = {:Users/yosef/Downloads/Reliability_Population_Classification_and_Weightin.pdf:pdf},
	keywords     = {Deprivation,Poverty,Relative entropy,Reliability,Weighting}
}

@article{tavakol2011making,
	title        = {Making sense of Cronbach's alpha},
	author       = {Tavakol, Mohsen and Dennick, Reg},
	year         = 2011,
	journal      = {International journal of medical education},
	publisher    = {IJME},
	volume       = 2,
	pages        = 53
}
@inproceedings{khavas,
	title        = {Modeling Trust in Human-Robot Interaction: A Survey},
	author       = {Khavas, Zahra Rezaei and Ahmadzadeh, S. Reza and Robinette, Paul},
	year         = 2020,
	booktitle    = {Social Robotics},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {529--541},
	isbn         = {978-3-030-62056-1},
	editor       = {Wagner, Alan R. and Feil-Seifer, David and Haring, Kerstin S. and Rossi, Silvia and Williams, Thomas and He, Hongsheng and Sam Ge, Shuzhi},
	abstract     = {As the autonomy and capabilities of robotic systems increase, they are expected to play the role of teammates rather than tools and interact with human collaborators in a more realistic manner, creating a more human-like relationship. Given the impact of trust observed in human-robot interaction (HRI), appropriate trust in robotic collaborators is one of the leading factors influencing the performance of human-robot interaction. Team performance can be diminished if people do not trust robots appropriately by disusing or misusing them based on limited experience. Therefore, trust in HRI needs to be calibrated properly, rather than maximized, to let the formation of an appropriate level of trust in human collaborators. For trust calibration in HRI, trust needs to be modeled first. There are many reviews on factors affecting trust in HRI[22], however, as there are no reviews concentrated on different trust models, in this paper, we review different techniques and methods for trust modeling in HRI. We also present a list of potential directions for further research and some challenges that need to be addressed in future work on human-robot trust modeling.}
}
@article{javor2014correlation,
	title        = {Correlation of plasma and salivary oxytocin in healthy young men—experimental evidence},
	author       = {Javor, Andrija and Riedl, Ren{\'e} and Kindermann, Harald and Brandst{\"a}tter, Walter and Ransmayr, Gerhard and Gabriel, Michael},
	year         = 2014,
	journal      = {Neuroendocrinol Lett},
	volume       = 35,
	number       = 470,
	pages        = 3
}
@article{riedl2012biology,
	title        = {The biology of trust: Integrating evidence from genetics, endocrinology, and functional brain imaging.},
	author       = {Riedl, Ren{\'e} and Javor, Andrija},
	year         = 2012,
	journal      = {Journal of Neuroscience, Psychology, and Economics},
	publisher    = {Educational Publishing Foundation},
	volume       = 5,
	number       = 2,
	pages        = 63
}

@book{Khalid2020,
	title        = {{Determinants of trust in human-robot interaction: Modeling, measuring, and predicting}},
	author       = {Khalid, Halimahtun M. and Helander, Martin G. and Lin, Mei Hua},
	year         = 2020,
	booktitle    = {Trust in Human-Robot Interaction},
	publisher    = {Elsevier Inc.},
	pages        = {85--121},
	doi          = {10.1016/B978-0-12-819472-0.00004-6},
	isbn         = 9780128194720,
	url          = {http://dx.doi.org/10.1016/B978-0-12-819472-0.00004-6},
	abstract     = {Can humans trust humanoid robots to perform social tasks? Trust is a multifactorial concept with several determinants that influence the interaction between humans and robots. These determinants include factors concerning human, robot, social, and context. To measure human trust in humanoid robots, interactive dialogues were used with subjective measures of general trust embedded in the dialogues. General trust was made up of three components: ability, benevolence, and integrity, each characterized by five attributes. These measures were mapped to objective measures of physiological trust comprising facial expressions, voiced speech, vision-based heart rate, and postural gestures. The purpose was to predict trust from a neural network of psychophysiological criteria. To validate the reliability of trust measures, a series of three experiments was conducted at three levels of interaction: human-human, human-robot and human-robot-human. While the methodology was used to measure human trust in robots, it has potential for measuring robot trust in humans.},
	file         = {:Users/yosef/Downloads/Trust In Human-Robot Interaction_Book_2021/Chapter-4---Determinants-of-trust-in-human-robot-inte_2021_Trust-in-Human-Ro.pdf:pdf},
	keywords     = {Human-robot interaction,Interactive dialogue,Neural network,Psychophysiological criteria,Trust determinants},
	mendeley-groups = {New Papers on Trust that are key}
}
@book{Matthews2021,
	title        = {{Super-machines or sub-humans: Mental models and trust in intelligent autonomous systems}},
	author       = {Matthews, Gerald and Panganiban, April Rose and Lin, Jinchao and Long, Michael and Schwing, Michaela},
	year         = 2020,
	booktitle    = {Trust in Human-Robot Interaction},
	publisher    = {Elsevier Inc.},
	pages        = {59--84},
	isbn         = 9780128194720,
	file         = {:Users/yosef/Downloads/Trust In Human-Robot Interaction_Book_2021/Chapter-4---Determinants-of-trust-in-human-robot-inte_2021_Trust-in-Human-Ro.pdf:pdf},
	keywords     = {Human-robot interaction,Interactive dialogue,Neural network,Psychophysiological criteria,Trust determinants},
	mendeley-groups = {New Papers on Trust that are key}
}
@article{Khalid2019,
	title        = {{Creativity in Measuring Trust in Human-Robot Interaction Using Interactive Dialogs}},
	author       = {Khalid, Halimahtun and Liew, Wei Shiung and Voong, Bin Sheng and Helander, Martin},
	year         = 2019,
	booktitle    = {Advances in Intelligent Systems and Computing},
	publisher    = {Springer International Publishing},
	volume       = 824,
	pages        = {1175--1190},
	doi          = {10.1007/978-3-319-96071-5_119},
	isbn         = 9783319960708,
	issn         = 21945357,
	url          = {http://dx.doi.org/10.1007/978-3-319-96071-5_119},
	file         = {:Users/yosef/Downloads/9bb0a3fcbe588ec2b40407c4c20d6bcd.pdf:pdf},
	keywords     = {Dialog design,Human-Robot interaction,Trust},
	mendeley-groups = {New Papers on Trust that are key}
}
@article{Khalid2016,
	title        = {{Exploring psycho-physiological correlates to trust: Implications for human-robot-human interaction}},
	author       = {Khalid, Halimahtun M. and Shiung, Liew Wei and Nooralishahi, Parham and Rasool, Zeeshan and Helander, Martin G. and Kiong, Loo Chu and Ai-Vyrn, Chin},
	year         = 2016,
	journal      = {Proceedings of the Human Factors and Ergonomics Society},
	volume       = 60,
	number       = 1,
	pages        = {696--700},
	doi          = {10.1177/1541931213601160},
	issn         = 10711813,
	file         = {:Users/yosef/Downloads/1541931213601160.pdf:pdf},
	mendeley-groups = {New Papers on Trust that are key}
}
@book{rotenberg2019psychology,
	title        = {The psychology of interpersonal trust: theory and research},
	author       = {Rotenberg, Ken J},
	year         = 2019,
	publisher    = {Routledge}
}
@book{kreps1990corporate,
	title        = {Corporate culture and economic theory},
	author       = {Kreps, David M},
	year         = 1990,
	booktitle    = {Perspectives on positive political economy},
	location     = {New York},
	publisher    = {Cambridge University Press},
	editor       = {Alt,James E. and Shepsle, Kenneth A.},
	chapter      = 4
}
@article{Berg1995,
	title        = {{Trust, reciprocity, and social history}},
	author       = {Berg, Joyce and Dickhaut, John and McCabe, Kevin},
	year         = 1995,
	journal      = {Games and Economic Behavior},
	volume       = 10,
	number       = 1,
	pages        = {122--142},
	doi          = {10.1006/game.1995.1027},
	issn         = 10902473,
	file         = {:Users/yosef/Downloads/Trust, Reciprocity and Social History GEB 1995.pdf:pdf},
	mendeley-groups = {Trust Games in GPS}
}
@book{coleman1994foundations,
	title        = {Foundations of social theory},
	author       = {Coleman, James S},
	year         = 1994,
	publisher    = {Harvard university press}
}
@inproceedings{razin2021watch,
	title        = {{Watch For Failing Objects: What Inappropriate Compliance Reveals About Shared Mental Models In Autonomous Cars}},
	author       = {Razin, Yosef S and Gale, Jack and Fan, Jiaojiao and Smith, Jaznae' and Feigh, Karen M},
	year         = 2021,
	booktitle    = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	volume       = 65,
	number       = 1,
	pages        = {643--647},
	organization = {SAGE Publications Sage CA: Los Angeles, CA}
}
@article{ruscio2012determining,
	title        = {Determining the number of factors to retain in an exploratory factor analysis using comparison data of known factorial structure.},
	author       = {Ruscio, John and Roche, Brendan},
	year         = 2012,
	journal      = {Psychological assessment},
	publisher    = {American Psychological Association},
	volume       = 24,
	number       = 2,
	pages        = 282
}
@manual{RPsych,
	title        = {psych: Procedures for Psychological, Psychometric, and Personality Research},
	author       = {William Revelle},
	year         = 2022,
	address      = {Evanston, Illinois},
	url          = {https://CRAN.R-project.org/package=psych},
	note         = {R package version 2.2.9},
	organization = {Northwestern University}
}
@article{Sheng2012,
	title        = {{Is coefficient alpha robust to non-normal data?}},
	author       = {Sheng, Yanyan and Sheng, Zhaohui},
	year         = 2012,
	journal      = {Frontiers in Psychology},
	volume       = 3,
	number       = {FEB},
	pages        = {1--13},
	doi          = {10.3389/fpsyg.2012.00034},
	issn         = 16641078,
	file         = {:Users/yosef/Downloads/fpsyg-03-00034.pdf:pdf},
	keywords     = {Coefficient alpha,Error score distribution,Kurtosis,Monte Carlo,Non-normality,Power method polynomials,Skew,True score distribution}
}
@article{Zhang2016,
	title        = {{Robust Coefficients Alpha and Omega and Confidence Intervals With Outlying Observations and Missing Data: Methods and Software}},
	author       = {Zhang, Zhiyong and Yuan, Ke Hai},
	year         = 2016,
	journal      = {Educational and Psychological Measurement},
	volume       = 76,
	number       = 3,
	pages        = {387--411},
	doi          = {10.1177/0013164415594658},
	issn         = 15523888,
	file         = {:Users/yosef/Downloads/10.1177_0013164415594658.pdf:pdf},
	keywords     = {R package coefficientalpha,confidence intervals,missing data,outlying observations,robust Cronbach's alpha,robust McDonald's omega}
}
@article{Filkowski2016,
	title        = {{Trying to trust: Brain activity during interpersonal social attitude change}},
	author       = {Filkowski, Megan M. and Anderson, Ian W. and Haas, Brian W.},
	year         = 2016,
	journal      = {Cognitive, Affective and Behavioral Neuroscience},
	volume       = 16,
	number       = 2,
	pages        = {325--338},
	doi          = {10.3758/s13415-015-0393-0},
	issn         = 15307026,
	file         = {:Users/yosef/Downloads/s13415-015-0393-0.pdf:pdf},
	keywords     = {Cognitive control,Emotion,Functional connectivity,Interpersonal trust,Neural network,Social attitudes},
	pmid         = 26567160
}
@article{Fett2014,
	title        = {{Default distrust? An fmri investigation of the neural development of trust and cooperation}},
	author       = {Fett, Anne Kathrin J. and Gromann, Paula M. and Giampietro, Vincent and Shergill, Sukhi S. and Krabbendam, Lydia},
	year         = 2014,
	journal      = {Social Cognitive and Affective Neuroscience},
	volume       = 9,
	number       = 4,
	pages        = {395--402},
	doi          = {10.1093/scan/nss144},
	issn         = 17495024,
	file         = {:Users/yosef/Downloads/nss144.pdf:pdf},
	keywords     = {Development,FMRI,Perspective taking,Theory of mind,Trust game},
	pmid         = 23202661
}
@article{endsley1995toward,
	title        = {{Toward a theory of situation awareness in dynamic systems}},
	author       = {Endsley, Mica R},
	year         = 1995,
	journal      = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	publisher    = {SAGE Publications},
	volume       = 37,
	number       = 1,
	pages        = {32--64}
}
@article{8132809,
	title        = {Relevance Realization and the Emerging Framework in Cognitive Science},
	author       = {Vervaeke, John and Lillicrap, Timothy P. and Richards, Blake A.},
	year         = 2012,
	journal      = {Journal of Logic and Computation},
	volume       = 22,
	number       = 1,
	pages        = {79--99},
	doi          = {10.1093/logcom/exp067}
}
@book{Mittu,
	title        = {{Robust Intelligence and Trust in Autonomous Systems}},
	author       = {Mittu, Ranjeev and Sofge, Donald and Wagner, Alan},
	isbn         = 9781489976666,
	file         = {:Users/yosef/Downloads/2016_Book_RobustIntelligenceAndTrustInAu.pdf:pdf},
	mendeley-groups = {Trust_Metrics_Analysis}
}
@article{axelrod1981evolution,
	title        = {The evolution of cooperation},
	author       = {Axelrod, Robert and Hamilton, William D},
	year         = 1981,
	journal      = {science},
	publisher    = {American Association for the Advancement of Science},
	volume       = 211,
	number       = 4489,
	pages        = {1390--1396}
}
@article{converse1993shared,
	title        = {Shared mental models in expert team decision making},
	author       = {Converse, Sharolyn and Cannon-Bowers, JA and Salas, E},
	year         = 1993,
	journal      = {Individual and group decision making: Current issues},
	publisher    = {Hillsdale},
	volume       = 221,
	pages        = {221--46}
}
@article{Oleson2011,
	title        = {{Antecedents of Trust in Human-Robot Collaborations}},
	author       = {Oleson, Kristin E and Billings, D R and Kocsis, Vivien and Chen, Jessie Y C and Hancock, P A},
	year         = 2011,
	publisher    = {IEEE},
	pages        = {175--178},
	isbn         = 9781612847863,
	file         = {:Users/yosef/Downloads/05753439.pdf:pdf},
	mendeley-groups = {Trust_Metrics_Analysis}
}
@book{newell1972human,
	title        = {Human problem solving},
	author       = {Newell, Allen and Simon, Herbert Alexander and others},
	year         = 1972,
	publisher    = {Prentice-hall Englewood Cliffs, NJ},
	volume       = 104,
	number       = 9
}

@misc{ZhangA2016,
	    title = {{coefficeintalpha}: {Robust Coefficient Alpha and Omega with Missing and Non-Normal Data}},
	    author = {Zhang, Zhiyong and Yuan, Ke Hai},
	   year  = 2020,
	     url   = {https://cran.r-project.org/web/packages/coefficientalpha/coefficientalpha.pdf}
}
@article{Srivastava,
	title        = {{The Impact of Improving Shared Situation Awareness on AI-Advised Decision Making}},
	author       = {Srivastava, Divya K and Lilly, J Mason and Feigh, Karen M},
	file         = {:Users/yosef/Downloads/IEEE_ICHMS_2022_Paper.pdf:pdf}
}
@article{gershgoren2013coaching,
	title        = {{Coaching shared mental models in soccer: A longitudinal case study}},
	author       = {Gershgoren, Lael and {Medeiros Filho}, Edson and Tenenbaum, Gershon and Schinke, Robert J},
	year         = 2013,
	journal      = {Journal of Clinical Sport Psychology},
	publisher    = {Human Kinetics, Inc.},
	volume       = 7,
	number       = 4,
	pages        = {293--312},
	mendeley-groups = {SMM}
}
@article{scheutz2017framework,
	title        = {{A framework for developing and using shared mental models in human-agent teams}},
	author       = {Scheutz, Matthias and DeLoach, Scott A and Adams, Julie A},
	year         = 2017,
	journal      = {Journal of Cognitive Engineering and Decision Making},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 11,
	number       = 3,
	pages        = {203--224},
	mendeley-groups = {SMM}
}
@article{salas1994role,
	title        = {{The role of shared mental models in developing shared situational awareness}},
	author       = {Salas, Eduardo and Stout, R J and Cannon-Bowers, J A},
	year         = 1994,
	journal      = {Situational awareness in complex systems},
	publisher    = {Embry-Riddle Aeronautical University Press Daytona Beach, FL},
	pages        = {297--304},
	mendeley-groups = {SMM}
}
@incollection{cooke2013retention,
	title        = {{Retention of team coordination skill}},
	author       = {Cooke, Nancy J and Gorman, Jamie C and Duran, Jasmine and Myers, Christopher W and Andrews, Dee},
	year         = 2013,
	booktitle    = {Individual and team skill decay},
	publisher    = {Routledge},
	pages        = {368--387},
	mendeley-groups = {SMM}
}
@phdthesis{Bergiel2006,
	title        = {{Shared mental models and team performance: Clarifying the group process mediator of cohesion}},
	author       = {Bergiel, Erich Blaise},
	year         = 2006,
	booktitle    = {ProQuest Dissertations and Theses},
	publisher    = {Mississippi State University},
	address      = {Ann Arbor},
	pages        = 173,
	isbn         = {978-0-542-60666-3},
	url          = {https://go.openathens.net/redirector/gatech.edu?url=https://search.proquest.com/dissertations-theses/shared-mental-models-team-performance-clarifying/docview/305306191/se-2?accountid=11107 https://gatech-primo.hosted.exlibrisgroup.com/openurl/01GALI_GIT/0},
	annote       = {
		Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.

		Last updated - 2021-05-21
	},
	keywords     = {0454:Management,Cohesion,Group process,Groups,Management,Mediation,Mediator,Mental models,Models,Social sciences,Studies,Team performance,Teams},
	language     = {English},
	mendeley-groups = {SMM}
}

@article{hanna2018impact,
	title        = {{The impact of multimodal communication on a shared mental model, trust, and commitment in human--intelligent virtual agent teams}},
	author       = {Hanna, Nader and Richards, Deborah},
	year         = 2018,
	journal      = {Multimodal Technologies and Interaction},
	publisher    = {Multidisciplinary Digital Publishing Institute},
	volume       = 2,
	number       = 3,
	pages        = 48,
	mendeley-groups = {SMM}
}
@article{van2011team,
	title        = {{Team learning: building shared mental models}},
	author       = {den Bossche, Piet and Gijselaers, Wim and Segers, Mien and Woltjer, Geert and Kirschner, Paul},
	year         = 2011,
	journal      = {Instructional Science},
	publisher    = {Springer},
	volume       = 39,
	number       = 3,
	pages        = {283--301},
	mendeley-groups = {SMM}
}
@article{razzouk2013case,
	title        = {{Case studies' effect on undergraduates' achievement, attitudes, and team shared mental models in educational psychology}},
	author       = {Razzouk, Rim and Johnson, Tristan E},
	year         = 2013,
	journal      = {Educational Technology Research and Development},
	publisher    = {Springer},
	volume       = 61,
	number       = 5,
	pages        = {751--766},
	mendeley-groups = {SMM}
}
@inproceedings{gervits2020toward,
	title        = {{Toward genuine robot teammates: Improving human-robot team performance using robot shared mental models}},
	author       = {Gervits, Felix and Thurston, Dean and Thielstrom, Ravenna and Fong, Terry and Pham, Quinn and Scheutz, Matthias},
	year         = 2020,
	booktitle    = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
	pages        = {429--437},
	mendeley-groups = {SMM}
}
@article{Mueller2019,
	title        = {{Explanation in Human-AI Systems: A Literature Meta-Review}},
	author       = {Mueller, Shane T and Hoffman, Robert R and Clancey, William and Emrey, Abigail and Klein, Gary},
	year         = 2019,
	journal      = {Defence Advanced Research Projects Agency (DARPA)},
	number       = {February 2019},
	pages        = 204,
	file         = {:Users/yosef/Downloads/1902.01876.pdf:pdf},
	mendeley-groups = {SMM}
}

@article{Andrews2022,
  title={The role of shared mental models in human-AI teams: a theoretical review},
  author={Andrews, Robert W and Lilly, J Mason and Srivastava, Divya and Feigh, Karen M},
  journal={Theoretical Issues in Ergonomics Science},
  volume={24},
  number={2},
  pages={129--175},
  year={2023},
  publisher={Taylor \& Francis}
}

@article{nowak2006five,
	title        = {Five rules for the evolution of cooperation},
	author       = {Nowak, Martin A},
	year         = 2006,
	journal      = {science},
	publisher    = {American Association for the Advancement of Science},
	volume       = 314,
	number       = 5805,
	pages        = {1560--1563}
}
@article{tversky1992advances,
	title        = {Advances in prospect theory: Cumulative representation of uncertainty},
	author       = {Tversky, Amos and Kahneman, Daniel},
	year         = 1992,
	journal      = {Journal of Risk and uncertainty},
	publisher    = {Springer},
	volume       = 5,
	number       = 4,
	pages        = {297--323}
}
@article{holt2002risk,
	title        = {Risk aversion and incentive effects},
	author       = {Holt, Charles A and Laury, Susan K},
	year         = 2002,
	journal      = {American economic review},
	volume       = 92,
	number       = 5,
	pages        = {1644--1655}
}
@article{bohnet2004trust,
	title        = {Trust, risk and betrayal},
	author       = {Bohnet, Iris and Zeckhauser, Richard},
	year         = 2004,
	journal      = {Journal of Economic Behavior \& Organization},
	publisher    = {Elsevier},
	volume       = 55,
	number       = 4,
	pages        = {467--484}
}
@article{Alos-Ferrer2019,
	title        = {{Trust Games and Beyond}},
	year         = 2019,
	journal      = {Frontiers in Neuroscience},
	volume       = 13,
	number       = {September},
	pages        = {1--14},
	doi          = {10.3389/fnins.2019.00887},
	issn         = {1662453X},
	file         = {:Users/yosef/Downloads/Trust_Games_and_Beyond.pdf:pdf},
	keywords     = {oxytocin,reciprocity,social neuroscience,social preferences,survey measures,theory of mind,trust,trustworthiness},
	mendeley-groups = {Trust Games in GPS}
}
@article{Pelligra1998,
	title        = {{Under trusting eyes : the responsive nature of trust}},
	author       = {Pelligra, Vittorio},
	year         = 1998,
	pages        = {105--124},
	file         = {:Users/yosef/Downloads/under-trusting-eyes-bozze-2.pdf:pdf},
	mendeley-groups = {Trust_Game_Theory}
}
@article{Buskens2020,
	title        = {{3. Rational Choice Research on Social Dilemmas: Embeddedness Effects on Trust}},
	author       = {Buskens, Vincent and Raub, Werner},
	year         = 2020,
	journal      = {The Handbook of Rational Choice Social Research},
	pages        = {113--150},
	doi          = {10.1515/9780804785501-006},
	file         = {:Users/yosef/Downloads/10.1.1.175.1867-2.pdf:pdf},
	mendeley-groups = {Trust Games in GPS}
}
@article{rapoport2015tit,
	title        = {Is tit-for-tat the answer? On the conclusions drawn from Axelrod's tournaments},
	author       = {Rapoport, Amnon and Seale, Darryl A and Colman, Andrew M},
	year         = 2015,
	journal      = {PloS one},
	publisher    = {Public Library of Science San Francisco, CA USA},
	volume       = 10,
	number       = 7,
	pages        = {e0134128}
}
@article{Muir1994,
	title        = {{Trust in Automation: Part I}},
	author       = {Muir, Bonnie M},
	year         = 1994,
	journal      = {Ergonomics},
	volume       = 37,
	number       = 11,
	pages        = {1905--1922},
	file         = {:Users/yosef/Library/Application Support/Mendeley Desktop/Downloaded/Muir - 1994 - Trust in Automation Part I.pdf:pdf},
	keywords     = {Trust},
	mendeley-groups = {Trust,NASA,Trust_For_Paper},
	mendeley-tags = {Trust}
}
@article{stewart2012extortion,
	title        = {Extortion and cooperation in the Prisoner’s Dilemma},
	author       = {Stewart, Alexander J and Plotkin, Joshua B},
	year         = 2012,
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {National Acad Sciences},
	volume       = 109,
	number       = 26,
	pages        = {10134--10135}
}
@article{axelrod1980effective,
	title        = {Effective choice in the prisoner's dilemma},
	author       = {Axelrod, Robert},
	year         = 1980,
	journal      = {Journal of conflict resolution},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 24,
	number       = 1,
	pages        = {3--25}
}
@article{Molloy_1996_Monitoringautomatedsystem,
	title        = {{Monitoring an automated system for a single failure: vigilance and task complexity effects}},
	author       = {Molloy, R and Parasuraman, Raja},
	year         = 1996,
	journal      = {Human Factors},
	volume       = 38,
	pages        = {311--322},
	annote       = {NULL}
}
@article{Parasuraman_1997_Humansandautomation:,
	title        = {{Humans and automation: Use, misuse, disuse, abuse}},
	author       = {Parasuraman, Raja and Riley, Victor},
	year         = 1997,
	journal      = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	publisher    = {SAGE Publications},
	volume       = 39,
	number       = 2,
	pages        = {230--253},
	annote       = {NULL}
}

@article{Masalonis1999,
	title        = {{Trust as a Construct for Evaluation of Automated Aids: Past and Future Theory and Research}},
	author       = {Masalonis, a. J. and Parasuraman, Raja},
	year         = 1999,
	journal      = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	volume       = 43,
	number       = 3,
	pages        = {184--187},
	doi          = {10.1177/154193129904300312},
	isbn         = {1071-1813; 0-945289-12-X},
	issn         = {1071-1813},
	abstract     = {As complex automated aids proliferate in transportation and manufacturing domains, examining human users' trust in such systems gains importance. The authors review some of the growing literature on trust in automated systems, and outline a programme for future studies and theoretical developments. Trust is an intervening variable between automation reliability and use, among other factors. Consistent reliable machine performance can increase trust, and discrete errors can decrease trust. Trust tends to resist change over time. The association between trust and subsequent usage is positive but not clear-cut, and may be mediated by risk and self-confidence. The place of trust in the overall picture of human-automation system performance must be established. The suggested research programme accomplishes this by investigating training issues and individual differences, employing new measures, and examining dynamics of trust and usage in automation possessing different reliability in different situations, automation with multiple modes, and adaptive automation.},
	file         = {:Users/yosef/Library/Application Support/Mendeley Desktop/Downloaded/Masalonis, Parasuraman - 1999 - Trust as a Construct for Evaluation of Automated Aids Past and Future Theory and Research.pdf:pdf}
}

@article{Parasuraman2004,
	title        = {{Trust and etiquette in high-criticality automated systems}},
	author       = {Parasuraman, Raja and Miller, Christopher a.},
	year         = 2004,
	journal      = {Communications of the ACM},
	volume       = 47,
	number       = 4,
	pages        = 51,
	doi          = {10.1145/975817.975844},
	isbn         = {0001-0782},
	issn         = {00010782},
	abstract     = {Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.},
	file         = {:Users/yosef/Downloads/10.1.1.411.4272.pdf:pdf},
	mendeley-groups = {NASA}
}

@article{Miller_2007_Designingflexibleinteraction,
	title        = {{Designing for flexible interaction between humans and automation: Delegation interfaces for supervisory control}},
	author       = {Miller, Christopher A and Parasuraman, Raja},
	year         = 2007,
	journal      = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	publisher    = {Sage Publications},
	volume       = 49,
	number       = 1,
	pages        = {57--75},
	annote       = {NULL}
}


@article{Hancock2011a,
	title        = {{A meta-analysis of factors affecting trust in human-robot interaction}},
	author       = {Hancock, Peter A. and Billings, Deborah R. and Schaefer, Kristin E. and Chen, Jessie Y.C. and {De Visser}, Ewart J. and Parasuraman, Raja},
	year         = 2011,
	journal      = {Human Factors},
	volume       = 53,
	number       = 5,
	pages        = {517--527},
	doi          = {10.1177/0018720811417254},
	isbn         = {0018720811417},
	issn         = {00187208},
	abstract     = {Objective: We evaluate and quantify the effects of human, robot, and environmental factors on perceived trust in human-robot interaction (HRI). Background: To date, reviews of trust in HRI have been qualitative or descriptive. Our quantitative review provides a fundamental empirical foundation to advance both theory and practice. Method: Meta-analytic methods were applied to the available literature on trust and HRI. A total of 29 empirical studies were collected, of which 10 met the selection criteria for correlational analysis and 11 for experimental analysis. These studies provided 69 correlational and 47 experimental effect sizes. Results: The overall correlational effect size for trust was[IMG] f1.gif" ALT="r" BORDER="0"> = +0.26, with an experimental effect size of[IMG] f2.gif" ALT="d" BORDER="0"> = +0.71. The effects of human, robot, and environmental characteristics were examined with an especial evaluation of the robot dimensions of performance and attribute-based factors. The robot performance and attributes were the largest contributors to the development of trust in HRI. Environmental factors played only a moderate role. Conclusion: Factors related to the robot itself, specifically, its performance, had the greatest current association with trust, and environmental factors were moderately associated. There was little evidence for effects of human-related factors. Application: The findings provide quantitative estimates of human, robot, and environmental factors influencing HRI trust. Specifically, the current summary provides effect size estimates that are useful in establishing design and training guidelines with reference to robot-related factors of HRI trust. Furthermore, results indicate that improper trust calibration may be mitigated by the manipulation of robot design. However, many future research needs are identified.},
	file         = {:Users/yosef/Downloads/ADA553574.pdf:pdf},
	keywords     = {human-robot team,robotics,trust,trust development},
	mendeley-groups = {NASA},
	pmid         = 22046724
}

@article{Hancock2011,
	title        = {{A Meta-Analysis of Factors Affecting Trust in Human-Robot Interaction}},
	author       = {Hancock, Peter A. and Billings, Deborah R. and Schaefer, Kristin E. and Chen, Jessie Y. C. and de Visser, Ewart J. and Parasuraman, Raja},
	year         = 2011,
	journal      = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	doi          = {10.1177/0018720811417254},
	isbn         = {0018720811417},
	issn         = {0018-7208},
	abstract     = {Objective: We evaluate and quantify the effects of human, robot, and environmental factors on perceived trust in human-robot interaction (HRI). Background: To date, reviews of trust in HRI have been qualitative or descriptive. Our quantitative review provides a fundamental empirical foundation to advance both theory and practice. Method: Meta-analytic methods were applied to the available literature on trust and HRI. A total of 29 empirical studies were collected, of which 10 met the selection criteria for correlational analysis and 11 for experimental analysis. These studies provided 69 correlational and 47 experimental effect sizes. Results: The overall correlational effect size for trust was[IMG] f1.gif" ALT="r" BORDER="0"> = +0.26, with an experimental effect size of[IMG] f2.gif" ALT="d" BORDER="0"> = +0.71. The effects of human, robot, and environmental characteristics were examined with an especial evaluation of the robot dimensions of performance and attribute-based factors. The robot performance and attributes were the largest contributors to the development of trust in HRI. Environmental factors played only a moderate role. Conclusion: Factors related to the robot itself, specifically, its performance, had the greatest current association with trust, and environmental factors were moderately associated. There was little evidence for effects of human-related factors. Application: The findings provide quantitative estimates of human, robot, and environmental factors influencing HRI trust. Specifically, the current summary provides effect size estimates that are useful in establishing design and training guidelines with reference to robot-related factors of HRI trust. Furthermore, results indicate that improper trust calibration may be mitigated by the manipulation of robot design. However, many future research needs are identified.},
	file         = {:Users/yosef/Library/Application Support/Mendeley Desktop/Downloaded/Hancock et al. - 2011 - A Meta-Analysis of Factors Affecting Trust in Human-Robot Interaction.pdf:pdf},
	pmid         = 22046724
}
@article{article,
	title        = {{Almost Human: Anthropomorphism Increases Trust Resilience in Cognitive Agents}},
	author       = {de Visser, Ewart and Monfort, Samuel and Mckendrick, Ryan and Smith, Melissa and Mcknight, Patrick and Krueger, Frank and Parasuraman, Raja},
	year         = 2016,
	journal      = {Journal of Experimental Psychology: Applied},
	volume       = 22,
	doi          = {10.1037/xap0000092},
	mendeley-groups = {Trust_Metrics_Analysis}
}

@article{Dzindolet2001,
	title        = {{A framework of automation use}},
	author       = {Dzindolet, Mary T. and Beck, HP and Pierce, LG and Dawe, LA},
	year         = 2001,
	number       = {March},
	isbn         = {ARL-TR-2412},
	url          = {http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA390225},
	file         = {:Users/yosef/Downloads/ADA390225.pdf:pdf}
}
@article{kohn2021measurement,
	title        = {Measurement of trust in automation: A narrative review and reference guide},
	author       = {Kohn, Spencer C and De Visser, Ewart J and Wiese, Eva and Lee, Yi-Ching and Shaw, Tyler H},
	year         = 2021,
	journal      = {Frontiers in Psychology},
	publisher    = {Frontiers Media SA},
	volume       = 12
}
@book{nam2020trust,
	title        = {Trust in Human-Robot Interaction},
	author       = {Nam, Chang S and Lyons, Joseph B},
	year         = 2020,
	publisher    = {Academic Press}
}
@inproceedings{BusTrust,
	title        = {{Supporting trust in autonomous driving}},
	author       = {H{\"{a}}uslschmid, Renate and von Buelow, Max and Pfleging, Bastian and Butz, Andreas},
	year         = 2017,
	booktitle    = {Proc. of the 22nd intl. conf. on intelligent user interfaces},
	pages        = {319--329},
	file         = {:Users/yosef/Downloads/haeuslschmid2017iui.pdf:pdf},
	organization = {ACM}
}
@article{rusbult_80,
	title        = {{Commitment and satisfaction in romantic associations: A test of the investment model}},
	author       = {Rusbult, Caryl E},
	year         = 1980,
	journal      = {J. of Experimental Social Psych.},
	publisher    = {Elsevier},
	volume       = 16,
	number       = 2,
	pages        = {172--186}
}
@article{rempel,
	title        = {{Trust in close relationships.}},
	author       = {Rempel, John K and Holmes, John G and Zanna, Mark P},
	year         = 1985,
	journal      = {J. of Personality and Social Psych.},
	publisher    = {American Psychological Association},
	volume       = 49,
	number       = 1,
	pages        = 95
}
@article{WeRobot,
	title        = {{Framing human-automation regulation: A new modus operandi from cognitive engineering}},
	author       = {Canellas, Marc C and Miller, Matthew J and Razin, Yosef S and Minotra, Dev and Bhattacharyya, Raunak and Haga, Rachel A},
	year         = {2017. Unpublished},
	journal      = {WeRobot Conference},
	address      = {New Haven: WeRobot Conference},
	institution  = {Georgia Institute of Technology}
}
@article{rusbult_93,
	title        = {{Commitment processes in close relationships: An interdependence analysis}},
	author       = {Rusbult, Caryl E and Buunk, Bram P},
	year         = 1993,
	journal      = {J. of Social and Personal Relationships},
	publisher    = {Sage Publications Sage CA: Thousand Oaks, CA},
	volume       = 10,
	number       = 2,
	pages        = {175--204}
}
@article{yamagishi,
	title        = {{Trust as a form of social intelligence.}},
	author       = {Yamagishi, Toshio},
	year         = 2001,
	journal      = {Trust in Society},
	publisher    = {Russell Sage Foundation},
	address      = {New York, New York, USA},
	pages        = {121--147},
	editor       = {Cook, K.S.}
}
@book{kelley_03,
	title        = {{An Atlas of Interpersonal Situations}},
	author       = {Kelley, Harold H and Holmes, John G and Kerr, Norbert L and Reis, Harry T and Rusbult, Caryl E and {Van Lange}, Paul A M},
	year         = 2003,
	publisher    = {Cambridge University Press},
	address      = {New York, NY}
}
@book{thibaut_59,
	title        = {{The Social Psychology of Groups.}},
	author       = {Thibaut, John W and Kelley, Harold H},
	year         = 1959,
	publisher    = {J. Wiley \& Sons},
	address      = {New York}
}
@article{higgins,
	title        = {{Information Security Management: The ISO 27000 (ISO 27K) Series}},
	author       = {Higgins, Sarah},
	year         = 2009,
	journal      = {Digital Curation Center}
}
@book{giddens,
	title        = {{The Consequences of Modernity}},
	author       = {Giddens, Anthony},
	year         = 1990,
	publisher    = {Polity Press},
	address      = {Standford, CA}
}
@book{gottman_05,
	title        = {{The Mathematics of Marriage: Dynamic nonlinear models}},
	author       = {Gottman, John Mordechai},
	year         = 2005,
	publisher    = {MIT Press},
	address      = {Cambridge, MA}
}
@article{nash,
	title        = {{Equilibrium points in n-person games}},
	author       = {Nash, John F},
	year         = 1950,
	journal      = {Proc. of the National Academy of Sciences},
	publisher    = {USA},
	volume       = 36,
	number       = 1,
	pages        = {48--49}
}

@article{vanLange,
	title        = {{Interdependence theory}},
	author       = {{Van Lange}, Paul A M and Rusbult, Caryl E},
	year         = 2011,
	journal      = {Handbook of theories of soc. psych.},
	publisher    = {Sage Thousand Oaks, CA},
	volume       = 2,
	pages        = {251--272}
}
@article{wieselquist,
	title        = {{Commitment, pro-relationship behavior, and trust in close relationships.}},
	author       = {Wieselquist, Jennifer and Rusbult, Caryl E and Foster, Craig A and Agnew, Christopher R},
	year         = 1999,
	journal      = {Journal of personality and social psychology},
	publisher    = {American Psychological Association},
	volume       = 77,
	number       = 5,
	pages        = 942
}
@article{hazan,
	title        = {{Romantic love conceptualized as an attachment process.}},
	author       = {Hazan, Cindy and Shaver, Phillip},
	year         = 1987,
	journal      = {J. of Personality and Social Psych.},
	publisher    = {American Psychological Association},
	volume       = 52,
	number       = 3,
	pages        = 511
}
@book{luhmann,
	title        = {{Trust and Power}},
	author       = {Luhmann, Niklas},
	year         = 1979,
	publisher    = {J. Wiley {\&} Sons},
	address      = {Stuttgart}
}
@book{castelfranchi,
	title        = {{Trust Theory: A Socio-Cognitive and Computational Model}},
	author       = {Castelfranchi, Christiano and Falcone, Rino},
	year         = 2010,
	publisher    = {J. Wiley {\&} Sons},
	address      = {West Sussex}
}
@phdthesis{robinette,
	title        = {{Developing robots that impact human-robot trust in emergency evacuations}},
	author       = {Robinette, Paul},
	year         = 2015,
	school       = {Georgia Institute of Technology}
}
@phdthesis{wagner,
	title        = {{The role of trust and relationships in human-robot social interaction}},
	author       = {Wagner, Alan Richard},
	year         = 2009,
	school       = {Georgia Institute of Technology}
}
@book{kelley_78,
	title        = {{Interpersonal Relations: A Theory of Interdependence}},
	author       = {Kelley, H H and Thibaut, J W},
	year         = 1978,
	publisher    = {J. Wiley {\&} Sons},
	address      = {New York, NY}
}
@article{Rotter_67,
	title        = {{A new scale for the measurement of interpersonal trust}},
	author       = {Rotter, J.B.},
	year         = 1967,
	journal      = {J. of Personality},
	volume       = 35,
	number       = 4,
	pages        = {651--655},
	pmid         = 4865583
}
@article{Deutsch_60,
	title        = {{Trust, trustworthiness, and the F scale}},
	author       = {Deutsch, Morton},
	year         = 1960,
	journal      = {J. of Abnormal \& Social Psych.},
	volume       = 61,
	number       = 1,
	pages        = {138--140}
}
@article{Johnson_14,
	title        = {{Coactive design: Designing support for interdependence in joint activity}},
	author       = {Johnson, Matthew and Bradshaw, Jeffrey M and Feltovich, Paul J and Jonker, Catholijn M and van Riemsdijk, M Birna and Sierhuis, Maarten},
	year         = 2014,
	journal      = {J. of Human-Robot Interaction},
	volume       = 3,
	number       = 1,
	pages        = {43--69},
	annote       = {NULL}
}
@article{Lee_94,
	title        = {{Trust, self-confidence, and operators' adaptation to automation}},
	author       = {Lee, John D. and Moray, Neville},
	year         = 1994,
	journal      = {Intl. J. of Human-Computer Studies},
	volume       = 40,
	pages        = {153--184},
	doi          = {10.1006/ijhc.1994.1007},
	isbn         = {1071-5819},
	issn         = 10715819,
	url          = {http://linkinghub.elsevier.com/retrieve/pii/S107158198471007X},
	annote       = {NULL},
	file         = {:Users/yosef/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Moray - 1994 - Trust, self-confidence, and operators' adaptation to automation.pdf:pdf}
}
@article{o1998empirical,
	title        = {The empirical assessment of construct validity},
	author       = {O'Leary-Kelly, Scott W and Vokurka, Robert J},
	year         = 1998,
	journal      = {Journal of operations management},
	publisher    = {Elsevier},
	volume       = 16,
	number       = 4,
	pages        = {387--405}
}
@inproceedings{kizilcec,
	title        = {How much information?: Effects of transparency on trust in an algorithmic interface},
	author       = {Kizilcec, Ren{\'e} F},
	year         = 2016,
	booktitle    = {Proc. of the 2016 CHI Conference on Human Factors in Computing Systems},
	pages        = {2390--2395},
	organization = {ACM}
}
@inproceedings{mathur2009uncanny,
	title        = {An uncanny game of trust: Social trustworthiness of robots inferred from subtle anthropomorphic facial cues},
	author       = {Mathur, Maya B and Reichling, David B},
	year         = 2009,
	booktitle    = {2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
	pages        = {313--314},
	organization = {IEEE}
}
@article{schriesheim1981controlling,
	title        = {Controlling acquiescence response bias by item reversals: The effect on questionnaire validity},
	author       = {Schriesheim, Chester A and Hill, Kenneth D},
	year         = 1981,
	journal      = {Educational and psychological measurement},
	publisher    = {Sage Publications Sage CA: Los Angeles, CA},
	volume       = 41,
	number       = 4,
	pages        = {1101--1114}
}
@article{barnette2000effects,
	title        = {Effects of stem and Likert response option reversals on survey internal consistency: If you feel the need, there is a better alternative to using those negatively worded stems},
	author       = {Barnette, J Jackson},
	year         = 2000,
	journal      = {Educational and psychological measurement},
	publisher    = {Sage Publications Sage CA: Thousand Oaks, CA},
	volume       = 60,
	number       = 3,
	pages        = {361--370}
}
@article{de2016almost,
	title        = {Almost human: Anthropomorphism increases trust resilience in cognitive agents.},
	author       = {de Visser, Ewart J and Monfort, Samuel S and McKendrick, Ryan and Smith, Melissa AB and McKnight, Patrick E and Krueger, Frank and Parasuraman, Raja},
	year         = 2016,
	journal      = {J. of Experimental Psych.: Applied},
	publisher    = {American Psychological Association},
	volume       = 22,
	number       = 3,
	pages        = 331
}
@article{Deligianis2017,
	title        = {{The Impact of Intergroup Bias on Trust and Approach Behaviour Towards a Humanoid Robot}},
	author       = {Deligianis, Christopher and Stanton, Christopher and Mcgarty, Craig and Stevens, Catherine J},
	year         = 2017,
	volume       = 6,
	number       = 3,
	pages        = {4--20},
	doi          = {10.5898/JHRI.6.3.Deligianis},
	file         = {:Users/yosef/Downloads/JHRI.6.3.Deligianis.pdf:pdf},
	keywords     = {compliance,gaze,hri,intergroup bias,proxemics,social identity theory,trust},
	mendeley-groups = {Trust_Metrics_Analysis},
 journal={Journal of human-robot interaction}
}
@inproceedings{BusTrust,
	title        = {{Supporting trust in autonomous driving}},
	author       = {H{\"{a}}uslschmid, Renate and von Buelow, Max and Pfleging, Bastian and Butz, Andreas},
	year         = 2017,
	booktitle    = {Proceedings of the 22nd international conference on intelligent user interfaces},
	pages        = {319--329},
	file         = {:Users/yosef/Downloads/haeuslschmid2017iui.pdf:pdf},
	mendeley-groups = {ATC,Trust_Metrics_Analysis},
	organization = {ACM}
}
@article{Zanatto2020,
	title        = {{Generalisation of Anthropomorphic Stereotype}},
	author       = {Zanatto, Debora and Patacchiola, Massimiliano and Cangelosi, Angelo and Goslin, Jeremy},
	year         = 2020,
	journal      = {International Journal of Social Robotics},
	publisher    = {Springer Netherlands},
	volume       = 12,
	number       = 1,
	pages        = {163--172},
	doi          = {10.1007/s12369-019-00549-4},
	issn         = {1875-4805},
	url          = {https://doi.org/10.1007/s12369-019-00549-4},
	file         = {:Users/yosef/Downloads/Zanatto2020_Article_GeneralisationOfAnthropomorphi.pdf:pdf},
	keywords     = {Stereotype activation,Anthropomorphism,Trust,Human,anthropomorphism,human,robot interaction,stereotype activation,trust},
	mendeley-groups = {Trust_Metrics_Analysis}
}
@article{boudreau2001validation,
	title        = {{Validation in information systems research: a state-of-the-art assessment}},
	author       = {Boudreau, Marie-Claude and Gefen, David and Straub, Detmar W},
	year         = 2001,
	journal      = {MIS quarterly},
	publisher    = {JSTOR},
	pages        = {1--16},
	file         = {:Users/yosef/Downloads/Validation_in_Information_Systems_Resear20160427-24532-14o1lue-with-cover-page-v2.pdf:pdf},
	mendeley-groups = {Reliability and Validation}
}
@article{Straub1989,
	title        = {{Validating Instruments in MIS Linking MiS Research and}},
	author       = {Straub, Detmar W.},
	year         = 1989,
	journal      = {MIS Quarterly},
	number       = {June 1986},
	pages        = {103--118},
	file         = {:Users/yosef/Downloads/ContentServer.asp-3.pdf:pdf},
	keywords     = {cai measurement,construct validity,content va-,empiri-,lidity,mis research methodology,reliability,theory construc-,tion and development},
	mendeley-groups = {Reliability and Validation}
}
@article{MacKenzie2011,
	title        = {{Construct measurement and validation procedures in MIS and behavioral research: Integrating new and existing techniques}},
	author       = {MacKenzie, Scott B. and Podsakoff, Philip M. and Podsakoff, Nathan P.},
	year         = 2011,
	journal      = {MIS Quarterly: Management Information Systems},
	volume       = 35,
	number       = 2,
	pages        = {293--334},
	doi          = {10.2307/23044045},
	issn         = {02767783},
	file         = {:Users/yosef/Downloads/mackenzie_et_al_2011_kapitel_3_02.pdf:pdf},
	keywords     = {Construct validation procedures,Content, convergent, discriminant and nomological validity,Formative and reflective indicator models,Scale development and validation},
	mendeley-groups = {Reliability and Validation}
}
@article{rajaonah2008role,
	title        = {The role of intervening variables in driver--ACC cooperation},
	author       = {Rajaonah, Bako and Tricot, Nicolas and Anceaux, Fran{\c{c}}oise and Millot, Patrick},
	year         = 2008,
	journal      = {International journal of human-computer studies},
	publisher    = {Elsevier},
	volume       = 66,
	number       = 3,
	pages        = {185--197}
}
@article{selkowitz2017using,
	title        = {Using agent transparency to support situation awareness of the Autonomous Squad Member},
	author       = {Selkowitz, Anthony R and Lakhmani, Shan G and Chen, Jessie YC},
	year         = 2017,
	journal      = {Cognitive Systems Research},
	publisher    = {Elsevier},
	volume       = 46,
	pages        = {13--25}
}
@article{nomura2008prediction,
	title        = {Prediction of human behavior in human--robot interaction using psychological scales for anxiety and negative attitudes toward robots},
	author       = {Nomura, Tatsuya and Kanda, Takayuki and Suzuki, Tomohiro and Kato, Kensuke},
	year         = 2008,
	journal      = {IEEE transactions on robotics},
	publisher    = {IEEE},
	volume       = 24,
	number       = 2,
	pages        = {442--451}
}
@article{Kim2020,
	title        = {{Factors affecting trust in high-vulnerability human-robot interaction contexts: A structural equation modelling approach}},
	author       = {Kim, Wonjoon and Kim, Nayoung and Lyons, Joseph B. and Nam, Chang S.},
	year         = 2020,
	journal      = {Applied Ergonomics},
	publisher    = {Elsevier Ltd},
	volume       = 85,
	number       = {November 2019},
	pages        = 103056,
	doi          = {10.1016/j.apergo.2020.103056},
	issn         = 18729126,
	url          = {https://doi.org/10.1016/j.apergo.2020.103056},
	abstract     = {The current research proposed and tested a structural equation model (SEM) that describes hypothesized relationships among factors affecting trust in human-robot interaction (HRI) such as trustworthiness, human-likeness, intelligence, perfect automation schema (PAS), and affect. A video stimulus depicting an autonomous guard robot interacting with humans was employed as a stimulus via Amazon's Mechanical Turk to recruit 233 participants. Human-related and robot-related metrics were found to affect trustworthiness that subsequently affected trust. In particular, ability (as a trustworthiness facet) was a dominant factor affecting trust in HRI. Integrity was found to mediate the relationships between robot- and human-related metrics and trustworthiness. This study also showed a correlation between intelligence and trustworthiness, as well as between PAS and trustworthiness. The findings of the present study have significant implications for both theory and practice on factors and levels that affect trust in HRI.},
	file         = {:Users/yosef/Downloads/1-s2.0-S0003687020300132-main.pdf:pdf},
	keywords     = {Amazon's mechanical turk,Autonomous guard robot,Human-robot interaction,Structural equation modelling,Trustworthiness},
	pmid         = 32174344
}
@article{Banks2019,
	title        = {{A perceived moral agency scale: Development and validation of a metric for humans and social machines}},
	author       = {Banks, Jaime},
	year         = 2019,
	journal      = {Computers in Human Behavior},
	publisher    = {Elsevier},
	volume       = 90,
	number       = {November 2017},
	pages        = {363--371},
	doi          = {10.1016/j.chb.2018.08.028},
	issn         = {07475632},
	url          = {https://doi.org/10.1016/j.chb.2018.08.028},
	file         = {:Users/yosef/Downloads/1-s2.0-S0747563218304035-main.pdf:pdf},
	keywords     = {Anthropomorphism,Attraction,Moral agency,Robots,Scale development,Social machines,Trust}
}
@article{de1979rationality,
	title        = {{The rationality of emotions}},
	author       = {{De Sousa}, Ronald},
	year         = 1979,
	journal      = {Dialogue: Canadian Philosophical Review/Revue Canadienne de Philosophie},
	publisher    = {Cambridge University Press},
	volume       = 18,
	number       = 1,
	pages        = {41--63},
	mendeley-groups = {Rationality of Emtions}
}
@article{haselton2006irrational,
	title        = {{Irrational emotions or emotional wisdom? The evolutionary psychology of affect and social behavior}},
	author       = {Haselton, Martie G and Ketelaar, Timothy},
	year         = 2006,
	journal      = {Affect in social thinking and behavior},
	publisher    = {Psychology Press New York},
	volume       = 8,
	pages        = 21,
	mendeley-groups = {Rationality of Emtions}
}
@article{solomon1973emotions,
	title        = {{Emotions and choice}},
	author       = {Solomon, Robert C},
	year         = 1973,
	journal      = {The Review of Metaphysics},
	volume       = 27,
	number       = 1,
	pages        = {20--41},
	mendeley-groups = {Rationality of Emtions}
}
@article{gubka2022there,
	title        = {{There Are No Irrational Emotions}},
	author       = {Gubka, Steven},
	year         = 2022,
	journal      = {Pacific Philosophical Quarterly},
	publisher    = {Wiley Online Library},
	volume       = 103,
	number       = 2,
	pages        = {293--317},
	mendeley-groups = {Rationality of Emtions}
}
@article{pham2007emotion,
	title        = {{Emotion and rationality: A critical review and interpretation of empirical evidence}},
	author       = {Pham, Michel Tuan},
	year         = 2007,
	journal      = {Review of general psychology},
	publisher    = {SAGE Publications Sage CA: Los Angeles, CA},
	volume       = 11,
	number       = 2,
	pages        = {155--178},
	mendeley-groups = {Rationality of Emtions}
}
@article{van1995might,
	title        = {What might cognition be, if not computation?},
	author       = {Van Gelder, Tim},
	year         = 1995,
	journal      = {The Journal of Philosophy},
	publisher    = {JSTOR},
	volume       = 92,
	number       = 7,
	pages        = {345--381}
}
@book{piccinini2020neurocognitive,
	title        = {Neurocognitive mechanisms: Explaining biological cognition},
	author       = {Piccinini, Gualtiero},
	year         = 2020,
	publisher    = {Oxford University Press}
}
@incollection{clarke2013trust,
	title        = {{A Trust-Based Argument against Paternalism}},
	author       = {Clarke, Simon},
	year         = 2013,
	booktitle    = {Trust},
	publisher    = {Brill},
	pages        = {53--75},
	mendeley-groups = {Paternalism}
}
@article{Kocak2021,
	title        = {{How Does Paternalistic Leadership Affect Employees' Work Engagement? The Mediating Roles of Workaholism and Trust-in-Leader}},
	author       = {Ko{\c{c}}ak, {\"{O}}mer Erdem and {Aydın K{\"{u}}{\c{c}}{\"{u}}k}, Burcu},
	year         = 2021,
	journal      = {Journal of Humanity and Society (insan \& toplum)},
	volume       = 11,
	number       = 3,
	pages        = {179--196},
	doi          = {10.12658/m0631},
	abstract     = {In this study, we contend that paternalist leadership can be an effective way of managing people and can pave the way for employee motivation and well-being, despite several previous studies linking it to adverse outcomes. In addition, we propose two possible underlying mechanisms (i.e., workaholism, trust in leadership) linking a leader's paternalistic style to employee work engagement. By doing so, we aim to understand whether paternalist leaders positively influence their subordinates through a social connection path (trust in leader) or task-focusing path (workaholism). We conducted a field survey and collected cross-sectional data using online surveys from 413 participants working in various industries in Istanbul to test the hypotheses. The results indicate a positive relationship to exist between paternalistic leadership and employee work engagement. Therefore, we put forth that the paternalistic leadership style can be beneficial through the task-focusing and social connection paths, contrary to the beliefs commonly shared in Western countries.},
	file         = {:Users/yosef/Downloads/Kocak-Omer-2021.pdf:pdf},
	keywords     = {ayrıca,babacan liderliğin insanları y{\"{o}}netmenin,bu {\c{c}}alışmada,etkili bir yolu olabileceği,ile,işkoliklik ve lidere g{\"{u}}ven,liderin babacan tarzını {\c{c}}alışanın,miştir,olumsuz sonu{\c{c}}larla ilişkilendirilmesine rağmen,paternalistic leader,trust in leader,tutkun {\c{c}}alışmasıyla ilişkilendirmek i{\c{c}}in,ve daha {\"{o}}nceki birka{\c{c}},work engagement,workaholism,zemin hazırlayabileceği iddia edil-,{\c{c}}alışan motivasyonu ve esenliğine,{\c{c}}alışmada,{\"{o}}z},
	mendeley-groups = {Paternalism}
}
@article{Pellegrini2008,
	title        = {{Paternalistic leadership: A review and agenda for future research}},
	author       = {Pellegrini, Ekin K. and Scandura, Terri A.},
	year         = 2008,
	journal      = {Journal of Management},
	volume       = 34,
	number       = 3,
	pages        = {566--593},
	doi          = {10.1177/0149206308316063},
	issn         = {01492063},
	abstract     = {The growing interest in paternalistic leadership research has led to a recent proliferation of diverse definitions and perspectives, as well as a limited number of empirical studies. Consequently, the diversity of perspectives has resulted in conceptual ambiguities, as well as contradictory empirical findings. In this article, the authors review research on paternalistic leadership in an effort to assess the current state of the literature. They investigate the construct of paternalistic leadership and review the findings related to its outcomes and antecedents as well as the various measurement scales used in paternalistic leadership research. On the basis of this review, the article concludes with an agenda for future theoretical and empirical research on this emerging and intriguing new area for leadership research. {\textcopyright} 2008 Southern Management Association. All rights reserved.},
	file         = {:Users/yosef/Downloads/0149206308316063.pdf:pdf},
	keywords     = {Cross-cultural,Leadership,Paternalism,Paternalistic leadership},
	mendeley-groups = {Paternalism}
}
@article{Shi2020,
	title        = {{Exploring the Relationship Between Paternalistic Leadership, Teacher Commitment, and Job Satisfaction in Chinese Schools}},
	author       = {Shi, Xiao and Yu, Zeyuan and Zheng, Xin},
	year         = 2020,
	journal      = {Frontiers in Psychology},
	volume       = 11,
	number       = {July},
	pages        = {1--12},
	doi          = {10.3389/fpsyg.2020.01481},
	issn         = 16641078,
	abstract     = {Paternalistic leadership (PL) is prevalent in organizations in East Asia, but few studies have examined its potential effects in school contexts. This study explored the relationship between PL, trust in the principal, and teachers' satisfaction and commitment to students, with a focus on the mediating role of trust in the principal in Chinese schools. Using a quantitative method, the study investigated 408 primary schoolteachers in mainland China. The results showed that the three dimensions of PL had different effects on teachers' job satisfaction, trust in the principal, and commitment to students. Moral leadership had positive effects, while authoritarian leadership had negative effects on teachers' job satisfaction and commitment to students. Meanwhile, trust in the principal played a mediating role of authoritarian and moral leadership on teachers' job satisfaction and commitment to students. Finally, implications and suggestions are discussed for leadership practices in Chinese schools and those in similar cultures.},
	file         = {:Users/yosef/Downloads/fpsyg-11-01481.pdf:pdf},
	keywords     = {Chinese contexts,commitment to students,job satisfaction,paternalistic leadership,teacher commitment,trust in the principal},
	mendeley-groups = {Paternalism}
}
@article{Krausman2022,
	title        = {{Trust Measurement in Human-Autonomy Teams: Development of a Conceptual Toolkit}},
	author       = {Krausman, Andrea and Neubauer, Catherine and Forster, Daniel and Lakhmani, Shan and Baker, Anthony L. and Fitzhugh, Sean M. and Gremillion, Gregory and Wright, Julia L. and Metcalfe, Jason S. and Schaefer, Kristin E.},
	year         = 2022,
	journal      = {ACM Transactions on Human-Robot Interaction},
	volume       = 11,
	number       = 3,
	doi          = {10.1145/3530874},
	issn         = 25739522,
	abstract     = {The rise in artificial intelligence capabilities in autonomy-enabled systems and robotics has pushed research to address the unique nature of human-autonomy team collaboration. The goal of these advanced technologies is to enable rapid decision-making, enhance situation awareness, promote shared understanding, and improve team dynamics. Simultaneously, use of these technologies is expected to reduce risk to those who collaborate with these systems. Yet, for appropriate human-autonomy teaming to take place, especially as we move beyond dyadic partnerships, proper calibration of team trust is needed to effectively coordinate interactions during high-risk operations. But to meet this end, critical measures of team trust for this new dynamic of human-autonomy teams are needed. This article seeks to expand on trust measurement principles and the foundation of human-autonomy teaming to propose a "toolkit" of novel methods that support the development, maintenance, and calibration of trust in human-autonomy teams operating within uncertain, risky, and dynamic environments.},
	file         = {:Users/yosef/Downloads/3530874.pdf:pdf},
	keywords     = {Human-autonomy teaming,robot,team trust,trust measurement}
}
@article{Niu2018,
	title        = {{Anthropomorphizing information to enhance trust in autonomous vehicles}},
	author       = {Niu, Dongfang and Terken, Jacques and Eggen, Berry},
	year         = 2018,
	journal      = {Human Factors and Ergonomics In Manufacturing},
	volume       = 28,
	number       = 6,
	pages        = {352--359},
	doi          = {10.1002/hfm.20745},
	issn         = 15206564,
	abstract     = {Trust is an essential condition for accepting and relying on autonomous vehicles. One of the well-studied factors contributing to trust in automation systems is anthropomorphism. It is expected that anthropomorphism may also enhance trust in the field of autonomous vehicles. A study is presented that investigated the effect of anthropomorphic embodiment for information about the vehicle's maneuvers on people's trust in autonomous vehicles. In a driving simulator experiment deploying a between-subjects design, participants (N = 39) were exposed to varying amounts of information displaying upcoming actions of an autonomous vehicle (symbolic information or symbolic + anthropomorphic information). Each group rated trust and liking for the test condition against a reference condition where no information about the upcoming actions was provided through questionnaires. Symbolic + anthropomorphic information resulted in significantly more trust than symbolic information. Through one-sample tests, it was found that ratings for symbolic + anthropomorphic information were significantly different from no information, while symbolic information by itself was not. Ratings for perceived anthropomorphism were positively correlated with trust and liking. It is concluded that anthropomorphizing information may foster the perception of autonomous vehicles as social agents and enhance trust in those vehicles.},
	file         = {:Users/yosef/Downloads/hfm.20745.pdf:pdf},
	keywords     = {anthropomorphism,autonomous vehicles,human-vehicle interaction,information transparency,trust},
	mendeley-groups = {Jian_Experiments}
}
@article{Beggiato2013,
	title        = {{The evolution of mental model, trust and acceptance of adaptive cruise control in relation to initial information}},
	author       = {Beggiato, Matthias and Krems, Josef F.},
	year         = 2013,
	journal      = {Transportation Research Part F: Traffic Psychology and Behaviour},
	publisher    = {Elsevier Ltd},
	volume       = 18,
	pages        = {47--57},
	doi          = {10.1016/j.trf.2012.12.006},
	issn         = 13698478,
	url          = {http://dx.doi.org/10.1016/j.trf.2012.12.006},
	abstract     = {Adaptive cruise control (ACC) automates vehicle speed and distance control. Due to sensor limitations, not every situation can be handled by the system and, therefore, driver intervention is required. Trust, acceptance and mental model of system functionality are considered key variables for appropriate system use. This study systematically investigates the effect of divergent initial mental models of ACC (i.e., varying according to correctness) on trust, acceptance and mental model evolvement. A longitudinal driving simulator study was conducted, using a two-way (3 × 3) repeated measures mixed design with a matched sample of 51 subjects. Three experimental groups received (1) a correct ACC description, (2) an incomplete and idealised account omitting potential problems, and (3) an incorrect description including non-occurring problems. All subjects drove a 56-km track of highway with an identical ACC system, three times, and within a period of 6 weeks. After using the system, participants' mental model of ACC converged towards the profile of the correct group. Non-experienced problems tended to disappear from the mental model network when they were not activated by experience. Trust and acceptance grew steadily for the correct condition. The same trend was observed for the group with non-occurring problems, starting from a lower initial level. Omitted problems in the incomplete group led to a constant decrease in trust and acceptance without recovery. This indicates that automation failures do not negatively affect trust and acceptance if they are known beforehand. A strategy reliant upon trial-and-error alone is considered insufficient for developing an appropriate trust, acceptance and mental model. Implications on information and learning strategies are discussed. {\textcopyright} 2012 Elsevier Ltd. All rights reserved.},
	annote       = {Also there own mental model scale and acceptance scale from van der laan},
	file         = {:Users/yosef/Downloads/1-s2.0-S1369847813000028-main.pdf:pdf},
	keywords     = {ACC,Acceptance,Adaptive cruise control,Initial information,Mental model,Trust},
	mendeley-groups = {Misc. Trust Measures,Jian_Experiments}
}
@article{Verberne2015,
	title        = {{Trusting a Virtual Driver That Looks, Acts, and Thinks Like You}},
	author       = {Verberne, Frank M.F. and Ham, Jaap and Midden, Cees J.H.},
	year         = 2015,
	journal      = {Human Factors},
	volume       = 57,
	number       = 5,
	pages        = {895--909},
	doi          = {10.1177/0018720815580749},
	issn         = 15478181,
	abstract     = {Objective: We examined whether participants would trust an agent that was similar to them more than an agent that was dissimilar to them. Background: Trust is an important psychological factor determining the acceptance of smart systems. Because smart systems tend to be treated like humans, and similarity has been shown to increase trust in humans, we expected that similarity would increase trust in a virtual agent. Methods: In a driving simulator experiment, participants (N = 111) were presented with a virtual agent that was either similar to them or not. This agent functioned as their virtual driver in a driving simulator, and trust in this agent was measured. Furthermore, we measured how trust changed with experience. Results: Prior to experiencing the agent, the similar agent was trusted more than the dissimilar agent. This effect was mediated by perceived similarity. After experiencing the agent, the similar agent was still trusted more than the dissimilar agent. Conclusion: Just as similarity between humans increases trust in another human, similarity also increases trust in a virtual agent. When such an agent is presented as a virtual driver in a self-driving car, it could possibly enhance the trust people have in such a car. Application: Displaying a virtual driver that is similar to the human driver might increase trust in a selfdriving car.},
	file         = {:Users/yosef/Downloads/0018720815580749.pdf:pdf},
	keywords     = {facial similarity,liking,mimicry,perceived similarity,shared goals,similarity,trust,virtual agent},
	mendeley-groups = {Jian_Experiments},
	pmid         = 25921302
}
@article{spain2008effect,
	title        = {The effect of sonification display pulse rate and reliability on operator trust and perceived workload during a simulated patient monitoring task},
	author       = {Spain, Randall D and Bliss, James P},
	year         = 2008,
	journal      = {Ergonomics},
	publisher    = {Taylor \& Francis},
	volume       = 51,
	number       = 9,
	pages        = {1320--1337}
}
@inproceedings{gutzwiller2019positive,
	title        = {Positive bias in the ‘Trust in Automated Systems Survey’? An examination of the Jian et al.(2000) scale},
	author       = {Gutzwiller, Robert S and Chiou, Erin K and Craig, Scotty D and Lewis, Christina M and Lematta, Glenn J and Hsiung, Chi-Ping},
	year         = 2019,
	booktitle    = {Proceedings of the Human Factors and Ergonomics Society annual meeting},
	volume       = 63,
	number       = 1,
	pages        = {217--221},
	organization = {SAGE Publications Sage CA: Los Angeles, CA}
}
@inproceedings{spain2008towards,
	title        = {Towards an empirically developed scale for system trust: Take two},
	author       = {Spain, Randall D and Bustamante, Ernesto A and Bliss, James P},
	year         = 2008,
	booktitle    = {Proceedings of the human factors and ergonomics society annual meeting},
	volume       = 52,
	number       = 19,
	pages        = {1335--1339},
	organization = {SAGE Publications Sage CA: Los Angeles, CA}
}