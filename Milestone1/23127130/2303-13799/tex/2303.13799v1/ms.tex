\documentclass[journal,peerreview,twocolumn]{IEEEtran}

\usepackage[pdftex]{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage[noadjust]{cite}
%\usepackage{flushend}
\usepackage{amsmath}
\interdisplaylinepenalty=2500
\usepackage{algorithmic}
\usepackage{array}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{slashbox,pict2e}
\usepackage{colortbl}
\usepackage{adjustbox}
\usepackage[pdftex]{lscape} 
\usepackage{longtable}
\usepackage{caption}
\usepackage{apacite}
\usepackage{balance}
% \usepackage{lineno}
\renewcommand{\thetable}{\arabic{table}}
\usepackage{setspace}
\usepackage{amssymb}
\usepackage{marvosym}


\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}
\newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1pt\hskip\tabcolsep}}
\makeatother


\begin{document}
%\linenumbers
\title{ 

Converging Measures and an Emergent Model: A Meta-Analysis of Human-Automation Trust Questionnaires} 

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Yosef S. Razin$^*$ \& Karen M. Feigh}\\
\IEEEauthorblockA{School of Aerospace Engineering\\
Georgia Institute of Technology\\
Atlanta, Georgia 30332--0250\\
$^*$Email: yrazin@gatech.edu}
}


\maketitle

\begin{minipage}{\textwidth}
\centering
\textit{Running Head:} A META-ANALYSIS OF HAI TRUST QUESTIONNAIRES\\
\textit{Manuscript Type:} Review Article\\
\textit{Acknowledgements:} Address correspondence to Yosef Razin, Georgia Institute of Technology, Atlanta, GA; yrazin@gatech.edu
\end{minipage}

\clearpage


%One of the greatest challenges to measuring human-robot and human-automation trust is the sheer amount of construct proliferation, models, and available questionnaires with highly variable validation.  This work identifies the most frequently cited and best-validated human-automation and human-robot trust questionnaires as well as the most well-established factors, which form the dimensions and antecedents of such trust.  To reduce both confusion and construct proliferation, we provide a detailed mapping of terminology between the survey instruments.  Furthermore, we perform a meta-analysis of the regression models which emerged from those experiments which used multi-factorial survey instruments.  Based on this meta-analysis, we provide the most complete, experimentally validated model of human-automation and human-robot trust.  This convergent, validated model establishes a solid and integrated framework for future research.  It identifies the current boundaries of trust measurement and where further investigation is necessary.  We close by discussing how to choose an appropriate trust survey instrument and further design considerations.


\begin{minipage}{\textwidth}
\begin{center}
    
    \large\textbf{Abstract Page}
\end{center}

% \textbf{Objective:} We synthesize a consensus model for trust in human-automation interaction by performing a meta-analysis of validated and reliable trust survey instruments.\\
% \textbf{Background:} A significant challenge to measuring human-automation trust is the amount of construct proliferation, models, and questionnaires with highly variable validation.  However, all agree that trust is a crucial element of technological acceptance, as well as continued usage, fluency, and teamwork.\\
% \textbf{Method:} This work identifies the most frequently cited and best-validated human-automation and human-robot trust questionnaires as well as the most well-established factors, which form the dimensions and antecedents of such trust.  To reduce both confusion and construct proliferation, we provide a detailed mapping of terminology between questionnaires.  Furthermore, we perform a meta-analysis of the regression models which emerged from those experiments which used multi-factorial survey instruments.\\
% \textbf{Results:} Based on this meta-analysis, we provide the most complete, experimentally validated model of human-automation and human-robot trust.  This convergent model establishes an integrated framework for future research.  It identifies the current boundaries of trust measurement and where further investigation is necessary.  We close by discussing how to choose and design an appropriate trust survey instrument.\\
% \textbf{Conclusion:} By comparing, mapping, and analyzing well-constructed trust survey instruments, a consensus structure of trust in human-automation interaction emerges.
% \\
% \textbf{Application:} By identifying the internal workings of trust, a more complete basis for measuring trust emerges that is widely applicable.  It integrates the academic idea of trust with the colloquial, common-sense one.  Trust is of increasing importance and is critical to properly understand and capture it.\\

A significant challenge to measuring human-automation trust is the amount of construct proliferation, models, and questionnaires with highly variable validation.  However, all agree that trust is a crucial element of technological acceptance, continued usage, fluency, and teamwork.  Herein, we synthesize a consensus model for trust in human-automation interaction by performing a meta-analysis of validated and reliable trust survey instruments.  To accomplish this objective, this work identifies the most frequently cited and best-validated human-automation and human-robot trust questionnaires, as well as the most well-established factors, which form the dimensions and antecedents of such trust.  To reduce both confusion and construct proliferation, we provide a detailed mapping of terminology between questionnaires.  Furthermore, we perform a meta-analysis of the regression models that emerged from those experiments which used multi-factorial survey instruments.  Based on this meta-analysis, we demonstrate a convergent experimentally validated model of human-automation trust.  This convergent model establishes an integrated framework for future research.  It identifies the current boundaries of trust measurement and where further investigation is necessary.  We close by discussing choosing and designing an appropriate trust survey instrument.  By comparing, mapping, and analyzing well-constructed trust survey instruments, a consensus structure of trust in human-automation interaction is identified.  Doing so discloses a more complete basis for measuring trust that is widely applicable.   Given the increasingly recognized importance of trust, especially in human-automation interaction, this work leaves us better positioned to understand and measure it.\\




\textbf{Key Words\textemdash}
Trust in Automation, Shared Mental Models, Human-Automation Interaction, Trust Measurement and Research\\


\textbf{Pr\'ecis: }We present a new approach to achieving clarity in human-automation trust by synthesizing the factors that emerge from reliable and validated survey instruments.  A further meta-analysis of these factors reveals that an experimental consensus is finally developing around the cognitive structure of trust in technology.

\end{minipage}



\clearpage


\section{Introduction and Background}
 Though still fractured into multiple sub-fields and suffering from construct proliferation, after three decades of sustained research, a complete model is finally emerging for human-robot and human-automation trust.  The past few years have proven particularly fruitful, yielding many models, measurement instruments, and meta-reviews.  However, much research in these areas is heavily siloed, such that established researchers tend to use assessments that are familiar, and new researchers seeking a trust assessment are often at a loss, trying to make sense of the various models and distinct terminology used by each sub-field and even specific research groups.  The greatest challenge they face is which survey instrument to use to measure trust and how to assess its validity.  This gap in standards has led many to create their own instruments, making cross comparisons difficult.  
 
 In response, numerous attempts have been made to bring order to the chaos. Most notably, \citeA{McKnight2001} performed a meta-review to create a trust typology for e-commerce, which later developed into their work on trust in specific technology \cite{McKnightD2011}.  \citeA{Lee2004} produced a seminal review of trust in automation, sorting and categorizing trust definitions and their keywords, and presented an integrated model of trust calibration over time.  \citeA{Gefen2003} presented an extensive mapping of trust conceptualizations in e-commerce and then proceeded to validate an integrative model experimentally.  Further systematic reviews of trust in automation models and findings were performed by \citeA{Hoff2015} and  \citeA{Hancock2011a}, the latter including a meta-analysis of results, which was recently updated \cite{Hancock2020}.  A general review of trust measurement was presented by \citeA{french2018trust}, and a small qualitative meta-analysis of trust survey instruments has also been presented \cite{brzowski2019trust}.  Beyond these critical works were also dozens of papers and dissertations that made varying contributions  \cite<e.g.,>{schaefer2013perception,Yagoda,Desai2012,helldin2014transparency,Ross2008,Hoffman2013}.
 
 While each of these works made significant contributions, it is clear that much work remains.  Many assessments still focus on fulfilling niche requirements, and there is limited cross-communication between sub-fields.  Results are often mixed, and the way forward remains unaddressed.  Progress has been most concrete in the development of survey instruments \cite{McKnightD2011,Gefen2003,schaefer2013perception}, but empirical validation is spotty, and construct proliferation has stifled progress.
 
 This paper proposes a way forward, leveraging the reliability and validation analysis of the existing survey instruments,  to demonstrate the current convergent, established state of the overall field thus far.  We take a similar approach to that performed for human-human trust by \citeA{mcevily2011measuring} and utilize a meta-analysis.  The meta-analysis will 1) identify the most cited survey instruments and examine their validity and reliability, 2) identify the best-validated instruments, 3) identify trust-related factors within survey instruments that have shown reliability and validity and provide a comprehensive mapping of terminology between these factors, 4) establish the internal model from which trust is composed and how the various factors influence one another from the meta-analysis data, 5) provide guidance on choosing and designing future assessments of trust and, finally, 6) outline what the next steps are in validating the overall model and where we go from here.  

 Many trust papers at this point review the multitude of trust definitions \cite{schaefer2013perception,Gefen2003,mcknight2001while,muir1996trust}.   As this paper aims to identify the converging validated concept beneath human-machine trust from the current survey instruments, we leave the definition as open as possible.  Trust is a \textit{state effectuated by the trustor in which the trustee has power over some subset of the trustor's goals that the trustor believes they could not accomplish with a better net outcome on their own}\label{def:trust}.  The \textit{state} may be an attitude \cite{Merritt2008,Lee2004}, belief \cite{Gefen2003}, expectation \cite{muir1996trust}, judgement \cite{Merritt2008,Lee2004}, willingness \cite{Madsen2000,SATI,mcknight2001while,Korber2018}, confidence \cite{Madsen2000,SATI}, or reliance \cite{Hancock2011}. The perceived \textit{power} of the trustee over the trustor's goals is often expressed in terms of the trustor's vulnerability, uncertainty, or risk \cite{schaefer2013perception}.  

 
 %There have been dozens, if not hundreds, of attempts to pin trust to a propositional definition (e.g., \cite{Gefen2003,law2021trust}).  These definitions only add noise to the confusion around terminology and construct proliferation.  While it is vital to differentiate trust from its antecedents or correlates, we propose a new way forward.  
Here we take a bottom-up approach to demonstrate what factors and interactions influence
trust and its correlates and how to measure them.  Only then will we attempt to extract what parts are trusted directly instead of antecedents or correlates.  When needed for clarity, we will note when factor names differ from standard definitions or are used in non-standard ways.
 
Thus, this work aims to cover the cognitive structure underlying trust.  By cognitive, we do not mean capability-based trust but trust as it is explicitly and consciously conceived of by participants and, therefore, amenable to self-report.  Thus, for example, we are not looking at the appearance of the robot per se but the human's perception of the robot, e.g., that it appears friendly or that they are comfortable around it.  Design, appearance, and other external factors certainly shape trust, but we focus only on identifiable internal cognitive factors that compose trust.  This focus on \emph{internal}, cognitive trust establishes a stronger foundation for understanding how and why external factors affect trust.

Trust is not just about a set of beliefs, an attitude, or an expectation but also about their calibration \cite{Lee2004}.  Additionally, these continuous epistemic beliefs or expectations may result in discrete decisions using thresholds or constraints \cite{Lee_94,Razin2021b}.  Calibration will be addressed tangentially in our discussion of Intention to Trust and External Learned Trust beliefs toward the end.  The mechanism by which these continuous beliefs collapse into discrete decisions is out of this work's scope, but a possible way to close that gap is also left to our final discussion.

This work begins by reviewing survey instruments that explicitly capture trust purposefully sourced from various related fields.  Classically, the most basic way to assess trust was to ask whether something or someone was trusted (binary/discrete) or how much it was to be trusted (continuous) \cite{Abbass,Cho2015,Ajenaghughrure2020,Cohen1998,Lewicki1998}.  As we will show, the one-item approach is still quite common.  Following the works of \citeA{Lee_94} and \citeA{Lewicki1998}, came the rise of two-item trust scales.  The former was concerned with self-confidence vs. trust in automation \cite{Lee_94}, while the latter dissociated trust and distrust and called for their independent measurement \cite{Lewicki1998}.  Both approaches remain popular, requiring two Likert scale questions and having been well-validated and replicated many times.  While at least \citeA{Lewicki1998} hints at trust being at least bi-dimensional, neither scale sheds much light on what composes an internal psychological model of trust as much as it defines what it is not.

Beyond one- and two-item assessments, alternatives exist for classifying more complex trust survey instruments.  Some try to create a single scale to just capture (dis)trust as opposed to those that posit multiple layers (vertical) \cite{merritt2019automation,merritt2011affective,McKnightD2011,SATI} or constructs (horizontal) \cite{muir1996trust,Madsen2000,Lee_94,Lee2015}.  Among multi-dimensional survey creators, some are focused on performance-based trust \cite<e.g.,>{Madsen2000,Wojton2020}, and others are relation-based (sometimes termed \textit{affective}) \cite<e.g.,>{Wechsung2013,Rupp2016}, with a recent trend towards more mixed approaches \cite<e.g.,>{McKnightD2011,Gefen2003,Wang2005,Park2020}, as described in \citeA{law2021trust}.  Finally, some aim to assess dispositional trust, as defined by \citeA{Hoff2015}, looking at factors such as faith in people or general fear of robots or technology.  On the other hand, others try to assess trust in a specific person, technology, or team.  Considering these methodological and conceptual differences as we review the survey instruments is worthwhile.

As trust measurement expanded from discrete to continuous and from one-item to multi-factor, a pattern is emerging.  While researchers may deliberately choose how to define and construct trust and capture specific aspects of it depending on the application, we hypothesize that they all are facets of a common internal psychological model.  This paper, therefore, focuses on capturing this latent multi-factorial, multi-dimensional \emph{internal} trust.  That being said, measuring just one or two aspects of trust or using fewer questions can be appropriate, depending on the context of the interaction.  Therefore, we conclude this paper by discussing how to choose an appropriate survey instrument or measure of trust.
 

\input{HF_Formatted_Version/Chap3.tex}
\input{HF_Formatted_Version/Chap4.tex}
\input{HF_Formatted_Version/Chap3b.tex}

 

% Most interestingly they also compared their questionnaire against using a two-item scale and explaining the pros and cons of the more complete questionnaire with multiple items and sub-constructs against the short form. A matter to which we will return at the end.


\section{Discussion}
\subsection{Picking an Appropriate Trust Instrument}

\begin{table*}[]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{l"c|c|c|c|c|c|c|c|c}
        Instrument & \# of items & \# of Factors &  Dispositional & Situational & Shared Mental Model & Learned & Intention &Reliability&Validity
        \\\thickhline
        HMI Trust Scale \cite{Muir1987}  &9&1&0&0&0&1&0&\cellcolor{yellow!40}Limited&\cellcolor{red!60}None\\\hline
        TAS \cite{jian}&12&1&0&1&0&4&0&\cellcolor{green!15}Good&\cellcolor{yellow!40}Limited\\\hline
        HCT \cite{Madsen2000}&25&5&0&1&1&2&0&\cellcolor{green!15}Good&\cellcolor{yellow!40}Limited\\\hline
        Gefen's TAM with Trust \cite{Gefen2003}&25&8&0&2&1&4&1&\cellcolor{green!15}Good&\cellcolor{blue!15}Decent\\\hline
        SATI \cite{SATI}&17&?&0&1&1&2&0&\cellcolor{red!60}None&\cellcolor{yellow!40}Limited\\\hline
        UTUAT with Trust \cite{Heerink2009,Heerink2011}&41&12&0&3&1&3&1&\cellcolor{green!15}Good&\cellcolor{yellow!40}Limited\\\hline
        Trust in Specific Technology \cite{McKnightD2011}&39&10&1&1&0&4&1&\cellcolor{green!15}Good&\cellcolor{green!15}Good\\\hline
        \begin{tabular}{@{}l@{}}Merritt's collected Trust-related Scales\\\cite{Merritt2008,Merritt2011,merritt2015measuring,merritt2019automation}\end{tabular}&28&4&1&1&0&2&0&\cellcolor{green!15}Good&\cellcolor{blue!15}Decent\\\hline
        Subjective Fluency Metric Scale \cite{Hoffman2013}&30&7&0&1&1&3&0&\cellcolor{green!15}Good&\cellcolor{red!60}None\\\hline
        TPS-HRI \cite{schaefer2013perception}&40 (14)&4&0&1$^*$&1$^*$&2$^*$&0&\cellcolor{green!15}Good&\cellcolor{yellow!40}Limited\\\hline
        CTI \cite{Chien2014}&19&4&1&0&1&4&0&\cellcolor{green!15}Good&\cellcolor{blue!15}Decent\\\hline
        German TiA \cite{Korber2018}&17 &5&1&1&1&3&0&\cellcolor{green!15}Good&\cellcolor{green!15}Good\\\hline
        \textbf{Maximum}&&&1&3&1&4&1&&
    \end{tabular}}
    \caption{Coverage of the core identified internal trust factors by the Top 12.  \# of items are those specific to trust and its factors. \# of Factors is the number the authors find or confirm, whether they match ours or not.  The next five columns indicate whether they have some number of factors in those layers, with the maximum number per column given in the final row.  Quality of empirical reliability and validity is rated in order of None, Limited, Decent, Good. }
    \label{tab:top12sum}
\end{table*}


When deciding how to pick an appropriate trust survey for an experiment or deciding whether to design your own, consider the following in Table \ref{tab:top12sum} and the following questions:

\begin{itemize}
    \item Are you interested in trust in general or also the factors that lead to trust?
    \item Are you administering the survey before, during, and/or after the experiment?  How interruptible/chunkable is the task?
    \item How much previous experience/exposure does the person have to the technology?
    \item Are you interested in trusting attitudes in general towards technology or a specific type of technology (dispositional) or interested in trust during an interaction or a specific relationship with one piece of technology?
    \item What is the expected effect size that trust will play?/What is the expected effect size on trust? 
\end{itemize}

Single-factor trust surveys \cite<e.g.,>{Muir1987,jian,Merritt2008} are appropriate for those interested just in trust as a single stand-alone construct as well as are generally shorter and faster to administer, making them well-suited for quick in-task trust probing.  If the effect size is expected to be large, fewer questions are needed \cite{Korber2018}, but more if it is expected to be small.  At least three questions are recommended if possible, though one along the lines of Merritt's Trust Scale \cite{Merritt2011} would be even better.  Furthermore, if trust in technology is not the opposite of distrust but is somewhat inverse to self-confidence, then separate questions on distrust, uncertainty, and self-confidence should all be measured in order to get a convergent estimate of internal trust.

Multi-factor trust surveys are appropriate both before and after a trust interaction, as opposed to during a task.  They tend to take longer, and the temporal dynamics of trust and the interaction should be accounted for when such instruments are chosen.  If one is interested in dispositional trust, then a number of well-validated Dispositional Trust-specific instruments are available.  The same goes for General Trust in a specific technology, e.g., trust in autonomous vehicles.  Even when one is only interested in general, dispositional trust, it is maybe worthwhile to probe affective, structural, and Capability-based factors, as these are all recursively fed back to one's General Trust attitudes. 

Differences arise if one is gauging trust in new users, experts, or trust over time.  As mentioned above, the upper-level factors such as \textbf{Faith in Technology}, \textbf{Familiarity}, and \textbf{Situational Normality} will matter more earlier on, especially at the stage of initial acceptance \cite{Merritt2008,Ekman2018}.  It may not be worth measuring these if the interval is greater than six weeks \cite{sollner2016longitudinal}, though over that period one could watch how these effects decrease as the effect of factors within Learned Trust increase.  Either way, multi-factorial instruments are recommended for such longitudinal studies \cite{Korber2018}.

A common question may arise as to whether Affective and Structural Trust are both worth measuring, as many may not see how they are relevant to their specific robot pick-and-place task in the lab, for example.  That the robot could be nefarious and have competitive goals (Affective Trust) or not follow rules (Structural Trust) is often not even considered).  Asking about deception can, in fact, elicit suspicion in both of these categories, an issue to which we will return.  Affective Trust is likely always at play, even if it is only in the narrowest calculative-based sense, i.e., \citeA{Gefen2003}.  The best way to conceive of Structural Trust is to go back to \citeA{luhmann}, the originator of the whole science of trust.  Structural, or what he called societal trust, is the abstraction of affective or personal trust.  Affective Trust is founded in the dyadic trust relationship and is rooted in personal knowledge, and is realized through individual attachment, bond, loyalty, and love.  Structural Trust universalized this in a world of strangers.  Instead of personal, special treatment customized to individuals, Structural Trust means to be fair through rules equally applied to all.  The direction of technological development has meant that robots and automation are usually seen as tools that have no knowledge of the individual, so they are usually bound by Structural Trust.  Newer systems that learn from individuals and are becoming more adaptive and personalized may be expected to be governed more effectively.  However, Structural Trust is often ignored in lab settings, whereas it has clear effects in the real world with regard to branding, attitudes toward tech companies, government and industry regulations, and insurance liability.  Affective Trust can be difficult to separate from structural at times since we buy, accept, and use technologies that we believe will help fulfill our goals, but we choose goals with enabling constraints that conform to societal norms.  Such questions are at the bleeding edge, at the border of science fiction, which has often dealt with villains who override ethical constraints in AI agents (violating Structural Trust) to align these AIs with their own personal goals (establishing Affective Trust).  In general, AIs and robots are assumed to be more fair and ethical than humans \cite{starke2021fairness}, demonstrating potential bias in Structural Trust.  Though others have shown that this is domain-specific; for instance, in a driving simulator, autonomous cars were presumed to follow the law as much as human drivers \cite{razin2019}.

As noted above, even asking about or mentioning trust can plant seeds of suspicion.  This is the paradox of trust signaling.  Explicit displays of trust can build the trustee's confidence in the trusting relationship, whereas explicit mentions of trustworthiness can poison the well and lead to uncertainty and potential distrust.  This is a potential bias of all explicit trust survey instruments.

\subsection{Choosing and using an Appropriate Instrument}
What we have been working toward throughout this work is an attempt to characterize the current state-of-the-art so that this work can ultimately help guide practitioners in choosing an appropriate instrument and even designing future ones.    An excellent start to this discussion has already been provided by \citeA{Korber2018} and we do not intend to reproduce it here in full.    Instead, we aim to provide a simple guide for quick and easy reference.

\begin{enumerate}
    \item Single Factor: If you are interested in capturing trust as a single factor during an interaction, whether its antecedents simply do not matter to your application OR if you need to capture trust repeatedly over an interaction and therefore need to minimize interruption time, then a short survey is appropriate. 
    
    \begin{enumerate}
    \item Choosing a Pre-Existing Survey: Given the validity issues of Jian and Muir, Merrit's trust scale is a reasonable choice, though it is entirely focused on \textbf{General} and \textbf{Capability-based} trust and not suitable for more socially situated or effective applications.    Their short \emph{Liking} scale can be used to capture \textbf{Emotional Response} and their various short \textbf{\emph{Dispositional}} scales can be used for \textbf{Faith in Technology}. However, even Merritt's scales need further reliability and validation testing, and it would be useful for future works to carry these out and properly report them.
    
    \item For Designing and Assessing New Surveys: Any short one-factor survey must ensure to include at least three items
    \begin{enumerate}
        \item Construction: Each item must use different enough wording
        \item Reliability: Minimally Cronbach's $\alpha$ and McDonald's $\omega$ and the inter-item correlation, if not other methods such as test-retest
        \item Validity: Factor analysis is performed, and the items' loadings are sufficient on that single factor, generally $> 0.45$, though at least 4 loadings over 0.6 is preferred (if 3 items all, 3 should be over 0.6)
    \end{enumerate}
     
      Other items to be reported: 
      \begin{enumerate}
          \item Correlation matrix of items
          \item A collinearity check using the variance inflation factor ($< 5$)
          \item Kaiser-Meyer-Olkin measure of sampling adequacy ($> 0.6$)
          \item Loadings and communalities for each item on the single factor
          \item The explained variance
      \end{enumerate}
        A more complete list of recommendations, including sample size, treatment of missing data, variability, linearity, normality, and other methodological considerations, can be found in \citeA{watkins2018}.
    \end{enumerate}
    \item Multiple Factors: Sometimes, a more in-depth understanding of the {\fontfamily{qzc}\selectfont internal}, psychological factors of trust, and its antecedents are desired.  This sort of survey takes more time and is best suited for prior- and post-interaction administration as they tend to range from 15-45 questions.    This survey type is recommended for research questions such as how an interface design affects \textbf{Situational Normality} or an \textbf{Emotional Response} and therefore trust, how \textbf{Faith in Technology} influences final \textbf{Intent to Use}, or how programming law-following into an autonomous car influences \textbf{Structural} and \textbf{ General Trust}.
    
    \begin{enumerate}
    \item Choosing a Pre-Existing Surveys: At the time of writing, McKnight's survey is the strongest and most complete of the Top 12 in that it covers the most factors with the best validity.  A shorter and more limited choice would be K\"{o}rber's survey (including their additional survey for \textbf{Faith in Technology}). Sometimes though shorter can be preferable.  K\"{o}rber's is mainly limited by its lack of \textbf{Structural Trust}.  From our general survey of the 62 instruments, a few of the most recent instruments are very promising and provide broad coverage of the identified factors, though lacking verification of reliability through repeated use \cite{Park2020,Chi2021}.
     \item For Designing and Assessing New Surveys: Almost all of the same advice from the one-factor survey development applies here, except the number of factors retained must be justified and multicollinearity is instead indicated by cross-loadings $>0.3$.    In addition, Bartlett's Test for Sphericity should be significant and reported, as should the rotation used, item uniqueness, and the explained variance of each factor. 
     
     If a confirmatory factor analysis or structural equation model is performed, then several additional elements should be carried out and reported \cite{jackson2009reporting}:
     \begin{enumerate}
         \item Specification of multiple prior models to test
         \item Theoretical justification of those various models
         \item Choice of estimation procedure.    If the method used assumes multivariate normality, such as Maximum Likelihood (ML), then normality reporting is even more critical.
         \item Reporting of fit tests for each model: $\chi^2$ and root mean squared error of approximation (RMSEA) are most important, but additional indices such as the Comparative Fix Index (CFI), Tucker-Lewis Index (TLI), and Standardized Root Mean Residual (SRMR) are also highly recommended.    CFI and TLI should minimally be $>0.9$ though more recent recommendations range from 0.95-0.97.    However, strict cutoffs are also advised against.    RMSEA and SRMR should at least be below 0.1, though under 0.06 is preferred by many.
         \item Other elements to report: Parameter estimates, the variance of exogenous variables with standard errors, explained variance by endogenous variables, structure coefficients
         \item Finally, the preferred model and justification of it both on the grounds of its quality of fit as well as theoretically.
     \end{enumerate}
     A full checklist for CFA reporting can be found in the Appendix of \citeA{jackson2009reporting}.    If an SEM is reported, researchers must understand to what extent an SEM supports claims of causality and what assumptions such an analysis entails \cite{bollen2013eight}.
    \end{enumerate}
\end{enumerate}


\subsection{Outstanding Issues}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{Expanded_Trust_Layers5.003.png}
%     \caption{The validated trust model with likely external dispositional and learned factors}
%     \label{fig:expanded_trust}
% \end{figure*}

\subsubsection{External Trust Factors}
While the extant surveys converge on the trust model presented above, there is good reason to believe from numerous experiments that other factors may have some impact.  While the literature has explored dozens of potential factors \cite{Hancock2011a,Hoff2015}, only a few have thus far withstood some rigor and repeatability, as shown in Fig. \ref{fig:expanded_trust}.  We will not attempt a complete review of these additional factors here.  However, we will outline their potential roles and give an overview of how they interact with the emergent validated trust model.

Continuing to follow the three-layer schema of \citeA{Hoff2015}, these additional factors may be divided into three main categories: expanded dispositional trust factors or antecedents, external situational trust factors, and external learned trust factors.  We introduce the language of externality here to indicate that these factors are not part of the internal trust model and are directly contingent on the environment and embodiment of the agents in that environment.  This is clearest for robot form, which is a physical property of the actual technology.  Monitoring is also external in the sense that it is the direct sensory link between the human, the technology, and the environment.  Perceived risk is likewise an emergent property of this three-way interaction.  Cognitive load is more indirect but serves as a constraint on cognitive processing, and while it affects trust is external to it.

On the other hand, external dispositional antecedents of trust are not directly trust-related and perhaps are best understood as the foundations of trust that arise out of those factors outside of the individual's control, such as their age, culture, personality, which in turn give rise to their group membership, education.  These shape trust-related factors directly more directly, such as the willingness to take risks and one's faith in people, both in individual strangers as well as institutions.  These expanded dispositional factors are discussed in detail in \citeA{Hoff2015}, with the role of Personality getting a more complete and up-to-date treatment in \citeA{alarcon2021role}.  Short, updated treatments of the remaining areas are presented below.

\subsubsection{Gender}
Multiple studies to date have not found gender to have a significant effect on trust in automation or robots \cite{schaefer2013perception} while others have shown mixed effects.  For instance, in one study where men had more experience with computers, they unsurprisingly found them easier to use but not necessarily trust \cite{Heerink2011}.  On the other hand, another study found that women trusted a robot more, perhaps because men had a higher need for cognition \cite{Robert2009}, thus indicating a mediating effect on the relationship between confidence in the robot and self-confidence.  Adding further complexity, \citeA{kuchenbrandt2014keep} found that gender may have a mediating role in setting group expectations.  One study indicated that a weak interaction effect occurs for men interacting with a gendered robot but not women.  In all cases, when gender has been found to have any sort of effect, it has been weak.  It is also unclear whether gender has an immediate effect on trust or just mediates training, group membership, and cognition.


\subsubsection{Age}
Age has shown a clearer effect than gender on trust.  Aging, for instance, decreases willingness to take risks \cite{dohmen2018identifying}, which in turn decreases willingness to trust \cite{Desai2012}.  On the other hand, age lends familiarity \cite{sundar2016hollywood}, and more experience trusting in general.  This, in turn, translates into better trust calibration and an understanding of what factors in the interaction actually matter \cite{Hoff2015}.  


\subsubsection{Culture}
The role of culture in trust of technologies has also seen limited exploration thus far.  Culture, when it is studied, has been taken in the most narrow, reductionist sense - either as nationality \cite{Chien2014,yerdon2017investigating} or along the axes of horizontal vs. vertical achievement and individualist vs. collectivist \cite{huang2017users}.  Similar to the findings on gender, culture shapes perceptions of group membership \cite{wagner2015robots}, faith in others \cite{yamagishi2001}, and exposure/education, and thus familiarity.  The same cultural dimensions on which men score higher are those that lower trust, indicating a complex interaction of culture and gender \cite{zhang2011effect,zeffane2020gender}.   Cultures also shape general attitudes toward technologies \cite{Chien2014}, but through access as well as portrayal in the media, as illustrated by the Hollywood Robot Syndrome \cite{sundar2016hollywood}.  When culture has been shown to affect General Trust, it has been weak and further weakens over time, like other dispositional factors \cite{Chien2014}.

\subsubsection{Faith in People}
Like Faith in Technology, Faith in People seems to have a weak effect on Intention to Use, and its effect on General Trust lessens with the usage of the specific technology \cite{uslaner2015measuring}.  However, many of the experimental designs have essentially excluded the more salient aspects of Faith in People by not looking at it through the lens of dispositional institutional trust, which serves as the foundation of specific Structural Trust.  (This being a primary focus of human-human trust surveys deriving from \citeA{Rotter_67}) Faith in people may also set the \textit{a priori} standards expected of Capability-based Trust, shared mental models, and familiarity, as it creates a baseline against which to compare technology and confidence in the designers to be ethical, follow known standards, and design for normative interactions \cite{razin2019}.

\subsubsection{Group Membership}
At the root of trust from the cognitive perspective lies group membership \cite{williams2001whom}.  The famous trust `sniff test' found that intranasal oxytocin increased in-group trust but decreased out-group trust \cite{van2012sniff}, indicating that propensity to trust is deeply entwined with the moral dimension of In-Group Loyalty \cite{haidt2007new}.  Furthermore, when this experiment was tried in an HRI context, they found that oxytocin specifically increased trust in the `uncanny valley' \cite{de2017little}, which fits well with the finding in human-human trust that the Big 5 personality trait of Agreeableness has a strong effect on trusting strangers \cite{freitag2016personality}.  Furthermore, group membership serves to form prior expectations of Familiarity, Situation Normality, and Emotional Response that proceed to form the shared mental model necessary to capture the relevant qualities of the trustee \cite{tanis2005social}.  Such stereotyping has been exploited in HRI trust experiments by \citeA{wagner2015robots}, who uses it to literally extract reward, goal, and capability beliefs in forming prior trust expectations.  Furthermore, expectations associated with in-group hierarchies regarding age and gender seem to play an important role in trust biasing \cite{pak2012decision} and have a direct impact on robot form.

\subsubsection{(Robot) Form}
Trust's interaction with robot form is a popular area of study, with much work focused on anthropomorphism.  However, results concerning the effect of form on trust have had some mixed results \cite{hauslschmid2017supportingtrust,waytz2014mind,de2017little}.  It seems that at least in the cultures thus far studied, there is a general initial overtrust of automation above and beyond human-human overtrust.  This overtrust stems from a belief that machines are less biased, fairer, and thus more just \cite{starke2021fairness}.  When this belief is uprooted, trust repair is much harder.  In part, this is explained by the `higher they are, the farther they fall' in terms of miscalibration.  Making the robot more human-like can increase trust resilience under uncertainty and better calibrates initial over-trust \cite{de2017little}.   However, other results have shown what seems to be an `uncanny valley' of trust, where seeming too human also lowers perceived trustworthiness \cite{hauslschmid2017supportingtrust}.   These results are further complexified by the multitude of interactions mentioned above between age, culture, gender, personality, group membership, and educational background that shape how forms are perceived and interacted with.  
In terms of the emergent validated mode, anthropomorphism is thought to invoke Situation Normality and Emotional Response that allow for the creation of Shared Mental Models by helping people determine the generalized model by which to start forming expectations. 

Anthropomorphism is not the end of the story, though.  Form shapes the user interface by which the human can assess and monitor the trustee as needed.  We simply refer the curious reader to the vast literature which already exists on interface design for trust \cite{wang2005trust,wang2005overview,Skarlatidou2013,Wojton2020}, and legibility \cite{Dragan2015}.

\subsubsection{External Learned Trust Factors}
The three primary factors that seem to affect learned trust are \textit{cognitive load, perceived risk, and monitoring} which supports the core of the feedback loop.  The ability to monitor the perception of data \cite{morra2019building}, behavior \cite{xu2015optimo},  or via explainable AI \cite{adams2005human} are crucial in supporting the full range of learning within learned trust.   Monitoring of the system both adds to cognitive load and can affect learned trust in all sorts of ways, updating estimates of goals, capabilities, loyalties, and rule-following as well.  One of the best-studied external factors that influences learned trust is cognitive load.  Almost always assessed by the NASA TLX \cite{hart1988development}, cognitive load has been shown to increase intent to use \cite{Desai2012}.  It is important to note that workload does not significantly change due to the system's capability/accuracy \cite{Dadashi2013, deVisser, Wang2018}, but by the quality of the information provided as feedback during monitoring \cite{Dadashi2013,helldin2014transparency}.  As tasks get harder to monitor, frustration and effort increase \cite{helldin2014transparency}, self-confidence decreases, and the need to trust in the technology is essentially coerced into complacency \cite{IdramaniL.SinghRobertMalloy1993}.     In some ways, perceived risk has a similar effect on intent to use as cognitive load if the risk makes the task more difficult \cite{Yagoda}.  However, as the potential goals of the trust themselves have more extreme payoffs/costs associated with them, the more need there is for some combination of monitoring, self-confidence, and trust in the system to compensate.  One critical area that needs more attention is the interaction between willingness to risk, perceived risk, and types of risk.

The external antecedents and factors that influence trust are by no means exhaustive.  However, we feel that the ones described above present the next frontier for validation and testing beyond the `internal' emergent validated trust model.  The model we have presented, therefore, is meant to serve as a foundation for integrating past research into the converging language of trust in technology and automation while directing future research to attend to the myriad interplays of external factors with trust.


\subsubsection{Missing Internal Trust Factors}
While we have attempted to create broad categories to describe the results of the emergent validated trust model, there are nuances and differences within factors that we have not fully explored, which are left for future work.  

As we have already discussed, there does seem to be a general and detectable distinction between expectations and confidence in those expectations; this is best seen by the dichotomy in many \textit{Capability-based Trust }measures, where competence/performance/capability are often found to be distinct from reliability/predictability.  While less clear, a similar distinction may be evident within \textit{Structural Trust}, between the sub-factors of \textit{ethical} and \textit{sincere} \cite{malle2021multidimensional}.    There are also some important distinctions expected within \textit{Structural Trust} as to whether it is internally or externally motivated \cite{dunning2014trust}. 

It is still somewhat unclear whether the distinction between \textit{Propensity to Trust} and \textit{Trusting Stance} is fully warranted. 

The \textit{Situational trust} factors have also not been tested together for validity and reliability, and their correlational, and potentially causal, structure is still very unclear.  It may be that \textbf{Familiarity} and \textbf{Situation Normality} are sub-sets of one another.  It is interesting to note how much they co-vary with \textbf{Emotional Response}, speaking to our intuitive understanding that how much comfort or liking we 
have is dependent on how familiar or normal an agent or interaction appears to be.

\textbf{Emotional Response} may be too broad as a single category, but it at least seems that the factors of emotional attachment, liking, enjoyment, warmth, and engagement may be distinct but overlapping in the roles they play in trust development.

Perceived Usefulness has also proven to be a complex category, but mostly due to its textual ambiguity.  Many take it descriptively\textemdash how useful something is\textemdash  in the vein of \textbf{Capability-based Trust}.  However, some authors have used the term prescriptively, as in how useful something has the potential to be.  One can see from the extended general model that Perceived Risk is accounted for but not potential gains or reward.  The need to account for value-added benefit has been specifically called out in UTUAT critiques \cite{Shachak2019} but has received little to no attention in the robotics or automation world in this explicit survey domain.  Alternative game theory-based approaches using interdependence theory have begun to shed light on the role of relative expected gains \cite{Razin2021b}.

\subsubsection{Distrust}
This model does not explicitly deal with distrust.  The debate over the nature of distrust and its relationship with trust is less than clear.  Ever since \citeA{Lewicki1998} posited that trust and distrust were not opposites and should not be measured on the same scale, their relationship with one another has become blurred. 

\citeA{Lee2004} essentially divorced distrust from the trust calibration conversation, as it was neither over nor under trust.  The opposite of trust can better be categorized as a combination of distrust and uncertainty.  But distrust in the trust literature is often confined very narrowly in the sense of negative Affective Trust (malevolence) or suspicion of such (the negative valence confidence sub-factor of Affective Trust).   \citeA{mcknight2001while}, and \citeA{dimoka2010does} have given strong descriptions of models that test distrust from a triadic perspective (structural, capability, and affective).  An interesting inversion was attempted by \citeA{mcknight2006distrust}, where they tested a positively framed trust survey against an equivalent negatively framed one.  While \textit{insufficient reliability} and \textit{insufficient validity} measures were published on the `Negative' survey, they demonstrated that the negatively valenced survey actually may be more sensitive and have higher explained variance.  This is less a question of the underlying model as much as the framing of said model in its measurement.  It also speaks to the imbalance in the trust signaling problem.



\subsection{Comparison with Human-Human Trust Measurement}
While early works on human-human trust were entirely focused on dispositional trust measures \cite{Rotter_67, gillespie2003measuring, mcevily2011measuring}, most focused on political/societal trust more generally.  Measures for inter-personal trust started developing in many professional contexts, from organizational trust between employees and managers, between firms, and within business networks \cite{Mayer1995,bhattacherjee2002individual,schoorman2007integrative,johnson2005cognitive}.  A second major approach developed, which studied trust in friendships and intimate relationships \cite{gottman2011science,bukowski1994measuring}.  Building off these approaches, other veins developed in the healthcare community between patients and doctors \cite{thom2004measuring,anderson1990development}, as well as trust in media \cite{matthes2008content}, and trust in strangers \cite{ermisch2009measuring}.  

Those studying HAI, HCI, and HRI trust have often turned to human-human studies both directly and indirectly.  Fluency scales \cite{Hoffman2013} are primarily built off the Working Alliance Inventory originally developed for patient-therapist trust \cite{horvath1989development}.  Other HRI studies simply lifted scales from a doctor-patient trust (\citeA<e.g.,>{Mann2015} using \citeA{anderson1990development}).  Many still cite or even test with \citeA{Rotter_67}, and the influence of \citeA{Mayer1995}, who primarily worked in organizational trust, is hard to overstate.

All branches of human trust measurement are plagued by similar issues as HAI and HRI trust.  A similar review to our own here was performed on human-human trust scales \cite{mcevily2011measuring}.  Interestingly, they identified approximately the same number of trust scales in human-human trust, and a very similar pattern, where 60\% had created their own ad hoc measures and 40\% re-used a previously validated instrument.  They similarly discussed the reliability and construct validity patterns over time.  During their review, they found even fewer of the human-human scales had reported empirical validity measures.  They found only 22\% had considered multi-factorial trust as opposed to single uni-dimensional items.

Of the dimensions reported, they fall closely in line with our own, as can be seen in Table \ref{tab:hu-hum}. 

% \begin{table}[]
%     \centering
%     \begin{tabular}{l|c}
%       Factor&\# of times operationalized\\\thickhline
%        Capability-based Trust  & 58  \\\hline
%        Affective Trust     &  40 \\\hline
%        Structural Trust & 22 \\\hline
%        General Trust & 9 \\\hline
%        Familiarity & 2\\\hline
%        Emotional Response & 3\\\hline
%        Faith in People & 4\\\hline
%        Willingness to Risk & 4 \\\hline
%        \begin{tabular}{@{}l@{}}\textbf{Openness, Availability,}\\ \textbf{Receptivity, Forbearance}\end{tabular} & 13\\ 
%     \end{tabular}
%     \caption{Operationalized factors of human-human trust (McEvily, 2011)}
%     \label{tab:hu-hum}
% \end{table}

Beyond a general similarity in factors, the similarity in structures has become more than apparent.   There was only one major grouping that our model did not capture, which was how available, open, and receptive the potential trustees and trustors are.  Likely, this grouping is closely tied to the importance of Agreeableness in trust from the Personality research \cite{freitag2016personality} as well as our Emotional Response factor.  However, it seems distinct enough within human-human trust to merit its own factor and a potential focus of future work.

Human-human trust is seeing similar re-analysis as to the importance of the shared mental model and mutual modeling in teams.  It introduced the great divide between affective- and Capability-based Trust (the latter they often call cognitive), as well as the trust-trustworthiness calibration problem.  Similar attempts at consensus and convergence are apart across both fields.  While many have considered the deep differences between human-human and human-robot trust, the work of measuring the factors of trust is extremely similar, to the point that we plan more general works to help bridge these gaps in the future.



\subsection{Limitations}
\label{Limits}
Gefen's work was a Trust extension for TAM, which has seen its own extensive testing and validation.  A complete review of the history, development, meta-analyses, strengths, and weaknesses of TAM can be found in \citeA{Chuttur}.  That work brings up many critical limitations  - that TAM does not account for relationship dynamics, that the connection between intention and behavior is not direct, that dispositional and Emotional Response factors are lacking, and that voluntary vs. mandatory use leads to differential strengths of Capability-based and Structural Trust.

Similar critiques of UTUAT exist to those of TAM \cite{Ammenwerth2019, Shachak2019}.  Both can explain actual use only so well (explained variances for intended use range from 0.3-0.7), and their power of explainability seems to have plateaued \cite{Ammenwerth2019}.  Neither is meant for measuring acceptance of mandated technologies, only voluntary ones.  Both have seen a good deal of instability in the language of their survey instruments.  Both are meant to be used as one-off assessments, usually post-task, and do not measure temporal changes \cite{Chuttur, Shachak2019}.  Several questions surround their assumptions.  Furthermore, Perceived Usefulness has proven more consistent in predicting Intended Use than Perceived Ease of Use or Social Influence, leading some to question their validity \cite{Ammenwerth2019}. 

Many of these same concerns have been voiced concerning survey instruments \cite{Korber2018}.  Wide-spread issues with validity, unstable language, cherry-picking of items to form new factors, and lack of attention to measuring temporality are not confined to TAM and UTUAT.  We have endeavored to show how Intent to Use is influenced by many factors beyond trust, and do not see this as a fundamental critique of the emergent validated model as such, whose purpose is to show the convergent model of trust, as opposed to these models which attempt to fully capture acceptance and usage.  


\subsection{Trust as attitude, beliefs, intentions, decision, and behaviors}
This work has solely focused on capturing trust attitudes, beliefs, and intentions.  Many have pointed out the divide between trust beliefs and actual decisions and behaviors.  This is partly addressed by the external learned factors that affect trust.  Trust alone has proven insufficient to predict use \cite{dunning2014trust,Gefen2003,Desai2012}, as risk, need, coercion, and stress all interact with trust in complex ways.  Elsewhere, we have presented an alternative approach to trust decision-making, drawing from game theory, which is not covered herein.   There we present successful models that both estimate and predict trust in discrete situations using payoff calculations that we have argued reflect a trust index, as well as commitment, coercion, and coordination metrics.  In those works, we have argued that these indices and metrics interact with calculable thresholds for trust decision-making.  While much work remains to be done, clear lines can be drawn between the factors of trust we have outlined above and these measures.  Capability-based Trust allows for setting the payoffs, and the shared mental model and familiarity allow us to form probabilistic expectations of fulfillment.  Affective and Structural Trust both shape perceived commitment to goals as well as coercion through norms, requirements, and sanctions as well as incentives.  Dispositional trust allows us to form prior beliefs from which to refine our estimates, and willingness to risk supplies a threshold.  Cognitive workload can increase our commitment by decreasing our alternative options.  More work is needed to capture how these interactions play out and especially in how to measure perceived risk and reward, but progress is being made in explicating trust beliefs and trusting actions.  A major step forward would be a reliable, valid, multi-factorial trust survey instrument that can be used to measure trust, both broadly as well as temporally, as it evolves over time (forthcoming).




\section{Future Work and Conclusion}
This work has described the current state of measuring trust between humans and various technology categories.  It has assessed the types of metrics used, especially in terms of reliability, validation, and factor terminology.  The latter allowed us to identify ten major trust factors, and the accompanying meta-analysis helped us discern the general structure.  We also discussed how trust should be measured going forward, what areas of the model need further examination, and potential avenues to explore - from external trust factors and internal nuances to how trust beliefs become decisions.  By evaluating and integrating the results of these previous survey instruments and constructs, the model presented in this work provides a robust foundation for the future of human-machine trust research.


% use section* for acknowledgment
\section*{Acknowledgments}
We want to thank Matthew Brzowski and Dr. Dan Nathan-Roberts of San Jos State University for providing us with their raw data set.

\section*{Key Points}
\begin{itemize}
 \item  Both modeling and measuring trust have proven difficult due to construct proliferation, terminological mismatches, and the tendency of researchers to create their own survey instruments from scratch
 \item While many trust questionnaires lack reliability and validity, we can use the ones that are reliable and valid to extract reliable and valid factors.  Furthermore, the reliable and validated factors can be mapped across surveys based on the similarity of items within the factor, clearing away the terminological confusion
 \item Once factors are mapped, a consensus can be seen around the structure of trust, and how the factors affect each other, resolving much of the construct proliferation
 \item The most popular and impactful trust questionnaires are assessed for reliability and validity and compared against the commonly-shared factors and consensus model.  In general, this emergent model provides guidance for future development and testing of the model, as well as choice and creation of future survey instruments
\end{itemize}


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{30}
\bibliographystyle{apacite}
\bibliography{biblio2}

\section*{}
Yosef S. Razin is a Ph.D. candidate in Robotics at the Georgia  Institute of Technology's School of  Aerospace  Engineering.  He earned a BSE in Mechanical and Aerospace Engineering from Princeton University in 2011.  The focus of his work is on human-robot interaction and human-automation trust.\\

Karen  M.  Feigh is a professor at the Georgia Institute of Technology's School of Aerospace Engineering.  She earned a Ph.D. in Industrial and Systems Engineering from the Georgia  Institute of Technology.  She leads the Cognitive Engineering Center, focusing on decision support and incorporating computational cognitive modeling in engineering design.

\newpage
\singlespacing
\onecolumn
\appendix
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\vspace{3in}
\begin{center}
    {\Huge Appendix A}
\end{center}



\begin{landscape}

\begin{longtable}[c]{l"c|c|c|c|c}
    %\centering
    \caption{Mapping Terms between Identified Dispositional and Situational Trust Factors of Significant Reliability and Validity.  Bolded authors are among those in the Top 12 survey instruments.  Entries in red were found to be reliable and valid but span more than one factor.}\\
     \label{tab:termMap1}
% \endhead
    %\begin{tabular}
   \textbf{Author} &	\textbf{Faith in Technology} &	\textbf{Emotional Response} &	\textbf{Shared Mental Model} &	\textbf{Situation Normality} &	\textbf{Familiarity} \\\thickhline
CPRS: \citeA{IdramaniL.SinghRobertMalloy1993} &	Propensity to Trust	&&&&			\\\hline
\textbf{\citeA{Madsen2000}}	&&	Personal Attachment & \begin{tabular}{@{}c@{}}	Perceived Understandability,\\Technical Competence\end{tabular}&	&\\\hline
\textbf{\citeA{Gefen2003}}	&&&	Perceived Ease of Use&	Situation Normality&	Knowledge-based Familiarity	\\\hline
\textbf{\citeA{Korber2018}}	&Propensity to Trust&&		\begin{tabular}{@{}c@{}}Understanding/Predictability,\\Intention of Developers\end{tabular} &	&	Familiarity	\\\hline
\citeA{Scopelliti2005}&	\begin{tabular}{@{}c@{}}Attitude Towards\\ New Technologies\end{tabular}&	\begin{tabular}{@{}c@{}}Emotional Response\\ to Robots\end{tabular}&	\begin{tabular}{@{}c@{}}\color{red}Human-Robot\\ \color{red}Interaction\end{tabular} &\begin{tabular}{@{}c@{}}\color{red}Human-Robot\\ \color{red}Interaction\end{tabular}&	Capabilities of Robots	\\\hline
\begin{tabular}{@{}l@{}}\citeA{Wang2005}\\\citeA{Komiak2006}\end{tabular}	&&Emotional Trust&		Perceived Ease of Use	&&Familiarity\\\hline
\textbf{Heerink (2009, 2011)}& & \begin{tabular}{@{}c@{}}Anxiety,\\ Perceived Enjoyment,\\Perceived Sociability\end{tabular}&	Perceived Ease of Use&Social Presence&\begin{tabular}{@{}c@{}}Facilitating Conditions,\\Social Influence\end{tabular}\\\hline	
\textbf{\citeA{McKnightD2011}}&	\begin{tabular}{@{}c@{}}Faith in General Technology,\\Trusting Stance\end{tabular}&&	&	Situation Normality&\\\hline
\textbf{Merritt et al. (2008, 2011a)}	&Propensity to Trust &	Liking Scale&&&\\\hline
\citeA{Sollner2012}	&&	&	Process (formative)	&&	\\\hline
\textbf{\citeA{schaefer2013perception}}&&\begin{tabular}{@{}c@{}}\color{red}Robot Behaviors +\\\color{red}Communication\end{tabular}	&\begin{tabular}{@{}c@{}}\color{red}Robot Behaviors +\\\color{red}Communication\end{tabular}&&			\\\hline		
\citeA{Sollner2013}	&&&&&	\\\hline
Wechsung et al. (2013)&&		\begin{tabular}{@{}c@{}}Likeability,\\Entertainment\end{tabular}&&		Naturalness		\\\hline
Ullman et al. (2014, 2021)&&		Intelligence, Other&	&&			\\\hline		
\citeA{cherif2014impact} 	&&	Social Presence	&&&		\\\hline
\textbf{\citeA{Chien2014}}	&Purpose Influence&&		Process Transparency	&&			\\\hline
\citeA{Lee2015}	&&	Perceived Warmth&&&	\\\hline					
\citeA{Tussyadiah2020}&	\begin{tabular}{@{}c@{}}Faith in General Technology,\\Trusting Stance, NARS\end{tabular}	&&&&					\\\hline
\textbf{\citeA{Hoffman2013}}	&&	\begin{tabular}{@{}c@{}}Positive Teammate\\ Traits\end{tabular}&\begin{tabular}{@{}c@{}}Working Alliance-\\ Bond Subscale\end{tabular}&&	\\\hline
\citeA{Rupp2016}&	&&		Wearable Technology Trust	&&\\\hline
\citeA{Hegner2019}&	Personal Innovativeness	&&	Perceived Ease of Use&&\\\hline
\citeA{Freude2019} &	Disposition	&&&		\begin{tabular}{@{}c@{}}Institution-Based\\ Situation Normality\end{tabular}	&Knowledge-based Familiarity\\\hline
\citeA{Park2020}&&		Engagement&	Process&	Situational Normality	&\\\hline
\citeA{Wojton2020}	&&&		Understanding		&&	\\\hline
\citeA{Chi2021}&	Trusting Stance&	\begin{tabular}{@{}c@{}}Technological\\ Attachment\end{tabular}&&&			\begin{tabular}{@{}c@{}}Familiarity,\\ Social Influence\end{tabular}
  %  \end{tabular}
\end{longtable}
\end{landscape}


\begin{landscape}
\begin{longtable}[c]{l"c|c|c|c|c}
\caption{Mapping Terms between Identified Learned Trust Factors of Significant Reliability and Validity.  Bolded authors are among those in the Top 12 survey instruments.  Entries in red were found to be reliable and valid but span more than one factor.}\\
\label{tab:termMap2}
    %\begin{tabular}{l"c|c|c|c|c}
   \textbf{Author}& \textbf{	Structural Trust} &	\textbf{	Capability-Based Trust} &	\textbf{	Affective Trust} &	\textbf{	General Trust} &	\textbf{	Intention to Use}\\\thickhline
CPRS: \citeA{IdramaniL.SinghRobertMalloy1993}&	Reliance&	Trust&	Safety&&		Confidence\\\hline
\textbf{\citeA{Madsen2000}}&&\begin{tabular}{@{}c@{}}		Perceived Reliability\\\end{tabular} &&		Faith&	\\\hline
\textbf{\citeA{Gefen2003}}&	\begin{tabular}{@{}c@{}}Institution-Based\\ Structural Assurance\end{tabular}&	Perceived Usefulness&	Calculative-Based Beliefs&	Trust&	Intended Use\\\hline
\textbf{\citeA{Korber2018}}&&		Reliability, Competence&	Intention of Developers&	Trust in Automation&	\\\hline
\citeA{Scopelliti2005}&&&&&					\\\hline
\begin{tabular}{@{}l@{}}\citeA{Wang2005}\\\citeA{Komiak2006}\end{tabular}&	Integrity&	Competence&	\begin{tabular}{@{}c@{}}Benevolence\\(Perceived Personalization)\end{tabular}	&&	\begin{tabular}{@{}c@{}}Intention to Adopt\\ Intention to Delegate\end{tabular}\\\hline
\textbf{Heerink (2009, 2011)}&&\begin{tabular}{@{}c@{}}Perceived Usefulness,\\ Attitude\end{tabular}&Perceived Adaptability &Trust&Intention to Use\\\hline
\textbf{\citeA{McKnightD2011}}&	Structural Assurance&	\begin{tabular}{@{}c@{}}Functionality,\\Reliability\end{tabular}&	Helpfulness&	\begin{tabular}{@{}c@{}}Trusting Beliefs\\ in Specific Technology\end{tabular}&	\begin{tabular}{@{}c@{}}Intention to Explore\\Deep Structure Usage\end{tabular}\\\hline
\textbf{Merritt et al. (2008, 2011a)}&&		Trust Scale	&&	Perceived Machine Accuracy&	Reliance\\\hline
\citeA{Sollner2012}	&&		Performance (formative)&	Purpose (formative)&	Trust (reflective)&	\\\hline
\textbf{\citeA{schaefer2013perception}}&	\begin{tabular}{@{}c@{}}\color{red}Robot Behaviors +\\\color{red}Communication\end{tabular}&	\begin{tabular}{@{}c@{}}Performance-Based\\ Functionality\end{tabular}	&&&	\\\hline
\citeA{Sollner2013}	&&		Predictability, Performance&	Helpfulness&	Trust in IT Artifacts&	\\\hline
Wechsung et al. (2013)&	Trust&&		Helpfulness	&&	\\\hline
Ullman et al. (2014, 2021)&	Sincere, Ethical&	Reliable, Capable&&&\\\hline			
\citeA{cherif2014impact} & 	\begin{tabular}{@{}c@{}}\color{red}Recommendation Agent\\ \color{red}Trust\end{tabular}&		\begin{tabular}{@{}c@{}}\color{red}Recommendation Agent\\ \color{red}Trust\end{tabular}&		\begin{tabular}{@{}c@{}}\color{red}Recommendation Agent\\ \color{red}Trust\end{tabular}&	Website Trust&	Intentions\\\hline
\textbf{\citeA{Chien2014}}	&\color{red}Factor 5&		\begin{tabular}{@{}c@{}}Performance Expectancy\end{tabular}&  \color{red}Factor 5& \color{red}Factor 5&	\\\hline	
\citeA{Lee2015}		&&&&& 		\\\hline		
\citeA{Tussyadiah2020}	&&	Reliability, Functionality&	Helpfulness&		Trusting& Intention\\\hline
\textbf{\citeA{Hoffman2013}}&&		\begin{tabular}{@{}c@{}}Human-Robot Fluency,\\Robot Relative\\ Contribution\end{tabular}&	\begin{tabular}{@{}c@{}}Working Alliance\\ Goal Subscale\end{tabular}&	Trust in Robot&	\\\hline
\citeA{Rupp2016}&&&			(Affective Parts of Jian)	&&	Predicted Use\\\hline
\citeA{Hegner2019}&&&&			Trust&	Adoption Intention\\\hline
\citeA{Freude2019}&	\begin{tabular}{@{}c@{}}Institution-Based\\ Structural Assurance\end{tabular}&		Calculative-Based Beliefs&&	Trust&	Intention\\\hline
\citeA{Park2020}&	Structural Assurance&	Performance&	Purpose&	Trust in Service Robots&	\begin{tabular}{@{}c@{}}Intention to Stay\\ in the Hotel\end{tabular}\\\hline
\citeA{Wojton2020}	&&	Performance	&&&		\\\hline
\citeA{Chi2021} &&	\begin{tabular}{@{}c@{}}Trustworthy Robot\\ Function and Design\end{tabular}&	\begin{tabular}{@{}c@{}}Robot-Service Fit\\Trustworthy Service Task\\ and Context\end{tabular}&	SSRIT&
   % \end{tabular}

\end{longtable}
\end{landscape}

\end{document}





