\section{Indirect thinning of general exponential families} \label{sec:general-exp}

Sometimes rather than thinning $X$, we may choose  to thin a function $S(X)$.  When $S(X)$ is sufficient for $\theta$ based on $X$, the next proposition tells us that  thinning $S(X)$ rather than $X$ does not result in a loss of  information about $\theta$.  We emphasize that we are using the concept of sufficiency in two ways  here: (i) $S(X)$ is sufficient for $\theta$ based on $X\sim P_\theta$, and (ii)  $T(\Xt{1},\ldots,\Xt{K})$  is sufficient for $\theta$ based on $(\Xt{1},\ldots,\Xt{K})\sim \Qt{1}_\theta\times\cdots\times\Qt{K}_\theta$.  

\begin{proposition}[Thinning a sufficient statistic preserves information]\label{prop:thin-S}
Suppose $X\sim P_\theta\in\mathcal P$ has a sufficient statistic $S(X)$ for $\theta$, and we thin $S(X)$ by  $T(\cdot)$. That is, conditional on $S(X)$ (and without knowledge of $\theta$) we are able to sample $\Xt{1},\ldots,\Xt{K}$ that are mutually independent and satisfy
\begin{equation}\label{eq:thin-S}
S(X)=T(\Xt{1},\ldots,\Xt{K}).
\end{equation}
Under regularity conditions on the distributions of $X$, $S(X)$, and $\Xt{1},\ldots,\Xt{K}$ needed for Fisher information to exist, we have that
$$
I_X(\theta)=\sum_{k=1}^K I_{\Xt{k}}(\theta).
$$
\end{proposition}

This proposition shows that thinning $S(X)$, rather than $X$, does not result in a loss of information about $\theta$. %allows us to generate $\Xt{1},\ldots,\Xt{K}$ from $X$ without losing any information about $\theta$.  
Its proof (provided in Appendix~\ref{app:pf-thin-S}) follows easily from multiple applications of the fact that sufficient statistics preserve information.

We formalise this strategy in the following definition.

\begin{definition}[Indirect thinning] 
\label{def:indirect}
Consider $X\sim P_\theta\in \mathcal P$. Suppose we thin a sufficient statistic $S(X)$ for $\theta$ by a function $T(\cdot)$. 
We say that the family $\mathcal P$ is {\em indirectly thinned through $S(\cdot)$ by $T(\cdot)$}. 
\end{definition}
In light of Proposition \ref{prop:thin-S}, indirect thinning of a random variable $X$ does not result in a loss of information.

When $S(\cdot)$ is invertible, then
$$
X=S^{-1}(T(\Xt{1},\ldots,\Xt{K})),
$$
which implies that we can thin $X$ directly by $S^{-1}(T(\cdot))$.
It turns out that, regardless of whether we thin $X$ by $S^{-1}(T(\cdot))$ or indirectly thin $X$ through $S(\cdot)$ by $T(\cdot)$, there is little difference between the resulting form of $G_t$ in Theorem~\ref{thm:generalized-thinning}. In the former case, $G_t$ is the conditional distribution of $(\Xt{1},\dots,\Xt{K})$ given $S^{-1}( T(\Xt{1},\dots,\Xt{K}))=  t$. In the  latter case, it is  the conditional distribution of $(\Xt{1},\dots,\Xt{K})$ given $ T(\Xt{1},\dots,\Xt{K})=  t$. Since $S(\cdot)$ is invertible, these two conditional distributions are identical following a reparameterization. 

We now return to the setting of Proposition \ref{prop:thin-S}, where $S(\cdot)$ may or may not be invertible.

\begin{remark}[Indirect thinning of general exponential families]
\label{remark:genexp}
Let $\mathcal{P} = \{P_\theta: \theta \in \Omega\}$ be a general exponential family. That is,

$$
dP_\theta(x) = \exp\{[S(x)]^\top\eta(\theta) - \psi(\theta)\}dH(x),
$$
where $e^{-\psi(\theta)}$ is the normalising constant. Since $S(X)$ is sufficient for $\theta$, we can  indirectly thin $X$ through $S(\cdot)$ without a loss of Fisher information (Proposition~\ref{prop:thin-S}). 
Furthermore,  $S(X)$ belongs to a natural exponential family \citep[Lemma~2.7.2(i)]{lehmann2005testing}. We can thus indirectly thin $X$ through $S(\cdot)$ as follows: 
\begin{enumerate}
\item Provided that the necessary and sufficient condition of Theorem \ref{thm:natural-exponential-families} holds for $S(X)$, we can indirectly thin $X$ through $S(\cdot)$ by addition into $\Xt{1},\ldots,\Xt{K}$ that follow natural exponential families, i.e. general exponential families where  $T^{(k)}(\cdot)$ in  \eqref{eq:exp-fam}  is the identity.
\item  We now consider $\Xt{1},\ldots,\Xt{K}$ that belong to a general exponential family, where
 $T^{(k)}(\cdot)$ in \eqref{eq:exp-fam} is not necessarily the identity. Suppose further that
$S(X)\overset{D}{=}\sum_{k=1}^KT^{(k)}(\Xt{k})$. 
 Then, by Proposition~\ref{prop:exp-family}, we can indirectly thin $X$ through $S(\cdot)$ into 
$\Xt{1},\ldots,\Xt{K}$,
by  $T(\xt{1},\ldots,\xt{K})=\sum_{k=1}^KT^{(k)}(\xt{k})$.
  
\end{enumerate}
We see that 1) is a special case of 2). 
\end{remark}

We now demonstrate the usefulness of indirect thinning with some examples.   
First, we consider a $\text{Beta}(\theta,\beta)$ random variable, with $\beta$ a known parameter. The beta family is not a natural exponential family, and so the results in Section \ref{sec:natural-exp-fam} are not directly applicable.  The beta distribution also differs from some of the other examples that we have seen in the following ways:  (i)  it is not convolution-closed; (ii) it has finite support; and (iii) the sufficient statistic for a sample of independent and identically distributed beta random variables  has an unnamed distribution. 

\begin{exmp}[Thinning $\text{Beta}(\theta,\beta)$ with $\beta$ known] \label{ex:beta}
We start with $\Xt{k} \overset{ind}{\sim} \text{Beta}\left(\frac{1}{K}\theta + \frac{k-1}{K}, \frac{1}{K}\beta\right)$, for  $k=1,\dots,K$; this is a 
general exponential family \eqref{eq:exp-fam} with $T^{(k)}(\xt{k}) = \frac1{K}\log(\xt{k})$. Since $ \sum_{k=1}^K T^{(k)}(\Xt{k})$ is sufficient for $\theta$ based on $\Xt{1},\ldots,\Xt{K}$, we can apply Proposition~\ref{prop:exp-family} to thin the distribution of $\sum_{k=1}^K T^{(k)}(\Xt{k})$ by the function
\begin{equation}\label{eq:beta}
T(\xt{1},\dots,\xt{K})=\sum_{k=1}^K T^{(k)}(\xt{k})=\frac1{K}\sum_{k=1}^K\log(\xt{k})=\log\left[\left(\prod_{k=1}^K\xt{k}\right)^{1/K}\right]. 
\end{equation}

Furthermore, we show in Appendix~\ref{subsec:beta-proofs}  that $\exp\left( T(\Xt{1},\dots,\Xt{K})\right) =\left(\prod_{k=1}^K\Xt{k}\right)^{1/K}$, the geometric mean of $\Xt{1},\ldots,\Xt{K}$,   
follows a $\text{Beta}(\theta, \beta)$ distribution. Therefore, we can indirectly thin a $\text{Beta}(\theta, \beta)$  random variable through  $S(x)=\log(x)$ by $T(\cdot)$ defined in \eqref{eq:beta}. This results in independent random variables $\Xt{k} \sim \text{Beta}\left(\frac{1}{K}\theta + \frac{k-1}{K}, \frac{1}{K}\beta\right)$, for $k=1,\ldots,K$. 

 It turns out that in this example, the thinning operation can be simplified using the fact that $S(x)=\log(x)$ is invertible:  we can  \emph{directly} thin $X\sim\text{Beta}(\theta, \beta)$  by the function 
\begin{equation}
    \label{eq:betaprime}
    T'(\xt{1},\dots,\xt{K})= S^{-1} (T(  \xt{1},\dots,\xt{K})) = \left(\prod_{k=1}^K\xt{k}\right)^{1/K}.
\end{equation}


To thin $\text{Beta}(\theta,\beta)$ using either of the strategies listed above, we need to sample from  $G_t$ defined in Theorem~\ref{thm:generalized-thinning}. This unnamed distribution  can be easily sampled from using numerical methods, as detailed in Appendix \ref{subsec:beta-proofs}.  
\end{exmp}  

By symmetry of the beta distribution, we can also apply the thinning operations detailed in Example~\ref{ex:beta} to thin a $\text{Beta}(\alpha, \theta)$ random variable with $\alpha$ known. 

Next, we consider thinning the gamma distribution with unknown shape parameter.

\begin{exmp}[Thinning $\text{Gamma}(\theta,\beta)$ with $\beta$ known] \label{ex:gamma1} 
We start with $\Xt{k} \overset{ind}{\sim} \text{Gamma}\left(\frac{1}{K}\theta + \frac{k-1}{K}, \frac{1}{K}\beta\right)$,  for  $k=1,\dots,K$; this is a general exponential family (\ref{eq:exp-fam}) with $T^{(k)}(\Xt{k})=\frac{1}{K}\log(\xt{k})$. Note that
$T(\Xt{1},\dots,\Xt{K})=\sum_{k=1}^K T^{(k)}(\Xt{k})$ is sufficient for $\theta$ based on $\Xt{1},\ldots,\Xt{K}$. As $T^{(k)}(\cdot)$ is shared with Example \ref{ex:beta}, we can apply Proposition \ref{prop:exp-family} to thin the distribution of $\sum_{k=1}^K T^{(k)}(\Xt{k})$ by the function defined in (\ref{eq:beta}).

In Appendix~\ref{subsec:gamma-proofs} we show that $\exp\left(T(\Xt{1},\dots,\Xt{K}\right)=\left(\prod_{k=1}^K\Xt{k}\right)^{1/K}$
follows a $\text{Gamma}(\theta, \beta)$ distribution. Thus, we can indirectly thin a $\text{Gamma}(\theta,\beta)$ random variable through $S(x)=\log(x)$ by $T(\cdot)$ defined in (\ref{eq:beta}). This produces independent random variables $\Xt{k}\sim\text{Gamma}(\frac{1}{K}\theta+\frac{k-1}{K},\frac{1}{K}\beta)$ for $k=1,\dots,K$. Once again, noting that $S(\cdot)$ is invertible, we can instead directly thin $X\sim\text{Gamma}(\theta,\beta)$ by the function defined in \eqref{eq:betaprime}.

To apply either of these thinning strategies to a $\text{Gamma}(\theta,\beta)$ random variable, we must sample from $G_t$ as defined in Theorem \ref{thm:generalized-thinning}. This is an unnamed distribution over the set of $K$ positive real numbers with geometric mean $t$. We can sample from this unnamed distribution using numerical methods as detailed in Appendix \ref{subsec:gamma-proofs}. 
\end{exmp}

We emphasize that Example~\ref{ex:gamma1} is different from the gamma thinning example from \cite{neufeld2023data}: that involves thinning a $\text{Gamma}(\alpha,\theta)$ random variable with $\alpha$ known, whereas here we thin a $\text{Gamma}(\theta,\beta)$ random variable with $\beta$ known.

 Examples \ref{ex:beta} and \ref{ex:gamma1} enable us to  thin a random variable into an arbitrary number of folds. However, unlike in the  examples seen in Section~\ref{sec:natural-exp-fam}, the  resulting folds are not identically distributed: they are independent beta or gamma distributions with slightly different parameters.  

In Examples~\ref{ex:beta} and \ref{ex:gamma1}, the function  $S(\cdot)$ through which we indirectly thin $X$ is invertible. 
We now consider a case in which $S(\cdot)$ is not invertible, and furthermore, is not scalar-valued.
Specifically, we let $X=(X_1,\ldots,X_n)$ represent a sample of $n$ independent and identically distributed normal observations with unknown mean $\theta_1$ and variance $\theta_2$. Note that the sample mean and sample variance are jointly sufficient for $\theta_1$ and $\theta_2$. We will now apply ideas from Theorem~\ref{thm:natural-exponential-families} and Proposition~\ref{prop:exp-family} to  thin $X$ through the sample mean and sample variance. 

\begin{exmp}[Indirect thinning of  $N_n(\theta_11_n, \theta_2I_n)$  through the sample mean and sample variance] \label{ex:normal} 
We assume that 
 $X\sim N_n(\theta_11_n, \theta_2I_n)$. Then $S(X)$ is sufficient for $\theta=(\theta_1,\theta_2)$, where
$$S(x) =  \left( \frac{1}{n} \sum_{i=1}^n x_i, \frac{1}{n-1}\sum_{i=1}^{n}\left(x_i -  \frac{1}{n} \sum_{i'=1}^n x_{i'}\right)^2 \right)$$ 
is the function that computes the sample mean and sample variance of the entries of a vector $x$.  We will indirectly thin $X$ through $S(\cdot)$ into $K=n$ univariate normals.
To do so, we start with  $\Xt{k} \overset{iid}{\sim} N\left(\theta_1,\theta_2\right)$ for $k=1,\ldots,K$. A sufficient statistic for $\theta$ based on $(\Xt{1},\ldots,\Xt{K})$ is $T(\Xt{1},\ldots,\Xt{K})$,  where
$$
T(\xt{1},\ldots,\xt{K})=S((\xt{1},\ldots,\xt{K})^\top),
$$
i.e., we concatenate the $K$ entries into a vector and apply $S(\cdot)$.  
Furthermore, $T(\Xt{1},\ldots,\Xt{K})$ has the same distribution as $S(X)$, since  $(\Xt{1},\ldots,\Xt{K})^\top$ and $X$ have the same distribution (recall that we have taken $K=n$ in this example).  This establishes that we can indirectly thin $X$ through $S(\cdot)$ by $T(\cdot)$. 

By Theorem~\ref{thm:generalized-thinning}, we define $G_t$ to be the conditional distribution of  $(\Xt{1},\ldots,\Xt{K})$ given $T(\Xt{1},\ldots,\Xt{K})=t$, i.e. it is the distribution of a $N_K(\theta_11_K, \theta_2I_K)$ random vector conditional on its sample mean and sample variance equalling $t=(t_1,t_2)$.  We will show by a symmetry argument that this conditional distribution is uniform over the set of points in $\mathbb{R}^K$ with sample mean $t_1$ and sample variance $t_2$.  To see this, note that $G_t$ cannot depend on $\theta$ (by sufficiency), so we can take $\theta=(t_1,t_2)$ and equivalently describe $G_t$ as the distribution of a $N_K(t_11_K, t_2I_K)$ random vector conditional on its sample mean and sample variance equalling $t=(t_1,t_2)$.  Such a distribution has constant density on a sphere centered at $t_11_K$.  Thus, the conditional distribution is uniform over the set of points in $\mathbb{R}^K$ with sample mean $t_1$ and sample variance $t_2$.

Finally, we obtain $(\Xt{1},\ldots,\Xt{K})$ by sampling from $G_X$.
\end{exmp}
In effect, Example~\ref{ex:normal} shows that given a realization of $X\sim N_n(\theta_11_n, \theta_2I_n)$, we can generate a new random vector $(\Xt{1},\ldots,\Xt{n})^\top$ with the identical distribution,  and with the same sample mean and sample variance, without knowledge of the true mean or true variance. 
This might have applications in cases where the true values of the observations cannot be shared. %in cases where the privacy of $n$ individuals must be preserved \citep{dwork2014algorithmic}. 
Furthermore, the ideas in  Example~\ref{ex:normal} can be applied in settings where only the sufficient statistics of a realization of $X\sim N_n(\theta_11_n, \theta_2I_n)$ are available, and we wish to generate a ``plausible" sample that could have led to those sufficient statistics. 

Indirect thinning is a very flexible approach, and we briefly list a few more examples illustrating its use. 

\begin{exmp}[Additional examples of indirect thinning]\label{ex:other}

\textcolor{white}{.}

\begin{enumerate}
    \item Suppose we observe $X\sim N(\mu,\theta)$ where $\mu$ is known; here $\mu$ denotes the mean and $\theta$ the variance. Then $S(X)=(X-\mu)^2\sim\theta\chi_1^2=\text{Gamma}\left(\frac{1}{2},\frac{1}{2\theta}\right)$. Thus, by applying the Gamma thinning strategy of \cite{neufeld2023data} discussed in Example \ref{ex:dtgamma} to $S(X)$, we can indirectly thin a normal distribution with unknown variance through $S(\cdot)$. 
    \item Suppose we observe $X\sim\text{Weibull}(\theta, \nu)$ where $\nu$ is known. Then, $S(X)=X^\nu\sim\text{Exp}\left(\theta^{-\nu}\right)$. Thus, by applying the Gamma thinning strategy of Example \ref{ex:dtgamma} or \ref{ex:scaled-normal} to $S(X)$, we can indirectly thin a Weibull distribution with unknown rate through $S(\cdot)$.
    \item Suppose we observe $X\sim\text{Pareto}(\nu,\theta)$ where $\nu$ is known. Then $S(X)=\log \left( X/\nu \right) \sim\text{Exp}(\theta)$. Thus, by applying the Gamma thinning strategy of Example \ref{ex:dtgamma} or \ref{ex:scaled-normal} to $S(X)$, we can indirectly thin a Pareto distribution with unknown shape through $S(\cdot)$. 
\end{enumerate}
\end{exmp}

In Definition~\ref{def:thinning}, we specified that the thinning function $T(\cdot)$ is deterministic. We conclude with a remark noting that indirect thinning can be thought of as a means of relaxing this requirement. 

\begin{remark}[Indirect thinning through non-invertible functions]
Consider the setting where we indirectly thin a random variable $X$ through a non-invertible function $S(\cdot)$. This could instead be viewed as directly thinning $X$ by some stochastic function $T'(\cdot)$ where the randomness is over the set of values for $X$ that map to $S(X)$. However, as noted in Proposition~\ref{prop:thin-S}, as $S(X)$ is a sufficient statistic for $\theta$, this randomness offers no information about $\theta$. For simplicity, we thus conclude that our proposed indirect thinning method is preferable to such a stochastic direct thinning.
\end{remark}

