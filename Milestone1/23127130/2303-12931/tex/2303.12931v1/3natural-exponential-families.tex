\section{Thinning natural exponential families}
\label{sec:natural-exp-fam}

In Section~\ref{subsec:natural}, we show how to thin a natural exponential family into two or more natural exponential families.  In Section~\ref{subsec:neufeld}, we show how the convolution-closed thinning proposal of \citet{neufeld2023data} can be understood in light of natural exponential family thinning.  Finally, in Section~\ref{subsec:natural-to-general}, we show how natural exponential families can be thinned into more general (i.e., not necessarily natural) exponential families.

\subsection{Thinning natural exponential families into natural exponential families} 
\label{subsec:natural}

In this section, we apply Algorithm~\ref{alg:recipe} to 
the important special case of natural exponential families \citep{lehmann2005testing}.  A natural exponential family starts with a known probability distribution $H$, and then forms a family of distributions $\mathcal P^H=\{P^H_\theta:\theta\in\Omega\}$ based on $H$, as follows: 
\begin{equation}
    dP^H_\theta(x)=e^{x^\top\theta-\psi_H(\theta)}dH(x).\label{eq:natural}
\end{equation}
Here $e^{-\psi_H(\theta)}$ is the normalization constant needed to ensure that $P_\theta$ is a probability distribution, and we take $\Omega$ to be the set of $\theta$ for which this normalization is possible (i.e. for which $\psi_H(\theta)<\infty$).  

The next theorem presents a property of $H$ that is  necessary and sufficient for the resulting natural exponential family $\mathcal P^H$ to be thinned by addition into $K$ natural exponential families.  To streamline the statement of the theorem, we start with a definition.
 
\begin{definition}[$K$-way convolution distribution]
\label{def:conv}
We say that a probability distribution $H$ is the $K$-way convolution of distributions $H_1,\ldots, H_K$ if $\sum_{k=1}^KY_k\sim H$ for $(Y_1,\ldots,Y_K)\sim H_1\times\cdots\times H_K$.
\end{definition}

\begin{theorem}[Thinning natural exponential families by addition]\label{thm:natural-exponential-families}
Consider the natural exponential family $\mathcal P^H$ defined in \eqref{eq:natural}.  This family can be thinned by the function $T(\Xt{1},\ldots,\Xt{K})=\sum_{k=1}^K\Xt{k}$ into $K$ natural exponential families $\mathcal P^{H_1},\ldots,\mathcal P^{H_K}$ if and only if $H$ is the $K$-way convolution of $H_1,\ldots,H_K$.
\end{theorem}
This theorem characterizes the set of natural exponential families that can be thinned by addition into $K$ natural exponential families.  These $K$ natural exponential families can be different from each other, but they are all indexed by the same $\theta\in\Omega$ that was used in the original family $\mathcal P^H$.

The proof of Theorem~\ref{thm:natural-exponential-families} is in Appendix~\ref{app:pf-exp-fam}. We provide a brief sketch here. We apply Algorithm~\ref{alg:recipe}, with each $\cQt{k}$ chosen to be a natural exponential family $\mathcal P^{H_k}$, where $H_k$ is an arbitrary distribution. We then imagine observing $(\Xt{1},\ldots,\Xt{K})\sim \Qt{1}_\theta\times\cdots\times\Qt{K}_\theta$. The sum $\Xt{1}+\cdots+\Xt{K}$ is sufficient for $\theta$, so the distribution of this sum can be thinned by addition.  Finally, we note  that $\Xt{1}+\cdots+\Xt{K}\sim P_\theta^{H}$, where $H$ is the convolution of $H_1,\ldots,H_K$.

\cite{neufeld2023data} show that it is possible to thin a Gaussian random variable by addition into $K$ independent Gaussians.  We now see that this result follows from Theorem~\ref{thm:natural-exponential-families}. 

\begin{exmp}[Thinning $N_n(\theta, I_n)$] 
\label{exmp:gaussian}
The family of $N_n(\theta, I_n)$ distributions is a natural exponential family indexed by $\theta\in\mathbb R^n$. It can be written in the notation of this section as 
$\mathcal{P}^H$, where $H$ represents the $N_n(0_n, I_n)$ distribution.  Furthermore, $H$ is the $K$-way convolution of $H_k = N_n(0_n, \epsilon_k I_n)$ for $k = 1, 2, \ldots, K$, where $\epsilon_1,\ldots,\epsilon_K>0$ and $\sum_{k=1}^K \epsilon_k=1$.  Thus, by Theorem~\ref{thm:natural-exponential-families},   we can thin  $\mathcal{P}^H$ by addition into $\mathcal P^{H_1}, \ldots, \mathcal P^{H_K}$, where $P_\theta^{H_k}=N_n(\epsilon_k \theta, \epsilon_k I_n)$. %Unlike the Poisson example, here each $\mathcal P^{H_k}$ is a different family unless $\epsilon_k=1/K$.
\end{exmp}

The above example is particularly notable for its connection to a randomization strategy that has been frequently used in the literature.

\begin{remark}[Connecting Example~\ref{exmp:gaussian} to prior work] 
Other authors have considered obtaining two independent Gaussian random variables $U$ and $V$ from a single Gaussian random variable $X \sim N_n(\theta, I_n)$ by generating $W \sim N_n(0_n, \gamma I_n)$ for a tuning parameter $\gamma > 0$, and then setting $U = X + W$ and  $V = X - \gamma^{-1}W$. Then, $U \sim N_n(\theta, (1 + \gamma) I_n)$ and $V \sim N_n(\theta, (1 + \gamma^{-1}) I_n)$ are independent.
\citet{rasines2021splitting} and \citet{leiner2022data} applied this decomposition to address Scenario 1 in Section \ref{sec:introduction}. Additionally, \citet{rasines2021splitting} showed that this leads to asymptotically valid inference under certain regularity conditions, even when $X$ is not normally distributed.  \citet{tian2020prediction} and \citet{oliveira2021unbiased} applied this decomposition to address Scenario 2 in Section \ref{sec:introduction}. 

This decomposition is in fact identical to Example~\ref{exmp:gaussian} up to scaling, with $\Xt{1}=\epsilon_1U$, $\Xt{2}=\epsilon_2V$, $\epsilon_1=(1+\gamma)^{-1}$, and $\epsilon_2=1-\epsilon_1$.
In particular, to thin $X \sim N_n(\theta, I_n)$ by addition into $(\Xt{1},\Xt{2})$, we sample from $G_X$ where $G_t$, defined in Theorem~\ref{thm:generalized-thinning}, can be shown to equal the singular multivariate normal distribution
$$
N_{2n}\left(\begin{pmatrix}\epsilon_1t\\ \epsilon_2t\end{pmatrix},\epsilon_1\epsilon_2\begin{pmatrix}I_n&-I_n\\-I_n&I_n\end{pmatrix}\right).
$$

Sampling from $G_X$ is equivalent to sampling $W \sim N_n(0_n, \gamma I_n)$ (independent of $X$) and then generating $\Xt{1} = \epsilon_1 (X + W)$ and $\Xt{2} = \epsilon_2 (X - \gamma^{-1} W)$.
These ideas can easily be generalized to thin $X \sim N_n(\theta, \Sigma)$ with a known positive definite covariance matrix $\Sigma$. %\fromdaniela{end of new short subsection}
\end{remark}

Furthermore, Theorem~\ref{thm:natural-exponential-families} implies that we can thin a natural exponential family by addition into natural exponential families \emph{only if} $H$ is the convolution of two or more distributions.
Not all distributions satisfy this property.  For example, the distribution $H=\text{Bernoulli}(0.5)$ cannot be written as the sum of two independent, non-constant random variables.  Since $\mathcal P^{\text{Bernoulli}(0.5)}$ is the $\text{Bernoulli}([1+e^{-\theta}]^{-1})$ natural exponential family, this tells us that Bernoulli random variables cannot be thinned by addition.  In fact, in Section~\ref{sec:bernoulli} we will prove a stronger statement: namely, that {\em no function} $T(\cdot)$ can thin the Bernoulli family.

\subsection{Connections to \citet{neufeld2023data}}
\label{subsec:neufeld}

\cite{neufeld2023data} focus on convolution-closed families, i.e., those for which convolving two or more distributions (see Definition~\ref{def:conv}) that are in the family produces a distribution that is still in the family. 
They provide a recipe for decomposing 
a random variable $X$ drawn from a distribution in such a family into independent random variables $X^{(1)}, \ldots, X^{(K)}$ that sum to yield $X$.  
In this section, we will show that their decompositions of exponential dispersion families are encompassed by Theorem~\ref{thm:natural-exponential-families}.

\emph{Exponential dispersion families} \citep{jorgensen1992exponential, jorgensen1998stationary} are a subclass of convolution-closed families that include many families of interest.  Given a distribution $H$ with $\psi_H(\theta)<\infty$ for $\theta\in\Omega$ (as in \eqref{eq:natural}), we identify the set of distributions $H_\lambda$ for which $\psi_{H_\lambda}(\cdot)=\lambda\psi_H(\cdot)$ (i.e., distributions whose cumulant generating function is a multiple of $H$'s cumulant generating function).  We define $\Lambda$ to be the set of $\lambda$ for which such a distribution $H_\lambda$ exists.  Then, an (additive) \emph{exponential dispersion family} is $\mathcal{P} = \bigcup_{\lambda \in \Lambda}\mathcal P^{H_\lambda}$, where $\mathcal P^{H_\lambda}$ is the natural exponential family generated by $H_\lambda$ (see \eqref{eq:natural}).  The distributions in $\mathcal P$ are indexed over $(\theta,\lambda)\in\Omega\times\Lambda$ and take the following form:
\begin{equation}
    \label{eq_ED*}
dP^{H_\lambda}_{\theta} (x) = e^{x^\top \theta - \lambda \psi_H(\theta)} dH_\lambda(x).
\end{equation}
  In words, an exponential dispersion family results from  combining a collection of related natural exponential families.  For example, starting with $H=\text{Bernoulli(1/2)}$, we can take $\Lambda=\mathbb Z^+$ since for any positive integer $\lambda$, $H_\lambda=\text{Binomial}(\lambda,1/2)$ satisfies the necessary cumulant generating function relationship.  Then, $\mathcal P^{\text{Binomial}(\lambda,1/2)}$ corresponds to the binomial natural exponential family that results from fixing $\lambda$. 
  Finally, allowing $\lambda$ to vary gives the full binomial exponential dispersion family, which is the set of all binomial distributions (varying both of the parameters of the binomial distribution). 


By construction, for any $\lambda_1,\ldots,\lambda_K\in\Omega$, convolving $P^{H_{\lambda_1}}_\theta,\ldots,P^{H_{\lambda_K}}_\theta$ gives the distribution $P^{H_\lambda}_\theta$, where $\lambda=\sum_{k=1}^K\lambda_k$.  The next corollary is an immediate application of Theorem~\ref{thm:natural-exponential-families} in the context of exponential dispersion families.
Notably, 
 the distributions $\Qt{k}_\theta$ themselves still belong to the exponential dispersion family $\mathcal P$ to which the distribution of $X$ belongs. 
\begin{corollary}[Thinning while remaining inside an exponential dispersion family] \label{cor:neufeld}
Consider an exponential dispersion family $\mathcal{P} = \bigcup_{\lambda \in \Lambda}\mathcal P^{H_\lambda}$ and suppose $\lambda_1,\ldots,\lambda_K\in\Lambda$.  Then for $\lambda=\sum_{k=1}^K\lambda_k$, we can thin the natural exponential family $\mathcal P^{H_\lambda}$ by $T(\xt{1},\ldots,\xt{K})=\sum_{k=1}^K\xt{k}$ into the natural exponential families $\mathcal P^{H_{\lambda_1}},\ldots,\mathcal P^{H_{\lambda_K}}$.
\end{corollary}
This result corresponds exactly to the data thinning proposal of  \cite{neufeld2023data}.  We see from Corollary~\ref{cor:neufeld} that that proposal thins a natural exponential family, $\mathcal P^{H_\lambda}$, into a \emph{different} set of natural exponential families, 
$\mathcal P^{H_{\lambda_1}},\ldots,\mathcal P^{H_{\lambda_K}}$. However, 
 from the perspective of exponential dispersion families, it thins an exponential dispersion family into the same exponential dispersion family.  Continuing the binomial example from above, the corollary tells us that we can thin the binomial family with $\lambda$ as the number of trials into two or more binomial families with smaller numbers of trials, provided that $\lambda>1$.

\cite{neufeld2023data} focus on convolution-closed families, not exponential dispersion families. However, \cite{jorgensen1998stationary} note that all convolution-closed families that have moment-generating functions can be written as exponential dispersion families. The Cauchy distribution is convolution-closed, but does not have a moment generating function and thus is not an exponential dispersion family. As we will see in Example~\ref{ex:cauchy}, the $\text{Cauchy}(\theta_1, \theta_2)$ distribution cannot be thinned by addition: decomposing it using the recipe of \citet{neufeld2023data} requires knowledge of both unknown parameters. Thus, not all convolution-closed distributions can be thinned by addition in the sense of Definition~\ref{def:thinning}. However, \cite{neufeld2023data} claim that all convolution-closed distributions \emph{can} be thinned. This apparent discrepancy  is due to a slight difference in the definition of thinning between our paper and theirs: in Definition~\ref{def:thinning}, we require that $G_t$ not depend on $\theta$; however, \cite{neufeld2023data} have no such requirement. In practice, data thinning is useful only if $G_t$ does not depend on $\theta$, and so there is no meaningful difference between the two definitions.
 


\subsection{Thinning natural exponential families into general exponential families} 
\label{subsec:natural-to-general}


In this section, we apply Algorithm~\ref{alg:recipe} in the case that $\cQt{k}$ are (possibly non-natural) exponential families, for which the sufficient statistic need not be the identity. 
In particular, for $k=1,\ldots, K$, we let $\cQt{k}=\{\Qt{k}_\theta:\theta\in\Omega\}$ denote an exponential family based on a known distribution $H_k$ and sufficient statistic $T^{(k)}(\cdot)$:
\begin{equation}\label{eq:exp-fam}
d\Qt{k}_\theta(x)=\exp\{[T^{(k)}(x)]^\top\eta(\theta)-\psi_k(\theta)\}dH_k(x).
\end{equation}
As in Section \ref{subsec:natural}, $e^{-\psi_k(\theta)}$ is the normalization constant needed to ensure that $\int d\Qt{k}_\theta(x)=1$ and $\Omega$ is the set of $\theta$ for which $\psi_k(\theta)<\infty$.  The function $\eta(\cdot)$ maps $\theta$ to the natural parameter.  We note that  $\sum_{k=1}^K T^{(k)}(\Xt{k})$ is a sufficient statistic for $\theta$ based on $(\Xt{1},\dots,\Xt{K})\sim \Qt{1}_\theta\times\cdots\times\Qt{K}_\theta$.   Then  Algorithm~\ref{alg:recipe} tells us that we can thin the distribution of this sufficient statistic. This  leads to the next result.

\begin{proposition}[Thinning natural exponential families with more general functions $T(\cdot)$] \label{prop:exp-family}
Let $\Xt{1},\dots,\Xt{K}$ be independent random variables with $\Xt{k}\sim\Qt{k}_\theta$ for $k=1,\ldots,K$ from any (i.e., possibly non-natural) exponential families $\cQt{k}$ as in \eqref{eq:exp-fam}.
Let $P_\theta$ denote the distribution of $\sum_{k=1}^K T^{(k)}(\Xt{k})$. Then, $\mathcal P=\{P_\theta:\theta\in\Omega\}$ is a natural exponential family, and we can thin it  into $\Xt{1},\ldots,\Xt{K}$ using the function $T(\xt{1},\dots,\xt{K})=\sum_{k=1}^K T^{(k)}(\xt{k})$.
\end{proposition}
The fact that $\mathcal P$ in this result is a natural exponential family follows from recalling that the sufficient statistic of an exponential family follows a natural exponential family \citep[Lemma~2.7.2(i)]{lehmann2005testing}. 
Many named exponential families are not natural exponential families, involving non-identity functions $T^{(k)}(\cdot)$, such as the logarithm or polynomials.  Therefore, to thin into those families, Proposition~\ref{prop:exp-family} will be useful.
We illustrate the flexibility provided by Proposition~\ref{prop:exp-family} with a number of examples. 

In particular, we demonstrate that a natural exponential family $\mathcal P$ can be thinned by different functions $T(\cdot)$, leading to families of distributions $\cQt{1},\ldots,\cQt{K}$ different from $\mathcal P$. Specifically, the following examples demonstrate three possible $K$-fold thinning strategies for a gamma distribution when the shape, $\alpha$, is known but the rate\footnote{Although $\theta$ is often used in the gamma distribution to denote the scale parameter, here we use it to denote the rate parameter.} $\theta$, is unknown: the first uses the method in \cite{neufeld2023data}, the second applies when $\alpha$ can be expressed as half of a natural number and decomposes the gamma observations into centred normal data, and the final applies when $\alpha$ is a natural number and decomposes the gamma observations into Weibull data with shape $\nu$, a value to be chosen by the analyst.  

\begin{exmp}[Thinning $\text{Gamma}(\alpha,\theta)$ with $\alpha$ known, approach 1] \label{ex:dtgamma} 
Following Algorithm~\ref{alg:recipe}, we start with $\Xt{k} \overset{iid}{\sim} \text{Gamma}\left(\frac{\alpha}{K},\theta\right)$ for $k=1,\ldots,K$, and note that $T(\Xt{1},\dots,\Xt{K})=\sum_{k=1}^K\Xt{k}$ is sufficient for $\theta$. Thus, we can thin the distribution of $\sum_{k=1}^K\Xt{k}$.  A well-known property of the gamma distribution tells us that this is a $\text{Gamma}(\alpha,\theta)$ distribution.  Sampling from $G_t$ as in Theorem~\ref{thm:generalized-thinning} 
corresponds exactly to the multi-fold gamma data thinning recipe of \cite{neufeld2023data} where $\epsilon_k=\frac{1}{K}$.
\end{exmp}

The next two examples apply Proposition~\ref{prop:exp-family} to decompose the gamma family into different families. 

\begin{exmp}[Thinning $\text{Gamma}(\alpha,\theta)$ with $\alpha=K/2$ known, approach 2] \label{ex:scaled-normal}
Starting with $\Xt{k} \overset{iid}{\sim} N(0,\frac{1}{2\theta})$, notice that $T^{(k)}(\xt{k})=(\xt{k})^2$. We can thus apply Proposition~\ref{prop:exp-family} using the function 
$$
T(\xt{1},\dots,\xt{K})=\sum_{k=1}^K(\xt{k})^2
$$
to thin the sufficient statistic, $\sum_{k=1}^K(\Xt{k})^2\sim\frac{1}{2\theta}\chi_K^2=\text{Gamma}\left(\frac{K}{2},\theta\right)$, into $(\Xt{1},\dots,\Xt{K})$. The function $G_t$ from Theorem~\ref{thm:generalized-thinning} is the conditional distribution
$$
(\Xt{1},\dots,\Xt{K})|\sum_{k=1}^K(\Xt{k})^2=t.
$$
By the rotational symmetry of the $N_K(0,(2\theta)^{-1}I_K)$ distribution (which is the joint distribution of $(\Xt{1},\dots,\Xt{K})$), $G_t$ corresponds to the uniform distribution on the $(K-1)$-sphere of radius $t^{1/2}$. 
\end{exmp}

To sample from this conditional distribution, we %To generate $(\Xt{1},\dots,\Xt{K})$, the above example requires sampling conditional on $X$ from a sphere of radius $X^{1/2}$. To do so, we can 
generate $Z\sim N_K(0,I_K)$ and then take  $(\Xt{1},\dots,\Xt{K})|X$ to be $X^{1/2}\frac{Z}{\|Z\|_2}$.

The next example produces not just a single thinning function $T(\cdot)$ but a {\em family} of thinning functions, indexed by a parameter $\nu>0$ that the analyst can choose.

\begin{exmp}[Thinning $\text{Gamma}(\alpha,\theta)$ with $\alpha=K$ known, approach 3]
\label{ex:weibull}
Recall that the Weibull distribution with known shape parameter $\nu$ (but varying $\theta$) is a member of a general exponential family.  Then, starting with $\Xt{k} \overset{iid}{\sim} \text{Weibull}(\lambda, \nu)$ for $k=1,\dots,K$, we have that $T^{(k)}(\xt{k}) = (\xt{k})^\nu$. We can thus apply Proposition~\ref{prop:exp-family} using the function
$$
T(\xt{1},\dots,\xt{K})=\sum_{k=1}^K(\xt{k})^\nu
$$
to thin the distribution of $\sum_{k=1}^K(\Xt{k})^\nu$ into $(\Xt{1},\dots,\Xt{K})$. In Appendix~\ref{subsec:weibull-proofs}, we show that $\sum_{k=1}^K(\Xt{k})^\nu\sim\text{Gamma}(K,\lambda^{-\nu})$, so then taking $\lambda=\theta^{-1/\nu}$ yields the desired result.  

To generate $(\Xt{1},\dots,\Xt{K})$, we can first apply the $K$-fold gamma thinning result discussed in Example \ref{ex:dtgamma} with $\epsilon_k = \frac{1}{K}$ to generate $Y^{(k)} \overset{iid}{\sim} \text{Exp}(\lambda^{-\nu})$, and then compute $\Xt{k} = (Y^{(k)})^{\frac{1}{\nu}}$. 
\end{exmp}

Examples \ref{ex:dtgamma}, \ref{ex:scaled-normal}, and \ref{ex:weibull} present three distinct strategies for thinning a gamma distribution with unknown rate parameter. The first approach applies most generally, allowing an analyst to thin any gamma distribution with known $\alpha$, not just the subset with $\alpha$ parameters that are integers or halves of integers. Yet the second and third can have contextual advantages. For instance, if $X$ is a sample variance that follows a scaled chi-squared distribution, then recovering an underlying sample of normal observations may be more useful than a sample of gamma data. Similarly, if $X$ is the total waiting time for $k$ units, then decomposing it into Weibull realizations may be preferable. 

