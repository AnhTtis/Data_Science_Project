\section{Introduction}
\label{sec:introduction}

Suppose that we want to \emph{fit} and \emph{validate} a model on the basis of a single dataset.  Two example scenarios are as follows:
\begin{list}{}{}
\item{\emph{Scenario 1.}} We  want to use the data both to generate and to test a hypothesis. 
\item{\emph{Scenario 2.}} We want to use the  data both to fit a complicated model, and to obtain an accurate estimate of the expected prediction error. 
\end{list}
In either case, it is clear that a naive approach that fits and validates a model on the same data is deeply problematic. In Scenario 1, testing a hypothesis on the same data used to generate it will lead to hypothesis tests that do not control the Type 1 error, and to confidence intervals that do not attain the nominal coverage \citep{fithian2014optimal}.   And in Scenario 2, estimating the expected prediction error on the same data used to fit the model will lead to massive downward bias  \citep[see][for recent reviews]{tian2020prediction,oliveira2021unbiased}.

In the case of Scenario 1, recent interest  has focused on \emph{selective inference}, a framework that enables a data analyst to generate and test a hypothesis on the same data \citep[see, e.g.,][]{taylor2015statistical}. The main idea is as follows: to test a hypothesis generated from the data, we should condition on the event that we selected this particular hypothesis. Despite promising applications of this framework to a number of problems, such as inference after regression \citep{lee2016exact}, changepoint detection \citep{jewell2022testing,hyun2021post}, clustering \citep{gao2020selective,chen2022selective,yun2023selective}, and outlier detection \citep{chen2020valid}, it suffers from some drawbacks: 
\begin{enumerate}
\item To perform selective inference, the  procedure used to generate the null hypothesis must be fully-specified in advance.  For instance, if a researcher wishes to cluster the data and then test for a difference in means between the clusters, as in \cite{gao2020selective} and \cite{chen2022selective}, then they must fully specify the clustering procedure (e.g., hierarchical clustering with squared Euclidean distance and complete linkage, cut to obtain $K$ clusters) in advance. 
\item Finite-sample selective inference typically requires an assumption of multivariate Gaussianity, though in some cases this can be relaxed to obtain asymptotic results \citep{taylor2018post,tian2017asymptotics,tibshirani2018uniform,tian2018selective}.
\end{enumerate}
Thus, it is clear that selective inference does not provide a flexible, ``one-size-fits-all" approach to Scenario 1. 

In the case of Scenario 2, proposals to de-bias the ``in-sample" estimate of expected prediction error  tend to be specialized to simple models, and thus do not provide an all-purpose tool that is broadly applicable to complex contemporary settings \citep{oliveira2021unbiased}.

\emph{Sample splitting} \citep{cox1975note} is an intuitive  approach that is broadly applicable to a variety of settings, including Scenarios 1 and 2; see the left-hand panel of Figure~\ref{fig:samplesplit_vs_datathin}. We split a dataset containing $n$ observations into two sets, containing $n_1$ and $n_2$ observations, respectively (where $n_1+n_2=n$). Then we can generate a hypothesis based on the first set and test it on the second set (Scenario 1), or we can fit a model to the first set and estimate its error on the second set (Scenario 2). Sample splitting also forms the basis for cross-validation, an important tool for a practicing data scientist \citep{hastie2009elements}. 

While sample splitting often can 
 adequately address both Scenarios 1 and 2, it also suffers from some drawbacks: 
\begin{enumerate} 
    \item If the data contain outliers, then each outlier is assigned to a single subsample. %Again, this may not be desirable.
    \item If the observations are not independent (for instance, if they correspond to a time series) then the subsamples that result from sample splitting are not independent, and so sample splitting does not provide a solution to either Scenario 1 or Scenario 2.
    \item If one is interested in drawing conclusions at a per-observation level, then sample splitting is unsuitable.  For example, if sample splitting is applied to a dataset consisting of the 50 states of the United States, then one can only conduct inference or perform validation on those states not used in fitting.
    \item If the model of interest is fit using  unsupervised learning, then  sample splitting may  not provide an adequate solution in either Scenario 1 or 2.  The issue relates to \#3 above. This is discussed in \cite{gao2020selective,chen2022selective}, and \cite{neufeld2022inference} in the context of Scenario 1. 
\end{enumerate}

In recent work, \cite{neufeld2023data} proposed an approach for \emph{convolution-closed data thinning} that addresses these drawbacks. They consider splitting, or \emph{thinning}, a random variable $X$ drawn from a convolution-closed family into $K$ independent random variables $\Xt{1},\ldots,\Xt{K}$ such that
$X=\sum_{k=1}^K\Xt{k}$, 
and $\Xt{1},\ldots,\Xt{K}$ come from the same family of distributions as $X$ (see the right-hand panel of Figure~\ref{fig:samplesplit_vs_datathin}). 
For instance, they show that $X \sim N(\mu, \sigma^2)$ can be thinned into two independent $N(\epsilon \mu, \epsilon \sigma^2)$ and $N((1-\epsilon) \mu, (1-\epsilon) \sigma^2)$ random variables that sum to $X$. 
Finally, and most critically, if  $X$ is drawn from a Gaussian, Poisson, negative binomial, binomial, multinomial, or gamma distribution, then they can thin it  \emph{even when parameters of its distribution are unknown}. Because the thinned random variables are independent, this
provides a new approach to tackle Scenarios 1 and 2:  
one thins the dataset into two independent datasets.  One then fits a model to one dataset, and  validates it on the other. 




On the surface, it is quite remarkable that one can break up a random variable $X$ into two or more {\em independent} random variables that sum to $X$ without knowing some (or sometimes any) of the parameters.  In this paper, we seek to explain the underlying principles that make this possible.  In doing so, we show that convolution-closed data thinning can be generalized so as to make it more flexible and much more widely applicable. The convolution-closed data thinning property $X=\sum_{k=1}^K\Xt{k}$ is desirable because it ensures that no information has been lost in the thinning process. However, clearly this would be equally true if we were to replace the summation by any other deterministic function.  Likewise, the fact that $\Xt{1},\ldots,\Xt{K}$ are from the same family as $X$, while convenient, is nonessential. 

Our generalization of convolution-closed data thinning is thus a procedure for splitting $X$ into $K$ random variables such that  the following two properties hold: 
$$
\text{  (i) } X=T(\Xt{1},\ldots,\Xt{K}); \text{ and  (ii) }\Xt{1},\ldots,\Xt{K} \text{ are mutually independent}.
$$
This generalization is broad enough to simultaneously encompass both convolution-closed data thinning and sample splitting. Furthermore, it greatly increases the scope of distributions that can be thinned. In the $K=2$ case, this generalized goal has been stated before \citep[see][``P1'' property]{leiner2022data} as we will describe later.  However, we are the first to develop a widely applicable strategy for achieving this goal.   Not only are we able to thin exponential families that were not previously possible (such as the  beta family), but we can even thin outside of the exponential family.  For example, generalized thinning enables us to thin $X \sim \text{Unif}(0, \theta)$ into
 $\Xt{k} \overset{\text{iid}}{\sim} \theta \cdot\text{Beta}\left(\frac{1}{K},1\right)$, for $k=1,\dots,K$, in such a way that $X=\max\{\Xt{1},\ldots,\Xt{K}\}$.




The primary contributions of our paper are as follows:
\begin{enumerate}
\item We propose \emph{generalized data thinning}, a general strategy for thinning a single random variable $X$ into two or more independent random variables, $\xo,\ldots,\Xt{K}$, without knowledge of the parameter value(s).  
%Unlike in \cite{neufeld2023data}, we recover the original random variable $X$ using the function $T(\xo,\ldots,\Xt{k})$, where $T(\cdot)$ need not be addition, and the distributions of the $\Xt{k}$'s need not be the same as $X$.  
Importantly, we show that {\em sufficiency} is the key property underlying the choice of the function $T(\cdot)$.
\item We show that it is possible to apply generalized data thinning to distributions far outside the scope of consideration of \cite{neufeld2023data}: these include the beta, uniform, and shifted exponential distributions, among others.  A summary of distributions covered by this work is provided in Table~\ref{table:maintable}.  In light of results by \cite{darmois, koopman}, and \cite{pitman_1936} (see the end of Section~\ref{sec:method}), we believe our examples are representative of the full range of cases to which this sufficiency-based approach can be applied. 
\item We show that sample splitting --- which, on its surface, bears little resemblance to convolution-closed data thinning --- is in fact based on the same principle: both are special cases of generalized data thinning with different choices of the function $T(\cdot)$.  In other words, our proposal is a direct \emph{generalization} of sample splitting. 
\end{enumerate}



We are not the first to propose generalizations of sample splitting.  Inspired by \cite{tian2018selective}'s use of randomized responses, \cite{rasines2021splitting} define what they call the $(U,V)$-decomposition, which injects independent noise $W$ to create two independent random variables $U=u(X,W)$ and $V=v(X,W)$ that together are jointly sufficient for the unknown parameters.  However, they do not describe how to perform a $(U,V)$-decomposition other than in the special case of a Gaussian random vector with known covariance.  Our generalized thinning framework achieves the goal set out in their paper, providing a concrete recipe for finding such decompositions in a broad set of examples.  Another paper with a similar goal is \cite{leiner2022data}.  They define ``data fission'', which seeks to find random variables $f(X)$ and $g(X)$ for which the distributions of $f(X)$ and $g(X) \mid f(X)$ are known and for which $X=h(f(X),g(X))$. When these two random variables are independent (which they describe as the ``P1'' property), their proposal aligns with generalized thinning.  However, like \cite{rasines2021splitting}, they do not provide a general strategy for performing P1-fission, and the only two examples they provide are the Gaussian vector with known covariance and the Poisson.

The rest of our paper is organized as follows. In Section~\ref{sec:method}, we define generalized data thinning, present our main theorem, and provide a simple recipe for thinning that is followed throughout the paper. In Section~\ref{sec:natural-exp-fam}, we consider the case of thinning natural exponential families; this section also revisits the convolution-closed data thinning proposal of \cite{neufeld2023data}, and clarifies the class of distributions that can be thinned using that approach. In Section~\ref{sec:general-exp}, we show that we can apply data thinning to  general exponential families. We consider distributions outside of the exponential family in Section~\ref{sec:outside-exp-fam}. Section~\ref{sec:counterexamples} contains examples of distributions that \emph{cannot} be thinned using the approaches in this paper; these examples provide insight into the fact that sufficiency is the key property needed for (generalized) data thinning to ``work".  We verify our results numerically in Section~\ref{sec:experiments}.  Finally, we close with a discussion in Section~\ref{sec:discussion}; derivations and additional technical details are deferred to the appendix.  


\begin{figure}
  \hspace{39mm}  Sample splitting    \hspace{10mm} Generalized data thinning 
  \vspace{-3mm}
\begin{center} 
\centering
\includegraphics[scale=0.18,trim={3cm 8cm 39cm 8cm},clip]{figures/schematics-002.png}
\includegraphics[scale=0.18,trim={3cm 8cm 40cm 8cm},clip]{figures/schematics-v3-003.png} 
\caption{\emph{Left:} Sample splitting assigns each observation into either a training set or a test set. 
\emph{Right:} Generalized data thinning, the proposal of this paper, splits each observation into two parts that are independent and that can be used to recover the original observation $T(\xo, \Xt{2})=X$. In some cases, they are drawn from the same distributional family as $X$.  
\label{fig:samplesplit_vs_datathin}}
\end{center}
\end{figure}

\input{table}
