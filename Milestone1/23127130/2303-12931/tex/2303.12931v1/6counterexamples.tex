\section{Counterexamples}
\label{sec:counterexamples}

In this section, we present two examples in which thinning strategies do not work.
The first  involves a natural exponential family that is based on a distribution that {\em cannot} be written as the convolution of two distributions. In this case, Theorem~\ref{thm:natural-exponential-families} implies that we cannot thin it by addition. In fact, we will prove a stronger statement: namely, that  there does not exist \emph{any} function $T(\cdot)$ that can thin it.  The second example involves a convolution-closed family outside of the natural exponential family in which addition is not sufficient. In this case, taking $T(\cdot)$ to be addition does not enable thinning, as Theorem \ref{thm:generalized-thinning} does not apply.

\subsection{The Bernoulli family cannot be thinned}
\label{sec:bernoulli}

Let $P_{\theta}$ denote the $\mathrm{Bernoulli}(\theta)$ distribution, where $\theta$ is the probability of success. While this distribution can be written as a natural exponential family (with natural parameter $\log \left( \frac{\theta}{1 - \theta} \right)$), it cannot be written as the sum of two independent, non-degenerate random variables. Thus, Theorem~\ref{thm:natural-exponential-families} does not apply, i.e. it cannot be thinned by addition into natural exponential families.

In this section, we prove a stronger result---namely, that there is {\em no function} $T(\cdot)$ that enables us to thin $P_{\theta}$ into distributions $\Qt{1}_{\theta}$ and $\Qt{2}_{\theta}$
with well-defined, positive Fisher information.

Based on Theorem~\ref{thm:generalized-thinning} from Section~\ref{sec:method}, we can thin $X \sim P_{\theta}$ into independent $\Qt{1}_{\theta}$ and $\Qt{2}_{\theta}$ if and only if there exists a function $T(\cdot)$ such that $X = T(\Xt{1},\Xt{2})$ and $T(\Xt{1}, \Xt{2})$ is sufficient for $\theta$ in the joint distribution of $\Xt{1}$ and $\Xt{2}$. The next result says that if such $\Qt{1}_{\theta}$ and $\Qt{2}_{\theta}$ have positive Fisher information, then no such function $T(\cdot)$ exists. 

\begin{theorem}
\label{thm:bernoulli}
Suppose that
\begin{itemize}
\item $T(\Xt{1}, \Xt{2}) \sim \mathrm{Bernoulli}(\theta)$,
\item $\Xt{1} \sim \Qt{1}_{\theta}, \Xt{2} \sim \Qt{2}_{\theta}$, and $\Xt{1}$ and $\Xt{2}$ are independent, where $\Qt{1}_{\theta}$ and $\Qt{2}_{\theta}$ are from families where Fisher information exists, and 
\item $I_{\Xt{1}}(\theta) > 0$ and $I_{\Xt{2}}(\theta) > 0$. 
\end{itemize}
Then  $T(\Xt{1}, \Xt{2})$ is not  sufficient for $\theta$ based on $\Xt{1}$ and $\Xt{2}$. 
\end{theorem}

The proof of Theorem~\ref{thm:bernoulli} is in Appendix~\ref{appendix:bernoulli-proof}. In summary, Bernoulli is a natural exponential family that cannot be thinned by any function into any distribution with well-defined Fisher information. 
A similar argument reveals that the categorical distribution (i.e., a multinomial with a single trial) also cannot be thinned into distributions with well-defined, positive Fisher information.



\subsection{Not all convolution-closed families can be thinned by addition}
\label{subsec:cauchy}

Suppose now that our interest lies in a random variable $X=T(\Xt{1},\Xt{2})$, where $T(\Xt{1}, \Xt{2})$ is \emph{not} sufficient for the parameter $\theta$ based on $(\Xt{1},\Xt{2})$. This means that the conditional distribution of $(\Xt{1},\Xt{2})$ given $T(\Xt{1},\Xt{2})$ depends on $\theta$, and thus that we cannot thin $X$ by $T(\cdot)$.  We see this in the following example.

\begin{exmp}[The trouble with thinning $\text{Cauchy}(\theta_1, \theta_2)$ by addition] \label{ex:cauchy}
Recall that the Cauchy family, $\text{Cauchy}(\theta_1, \theta_2)$, indexed by $\theta=(\theta_1,\theta_2)$,  is convolution-closed. In particular, if $\Xt{1},\Xt{2}\overset{iid}{\sim}\text{Cauchy}\left(\frac{1}{2}\theta_1, \frac{1}{2}\theta_2\right)$, then $\Xt{1} + \Xt{2}\sim\text{Cauchy}(\theta_1, \theta_2)$.  It is tempting therefore to try thinning this family by $T(\xt{1},\xt{2})=\xt{1}+\xt{2}$.  However, the sum $\Xt{1} + \Xt{2}$ is not sufficient for $\theta$, which means that Theorem~\ref{thm:generalized-thinning} does not apply and in particular $G_t$, the conditional distribution of $(\Xt{1},\Xt{2})$ given $\Xt{1} + \Xt{2}=t$, \emph{is} a function of $\theta$. Therefore, we cannot thin the Cauchy family with unknown parameters by addition. 
\end{exmp}

In fact, we can take this result a step further: for a sample of Cauchy random variables, the only sufficient statistic for $\theta$ is the vector of order statistics \citep[p.~275]{CaseBerg}. 
Thus, there is no family $\mathcal{P}$ and corresponding function $T(\cdot)$ that reduces the data for which thinning will produce independent Cauchy random variables.
However, sample splitting (which as shown in Section~\ref{sec:sample-splitting} is based on a $T(\cdot)$ that does not reduce the data) where $X$ is a sample of $K$ independent Cauchy random variables would lead to thinning into $K$ independent Cauchy random variables.
