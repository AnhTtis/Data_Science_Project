\section{Proofs} \label{sec:appendix}

\subsection{Proof of Theorem~\ref{thm:generalized-thinning}} \label{app:pf-sufficiency-thinning}

\begin{proof}
By their construction in Definition \ref{def:thinning}, the random variables $(\Xt{1},\ldots,\Xt{K})\sim \Qt{1}_\theta\times\cdots\times\Qt{K}_\theta$ have conditional distribution
$$
(\Xt{1},\ldots,\Xt{K})|\{X=t\}\sim G_t.
$$
Furthermore, Definition~\ref{def:thinning} tells us that $X=T(\Xt{1},\ldots,\Xt{K})$. This means that
$$
(\Xt{1},\ldots,\Xt{K})|\{T(\Xt{1},\ldots,\Xt{K})=t\}\sim G_t,
$$
which establishes part (b) of the theorem.

The distribution $G_t$ in Definition~\ref{def:thinning} does not depend on $\theta$ (note that it is associated with the entire family $\mathcal P$, not a particular distribution $P_\theta$). By the definition of sufficiency, the fact that the conditional distribution $(\Xt{1},\ldots,\Xt{K})|T(\Xt{1},\ldots,\Xt{K})$ does not depend on $\theta$ implies that $T(\Xt{1},\ldots,\Xt{K})$ is sufficient for $\theta$. This proves (a).
\end{proof}

\subsection{Proof of Theorem~\ref{thm:natural-exponential-families}} \label{app:pf-exp-fam}

\begin{proof}
We start by proving the $\impliedby$ direction.
Suppose $H$ is the convolution of $H_1,\ldots, H_K$.  We follow the recipe given in Algorithm~\ref{alg:recipe}:

\begin{enumerate}
    \item We choose $\cQt{k}=\mathcal P^{H_k}$ for $k=1,\ldots,K$.
    \item Let $(\Xt{1},\ldots,\Xt{K})\sim P^{H_1}_\theta\times \cdots\times P^{H_K}_\theta$.  This joint distribution satisfies
    $$
    \prod_{k=1}^KdP^{H_k}_\theta(\xt{k})=\exp\left\{\left(\sum_{k=1}^K\xt{k}\right)^\top\theta-\sum_{k=1}^K\psi_{H_k}(\theta)\right\}\prod_{k=1}^KdH_k(\xt{k}).
    $$
    By the factorization theorem, we find that $T(\Xt{1},\ldots,\Xt{K})=\sum_{k=1}^K\Xt{k}$ is sufficient for $\theta$.
    \item It remains to determine the distribution of $U=T(\Xt{1},\ldots,\Xt{K})$.  This random variable is the convolution of $P^{H_1}_\theta\times \cdots\times P^{H_K}_\theta$, and its distribution $\mu$ is defined by the following $K$-way integral:
    \begin{align*}
d\mu(u)&=\int\cdots\int1\left\{\sum_{k=1}^K\xt{k}=u\right\}\prod_{k=1}^KdP^{H_k}_\theta(\xt{k})\\
&=\exp\left\{u^\top\theta-\sum_{k=1}^K\psi_{H_k}(\theta)\right\}\int\cdots\int1\left\{\sum_{k=1}^K\xt{k}=u\right\}\prod_{k=1}^KdH_k(\xt{k})\\
&=\exp\left\{u^\top\theta-\psi_{H}(\theta)\right\}dH(u)\\
&=dP^H_\theta(u),
    \end{align*}
where in the second-to-last equality we use the assumption that $H$ is the $K$-way convolution of $H_1,\ldots,H_K$ and the fact that the moment generating function of a convolution is the product of the individual moment generating functions (and recalling that $\psi_H$ is the logarithm of the moment generating function of $H$). This establishes that $T(\Xt{1},\ldots,\Xt{K})\sim P^H_\theta$. By Theorem~\ref{thm:generalized-thinning}, the family $\mathcal P^H$ is thinned by this choice of $T(\cdot)$.
\end{enumerate}

We now prove the $\implies$ direction.
Suppose that $\mathcal P^H$ can be $K$-way thinned into $\mathcal P^{H_1},\ldots, \mathcal P^{H_K}$ using the summation function.  Then applying Definition~\ref{def:thinning} with $\theta=0$, we can take $X\sim P^{H}_0$ and produce $(\Xt{1},\ldots,\Xt{K})\sim P^{H_1}_0\times\cdots\times P^{H_K}_0$ for which $X=\Xt{1}+\cdots+\Xt{K}$.  Noting that $P^{H_k}_0=H_k$ for all $k$ and $P^{H}_0=H$, this proves that $H$ is a $K$-way convolution of $H_1,\ldots,H_K$. 
\end{proof}

\subsection{Proof of Proposition~\ref{prop:thin-S}} \label{app:pf-thin-S}

\begin{proof}
The result follows from a chain of equalities:
\begin{align*}
    I_X(\theta)&=I_{S(X)}(\theta)\\
    &=I_{T(\Xt{1},\ldots,\Xt{K})}(\theta)\\
    &=I_{(\Xt{1},\ldots,\Xt{K})}(\theta)\\
    &=\sum_{k=1}^K I_{\Xt{k}}(\theta).
\end{align*}
The first equality is true because $S(X)$ is sufficient for $\theta$ based on $X$.  The second equality follows from the definition of thinning $S(X)$ into $\Xt{1},\ldots,\Xt{K}$ using $T(\cdot)$.  The third equality follows from Theorem~\ref{thm:generalized-thinning}, which tells us that $T(\Xt{1},\ldots,\Xt{K})$ is sufficient for $\theta$ based on $(\Xt{1},\ldots,\Xt{K})$. The final equality follows from independence.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:bernoulli}}
\label{appendix:bernoulli-proof}

\begin{proof}
Suppose $0.5 < \theta < 1$.  We prove the result assuming that $0.5 < \theta < 1$ since if
$T(X^{(1)}, X^{(2)})$ cannot be sufficient when $\theta > 0.5$, it cannot be sufficient when $\theta\in(0,1)$. 

As $T(X^{(1)}, X^{(2)})$ is Bernoulli, we can write it as an indicator $T(X^{(1)}, X^{(2)}) = I((X^{(1)}, X^{(2)}) \in \mathcal{C})$ for some set $\mathcal{C}$. Let $\mathcal{C}_1$ denote the projection of $\mathcal{C}$ onto the $X^{(1)}$ axis, meaning that $(X^{(1)}, X^{(2)}) \in \mathcal{C}$ implies $X^{(1)} \in \mathcal{C}_1$. 

Next, let $U\left(X^{(1)}\right) = I\left(X^{(1)} \in \mathcal{C}_1\right)$. Note that this is also a Bernoulli random variable, and note that its success probability is $\theta+\epsilon,$ where $\epsilon \geq 0$, because  $(X^{(1)}, X^{(2)}) \in \mathcal{C}$ implies $X^{(1)} \in \mathcal{C}_1$. 

First, suppose that $\theta+\epsilon = 1$. In this case, the function $T(X^{(1)}, X^{(2)})$ does not depend on $X^{(1)}$, and thus can be written as $\tilde T(X^{(2)})$. In this case,  note that
\begin{align*}
I_{T(X^{(1)}, X^{(2)})}(\theta) &= I_{\tilde T(X_2)}(\theta) \\
&\leq I_{X^{(2)}}(\theta) \\
&< I_{X^{(1)}}(\theta)+I_{X^{(2)}}(\theta) = I_{X^{(1)}, X^{(2)}}(\theta).
\end{align*}
The inequality in the second line follows because a statistic of a random variable can never contain more information than the variable itself. The strict inequality in the third line follows from our assumption that $I_{X^{(1)}}(\theta) > 0$, and the final equality follows from independence of $X^{(1)}$ and $X^{(2)}$. 
We conclude that $T\left(X^{(1)}, X^{(2)}\right)$ is not sufficient for $\theta$, because it contains strictly less information than $\left(X^{(1)}, X^{(2)}\right)$. 



Next, suppose that $\theta+\epsilon < 1$. Direct calculation yields that $I_{T(X^{(1)}, X^{(2)})}(\theta) = \frac{1}{\theta(1-\theta)}$, and that $I_{U(X^{(1)})}(\theta) = \frac{1}{(\theta+\epsilon)(1-(\theta+\epsilon))}$. Because we assumed that $0.5 \leq \theta \leq \theta+\epsilon < 1$, we see that $I_{T(X^{(1)}, X^{(2)})}(\theta) \leq I_{U(X^{(1)})}(\theta)$. This then implies that
\begin{align*}
I_{T(X^{(1)}, X^{(2)})}(\theta) &\leq I_{U(X^{(1)})}(\theta)\\
&\leq I_{X^{(1)}}(\theta) \\
&< I_{X^{(1)}(\theta)}+I_{X^{(2)}}(\theta) =  I_{X^{(1)}, X^{(2)}}(\theta).
\end{align*}
The inequality in the second line follows from the fact that a statistic of a random variable can never contain more information than the variable itself. The strict inequality in the third line follows from our assumption that $I_{X^{(2)}}(\theta) > 0$. We conclude that the function $T(X^{(1)}, X^{(2)})$ cannot be sufficient, because it contains strictly less information about $\theta$
than $(X^{(1)}, X^{(2)})$.
\end{proof}
