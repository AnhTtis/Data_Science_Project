\section{The generalized thinning proposal}
\label{sec:method}
 Here and throughout, we write $X$ to denote a random variable that can be scalar-, vector-, or matrix-valued (and likewise for $\Xt{1},\ldots,\Xt{K}$). 
We define \emph{generalized data thinning} as follows.
\begin{definition}[Generalized  data thinning] 
\label{def:thinning}
Consider a family of distributions $\mathcal P=\{P_\theta:\theta\in\Omega\}$. Suppose that there exists a distribution $G_t$, not depending on $\theta$, and a deterministic function $T(\cdot)$ such that when we sample $(\Xt{1},\ldots,\Xt{K})|X$ from $G_X$, for  $X \sim P_\theta$, the following properties hold: 
\begin{enumerate}
  \item $\Xt{1},\ldots,\Xt{K}$ are mutually independent (and non-degenerate),  and
    \item $X=T(\Xt{1},\ldots,\Xt{K})$. 
\end{enumerate}
Then we say that $\mathcal{P}$ is \emph{thinned} by the function $T(\cdot)$.
\end{definition}
When clear from context, sometimes we will say that ``$P_\theta$ is thinned'' or ``X is thinned,'' by which we mean that the corresponding family $\mathcal P$ is thinned. 
Intuitively, we can think of thinning as breaking $X$ up into $K$ independent pieces, but in a very particular way that ensures that none of the information about $\theta$ is lost. The fact that no information is lost is evident from the requirement that $X=T(\Xt{1},\ldots,\Xt{K})$. 

It turns out that sample splitting \citep{cox1975note} can be viewed as a special case of generalized data thinning. 
\begin{remark}[Sample splitting] \label{remark:samplesplit}
Sample splitting, in which a sample of $n$ independent and identically distributed random variables is partitioned into $K$ subsamples, is a special case of generalized thinning.  Here, $T(\cdot)$ is the function that takes in the subsamples as arguments, and concatenates and sorts their elements. For more details, see Section~\ref{sec:sample-splitting}.
\end{remark}

Furthermore, Definition~\ref{def:thinning} is closely related to the proposal of \cite{neufeld2023data}.   




\begin{remark}[Thinning convolution-closed families of distributions] \label{remark:convclosed}

 \cite{neufeld2023data} show that some well-known families of convolution-closed distributions, such as the binomial, negative binomial, gamma, Poisson, and Gaussian, can be thinned, in the sense of Definition~\ref{def:thinning}, by addition:  
$T\left(\xt{1}, \ldots, \xt{K}\right) = \sum_{k=1}^K \xt{k}$. 
\end{remark}

The  two examples above do not resemble each other: the first involves a non-parametric family of distributions and applies quite generally, while the second depends on a  specific property of the family of distributions.  Furthermore, the functions $T(\cdot)$ are quite different from each other.  It is natural to ask: How can we find families $\mathcal P$ that can be thinned?  Is there a unifying principle for the choice of $T(\cdot)$? How can we ensure that there exists a distribution $G_t$ as in Definition~\ref{def:thinning} that does not depend on $\theta$?  The following theorem answers these questions, and indicates that \emph{sufficiency} is the key principle required to ensure that the distribution $G_t$ does not depend on $\theta$.



\begin{theorem}[Main theorem] \label{thm:generalized-thinning}
Suppose $\mathcal P$ is thinned by a function $T(\cdot)$ and, for $X\sim P_\theta$, let $\Qt{1}_\theta\times\cdots\times\Qt{K}_\theta$ denote the distribution of the mutually independent random variables, $(\Xt{1},\ldots,\Xt{K})$, sampled as in Definition~\ref{def:thinning}. Then, the following hold:
\begin{enumerate}[(a)]
    \item $T(\Xt{1},\ldots,\Xt{K})$ is a sufficient statistic for $\theta$ based on $(\Xt{1},\ldots,\Xt{K})$.
    \item The distribution $G_t$  in Definition~\ref{def:thinning} is the conditional distribution
    $$
    (\Xt{1},\ldots,\Xt{K})|T(\Xt{1},\ldots,\Xt{K})=t,
    $$
    where $(\Xt{1},\ldots,\Xt{K})\sim \Qt{1}_\theta\times\cdots\times\Qt{K}_\theta$.
    \end{enumerate}
\end{theorem}

Theorem~\ref{thm:generalized-thinning} is proven in Appendix~\ref{app:pf-sufficiency-thinning}. 
This theorem further suggests a simple strategy for finding families of distributions  $\mathcal{P}$ and functions $T(\cdot)$ such that $\mathcal P$ can be thinned by $T(\cdot)$.  

\begin{algorithm}[Finding distributions that can be thinned]\label{alg:recipe}
\textcolor{white}{.}
\begin{enumerate}
    \item Choose $K$ families of distributions, $\cQt{k}=\{\Qt{k}_\theta:\theta\in\Omega\}$ for $k=1,\ldots, K$.
    \item Let $(\Xt{1},\ldots,\Xt{K})\sim \Qt{1}_\theta\times\cdots\times\Qt{K}_\theta$, and let  $T(\Xt{1},\ldots,\Xt{K})$ denote a sufficient statistic for $\theta$.
    \item Let $P_\theta$ denote the distribution of $T(\Xt{1},\ldots,\Xt{K})$.  
\end{enumerate}
By Theorem~\ref{thm:generalized-thinning}, the family $\mathcal P=\{P_\theta:\theta\in\Omega\}$ is thinned by $T(\cdot)$.
\end{algorithm}

This recipe gives us a very succinct way to describe the distributions that can be thinned: \emph{we can thin the distributions of sufficient statistics}.
In particular, the recipe takes as input a joint distribution $\Qt{1}_\theta\times\cdots\times\Qt{K}_\theta$, and requires us to choose a sufficient statistic for $\theta$. Then, that statistic's distribution is the $P_\theta$ that can be thinned.

The next three sections of this paper focus on demonstrating the utility of Theorem~\ref{thm:generalized-thinning} and Algorithm~\ref{alg:recipe} in a series of examples. Outside of the exponential family, the sufficient statistic grows in dimension with the sample size, except for some distributions that have support controlled by a parameter \citep{darmois, koopman, pitman_1936}. Motivated by this result, we focus on exponential families in Sections~\ref{sec:natural-exp-fam} and \ref{sec:general-exp}, and on examples outside of the exponential family in Section~\ref{sec:outside-exp-fam}.  The full set of examples considered in the coming sections is listed in Table~\ref{table:maintable}.
