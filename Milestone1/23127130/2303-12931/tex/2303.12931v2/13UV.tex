\section{Connecting Example~\ref{exmp:gaussian} to prior work} \label{app:UV}
 
Other authors have considered obtaining two independent Gaussian random variables $U$ and $V$ from a single Gaussian random variable $X \sim N_n(\theta, I_n)$ by generating $W \sim N_n(0_n, \gamma I_n)$ for a tuning parameter $\gamma > 0$, and then setting $U = X + W$ and  $V = X - \gamma^{-1}W$. Then, $U \sim N_n(\theta, (1 + \gamma) I_n)$ and $V \sim N_n(\theta, (1 + \gamma^{-1}) I_n)$ are independent.
\citet{rasines2021splitting} and \citet{leiner2022data} applied this decomposition to address Scenario 1 in Section \ref{sec:introduction}. Additionally, \citet{rasines2021splitting} showed that this leads to asymptotically valid inference under certain regularity conditions, even when $X$ is not normally distributed.  \citet{tian2020prediction} and \citet{oliveira2021unbiased} applied this decomposition to address Scenario 2 in Section \ref{sec:introduction}. 

This decomposition is in fact identical to Example~\ref{exmp:gaussian} up to scaling, with $\Xt{1}=\epsilon_1U$, $\Xt{2}=\epsilon_2V$, $\epsilon_1=(1+\gamma)^{-1}$, and $\epsilon_2=1-\epsilon_1$.
In particular, to thin $X \sim N_n(\theta, I_n)$ by addition into $(\Xt{1},\Xt{2})$, we sample from $G_X$ where $G_t$, defined in Theorem~\ref{thm:generalized-thinning}, can be shown to equal the singular multivariate normal distribution
$$
N_{2n}\left(\begin{pmatrix}\epsilon_1t\\ \epsilon_2t\end{pmatrix},\epsilon_1\epsilon_2\begin{pmatrix}I_n&-I_n\\-I_n&I_n\end{pmatrix}\right).
$$

Sampling from $G_X$ is equivalent to sampling $W \sim N_n(0_n, \gamma I_n)$ (independent of $X$) and then generating $\Xt{1} = \epsilon_1 (X + W)$ and $\Xt{2} = \epsilon_2 (X - \gamma^{-1} W)$.
These ideas can easily be generalized to thin $X \sim N_n(\theta, \Sigma)$ with a known positive definite covariance matrix $\Sigma$.
