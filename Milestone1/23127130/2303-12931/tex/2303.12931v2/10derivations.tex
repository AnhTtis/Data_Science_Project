\section{Derivations of decompositions} \label{sec:derivations}

\subsection{Exponential families}

\subsubsection{Weibull distribution}
\label{subsec:weibull-proofs}

The gamma family with known shape $\alpha$ and unknown rate $\theta$ admits a collection of thinning functions, indexed by a hyperparameter $\nu>0$, that thin the gamma family into the Weibull family.

\begin{exmp}[Thinning $\text{Gamma}(\alpha,\theta)$ with $\alpha=K$ known, approach 3]
\label{ex:weibull}
Recall that the Weibull distribution with known shape parameter $\nu$ (but varying scale $\lambda$) is a general exponential family.  Then, starting with $\Xt{k} \overset{iid}{\sim} \text{Weibull}(\lambda, \nu)$ for $k=1,\dots,K$, we have that $T^{(k)}(\xt{k}) = (\xt{k})^\nu$. We can thus apply Proposition~\ref{prop:exp-family} using the function
$$
T(\xt{1},\dots,\xt{K})=\sum_{k=1}^K(\xt{k})^\nu
$$
to thin the distribution of $\sum_{k=1}^K(\Xt{k})^\nu$ into $(\Xt{1},\dots,\Xt{K})$. 
As $\sum_{k=1}^K(\Xt{k})^\nu\sim\text{Gamma}(K,\lambda^{-\nu})$, %so then 
taking $\lambda=\theta^{-1/\nu}$ yields the desired result.  

To generate $(\Xt{1},\dots,\Xt{K})$, we can first apply the $K$-fold gamma thinning result discussed in Example \ref{ex:dtgamma} with $\epsilon_k = \frac{1}{K}$ to generate $Y^{(k)} \overset{iid}{\sim} \text{Exp}(\lambda^{-\nu})$, and then compute $\Xt{k} = (Y^{(k)})^{\frac{1}{\nu}}$. 
\end{exmp}

\begin{proof}[Proof of Example~\ref{ex:weibull}]
We must prove that if $\Xt{k} \overset{iid}{\sim} \text{Weibull}(\lambda, \nu)$, for $k=1,\dots,K$, then $\sum_{k=1}^K\left(\Xt{k}\right)^\nu\sim \text{Gamma}(K, \lambda^{-\nu})$.

Recalling that the gamma distribution is convolution-closed in its shape parameter, it is sufficient to show for a single $\Xt{k}\sim\text{Weibull}(\lambda, \nu)$ random variable that $(\Xt{k})^\nu \sim \text{Gamma}(1,\lambda^{-\nu})=\text{Exp}(\lambda^{-\nu})$, where $\nu>0$. Denote $Z=(\Xt{k})^\nu$. Then,

\begin{align*}
    f_Z(z) &= f_{\Xt{k}}\left(z^{\frac{1}{\nu}}\right)\left|\frac{d \xt{k}}{d z}\right| \\
    &\propto \left(z^{\frac{1}{\nu}}\right)^{\nu-1} \exp\left(-\left(\frac{z^{\frac{1}{\nu}}}{\lambda}\right)^\nu\right)\left|\frac{1}{\nu}z^{-\frac{\nu-1}{\nu}}\right| \\
    &\propto \exp\left(-\lambda^{-\nu}z\right).
\end{align*}

The above implies that $(\Xt{k})^\nu \sim \text{Exp}(\lambda^{-\nu})$, and thus $\sum_{k=1}^K\left(\Xt{k}\right)^\nu\sim \text{Gamma}(K, \lambda^{-\nu})$ as required.
    
\end{proof}

\subsubsection{Beta distribution}
\label{subsec:beta-proofs}

\begin{proof}[Proof of Example~\ref{ex:beta}]
We must prove the following three claims:
\begin{enumerate}
    \item \emph{If $\Xt{k} \sim \text{Beta}\left(\frac{1}{K}\theta + \frac{k-1}{K}, \frac{1}{K}\beta\right)$, for $k=1,\dots,K$, and $\Xt{k}$ are mutually independent, then $\left(\prod_{k=1}^K \Xt{k}\right)^{\frac{1}{K}} \sim \text{Beta}(\theta, \beta)$.} 


Recall that the beta distribution is fully characterised by its moments due to its finite support \citep{feller1971}. Also recall that the expectation of the $r$th power of a $X\sim\text{Beta}(\theta,\beta)$ random variable is $E[X^r]=\frac{B(\theta+r,\beta)}{B(\theta,\beta)}$ where $B$ is the beta function. Finally, note that the Gauss multiplication theorem \citep[page 256]{abramowitz1972handbook} says that
$$
\prod_{k=1}^K\Gamma\left(z+\frac{k-1}{K}\right)=(2\pi)^{\frac{K-1}{2}}K^{\frac{1}{2}-Kz}\Gamma(Kz).
$$


Then, the $r$th moment of $\left(\prod_{k=1}^K \Xt{k}\right)^{\frac{1}{K}}$ is

\begin{align*} 
E\left[\left(\left(\prod_{k=1}^K \Xt{k}\right)^{\frac{1}{K}}\right)^r\right] &= \prod_{k=1}^KE\left[ \left(\Xt{k}\right)^{\frac{r}{K}}\right] \\
&= \prod_{k=1}^K\frac{B(\frac{1}{K}\theta+\frac{k-1}{K}+\frac{r}{K},\frac{1}{K}\beta)}{B(\frac{1}{K}\theta+\frac{k-1}{K},\frac{1}{K}\beta)} \\
&= \prod_{k=1}^K\frac{\frac{\Gamma(\frac{1}{K}\theta+\frac{k-1}{K}+ \frac{r}{K})\Gamma(\frac{1}{K}\beta)}{\Gamma(\frac{1}{K}\theta +\frac{k-1}{K}+ \frac{r}{K}+\frac{1}{K}\beta)}}{\frac{\Gamma(\frac{1}{K}\theta+\frac{k-1}{K})\Gamma(\frac{1}{K}\beta)}{\Gamma(\frac{1}{K}\theta+\frac{k-1}{K}+\frac{1}{K}\beta)}} \\
&= \prod_{k=0}^{K-1}\frac{\Gamma(\frac{1}{K}\theta+ \frac{r}{K}+\frac{k}{K})\Gamma(\frac{1}{K}\theta+\frac{1}{K}\beta+\frac{k}{K})}{\Gamma(\frac{1}{K}\theta+\frac{1}{K}\beta+ \frac{r}{K}+\frac{k}{K})\Gamma(\frac{1}{K}\theta+\frac{k}{K})} \\
&= \frac{\left[\prod_{k=0}^{K-1}\Gamma(\frac{1}{K}\theta+ \frac{r}{K}+\frac{k}{K})\right]\left[\prod_{k=0}^{K-1}\Gamma(\frac{1}{K}\theta+\frac{1}{K}\beta+\frac{k}{K})\right]}{\left[\prod_{k=0}^{K-1}\Gamma(\frac{1}{K}\theta+\frac{1}{K}\beta+ \frac{r}{K}+\frac{k}{K})\right]\left[\prod_{k=0}^{K-1}\Gamma(\frac{1}{K}\theta+\frac{k}{K})\right]} \\
&= \frac{\left[(2\pi)^{\frac{K-1}{2}}K^{\frac{1}{2}-(\theta+r)}\Gamma(\theta+r)\right]\left[(2\pi)^{\frac{K-1}{2}}K^{\frac{1}{2}-(\theta+\beta)}\Gamma(\theta+\beta)\right]}{\left[(2\pi)^{\frac{K-1}{2}}K^{\frac{1}{2}-(\theta+\beta+r)}\Gamma(\theta+\beta+r)\right]\left[(2\pi)^{\frac{K-1}{2}}K^{\frac{1}{2}-\theta}\Gamma(\theta)\right]} \\
&= \frac{\Gamma(\theta+r)\Gamma(\beta)\Gamma(\theta+\beta)}{\Gamma(\theta+\beta+r)\Gamma(\theta)\Gamma(\beta)} \\
&= \frac{B(\theta+r,\beta)}{B(\theta,\beta)}.
\end{align*}
This matches the moments of a $\text{Beta}(\theta, \beta)$ distribution, implying that $\left(\prod_{k=1}^K \Xt{k}\right)^{\frac{1}{K}} \sim \text{Beta}(\theta, \beta)$ as required. 
    
    \item \emph{A sufficient statistic for $\theta$ in the joint distribution of $\Xt{1},\dots,\Xt{K}$ is 
    $$T(\Xt{1},\dots,\Xt{K})=\left(\prod_{k=1}^K \Xt{k}\right)^{\frac1{K}}.$$}

By the mutual independence of $\Xt{k}$, the joint density of $\Xt{1},\dots,\Xt{K}$ can be written as

\begin{align*} 
f_{\Xt{1},\dots,\Xt{K}}(\xt{1},\dots,\xt{K}) &= \prod_{k=1}^K f_{\Xt{k}}(\xt{k}) \\
&\propto \prod_{k=1}^K \left(\xt{k}\right)^{\frac{1}{K}\theta + \frac{k-1}{K} - 1}\left(1-\xt{k}\right)^{\frac{1}{K}\beta-1} \\
&=\left[\left(\prod_{k=1}^K \xt{k}\right)^{\frac{1}{K}}\right]^{\theta}\left[\prod_{k=1}^K \left(\xt{k}\right)^{\frac{k-1}{K}-1}\right]\left[\prod_{k=1}^K\left(1-\xt{k}\right)\right]^{\frac{1}{K}\beta-1}.
\end{align*}

    By the factorization theorem, $T(\Xt{1},\dots,\Xt{K})=\left(\prod_{k=1}^K \Xt{k}\right)^{\frac{1}{K}}$ is a sufficient statistic for $\theta$.

    \item \emph{To sample from $G_t$, i.e., the distribution of $(\Xt{1},\dots,\Xt{K})|T(\Xt{1},\dots,\Xt{K})=t$ 
    , we first sample from $(\Xt{1},\dots,\Xt{K-1})|T(\Xt{1},\dots,\Xt{K})=t$ and then recover $\Xt{K}$.}

We will show that the conditional density $f_{\Xt{1},\dots,\Xt{K-1}|T(\Xt{1},\dots,\Xt{K})=t}(\xt{1},\dots,\xt{K-1})$ is, up to a normalizing constant involving $t$, 
$$
\left[\prod_{k=1}^{K-1}\left(\xt{k}\right)^{\frac{k-K}{K}-1}\right]\left[\left(\prod_{k=1}^{K-1}\left(1-\xt{k}\right)\right)\left(1-\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)\right]^{\frac{1}{K}\beta-1}.$$
We derive this as follows (where any factors not involving $\xt{1},\ldots,\xt{K-1}$ are omitted, and we write $\theta_k=\frac{\theta}{K}+\frac{k-1}{K}$):

\begin{align*}
&\quad f_{\Xt{1},\dots,\Xt{K-1}|T(\Xt{1},\dots,\Xt{K})=t}(\xt{1},\dots,\xt{K-1}) \\ 
&\propto f_{\Xt{1},\dots,\Xt{K-1},T(\Xt{1},\dots,\Xt{K})}(\xt{1},\dots,\xt{K-1},t) \\
&= f_{\Xt{1},\dots,\Xt{K}}\left(\xt{1},\dots,\xt{K-1},\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)\left|\frac{\partial}{\partial t}\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right| \\
&=\left(\prod_{k=1}^{K-1}f_{\Xt{k}}\left(\xt{k}\right)\right)f_{\Xt{K}}\left(\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)\left|\frac{Kt^{K-1}}{\prod_{k=1}^{K-1}\xt{k}}\right| \\
&\propto \left(\prod_{k=1}^{K-1}\left(\xt{k}\right)^{\theta_k-1}\left(1-\xt{k}\right)^{\frac{1}{K}\beta-1}\right) \left(\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)^{\theta_K-1}\left(1-\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)^{\frac{1}{K}\beta-1}\frac{1}{\prod_{k=1}^{K-1}\xt{k}} \\
&\propto \left[\prod_{k=1}^{K-1}\left(\xt{k}\right)^{\theta_k-\theta_K-1}\right]\left[\left(\prod_{k=1}^{K-1}\left(1-\xt{k}\right)\right)\left(1-\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)\right]^{\frac{1}{K}\beta-1}.
\end{align*}
It remains to note that $\theta_k-\theta_K=\frac{k-K}{K}$.

To generate $(\Xt{1},\dots,\Xt{K})$, first sample from $(\Xt{1},\dots,\Xt{K-1})|T(\Xt{1},\dots,\Xt{K})=t$ with numerical sampling methods. In this example, a Metropolis algorithm with a uniform proposal over $[t^{K},1)^K$ is effective \citep{metropolis1953equation}. Then, compute $\Xt{K}=\frac{t^K}{\prod_{k=1}^{K-1}\Xt{k}}$. 
\end{enumerate}
\end{proof}

\subsubsection{Gamma distribution}
\label{subsec:gamma-proofs}

\begin{proof}[Proof of Example~\ref{ex:gamma1}]
We must prove the following three claims:
\begin{enumerate}
    \item \emph{If for $k=1,\dots,K$, $\Xt{k} \sim \text{Gamma}\left(\frac{1}{K}\theta + \frac{k-1}{K}, \frac{1}{K}\beta\right)$ and $\Xt{k}$ are mutually independent, then $\left(\prod_{k=1}^K \Xt{k}\right)^{\frac{1}{K}} \sim \text{Gamma}(\theta, \beta)$.} 

Rather than work with gamma random variables directly, we will find it convenient to work with the logarithm of gamma random variables. We start by deriving the moment generating function of this distribution.
\begin{lemma}\label{lemma:log-gamma}
Consider a random variable $Y$ such that $e^Y\sim \text{Gamma}(\theta, \beta)$.  Then the moment generating function of $Y$ exists in a neighborhood around 0 and is given by 
$$
\Phi_{Y}(t)=\frac{\Gamma(\theta+t)}{\Gamma(\theta)\beta^{t}}.
$$
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:log-gamma}]
The density of $Y$ is given by
\begin{align*}
    f_Y(y)&=f_{\text{Gamma}(\theta, \beta)}(e^y)e^y\\
    &=\frac{\beta^\theta}{\Gamma(\theta)}e^{y(\theta-1)}e^{-\beta e^y}e^y\\
    &=\frac{\beta^\theta}{\Gamma(\theta)}e^{y\theta-\beta e^y},
\end{align*}
where the extra factor of $e^y$ in the first equality is the Jacobian of the transformation.  For $t>-\theta$, the moment generating function for this random variable is given by
\begin{align*}
\Phi_Y(t)&=\mathbb E[e^{tY}]\\
    &=\int e^{ty}\frac{\beta^\theta}{\Gamma(\theta)}e^{y\theta-\beta e^y}dy\\
    &=\frac{\beta^\theta}{\Gamma(\theta)}\frac{\Gamma(\theta+t)}{\beta^{\theta+t}}\int\frac{\beta^{\theta+t}}{\Gamma(\theta+t)} e^{y(\theta+t)-\beta e^y}dy\\
    &=\frac{\Gamma(\theta+t)}{\Gamma(\theta)\beta^{t}}\int\frac{\beta^{\theta+t}}{\Gamma(\theta+t)} e^{y(\theta+t)-\beta e^y}dy\\
    &=\frac{\Gamma(\theta+t)}{\Gamma(\theta)\beta^{t}}.
    \end{align*}
The assumption that $t>-\theta$ ensures that $\theta+t>0$ so that the integrand in the second-to-last line is the density of the logarithm of a $\text{Gamma}(\theta+t,\beta)$ random variable.  Since $\theta>0$, we have established that the moment generating function exists in a neighborhood around 0.
\end{proof}

Defining $Y^{(k)}=\log\Xt{k}$ and $\bar Y_K=\frac{1}{K}\sum_{k=1}^KY^{(k)}$, observe that
$$
\left(\prod_{k=1}^K \Xt{k}\right)^{\frac{1}{K}}=e^{\bar Y_K}.
$$
Thus, our goal is to prove that $e^{\bar Y_K}\sim \text{Gamma}(\theta, \beta)$.  Since the moment generating function completely characterizes a distribution, it is sufficient to show that 
$\Phi_{\bar Y_K}(t)$ matches the expression in Lemma~\ref{lemma:log-gamma}. Applying Lemma~\ref{lemma:log-gamma} to $e^{Y^{(k)}}\sim\text{Gamma}(\theta_k,\beta_k)$, where $\theta_k=\theta/K+(k-1)/K$ and $\beta_k=\beta/K$, implies that 
$$
\Phi_{Y^{(k)}}=\frac{\Gamma(\theta_k+t)}{\Gamma(\theta_k)\beta_k^{t}}.
$$
By independence of $Y^{(1)},\ldots,Y^{(K)}$ and standard properties of the moment generating function,
\begin{align*}    
\Phi_{\bar Y_K}(t)&=\prod_{k=1}^K\Phi_{Y_K/K}(t)\\
&=\prod_{k=1}^K\Phi_{Y_K}(t/K)\\
&=\prod_{k=1}^K\frac{\Gamma(\theta_k+t/K)}{\Gamma(\theta_k)\beta_k^{t/K}}.
\end{align*}

Recalling the form of $\theta_k$ and $\beta_k$ and applying the Gauss multiplication theorem \citep[page 256]{abramowitz1972handbook} to both the numerator and denominator gives
$$
\Phi_{\bar Y_K}(t)=\frac{K^{-(\theta+t)}\Gamma(\theta+t)}{K^{-\theta}\Gamma(\theta)}\frac{1}{(\beta/K)^t}=\frac{\Gamma(\theta+t)}{\Gamma(\theta)\beta^t}.
$$
This completes the proof.

    \item \emph{A sufficient statistic for $\theta$ in the joint distribution of $\Xt{1},\dots,\Xt{K}$ is $T(\Xt{1},\dots,\Xt{K})=\left(\prod_{k=1}^K \Xt{k}\right)^{\frac{1}{K}}$.}

By the mutual independence of $\Xt{k}$, the joint density of $\Xt{1},\dots,\Xt{K}$ can be written as,

\begin{align*} 
f_{\Xt{1},\dots,\Xt{K}}(\xt{1},\dots,\xt{K}) &= \prod_{k=1}^K f_{\Xt{k}}(\xt{k}) \\
&\propto \prod_{k=1}^K \left(\xt{k}\right)^{\frac{1}{K}\theta + \frac{k-1}{K} - 1}\exp\left(-\frac{\beta}{K}\xt{k}\right) \\
&=\left[\left(\prod_{k=1}^K \xt{k}\right)^{\frac{1}{K}}\right]^{\theta}\left[\prod_{k=1}^K \left(\xt{k}\right)^{\frac{k-1}{K}-1}\right]\exp\left(-\frac{\beta}{K}\sum_{k=1}^K\xt{k}\right)
\end{align*}

    By the factorization theorem, $T(\Xt{1},\dots,\Xt{K})=\left(\prod_{k=1}^K \Xt{k}\right)^{\frac{1}{K}}$ is a sufficient statistic for $\theta$ as required.

    \item \emph{To sample from $G_t$, the conditional distribution $(\Xt{1},\dots,\Xt{K})|T(\Xt{1},\dots,\Xt{K})=t$
    , we first sample from $(\Xt{1},\dots,\Xt{K-1})|T(\Xt{1},\dots,\Xt{K})=t$ and then recover $\Xt{K}$.}

The conditional density $f_{\Xt{1},\dots,\Xt{K-1}|T(\Xt{1},\dots,\Xt{K})=t}(\xt{1},\dots,\xt{K-1})$, up to a normalizing constant depending on $t$, is
$$
\left[\prod_{k=1}^{K-1}\left(\xt{k}\right)^{\frac{k-K}{K}-1}\right]\exp\left(-\frac{\beta}{K}\left(\sum_{k=1}^{K-1}\xt{k} + \frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)\right).
$$
The derivation is as follows (where any factors not involving $\xt{1},\ldots,\xt{K-1}$ are omitted and we write $\theta_k=\frac{\theta}{K}+\frac{k-1}{K}$):

\begin{align*}
&\quad f_{\Xt{1},\dots,\Xt{K-1}|T(\Xt{1},\dots,\Xt{K})=t}(\xt{1},\dots,\xt{K-1}) \\ 
&\propto f_{\Xt{1},\dots,\Xt{K-1},T(\Xt{1},\dots,\Xt{K})}(\xt{1},\dots,\xt{K-1},t) \\
&= f_{\Xt{1},\dots,\Xt{K}}\left(\xt{1},\dots,\xt{K-1},\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)\left|\frac{\partial}{\partial t}\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right| \\
&=\left(\prod_{k=1}^{K-1}f_{\Xt{k}}\left(\xt{k}\right)\right)f_{\Xt{K}}\left(\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)\left|\frac{Kt^{K-1}}{\prod_{k=1}^{K-1}\xt{k}}\right| \\
&\propto \left(\prod_{k=1}^{K-1}\left(\xt{k}\right)^{\theta_k-1}\exp\left(-\frac{\beta}{K}\xt{k}\right)\right) \left(\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)^{\theta_K-1}\exp\left(-\frac{\beta}{K}\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)\frac{1}{\prod_{k=1}^{K-1}\xt{k}} \\
&\propto \left(\prod_{k=1}^{K-1}\left(\xt{k}\right)^{\frac{k-K}{K}-1}\right)\exp\left(-\frac{\beta}{K}\left(\sum_{k=1}^{K-1}\xt{k}+\frac{t^K}{\prod_{k=1}^{K-1}\xt{k}}\right)\right),
\end{align*}
where in the last step we used that $\theta_k-\theta_K=\frac{k-K}{K}$.

To generate $(\Xt{1},\dots,\Xt{K})$, first sample from $(\Xt{1},\dots,\Xt{K-1})|T(\Xt{1},\dots,\Xt{K})=t$ with numerical sampling methods. In this example, MCMC methods work well though the choice of proposal distribution should consider $K$ and $\beta$. Then, compute $\Xt{K}=\frac{t^K}{\prod_{k=1}^{K-1}\Xt{k}}$. 
\end{enumerate}
\end{proof}

\subsection{Families with support controlled by an unknown parameter}

\subsubsection{Scaled beta distribution}
\label{subsec:scaled-beta-proofs}

We consider the family of distributions obtained by scaling a $\text{Beta}(\alpha,1)$ distribution (with $\alpha$ fixed) by an unknown scale parameter $\theta>0$.  In the special case that $\alpha=1$, this corresponds to the $\text{Unif}(0,\theta)$ family presented in Example~\ref{ex:scaled-uniform}.

\begin{exmp}[Thinning $\theta\cdot\text{Beta}(\alpha,1)$ with $\alpha$ known] \label{ex:scaled-beta}
We start with  $\Xt{k} \overset{iid}{\sim} \theta\cdot\text{Beta}\left(\frac{\alpha}{K},1\right)$ for $k=1,\ldots,K$, and note that $T(\Xt{1},\dots,\Xt{K}) = \max(\Xt{1},\dots,\Xt{K})$ is sufficient for $\theta$. 
 Furthermore, $\max(\Xt{1},\dots,\Xt{K}) \sim \theta\cdot\text{Beta}(\alpha, 1)$. Thus, we define $G_t$ to be the conditional distribution of 
 $(\Xt{1},\dots,\Xt{K})$ given $\max(\Xt{1},\dots,\Xt{K}) = t$. Then, 
 by Theorem~\ref{thm:generalized-thinning}, 
 we can thin  $X \sim \theta\cdot\text{Beta}(\alpha,1)$ by 
 sampling from  $G_X$. 


To sample from this conditional distribution, we first   
draw $C\sim\text{Categorical}_K\left(1/K, \ldots, 1/K\right)$. Then, $\Xt{k} = C_kX+(1-C_k)Z_k$ where $Z_k \overset{iid}{\sim} X\cdot\text{Beta}\left(\frac{\alpha}{K},1\right)$.
\end{exmp}

\begin{proof}[Proof of Example~\ref{ex:scaled-beta}]
We must prove the following three claims:
\begin{enumerate}
    \item \emph{If for $k=1,\dots,K$, $\Xt{k} \overset{iid}{\sim} \theta \cdot\text{Beta}\left(\frac{\alpha}{K},1\right)$, then $\max(\Xt{1}, \dots,\Xt{K}) \sim \theta \cdot\text{Beta}\left(\alpha,1\right)$.}  
    
First, note that $\frac{1}{\theta}\Xt{k} \overset{iid}{\sim} \text{Beta}\left(\frac{\alpha}{K},1\right)$. The distribution of $\max(\Xt{1},\dots,\Xt{K})$ can be derived using the CDF method as follows:

\begin{align*}
    P(\max(\Xt{1},\dots,\Xt{K})\le z) &= P(\Xt{1} \le z,\dots, \Xt{K} \le z) \\
    &= \prod_{k=1}^K P(\Xt{k} \le z) \\
    &= \prod_{k=1}^K P\left(\frac{1}{\theta}\Xt{k} \le \frac{z}{\theta}\right) \\
    &= \prod_{k=1}^K \left(\frac{z}{\theta}\right)^{\frac{\alpha}{K}} \\
    &= \left(\frac{z}{\theta}\right)^\alpha,
\end{align*}
where we have used that $P(\text{Beta}(\alpha,1)\le x)=x^\alpha$ for $x\in(0,1)$. 
The above implies that $\max(\Xt{1},\dots,\Xt{K})\sim\theta \cdot\text{Beta}\left(\alpha,1\right)$ as required.
    \item \emph{A sufficient statistic for $\theta$ based on $\Xt{1},\dots,\Xt{K}$ is $\max(\Xt{1},\dots,\Xt{K})$.} 

Using the independence of $\Xt{k}$, the joint distribution can be written as

\begin{align*} 
&\quad f_{\Xt{1},\dots,\Xt{K}}(\xt{1},\dots,\xt{K}) \\
&= \prod_{k=1}^K f_{\Xt{k}}(\xt{k}) \\
&\propto \prod_{k=1}^K\left(\xt{k}\right)^{\frac{\alpha}{K}-1}I\{0<\xt{k}<\theta\} \\
&= \left(\prod_{k=1}^K\xt{k}\right)^{\frac{\alpha}{K}-1}I\{\min(\xt{1},\dots,\xt{K})>0\}I\{\max(\xt{1},\dots,\xt{K})<\theta\}.
\end{align*}

By the factorization theorem, we conclude that $\max(\Xt{1},\dots,\Xt{K})$ is a sufficient statistic for $\theta$ as required.

    \item \emph{We can sample from the conditional distribution $(\Xt{1},\dots,\Xt{K})|\max(\Xt{1},\dots,\Xt{K})=t$ by taking $\Xt{k}=C_kt+(1-C_k)Z_k$ where $C \sim \text{Categorical}_K(1/K,\dots,1/K)$ and $Z_k\sim t\cdot\text{Beta}(\frac{\alpha}{K},1)$.} 

Without loss of generality, consider $\Xt{k}$. Given that $\Xt{1},\dots,\Xt{K}$ are identically distributed, $P(\Xt{k}=t)=\frac{1}{K}$. Hence, in the first stage, we can draw one sample, $C\sim\text{Categorical}_K(1/K,\dots,1/K)$ to determine if $\Xt{k}$ is the maximum. If $\Xt{k}$ is not the maximum then we know that $\Xt{k} \le t$. We can compute the density of $Z_k\overset{D}{=}(\Xt{k}|\Xt{k}\le t)$ as follows,

$$
f_{\Xt{k}|\Xt{k}\le t}(\xt{k}) = \frac{f_{\Xt{k}}(\xt{k})}{P(\Xt{k}\le t)} =\frac{\frac{1}{\theta^\frac{\alpha}{K} B\left(\frac{\alpha}{K},1\right)} \left(\xt{k}\right)^{\frac{\alpha}{K}-1}}{\left(\frac{t}{\theta}\right)^{\frac{\alpha}{K}}} = \frac{1}{t^\frac{\alpha}{K} B\left(\frac{\alpha}{K},1\right)} \left(\xt{k}\right)^{\frac{\alpha}{K}-1}.
$$

The above implies that $Z_k \sim t\cdot\text{Beta}\left(\frac{\alpha}{K},1\right)$ as required.
    
\end{enumerate}
The result then follows from Theorem~\ref{thm:generalized-thinning}.
\end{proof}

\subsubsection{Shifted exponential distribution}
\label{subsec:shifted-exponential-proofs}

We consider $X\sim \mathrm{SExp}(\theta,\lambda)$, which is the location family generated by shifting an exponential random variable by an amount $\theta$.  It has density
$$
p_{\theta,\lambda}(x)=\lambda e^{-\lambda (x-\theta)}1\{x\ge \theta\}.
$$

\begin{exmp}[Thinning a $\mathrm{SExp}(\theta,\lambda)$ random variable with known $\lambda$] \label{ex:shifted-exponential}
We begin with $\Xt{k} \overset{iid}{\sim} \mathrm{SExp}(\theta,\lambda/K)$ for $k=1,\ldots,K$, and note   that $T(\Xt{1},\dots,\Xt{K})=\min(\Xt{1},\dots,\Xt{K})$ is sufficient for $\theta$.  Furthermore,  $\min(\Xt{1},\dots,\Xt{K})\sim \mathrm{SExp}(\theta,\lambda)$. We define $G_t$ to be the conditional distribution of $(\Xt{1},\ldots,\Xt{K})$ given $  \min(\Xt{1},\dots,\Xt{K})=t$.  Then, by Theorem~\ref{thm:generalized-thinning},   we can thin $X\sim \mathrm{SExp}(\theta,\lambda)$  by sampling from $G_X$.


To sample from $G_X$, we first draw  $C\sim\text{Categorical}_K\left(1/K,\ldots,1/K \right)$. We then take 
$\Xt{k}=X+(1-C_k)Z_k$,
where  $Z_k\overset{iid}{\sim} \mathrm{Exp}(\lambda/K)$. 

\end{exmp}

\begin{proof}[Proof of Example~\ref{ex:shifted-exponential}]
We must prove the following three claims:
\begin{enumerate}
    \item \emph{If for $k=1,\dots,K$, $\Xt{k} \overset{iid}{\sim} \text{SExp}\left(\theta,\lambda/K\right)$, then $\min(\Xt{1}, \dots,\Xt{K}) \sim \text{SExp}(\theta,\lambda)$.}  
    
First, note that $\Xt{k} - \theta \overset{iid}{\sim} \text{Exp}\left(\lambda/K\right)$. The distribution of $\min(\Xt{1},\dots,\Xt{K})$ can be derived using the CDF method as follows,

\begin{align*}
    P(\min(\Xt{1},\dots,\Xt{K})\ge z) &= P(\Xt{1} \ge z,\dots, \Xt{K} \ge z) \\
    &= \prod_{k=1}^K P(\Xt{k} \ge z) \\
    &= \prod_{k=1}^K P\left(\Xt{k}-\theta \ge z-\theta\right) \\
    &= \prod_{k=1}^K \exp\left(-\frac{\lambda}{K}(z-\theta)\right) \\
    &= \exp\left(-\lambda(z-\theta)\right).
\end{align*}

The above implies that $\min(\Xt{1},\dots,\Xt{K})\sim\text{SExp}(\theta,\lambda)$ as required.
    \item \emph{A sufficient statistic for $\theta$ based on $\Xt{1},\dots,\Xt{K}$ is $\min(\Xt{1},\dots,\Xt{K})$.} 

Using the independence of $\Xt{k}$, the joint distribution can be written as

\begin{align*} 
&\quad f_{\Xt{1},\dots,\Xt{K}}(\xt{1},\dots,\xt{K}) \\
&= \prod_{k=1}^K f_{\Xt{k}}(\xt{k}) \\
&\propto \prod_{k=1}^K \exp\left(-\frac{\lambda}{K}(\xt{k}-\theta)\right)I\{\xt{k}>\theta\} \\
&\propto \exp\left(-\frac{\lambda}{K}\sum_{k=1}^K\xt{k}\right)I\{\min(\xt{1},\dots,\xt{K})>\theta\}. 
\end{align*}

Given that the joint distribution can be written such that $\theta$ only interacts with the data through the $I\{\min(\xt{1},\dots,\xt{K})>\theta\}$ term, we conclude that $\min(\Xt{1},\dots,\Xt{K})$ is a sufficient statistic for $\theta$ as required.

    \item \emph{We can sample from the conditional distribution $\Xt{1},\dots,\Xt{K}|\min(\Xt{1},\dots,\Xt{K})=t$ by taking $\Xt{k}=t+(1-C_k)Z_k$ where $C \sim \text{Categorical}_K(1/K,\dots,1/K)$ and $Z_k\sim\text{Exp}(\lambda/K)$.} 

Without loss of generality, consider $\Xt{k}$. Given that $\Xt{1},\dots,\Xt{K}$ are identically distributed, $P(\Xt{k}=X)=\frac{1}{K}$. Hence, in the first stage, we can draw one sample, $C\sim\text{Categorical}_K(1/K,\dots,1/K)$ to determine if $\Xt{k}$ is the minimum. Otherwise, we require that $ \Xt{k} \ge t$. We know that the density of $Z_k \overset{D}{=}(\Xt{k}|\Xt{k}\ge t) \overset{D}{=} \Xt{k}$ by the memoryless property of the exponential distribution. Thus, $Z_k \sim \text{Exp}\left(\lambda/K\right)$ as required.
    
\end{enumerate}
The result then follows from Theorem~\ref{thm:generalized-thinning}.

\end{proof}

