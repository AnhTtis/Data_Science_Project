\section{Discussion}
\label{sec:discussion}

Our generalized data thinning proposal encompasses a diverse set of existing approaches for splitting a random variable into  independent random variables, from convolution-closed data thinning \citep{neufeld2023data} to sample splitting \citep{cox1975note}. It provides a lens through which these existing approaches follow from the same simple principle --- sufficiency ---  and can be derived through the same simple recipe (Algorithm~\ref{alg:recipe}).  


The principle of sufficiency is key to generalized data thinning, as it enables a sampling mechanism that does not depend on unknown parameters.  When no sufficient statistic that reduces the data is available, as in the non-parametric setting of Section~\ref{sec:sample-splitting} and the Cauchy example of Section~\ref{subsec:cauchy}, then sample splitting is still possible, provided that the observations are independent and identically distributed. Conversely, in a setting with $n=1$ or where the elements of $X=(X_1,\ldots,X_n)$ are not independent and identically distributed, sample splitting may not be possible, but other  generalized thinning approaches may be available. 

For example, consider a regression setting with a fixed design, in which  each response $Y_i$ has a potentially distinct distribution determined by its corresponding feature vector $x_i$, for $i=1,\ldots,n$.  It is typical to recast this as random pairs $(x_1,Y_1),\ldots,(x_n, Y_n)$ that are independent and identically distributed from some joint distribution, thereby justifying sample splitting.  However, this amounts to viewing the model as arising from a random design, which  may not match the reality of how the design matrix was generated, and may not be well-aligned with the goals of the data analysis.  For instance, recall the example given in the introduction: given a dataset consisting of the  $n=50$ states of the United States, it is unrealistic to treat each state as an independent and identically distributed draw, and undesirable to perform inference only on the states that were ``held out" of training. In this example, generalized data thinning could provide a more suitable alternative to sample splitting that stays true to the fixed design model underlying the data. 

The starting place for any generalized thinning strategy---whether sample splitting or otherwise---is  the assumption that the data are drawn from a distribution belonging to a family $\mathcal P$.  
An important topic of future study is the effect of model misspecification.  In particular, if we falsely assume that $X\sim P_\theta\in\mathcal P$, 
what goes wrong?  The  random variables $\Xt{1},\ldots,\Xt{K}$ generated by thinning will still satisfy the property that $X=T(\Xt{1},\ldots,\Xt{K})$; however, $\Xt{1},\ldots,\Xt{K}$ may not be independent and may no longer have the intended marginals $\Qt{1}_\theta,\ldots,\Qt{K}_\theta$.  Can we quantify the effect of the model misspecification?  I.e., if the true family is ``close" to the assumed family, will  $\Xt{1},\ldots,\Xt{K}$ be only weakly dependent, and will the marginals be close to $\Qt{1}_\theta,\ldots,\Qt{K}_\theta$? Some initial answers to these questions can be found in \cite{neufeld2023data} and \cite{rasines2021splitting}.

In the introduction, we noted that generalized thinning with $K=2$  is a $(U,V)$-decomposition, as defined in \cite{rasines2021splitting}.  We elaborate on that connection here.  The $(U,V)$-decomposition seeks independent random variables $U=u(X,W)$ and $V=v(X,W)$ such that $U$ and $V$ are jointly sufficient for the unknowns, where $W$ is a random variable possibly depending on $X$.  Suppose we can indirectly thin $X$ through $S(\cdot)$ by $T(\cdot)$.  This means we have produced independent random variables $\Xt{1}$ and $\Xt{2}$ for which $S(X)=T(\Xt{1},\Xt{2})$.  Since $S(X)$ is sufficient for $\theta$ on the basis of $X$, this implies that $(\Xt{1},\Xt{2})$ is jointly sufficient for $\theta$.   It follows that $(\Xt{1},\Xt{2})$ is a $(U,V)$-decomposition of $X$.  It is of interest to investigate whether there are $(U,V)$-decompositions that cannot be achieved through either direct or indirect generalized thinning.

In Section~\ref{sec:bernoulli}, we provided an example of a family for which it is impossible to perform (non-trivial) thinning.  In such situations, one may choose to drop the requirement of independence between $\Xt{1}$ and $\Xt{2}$. We expand on this extension in Supplement~\ref{app:fission}.

The data thinning strategies outlined in this paper will be implemented in the \texttt{datathin} R package, available at \url{https://anna-neufeld.github.io/datathin/}.
