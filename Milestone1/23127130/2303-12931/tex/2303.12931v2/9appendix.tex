\section{Proofs} \label{sec:appendix}

\subsection{Proof of Theorem~\ref{thm:generalized-thinning}} \label{app:pf-sufficiency-thinning}

\begin{proof}
By their construction in Definition \ref{def:thinning}, the random variables $(\Xt{1},\ldots,\Xt{K})\sim \Qt{1}_\theta\times\cdots\times\Qt{K}_\theta$ have conditional distribution
$$
(\Xt{1},\ldots,\Xt{K})|\{X=t\}\sim G_t.
$$
Furthermore, Definition~\ref{def:thinning} tells us that $X=T(\Xt{1},\ldots,\Xt{K})$. This means that
$$
(\Xt{1},\ldots,\Xt{K})|\{T(\Xt{1},\ldots,\Xt{K})=t\}\sim G_t,
$$
which establishes part (b) of the theorem.

The distribution $G_t$ in Definition~\ref{def:thinning} does not depend on $\theta$ (note that it is associated with the entire family $\mathcal P$, not a particular distribution $P_\theta$). By the definition of sufficiency, the fact that the conditional distribution $(\Xt{1},\ldots,\Xt{K})|T(\Xt{1},\ldots,\Xt{K})$ does not depend on $\theta$ implies that $T(\Xt{1},\ldots,\Xt{K})$ is sufficient for $\theta$. This proves (a).
\end{proof}


\subsection{Proof of Theorem~\ref{thm:natural-exponential-families}} \label{app:pf-exp-fam}

\begin{proof}
We start by proving the $\impliedby$ direction.
Suppose $H$ is the convolution of $H_1,\ldots, H_K$.  We follow the recipe given in Algorithm~\ref{alg:recipe}:

\begin{enumerate}
    \item We choose $\cQt{k}=\mathcal P^{H_k}$ for $k=1,\ldots,K$.
    \item Let $(\Xt{1},\ldots,\Xt{K})\sim P^{H_1}_\theta\times \cdots\times P^{H_K}_\theta$.  This joint distribution satisfies
    $$
    \prod_{k=1}^KdP^{H_k}_\theta(\xt{k})=\exp\left\{\left(\sum_{k=1}^K\xt{k}\right)^\top\theta-\sum_{k=1}^K\psi_{H_k}(\theta)\right\}\prod_{k=1}^KdH_k(\xt{k}).
    $$
    By the factorization theorem, we find that $T(\Xt{1},\ldots,\Xt{K})=\sum_{k=1}^K\Xt{k}$ is sufficient for $\theta$.
    \item It remains to determine the distribution of $U=T(\Xt{1},\ldots,\Xt{K})$.  This random variable is the convolution of $P^{H_1}_\theta\times \cdots\times P^{H_K}_\theta$, and its distribution $\mu$ is defined by the following $K$-way integral:
    \begin{align*}
d\mu(u)&=\int\cdots\int1\left\{\sum_{k=1}^K\xt{k}=u\right\}\prod_{k=1}^KdP^{H_k}_\theta(\xt{k})\\
&=\exp\left\{u^\top\theta-\sum_{k=1}^K\psi_{H_k}(\theta)\right\}\int\cdots\int1\left\{\sum_{k=1}^K\xt{k}=u\right\}\prod_{k=1}^KdH_k(\xt{k})\\
&=\exp\left\{u^\top\theta-\psi_{H}(\theta)\right\}dH(u)\\
&=dP^H_\theta(u),
    \end{align*}
where in the second-to-last equality we use the assumption that $H$ is the $K$-way convolution of $H_1,\ldots,H_K$ and the fact that the moment generating function of a convolution is the product of the individual moment generating functions (and recalling that $\psi_H$ is the logarithm of the moment generating function of $H$). This establishes that $T(\Xt{1},\ldots,\Xt{K})\sim P^H_\theta$. By Theorem~\ref{thm:generalized-thinning}, the family $\mathcal P^H$ is thinned by this choice of $T(\cdot)$.
\end{enumerate}

We now prove the $\implies$ direction.
Suppose that $\mathcal P^H$ can be $K$-way thinned into $\mathcal P^{H_1},\ldots, \mathcal P^{H_K}$ using the summation function.  Then applying Definition~\ref{def:thinning} with $\theta=0$, we can take $X\sim P^{H}_0$ and produce $(\Xt{1},\ldots,\Xt{K})\sim P^{H_1}_0\times\cdots\times P^{H_K}_0$ for which $X=\Xt{1}+\cdots+\Xt{K}$.  Noting that $P^{H_k}_0=H_k$ for all $k$ and $P^{H}_0=H$, this proves that $H$ is a $K$-way convolution of $H_1,\ldots,H_K$. 
\end{proof}


\subsection{Proof of Theorem~\ref{thm:backwards-natexpfam}}
\label{app:pf-backwards}

\begin{proof}
Suppose that $X\sim P_\theta$ is a natural exponential family with $d$-dimensional parameter $\theta$ that can be thinned by $T(\cdot)$ into $\Xt{k}\overset{ind}\sim Q_\theta^{(k)}$ for $k=1,\dots K$. By Theorem \ref{thm:generalized-thinning}, $T(\Xt{1},\dots,\Xt{K})$ is a sufficient statistic for $\theta$ on the basis of $\Xt{1},\dots,\Xt{K}$, which implies that the conditional distribution $(\Xt{1},\dots,\Xt{K})|T(\Xt{1},\dots,\Xt{K})=t$ does not depend on $\theta$. We can write the conditional density with respect to the appropriate dominating measure as
\begin{align*}
& f_{\Xt{1},\dots,\Xt{K}|T(\Xt{1},\dots,\Xt{K})=t}(\xt{1},\dots,\xt{K}) \\ 
&= \frac{f_{\Xt{1},\dots,\Xt{K}}(\xt{1},\dots,\xt{K})1\{T(\xt{1},\dots,\xt{K})=t\}}{f_{T(\Xt{1},\dots,\Xt{K})}(t)} \\
&= \frac{q^{(1)}_\theta(\xt{1})\dots q^{(K)}_\theta(\xt{K}) 1\{T(\xt{1},\dots,\xt{K})=t\}}{\exp(T(\xt{1},\dots,\xt{K})^\top\theta - \psi(\theta))h(T(\xt{1},\dots,\xt{K}))} \\
&= \frac{\prod_{k=1}^Kq^{(k)}_\theta(\xt{k})}{\exp(T(\xt{1},\dots,\xt{K})^\top\theta - \psi(\theta))} \cdot \frac{ 1\{T(\xt{1},\dots,\xt{K})=t\}}{h(T(\xt{1},\dots,\xt{K}))},
\end{align*}
where in the second equality, we used that $T(\Xt{1},\dots,\Xt{K})=X\sim P_\theta$.

As this distribution cannot depend on $\theta$, the first fraction must be constant in $\theta$. That is, for any fixed $\theta_0\in\Omega$,
\begin{align}\label{eq:qtheta}
&\frac{\prod_{k=1}^Kq^{(k)}_\theta(\xt{k})}{\exp(T(\xt{1},\dots,\xt{K})^\top\theta - \psi(\theta))} = \frac{\prod_{k=1}^Kq^{(k)}_{\theta_0}(\xt{k})}{\exp(T(\xt{1},\dots,\xt{K})^\top\theta_0 - \psi(\theta_0))}  \nonumber\\
\iff&\prod_{k=1}^K \frac{q^{(k)}_\theta(\xt{k})}{q^{(k)}_{\theta_0}(\xt{k})} = \exp(T(\xt{1},\dots,\xt{K})^\top(\theta-\theta_0) - (\psi(\theta) - \psi(\theta_0))) \nonumber\\
\iff&T(\xt{1},\dots,\xt{K})^\top(\theta-\theta_0)  = \sum_{k=1}^K \left[\log q^{(k)}_\theta(\xt{k}) + \frac{1}{K}\psi(\theta) - \log q^{(k)}_{\theta_0}(\xt{k}) - \frac{1}{K}\psi(\theta_0)\right]. 
\end{align}

To proceed, we must first confirm that the term inside the summation on the right-hand side is linear in $\theta-\theta_0$. To see this, observe that if we replace $\xt{1}$ with $\tilde{x}^{(1)}$, then 
\begin{align*}
&\left[T(\xt{1},\xt{2},\dots,\xt{K})- T(\tilde{x}^{(1)},\xt{2},\dots,\xt{K})\right]^\top(\theta-\theta_0) \\
&= \log q^{(1)}_\theta(\xt{1}) - \log q^{(1)}_{\theta_0}(\xt{1}) - \log q^{(1)}_\theta(\tilde{x}^{(1)}) + \log q^{(1)}_{\theta_0}(\tilde{x}^{(1)}) \\
&= a^{(1)}(\xt{1},\theta) - a^{(1)}(\xt{1},\theta_0) - a^{(1)}(\tilde{x}^{(1)},\theta) + a^{(1)}(\tilde{x}^{(1)},\theta_0)
\end{align*}
where $a^{(1)}(x,\theta)=\log q_\theta^{(1)}(x)$. Since the initial expression in the previous string of equalities is linear in $\theta$, the same must be true for the final expression, implying that $a^{(1)}$ must be of the form 
$$
a^{(1)}(x,\theta) = T^{(1)}(x)^\top\theta + f^{(1)}(x) + g^{(1)}(\theta) 
$$
for some functions $T^{(1)}(\cdot)$, $f^{(1)}(\cdot)$, and $g^{(1)}(\cdot)$.

Substituting into the above, 
$$
\left[T(\xt{1},\xt{2},\dots,\xt{K})- T(\tilde{x}^{(1)},\xt{2},\dots,\xt{K})\right]^\top(\theta-\theta_0)
= \left[T^{(1)}(\xt{1})-T^{(1)}(\tilde{x}^{(1)})\right]^\top(\theta-\theta_0).
$$

Applying the same logic to every $k=1,\dots,K$ in sequence, we have that for any $k$,
\begin{align*}
&\left[T(\tilde{x}^{(1)},\dots,\tilde{x}^{(k-1)},\xt{k},\xt{k+1},\dots,\xt{K})- T(\tilde{x}^{(1)},\dots,\tilde{x}^{(k-1)},\tilde{x}^{(k)},\xt{k+1},\dots,\xt{K})\right]^\top(\theta-\theta_0) \\
&= \left[T^{(k)}(\xt{k})-T^{(k)}(\tilde{x}^{(k)})\right]^\top(\theta-\theta_0)
\end{align*}
for some function $T^{(k)}(\cdot)$.

Summing over $k=1,\dots,K$ then yields
$$
\left[T(\xt{1},\dots,\xt{K})- T(\tilde{x}^{(1)},\dots,\tilde{x}^{(K)})\right]^\top(\theta-\theta_0) 
= \left[\sum_{k=1}^KT^{(k)}(\xt{k})-\sum_{k=1}^KT^{(k)}(\tilde{x}^{(k)})\right]^\top(\theta-\theta_0).
$$

Since $\mathcal{P}=\{P_\theta:\theta\in\Omega\}$ is a $d$-dimensional full-rank natural exponential family, there exists a $\theta_0\in\Omega$ and $\epsilon>0$ such that $\theta = \theta_0 + \epsilon v\in\Omega$ for every $v\in\mathbb{R}^d$ such that $\|v\|_2=1$. 

Since the previous display is true for every pair of $\theta$ and $\theta_0$, selecting pairs such that $\theta-\theta_0=\epsilon v$ simplifies the above into
$$
\left[T(\xt{1},\dots,\xt{K})- T(\tilde{x}^{(1)},\dots,\tilde{x}^{(K)})\right]^\top v 
= \left[\sum_{k=1}^KT^{(k)}(\xt{k})-\sum_{k=1}^KT^{(k)}(\tilde{x}^{(k)})\right]^\top v.
$$

As the above holds for all $v\in\mathbb{R}^d$ such that $||v||_2=1$, restricting our attention to the standard basis vectors implies that 
$$
T(\xt{1},\dots,\xt{K})- T(\tilde{x}^{(1)},\dots,\tilde{x}^{(K)})
= \sum_{k=1}^KT^{(k)}(\xt{k})-\sum_{k=1}^KT^{(k)}(\tilde{x}^{(k)})
$$

and furthermore that 
$$
T(\xt{1},\dots,\xt{K})
= \sum_{k=1}^KT^{(k)}(\xt{k})+c.
$$


Without loss of generality, $c$ can be absorbed into the $T^{(k)}(\cdot)$ functions, thus proving the claim that if a natural exponential family can be thinned, then the function $T(\cdot)$ must be a summation of the form $T(\Xt{1},\dots,\Xt{K})=\sum_{k=1}^K T^{(k)}(\Xt{K})$ for some functions $T^{(k)}(\cdot)$ for $k=1,\dots,K$.

Finally, plugging this expression into \eqref{eq:qtheta} gives
$$
\sum_{k=1}^KT^{(k)}(\xt{k})^\top(\theta-\theta_0)  = \sum_{k=1}^K \left[\log q^{(k)}_\theta(\xt{k}) + \frac{1}{K}\psi(\theta) - \log q^{(k)}_{\theta_0}(\xt{k}) - \frac{1}{K}\psi(\theta_0)\right], 
$$
which shows that the functions $q_\theta^{(k)}(\cdot)$ can be characterised as
\begin{align*}
&T^{(k)}(\xt{k})^\top (\theta-\theta_0) = \log q^{(k)}_\theta(\xt{k}) + \frac{1}{K}\psi(\theta) - \log q^{(k)}_{\theta_0}(\xt{k}) - \frac{1}{K}\psi(\theta_0) \\
\iff&\log q^{(k)}_\theta(\xt{k}) = T^{(k)}(\xt{k})^\top (\theta-\theta_0) - \frac{1}{K}\psi(\theta) + \log q^{(k)}_{\theta_0}(\xt{k}) + \frac{1}{K}\psi(\theta_0) \\
\iff& q^{(k)}_\theta(\xt{k}) =  q^{(k)}_{\theta_0}(\xt{k})\exp\left(T^{(k)}(\xt{k})^\top (\theta-\theta_0) - \frac{1}{K}\psi(\theta) + \frac{1}{K}\psi(\theta_0)\right).
\end{align*}

Thus, $q_\theta^{(k)}(\cdot)$ is the density of an exponential family with sufficient statistic $T^{(k)}(\cdot)$ and carrier density given by $h^{(k)}(\xt{k})\propto q^{(k)}_{\theta_0}(\xt{k})\exp(-T^{(k)}(\xt{k})^\top \theta_0)$.

\end{proof}


\subsection{Proof of Proposition~\ref{prop:thin-S}} \label{app:pf-thin-S}

\begin{proof}
The result follows from a chain of equalities:
\begin{align*}
    I_X(\theta)&=I_{S(X)}(\theta)\\
    &=I_{T(\Xt{1},\ldots,\Xt{K})}(\theta)\\
    &=I_{(\Xt{1},\ldots,\Xt{K})}(\theta)\\
    &=\sum_{k=1}^K I_{\Xt{k}}(\theta).
\end{align*}
The first equality is true because $S(X)$ is sufficient for $\theta$ based on $X$.  The second equality follows from the definition of thinning $S(X)$ into $\Xt{1},\ldots,\Xt{K}$ using $T(\cdot)$.  The third equality follows from Theorem~\ref{thm:generalized-thinning}, which tells us that $T(\Xt{1},\ldots,\Xt{K})$ is sufficient for $\theta$ based on $(\Xt{1},\ldots,\Xt{K})$. The final equality follows from independence.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:bernoulli}} \label{app:bernoulli-convolution}

\begin{proof}
We begin by providing some intuition. Since $Z^{(1)}$ and $Z^{(2)}$ are non-constant random variables, their supports each must contain more than one element. Therefore, by independence, the support of $Z^{(1)} + Z^{(2)}$ must contain more than two elements and thus cannot be Bernoulli.

Formally, let $\Qt{1}$ and $\Qt{2}$ be the distributions of $Z^{(1)}$ and $Z^{(2)}$, respectively.
If $Z^{(1)}+Z^{(2)}$ were Bernoulli, then
\begin{align*}
    1&=\mathbb P(Z^{(1)}+Z^{(2)}\in\{0,1\})\\
    &=\int\mathbb P(Z^{(2)}\in\{0-z^{(1)},~1-z^{(1)}\}|Z^{(1)}=z^{(1)})d\Qt{1}(z^{(1)})\\
    &=\int\mathbb P(Z^{(2)}\in\{0-z^{(1)},~1-z^{(1)}\})d\Qt{1}(z^{(1)}),
\end{align*}
where the last equality follows by independence of $Z^{(1)}$ and $Z^{(2)}$.  
For this integral to equal 1, we would need
\begin{equation}\label{eq:prob1}
    \mathbb P(Z^{(2)}\in\{0-z^{(1)},~1-z^{(1)}\})=1 \text{ for $\Qt{1}$-almost every $z^{(1)}$}
\end{equation}
since $P(Z^{(2)}\in\{0-z^{(1)},~1-z^{(1)}\})$ is bounded above by $1$. For $Z^{(1)}$ to be non-constant, there must be at least two distinct points $a$ and $b$ such that \eqref{eq:prob1} holds with $z^{(1)}=a$ and holds with $z^{(1)}=b$.  Since the intersection of two probability 1 sets is a set that holds with probability 1, we have that 
$$
\mathbb P(Z^{(2)}\in\{-a,~1-a\}\cap\{-b,~1-b\})=1,
$$
from which it follows that $\{-a,~1-a\}\cap\{-b,~1-b\}$ is non-empty.  However, there is no choice of $a\neq b$ for which this intersection has more than one element (which is required for $Z^{(2)}$ to be non-constant).  Thus we arrive at a contradiction.

\end{proof}

\subsection{Proof of Corollary~\ref{cor:bern2}} \label{app:bern2}

\begin{proof}
Since the Bernoulli family is a natural exponential family, if at least one of the conclusions of Theorem $\ref{thm:backwards-natexpfam}$ is always false for the Bernoulli, then the contrapositive of Theorem $\ref{thm:backwards-natexpfam}$ will imply that the Bernoulli distribution cannot be thinned by any function $T(\cdot)$. 

Suppose that $X\sim\text{Bernoulli}(\theta)$. Consider the first conclusion, namely that the thinning function $T(\xt{1},\dots,\xt{K})$ is of the form $\sum_{k=1}^KT^{(k)}(\xt{k})$. This would imply that $\sum_{k=1}^KT^{(k)}(\Xt{k})=X\sim\text{Bernoulli}(\theta)$.  However, by Theorem \ref{thm:bernoulli}, $T(\cdot)$ cannot be a convolution of independent, non-constant random variables. Therefore, the second conclusion can only be true if $\Xt{1},\dots,\Xt{K}$ are not mutually independent, some or all of $\Xt{1},\dots,\Xt{K}$ are constant, or some or all of $T^{(1)}(\cdot),\dots,T^{(K)}(\cdot)$ are constant functions. All three cases violate the second conclusion of Theorem \ref{thm:backwards-natexpfam} that $\Xt{k}$ are independent exponential families. Therefore, both conclusions of Theorem \ref{thm:backwards-natexpfam} cannot be simultaneously true, thus proving the claim. 
\end{proof}


