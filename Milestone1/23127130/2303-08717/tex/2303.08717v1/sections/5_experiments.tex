\section{Experiments}\label{sec:experiments}
\vspace{-0.2cm}
\input{results/0_hdw_specs_tb}
\input{results/desktop}

% \subsection{Datasets}
%Next, we conduct a comprehensive empirical study on the capabilities of \methodname by analyzing its performance on multiple scenes and on a variety of devices.
Next, we conduct an extensive study of \methodname performance on multiple scenes and devices.
\vspace{-0.5cm}
\paragraph{Datasets.}
We experiment on both synthetic and real data by using two standard datasets:
\vspace{-0.2cm}
\begin{itemize}
\item \textbf{Synthetic 360° dataset}~\cite{barron2022mip}, eight synthetic scenes with intricate geometries and non-Lambertian materials.
Each scene has 100 views for training and 200 views for testing, both at a resolution of $800$px$\times800$px.
\vspace{-0.2cm}
\item \textbf{Tanks and Temples (T\&T) dataset}~\cite{tandt}, an unbounded dataset consisting of hand-held 360° captures of four large-scale scenes. 
%with camera poses estimated using COLMAP~\cite{colmap}.
We use the dataset configuration of~\cite{nerf++}.
%The majority of the background content is covered by the unit sphere by normalizing such that all cameras are inside the sphere of radius $\nicefrac{1}{8}$. 
% $\displaystyle \frac{1}{8}$
% \item \textbf{Unbounded 360° dataset}~\cite{mildenhall2021nerf}, which includes indoor and outdoor scenes displaying a central object or area and a detailed background.
% Each scene has between 100 and 330 views, whose camera poses were estimated by COLMAP SfM~\cite{colmap}.
% Following~\cite{chen2022mobilenerf}, we use five scenes from this dataset. % (Bicycle Flower Garden Stump Treehill)
\vspace{-0.5cm}
\end{itemize}


\paragraph{Devices.}
Our main goal is to test our method on hardware-constrained devices such as mobile phones, tablets, and VR headsets. 
Nonetheless, for completeness, we also test \methodname on more powerful laptops and desktops. 
In total, we test \methodname on seven devices reported in \Table{0_hdw_specs_tb}. 
\vspace{-0.1cm}



\subsection{Implementation details} 
\vspace{-0.1cm}
% Here we provide full details of our implementation.

\noindent\textbf{Pre-trained NeRFs.}
For the pre-trained NeRF models $R$, we use MipNeRF~\cite{barron2021mipnerf} (in the Synthetic 360° dataset), % MipNeRF-360~\cite{mildenhall2021nerf} (in the Unbounded 360° dataset), 
and NeRF++~\cite{nerf++} (in the T\&T dataset). 

\noindent\textbf{Meshing.}
Obtaining the mesh $\chi$ from a pre-trained NeRF requires distilling the learnt geometry. %  learnt by the NeRF.
% sampling the densities learned by $R$ on a volumetric grid of side $K$.
For the Synthetic 360° dataset, we run Marching Cubes~\cite{lorensen1987marching} on a density grid of side $K = 256$, except for the \textit{ficus} object, in which we use $K = 512$ to capture finer geometric details.
For the T\&T dataset, we use a grid with $K = 512$.
% For the Unbounded 360° dataset, we follow~\cite{nerfstudio}, and run Marching Cubes on a Truncated Signed Distance Function, constructed from Mip-NeRF-360's depth maps.
We remove small connected components resulting from noisy estimates, and decimate the meshes to around $400$k faces. % a maximum of $250$k triangles.
Finally, we enclose unbounded scenes within a dome. %  semi-sphere of radius $1$ and add an approximate ground plane in order to obtain a closed mesh.

\input{imgs/2_SynQualitative}

\noindent\textbf{Pseudo-Image Generation.}
Training the Factorized NeLF via Eq.~\eqref{eq:fnelf_loss} requires the set $\mathcal{I}_{\text{pseu}}$ of pseudo-images.
% Once a rough mesh has been extracted from the NeRF model, we render the set of pseudo-images $\mathcal{I}_{\text{pseu}}$ which will be used for training \methodname.
We obtain these images by using the NeRF to render $10$k images from random camera poses for each scene. % , and using each scene's NeRF  to perform rendering.
% We render $10$k images for all scenes. 
% , except for \textit{ficus} and \textit{mic}, for which we render $14$k images. % for ficus and mic and $10K$ for all other objects with randomly sampled camera poses.
% We acquire the NeLF pseudo data for training by randomly sampling the camera positions and orientations from the trained NeRF .

\noindent\textbf{Factorized NeLF Training.}
We implement the position- and direction-dependent MLPs ($L_{\text{pos}}$ and $L_{\text{dir}}$) by following~\cite{r2l}, and thus employ intensive residual blocks~\cite{resnet} and deep architectures of $88$ layers. %  and deep MLPs for each .
We train these MLPs with hard-ray sampling~\cite{r2l} and learning rate warm-up strategies.
We train with a batch size of $200$k rays for 2.5 days on one NVIDIA A100 GPU.

\noindent\textbf{Baking Light Field Embeddings.}
For baking $\boldsymbol\beta$, we uniformly sample elevation and azimuth angles in $[0$-$180^{\circ}]$ and $[0$-$360^{\circ}]$, respectively. % on the surface of a sphere.
We use $1024$ samples for synthetic scenes and $2048$ for real scenes.
When quantizing $\mathbf{u}$, $\mathbf{v}$, $\mathbf{w}$, and $\boldsymbol\beta$ for their storage as PNG files, we perform per-channel min-max normalization.
% It takes approximately 50 mins to generate all textures.
Unless otherwise stated, we use $18 = \lceil\nicefrac{6\times6}{2}\rceil$ texels per triangle face. 
% \sara{yes or not?}



\noindent\textbf{Rendering with Shaders.}
Since \methodname requires no MLP queries, we implement it in a simple fragment shader.
This implementation allows deploying \methodname across not only various devices, but also different graphics frameworks. 
Notably, this implementation allows us to deploy on a VR headset (Meta Quest Pro), which runs Unity shaders.
% This combination is computed in the vertex shader.
The shader computes color by combining the position- and direction-dependent embeddings according to Eq.~\eqref{eq:factorized-nelf}, where each embedding is queried from % $\mathbf{u}$, $\mathbf{v}$, $\mathbf{w}$, and $\boldsymbol\beta$ in 
its corresponding texture map.
In turn, each texture map is a $2\times4$ grid stored in a $4$-channeled PNG image, %  (\ie the PNG), 
thus fully accounting for our default embedding dimension $D = 32 = 2\times4\times4$.
We obtain the position embeddings by indexing $\mathbf{M}_{\mathbf{u},\mathbf{v},\mathbf{w}}$ with the fragment's $uv$ coordinates, while the direction embeddings are obtained by indexing $\mathbf{M}_{\boldsymbol\beta}$ with the azimuth and elevation angles.
Since texture values are 8-bit quantized, we map to the original range by reverting the per-channel min-max normalization. %  to approximate the original values ($32$ values for each $\mathbf{u}, \mathbf{v}, \mathbf{w},$ and $\boldsymbol\beta$).
% We compute the fragment's color as the dot product of $(\mathbf{u}, \mathbf{v}, \mathbf{w})$ and $\boldsymbol\beta$, followed by a sigmoid function.
% Our use of light fields allows use to dispose of alpha compositing, and thus we set alpha values to~$1$. 
We provide a full implementation of our shaders in the \SM.

\input{imgs/3_RealQualitative} 
\subsection{Main Results}
We compare against two methods intended for fast rendering, SNeRG~\cite{hedman2021snerg} and MobileNeRF~\cite{chen2022mobilenerf}. % with the capacity to render in real-time on devices. % , in \Table{1_Rendering_speed_tb}.
\vspace{-0.5cm}
\paragraph{Quantitative Results.}
The first three rows of \Table{desktop} show how \methodname can achieve real-time rendering when deployed on a mobile.
For both datasets, we find that, while SNeRG achieves reasonable disk usage and photo-metric quality, it is ultimately incapable of interactive rendering.
\underline{On the Synthetic 360° dataset}, we find that \methodname can render at over 54 FPS, outperforming Mobile-NeRF by more than 30\%.
Furthermore, such fast rendering speed comes at a negligible drop in performance of $\sim$1 dB. %  with respect to Mobile-NeRF.
Moreover, these benefits over MobileNeRF are accompanied by simpler meshes (\ie faces and vertex quantity).
% Furthermore, 
\underline{On the realistic Unbounded 360° dataset}, our method outperforms competing approaches by even larger margins than on synthetic data.
In particular, we improve upon MobileNeRF \textit{both} in rendering quality and speed.
That is, while SNeRG and MobileNeRF achieve a PSNR of 14.0 and 15.6, respectively, \methodname attains 17.9.
Impressively, this outstanding gain in photo-metric quality of over 2 dB is accompanied by an also impressive superiority in rendering speed of over 10 FPS over MobileNeRF~(22.79 \textit{vs.} 33.46 FPS).
% Once again, \methodname's superiority over MobileNeRF come with the added benefits of simpler, \ie smaller, meshes.
% From the methods we mention here, only \methodname and MobileNeRF use polygons to model scenes.
% As such, the size of the meshes used by these methods has an impact on their cost, storage and performance.
% We underscore how, despite using substantially smaller meshes than MobileNeRF to model real scenes, \methodname still outperforms this competitor by significant margins both in speed. %  (\Table{1_Rendering_speed_tb}) and quality (\Table{desktop}).
% The gains we achieve can be attributed to our light field-based formulation, that allows us to work with smaller meshes that thus possess smaller expressive power.
The gains we achieve can be attributed to our light field-based formulation, that allows us to work with smaller meshes while still maintaining high directional expressivity.
% \vspace{-0.1cm}
% The last seven rows of \Table{desktop} report an upper-bound to the performance of all methods, by evaluating on a desktop computer at a higher resolution of 8k$\times$4k pixels.
The bottom rows of \Table{desktop} report the performance of all methods in a non-constrained scenario, which is a desktop.
%, by evaluating on a desktop computer at a higher resolution of 8k$\times$4k pixels.
When provided with such larger computational resources, our method's speed improves both in absolute and relative terms.
Specifically, while the gap in mobiles w.r.t. MobileNeRF was $\sim$30\% (on the Synthetic dataset), this gap jumps to over 460\% on the desktop.
This increased gap is even more pronounced in the Unbounded dataset: the gap in mobiles was $\sim$45\% (from 22 FPS to 33), while the gap in desktop becomes over 1,500\%.
The larger computational resources also allow us to test a larger version of \methodname, which achieves remarkable PSNRs of 30.1 and 18.0 in the Synthetic and Unbounded datasets, respectively, while also remaining much faster than MobileNeRF.
The last three rows of \Table{desktop} report foundational methods (\ie NeRF, Mip-NeRF and NeRF++) as reference of high quality, although these methods are inefficient for rendering.
\vspace{-0.5cm}

\paragraph{Qualitative Results.} We next compare \methodname performance against the other real-time rendering methods. 
%We can gain a better understanding of rendering quality by looking at qualitative results of \methodname compared to other methods for real-time NeRF rendering. 
We show 
%several of these 
results for both synthetic (\Figure{SyntheticQualitative}) and real scenes (\Figure{RealQualitative}).
Note \methodname higher quality, particularly in real scenes, where our method preserves sharper object boundaries, and crisper surface details. 
\methodname allows real-time rendering while preserving strong image quality.



\input{results/1_Rendering_speed_tb}

\subsection{Rendering Speed on Devices}
% This table reports render speed in frames-per-second (FPS), % GPU memory consumption and disk storage (MB), and PSNR for assessing render quality. 
% compare against methods with capabilities of real-time rendering on devices, mainly MobileNeRF~\cite{chen2022mobilenerf} and SNeRG~\cite{hedman2021snerg}. 
% Here, rendering speed was measured 
We measure frames-per-second (FPS) in seven devices~(via the same procedure as~\cite{chen2022mobilenerf}), and report results in \Table{1_Rendering_speed_tb}.
Our results illustrate how \methodname significantly outperforms other methods in rendering speed. 
The advantages in performance are particularly large in unbounded scenes. 
In this scenario, \methodname is, on average $2.6\times$ faster than MobileNeRF. %  while maintaining higher quality.
% Namely, in this scenario, \methodname is, on average $2.6\times$ faster than MobileNeRF. %  while maintaining higher quality.
For synthetic scenes, our method provides sizable speed gains of 35\%. %  still come only with a minor loss in quality for synthetic scenes.
Importantly, we find that \methodname is capable of real-time rendering in a VR headset even reaching the device's limit of 74 FPS. %  (\Table{1_Rendering_speed_tb}). 
% Furthermore, speed gains come with minor to no loss in quality, as evidenced by the PSNR row. %  in \Table{1_Rendering_speed_tb}. 
% Furthermore, speed gains come only with a minor loss in quality for synthetic scenes as evidenced by the PSNR. %  in \Table{1_Rendering_speed_tb}. 
% \methodname achieves these results while requiring less dense meshes (\Table{4_Polygon_count_tb}).

% Notably, this implementation allows us to deploy our method on a VR headset (Meta Quest Pro), which runs Unity shaders. 
% For both synthetic and real scenes, we find that \methodname is capable of rendering in the headset in real time, even reaching the device's capped limit of 74 FPS (\Table{1_Rendering_speed_tb}). 
% The flexibility of our framework allows us to easily deploy on any device that supports a standard graphics pipeline.
\begin{comment}
\sara{Update}  \subsection{Rendering quality}

\input{results/3_Quantitative_tb}
% \input{results/4_Polygon_count_tb}

We compare the rendering quality of \methodname against other methods via common quantitative measures (PSNR, SSIM~\cite{SSIM} and LPIPS~\cite{LPIPS}). %  commonly used for testing image quality. 
We report this comparison in \Table{3_Quantitative_tb}. % , along with equivalent ones for several NeRF methods. 
When compared to more complex NeRF architectures, \methodname could be expected to suffer small declines in quality. 
However, as \Table{3_Quantitative_tb} illustrates, our method still achieves comparable performance. 
We argue that, given the substantial gains in rendering speed provided by our method, the magnitude of these drops is a manageable. 

We can gain a better understanding of rendering quality by looking at qualitative results of \methodname compared to other methods for real-time NeRF rendering. 
We show several of these results for both synthetic \sara{Update} (\Figure{SyntheticQualitative}) and real scenes (\Figure{RealQualitative}).
We can observe the higher quality of \methodname renderings, particularly in real scenes, where our method manages to preserve sharper object boundaries, and crispier surface details. 
\methodname not only allows fast real-time rendering, but also preserves strong image quality.

From the methods we mention here, only our \methodname and MobileNeRF use polygons to model scenes.
As such, the size of the meshes used by these methods has an impact on their cost, storage and performance.
\sara{Update} \Table{4_Polygon_count_tb} compares the sizes of the meshes used by these methods.
Notably, our method uses smaller meshes, and thus is more economical for storage. %  is more economical in mesh requirements size. 
We underscore how, despite using substantially smaller meshes than MobileNeRF to model real scenes, \methodname still outperforms this competitor by significant margins both in speed (\Table{1_Rendering_speed_tb}) and quality (\Table{3_Quantitative_tb}).
The gains we achieve can be attributed to our light field-based formulation, that allows us to work with smaller meshes that thus possess smaller expressive power.
\end{comment}

\subsection{Quality \textit{vs.} Representation Size}
Two main factors affect the size of our representation: the texel count and the dimensionality of the light field embeddings ($D$ in Eq.~\eqref{eq:dir-pos}).
Here we study how these factors, in turn, affect the photo-metric quality of \methodname.
% The amount of texels tells us how big or small the image applied as a texture is.
\underline{Texel count.} 
We examine the effect that varying the number of texels assigned to each face in the mesh has on rendering quality and disk space. %  by adjusting the number of texels assigned to each triangle face. 
\Figure{ablation} (left) and \Figure{ablation2} show that, for both datasets, an increased texel count is accompanied by a drastic rise in quality of the reconstruction and disk space. %  for both datasets. 
% The disk space for synthetic scenes is lower than that of real scenes, due to a lower average number of triangle faces per scene.
\underline{Embedding dimensionality.} 
We now examine the effect that varying the dimensionality of the light field embeddings has on the ability to represent light effects.
% \Figure{ablation} (right) reports reflectance maps for various dimensionalities.
% With respect to light effects, our results suggest that the capacity of \methodname to model challenging view-dependent effects, such as lighting, is rather robust, providing reasonable performance even with low dimensionalities.
% \methodname is capable of modeling challenging view-dependent effects even with low dimensionality embeddings, but larger embeddings allow it to model more complex reflections.
Reflectance maps in \Figure{ablation} (right) demonstrate that \methodname can model challenging view-dependent effects even with low-dimensional embeddings, but larger embeddings enable more complex reflections. 
Together, our experiments demostrate that exchanging disk space for renderings of higher quality, and suggest that increasing the texel count can improve rendering quality.


\input{imgs/4_Ablation.tex}
\input{imgs/5_AblaQuali.tex}
\subsection{Compositional scenes}

In \Figure{app}, we showcase the practical application of \methodname for scene composition.
Our approach enables efficient rendering of 2500 materials and ficus objects in single scenes at 130 FPS each on a desktop.
Additionally, we demonstrate an AR application that uses an AR/VR headset to render four chairs in real-time in a real-world setting.
% This exemplifies our approach's ability to seamlessly blend virtual and real environments, creating an immersive visual experience for users.
% Our method's effectiveness in this context unlocks new possibilities for powerful applications.
This exemplifies our approach's ability to seamlessly blend virtual and real environments, unlocking new possibilities for immersive visual experiences.
\input{imgs/applications.tex}

% \input{imgs/meshes.tex}

% \subsection{Additional Results}
%\vspace{2pt}\noindent\textbf{View-dependent Effects.}
%To study the impact of viewpoint in \methodname we report reflectance maps in Figure~\ref{fig:reflectance_maps}. \albert{I guess this should go now, it is in the main paper}
% \input{imgs/6_ReflectanceMaps}
\textbf{Additional Results.} Finally, we report detailed qualitative and quantitative results, validation of view-dependent effects, sensitivity to geometry variations and photo-metric quality depending on the dimensionality $D$ of \methodname in the \SM.

\begin{comment}
\vspace{2pt}\noindent\textbf{Qualitative and Quantitative Results.}
Due to space constraints, we comprehensively report qualitative and quantitative results in the Appendix.

\vspace{2pt}\noindent\textbf{Comparison against RGB-textured mesh.}
Our approach is essentially a mesh representation whose texture can model view-dependent effects.
Thus, for completeness, we compare \methodname against a naive RGB-texturized mesh in the Appendix.

\vspace{2pt}\noindent\textbf{Sensitivity to geometry variations.}
For a fixed mesh $\chi$, \methodname distills the color information of the scene into a NeLF.
In the paper, we construct $\chi$ to accurately approximate the scene's geometry.
In the Appendix, we explore how degraded geometries affect the photo-metric quality achieved by \methodname.

\vspace{2pt}\noindent\textbf{Sensitivity to embedding size.}
In the Appendix, we report how photo-metric quality is affected by the dimensionality $D$ of the light field embeddings, as defined in Eq.~\eqref{eq:dir-pos}.

\end{comment}

% \input{results/performance}
% \begin{itemize}
%     \item We use MipNerF for  Realistic Synthetic 360° dataset and Nerf++ for 360° Unbounded Tanks and Temples dataset.
%     \item We sample 256x256x256 density points for Realistic Synthetic 360° dataset except ficus (512x512x512) and 512x512x512 for for 360° Unbounded Tanks and Temples dataset
%     \item We decimated all scene up to 250.000, for real scene we aggregate a semi sphere and a plane to recreate the floor of the scene. We remove small components inn all the scenes.
%     \item We generate 10,000 images for all objects except ficus, mic (because low number of intersected points) in which we obtained 14,000. Random poses for Realistic Synthetic 360° dataset and interpolation rnaodm poses.
%     the psnr for the first stages are these ... TABLE
%     \item Following~\cite{r2l}, we employ intensive residual blocks~\cite{resnet} and deep MLPs for each $G_{\text{pos}}$ and $G_{\text{dir}}$ network. We utilize hard ray and warm up strategies.
%     \item  In \cite{resnet}, residual connections were demonstrated to be essential for enabling the much greater network depth, and the same holds here for learning the light field.
%     \item training time is 1 day on one A100, batch size 200,000.
%     \item We extract a png for each uvwb component. we normalized min max per channel. 
%     \item time to create texture map 50 min. 
%     \item 0-360 azimuth and elevation, 1024x1024 synthetic dataset and 2048x2048 real scenes. floor approximation for angle query.
%     \item texel per triangle used in all experiments except plot X is 5x5/2.
%     \item We implement a fragment shader to combine uvw with direction.
%     \item uvw and beta are stored in a single texture each (4 textures total), each encoded as a 2x4 grid of matrices
    
%     \item direction is calculated in a vertex shader as the vector from camera to vertex, resulting in a direction vector interpolated from all face vertices which is then input to the fragment shader
%     \item the shader converts direction from cartesian to spherical coordinates, and uses the resulting [0,1] normalized elevation and azimuth to index the beta texture
%     \item fragment UV coordinates are used to index the uvw feature textures.
%     % \item Since we use triangular meshes, 
%     \item Since textures are stored as discretized 8-bit integers, we map to their original values by applying the inverse of the min-max normalization per-channel (32 values each for uvw, beta)
%     \item We compute the final color for each fragment by performing a dot product between the uvw and beta before applying a sigmoid function
%     \item No alpha compositing is required since our pipeline takes into account all transparencies with the light field formulation. Thus fragment alpha is set to 1.
    
    
% \end{itemize}

% In \cite{resnet}, residual connections were demonstrated to be essential for enabling the much greater network depth, and the same holds here for learning the light field.



% \input{results/0_hdw_specs_tb}
% \input{results/1_Rendering_speed_tb}
% \input{results/2_Resources_tb}
% \input{results/3_Quantitative_tb}
% \input{results/4_Polygon_count_tb}