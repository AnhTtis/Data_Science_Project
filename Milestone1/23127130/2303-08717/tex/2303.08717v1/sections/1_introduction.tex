\vspace{-.5cm}
\section{Introduction}
\label{sec:intro}
Neural Radiance Fields (NeRFs)~\cite{mildenhall2021nerf} have revolutionized the field of novel view synthesis, as demonstrated by their impressive capacity to reconstruct complex objects and scenes with remarkable detail~\cite{barron2021mipnerf, tewari2020state}. 
% Novel view synthesis has experienced significant progress in recent years with the introduction of Neural Radiance Fields~(NeRFs) by Mildenhall~\etal~\cite{mildenhall2021nerf}.
% NeRFs have shown capacity to reconstruct challenging objects and scenes with remarkable detail~\cite{barron2021mipnerf, tewari2020state}.
The impressive performance of NeRFs casts them as a decisive tool for capturing and representing 3D objects and scenes.
As a result, NeRFs hold great promise for countless practical applications, including video games, movies, and Augmented/Virtual Reality (AR/VR).


However, the impressive performance of NeRFs comes with significant computational costs when rendering novel views.
This slow rendering is mainly due to two limiting properties of NeRFs, which dramatically increase their computational requirements.
Firstly, they use a volumetric representation to model scenes~\cite{drebin1988volume}, implying that rendering a single pixel requires hundreds of queries in space.
Secondly, they leverage a Multilayer Perceptron~(MLP) to model radiance and density across space, meaning that each of those spatial queries involves evaluating an expensive MLP.
These properties make it challenging to render NeRFs in real-time on resource-constrained devices, which hinders their practical deployment.

\input{imgs/teaser}

A plethora of research efforts target the efficiency of NeRFs.
A line of work focused on shortening training times~\cite{yu_and_fridovichkeil2021plenoxels, kangle2021dsnerf, mueller2022instant}, while another looked at accelerating rendering times~\cite{neff2021donerf, reiser2021kilonerf}.
Notably, some methods pre-tabulate a NeRF's output on a sparse grid, and achieve real-time rendering by leveraging powerful GPUs~\cite{yu2021plenoctrees, hedman2021snerg}.
However, these methods are still incompatible with widely-available graphics pipelines that enable rendering on resource-constrained devices via popular frameworks such as WebGL and OpenGL ES.
This incompatibility stems from the rendering approach inherited from NeRFs, as volume rendering demands ray marching, which is dramatically more expensive than mesh rasterization.

In this paper, we present \textbf{\methodname}, a method that enables \underline{Re}al-time \underline{Re}ndering of \underline{N}eRFs across \underline{D}evices, including resource-constrained devices such as VR headsets and mobile phones.
To attain this end goal, we define and achieve three objectives: 
\textit{(i)}~enabling compatibility with widely-available graphics pipelines,
\textit{(ii)}~obtaining ray color with a single query, and
\textit{(iii)}~avoiding MLP evaluations for such queries.
Given a pre-trained NeRF as input, \methodname renders it in real-time by transforming the knowledge learned by the NeRF into an alternative representation. 
In particular, \methodname distills the NeRF by extracting the learned density into a mesh, and the learned color information into a set of matrices that efficiently factorize the scene's light field.
\methodname is thus capable of rendering a NeRF in real-time, making it deployable on a wide range of devices. 
When rendering challenging unbounded real scenes, our method achieves over 2.6-fold speed improvements above the state-of-the-art, while maintaining comparable quality. 
Furthermore, given that our rendering entirely disposes of MLPs, we can easily deploy it on other constrained devices, like VR headsets, where other methods cannot adapt. 
Please refer to Figure~\ref{fig:pull} for an overview of the capabilities of \methodname.

In summary, our contributions are threefold: 
% \begin{enumerate}[\itshape(i)]
%     \item 
    \textbf{(i)} We introduce \methodname, a method enabling real-time rendering of a pre-trained NeRF on resource-constrained devices.
    \methodname works on a wide variety of NeRFs, achieves remarkable rendering speeds at negligible costs to photo-metric quality, and is compatible with popular graphics pipelines available in common devices.
    % \item 
    \textbf{(ii)} \methodname enables fast rendering of a NeRF by transforming it into a representation with three fundamental qualities: 
    it resembles a graphics-friendly representation (\ie a mesh-texture package), 
    it obtains ray color via light fields (avoiding expensive volume rendering), and 
    it factorizes the light field computation as an MLP-free matrix multiplication.
    % \item 
    \textbf{(iii)} We conduct a comprehensive empirical study of \methodname across resource-constrained devices.
    Our results find remarkable boosts in rendering time that come at insignificant costs to image quality.
    Notably, we find \methodname boosts rendering speeds by $2.6\times$ at low memory expenses in challenging real scenes, enabling real-time rendering on various devices. 
% \end{enumerate}

Striving for reproducibility, we provide our implementation of \methodname, written in PyTorch~\cite{NEURIPS2019_9015}, in the \SM.
\vspace{-0.6cm}