\input{imgs/0_Rendering.tex}

\section{Related Work}\label{sec:relatedwork}
\vspace{-0.5em}
\vspace{2pt}\noindent\textbf{Light Field Representations.}
Light fields~\cite{levoy1996light} represent a scene as integrated radiance along rays, \ie they are the integral of radiance fields.
% In contrast to radiance fields, light fields do not obtain color by sampling along the ray, but rather by directly mapping ray origin and direction to color.
% Thus, the light field formulation computes a pixel's color with a single query to the field, in contrast to the hundreds of queries required in the radiance field formulation.
Unlike radiance fields, light fields directly map ray origin and direction to color, allowing for a single query to compute a pixel's color instead of multiple queries.
Innovative works have aimed at parameterizing light fields with neural networks~\cite{sitzmann2021light, suhail2022light, attal2022learning, feng2021signet, r2l}.
However, despite their desirable properties, light field representations have been found to be challenging to learn~\cite{lassner2021pulsar}.
This difficulty mainly stems from 
\textit{(i)}~their reductive formulation of appearance requiring compensation with clever parameterizations~\cite{sitzmann2021light, feng2021signet} or larger and data-hungry architectures~\cite{r2l}, and 
\textit{(ii)}~their indirect use of geometry that calls for injecting geometric priors elsewhere in the pipeline~\cite{suhail2022light, attal2022learning}.
Our proposed \methodname renders a pre-trained NeRF in real time by distilling the NeRF's color knowledge into a light field, thus leveraging the inherent one-query-per-pixel virtue of these fields.
In particular, our light field formulation is similar to~\cite{wood2000surface}, whereby
% Surface light fields for 3D photography
% the light field produces the color along a camera ray when evaluated at the \textit{collision} point between the ray and the scene's geometry, also called, lumispheres. % ---in contrast to light fields' evaluation of camera-oriented rays.
the field generates the color at the point of intersection between the camera ray and the scene's geometry. %(also known as lumispheres, or \textit{surface} light fields).
% We extract scene geometry and additional views from a NeRF, inspired by the radiance-to-light field distillation presented in~\cite{r2l}.
We further accelerate rendering by factorizing the light field into matrices, and enabling compatibility with graphics pipelines by storing such matrices into texture~map-like arrays.


\vspace{2pt}\noindent\textbf{Factorizing Neural Fields.}
NeRFs~\cite{mildenhall2021nerf} use an MLP to map a position $\mathbf{p} \in \mathbb{R}^3$ and a view direction $\mathbf{d} \in \mathbb{R}^2$ to color and density.
Various works have studied the complex interplay between position and view direction that happens inside NeRFs~\cite{hedman2021snerg, garbin2021fastnerf, yu2021plenoctrees, yu_and_fridovichkeil2021plenoxels, Wizadwongsa2021NeX}.
Notably, Garbin~\etal~\cite{garbin2021fastnerf} highlight how caching a NeRF's output is precluded by the dependence of color on \textit{both} $\mathbf{p}$ and $\mathbf{d}$.
Thus, the authors propose to factorize color into two independent functions (separately for $\mathbf{p}$ and $\mathbf{d}$), such that the inner product of their outputs generates color.
This factorization allows for caching the NeRF's output, and thus effectively disposes of MLPs for rendering.
% Factorizing view-dependent from view-independent components in neural fields is commonplace since the first implementation of Neural Radiance Fields (NeRF)~\cite{mildenhall2021nerf}. 
% This factorization models appearance as depending both on position and viewing angle, while modeling geometry as solely depending on position. 
% Later works~\cite{hedman2021snerg, garbin2021fastnerf, yu2021plenoctrees} noted how NeRF’s original implementation presented entanglement between position and viewing angle. 
% Thus, these works achieved disentangled by introducing explicit factorization of these components in NeRF’s architecture~\cite{yu_and_fridovichkeil2021plenoxels, Wizadwongsa2021NeX}. 
Our work enables real-time rendering by combining this factorization approach with the color formulation of light fields.
% by factorizing a Neural \textit{Light} Field (NeLF) on a standard graphics pipeline. 
Specifically, we leverage the NeRF-factorization method of~\cite{garbin2021fastnerf} to factorize a Neural \textit{Light} Field.
This combination allows \methodname to enjoy both the factorization's MLP-free rendering, and the one-query-per-pixel virtue of light fields.
% In particular, by representing a scene as a mesh textured with \textit{light embedding} maps, we enjoy the speed-ups provided by \textit{(i)} the light field's one-query-per-pixel virtue, \textit{(ii)} the factorization's disposal of MLPs for computing final radiance, and \textit{(iii)} the graphics pipeline.


\vspace{2pt}\noindent\textbf{Rendering of Neural Fields.}
Neural fields achieve impressive photo-realistic quality at undesirably large computational costs~\cite{mildenhall2021nerf}.
Various works have addressed training costs~\cite{mueller2022instant, yu_and_fridovichkeil2021plenoxels, Chen2022ECCV, SunSC22}, while others focused on rendering.
For rendering, advances in differentiable and fast rendering~\cite{kato2020differentiable, Niemeyer2020CVPR, cole2021differentiable} have enabled novel advances and applications~\cite{zhang2021nerfactor}.
While several approaches achieve real-time rendering on power-intensive setups~\cite{garbin2021fastnerf, yu2021plenoctrees, reiser2021kilonerf, lindell2021autoint, rebain2021derf} with access to GPUs, we focus on enabling real-time rendering on resource-constrained devices such as AR/VR headsets and mobiles.
Rendering can be accelerated by exploiting graphics pipelines offered by the hardware of these devices. 
Both SNeRG~\cite{hedman2021snerg} and PlenOctrees~\cite{yu2021plenoctrees} leveraged optimized graphics routines for in-browser rendering, but did not exploit efficient mesh-oriented pipelines.
These pipelines are difficult to exploit in NeRF's formulation, since the volume rendering~\cite{drebin1988volume} nature of NeRFs is incompatible with the polygon-oriented paradigm of mesh rasterization.
In this work, we accelerate the rendering of NeRFs by transforming their learned knowledge into a mesh-friendly representation on which efficient mesh-rasterization pipelines can operate. % , and exploit light fields with a mesh-optimized graphics pipeline to accelerate rendering. 
In particular, we represent a scene as a mesh whose texture maps are filled with ``light field embeddings'', which result from factorizing the scene's light field.
Concurrently with our work, Chen~\etal~\cite{chen2022mobilenerf} propose MobileNeRF, which also exploits mesh-rasterization pipelines for fast rendering on devices. 
While MobileNeRF uses surface-based neural fields, \methodname uses light fields at the surface of objects.
Furthermore, our method acts as a framework for \textit{transforming} a pre-trained NeRF to an MLP-free and rasterization-friendly representation that is capable of real-time rendering across devices. 
% \juan{MobileNeRF uses surface-based neural fields, while we use surface light fields.}
% \vspace{-0.3cm}
% \vspace{-0.6cm}
