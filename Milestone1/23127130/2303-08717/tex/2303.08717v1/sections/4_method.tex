% \section{Methodology}

\input{imgs/1_Training.tex}
\section{Method: \methodname}
\label{sec:methodology}

% \albert{No ref to fig 3}
% Idea
% Framework for transforming/converting NeRF to other thing
We now introduce \methodname, a method enabling real-time rendering of a pre-trained NeRF on resource-constrained devices.
\methodname receives a pre-trained NeRF $R$ as input and transfers its knowledge to an alternative representation $\mathcal{S}$ that is % amenable for real-time rendering on resource-constrained devices. 
fast and inexpensive to render.
The representation generated by \methodname achieves fast rendering by 
\textit{(i)}~representing the learnt scene as a mesh whose texture maps store embeddings, 
\textit{(ii)}~obtaining ray color via light fields~(instead of radiance fields required by expensive volume rendering), and 
\textit{(iii)}~factorizing the light field computation as an MLP-free matrix multiplication.
Please refer to Figure~\ref{fig:Rendering} for an overview of \methodname.

In essence, \methodname transforms a pre-trained NeRF $R$ into a rasterization-friendly representation $\mathcal{S} = \{\chi,\mathcal{M}\}$.
This representation is composed of a mesh $\chi$, % as in Section~\ref{sec:formulation}, 
and a set of four texture-map-like matrices $\mathcal{M} = \{ \mathbf{M}_\mathbf{u}, \mathbf{M}_\mathbf{v}, \mathbf{M}_\mathbf{w}, \mathbf{M}_{\boldsymbol\beta} \}$.
Each matrix $\mathbf{M}_i$ % follows a texture map-configuration, and 
stores representations we dub ``light field embeddings'', where position-dependent effects are modeled by the first three matrices, while the fourth one accounts for view-dependent effects.
% The representation $\mathcal{S}$ is composed of a polygonal mesh~$\chi$ and the set of matrices $\mathcal{M}$.
While obtaining the mesh $\chi$ is straightforward from the densities generated by $R$, %  learned by $F_\Theta$ to a volumetric grid and running Marching Cubes~\cite{lorensen1987marching}. %  to obtain a triangular mesh $\chi$. % \ali{Do we want to mention that we extract "rough" meshes? Or that we are robust to the bad geometry that comes from NeRF?}
% obtaining the light field embeddings required to 
constructing $\mathcal{M}$ is challenging.
% We directly extract the scene's polygonal mesh $\chi$ from $F_\Theta$, and then leverage light fields to construct $\mathcal{M}$. % training our Factorized NeLF.
% \paragraph{Mesh extraction.}

In the next two sections, we describe how we obtain~$\mathcal{M}$.
In particular, Section~\ref{sec:nelfs} describes how we approximate $R$ with a Factorized Neural Light Field (NeLF), and Section~\ref{sec:fnelfs2rere} explains how we extract $\mathcal{M}$ from this NeLF.

% from this in  baking a Factorized NeLF from $F_\Theta$. 
% \juan{fix this}
% and then how we bake this Factoriz extracting $\mathcal{M}$.
% to obtain $\mathcal{M}$
% \textit{(i)} % can be achieved by transforming the scene learnt by the NeRF into a representation that is compatible with standard graphics pipelines.
% In particular, we aim at modeling the scene in a mesh-texture map configuration, whereby the texture maps are filled with information capable of accounting for the scene's view-dependent effects.
% Objective number \textit{(ii)} can be attained by disposing of NeRFs' 

\begin{comment}
in  renders it in real time by transforming the knowledge learned by the NeRF into an auxiliary representation
In particular, \methodname distills the NeRF by extracting the learned density into a mesh,
% \ali{do we want to highlight that it is a "rough" mesh?}, 
and the learnt color information into a set of matrices from which the scene's light field can be efficiently computed.
% Contrary to the canonical texture representation, we fill the texture maps with light field-based embeddings that account for view-dependent phenomena \textit{without} MLPs.
Our method is thus capable of rendering a NeRF on resource-constrained devices at % inherits the one-query-per-pixel quality of light fields, and entirely avoids MLPs for computing color.
\textbf{100 bazillion FPS while maintaining an average PSNR of 99 quintillion}.
We propose a light field-based representation for real-time rendering on low-power devices such as mobiles and AR/VR headsets \textcolor{red}{\textbf{(?)}}.
Our representation enables fast rendering by achieving the following 3 objectives: compatibility with graphics pipelines, single query per pixel, and no MLP evaluations. We're compatible with standard graphics pipelines by representing a scene as a polygonal mesh and an associated texture map~(\Figure{PullingFigure}a) etc...
\end{comment}


\begin{comment}
    
\begin{enumerate}[\itshape(i)]
    \item %\textcolor{purple}{\textbf{(i)}}~
    Compatibility with standard graphical pipelines.
    \item %\textcolor{olive}{\textbf{(ii)}}~
    Obtaining pixel color with a single query.
    \item %\textcolor{blue}{\textbf{(iii)}}~
    Entirely avoiding evaluating MLPs.
\end{enumerate}
We achieve objectives \textit{(i)}~and \textit{(ii)}~by representing a scene as a polygonal mesh and an associated texture map~(\Figure{PullingFigure}a).
We achieve objective \textit{(iii)}~by using said texture maps to store \textit{light-field embedding} maps baked from a \textit{Factorized NeLF}, an architecture we propose combining NeLFs with the MLP-free factorization proposed in~\cite{garbin2021fastnerf} to disentangle the factors of position and view direction.

\end{comment}
% Our representation's essential components are, thus, the Factorized NeLF and the scene's polygonal mesh.
% Next, we describe how we distill a NeRF to obtain these components, our proposed Factorized NeLF architecture, and how we integrate these elements for real-time rendering on low power devices such as mobiles and AR/VR headsets \textcolor{red}{\textbf{(?)}}.


% \subsection{Transforming $F_\Theta$ into $\mathcal{S}$}
\subsection{Representing $R$ with Factorized NeLFs}\label{sec:nelfs}
We construct $\mathcal{M}$ by using the radiance field $R$ as supervision to train a \textit{Factorized} Neural Light Field~(NeLF).
Factorized NeLFs are light fields whose computation can be expressed as inexpensive matrix multiplications.
We first explain the formulation for constructing a NeLF, and then the formulation for their factorization. %  counterparts.
% Our input is a pre-trained NeRF. We've experimented with bla, but are essentially agnostic to X.
% We first extract a rough mesh from the pre-trained NeRF, which is used to build our factorized NeLF and the final rendering. Note that our method is robust to mesh accuracy bla bla bla.
% Inspired by [8] and [28] we want to bake and factorize the NeRF to allow for real-time rendering. bla bla bla. Our choices allow for single query per pixel...
% \paragraph{NeRF distillation.}
% Inspired by the distillation of NeRF introduced in Wang~\etal~\cite{r2l}, we start with a pre-trained NeRF $F_\Theta$. % on a set of calibrated images. \ali{Why are we saying "calibrated images"? Can't we just say a pre-trained NeRF?}

\vspace{2pt}\noindent\textbf{NeLF.}
We extract the color information learned by the NeRF $R$ into a NeLF $L$.
Light fields are the integral of radiance fields: while radiance fields reconstruct the individual integrands of Eq.~\eqref{eq:color} along the ray, light fields directly estimate the integral's value, \ie~the ray's color.
Thus, light fields are more suitable for our purposes, as they directly yield $C(\mathbf{r})$ instead of computing the integral of Eq.~\eqref{eq:color}.

Formally, given a position $\mathbf{p}$ and direction $\mathbf{d}$, the light field models the integrated radiance along the ray with origin at $\mathbf{p}$ and direction $\mathbf{d}$, \ie
% and models $\mathbf{r}(t) = \mathbf{p} + t\:\mathbf{d},\:t\in[0,+\infty]$, and models the integrated radiance along $\mathbf{r}$, that is:
$C(\mathbf{p} + t\:\mathbf{d}),\:t\in[0,+\infty)$.
If the scene's geometry is known, empty space can be skipped by considering a ray whose origin is the collision point of the ray and the scene's geometry.
We use our mesh $\chi$ as an estimate to this geometry, and define $\mathbf{p}^{\mathbf{r}}_\chi \coloneqq \mathbf{r} \wedge \chi$ as the first intersection point between ray $\mathbf{r}$ and mesh $\chi$.
The NeLF $L$ thus must approximate $C$, that is,
\vspace{-0.2cm}
\begin{equation}\label{eq:red_light_field}
\vspace{-0.2cm}
    L(\mathbf{p}^{\mathbf{r}}_\chi,\mathbf{d}) \approx % L(\mathbf{p}^{\mathbf{r}}_\chi,\mathbf{d}) =
    C(\mathbf{p} + t\:\mathbf{d}),\:t\in[0,+\infty).
% \vspace{-4pt}
\end{equation} 

Inspired by~\cite{r2l}, and using the formulation of Eq.~\eqref{eq:red_light_field}, we can distill a NeRF $R$ into a NeLF $L$.
Specifically, we can use $R$ to compute the right hand side of this equation, and use these values as supervision to learn $L$. % , parameterized by an MLP, denoted by~$G$.
Thus, % by learning $L(\mathbf{p}^{\mathbf{r}}_\chi,\mathbf{d})$, which approximates $C(\mathbf{p} + t\:\mathbf{d}),\:t\in[0,+\infty)$, 
the NeLF is tasked with predicting the color of ray $\mathbf{r}$ when evaluated at the collision point $\mathbf{p}^{\mathbf{r}}_\chi$.
% ,  to compute the RHS, and aim at learning the light field, \ie the LHS of Eq.~\ref{eq:red_light_field}.
\begin{comment}
That is,
\begin{equation}
    L(\mathbf{p}, \mathbf{d}) = \int_{}
\end{equation}
\end{comment}
% Training light field-based representations is challenging.
% In~\cite{r2l}, the authors successfully trained NeLFs by leveraging large amounts of pseudo data distilled from a NeRF.
% Emphaisze evaluation on a single point
% We follow this approach to train our Factorized NeLF, and combine it with knowledge on the extracted mesh to accelerate training. 
In practice, we sample camera views (and their associated set of rays $\mathcal{R}$) and use $R$ to compute a set of pseudo-images~$\mathcal{I}_{\text{pseu}}$. %  from such views.
Furthermore, we use $\chi$ to pre-compute the collision points between the rays in $\mathcal{R}$ and the scene.
We then evaluate $L$ at these points, and task it with predicting the colors given by~$\mathcal{I}_{\text{pseu}}$.

\vspace{2pt}\noindent\textbf{Factorized NeLFs.}
For a given camera ray, the NeLF $L$ predicts ray color when evaluated at the collision point of the camera ray and the scene.
Because of the light field formulation, this approach can compute pixel colors with a single MLP query. %  enjoys the one-query-per-pixel virtue of light fields.
However,~$L$'s evaluations are still expensive due to its MLP parameterization.
Hence, we propose to rather learn a Factorized NeLF.
The architecture of a Factorized NeLF is amenable to ``baking'',~\ie~pre-computing and storing network outputs, to dispose of MLPs at rendering time.
% We further accelerate our approach by factorizing the architecture to enable ``baking'',~\ie~pre-computing and storing network outputs, by combining NeLF with the factorization proposed in~\cite{garbin2021fastnerf}.
We next describe Factorized NeLFs in detail.

Recall that our NeLF is a function $L~:~(\mathbf{p}^{\mathbf{r}}_\chi,\mathbf{d})\mapsto\mathbf{c}$, parameterized by an MLP.
This function maps a point~(on the mesh)~$\mathbf{p}^{\mathbf{r}}_\chi \in \R^3$ and a ray direction~$\mathbf{d} \in \R^2$ to an RGB color $\mathbf{c} \in \R^3$.
A \textit{Factorized} NeLF $L_F$ shares the same signature with $L$, but internally processes $\mathbf{p}^{\mathbf{r}}_\chi$ and $\mathbf{d}$ with two independent MLPs whose outputs produce color via an inexpensive matrix multiplication.
We define these underlying direction- and position-dependent MLPs~\cite{garbin2021fastnerf} as
\begin{equation}\label{eq:dir-pos}
\begin{aligned}
    L_{\text{pos}}: &\:\mathbf{p}^{\mathbf{r}}_\chi \mapsto [\mathbf{u}, \mathbf{v}, \mathbf{w}]  \in \R^{D\times 3}, \\
    L_{\text{dir}}: &\:\mathbf{d}                   \mapsto \boldsymbol\beta \in \R^{D}.
\end{aligned}
\end{equation}
We refer to the vectors % position-dependent 
$\mathbf{u}$, $\mathbf{v}$, $\mathbf{w}$, % vectors % are weighed by 
and % the direction-dependent vector 
$\boldsymbol\beta$
% We dub the $\mathbf{u}$, $\mathbf{v}$, $\mathbf{w}$, and $\boldsymbol\beta$ vectors as 
as \textit{light field embeddings}.
Our Factorized NeLF $L_F$ is thus defined as
\begin{equation} \label{eq:factorized-nelf}
\begin{aligned}
    L_F(\mathbf{p}^{\mathbf{r}}_\chi, \mathbf{d}) &= \mathrm{Sig}\left(L_{\text{pos}}(\mathbf{p}_\chi)^\top L_{\text{dir}}(\mathbf{d})\right) \\
    &= \mathrm{Sig}\left([\mathbf{u}, \mathbf{v}, \mathbf{w}]^\top\boldsymbol\beta\right),
\end{aligned}
\end{equation}
where $\mathrm{Sig}$ is the sigmoid function.
Note, in Eq.~\eqref{eq:factorized-nelf}, the position-dependent embeddings ($\mathbf{u}$, $\mathbf{v}$, $\mathbf{w}$) are weighed by the direction-dependent embeddings $\boldsymbol\beta$.
This formulation enables accelerated rendering via baking: $L_{\text{pos}}$ and $L_{\text{dir}}$ outputs are pre-computed and stored, so that $L_F$ can be approximated at test time with an inexpensive MLP-free operation.

\vspace{2pt}\noindent\textbf{Training Factorized NeLFs.}
We train $L_F$ to predict the colors given by the set of pseudo-images~$\mathcal{I}_{\text{pseu}}$ when evaluated at the point of collision with the scene.
Formally, % by defining $\mathbf{p}^{\mathbf{r}}_\chi \coloneqq \mathbf{r} \wedge \chi$ as the (first) intersection point between camera ray $\mathbf{r}$ and mesh $\chi$, 
we train~$L_F$ to minimize the photo-metric loss
\begin{equation}\label{eq:fnelf_loss}
    \mathcal{L} = \sum_{\mathbf{r} \in \mathcal{R}}\norm{L_F\left(\mathbf{p}^{\mathbf{r}}_\chi, \mathbf{d}\right) - \mathcal{I}_{\text{pseu}}(\mathbf{r})}_2^2,
\end{equation}
where $\mathcal{R}$ is the set of rays with which the set of pseudo-images $\mathcal{I}_{\text{pseu}}$ 
% \albert{should be I bold}\juan{I think $\mathcal{I}_{\text{pseu}}$ is a set, no?} 
was computed, and $\mathcal{I}_{\text{pseu}}(\mathbf{r})$ further denotes the color assigned to ray $\mathbf{r}$ in the pseudo-images.
% Furthermore, $C_{\text{pseu}}(\mathbf{r})$ is the pseudo ground-truth color, as given by $\mathcal{I}_{\text{pseu}}$.
% Note that, given this training objective, rendering a pixel in our framework amounts to computing the collision between the camera ray and $\chi$, and then evaluating $G_F$ at such point.

\begin{comment}
$G_{\text{pos}} : p_\chi \rightarrow (u,v,w)$, a position-dependent network mapping a 3D intersected position in the mesh $p_\chi \in \R^3$ to a deep radiance map $(u,v,w) \in \R^{D3}$ where $D$ is the vector dimension.
The direction-dependent $G_{dir}: d \rightarrow \beta$ maps a ray direction $d \in \R^2$ to D-dimensional vector of weights $\beta \in \R^{D}$ for the $D$ components of the deep radiance map. 
inde characteristic of this architecture entails that, internally, $p_\chi$ $d \in \R^2$ are independently processed,

factorizing position and view-depen , is precluded by 

Both the pseudo images and the mesh are extracted from a pretrained NeRF.
Inspired by \cite{garbin2021fastnerf}, the network (\Figure{PullingFigure}b) is composed of a position-dependent network that produces a deep radiance map $(u, v, w)$ and a direction-dependent network that produces weights $(\beta_1, ..., \beta_D)$.
The inner product of the weights and the deep radiance map estimates the color in the scene as seen from the given direction at the intersected location in the mesh.

from a collection of calibrated images.
given a collection of pseudo images.

At rendering time, given a camera pose, we rasterize the mesh to image space and retrieve deep radiance map from the texture maps.
We convert these deep radiance maps into a color image by computing the inner product with the stored weights.

\subsection{Background: Factorized Neural Radiance Fields}
A Vanilla Neural Radiance Field (NeRF) captures a volumetric 3D representation of a scene within the weights of a neural networks. 
NeRFâ€™s neural network $F_{NeRF} : (p, d) \rightarrow (c, \sigma)$ maps a 3D position $p \in \R^3$ and a ray direction $d \in \R^2$ to a color value $c
$ and transparency $\sigma$.

In the case of Factorized Neural Radiance field \cite{garbin2021fastnerf}, the weights are encoded in two neural networks.
One network only depends on a position $p$ and the other on a ray direction $d$.
The position-dependent network $F_{pos} : p \rightarrow \{\sigma, (u,v,w)\}$ maps a 3D position $p \in \R^3$ to a density $\delta \in \R$ and a deep radiance map $(u,v,w) \in \R^{D3}$ where $D$ is the vector dimension.
The direction-dependent network $F_{dir}: d \rightarrow \beta$ maps a ray direction $d \in \R^2$ to D-dimensional vector
of weights $\beta \in \R^{D}$ for the $D$ components of the deep radiance map. 
The inner product of the weights and the deep radiance map
\begin{equation}
  c = (r, g, b) = \sum_{i=1}^{D} \beta_i(u_i, v_i, w_i) = \beta^T\cdot(u, v, w)
\label{eq:fnerf}
\end{equation}
results in the estimated color $c = (r, g, b)$ at position $p$ observed from direction $d$.

A single image pixel can be rendered by casting a ray from the camera center, passing through that pixel and into the scene. 
We denote the direction of this ray as $d$. A number of 3D positions $(p_1, ..., p_N)$ are then sampled along the ray between its near and far bounds defined by the camera parameters. 
The neural network $F_NeRF$ is evaluated at each position $p_i$ and ray direction $d$ to produce color $c_i$ and transparency $\sigma_i$. 
These intermediate outputs are then integrated as follows to produce the final pixel color $\hat{c}$:
\begin{equation}
    \hat{c} = \sum_{i=1}^{N} T_i(1-\exp(-\sigma_i \delta_i)) c_i
\label{eq:nerfvanilla}
\end{equation}
where $T_i = \exp(-\sum_{j=i}^{i-1} \sigma_j \delta_j)$ is the transmittance and $\delta_i = (p_{i+1}-p_i)$ is the distance between the samples.

\subsection{Factorized Neural \textit{Light} Fields}
In addition to \textit{radiance} fields, which are used in neural rendering, a scene can also be represented as a \textit{light} field that has been parameterized by a neural network.
The neural \textit{light} field eliminates the need to sample various places along the camera ray by directly mapping ray origin and direction into the related RGB values.
As a result, rendering a pixel is substantially quicker than the radiance scene representation since it only needs one query \cite{r2l}.

This observation enables us to return to our proposed model. 
First, we distill the knowledge from a pretrained Vanilla NeRF model to our Neural Light Fields network $G_{NeLF}$, by generating pseudo data and a polygonal mesh using Marching Cubes \cite{lorensen1987marching}.
Second, the network $G_{NeLF} : (p_\chi, d) \rightarrow c $ learns to map directly a intersected 3D point in the mesh along a oriented ray $p_\chi \in \R^3$ and a ray direction $d \in \R^2$ to a color value $c$.

$G_{NeLF}$ actually consists of a Factorized NeRF's neural network, however it models \textit{light} fields (\ie Factorized NeLF). 
The position-dependent network $G_{pos} : p_\chi \rightarrow  (u,v,w)$ maps a 3D intersected position in the mesh $p_\chi \in \R^3$ to a deep radiance map $(u,v,w) \in \R^{D3}$ where $D$ is the vector dimension.
The direction-dependent $G_{dir}: d \rightarrow \beta$ maps a ray direction $d \in \R^2$ to D-dimensional vector of weights $\beta \in \R^{D}$ for the $D$ components of the deep radiance map. 

To get the deep radiance maps and weights of a Factored NeRF network, numerous points along the ray must still be sampled during training.
Furthermore, caching such features requires up to 10 gigabytes of memory for a straightforward scene.
With the help of our Factorized NeLF architecture, caching is lighter and quicker.
It only samples one point every pixel, which is the intersection point of the oriented ray and the mesh.
Additionally, rather of using a voxel grid like in \cite{garbin2021fastnerf}, we only save deep radiance maps in each of the mesh faces.
\end{comment}

\subsection{Factorized NeLFs to Real-time Rendering}\label{sec:fnelfs2rere}
% Re-sell. Emphasize REAL-TIME
We now use our Factorized NeLF to build $\mathcal{M}$, the missing piece in our a rasterization-friendly representation $\mathcal{S} = \{\chi,\mathcal{M}\}$.
Recall $\mathcal{M} = \{ \mathbf{M}_\mathbf{u}, \mathbf{M}_\mathbf{v}, \mathbf{M}_\mathbf{w}, \mathbf{M}_{\boldsymbol\beta} \}$ is a set of four matrices, % , where each $\mathbf{M}_i$ follows a texture map configuration.
where the first three matrices model position-dependent effects, and the fourth one 
%accounts for 
view-dependent effects.
Here we describe how we leverage our Factorized NeLF to construct $\mathcal{M}$, %thus completing $\mathcal{S}$, 
and then how we integrate this representation with mesh-rasterization pipelines.

\vspace{2pt}\noindent\textbf{Constructing $\mathcal{M}$.}
The Factorized NeLF formulation from Eq.~\eqref{eq:factorized-nelf} is amenable to baking,~\ie~pre-computation and storage.
Namely, if the outputs of $L_{\text{pos}}$ and $L_{\text{dir}}$ are baked, computing a ray's color is reduced to an inexpensive MLP-free operation.
To enjoy this MLP-free property when rendering with a mesh-rasterization pipeline, we use $\mathcal{M}$ to store the baked outputs.
Specifically, we pre-compute light field embeddings at various inputs and store them in the corresponding matrices $\mathbf{M}_\mathbf{u}$, $\mathbf{M}_\mathbf{v}$, $\mathbf{M}_\mathbf{w}$ and $\mathbf{M}_{\boldsymbol\beta}$.
For the position-dependent embeddings, we traverse $\chi$'s faces, extract $N_\text{pos}$ points in each face's normalized $uv$ coordinates, % \albert{?}, 
and then evaluate $L_{\text{pos}}$ at such positions.
Analogously, for the direction-dependent embeddings, % $\mathbf{M}_\beta$, is filled with the direction-dependent light field embedding $\boldsymbol\beta$.
% ToThe direction-dependent weights~$\boldsymbol\beta$, on the other hand, require sampling directions for baking.
% To construct this matrix, 
%we independently sample $N_\text{dir}$ azimuth and elevation angles, % between $0$ and $2\pi$, 
%and then evaluate $L_{\text{dir}}$ at such directions. %  to obtain $\boldsymbol\beta$.
we  evaluate $L_{\text{dir}}$ at independently sampled $N_\text{dir}$ azimuth and elevation angles.

Once constructed $\mathcal{M}$, we are ready to integrate our representation $\mathcal{S}$ with the rasterization pipeline.

\vspace{2pt}\noindent\textbf{Integration with mesh rasterization.}
We render our representation $\mathcal{S} = \{\chi,\mathcal{M}\}$ on a standard graphics pipeline, 
% Compatibility with this pipeline allows for fast and flexible rendering, since it has been developed with the aim of speed and compatibility with popular hardware.
%By leveraging standard graphics pipelines, \methodname enables
enabling real-time rendering across devices, from mobiles all the way to VR headsets.
% that employs mesh rasterization and fragment shaders. %  for accelerated rendering.
% This deployment allows us to achieve real-time rendering on a wide variety of resource-constrained devices such as AR/VR headsets \textcolor{red}{\textbf{(?)}} and mobiles.
Note that our scene representation is compatible with standard graphics pipelines: the required mesh is compatible with the $\chi$ mesh extracted from $R$, and the texture are our four matrices in $\mathcal{M}$.
%The graphics pipeline we use requires scenes to be represented as a mesh and an associated texture map.
%The required mesh is compatible with the $\chi$ mesh we extracted from $R$, so we simply export $\chi$ to an OBJ file.
%For texture maps, we pass the four matrices we stored in $\mathcal{M}$.
In particular, 
as common in production rendering~\cite{burley2008ptex}, 
we store the position-dependent info ($\mathbf{M}_\mathbf{u}$, $\mathbf{M}_\mathbf{v}$ and $\mathbf{M}_\mathbf{w}$) into a texture map-like with per-face textures, and the direction-dependent info ($\mathbf{M}_{\boldsymbol\beta}$) in a texture map sorted by its corresponding sampling angles.
% a separate texture per quad face of the subdivision control mesh
We then quantize all baked outputs, and export them as PNG files. %  required by the graphics pipeline. 
Finally, we deploy our rendering pipeline within a fragment shader for compatibility with standard rasterization frameworks~(please refer to Figure~\ref{fig:Training}).
% Please refer to the \SM for implementation details on the process of integrating our approach with a mesh-rasterization pipeline. 
% \ali{If the reviewers are aware of MobileNeRF, they might question the differences when reading this last paragraph. We can avoid that by adding yet another sentences differentiating us from them, and pointing out how this difference results in substantial gains as can be seen by our experiments.} \ali{Let's a add a sentence to say that we will provide more details on the texture construction in the supplementary material.}

\begin{comment}
The required texture map, on the other hand, is incompatible with how we model color in Eq.~\eqref{eq:dir-pos}.
We resolve this incompatibility by injecting our light embeddings ($\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$) and direction-dependent weights ($\boldsymbol\beta$) into four texture map-like arrays.

In particular, we fill these texture map-like arrays with baked values for $\mathbf{u}$, $\mathbf{v}$, $\mathbf{w}$ and $\boldsymbol\beta$.
The light embeddings~($\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$) are position dependent, and so their baking requires sampling points on $\chi$'s surface.
We sample these points by traversing the mesh's triangular faces, extracting $N=X$ locations in normalized $uv$ coordinates, and evaluating $G_{\text{pos}}$ at such points.
The direction-dependent weights~$\boldsymbol\beta$, on the other hand, require sampling directions for baking.
In particular, we independently sample azimuth and elevation angles between $0$ and $2\pi$, and then evaluate $G_{\text{dir}}$ at such directions to obtain $\boldsymbol\beta$.
\end{comment}

\begin{comment}
The texture map is divided into triangular regions, each corresponding to a triangular face in the mesh.
Thus, to inject the information encoded in the four vectors $\boldsymbol\beta$, $\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$ (from Eq.~\eqref{eq:dir-pos}) into the texture map.
In particular, the texture of each scene is represented by four texture map-like PNG images.

To fill the texture map, we first discretize the outputs of $G_{\text{pos}}$ and $G_{\text{dir}}$.
We thus evaluate $G_{\text{pos}}$ on all of $\chi$'s visible faces to pre-compute the set of deep textures ($\mathbf{u}$, $\mathbf{v}$, and $\mathbf{w}$).
Moreover, we evaluate $G_{\text{dir}}$ on a uniform grid of angles in \textbf{X} to pre-compute the set of view-dependent weights ($\boldsymbol\beta$).
We then inject these pre-computed results into \textit{four} texture maps, one for $\mathbf{u}$, $\mathbf{v}$, $\mathbf{w}$ and $\boldsymbol\beta$, and maintaining order w.r.t. the corresponding mesh faces.
\end{comment}
