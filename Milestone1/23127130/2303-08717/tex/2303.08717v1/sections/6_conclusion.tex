\section{Conclusions and Limitations}\label{sec:conclusions}

We present \textbf{\methodname}, a method enabling \underline{Re}al-time \underline{Re}ndering of \underline{N}eRFs across \underline{D}evices. 
Our method receives a pre-trained NeRF as input and transforms it into a representation that renders real-time in various devices, from mobile phones to VR headsets.
% We present Factorized Neural Light Field (NeLF), an explicit mesh representation that democratizes fast neural rendering on low power devices such as mobiles and AR/VR headsets \textcolor{red}{\textbf{(?)}}.
% Our representation achieves fast rendering by leveraging compatibility with standard graphics pipelines, obtaining pixel color with a single query, and entirely avoiding MLP evaluations.
Our approach achieves fast rendering by leveraging standard graphics pipelines, obtaining pixel color with a single query, and avoiding MLP evaluations altogether.
% We achieve these objectives by distilling the NeRF and extracting the learned density into a mesh, and the learnt color information into a set of matrices from which the scene's light field can be efficiently computed.
We achieve these objectives by distilling NeRF and extracting density into a mesh, and color information into matrices for efficient light field computation.

We test \methodname on a variety of resource-constrained devices, and find outstanding trade-off between rendering time and photo-metric quality. 
In particular, \methodname achieves over $2.6\times$ FPS improvements when rendering challenging and unbounded real scenes. 
Furthermore, our method allows rendering of these scenes on VR headsets in real-time, a task out of hand for current NeRF architectures. 
% In particular, we show Factorized NeLF can achieve rendering on mobile devices and headsets at over \textbf{5 trillion} FPS while providing competitive reconstruction quality.

% Our approach is, however, not exempt of limitations.
% In particular, we rely on reasonable scene-geometry reconstructions. % , as provided by a pre-trained NeRF.
% Furthermore, although our representation of color is computationally efficient due to its MLP-free nature, it requires large embeddings to provide sensible reconstructions.
% Improvements across any of these directions can provide even further boosts on rendering speed and quality.
Limitations of our approach include relying on pre-trained NeRF for reasonable scene-geometry reconstructions and the requirement for large embeddings to achieve sensible reconstructions. Improvements in these areas could further enhance rendering speed and quality.