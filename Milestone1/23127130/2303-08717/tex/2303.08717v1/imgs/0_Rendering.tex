\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{imgs/0_Rendering3.png}
    \vspace{-0.4cm}
    \caption{
    \textbf{Rendering a NeRF using \methodname.} 
    Given a pre-trained NeRF, \methodname renders it in real time by leveraging a mesh and light field embedding maps ($\mathbf{u}, \mathbf{v}, \mathbf{w}$ and $\boldsymbol\beta$) which are compatible with a standard rasterization pipeline with programmable shaders.
    Rendering occurs in three steps:
    \textbf{a)} The mesh gets rasterized, and its vertices go through a regular vertex shader. %  For rendering. %  and gets rasterized. 
    \textbf{b)} The light field embedding maps are indexed (with the $uv$ coordinates from the rasterization) to obtain values for the position-dependent ($\mathbf{u}, \mathbf{v}$ and $\mathbf{w}$) and direction-dependent ($\boldsymbol\beta$) embeddings. 
    \textbf{c)} Pixel colors are obtained by combining $\mathbf{u}, \mathbf{v}, \mathbf{w}$ and $\boldsymbol\beta$ in a custom fragment shader implementing an inexpensive dot product. %  to obtain the final pixel color. 
    Since \methodname uses light fields, alpha compositing is unnecessary, and so we set alpha values to 1. 
    Note that our method entirely disposes of MLPs at render time, and thus enjoys substantial boosts in rendering speed.
    }
    \vspace{-0.4cm}
    \label{fig:Rendering}
\end{figure*}