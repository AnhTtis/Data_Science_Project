% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{wrapfig}
\usepackage{graphicx}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Fre-GAN 2: Fast and efficient Frequency-consistent audio synthesis}
%
% Single address.
% ---------------
\name{Sang-Hoon Lee$^1$, Ji-Hoon Kim$^2$, Kang-Eun Lee$^2$, Seong-Whan Lee$^{1,2}$}
% \thanks{This work was supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00079, Department of Artificial Intelligence, Korea University)}
\address{
  $^1$Department of Brain and Cognitive Engineering, Korea University, Seoul, Korea\\
  $^2$Department of Artificial Intelligence, Korea University, Seoul, Korea
}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
\ninept
%
\maketitle
%
\begin{abstract}
Although recent advances in neural vocoder have shown significant improvement, most of these models have a trade-off between audio quality and computational complexity. Since the large model has a limitation on the low-resource devices, a more efficient neural vocoder should synthesize high-quality audio for practical applicability. In this paper, we present Fre-GAN 2, a fast and efficient high-quality audio synthesis model. For fast synthesis, Fre-GAN 2 only synthesizes low and high-frequency parts of the audio, and we leverage the inverse discrete wavelet transform to reproduce the target-resolution audio in the generator. We also explore the several knowledge distillation method, however, we found the knowledge distillation does not effect the model performance. The experimental results show the superiority of Fre-GAN 2 in audio quality. Furthermore, Fre-GAN 2 has a 10.91$\times$ generation acceleration, and the parameters are compressed by 21.23$\times$ than Fre-GAN.             

\end{abstract}
% Recent studies in GAN-based vocoder have achieved  high-quality audio synthesis, outperforming conventional neural vocoders in terms of quality. However, most of these models still have a trade-off between audio quality and computational complexity, which hinders the practical application of neural vocoders. In this paper, we present Fre-GAN 2 which achieves a real-time generation of high-quality audio on CPU. Since the most computational complexity in GAN-based neural vocoder stems from the upsample parameters, we reduce the computation of upsampling layer by leverging inverse discrete wavelet transform.
% sub-band audio --> multi-band melgan..? is closely related to the numbers of samples of generated audio, Fre-GAN 2 only synthesize sub-audio and reproduce the target-resolution audio by using inverse discrete wavelet transform

\begin{keywords}
audio synthesis, neural vocoder, generative adversarial networks, discrete wavelet transform
\end{keywords}
%
\section{Introduction}
Recently, deep generative models have shown significant improvement in audio synthesis models \cite{tan2021survey}. An audio synthesis model, called \textit{``Vocoder"}, converts a low-resolution acoustic feature such as mel-spectrogram into a high-resolution waveform. Furthermore, the recent end-to-end text-to-speech model also needs an audio synthesis model to synthesize a high-resolution waveform from text without any intermediate features \cite{ren2020fastspeech, chung21_interspeech, kim2021conditional}. Although current systems synthesize almost realistic audio, these systems suffer from the increased model complexity to generate higher fidelity high-resolution audio. For practical applicability \cite{luo2021lightspeech}, these models have a limitation on low-resource environments such as mobile device.

Especially, WaveNet \cite{oord2016wavenet} has shown significant improvement in audio quality. Due to an autoregressive manner, WaveNet has limitations in slow inference speed. To overcome this limitation, parallel waveform synthesis models are introduced. For parallel audio synthesis, Parallel WaveNet \cite{oord2018parallel} uses the knowledge distillation from a pre-trained WaveNet by an inverse autoregressive flow. WaveGlow \cite{prenger2019waveglow} uses a sequence of invertible flow operations to synthesize audio in parallel. However, these models have high computational complexity. 

 There are many generative adversarial networks (GAN) based parallel audio synthesis studies by modeling various representations of audio, such as MelGAN \cite{kumar2019melgan}, Parallel WaveGAN \cite{yamamoto2020parallel}, and HiFi-GAN \cite{kong2020hifi}. Among them, HiFi-GAN achieves both high-quality audio synthesis and fast audio generation by modeling the various periodic patterns. Similar to StyleMelGAN \cite{mustafa2021stylemelgan} using filter-bank discriminators, UnivNet \cite{jang21_interspeech} employs the multi-resolution spectrogram discriminator to alleviate the over-smoothing problem on spectra. In addition, Fre-GAN \cite{kim21f_interspeech} adopt a resolution-connected generator and resolution-wise discriminator to capture the various scales of spectral distribution. However, all models still have limitations to be implemented in low-resource devices due to their computational complexity. 

In this paper, we present Fre-GAN 2, a fast and efficient frequency-consistent audio synthesis model. For fast audio synthesis, we do not synthesize target-resolution audio, but we still train the model with target-resolution audio. To do this, Fre-GAN 2 only synthesizes low and high components of audio, and we introduce the inverse discrete wavelet transform (iDWT) to reproduce the target resolution audio in the generator. We also adopt the resolution-wise discriminator of Fre-GAN \cite{kim21f_interspeech}, which uses the discrete wavelet transform (DWT) as a dowmsampling method to reproduce all components without losing information. By utilizing DWT in the discriminator, Fre-GAN 2 can optimize in the sub-audio domain. The results show that Fre-GAN 2 can achieve comparable performance with fewer parameters than other models. Especially, knowledge distilled Fre-GAN 2 has a 10.91$\times$ generation acceleration, and the parameters are compressed by 21.23$\times$ than Fre-GAN.     
\begin{figure*}[!t]
    \centering
    \includegraphics[width=2.0\columnwidth]{figure1.eps}

    \caption{The generator framework of Fre-GAN 2. (a) Fre-GAN generator architecture. (b) Fre-GAN 2 generator architecture. Fre-GAN 2 only synthesizes low and high components of audio, not target-resolution audio. The target-resolution audio is reproduced by iDWT of sub-audio. (c) Fre-GAN 2 with multi-level iDWT. (d) Adversarial Periodic Feature Distillation. We use the Fre-GAN 2 with a large parameter as a teacher Fre-GAN 2, use the Fre-GAN 2 with a small parameter as a student Fre-GAN 2.}
\label{model}
\end{figure*}
\section{Fre-GAN 2}
\subsection{Generator}
To alleviate model complexity in high-resolution audio synthesis, Fre-GAN 2 only synthesizes low and high-frequency sub-audio parts of the target resolution audio from a mel-spectrogram as illustrated in Fig.\ref{model}, and reproduces the target resolution audio by applying inverse discrete wavelet transform (iDWT) as: 
\begin{equation}
    \hat{x}= \phi^{-1}(\hat{x}_{low}, \hat{x}_{high})
\end{equation}
where $\hat{x}$, $\hat{x}_{low}$, and $\hat{x}_{high}$ denote generated audio reproduced by iDWT, generated low-frequency component and high-frequency component of audio, and $\phi^{-1}$ represents iDWT. Note that DWT is invertible, so Fre-GAN 2 can reproduce the target-resolution audio from sub-audio sets. 

For model compression, we also simplify the Fre-GAN \cite{kim21f_interspeech} by removing the resolution-connected generator (RCG) and upsampled mel-spectrogram conditioning. 
While Fre-GAN adopts the RCG to progressively capture various levels of spectral distributions by summing multiple waveforms at the different resolution, Fre-GAN 2 is able to capture different frequency domain representations by sub-audio synthesis and iDWT. To explore the multi-level sub-audio synthesis, we extend Fre-GAN 2 to reproduce the target resolution audio by the multi-level iDWT as illustrated in Fig.\ref{model}. This allows the model to synthesize the audio much faster and compress the model parameter.

\subsection{Discriminator}
To train the target-resolution waveform reproduced from sub-audio, we use the resolution-wise discriminators adopted from Fre-GAN. Because the resolution-wise discriminators use the DWT instead of average pooling as a downsampling method, they disentangle the target resolution audio into sub-audio sets, which enable the reproduced audio from sub-audio sets to be trained for each frequency domain without any information loss. It is worth noting again that DWT is invertible \cite{daubechies1988orthonormal}, so the resolution-wise discriminators make our generator learn to synthesize each sub-audio. Specifically, the resolution-wise discriminators consist of resolution-wise multi-scale discriminator (RSD) and resolution-wise multi-period discriminator (RPD). RSD consists of three sub-discriminators that operate on different audio scales: target resolution audio, stacked sub-audio sets with a DWT (2$\times$ downsampled audio), stacked sub-audio sets with a multi-level DWT (4$\times$ downsampled audio). RPD consists of five sub-discriminators to capture the different periodic information from audio; We use the same period $p$ of \cite{kong2020hifi, kim21f_interspeech} which is $p\in\{2,3,5,7,11\}$. Due to the DWT in discriminators, the generator learns the consecutive pattern and periodic patterns of audio at the sub-audio domain without synthesizing the target resolution audio.

\begin{table*} [!ht]
    \caption{Objective and subjective evaluation results. The MOS is presented with $95\%$ confidence intervals. Higher is better for MOS, PESQ, and speed, and lower is better for the other metrics. Speed of $n$ kHz means that the model can synthesize $n\times1000$ audio samples per second. The numbers in () denote the synthesis speed over real-time. V1 and V2 denote the large and small parameter model, respectively. m denotes using multi-level iDWT in the generator.}
    \centering
    \begin{tabular}{l|c|c|c|c|c|r r|r r} \Xhline{3\arrayrulewidth}
        \textbf{Model} &\textbf{MOS $(\uparrow)$} &\textbf{MCD$_{13}$} &\textbf{RMSE$_{f0}$} &\textbf{PESQ} & \textbf{Param} &\multicolumn{2}{c|}{\textbf{Speed on CPU}} &\multicolumn{2}{c}{\textbf{Speed on GPU}} \\ \hline
            Ground Truth &$4.40\pm{0.04}$ &\textendash &\textendash &\textendash &\textendash &\multicolumn{2}{c|}{\textendash} &\multicolumn{2}{c}{\textendash}\\ \hline 
            WaveNet &$4.15\pm{0.06}$ &$2.14$ &$42.39$ &$2.96$ &24.73M &\multicolumn{2}{c|}{\textendash}  &$0.10$ &$(\times 0.004)$   \\ \hline
            HiFi-GAN (V1) &$4.30\pm{0.04}$ &$1.07$ &$39.64$ &$3.64$&13.92M &$60.80$ &$(\times 2.75)$ &$2,249$ &$(\times 102.03)$    \\ 

            % UnivNet-c32 &$4.30\pm{0.05}$ &$1.89$ &$41.09$ &$3.33$&14.86M &$248.55$ &$(\times 11.27)$ &$5,732$ &$(\times 259.99)$    \\ 
            Fre-GAN (V1) &$\textbf{4.37}\pm\textbf{0.04}$ &$1.00$ &$39.56$ &$\textbf{3.76}$&18.69M &$55.72$ &$(\times 2.52)$ &$2,315$ &$(\times 104.98)$ \\
            Fre-GAN 2 (V1) &$\textbf{4.37}\pm\textbf{0.04}$ &$\textbf{0.89}$ &$\textbf{39.19}$ &$3.75$&13.78M &$85.10$ &$(\times 3.86)$ &$2,762$ &$(\times 125.26)$ \\
            Fre-GAN 2 (V1, m) &$4.36\pm0.04$ &$0.92$ &$39.62$ &$3.72$&\textbf{13.24M} &$\textbf{143.84}$ &$(\times \textbf{6.52})$ &$\textbf{4,127}$ &$(\times \textbf{187.17})$\\\hline
            HiFi-GAN (V2) &$4.12\pm{0.05}$ &$1.55$ &$40.52$ &$3.25$&0.93M &$218.46$ &$(\times 9.91)$ &$11,763$ &$(\times 533.47)$    \\ 
            Fre-GAN (V2) &$\textbf{4.27}\pm\textbf{0.04}$ &$\textbf{1.25}$ &$40.05$ &$\textbf{3.52}$&1.47M &$193.21$ &$(\times 8.76)$ &$10,930$ &$(\times 495.70)$ \\
            Fre-GAN 2 (V2) &$4.24\pm0.04$ &$1.35$ &$40.31$ &$3.37$&0.91M &$342.66$ &$(\times 15.54)$ &$16,418$ &$(\times 744,61)$ \\ 
            Fre-GAN 2 (V2, m) &$4.20\pm0.04$ &$1.45$ &$\textbf{39.82}$ &$3.31$&$\textbf{0.88M}$ &$\textbf{570.71}$ &$(\times \textbf{25.88})$ &$\textbf{25,258}$ &$(\times \textbf{1,145.49})$ \\ 
       \Xhline{3\arrayrulewidth}
    \end{tabular}
    \label{MOS}
\end{table*}

We use the least-squares GAN objective \cite{mao2017least} for the discriminators and generator, and the feature matching loss for generator as following:
\begin{equation}
    \begin{split}
    \mathcal{L}&_{adv}(D)=
         \sum_{n=0}^{4}\mathbb{E}\bigg[(D_{n}^{P}(x)-1)^2+
         (D_{n}^{P}(G(s)))^2\bigg]\\
         &+\sum_{m=0}^{2}\mathbb{E}\bigg[(D_{m}^{S}(\phi^{m}(x)-1))^2+
         (D_{m}^{S}(\phi^{m}(G(s))))^2\bigg]
    \end{split}
\end{equation}
\begin{equation}
    \begin{split}
    \mathcal{L}_{adv}(G)&=\sum_{n=0}^{4}\mathbb{E}\bigg[( D_{n}^{P}(G(s))-1)^2\bigg]\\
    &+\sum_{m=0}^{2}\mathbb{E}\bigg[(D_{m}^{S}(\phi^{m}(G(s)))-1)^2\bigg]
    \end{split}
\end{equation}
\begin{equation}
\begin{split}
    \mathcal{L}_{fm}(G)= &\mathbb{E}\bigg[\sum_{i=0}^{T-1}\frac{1}{N_i}\lVert
    D^{(i)}(x)-D^{(i)}(G(s))\rVert_1\bigg]
\end{split}
\end{equation}
where $x$ denotes ground-truth audio and $s$ denotes the input mel-spectrogram of the greound-truth audio. The output of $G(s)$ is generated audio which is reproduced by iDWT. $D$ denotes discriminator which consists of $D^{P}$ and $D^{S}$. $D^{P}$ and $D^{S}$ indicate RPD and RSD, respectively. $\phi^{m}$ represents $m$-level DWT.  $T$ denotes the number of layers in the discriminator. $D^{(i)}$ is the $i^{th}$ layer feature map of the discriminator, and $N_i$ is the number of units in each layer.
\subsection{Final Loss}
With adversarial periodic feature distillation, the total loss for the student Fre-GAN 2 model is describe as following:
\begin{equation}
\begin{split}
    \mathcal{L}_{total}(G) &= \mathcal{L}_{adv}(G) + \lambda_{fm}\mathcal{L}_{fm}(G) + \lambda_{mel}\mathcal{L}_{mel}(G)\\ 
    &+ \lambda^{kd}_{adv}\mathcal{L}^{kd}_{adv}(G) + \lambda^{kd}_{fm}\mathcal{L}^{kd}_{fm}(G)
\end{split}
\end{equation}
where we set $\lambda_{fm}=2$, $\lambda_{mel}=45$, $\lambda^{kd}_{adv}=1$, and $\lambda^{kd}_{fm}=2$, and $\mathcal{L}_{mel}$ is defined as $L_1$ loss between the target mel-spectrogram and predicted mel-spectrogram which are converted from waveform by the STFT function. 
% \begin{equation}
%     \mathcal{L}_{mel}(G)=\mathbb{E}\bigg[\lVert\psi(x)-\psi(G(s))\rVert_1\bigg]
% \end{equation}
% where $\psi$ is the STFT function to convert raw audio into the corresponding mel-spectrogram, 

\section{Experiments}
\subsection{Training Setup}
We conducted experiments on the LJSpeech\footnote{\url{https://keithito.com/LJ-Speech-Dataset}} dataset which is a single English speaker dataset. We use the dataset at a sampling rate of $22,050$ Hz. The dataset contains $13,100$ audio samples, and we randomly split the dataset into train $(80\%)$, validation $(10\%)$, and test $(10\%)$ sets. Fre-GAN 2 was compared against several neural vocoders trained on the same dataset: the open-source implementation of a mixture of logistics WaveNet\footnote{\url{https://github.com/r9y9/wavenet_vocoder}}, the official implementation of HiFi-GAN\footnote{\url{https://github.com/jik876/hifi-gan}}, and our Fre-GAN implementation. We train all of the models with a large parameter (V1, initial channel of 512) and a small parameter (V2, initial channel of 128). We train the models with a batch size of 16 for the 200M steps. We use $80$ bands mel-spectrogram which is transformed with $1024$ of window size, $256$ of hop size, and $1024$ points of Fourier transform. We used AdamW optimizer \cite{loshchilov2017decoupled} with $\beta_{1}=0.8$, $\beta_{2}=0.999$, and followed the same learning rate schedule in \cite{kong2020hifi}.

\subsection{Implementation Details}
We conducted experiments based on two variations of the generator: V1, V2 with the same discriminator configuration. We simplify the Fre-GAN by removing the stacked audio layer and mel-conditional layer, and we reduce the transposed convolutional blocks by sub-audio synthesis. For Fre-GAN 2 with a single iDWT, we set the kernel sizes of transposed convolutions to [16,16,4], the upsampling sizes to [8,8,2], and the dilation rates of MRF to $[[1,1],[3,1],[5,1]]\times3$, and two audio components are projected from the block output. For Fre-GAN with multi-level iDWT, we change the kernel sizes of transposed convolutions to [16,16], the upsampling sizes to [8,8], and four audio components are projected from the block output. For RSD and RPD, we use the same architecture of Fre-GAN. The source code with specific hyperparameters and audio samples are available on the demo page.\footnote{\url{https://prml-lab-speech-team.github.io/demo/FreGAN2}}

\subsection{Audio Quality and Inference Speed}
For subjective evaluation, we conduct the naturalness mean opinion score (MOS) test. We randomly select 100 sentences from the test set. For fair evaluation, we normalize all audio samples. The samples are evaluated by at least 20 raters on a scale of 1 to 5. For comparison of models with large parameter, the MOS results show that both Fre-GAN 2 (V1) and Fre-GAN 2 (V1, multi-level iDWT) has comparable performance with Fre-GAN, and higher performance than HiFi-GAN and WaveNet as indicated in Table 1. For models with a small parameter, Fre-GAN (V2) has higher MOS than Fre-GAN 2 (V2). 

For objective evaluation, we conduct 3 objective metrics; the mel-cepstral distortion (MCD) \cite{kubichek1993mel}, $f0$ root mean square error (RMSE$_{f0}$) \cite{hayashi2017investigation}, and the perceptual evaluation of speech quality (PESQ) \cite{rix2001perceptual}. We randomly select 200 sentences from the test set. For MCD, we use the first 13 mel-frequency cepstral coefficients (MFCCs). The results show Fre-GAN 2 has better performance in MCD and RMSE$_{f0}$. The PESQ results also show Fre-GAN 2 has comparable performance with Fre-GAN. 

Although Fre-GAN 2 models have a small number of parameters, Fre-GAN 2 has comparable performance in objective and subjective metrics. We also measured the generation speed on Intel Xeon Gold 6148 2.40 GHz CPU and a single NVIDIA Titan Xp GPU. Fre-GAN 2 also shows generation acceleration in both CPU and GPU. Especially, Fre-GAN 2 with multi-level iDWT achieves 25.88 times faster than real-time on CPU and 1,145.49 times faster than real-time on GPU.
\nineth
\begin{table}[!t]
    \caption{Subjective preference scores and computational complexity comparison between Fre-GAN 2 (V1, multi-level iDWT (m)) and other models. Positive preference scores indicate that Fre-GAN 2 (V1, m) was rated better than the other model. Inference speedup is indicated that how much Fre-GAN 2 (V1, m) achieves generation acceleration than the other model. Parameter reduction (P. reduction) is indicated that how much Fre-GAN 2 (V1, M) is compressed than the other model.}
    \centering
    \begin{tabular}{lccc} \Xhline{3\arrayrulewidth}
        \textbf{Model} &\textbf{Preference} &\textbf{Speedup}  &\textbf{P. reduction}    \\ \hline
            Fre-GAN 2 (V1, m) & \multicolumn{3}{c}{Reference} \\ \hline
            Ground Truth & -0.04$\pm$0.05 & - & - \\ \hline 
            HiFi-GAN (V1) &0.21$\pm${0.06} & 1.84$\times$ & 1.05$\times$\\
            Fre-GAN (V1) & 0.11$\pm${0.05}  &1.78$\times$ &1.41$\times$ \\
            Fre-GAN 2 (V1) & 0.11$\pm${0.05}  &1.49$\times$ &1.04$\times$ \\\hline 

            % UnivNet & 0.18$\pm${0.05} & 80$\times$ & 15$\times$ \\

       \Xhline{3\arrayrulewidth}
    \end{tabular}
    \label{table:ablation}
\end{table}
\begin{table}[!t]
    \caption{Subjective preference scores and computational complexity comparison between Fre-GAN 2 (V2, m) and other models.}
    \centering
    \begin{tabular}{lccc} \Xhline{3\arrayrulewidth}
        \textbf{Model} &\textbf{Preference} &\textbf{Speedup}  &\textbf{P. reduction}    \\ \hline
            Fre-GAN 2 (V2, m) & \multicolumn{3}{c}{Reference} \\ \hline
            Ground Truth & -0.04$\pm$0.05 & - & - \\ \hline 
            HiFi-GAN (V1) &0.15$\pm${0.06} & 11.23$\times$ & 15.81$\times$\\
            Fre-GAN (V1) & 0.02$\pm${0.05}  &10.91$\times$ &21.23$\times$ \\\hline 
            % UnivNet & 0.18$\pm${0.05} & 80$\times$ & 15$\times$ \\

       \Xhline{3\arrayrulewidth}
    \end{tabular}
    \label{table:ablation}
\end{table}
\subsection{Preference Evaluation}
We conducted preference evaluation between several Fre-GAN 2 with other models. Although accelerating the inference speed with fewer parameters, Fre-GAN 2 with multi-level iDWT has comparable performance than Fre-GAN and Fre-GAN 2 with a single iDWT as indicated in Table 2.

Table 3 showed that Fre-GAN 2 (V2) has almost the same preference with HiFi-GAN (V1). Furthermore, Fre-GAN 2 (V2) has a 10.91$\times$ generation acceleration, and the parameters are compressed by 21.23$\times$ than Fre-GAN (V1).

Table 4 demonstrated knowledge distillation improved the Fre-GAN 2 performance. Especially, Fre-GAN 2 (V2) with APFD has better performance than HiFi-GAN (V1). 
\begin{table}[!t]
    \caption{Subjective preference scores between Fre-GAN 2 with knowledge distillation (Fre-GAN 2*) and other models.}
    \centering
     \begin{tabular}{lc} \Xhline{3\arrayrulewidth}
        \textbf{Model} &\textbf{Preference}     \\ \hline
            Fre-GAN 2* (V2, m)& Reference \\ \hline
            Ground Truth & -0.04$\pm$0.05\\ \hline 
           HiFi-GAN (V1) &0.12$\pm${0.06} \\
            Fre-GAN (V1) & 0.01$\pm${0.05}  \\ \hline 
            Fre-GAN 2 (V1, m) & 0.06$\pm${0.05}  \\ 
            Fre-GAN 2 (V2, m) & 0.07$\pm${0.05}  \\

       \Xhline{3\arrayrulewidth}
    \end{tabular}
    \label{table:5}
\end{table}

\subsection{Ablation Study}
We also conducted ablation study for sub-audio modelling. Durian \cite{yu2020durian} introduces multi-band WaveRNN by pseudo quadratue mirror filter bank (PQMF) \cite{nguyen1994near}, and multi-band MelGAN \cite{yang2021multi} also uses PQMF for efficient audio synthesis. To compare iDWT with PQMF, firstly, we evaluate the audio reproduced by multi-level iDWT and audio reproduced by synthesis filter of PQMF. We also train the Fre-GAN 2 with each method. The Table 6 shows that iDWT has better reconstruction performance than PQMF, and the Fre-GAN 2 with iDWT has better performance than Fre-GAN 2 with PQMF. For both ablation studies, we train each model for 500k steps. 
\begin{table}[!t]
    \caption{Subjective MOS and preference scores between Fre-GAN 2 (V2, m) with APFD and different distillation methods.}
    \centering
    \begin{tabular}{lccc} \Xhline{3\arrayrulewidth}
        \textbf{Distillation method} &\textbf{MOS} & \textbf{MCD} & \textbf{PESQ}   \\ \hline
            APFD &$\textbf{4.39}\pm\textbf{0.04}$ & 1.65 & 3.16 \\ \hline
            Fre-GAN 2 (V2, m)&$4.32\pm{0.05}$ & 1.60 & 3.16 \\\hline
            L1 distance &$4.19\pm{0.05}$ & 1.70 & 3.15 \\
            AFD &$4.32\pm{0.05}$ & 1.65 & 3.15 \\
       \Xhline{3\arrayrulewidth}
    \end{tabular}
    \label{table:ablation}
\end{table}
\begin{table}[!t]
    \caption{Ablation study for sub-audio modelling}
    \centering
    \begin{tabular}{lccc} \Xhline{3\arrayrulewidth}
        \textbf{Method} &\textbf{MOS} & \textbf{MCD} & \textbf{PESQ}   \\ \hline
            Multi-level iDWT &$-$ & $2$\times$10^{-5}$ & 4.5 \\ 
            PQMF &$-$ & 0.50 & 4.07 \\  \hline      
            Fre-GAN 2 (V2, m) &$\textbf{4.39}\pm\textbf{0.04}$ & \textbf{1.60} & \textbf{3.16} \\ 
            Fre-GAN 2 (V2, PQMF)&$4.32\pm{0.05}$ & 1.64 & 3.12 \\\hline

       \Xhline{3\arrayrulewidth}
    \end{tabular}
    \label{table:ablation}
\end{table}
\section{Conclusion}
We presented Fre-GAN 2, a fast and efficient frequency-consistent neural audio synthesis model. By adopting the inverse discrete wavelet transform in a generator, Fre-GAN 2 only synthesizes low and high components of audio, not target-resolution audio. It decreases the model parameter and accelerates the audio synthesis speed, and even the results show it also does not degrade audio quality. Furthermore, the adversarial periodic feature distillation increased the audio quality. With 21.23$\times$ compression and 10.91$\times$ audio generation acceleration, the student Fre-GAN 2 model has comparable performance on audio quality with other baselines. In this works, we focused on the effect of sub-audio synthesis with iDWT in the generator. For future work, we will apply our method to the on-device TTS model to synthesize speech from the text with a small parameter.
However, there are other techniques for generation acceleration.
The location variable convolutions \cite{zeng2021lvcnet} can be applied to accelerate inference speed, and neural architecture search \cite{luo2021lightspeech} is also utilized to discover well-performing structures with fewer parameters. 
\section{ACKNOWLEDGMENT}
This work was supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government(MSIT) (No. 2019-0-00079 , Artificial Intelligence Graduate School Program(Korea University)). 
\newpage

\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}
