\section{Details of Motivating Experiment in Figure \ref{fig:motivation}}
\label{appx:details_motivating_experiments}

In Table\,\ref{tab:os_oracle}, we described all the selected classes from ImageNet to construct $\textit{OS}_\text{oracle}$, for each target dataset. For clarity, we also visualized examples for each selected category in Figures\,\ref{fig:aircraft_oracle}--\ref{fig:cub_oracle}. Also, Table\,\ref{tab:exact_motivation} summarizes the exact numbers of the motivating experiment results.

\input{table/os_oracle}

\begin{figure}[h]
    \vspace{-0.2in}
    \centering
    \includegraphics[width=0.75\textwidth]{figs/aircraft_oracle.png}
    \vspace{-5pt}
    \caption{Visualization of examples whose classes belong to $\textit{OS}_\text{oracle}$ ($X$ = FGVC-Aircraft).}
    \label{fig:aircraft_oracle}
\end{figure}

\begin{figure}[h]
    \vspace{-0.1in}
    \centering
    \includegraphics[width=0.75\textwidth]{figs/cars_oracle.png}
    \vspace{-5pt}
    \caption{Visualization of examples whose classes belong to $\textit{OS}_\text{oracle}$ ($X$ = Stanford Cars).}
    \label{fig:cars_oracle}
\end{figure}

\clearpage
\begin{figure}[h]
    \centering
    \vspace{30pt}
    \includegraphics[width=.75\textwidth]{figs/pet_oracle.png}
    \caption{Visualization of examples whose classes belong to $\textit{OS}_\text{oracle}$ ($X$ = Oxford-IIIT Pet).}
    \label{fig:pet_oracle}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.75\textwidth]{figs/cub_oracle.png}
    \caption{Visualization of examples whose classes belong to $\textit{OS}_\text{oracle}$ ($X$ = Caltech-UCSD Birds).}
    \label{fig:cub_oracle}
\end{figure}

\input{table/exact_motivation}

\newpage

\section{Experimental Settings}\label{appx:experimental_settings}

We used eleven fine-grained datasets and four open-sets in the main experiments. We summarized the dataset configurations in the following Table\,\ref{tab:exp_setup}.
We followed the linear evaluation protocol used in recent SSL literature\cite{chen2020simple, grill2020bootstrap}: pretraining the encoder and fine-tuning only the classifier with the frozen encoder part. 

\subsection{Datasets}
Since the SSL pretraining does not rely on any label information, we incorporate the train and validation set of the original benchmarks, such as FGVC-Aircraft, Oxford 102 Flower, Describable Textures, and Food 11, to enlarge the number of training samples. Besides, in the case of CelebAMask-HQ, we exclude identities that contain less than 15 images. The original number of 30,000 images thus have reduced to 4,263, and the number of identities have reduced from 6,217 to 307. 

\input{table/exp_setup}


\vspace{-10pt}
\subsection{Encoder Pretraining}

\paragraph{Basic settings:}
We set the cost of pretraining to 8 GPU days (4 GPUs $\times$ 2 days) since the train set sizes vary depending on the pretraining dataset.
For example, we pretrained the model for longer epochs of 5K on small-scale fine-grained datasets, because one epoch is a few hundred times shorter in iteration than an epoch of \textit{OS}.
Meanwhile, although it took more than 8 GPU days, we trained 200 epochs for ImageNet open-set\,(\textit{OS}) and $X$+\textit{OS} experiments for full pretraining.

For SimCore pretraining, every 1\% sampling last 5K epochs, but every 5\% sampling last 2K epochs due to the excess of 8 GPU days.
Since SimCore with a stopping criterion had different sampling ratios according to the target dataset, in this case, we trained the model for $\min(\text{5K}, \text{10K}/p)$ epochs. If the open-set changes, we increased or decreased the epochs by the ratio between the open-set size and ImageNet size, \ie, $X$+$\text{Places365}_{\text{SimCore}}$ uses $\times 6.3$ shorter epochs than $X$+$\text{ImageNet}_{\text{SimCore}}$.
Also, when using SimCore with a stopping criterion, a budget limit is not necessarily needed. Nonetheless, in case there are sufficiently large number of core-samples in the open-set, we set the budget to $\times$50 of the target dataset size to see the effect of a small coreset.
Note that we used the stopping threshold $\tau$ as 0.95 in default.

\vspace{-12pt}
\paragraph{SimCLR method:}
When pretraining the encoder with the SimCLR method, SGD optimizer is used with an initial learning rate of 0.1 and $\ell_2$ regularization parameter 1e-4, with 512 batch size. The learning rate was scheduled by cosine annealing\cite{cosine_schedule}, decayed to zero on the last epoch.
We followed the same hyperparameters, the temperature of 0.07 and the projection dimension of 128, as in the original paper\cite{chen2020simple}. For data augmentation, we used a random resized crop, horizontal flip, color jitter, and random gray-scale.


\vspace{-12pt}
\paragraph{BYOL method:}
We used an Adam optimizer\cite{kingma2014adam} with a learning rate of 1e-3 and $\ell_2$ regularization parameter 1e-6. BYOL method is highly sensitive to the exponential moving average\,(EMA) value of a momentum encoder\cite{he2020momentum}. Therefore, we followed the same EMA scheduler as in the original implementation\cite{grill2020bootstrap}, of which the momentum starts from 0.996 to 1. We should also note that we did not use the EMA scheduler for the experiments with short epochs.

\vspace{-12pt}
\paragraph{SwAV method:}
We mostly followed the same settings as the SimCLR method, such as an SGD optimizer with a learning rate of 0.1 and weight decay of 1e-4. In the case of pretraining on ImageNet, we used the temperature value of 0.1 and an epsilon value of 0.02, while we froze the 3,000-dimensional prototypes for one epoch. For every other cases, we froze the 100-dimensional prototypes for ten epochs.


\vspace{-12pt}
\paragraph{DINO method:}
We mostly followed the pretraining setups as in the original paper\cite{caron2021emerging}. We used an AdamW optimizer\,\cite{loshchilov2017decoupled} with a learning rate of 1e-3. Cosine annealing and warmup were used during the 2\% of total epochs.
We scheduled an $\ell_2$ regularization parameter from 0.04 to 0.4, and the EMA momentum was increased from 0.996 to 1.0. Note that the $\ell_2$ regularization had not been applied to any biases and normalization parameters.
Besides, we scheduled the teacher temperature from 0.04 to 0.07 for the 40\% of total epochs and set the student temperature as 0.1.
The center momentum was 0.9, and we set the dimension of projector as 65,536. As a data augmentation technique, we used additional four local croppings\cite{caron2020unsupervised} (96$\times$96). The encoder's last layer was frozen during the first 10 epochs.
These numerous hyperparameters were highly fit to the ImageNet pretraining, whereas they did not seem effective in several fine-grained datasets.
In practice, the stopping threshold $\tau$ of 0.6 was optimal for the DINO method, which can be attributed to the large dimension of the projector.


\vspace{-12pt}
\paragraph{MAE method:}
We used an AdamW optimizer with a learning rate of 3e-4, cosine annealing, and warmup epochs of 100. Furthermore, we used the constant $\ell_2$ regularization parameter of 0.05. As in the original paper\cite{he2022masked}, we set the mask ratio as 75\%, and we normalized the pixel values when calculating the reconstruction loss.


\subsection{Fine-Tuning for Linear Evaluation}
We fine-tuned a linear classifier for 100 epochs and searched the optimal learning rate among five logarithmically spaced values from $1$ to $10^2$. We decayed the learning rate by 0.1 at 60 and 80 epochs, and any regularization techniques, such as weight decay, were not used. 


\subsection{Open-Set Semi-Supervised and Webly Supervised Learning}

\paragraph{SimCore fine-tuning:}
For SimCore fine-tuning baselines, we fine-tuned the SimCore pretrained models for 200 epochs with the batch size of 256. We used an SGD optimizer with $\ell_2$ regularization parameter of 1e-4. The learning rate started from the value of 3e-2 and was decayed by a cosine annealing scheduler.

\vspace{-12pt}
\paragraph{OpenSemi framework:}
Under the open-set semi-supervised learning (OpenSemi) framework, we used an SGD optimizer with a learning rate of 3e-2, $\ell_2$ regularization parameter of 1e-4, and cosine annealing.
In default, we trained the random initialized model for 128 epochs with iteration steps of 512 and the batch size of 64. For the models pretrained by SimCore, we used the shorter epochs of 32.

For the Self-Training algorithm\,\cite{su2021realistic}, we used the temperature of 1.0 and set the coefficient of the unlabeled loss as 0.5.
We trained a teacher network from scratch for 500 epochs and fine-tuned the SimCore initialized model for 100 epochs.

For the OpenMatch algorithm\,\cite{saito2021openmatch}, we set the coefficient of open-set entropy minimization (OEM) loss and soft open-set consistency regularization (SOCR) loss as 0.1 and 0.5, respectively.
Besides, we set the epochs to start FixMatch training as 20 for random initialized models and 5 for SimCore pretrained models.

\vspace{-12pt}
\paragraph{WeblySup framework:}
We used an SGD optimizer with a learning rate of 3e-2, $\ell_2$ regularization parameter of 1e-4, and cosine annealing.
For the Co-Teaching algorithm\,\cite{han2018co}, we used the batch size of 256 and the forget ratio of 0.2. In addition, we followed the values of other hyperparameters as in Han \etal. We set the epochs as 1,000 for training from scratch, and 200 for the experiments of SimCore initialization. Note that we modified the learning rate and forget ratio to 0.1 for Cars dataset.

For the DivideMix method\,\cite{li2020dividemix}, we trained the model with 400 epochs and the batch size of 128. We set the probability threshold as 0.2 and warmup epochs as 30 for every dataset. Since the DivideMix algorithm includes the training schemes of MixMatch\cite{berthelot2019mixmatch}, we used the default hyperparameters of the MixMatch algorithm, such as the unlabeled loss coefficient of 75 and alpha value of 0.75.

\subsection{Downstream Tasks}
For the object detection task, we used an Adam optimizer with a learning rate of 1e-4 and the batch size of 16. Besides, we set the total epochs and IoU threshold of non-maximum suppression as 30 and 0.2, respectively. We should note that the backbone network of DeepLabV3+ is frozen during fine-tuning, and we trained an FPN network from scratch.

For the semantic segmentation task, we have also frozen the SSL pretrained ResNet-50 encoder of RetinaNet. We used an SGD optimizer with the batch size of 256, $\ell_2$ regularization parameter of 1e-4, and the Nesterov momentum. The optimal learning rate is chosen among five logarithmically spaced values from 1e-1 to 1e-3.


\newpage

\section{Sampling Ratios of SimCore Experiments}

The coreset denotes a subset of the open-set that is semantically similar to the target dataset.
Since we measure this similarity on the feature space generated by a retrieval model, the sampling ratio with a stopping criterion could vary depending on several factors, such as model architectures, SSL losses, open-set, or target dataset.
That is, an image pair can be recognized as similar or dissimilar according to how they are represented.

The exact values of sampling ratios in our experiments are summarized in Tables\,\ref{tab:sampling_ratio}--\ref{tab:sampling_ratio_openset}.
In effect, SimCLR with any architecture samples less than 1\% as the coreset of Cars, but BYOL with ResNet50 samples over 30\% of the open-set.
Although the proper sampling ratios were different across each method, SimCore has consistently improved performances with the selected samples. 

\begin{table}[!h]
    \small
    \centering
    \begin{tabular}{llcccc}
    \toprule
    method & architecture & \!\!Aircraft\!\! & Cars & Pet & Birds \\
    \midrule
    SimCLR & EfficientNet & 0.59\% & 0.37\% & 2.16\% & 1.16\% \\
    SimCLR & ResNet18 & 0.31\% & 0.35\% & 0.96\% & 1.09\% \\
    SimCLR & ResNeXt50 & 0.79\% & 0.96\% & 14.36\% & 13.17\% \\
    SimCLR & ResNet101 & 0.64\% & 0.65\% & 8.51\% & 11.37\% \\
    \midrule
    BYOL & ResNet50 & 4.78\% & 31.78\% & 14.36\% & 23.38\% \\
    SwAV & ResNet50 & 26.02\% & 31.78\% & 14.36\% & 23.38\% \\
    DINO & ViT-Ti/16 & 12.19\% & 2.51\% & 14.36\% & 21.41\% \\
    MAE & ViT-B/16 & 3.23\% & 1.11\% & 8.47\% & 5.69\% \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Sampling ratios of the SimCore experiments in Table\,\ref{tab:different_arch} and Table\,\ref{tab:different_ssl}.}
    \label{tab:sampling_ratio}
\end{table}


Furthermore, SimCore samples the reasonable amount with any open-set samples, \eg, sampling number of iNaturalist\,(32K) vs. Places365\,(268K) in Indoor. Along with Figure\,\ref{fig:diff_openset} and Table\,\ref{tab:uncurated_openset}, the performance gain of SimCore is correlated with the semantic similarity between $X$ and \textit{OS}, suggesting the benefit of suitable open-sets.

\begin{table}[!h]
    \small
    \centering
    \begin{tabular}{lcccccc}
    \toprule
    \textit{OS} for SimCore & Pet & Birds & Action & Indoor & Aircraft & Cars \\
    \midrule
    COCO (0.12M) & 29.07\% & 29.60\% & 59.88\% & 37.49\% & - & - \\
    iNaturalist (0.5M) & 22.05\% & 13.03\% & 21.87\% & 6.31\% & - & - \\
    Places365 (8M) & 2.29\% & 3.73\% & 2.49\% & 3.34\% & - & - \\
    ImageNet (1.3M) & 14.36\% & 13.68\% & 15.61\% & 13.46\% & 1.03\% & 0.95\%  \\
    \midrule
    \texttt{ALL} (9.9M) & 1.85\% & 3.02\% & 2.01\% & 2.70\% & 0.20\% & 0.43\% \\
    WebVision (2.4M) & 7.52\% & 12.24\% & 8.18\% & 10.96\% & 0.39\% & 0.76\% \\
    WebFG-496 (0.05M) & - & 34.47\% & - & - & 25.32\% & 40.21\% \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Sampling ratios of the SimCore experiments in Figure\,\ref{fig:diff_openset} and Table\,\ref{tab:uncurated_openset}. The ratios are calculated based on the size of each open-set.}
    \label{tab:sampling_ratio_openset}
\end{table}


\section{Implementation Details of Feature Distribution Analysis}
\label{appx:implementation_details_feat_dist}

Figure\,\ref{fig:feat_dist} in Section \ref{subsec:analysis_of_simcore} visualizes feature distribution of each pretraining dataset. We extracted the feature embeddings with the retrieval model (\ie, pretrained encoder on the target dataset for short epochs). Then, we reduced all representations to two-dimensional vectors via t-SNE\cite{van2008visualizing}. These representation vectors are distributed on a unit ring by Gaussian kernel density estimation, as introduced in \cite{wang2020understanding}. 

In detail, we visualized the feature distributions of \textit{OS}, $X$, and the coreset sampled from \textit{OS}. For the visualization of \textit{OS}, ImageNet in this case, we randomly selected 10\% of \textit{OS} for faster convergence of t-SNE. For the coreset, SimCore samples 1\% of \textit{OS}.
Unlike in Aircraft and Cars, in Pet and Birds, the distribution of $X$ features are more overlapped with that of \textit{OS} features. This implies high semantic similarity between the open-set\,(ImageNet) and the target dataset\,(Pet or Birds). 

\newpage
\section{Sensitivity Study}

\subsection{Stopping Threshold}

The stopping criterion in SimCore algorithm requires a threshold value $\tau$. In default, we used $\tau=0.95$ throughout the experiments.
Here, we investigate the sensitivity of SimCore performance to its stopping threshold value. 
In Table\,\ref{tab:sensitivity_threshold}, we summarized both sampling ratios and classification accuracies by varying the stopping threshold.
We could observe that the $\tau$ value of $0.95$ generally performs well in both SimCLR and MAE.


\begin{table}[!h]
    \small
    \centering
    % \resizebox{\linewidth}{!}{
    \addtolength{\tabcolsep}{-1pt}
    \begin{tabular}{lcccccccccccc}
    \toprule
    & \multicolumn{6}{c}{SimCLR with ResNet50} & \multicolumn{6}{c}{MAE with ViT-B/16} \\
    \cmidrule(l{2pt}r{4pt}){2-7}\cmidrule(l{4pt}r{2pt}){8-13}
    target & \multicolumn{2}{c}{$\tau=0.99$} & \multicolumn{2}{c}{$\mathbf{\boldsymbol{\tau}=0.95}$} & \multicolumn{2}{c}{$\tau=0.9$} & \multicolumn{2}{c}{${\tau=0.99}$} & \multicolumn{2}{c}{$\mathbf{\boldsymbol{\tau}=0.95}$} & \multicolumn{2}{c}{$\tau=0.9$} \\
    \midrule
    Aircraft & \textcolor{gray}{0.21\%} & 46.7 & \textcolor{gray}{1.03\%} & 48.3 & \textcolor{gray}{5.27\%} & 47.6 & \textcolor{gray}{0.26\%} & 49.5 & \textcolor{gray}{3.23\%} & 48.1 & \textcolor{gray}{17.18\%} & 52.9 \\
    Cars & \textcolor{gray}{0.24\%} & 56.5 & \textcolor{gray}{0.95\%} & 60.3 & \textcolor{gray}{4.52\%} & 52.5 & \textcolor{gray}{0.29\%} & 62.1 & \textcolor{gray}{1.11\%} & 52.4 & \textcolor{gray}{5.22\%} & 62.9 \\
    Pet & \textcolor{gray}{1.96\%} & 79.8 & \textcolor{gray}{14.36\%} & 79.7 & \textcolor{gray}{14.36\%} & 79.7 & \textcolor{gray}{0.45\%} & 52.5 & \textcolor{gray}{8.47\%} & 77.8 & \textcolor{gray}{14.36\%} & 75.8 \\
    Birds & \textcolor{gray}{0.71\%} & 36.2 & \textcolor{gray}{13.68\%} & 37.7 & \textcolor{gray}{23.38\%} & 37.7 & \textcolor{gray}{0.43\%} & 31.1 & \textcolor{gray}{5.69\%} & 42.1 & \textcolor{gray}{23.38\%} & 35.3 \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Sensitivity study to the stopping threshold of SimCore. For each threshold value, \textcolor{gray}{first column} indicates the sampling ratio according to the threshold, and second column indicates the corresponding linear evaluation accuracy.}
    \label{tab:sensitivity_threshold}
\end{table}


\subsection{Retrieval Model}
Prior to the coreset sampling, we pretrain the encoder on a target dataset for short epochs, 1K in our experiments. We refer this model, used in the coreset selection, to a \emph{retrieval model}. This is necessary because we should measure the similarity between the features from the target dataset and from the open-set. Figure\,\ref{fig:pretrain_epoch} presents SimCore performances, given different pretraining epochs of retrieval models. For comparison, the sampling ratio is set to $p=1\%$, and the random sampling strategy is also included.

As a result, 0.5K ep. model performed marginally worse than other SimCore models, whereas 1K ep. was comparable to 5K ep. model. Still, every SimCore models outperformed random sampling or without \textit{OS}. We should note that 1K ep. pretraining takes up a small portion of the entire training process. On the basis of Pet, 1K pretraining of the retrieval model only costs 4.3\% of the total iterations.

However, one can still have a concern on the training cost for the retrieval model. To address this, we conducted an additional experiment, where the models are pretrained on only target datasets for the same number of iterations as our SimCore with the sampling ratio of 1\%.
For example, we trained the models on Pet for 18,925 epochs to exactly match the iterations to SimCore 1\% experiment.
Interestingly, we obtained the results as follows, compared to SimCore: 52.26\% in Aircraft ($+$3.81\%), 52.97\% in Cars ($-$6.03\%), 60.80\% in Pet ($-$16.33\%), and 30.59\% in Birds ($-$5.97\%).
Based on these results, we have confirmed the efficacy of the sampled coreset, as simply increasing the pretraining epochs does not result in the performance improvement.



\begin{figure}[!h]
\vspace{-3pt}
\centering
\begin{subfigure}[b]{0.4\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/pretrained/legend.pdf}
    \vspace{-12pt}
\end{subfigure}

\begin{subfigure}[b]{0.22\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/pretrained/pet.pdf}
    \vspace{-15pt}
    \caption{Pet}
\end{subfigure}
\begin{subfigure}[b]{0.22\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/pretrained/birds.pdf}
    \vspace{-15pt}
    \caption{Birds}
\end{subfigure}
\vspace{-7pt}
\caption{Performance comparisons with different pretraining epochs of retrieval models. ``w/o OS'': training on $X$ for 5K epochs. ``random'': random sampling followed by 5K epochs training on $X$+$\textit{OS}_\text{rand}$. ``$N$ ep.'': $N$ epochs pretraining on $X$ before the coreset sampling, followed by 5K epochs training on $X$+$\textit{OS}_\text{SimCore}$.}
\label{fig:pretrain_epoch}
\end{figure}


\newpage
\subsection{Number of Clusters}
To reduce the complexity, we have used 100 centroids to calculate the $\hat{f}(\mathcal{S})$ value after $k$-means clustering. Here, we show that SimCore is robust to the number of $k$. Figure\,\ref{fig:k_sensitivity} shows SimCore performance results according to different $k$ values, \{$1$, $10$, $10^2$, $10^3$, $|X|$\}, with 1\% coreset sampling from ImageNet. Except for the single centroid case, SimCore exhibited overall high accuracies.

\begin{figure}[!h]
\vspace{-3pt}
    \centering
    \begin{subfigure}[b]{0.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/pet_k.pdf}
        \caption{Pet: $|X|$ = 3,680}
        \vspace{-5pt}
        \label{fig:k_sensitivity_pet}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/cub_k.pdf}
        \caption{Birds: $|X|$ = 5,990}
        \vspace{-5pt}
        \label{fig:k_sensitivity_birds}
    \end{subfigure}
    \caption{Sensitivity study of the SimCore performance according to the number of centroids. The maximum feasible centroid number is 3,680 and 5,990 for Pet and Birds, respectively.}
    \label{fig:k_sensitivity}
\end{figure}


\section{Additional Experiments}
\subsection{Two-Stage SSL Pretraining}

If a good initialization pretrained on \textit{OS} is available, we could further pretrain this model on either $X$\,($\textit{OS} \rightarrow X$) or the union of $X$ and the coreset\,($\textit{OS} \rightarrow X+\textit{OS}_\text{SimCore}$).
Therefore, we additionally experimented with the checkpoint of the official code of SimCLR\,\cite{chen2020simple}, which is already pretrained on the ImageNet dataset. Here, we used an Adam optimizer with a learning rate of 1e-3 and reduced the epochs to 20\% compared to training from scratch.
Table\,\ref{tab:two_stage_ssl} summarizes the evaluation results of linear probing for three pretraining schemes.
In practice, our SimCore outperforms baselines in 10 out of 11 fine-grained datasets.
This demonstrates that a well-pretrained model can also be effectively exploited through two-stage pretraining schemes with the SimCore algorithm.

\begin{table*}[!h]
    \small
    \centering
    \begin{tabular}{lccccccccccc}
    \toprule
    pretrain & \!\!Aircraft\!\! & Cars & Pet & Birds & Dogs & \!Flowers\!\! & Action & Indoor & \!\!Textures\! & Faces & Food \\
    \midrule
    \textit{OS} & 41.90 & 45.79 & 78.35 & 41.95 & 61.32 & 89.67 & 68.67 & 68.04 & 71.81 & 53.91 & 88.05 \\
    $\textit{OS}\,\rightarrow\,X$ & 55.77 & 53.07 & 77.46 & 39.97 & 61.53 & 91.29 & 66.25 & 72.02 & 72.66 & \textbf{61.48} & 92.41 \\
    $\textit{OS}\,\rightarrow\,X+\textit{OS}_\text{SimCore}$ & \textbf{55.83} & \textbf{57.03} & \textbf{84.10} & \textbf{44.99} & \textbf{68.44} & \textbf{91.45} & \textbf{72.31} & \textbf{75.20} & \textbf{73.19} & 59.09 & \textbf{92.68} \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Comparisons between different pretraining schemes on eleven fine-grained datasets. The models are all initialized from the SimCLR's checkpoint that is already pretrained on ImageNet.}
    \label{tab:two_stage_ssl}
\end{table*}



\subsection{Sampling Strategy}
In our default SimCore sampling, we sampled the coreset only once after the training of the retrieval model. In practice, the sampling procedure only takes $\sim$20 minutes, mostly for computing the pairwise similarity (note that SSL pretraining takes $\sim$2 days). There are several available variations on this sampling strategy. One na\"ive way is to sample a coreset several times during the pretraining, since the model evolves and thus retrieves different coresets throughout the training.

In this sense, we resampled the coreset ($p=1\%$) three times during 5K epochs training, while maintaining the entire cost for pretraining. 
As a result, this sampling strategy yielded the performances of 49.99\% in Aircraft ($+$1.54\%), 57.63\% in Cars ($-$1.37\%), 79.75\% in Pet ($+$2.62\%), and 38.08\% in Birds ($+$1.52\%). Thus, exploring the coreset sampling strategies may be a future work that potentially improves the performance of SimCore.


\newpage
\subsection{Additional Fine-Tuning Schemes}
\label{subsec:additional_finetuning}

\paragraph{Semi-supervised learning:}

Table\,\ref{tab:semisup_finetuning} presents the experimental results on fine-tuning using various semi-supervised learning methods, including MixMatch\cite{berthelot2019mixmatch}, ReMixMatch\cite{berthelotremixmatch}, FixMatch\cite{sohn2020fixmatch}, and FlexMatch\cite{zhang2021flexmatch}.
We would note that the semi-supervised learning approach in Table\,\ref{tab:semi-supervised_learning} has simply fine-tuned models with a small portion of labeled data, following the learning protocols described in \cite{chen2020simple, grill2020bootstrap}.
In terms of implementation details, we used the default hyperparameter settings for each method and fine-tuned models for 32 epochs, with iteration steps of 512. In most cases, we found that our SimCore pretraining approach showed the significant performance improvements, regardless of semi-supervised learning methods.
This suggests that a self-supervised pretraining with coreset selection, followed by the use of semi-supervised algorithms, can be an effective approach when only a small amount of labeled data is available.



\begin{table}[!h]
    \vspace{5pt}
    \small
    \centering
    \begin{tabular}{llcccc|llcccc}
    \toprule
    method & pretrain & \!\!Aircraft\!\! & Cars & Pet & Birds & method & pretrain & \!\!Aircraft\!\! & Cars & Pet & Birds \\
    \midrule
    MixMatch & $X$ & 37.2 & 38.1 & 62.0 & 21.9 & FixMatch & $X$ & 30.4 & \textbf{33.8} & 52.8 & 13.7 \\
    MixMatch & \textit{OS} & 44.3 & 30.5 & 73.7 & 24.4 & FixMatch &  \textit{OS} & 25.1 & 21.4 & 63.8 & 13.3 \\
    MixMatch & \bf SimCore &  \textbf{45.1} & \textbf{38.6} & \textbf{74.3} & \textbf{26.0} & FixMatch &  \bf SimCore & \textbf{31.2} & 30.5 & \textbf{67.3} & \textbf{13.8} \\
    \midrule
    ReMixMatch & $X$ & 48.6 & 55.5 & 76.8 & 34.5 & FlexMatch & $X$ & 36.9 & \textbf{43.2} & 54.9 & \textbf{16.7} \\
    ReMixMatch & \textit{OS}  & \textbf{57.2} & 44.5 & 79.9 & 33.1 & FlexMatch &  \textit{OS} & 24.2 & 6.8 & 65.4 & 13.4 \\
    ReMixMatch & \bf SimCore &  56.0 & \textbf{56.4} & \textbf{82.6} & \textbf{36.7} & FlexMatch &  \bf SimCore & \textbf{41.9} & 38.0 & \textbf{71.8} & 10.7 \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Fine-tuning performances with various semi-supervised learning methods. We assumed that only 10\% of the data were annotated for semi-supervised fine-tuning.}
    \label{tab:semisup_finetuning}
\end{table}



\vspace{-12pt}
\paragraph{Active learning:}
We conducted additional experiments on various active learning methods for fine-tuning the SSL-pretrained models. Table\,\ref{tab:active_learning_finetuning} presents the experimental results with four standard active selection methods, such as Random, Entropy\cite{confidence_sampling}, Coreset\cite{coreset}, and BADGE\cite{badge}.
We randomly selected the first 10\% of the data and sequentially queried 10\% ratio using each active learning algorithm. After annotating the queried samples, we fine-tuned models for 100 epochs, starting from the checkpoint of the previous active learning round.
As an effective initialization, our SimCore pretraining approach outperformed both pretraining baselines using either $X$ or \textit{OS}, in the active learning fine-tuning schemes as well.


\begin{table}[!h]
    \vspace{5pt}
    \small
    \centering
    \addtolength{\tabcolsep}{-1pt}
    \begin{tabular}{lcccccccccccccccc}
    \toprule
    & \multicolumn{4}{c}{Aircraft} & \multicolumn{4}{c}{Cars}  & \multicolumn{4}{c}{Pet}  & \multicolumn{4}{c}{Birds} \\
    \cmidrule(l{2pt}r{2pt}){2-5} \cmidrule(l{2pt}r{2pt}){6-9} \cmidrule(l{2pt}r{2pt}){10-13} \cmidrule(l{2pt}r{2pt}){14-17}
    method & 10\% & 20\% & 30\% & 40\% & 10\% & 20\% & 30\% & 40\% & 10\% & 20\% & 30\% & 40\% & 10\% & 20\% & 30\% & 40\% \\
    \midrule
    \multicolumn{17}{c}{pretrain: $X$} \\
    \hline
    Random & 28.8 & 45.2 & 50.0 & 55.3 & 26.2 & 52.8 & 63.4 & 71.0 & 47.2 & 56.8 & 59.4 & 64.8 & 13.5 & 24.8 & 31.9 & 41.4 \\
    Entropy & - & 38.9 & 46.2 & 53.8 & - & 47.3 & 62.9 & 72.4 & - & 57.6 & 60.4 & 66.2 & - & 25.5 & 32.6 & 40.5 \\
    Coreset & - & 45.2 & 51.4 & 56.3 & - & 53.3 & 66.3 & 74.3 & - & 57.6 & 62.7 & 66.5 & - & 24.2 & 31.7 & 40.2 \\
    BADGE & - & 45.3 & 51.3 & 56.1 & - & 51.9 & \textbf{66.6} & 73.7 & - & 58.8 & 61.4 & 66.9 & - & 25.7 & 31.9 & 40.4 \\
    \midrule
    \multicolumn{17}{c}{pretrain: \textit{OS}} \\
    \hline
    Random & 20.7 & 36.0 & 45.5 & 54.8 & 10.9 & 32.5 & 47.1 & 59.5 & 35.0 & 62.0 & 68.3 & 72.8 & 11.2 & 20.7 & 20.6 & 30.1 \\
    Entropy & - & 34.1 & 43.3 & 52.0 & - & 29.0 & 45.7 & 58.6 & - & 61.9 & 69.5 & 75.1 & - & 21.8 & 30.3 & 39.3 \\
    Coreset & - & 35.3 & 44.5 & 51.8 & - & 27.2 & 43.4 & 56.1 & - & 60.3 & 71.1 & 75.4 & - & 19.9 & 28.1 & 38.4 \\
    BADGE & - & 35.9 & 46.5 & 53.6 & - & 29.5 & 45.0 & 58.3 & - & 62.4 & 70.4 & 76.0 & - & 20.8 & 30.0 & 39.7 \\
    \midrule
    \multicolumn{17}{c}{pretrain: \textbf{SimCore}} \\
    \hline
    Random & 33.7 & 49.6 & 57.6 & 62.8 & 25.2 & 54.3 & 63.9 & 71.5 & 50.5 & 66.2 & 71.6 & 75.5 & 8.1 & 24.7 & 32.1 & \textbf{43.0} \\
    Entropy & - & 43.4 & 53.4 & 62.7 & - & 49.0 & 64.2 & 73.2 & - & 65.5 & 71.0 & 76.4 & - & \textbf{25.8} & 32.7 & 42.8 \\
    Coreset & - & 50.1 & 57.5 & 63.5 & - & 52.1 & 64.9 & 73.9 & - & 67.7 & 72.9 & \textbf{78.2} & - & 23.5 & 31.3 & 41.0 \\
    BADGE & - & \textbf{50.9} & \textbf{58.7} & \textbf{65.0} & - & \textbf{53.4} & 66.5 & \textbf{74.5} & - & \textbf{70.0} & \textbf{75.2} & 77.8 & - & 24.8 & \textbf{32.9} & 42.4 \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Fine-tuning performances with various active learning methods.}
    \label{tab:active_learning_finetuning}
\end{table}




\newpage
\section{Comparisons with Hard Negative Mining}
\label{subsec:comparisons_with_hnm}

SimCore can be thought of as hard sample mining from the open-set. In SSL literature, hard negative mining\,(HNM) is a well-known technique for leveraging the informative sample features. Robinson \etal\cite{robinson2020contrastive} proposed an implicit method for mining the negatives that are similar to an anchor\,($z_i$ in Eq.\,\ref{eq:contrastive}), while Wang \etal\cite{wang2021understanding} suggested explicit sampling to inflict large gradient penalties.

Table\,\ref{tab:hnm} compares SimCore with the existing HNM approaches. Hard negative sampling\,(HNS) and explicit HNS\,(EHNS) slightly improve the performance; however, both still use all open-set samples, some of which may not be relevant to the target. In other words, they employ hard negatives for every open-set anchors, which may reduce the performance due to the distribution mismatch to the target dataset.

SimCore, on the other hand, retrieves only the coreset, reducing the need for an additional negative mining in the loss function.
Nonetheless, SimCore can be applied to any SSL method including HNM-based losses. Thus, we have tried using HNS loss after the explicit coreset sampling, achieving a small gain in the Birds dataset. 


\input{table/hnm}

\vspace{-12pt}
\paragraph{Explicit HNS with memory bank:}
We have compared SimCore with previous hard negative mining techniques: HNS\cite{robinson2020contrastive} and EHNS\cite{wang2021understanding}. However, they only slightly improved the performance because they still use all open-set samples as an anchor. To this end, we modified the sampling strategy of EHNS to better align with our conception.

Specifically, because EHNS is an explicit sampling method based on the similarities with an anchor, we can only use target data samples as the anchor and utilize hard negatives in the open-set. 
There are two ways to make it available:
\vspace{-2pt}
\begin{enumerate}
    \item We can sample a minibatch from $X$+\textit{OS} as before, but only calculates the EHNS loss with the target anchors.
    \vspace{-4pt}
    \item We can sample a minibatch only from $X$. The negative pairs are from other target sample's features and open-set features, where the open-set features are saved in a memory bank\cite{he2020momentum}.
\end{enumerate}
\vspace{-2pt}

\noindent For the first strategy, the size difference between \textit{OS} and $X$ is so large that there are not enough target samples in a minibatch, which prevented the model from being converged. The second strategy is more reasonable in that a minibatch is comprised of only $X$ samples, while the explicit hard negative sampling is done with other target features and open-set features. 

Table\,\ref{tab:hnm} presents the result of EHNS with memory bank (EHNS-m), showing that EHNS-m is not much effective. This is because negative pairs from the memory bank do not impose any gradients--only the target samples are directly involved in learning. To make the pretraining effective, therefore, it is crucial to sample a coreset and employ the entire coreset samples into the training.

