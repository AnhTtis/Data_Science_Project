\subsection{Performance Evaluation on Target Tasks}

\input{table/main_exp}

\paragraph{Linear evaluation:}

Conventional SSL literature evaluates linear probing performance with frozen pretrained encoder to measure the representation quality\cite{chen2020simple, grill2020bootstrap}.
Table\,\ref{tab:main_exp} summarizes the linear evaluation results on eleven fine-grained datasets. We note two observations from this experiment.
First, we evaluated if the coreset sampled by SimCore is qualitative as pretraining data. To this end, we set the budget size to $p=1\%$ or $p=5\%$ (\ie, sampling $p$-ratio of the open-set) and compared them with $p\%$ random sampling strategy.
In every case, SimCore outperformed the random sampling. This demonstrates that exploiting the coreset is actually crucial compared to na\"ively using random samples.
Second, using a number of cluster centroids of the target dataset is more advantageous than a single cluster, although SimCore with $k=1$ also outperformed the random sampling.


\input{table/different_arch}


Interestingly, the different trend across target datasets gives us a hint about the optimal coreset size based on the level of distribution mismatch to the open-set.
For example, in datasets like Pet and Birds, \textit{OS} pretraining was pretty effective, and in those datasets, SimCore took advantage of the large budget size.
This implies that several target datasets require more coreset samples than do others. 
However, in practice, we cannot pre-define the optimal budget size, since we do not have much knowledge about an uncurated open-set. 
Therefore, we should handle SimCore with a stopping criterion, as we already have proposed in Section\,\ref{subsec:simcore}. 

Surprisingly, SimCore with a stopping criterion highly improves the accuracy by +10.5\% (averaged over 11 datasets), compared to the $X$ pretraining. This is much larger gain compared to the large-scale \textit{OS} pretraining (+2.7\%) and 1\% random sampling (+1.3\%).
This is because SimCore adaptively samples a proper amount of coreset, and this amount differs by each target dataset.
For Aircraft and Cars, SimCore sampled around 1\% of ImageNet. This is a reasonable number, because in the ImageNet dataset\cite{deng2009imagenet}, there are actually 4 aircraft-related and 10 cars-related classes (refer to Appendix\,\ref{appx:details_motivating_experiments}), out of 1,000 classes in total.

\input{table/different_ssl}


\vspace{-12pt}
\paragraph{Different encoder architectures and SSL methods:} 
In Table\,\ref{tab:different_arch}, we have applied SimCore to different architectures: EfficientNet-B0\cite{tan2019efficientnet}, ResNet18\cite{he2016deep}, ResNeXt50\cite{xie2017aggregated}, and ResNet101\cite{he2016deep}.
Regardless of whether the encoder architecture is much smaller or larger, SimCore greatly improves pretraining on the target dataset.
Moreover, we have experimented with various SSL methods, such as BYOL\cite{grill2020bootstrap}, SwAV\cite{caron2020unsupervised}, DINO\cite{caron2021emerging}, and MAE\cite{he2022masked}, in Table\,\ref{tab:different_ssl}.
SimCore consistently demonstrates the effect of merging the coreset samples, even with the recent autoencoder-based SSL.



\begin{figure}[!t]
% \vspace{-7pt}
\begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/diff_openset/label.pdf}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/diff_openset/pet.pdf}
    \vspace{-15pt}
    \caption{Pet}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/diff_openset/birds.pdf}
    \vspace{-15pt}
    \caption{Birds}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/diff_openset/action.pdf}
    \vspace{-15pt}
    \caption{Action}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{figs/diff_openset/indoor.pdf}
    \vspace{-15pt}
    \caption{Indoor}
\end{subfigure}
\vspace{-5pt}
\caption{SimCore performances compared to the $X$ pretraining (w/o \textit{OS}). In addition to ImageNet-1k, we included MS COCO, iNaturalist 2021-mini, and Places365 as an open-set. For target datasets, we used Pet and Birds for natural image datasets and Action and Indoor for unnatural image datasets.}
\label{fig:diff_openset}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/diff_openset/places_pet_birds.pdf} \\
    \vspace{3pt}
    \includegraphics[width=\linewidth]{figs/diff_openset/inat_action_indoor.pdf}
    \vspace{-17pt}
    \caption{Selected samples of Places365 coreset\,(top) for Pet\,(left) and Birds\,(right) and iNaturalist coreset\,(bottom) for Action\,(left) and Indoor\,(right). Captions are the actual labels in each open-set.}
    \label{fig:places365_coreset}
\end{figure}

\subsection{SimCore on Various Open-Sets}
\label{subsec:simcore_on_various_opensets}

Thus far, we have used ImageNet-1k benchmark\cite{deng2009imagenet} as the open-set, which is a well-curated dataset covering general domains\cite{tian2021divide, goyal2021self}. Drawing a coreset from ImageNet has shown large performance gains in the target tasks. However, in practice, an open-set is far from what we know about; it is rather a grouping of data arbitrarily drawn from the web or database. 
Here, we show that our SimCore with any open-set robustly improves pretraining because it finds a well-matched coreset.
We experimented with three other open-sets: MS COCO\cite{lin2014microsoft}, iNaturalist 2021-mini\cite{van2018inaturalist}, and Places365\cite{zhou2017places}.

Figure\,\ref{fig:diff_openset} shows that SimCore consistently outperforms the pretraining without \textit{OS}, while the performance of SimCore depends on the open-set. 
In unnatural fine-grained datasets like Action and Indoor, using iNaturalist is not as effective as ImageNet, but it offers a good coreset for Birds. As expected, for Indoor, Places365 offers the best coreset among every open-sets because it contains a lot of scenery semantics.

Nevertheless, we have observed the \textit{unexpected} gains, such as Places365 open-set for Pet target dataset. Figure\,\ref{fig:places365_coreset} illustrates a few selected samples of those coresets. It is interesting to see that SimCore has found the animal images, although the actual labels correspond to the locations. Also, the iNaturalist coreset contains natural creatures that are held by humans or located in indoor. This might help in Action target, part of which are humans taking photos, fishing, or gardening, as well as in Indoor scenery target.




\vspace{-12pt}
\paragraph{Uncurated open-sets:}
To demonstrate the effect of SimCore in the more realistic scenario, we have tested SimCore with uncurated open-sets.
First, we used a combined dataset of all four pre-mentioned open-sets\,(\texttt{ALL}), to simulate the more large-scaled and heterogeneous open-set case.
In addition, we further used web-crawled image dataset queried by ImageNet classes\,(WebVision\,\cite{li2017webvision}), and the images queried by Aircraft+Cars+Birds classes\,(WebFG-496\,\cite{sun2021webly}). Interestingly, in the case of WebFG-496 that includes noisy instances, SimCore sampled slightly less of the actual crawled set for each target.
For example, while WebFG-496 contains 13,508 number of queried instances for Aircraft, SimCore sampled a coreset of 8,089 instances.
Table\,\ref{tab:uncurated_openset} demonstrates that these uncurated open-sets are comparable to the curated ones and significantly outperform the pretraining without open-sets. Indeed, SimCore could sample a useful coreset regardless of how uncurated an open-set is.

\begin{table}[!t]
    \small
    \centering
    \resizebox{\linewidth}{!}{
    \addtolength{\tabcolsep}{2.0pt}
    \renewcommand*{\arraystretch}{0.9}
    \begin{tabular}{lcccccc}
        \toprule
        \textit{OS} & \!\!\!Aircraft\!\!\! & Cars & Birds & Pet & \!\!\!Action\!\!\! & \!\!Indoor\!\! \\
        \midrule
        \xmark & 46.6 & 55.4 & 29.3 & 59.2 & 43.8 & 54.1 \\
        ImageNet-1k & 48.3 & 60.3 & 37.7 & 79.7 & \textbf{67.5} & 72.0 \\
        \midrule
        \texttt{ALL} & \bf 58.8 & \bf 64.8 & \textbf{38.0} & 79.5 & 67.2 & \textbf{75.1} \\
        WebVision & 48.2 & 59.1 & 36.7 & \textbf{80.3} & 67.1 & 72.6 \\
        WebFG-496 & 55.4 & 63.1 & 37.7 & - & - & - \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-5pt}
    \caption{Linear evaluation of SimCore with uncurated open-sets.}
    \label{tab:uncurated_openset}
\end{table}

