\section{Related Works}\label{sec:related_works}

\input{table/related_works}

\subsection{Self-Supervised Learning}

After Oord \etal\cite{oord2018representation} proposed an InfoNCE loss, contrastive learning algorithms began to show remarkable improvements in representation learning\cite{chen2020simple, he2020momentum, grill2020bootstrap, caron2020unsupervised, chen2021exploring, li2021efficient, caron2021emerging}. 
While a large-scale open-set enhances the generalization of learned representation \cite{jaiswal2020survey, tian2021divide, cole2022does}, recent literature has pointed out the distribution mismatch between pretraining and fine-tuning datasets \cite{el2021large, tian2021divide, goyal2021self}.
Particularly, El \etal\cite{el2021large} claimed that pretraining on ImageNet may not always be effective on the target task from different domains.
Tian \etal\cite{tian2021divide} found that pretraining with uncurated data, a more realistic scenario, deteriorates the target task performance.
Although the motivations coincide with ours, their proposed methods are focused on a single data scheme: a denoising autoencoder\cite{el2021large} only for fine-grained dataset, and distillation from expert models\cite{tian2021divide} or a novel architecture\cite{goyal2021self} for uncurated open-sets.
In contrast, we propose an explicit sampling strategy from an open-set, which becomes more effective by augmenting fine-grained dataset with well-matched samples, as well as achieving robustness to the distribution discrepancy or curation of the open-set.


\subsection{Coreset Selection from Open-Set}
In an OpenSSL problem, we denote an open-set as the additional unlabeled pretraining set that includes instances either from relevant or irrelevant domain to target dataset. The assumption of available open-set is also common in other research fields, such as open-set recognition\cite{scheirer2012toward, bendale2016towards, chen2021adversarial, vaze2021open}, webly supervised learning\cite{chen2015webly, li2020mopro, sun2021webly}, open-set\cite{oliver2018realistic, chen2020semi, saito2021openmatch,killamsetty2021retrieve} or open-world\cite{bendale2015towards, cao2021open, boult2019learning} semi-supervised learning, and open-set annotation\cite{ning2022active}, although its detailed meaning varies in each field. We summarize the details in Table\,\ref{tab:related_works}.
Especially, our OpenSSL task is to recognize coreset from a large-scale open-set, without exploiting any label information.
From this perspective, recent coreset selection approaches give us a good intuition.
Existing studies find the representative subset of the unlabeled set for active selection\cite{wei2015submodularity, coreset}, or find the subset of current task data to avoid catastrophic forgetting for continual learning\cite{aljundi2019gradient, yoon2021online, tiwari2022gcr}.
In the meantime, several works on self-supervised learning have developed a novel loss function that leverages hard negative samples, \ie, hard negative mining\cite{robinson2020contrastive, wang2021understanding}, which shares similar concepts to our coreset.
Our problem setup further requires an effective algorithm that takes into consideration of the distribution discrepancy in the open-set.