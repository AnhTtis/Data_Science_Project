\vspace{-3pt}
\section{Introduction}\label{sec:introduction}
\vspace{-2pt}

The success of deep learning in general computer vision tasks has encouraged its widespread applications to specific domains of industry and research\cite{el2021large, wei2021fine, luo2019cross}, such as facial recognition or vehicle identification.
We particularly focus on the visual recognition of fine-grained datasets, where the goal is to differentiate between hard-to-distinguish images.
However, real-world application for fine-grained tasks poses two challenges for practitioners and researchers developing algorithms.
%
First, it requires a number of experts for annotation, which incurs a large cost\cite{badge, chen2020simple, sohn2020fixmatch}.
For example, ordinary people do not have professional knowledge about aircraft types or fine-grained categories of birds. Therefore, a realistic presumption for a domain-specific fine-grained dataset is that there may be no or very few labeled samples.
% 
Second, fine-grained datasets are often re-purposed or used for various tasks according to the user's demand, which motivates development of a versatile pretrained model.
%
%%%%%
One might ask, as a target task, that bird images be classified by species or even segmented into foreground and background.
%%%%%
A good initialization model can handle a variety of annotations for fine-grained datasets, such as multiple attributes\cite{birds, aircraft, liu2015faceattributes}, pixel-level annotations\cite{birds, pet}, or bounding boxes\cite{pet, aircraft, cars}.


\begin{figure*}[!t]
\centering
\vspace{-5pt}
\includegraphics[width=0.97\textwidth]{figs/final_concept3.pdf}
\vspace{-3pt}
\caption{
Overview of an OpenSSL problem. For any downstream tasks, we pretrain an effective model with the fine-grained dataset via self-supervised learning (SSL). Here, the assumption for a large-scale unlabeled open-set in the pretraining phase is well-suited for a real-world scenario. The main goal is to find a coreset, highlighted by the blue box, among the open-set to enhance fine-grained SSL.}
\label{fig:concept}
\vspace{-2pt}
\end{figure*}

\begin{figure*}[!t]
\begin{minipage}[!t]{0.70\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/total_motive2.pdf}
    \label{fig:motivation_a}
    % \vspace{-10pt}
\end{minipage}
\hfill
\begin{minipage}[!t]{0.29\textwidth}
    \vspace{-9pt}
    \small
    % \centering
    \raggedright
    \addtolength{\tabcolsep}{-2.5pt}
    \resizebox{\linewidth}{!}{
    \renewcommand{\arraystretch}{1.09}
    \begin{tabular}{|l|l|}
    % \Xhline{2\arrayrulewidth}
    \hline
    Target\,($X$) & Classes for $\textit{OS}_\text{oracle}$ (\#)  \\
    \hline
    Aircraft & airliner, warplane, ... (8) \\
    \hline
    Cars  & convertible, jeep, ... (10) \\
    \hline
    Pet & Persian cat, beagle, ... (24) \\
    \hline
    Birds & goldfinch, junco, ... (20) \\
    \hline
    % \Xhline{2\arrayrulewidth}
    \end{tabular}}
    \label{fig:motivation_b}
    % \vspace{-10pt}
\end{minipage}
\vspace{-18pt}
\caption{Linear evaluation performance on the fine-grained target dataset. Each color corresponds to a pretraining dataset, while ``+'' means merging two datasets. The table on the right side shows the manually selected categories from an open-set\,(\textit{OS}), ImageNet-1k\cite{deng2009imagenet} in this case, according to each target dataset\,($X$).
Selected categories and exact numbers are detailed in Appendix\,\ref{appx:details_motivating_experiments}.
We followed the typical linear evaluation protocol\cite{chen2020simple, grill2020bootstrap} and used the SimCLR method\cite{chen2020simple} on ResNet50 encoder\cite{he2016deep}.}
\label{fig:motivation}
\vspace{-5pt}
\end{figure*}



Recently, self-supervised learning\,(SSL) \cite{chen2020simple, he2020momentum, grill2020bootstrap, caron2021emerging} has enabled learning how to represent the data even without annotations, such that the representations serve as an effective initialization for any future downstream tasks. Since labeling is not necessary, SSL generally utilizes an \textit{open-set}, or large-scale unlabeled dataset, which can be easily obtained by web crawling\cite{goyal2021self, tian2021divide}, for the pretraining.
In this paper, we introduce a novel Open-Set Self-Supervised Learning\,(OpenSSL) problem, where we can leverage the open-set as well as the training set of fine-grained target dataset. Refer to Figure\,\ref{fig:concept} for the overview of OpenSSL.


In the OpenSSL setup, since the open-set may contain instances irrelevant to the fine-grained dataset, we should consider the distribution mismatch. A large distribution mismatch might inhibit representation learning for the target task. 
For instance, in Figure\,\ref{fig:motivation}, SSL on the open-set\,(\textit{OS}) does not always outperform SSL on the fine-grained dataset\,($X$) because it depends on the semantic similarity between $X$ and \textit{OS}. This is in line with the previous observations\cite{ericsson2021well, el2021large, tian2021divide} that the performance of self-supervised representation on downstream tasks is correlated with similarity of pretraining and fine-tuning datasets.

To alleviate this mismatch issue, we could exploit a \textit{coreset}, a subset of an open-set, which shares similar semantics with the target dataset. As a motivating experiment, we manually selected the relevant classes from ImageNet\,($\textit{OS}_\text{oracle}$) that are supposed to be helpful according to each target dataset.
Interestingly, in Figure\,\ref{fig:motivation}, merging $\textit{OS}_\text{oracle}$ to $X$ shows a significant performance gain, and its superiority over merging the entire open-set\,($X$+\textit{OS}) or the randomly sampled subset\,($X$+$\textit{OS}_\text{rand}$) implies the necessity of a sampling algorithm for the coreset in the OpenSSL problem.

Therefore, we propose \textbf{SimCore}, a simple yet effective coreset sampling algorithm from an unlabeled open-set. 
Our main goal is to find a subset semantically similar to the target dataset.
We formulate the data subset selection problem to obtain a coreset that has a minimum distance to the target dataset in the latent space.
SimCore significantly improves performance in extensive experimental settings (eleven fine-grained datasets and seven open-sets), and shows consistent gains with different architectures, SSL losses, and downstream tasks. 
Our contributions are outlined as follows:
\vspace{-5pt}
\begin{itemize}[leftmargin=10pt]
\setlength\itemsep{-0.2em}
    \item We first propose a realistic OpenSSL task, assuming an unlabeled open-set available during the pretraining phase on the fine-grained dataset.
    \item We propose a coreset selection algorithm, SimCore, to leverage a subset semantically similar to the target dataset.
    \item Our extensive experiments with eleven fine-grained datasets and seven open-sets substantiate the significance of data selection in our OpenSSL problem.
\end{itemize}