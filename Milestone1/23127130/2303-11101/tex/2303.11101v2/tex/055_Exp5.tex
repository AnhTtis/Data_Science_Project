\subsection{Comparisons with Open-Set Semi-Supervised and Webly Supervised Learning}

\input{table/downstream_tasks}

Prior literature has similarly utilized unlabeled or noisy-labeled open-sets, such as open-set semi-supervised learning (OpenSemi)\,\cite{su2021realistic, saito2021openmatch} and webly supervised learning (WeblySup)\,\cite{chen2015webly, sun2021webly}.
OpenSemi and WeblySup work with predefined labels and co-train with the entire huge-scale open-set. 
Our OpenSSL, on the other hand, is a valuable problem itself in that a model can be pretrained without any label information and with efficient subset sampling.
We thus design an experiment under each framework, setting ($X$\,/\,\textit{OS}) as the following example: ($\text{Birds}^{\text{50\%}}$\,/\,$\text{Birds}^{\text{50\%}}$+$\text{WebFG}$) and ($\text{Birds}^{\text{100\%}}$\,/\,$\text{WebFG}^\text{Birds}$), respectively.

Table\,\ref{tab:opensemi_weblysup} summarizes the comparisons with the representative methods of each learning framework.
We observed two findings from Table\,\ref{tab:opensemi_weblysup}. (1)\,When the SimCore model, using WebFG-496 as an open-set, is simply fine-tuned on each target, it outperforms OpenSemi and WeblySup methods. (2)\,SimCore can synergize with both frameworks, serving as an effective initialization.
These results are in line with Su \etal\cite{su2021realistic}, where Self-Training with SSL pretrained model was most preferred.