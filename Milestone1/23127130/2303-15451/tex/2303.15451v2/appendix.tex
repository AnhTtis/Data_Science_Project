\appendix

\section{Neural network architecture}
\label{app:NN}

The neural network is constructed in this paper to generalize and reuse the calculation results. The details of the NN architecture are summarized below.

\begin{itemize}
\item \textbf{Layers and activation functions.} The NN input layer takes the values of each parameter, and the output layer containing 1~neuron produces the solution time estimate for a given parameter vector. The three hidden dense layers, with 512, 256, and 128~neurons, respectively, follow the input layer. Hidden layers have sigmoid activation functions; the output neuron has a linear activation function. The choice of the activation functions and the size of the hidden layers have been optimized using the KerasTuner random search scheme~\cite{omalley2019kerastuner}. The dropout layers are used after each hidden layer to prevent overfitting. The dropout rate is set equal to~0.25 in accordance with the results of the grid search procedure.

\item \textbf{Loss Function.} The mean squared error~(MSE), defined as
\begin{equation}
MSE = \frac{1}{N_t} \sum_{i=1}^{N_t} \left(T^*_i - \hat{T_i} \right)^2,
\end{equation}
where $N_t$ is the size of the training dataset, $\hat{T_i}$ is the predicted value, and ${T^*_i}$ is the input data, is a commonly used loss function for regression problems~\cite{LossSurvey, LossFuncInvestigation}. The MSE loss function minimizes the difference between the predicted and actual values in the whole data range.

\item \textbf{Model optimizer.} The ADAM (Adaptive Moment Estimation) optimizer is used for training the NN model. The initial learning rate is set to $10^{-3}$. The number of epochs as a function of the training dataset size is selected in accordance with the following empirical formula:
\begin{equation}
N_{epochs} = \frac{N_t}{50}+50.
\end{equation}
This estimate ensures stable training of the constructed model in the range of used dataset sizes ($N_t \sim 10^4-10^5$) and the number of parameters ($N \sim 20$) for the test problems performed.

\item \textbf{Cross-validation technique.} The data cross-validation technique is used to provide an effective use of the input dataset and a representative assessment of the accuracy of the model. For the proposed model, a single hold-out random subsampling method is applied~\cite{crossval}. The input dataset is split into the training and validation parts. The corresponding method applies a random permutation of the samples in the input dataset, thus mixing the data between the training and validation sets at the subsequent training sessions.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{List of optimized parameters}
\label{app:list}

The present paper deals with the optimization of three groups of parameters containing 7, 13, and 21~numerical method parameters. These parameters correspond to the BiCGStab iterative method and the classical algebraic multigrid preconditioner. The Chebyshev polynomial method is chosen as both the pre- and post-smoother (with their own method parameters), and the direct solver is used at the coarsest level; these basic methods are fixed and do not change during the optimization. The full list of the XAMG library parameters targeted for optimization and their equivalents in \textit{hypre} are summarized in Table~\ref{tab:params}. The specific parameters included in the three optimization sets are shown in Table~\ref{tab:3sets}.

Table~\ref{tab:params} also contains the \textit{baseline} and \textit{default} solver configurations used in this paper. The \textit{baseline} linear solver configuration used for the tests is a universal and robust one formulated as a result of some modeling experience. The \textit{default} configuration is constructed based on the recommendations provided in the \textit{hypre} library documentation. A slightly better variant can probably be proposed for the problems considered in the paper as a result of thorough tuning. This may slightly change the reference values and speedup numbers, but it does not affect the key results discussed in the paper.

\begin{table*}[!hbt]
\caption{The list of parameters assigned for optimization and the predefined parameter configurations.}
\begin{center}
\begin{tabular}{|l | l | l | c | c |}
\hline
XAMG & \textit{hypre} & Allowed values & \textit{baseline} & \textit{default} \\
\hline
\multicolumn{5}{|l|} {\hspace{0.5cm}Preconditioner: \texttt{Multigrid}} \\
\hline
\texttt{max\_iters}  & MaxIter & 1, 2, 3 & 1 & 1 \\
\hline
\texttt{mg\_cycle}  & CycleType & $V, W, F^*$ & $V$ & $V$ \\
\hline
\texttt{mg\_max\_row\_sum}  & MaxRowSum & $\varepsilon^{**}$, 0.05, \ldots, 1 & 1 & 0.9 \\
\hline
\texttt{mg\_nonGalerkin\_tol}  & NonGalerkinTol & 0, 0.05, \ldots, 1 & 0 & 0 \\
\hline
\texttt{mg\_coarse\_matrix\_size}  & MaxCoarseSize & 50, 100, \ldots, 500 & 500 & 9 \\
\hline
\texttt{mg\_num\_paths}  & NumPaths & 1-4 & 3 & 1 \\
\hline
\texttt{mg\_coarsening\_type}  & CoarsenType & 0, 3, 6, 8 & 6 & 6 \\
\hline
\texttt{mg\_interpolation\_type}  & InterpType & 0, 3-9, 14 & 0 & 0 \\
\hline
\texttt{mg\_strength\_threshold}  & StrongThreshold & 0, 0.1, \ldots, 0.9 & 0.25 & 0.5 \\
\hline
\texttt{mg\_trunc\_factor}  & TruncFactor & 0, 0.1, \ldots, 0.9 & 0.3 & 0 \\
\hline
\texttt{mg\_Pmax\_elements}  & PMaxElmts & 0-10 & 4 & 4 \\
\hline
\texttt{mg\_agg\_num\_levels}  & AggNumLevels & 0-10 & 2 & 0 \\
\hline
\texttt{mg\_agg\_interpolation\_type}  & AggInterpType & 1-4 & 4 & 1 \\
\hline
\texttt{mg\_agg\_trunc\_factor}  & AggTruncFactor & 0, 0.1, \ldots, 0.9 & 0.3 & 0 \\
\hline
\texttt{mg\_agg\_P12\_trunc\_factor}  & AggP12TruncFactor & 0, 0.05, \ldots, 1 & 0 & 0 \\
\hline
\texttt{mg\_agg\_Pmax\_elements}  & AggPMaxElmts & 1-10 & 4 & 4 \\
\hline
\texttt{mg\_agg\_P12max\_elements}  & AggP12MaxElmts & 1-10 & 4 & 4 \\
\hline
\multicolumn{5}{|l|} {\hspace{0.5cm}Smoother: \texttt{Chebyshev}} \\
\hline
\texttt{polynomial\_order}  & ChebyOrder & 1-4 & 2 & 2 \\
\hline
\texttt{spectrum\_fraction}  & ChebyFraction & $\varepsilon$, 0.1, \ldots, 0.9 & 0.3 & 0.3 \\
\hline
\end{tabular}
\end{center}
 * not supported by \textit{hypre}
\\ ** $\varepsilon$ is set equal to $10^{-10}$
\label{tab:params}
\end{table*}


\begin{table*}[!hbt]
\caption{Lists of parameters included in the optimization sets.}
\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
\multirow{2}*{Parameter} & \multicolumn{3}{|c|}{Optimized set} \\
\cline{2-4}
 & 7 parameters & 13 parameters & 21 parameters \\
\hline
% \multicolumn{4}{|l|} {\hspace{0.5cm}Preconditioner: \texttt{Multigrid}} \\
% \hline
\texttt{max\_iters}  & 1 & 1 & * \\
\hline
\texttt{mg\_cycle}  & $V$ & $V$ & * \\
\hline
\texttt{mg\_max\_row\_sum}  & 0.9 & * & * \\
\hline
\texttt{mg\_nonGalerkin\_tol}  & 0.0 & * & * \\
\hline
\texttt{mg\_coarse\_matrix\_size}  & 100 & * & * \\
\hline
\texttt{mg\_num\_paths}  & * & * & * \\
\hline
\texttt{mg\_coarsening\_type}  & * & * & * \\
\hline
\texttt{mg\_interpolation\_type}  & * & * & * \\
\hline
\texttt{mg\_strength\_threshold}  & 0.5 & * & * \\
\hline
\texttt{mg\_trunc\_factor}  & 0.25 & * & * \\
\hline
\texttt{mg\_Pmax\_elements}  & 4 & 4 & * \\
\hline
\texttt{mg\_agg\_num\_levels}  & * & * & * \\
\hline
\texttt{mg\_agg\_interpolation\_type}  & * & * & * \\
\hline
\texttt{mg\_agg\_trunc\_factor}  & 0.25 & * & * \\
\hline
\texttt{mg\_agg\_P12\_trunc\_factor}  & 0 & 0 & * \\
\hline
\texttt{mg\_agg\_Pmax\_elements}  & 4 & 4 & * \\
\hline
\texttt{mg\_agg\_P12max\_elements}  & 4 & 4 & * \\
\hline
% \multicolumn{4}{|l|} {\hspace{0.5cm}Smoother: \texttt{Chebyshev}} \\
% \hline
\texttt{polynomial\_order}$^{**}$  & * & * & * \\
\hline
\texttt{spectrum\_fraction}$^{**}$  & 0.3 & 0.3 & * \\
\hhline{|=|=|=|=|}
\textit{Total combinations:} & $10^6$ & $4\cdot10^{12}$ & $7\cdot10^{18}$ \\
\hline
\end{tabular}
\end{center}
 * parameter is assigned for optimization \\ 
 ** independent parameters for pre- and post-smoother
\label{tab:3sets}
\end{table*}
