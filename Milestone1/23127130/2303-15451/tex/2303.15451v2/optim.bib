% Encoding: UTF-8
@article{Wang2021prediction,
title = {Prediction of tubular solar still performance by machine learning integrated with {Bayesian} optimization algorithm},
journal = {Applied Thermal Engineering},
volume = {184},
pages = {116233},
year = {2021},
issn = {1359--4311},
doi = {10.1016/j.applthermaleng.2020.116233},
author = {Yunpeng Wang and A.W. Kandeal and Ahmed Swidan and Swellam W. Sharshir and Gamal B. Abdelaziz and M.A. Halim and A.E. Kabeel and Nuo Yang},
keywords = {Tubular solar still, Machine learning, Artificial neural network, Random forest, Regression model, Bayesian optimization},
abstract = {In this study, accurate and convenient prediction models of tubular solar still performance, expressed as hourly production, were developed by utilizing machine learning. Based on experimental data, the models were developed and compared, such as classical artificial neural network with/without Baysian optimization, random forest with/without Baysian optimization, and traditional multilinear regression. Before applying Bayesian optimization, both random forest and artificial neural network predict hourly production. But the superiority of random forest is well behaved with insignificant error. The prediction performance of random forest, artificial neural network and multilinear regression were calculated as 0.9758, 0.9614, 0.9267 for determination coefficients, and 5.21%, 7.697%, 10.911% for mean absolute percentage error, respectively. Additionally, when applying Bayesian optimization for searching most appropriate hyper parameters, the performance of artificial neural network was significantly improved by 35%. Moreover, optimization findings revealed that random forest was less sensitive to hyper parameters than artificial neural network. Based on the robustness performance and high accuracy, the random forest is recommended in predicting production of tubular solar still.}
}

@article{LossFuncInvestigation,
author = {Nie, Feiping and Zhanxuan, Hu and Li, Xuelong},
year = {2018},
month = {01},
pages = {37--52},
title = {An investigation for loss functions widely used in machine learning},
volume = {18},
journal = {Communications in Information and Systems},
doi = {10.4310/CIS.2018.v18.n1.a2}
}

@misc{omalley2019kerastuner,
    title        = {{KerasTuner}},
    author       = {O'Malley, Tom and Bursztein, Elie and Long, James and Chollet, Fran\c{c}ois and Jin, Haifeng and Invernizzi, Luca and others},
    year         = 2019,
    howpublished = {\url{https://github.com/keras-team/keras-tuner}}
}

@incollection{crossval,
title = {Cross-Validation},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {542--545},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {10.1016/B978-0-12-809633-8.20349-X},
author = {Daniel Berrar},
keywords = {Data resampling, Jackknife, -fold cross-validation, -fold random subsampling, Leave-one-out cross-validation, Overfitting, Prediction error, Single hold-out subsampling, Underfitting},
abstract = {Cross-validation is one of the most widely used data resampling methods to estimate the true prediction error of models and to tune model parameters. This article provides an introduction to the most common types of cross-validation and their related data resampling methods.}
}

@TECHREPORT{GDP_Regression,
title = {Artificial neural network regression models: Predicting {GDP} growth},
author = {Jahn, Malte},
year = {2018},
institution = {Hamburg Institute of International Economics (HWWI)},
type = {HWWI Research Papers},
number = {185},
abstract = {Artificial neural networks have become increasingly popular for statistical model fitting over the last years, mainly due to increasing computational power. In this paper, an introduction to the use of artificial neural network (ANN) regression models is given. The problem of predicting the GDP growth rate of 15 industrialized economies in the time period 1996-2016 serves as an example. It is shown that the ANN model is able to yield much more accurate predictions of GDP growth rates than a corresponding linear model. In particular, ANN models capture time trends very flexibly. This is relevant for forecasting, as demonstrated by out-of-sample predictions for 2017.},
keywords = {neural network; forecasting; panel data},
url = {https://EconPapers.repec.org/RePEc:zbw:hwwirp:185}
}

@article{MURTAGH1991183,
title = {Multilayer perceptrons for classification and regression},
journal = {Neurocomputing},
volume = {2},
number = {5},
pages = {183--197},
year = {1991},
issn = {0925-2312},
doi = {10.1016/0925-2312(91)90023-5},
author = {Fionn Murtagh},
keywords = {Multilayer perceptron, discriminant analysis, supervised classification, regression, function approximation},
abstract = {We review the theory and practice of the multilayer perceptron. We aim at addressing a range of issues which are important from the point of view of applying this approach to practical problems. A number of examples are given, illustrating how the multilayer perceptron compares to alternative, conventional approaches. The application fields of classification and regression are especially considered. Questions of implementation, i.e. of multilayer perceptron architecture, dynamics, and related aspects, are discussed. Recent studies, which are particularly relevant to the areas of discriminant analysis, and function mapping, are cited.}
}

@Article{LossSurvey,
  author   = {Qi Wang and Yue Ma and Kun Zhao and Yingjie Tian},
  journal  = {Annals of Data Science},
  title    = {{A Comprehensive Survey of Loss Functions in Machine Learning}},
  year     = {2022},
  month    = {April},
  number   = {2},
  pages    = {187--212},
  volume   = {9},
  abstract = {As one of the important research topics in machine learning, loss function plays an important role in the construction of machine learning algorithms and the improvement of their performance, which has been concerned and explored by many researchers. But it still has a big gap to summarize, analyze and compare the classical loss functions. Therefore, this paper summarizes and analyzes 31 classical loss functions in machine learning. Specifically, we describe the loss functions from the aspects of traditional machine learning and deep learning respectively. The former is divided into classification problem, regression problem and unsupervised learning according to the task type. The latter is subdivided according to the application scenario, and here we mainly select object detection and face recognition to introduces their loss functions. In each task or application, in addition to analyzing each loss function from formula, meaning, image and algorithm, the loss functions under the same task or application are also summarized and compared to deepen the understanding and provide help for the selection and improvement of loss function.},
  doi      = {10.1007/s40745-020-00253-5},
  keywords = {Loss function; Machine learning; Deep learning; Survey},
}

@article{Costa_2018,
   title={A {FFT}-based finite-difference solver for massively-parallel direct numerical simulations of turbulent flows},
   volume={76},
   ISSN={0898-1221},
   DOI={10.1016/j.camwa.2018.07.034},
   number={8},
   journal={Computers and Mathematics with Applications},
   publisher={Elsevier BV},
   author={Costa, Pedro},
   year={2018},
   pages={1853--1862}
}

@Article{KrasnopolskyIR2021,
  author  = {B. Krasnopolsky and A. Medvedev},
  journal = {Supercomputing Frontiers and Innovations},
  title   = {Evaluating Performance of Mixed Precision Linear Solvers with Iterative Refinement},
  year    = {2021},
  number  = {3},
  pages   = {4--16},
  volume  = {8},
  doi     = {10.14529/jsfi210301},
}

@Article{KrasnopolskyCPC2018,
  Title                    = {An Approach for Accelerating Incompressible Turbulent Flow Simulations Based on Simultaneous Modelling of Multiple Ensembles},
  Author                   = {B. Krasnopolsky},
  Journal                  = {Computer Physics Communications},
  Year                     = {2018},
  Pages                    = {8--19},
  Volume                   = {229},
  Doi                      = {10.1016/j.cpc.2018.03.023}
}

@article{Ahangar2010,
  author    = {Reza Gharoie Ahangar and
               Mahmood Yahyazadehfar and
               Hassan Pournaghshband},
  title     = {The Comparison of Methods Artificial Neural Network with Linear Regression
               Using Specific Variables for Prediction Stock Price in {Tehran} Stock
               Exchange},
  journal   = {CoRR},
  volume    = {abs/1003.1457},
  year      = {2010},
  archivePrefix = {arXiv},
  eprint    = {1003.1457},
}

@article{10.1145/3200691.3178495,
author = {Zhao, Yue and Li, Jiajia and Liao, Chunhua and Shen, Xipeng},
title = {Bridging the Gap between Deep Learning and Sparse Matrix Format Selection},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0362-1340},
doi = {10.1145/3200691.3178495},
abstract = {This work presents a systematic exploration on the promise and special challenges of deep learning for sparse matrix format selection---a problem of determining the best storage format for a matrix to maximize the performance of Sparse Matrix Vector Multiplication (SpMV). It describes how to effectively bridge the gap between deep learning and the special needs of the pillar HPC problem through a set of techniques on matrix representations, deep learning structure, and cross-architecture model migrations. The new solution cuts format selection errors by two thirds, and improves SpMV performance by 1.73X on average over the state of the art.},
journal = {SIGPLAN Not.},
pages = {94--108},
numpages = {15},
keywords = {convolutional neural network, SpMV, sparse matrix, format selection, deep learning}
}

@ARTICLE{Swarztrauber1974,
  author = {P.N. Swarztrauber},
  title = {A direct method for the discrete solution of separable elliptic equations},
  journal = {SIAM Journal on Numerical Analysis},
  year = {1974},
  volume = {11},
  pages = {1136--1150},
  number = {6},
  url = {http://www.jstor.org/stable/2156231}
}

@Article{Hestenes1952,
  Title                    = {Methods of Conjugate Gradients for Solving Linear Systems},
  Author                   = {Hestenes, Magnus R. and Stiefel, Eduard},
  Journal                  = {Journal of Research of the National Bureau of Standards},
  Year                     = {1952},
  Number                   = {6},
  Pages                    = {409--436},
  Volume                   = {49}
}

@Article{RBiCGStab,
  Title                    = {The reordered {BiCGStab} method for distributed memory computer systems},
  Author                   = {Boris Krasnopolsky},
  Journal                  = {Procedia Computer Science},
  Year                     = {2010},
  Note                     = {{ICCS} 2010},
  Number                   = {1},
  Pages                    = {213--218},
  Volume                   = {1},

  Abstract                 = {A new reordered formulation of the preconditioned BiCGStab iterative method for the system of linear equations with large sparse nonsymmetric matrix is presented. The algorithm is reformulated in order to improve the efficiency on distributed memory computer systems. It allows to avoid all global synchronization points of the inner product operations. The order of computations permits to overlap the communication time of the inner products by the preconditioning computations. The efficiency of the implemented method with the algebraic multigrid preconditioner is demonstrated by the scalability results for the MPI and the hybrid (MPI +SHM) programming models.},
  Doi                      = {10.1016/j.procs.2010.04.024},
  ISSN                     = {1877-0509},
  Keywords                 = {Krylov subspace iterative methods, BiCGStab, Preconditioning, Distributed memory computer systems}
}

@Article{McCalpin1995,
  Title                    = {Memory Bandwidth and Machine Balance in Current High Performance Computers},
  Author                   = {John D. McCalpin},
  Journal                  = {IEEE Computer Society Technical Committee on Computer Architecture (TCCA) Newsletter},
  Year                     = {1995},

  Month                    = dec,
  Pages                    = {19--25},

  Abstract                 = {The ratio of cpu speed to memory speed in current high-performance computers is growing rapidly, with significant implications for the design and implementation of algorithms in scientific computing. I present the results of a broad survey of memory bandwidth and machine balance for a large variety of current computers, including uniprocessors, vector processors, shared-memory systems, and districuted-memory systems. The results are analyzed in terms of the sustainable data transfer rates for uncached unit-stride vector operation for each machine, and for each class.},
  File                     = {dec95_mccalpin.ps:http\://tab.computer.org/tcca/NEWS/DEC95/dec95_mccalpin.ps:PDF}
}

@Book{Saad2003,
  Title                    = {Iterative methods for sparse linear systems, 2nd edition},
  Author                   = {Y. Saad},
  Publisher                = {SIAM},
  Year                     = {2003},

  Address                  = {Philadelpha, PA},

  Pages                    = {528}
}

@Article{Saad1986,
  Title                    = {{GMRES}: a generalized minimal residual algorithm for solving nonsymmetric linear systems},
  Author                   = {Saad, Youcef and Schultz, Martin H},
  Journal                  = {SIAM J. Sci. Stat. Comput.},
  Year                     = {1986},
  Number                   = {3},
  Pages                    = {856--869},
  Volume                   = {7},

  Address                  = {Philadelphia, PA, USA},
  Doi                      = {10.1137/0907058},
  ISSN                     = {0196-5204},
  Publisher                = {Society for Industrial and Applied Mathematics}
}

@Article{Vorst1992,
  Title                    = {{BI-CGSTAB}: a fast and smoothly converging variant of {BI-CG} for the solution of nonsymmetric linear systems},
  Author                   = {van der Vorst, H. A.},
  Journal                  = {SIAM J. Sci. Stat. Comput.},
  Year                     = {1992},
  Number                   = {2},
  Pages                    = {631--644},
  Volume                   = {13},

  Address                  = {Philadelphia, PA, USA},
  Doi                      = {10.1137/0913035},
  ISSN                     = {0196-5204},
  Publisher                = {Society for Industrial and Applied Mathematics}
}

@Electronic{hypre,
  Title                    = {{HYPRE}: {H}igh performance preconditioners},
  year                     = {2020 (acccessed 27 December, 2020)},
  Url                      = {http://www.llnl.gov/CASC/hypre/}
}

@BOOK{Trottenberg,
  title = {Multigrid},
  publisher = {Academic Press},
  year = {2001},
  author = {U. Trottenberg and C.W. Oosterlee and A. Schuller},
  address = {New York},
  numpages = {631}
}

@InProceedings{Krasnopolsky_Medvedev_2016,
  author    = {Krasnopolsky, B. and Medvedev, A.},
  booktitle = {Parallel Computing: On the Road to Exascale},
  title     = {Acceleration of Large Scale {OpenFOAM} Simulations on Distributed Systems with Multicore {CPUs} and {GPUs}},
  year      = {2016},
  pages     = {93--102},
  series    = {Advances in Parallel Computing},
  volume    = {27},
  doi       = {10.3233/978-1-61499-621-7-93},
}

@Article{Gorobets2018,
  author  = {Álvarez, X. and Gorobets, A. and Trias, F. X. and Borrell, R. and Oyarzun, G.},
  journal = {Computers \& Fluids},
  title   = {{$\mbox{{HPC}}^2$} -- {A} fully-portable, algebra-based framework for heterogeneous computing. {Application} to {CFD}},
  year    = {2018},
  pages   = {285--292},
  volume  = {173},
  doi     = {10.1016/j.compfluid.2018.01.034},
}

@Electronic{xamg_params_ref,
    title = {{XAMG:} {Parameters} {of} {the} {numerical} {methods}},
    year   = {2020 (acccessed 27 December, 2020)},
    url = {https://gitlab.com/xamg/xamg/-/wikis/docs/XAMG_params_reference}
}

@Electronic{xamg_repo,
  author  = {Boris Krasnopolsky and Alexey Medvedev},
    title = {{XAMG:} {Source code repository}},
    year  = {2022},
    note  = {{accessed: 2022-06-19}},
    url   = {https://gitlab.com/xamg/xamg}
}

@InProceedings{Medvedev2020,
  author    = {Medvedev, Alexey V.},
  booktitle = {Parallel Computing: Technology Trends},
  title     = {Towards Benchmarking the Asynchronous Progress of Non-Blocking {MPI} Operations},
  year      = {2020},
  pages     = {419--428},
  publisher = {IOS Press},
  series    = {Advances in Parallel Computing},
  volume    = {36},
  doi       = {10.3233/APC200067},
  issn      = {0927-5452},
}

@InProceedings{Mishev2008,
  author    = {Mishev, I.D. and Fedorova, N. and Terekhov, S. and Beckner, B.L. and Usadi, A.K. and Ray, M.B. and Diyankov, O.},
  booktitle = {Proceedings of ECMOR XI - 11th European Conference on the Mathematics of Oil Recovery},
  title     = {Adaptive Control for Solver Performance Optimization in Reservoir Simulation},
  year      = {2008},
  doi       = {10.3997/2214-4609.20146368},
}

@InProceedings{Bagaev2017,
  author    = {Bagaev, Dmitry and Konshin, Igor and Nikitin, Kirill},
  booktitle = {Supercomputing},
  title     = {Dynamic Optimization of Linear Solver Parameters in Mathematical Modelling of Unsteady Processes},
  year      = {2017},
  editor    = {Voevodin, Vladimir and Sobolev, Sergey},
  number    = {793},
  pages     = {54--66},
  publisher = {Springer International Publishing},
  abstract  = {The optimization of linear solver parameters in unsteady multiphase groundflow modelling is considered. Two strategies of dynamic parameters setting for the linear solver are proposed when the linear systems properties are modified during simulation in the INMOST framework. It is shown that the considered algorithms for dynamic selection of linear solver parameters provide a more efficient solution than any prescribed set of parameters. The results of numerical experiments on the INM RAS cluster are presented.},
  doi       = {10.1007/978-3-319-71255-0\_5},
  isbn      = {978-3-319-71255-0},
}

@Article{Krasnopolsky2021xamg,
  author  = {Boris Krasnopolsky and Alexey Medvedev},
  journal = {SoftwareX},
  title   = {{XAMG}: {A} library for solving linear systems with multiple right-hand side vectors},
  year    = {2021},
  pages   = {100695},
  volume  = {14},
  doi     = {10.1016/j.softx.2021.100695},
}

@Article{SuiteSparse,
  author    = {Timothy A. Davis and Yifan Hu},
  journal   = {ACM Trans. Math. Software},
  title     = {The university of {Florida} sparse matrix collection},
  year      = {2011},
  month     = {nov},
  number    = {1},
  pages     = {1--25},
  volume    = {38},
  doi       = {10.1145/2049662.2049663},
  publisher = {Association for Computing Machinery ({ACM})},
}

@Article{Bienz2016,
  author  = {Bienz, Amanda and Falgout, Robert D. and Gropp, William and Olson, Luke N. and Schroder, Jacob B.},
  journal = {SIAM Journal on Scientific Computing},
  title   = {Reducing Parallel Communication in Algebraic Multigrid through Sparsification},
  year    = {2016},
  number  = {5},
  pages   = {S332-S357},
  volume  = {38},
  doi     = {10.1137/15M1026341},
}

@Article{Yang2010,
  author   = {Yang, Ulrike Meier},
  journal  = {Numerical Linear Algebra with Applications},
  title    = {On long-range interpolation operators for aggressive coarsening},
  year     = {2010},
  number   = {2-3},
  pages    = {453-472},
  volume   = {17},
  doi      = {https://doi.org/10.1002/nla.689},
  keywords = {algebraic muligrid, long-range interpolation, aggressive coarsening, parallel computing},
}

@InProceedings{Gahvari2013,
  author    = {Gahvari, Hormozd and Gropp, William and Jordan, Kirk E. and Schulz, Martin and Yang, Ulrike Meier},
  booktitle = {2013 IEEE International Symposium on Parallel Distributed Processing, Workshops and Phd Forum},
  title     = {Systematic Reduction of Data Movement in Algebraic Multigrid Solvers},
  year      = {2013},
  pages     = {1675-1682},
  doi       = {10.1109/IPDPSW.2013.164},
}

@InProceedings{Bhowmick2006,
  author = {Sanjukta Bhowmick and Victor Eijkhout and Yoav Freund and Erika Fuentes and David E. Keyes},
  title  = {Application of Machine Learning in Selecting Sparse Linear Solvers},
  year   = {2006},
  url    = {http://www.icl.utk.edu/sites/icl/files/publications/2006/icl-utk-287-2006.pdf},
}

@InBook{Bhowmick2010,
  author    = {Bhowmick, Sanjukta and Eijkhout, Victor and Freund, Yoav and Fuentes, Erika and Keyes, David},
  editor    = {Naono, Ken and Teranishi, Keita and Cavazos, John and Suda, Reiji},
  pages     = {153--173},
  publisher = {Springer New York},
  title     = {Application of Alternating Decision Trees in Selecting Sparse Linear Solvers},
  year      = {2010},
  address   = {New York, NY},
  isbn      = {978-1-4419-6935-4},
  abstract  = {The solution of sparse linear systems, a fundamental and resource-intensive task in scientific computing, can be approached through multiple algorithms. Using an algorithm well adapted to characteristics of the task can significantly enhance the performance, such as reducing the time required for the operation, without compromising the quality of the result. However, the ``best'' solution method can vary even across linear systems generated in course of the same PDE-based simulation, thereby making solver selection a very challenging problem. In this paper, we use a machine learning technique, Alternating Decision Trees (ADT), to select efficient solvers based on the properties of sparse linear systems and runtime-dependent features, such as the stages of simulation. We demonstrate the effectiveness of this method through empirical results over linear systems drawn from computational fluid dynamics and magnetohydrodynamics applications. The results also demonstrate that using ADT can resolve the problem of ``over-fitting'', which occurs when limited amount of data is available.},
  booktitle = {Software Automatic Tuning: From Concepts to State-of-the-Art Results},
  doi       = {10.1007/978-1-4419-6935-4_10},
  url       = {https://doi.org/10.1007/978-1-4419-6935-4_10},
}

@Article{Kotthoff_2014,
  author  = {Kotthoff, Lars},
  journal = {AI Magazine},
  title   = {Algorithm Selection for Combinatorial Search Problems: A Survey},
  year    = {2014},
  month   = {Sep.},
  number  = {3},
  pages   = {48--60},
  volume  = {35},
  doi     = {10.1609/aimag.v35i3.2460},
}

@InProceedings{Kuefler2008,
  author    = {Kuefler, Erik and Chen, Tzu-Yi},
  booktitle = {Computational Science -- ICCS 2008},
  title     = {On Using Reinforcement Learning to Solve Sparse Linear Systems},
  year      = {2008},
  address   = {Berlin, Heidelberg},
  editor    = {Bubak, Marian and van Albada, Geert Dick and Dongarra, Jack and Sloot, Peter M. A.},
  pages     = {955--964},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {This paper describes how reinforcement learning can be used to select from a wide variety of preconditioned solvers for sparse linear systems. This approach provides a simple way to consider complex metrics of goodness, and makes it easy to evaluate a wide range of preconditioned solvers. A basic implementation recommends solvers that, when they converge, generally do so with no more than a 17{\%} overhead in time over the best solver possible within the test framework. Potential refinements of, and extensions to, the system are discussed.},
  isbn      = {978-3-540-69384-0},
  doi       = {10.1007/978-3-540-69384-0_100},
}

@InProceedings{Eller2012,
  author    = {Eller, Paul R. and Cheng, Jing-Ru C. and Maier, Robert S.},
  booktitle = {2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops PhD Forum},
  title     = {Dynamic Linear Solver Selection for Transient Simulations Using Machine Learning on Distributed Systems},
  year      = {2012},
  pages     = {1915--1924},
  doi       = {10.1109/IPDPSW.2012.239},
}

@Article{Jessup2016,
  author  = {Jessup, Elizabeth and Motter, Pate and Norris, Boyana and Sood, Kanika},
  journal = {SIAM Journal on Scientific Computing},
  title   = {Performance-Based Numerical Solver Selection in the {Lighthouse} Framework},
  year    = {2016},
  number  = {5},
  pages   = {S750--S771},
  volume  = {38},
  doi     = {10.1137/15M1028406},
}

@InProceedings{George2008,
  author    = {George, Thomas and Gupta, Anshul and Sarin, Vivek},
  booktitle = {2008 Eighth IEEE International Conference on Data Mining},
  title     = {A Recommendation System for Preconditioned Iterative Solvers},
  year      = {2008},
  pages     = {803--808},
  doi       = {10.1109/ICDM.2008.105},
}

@PhdThesis{GeorgePhD,
  author = {Thomas George},
  school = {Texas A\&M University},
  title  = {A Recommendation System for Preconditioned Iterative Solvers},
  year   = {2009},
  url    = {https://hdl.handle.net/1969.1/ETD-TAMU-2009-12-7458},
}

@InProceedings{Yeom2016,
  author    = {Yeom, Jae-Seung and Thiagarajan, Jayaraman J. and Bhatele, Abhinav and Bronevetsky, Greg and Kolev, Tzanio},
  booktitle = {2016 7th International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)},
  title     = {Data-Driven Performance Modeling of Linear Solvers for Sparse Matrices},
  year      = {2016},
  pages     = {32--42},
  doi       = {10.1109/PMBS.2016.009},
}

@InCollection{McInnes2003,
  author    = {L. McInnes and B. Norris and S. Bhowmick and P. Raghavan},
  booktitle = {Computational Fluid and Solid Mechanics 2003},
  publisher = {Elsevier Science Ltd},
  title     = {Adaptive sparse linear solvers for implicit {CFD} using {Newton-Krylov} algorithms},
  year      = {2003},
  address   = {Oxford},
  editor    = {K.J. Bathe},
  isbn      = {978-0-08-044046-0},
  pages     = {1024-1028},
  abstract  = {This chapter considers the simulation of three-dimensional transonic Euler flow using pseudo-transient Newton-Krylov methods. The main computation involves solving a large, sparse linear system at each Newton iteration. The chapter develops a technique for adaptively selecting the linear solver method to better match the numeric properties of the linear systems as they evolve during the course of the nonlinear iterations. In addition, the chapter also discusses the way such adaptive methods can be implemented using advanced software environments, leading to significant improvements in simulation time. Implicit solution methods play a critical role in computational dynamics (CFD) applications modeled by partial differential equations (PDEs) with different temporal and spatial scales. The solution of the classical problem of three-dimensional transonic Euler flow about an ONERA M6 wing using pseudo-transient Newton-Krylov methods are also considered in the chapter. It further solves the steady-state, three-dimensional compressible Euler equations on mapped, structured meshes using a second-order, Roe-type, and finite-volume discretization.},
  doi       = {10.1016/B978-008044046-0.50250-5},
}

@Article{Peairs2011,
  author   = {Lisa Peairs and Tzu-Yi Chen},
  journal  = {Procedia Computer Science},
  title    = {Using reinforcement learning to vary the m in {GMRES(m)}},
  year     = {2011},
  issn     = {1877-0509},
  note     = {Proceedings of the International Conference on Computational Science, ICCS 2011},
  pages    = {2257--2266},
  volume   = {4},
  abstract = {While the original GMRES(m) iterative solver assumes the restart parameter m stays ﬁxed throughout the solve, in practic varying m can improve the convergence behavior of the solver. Previous work tried to take advantage of this fact by choosing the restart value at random for each outer iteration or by adaptively changing the restart value based on a measure of the progress made towards computing the solution in successive iterations. In this work a novel application of reinforcement learning to the problem of adaptively choosing values of m is described and then compared to the two existing strategies on matrices from a range of application areas.},
  doi      = {10.1016/j.procs.2011.04.246},
  keywords = {iterative methods, reinforcement learning, GMRES(m)},
}

@Article{Demmel2005,
  author  = {Demmel, J. and Dongarra, J. and Eijkhout, V. and Fuentes, E. and Petitet, A. and Vuduc, R. and Whaley, R.C. and Yelick, K.},
  journal = {Proceedings of the IEEE},
  title   = {Self-Adapting Linear Algebra Algorithms and Software},
  year    = {2005},
  number  = {2},
  pages   = {293--312},
  volume  = {93},
  doi     = {10.1109/JPROC.2004.840848},
}

@Article{Werner_2021,
  author    = {Tino Werner},
  journal   = {Machine Learning},
  title     = {A review on instance ranking problems in statistical learning},
  year      = {2021},
  month     = {nov},
  number    = {2},
  pages     = {415--463},
  volume    = {111},
  doi       = {10.1007/s10994-021-06122-3},
  publisher = {Springer Science and Business Media {LLC}},
}

@article{kendall1938measure,
  added-at = {2010-08-01T18:03:35.000+0200},
  author = {Kendall, M. G.},
  biburl = {https://www.bibsonomy.org/bibtex/290d45cb27d725a53c9c9bc740785b295/folke},
  copyright = {Copyright © 1938 Biometrika Trust},
  description = {JSTOR: Biometrika, Vol. 30, No. 1/2 (Jun., 1938), pp. 81-93},
  interhash = {bbc3cdb27ab7144174657276608f834d},
  intrahash = {90d45cb27d725a53c9c9bc740785b295},
  issn = {00063444},
  journal = {Biometrika},
  jstor_articletype = {primary_article},
  jstor_formatteddate = {Jun., 1938},
  keywords = {correlation ir measure rank ranking},
  number = {1/2},
  pages = {81--93},
  publisher = {Biometrika Trust},
  timestamp = {2010-08-01T18:03:35.000+0200},
  title = {A New Measure of Rank Correlation},
  url = {http://www.jstor.org/stable/2332226},
  volume = 30,
  year = 1938
}

@Article{Nikitin2006ijnmf,
  author   = {Nikitin, Nikolay},
  journal  = {International Journal for Numerical Methods in Fluids},
  title    = {Third-order-accurate semi-implicit {Runge-Kutta} scheme for incompressible {Navier-Stokes} equations},
  year     = {2006},
  number   = {2},
  pages    = {221--233},
  volume   = {51},
  abstract = {Abstract A semi-implicit three-step Runge-Kutta scheme for the unsteady incompressible Navier-Stokes equations with third-order accuracy in time is presented. The higher order of accuracy as compared to the existing semi-implicit Runge–Kutta schemes is achieved due to one additional inversion of the implicit operator I-τγL, which requires inversion of tridiagonal matrices when using approximate factorization method. No additional solution of the pressure-Poisson equation or evaluation of Navier–Stokes operator is needed. The scheme is supplied with a local error estimation and time-step control algorithm. The temporal third-order accuracy of the scheme is proved analytically and ascertained by analysing both local and global errors in a numerical example. Copyright © 2005 John Wiley \& Sons, Ltd.},
  doi      = {10.1002/fld.1122},
  keywords = {Navier-Stokes equations, semi-implicit Runge-Kutta method, third-order accuracy},
}

@Article{Nikitin2006jcp,
  author   = {Nikitin, Nikolay},
  journal = {Journal of Computational Physics},
  title    = {Finite-difference method for incompressible {Navier-Stokes} equations in arbitrary orthogonal curvilinear coordinates},
  volume = {217},
  number = {2},
  pages = {759--781},
  year = {2006},
  doi      = {10.1016/j.jcp.2006.01.036},
  keywords = {Navier-Stokes equations, Turbulent flows, Curvilinear orthogonal coordinates, Central differences, Energy conservation, Semi-implicit Runge-Kutta method},
}

@Article{Trias2011,
  author    = {F. X. Trias and O. Lehmkuhl},
  journal   = {Numerical Heat Transfer, Part B: Fundamentals},
  title     = {A Self-Adaptive Strategy for the Time Integration of {Navier-Stokes} Equations},
  year      = {2011},
  number    = {2},
  pages     = {116--134},
  volume    = {60},
  doi       = {10.1080/10407790.2011.594398},
  publisher = {Taylor & Francis},
}

@Article{Moin1982,
  author    = {Moin, Parviz and Kim, John},
  journal   = {Journal of Fluid Mechanics},
  title     = {Numerical investigation of turbulent channel flow},
  year      = {1982},
  pages     = {341–377},
  volume    = {118},
  doi       = {10.1017/S0022112082001116},
  publisher = {Cambridge University Press},
}

@Book{Mitchell1996,
  author    = {Melanie Mitchell},
  publisher = {MIT Press},
  title     = {An Introduction to Genetic Algorithms},
  year      = {1996},
}

@Book{Goldberg2006,
  author    = {David E. Goldberg},
  publisher = {Pearson Education India},
  title     = {Genetic algorithms},
  year      = {2006},
}

@Article{Kohler2021rate,
  author    = {Michael Kohler and Sophie Langer},
  journal   = {The Annals of Statistics},
  title     = {{On the rate of convergence of fully connected deep neural network regression estimates}},
  year      = {2021},
  number    = {4},
  pages     = {2231 -- 2249},
  volume    = {49},
  doi       = {10.1214/20-AOS2034},
  keywords  = {curse of dimensionality, deep learning, neural networks, Nonparametric regression, rate of convergence},
  publisher = {Institute of Mathematical Statistics},
}

@InProceedings{Petrushov2021,
  author    = {Petrushov, Andrey and Krasnopolsky, Boris},
  booktitle = {Supercomputing},
  title     = {Advanced Genetic Algorithm in the Problem of Linear Solver Parameters Optimization},
  year      = {2021},
  address   = {Cham},
  editor    = {Voevodin, Vladimir and Sobolev, Sergey},
  pages     = {297--309},
  publisher = {Springer International Publishing},
  abstract  = {In several areas in computational fluid dynamics, there is a need to solve differential equations of elliptic type. After discretization on a computational grid, the problem is reduced to solving a system of linear algebraic equations (SLAE). The numerical methods widely used for high-fidelity simulations of incompressible turbulent flows require solving a sequence of SLAEs with a constant matrix and changing the right-hand side. A practically important issue is the choice of the parameters of linear solvers, which can have a tangible impact on the SLAE solution time. The paper presents an algorithm for automatic parameters selection for SLAE solving methods. The proposed algorithm finds appropriate parameters for the specified configuration of numerical methods. An approach is based on a genetic algorithm in conjunction with a neural network model. The last one is trained to predict the SLAE solution time with specific parameters. Thus the neural network model acts as a source of knowledge about the influence of each parameter on the linear solver performance. It is shown that optimal parameters set for large SLAE solving can be obtained using solution statistics for smaller SLAEs, which is an important practical aspect. The performance of the algorithm is investigated for both the model SLAEs for the Poisson equation and the SLAEs from the fluid dynamics simulations. The algorithm allows to determine the corresponding optimized parameters of the linear solver and significantly reduce the overall calculations time. The corresponding speedup can reach up to 30{\%} compared to the manually optimized solver parameters.},
  doi       = {10.1007/978-3-030-92864-3_23},
  isbn      = {978-3-030-92864-3},
}

@InProceedings{Krasnopolsky2021rsd,
  author    = {Krasnopolsky, Boris and Medvedev, Alexey},
  booktitle = {Supercomputing},
  title     = {Investigating Performance of the {XAMG} Library for Solving Linear Systems with Multiple Right-Hand Sides},
  year      = {2021},
  address   = {Cham},
  editor    = {Voevodin, Vladimir and Sobolev, Sergey},
  pages     = {337--351},
  publisher = {Springer International Publishing},
  abstract  = {The paper presents capabilities and implementation details for the newly developed XAMG library for solving systems of linear algebraic equations with multiple right-hand sides. The underlying code design principles and the basic data objects implemented in the library are described. Several specific optimizations providing significant speedup compared to alternative state of the art open-source libraries are highlighted. A great attention is paid to the XAMG library thorough performance investigation. The step-by-step evaluation, performed for two compute systems, compares the single right-hand side calculations against hypre, the performance gain due to simultaneous solution of system with multiple right-hand sides, the effect of mixed-precision calculations, and the advantages of the hierarchical MPI+POSIX shared memory hybrid programming model. The obtained results demonstrate more than twofold speedup for the XAMG library against hypre for the equal numerical method configurations. The solution of systems with multiple right-hand sides provides 2--2.5 times speedup compared to multiple solutions of systems with a single right-hand side.},
  doi       = {10.1007/978-3-030-92864-3_26},
  isbn      = {978-3-030-92864-3},
}

@InBook{Sloss2020,
  author    = {Sloss, Andrew N. and Gustafson, Steven},
  editor    = {Banzhaf, Wolfgang and Goodman, Erik and Sheneman, Leigh and Trujillo, Leonardo and Worzel, Bill},
  pages     = {307--344},
  publisher = {Springer International Publishing},
  title     = {2019 Evolutionary Algorithms Review},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-39958-0},
  abstract  = {Evolutionary algorithm research and applications began over 50 years ago. Like other artificial intelligence techniques, evolutionary algorithms will likely see increased use and development due to the increased availability of computation, more robust and available open source software libraries, and the increasing demand for artificial intelligence techniques. As these techniques become more adopted and capable, it is the right time to take a perspective of their ability to integrate into society and the human processes they intend to augment. In this review, we explore a new taxonomy of evolutionary algorithms and resulting classifications that look at five main areas: the ability to manage the control of the environment with limiters, the ability to explain and repeat the search process, the ability to understand input and output causality within a solution, the ability to manage algorithm bias due to data or user design, and lastly, the ability to add corrective measures. These areas are motivated by today's pressures on industry to conform to both societies concerns and new government regulatory rules. As many reviews of evolutionary algorithms exist, after motivating this new taxonomy, we briefly classify a broad range of algorithms and identify areas of future research.},
  booktitle = {Genetic Programming Theory and Practice XVII},
  doi       = {10.1007/978-3-030-39958-0_16},
}

@Article{Beyer2002,
  author    = {Hans-Georg Beyer and Hans-Paul Schwefel},
  journal   = {Natural Computing},
  title     = {Evolution strategies -- {A} comprehensive introduction},
  year      = {2002},
  number    = {1},
  pages     = {3--52},
  volume    = {1},
  doi       = {10.1023/a:1015059928466},
  publisher = {Springer Science and Business Media {LLC}},
}

@Book{Sivanandam2008,
  author    = {S.N. Sivanandam and S.N. Deepa},
  publisher = {Springer Berlin Heidelberg},
  title     = {Introduction to Genetic Algorithms},
  year      = {2008},
  doi       = {10.1007/978-3-540-73190-0},
}

@Misc{tensorflow2015,
  author = {Mart\'{i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  note   = {Software available from tensorflow.org},
  title  = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  year   = {2015},
  url    = {https://www.tensorflow.org/},
}

@Misc{TensorFlowDevelopers2022,
  author    = {{TensorFlow Developers}},
  title     = {TensorFlow},
  year      = {2022},
  copyright = {Apache License 2.0},
  doi       = {10.5281/ZENODO.4724125},
  publisher = {Zenodo},
}

@Article{Meinders1999,
  author   = {E.R. Meinders and K. Hanjalić},
  journal  = {International Journal of Heat and Fluid Flow},
  title    = {Vortex structure and heat transfer in turbulent flow over a wall-mounted matrix of cubes},
  year     = {1999},
  issn     = {0142-727X},
  number   = {3},
  pages    = {255--267},
  volume   = {20},
  abstract = {The paper reports on the turbulent flow structure and the distribution of the local surface heat transfer coefficient of a cube placed in a spatially periodic in-line matrix of cubes mounted on one of the walls of a plane channel. Infrared thermography was applied to measure the surface temperature at the cube walls, from which the distribution of the local heat transfer coefficient was determined. The velocity field and its structure were evaluated from Laser Doppler Anemometry (LDA) measurements and flow visualizations. The spatial periodicity was confirmed from flow field and heat transfer measurements across the entire matrix. The results showed that the flow has a marked vortex structure only in the immediate proximity of the cube, while the flow above the cube and in the streamwise corridors was only mildly distorted, except for a high level of turbulence intensity. Flow separation at the sharp leading top and side edges led to flow recirculations with subsequent flow reattachment at these faces. Reattachment of the top shear layer at the channel floor downstream of the cube produced a two-cell structure in the inter-obstacle space: an arc-type vortex in the wake of the upstream cube and a horseshoe-type vortex in front of the downstream cube. Flow instabilities caused vortex shedding at the side faces of the cube which led to periodic motions in its wake. The measured Strouhal number showed a constant value of St=0.109 over the range of Reynolds numbers considered. The observed local flow structure, in particular flow separation and reattachment, caused marked variation in the distribution of the local heat transfer coefficient, with large gradients detected particularly at the top and side faces of the cube.},
  doi      = {10.1016/S0142-727X(99)00016-8},
}

@Article{Whaley2001,
  author   = {R. {Clint Whaley} and Antoine Petitet and Jack J. Dongarra},
  journal  = {Parallel Computing},
  title    = {Automated empirical optimizations of software and the {ATLAS} project},
  year     = {2001},
  issn     = {0167-8191},
  note     = {New Trends in High Performance Computing},
  number   = {1},
  pages    = {3--35},
  volume   = {27},
  doi      = {10.1016/S0167-8191(00)00087-9},
  keywords = {ATLAS, BLAS, Portable performance, AEOS},
}

@Article{Vuduc2005,
  author   = {Richard Vuduc and James W Demmel and Katherine A Yelick},
  journal  = {Journal of Physics: Conference Series},
  title    = {OSKI: A library of automatically tuned sparse matrix kernels},
  year     = {2005},
  number   = {1},
  pages    = {521},
  volume   = {16},
  abstract = {The Optimized Sparse Kernel Interface (OSKI) is a collection of low-level primitives that provide automatically tuned computational kernels on sparse matrices, for use by solver libraries and applications. These kernels include sparse matrix-vector multiply and sparse triangular solve, among others. The primary aim of this interface is to hide the complex decisionmaking process needed to tune the performance of a kernel implementation for a particular user's sparse matrix and machine, while also exposing the steps and potentially non-trivial costs of tuning at run-time. This paper provides an overview of OSKI, which is based on our research on automatically tuned sparse kernels for modern cache-based superscalar machines.},
  doi      = {10.1088/1742-6596/16/1/071},
}

@Article{,
}

@Article{Dinkelbach2022,
  author   = {Dinkelbach, Helge {\"Ulo} and Bouhlal, Badr-Eddine and Vitay, Julien and Hamker, Fred H.},
  journal  = {Frontiers in Neuroinformatics},
  title    = {Auto-Selection of an Optimal Sparse Matrix Format in the Neuro-Simulator {ANNarchy}},
  year     = {2022},
  issn     = {1662-5196},
  volume   = {16},
  abstract = {Modern neuro-simulators provide efficient implementations of simulation kernels on various parallel hardware (multi-core CPUs, distributed CPUs, GPUs), thereby supporting the simulation of increasingly large and complex biologically realistic networks. However, the optimal configuration of the parallel hardware and computational kernels depends on the exact structure of the network to be simulated. For example, the computation time of rate-coded neural networks is generally limited by the available memory bandwidth, and consequently, the organization of the data in memory will strongly influence the performance for different connectivity matrices. We pinpoint the role of sparse matrix formats implemented in the neuro-simulator ANNarchy with respect to computation time. Rather than asking the user to identify the best data structures required for a given network and platform, such a decision could also be carried out by the neuro-simulator. However, it requires heuristics that need to be adapted over time for the available hardware. The present study investigates how machine learning methods can be used to identify appropriate implementations for a specific network. We employ an artificial neural network to develop a predictive model to help the developer select the optimal sparse matrix format. The model is first trained offline using a set of training examples on a particular hardware platform. The learned model can then predict the execution time of different matrix formats and decide on the best option for a specific network. Our experimental results show that using up to 3,000 examples of random network configurations (i.e., different population sizes as well as variable connectivity), our approach effectively selects the appropriate configuration, providing over 93% accuracy in predicting the suitable format on three different NVIDIA devices.},
  doi      = {10.3389/fninf.2022.877945},
}

@Article{Gavrilakis1992,
  author    = {Gavrilakis, S.},
  journal   = {Journal of Fluid Mechanics},
  title     = {Numerical simulation of {low-Reynolds-number} turbulent flow through a straight square duct},
  year      = {1992},
  pages     = {101--129},
  volume    = {244},
  doi       = {10.1017/S0022112092002982},
  publisher = {Cambridge University Press},
}

@Article{Pinelli2010,
  author    = {Alfredo Pinelli and Markus Uhlmann and Atsushi Sekimoto and Genta Kawahara},
  journal   = {Journal of Fluid Mechanics},
  title     = {Reynolds number dependence of mean flow structure in square duct turbulence},
  year      = {2010},
  pages     = {107--122},
  volume    = {644},
  doi       = {10.1017/S0022112009992242},
  publisher = {Cambridge University Press},
}

@Comment{jabref-meta: databaseType:bibtex;}
