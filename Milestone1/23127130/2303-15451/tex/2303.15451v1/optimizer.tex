\documentclass[preprint,12pt, a4paper]{elsarticle}
% \documentclass[final,times,2p,a4paper,twocolumn]{elsarticle}
% \documentclass[final,3p,times,twocolumn]{elsarticle}
% \documentclass[final,5p,times,twocolumn]{elsarticle}
% \documentclass[preprint,review,12pt]{elsarticle}

% \usepackage{lineno}
% \modulolinenumbers[5]
\usepackage{hyperref}
\usepackage{setspace}

\usepackage{float}
\restylefloat{table}
\usepackage{hhline}

\usepackage{graphics}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{listings}

\lstset{
language=C,                 % выбор языка для подсветки (здесь это С)
basicstyle=\scriptsize\ttfamily, % размер и начертание шрифта для подсветки
numbers=left,               % где поставить нумерацию строк (слева\справа)
numberstyle=\tiny,           % размер шрифта для номеров строк
stepnumber=0,                   % размер шага между двумя номерами строк
numbersep=5pt,                % как далеко отстоят номера строк от подсвечиваемого кода
backgroundcolor=\color{white}, % цвет фона подсветки - используем \usepackage{color}
showspaces=false,            % показывать или нет пробелы специальными отступами
showstringspaces=false,      % показывать или нет пробелы в строках
showtabs=false,             % показывать или нет табуляцию в строках
frame=single,              % рисовать рамку вокруг кода
tabsize=2,                 % размер табуляции по умолчанию равен 2 пробелам
captionpos=t,              % позиция заголовка вверху [t] или внизу [b] 
breaklines=true,           % автоматически переносить строки (да\нет)
breakatwhitespace=false, % переносить строки только если есть пробел
escapeinside={\%*}{*)}   % если нужно добавить комментарии в коде
}


\usepackage{multirow}
\usepackage{makecell}
\usepackage[usenames]{xcolor}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{lineno}

% \usepackage{fancyhdr}
% \usepackage[yyyymmdd,hhmmss]{datetime}
% \pagestyle{fancy}
% \renewcommand{\headrulewidth}{0pt}
% \lhead{}
% \chead{}
% \rhead{}
% \rfoot{Compiled on \today\ at \currenttime}
% \cfoot{}
% \lfoot{Page \thepage}

%\journal{Journal of Computational Physics}

\begin{document}

\begin{frontmatter}

% \title{Automated tuning approach for choosing the parameters of linear solvers}
\title{Automated tuning for the parameters of linear solvers}

\author{Andrey~Petrushov}
\author{Boris~Krasnopolsky\corref{author}}
% \ead{krasnopolsky@imec.msu.ru}

\cortext[author] {\textit{E-mail address:} krasnopolsky@imec.msu.ru}
 
\address{Institute of Mechanics, Lomonosov Moscow State University, \\ 119192 Moscow, Michurinsky
ave.~1, Russia}

\begin{abstract}
Robust iterative methods for solving large sparse systems of linear algebraic equations often suffer from the problem of optimizing the corresponding tuning parameters. To improve the performance for the problem of interest, the specific parameter tuning is required, which in practice can be a time-consuming and tedious task. The present paper deals with the problem of automating the optimization of the numerical method parameters to improve the performance of the mathematical physics simulations and simplify the modeling process.

The paper proposes the hybrid evolution strategy applied to tune the parameters of the Krylov subspace and algebraic multigrid iterative methods when solving a sequence of linear systems with a constant matrix and varying right-hand side. The algorithm combines the traditional evolution strategy with the pre-trained neural network, which filters the individuals in the new generation. The proposed coupling of two optimization approaches allows to integrate the adaptivity properties of the evolution strategy with a priori knowledge realized by the neural network. Furthermore, the use of the neural network as a preliminary filter allows for significant weakening of the prediction accuracy requirements and reusing the pre-trained network with a wide range of linear systems.

The detailed algorithm efficiency evaluation is performed for a set of model linear systems, including the ones from the SuiteSparse Matrix Collection and the systems from the turbulent flow simulations. The obtained results show that the pre-trained neural network can be effectively reused to optimize parameters for various linear systems, and a significant speedup in the calculations can be achieved at the cost of about 100~trial solves. The algorithm decreases the calculation time by more than 6~times for the black box matrices from the SuiteSparse Matrix Collection and by a factor of 1.5--1.8 for the turbulent flow simulations considered in the paper.
\end{abstract}

\begin{keyword}
systems of linear algebraic equations \sep algebraic multigrid method \sep hybrid evolution strategy \sep machine learning \sep parameters optimization
\end{keyword}

\end{frontmatter}

%\linenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:Intro}

The solution of systems of linear algebraic equations (SLAEs) plays an important role in mathematical modeling. Despite the variety of numerical methods developed to date, no single method provides the best possible performance in all cases. The proper selection of the method for the specific problem still remains akin to an art and requires the corresponding qualification of the researcher.

The problem of choosing linear solvers (and solver parameters) has attracted sustained interest by researchers for a long time due to its high practical impact. Several basic research directions dealing with automated optimization problems can be highlighted in the literature. The most popular ones include (\textit{i})~predicting the best numerical method for the specific SLAE, (\textit{ii})~optimizing the solver configuration for the sequence of SLAEs (e.g., for the transient simulations), and (\textit{iii})~tuning the algorithm implementation details for the specific computing platform. 

The development of assistive algorithms and tools to simplify the choice of efficient solver configurations based on some specific matrix heuristics has been a popular research topic for decades. The tremendous growth of practical interest was associated with developing the machine learning algorithms, which were realized in a series of publications~\cite{Bhowmick2006, Kuefler2008, George2008, Jessup2016, Yeom2016}, to name but a few examples. The corresponding publications investigate various machine learning algorithms, training datasets, lists of features used to characterize the SLAEs, predefined solver configurations, and others. As a result, several software tools (e.g., SALSA~\cite{Demmel2005} and Lighthouse~\cite{Jessup2016}) were developed to recommend the best solver from a predefined set of configurations. These tools, however, being good assistants for novice users, cannot provide highly optimized method configurations with the modified solver parameters set, tuned for the specific SLAE. Moreover, a significant portion of time may be required to calculate matrix heuristics and provide corresponding recommendations.

The second research topic is related to tuning linear solvers (or linear solver parameters) during the calculations when solving a sequence of SLAEs. In~\cite{McInnes2003} the heuristic algorithm of switching between several predefined linear solver configurations depending on the time integration step was discussed. The same idea is often realized based on the convergence rate history of solving SLAEs at several previous time steps. The machine learning algorithms were applied to predict optimal linear solvers in the transient simulations in~\cite{Eller2012}. An alternative approach based on a genetic algorithm for dynamic solver adaptation in reservoir simulations was investigated in~\cite{Mishev2008}. The algorithm was used for tuning 3--4~solver parameters with several basic numerical method configurations. The proposed approach resulted in an observable decrease in the calculation time, providing a speedup by a factor of~1.1--1.6.

The use of dynamic optimization algorithms is an obvious way to significantly accelerate complex simulations; however, it imposes a number of restrictions when applied to transient problems. The key issue is related to the reproducibility of the simulation results. Some random factors, like minor fluctuations in the calculation time for the specific SLAE (e.g., due to some background load of the computing system), may affect on the further choice of the method for the next time step. The corresponding variations in the choice of the numerical method to solve SLAEs may lead to variance in the obtained solution (still satisfying the prescribed solution tolerance). This is especially important when modeling stochastic problems, as any random perturbation eventually brings to a completely different instantaneous solution. Results varying from run to run may significantly complicate the analysis of simulations and are strongly undesirable in practice.

The detailed linear solver parameter optimization for the specific SLAE is a less popular~\cite{GeorgePhD}, but still important research topic. An example of a problem for which the corresponding results can be utilized is the simulation of incompressible turbulent flows. The use of high-fidelity models like large eddy simulation or direct numerical simulation (DNS) requires performing long integration in time (up to $10^5$ or even more time steps depending on the specific problem statement) to obtain reliable statistically averaged results. The specific high-order explicit or semi-implicit time integration schemes, e.g.,~\cite{Nikitin2006ijnmf, Trias2011, Moin1982}, are typically used in high-fidelity simulations. The key computational issue with these schemes is related to solving the pressure Poisson equation (PPE) at each time step. Solving the SLAEs corresponding to the elliptic differential equation takes up the most of the calculation time, and accelerating these calculations is of paramount importance for speeding up the turbulent flow simulations.

The stochastic nature of the physical process modeled does not allow for the use of dynamic adaptation algorithms during the calculations due to the reproducibility issues mentioned above. However, the sequence of SLAEs solved typically varies in the right-hand side (RHS) vector, preserving the matrix constant during the whole calculation. The RHS vector is determined as a divergence of the predicted velocity field, and in a sense, they are close to each other when modeling statistically steady turbulent flow. This gives reasons to believe that the optimized numerical method configuration determined for some specific RHS vector can be effectively reused to solve a sequence of PPEs in the whole simulation.

The combination of Krylov subspace and multigrid iterative methods is a popular choice for solving SLAEs corresponding to PPEs. These methods, however, have lots of tuning parameters. The algebraic multigrid method~\cite{Trottenberg} is an evident example of such an algorithm: the basic set of tuning parameters affecting the method productivity is counted in several tens, while the per-level multigrid hierarchy parameters specification increases this number to hundreds. While being among the most powerful and robust numerical methods for solving large sparse SLAEs, this method suffers serious difficulties due to the need to choose the corresponding set of parameters for the specific SLAE of interest. The method has no default universal parameters able to provide an acceptable performance level and be applicable for a wide range of SLAEs. The set of parameters optimal for one system can demonstrate poor convergence or divergence for the other SLAEs, even the ones corresponding to the same type of differential equation. Having only rough formal guidelines on how to choose the method parameters, minimization of the SLAE solution time becomes a challenging issue where the researchers can only rely on their experience and intuition. In practice, this issue complicates the large-scale simulations and increases the calculation time, as in most cases, far from optimal linear solver configurations are used to perform the calculations.

The current paper deals with the problem mentioned above and proposes an automated linear solver parameter optimization algorithm. The paper focuses on tuning multigrid-related method parameters and solving SLAEs resulting from discretization of the partial differential equations of elliptic type, with a specific application on modeling incompressible turbulent flows. The method proposed, however, has no strict limitations on the scope of applicability and can be applied to a wider range of applications and areas of mathematical physics.

The rest of the paper is organized as follows. The second section formulates the basic algorithm requirements and limitations caused by the specific problem statement of interest. The proposed optimization algorithm is presented in the third section. The software used for algorithm evaluation and some implementation details, together with the performance evaluation methodology, are summarized in the fourth section. The fifth section presents methodological study results and analyzes the influence of optimization algorithm control parameters. The sixth and seventh sections deal with the proposed algorithm efficiency evaluation. The sixth section investigates the potential of reusing the pre-trained neural networks when optimizing the solver configurations for various linear systems, and the seventh section demonstrates the effect of automated parameter tuning when performing turbulent flow simulations. Finally, the conclusion summarizes the paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem formulation}
\label{sec:problem}

\subsection{Basic requirements}

The current paper deals with developing an automated linear solver parameters optimization algorithm for minimizing the time to solve a sequence of SLAEs
\begin{gather}
\textbf{A} \textbf{x}_i = \textbf{b}_i, \, \, \, \, \, \textbf{A} \in \mathbb{R}^{n \times n}, \, \, \, \textbf{x}_i, \textbf{b}_i \in \mathbb{R}^{n} \label{eq:system}
\end{gather}
occurring in turbulent flow simulations. Accounting for the specific problem statement, the following conditions and limitations are formulated:
\begin{itemize}
  \item as a consequence of the stochastic nature of high-fidelity turbulent flow models and the limitation of the simulation results reproducibility, the optimization procedure can be applied once at the beginning of the simulation;
  \item the basic numerical methods configuration, the list of optimizing parameters, and the parameter optimization range are specified by the user; 
  \item the algorithm must be capable of effectively tuning at least several tens of parameters;
  \item the tuned set of parameters must provide an observable calculation speedup to compensate the time spent on optimization;
  \item some a priori information can be used to increase the optimization algorithm efficiency and performance; however, this information must be sufficiently versatile (i.e., valid for at least some group of SLAEs).
\end{itemize}

The problem~\eqref{eq:system} focuses on a series of SLAEs with constant matrix. This allows for the time-consuming initialization stage to be performed only once before the start of calculations. All the results presented below account for the corresponding SLAE solution time only, ignoring the time spent on solver initialization. The algorithm developed, however, can be used to optimize the solver while accounting for the initialization time.

Without loss of generality, the further narration will be focused on the BiCGStab iterative method~\cite{Vorst1992} with the classical algebraic multigrid preconditioner~\cite{Trottenberg} -- a popular combination for solving systems of linear algebraic equations derived from elliptic differential equations. Algebraic multigrid is an example of a method with many tuning parameters that have a significant impact on the efficiency when solving specific SLAE.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Numerical method parameters specification}

In practice, the parameters of the optimizing numerical method can be represented in three ways: as a continuous parameter range, as a range of allowed discrete values, or as a list of allowed discrete values. All of them are managed by the optimization algorithm in a unified manner. An intermediate parameter mapping is introduced to represent all the parameters in the form of a range of discrete values. The continuous parameter range is discretized with some user-defined stepping to match the corresponding mapping. This method does not allow for fine-tuning of the parameters between the given points. Estimating the practical significance of this issue as well as possible solutions will be the topics of further research.

The choice of a specific parameter may affect the presence of some other parameters. For example, switching between different smoothers (e.g., Jacobi and Chebyshev polynomial methods) changes the list of dependent parameters: the relaxation factor valid for the Jacobi method is not used with the Chebyshev polynomial method, and the polynomial order of the Chebyshev method is useless for the Jacobi method. The current implementation of the optimization algorithm does not allow performing optimization with a varying list of parameters. Specifically, the list of methods (main solver, preconditioner, pre- and post-smoother, and coarse grid solver) is fixed and must be specified by the user as the input parameters. This limitation, however, is of the technical nature of integration with the linear solver and will be eliminated in the future. The detailed list of parameters assigned for tuning in this paper is summarized in~\ref{app:list}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Optimization algorithm}

Let us denote the following notation. The vector with $N$ parameters is written in the form 
\begin{gather}
\textbf{p} = (p_1, p_2, ... , p_N), \, \textbf{p} \in \mathcal{P}, \\
\mathcal{P} = \{ p_1^i, i=\overline{1,K_1} \} \times \{ p_2^i, i=\overline{1,K_2} \} \times \ldots \times \{ p_N^i, i=\overline{1,K_N} \},
\end{gather}
where $\mathcal{P}$ is the parameter search space, $p_j$ corresponds to a specific numerical method parameter, and each parameter $p_j$ has $K_j$ possible discrete values. The total number of combinations for the parameter vector \textbf{p} equals:
\begin{gather}
K = K_1 \cdot K_2 \cdot ... \cdot K_N.\label{eq:combinations}
\end{gather}
The SLAE solution time with the specific parameter vector \textbf{p} is denoted as
\begin{gather}
T({\textbf{p}}) = T(p_1, p_2, ... , p_N), \label{eq:regression_equation}
\end{gather}
and the role of the optimization algorithm is to find the vector \textbf{p} minimizing the corresponding SLAE solution time.

The form of the expression~\eqref{eq:combinations} clearly shows that simple random search algorithms~\cite{Sivanandam2008} are inapplicable for optimizing problems with several tens of parameters and that more flexible algorithms must be constructed. An example of such an algorithm successfully used for the optimization of the linear solver parameters in~\cite{Mishev2008} is the genetic algorithm, a popular variation of the evolutionary algorithms~\cite{Sivanandam2008, Sloss2020}. The method was applied for optimizing problems with 3 and 4~parameters in the transient simulation and demonstrated observable calculation speedups. The current paper, however, focuses on a significantly larger number of optimizing parameters, and this factor substantially complicates the problem. Another issue is related to optimization prior to simulation: the test runs to perform the optimization produce overhead for the entire simulation, and the number of these trials must be minimized. 

This paper proposes the hybrid variant of the evolution strategy~(ES)~\cite{Beyer2002}, another popular variation of the evolutionary algorithms. The hybrid ES combines the next-generation vectors produced by the mutation operators with the ones proposed by the neural network~(NN). The neural network is used as an input filter, selecting the best individuals from a pool of randomly generated candidates. The basic idea behind this approach is to combine two features: adaptivity to the specific problem of interest provided by the evolution strategy, and reuse of a priori knowledge realized by the neural network. The list of optimizing parameters may consist of many parameters, leading to a large search space. The search space contains regions with knowingly inefficient parameter combinations, which can be ignored without test SLAE solutions, thus significantly reducing the parameter search space and the number of time-consuming trials.

The algorithm discussed in detail below has an important feature: the proposed scenario of using NN does not require an exact prediction of the specific SLAE solution time, which in practice can be a challenging issue. The key requirement for the NN is only a correct ranking of the parameter configurations. This aspect increases the versatility of the algorithm developed and raises the potential for reusing the pre-trained NN with various SLAEs and compute platforms.

%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evolution strategy}
\label{sec:ES}
An evolution strategy is a population-based search algorithm suitable for a wide range of optimization problems. A typical ES cycle starts with creating the new generation, which contains several individuals, using the pool of parents. The best individuals are chosen based on the fitness value evaluation and added to a new pool of parents to produce the next generation of individuals. The corresponding iterative procedure of constructing the next generation, evaluating fitness values, and selecting the candidates is repeated until some convergence criteria is achieved.

The proposed optimization algorithm is based on the $(1+\lambda)-ES$ version of the evolution strategy with two mutation operators (Figure~\ref{fig:sketch}). The algorithm uses the following configuration of the ES:
\begin{itemize}
  \item the single parent is used for each individual, $\rho = 1$ (i.e., new offsprings are clones of the parent);
  \item the soft mutation operator, $M_S$, produces random perturbations in the cloned offspring:
\begin{gather}
\textbf{p} = M_S(\tilde{\textbf{p}}) = ( \tilde{p}_1+s_1, \tilde{p}_2+s_2, ... ,\tilde{p}_N+s_N ),
\end{gather}
where $s_j = \{0,\pm \delta_j\}$, and $\delta_j$ is the $j$-th parameter change quantum; the probability of choosing the corresponding value can vary depending on the specific algorithm configuration;
  \item the randomization operator, $M_R$, is used to introduce some new randomly generated individuals that are generally independent of the cloned offspring;
  \item the selection operator chooses the best individual with the lowest fitness value, $\mu = 1$;
  \item the simple \textit{truncation} technique is applied for choosing the best individual;
  \item the parent is included into the selection process (\textit{plus} selection).
\end{itemize}

The combination of two mutation operators pursues the goal of providing the mechanisms for both local adaptation of the current best individual and searching for completely different individuals in the other areas of the parameter search space. While playing an important role in overcoming the local minima problem, the generation of a completely random new individual has significant drawbacks. In practice, the parameter search space for the function $T(\textbf{p})$ has many local minima. Additionally, some parameter combinations may be incompatible with the linear solver, i.e., the solver would not be able to find the solution of the SLAE with the specified set of numerical method parameters. To avoid falling into ``ineffective'' areas in the parameter search space, it was decided to introduce the preliminary filtering of the newly generated random input vectors before calculating the fitness value (solving the SLAE). The role of the filter is delegated to the pre-trained NN.

\begin{figure}[t!]
\center{\includegraphics[width=0.9 \textwidth]{sketch3.pdf}}
\caption{Sketch of the proposed optimization algorithm.}
\label{fig:sketch}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Neural network model}
\label{sec:NN}

In order to bring some a priori knowledge about the typical influence of each parameter, a neural network model is built and trained to predict the SLAE solution time for a given vector of parameters. The fully-connected architecture is chosen to construct the NN due to its simplicity and ability to solve regression problems~\cite{MURTAGH1991183, kohler2021rate, GDP_Regression}. The following NN configuration is proposed:
\begin{itemize}
\item \textbf{Layers and activation functions.} The NN input layer takes the values of each parameter, and the output layer containing 1~neuron produces the solution time estimate for a given parameter vector. The three hidden dense layers with 512, 256, and 128~neurons, respectively, follow the input layer. Hidden layers have sigmoid activation functions; the output neuron has a linear activation function. The choice of the activation functions and the size of the hidden layers have been optimized using the KerasTuner random search scheme~\cite{omalley2019kerastuner}. The dropout layers are used after each hidden layer to prevent overfitting. The dropout rate is set equal to~0.25 in accordance with the results of the grid search procedure.

\item \textbf{Training data.} Solution statistics for some SLAE is used to train the NN. In the ideal case, the corresponding data represents a combination of the input parameter vectors and the resulting solution times. In practice, however, some parameter vectors may produce too high SLAE solution times or even lead to solver divergence. The use of NN as a filter with the ES softens the requirements for prediction accuracy: the practical use case scenario requires accurate predictions for the configurations with small execution times only (e.g., values below the median), while for the big ones, rough estimates providing the correct order of magnitude can be acceptable (for details, see section~\ref{sec:HES}).
\\
The specific problem requirements allow performing initial data filtering in order to improve the predictions accuracy and reduce the training dataset size. The input data preparation includes two modifications: the finite solution times exceeding the third quartile~($Q_3$) and the values for non-converged runs (infinite solution times) are replaced with $Q_3$ constant. The following notation is used in the further narration: the original dataset is called ``raw dataset'', the dataset with filtered non-converged runs is called ``unbalanced dataset'', and the dataset with both filtered non-converged runs and times exceeding the third quartile is called ``balanced dataset''.
\\
An example of the proposed data filtering is shown in Figure~\ref{fig:data_filtering}. This filtering addresses two issues: (\textit{i})~the formally infinite values for the non-converged runs are replaced with the finite ones, and (\textit{ii})~the dataset is balanced to minimize the prediction errors in the practically important values range. For example, the data modification shown in Figure~\ref{fig:data_filtering} allows for a decrease in the range of predicted values by two orders of magnitude.
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\linewidth]{filter5.pdf}
\end{center}
\vspace{-0.5cm}
\caption{Training dataset initial filtering.}
\label{fig:data_filtering}
\end{figure}

\item \textbf{Loss Function.} The mean squared error~(MSE), defined as
\begin{equation}
MSE = \frac{1}{N_t} \sum_{i=1}^{N_t} \left(T^*_i - \hat{T_i} \right)^2,
\end{equation}
where $N_t$ is the training dataset size, $\hat{T_i}$ is the predicted value, and ${T^*_i}$ is the input data, is a commonly used loss function for regression problems~\cite{LossSurvey, LossFuncInvestigation}. The MSE loss function minimizes the difference between the predicted and actual values in the whole data range. 

\item \textbf{Model optimizer.} The ADAM (Adaptive Moment Estimation) optimizer is used for training the NN model. The initial learning rate is set to $10^{-3}$. The number of epochs as a function of the training dataset size is selected in accordance with the following empirical formula:
\begin{equation}
N_{epochs} = \frac{N_t}{50}+50.
\end{equation}
This estimate ensures stable training of the constructed model in the range of used dataset sizes ($N_t \sim 10^4-10^5$) and the number of parameters ($N \sim 20$) for the test problems performed.

\item \textbf{Cross-validation technique.} The data cross-validation technique is used to provide an effective use of the input dataset and a representative assessment of the model accuracy. For the proposed model, a single hold-out random subsampling method is applied~\cite{crossval}. The input dataset is split into the training and validation parts. The corresponding method applies a random permutation of the samples in the input dataset, thus mixing the data between the training and validation sets at the subsequent training sessions.

\item \textbf{Prediction quality criteria.} The coefficient of determination, $R^2$, which represents the proportion of the variance explained by the model, is widely used to evaluate the accuracy of the predictive models~(e.g.,~\cite{Ahangar2010, Wang2021prediction}). The coefficient is defined as:
\begin{equation}
R^2 = 1-\frac{SS_{R}}{SS_{T}},
\end{equation}
where the residual sum of squares, $SS_{R}$, and the total sum of squares, $SS_{T}$, are:
\begin{gather}
SS_{R} = \sum_{i=1}^{N_v} \left(T^*_i - \hat{T_i}\right)^2,\\
SS_{T} = \sum_{i=1}^{N_v} \left(T^*_i - \bar{T}\right)^2.
\end{gather}
Here $\bar{T}$ is the mean value over the validation dataset
\begin{equation}
\bar{T} = \frac{1}{N_v} \sum_{i=1}^{N_v} T^*_i,
\end{equation}
and $N_v$ is the corresponding dataset size. In case the predicted values show good match with the actual ones coefficient of determination tends to~1. Predictions with a squared error similar to the squared error of the mean provide $R^2$ equal to zero. Negative $R^2$ values indicate that predictions are worse than the mean value.

$R^2$ is an integral parameter indicating the proximity of the entire validation dataset and corresponding predictions. While indicating the quality of predictions in general, this metric does not account for the specific requirements of the optimization algorithm being developed, where the key one is the accuracy of predictions for the least values dataset range. 

An additional customized prediction quality criterion accounting for NN usage specifics is proposed. This metric indicates the probability of finding the least predicted values among the least true ones. With the validation dataset and the corresponding predictions, the parameter is defined as
\begin{equation}
\mathcal{F}_{\alpha} = \frac{N^{*}_{\alpha}}{N_{\alpha}}.
\end{equation}
Here, $\alpha$ is the fraction of the validation dataset considered, $N_{\alpha} = \alpha N_v$ is the number of samples in the corresponding fraction of the validation dataset, and $N^{*}_{\alpha}$ is the number of least predicted values falling in the list of least true values. The proposed parameter is illustrated in Figure~\ref{fig:prediction_probability}. According to this figure, for $\alpha_0=4/11$ the size of the least values list is~$N_{\alpha_0}=4$, the number of samples both falling in the lists of least predicted and true values is equal to~$N^{*}_{\alpha_0}=3$, and $\mathcal{F}_{\alpha_0} = 0.75$.

\begin{figure}[h]
\center{\includegraphics[width=0.6\textwidth]{quality3.pdf}}
\vspace{-0.5cm}
\caption{Prediction quality criteria illustration: $\alpha_0=4/11$, green circles -- least predicted values, purple circles -- least true values.}
\label{fig:prediction_probability}
\end{figure}

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Hybrid optimization algorithm}
\label{sec:HES}
The proposed hybrid ES (HES) combines both the ES and NN optimization techniques discussed above. The basic $(1+\lambda)-ES$ optimization algorithm is presented in Algorithm~\ref{alg:HES}, and the random mutation operator incorporating the NN is outlined in Algorithm~\ref{alg:M_R}. 

The random mutation operator, if configured to use the neural network, generates a large preliminary set of parameter configurations containing $L$~samples. The predictions for these samples are obtained using the NN, and $L_{\alpha} = \alpha L$ samples with the least predicted values are selected. $\lambda_R$ vectors are randomly chosen from the selected list and passed to the ES.

In the limiting case, $L_{\alpha}$ equals to $\lambda_R$, and $L = \lambda_R / \alpha$ provides the lower bound estimate for the trial dataset size. This, however, is not recommended as can affect the probability of finding the least true values in the least predicted values list (see, section~\ref{sec:F_alpha}). To produce reliable results for the $\mathcal{F}_{\alpha}$ criteria, the size of the preliminary set of combinations should be chosen in accordance with the size of the validation dataset.

 The random selection of $\lambda_R$ vectors from the larger least value parameters list presumes the same probability of finding the true least values, and $\lambda_R$ vectors would contain on average $\mathcal{F}_{\alpha} \lambda_R$ parameter combinations from the true least values list in each ES generation. At least one true least value is expected to be passed to the ES in each generation. The reasonable upper bound of the trial dataset can be obtained based on the typical validation dataset size required for accurate enough prediction of the parameter $\mathcal{F}_{\alpha}$. The corresponding question is discussed in detail in section~\ref{sec:F_alpha}.

The algorithm formulated below operates with the parameter $\alpha$. In general, selecting $\alpha$ as little as possible can be preferable, as it will shrink the predicted least values list and preserve the parameters with the smallest predicted values. However, the decrease of $\alpha$ also affects the $\mathcal{F}_{\alpha}$ parameter, and the probability of predicting true least values decreases. The optimal value $\alpha^*$ can be found by maximizing the ratio
\begin{equation}
P(\alpha) = \frac{\mathcal{F}_{\alpha}}{\alpha}\label{eq:alpha_estimate}
\end{equation}
with $\mathcal{F}_{\alpha} \geq 1/\lambda_R$ as a constraint. In practice, a moderate value, such as $\alpha^* = 0.05$, can be used as a baseline value, and it can be tuned in each case depending on the quality of predictions produced by the specific NN.

The mutation operator $M_R$ in its purely random form can also be used to generate completely random individuals. This option, however, is mostly used for testing purposes to show the effect of NN parameter filtering and its impact on the overall $(1+\lambda)-ES$ algorithm efficiency.

\begin{algorithm}[t]
\caption{Hybrid evolution strategy $(1+\lambda)-ES$.} 
\begin{algorithmic}[1]
\label{alg:HES}
\STATE $l = 0$;
\STATE $P^0 = \{ \textbf{p}_1^0, \textbf{p}_2^0, \ldots,  \textbf{p}_{1+\lambda}^0 \}$ -- initial generation with $\lambda+1$ individuals;
\STATE Evaluate fitness values for $P^0$ and select the best individual $\textbf{p}^{0,*}$;
    \WHILE {(\textit{convergence criteria is not met})}
    \STATE Create $\lambda+1$ offsprings $\tilde{\textbf{p}}^{l+1}$ by cloning the selected individual $\textbf{p}^{l,*}$
    \STATE Apply soft mutation operator, $M_S$, to $\lambda_S$ individuals: \\ $\textbf{p}^{l+1}_k = M_S(\tilde{\textbf{p}}^{l+1}_k), \, \, k=\overline{2,1+\lambda_S}$;
    \STATE Apply random mutation operator, $M_R$, to $\lambda_R$ individuals (Algorithm~\ref{alg:M_R}): \\ 
           $\textbf{p}^{l+1}_k = M_R(\tilde{\textbf{p}}^{l+1}_k), \, \, k=\overline{2+\lambda_S, 1+\lambda_S+\lambda_R}$, \\ where $\lambda_S+\lambda_R = \lambda$;
    \STATE Evaluate fitness values for the new generation $P^{l+1}$ and select the best individual $\textbf{p}^{l+1,*}$;
    \STATE $l = l+1$;
\ENDWHILE 
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Random mutation operator, $M_R$.} 
\begin{algorithmic}[1]
\label{alg:M_R}
\IF{(\textit{use NN filtering})}
  \STATE Generate $L$ trial vectors $\tilde{\textbf{p}}_k, \, k=\overline{1,L}$, $L \gg \lambda_R$;
  \STATE Predict solution ``times'' $T(\tilde{\textbf{p}}_k), \, k=\overline{1,L}$ with the NN;
  \STATE Choose $L_{\alpha}$ vectors with the least predicted values;\label{alg:choose}
  \STATE Randomly choose $\lambda_R$ vectors from $L_{\alpha}$ candidates selected at the step~\ref{alg:choose}, assign vectors as new individuals in ES;
\ELSE
\STATE Generate $\lambda_R$ vector parameters $\tilde{\textbf{p}}_k, \, k=\overline{1,\lambda_R}$;
\STATE Assign generated parameters as new individuals in ES.
\ENDIF
\end{algorithmic}
\end{algorithm}

To complete, the ES iterations stopping criteria must be provided. A couple of options can be used in practice, e.g., non-decreasing or slowly decreasing fitness value during the last $m$~iterations. The current paper uses the condition of non-decreasing fitness value in the last 5~iterations together with the limitation on the overall number of generations (50~generations).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Algorithm evaluation details}

The proposed hybrid evolution strategy has been tested in detail with a set of model SLAEs and integrated with in-house code for modeling incompressible turbulent flows. The software implementation details and the test matrices used for testing are presented below.

\subsection{Implementation details}

The HES is implemented as a part of the XAMG library~\cite{xamg_repo} for solving SLAEs with multiple right-hand sides. The library provides implementation of a set of iterative methods, including the Krylov subspace and algebraic multigrid methods, and supports mixed precision calculations~\cite{KrasnopolskyIR2021}. The library is C++ code based on the C++11 standard specification. It provides hierarchical multilevel parallelization with the MPI+POSIX shared memory hybrid programming model. Some additional XAMG library implementation details and performance evaluation results can be found, e.g., in~\cite{Krasnopolsky2021xamg, Krasnopolsky2021rsd}.

The TensorFlow framework~\cite{tensorflow2015} is used to operate with NN. The NN training process is organized as a standalone task, performed separately with the Python code developed. The inference stage is incorporated into the XAMG library using the TensorFlow C API, and the corresponding functionality is directly used from the XAMG library code.

\subsection{Compute platform}

The calculations presented in the paper have been performed on cluster compute nodes with 2 x 14-core Intel Xeon E5-2680~v4 processors. Unless explicitly stated, the test results are obtained with a single compute node. The hybrid 3-level solver configuration corresponding to the hardware architecture is used in the XAMG library. All available CPU cores per node are utilized in the calculations.

\subsection{Test SLAEs}
\label{sect:test_matrices}
A set of linear systems predominantly corresponding to the elliptic differential equations is used to evaluate the efficiency of the proposed optimization algorithm. The test set includes several model SLAEs widely used to evaluate the efficiency of numerical methods, e.g.,~\cite{Yang2010, Gahvari2013, Bienz2016}, a subset of matrices from the SuiteSparse Matrix Collection~\cite{SuiteSparse}, and a group of SLAEs coming from the turbulent flow simulations~\cite{KrasnopolskyCPC2018}.

The first group of linear systems includes two SLAEs corresponding to 3D problems in a cubic computational domain. Regular grid and 7-point discretization stencil are used in both cases to solve the diffusion equation
\begin{gather}
- \nabla \cdot \left( \mathcal{K} \nabla u \right) = f, \label{eqn:diffusion}
\end{gather} 
with Dirichlet boundary conditions, and the cases considered differ in the form of the diffusion coefficient, $\mathcal{K}$. The first case denoted as \texttt{cube} corresponds to a constant diffusion coefficient, $\mathcal{K} = 1$. The second case, \texttt{jumps}, considers the stepwise distribution of the diffusion coefficient:
\begin{gather}
\mathcal{K}(\textbf{x}) = 
\begin{cases}
1000, & \textbf{x} \in [0.1, 0.9]^3, \\
0.1, & \textbf{x} \in \mbox{cubes of size} \, 0.1^3 \, \mbox{at the corners of the domain}, \\
1, & \mbox{elsewhere}.
\end{cases}
\end{gather}

The second group includes a subset of the matrices from the SuiteSparse Matrix Collection. Square matrices with nonzero diagonal elements and unknowns in the range of $10^5$ to $2\cdot10^6$ are selected and supplemented with the constant right-hand side vector $\textbf{b} = \textbf{1}$. The ones that can be solved with the ``default'' BiCGStab method and algebraic multigrid preconditioner configuration (for details, see~\ref{app:default}) are included in the SSMC test set for HES evaluation.

Finally, the third group of test SLAEs relates to the systems occurring when modeling incompressible turbulent flows, the target problem of the current research. The SLAEs correspond to the solution of PPE arising in direct numerical simulation of turbulent flow in a channel with a matrix of wall-mounted cubes~\cite{KrasnopolskyCPC2018}. The two cases, \texttt{D1} and \texttt{D2}, correspond to two structured hexahedral grids of size 2.3 and 9.7~million unknowns, respectively.

\subsection{Numerical method configurations}

The three sets containing 7, 13, and 21 optimized parameters are considered for evaluating the efficiency of the HES. The parameters are related to the classical algebraic multigrid and Chebyshev polynomial methods used as a preconditioner and smoother with the BiCGStab solver, respectively. The detailed lists of parameters included in these optimized sets are shown in \ref{app:list}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodological studies}

The detailed methodological studies are performed to evaluate the efficiency of the proposed HES and the typical range of control parameters affecting its productivity and performance. These include both the parameters of the evolution strategy and the neural network-related parameters. These questions are discussed in detail in the following sections.

The test results presented below deal with the NNs trained with the data for three test systems: two \texttt{cube} systems constructed for $40^3$ and $100^3$~grid points, and \texttt{apache2} system from the SSMC. The three linear solver configurations with 7, 13, and 21 parameters are considered for each system. The randomly generated parameter vectors are used to produce SLAE solution time statistics and to train the neural network model. In total, 50~thousand samples are collected for each system and parameter set. It should be noted that the obtained datasets sufficiently differ in the number of non-converged samples: for example, for the \texttt{cube40} system with a 7-parameter set, all the combinations produce finite solution times, while for \texttt{apache2} system with a 13-parameter set, the fraction of non-converged combinations exceeds~66\%. The corresponding statistic for the datasets considered is presented in Table~\ref{tab:non_converged}.

\begin{table*}[t]
\caption{The proportion of non-converged combinations in the input datasets for three basic optimized parameter sets and three test systems.}
\centering
\begin{tabular}{| c | c | c | c |}
\hline
\multirow{2}*{Solver configuration} & \multicolumn{3}{c|}{Non-converged, \% } \\
\cline{2-4}
& \texttt{cube40} & \texttt{cube100} & \texttt{apache2} \\
\hline
7 parameters & 0 & 1.5 & 19.9 \\
\hline
13 parameters & 6.5 & 19 & 66.6 \\
\hline
21 parameter & 8.5 & 17.7 & 55.3 \\
\hline
\end{tabular}
\label{tab:non_converged}
\end{table*}


\subsection{Neural network training}

% This section investigates the influence of the control parameters in the NN training and the accuracy of the corresponding predictions.
The methodological studies begin with investigating the aspects of neural network training. This section analyzes the influence of the control parameters when training the NN and estimates the accuracy of NN predictions.

\subsubsection{$\mathcal{F}_{\alpha}$ prediction quality criteria}
\label{sec:F_alpha}
The methodological studies are started with estimating the prediction quality criteria for the trained neural networks and analyzing the required validation dataset size to obtain reliable $\mathcal{F}_{\alpha}$ estimates. The tests use the NN trained with 45~thousand samples, and the other 5~thousand samples are used for validation. The most complex configuration with 21~optimizing parameters is considered. To estimate the spread of results obtained, in accordance with the cross-validation procedure, the corresponding learning process and $\mathcal{F}_{\alpha}$ calculations are repeated 5~times, and the mean over 5~trials as well as the standard deviation are presented.

The obtained results for $\mathcal{F}_{\alpha}$ with $\alpha = 0.002$, $0.01$, $0.05$, and $0.2$ as a function of the validation dataset size are shown in Figure~\ref{fig:FL_cone}. The results presented demonstrate that the spread over trials decreases with increasing the validation dataset size, and the dataset with $N_v = 5000$~samples (in most cases, even lower) allows for reliable $\mathcal{F}_{\alpha}$ estimates. As expected, lowering the fraction of the selected least predicted values leads to a decrease in the absolute values of the prediction quality criteria. However, in accordance with~\eqref{eq:alpha_estimate}, the obtained values for $\mathcal{F}_{0.002}$ still can be used in the HES with a large enough number of randomly mutated individuals.

\begin{figure}[h]
\center{\includegraphics[width=\textwidth]{FL_cone_21param_ed}}
\vspace{-0.5cm}
\caption{Dependence of the $\mathcal{F}_{\alpha}$ prediction quality metric on the validation dataset size, $N_v$.}
\label{fig:FL_cone}
\end{figure}

\subsubsection{Data balancing}

Data balancing plays an essential role when training the neural network. The specific balancing procedure proposed in section~\ref{sec:NN} helps improve the accuracy of predictions. The preliminary methodological studies have shown that the presence of the extremely large SLAE solution times in the dataset significantly coarsens the predictions for the small ones. An example of the corresponding training is the one obtained for the unbalanced data containing 20~thousand samples for the configuration with 21~optimizing parameters and \texttt{cube40} test system. Figure~\ref{fig:balancing_results} shows the real solution times from the validation dataset sorted in ascending order (blue line) and NN predictions. The plot clearly demonstrates the weak quality of the predictions for the unbalanced data and the tendency to predict some constant value. The data filtering procedure applied to the dataset before NN training replaces largest solution times. This allows to reduce the spread of the solution times (for example, in Figure \ref{fig:balancing_results} -- from $0.01 \div 3$~s to $0.01 \div 0.073$~s). Training with the balanced dataset significantly improves the accuracy of predicting the lowest values for the validation dataset. The predicted values correctly reproduce the general trend for the practically important combinations when using with HES.

The effect of data balancing is also illustrated with the $\mathcal{F}_{0.05}$ and $R^2$ metrics calculated for three test matrices and three optimizing parameter combinations with training datasets containing 20 and 45~thousand samples (Table~\ref{tab:balancing_table_20K} and~\ref{tab:balancing_table_45K}). Both datasets with unbalanced and balanced data allow for reliable results for 7 and 13~parameter solver configurations. The extended configuration with 21~parameters, on the other hand, clearly shows the data balancing effect: the use of balanced input data allows for a significant increase in both the $\mathcal{F}_{0.05}$ and $R^2$ results. An increase in the training dataset size improves the quality of predictions in all cases, and the most notable one is the 21~parameter solver configuration.

It should be noted that the $\mathcal{F}_{\alpha}$ parameter shows more sensitive behavior with regard to data balancing compared with $R^2$. Using unbalanced input data for the 21~parameter case and \texttt{cube40} test system, for example,  provides rather good value $R^2=0.66$, whereas the proposed metric yields only $\mathcal{F}_{0.05} = 0.26$. Balancing the data raises $R^2$ by 20\%, while increasing $\mathcal{F}_{0.05}$ by 120\%. This aspect indicates the need to control the $\mathcal{F}_{\alpha}$ metric in addition to $R^2$ when training the NN for use with HES.

\begin{figure}[ht]
\center{\includegraphics[width=0.9\textwidth]{data_balance_example.pdf}}
\vspace{-0.4cm}
\caption{Comparing the quality of predictions for unbalanced and balanced datasets. Test case: \texttt{cube40}, 21~parameter set.}
\label{fig:balancing_results}
\end{figure}

\begin{table*}[ht!]
\caption{Comparing the NN prediction quality criteria for unbalanced and balanced data, 20~thousand samples.}
\centering
\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
Solver & Test & \multicolumn{2}{c|}{Unbalanced}  & \multicolumn{2}{c|}{Balanced} & Improvement, \\
\cline{3-6}
 configuration & system & $\mathcal{F}_{0.05}$  & $R^2$ & $\mathcal{F}_{0.05}$  & $R^2$ & $\mathcal{F}_{0.05}$, in \% \\
\hline
\multirow{3}*{7 parameters} & \texttt{cube40} & 0.79 & 0.98 & 0.80 & 0.99 & 1.5\\
				& \texttt{cube100} & 0.83 & 0.97 & 0.84 & 0.97 & 1.1\\
				& \texttt{apache2} & 0.83 & 0.96 & 0.84 & 0.96 & 1.0\\
\hline
\multirow{3}*{13 parameters} & \texttt{cube40} & 0.62 & 0.87 & 0.62  & 0.81 & -0.6\\
				& \texttt{cube100} & 0.68 & 0.79 & 0.73 & 0.80 & 6.8\\
				& \texttt{apache2} & 0.59 & 0.57 & 0.55 & 0.60 & -6.7\\
\hline
\multirow{3}*{21 parameter} & \texttt{cube40} & 0.26 & 0.66 & 0.59 & 0.79 & \textbf{123.6}\\
				& \texttt{cube100} & 0.14 & 0.38 & 0.49  & 0.63 & \textbf{248.3}\\
				& \texttt{apache2} & 0.15 & 0.31 & 0.38 & 0.46 & \textbf{155.9}\\
\hline
\end{tabular}
\label{tab:balancing_table_20K}
\end{table*}

\begin{table*}[ht!]
\caption{Comparing the NN prediction quality criteria for unbalanced and balanced data, 45~thousand samples.}
\centering
\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
Solver & Test & \multicolumn{2}{c|}{Unbalanced}  & \multicolumn{2}{c|}{Balanced} & Improvement, \\
\cline{3-6}
 configuration & system & $\mathcal{F}_{0.05}$  & $R^2$ & $\mathcal{F}_{0.05}$  & $R^2$ & $\mathcal{F}_{0.05}$, in \% \\
\hline
\multirow{3}*{7 parameters} & \texttt{cube40} & 0.84 & 0.99 & 0.87 & 0.99 & 3.9\\
& \texttt{cube100} & 0.85 & 0.99 & 0.86 & 0.98 & 0.8\\
& \texttt{apache2} & 0.86 & 0.97 & 0.88 & 0.98 & 2.4\\
\hline
\multirow{3}*{13 parameters} & \texttt{cube40} & 0.72 & 0.92 & 0.68  & 0.88 & -5.1\\
& \texttt{cube100} & 0.72 & 0.85 & 0.76 & 0.86 & 6.0\\
& \texttt{apache2} & 0.65 & 0.65 & 0.67 & 0.75 & 2.1\\
\hline
\multirow{3}*{21 parameter} & \texttt{cube40} & 0.28 & 0.73 & 0.65 & 0.83 & \textbf{129.3}\\
& \texttt{cube100} & 0.20 & 0.52 & 0.57  & 0.70 & \textbf{189.4}\\
& \texttt{apache2} & 0.19 & 0.40 & 0.49 & 0.58 & \textbf{165.1}\\
\hline
\end{tabular}
\label{tab:balancing_table_45K}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Training dataset size estimates}
The dependence of the characteristic dataset size for training the neural network on the number of optimized parameters is investigated. A series of tests with progressively increasing balanced dataset sizes is performed to estimate the corresponding requirements. The $\mathcal{F}_{\alpha}$ parameter is evaluated in these tests, and the validation dataset with 5~thousand samples is used to calculate $\mathcal{F}_{\alpha}$.

The Figure~\ref{fig:NN_training_dataset} shows the typical testing results. The plots presented for $\alpha=0.05$ demonstrate acceptable quality of predictions starting from 20~thousand samples, and further increasing the size of the training dataset only slightly increases the corresponding parameter values. A similar picture is observed when varying the $\alpha$ parameter: only a minor difference in the absolute values takes place, while the corresponding trends remain the same. These results indicate that the dataset with 45~thousand samples, used in the tests presented below, is sufficient for training the NN and obtaining reliable predictions.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{FL_all_ed}
\vspace{-0.8cm}
\caption{$\mathcal{F}_{0.05}$ as a function of the training dataset size.}\label{fig:NN_training_dataset}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Hybrid evolution strategy}

HES has several control parameters affecting the overall performance. These include the generation size and the number of soft and random mutations used to produce new individuals. In addition to them, the fraction of the least values range must be specified when using NN as a random mutation filter. The following sections investigate the effect of varying these parameters, their impact on the optimization results, and provide some basic recommendations for choosing these values.

The randomization process is an essential part of the evolution strategy. This, however, affects the result of the specific trial run and requires some statistical averaging to produce reliable data. The calculation results presented below are obtained by averaging over 20~trials (unless otherwise specified), and the mean and standard deviation are typically shown.

The results presented in the following subsections are performed for three test systems \texttt{cube40}, \texttt{cube100}, and \texttt{apache2}, and for three solver configurations with 7, 13, and 21 parameters. The NNs used when optimizing the linear solver configuration are trained with the datasets containing the information for the same SLAE.

\subsubsection{Neural network as a pre-filter}
The first test series demonstrates the effect of using the NN as a pre-filter in the random mutation operator. The tests evaluate the HES with 5~soft and 10~random mutations, and the random mutation operators both with and without NN are considered. For NN-assisted cases, the parameter $\alpha=0.05$ is used. In addition to optimal SLAE solution times, the HES calculation times spent on finding the optimum are also collected and analyzed.

The obtained results (Figure~\ref{fig:NN_filter}) confirm the initial expectations regarding the use of NN as a pre-filter:
\begin{itemize}
  \item the NN-assisted random mutation operator improves the optimized SLAE solution time: for the SLAEs considered the solution time reduction reaches 30\%;
  \item the NN filtering allows for a decrease in the HES calculation time, and a speedup by a factor of~1.7 can be achieved;
  \item the spread for both the solution and calculation times is significantly decreased when using the NN filter.
\end{itemize}

The use of NN does not change the mean number of generations of HES until reaching convergence. The convergence is typically achieved in 10~generations, which is equivalent to 100-200~trial SLAE solutions. Comparing the resulting SLAE solution times and HES calculation times, however, one can see that these times differ by three orders of magnitude. This is a consequence of a number of aspects. The SLAE solution times during the optimization for ineffective combinations can be much higher than the resulting ones. Additionally, the multigrid setup phase implementation in the XAMG library, opposite to the solve phase, is not well optimized yet, which also increases the optimization algorithm calculation time.

\begin{figure}[t!]
\centering
\includegraphics[width=13cm]{pic1_solution2}
\includegraphics[width=13cm]{pic1_optim2}
\vspace{-0.5cm}
\caption{Effect of using NN as a pre-filter in the random mutation operator: top -- optimized solution times, bottom -- HES calculation times.}
\label{fig:NN_filter}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Soft and random mutations}
The generation size and the number of soft and random mutations are the important parameters affecting the quality of optimization and optimization time. A detailed study is performed to estimate the optimal ratio between the soft and random mutations. Four configurations are considered:
\begin{itemize}
  \item \texttt{S5/R5}: 5 soft and 5 random mutations;
  \item \texttt{S5/R10}: 5 soft and 10 random mutations;
  \item \texttt{S10/R5}: 10 soft and 5 random mutations;
  \item \texttt{S10/R10}: 10 soft and 10 random mutations.
\end{itemize}
The NN filtering is used with the random mutation operator, and the fraction of the least values range is set equal to $\alpha=0.05$.

The analysis of the calculation results shows that an increase in the number of individuals (generation size) reduces the resulting optimized SLAE solution time (Figure~\ref{fig:mutations}). However, any substantial difference in SLAE solution times is observed for the 21~parameter case only when the variance across the test configurations reaches 10\%. In all other cases, the use of different HES configurations leads to negligible variation in the optimized results, but the algorithm calculation time increases in proportion to the generation size. These results allow us to conclude that the use of \texttt{S5/R5} HES configuration can be an acceptable choice when using the optimization algorithm with the NN-assisted mutation operator.

\begin{figure}[t]
\centering
\includegraphics[width=12.0cm]{pic2_SR}
\vspace{-0.5cm}
\caption{Effect of varying the number of soft and random mutations.}
\label{fig:mutations}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Choosing the fraction of the least values range}
\label{sec:var_alpha}
One more parameter used with NNs is the fraction of the least predicted values when producing the individuals for the NN-assisted random mutation operator. The simple relation for determining the optimal value is proposed~\eqref{eq:alpha_estimate}. Following the results presented in section~\ref{sec:F_alpha}, the value $\alpha^* = 0.002$ must be close to the optimal one for all the NNs trained in this work. The numerical experiments performed focus on validating these predictions.

This test series performs the experiments for the HES configuration with 5~soft and 5~random mutations, and with four $\alpha$ values: 0.002, 0.01, 0.05, and 0.2. The corresponding results, together with the optimization results for the random mutation operator without NN filtering, are summarized in Figure~\ref{fig:alpha}. The clear tendency to improve the quality of optimization with decreasing the $\alpha$ parameter can be seen (except only the \texttt{cube40} system and 7~parameter solver configuration), which coincides with predictions~\eqref{eq:alpha_estimate}. Reducing $\alpha$ from 0.2 to 0.002 results in a 25\% reduction in SLAE solution time while maintaining the HES calculation time.

\begin{figure}[t]
\centering
\includegraphics[width=12.0cm]{pic3_alpha}
\vspace{-0.5cm}
\caption{Effect of varying the fraction of least values range, predicted by the NN.}
\label{fig:alpha}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Reusing the pre-trained neural networks}

The tests shown above have been performed using the NNs trained with the same linear systems statistics. While allowing for the investigation of the influence of the optimization algorithm control parameters, the corresponding configuration is an idealization of the practical use cases. The following section investigates the perspectives of reusing the pre-trained NNs to optimize linear solver configurations for various linear systems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Test systems cross-optimization}
The first test series performs the cross-optimization for the basic test systems (\texttt{cube40}, \texttt{cube100}, and \texttt{apache2}) and the NNs trained for the same matrices to estimate the potential of reusing the pre-trained NNs. \texttt{Cube40} and \texttt{cube100} systems have the same origin (finite difference discretization of the 3D Poisson equation in a cubic domain), but differ in the computational grid and matrix size by about 6~times; the \texttt{cube100} and \texttt{apache2} matrices are of about the same size, but produced by various differential equations (\texttt{apache2} matrix is a result of the finite difference discretization of the 3D structural mechanics problem). The \texttt{S5/R5} HES configuration is used in these tests, and two values, $\alpha=0.002$ and $\alpha=0.05$, are compared.

The obtained calculation results confirm an important HES feature -- the possibility of reusing the pre-trained NNs with various systems (Figure~\ref{fig:cross_opt}). The optimization of the \texttt{cube40} SLAE solution time can be done with the same effect with both the \texttt{cube40} and \texttt{cube100} pre-trained NNs. The \texttt{apache2} NN demonstrates lower solution time improvement; however, it still outperforms the ES without the NN pre-filter. Additionally, the NN pre-filter with any of the NNs considered significantly decreases the variation across the trial runs.

The \texttt{cube100} SLAE can be successfully optimized with any NN among the considered ones. The native \texttt{cube100} NN demonstrate slightly better results, but the variance of the corresponding times does not exceed several percent. The same situation is observed for the \texttt{apache2} system: the evident advantage of using the native \texttt{apache2} pre-trained NN is seen for the 21~parameter case with $\alpha=0.05$, while all other test configurations show negligible difference in the optimized solution times.

A comparison of the results for various fractions of the least values range reveals that using $\alpha=0.002$ is superior to $\alpha=0.05$ and allows for a 5\% reduction in solution time as well as variation across trials. This observation is consistent with the results discussed in the section~\ref{sec:var_alpha}.

\begin{figure}[t]
\centering
\includegraphics[width=6.0cm]{pic4_cross_opt_0.002}
\includegraphics[width=6.0cm]{pic4_cross_opt_0.05}
\vspace{-0.5cm}
\caption{Cross-optimization using various pre-trained NNs: left -- $\alpha = 0.002$, right -- $\alpha = 0.05$.}
\label{fig:cross_opt}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Linear systems for elliptic differential equations}
The second test series performs the optimization of SLAE solution times for the \texttt{jumps}, \texttt{DNS1}, and \texttt{DNS2} systems. These systems correspond to elliptic differential equations and are similar to those that are of our interest. The tests also use the \texttt{S5/R5} HES configuration with $\alpha=0.002$, and two pre-trained NNs for \texttt{cube100} and \texttt{apache2} systems. These calculations are done with 4~compute nodes, and the presented results are averaged over 10~trial runs. In addition to optimized SLAE solution times, the baseline values for the ``default'' linear solver configuration formed on the basis of some calculations experience are also collected.

The obtained calculation results are summarized in Figure~\ref{fig:elliptic_optimization}. These results show that the basic evolution strategy can significantly improve baseline times and tailor the linear solver configuration to the specific linear system. The use of \texttt{cube100} or \texttt{apache2} pre-trained NNs improves the optimization results even further. The choice of the optimal NN varies depending on the linear system and optimizing parameter set, but the variance of the mean values is within 10-15\%.  Despite this, the use of any of the NNs considered allows for a reduced optimized solution time compared to the basic ES. The speedup by a factor of 1.3--2.7 is observed for the ES, and it increases to 1.8--3.6 when using the pre-trained NN. In addition, the use of the NN pre-filter for the random mutation operator reduces the algorithm calculation times, and this effect becomes more and more valuable with increasing the number of optimizing parameters: up to a 5-fold HES calculation time speedup when optimizing the 21~parameter configuration is observed with \texttt{cube100} NN compared to a basic ES run.

\begin{figure}[t]
\centering
\includegraphics[width=12.0cm]{pic5_opt_all}
\vspace{-0.5cm}
\caption{Linear solver configurations optimization results for various test systems.}
\label{fig:elliptic_optimization}
\end{figure}

The best solution times after the optimization are observed for the 13~parameter configuration. The use of a more general 21~parameter case, which further extends the parameter search space, typically does not allow for any productivity improvement. This fact indicates that some further optimization algorithm tuning or more conscious choice of the optimizing parameters may produce additional productivity gains.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{SuiteSparse Matrix Collection}

The final test series is performed for the SSMC systems. The details of selecting the subset of test systems from the SSMC are presented in section~\ref{sect:test_matrices}. Unlike the cases considered above, these SLAEs come from different areas of applied mathematics and do not necessarily correspond to elliptic differential equations. An attempt to optimize the solution time with the HES is made for the selected SLAEs, and the pre-trained NNs for \texttt{cube100} and \texttt{apache2} are evaluated. Following the methodological study results, the \texttt{S5/R5} HES configuration is chosen, and the fraction of the least predicted values for the hybrid random mutation operator is set to~0.002.

The numerical experiments conducted have shown that the use of the evolution strategy allows for a significant improvement in the SLAE solution time (Figure~\ref{fig:SSMC_results}). In all cases except the \texttt{ss1} matrix and 7~parameter set, HES produces the solver parameter configurations capable of solving the test SLAEs. In the worst case (\texttt{thermomech\_TK}, 21~parameters, basic ES) the obtained solution time exceeds the baseline by 16\%. In all other cases, the baseline times are significantly improved, and the peak acceleration by a factor of 6.2 is observed for \texttt{ss1} system with 21~parameter configuration. The average SLAE solution time reduction of the basic ES reaches 2.18 for 7~parameters, 2.42 for 13~parameters, and 2.26 for a 21~parameter configuration. 

The use of NNs provides limited advantages in terms of optimized solution time and calculation time for a set of black box matrices. The minor SLAE solution time improvement by a factor of 1.05--1.2 can be seen. The same behavior takes place for the optimization time: both the slowdown by a factor of~1.1 and the speedup by a factor of~1.4 are observed for different test configurations. The NN filtering typically gives no advantage when the basic ES shows significant solution time improvement. However, it allows for optimizing the SLAE solver configurations for the cases when the ES fails to produce anything better than the baseline results. On the one hand, this observation indicates the importance of using NN filtering with the HES. On the other hand, a more careful selection of the pre-trained NNs may be required to further improve the ES productivity when working with black box linear systems. This topic is planned to be investigated in the future.

\begin{figure}[t]
\centering
\includegraphics[width=11cm]{SSMC_bar_plot_triple.pdf}
\caption{Optimizing the linear solver configurations for the SSMC test matrices.}
\label{fig:SSMC_results}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Direct numerical simulation of turbulent flow}

The second part of the performance evaluation tests considers the modeling of incompressible turbulent flow. The direct numerical simulation of the turbulent flow is performed, and the potential calculation speedup due to automated optimization of the linear solver parameters is demonstrated. The problems simulated are the modeling of incompressible turbulent flow in the plain channel and the modeling of turbulent flow in the channel with a matrix of wall-mounted cubes~\cite{KrasnopolskyCPC2018, Meinders1999}. The 13~parameter solver configuration is used for the simulations following the methodological tests performed as the most suitable one in terms of resulting SLAE solution time, optimization time, and the spread of results from run to run. The HES uses the~\texttt{S5/R5} configuration with \texttt{cube100} pre-trained NN, and $\alpha$ is set equal to~0.002.

The in-house computational code is used to perform DNS. The code is based on a second-order spatial finite difference scheme operating with curvilinear orthogonal coordinates and a third-order Runge-Kutta scheme for advancing in time~\cite{Nikitin2006jcp, Nikitin2006ijnmf}. The time integration scheme used requires three solves of the Poisson equation per time step, which takes up the overwhelming majority of the calculations. The XAMG library is used to solve these linear systems. 

The two runs are performed for each test case with different linear solver configurations -- the ``default'' configuration and the one produced by the HES optimization algorithm. The turbulent flow calculation times and the number of linear solver iterations are analyzed to demonstrate the possibility of using the optimized solver configuration produced by the HES to solve a series of SLAEs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Plain channel flow}

The direct numerical simulation of the turbulent flow in a plain channel with $\mbox{Re}_{\tau} = 180$ is considered. The flow is modeled in $2\pi h \times 2h \times \pi h$ box with the grid of $160 \times 140 \times 160$ cells, where $h$ is the channel half-height. The grid is uniform in streamwise and spanwise directions and stretched in the wall-normal direction. The problem statement is similar to the one used in~\cite{KrasnopolskyCPC2018}.

The simulation starts with perturbed laminar Poiseuille flow; the evolution of these perturbations leads to a transition to turbulence. About 200~time units must be modeled to obtain statistically steady turbulent flow, and further integration is performed to collect statistics. In these tests, the overall duration of the simulation time interval is set equal to $T=500$ with a time integration step $\tau = 0.02$. These calculations are performed on 5~compute nodes.

The ``default'' solver configuration provides the turbulent flow calculation results in 231~min, of which 217~min are spent on solving the pressure Poisson equation. The distribution of the cumulative number of iterations per time step is shown in Figure~\ref{fig:channel_iters}. The number of iterations correlates with the evolution of the wall friction, and a slight increase from 10--12~to 13~iterations is observed after the transition of the perturbed laminar flow to the turbulence. The optimized linear solver configuration allows for reducing the cumulative SLAE solution time to 103~min and performing the DNS in 117~min. Accounting for the extra 10~min for linear solver optimization, the overall computational time reduction reaches~45\% (by a factor of~1.8). The evolution of the number of iterations reproduces the same behavior as for the ``default'' solver configuration: the average number of iterations per time step increases from 14 to 18 when the transition from the laminar to the turbulent flow regime occurs. Despite the increase in the cumulative number of iterations, the calculation time improvement is achieved due to constructing the more lightweight multigrid matrix hierarchy and performing each iteration much faster compared to the ``default'' configuration.

\begin{figure}[t]
\centering
\includegraphics[width=12cm]{DNS_results_channel}
\caption{Evolution of the number of iterations during the simulation, plain channel flow.}
\label{fig:channel_iters}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Flow in a channel with a matrix of wall-mounted cubes}

The second test case performs the DNS in the channel with a matrix of equally spaced cubes mounted on a channel wall. The flow is modeled in a periodic box with dimensions $4h \times 3.4h \times 4h$, where $h$ is the cube height. The modeling is performed for the $\mbox{Re}_b = 3854$, defined using the bulk velocity and cube height. The grid stretching is applied for each wall, and the overall grid size constructed is 2.32~mln. cells. More details about the problem statement can be found in~\cite{KrasnopolskyCPC2018}. The simulation models $T=600$ time units, and the transition to turbulence occurs in the first 100~time units. The time integration step is set to $\tau = 4 \cdot 10^{-3}$.

The formulated case is calculated on 6~compute nodes. The simulation performed with ``default'' linear solver configuration is done in 1007~min, which includes 967~min spent on solving SLAEs. The evolution of the cumulative number of linear solver iterations per time step shows almost constant variation during the whole run (Figure~\ref{fig:channel_cube_iters}), and the number of iterations changes in the range of 17--21.

Modeling with the optimized linear solver configuration allows for a significant reduction in calculation time. The corresponding simulation takes 662~min (621~min for solving SLAEs), and even accounting for 12~min for SLAE solver optimization, the resulting advantage is by a factor of~1.5. The number of iterations for the optimized solver configuration varies in the range of 26--31.

\begin{figure}[t]
\centering
\includegraphics[width=12cm]{DNS_results_channel_cube}
\caption{Evolution of the number of iterations during the simulation, flow in a channel with a matrix of wall-mounted cubes.}
\label{fig:channel_cube_iters}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Analysis of the DNS simulation results}

The calculation results presented above confirm the possibility of using the proposed hybrid evolution strategy to tune the parameters of linear solvers when modeling incompressible turbulent flow simulations. On par with the significant calculation speedup, the HES automates, and, thus, dramatically simplifies the question of choosing the linear solver configuration for the specific problem of interest. The potential of reusing the pre-trained NNs to successfully optimize various linear systems (at least those derived from the elliptic differential equations) is also demonstrated, significantly increasing the scope of the proposed algorithm.

The ``default'' linear solver configuration used for the tests is a universal one formulated as a result of some modeling experience. A slightly better variant can probably be proposed for the problems considered in the paper as a result of thorough tuning. This may slightly change the reference values and speedup numbers, but it does not affect the key results discussed in the paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}

The use of advanced iterative methods for solving systems of linear algebraic equations suffers lots of difficulties due to a large number of tuning parameters affecting the corresponding methods productivity. The present paper proposes an automated parameters tuning algorithm capable for optimizing the iterative method parameters to the specific linear system of interest. The algorithm is based on the evolution strategy with the hybrid random mutation operator, involving the pre-trained neural network. The algorithm combines two distinct properties: the adaptivity, realized by the evolution strategy, and the a~priori knowledge, introduced by the neural network. The neural network usage scenario does not require accurate predictions for the solution times with a specific linear system and hardware platform, only limiting with some relative values. This aspect greatly simplifies the neural network training process and allows for reusing the pre-trained ones with various linear systems.

The paper provides a detailed formulation of the proposed optimization algorithm and the neural network architecture. Several methodological aspects of training the neural network accounting for the specific problem of interest are discussed. A series of numerical experiments is performed to indicate the influence of hybrid evolution strategy control parameters on the optimization results and the required calculation time. These tests demonstrate the potential of SLAE solution acceleration with the proposed evolution strategy at the cost of about 100~trial solutions. The use of the hybrid mutation operator with the neural network filtering allows for further improvement of optimization quality, a decrease in variation across the repeated launches, and a reduction in the optimization algorithm execution time.

The incompressible turbulent flow simulations are performed to indicate the DNS calculation time reduction when using the hybrid evolution strategy to tune the linear solver parameters. The speedup by a factor of 1.5--1.8 is achieved, even after accounting for the extra time spent on parameter optimization. The pre-trained networks can be successfully reused to optimize various systems, and the parameter configurations determined for the constant right-hand side show good results when solving the linear systems corresponding to laminar and turbulent flow regimes. Another important feature of this algorithm is the simplification of the modeling process and the automation of the manual routine work done by the researchers.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Declaration of competing interest} 
The authors declare that they have no known competing financial interests or personal relationships
that could have appeared to influence the work reported in this paper.

%%%%

\section*{Acknowledgments}
\label{lab:ack}
The presented work is supported by RSF grant No. 18-71-10075. The research is carried out using the equipment of the shared research facilities of HPC computing resources at Lomonosov Moscow State University and the computing resources of the federal collective usage center Complex for Simulation and Data Processing for Mega-science Facilities at the NRC ``Kurchatov Institute'', http://ckp.nrcki.ru/.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{appendix} 

%%%%

\bibliographystyle{elsarticle-num}
\bibliography{optim}

%%%%

\end{document}
