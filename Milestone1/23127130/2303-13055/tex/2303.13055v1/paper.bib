@inproceedings{liu_wenyin_smart_2001,
	title = {Smart {Sketchpad}-an on-line graphics recognition system},
	doi = {10.1109/ICDAR.2001.953946},
	abstract = {An online graphics recognition system is presented, which provides users a natural, convenient, and efficient way to input rigid and regular shapes or graphic objects (e.g., triangles, rectangles, ellipses, straight line, arrowheads, etc.) by quickly drawing their sketchy shapes in single or multiple strokes. An input sketchy (hand-drawn) shape is immediately converted into the user-intended rigid shape based on the shape similarity and the time constraint of the sketchy line. Three different (rule-based, SVM-based, and ANN-based) approaches have been applied and compared in the system. Experiments and evaluation are also presented, which show good performance of the system.},
	booktitle = {Proceedings of {Sixth} {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Liu Wenyin and Wenjie Qian and Rong Xiao and Xiangyu Jin},
	month = sep,
	year = {2001},
	keywords = {ANN-based approach, Artificial neural networks, computer graphic equipment, Computer graphics, Filters, graphic objects, Humans, Mice, neural net approach, online graphics recognition system, online operation, regular shapes, Rendering (computer graphics), rigid shapes, rule-based approach, Shape, Smart Sketchpad, Support vector machines, SVM-based approach, Time factors, User interfaces},
	pages = {1050--1054},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/75GTZTCN/953946.html:text/html;Submitted Version:/Users/m3sibti/Zotero/storage/RIKZGZH2/Liu Wenyin et al. - 2001 - Smart Sketchpad-an on-line graphics recognition sy.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/ZIEMQAJH/953946.html:text/html}
}

@inproceedings{zou_overlapped_2011,
	title = {Overlapped {Handwriting} {Input} on {Mobile} {Phones}},
	doi = {10.1109/ICDAR.2011.82},
	abstract = {In this paper, we propose an overlapped handwriting input method on handheld devices, which allows users to write continuously without breaks on a single size-restricted writing area. 2 issues have been considered during the implementation of the overlapped input method: previous characters on the background may obstruct the clear viewing of current character and the messy overlapped handwriting is difficult to be segmented and recognized. In our method, a quick segmentation method based on an artificial neural network is used to tackle the first problem and a novel system is implemented to recognize the messy handwriting based on the output of an isolated character recognition engine and a language model. The recognition rate for Chinese characters is about 92.5\% for a testing database containing GB2312 Chinese characters and other frequently used symbols. The positive feedbacks from testers have also confirmed the validity of the proposed method.},
	booktitle = {2011 {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Zou, Yanming and Liu, Yingfei and Liu, Ying and Wang, Kongqiao},
	month = sep,
	year = {2011},
	note = {ISSN: 2379-2140},
	keywords = {artificial neural network, Character recognition, Chinese characters, Engines, Handheld computers, handheld devices, handwriting recognition, Handwriting recognition, image segmentation, image segmentation method, language model, mobile computing, mobile handsets, mobile phones, natural language processing, neural nets, overlapped handwriting input, overlapped handwriting recognition, Testing, Training, Writing},
	pages = {369--373},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/X56CP8E4/6065337.html:text/html}
}

@inproceedings{ahmed_targeted_2019,
	address = {San Diego, California},
	series = {{ASE} '19},
	title = {Targeted example generation for compilation errors},
	isbn = {978-1-72812-508-4},
	url = {https://doi.org/10.1109/ASE.2019.00039},
	doi = {10.1109/ASE.2019.00039},
	abstract = {We present TEGCER, an automated feedback tool for novice programmers. TEGCER uses supervised classification to match compilation errors in new code submissions with relevant pre-existing errors, submitted by other students before. The dense neural network used to perform this classification task is trained on 15000+ error-repair code examples. The proposed model yields a test set classification Pred@3 accuracy of 97.7\% across 212 error category labels. Using this model as its base, TEGCER presents students with the closest relevant examples of solutions for their specific error on demand. A large scale (N {\textgreater} 230) usability study shows that students who use TEGCER are able to resolve errors more than 25\% faster on average than students being assisted by human tutors.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Ahmed, Umair Z. and Sindhgatta, Renuka and Srivastava, Nisheeth and Karkare, Amey},
	month = nov,
	year = {2019},
	keywords = {compilation error, example generation, intelligent tutoring systems, introductory programming, neural networks},
	pages = {327--338},
	file = {Submitted Version:/Users/m3sibti/Zotero/storage/46ZTXBTK/Ahmed et al. - 2019 - Targeted example generation for compilation errors.pdf:application/pdf}
}

@inproceedings{lavania_weakly_2016,
	address = {New York, NY, USA},
	series = {{UbiComp} '16},
	title = {A weakly supervised activity recognition framework for real-time synthetic biology laboratory assistance},
	isbn = {978-1-4503-4461-6},
	url = {https://doi.org/10.1145/2971648.2971716},
	doi = {10.1145/2971648.2971716},
	abstract = {We describe the design of a hybrid system -- a combination of a Dynamic Graphical Model (DGM) with a Deep Neural Network (DNN) -- to identify activities performed during synthetic biology experiments. The purpose is to provide real-time feedback to experimenters, thus helping to reduce human errors and improve experimental reproducibility. The data consists of unlabeled videos of recorded experiments and "weakly supervised" information (i.e., "theoretical" and asynchronous knowledge of sets of high level activity sequences in the experiment) used to train the system. Multiple activity sequences are modeled using a trellis, and deep features are extracted from video images. Model performance is accessed using real-time online statistical inference. The trellis incorporates variations during experiment execution, making our model very general and capable of high performance.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 2016 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Lavania, Chandrashekhar and Thulasidasan, Sunil and LaMarca, Anthony and Scofield, Jeffrey and Bilmes, Jeff},
	month = sep,
	year = {2016},
	keywords = {activity recognition, deep learning, dynamic graphical models, synthetic biology},
	pages = {37--48}
}

@inproceedings{wen_ubitouch_2016,
	address = {New York, NY, USA},
	series = {{UbiComp} '16},
	title = {{UbiTouch}: ubiquitous smartphone touchpads using built-in proximity and ambient light sensors},
	isbn = {978-1-4503-4461-6},
	shorttitle = {{UbiTouch}},
	url = {https://doi.org/10.1145/2971648.2971678},
	doi = {10.1145/2971648.2971678},
	abstract = {Smart devices are increasingly shrinking in size, which results in new challenges for user-mobile interaction through minuscule touchscreens. Existing works to explore alternative interaction technologies mainly rely on external devices which degrade portability. In this paper, we propose UbiTouch, a novel system that extends smartphones with virtual touchpads on desktops using built-in smartphone sensors. It senses a user's finger movement with a proximity and ambient light sensor whose raw sensory data from underlying hardware are strongly dependent on the finger's locations. UbiTouch maps the raw data into the finger's positions by utilizing Curvilinear Component Analysis and improve tracking accuracy via a particle filter. We have evaluate our system in three scenarios with different lighting conditions by five users. The results show that UbiTouch achieves centimetre-level localization accuracy and poses no significant impact on the battery life. We envisage that UbiTouch could support applications such as text-writing and drawing.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 2016 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Wen, Elliott and Seah, Winston and Ng, Bryan and Liu, Xuefeng and Cao, Jiannong},
	month = sep,
	year = {2016},
	pages = {286--297}
}

@inproceedings{jungwirth_mobeyele_2019,
	address = {New York, NY, USA},
	series = {{UbiComp}/{ISWC} '19 {Adjunct}},
	title = {{mobEYEle}: an embedded eye tracking platform for industrial assistance},
	isbn = {978-1-4503-6869-8},
	shorttitle = {{mobEYEle}},
	url = {https://doi.org/10.1145/3341162.3350842},
	doi = {10.1145/3341162.3350842},
	abstract = {The eyes are a particularly interesting modality for cognitive industrial assistance systems, as gaze analysis can reveal cognition- and task-related aspects, while gaze interaction depicts a lightweight and fast method for hands-free machine control. In this paper, we present mobEYEle, a body-worn eye tracking platform that performs the entire computation directly on the user, as opposed to primarily streaming the data to a centralized unit for online processing and hence restricting its pervasiveness. The applicability of the platform is demonstrated throughout extensive performance and battery runtime tests. Moreover, a self-contained calibration method is outlined that enables the usage of mobEYEle without any supervisor nor digital screen.},
	urldate = {2020-10-11},
	booktitle = {Adjunct {Proceedings} of the 2019 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing} and {Proceedings} of the 2019 {ACM} {International} {Symposium} on {Wearable} {Computers}},
	publisher = {Association for Computing Machinery},
	author = {Jungwirth, Florian and Murauer, Michaela and Selymes, Johannes and Haslgrübler, Michael and Gollan, Benedikt and Ferscha, Alois},
	month = sep,
	year = {2019},
	keywords = {eye-tracking, industrial assistance, pervasive computing},
	pages = {1113--1119}
}

@inproceedings{kim_say_2019,
	address = {New York, NY, USA},
	series = {{UIST} '19},
	title = {Say and {Find} it: {A} {Multimodal} {Wearable} {Interface} for {People} with {Visual} {Impairment}},
	isbn = {978-1-4503-6817-9},
	shorttitle = {Say and {Find} it},
	url = {https://doi.org/10.1145/3332167.3357104},
	doi = {10.1145/3332167.3357104},
	abstract = {Recent advances in computer vision and natural language processing using deep neural networks (DNNs) have enabled rich and intuitive multimodal interfaces. However, research on intelligent assistance systems for persons with visual impairment has not been well explored. In this work, we present an interactive object recognition and guidance interface based on multimodal interaction for blind and partially sighted people using an embedded mobile device. We demonstrate that the proposed solution using DNNs can effectively assist visually impaired people. We believe that this work will provide new and helpful insights for designing intelligent assistance systems in the future.},
	urldate = {2020-10-11},
	booktitle = {The {Adjunct} {Publication} of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Taeyong and Kim, Sanghong and Choi, Joonhee and Lee, Youngsun and Lee, Bowon},
	month = oct,
	year = {2019},
	keywords = {assistive system, mobile interface, multimodal wearable interface, visual impairment},
	pages = {27--29}
}

@inproceedings{bylinskii_learning_2017,
	address = {New York, NY, USA},
	series = {{UIST} '17},
	title = {Learning {Visual} {Importance} for {Graphic} {Designs} and {Data} {Visualizations}},
	isbn = {978-1-4503-4981-9},
	url = {https://doi.org/10.1145/3126594.3126653},
	doi = {10.1145/3126594.3126653},
	abstract = {Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 30th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Bylinskii, Zoya and Kim, Nam Wook and O'Donovan, Peter and Alsheikh, Sami and Madan, Spandan and Pfister, Hanspeter and Durand, Fredo and Russell, Bryan and Hertzmann, Aaron},
	month = oct,
	year = {2017},
	keywords = {computer vision, deep learning, eye tracking, graphic design, machine learning, retargeting, saliency, visualization},
	pages = {57--69},
	file = {Submitted Version:/Users/m3sibti/Zotero/storage/BMFPE8NF/Bylinskii et al. - 2017 - Learning Visual Importance for Graphic Designs and.pdf:application/pdf}
}
@inproceedings{yeo_opisthenar_2019,
	address = {New York, NY, USA},
	series = {{UIST} '19},
	title = {Opisthenar: {Hand} {Poses} and {Finger} {Tapping} {Recognition} by {Observing} {Back} of {Hand} {Using} {Embedded} {Wrist} {Camera}},
	isbn = {978-1-4503-6816-2},
	shorttitle = {Opisthenar},
	url = {https://doi.org/10.1145/3332165.3347867},
	doi = {10.1145/3332165.3347867},
	abstract = {We introduce a vision-based technique to recognize static hand poses and dynamic finger tapping gestures. Our approach employs a camera on the wrist, with a view of the opisthenar (back of the hand) area. We envisage such cameras being included in a wrist-worn device such as a smartwatch, fitness tracker or wristband. Indeed, selected off-the-shelf smartwatches now incorporate a built-in camera on the side for photography purposes. However, in this configuration, the fingers are occluded from the view of the camera. The oblique angle and placement of the camera make typical vision-based techniques difficult to adopt. Our alternative approach observes small movements and changes in the shape, tendons, skin and bones on the opisthenar area. We train deep neural networks to recognize both hand poses and dynamic finger tapping gestures. While this is a challenging configuration for sensing, we tested the recognition with a real-time user test and achieved a high recognition rate of 89.4\% (static poses) and 67.5\% (dynamic gestures). Our results further demonstrate that our approach can generalize across sessions and to new users. Namely, users can remove and replace the wrist-worn device while new users can employ a previously trained system, to a certain degree. We conclude by demonstrating three applications and suggest future avenues of work based on sensing the back of the hand.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Yeo, Hui-Shyong and Wu, Erwin and Lee, Juyoung and Quigley, Aaron and Koike, Hideki},
	month = oct,
	year = {2019},
	keywords = {back of the hand, finger tapping, hand pose, opisthenar},
	pages = {963--971},
	file = {Accepted Version:/Users/m3sibti/Zotero/storage/X5A27ZQX/Yeo et al. - 2019 - Opisthenar Hand Poses and Finger Tapping Recognit.pdf:application/pdf}
}

@inproceedings{kong_selecting_2016,
	address = {New York, NY, USA},
	series = {{UbiComp} '16},
	title = {Selecting home appliances with smart glass based on contextual information},
	isbn = {978-1-4503-4461-6},
	url = {https://doi.org/10.1145/2971648.2971651},
	doi = {10.1145/2971648.2971651},
	abstract = {We propose a method for selecting home appliances using a smart glass, which facilitates the control of network-connected appliances in a smart house. Our proposed method is image-based appliance selection and enables smart glass users to easily select a particular appliance by just looking at it. The main feature of our method is that it achieves high precision appliance selection using user contextual information such as position and activity, inferred from various sensor data in addition to camera images captured by the glass because such contextual information is greatly related in the home appliance that a user wants to control in her daily life. We design a state-of-the-art appliance selection method by fusing image features extracted by deep learning techniques and context information estimated by non-parametric Bayesian techniques within a framework of multiple kernel learning. Our experimental results, which use sensor data obtained in an actual house equipped with many network-connected appliances, show the effectiveness of our method.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 2016 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Kong, Quan and Maekawa, Takuya and Miyanishi, Taiki and Suyama, Takayuki},
	month = sep,
	year = {2016},
	keywords = {home appliances, smart glass, wearable computers},
	pages = {97--108},
	file = {Full Text PDF:/Users/m3sibti/Zotero/storage/MLLGRB38/Kong et al. - 2016 - Selecting home appliances with smart glass based o.pdf:application/pdf}
}

@inproceedings{mairittha_improving_2020,
	address = {New York, NY, USA},
	series = {{UbiComp}-{ISWC} '20},
	title = {Improving activity data collection with on-device personalization using fine-tuning},
	isbn = {978-1-4503-8076-8},
	url = {https://doi.org/10.1145/3410530.3414370},
	doi = {10.1145/3410530.3414370},
	abstract = {One of the biggest challenges of activity data collection is the unavoidability of relying on users and keep them engaged to provide labels consistently. Recent breakthroughs in mobile platforms have proven effective in bringing deep neural networks powered intelligence into mobile devices. In this study, we propose on-device personalization using fine-tuning convolutional neural networks as a mechanism in optimizing human effort in data labeling. First, we transfer the knowledge gained by on-cloud pre-training based on crowdsourced data to mobile devices. Second, we incrementally fine-tune a personalized model on every individual device using its locally accumulated input. Then, we utilize estimated activities customized according to the on-device model inference as feedback to motivate participants to improve data labeling. We conducted a verification study and gathered activity labels with smartphone sensors. Our preliminary evaluation results indicate that the proposed method outperformed the baseline method by approximately 8\% regarding accuracy recognition.},
	urldate = {2020-10-11},
	booktitle = {Adjunct {Proceedings} of the 2020 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing} and {Proceedings} of the 2020 {ACM} {International} {Symposium} on {Wearable} {Computers}},
	publisher = {Association for Computing Machinery},
	author = {Mairittha, Nattaya and Mairittha, Tittaya and Inoue, Sozo},
	month = sep,
	year = {2020},
	keywords = {activity recognition, data collection, fine-tuning, on-device deep learning},
	pages = {255--260},
	file = {Full Text PDF:/Users/m3sibti/Zotero/storage/PPGNYSDN/Mairittha et al. - 2020 - Improving activity data collection with on-device .pdf:application/pdf}
}

@inproceedings{stiehl_towards_2015,
	title = {Towards a {SignWriting} recognition system},
	doi = {10.1109/ICDAR.2015.7333719},
	abstract = {SignWriting is a writing system for sign languages. It is based on visual symbols to represent the hand shapes, movements and facial expressions, among other elements. It has been adopted by more than 40 countries, but to ensure the social integration of the deaf community, writing systems based on sign languages should be properly incorporated into the Information Technology. This article reports our first efforts toward the implementation of an automatic reading system for SignWiring. This would allow converting the SignWriting script into text so that one can store, retrieve, and index information in an efficient way. In order to make this work possible, we have been collecting a database of hand configurations, which at the present moment sums up to 7,994 images divided into 103 classes of symbols. To classify such symbols, we have performed a comprehensive set of experiments using different features, classifiers, and combination strategies. The best result, 94.4\% of recognition rate, was achieved by a Convolutional Neural Network.},
	booktitle = {2015 13th {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Stiehl, D. and Addams, L. and Oliveira, L. S. and Guimarães, C. and Britto, A. S.},
	month = aug,
	year = {2015},
	keywords = {automatic reading system, classifiers, convolutional neural network, Face, handicapped aids, image classification, neural nets, Optical devices, Optical imaging, sign language recognition, sign languages, SignWriting recognition system, SignWriting script, Thumb, Timing, visual symbols},
	pages = {26--30},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/6VU3D2BR/7333719.html:text/html}
}

@inproceedings{tensmeyer_convolutional_2017,
	title = {Convolutional {Neural} {Networks} for {Font} {Classification}},
	volume = {01},
	doi = {10.1109/ICDAR.2017.164},
	abstract = {Classifying pages or text lines into font categories aids transcription because single font Optical Character Recognition (OCR) is generally more accurate than omni-font OCR. We present a simple framework based on Convolutional Neural Networks (CNNs), where a CNN is trained to classify small patches of text into predefined font classes. To classify page or line images, we average the CNN predictions over densely extracted patches. We show that this method achieves state-of-the-art performance on a challenging dataset of 40 Arabic computer fonts with 98.8\% line level accuracy. This same method also achieves the highest reported accuracy of 86.6\% in predicting paleographic scribal script classes at the page level on medieval Latin manuscripts. Finally, we analyze what features are learned by the CNN on Latin manuscripts and find evidence that the CNN is learning both the defining morphological differences between scribal script classes as well as overfitting to class-correlated nuisance factors. We propose a novel form of data augmentation that improves robustness to text darkness, further increasing classification performance.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Tensmeyer, Chris and Saunders, Daniel and Martinez, Tony},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Arabic computer fonts, character sets, class-correlated nuisance factors, CNN predictions, Computational modeling, Computer architecture, convolutional neural networks, Convolutional Neural Networks, data augmentation, Data Augmentation, Deep Learning, densely extracted patches, Document Image Classification, document image processing, feature extraction, Feature extraction, font classification, image classification, learning (artificial intelligence), medieval Latin manuscripts, morphological differences, Network Architecture, neural nets, optical character recognition, Optical character recognition software, paleographic scribal script classes, predefined font classes, Preprocessing, single font optical character recognition, Task analysis, text darkness, text lines, Training, Training data},
	pages = {985--990},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/836FA5XM/8270095.html:text/html;Submitted Version:/Users/m3sibti/Zotero/storage/S3REZZ5B/Tensmeyer et al. - 2017 - Convolutional Neural Networks for Font Classificat.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/KJK5RQ6V/8270095.html:text/html}
}

@inproceedings{mohian_doodle2app_2020,
	address = {New York, NY, USA},
	series = {{MOBILESoft} '20},
	title = {{Doodle2App}: native app code by freehand {UI} sketching},
	isbn = {978-1-4503-7959-5},
	shorttitle = {{Doodle2App}},
	url = {https://doi.org/10.1145/3387905.3388607},
	doi = {10.1145/3387905.3388607},
	abstract = {User interface development typically starts with freehand sketching, with pen on paper, which creates a big gap in the software development process. Recent advances in deep neural networks that have been trained on large sketch stroke sequence collections have enabled online sketch detection that supports many sketch element classes at high classification accuracy. This paper leverages the recent Google Quick, Draw! dataset of 50M sketch stroke sequences to pre-train a recurrent neural network and retrains it with sketch stroke sequences we collected via Amazon Mechanical Turk. The resulting Doodle2App website offers a paper substitute, i.e., a drawing interface with interactive UI preview and can convert sketches to a compilable single-page Android application. On 712 sketch samples Doodle2App achieved higher accuracy than the state-of-the-art tool Teleport. A video demo is at https://youtu.be/P4sb0pKTNEY},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the {IEEE}/{ACM} 7th {International} {Conference} on {Mobile} {Software} {Engineering} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mohian, Soumik and Csallner, Christoph},
	month = jul,
	year = {2020},
	keywords = {deep learning, GUI, prototyping, sketching, user interface design},
	pages = {81--84},
	file = {Full Text PDF:/Users/m3sibti/Zotero/storage/JLBCAK88/Mohian and Csallner - 2020 - Doodle2App native app code by freehand UI sketchi.pdf:application/pdf}
}

@inproceedings{zhang_type_2019,
	address = {New York, NY, USA},
	series = {{UIST} '19},
	title = {Type, {Then} {Correct}: {Intelligent} {Text} {Correction} {Techniques} for {Mobile} {Text} {Entry} {Using} {Neural} {Networks}},
	isbn = {978-1-4503-6816-2},
	shorttitle = {Type, {Then} {Correct}},
	url = {https://doi.org/10.1145/3332165.3347924},
	doi = {10.1145/3332165.3347924},
	abstract = {Current text correction processes on mobile touch devices are laborious: users either extensively use backspace, or navigate the cursor to the error position, make a correction, and navigate back, usually by employing multiple taps or drags over small targets. In this paper, we present three novel text correction techniques to improve the correction process: Drag-n-Drop, Drag-n-Throw, and Magic Key. All of the techniques skip error-deletion and cursor-positioning procedures, and instead allow the user to type the correction first, and then apply that correction to a previously committed error. Specifically, Drag-n-Drop allows a user to drag a correction and drop it on the error position. Drag-n-Throw lets a user drag a correction from the keyboard suggestion list and "throw" it to the approximate area of the error text, with a neural network determining the most likely error in that area. Magic Key allows a user to type a correction and tap a designated key to highlight possible error candidates, which are also determined by a neural network. The user can navigate among these candidates by directionally dragging from atop the key, and can apply the correction by simply tapping the key. We evaluated these techniques in both text correction and text composition tasks. Our results show that correction with the new techniques was faster than de facto cursor and backspace-based correction. Our techniques apply to any touch-based text entry method.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Mingrui Ray and Wen, He and Wobbrock, Jacob O.},
	month = oct,
	year = {2019},
	keywords = {gestures, natural language processing, text editing, touch},
	pages = {843--855}
}

@inproceedings{huang_sketchforme_2019,
	address = {New York, NY, USA},
	series = {{UIST} '19},
	title = {Sketchforme: {Composing} {Sketched} {Scenes} from {Text} {Descriptions} for {Interactive} {Applications}},
	isbn = {978-1-4503-6816-2},
	shorttitle = {Sketchforme},
	url = {https://doi.org/10.1145/3332165.3347878},
	doi = {10.1145/3332165.3347878},
	abstract = {Sketching is an effective communication medium that augments and enhances what can be communicated in text. We introduce Sketchforme, the first neural-network-based system that can generate complex sketches based on text descriptions specified by users. Sketchforme's key contribution is to factor complex sketch rendering into layout and rendering subtasks using neural networks. The sketches composed by Sketchforme are expressive and realistic: we show in our user study that these sketches convey descriptions better than human-generated sketches in several cases, and 36.5\% of those sketches were identified as human-generated. We develop some interactive applications using these generated sketches, and show that Sketchforme can significantly improve language learning applications and support intelligent language-based sketching assistants.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Forrest and Canny, John F.},
	month = oct,
	year = {2019},
	keywords = {deep learning, generative models, interactive applications, interactive machine learning, natural language, neural networks, sketching, transformer},
	pages = {209--220},
	file = {Full Text PDF:/Users/m3sibti/Zotero/storage/HALD8C28/Huang and Canny - 2019 - Sketchforme Composing Sketched Scenes from Text D.pdf:application/pdf}
}

@inproceedings{sun_lip-interact_2018,
	address = {New York, NY, USA},
	series = {{UIST} '18},
	title = {Lip-{Interact}: {Improving} {Mobile} {Device} {Interaction} with {Silent} {Speech} {Commands}},
	isbn = {978-1-4503-5948-1},
	shorttitle = {Lip-{Interact}},
	url = {https://doi.org/10.1145/3242587.3242599},
	doi = {10.1145/3242587.3242599},
	abstract = {We present Lip-Interact, an interaction technique that allows users to issue commands on their smartphone through silent speech. Lip-Interact repurposes the front camera to capture the user's mouth movements and recognize the issued commands with an end-to-end deep learning model. Our system supports 44 commands for accessing both system-level functionalities (launching apps, changing system settings, and handling pop-up windows) and application-level functionalities (integrated operations for two apps). We verify the feasibility of Lip-Interact with three user experiments: evaluating the recognition accuracy, comparing with touch on input efficiency, and comparing with voiced commands with regards to personal privacy and social norms. We demonstrate that Lip-Interact can help users access functionality efficiently in one step, enable one-handed input when the other hand is occupied, and assist touch to make interactions more fluent.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 31st {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Ke and Yu, Chun and Shi, Weinan and Liu, Lan and Shi, Yuanchun},
	month = oct,
	year = {2018},
	keywords = {lip interaction, mobile interaction, semantic gesture, silent speech, touch-free, vision-based recognition},
	pages = {581--593}
}

@inproceedings{zhou_lancer_2019,
	title = {Lancer: {Your} {Code} {Tell} {Me} {What} {You} {Need}},
	shorttitle = {Lancer},
	doi = {10.1109/ASE.2019.00137},
	abstract = {Programming is typically a difficult and repetitive task. Programmers encounter endless problems during programming, and they often need to write similar code over and over again. To prevent programmers from reinventing wheels thus increase their productivity, we propose a context-aware code-to-code recommendation tool named Lancer. With the support of a Library-Sensitive Language Model (LSLM) and the BERT model, Lancer is able to automatically analyze the intention of the incomplete code and recommend relevant and reusable code samples in real-time. A video demonstration of Lancer can be found at https://youtu.be/tO9nhqZY35g. Lancer is open source and the code is available at https://github.com/sfzhou5678/Lancer.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Zhou, Shufan and Shen, Beijun and Zhong, Hao},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {BERT model, Cloning, Code recommendation, Code reuse, context-aware code-to-code recommendation tool, incomplete code, Lancer, Language model, Libraries, library-sensitive language model, LSLM, Mathematical model, open source, programming, Programming, public domain software, recommender systems, reusable code samples, Semantics, Task analysis, Tools, ubiquitous computing},
	pages = {1202--1205},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/K65CWQ5I/8952168.html:text/html}
}

@inproceedings{bhatt_digital_2019,
	title = {Digital {Auditor}: {A} {Framework} for {Matching} {Duplicate} {Invoices}},
	shorttitle = {Digital {Auditor}},
	doi = {10.1109/ICDAR.2019.00076},
	abstract = {Duplicate invoice payment is one of the most prominent challenges encountered by accounts payable operations, and whenever it occurs, it costs to the company. Due to large volume and variety of invoices across multiple suppliers, it is not pragmatic to manually examine every invoice to check if it is legitimate and has not been previously financed. This paper presents Digital Auditor (DA), an automated framework for detecting duplicate invoices. It is based on two principles 1) converting invoices into structured templates by extracting relevant information from the invoices and organizing it as key-value pairs and 2) machine learning based duplicate detection algorithm which compares corresponding fields between two invoices and identifies duplicate invoice pairs. Digital Auditor efficiently identifies duplicate pairs, and thus alleviates laborious manual efforts and time in inspecting the invoices against the previously paid invoices. To demonstrate the efficacy of Digital Auditor, this paper presents comprehensive experimental results and key observations from user-trials by business professionals on a large sample of invoices from a non-production environment.},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Bhatt, Himanshu Sharad and Roy, Shourya and Bhatnagar, Lokesh and Lohani, Chetan and Jain, Vinit},
	month = sep,
	year = {2019},
	note = {ISSN: 2379-2140},
	keywords = {automated framework, business professionals, Data mining, Databases, Dictionaries, digital auditor, Duplicate Detection, duplicate detection algorithm, duplicate invoice pairs, duplicate invoice payment, Face, Invoice Processing, invoicing, key-value pairs, Layout, learning (artificial intelligence), machine learning, Machine learning, Machine Learning, nonproduction environment, paid invoices, pattern matching, structured templates, Task analysis},
	pages = {434--441},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/Q7E7A888/8978075.html:text/html}
}

@inproceedings{chen_ui_2018,
	address = {New York, NY, USA},
	series = {{ICSE} '18},
	title = {From {UI} design image to {GUI} skeleton: a neural machine translator to bootstrap mobile {GUI} implementation},
	isbn = {978-1-4503-5638-1},
	shorttitle = {From {UI} design image to {GUI} skeleton},
	url = {https://doi.org/10.1145/3180155.3180240},
	doi = {10.1145/3180155.3180240},
	abstract = {A GUI skeleton is the starting point for implementing a UI design image. To obtain a GUI skeleton from a UI design image, developers have to visually understand UI elements and their spatial layout in the image, and then translate this understanding into proper GUI components and their compositions. Automating this visual understanding and translation would be beneficial for bootstraping mobile GUI implementation, but it is a challenging task due to the diversity of UI designs and the complexity of GUI skeletons to generate. Existing tools are rigid as they depend on heuristically-designed visual understanding and GUI generation rules. In this paper, we present a neural machine translator that combines recent advances in computer vision and machine translation for translating a UI design image into a GUI skeleton. Our translator learns to extract visual features in UI images, encode these features' spatial layouts, and generate GUI skeletons in a unified neural network framework, without requiring manual rule development. For training our translator, we develop an automated GUI exploration method to automatically collect large-scale UI data from real-world applications. We carry out extensive experiments to evaluate the accuracy, generality and usefulness of our approach.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Chunyang and Su, Ting and Meng, Guozhu and Xing, Zhenchang and Liu, Yang},
	month = may,
	year = {2018},
	keywords = {deep learning, reverse engineering, user interface},
	pages = {665--676}
}

@inproceedings{ge_android_2019,
	address = {Montreal, Quebec, Canada},
	series = {{ICSE} '19},
	title = {Android {GUI} search using hand-drawn sketches},
	url = {https://doi.org/10.1109/ICSE-Companion.2019.00060},
	doi = {10.1109/ICSE-Companion.2019.00060},
	abstract = {GUI design is crucial to mobile apps. In the early stages of mobile app development, having access to visually similar apps can help designers and programmers gain inspiration for revising their designs or even reuse existing GUI code. We propose an intuitive sketch modelling language to draw GUI sketches, and a deep learning based method to search for visually similar apps according to the sketches. Preliminary results show the potential of our approach.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings}},
	publisher = {IEEE Press},
	author = {Ge, Xiaofei},
	month = may,
	year = {2019},
	keywords = {Android app, code search and recommendation, graphical user interface, reverse engineering},
	pages = {141--143}
}

@inproceedings{zheng_ifeedback_2019,
	title = {{iFeedback}: {Exploiting} {User} {Feedback} for {Real}-{Time} {Issue} {Detection} in {Large}-{Scale} {Online} {Service} {Systems}},
	shorttitle = {{iFeedback}},
	doi = {10.1109/ASE.2019.00041},
	abstract = {Large-scale online systems are complex, fast-evolving, and hardly bug-free despite the testing efforts. Backend system monitoring cannot detect many types of issues, such as UI related bugs, bugs with small impact on backend system indicators, or errors from third-party co-operating systems, etc. However, users are good informers of such issues: They will provide their feedback for any types of issues. This experience paper discusses our design of iFeedback, a tool to perform real-time issue detection based on user feedback texts. Unlike traditional approaches that analyze user feedback with computation-intensive natural language processing algorithms, iFeedback is focusing on fast issue detection, which can serve as a system life-condition monitor. In particular, iFeedback extracts word combination-based indicators from feedback texts. This allows iFeedback to perform fast system anomaly detection with sophisticated machine learning algorithms. iFeedback then further summarizes the texts with an aim to effectively present the anomaly to the developers for root cause analysis. We present our representative experiences in successfully applying iFeedback in tens of large-scale production online service systems in ten months.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Zheng, Wujie and Lu, Haochuan and Zhou, Yangfan and Liang, Jianming and Zheng, Haibing and Deng, Yuetang},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Anomaly detection, backend system indicators, backend system monitoring, Bug and vulnerability detection, bug-free systems, bugs, computation-intensive natural language processing algorithms, Computer bugs, condition monitoring, fast system anomaly detection, iFeedback, large-scale online systems, large-scale production online service systems, learning (artificial intelligence), machine learning algorithms, Monitoring, natural language processing, online service systems, program debugging, real-time issue detection, Real-time systems, Runtime, system life-condition monitor, Task analysis, third-party co-operating systems, user feedback texts, Web services, word combination-based indicators},
	pages = {352--363},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/S95UQXIZ/8952229.html:text/html}
}

@inproceedings{gu_what_2015,
	title = {"{What} {Parts} of {Your} {Apps} are {Loved} by {Users}?" ({T})},
	shorttitle = {"{What} {Parts} of {Your} {Apps} are {Loved} by {Users}?},
	doi = {10.1109/ASE.2015.57},
	abstract = {Recently, Begel et al. found that one of the most important questions software developers ask is "what parts of software are used/loved by users." User reviews provide an effective channel to address this question. However, most existing review summarization tools treat reviews as bags-of-words (i.e., mixed review categories) and are limited to extract software aspects and user preferences. We present a novel review summarization framework, SUR-Miner. Instead of a bags-of-words assumption, it classifies reviews into five categories and extracts aspects for sentences which include aspect evaluation using a pattern-based parser. Then, SUR-Miner visualizes the summaries using two interactive diagrams. Our evaluation on seventeen popular apps shows that SUR-Miner summarizes more accurate and clearer aspects than state-of-the-art techniques, with an F1-score of 0.81, significantly greater than that of ReviewSpotlight (0.56) and Guzmans' method (0.55). Feedback from developers shows that 88\% developers agreed with the usefulness of the summaries from SUR-Miner.},
	booktitle = {2015 30th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Gu, Xiaodong and Kim, Sunghun},
	month = nov,
	year = {2015},
	keywords = {bags-of-words, data mining, Data mining, Data Mining, diagrams, feature extraction, Feature extraction, grammars, interactive diagram, Market research, pattern classification, pattern-based parser, review classification, Review Summarization, review summarization tool, Semantics, Sentiment Analysis, Software, software aspect extraction, software development, software engineering, Software engineering, software reviews, SUR-Miner, User Feedback, user preference, Visualization},
	pages = {760--770},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/TC3BLLB7/7372064.html:text/html}
}

@inproceedings{wang_e-book_2012,
	title = {E-book recommender system design and implementation based on data mining},
	volume = {8350},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8350/835017/E-book-recommender-system-design-and-implementation-based-on-data/10.1117/12.920279.short},
	doi = {10.1117/12.920279},
	abstract = {In the knowledge explosion, rapid development of information age, how quickly the user or users interested in useful information for feedback to the user problem to be solved in this article. This paper based on data mining, association rules to the model and classification model a combination of electronic books on the recommendation of the user's neighboring users interested in e-books to target users. Introduced the e-book recommendation and the key technologies, system implementation algorithms, and implementation process, was proved through experiments that this system can help users quickly find the required e-books.},
	urldate = {2020-10-11},
	booktitle = {Fourth {International} {Conference} on {Machine} {Vision} ({ICMV} 2011): {Computer} {Vision} and {Image} {Analysis}; {Pattern} {Recognition} and {Basic} {Technologies}},
	publisher = {International Society for Optics and Photonics},
	author = {Wang, Zongjiang},
	month = jan,
	year = {2012},
	pages = {835017},
	file = {Snapshot:/Users/m3sibti/Zotero/storage/RFMW42TZ/12.920279.html:text/html}
}

@inproceedings{zhang_mallard_2019,
	address = {New York, NY, USA},
	series = {{UIST} '19},
	title = {Mallard: {Turn} the {Web} into a {Contextualized} {Prototyping} {Environment} for {Machine} {Learning}},
	isbn = {978-1-4503-6816-2},
	shorttitle = {Mallard},
	url = {https://doi.org/10.1145/3332165.3347936},
	doi = {10.1145/3332165.3347936},
	abstract = {Machine learning (ML) can be hard to master, but what first trips up novices is something much more mundane: the incidental complexities of installing and configuring software development environments. Everyone has a web browser, so can we let people experiment with ML within the context of any webpage they visit? This paper's contribution is the idea that the web can serve as a contextualized prototyping environment for ML by enabling analyses to occur within the context of data on actual webpages rather than in isolated silos. We realized this idea by building Mallard, a browser extension that scaffolds acquiring and parsing web data, prototyping with pretrained ML models, and augmenting webpages with ML-driven results and interactions. To demonstrate the versatility of Mallard, we performed a case study where we used it to prototype nine ML-based browser apps, including augmenting Amazon and Twitter websites with sentiment analysis, augmenting restaurant menu websites with OCR-based search, using real-time face tracking to control a Pac-Man game, and style transfer on Google image search results. These case studies show that Mallard is capable of supporting a diverse range of hobbyist-level ML prototyping projects.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Xiong and Guo, Philip J.},
	month = oct,
	year = {2019},
	keywords = {contextualized machine learning, ml prototyping},
	pages = {605--618}
}

@inproceedings{ji_uichecker_2018,
	title = {{UIChecker}: {An} {Automatic} {Detection} {Platform} for {Android} {GUI} {Errors}},
	shorttitle = {{UIChecker}},
	doi = {10.1109/ICSESS.2018.8663923},
	abstract = {At present, Android automated GUI testing has been widely used in mobile application testing. Automated GUI test input generation technology and tools are hot topics for practitioners, but errors in some test screenshots generated by automated test input tools still need to be reviewed manually. In this paper, we creatively proposed an automatic detection platform for GUI errors, detecting the GUI errors of mobile related and image-related widget error classification model through machine learning, which detects the error of widgets. On all experimental App test sets, the accuracy of the text-related widget error classification model reached an average of 98.06\%, and the accuracy of image-related widgets error classification model achieved an average of 95.44\%, which greatly reduced the time cost of reviewing GUI errors manually. In addition, we analyze the relative positional relationship between the widgets, and use the Wilson score sorting algorithm to analyze the symbiosis and interdependence between the widgets, and finally generate the assertion tables, thus more complex GUI errors can be detected.},
	booktitle = {2018 {IEEE} 9th {International} {Conference} on {Software} {Engineering} and {Service} {Science} ({ICSESS})},
	author = {Ji, Meichen},
	month = nov,
	year = {2018},
	note = {ISSN: 2327-0594},
	keywords = {Android (operating system), Android automated GUI testing, Android GUI errors, assertions, automated GUI test input generation technology, automated GUI testing, automated test input tools, automated traversal tool, automatic detection platform, complex GUI errors, experimental app test sets, Feature extraction, graphical user interfaces, Graphical user interfaces, GUI error detection, image-related widgets error classification model, machine learning, mobile application testing, Mobile applications, mobile computing, mobile related image-related widget error classification model, pattern classification, program testing, Symbiosis, test screenshots, Testing, text-related widget error classification model, Tools, Training},
	pages = {957--961},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/86A5GZG5/8663923.html:text/html}
}

@inproceedings{wu_exception_2017,
	title = {Exception beyond {Exception}: {Crashing} {Android} {System} by {Trapping} in "{Uncaught} {Exception}"},
	shorttitle = {Exception beyond {Exception}},
	doi = {10.1109/ICSE-SEIP.2017.12},
	abstract = {Android is characterized as a complicated open source software stack created for a wide array of devices with different form of factors, whose latest release has over one hundred million lines of code. Such code is mainly developed with the Java language, which builds complicated logic and brings implicit information flows among components and the inner framework. By studying the source code of system service interfaces, we discovered an unknown type of code flaw, which is named uncaughtException flaw, caused by un-well implemented exceptions that could crash the system and be further vulnerable to system level Denial-of-Service (DoS) attacks. We found that exceptions are used to handle the errors and other exceptional events but sometimes they would kill some critical system services exceptionally. We designed and implemented ExHunter, a new tool for automatic detection of this uncaughtException flaw by dynamically reflecting service interfaces, continuously fuzzing parameters and verifying the running logs. On 11 new popular Android devices, ExHunter extracted 1045 system services, reflected 758 suspicious functions, discovered 132 uncaughtException flaws which are 0-day vulnerabilities that have never been known before and generated 275 system DoS attack exploitations. The results showed that: (1) almost every type of Android phone suffers from this flaw, (2) the flaws are different from phone by phone, and (3) all the vulnerabilities can be exploited by direct/indirect trapping. To mitigate uncaughtException flaws, we further developed ExCatcher to re-catch the exceptions. Finally, we informed four internationally renowned manufacturers and provided secure improvements in their commercial phones.},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} {Track} ({ICSE}-{SEIP})},
	author = {Wu, Jingzheng and Liu, Shen and Ji, Shouling and Yang, Mutian and Luo, Tianyue and Wu, Yanjun and Wang, Yongji},
	month = may,
	year = {2017},
	keywords = {Android system crashing, Android System Service, Computer crashes, Computer crime, denial-of-service attacks, DoS attack, DoS Attack, ExCatcher, Exception, ExHunter, Java, Message systems, mobile computing, open source software stack, public domain software, security of data, Smart phones, Software, source code, uncaughtException flaw, Vulnerability},
	pages = {283--292},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/IGMTF85V/7965452.html:text/html}
}

@inproceedings{jalaliniya_touch-less_2013,
	address = {New York, NY, USA},
	series = {{UbiComp} '13 {Adjunct}},
	title = {Touch-less interaction with medical images using hand \&amp; foot gestures},
	isbn = {978-1-4503-2215-7},
	url = {https://doi.org/10.1145/2494091.2497332},
	doi = {10.1145/2494091.2497332},
	abstract = {Sterility restrictions in surgical settings make touch-less interaction an interesting solution for surgeons to interact directly with digital images. The HCI community has already explored several methods for touch-less interaction including those based on camera-based gesture tracking and voice control. In this paper, we present a system for gesture-based interaction with medical images based on a single wristband sensor and capacitive floor sensors, allowing for hand and foot gesture input. The first limited evaluation of the system showed an acceptable level of accuracy for 12 different hand \& foot gestures; also users found that our combined hand and foot based gestures are intuitive for providing input.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 2013 {ACM} conference on {Pervasive} and ubiquitous computing adjunct publication},
	publisher = {Association for Computing Machinery},
	author = {Jalaliniya, Shahram and Smith, Jeremiah and Sousa, Miguel and Büthe, Lars and Pederson, Thomas},
	month = sep,
	year = {2013},
	keywords = {floor sensor, gesture-based interaction, touch-less interaction in hospital, wearable sensor},
	pages = {1265--1274}
}

@inproceedings{gollan_demonstrator_2016,
	address = {New York, NY, USA},
	series = {{UbiComp} '16},
	title = {Demonstrator for extracting cognitive load from pupil dilation for attention management services},
	isbn = {978-1-4503-4462-3},
	url = {https://doi.org/10.1145/2968219.2968550},
	doi = {10.1145/2968219.2968550},
	abstract = {Attention Management has become a fundamental requirement in interaction design of information systems, considering the information overload distributed by omnipresent wearable devices. This implies the management of notifications and interruptions according to current user activities, context and especially cognitive load and perception capabilities. This paper presents a demonstrator designed towards the real-time assessment of cognitive load from pupil dilation as a somatic indicator of attention, which can be exploited as input for the control of future attention-aware interaction designs. Cognitive Load is modeled from pupil dilation via exploiting the task-evoked pupil response while considering disturbances of illumination and blink activities.},
	urldate = {2020-10-11},
	booktitle = {Proceedings of the 2016 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}: {Adjunct}},
	publisher = {Association for Computing Machinery},
	author = {Gollan, Benedikt and Haslgrübler, Michael and Ferscha, Alois},
	month = sep,
	year = {2016},
	keywords = {attention \& interruption management, attention estimation, cognitive load, pupillometry},
	pages = {1566--1571}
}

@inproceedings{ren_immersive_2010,
	title = {Immersive and perceptual human-computer interaction using computer vision techniques},
	doi = {10.1109/CVPRW.2010.5543161},
	abstract = {Computer vision techniques have been widely applied to immersive and perceptual human-computer interaction for applications like computer gaming, education, and entertainment. In this paper, relevant techniques are surveyed in terms of image capturing, normalization, motion detection, tracking, feature representation and recognition. In addition, applications of vision techniques in HCI in computer gaming are also summarized in several categories including vision enabled pointing and positioning, vision for manipulating objects, training and education, and miscellaneous applications. The characteristics of existing work are analyzed and discussed, along with corresponding challenges and future research directions proposed.},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Workshops}},
	author = {Ren, Jinchang and Vlachos, Theodore and Argyriou, Vasileios},
	month = jun,
	year = {2010},
	note = {ISSN: 2160-7516},
	keywords = {Application software, Computer applications, computer gaming, Computer science education, computer vision, Computer vision, computer vision techniques, Face recognition, feature extraction, feature recognition, feature representation, human computer interaction, Human computer interaction, image capturing, Image recognition, motion detection, Motion detection, object recognition, perceptual human computer interaction, Tracking, User interfaces},
	pages = {66--72},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/537XQXR3/5543161.html:text/html}
}

@inproceedings{kolthoff_automatic_2019,
	title = {Automatic {Generation} of {Graphical} {User} {Interface} {Prototypes} from {Unrestricted} {Natural} {Language} {Requirements}},
	doi = {10.1109/ASE.2019.00148},
	abstract = {High-fidelity GUI prototyping provides a meaningful manner for illustrating the developers' understanding of the requirements formulated by the customer and can be used for productive discussions and clarification of requirements and expectations. However, high-fidelity prototypes are time-consuming and expensive to develop. Furthermore, the interpretation of requirements expressed in informal natural language is often error-prone due to ambiguities and misunderstandings. In this dissertation project, we will develop a methodology based on Natural Language Processing (NLP) for supporting GUI prototyping by automatically translating Natural Language Requirements (NLR) into a formal Domain-Specific Language (DSL) describing the GUI and its navigational schema. The generated DSL can be further translated into corresponding target platform prototypes and directly provided to the user for inspection. Most related systems stop after generating artifacts, however, we introduce an intelligent and automatic interaction mechanism that allows users to provide natural language feedback on generated prototypes in an iterative fashion, which accordingly will be translated into respective prototype changes.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Kolthoff, Kristian},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Adaptation models, Automatic, automatic generation, automatic interaction mechanism, Computational modeling, domain-specific language, DSL, Generation, Graphical, graphical user interface prototypes, graphical user interfaces, Graphical user interfaces, GUI, high-fidelity GUI prototyping, Intelligent, intelligent interaction mechanism, Interaction, Interface, Language, Natural, natural language feedback, natural language interfaces, natural language processing, Natural languages, Navigation, Processing, Prototypes, Prototyping, Requirements, Software, specification languages, target platform prototypes, unrestricted natural language requirements, User},
	pages = {1234--1237},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/5DZX9L5G/8952477.html:text/html}
}
@article{ye_aranimator_2020,
	title = {\textit{{ARAnimator}}: in-situ character animation in mobile {AR} with user-defined motion gestures},
	volume = {39},
	issn = {0730-0301},
	shorttitle = {\textit{{ARAnimator}}},
	url = {https://doi.org/10.1145/3386569.3392404},
	doi = {10.1145/3386569.3392404},
	abstract = {Creating animated virtual AR characters closely interacting with real environments is interesting but difficult. Existing systems adopt video see-through approaches to indirectly control a virtual character in mobile AR, making close interaction with real environments not intuitive. In this work we use an AR-enabled mobile device to directly control the position and motion of a virtual character situated in a real environment. We conduct two guessability studies to elicit user-defined motions of a virtual character interacting with real environments, and a set of user-defined motion gestures describing specific character motions. We found that an SVM-based learning approach achieves reasonably high accuracy for gesture classification from the motion data of a mobile device. We present ARAnimator, which allows novice and casual animation users to directly represent a virtual character by an AR-enabled mobile phone and control its animation in AR scenes using motion gestures of the device, followed by animation preview and interactive editing through a video see-through interface. Our experimental results show that with ARAnimator, users are able to easily create in-situ character animations closely interacting with different real environments.},
	number = {4},
	urldate = {2020-10-09},
	journal = {ACM Transactions on Graphics},
	author = {Ye, Hui and Kwan, Kin Chung and Su, Wanchao and Fu, Hongbo},
	month = jul,
	year = {2020},
	keywords = {character animation, gesture Classification, interactive system, mobile augmented reality, user defined gestures},
	pages = {83:83:1--83:83:12}
}

@article{jin_voco_2017,
	title = {{VoCo}: text-based insertion and replacement in audio narration},
	volume = {36},
	issn = {0730-0301},
	shorttitle = {{VoCo}},
	url = {https://doi.org/10.1145/3072959.3073702},
	doi = {10.1145/3072959.3073702},
	abstract = {Editing audio narration using conventional software typically involves many painstaking low-level manipulations. Some state of the art systems allow the editor to work in a text transcript of the narration, and perform select, cut, copy and paste operations directly in the transcript; these operations are then automatically applied to the waveform in a straightforward manner. However, an obvious gap in the text-based interface is the ability to type new words not appearing in the transcript, for example inserting a new word for emphasis or replacing a misspoken word. While high-quality voice synthesizers exist today, the challenge is to synthesize the new word in a voice that matches the rest of the narration. This paper presents a system that can synthesize a new word or short phrase such that it blends seamlessly in the context of the existing narration. Our approach is to use a text to speech synthesizer to say the word in a generic voice, and then use voice conversion to convert it into a voice that matches the narration. Offering a range of degrees of control to the editor, our interface supports fully automatic synthesis, selection among a candidate set of alternative pronunciations, fine control over edit placements and pitch profiles, and even guidance by the editors own voice. The paper presents studies showing that the output of our method is preferred over baseline methods and often indistinguishable from the original voice.},
	number = {4},
	urldate = {2020-10-09},
	journal = {ACM Transactions on Graphics},
	author = {Jin, Zeyu and Mysore, Gautham J. and Diverdi, Stephen and Lu, Jingwan and Finkelstein, Adam},
	month = jul,
	year = {2017},
	keywords = {audio, human computer interaction},
	pages = {96:1--96:13}
}

@article{liu_understanding_2018,
	title = {Understanding {Diverse} {Usage} {Patterns} from {Large}-{Scale} {Appstore}-{Service} {Profiles}},
	volume = {44},
	issn = {1939-3520},
	doi = {10.1109/TSE.2017.2685387},
	abstract = {The prevalence of smart mobile devices has promoted the popularity of mobile applications (a.k.a. apps). Supporting mobility has become a promising trend in software engineering research. This article presents an empirical study of behavioral service profiles collected from millions of users whose devices are deployed with Wandoujia, a leading Android app-store service in China. The dataset of Wandoujia service profiles consists of two kinds of user behavioral data from using 0.28 million free Android apps, including (1) app management activities (i.e., downloading, updating, and uninstalling apps) from over 17 million unique users and (2) app network usage from over 6 million unique users. We explore multiple aspects of such behavioral data and present patterns of app usage. Based on the findings as well as derived knowledge, we also suggest some new open opportunities and challenges that can be explored by the research community, including app development, deployment, delivery, revenue, etc.},
	number = {4},
	journal = {IEEE Transactions on Software Engineering},
	author = {Liu, Xuanzhe and Li, Huoran and Lu, Xuan and Xie, Tao and Mei, Qiaozhu and Feng, Feng and Mei, Hong},
	month = apr,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Android (operating system), Androids, app development, app management activities, app network usage, app store, app usage, behavioral service profiles, Biological system modeling, diverse usage patterns, Electronic mail, free Android apps, Humanoid robots, large-scale appstore-service profiles, leading Android app-store service, mobile applications, Mobile apps, Mobile communication, mobile computing, public domain software, smart mobile devices, Software, software engineering, Software engineering, software engineering research, user behavior analysis, user behavioral data, Wandoujia service profiles},
	pages = {384--411},
	file = {IEEE Xplore Full Text PDF:/Users/m3sibti/Zotero/storage/JBADKSYE/Liu et al. - 2018 - Understanding Diverse Usage Patterns from Large-Sc.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/ZEN3PSTW/7883939.html:text/html}
}

@article{zhong_dynamic_2020,
	title = {A {Dynamic} {User} {Interface} {Based} {BCI} {Environmental} {Control} {System}},
	volume = {36},
	issn = {1044-7318},
	url = {https://doi.org/10.1080/10447318.2019.1604473},
	doi = {10.1080/10447318.2019.1604473},
	abstract = {In this study, a dynamic user interface (UI) is proposed in visual P300 Brain-Computer Interface (BCI) based environmental control system. A head-mounted Augmented Reality (AR) glass is used as the interactive media, which is used to assists the BCI system to build the dynamic UI with the scene in subject’s field of view. In the dynamic UI, based on the objects detected by the AR glass, options are dynamically generated. The subject can assign tasks by selecting different options in the dynamic UI. Five subjects successfully completed the task of controlling household appliances and navigating wheelchairs to designated destinations. Compared to static UI, the proposed dynamic UI has a 17.4\% improvement in time delay. On average, only 1.9\% of the commands resulted in incorrect operations. The dynamic UI makes progress in reducing time delay and incorrect operations. The proposed system provides a brand-new interactive method in BCI based applications.},
	number = {1},
	urldate = {2020-10-09},
	journal = {International Journal of Human–Computer Interaction},
	author = {Zhong, Saisai and Liu, Yadong and Yu, Yang and Tang, Jingsheng and Zhou, Zongtan and Hu, Dewen},
	month = jan,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2019.1604473},
	pages = {55--66},
	file = {Snapshot:/Users/m3sibti/Zotero/storage/NZNWNC2R/10447318.2019.html:text/html}
}

@article{yang_tapsix_2020,
	title = {{TapSix}: {A} {Palm}-{Worn} {Glove} with a {Low}-{Cost} {Camera} {Sensor} that {Turns} a {Tactile} {Surface} into a {Six}-{Key} {Chorded} {Keyboard} by {Detection} {Finger} {Taps}},
	volume = {36},
	issn = {1044-7318},
	shorttitle = {{TapSix}},
	url = {https://doi.org/10.1080/10447318.2019.1597573},
	doi = {10.1080/10447318.2019.1597573},
	abstract = {TapSix is a one-handed wearable keyboard that enables typing in situations where using a keyboard is not possible. It detects finger taps on six virtual keys on a tactile surface while users can type without paying visual attention to their fingers. Its unique palm-worn design provides the stable view of all five fingers for the low-cost camera sensor, even when there is unusual motion in the upper limb. The captured image is processed using the proposed algorithm that both robustly detects finger taps on keys using only geometric features and measures the distance between each finger and the tactile surface. A new letter-to-tap mapping that fully utilizes the six keys in light of learnability, anatomical comfort, and algorithm accuracy is also proposed. We demonstrate the utility of TapSix in a virtual reality environment and evaluate the algorithm’s accuracy, typing performance, and user acceptance by comparing it with three commercial virtual reality interfaces.},
	number = {1},
	urldate = {2020-10-09},
	journal = {International Journal of Human–Computer Interaction},
	author = {Yang, Dongseok and Lee, Kanghee and Choi, Younggeun},
	month = jan,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2019.1597573},
	pages = {1--14},
	file = {Snapshot:/Users/m3sibti/Zotero/storage/JNRBVB4H/10447318.2019.html:text/html}
}

@article{caggianese_freehand-steering_2020,
	title = {Freehand-{Steering} {Locomotion} {Techniques} for {Immersive} {Virtual} {Environments}: {A} {Comparative} {Evaluation}},
	volume = {36},
	issn = {1044-7318},
	shorttitle = {Freehand-{Steering} {Locomotion} {Techniques} for {Immersive} {Virtual} {Environments}},
	url = {https://doi.org/10.1080/10447318.2020.1785151},
	doi = {10.1080/10447318.2020.1785151},
	abstract = {Virtual reality has achieved significant popularity in recent years, and allowing users to move freely within an immersive virtual world has become an important factor critical to realize. The user’s interactions are generally designed to increase the perceived realism, but the locomotion techniques and how these affect the user’s task performance still represent an open issue, much discussed in the literature. In this article, we evaluate the efficiency and effectiveness of, and user preferences relating to, freehand locomotion techniques designed for an immersive virtual environment performed through hand gestures tracked by a sensor placed in the egocentric position and experienced through a head-mounted display. Three freehand locomotion techniques have been implemented and compared with each other, and with a baseline technique based on a controller, through qualitative and quantitative measures. An extensive user study conducted with 60 subjects shows that the proposed methods have a performance comparable to the use of the controller, further revealing the users’ preference for decoupling the locomotion in sub-tasks, even if this means renouncing precision and adapting the interaction to the possibilities of the tracker sensor.},
	number = {18},
	urldate = {2020-10-09},
	journal = {International Journal of Human–Computer Interaction},
	author = {Caggianese, Giuseppe and Capece, Nicola and Erra, Ugo and Gallo, Luigi and Rinaldi, Michele},
	month = nov,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2020.1785151},
	pages = {1734--1755},
	file = {Snapshot:/Users/m3sibti/Zotero/storage/5I5CWGCG/10447318.2020.html:text/html}
}

@article{biswas_designing_2012,
	title = {Designing {Inclusive} {Interfaces} {Through} {User} {Modeling} and {Simulation}},
	volume = {28},
	issn = {1044-7318},
	url = {https://doi.org/10.1080/10447318.2011.565718},
	doi = {10.1080/10447318.2011.565718},
	abstract = {Elderly and disabled people can be hugely benefited through the advancement of modern electronic devices, as those can help them to engage more fully with the world. However, existing design practices often isolate elderly or disabled users by considering them as users with special needs. This article presents a simulator that can reflect problems faced by elderly and disabled users while they use computer, television, and similar electronic devices. The simulator embodies both the internal state of an application and the perceptual, cognitive, and motor processes of its user. It can help interface designers to understand, visualize, and measure the effect of impairment on interaction with an interface. Initially a brief survey of different user modeling techniques is presented, and then the existing models are classified into different categories. In the context of existing modeling approaches the work on user modeling is presented for people with a wide range of abilities. A few applications of the simulator, which shows the predictions are accurate enough to make design choices and point out the implication and limitations of the work, are also discussed.},
	number = {1},
	urldate = {2020-10-09},
	journal = {International Journal of Human–Computer Interaction},
	author = {Biswas, Pradipta and Robinson, Peter and Langdon, Patrick},
	month = jan,
	year = {2012},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2011.565718},
	pages = {1--33},
	file = {Snapshot:/Users/m3sibti/Zotero/storage/VXJNZ458/10447318.2011.html:text/html}
}

@article{porta_vision-based_2002,
	title = {Vision-based user interfaces: methods and applications},
	volume = {57},
	issn = {1071-5819},
	shorttitle = {Vision-based user interfaces},
	url = {http://www.sciencedirect.com/science/article/pii/S1071581902910128},
	doi = {10.1006/ijhc.2002.1012},
	abstract = {Within the class of perceptive user interfaces (i.e. interfaces providing the computer with perceptive capabilities), artificial vision is being exploited more and more as a new input modality, in addition to or in replacement of standard interaction paradigms. The aim of this paper is to provide a global view on the field of vision-based interfaces (VBIs), through the analysis of the methods used for their implementation and the exploration of the practical systems in which they have been employed. The focus will mostly be on techniques and prototypes intended for office and home PC-based use, as we are mainly interested in vision technology applied to ordinary computing environments. After a brief introduction to basic concepts about interfaces and image processing, the attention will be shifted to the four main areas in which VBIs find their maximum expression, namely head tracking, face/facial expression recognition, eye tracking and gesture recognition.},
	language = {en},
	number = {1},
	urldate = {2020-10-09},
	journal = {International Journal of Human-Computer Studies},
	author = {Porta, MARCO},
	month = jul,
	year = {2002},
	keywords = {eye tracking, face/facial expression recognition, gesture recognition, head tracking, perceptive interfaces, vision-based interfaces},
	pages = {27--73},
	file = {ScienceDirect Snapshot:/Users/m3sibti/Zotero/storage/DP4994ES/S1071581902910128.html:text/html}
}
@article{zhang_two-stage_2018,
	title = {Two-stage sketch colorization},
	volume = {37},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3272127.3275090},
	doi = {10.1145/3272127.3275090},
	abstract = {Sketch or line art colorization is a research field with significant market demand. Different from photo colorization which strongly relies on texture information, sketch colorization is more challenging as sketches may not have texture. Even worse, color, texture, and gradient have to be generated from the abstract sketch lines. In this paper, we propose a semi-automatic learning-based framework to colorize sketches with proper color, texture as well as gradient. Our framework consists of two stages. In the first drafting stage, our model guesses color regions and splashes a rich variety of colors over the sketch to obtain a color draft. In the second refinement stage, it detects the unnatural colors and artifacts, and try to fix and refine the result. Comparing to existing approaches, this two-stage design effectively divides the complex colorization task into two simpler and goal-clearer subtasks. This eases the learning and raises the quality of colorization. Our model resolves the artifacts such as water-color blurring, color distortion, and dull textures. We build an interactive software based on our model for evaluation. Users can iteratively edit and refine the colorization. We evaluate our learning model and the interactive system through an extensive user study. Statistics shows that our method outperforms the state-of-art techniques and industrial applications in several aspects including, the visual quality, the ability of user control, user experience, and other metrics.},
	number = {6},
	urldate = {2020-10-09},
	journal = {ACM Transactions on Graphics},
	author = {Zhang, Lvmin and Li, Chengze and Wong, Tien-Tsin and Ji, Yi and Liu, Chunping},
	month = dec,
	year = {2018},
	keywords = {colorization, line arts, sketch},
	pages = {261:1--261:14}
}

@article{sun_smartpaint_2019,
	title = {{SmartPaint}: a co-creative drawing system based on generative adversarial networks},
	volume = {20},
	issn = {2095-9230},
	shorttitle = {{SmartPaint}},
	url = {https://doi.org/10.1631/FITEE.1900386},
	doi = {10.1631/FITEE.1900386},
	abstract = {Artificial intelligence (AI) has played a significant role in imitating and producing large-scale designs such as e-commerce banners. However, it is less successful at creative and collaborative design outputs. Most humans express their ideas as rough sketches, and lack the professional skills to complete pleasing paintings. Existing AI approaches have failed to convert varied user sketches into artistically beautiful paintings while preserving their semantic concepts. To bridge this gap, we have developed SmartPaint, a co-creative drawing system based on generative adversarial networks (GANs), enabling a machine and a human being to collaborate in cartoon landscape painting. SmartPaint trains a GAN using triples of cartoon images, their corresponding semantic label maps, and edge detection maps. The machine can then simultaneously understand the cartoon style and semantics, along with the spatial relationships among the objects in the landscape images. The trained system receives a sketch as a semantic label map input, and automatically synthesizes its edge map for stable handling of varied sketches. It then outputs a creative and fine painting with the appropriate style corresponding to the human’s sketch. Experiments confirmed that the proposed SmartPaint system successfully generates high-quality cartoon paintings.},
	language = {en},
	number = {12},
	urldate = {2020-10-09},
	journal = {Frontiers of Information Technology \& Electronic Engineering},
	author = {Sun, Lingyun and Chen, Pei and Xiang, Wei and Chen, Peng and Gao, Wei-yue and Zhang, Ke-jun},
	month = dec,
	year = {2019},
	pages = {1644--1656},
	file = {Springer Full Text PDF:/Users/m3sibti/Zotero/storage/3U7VCA7Q/Sun et al. - 2019 - SmartPaint a co-creative drawing system based on .pdf:application/pdf}
}

@article{zhang_anticipating_2019,
	title = {Anticipating {Where} {People} will {Look} {Using} {Adversarial} {Networks}},
	volume = {41},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2018.2871688},
	abstract = {We introduce a new problem of gaze anticipation on future frames which extends the conventional gaze prediction problem to go beyond current frames. To solve this problem, we propose a new generative adversarial network based model, Deep Future Gaze (DFG), encompassing two pathways: DFG-P is to anticipate gaze prior maps conditioned on the input frame which provides task influences; DFG-G is to learn to model both semantic and motion information in future frame generation. DFG-P and DFG-G are then fused to anticipate future gazes. DFG-G consists of two networks: a generator and a discriminator. The generator uses a two-stream spatial-temporal convolution architecture (3D-CNN) for explicitly untangling the foreground and background to generate future frames. It then attaches another 3D-CNN for gaze anticipation based on these synthetic frames. The discriminator plays against the generator by distinguishing the synthetic frames of the generator from the real frames. Experimental results on the publicly available egocentric and third person video datasets show that DFG significantly outperforms all competitive baselines. We also demonstrate that DFG achieves better performance of gaze prediction on current frames in egocentric and third person videos than state-of-the-art methods.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, Mengmi and Ma, Keng Teck and Lim, Joo Hwee and Zhao, Qi and Feng, Jiashi},
	month = aug,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {3D-CNN, adversarial networks, convolutional neural nets, deep future gaze, DFG-G, DFG-P, egocentric datasets, Egocentric videos, feature extraction, frame generation, gaze anticipation, gaze prediction problem, gaze tracking, generative adversarial network, generative adversarial network based model, Generative adversarial networks, Generators, input frame, learning (artificial intelligence), Predictive models, saliency, semantic motion information, spatial-temporal convolution architecture, Streaming media, synthetic frames, Task analysis, third person video datasets, Training, video signal processing, visual attention, Visualization},
	pages = {1783--1796},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/KRYM4G6Q/8471119.html:text/html}
}

@article{lee_smartphone_2019,
	title = {Smartphone help contents re-organization considering user specification via conditional {GAN}},
	volume = {129},
	issn = {1071-5819},
	url = {http://www.sciencedirect.com/science/article/pii/S1071581918303677},
	doi = {10.1016/j.ijhcs.2019.04.002},
	abstract = {There are various help systems embedded in smartphones that are intended to provide assistance to users. These systems should be conveniently accessible in locations where users may need assistance. Moreover, when help contents are provided without regard to the users interest, it makes it difficult for the user to find relevant content. Thus, the present study provides a new method of re-organizing help content by considering each users interests and preferences using their app usage sequence. Based on the user specification derived from the app usage sequence, help contents usage prediction is generated for each of them with conditional generative adversarial network (GAN) architecture in a new way. Further, another method to pre-process data in applying conditional GAN, originally devised to generate image data, is proposed in our problem. The experiment result showed a higher absolute performance level of help contents usage prediction and better performance of effectiveness in re-organization of top-k contents compared to the existing benchmark method. Thus, the proposed method reflects the users interest and provides appropriate help contents for each user effectively.},
	language = {en},
	urldate = {2020-10-09},
	journal = {International Journal of Human-Computer Studies},
	author = {Lee, Younghoon and Cho, Sungzoon and Choi, Jinhae},
	month = sep,
	year = {2019},
	keywords = {App usage sequence, Conditional GAN, Contents re-organization, Help systems, Seq2seq, User specification},
	pages = {108--115},
	file = {ScienceDirect Snapshot:/Users/m3sibti/Zotero/storage/E4DVKW5W/S1071581918303677.html:text/html}
}
@inproceedings{tufano_empirical_2018,
	title = {An {Empirical} {Investigation} into {Learning} {Bug}-{Fixing} {Patches} in the {Wild} via {Neural} {Machine} {Translation}},
	doi = {10.1145/3238147.3240732},
	abstract = {Millions of open-source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. We mine millions of bug-fixes from the change histories of GitHub repositories to extract meaningful examples of such bug-fixes. Then, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. Our model is able to fix hundreds of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9\% of the cases.},
	booktitle = {2018 33rd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys},
	month = sep,
	year = {2018},
	note = {ISSN: 2643-1572},
	keywords = {bug-fixes, bug-fixing patches, buggy code, code repositories, fixed patches, fixed version, language translation, learning (artificial intelligence), neural machine translation, neural machine translation techniques, open-source projects, program debugging, programming bugs, public domain software, software development histories},
	pages = {832--837},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/SAMB9SIW/9000077.html:text/html}
}

@article{lekschas_peax_2020,
	title = {Peax: {Interactive} {Visual} {Pattern} {Search} in {Sequential} {Data} {Using} {Unsupervised} {Deep} {Representation} {Learning}},
	volume = {39},
	copyright = {© 2020 The Author(s) Computer Graphics Forum © 2020 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {Peax},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13971},
	doi = {10.1111/cgf.13971},
	abstract = {We present Peax, a novel feature-based technique for interactive visual pattern search in sequential data, like time series or data mapped to a genome sequence. Visually searching for patterns by similarity is often challenging because of the large search space, the visual complexity of patterns, and the user's perception of similarity. For example, in genomics, researchers try to link patterns in multivariate sequential data to cellular or pathogenic processes, but a lack of ground truth and high variance makes automatic pattern detection unreliable. We have developed a convolutional autoencoder for unsupervised representation learning of regions in sequential data that can capture more visual details of complex patterns compared to existing similarity measures. Using this learned representation as features of the sequential data, our accompanying visual query system enables interactive feedback-driven adjustments of the pattern search to adapt to the users’ perceived similarity. Using an active learning sampling strategy, Peax collects user-generated binary relevance feedback. This feedback is used to train a model for binary classification, to ultimately find other regions that exhibit patterns similar to the search target. We demonstrate Peax's features through a case study in genomics and report on a user study with eight domain experts to assess the usability and usefulness of Peax. Moreover, we evaluate the effectiveness of the learned feature representation for visual similarity search in two additional user studies. We find that our models retrieve significantly more similar patterns than other commonly used techniques.},
	language = {en},
	number = {3},
	urldate = {2020-10-09},
	journal = {Computer Graphics Forum},
	author = {Lekschas, Fritz and Peterson, Brant and Haehn, Daniel and Ma, Eric and Gehlenborg, Nils and Pfister, Hanspeter},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13971},
	pages = {167--179},
	file = {Snapshot:/Users/m3sibti/Zotero/storage/9MY2N224/cgf.html:text/html;Submitted Version:/Users/m3sibti/Zotero/storage/P46XLV42/Lekschas et al. - 2020 - Peax Interactive Visual Pattern Search in Sequent.pdf:application/pdf}
}
@article{fowkes_autofolding_2017,
	title = {Autofolding for {Source} {Code} {Summarization}},
	volume = {43},
	issn = {1939-3520},
	doi = {10.1109/TSE.2017.2664836},
	abstract = {Developers spend much of their time reading and browsing source code, raising new opportunities for summarization methods. Indeed, modern code editors provide code folding, which allows one to selectively hide blocks of code. However this is impractical to use as folding decisions must be made manually or based on simple rules. We introduce the autofolding problem, which is to automatically create a code summary by folding less informative code regions. We present a novel solution by formulating the problem as a sequence of AST folding decisions, leveraging a scoped topic model for code tokens. On an annotated set of popular open source projects, we show that our summarizer outperforms simpler baselines, yielding a 28 percent error reduction. Furthermore, we find through a case study that our summarizer is strongly preferred by experienced developers. More broadly, we hope this work will aid program comprehension by turning code folding into a usable and valuable tool.},
	number = {12},
	journal = {IEEE Transactions on Software Engineering},
	author = {Fowkes, Jaroslav and Chanthirasegaran, Pankajan and Ranca, Razvan and Allamanis, Miltiadis and Lapata, Mirella and Sutton, Charles},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {AST folding decisions, autofolding problem, code folding, code tokens, Complexity theory, Feature extraction, informative code regions, modern code editors, Natural languages, open source projects, public domain software, Software development, source code (software), source code summarization, Source code summarization, program comprehension, topic modelling, Source coding},
	pages = {1095--1109},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/QQXSCUVD/7843666.html:text/html;Submitted Version:/Users/m3sibti/Zotero/storage/H4NUAYIE/Fowkes et al. - 2017 - Autofolding for Source Code Summarization.pdf:application/pdf}
}
@article{wang_vision-language_2020,
	title = {Vision-{Language} {Navigation} {Policy} {Learning} and {Adaptation}},
	volume = {PP},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2020.2972281},
	abstract = {Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms baseline methods by 10\% on Success Rate weighted by Path Length (SPL) and achieves the state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore and adapt to unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7\% to 11.7\%).},
	language = {eng},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Wang, Xin and Huang, Qiuyuan and Celikyilmaz, Asli and Gao, Jianfeng and Shen, Dinghan and Wang, Yuan-Fang and Wang, William and Zhang, Lei},
	month = feb,
	year = {2020},
	pmid = {32054568}
}

@article{dou_webthetics_2019,
	title = {Webthetics: {Quantifying} webpage aesthetics with deep learning},
	volume = {124},
	issn = {1071-5819},
	shorttitle = {Webthetics},
	url = {http://www.sciencedirect.com/science/article/pii/S1071581918306682},
	doi = {10.1016/j.ijhcs.2018.11.006},
	abstract = {As web has become the most popular media to attract users and customers worldwide, webpage aesthetics plays an increasingly important role for engaging users online and impacting their user experience. We present a novel method using deep learning to automatically compute and quantify webpage aesthetics. Our deep neural network, named as Webthetics, which is trained from the collected user rating data, can extract representative features from raw webpages and quantify their aesthetics. To improve the model performance, we propose to transfer the knowledge from image style recognition task into our network. We have validated that our method significantly outperforms previous method using hand-crafted features such as colorfulness and complexity. These promising results indicate that our method can serve as an effective and efficient means for providing objective aesthetics evaluation during the design process.},
	language = {en},
	urldate = {2020-10-09},
	journal = {International Journal of Human-Computer Studies},
	author = {Dou, Qi and Zheng, Xianjun Sam and Sun, Tongfang and Heng, Pheng-Ann},
	month = apr,
	year = {2019},
	keywords = {Deep learning, User experience, Web visual design, Webpage aesthetics},
	pages = {56--66},
	file = {ScienceDirect Snapshot:/Users/m3sibti/Zotero/storage/R5977ECD/S1071581918306682.html:text/html}
}

@article{halter_vian_2019,
	title = {{VIAN}: {A} {Visual} {Annotation} {Tool} for {Film} {Analysis}},
	volume = {38},
	copyright = {© 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	shorttitle = {{VIAN}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13676},
	doi = {10.1111/cgf.13676},
	abstract = {While color plays a fundamental role in film design and production, existing solutions for film analysis in the digital humanities address perceptual and spatial color information only tangentially. We introduce VIAN, a visual film annotation system centered on the semantic aspects of film color analysis. The tool enables expert-assessed labeling, curation, visualization and Classification of color features based on their perceived context and aesthetic quality. It is the first of its kind that incorporates foreground-background information made possible by modern deep learning segmentation methods. The proposed tool seamlessly integrates a multimedia data management system, so that films can undergo a full color-oriented analysis pipeline.},
	language = {en},
	number = {3},
	urldate = {2020-10-09},
	journal = {Computer Graphics Forum},
	author = {Halter, Gaudenz and Ballester‐Ripoll, Rafael and Flueckiger, Barbara and Pajarola, Renato},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13676},
	keywords = {• Human-centered computing → Visualization systems and tools, •Applied computing → Media arts, CCS Concepts},
	pages = {119--129},
	file = {Snapshot:/Users/m3sibti/Zotero/storage/HHK4USI7/cgf.html:text/html}
}

@article{bao_psc2code_2020,
	title = {psc2code: {Denoising} {Code} {Extraction} from {Programming} {Screencasts}},
	volume = {29},
	issn = {1049-331X},
	shorttitle = {psc2code},
	url = {https://doi.org/10.1145/3392093},
	doi = {10.1145/3392093},
	abstract = {Programming screencasts have become a pervasive resource on the Internet, which help developers learn new programming technologies or skills. The source code in programming screencasts is an important and valuable information for developers. But the streaming nature of programming screencasts (i.e., a sequence of screen-captured images) limits the ways that developers can interact with the source code in the screencasts. Many studies use the Optical Character Recognition (OCR) technique to convert screen images (also referred to as video frames) into textual content, which can then be indexed and searched easily. However, noisy screen images significantly affect the quality of source code extracted by OCR, for example, no-code frames (e.g., PowerPoint slides, web pages of API specification), non-code regions (e.g., Package Explorer view, Console view), and noisy code regions with code in completion suggestion popups. Furthermore, due to the code characteristics (e.g., long compound identifiers like ItemListener), even professional OCR tools cannot extract source code without errors from screen images. The noisy OCRed source code will negatively affect the downstream applications, such as the effective search and navigation of the source code content in programming screencasts. In this article, we propose an approach named psc2code to denoise the process of extracting source code from programming screencasts. First, psc2code leverages the Convolutional Neural Network (CNN) based image classification to remove non-code and noisy-code frames. Then, psc2code performs edge detection and clustering-based image segmentation to detect sub-windows in a code frame, and based on the detected sub-windows, it identifies and crops the screen region that is most likely to be a code editor. Finally, psc2code calls the API of a professional OCR tool to extract source code from the cropped code regions and leverages the OCRed cross-frame information in the programming screencast and the statistical language model of a large corpus of source code to correct errors in the OCRed source code. We conduct an experiment on 1,142 programming screencasts from YouTube. We find that our CNN-based image classification technique can effectively remove the non-code and noisy-code frames, which achieves an F1-score of 0.95 on the valid code frames. We also find that psc2code can significantly improve the quality of the OCRed source code by truly correcting about half of incorrectly OCRed words. Based on the source code denoised by psc2code, we implement two applications: (1) a programming screencast search engine; (2) an interaction-enhanced programming screencast watching tool. Based on the source code extracted from the 1,142 collected programming screencasts, our experiments show that our programming screencast search engine achieves the precision@5, 10, and 20 of 0.93, 0.81, and 0.63, respectively. We also conduct a user study of our interaction-enhanced programming screencast watching tool with 10 participants. This user study shows that our interaction-enhanced watching tool can help participants learn the knowledge in the programming video more efficiently and effectively.},
	number = {3},
	urldate = {2020-10-09},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Bao, Lingfeng and Xing, Zhenchang and Xia, Xin and Lo, David and Wu, Minghui and Yang, Xiaohu},
	month = jun,
	year = {2020},
	keywords = {code search, deep learning, Programming videos},
	pages = {21:1--21:38}
}

@article{han_deepsketch2face_2017,
	title = {{DeepSketch2Face}: a deep learning based sketching system for {3D} face and caricature modeling},
	volume = {36},
	issn = {0730-0301},
	shorttitle = {{DeepSketch2Face}},
	url = {https://doi.org/10.1145/3072959.3073629},
	doi = {10.1145/3072959.3073629},
	abstract = {Face modeling has been paid much attention in the field of visual computing. There exist many scenarios, including cartoon characters, avatars for social media, 3D face caricatures as well as face-related art and design, where low-cost interactive face modeling is a popular approach especially among amateur users. In this paper, we propose a deep learning based sketching system for 3D face and caricature modeling. This system has a labor-efficient sketching interface, that allows the user to draw freehand imprecise yet expressive 2D lines representing the contours of facial features. A novel CNN based deep regression network is designed for inferring 3D face models from 2D sketches. Our network fuses both CNN and shape based features of the input sketch, and has two independent branches of fully connected layers generating independent subsets of coefficients for a bilinear face representation. Our system also supports gesture based interactions for users to further manipulate initial face models. Both user studies and numerical results indicate that our sketching system can help users create face models quickly and effectively. A significantly expanded face database with diverse identities, expressions and levels of exaggeration is constructed to promote further research and evaluation of face modeling techniques.},
	number = {4},
	urldate = {2020-10-09},
	journal = {ACM Transactions on Graphics},
	author = {Han, Xiaoguang and Gao, Chang and Yu, Yizhou},
	month = jul,
	year = {2017},
	keywords = {deep learning, face caricatures, face database, face modeling, gestures, sketch-based modeling},
	pages = {126:1--126:12},
	file = {Full Text PDF:/Users/m3sibti/Zotero/storage/DE6QMX7A/Han et al. - 2017 - DeepSketch2Face a deep learning based sketching s.pdf:application/pdf}
}

@article{bell_learning_2015,
	title = {Learning visual similarity for product design with convolutional neural networks},
	volume = {34},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2766959},
	doi = {10.1145/2766959},
	abstract = {Popular sites like Houzz, Pinterest, and LikeThatDecor, have communities of users helping each other answer questions about products in images. In this paper we learn an embedding for visual search in interior design. Our embedding contains two different domains of product images: products cropped from internet scenes, and products in their iconic form. With such a multi-domain embedding, we demonstrate several applications of visual search including identifying products in scenes and finding stylistically similar products. To obtain the embedding, we train a convolutional neural network on pairs of images. We explore several training architectures including re-purposing object classifiers, using siamese networks, and using multitask learning. We evaluate our search quantitatively and qualitatively and demonstrate high quality results for search across multiple visual domains, enabling new applications in interior design.},
	number = {4},
	urldate = {2020-10-09},
	journal = {ACM Transactions on Graphics},
	author = {Bell, Sean and Bala, Kavita},
	month = jul,
	year = {2015},
	keywords = {deep learning, interior design, search, visual similarity},
	pages = {98:1--98:10}
}

@article{nishida_interactive_2016,
	title = {Interactive sketching of urban procedural models},
	volume = {35},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2897824.2925951},
	doi = {10.1145/2897824.2925951},
	abstract = {3D modeling remains a notoriously difficult task for novices despite significant research effort to provide intuitive and automated systems. We tackle this problem by combining the strengths of two popular domains: sketch-based modeling and procedural modeling. On the one hand, sketch-based modeling exploits our ability to draw but requires detailed, unambiguous drawings to achieve complex models. On the other hand, procedural modeling automates the creation of precise and detailed geometry but requires the tedious definition and parameterization of procedural models. Our system uses a collection of simple procedural grammars, called snippets, as building blocks to turn sketches into realistic 3D models. We use a machine learning approach to solve the inverse problem of finding the procedural model that best explains a user sketch. We use non-photorealistic rendering to generate artificial data for training convolutional neural networks capable of quickly recognizing the procedural rule intended by a sketch and estimating its parameters. We integrate our algorithm in a coarse-to-fine urban modeling system that allows users to create rich buildings by successively sketching the building mass, roof, facades, windows, and ornaments. A user study shows that by using our approach non-expert users can generate complex buildings in just a few minutes.},
	number = {4},
	urldate = {2020-10-09},
	journal = {ACM Transactions on Graphics},
	author = {Nishida, Gen and Garcia-Dorado, Ignacio and Aliaga, Daniel G. and Benes, Bedrich and Bousseau, Adrien},
	month = jul,
	year = {2016},
	keywords = {inverse procedural modeling, machine learning, sketching},
	pages = {130:1--130:11}
}

@article{shao_interactive_2012,
	title = {An interactive approach to semantic modeling of indoor scenes with an {RGBD} camera},
	volume = {31},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2366145.2366155},
	doi = {10.1145/2366145.2366155},
	abstract = {We present an interactive approach to semantic modeling of indoor scenes with a consumer-level RGBD camera. Using our approach, the user first takes an RGBD image of an indoor scene, which is automatically segmented into a set of regions with semantic labels. If the segmentation is not satisfactory, the user can draw some strokes to guide the algorithm to achieve better results. After the segmentation is finished, the depth data of each semantic region is used to retrieve a matching 3D model from a database. Each model is then transformed according to the image depth to yield the scene. For large scenes where a single image can only cover one part of the scene, the user can take multiple images to construct other parts of the scene. The 3D models built for all images are then transformed and unified into a complete scene. We demonstrate the efficiency and robustness of our approach by modeling several real-world scenes.},
	number = {6},
	urldate = {2020-10-09},
	journal = {ACM Transactions on Graphics},
	author = {Shao, Tianjia and Xu, Weiwei and Zhou, Kun and Wang, Jingdong and Li, Dongping and Guo, Baining},
	month = nov,
	year = {2012},
	keywords = {depth images, indoor scene, labeling, random regress forest, segmentation},
	pages = {136:1--136:11}
}
@article{villegas_neural_1994,
	title = {A neural network tool for identifying text-editing goals},
	volume = {40},
	issn = {1071-5819},
	url = {http://www.sciencedirect.com/science/article/pii/S1071581984710391},
	doi = {10.1006/ijhc.1994.1039},
	abstract = {When performing a computer task, a user will decompose the task into cognitive goals and subgoals. These goals are accomplished through the use of external operators (e.g. keystrokes, mouse button presses) or internal mental operators (e.g. reading parts of the display, deciding on the goal). Users may utilize different goals and sequence the goals differently to accomplish the same overall task. Determining the goals and the sequencing of the goals could be useful for several reasons, such as providing a means for on-line assistance with the task. Determining these goals in the past, however, has been a time-consuming process. A neural network tool for automatically identifying cognitive text-editing goals from operators is investigated. The first of three memos edited by subjects was used to train the neural network successfully to map the operators (keystrokes) to cognitive goals. In a test of the trained network's ability to generalize to new input—the second and third memos edited by the subjects—the net could identify the cognitive goals with an overall performance accuracy of 96\%. Two methods were used to investigate the validity of the goals which were identified by the tool. The characteristics of the goals were consistent with that which could be expected based upon previous research. This research illustrates that a neural network tool can identify the cognitive goals of a task.},
	language = {en},
	number = {5},
	urldate = {2020-10-09},
	journal = {International Journal of Human-Computer Studies},
	author = {Villegas, Leticia and Eberts, Ray E.},
	month = may,
	year = {1994},
	pages = {813--833},
	file = {ScienceDirect Snapshot:/Users/m3sibti/Zotero/storage/3NKEFRBL/S1071581984710391.html:text/html}
}

@article{kapoor_automatic_2007,
	title = {Automatic prediction of frustration},
	volume = {65},
	issn = {1071-5819},
	url = {http://www.sciencedirect.com/science/article/pii/S1071581907000377},
	doi = {10.1016/j.ijhcs.2007.02.003},
	abstract = {Predicting when a person might be frustrated can provide an intelligent system with important information about when to initiate interaction. For example, an automated Learning Companion or Intelligent Tutoring System might use this information to intervene, providing support to the learner who is likely to otherwise quit, while leaving engaged learners free to discover things without interruption. This paper presents the first automated method that assesses, using multiple channels of affect-related information, whether a learner is about to click on a button saying “I’m frustrated.” The new method was tested on data gathered from 24 participants using an automated Learning Companion. Their indication of frustration was automatically predicted from the collected data with 79\% accuracy (chance=58\%). The new assessment method is based on Gaussian process classification and Bayesian inference. Its performance suggests that non-verbal channels carrying affective cues can help provide important information to a system for formulating a more intelligent response.},
	language = {en},
	number = {8},
	urldate = {2020-10-09},
	journal = {International Journal of Human-Computer Studies},
	author = {Kapoor, Ashish and Burleson, Winslow and Picard, Rosalind W.},
	month = aug,
	year = {2007},
	keywords = {Affect recognition, Affective Learning Companion, Intelligent Tutoring System, Learner state assessment},
	pages = {724--736},
	file = {ScienceDirect Snapshot:/Users/m3sibti/Zotero/storage/F84NEEKY/S1071581907000377.html:text/html}
}

@article{vizer_automated_2009,
	title = {Automated stress detection using keystroke and linguistic features: {An} exploratory study},
	volume = {67},
	issn = {1071-5819},
	shorttitle = {Automated stress detection using keystroke and linguistic features},
	url = {http://www.sciencedirect.com/science/article/pii/S1071581909000937},
	doi = {10.1016/j.ijhcs.2009.07.005},
	abstract = {Monitoring of cognitive and physical function is central to the care of people with or at risk for various health conditions, but existing solutions rely on intrusive methods that are inadequate for continuous tracking. Less intrusive techniques that facilitate more accurate and frequent monitoring of the status of cognitive or physical function become increasingly desirable as the population ages and lifespan increases. Since the number of seniors using computers continues to grow dramatically, a method that exploits normal daily computer interactions is attractive. This research explores the possibility of detecting cognitive and physical stress by monitoring keyboard interactions with the eventual goal of detecting acute or gradual changes in cognitive and physical function. Researchers have already attributed a certain amount of variability and “drift” in an individual's typing pattern to situational factors as well as stress, but this phenomenon has not been explored adequately. In an attempt to detect changes in typing associated with stress, this research analyzes keystroke and linguistic features of spontaneously generated text. Results show that it is possible to classify cognitive and physical stress conditions relative to non-stress conditions based on keystroke and linguistic features with accuracy rates comparable to those currently obtained using affective computing methods. The proposed approach is attractive because it requires no additional hardware, is unobtrusive, is adaptable to individual users, and is of very low cost. This research demonstrates the potential of exploiting continuous monitoring of keyboard interactions to support the early detection of changes in cognitive and physical function.},
	language = {en},
	number = {10},
	urldate = {2020-10-09},
	journal = {International Journal of Human-Computer Studies},
	author = {Vizer, Lisa M. and Zhou, Lina and Sears, Andrew},
	month = oct,
	year = {2009},
	keywords = {Stress detection, Text feature analysis, Unobtrusive monitoring},
	pages = {870--886},
	file = {ScienceDirect Snapshot:/Users/m3sibti/Zotero/storage/V5A35NIG/S1071581909000937.html:text/html}
}

@article{van_tonder_improving_2012,
	title = {Improving the controllability of tilt interaction for mobile map-based applications},
	volume = {70},
	issn = {1071-5819},
	url = {https://doi.org/10.1016/j.ijhcs.2012.08.001},
	doi = {10.1016/j.ijhcs.2012.08.001},
	abstract = {This article proposes enhanced techniques for improving the controllability of tilt interaction. Tilt interaction offers an intuitive, one-handed form of interaction for mobile applications. Tilt interaction is particularly well-suited to mobile map-based applications, where traditional mobile interaction techniques suffer from several shortcomings. Current implementations of tilt interaction, however, suffer from several shortcomings. Existing zooming techniques used in conjunction with tilt interaction are often difficult to control. Tilt-controlled panning can also be difficult to control in a mobile context of use, where the user's walking motion affects the accuracy of tilt interaction. The use of rate-controlled tilt interaction to perform zooming is proposed. Two different approaches to sensitivity adaptation are investigated, where the sensitivity of tilt-controlled panning is automatically adjusted to compensate for the user's current context of use. A usability study was conducted with thirty participants to evaluate the proposed techniques. The results of the usability study showed that tilt zooming offered better efficiency, user satisfaction and perceived workload than gesture zooming. The use of a static dampening factor to compensate for walking motion was found to provide actual and perceived controllability improvements.},
	number = {12},
	urldate = {2020-10-09},
	journal = {International Journal of Human-Computer Studies},
	author = {Van Tonder, Bradley Paul and Wesson, Janet Louise},
	month = dec,
	year = {2012},
	keywords = {Adaptation, Controllability, Maps, Tilt interaction},
	pages = {920--935}
}

@article{lian_easyfont_2018,
	title = {{EasyFont}: {A} {Style} {Learning}-{Based} {System} to {Easily} {Build} {Your} {Large}-{Scale} {Handwriting} {Fonts}},
	volume = {38},
	issn = {0730-0301},
	shorttitle = {{EasyFont}},
	url = {https://doi.org/10.1145/3213767},
	doi = {10.1145/3213767},
	abstract = {Generating personal handwriting fonts with large amounts of characters is a boring and time-consuming task. For example, the official standard GB18030-2000 for commercial font products consists of 27,533 Chinese characters. Consistently and correctly writing out such huge amounts of characters is usually an impossible mission for ordinary people. To solve this problem, we propose a system, EasyFont, to automatically synthesize personal handwriting for all (e.g., Chinese) characters in the font library by learning style from a small number (as few as 1\%) of carefully-selected samples written by an ordinary person. Major technical contributions of our system are twofold. First, we design an effective stroke extraction algorithm that constructs best-suited reference data from a trained font skeleton manifold and then establishes correspondence between target and reference characters via a non-rigid point set registration approach. Second, we develop a set of novel techniques to learn and recover users’ overall handwriting styles and detailed handwriting behaviors. Experiments including Turing tests with 97 participants demonstrate that the proposed system generates high-quality synthesis results, which are indistinguishable from original handwritings. Using our system, for the first time, the practical handwriting font library in a user’s personal style with arbitrarily large numbers of Chinese characters can be generated automatically. It can also be observed from our experiments that recently-popularized deep learning based end-to-end methods are not able to properly handle this task, which implies the necessity of expert knowledge and handcrafted rules for many applications.},
	number = {1},
	urldate = {2020-10-09},
	journal = {ACM Transactions on Graphics},
	author = {Lian, Zhouhui and Zhao, Bo and Chen, Xudong and Xiao, Jianguo},
	month = dec,
	year = {2018},
	keywords = {Chinese, fonts, Handwriting, style learning},
	pages = {6:1--6:18}
}

@article{li_applications_2017,
	title = {Applications of artificial intelligence in intelligent manufacturing: a review},
	volume = {18},
	issn = {2095-9230},
	shorttitle = {Applications of artificial intelligence in intelligent manufacturing},
	url = {https://doi.org/10.1631/FITEE.1601885},
	doi = {10.1631/FITEE.1601885},
	abstract = {Based on research into the applications of artificial intelligence (AI) technology in the manufacturing industry in recent years, we analyze the rapid development of core technologies in the new era of ‘Internet plus AI’, which is triggering a great change in the models, means, and ecosystems of the manufacturing industry, as well as in the development of AI. We then propose new models, means, and forms of intelligent manufacturing, intelligent manufacturing system architecture, and intelligent manufacturing technology system, based on the integration of AI technology with information communications, manufacturing, and related product technology. Moreover, from the perspectives of intelligent manufacturing application technology, industry, and application demonstration, the current development in intelligent manufacturing is discussed. Finally, suggestions for the application of AI in intelligent manufacturing in China are presented.},
	language = {en},
	number = {1},
	urldate = {2020-10-09},
	journal = {Frontiers of Information Technology \& Electronic Engineering},
	author = {Li, Bo-hu and Hou, Bao-cun and Yu, Wen-tao and Lu, Xiao-bing and Yang, Chun-wei},
	month = jan,
	year = {2017},
	pages = {86--96},
	file = {Springer Full Text PDF:/Users/m3sibti/Zotero/storage/EAI8VHTH/Li et al. - 2017 - Applications of artificial intelligence in intelli.pdf:application/pdf}
}

@article{liu_deep_2020,
	title = {Deep {Learning} {Based} {Program} {Generation} from {Requirements} {Text}: {Are} {We} {There} {Yet}?},
	issn = {1939-3520},
	shorttitle = {Deep {Learning} {Based} {Program} {Generation} from {Requirements} {Text}},
	doi = {10.1109/TSE.2020.3018481},
	abstract = {To release developers from time-consuming software development, many approaches have been proposed to generate source code automatically according to software requirements. With significant advances in deep learning and natural language processing, deep learning-based approaches are proposed to generate source code from natural language descriptions. The key insight is that given a large corpus of software requirements and their corresponding implementations, advanced deep learning techniques may learn how to translate software requirements into source code that fulfill such requirements. Although such approaches are reported to be highly accurate, they are evaluated on datasets that are rather small, lack of diversity, and significantly different from real-world software requirements. To this end, we build a large scale dataset that is composed of longer requirements as well as validated implementations. We evaluate the state-of-the-art approaches on this new dataset, and the results suggest that their performance on our dataset is significantly lower than that on existing datasets concerning the common metrics, i.e. BLEU. Evaluation results also suggest that the generated programs often contain syntactic and semantical errors, and none of them can pass even a single predefined test case. Further analysis reveals that the state-of-the-art approaches learn little from software requirements, and most of the successfully generated statements are popular statements in the training programs. Based on this finding, we propose a popularity-based approach that always generates the most popular statements in training programs regardless of the input (software requirements). Evaluation results suggest that none of the state-of-the-art approaches can outperform this simple statistics-based approach. As a conclusion, deep learning-based program generation requires significant improvement in the future, and our dataset may serve as a basis for future research in this direction.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Liu, Hui and Shen, Mingzhu and Zhu, Jiaqi and Niu, Nan and Li, Ge and Zhang, Lu},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Code Generation, Data Set, Deep learning, Deep Learning, DSL, Object oriented modeling, Software, Software Requirements, Syntactics, Tools, Unified modeling language},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/KWP5AE6U/9173704.html:text/html}
}

@article{ma_easy--deploy_2019,
	title = {Easy-to-{Deploy} {API} {Extraction} by {Multi}-{Level} {Feature} {Embedding} and {Transfer} {Learning}},
	issn = {1939-3520},
	doi = {10.1109/TSE.2019.2946830},
	abstract = {Application Programming Interfaces (APIs) have been widely discussed on social-technical platforms (e.g., Stack Overflow). Extracting API mentions from such informal software texts is the prerequisite for API-centric search and summarization of programming knowledge. Machine learning based API extraction has demonstrated superior performance than rule-based methods in informal software texts that lack consistent writing forms and annotations. However, machine learning based methods have a significant overhead in preparing training data and effective features. In this paper, we propose a multi-layer neural network based architecture for API extraction. Our architecture automatically learns character-, word- and sentence-level features from the input texts, thus removing the need for manual feature engineering and the dependence on advanced features (e.g., API gazzetter) beyond the input texts. We also propose to adopt transfer learning to adapt a source-library-trained model to a target-library, thus reducing the overhead of manual training-data labeling when the software text of multiple programming languages and libraries need to be processed. We conduct extensive experiments with six libraries of four programming languages which support diverse functionalities and have different API-naming and API-mention characteristics. Our experiments investigate the performance of our neural architecture for API extraction in informal software texts, the importance of different features, the effectiveness of transfer learning. Our results confirm not only the superior performance of our neural architecture than existing machine learning based methods for API extraction in informal software texts, but also the easy-to-deploy characteristic of our neural architecture.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Ma, Suyu and Xing, Zhenchang and Chen, Chunyang and Chen, Cheng and Qu, Lizhen and Li, Guoqiang},
	year = {2019},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {API extraction, CNN, Computer architecture, Feature extraction, Libraries, LSTM, Machine learning, Manuals, Software, Training data, Transfer learning, Word embedding},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/m3sibti/Zotero/storage/MX89D2XQ/8865646.html:text/html}
}

@inproceedings{chen_fromui_2018,
author = {Chen, Chunyang and Su, Ting and Meng, Guozhu and Xing, Zhenchang and Liu, Yang},
title = {From UI Design Image to GUI Skeleton: A Neural Machine Translator to Bootstrap Mobile GUI Implementation},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180240},
doi = {10.1145/3180155.3180240},
abstract = {A GUI skeleton is the starting point for implementing a UI design image. To obtain a GUI skeleton from a UI design image, developers have to visually understand UI elements and their spatial layout in the image, and then translate this understanding into proper GUI components and their compositions. Automating this visual understanding and translation would be beneficial for bootstraping mobile GUI implementation, but it is a challenging task due to the diversity of UI designs and the complexity of GUI skeletons to generate. Existing tools are rigid as they depend on heuristically-designed visual understanding and GUI generation rules. In this paper, we present a neural machine translator that combines recent advances in computer vision and machine translation for translating a UI design image into a GUI skeleton. Our translator learns to extract visual features in UI images, encode these features' spatial layouts, and generate GUI skeletons in a unified neural network framework, without requiring manual rule development. For training our translator, we develop an automated GUI exploration method to automatically collect large-scale UI data from real-world applications. We carry out extensive experiments to evaluate the accuracy, generality and usefulness of our approach.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {665–676},
numpages = {12},
keywords = {user interface, reverse engineering, deep learning},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{ferris_framework_2016,
	title = {A {Framework} for {Selecting} and {Optimizing} {Color} {Scheme} in {Web} {Design}},
	doi = {10.1109/HICSS.2016.73},
	abstract = {Based on the foundation of color theory, the symbolism and human emotions commonly attached to color, and how these factors can directly influence business branding, we developed a framework that helps web designers to select and optimize color scheme for websites and web applications, while avoiding common pitfalls that drive customers away, and standing out in today's fiercely competitive Internet marketplace. To evaluate the effectiveness of the framework, we conducted an experiment where subjects were asked to design the color scheme for a website. The experimental group was introduced to the web color framework before being evaluated, while the control group was evaluated based only off their personal knowledge and experience. We concluded with overall positive results in favor of the framework.},
	booktitle = {2016 49th {Hawaii} {International} {Conference} on {System} {Sciences} ({HICSS})},
	author = {Ferris, Kevin and Zhang, Sonya},
	month = jan,
	year = {2016},
	keywords = {business branding, color, Color, color scheme, color theory, colour, Companies, Industries, marketing, Visualization, Web color framework, web design, Web design, web development},
	pages = {532--541}
}

@incollection{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	urldate = {2020-02-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {2672--2680}
}

@article{radford_unsupervised_2015,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	volume = {abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	journal = {CoRR},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	year = {2015},
	keywords = {Computer vision, Discriminator, Generative adversarial networks, Machine learning, Supervised learning, Unsupervised learning}
}

@article{berthelot_began_2017,
	title = {{BEGAN}: {Boundary} {Equilibrium} {Generative} {Adversarial} {Networks}},
	shorttitle = {{BEGAN}},
	url = {http://arxiv.org/abs/1703.10717},
	abstract = {We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.},
	urldate = {2020-02-09},
	journal = {arXiv:1703.10717 [cs, stat]},
	author = {Berthelot, David and Schumm, Thomas and Metz, Luke},
	month = may,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{karras_progressive_2018,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {http://arxiv.org/abs/1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024ˆ2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
	urldate = {2020-02-09},
	journal = {arXiv:1710.10196 [cs, stat]},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{brock_large_2019,
	title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/1809.11096},
	abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
	urldate = {2020-02-09},
	journal = {arXiv:1809.11096 [cs, stat]},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	month = feb,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{karras_style-based_2019,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1812.04948},
	abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
	urldate = {2020-02-09},
	journal = {arXiv:1812.04948 [cs, stat]},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	month = mar,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@book{noauthor_deep_2020,
	title = {A {Deep} {Study} into the {History} of {Web} {Design} {IU} {Computer} {Vision} {Lab}},
	url = {http://vision.soic.indiana.edu/webdesign/},
	urldate = {2020-02-09},
	month = feb,
	year = {2020}
}

@inproceedings{liu_buttontips_2019,
	title = {{ButtonTips}: {Design} {Web} {Buttons} with {Suggestions}},
	shorttitle = {{ButtonTips}},
	doi = {10.1109/ICME.2019.00087},
	abstract = {Buttons are fundamental in web design. An effective button is important for higher click-through and conversion rates. However, designing effective buttons can be challenging for novices. This paper presents a novel interactive method to aid the button design process by making design suggestions. Our method proceeds in three steps: 1) button presence prediction, 2) button layout suggestion and 3) button color selection. We investigate two distinct but complementary interfaces for button design suggestion: 1) region selection interface, where the button will appear in a user-specific region; 2) element selection interface, where the button will be associated with a user-selected element. We compare our method with an existing website building tool, and show that for novice designers, both interfaces require significantly less manual efforts, and produce significantly better button design, as evaluated by professional web designers.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Liu, Dawei and Cao, Ying and Lau, Rynson W.H. and Chan, Antoni B.},
	month = jul,
	year = {2019},
	keywords = {Color, web design, Web design, Blogs, button design process, button design suggestion, button layout, Cats, data visualisation, data-driven method, Layout, Prediction algorithms, professional web designers, Tools, user interfaces, user-selected element},
	pages = {466--471}
}

@article{lee_design_2019,
	title = {Design of {Semantic}-{Based} {Colorization} of {Graphical} {User} {Interface} {Through} {Conditional} {Generative} {Adversarial} {Nets}},
	volume = {0},
	issn = {1044-7318},
	url = {https://doi.org/10.1080/10447318.2019.1680921},
	doi = {10.1080/10447318.2019.1680921},
	abstract = {There has recently been a significant movement toward aiding graphic design tasks based on artificial intelligence or machine learning. In addition, colorization plays an important role within the topic of GUI design. Previous studies regarding automatic colorization have focused on a consideration of the realistic aspects of an image without consideration of the design semantics or usability, which are critical aspects for a practical GUI design. We, therefore, propose an end-to-end network for a generative combination of color sets for a GUI design based on the design semantics, while utilizing thousands of actual GUI design datasets acquired from LG Electronics to train the network. By utilizing the GUI design dataset, our network effectively generates color sets for a GUI design by considering various design aspects, such as the usability factors. In detail, we concatenate the textual design concept, characteristics of the application, and usage frequency for the elements of the design semantics. We then construct a conditional generative adversarial net processing of the design semantics as a condition to generate suitable color sets and construct the GUI design based on these sets. The experiments indicate that our proposed method effectively generates color sets for a GUI design based on the design semantics. In addition, our proposed method shows a better score than other methods on a user test conducted to verify the practicality, perception, recognition, diversity, and esthetic features. Moreover, experimental results prove that users can effectively grasp the intended design concept of our generated GUI design with higher top-1, top-2, and top-3 levels of accuracy.},
	number = {0},
	urldate = {2020-02-09},
	journal = {International Journal of Human–Computer Interaction},
	author = {Lee, Younghoon and Cho, Sungzoon},
	month = oct,
	year = {2019},
	pages = {1--10}
}

@inproceedings{liu_learning_2018,
	series = {{UIST} '18},
	title = {Learning {Design} {Semantics} for {Mobile} {Apps}},
	isbn = {978-1-4503-5948-1},
	url = {https://doi.org/10.1145/3242587.3242650},
	doi = {10.1145/3242587.3242650},
	abstract = {Recently, researchers have developed black-box approaches to mine design and interaction data from mobile apps. Although the data captured during this interaction mining is descriptive, it does not expose the design semantics of UIs: what elements on the screen mean and how they are used. This paper introduces an automatic approach for generating semantic annotations for mobile app UIs. Through an iterative open coding of 73k UI elements and 720 screens, we contribute a lexical database of 25 types of UI components, 197 text button concepts, and 135 icon classes shared across apps. We use this labeled data to learn code-based patterns to detect UI components and to train a convolutional neural network that distinguishes between icon classes with 94\% accuracy. To demonstrate the efficacy of our approach at scale, we compute semantic annotations for the 72k unique UIs in the Rico dataset, assigning labels for 78\% of the total visible, non-redundant elements.},
	urldate = {2020-02-08},
	booktitle = {Proceedings of the 31st {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Thomas F. and Craft, Mark and Situ, Jason and Yumer, Ersin and Mech, Radomir and Kumar, Ranjitha},
	month = oct,
	year = {2018},
	note = {event-place: Berlin, Germany},
	keywords = {design semantics, machine learning, mobile app design},
	pages = {569--579}
}

@article{moran_machine_2018,
	title = {Machine {Learning}-{Based} {Prototyping} of {Graphical} {User} {Interfaces} for {Mobile} {Apps}},
	url = {http://arxiv.org/abs/1802.02312},
	abstract = {It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection, classification, and assembly. First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically assembled. We implemented this approach for Android in a system called ReDraw. Our evaluation illustrates that ReDraw achieves an average GUI-component classification accuracy of 91\% and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw's potential to improve real development workflows.},
	urldate = {2020-02-09},
	journal = {arXiv:1802.02312 [cs]},
	author = {Moran, Kevin and Bernal-Cárdenas, Carlos and Curcio, Michael and Bonett, Richard and Poshyvanyk, Denys},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Androids, CNN, Computer Science - Software Engineering, Graphical user interfaces, GUI, Humanoid robots, machine-learning, Metadata, mining software repositories, mobile, Prototypes, prototyping, Software, Task analysis, datasets}
}

@inproceedings{nguyen_deep_2018,
	title = {Deep {Learning} {UI} {Design} {Patterns} of {Mobile} {Apps}},
	abstract = {User interface (UI) is one of the most important components of a mobile app and strongly influences users' perception of the app. However, UI design tasks are typically manual and time-consuming. This paper proposes a novel approach to (semi)-automate those tasks. Our key idea is to develop and deploy advanced deep learning models based on recurrent neural networks (RNN) and generative adversarial networks (GAN) to learn UI design patterns from millions of currently available mobile apps. Once trained, those models can be used to search for UI design samples given user-provided descriptions written in natural language and generate professional-looking UI designs from simpler, less elegant design drafts.},
	booktitle = {2018 {IEEE}/{ACM} 40th {International} {Conference} on {Software} {Engineering}: {New} {Ideas} and {Emerging} {Technologies} {Results} ({ICSE}-{NIER})},
	author = {Nguyen, Tam and Vu, Phong and Pham, Hung and Nguyen, Tung},
	month = may,
	year = {2018},
	keywords = {Generative adversarial networks, Machine learning, Layout, user interfaces, Bars, Deep Learning, deep learning models, GAN, generative adversarial networks, learning (artificial intelligence), mobile apps, mobile computing, natural language, Natural languages, Navigation, Password, recurrent neural nets, recurrent neural networks, RNN, simpler design drafts, UI design patterns, UI Design Patterns, UI design samples, user interface, User Interface, user-provided descriptions, users perception},
	pages = {65--68}
}

@article{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	urldate = {2020-02-09},
	journal = {arXiv:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence}
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	abstract = {The problem this paper is concerned with is that of unsupervised learning. Mainly, what does it mean to learn a probability distribution? The classical answer to this is to learn a probability density. This is often done by defining a parametric family of densities (Pθ)θ∈Rd and finding the one that maximized the likelihood on our data: if we have real data examples \{x\}i=1, we would solve the problem},
	journal = {ArXiv},
	author = {Arjovsky, Martín and Chintala, Soumith and Bottou, Léon},
	year = {2017}
}

@book{noauthor_deep_2020-1,
	title = {Deep {Learning} {UI} {Design} {Patterns} of {Mobile} {Apps} - {Google} {Search}},
	url = {https://www.google.com/search?q=Deep+Learning+UI+Design+Patterns+of+Mobile+Apps&rlz=1C1CHZL_enPK889PK889&oq=Deep+Learning+UI+Design+Patterns+of+Mobile+Apps&aqs=chrome..69i57j69i59l2j69i61.754j0j7&sourceid=chrome&ie=UTF-8},
	urldate = {2020-02-23},
	month = feb,
	year = {2020}
}

@inproceedings{chen_storydroid_2019,
	series = {{ICSE} '19},
	title = {{StoryDroid}: automated generation of storyboard for {Android} apps},
	shorttitle = {{StoryDroid}},
	url = {https://doi.org/10.1109/ICSE.2019.00070},
	doi = {10.1109/ICSE.2019.00070},
	abstract = {Mobile apps are now ubiquitous. Before developing a new app, the development team usually endeavors painstaking efforts to review many existing apps with similar purposes. The review process is crucial in the sense that it reduces market risks and provides inspiration for app development. However, manual exploration of hundreds of existing apps by different roles (e.g., product manager, UI/UX designer, developer) in a development team can be ineffective. For example, it is difficult to completely explore all the functionalities of the app in a short period of time. Inspired by the conception of storyboard in movie production, we propose a system, StoryDroid, to automatically generate the storyboard for Android apps, and assist different roles to review apps efficiently. Specifically, StoryDroid extracts the activity transition graph and leverages static analysis techniques to render UI pages to visualize the storyboard with the rendered pages. The mapping relations between UI pages and the corresponding implementation code (e.g., layout code, activity code, and method hierarchy) are also provided to users. Our comprehensive experiments unveil that StoryDroid is effective and indeed useful to assist app development. The outputs of StoryDroid enable several potential applications, such as the recommendation of UI design and layout code.},
	urldate = {2020-02-23},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Chen, Sen and Fan, Lingling and Chen, Chunyang and Su, Ting and Li, Wenhe and Liu, Yang and Xu, Lihua},
	month = may,
	year = {2019},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {android app, app review, competitive analysis, storyboard},
	pages = {596--607}
}

@inproceedings{li_layoutgan_2018,
	title = {{LayoutGAN}: {Generating} {Graphic} {Layouts} with {Wireframe} {Discriminators}},
	shorttitle = {{LayoutGAN}},
	url = {https://openreview.net/forum?id=HJxB5sRcFQ},
	abstract = {Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of...},
	urldate = {2020-02-23},
	author = {Li, Jianan and Yang, Jimei and Hertzmann, Aaron and Zhang, Jianming and Xu, Tingfa},
	month = sep,
	year = {2018}
}

@inproceedings{potluri_ai-assisted_2019,
	title = {{AI}-{Assisted} {UI} {Design} for {Blind} and {Low}-{Vision} {Creators}},
	url = {https://makeabilitylab.cs.washington.edu/media/publications/Potluri_AiAssistedUiDesignForBlindAndLowVisionCreators_2019.pdf},
	abstract = {Visual aesthetics are critical to user interface (UI) design and usability. Prior work has shown that website aesthetics— which users evaluate in a ‘split second’ upon page load—are a definitive factor not just in engaging users online but also in impacting opinions about usability, trustworthiness, and overall user satisfaction. Currently, however, there is limited support for blind or low-vision (BLV) creators in designing, implementing, and/or assessing the visual aesthetics of their UI creations. In this workshop paper, we consider AIassisted user interface design as a potential solution. We provide background on related research in AI-assisted design and accessible programming, describe two preliminary studies examining BLV users’ current understanding of UIs and their ability to represent them with lo-fi methods, and close by discussing key open areas such as supporting BLV creators throughout the UI design process},
	booktitle = {{ASSETS}'19 {Workshop}: {AI} {Fairness} for {People} with {Disabilities}},
	author = {Potluri, Venkatesh and Grindeland, Tad and Froehlich, Jon E. and Mankoff, Jennifer},
	year = {2019},
	note = {event-place: Pittsburgh, PA}
}

@inproceedings{huang_swire_2019,
	series = {{CHI} '19},
	title = {Swire: {Sketch}-based {User} {Interface} {Retrieval}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Swire},
	url = {https://doi.org/10.1145/3290605.3300334},
	doi = {10.1145/3290605.3300334},
	abstract = {Sketches and real-world user interface examples are frequently used in multiple stages of the user interface design process. Unfortunately, finding relevant user interface examples, especially in large-scale datasets, is a highly challenging task because user interfaces have aesthetic and functional properties that are only indirectly reflected by their corresponding pixel data and meta-data. This paper introduces Swire, a sketch-based neural-network-driven technique for retrieving user interfaces. We collect the first large-scale user interface sketch dataset from the development of Swire that researchers can use to develop new sketch-based data-driven design interfaces and applications. Swire achieves high performance for querying user interfaces: for a known validation task it retrieves the most relevant example as within the top-10 results for over 60\% of queries. With this technique, for the first time designers can accurately retrieve relevant user interface examples with free-form sketches natural to their design workflows. We demonstrate several novel applications driven by Swire that could greatly augment the user interface design process.},
	urldate = {2020-02-23},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Forrest and Canny, John F. and Nichols, Jeffrey},
	month = may,
	year = {2019},
	note = {event-place: Glasgow, Scotland Uk},
	keywords = {computer vision, data-driven design, deep learning, design examples, information retrieval, sketching, user interface design},
	pages = {1--10}
}

@inproceedings{wu_understanding_2019,
	series = {{CHI} '19},
	title = {Understanding and {Modeling} {User}-{Perceived} {Brand} {Personality} from {Mobile} {Application} {UIs}},
	isbn = {978-1-4503-5970-2},
	url = {https://doi.org/10.1145/3290605.3300443},
	doi = {10.1145/3290605.3300443},
	abstract = {Designers strive to make their mobile apps stand out in a competitive market by creating a distinctive brand personality. However, it is unclear whether users can form a consistent impression of brand personality by looking at a few user interface (UI) screenshots in the app store, and if this process can be modeled computationally. To bridge this gap, we first collect crowd assessment on brand personalities depicted by the UIs of 318 applications, and statistically confirm that users can reach substantial agreement. To further model how users process mobile UI visually, we compute UI descriptors including Color, Organization, and Texture at both element and page levels. We feed these descriptors to a computational model, achieving a high accuracy of predicting perceived brand personality (MSE = 0.035 and Rˆ2 = 0.78). This work could benefit designers by highlighting contributing visual factors to brand personality creation and providing quick, low-cost design feedback.},
	urldate = {2020-02-23},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Ziming and Kim, Taewook and Li, Quan and Ma, Xiaojuan},
	month = may,
	year = {2019},
	note = {event-place: Glasgow, Scotland Uk},
	keywords = {brand personality, computational design assessment, mobile user interfaces},
	pages = {1--12}
}

@article{patil_read_2019,
	title = {{READ}: {Recursive} {Autoencoders} for {Document} {Layout} {Generation}},
	shorttitle = {{READ}},
	abstract = {Layout is a fundamental component of any graphic design. Creating large varieties of plausible document layouts can be a tedious task, requiring numerous constraints to be satisfied, including local ones relating different semantic elements and global constraints on the general appearance and spacing. In this paper, we present a novel framework, coined READ, for REcursive Autoencoders for Document layout generation, to generate plausible 2D layouts of documents in large quantities and varieties. First, we devise an exploratory recursive method to extract a structural decomposition of a single document. Leveraging a dataset of documents annotated with labeled bounding boxes, our recursive neural network learns to map the structural representation, given in the form of a simple hierarchy, to a compact code, the space of which is approximated by a Gaussian distribution. Novel hierarchies can be sampled from this space, obtaining new document layouts. Moreover, we introduce a combinatorial metric to measure structural similarity among document layouts. We deploy it to show that our method is able to generate highly variable and realistic layouts. We further demonstrate the utility of our generated layouts in the context of standard detection tasks on documents, showing that detection performance improves when the training data is augmented with generated documents whose layouts are produced by READ.},
	journal = {ArXiv},
	author = {Patil, Akshay Gadi and Ben-Eliezer, Omri and Perel, Or and Averbuch-Elor, Hadar},
	year = {2019}
}

@inproceedings{swearngin_rewire_2018,
	title = {Rewire: {Interface} {Design} {Assistance} from {Examples}},
	shorttitle = {Rewire},
	doi = {10.1145/3173574.3174078},
	abstract = {Interface designers often use screenshot images of example designs as building blocks for new designs. Since images are unstructured and hard to edit, designers typically reconstruct screenshots with vector graphics tools in order to reuse or edit parts of the design. Unfortunately, this reconstruction process is tedious and slow. We present Rewire, an interactive system that helps designers leverage example screenshots. Rewire automatically infers a vector representation of screenshots where each UI component is a separate object with editable shape and style properties. Based on this representation, the system provides three design assistance modes that help designers reuse or redraw components of the example design. The results from our quantitative and user evaluations demonstrate that Rewire can generate accurate vector representations of interface screenshots found in the wild and that design assistance enables users to reconstruct and edit example designs more efficiently compared to a baseline design tool.},
	booktitle = {{CHI} '18},
	author = {Swearngin, Amanda and Dontcheva, Mira and Li, Wilmot and Brandt, Joel and Dixon, Morgan and Ko, A. J.},
	year = {2018}
}

@inproceedings{huang_deep-learning-based_2020,
	title = {Deep-learning-based {Machine} {Understanding} of {Sketches}: {Recognizing} and {Generating} {Sketches} with {Deep} {Neural} {Networks}},
	shorttitle = {Deep-learning-based {Machine} {Understanding} of {Sketches}},
	abstract = {Sketching is an effective and natural method of visual communication among engineers, artists, and designers. This thesis explores several deep-learning-driven techniques for recognizing and generating sketches. We introduce two novel systems: 1) Swire, a system for querying large repositories of design examples with sketches; and 2) Sketchforme, a system that automatically composes sketched scenes from user-specified natural language descriptions. Through the development of these systems, we introduce multiple state-of-the-art techniques to perform novel sketch understanding and generation tasks supported by these systems. We also evaluate the performance of these systems using established metrics and user studies of interactive use-cases. Our evaluations show that these systems can effectively support interactive applications and open up new avenues of human-computer interaction in the domains of art, education, design, and beyond.},
	author = {Huang, Zifeng},
	year = {2020}
}

@book{odonovan_learning_2014,
	title = {Learning {Layouts} for {Single}-{PageGraphic} {Designs}},
	url = {/paper/Learning-Layouts-for-Single-PageGraphic-Designs-O&#39;Donovan-Agarwala/0042e77533923466961fcb459ac5e8b950d451e2},
	abstract = {This paper presents an approach for automatically creating graphic design layouts using a new energy-based model derived from design principles. The model includes several new algorithms for analyzing graphic designs, including the prediction of perceived importance, alignment detection, and hierarchical segmentation. Given the model, we use optimization to synthesize new layouts for a variety of single-page graphic designs. Model parameters are learned with Nonlinear Inverse Optimization (NIO) from a small number of example layouts. To demonstrate our approach, we show results for applications including generating design layouts in various styles, retargeting designs to new sizes, and improving existing designs. We also compare our automatic results with designs created using crowdsourcing and show that our approach performs slightly better than novice designers.},
	urldate = {2020-02-23},
	author = {O'Donovan, Peter and Agarwala, Aseem and Hertzmann, Aaron},
	year = {2014}
}

@article{lee_guicomp_2020,
	title = {{GUIComp}: {A} {GUI} {Design} {Assistant} with {Real}-{Time}, {Multi}-{Faceted} {Feedback}},
	shorttitle = {{GUIComp}},
	abstract = {Users may face challenges while designing graphical user interfaces, due to a lack of relevant experience and guidance. This paper aims to investigate the issues that users with no experience face during the design process, and how to resolve them. To this end, we conducted semi-structured interviews, based on which we built a GUI prototyping assistance tool called GUIComp. This tool can be connected to GUI design software as an extension, and it provides real-time, multifaceted feedback on a user’s current design. Additionally, we conducted two user studies, in which we asked participants to create mobile GUIs with or without GUIComp, and requested online workers to assess the created GUIs. The experimental results show that GUIComp facilitated iterative design and the participants with GUIComp had better a user experience and produced more acceptable designs than those who did not.},
	journal = {ArXiv},
	author = {Lee, Chunggi and Kim, Sanghoon and Han, Dongyun and Yang, Hongjun and Park, Youngwoo and Kwon, Bum Chul and Ko, Sungahn},
	year = {2020}
}

@article{schlattner_learning_2019,
	title = {Learning to {Infer} {User} {Interface} {Attributes} from {Images}},
	abstract = {We explore a new domain of learning to infer user interface attributes that helps developers automate the process of user interface implementation. Concretely, given an input image created by a designer, we learn to infer its implementation which when rendered, looks visually the same as the input image. To achieve this, we take a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), use it to generate a suitable synthetic training dataset, and then train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, we also use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values. We instantiate our approach to the task of inferring Android Button attribute values and achieve 92.5\% accuracy on a dataset consisting of real-world Google Play Store applications.},
	journal = {ArXiv},
	author = {Schlattner, Philippe and Bielik, Pavol and Vechev, Martin T.},
	year = {2019}
}

@book{shneiderman_designing_2016,
	address = {Boston},
	edition = {6 edition},
	title = {Designing the {User} {Interface}: {Strategies} for {Effective} {Human}-{Computer} {Interaction}},
	isbn = {978-0-13-438038-4},
	shorttitle = {Designing the {User} {Interface}},
	abstract = {For courses in Human-Computer Interaction The Sixth Edition of Designing the User Interface provides a comprehensive, authoritative, and up-to-date introduction to the dynamic field of human-computer interaction (HCI) and user experience (UX) design. This classic book has defined and charted the astonishing evolution of user interfaces for three decades. Students and professionals learn practical principles and guidelines needed to develop high quality interface designs that users can understand, predict, and control. The book covers theoretical foundations and design processes such as expert reviews and usability testing. By presenting current research and innovations in human-computer interaction, the authors strive to inspire students, guide designers, and provoke researchers to seek solutions that improve the experiences of novice and expert users, while achieving universal usability. The authors also provide balanced presentations on controversial topics such as augmented and virtual reality, voice and natural language interfaces, and information visualization. Updates include current HCI design methods, new design examples, and totally revamped coverage of social media, search and voice interaction. Major revisions were made to EVERY chapter, changing almost every figure (170 new color figures) and substantially updating the references.},
	publisher = {Pearson},
	author = {Shneiderman, Ben and Plaisant, Catherine and Cohen, Maxine and Jacobs, Steven and Elmqvist, Niklas and Diakopoulos, Nicholas},
	month = apr,
	year = {2016}
}

@book{benyon_designing_2013,
	address = {Boston},
	edition = {Comprehensive edition},
	title = {Designing {Interactive} {Systems}: {A} {Comprehensive} {Guide} to {HCI}, {UX} \& {Interaction} {Design}, 3rd ed.},
	isbn = {978-1-4479-2011-3},
	shorttitle = {Designing {Interactive} {Systems}},
	abstract = {Designing Interactive Systems is the definitive companion to the study of human–computer interaction (HCI), usability, user experience (UX) and interaction design. David Benyon has fully updated the content to include the newest and most exciting advancements within this rapidly changing field. The book covers the whole of the HCI and UX curriculum for students and practitioners alike. The book includes numerous case studies and illustrations taken from the author’s extensive experience of designing interactive systems and creating engaging user experiences. Each chapter includes thought-provoking exercises and challenges and reflective pull-outs pointing readers to related areas of study.},
	publisher = {Trans-Atlantic Publications, Inc.},
	author = {Benyon, David},
	month = sep,
	year = {2013}
}

@inproceedings{zhu_unpaired_2017,
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	doi = {10.1109/ICCV.2017.244},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = oct,
	year = {2017},
	keywords = {learning (artificial intelligence), computer vision, adversarial loss, collection style transfer, cycle consistency loss, cycle-consistent adversarial networks, Extraterrestrial measurements, Graphics, graphics problem, image distribution, image pair alignment, inverse mapping, learning, object transfiguration, Painting, photo enhancement, Semantics, Training, Training data, unpaired image-to-image translation, vision problem},
	pages = {2242--2251}
}

@article{karnewar_msg-gan_2019,
	title = {{MSG}-{GAN}: {Multi}-{Scale} {Gradient} {GAN} for {Stable} {Image} {Synthesis}},
	shorttitle = {{MSG}-{GAN}},
	abstract = {While Generative Adversarial Networks (GANs) have seen huge successes in image synthesis tasks, they are notoriously difficult to use, in part due to instability during training. One commonly accepted reason for this instability is that gradients passing from the discriminator to the generator can quickly become uninformative, due to a learning imbalance during training. In this work, we propose the Multi-Scale Gradient Generative Adversarial Network (MSG-GAN), a simple but effective technique for addressing this problem which allows the flow of gradients from the discriminator to the generator at multiple scales. This technique provides a stable approach for generating synchronized multi-scale images. We present a very intuitive implementation of the mathematical MSG-GAN framework which uses the concatenation operation in the discriminator computations. We empirically validate the effect of our MSG-GAN approach through experiments on the CIFAR10 and Oxford102 flowers datasets and compare it with other relevant techniques which perform multi-scale image synthesis. In addition, we also provide details of our experiment on CelebA-HQ dataset for synthesizing 1024 x 1024 high resolution images.},
	journal = {ArXiv},
	author = {Karnewar, Animesh and Wang, Oliver and Iyengar, Raghu Sesha},
	year = {2019}
}

@article{tschandl_ham10000_2018,
	title = {The {HAM10000} dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions},
	volume = {5},
	issn = {2052-4463},
	url = {http://arxiv.org/abs/1803.10417},
	doi = {10.1038/sdata.2018.161},
	abstract = {Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images. We tackle this problem by releasing the HAM10000 ("Human Against Machine with 10000 training images") dataset. We collected dermatoscopic images from different populations acquired and stored by different modalities. Given this diversity we had to apply different acquisition and cleaning methods and developed semi-automatic workflows utilizing specifically trained neural networks. The final dataset consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions. More than 50\% of lesions have been confirmed by pathology, while the ground truth for the rest of the cases was either follow-up, expert consensus, or confirmation by in-vivo confocal microscopy.},
	number = {1},
	urldate = {2020-03-12},
	journal = {Scientific Data},
	author = {Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
	month = dec,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {180161}
}

@inproceedings{mallikarjuna_kth-tips_2006,
	title = {{THE} {KTH}-{TIPS} 2 database},
	abstract = {1 Background This document provides a brief Users' Guide to the KTH-TIPS2 image database (KTH is the abbreviation of our university, and TIPS stands for Textures under varying Illumination, Pose and Scale). The KTH-TIPS2 provides a considerable extension to our previous database of images of materials — KTH-TIPS. The guide describes which materials are contained in the database (Section 2), how images were acquired (Section 3) and subsequently cropped to remove the background (Section 4), and we also discuss some non-ideal artifacts, like poor focus, in some pictures (Section 5). The objective with this database was to provide a more satisfactory means of evaluating algorithms for classifying materials. As we argued in [4, 1], a very relevant task is to recognise categories of materials such as " wood " or " wool " as opposed to one particular physical sample. The KTH-TIPS2 contains four physical samples of 11 different materials. In addition, it is frequently necessary to perform recognition in unstructured environments. Thus the database provides images with variations in scale as well as variations in pose and illumination, following on from the philosophy of the KTH-TIPS, and in part the CUReT image database [2]. The 11 materials in the KTH-TIPS2 database are all present also in the CUReT database [2], which opens the possibility of conducting experiments on a combination of the two databases. The cropped database is freely available on the internet [5]. Those interested in the full-size images should contact Eric Hayman (hayman@nada.kth.se). The database was first presented and used in [1]. The KTH-TIPS2 database contains images of 11 materials (Table 1 and Figure 1), each of which are also present in the CUReT database [2], and six of which were also included in the first KTH-TIPS database [3]. Each of the samples is planar.},
	author = {Mallikarjuna, P. and Targhi, Alireza Tavakoli and Fritz, Mario and Hayman, Eric and Caputo, Barbara and Eklundh, Jan-Olof},
	year = {2006}
}

@book{noauthor_gans_nodate,
	title = {{GANs} {Review}},
	abstract = {In the last few years, a type of generative model known as Generative Adversarial Networks (GANs), has achieved tremendous success mainly in the field of computer vision, image classification, speech and language processing, etc. GANs are the models which are used to produce new samples which have similar data distribution as of the training dataset. In this review paper, we will first introduce the idea behind the GANs, followed by a brief overview of various types of GANs as well as comparing it with different generative models. Then, we will discuss the application range and finally the future work with its associated research frontiers.}
}

@book{noauthor_mot_nodate,
	title = {{MOT} {Review} 1},
	abstract = {Object tracking is the process of locating moving objects over time using the camera in video sequences. The objective of object tracking is to associate target objects in consecutive video frames. Object tracking requires location and shape or features of objects in the video frames. So, object detection and object classification is the preceding steps of object tracking in computer vision application. To detect or locate the moving object in frame, Object detection is first stage in tracking. After that, detected object can be classified as vehicles, human, swaying tree, birds and other moving objects. It is challenging or difficult task in the image processing to track the objects into consecutive frames. Various challenges can arise due to complex object motion, irregular shape of object, occlusion of object to object and object to scene and real time processing requirements. Object tracking has a variety of uses, some of which are: surveillance and security, traffic monitoring, video communication, robot vision and animation. This paper presents the various techniques of object tracking in video sequences through different phases using image processing.}
}

@book{noauthor_mot_nodate-1,
	title = {{MOT} {Review} 2},
	abstract = {Image processing is a term which indicates the processing on image or video frame which is taken as an input and the result set of processing is may be a set of related parameters of an image. The used technique is to compare pixel by pixel a still frame from the video (the background image) with all other frames. Every time the pixels of a frame differ from the ones of the background image a simple comparation is done to get the location of those pixels and a red rectangle appears on screen following those pixels Some methods commonly use in it are background subtraction, Frame difference, template matching and shape based methods. We are going to discuss issues about detection and tracking. To analyse and study object detecting and tracking a literature review on some issue related to the subject is done and on the basis some concluded points are stated in the paper}
}

@book{noauthor_mot_nodate-2,
	title = {{MOT} {Review} 3},
	abstract = {Object tracking is one of the major fundamental challenging problems in computer vision applications due to difficulties in tracking of objects can arises due to intrinsic and extrinsic factors like deformation, camera motion, motion blur and occlusion. This paper proposes a literature review on several state–of–the-art object detection and tracking algorithms in order to reduce the tracking drift.}
}

@book{noauthor_mot_nodate-3,
	title = {{MOT} {Review} 4},
	abstract = {Moving Object Tracking is one of the testing issues in the field of computer vision, surveillance, traffic monitoring and so forth. Distinguishing the objects in the video and tracking its motion to recognize its qualities has been a demanding examination zone in the field of image processing and computer vision. The objective of object tracking is to place a moving object in sequential video frames. The goal of this paper is to review of different moving object detection and object tracking systems. This paper includes various methods for object detection and object tracking. Comparing all the methods researcher can use the better method for their future research.}
}

@book{noauthor_survey_nodate,
	title = {Survey on {Graphical} {User} {Interface} and {Machine} {Learning} {Based} {Testing} {Techniques} - {SciAlert} {Responsive} {Version}},
	url = {https://scialert.net/fulltextmobile/?doi=jai.2014.94.112},
	abstract = {Fulltext - Survey on Graphical User Interface and Machine Learning Based Testing Techniques, Survey on Graphical User Interface and Machine Learning Based Testing Techniques},
	urldate = {2020-04-10},
	doi = {10.3923/jai.2014.94.112}
}

@article{ratanasit_representing_2005,
	title = {Representing {Graphical} {User} {Interfaces} with {Sound}: {A} {Review} of {Approaches}},
	volume = {99},
	issn = {0145-482X},
	shorttitle = {Representing {Graphical} {User} {Interfaces} with {Sound}},
	url = {https://doi.org/10.1177/0145482X0509900202},
	doi = {10.1177/0145482X0509900202},
	abstract = {The inability of computer users who are visually impaired to access graphical user interfaces (GUIs) has led researchers to propose approaches for adapting GUIs to auditory interfaces, with the goal of providing access for visually impaired people. This article outlines the issues involved in nonvisual access to graphical user interfaces, reviews current research in this field, classifies methods and approaches, and discusses the extent to which researchers have resolved these issues.},
	number = {2},
	urldate = {2020-04-10},
	journal = {Journal of Visual Impairment \& Blindness},
	author = {Ratanasit, Dan and Moore, Melody M.},
	month = feb,
	year = {2005},
	pages = {69--84}
}

@book{boyd_graphical_1990,
	title = {The {Graphical} {User} {Interface} {Crisis}: {Danger} and {Opportunity}},
	shorttitle = {The {Graphical} {User} {Interface} {Crisis}},
	url = {https://eric.ed.gov/?id=ED333687},
	abstract = {This paper examines graphic computing environments, identifies potential problems in providing access to blind people, and describes programs and strategies being developed to provide this access. The paper begins with an explanation of how graphic user interfaces differ from character-based systems in their use of pixels, visual metaphors such as icons and windows, locational and contextual information, and mouse-controlled interaction and random access. The paper then analyzes how much of the benefits of the graphic user interface are shared by blind users. Three stages of access to the graphic user interface are described: (1) the customizing stage; (2) the ssingle-sensory mouseless strategy for providing access to standard text, icons and simple graphics, standard graphical structures, and navigation and control; (3) the multisensory approach with a resurrected mouse, designed to extend compatibility across applications and operating systems and extend access to complex graphics and other benefits. The paper concludes that, as problems are overcome, the resulting computer access systems will provide persons who are blind with new capabilities that were not possible with earlier character-based systems. (Includes 10 references.) (JDD)},
	urldate = {2020-04-10},
	publisher = {University of Wisconsin, TRACE Research \& Development Center, Waisman Center, 1500 Highland Ave},
	author = {Boyd, Lawrence H. and Others, And},
	month = sep,
	year = {1990},
	keywords = {Accessibility (for Disabled), Blindness, Computer Graphics, Computer Software Development, Computers, Design Requirements, Developmental Stages, Input Output Devices, Microcomputers, Problems, Technological Advancement}
}

@book{martinez_graphical_2011,
	title = {Graphical user interfaces},
	url = {https://doi.org/10.1002/wics.150},
	abstract = {This article provides a brief introduction to graphical user interfaces or GUIs. The first section defines graphical user interfaces, describes interface components, and the different types of GUIs. This is followed by a short discussion of GUI design principles and descriptions of some tools for easily creating GUIs. The article concludes with some examples of GUIs that would be of interest to researchers and statisticians. WIREs Comp Stat 2011 3 119-133 DOI: 10.1002/wics.150},
	urldate = {2020-04-10},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Martinez, Wendy L.},
	month = mar,
	year = {2011},
	keywords = {CRAN R, GUI design, MATLAB, software development}
}

@inproceedings{singh_designing_1988,
	series = {{UIST} '88},
	title = {Designing the interface designer's interface},
	isbn = {978-0-89791-283-9},
	url = {https://doi.org/10.1145/62402.62422},
	doi = {10.1145/62402.62422},
	abstract = {The concepts of a user interface management system (UIMS) and user interface designer have become well known in the user interface and graphics community. Most UIMSs developed so far have concentrated on the efficiency of generating the user interface; the user interface designer's interface has received relatively little attention. We identify the important features of the user interface designer's interface. A UIMS incorporating these features in its interface has been developed, and is described in this paper.},
	urldate = {2020-04-10},
	booktitle = {Proceedings of the 1st annual {ACM} {SIGGRAPH} symposium on {User} {Interface} {Software}},
	publisher = {Association for Computing Machinery},
	author = {Singh, Gurminder and Green, Mark},
	month = jan,
	year = {1988},
	note = {event-place: Alberta, Canada},
	pages = {109--116}
}

@book{colborne_simple_2010,
	address = {Berkeley, CA},
	edition = {1 edition},
	title = {Simple and {Usable} {Web}, {Mobile}, and {Interaction} {Design}},
	isbn = {978-0-321-70354-5},
	abstract = {In a complex world, products that are easy to use win favor with consumers. This is the first book on the topic of simplicity aimed specifically at interaction designers. It shows how to drill down and simplify user experiences when designing digital tools and applications. It begins by explaining why simplicity is attractive, explores the laws of simplicity, and presents proven strategies for achieving simplicity. Remove, hide, organize and displace become guidelines for designers, who learn simplicity by seeing before and after examples and case studies where the results speak for themselves.},
	publisher = {New Riders},
	author = {Colborne, Giles},
	month = sep,
	year = {2010}
}

@inproceedings{rehman_development_2018,
	title = {Development of {User} {Interface} for {Multi}-platform {Applications} {Using} the {Model} {Driven} {Software} {Engineering} {Techniques}},
	doi = {10.1109/IEMCON.2018.8615013},
	abstract = {Mobile application development is recently emerging rapidly creating competitive market. Every person has access to mobile applications been developed with time which has increased the need of application development for multiple platforms. Developing multi-platform mobile applications consist of various issues like time to market, extra effort on development etc. Another issue with multi-platform development is the poor User Interface (UI). Since U ser Interface is the main part of mobile application hence inadequate UI may cause neglection of application by users. To overcome these issues a UML profile is proposed in this paper. Code is generated automatically for multi-platform applications through profiling which consists of different UI classes, elements and stereotypes. This Model-Driven approach will convert UML model into native codes. This model will help to develop mobile applications in less time which will generate high revenue for developers.},
	booktitle = {2018 {IEEE} 9th {Annual} {Information} {Technology}, {Electronics} and {Mobile} {Communication} {Conference} ({IEMCON})},
	author = {Rehman, Saad and Ullah, Rana Muhammad Kaleem and Tanvir, Sara and Azam, Farooque},
	month = nov,
	year = {2018},
	keywords = {user interfaces, mobile computing, user interface, mobile application, mobile application development, Mobile applications, Mobile communication, Mobile handsets, Mobile User Interface (UI), Model Driven Software Engineering (MDSE), model driven software engineering techniques, model-driven approach, Multi-platform Application, multiplatform development, multiplatform mobile applications, Operating systems, Organizations, Profile, software architecture, UML profile, Unified modeling language, Unified Modeling Language, Unified Modeling Language (UML), User interfaces},
	pages = {1152--1158}
}

@book{punchoojit_usability_2017,
	title = {Usability {Studies} on {Mobile} {User} {Interface} {Design} {Patterns}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Usability {Studies} on {Mobile} {User} {Interface} {Design} {Patterns}},
	url = {https://www.hindawi.com/journals/ahci/2017/6787504/},
	abstract = {Mobile platforms have called for attention from HCI practitioners, and, ever since 2007, touchscreens have completely changed mobile user interface and interaction design. Some notable differences between mobile devices and desktops include the lack of tactile feedback, ubiquity, limited screen size, small virtual keys, and high demand of visual attention. These differences have caused unprecedented challenges to users. Most of the mobile user interface designs are based on desktop paradigm, but the desktop designs do not fully fit the mobile context. Although mobile devices are becoming an indispensable part of daily lives, true standards for mobile UI design patterns do not exist. This article provides a systematic literature review of the existing studies on mobile UI design patterns. The first objective is to give an overview of recent studies on the mobile designs. The second objective is to provide an analysis on what topics or areas have insufficient information and what factors are concentrated upon. This article will benefit the HCI community in seeing an overview of present works, to shape the future research directions.},
	urldate = {2020-04-10},
	author = {Punchoojit, Lumpapun and Hongwarittorrn, Nuttanont},
	year = {2017},
	doi = {10.1155/2017/6787504},
	doi = {10.1155/2017/6787504}
}

@inproceedings{latif_review_2017,
	title = {Review of mobile cross platform and research orientations},
	doi = {10.1109/WITS.2017.7934674},
	abstract = {Nowadays, mobile development for each different platform has become a critical challenge, due to the fact that it is still time and budget consuming. The cross-platform approaches are emerging as an alternative solution that many companies are starting to embrace, since it allows the development of mobile applications once for all platforms. The aim of our work is to give an exhaustive survey of cross platform approaches and tools, since to the best of our knowledge there is no paper similar to our work. In this paper, we will enhance our previous work of cross-platform approaches by giving an overview of recent tools and platforms for each approach and discussing the advantages and limits of each one.},
	booktitle = {2017 {International} {Conference} on {Wireless} {Technologies}, {Embedded} and {Intelligent} {Systems} ({WITS})},
	author = {Latif, Mounaim and Lakhrissi, Younes and Nfaoui, El Habib and Es-Sbai, Najia},
	month = apr,
	year = {2017},
	keywords = {mobile computing, Mobile applications, Mobile communication, User interfaces, Application programming interfaces, Cascading style sheets, Computational modeling, Context modeling, Cross-platforms, mobile applications, mobile cross-platform, mobile handsets, Model Driven Engineering, platforms mobile application development, research orientation},
	pages = {1--4}
}

@book{noauthor_uw_nodate,
	title = {{UW} {Interactive} {Data} {Lab} {Papers}},
	url = {http://idl.cs.washington.edu/papers/webzeitgeist},
	urldate = {2020-04-10}
}

@inproceedings{deka_rico_2017,
	series = {{UIST} '17},
	title = {Rico: {A} {Mobile} {App} {Dataset} for {Building} {Data}-{Driven} {Design} {Applications}},
	isbn = {978-1-4503-4981-9},
	shorttitle = {Rico},
	url = {https://doi.org/10.1145/3126594.3126651},
	doi = {10.1145/3126594.3126651},
	abstract = {Data-driven models help mobile app designers understand best practices and trends, and can be used to make predictions about design performance and support the creation of adaptive UIs. This paper presents Rico, the largest repository of mobile app designs to date, created to support five classes of data-driven applications: design search, UI layout generation, UI code generation, user interaction modeling, and user perception prediction. To create Rico, we built a system that combines crowdsourcing and automation to scalably mine design and interaction data from Android apps at runtime. The Rico dataset contains design data from more than 9.7k Android apps spanning 27 categories. It exposes visual, textual, structural, and interactive design properties of more than 72k unique UI screens. To demonstrate the kinds of applications that Rico enables, we present results from training an autoencoder for UI layout similarity, which supports query- by-example search over UIs.},
	urldate = {2020-04-25},
	booktitle = {Proceedings of the 30th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha},
	month = oct,
	year = {2017},
	note = {event-place: Québec City, QC, Canada},
	keywords = {mobile app design, app datasets, design mining, design search, datasets},
	pages = {845--854}
}

@inproceedings{deka_erica_2016,
	series = {{UIST} '16},
	title = {{ERICA}: {Interaction} {Mining} {Mobile} {Apps}},
	isbn = {978-1-4503-4189-9},
	shorttitle = {{ERICA}},
	url = {https://doi.org/10.1145/2984511.2984581},
	doi = {10.1145/2984511.2984581},
	abstract = {Design plays an important role in adoption of apps. App design, however, is a complex process with multiple design activities. To enable data-driven app design applications, we present interaction mining – capturing both static (UI layouts, visual details) and dynamic (user flows, motion details) components of an app's design. We present ERICA, a system that takes a scalable, human-computer approach to interaction mining existing Android apps without the need to modify them in any way. As users interact with apps through ERICA, it detects UI changes, seamlessly records multiple data-streams in the background, and unifies them into a user interaction trace. Using ERICA we collected interaction traces from over a thousand popular Android apps. Leveraging this trace data, we built machine learning classifiers to detect elements and layouts indicative of 23 common user flows. User flows are an important component of UX design and consists of a sequence of UI states that represent semantically meaningful tasks such as searching or composing. With these classifiers, we identified and indexed more than 3000 flow examples, and released the largest online search engine of user flows in Android apps.},
	urldate = {2020-04-25},
	booktitle = {Proceedings of the 29th {Annual} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Deka, Biplab and Huang, Zifeng and Kumar, Ranjitha},
	month = oct,
	year = {2016},
	note = {event-place: Tokyo, Japan},
	keywords = {design mining, app design, interaction mining, user flows, datasets},
	pages = {767--776}
}

@book{noauthor_ux_nodate,
	title = {{UX} {Archive}},
	url = {http://uxarchive.com/},
	urldate = {2020-04-25}
}

@book{noauthor_prima_nodate,
	title = {{PRImA} {RDCL2015}},
	url = {https://www.primaresearch.org/RDCL2015/},
	urldate = {2020-04-25}
}

@article{serban_user_2016,
	title = {User {Interface} {Design} in {Medical} {Distributed} {Web} {Applications}},
	volume = {223},
	issn = {1879-8365},
	abstract = {User interfaces are important to facilitate easy learning and operating with an IT application especially in the medical world. An easy to use interface has to be simple and to customize the user needs and mode of operation. The technology in the background is an important tool to accomplish this. The present work aims to creating a web interface using specific technology (HTML table design combined with CSS3) to provide an optimized responsive interface for a complex web application. In the first phase, the current icMED web medical application layout is analyzed, and its structure is designed using specific tools, on source files. In the second phase, a new graphic adaptable interface to different mobile terminals is proposed, (using HTML table design (TD) and CSS3 method) that uses no source files, just lines of code for layout design, improving the interaction in terms of speed and simplicity. For a complex medical software application a new prototype layout was designed and developed using HTML tables. The method uses a CSS code with only CSS classes applied to one or multiple HTML table elements, instead of CSS styles that can be applied to just one DIV tag at once. The technique has the advantage of a simplified CSS code, and a better adaptability to different media resolutions compared to DIV-CSS style method. The presented work is a proof that adaptive web interfaces can be developed just using and combining different types of design methods and technologies, using HTML table design, resulting in a simpler to learn and use interface, suitable for healthcare services.},
	journal = {Studies in Health Technology and Informatics},
	author = {Serban, Alexandru and Crisan-Vida, Mihaela and Mada, Leonard and Stoicu-Tivadar, Lacramioara},
	year = {2016},
	pmid = {27139407},
	keywords = {Software, Internet, Medical Informatics, Software Design, User-Computer Interface},
	pages = {223--229}
}

@article{hassenzahl_user_2006,
	title = {User experience - a research agenda},
	volume = {25},
	issn = {0144-929X},
	url = {https://doi.org/10.1080/01449290500330331},
	doi = {10.1080/01449290500330331},
	abstract = {Over the last decade, 'user experience' (UX) became a buzzword in the field of human - computer interaction (HCI) and interaction design. As technology matured, interactive products became not only more useful and usable, but also fashionable, fascinating things to desire. Driven by the impression that a narrow focus on interactive products as tools does not capture the variety and emerging aspects of technology use, practitioners and researchers alike, seem to readily embrace the notion of UX as a viable alternative to traditional HCI. And, indeed, the term promises change and a fresh look, without being too specific about its definite meaning. The present introduction to the special issue on 'Empirical studies of the user experience' attempts to give a provisional answer to the question of what is meant by 'the user experience'. It provides a cursory sketch of UX and how we think UX research will look like in the future. It is not so much meant as a forecast of the future, but as a proposal - a stimulus for further UX research.},
	number = {2},
	urldate = {2020-04-28},
	journal = {Behaviour \& Information Technology},
	author = {Hassenzahl, Marc and Tractinsky, Noam},
	month = mar,
	year = {2006},
	pages = {91--97}
}

@book{noauthor_40_nodate,
	title = {40 {Fascinating} {UX} {Statistics} to {Watch} out for in 2020 {TrueList}},
	url = {https://truelist.co/blog/ux-statistics/},
	urldate = {2020-04-28}
}

@techreport{rumelhart_learning_1985,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	url = {https://apps.dtic.mil/docs/citations/ADA164453},
	abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent. Keywords: Learning; networks; Perceptrons; Adaptive systems; Learning machines; and Back propagation.},
	number = {ICS-8506},
	urldate = {2020-04-28},
	institution = {CALIFORNIA UNIV SAN DIEGO LA JOLLA INST FOR COGNITIVE SCIENCE},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = sep,
	year = {1985}
}

@book{noauthor_modeling_nodate,
	title = {Modeling {Fonts} in {Context}: {Font} {Prediction} on {Web} {Designs}},
	url = {http://nxzhao.com/projects/font_in_context/},
	urldate = {2020-04-28}
}

@inproceedings{antonacopoulos_icdar2015_2015,
	title = {{ICDAR2015} competition on recognition of documents with complex layouts - {RDCL2015}},
	doi = {10.1109/ICDAR.2015.7333941},
	abstract = {This paper presents an objective comparative evaluation of page segmentation and region classification methods for documents with complex layouts. It describes the competition (modus operandi, dataset and evaluation methodology) held in the context of ICDAR2015, presenting the results of the evaluation of eight methods - four submitted, two state-of-the-art systems (one commercial and one open-source) and their two immediately previous versions. Three scenarios are reported in this paper, one evaluating the ability of methods to accurately segment regions and two evaluating both segmentation and region classification (one with emphasis on text and the other focusing only on text). The results indicate that an innovative approach has a clear advantage but there is still a considerable need to develop robust methods that deal with layout challenges, especially with the non-text content.},
	booktitle = {2015 13th {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Antonacopoulos, A. and Clausner, C. and Papadopoulos, C. and Pletschacher, S.},
	month = aug,
	year = {2015},
	keywords = {datasets, document image processing, document recognition, ICDAR2015 competition, image classification, image segmentation, innovative approach, layout analysis, nontext content, Optical character recognition software, page segmentation, page segmentation method, performance evaluation, recognition, region classification, region classification method, region segmentation, text analysis},
	pages = {1151--1155}
}

@book{noauthor_prima_nodate-1,
	title = {{PRImA}},
	url = {https://www.primaresearch.org/},
	urldate = {2020-04-29}
}

@article{zheng_content-aware_2019,
	title = {Content-aware generative modeling of graphic design layouts},
	volume = {38},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3306346.3322971},
	doi = {10.1145/3306346.3322971},
	abstract = {Layout is fundamental to graphic designs. For visual attractiveness and efficient communication of messages and ideas, graphic design layouts often have great variation, driven by the contents to be presented. In this paper, we study the problem of content-aware graphic design layout generation. We propose a deep generative model for graphic design layouts that is able to synthesize layout designs based on the visual and textual semantics of user inputs. Unlike previous approaches that are oblivious to the input contents and rely on heuristic criteria, our model captures the effect of visual and textual contents on layouts, and implicitly learns complex layout structure variations from data without the use of any heuristic rules. To train our model, we build a large-scale magazine layout dataset with fine-grained layout annotations and keyword labeling. Experimental results show that our model can synthesize high-quality layouts based on the visual semantics of input images and keyword-based summary of input text. We also demonstrate that our model internally learns powerful features that capture the subtle interaction between contents and layouts, which are useful for layout-aware design retrieval.},
	number = {4},
	urldate = {2020-05-02},
	journal = {ACM Transactions on Graphics},
	author = {Zheng, Xinru and Qiao, Xiaotian and Cao, Ying and Lau, Rynson W. H.},
	month = jul,
	year = {2019},
	keywords = {content-aware, deep generative networks, graphic design, layout},
	pages = {133:1--133:15}
}

@article{gui_review_2020,
	title = {A {Review} on {Generative} {Adversarial} {Networks}: {Algorithms}, {Theory}, and {Applications}},
	shorttitle = {A {Review} on {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2001.06937},
	abstract = {Generative adversarial networks (GANs) are a hot research topic recently. GANs have been widely studied since 2014, and a large number of algorithms have been proposed. However, there is few comprehensive study explaining the connections among different GANs variants, and how they have evolved. In this paper, we attempt to provide a review on various GANs methods from the perspectives of algorithms, theory, and applications. Firstly, the motivations, mathematical representations, and structure of most GANs algorithms are introduced in details. Furthermore, GANs have been combined with other machine learning algorithms for specific applications, such as semi-supervised learning, transfer learning, and reinforcement learning. This paper compares the commonalities and differences of these GANs methods. Secondly, theoretical issues related to GANs are investigated. Thirdly, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, medical field, and data science are illustrated. Finally, the future open research problems for GANs are pointed out.},
	urldate = {2020-05-02},
	journal = {arXiv:2001.06937 [cs, stat]},
	author = {Gui, Jie and Sun, Zhenan and Wen, Yonggang and Tao, Dacheng and Ye, Jieping},
	month = jan,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{litjens_survey_2017,
	title = {A survey on deep learning in medical image analysis},
	volume = {42},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841517301135},
	doi = {10.1016/j.media.2017.07.005},
	abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
	urldate = {2020-05-05},
	journal = {Medical Image Analysis},
	author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A. W. M. and van Ginneken, Bram and Sánchez, Clara I.},
	month = dec,
	year = {2017},
	keywords = {Convolutional neural networks, Deep learning, Medical imaging, Survey},
	pages = {60--88}
}

@article{berman_survey_2019,
	title = {A {Survey} of {Deep} {Learning} {Methods} for {Cyber} {Security}},
	volume = {10},
	url = {https://www.mdpi.com/2078-2489/10/4/122},
	doi = {10.3390/info10040122},
	abstract = {This survey paper describes a literature review of deep learning (DL) methods for cyber security applications. A short tutorial-style description of each DL method is provided, including deep autoencoders, restricted Boltzmann machines, recurrent neural networks, generative adversarial networks, and several others. Then we discuss how each of the DL methods is used for security applications. We cover a broad array of attack types including malware, spam, insider threats, network intrusions, false data injection, and malicious domain names used by botnets.},
	number = {4},
	urldate = {2020-05-05},
	journal = {Information},
	author = {Berman, Daniel S. and Buczak, Anna L. and Chavis, Jeffrey S. and Corbett, Cherita L.},
	month = apr,
	year = {2019},
	keywords = {deep learning, convolutional neural networks, cyber analytics, deep autoencoders, deep belief networks, deep neural networks, restricted Boltzmann machines},
	pages = {122}
}

@article{ruiz-del-solar_survey_2018,
	title = {A {Survey} on {Deep} {Learning} {Methods} for {Robot} {Vision}},
	url = {http://arxiv.org/abs/1803.10862},
	abstract = {Deep learning has allowed a paradigm shift in pattern recognition, from using hand-crafted features together with statistical classifiers to using general-purpose learning procedures for learning data-driven representations, features, and classifiers together. The application of this new paradigm has been particularly successful in computer vision, in which the development of deep learning methods for vision applications has become a hot research topic. Given that deep learning has already attracted the attention of the robot vision community, the main purpose of this survey is to address the use of deep learning in robot vision. To achieve this, a comprehensive overview of deep learning and its usage in computer vision is given, that includes a description of the most frequently used neural models and their main application areas. Then, the standard methodology and tools used for designing deep-learning based vision systems are presented. Afterwards, a review of the principal work using deep learning in robot vision is presented, as well as current and future trends related to the use of deep learning in robotics. This survey is intended to be a guide for the developers of robot vision systems.},
	urldate = {2020-05-05},
	journal = {arXiv:1803.10862 [cs]},
	author = {Ruiz-del-Solar, Javier and Loncomilla, Patricio and Soto, Naiomi},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 68T45}
}

@article{zacharias_survey_2018,
	title = {A {Survey} on {Deep} {Learning} {Toolkits} and {Libraries} for {Intelligent} {User} {Interfaces}},
	url = {http://arxiv.org/abs/1803.04818},
	abstract = {This paper provides an overview of prominent deep learning toolkits and, in particular, reports on recent publications that contributed open source software for implementing tasks that are common in intelligent user interfaces (IUI). We provide a scientific reference for researchers and software engineers who plan to utilise deep learning techniques within their IUI research and development projects.},
	urldate = {2020-05-05},
	journal = {arXiv:1803.04818 [cs]},
	author = {Zacharias, Jan and Barz, Michael and Sonntag, Daniel},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Human-Computer Interaction, H.5.2}
}

@article{kamilaris_deep_2018,
	title = {Deep learning in agriculture: {A} survey},
	volume = {147},
	issn = {0168-1699},
	shorttitle = {Deep learning in agriculture},
	url = {http://www.sciencedirect.com/science/article/pii/S0168169917308803},
	doi = {10.1016/j.compag.2018.02.016},
	abstract = {Deep learning constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture. In this paper, we perform a survey of 40 research efforts that employ deep learning techniques, applied to various agricultural and food production challenges. We examine the particular agricultural problems under study, the specific models and frameworks employed, the sources, nature and pre-processing of data used, and the overall performance achieved according to the metrics used at each work under study. Moreover, we study comparisons of deep learning with other existing popular techniques, in respect to differences in classification or regression performance. Our findings indicate that deep learning provides high accuracy, outperforming existing commonly used image processing techniques.},
	urldate = {2020-05-05},
	journal = {Computers and Electronics in Agriculture},
	author = {Kamilaris, Andreas and Prenafeta-Boldú, Francesc X.},
	month = apr,
	year = {2018},
	keywords = {Deep learning, Survey, Agriculture, Convolutional Neural Networks, Food systems, Recurrent Neural Networks, Smart farming},
	pages = {70--90}
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning}
}

@article{nwankpa_activation_2018,
	title = {Activation {Functions}: {Comparison} of trends in {Practice} and {Research} for {Deep} {Learning}},
	shorttitle = {Activation {Functions}},
	abstract = {Deep neural networks have been successfully used in diverse emerging domains to solve real world complex problems with may more deep learning(DL) architectures, being developed to date. To achieve these state-of-the-art performances, the DL architectures use activation functions (AFs), to perform diverse computations between the hidden layers and the output layers of any given DL architecture. This paper presents a survey on the existing AFs used in deep learning applications and highlights the recent trends in the use of the activation functions for deep learning applications. The novelty of this paper is that it compiles majority of the AFs used in DL and outlines the current trends in the applications and usage of these functions in practical deep learning deployments against the state-of-the-art research results. This compilation will aid in making effective decisions in the choice of the most suitable and appropriate activation function for any given application, ready for deployment. This paper is timely because most research papers on AF highlights similar works and results while this paper will be the first, to compile the trends in AF applications in practice against the research results from literature, found in deep learning research to date.},
	journal = {ArXiv},
	author = {Nwankpa, Chigozie and Ijomah, Winifred and Gachagan, Anthony and Marshall, Stephen},
	year = {2018}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	number = {6088},
	urldate = {2020-05-07},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536}
}

@article{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2020-05-07},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	keywords = {Computer Science - Machine Learning}
}

@article{botchkarev_performance_2019,
	title = {Performance {Metrics} ({Error} {Measures}) in {Machine} {Learning} {Regression}, {Forecasting} and {Prognostics}: {Properties} and {Typology}},
	volume = {14},
	issn = {1555-1229, 1555-1237},
	shorttitle = {Performance {Metrics} ({Error} {Measures}) in {Machine} {Learning} {Regression}, {Forecasting} and {Prognostics}},
	url = {http://arxiv.org/abs/1809.03006},
	doi = {10.28945/4184},
	abstract = {Performance metrics (error measures) are vital components of the evaluation frameworks in various fields. The intention of this study was to overview of a variety of performance metrics and approaches to their classification. The main goal of the study was to develop a typology that will help to improve our knowledge and understanding of metrics and facilitate their selection in machine learning regression, forecasting and prognostics. Based on the analysis of the structure of numerous performance metrics, we propose a framework of metrics which includes four (4) categories: primary metrics, extended metrics, composite metrics, and hybrid sets of metrics. The paper identified three (3) key components (dimensions) that determine the structure and properties of primary metrics: method of determining point distance, method of normalization, method of aggregation of point distances over a data set.},
	urldate = {2020-05-07},
	journal = {Interdisciplinary Journal of Information, Knowledge, and Management},
	author = {Botchkarev, Alexei},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	pages = {045--076}
}

@book{noauthor_image_nodate,
	title = {Image {Kernels} explained visually},
	url = {https://setosa.io/ev/image-kernels/},
	urldate = {2020-05-08}
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	keywords = {Machine learning, document recognition, Optical character recognition software, 2D shape variability, back-propagation, backpropagation, Character recognition, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition systems, Feature extraction, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, Hidden Markov models, high-dimensional patterns, language modeling, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, Neural networks, optical character recognition, Optical computing, Pattern recognition, performance measure minimization, Principal component analysis, segmentation recognition, LeNet},
	pages = {2278--2324}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2020-05-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	keywords = {AlexNet},
	pages = {1097--1105}
}

@inproceedings{liu_very_2015,
	title = {Very deep convolutional neural network based image classification using small training sample size},
	doi = {10.1109/ACPR.2015.7486599},
	abstract = {Since Krizhevsky won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 competition with the brilliant deep convolutional neural networks (D-CNNs), researchers have designed lots of D-CNNs. However, almost all the existing very deep convolutional neural networks are trained on the giant ImageNet datasets. Small datasets like CIFAR-10 has rarely taken advantage of the power of depth since deep models are easy to overfit. In this paper, we proposed a modified VGG-16 network and used this model to fit CIFAR-10. By adding stronger regularizer and using Batch Normalization, we achieved 8.45\% error rate on CIFAR-10 without severe overfitting. Our results show that the very deep CNN can be used to fit small datasets with simple and proper modifications and don't need to re-design specific small networks. We believe that if a model is strong enough to fit a large dataset, it can also fit a small one.},
	booktitle = {2015 3rd {IAPR} {Asian} {Conference} on {Pattern} {Recognition} ({ACPR})},
	author = {Liu, Shuying and Deng, Weihong},
	month = nov,
	year = {2015},
	keywords = {Training, Computational modeling, image classification, Neural networks, Acceleration, batch normalization, CIFAR-10, Convolution, D-CNNs, Data models, deep convolutional neural networks, Error analysis, ImageNet datasets, imagenet large scale visual recognition challenge, Krizhevsky, neural nets, VGG-16 network, VGGNet},
	pages = {730--734}
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	keywords = {Visualization, learning (artificial intelligence), Training, image classification, Neural networks, CIFAR-10, neural nets, COCO object detection dataset, COCO segmentation, Complexity theory, deep residual learning, deep residual nets, deeper neural network training, Degradation, ILSVRC \& COCO 2015 competitions, ILSVRC 2015 classification task, image recognition, Image recognition, Image segmentation, ImageNet dataset, ImageNet localization, ImageNet test set, object detection, residual function learning, residual nets, VGG nets, visual recognition tasks},
	pages = {770--778}
}

@inproceedings{szegedy_going_2015,
	title = {Going deeper with convolutions},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	keywords = {Visualization, Computer vision, image classification, convolution, Neural networks, object detection, architectural decision, Computer architecture, Convolutional codes, convolutional neural network architecture, decision making, feature extraction, Hebbian learning, Hebbian principle, neural net architecture, object classification, Object detection, resource allocation, resource utilization, Sparse matrices},
	pages = {1--9}
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	number = {7553},
	urldate = {2020-05-09},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444}
}

@article{lillicrap_backpropagation_2019,
	series = {Machine {Learning}, {Big} {Data}, and {Neuroscience}},
	title = {Backpropagation through time and the brain},
	volume = {55},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438818302009},
	doi = {10.1016/j.conb.2019.01.011},
	abstract = {It has long been speculated that the backpropagation-of-error algorithm (backprop) may be a model of how the brain learns. Backpropagation-through-time (BPTT) is the canonical temporal-analogue to backprop used to assign credit in recurrent neural networks in machine learning, but there's even less conviction about whether BPTT has anything to do with the brain. Even in machine learning the use of BPTT in classic neural network architectures has proven insufficient for some challenging temporal credit assignment (TCA) problems that we know the brain is capable of solving. Nonetheless, recent work in machine learning has made progress in solving difficult TCA problems by employing novel memory-based and attention-based architectures and algorithms, some of which are brain inspired. Importantly, these recent machine learning methods have been developed in the context of, and with reference to BPTT, and thus serve to strengthen BPTT's position as a useful normative guide for thinking about temporal credit assignment in artificial and biological systems alike.},
	urldate = {2020-05-09},
	journal = {Current Opinion in Neurobiology},
	author = {Lillicrap, Timothy P and Santoro, Adam},
	month = apr,
	year = {2019},
	pages = {82--89}
}

@inproceedings{pascanu_difficulty_2013,
	series = {{ICML}'13},
	title = {On the difficulty of training recurrent neural networks},
	abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
	urldate = {2020-05-09},
	booktitle = {Proceedings of the 30th {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 28},
	publisher = {JMLR.org},
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	month = jun,
	year = {2013},
	note = {event-place: Atlanta, GA, USA},
	keywords = {gradient clipping},
	pages = {III--1310--III--1318}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2020-05-09},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	keywords = {lstm},
	pages = {1735--1780}
}

@article{cho_learning_2014,
	title = {Learning phrase representations using {RNN} encoder-decoder for statistical machine translation},
	url = {https://nyuscholars.nyu.edu/en/publications/learning-phrase-representations-using-rnn-encoder-decoder-for-sta},
	urldate = {2020-05-09},
	journal = {Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)},
	author = {Cho, Kyunghyun and Merrienboer, B. van and Gulcehre, Caglar and Bougares, F. and Schwenk, H. and Bengio, Yoshua},
	year = {2014},
	keywords = {gated recurrent unit, gru}
}

@inproceedings{vincent_extracting_2008,
	series = {{ICML} '08},
	title = {Extracting and composing robust features with denoising autoencoders},
	isbn = {978-1-60558-205-4},
	url = {https://doi.org/10.1145/1390156.1390294},
	doi = {10.1145/1390156.1390294},
	abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
	urldate = {2020-05-10},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	month = jul,
	year = {2008},
	note = {event-place: Helsinki, Finland},
	keywords = {denoising autoencoders},
	pages = {1096--1103}
}

@inproceedings{jiang_novel_2013,
	title = {A novel sparse auto-encoder for deep unsupervised learning},
	doi = {10.1109/ICACI.2013.6748512},
	abstract = {This paper proposes a novel sparse variant of auto-encoders as a building block to pre-train deep neural networks. Compared with sparse auto-encoders through KL-divergence, our method requires fewer hyper-parameters and the sparsity level of the hidden units can be learnt automatically. We have compared our method with several other unsupervised leaning algorithms on the benchmark databases. The satisfactory classification accuracy (97.92\% on MNIST and 87.29\% on NORB) can be achieved by a 2-hidden-layer neural network pre-trained using our algorithm, and the whole training procedure (including pre-training and fine-tuning) takes far less time than the state-of-art results.},
	booktitle = {2013 {Sixth} {International} {Conference} on {Advanced} {Computational} {Intelligence} ({ICACI})},
	author = {Jiang, Xiaojuan and Zhang, Yinghua and Zhang, Wensheng and Xiao, Xian},
	month = oct,
	year = {2013},
	keywords = {Training, image classification, Principal component analysis, Data models, neural nets, 2-hidden-layer neural network pretraining, benchmark databases, classification accuracy, Classification algorithms, deep neural network pretraining, deep-unsupervised learning, fine-tuning procedure, hidden unit sparsity level, hyper-parameters, image coding, KL-divergence, Lead, MNIST dataset, NORB dataset, sparse auto-encoders, unsupervised learning, sparse autoencoders},
	pages = {256--261}
}

@article{vincent_stacked_2010,
	title = {Stacked {Denoising} {Autoencoders}: {Learning} {Useful} {Representations} in a {Deep} {Network} with a {Local} {Denoising} {Criterion}},
	volume = {11},
	issn = {1532-4435},
	shorttitle = {Stacked {Denoising} {Autoencoders}},
	abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
	journal = {The Journal of Machine Learning Research},
	author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	month = dec,
	year = {2010},
	keywords = {stacked autoencoders},
	pages = {3371--3408}
}

@article{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2020-05-10},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{rezende_stochastic_2014,
	title = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1401.4082},
	abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation – rules for back-propagation through stochastic variables – and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
	urldate = {2020-05-10},
	journal = {arXiv:1401.4082 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	month = may,
	year = {2014},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Methodology, Statistics - Computation}
}

@article{nash_equilibrium_1950,
	title = {Equilibrium points in n-person games},
	volume = {36},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/36/1/48},
	doi = {10.1073/pnas.36.1.48},
	abstract = {One may define a concept of an n -person game in which each player has a finite set of pure strategies and in which a definite set of payments to the n players corresponds to each n -tuple of pure strategies, one strategy being taken for each player. For mixed strategies, which are probability distributions over the pure strategies, the pay-off functions are the expectations of the players, thus becoming polylinear forms …},
	number = {1},
	urldate = {2020-05-11},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nash, John F.},
	month = jan,
	year = {1950},
	pmid = {16588946},
	pages = {48--49}
}

@article{albelwi_framework_2017,
	title = {A {Framework} for {Designing} the {Architectures} of {Deep} {Convolutional} {Neural} {Networks}},
	volume = {19},
	url = {https://www.mdpi.com/1099-4300/19/6/242},
	doi = {10.3390/e19060242},
	abstract = {Recent advances in Convolutional Neural Networks (CNNs) have obtained promising results in difficult deep learning tasks. However, the success of a CNN depends on finding an architecture to fit a given problem. A hand-crafted architecture is a challenging, time-consuming process that requires expert knowledge and effort, due to a large number of architectural design choices. In this article, we present an efficient framework that automatically designs a high-performing CNN architecture for a given problem. In this framework, we introduce a new optimization objective function that combines the error rate and the information learnt by a set of feature maps using deconvolutional networks (deconvnet). The new objective function allows the hyperparameters of the CNN architecture to be optimized in a way that enhances the performance by guiding the CNN through better visualization of learnt features via deconvnet. The actual optimization of the objective function is carried out via the Nelder-Mead Method (NMM). Further, our new objective function results in much faster convergence towards a better architecture. The proposed framework has the ability to explore a CNN architecture’s numerous design choices in an efficient way and also allows effective, distributed execution and synchronization via web services. Empirically, we demonstrate that the CNN architecture designed with our approach outperforms several existing approaches in terms of its error rate. Our results are also competitive with state-of-the-art results on the MNIST dataset and perform reasonably against the state-of-the-art results on CIFAR-10 and CIFAR-100 datasets. Our approach has a significant role in increasing the depth, reducing the size of strides, and constraining some convolutional layers not followed by pooling layers in order to find a CNN architecture that produces a high recognition performance.},
	number = {6},
	urldate = {2020-05-11},
	journal = {Entropy},
	author = {Albelwi, Saleh and Mahmood, Ausif},
	month = jun,
	year = {2017},
	keywords = {deep learning, CNN architecture design, convolutional neural networks (CNNs), correlation coefficient (Corr), deconvolutional networks (deconvnet), Nelder-Mead method (NMM), objective function, convdiagram},
	pages = {242}
}

@book{noauthor_understanding_nodate,
	title = {Understanding {LSTM} {Networks} – colah's blog},
	url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2020-05-11},
	keywords = {rnndiagram}
}

@book{birla_autoencoders_2019,
	title = {Autoencoders},
	url = {https://medium.com/@birla.deepak26/autoencoders-76bb49ae6a8f},
	abstract = {Autoencoders (AE) are type of artificial neural network that aims to copy their inputs to their outputs . They work by compressing the…},
	urldate = {2020-05-11},
	author = {Birla, Deepak},
	month = mar,
	year = {2019},
	keywords = {autoencoderdiagram}
}

@book{noauthor_getting_2020,
	title = {Getting started with {Generative} {Adversarial} {Networks} ({GANs})},
	url = {https://www.analyticsvidhya.com/blog/2020/01/generative-models-gans-computer-vision/},
	abstract = {A beginner-level introduction to Generative Models and GANs. This is an end-to-end guide to understand how GANs work in computer vision.},
	urldate = {2020-05-11},
	month = jan,
	year = {2020},
	keywords = {gandiagram1}
}

@article{barrios_partial_2019,
	title = {Partial {Discharge} {Classification} {Using} {Deep} {Learning} {Methods}—{Survey} of {Recent} {Progress}},
	volume = {12},
	doi = {10.3390/en12132485},
	abstract = {This paper examines the recent advances made in the field of Deep Learning (DL) methods for the automated identification of Partial Discharges (PD). PD activity is an indication of the state and operational conditions of electrical equipment systems. There are several techniques for on-line PD measurements, but the typical classification and recognition method is made off-line and involves an expert manually extracting appropriate features from raw data and then using these to diagnose PD type and severity. Many methods have been developed over the years, so that the appropriate features expertly extracted are used as input for Machine Learning (ML) algorithms. More recently, with the developments in computation and data storage, DL methods have been used for automated features extraction and classification. Several contributions have demonstrated that Deep Neural Networks (DNN) have better accuracy than the typical ML methods providing more efficient automated identification techniques. However, improvements could be made regarding the general applicability of the method, the data acquisition, and the optimal DNN structure.},
	journal = {Energies},
	author = {{Barrios} and {Buldain} and {Comech} and {Gilbert} and {Orue}},
	month = jun,
	year = {2019},
	keywords = {gandiagram2},
	pages = {2485}
}

@book{chollet_deep_2017,
	address = {Shelter Island, New York},
	edition = {1st edition},
	title = {Deep {Learning} with {Python}},
	isbn = {978-1-61729-443-3},
	abstract = {SummaryDeep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher François Chollet, this book builds your understanding through intuitive explanations and practical examples.Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.About the TechnologyMachine learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learning—a combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications.About the BookDeep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher François Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects. What's InsideDeep learning from first principlesSetting up your own deep-learning environment Image-classification modelsDeep learning for text and sequencesNeural style transfer, text generation, and image generationAbout the ReaderReaders need intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required.About the AuthorFrançois Chollet works on deep learning at Google in Mountain View, CA. He is the creator of the Keras deep-learning library, as well as a contributor to the TensorFlow machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition (CVPR), the Conference and Workshop on Neural Information Processing Systems (NIPS), the International Conference on Learning Representations (ICLR), and others.Table of ContentsPART 1 - FUNDAMENTALS OF DEEP LEARNING What is deep learning?Before we begin: the mathematical building blocks of neural networks Getting started with neural networksFundamentals of machine learningPART 2 - DEEP LEARNING IN PRACTICEDeep learning for computer visionDeep learning for text and sequencesAdvanced deep-learning best practicesGenerative deep learningConclusionsappendix A - Installing Keras and its dependencies on Ubuntuappendix B - Running Jupyter notebooks on an EC2 GPU instance},
	publisher = {Manning Publications},
	author = {Chollet, François},
	month = dec,
	year = {2017}
}

@article{weng_gan_2019,
	title = {From {GAN} to {WGAN}},
	url = {http://arxiv.org/abs/1904.08994},
	abstract = {This paper explains the math behind a generative adversarial network (GAN) model and why it is hard to be trained. Wasserstein GAN is intended to improve GANs' training by adopting a smooth metric for measuring the distance between two probability distributions.},
	urldate = {2020-05-13},
	journal = {arXiv:1904.08994 [cs, stat]},
	author = {Weng, Lilian},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{gulrajani_improved_2017,
	series = {{NIPS}'17},
	title = {Improved training of wasserstein {GANs}},
	isbn = {978-1-5108-6096-4},
	abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
	urldate = {2020-05-13},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	month = dec,
	year = {2017},
	note = {event-place: Long Beach, California, USA},
	keywords = {WGAN-GP},
	pages = {5769--5779}
}

@article{mirza_conditional_2014-1,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	journal = {ArXiv},
	author = {Mirza, Mehdi and Osindero, Simon},
	year = {2014}
}

@article{zhu_semantically_2020,
	title = {Semantically {Multi}-modal {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2003.12697},
	abstract = {In this paper, we focus on semantically multi-modal image synthesis (SMIS) task, namely, generating multi-modal images at the semantic level. Previous work seeks to use multiple class-specific generators, constraining its usage in datasets with a small number of classes. We instead propose a novel Group Decreasing Network (GroupDNet) that leverages group convolutions in the generator and progressively decreases the group numbers of the convolutions in the decoder. Consequently, GroupDNet is armed with much more controllability on translating semantic labels to natural images and has plausible high-quality yields for datasets with many classes. Experiments on several challenging datasets demonstrate the superiority of GroupDNet on performing the SMIS task. We also show that GroupDNet is capable of performing a wide range of interesting synthesis applications. Codes and models are available at: https://github.com/Seanseattle/SMIS.},
	urldate = {2020-05-27},
	journal = {arXiv:2003.12697 [cs]},
	author = {Zhu, Zhen and Xu, Zhiliang and You, Ansheng and Bai, Xiang},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@book{tam_can_2019,
	title = {Can artificial intelligence replace (graphic) designers?},
	url = {https://medium.com/invisibledesigns/can-artificial-intelligence-replace-graphic-designers-90b7ad82b212},
	abstract = {A few thoughts on the role of graphic designers, prompted by Ken Garland’s words from 1966.},
	urldate = {2020-06-01},
	author = {Tam, Keith},
	month = apr,
	year = {2019}
}

@inproceedings{long_fully_2015,
	title = {Fully convolutional networks for semantic segmentation},
	doi = {10.1109/CVPR.2015.7298965},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = jun,
	year = {2015},
	keywords = {learning (artificial intelligence), learning, Semantics, Training, image classification, image segmentation, Convolution, Image segmentation, Computer architecture, Adaptation models, contemporary classification networks, Deconvolution, fully convolutional networks, inference, inference mechanisms, NYUDv2, PASCAL VOC, pixels-to-pixels, semantic segmentation, SIFT flow, visual models},
	pages = {3431--3440}
}

@article{badrinarayanan_segnet_2017,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	volume = {39},
	issn = {1939-3539},
	shorttitle = {{SegNet}},
	doi = {10.1109/TPAMI.2016.2644615},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = dec,
	year = {2017},
	keywords = {learning (artificial intelligence), Semantics, Training, image classification, image segmentation, Neural networks, Image segmentation, Computer architecture, Convolutional codes, feature extraction, Decoding, image resolution, inference mechanisms, Caffe implementation, competitive inference time, convolutional layers, core trainable segmentation engine, decoder, decoder network, DeconvNet architectures, deep convolutional encoder-decoder architecture, Deep convolutional neural networks, DeepLab-LargeFOV, dense feature maps, encoder, encoder network, FCN, gradient methods, image colour analysis, image representation, indoor scenes, low resolution encoder feature maps, lower resolution input feature map, max-pooling step, nonlinear upsampling, pixel-wise classification layer, pixel-wise segmentation, pooling, practical deep fully convolutional neural network architecture, road scenes, SegNet, self-organising feature maps, semantic pixel-wise segmentation, stochastic gradient descent, SUN RGB-D indoor scene segmentation, SUN RGB-D indoor scene segmentation tasks, topology, upsampling, VGG16 network},
	pages = {2481--2495}
}

@article{guo_review_2018,
	title = {A review of semantic segmentation using deep neural networks},
	volume = {7},
	issn = {2192-662X},
	url = {https://doi.org/10.1007/s13735-017-0141-z},
	doi = {10.1007/s13735-017-0141-z},
	abstract = {During the long history of computer vision, one of the grand challenges has been semantic segmentation which is the ability to segment an unknown image into different parts and objects (e.g., beach, ocean, sun, dog, swimmer). Furthermore, segmentation is even deeper than object recognition because recognition is not necessary for segmentation. Specifically, humans can perform image segmentation without even knowing what the objects are (for example, in satellite imagery or medical X-ray scans, there may be several objects which are unknown, but they can still be segmented within the image typically for further investigation). Performing segmentation without knowing the exact identity of all objects in the scene is an important part of our visual understanding process which can give us a powerful model to understand the world and also be used to improve or augment existing computer vision techniques. Herein this work, we review the field of semantic segmentation as pertaining to deep convolutional neural networks. We provide comprehensive coverage of the top approaches and summarize the strengths, weaknesses and major challenges.},
	number = {2},
	urldate = {2020-06-02},
	journal = {International Journal of Multimedia Information Retrieval},
	author = {Guo, Yanming and Liu, Yu and Georgiou, Theodoros and Lew, Michael S.},
	month = jun,
	year = {2018},
	pages = {87--93}
}

@book{brownlee_generative_2019,
	title = {Generative {Adversarial} {Networks} with {Python}: {Deep} {Learning} {Generative} {Models} for {Image} {Synthesis} and {Image} {Translation}},
	shorttitle = {Generative {Adversarial} {Networks} with {Python}},
	abstract = {Step-by-step tutorials on generative adversarial networks in python for image synthesis and image translation.},
	publisher = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jul,
	year = {2019},
	keywords = {Computers / Programming / Algorithms}
}

@article{chauhan_comparative_nodate,
	title = {Comparative {Study} of {GAN} and {VAE}},
	volume = {182},
	abstract = {Generative models are very popular in a ﬁeld of unsupervised learning.They are tremendously successful to learn underlying data distribution of training data and generate a new data with some variations.This paper presents a detailed study of generative models and how they differ from traditional discriminative models.The paper more focus on two most popular generative models such as Variational Autoencoder(VAE) and Generative Adversarial Network(GAN).The paper includes working of these generative models, their architecture and an experiment is conducted to generate images using very popular MNIST data set.The comparison between these two models and their advantages and disadvantages are presented based on an experiment.At last, some solutions are presented to further improve these models.},
	journal = {International Journal of Computer Applications},
	author = {Chauhan, Jaydeep T},
	pages = {5}
}

@inproceedings{sami_comparative_2019,
	title = {A {Comparative} {Study} on {Variational} {Autoencoders} and {Generative} {Adversarial} {Networks}},
	doi = {10.1109/ICAIIT.2019.8834544},
	abstract = {Generative Adversarial Networks (GAN) have been remarkable at generating artificial data, especially realistic looking images. This learning framework has proven itself to be effective in synthetic image generation, semantic image hole filling, semantic image editing, style transfer and many more. On the other hand, variational auto-encoders (VAE) have also been quite effective, so much so that mathematically it is often more accurate at generating images resembling to its original dataset. Nevertheless, images generated by VAE suffer from blurriness and are generally less realistic looking from human perception. In this paper we take a broad view on both systems and propose a theoretical approach to combine them and bring out the best of both.},
	booktitle = {2019 {International} {Conference} of {Artificial} {Intelligence} and {Information} {Technology} ({ICAIIT})},
	author = {Sami, Mirza and Mobin, Iftekharul},
	month = mar,
	year = {2019},
	keywords = {Generative adversarial networks, GAN, generative adversarial networks, learning (artificial intelligence), Training, Computational modeling, image classification, image segmentation, Data models, neural nets, Decoding, artificial data, Autoencoders, Gallium nitride, Generators, realistic looking images, semantic image editing, semantic image hole, synthetic image generation, variational auto-encoders, variational autoencoders, Variational Inference},
	pages = {1--5}
}

@book{noauthor_meet_nodate,
	title = {Meet {Android} {Studio} {Android} {Developers}},
	url = {https://developer.android.com/studio/intro},
	abstract = {Android Studio is the official Integrated Development Environment (IDE) for Android app development, based on IntelliJ IDEA.},
	urldate = {2020-07-29}
}

@book{noauthor_xcode_nodate,
	title = {Xcode},
	url = {https://developer.apple.com/xcode/},
	abstract = {Xcode includes everything you need to create amazing apps for iPhone, iPad, Mac, Apple Watch, and Apple TV.},
	urldate = {2020-07-29}
}

@book{noauthor_visual_nodate,
	title = {Visual {Studio} {IDE}, {Code} {Editor}, {Azure} {DevOps}, \& {App} {Center}},
	url = {https://visualstudio.microsoft.com},
	abstract = {Visual Studio dev tools \& services make app development easy for any platform \& language. Try our Mac \& Windows code editor, IDE, or Azure DevOps for free.},
	urldate = {2020-07-29}
}

@article{sherstinsky_fundamentals_2020,
	title = {Fundamentals of {Recurrent} {Neural} {Network} ({RNN}) and {Long} {Short}-{Term} {Memory} ({LSTM}) network},
	volume = {404},
	issn = {0167-2789},
	url = {http://www.sciencedirect.com/science/article/pii/S0167278919305974},
	doi = {10.1016/j.physd.2019.132306},
	abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of “unrolling” an RNN is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the “Vanilla LSTM”1 1The nickname “Vanilla LSTM” symbolizes this model’s flexibility and generality (Greff et al., 2015). network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
	urldate = {2020-07-29},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Sherstinsky, Alex},
	month = mar,
	year = {2020},
	keywords = {RNN, Convolutional input context windows, External input gate, LSTM, RNN unfolding/unrolling},
	pages = {132306}
}

@book{noauthor_operating_nodate,
	title = {Operating {System} {Market} {Share} {Worldwide}},
	url = {https://gs.statcounter.com/os-market-share},
	abstract = {This graph shows the market share of operating systems worldwide based on over 10 billion monthly page views.},
	urldate = {2020-08-05}
}

@book{noauthor_how_nodate,
	title = {How to {Generate} ({Almost}) {Anything}},
	url = {http://howtogeneratealmostanything.com},
	abstract = {The Ultimate Human-AI Collaboration: Can humans and AI work together to create things that wouldn't have existed otherwise?},
	urldate = {2020-08-05}
}

@inproceedings{qiao_mirrorgan_2019,
	title = {{MirrorGAN}: {Learning} {Text}-{To}-{Image} {Generation} by {Redescription}},
	shorttitle = {{MirrorGAN}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Qiao_MirrorGAN_Learning_Text-To-Image_Generation_by_Redescription_CVPR_2019_paper.html},
	urldate = {2020-08-05},
	author = {Qiao, Tingting and Zhang, Jing and Xu, Duanqing and Tao, Dacheng},
	year = {2019},
	pages = {1505--1514}
}

@book{noauthor_invision_nodate,
	title = {{InVision} {Digital} product design, workflow \& collaboration},
	url = {https://www.invisionapp.com/},
	abstract = {InVision is the digital product design platform used to make the world’s best customer experiences. InVisionApp, Inc.},
	urldate = {2020-08-05}
}

@book{noauthor_uiux_nodate,
	title = {{UI}/{UX} design and collaboration tool {Adobe} {XD}},
	url = {https://www.adobe.com/in/products/xd.html},
	abstract = {Adobe XD is your UI/UX design solution for website and mobile app creation. Design, prototype and share. Try it now.},
	urldate = {2020-08-05}
}

@book{noauthor_digital_nodate,
	title = {The digital design toolkit},
	url = {https://www.sketch.com/},
	abstract = {Sketch is a design toolkit built to help you create your best work — from your earliest ideas, through to final artwork.},
	urldate = {2020-08-05}
}

@article{von_wangenheim_we_2018,
	title = {Do we agree on user interface aesthetics of {Android} apps?},
	url = {http://arxiv.org/abs/1812.09049},
	abstract = {Context: Visual aesthetics is increasingly seen as an essential factor in perceived usability, interaction, and overall appraisal of user interfaces especially with respect to mobile applications. Yet, a question that remains is how to assess and to which extend users agree on visual aesthetics. Objective: This paper analyzes the inter-rater agreement on visual aesthetics of user interfaces of Android apps as a basis for guidelines and evaluation models. Method: We systematically collected ratings on the visual aesthetics of 100 user interfaces of Android apps from 10 participants and analyzed the frequency distribution, reliability and influencing design aspects. Results: In general, user interfaces of Android apps are perceived more ugly than beautiful. Yet, raters only moderately agree on the visual aesthetics. Disagreements seem to be related to subtle differences with respect to layout, shapes, colors, typography, and background images. Conclusion: Visual aesthetics is a key factor for the success of apps. However, the considerable disagreement of raters on the perceived visual aesthetics indicates the need for a better understanding of this software quality with respect to mobile apps.},
	urldate = {2020-09-07},
	journal = {arXiv:1812.09049 [cs]},
	author = {von Wangenheim, Christiane G. and Porto, João V. Araujo and Hauck, Jean C. R. and Borgatto, Adriano F.},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.09049},
	keywords = {Computer Science - Software Engineering, Computer Science - Human-Computer Interaction, H.5.2, D.2.10}
}

@article{chen_object_2020,
	title = {Object {Detection} for {Graphical} {User} {Interface}: {Old} {Fashioned} or {Deep} {Learning} or a {Combination}?},
	shorttitle = {Object {Detection} for {Graphical} {User} {Interface}},
	url = {http://arxiv.org/abs/2008.05132},
	doi = {10.1145/3368089.3409691},
	abstract = {Detecting Graphical User Interface (GUI) elements in GUI images is a domain-specific object detection task. It supports many software engineering tasks, such as GUI animation and testing, GUI search and code generation. Existing studies for GUI element detection directly borrow the mature methods from computer vision (CV) domain, including old fashioned ones that rely on traditional image processing features (e.g., canny edge, contours), and deep learning models that learn to detect from large-scale GUI data. Unfortunately, these CV methods are not originally designed with the awareness of the unique characteristics of GUIs and GUI elements and the high localization accuracy of the GUI element detection task. We conduct the first large-scale empirical study of seven representative GUI element detection methods on over 50k GUI images to understand the capabilities, limitations and effective designs of these methods. This study not only sheds the light on the technical challenges to be addressed but also informs the design of new GUI element detection methods. We accordingly design a new GUI-specific old-fashioned method for non-text GUI element detection which adopts a novel top-down coarse-to-fine strategy, and incorporate it with the mature deep learning model for GUI text detection.Our evaluation on 25,000 GUI images shows that our method significantly advances the start-of-the-art performance in GUI element detection.},
	urldate = {2020-09-07},
	journal = {arXiv:2008.05132 [cs]},
	author = {Chen, Jieshan and Xie, Mulong and Xing, Zhenchang and Chen, Chunyang and Xu, Xiwei and Zhu, Liming},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.05132},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Software Engineering, Computer Science - Human-Computer Interaction}
}

@article{chen_wireframe-based_2020,
	title = {Wireframe-based {UI} {Design} {Search} through {Image} {Autoencoder}},
	volume = {29},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3391613},
	doi = {10.1145/3391613},
	abstract = {UI design is an integral part of software development. For many developers who do not have much UI design experience, exposing them to a large database of real-application UI designs can help them quickly build up a realistic understanding of the design space for a software feature and get design inspirations from existing applications. However, existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face of the great variations in UI designs. In this article, we propose a deep-learning-based UI design search engine to fill in the gap. The key innovation of our search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs. We implement our approach for Android UI design search, and conduct extensive experiments with artificially created relevant UI designs and human evaluation of UI design search results. Our experiments confirm the superior performance of our search engine over existing image-similarity or component-matching-based methods and demonstrate the usefulness of our search engine in real-world UI design tasks.},
	number = {3},
	urldate = {2020-09-07},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Chen, Jieshan and Chen, Chunyang and Xing, Zhenchang and Xia, Xin and Zhu, Liming and Grundy, John and Wang, Jinshui},
	month = jun,
	year = {2020},
	keywords = {deep learning, Android, auto-encoder, UI search},
	pages = {19:1--19:31}
}

@inproceedings{li_humanoid_2019,
	title = {Humanoid: {A} {Deep} {Learning}-{Based} {Approach} to {Automated} {Black}-box {Android} {App} {Testing}},
	shorttitle = {Humanoid},
	doi = {10.1109/ASE.2019.00104},
	abstract = {Automated input generators must constantly choose which UI element to interact with and how to interact with it, in order to achieve high coverage with a limited time budget. Currently, most black-box input generators adopt pseudo-random or brute-force searching strategies, which may take very long to find the correct combination of inputs that can drive the app into new and important states. We propose Humanoid, an automated black-box Android app testing tool based on deep learning. The key technique behind Humanoid is a deep neural network model that can learn how human users choose actions based on an app's GUI from human interaction traces. The learned model can then be used to guide test input generation to achieve higher coverage. Experiments on both open-source apps and market apps demonstrate that Humanoid is able to reach higher coverage, and faster as well, than the state-of-the-art test input generators. Humanoid is open-sourced at https://github.com/yzygitzh/Humanoid and a demo video can be found at https://youtu.be/PDRxDrkyORs.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Li, Yuanchun and Yang, Ziyue and Guo, Yao and Chen, Xiangqun},
	month = nov,
	year = {2019},
	keywords = {Tools, Graphical user interfaces, GUI, Humanoid robots, learning (artificial intelligence), deep learning, mobile application, Mobile applications, neural nets, Generators, graphical user interface, graphical user interfaces, program testing, Android, Android (operating system), automated black-box Android app testing tool, automated input generators, automated test input generation, black-box input generators, brute-force searching strategies, deep learning-based approach, deep neural network model, human interaction traces, humanoid, Open source software, open-source apps, software testing, test input generation, Testing},
	pages = {1070--1073}
}

@inproceedings{micallef_investigating_2018,
	series = {{UbiComp} '18},
	title = {Investigating {Login} {Features} in {Smartphone} {Apps}},
	isbn = {978-1-4503-5966-5},
	url = {https://doi.org/10.1145/3267305.3274172},
	doi = {10.1145/3267305.3274172},
	abstract = {Recent revelations about data breaches have heightened users' consciousness about the privacy of their online activity. An often overlooked avenue of collection of users' personal information are registration processes and/or social logins, such as login with Facebook or Google, implemented by smartphone apps. Although the extent of social login implementations on websites has been widely studied, there is negligible research on the extent of implementation of login features on smartphone apps. Hence, this work contributes to further the understanding of smartphone apps ecosystem through investigating whether smartphone apps use login features, and what relationships exist between login features and apps popularity. To address this research gap this work presents the systematic analysis of the publicly available Rico dataset, which contains 72,000 unique UI screen designs that describes smartphone apps design properties for 9,717 Android apps.},
	urldate = {2020-09-06},
	booktitle = {Proceedings of the 2018 {ACM} {International} {Joint} {Conference} and 2018 {International} {Symposium} on {Pervasive} and {Ubiquitous} {Computing} and {Wearable} {Computers}},
	publisher = {Association for Computing Machinery},
	author = {Micallef, Nicholas and Adi, Erwin and Misra, Gaurav},
	month = oct,
	year = {2018},
	note = {event-place: New York, NY, USA},
	keywords = {Apps ecosystem, Logins, Smartphone apps, Social logins},
	pages = {842--851}
}

@article{you_automatic_2020,
	title = {Automatic synthesis of advertising images according to a specified style},
	issn = {2095-9230},
	url = {https://doi.org/10.1631/FITEE.1900367},
	doi = {10.1631/FITEE.1900367},
	abstract = {Images are widely used by companies to advertise their products and promote awareness of their brands. The automatic synthesis of advertising images is challenging because the advertising message must be clearly conveyed while complying with the style required for the product, brand, or target audience. In this study, we proposed a data-driven method to capture individual design attributes and the relationships between elements in advertising images with the aim of automatically synthesizing the input of elements into an advertising image according to a specified style. To achieve this multi-format advertisement design, we created a dataset containing 13 280 advertising images with rich annotations that encompassed the outlines and colors of the elements, in addition to the classes and goals of the advertisements. Using our probabilistic models, users guided the style of synthesized advertisements via additional constraints (e.g., context-based keywords). We applied our method to a variety of design tasks, and the results were evaluated in several perceptual studies, which showed that our method improved users’ satisfaction by 7.1\% compared to designs generated by nonprofessional students, and that more users preferred the coloring results of our designs to those generated by the color harmony model and Colormind.},
	urldate = {2020-09-07},
	journal = {Frontiers of Information Technology \& Electronic Engineering},
	author = {You, Wei-tao and Jiang, Hao and Yang, Zhi-yuan and Yang, Chang-yuan and Sun, Ling-yun},
	month = aug,
	year = {2020}
}

@article{lee_neural_2020,
	title = {Neural {Design} {Network}: {Graphic} {Layout} {Generation} with {Constraints}},
	shorttitle = {Neural {Design} {Network}},
	url = {http://arxiv.org/abs/1912.09421},
	abstract = {Graphic design is essential for visual communication with layouts being fundamental to composing attractive designs. Layout generation differs from pixel-level image synthesis and is unique in terms of the requirement of mutual relations among the desired components. We propose a method for design layout generation that can satisfy user-specified constraints. The proposed neural design network (NDN) consists of three modules. The first module predicts a graph with complete relations from a graph with user-specified relations. The second module generates a layout from the predicted graph. Finally, the third module fine-tunes the predicted layout. Quantitative and qualitative experiments demonstrate that the generated layouts are visually similar to real design layouts. We also construct real designs based on predicted layouts for a better understanding of the visual quality. Finally, we demonstrate a practical application on layout recommendation.},
	urldate = {2020-09-07},
	journal = {arXiv:1912.09421 [cs]},
	author = {Lee, Hsin-Ying and Jiang, Lu and Essa, Irfan and Le, Phuong B. and Gong, Haifeng and Yang, Ming-Hsuan and Yang, Weilong},
	month = jul,
	year = {2020},
	note = {arXiv: 1912.09421},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{shinahara_serif_2019,
	title = {Serif or {Sans}: {Visual} {Font} {Analytics} on {Book} {Covers} and {Online} {Advertisements}},
	shorttitle = {Serif or {Sans}},
	doi = {10.1109/ICDAR.2019.00170},
	abstract = {In this paper, we conduct a large-scale study of font statistics in book covers and online advertisements. Through the statistical study, we try to understand how graphic designers relate fonts and content genres and identify the relationship between font styles, colors, and genres. We propose an automatic approach to extract font information from graphic designs by applying a sequence of character detection, style classification, and clustering techniques to the graphic designs. The extracted font information is accumulated together with genre information, such as romance or business, for further trend analysis. Through our unique empirical study, we show that the collected font statistics reveal interesting trends in terms of how typographic design represents the impression and the atmosphere of the content genres.},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Shinahara, Yuto and Karamatsu, Takuro and Harada, Daisuke and Yamaguchi, Kota and Uchida, Seiichi},
	month = sep,
	year = {2019},
	keywords = {Color, Visualization, image classification, text analysis, feature extraction, advertising data processing, book cover design, book covers, character detection, character recognition, character sets, clustering techniques, collected font statistics, content genres, font information, font statistics, font styles, genre information, graphic designers, graphic designs, Image color analysis, large-scale study, Market research, online advertisement design, online advertisements, pattern clustering, Psychology, statistical study, style classification, Text recognition, typographic design, unique empirical study, visual design analytics, visual font analytics, Web sites},
	pages = {1041--1046}
}

@inproceedings{moran_detecting_2018,
	series = {{ASE} 2018},
	title = {Detecting and summarizing {GUI} changes in evolving mobile apps},
	isbn = {978-1-4503-5937-5},
	url = {https://doi.org/10.1145/3238147.3238203},
	doi = {10.1145/3238147.3238203},
	abstract = {Mobile applications have become a popular software development domain in recent years due in part to a large user base, capable hardware, and accessible platforms. However, mobile developers also face unique challenges, including pressure for frequent releases to keep pace with rapid platform evolution, hardware iteration, and user feedback. Due to this rapid pace of evolution, developers need automated support for documenting the changes made to their apps in order to aid in program comprehension. One of the more challenging types of changes to document in mobile apps are those made to the graphical user interface (GUI) due to its abstract, pixel-based representation. In this paper, we present a fully automated approach, called GCAT, for detecting and summarizing GUI changes during the evolution of mobile apps. GCAT leverages computer vision techniques and natural language generation to accurately and concisely summarize changes made to the GUI of a mobile app between successive commits or releases. We evaluate the performance of our approach in terms of its precision and recall in detecting GUI changes compared to developer specified changes, and investigate the utility of the generated change reports in a controlled user study. Our results indicate that GCAT is capable of accurately detecting and classifying GUI changes - outperforming developers - while providing useful documentation.},
	urldate = {2020-09-07},
	booktitle = {Proceedings of the 33rd {ACM}/{IEEE} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Moran, Kevin and Watson, Cody and Hoskins, John and Purnell, George and Poshyvanyk, Denys},
	month = sep,
	year = {2018},
	note = {event-place: New York, NY, USA},
	keywords = {GUI changes, Android, Mobile Apps, Software Evolution},
	pages = {543--553}
}

@inproceedings{yu_lirat_2019,
	title = {{LIRAT}: {Layout} and {Image} {Recognition} {Driving} {Automated} {Mobile} {Testing} of {Cross}-{Platform}},
	shorttitle = {{LIRAT}},
	doi = {10.1109/ASE.2019.00103},
	abstract = {The fragmentation issue spreads over multiple mobile platforms such as Android, iOS, mobile web, and WeChat, which hinders test scripts from running across platforms. To reduce the cost of adapting scripts for various platforms, some existing tools apply conventional computer vision techniques to replay the same script on multiple platforms. However, because these solutions can hardly identify dynamic or similar widgets. It becomes difficult for engineers to apply them in practice. In this paper, we present an image-driven tool, namely LIRAT, to record and replay test scripts cross platforms, solving the problem of test script cross-platform replay for the first time. LIRAT records screenshots and layouts of the widgets, and leverages image understanding techniques to locate them in the replay process. Based on accurate widget localization, LIRAT supports replaying test scripts across devices and platforms. We employed LIRAT to replay 25 scripts from 5 application across 8 Android devices and 2 iOS devices. The results show that LIRAT can replay 88\% scripts on Android platforms and 60\% on iOS platforms. The demo can be found at: https: //github.com/YSC9848/LIRAT.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Yu, Shengcheng and Fang, Chunrong and Feng, Yang and Zhao, Wenyuan and Chen, Zhenyu},
	month = nov,
	year = {2019},
	keywords = {Layout, Tools, mobile computing, computer vision, Optical character recognition software, Feature extraction, image recognition, Image recognition, computer vision techniques, Android (operating system), Testing, Android platforms, authoring languages, Cross-Platform Testing, dynamic widgets, Image Recognition, image-driven tool, iOS platforms, layout and image recognition driving automated mobile testing, LIRAT, mobile web, multiple mobile platforms, Record and Replay, Smart phones, test script cross-platform replay, test scripts},
	pages = {1066--1069}
}

@inproceedings{xing_convolutional_2019,
	title = {Convolutional {Character} {Networks}},
	doi = {10.1109/ICCV.2019.00922},
	abstract = {Recent progress has been made on developing a unified framework for joint text detection and recognition in natural images, but existing joint models were mostly built on two-stage framework by involving ROI pooling, which can degrade the performance on recognition task. In this work, we propose convolutional character networks, referred as CharNet, which is an one-stage model that can process two tasks simultaneously in one pass. CharNet directly outputs bounding boxes of words and characters, with corresponding character labels. We utilize character as basic element, allowing us to overcome the main difficulty of existing approaches that attempted to optimize text detection jointly with a RNN-based recognition branch. In addition, we develop an iterative character detection approach able to transform the ability of character detection learned from synthetic data to real-world images. These technical improvements result in a simple, compact, yet powerful one-stage model that works reliably on multi-orientation and curved text. We evaluate CharNet on three standard benchmarks, where it consistently outperforms the state-of-the-art approaches [25, 24] by a large margin, e.g., with improvements of 65.33\%→71.08\% (with generic lexicon) on ICDAR 2015, and 54.0\%→69.23\% on Total-Text, on end-to-end text recognition. Code is available at: https://github.com/MalongTech/research-charnet.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Xing, Linjie and Tian, Zhi and Huang, Weilin and Scott, Matthew},
	month = oct,
	year = {2019},
	keywords = {Task analysis, recurrent neural nets, Training, Character recognition, Neural networks, image recognition, Image recognition, Text recognition, character labels, CharNet, convolutional character networks, convolutional neural nets, curved text, end-to-end text recognition, ICDAR 2015, iterative character detection approach, iterative methods, Iterative methods, joint text detection, multiorientation curved text, natural images, one-stage model, RNN-based recognition branch, ROI pooling, text detection, Total-Text, two-stage framework},
	pages = {9125--9135}
}
