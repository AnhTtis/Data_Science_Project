
\section{Model}\label{sec:model}

%We first formally define the problem of video prediction as follows. Given a video stream consisting of continuous video frames, we denote by $\boldsymbol{X}=(X_{t-n+1},...,X_{t-1},X_t)$ the existing $\emph{n}$ video frames, where $ X_i \in \mathbb{R} ^{H \times W\times C} $ is the $\emph{i}$-th frame, $\emph{H}$, $\emph{W}$ and $\emph{C}$ are the height, width and number of channels, respectively. 
We start by formally defining the video prediction problem as follows. Given a stream of $\emph{n}$ continuous video frames, $\boldsymbol{X}=(X_{t-n+1},...,X_{t-1},X_t)$, $ X_i \in \mathbb{R} ^{H \times W\times C} $ denotes the $\emph{i}$-th frame for which $\emph{H}$, $\emph{W}$, and $\emph{C}$ denote the height, width, and number of channels, respectively. 
%
The objective is to predict the next $m$ video frames
$\boldsymbol{Y}=(Y_{t+1},Y_{t+2},...,Y_{t+m})$ using the input
$\boldsymbol{X}$.
%
\begin{equation}
%
 (X_{t-n+1},...,X_{t-1},X_t)\stackrel{predict}{\longrightarrow}
    (Y_{t+1},Y_{t+2},...,Y_{t+m}).
%
\end{equation}
%
Next, we present the dedicated design of \sysname\ for this
task. As depicted in Figure \ref{structure-1}, \sysname\ consists of two main modules, namely, the keypoint detector and the predictor module. 

\subsection{Keypoint Detector}
\label{subsec:detector}
\sysname\ employs a keypoint detector to detect the keypoints that are most likely moving. As illustrated in Figure \ref{structure-detector}, the detector extracts the keypoints as coordinate points. %\textcolor{blue}{Due to space limitations, we only introduce here the abstract representation, the training process and the structure of the encoder and decoder, other details we put in the Supplementary Material.}
Here we describe the abstract representation, training procedure, and the structures of the encoder and the decoder. Please refer to the supplemental material for additional information. 

\vskip 0.1in \noindent\textbf{Abstract representation.}Let $ X,X^{'} \in \boldsymbol{X} $ denote any two frames in $\boldsymbol{X}$, and $X$ referred as the source frame and $X^{'}$
the target frame. The keypoints in a video frame can be represented by ${P}=(p_{1},p_{2},...,p_{K}) \in {\Omega}^K$, where $K$ represents the number of keypoints, $ {\Omega}$ the coordinates. Assume that function $\mathbb{F}$ can extract the keypoints and $\mathbb{G}$ can reconstruct the target frame $X^{'}$ by using $K$ keypoints of $X^{'}$ and the features of the source frame $X$ :
\begin{subequations}
\begin{numcases}{}
\mathbb{F}(X^{'}) = P^{'}   \label{keydete}\\
\mathbb{G}(X;P^{'}) = \hat{X^{'}} \label{reconstrct},
\end{numcases}
\end{subequations}
where $\hat{X^{'}}$ denotes the reconstructed frame. By minimizing the difference between $\hat{X^{'}}$ and $X^{'}$ , the $P^{'}$ obtained by $\mathbb{F}$ represents the different parts between $X$ and $X^{'}$, which become what we call \emph{keypoints}. We use the pixel-wise $L_2$ frame loss to measure the difference between ${X^{'}}$ and $\hat{X^{'}}$ as follows:
\begin{equation}
 L_{rec}=  \Vert X^{'}-\hat{X^{'}} \Vert_2 \label{recloss}.
\end{equation}
%
%\pz{the superscript "2" should be removed? what's "rec"?}  
%It can be found that we do not label $X$ and $X^{'}$, and can learn both fuction $\mathbb{F}$  and $\mathbb{G}$  just using $ L_{rec}$ in \eqref{recloss} so  it is  an end-to-end unsupervised learning process.
$\mathbb{F}$ and $\mathbb{G}$ can be learned using $ L_{rec}$ in an end-to-end unsupervised learning process without labeling $X$ or $X^{'}$.

As shown in Figure \ref{structure-detector}, $\mathbb{F}$ consists of a n-layer CNN encoder $E$, and a coordinate generator $CG$ which converts each heatmap output of $E$ to $p_i^{'}=(p_{ix}^{'},p_{iy}^{'},p_{iv}^{'})$, where $p_i^{'}$ denotes the $i$-th keypoint of $X^{'}$, $(p_{ix}^{'},p_{iy}^{'})$ represents the coordinates of $p_i^{'}$ and $p_{iv}^{'}$ denotes the intensity. $\mathbb{G}$ consists of a heatmap generator $HG$ which converts the $K$ keypoints to a heatmap and a n-layer CNN decoder $D$ which has a symmetrical structure with $E$. %Please refer to the supplementary material for the detailed structures of $CG$ and $HG$.
\vskip 0.1in \noindent\textbf{Encoder and Decoder.} %\textcolor{red}{the paragraph title is encoder but the paragraph also introduces the decoder, some content is overlapped with "Decoder" paragraph and should be removed}
%\textcolor{blue}{ It is clearer to present the encoder and decoder together when introducing the structure of Figure 3}
We compared three structures of the encoder and the decoder. The first structure, as shown in Fig.~\ref{fig:encoder1}, is proposed by Minderer~\etal~\cite{minderer2019unsupervised}, in which the output heatmaps serves both as the generation feature of keypoints and the input feature for the background of the decoder (the heatmaps here corresponds to X in Eq.~\eqref{reconstrct}). Although this structure is simple, it needs a lot of encoder layers and a high feature dimension to extract both key points and background information. Besides, our experimental results show that this structure does not perform well in reconstructing the target frame. The structure in Fig.~\ref{fig:encoder2} uses two networks to extract the keypoints information and the background information separately. While it performs well at information disentangling in our experiments, its structure is too complex and thus requires high computation cost. Therefore, in \sysname\, we design a structure shown in Fig.~\ref{fig:encoder3} adopting the ``skip connection'' proposed by~\cite{ronneberger2015u}, to allow the encoder to disentangle the background information layer by layer and only focus on outputting the keypoints, and synthesize the disentangled background information into the decoder via skip connection. Experimental results show that the structure in Fig.~\ref{fig:encoder3} can reconstruct frames better with a lower computation cost.%, so we choose the structure in Fig.~\ref{fig:encoder3} for the encoder and decoder. \textcolor{red}{Fig.~\ref{fig:encoder2} is not used?  Why keep it? Maybe better remove it.}\textcolor{blue}{The structure in Figure 3(b) is the simplest way to separate keypoints and  background information, and serves as a bridge between the structure Figure 3(a) and Figure 3(c) , as well as providing a comparative experiment for c, showing that the  structure of Figure 3(c) is  simpler and can separate the information at the same time}
\par Let $E_i$ denote the $i$-th layer of $E$ and $h_i \in \mathbb{R} ^{H_i \times W_i\times C_i} $ denote its output heatmap, where $h_1^{'}=E_1(X^{'})$ and each subsequent layer can be expressed as follows:
\begin{equation}
%
E_{i}(h_{i-1}^{'}) = h_{i}^{'}, i\in\{2,n\}.
%
\end{equation}  

\par
The outout of the last layer $h_n^{'}$ is fed into coordinate generator $CG$ to extract the keypoints :
\begin{equation}
%
CG(h_n^{'}) =(p_1^{'},...,p_i^{'},...,p_K^{'}).
%
\end{equation} 
%where $p_i^{'}=(p_{ix}^{'},p_{iy}^{'},p_{iv}^{'})$ and these $K$ keypoints  correspond to $P^{'}$ in the previous section.
\vskip 0.1in \noindent\textbf{Coordinate Generation (CG)}
module converts the heatmap generated by the encoder's last layer to the keypoints. % in the keypoint detector .
%The role of CG is to convert the heatmap into coordinate form.
We use a similar CG structure as in \cite{jakab2018conditional} which first uses a fully connected layer to convert the encoder heatmap $h_n$ from $ \mathbb{R} ^{H_n \times W_n\times C_n}$ into $ \mathbb{R} ^{H_n \times W_n\times K}$, where $K$ refers to the number of keypoints. We do this in the hope of compressing $H_n \times W_n$ into the form of point coordinates in the dimension of $K$. The converted heatmap $h^{'}_n$ can be rewritten as $h^{'}(x;y;i)$, where $x=1,2,...,W_n,y=1,2...,H_n,i=1,2,...,K$, represent the three dimensions of $h^{'}_n $, respectively. Then we can calculate the coordinates of the $k$-th keypiont in width $ p_{ix}$ as follows:
%
\begin{equation}
%
h_n^{'}(x;i)= \frac{\sum_{y} h_n^{'}(x;y;i)}{\sum_{x,y} h_n^{'}(x;y;i)}  , \label{4}
%
\end{equation}
%
\begin{equation}
%
p_{ix}= \sum_{x} h_{x}h_n^{'}(x;i)   ,  \label{5}     
%
\end{equation}
where $h_x$ is a vector of length $W_n$ consisting of values uniformly sampled from -1 to 1 (for example , if $W_n=4$ then $h_x=[-1,-0.333,0.333,1]$). By doing so, we add an axis to the heatmaps at the dimension $W_n$ where $p_{ix}$ is the position of the $i$-th keypoints on the width. Similarly, we can calculate the coordinate in height $ p_{iy} $ by exchanging the position of $x$ and $y$ using Equation~\eqref{4} and Equation~\eqref{5}. We also need the feature values at these coordinates to reconstruct the following frames with keypoints. We express such values with the averages on both the $H_n$ and $W_n$ dimension. We use $p_{iv}$ to represent the value of the $k$-th keypoint:
\begin{equation}
   p_{iv}=\dfrac{1}{H_n \times W_n} \sum_{x,y}h_n(x;y;i).
\end{equation}
As such , we extract the keypoints as $p_i=(p_{ix},p_{iy},p_{iv})$ ,$i=1,2,...,K$.

\vskip 0.1in \noindent
\par Next, the features of $X$ and $P^{'}$ are input to $\mathbb{G}$ as shown in  Eq. \eqref{reconstrct}. The features of $X$, $h_n \in  \mathbb{R} ^{H_n \times W_n\times C_n}$, are obtained via $E$. $P^{'}$ is converted to a heatmap $h_p^{'}$ via $HG$ :
\begin{equation}
%
HG(p_1^{'},...,p_i^{'},...,p_K^{'})=h_p^{'}.
%
\end{equation}

\vskip 0.1in \noindent\textbf{Heatmap Generation (HG)}
module is a reversed process of CG that converts coordinates to the heatmap. We use a 2-D Gaussian distribution to reconstruct the heatmaps. We first convert the coordinates $p_x,p_y$ into 1-D Gaussian vectors, $x_{vec}$ and $y_{vec}$, where $p_x=(p_{1x},p_{2x},...,p_{Kx}),p_y=(p_{1y},p_{2y},...,p_{Ky})$, as follows:
\begin{equation}
x_{vec}=exp(-\frac{1}{2\sigma_{2}}\Vert p_x - \bar{p}_x \Vert^{2}),
\end{equation}
\begin{equation}
y_{vec}=exp(-\frac{1}{2\sigma_{2}}\Vert p_y - \bar{p}_y \Vert^{2}),
\end{equation}
where $\bar{p}_x$ and $\bar{p}_y$ are the expectations of $p_x$ and $p_y$, respectively. By multiplying $x_{vec}$ and $y_{vec}$ we can get the 2-D Gaussian maps $G\_maps$ as follows:
 \begin{equation}
    G\_maps=x_{vec} \times y_{vec}.
 \end{equation}
Finally we calculate the Hadamard product of $G\_maps$ and $p_v$ to get the $h^{'}_p$:
  \begin{equation}
    h^{'}_p=G\_maps \circ p_v .
 \end{equation}
We align the dimension of $h_p^{'}$ with $h_n$ to allow their direct concatenation sent to the decoder for reconstruction.
As mentioned, inspired by the ``skip connetion'' in UNet~\cite{ronneberger2015u} and Ladder Net~\cite{rasmus2015semi}, which can reconstruct images better with fewer encoder and decoder layers, we input the heatmaps $h_1,h_2,..h_n$ obtained by each encoder layer to the decoder through ``skip connection''. Let $D_i$ denote the $i$-th decoder layer and $d_i$ denote its output heatmap, where  $d_1=D_1(concat(h_p^{'},h_n))$ and each subsequent layer can be expressed as follows:
\begin{equation}
d_{i}=D_{i}(concat(d_{i-1},h_{n-i+1})), i \in \{2,n\} ,
\end{equation}
where $d_n$ is $\hat{X^{'}}$ . In this manner, the decoder learns the ``background'' (i.e., the static information) features eliminated by the encoder, thus improving the higher level representation details of the model. The additional ``background'' information also allows the encoder to focus more on the keypoints. %\textcolor{blue}{Compared with~\cite{minderer2019unsupervised}, \sysname\ has fewer encoder and decoder layers and requires fewer keypoints for frame reconstruction.}

\begin{figure}[t!]
\centering
\includegraphics[width=4cm]{figures/transformer.png}
\caption{The structure of transformer encoder. Embedded keypoints are the explicit keypoints coordinates mapping to the high-dimensional implicit representations combined with temporal position encoding.}
\label{fig:transformer}
\end{figure}
\subsection{Predictor}

The original prediction task is transformed, via the keypoint detector's encoding, into predicting the subsequent $m$  groups of keypoints $P_{t+1},...,P_{t+m}$, based on the prior $n$ groups of keypoints $P_{t-n+1},...,P_{t}$. We select Transformer as the predictor due to the benefits explained in the Related Work. Our experiments revealed that utilizing only the transformer's encoder to encode the temporal relationship between keypoints yields better prediction results to using the transformer's entire structure, and at a faster rate. Therefore, we opted to use only the encoder part. %\textcolor{red}{R1C2}
%We use only the encoder part of the transformer as in Bert \cite{devlin2018bert}.
\vskip 0.1in \noindent\textbf{Transformer encoder.}
We use the transformer encoder structure from \cite{vaswani2017attention} as shown in Figure~\ref{fig:transformer}. The transformer encoder uses a self-attention mechanism that calculates the relationship between the current and other inputs. In this paper, we use the transformer encoder to calculate how the keypoints of the current frame are composed of the previous keypoints. Self-attention is a function that maps a query (Q) and a set of key-value (K,V) pairs to an output.% Note that the query(q), key(k) and value(v) mentioned here are not the same things as in our paper.
\par
Each input corresponds to a vector query of length $d_q$, a vector key of length $d_k$, and a vector value of length $d_v$. Suppose we have $l$ inputs denoted as $I \in \mathbb{R}^{l \times d_{model}}$, each of which has a dimension of $d_{model}$ as mentioned in the paper, there are $l$ query, key, and value vectors dentoed as $Q \in \mathbb{R}^{l \times d_k}$, $K \in \mathbb{R}^{l \times d_k}$, and $V \in \mathbb{R}^{l \times d_v}$, respectively. We use the fully connected layer to convert the inputs to Q, K and V:
\begin{equation}
\left\{
%
\begin{array}{rcl}
%
Q=IW^{Q}\\
K=IW^{K}\\
V=IW^{V} ,
%
\end{array}
%
\right.
%   
\end{equation}
where $W^{Q} \in \mathbb{R}^{d_{model} \times d_k}$, $W^{K} \in \mathbb{R}^{d_{model} \times d_k}$, and $W^{V} \in \mathbb{R}^{d_{model} \times d_v}$ are the conversion matrices. The self-attention of these matrices can be expressed as follows:
\begin{equation}
    Attention(Q,K,V) = softmax(\frac{QK^{T}}{\sqrt{d_k}})V  \label{attention},
\end{equation}
where $\sqrt{d_k}$ is used to prevent the gradient disappearance when the Q and K dot product is too big. In fact, we use multi-head self-attention mechanisms for better performance with more parameters. Each head refers a self-attention in Eq.~\eqref{attention}. We concatenate all the heads together and pass them through a fully connected layer to get the result of the same dimension with the input $I$,
\begin{equation}
    Multihead(Q,K,V)=concat(head_1,...,head_n)W^{h},
\end{equation}
where $head_i= Attention(IW^Q_i,IW^K_i,IW^K_i), i=1,2,...,n$, $W^{h} \in  \mathbb{R}^{ nd_k \times d_{model}}$. We add the result to the input $I$ by residual connection and then normalize them  by LayerNorm to solve the  gradient disappearance:
\begin{equation}
    x=LayerNorm(I+Multihead(Q,K,V)) .  \label{layernorm}
\end{equation}
Considering that the self-attention mechanism may not fit complex processes well, the model is enhanced by adding two  fully connected  layers with a ReLU activation in between called  Feed-Forward Network:
\begin{equation}
    FFN(x)=max(0,xW_1+b_1)W_2+b_2 ,
\end{equation}
Then we use Eq.~\eqref{layernorm} again to get the final result. We can either output the result directly or send it as input to the Transformer encoder for multiple iterations.



\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/tkn_fbf.pdf}
\caption{Detailed structure of TKN-Sequential. It uses the same keypoint detector and predictor structures with TKN but has a different prediction process. Particularly, it uses the previous predicted frame's background as the following one's to ensure background consistency.}
\label{fig:tkn-long2}
\end{figure*}

The transformer encoder requires the input to be a one-dimensional vector with a length of $d_{model}$, e.g., $(512,768,1024)$. 
%However, video prediction methods generally take the features obtained from downsampled frames as the input, which is a three-dimensional tensor $ \mathbb{R} ^{H_n \times W_n \times C_n}$. The tensor size is usually up to tens of thousands thus quite difficult to be input into transformer. To address this challenge, we employ the keypoint detector as described in~Section~\ref{subsec:detector} to encode the keypoints as K simple triples of $(p_{ki},p_{kj},p_{ks})$ with the size of only $3 \times K$. Hence the encoded data can be easily input to the transformer via following steps. 
Hence we first convert the K keypoint-triples $\{(p_{ix},p_{iy},p_{iv})|i=1,..,K\}$ to a one-dimensional vector $\bar{P}$:  
$$
P=
\left\{
\begin{array}{c}
p_1 = (p_{1x},p_{1y},p_{1v})   \\
p_2 = (p_{2x},p_{2y},p_{2v})   \\
 \vdots \notag         \\    
p_K = (p_{Kx},p_{Ky},p_{Kv})  \\ 
\end{array}
\right. 
$$
$$\downarrow$$
\begin{equation}
\bar{P} = (p_{1x},p_{1y},p_{1v},...,p_{Kx},p_{Ky},p_{Kv}).
\end{equation}

\begin{figure} [t]
	\centering
	\subfloat[\label{fig:model1}]{
		\includegraphics[width=\linewidth]{figures/new_model1.pdf}}
    \
    \subfloat[\label{fig:model3}]{
		\includegraphics[width=\linewidth]{figures/new_model3.pdf} }
	\caption{ Structure comparison of TKN and TKN-Sequential. (a)  
TKN is a parallel prediction scheme that inputs a batch of frames simultaneously and predicts a batch of frames simultaneously. It uses one transformer encoder to predict t keypoints via the background reduction of the t-th frame.
(b)TKN-Sequential is a sequential prediction scheme that makes prediction frame by frame. It uses one transformer encoder per predicted frame via the background information from each previous frame.}
	\label{fig:tkns} 
\end{figure}
\begin{comment}
\begin{figure}[t!]
\centering
\subfigure[TKN]{
\includegraphics[width=\linewidth]{figures/new_model1.pdf} 
\label{fig:tkn}
}

\subfigure[TKN-Long]{
\includegraphics[width=\linewidth]{figures/new_model3.pdf} 
\label{fig:long}
}
%\vspace{-0.15in}
\caption{Structure comparison of TKN and TKN-Long.  
TKN uses one transformer encoder to predict t keypoints via the background reduction of the t-th frame. 
TKN-Long uses one transformer encoder per predicted frame via the background information from each previous frame.  
}
\label{fig:tkns}
%\vspace{-0.15in}
\end{figure}
\end{comment}
$\bar{P}$ represents the low-dimensional \textit{explicit} spatial coordinates. Inputting $\bar{P}$ directly into the transformer necessitates adjusting the dimension of the intermediate parameter values according to $K$ in each prediction instance. Consequently, training and testing would become difficult.
%而且P表示的是空间位置的低维显式坐标，目前已经有大量工作（比如nerf）表明了了高维隐示表达在处理复杂问题时的好处，所以我们基于这些问题，使用一层MLP将P转换位固定长度的高维隐示空间特征。
Moreover, many works~\cite{meng2018zero,tao2020latent,zhou2021latent} have demonstrated the benefits of \textit{latent} representations. % facing complex problems\textcolor{blue}{
We also found latent representations well capture the regularity of keypoints over time in our experiments. %Unlike~\cite{mildenhall2021nerf,kasten2021layered,bar2022text2live} which use network parameters as implicit representations, 
Therefore, we use a matrix to map the explicit coordinates representation to a latent space to obtain a high-dimensional latent representation vector. Specifically, we convert the variable-length and low-dimensional $\bar{P}$ to a fixed-length and high-dimensional $Q$ by converting $\mathbb{R}^{3K}$ to $\mathbb{R}^{d_{model}}$ via a mapping matrix $W$, where $W \in \mathbb{R}^{d_{model} \times 3K }$, as follows:
\begin{equation}
    Q = W \cdot \bar{P} .
\end{equation}
 To compensate the lack of time-sensitive capability, we manually add location information position embedding (PE) to $Q$, 
\begin{equation}
   Q_{input} = concat(Q_{t-n+1},...,Q_{t}) + PE , \label{Qinput}
\end{equation} 
where PE is the trigonometric function as defined in \cite{vaswani2017attention}:
\begin{equation}
\left\{
\begin{array}{rcl}
PE_{pos,2i} = \sin{\dfrac{pos}{10000^{\frac{2i}{d_{model}}}}}  \\ 
PE_{pos,2i+1} = \cos{\dfrac{pos}{10000^{\frac{2i}{d_{model}}}}}   ,
\end{array}
\right. 
\end{equation}
where $pos$ and $i$ represent the sequential order of the input and the $i$-th element of the input. The number of input sequences and output sequences are equal for the transformer encoder. Therefore, we can use transformer encoder to get the predictions $Q_{t+1}^{'},...,Q_{2t}^{'}$ using the input $Q_{input}$:
\begin{equation}
Q_{t+1}^{'},...,Q_{2t}^{'} = Trans\_encoder(Q_{input}).
\end{equation}

Finally, we use an invert mapping matrix $ W^{'} \in  \mathbb{R}^{3K \times d_{model}} $ to reconstruct the high-dimensional sequence $Q_{pred}=(Q_{t+1}^{'},...,Q_{2t}^{'})$ back to the low-dimensional keypoints spatio-temporal sequence $P_{pred}$, which is then input to the decoder to generate the predicted frames:
\begin{equation}
    P_{pred} = W^{'} \cdot Q_{pred}.
\end{equation}
We only need to calculate the loss of sequence $P_{real}=(\bar{P}_{t+1},...,\bar{P}_{2t})$ which is output by $X_{t+1},...,X_{2t}$ using keypoint detector and $P_{pred}$ to complete the training of the predictor using a well-trained keypoint detector, for which we also use the $L_2$ loss:
\begin{equation}
L_{pred}= \Vert P_{real} -P_{pred} \Vert_2  \label{predloss}.
\end{equation}


%We noticed via empirical evaluations that the performance of a single transformer encoder was limited regardless of the encoder depth and the number of heads. Therefore, we choose to use multiple encoders each of which predicts one output. This structure increases the prediction accuracy significantly with negligible reduction of the prediction speed. Let  $\boldsymbol{Q}=(Q_{t-n+1},...,Q_{t})$ denote the transformer input and $(Q_{t+1},Q_{t+2}..  Q_{t+m})$ the prediction target. Assuming we have successfully predicted $(Q_{t+1},...,\hat{Q}_{t+i}) $, the next $\hat{Q}_{t+i+1}$ can be denoted as:


%



\subsection{Prediction Processes}\label{subsec:prediction}
%我们之前讲了，我们把以前方法类似于循环网络结构的预测流程称为逐帧预测，我们的称之为多帧预测，我们并行提取特征是为了追求预测时的推理速度，追求速度方法往往会带来精确度方面的下降，所以我们将我们的主要结构不做变化，仅仅将多帧预测流程变成逐帧预测的流程，对比图如图所示。这两种流程最大的区别是，在多帧预测流程中，我们使用输入帧最后一帧的背景，作为其他预测帧的生成，而逐帧预测流程中，是用相邻帧的背景，作为下一帧的生成，这里需要注意即便是时间上非常近的两帧，提取到的背景信息也不是完全一样的，所以并不会出现每次用相同的背景信息作为预测帧的合成，否则就和多帧预测没区别了。
%\textcolor{red}{As we mentioned before, we call the prediction process of the previous method , which similar to the recurrent neural network structure frame-by-frame prediction, ours is called multi-frame prediction. We extract features in parallel in order to pursue the speed of inference when predicting, and usually the pursuit of speed methods often brings about a decrease in accuracy, so for comparison we leave our main structure unchanged and simply turn the multi-frame prediction process into a frame-by-frame prediction process, and the comparison is shown in \ref{fig:tkns}. It use the background information of the last frame of the input frames as the generation of the other prediction frames in the multi-frame prediction process . However in the frame-by-frame prediction process ,it uses the background information of a newly predicted frame as the following target's background.}
%\begin{comment}
    
Sequential prediction is time consuming. Since most subsequent frames in high frame-rate videos are fairly similar, we can use the background of the frame immediately just before the prediction target as the background of the prediction target frame. We can then combine the predicted keypoints with the background to generate the integrated predicted frames. As illustrated in Fig.~\ref{structure-1}, we integrate the $P_{pred}$ generated by Transformer encoder to the background of t-th frame and directly generate the subsequent t prediction frames $X_{t+1}^{'},X_{t+2}^{'},...,X_{2t}^{'}$. This parallel prediction mechanism, i.e., TKN, can input and predict multiple frames as batches to significantly accelerate the prediction process. %Note that the ``background'' in this context also refers to the parts that do not change between frames.

%Specifically, as Figure~\ref{fig:tkn} depicts, \emph{TKN} employs a multi-head encoder which converts prior $T$ keypoints $\{p_1,p_2,...,p_T\}$ to future $T$ keypoints $\{p_{T+1}, p_{T+2}, ..., p_{2T}\}$, and uses the background of frame $X_T$ as that of the frames $\{X_{T+1}, X_{T+2}, ..., X_{2T}\}$. 

We reason that frame-by-frame prediction structure has higher accuracy to predict frames with frequent changes (as proved by experiment results). Hence we also provide a sequential variation of TKN, TKN-Sequential, which uses the previous predicted frame's background as the following one's to ensure background consistency. Fig.~\ref{fig:tkn-long2} shows the detailed structure of \sysname-Sequential amd Fig.~\ref{fig:tkns} depicts its comparison with TKN.

Since the output of the transformer encoder has the same length as the input sequence, we take the averaged predictor's output as the predicted frame. 
%\pz{the motivation is not clear, why output has same length with input justify using the avg?}. %\textcolor{red}{The output of each step of TKN Long is predicted one frame, but the transformer encoder is N input N output at a time, so you need to compress the N outputs into one output, and here the avg value is used to take one output} .
Suppose we have predicted $i$ frames, $Q_{t+1}^{'},...,Q_{t+i}^{'}$, then $ Q_{t+i+1}^{'}$ can be expressed as:
\begin{equation}
    Q_{t+i+1} ^{'} = \dfrac{1}{t+i}\sum_{j=1}^{t+i}Trans\_encoder(Q_{input};j) ,
\end{equation}
where $Q_{input}=concat(Q_{t-n+1},...,Q_{t},Q^{'}_{t+1},...,Q_{t+i}^{'}) + PE$ according to \eqref{Qinput}.

 $X^{'}_{t+1}$ is the combination of predicted $\bar{P}^{'}_{t+1}$ and the background information of $X_{t}$ extracted by the decoder. Note that  ``$\bar{P}^{''}_{t+1}$ and the background information of $X^{'}_{t+1}$'' are not equal to ``$\bar{P}^{'}_{t+1}$ and the background information of $X_{t}$'', albeit they are both extracted by the encoder from the $X^{'}_{t+1}$. It is because two consecutive frames are very similar but still have some minor differences in the keypoints and background information, otherwise it would be no different from multi-frame prediction process.
