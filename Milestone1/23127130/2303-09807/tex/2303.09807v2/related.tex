%
%
\section{Related Works}
\label{sec:related}

Unsupervised methods can reduce the cost of manual annotation which is a common requirement for video datasets.

\noindent\textbf{Unsupervised keypoint learning.}
Due to the similarity of pixels in consecutive video frames, the keypoints in each frame can be learned via unsupervised reconstruction of the other frames. Jakab~\etal~\cite{jakab2018conditional} propose to learn the object landmarks via conditional image generation and representation space shaping.
%
%Minderer~\etal~\cite{minderer2019unsupervised} adopted stochastic dynamics learning and applied keypoints to video prediction for the first time. It greatly reduces the computational complexity and enables real-time prediction. 
Minderer~\etal~\cite{minderer2019unsupervised} introduce keypoints to video prediction using stochastic dynamics learning for the first time, which drastically reduces computational complexity.
%
Gao~\etal~\cite{gao2021accurate} applied grids on top of~\cite{minderer2019unsupervised} for a clearer expression of the keypoint distribution.

\noindent\textbf{Unsupervised video prediction}
uses the pixel values of the video frames as the labels for unsupervised prediction. Existing studies can be classified into two categories, as shown by Fig.~\ref{fig:otherstrutures}. 
%
The first category of works focuses on improving the performance of the well-known RNN by adapting the intermediate recurrent structure~\cite{wang2018eidetic,wang2021predrnn,oliu2018folded,wang2018predrnn++,castrejon2019improved}. For example, E3D-LSTM~\cite{wang2018eidetic} integrates 3DCNN with LSTM to extract short-term dependent representations and motion features. PredRNN~\cite{wang2021predrnn} enables the cross-level communication for the learned visual dynamics by propagating the memory flow in both bottom-up and top-down orientations. 
The second category focuses on disentangling the dynamic objects and the static background in the video frames, mostly by adapting the CNN structure~\cite{ying2018better,guen2020disentangling,denton2017unsupervised,blattmann2021understanding,xu2019unsupervised}.
%
For instance, DGGAN~\cite{ying2018better} trains a multi-stage generative network for prediction guided by synthetic inter-frame difference. PhyDNet~\cite{guen2020disentangling} uses a latent space to untangle physical dynamics from residual information. %Our work can also be classified in the second category.

The methods in both categories use so-called ``sequential prediction'', that is, using the previous prediction frame as the input frame for the next round of prediction. The prediction speed is proportional to the number of frames to be predicted and thus leads to an intolerably long delay for long-term prediction. 
Therefore, we propose a parallel prediction scheme, as shown in Fig.~\ref{fig:mystructure}, to extract the features of multiple frames and output multiple predicted frames in parallel, which greatly accelerates the prediction process.
%\par \textcolor{red}{The prediction process of both categories of methods can mostly be represented by the \ref{fig:otherstrutures}. For the first category of methods , they focus on improving the intermediate recurrent structure, while the second category of methods mostly change the CNN structure to better disentangle the static and dynamic information in videos. They both use the prediction frame as the next input frame for the next round of prediction and this prediction process can be called frame-by-frame prediction. Obviously, this prediction inference time is T times that of a single prediction process time, and as mentioned before, if the network structure is particularly complex, the time of a single prediction process is very long, so it is impossible to predict long video sequences in a very short time. Therefore, we propose a multi-frame prediction process, as shown in \ref{fig:mystructure} , which can extract the features of  video frames at the same time, and output the predicted T frames at the same time, which can greatly reduce the prediction time.}
%目前的方法按照结构可以分为两种，这两类方法的预测流程大都可以用图1中a的来表示，对于第一类方法是改进中间层的预测结构，而第二类方法多是改变输入和输出的CNN结构。他们流程都是使用预测帧作为下一输入帧从而进行下一轮的预测，这种预测流程我们可以称之为逐帧预测，很明显，这种预测推理时间是单次预测的T倍，如之前所说，如果网络结构特别复杂，导致单次预测流程的时间都很长，这样是无法做到极短时间内预测较长视频序列。所以我们提出了一个多帧预测的流程，可以同时提取不同时间点视频帧的特征，并且输出时也是同时输出预测T帧，通过这种方式就可以极大缩短预测的时间。
\begin{figure} [t!]
	\centering
	\subfloat[\label{fig:otherstrutures}]{
		\includegraphics[width=\columnwidth]{figures/otherstructure.png}}
	\\
	\subfloat[\label{fig:mystructure}]{
		\includegraphics[width=\columnwidth]{figures/mystructure.png} }

	\caption{(a) The sequential prediction scheme generally takes a long time to predict frames due to the sequential scheme. (b) The parallel prediction scheme we propose can greatly accelerate the prediction speed.}%represents the majority of unsupervised video prediction processes, which can only be performed frame by frame and can not  predict  long periods of  a short time. (b) is a multi-frame prediction process, which saves large amount of time in the feature extraction and output stages compared to (a).}
	\label{fig3} 
\end{figure}


\vskip 0.1in \noindent\textbf{Transformer} 
%\label{subsec:transformer}
has been utilized extensively in NLP due to its benefits over RNN in feature extraction and long-range feature capture. It monitors global attention to prevent the loss of prior knowledge which often occurs with RNN. Its parallel processing capacity can significantly accelerate the process.
%
Recently, the field of computer vision has begun to explore its potential and produced positive results~\cite{dosovitskiy2020image,liu2021swin,liu2021video,arnab2021vivit,liang2021swinir}. Most related works input segmented patches of images to the transformer to calculate inter-patch attention and obtain the features.
There are also a number of vision transformer (VIT) approaches applied to video analysis. For example, VIVIT~\cite{arnab2021vivit} proposes four different video transformer structures to solve video classification problems using the spatio-temporal attention mechanism. \cite{liu2022video} applies the swin transformer structure to video and uses an inductive bias of locality. In this paper we select CNN as the feature extractor instead of the VIT structure because of the huge computational cost of VIT compared to CNN. We select the transformer structure as the predictor because it outperformed RNN, mix-mlp, and other structures, in terms of predicting spatio-temporal features in our empirical experiments.
\vskip 0.1in Most of the aforementioned video prediction methods extract from each frame complex features, typically of tens of thousands of bytes~\cite{shi2015convolutional,wang2018eidetic,guen2020disentangling,akan2021slamp}, resulting in excessive numbers of floating point operations in both the feature extraction module and the prediction module. Moreover, they employ sequential (frame-by-frame) prediction process. Hence, both training and testing consume a great deal of time and memory. In the meanwhile, many videos, particularly human activity records, have a significant amount of background redundancy~\cite{schuldt2004recognizing,h36m_pami} that can be removed by extracting information only from the key motions. 
Therefore, in this work, we try to couple the unique advantages of the transformer and the keypoint-based prediction methods to maximize their benefits.
%
%Note that the commonly adopted feature extraction based methods normally involve complex data distribution, for which transformer requires massive training data and well-trained or pre-trained model. However, keypoint prediction is a much easier and faster task for transformer thanks to its simple and sparse data distribution.
\begin{figure}[t]  
\centering
\includegraphics[width=9cm]{figures/detector.pdf} 
%\vspace{-0.3in}
\caption{Detailed structure of Keypoint Detector}       %对图进行说明 
\label{structure-detector}  
%\vspace{-0.1in}
\end{figure}


\begin{figure} [t]
	\centering
	\subfloat[\label{fig:encoder1}]{
		\includegraphics[width=4.5cm]{figures/encoder1.png}}
	\subfloat[\label{fig:encoder2}]{
		\includegraphics[width=4cm]{figures/encoder2.png} }
    \
    \subfloat[\label{fig:encoder3}]{
		\includegraphics[width=7cm]{figures/encoder3.png} }
	\caption{ Comparison of three different encoder and decoder structures. (a) The structure proposed by \cite{minderer2019unsupervised}  requires more network layers while performing poorly at disentangling keypoints and background information. (b) A structure that can well disentangle keypoints and background information at the cost of complex network architecture and high computation cost. (c) We adopt the well-known skip connection to achieve good performance on information disentangling with simple structure.}
	\label{fig:encoder_structure} 
\end{figure}

\begin{figure*}[t]  
\centering
\includegraphics[width=\textwidth]{figures/TKN.pdf} 
%\vspace{-0.15in}
\caption{Detailed structure of TKN. Two main modules are the Keypoint Detector and the Predictor marked with the red dashed lines. The predicted frame uses the background information extracted from the last frame of the input. Both the inputting stage and prediction stage allow batch processing (e.g., input multiple frames simultaneously) and thus enable temporal parallelism. Note that the ground truth keypoints information, $P_{real}=(\bar{P}_{t+1},...,\bar{P}_{2t})$, is output by $X_{t+1},...,X_{2t}$ using keypoint detector (excluded from the figure for simplicity).}       %对图进行说明 
\label{structure-1}  
%\vspace{-0.1in}
\end{figure*}
