
\section{Experimental Setup}


\label{sec:setup}
\vskip 0.1in \noindent\textbf{Dataset.}
\begin{table*}[t!]
\small
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c}  \hline
         Dataset&Method& SSIM$\uparrow$  & PSNR $\uparrow$  & TIME (s)$\downarrow$& TIME (ms)$\downarrow$& FPS$\uparrow$ &Memory (MB)$\downarrow$ & Memory (MB)$\downarrow$ \\   
                 &    &      &      & (train)& (test)& (test)&(train) &(test)    \\         \cline{1-9}
     \multirow{9}{*}{KTH}& ConvLSTM &0.712*  & 23.58* &   61 & 72&278&    8,055 & 1,779             \\ 
      &  PredRNN & 0.839*  & 27.55* &   204 & 184  &109&    6,477& 1,721              \\  
     &  PredRNNv2&0.838*  &  28.37* &   246 &  222 &    90&8,307  & 1,779              \\
      & PhyDNet& 0.854  & 26.9  &   108  &  240 &83&    8,491 &  2,704             \\
      &  SLAMP&     0.864*(30)  &   28.72(30)  &465 & 388&52&   21,103(16)  &  2,295   \\  
    &    E3D-LSTM&  \textbf{0.879}* & \textbf{29.31}* &   879 &   338 &59&   21,723(16)  & 2,687              \\  \cline{2-9}
    & Grid-Keypoint&  0.837* & 27.11*  & 145 &  252 & 79&  12,661 & 2,259              \\ 
     & Struct-VRNN&  0.766* & 24.29* & 111 & 151  &  132& 5,661  &   1,817            \\  \cline{2-9}
   &    \textbf{TKN (w/o tp)}  &0.871 & 27.71  & \textbf{35}  &  86  &  233 & \textbf{ 3,777}  & \textbf{1,447}              \\   
   &    \textbf{TKN-Sequential} &  0.862 & 27.73 & 44  &   154  &130  &  6,309  &  1,785         \\ 
   &    \textbf{TKN}  &0.871 & 27.71 & \textbf{35}  &  \textbf{17}  &  \textbf{1,176} & 4,945  & 1,705              \\\hline
   \multirow{7}{*}{Human3.6}& ConvLSTM &   0.776*& - &  63      &  32 & 125 &  6,561&   1,857        \\  
   &    PredRNN &    0.781* & - & 462     &   47&  85 &   5,829   &  1,743             \\   
    &   E3D-LSTM &    0.869*  &  - &  3154 &    167&  24&    18,819(8)    &  5,767             \\   
    &   PhyDNet &    0.901* &  -& 207  &  88   & 45 &      12,213&  2,353             \\     \cline{2-9}
    &  Grid-Keypoint &   0.928   & 28.76&  114  &  106& 38 &      9,891     &2,003      \\
& Struct-VRNN&  0.916  &  26.97 &   67  &  41   &   98    &   5,015     &    1,962      \\   \cline{2-9}
  &     \textbf{TKN (w/o tp)} &  \textbf{0.958}  &  \textbf{30.89}   &    \textbf{63}&   30  & 133&  \textbf{2,179}    & \textbf{1,521}              \\   
  &   \textbf{TKN-Sequential} &   0.946 & 29.56  &    75 & 35 &114  &     2,653          &   1,763            \\  
   &\textbf{TKN} &  \textbf{0.958}  &  \textbf{30.89}   &    64&   \textbf{11}  & \textbf{364} &  2,561    & 1,587              \\   \hline
\end{tabular}
\caption{The results on KTH and Huamn3.6. $\uparrow$ means the higher the better and $\downarrow$ means the less the better. We skipped some tests due to the lack of original code. Instead, we used the results provided by the original papers (indicated by ``*''), or skipped if the papers didn't provide results (indicated by ``-''). ``w/o tp'' means without temporal parallel and ``seq'' means sequential. We used 32 and 16 as the default batch sizes for KTH and Human3.6, but 16 and 8 for a few exceptions which otherwise exceeded the GPU capacity due to too many intermediate results generated by the algorithms. (30) indicates using 10 input frames to predict 30 frames with SLAMP. %\pz{why (30)}.  \textcolor{red}{ Because the results of their code run on git are very poor, we quote the results from their paper,Only 10 predictions of 30 in his paper}. 
\textbf{Struct-VRNN} and \textbf{Grid-Keypoint} are Keypoint-based baselines.}
\label{tab:1}
\end{table*}
We used two real action datasets, KTH~\cite{schuldt2004recognizing} and Human3.6~\cite{h36m_pami}, to verify the real-time and efficient performance of the proposal under different patterns.
\begin{comment}


\par KTH dataset includes 6 types of movements (walking, jogging, running, boxing, hand waving, and hand clapping) performed by 25 people in 4 different scenarios, for a total of 2391 video samples. The database contains scale variations, clothing variations, and lighting variations. We use people 1–16 for training and 17-25 for testing. Each image is converted to the shape of $(64, 64, 3)$.
\par Human3.6 dataset contains 3.6 million 3D human poses performed by 11 professional actors in 17 scenarios (discussion, smoking, taking photos and so on). We use scenario 1, 5, 6, 7, and 8 for training and 9 and 11 for testing. Each image is converted to the shape of $(128, 128, 3)$.
\end{comment}

\begin{itemize}
    \item KTH dataset includes 6 types of movements (walking, jogging, running, boxing, hand waving, and hand clapping) performed by 25 people in 4 different scenarios, for a total of 2391 video samples. The database contains scale variations, clothing variations, and lighting variations. We use people 1–16 for training and 17-25 for testing. Each image is converted to the shape of $(64, 64, 3)$.
    \item Human3.6 dataset contains 3.6 million 3D human poses performed by 11 professional actors in 17 scenarios (discussion, smoking, taking photos and so on). We use scenario 1, 5, 6, 7, and 8 for training and 9 and 11 for testing. Each image is converted to the shape of $(128, 128, 3)$.
\end{itemize} 
%Please refer to the supplementary material for the details of dataset processing.
%\par We put the processing of the dataset in the Supplementary Material.
\vskip 0.1in \noindent\textbf{Implementation.}
The experiments were run on a server equipped with an Nvidia GeForce RTX 3090 GPU. 
We conducted a two-step training: first we trained the keypoint detector using $L_{rec}$ in Eq. \eqref{recloss} and then froze its parameters, then we trained the predictor using $L_{pred}$ in \eqref{predloss}. We found that this method trained faster than the end-to-end training which trained the keypoint detector and predictor together using both $L_{rec}$ and $L_{pred}$. %Please refer to the supplementary material for the detailed experimental structure.
%We used the loss image in keypoint detector training, and the loss image plus the loss keypoints in end-to-end training, with the $\lambda$ taken as 0.1. We put the detailed experimental structure in the supplementary files.
\vskip 0.1in \noindent\textbf{Model structures.}
TKN and TKN-Sequential have the same keypoint detector structure, which has a 6-layer encoder and a 6-layer decoder. Each encoder layer includes Conv2D, GroupNorm, and LeakyRelu. Each decoder layer includes TransposedConv2D, GroupNorm and LeakyRelu. Since the skip connection is used between encoder and decoder, the input dimension of each decoder layer is twice the output dimension of the corresponding encoder layer.
\par 
For the predictor, TKN uses a 6-layer transformer encoder with the input sequence length of 10, and TKN-Sequential uses 10 single-layer transformer encoders, each with an input sequence length of 10, 11, ..., 19. As mentioned in Section Prediction Processes, each transformer encoder's output of TKN-Sequential is averaged according to the input length, hence the output of both TKN and TKN-Sequential has a length of 10. All transformer encoders employed by the baselines share the same parameters: $d_k=d_v=64,d_{model}=512,d_{inner}=2048,n_{head}=8,dropout=0$
\vskip 0.1in \noindent\textbf{Evaluation metrics.}
Traditional evaluation metrics include structural similarity~(SSIM) and peak signal to noise ratio~(PSNR).
%
Higher SSIM indicates a higher similarity between the predicted image and the real image. Higher PSNR indicates better quality of the reconstructed image. 
%
We also quantify the resource (time and memory) consumption, for which a uniform batch size of 32 and 1 are used for KTH dataset during training and testing, and 16 and 1 are used for Human3.6 dataset during training and testing.

In addition, we measure the FLOPs (floating-point operations per second) to assess the computational cost and the number of parameters of the model with the thop\footnote{\url{https://pypi.org/project/thop/}} package. %\textcolor{red}{R1C3}



\vskip 0.1in \noindent\textbf{Baselines.}
To validate the performance of TKN, we select 8 most classical and effective SOTA methods as the baselines, all of which are implemented with Pytorch for fair comparisons.
%
%
The 8 baselines include:  ConvLSTM\cite{shi2015convolutional},  Struct-VRNN\cite{minderer2019unsupervised}, Grid-Keypoint \cite{gao2021accurate}, Predrnn\cite{wang2017predrnn}, Predennv2\cite{wang2021predrnn}, PhyDNet\cite{guen2020disentangling}, E3D-LSTM\cite{wang2018eidetic}, SLAMP\cite{akan2021slamp}.
\begin{enumerate}
\item  ConvLSTM\cite{shi2015convolutional} is one of the oldest and most classic video prediction method based on LSTM. 
\item Struct-VRNN\cite{minderer2019unsupervised} is the first one to use keypoints to make prediction. 
\item Grid-Keypoint\cite{gao2021accurate} is a grid-based keypoint video prediction method.
\item Predrnn\cite{wang2017predrnn} is a classic prediction method adapted from LSTM.
\item Predennv2\cite{wang2021predrnn} can be generalized to most predictive learning scenarios by improving PredRNN with a new curriculum learning strategy. %We can not train it on Human3.6, so we  only use it on the KTH dataset.
\item PhyDNet\cite{guen2020disentangling} disentangles the dynamic objects and the static background in the video frames. % that achieves the best performance on the Human3.6.
\item E3D-LSTM\cite{wang2018eidetic} combines 3DCNN and LSTM to improve prediction performance. % a prediction method combining 3DCNN and LSTM and is one of the most effective methods on KTH dataset. % one of the most effective methods on KTH dataset.
\item SLAMP\cite{akan2021slamp} is an advanced stochastic video prediction method. 
\item To highlight the importance of parallel prediction in terms of the fast prediction of TKN, particularly when compared with sequential keypoints method Struct-VRNN and Grid-Keypoint, we tested \emph{TKN(w/o tp)} which has the same structure as TKN but lacks the parallel scheme of the keypoint detector. 
\end{enumerate}
Due to the lack or incompleteness of open-sourced code, we tested Predennv2 and SLAMP only on KTH dataset while the others on both datasets.





%\pz{please use itemize, currently it looks very messy}
%
%\begin{enumerate}
%\item ConvLSTM\cite{shi2015convolutional} is one of the oldest and most classic video prediction method based on LSTM. 
%
%\item Struct-VRNN\cite{minderer2019unsupervised} is the first one to use keypoints to make prediction. 
%
%\item Grid-Keypoint \cite{gao2021accurate} is a grid-based keypoint video prediction method.
%
%\item Predennv2\cite{wang2021predrnn}  can be  generalized to most predictive learning scenarios by improving PredRNN with a new curriculum learning strategy. %We can not train it on Human3.6, so we  only use it on the KTH dataset.
%\item PhyDNet\cite{guen2020disentangling} disentangles the dynamic objects and the static background in the video frames. % that achieves the best performance on the Human3.6.
%
%\item E3D-LSTM\cite{wang2018eidetic} combines 3DCNN and LSTM to improve prediction performance. % a prediction method combining 3DCNN and LSTM and is one of the most effective methods on KTH dataset. % one of the most effective methods on KTH dataset.
%
%\item SLAMP\cite{akan2021slamp} is an advanced stochastic video prediction method. %Since the authors did not provide the code for training with the Human3.6 dataset, we tested it with KTH dataset only to avoid the result deviation caused by reproduced code's difference from original proposal.% one of the most effective methods on KTH dataset. 
%
% We can not run their code successfully and we only use its reconstruction data in their paper on KTH.
%\end{enumerate}


\begin{comment}
\par (1) ConvLSTM\cite{shi2015convolutional} is one of the oldest and most classic video prediction method based on LSTM. 
\par (2) Struct-VRNN\cite{minderer2019unsupervised} is the first one to use keypoints to make prediction. 
\par (3) Grid-Keypoint\cite{gao2021accurate} is a grid-based keypoint video prediction method.
\par (4) Predrnn\cite{wang2017predrnn} is a classic prediction method adapted from LSTM.
\par (5) Predennv2\cite{wang2021predrnn} can be generalized to most predictive learning scenarios by improving PredRNN with a new curriculum learning strategy. %We can not train it on Human3.6, so we  only use it on the KTH dataset.
\par (6) PhyDNet\cite{guen2020disentangling} disentangles the dynamic objects and the static background in the video frames. % that achieves the best performance on the Human3.6.
\par (7) E3D-LSTM\cite{wang2018eidetic} combines 3DCNN and LSTM to improve prediction performance. % a prediction method combining 3DCNN and LSTM and is one of the most effective methods on KTH dataset. % one of the most effective methods on KTH dataset.
\par (8) SLAMP\cite{akan2021slamp} is an advanced stochastic video prediction method. %
Due to the lack or incompleteness of code provision, we tested Predennv2 and SLAMP only on KTH dataset while the others on both datasets.
%We choose two classical keypoints-based models, ConvLSTM and Struck VRNN. We compared TKN's performance with several other top prediction methods including PhyDNet, PredRNN, PredRNNv2, E3D-LSTM, SLAMP. Please refer to Section~\ref{sec:related} for their detailed descriptions.
\end{comment}