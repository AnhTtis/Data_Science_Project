
\section{Results and Analysis}
\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/kth.png}
\caption{Results of long-range predictions on KTH. TKN and TKN-Sequential perform better than the baselines. TKN-Sequential provides more precise details.}
\label{fig:kthlong}
\end{figure*}



\begin{table}[t]
\small
 %   \vspace{-0.15in}
    \centering
    \begin{tabular}{c|c|c|c}  \hline
            Method  &boxing&handclapping&handwaving \\ \hline
         TKN&  0.897   &0.908& 0.898      \\
         TKN-Sequential&  0.878  & 0.874 & 0.857    \\ \hline
           Method   & jogging&running&walking  \\ \hline
         TKN&0.762 & 0.759  &  0.806      \\
         TKN-Sequential&   0.783  & 0.775 &  0.820  \\ \hline
    \end{tabular}
     \caption{SSIM performances on different KTH's actions.}
\label{different actions}
\end{table}
\begin{figure} [t]
	\centering
	
	\includegraphics[width=8cm]{figures/human.png}
	\caption{ Results of short-range predictions on Human3.6.}
	\label{fig:human36} 
\end{figure}

\begin{comment}
\begin{figure*} [t]
	\centering
	\subfloat[\label{fig:human1}]{
		\includegraphics[width=8cm]{figures/human_s1.png}}
    \subfloat[\label{fig:human2}]{
		\includegraphics[width=8cm]{figures/human_s2.png} }
	\caption{ Results of short-range predictions on Human3.6.}
	\label{fig:human36} 
\end{figure*}
\end{comment}
\begin{comment}
\begin{figure*}
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[width=8.5cm]{figures/human_s1.png}
	\end{minipage}
	\begin{minipage}[t]{0.5\linewidth}
           
		\centering
		\includegraphics[width=8.5cm]{figures/human_s2.png}
  
	\end{minipage}
	\caption{Results of short-range predictions on Human3.6.}
	\label{fig:end}
\end{figure*}
\end{comment}

\subsection{Speed and Accuracy}
\begin{table}[t]
\centering
\begin{tabular}{c|c|c}  \hline
         Method& FLOPs (G) $\downarrow$  & Params (M)$\downarrow$   \\    \hline
     ConvLSTM &  93.7  & 4.8   \\ 
        PredRNN &   29.4 & 6.0          \\  
       PredRNNv2&  29.6  &   6.1  \\
       PhyDNet& 21.7  &\textbf{3.0}           \\
        SLAMP&  95.0    & 49.4      \\  
       E3D-LSTM&   270.2 &3.7           \\ \hline
     GridNet& 26.2 &  3.3         \\ 
      Struct-VRNN&   11.8 &4.3       \\ \hline
      \textbf{ TKN } &    \textbf{1.6}  & 19.1       \\   
      \textbf{TKN (w/o tp)} & \textbf{1.6}  &19.1   \\
      \textbf{TKN-Sequential} &   3.5 & 105  \\   \hline
   %\caption{The results of FLOPs and the number of parameters}
\end{tabular}
\caption{The results of FLOPs and the number of parameters.$\downarrow$ means the less the better.}
\label{FLOPs:1}
\end{table}


\begin{table*}[t!]
    \centering
     \begin{tabular}{c|c|c|c|c|c|c}   \hline
        \multirow{2}{*}{Method}  &  \multicolumn{3}{c|}{Keypoint detector } & \multicolumn{3}{c}{Predictor}   \\ 
        \cline{2-7}
        &Time (ms)  &  FLOPs (G)  & Params (M) & Time (ms)&FLOPs (G)& Params (M)  \\ \hline
  Struct-VRNN& 104&11.8 &  0.8 & 38&  \textbf{0.1} & 3.5  \\  
  Grid-Keypoint& 142&17.7  & 1.8 & 84& 8.5  & \textbf{1.7} \\     \hline
   \textbf{TKN (w/o tp) }  & 67&  \textbf{1.4} & \textbf{0.1} &  \textbf{8.3}& 0.2 &  18.9         \\  
       \textbf{TKN}   & \textbf{8.2}&  \textbf{1.4} & \textbf{0.1} &  \textbf{8.3} & 0.2 &  18.9         \\   \hline
         
\end{tabular}
    \caption{Time, FLOPs, and the number of parameters comparisons between the Keypoint-based models.}
\label{FLOPs:2}
\end{table*}

Table~\ref{tab:1} summarizes the performance comparison on KTH and Human3.6 datasets. For KTH, we input 10 frames to predict 10 frames during training and 20 frames during testing. For Human3.6, we input 4 frames to predict 4 frames during both training and testing. \emph{TIME (train)} refers to the average period length per training epoch in seconds. \emph{Time (test)} indicates the period length from inputting the frames to after generating the predicted frames in milliseconds. \emph{FPS} is the number of generated frames per second calculated via \emph{Time (test)}. \emph{Memory} indicates the maximum memory consumption at a stable status. Note that to ensure fair comparisons with the end-to-end training methods, \emph{TIME (train)} and \emph{Memory (train)} of TKN, Struct-VRNN and Grid-Keypoint in Table~\ref{tab:1} are all tested without freezing parameters. The results show that \sysname\ outperforms most baselines in both speed and memory consumption significantly with only minor accuracy deterioration on both datasets. 




\vskip 0.1in \noindent\textbf{KTH} results show that \sysname\ performed 19 times faster than the best method E3D-LSTM with only 0.9\% and 5.5\% degradation in SSIM and PSNR during testing, while reducing memory consumption by at least 12.7\% (training) and 0.9\% (testing) compared to the second best methods Struct-VRNN and PredRNN. As such, \sysname\ can bear up to as large a batch size as 150 with up to 24 GB memory which no baseline can even come close to. \sysname\ is 4 times faster than TKN (w/o tp). Fig.~\ref{fig:kthlong} shows the performance of long-range prediction performance tested on the walking class of KTH, with 10 frames as input for predicting 40 frames. The result shows that the \sysname\ predicts the position and pose of a person fairly well while TKN-Sequential presents more and clearer details, because TKN only uses the background information of a fixed frame to synthesize the following frames.%But why the \sysname\ perform better at SSIM in Table \ref{tab:1} ,we  have done a more detailed experiment below.}
%which validates that it has the  advantage in long-range prediction. 
%\textcolor{red}{We put a more detailed comparison of TKN and TKN-Long in the Supplemental Material.}
%We put a more detailed comparison of TKN and TKN-Long in the Supplemental Material.
%We also compare the performance of our models on the different action classes contained in KTH, each class with 100 randomly selected video sequences. %} \pz{100 data is wrong, is a data here a video?} \textcolor{red}{yes,is video sequence} of each KTH's action class for tests. 
%As summarized in Table \ref{different actions}, TKN-Long performs better than TKN on actions with large movements such as walking, jogging, and running, while TKN performs better on handwaving, handclapping, and boxing which have smaller movements.
\par We  compare the performance of our models on the different action classes contained in KTH, each class with 100 randomly selected video sequences of each KTH's action class for tests. As summarized in Table \ref{different actions}, TKN-Sequential performs better than TKN on actions with large movements such as walking, jogging, and running, while TKN performs better on handwaving, handclapping, and boxing which have smaller movements.
\begin{comment}
\begin{figure}[t!]  
\centering
\includegraphics[width=.8\columnwidth]{figures/human36.png} 
\caption{ Results of short-range predictions on Human3.6. Due to space limitations, we show more comparison figure in supplements.}       
\label{fig:human}   
\end{figure}
\end{comment}


\begin{figure}[t]  
\centering
\includegraphics[width=7cm]{figures/hours.png}
\caption{Total training time of \sysname and baselines.}
\label{hours}  
\end{figure}



\vskip 0.1in \noindent\textbf{Human3.6} results show that \sysname\ outperforms the baselines on accuracy performance. Moreover, \sysname\ reduces time and memory consumption by 6\% and 49\% during training, and 66\% and 9\% during testing, % \pz{the decrease calculation should use the deceased value divided by the original value (the larger one)}, 
compared to the second best alternative. Figure \ref{fig:human36} depict the comparison of TKN and baselines on the Human3.6 dataset for short-range prediction, as most baselines do. The changes of the actions are small within a short period. %seem obvious due to small number of frames \pz{reviewer may ask why not put more frames for better observation of action change} \textcolor{red}{Because most baselines on Human 3.6 are testing short-range predictions}. 
But upon closer observation, we can tell that the lighting of the background and the movement of the person in TKN are closest to the goundtruth.%Figure~\ref{fig:human} verifies \sysname\ performs well on short-range predictions on Human3.6.




\begin{table}[t]
\centering
\begin{tabular}{c|c|c|c|c|c}  \hline
             Conv Kernel & SSIM   & PSNR  & Speed(ms)&  FLOPs(G)  & Params(M)     \\    \hline
       $3 \times 3$ &  \textbf{0.916}  & \textbf{31.91}  &  8.2  & 1.4  & 0.14    \\ 
        $2 \times 2$ &  0.862  & 28.55  &  7.6 &  0.5 &  0.06     \\ 
        $1 \times 1$ &  0.851  & 28.07  &  \textbf{7.1}   & \textbf{0.2}  &  \textbf{0.02}   \\ 
$3 \times 3$, $1 \times 1$ &  0.900  & 30.84  &  7.8 &  0.9  &    0.08     \\   \hline
   %\caption{The results of FLOPs and the number of parameters}
\end{tabular}
\caption{The influence of convolution kernel size on the reconstruction accuracy and inference speed of keypoint detector.  ``$3 \times 3$, $1 \times 1$'' indicates using $1 \times 1$ convolution kernel for layers that change only the channel size and not the heatmap size and $3 \times 3$ convolution kernel for the other layers.}
\label{tab:diff_conv_size}
\end{table}


\begin{table}[t!]
\small
    \centering
    \begin{tabular}{c|c|c|c|c|c}   \hline
        \multirow{2}{*}{Method}  &  \multirow{2}{*}{Num}& \multicolumn{2}{c|}{Reconstruction} & \multicolumn{2}{c}{Prediction}   \\ 
        \cline{3-6}
       &  &   SSIM  & PSNR  & SSIM  & PSNR   \\ \hline
  Struct-VRNN& 12&0.821* &   27.86* &  0.766* &24.29*  \\  
  Grid-Keypoint& 12&0.862* &   29.68*&  0.837* &27.11*  \\    
  Separation-Net&16 &0.900 &  30.95 & 0.855 &  26.81 \\\hline
        \multirow{7}{*}{\textbf{TKN}}&4     &  0.895 & 30.43&   0.850   &  26.66           \\   
        & 8    & 0.909 &  31.28&  0.854   & 26.72               \\   
         &12    &  0.914   & 31.71   &   0.863   &  27.40           \\
         &16    & 0.916  &  31.91  & \textbf{0.871}  &   \textbf{27.71}          \\
        & 20   &    0.915  & 31.95   & 0.861   & 27.23  \\
         &32    & 0.922&  32.48 &0.858   &     26.92     \\
        &64    &\textbf{0.925}   & \textbf{32.71}  & 0.849     &     26.46          \\   \hline
         
     \end{tabular}
      \caption{The results of frame reconstruction and prediction using different numbers of keypoints. ``Separation-Net'' refers to the structure in Figure~\ref{fig:encoder2}. 12, 12, 16, are the number of keypoints when Struct-VRNN, Grid-Keypoint, and Separation-Net achieved their best performances, respectively.}%\pz{where's the major caption? For example, the xxx performance of xxx} }
          \label{tab:diff_num_keypoint}
\end{table}

\begin{table*}[t!]
\small
    %\vspace{-0.15in}
    \centering
   % \setlength{\tabcolsep}{4.5mm}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}  \hline
              Method&SSIM&PSNR& TIME (s)& TIME (ms)& FPS&Memory (MB) & Memory (MB) & FLOPs(G)&Params(M)\\ 
               &      &      & train& test& test& train &test & &   \\         \hline
         TKN&  \textbf{0.871}&\textbf{27.71}& \textbf{13} &   17 & 1,176&  4,945 &\textbf{1,705}  & 1.6& 19.1        \\
         TKN+RNN& 0.826&25.61 &\textbf{13}&   15&  1,333 & 5,479 & 2,131     &1.9 &47.5                 \\
         TKN+LSTM& 0.815& 25.19& 19&   23 & 870 & 9,343 & 4,287         & 3.3&189.1        \\
         TKN+GRU& 0.829 &25.82  & 17 &   22 & 1,000 & 8,029 &   3,567   & 2.8&141.9               \\ 
        TKN+MLP& 0.814 &25.12  & \textbf{13} &   \textbf{13} & \textbf{1,538} & \textbf{4,841} &    1,733  &1.5 & 14.4               \\ \hline
    \end{tabular}
    \caption{Performance comparison between TKN employing different predictors.}
    
\label{tab:diff_predictor}
\end{table*}


\begin{figure}[t!]  
\centering
\includegraphics[width=8.2cm]{figures/keypoints.png} 
\caption{Comparison of the keypoints extracted by different methods}   
\label{fig:keypoints}   
\end{figure}

\begin{table*}[t!]
\centering
 \begin{tabular}{c|c|c|c|c|c|c}  \hline
        Method& SSIM  & PSNR  & TIME (ms)& FPS  &Memory (MB) &TIMES (ms) \\
             &                  &                & all model(test)              & (test)         &(test)  &Predictor(test)    \\         \hline
       TKN (employs only the encoder)   &\textbf{0.871} & \textbf{27.71}    & \textbf{17}  &  \textbf{1,176} & \textbf{1,705}  &   \textbf{8.3}        \\   
     TKN (employs whole transformer) &  0.800&  25.87 &  93 &   215 & 1,759  &     74       \\\hline
\end{tabular}
\caption{Performance comparison of TKN's prediction module between using only the transformer encoder and whole transformer.}
\label{tab:transformer}
\end{table*}

\vskip 0.1in \noindent\textbf{Deeper speed analysis.} To help understand why TKN runs so much faster than SOTA algorithms, we measure FLOPs and the number of parameters for each method. \sysname\ and TKN(w/o tp) share the same structure and thus have the same numbers of FLOPs and parameters. As shown in Table \ref{FLOPs:1}, \sysname\  and TKN-Sequential have much fewer FLOPs than the baselines, indicating their much higher computation efficiencies. Table~\ref{FLOPs:2} summarizes the detailed comparison between TKN, TKN(w/o tp), and the other two Keypoint-based methods. For the choice of predictor, \sysname\ uses transformer encoder while Grid-Keypoint uses convlstm and Struct-VRNN uses VRNN. The results in Table~\ref{FLOPs:2} show that \sysname\ presents the fastest speed in both modules and provides an 8 times speedup compared with TKN(w/o tp), indicating the key role of keypoint detector in terms of prediction speed and the advantage of parallel scheme. We can also find that although the predictor of TKN has more FLOPs and number of parameters due to the larger number of parameters of the employed transformer encoder~\cite{vaswani2017attention}, it runs about 5 to 10 times faster than the others. 
%That is because the transformer encoder can be calculated in parallel while Struct-VRNN uses the RNN's step-by-step loop method.% As for the number of parameters, because the transformer encoder and the transformer structure have large numbers of parameters \cite{vaswani2017attention} as shown in Table \ref{FLOPs:2},\sysname\ and TKN-Long do not have as few parameters as other baselines.

\vskip 0.1in \noindent\textbf{Overall training time.} We compare the overall training time considering various numbers of required epochs before convergence. Note that here \sysname, Grid-Keypoint, and Struct-VRNN use the two-step training as mentioned in Section~\ref{sec:setup}. Although TKN's predictor trains slower than Grid-Keypoint and Struct-VRNN because its Transformer encoder takes 750 epochs to reach the optima while Convlstm and VRNN takes only 20 and 50, TKN's overall training speed is up to 2 to 3 times faster than the baselines as shown in Figure \ref{hours}.



\begin{comment}
\vskip 0.1in \noindent\textbf{Step by step.}
To help understand the reason why TKN runs so much faster than SOTA algorithms, we select Grid-Keypoint and Struct-VRNN, which have the most similar structure, to conduct a step-by-step process delay analysis. Specifically, we dove deeper to analyze the time consumption of the keypoint detector and the predictor separately for \sysname\, Grid-Keypoint, and Struct-VRNN. In the test, we used the batch size of 1, 10 \pz{1 or 10?} \textcolor{red}{batchsize is 1,10 10 is for 10 frames input  20 frames be predicted} frames per input and 20 frames per prediction. The keypoint detector of all the methods contains an encoder and a decoder. For the choice of predictor, \sysname\ uses transformer encoder while Grid-Keypoint uses convlstm and Struct-VRNN uses VRNN. As shown in Table~\ref{tab:3}, \sysname\ presents the fastest speed in both modules and its predictor runs about 5 to 10 times faster than the others.

%表1中的训练时间只是一个epoch的时间，每种方法收敛需要的epoch并不相同，所以我们还统计了表一中所有方法训练完成的总时间（训练集上的loss不再降低），结果如图x所示，蓝色部分是指TKN和Struct-VRNN 训练keypoint detector的时间，紫色代表训练预测器的时间，对于其它方法，紫色就是训练全部模型的时间。可以看到我们的TKN训练最快，但是与Struct-VRNN 相比，我们的keypoint detector训练更快而我们的预测器训练要花更长的时间，这是因为我们的Transformer encoder需要750epoch才能达到最佳值，而VRNN只需要20epoch，虽然在表一中训练一轮时间快但是整体上时间要慢于Struct-VRNN
\vskip 0.1in \noindent\textbf{Overall.} Now we compare the overall training time consumption considering varied required numbers of epochs before convergence. Note that here \sysname, Grid-Keypoint, and Struct-VRNN use the two-step training as mentioned in Section~\ref{} \pz{add cross reference, not sure which section}. Although TKN's predictor trains slower than Grid-Keypoint and Struct-VRNN because its Transformer encoder takes 750 epochs to reach the optima while convlstm and VRNN takes only 20 and 50, TKN's overall training speed is up to 2 to 3 times faster than the baselines as shown in Figure \ref{hours}.

\begin{figure}[t!]  
\centering
\includegraphics[width=7cm]{figures/hours.png}
\caption{Total training time.}
\label{hours}  
\end{figure}

\end{comment}

\subsection{Ablation Experiments}
\vskip 0.1in \noindent\textbf{Keypoint Detector.} We test the impact of convolution kernel size and find that it presents a much larger impact on reconstruction accuracy than on inference speed of keypoint detector as summarized in Table~\ref{tab:diff_conv_size}. Therefore we choose to use the $3 \times 3$ convolution kernel.

We then compare the influence of different structures and numbers of keypoints on reconstruction and prediction. Table~\ref{tab:diff_num_keypoint} shows that \sysname\ reconstructs frames better than the two Keypoints-based baselines and the sequential structure in Fig.~\ref{fig:encoder2}, and achieves better performance with more keypoints. Meanwhile, the prediction module hits a performance bottleneck at 20 keypoints, indicating that too many keypoints pose difficulties for prediction. Figure~\ref{fig:keypoints} shows that the keypoint detector of \sysname\ performs better at capturing dynamic information than that of Struct-VRNN and Grid-keypoint on different actions.% \pz{worse than Grid-keypoint?} \textcolor{red}{forgot to write}.


\vskip 0.1in \noindent\textbf{Predictor} relies on the results of the keypoint detector. We verified its performance by replacing its Transformer encoder with alternative modules, i.e., RNN~\cite{elman1990finding}, LSTM~\cite{hochreiter1997long},  GRU~\cite{cho2014learning}, and MLP, which are widely used in prediction tasks. We adjusted the input dimension to $\mathbb{R}^{d_{model}} $ (512), number of layers to 6, and the dimension of the hidden layers to 2048, in all modules, for fair comparisons. We use the encapsulated RNN, LSTM, and GRU modules from PyTorch and  use the MLP structure in Mlp-mixer\cite{tolstikhin2021mlp}). As shown in Table~\ref{tab:diff_predictor}, our predictor module presents comparable training and testing speeds with significantly higher accuracy and memory efficiency.


\begin{table}[t]
\centering
\begin{tabular}{c|c|c|c|c|c}  \hline
          Method    & SSIM   & PSNR  & Speed (ms) &  FLOPs(G)  & Params(M)\\    \hline
      explicit     &  0.852  & 26.50  &  \textbf{6.7} &     \textbf{0.01}   & \textbf{0.7}     \\ 
      latent   &  \textbf{0.871}  & \textbf{27.71} &  8.3  &   0.19     & 18.9      \\   \hline
   %\caption{The results of FLOPs and the number of parameters}
\end{tabular}
\caption{Comparison between using explicit and latent representation of keypoints.}
\label{tab:implicit}
\end{table}
%这实际上是必然的，因为transformer最开始是解决自然语言处理方向的问题，在使用它之前需要对所有的词标注id,然后对id进行嵌入处理，嵌入向量和词语id是一一对应的，它的翻译流程实际上与RNN的循环原理是相似的，每次输出的高维结果会与嵌入向量对比然后得到相应的词语id（一般是取与输出相似度最高的嵌入向量），然后再将这个词语id的嵌入向量作为输入，送到transformer中去翻译下一个词。也就是说他们的任务输入和输出都是有限的离散量，而我们的任务预测的keypoints每个元素实际上都是连续的量，也就是说我们没办法用有限的id来标注，所以在翻译过程用中就没有从高维变量到id那一个步骤，对于transformer如果翻译正确找到正确的id，那么这一次翻译的正确率就是100%,下次的输入用的是正确的id的嵌入向量，而我们的任务每次翻译不可能100%正确的输出浮点指，所以在下一次翻译的时候，输入就会有误差，这样子整个过程下来就会造成错误的累加
\par Table~\ref{tab:transformer} compares the performance of TKN's predictor employing the complete transformer structure with when using only its encoder part. We can see that the encoder-only method works much better in terms of both prediction speed and accuracy. This is because the transformer initially proposed for NLP problems requires embedding each word's label ID into a vector. Its translation process is similar to RNN's cycle principle which compares each high-dimensional output with the embedding vectors to get the word ID, and then inputs the corresponding embedding vector to the transformer to translate the next word. In short, their input and output are finite discrete quantities, while the keypoints in our prediction task are continuous quantities which cannot be labeled with finite IDs, hence excluding the possibility of mapping the high-dimensional output to IDs. Moreover, each output in our task, which is the next input, is a floating point which cannot be acquired with 100\% accuracy. Thus, the small errors in each transformer cycle are accumulated.
%And if the transformer translates well  and it can find the correct id, then the next input is the embedding vector with the correct id, This id and its embedding vector as input are 100\% correct and our task cannot output the floating point finger with 100\% correct translation each time, so the input will have error in the next translation, which will cause the accumulation of errors in the whole process.
%\pz{edition currently stop here}
TKN employing the complete transformer has a long prediction time because the translation part of the transformer is a step-by-step process and each step goes through a complete transformer, while encoder-only TKN outputs all the results in one step.

Further, we test the impact of using explicit or latent representation of keypoints. As shown in Table~\ref{tab:implicit}, latent representation in high-dimension presents higher prediction accuracy.
On the other hand, explicit representation only has limited speed improvement albeit it has fewer FLOPs and parameters.
\par %We also provide a comparison video of TKN and baselines in the attached file. The first second shows the observed content, and the later seconds show the predicted content. It can be seen that TKN and TKN-Long have clearer and more accurate video content than baselines. And TKN-Long is more accurate in details and shows more precise position predictions than TKN.%
\par In summary, the results validate that TKN greatly simplifies the complexity of prediction and accelerates the training and inference speed while improving the prediction accuracy.




