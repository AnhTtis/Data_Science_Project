%预测未来一直是人类梦寐以求的能力，它可以让我们抓住机会对未来要发生的事情做好准备。在一些突发情况中，不仅需要正确的预测，更需要快速实时的去预测，比如说在一辆疾驰的汽车中，司机的应对危险的时间通常在2.3-3秒，如果我们预测系统预测时间太长就会造成严重的危险。现在随着人工智能在计算机视觉领域的的发展，预测未来正逐渐成为现实，视频预测这个领域正是可以把未来的以视频的方法展现出来，但是目前的方法都专注于预测的精准度，忽略了关键的预测速度，所以我们通过高效的关键点提取，使用可以并行计算的自注意力模块实现了实时的视频预测。
%视频预测是根据已知的视频帧序列，预测后续的视频帧序列，它隶属于时间序列预测问题，最早应用于降水的雷达回波图的预测，后面逐渐应用到了人类活动中。目前的主流视频预测方法可以分为两种：第一类，是改进著名的递归神经网RNN，使之它可以更好的捕捉编码相邻帧之间的规律。然而基于RNN的方法将隐藏层信息在序列间按顺序传递，这就会导致最初的信息在传递过程中逐渐消失，从而导致所谓的短时记忆，影响长序列预测的精度。第二类是先对视频帧进行比较分析，一般会把视频帧解纠缠分成运动部分和静止部分，然后对两个模块分别进行预测处理。
%上面所说的两类视频预测方法为了获取精确的预测结果，大都会对每一帧提取复杂的特征，然后对这些特征送入预测网络进行预测，一般单张帧的特征就会达到上万字节，这样无论是在提取特征的模块还是在预测模块都会造成大量的浮点计算，所以无论是在训练还是测试中都会有大量的时间和显存上的消耗。基于以上挑战，我们希望提出一种能够极大减少计算量从而减少视频预测时间以及所消耗显存的方法。

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
\IEEEPARstart{P}{redicting} the future has always been a coveted ability that allows users to be well prepared for upcoming events. With the advancement of artificial intelligence in the field of computer vision, the ability to predict the future is gradually becoming a reality. One of the most popular methods is video prediction, which predicts subsequent video frame sequences based on prior ones. It belongs to the time series prediction problem and was initially applied to the prediction of radar echo maps of precipitation~\cite{shi2015convolutional} then to human activities~\cite{wang2017predrnn,wang2018eidetic}. 
Current mainstream video prediction methods can be divided into two categories. %\textcolor{red}{according to their structure}. 
The first is to improve the well-known recurrent neural network (RNN) to accurately capture the inter-frame pattern~\cite{wang2018eidetic,wang2021predrnn,wang2017predrnn}. However, RNN-based methods often gradually lose the initial information during the sequential information transmission across the hidden layers, resulting in the so-called \emph{short memory} that negatively impacts the long sequence prediction accuracy~\cite{zhao2020rnn}. The second category divides a video frame into a moving portion and a stationary portion and then predicts the two portions separately~\cite{ying2018better,guen2020disentangling,minderer2019unsupervised,gao2021accurate}.  

%shows the future in a video method. 
Most existing works focus on improving accuracy by a few percentage points while ignoring the prediction speed, which is actually crucial for many real-time applications. For instance, in a speeding car, the driver can typically afford a reaction time to danger below 3 seconds \cite{mcgehee2000driver} otherwise would face a grave risk. Assume we want to predict the video frames for the next 3 seconds 
%\textcolor{red}{If we want to predict what will happen after 3 seconds} \textcolor{blue}{3 seconds mentioned earlier} \pz{is "1 second predicts 3 seconds" a standard speed? we needs some references to backup this assumption} 
with a typical vehicular front camera rate of 60 frames per second (fps), the video prediction method has to reach at least 180~fps to finish the prediction within one second. However, existing methods can normally support a frame rate only up to 80 to 100 fps~\cite{guen2020disentangling,akan2021slamp,gao2021accurate}, which can barely help in reality. The reason is threefold: 1) existing methods extract complex features for the sake of higher accuracy, resulting in an excessive number of floating point operations\cite{wang2018eidetic,akan2021slamp,chen2020long}; 2) they waste considerable time on learning similar background information often shared by consecutive frames~\cite{schuldt2004recognizing,h36m_pami}; 3) they use a sequential prediction process where the next frame's input depends on the previous frame's output. Consequently, these methods are poor at processing efficiency and can not predict multiple frames in parallel.
%, but current methods focus on the accuracy of prediction and ignore the critical prediction speed, so we achieve real-time video prediction by efficient key point extraction and using a self-attentive module that can be computed in parallel.
\begin{comment}
The major reason is that they extract complex features for the sake of higher accuracy, resulting in a large number of floating point operations, and the input of the next frame depends on the output of the previous frame. Consequently, these methods can not achieve temporal parallels. Meanwhile, the significant amount of background redundancy in videos~\cite{schuldt2004recognizing,h36m_pami} suggests that lots of the extracted information can be removed for higher efficiency.
%, but current methods focus on the accuracy of prediction and ignore the critical prediction speed, so we achieve real-time video prediction by efficient key point extraction and using a self-attentive module that can be computed in parallel.
\end{comment}
%Video prediction predicts subsequent video frame sequences based on prior video frame sequences. It belongs to the time series prediction problem and was initially applied to the prediction of radar echo maps of precipitation~\cite{shi2015convolutional} then to human activities~\cite{wang2017predrnn,wang2018eidetic}. Current mainstream video prediction methods can be divided into two categories. The first is to improve the well-known recurrent neural network (RNN) to capture the pattern between consecutive coded frames more accurately~\cite{wang2018eidetic,wang2021predrnn}. However, RNN-based methods often transfer information sequentially across the hidden layers and thus gradually lose the initial information during the process, resulting in the so-called \emph{short memory}, which negatively impacts the accuracy of long sequence prediction~\cite{zhao2020rnn}. The second category of methods compares and analyzes the video frames first, which often divides a video frame into a moving portion and a stationary portion, and then predict the two portions separately~\cite{ying2018better,guen2020disentangling}.  

%Most of the aforementioned video prediction methods extract complex features from each frame, which are sent to the prediction network for prediction. Typically, the data of each frame's features is tens of thousands of bytes~\cite{shi2015convolutional,wang2018eidetic,guen2020disentangling,akan2021slamp}, resulting in a large number of floating point operations in both the feature extraction module and the prediction module. Hence, both training and testing consume a great deal of time and memory. In the meanwhile, many videos, particularly human activity records, have a significant amount of background redundancy~\cite{schuldt2004recognizing,h36m_pami} that can be removed by extracting information only from the key motions. 

%Therefore, the field demands a solution that can significantly minimize the amount of computation and, consequently, the time and memory required for video prediction.
%\textcolor{blue}{In fact there will be a large amount of background redundancy in the video of real human activity, if we can extract the  key parts of the motion, predict  these parts only, and then composite the video frames with the background,in this way can we greatly reduce the computational effort}.
As such, we propose a Transformer-based Keypoint extraction neural Network (TKN), which is an unsupervised learning method consisting of a keypoint detector and predictor. \sysname\ can predict video frames by predicting only the keypoints. The keypoint detector extracts feature data for only a few tens of bytes and achieves temporal parallelism, hence greatly reducing the number of floating-point operations, the prediction time, and memory consumption. The predictor further accelerates the process by gathering global attention information in a parallel manner via a self-attention mechanism without disregarding past information. Our contributions are threefold as follows.

%model 改完以后再改
\begin{itemize}
    \item \sysname\ incorporates the advantages of both Keypoint and Transformer structures to guarantee high prediction accuracy, fast training and testing, and low memory consumption. In order to accurately predict videos that contain frequent changes, we additionally propose a sequential variation of \sysname\ called \sysname-Sequential.
    \item The keypoint detector of \sysname\ predict multiple frames in parallel and outperforms keypoint-based state-of-the-art (SOTA) methods in the field of video prediction in terms of keypoint capture and frame reconstruction, resulting in increasing SSIM by 6.3\% and PSNR by 7.5\% with 88.1\% fewer floating-point operations.%%\textcolor{red}{R2C5}
    \item Extensive experimental evaluations have demonstrated the superiority of \sysname\, which achieves a prediction speed of 1176~fps and thus realizing the first real-time video prediction to our best knowledge. % and  \pz{add some niubi result numbers here, especially those more than 100\%} \textcolor{red}{
    Compared to existing methods, TKN is 11 times faster at prediction while reducing 17.4\% GPU memory consumption. As such, \sysname\ lays the groundwork for future real-time multimedia technologies.
\end{itemize}



