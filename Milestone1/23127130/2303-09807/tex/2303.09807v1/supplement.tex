%File: anonymous-submission-latex-2023.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai23}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
%\usepackage{algorithm}
%\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
	
%\floatstyle{ruled}
%\newfloat{listing}{tb}{lst}{}
%\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2023.1)
}

\usepackage[utf8]{inputenc}
%\usepackage{natbib}
%\usepackage{verbatim}
\usepackage{multirow}
\usepackage[british]{babel}
%\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsmath,amsfonts}
\usepackage{textcomp}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{verbatim}
\usepackage{cases} 
\usepackage{subfigure} 
\usepackage{caption}
\let\Bbbk\relax
\usepackage{booktabs}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{color}

\usepackage[capitalize]{cleveref}
%\crefname{section}{Sec.}{Secs.}
%\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}
%\setcopyright{rightsretained}
%\settopmatter{printacmref=false, printccs=false, printfolios=false}
%\renewcommand\footnotetextcopyrightpermission[1]{} % 


\newcommand{\sysname}{{TKN}}

\newcommand{\etal}{{\it et al.}}
\newcommand{\eg}{{\it e.g.}}
\newcommand{\ie}{{\it i.e.}}
\newcommand{\etc}{{\it etc}}

\newcommand{\hxie}[1]{{\color{red} [[comment by hxie]: #1]}}
\newcommand{\pz}[1]{{\color{blue} [[comment by pz]: #1]}}

\title{Supplementary Material}
\author{Anonymous submission}
\setcounter{secnumdepth}{0}


\author{
    Author Name
}
\usepackage{bibentry}


\begin{document}


\maketitle

\section{Model}
\vskip 0.1in \noindent\textbf{Coordinate Generation (CG)}
module converts the heatmap generated by the encoder's last layer to the keypoints. % in the keypoint detector .
%The role of CG is to convert the heatmap into coordinate form.
We use a similar CG structure as in \cite{jakab2018conditional} which first uses a fully connected layer to convert the encoder heatmap $h_n$ from $ \mathbb{R} ^{H_n \times W_n\times C_n}$ into $ \mathbb{R} ^{H_n \times W_n\times K}$, where $K$ refers to the number of keypoints. We do this in the hope of compressing $H_n \times W_n$ into the form of point coordinates in the dimension of $K$. The converted heatmap $h^{'}_n$ can be rewritten as $h^{'}(x;y;i)$, where $x=1,2,...,W_n,y=1,2...,H_n,i=1,2,...,K$, represent the three dimensions of $h^{'}_n $, respectively. Then we can calculate the coordinates of the $k$-th keypiont in width $ p_{ix}$ as follows:
%
\begin{equation}
%
h_n^{'}(x;i)= \frac{\sum_{y} h_n^{'}(x;y;i)}{\sum_{x,y} h_n^{'}(x;y;i)}  , \label{4}
%
\end{equation}
%
\begin{equation}
%
p_{ix}= \sum_{x} h_{x}h_n^{'}(x;i)   ,  \label{5}     
%
\end{equation}
where $h_x$ is a vector of length $W_n$ consisting of values uniformly sampled from -1 to 1 (for example , if $W_n=4$ then $h_x=[-1,-0.333,0.333,1]$). By doing so, we add an axis to the heatmaps at the dimension $W_n$ where $p_{ix}$ is the position of the $i$-th keypoints on the width. Similarly, we can calculate the coordinate in height $ p_{iy} $ by exchanging the position of $x$ and $y$ using Equation~\eqref{4} and Equation~\eqref{5}. We also need the feature values at these coordinates to reconstruct the following frames with keypoints. We express such values with the averages on both the $H_n$ and $W_n$ dimension. We use $p_{iv}$ to represent the value of the $k$-th keypoint:
\begin{equation}
   p_{iv}=\dfrac{1}{H_n \times W_n} \sum_{x,y}h_n(x;y;i).
\end{equation}
As such , we extract the keypoints as $p_i=(p_{ix},p_{iy},p_{iv})$ ,$i=1,2,...,K$.

\vskip 0.1in \noindent\textbf{Heatmap Generation (HG)}
module is a reversed process of CG that converts coordinates to the heatmap. We use a 2-D Gaussian distribution to reconstruct the heatmaps. We first convert the coordinates $p_x,p_y$ into 1-D Gaussian vectors, $x_{vec}$ and $y_{vec}$, where $p_x=(p_{1x},p_{2x},...,p_{Kx}),p_y=(p_{1y},p_{2y},...,p_{Ky})$, as follows:
\begin{equation}
x_{vec}=exp(-\frac{1}{2\sigma_{2}}\Vert p_x - \bar{p}_x \Vert^{2}),
\end{equation}
\begin{equation}
y_{vec}=exp(-\frac{1}{2\sigma_{2}}\Vert p_y - \bar{p}_y \Vert^{2}),
\end{equation}
where $\bar{p}_x$ and $\bar{p}_y$ are the expectations of $p_x$ and $p_y$, respectively. By multiplying $x_{vec}$ and $y_{vec}$ we can get the 2-D Gaussian maps $G\_maps$ as follows:
 \begin{equation}
    G\_maps=x_{vec} \times y_{vec}.
 \end{equation}
Finally we calculate the Hadamard product of $G\_maps$ and $p_v$ to get the $h^{'}_p$:
  \begin{equation}
    h^{'}_p=G\_maps \circ p_v .
 \end{equation}

\begin{figure}[t!]
\centering
\includegraphics[width=4cm]{figures/transformer.png}
\caption{Structure of the transformer encoder}
\label{fig:transformer}
\end{figure}
\begin{figure*}[t!]
\centering
\subfigure[Train the Keypoint Detector using $L_{rec}$ in Eq.(3).]{
\includegraphics[width=.95\columnwidth]{figures/L_rec.png} 
\label{fig:L_rec}
}
\subfigure[Freeze the parameters of Keypoint Detector and train the Predictor using $L_{pred}$ in Eq.(14).]{
\includegraphics[width=.95\columnwidth]{figures/L_pred.png} 
\label{fig:L_pred}
}
\caption{Two-step training}
\label{fig:two-step}
\end{figure*}
\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{LaTeX/figures/tkn-long2.png}
\caption{Detailed structure of TKN-Sequential}
\label{fig:tkn-long2}
\end{figure*}


\vskip 0.1in \noindent\textbf{Transformer encoder.}
We use the transformer encoder structure from \cite{vaswani2017attention} as shown in Figure~\ref{fig:transformer}. The transformer encoder uses a self-attention mechanism that calculates the relationship between the current and other inputs. In this paper, we use the transformer encoder to calculate how the keypoints of the current frame are composed of the previous keypoints. Self-attention is a function that maps a query (Q) and a set of key-value (K,V) pairs to an output.% Note that the query(q), key(k) and value(v) mentioned here are not the same things as in our paper.
\par
Each input corresponds to a vector query of length $d_q$, a vector key of length $d_k$, and a vector value of length $d_v$. Suppose we have $l$ inputs denoted as $I \in \mathbb{R}^{l \times d_{model}}$, each of which has a dimension of $d_{model}$ as mentioned in the paper, there are $l$ query, key, and value vectors dentoed as $Q \in \mathbb{R}^{l \times d_k}$, $K \in \mathbb{R}^{l \times d_k}$, and $V \in \mathbb{R}^{l \times d_v}$, respectively. We use the fully connected layer to convert the inputs to Q, K and V:
\begin{equation}
\left\{
%
\begin{array}{rcl}
%
Q=IW^{Q}\\
K=IW^{K}\\
V=IW^{V} ,
%
\end{array}
%
\right.
%   
\end{equation}
where $W^{Q} \in \mathbb{R}^{d_{model} \times d_k}$, $W^{K} \in \mathbb{R}^{d_{model} \times d_k}$, and $W^{V} \in \mathbb{R}^{d_{model} \times d_v}$ are the conversion matrices. The self-attention of these matrices can be expressed as follows:
\begin{equation}
    Attention(Q,K,V) = softmax(\frac{QK^{T}}{\sqrt{d_k}})V  \label{attention},
\end{equation}
where $\sqrt{d_k}$ is used to prevent the gradient disappearance when the Q and K dot product is too big. In fact, we use multi-head self-attention mechanisms for better performance with more parameters. Each head refers a self-attention in Eq.~\eqref{attention}. We concatenate all the heads together and pass them through a fully connected layer to get the result of the same dimension with the input $I$,
\begin{equation}
    Multihead(Q,K,V)=concat(head_1,...,head_n)W^{h},
\end{equation}
where $head_i= Attention(IW^Q_i,IW^K_i,IW^K_i), i=1,2,...,n$, $W^{h} \in  \mathbb{R}^{ nd_k \times d_{model}}$. We add the result to the input $I$ by residual connection and then normalize them  by LayerNorm to solve the  gradient disappearance:
\begin{equation}
    x=LayerNorm(I+Multihead(Q,K,V)) .  \label{layernorm}
\end{equation}
Considering that the self-attention mechanism may not fit complex processes well, the model is enhanced by adding two  fully connected  layers with a ReLU activation in between called  Feed-Forward Network:
\begin{equation}
    FFN(x)=max(0,xW_1+b_1)W_2+b_2 ,
\end{equation}
Then we use Eq.~\eqref{layernorm} again to get the final result. We can either output the result directly or send it as input to the Transformer encoder for multiple iterations.

\vskip 0.1in \noindent\textbf{Two-step training.}
As shown in Figure \ref{fig:two-step}, we used a two-step training: first we trained the Keypoint Detector using $L_{rec}$ in Eq.$(3)$ in the paper and then froze its parameters, then we trained the Predictor using $L_{pred}$ in Eq.$(14)$ in the paper. 

\vskip 0.1in \noindent\textbf{Prediction Processes.} Figure \ref{fig:tkn-long2} shows the detailed structure of TKN-Sequential, in which $X^{'}_{t+1}$ is the combination of predicted $\bar{P}^{'}_{t+1}$ and the background information of $X_{t}$ extracted by the decoder. Note that  ``$\bar{P}^{''}_{t+1}$ and the background information of $X^{'}_{t+1}$'' are not equal to ``$\bar{P}^{'}_{t+1}$ and the background information of $X_{t}$'', albeit they are both extracted by the encoder from the $X^{'}_{t+1}$. It is because two consecutive frames are very similar but still have some minor differences in the keypoints and background information, as mentioned in Section Prediction Processes in the paper. Following these processes, TKN-Sequential can capture the background information more accurately than TKN in long-range predictions when the background changes a lot.




\section{Experimental setup}
\begin{table}[t]
\small
 %   \vspace{-0.15in}
    \centering
    \begin{tabular}{c|c|c|c}  \hline
              &boxing&handclapping&handwaving \\ \hline
         TKN&  0.897   &0.908& 0.898      \\
         TKN-Sequential&  0.878  & 0.874 & 0.857    \\ \hline
              & jogging&running&walking  \\ \hline
         TKN&0.762 & 0.759  &  0.786      \\
         TKN-Sequential&   0.783  & 0.775 &  0.820  \\ \hline
    \end{tabular}
     \caption{SSIM performances on different KTH's actions.}
\label{different actions}
\end{table}

\vskip 0.1in \noindent\textbf{Dataset.}

\par KTH dataset includes 6 types of movements (walking, jogging, running, boxing, hand waving, and hand clapping) performed by 25 people in 4 different scenarios, for a total of 2391 video samples. The database contains scale variations, clothing variations, and lighting variations. We use people 1–16 for training and 17-25 for testing. Each image is converted to the shape of $(64, 64, 3)$.
\par Human3.6 dataset contains 3.6 million 3D human poses performed by 11 professional actors in 17 scenarios (discussion, smoking, taking photos and so on). We use scenario 1, 5, 6, 7, and 8 for training and 9 and 11 for testing. Each image is converted to the shape of $(128, 128, 3)$.

Following the manner of most baselines \cite{guen2020disentangling,wang2017predrnn,wang2018eidetic}, we test long-range prediction performance on KTH and short-range prediction performance on Human3.6. 
%\pz{because most baselines do that?}\textcolor{red}{yes}

\vskip 0.1in \noindent\textbf{Baselines.}

\begin{figure}[t!]  
\centering
\includegraphics[width=7cm]{figures/hours.png}
\caption{Total training time.}
\label{hours}  
\end{figure}


\begin{table*}[t!]
\centering
 \begin{tabular}{c|c|c|c|c|c|c}  \hline
        Method& SSIM$\uparrow$  & PSNR $\uparrow$ & TIME (ms)$\downarrow$& FPS$\uparrow$  &Memory (MB)$\downarrow$ &TIMES (ms) \\
             &                  &                & all model(test)              & (test)         &(test)  &Predictor(test)    \\         \hline
       TKN (employs only the encoder)   &0.865 & 27.49    &  86  &  233 & 1,447  &     7.2        \\   
     TKN (employs whole transformer) &  0.800&  25.87 &  153 &   131 & 1,450  &     74       \\\hline
\end{tabular}
\caption{Performance comparison of TKN's prediction module between using only the transformer encoder and whole transformer.}
\label{transformer}
\end{table*}

\begin{figure*}[t!]
\centering
\subfigure[Human3.6(1)]{
\includegraphics[width=8.5cm]{figures/human_s1.png} 
\label{human1}
}
\subfigure[Human3.6(2)]{
\includegraphics[width=8.5cm]{figures/human_s2.png} 
\label{human2}
}
\caption{Results of short-range predictions on Human3.6.}
\label{fig:end}
\end{figure*}

To validate the performance of TKN, we select 8 most classical and effective SOTA methods as the baselines. For fair comparison, all these baselines are tested on Pytorch.
%
The 8 baselines include:  ConvLSTM\cite{shi2015convolutional},  Struct-VRNN\cite{minderer2019unsupervised}, Grid-Keypoint \cite{gao2021accurate}, Predrnn\cite{wang2017predrnn}, Predennv2\cite{wang2021predrnn}, PhyDNet\cite{guen2020disentangling}, E3D-LSTM\cite{wang2018eidetic}, SLAMP\cite{akan2021slamp}.
\begin{enumerate}
\item  ConvLSTM\cite{shi2015convolutional} is one of the oldest and most classic video prediction method based on LSTM. 
\item Struct-VRNN\cite{minderer2019unsupervised} is the first one to use keypoints to make prediction. 
\item Grid-Keypoint\cite{gao2021accurate} is a grid-based keypoint video prediction method.
\item Predrnn\cite{wang2017predrnn} is a classic prediction method adapted from LSTM.
\item Predennv2\cite{wang2021predrnn} can be generalized to most predictive learning scenarios by improving PredRNN with a new curriculum learning strategy. %We can not train it on Human3.6, so we  only use it on the KTH dataset.
\item PhyDNet\cite{guen2020disentangling} disentangles the dynamic objects and the static background in the video frames. % that achieves the best performance on the Human3.6.
\item E3D-LSTM\cite{wang2018eidetic} combines 3DCNN and LSTM to improve prediction performance. % a prediction method combining 3DCNN and LSTM and is one of the most effective methods on KTH dataset. % one of the most effective methods on KTH dataset.
\item SLAMP\cite{akan2021slamp} is an advanced stochastic video prediction method. %
\end{enumerate}
Due to the lack or incompleteness of open-sourced code, we tested Predennv2 and SLAMP only on KTH dataset while the others on both datasets.

\vskip 0.1in \noindent\textbf{Model structures.}
TKN and TKN-Sequential have the same keypoint detector structure, which has a 6-layer encoder and a 6-layer decoder. Each encoder layer includes Conv2D, GroupNorm, and LeakyRelu. Each decoder layer includes TransposedConv2D, GroupNorm and LeakyRelu. Since the skip connection is used between encoder and decoder, the input dimension of each decoder layer is twice the output dimension of the corresponding encoder layer.
\par 
For the predictor, TKN uses a 6-layer transformer encoder with the input sequence length of 10, and TKN-Sequential uses 10 single-layer transformer encoders, each with an input sequence length of 10, 11, ..., 19. As mentioned in Section Prediction Processes, each transformer encoder's output of TKN-Sequential is averaged according to the input length, hence the output of both TKN and TKN-Sequential has a length of 10. All transformer encoders employed by the baselines share the same parameters: $d_k=d_v=64,d_{model}=512,d_{inner}=2048,n_{head}=8,dropout=0$






\section{Results}
\subsection{KTH}

We  compare the performance of our models on the different action classes contained in KTH, each class with 100 randomly selected video sequences of each KTH's action class for tests. As summarized in Table \ref{different actions}, TKN-Sequential performs better than TKN on actions with large movements such as walking, jogging, and running, while TKN performs better on handwaving, handclapping, and boxing which have smaller movements.

\vskip 0.1in \noindent\textbf{Overall training time.} We compare the overall training time consumption considering various numbers of required epochs before convergence. Note that here \sysname, Grid-Keypoint, and Struct-VRNN use the two-step training as mentioned in Section Experimental Setup of the paper. Although TKN's predictor trains slower than Grid-Keypoint and Struct-VRNN because its Transformer encoder takes 750 epochs to reach the optima while Convlstm and VRNN takes only 20 and 50, TKN's overall training speed is up to 2 to 3 times faster than the baselines as shown in Figure \ref{hours}.



\subsection{Human3.6}

Figure \ref{human1} and \ref{human2} depict the comparison of TKN and baselines on the Human3.6 dataset for short-range prediction, as most baselines do. The changes of the actions may not be considerable due to the short period. %seem obvious due to small number of frames \pz{reviewer may ask why not put more frames for better observation of action change} \textcolor{red}{Because most baselines on Human 3.6 are testing short-range predictions}. 
But upon closer observation, we can tell that the lighting of the background and the movement of the person in TKN are closest to the goundtruth.





\subsection{Ablation Experiments}
\vskip 0.1in \noindent\textbf{Predictor.} 
%这实际上是必然的，因为transformer最开始是解决自然语言处理方向的问题，在使用它之前需要对所有的词标注id,然后对id进行嵌入处理，嵌入向量和词语id是一一对应的，它的翻译流程实际上与RNN的循环原理是相似的，每次输出的高维结果会与嵌入向量对比然后得到相应的词语id（一般是取与输出相似度最高的嵌入向量），然后再将这个词语id的嵌入向量作为输入，送到transformer中去翻译下一个词。也就是说他们的任务输入和输出都是有限的离散量，而我们的任务预测的keypoints每个元素实际上都是连续的量，也就是说我们没办法用有限的id来标注，所以在翻译过程用中就没有从高维变量到id那一个步骤，对于transformer如果翻译正确找到正确的id，那么这一次翻译的正确率就是100%,下次的输入用的是正确的id的嵌入向量，而我们的任务每次翻译不可能100%正确的输出浮点指，所以在下一次翻译的时候，输入就会有误差，这样子整个过程下来就会造成错误的累加
Table~\ref{transformer} compares the results of Predictor in \sysname\ using the full transformer structure and only its encoder part. We can see that the encoder-only method works much better in terms of both prediction speed and accuracy. This is because the transformer, which was initially proposed to solve NLP problems, requires embedding each word's label ID to a vector. Its translation process is similar to RNN's cycle principle which compares each high-dimensional output with the embedding vectors to get the word ID, and then sends the corresponding embedding vector as the input to the transformer to translate the next word. In short, their input and output are finite discrete quantities, while the keypoints in our prediction task are continuous quantities that can't be labeled with finite IDs, hence excluding the possibility of mapping the high-dimensional output to IDs. Moreover, since each output in our task, which is the next input, is a floating point which can't be acquired with 100\% accuracy, the small errors in each transformer cycle are accumulated.
%And if the transformer translates well  and it can find the correct id, then the next input is the embedding vector with the correct id, This id and its embedding vector as input are 100\% correct and our task cannot output the floating point finger with 100\% correct translation each time, so the input will have error in the next translation, which will cause the accumulation of errors in the whole process.
%\pz{edition currently stop here}

The long prediction time of TKN (employs whole transformer) is because the translation part of the transformer is a step-by-step process and each step goes through a complete transformer, while TKN (emplys only the encoder) is able to output all the results in one step.

\par We also provide a comparison video of TKN and baselines in the attached file. The first second shows the observed content, and the later seconds show the predicted content. It can be seen that TKN and TKN-Sequential have clearer and more accurate video content than baselines. And TKN-Sequential is more accurate in details and shows more precise position predictions than TKN.% Please see Section Model for more details.

\bibliography{aaai23}
\end{document}

