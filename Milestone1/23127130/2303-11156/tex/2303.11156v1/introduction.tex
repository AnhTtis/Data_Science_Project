\section{Introduction}

Artificial Intelligence (AI) has made tremendous advances in recent years, from generative models in computer vision \citep{stablediff, imagen} to generative models in natural language processing (NLP) \citep{gpt3, opt, t5}. Large Language Models (LLMs) can now generate texts of supreme quality with the potential in many applications. For example, the recent model of ChatGPT \citep{chatgpt} can generate human-like texts for various tasks such as writing codes for computer programs, lyrics for songs, completing documents, and question answering; its applications are endless. The trend in NLP shows that these LLMs will even get better with time. However, this comes with a significant challenge in terms of authenticity and regulations. AI tools have the potential to be misused by users for unethical purposes such as plagiarism, generating fake news, spamming, generating fake product reviews, and manipulating web content for social engineering in ways that can have negative impacts on society \citep{adelani2020generating, weiss2019deepfake}. Some news articles rewritten by AI have led to many fundamental errors in them \citep{cnet}. Hence, there is a need to ensure the responsible use of these generative AI tools. In order to aid this, a lot of recent research focuses on detecting AI-generated texts. 

Several detection works study this problem as a binary classification problem \citep{openaidetectgpt2, jawahar2020automatic, mitchell2023detectgpt, bakhtin2019real, fagni2020tweepfake}. For example, OpenAI fine-tunes RoBERTa-based \citep{liu2019roberta} GPT-2 detector models to distinguish between non-AI generated and GPT-2 generated texts \citep{openaidetectgpt2}. This requires such a detector to be fine-tuned with supervision on each new LLM for reliable detection. Another stream of work focuses on zero-shot AI text detection without any additional training overhead \citep{solaiman2019release, ippolito2019automatic, gehrmann2019gltr}. These works evaluate the expected per-token log probability of texts and perform thresholding to detect AI-generated texts. \cite{mitchell2023detectgpt} observe that AI-generated passages tend to lie in negative curvature of log probability of texts. They propose DetectGPT, a zero-shot LLM text detection method, to leverage this observation. Since these approaches rely on a neural network for their detection, they can be vulnerable to adversarial and poisoning attacks \citep{goodfellow2014explaining, sadasivan2023cuda, kumar2022certifying, wang2022improved}. Another line of work aims to watermark AI-generated texts to ease their detection \citep{atallah2001natural, wilson2014linguistic, kirchenbauer2023watermark, zhao2023protecting}. Watermarking eases the detection of LLM output text by imprinting specific patterns on them. Soft watermarking proposed in \cite{kirchenbauer2023watermark} partitions tokens into  green and red lists to help create these patterns. A watermarked LLM samples a token, with high probability, from the green list determined by its prefix token. These watermarks are often imperceptible to humans. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/title-wm.png}
    \caption{An illustration of vulnerabilities of existing AI-text detectors. We consider both watermarking-based and non-watermarking-based detectors and show that they are not reliable in practical scenarios. Colored arrow paths show the potential pipelines for adversaries to avoid detection. In \textcolor{red}{red}: an attacker can use a paraphraser to remove the LLM signatures from an AI-generated text to avoid detection. We show that this attack can break a wide range of detectors. We provide an {\it impossibility result} indicating that for a sufficiently good language model, even the best-possible detector can perform only marginally better than a random classifier. In \textcolor{blue}{blue}: An adversary can query the soft watermarked LLM multiple times to learn its watermarking scheme. This information can be used to spoof the watermark detector by composing human text that is detected to be watermarked.}  
    \label{fig:title}
\end{figure}  


In this paper, through both empirical and theoretical analysis, we show that state-of-the-art AI-text detectors are not reliable in practical scenarios. We first study empirical attacks on soft watermarking \citep{kirchenbauer2023watermark}, and a wide range of zero-shot \citep{mitchell2023detectgpt} and neural network-based detectors \citep{openaidetectgpt2}. We show that a {\it paraphrasing attack}, where a lightweight neural network-based paraphraser is applied to the output text of the AI-generative model, can evade various types of detectors. Before highlighting the results, let us provide an intuition why this attack is successful. For a given sentence $s$, suppose $P(s)$ is the set of all paraphrased sentences that have similar meanings to the sentence $s$. Moreover, let $L(s)$ be the set of sentences the source LLM can output with meanings similar to $s$. Suppose a user has generated $s$ using an LLM and wants to evade detection. If $|L(s)| \ll |P(s)|$, the user can randomly sample from $P(s)$ and avoid detection (if the detection model has a reasonably low false positive rate). Moreover, if $|L(s)|$ is comparable to $|P(s)|$, the detector cannot have low false positive and negative rates simultaneously. 

With this intuition in mind, in \S \ref{sec:aigentextnotdetected}, we use light-weight neural network-based paraphrasers ($2.3\times$ and $5.5\times$ smaller than the source LLM in terms of the number of parameters) to rephrase the source LLM's output text. Our experiments show that this automated paraphrasing attack can drastically reduce the accuracy of various detectors, including the ones using soft watermarking as well as neural network-based detectors and zero-shot classifiers. For example, a PEGASUS-based paraphraser \citep{zhang2019pegasus} can drop the soft watermarking detector's \citep{kirchenbauer2023watermark} accuracy from $97\%$ to $80\%$ with just a degradation of 3.5 in the perplexity score. The area under the ROC curves of zero-shot detectors \citep{mitchell2023detectgpt} drop from $96.5\%$ to $25.2\%$ using a T5-based paraphraser \citep{prithivida2021parrot}. We also observe that the performance of neural network-based trained detectors \citep{openaidetectgpt2} deteriorate significantly after our paraphrasing attack. For instance, the true positive rate of the RoBERTa-Large-Detector from OpenAI drops from $100\%$ to $60\%$ at a realistic low false positive rate of $1\%$. 


In \S \ref{sec:impossibilityresult}, we present an impossibility result regarding the detection of AI-generated texts.
As language models improve over time, AI-generated texts become increasingly similar to human-generated texts, making them harder to detect.
This similarity is reflected in the decreasing total variation distance between the distributions of human and AI-generated text sequences \citep{gpt4}.  
Theorem~\ref{thm:ROC_bound} bounds the area under the receiver operating characteristic (ROC) curve of the best possible detector $D$ as:
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2}\]
where $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ is the total variation distance between the text distributions produced by an AI-model $\mathcal{M}$ and humans $\mathcal{H}$.
It shows that as the total variation distance diminishes, the best-possible detection performance approaches $1/2$, which represents the AUROC corresponding to a classifier that randomly labels text as AI or human-generated.
Thus, for a sufficiently advanced language model, even the best-possible detector performs only marginally better than a random classifier.
The aim of this analysis is to urge caution when dealing with detection systems that purport to detect text produced by any AI model.
We complement our result with a tightness analysis, where we demonstrate that for a given human distribution $\mathcal{H}$, there exists a distribution $\mathcal{M}$ and a detector $D$ for which the above bound holds with equality.

Although our analysis considers the text generated by all humans and general language models, it can also be applied to specific scenarios, such as particular writing styles or sentence paraphrasing, by defining $\mathcal{M}$ and $\mathcal{H}$ appropriately. For example, it could be used to show that AI-generated text, even with an embedded watermark, can be made difficult to detect by simply passing it through a paraphrasing tool. For a sequence $s$ generated by a language model, we set $\mathcal{M}$ and $\mathcal{H}$ to be the distributions of sequences of similar meaning to $s$ produced by the paraphraser and humans. The goal of the paraphraser is to make its output distribution similar to the distribution of human-generated sequences with respect to the total variation distance.
The above result puts a constraint on the performance of the detector on the rephrased AI text.



% \looseness -1
Finally, we discuss the possibility of {\it spoofing attacks} on text generative models in  \S \ref{sec:humantextdetected}. In this setting, an attacker generates a non-AI text that is detected to be AI-generated. An adversary can potentially launch spoofing attacks to produce derogatory texts that are detected to be AI-generated to affect the reputation of the target LLM's developers. As a proof-of-concept, we show that the soft watermarking detectors \citep{kirchenbauer2023watermark} can be spoofed to detect texts composed by humans as watermarked. Though the random seed used for generating watermarked text is private, we develop an attack that smartly queries the target LLM multiple times to learn its watermarking scheme. An {\it adversarial human} can then use this information to compose texts that are detected to be watermarked. Figure \ref{fig:title} shows an illustration of vulnerabilities of existing AI-text detectors.



\looseness -1
Identifying AI-generated text is a critical problem to avoid their misuse by users for unethical purposes such as plagiarism, generating fake news and spamming. However, deploying vulnerable detectors is {\it not} the right solution to tackle this issue since it can cause its own damages such as falsely accusing a human of plagiarism. Our results highlight sensitivities of a wide range of detectors to simple practical attacks such as paraphrasing attacks. More importantly, our results indicate the impossibility of developing reliable detectors in practical scenarios--- to maintain reliable detection performance, LLMs would have to trade off their performance. We hope that these findings can initiate an honest dialogue within the community concerning the ethical and dependable utilization of AI-generated text.  