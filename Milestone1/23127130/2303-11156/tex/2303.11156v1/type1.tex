\section{Spoofing Attacks on AI-text Generative Models}
\label{sec:humantextdetected}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/the.png}
    \caption{Inferred {\it green list score} for the token ``the''. The plot shows the top 50 words from our set of common words that are likely to be in the green list. The word ``first'' occurred $\sim 25\%$ of the time as suffix to ``the''.}
    \label{fig:the}
\end{figure}

\begin{table}[t]
\small
    \centering
    \begin{tabular}{M{7.6cm} | P{1.5cm} | P{1cm} | P{1.5cm} }
    \toprule
        \multicolumn{1}{P{7.6cm}|}{Human text} & $\%$ tokens in green list & z-score & Detector output \\ \midrule \midrule
        the first thing you do will be the best thing you do. this is the reason why you do the first thing very well. if most of us did the first thing so well this world would be a lot better place. and it is a very well known fact. people from every place know this fact. time will prove this point to the all of us. as you get more money you will also get this fact like other people do. all of us should do the first thing very well. hence the first thing you do will be the best thing you do. & 42.6 & 4.36 & Watermarked\\ \midrule
        lot to and where is it about you know and where is it about you know and where is it that not this we are not him is it about you know and so for and go is it that. & 92.5 & 9.86 & Watermarked \\ \bottomrule 
    \end{tabular}
    \vspace{0.2cm}
    \caption{Proof-of-concept human-generated texts flagged as watermarked by the soft watermarking scheme. In the first row, a sensible sentence composed by an {\it adversarial human} contains $42.6\%$ tokens from the green list. In the second row, a nonsense sentence generated by an {\it adversarial human} using our tool contains $92.5\%$ green list tokens. The z-test threshold for watermark detection is 4.}
    \label{tab:adversarialhuman}
\end{table}


A strong AI text detection scheme should have both low type-I error (i.e., human text detected as AI-generated) and type-II error (i.e., AI-generated text not detected). An AI language detector without a low type-I error can cause harms as it might wrongly accuse a human of plagiarizing using an LLM. Moreover, an attacker (adversarial human) can generate a non-AI text that is detected to be AI-generated. This is called the {\it spoofing attack}. An adversary can potentially launch spoofing attacks to produce derogatory texts that are detected to be AI-generated to affect the reputation of the target LLM's developers. In this section, as a proof-of-concept, we show that the soft watermarking detectors \citep{kirchenbauer2023watermark} can be spoofed to detect texts composed by humans as watermarked. 
They watermark LLM outputs by asserting the model to output tokens with some specific pattern that can be easily detected with meager error rates. Soft watermarked texts are majorly composed of {\it green list} tokens. If an adversary can learn the green lists for the soft watermarking scheme, they can use this information to generate human-written texts that are detected to be watermarked. Our experiments show that the soft watermarking scheme can be spoofed efficiently. Though the soft watermarking detector can detect the presence of a watermark very accurately, it cannot be certain if this pattern is actually generated by a human or an LLM.  An {\it adversarial human} can compose derogatory watermarked texts in this fashion that are detected to be watermarked, which might cause reputational damage to the developers of the watermarked LLM. Therefore, it is important to study {\it spoofing attacks} to avoid such scenarios.




\textbf{The attack methodology:} For an output word $s^{(t)}$, soft watermarking samples a word from its green list with high probability. The prefix word $s^{(t-1)}$ determines the green list for selecting the word $s^{(t)}$. The attacker's objective is to compute a proxy of green lists for the $N$ most commonly used words in the vocabulary. A smaller $N$, when compared to the size of the vocabulary, helps faster computations with a trade-off in the attacker's knowledge of the watermarking scheme. We use a small value of $N=181$ for our experiments. The attacker can query the watermarked LLM multiple times to learn the pair-wise occurrences of these $N$ words in the LLM output. Observing these outputs, the attacker can compute the probability of occurrence of a word given a prefix word $s^{(t-1)}$. This score can be used as a proxy for computing the green list for the prefix word $s^{(t-1)}$. An attacker with access to these proxy green lists can compose a text detected to be watermarked, thus spoofing the detector. In our experiments, we query the watermarked OPT-1.3B \citep{opt} $10^6$ times to evaluate the {\it green list scores} to evaluate the green list proxies. We find that inputting nonsense sentences composed of the $N$ common words encourages the LLM to output text majorly only composed of these words. This makes the querying more efficient. In Figure \ref{fig:the}, we show the learned {\ green list scores} for the prefix word ``the'' using our querying technique. We build a simple tool that lets a user create passages token by token. At every step, the user is provided with a list of potential green list words sorted based on the { green list score}. These users or {adversarial humans} try to generate meaningful passages assisted by our tool. Since most of the words selected by { adversarial humans} are likely to be in the green list, we expect the watermarking scheme to detect these texts to be watermarked. Table \ref{tab:adversarialhuman} shows examples of sentences composed by { adversarial humans} that are detected to be watermarked. Even a nonsense sentence generated by an adversarial human can be detected as watermarked with very high confidence.


