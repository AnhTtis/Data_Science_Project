
\documentclass[10pt]{article} % For LaTeX2e
\usepackage[preprint]{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
% \usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\usepackage{xcolor}     
\hypersetup{colorlinks=true, linkcolor=purple, urlcolor=purple, citecolor=purple}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{wrapfig}
\usepackage{float}
\usepackage{todonotes}
\usepackage{amsthm}
\usepackage{adjustbox}
\newtheorem{theorem}{Theorem}
\newtheorem{apptheorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage[bb=dsserif]{mathalpha}
\usepackage{soul}
\usepackage{makecell}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{M}[1]{>{\arraybackslash}m{#1}}
\newcommand{\tv}{\mathsf{TV}}
\newcommand{\arc}{\mathsf{AUROC}}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
\newcommand{\newcontent}[1]{\textcolor{black}{#1}}
\newcommand{\revision}[1]{\textcolor{black}{#1}}
\newcommand{\SF}[1]{\textcolor{red}{SF: #1}}




\title{Can AI-Generated Text be Reliably Detected?
}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Vinu Sankar Sadasivan \email vinu@umd.edu \\
      \addr Department of Computer Science,
      University of Maryland
      \AND
      \name Aounon Kumar \email aokumar@hbs.edu \\
      \addr Department of Computer Science,
      Harvard University
      \AND
      \name Sriram Balasubramanian \email sriramb@umd.edu \\
      \addr Department of Computer Science,
      University of Maryland
      \AND
      \name Wenxiao Wang \email wwx@umd.edu \\
      \addr Department of Computer Science,
      University of Maryland
      \AND
      \name Soheil Feizi \email sfeizi@umd.edu \\
      \addr Department of Computer Science,
      University of Maryland}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{01}  % Insert correct month for camera-ready version
\def\year{2025} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=OOgsAZdFOt}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

% \begin{abstract}
% The rapid progress of Large Language Models (LLMs) has made them capable of performing astonishingly well on various tasks, including document completion and question answering. The unregulated use of these models, however, can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, we show that these detectors can break in \newcontent{several} scenarios \newcontent{in the presence of an adversary}. In particular, we develop a {\it recursive paraphrasing} attack to \newcontent{stress test these AI text detectors}, which can break a whole range of detection schemes, including the ones using the watermarking schemes as well as neural network-based detectors, zero-shot classifiers, and retrieval-based detectors. Our experiments include passages around 300 tokens in length, showing the sensitivity of the detectors even in the case of relatively long passages. We also observe that our {\it recursive paraphrasing} only degrades text quality slightly --- measured via human studies, and metrics such as perplexity scores and accuracy on text benchmarks --- \newcontent{making it a practical attack for many social engineering applications such as spamming and spreading fake news}. 
% Additionally, we show that even LLMs protected by watermarking schemes can be vulnerable to spoofing attacks adversarially aimed to mislead detectors to classify human-written text as AI-generated, potentially causing reputational damages to the developers. In particular, we show that an adversary can infer hidden AI text signatures of the LLM outputs without having white-box access to the detection method. Finally, we provide a theoretical connection between the AUROC of the best possible detector and the Total Variation distance between human and AI text distributions that can be used to study the fundamental hardness of the reliable detection problem for advanced language models.
% \end{abstract}

\begin{abstract}
\newcontent{
Large Language Models (LLMs) can perform impressively well in various applications, such as document completion and question-answering. 
However, the potential for misuse of these models in activities such as plagiarism, generating fake news, and spamming has raised concerns about their responsible use.  
Consequently, the reliable detection of AI-generated text has become a critical area of research.
Recent works have attempted to address this challenge through various methods, including the identification of model signatures in generated text outputs and the application of watermarking techniques to detect AI-generated text.
These detectors have shown to be effective under their specific settings.
In this paper, we stress-test the robustness of these AI text detectors in the presence of an attacker.
We introduce {\it recursive paraphrasing} attack to stress test a wide range of detection schemes, including the ones using the watermarking as well as neural network-based detectors, zero-shot classifiers, and retrieval-based detectors.
Our experiments conducted on passages, each approximately 300 tokens long, reveal the varying sensitivities of these detectors to our attacks.
We also observe that these paraphrasing attacks add slight degradation to the text quality.
We analyze the trade-offs between our attack strength and the resulting text quality, measured through human studies, perplexity scores, and accuracy on text benchmarks. 
Our findings indicate that while our recursive paraphrasing method can significantly reduce detection rates, it only slightly degrades text quality in many cases, highlighting potential vulnerabilities in current detection systems in the presence of an attacker.
Additionally, we investigate the susceptibility of watermarked LLMs to spoofing attacks aimed at misclassifying human-written text as AI-generated. 
We demonstrate that an attacker can infer hidden AI text signatures without white-box access to the detection method, potentially leading to reputational risks for LLM developers.
Finally, we provide a theoretical framework connecting the AUROC of the best possible detector to the Total Variation distance between human and AI text distributions. 
This analysis offers insights into the fundamental challenges of reliable detection as language models continue to advance.
Our code is publicly available at \url{https://github.com/vinusankars/Reliability-of-AI-text-detectors}.}
\end{abstract}





\input{introduction}
\input{paraphrasing}
\input{spoofing}
\input{theory}
% \input{discussion}

% \subsubsection*{Broader Impact Statement}
% \subsubsection*{Author Contributions}
% \subsubsection*{Acknowledgments}

\section*{Acknowledgments and Disclosure of Funding}

This project was supported in part by NSF CAREER AWARD 1942230, ONR YIP award N00014-22-1-2271, NIST 60NANB20D134, Meta award 23010098, HR001119S0026 (GARD), Army Grant No. W911NF2120076, a capital one grant, and the NSF award CCF2212458 and an Amazon Research Award. Sadasivan is also supported by the Kulkarni Research Fellowship. The authors would like to thank Keivan Rezaei and Mehrdad Saberi for their insights on this work. The authors also acknowledge the use of OpenAI's ChatGPT to improve clarity and readability.

\bibliography{main}
\bibliographystyle{tmlr}

\appendix
\input{suppl}

\end{document}
