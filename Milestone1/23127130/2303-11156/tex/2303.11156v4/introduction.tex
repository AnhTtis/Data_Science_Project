\section{Introduction}

Artificial Intelligence (AI) has made tremendous advances in recent years, from generative models in computer vision \citep{stablediff, imagen} to generative models in natural language processing (NLP) \citep{gpt3, opt, t5}. Large Language Models (LLMs) can now generate texts of supreme quality with the potential in many applications. For example, the recent model of ChatGPT \citep{chatgpt} can generate human-like texts for various tasks such as writing codes for computer programs, lyrics for songs, completing documents, and question answering; its applications are endless. The trend in NLP shows that these LLMs will even get better with time. However, this comes with a significant challenge in terms of authenticity and regulations. AI tools have the potential to be misused by users for unethical purposes such as plagiarism, generating fake news, spamming, generating fake product reviews, and manipulating web content for social engineering in ways that can have negative impacts on society \citep{adelani2020generating, weiss2019deepfake}. Some news articles rewritten by AI have led to many fundamental errors in them \citep{cnet}. Hence, there is a need to ensure the responsible use of these generative AI tools. In order to aid this, a lot of recent research focuses on detecting AI-generated texts. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/title-wm.png}
    % \vspace{-2mm}
    \caption{An illustration of vulnerabilities of existing AI-text detectors. We consider both watermarking-based and non-watermarking-based detectors and show that they are not reliable in practical scenarios. Colored arrow paths show the potential pipelines for adversaries to avoid detection. In \textcolor{red}{red}: an attacker can use a paraphraser to remove the LLM signatures from an AI-generated text to avoid detection. In \textcolor{blue}{blue}: an adversary can query the watermarked LLM multiple times to learn its watermarking scheme. This information can be used to spoof the watermark detector. 
    }  
    \label{fig:title}
    % \vspace{-5mm}
\end{figure}  

\newcontent{
Recent works propose several ways, such as using trained neural network-based detectors, zero-shot detectors, watermarking, and retrieval-based detectors for flagging AI-generated text.
These detectors, especially watermarking, have shown to be effective in various settings.
Neural network-based detectors approach the detection problem as a binary classification task \citep{openaidetectgpt2, jawahar2020automatic, mitchell2023detectgpt, bakhtin2019real, fagni2020tweepfake, li2024magemachinegeneratedtextdetection}. For example, OpenAI fine-tunes RoBERTa-based \citep{liu2019roberta} GPT-2 detector models to distinguish between non-AI generated and GPT-2 generated texts \citep{openaidetectgpt2}. 
This requires such a detector to be fine-tuned with supervision on each newly released LLM for reliable detection.
Zero-shot detectors address this downside of trained detectors by performing the detection task without any additional training overhead \citep{solaiman2019release, ippolito2019automatic, gehrmann2019gltr}. These works evaluate the expected per-token log probability of texts and perform thresholding to detect AI-generated texts. \citet{mitchell2023detectgpt} observe that AI-generated passages tend to lie in negative curvature of log probability of texts. To leverage this observation, they propose DetectGPT, a zero-shot LLM text detection method. 
However, zero-shot detectors require access to the original model used to generate the AI text to achieve the best performance.
Additionally, neural network-based and zero-shot detectors rely on a deep net for their detection, and they can be vulnerable to adversarial and poisoning attacks \citep{goodfellow2014explaining, sadasivan2023cuda, kumar2022certifying, wang2022improved}.
In comparison to these methods, watermarking significantly eases the detection of AI-generated text by imprinting specific patterns on them that are imperceptible to humans  \citep{atallah2001natural, wilson2014linguistic, kirchenbauer2023watermark, zhao2023protecting, kuditipudi2024robustdistortionfreewatermarkslanguage, zhao2023provablerobustwatermarkingaigenerated}.
Soft watermarking proposed in \citet{kirchenbauer2023watermark} partitions tokens into ``green'' and ``red'' lists, as they define, to help create these patterns. A watermarked LLM samples a token, with high probability, from the green list determined by a pseudo-random generator seeded by its prefix token. The watermarking detector would classify a passage with a large number of tokens from the green list as AI-generated.
The soft watermarking approach of \citet{kirchenbauer2023watermark} has been shown to be effective in various settings and remains one of the popular approaches for detecting AI-generated text. 
For example, their watermarking scheme can achieve a high true positive rate of 99.8\% at a false positive rate of 1\% on a task for classifying AI text against human-written news articles.
However, for watermarking to be an effective tool for preventing AI misuse, it has to be enforced on all the major LLM generators. Otherwise, an adversary could use a non-watermarking LLM for their purposes.
\citet{krishna2023paraphrasing} introduces an information retrieval-based detector by storing the outputs of the LLM in a database. For a candidate passage, their algorithm searches this database for semantically similar matches for detection. However, storing user-LLM conversations might cause serious privacy concerns.
}



\newcontent{Several recent news \citep{news1, news2, news3, news4, news5} show that some of these popular AI-text detectors fail in practical settings.
In this paper, through several experiments, we stress-test state-of-the-art AI-text detectors to evaluate their robustness in the presence of an attacker \citep{wolff, scottaaronson, liang2023gpt, deepfaketext, m4}.
In \S \ref{sec:aigentextnotdetected}, we have developed a {\it recursive paraphrasing attack} that use neural network-based paraphrasing to recursively paraphrase the source LLM's output text. 
We perform experiments with our automated recursive paraphrasing to show the sensitivity of a range of AI text detectors to {\it type-II error} (detecting AI text as human text).
For instance, {\bf recursive paraphrasing attack on watermarked texts, even over relatively long passages of 300 tokens in length, can drop the detection rate (true positive rate at \bm{$1\%$} false positive rate or TPR@\bm{$1\%$}FPR) from $\bm{99.3\%}$ to $\bm{9.7\%}$.} 
We find that our attack can add slight degradations to the AI text quality.
Hence, we analyze the trade-offs between our attack and the resulting text quality, measured through human studies, perplexity scores, and accuracy of text benchmarks.
Our attack differs from the relatively weaker attack from \cite{kirchenbauer2023watermark} where they perform span replacement by replacing random tokens (in-place) using an LLM. Thus, our experiments show the sensitivity of the watermarking scheme against paraphrasing attacks in the presence of a stronger attacker.
}
\revision{
\cite{zhang2024watermarkssandimpossibilitystrong} and \cite{sico} are also substitution-based attacks. 
\cite{zhang2024watermarkssandimpossibilitystrong} have a different attack objective when compared to our attack. Their attack is performed to maintain the quality of the text alone and not semantic similarity. Hence, their attack can change the original content.
\cite{sico} employs in-context optimization through iterative generation of word- or sentence-level substitutions using a proxy AI text detector to guide the substitutions. This positions their attack as an adversarial algorithm for evading text detection. In contrast, our approach focuses on non-adversarial iterative or recursive text paraphrasing attacks. 
}


After paraphrasing, the area under the receiver operating characteristic (AUROC) curves of zero-shot detectors \citep{mitchell2023detectgpt} drops from $96.5\%$ to $25.2\%$. We also observe that the performance of neural network-based trained detectors \citep{openaidetectgpt2} deteriorates significantly after our paraphrasing attack. For instance, the TPR@$1\%$FPR of the RoBERTa-Large-Detector from OpenAI drops from $100\%$ to $60\%$ after paraphrasing. In addition, we show that the retrieval-based detector by \citet{krishna2023paraphrasing} designed to evade paraphrase attacks is vulnerable to our recursive paraphrasing. In fact, the accuracy of their detector falls from $100\%$ to below $60\%$ with our recursive paraphrase attack. 


% We also observe that the quality of the paraphrased passages degrades, but only slightly, compared to the original ones.
\newcontent{
To quantify the drop in text quality after recursive paraphrasing, we perform MTurk human evaluation studies and measure other automated metrics such as perplexity and text benchmark accuracy. \textbf{Our human evaluation study shows that $77\%$ of the recursively paraphrased passages are rated high quality in terms of content preservation, and $89\%$ of them are rated high quality in terms of grammar or text quality.} 
We also show that our \textbf{recursive paraphrasing, when applied to a text benchmark such as a question-answering dataset, does not affect the performance}, providing additional evidence that recursive paraphrasing does not hurt the content of the original text.}
\newcontent{Though an attacker may further improve the text quality with manual interventions, paraphrasing attacks can be sufficient for an adversary to perform social engineering tasks such as spamming, phishing, or spreading propaganda.}





% With this intuition in mind, in \S \ref{sec:aigentextnotdetected}, we use light-weight neural network-based paraphrasers ($2.3\times$ and $5.5\times$ smaller than the source LLM in terms of the number of parameters) to rephrase the source LLM's output text. Our experiments show that this automated paraphrasing attack can drastically reduce the accuracy of various detectors, including those using soft watermarking. For example, a PEGASUS-based paraphraser \citep{zhang2019pegasus} can drop the soft watermarking detector's \citep{kirchenbauer2023watermark} accuracy from $97\%$ to $80\%$ with just a degradation of 3.5 in the perplexity score. The area under the receiver operating characteristic (AUROC) curves of zero-shot detectors \citep{mitchell2023detectgpt} drop from $96.5\%$ to $25.2\%$ using a T5-based paraphraser \citep{prithivida2021parrot}. We also observe that the performance of neural network-based trained detectors \citep{openaidetectgpt2} deteriorate significantly after our paraphrasing attack. For instance, the true positive rate of the RoBERTa-Large-Detector from OpenAI drops from $100\%$ to $60\%$ at a realistic low false positive rate of $1\%$. In addition, we show that retrieval-based detector by \citet{krishna2023paraphrasing} designed to evade paraphrase attacks are vulnerable to {\it recursive} paraphrasing. In fact, the accuracy of their detector falls from $100\%$ to $25\%$ with our {\it recursive} paraphrase attack.

Moreover, we show the possibility of {\bf spoofing attacks} on various AI text detectors in  \S \ref{sec:humantextdetected}. In this setting, an attacker generates a non-AI text that is detected to be AI-generated, thus adversarially increasing \textit{type-I error} (falsely detecting human text as AI text). An adversary can potentially launch spoofing attacks to produce derogatory texts that are detected to be AI-generated to affect the reputation of the target LLM's developers. In particular, we show that an adversary can infer hidden AI text signatures without having white-box access to the detection method. For example, though the pseudo-random generator used for generating watermarked text is private, we develop an attack that adaptively queries the target LLM multiple times to learn its watermarking scheme. An {\it adversarial human} can then use this information to compose texts that are detected to be watermarked. Figure \ref{fig:title} shows an illustration of some of the vulnerabilities of the existing AI-text detectors.
\revision{
\cite{gu2024learnabilitywatermarkslanguagemodels} build upon our spoofing attacks by employing watermarked data distillation to train a student model to replicate the next-token distribution. 
}

Finally, in \S \ref{sec:impossibilityresult}, we present a theoretical result regarding the hardness of AI-text detection. Our main result in Theorem \ref{thm:ROC_bound} states that the AUROC of the best possible detector differentiating two distributions $\mathcal{H}$ (e.g., human text) and $\mathcal{M}$ (e.g., AI-generated text) reduces as the total variation distance $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ between them decreases. Note that this result is true for any two arbitrary distributions $\mathcal{H}$ and $\mathcal{M}$. For example, $\mathcal{H}$ could be the text distribution for a person or group and $\mathcal{M}$ could be the output text distribution of a general LLM or an LLM trained by an adversary to mimic the text of a particular set of people. Essentially, adversaries can train LLMs to mimic human text as they get more sophisticated, potentially reducing the TV distance between human and AI text, leading to an increasingly more difficult detection problem according to our Theorem \ref{thm:ROC_bound}. Although estimating the exact TV between text distributions from a finite set of samples is a challenging problem, we provide some empirical evidence, over simulated data or via TV estimations, showing that more advanced LLMs can potentially lead to smaller TV distances. Thus, \textbf{our Theorem \ref{thm:ROC_bound} would indicate an increasingly more difficult reliable detection problem} in such cases. 
Our theory also indicates that if a detector becomes more robust to type-I errors, type-II errors will increase, revealing a fundamental tradeoff between type-I and type-II errors for the AI text detection problem.
Similar tradeoffs have been explored in other domains as well.
For example, \citet{KhajaviK16} study the relationship between detection performance and KL divergence between input distributions in the context of covariance selection.
\citet{thapliyal2022datadriven} show that undetectable cyberattacks can be generated by mimicking the input-output data distribution of network control systems.
Although not a surprising result, Theorem~\ref{thm:ROC_bound} is the first to link this tradeoff to the detection of AI-generated content to our knowledge. 


Identifying AI-generated text is a critical problem to avoid its misuse by users for unethical purposes such as plagiarism, generating fake news, and spamming. However, \newcontent{blindly relying on these} detectors may {\it not} be the right solution to tackle this issue since it can cause its own damages, such as falsely accusing a human of plagiarism. Our results highlight the sensitivities of a wide range of detectors to both evasion and spoofing attacks and indicate the difficulty of developing reliable detectors \newcontent{in the presence of an attacker}. 
\newcontent{We hope to reveal the sensitivity of AI text detectors to various attacks in our stress tests experiments.}


In summary, we make the following contributions in this work.
\begin{itemize}
    \item \newcontent{Our work is the {\it first to comprehensively analyze} the robustness of four different classes of detectors, including watermarking-based, neural network-based, zero-shot, and retrieval-based detectors, and stress-test them in the presence of an attacker (in \S\ref{sec:aigentextnotdetected}). In particular, the {\it recursive paraphrasing attack} that we develop is the first method that can break watermarking \citep{kirchenbauer2023watermark} and retrieval-based \citep{krishna2023paraphrasing} detectors. We also provide experiments to analyze the resulting text quality after our attack to find that recursive paraphrasing only leads to a slight text quality degradation in many cases.} 
    
    \item Our work is the first to show that existing detectors are vulnerable against {\it spoofing attacks} where an adversarial human aims to write a (potentially derogatory) passage falsely detected as AI-generated {\it without} having a white-box access to the detection methods (in \S\ref{sec:humantextdetected}). For instance, as proof of concept, we show that an adversary can infer the watermarking signatures by probing the watermarked LLM and analyzing the statistics of the generated tokens.     
    
    \item Our work is the first to establish a theoretical connection between the AUROC of the best possible detector and the TV distance between human and AI-text distributions that can be used to study the hardness of the reliable text detection problem (in \S\ref{sec:impossibilityresult}).  Our theory also reveals a fundamental tradeoff between type-I and type-II errors for the AI text detection problem.
%    . Acknowledging difficulties in estimating exact TV distances for complex text distributions, we provide empirical evidences, over simulated data or via TV estimates, showing that more advanced LLMs can potentially lead to smaller TV distances, resulting in a fundamentally more difficult detection problem. 
\end{itemize}


%initiate an honest dialogue within the community concerning the ethical and dependable utilization of AI-generated text.