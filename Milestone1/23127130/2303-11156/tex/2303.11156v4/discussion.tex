\section{Conclusion}
% \begin{wrapfigure}{R}{0.4\textwidth}
% \vspace{-8mm}
%   \begin{center}
%     \includegraphics[width=0.4\textwidth]{images/watermark_roc.png}
%   \end{center}
%   \vspace{-4mm}
%   \caption{ROC curves for the watermark-based detector. The performance of such a strong detection model can deteriorate with paraphrasing and spoofing attacks. \texttt{ppi} refers to \texttt{i} recursive paraphrasing.}
%   \vspace{-2mm}
%     \label{fig:watermark-roc}
% \end{wrapfigure}

\newcontent{In this paper, we stress-test the performance of four different classes of detectors, including watermarking, neural net, zero-shot, and retrieval-based detectors in the presence of an attacker. We develop a strong evasion attack called {\it recursive paraphrasing} that can break recently proposed watermarking and retrieval-based detectors. We use MTurk human studies and other automated metrics to quantify the degradation in text quality after our attack. We also show that adversaries can spoof these detectors to increase their type-I errors and cause reputational damage to LLM developers. Finally, we establish a theoretical connection between the AUROC of the best possible detector and the TV distance between human and AI-text distributions that can be used to study the fundamental hardness of the reliable detection problem for more advanced LLMs.}


\newcontent{In the future, attackers could adversarially train LLMs to explicitly mimic a specific group of people to minimize TV distances based on our theory to evade detection easily. It might be interesting to see more work on this aspect.  Though the existing paraphrasers we use are powerful, they might not perform as well in specific technical domains such as clinical text data. However, stronger paraphrasers in the future might overcome these issues. By showing empirical evidence on larger models having smaller TV distance estimates, we hypothesize that reliable detection would get harder as LLMs get more powerful.}
% A precise analysis of this assumption is quite difficult because estimating the TV of the text distributions from a finite set of samples is extremely challenging. Hence, we provide empirical experiments over synthetic data showing that bigger language models can potentially lead to smaller TV distances.}


A detector should ideally be helpful in reliably flagging AI-generated texts to prevent the misuse of LLMs. However, the cost of misidentification by a detector can be huge. If the false positive rate of the detector is not low enough, humans (e.g., students) could be falsely accused of AI plagiarism. Moreover, a disparaging passage falsely detected to be AI-generated could affect the reputation of the LLM's developers. As a result, the practical applications of AI-text detectors can become unreliable and invalid. Security methods need not be foolproof. However, we need to make sure that it is not an easy task for an attacker to break these security defenses. Thus, stress testing current and future detectors can be vital to avoid creating a false sense of security. 
% We hope that the results presented in this work can encourage an open and honest discussion in the community about the ethical and trustworthy applications of generative LLMs. 







%\textcolor{red}{
%\citet{chakraborty2023possibilities} attempt to theoretically show that AI text detection is ``almost always possible'' using standard results in information theory. However, the underlying assumption of their result is that several {\it independent} samples are available to the detector from either distribution. This is not a practical assumption since sentences in a document are often correlated with each other.
%} 

%We hope that the results presented in this work can encourage an open and honest discussion in the community about the ethical and trustworthy applications of generative LLMs. 




% On the flip side, \citet{chakraborty2023possibilities} attempt to theoretically show that AI-text detection is ``almost always possible'' using standard results in information theory. However, the underlying assumption of their result is that several {\it independent} samples are available to the detector from either distribution. This is not a practical assumption since sentences in a document are often correlated with each other. This may open up more possibilities for an adversary to evade detection (e.g., an AI Twitter bot can evade detection by tweeting using both AI and human text.)