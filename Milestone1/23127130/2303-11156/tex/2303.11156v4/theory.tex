% \begin{figure}[t]
%     \begin{minipage}{0.55\textwidth}
%     \centering
%         \includegraphics[width=0.85\textwidth]{images/IR_attack.png}
%         \caption{Recursive paraphrasing breaks the retrieval-based detector \citep{krishna2023paraphrasing} with only slight degradation in text quality. \texttt{ppi} refers to $\texttt{i}$ recursion(s) of paraphrasing. Numbers next to markers denote the perplexity scores of the paraphraser output.}
%     \label{fig:ir-attack}
%     \end{minipage}
%     \hfill 
%     \begin{minipage}{0.41\textwidth}
%     \centering
%         \includegraphics[width=\textwidth]{images/roc_bound.png}
%         \caption{Comparing the performance, in terms of AUROC, of the best possible detector to that of the baseline performance corresponding to a random classifier.}
%     \label{fig:roc_bound}
%     \end{minipage}
%     \vspace{-3mm}
% \end{figure}


% \section{Impossibility Results for Reliable Detection of AI-Generated Text}
\section{Hardness of Reliable AI Text Detection}
\label{sec:impossibilityresult}

% Detecting the misuse of language models in the real world, such as plagiarism and mass propaganda, necessitates the identification of text produced by all kinds of language models, including those without watermarks.
% However, as these models improve over time, the generated text looks increasingly similar to human text, which complicates the detection process.
% More formally, the total variation distance between the distributions of AI-generated and human-generated text sequences diminishes as language models become more sophisticated (see \S~\ref{sec:tv_estimation}).
% This section presents a fundamental constraint on general AI-text detection, demonstrating that the performance of even the best possible detector decreases as models get bigger and more powerful.
% For a sufficiently advanced language model, a detector can only perform marginally better than a random classifier.
% The purpose of this analysis is to caution against relying too heavily on detectors that claim to identify AI-generated text. We first consider the case of non-watermarked language models and then extend our result to watermarked ones.\looseness=-1

% \textcolor{red}{
% In this section, we provide a theoretical result that states that the performance of the best possible detector reduces as the Total Variation (TV) distance between AI-generated and human text distributions decreases. This means that as LLMs become more powerful and human-like, it may potentially lead to lower TV distances between human and AI text, increasing the difficulty in reliably detecting AI text. Additionally, based on our theory, an adversary can use advanced LLMs to mimic human text or use paraphrasers to explicitly reduce the TV distance between human and AI text distributions to evade text detection systems.
% }



In this section, we formally upper bound the AUROC of an arbitrary detector in terms of the TV between the distributions for $\mathcal{M}$ (e.g., AI text) and $\mathcal{H}$ (e.g., human text) over the set of all possible text sequences $\Omega$. We note that this result holds for any two arbitrary
distributions $\mathcal{H}$ and $\mathcal{M}$. For example, $\mathcal{H}$ could be the text distribution for a person or group, while $\mathcal{M}$ could be the output text distribution of a general LLM or an LLM trained by an adversary to mimic
the text of a particular set of people. 


We use $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ to denote the TV between these two distributions and model a detector as a function $D: \Omega \rightarrow \mathbb{R}$ that maps every sequence in $\Omega$ to a real number.
% detection parameter $\gamma$.
Sequences are classified into AI-generated or human-generated by applying a threshold $\gamma$ on this number.
By adjusting the parameter $\gamma$, we can tune the sensitivity of the detector to AI and human-generated texts to obtain an ROC curve.



\begin{theorem}
\label{thm:ROC_bound}
The area under the ROC of any detector $D$ is bounded as
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2}.\]
\end{theorem}


\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \vspace{-4mm}
        \includegraphics[width=1.05\linewidth]{images/roc_bound.png}
        \caption{Comparing the performance, in terms of AUROC, of the best possible detector to that of the baseline performance corresponding to a random classifier.}
    \label{fig:roc_bound}
    \vspace{-5mm}
\end{wrapfigure}
The proof is deferred to Appendix~\ref{app:proof_roc_bnd}. Figure~\ref{fig:roc_bound} shows how the above bound grows as a function of the TV distance.
This theorem states that as the TV distance between AI and human text distributions reduces, the AUROC of the best possible detector decreases. Based on our theory, an adversary can use advanced LLMs to mimic human text to  reduce the TV distance between human and AI text distributions to evade text detection systems.


For a detector to have a good performance (say, AUROC $> 0.9$), the distributions of human and AI-generated texts must be very different from each other (TV $> 0.5$ based on the figure). As $\mathcal{M}$ gets more similar to $\mathcal{H}$ (say, TV $< 0.2$), the performance of even the best-possible detector becomes unreliable (AUROC $< 0.7$). For some applications, say AI-text plagiarism, reliable detection should have a low false positive rate (say, $<0.01$) and a  high true positive rate (say, $>0.9$). Based on our theory, this cannot be achieved even when the overlap between the distributions is relatively low, say $11\%$ (or TV $= 0.9-0.01 = 0.89$, {based on equation \ref{eq:TPR_FPR_TV_bound} in Appendix \ref{app:proof_roc_bnd}).

%This indicates that distinguishing the text produced by AI from a human-generated one is a fundamentally difficult task as LLMs get powerful and more human-like.



Note that, for a watermarked model, the above bound can be close to one as the TV between the watermarked distribution and human-generated distribution can be high. Corollary~\ref{corollary:rephrase} in Appendix~\ref{app:corollaries} discusses how paraphrasing attacks can be effective in evading watermarks using Theorem~\ref{thm:ROC_bound}. In Appendix~\ref{app:tightness}, we also present a tightness analysis of the bound in Theorem~\ref{thm:ROC_bound}, where we show that for any distribution $\mathcal{H}$ there exists $\mathcal{M}$ and a detector $D$ for which the bound holds with equality. We also discuss general trade-offs between true positive and false positive rates of detection in Corollaries \ref{corollary:rephrasing_wm} and \ref{corollary:rephrasing_no_wm} in Appendix \ref{app:corollaries}. Theorem~\ref{thm:ROC_bound_computational} in Appendix~\ref{app:PRG_bound}  extends Theorem~\ref{thm:ROC_bound} to bound the AUROC of the best possible detector by a function of the TV distance between LLM outputs generated using pseudorandomness and human text distributions.

\begin{figure}[t]
\begin{minipage}{0.46\textwidth}
    \centering
        \includegraphics[width=0.85\textwidth]{images/rebuttal_lstm_vocab.png}
        \vspace{-3mm}
        \caption{Increasing model size reduces the exact TV between the true synthetic data distribution and the learned distribution. Error bars report standard deviations after 5 independent trials.}
    \label{fig:synthetic_tvd}
    \end{minipage}
    \hfill 
    \begin{minipage}{0.42\textwidth}
    \centering
        \includegraphics[width=1\textwidth]{images/tv.png}
        \vspace{-8mm}
        \caption{Estimated TV distances of %different
        GPT-2 output datasets from the WebText dataset using meta-token sequences of varying lengths. TV decreases with model size for each length.}
    \label{fig:tvd-gpt-synthetic}
    \end{minipage}
    \vspace{-8mm}
\end{figure}

% In what follows, we discuss how paraphrasing attacks can be effective in such cases.


% \textbf{General Trade-offs between True Positive and False Positive Rates.} Another way to understand the limitations of AI-generated text detectors is directly through the characterization of the trade-offs between true positive rates and false positive rates. Adapting inequality \ref{eq:TPR_FPR_bound}, we have the following corollaries: 


% \begin{corollary}
% \label{corollary:rephrasing_wm}
% For any watermarking scheme $W$,
% \begin{align*}
%     \Pr_{s_w\sim \mathcal{R}_{\mathcal{M}}(s)} [\text{$s_w$ is watermarked using $W$}] \leq &
%     \Pr_{s_w\sim \mathcal{R}_{\mathcal{H}}(s)}[\text{$s_w$ is watermarked using $W$}] \\
%     & + \mathsf{TV}(\mathcal{R_M}(s), \mathcal{R_H}(s)),
% \end{align*}
% where $\mathcal{R_M}(s)$ and $\mathcal{R_H}(s)$ are the distributions of rephrased sequences for $s$ produced by the paraphrasing model and humans, respectively.
% \end{corollary}


% Humans may have different writing styles. Corollary \ref{corollary:rephrasing_wm} indicates that if a rephrasing model resembles certain human text distribution $\mathcal{H}$ (i.e. $\mathsf{TV}(\mathcal{R_M}(s), \mathcal{R_H}(s))$ is small), then either certain people's writing will be detected falsely as watermarked (i.e. $\Pr_{s_w\sim \mathcal{R}_{\mathcal{H}}(s)} [\text{$s_w$ is watermarked using $W$}]$ is high) or the paraphrasing model can remove the watermark (i.e. $\Pr_{s_w\sim \mathcal{R}_{\mathcal{M}}(s)} [\text{$s_w$ is watermarked using $W$}]$ is low).


% \begin{corollary}
% \label{corollary:rephrasing_no_wm}
% For any AI-text detector $D$, 
% \begin{align*}
%     \Pr_{s\sim \mathcal{M}} [\text{$s$ is detected as AI-text by $D$}] \leq \Pr_{s\sim \mathcal{H}}[\text{$s$ is detected as AI-text by $D$}] + \mathsf{TV}(\mathcal{M}, \mathcal{H}),
% \end{align*}
% where $\mathcal{M}$ and $\mathcal{H}$ denote text distributions by the model and by humans, respectively.
% \end{corollary}

% Corollary \ref{corollary:rephrasing_no_wm} indicates that if a model resembles certain human text distribution $\mathcal{H}$ (i.e. $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ is small), then either certain people's writing will be detected falsely as AI-generated (i.e. $\Pr_{s\sim \mathcal{H}} [\text{$s$ is detected as AI-text by $D$}]$ is high) or the AI-generated text will not be detected reliably (i.e. $\Pr_{s\sim \mathcal{M}} [\text{$s$ is detected as AI-text by $D$}]$ is low).

% % \looseness -1
% These results demonstrate fundamental limitations for AI-text detectors, with and without watermarking schemes.
% In Appendix~\ref{app:tightness}, we present a tightness analysis of the bound in Theorem~\ref{thm:ROC_bound}, where we show that for any distribution $\mathcal{H}$ there exists $\mathcal{M}$ and a detector $D$ for which the bound holds with equality. We also discuss general trade-offs between true positive and false positive rates of detection in Corollaries \ref{corollary:rephrasing_wm} and \ref{corollary:rephrasing_no_wm} in Appendix \ref{app:corollaries}.


% \begin{figure}[t]
%   \centering
%   \captionsetup[subfigure]{labelformat=empty}
% \begin{subfigure}{.4\textwidth}
%   \centering
%   \includegraphics[width=\linewidth]{images/rebuttal_lstm_vocab.png}
%   \caption{}
%   \label{fig:synthetic_tvd1}
% \end{subfigure}% 
% \hspace{1cm}
% \begin{subfigure}{.4\textwidth}
%   \centering
%   \includegraphics[width=\linewidth]{images/rebuttal_lstm_seqlen.png}
%   \caption{}
%   \label{fig:synthetic_tvd2}
% \end{subfigure}\hspace{0.5cm}
% \vspace{-0.9cm}
% \caption{\textcolor{red}{Increasing model size reduces the exact TVD between the true synthetic data distribution and the learned distribution. Here, we use LSTMs to train on the synthetic toy dataset with varying vocabulary sizes (on the left) and sequence lengths (on the right). The trend of decrease in TVD as the model gets bigger is consistent in all settings. Error bars report standard deviations of TVD using trained models from 5 independent trials. \ak{I don't understand how the TV is decreasing w.r.t sequence length in the second plot. My intuition says that the TV for higher sequence lengths should be at least as high as the smaller ones.}}}
% \vspace{-2mm}
% \label{fig:synthetic_tvd}
% \end{figure}






% \subsection{Estimating TV distance between Human and AI Text Distributions}
% \label{sec:tv_estimation}



In studying the hardness of the detection problem, we consider the following assumption that for a given human-text distribution $\mathcal{H}$, more advanced LLMs mimicking $\mathcal{H}$ can lead to smaller TV. Thus, using Theorem \ref{thm:ROC_bound}, the detection problem becomes increasingly more difficult. This is the core argument of our hardness result on AI text detection. Although the underlying assumption seems to be intuitive given the capabilities of LLMs such as GPT-4 \citep{gpt4}, a precise analysis of this assumption is quite difficult because estimating the true TV of the text distributions from a finite set of samples is extremely challenging. Nevertheless, we provide some empirical evidence supporting this assumption using two sets of experiments. In all the experiments, we consistently observe that the TV distance estimates between human and AI text distributions reduce as language models get more advanced, indicating the increasing difficulty associated with AI text detection.\looseness=-1

\textbf{(i) Using synthetic text data.} We perform experiments on a toy synthetic text dataset where the exact TV distance can be calculated. We use the Markov assumption to generate the synthetic text data with sequence length 3 using a randomly generated token transition matrix for varying vocabulary sizes. We use single-layer LSTMs of different hidden unit sizes to train on a dataset of size 20,000 sampled from this synthetic data distribution using a default AdamW optimizer \citep{adamw}. We compute the learned token transition matrix for the LSTM output distribution using the softmax logit values of the trained model. Using transition matrices of both distributions, we compute the exact TV. Figure \ref{fig:synthetic_tvd} shows that the exact TV distances between the learned and true synthetic distributions reduce as the LSTM model size increases.


\textbf{(ii) Using projection.} For discrete distributions, the TV distance can be computed as 1/2 of the sum of the point-wise differences between their probability density functions (PDFs). While this is mathematically simple since texts can be considered as token sequences with bounded length, it is not practical to compute true TV distances directly through estimating PDFs due to the size of the sample space, which is approximately \textit{the size of the token set} to the power of \textit{sequence length}.
%{\bf ***} In this experiment, we divide the original token set into 5 parts and map each part of the tokens to a different meta-token. Since the size of the meta-token set is small, estimating PDFs becomes feasible. We collect the corresponding meta-token sequences of multiple lengths from both the WebText and AI-generated (GPT-2) texts.
%Figure \ref{fig:tvd-gpt-synthetic} shows the estimation of TV distances through PDF estimation for meta-token sequences, where we again observe that the TV estimate decreases with an increase in model size for all sequence lengths.
%\ak{[rewrite suggestion for ***]
To tackle this issue, we split the original token set into five roughly equal partitions and assign a meta-token to each partition.
Given a sequence of tokens from the original set, we construct a new sequence by replacing each token with the corresponding meta-token.
We estimate the PDFs of the sequences of meta-tokens created using texts from the WebText and GPT-2 output datasets.
Since the set of meta-tokens is significantly smaller than the original token set, estimating PDFs becomes much more tractable.
We then use these PDFs to estimate the total variation distances of the output distributions of different GPT-2 models (GPT-2-Small, GPT-2-Medium, GPT-2-Large, and GPT-2-XL) from the WebText dataset.
Figure \ref{fig:tvd-gpt-synthetic} plots these TV estimates for different sequence lengths, averaged over 30 runs of the experiment.
We observe that the TV distance consistently decreases with increasing model size for all sequence lengths.
%}


These experiments provide empirical evidence that more advanced LLMs can lead to smaller TV distances. Thus, based on Theorem~\ref{thm:ROC_bound}, reliable AI text detection would become increasingly difficult.






% \textcolor{red}{
% \textbf{(iii) Using TV lower bounds.} For distributions $\mathcal{H}$ and $\mathcal{M}$, the TVD between them is defined as the maximum difference between the probabilities assigned by them to any event $E$ over the sample space $\Omega$, i.e.,
% $\mathsf{TV}(\mathcal{H}, \mathcal{M}) = \max_E \big|\mathbb{P}_{s \sim \mathcal{H}}[s \in E] - \mathbb{P}_{s \sim \mathcal{M}}[s \in E]\big|.$ Thus, for any event $E$, this difference in probabilities is a valid {\it lower bound} on the total variation between the two distributions.
% Since we do not know the probability density functions for $\mathcal{H}$ and $\mathcal{M}$, solving the above maximization problem over the entire event space is intractable.
% Thus, we approximate the event space as a class of events defined by a neural network with parameters $\theta$ that maps a text sequence in $\Omega$ to a real number.
% The corresponding event $E_\theta$ is said to occur when the output of the neural network is above a threshold $\tau$.
% }

% \textcolor{red}{
% We train a RoBERTa large classifier~\cite{liu2019roberta} on samples from human and AI text distributions.
% Assuming the AI text distribution as the positive class, we pick a threshold for the output that maximizes the difference between the true positive rate and the false positive rate using samples from a validation set.
% Finally, we estimate the total variation as the difference between the probability of the output being above the threshold under the human text distribution and the AI text distribution using a test set.
% Figure~\ref{fig:gpt2-wt} plots the total variation for four GPT-2 models (small, medium, large, and XL) for four sequence lengths (25, 50, 75, and 100) estimated using a RoBERTa-large model.
% We observe that, as models grow in size, the TV estimates between human and AI-text distributions decrease.
% }

% We estimate the TVD between human text distribution (WebText) and the output distribution of several models from OpenAI's GPT-2 series.\footnote{\url{https://github.com/openai/gpt-2-output-dataset}}
% For distributions $\mathcal{H}$ and $\mathcal{M}$, the total variation between them is defined as the maximum difference between the probabilities assigned by them to any event $E$ over the sample space $\Omega$, i.e.,
% \[\mathsf{TV}(\mathcal{H}, \mathcal{M}) = \max_E \big|\mathbb{P}_{s \sim \mathcal{H}}[s \in E] - \mathbb{P}_{s \sim \mathcal{M}}[s \in E]\big|.\]
% Thus, for any event $E$, this difference in probabilities is a valid {\it lower bound} on the total variation between the two distributions.
% Since we do not know the probability density functions for $\mathcal{H}$ and $\mathcal{M}$, solving the above maximization problem over the entire event space is intractable.
% Thus, we approximate the event space as a class of events defined by a neural network with parameters $\theta$ that maps a text sequence in $\Omega$ to a real number.
% The corresponding event $E_\theta$ is said to occur when the output of the neural network is above a threshold $\tau$.

% \begin{wrapfigure}{r}{0.26\textwidth}
%     \vspace{-4mm}
%     \hspace{-4mm}
%     \includegraphics[width=1.12\linewidth]{images/tv_roberta-large_small-117M.png}
%     %\vspace{-4mm}
%     \caption{TV between WebText and outputs of GPT-2 models (small, medium, large, and XL) for varying sequence lengths.}
%     \label{fig:gpt2-wt}
%     \vspace{-6mm}
% \end{wrapfigure}


% We train a RoBERTa large classifier~\cite{liu2019roberta} on samples from human and AI text distributions.
% Assuming the AI text distribution as the positive class, we pick a threshold for the output that maximizes the difference between the true positive rate and the false positive rate using samples from a validation set.
% Finally, we estimate the total variation as the difference between the probability of the output being above the threshold under the human text distribution and the AI text distribution using a test set.
% Figure~\ref{fig:gpt2-wt} plots the total variation for four GPT-2 models (small, medium, large, and XL) for four sequence lengths (25, 50, 75, and 100) estimated using a RoBERTa-large model.
% We observe that, as models grow in size, the TV estimates between human and AI-text distributions decrease.
% This indicates that as language models become more powerful, their output becomes more similar to human text.
% We include similar results for models in the GPT-3 series using WebText and ArXiv abstracts datasets as human text in the appendix.