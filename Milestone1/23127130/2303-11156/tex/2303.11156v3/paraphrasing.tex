\section{Evading AI-Detectors using Paraphrasing Attacks}
\label{sec:aigentextnotdetected}

In this section, we first present the experimental setup for our paraphrasing attacks in \S\ref{sec:exp-setup}. We also provide experiments in \S\ref{sec:quality} showing that our novel recursive paraphrasing attacks only lead to a slight degradation in text quality. \S\ref{sec:watermark_evade} and \S\ref{sec:paraphrase_nonwatermark} show the effect of the paraphrasing attacks on watermarking and non-watermarking detectors, respectively.


\subsection{Setup and Paraphrasing Methods}
\label{sec:exp-setup}


\begin{wrapfigure}{r}{0.35\textwidth}
    \centering
    \vspace{-6mm}
    \hspace{5mm}
    \includegraphics[width=0.6\linewidth]{images/rec-para.png}
    \vspace{-2mm}
    \caption{Recursive paraphrasing}
    \label{fig:rec-para}
    % \vspace{-2mm}
\end{wrapfigure}
We use the ``document'' features of the XSum dataset \citep{xsum} containing 1000 long news articles ($\sim$300 tokens in length) for our experiments. In Appendix~\ref{app:moreexps}, we perform experiments with additional datasets -- a medical text dataset PubMedQA \citep{jin-etal-2019-pubmedqa} and a dataset with articles from 10 different domains Kafkai \citep{kafkai}. 
As target LLMs, we use OPT-1.3B and OPT-13B \citep{opt} language models with 1.3B and 13B parameters, respectively.
In Appendix~\ref{app:moreexps}, we also evaluate our attacks with GPT-2 Medium \citep{gpt2} as the target model.
We use three different neural network-based paraphrasers -- DIPPER with 11B parameters \citep{krishna2023paraphrasing}, LLaMA-2-7B-Chat with 7B parameters \citep{llama}, and T5-based paraphraser \citep{prithivida2021parrot} with 222M parameters.
Suppose a passage $S = (s_1, s_2, ..., s_n)$ where $s_i$ is the $i^{th}$ sentence. DIPPER and LLaMA-2-7B-Chat paraphrase $S$ to be $S' = f_{strong}(S)$ in one-shot while the light-weight T5-based paraphraser would output $S' = (f_{weak}(s_1), f_{weak}(s_2), ..., f_{weak}(s_n))$ where they can only paraphrase sentence-by-sentence. DIPPER also has the ability to input a context prompt text $C$ to generate higher-quality paraphrasing $S' = f_{strong}(S, C)$. We can also vary two different hyperparameters of DIPPER to generate a diverse number of paraphrases for a single input passage.





\begin{table}[t]
    \centering\renewcommand\cellalign{lc}
    \setcellgapes{3pt}\makegapedcells
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{c|c||c|c|c|c|c||c}
    \toprule
    \multicolumn{2}{c||}{\textbf{ppi}} & i=1 & i=2 & i=3 & i=4 & i=5 & All ppi \\ \midrule \midrule
    \multirow{2}{*}{\makecell{\textbf{Content}\\\textbf{preservation}}} & Avg. rating & {$4.0\pm0.8$} & {$4.1\pm0.8$} & {$3.9\pm0.9$} & {$4.2\pm0.9$} & {$3.7\pm1.1$} & {$4.0\pm0.9$} \\ \cline{2-8}
    {} & Ratings 5\&4 & {$70.2\%$} & {$77.2\%$} & {$63.2\%$} & {$80.0\%$} & {$61.4\%$} & {{$70.4\%$}} \\ \midrule
    \multirow{2}{*}{\makecell{\textbf{Grammar or}\\\textbf{text quality}}} & Avg. rating & {$4.28 \pm 0.67$} & {$4.12 \pm 0.50$} & {$4.12 \pm 0.53$} & {$4.11 \pm 0.64$} & {$4.07 \pm 0.53$} & {{$ 4.14\pm 0.58$}}\\ \cline{2-8}
    {} & Ratings 5\&4 & {$87.72\%$} & {$92.98\%$} & {$91.23\%$} & {$84.21\%$} & {$89.47\%$} & {{$89.12\%$}}\\ 
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Summary of the MTurk human evaluation study on content preservation and grammar or text quality of the recursive paraphrases with DIPPER that we use for our attacks. Ratings are on a Likert scale of 1 to 5. See Appendix~\ref{app:human_study} for details.}
    \label{tab:summary-mturk}
    
    \vspace{2mm}
    \centering\renewcommand\cellalign{lc}
    \setcellgapes{3pt}\makegapedcells
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{c|c||c|c|c|c|c||c}
    \toprule
    \multicolumn{2}{c||}{\textbf{ppi}} & i=1 & i=2 & i=3 & i=4 & i=5 & {All ppi} \\ \midrule \midrule
    \multirow{2}{*}{\makecell{\textbf{Content}\\\textbf{preservation}}} & Avg. rating & {$4.37\pm0.63$} & {$4.18\pm0.67$} & {$3.93\pm0.71$} & {$3.9\pm0.75$} & {$3.85\pm0.78$} & {{$4.05\pm0.2$}} \\ \cline{2-8}
    {} & Ratings 5\&4 & {$91.67\%$} & {$85.0\%$} & {$80.0\%$} & {$78.3\%$} & {$80.0\%$} & {{$83.0\%$}} \\ \midrule
    \multirow{2}{*}{\makecell{\textbf{Grammar or}\\\textbf{text quality}}} & Avg. rating & {$4.62 \pm 0.55$} & {$4.28\pm 0.73$} & {$4.26 \pm 0.65$} & {$4.22 \pm 0.64$} & {$4.17\pm0.74$} & {{$ 4.31\pm 0.35$}}\\ \cline{2-8}
    {} & Ratings 5\&4 & {$96.67\%$} & {$83.33\%$} & {$88.33\%$} & {$88.3\%$} & {$83.33\%$} & {{$88.0\%$}}\\ 
    \bottomrule
    \end{tabular}
    \end{adjustbox} 
    \caption{
    Summary of the MTurk human evaluation study of the recursive paraphrases with LLaMA-2-7B-Chat.}
    \label{tab:summary-mturk-llama}

    \vspace{2mm}
    \begin{adjustbox}{max width=.76\textwidth}
    \begin{tabular}{l|l|c|c|c|c|c|c}
    \toprule
    \multicolumn{1}{c|}{\textbf{Paraphraser}} &
      \multicolumn{1}{c|}{\textbf{Evaluation}} &
      \textbf{AI text} &
      \textbf{pp1} &
      \textbf{pp2} &
      \textbf{pp3} &
      \textbf{pp4} &
      \textbf{pp5} \\ \midrule \midrule
    \multirow{2}{*}{DIPPER}     & Mean perplexity       & 5.2  & 7.7  & 7.8  & 8.5  & 7.7  & 8.7    \\ \cmidrule{2-8} 
                                & QA performance & 97\% & 97\% & 96\% & 96\% & 96\% & 95.5\% \\ \midrule
    \multirow{2}{*}{LLaMA-2-7B-Chat} & Mean perplexity       & 5.2  & 8.1  & 9.3  & 9.0  & 10.3 & 10.5   \\ \cmidrule{2-8} 
                                & QA Performance & 97\% & 97\% & 97\% & 97\% & 97\% & 97\%   \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{
    Automated evaluation of the text quality of recursive paraphrases using perplexity measures with respect to OPT-13B and question-answering benchmark accuracy.}
    \label{tab:quality-eval}
    
\end{table}




We use DIPPER and LLaMA-2-7B-Chat for recursive paraphrasing attacks since they provide high-quality paraphrasing when compared to the 222M parameter T5 model.
Let an LLM $L$ generate AI text output $S = L(C)$ for an input prompt $C$. DIPPER or LLaMA-2-7B-Chat can be used to generate a paraphrase \texttt{pp1} $=f_{strong}(S, C)$. This paraphrasing can be performed in recursion (see Figure~\ref{fig:rec-para}). That is, \texttt{ppi} $= f_{strong}($\texttt{pp(i-1)}$, C)$. 

While DIPPER is explicitly trained to be a paraphraser, LLaMA-2-7B-Chat is an instruction-tuned model for chat purposes.
We design a system prompt (see Appendix~\ref{app:systemprompts}) with the LLaMA-2-7B-Chat model to use it as a paraphraser.
In \S\ref{sec:watermark_evade} and \S\ref{sec:paraphrase_nonwatermark}, we show that recursive paraphrasing is effective in evading the strong watermark and retrieval-based detectors when compared to a single round of paraphrasing. 
Using human and other automated evaluation techniques in \S\ref{sec:quality}, we show that our recursive paraphrasing method only degrades the text quality slightly.

\subsection{Quality of the Paraphrases}
\label{sec:quality}

% \noindent{\bf Quality of the paraphrased passages.}
In order to reliably study the quality of the recursive paraphrases we use in our experiments using DIPPER and LLaMA-2-7B-Chat, we perform human evaluations using MTurk and other automated techniques. 
The AI-text used in this study is generated using a watermarked OPT-13B model. 
Tables~\ref{tab:summary-mturk}~and~\ref{tab:summary-mturk-llama} provide a summary of the study.  We investigate the content preservation and text quality or grammar of the recursive paraphrases with respect to the AI-generated texts (see Tables~\ref{tab:mturk-content}-\ref{tab:mturk-grammar-llama} in Appendix \ref{app:human_study} for more details). 
{\bf In terms of content preservation with DIPPER, \bm{$70\%$} of the paraphrases were rated high quality and \bm{$23\%$} somewhat equivalent. In terms of text quality or grammar, \bm{$89\%$} of the paraphrases were rated high quality.} 
On a Likert scale of 1 to 5, the DIPPER paraphrases that we use received an average rating of $4.14 \pm 0.58$ for text quality or grammar and $4.0 \pm 0.9$ for content preservation. 
\textbf{Similarly, \bm{$83\%$} of the recursive paraphrases we obtain with LLaMA-2-7B-Chat were rated high quality.}
See Appendix \ref{app:human_study} for more details on the human study.


For automated text quality evaluations, we use perplexity measures and a question-answering (QA) benchmark in Table~\ref{tab:quality-eval}.  
We measure the perplexity scores using OPT-13B. 
As shown in the table, the perplexity scores degrade slightly from 5.5 to 8.7 and 10.5, respectively, for DIPPER and LLaMA-2-7B-Chat after 5 rounds of paraphrasing.
We also use a QA benchmark SQuAD-v2 \citep{squadv2} to evaluate the effect of recursive paraphrasing. For this, we use two hundred data points from SQuAD-v2. Each data point consists of a context passage, a question, and an answer.
We evaluate a QA model on the SquAD-v2 benchmark to observe that it achieves $97\%$ accuracy in the QA task.
For the QA model, we use the LLaMA-2-13B-Chat model with a carefully written system prompt (see Appendix~\ref{app:systemprompts}).
To evaluate the quality of paraphrases, we paraphrase the context passages recursively and use these to evaluate the QA accuracy with the QA model.
If the QA model can answer the question correctly based on the paraphrased context, then the information is preserved in the paraphrase.
As we observe, the QA performance with recursive paraphrasing is similar to that with the clean context passage.
These results confirm that AI text detectors can be effectively attacked using recursive paraphrasing with only a slight degradation in text quality. 




\begin{figure}[h!]
\centering
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, left}{images/wm_opt13b_dipper.png}
  % \vspace{-2mm}
  \caption{Recursive paraphrasing with DIPPER}
  \label{fig:watermark-opt13-dipper}
\end{subfigure}% 
\hfill
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, right}{images/wm_opt13b_llama.png}
  % \vspace{-2mm}
  \caption{Recursive paraphrasing with LLaMA-2-7B-Chat}
  \label{fig:watermark-opt13-llama}
\end{subfigure}
% \vspace{-2mm}
\caption{ROC plots for soft watermarking with recursive paraphrasing attacks. AUROC, TPR@1$\%$FPR, and perplexity scores measured using OPT-13B are given in the legend. The target LLM OPT-13B is used to generate watermarked output that are 300 tokens in length.}
\label{fig:watermarkroc-op13b}
\vspace{-0mm}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, left}{images/rebuttal_wm_rec_paraphrase.png}
  % \vspace{-2mm}
  \caption{Watermarked text with mean token length 300}
  \label{fig:watermarkroc1}
\end{subfigure}% 
\hfill
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, right}{images/rebuttal_wm_rec_paraphrase_vary_tokens.png}
  % \vspace{-2mm}
  \caption{Watermarked text with varying token lengths}
  \label{fig:watermarkroc2}
\end{subfigure}
% \vspace{-2mm}
\caption{ROC plots for soft watermarking with recursive paraphrasing attacks. AUROC, TPR@1$\%$FPR, and perplexity scores measured using OPT-13B are given in the legend. The target LLM is OPT-1.3B. (a) Even for 300 tokens long watermarked passages, recursive paraphrasing is effective. As paraphrasing rounds proceed, detection rates degrade significantly with a slight trade-off in text quality. (b) Attacking watermarked passages become easier as their length reduces.}
\label{fig:watermarkroc}
\vspace{-0mm}
\end{figure}

\subsection{Paraphrasing Attacks on Watermarked AI Text}
\label{sec:watermark_evade}

In this section, we evaluate our recursive paraphrasing attacks on the soft watermarking scheme proposed in \cite{kirchenbauer2023watermark}.  Soft watermarking encourages LLMs to output token $s^{(t)}$ at time-step $t$ that belongs to a ``green list''. The green list for $s^{(t)}$ is created using a private pseudo-random generator that is seeded with the prior token $s^{(t-1)}$. A watermarked output from the LLM is designed to have tokens that are majorly selected from the green list. Hence, a watermark detector with the pseudo-random generator checks the number of \textit{green} tokens in a candidate passage to detect whether it is watermarked or not. 
Here, we target watermarked OPT-13B with 13B parameters in Figure~\ref{fig:watermarkroc-op13b} and watermarked OPT-1.3B in Figure~\ref{fig:watermarkroc} for our experiments. In Appendix~\ref{app:moreexps_wm}, we also evaluate our attacks on GPT-2 Medium \citep{gpt2} and other datasets -- PubMedQA \citep{jin-etal-2019-pubmedqa} and Kafkai \citep{kafkai}. 

\noindent{\bf Dataset.} We perform our experiments on 2000 text passages that are around 300 tokens in length \textcolor{black}{(1000 passages per human and AI text classes)}. We pick 1000 long news articles from the XSum ``document'' feature. For each article, the first $\sim$300 tokens are input to the target OPT-1.3B \textcolor{black}{to generate 1000 watermarked AI text passages that are each $\sim$300 tokens in length. The second 300 tokens from the 1000 news articles in the dataset are treated as baseline human text.} We note that our considered dataset has more and longer passages compared to the experiments in \cite{kirchenbauer2023watermark}.

\noindent{\bf Detection results after paraphrasing attack.}
Weaker paraphrasing attacks discussed in \cite{kirchenbauer2023watermark} are not effective in removing watermarks. They perform ``span replacement'' by replacing random tokens (in-place) using a language model. 
However, after a single round of paraphrasing (\texttt{pp1}) with a watermarked OPT-13B as the target LLM, TPR@$1\%$FPR of watermark detector degrades from $99.8\%$ to $80.7\%$ and $54.6\%$, respectively, with DIPPER and LLaMA-2-7B-Chat paraphrasers. 
We also show that our stronger recursive paraphrasing attacks can effectively evade watermark detectors with only a slight degradation in text quality. 
As shown in Figures~\ref{fig:watermarkroc-op13b}-\ref{fig:watermarkroc}, the recursive paraphrase attack further degrades the detection rate of the detector to below $20\%$ after 5 rounds of paraphrasing (\texttt{pp5}). 
Note that in all the settings \texttt{pp2} or 2 rounds of paraphrasing is sufficient to degrade TPR@$1\%$FPR to below $50\%$. 
As shown in Figure~\ref{fig:watermarkroc-op13b}, DIPPER shows a clearer and more consistent trend in improving attack performance over recursions of paraphrasing in comparison to LLaMA-2. This is because DIPPER is trained explicitly to be a paraphraser with hyperparameters that can control the quality of paraphrasing. Therefore, we mainly employ DIPPER for our recursive paraphrase attacks.
\texttt{Best of ppi} in the figure refers to the method where, for each passage, we select the paraphrase out of all the \texttt{ppi}'s that has the worst detector score. For \texttt{Best of ppi} with OPT-1.3B,  the detection rate reduces drastically from $99.8\%$ to $4.0\%$ with only a trade-off of $1.5$ in the perplexity score (Figure~\ref{fig:watermarkroc1}). 
\texttt{Best of ppi}, unlike the \texttt{ppi} attacks, assume black box query access to the detector.
Figure \ref{fig:watermarkroc2} shows that the watermarking detector becomes weaker as the length of the watermarked text reduces. Note that for watermarked texts that are 50 or 100 tokens long, the detection performance after the recursive paraphrasing attack is similar to that of a random detector. We provide examples of paraphrased text that we use for our attacks in Appendix \ref{app:example-paraphrases}.



\subsection{Paraphrasing Attacks on Non-Watermarked AI Text}
\label{sec:paraphrase_nonwatermark}

Neural network-based trained detectors such as RoBERTa-Large-Detector from OpenAI \citep{openaidetectgpt2} are trained or fine-tuned for binary classification with datasets containing human and AI-generated texts. Zero-shot classifiers leverage specific statistical properties of the source LLM outputs for their detection. Retrieval-based methods search for a candidate passage in a database that stores the LLM outputs. Here, we perform experiments on these non-watermarking detectors to show they are vulnerable to our paraphrasing attack.\looseness=-1


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/nonwatermark_roc_new.png}
    \vspace{-2mm}
   \caption{ROC curves for various trained and zero-shot detectors. \textbf{Left:} Without attack. \textbf{Middle:} After paraphrasing attack using T5-based paraphraser. The performance of zero-shot detectors drops significantly. \textbf{Right:} Here, we assume we can query the detector ten times for the paraphrasing attack. We generate ten paraphrasings for each passage and query multiple times to evade detection. Notice how all detectors have low TPR@$1\%$FPR. In the plot legend -- \texttt{perturbation} refers to the zero-shot methods in \cite{mitchell2023detectgpt}; \texttt{threshold} refers to the zero-shot methods in \cite{solaiman2019release, gehrmann2019gltr, ippolito2019automatic}; \texttt{roberta} refers to OpenAI's trained detectors \citep{openaidetectgpt2}. The \newcontent{TPR@$1\%$FPR} scores of different detectors before the attack, after the attack, and after the attack with multiple queries, respectively, are provided in the plot legend.}
    \label{fig:roc_nonwatermark}
    \vspace{-5mm}
\end{figure}



\begin{wrapfigure}{r}{0.5\textwidth}
        \centering
        \vspace{-8mm}
        \includegraphics[width=1.03\linewidth]{images/IR_attack.png}
        \caption{\textcolor{black}{Recursive paraphrasing breaks the retrieval-based detector \citep{krishna2023paraphrasing} with only slight degradation in text quality. \texttt{ppi} refers to $\texttt{i}$ recursion(s) of paraphrasing. Numbers next to markers denote the perplexity scores of the paraphraser output.}}
        \vspace{-4mm}
    \label{fig:ir-attack}
\end{wrapfigure}
\textbf{Trained and Zero-shot detectors.} We use a pre-trained GPT-2 Medium
% \footnote{\url{https://huggingface.co/gpt2-medium}}
model \citep{gpt2} with 355M parameters as the target LLM to evaluate our attack on \textcolor{black}{1000} long passages from the XSum dataset \citep{xsum}. We use the T5-based paraphrasing model \citep{prithivida2021parrot} with 222M parameters to rephrase the \textcolor{black}{1000} output texts generated using the target GPT-2 Medium model. Figure~\ref{fig:roc_nonwatermark} shows the effectiveness of the paraphrasing attack over these detectors. {\bf The AUROC scores of DetectGPT \citep{mitchell2023detectgpt} drop from \bm{$96.5\%$} (before the attack) to \bm{$59.8\%$} (after the attack).} Note that AUROC of $50\%$ corresponds to a random detector. The rest of the zero-shot detectors \citep{solaiman2019release, gehrmann2019gltr, ippolito2019automatic} also perform poorly after our attack. Though the performance of the trained neural network-based detectors \citep{openaidetectgpt2} is better than that of zero-shot detectors, they are also not reliable. For example,\newcontent{ TPR@$1\%$FPR of OpenAI's RoBERTa-Large-Detector drops from $100\%$ to around $92\%$ after our attack.}






In another setting, we assume the attacker may have multiple access to the detector. That is, the attacker can query the detector with an input AI text passage, and the detector would reveal the detection score to the attacker. For this scenario, we generate ten different paraphrases for an input passage and query the detector for the detection scores. For each AI text passage, we then select the paraphrase with the worst detection score for evaluating the ROC curves. As shown in Figure~\ref{fig:roc_nonwatermark}, \newcontent{\bf with multiple queries to the detector, an adversary can paraphrase more efficiently to bring down TPR@\bm{$1\%$}FPR of the RoBERTa-Large-Detector from \bm{$100\%$} to \bm{$80\%$}.} In Appendix~\ref{app:moreexps_zs}, we show more experiments with more datasets and target LLMs.



\textbf{Retrieval-based detectors.} Detector in \citet{krishna2023paraphrasing} is designed to be robust against paraphrase attacks. However, we show that they can suffer from the recursive paraphrase attacks that we develop using DIPPER. 
\textcolor{black}{We use 2000 passages (1000 generated by OPT-1.3B and 1000 human passages) from the XSum dataset. AI outputs are stored in the AI database by the detector. As shown in Figure \ref{fig:ir-attack}, this detector detects almost all of the AI outputs even after a round of paraphrasing. However, \textbf{the detection accuracy drops below \bm{$\sim60\%$} after five rounds of recursive paraphrasing.} As marked in the plot, the perplexity score of the paraphrased text only degrades by $1.7$ at a detection accuracy of $\sim60\%$.} 
Moreover, retrieval-based detectors are concerning since they might lead to \textbf{serious privacy issues} from storing users' LLM conversations. In Appendix~\ref{app:moreexps_retrieval}, we show more experiments with more datasets and target LLMs.