
\section{Hardness of Reliable AI Text Detection}
\label{sec:impossibilityresult}



\begin{wrapfigure}{r}{0.55\textwidth}
    \centering
    \vspace{-9mm}
        \includegraphics[width=1.05\linewidth]{images/roc_bound.png}
        \caption{Comparing the performance, in terms of AUROC, of the best possible detector to that of the baseline performance corresponding to a random classifier.}
    \label{fig:roc_bound}
    \vspace{-5mm}
\end{wrapfigure}
In this section, we formally upper bound the AUROC of an arbitrary detector in terms of the TV between the distributions for $\mathcal{M}$ (e.g., AI text) and $\mathcal{H}$ (e.g., human text) over the set of all possible text sequences $\Omega$. We note that this result holds for any two arbitrary
distributions $\mathcal{H}$ and $\mathcal{M}$. For example, $\mathcal{H}$ could be the text distribution for a person or group, while $\mathcal{M}$ could be the output text distribution of a general LLM or an LLM trained by an adversary to mimic
the text of a particular set of people. 

We use $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ to denote the TV between these two distributions and model a detector as a function $D: \Omega \rightarrow \mathbb{R}$ that maps every sequence in $\Omega$ to a real number.
% detection parameter $\gamma$.
Sequences are classified into AI-generated or human-generated by applying a threshold $\gamma$ on this number.
By adjusting the parameter $\gamma$, we can tune the sensitivity of the detector to AI and human-generated texts to obtain an ROC curve.



\begin{theorem}
\label{thm:ROC_bound}
The area under the ROC of any detector $D$ is bounded as
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2}.\]
\end{theorem}

The proof is deferred to Appendix~\ref{app:proof_roc_bnd}. Figure~\ref{fig:roc_bound} shows how the above bound grows as a function of the TV distance.
This theorem states that as the TV distance between AI and human text distributions reduces, the AUROC of the best possible detector decreases. Based on our theory, an adversary can use advanced LLMs to mimic human text to  reduce the TV distance between human and AI text distributions to evade text detection systems.


For a detector to have a good performance (say, AUROC $> 0.9$), the distributions of human and AI-generated texts must be very different from each other (TV $> 0.5$ based on the figure). As $\mathcal{M}$ gets more similar to $\mathcal{H}$ (say, TV $< 0.2$), the performance of even the best-possible detector becomes unreliable (AUROC $< 0.7$). For some applications, say AI-text plagiarism, reliable detection should have a low false positive rate (say, $<0.01$) and a  high true positive rate (say, $>0.9$). Based on our theory, this cannot be achieved even when the overlap between the distributions is relatively low, say $11\%$ (or TV $= 0.9-0.01 = 0.89$, {based on equation \ref{eq:TPR_FPR_TV_bound} in Appendix \ref{app:proof_roc_bnd}).


Note that, for a watermarked model, the above bound can be close to one as the TV between the watermarked distribution and human-generated distribution can be high. Corollary~\ref{corollary:rephrase} in Appendix~\ref{app:corollaries} discusses how paraphrasing attacks can be effective in evading watermarks using Theorem~\ref{thm:ROC_bound}. In Appendix~\ref{app:tightness}, we also present a tightness analysis of the bound in Theorem~\ref{thm:ROC_bound}, where we show that for any distribution $\mathcal{H}$ there exists $\mathcal{M}$ and a detector $D$ for which the bound holds with equality. We also discuss general trade-offs between true positive and false positive rates of detection in Corollaries \ref{corollary:rephrasing_wm} and \ref{corollary:rephrasing_no_wm} in Appendix \ref{app:corollaries}. Theorem~\ref{thm:ROC_bound_computational} in Appendix~\ref{app:PRG_bound}  extends Theorem~\ref{thm:ROC_bound} to bound the AUROC of the best possible detector by a function of the TV distance between LLM outputs generated using pseudorandomness and human text distributions.

\begin{figure}[t]
\begin{minipage}{0.52\textwidth}
    \centering
        \includegraphics[width=0.85\textwidth]{images/rebuttal_lstm_vocab.png}
        \caption{Increasing model size reduces the exact TV between the true synthetic data distribution and the learned distribution in all settings. Error bars report standard deviations after 5 independent trials.}
    \label{fig:synthetic_tvd}
    \end{minipage}
    \hfill 
    \begin{minipage}{0.45\textwidth}
    \centering
        \includegraphics[width=1\textwidth]{images/tv.png}
        \caption{Estimated TV distances of %different
        GPT-2 output datasets from the WebText dataset using meta-token sequences of varying lengths. TV decreases with model size for each length.}
    \label{fig:tvd-gpt-synthetic}
    \end{minipage}
    \vspace{-5mm}
\end{figure}


In studying the hardness of the detection problem, we consider the following assumption that for a given human-text distribution $\mathcal{H}$, more advanced LLMs mimicking $\mathcal{H}$ can lead to smaller TV. Thus, using Theorem \ref{thm:ROC_bound}, the detection problem becomes increasingly more difficult. This is the core argument of our hardness result on AI text detection. Although the underlying assumption seems to be intuitive given the capabilities of LLMs such as GPT-4 \citep{gpt4}, a precise analysis of this assumption is quite difficult because estimating the true TV of the text distributions from a finite set of samples is extremely challenging. Nevertheless, we provide some empirical evidence supporting this assumption using two sets of experiments. In all the experiments, we consistently observe that the TV distance estimates between human and AI text distributions reduce as language models get more advanced, indicating the increasing difficulty associated with AI text detection.

\textbf{(i) Using synthetic text data.} We perform experiments on a toy synthetic text dataset where the exact TV distance can be calculated. We use the Markov assumption to generate the synthetic text data with sequence length 3 using a randomly generated token transition matrix for varying vocabulary sizes. We use single-layer LSTMs of different hidden unit sizes to train on a dataset of size 20,000 sampled from this synthetic data distribution using a default AdamW optimizer \citep{adamw}. We compute the learned token transition matrix for the LSTM output distribution using the softmax logit values of the trained model. Using transition matrices of both distributions, we compute the exact TV. Figure \ref{fig:synthetic_tvd} shows that the exact TV distances between the learned and true synthetic distributions reduce as the LSTM model size increases.


\textbf{(ii) Using projection.} For discrete distributions, the TV distance can be computed as 1/2 of the sum of the point-wise differences between their probability density functions (PDFs). While this is mathematically simple since texts can be considered as token sequences with bounded length, it is not practical to compute true TV distances directly through estimating PDFs due to the size of the sample space, which is approximately \textit{the size of the token set} to the power of \textit{sequence length}.
To tackle this issue, we split the original token set into five roughly equal partitions and assign a meta-token to each partition.
Given a sequence of tokens from the original set, we construct a new sequence by replacing each token with the corresponding meta-token.
We estimate the PDFs of the sequences of meta-tokens created using texts from the WebText and GPT-2 output datasets.
Since the set of meta-tokens is significantly smaller than the original token set, estimating PDFs becomes much more tractable.
We then use these PDFs to estimate the total variation distances of the output distributions of different GPT-2 models (GPT-2-Small, GPT-2-Medium, GPT-2-Large, and GPT-2-XL) from the WebText dataset.
Figure \ref{fig:tvd-gpt-synthetic} plots these TV estimates for different sequence lengths, averaged over 30 runs of the experiment.
We observe that the TV distance consistently decreases with increasing model size for all sequence lengths.


These experiments provide empirical evidence that more advanced LLMs can lead to smaller TV distances. Thus, based on Theorem~\ref{thm:ROC_bound}, reliable AI text detection would become increasingly difficult.


