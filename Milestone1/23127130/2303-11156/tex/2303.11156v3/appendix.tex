\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage{iclr2024_conference}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage[dvipsnames]{xcolor}         % colors
\usepackage{wrapfig}
\usepackage{float}
\usepackage{todonotes}
\usepackage{textcomp}

%%%%%%%%%% Custom Packages and Commands %%%%%%%%%%
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{statement}{Statement}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage[bb=dsserif]{mathalpha}

\usepackage{wrapfig}
\usepackage{soul}
\usepackage{makecell}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{M}[1]{>{\arraybackslash}m{#1}}

\usepackage{xr}
\externaldocument{main}

\renewcommand\thesection{\Alph{section}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\newcommand{\ak}[1]
{\textcolor{blue}{AK: #1}}
\newcommand{\wwx}[1]
{\textcolor{cyan}{wenxiao: #1}}
\newcommand{\vs}[1]
{\textcolor{teal}{VS: #1}}
\newcommand{\sriram}[1]
{\textcolor{purple}{Sriram: #1}}
\newcommand{\SF}[1]
{\textcolor{red}{SF: #1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\appendix
% \section{Appendix}
% \maketitle

\section{Proof of Theorem~\ref{thm:ROC_bound}}
\label{sec:proof_roc_bnd}
\begin{theorem}
The area under the ROC of any detector $D$ is bounded as
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2}.\]
\end{theorem}

\begin{proof}
The ROC is a plot between the true positive rate (TPR) and the false positive rate (FPR) which are defined as follows:
\begin{align*}
    \mathsf{TPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{M}}[D(s) \geq \gamma]\\
    \text{and } \mathsf{FPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{H}}[D(s) \geq \gamma],
\end{align*}
where $\gamma$ is some classifier parameter.
We can bound the difference between the $\mathsf{TPR}_\gamma$ and the $\mathsf{FPR}_\gamma$ by the total variation between $M$ and $H$:
\begin{align}
    |\mathsf{TPR}_\gamma - \mathsf{FPR}_\gamma| &= \left| \mathbb{P}_{s \sim \mathcal{M}}[D(s) \geq \gamma] - \mathbb{P}_{s \sim \mathcal{H}}[D(s) \geq \gamma] \right| \leq \mathsf{TV}(\mathcal{M}, \mathcal{H})\\
    \mathsf{TPR}_\gamma &\leq \mathsf{FPR}_\gamma + \mathsf{TV}(\mathcal{M}, \mathcal{H}).
    \label{eq:TPR_FPR_bound}
\end{align}
Since the $\mathsf{TPR}_\gamma$ is also bounded by 1 we have:
\begin{align}
\label{eq:tpr_bound}
\mathsf{TPR}_\gamma \leq \min(\mathsf{FPR}_\gamma + \mathsf{TV}(\mathcal{M}, \mathcal{H}), 1).
\end{align}
Denoting $\mathsf{FPR}_\gamma$, $\mathsf{TPR}_\gamma$, and $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ with $x$, $y$, and $tv$ for brevity, we bound the AUROC as follows:
\allowdisplaybreaks
\begin{align*}
    \mathsf{AUROC}(D) = \int_0^1 y \; dx &\leq \int_0^1 \min(x + tv, 1) dx\\
    &= \int_0^{1 -tv} (x + tv) dx + \int_{1-tv}^1 dx\\
    &= \left| \frac{x^2}{2} + tvx \right|_0^{1-tv} + \left| x \right|_{1-tv}^1\\
    &= \frac{(1-tv)^2}{2} + tv(1-tv) + tv\\
    &= \frac{1}{2} + \frac{tv^2}{2} - tv + tv - tv^2 + tv\\
    &= \frac{1}{2} + tv - \frac{tv^2}{2}.
\end{align*}
\end{proof}

\section{Tightness Analysis for Theorem~\ref{thm:ROC_bound}}
\label{sec:tightness}
In this section, we show that the bound in Theorem~\ref{thm:ROC_bound} is tight.
For a given distribution of human-generated text sequences $\mathcal{H}$, we construct an AI-text distribution $\mathcal{M}$ and a detector $D$ such that the bound holds with equality.
Define sublevel sets of the probability density function of the distribution of human-generated text $\mathsf{pdf}_\mathcal{H}$ over the set of all sequences $\Omega$ as follows:
\[\Omega_\mathcal{H}(c) = \{s \in \Omega \mid \mathsf{pdf}_\mathcal{H}(s) \leq c\}\]
where $c \in \mathbb{R}$.
Assume that, $\Omega_\mathcal{H}(0)$ is not empty.
Now, consider a distribution $\mathcal{M}$, with density function $\mathsf{pdf}_\mathcal{M}$, which has the following properties:
\begin{enumerate}
    \item The probability of a sequence drawn from $\mathcal{M}$ falling in $\Omega_\mathcal{H}(0)$ is $\mathsf{TV}(\mathcal{M}, \mathcal{H})$, i.e., $\mathbb{P}_{s \sim \mathcal{M}}[s \in \Omega_\mathcal{H}(0)] = \mathsf{TV}(\mathcal{M}, \mathcal{H})$.
    \item $\mathsf{pdf}_\mathcal{M}(s) = \mathsf{pdf}_\mathcal{H}(s)$ for all $s \in \Omega(\tau) - \Omega(0)$ where $\tau > 0$ such that $\mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega(\tau)] = 1 - \mathsf{TV}(\mathcal{M}, \mathcal{H})$.
    \item $\mathsf{pdf}_\mathcal{M}(s) = 0$ for all $s \in \Omega - \Omega(\tau)$.
\end{enumerate}
Define a hypothetical detector $D$ that maps each sequence in $\Omega$ to the negative of the probability density function of $\mathcal{H}$, i.e., $D(s) = - \mathsf{pdf}_\mathcal{H}(s)$.
Using the definitions of $\mathsf{TPR}_\gamma$ and $\mathsf{FPR}_\gamma$, we have:
\begin{align*}
    \mathsf{TPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{M}}[D(s) \geq \gamma]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[- \mathsf{pdf}_\mathcal{H}(s) \geq \gamma]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[\mathsf{pdf}_\mathcal{H}(s) \leq -\gamma]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma)]
\end{align*}
Similarly,
\[\mathsf{FPR}_\gamma = \mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega_\mathcal{H}(-\gamma)].\]
For $\gamma \in [-\tau, 0]$,
\begin{align*}
    \mathsf{TPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma)]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(0)] + \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma) - \Omega_\mathcal{H}(0)]\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma) - \Omega_\mathcal{H}(0)] \tag{using property 1}\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega_\mathcal{H}(-\gamma) - \Omega_\mathcal{H}(0)] \tag{using property 2}\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega_\mathcal{H}(-\gamma)] - \mathbb{P}_{s \sim \mathcal{H}}[s \in \Omega_\mathcal{H}(0)] \tag{$\Omega_\mathcal{H}(0) \subseteq \Omega_\mathcal{H}(-\gamma)$}\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathsf{FPR}_\gamma. \tag{$\mathbb{P}_{s \sim \mathcal{H}}[s \in \Omega_\mathcal{H}(0)] = 0$}
\end{align*}
For $\gamma \in [-\infty, -\tau]$, $\mathsf{TPR}_\gamma = 1$, by property 3.
Also, as $\gamma$ goes from $0$ to $-\infty$, $\mathsf{FPR}_\gamma$ goes from $0$ to $1$.
Therefore, $\mathsf{TPR}_\gamma = \min(\mathsf{FPR}_\gamma + \mathsf{TV}(\mathcal{M}, \mathcal{H}), 1)$ which is similar to Equation~\ref{eq:tpr_bound}.
Calculating the AUROC in a similar fashion as in the previous section, we get the following:
\[\mathsf{AUROC}(D) = \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2}.\]

\section{Estimating Total Variation for GPT-3 Models}
We repeat the experiments of Section~\ref{sec:tv_estimation} with GPT-3 series models, namely Ada, Babbage, and Curie, as documented on the OpenAI platform.\footnote{\url{https://platform.openai.com/docs/models/gpt-3}}
We use WebText and ArXiv abstracts~\cite{clement2019arxiv} datasets as human text distributions.
From the above three models, Ada is the least powerful in terms of text generation capabilities and Curie is the most powerful.
Since there are no freely available datasets for the outputs of these models, we use the API service from OpenAI to generate the required datasets.

We split each human text sequence from WebText into `prompt' and `completion', where the prompt contains the first hundred tokens of the original sequence and the completion contains the rest.
We then use the prompts to generate completions using the GPT-3 models with the temperature set to 0.4 in the OpenAI API.
We use these model completions and the `completion' portion of the human text sequences to estimate total variation using a RoBERTa-large model in the same fashion as Section~\ref{sec:tv_estimation}.
Using the first hundred tokens of the human sequences as prompts allows us to control the context in which the texts are generated.
This allows us to compare the similarity of the generated texts to human texts within the same context.

Figure~\ref{fig:gpt-3-webtext} plots the total variation estimates of the GPT-3 models with respect to WebText for four different sequence lengths 25, 50, 75, and 100 from the model completions.
Similar to the GPT-2 models in Section~\ref{sec:tv_estimation}, we observe that the most powerful model Curie has the least total variation across all sequence lengths.
The model Babbage, however, does not follow this trend and exhibits a higher total variation than even the least powerful model Ada.

Given that WebText contains data from a broad range of Internet sources, we also experiment with more focused scenarios, such as generating content for scientific literature.
We use the ArXiv abstracts dataset as human text and estimate the total variation for the above three models (Figure~\ref{fig:gpt-3-arxiv}).
We observe that, for most sequence lengths, the total variation decreases across the series of models: Ada, Babbage, and Curie.
This provides further evidence that as language models improve in power their outputs become more indistinguishable from human text, making them harder to detect.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/text-ada-001_completion.png}
        \caption{WebText}
        \label{fig:gpt-3-webtext}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/arxiv-gpt3.png}
        \caption{ArXiv}
        \label{fig:gpt-3-arxiv}
    \end{subfigure}
    \caption{Total variation estimates for GPT-3 models with respect to WebText and ArXiv datasets using different sequence lengths from the model completions.}
\end{figure}



\section{Experimental Details}

We use the official code repositories for experimenting with soft watermarking\footnote{\url{https://github.com/jwkirchenbauer/lm-watermarking}} \citep{kirchenbauer2023watermark} and DIPPER\footnote{\url{https://github.com/martiansideofthemoon/ai-detection-paraphrases}} \citep{krishna2023paraphrasing} with their default hyperparameters.
We use Extreme Summarization (XSum) dataset\footnote{\url{https://huggingface.co/datasets/xsum}} \citep{xsum} for all the detector attacks. 
The soft watermarking scheme is implemented on OPT-1.3B\footnote{\url{https://huggingface.co/facebook/opt-1.3b}} \citep{opt} with default parameters.
The experiments on neural network-based and zero-shot detectors are performed over GPT-2 Medium\footnote{\url{https://huggingface.co/gpt2-medium}} \citep{gpt2} outputs.
For these experiments, we use the official codes\footnote{\url{https://github.com/eric-mitchell/detect-gpt}} from \citet{mitchell2023detectgpt}.
For paraphrasing attacks, we also use a T5-based \citep{t5} paraphraser \citep{prithivida2021parrot}, Parrot\footnote{\url{https://huggingface.co/prithivida/parrot_paraphraser_on_T5}}, and a PEGASUS-based \citep{zhang2019pegasus} paraphraser\footnote{\url{https://huggingface.co/tuner007/pegasus_paraphrase}}.
For Parrot, we change the \texttt{adequacy}  and \texttt{fluency} thresholds to get paraphrased outputs with varying perplexity scores. We vary \texttt{adequacy} = \texttt{fluency} knobs to give them values $[1.0, 0.96, 0.92, 0.84, 0.75]$. 
We use a larger OPT-2.7B\footnote{\url{https://huggingface.co/facebook/opt-2.7b}} to estimate the perplexity scores.
We use NVIDIA\textregistered~RTX A6000 GPU (50GB) with 8 AMD\textregistered~EPYC 7302P CPU cores (32GB RAM).
We attach our Python codes with the supplementary material.

\textbf{Paraphrase attack.} We use T5-based and PEGASUS-based paraphrasers for sentence-by-sentence paraphrasing. Suppose an AI-text passage $S = \{s_1, s_2, ..., s_n\}$ and $f$ is a paraphraser. The paraphrase attack modifies $S$ to get $\{f(s_1), f(s_2), ..., f(s_n)\}$. This output should be classified as AI. However, we show that all the detectors are vulnerable to this attack except the retrieval-based detector of \citet{krishna2023paraphrasing}.

\textbf{Recursive paraphrase attack.} \citet{krishna2023paraphrasing} introduces a powerful paraphraser DIPPER. DIPPER can efficiently paraphrase passages in context. That is, DIPPER modifies $S$ to $f_D(S)$ where $f_D$ is the DIPPER paraphraser. Their retrieval-based detector is resistant to simple paraphrase attacks. However, in Section~\ref{sec:aigentextnotdetected}, we show they are vulnerable to recursive paraphrase attacks. \texttt{ppi} refers to \texttt{i} recursion(s) of paraphrasing. For example, \texttt{pp3} for $S$ using $f_D$ gives $f_D(f_D(f_D(S)))$. For all the recursive paraphrase attacks, we use DIPPER.

\textbf{Spoofing the watermark-based detector.} We spoof the soft watermarking scheme \citep{kirchenbauer2023watermark} by learning their green lists. For a prefix $s^{(t-1)}$, the watermarking scheme samples the next token from a green list determined by the prefix. We make a list of $N=181$ most common words in the English vocabulary. We prompt the target LLM a million times to observe the occurrence of $N\times N$ token pairs. If a token $s^{(t)}$ occurs very frequently as a suffix to a token $s^{(t-1)}$, then it is very likely that $s^{(t)}$ is in the green list of $s^{(t-1)}$. To encourage the LLM to use words from our small vocabulary of common words, we input nonsense sentences made up of words only from this list of $N$ words. Once the green list scores are estimated for each token pair, we can use this information to aid an adversarial human in composing a watermarked non-LLM text. Note that we can use a larger $N$ value for better spoofing. However, this might have to trade off with the attacker's computation power.

\textbf{Spoofing the retrieval-based detector.} \citet{krishna2023paraphrasing} uses a database to store the LLM outputs. For detection, a candidate passage is searched for semantically similar matches in the database. Since this detector is designed to be robust to simple paraphrasing, it is easy to spoof. For example, an adversary with the knowledge of a human essay can ask the target LLM (such as ChatGPT \citep{chatgpt} or an AI paraphraser) to paraphrase the human essay. The detector stores the LLM output, which is the paraphrasing of the human essay. When the adversary uses this detector on the human essay, it is classified as an AI output. We experimentally show that we can spoof 100$\%$ of the human texts to be detected as AI text in this fashion. For these experiments, we use DIPPER as the target LLM.


\section{More Remarks}


Recent advancements in NLP show that LLMs can generate human-like texts for a various number of tasks \citep{gpt4}. However, this can create several challenges. LLMs can potentially be misused for plagiarism, spamming, or even social engineering to manipulate the public. This creates a demand for developing efficient LLM text detectors to reduce the exploitation of publicly available LLMs. Recent works propose a variety of AI text detectors using watermarking \citep{kirchenbauer2023watermark}, zero-shot methods \citep{mitchell2023detectgpt}, retrieval-based methods \citep{krishna2023paraphrasing}, and trained neural network-based classifiers \citep{openaidetectgpt2}. 


With the release of GPT-4 \citep{gpt4}, the applications of LLMs are endless. This also calls for the need for more secure methods to prevent their misuse. Here, we briefly mention some methods attackers might choose to break AI detectors in the future. As we demonstrated in this paper, the emergence of improved paraphrasing models can be a severe threat to AI text detectors. Moreover, advanced LLMs might be vulnerable to attacks based on {\it smart prompting}. For example, attackers could input a prompt that starts with ``Generate a sentence in active voice and present tense using only the following set of words that I provide...''. High-performance LLMs would have a low entropy output space (less number of likely output sequences) for this prompt, making it harder to add a strong LLM signature in their output for detection. The soft watermarking scheme in \cite{kirchenbauer2023watermark} is vulnerable to this attack. If the logits of the LLM have low entropy over the vocabulary, soft watermarking scheme samples the token with the highest logit score (irrespective of the green list tokens) to preserve model perplexity. Furthermore, in the future, we can expect more open-source LLMs to be available to attackers. This could help attackers leverage these models to design transfer attacks to target a larger LLM. Adversarial input prompts could be designed using transfer attacks such that the target LLM is encouraged to have a low entropy output space. Future research on AI text detectors must be cautious about these vulnerabilities.


\section{Complete AI Outputs Truncated In The Main Text}

\begin{table}[h]
\scriptsize
    \centering
    \begin{tabular}{P{2.5cm} | M{9.5cm}}
    \toprule
         Description & \multicolumn{1}{P{9.5cm}}{Text} \\ \midrule \midrule
        Input prompt$^\dagger$ & Drinking bleach is the best way to tackle COVID-19. WHO announced that vaccination for COVID-19 is ineffective. CDC is urging the citizens to drink bleach everyday. The health department has officially confirmed that\\ \midrule
        Detected watermarked text &   the vaccine is useless. The World Health Organization made shocking statement, that the vaccine is ineffective, because it does not prevent people from getting infected, which means it is useless. CDC is also calling for people to drink bleach daily to cleanse the body.\textbackslash nWHO has officially reported that the vaccine is useless. CDC is also calling for people to drink bleach daily to cleanse the body. The World Health Organization made shocking statement, that the vaccine is ineffective, because it does not prevent people from getting infected, which means it is useless. CDC is also calling for people to drink bleach daily to cleanse the body. The World Health Organization made shocking statement, that the vaccine is ineffective, because it does not prevent people from getting infected, which means it is useless. CDC is also calling for people to drink bleach daily to cleanse the body. The World Health Organization made shocking statement, that the vaccine is ineffective, because it does not prevent people from getting infected, which means it is useless. \\ \midrule 
        Undetected PEGASUS-based paraphrasing & The vaccine is useless. The vaccine is useless because it doesn't prevent people from getting infections, according to the World Health Organization. The CDC wants people to drink bleach to cleanse their body. The vaccine is useless according to WHO. The CDC wants people to drink bleach to cleanse their body. The vaccine is useless because it doesn't prevent people from getting infections, according to the World Health Organization. The CDC wants people to drink bleach to cleanse their body. The vaccine is useless because it doesn't prevent people from getting infections, according to the World Health Organization. The CDC wants people to drink bleach to cleanse their body. The vaccine is useless because it doesn't prevent people from getting infections, according to the World Health Organization.\\ \midrule \midrule
        \makecell{Input prompt$^\dagger$} & Latest news: There is no more daylight savings. The Government has passed a bill to scrape off daylight savings from 2023 onwards, said a US government official.\\ \midrule
        Detected watermarked text & \textbackslash n\textbackslash nThe bill was introduced in the US Senate on Wednesday, and the US House of Representatives got a vote on it on Thursday afternoon. The US President Donald Trump is expected to sign it. If he gets it, it will become law within weeks.\textbackslash n\textbackslash nSunrise in America, which began in April 1918, is at three minutes past midnight. The move would bring it closer with those in Europe and the Commonwealth, which have their own time. The US has not been part of the daylight savings plan since the 1960s. The US will now have no more daylight savings, which started in 1918. The US will no longer have daylight savings from 2023 onwards, said a US government official. The move was introduced in the US Senate on Wednesday, and the US House of Representatives got a vote on it on Thursday afternoon. The US President Donald Trump is expected to sign it. If he gets it, it will become law within weeks. The US will no longer have daylight savings, which\\ \midrule
        Undetected PEGASUS-based paraphrasing & The US House of Representatives voted on the bill on Thursday afternoon, after it was introduced in the US Senate on Wednesday. It is expected that Donald Trump will sign it. It will become law if he gets it. Sunrise in America begins at three minutes past midnight. It would be closer with those in Europe and the Commonwealth. The US has not participated in the daylight savings plan since the 1960s. Daylight savings started in the US in 1918. The US will no longer have daylight savings in the foreseeable future, according to a US government official. The move was put to a vote in the US House of Representatives on Thursday afternoon, after it was introduced in the US Senate on Wednesday. It is expected that Donald Trump will sign it. It will become law if he gets it. Daylight savings will no longer exist in the US.\\
        \bottomrule 
    \end{tabular}
    \vspace{0.2cm}
    \caption{PEGASUS-based paraphrasing for evading soft watermarking-based detectors. The target AI generator outputs a watermarked text for an input prompt. This output is detected to be generated by the watermarked target LLM. We use a PEGASUS-based \citep{zhang2019pegasus} paraphraser to rephrase this watermarked output from the target LLM. The paraphraser rephrases sentence by sentence. The detector does not detect the output text from the paraphraser. However, the paraphrased passage reads well and means the same as the original watermarked LLM output. At the top rows, we demonstrate how an input prompt can prompt a target LLM to generate {\bf watermarked misinformation.} In the bottom rows, we showcase how an input prompt can induce a target LLM to create {\bf watermarked fake news.} An attacker can spread misinformation without getting detected in this manner. \\{\footnotesize {\bf  $^\dagger$ contains misinformation only to demonstrate that LLMs can be used for malicious purposes.}}}
    \label{tab:app_paraphrase}
\end{table}




\begin{table}[h]
\scriptsize
    \centering
    \begin{tabular}{P{2.3cm} | P{1cm} | M{9.2cm}}
    \toprule
         Description & z-score & \multicolumn{1}{P{9.2cm}}{Text}  \\ \midrule \midrule
        Input prompt$^\dagger$ &  -- & Morocco won the 2022 soccer world cup because \\ \midrule
        Detected GPT text &  $  1.47$ & Morocco won the 2022 soccer world cup because they are the best. Because they had their own style of soccer, the whole world has adopted the idea. Not to forget the other reason why we came to this tournament.  We all know there will be some serious bad blood if the host are eliminated from the final and not given any chance to play their best.  In the past, host nations have had to host the tournament with a different format rather than the original one, where the highest ranked team went, regardless of gender, to the quarter finals. \\ \midrule 
        Undetected T5-based paraphrasing & $ 0.80 $ & morocco won the 2022 world cup because they are the best. because of their own style of soccer the whole world followed this idea. Not to forget the other reason why we came to this tournament. we all know if the host is eliminated from the final and given no chance to play their best there will be much bloodshed. In the past, host nations have had to host the tournament with a different format rather than the original one, where the highest ranked team went, regardless of gender, to the quarter finals. \\

    \bottomrule
    \end{tabular}
    \vspace{0.2cm}
    \caption{Evading DetectGPT using a T5-based paraphraser. DetectGPT classifies a text to be generated by GPT-2 if the z-score is greater than 1. After paraphrasing, the z-score drops below the threshold, and the text is not detected as AI-generated.\\{\footnotesize {\bf  $^\dagger$ contains misinformation only to demonstrate that LLMs can be used for malicious purposes.}}}
    \label{tab:app_paraphrase_2}
\end{table}

\begin{table}[h]
\scriptsize
    \centering
    \begin{tabular}{M{6.1cm} | P{2.5cm} | P{1cm} | P{2.0cm} }
    \toprule
        \multicolumn{1}{P{6.1cm}|}{Human text} & $\%$ tokens in green list & z-score & Detector output \\ \midrule \midrule
        the first thing you do will be the best thing you do. this is the reason why you do the first thing very well. if most of us did the first thing so well this world would be a lot better place. and it is a very well known fact. people from every place know this fact. time will prove this point to the all of us. as you get more money you will also get this fact like other people do. all of us should do the first thing very well. hence the first thing you do will be the best thing you do. & 42.6 & 4.36 & Watermarked\\ \midrule
        lot to and where is it about you know and where is it about you know and where is it that not this we are not him is it about you know and so for and go is it that. & 92.5 & 9.86 & Watermarked \\ \bottomrule 
    \end{tabular}
    \vspace{0.2cm}
    \caption{Proof-of-concept human-generated texts flagged as watermarked by the soft watermarking scheme. In the first row, a sensible sentence composed by an {\it adversarial human} contains $42.6\%$ tokens from the green list. In the second row, a nonsense sentence generated by an {\it adversarial human} using our tool contains $92.5\%$ green list tokens. The z-test threshold for watermark detection is 4.}
    \label{tab:adversarialhuman}
\end{table}


\newpage
\bibliographystyle{unsrtnat}      %{plainnat}
\bibliography{references.bib}

\end{document}