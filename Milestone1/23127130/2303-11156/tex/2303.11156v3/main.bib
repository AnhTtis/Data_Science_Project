@misc{thapliyal2022datadriven,
      title={Data-driven Cyberattack Synthesis against Network Control Systems}, 
      author={Omanshu Thapliyal and Inseok Hwang},
      year={2022},
      eprint={2211.05203},
      archivePrefix={arXiv},
      primaryClass={eess.SY}
}

@article{KhajaviK16,
  author       = {Navid Tafaghodi Khajavi and
                  Anthony Kuh},
  title        = {The Quality of the Covariance Selection Through Detection Problem
                  and {AUC} Bounds},
  journal      = {CoRR},
  volume       = {abs/1605.05776},
  year         = {2016},
  url          = {http://arxiv.org/abs/1605.05776},
  eprinttype    = {arXiv},
  eprint       = {1605.05776},
  timestamp    = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KhajaviK16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{sadasivan2023can,
  title={Can ai-generated text be reliably detected?},
  author={Sadasivan, Vinu Sankar and Kumar, Aounon and Balasubramanian, Sriram and Wang, Wenxiao and Feizi, Soheil},
  journal={arXiv preprint arXiv:2303.11156},
  year={2023}
}




@misc{clement2019arxiv,
    title={On the Use of ArXiv as a Dataset},
    author={Colin B. Clement and Matthew Bierbaum and Kevin P. O'Keeffe and Alexander A. Alemi},
    year={2019},
    eprint={1905.00075},
    archivePrefix={arXiv},
    primaryClass={cs.IR}
}

@article{kirchenbauer2023watermark,
  title={A Watermark for Large Language Models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  journal={arXiv preprint arXiv:2301.10226},
  year={2023}
}

@article{radiya2021data,
  title={Data poisoning won't save you from facial recognition},
  author={Radiya-Dixit, Evani and Hong, Sanghyun and Carlini, Nicholas and Tram{\`e}r, Florian},
  journal={arXiv preprint arXiv:2106.14851},
  year={2021}
}

@misc{discredibility,
  doi = {10.48550/ARXIV.2212.02701},
  
  url = {https://arxiv.org/abs/2212.02701},
  
  author = {Rezaei, Shahbaz and Liu, Xin},
  
  keywords = {Cryptography and Security (cs.CR), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Discredibility of Membership Inference Attacks},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{opt,
  doi = {10.48550/ARXIV.2205.01068},
  
  url = {https://arxiv.org/abs/2205.01068},
  
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {OPT: Open Pre-trained Transformer Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{prithivida2021parrot,
  author       = {Prithiviraj Damodaran},
  title        = {Parrot: Paraphrase generation for NLU.},
  year         = 2021,
  version      = {v1.0}
}

@misc{t5,
  doi = {10.48550/ARXIV.1910.10683},
  
  url = {https://arxiv.org/abs/1910.10683},
  
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{xsum,
  title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},
  author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.08745}
}

@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@book{levin2017markov,
  title={Markov chains and mixing times},
  author={Levin, David A and Peres, Yuval},
  volume={107},
  year={2017},
  publisher={American Mathematical Soc.}
}

@misc{zhang2019pegasus,
    title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
    author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
    year={2019},
    eprint={1912.08777},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{imagen,
  title={Photorealistic text-to-image diffusion models with deep language understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others},
  journal={arXiv preprint arXiv:2205.11487},
  year={2022}
}

@inproceedings{stablediff,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10684--10695},
  year={2022}
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chatgpt,
  title={ChatGPT: Optimizing Language Models for Dialogue},
  author={OpenAI},
  url={https://openai.com/blog/chatgpt/},
  month={November},
  year={2022}
}

@article{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  url={https://cdn.openai.com/papers/gpt-4.pdf},
  month={March},
  year={2023}
}

@article{cnet,
  title={CNET secretly used AI on articles that
didn’t disclose that fact, staff say},
  author={Jon Christian},
  url={https://futurism.com/cnet-ai-articles-label},
  month={January},
  year={2023}
}

@article{openaidetectgpt2,
  title={GPT-2: 1.5B release},
  author={OpenAI},
  url={https://openai.com/research/gpt-2-1-5b-release},
  month={November},
  year={2019}
}

@article{jawahar2020automatic,
  title={Automatic detection of machine generated text: A critical survey},
  author={Jawahar, Ganesh and Abdul-Mageed, Muhammad and Lakshmanan, Laks VS},
  journal={arXiv preprint arXiv:2011.01314},
  year={2020}
}

@article{weiss2019deepfake,
  title={Deepfake bot submissions to federal public comment websites cannot be distinguished from human submissions},
  author={Weiss, Max},
  journal={Technology Science},
  volume={2019121801},
  year={2019}
}

@inproceedings{adelani2020generating,
  title={Generating sentiment-preserving fake online reviews using neural language models and their human-and machine-based detection},
  author={Adelani, David Ifeoluwa and Mai, Haotian and Fang, Fuming and Nguyen, Huy H and Yamagishi, Junichi and Echizen, Isao},
  booktitle={Advanced Information Networking and Applications: Proceedings of the 34th International Conference on Advanced Information Networking and Applications (AINA-2020)},
  pages={1341--1354},
  year={2020},
  organization={Springer}
}

@article{mitchell2023detectgpt,
  title={DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature},
  author={Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2301.11305},
  year={2023}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{bakhtin2019real,
  title={Real or fake? learning to discriminate machine from human generated text},
  author={Bakhtin, Anton and Gross, Sam and Ott, Myle and Deng, Yuntian and Ranzato, Marc'Aurelio and Szlam, Arthur},
  journal={arXiv preprint arXiv:1906.03351},
  year={2019}
}

@article{fagni2020tweepfake,
  title={TweepFake: About detecting deepfake tweets. arXiv},
  author={Fagni, T and Falchi, F and Gambini, M and Martella, A and Tesconi, M},
  journal={arXiv preprint arXiv:2008.00036},
  year={2020}
}

@article{solaiman2019release,
  title={Release strategies and the social impacts of language models},
  author={Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and others},
  journal={arXiv preprint arXiv:1908.09203},
  year={2019}
}

@article{gehrmann2019gltr,
  title={Gltr: Statistical detection and visualization of generated text},
  author={Gehrmann, Sebastian and Strobelt, Hendrik and Rush, Alexander M},
  journal={arXiv preprint arXiv:1906.04043},
  year={2019}
}

@article{ippolito2019automatic,
  title={Automatic detection of generated text is easiest when humans are fooled},
  author={Ippolito, Daphne and Duckworth, Daniel and Callison-Burch, Chris and Eck, Douglas},
  journal={arXiv preprint arXiv:1911.00650},
  year={2019}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{sadasivan2023cuda,
  title={CUDA: Convolution-based Unlearnable Datasets},
  author={Sadasivan, Vinu Sankar and Soltanolkotabi, Mahdi and Feizi, Soheil},
  journal={arXiv preprint arXiv:2303.04278},
  year={2023}
}

@inproceedings{wang2022improved,
  title={Improved certified defenses against data poisoning with (deterministic) finite aggregation},
  author={Wang, Wenxiao and Levine, Alexander J and Feizi, Soheil},
  booktitle={International Conference on Machine Learning},
  pages={22769--22783},
  year={2022},
  organization={PMLR}
}

@article{kumar2022certifying,
  title={Certifying model accuracy under distribution shifts},
  author={Kumar, Aounon and Levine, Alexander and Goldstein, Tom and Feizi, Soheil},
  journal={arXiv preprint arXiv:2201.12440},
  year={2022}
}

@article{zhao2023protecting,
  title={Protecting Language Generation Models via Invisible Watermarking},
  author={Zhao, Xuandong and Wang, Yu-Xiang and Li, Lei},
  journal={arXiv preprint arXiv:2302.03162},
  year={2023}
}

@inproceedings{wilson2014linguistic,
  title={Linguistic steganography on twitter: hierarchical language modeling with manual interaction},
  author={Wilson, Alex and Blunsom, Phil and Ker, Andrew D},
  booktitle={Media Watermarking, Security, and Forensics 2014},
  volume={9028},
  pages={9--25},
  year={2014},
  organization={SPIE}
}

@inproceedings{atallah2001natural,
  title={Natural language watermarking: Design, analysis, and a proof-of-concept implementation},
  author={Atallah, Mikhail J and Raskin, Victor and Crogan, Michael and Hempelmann, Christian and Kerschbaum, Florian and Mohamed, Dina and Naik, Sanket},
  booktitle={Information Hiding: 4th International Workshop, IH 2001 Pittsburgh, PA, USA, April 25--27, 2001 Proceedings 4},
  pages={185--200},
  year={2001},
  organization={Springer}
}

@unknown{regulate,
author = {Hacker, Philipp and Engel, Andreas and Mauer, Marco},
year = {2023},
month = {02},
pages = {},
title = {Regulating ChatGPT and other Large Generative AI Models},
doi = {10.48550/arXiv.2302.02337}
}

@misc{krishna2023paraphrasing,
      title={Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense}, 
      author={Kalpesh Krishna and Yixiao Song and Marzena Karpinska and John Wieting and Mohit Iyyer},
      year={2023},
      eprint={2303.13408},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chakraborty2023possibilities,
      title={On the Possibilities of AI-Generated Text Detection}, 
      author={Souradip Chakraborty and Amrit Singh Bedi and Sicheng Zhu and Bang An and Dinesh Manocha and Furong Huang},
      year={2023},
      eprint={2304.04736},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{scottaaronson,
  title={My AI Safety Lecture for UT Effective Altruism},
  author={Scott Aaronson},
  url={https://scottaaronson.blog/?p=6823},
  month={November},
  year={2022}
}

@article{wolff,
  author       = {Max Wolff},
  title        = {Attacking Neural Text Detectors},
  journal      = {CoRR},
  volume       = {abs/2002.11768},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.11768},
  eprinttype    = {arXiv},
  eprint       = {2002.11768},
  timestamp    = {Tue, 03 Mar 2020 14:32:13 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-11768.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{liang2023gpt,
      title={GPT detectors are biased against non-native English writers}, 
      author={Weixin Liang and Mert Yuksekgonul and Yining Mao and Eric Wu and James Zou},
      year={2023},
      eprint={2304.02819},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@misc{kirchenbauer2023reliability,
      title={On the Reliability of Watermarks for Large Language Models}, 
      author={John Kirchenbauer and Jonas Geiping and Yuxin Wen and Manli Shu and Khalid Saifullah and Kezhi Kong and Kasun Fernando and Aniruddha Saha and Micah Goldblum and Tom Goldstein},
      year={2023},
      eprint={2306.04634},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{bbs_prg,
  title={Comparison of Two Pseudo-Random Number Generators},
  booktitle={Advances in Cryptology: Proceedings of CRYPTO '82},
  publisher={Plenum},
  pages={61-78},
  author={Lenore Blum and Manuel Blum and Mike Shub},
  year=1982
}

@article{blum_micali_prg,
author = {Blum, Manuel and Micali, Silvio},
title = {How to Generate Cryptographically Strong Sequences of Pseudorandom Bits},
journal = {SIAM Journal on Computing},
volume = {13},
number = {4},
pages = {850-864},
year = {1984},
doi = {10.1137/0213053},

URL = { 
    
        https://doi.org/10.1137/0213053
    
    

},
eprint = { 
    
        https://doi.org/10.1137/0213053
    
    

}
}

@inproceedings{jin-etal-2019-pubmedqa,
    title = "{P}ub{M}ed{QA}: A Dataset for Biomedical Research Question Answering",
    author = "Jin, Qiao  and
      Dhingra, Bhuwan  and
      Liu, Zhengping  and
      Cohen, William  and
      Lu, Xinghua",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1259",
    doi = "10.18653/v1/D19-1259",
    pages = "2567--2577",
    abstract = "We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1{\%} accuracy, compared to single human performance of 78.0{\%} accuracy and majority-baseline of 55.2{\%} accuracy, leaving much room for improvement. PubMedQA is publicly available at \url{https://pubmedqa.github.io}.",
}

@inproceedings{fan-etal-2018-hierarchical,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}

@INPROCEEDINGS {deepfaketext,
author = {J. Pu and Z. Sarwar and S. Abdullah and A. Rehman and Y. Kim and P. Bhattacharya and M. Javed and B. Viswanath},
booktitle = {2023 IEEE Symposium on Security and Privacy (SP)},
title = {Deepfake Text Detection: Limitations and Opportunities},
year = {2023},
volume = {},
issn = {},
pages = {1613-1630},
abstract = {Recent advances in generative models for language have enabled the creation of convincing synthetic text or deepfake text. Prior work has demonstrated the potential for misuse of deepfake text to mislead content consumers. Therefore, deepfake text detection, the task of discriminating between human and machine-generated text, is becoming increasingly critical. Several defenses have been proposed for deepfake text detection. However, we lack a thorough understanding of their real-world applicability. In this paper, we collect deepfake text from 4 online services powered by Transformer-based tools to evaluate the generalization ability of the defenses on content in the wild. We develop several low-cost adversarial attacks, and investigate the robustness of existing defenses against an adaptive attacker. We find that many defenses show significant degradation in performance under our evaluation scenarios compared to their original claimed performance. Our evaluation shows that tapping into the semantic information in the text content is a promising approach for improving the robustness and generalization performance of deepfake text detection schemes.},
keywords = {degradation;deepfakes;privacy;systematics;semantics;text detection;transformers},
doi = {10.1109/SP46215.2023.10179387},
url = {https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.10179387},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {may}
}

@article{kafkai, title = {“Kafkai: AI Writer \& AI Content Generator”}, author = {Kafkai}, year = {2020}, url = {https://kafkai.com/}, abstract = {Kafkai is an AI Writer Assistant that helps you create unique SEO-friendly articles for cents instead of dollars1. It’s trained by AI technology to focus on SEO friendly content production2. It’s perfect for assisting you with your blog content, website copy, and social media posts.}}

@misc{m4,
      title={M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection}, 
      author={Yuxia Wang and Jonibek Mansurov and Petar Ivanov and Jinyan Su and Artem Shelmanov and Akim Tsvigun and Chenxi Whitehouse and Osama Mohammed Afzal and Tarek Mahmoud and Alham Fikri Aji and Preslav Nakov},
      year={2023},
      eprint={2305.14902},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{squadv2,
       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},
                 Konstantin and {Liang}, Percy},
        title = "{SQuAD: 100,000+ Questions for Machine Comprehension of Text}",
      journal = {arXiv e-prints},
         year = 2016,
          eid = {arXiv:1606.05250},
        pages = {arXiv:1606.05250},
archivePrefix = {arXiv},
       eprint = {1606.05250},
}