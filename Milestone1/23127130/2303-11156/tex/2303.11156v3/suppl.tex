\begin{figure}[t!]
\centering
\begin{subfigure}{.47\textwidth}
  \centering
  \adjustimage{width=1\linewidth, left}{images/wm_opt13b_dipper.png}
  \caption{XSum with OPT-13B paraphrased with DIPPER}
  \label{fig:wmopt13bdipper}
\end{subfigure}% 
\hfill
\begin{subfigure}{.47\textwidth}
  \centering
  \adjustimage{width=1\linewidth, right}{images/wm_opt13b_llama.png}
  \caption{XSum with OPT-13B paraphrased with LLaMA-2}
  \label{fig:wmopt13bllama}
\end{subfigure}\\
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, left}{images/rebuttal_wm_xsum.png}
  \vspace{-7mm}
  \caption{XSum with OPT-1.3B paraphrased with DIPPER}
  \label{fig:wmopt}
\end{subfigure}% 
\hfill
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, right}{images/rebuttal_wm_gpt2.png}
  \vspace{-7mm}
  \caption{XSum with GPT-2-Medium paraphrased with DIPPER}
  \label{fig:wmgpt}
\end{subfigure}\\
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, left}{images/rebuttal_wm_pubmed.png}
  \vspace{-7mm}
  \caption{PubMedQA with OPT-1.3B paraphrased with DIPPER}
  \label{fig:wmpubmed}
\end{subfigure}% 
\hfill
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, right}{images/rebuttal_wm_kafkai.png}
  \vspace{-7mm}
  \caption{Kafkai with OPT-1.3B paraphrased with DIPPER}
  \label{fig:wmkafkai}
\end{subfigure}
% \vspace{-2mm}
\caption{ROC plots for soft watermarking \citep{kirchenbauer2023watermark} with our recursive paraphrasing attacks. AUROC, TPR@1$\%$FPR, and perplexity scores measured using OPT-13B are given in the legend. Detection performance on the XSum dataset using 3 different LLMs --- OPT-13B, OPT-1.3B, and GPT-2-Medium --- are evaluated in (a), (c), and (d), respectively. (b) show the performance of the detector with recursives paraphrases from LLaMA-2-7B-Chat. (e) and (f), respectively, show the performance of the detector on two datasets --- PubMedQA and Kafkai --- with distribution shifts using OPT-1.3B. In all the settings, we observe that the detection performance of the watermarking-based detector reduces drastically with only a slight degradation in perplexity measures.}
\label{fig:wmapp}
\vspace{-0mm}
\end{figure}

\section{Experiments with More Datasets and Models}
\label{app:moreexps}

In this section, we consider multiple datasets (XSum \citep{xsum}, PubMedQA \citep{jin-etal-2019-pubmedqa}, and Kafkai \citep{deepfaketext}) and target LLMs (OPT-1.3B \citep{opt} and GPT-2-Medium \citep{gpt2}) for analyzing our attacks. 

\textbf{Datasets.} As discussed in the \S~\ref{sec:exp-setup}, we use 2000 text passages (1000 passages each for human and AI-generated text classes) of $\sim$300 tokens in length from the XSum dataset for analyzing our attacks. For the rest of the datasets, we use 1000 text passages (500 passages each for human and AI-generated text classes) of $\sim$200 tokens in length.
XSum contains long news articles in its ``document'' feature.  To evaluate the robustness of our attacks to distribution shifts, we include more datasets. We use PubMedQA, which is a medical text dataset. Kafkai dataset \citep{deepfaketext} contains real and fake articles (generated using privately fine-tuned OpenAI models) from 10 different domains, such as cybersecurity, SEO, and marketing. It is generated using Kafkai text generation service \citep{kafkai}.





\subsection{Watermark-based Detectors} \label{app:moreexps_wm}
In this section, we analyze the soft watermarking scheme in \cite{kirchenbauer2023watermark}.
We use the powerful DIPPER paraphraser from \cite{krishna2023paraphrasing} with 11B parameters for our recursive paraphrasing attack on the watermarking detector. On average, five rounds of our recursive paraphrase attack take around 36 seconds per text passage, 300 tokens in length. 
OPT-13B is used to measure the perplexity scores for all the settings.
As shown in Table~\ref{tab:summary-mturk} and Appendix~\ref{app:human_study}, we perform a human study over the XSum dataset to evaluate the semantic drifts in our recursive paraphrasing framework. 
The MTurk human evaluation reveals that $70\%$ of our recursive paraphrases maintain high-quality content preservation, and $89\%$ of our recursive paraphrases have high-quality text or grammar.

Figure~\ref{fig:wmapp} shows the performance of the soft watermarking detector in multiple settings. In all the settings, the detection performance drops as rounds of recursive paraphrasing proceed with a slight degradation in perplexity scores. 
After two rounds of paraphrasing (\texttt{pp2}), the detection performance (TPR@1$\%$FPR) in all the settings drops below $50\%$.
\texttt{Best of ppi}, which selects the paraphrase with the worst detection score, significantly degrades the detection performance to below $10\%$ in all the settings with only slight degradation of 1.5, 0.5, 2.0, and 2.7 in perplexity measures.




\subsection{Zero-shot and Trained Detectors} \label{app:moreexps_zs}

In this section, we analyze the zero-shot and trained detectors in prior literature \citep{mitchell2023detectgpt,solaiman2019release, gehrmann2019gltr, ippolito2019automatic, openaidetectgpt2}. 
We use the T5-based paraphraser \citep{prithivida2021parrot}, Parrot, to paraphrase the AI-generated text and use OPT-13B to measure the perplexity scores for all the settings. We perform our experiments on the XSum \citep{xsum}, PubMedQA \citep{jin-etal-2019-pubmedqa}, and Kafkai \citep{kafkai} datasets with GPT-2-Medium and OPT-1.3B as the target generative models. In Figure \ref{fig:roc_additional} (ROC curves) and Tables \ref{tab:tpr_at_fpr} (TPR@$1\%$FPR values) and \ref{tab:auroc} (AUROC scores), we present our results. 1d, 1z, 10d, and 10z in Tables \ref{tab:tpr_at_fpr} and \ref{tab:auroc} refer to different variants of the DetectGPT \citep{mitchell2023detectgpt}. 

Figure~\ref{fig:roc_additional} shows the performance of various zero-shot and trained detectors in multiple settings. The performance of these detectors drops significantly when the AI-generated text is paraphrased, and when given 5 queries to the detector, an adversary can fool most detectors effectively. Some detectors like OpenAI's RoBERTa-based models are more resilient on datasets like XSum, but are not reliable on other datasets like Kafkai. The perplexity scores of the GPT-2 generated text before any paraphrasing were 15.58 for XSum, 12.80 for PubMedQA, 19.11 for Kafkai, while the perplexity of OPT-1.3B generated text was 9.31. After paraphrasing, the perplexity scores are 20.06, 16.45, 20.01, and 13.96, respectively. 


\begin{figure}[t]
    \centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/kafkai_zs_trained.png}
  \caption{Kafkai with GPT-2}
  \label{fig:gpt_kafkai}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/pubmed_zs_trained.png}
  \caption{PubMedQA with GPT-2}
  \label{fig:gpt_pubmed}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/gpt_xsum_zs_trained.png}
  \caption{XSum with GPT-2}
  \label{fig:gpt_xsum}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/opt_xsum_zs_trained.png}
  \caption{XSum with OPT-1.3B}
  \label{fig:opt_xsum}
\end{subfigure}% 
\caption{ROC curves for performance of various zero-shot and trained detectors for different models and datasets (\textbf{Left}) before attack, (\textbf{Middle}) after paraphrasing attack, (\textbf{Right}) applying paraphrasing attack with multiple queries to detector.}
\label{fig:roc_additional}
\end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.7\textwidth]{images/gpt_xsum_dipper.png}
%     \caption{ROC curves for performance of various zero-shot and trained detectors for GPT-2 on XSum after paraphrasing by DIPPER}
%     \label{fig:enter-label}
% \end{figure}


\begin{table}[]
    \centering
    \begin{adjustbox}{max width=.95\textwidth}
    \begin{tabular}{l|p{7mm}p{7mm}p{7mm}p{7mm}|p{8mm}p{7mm}p{7mm}p{11mm}|p{7mm}p{7mm}}
    \toprule
    & \multicolumn{4}{c|}{ \textbf{DetectGPT}} & \multicolumn{4}{c|}{ \textbf{Threshold} by }  & \multicolumn{2}{c}{\textbf{RoBERTa}} \\
    & 1 d & 1 z & 10 d & 10 z & Likeli- hood & Rank & Log Rank & Entropy &  Base &  Large \\
    \midrule\midrule
\textbf{OPT-1.3B on XSum} & & & & & & & & & & \\
No attack & 0.079 & 0.079 & 0.083 & 0.125 & 0.237 & 0.382 & 0.288 & 0.017 & 0.694 & 0.956\\ 
pp attack  & 0.014 & 0.014 & 0.018 & 0.006 & 0.006 & 0.006 & 0.006 & 0.326 & 0.025 & 0.479\\ 
5 pp attack & 0.0 & 0.0 & 0.005 & 0.0 & 0.004 & 0.001 & 0.002 & 0.202 & 0.003 & 0.244\\ 
\midrule
\textbf{GPT-2 on PubMedQA} & & & & & & & & & & \\
No attack & 0.05 & 0.05 & 0.598 & 0.481 & 0.085 & 0.379 & 0.144 & 0.029 & 0.748 & 0.902\\ 
pp attack & 0.052 & 0.052 & 0.19 & 0.054 & 0.015 & 0.042 & 0.017 & 0.202 & 0.181 & 0.606\\ 
5 pp attack  & 0.0 & 0.0 & 0.031 & 0.002 & 0.008 & 0.01 & 0.012 & 0.135 & 0.088 & 0.452\\  
\midrule
\textbf{GPT-2 on Kafkai} & & & & & & & & & & \\
No attack & 0.077 & 0.077 & 0.669 & 0.625 & 0.088 & 0.352 & 0.085 & 0.0 & 0.048 & 0.006\\ 
pp attack & 0.056 & 0.056 & 0.125 & 0.081 & 0.004 & 0.021 & 0.004 & 0.002 & 0.01 & 0.0\\ 
5 pp attack   & 0.0 & 0.0 & 0.023 & 0.006 & 0.002 & 0.004 & 0.002 & 0.0 & 0.0 & 0.0\\ 
\midrule
\textbf{GPT-2 on XSum} & & & & & & & & & & \\
No attack & 0.169 & 0.169 & 0.599 & 0.326 & 0.114 & 0.444 & 0.186 & 0.026 & 0.881 & 1.0\\ 
pp attack & 0.038 & 0.038 & 0.084 & 0.015 & 0.003 & 0.005 & 0.003 & 0.411 & 0.105 & 0.925\\ 
10 pp attack  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.058 & 0.02 & 0.792\\
% \hspace{5pt} 1 query (with DIPPER) & 0.006 & 0.006 & 0.005 & 0.011 & 0.002 & 0.017 & 0.006 & 0.089 & 0.852 & 0.996\\ 
\bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{TPR@1$\%$FPR for trained and zero-shot detectors in different settings. For all attacks, we use the T5-based paraphraser. Here, ``pp attack'' refers to the paraphrasing attack where the AI output is paraphrased by the T5-based model. ``i pp attack'' refers to the setting where the attacker has black-box access to the detector. Here, the paraphraser generates ``i'' paraphrases for every passage, and the attacker selects the passage that has the worst detection score after ``i'' queries to the detector.}
    \label{tab:tpr_at_fpr}
    \vspace{2mm}

    \centering
    \begin{adjustbox}{max width=0.95\textwidth}
    \begin{tabular}{l|p{7mm}p{7mm}p{7mm}p{7mm}|p{8mm}p{7mm}p{7mm}p{11mm}|p{7mm}p{7mm}}
    \toprule
    & \multicolumn{4}{c|}{ \textbf{DetectGPT}} & \multicolumn{4}{c|}{ \textbf{Threshold} by }  & \multicolumn{2}{c}{\textbf{RoBERTa}} \\
    & 1 d & 1 z & 10 d & 10 z & Likeli- hood & Rank & Log Rank & Entropy &  Base &  Large \\
    \midrule\midrule
   \textbf{OPT-1.3B on XSum} & & & & & & & & & & \\
No attack & 0.769 & 0.769 & 0.9 & 0.859 & 0.918 & 0.844 & 0.943 & 0.482 & 0.974 & 0.998\\ 
pp attack & 0.487 & 0.487 & 0.453 & 0.41 & 0.241 & 0.387 & 0.282 & 0.868 & 0.562 & 0.945\\ 
5 pp attack & 0.162 & 0.162 & 0.244 & 0.182 & 0.153 & 0.216 & 0.181 & 0.821 & 0.316 & 0.9\\ 
\midrule

\textbf{GPT-2 on PubMedQA} & & & & & & & & & & \\
No attack & 0.816 & 0.816 & 0.973 & 0.955 & 0.804 & 0.796 & 0.892 & 0.615 & 0.982 & 0.998\\ 
pp attack & 0.671 & 0.671 & 0.796 & 0.743 & 0.4 & 0.497 & 0.494 & 0.798 & 0.823 & 0.98\\ 
5 pp attack & 0.33 & 0.33 & 0.601 & 0.541 & 0.327 & 0.314 & 0.409 & 0.752 & 0.712 & 0.967\\ 
\midrule

\textbf{GPT-2 on Kafkai} & & & & & & & & & & \\
No attack & 0.814 & 0.814 & 0.976 & 0.971 & 0.865 & 0.86 & 0.89 & 0.394 & 0.817 & 0.86\\ 
pp attack & 0.661 & 0.661 & 0.757 & 0.742 & 0.497 & 0.719 & 0.515 & 0.651 & 0.486 & 0.629\\ 
5 pp attack & 0.353 & 0.353 & 0.532 & 0.513 & 0.412 & 0.627 & 0.426 & 0.576 & 0.358 & 0.53\\ 
\midrule

 \textbf{GPT-2 on XSum} & & & & & & & & & & \\
No attack & 0.837 & 0.837 & 0.976 & 0.949 & 0.879 & 0.868 & 0.93 & 0.617 & 0.993 & 1.0\\ 
pp attack & 0.566 & 0.566 & 0.587 & 0.524 & 0.171 & 0.277 & 0.23 & 0.916 & 0.726 & 0.995\\ 
10 pp attack & 0.115 & 0.115 & 0.202 & 0.177 & 0.075 & 0.104 & 0.108 & 0.744 & 0.464 & 0.983\\ 
% gpt-xsum-1020-dipper & 0.507 & 0.507 & 0.511 & 0.519 & 0.495 & 0.539 & 0.582 & 0.78 & 0.99 & 1.0\\ 
\bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{AUROC for trained and zero-shot detectors in different settings. Here, ``pp attack'' refers to the paraphrasing attack where the AI output is paraphrased by the T5-based model. ``i pp attack'' refers to the setting where the attacker has black-box access to the detector. Here, the paraphraser generates ``i'' paraphrases for every passage, and the attacker selects the passage that has the worst detection score after ``i'' queries to the detector.}
    \label{tab:auroc}
\end{table}


\subsection{Retrieval-based Detectors} \label{app:moreexps_retrieval}

\begin{figure}[t]
\centering
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, left}{images/IR_attack_xsum.png}
  \vspace{-2mm}
  \caption{XSum with OPT-1.3B}
  \label{fig:iropt}
\end{subfigure}% 
\hfill
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, right}{images/IR_attack_gpt2.png}
  \vspace{-2mm}
  \caption{XSum with GPT-2-Medium}
  \label{fig:irgpt}
\end{subfigure}\\
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, left}{images/IR_attack_pubmed.png}
  \vspace{-2mm}
  \caption{PubMedQA with OPT-1.3B}
  \label{fig:irpubmed}
\end{subfigure}% 
\hfill
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, right}{images/IR_attack_kafkai.png}
  \vspace{-2mm}
  \caption{Kafkai with OPT-1.3B}
  \label{fig:irkafkai}
\end{subfigure}
% \vspace{-2mm}
\caption{ROC plots for soft watermarking \citep{kirchenbauer2023watermark} with our recursive paraphrasing attacks. AUROC, TPR@1$\%$FPR, and perplexity scores measured using OPT-13B are given in the legend. Detection performance on the XSum dataset using two different LLMs --- OPT-1.3B and GPT-2-Medium --- are evaluated in (a) and (b), respectively. (c) and (d), respectively, show the performance of the detector on two datasets --- PubMedQA and Kafkai --- with distribution shifts using OPT-1.3B. In all the settings, we observe that the detection performance of the watermarking-based detector reduces drastically with only a slight degradation in perplexity measures.}
\label{fig:irapp}
\vspace{-0mm}
\end{figure}

In this section, we analyze the retrieval-based detector proposed in \cite{krishna2023paraphrasing}. 
We show that our recursive paraphrasing attack is effective in breaking their detector. 
We use the 11B parameter DIPPER paraphraser \citep{krishna2023paraphrasing} for our attack. 
OPT-13B is used to measure the perplexity scores in all the settings.

Figure~\ref{fig:irapp} shows the performance of the retrieval-based detector in multiple settings. In all the settings, the detection accuracy drops as rounds of recursive paraphrasing proceed with only a slight degradation in perplexity scores. We observe that the detector works well after a single round of paraphrasing (\texttt{pp1}). However, after five rounds of paraphrasing, \texttt{Best of ppi} reduces the detector's accuracy to close to $50\%$ with only a slight degradation in perplexity scores. We also find that we can easily spoof the retrieval-based detector as discussed in \S\ref{sec:humantextdetected} to deteriorate the detector's performance to $0\%$. 
Note that retrieval-based detectors are concerning since they might lead to serious privacy issues from storing usersâ€™ LLM
conversations.



\section{More Details on AI Paraphrasers}
\label{app:aiparaphraser}

\subsection{Human Evaluation Study on Paraphrases}
\label{app:human_study}

Apart from measuring the perplexity scores of the paraphrases using OPT-13B and performance on the SQUaD-v2 benchmark, we perform two human evaluation studies to investigate the quality of the paraphrases from DIPPER and LLaMA-2-7B-Chat we use for the paraphrasing attack. We pick 20 random watermarked passages and their corresponding five rounds of recursive paraphrases (\texttt{pp1} to \texttt{pp5}) for each of the paraphrasers for human evaluation. Each paraphrase is evaluated by 3 unique MTurk workers. 
% One of the twenty watermarked passages generated by the target LLM was non-English, and hence eliminated from the human evaluation. 
% Therefore, our study includes a total of 95 recursive paraphrases. 
We use the same setup as \cite{krishna2023paraphrasing} for our human study. As shown in Figure \ref{fig:mturkui}, users are given a source text with some highlighted portion. The non-highlighted portion of the source text is input into the target OPT-13B model that generates watermarked text which is highlighted for the user's reference. DIPPER or LLaMA-2 paraphrases of the highlighted text are provided as the paraphrasing. The user is supposed to evaluate the quality of the paraphrases with respect to the highlighted watermarked text. They are supposed to rate it on a Likert scale of 1 to 5. See Tables~\ref{tab:mturk-content},\ref{tab:mturk-content-llama} for the evaluation summary on content preservation of DIPPER paraphrasers based on the user study. Tables~\ref{tab:mturk-grammar},\ref{tab:mturk-grammar-llama} show the summary of the evaluation of text quality/grammar of the paraphrases. For the content preservation study, we use the following Likert scale: 5 -- preserves the meaning of the source but differs in words and/or structure. 4 -- preserves most information in the source but differs in some minor factual details. 3 -- reserves some information in the source but differs in certain significant ways. 2 -- topically related to the source but most information in the source is not preserved. 1 -- not topically related.
For the text quality or grammar quality study, we use the following Likert scale: 5 -- the paraphrase has excellent grammar/quality with respect to the highlighted source. 4 -- the paraphrase is clear and correct with minor grammatical errors. 3 -- the paraphrase has few grammatical errors, but remains clear and comparable to highlighted source text. 2 -- the paraphrase has significant number of grammatical errors, but remains understandable. 1 -- the paraphrase is inferior to the highlighted source text with a lot of grammatical errors, may be difficult to comprehend.


Based on the evaluations, $70\%$ and $83\%$ of the paraphrases are rated high quality in terms of content preservation for DIPPER and LLaMA-2, respectively. $89\%$ and $88\%$ of the paraphrases are rated to have high-quality text/grammar for DIPPER and LLaMA-2, respectively. Hence, our human study indicates that watermarking detectors can be evaded using recursive paraphrases with only a slight degradation in text quality.


\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/mturk-ui.pdf}
    \caption{MTurk user interface for human evaluation of paraphrases for content preservation.}
    \label{fig:mturkui}
\end{figure}

\begin{table}[h!]
    \centering\renewcommand\cellalign{lc}
    \setcellgapes{3pt}\makegapedcells
    \begin{adjustbox}{max width=.95\textwidth}
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    \textbf{{ppi}} & \makecell{\textbf{Average}\\\textbf{rating}} & \makecell{\textbf{Sum of}\\\textbf{5 \& 4 (\%)}} & \makecell{\textbf{5 - Approx.}\\\textbf{equivalent (\%)}} & \makecell{\textbf{4 - nearly}\\\textbf{equivalent (\%)}} & \makecell{\textbf{3 - Somewhat}\\\textbf{equivalent (\%)}} & \makecell{\textbf{2 - Topically}\\\textbf{related (\%)}} & \makecell{\textbf{1 - Topically}\\\textbf{unrelated (\%)}}\\
    \midrule \midrule
    i=1 & 4.0 $\pm$ 0.8 & 70.2	& 29.8	& 40.4	& 29.8	& 0.0 & 0.0\\
    \midrule
    i=2&	4.1 $\pm$ 0.8	&77.2	&33.3	&43.9	&19.3	&3.5& 0.0\\
    \midrule
    i=3&	3.9 $\pm$ 0.9&	63.2&	33.3&	29.8&	33.3	&3.5& 0.0\\
    \midrule
    i=4&	4.2 $\pm$ 0.9&	80.0&	49.1&	30.9&	14.5&	5.5& 0.0\\
    \midrule
    i=5	&3.7 $\pm$ 1.1&	61.4&	29.8&	31.6	&21.1&	17.5& 0.0\\
    \midrule \midrule
    \textbf{All ppi}	&\textbf{4.0 $\pm$ 0.9}	&\textbf{70.4}	&\textbf{35.1}	&\textbf{35.3}	&\textbf{23.6}&	\textbf{6.0}& \textbf{0.0} \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{0.2cm}
    \caption{{ MTurk human evaluation of recursive paraphrases with DIPPER for content preservation.} \texttt{ppi} represents the \texttt{i}$^{th}$ round of recursive paraphrasing.}
    \label{tab:mturk-content}
    \vspace{2mm}

        \centering\renewcommand\cellalign{lc}
    \setcellgapes{3pt}\makegapedcells
    \begin{adjustbox}{max width=.95\textwidth}
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    \textbf{{ppi}} & \makecell{\textbf{Average}\\\textbf{rating}} & \makecell{\textbf{Sum of}\\\textbf{5 \& 4 (\%)}} & \makecell{\textbf{5 - Approx.}\\\textbf{equivalent (\%)}} & \makecell{\textbf{4 - nearly}\\\textbf{equivalent (\%)}} & \makecell{\textbf{3 - Somewhat}\\\textbf{equivalent (\%)}} & \makecell{\textbf{2 - Topically}\\\textbf{related (\%)}} & \makecell{\textbf{1 - Topically}\\\textbf{unrelated (\%)}}\\
    \midrule \midrule
    i=1 & 4.37 $\pm$ 0.63 & 91.67	& 45.0	& 46.67	& 8.3	& 0.0 & 0.0\\
    \midrule
    i=2&	4.18 $\pm$ 0.67 & 85.0	& 33.33	& 51.67	& 15.0	&0.0& 0.0\\
    \midrule
    i=3&	3.93 $\pm$ 0.71&	80.0&	21.67&	58.33&	16.67	&3.3& 0.0\\
    \midrule
    i=4&	3.85 $\pm$ 0.78&	78.33&	21.67&	56.67&	16.67&	5.0& 0.0\\
    \midrule
    i=5	&3.7 $\pm$ 1.1&	80.0&	18.33&	61.67	&11.67&	8.33& 0.0\\
    \midrule \midrule
    \textbf{All ppi}	&\textbf{4.05 $\pm$ 0.2}	&\textbf{83.0}	&\textbf{28.0}	&\textbf{55.0}	&\textbf{13.67}&	\textbf{3.33}& \textbf{0.0} \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{0.2cm}
    \caption{{ MTurk kuman evaluation of recursive paraphrases with LLaMA-2-7B-Chat for content preservation.} \texttt{ppi} represents the \texttt{i}$^{th}$ round of recursive paraphrasing.}
    \label{tab:mturk-content-llama}

    \vspace{2mm}
    
    \centering\renewcommand\cellalign{lc}
    \setcellgapes{3pt}\makegapedcells
    \begin{adjustbox}{max width=0.8\textwidth}
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    \textbf{{ppi}} & \makecell{\textbf{Average}\\\textbf{rating}} & \makecell{\textbf{Sum of}\\\textbf{5 \& 4 (\%)}} & \makecell{\textbf{5 - Excellent}\\\textbf{(\%)}} & \makecell{\textbf{4 - Good}\\\textbf{(\%)}} & \makecell{\textbf{3 - Fair}\\\textbf{(\%)}} & \makecell{\textbf{2 - Adequate}\\\textbf{(\%)}} & \makecell{\textbf{1 - Poor}\\\textbf{(\%)}}\\
    \midrule \midrule
    i=1	&4.28$\pm$ 0.67	&87.72	&40.35	&47.37	&12.28&	0.00	&0.00\\
    \midrule
i=2	&4.12 $\pm$ 0.50	&92.98	&19.30	&73.68&	7.02	&0.00	&0.00\\
    \midrule
i=3	&4.12 $\pm$ 0.53	&91.23	&21.05	&70.18&	8.77&	0.00	&0.00\\
    \midrule
i=4	&4.11 $\pm$0.64&	84.21	&26.32&	57.89	&15.79	&0.00	&0.00\\
    \midrule
i=5	&4.07 $\pm$ 0.53	&89.47	&17.54	&71.93&	10.53&	0.00	&0.00\\
    \midrule
    \midrule 
    \textbf{All ppi}	&\textbf{4.14 $\pm$ 0.58	}	&\textbf{89.12}	&\textbf{24.91}	&\textbf{64.21}	&\textbf{10.88}&	\textbf{0.0}& \textbf{0.0} \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{0.2cm}
    \caption{{ Human evaluation of recursive paraphrases using MTurk for text quality/grammar.} \texttt{ppi} represents the \texttt{i}$^{th}$ round of recursive paraphrasing.}
    \label{tab:mturk-grammar}

    \vspace{2mm}

    \centering\renewcommand\cellalign{lc}
    \setcellgapes{3pt}\makegapedcells
    \begin{adjustbox}{max width=0.8\textwidth}
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    \textbf{{ppi}} & \makecell{\textbf{Average}\\\textbf{rating}} & \makecell{\textbf{Sum of}\\\textbf{5 \& 4 (\%)}} & \makecell{\textbf{5 - Excellent}\\\textbf{(\%)}} & \makecell{\textbf{4 - Good}\\\textbf{(\%)}} & \makecell{\textbf{3 - Fair}\\\textbf{(\%)}} & \makecell{\textbf{2 - Adequate}\\\textbf{(\%)}} & \makecell{\textbf{1 - Poor}\\\textbf{(\%)}}\\
    \midrule \midrule
    i=1	&4.62$\pm$ 0.55	&96.67	&65.0	&31.67	& 3.33 &	0.00	&0.00\\
    \midrule
i=2	&4.28 $\pm$ 0.73	&83.33	& 45.0 & 38.33 &	16.67	&0.00	&0.00\\
    \midrule
i=3	&4.26 $\pm$ 0.65	&88.33	& 43.33	& 45.0 &	11.67&	0.00	&0.00\\
    \midrule
i=4	&4.22 $\pm$0.64&	88.33	&38.33&	50.0	&11.67	&0.00	&0.00\\
    \midrule
i=5	&4.17 $\pm$ 0.74	&83.33	& 40.0	& 43.33&	15.0 &	1.67&0.00\\
    \midrule
    \midrule 
    \textbf{All ppi}	&\textbf{4.32 $\pm$ 0.35	}	&\textbf{88.0}	&\textbf{46.33}	&\textbf{41.67}	&\textbf{11.67}&	\textbf{0.33}& \textbf{0.0} \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{0.2cm}
    \caption{{ Human evaluation of recursive paraphrases using MTurk for text quality/grammar.} \texttt{ppi} represents the \texttt{i}$^{th}$ round of recursive paraphrasing.}
    \label{tab:mturk-grammar-llama}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{LLaMA-2 Chat Template}
\label{app:systemprompts}

Below, we provide the chat template we use to employ LLaMA-2-7B-Chat as a paraphraser.

\noindent\fbox{%
    \parbox{\textwidth}{%
    \textbf{System Prompt:} You are a paraphraser. You are given an input passage `INPUT'.
        You should paraphrase `INPUT' to print `OUTPUT'.
        `OUTPUT' shoud be diverse and different as much as possible from `INPUT' and should not copy any part verbatim from `INPUT'. 
        `OUTPUT' should preserve the meaning and content of 'INPUT' while maintaining text quality and grammar. 
        `OUTPUT' should not be much longer than `INPUT'. 
        You should print `OUTPUT' and nothing else so that its easy for me to parse.\\
   \textbf{User Prompt:} INPUT: \textcolor{blue}{[Add input passage here]}
    }%
}

Below, we provide the chat template we use to employ LLaMA-2-13B-Chat as a question-answering model.

\noindent\fbox{%
    \parbox{\textwidth}{%
    \textbf{System Prompt:} You are given a context `C: \textcolor{blue}{[Add context passage here]}' and a question `Q: \textcolor{blue}{[Add question here]}'.
        Let `A' be the answer to question `Q' solely based on context `C'. 
        You are given the true answer `A1: \textcolor{blue}{[Add ground truth answer here]}' for question `Q'. \\
   \textbf{User Prompt:} INPUT: Do answers `A1' and `A' match? 
        You SHOULD only be printing either `YES' or 'NO'.
    }%
}


\clearpage
\subsection{Example Paraphrases}
\label{app:example-paraphrases}
\begin{table}[h!]
    \centering
    \begin{adjustbox}{totalheight=\textheight-6\baselineskip}
    \begin{tabular}{P{1.8cm}|M{19.5cm}}
    \toprule
    Description     &  \multicolumn{1}{P{19.5cm}}{Text} \\ \midrule \midrule
    Input     &  {The draw was conducted by former Premier League referee Mark Clattenburg, who had been involved in the draw for the first match of the season. He is employed by Premier League broadcast partner Channel 5 as a television match official. The draw for the first round was conducted on live television at 10.30pm Thai time, with the match to be shown on Thailand's Channel 5 as part of the BBC's coverage of the tournament. The fourth round was drawn on live television at 10pm Thai time, with matches to be shown on Thailand's Channel 5 as part of the BBC's coverage of the tournament. The draw for the fifth round was conducted on live television at 10.25pm Thai time, with matches to be shown on Thailand's Channel 5 as part of the BBC's coverage of the tournament. The sixth round was drawn on live television at 10.15pm Thai time, with matches to be shown on Thailand's Channel 5 as part of the BBC's coverage of the tournament. The seventh round was drawn on live television at 10.15pm Thai time, with matches to be shown on Thailand's Channel 5 as part of the BBC's coverage of the tournament. The draw for the eighth round was conducted on live television at 10.15pm Thai time, with matches to be shown on Thailand's Channel 5 as part of the BBC's coverage of the tournament. The draw for the ninth round was conducted on live television at 10.15pm Thai time, with matches to be} \\ \midrule
    {Paraphrase} & { Mark Clattenburg, a former Premier League referee, who officiated at the opening game of the season, was in charge of the draw. He is now employed by the FA's broadcast partner, Channel 5, as a television match official. The draw for the first round was made on live TV at 10.30pm local time, with Channel 5 in Thailand broadcasting the event as part of the BBC's coverage of the tournament. The fourth round was drawn live on Thai TV at 10pm local time, with matches to be screened on Channel 5 in Thailand as part of the BBC's coverage of the competition. The draw for the fifth round took place live on Thai TV at 10.25pm local time, with Channel 5 in Thailand to broadcast the ties as part of the BBC's coverage. The sixth round was drawn live on Thai TV at 10.15pm. Channel 5 in Thailand to show matches as part of BBC's coverage. The draw for the eighth round was made live on Thai TV at 10.15pm. Channel 5 in Thailand to show matches as part of BBC's coverage of the competition.}  \\ \midrule \midrule
    Input     &  { "We had to cancel his operation and put off on him, he was supposed to have it on Christmas Eve." He said Josh and his family had been in touch with some people from the US and Australia who are in Belfast right now. He is one of the first people from the US to come to Belfast to meet and speak with Josh, and the rest of the family will visit from the US in the next few days. "Josh has become a worldwide man's man, and it has been incredible to see how everybody has taken to him. He's an ambassador for Irish people and for Ireland, and the people are really behind him. He's a great ambassador for Ireland and for his country and it has been really good." Josh was taken to Queen's University Hospital for surgery and they discovered and cut down his appendix, which had been causing him pain for more than a year. The surgery took place on January 11. He is still on painkillers, and the family are encouraging people to keep praying for his recovery. The hashtag \#prayforjosh and the post and the message he messages are now being shared widely through social media by people from the US and Australia. The most recent tweets from his father's account are about his recovery in Belfast. He is also being accompanied by a "pray warriors" team and the hashtag \#prayforjosh is now being used by other campaigners to urge people to send support to Josh. The church is also asking people to help} \\ \midrule
    {Paraphrase} & { "His operation was cancelled and put back - it was due to be on Christmas Eve." He said that the family and friends of Josh have been in touch with people from the United States and Australia who are currently in Belfast. He himself was one of the first to arrive in the city to meet and talk to the teenager, and more of the family will arrive in a few days. "Josh has become a 'global' man and it's been amazing to see how people have taken to him. He's a real Irishman and an ambassador for Ireland and it's been really good for him." He said that on January 11th, when he was taken to Queen's University Hospital in Belfast for the operation, a large primary cancer and five smaller growths were removed. He is still recovering and the family ask people to keep praying for his recovery. The post with the hashtag \#prayforjosh and the messages it contains have now been shared all over social media by people in the US and Australia. The last few posts on his father's Twitter account are about his recovery in Belfast. He has also been surrounded by a team of prayer warriors and the campaign has now widened, with people using the tag \#prayforjosh to urge others to show their support for him. The church is also calling on people to help in other ways.}  \\ \midrule \midrule
    Input     &  { I've enjoyed my time with the team."
Wales, who won 11 medals in total in Brazil, failed to make the podium at three consecutive Games from 1996 to 2008 before Doull stepped into the team and helped to end the long wait for a Welsh team pursuit gold. The Welsh team were in fine form on Saturday as they captured their 11th medal, equalling the record of 14 won in 1972. The Welsh team's run to gold this year was well-documented. Geraint Thomas, the 2008 and 2012 Olympic champion, set an Olympic record of 31 minutes 59.4 seconds to help Great Britain to gold. Born in Cardiff but living in Aberystwyth, Anthony Iwan Thomas was selected as one of the two Welsh track riders for Rio and earned a silver medal in the men's road race, which he won in August.
"It's a dream come true to come back and win Olympic gold here," said Thomas. "To do it in this stadium, it's just surreal. I can't explain it. Let me get home and I'll be really grateful to everyone." He missed the 2012 Games through injury but returned to gold-winning form in 2016 when he won his third consecutive world championship in the men's road race, finishing second in Rio.
"It's a real honour to be on the podium today," added the 35-year-old. "There's a lot of times when you think 'this is it' and you think} \\ \midrule
    {Paraphrase} & { I've loved every minute with the team. " Wales, who won 11 medals in Brazil, had not finished on the podium at three consecutive Olympic games from 1996 to 2008 before Doull helped end the country's long wait for a team pursuit gold medal. The nation's 11th medal on the last day equalled the record of 14 set in 1972. So many stories of success for the Welsh riders in Rio have already been written. Born in Cardiff but now based in the Ceredigion resort of Aberystwyth, the son of Geraint Thomas won silver in the men's road race. Thomas said: "It's just a dream come true to come back and win gold here and it's even more special to do it in this stadium. " If I get home I'll thank everybody. " The 38-year-old had to miss the 2012 Olympic games with injury but was back to winning ways in 2016 when he retained his world title and finished second in Rio. "There's so many times when you think, 'This is the day' and it never comes."}  \\ \midrule \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{0.2cm}
    \caption{Examples of paraphrased passage from the XSum dataset. The paraphrasing is performed using DIPPER \citep{krishna2023paraphrasing}.}
    \label{tab:para-examples-1}
\end{table}
\begin{table}[h!]
    \centering
    \begin{adjustbox}{totalheight=\textheight-6\baselineskip}
    \begin{tabular}{P{1.8cm}|M{19.5cm}}
    \toprule
    Description     &  \multicolumn{1}{P{19.5cm}}{Text} \\ \midrule \midrule
    Input     &  { The force has said it has no intention of changing policy or use of force, and will continue to use the weapons as part of its role policing the public. A spokesman for Scotland Yard said: "Officers in Scotland will continue to have a range of weapons on-hand, including tasers, pepper spray and a handgun and they will be used in accordance with Taser International guidelines. "We also have well established policies governing the use of these weapons and the use of force, and the use of force is judged to be proportionate when appropriate." The move was welcomed by a leading rights campaign group. John O'Donnell, the head of the High Court, said Tasers should have been compulsory in Scotland in 2015 following the death of a journalist in custody in Newcastle and the deaths of two more while in police custody. He said: "The use of these weapons by police in Scotland has been widely condemned and the High Court ruled they should be used to protect the public." A spokesman for the Department for Transport said: "No police force should decide for itself what constitutes necessary force and the use of these weapons by police in Scotland has been widely condemned. "The use of these weapons by officers must be subject to strict rules which safeguard the public and the officers' rights." The London police said they "do not comment on intelligence matters".?} \\ \midrule
    {Paraphrase} & { The Met said it had no plans to change its policy on the use of force and said the Tasers would be used "when it is appropriate and proportionate". A spokesman said: "Officers in Scotland will continue to carry a range of weapons including Tasers, pepper spray and handguns. "We have well-established policies on the use of these weapons and the use of force in general and we will continue to ensure these are followed." It was welcomed by the Scottish Human Rights Commission. The high court judge, Lord O'Donnell, said in February that the use of Tasers by police in Scotland should have been compulsory in the wake of the death of a journalist in police custody in Newcastle, and the subsequent deaths of two more people in police custody. He said: "The use of these weapons by police in Scotland has been widely condemned and the high court has ruled they must be used to protect the public." The Department for Transport said police should not be able to decide for themselves when it was appropriate to use force. "The use of these weapons by officers must be subject to strict rules which protect the public and officers' rights." The Met said it did not comment on intelligence matters. Would you like to book an appointment with the British Transport Police?}  \\ \midrule \midrule
    Input     &  { "He gets on with me. I have no doubt about his motives." The move is also an enormous gamble but Koukash, a self-made millionaire who made his fortune as a real estate mogul in Dubai, is determined to make sure the gamble pays off. He is willing, he says, to let the man who guided Salford to seven Grand Final appearances head a great project. "He has an incredible track record of creating people and businesses," said Noble. "He has done it in Qatar, in Dubai, in America, here." He is also one of the game's most ruthless businessmen. Koukash, who moved to the UK from Sudan with his parents as a boy and has spent the past 30 years building his empire of clubs and businesses, has seen the game of rugby league decline dramatically over the past 10 years. He claims that the game has never been more popular than it is today. He is also a fervent supporter of the game and this makes his interest in rugby league even more compelling. Salford have been in dire straits and Koukash, who has ploughed much of his own money in the club, has promised to help them become one of the great clubs of the game. The club has all the right qualities and Koukash wants to make sure it happens. He has spent the past two weeks scouting for players to recruit and has already seen the arrival of two promising youngsters. The Reds need players to make them competitive} \\ \midrule
    {Paraphrase} & { He added: "I know him. I have no doubt about his intentions." It's a huge gamble for the self-made millionaire who has made his money in property in Dubai. He is happy to entrust the man who took Salford to seven Challenge Cup finals with his great plan. " He's got an incredible record of turning around people and businesses," Noble said. "He's done it in Qatar, in Dubai, in America, here." He's also one of the most ruthless businessmen in the game. Having arrived in Britain as a boy from Sudan with his parents, he's spent the past 30 years building an empire of clubs and businesses. He's also seen the game decline in popularity over the past decade, but insists it's now more popular than ever. He is a huge supporter of the game and that's why he's interested in the sport. Salford have been in crisis and he's promised to help them become one of the great clubs. The club has all the attributes and Koukash is determined to make it happen. He has spent the past fortnight looking at players and has already recruited two. He has to make the Reds a more competitive side and has already brought in a couple of players who have impressed.}  \\ \midrule \midrule
    Input     &  { She said he would not stop attacking and asked for help. She said she wanted "peace" but "not death". Henderson-McCarroll, of St Nicholas Drive, Newry, admitted manslaughter while in charge of a dangerous drug. She said her actions were a "blip in my mind" as a result of a "bad decision" to take drugs. Justice Treacy said he would not impose a custodial sentence on Henderson-McCarroll, but instead sentenced her to three years' imprisonment. The judge said he would not impose the maximum sentence for manslaughter given the circumstances, but felt he would not impose the minimum of two and a half years. He told Henderson-McCarroll: "He (Mr Girvan) would not be in his right mind if he would not have let his guard down. If there was one thing the jury should have heard - it was that your actions were a blip in my mind. You didn't intend to kill him. You were acting in self-defence. You poked him and your actions were a blip and a bit of a lapse in judgment." The judge said the maximum sentence for manslaughter given Henderson's previous convictions would have amounted to between five and seven years. The judge said it was "not an uncommon crime" to kill someone in self-defence. He said sentencing Henderson was an "ugly case of drug-induced madness." He added: "He (Mr Girvan) must have suffered terribly."} \\ \midrule
    {Paraphrase} & { She said he had not stopped attacking her and she called for help. She said she wanted "peace" but not death. Henderson-McCarroll, of St Nicholas Drive in Newry, admitted manslaughter while under the influence of a sedative. She said her actions were the result of a "mistake" after taking drugs. Mr Justice Treacy said he would not grant a suspended sentence, but instead would sentence her to three years in prison. He said he would not impose the maximum sentence for manslaughter, in the circumstances, but did not feel he should impose the minimum term of two and a half years. He said to the defendant: "You could not in your right mind have left your guard down, you did not intend to kill him. If there was one thing the jury ought to have heard, it was that your actions were a momentary lapse of reason. You acted in self defence, you poked him with a knife, your actions were momentary and a lapse of reason." The judge said that given the defendant's previous record the maximum sentence for manslaughter, with a minimum term of a year, would have been five to seven years. He said it was not an uncommon crime for someone to kill in self defence. " But in this case, it was an ugly case of drug induced madness. " He added: "Mr Girvan must have suffered horribly."}  \\ \midrule \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{0.2cm}
    \caption{Examples of paraphrased passage from the XSum dataset. The paraphrasing is performed using DIPPER \citep{krishna2023paraphrasing}.}
    \label{tab:para-examples-2}
\end{table}
\begin{table}[h!]
    \centering
    \begin{adjustbox}{totalheight=\textheight-6\baselineskip}
    \begin{tabular}{P{1.8cm}|M{14cm}}
    \toprule
    Description     &  \multicolumn{1}{P{14cm}}{Text} \\ \midrule \midrule
    Input     &  {In the year 2014-2015, Prison Link Cymru made 1,099 referrals, and said some offenders waited up to a year before finding suitable accommodation. Staff from the charitable organization say that the investment in housing would be cheaper than having to re-lock homeless offenders. The Welsh government said that more people than ever are getting help to deal with their housing problems. Changes to the Welsh Housing Act in 2015 removed priority rights for ex-offenders. Prison Link Cymru, which helps people to find accommodation after they have been released, said that women generally had a good time, as they had to deal with children and other problems.} \\ \midrule
    \texttt{pp1} & {In the year 2014-2015, Prison Link Cymru made 1,099 referrals, and said some offenders waited up to a year before finding suitable accommodation. Staff from the charitable organization say that the investment in housing would be cheaper than having to re-lock homeless offenders. The Welsh government said that more people than ever are getting help to deal with their housing problems. Changes to the Welsh Housing Act in 2015 removed priority rights for ex-offenders. Prison Link Cymru, which helps people to find accommodation after they have been released, said that women generally had a good time, as they had to deal with children and other problems.}  \\ \midrule
    \texttt{pp2} & {In 2015-16, Prison Link Cymru had 1,099 referrals and said some offenders were living rough for up to a year before finding suitable accommodation. Workers at the charity claim investment in housing would be cheaper than locking up homeless repeat offenders. The Welsh Government said more people than ever before were getting help to address housing problems. Prison Link Cymru, which helps people find accommodation after they have left prison, said things were generally good for women because issues such as children or domestic violence were often the main reasons they returned to prison.} \\ \midrule
    \texttt{pp3} & {In fact, it was one of the main reasons why they came back to prison. In the year 2015-16, Prison Link Cymru made 1,099 referrals and said that offenders had lived for up to a year before finding suitable accommodation. The workers at the charitable organization say that the investment in housing would be cheaper than re-locking homeless offenders. The government of Wales says that more people than ever are being helped to deal with their housing problems. Prison Link Cymru, which helps people to find accommodation after being released, says that women are generally in a good position, as children and other problems are often the main reason why they are returned to prison.} \\ \midrule
    \texttt{pp4} & {In the year to the end of March, Prison Link Cymru had 1,099 referrals and said offenders had been living rough for up to a year before finding suitable accommodation. Workers at the charity say investment in housing would be cheaper than re-imprisoning homeless repeat offenders. The Welsh Government says more people than ever before are getting help to address their housing problems. Prison Link Cymru, which helps people find accommodation after they have been released from prison, said generally things were good for women because issues such as children and other problems were often the main reasons they returned to prison.
} \\ \midrule
    \texttt{pp5} & {The government of Wales says that more people than ever before are being helped to deal with their housing problems. In the year 2015-16, Prison Link Cymru referred 1,099 people and said that homeless people had lived up to a year before finding suitable accommodation. The workers at the charitable organization say that the investment in housing would be cheaper than imprisoning homeless offenders again. Prison Link Cymru, which helps people find accommodation after being released, says that women are generally well, because children and other problems are often the main reason why they return to prison.} \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{0.2cm}
    \caption{Example of a recursively paraphrased passage from the XSum dataset. The paraphrasing is performed using DIPPER \citep{krishna2023paraphrasing}. \texttt{ppi} refers to the output after \texttt{i} rounds of recursive paraphrasing.}
    \label{tab:rec-para-examples}
\end{table}




\begin{table}[h!]
    \centering
    \begin{adjustbox}{totalheight=\textheight-6\baselineskip}
    \begin{tabular}{P{1.8cm}|M{17cm}}
    \toprule
    Description     &  \multicolumn{1}{P{17cm}}{Text} \\ \midrule \midrule
    Input     &  { The final round of fixtures will be played in Belfast on Tuesday. Elsewhere, Ireland's U19 women face Wales on Saturday in their Euro U19 women qualifier. The Irish women are already assured of a third-placed finish and a place at next year's finals in Northern Ireland after wins over Bulgaria and Greece. If they win against Wales they will finish second and qualify for a play-off for the finals. The Irish U19 women face Wales in Aberystwyth on Saturday. The match kicks off at 1.15pm. The Irish U19 women are in excellent form having won their last four matches. The winning margins have been 4-0, 4-0, 3-0, 3-0 and 3-0. The Irish U19 women sit fourth in the qualifying group and could qualify for the finals if they beat Wales on Saturday. The Irish women are in impressive form having won their last four games and qualified for next year's finals. Saturday's game against Wales is one of the most important of the campaign as a win will ensure they finish second and qualify for a play-off for the finals. The Irish will come up against whoever comes top of the group - Denmark, Portugal, Germany and Sweden - with the second place finisher going into a play-off for a final spot. The Irish U19 women took down Bulgaria 3-0, won 3-0 away to Greece and hammered home five against a young Spanish team.} \\ \midrule
    \texttt{pp1} & {On Tuesday, the final round of matches will take place in Belfast. Meanwhile, Ireland's U19 women's team faces Wales in their Euro U19 women's qualifier on Saturday in Aberystwyth. The Irish are already assured of a third-placed finish and a spot in next year's finals after their wins over Bulgaria and Greece. A victory against Wales would see them finish second and qualify for a playoff for the finals. The Irish U19 women have been in impressive form, winning their last four matches by margins of 4-0, 4-0, 3-0, 3-0, and 3-0. They sit fourth in the qualifying group and could qualify for the finals if they beat Wales on Saturday. The Irish team will face the top finisher from the group - Denmark, Portugal, Germany, or Sweden - in a playoff for a final spot. The Irish U19 women have had a strong campaign so far, defeating Bulgaria 3-0, winning 3-0 away to Greece, and scoring five goals against a young Spanish team.}  \\ \midrule
    \texttt{pp2} & {On the eve of the final round of matches in Belfast, the Irish U19 women's team prepares to face Wales in their Euro U19 women's qualifier on Saturday in Aberystwyth. While Ireland has already secured a third-placed finish and a spot in next year's finals with their wins over Bulgaria and Greece, a victory against Wales would see them finish second and qualify for a playoff for the finals. The Irish U19 women have been in impressive form, winning their last four matches by margins of 4-0, 4-0, 3-0, 3-0, and 3-0. They currently sit fourth in the qualifying group and could potentially qualify for the finals if they beat Wales on Saturday. The team will face the top finisher from the group - Denmark, Portugal, Germany, or Sweden - in a playoff for a final spot. Ireland has had a strong campaign so far, defeating Bulgaria 3-0, winning 3-0 away to Greece, and scoring five goals against a young Spanish team.} \\ \midrule
    \texttt{pp3} & {On the eve of the final round of matches in Belfast, the Northern Irish U19 women's team prepares to face Wales in their Euro U19 women's qualifier on Saturday in Aberystwyth. While Northern Ireland has already secured a third-placed finish and a spot in next year's finals with their wins over Spain and Scotland, a victory against Wales would see them finish second and qualify for a playoff for the finals. The Northern Irish U19 women have been in impressive form, winning their last four matches by margins of 4-0, 4-0, 3-0, and 3-0, and currently sit fourth in the qualifying group. They could potentially qualify for the finals if they beat Wales on Saturday, and face the top finisher from the group - Denmark, Portugal, Germany, or Sweden - in a playoff for a final spot. Northern Ireland has had a strong campaign so far, defeating Spain 2-1, winning 3-0 away to Scotland, and scoring five goals against a young Irish team.} \\ \midrule
    \texttt{pp4} & {On the eve of the final round of matches in Northern Ireland, the local U19 women's team prepares to face Wales in their Euro U19 women's qualifier on Saturday in Cardiff. While Northern Ireland has already secured a third-placed finish and a spot in next year's finals with their recent wins, a victory against Wales would see them finish second and qualify for a playoff for the finals. The Northern Irish U19 women have been in impressive form, winning their last four matches by margins of 4-0, 4-0, 3-0, and 3-0, and currently sit fifth in the qualifying group. They could potentially qualify for the finals if they beat Wales on Saturday, and face the top finisher from the group - Denmark, Portugal, Germany, or Sweden - in a playoff for a final spot. Northern Ireland has had a strong campaign so far, defeating Spain 2-1, winning 3-0 away to Scotland, and scoring five goals against a young Irish team.
} \\ \midrule
    \texttt{pp5} & {On the eve of the crucial final match in Cardiff, the Northern Ireland U19 women's team prepares to face Wales in their Euro U19 women's qualifier on Saturday. With their recent string of victories, the Northern Irish have already secured a third-placed finish and a spot in next year's finals, but a win against Wales would see them finish second and qualify for a playoff for the finals. The team has been in impressive form, winning their last four matches by margins of 4-0, 4-0, 3-0, and 3-0, and currently sit fifth in the qualifying group. They could potentially qualify for the finals if they beat Wales on Saturday, and face the top finisher from the group - Denmark, Portugal, Germany, or Sweden - in a playoff for a final spot. Despite their strong campaign so far, the Northern Irish will face a tough challenge against Wales, who have also had a impressive run of form in the qualifiers.} \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{0.2cm}
    \caption{Example of a recursively paraphrased passage from the XSum dataset. The paraphrasing is performed using LLaMA-2-7B-Chat \citep{llama}. \texttt{ppi} refers to the output after \texttt{i} rounds of recursive paraphrasing.}
    \label{tab:rec-para-examples-llama}
\end{table}

\clearpage
\section{Proofs and Corollaries}
\label{app:proof-corollary}

\subsection{Proof of Theorem~\ref{thm:ROC_bound}}
\label{app:proof_roc_bnd}
\begin{apptheorem}
The area under the ROC of any detector $D$ is bounded as
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2}.\]
\end{apptheorem}

\begin{proof}
The ROC is a plot between the true positive rate (TPR) and the false positive rate (FPR), which are defined as follows:
\begin{align*}
    \mathsf{TPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{M}}[D(s) \geq \gamma]\\
    \text{and } \mathsf{FPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{H}}[D(s) \geq \gamma],
\end{align*}
where $\gamma$ is some classifier parameter.
We can bound the difference between the $\mathsf{TPR}_\gamma$ and the $\mathsf{FPR}_\gamma$ by the total variation between $M$ and $H$:
\begin{align}
    |\mathsf{TPR}_\gamma - \mathsf{FPR}_\gamma| &= \left| \mathbb{P}_{s \sim \mathcal{M}}[D(s) \geq \gamma] - \mathbb{P}_{s \sim \mathcal{H}}[D(s) \geq \gamma] \right| \leq \mathsf{TV}(\mathcal{M}, \mathcal{H})
    \label{eq:TPR_FPR_TV_bound}\\
    \mathsf{TPR}_\gamma &\leq \mathsf{FPR}_\gamma + \mathsf{TV}(\mathcal{M}, \mathcal{H}).
    \label{eq:TPR_FPR_bound}
\end{align}
Since the $\mathsf{TPR}_\gamma$ is also bounded by 1 we have:
\begin{align}
\label{eq:tpr_bound}
\mathsf{TPR}_\gamma \leq \min(\mathsf{FPR}_\gamma + \mathsf{TV}(\mathcal{M}, \mathcal{H}), 1).
\end{align}
Denoting $\mathsf{FPR}_\gamma$, $\mathsf{TPR}_\gamma$, and $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ with $x$, $y$, and $tv$ for brevity, we bound the AUROC as follows:
\allowdisplaybreaks
\begin{align*}
    \mathsf{AUROC}(D) = \int_0^1 y \; dx &\leq \int_0^1 \min(x + tv, 1) dx\\
    &= \int_0^{1 -tv} (x + tv) dx + \int_{1-tv}^1 dx\\
    &= \left| \frac{x^2}{2} + tvx \right|_0^{1-tv} + \left| x \right|_{1-tv}^1\\
    &= \frac{(1-tv)^2}{2} + tv(1-tv) + tv\\
    &= \frac{1}{2} + \frac{tv^2}{2} - tv + tv - tv^2 + tv\\
    &= \frac{1}{2} + tv - \frac{tv^2}{2}.
\end{align*}
\end{proof}



\subsection{General Trade-offs For Detection}
\label{app:corollaries}

\textbf{Paraphrasing to Evade Detection.} Although our analysis considers general distributions, it can also be applied to specific scenarios, such as particular writing styles or sentence paraphrasing, by defining $\mathcal{M}$ and $\mathcal{H}$ appropriately.
For example, $\mathcal{M}$ can be the outputs from an LLM trained to mimic a particular set of people, or $\mathcal{H}$ can be the text distribution of a specific person.
Similarly, Corollary \ref{corollary:rephrase} shows that if a paraphraser's goal is to lower the TV between paraphrased AI text and human text, then detection gets harder.


Set $\mathcal{M} = \mathcal{R_M}(s)$ and $\mathcal{H} = \mathcal{R_H}(s)$ to be the distribution of sequences with similar meanings to $s$ produced by the paraphraser and humans, respectively.



\begin{corollary}
\label{corollary:rephrase}
The area under the ROC of the detector $D$ is bounded as
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{R_M}(s), \mathcal{R_H}(s)) - \frac{\mathsf{TV}(\mathcal{R_M}(s), \mathcal{R_H}(s))^2}{2}.\]
\end{corollary}

Another way to understand the limitations of AI-generated text detectors is directly through the characterization of the trade-offs between true positive rates and false positive rates. Adapting inequality \ref{eq:TPR_FPR_bound}, we have the following corollaries: 


\begin{corollary}
\label{corollary:rephrasing_wm}
For any watermarking scheme $W$,
\begin{align*}
    \Pr_{s_w\sim \mathcal{R}_{\mathcal{M}}(s)} [\text{$s_w$ is watermarked using $W$}] \leq &
    \Pr_{s_w\sim \mathcal{R}_{\mathcal{H}}(s)}[\text{$s_w$ is watermarked using $W$}] \\
    & + \mathsf{TV}(\mathcal{R_M}(s), \mathcal{R_H}(s)),
\end{align*}
where $\mathcal{R_M}(s)$ and $\mathcal{R_H}(s)$ are the distributions of rephrased sequences for $s$ produced by the paraphrasing model and humans, respectively.
\end{corollary}


Humans may have different writing styles. Corollary \ref{corollary:rephrasing_wm} indicates that if a rephrasing model resembles certain human text distribution $\mathcal{H}$ (i.e. $\mathsf{TV}(\mathcal{R_M}(s), \mathcal{R_H}(s))$ is small), then either certain people's writing will be detected falsely as watermarked (i.e. $\Pr_{s_w\sim \mathcal{R}_{\mathcal{H}}(s)} [\text{$s_w$ is watermarked using $W$}]$ is high) or the paraphrasing model can remove the watermark (i.e. $\Pr_{s_w\sim \mathcal{R}_{\mathcal{M}}(s)} [\text{$s_w$ is watermarked using $W$}]$ is low).


\begin{corollary}
\label{corollary:rephrasing_no_wm}
For any AI-text detector $D$, 
\begin{align*}
    \Pr_{s\sim \mathcal{M}} [\text{$s$ is detected as AI-text by $D$}] \leq \Pr_{s\sim \mathcal{H}}[\text{$s$ is detected as AI-text by $D$}] + \mathsf{TV}(\mathcal{M}, \mathcal{H}),
\end{align*}
where $\mathcal{M}$ and $\mathcal{H}$ denote text distributions by the model and by humans, respectively.
\end{corollary}

Corollary \ref{corollary:rephrasing_no_wm} indicates that if a model resembles certain human text distribution $\mathcal{H}$ (i.e. $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ is small), then either certain people's writing will be detected falsely as AI-generated (i.e. $\Pr_{s\sim \mathcal{H}} [\text{$s$ is detected as AI-text by $D$}]$ is high) or the AI-generated text will not be detected reliably (i.e. $\Pr_{s\sim \mathcal{M}} [\text{$s$ is detected as AI-text by $D$}]$ is low).

A recent work \citep{chakraborty2023possibilities} shows a trade-off on the detection problem with respect to the availability of the number of data samples for detection. They show a TV upper bound for the detector's AUROC using an information theoretic approach. However, the underlying assumption of their result is that several {\it independent} samples are available to the detector from either human or text distribution, which might not be a practical assumption since sentences in a document are often correlated with each other. Also, a large number of data samples need not be available for pragmatic scenarios. For example, it may not be practical for a text detector to ask a student to write multiple essays for an assignment or to assume that a Twitter bot would publish longer tweets that are completely written by the AI without any human intervention.


\subsection{Tightness Analysis for Theorem~\ref{thm:ROC_bound}}
\label{app:tightness}
In this section, we show that the bound in Theorem~\ref{thm:ROC_bound} is tight.
For a given distribution of human-generated text sequences $\mathcal{H}$, we construct an AI-text distribution $\mathcal{M}$ and a detector $D$ such that the bound holds with equality.
Define sublevel sets of the probability density function of the distribution of human-generated text $\mathsf{pdf}_\mathcal{H}$ over the set of all sequences $\Omega$ as follows:
\[\Omega_\mathcal{H}(c) = \{s \in \Omega \mid \mathsf{pdf}_\mathcal{H}(s) \leq c\}\]
where $c \in \mathbb{R}$.
Assume that, $\Omega_\mathcal{H}(0)$ is not empty.
Now, consider a distribution $\mathcal{M}$, with density function $\mathsf{pdf}_\mathcal{M}$, which has the following properties:
\begin{enumerate}
    \item The probability of a sequence drawn from $\mathcal{M}$ falling in $\Omega_\mathcal{H}(0)$ is $\mathsf{TV}(\mathcal{M}, \mathcal{H})$, i.e., $\mathbb{P}_{s \sim \mathcal{M}}[s \in \Omega_\mathcal{H}(0)] = \mathsf{TV}(\mathcal{M}, \mathcal{H})$.
    \item $\mathsf{pdf}_\mathcal{M}(s) = \mathsf{pdf}_\mathcal{H}(s)$ for all $s \in \Omega(\tau) - \Omega(0)$ where $\tau > 0$ such that $\mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega(\tau)] = 1 - \mathsf{TV}(\mathcal{M}, \mathcal{H})$.
    \item $\mathsf{pdf}_\mathcal{M}(s) = 0$ for all $s \in \Omega - \Omega(\tau)$.
\end{enumerate}
Define a hypothetical detector $D$ that maps each sequence in $\Omega$ to the negative of the probability density function of $\mathcal{H}$, i.e., $D(s) = - \mathsf{pdf}_\mathcal{H}(s)$.
Using the definitions of $\mathsf{TPR}_\gamma$ and $\mathsf{FPR}_\gamma$, we have:
\begin{align*}
    \mathsf{TPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{M}}[D(s) \geq \gamma]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[- \mathsf{pdf}_\mathcal{H}(s) \geq \gamma]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[\mathsf{pdf}_\mathcal{H}(s) \leq -\gamma]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma)]
\end{align*}
Similarly,
\[\mathsf{FPR}_\gamma = \mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega_\mathcal{H}(-\gamma)].\]
For $\gamma \in [-\tau, 0]$,
\begin{align*}
    \mathsf{TPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma)]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(0)] + \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma) - \Omega_\mathcal{H}(0)]\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma) - \Omega_\mathcal{H}(0)] \tag{using property 1}\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega_\mathcal{H}(-\gamma) - \Omega_\mathcal{H}(0)] \tag{using property 2}\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega_\mathcal{H}(-\gamma)] - \mathbb{P}_{s \sim \mathcal{H}}[s \in \Omega_\mathcal{H}(0)] \tag{$\Omega_\mathcal{H}(0) \subseteq \Omega_\mathcal{H}(-\gamma)$}\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathsf{FPR}_\gamma. \tag{$\mathbb{P}_{s \sim \mathcal{H}}[s \in \Omega_\mathcal{H}(0)] = 0$}
\end{align*}
For $\gamma \in [-\infty, -\tau]$, $\mathsf{TPR}_\gamma = 1$, by property 3.
Also, as $\gamma$ goes from $0$ to $-\infty$, $\mathsf{FPR}_\gamma$ goes from $0$ to $1$.
Therefore, $\mathsf{TPR}_\gamma = \min(\mathsf{FPR}_\gamma + \mathsf{TV}(\mathcal{M}, \mathcal{H}), 1)$ which is similar to Equation~\ref{eq:tpr_bound}.
Calculating the AUROC in a similar fashion as in the previous section, we get the following:
\[\mathsf{AUROC}(D) = \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2}.\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pseudorandomness in LLMs }
\label{app:PRG_bound}
Most machine learning models, including LLMs, use pseudorandom number generators in one form or another to produce their outputs.
For example, an LLM may use a pseudorandom number generator to sample the next token in the output sequence.
In discussing our hardness result, \cite{kirchenbauer2023reliability} in a more recent work argue that this pseudorandomness makes the AI-generated text distribution very different from the human-generated text distribution.
This is because the pseudorandom AI-generated distribution is a collection of Dirac delta function distributions, and a human is exorbitantly unlikely to produce a sample corresponding to any of the delta functions.
In our framework, this means that the TV between the human and pseudorandom AI-generated distributions is almost one, making the bound in Theorem~\ref{thm:ROC_bound} vacuous.

We argue that although the true TV between the human and pseudorandom AI-generated distributions is high and there exists (in theory) a detector function that can separate the distributions almost perfectly, this function may not be efficiently computable.
Any polynomial-time computable detector can only achieve a negligible advantage from the use of pseudorandomness instead of true randomness.
If we had knowledge of the seed used for the pseudorandom number generator, we would be able to predict the pseudorandom samples.
However, an individual seeking to evade detection could simply randomize this seed making it computationally infeasible to predict the samples.

We modify the bound in Theorem~\ref{thm:ROC_bound} to include a negligible correction term $\epsilon$ to account for the use of pseudorandomness.
We prove that the performance of a polynomial-time computable detector $D$ on a pseudorandom version of the AI-generated distribution $\widehat{\mathcal{M}}$ is bounded by the total variation for the truly random distribution $\mathcal{M}$ (resulting from the LLM using true randomness) as follows:
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2} + \epsilon.\]
The term $\epsilon$ represents the gap between the probabilities assigned by $\mathcal{M}$ and $\widehat{\mathcal{M}}$ to any polynomial-time computable $\{0, 1\}$-function $f$, i.e.,
\begin{equation}
\label{eq:eps_adv}
\big|\mathbb{P}_{s \in \mathcal{M}}[f(s) = 1] - \mathbb{P}_{s \in \widehat{\mathcal{M}}}[f(s) = 1]\big| \leq \epsilon.
\end{equation}
This term is orders of magnitude smaller than any of the terms in the bound and can be safely ignored.
For example, commonly used pseudorandom generators\footnote{Cryptographic PRNGs:\\\url{https://en.wikipedia.org/wiki/Pseudorandom_number_generator}} can achieve an $\epsilon$ that is bounded by a negligible function $1/b^t$ of the number of bits $b$ used in the seed of the generator for a positive integer $t$\footnote{Negligible function: \url{https://en.wikipedia.org/wiki/Negligible_function}} \citep{bbs_prg, blum_micali_prg}.
From a computational point of view, the TV for the pseudorandom distribution is almost the same as the truly random AI-generated distribution.
Thus, our framework provides a reasonable approximation for real-world LLMs, and the hardness result holds even in the presence of pseudorandomness.

\textbf{Computational Total Variation Distance.} Just as the total variation distance $\tv$ between two probability distributions is defined as the difference in probabilities assigned by the two distributions to any $\{0, 1\}$-function, we define a computational version of this distance $\tv_c$ for polynomial-time computable functions:
\[\tv_c(A, B) = \max_{f \in \mathcal{P}} \big|\mathbb{P}_{s \sim A}[f(s) = 1] - \mathbb{P}_{s \sim B}[f(s) = 1]\big|,\]
where $\mathcal{P}$ represents the set of polynomial-time computable $\{0, 1\}$-functions.
$\mathcal{P}$ could also be defined as the set of all polynomial-size circuits which could be more appropriate for deep neural network-based detectors.
The function $f$ could be thought of as the indicator function for the detection parameter being above a certain threshold, i.e., $D(s) \geq \gamma$ as in the proof of Theorem~\ref{thm:ROC_bound}.
The following lemma holds for the performance of a polynomial-time detector $D$:
\begin{lemma}
\label{lem:pseudo_bnd}
The area under the ROC of any polynomial-time computable detector $D$ is bounded as
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \tv_c(\widehat{\mathcal{M}}, \mathcal{H}) - \frac{\tv_c(\widehat{\mathcal{M}}, \mathcal{H})^2}{2}.\]
\end{lemma}
This lemma can be proved in the same way as Theorem~\ref{thm:ROC_bound} by replacing the truly random AI-generated distribution $\mathcal{M}$ with its pseudorandom version $\widehat{\mathcal{M}}$ and the true total variation $\tv$ with its computaional variant $\tv_c$.

Next, we relate the computational total variation $\tv_c$ between $\mathcal{H}$ and the pseudorandom distribution $\widehat{\mathcal{M}}$ with the total variation $\tv$ between $\mathcal{H}$ and the truly random distribution $\mathcal{M}$.
\begin{lemma}
\label{lem:tv_relation}
For human distribution $\mathcal{H}$, truly random AI-generated distribution $\mathcal{M}$ and its pseudorandom version $\widehat{\mathcal{M}}$,
    \[\tv_c(\widehat{\mathcal{M}}, \mathcal{H}) \leq \tv(\mathcal{M}, \mathcal{H}) + \epsilon.\]
\end{lemma}
\begin{proof}
    \begin{align*}
    \tv_c(\widehat{\mathcal{M}}, \mathcal{H}) &= \max_{f \in \mathcal{P}} \big|\mathbb{P}_{s \sim \mathcal{H}}[f(s) = 1] - \mathbb{P}_{s \sim \widehat{\mathcal{M}}}[f(s) = 1]\big| \tag{from definition of $\tv_c$}\\
    &= \max_{f \in \mathcal{P}} \big|\mathbb{P}_{s \sim \mathcal{H}}[f(s) = 1] - \mathbb{P}_{s \sim \mathcal{M}}[f(s) = 1]\\
    &\quad \quad \quad + \mathbb{P}_{s \sim \mathcal{M}}[f(s) = 1] - \mathbb{P}_{s \sim \widehat{\mathcal{M}}}[f(s) = 1]\big| \tag{+/-ing $\mathbb{P}_{s \sim \mathcal{M}}[f(s) = 1]$}\\
    &\leq \max_{f \in \mathcal{P}} \big|\mathbb{P}_{s \sim \mathcal{H}}[f(s) = 1] - \mathbb{P}_{s \sim \mathcal{M}}[f(s) = 1]\big| \\
    &\quad \quad \quad + \big|\mathbb{P}_{s \sim \mathcal{M}}[f(s) = 1] - \mathbb{P}_{s \sim \widehat{\mathcal{M}}}[f(s) = 1]\big| \tag{using $|a + b| \leq |a| + |b|$}\\
    &\leq \tv(\mathcal{M}, \mathcal{H}) + \epsilon. \tag{from definition of $\tv$ and bound~\ref{eq:eps_adv}}
    \end{align*}
\end{proof}

We now use this to prove the modified version of our computational hardness result.

\begin{theorem}[\textbf{Computational Hardness Result}] \label{thm:ROC_bound_computational}
The AUROC of any polynomial-time computable detector $D$ for $\mathcal{H}$ and the pseudorandom distribution $\widehat{\mathcal{M}}$ is bounded using the $\tv$ for the truly random distribution $\mathcal{M}$ as
    \[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2} + \epsilon.\]
\end{theorem}
\begin{proof}
    \begin{align*}
        \mathsf{AUROC}(D) &\leq \frac{1}{2} + \tv_c(\widehat{\mathcal{M}}, \mathcal{H}) - \frac{\tv_c(\widehat{\mathcal{M}}, \mathcal{H})^2}{2} \tag{from Lemma~\ref{lem:pseudo_bnd}}\\
        &\leq \frac{1}{2} + \tv(\mathcal{M}, \mathcal{H}) + \epsilon - \frac{\left(\tv(\mathcal{M}, \mathcal{H}\right) + \epsilon)^2}{2} \tag{from Lemma~\ref{lem:tv_relation} and since $\frac{1}{2} + x -\frac{x^2}{2}$ is increasing in $[0,1]$}\\
        &= \frac{1}{2} + \tv(\mathcal{M}, \mathcal{H}) + \epsilon - \frac{\tv(\mathcal{M}, \mathcal{H})^2 + \epsilon^2 + 2\epsilon\tv(\mathcal{M}, \mathcal{H})}{2}\\
        &\leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2} + \epsilon. \tag{dropping negative terms containing $\epsilon$}
    \end{align*}
\end{proof}

\subsection{Estimating TV Distance}
\label{app:estimating_tv}

In \S\ref{sec:impossibilityresult}, we show experiments supporting the assumption that more advanced LLMs lead to smaller TV distance between human and machine text distributions. We present two experimental settings --- (i) Using synthetic text data and (ii) Using projection. In Figure~\ref{fig:tv_estimate_4i}, we show the TV distances computed with varying vocabulary sizes and sequence lengths. In all the experiments, we consistently find that the TV distances reduce as the network size increases.


\begin{figure}[t]
\centering
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, left}{images/rebuttal_lstm_vocab.png}
  \vspace{-2mm}
  \caption{Varying vocabulary size}
  \label{fig:tv_estimate_4i_vocab}
\end{subfigure}% 
\hfill
\begin{subfigure}{.495\textwidth}
  \centering
  \adjustimage{width=1\linewidth, right}{images/rebuttal_lstm_seqlen.png}
  \vspace{-2mm}
  \caption{Varying sequence length}
  \label{fig:tv_estimate_4i_seqlen}
\end{subfigure}
% \vspace{-2mm}
\caption{TV distances between synthetic toy data distributions and LSTM model generation distributions. TV distances are computed for multiple settings, varying the vocabulary size and sequence length of the training dataset and varying the size of the LSTM network used for training.}
\label{fig:tv_estimate_4i}
\vspace{-0mm}
\end{figure}


\section{More Details on Spoofing}
\label{app:spoofing}

\subsection{Soft Watermark Detector}

    

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/the.png}
    % \vspace{-6mm}
    \caption{Inferred {\it green list score} for the token ``the''. The plot shows the top 50 words from our set of common words that are likely to be in the green list. The word ``first'' occurred $\sim 25\%$ of the time as suffix to ``the''.}
    \vspace{-3.3mm}
    \label{fig:the}
\end{figure}

In \citet{kirchenbauer2023watermark}, they
watermark LLM outputs by asserting the model to output tokens with some specific pattern that can be easily detected with meager error rates. Soft watermarked texts are majorly composed of {\it green list} tokens. If an adversary can learn the green lists for the soft watermarking scheme, they can use this information to generate human-written texts that are detected to be watermarked. Our experiments show that the soft watermarking scheme can be spoofed efficiently. Though the soft watermarking detector can detect the presence of a watermark very accurately, it cannot be certain if this pattern is actually generated by a human or an LLM.  An {\it adversarial human} can compose derogatory watermarked texts in this fashion that are detected to be watermarked, which might cause reputational damage to the developers of the watermarked LLM. Therefore, it is important to study {\it spoofing attacks} to avoid such scenarios.


In watermarking, the prefix word $s^{(t-1)}$ determines the green list for selecting the word $s^{(t)}$. The attacker's objective is to compute a proxy of green lists for the $N$ most commonly used words in the vocabulary.
We use a small value of $N=181$ for our experiments. 
The attacker queries the watermarked OPT-1.3B \cite{opt} $10^6$ times to observe pair-wise token occurrences in its output to estimate {\it green list score} for the $N$ tokens.  We find that inputting nonsense sentences composed of the $N$ common words encourages the LLM to output text mostly composed of these words. This makes the querying more efficient. A token with a high green list score for a prefix $s^{(t)}$ might be in its green list (see Figure \ref{fig:the}). We build a tool that helps adversarial humans create watermarked sentences by providing them with proxy green list. In this manner, we can spoof watermarking models easily. See Table \ref{tab:adversarialhuman} for example sentences created by an adversarial human.
Figure \ref{fig:watermark-roc-spoof} shows that the performance of watermark-based detectors degrades significantly in the presence of paraphrasing and spoofing attacks, showing that they are not reliable. 


\begin{table}[h]
\small
    \centering
    \begin{tabular}{M{7cm} | P{1.6cm} | P{1cm} | P{2.0cm} }
    \toprule
        \multicolumn{1}{P{7cm}|}{Human text} & $\%$ tokens in green list & z-score & Detector output \\ \midrule \midrule
        the first thing you do will be the best thing you do. this is the reason why you do the first thing very well. if most of us did the first thing so well this world would be a lot better place. and it is a very well known fact. people from every place know this fact. time will prove this point to the all of us. as you get more money you will also get this fact like other people do. all of us should do the first thing very well. hence the first thing you do will be the best thing you do. & 42.6 & 4.36 & Watermarked\\ 
        \bottomrule 
    \end{tabular}
    \vspace{0.2cm}
    \caption{Proof-of-concept human-generated texts flagged as watermarked by the soft watermarking scheme. A sentence composed by an {\it adversarial human} contains $42.6\%$ tokens from the green list. The z-test threshold for watermark detection is 4, the same as the default hyperparameter in \cite{kirchenbauer2023watermark}.}
    \label{tab:adversarialhuman}
\end{table}

\subsection{Zero-Shot and Trained Detectors}

We report the false positive rate fixed at a true positive rate of $90\%$ and the true positive rate at a false positive rate of $1\%$ in Table~\ref{tab:spoof_fpr_tpr}. The ROC curves before and after spoofing the detectors are provided in Figure~\ref{fig:spoof_roc}.
Our experiments demonstrate that most of these detection methods show a significant increase in false positive rates at a fixed true positive rate of $90\%$ after spoofing.
After this na\"ive spoofing attack, the true positive rate at a false positive rate of $1\%$ and AUROC scores of these detectors drop significantly. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.49\textwidth]{images/before-spoof-semilogx_roc_curves.png}
    \includegraphics[width=0.49\textwidth]{images/after-spoof-semilogx_roc_curves.png}
    \caption{ROC curves before (left) and after (right) spoofing attack (\S~\ref{sec:humantextdetected}). Most detectors exhibit quality degradation after our spoofing attack.}
    \label{fig:spoof_roc}
\end{figure}

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{c| c | c}
    \toprule
    Detection Methods  & T@F & F@T \\          \midrule \midrule
Entropy threshold \citep{gehrmann2019gltr} & \textbf{0.025} (0.045) & \textbf{0.995} (0.845)\\
Likelihood threshold \citep{solaiman2019release} & \textbf{0.050} (0.075) & \textbf{0.995} (0.310)\\
Logrank threshold & 0.165 (0.155)  & \textbf{0.690} (0.190)\\
Rank threshold \citep{gehrmann2019gltr} & 0.530 (0.335) & \textbf{0.655} (0.590)\\
Roberta (base) OpenAI detector \citep{openaidetectgpt2} & 0.900 (0.765) & 0.010 (0.035)\\
Roberta (large) OpenAI detector \citep{openaidetectgpt2} & \textbf{0.985} (0.990) & 0.000 (0.000) \\
% Perturbation (d) & 0.115 & 0.240 \\
DetectGPT \citep{mitchell2023detectgpt} & \textbf{0.055} (0.240) & \textbf{0.555} (0.145)\\
\bottomrule
    \end{tabular}
    \vspace{0.2cm}
    \caption{True positive rates at $1\%$ false positive rate (T@F)  and  false positive rates at $90\%$ true positive rate (F@T) after (before the attack in parentheses) the spoofing attack described in \S \ref{sec:humantextdetected}. Bolded numbers show successful attacks where T@F decreases, or F@T increases after spoofing.}
    \label{tab:spoof_fpr_tpr}
\end{table}