\section{Introduction}

Artificial Intelligence (AI) has made tremendous advances in recent years, from generative models in computer vision \citep{stablediff, imagen} to generative models in natural language processing (NLP) \citep{gpt3, opt, t5}. Large Language Models (LLMs) can now generate texts of supreme quality with the potential in many applications. For example, the recent model of ChatGPT \citep{chatgpt} can generate human-like texts for various tasks such as writing codes for computer programs, lyrics for songs, completing documents, and question answering; its applications are endless. The trend in NLP shows that these LLMs will even get better with time. However, this comes with a significant challenge in terms of authenticity and regulations. AI tools have the potential to be misused by users for unethical purposes such as plagiarism, generating fake news, spamming, generating fake product reviews, and manipulating web content for social engineering in ways that can have negative impacts on society \citep{adelani2020generating, weiss2019deepfake}. Some news articles rewritten by AI have led to many fundamental errors in them \citep{cnet}. Hence, there is a need to ensure the responsible use of these generative AI tools. In order to aid this, a lot of recent research focuses on detecting AI-generated texts. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/title-wm.png}
    % \vspace{-2mm}
    \caption{An illustration of vulnerabilities of existing AI-text detectors. We consider both watermarking-based and non-watermarking-based detectors and show that they are not reliable in practical scenarios. Colored arrow paths show the potential pipelines for adversaries to avoid detection. In \textcolor{red}{red}: an attacker can use a paraphraser to remove the LLM signatures from an AI-generated text to avoid detection. In \textcolor{blue}{blue}: an adversary can query the watermarked LLM multiple times to learn its watermarking scheme. This information can be used to spoof the watermark detector.   
    }  
    \label{fig:title}
\end{figure}  

Several detection works study this problem as a binary classification problem \citep{openaidetectgpt2, jawahar2020automatic, mitchell2023detectgpt, bakhtin2019real, fagni2020tweepfake} and use \textbf{neural network-based detectors}. For example, OpenAI fine-tunes RoBERTa-based \citep{liu2019roberta} GPT-2 detector models to distinguish between non-AI generated and GPT-2 generated texts \citep{openaidetectgpt2}. This requires such a detector to be fine-tuned with supervision on each new LLM for reliable detection. Another stream of work focuses on \textbf{zero-shot AI text detection} without any additional training overhead \citep{solaiman2019release, ippolito2019automatic, gehrmann2019gltr}. These works evaluate the expected per-token log probability of texts and perform thresholding to detect AI-generated texts. \citet{mitchell2023detectgpt} observe that AI-generated passages tend to lie in negative curvature of log probability of texts. They propose DetectGPT, a zero-shot LLM text detection method, to leverage this observation. Since these approaches rely on a neural network for their detection, they can be vulnerable to adversarial and poisoning attacks \citep{goodfellow2014explaining, sadasivan2023cuda, kumar2022certifying, wang2022improved}. Another line of work aims to \textbf{watermark AI-generated texts} to ease their detection \citep{atallah2001natural, wilson2014linguistic, kirchenbauer2023watermark, zhao2023protecting}. Watermarking eases the detection of LLM output text by imprinting specific patterns on them. Soft watermarking proposed in \citet{kirchenbauer2023watermark} partitions tokens into ``green'' and ``red'' lists, as they define, to help create these patterns. A watermarked LLM samples a token, with high probability, from the green list determined by a pseudo-random generator seeded by its prefix token. The watermarking detector would classify a passage with a large number of tokens from the green list as AI-generated. These watermarks are often imperceptible to humans. \citet{krishna2023paraphrasing} introduces an \textbf{information retrieval-based detector} by storing the outputs of the LLM in a database. For a candidate passage, their algorithm searches this database for semantically similar matches for detection. However, storing user-LLM conversations might cause serious privacy concerns.




In this paper, through several experiments, we show that these state-of-the-art AI-text detectors are unreliable in practical scenarios \citep{wolff, scottaaronson, liang2023gpt, deepfaketext, m4}. In \S \ref{sec:aigentextnotdetected}, we have developed a {\it recursive paraphrasing attack} that use neural network-based paraphrasing to recursively paraphrase the source LLM's output text. Our experiments show that this automated paraphrasing attack can drastically reduce the accuracy of various detectors, including those using soft watermarking \citep{kirchenbauer2023watermark}, to increase \textit{type-II error} (detecting AI text as human text). For instance, {\bf our recursive paraphrasing attack on watermarked texts, even over relatively long passages of 300 tokens in length, can drop the detection rate (true positive rate at \bm{$1\%$} false positive rate or TPR@\bm{$1\%$}FPR) from $\bm{99.3\%}$ to $\bm{9.7\%}$ with only degradation of $\bm{2.2}$ in perplexity score.} We note that \cite{kirchenbauer2023watermark} considers a relatively weak paraphrasing attack in their experiments where they perform span replacement by replacing random tokens (in-place) using an LLM. Our experiments, however, show the vulnerability of the watermarking scheme against stronger paraphrasing attacks that we use.


After paraphrasing, the area under the receiver operating characteristic (AUROC) curves of zero-shot detectors \citep{mitchell2023detectgpt} drops from $96.5\%$ to $25.2\%$. We also observe that the performance of neural network-based trained detectors \citep{openaidetectgpt2} deteriorates significantly after our paraphrasing attack. For instance, the TPR@$1\%$FPR of the RoBERTa-Large-Detector from OpenAI drops from $100\%$ to $60\%$ after paraphrasing. In addition, we show that the retrieval-based detector by \citet{krishna2023paraphrasing} designed to evade paraphrase attacks is vulnerable to our recursive paraphrasing. In fact, the accuracy of their detector falls from $100\%$ to below $60\%$ with our recursive paraphrase attack. 


We also observe that the quality of the paraphrased passages degrades, but only slightly, compared to the original ones. We quantify this via MTurk human evaluation studies and metrics such as perplexity and text benchmark accuracy. In particular, \textbf{our human evaluation study shows that $77\%$ of the recursively paraphrased passages are rated high quality in terms of content preservation, and $89\%$ of them are rated high quality in terms of grammar or text quality.} 
We also show that our recursive paraphrasing, when applied to a text benchmark such as a question-answering dataset, does not affect the performance, providing additional evidence that recursive paraphrasing does not hurt the content of the original text.





Moreover, we show the possibility of {\bf spoofing attacks} on various AI text detectors in  \S \ref{sec:humantextdetected}. In this setting, an attacker generates a non-AI text that is detected to be AI-generated, thus increasing \textit{type-I error} (falsely detecting human text as AI text). An adversary can potentially launch spoofing attacks to produce derogatory texts that are detected to be AI-generated to affect the reputation of the target LLM's developers. In particular, we show that an adversary can infer hidden AI text signatures without having white-box access to the detection method. For example, though the pseudo-random generator used for generating watermarked text is private, we develop an attack that adaptively queries the target LLM multiple times to learn its watermarking scheme. An {\it adversarial human} can then use this information to compose texts that are detected to be watermarked. Figure \ref{fig:title} shows an illustration of some of the vulnerabilities of the existing AI-text detectors.\looseness=-1

Finally, in \S \ref{sec:impossibilityresult}, we present a theoretical result regarding the hardness of AI-text detection. Our main result in Theorem \ref{thm:ROC_bound} states that the AUROC of the best possible detector differentiating two distributions $\mathcal{H}$ (e.g., human text) and $\mathcal{M}$ (e.g., AI-generated text) reduces as the total variation distance $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ between them decreases. Note that this result is true for any two arbitrary distributions $\mathcal{H}$ and $\mathcal{M}$. For example, $\mathcal{H}$ could be the text distribution for a person or group and $\mathcal{M}$ could be the output text distribution of a general LLM or an LLM trained by an adversary to mimic the text of a particular set of people. Essentially, adversaries can train LLMs to mimic human text as they get more sophisticated, potentially reducing the TV distance between human and AI text, leading to an increasingly more difficult detection problem according to our Theorem \ref{thm:ROC_bound}. Although estimating the exact TV between text distributions from a finite set of samples is a challenging problem, we provide some empirical evidence, over simulated data or via TV estimations, showing that more advanced LLMs can potentially lead to smaller TV distances. Thus, \textbf{our Theorem \ref{thm:ROC_bound} would indicate an increasingly more difficult reliable detection problem} in such cases. 
Our theory also indicates that if a detector becomes more robust to type-I errors, type-II errors will increase, revealing a fundamental tradeoff between type-I and type-II errors for the AI text detection problem.
Similar tradeoffs have been explored in other domains as well.
For example, \citet{KhajaviK16} study the relationship between detection performance and KL divergence between input distributions in the context of covariance selection.
\citet{thapliyal2022datadriven} show that undetectable cyberattacks can be generated by mimicking the input-output data distribution of network control systems.
Although not a surprising result, Theorem~\ref{thm:ROC_bound} is the first to link this tradeoff to the detection of AI-generated content to our knowledge. 

Identifying AI-generated text is a critical problem to avoid its misuse by users for unethical purposes such as plagiarism, generating fake news, and spamming. However, deploying vulnerable detectors may {\it not} be the right solution to tackle this issue since it can cause its own damages, such as falsely accusing a human of plagiarism. Our results highlight the sensitivities of a wide range of detectors to both evasion and spoofing attacks and indicate the difficulty of developing reliable detectors in practical scenarios --- to maintain reliable detection performance, LLMs would have to trade off their performance. We hope that these findings can help the ethical and dependable utilization of AI-generated text.

In summary, we make the following contributions in this work.
\begin{itemize}
    \item Our work is the {\it first to comprehensively analyze} the performance of four different classes of detectors, including watermarking-based, neural network-based, zero-shot, and retrieval-based detectors, and reveal their reliability issues (in \S\ref{sec:aigentextnotdetected}). In particular, the {\it recursive paraphrasing attack} that we develop is the first method that can break watermarking \citep{kirchenbauer2023watermark} and retrieval-based \citep{krishna2023paraphrasing} detectors with only a small degradation in text quality. 
    
    \item Our work is the first to show that existing detectors are vulnerable against {\it spoofing attacks} where an adversarial human aims to write a (potentially derogatory) passage falsely detected as AI-generated {\it without} having a white-box access to the detection methods (in \S\ref{sec:humantextdetected}). For instance, as proof of concept, we show that an adversary can infer the watermarking signatures by probing the watermarked LLM and analyzing the statistics of the generated tokens.     
    
    \item Our work is the first to establish a theoretical connection between the AUROC of the best possible detector and the TV distance between human and AI-text distributions that can be used to study the hardness of the reliable text detection problem (in \S\ref{sec:impossibilityresult}).  Our theory also reveals a fundamental tradeoff between type-I and type-II errors for the AI text detection problem.
\end{itemize}