\section{Conclusion}


In this paper, we analyze the performance of four different classes of detectors including watermarking-based, neural-net based, zero-shot based and retrieval-based detectors and show their reliability issues. In particular, we develop a strong attack called {\it recursive paraphrasing} that can break recently proposed watermarking and retrieval-based detectors. Using perplexity score computation as well as conducting various MTurk human study, we observe that our recursive paraphrasing only degrades text quality slightly. We also show that adversaries can spoof these detectors to increase their type-I errors. Spoofing attacks can lead to the generation of derogatory passages detected as AI-generated that might affect the reputation of the LLM detector developers. Finally, we establish a theoretical connection between the AUROC of the best possible detector to the TV distance between human and AI-text distributions that can be used to study the fundamental hardness of the reliable detection problem for more advanced LLMs.

A detector should ideally be helpful in reliably flagging AI-generated texts to prevent the misuse of LLMs. However, the cost of misidentification by a detector can be huge. If the false positive rate of the detector is not low enough, humans (e.g., students) could be falsely accused of AI plagiarism. Moreover, a disparaging passage falsely detected to be AI-generated could affect the reputation of the LLM's developers. As a result, the practical applications of AI-text detectors can become unreliable and invalid. Security methods need not be foolproof. However, we need to make sure that it is not an easy task for an attacker to break these security defenses. Thus, analyzing the risks of using current and future detectors can be vital to avoid creating a false sense of security. 




