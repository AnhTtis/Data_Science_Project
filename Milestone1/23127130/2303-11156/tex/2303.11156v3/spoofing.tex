\section{Spoofing Attacks on Generative AI-text Models}
\label{sec:humantextdetected}


An AI language detector without a low type-I error can cause harm as it might wrongly accuse a human of plagiarizing using an LLM. Moreover, an attacker ({\it adversarial human}) can generate a non-AI text to be detected as AI-generated. This is called the {\it spoofing attack}. An adversary can potentially launch spoofing attacks to produce derogatory texts to damage the reputation of the target LLM's developers. In this section, as a proof-of-concept, we show that current text detectors can be spoofed to detect texts composed by adversarial humans as AI-generated. More details on the spoofing experiments are presented in Appendix \ref{app:spoofing}.

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \vspace{-6mm}
    % \hspace{5mm}
   \includegraphics[width=1\linewidth,]{images/wm_spoof.png}
   \vspace{-8mm}
    \caption{ROC curve of a soft watermarking-based detector \citep{kirchenbauer2023watermark} after our spoofing attack.}
    \label{fig:watermark-roc-spoof}
    \vspace{-4mm}
\end{wrapfigure}
\textbf{Soft watermarking.} As discussed in \S \ref{sec:aigentextnotdetected}, soft watermarked LLMs \citep{kirchenbauer2023watermark} generate tokens from the ``green list'' that are determined by a pseudo-random generator seeded by the prefix token. Though the pseudo-random generator is private, an attacker can estimate the green lists by observing multiple token pairs in the watermarked texts from the target LLM. An adversarial human can then leverage the estimated green lists to compose texts by themselves that are detected to be watermarked. In our experiments, we estimate the green lists for 181 most commonly used words in the English vocabulary. We query the target watermarked OPT-1.3B model one million times to observe the token pair distributions within this smaller vocabulary subset we select. 
Note that this attack on a watermarked model only needs to be performed once to learn the watermarking pattern to spoof it thereafter. 
Based on the frequency of tokens that follow a prefix token in the observed generative outputs, we estimate green lists for each of the 181 common words. We build a tool that helps adversarial humans create watermarked
sentences by providing them with the proxy green list we learn.
We observe that the \textbf{soft watermarking scheme can be spoofed to degrade its detection AUROC from \bm{$99.8\%$} to \bm{$1.3\%$}} (see Figure~\ref{fig:watermark-roc-spoof}).



\textbf{Retrieval-based detectors.} \cite{krishna2023paraphrasing} use a database to store LLM outputs to detect AI-text by retrieval. We find in our experiments (see Figure~\ref{fig:irapp}) that an \textbf{adversary can spoof this detector \bm{$100\%$} of the time, even if the detector maintains a private database}. Suppose an adversary, say a teacher, has access to a human written document $S$, say a student's essay. The adversary can prompt the target LLM to paraphrase $S$ to get $S'$. This results in the LLM, by design, storing its output $S'$ in its private database for detection purposes. Now, the detector would classify the original human text $S$ as AI-generated since a semantically similar copy $S'$ is present in its database. In this manner, a teacher can purposefully allege an innocent student to have plagiarised using the retrieval-based detector. 
Note that manipulating retrieval-based detectors is easier using this approach compared to watermarking techniques. 
This observation implies a practical tradeoff between type-I and type-II errors. 
When a detector is strengthened against type-II errors, it tends to result in a deterioration of its performance in terms of type-I errors.



\textbf{Zero-shot and neural network-based detectors.} In this setting, a malicious adversary could write a short text in a collaborative work, which may lead to the entire text being classified as AI-generated. To simulate this, we prepend a human-written
text marked as AI-generated by the detector to all the other human-generated text for spoofing. In other words, from 200 long passages in the XSum dataset, we pick the human text with the worst detection score for each detector considered in \S \ref{sec:paraphrase_nonwatermark}. We then prepend this text to all the other human
texts, ensuring that the length of the prepended text does not exceed the length of the original text. Our experiments show that the \textbf{AUROC of all these detectors drops after spoofing} (see plots in Appendix \ref{app:spoofing}). After this
na\"ive spoofing attack, the TPR@$1\%$FPR of most of these detectors drop significantly.