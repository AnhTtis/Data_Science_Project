\section{Impossibility Results for Reliable Detection of AI-Generated Text}
\label{sec:impossibilityresult}
Detecting the misuse of language models in the real world, such as plagiarism and mass propaganda, necessitates the identification of text produced by all kinds of language models, including those without watermarks.
However, as these models improve over time, so does our ability to emulate human text with AI-generated text.
The problem of AI-text detection becomes more important and interesting in the presence of language models designed to mimic humans and evade detection.
The generated text looks increasingly similar to human text and the statistical difference between the corresponding distributions decreases, which complicates the detection process.
More formally, the total variation distance between the distributions of AI-generated and human-generated text sequences diminishes as language models become more sophisticated (see \S~\ref{sec:tv_estimation}).



This section presents a fundamental constraint on general AI-text detection, demonstrating that the performance of even the best possible detector decreases as models get bigger and more powerful.
%For a sufficiently advanced language model, a detector can only perform marginally better than a random classifier.
Even with a moderate overlap between the two distributions, detection performance may not be sufficiently good for real-world deployment and may result in a high false positive rate.
%For instance, a plagiarism detector may be considered useful if it can achieve a high true positive rate (assuming AI-text as positive), say 90\%, and a low false positive rate, say 1\%.
%This is impossible to achieve when the two distributions overlap more than 11\% (i.e., total variation  < 0.89).
The purpose of this analysis is to caution against relying too heavily on detectors that claim to identify AI-generated text.
Detectors should be independently and rigorously evaluated for reliability, ideally on language models designed to evade detection, before deployment in the real world.
%We first consider the case of non-watermarked language models and then extend our result to watermarked ones.\looseness=-1


\begin{wrapfigure}{r}{0.55\textwidth}
    % \centering
    \vspace{-6mm}
    \hspace{-3mm}
    \includegraphics[width=1.1\linewidth, trim={0 0 0 6mm},clip]{images/roc_bound.png}
    \vspace{-4mm}
    \caption{Comparing the performance, in terms of area under the ROC curve, of the best-possible detector to that of the baseline performance corresponding to a random classifier.}
    \label{fig:roc_bound}
    \vspace{-4mm}
\end{wrapfigure}
In the following theorem, we formalize the above statement by showing an upper bound on the area under the ROC curve of an arbitrary detector in terms of the total variation distance between the distributions for AI and human-generated text.
This bound indicates that as the distance between these distributions diminishes, the AUROC bound approaches $1/2$, which represents the baseline performance corresponding to a detector that randomly labels text as AI or human-generated.
We define $\mathcal{M}$ and $\mathcal{H}$ as the text distributions produced by AI and humans, respectively, over the set of all possible text sequences $\Omega$.
We use $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ to denote the total variation distance between these two distributions and model a detector as a function $D: \Omega \rightarrow \mathbb{R}$ that maps every sequence in $\Omega$ to a real number.
% detection parameter $\gamma$.
Sequences are classified into AI and human-generated by applying a threshold $\gamma$ on this number.
By adjusting the parameter $\gamma$, we can tune the sensitivity of the detector to AI and human-generated texts to obtain a ROC curve.
\vspace{0.5cm}

\begin{theorem}
\label{thm:ROC_bound}
The area under the ROC of any detector $D$ is bounded as
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2}.\]
\end{theorem}

\begin{proof}
The ROC is a plot between the true positive rate (TPR) and the false positive rate (FPR) which are defined as follows:
\begin{align*}
    \mathsf{TPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{M}}[D(s) \geq \gamma]\\
    \text{and } \mathsf{FPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{H}}[D(s) \geq \gamma],
\end{align*}
where $\gamma$ is some classifier parameter.
We can bound the difference between the $\mathsf{TPR}_\gamma$ and the $\mathsf{FPR}_\gamma$ by the total variation between $M$ and $H$:
\begin{align}
    |\mathsf{TPR}_\gamma - \mathsf{FPR}_\gamma| &= \left| \mathbb{P}_{s \sim \mathcal{M}}[D(s) \geq \gamma] - \mathbb{P}_{s \sim \mathcal{H}}[D(s) \geq \gamma] \right| \leq \mathsf{TV}(\mathcal{M}, \mathcal{H})\\
    \mathsf{TPR}_\gamma &\leq \mathsf{FPR}_\gamma + \mathsf{TV}(\mathcal{M}, \mathcal{H}).
    \label{eq:TPR_FPR_bound}
\end{align}
Since the $\mathsf{TPR}_\gamma$ is also bounded by 1 we have:
\begin{align}
\label{eq:tpr_bound}
\mathsf{TPR}_\gamma \leq \min(\mathsf{FPR}_\gamma + \mathsf{TV}(\mathcal{M}, \mathcal{H}), 1).
\end{align}
Denoting $\mathsf{FPR}_\gamma$, $\mathsf{TPR}_\gamma$, and $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ with $x$, $y$, and $tv$ for brevity, we bound the AUROC as follows:
\allowdisplaybreaks
\begin{align*}
    \mathsf{AUROC}(D) = \int_0^1 y \; dx &\leq \int_0^1 \min(x + tv, 1) dx\\
    &= \int_0^{1 -tv} (x + tv) dx + \int_{1-tv}^1 dx\\
    &= \left| \frac{x^2}{2} + tvx \right|_0^{1-tv} + \left| x \right|_{1-tv}^1\\
    &= \frac{(1-tv)^2}{2} + tv(1-tv) + tv\\
    &= \frac{1}{2} + \frac{tv^2}{2} - tv + tv - tv^2 + tv\\
    &= \frac{1}{2} + tv - \frac{tv^2}{2}.
\end{align*}
\end{proof}

%The proof is deferred to Appendix~\ref{sec:proof_roc_bnd}. 
Figure~\ref{fig:roc_bound} shows how the above bound grows as a function of the total variation.
For a detector to have a good performance (say, AUROC $\geq 0.9$), the distributions of human and AI-generated texts must be very different from each other (total variation $> 0.5$).
As the two distributions become similar (say, total variation $\leq 0.2$), the performance of even the best-possible detector is not good (AUROC $< 0.7$).
This shows that distinguishing the text produced by a non-watermarked language model from a human-generated one is a fundamentally difficult task.
Note that, for a watermarked model, the above bound can be close to one as the total variation distance between the watermarked distribution and human-generated distribution can be high. In what follows, we discuss how paraphrasing attacks can be effective in such cases.

%\textbf{Paraphrasing to Evade Detection:}
\subsection{Paraphrasing to Evade Detection}
Although our analysis considers the text generated by all humans and general language models, it can also be applied to specific scenarios, such as particular writing styles or sentence paraphrasing, by defining $\mathcal{M}$ and $\mathcal{H}$ appropriately.
For example, it could be used to show that AI-generated text, even with watermarks, can be made difficult to detect by simply passing it through a paraphrasing tool.
Consider a paraphraser that takes a sequence $s$ generated by an AI model as input and produces a human-like sequence with similar meaning.
Set $\mathcal{M} = \mathcal{R_M}(s)$ and $\mathcal{H} = \mathcal{R_H}(s)$ to be the distribution of sequences with similar meanings to $s$ produced by the paraphraser and humans, respectively.
The goal of the paraphraser is to make its distribution $\mathcal{R_M}(s)$ as similar to the human distribution $\mathcal{R_H}(s)$ as possible, essentially reducing the total variation distance between them.
Theorem~\ref{thm:ROC_bound} puts the following bound on the performance of a detector $D$ that seeks to detect the outputs of the paraphraser from the sequences produced by humans.


\begin{corollary}
The area under the ROC of the detector $D$ is bounded as
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{R_M}(s), \mathcal{R_H}(s)) - \frac{\mathsf{TV}(\mathcal{R_M}(s), \mathcal{R_H}(s))^2}{2}.\]
\end{corollary}


\textbf{General Trade-offs between True Positive and False Positive Rates.} Another way to understand the limitations of AI-generated text detectors is directly through the characterization of the trade-offs between true positive rates and false positive rates. Adapting inequality \ref{eq:TPR_FPR_bound}, we have the following corollaries: 


\begin{corollary}
\label{corollary:rephrasing_wm}
For any watermarking scheme $W$,
\begin{align*}
    \Pr_{s_w\sim \mathcal{R}_{\mathcal{M}}(s)} [\text{$s_w$ is watermarked using $W$}] \leq &
    \Pr_{s_w\sim \mathcal{R}_{\mathcal{H}}(s)}[\text{$s_w$ is watermarked using $W$}] \\
    & + \mathsf{TV}(\mathcal{R_M}(s), \mathcal{R_H}(s)),
\end{align*}
where $\mathcal{R_M}(s)$ and $\mathcal{R_H}(s)$ are the distributions of rephrased sequences for $s$ produced by the paraphrasing model and humans, respectively.
\end{corollary}


Humans may have different writing styles. Corollary \ref{corollary:rephrasing_wm} indicates that if a rephrasing model resembles certain human text distribution $\mathcal{H}$ (i.e. $\mathsf{TV}(\mathcal{R_M}(s), \mathcal{R_H}(s))$ is small), then either certain people's writing will be detected falsely as watermarked (i.e. $\Pr_{s_w\sim \mathcal{R}_{\mathcal{H}}(s)} [\text{$s_w$ is watermarked using $W$}]$ is high) or the paraphrasing model can remove the watermark (i.e. $\Pr_{s_w\sim \mathcal{R}_{\mathcal{M}}(s)} [\text{$s_w$ is watermarked using $W$}]$ is low).


\begin{corollary}
\label{corollary:rephrasing_no_wm}
For any AI-text detector $D$, 
\begin{align*}
    \Pr_{s\sim \mathcal{M}} [\text{$s$ is detected as AI-text by $D$}] \leq \Pr_{s\sim \mathcal{H}}[\text{$s$ is detected as AI-text by $D$}] + \mathsf{TV}(\mathcal{M}, \mathcal{H}),
\end{align*}
where $\mathcal{M}$ and $\mathcal{H}$ denote text distributions by the model and by humans, respectively.
\end{corollary}

Corollary \ref{corollary:rephrasing_no_wm} indicates that if a model resembles certain human text distribution $\mathcal{H}$ (i.e. $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ is small), then either certain people's writing will be detected falsely as AI-generated (i.e. $\Pr_{s\sim \mathcal{H}} [\text{$s$ is detected as AI-text by $D$}]$ is high) or the AI-generated text will not be detected reliably (i.e. $\Pr_{s\sim \mathcal{M}} [\text{$s$ is detected as AI-text by $D$}]$ is low).

\looseness -1
These results demonstrate fundamental limitations for AI-text detectors, with and without watermarking schemes.
In Appendix~\ref{sec:tightness}, we present a tightness analysis of the bound in Theorem~\ref{thm:ROC_bound}, where we show that for any human distribution $\mathcal{H}$ there exists an AI distribution and a detector $D$ for which the bound holds with equality.

\subsection{Tightness Analysis}
\label{sec:tightness}
In this section, we show that the bound in Theorem~\ref{thm:ROC_bound} is tight.
This bound need not be tight for any two distributions $\mathcal{H}$ and $\mathcal{M}$, e.g., two identical normal distributions in one dimension shifted by a distance.
However, tightness can be shown for every human distribution $\mathcal{H}$.
For a given distribution of human-generated text sequences $\mathcal{H}$, we construct an AI-text distribution $\mathcal{M}$ and a detector $D$ such that the bound holds with equality.

Define sublevel sets of the probability density function of the distribution of human-generated text $\mathsf{pdf}_\mathcal{H}$ over the set of all sequences $\Omega$ as follows:
\[\Omega_\mathcal{H}(c) = \{s \in \Omega \mid \mathsf{pdf}_\mathcal{H}(s) \leq c\}\]
where $c \in \mathbb{R}$.
Assume that, $\Omega_\mathcal{H}(0)$ is not empty.
Now, consider a distribution $\mathcal{M}$, with density function $\mathsf{pdf}_\mathcal{M}$, which has the following properties:
\begin{enumerate}
    \item The probability of a sequence drawn from $\mathcal{M}$ falling in $\Omega_\mathcal{H}(0)$ is $\mathsf{TV}(\mathcal{M}, \mathcal{H})$, i.e., $\mathbb{P}_{s \sim \mathcal{M}}[s \in \Omega_\mathcal{H}(0)] = \mathsf{TV}(\mathcal{M}, \mathcal{H})$.
    \item $\mathsf{pdf}_\mathcal{M}(s) = \mathsf{pdf}_\mathcal{H}(s)$ for all $s \in \Omega(\tau) - \Omega(0)$ where $\tau > 0$ such that $\mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega(\tau)] = 1 - \mathsf{TV}(\mathcal{M}, \mathcal{H})$.
    \item $\mathsf{pdf}_\mathcal{M}(s) = 0$ for all $s \in \Omega - \Omega(\tau)$.
\end{enumerate}
Define a hypothetical detector $D$ that maps each sequence in $\Omega$ to the negative of the probability density function of $\mathcal{H}$, i.e., $D(s) = - \mathsf{pdf}_\mathcal{H}(s)$.
Using the definitions of $\mathsf{TPR}_\gamma$ and $\mathsf{FPR}_\gamma$, we have:
\begin{align*}
    \mathsf{TPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{M}}[D(s) \geq \gamma]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[- \mathsf{pdf}_\mathcal{H}(s) \geq \gamma]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[\mathsf{pdf}_\mathcal{H}(s) \leq -\gamma]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma)]
\end{align*}
Similarly,
\[\mathsf{FPR}_\gamma = \mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega_\mathcal{H}(-\gamma)].\]
For $\gamma \in [-\tau, 0]$,
\begin{align*}
    \mathsf{TPR}_\gamma &= \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma)]\\
    &= \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(0)] + \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma) - \Omega_\mathcal{H}(0)]\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathbb{P}_{s \sim \mathcal{M}}[ s \in \Omega_\mathcal{H}(-\gamma) - \Omega_\mathcal{H}(0)] \tag{using property 1}\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega_\mathcal{H}(-\gamma) - \Omega_\mathcal{H}(0)] \tag{using property 2}\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathbb{P}_{s \sim \mathcal{H}}[ s \in \Omega_\mathcal{H}(-\gamma)] - \mathbb{P}_{s \sim \mathcal{H}}[s \in \Omega_\mathcal{H}(0)] \tag{$\Omega_\mathcal{H}(0) \subseteq \Omega_\mathcal{H}(-\gamma)$}\\
    &= \mathsf{TV}(\mathcal{M}, \mathcal{H}) + \mathsf{FPR}_\gamma. \tag{$\mathbb{P}_{s \sim \mathcal{H}}[s \in \Omega_\mathcal{H}(0)] = 0$}
\end{align*}
For $\gamma \in [-\infty, -\tau]$, $\mathsf{TPR}_\gamma = 1$, by property 3.
Also, as $\gamma$ goes from $0$ to $-\infty$, $\mathsf{FPR}_\gamma$ goes from $0$ to $1$.
Therefore, $\mathsf{TPR}_\gamma = \min(\mathsf{FPR}_\gamma + \mathsf{TV}(\mathcal{M}, \mathcal{H}), 1)$ which is similar to Equation~\ref{eq:tpr_bound}.
Calculating the AUROC in a similar fashion as in the previous section, we get the following:
\[\mathsf{AUROC}(D) = \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2}.\]

\subsection{Pseudorandomness in LLMs }
\label{sec:PRG_bound}
Most machine learning models, including large language models (LLMs), use pseudorandom number generators in one form or another to produce their outputs.
For example, an LLM may use a pseudorandom number generator to sample the next token in the output sequence.
In discussing our impossibility result, \citet{kirchenbauer2023reliability} in a more recent work argue that this pseudorandomness makes the AI-generated text distribution very different from the human-generated text distribution.
This is because the pseudorandom AI-generated distribution is a collection of Dirac delta function distributions and a human is exorbitantly unlikely to produce a sample corresponding to any of the delta functions.
In our framework, this means that the total variation between the human and pseudorandom AI-generated distributions is almost one, making the bound in Theorem~\ref{thm:ROC_bound} vacuous.

We argue that, although the true total variation between the human and pseudorandom AI-generated distributions is high and there exists (in theory) a detector function that can separate the distributions almost perfectly, this function may not be efficiently computable.
Any polynomial-time computable detector can only achieve a negligible advantage from the use of pseudorandomness instead of true randomness.
If we had knowledge of the seed used for the pseudorandom number generator, we would be able to predict the pseudorandom samples.
However, an individual seeking to evade detection could simply randomize this seed making it computationally infeasible to predict the samples.
%the pseudorandom samples infeasible to predict.

We modify the bound in Theorem~\ref{thm:ROC_bound} to include a negligible correction term $\epsilon$ to account for the use of pseudorandomness.
We prove that the performance of a polynomial-time computable detector $D$ on a pseudorandom version of the AI-generated distribution $\widehat{\mathcal{M}}$ is bounded by the total variation for the truly random distribution $\mathcal{M}$ (resulting from the LLM using true randomness) as follows:
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2} + \epsilon.\]
The term $\epsilon$ represents the gap between the probabilities assigned by $\mathcal{M}$ and $\widehat{\mathcal{M}}$ to any polynomial-time computable $\{0, 1\}$-function $f$, i.e.,
\begin{equation}
\label{eq:eps_adv}
\big|\mathbb{P}_{s \in \mathcal{M}}[f(s) = 1] - \mathbb{P}_{s \in \widehat{\mathcal{M}}}[f(s) = 1]\big| \leq \epsilon.
\end{equation}
This term is orders of magnitude smaller than any of the terms in the bound and can be safely ignored.
For example, commonly used pseudorandom generators\footnote{Cryptographic PRNGs: \url{https://en.wikipedia.org/wiki/Pseudorandom_number_generator}} can achieve an $\epsilon$ that is bounded by a negligible function $1/b^t$ of the number of bits $b$ used in the seed of the generator for a positive integer $t$\footnote{Negligible function: \url{https://en.wikipedia.org/wiki/Negligible_function}} \citep{bbs_prg, blum_micali_prg}.
From a computational point of view, the total variation for the pseudorandom distribution is almost the same as the truly random AI-generated distribution.
Thus, our framework provides a reasonable approximation for real-world LLMs and the impossibility result holds even in the presence of pseudorandomness.

\textbf{Computational Total Variation Distance:} Just as the total variation distance $\tv$ between two probability distributions is defined as the difference in probabilities assigned by the two distributions to any $\{0, 1\}$-function, we define a computational version of this distance $\tv_c$ for polynomial-time computable functions:
\[\tv_c(A, B) = \max_{f \in \mathcal{P}} \big|\mathbb{P}_{s \sim A}[f(s) = 1] - \mathbb{P}_{s \sim B}[f(s) = 1]\big|,\]
where $\mathcal{P}$ represents the set of polynomial-time computable $\{0, 1\}$-functions.
$\mathcal{P}$ could also be defined as the set of all polynomial-size circuits which could be more appropriate for deep neural network-based detectors.
The function $f$ could be thought of as the indicator function for the detection parameter being above a certain threshold, i.e., $D(s) \geq \gamma$ as in the proof of Theorem~\ref{thm:ROC_bound}.
The following lemma holds for the performance of a polynomial-time detector $D$:
\begin{lemma}
\label{lem:pseudo_bnd}
The area under the ROC of any polynomial-time computable detector $D$ is bounded as
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \tv_c(\widehat{\mathcal{M}}, \mathcal{H}) - \frac{\tv_c(\widehat{\mathcal{M}}, \mathcal{H})^2}{2}.\]
\end{lemma}
This lemma can be proved in the same way as Theorem~\ref{thm:ROC_bound} by replacing the truly random AI-generated distribution $\mathcal{M}$ with its pseudorandom version $\widehat{\mathcal{M}}$ and the true total variation $\tv$ with its computaional variant $\tv_c$.

Next, we relate the computational total variation $\tv_c$ between $\mathcal{H}$ and the pseudorandom distribution $\widehat{\mathcal{M}}$ with the total variation $\tv$ between $\mathcal{H}$ and the truly random distribution $\mathcal{M}$.
\begin{lemma}
\label{lem:tv_relation}
For human distribution $\mathcal{H}$, truly random AI-generated distribution $\mathcal{M}$ and its pseudorandom version $\widehat{\mathcal{M}}$,
    \[\tv_c(\widehat{\mathcal{M}}, \mathcal{H}) \leq \tv(\mathcal{M}, \mathcal{H}) + \epsilon.\]
\end{lemma}
\begin{proof}
    \begin{align*}
    \tv_c(\widehat{\mathcal{M}}, \mathcal{H}) &= \max_{f \in \mathcal{P}} \big|\mathbb{P}_{s \sim \mathcal{H}}[f(s) = 1] - \mathbb{P}_{s \sim \widehat{\mathcal{M}}}[f(s) = 1]\big| \tag{from definition of $\tv_c$}\\
    &= \max_{f \in \mathcal{P}} \big|\mathbb{P}_{s \sim \mathcal{H}}[f(s) = 1] - \mathbb{P}_{s \sim \mathcal{M}}[f(s) = 1]\\
    &\quad \quad \quad + \mathbb{P}_{s \sim \mathcal{M}}[f(s) = 1] - \mathbb{P}_{s \sim \widehat{\mathcal{M}}}[f(s) = 1]\big| \tag{+/-ing $\mathbb{P}_{s \sim \mathcal{M}}[f(s) = 1]$}\\
    &\leq \max_{f \in \mathcal{P}} \big|\mathbb{P}_{s \sim \mathcal{H}}[f(s) = 1] - \mathbb{P}_{s \sim \mathcal{M}}[f(s) = 1]\big| \\
    &\quad \quad \quad + \big|\mathbb{P}_{s \sim \mathcal{M}}[f(s) = 1] - \mathbb{P}_{s \sim \widehat{\mathcal{M}}}[f(s) = 1]\big| \tag{using $|a + b| \leq |a| + |b|$}\\
    &\leq \tv(\mathcal{M}, \mathcal{H}) + \epsilon. \tag{from definition of $\tv$ and bound~\ref{eq:eps_adv}}
    \end{align*}
\end{proof}
This shows that although the true total variation may be high due to pseudorandomness, the effective total variation is still low.
We now use this to prove the modified version of our impossibility result.
\begin{theorem}[\textbf{Computational Impossibility Result}] The AUROC of any polynomial-time computable detector $D$ for $\mathcal{H}$ and the pseudorandom distribution $\widehat{\mathcal{M}}$ is bounded using the $\tv$ for the truly random distribution $\mathcal{M}$ as
    \[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2} + \epsilon.\]
\end{theorem}
\begin{proof}
    \begin{align*}
        \mathsf{AUROC}(D) &\leq \frac{1}{2} + \tv_c(\widehat{\mathcal{M}}, \mathcal{H}) - \frac{\tv_c(\widehat{\mathcal{M}}, \mathcal{H})^2}{2} \tag{from Lemma~\ref{lem:pseudo_bnd}}\\
        &\leq \frac{1}{2} + \tv(\mathcal{M}, \mathcal{H}) + \epsilon - \frac{\left(\tv(\mathcal{M}, \mathcal{H}\right) + \epsilon)^2}{2} \tag{from Lemma~\ref{lem:tv_relation} and since $\frac{1}{2} + x -\frac{x^2}{2}$ is increasing in $[0,1]$}\\
        &= \frac{1}{2} + \tv(\mathcal{M}, \mathcal{H}) + \epsilon - \frac{\tv(\mathcal{M}, \mathcal{H})^2 + \epsilon^2 + 2\epsilon\tv(\mathcal{M}, \mathcal{H})}{2}\\
        &\leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2} + \epsilon. \tag{dropping negative terms containing $\epsilon$}
    \end{align*}
\end{proof}

\section{Estimating Total Variation between Human and AI Text Distributions}
\label{sec:tv_estimation}
We estimate the total variation (TV) between human text distribution (WebText) and the output distribution of several models from OpenAI's GPT-2 series.\footnote{\url{https://github.com/openai/gpt-2-output-dataset}}
For two distributions $\mathcal{H}$ and $\mathcal{M}$, the total variation between them is defined as the maximum difference between the probabilities assigned by them to any event $E$ over the sample space $\Omega$, i.e.,
\[\mathsf{TV}(\mathcal{H}, \mathcal{M}) = \max_E \big|\mathbb{P}_{s \sim \mathcal{H}}[s \in E] - \mathbb{P}_{s \sim \mathcal{M}}[s \in E]\big|.\]
Thus, for any event $E$, this difference in probabilities is a valid {\it lower bound} on the total variation between the two distributions.
Since we do not know the probability density functions for $\mathcal{H}$ and $\mathcal{M}$, solving the above maximization problem over the entire event space is intractable.
Thus, we approximate the event space as a class of  events defined by a neural network with parameters $\theta$ that maps a text sequence in $\Omega$ to a real number.
The corresponding event $E_\theta$ is said to occur when the output of the neural network is above a threshold $\tau$.
We seek to find an event $E_\theta$ which obtains as tight a lower bound on the total variation as possible.

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \vspace{-6mm}
    %\hspace{-2mm}
    \includegraphics[width=1.1\linewidth, trim={0 0 0 6mm},clip]{images/tv_roberta-large_small-117M.png}
    %\vspace{-4mm}
    \caption{TV between WebText and outputs of GPT-2 models (small, medium, large, and XL) for varying sequence lengths.}
    \label{fig:gpt2-wt}
    \vspace{-6mm}
\end{wrapfigure}


\textbf{Estimation Procedure:} We train a RoBERTa large classifier~\cite{liu2019roberta} on samples from human and AI text distributions.
Given a text sequence, this classifier produces a score between 0 and 1 that represents how likely the model thinks the sequence is AI-generated.
Assuming the AI text distribution as the positive class, we pick a threshold for this score that maximizes the difference between the true positive rate (TPR) and the false positive rate (FPR) using samples from a validation set.
Finally, we estimate the total variation as the difference between the TPR and the FPR on a test set.
This difference is essentially the gap between the probabilities assigned by the human and AI-generated text distributions to the above classifier for the computed threshold, which is a lower bound on the total variation. 
% of the output being above the threshold under the human text distribution and the AI text distribution using a test set.
% This difference is essentially the gap between the TPR and the FPR on the test set.

Figure~\ref{fig:gpt2-wt} plots the total variation estimates for four GPT-2 models (small, medium, large, and XL) for four different text sequence lengths (25, 50, 75, and 100) estimated using a RoBERTa-large architecture.
We train a separate instance of this architecture for each GPT-2 model and sequence length to estimate the  total variation for the corresponding distribution.
We observe that, {\it as models become larger and more sophisticated, the TV estimates between human and AI-text distributions decrease.}
This indicates that as language models become more powerful, the statistical difference between their output distribution and human-generated text distribution vanishes.
%their output becomes more similar to human text.
% We include similar results for models in the GPT-3 series using WebText and ArXiv abstracts datasets as human text in the appendix.

\subsection{Estimating Total Variation for GPT-3 Models}
We repeat the above experiments %of Section~\ref{sec:tv_estimation}
with GPT-3 series models, namely Ada, Babbage, and Curie, as documented on the OpenAI platform.\footnote{\url{https://platform.openai.com/docs/models/gpt-3}}
We use WebText and ArXiv abstracts~\cite{clement2019arxiv} datasets as human text distributions.
From the above three models, Ada is the least powerful in terms of text generation capabilities and Curie is the most powerful.
Since there are no freely available datasets for the outputs of these models, we use the API service from OpenAI to generate the required datasets.

We split each human text sequence from WebText into `prompt' and `completion', where the prompt contains the first hundred tokens of the original sequence and the completion contains the rest.
We then use the prompts to generate completions using the GPT-3 models with the temperature set to 0.4 in the OpenAI API.
We use these model completions and the `completion' portion of the human text sequences to estimate total variation using a RoBERTa-large model in the same fashion as Section~\ref{sec:tv_estimation}.
Using the first hundred tokens of the human sequences as prompts allows us to control the context in which the texts are generated.
This allows us to compare the similarity of the generated texts to human texts within the same context.

Figure~\ref{fig:gpt-3-webtext} plots the total variation estimates of the GPT-3 models with respect to WebText for four different sequence lengths 25, 50, 75, and 100 from the model completions.
Similar to the GPT-2 models in \S~\ref{sec:tv_estimation}, we observe that the most powerful model Curie has the least total variation across all sequence lengths.
The model Babbage, however, does not follow this trend and exhibits a higher total variation than even the least powerful model Ada.

Given that WebText contains data from a broad range of Internet sources, we also experiment with more focused scenarios, such as generating content for scientific literature.
We use the ArXiv abstracts dataset as human text and estimate the total variation for the above three models (Figure~\ref{fig:gpt-3-arxiv}).
We observe that, for most sequence lengths, the total variation decreases across the series of models: Ada, Babbage, and Curie.
This provides further evidence that as language models improve in power their outputs become more indistinguishable from human text, making them harder to detect.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=1.1\textwidth]{images/text-ada-001_completion.png}
        \caption{WebText}
        \label{fig:gpt-3-webtext}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=1.1\textwidth]{images/arxiv-gpt3.png}
        \caption{ArXiv}
        \label{fig:gpt-3-arxiv}
    \end{subfigure}
    \caption{Total variation estimates for GPT-3 models with respect to WebText and ArXiv datasets using different sequence lengths from the model completions.}
\end{figure}