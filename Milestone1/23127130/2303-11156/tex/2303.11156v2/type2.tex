\section{Evading AI-Detectors using Paraphrasing Attacks}
\label{sec:aigentextnotdetected}

\begin{table}[t]
\small
    \centering
    \begin{tabular}{c|c|c|c|c} \toprule
    Text & \# tokens & \# green tokens &  Detector accuracy & Perplexity\\ \midrule \midrule
    Watermarked LLM output & 19042 & 11078  & 97$\%$ & 6.7 \\ \midrule
    PEGASUS-based paraphrasing & 16773 & 7412  & 80$\%$ & 10.2 \\ \midrule
    T5-based paraphrasing & 15164 & 6493  & 64$\%$ & 16.7 \\ \midrule
    T5-based paraphrasing & 14913 & 6107  & 57$\%$ & 18.7 \\ 
    \bottomrule
    \end{tabular}
    \vspace{0.25cm}
    \caption{Results of paraphrasing attacks on soft watermarking \citep{kirchenbauer2023watermark}. For testing, we consider 100 text passages from XSum \citep{xsum}. The watermarked output text from the target AI model consists of $\sim 58\%$ green list tokens. The PEGASUS-based \citep{zhang2019pegasus} paraphrased text consists of only $\sim 44\%$ green list tokens. Hence, the detector accuracy drops from $97\%$ to $80\%$, making it unreliable. Note that these PEGASUS-based paraphraser texts only degrade the perplexity measure by 3.5. Even a lighter T5-based paraphraser can affect the detector accuracy  without degrading the text quality significantly.}
    \label{tab:paraphraseresult}
    % \vspace{-0.33cm}
\end{table}


\begin{table}[t]
\small
    \centering
    \begin{tabular}{P{2.5cm} | M{9.5cm}}
    \toprule
         Description & \multicolumn{1}{P{9.5cm}}{Text} \\ \midrule \midrule
        Input prompt (\textbf{misinformation}$^\dagger$) & Drinking bleach is the best way to tackle COVID-19. WHO announced that vaccination for COVID-19 is ineffective. CDC is urging the citizens to drink bleach everyday. The health department has officially confirmed that\\ \midrule
        Detected watermarked text &   the vaccine is useless. The World Health Organization made shocking statement, that the vaccine is ineffective, because it does not prevent people from getting infected, which means it is useless. CDC is also calling for people to drink bleach daily to cleanse the body. [continues...] \\ \midrule 
        Undetected PEGASUS-based paraphrasing & The vaccine is useless. The vaccine is useless because it doesn't prevent people from getting infections, according to the World Health Organization. The CDC wants people to drink bleach to cleanse their body. The vaccine is useless according to WHO. The CDC wants people to drink bleach to cleanse their body. [continues...]\\ \midrule \midrule
        \makecell{Input prompt \\(\textbf{fake news}$^\dagger$)} & Latest news: There is no more daylight savings. The Government has passed a bill to scrape off daylight savings from 2023 onwards, said a US government official.\\ \midrule
        Detected watermarked text & \textbackslash n\textbackslash nThe bill was introduced in the US Senate on Wednesday, and the US House of Representatives got a vote on it on Thursday afternoon. The US President Donald Trump is expected to sign it. [continues...]\\ \midrule
        Undetected PEGASUS-based paraphrasing & The US House of Representatives voted on the bill on Thursday afternoon, after it was introduced in the US Senate on Wednesday. It is expected that Donald Trump will sign it. It will become law if he gets it. [continues...]\\
        \bottomrule 
    \end{tabular}
    \vspace{0.25cm}
    \caption{PEGASUS-based paraphrasing for evading soft watermarking-based detectors. The target AI generator outputs a watermarked text for an input prompt. This output is detected to be generated by the watermarked target LLM. We use a PEGASUS-based \citep{zhang2019pegasus} paraphraser to rephrase this watermarked output from the target LLM. The paraphraser rephrases sentence by sentence. The detector does not detect the output text from the paraphraser. However, the paraphrased passage reads well and means the same as the original watermarked LLM output. At the top rows, we demonstrate how an input prompt can prompt a target LLM to generate {\bf watermarked misinformation.} In the bottom rows, we showcase how an input prompt can induce a target LLM to create {\bf watermarked fake news.} Using paraphrasing attacks in this manner, an attacker can spread fake news or misinformation without getting detected. \\{\footnotesize {\bf  $^\dagger$ contains misinformation only to demonstrate that LLMs can be used for malicious purposes.}}}
    \label{tab:paraphrase}
    % \vspace{-0.33cm}
\end{table}
%\vspace{-1cm}



Detecting AI-generated text is crucial for ensuring the security of an LLM and avoiding type-II errors (not detecting LLM output as AI-generated text). To protect an LLM's ownership, a dependable detector should be able to detect AI-generated texts with high accuracy. In this section, we discuss {\it paraphrasing attacks} that can degrade type-II errors of state-of-the-art AI text detectors such as soft watermarking \citep{kirchenbauer2023watermark}, zero-shot detectors \citep{mitchell2023detectgpt}, trained neural network-based detectors \citep{openaidetectgpt2}, and retrieval-based detectors \citep{krishna2023paraphrasing}. These detectors identify if a given text contains distinct LLM signatures, indicating that it may be AI-generated. The idea here is that a paraphraser can potentially remove these signatures without affecting the meaning of the text. While we discuss this attack theoretically in \S \ref{sec:impossibilityresult}, the main intuition here is as follows: 


Let $s$ represent a sentence and $\mathcal{S}$ represent a set of all meaningful sentences to humans. Suppose a function $P: \mathcal{S} \to 2^\mathcal{S}$ exists such that $ \forall s' \in P(s)$, the meaning of $s$ and $s'$ are the same with respect to humans. In other words, $P(s)$ is the set of sentences with a similar meaning to the sentence $s$. 
Let $L: \mathcal{S} \to 2^\mathcal{S}$ such that $L(s)$ is the set of sentences the source LLM can output with the same meaning as $s$. Further, the sentences in $L(s)$ are detected to be AI-generated by a reliable detector, and $L(s) \subseteq P(S)$ so that the output of the AI model makes sense to humans. 
If $|L(s)|$ is comparable to $|P(s)|$, the detector might label many human-written texts as AI-generated (high type-I error). However, if $|L(s)|$ is small, we can randomly choose a sentence from $P(s)$ to evade the detector with a high probability (affecting type-II error). Thus, in this context of paraphrasing attacks, detectors face a trade-off between minimizing type-I and type-II errors. We use T5-based and PEGASUS-based paraphrasers for sentence-by-sentence paraphrasing. Suppose an AI-text passage $S = (s_1, s_2, ..., s_n)$ and $f$ is a paraphraser. The paraphrase attack modifies $S$ to get $(f(s_1), f(s_2), ..., f(s_n))$. This output should be ideally classified as AI-generated. 
The 11B parameter DIPPER paraphraser proposed in \citet{krishna2023paraphrasing} is powerful and it can paraphrase $S$ to get $f(S)$ in one-shot. Here, the output of $f(S)$ could also be conditioned by an input prompt. We use DIPPER for recursive paraphrase attacks. For \texttt{i} rounds of paraphrasing of $S$ (represented as \texttt{ppi}), we use DIPPER $f$ recursively on $S$ \texttt{i} times. That is, for generation \texttt{ppi}, we apply $f$ on \texttt{pp(i-1)}. We also condition DIPPER with a prompt \texttt{pp(i-2)} to encourage the paraphrasing to improve its quality.









\subsection{Paraphrasing Attacks on Watermarked AI-generated Text}

% \begin{wrapfigure}{r}{0.5\textwidth}
% \vspace{-4mm}
%   \begin{center}
%     \includegraphics[width=0.5\textwidth]{images/rephrase.png}
%   \end{center}
%   \caption{Accuracy of the soft watermarking detector on paraphrased LLM outputs plotted against perplexity. The lower the perplexity is, the better the quality of the text is.}
%     \label{fig:rephrase-tradeoff}
% \end{wrapfigure}

% \begin{wrapfigure}{R}{0.4\textwidth}
% \vspace{-8mm}
%   \begin{center}
%     \includegraphics[width=0.4\textwidth]{images/watermark_roc.png}
%   \end{center}
%   \vspace{-4mm}
%   \caption{ROC curves for the watermark-based detector. The performance of such a strong detection model can deteriorate with paraphrasing and spoofing attacks. \texttt{ppi} refers to \texttt{i} recursive paraphrasing.}
%   \vspace{-2mm}
%     \label{fig:watermark-roc}
% \end{wrapfigure}

\begin{figure}[t] 
	\centering 
	\begin{minipage}[t]{6.2cm} 
		\centering 
		\includegraphics[width=\linewidth]{images/rephrase.png}
  
		\caption{Accuracy of the soft watermarking detector on paraphrased LLM outputs plotted against perplexity. The lower the perplexity is, the better the quality of the text is.} 
  \label{fig:rephrase-tradeoff}
	\end{minipage} \hfill
	\begin{minipage}[t]{7cm} 
		\centering 
		\includegraphics[width=\linewidth]{images/rec_para_watermark.png}
  
		\caption{ROC curves for the watermark-based detector. The performance of such a strong detection model can deteriorate with recursive paraphrasing attacks. \texttt{ppi} refers to \texttt{i} recursive paraphrasing.} 
  \label{fig:watermark-roc}
	\end{minipage} 
\end{figure} 


Here, we perform our experiments on the soft watermarking scheme
% \footnote{\url{https://github.com/jwkirchenbauer/lm-watermarking}}
proposed in \cite{kirchenbauer2023watermark}. In this scheme, an output token of the LLM is selected from a {\it green list} determined by its prefix. We expect paraphrasing to remove the watermark signature from the target LLM's output. The target AI text generator uses a transformer-based OPT-1.3B \citep{opt} architecture with 1.3B parameters.
% \footnote{\url{https://huggingface.co/facebook/opt-1.3b}}
 We use a T5-based \citep{t5} paraphrasing model \citep{prithivida2021parrot} with 222M parameters
% \footnote{\url{https://huggingface.co/prithivida/parrot_paraphraser_on_T5}}
and a PEGASUS-based \citep{zhang2019pegasus} paraphrasing model with 568M parameters\footnote{\url{https://huggingface.co/tuner007/pegasus_paraphrase}} ($2.3\times$ and $5.8\times$ smaller than the target LLM, respectively). The target LLM is trained to perform text completion tasks on extensive data, while the smaller paraphrasing models are fine-tuned only for paraphrasing tasks. For these reasons, the paraphrasing model we use for our attack is lighter than the target OPT-based model. 



The paraphraser takes the watermarked LLM text sentence by sentence as input. We use 100 passages from the Extreme Summarization (XSum) dataset \citep{xsum} for our evaluations.
% \footnote{\url{https://huggingface.co/datasets/xsum}}
The passages from this dataset are input to the target AI model to generate watermarked text. Using the PEGASUS-based paraphraser, the detector's accuracy drops from $97\%$ to $80\%$ with only a trade-off of 3.5 in perplexity score (see Table \ref{tab:paraphraseresult}). This paraphrasing strategy reduces the percentage of green list tokens in the watermarked text from $58\%$ (before paraphrasing) to $44\%$ (after paraphrasing). Table \ref{tab:paraphrase} shows an example output from the target soft watermarked LLM before and after paraphrasing. We also use a much smaller T5-based paraphraser \citep{prithivida2021parrot} to show that even such a na\"ive paraphraser can drop the detector's accuracy from $97\%$ to $57\%$. Figure \ref{fig:rephrase-tradeoff} shows the trade-off between the detection accuracy and the T5-based paraphraser's output text quality (measured using perplexity score). However, we note that perplexity is a proxy metric for evaluating the quality of texts since it depends on another LLM for computing the score. We use a larger OPT-2.7B
% \footnote{\url{https://huggingface.co/facebook/opt-2.7b}}
\citep{opt} with 2.7B parameters for computing the perplexity scores. Figure \ref{fig:watermark-roc} shows the performance of the watermarking model with recursive paraphrase attack using DIPPER \citep{krishna2023paraphrasing}.
DIPPER can efficiently paraphrase passages in context. That is, DIPPER modifies $S$ to $f(S)$ where $f$ is the DIPPER paraphraser. \texttt{ppi} refers to the \texttt{i}-th recursion of paraphrasing. For example, \texttt{pp3} for $S$ using $f$ gives $f(f(f(S)))$. 
We use 100 human-written passages from XSum and 100 watermarked XSum passage completions to evaluate the ROC curve.
{\bf We observe that the true positive rate of the watermarking model at a $1\%$ false positive rate degrades from $\mathbf{99\%}$ (no attack) to $\mathbf{15\%}$ (\texttt{pp5}) after five rounds of recursive paraphrasing.} The AUROC of the detector drops from $99.8\%$ to $67.9\%$. Table \ref{tab:rec_para} shows an example of a recursively paraphrased passage.

\begin{figure}[t]
    \centering
    % \begin{subfigure}{0.75\textwidth}
    %  \includegraphics[width=\textwidth]{images/original_detectgpt_roc_curves.png}
    %   \caption{\textbf{Before attack}: ROC curves for various trained and zero-shot classifiers when detecting output text from GPT-2.\vspace{0.75cm}}
    %   \label{fig:roc_ai1}
    % \end{subfigure} %
    
    %  \begin{subfigure}{0.75\textwidth}
    %  \includegraphics[width=\textwidth]{images/parrot_detectgpt_fl=0.9_roc_curves.png}
    %  \caption{\textbf{After attack}: ROC curves for non-watermarking detectors when detecting paraphrased texts. The performance of the zero-shot classifiers drops significantly. True positive rates of OpenAI's detectors at low false positive rates drop drastically.\vspace{0.75cm}}
    %  \label{fig:roc_ai2}
    %  \end{subfigure}   %
    %   \begin{subfigure}{0.75\textwidth}
    %  \includegraphics[width=\textwidth]{images/parrot_detectgpt_fl=0.9_8_trials_roc_curves.png}
     % \caption{\textbf{After attack with eight queries to the detectors}: If we assume modest query access to the detectors, the attack can be more efficient. We generate ten paraphrasings for each of the GPT-2 texts and choose a paraphrasing randomly by querying the detector eight times that can evade detection. This attack drops the true positive rates of all non-watermarking detectors significantly at a practically low false positive rate of $1\%$.      
     % }
     % \label{fig:roc_ai3}
     % \end{subfigure} 

    \includegraphics[width=1\textwidth]{images/nonwatermark_roc.png}
    % \vspace{}
   \caption{ROC curves for various trained and zero-shot detectors. Left: Without attack. Middle: After paraphrasing attack. The performance of zero-shot detectors drops significantly. Right: Here, we assume we can query the detector eight times for the paraphrasing attack. We generate ten paraphrasings for each passage and query multiple times to evade detection. Notice how all detectors have low true positives at $1\%$ false positives. In the plot legend -- \texttt{perturbation} refers to the zero-shot methods in \cite{mitchell2023detectgpt}; \texttt{threshold} refers to the zero-shot methods in \cite{solaiman2019release, gehrmann2019gltr, ippolito2019automatic}; \texttt{roberta} refers to OpeanAI's trained detectors \citep{openaidetectgpt2}. AUROC scores for each plot are given in parentheses.}
    \label{fig:roc_nonwatermark}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{images/recursive_3_paraphrase_semilogx_roc_curves.png}~
    \includegraphics[width=0.49\textwidth]{images/recursive_5_paraphrase_semilogx_roc_curves.png}
    
    \caption{ROC curves illustrating the detection of recursively paraphrased text. Left: Three rounds of recursive paraphrasing with three queries to the detector. Right: Five rounds of recursive paraphrasing with five queries to the detector.}
    \label{fig:recursive-ml-detect}
\end{figure}

\begin{table}[]
\small
    \centering
    \begin{tabular}{P{2cm}|M{10cm}}
    \toprule
    Description     &  \multicolumn{1}{P{10cm}}{Text} \\ \midrule \midrule
    Input     &  {Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\textbackslash nWorkers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders.\textbackslash nThe Welsh Government [continues...]} \\ \midrule
    \texttt{pp1} & {In the year 2014-2015, Prison Link Cymru made 1,099 referrals, and said some offenders waited up to a year before finding suitable accommodation. Staff from the charitable organization say that the investment in housing would be cheaper than having to re-lock homeless offenders. The Welsh government [continues...]}  \\ \midrule
    \texttt{pp2} & {In 2015-16, Prison Link Cymru had 1,099 referrals and said some offenders were living rough for up to a year before finding suitable accommodation. Workers at the charity claim investment in housing would be cheaper than locking up homeless repeat offenders. The Welsh Government [continues...]} \\ \midrule
    \texttt{pp3} & {In fact, it was one of the main reasons why they came back to prison. In the year 2015-16, Prison Link Cymru made 1,099 referrals and said that offenders had lived for up to a year before finding suitable accommodation. The workers at the charitable organization say that the investment in housing would be cheaper than re-locking homeless offenders. The government of Wales [continues...]} \\ \midrule
    \texttt{pp4} & {In the year to the end of March, Prison Link Cymru had 1,099 referrals and said offenders had been living rough for up to a year before finding suitable accommodation. Workers at the charity say investment in housing would be cheaper than re-imprisoning homeless repeat offenders. The Welsh Government [continues...]} \\ \midrule
    \texttt{pp5} & {The government of Wales says that more people than ever before are being helped to deal with their housing problems. In the year 2015-16, Prison Link Cymru referred 1,099 people and said that homeless people had lived up to a year before finding suitable accommodation. The workers at the charitable organization say that the investment in housing would be cheaper than imprisoning homeless offenders again. Prison Link Cymru [continues...]} \\ \bottomrule
    \end{tabular}
    \vspace{0.2cm}
    \caption{Example of a recursively paraphrased passage from the XSum dataset. The paraphrasing is performed using DIPPER \citep{krishna2023paraphrasing}. \texttt{ppi} refers to the output after \texttt{i} rounds of recursive paraphrasing.}
    \label{tab:rec_para}
\end{table}

%%%% DO NOT DELETE THIS %%%%%%%
%%%% FULL TABLE CONTENT %%%%%%%
% input
% In the year 2014-2015, Prison Link Cymru made 1,099 referrals, and said some offenders waited up to a year before finding suitable accommodation. Staff from the charitable organization say that the investment in housing would be cheaper than having to re-lock homeless offenders. The Welsh government said that more people than ever are getting help to deal with their housing problems. Changes to the Welsh Housing Act in 2015 removed priority rights for ex-offenders. Prison Link Cymru, which helps people to find accommodation after they have been released, said that women generally had a good time, as they had to deal with children and other problems.

% pp1
% In the year 2014-2015, Prison Link Cymru made 1,099 referrals, and said some offenders waited up to a year before finding suitable accommodation. Staff from the charitable organization say that the investment in housing would be cheaper than having to re-lock homeless offenders. The Welsh government said that more people than ever are getting help to deal with their housing problems. Changes to the Welsh Housing Act in 2015 removed priority rights for ex-offenders. Prison Link Cymru, which helps people to find accommodation after they have been released, said that women generally had a good time, as they had to deal with children and other problems.

% pp2
% In 2015-16, Prison Link Cymru had 1,099 referrals and said some offenders were living rough for up to a year before finding suitable accommodation. Workers at the charity claim investment in housing would be cheaper than locking up homeless repeat offenders. The Welsh Government said more people than ever before were getting help to address housing problems. Prison Link Cymru, which helps people find accommodation after they have left prison, said things were generally good for women because issues such as children or domestic violence were often the main reasons they returned to prison.

% pp3
% In fact, it was one of the main reasons why they came back to prison. In the year 2015-16, Prison Link Cymru made 1,099 referrals and said that offenders had lived for up to a year before finding suitable accommodation. The workers at the charitable organization say that the investment in housing would be cheaper than re-locking homeless offenders. The government of Wales says that more people than ever are being helped to deal with their housing problems. Prison Link Cymru, which helps people to find accommodation after being released, says that women are generally in a good position, as children and other problems are often the main reason why they are returned to prison.

% pp4
% In the year to the end of March, Prison Link Cymru had 1,099 referrals and said offenders had been living rough for up to a year before finding suitable accommodation. Workers at the charity say investment in housing would be cheaper than re-imprisoning homeless repeat offenders. The Welsh Government says more people than ever before are getting help to address their housing problems. Prison Link Cymru, which helps people find accommodation after they have been released from prison, said generally things were good for women because issues such as children and other problems were often the main reasons they returned to prison.

% pp5
% The government of Wales says that more people than ever before are being helped to deal with their housing problems. In the year 2015-16, Prison Link Cymru referred 1,099 people and said that homeless people had lived up to a year before finding suitable accommodation. The workers at the charitable organization say that the investment in housing would be cheaper than imprisoning homeless offenders again. Prison Link Cymru, which helps people find accommodation after being released, says that women are generally well, because children and other problems are often the main reason why they return to prison.



\subsection{Paraphrasing Attacks on Non-Watermarked AI-generated texts}
\label{sec:nonwatermark}


\begin{table}[t]
\small
    \centering
    \begin{tabular}{P{2.3cm} | P{1cm} | M{9.2cm}}
    \toprule
         Description & z-score & \multicolumn{1}{P{9.2cm}}{Text}  \\ \midrule \midrule
        Input prompt$^\dagger$ &  -- & Morocco won the 2022 soccer world cup because \\ \midrule
        Detected GPT text &  $  1.47$ & Morocco won the 2022 soccer world cup because they are the best. Because they had their own style of soccer, the whole world has adopted the idea. Not to forget the other reason why we came to this tournament.  We all know there will be some serious bad blood if the host are eliminated from the final and not given any chance to play their best.  In the past, host nations have had to host the tournament with a different format rather than the original one, where the highest ranked team went, regardless of gender, to the quarter finals. \\ \midrule 
        Undetected T5-based paraphrasing & $ 0.80 $ & morocco won the 2022 world cup because they are the best. because of their own style of soccer the whole world followed this idea. Not to forget the other reason why we came to this tournament. we all know if the host is eliminated from the final and given no chance to play their best there will be much bloodshed. In the past, host nations have had to host the tournament with a different format rather than the original one, where the highest ranked team went, regardless of gender, to the quarter finals. \\

    \bottomrule
    \end{tabular}
    \vspace{0.2cm}
    \caption{Evading DetectGPT using a T5-based paraphraser. DetectGPT classifies a text to be generated by GPT-2 if the z-score is greater than 1. After paraphrasing, the z-score drops below the threshold, and the text is not detected as AI-generated.\\{\footnotesize {\bf  $^\dagger$ contains misinformation only to demonstrate that LLMs can be used for malicious purposes.}}}
    % \vspace{-3.3mm}
    \label{tab:paraphrase_2}
\end{table}








Non-watermarking detectors such as trained classifiers \citep{openaidetectgpt2}, retrieval-based detectors \citep{krishna2023paraphrasing}, and zero-shot classifiers \citep{mitchell2023detectgpt, gehrmann2019gltr, ippolito2019automatic, solaiman2019release} use the presence of LLM-specific signatures in AI-generated texts for their detection. 
Neural network-based trained detectors such as RoBERTa-Large-Detector from OpenAI \citep{openaidetectgpt2} are trained or fine-tuned for binary classification with datasets containing human and AI-generated texts. Zero-shot classifiers leverage specific statistical properties of the source LLM outputs for their detection. Retrieval-based methods search for a candidate passage in a database that stores the LLM outputs. Here, we perform experiments on these non-watermarking detectors to show they are vulnerable to our paraphrasing attack.


\begin{wrapfigure}{r}{0.6\textwidth}
    \vspace{-2mm}
    \centering
    % \hspace{-4mm}
    \includegraphics[width=0.95\linewidth, trim={4mm 0 6mm 4mm},clip]{images/IR_attack.png}
  \caption{Recursive paraphrasing breaks the retrieval-based detector \citep{krishna2023paraphrasing} without degrading text quality. \texttt{ppi} refers to $\texttt{i}$ recursion(s) of paraphrasing. Numbers next to markers denote the perplexity scores of the paraphraser output.}
    \label{fig:ir-attack}
    \vspace{-3mm}
\end{wrapfigure}
We use a pre-trained GPT-2 Medium
% \footnote{\url{https://huggingface.co/gpt2-medium}}
model \citep{gpt2} with 355M parameters to evaluate our attack on 200 passages from the XSum dataset \citep{xsum}. We use a T5-based paraphrasing model \citep{prithivida2021parrot} with 222M parameters to rephrase the output texts from the target GPT-2 Medium model. Figure \ref{fig:roc_nonwatermark} shows the effectiveness of the paraphrasing attack over these detectors. The AUROC scores of DetectGPT \citep{mitchell2023detectgpt} drop from $96.5\%$ (before the attack) to $59.8\%$ (after the attack). Note that AUROC of $50\%$ corresponds to a random detector. The rest of the zero-shot detectors \citep{solaiman2019release, gehrmann2019gltr, ippolito2019automatic} also perform poorly after our attack. Though the performance of the trained neural network-based detectors \citep{openaidetectgpt2} is better than that of zero-shot detectors, they are also not reliable. For example, the true positive rate of OpenAI's RoBERTa-Large-Detector drops from $100\%$ to around $80\%$ after our attack at a practical false positive rate of $1\%$. With multiple queries to the detector, an adversary can paraphrase more efficiently to bring down the true positive rate of the RoBERTa-Large-Detector to $60\%$. 
Table \ref{tab:paraphrase_2} shows an example of outputs from the GPT-2 model before and after paraphrasing. 
The output of the paraphraser reads well and means the same as the detected GPT-2 text. We measure the perplexities of the GPT-2 output text before the attack, after the paraphrase attack, and after multiple query paraphrase attack to be 16.3, 27.2, and 18.3, respectively.
% (Figure~\ref{fig:roc_ai1}). GPT-2 is a relatively old LLM, and it performs poorly when compared to more recent LLMs. The perplexity of the GPT-2 text after paraphrasing is 27.2 (Figure~\ref{fig:roc_ai2}). The perplexity score only degrades by 2 with multiple queries to the detector (Figure~\ref{fig:roc_ai3}).



We also examine the effectiveness of recursive paraphrasing on these detectors. Here we use the DIPPER paraphraser \texttt{i} times recursively (\texttt{ppi}) to generate \texttt{i} paraphrases of the GPT-2 generated text. We select the paraphrased text with the worst detection score out of the \texttt{i} paraphrased versions assuming black-box access to the detector. We present the ROC curves in Figure \ref{fig:recursive-ml-detect}. We observe a substantial decline in the AUROC values for all the detectors, highlighting the fragility of these detection methods with recursive paraphrasing. 
% For instance, the true positive rate of OpenAI's RoBERTa-Large detector \citep{openaidetectgpt2} drops from ${100\%}$ to ${85\%}$ at ${1\%}$ false positive rate.
For instance, \textbf{the AUROC curve values drops from $\mathbf{82\%}$ to $\mathbf{18\%}$ for DetectGPT \citep{mitchell2023detectgpt} after attack}.


\subsection{Paraphrase Attacks on Retrieval-based Defenses}


The retrieval-based detector in \citet{krishna2023paraphrasing} is designed to be robust against paraphrase attacks. They propose to maintain a database that stores the users' conversations with the LLM.  For a candidate passage, their detector relies on retrieving semantically-similar passages from this database. If the similarity is larger than a fixed threshold, the candidate passage is classified as AI-generated. They empirically show that their defense is robust to paraphrase attacks using their heavy-duty 11B parameter paraphraser, DIPPER, compared to other text detectors. 


However, we show that they can suffer from recursive paraphrase attacks. 
We use their codes\footnote{\url{https://github.com/martiansideofthemoon/ai-detection-paraphrases}}, and DIPPER \cite{krishna2023paraphrasing} for our experiments. We use 100 passages from the XSum dataset labeled as AI outputs and store them in the detector's database. As shown in Figure \ref{fig:ir-attack}, this detector detects all the AI outputs after a round of simple paraphrasing. However, {\bf the detection accuracy drops significantly to $\mathbf{25\%}$ after five rounds of recursive paraphrasing.} 
This shows that recursive paraphrasing can evade their semantic matching algorithm that aids in retrieval.
Using a heavy-duty paraphraser DIPPER helps preserve the perplexity scores as shown in Figure \ref{fig:ir-attack}. Moreover, retrieval is impractical since this leads to serious privacy concerns from storing users' LLM conversations.



