\section{Spoofing Attacks on AI-text Generative Models}
\label{sec:humantextdetected}

\begin{table}[t]
\small
    \centering
    \begin{tabular}{M{7cm} | P{1.6cm} | P{1cm} | P{2.0cm} }
    \toprule
        \multicolumn{1}{P{7cm}|}{Human text} & $\%$ tokens in green list & z-score & Detector output \\ \midrule \midrule
        the first thing you do will be the best thing you do. this is the reason why you do the first thing very well. if most of us did the first thing so well this world would be a lot better place. and it is a very well known fact. people from every place know this fact. time will prove this point to the all of us. as you get more money you will also get this fact like other people do. all of us should do the first thing very well. hence the first thing you do will be the best thing you do. & 42.6 & 4.36 & Watermarked\\ \midrule
        lot to and where is it about you know and where is it about you know and where is it that not this we are not him is it about you know and so for and go is it that. & 92.5 & 9.86 & Watermarked \\ \bottomrule 
    \end{tabular}
    \vspace{0.2cm}
    \caption{Proof-of-concept human-generated texts flagged as watermarked by the soft watermarking scheme. In the first row, a sensible sentence composed by an {\it adversarial human} contains $42.6\%$ tokens from the green list. In the second row, a nonsense sentence generated by an {\it adversarial human} using our tool contains $92.5\%$ green list tokens. The z-test threshold for watermark detection is 4.}
    \label{tab:adversarialhuman}
    \vspace{-5mm}
\end{table}

% DO NOT DELETE THIS
% COMPLETE TABLE TEXT ENTRIES
% the first thing you do will be the best thing you do. this is the reason why you do the first thing very well. if most of us did the first thing so well this world would be a lot better place. and it is a very well known fact. people from every place know this fact. time will prove this point to the all of us. as you get more money you will also get this fact like other people do. all of us should do the first thing very well. hence the first thing you do will be the best thing you do.
% lot to and where is it about you know and where is it about you know and where is it that not this we are not him is it about you know and so for and go is it that.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/the.png}
    % \vspace{-6mm}
    \caption{Inferred {\it green list score} for the token ``the''. The plot shows the top 50 words from our set of common words that are likely to be in the green list. The word ``first'' occurred $\sim 25\%$ of the time as suffix to ``the''.}
    \vspace{-3.3mm}
    \label{fig:the}
\end{figure}


A strong AI text detection scheme should have both low type-I error (i.e., human text detected as AI-generated) and type-II error (i.e., AI-generated text not detected). An AI language detector without a low type-I error can cause harm as it might wrongly accuse a human of plagiarizing using an LLM. Moreover, an attacker (adversarial human) can generate a non-AI text that is detected to be AI-generated. This is called the {\it spoofing attack}. An adversary can potentially launch spoofing attacks to produce derogatory texts that are detected to be AI-generated to affect the reputation of the target LLM's developers. In this section, as a proof-of-concept, we show that the soft watermarking  \citep{kirchenbauer2023watermark} and retrieval-based \citep{krishna2023paraphrasing} detectors can be spoofed to detect texts composed by humans as watermarked. 



\subsection{Spoofing Attacks on Watermarked Models}



\begin{wrapfigure}{r}{0.55\textwidth}
    \centering
    \includegraphics[width=1\linewidth,]{images/wm_spoof.png}
  \caption{ROC curve of a soft watermarking-based detector \citep{kirchenbauer2023watermark} after our spoofing attack.}
  \vspace{-3.3mm}
    \label{fig:watermark-roc-spoof}
\end{wrapfigure}
In \citet{kirchenbauer2023watermark}, they
watermark LLM outputs by asserting the model to output tokens with some specific pattern that can be easily detected with meager error rates. Soft watermarked texts are majorly composed of {\it green list} tokens. If an adversary can learn the green lists for the soft watermarking scheme, they can use this information to generate human-written texts that are detected to be watermarked. Our experiments show that the soft watermarking scheme can be spoofed efficiently. Though the soft watermarking detector can detect the presence of a watermark very accurately, it cannot be certain if this pattern is actually generated by a human or an LLM.  An {\it adversarial human} can compose derogatory watermarked texts in this fashion that are detected to be watermarked, which might cause reputational damage to the developers of the watermarked LLM. Therefore, it is important to study {\it spoofing attacks} to avoid such scenarios.


% \textbf{The attack methodology:} 
% For an output word $s^{(t)}$, soft watermarking samples a word from its green list with high probability. 
In watermarking, the prefix word $s^{(t-1)}$ determines the green list for selecting the word $s^{(t)}$. The attacker's objective is to compute a proxy of green lists for the $N$ most commonly used words in the vocabulary.
% A smaller $N$, when compared to the size of the vocabulary, helps faster computations with a trade-off in the attacker's knowledge of the watermarking scheme. 
We use a small value of $N=181$ for our experiments. 
The attacker queries the watermarked OPT-1.3B \cite{opt} $10^6$ times to observe pair-wise token occurrences in its output to estimate {\it green list score} for the $N$ tokens. A token with a high green list score for a prefix $s^{(t)}$ might be in its green list (see Figure \ref{fig:the}). We build a tool that helps adversarial humans create watermarked sentences by providing them with proxy green list. In this manner, we can spoof watermarking models easily. See Table \ref{tab:adversarialhuman} for example sentences created by an adversarial human.
% The attacker can query the watermarked LLM multiple times to learn the pair-wise occurrences of these $N$ words in the LLM output. Observing these outputs, the attacker can compute the probability of occurrence of a word given a prefix word $s^{(t-1)}$. This score can be used as a proxy for computing the green list for the prefix word $s^{(t-1)}$. An attacker with access to these proxy green lists can compose a text detected to be watermarked, thus spoofing the detector. In our experiments, we query the watermarked OPT-1.3B \citep{opt} $10^6$ times to evaluate the {\it green list scores} to evaluate the green list proxies. We find that inputting nonsense sentences composed of the $N$ common words encourages the LLM to output text majorly only composed of these words. This makes the querying more efficient. In Figure \ref{fig:the}, we show the learned {\ green list scores} for the prefix word ``the'' using our querying technique. We build a simple tool that lets a user create passages token by token. At every step, the user is provided with a list of potential green list words sorted based on the { green list score}. These users or {adversarial humans} try to generate meaningful passages assisted by our tool. Since most of the words selected by { adversarial humans} are likely to be in the green list, we expect the watermarking scheme to detect these texts to be watermarked. Table \ref{tab:adversarialhuman} shows examples of sentences composed by { adversarial humans} that are detected to be watermarked. Even a nonsense sentence generated by an adversarial human can be detected as watermarked with very high confidence.
Figure \ref{fig:watermark-roc-spoof} shows that the performance of watermark-based detectors degrades significantly in the presence of paraphrasing and spoofing attacks, showing that they are not reliable. 

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.6\linewidth]{images/the.png}
%     \vspace{-2mm}
%     \caption{Inferred {\it green list score} for the token ``the''. The plot shows the top 50 words from our set of common words that are likely to be in the green list. The word ``first'' occurred $\sim 25\%$ of the time as suffix to ``the''.}
%     \label{fig:the}
%     \vspace{-2mm}
% \end{figure}

\subsection{Spoofing Attacks on Retrieval-based Defenses}
Retrieval-based detectors store LLM outputs in a database. Although {\bf storing user-LLM conversations can cause serious privacy concerns in the real world}, they use this database to detect if a candidate passage is AI-generated by searching for semantically similar passages. An adversary with access to human-written passages can feed them into this database and later use the detector to falsely accuse these humans of plagiarism. For our experiments, we consider outputs from DIPPER, a paraphrasing LLM introduced in \cite{krishna2023paraphrasing}, as AI outputs. An adversary asks DIPPER to paraphrase 100 human passages from XSum, and this retrieval-based detector stores the paraphraser outputs in its database. Our experiments show that the retrieval-based detector falsely classifies all 100 human passages as AI-generated. In our experiments, this retrieval-based detector achieves $0\%$ human text detection when spoofed.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{c| c | c}
    \toprule
    Detection Methods  & T@F & F@T \\          \midrule \midrule
Entropy threshold \cite{gehrmann2019gltr} & \textbf{0.025} (0.045) & \textbf{0.995} (0.845)\\
Likelihood threshold \cite{solaiman2019release} & \textbf{0.050} (0.075) & \textbf{0.995} (0.310)\\
Logrank threshold & 0.165 (0.155)  & \textbf{0.690} (0.190)\\
Rank threshold \cite{gehrmann2019gltr} & 0.530 (0.335) & \textbf{0.655} (0.590)\\
Roberta (base) OpenAI detector \cite{openaidetectgpt2} & 0.900 (0.765) & 0.010 (0.035)\\
Roberta (large) OpenAI detector \cite{openaidetectgpt2} & \textbf{0.985} (0.990) & 0.000 (0.000) \\
% Perturbation (d) & 0.115 & 0.240 \\
DetectGPT \cite{mitchell2023detectgpt} & \textbf{0.055} (0.240) & \textbf{0.555} (0.145)\\
\bottomrule
    \end{tabular}
    \vspace{0.2cm}
    \caption{True positive rates at $1\%$ false positive rate (T@F)  and  false positive rates at $90\%$ true positive rate (F@T) after (before the attack in parentheses) the spoofing attack described in \S \ref{sec:spoof_detector}. Bolded numbers show successful attacks where T@F decreases, or F@T increases after spoofing.}
    \label{tab:spoof_fpr_tpr}
\end{table}


% \begin{table}[t]
%     \centering
%     \small
%     \begin{tabular}{c|c | c}
%     \toprule
%     Detection Methods  & Before Spoofing & After Spoofing \\          \midrule \midrule
% Entropy threshold \cite{gehrmann2019gltr} & 0.845 & 0.995 \\
% Likelihood threshold \cite{solaiman2019release} & 0.310 & 0.995 \\
% Logrank threshold & 0.190 & 0.690 \\
% Rank threshold \cite{gehrmann2019gltr} & 0.590 & 0.655 \\
% Roberta (base) OpenAI detector \cite{openaidetectgpt2} & 0.035 & 0.010 \\
% Roberta (large) OpenAI detector \cite{openaidetectgpt2} & 0.000 & 0.000 \\
% % Perturbation (d) & 0.115 & 0.240 \\
% Detect-GPT \cite{mitchell2023detectgpt} & 0.145 & 0.555 \\
% \bottomrule
%     \end{tabular}
%     \vspace{0.2cm}
%     \caption{\textcolor{blue}{False positive rate at $90\%$ true positive rate before and after the spoofing attack described in \S \ref{sec:spoof_detector}. }}
%     \label{tab:spoof_fpr_tpr}
% \end{table}

% \begin{table}[t]
%     \centering
%     \small
%     \begin{tabular}{c|cc}
%     \toprule
%    Detection Methods & Before Spoofing & After Spoofing \\
%     \midrule \midrule
%         Entropy threshold \cite{gehrmann2019gltr} & 0.045 & 0.025 \\
% Likelihood threshold \cite{solaiman2019release} & 0.075 & 0.050 \\
% Logrank threshold & 0.155 & 0.165 \\
% Rank threshold \cite{gehrmann2019gltr} & 0.335 & 0.530 \\
% RoBERTa (base) OpenAI detector \cite{openaidetectgpt2} & 0.765 & 0.900 \\
% RoBERTa (large) OpenAI detector \cite{openaidetectgpt2} & 0.990 & 0.985 \\
% % Perturbation (d) & 0.640 & 0.610 \\
% Detect-GPT \cite{mitchell2023detectgpt} & 0.240 & 0.055 \\
% \bottomrule
%     \end{tabular}
%     \vspace{0.2cm}
%     \caption{\textcolor{blue}{True positive rate at $1\%$ false positive rate before and after the spoofing attack described in \S \ref{sec:spoof_detector}.}}
%     \label{tab:spoof_tpr_fpr}
% \end{table}
\begin{figure}[t]

    \centering
    \includegraphics[width=0.49\textwidth]{images/before-spoof-semilogx_roc_curves.png}
    \includegraphics[width=0.49\textwidth]{images/after-spoof-semilogx_roc_curves.png}
    \caption{ROC curves before (left) and after (right) spoofing attack (\S~\ref{sec:spoof_detector}). Most detectors exhibit quality degradation after our spoofing attack.}
    \label{fig:spoof_roc}
\end{figure}

\subsection{Spoofing Attacks on Zero-Shot and Trained Detectors}
\label{sec:spoof_detector}

We further show that zero-shot and trained detectors may also be vulnerable to spoofing attacks. In this setting, a malicious adversary could write a short text in a collaborative work which may lead to the entire text being classified as AI-generated. To simulate this, we prepend a human-written text marked as AI-generated by the detector to all the other human-generated text for spoofing. In other words, from the first 200 samples in the XSum dataset, we pick the human text with the worst detection score for each detector considered in \S~\ref{sec:nonwatermark}. We then prepend this text to all the other human texts, ensuring that the length of the prepended text does not exceed the length of the original text. We report the false positive rate fixed at a true positive rate of $90\%$ and the true positive rate at a false positive rate of $1\%$ in Table~\ref{tab:spoof_fpr_tpr}. The ROC curves before and after spoofing the detectors are provided in Figure~\ref{fig:spoof_roc}. Our experiments demonstrate that most of these detection methods show a significant increase in false positive rates at a fixed true positive rate of $90\%$ after spoofing.
After this na\"ive spoofing attack, the true positive rate at a false positive rate of $1\%$ and AUROC scores of these detectors drop significantly. 


