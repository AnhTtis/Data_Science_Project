\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/title-wm.png}
    \vspace{-2mm}
    \caption{An illustration of vulnerabilities of existing AI-text detectors. We consider both watermarking-based \citep{kirchenbauer2023watermark} and non-watermarking-based detectors \citep{mitchell2023detectgpt, openaidetectgpt2, krishna2023paraphrasing} and show that they are not reliable in practical scenarios. Colored arrow paths show the potential pipelines for adversaries to avoid detection. In \textcolor{red}{red}: an attacker can use a paraphraser to remove the LLM signatures from an AI-generated text to avoid detection. We show that this attack can break a wide range of detectors, including the ones with retrieval-based defense \citep{krishna2023paraphrasing} that are built to be robust against paraphrasing. We provide an {\it impossibility result} indicating that for a sufficiently good language model, even the best-possible detector can perform only marginally better than a random classifier. In \textcolor{blue}{blue}: An adversary can query the soft watermarked LLM multiple times to learn its watermarking scheme. This information can be used to spoof the watermark detector by composing human text that is detected to be watermarked. In \S \ref{sec:humantextdetected}, we show that all other detectors can also be easily spoofed to make genuine human text detected as AI-generated.}
    \label{fig:title}
    % \vspace{-5mm}
\end{figure}  

Artificial Intelligence (AI) has made tremendous advances in recent years, from generative models in computer vision \citep{stablediff, imagen} to large language models (LLMs) % generative models
in natural language processing (NLP) \citep{gpt3, opt, t5}.
%Large Language Models
LLMs can now generate texts of supreme quality with the potential in many applications. For example, the recent model of ChatGPT \citep{chatgpt} can generate human-like texts for various tasks such as writing codes for computer programs, lyrics for songs, completing documents, and question answering; its applications are endless. The trend in NLP shows that these LLMs will even get better with time. However, this comes with a significant challenge in terms of authenticity and regulations. AI tools have the potential to be misused by users for unethical purposes such as plagiarism, generating fake news, spamming, generating fake product reviews, and manipulating web content for social engineering in ways that can have negative impacts on society \citep{adelani2020generating, weiss2019deepfake}. Some news articles rewritten by AI have led to many fundamental errors in them \citep{cnet}. Hence, there is a need to ensure the responsible use of these generative AI tools. In order to aid this, a lot of recent research focuses on detecting AI-generated texts. 

Several detection works study this problem as a binary classification problem \citep{openaidetectgpt2, jawahar2020automatic, mitchell2023detectgpt, bakhtin2019real, fagni2020tweepfake}. 
For example, OpenAI fine-tunes RoBERTa-based \citep{liu2019roberta} GPT-2 detector models to distinguish between non-AI generated and GPT-2 generated texts \citep{openaidetectgpt2}. This requires such a detector to be fine-tuned with supervision on each new LLM for reliable detection.
These approaches that rely on a neural network for their detection, can be vulnerable to adversarial and poisoning attacks \citep{goodfellow2014explaining, sadasivan2023cuda, kumar2022certifying, wang2022improved}.
Another stream of work focuses on zero-shot AI text detection without any additional training overhead \citep{solaiman2019release, ippolito2019automatic, gehrmann2019gltr}. These works evaluate the expected per-token log probability of texts and perform thresholding to detect AI-generated texts. \citet{mitchell2023detectgpt} observe that AI-generated passages tend to lie in negative curvature of log probability of texts. They propose DetectGPT, a zero-shot LLM text detection method, to leverage this observation. 
Another line of work aims to watermark AI-generated texts to ease their detection \citep{atallah2001natural, wilson2014linguistic, kirchenbauer2023watermark, zhao2023protecting}. Watermarking eases the detection of LLM output text by imprinting specific patterns on them. Soft watermarking proposed in \citet{kirchenbauer2023watermark} partitions tokens into  green and red lists to help create these patterns. A watermarked LLM samples a token, with high probability, from the green list determined by its prefix token. These watermarks are often imperceptible to humans. However, watermarking might not be a useful tool to prevent LLM exploitation unless all the powerful LLMs are protected similarly. \citet{krishna2023paraphrasing} introduces an information retrieval-based detector by storing the outputs of the LLM in a database. For a candidate passage, their algorithm searches this database for semantically similar matches to make their detection robust to paraphrasing. However, storing user-LLM conversations might lead to serious privacy concerns.




In this paper, through empirical and theoretical analysis, we show that these state-of-the-art AI-text detectors are unreliable in practical scenarios \citep{wolff, scottaaronson, liang2023gpt}. We study empirical attacks on soft watermarking \citep{kirchenbauer2023watermark}, and a wide range of zero-shot \citep{mitchell2023detectgpt}, retrieval-based \citep{krishna2023paraphrasing}, and neural network-based detectors \citep{openaidetectgpt2}.

\textbf{Paraphrasers:} We show that a {\it paraphrasing attack}, where a lightweight neural network-based paraphraser is applied to the output text of the AI-generative model, can evade various types of detectors. Before highlighting the results, let us provide an intuition why this attack is successful. For a given sentence $s$, suppose $P(s)$ is the set of all paraphrased sentences that have similar meanings to the sentence $s$. Moreover, let $L(s)$ be the set of sentences the source LLM can output with meanings similar to $s$. Suppose a user has generated $s$ using an LLM and wants to evade detection. If $|L(s)| \ll |P(s)|$, the user can randomly sample from $P(s)$ and avoid detection (if the detection model has a reasonably low false positive rate). Moreover, if $|L(s)|$ is comparable to $|P(s)|$, the detector cannot have low false positive and negative rates simultaneously. 

\looseness -1
With this intuition in mind, in \S \ref{sec:aigentextnotdetected}, we use light-weight neural network-based paraphrasers ($2.3\times$ and $5.5\times$ smaller than the source LLM in terms of the number of parameters) to rephrase the source LLM's output text. Our experiments show that this automated paraphrasing attack can drastically reduce the accuracy of various detectors, including those using soft watermarking. For example, a PEGASUS-based paraphraser \citep{zhang2019pegasus} can drop the soft watermarking detector's \citep{kirchenbauer2023watermark} accuracy from $97\%$ to $80\%$ with just a degradation of 3.5 in the perplexity score. The area under the receiver operating characteristic (AUROC) curves of zero-shot detectors \citep{mitchell2023detectgpt} drop from $96.5\%$ to $25.2\%$ using a T5-based paraphraser \citep{prithivida2021parrot}. We also observe that the performance of neural network-based trained detectors \citep{openaidetectgpt2} deteriorate significantly after our paraphrasing attack. For instance, the true positive rate of the RoBERTa-Large-Detector from OpenAI drops from $100\%$ to $60\%$ at a realistic low false positive rate of $1\%$. In addition, we show that retrieval-based detector by \citet{krishna2023paraphrasing} designed to evade paraphrase attacks are vulnerable to {\it recursive} paraphrasing. In fact, the accuracy of their detector falls from $100\%$ to $25\%$ with our {\it recursive} paraphrase attack. We also show that {\it recursive paraphrasing} can further deteriorate the performance of watermarking-based \citep{kirchenbauer2023watermark} detectors (from $99\%$ to $15\%$ true positive rate at $1\%$ false positive rate), zero-shot \citep{mitchell2023detectgpt}, and neural network-based detectors \citep{openaidetectgpt2}.


\textbf{Impossibility of Detection:} In \S \ref{sec:impossibilityresult}, we present an impossibility result regarding the detection of AI-generated texts.
As language models advance, so does their ability to emulate human text. Indeed,
the problem of AI-text detection becomes more important and interesting in the presence of language models that are designed to mimic humans and evade detection\footnote{Evasion tools: \url{https://goldpenguin.org/blog/make-ai-writing-undetectable/}}\footnote{Undetectable AI: \url{https://undetectable.ai/}}\footnote{StealthGPT: \url{https://www.stealthgpt.ai/}} \citep{lu2023large}.
With new advances in LLMs, the distribution of AI-generated texts becomes increasingly similar to human-generated texts, making them harder to detect.
This similarity is reflected in the decreasing total variation distance between the distributions of human and AI-generated text sequences. %\citep{gpt4}. 
Adversaries, by seeking to mimic the human-generated text distribution using AI models, implicitly reduce the total variation distance between the two distributions to evade detection.

%There is an implicit assumption of ``adversarialness'' on the AI-generated text distribution as it seeks to mimic the human-generated text distribution. Otherwise, the total variation distance between the two distributions may be high.

Theorem~\ref{thm:ROC_bound} shows that as the total variation between the two distributions decreases, the performance of even the best possible detector deteriorates.
It bounds the area under the receiver operating characteristic curve (AUROC) of the best possible detector $D$ as %$\mathsf{AUROC}(D) \leq 1/2 + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \mathsf{TV}(\mathcal{M}, \mathcal{H})^2/2$
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2}\]
where $\mathsf{TV}(\mathcal{M}, \mathcal{H})$ is the total variation distance between model-generated text distribution $\mathcal{M}$ and human-generated text distribution $\mathcal{H}$.
%produced by an AI-model $\mathcal{M}$ and humans $\mathcal{H}$.
It shows that as the total variation distance diminishes, the best-possible detection performance approaches $1/2$, which represents the AUROC corresponding to a classifier that randomly labels text as AI or human-generated.

Our impossibility result does {\it not} imply that detection performance will necessarily become as bad as random, but that reliable detection may be unachievable.
In most real-world scenarios, a detector is considered good if it can achieve a high true positive rate, say 90\%, while maintaining a low false positive rate, say 1\%.
This is impossible to achieve when the two distributions overlap more than 11\% (i.e., total variation  $< 0.89$).
% Thus, for a sufficiently advanced language model, even the best-possible detector performs only marginally better than a random classifier.
% Even when the total variation between the distributions is moderate (say, $\approx 0.5$), detection performance may still be unreliable for real-world applications due to a high false positive rate.
The aim of this analysis is to urge caution when dealing with detection systems that purport to detect text produced by an AI model.
Any such system needs to be independently and rigorously evaluated for reliability and bias, ideally on language models designed to evade detection, before deployment in the real world.

%As better language models are developed, existing detectors should be re-evaluated and updated periodically to ensure that they remain reliable.}

We complement our result with a tightness analysis, where we demonstrate that for a given human distribution $\mathcal{H}$, there exists a distribution $\mathcal{M}$ and a detector $D$ for which the above bound holds with equality.
In \S \ref{sec:tv_estimation}, we also present a method to empirically estimate the total variation distance and show that it decreases with an increase in model size.

\textbf{Generalizability:} Although our analysis considers the text generated by all humans and general language models, it can also be applied to specific scenarios, such as particular writing styles, clever prompt engineering, or sentence paraphrasing, by defining $\mathcal{M}$ and $\mathcal{H}$ appropriately.
Our impossibility result does {\it not} make any assumptions on the nature of the distributions, such as $\mathcal{H}$ being a monolithic distribution shared by all humans, as claimed by \citet{kirchenbauer2023reliability} in more recent work.
The same result can be derived using $\mathcal{H}$ to represent the text distribution produced by an individual human (e.g. Donald Trump or Barack Obama, to borrow the example used in~\cite{kirchenbauer2023reliability}) and $\mathcal{M}$ to represent the output distribution of an AI seeking to mimic the individual's writing style.
$\mathcal{M}$ could also represent the text distribution of a general-purpose language model like GPT-4 when prompted in a specific manner, e.g., `Write a speech in the style of ...', to induce a certain writing style.
$\mathcal{H}$ and $\mathcal{M}$ could also represent a collection of human distributions and a collection of model distributions (mimicking the human distributions) combined together using appropriate weights based on context.

In Section~\ref{sec:PRG_bound}, we extend our impossibility result to include the case where \emph{pseudorandom} number generators are used in LLMs (e.g., sampling tokens) instead of true randomness.
This pseudorandomness could make the AI-generated text distribution very different from the human-generated text distribution~\cite{kirchenbauer2023reliability}.
This is because the pseudorandom AI-generated distribution is a collection of Dirac delta function distributions and a human is exorbitantly unlikely to produce a sample corresponding to any of the delta functions.
This means that the total variation between the human and pseudorandom AI-generated distributions is almost one, implying that an almost perfect detector could exist in theory.
However, the existence of such a detector does not imply that it could be computed efficiently.
We show that, for a computationally bounded detector, the bound in Theorem~\ref{thm:ROC_bound} could be modified with a small correction term $\epsilon$ to account for the presence of pseudorandomness.
Formally, for a polynomial-time computable detector $D$, we prove the following bound on its performance:
\[\mathsf{AUROC}(D) \leq \frac{1}{2} + \mathsf{TV}(\mathcal{M}, \mathcal{H}) - \frac{\mathsf{TV}(\mathcal{M}, \mathcal{H})^2}{2} + \epsilon,\]
where $\epsilon$ is a negligible function $1/b^t$ of the number of bits $b$ used in the seed of the pseudorandom generator for a positive integer $t$.

%For example, it
Our impossibility result could also be used to show that AI-generated text, even with an embedded watermark, can be made difficult to detect by simply passing it through a paraphrasing tool. For a sequence $s$ generated by a language model, we set $\mathcal{M}$ and $\mathcal{H}$ to be the distributions of sequences of similar meaning to $s$ produced by the paraphraser and humans. The goal of the paraphraser is to make its output distribution similar to the distribution of human-generated sequences with respect to the total variation distance.
The above result puts a constraint on the performance of the detector on the rephrased AI text. 

% \looseness -1
\textbf{Spoofing Attacks:} Finally, we discuss the possibility of {\it spoofing attacks} on text generative models in  \S \ref{sec:humantextdetected}. In this setting, an attacker generates a non-AI text that is detected to be AI-generated. An adversary can potentially launch spoofing attacks to produce derogatory texts that are detected to be AI-generated to affect the reputation of the target LLM's developers.
% As a proof-of-concept, we show that the soft watermarking \citep{kirchenbauer2023watermark} and retrieval-based \citep{krishna2023paraphrasing} detectors can be spoofed to detect texts composed by humans as AI-generated. 
Though the random seed used for generating watermarked text is private \citep{kirchenbauer2023watermark}, we develop an attack that smartly queries the target LLM multiple times to learn its watermarking scheme. An {\it adversarial human} can then use this information to compose texts that are detected to be watermarked. 
Retrieval-based detectors \citep{krishna2023paraphrasing} can also be easily spoofed by registering paraphrased versions of human essays in their database. An adversarial human with the knowledge of another person's essay could prompt an LLM to paraphrase it. The output of the LLM (paraphrased version of the human essay) would be stored in its database. The original human essay will now be detected as AI-generated since retrieval-based detectors can be robust to simple paraphrasing (while they are not robust to recursive paraphrasing). For zero-shot and neural network-based detectors, we find that a na\"ive attack where different human texts are combined leads to them being classified as AI-generated text.
Figure~\ref{fig:title} illustrates vulnerabilities of existing AI-text detectors.


Identifying AI-generated text is a critical problem to avoid its misuse by users for unethical purposes such as plagiarism, generating fake news and spamming. However, deploying %vulnerable
{\it unreliable} detectors is {\it not} the right solution to tackle this issue as a detector with a high false positive rate will cause more harm than good in society.
%its own damages such as falsely accusing a human of plagiarism.
Our results highlight the sensitivities of a wide range of detectors to simple practical attacks such as paraphrasing attacks. More importantly, our results indicate the impossibility of developing reliable detectors in practical scenarios --- to maintain reliable detection performance, LLMs would have to trade off their performance. We hope that these findings can initiate an honest dialogue within the community concerning the ethical and dependable utilization of AI-generated text.\looseness=-1