\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{url}
\usepackage{algorithm,algorithmic}
\usepackage{booktabs,makecell,multirow}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{14} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Multimodal Continuous Emotion Recognition: A Technical Report for ABAW5}

\author{Su Zhang\textsuperscript{1}, Ziyuan Zhao\textsuperscript{1}, Cuntai Guan\textsuperscript{1} \\
\textsuperscript{1}Nanyang Technological University\\
{\tt\small sorazcn@gmail.com, S210088@e.ntu.edu.sg, ctguan@ntu.edu.sg}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

% \author{Su Zhang\textsuperscript{1}, Ziyuan Zhao\textsuperscript{1}, Cuntai Guan\textsuperscript{1, \thanks{This work is partially supported by the RIE2020 AME Programmatic Fund, Singapore (No. A20G8b0102).}} \\
% \textsuperscript{1}Nanyang Technological University\\
% {\tt\small sorazcn@gmail.com, S210088@e.ntu.edu.sg, ctguan@ntu.edu.sg}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
We used two multimodal models for continuous valence-arousal recognition using visual, audio, and linguistic information. The first model is the same as we used in ABAW2 and ABAW3, which employs the leader-follower attention. The second model has the same architecture for spatial and temporal encoding. As for the fusion block, it employs a compact and straightforward channel attention, borrowed from the End2You toolkit. Unlike our previous attempts that use Vggish feature directly as the audio feature, this time we feed the pre-trained VGG model using logmel-spectrogram and finetune it during the training. To make full use of the data and alleviate over-fitting, cross-validation is carried out. The fold with the highest concordance correlation coefficient is selected for submission. The code is available at https://github.com/sucv/ABAW5.
\end{abstract}


\section{Introduction}
Emotion recognition is the process of identifying human emotion. It plays a crucial role in behavioral modeling, human-computer interaction, and affective computing. By using the dimensional model \cite{sandbach2012static}, any emotional state can be taken as a point located in a continuous space, with the valence and arousal being the axes. Continuous emotion recognition seeks to map the $N$ sequential data points into $M$ sequential emotional state points, where $M$ usually equals $N$. This report details our methodology for the valence-arousal estimation challenge from the fifth affective behavior analysis in-the-wild (ABAW5) workshop \cite{kollias2022abaw, kollias2021analysing, kollias2020analysing, kollias2021distribution, kollias2021affect, kollias2019expression, kollias2019face, kollias2019deep, zafeiriou2017aff}. ABAW5 aims for the affective behavior analysis in-the-wild. The valence-arousal estimation sub-challenge bases on the extended version of the Aff-Wild2 database, in which the valence and arousal are annotated continuously for each video frame.

Our work is an extension of the last year's attempt \cite{zhang2022continuous} on ABAW5 \cite{kollias2021analysing}. The extension are twofold. First, a pre-trained audio VGGnet is employed to serve as the audio backbone for the vggish \cite{hershey2017cnn} extraction on-the-go. Thus, the expressiveness of deep feature on visual and audio modalities can be further improved through finetuning. Second, a compact and straightforward fusion block is borrowed from the End2You toolkit, in which the channel attention is employed to fuse the multimodal information. The results of using leader-follower attention and channel attention are reported.

%\cite{shrout1979intraclass}.

% % Component models of emotion can inform the quest for emotiona competence

% \begin{figure*}[t]
% \centering
% \includegraphics[width=\textwidth]{architecture.jpg}
% \caption{The architecture of our proposed model. The model consists of four components, i.e., the visual, audio, linguistic, and co-attention blocks. The visual block has a cascaded 2DCNN-TCN structure, and the audio and linguistic blocks each contain a TCN. The three branches yield three independent spatial-temporal feature vectors. They are then fed to the attentive fusion block. Three independent attention encoders are used. For the $i$-th branch, its encoder consists of three independent linear layers, they adjust the dimension of the feature vector producing a query $\mathbf{Q}_i$, a key $\mathbf{K}_i$, and a value $\mathbf{V}_i$. They are then regrouped and concatenated to form the cross-modal counterparts. For example, the cross-modal query $\mathbf{Q}=[\mathbf{Q}_1,\mathbf{Q}_2,\mathbf{Q}_3]$. An attention score is obtained by Eq. \ref{eq:att}.}\label{fig:arc}
% \end{figure*}

The remainder of the paper is arranged as follows. Section \ref{sec:model_architecture} details the model architecture including the visual, aural, linguistic, and attentive fusion blocks. Section \ref{sec:implementation_details} elaborates the implementation details including the data pre-processing, training settings, and post-processing. Section \ref{sec:result} provides the continuous emotion recognition results. Section \ref{sec:conclusion} concludes the work.


\section{Model Architecture}
\label{sec:model_architecture}
There are two architectures for our model, the leader-follower attention network (LFAN) and channel attention network (CAN). Both of them consists of independent branches to extract the spatiotemporal encoding for each modality, followed by the fusion block using leader-follower attention and channel attention, respectively. For each branch, a CNN backbone is employed to extract the spatial deep feature on-the-go for video frames and logmel-spectrogram, followed by a temporal convolutional network (TCN) \cite{bai2018empirical} to further learn the spatiotemporal encoding. For the linguistic branch, the pre-extracted bert features \cite{radford2019language} are taken as the input to feed the TCN. No linguistic backbone is employed as the speech tokens require post-processing to align onto the frame-wise valence-arousal annotation. For branches fed by low-level features, no backbone is employed as well. Please refer to our previous paper \cite{zhang2022continuous} and End2You paper \cite{tzirakis2018end2you} (\href{https://github.com/end2you/end2you/blob/master/end2you/models/multimodal/fusion/fusion_layer.py}{code}) for the details of LFAN and CAN.

\section{Implementation Details}
\label{sec:implementation_details}
\subsection{Database}
The ABAW3 competition uses the Aff-Wild2 database. The corpora of the valence-arousal estimation sub-challenge includes $564$ trials. The database is split into the training, validation and test sets. The partitioning is done in a subject independent manner so that any subject's data are included in only one partition. The partitioning produces $341$, $71$, and $152$ trials for the training, validation, and test sets. Four experts annotate the videos using the method proposed in \cite{cowie2000feeltrace}. In addition to the annotations (for the training and validation sets only) and the raw videos, the bounding boxes and landmarks for each raw video are also available to the participants.


\subsection{Preprocessing}
The visual preprocessing is carried out as follows. The cropped-aligned image data provided by the organizer are used. All the images are resized to $48\times 48\times 3$ as a compromise to limited computational power. Given a trial from the training or validation set, the length $N$ is determined by the number of the rows of the annotation text file which does not include $-5$. For the test set, the length $N$ is determined by the frame number of the raw video. A zero matrix $\mathbf{B}$ of size $N\times 48\times 48\times 3$ is initialized and then iterated over the rows. For the $i$-th row of $\mathbf{B}$, it is assigned as the $i$-th jpg image if it exists, otherwise doing nothing. 

\begin{table*}[ht]
\centering
\caption{The CCC results from the 6-fold cross-validation on the validation and test sets. Fold 0 is exactly the original data partitioning provided by ABAW5. $-$ denote instances without results.}\label{table:result}
\setlength{\tabcolsep}{4mm}{
\begin{tabular}{ccccccccc}
\toprule
 
%\multirow{2}*{trajectory\\prediction}&{prediction\\Horizon(s)}&{C\\V} \\
\makecell[c]{Emotion} &\makecell[c]{Partition}&\makecell[c]{Method}&\makecell[c]{ Fold 0}& \makecell[c] {{\textbf{Fold 1}}}  &\makecell[c]{Fold 2} &\makecell[c]{Fold 3} &\makecell[c]{Fold 4} &\makecell[c]{ Fold 5}       \\
 
\midrule
%7&    5&3&2&1&2&1&3\\
%6&1&8&1&3&4&5&8\\
\multirowcell{4}{Valence}&\multirowcell{2}{Validation}&LFAN&$0.441$&$-$&$-$&$-$&$-$&$-$\\
&&CAN&$0.423$&$\mathbf{0.612}$&$0.484$&$0.529$&$0.543$&$0.488$\\
\cmidrule(r){3-9}
&\multirowcell{2}{Test}&LFAN&$-$&$-$&$-$&$-$&$-$&$-$\\
&&CAN&$-$&$-$&$-$&$-$&$-$&$-$\\
&&Baseline&$0.200$&$-$&$-$&$-$&$-$&$-$\\
\midrule
\multirowcell{4}{Arousal}&\multirowcell{2}{Validation}&LFAN&$0.645$&$-$&$-$&$-$&$-$&$-$\\
&&CAN&$0.670$&$\mathbf{0.680}$&$0.592$&$0.618$&$0.655$&$0.610$\\
\cmidrule(r){3-9}
&\multirowcell{2}{Test}&LFAN&$-$&$-$&$-$&$-$&$-$&$-$\\
&&CAN&$-$&$-$&$-$&$-$&$-$&$-$\\
&&Baseline&$0.240$&$-$&$-$&$-$&$-$&$-$\\

\bottomrule
\end{tabular}}
\end{table*}

The audio preprocessing firstly converts all the videos to mono with a $16K$ sampling rate in wav format. After which, the low-level EGEMAPS feature is extracted using the OpenSmile toolkit \cite{eyben2010opensmile}, the logmel-spectrogram is extracted following the Vggish preprocessing code \footnote{https://github.com/harritaylor/torchvggish}. Note that the hop length for these features are set to be $1/frame rate$ of the raw video, in order to synchronize with other modalities and annotations.  

The linguistic preprocessing is carried out as follows. The mono wav file obtained from the audio preprocessing is fed to a pretrained speech recognition model from the Vosk toolkit\footnote{https://alphacephei.com/vosk/models/vosk-model-en-us-0.22.zip}, from which the recognized words and the word-level timestamp are obtained. The recognized words are then fed to a pretrained punctuation and capitalization model from the deepmultilingualpunctuation toolkit\footnote{https://pypi.org/project/deepmultilingualpunctuation/}. After which a pretrained BERT model from the Pytorch library is employed to extract the word-level linguistic features. The linguistic features are obtained by summing together the last four layers of the BERT model \cite{sun2020multi}. To synchronize, the word-level linguistic features are populated according to the timestamp of each word and each frame. Specifically, a word usually has a larger time span than that for a frame. Therefore, for one word, its feature is repetitively assigned to the time steps of all the frames within the time span.

For the valence-arousal labels, all the rows containing $-5$ are excluded. To ensure that the features and annotations have the same length, the feature matrices are either repeatedly padded (using the last feature points) or trimmed, depending on whether the feature length is shorter and longer than the trial length, respectively.

\subsection{Data Expansion}

The AffWild2 database employed by ABAW5 contains $360$, $72$, and $162$ trials in the training, validation, and testing sets, respectively. To make full use of the available data and alleviate over-fitting, 6-fold cross-validation is employed. By evenly splitting the training set into $5$ folds, we have $6\times 72$ trials in total. Note that the $0$-th fold is exactly the original data partitioning. And there is no subject overlap across different folds. Moreover, during training and validation, the resampling window has a $33\%$ overlap, resulting in $33\%$ more data. 

\subsection{Training}
The batch size is $12$. For each batch, the resampling window length and hop length are $300$ and $200$, respectively. I.e., the dataloader loads consecutive $300$ feature points to form a minibatch, with a stride of $200$. For any trials having feature points smaller than the window length, zero padding is employed. For visual data, the random flip, random crop with a size of $40$ are employed for training and only the center crop is employed for validation. The data are then normalized so that $mean=std=0.5$. For EGEMAPS and bert features, they are normalized so that $mean=0$ and $std=1$.

The CCC loss is used as the loss function. The Adam optimizer with a weight decay of $0.001$ is employed. The maximal epoch number and early stopping counter are set to $100$ and $20$, respectively. The learning rate and minimal learning rate are set to $1e-5$ and $1e-8$, respectively. 

The warmup scheme with \textit{ReduceLROnPlateau} scheduler with a patience of $5$ and factor of $0.1$ is employed based on the validation CCC. Three groups of layers for the visual Resnet50 backbone and audio VGG backbone are manually selected for further fine-tuning. When epoch$=0$, the first group is unfrozen. The learning rate then linearly warmed up to $1e-5$. After which the \textit{ReduceLROnPlateau} takes over to update the learning rate. When no higher validation CCC appears for a consecutive 5 epochs, the learning rate would drop to $1e-6$ and so far so forth. When the learning rate is already $1e-8$ and no higher validation CCC appears for the latest 5 epochs, the second group is unfrozen, the learning rate is also reset to undergo the warm-up scheme and \textit{ReduceLROnPlateau} again. The procedure is repeated until no groups are available to unfrozen. Note that at the end of each epoch, the current best model state dictionary (i.e., the one with the greatest validation CCC) is loaded. By doing so, when there is no improvement on validation CCC, the training CCC cannot keep increasing and ends up with over-fitting. 

Note that unlike our previous attempts on ABAW2 and ABAW3, no post-processing is employed. Given the results from 6 folds, we simply choose the ones with the highest validation CCC to submit. Also note that the results for any fold are selected from $15$ instances using different seeds.



%



\section{Result}
\label{sec:result}

The current experiment results are reported in Table \ref{table:result}. 

%The best results from all the teams are reported in Table \ref{table:all}.


% \begin{table}[ht]
% \centering
% \caption{The overall test results in CCC. The bold fonts indicate the best results. Citations for several teams are not available from Google Scholar by the time when this report was drafted. }\label{table:all}
% \vspace{0.25cm}
% \begin{tabular}{|p{3cm}|l|l|l|}
% \hline
% Method                     & Valence & Arousal & Mean  \\ \hline
% Situ-RUCAIM3 \cite{meng2022multi} & \textbf{0.606} & 0.596 & \textbf{0.601}\\ \hline
% Ours  & 0.520 & \textbf{0.602} & 0.561 \\ \hline
% PRL \cite{nguyen2022ensemble} & 0.450 & 0.445 & 0.448 \\ \hline
% HSE-NN  & 0.417 & 0.454 & 0.436 \\ \hline
% AU-NO  & 0.418 & 0.407 & 0.413 \\ \hline
% LIVIA-2022  & 0.374 & 0.363 & 0.369 \\ \hline
% Netease Fuxi Virtual Human \cite{zhang2022transformer} & 0.300 & 0.244 & 0.272 \\ \hline
% Baseline \cite{kollias2022abaw}                  & 0.180             & 0.170             & 0.175  \\ \hline 
% \end{tabular}
% \end{table}

% First and foremost, the multimodal model achieves great improvement against the unimodal counterpart, which is up to $70.41\%$ and $58.42\%$ gain (by comparing UM against MM in table \ref{table:overall2}) over the valence and arousal, respectively. The employment of cross-validation provides incremental improvement on the multimodal result. 

% We can also see that the three unimodal scenarios and multimodal scenarios have a sharp performance gap, whereas on the validation set the gap is incremental. We hypothesize that the unimodal models, fed only by visual information, suffer from over-fitting and insufficient robustness on the test set. The issue is alleviated by the fusion with aural information. Further investigation will be carried out in our future work using other audio-visual databases where labels of the test set are available.


\section{Conclusion}
We used two multimodal models for continuous valence-arousal recognition using visual, audio, and linguistic information. 

% The model employs three parallel blocks to learn the feature from the three modalities. The learned features are then fused by the cross-modal co-attention block. Experiments are conducted on the Aff-Wild2 database. The achieved CCC on the test set is $0.520$ for valence and $0.602$ for arousal, which significantly outperforms the baseline method with the corresponding CCC of $0.180$ and $0.170$ for valence and arousal, respectively.

\label{sec:conclusion}
\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}
