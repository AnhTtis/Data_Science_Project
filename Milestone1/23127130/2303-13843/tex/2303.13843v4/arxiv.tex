% $Id: template.tex 11 2007-04-03 22:25:53Z jpeltier $

\documentclass{vgtc}                          % final (conference style)
% \documentclass[review]{vgtc}                 % review
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint]{vgtc}               % preprint
%\documentclass[electronic]{vgtc}             % electronic version

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Figures should be in CMYK or Grey scale format, otherwise, colour 
%% shifting may occur during the printing process.

%% it is recomended to use ``\cref{sec:bla}'' instead of ``Fig.~\ref{sec:bla}''
\graphicspath{{figures/}{pictures/}{images/}{./}} % where to search for the images

\usepackage{times}                     % we use Times as the main font
\renewcommand*\ttdefault{txtt}         % a nicer typewriter font

%% Only used in the template examples. You can remove these lines.
\usepackage{tabu}                      % only used for the table example
\usepackage{booktabs}                  % only used for the table example
\usepackage{lipsum}                    % used to generate placeholder text
\usepackage{mwe}                       % used to generate placeholder figures
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[percent]{overpic}
\usepackage{amsmath}
\usepackage{boldline}
\usepackage{bbding}
\usepackage{newfloat}
\usepackage{listings}
%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.
\usepackage{mathptmx}                  % use matching math font


%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{1222}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}

%% allow for this line if you want the electronic option to work properly
\vgtcinsertpkg

%% In preprint mode you may define your own headline. If not, the default IEEE copyright message will appear in preprint mode.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

%% This adds a link to the version of the paper on IEEEXplore
%% Uncomment this line when you produce a preprint version of the article 
%% after the article receives a DOI for the paper from IEEE
%\ieeedoi{xx.xxxx/TVCG.201x.xxxxxxx}


%% Paper title.

\title{CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D
Scene Layout}

%% This is how authors are specified in the conference style

%% Author and Affiliation (single author).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}}
%%\affiliation{\scriptsize Allied Widgets Research}

%% Author and Affiliation (multiple authors with single affiliations).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com} %
%%\and Ed Grimley\thanks{e-mail:ed.grimley@aol.com} %
%%\and Martha Stewart\thanks{e-mail:martha.stewart@marthastewart.com}}
%%\affiliation{\scriptsize Martha Stewart Enterprises \\ Microsoft Research}

%% Author and Affiliation (multiple authors with multiple affiliations)
\author{Haotian Bai\thanks{e-mail: haotianwhite@outlook.com}\\ %
        \scriptsize HKUST(GZ) %
\and Yuanhuiyi Lyu\thanks{e-mail: yuanhuiyilv@hkust-gz.edu.cn}\\ %
     \scriptsize HKUST(GZ) %
\and Lutao Jiang\thanks{e-mail:  jianglutao98@gmail.com}\\ %
     \scriptsize HKUST(GZ) %
% \and Sijia Li, Haonan Lu, Xiaodong Lin\thanks{e-mail: \{lisijia, luhaonan, linxiaodong\}@oppo.com}\\ %
\and Sijia Li\\
\scriptsize OPPO\\
\and Haonan Lu\\
\scriptsize OPPO\\
\and Xiaodong Lin\\
\scriptsize OPPO\\
\and Lin Wang\thanks{Corresponding author, email:  linwang@ust.hk}\\ %
     \parbox{1.4in}{\scriptsize \centering HKUST(GZ) \\ HKUST}}
%% A teaser figure can be included as follows
\teaser{
  \centering
      \vspace{-12pt}
  {Project homepage: \url{https://vlis2022.github.io/componerf}}
\includegraphics[width=0.9\linewidth]{figures/teaser.pdf}
  \caption{We introduce CompoNeRF, a novel framework that synthesizes coherent multi-object scenes by integrating textual descriptions and layouts. CompoNeRF allows for individual NeRFs, each denoted by a unique prompt color, to be \underline{composed}, \underline{decomposed}, and \underline{recomposed} with ease, streamlining the construction of complex scenes from cached models after decomposition. \textbf{(a)} displays the composed results. \textbf{(b), (c), (d), (e)} are recomposition results after manipulation demos shown above, including duplication, transformation, loading decomposed NeRFs, and semantic editing conducted separately. }
  \label{fig:teaser}
}

%% Abstract section.
\abstract{
Text-to-3D form plays
a crucial role in creating editable 3D scenes for AR/VR.  
Recent advances have shown promise in merging neural radiance fields (NeRFs) with pre-trained diffusion models for text-to-3D object generation. 
However, one enduring challenge is their inadequate capability to accurately parse and regenerate consistent \textbf{multi-object} environments.
Specifically, these models encounter difficulties in accurately representing quantity and style prompted by multi-object texts, often resulting in a collapse of the rendering fidelity that fails to match the semantic intricacies. Moreover, amalgamating these elements into a coherent 3D scene is a substantial challenge, stemming from generic distribution inherent in diffusion models.
%
To tackle the issue of 'guidance collapse' and further enhance scene consistency, we propose a novel framework, dubbed \textbf{CompoNeRF}, 
by integrating an editable 3D scene layout with object-specific and scene-wide guidance mechanisms.
It initiates by interpreting a complex text into the layout populated with multiple NeRFs, each paired with a corresponding subtext prompt for precise object depiction. 
%
Next, a tailored composition module seamlessly blends these NeRFs, promoting consistency, while the dual-level text guidance reduces ambiguity and boosts accuracy. 
%
Noticeably, our composition design permits decomposition. 
% , facilitating their reconfiguration.
This enables flexible scene editing and recomposition into new scenes based on the edited layout or text prompts. 
Utilizing the open-source Stable Diffusion model, CompoNeRF generates multi-object scenes with high fidelity.  Remarkably, our framework achieves up to a \textbf{54\%} improvement by the multi-view CLIP score metric.  
%
Our user study indicates that our method has significantly improved semantic accuracy, multi-view consistency, and individual recognizability for multi-object scene generation.
% Code is available at \url{https://github.com/hbai98/Componerf}.

} % end of abstract

%% Keywords that describe your work. Will show as 'Index Terms' in journal
%% please capitalize first letter and insert punctuation after last keyword.
\keywords{Multimodel capturing and reconstruction, Text-to-3D, Neural Radiance Field. }

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
% \nocopyrightspace

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}

\maketitle
Consider the last time you wanted to relive an imaginary virtual scene through VR/AR devices. As illustrated in \cref{fig:teaser}, picture a tranquil bedroom where the day comes to a serene close. Envision a bed adorned with soft pillows, a bedside table, and a lamp casting a soothing light. Our mental imagery often includes a variety of objects and their interplay. How do we then create these scenes from descriptions, such as text prompts, and convert them into cohesive, editable environments within virtual reality?

%% \section{Introduction} %for journal use above \firstsection{..} instead
Recent advances in text-to-image generation have been driven by the integration of vision-language pre-trained models~\cite{radford2021learning,li2022blip} with diffusion processes~\cite{ho2020denoising,nichol2021improved,rombach2022high}, leading to impressive outcomes. Pioneering text-to-3D approaches~\cite{jain2022zero,sanghi2022clip,hong2022avatarclip,JunGao2022GET3DAG,mohammad2022clip,lee2022understanding,xu2022dream3d} have built upon these successes, employing these robust vision-language models to enrich 3D generative models with the structured understanding provided by Neural Radiance Fields (NeRFs)~\cite{mildenhall2020nerf,mip-nerf,muller2022instant}. This synergy~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} facilitates the creation of 3D models which, when rendered from different views, cohere with the learned text-to-image diffusion model distribution, opening new avenues for the direct synthesis of 3D content from textual descriptions. 

% Text-to-image generation has achieved tremendous success
% Text-to-image generation has recently made significant strides, 
% coupling the vision-language pre-trained models~\cite{radford2021learning,li2022blip} with advanced diffusion techniques~\cite{ho2020denoising,nichol2021improved,rombach2022high} to achieve remarkable results.
% % These breakthroughs have also yielded far-reaching implications in text-to-3D generation
% Pioneering methods~\cite{jain2022zero,sanghi2022clip,hong2022avatarclip,JunGao2022GET3DAG,mohammad2022clip,lee2022understanding,xu2022dream3d} in text-to-3D leverages the prowess of pre-trained vision-language models inform 3D generative model utilizing NeRFs representations~\cite{mildenhall2020nerf,mip-nerf,muller2022instant}. 
% %
% This synergy ~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} enables the generation of 3D models that, when rendered from different perspectives, align seamlessly with the distribution learned from text-to-image diffusion models, thereby unlocking new potentials in synthesizing 3D content directly from text prompts. 
% using powerful vision-language pre-trained models. 
% More recently, several text-to-3D methods~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} have shown that matching the rendered views from the differential 3D model, such as Neural Radiance Fields (NeRFs)~\cite{mildenhall2020nerf,mip-nerf,muller2022instant}, with the learned text-to-image distribution from pre-trained diffusion model can produce remarkable results.

% The rapid development of text-image models~\cite{radford2021learning,li2022blip} has yielded far-reaching implications in text-guided image generation~\cite{ramesh2021zero,patashnik2021styleclip,ramesh2022hierarchical,saharia2022photorealistic,rombach2022high}.
% % models~\cite{ramesh2022hierarchical,saharia2022photorealistic,rombach2022high}.
% Notably,  recently text-guided image generation has achieved tremendous success when coupled with diffusion models~\cite{ho2020denoising,nichol2021improved,rombach2022high}.
% Text-guided image generation has recently gained much attention and success in both academia and industry,  
 % achieved tremendous success
% These breakthroughs have also accelerated the development of text-guided 3D generation ~\cite{jain2022zero,sanghi2022clip,hong2022avatarclip,JunGao2022GET3DAG,mohammad2022clip,lee2022understanding,xu2022dream3d} using powerful pre-trained text-image models. 
% More recently, 
% % text-conditional 3D generation methods have
% it has been shown that the diffusion model can serve as a powerful critic to optimize the underlying differential 3D representations~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score}, such as Neural Radiance Fields (NeRFs)~\cite{mildenhall2020nerf} for text-to-3D generation.
\begin{figure}[t]
    \centering
    % \vspace{-2pt}
    \includegraphics[width=\linewidth]{figures/intro.pdf}
    \vspace{-15pt}
    \caption{
    % Comparison of our CompoNeRF with Latent-NeRF~\cite{mirzaei2022laterf} and SJC~\cite{wang2022score}.
    % It shows that using different level guidance in multi-object text can address the unconstrained generation of Stable Diffusion~\cite{rombach2022high}.
    \textbf{The guidance collapse issue}.  
\textbf{(a)} Generation of the multi-object scene involves utilizing the frozen Stable Diffusion. \textbf{(b)} Instances of guidance collapse are observed when using the global text directly. \textbf{(c)} Comparison of rendering results . 
    }
    % and \textit{guidance collapse} issue on generating a simple multi-object scene, ``a red apple and a yellow banana".
    % unconstrained generation problem associated with Stable Diffusion~\cite{rombach2022high} in multi-object scenarios.
    % The unconstrained generation in Stable Diffusion.
    % The unconstrained generation in Stable Diffusion.
    %  poses a challenge, particularly for sentences that deßscribe multiple objects.
    % % Stable Diffusion struggles to distinguish the context and quantity of rendering targets in 2D, which can mislead Text-to-3D generation methods. 
    % In contrast, by providing guidance from both global (scene) and local (object) levels, our object-aware modeling approach can produce more accurate and faithful scenes
    \label{fig:intro}
    % \vspace{-14pt}
\end{figure}
Textual descriptions can be vague and open to interpretation. Converting these prompts into visual images, particularly for complex scenes with numerous objects, is not a simple task. 
Specifically, diffusion models like Stable Diffusion~\cite{rombach2022high} have undergone extensive training using large-scale text-image datasets~\cite{schuhmann2022laion}. Despite this, they frequently encounter difficulties when dealing with multiple object texts, particularly when those objects have a limited presence in the training data. 
%
As a result, this process often results in images that either leave out certain objects or depict them incorrectly. \cref{fig:intro}(a) demonstrates how even in a simple scenario involving just two objects, Stable Diffusion may not consistently preserve the integrity of the scene.
%
In detail, it might overlook an apple or banana or render them with inaccurate colors.
%
Specifically, it may neglect to include an apple or banana, or it might portray them in incorrect colors. This problem, referred to as 'guidance collapse' as illustrated in (b), is especially problematic when rendering scenes involving multiple objects based on text prompts. As shown in (c), even advanced models like Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} struggle to accurately generate the configurations described in texts involving multiple objects. This limitation significantly hampers their capability to construct 3D scenes derived from descriptive prompts.

% However, the textual description is often an abstract specification for a desired target 3D model or a 2D image.
% Despite that the powerful diffusion models, \eg, Stable Diffusion~\cite{rombach2022high}, have been trained on billions of text-image pairs~\cite{schuhmann2022laion}, it is still a challenge to generate geometrically coherent images across different viewpoints from the text.
% Unfortunately, the diffusion model may produce inaccurate results~\cite{feng2022training} given text containing multiple objects, resulting in missing objects or semantic confusion.
% For example, Fig.~\ref{fig:intro} demonstrates that Stable Diffusion fails to maintain object identities and geometric coherence even with a simple multi-object text.
% It obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash \textit{guidance collapse}, especially when rendering multi-object scenes with text prompts.
% As a result, state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models can only generate part of concepts in the multi-object text shown in Fig.~\ref{fig:intro}, limiting their application for object-compositional 3D scene generation from text prompts.



% However, the learned 2D generative distribution in pre-trained diffusion model 
% Despite they have , the 2D supervision unconstrained generative capacity of the diffusion model is still considered as the major challenging issue towards comprehensive text-to-3D generation.
% across different viewpoints, given the input text prompt.
% may produce inaccurate results~\cite{feng2022training}, resulting in missing objects or semantic confusion.
% This obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash guidance collapse, especially when rendering complex scenes from multi-object texts.
% For example, Fig.~\ref{fig:intro} demonstrates that the state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models fail to maintain the correctness of object identities and the geometric coherence when presented with the simple multi-object text.
% The reason is that the text-to-image generative ability in the diffusion model remains inherently unconstrained in the 3D space. 

% Despite the impressive text-to-3D results achieved by the diffusion model, the diffusion model may produce inaccurate results~\cite{feng2022training}, resulting in missing objects or semantic confusion.
% This obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash guidance collapse, especially when rendering complex scenes from multi-object texts.
% For example, Fig.~\ref{fig:intro} demonstrates that the state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models fail to maintain the correctness of object identities and the geometric coherence when presented with the simple multi-object text.
% Although these text-to-3D models have shown , the diffusion model may produce inaccurate results~\cite{feng2022training}, resulting in missing objects or semantic confusion.
% This obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash guidance collapse, especially when rendering complex scenes from multi-object texts.
% For example, Fig.~\ref{fig:intro} demonstrates that the state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models fail to maintain the correctness of object identities and the geometric coherence when presented with the simple multi-object text.
% 
% Moreover, when given text that includes multiple objects, the diffusion model may produce inaccurate results~\cite{feng2022training}, resulting in the missing object or semantic confusion, leading to guidance collapse.
% Although text-to-3D models have shown impressive results, a hurdle for them is that they are hindered by the guidance collapse  
% An example in 
% The reason is that the text-to-image generative ability in the diffusion model remains inherently unconstrained in the 3D space. 


% Therefore, it is natural to ask whether the multi-object text guidance between 3D representations and diffusion models can be better constrained for multi-object generation.
% A solution is to make the 3D representation and rendering network object-aware by binding object-related information to particular locations in the 3D space.
% However, existing approaches~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to encode the entire scene into a single neural network, which can successfully handle object-centric scenes but is generally agnostic to the object's identity.
% This leads to an intriguing inquiry: Can the generic distribution learned by diffusion models accurately encapsulate and reconstruct the nuanced elements of a text describing multiple objects for 3D scene creation? Our observations, as depicted in Figure~\ref{fig:intro}, reveal that diffusion models more reliably render individual objects when provided with localized text prompts for each. Inspired by this, we propose the implementation of granular textual guidance as a solution to the prevalent ‘guidance collapse’ encountered in current models~\cite{metzer2022latent,wang2022score} with multi-object descriptions. 
% The logical approach is to apply targeted guidance for each object individually, enhancing object-specific awareness within the 3D modeling and rendering processes. Nevertheless, the prevailing methods~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to rely on a singular neural network to process the entire scene holistically, which poses significant challenges in assimilating segregated guidance for multiple objects during training due to their overarching scene-centric nature.

% Therefore, it naturally raises the question:  \textit{whether the agnostic distribution of the diffusion model can accurately learn and compose all the concepts in a multi-object text for 3D scene generation.}

This raises a compelling question: \textit{Can we design a model using diffusion models armed with a generic distribution, not only identify and recreate individual elements in multi-object texts but also amalgamate them into a coherent 3D scene?}
%
% This motivates us to introduce more fine-grained text guidance to tackle the guidance collapse issue in existing frameworks~\cite{metzer2022latent,wang2022score} when using multi-object text prompts.
%
% Thus, a straightforward solution is to bind object-oriented guidance to each object, making the 3D representation and rendering pipeline object-aware.
% Existing approaches~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to use a single neural network to encode the entire scene, making it hard to incorporate the decomposed guidance during training as they are generally agnostic to the object's identity and quantity.



% we introdce a compositional NeRF framework, called \textbf{CompoNeRF}, interpreting multi-object text guidance as editable 3D scene boxes with fine-grained text prompts.
% The scene layout collects individual object entities from the input text and gathers their editable 3D bounding boxes as shown in Fig.~\ref{fig:teaser}.
% % As shown in Fig.~\ref{fig:teaser}, the scene layout collects individual object entities from the input text, input 3D bounding boxes, and their corresponding coordinates and scales.
% In CompoNeRF, each box in the 3D scene layout is modeled by a local NeRF for representation learning, and global views are rendered by compositing the learned 3D representations from local NeRFs.
% However, direct composition from all local NeRFs may not ensure coherent global views without addressing the following two issues.
% Nevertheless, the direct composition from all local NeRFs may not ensure the coherent global views, because it does not tackle the following two key issues.
% directly from the composited local views as 

% The first obstacle in achieving global view consistency across multiple local NeRFs is that they are optimized independently, making it difficult to learn their global spatial or semantic correlations. 
% To address this,
% \textbf{1) Independence}: capturing consistent global views across multiple local NeRFs is hard due to non-shared parameters.
% To solve this, we use a global MLP to calibrate the local NeRF predictions based on their samples' global coordinates and ray directions.
% The model can gradually learn global consistency across multiple objects by passing the global constraint into local NeRFs.

% \textbf{2) Occlusion}: inaccurate text guidance may result from fully occluded objects due to random camera positions in the training data.
% To tackle the occlusion issue, we apply local text guidance to each individual NeRF, rendering locally and utilizing the diffusion model to provide more precise guidance for object identification despite occlusions.
% To tackle the occlusion issue, for each local NeRF, we separately apply local text guidance on the locally rendered view, as the diffusion model can provide more accurate guidance to shape the object identity despite occlusions.
% To tackle the occlusion issue, we employ local text guidance on every individual NeRF-rendered view. Thanks to the use of the diffusion model, we are able to provide precise guidance for object identification even in the presence of occlusion.
% % As a result, our approach ensures the global consistency, generating realistic 3D content, even in crowded scenes, 
% % as demonstrated in Figure~\ref{fig:teaser}, without missing objects or ambiguity.
% As a result, our approach ensures coherent and realistic 3D generation, even in crowded scenes that include multiple objects as depicted in Fig.~\ref{fig:teaser} without object missing or text ambiguity.


% The process is initiated by extracting individual objects from textual input and positioning them within modifiable 3D bounding boxes. Each bounding box in CompoNeRF is not only adjustable but also anchored by a distinct local NeRF that facilitates the learning of object representations. The global scene is then rendered by strategically merging these localized models with our tailored composition module.
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/compo.pdf}
    \vspace{-10pt}
    % \caption{Ablation study on text guidance. (a) without local SDS losses. (b) without global SDS losses. (c) vanilla SDS losses without perturb and average scoring~\cite{wang2022score}. (d) full model.}
    \caption{\textbf{(a)} CompoNeRF supports cashing and loading to facilitate NeRF composition.
    \textbf{(b)} The composition module composites multiple NeRFs for coherent scenes. Its enhanced effect is accentuated by the red boxes, showcasing superior scene coherency. 
    }
    \label{fig:compo_intro}
    % \vspace{-18pt}
\end{figure}
In this paper, we present \textbf{CompoNeRF}, a compositional NeRF framework that interprets multi-object text prompts as editable 3D scene layouts with granular text prompts.
The procedure, illustrated in \cref{fig:compo_intro}(a), begins by identifying individual objects from the textual description and positioning them within customizable 3D bounding boxes. Each box is supported by a distinct NeRF and a subtext label. Thus, as shown at \cref{fig:teaser}, CompoNeRF is designed to accommodate alterations, allowing for manipulations in the layout—like moving, scaling, or removal, as well as loading decomposed nodes and direct text edition.
As depicted in \cref{fig:compo_intro}(b), our composition module guarantees that the overall scene is not merely a static collection but an orchestrated assembly. For example, after composition, the lamp illuminates the bed, creating a lively scene with such interaction among objects.
 % The qualitative validation of CompoNeRF’s performance is presented in Table~\ref{tab:compare}, benchmarking it favorably against contemporary methods.

CompoNeRF distinguishes itself with three core capabilities: it \textit{composes} multi-object scenes from textual prompts, \textit{decomposes} by archiving each NeRF for subsequent utilization, and \textit{recomposes} by employing this curated content gallery to rapidly generate elaborate 3D scenes, thereby streamlining the 3D content creation workflow. 
%
Next, we 
% introduce a more objective evaluation method to address the lack of rigorous quantitative analysis in prior works, which often rely solely on visual quality comparisons. 
employ the averaged CLIP score~\cite{wang2022clip} on rendering views of 3D content against global text prompts, we quantitatively measure the alignment of our generated scenes with their textual prompts.
For a more comprehensive evaluation, we carry out a user study to measure \textbf{1)} composition correctness regarding both semantic and multi-view consistency; \textbf{2)} generation quality in terms of users' overall preferences \textbf{3)} recognizability for each component within the scenes. 
%
The studies demonstrate our effectiveness in producing detailed and coherent 3D scenes that accurately reflect the given text descriptions. 


% Hence, CompoNeRF stands as a comprehensive solution that significantly enhances the quality and coherence of text-driven 3D scene generation.”
% Moreover, scene editing is facilitated by the use of 3D scene layout, allowing flexible manipulation of text and objects, including scaling, movement, and removal.
% Using a vast pre-trained content gallery, we can rapidly generate desired 3D scenes with text prompts and layouts, democratizing 3D content creation.
% The comparisons of related approaches are summarized in Tab.~\ref{tab:compare}.
% Additionally, previous studies fail to include a quantitative comparison, resulting in inadequate evaluation relying solely on visual comparison. We thus propose to employ the extensively recognized CLIP score~\cite{wang2022clip} to assess the efficiency of generating 3D models from text descriptions. By doing so, we provide a more comprehensive demonstration of our proficiency in producing intricate multi-object scenes.

To encapsulate, our paper makes three key contributions:  
(\textbf{I})  
% We solve the guidance collapse issue in multi-object 3D scene generation by integrating an editable 3D layout with multiple local NeRFs to precisely associate guidance with specific objects. Additionally, all local NeRFs can be cached, edited, and reused for composing other scenes.
We address the ‘guidance collapse’ problem in creating multi-object 3D scenes. Our innovative use of editable 3D layouts coupled with multiple localized NeRFs allows for precise direction over individual object representations. Moreover, these localized NeRF models are designed to be storable and reusable, enhancing efficiency in scene composition. 
(\textbf{II}) We introduce a composition module 
% to calibrate the overall rendering and varying levels of text guidance to maintain the identity of individual entities while ensuring global coherence. 
enables fine-tuning of the rendering process and text-based guidance, ensuring both the distinctiveness of individual objects and the holistic integration within the scene.
(\textbf{III}) 
We conduct extensive evaluations of CompoNeRF’s performance in multi-object scene generation, employing both qualitative and quantitative assessment for the multi-object text-to-3D task. The rigorous testing confirms that our CompoNeRF outperforms existing models in generating multi-object scenes that closely align with textual prompts.
% We thoroughly evaluate the effectiveness of our proposed method across various multi-object scenarios, demonstrating its ability to composite and edit 3D scenes via qualitative and quantitative analysis. We propose to use the CLIP score as the metric to evaluate the similarity of the generated 3D assets to the prompt text, and our CompoNeRF achieves the best performances on various multi-object scenes.
% In addition, the editable 3D layout facilitates object editing by allowing users to modify text and manipulate objects flexibly, including moving, scaling, and duplicating. With the aid of a vast pre-trained content gallery, users can rapidly generate their desired 3D scenes using text prompts and 3D scene layouts, democratizing the process of 3D content creation. 
% we compare CompoNeRF and other editing approaches summarized in Tab.~\ref{tab:compare}.
% A solution is to make the 3D representation and rendering network object-aware by binding object-related information to particular locations in the 3D space.
% However, existing approaches~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to encode the entire scene into a single neural network, which can successfully handle object-centric scenes but is generally agnostic to the object's identity.


% To overcome these challenges,
%  we propose a novel framework, called \textbf{CompoNeRF} that allows generating and editing scenes with editable 3D layouts in a compositional way.
% Given a text prompt involving multiple objects, CompoNeRF consisting three steps to composite object (\ie, local) views:
%  1) we parse the input text into individual object entities and gather coarse 3D bounding box information for each object, as such 3D layout can be easily obtained from the input text or users;
% 2) using the generated 3D scene layout, each object entity has its own local NeRF model to produce 3D information based on object-specific text guidance; 
% 3) we construct a unified scene rendering pipeline to render the complete scene by compositing all object-specific 3D information from local NeRFs into the scene view.

% To , given a text prompt involving multiple objects, we parse the input text into individual object entities and gather coarse 3D bounding box information for each object, as such 3D layout can be easily obtained from the input text or users.

% Despite the simplicity of our framework design,
% Nevertheless, these compositional designs can not ensure learning the coherent scene (\ie, global) views directly from the composited local views if the following two issues are not tackled.
% Firstly, the scene view consistency is hard to capture across multiple local NeRFs models as the none shared parameters.
% Therefore, we further adopt a global NeRF to calibrate the local NeRF prediction using the scene coordinate as conditional input.
% With the global-level text guidance applied to the global views composited from the local views, the global consistency can be gradually learned among multiple objects.
% Secondly, some objects can be completely occluded in the scene views, caused by random camera poses during training, rendering both global and local text guidance inaccurate.
% To tackle the occlusion issue, for each local NeRF, we separately apply local text guidance on the local rendered view as the diffusion model can provide more accurate guidance to shape the object identity, as shown in Fig.~\ref{fig:intro}.
%
% %
% However, global semantics often require additional transformation and composition of individual objects. 
% For example, consider the scene in Fig.~\ref{fig:intro} containing an apple and a banana with overlapping bounding boxes. 
% When the bounding boxes of two objects overlap, they may appear merged in the final rendered image, causing ambiguity in the scene. 
% To prevent this, the global scene needs to adjust the latent expression of each object to ensure that they maintain their distinct identities. 
% Our scene representation model helps in achieving this by adjusting the object-level latent expression with the scene coordination and the full context of input text, which improves view consistency in the scene view.
% %
% We design both scene-level and object-level score distillation losses to better balance the rendering guidance within our hierarchical scene representation and rendering framework. 
% The scene-level loss helps the model to learn global semantics, such as the spatial relationship between objects and guides the model to generate a coherent scene. 
% The object-level loss enables the optimization of the representation to keep each object's identity and fits the interactions between objects based on the global semantics.

% \begin{table*}[t!]
% \centering
% \resizebox{\linewidth}{!}{ 
%     \begin{tabular}{l|c|c|c|c|c|c}
%     \toprule
%     \textbf{Methods} & \textbf{Diffusion Model} & \textbf{\textbf{3D Representation}} & \textbf{Scene Rendering} & \textbf{Input Prompt} & \textbf{Scene Editing} & \textbf{recomposition}\\  \hline
%     DreamFusion~\cite{poole2022dreamfusion} & Imagen~~\cite{sahariaphotorealistic} & Mip-NeRF 360~\cite{barron2022mip} & Object-centric & Text & T & F\\
%     Magic3D~\cite{lin2022magic3d} & eDiff-I~\cite{balaji2022ediffi} + SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-centric & Text & T & F\\
%     SJC~\cite{wang2022score} & SD~\cite{rombach2022high} & voxel radiance field & Object-centric & Text & T & F\\
%     DreamBooth3D~\cite{raj2023dreambooth3d} & DreamBooth~\cite{ruiz2023dreambooth}+DreamFusion~\cite{poole2022dreamfusion} & Mip-NeRF~\cite{JonathanTBarron2021MipNeRFAM} & Object-centric & Text+Images & T & F \\
%     Points-to-3D~\cite{yu2023points} & ControlNet~\cite{zhang2023adding}+Point-E~\cite{nichol2022point} & Instant-NGP~\cite{muller2022instant} & Object-centric & Text+Image & T & F \\
%     \hline
%     Latent-NeRF~\cite{metzer2022latent} & SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-centric & Text+Fine Shape & T & F\\
%     Ours & SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-compostional & Text+3D Coarse Boxes & T/M/S/R & T \\ \bottomrule
%     \end{tabular}
% }
% % \vspace{-7pt}
% \caption{Comparison of our method and the related works for text-to-image generation. SD denotes Stable Diffusion. For scene editing, we use T(editing object with text), M(moving object), S(scaling object), and R(removing object) for short.}
% % \vspace{-8pt}
% \label{tab:compare}
% \end{table*}




% Our results suggest that our approach provides a compelling solution for these tasks.
% \begin{itemize}
% \item We incorporate the editable 3D layout with multiple local object NeRF to accurately associate the guidance for specific structure in text-to-3D scene generation.
% \item We design a global color calibration module and different levels text guidance procedures to balance maintaining objects' identity and global coherence.
% \item We validate the proposed method under various scenarios, showing that it offers a compelling solution for compositional scene generation and flexible editing capacity.
% \end{itemize}
\section{Related Works}
\label{sec:related_works}
\noindent\textbf{Neural Rendering for 3D Modeling.}
Recent endeavors have been made to integrate 3D content with NeRF for real-time viewing and interaction in AR/VR. For instance, works such as~\cite{rojas2023rerendrealtimerenderingnerfs, li2022rtnerfrealtimeondeviceneural}have provided real-time experience using VR/AR headsets with NeRF rendering, offering immersive virtual experiences. Recent efforts~\cite{deng2022fovnerffoveatedneuralradiance} also optimize computational resources by capturing the user’s gaze for enhanced viewing and interaction.
% The advancement of NeRF has greatly improved the performance of neural renderers.
% NeRF models~\cite{nerf,lindell2021autoint,muller2022instant,Liu2020NeuralSV,mip-nerf,ref-nerf,KaiZhang2020NeRFAA} are a family of volume rendering algorithms that utilize coordinate-based MLPs to directly predict color and opacity from the 3D position and 2D viewing direction. 
% The photo-realistic synthesized views from these models have led to the widespread adoption of differential volume rendering in various applications, \eg, relighting~\cite{srinivasan2021nerv,zhang2021nerfactor}, dynamic scene reconstruction~\cite{gao2021dynamic,pumarola2021d,xian2021space,tretschk2021non}, editable scenes and avatars~\cite{liu2021neural,yang2021learning}, and surface reconstruction~\cite{azinovic2022neural,wang2021neus}.
% These methods commonly use \textit{single} MLP to encode the entire scene, which can be ambiguous \wrt specific object identities within the scene.
% Note that our method involves rendering the scene using multiple local NeRFs, and the rendering process requires compositing all the local NeRF predictions based on their spatial relationships.
The evolution of NeRF has elevated the capabilities of neural rendering. NeRF-based models~\cite{nerf,lindell2021autoint,muller2022instant,Liu2020NeuralSV,mip-nerf,ref-nerf, Bai2023DynamicPF} have redefined volume rendering~\cite{kajiya1984ray} through the use of coordinate-based MLPs that infer color and density from spatial and directional inputs. Their capacity to produce photo-realistic views has cemented differential volume rendering as a key component in a variety of applications, such as scene relighting~\cite{srinivasan2021nerv,zhang2021nerfactor}, dynamic scene reconstruction~\cite{gao2021dynamic,pumarola2021d,xian2021space,tretschk2021non}, and scene and avatar editing~\cite{liu2021neural,yang2021learning}, as well as surface reconstruction~\cite{azinovic2022neural, jiang2023sdf,wang2021neus}.
%
Typically, these approaches rely on a \textit{single} MLP to encode an entire scene, which may introduce ambiguity in differentiating between objects. Our method, in contrast, renders scenes through a composite of multiple NeRFs, each responsible for a distinct part of the scene. This composition takes into consideration the interactions between NeRFs, allowing for the existence of individual objects while maintaining overall coherence.

% The resulting synthesized views exhibit photo-realistic quality, which has led to the widespread adoption of differential volume rendering in many applications, such as object and scene relighting~\cite{srinivasan2021nerv,zhang2021nerfactor}, 
% % unbounded scenes~\cite{KaiZhang2020NeRFAA,barron2022mip}, 
% dynamic scene reconstruction from videos~\cite{gao2021dynamic,pumarola2021d,xian2021space,tretschk2021non}, editable scenes, avatars~\cite{liu2021neural,yang2021learning}, and object surface reconstruction~\cite{azinovic2022neural,wang2021neus}.
% Differently, our framework improves the compositional capability of scene representation by employing multiple implicit representations to model individual objects separately. 
% In this work, we use Instant-npg~\cite{muller2022instant} as basic NeRF representation due to its fast training and inference speed.
% Given some object representations gallery, OSFs~\cite{guo2020object} introduces a volumetric path tracing procedure to enable rendering scenes with moving objects and lights without retraining.
% Several works~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe,ost2021neural,xu2022discoscene,song2022towards} attempt to perform compositional scene modeling by directly decomposing object representations from the scene image. These methods can be broadly categorized as either semantic-based~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe} or 3D layout-based~\cite{ost2021neural,xu2022discoscene,song2022towards}, depending on the type of additional object-level information utilized. 
% semantic based and 3D layout based.
% However, obtaining well-trained object representation is impractical for the real-world application as in most cases only the whole scene images are collected. 
% Therefore, several works focus on decomposing the object representations from the scene image directly using various object-level information, which can be roughly categorized as semantic-based~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe} and 3D layout based~\cite{ost2021neural,xu2022discoscene,song2022towards}.
% For example, instance masks can guide the object decomposition process for editable scene rendering~\cite{yang2021learning} and surface reconstruction~\cite{wu2022object}.
% For example, SemanticNeRF~\cite{zhi2021place} combines an additional semantic head into NeRF to predict the semantic label for input 3D position.
% Similarly, instance masks also can be used to guide the object decomposition process for editable scene rendering \cite{yang2021learning} and surface reconstruction~\cite{wu2022object}.
% LaTeRF~\cite{mirzaei2022laterf} use feature from pre-trained model to evaluate the objectness at each 3D points and in-paint the occluded parts of the object.
% GIRAFFE~\cite{niemeyer2021giraffe} extends the generative ability of scene composition by introducing conditions latent codes for each object representation and discriminator for adversarial scene learning.
% Similarly, DisCoScene~\cite{xu2022discoscene} adopts 3D boxes to spatially disentangle the entire scene into object-centric generative radiance fields.
% and learn the scene representation on 2D images with global-local discrimination.

\noindent\textbf{Text-guided 3D Generative Models.}
% The field of 3D model generation has sparked numerous research that explores various 3D representations, including 3D voxel grids~\cite{gadelha20173d}, point clouds~\cite{achlioptas2018learning}, meshes~\cite{JunGao2022GET3DAG}, implicit representations~\cite{ZhiqinChen2018LearningIF}, and octree representations~\cite{MoritzIbing2022OctreeTA}.
% Inspired by the achievements of vision-language models such as CLIP~\cite{radford2021learning}, there has been significant progress in text-guided 3D generation. Several works~\cite{jain2022zero, wang2022clip, mohammad2022clip, sanghi2022clip, xu2022dream3d} have employed pre-trained vision-language models to align image features extracted from rendered 3D views with text features by minimizing feature similarity. 
% As the pre-trained vision-language model can offer robust alignment between image and text features, 
% incorporating text guidance into the generation process has become a popular approach in recent works~\cite{jain2022zero,wang2022clip,mohammad2022clip,sanghi2022clip,xu2022dream3d}.
% They achieve text-to-3D generation by minimizing the embedding similarity between the 2D rendering results obtained from 3D representations and the input text.
% However, despite that large-scale pre-trained vision-language models can provide strong 2D guidance for 3D representation learning, their 2D rendering results may be less realistic due to limited feature details.
% Though pre-trained large-scale vision-language models can offer strong 2D guidance for 3D representation learning, the quality of their 2D rendering results may be less realistic due to insufficient details in the features.
% The convergence of vision-language models like CLIP~\cite{radford2021learning} with 3D generative technologies has marked a significant stride in text-guided 3D generation.
% % Motivated by the success of the vision-language models,\eg,~CLIP~\cite{radford2021learning}, text-guided 3D generation has also been rapidly progressing.
% Efforts such as~\cite{jain2022zero,wang2022clip,mohammad2022clip,sanghi2022clip,xu2022dream3d} 
% % utilize the pre-trained vision-language model to provide robust alignment between image features extracted from rendered views of 3D representations and text features by minimizing feature similarity.
% leverage these advanced vision-language models to bridge rendered 3D views and text descriptions, enhancing feature alignment through similarity minimization. 
% Despite their strengths, these models often yield renderings with limited detail, hindering the realism of the 3D output.
% Recently, DreamFusion~\cite{poole2022dreamfusion} demonstrates remarkable capability in text-to-3D object generation by incorporating a powerful pre-trained text-to-image diffusion model~\cite{saharia2022photorealistic} through score distillation sampling.
% Similarly, Magic3D~\cite{lin2022magic3d} proposes a two-stage super-resolution method to enhance generation quality with hybrid 3D representation.
% Latent-NeRF~\cite{metzer2022latent} demonstrates that updating NeRF in the latent space using score distillation sampling can also produce realistic results. 
% SJC~\cite{wang2022score} improves the sampling process by introducing a perturb-and-average scoring scheme to address distribution mismatching issues.
% DreamBooth3D~\cite{raj2023dreambooth3d} proposes a three-stage method that utilizes DreamBooth~\cite{ruiz2023dreambooth} and DreamFusion to generate 3D object from the text and 3-6 images inputs.
% Points-to-3D~\cite{yu2023points} utilizes the sparse 3D point cloud as the immediate representation to guide the 3D generative NeRF model.
% The convergence of vision-language models such as CLIP with 3D generative methods has marked a significant leap in text-driven 3D content creation. Initial models, exemplified by the works of Jain et al. and Wang et al., have adeptly demonstrated the alignment of 3D renderings with textual descriptions, yet they frequently encounter limitations in rendering detailed features, which is crucial for achieving high realism. To advance beyond these constraints, pioneering techniques such as DreamFusion, Magic3D, and Latent-NeRF have integrated text-to-image diffusion models and harnessed score distillation sampling in the latent space to enhance detail and fidelity. Further innovations by SJC and DreamBooth3D have addressed distribution mismatches and facilitated image-based 3D generation, while Points-to-3D has introduced 3D point clouds as a guiding mechanism.
%
%
To facilitate 3D asset creation in VR/AR, the integration of vision-language models like CLIP with 3D generative methods has propelled text-guided 3D generation forward. Models that harness these advancements, such as those by Jain et al.\cite{jain2022zero} and Wang et al.\cite{wang2022clip}, have excelled in aligning 3D renderings with text descriptors but often fall short in detail, limiting realism. Innovative approaches like DreamFusion~\cite{poole2022dreamfusion}, Magic3D~\cite{lin2022magic3d}, and Latent-NeRF~\cite{metzer2022latent} have sought to enhance this through text-to-image diffusion models and score distillation sampling in the latent space, with SJC~\cite{wang2022score} and DreamBooth3D~\cite{raj2023dreambooth3d} further refining the process to address distribution mismatches and enable image-based 3D generation, respectively. Points-to-3D~\cite{yu2023points} takes a novel route by utilizing 3D point clouds for guidance, whereas Fantasia3D~\cite{Chen_2023_ICCV} innovatively disentangles geometry and appearance tasks, it employs the Stable Diffusion model for learning geometry and utilizes the Physically-Based Rendering (PBR) material model~\cite{McAuley_Hill_Hoffman_Gotanda_Smits_Burley_Martinez_2012} for appearance learning. 
Moreover, several studies~\cite{liang2024luciddreamer, wang2024prolificdreamer, yu2023text, ma2024scaledreamer, li2024connecting} are dedicated to enhancing the SDS loss to provide more detailed supervision.
% , treating these two critical aspects of 3D modeling.
%
%
Departing from these singular approaches, our CompoNeRF introduces a novel approach for the creation of multi-object 3D scenes. It adopts an \textit{object-compositional} strategy, utilizing an editable 3D scene layout that conceptualizes the scene not as a singular but as a constellation of discrete NeRFs. Each NeRF is associated with its spatial 3D bounding box and a corresponding text prompt, allowing for guidance from both the global texts and their subtexts. This dual-text framework ensures that each object is not only individually delineated but also integrated into the composite scene, thereby enhancing the authenticity of the generated 3D scenes.
%
% Note that we omit Fantasia3D, as it relies on a hybrid scene representation DMTET~\cite{Shen_Gao_Yin_Liu_Fidler_2021}, w
% Distinct from these approaches, our CompoNeRF innovates in the generation of complex, multi-object 3D scenes by employing an \textit{object-compositional} method. It leverages a unique editable 3D scene layout, which represents objects not as a monolithic entity but as individual local NeRFs, each grounded in both a spatial 3D box and a descriptive text. 
% By contrast, our CompoNeRF generates multi-object 3D scenes in an \textit{object-compositional} way by utilizing multiple local NeRFs to model the scene instead of a holistic object-centric representation.
% Our editable 3D scene layout allows for editing of scenes by changing the text prompt similarly as ~\cite{poole2022dreamfusion, lin2022magic3d}. Also, it enables manipulation of the spatial arrangement of individual objects in a crowded scene, as summarized in Tab.~\ref{tab:compare}.
% Moreover, the layout allows for recomposition with other off-the-shelf representations, enabling the rapid generation of new scenes.

\noindent\textbf{Object-Compositional Scene Modeling.}
The creation of new scenes from individual, object-centric components represents a trend in scene generation, as evidenced by existing research~\cite{zhi2021place, yang2021learning, wu2022object, mirzaei2022laterf, xu2022discoscene, song2022towards}. These efforts typically adopt one of two approaches: semantic-based or 3D layout-based.
%
Semantic-based methods enhance object representations by incorporating additional semantic information, such as segmentation labels~\cite{zhi2021place}, instance masks~\cite{yang2021learning, wu2022object}, or features extracted using pre-trained vision-language models~\cite{mirzaei2022laterf}. On the other hand, 3D layout-based approaches, exemplified by NSG~\cite{ost2021neural} and its successors~\cite{song2022towards, epstein2024disentangled}, focus on spatial coordinates, using explicit 3D object placement data to guide object and scene composition.
%
Diverging from conventional techniques, our method innovates by utilizing decomposed, object-specific 3D layouts. This approach enables precise control over scene dynamics, encompassing both object-specific text prompt modifications~\cite{poole2022dreamfusion, lin2022magic3d} and spatial manipulation.
% , akin to DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d}.
%
CompoNeRF's distinctive feature lies in its capability to recompose scenes by interfacing with decomposed NeRFs, thereby accelerating the creation of new scenes. In contrast to the mesh-based method in Fantasia3D, which requires considerable human effort in mesh modification and graphics engine support for editing, CompoNeRF offers a more streamlined process. Our composition module seamlessly integrates components, requiring minimal adjustments in layout or text prompts, followed by fine-tuning existing offline models to align with the global context during training.
% The idea of generating new scenes by compositing multiple object-centric representations is a straightforward approach in scene generation~\cite{guo2020object}.
% Several works~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe,ost2021neural,xu2022discoscene,song2022towards} attempt to directly decompose object representations from the scene image to perform compositional scene modeling. Based on the additional object-level information, they can be roughly categorized as semantic based~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe} and 3D layout based~\cite{ost2021neural,xu2022discoscene,song2022towards}.
% %
% Semantic based methods incorporate extra semantic information, such as segmentation labels~\cite{zhi2021place}, object instance mask~\cite{yang2021learning,wu2022object}, and features from the pre-trained vision-language model~\cite{mirzaei2022laterf}, to learn the object representations.
% %
% On the other hand, 3D layout-based methods directly use the 3D object coordinate information as the object guidance for learning object-specific representation and generating the full scene by compositing all object representations.
% For instance, NSG~\cite{ost2021neural} and its variant~\cite{song2022towards} use a scene graph structure to model dynamic scenes, associating each object with a 3D box and a node representation.
% %
% While previous works focused on decomposing object entities from real-world images, our work serves as the \textit{\textbf{first}} attempt to tackle the text-to-3D generation via decomposed representations with 3D scene layout.
% The assembly of new scenes from individual object-centric components is a well-established practice in scene generation~\cite{guo2020object}. Efforts in this space~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe,ost2021neural,xu2022discoscene,song2022towards} have made strides in decomposing scenes into object-based representations for more dynamic modeling. These techniques generally fall into two categories: semantic-based and 3D layout-based approaches.
% %
% Semantic-based methods enrich object representations with additional semantic information, utilizing tools like segmentation labels~\cite{zhi2021place}, instance masks~\cite{yang2021learning,wu2022object}, or leveraging pre-trained vision-language models for feature extraction~\cite{mirzaei2022laterf}.
% %
% Conversely, 3D layout-based methods prioritize spatial coordinates, harnessing explicit 3D object placement data to guide the composition of each object’s representation and the scene as a whole. Notable implementations include NSG~\cite{ost2021neural} and its developments~\cite{song2022towards}, which use scene graphs to contextualize objects within dynamic environments.
% %
% Our work diverges significantly from traditional image decomposition methods, pioneering the use of decomposed, object-specific 3D layouts for text-to-3D scene generation. This innovative approach carves a new path in the field, offering a flexible and editable layout that allows for fine-grained control over the dynamics of a scene. This control extends to object-specific text prompt modifications, similar to the methods employed in DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d}, as well as to the spatial manipulation within complex scenes.
% %
% A key feature that sets CompoNeRF apart is its ability to recompose scenes by interfacing with a variety of pre-existing models. This functionality significantly expedites the creation of novel scenes, an aspect that is highlighted in Table~\ref{tab:compare}.
% %
% It is noteworthy that while the mesh recomposition technique utilized in Fantasia3D necessitates extensive human input in crafting meshes and relies on graphics engines for scene editing and physical simulation, CompoNeRF streamlines this process. Our tailored composition module integrates these elements seamlessly, requiring only adjusting layout or text prompts, then fine-tuning on existing offline models to ensure their compatibility with the global environment during training.
% Distinct from the decomposition of images into objects, our work pioneers the approach of using decomposed, object-specific 3D layouts to facilitate text-to-3D scene generation, marking a novel direction in this field.
% This editable layout affords unprecedented control over scene dynamics, allowing for object-specific text prompt modifications, similar to ~\cite{poole2022dreamfusion, lin2022magic3d}, and spatial manipulation within intricate scenes. 
% % Departing
% Further distinguishing our work, CompoNeRF enables the recomposition of scenes by interfacing with various off-the-shelf representations, thus accelerating the novel scenes creation, a capability we underscore in Tab.~\ref{tab:compare}. 
% %
% Note that mesh recomposition proposed by Fantasia3D  requires tedious human labors on crafting meshes, and further support from graphics engines for scene editing and physical stimulation.  On the contrary, our tailored composition module integrate them with only finetuing on existing offline models and harmonize them with global environment during traininging.  
%% if specified like this the section will be committed in review mode
\section{Method}
\label{sec: method}
% This section introduces the rendering pipeline of our proposed hierarchical compositional scene. 
% our pipeline consists of three processes, including decomposing the text into editable 3D layout, rendering the compositional views with local (object) NeRFs and global (scene) NeRF and the joint optimization on these hierarchical 3D representations.

% Note that the transformation between the object and the scene frame is defined by ${p}_o$ and ${D}_o$. 
%
% Next, we build a residual connection to add ${\sigma}_o$ and the referenced global color, and the rendering result will be used to calculate the SDS loss based on the global text.  
% Fig.~\ref{fig:framework} illustrates our pipeline, which consists of three main components, including the editable 3D scene layout based on multi-object text (Sec.~\ref{ssec:layout}), the scene rendering pipeline that composites the predictions from all local NeRFs (Sec.~\ref{ssec:render}), and the joint optimization on both local and global representation models (Sec.~\ref{sec:optimization}).
% To elaborate, our editable 3D scene layout represents a global frame of the scene by decomposing it into a set of local frames, where each is parameterized by a local NeRF, a 3D bounding box, and a corresponding local text prompt.
% For instance, the text prompt `A teddy bear and a stuffed monkey sit side by side' is interpreted as a 3D scene layout, as shown in Fig.~\ref{fig:framework}.  
% The whole 3D layout, \ie, scene frame, consists of two 3D bounding boxes, \ie local frames \#1 and \#2, with specific local text prompts, \ie, `a teddy bear' and `a stuffed monkey'. 
% %
% To render the scene view, we first calculate the ray-box intersections between the boxes and rays $({\boldsymbol{r}}_o, \boldsymbol{\phi}_d, {\boldsymbol{\theta}}_d)$, where the ${\boldsymbol{r}}_o$ is the ray origin and the $({\boldsymbol{r}}_o, \boldsymbol{\phi}_d)$ is its direction.
% Then, to infer each object's properties in local NeRFs, we sample the global points $({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g)$ in the global frame within the ray-box intersection intervals and project them into the normalized local location $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$ in the local frame.
% %
% Given the local sampling points $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$, the implicit local NeRF ${\boldsymbol{\theta}}_l$ outputs four pseudo-color channels ${\boldsymbol{C}}_l$ and density $\boldsymbol{\sigma}$, which can be used to render a local view of the local frame to match its local text prompt.
% %
% We further calibrate the predicted pseudo-color $\boldsymbol{C}_l$ from local frames by adding the global embeddings ${\boldsymbol{emb}}_g$ to improve the global view consistency.
% Then, the calibrated predictions after composition are used to reconstruct the scene view by volumetric rendering along the rays.
% %
% Lastly, the rendered views based on local and global frames are guided by score distillation sampling loss $\nabla \mathcal{L}_{\text{SDS}}$~\cite{poole2022dreamfusion} to optimize all the learnable parameters. 
To resolve the issue of guidance collapse, our principal strategy is to \textit{decompose the scene into reusable components and compose/recompose them into a unified and consistent one}.
This enables flexible control over the generated content with direct use of prompts and box layouts.
%
As illustrated in \cref{fig:teaser}, our proposed CompoNeRF confers several key benefits:
1) \textbf{Semantic Coherence}: It reliably creates 3D objects with detailed textures and global consistency, exemplified by authentic light interactions, such as reflections on the bed surface.
2) \textbf{Modularity and Reusability}: CompoNeRF functions as an ensemble of independently trained NeRF models. These can be efficiently stored and later retrieved from a cached dataset, enabling their reuse in various cases.
3) \textbf{Editability}: Our approach allows for flexible scene modification, such as interchanging the lamp for a vase filled with sunflowers or altering its scale, by simply adjusting the box dimensions for later finetuning. This feature enhances flexibility and creative possibilities. 


% Furthermore, the usage of layout boxes enables more flexible control over the generated content compared with the intricate sketch shape in Latent-NeRF\cite{metzer2022latent}. 

\subsection{Preliminaries}
Defining individual object bounding boxes as \textit{local frames} and the overall scene coordinate system as the \textit{global frame}, we build the foundation of NeRF and diffusion processes.

\label{sec:background}
\noindent \textbf{3D Representation in Latent Space.}
Our methodology capitalizes on the state-of-the-art text-to-image generative model—Stable Diffusion as described by Rombach et al\cite{rombach2022high}.
We build upon the Latent-NeRF framework~\cite{metzer2022latent}, which computes latent colors for individual objects by considering their sample positions within a localized frame. Specifically, it maps a three-dimensional point in local coordinates \(\boldsymbol{x}_l = (x_l, y_l, z_l)\) to a volumetric density \(\boldsymbol{\sigma}_l\) and an associated color \(\boldsymbol{C}_l\), expressed as \((\boldsymbol{C}_l, \boldsymbol{\sigma}_l) = f_{\boldsymbol{\theta}_l}(x_l, y_l, z_l)\). Here, \(f\) represents a Multi-Layer Perceptron (MLP) characterized by parameters \(\boldsymbol{\theta}_l\).
 This NeRF-generated color is then assessed in the context of the Stable Diffusion model, using text prompts to guide NeRF toward spatially coherent inference with intricate context.
% to infer pseudo-color for each object using local NeRF.
% Specifically, the representation maps a point $\boldsymbol{x}_l = \left({x}_l, {y}_l, {z}_l\right)\in [-1, 1]$ in the local frame to its corresponding volumetric density $\boldsymbol{\sigma}_l$ and emitted color $\boldsymbol{C}_l$, \ie,  $\left(\boldsymbol{C}_l, {\boldsymbol{\sigma}_l}\right)=\boldsymbol{\theta}_{_l}\left({x_l}, {y}_l, {z}_l\right)$.
% The predicted pseudo-color is fed forward into the decoder of the Stable Diffusion model to obtain the final rendering result.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/method.pdf}
    % \vspace{-12pt}
    \caption{\textbf{Framework Overview}.
The CompoNeRF model unfolds in three stages: 1) Editing 3D scene, which initiates the process by structuring the scene with 3D boxes and textual prompts; 2) Scene rendering, which encapsulates the composition/recomposition process, facilitating the transformation of NeRFs to a global frame, ensuring cohesive scene construction. Here, we specify design choices between density-based or color-based(without refining density) composition; 3) Joint Optimization, which leverages textual directives to amplify the rendering quality of both global and local views, while also integrating revised text prompts and NeRFs for refined scene depiction.
  % The model is structured into three components: Composition, Decomposition, and Recomposition. Composition deals with the foundational setup, detailed with choices for density-based and color-based composition. Decomposition utilizes the modularity of the CompoNeRF feature, caching each NeRF module offline for efficient recalibration. Recomposition reuses these cached NeRFs and adjusts the semantic context, providing a revised output with the inclusion of the offline NeRF enhancements.
    % Our model consists of two branches where the upper part is individual NeRFs, and the lower part denotes global calibration with our tailored composition model. The specific designs for density-based and color-based composition modules are highlighted. 
    % CompoNeRF consists of three parts: 1). The editable 3D scene layout configures the scene representations with 3D boxes and text prompts; 2).  The scene rendering includes the global calibration and the compositional process; 3). The joint optimization applies global and local text guidance on global and local render views.
    % The global frame (scene space) contains a set of local frames. Each is  represented by a local NeRF associated with a 3D box and text prompt defined by the editable 3D layout.
    % The scene view is volumetric rendered by sampling the points $({\boldsymbol{x}}_g, \boldsymbol{y}_g, \boldsymbol{z}_g)$ intersected with any local frame along the ray $(\boldsymbol{r}_o, {\boldsymbol{\phi}}_d, \boldsymbol{\theta}_d)$.
    % The sampling points are first inferred through the local NeRF with the local frame locations $({\boldsymbol{x}}_l, \boldsymbol{y}_l, \boldsymbol{z}_l)$ projected from the global location $({\boldsymbol{x}}_g, \boldsymbol{y}_g, \boldsymbol{z}_g)$.
    % And then, all the local predictions are calibrated by a global MLP with conditional input to render the scene view.
    % During the optimization, the text guidance is applied to both local views predicted by local frames only and global views predicted by the composition of all local frame predictions.
    }
    \label{fig:framework}
    % \vspace{-8pt}
\end{figure*}
\noindent \textbf{Volume Rendering with Multiple Objects.}
% For each local frame $j$ with NeRF parameterized as $\theta_j$, we follow original NeRF design\cite{nerf} to integrate $(\boldsymbol{C}_l, \boldsymbol{\sigma}_l)$ of   sampled points from any hit ray $r_l=(\boldsymbol{o}_l, \boldsymbol{d}_l)$ by,
% For consistent scene rendering, object transmittance $T_k$ must be recalculated in the global frame based on independent properties inferred from local NeRFs. Hence, we sort predictions according to their distance to $\boldsymbol{o}_g$. 
% Similar to \cref{eq:volrend}, global color $\hat{\boldsymbol{C}}_g$ of ray $\boldsymbol{r}_g=(\boldsymbol{o}_g, \boldsymbol{d}_g)$ is predicted by the volumetric rendering integrating over $m$ objects,
We extend the volume rendering process to accommodate multiple objects by assigning each a local frame, denoted as $j$, with NeRF parameters $\boldsymbol{\theta}_{l, j}$. Drawing from the foundational NeRF approach \cite{nerf}, in each local frame, we integrate the color $\boldsymbol{C}_l$ and density $\boldsymbol{\sigma}_l$ for points $\boldsymbol{x}_l$ sampled along a ray $\boldsymbol{r}_l$, emanates from the camera origin $\boldsymbol{o}_l$ in direction $\boldsymbol{d}_l$. This is formalized in the predicted color integration for $\hat{\boldsymbol{C}}_l$ as:
{\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eq:volrend}
{\hat{\boldsymbol{C}}_l}({\boldsymbol{r}_l})=\sum_{k=1}^{N} T_{l, k} \left(1-\exp \left(-\sigma_{l, k} \delta_k\right) \right) {\boldsymbol{C}}_{l,k},
\end{equation}}where $T_{l, k}=\exp \left(-\sum_{j=1}^{k-1} \sigma_{l,j} \delta_j\right)$ represents the transmittance to the $k$-th of total $N$ sample, calculated exponentially over the cumulative density along $\boldsymbol{r}_l$, and $\delta_k$ is the interval between adjacent samples.
%
To synthesize a coherent scene, we transition from processing individual local frames to a collective global frame. Within this global context, we reconcile object attributes inferred from their individual local NeRFs for refined $\boldsymbol{\sigma}_g, \boldsymbol{C}_g$ along with $T_{g, k}$. The samples $\boldsymbol{x}_g$ are ordered based on their spatial distances from the origin $\boldsymbol{o}_g$ following the coordinate transformation. We then express the volumetric rendering of a ray $\boldsymbol{r}_g$ integrating $m$ objects within the global frame as follows:
{
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eq:multi_volrend}
{\hat{\boldsymbol{C}}_g}({\boldsymbol{r}_g})=\sum_{k=1}^{m*N} T_{g, k} \left(1-\exp \left(-\sigma_{g, k} \delta_k\right) \right) {\boldsymbol{C}}_{g,k}. 
\end{equation}}

\noindent \textbf{Score Distillation Sampling.}
% During the SDS process, a noise image $\boldsymbol{X}_t$ is first generated by adding a sampled noise $\epsilon \sim \mathcal{N}(0, I)$ in noise level $t$ into a rendered view $\boldsymbol{X}$ from a NeRF.
To facilitate the conversion from text descriptions to 3D models, DreamFusion~\cite{poole2022dreamfusion} utilizes Score Distillation Sampling (SDS), leveraging the generative capabilities of a diffusion model, denoted as $\phi$, to guide the optimization of NeRF parameters, symbolized as $\boldsymbol{\theta}$.
%
Initially, SDS creates a noisy image $\boldsymbol{X}_t$ by infusing a randomly sampled noise $\epsilon$, which follows a normal distribution $\mathcal{N}(0, I)$, into a NeRF-rendered image $\boldsymbol{X}$ at a given noise level $t$.
The diffusion model $\phi$ then estimates the noise $\epsilon_\phi\left(\boldsymbol{X}_t, t, T\right)$ from this noisy image, conditioned by the noise level $t$ and an optional text prompt $T$. 
The key step in SDS involves calculating the gradient of the loss function, which measures the discrepancy between the estimated noise and the originally added noise:
{\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eq:sds_loss}
\nabla_\theta \mathcal{L}_{\text{SDS}}(\boldsymbol{X}_t, T)=  w(t)\left(\epsilon_\phi\left(\boldsymbol{X}_t, t, T\right)-\epsilon\right),
\end{equation}}where $w(t)$ is a weighting function that adjusts the influence of the gradient based on the noise level. 
The gradients across all rendered views direct the update of $\boldsymbol{\theta}$, ensuring that the NeRF-generated images align with the text descriptions. Additionally, we incorporate the 'perturb and average' technique from SJC for more robust $\mathcal{L}_{\text{SDS}}$. For a comprehensive understanding of these methods, the reader is directed to the detailed explanations provided in \cite{poole2022dreamfusion,wang2022score}.

%
%
% \subsection{Editable 3D Scene Layout}
% \label{ssec:layout}
% The 3D scene layout explicitly combines language structures with 3D layouts in an editable way.
% Given the input text prompt $T$, the attribute-object pairs can be easily obtained based on user control.
% Note that the text prompt indicates the multi-object text prompt by default.
% % available for free in many structured representations, such as the constituency tree.
% As shown in Fig.~\ref{fig:framework}, we can extract multiple noun phrases with their binding attributes and map these local text prompts into corresponding regions.
% Specifically, we define the scene structure with $m$ local frames, each employs a local NeRF $\boldsymbol{\theta}_l$ as representation, the local text prompt $T_{l} \subseteq{T}$ and its spatial layout with 3D boxes $\mathbf{b} = \{\mathbf{p}, \mathbf{s}\} \in  \mathbb{R}^6$ of each object entity, where $\mathbf{p}=\{p_x, p_y, p_z\}$ refers to the center point and $\mathbf{s}=\{s_x, s_y, s_z\}$ denotes the box scale. 
% \textit{Our editable 3D layout is easy to be collected and edited with its simplicity, allowing for versatile and interactive user control by modifying the box's or text's properties to define a new scene}.
% Moreover, as depicted in Fig.~\ref{fig:teaser}, each component in a 3D scene layout can be replaced or re-composited with other trained local NeRFs, which is more friendly for flexible user editions compared with using only text prompts.
% We fine-tuned the new layout by global rendering, which enables scalable re-editing.
% Each relationship $r_k \in R$ is a triplet in a <subject-predictive object> format, where a subject node is. After we generate the scene graph from the complex prompts, we can sample the closest relationship with the 2d spatial layout as the initial 3D position. fine-tuned the new layout by global rendering, which enables scalable re-editing
%
% \subsection{Scene Rendering Pipeline}
% \label{ssec:render}
% In CompoNeRF, the scene images are rendered by a ray-casting approach following the design of NeRF.
% % Each ray to be cast is generated based on the camera pose, intrinsic, and transformation.
% The camera is defined by a pinhole camera model, casting a set of rays $(\boldsymbol{r}_o, \boldsymbol{\phi}_d, {\boldsymbol{\theta}}_d)=\boldsymbol{o}+t\boldsymbol{d}$ through each pixel on the frame of size $H \times W$, where the $\boldsymbol{r}_o \in  \mathbb{R}^3$ is the origin and the $(\boldsymbol{\phi}_d, \boldsymbol{\theta}_d)$ is the viewing direction.
% Along this ray, we sample all the points intersected with any layout box of local frames.
% For each hit sampled point, the color and volumetric density are computed through the local NeRF of the hit local frame.
% The ray color perdition is calculated by the differentiable integration applied on all the point-predicted colors and volumetric density along the ray.
%
% \noindent \textbf{Ray-box Intersection with Local Frames.}
% Given a ray $\boldsymbol{r}_i$, each box $\boldsymbol{b}_j$ of the local frame is applied with the AABB ray intersection test algorithm to check the intersections.
% When the ray $r_i$ is hit with a box $\boldsymbol{b}_j$ of the local frame, we use the entrance and exit points as near $\boldsymbol{t}_{in}$ and far $\boldsymbol{t}_{out}$ bounds to sample $N$ equidistant quadrature points, $
% \boldsymbol{t}_{i,j,n}=\frac{n-1}{N-1}\left(\boldsymbol{t}_{out}-\boldsymbol{t}_{in}\right)+\boldsymbol{t}_{in} , n \in \left[1, N\right]$
% % Despite each local frame only having a small number of hit rays compared to the scene, we observe that it is enough to represent each object accurately while maintaining short rendering times.
% Note that the coordinates of sampled points are first projected into normalized coordinates using the box scale of local frames to enable each local NeRF to learn the scale-independent representation.
% The bounding box $\mathbf{b}$ of the local frame in global coordinate can be transformed into a canonical bounding box by ${(\mathbf{b}} - \boldsymbol{p}) / \mathbf{s}$.
% Considering the rendering efficiency, we only calculate the valid points, interacted with the boxes, and set all the empty points with a constant background color.
%
% The appearance of a set object representations depends on its interaction with the scene and illumination which should be decided by the local frame location.
% To ensure the volumetric consistency, we only calibrate the emitted color with scene location, while the gradient still can be propagated.
% Since the overall color depends on both the global  positions $({x}_w, {y}_w, {z}_w)$ and ray directions $({\phi}_d, {\theta}_d)$, the global color embedding is learned based on both the positions and ray directions.
% Since the overall color depends on both the global  positions $({x}_w, {y}_w, {z}_w)$ and ray directions $({\phi}_d, {\theta}_d)$, the global color embedding is learned based on both the positions and ray directions.
% \subsection{The Proposed CompoNeRF}
% \subsubsection{Composition Module}
% CompoNeRF aims to composite multiple NeRFs to reconstruct multi-object scenes with both box and prompt guidance.
% %
% Our framework, as shown in \cref{fig:framework}, applies the AABB ray intersection test algorithm to check for intersections on each box in the global frame. We then samples $\boldsymbol{x}_g$ within the ray box intervals, and project them to $\boldsymbol{x}_l$ to infer  $\left(\boldsymbol{C}_l, {\boldsymbol{\sigma}_l}\right)$ in separate NeRF models. 
% %
% We then utilize volume rendering to obtain rendered views for each local frame respectively. 
% %
% After that, they would be passed on to our tailored composition Module to infer 
% $\left(\boldsymbol{C}_g, {\boldsymbol{\sigma}_g}\right)$
% for global rendering. 
% Next, we match local and global texts with their corresponding image outputs by SDS losses. 
% We also support recomposition by passing samples from cached models into $\boldsymbol{x}_l$ to continue the above process.

\subsection{The Proposed CompoNeRF}
\subsubsection{Composition Module}
CompoNeRF is designed to composite multiple NeRFs to reconstruct scenes featuring multiple objects, utilizing guidance from both bounding boxes and textual prompts. Within our framework, depicted in \cref{fig:framework}, the Axis-Aligned Bounding Box (AABB) ray intersection test algorithm is applied to ascertain intersections across each box in the global frame. Subsequently, we sample points \(\boldsymbol{x}_g\) within the intervals of the ray-box and project them to \(\boldsymbol{x}_l\) to deduce the corresponding color \(\boldsymbol{C}_l\) and density \(\boldsymbol{\sigma}_l\) within individual NeRF models.
%
These properties are processed through our composition module to infer the global color \(\boldsymbol{C}_g\) and density \(\boldsymbol{\sigma}_g\), crucial for the global rendering.
%
Volume rendering techniques~\cite{kajiya1984ray} are then employed to procure the rendered views for both local and global frames. We propose dual SDS losses to ensure coherence between the image outputs and their corresponding textual descriptions. Additionally, our approach facilitates recomposition by channeling samples from cached models back into local frames along with the text revision, thereby streamlining the integration.

% As shown in \cref{fig:abls}(a), we verify its necessity by dropping $\nabla \mathcal{L}_{\text{SDS}_g}$. 
% %
% Compared with our full model, its layout does not fit our shared sense of a room, \ie, \emph{nightstand} is usually lower than \emph{bed}; \emph{lamp} needs a base to support it. Additionally,  it lacks global consistency, such as light reflection, to make it more realistic. 
% %
% Therefore, we leverage the full text semantics to ensure consistent global rendering across local frames. 
% %
% Instead of conditioning the global rendering view with the full prompt directly, we note that global calibration is necessary for geometry and color to be learned sufficiently.
% For example, we observe that geometric completeness and texture of \emph{nightstand} are not ideal. Although reflection appears around \emph{nightstand}, \emph{bed} is stripped of the light. 
% %
% Therefore, we opt to leverage the correlation between the rendering output of the combined NeRFs and the overall semantics to perform multi-object scene reconstruction.  
%

\noindent\textbf{Global Composition.}
The independent optimization of each local frame may inadvertently result in a lack of global coherence within the scene. To address this, our scene composition process is designed to integrate these frames, thereby achieving a more consistent result.
%
Before exploring the specifics of the module, it is imperative to discuss two critical design decisions within the composition module, as depicted in \cref{fig:framework}.
%
Upon integrating the properties inferred from \(\boldsymbol{x}_g\) into the composition module, they are fine-tuned through gradients derived from the global SDS loss.  This process leads to a critical consideration: the necessity of refining the global density \(\boldsymbol{\sigma}_g\). There are two potential methods: \textbf{1) Density-based:} The advantage of adjusting \(\boldsymbol{\sigma}_g\) is that it can adjust geometry, thus yielding a scene more congruent with the global text prompt. 
However, this comes at the cost of potentially compromising the optimal color \(\boldsymbol{C}_g\), as calibrating \(\boldsymbol{\sigma}_g\) introduces more uncertainty for subsequent color refinement as it requires prior density features $\boldsymbol{h}$.
\textbf{2) Color-based:} Conversely, directly employing \(\boldsymbol{\sigma}_l\) mitigates this uncertainty but has less geometric control, presenting a balance to strike in the pursuit of precise scene composition.
% , which may lead to suboptimal outcomes.

After thorough experiments, exemplified in \cref{fig:abls}, we have opted for the density-based approach to refine \(\boldsymbol{\sigma}_g\)  prioritizing both \textbf{accuracy and efficiency}. The test revealed that it excels in rendering intricate details, such as enhanced wood grain textures and more naturally contoured 'salad', as accentuated by boxes. This method also demonstrated a swifter convergence rate. Conversely, while the color-based improved reflections and reduced flickering on the 'wine cup', it was plagued by issues such as sparse density, which adversely brings holes at the base of the 'cup' and the corner of the 'table'.
Furthermore, upon close examination, it becomes evident that shadow artifacts of 'wine' on the 'table' are pronounced, suggesting that its disadvantages outweigh its advantages.
%  in this context
% \textbf{Global Composition.}
% Each local frame is optimized independently, causing a lack of global connections for scene composition.
% Before delving into module details, there are two choices (see \cref{fig:framework}) on the composition module design we need to elaborate on first. 
% %
% In \cref{fig:framework}, by taking $\boldsymbol{x}_g$ into the composition module, their inferred properties are calibrated with gradients propagated from the global SDS loss. 
% However, it remains unclear whether $\boldsymbol{\sigma}_g$ should be refined or not. 
% %
% The trade-off on its usage is the density adjustment bringing a more reasonable layout and more geometric details that fit the global text prompt. While its potential downside is that $\boldsymbol{C}_g$ may not be optimal as $\boldsymbol{\sigma}_g$ has more uncertainty compared to $\boldsymbol{\sigma}_l$, bringing sub-optimal rendering results. 

% We choose the density-based method after comparing them with the experiment shown in \cref{fig:abls}. 
% %
% Specifically, we test both designs on the scene \emph{table wine} and discover that the density-based design provides more intrinsic details(as indicated by green boxes), \eg, enriched wood grains, and a more natural shape for \emph{salad} and has much faster convergence speed. In contrast, the color-based method enhances the reflection and smooths flickering on \emph{wine cup}, (as indicated by red boxes), but it suffers from 1) sparse density, resulting in poorly generated geometry at the base of  \emph{cup} and the wood \emph{table} corner. Additionally, shadow artifacts appeared on \emph{table} when viewed up close, outweighing benefits of the color-based method.


\noindent\textbf{Network Design.}
The compositional framework of our network, as delineated in \cref{fig:compo}, is predicated on an architecture that employs a suite of MLPs, represented as \(\{\boldsymbol{\theta}_l\}_{l=1}^{m}\),  each dedicated to a distinct local frame. To harmonize \(\boldsymbol{\sigma}_l\) and \(\boldsymbol{C}_l\), we incorporate global MLPs, including density calibrator $f_{\boldsymbol{\theta}_{g_d}}$ and color calibrator $f_{\boldsymbol{\theta}_{g_c}}$.
%
A transformation module complements this system, tasked with maintaining the spatial coherence between the global and local frames. It governs the transformation of sampling points $\boldsymbol{x}$, ray directions $\boldsymbol{d}$, and adjacent sampling distances $\delta$. This module also orders the points $\{\boldsymbol{x}_{g,j}\}_j$ by their distance to the global camera origin $\boldsymbol{o}_g$, ensuring that each local point $\boldsymbol{x}_l$ is accurately matched with its corresponding global point $\boldsymbol{x}_g$ for subsequent volume rendering. 
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/abls.pdf}
    % \vspace{-22pt}
    % \caption{Ablation study on text guidance. (a) without local SDS losses. (b) without global SDS losses. (c) vanilla SDS losses without perturb and average scoring~\cite{wang2022score}. (d) full model.}
    \caption{\textbf{Design Impact Comparison: Density vs. Color-based Methods.} The top row illustrates the density-based approach's detailed rendering and quick convergence in the 'table wine' scene. The bottom row highlights the color-based method's enhancements and its drawbacks, such as geometric and shadow inaccuracies, particularly in close-up views and slow convergence.
    % \textbf{(a)} global text guidance(integrating local frames by \cref{eq:multi_volrend}) and global calibration(integrating local frames, then aligning the rendering result directly with the full text). 
    }
    \label{fig:abls}
    % \vspace{-20pt}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/compo_module.pdf}
    % \vspace{-24pt}
    % \caption{Ablation study on text guidance. (a) without local SDS losses. (b) without global SDS losses. (c) vanilla SDS losses without perturb and average scoring~\cite{wang2022score}. (d) full model.}
    \caption{\textbf{Detail of Composition module}: density-based design. 
    }
    \label{fig:compo}
    % \vspace{-18pt}
\end{figure}
The network design is:
{
\setlength\abovedisplayskip{4.5pt}
\setlength\belowdisplayskip{4.5pt}
\begin{align}
\label{eq:g_c_d}
{\boldsymbol{\sigma}_g}  &= \alpha_d f_{\boldsymbol{\theta}_{g_d}}({\boldsymbol{x}_g}) + \boldsymbol{\sigma}_l, \\
{\boldsymbol{C}_g}  &= \alpha_c f_{\boldsymbol{\theta}_{g_c}}(\boldsymbol{h}, {\boldsymbol{d}_g}) + \boldsymbol{C}_l. 
\end{align}}In contrast to the local frames, the global frame's color output $\boldsymbol{C}_g$ is inferred based on $\boldsymbol{h}$ and conditional on $\boldsymbol{d}_g$ to enable a view-dependent lighting effect.
% Denote the density features as $\boldsymbol{h}$. 
%
%
Residual learning is leveraged here, where \(\boldsymbol{\sigma}_l, \boldsymbol{C}_l\) serve as foundational elements that support the learning of global density \(\boldsymbol{\sigma}_g\) and color \(\boldsymbol{C}_g\). The parameters \(\alpha_d, \alpha_c\) are adjustable, allowing fine-tuning of the influence that local components exert on the global outputs.
%
It is imperative to acknowledge that in our color-based method, density calibration is intentionally excluded to concentrate solely on the refinement of color dynamics as shown at \cref{fig:framework}. This is achieved by conditioning the process on both spatial and directional global inputs \((\boldsymbol{x}_g, \boldsymbol{d}_g)\), as demonstrated in the following equations:
\begin{align}
\setlength\abovedisplayskip{4.5pt}
\setlength\belowdisplayskip{4.5pt}
\label{eq:g_c_c}
\boldsymbol{\sigma}_g = \boldsymbol{\sigma}_l, \quad
{\boldsymbol{C}_g} = \alpha_c f_{\boldsymbol{\theta}_{g_c}}({\boldsymbol{x}_g}, {\boldsymbol{d}_g}) + \boldsymbol{C}_l.
\end{align}
The integration of extra $\boldsymbol{x}_g$ aims to facilitate a fair comparison under same inputs with the density-based. It enhances the visual appeal of effects like the wine cup's reflection, as demonstrated in \cref{fig:abls}. However, this method is not without its compromises. It tends to produce artifacts and is characterized by a slower convergence rate. Additionally, this approach limits the ability to precisely control density, subsequently impacting the intricate geometric details.
 


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/sota.pdf}
    % \vspace{-24pt}
    \caption{\textbf{Qualitative comparison with other text-to-3D methods using multi-object text prompts}. We refer readers to our \textit{suppl.} and video demos for more visual results. 
    % Cases 1-3 demonstrate simpler settings characterized by compositions involving two objects. In contrast, Cases 4-8 delve into more intricate scenarios featuring compositions with more than two objects. Smaller images are presented to illustrate the generated local NeRFs(partially shown in Cases 4-8).
    }
    \label{fig:sota}
    % \vspace{-5pt}
\end{figure*}

% \begin{table*}[t!]
% \centering
% \resizebox{\textwidth}{!}
% {
% \begin{tabular}{cccccccc}
% \toprule
% Method            & \rotatebox{60}{table wine}  & \rotatebox{60}{teddy monkey} & \rotatebox{60}{computer mouse} & \rotatebox{60}{bed room}  & \rotatebox{60}{chess} & \rotatebox{60}{pisa tower} & \rotatebox{60}{astronaut} & \rotatebox{60}{tesla}  \\ \midrule
% LatentNeRF  & 21.55 & 27.38 & 17.13 & 21.86 & 31.19 & 24.31 & 27.07 & 25.16 \\
% SJC & 23.33 & 27.37 & 18.00 & 22.54 & 30.53 & \textbf{26.18 }& 27.84 & 23.55 \\
% CompoNeRF & \textbf{32.68} & \textbf{28.57}	 &\textbf{ 22.34} &\textbf{ 28.65} & \textbf{31.45} & \textbf{28.96} & 25.82 & 25.95 & 24.42 & \textbf{32.71} & \textbf{26.13 }& \textbf{26.38} & \textbf{30.98} & \textbf{33.37} \\
% \bottomrule
% \end{tabular}
% }
% \vspace{-10pt}
% \caption{Performance of our CompoNeRF in different 3D scenes. We use CLIP score \cite{parmar2023zero,zhang2023sine,wang2023imagen} as our evaluation metric, which is a common evaluation metric in text-to-image generation tasks to evaluate the similarity of the generated image to the text prompt. }
% \label{perclass}
% \end{table*}
%

% \cref{fig:framework} depicts the network architecture of the composition module. Denote $m$ as local MLP $\{\boldsymbol{\theta}_l\}_{l=1}^{m}$ for each local frame. Then, we introduce the global MLPs including density $\boldsymbol{\theta}_{g_d}$ and $\boldsymbol{\theta}_{g_c}$ calibrators to refine $\boldsymbol{\sigma}_l$ and $\boldsymbol{C}_l$. 
% %
% In detail, the network design is, 
% {
% % \setlength\abovedisplayskip{4.5pt}
% % \setlength\belowdisplayskip{4.5pt}
% \begin{align}
% \label{eq:g_c_d}
% {\boldsymbol{\sigma}_g}  &= \alpha_d \boldsymbol{\theta}_{g_d}({\boldsymbol{\sigma}_l}) + \boldsymbol{\sigma}_l, \\  
% {\boldsymbol{C}_g}  &= \alpha_c \boldsymbol{\theta}_{g_c}({\boldsymbol{C}_l},  {\boldsymbol{d}_g}) + \boldsymbol{C}_l, 
% \end{align}}
% %
% where residual $\boldsymbol{\sigma}_l, \boldsymbol{C}_l$ assist in learning $\boldsymbol{\sigma}_g$ and $\boldsymbol{C}_g$, while $\alpha_d, \alpha_c$ balance their contribution as learnable parameters.
% %
% Note that the color-based omits density calibration, and simply uses the shared color refinement.



% The 3D boxes are only used for the spatial configuration of local NeRFs, while the implicit representation of local NeRFs is inferred by the canonical samples inside the local frame without considering the global relationship across different objects.
% To relieve such location-dependent effects, we further calibrate the output color and density from the local NeRF with global coordinates $({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g)$ and ray directions $\left({\boldsymbol{\phi}}_{d}, {\boldsymbol{\theta}}_{d}\right)$ as the conditional input.
% % to inject the global visual clues.
% %
% %
% Specifically, we adopt a shared MLP $\boldsymbol{\theta}_{g}$ to calibrate all the predicted object colors, that is,
% {\setlength\abovedisplayskip{4.5pt}
% \setlength\belowdisplayskip{4.5pt}
% \begin{align}
% \label{eq:MLP_dyn_2}
% {\boldsymbol{C}_g} = {\boldsymbol{C}_l} + \boldsymbol{emb}_{g} &= {\boldsymbol{C}_l} + \boldsymbol{\theta}_{g}({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g, {\boldsymbol{\phi}}_{d}, {\boldsymbol{\theta}}_{d}),
% \end{align}}
% where ${\boldsymbol{C}_l}$ is the color predicted by the local NeRF.
% Therefore, the scene color can preserve the view-consistent behavior from the original architecture and add consistency across poses for the volumetric density.
% Since the color and density values share the same latent expression in $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$, we only calibrate the emitted scene color explicitly with the scene location, as the densities of local NeRFs also are implicitly adjusted during optimization.

% \noindent \textbf{Global and Local Volumetric Rendering.}
% After compositing all the interacted points, each ray $\boldsymbol{r}_i$ collects a set sampling points by $\{\boldsymbol{t}_{i,j,n} \}_{j=1, n=1}^{m_j, N}$, where $m_j$ is the number of the hit object.
% For each sampling point, the inference results with the respective 3D representations are the local color $\boldsymbol{c}_{l}$, global color $\boldsymbol{c}_{g}$, and density $\sigma$.

% In fact, the local view $\hat{C}_{l,j}$ of single object $j$ also can be rendered by the sampled points  belongs to the same local frames as shown at Fig.~\ref{fig:framework}.

\subsubsection{Recomposition}
Our architecture advances scene reconstruction by providing an intuitive interface for layout manipulation.  This capability is crucial for the reconfiguration of scene elements into novel scenes, as depicted in \cref{fig:framework}. Here, the input panel allows for adjustments in the attributes of bounding boxes, such as modifying the position and scale of the 'apple' bounding box prior to composition. The refinement process further involves sampling ray-box intervals from the global frame, leading to transformed coordinates with the corresponding ray samples that are then incorporated into the pipeline, as demonstrated in \cref{fig:compo}.
%
Each bounding box represents a NeRF, providing the flexibility to move, scale, or replace elements as needed. CompoNeRF's capabilities also extend to textual edits, exemplified by the transformation of 'wine' into 'juice'.
%
Since NeRFs have been well trained, we only finetune \(\theta_g, \theta_l\) to align text prompts to promote consistency of both local and global views.
%
Moreover, the NeRFs once retrained within the edited scene, are also structured to be decomposable and cacheable in future scene compositions.

% Our CompoNeRF architecture facilitates the seamless reconstruction of scenes leveraging existing models. It enables precise editing of bounding boxes parameterized by \(\{\boldsymbol{\theta}_l\}_{l=1}^{m}\), allowing for their reconfiguration into new layouts. Refer to \cref{fig:framework}, the input panel permits the modification of attributes such as the position and scale of the 'apple' node's bounding box prior to composition. The process is further refined by sampling from the updated ray-box intervals within the global frame, which are then projected onto \(\boldsymbol{x}_l\), ensuring a streamlined reconstruction that integrates the 'apple' effectively. This addition is executed with careful attention to color consistency, positioning the 'apple' adjacent to the 'French bread' to complement the scene's overall palette. Each bounding box represents an individual NeRF, which means they can be manipulated through moving, scaling, and removal operations. CompoNeRF also extends its editing prowess to textual modifications, as evidenced by the 'wine cup' now appearing filled with juice—a change propagated through both subtexts and the global test. 
% %
% Since NeRFs have been well trained, we only finetune $\theta_g, \theta_l$ to align text prompts to promote consistency of both local and global views . 
% %
% Moreover, the NeRFs, once retrained within the reimagined scene, are also structured to be decomposable and cacheable for subsequent scene compositions.

% , as shown in Fig.~\ref{fig:framework}.
% For each scene described by the multi-object text prompt $T$, we
% To enhance the guidance of local representations, we use the local text prompt $T_l \subseteq T$ of a single object to optimize the local NeRFs in local views.
% The scene views $\hat{\boldsymbol{X}}_g=\{\hat{\boldsymbol{C}}_{g,i}\}_{i=1}^{H\times W}$ is obtained from the predicted pixel values of $H \times W$ rays by compositing all the ray-box interaction values.
% Similarly, the rendered view $\hat{\boldsymbol{X}}_{l,j}$ of the local frame $\boldsymbol{\theta}_j$ without compositing other objects can be calculated by $\hat{\boldsymbol{C}}_{l,j}$, as depicted in Sec.~\ref{ssec:render}.
% We use the local color instead of the globally calibrated color to obtain a local view because the local NeRF should learn the object identity unrelated to its placed position, as the position can be different during user edition.
% % Compared to cropping the local region from a global view for training, separate rendering can avoid the undesired information from other objects brought by the occlusion and resolution adjustments.
% Formally, we employ the following loss as the learning objective,
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/editing.pdf}
    % \vspace{-23pt}
    \caption{\textbf{The scene editing.} Demonstrated here are the stages of our recomposition, utilizing cached source scenes. Each NeRF is individually identified by colorful labels. These decomposed nodes are then positioned in the initial layout and subsequently calibrated to form the final composition. The detailed description of the ambient environment is underscored, enhancing the scene's realism.}
    \label{fig:app}
    % \vspace{-12pt}
\end{figure*} 

%
\subsubsection{Optimization}
\label{sec:optimization}
During optimization, our method employs dual text guidance to align rendering results with both global and local textual descriptions. The optimization objective is:
{
\small
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eqn:loss_f}
\mathcal{L}= {\alpha_g}\nabla\mathcal{L}_{\text{SDS}}(\hat{\boldsymbol{X}}_{g}, T) + {\alpha_l}\sum_{j=1}^{m} \nabla\mathcal{L}_{\text{SDS}}(\hat{\boldsymbol{X}}_{l,j}, T_{l,j}) + \beta\mathcal{L}_{\text{sparse}},\nonumber
\end{equation}
}where $T$ signifies the global text prompt, while $T_{l}$ pertains to a specific object within the global context. The hyperparameters $\alpha_{g}, \alpha_{l}$, and $\beta$ modulate the respective loss weights. 
% $\nabla \mathcal{L}_{\text{SDS}}$ is the score distillation sampling loss, as described in Sec.~\ref{sec:background}.
As suggested in~\cite{metzer2022latent}, we use $L_{\text{sparse}}$ included to penalize the binary entropy of local NeRFs' densities, thereby mitigating the issue of extraneous floating radiance.
Additionally, incorporating directional cues such as "front view" or "side view" into the input text, as suggested by \cite{poole2022dreamfusion,metzer2022latent} proves beneficial in specifying camera poses during the training phase, further enhancing the alignment of our generated scenes with the intended perspectives.
% Note that the global calibration in the scene frame can adaptively revise both $({C}_l, {\sigma})$ in local NeRF with $\nabla \mathcal{L}_{SDS}$ along with the back-propagating gradient.
 We refer readers to the pseudo-code in our \textit{suppl.} for our training procedure.
\section{Experiments}
\label{sec: exp}

% \subsection{Experimental Setup}
% \noindent\textbf{Evaluation Benchmarks.}
% To address the compositional generation, we propose a new benchmark which consists of 6 our pre-defined compositional prompts and 12 natural prompts from MSCOCO~\cite{lin2014microsoft} where each contains at least two color words modifying different objects. We also switch the position of two color words to create a contrast caption similar to ~\cite{feng2022training}.
% For our benchmark, we conduct user studies to evaluate different methods based on user preferences.

% \noindent\textbf{Evaluation Metric.}
% We mainly rely on human evaluations for the visual quality and generation accuracy of compositional prompts.
% We show users two videos side by side rendered from a canonical view by two different algorithms using the same text prompt. We ask the users to select the more realistic and detailed one from Latent NeRF~\cite{metzer2022latent}, SJC~\cite{wang2022score}, and our method, respectively and indicate which 3D models demonstrate better semantic alignment or image fidelity.
% Each prompt is evaluated by 3 different users, resulting in XXX pairwise comparisons.
% We also investigate the object missing rate in our proposed benchmark from human evaluation.


\subsection{Qualitative Comparison}
% In Fig.~\ref{fig:sota}, we present qualitative comparisons of generated 3D assets given the same multi-object text prompt with our proposed CompoNeRF method, as well as the Latent-NeRF and SJC, which are the SoTA methods based on the same Stable Diffusion model.
% We observe that our method can accurately generate complex 3D models across a diverse set of prompts with \textbf{precise object identity} and more \textbf{sensible and fertile context} than others in the showcases.
In Fig.~\ref{fig:sota}, we conduct qualitative comparisons of 3D assets generated using our method against Latent-NeRF and SJC, all based on the same Stable Diffusion model. Our method exhibits a remarkable ability to generate complex 3D models from a wide array of multi-object text prompts, demonstrating \textbf{superior object identity accuracy} and \textbf{enhanced context relevance and richness} compared to its counterparts.
In simpler Case 1, our CompoNeRF method adeptly generates two distinct objects: a red \emph{apple} and a yellow \emph{banana}. In stark contrast, competing methods amalgamate the features of both fruits into a singular, indistinct object.
%
Case 2-4, which is more intricate, showcases the capability to render a realistic scene with accurately depicted objects.
% such as \emph{wine}, \emph{table}, \emph{salad}, \emph{bread}, and the reflective properties of \emph{glass}. 
Conversely, our baselines struggle to produce even recognizable objects.
% Note that our fidelity can be improved with more advanced pretrained models such as DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d} to generate more realistic scenes. 
% Moreover, our underlying 3D representation framework is compatible with most object-centric methods~\cite{poole2022dreamfusion,lin2022magic3d}. 
% Once these models are publicly available, integration with our CompoNeRF can further enhance their multi-object modeling capability.
% , leveraging our Latent-NeRF backbone.
%
% For example, in the simple Case 3, our method accurately generates two separate objects, a red \emph{apple} and a yellow \emph{banana}. In contrast, other methods generate a single mixed object with characteristics of both fruits.
% %
% In the complex Case 5, we provide a realistic scene with accurate representations of \emph{wine}, \emph{table}, \emph{salad}, \emph{bread}, and \emph{table}, including the glass reflection. However, other methods fail to generate even recognizable objects.
% Note that we cannot validate the predicted results of DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d} model as they are built upon on close-sourced diffusion models.
% Besides, our underlying 3D representation can also be equipped with most object-centric methods~\cite{poole2022dreamfusion,lin2022magic3d} once they are released to achieve, allowing us to achieve better single-object modeling, similar to the use of our Latent-NeRF backbone.
\begin{table}[h]
% \scalebox{0.8}
\renewcommand{\arraystretch}{1.2}
\fontsize{4pt}{4pt}
\selectfont 
\centering
% \vspace{-8pt}
\resizebox{\linewidth}{!}
{
% \begin{tabular}{lcccccccc}
% \hline
% Method     & table\_wine    & tesla          & pyramid        & chess          & apple and banana      & astronaut      & glass\_balls   & Eiffel\_tower    \\ \hline
% LatentNeRF & 21.55          & 25.16          & 27.43          & 31.19          & 27.69          & 27.07          & 29.51          & 26.32          \\
% SJC        & 23.33          & 23.55          & 25.62          & 30.53          & 28.21          & 27.84          & 28.76          &27.41 \\
% \textbf{CompoNeRF(Ours)}     & \textbf{32.68} & \textbf{26.13} & \textbf{28.96} & \textbf{31.45} & \textbf{33.37} & \textbf{32.71} & \textbf{30.98} & \textbf{28.44}          \\ \hline
% \end{tabular}
\begin{tabular}{lcccc}
\hline
Method                   & Case 1         & Case 2         & Case 3         & Case 4         \\ 
\hlineB{1.1}
LatentNeRF               & 27.69          & 31.19          & 21.55          & 29.51          \\
SJC                      & 28.21          & 30.53          & 23.33          & 28.76          \\
\textbf{CompoNeRF (Ours)} & \textbf{33.37} & \textbf{31.45} & \textbf{36.06} & \textbf{30.98} \\ \hlineB{1.1}
\end{tabular}
}

% \vspace{-6pt}
\caption{\textbf{Quantitative comparison}. For our evaluation metric, we utilize the average of CLIP scores~\cite{parmar2023zero,zhang2023sine,wang2023imagen} across different views, which serve to assess the similarity between the generated images and the global text prompt. }
\label{tb:perclass}
\end{table}

\subsection{Quantitative Comparison}
In our study, we employ the CLIP score as the primary evaluation metric to assess the congruence between the generated 3D assets and the associated text prompts. This score, commonly used in text-to-image generation research as noted in studies~\cite{parmar2023zero,zhang2023sine,wang2023imagen}, is derived from the cosine similarity between the embeddings of the text and the image, both encoded by the CLIP model. For 3D assets, we project images from various views and calculate the CLIP score concerning the global text prompt, with the overall CLIP score being the average of these values. As detailed in \cref{tb:perclass}, our quantitative comparisons demonstrate the superior alignment of our method with the text prompt compared to the Latent-NeRF and SJC approaches in diverse scene configurations. Notably, in the challenging Case 3, our method shows a remarkable 54\% improvement. The result of overall enhancement in multi-object scenes underscores the robustness of our global calibration strategy. We refer readers to our \textit{suppl.} for additional results.
\subsection{Recomposition for more complex scenes}
To further validate CompoNeRF's performance for more complex multi-object text inputs, we assess its performance using a lengthy sentence describing the color, texture, light, and relationships between scene components.
%
Fig.~\ref{fig:app} showcases our refined scene renderings originating from pre-trained source scenes including the 'lamp' and the 'nightstand' from the 'bedroom' scene. Then we add two glass balls 'Murano' and 'Dichroic' from Case.4 in \cref{fig:sota}. 
%
%
We observe that a direct amalgamation of these components can manifest various artifacts at the base of the lamp and incongruous shadows and reflections from the glass ball are notable, detracting from the authenticity of the scene's ambiance.
%
After composition, reconfigured objects are adeptly integrated, achieving a coherent and consistent global scene. The changes in the materials of the lamp, the nightstand, and their reflective surfaces 
demonstrate the system's adaptability to diverse source inputs, which also involves a challenging text prompt containing subtle interplay within a multi-object context. 
%
In a nutshell, our method unleashes its potential by composing scenes with reusable NeRF components, which also facilitates the following editing for scenes described by lengthy sentences even with complex relationships among objects.

% We use CLIP score as our evaluation metric to measure the similarity of generated 3D assets to text prompt. 
% The CLIP score, a widely utilized evaluation metric in text-to-image generation tasks~\cite{parmar2023zero,zhang2023sine,wang2023imagen}, is calculated as the cosine similarity between the embeddings of the text and image, both of which are encoded by the CLIP model.
% For 3D assets, we calculate the CLIP score between the projected images of the 3D assets and the prompt text in different views and take the average score as the overall CLIP score. In \cref{tb:perclass}, we show quantitive comparison of the similarity between generated 3D assets and the same text prompt with the Latent-NeRF, SJC and our method across diverse scenes. As shown as \cref{tb:perclass}, we achieve the best performance in all scenes, with more significant performance improvement in complex scenes. For example, in \emph{table wine} scene, our method achieves a \textbf{54\%} gain. \textbf{The excellent performance in complex scenes highlights the effectiveness of our global calibration.}

% \noindent\textbf{Local and Global Text Guidance.}
% We conduct ablations to demonstrate the importance of introducing global and local level guidance discussed in Sec.~\ref{sec:optimization}. 
% % We show ablation with different loss settings. 
% In Fig.~\ref{fig:ab_loss}, we observe that our complete method (Ours) improves the generation accuracy of object identity and better geometry.
% Note that, without local-level SDS losses, the object frame of 'banana' in Fig.~\ref{fig:ab_loss} (a) even can not be rendered with any details, while without global-level SDS loss, the local frame of 'banana' in Fig.~\ref{fig:ab_loss} (b) can not generate the 'banana' at accurate number.
% The phenomenon is consistent with our observation that the generative ability of the pre-trained diffusion model may fail to provide accurate guidance for the multi-object text prompt.
% In Fig.~\ref{fig:ab_loss} (c), we also show the perturb and average scoring strategy~\cite{wang2022score} can generate better geometry results against the vanilla SDS losses~\cite{poole2022dreamfusion}.

% \noindent\textbf{Global Calibration.}
% We further study the global calibration by rendering a scene with glass balls made of different materials in Fig.~\ref{fig:ab_loss}.
% The rendered results show that with the global calibration procedure, the light reflection and shadow of all balls have more view consistency.
% While without global calibration, the blue ball renders shadow artifacts that are against scene coherence.

% \subsection{Quantitative evaluation of Compositional Cases}

% Missing Rate

% Failure Cases
% In Fig.~\ref{fig:sd}, we show that for some complex prompt, the Stable Diffusion can not generate accurate information and thus fail to generate faithful images. 



% The compositional flexibility of our editable 3D scene layout enables intricate scene edits, including text modifications, repositioning, scaling, duplication, and removal of objects, culminating in the recomposition of an entirely new scene through the manipulation of individual learned local NeRFs.

% warmth and the twilight's progressive dimming, or the detailed context surrounding the 'mahogany' nightstand.
%
% Adjusting existing objects involves simple modifications to the bounding box properties like the center point and scale. Moreover, these manipulations can be collectively employed to forge a scene that conforms to multiple user-defined directives, 
% Alternatively, we can further finetune the edited scene with a few finetuning steps to improve the view consistency.
% As the duplication, removal, and recomposition, we can first input the 3D boxes and then load each box with a local text prompt from a learned local NeRF collection, \eg, copy the single \emph{nightstand} into the other at opposite locations.
% Nonetheless, we are unable to solve the multi-face problem in this paper since our box layout guidance is ambiguous for prompts with direction. In this section, we discuss these two points.

% For text editing on a specific object, we simply change a certain part of the text prompt, \eg, 'a vase of sunflower' to 'a vase of rose' at both global and local text prompts and finetune the scene with a few steps.
% \noindent\textbf{Editable Scene Rendering and Finetuning.}
% Due to the compositional capacity brought by the editable 3D scene layout, we can perform scene editing by text editing, moving, scaling, duplicating, removing the single object, and re-composting a new scene by manipulating the layout of each learned local NeRF.
% Fig.~\ref{fig:app} shows our edited scene rendering results based on a pre-trained scene.
% For instance, We utilize the source scene and glass balls, and the bedroom. Then each decomposed local NeRF are listed in the decomposition part, when they are put together directly, artifacts at the bottom of the lamp, or inconsistency the ball shadow are not natural and no reflections, or lack of description of the overall scene, warm glow with darkening twilight, or the intricate description of the context of the nightstand, 'mahogany'. 
% We can see that the manipulated objects are seamlessly integrated into the scene while ensuring the correct spatial relationship following the edited layout.
% Specifically, the material changes in the lamp and the body, with the contex of the nightstand, reflections. 
% %
% When moving and scaling existing objects, we only need to adjust the box property, such as the center point and box scale.
% Furthermore, all types of manipulation can be combined to generate a scene with multiple user control inputs.
% \noindent\textbf{Limitations.} 
% We summarizes our constraints including, 
% 1) limited ability to handle complex semantics, like ones including human or uncommon integration of objects for scene reconstruction. 
% 2) Multi-face problem exists without explicit supervision such as mesh since we only use bounding boxes, \eg,  duplicated face views may appear. 
% We refer readers for more comprehensive evaluation including videos, visualization and description in out suppl. materials. 
% \noindent\textbf{Limitations.} Our method has certain limitations:
% 1) restricted semantics processing, particularly interpreting uncommon object integration.
% 2) multi-face issue in the absence more intricate supervision. 
% For more thorough evaluation, we direct readers to our \textit{suppl.} materials.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/abls_compo.pdf}
    % \vspace{-24pt}
    % \caption{Ablation study on text guidance. (a) without local SDS losses. (b) without global SDS losses. (c) vanilla SDS losses without perturb and average scoring~\cite{wang2022score}. (d) full model.}
    \caption{\textbf{The composition strategy.} Our proposed strategies for multi-object scene composition align with \cref{eq:multi_volrend}. The areas of NeRF overlap are indicated in gray. The green nodes represent composited samples. Our design is highlighted by the \underline{dashed box}.
    }
    \label{fig:abls_compo}
    % \vspace{-18pt}
\end{figure}

 % The 3D scene layout interpreted the multi-object text prompt as a set of local NeRFs binding with a spatial box and object-specific text prompt.
% The whole scene view is rendered by compositing local NeRFs defined in the layout.
% We also designed a composition module and multi-level text guidance for improving the quality of generated 3D scenes.
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/dis_pose.pdf}
    % \vspace{-2pt}
    \caption{\textbf{Effect of adjusting camera distances.} Scaling camera distance can benefit rendering quality.
    % The first row is the rendering results, and the second row visualizes the rays that hit local frames.
    }
    \label{fig:dis_pose}
\end{figure}
\subsection{Ablation Study}
\noindent \textbf{Composition Strategy.}
% 1. Apply $gt$ as the overarching supervision.   
% 2. Employ specific subtexts to direct each NeRF object.
% 3. Form composite samples by following step (2).
% 4. Refine the composite output with $gt$, building on step (3).
Our initial tactic, as shown in ~\cref{fig:abls_compo}(a), attempts to manage this by assigning each object to a distinct NeRF with a shared context, diverging from the traditional single-network scene encodings~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score}. 
This approach aims to improve object recognition, yet it's prone to ‘guidance collapse' due to the global text prompts' inability to offer precise semantic delineation for individual objects.
%
Inspired by more accurate rendering of single objects with targeted subtext prompts as seen in \cref{fig:intro}, the subsequent refinement in (b) utilizes specific textual supervision for each object. This tailored guidance confirms that diffusion models more effectively render individual objects when provided with explicit textual for each, thereby overcoming the ‘guidance collapse’ issue in complex multi-object scenarios.
%
% However, when rendering NeRFs directly, we observe the occlusion is not correct in their overlapped regions due to lack of global refinement. 
Nevertheless, the direct rendering of NeRFs often results in incorrect occlusions within their overlapping regions, indicating a deficiency in global refinement.
%
To improve it, as detailed in (d), we refine sampling points that were originally guided by texts corresponding to individual objects, now with the incorporation of global textual guidance. This supplementary global oversight guarantees a harmonious rendering that upholds the unique identities of the objects while fostering overall compositional unity. Its vital role is underscored by its absence in (c), where its omission brings collapsed results. We refer readers to our \textit{suppl.} for more ablation studies on this composition strategy. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/sup_chess.pdf}
    \vspace{-15pt}
    \caption{
    % 1)The local NeRF also learns global context. 2) The global composition module learns stronger global semantic context. 3) The inconsistent object distribution may lead to ignoring objects in later training.
    \textbf{Recomposition process}. Individual components are labeled in \textcolor{blue}{blue}, while \textcolor{red}{red} boxes emphasize areas of contrast.
    }
    \label{fig:obs}
\end{figure*}
\noindent \textbf{Optimizing Diffusion Model Guidance for Scene Resolution.}
A critical aspect is the need to strike a balance between the overall scene's resolution and the rendering details. For example, when a single object is placed within a vast scene, its rendering results may not be clear as it occupying only a few pixels. This small pixel footprint can limit the amount of gradient information received during backpropagation.
%
\cref{fig:dis_pose} illustrates this scenario using the same text prompt but with varying scales of the global frames, ranging from 0.3 to 0.7. The results underscore a key insight: the more pixel rays an object interacts with, the better the quality of its rendering. This finding is particularly relevant for large-scale scene rendering, where multiple local frames coexist within the same space. A small object in such a setup may receive minimal ray-box interactions, potentially leading to training inefficiencies or collapse.
%
It's also important to consider that our scene models are optimized in the latent space of the Stable Diffusion model, which has a feature resolution of 
$64\times 64$. However, we decode these latent color features into RGB images with a resolution of 
$128\times 128$. This discrepancy affects the density of rays throughout the space and, by extension, the number of objects that can be effectively rendered.
%
% Therefore, reconstructing large-scale scenes using our current model settings remains a challenging issue. 
% As demonstrated in our papers, even small-scale scenes require approximately 5 hours of processing to learn from scratch, indicating potential limitations for larger-scale applications.
% \noindent \textbf{Resolution of Diffusion Model Guidance.}
% One suggestion for applying our framework design is that the resolution of the whole scene and rendering details need to be balanced well.
% For instance, a single object is placed in a large scene. Its rendered view becomes very small, resulting in few pixels with response to receive gradients during backpropagation.
% Fig.~\ref{fig:dis_pose} shows the results using the same text prompt varying scales of the global frames vary from 0.3 to 0.7.
% It reveals that the more pixels rays interact with, the better the rendering quality. 
% This observation is important for large scene rendering, as there are multiple local frames arranged in the same space with the same ray density. Therefore, it may result in a few ray-box interactions received by a small object, leading to the training collapse. 
% Please also note that we optimize the scene model in the latent space of the Stable Diffusion model, in which the feature resolution is $64\times 64$, 
% while we decode the latent color feature into RGB images with resolution $128\times 128$, which limits the ray density of the whole space and the subsequent object numbers. 
% Therefore, it's still an tricky issue for the large scale scene reconstruction using the current model settings, since the small scenes present in our papers need around 5 hours already. It may be restrictive for the larger scale usage. 
 % $64\times 64$


\noindent \textbf{Influence of Global MLP Size on Composition Capabilities.}
The complexity of a scene directly influences the required configuration of the parameters \(\boldsymbol{\theta}_g\) within the composition module. In Figure~\ref{fig:ab_mlp_size}, we experiment with varying the number of layers in the MLPs responsible for both densities \(f_{\boldsymbol{\theta}_{g_d}}\) and color calibration \(f_{\boldsymbol{\theta}_{g_c}}\). The results indicate that an insufficiently representative MLP can fail to preserve the distinct identities of individual NeRFs. For instance, the 'white king chess' piece struggles to manifest its characteristic whiteness, leading the global calibrators to compensate inadequately by projecting a flattened representation onto the chessboard surface.
%
By increasing the number of MLP layers, we observe a notable improvement in the accurate portrayal of each object's identity and overall scene quality. For example, the contextual details of the chessboard, like its grid pattern, are rendered more clearly and naturally.
%
Based on these findings, we recommend fine-tuning this hyperparameter to align the composition module's capabilities with the specific complexity of the scene.

% \noindent \textbf{The size of global MLP decides the composition capability.}
% The parameters $\boldsymbol{\theta}_g$ in the composition module need to be adapted to the scene complexity. As shown in \cref{fig:ab_mlp_size}, we vary the number of MLP layers for both density $f_{\boldsymbol{\theta}_{g_d}}$ and color calibrators $f_{\boldsymbol{\theta}_{g_c}}$. 
% The result reveals that inadequate representation may lead to the missing identity for individual NeRFs, such as the 'white king chess' has a limited ability to depict itself as white, while the global calibrators struggle to make a compliment by adding a flattened 'white king chess' on the chess board surface. 
% %
% After increasing the MLP layers, the situation is getting better with an accurate representation of identity and enhanced quality for each NeRF, \eg, the chessboard context of the grid becomes more evident naturally. 
% %
% We recommend finetuning this hyperparamter to adjust the composition ability depending on the scene complexity. 
 
% \noindent \textbf{Don't rely too much on global composition.}
% As shown in \cref{fig:obs}, composition may introduce artifacts if the gap between target semantics and the source is large. This can judged by the comparison of visual rendering results. For instance, the target \emph{juice} has large color and geometric differences with the original \emph{wine}. On the contrast, replacing the original \emph{salad} by an \emph{apple} is much easier, since there is no careful calibration process needed. Thus,
% it will be helpful to change a more similar decomposed NeRF model, or train another from scratch to tackle this issue. 
\begin{figure}[t]
    \centering
    \vspace{-10pt}    \includegraphics[width=\linewidth]{figures/sup_mlp_size.pdf}
    \vspace{-18pt}
    \caption{\textbf{Effect of MLP sizes. } Comparison of different parameter sizes for composition. }
    \label{fig:ab_mlp_size}
    % \vspace{-12pt}
\end{figure} 

\noindent \textbf{Global Context Assimilation by Local NeRFs and Composition Module.} 
% As illustrated in \cref{}, our composition module integrates global context, accommodating comprehensive semantic details such as reflections on surfaces. 
Despite the primary embedding of the context within the composition module, local NeRFs exhibit an ability to partially learn these global attributes. For instance, \cref{fig:obs} shows that initially, the local NeRF optimization does not occur in isolation; the table from Case 3 in \cref{fig:sota}, for example, bears a residual shadow from the 'French bread' in the original configuration as depicted in the upper left corner.
%
As the training progresses, these anomalies are resolved, and the local NeRFs gradually assimilate aspects of the global texture, albeit to a limited extent. For instance, while the black and white pattern of the chessboard is predominantly captured by the global composition module, the local representation of the table, highlighted in red box in their upper left corners, remains unchanged. However, as iterations advance,
%
despite there's no NeRF that is responsible for 'chess board', 
%
the global frame begins to discern it as the necessary environment and replicate the underlying 'chessboard' pattern. 
%
This reveals that NeRFs can initially embed global environmental context, while the composition module can possibly merge some necessary local patterns for consistency. 
On the other hand, the early stop may benefit 
the potential degradation in specific elements, such as the 'wine', which worsens as training progresses, as evidenced by the local frames' comparison in the upper left corner. Concurrently, the global rendering depicts the 'wine' as nearly imperceptible. This deterioration hints at the possibility that continued optimization may inadvertently diminish the representation of certain objects.
%
% This approach aims to maintain an equilibrium between rendering fidelity and semantic coherence, preventing the over-attenuation of less common scene elements.

% \noindent \textbf{Both local NeRFs and the composition module learn the global context.} As we have shown in Fig. 1 of the main paper, the composition module will introduce global connections to fit the full semantic context, such as the reflection on the bed. However, we also discover that the local NeRF has learned part of it, despite most of its context being embedded in the composition module. For instance, in \cref{fig:obs}, we observe that the initial local NeRF is not independently optimized, \eg, 
% %
% the table has some visible shadow of \emph{french bread} from the original scene. 
% After some time, this inconsistent feature will be eliminated. 
% Moreover, at later training iterations, this local representation continues to learn some global texture, while it remains trivial. For example, the majority of the black and white pattern on the table is still attributed to the global composition module. Specifically, the table in the local frame magnified in red stays the same, while the global frame starts to learn the hidden \emph{chess board}. 









\section{User Study}

In order to comprehensively evaluate the quality of the quality of 3D assets generated using our method, we conducted a user study involving 15 participants. The study consisted of three tasks: a composition correctness task, a generative quality evaluation task, and an object identification task.
The specific design of each task will be elaborated in subsequent sections. Data collection involved both quantitative analysis, through subjective ratings, and qualitative research, carried out via semi-structured interviews.

\subsection{Participants}
% The study involved 27 participants, with 66.7\% aged 18-24 and 33.3\% aged 25-34. The gender distribution was 59.3\% male and 40.7\% female. Also, 51.9\% of the participants had previous experience with 3D generative models, and 40.74\% of the participants had no Blender experience in the last six months. As shown in Fig.~\ref{fig: participants}, the experiments were conducted by watching videos recorded from the generated 3D assets.
The study included 11 participants, with 54.5\% falling within the 18-24 age group and 45.5\% in the 25-34 age range. The gender distribution was 63.6\% male and 36.4\% female, reflecting a balanced representation. In terms of prior experience, 18.2\% of participants had some familiarity with 3D generative models, while having experience with Blender within the last six months. As depicted in Figure~\ref{fig:user}, the experimental procedure involved participants viewing videos showcasing the 3D assets generated by the model. This approach allowed participants to evaluate the outputs in a controlled and standardized manner, ensuring consistent exposure to the same visual stimuli across all participants.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/user_1.pdf}
    % \vspace{-24pt}
    % \caption{Ablation study on text guidance. (a) without local SDS losses. (b) without global SDS losses. (c) vanilla SDS losses without perturb and average scoring~\cite{wang2022score}. (d) full model.}
    \vspace{-15pt}
    \caption{\textbf{User study with our questions demo.}
    }
    \label{fig:user}
    % \vspace{-18pt}
\end{figure}

\subsection{Task design}
% 1. semantic corresponding; 2. textual quality; 3. multi-view alignment
\begin{itemize}
% \item \textbf{Generative Consistency Evaluation.}
% In this task, we evaluate the consistency of generative 3D assets in two views, including semantic consistency and multi-view consistency. We collected four groups of samples and each group included the 3D assets generated by Latent-NeRF, SJC, and our method with the same text prompt. During the study, participants were asked to validate the generative consistency from semantic and multi-view from 1 (low) to 7 (high).

% \item \textbf{Generative Quality Evaluation.}
% We provided four groups (refer to the suppl.) of the generated 3D assets for each participant. For each group, the 3D assets were generated by Latent-NeRF, SJC, and our method with the same text prompt respectively. And then we asked participants to evaluate the quality of the generated 3D assets. Finally, we evaluated the ability of our method for multi-object generation and combination by calculating the match rate between the objects found by participants and the objects in the prompt.


% \item \textbf{Find Objects.}
% In this task, we selected six samples of the 3D assets generated by our method. And then we asked participants to find the objects presented in the 3D assets. 
\item \textbf{Composition Correctness Evaluation.} In this task, we assess the consistency of the generated 3D assets across two views, focusing on both semantic consistency and multi-view consistency. We collected four groups of samples, each comprising 3D assets generated by Latent-NeRF, SJC, and our method, using the same text prompt. Participants were asked to evaluate the generative consistency from both semantic and multi-view perspectives on a scale from 1 (low) to 7 (high).

\item \textbf{Generative Quality Evaluation.} We provided four groups of generated 3D assets (refer to the supplementary material) to each participant. For each group, the 3D assets were created using Latent-NeRF, SJC, and our method, all based on the same text prompt. Participants were then asked to assess the quality of these generated assets. Finally, we evaluated our method's capability for multi-object generation and combination by calculating the match rate between the objects identified by the participants and those described in the prompt.

\item \textbf{Object Identification.} For this task, we selected four samples of 3D assets generated using our method, comprising a total of seven objects. Participants were then asked to identify the objects depicted in these assets. To evaluate the multi-object generation and combination capabilities of our approach, we calculated the accuracy of the participants' object recognition.



\end{itemize}


\subsection{Measurements}
In the first task, we utilized a 7-point Likert scale to measure participants' perceptions across two dimensions, including semantic consistency and multiview consistency. For the second task, we also used the 7-point Likert scale to evaluate the generative quality and made a comparison with existing works, including Latent-NeRF and SJC. Lastly, we evaluate the effectiveness of our method for multi-object combinations by counting the accuracy of subjects in finding the objects provided in the prompt from the generated 3d assets. We also encourage participants to provide feedback on their emotional responses
and immersive experiences during the overall experience. For details of questions, please refer to \textit{suppl.}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/user_result.pdf}
    \vspace{-10pt}
    % \caption{Ablation study on text guidance. (a) without local SDS losses. (b) without global SDS losses. (c) vanilla SDS losses without perturb and average scoring~\cite{wang2022score}. (d) full model.}
    \caption{(a) Generative Consistency Evaluation (Semantic); (b) Generative Consistency Evaluation (Multi-view); (c) Generative Quality Evaluation; (d) Object Identification Task.
    }
    \label{fig:user_result}
    \vspace{-8pt}
\end{figure}


\subsection{Results}
As shown in Fig.~\ref{fig:user_result}(a), our method demonstrates significantly improved semantic consistency across all four cases, markedly outperforming previous approaches.
As shown in Fig.~\ref{fig:user_result}(b), benefiting from the reduction of semantic chaos, the multi-view consistency is also better than previous methods.
As shown in Fig.~\ref{fig:user_result}(c), in terms of generative quality evaluation, including the rendering quality of videos and object details, the performance of the three methods is comparable due to their utilization of the same Stable Diffusion version and identical training resolutions.
As shown in Fig.~\ref{fig:user_result}(d), the experiments demonstrate that the majority of our generated 3D scenes are correctly recognized, affirming the semantic consistency between the generated content and the input prompt.

\noindent \textbf{Discussion:} Our observations indicate that users prioritize the level of detail in individual objects when assessing the quality of generated scenes, even if the objects' semantics do not match the given text prompts. For example, as shown at our \textit{suppl.}, when presented with the prompt 'a football and a basketball,' users favored our baseline results, which depicted a single 'football' or 'basketball' with more intricate details, thus appearing more realistic. 
In contrast, our generated scenes, which included more objects, were perceived as less clear due to the increased complexity in rendering.


\section{Conclusion and Future Work}
In this work, we have proposed 
% a multi-object text-guided compositional 3D scene generation framework, called CompoNeRF, 
a novel framework for multi-object text-guided compositional 3D scene generation with an editable 3D scene layout.
% based on an editable 3D scene layout.
Our framework interpreted a multi-object text prompt as a collection of localized NeRFs, each associated with a spatial box and an object-specific text prompt, which were then composited to render the entire scene view. We have further enhanced the framework with a specialized composition module for global consistency, effectively mitigating the issue of guidance collapse in the multi-object generation. Utilizing Stable Diffusion model, we have demonstrated that our method, the first to apply a compositional NeRF design to the text-to-3D task, can produce high quality 3D models that feature multiple objects and perform well compared with contemporaneous methods.
% Despite that our flexible box layout may not be direction-aware and is unable to solve the multi-face problem, we have addressed the guidance collapse issue well in the multi-object scene reconstruction. 
% Working with the large-scale Stable Diffusion model, we demonstrated that our method, the first attempt to adopt compositional NeRF design in the text-to-3D task, can generate compelling 3D models with multiple objects, comparing favorably to available concurrent works. 
Looking ahead, we have explored a promising application of CompoNeRF in the realm of scene editing, which allows for the reuse of trained models in scene recomposition. This capability opens up new possibilities and identifies a rich vein of future work to be pursued in this domain.
% Finally, we investigated an exciting application of our method for scene editing and reusing trained models for scene recomposition, identifying an avenue for future work.

% Currently, our method relies on the user input 3D layout to render a text-guided scene, which can be further optimized by adopting a learnable dynamic layout to make the compositional pipeline in an end-to-end way. 
% To mitigate the influence of unconstrained generation ability of diffusion model, we can further automatically calibrate the language information for diffusion model to generate more consistent views. 
% Besides, to achieve more realistic scene editing, it is also promising to integrate the scene lighting model into the framework in the future work.

% \acknowledgments{
% The authors wish to thank A, B, and C. This work was supported in part by
% a grant from XYZ.}

%\bibliographystyle{abbrv}
\bibliographystyle{abbrv-doi}
%\bibliographystyle{abbrv-doi-narrow}
%\bibliographystyle{abbrv-doi-hyperref}
%\bibliographystyle{abbrv-doi-hyperref-narrow}

\bibliography{template}
\clearpage


%% This is how authors are specified in the conference style

%% Author and Affiliation (single author).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}}
%%\affiliation{\scriptsize Allied Widgets Research}

%% Author and Affiliation (multiple authors with single affiliations).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com} %
%%\and Ed Grimley\thanks{e-mail:ed.grimley@aol.com} %
%%\and Martha Stewart\thanks{e-mail:martha.stewart@marthastewart.com}}
%%\affiliation{\scriptsize Martha Stewart Enterprises \\ Microsoft Research}

%% Author and Affiliation (multiple authors with multiple affiliations)

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\firstsection{Supplementary Material}
\label{sec:sup}
We provide more details of the proposed method and experimental results in the
supplementary material.
Sec.1 and Sec.2 provide the algorithm and more implementation details.
Sec.3 provides more insights into our CompoNeRF model.
Sec.4 adds more details of visualization results. 
Sec.5 lists our attached material details for both scene reconstruction and editing.
% In summary, the whole algorithm to train the proposed
% CompoNeRF is shown in Algorithm ~\ref{alg:cap}.
% \begin{algorithm}[tb]
% \caption{Example algorithm}
% \label{alg:algorithm}
% \textbf{Input}: Your algorithm's input\\
% \textbf{Parameter}: Optional list of parameters\\
% \textbf{Output}: Your algorithm's output
% \begin{algorithmic}[1] %[1] enables line numbers
% \STATE Let $t=0$.
% \WHILE{condition}
% \STATE Do some action.
% \IF {conditional}
% \STATE Perform task A.
% \ELSE
% \STATE Perform task B.
% \ENDIF
% \ENDWHILE
% \STATE \textbf{return} solution
% \end{algorithmic}
% \end{algorithm}
\section{Algorithm}
\label{sec:algo}
The detailed algorithm of training our proposed
CompoNeRF is shown in Algorithm ~\ref{alg:cap}.


\begin{algorithm*}[tb]
\caption{Training for CompoNeRF}\label{alg:cap}
\textbf{Input}: a pre-trained text-to-image diffusion model $\phi$, multi-object text prompt $T$ and a set of boxes for 3D scene layout.
\textbf{Output}: learned parameters of local NeRFs $\{\theta_{l,i}\}_{i=1}^{m}$ and Global MLP $\theta_g$.
\begin{algorithmic}[1]
\FOR{\text{\textbf{Iter} $=0 <$ \text{\textbf{MaxIter}}} }
\STATE  Sample $H\times W$ rays from the random camera position and add the directional prompt into $T$.
\FOR{$i=0~\text{to}~H\times W$}

\STATE  Calculate the ray-box intersection for ray $r_i$ to get $m_i$ hits.

\FOR{$j=0 $ to $m_i$}{
    \STATE  Sample $N$ points with normalized location in the $j$ hit  local frame.
    \STATE  Calculate color $\boldsymbol{C_l}$ and density $\boldsymbol{\sigma}_l$ for each point from $\theta_{l,j}$.
    \STATE  Calculate the volumetric rendering color $\hat{\boldsymbol{C_{l,i,j}}}$ of the local Frame for ray $r_i$.}
\ENDFOR
\ENDFOR
\STATE  Map all points into global locations and sort them according to the depth.
\STATE  Calculate the calibrated color $\boldsymbol{C}_{g,i}, \boldsymbol{\sigma}_{g,i}$ via Eq.~4 and Eq.~5 for each point.
\STATE  Calculate the global volumetric rendering color $\hat{\boldsymbol{C_{g,i}}}$ for each ray $r_i$.
\ENDFOR
\STATE  Generate the local view from $\{\hat{\boldsymbol{C_{l,i,j}}}\}_{i=1}^{H\times W}$ and the global view from $\{\hat{\boldsymbol{C_{g,i}}}\}_{i=1}^{H\times W}$
\STATE  Perform score distillation sampling on the local render view and the global render view.
\STATE  Update network parameters via an Adam optimizer.
\end{algorithmic} \textbf{Eng}: Decompose local NeRFs and cache them into offline dataset.
\end{algorithm*}
\section{Implementation Details}
\label{sec:imple}
For score distillation sampling, we use the v1-4 checkpoint of Stable Diffusion based on the latent diffusion model~\cite{rombach2022high}. 
We utilize the code-base~\cite{metzer2022latent} for 3D representation and grid encoder from Instant-NGP~\cite{muller2022instant} as our NeRF model. The global MLP consists of 4 or 6 Linear layers with 64 hidden channels.
In the training loss, we set $\alpha_g=100, \alpha_l=100$, and $\beta=5e^{-4}$ if without specification.
Our 3D scenes are optimized with a batch size of 1 using the Adam~\cite{kingma2014adam} optimizer on a single RTX3090.
%
Our global frame is centered at the world origin and has a normalized side length of [-1,1]. To generate camera positions, we uniformly sample points on a hemisphere that covers the global frame with a random radius between 1.0 and 1.5.
The camera distance can also be scaled in the way discussed in the main paper. Plus,  
cameras are oriented to look toward the objects. During optimization, the camera field of view is randomly sampled between 40 and 70 degrees. At test time, the field of view is fixed at 60 degrees.
We use the Adam optimizer and perform gradient descent at a learning rate of 0.001 for 5,000 steps in simple prompts, such as ``apple and banana``, and 8,000 steps in more complex prompts for better quality. 
We follow the implementation of SJC~\cite{wang2022score} to perform the averaging implicitly, relying on the optimizer's momentum state when applying the perturb-and-average scoring strategy during training.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/sup_module.pdf}
    % \vspace{-18pt}
    \caption{Ablation study on module designs with the scene bedroom. (a) without global calibration. (b) without global text loss. (c) color-based design. (d) our density-based design.}
    \label{fig:sup_module}
    % \vspace{-12pt}
\end{figure} 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ab_calibration.pdf}
    % \vspace{-18pt}
    \caption{Multi view results with the text prompts on \cref{fig:sup_module}. Subtexts for individual NeRFs are highlighted in bold. }
    \label{fig:ab_view}
    % \vspace{-12pt}
\end{figure} 



\section{Discussion}

% Therefore, we needs to trade off the computation efficiency and rendered results quality in the 3D scene layout generation with CompoNeRF.

\noindent \textbf{More ablations on the design of composition module.}
In \cref{fig:sup_module}, we present further results from the ablation study of our composition module. As outlined in our main manuscript, our preference for a density-based approach is due to its effective and precise calibration of global density.
%
For example, the 'bedroom' scene builds upon the discussion from Fig.2(b.2) in the main paper. The complementary study in \cref{fig:sup_module}(a) demonstrates that direct global text supervision without compositional integration leads to a loss of material context, washing out the 'bed' and 'nightstand' in white. Conversely, \cref{fig:sup_module}(b) illustrates that omitting global text and relying solely on subtext supervision retains the familiar context of a 'white sheet' bed and a polished tan 'nightstand'. However, this approach introduces geometric inconsistencies, such as an overly tall nightstand and a lamp lacking a base, along with an absence of light reflection in the surrounding space.
%
The application of our composition module, depicted in \cref{fig:sup_module}(c) and 8(d), reveals that the density-based design affords enhanced control over density and, consequently, finer geometry. Instead of an empty space above the nightstand, the design aims to adjust the nightstand's height to achieve scene harmony, although it still exhibits limitations in controlling density, leading to subdued floating radiance.
%
\cref{fig:ab_view} provides a comprehensive visual comparison within the 'bedroom' context. When global calibration is absent, as seen in \cref{fig:ab_view}(a), the scene is plagued by sparse holes and a loss of color and texture detail. Neglecting the global branch entirely, as shown in \cref{fig:ab_view}(b), results in a lack of global consistency, evident in the disproportionate size of the 'nightstand' relative to the scene. Finally, the color-based solution in \cref{fig:ab_view}(c) fails to effectively correct the geometry, introducing additional artifacts. In contrast, the full model in \cref{fig:ab_view}(d) exhibits a marked improvement in these aspects.
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/dis_shape.pdf}
    % \vspace{-25pt}
    \caption{\textbf{(Left)} we observe the multi-face problem, \textit{e.g.}, duplicated face views with geometry collapse in all methods, even in single-object cases. \textbf{(Right)} We provide mesh as guidance instead of box layouts to solve this problem, which further proves our method's versatility and effectiveness.}
    \label{fig:dis_shape}
\end{figure}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/sup_float.pdf}
    % \vspace{-25pt}
    \caption{
An ablation study examines layout editing and the floating issue. The upper row shows the layout editing and the lower row indicates rendering views.\textbf{(Left)} Renderings exhibit floating objects due to a suboptimal layout. \textbf{(Right)} Improved outcomes following layout refinements.}
    \label{fig:layout}
\end{figure}
\noindent \textbf{Addressing the multi-face issue with enhanced prompts.}
Much like Latent-NeRF and SJC, our CompoNeRF framework encounters the multi-face challenge, where guidance from the Stable Diffusion model may result in conflicting facial features for certain objects, as illustrated in Figure~\ref{fig:dis_shape}. The reason lies in the fact that diffusion model does not always provide reliable guidance that aligns with the desired orientation corresponding to the camera's viewpoint during sampling.
%
To mitigate the multi-face problem, stronger constraints can be introduced to promote geometric consistency within the 3D representation. CompoNeRF incorporates mesh constraints, akin to those utilized in Latent-NeRF, offering a more detailed 3D layout compared to traditional bounding boxes. As demonstrated in Figure~\ref{fig:dis_shape}, the implementation of exact mesh constraints markedly mitigates the multi-face issue, though it may come at the expense of detail and adaptability.
%
Nevertheless, the requirement for accurate mesh input necessitates considerable manual editing, which may reduce the method's range of applications. Despite this, our approach illustrates that the 3D scene layout can be readily adapted to accommodate a broader range of input prompts. Further study is needed to solve the persistent multi-face issue in the text-to-3D tasks.

\noindent\textbf{Adjusting Layout to address Floating Artifacts.}
The process of scene composition begins with strategically positioning NeRFs within the predefined layout.
%
An overlap of object bounding boxes is critical, as highlighted in Fig.2 of the main document, to facilitate the generation of convincing scenes.
%
In our investigations, demonstrated in \cref{fig:layout}, we identified a 'floating' issue when bounding box overlaps are absent. This issue may stem from the regularization behavior within NeRFs, where the radiance fields—specifically, the regions responsive to gradient interactions, symbolized by ellipses centered on the boxes—fail to intersect. Such non-interaction can pose challenges, as it does not provide the necessary contiguous context for the global semantics to incorporate these objects seamlessly.
%
To rectify this, one straightforward approach we recommend is the judicious repositioning of bounding boxes to introduce overlaps. For example, a slight downward adjustment of a box can instigate detectable overlaps during training, facilitating better integration.
%
This insight opens up a potential avenue of research into the interplay between layout configurations and NeRFs, offering the possibility of more nuanced control over scene dynamics without the need for explicit layout modifications.
% \noindent\textbf{Adjusting Layout to Avoid the Floating Issue.}
% The composition process is initiated by allocating NeRFs to the setup layout. 
% %
% We observe that overlap among the object bounding boxes, as indicated in Fig.2 of the main paper, is necessary for a plausible scene generation. 
% %
% For instance, our experiment in \cref{fig:layout} exemplifies the floating issue that exists without leaving the bonding box overlap. The problem may originate from the NeRF regularization, as we empirically observe that the radiance field, in other words, ray hitting that has gradient response, denoted by the eclipses centering at the boxes' centers doesn't interact with each other. This may bring troubles like the global semantics may not accommodate them accordingly as they lack the necessary contact to support their joint adjustment. 
% Facing this issue, is one of the easiest solutions as we suggest moving the boxes to create the overlap, \eg, moving the box downward a bit, to enable their overlap can be detected during the training process. 
% This can also be a new direction for people to study the relationship between layout and NeRF for more adaptable control without explicit layout refinement. 


% \noindent\textbf{Multi-face Problem and Stronger Prompt.}
% % The CompoNeRF also has several limitations related to diffusion guidance.
% Similar to Latent-NeRF and SJC, the guidance generated from Stable Diffusion may produce a multi-face problem for certain objects as shown in Fig.~\ref{fig:dis_shape}.
% The diffusion model can not guarantee to generate satisfactory guidance with the desired direction along with the sampling camera pose.
% % In other words, the constraint provided by our 3D layout still can not force the diffusion model to generate satisfactory guidance for each specific object across different rendering views.
% An alternative to relieve the multi-face problem is adding stronger constraints to force the 3D representation to maintain geometric consistency.
% Our method also uses the mesh constraint, proposed by Latent-NeRF, as a more fine-grained 3D layout than the 3D box.
% Fig.~\ref{fig:dis_shape} shows that the multi-face problem can be largely relieved with the more accurate mesh constraint.
% However, accurate mesh input requires extensive editing, which reduces practical values during application.
% % However, we show that our 3D scene layout can be easily extended to more general types of input prompts.
\begin{table*}[t!]
\centering
\resizebox{\linewidth}{!}{ 
    \begin{tabular}{l|c|c|c|c|c|c}
    \toprule
    % \hlineB{2.5}
    \textbf{Methods} & \textbf{Diffusion Model} & \textbf{\textbf{3D Representation}} & \textbf{Scene Rendering} & \textbf{Input Prompt} & \textbf{Scene Editing} & \textbf{recomposition}\\  \hlineB{2.5}
    DreamFusion & Imagen & Mip-NeRF 360~\cite{Barron2021MipNeRF3U} & Object-centric & Text & T & \XSolidBrush\\    \hline
    Magic3D & eDiff-I + SD & Instant-NGP~\cite{Mller2022InstantNG} & Object-centric & Text & T & \XSolidBrush\\    \hline
    DreamBooth3D & DreamBooth+DreamFusion & Mip-NeRF~\cite{Barron2021MipNeRFAM} & Object-centric & Text+Images & T & \XSolidBrush \\    \hline
    Points-to-3D & ControlNet+Point-E & Instant-NGP & Object-centric & Text+Image & T & \XSolidBrush \\  \hline
    Fantasia3D & SD+PBR & DMTET~\cite{Shen2021DeepMT} & Object-centric & Text/Fine Shape & T/M/S/R & \Checkmark \\
    \hline
    Latent-NeRF  & SD V1.4 & Instant-NGP & Object-centric & Text+Fine Shape & T & \XSolidBrush \\    \hline
    SJC~\cite{wang2022score} & SD V1.5 & voxel radiance field & Object-centric & Text & T & \XSolidBrush\\  \hline
    \hline
    Set-the-Scene~\cite{Cohen-Bar_Richardson_Metzer_Giryes_Cohen-Or_2023} & SD V2.0 & - & Object-compositional & Text+3D Layout & T/M/S/R & \Checkmark  \\ 
    \hline
    \textbf{Ours} & SD V1.4 & Instant-NGP & Object-compositional & Text+3D Layout & T/M/S/R & \Checkmark \\  
    \bottomrule
     \hlineB{1.5}
    \end{tabular}
}
% \vspace{-7pt}
\caption{\textbf{Comparison of our method with the related works for text-to-image generation}. SD denotes Stable Diffusion. For scene editing, we use T(editing object with text), M(moving object), S(scaling object), and R(removing object) for short.}
% \vspace{-12pt}
\label{tab:compare}
\end{table*}
\begin{figure*}[t]
    \centering
    % \vspace{-6pt}
    \includegraphics[width=\linewidth]{figures/sup_fail.pdf}
    \caption{The failure cases are categorized as follows: \textbf{(Left)} issues encountered during scene reconstruction, and \textbf{(Right)} challenges arising in scene editing. The outputs from Stable Diffusion selected for illustration represent the most frequently occurring types generated by the model.
 }
    \label{fig:fail}
\end{figure*}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/sup_semantic.pdf}
    \caption{
    The failure cases with their color-labeled NeRFs components are shown beside. The textual guidance manipulation on global/local weights is shown below.  
    }
\label{fig:sup_semantic}
\end{figure*}
% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/sup_edit.pdf}
%     \caption{
%         xxx
%     }
%     \label{fig:sup_edit}
% \end{figure*}



% \noindent \textbf{Early Termination in the Scene Composition.} Our method's joint optimization aims to enhance the congruence between rendered views and multi-level text prompts. In later stages of training, the focus predominantly shifts to reinforcing global consistency as local fidelity typically reaches satisfactory levels by this point.
% %
% The observations from the experiment depicted in \cref{fig:obs} suggest a potential degradation in specific elements, such as the \emph{wine}, which worsens as training progresses, as evidenced by the local frames' comparison in the upper left corner. Concurrently, the global rendering depicts the \emph{wine} as nearly imperceptible. This deterioration hints at the possibility that continued optimization may inadvertently diminish the representation of certain objects.
% %
% Given these findings, we advocate for an early termination strategy when training models on less conventional scene compositions. This approach aims to maintain an equilibrium between rendering fidelity and semantic coherence, preventing the over-attenuation of less common scene elements.
% \noindent \textbf{Composition of unusual scene needs the early stop. } Since our joint optimization will strive to enhance the similarity between the rendering views and multi-level text prompts. The later training may mainly focus on improving the global level consistency, since the local ones are sufficiently well. 
% %
% The experiment shown in \cref{fig:obs} reveals that \emph{wine} is deteriorated with the training continues as indicated by the comparison of local frames on the upper left corner. On the other hand, the global rendering result shows \emph{wine} is nearly invisible. It may brings inaccurate predictions considering that later optimiztion would keep mute the objects. 
% %
% Therefore, we suggest for the early stop when training for less common scene composition, which strikes a balance between the rendering quality and semantics consistency.
 
\noindent \textbf{Analysis of Failure Cases in Scene Composition and Editing.}
%
Our composition module may sometimes fail to produce coherent scenes, often due to limited text description distributions within the training data of diffusion models as illustrated in \cref{fig:fail}. This can be mitigated by adjusting the loss weights governing the global and local guidance, such as \cref{fig:sup_semantic}.
%
In scene composition, the 'computer station' lacks accessories like cables and wires, the 'headphones' are misshapen, and the 'computer screen' lacks a base. Scene recomposition similarly shows the 'astronaut' and 'bed' placed together without sensible global calibration.
%
Moreover, the Stable Diffusion model's depiction of human figures often suffers from geometric distortions, potentially due to the multi-face problem, as shown in \cref{fig:sup_semantic}.
%
These failures are mostly due to uncommon layouts or the rarity of certain objects in text-to-image datasets. Repeated global text prompts on Stable Diffusion and examination of numerous samples have failed to yield images that align with our objectives. This challenge extends beyond guidance collapse, reflecting the scarcity of certain objects in the model's outputs.
%
CompoNeRF's effectiveness is inherently linked to the performance of large-scale text-to-image models, restricting its capabilities to generating primarily conventional scenes with well-defined global features.
%
To address these limitations, we can strategically adjust the weights of global textual guidance \({\alpha_g}\nabla\mathcal{L}_{\text{SDS}_g}\) and local textual guidance \({\alpha_l}\nabla\mathcal{L}_{\text{SDS}_l}\). This adjustment aims to find an equilibrium between the consistency of the overall scene and the accuracy of individual components. For example, increasing \({\alpha_g}\) enhances global consistency, as evidenced in Figure~\ref{fig:sup_semantic}. However, this can inadvertently lead to objects assimilating extraneous global context. In the 'boy riding bike' scenario, a heightened \({\alpha_g}\) may result in the 'bike' being erroneously represented as both a human figure and a bike. Similarly, in the 'breakfast' scene, amplifying global context might result in a more proportionate table, yet it complicates the distinction between the 'croissant' and its individual NeRF representation.

Ultimately, fine-tuning loss weight parameters is a delicate process that can mitigate identity issues, yet it demands careful calibration to maintain a harmony between scene integrity and the authenticity of each component. The limited representation of certain objects in pre-trained models remains a substantial obstacle, underscoring the need for further investigation into the issue of inadequate guidance in complex scene generation.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/abls_version.pdf}
    \caption{
    The influence of different weight versions of Stable Diffusion.
    }
    \label{fig: abls_version}
\end{figure*}


\noindent \textbf{Influence of SD version.}
As depicted in Fig.~\ref{fig: abls_version}, there is a significant gap between the results using SD V1.5 and SD V1.4, suggesting that upgrading to a more advanced diffusion model could further enhance the quality of the generated content.




% CompoNeRF's success is thus constrained by the capabilities of large-scale text-to-image models, limiting us to creating common scenes with accurate global features.
% %
% Mitigating such failures, we can manipulate the weights of global textual guidance \({\alpha_g}\nabla\mathcal{L}_{\text{SDS}_g}\) and local textual guidance \({\alpha_l}\nabla\mathcal{L}_{\text{SDS}_l}\) to adjust the balance between overall scene consistency and individual component accuracy. For instance, increasing \({\alpha_g}\) can improve scene consistency, as seen in Figure~\ref{fig:sup_semantic}, but may also cause objects to incorporate unwanted global context.
% For instance, the 'boy riding bike' scene which shows artifacts in bike may enhance global context by doubling the $\alpha_g$. However, it may also brings other context, like 'bike' takes the role of both human and the bike. It turns out that it might not easy to distinguish between local and global context range. The 'breakfast' scene can have more resaonable output with a larger table that can support the objects on the surface with higher global context, while it also hard to depart 'croissant' into an individual NeRF.

% In conclusion, modulating loss weight parameters can help tackle identity failures, but it requires a delicate balance between scene coherence and the credibility of components. The infrequent distribution of objects in pre-trained models poses a significant challenge, indicating a need for further exploration into 'lack of sufficient guidance.'

\section{More Visualization Results}
\label{sec:vis}
We provide the multi-view qualitative results from CompoNeRF in \cref{fig:vis1}. 
Note that we increase the resolution of the image latent features from $64 \times 64$ to $128\times128$ during inference for better results.
We also have attached video results in the supplemental materials for each case and the baseline Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score}. 
Please see the attached video for rotating frame results. 

% In addition, we included more visualization results and videos for the scene editing results.
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=.8\linewidth]{figures/supply_2.pdf}
%     % \caption{
%     % More qualitative results using multi-object text prompts. 
%     % }
%     \label{fig:vis1}
% \end{figure*}









\begin{figure*}[t]
    \centering
    \includegraphics[width=.98\linewidth]{figures/supply_1.pdf}
    \caption{
    More qualitative results using multi-object text prompts. 
    }
    \label{fig:vis1}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=.98\linewidth]{figures/basketball_soccer.pdf}
    \caption{
    More qualitative results using multi-object text prompts. 
    }
    \label{fig: basketball and soccer}
\end{figure*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=.8\linewidth]{figures/supply_2.pdf}
% \end{figure*}
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=.9\linewidth]{figures/supply_22.pdf}
%     \caption{Full results}
%     \label{fig:vis1}
% \end{figure*}


\begin{table}[t!]
% \scalebox{0.8}
\renewcommand{\arraystretch}{1.2}
\fontsize{4pt}{4pt}
\selectfont 
\centering
% \vspace{-8pt}
\resizebox{\linewidth}{!}
{
\begin{tabular}{lcccc}
\hline
Method                   & Case 5         & Case 6         & Case 7         & Case 8         \\ 
\hlineB{1.1}
LatentNeRF               & 25.16          & 27.07          & 26.32          & 27.43          \\
SJC                      & 23.55          & 27.84          & 27.41          & 25.62          \\
\textbf{CompoNeRF (Ours)} & \textbf{26.13} & \textbf{32.71} & \textbf{28.44} & \textbf{28.96} \\ \hlineB{1.1}
\end{tabular}
}
\caption{\textbf{More quantitative comparisons}. For our evaluation metric, we utilize the average of CLIP scores~\cite{parmar2023zero,zhang2023sine,wang2023imagen} across different views, which serve to assess the similarity between the generated images and the global text prompt. }
\label{tb:perclass}
\end{table}


\clearpage
\section{Case Details}

% Denote recomposition results from the source scenes with xxx.mp4 consisting of Y) 'xxx', where Y is the source scene sign.
% The video attached to our supplementary materials consists of the following sections:


\begin{enumerate}
        \item Text prompts : 
        \begin{enumerate}
            \item A tesla model three is running on the road.
            \item An astronaut is standing on the moon ground.
            \item A red apple and a yellow banana.
            \item A white king and a black queen chess piece on a chess board.
            \item Glasses of wine, salad and french bread on a wooden table.
            \item  The sydney opera house, the Leaning tower of Pisa, and the Eiffel tower are situated in a triangle.
            \item The Great Sphinx of Giza is situated near the Great Pyramid in the desert.
            \item Crystal ball, Dichroic glass ball, Murano glass ball, and Solar-powered glass ball are placed on the glass table.
            \item A bed is next to a nightstand with a table lamp on it in a room. 
            \item  A bunch of sunflowers in a barnacle encrusted clay vase. 
            \item A silvery wristwatch lay on a velvet cloth, its glossy surface reflecting the soft glow of a nearby candle, creating a dance of light and shadow.
            \item A glass of water on a wooden table refracts the morning sunlight, casting a rainbow onto the table's surface.
        \end{enumerate}
        \item Scene Editing:
        \begin{enumerate}
            \item 
            A bed is next to a nightstand with a vase of sunflowers on it in a room.
            \item A bed is next to nightstands with table lamps on them in a room.
            % \item room\_two\_stand.mp4 consists of duplication of nightstands and a bed.
            % \item room\_sunflower.mp4 consists i) bed and nightstand,  j)  bunch of sunflowers and vase.
            \item On the polished surface of the mahogany nightstand, a lamp with a shade of woven silk cast a warm glow over a Murano glass ball and its companion, a shimmering Dichroic glass ball, creating a dance of colors against the darkening twilight.
            \item Glasses of juice, salad, apple, and French bread on a wooden table.
            \item The game of black queen and white king chess was played on a table, with a table lamp providing the necessary illumination.
        \end{enumerate}
        \item Questions for User Study:
        \begin{enumerate}
            \item On a scale of 1 (low) to 7 (high), how would you rate the semantic consistency of the generated 3D assets? (Composition Correctness Evaluation task).
            \item On a scale of 1 (low) to 7 (high), how would you rate the multi-view consistency of the generated 3D assets? (Composition Correctness Evaluation task).
            \item On a scale of 1 (low) to 7 (high), how would you rate the quality of the generated images from the inputs? (Generative Quality Evaluation task).
            \item What objects do you find in the 3D assets? (Object Identification task).
        \end{enumerate}
\end{enumerate}
    %     \item Ablation Study Video:
    %     \begin{enumerate}
    %         \item (bed\_room\_without\_calibration.mp4) deprived of calibration module for \emph{bed room}.
    %         \item (bed\_room\_without\_global\_text.mp4) deprived of global branch/global text guidance for \emph{bed room}.
    %         \item (bed\_room\_color\_based.mp4) color-based calibration module for \emph{bed room}.
    %         \item (bed\_room\_full.mp4) our density-based calibration module for \emph{bed room}.
    %         \item (table\_wine\_color\_based.mp4) color-based calibration module for \emph{table wine}.
    %         \item (table\_wine\_density\_based.mp4) density-based calibration module for \emph{table wine}.
    %     \end{enumerate}
    % \end{enumerate}

% We also extend the training and testing progress of 'table\_wine' with the subfolder 'example'. 
% The training process has different files, including 
% 'step\_{num}\_{local}\_{id}\_image', where 'num' denotes the training step, 'local' or 'global' indicates the local/global frames, and 'id' is the NeRF node index. 

% If the file names end with 'weights\_sum', it denotes the ray hit that receives gradients. 

% Our validation stage adopts a circular camera trajectory and attaches the inferred surface norms following the training process' naming convention above. 
% \textbf{Training and Validation Demo}

% The demonstration of the training and validation process of the 'table\_wine' scene is incorporated in a specialized subfolder titled \textit{example}.

% \textbf{Training Process File Naming}

% Files in the training process adhere to a specific naming convention:
% \begin{itemize}
%     \item \textbf{Format}: \texttt{step\_\{num\}\_\{local/global\}\_\{id\}\_image}
%     \begin{itemize}
%         \item \texttt{num}: Training step number.
%         \item \texttt{local/global}: Indicates local or global frames in the training model.
%         \item \texttt{id}: NeRF node index.
%     \end{itemize}
% \end{itemize}

% Files ending with \texttt{weights\_sum} denote the ray hit that receives gradients, integral in image processing or neural rendering.


% 3. Validation Stage

% The validation stage involves:
% \begin{itemize}
%     \item A circular camera trajectory for comprehensive model assessment.
%     \item Attachment of inferred surface norms, following the training process's naming convention.
% \end{itemize}

% \maketitle
\end{document}
