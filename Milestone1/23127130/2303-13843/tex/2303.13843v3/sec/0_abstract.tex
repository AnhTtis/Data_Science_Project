\begin{abstract}
% Recent research endeavors have shown that combining neural radiance fields (NeRFs) with pre-trained diffusion models holds great potential for text-to-3D generation.
% However, a hurdle is that they often encounter guidance collapse when rendering multi-object scenes with relatively long sentences. 
% Specifically, text-to-image diffusion models are inherently unconstrained, making them less competent to accurately associate object semantics with 3D structures.
% to explicitly incorporate an editable 3D scene layout to provide effective guidance at the object (\ie, local) and scene (\ie, global) levels. 
% Firstly, we interpret the multi-object text as an editable 3D scene layout containing multiple local NeRFs associated with the object-specific 3D boxes and text prompt.
% Then, we introduce a composition module to calibrate the latent features from local NeRFs, which surprisingly improves the view consistency across different local NeRFs. 
% Lastly, we apply text guidance on global and local levels through their corresponding views to avoid guidance ambiguity.
% Additionally, NeRFs can be decomposed and cached for composing other scenes with fine-tuning.
Recent advances have shown promise in merging neural radiance fields (NeRFs) with pre-trained diffusion models for text-to-3D object generation. 
However, one enduring challenge is their inadequate capability to accurately parse and regenerate consistent \textbf{multi-object} environments.
Specifically, these models encounter difficulties in accurately representing quantity and style prompted by multi-object texts, often resulting in a collapse of the rendering fidelity that fails to match the semantic intricacies. Moreover, amalgamating these elements into a coherent 3D scene is a substantial challenge, stemming from generic distribution inherent in diffusion models.
%
To tackle the issue of 'guidance collapse' and enhance consistency, we propose a novel framework, dubbed \textbf{CompoNeRF}, 
by integrating an editable 3D scene layout with object specific and scene-wide guidance mechanisms.
It initiates by interpreting a complex text into an editable 3D layout populated with multiple NeRFs, each paired with a corresponding subtext prompt for precise object depiction. 
%
Next, a tailored composition module seamlessly blends these NeRFs, promoting consistency, while the dual-level text guidance reduces ambiguity and boosts accuracy. 
%
Noticeably, the unique modularity of CompoNeRF permits NeRF decomposition. 
% , facilitating their reconfiguration.
This enables flexible scene editing and recomposition into new scenes based on the edited layout or text prompts. 
Utilizing the open source Stable Diffusion model, CompoNeRF not only generates scenes with high fidelity but also paves the way for innovative multi-object composition using editable 3D layouts. Remarkably, our framework achieves up to a \textbf{54\%} improvement in performance, as measured by the multi-view CLIP score metric. 
Code is available at \url{https://github.com/hbai98/Componerf}.
\end{abstract}