\section{Method}
\label{sec: method}
% This section introduces the rendering pipeline of our proposed hierarchical compositional scene. 
% our pipeline consists of three processes, including decomposing the text into editable 3D layout, rendering the compositional views with local (object) NeRFs and global (scene) NeRF and the joint optimization on these hierarchical 3D representations.

% Note that the transformation between the object and the scene frame is defined by ${p}_o$ and ${D}_o$. 
%
% Next, we build a residual connection to add ${\sigma}_o$ and the referenced global color, and the rendering result will be used to calculate the SDS loss based on the global text.  
% Fig.~\ref{fig:framework} illustrates our pipeline, which consists of three main components, including the editable 3D scene layout based on multi-object text (Sec.~\ref{ssec:layout}), the scene rendering pipeline that composites the predictions from all local NeRFs (Sec.~\ref{ssec:render}), and the joint optimization on both local and global representation models (Sec.~\ref{sec:optimization}).
% To elaborate, our editable 3D scene layout represents a global frame of the scene by decomposing it into a set of local frames, where each is parameterized by a local NeRF, a 3D bounding box, and a corresponding local text prompt.
% For instance, the text prompt `A teddy bear and a stuffed monkey sit side by side' is interpreted as a 3D scene layout, as shown in Fig.~\ref{fig:framework}.  
% The whole 3D layout, \ie, scene frame, consists of two 3D bounding boxes, \ie local frames \#1 and \#2, with specific local text prompts, \ie, `a teddy bear' and `a stuffed monkey'. 
% %
% To render the scene view, we first calculate the ray-box intersections between the boxes and rays $({\boldsymbol{r}}_o, \boldsymbol{\phi}_d, {\boldsymbol{\theta}}_d)$, where the ${\boldsymbol{r}}_o$ is the ray origin and the $({\boldsymbol{r}}_o, \boldsymbol{\phi}_d)$ is its direction.
% Then, to infer each object's properties in local NeRFs, we sample the global points $({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g)$ in the global frame within the ray-box intersection intervals and project them into the normalized local location $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$ in the local frame.
% %
% Given the local sampling points $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$, the implicit local NeRF ${\boldsymbol{\theta}}_l$ outputs four pseudo-color channels ${\boldsymbol{C}}_l$ and density $\boldsymbol{\sigma}$, which can be used to render a local view of the local frame to match its local text prompt.
% %
% We further calibrate the predicted pseudo-color $\boldsymbol{C}_l$ from local frames by adding the global embeddings ${\boldsymbol{emb}}_g$ to improve the global view consistency.
% Then, the calibrated predictions after composition are used to reconstruct the scene view by volumetric rendering along the rays.
% %
% Lastly, the rendered views based on local and global frames are guided by score distillation sampling loss $\nabla \mathcal{L}_{\text{SDS}}$~\cite{poole2022dreamfusion} to optimize all the learnable parameters. 
To resolve the issue of guidance collapse, our principal strategy is to \textit{decompose the scene into reusable components and compose/recompose them into a unified and consistent one}.
This enables flexible control over the generated content with direct use of prompts and box layouts, as illustrated in \cref{fig:teaser}.
%
Our proposed CompoNeRF confers several key benefits:
1) \textbf{Semantic Coherence}: It reliably creates 3D objects with detailed textures and global consistency, exemplified by authentic light interactions, such as reflections on the bed surface.
2) \textbf{Modularity and Reusability}: CompoNeRF functions as an ensemble of independently trained NeRF models. These can be efficiently stored and later retrieved from a cached dataset, enabling their reuse in various cases.
3) \textbf{Editability}: Our approach allows for flexible scene modification, such as interchanging the lamp for a vase filled with sunflowers or altering its scale, by simply adjusting the box dimensions for later finetuning. This feature enhances flexibility and creative possibilities. 


% Furthermore, the usage of layout boxes enables more flexible control over the generated content compared with the intricate sketch shape in Latent-NeRF\cite{metzer2022latent}. 
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/method.pdf}
    % \vspace{-12pt}
    \caption{\textbf{Framework Overview}.
The CompoNeRF model unfolds in three stages: 1) Editing 3D scene, which initiates the process by structuring the scene with 3D boxes and textual prompts; 2) Scene rendering, which encapsulates the composition/recomposition process, facilitating the transformation of NeRFs to a global frame, ensuring cohesive scene construction. Here, we specify design choices between density-based or color-based(without refining density) composition; 3) Joint Optimization, which leverages textual directives to amplify the rendering quality of both global and local views, while also integrating revised text prompts and NeRFs for refined scene depiction.
  % The model is structured into three components: Composition, Decomposition, and Recomposition. Composition deals with the foundational setup, detailed with choices for density-based and color-based composition. Decomposition utilizes the modularity of the CompoNeRF feature, caching each NeRF module offline for efficient recalibration. Recomposition reuses these cached NeRFs and adjusts the semantic context, providing a revised output with the inclusion of the offline NeRF enhancements.
    % Our model consists of two branches where the upper part is individual NeRFs, and the lower part denotes global calibration with our tailored composition model. The specific designs for density-based and color-based composition modules are highlighted. 
    % CompoNeRF consists of three parts: 1). The editable 3D scene layout configures the scene representations with 3D boxes and text prompts; 2).  The scene rendering includes the global calibration and the compositional process; 3). The joint optimization applies global and local text guidance on global and local render views.
    % The global frame (scene space) contains a set of local frames. Each is  represented by a local NeRF associated with a 3D box and text prompt defined by the editable 3D layout.
    % The scene view is volumetric rendered by sampling the points $({\boldsymbol{x}}_g, \boldsymbol{y}_g, \boldsymbol{z}_g)$ intersected with any local frame along the ray $(\boldsymbol{r}_o, {\boldsymbol{\phi}}_d, \boldsymbol{\theta}_d)$.
    % The sampling points are first inferred through the local NeRF with the local frame locations $({\boldsymbol{x}}_l, \boldsymbol{y}_l, \boldsymbol{z}_l)$ projected from the global location $({\boldsymbol{x}}_g, \boldsymbol{y}_g, \boldsymbol{z}_g)$.
    % And then, all the local predictions are calibrated by a global MLP with conditional input to render the scene view.
    % During the optimization, the text guidance is applied to both local views predicted by local frames only and global views predicted by the composition of all local frame predictions.
    }
    \label{fig:framework}
    % \vspace{-8pt}
\end{figure*}

\subsection{Preliminaries}
Defining individual object bounding boxes as \textit{local frames} and the overall scene coordinate system as the \textit{global frame}, we build the foundation of NeRF and diffusion processes.

\label{sec:background}
\noindent \textbf{3D Representation in Latent Space.}
Our methodology capitalizes on the state-of-the-art text-to-image generative model—Stable Diffusion as described by Rombach et al\cite{rombach2022high}.
We build upon the Latent-NeRF framework~\cite{metzer2022latent}, which computes latent colors for individual objects by considering their sample positions within a localized frame. Specifically, it maps a three-dimensional point in local coordinates \(\boldsymbol{x}_l = (x_l, y_l, z_l)\) to a volumetric density \(\boldsymbol{\sigma}_l\) and an associated color \(\boldsymbol{C}_l\), expressed as \((\boldsymbol{C}_l, \boldsymbol{\sigma}_l) = f_{\boldsymbol{\theta}_l}(x_l, y_l, z_l)\). Here, \(f\) represents a Multi-Layer Perceptron (MLP) characterized by parameters \(\boldsymbol{\theta}_l\).
 This NeRF-generated color is then assessed in the context of the Stable Diffusion model, using text prompts to guide NeRF toward spatially coherent inference with intricate context.
% to infer pseudo-color for each object using local NeRF.
% Specifically, the representation maps a point $\boldsymbol{x}_l = \left({x}_l, {y}_l, {z}_l\right)\in [-1, 1]$ in the local frame to its corresponding volumetric density $\boldsymbol{\sigma}_l$ and emitted color $\boldsymbol{C}_l$, \ie,  $\left(\boldsymbol{C}_l, {\boldsymbol{\sigma}_l}\right)=\boldsymbol{\theta}_{_l}\left({x_l}, {y}_l, {z}_l\right)$.
% The predicted pseudo-color is fed forward into the decoder of the Stable Diffusion model to obtain the final rendering result.

\noindent \textbf{Volume Rendering with Multiple Objects.}
% For each local frame $j$ with NeRF parameterized as $\theta_j$, we follow original NeRF design\cite{nerf} to integrate $(\boldsymbol{C}_l, \boldsymbol{\sigma}_l)$ of   sampled points from any hit ray $r_l=(\boldsymbol{o}_l, \boldsymbol{d}_l)$ by,
% For consistent scene rendering, object transmittance $T_k$ must be recalculated in the global frame based on independent properties inferred from local NeRFs. Hence, we sort predictions according to their distance to $\boldsymbol{o}_g$. 
% Similar to \cref{eq:volrend}, global color $\hat{\boldsymbol{C}}_g$ of ray $\boldsymbol{r}_g=(\boldsymbol{o}_g, \boldsymbol{d}_g)$ is predicted by the volumetric rendering integrating over $m$ objects,
We extend the volume rendering process to accommodate multiple objects by assigning each a local frame, denoted as $j$, with NeRF parameters $\boldsymbol{\theta}_{l, j}$. Drawing from the foundational NeRF approach \cite{nerf}, in each local frame, we integrate the color $\boldsymbol{C}_l$ and density $\boldsymbol{\sigma}_l$ for points $\boldsymbol{x}_l$ sampled along a ray $\boldsymbol{r}_l$, emanates from the camera origin $\boldsymbol{o}_l$ in direction $\boldsymbol{d}_l$. This is formalized in the predicted color integration for $\hat{\boldsymbol{C}}_l$ as:
{\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eq:volrend}
{\hat{\boldsymbol{C}}_l}({\boldsymbol{r}_l})=\sum_{k=1}^{N} T_{l, k} \left(1-\exp \left(-\sigma_{l, k} \delta_k\right) \right) {\boldsymbol{C}}_{l,k},
\end{equation}}where $T_{l, k}=\exp \left(-\sum_{j=1}^{k-1} \sigma_{l,j} \delta_j\right)$ represents the transmittance to the $k$-th of total $N$ sample, calculated exponentially over the cumulative density along $\boldsymbol{r}_l$, and $\delta_k$ is the interval between adjacent samples.
%
To synthesize a coherent scene, we transition from processing individual local frames to a collective global frame. Within this global context, we reconcile object attributes inferred from their individual local NeRFs for refined $\boldsymbol{\sigma}_g, \boldsymbol{C}_g$ along with $T_{g, k}$. The samples $\boldsymbol{x}_g$ are ordered based on their spatial distances from the origin $\boldsymbol{o}_g$ following the coordinate transformation. We then express the volumetric rendering of a ray $\boldsymbol{r}_g$ integrating $m$ objects within the global frame as follows:
{
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eq:multi_volrend}
{\hat{\boldsymbol{C}}_g}({\boldsymbol{r}_g})=\sum_{k=1}^{m*N} T_{g, k} \left(1-\exp \left(-\sigma_{g, k} \delta_k\right) \right) {\boldsymbol{C}}_{g,k}. 
\end{equation}}

\noindent \textbf{Score Distillation Sampling.}
% During the SDS process, a noise image $\boldsymbol{X}_t$ is first generated by adding a sampled noise $\epsilon \sim \mathcal{N}(0, I)$ in noise level $t$ into a rendered view $\boldsymbol{X}$ from a NeRF.
To facilitate the conversion from text descriptions to 3D models, DreamFusion~\cite{poole2022dreamfusion} utilizes Score Distillation Sampling (SDS), leveraging the generative capabilities of a diffusion model, denoted as $\phi$, to guide the optimization of NeRF parameters, symbolized as $\boldsymbol{\theta}$.
%
Initially, SDS creates a noisy image $\boldsymbol{X}_t$ by infusing a randomly sampled noise $\epsilon$, which follows a normal distribution $\mathcal{N}(0, I)$, into a NeRF-rendered image $\boldsymbol{X}$ at a given noise level $t$.
The diffusion model $\phi$ then estimates the noise $\epsilon_\phi\left(\boldsymbol{X}_t, t, T\right)$ from this noisy image, conditioned by the noise level $t$ and an optional text prompt $T$. 
The key step in SDS involves calculating the gradient of the loss function, which measures the discrepancy between the estimated noise and the originally added noise:
{\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eq:sds_loss}
\nabla_\theta \mathcal{L}_{\text{SDS}}(\boldsymbol{X}_t, T)=  w(t)\left(\epsilon_\phi\left(\boldsymbol{X}_t, t, T\right)-\epsilon\right),
\end{equation}}where $w(t)$ is a weighting function that adjusts the influence of the gradient based on the noise level. 
The gradients across all rendered views direct the update of $\boldsymbol{\theta}$, ensuring that the NeRF-generated images align with the text descriptions. Additionally, we incorporate the 'perturb and average' technique from SJC for more robust $\mathcal{L}_{\text{SDS}}$. For a comprehensive understanding of these methods, the reader is directed to the detailed explanations provided in \cite{poole2022dreamfusion,wang2022score}.

%
%
% \subsection{Editable 3D Scene Layout}
% \label{ssec:layout}
% The 3D scene layout explicitly combines language structures with 3D layouts in an editable way.
% Given the input text prompt $T$, the attribute-object pairs can be easily obtained based on user control.
% Note that the text prompt indicates the multi-object text prompt by default.
% % available for free in many structured representations, such as the constituency tree.
% As shown in Fig.~\ref{fig:framework}, we can extract multiple noun phrases with their binding attributes and map these local text prompts into corresponding regions.
% Specifically, we define the scene structure with $m$ local frames, each employs a local NeRF $\boldsymbol{\theta}_l$ as representation, the local text prompt $T_{l} \subseteq{T}$ and its spatial layout with 3D boxes $\mathbf{b} = \{\mathbf{p}, \mathbf{s}\} \in  \mathbb{R}^6$ of each object entity, where $\mathbf{p}=\{p_x, p_y, p_z\}$ refers to the center point and $\mathbf{s}=\{s_x, s_y, s_z\}$ denotes the box scale. 
% \textit{Our editable 3D layout is easy to be collected and edited with its simplicity, allowing for versatile and interactive user control by modifying the box's or text's properties to define a new scene}.
% Moreover, as depicted in Fig.~\ref{fig:teaser}, each component in a 3D scene layout can be replaced or re-composited with other trained local NeRFs, which is more friendly for flexible user editions compared with using only text prompts.
% We fine-tuned the new layout by global rendering, which enables scalable re-editing.
% Each relationship $r_k \in R$ is a triplet in a <subject-predictive object> format, where a subject node is. After we generate the scene graph from the complex prompts, we can sample the closest relationship with the 2d spatial layout as the initial 3D position. fine-tuned the new layout by global rendering, which enables scalable re-editing
%
% \subsection{Scene Rendering Pipeline}
% \label{ssec:render}
% In CompoNeRF, the scene images are rendered by a ray-casting approach following the design of NeRF.
% % Each ray to be cast is generated based on the camera pose, intrinsic, and transformation.
% The camera is defined by a pinhole camera model, casting a set of rays $(\boldsymbol{r}_o, \boldsymbol{\phi}_d, {\boldsymbol{\theta}}_d)=\boldsymbol{o}+t\boldsymbol{d}$ through each pixel on the frame of size $H \times W$, where the $\boldsymbol{r}_o \in  \mathbb{R}^3$ is the origin and the $(\boldsymbol{\phi}_d, \boldsymbol{\theta}_d)$ is the viewing direction.
% Along this ray, we sample all the points intersected with any layout box of local frames.
% For each hit sampled point, the color and volumetric density are computed through the local NeRF of the hit local frame.
% The ray color perdition is calculated by the differentiable integration applied on all the point-predicted colors and volumetric density along the ray.
%
% \noindent \textbf{Ray-box Intersection with Local Frames.}
% Given a ray $\boldsymbol{r}_i$, each box $\boldsymbol{b}_j$ of the local frame is applied with the AABB ray intersection test algorithm to check the intersections.
% When the ray $r_i$ is hit with a box $\boldsymbol{b}_j$ of the local frame, we use the entrance and exit points as near $\boldsymbol{t}_{in}$ and far $\boldsymbol{t}_{out}$ bounds to sample $N$ equidistant quadrature points, $
% \boldsymbol{t}_{i,j,n}=\frac{n-1}{N-1}\left(\boldsymbol{t}_{out}-\boldsymbol{t}_{in}\right)+\boldsymbol{t}_{in} , n \in \left[1, N\right]$
% % Despite each local frame only having a small number of hit rays compared to the scene, we observe that it is enough to represent each object accurately while maintaining short rendering times.
% Note that the coordinates of sampled points are first projected into normalized coordinates using the box scale of local frames to enable each local NeRF to learn the scale-independent representation.
% The bounding box $\mathbf{b}$ of the local frame in global coordinate can be transformed into a canonical bounding box by ${(\mathbf{b}} - \boldsymbol{p}) / \mathbf{s}$.
% Considering the rendering efficiency, we only calculate the valid points, interacted with the boxes, and set all the empty points with a constant background color.
%
% The appearance of a set object representations depends on its interaction with the scene and illumination which should be decided by the local frame location.
% To ensure the volumetric consistency, we only calibrate the emitted color with scene location, while the gradient still can be propagated.
% Since the overall color depends on both the global  positions $({x}_w, {y}_w, {z}_w)$ and ray directions $({\phi}_d, {\theta}_d)$, the global color embedding is learned based on both the positions and ray directions.
% Since the overall color depends on both the global  positions $({x}_w, {y}_w, {z}_w)$ and ray directions $({\phi}_d, {\theta}_d)$, the global color embedding is learned based on both the positions and ray directions.
% \subsection{The Proposed CompoNeRF}
% \subsubsection{Composition Module}
% CompoNeRF aims to composite multiple NeRFs to reconstruct multi-object scenes with both box and prompt guidance.
% %
% Our framework, as shown in \cref{fig:framework}, applies the AABB ray intersection test algorithm to check for intersections on each box in the global frame. We then samples $\boldsymbol{x}_g$ within the ray box intervals, and project them to $\boldsymbol{x}_l$ to infer  $\left(\boldsymbol{C}_l, {\boldsymbol{\sigma}_l}\right)$ in separate NeRF models. 
% %
% We then utilize volume rendering to obtain rendered views for each local frame respectively. 
% %
% After that, they would be passed on to our tailored composition Module to infer 
% $\left(\boldsymbol{C}_g, {\boldsymbol{\sigma}_g}\right)$
% for global rendering. 
% Next, we match local and global texts with their corresponding image outputs by SDS losses. 
% We also support recomposition by passing samples from cached models into $\boldsymbol{x}_l$ to continue the above process.
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/abls.pdf}
    % \vspace{-22pt}
    % \caption{Ablation study on text guidance. (a) without local SDS losses. (b) without global SDS losses. (c) vanilla SDS losses without perturb and average scoring~\cite{wang2022score}. (d) full model.}
    \caption{\textbf{Design Impact Comparison: Density vs. Color-based Methods.} The top row illustrates the density-based approach's detailed rendering and quick convergence in the 'table wine' scene. The bottom row highlights the color-based method's enhancements and its drawbacks, such as geometric and shadow inaccuracies, particularly in close-up views and slow convergence.
    % \textbf{(a)} global text guidance(integrating local frames by \cref{eq:multi_volrend}) and global calibration(integrating local frames, then aligning the rendering result directly with the full text). 
    }
    \label{fig:abls}
    % \vspace{-20pt}
\end{figure}
\subsection{The Proposed CompoNeRF}
\subsubsection{Composition Module}
CompoNeRF is designed to composite multiple NeRFs to reconstruct scenes featuring multiple objects, utilizing guidance from both bounding boxes and textual prompts. Within our framework, depicted in \cref{fig:framework}, the Axis-Aligned Bounding Box (AABB) ray intersection test algorithm is applied to ascertain intersections across each box in the global frame. Subsequently, we sample points \(\boldsymbol{x}_g\) within the intervals of the ray-box and project them to \(\boldsymbol{x}_l\) to deduce the corresponding color \(\boldsymbol{C}_l\) and density \(\boldsymbol{\sigma}_l\) within individual NeRF models.
%
These properties are processed through our composition module to infer the global color \(\boldsymbol{C}_g\) and density \(\boldsymbol{\sigma}_g\), crucial for the global rendering.
%
Volume rendering techniques~\cite{kajiya1984ray} are then employed to procure the rendered views for both local and global frames. We propose dual SDS losses to ensure coherence between the image outputs and their corresponding textual descriptions. Additionally, our approach facilitates recomposition by channeling samples from cached models back into local frames along with the text revision, thereby streamlining the integration.

% As shown in \cref{fig:abls}(a), we verify its necessity by dropping $\nabla \mathcal{L}_{\text{SDS}_g}$. 
% %
% Compared with our full model, its layout does not fit our shared sense of a room, \ie, \emph{nightstand} is usually lower than \emph{bed}; \emph{lamp} needs a base to support it. Additionally,  it lacks global consistency, such as light reflection, to make it more realistic. 
% %
% Therefore, we leverage the full text semantics to ensure consistent global rendering across local frames. 
% %
% Instead of conditioning the global rendering view with the full prompt directly, we note that global calibration is necessary for geometry and color to be learned sufficiently.
% For example, we observe that geometric completeness and texture of \emph{nightstand} are not ideal. Although reflection appears around \emph{nightstand}, \emph{bed} is stripped of the light. 
% %
% Therefore, we opt to leverage the correlation between the rendering output of the combined NeRFs and the overall semantics to perform multi-object scene reconstruction.  
%

\noindent\textbf{Global Composition.}
The independent optimization of each local frame may inadvertently result in a lack of global coherence within the scene. To address this, our scene composition process is designed to integrate these frames, thereby achieving a more consistent result.
%
Before exploring the specifics of the module, it is imperative to discuss two critical design decisions within the composition module, as depicted in \cref{fig:framework}.
%
Upon integrating the properties inferred from \(\boldsymbol{x}_g\) into the composition module, they are fine-tuned through gradients derived from the global SDS loss.  This process leads to a critical consideration: the necessity and implications of refining the global density \(\boldsymbol{\sigma}_g\). This can be divided into two approaches: \textbf{1) Density-based:} The advantage of adjusting \(\boldsymbol{\sigma}_g\) is that it can adjust geometry, thus yielding a scene more congruent with the global text prompt. 
However, this comes at the cost of potentially compromising the optimal color \(\boldsymbol{C}_g\), as calibrating \(\boldsymbol{\sigma}_g\) introduces more uncertainty for subsequent color refinement as it requires prior density features $\boldsymbol{h}$ as shown at \cref{fig:compo}. 
\textbf{2) Color-based:} Conversely, directly employing \(\boldsymbol{\sigma}_l\) mitigates this uncertainty but at the expense of reduced geometric control, presenting a challenging balance to strike in the pursuit of precise scene composition.
% , which may lead to suboptimal outcomes.
%
After thorough experiments, exemplified in \cref{fig:abls}, we have opted for the density-based approach to refine \(\boldsymbol{\sigma}_g\)  prioritizing both \textbf{accuracy and efficiency}. The test revealed that it excels in rendering intricate details, such as enhanced wood grain textures and more naturally contoured 'salad', as accentuated by boxes. This method also demonstrated a swifter convergence rate. Conversely, while the color-based improved reflections and reduced flickering on the 'wine cup', it was plagued by issues such as sparse density, which adversely brings holes at the base of the 'cup' and the corner of the 'table'.
Furthermore, upon close examination, it becomes evident that shadow artifacts of 'wine' on the 'table' are pronounced, suggesting that its disadvantages outweigh its advantages.
%  in this context
% \textbf{Global Composition.}
% Each local frame is optimized independently, causing a lack of global connections for scene composition.
% Before delving into module details, there are two choices (see \cref{fig:framework}) on the composition module design we need to elaborate on first. 
% %
% In \cref{fig:framework}, by taking $\boldsymbol{x}_g$ into the composition module, their inferred properties are calibrated with gradients propagated from the global SDS loss. 
% However, it remains unclear whether $\boldsymbol{\sigma}_g$ should be refined or not. 
% %
% The trade-off on its usage is the density adjustment bringing a more reasonable layout and more geometric details that fit the global text prompt. While its potential downside is that $\boldsymbol{C}_g$ may not be optimal as $\boldsymbol{\sigma}_g$ has more uncertainty compared to $\boldsymbol{\sigma}_l$, bringing sub-optimal rendering results. 

% We choose the density-based method after comparing them with the experiment shown in \cref{fig:abls}. 
% %
% Specifically, we test both designs on the scene \emph{table wine} and discover that the density-based design provides more intrinsic details(as indicated by green boxes), \eg, enriched wood grains, and a more natural shape for \emph{salad} and has much faster convergence speed. In contrast, the color-based method enhances the reflection and smooths flickering on \emph{wine cup}, (as indicated by red boxes), but it suffers from 1) sparse density, resulting in poorly generated geometry at the base of  \emph{cup} and the wood \emph{table} corner. Additionally, shadow artifacts appeared on \emph{table} when viewed up close, outweighing benefits of the color-based method.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/compo_module.pdf}
    % \vspace{-24pt}
    % \caption{Ablation study on text guidance. (a) without local SDS losses. (b) without global SDS losses. (c) vanilla SDS losses without perturb and average scoring~\cite{wang2022score}. (d) full model.}
    \caption{\textbf{Detail of Composition module}: density-based design. 
    }
    \label{fig:compo}
    % \vspace{-18pt}
\end{figure}
\noindent\textbf{Network Design.}
The compositional framework of our network, as delineated in \cref{fig:compo}, is predicated on an architecture that employs a suite of MLPs, represented as \(\{\boldsymbol{\theta}_l\}_{l=1}^{m}\),  each dedicated to a distinct local frame. To harmonize \(\boldsymbol{\sigma}_l\) and \(\boldsymbol{C}_l\), we incorporate global MLPs, including density calibrator $f_{\boldsymbol{\theta}_{g_d}}$ and color calibrator $f_{\boldsymbol{\theta}_{g_c}}$.
%
A transformation module complements this system, tasked with maintaining the spatial coherence between the global and local frames. It governs the transformation of sampling points $\boldsymbol{x}$, ray directions $\boldsymbol{d}$, and adjacent sampling distances $\delta$. This module also orders the points $\{\boldsymbol{x}_{g,j}\}_j$ by their distance to the global camera origin $\boldsymbol{o}_g$, ensuring that each local point $\boldsymbol{x}_l$ is accurately matched with its corresponding global point $\boldsymbol{x}_g$ for subsequent volume rendering. 
%
The network design is:
{
\setlength\abovedisplayskip{4.5pt}
\setlength\belowdisplayskip{4.5pt}
\begin{align}
\label{eq:g_c_d}
{\boldsymbol{\sigma}_g}  &= \alpha_d f_{\boldsymbol{\theta}_{g_d}}({\boldsymbol{x}_g}) + \boldsymbol{\sigma}_l, \\
{\boldsymbol{C}_g}  &= \alpha_c f_{\boldsymbol{\theta}_{g_c}}(\boldsymbol{h}, {\boldsymbol{d}_g}) + \boldsymbol{C}_l. 
\end{align}}In contrast to the local frames, the global frame's color output $\boldsymbol{C}_g$ is inferred based on $\boldsymbol{h}$ and conditional on $\boldsymbol{d}_g$ to enable a view-dependent lighting effect.
% Denote the density features as $\boldsymbol{h}$. 
%
%
Residual learning is leveraged here, where \(\boldsymbol{\sigma}_l, \boldsymbol{C}_l\) serve as foundational elements that support the learning of global density \(\boldsymbol{\sigma}_g\) and color \(\boldsymbol{C}_g\). The parameters \(\alpha_d, \alpha_c\) are adjustable, allowing fine-tuning of the influence that local components exert on the global outputs.
%
It is imperative to acknowledge that in our color-based method, density calibration is intentionally excluded to concentrate solely on the refinement of color dynamics as shown at \cref{fig:framework}. This is achieved by conditioning the process on both spatial and directional global inputs \((\boldsymbol{x}_g, \boldsymbol{d}_g)\), as demonstrated in the following equations:
\begin{align}
\setlength\abovedisplayskip{4.5pt}
\setlength\belowdisplayskip{4.5pt}
\label{eq:g_c_c}
\boldsymbol{\sigma}_g = \boldsymbol{\sigma}_l, \quad
{\boldsymbol{C}_g} = \alpha_c f_{\boldsymbol{\theta}_{g_c}}({\boldsymbol{x}_g}, {\boldsymbol{d}_g}) + \boldsymbol{C}_l.
\end{align}
The integration of extra $\boldsymbol{x}_g$ aims to facilitate a fair comparison under same inputs with the density-based. It enhances the visual appeal of effects like the wine cup's reflection, as demonstrated in \cref{fig:abls}. However, this method is not without its compromises. It tends to produce artifacts and is characterized by a slower convergence rate. Additionally, this approach limits the ability to precisely control density, subsequently impacting the intricate geometric details.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/sota.pdf}
    % \vspace{-24pt}
    \caption{\textbf{Qualitative comparison with other text-to-3D methods using multi-object text prompts}. Cases 1-3 demonstrate simpler settings characterized by compositions involving two objects. In contrast, Cases 4-8 delve into more intricate scenarios featuring compositions with more than two objects. Smaller images are presented to illustrate the generated local NeRFs(partially shown in Cases 4-8).}
    \label{fig:sota}
    % \vspace{-5pt}
\end{figure*}
%
% \begin{table*}[t!]
% \centering
% \resizebox{\textwidth}{!}
% {
% \begin{tabular}{cccccccc}
% \toprule
% Method            & \rotatebox{60}{table wine}  & \rotatebox{60}{teddy monkey} & \rotatebox{60}{computer mouse} & \rotatebox{60}{bed room}  & \rotatebox{60}{chess} & \rotatebox{60}{pisa tower} & \rotatebox{60}{astronaut} & \rotatebox{60}{tesla}  \\ \midrule
% LatentNeRF  & 21.55 & 27.38 & 17.13 & 21.86 & 31.19 & 24.31 & 27.07 & 25.16 \\
% SJC & 23.33 & 27.37 & 18.00 & 22.54 & 30.53 & \textbf{26.18 }& 27.84 & 23.55 \\
% CompoNeRF & \textbf{32.68} & \textbf{28.57}	 &\textbf{ 22.34} &\textbf{ 28.65} & \textbf{31.45} & \textbf{28.96} & 25.82 & 25.95 & 24.42 & \textbf{32.71} & \textbf{26.13 }& \textbf{26.38} & \textbf{30.98} & \textbf{33.37} \\
% \bottomrule
% \end{tabular}
% }
% \vspace{-10pt}
% \caption{Performance of our CompoNeRF in different 3D scenes. We use CLIP score \cite{parmar2023zero,zhang2023sine,wang2023imagen} as our evaluation metric, which is a common evaluation metric in text-to-image generation tasks to evaluate the similarity of the generated image to the text prompt. }
% \label{perclass}
% \end{table*}
%
\begin{table*}[t!]
% \scalebox{0.8}
\renewcommand{\arraystretch}{1.2}
\fontsize{4pt}{4pt}
\selectfont 
\centering
% \vspace{-8pt}
\resizebox{\textwidth}{!}
{
% \begin{tabular}{lcccccccc}
% \hline
% Method     & table\_wine    & tesla          & pyramid        & chess          & apple and banana      & astronaut      & glass\_balls   & Eiffel\_tower    \\ \hline
% LatentNeRF & 21.55          & 25.16          & 27.43          & 31.19          & 27.69          & 27.07          & 29.51          & 26.32          \\
% SJC        & 23.33          & 23.55          & 25.62          & 30.53          & 28.21          & 27.84          & 28.76          &27.41 \\
% \textbf{CompoNeRF(Ours)}     & \textbf{32.68} & \textbf{26.13} & \textbf{28.96} & \textbf{31.45} & \textbf{33.37} & \textbf{32.71} & \textbf{30.98} & \textbf{28.44}          \\ \hline
% \end{tabular}
\begin{tabular}{lcccccccc}
\hline
Method                   & Case 1         & Case 2         & Case 3         & Case 4         & Case 5         & Case 6         & Case 7         & Case 8         \\ 
\hlineB{1.1}
LatentNeRF               & 25.16          & 27.07          & 27.69          & 31.19          & 21.55          & 26.32          & 27.43          & 29.51          \\
SJC                      & 23.55          & 27.84          & 28.21          & 30.53          & 23.33          & 27.41          & 25.62          & 28.76          \\
\textbf{CompoNeRF (Ours)} & \textbf{26.13} & \textbf{32.71} & \textbf{33.37} & \textbf{31.45} & \textbf{36.06} & \textbf{28.44} & \textbf{28.96} & \textbf{30.98} \\ \hlineB{1.1}
\end{tabular}
}

% \vspace{-6pt}
\caption{\textbf{Performance comparison of our CompoNeRF in different 3D scenes}. For our evaluation metric, we utilize the average of CLIP scores~\cite{parmar2023zero,zhang2023sine,wang2023imagen} across different views, which serve to assess the similarity between the generated images and the global text prompt. }
\label{tb:perclass}
\end{table*}
% \cref{fig:framework} depicts the network architecture of the composition module. Denote $m$ as local MLP $\{\boldsymbol{\theta}_l\}_{l=1}^{m}$ for each local frame. Then, we introduce the global MLPs including density $\boldsymbol{\theta}_{g_d}$ and $\boldsymbol{\theta}_{g_c}$ calibrators to refine $\boldsymbol{\sigma}_l$ and $\boldsymbol{C}_l$. 
% %
% In detail, the network design is, 
% {
% % \setlength\abovedisplayskip{4.5pt}
% % \setlength\belowdisplayskip{4.5pt}
% \begin{align}
% \label{eq:g_c_d}
% {\boldsymbol{\sigma}_g}  &= \alpha_d \boldsymbol{\theta}_{g_d}({\boldsymbol{\sigma}_l}) + \boldsymbol{\sigma}_l, \\  
% {\boldsymbol{C}_g}  &= \alpha_c \boldsymbol{\theta}_{g_c}({\boldsymbol{C}_l},  {\boldsymbol{d}_g}) + \boldsymbol{C}_l, 
% \end{align}}
% %
% where residual $\boldsymbol{\sigma}_l, \boldsymbol{C}_l$ assist in learning $\boldsymbol{\sigma}_g$ and $\boldsymbol{C}_g$, while $\alpha_d, \alpha_c$ balance their contribution as learnable parameters.
% %
% Note that the color-based omits density calibration, and simply uses the shared color refinement.



% The 3D boxes are only used for the spatial configuration of local NeRFs, while the implicit representation of local NeRFs is inferred by the canonical samples inside the local frame without considering the global relationship across different objects.
% To relieve such location-dependent effects, we further calibrate the output color and density from the local NeRF with global coordinates $({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g)$ and ray directions $\left({\boldsymbol{\phi}}_{d}, {\boldsymbol{\theta}}_{d}\right)$ as the conditional input.
% % to inject the global visual clues.
% %
% %
% Specifically, we adopt a shared MLP $\boldsymbol{\theta}_{g}$ to calibrate all the predicted object colors, that is,
% {\setlength\abovedisplayskip{4.5pt}
% \setlength\belowdisplayskip{4.5pt}
% \begin{align}
% \label{eq:MLP_dyn_2}
% {\boldsymbol{C}_g} = {\boldsymbol{C}_l} + \boldsymbol{emb}_{g} &= {\boldsymbol{C}_l} + \boldsymbol{\theta}_{g}({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g, {\boldsymbol{\phi}}_{d}, {\boldsymbol{\theta}}_{d}),
% \end{align}}
% where ${\boldsymbol{C}_l}$ is the color predicted by the local NeRF.
% Therefore, the scene color can preserve the view-consistent behavior from the original architecture and add consistency across poses for the volumetric density.
% Since the color and density values share the same latent expression in $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$, we only calibrate the emitted scene color explicitly with the scene location, as the densities of local NeRFs also are implicitly adjusted during optimization.

% \noindent \textbf{Global and Local Volumetric Rendering.}
% After compositing all the interacted points, each ray $\boldsymbol{r}_i$ collects a set sampling points by $\{\boldsymbol{t}_{i,j,n} \}_{j=1, n=1}^{m_j, N}$, where $m_j$ is the number of the hit object.
% For each sampling point, the inference results with the respective 3D representations are the local color $\boldsymbol{c}_{l}$, global color $\boldsymbol{c}_{g}$, and density $\sigma$.

% In fact, the local view $\hat{C}_{l,j}$ of single object $j$ also can be rendered by the sampled points  belongs to the same local frames as shown at Fig.~\ref{fig:framework}.

\subsubsection{Recomposition}
Our architecture advances scene reconstruction by providing an intuitive interface for layout manipulation.  This capability is crucial for the reconfiguration of scene elements into novel scenes, as depicted in \cref{fig:framework}. Here, the input panel allows for adjustments in the attributes of bounding boxes, such as modifying the position and scale of the 'apple' bounding box prior to composition. The refinement process further involves sampling ray-box intervals from the global frame, leading to transformed coordinates with the corresponding ray samples that are then incorporated into the pipeline, as demonstrated in \cref{fig:compo}.
%
Each bounding box represents an individual NeRF, providing the flexibility to move, scale, or remove elements as needed. CompoNeRF's capabilities also extend to textual edits, exemplified by the transformation of 'wine' into 'juice'.
%
Since NeRFs have been well trained, we only finetune \(\theta_g, \theta_l\) to align text prompts to promote consistency of both local and global views.
%
Moreover, the NeRFs once retrained within the edited scene, are also structured to be decomposable and cacheable in future scene compositions.
% Our CompoNeRF architecture facilitates the seamless reconstruction of scenes leveraging existing models. It enables precise editing of bounding boxes parameterized by \(\{\boldsymbol{\theta}_l\}_{l=1}^{m}\), allowing for their reconfiguration into new layouts. Refer to \cref{fig:framework}, the input panel permits the modification of attributes such as the position and scale of the 'apple' node's bounding box prior to composition. The process is further refined by sampling from the updated ray-box intervals within the global frame, which are then projected onto \(\boldsymbol{x}_l\), ensuring a streamlined reconstruction that integrates the 'apple' effectively. This addition is executed with careful attention to color consistency, positioning the 'apple' adjacent to the 'French bread' to complement the scene's overall palette. Each bounding box represents an individual NeRF, which means they can be manipulated through moving, scaling, and removal operations. CompoNeRF also extends its editing prowess to textual modifications, as evidenced by the 'wine cup' now appearing filled with juice—a change propagated through both subtexts and the global test. 
% %
% Since NeRFs have been well trained, we only finetune $\theta_g, \theta_l$ to align text prompts to promote consistency of both local and global views . 
% %
% Moreover, the NeRFs, once retrained within the reimagined scene, are also structured to be decomposable and cacheable for subsequent scene compositions.

% , as shown in Fig.~\ref{fig:framework}.
% For each scene described by the multi-object text prompt $T$, we
% To enhance the guidance of local representations, we use the local text prompt $T_l \subseteq T$ of a single object to optimize the local NeRFs in local views.
% The scene views $\hat{\boldsymbol{X}}_g=\{\hat{\boldsymbol{C}}_{g,i}\}_{i=1}^{H\times W}$ is obtained from the predicted pixel values of $H \times W$ rays by compositing all the ray-box interaction values.
% Similarly, the rendered view $\hat{\boldsymbol{X}}_{l,j}$ of the local frame $\boldsymbol{\theta}_j$ without compositing other objects can be calculated by $\hat{\boldsymbol{C}}_{l,j}$, as depicted in Sec.~\ref{ssec:render}.
% We use the local color instead of the globally calibrated color to obtain a local view because the local NeRF should learn the object identity unrelated to its placed position, as the position can be different during user edition.
% % Compared to cropping the local region from a global view for training, separate rendering can avoid the undesired information from other objects brought by the occlusion and resolution adjustments.
% Formally, we employ the following loss as the learning objective,
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/editing.pdf}
    % \vspace{-23pt}
    \caption{\textbf{Scene Editing Outcome:} Demonstrated here are the stages of our recomposition, utilizing cached source scenes. Each NeRF is individually identified by colorful labels. These decomposed nodes are then positioned in the initial layout and subsequently calibrated to form the final composition. The detailed description of the ambient environment is underscored, enhancing the scene's realism.}
    \label{fig:app}
    % \vspace{-12pt}
\end{figure*} 
\subsubsection{Optimization}
\label{sec:optimization}
During optimization, our method employs dual text guidance to align rendering results with both global and local textual descriptions. The optimization objective is:
{
\small
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eqn:loss_f}
\mathcal{L}= {\alpha_g}\nabla\mathcal{L}_{\text{SDS}}(\hat{\boldsymbol{X}}_{g}, T) + {\alpha_l}\sum_{j=1}^{m} \nabla\mathcal{L}_{\text{SDS}}(\hat{\boldsymbol{X}}_{l,j}, T_{l,j}) + \beta\mathcal{L}_{\text{sparse}},\nonumber
\end{equation}
}where $T$ signifies the global text prompt, while $T_{l}$ pertains to a specific object within the global context. The hyperparameters $\alpha_{g}, \alpha_{l}$, and $\beta$ modulate the respective loss weights. 
% $\nabla \mathcal{L}_{\text{SDS}}$ is the score distillation sampling loss, as described in Sec.~\ref{sec:background}.
As suggested in~\cite{metzer2022latent}, we use $L_{\text{sparse}}$ included to penalize the binary entropy of local NeRFs' densities, thereby mitigating the issue of extraneous floating radiance.
Additionally, incorporating directional cues such as "front view" or "side view" into the input text, as suggested by \cite{poole2022dreamfusion,metzer2022latent} proves beneficial in specifying camera poses during the training phase, further enhancing the alignment of our generated scenes with the intended perspectives.
% Note that the global calibration in the scene frame can adaptively revise both $({C}_l, {\sigma})$ in local NeRF with $\nabla \mathcal{L}_{SDS}$ along with the back-propagating gradient.
