\section{Discussion}

% The compositional flexibility of our editable 3D scene layout enables intricate scene edits, including text modifications, repositioning, scaling, duplication, and removal of objects, culminating in the recomposition of an entirely new scene through the manipulation of individual learned local NeRFs. 
Our proposed CompoNeRF represents a preliminary step in handling multi-object text for text-to-3D generation. It unleashes its potential by composing scenes with reusable NeRF components, which also facilitates later editing. 
%
Fig.~\ref{fig:app} showcases our refined scene renderings originating from pre-trained source scenes.
%
Our process commences with a foundational scene, to which we introduce additional entities, such as glass balls within a bedroom context. 
Each component of the scene is encapsulated by a localized NeRF.
However, a direct amalgamation of these components can manifest various flawsâ€”artifacting at the base of the lamp and incongruous shadows and reflections from the glass ball are notable, detracting from the authenticity of the scene's ambiance.
%
Subsequent to the manipulation phase, the reconfigured objects are adeptly integrated, achieving a coherent and consistent global scene in alignment with the edited layout. The nuanced changes in the materials of the lamp, the nightstand, and their reflective surfaces 
demonstrates the system's adaptability to diverse source inputs, which also involves a challenging text prompt containing subtle interplay within multi-object context. 
% warmth and the twilight's progressive dimming, or the detailed context surrounding the 'mahogany' nightstand.
%
% Adjusting existing objects involves simple modifications to the bounding box properties like the center point and scale. Moreover, these manipulations can be collectively employed to forge a scene that conforms to multiple user-defined directives, 
% Alternatively, we can further finetune the edited scene with a few finetuning steps to improve the view consistency.
% As the duplication, removal, and recomposition, we can first input the 3D boxes and then load each box with a local text prompt from a learned local NeRF collection, \eg, copy the single \emph{nightstand} into the other at opposite locations.
% Nonetheless, we are unable to solve the multi-face problem in this paper since our box layout guidance is ambiguous for prompts with direction. In this section, we discuss these two points.

% For text editing on a specific object, we simply change a certain part of the text prompt, \eg, 'a vase of sunflower' to 'a vase of rose' at both global and local text prompts and finetune the scene with a few steps.
% \noindent\textbf{Editable Scene Rendering and Finetuning.}
% Due to the compositional capacity brought by the editable 3D scene layout, we can perform scene editing by text editing, moving, scaling, duplicating, removing the single object, and re-composting a new scene by manipulating the layout of each learned local NeRF.
% Fig.~\ref{fig:app} shows our edited scene rendering results based on a pre-trained scene.
% For instance, We utilize the source scene and glass balls, and the bedroom. Then each decomposed local NeRF are listed in the decomposition part, when they are put together directly, artifacts at the bottom of the lamp, or inconsistency the ball shadow are not natural and no reflections, or lack of description of the overall scene, warm glow with darkening twilight, or the intricate description of the context of the nightstand, 'mahogany'. 
% We can see that the manipulated objects are seamlessly integrated into the scene while ensuring the correct spatial relationship following the edited layout.
% Specifically, the material changes in the lamp and the body, with the contex of the nightstand, reflections. 
% %
% When moving and scaling existing objects, we only need to adjust the box property, such as the center point and box scale.
% Furthermore, all types of manipulation can be combined to generate a scene with multiple user control inputs.
% \noindent\textbf{Limitations.} 
% We summarizes our constraints including, 
% 1) limited ability to handle complex semantics, like ones including human or uncommon integration of objects for scene reconstruction. 
% 2) Multi-face problem exists without explicit supervision such as mesh since we only use bounding boxes, \eg,  duplicated face views may appear. 
% We refer readers for more comprehensive evaluation including videos, visualization and description in out suppl. materials. 
\noindent\textbf{Limitations.} Our method has certain limitations:
1) restricted semantics processing, particularly interpreting uncommon object integration.
2) multi-face issue in the absence more intricate supervision. 
For more thorough evaluation, we direct readers to our \textit{suppl.} materials.

 % The 3D scene layout interpreted the multi-object text prompt as a set of local NeRFs binding with a spatial box and object-specific text prompt.
% The whole scene view is rendered by compositing local NeRFs defined in the layout.
% We also designed a composition module and multi-level text guidance for improving the quality of generated 3D scenes.
\section{Conclusion and Future Work}
In this work, we have proposed 
% a multi-object text-guided compositional 3D scene generation framework, called CompoNeRF, 
a novel framework for multi-object text-guided compositional 3D scene generation with an editable 3D scene layout.
% based on an editable 3D scene layout.
Our framework interpreted a multi-object text prompt as a collection of localized NeRFs, each associated with a spatial box and an object-specific text prompt, which were then composited to render the entire scene view. We have further enhanced the framework with a specialized composition module for global consistency and dual-level text guidance, which effectively mitigated the issue of guidance collapse in the multi-object generation. Utilizing Stable Diffusion model, we have demonstrated that our method, the first to apply a compositional NeRF design to the text-to-3D task, can produce high quality 3D models that feature multiple objects and perform well compared with contemporaneous methods.
% Despite that our flexible box layout may not be direction-aware and is unable to solve the multi-face problem, we have addressed the guidance collapse issue well in the multi-object scene reconstruction. 
% Working with the large-scale Stable Diffusion model, we demonstrated that our method, the first attempt to adopt compositional NeRF design in the text-to-3D task, can generate compelling 3D models with multiple objects, comparing favorably to available concurrent works. 
Looking ahead, we have explored a promising application of CompoNeRF in the realm of scene editing, which allows for the reuse of trained models in scene recomposition. This capability opens up new possibilities and identifies a rich vein of future work to be pursued in this domain.
% Finally, we investigated an exciting application of our method for scene editing and reusing trained models for scene recomposition, identifying an avenue for future work.

% Currently, our method relies on the user input 3D layout to render a text-guided scene, which can be further optimized by adopting a learnable dynamic layout to make the compositional pipeline in an end-to-end way. 
% To mitigate the influence of unconstrained generation ability of diffusion model, we can further automatically calibrate the language information for diffusion model to generate more consistent views. 
% Besides, to achieve more realistic scene editing, it is also promising to integrate the scene lighting model into the framework in the future work.