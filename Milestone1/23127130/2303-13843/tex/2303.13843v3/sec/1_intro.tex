\section{Introduction}

\label{sec:intro}
Recent advances in text-to-image generation have been driven by the integration of vision-language pre-trained models~\cite{radford2021learning,li2022blip} with state-of-the-art diffusion processes~\cite{ho2020denoising,nichol2021improved,rombach2022high}, leading to impressive outcomes. Pioneering text-to-3D approaches~\cite{jain2022zero,sanghi2022clip,hong2022avatarclip,JunGao2022GET3DAG,mohammad2022clip,lee2022understanding,xu2022dream3d} have built upon these successes, employing these robust vision-language models to enrich 3D generative models with the structured understanding provided by Neural Radiance Fields (NeRFs)~\cite{mildenhall2020nerf,mip-nerf,muller2022instant}. This synergy~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} facilitates the creation of 3D models which, when rendered from different views, cohere with the learned text-to-image diffusion model distribution, opening new avenues for the direct synthesis of 3D content from textual descriptions. 

% Text-to-image generation has achieved tremendous success
% Text-to-image generation has recently made significant strides, 
% coupling the vision-language pre-trained models~\cite{radford2021learning,li2022blip} with advanced diffusion techniques~\cite{ho2020denoising,nichol2021improved,rombach2022high} to achieve remarkable results.
% % These breakthroughs have also yielded far-reaching implications in text-to-3D generation
% Pioneering methods~\cite{jain2022zero,sanghi2022clip,hong2022avatarclip,JunGao2022GET3DAG,mohammad2022clip,lee2022understanding,xu2022dream3d} in text-to-3D leverages the prowess of pre-trained vision-language models inform 3D generative model utilizing NeRFs representations~\cite{mildenhall2020nerf,mip-nerf,muller2022instant}. 
% %
% This synergy ~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} enables the generation of 3D models that, when rendered from different perspectives, align seamlessly with the distribution learned from text-to-image diffusion models, thereby unlocking new potentials in synthesizing 3D content directly from text prompts. 
% using powerful vision-language pre-trained models. 
% More recently, several text-to-3D methods~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} have shown that matching the rendered views from the differential 3D model, such as Neural Radiance Fields (NeRFs)~\cite{mildenhall2020nerf,mip-nerf,muller2022instant}, with the learned text-to-image distribution from pre-trained diffusion model can produce remarkable results.

% The rapid development of text-image models~\cite{radford2021learning,li2022blip} has yielded far-reaching implications in text-guided image generation~\cite{ramesh2021zero,patashnik2021styleclip,ramesh2022hierarchical,saharia2022photorealistic,rombach2022high}.
% % models~\cite{ramesh2022hierarchical,saharia2022photorealistic,rombach2022high}.
% Notably,  recently text-guided image generation has achieved tremendous success when coupled with diffusion models~\cite{ho2020denoising,nichol2021improved,rombach2022high}.
% Text-guided image generation has recently gained much attention and success in both academia and industry,  
 % achieved tremendous success
% These breakthroughs have also accelerated the development of text-guided 3D generation ~\cite{jain2022zero,sanghi2022clip,hong2022avatarclip,JunGao2022GET3DAG,mohammad2022clip,lee2022understanding,xu2022dream3d} using powerful pre-trained text-image models. 
% More recently, 
% % text-conditional 3D generation methods have
% it has been shown that the diffusion model can serve as a powerful critic to optimize the underlying differential 3D representations~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score}, such as Neural Radiance Fields (NeRFs)~\cite{mildenhall2020nerf} for text-to-3D generation.
Textual descriptions provide a broad specification for 2D images or 3D models, thus the translation into coherent visual representations, especially for scenes with multiple objects, is far from straightforward. Diffusion models, such as Stable Diffusion~\cite{rombach2022high}, have been extensively trained on vast text-image datasets~\cite{schuhmann2022laion}, yet they often stumble when tasked with interpreting text involving multiple objects with sparse representation in the training data. This results in the generation of images that miss objects or depict them incorrectly. \cref{fig:intro}(a) exemplifies how Stable Diffusion does not reliably maintain the distinction and arrangement of objects in even simple multi-object scenarios.
%
For instance, it might overlook an apple or banana or render them with inaccurate colors.
%
This issue, known as ‘guidance collapse’, is particularly detrimental to the rendering of multi-object scenes with text prompts. As a consequence, sophisticated models like Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} fall short in their ability to fully realize the complex arrangements described in multi-object texts, which severely constrains their use for composing 3D scenes from textual descriptions.

% However, the textual description is often an abstract specification for a desired target 3D model or a 2D image.
% Despite that the powerful diffusion models, \eg, Stable Diffusion~\cite{rombach2022high}, have been trained on billions of text-image pairs~\cite{schuhmann2022laion}, it is still a challenge to generate geometrically coherent images across different viewpoints from the text.
% Unfortunately, the diffusion model may produce inaccurate results~\cite{feng2022training} given text containing multiple objects, resulting in missing objects or semantic confusion.
% For example, Fig.~\ref{fig:intro} demonstrates that Stable Diffusion fails to maintain object identities and geometric coherence even with a simple multi-object text.
% It obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash \textit{guidance collapse}, especially when rendering multi-object scenes with text prompts.
% As a result, state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models can only generate part of concepts in the multi-object text shown in Fig.~\ref{fig:intro}, limiting their application for object-compositional 3D scene generation from text prompts.



% However, the learned 2D generative distribution in pre-trained diffusion model 
% Despite they have , the 2D supervision unconstrained generative capacity of the diffusion model is still considered as the major challenging issue towards comprehensive text-to-3D generation.
% across different viewpoints, given the input text prompt.
% may produce inaccurate results~\cite{feng2022training}, resulting in missing objects or semantic confusion.
% This obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash guidance collapse, especially when rendering complex scenes from multi-object texts.
% For example, Fig.~\ref{fig:intro} demonstrates that the state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models fail to maintain the correctness of object identities and the geometric coherence when presented with the simple multi-object text.
% The reason is that the text-to-image generative ability in the diffusion model remains inherently unconstrained in the 3D space. 

% Despite the impressive text-to-3D results achieved by the diffusion model, the diffusion model may produce inaccurate results~\cite{feng2022training}, resulting in missing objects or semantic confusion.
% This obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash guidance collapse, especially when rendering complex scenes from multi-object texts.
% For example, Fig.~\ref{fig:intro} demonstrates that the state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models fail to maintain the correctness of object identities and the geometric coherence when presented with the simple multi-object text.
% Although these text-to-3D models have shown , the diffusion model may produce inaccurate results~\cite{feng2022training}, resulting in missing objects or semantic confusion.
% This obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash guidance collapse, especially when rendering complex scenes from multi-object texts.
% For example, Fig.~\ref{fig:intro} demonstrates that the state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models fail to maintain the correctness of object identities and the geometric coherence when presented with the simple multi-object text.
% 
% Moreover, when given text that includes multiple objects, the diffusion model may produce inaccurate results~\cite{feng2022training}, resulting in the missing object or semantic confusion, leading to guidance collapse.
% Although text-to-3D models have shown impressive results, a hurdle for them is that they are hindered by the guidance collapse  
% An example in 
% The reason is that the text-to-image generative ability in the diffusion model remains inherently unconstrained in the 3D space. 


% Therefore, it is natural to ask whether the multi-object text guidance between 3D representations and diffusion models can be better constrained for multi-object generation.
% A solution is to make the 3D representation and rendering network object-aware by binding object-related information to particular locations in the 3D space.
% However, existing approaches~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to encode the entire scene into a single neural network, which can successfully handle object-centric scenes but is generally agnostic to the object's identity.
% This leads to an intriguing inquiry: Can the generic distribution learned by diffusion models accurately encapsulate and reconstruct the nuanced elements of a text describing multiple objects for 3D scene creation? Our observations, as depicted in Figure~\ref{fig:intro}, reveal that diffusion models more reliably render individual objects when provided with localized text prompts for each. Inspired by this, we propose the implementation of granular textual guidance as a solution to the prevalent ‘guidance collapse’ encountered in current models~\cite{metzer2022latent,wang2022score} with multi-object descriptions. 
% The logical approach is to apply targeted guidance for each object individually, enhancing object-specific awareness within the 3D modeling and rendering processes. Nevertheless, the prevailing methods~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to rely on a singular neural network to process the entire scene holistically, which poses significant challenges in assimilating segregated guidance for multiple objects during training due to their overarching scene-centric nature.

% Therefore, it naturally raises the question:  \textit{whether the agnostic distribution of the diffusion model can accurately learn and compose all the concepts in a multi-object text for 3D scene generation.}
This provokes an intriguing question: \textit{Can diffusion models, equipped with the generic distribution, not only discern and regenerate individual components as specified by a multi-object text but also integrate them into a consistent 3D scene?}
%
Our initial tactic, as shown in ~\cref{fig:intro}(b.1), attempts to manage this by assigning each object to a distinct NeRF with a shared context, diverging from the traditional single-network scene encodings~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score}. 
This approach aims to improve object recognition, yet it's prone to ‘guidance collapse' due to the global text prompts' inability to offer precise semantic delineation for individual objects.
%
Inspired by more accurate rendering of single objects with targeted subtext prompts as seen in (a), the subsequent refinement in (b.2) utilizes specific textual supervision for each object. This tailored guidance confirms that diffusion models more effectively render individual objects when provided with explicit textual for each, thereby overcoming the ‘guidance collapse’ issue in complex multi-object scenarios.
%
% However, when rendering NeRFs directly, we observe the occlusion is not correct in their overlapped regions due to lack of global refinement. 
Nevertheless, the direct rendering of NeRFs often results in incorrect occlusions within their overlapping regions, indicating a deficiency in global refinement.
%
To improve it, as detailed in (b.4), we refine sampling points that were originally guided by texts corresponding to individual objects, now with the incorporation of global textual guidance. This supplementary global oversight guarantees a harmonious rendering that upholds the unique identities of the objects while fostering overall compositional unity. Its vital role is underscored by its absence in (b.3), where its omission confirms its necessity.
% This motivates us to introduce more fine-grained text guidance to tackle the guidance collapse issue in existing frameworks~\cite{metzer2022latent,wang2022score} when using multi-object text prompts.
%
% Thus, a straightforward solution is to bind object-oriented guidance to each object, making the 3D representation and rendering pipeline object-aware.
% Existing approaches~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to use a single neural network to encode the entire scene, making it hard to incorporate the decomposed guidance during training as they are generally agnostic to the object's identity and quantity.

\begin{figure}[t]
    \centering
    % \vspace{-2pt}
    \includegraphics[width=.94\linewidth]{figures/sd.pdf}
    % \vspace{-10pt}
    \caption{
    % Comparison of our CompoNeRF with Latent-NeRF~\cite{mirzaei2022laterf} and SJC~\cite{wang2022score}.
    % It shows that using different level guidance in multi-object text can address the unconstrained generation of Stable Diffusion~\cite{rombach2022high}.
    \textbf{The guidance collapse issue \& Our solutions}.  
    (a) Generation of multi-level scene utilizing the frozen Stable Diffusion. Instances of guidance collapse are observed when using global text ($gt$). 
    (b) Our proposed strategies for multi-object scene composition align with \cref{eq:multi_volrend}. The areas of NeRF overlap are indicated in gray. The green nodes represent composited samples. Our design is highlighted by the \underline{dashed box}.
    1. Apply $gt$ as the overarching supervision.   
    2. Employ specific subtexts to direct each individual NeRF object.
    3. Form composite samples by following step (2).
    4. Refine the composite output with $gt$, building on step (3).
    }
    % and \textit{guidance collapse} issue on generating a simple multi-object scene, ``a red apple and a yellow banana".
    % unconstrained generation problem associated with Stable Diffusion~\cite{rombach2022high} in multi-object scenarios.
    % The unconstrained generation in Stable Diffusion.
    % The unconstrained generation in Stable Diffusion.
    %  poses a challenge, particularly for sentences that deßscribe multiple objects.
    % % Stable Diffusion struggles to distinguish the context and quantity of rendering targets in 2D, which can mislead Text-to-3D generation methods. 
    % In contrast, by providing guidance from both global (scene) and local (object) levels, our object-aware modeling approach can produce more accurate and faithful scenes
    \label{fig:intro}
    % \vspace{-14pt}
\end{figure}

% we introdce a compositional NeRF framework, called \textbf{CompoNeRF}, interpreting multi-object text guidance as editable 3D scene boxes with fine-grained text prompts.
% The scene layout collects individual object entities from the input text and gathers their editable 3D bounding boxes as shown in Fig.~\ref{fig:teaser}.
% % As shown in Fig.~\ref{fig:teaser}, the scene layout collects individual object entities from the input text, input 3D bounding boxes, and their corresponding coordinates and scales.
% In CompoNeRF, each box in the 3D scene layout is modeled by a local NeRF for representation learning, and global views are rendered by compositing the learned 3D representations from local NeRFs.
% However, direct composition from all local NeRFs may not ensure coherent global views without addressing the following two issues.
% Nevertheless, the direct composition from all local NeRFs may not ensure the coherent global views, because it does not tackle the following two key issues.
% directly from the composited local views as 

% The first obstacle in achieving global view consistency across multiple local NeRFs is that they are optimized independently, making it difficult to learn their global spatial or semantic correlations. 
% To address this,
% \textbf{1) Independence}: capturing consistent global views across multiple local NeRFs is hard due to non-shared parameters.
% To solve this, we use a global MLP to calibrate the local NeRF predictions based on their samples' global coordinates and ray directions.
% The model can gradually learn global consistency across multiple objects by passing the global constraint into local NeRFs.

% \textbf{2) Occlusion}: inaccurate text guidance may result from fully occluded objects due to random camera positions in the training data.
% To tackle the occlusion issue, we apply local text guidance to each individual NeRF, rendering locally and utilizing the diffusion model to provide more precise guidance for object identification despite occlusions.
% To tackle the occlusion issue, for each local NeRF, we separately apply local text guidance on the locally rendered view, as the diffusion model can provide more accurate guidance to shape the object identity despite occlusions.
% To tackle the occlusion issue, we employ local text guidance on every individual NeRF-rendered view. Thanks to the use of the diffusion model, we are able to provide precise guidance for object identification even in the presence of occlusion.
% % As a result, our approach ensures the global consistency, generating realistic 3D content, even in crowded scenes, 
% % as demonstrated in Figure~\ref{fig:teaser}, without missing objects or ambiguity.
% As a result, our approach ensures coherent and realistic 3D generation, even in crowded scenes that include multiple objects as depicted in Fig.~\ref{fig:teaser} without object missing or text ambiguity.

\begin{table*}[t!]
\centering
\resizebox{\linewidth}{!}{ 
    \begin{tabular}{l|c|c|c|c|c|c}
    \toprule
    % \hlineB{2.5}
    \textbf{Methods} & \textbf{Diffusion Model} & \textbf{\textbf{3D Representation}} & \textbf{Scene Rendering} & \textbf{Input Prompt} & \textbf{Scene Editing} & \textbf{recomposition}\\  \hlineB{2.5}
    DreamFusion & Imagen & Mip-NeRF 360~\cite{Barron2021MipNeRF3U} & Object-centric & Text & T & \XSolidBrush\\    \hline
    Magic3D & eDiff-I + SD & Instant-NGP~\cite{Mller2022InstantNG} & Object-centric & Text & T & \XSolidBrush\\    \hline
    DreamBooth3D & DreamBooth+DreamFusion & Mip-NeRF~\cite{Barron2021MipNeRFAM} & Object-centric & Text+Images & T & \XSolidBrush \\    \hline
    Points-to-3D & ControlNet+Point-E & Instant-NGP & Object-centric & Text+Image & T & \XSolidBrush \\  \hline
    Fantasia3D & SD+PBR & DMTET~\cite{Shen2021DeepMT} & Object-centric & Text/Fine Shape & T/M/S/R & \Checkmark \\
    \hline
    \hline
    Latent-NeRF & SD & Instant-NGP & Object-centric & Text+Fine Shape & T & \XSolidBrush \\    \hline
    SJC & SD & voxel radiance field & Object-centric & Text & T & \XSolidBrush\\    \hline
    \textbf{Ours} & SD & Instant-NGP & Object-compositional & Text+3D Layout & T/M/S/R & \Checkmark \\  
    \bottomrule
     \hlineB{1.5}
    \end{tabular}
}
% \vspace{-7pt}
\caption{\textbf{Comparison of our method with the related works for text-to-image generation}. SD denotes Stable Diffusion. For scene editing, we use T(editing object with text), M(moving object), S(scaling object), and R(removing object) for short.}
% \vspace{-12pt}
\label{tab:compare}
\end{table*}
% The process is initiated by extracting individual objects from textual input and positioning them within modifiable 3D bounding boxes. Each bounding box in CompoNeRF is not only adjustable but also anchored by a distinct local NeRF that facilitates the learning of object representations. The global scene is then rendered by strategically merging these localized models with our tailored composition module.
We present \textbf{CompoNeRF}, a compositional NeRF framework that interprets multi-object text prompts as editable 3D scene layouts with granular text prompts, as shown in \cref{fig:teaser}. 
The procedure begins by identifying individual objects from the textual description and positioning them within customizable 3D bounding boxes. Each box is supported by a distinct NeRF and a subtext label. Thus, CompoNeRF is designed to accommodate alterations, allowing for manipulations in the layout—like moving, scaling, or removal, as well as direct text edits. Our specialized composition module ensures that the global scene emerges not from a static assembly but from an orchestrated composition of these adaptable local models. The qualitative validation of CompoNeRF’s performance is presented in Table~\ref{tab:compare}, benchmarking it favorably against contemporary methods.

CompoNeRF distinguishes itself with three core capabilities: it \textit{composes} multi-object scenes from textual prompts, \textit{decomposes} by archiving each NeRF for subsequent utilization, and \textit{recomposes} by employing this curated content gallery to rapidly generate elaborate 3D scenes, thereby streamlining the workflow of 3D content creation. 

Next, we introduce a more objective evaluation method to address the lack of rigorous quantitative analysis in prior works, which often rely solely on visual quality comparisons. By employing the averaged CLIP score~\cite{wang2022clip} on rendering views of 3D content against global text prompts, we quantitatively measure the alignment of our generated scenes with their textual prompts, demonstrating the effectiveness of CompoNeRF in producing detailed and coherent 3D scenes that accurately reflect the given text descriptions.

% Hence, CompoNeRF stands as a comprehensive solution that significantly enhances the quality and coherence of text-driven 3D scene generation.”
% Moreover, scene editing is facilitated by the use of 3D scene layout, allowing flexible manipulation of text and objects, including scaling, movement, and removal.
% Using a vast pre-trained content gallery, we can rapidly generate desired 3D scenes with text prompts and layouts, democratizing 3D content creation.
% The comparisons of related approaches are summarized in Tab.~\ref{tab:compare}.
% Additionally, previous studies fail to include a quantitative comparison, resulting in inadequate evaluation relying solely on visual comparison. We thus propose to employ the extensively recognized CLIP score~\cite{wang2022clip} to assess the efficiency of generating 3D models from text descriptions. By doing so, we provide a more comprehensive demonstration of our proficiency in producing intricate multi-object scenes.

To encapsulate, our paper makes three key contributions:  
(\textbf{I})  
% We solve the guidance collapse issue in multi-object 3D scene generation by integrating an editable 3D layout with multiple local NeRFs to precisely associate guidance with specific objects. Additionally, all local NeRFs can be cached, edited, and reused for composing other scenes.
We address the ‘guidance collapse’ problem in creating multi-object 3D scenes. Our innovative use of editable 3D layouts coupled with multiple localized NeRFs allows for precise direction over individual object representations. Moreover, these localized NeRF models are designed to be storable and reusable, enhancing efficiency in scene composition. 
(\textbf{II}) We introduce a composition module 
% to calibrate the overall rendering and varying levels of text guidance to maintain the identity of individual entities while ensuring global coherence. 
enables fine-tuning of the rendering process and text-based guidance, ensuring both the distinctiveness of individual objects and the holistic integration within the scene.
(\textbf{III}) 
We conduct extensive evaluations of CompoNeRF’s performance in multi-object scene generation, employing both qualitative and quantitative assessment for the multi-object text-to-3D task. The rigorous testing confirms that our CompoNeRF outperforms existing models in generating multi-object scenes that closely align with textual prompts.
% We thoroughly evaluate the effectiveness of our proposed method across various multi-object scenarios, demonstrating its ability to composite and edit 3D scenes via qualitative and quantitative analysis. We propose to use the CLIP score as the metric to evaluate the similarity of the generated 3D assets to the prompt text, and our CompoNeRF achieves the best performances on various multi-object scenes.
% In addition, the editable 3D layout facilitates object editing by allowing users to modify text and manipulate objects flexibly, including moving, scaling, and duplicating. With the aid of a vast pre-trained content gallery, users can rapidly generate their desired 3D scenes using text prompts and 3D scene layouts, democratizing the process of 3D content creation. 
% we compare CompoNeRF and other editing approaches summarized in Tab.~\ref{tab:compare}.
% A solution is to make the 3D representation and rendering network object-aware by binding object-related information to particular locations in the 3D space.
% However, existing approaches~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to encode the entire scene into a single neural network, which can successfully handle object-centric scenes but is generally agnostic to the object's identity.


% To overcome these challenges,
%  we propose a novel framework, called \textbf{CompoNeRF} that allows generating and editing scenes with editable 3D layouts in a compositional way.
% Given a text prompt involving multiple objects, CompoNeRF consisting three steps to composite object (\ie, local) views:
%  1) we parse the input text into individual object entities and gather coarse 3D bounding box information for each object, as such 3D layout can be easily obtained from the input text or users;
% 2) using the generated 3D scene layout, each object entity has its own local NeRF model to produce 3D information based on object-specific text guidance; 
% 3) we construct a unified scene rendering pipeline to render the complete scene by compositing all object-specific 3D information from local NeRFs into the scene view.

% To , given a text prompt involving multiple objects, we parse the input text into individual object entities and gather coarse 3D bounding box information for each object, as such 3D layout can be easily obtained from the input text or users.

% Despite the simplicity of our framework design,
% Nevertheless, these compositional designs can not ensure learning the coherent scene (\ie, global) views directly from the composited local views if the following two issues are not tackled.
% Firstly, the scene view consistency is hard to capture across multiple local NeRFs models as the none shared parameters.
% Therefore, we further adopt a global NeRF to calibrate the local NeRF prediction using the scene coordinate as conditional input.
% With the global-level text guidance applied to the global views composited from the local views, the global consistency can be gradually learned among multiple objects.
% Secondly, some objects can be completely occluded in the scene views, caused by random camera poses during training, rendering both global and local text guidance inaccurate.
% To tackle the occlusion issue, for each local NeRF, we separately apply local text guidance on the local rendered view as the diffusion model can provide more accurate guidance to shape the object identity, as shown in Fig.~\ref{fig:intro}.
%
% %
% However, global semantics often require additional transformation and composition of individual objects. 
% For example, consider the scene in Fig.~\ref{fig:intro} containing an apple and a banana with overlapping bounding boxes. 
% When the bounding boxes of two objects overlap, they may appear merged in the final rendered image, causing ambiguity in the scene. 
% To prevent this, the global scene needs to adjust the latent expression of each object to ensure that they maintain their distinct identities. 
% Our scene representation model helps in achieving this by adjusting the object-level latent expression with the scene coordination and the full context of input text, which improves view consistency in the scene view.
% %
% We design both scene-level and object-level score distillation losses to better balance the rendering guidance within our hierarchical scene representation and rendering framework. 
% The scene-level loss helps the model to learn global semantics, such as the spatial relationship between objects and guides the model to generate a coherent scene. 
% The object-level loss enables the optimization of the representation to keep each object's identity and fits the interactions between objects based on the global semantics.

% \begin{table*}[t!]
% \centering
% \resizebox{\linewidth}{!}{ 
%     \begin{tabular}{l|c|c|c|c|c|c}
%     \toprule
%     \textbf{Methods} & \textbf{Diffusion Model} & \textbf{\textbf{3D Representation}} & \textbf{Scene Rendering} & \textbf{Input Prompt} & \textbf{Scene Editing} & \textbf{recomposition}\\  \hline
%     DreamFusion~\cite{poole2022dreamfusion} & Imagen~~\cite{sahariaphotorealistic} & Mip-NeRF 360~\cite{barron2022mip} & Object-centric & Text & T & F\\
%     Magic3D~\cite{lin2022magic3d} & eDiff-I~\cite{balaji2022ediffi} + SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-centric & Text & T & F\\
%     SJC~\cite{wang2022score} & SD~\cite{rombach2022high} & voxel radiance field & Object-centric & Text & T & F\\
%     DreamBooth3D~\cite{raj2023dreambooth3d} & DreamBooth~\cite{ruiz2023dreambooth}+DreamFusion~\cite{poole2022dreamfusion} & Mip-NeRF~\cite{JonathanTBarron2021MipNeRFAM} & Object-centric & Text+Images & T & F \\
%     Points-to-3D~\cite{yu2023points} & ControlNet~\cite{zhang2023adding}+Point-E~\cite{nichol2022point} & Instant-NGP~\cite{muller2022instant} & Object-centric & Text+Image & T & F \\
%     \hline
%     Latent-NeRF~\cite{metzer2022latent} & SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-centric & Text+Fine Shape & T & F\\
%     Ours & SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-compostional & Text+3D Coarse Boxes & T/M/S/R & T \\ \bottomrule
%     \end{tabular}
% }
% % \vspace{-7pt}
% \caption{Comparison of our method and the related works for text-to-image generation. SD denotes Stable Diffusion. For scene editing, we use T(editing object with text), M(moving object), S(scaling object), and R(removing object) for short.}
% % \vspace{-8pt}
% \label{tab:compare}
% \end{table*}




% Our results suggest that our approach provides a compelling solution for these tasks.
% \begin{itemize}
% \item We incorporate the editable 3D layout with multiple local object NeRF to accurately associate the guidance for specific structure in text-to-3D scene generation.
% \item We design a global color calibration module and different levels text guidance procedures to balance maintaining objects' identity and global coherence.
% \item We validate the proposed method under various scenarios, showing that it offers a compelling solution for compositional scene generation and flexible editing capacity.
% \end{itemize}