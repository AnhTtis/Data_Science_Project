\section{Experiments}
\label{sec: exp}

% \subsection{Experimental Setup}
% \noindent\textbf{Evaluation Benchmarks.}
% To address the compositional generation, we propose a new benchmark which consists of 6 our pre-defined compositional prompts and 12 natural prompts from MSCOCO~\cite{lin2014microsoft} where each contains at least two color words modifying different objects. We also switch the position of two color words to create a contrast caption similar to ~\cite{feng2022training}.
% For our benchmark, we conduct user studies to evaluate different methods based on user preferences.

% \noindent\textbf{Evaluation Metric.}
% We mainly rely on human evaluations for the visual quality and generation accuracy of compositional prompts.
% We show users two videos side by side rendered from a canonical view by two different algorithms using the same text prompt. We ask the users to select the more realistic and detailed one from Latent NeRF~\cite{metzer2022latent}, SJC~\cite{wang2022score}, and our method, respectively and indicate which 3D models demonstrate better semantic alignment or image fidelity.
% Each prompt is evaluated by 3 different users, resulting in XXX pairwise comparisons.
% We also investigate the object missing rate in our proposed benchmark from human evaluation.


\subsection{Qualitative Comparison}
% In Fig.~\ref{fig:sota}, we present qualitative comparisons of generated 3D assets given the same multi-object text prompt with our proposed CompoNeRF method, as well as the Latent-NeRF and SJC, which are the SoTA methods based on the same Stable Diffusion model.
% We observe that our method can accurately generate complex 3D models across a diverse set of prompts with \textbf{precise object identity} and more \textbf{sensible and fertile context} than others in the showcases.
In Fig.~\ref{fig:sota}, we conduct qualitative comparisons of 3D assets generated using our method against Latent-NeRF and SJC, all based on the same Stable Diffusion model. Our method exhibits a remarkable ability to generate complex 3D models from a wide array of multi-object text prompts, demonstrating \textbf{superior object identity accuracy} and \textbf{enhanced context relevance and richness} compared to its counterparts.
In simplier Case 3, our CompoNeRF method adeptly generates two distinct objects: a red \emph{apple} and a yellow \emph{banana}. In stark contrast, competing methods amalgamate the features of both fruits into a singular, indistinct object.
%
Case 5, which is more intricate, showcases our method's capability to render a realistic scene encompassing accurately depicted objects.
% such as \emph{wine}, \emph{table}, \emph{salad}, \emph{bread}, and the reflective properties of \emph{glass}. 
Conversely, other methods struggle to produce even recognizable objects in this scenario.
Note that we cannot corroborate the outcomes from DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d}, as these models are predicated on closed-source diffusion models.
% Moreover, our underlying 3D representation framework is compatible with most object-centric methods~\cite{poole2022dreamfusion,lin2022magic3d}. 
Once these models are publicly available, integration with our CompoNeRF can further enhance their multi-object modeling capability.
% , leveraging our Latent-NeRF backbone.
%
% For example, in the simple Case 3, our method accurately generates two separate objects, a red \emph{apple} and a yellow \emph{banana}. In contrast, other methods generate a single mixed object with characteristics of both fruits.
% %
% In the complex Case 5, we provide a realistic scene with accurate representations of \emph{wine}, \emph{table}, \emph{salad}, \emph{bread}, and \emph{table}, including the glass reflection. However, other methods fail to generate even recognizable objects.
% Note that we cannot validate the predicted results of DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d} model as they are built upon on close-sourced diffusion models.
% Besides, our underlying 3D representation can also be equipped with most object-centric methods~\cite{poole2022dreamfusion,lin2022magic3d} once they are released to achieve, allowing us to achieve better single-object modeling, similar to the use of our Latent-NeRF backbone.


\subsection{Quantitative Comparison}
In our study, we employ the CLIP score as the primary evaluation metric to assess the congruence between the generated 3D assets and the associated text prompts. This score, commonly used in text-to-image generation research as noted in studies~\cite{parmar2023zero,zhang2023sine,wang2023imagen}, is derived from the cosine similarity between the embeddings of the text and the image, both encoded by the CLIP model. For 3D assets, we project images from various views and calculate the CLIP score concerning the global text prompt, with the overall CLIP score being the average of these values. As detailed in \cref{tb:perclass}, our quantitative comparisons demonstrate the superior alignment of our method with the text prompt compared to the Latent-NeRF and SJC approaches in diverse scene configurations. Notably, in the challenging Case 5, our method shows a remarkable 54\% improvement. The result of overall enhancement in multi-object scenes underscores the robustness of our global calibration strategy.

% We use CLIP score as our evaluation metric to measure the similarity of generated 3D assets to text prompt. 
% The CLIP score, a widely utilized evaluation metric in text-to-image generation tasks~\cite{parmar2023zero,zhang2023sine,wang2023imagen}, is calculated as the cosine similarity between the embeddings of the text and image, both of which are encoded by the CLIP model.
% For 3D assets, we calculate the CLIP score between the projected images of the 3D assets and the prompt text in different views and take the average score as the overall CLIP score. In \cref{tb:perclass}, we show quantitive comparison of the similarity between generated 3D assets and the same text prompt with the Latent-NeRF, SJC and our method across diverse scenes. As shown as \cref{tb:perclass}, we achieve the best performance in all scenes, with more significant performance improvement in complex scenes. For example, in \emph{table wine} scene, our method achieves a \textbf{54\%} gain. \textbf{The excellent performance in complex scenes highlights the effectiveness of our global calibration.}

% \noindent\textbf{Local and Global Text Guidance.}
% We conduct ablations to demonstrate the importance of introducing global and local level guidance discussed in Sec.~\ref{sec:optimization}. 
% % We show ablation with different loss settings. 
% In Fig.~\ref{fig:ab_loss}, we observe that our complete method (Ours) improves the generation accuracy of object identity and better geometry.
% Note that, without local-level SDS losses, the object frame of 'banana' in Fig.~\ref{fig:ab_loss} (a) even can not be rendered with any details, while without global-level SDS loss, the local frame of 'banana' in Fig.~\ref{fig:ab_loss} (b) can not generate the 'banana' at accurate number.
% The phenomenon is consistent with our observation that the generative ability of the pre-trained diffusion model may fail to provide accurate guidance for the multi-object text prompt.
% In Fig.~\ref{fig:ab_loss} (c), we also show the perturb and average scoring strategy~\cite{wang2022score} can generate better geometry results against the vanilla SDS losses~\cite{poole2022dreamfusion}.

% \noindent\textbf{Global Calibration.}
% We further study the global calibration by rendering a scene with glass balls made of different materials in Fig.~\ref{fig:ab_loss}.
% The rendered results show that with the global calibration procedure, the light reflection and shadow of all balls have more view consistency.
% While without global calibration, the blue ball renders shadow artifacts that are against scene coherence.

% \subsection{Quantitative evaluation of Compositional Cases}

% Missing Rate

% Failure Cases
% In Fig.~\ref{fig:sd}, we show that for some complex prompt, the Stable Diffusion can not generate accurate information and thus fail to generate faithful images. 

