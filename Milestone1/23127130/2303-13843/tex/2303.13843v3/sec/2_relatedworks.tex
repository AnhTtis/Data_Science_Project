\section{Related Works}
\label{sec:related_works}
\noindent\textbf{Text-guided 3D Generative Models.}
% The field of 3D model generation has sparked numerous research that explores various 3D representations, including 3D voxel grids~\cite{gadelha20173d}, point clouds~\cite{achlioptas2018learning}, meshes~\cite{JunGao2022GET3DAG}, implicit representations~\cite{ZhiqinChen2018LearningIF}, and octree representations~\cite{MoritzIbing2022OctreeTA}.
% Inspired by the achievements of vision-language models such as CLIP~\cite{radford2021learning}, there has been significant progress in text-guided 3D generation. Several works~\cite{jain2022zero, wang2022clip, mohammad2022clip, sanghi2022clip, xu2022dream3d} have employed pre-trained vision-language models to align image features extracted from rendered 3D views with text features by minimizing feature similarity. 
% As the pre-trained vision-language model can offer robust alignment between image and text features, 
% incorporating text guidance into the generation process has become a popular approach in recent works~\cite{jain2022zero,wang2022clip,mohammad2022clip,sanghi2022clip,xu2022dream3d}.
% They achieve text-to-3D generation by minimizing the embedding similarity between the 2D rendering results obtained from 3D representations and the input text.
% However, despite that large-scale pre-trained vision-language models can provide strong 2D guidance for 3D representation learning, their 2D rendering results may be less realistic due to limited feature details.
% Though pre-trained large-scale vision-language models can offer strong 2D guidance for 3D representation learning, the quality of their 2D rendering results may be less realistic due to insufficient details in the features.
% The convergence of vision-language models like CLIP~\cite{radford2021learning} with 3D generative technologies has marked a significant stride in text-guided 3D generation.
% % Motivated by the success of the vision-language models,\eg,~CLIP~\cite{radford2021learning}, text-guided 3D generation has also been rapidly progressing.
% Efforts such as~\cite{jain2022zero,wang2022clip,mohammad2022clip,sanghi2022clip,xu2022dream3d} 
% % utilize the pre-trained vision-language model to provide robust alignment between image features extracted from rendered views of 3D representations and text features by minimizing feature similarity.
% leverage these advanced vision-language models to bridge rendered 3D views and text descriptions, enhancing feature alignment through similarity minimization. 
% Despite their strengths, these models often yield renderings with limited detail, hindering the realism of the 3D output.
% Recently, DreamFusion~\cite{poole2022dreamfusion} demonstrates remarkable capability in text-to-3D object generation by incorporating a powerful pre-trained text-to-image diffusion model~\cite{saharia2022photorealistic} through score distillation sampling.
% Similarly, Magic3D~\cite{lin2022magic3d} proposes a two-stage super-resolution method to enhance generation quality with hybrid 3D representation.
% Latent-NeRF~\cite{metzer2022latent} demonstrates that updating NeRF in the latent space using score distillation sampling can also produce realistic results. 
% SJC~\cite{wang2022score} improves the sampling process by introducing a perturb-and-average scoring scheme to address distribution mismatching issues.
% DreamBooth3D~\cite{raj2023dreambooth3d} proposes a three-stage method that utilizes DreamBooth~\cite{ruiz2023dreambooth} and DreamFusion to generate 3D object from the text and 3-6 images inputs.
% Points-to-3D~\cite{yu2023points} utilizes the sparse 3D point cloud as the immediate representation to guide the 3D generative NeRF model.
% The convergence of vision-language models such as CLIP with 3D generative methods has marked a significant leap in text-driven 3D content creation. Initial models, exemplified by the works of Jain et al. and Wang et al., have adeptly demonstrated the alignment of 3D renderings with textual descriptions, yet they frequently encounter limitations in rendering detailed features, which is crucial for achieving high realism. To advance beyond these constraints, pioneering techniques such as DreamFusion, Magic3D, and Latent-NeRF have integrated text-to-image diffusion models and harnessed score distillation sampling in the latent space to enhance detail and fidelity. Further innovations by SJC and DreamBooth3D have addressed distribution mismatches and facilitated image-based 3D generation, while Points-to-3D has introduced 3D point clouds as a guiding mechanism.
The integration of vision-language models like CLIP with 3D generative methods has propelled text-guided 3D generation forward. Models that harness these advancements, such as those by Jain et al.\cite{jain2022zero} and Wang et al.\cite{wang2022clip}, have excelled in aligning 3D renderings with text descriptors but often fall short in detail, limiting realism. Innovative approaches like DreamFusion~\cite{poole2022dreamfusion}, Magic3D~\cite{lin2022magic3d}, and Latent-NeRF~\cite{metzer2022latent} have sought to enhance this through text-to-image diffusion models and score distillation sampling in the latent space, with SJC~\cite{wang2022score} and DreamBooth3D~\cite{raj2023dreambooth3d} further refining the process to address distribution mismatches and enable image-based 3D generation, respectively. Points-to-3D~\cite{yu2023points} takes a novel route by utilizing 3D point clouds for guidance, whereas Fantasia3D~\cite{Chen_2023_ICCV} innovatively disentangles geometry and appearance tasks, it employs the Stable Diffusion model for learning geometry and utilizes the Physically-Based Rendering (PBR) material model~\cite{McAuley_Hill_Hoffman_Gotanda_Smits_Burley_Martinez_2012} for appearance learning. 
% , treating these two critical aspects of 3D modeling.
%
%
Departing from these singular approaches, our CompoNeRF introduces a paradigm shift in the creation of intricate, multi-object 3D scenes. It adopts an \textit{object-compositional} strategy, utilizing a novel editable 3D scene layout that conceptualizes the scene not as a singular, homogenous entity but as a constellation of discrete NeRFs. Each NeRF is meticulously associated with its spatial 3D bounding box and a corresponding descriptive text prompt, allowing for a dual-text guided compositional method together with the global text. This dual-text framework ensures that each object is not only individually delineated but also coherently integrated into the composite scene, thereby substantially elevating the authenticity of the generated 3D scenes.
%
To ensure an equitable comparison, \textit{we employ the same Instant-NGP backbone, Stable Diffusion checkpoint, and text prompts with Latent-NeRF and SJC}.
% Note that we omit Fantasia3D, as it relies on a hybrid scene representation DMTET~\cite{Shen_Gao_Yin_Liu_Fidler_2021}, w
% Distinct from these approaches, our CompoNeRF innovates in the generation of complex, multi-object 3D scenes by employing an \textit{object-compositional} method. It leverages a unique editable 3D scene layout, which represents objects not as a monolithic entity but as individual local NeRFs, each grounded in both a spatial 3D box and a descriptive text. 
% By contrast, our CompoNeRF generates multi-object 3D scenes in an \textit{object-compositional} way by utilizing multiple local NeRFs to model the scene instead of a holistic object-centric representation.
% Our editable 3D scene layout allows for editing of scenes by changing the text prompt similarly as ~\cite{poole2022dreamfusion, lin2022magic3d}. Also, it enables manipulation of the spatial arrangement of individual objects in a crowded scene, as summarized in Tab.~\ref{tab:compare}.
% Moreover, the layout allows for recomposition with other off-the-shelf representations, enabling the rapid generation of new scenes.

\noindent\textbf{Neural Rendering for 3D Modeling.}
% The advancement of NeRF has greatly improved the performance of neural renderers.
% NeRF models~\cite{nerf,lindell2021autoint,muller2022instant,Liu2020NeuralSV,mip-nerf,ref-nerf,KaiZhang2020NeRFAA} are a family of volume rendering algorithms that utilize coordinate-based MLPs to directly predict color and opacity from the 3D position and 2D viewing direction. 
% The photo-realistic synthesized views from these models have led to the widespread adoption of differential volume rendering in various applications, \eg, relighting~\cite{srinivasan2021nerv,zhang2021nerfactor}, dynamic scene reconstruction~\cite{gao2021dynamic,pumarola2021d,xian2021space,tretschk2021non}, editable scenes and avatars~\cite{liu2021neural,yang2021learning}, and surface reconstruction~\cite{azinovic2022neural,wang2021neus}.
% These methods commonly use \textit{single} MLP to encode the entire scene, which can be ambiguous \wrt specific object identities within the scene.
% Note that our method involves rendering the scene using multiple local NeRFs, and the rendering process requires compositing all the local NeRF predictions based on their spatial relationships.
The evolution of NeRF has significantly elevated the capabilities of neural rendering. NeRF-based models~\cite{nerf,lindell2021autoint,muller2022instant,Liu2020NeuralSV,mip-nerf,ref-nerf, Bai2023DynamicPF} have redefined volume rendering~\cite{kajiya1984ray} through the use of coordinate-based MLPs that infer color and density from spatial and directional inputs. Their capacity to produce photo-realistic views has cemented differential volume rendering as a key component in a variety of applications, such as scene relighting~\cite{srinivasan2021nerv,zhang2021nerfactor}, dynamic scene reconstruction~\cite{gao2021dynamic,pumarola2021d,xian2021space,tretschk2021non}, and scene and avatar editing~\cite{liu2021neural,yang2021learning}, as well as surface reconstruction~\cite{azinovic2022neural, jiang2023sdf,wang2021neus}.
%
Typically, these approaches rely on a \textit{single} MLP to encode an entire scene, which may introduce ambiguity in differentiating between objects. Our method, in contrast, renders scenes through a composite of multiple NeRFs, each responsible for a distinct part of the scene. This compositional rendering takes into account the interrelations of NeRFs, enabling distinction of individual objects and their global consistency.

% The resulting synthesized views exhibit photo-realistic quality, which has led to the widespread adoption of differential volume rendering in many applications, such as object and scene relighting~\cite{srinivasan2021nerv,zhang2021nerfactor}, 
% % unbounded scenes~\cite{KaiZhang2020NeRFAA,barron2022mip}, 
% dynamic scene reconstruction from videos~\cite{gao2021dynamic,pumarola2021d,xian2021space,tretschk2021non}, editable scenes, avatars~\cite{liu2021neural,yang2021learning}, and object surface reconstruction~\cite{azinovic2022neural,wang2021neus}.
% Differently, our framework improves the compositional capability of scene representation by employing multiple implicit representations to model individual objects separately. 
% In this work, we use Instant-npg~\cite{muller2022instant} as basic NeRF representation due to its fast training and inference speed.
% Given some object representations gallery, OSFs~\cite{guo2020object} introduces a volumetric path tracing procedure to enable rendering scenes with moving objects and lights without retraining.
% Several works~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe,ost2021neural,xu2022discoscene,song2022towards} attempt to perform compositional scene modeling by directly decomposing object representations from the scene image. These methods can be broadly categorized as either semantic-based~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe} or 3D layout-based~\cite{ost2021neural,xu2022discoscene,song2022towards}, depending on the type of additional object-level information utilized. 
% semantic based and 3D layout based.
% However, obtaining well-trained object representation is impractical for the real-world application as in most cases only the whole scene images are collected. 
% Therefore, several works focus on decomposing the object representations from the scene image directly using various object-level information, which can be roughly categorized as semantic-based~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe} and 3D layout based~\cite{ost2021neural,xu2022discoscene,song2022towards}.
% For example, instance masks can guide the object decomposition process for editable scene rendering~\cite{yang2021learning} and surface reconstruction~\cite{wu2022object}.
% For example, SemanticNeRF~\cite{zhi2021place} combines an additional semantic head into NeRF to predict the semantic label for input 3D position.
% Similarly, instance masks also can be used to guide the object decomposition process for editable scene rendering \cite{yang2021learning} and surface reconstruction~\cite{wu2022object}.
% LaTeRF~\cite{mirzaei2022laterf} use feature from pre-trained model to evaluate the objectness at each 3D points and in-paint the occluded parts of the object.
% GIRAFFE~\cite{niemeyer2021giraffe} extends the generative ability of scene composition by introducing conditions latent codes for each object representation and discriminator for adversarial scene learning.
% Similarly, DisCoScene~\cite{xu2022discoscene} adopts 3D boxes to spatially disentangle the entire scene into object-centric generative radiance fields.
% and learn the scene representation on 2D images with global-local discrimination.
\noindent\textbf{Object-Compositional Scene Modeling.}
The creation of new scenes from individual, object-centric components represents a significant trend in scene generation, as evidenced by existing research~\cite{zhi2021place, yang2021learning, wu2022object, mirzaei2022laterf, xu2022discoscene, song2022towards}. These efforts typically adopt one of two approaches: semantic-based or 3D layout-based.
%
Semantic-based methods enhance object representations by incorporating additional semantic information, such as segmentation labels~\cite{zhi2021place}, instance masks~\cite{yang2021learning, wu2022object}, or features extracted using pre-trained vision-language models~\cite{mirzaei2022laterf}. On the other hand, 3D layout-based approaches, exemplified by NSG~\cite{ost2021neural} and its successors~\cite{song2022towards}, focus on spatial coordinates, using explicit 3D object placement data to guide object and scene composition.
%
Diverging from conventional techniques, our method innovates by utilizing decomposed, object-specific 3D layouts. This approach enables precise control over scene dynamics, encompassing both object-specific text prompt modifications~\cite{poole2022dreamfusion, lin2022magic3d} and spatial manipulation.
% , akin to DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d}.
%
CompoNeRF's distinctive feature lies in its capability to recompose scenes by interfacing with decomposed NeRFs, thereby accelerating the creation of new scenes, as highlighted in Table~\ref{tab:compare}. In contrast to the mesh-based method in Fantasia3D, which requires considerable human effort in mesh modification and graphics engine support for editing, CompoNeRF offers a more streamlined process. Our composition module seamlessly integrates components, requiring only minimal adjustments in layout or text prompts, followed by fine-tuning of existing offline models to align with the global environmental context during training.
% The idea of generating new scenes by compositing multiple object-centric representations is a straightforward approach in scene generation~\cite{guo2020object}.
% Several works~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe,ost2021neural,xu2022discoscene,song2022towards} attempt to directly decompose object representations from the scene image to perform compositional scene modeling. Based on the additional object-level information, they can be roughly categorized as semantic based~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe} and 3D layout based~\cite{ost2021neural,xu2022discoscene,song2022towards}.
% %
% Semantic based methods incorporate extra semantic information, such as segmentation labels~\cite{zhi2021place}, object instance mask~\cite{yang2021learning,wu2022object}, and features from the pre-trained vision-language model~\cite{mirzaei2022laterf}, to learn the object representations.
% %
% On the other hand, 3D layout-based methods directly use the 3D object coordinate information as the object guidance for learning object-specific representation and generating the full scene by compositing all object representations.
% For instance, NSG~\cite{ost2021neural} and its variant~\cite{song2022towards} use a scene graph structure to model dynamic scenes, associating each object with a 3D box and a node representation.
% %
% While previous works focused on decomposing object entities from real-world images, our work serves as the \textit{\textbf{first}} attempt to tackle the text-to-3D generation via decomposed representations with 3D scene layout.
% The assembly of new scenes from individual object-centric components is a well-established practice in scene generation~\cite{guo2020object}. Efforts in this space~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe,ost2021neural,xu2022discoscene,song2022towards} have made strides in decomposing scenes into object-based representations for more dynamic modeling. These techniques generally fall into two categories: semantic-based and 3D layout-based approaches.
% %
% Semantic-based methods enrich object representations with additional semantic information, utilizing tools like segmentation labels~\cite{zhi2021place}, instance masks~\cite{yang2021learning,wu2022object}, or leveraging pre-trained vision-language models for feature extraction~\cite{mirzaei2022laterf}.
% %
% Conversely, 3D layout-based methods prioritize spatial coordinates, harnessing explicit 3D object placement data to guide the composition of each objectâ€™s representation and the scene as a whole. Notable implementations include NSG~\cite{ost2021neural} and its developments~\cite{song2022towards}, which use scene graphs to contextualize objects within dynamic environments.
% %
% Our work diverges significantly from traditional image decomposition methods, pioneering the use of decomposed, object-specific 3D layouts for text-to-3D scene generation. This innovative approach carves a new path in the field, offering a flexible and editable layout that allows for fine-grained control over the dynamics of a scene. This control extends to object-specific text prompt modifications, similar to the methods employed in DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d}, as well as to the spatial manipulation within complex scenes.
% %
% A key feature that sets CompoNeRF apart is its ability to recompose scenes by interfacing with a variety of pre-existing models. This functionality significantly expedites the creation of novel scenes, an aspect that is highlighted in Table~\ref{tab:compare}.
% %
% It is noteworthy that while the mesh recomposition technique utilized in Fantasia3D necessitates extensive human input in crafting meshes and relies on graphics engines for scene editing and physical simulation, CompoNeRF streamlines this process. Our tailored composition module integrates these elements seamlessly, requiring only adjusting layout or text prompts, then fine-tuning on existing offline models to ensure their compatibility with the global environment during training.
% Distinct from the decomposition of images into objects, our work pioneers the approach of using decomposed, object-specific 3D layouts to facilitate text-to-3D scene generation, marking a novel direction in this field.
% This editable layout affords unprecedented control over scene dynamics, allowing for object-specific text prompt modifications, similar to ~\cite{poole2022dreamfusion, lin2022magic3d}, and spatial manipulation within intricate scenes. 
% % Departing
% Further distinguishing our work, CompoNeRF enables the recomposition of scenes by interfacing with various off-the-shelf representations, thus accelerating the novel scenes creation, a capability we underscore in Tab.~\ref{tab:compare}. 
% %
% Note that mesh recomposition proposed by Fantasia3D  requires tedious human labors on crafting meshes, and further support from graphics engines for scene editing and physical stimulation.  On the contrary, our tailored composition module integrate them with only finetuing on existing offline models and harmonize them with global environment during traininging.  