\clearpage
\setcounter{page}{1}
\maketitlesupplementary


% \section{Rationale}
% \label{sec:rationale}
% % 
% Having the supplementary compiled together with the main paper means that:
% % 
% \begin{itemize}
% \item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
% \item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
% \item When submitted to arXiv, the supplementary will already included at the end of the paper.
% \end{itemize}
% % 
% To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.
\section{Supplementary Material}
\label{sec:sup}
We provide more details of the proposed method and experimental results in the
supplementary material.
Sec.1 and Sec.2 provide the algorithm and more implementation details.
Sec.3 provides more insights into our CompoNeRF model.
Sec.4 adds more details of visualization results. 
Sec.5 lists our attached material details for both scene reconstruction and editing.
% In summary, the whole algorithm to train the proposed
% CompoNeRF is shown in Algorithm ~\ref{alg:cap}.
% \begin{algorithm}[tb]
% \caption{Example algorithm}
% \label{alg:algorithm}
% \textbf{Input}: Your algorithm's input\\
% \textbf{Parameter}: Optional list of parameters\\
% \textbf{Output}: Your algorithm's output
% \begin{algorithmic}[1] %[1] enables line numbers
% \STATE Let $t=0$.
% \WHILE{condition}
% \STATE Do some action.
% \IF {conditional}
% \STATE Perform task A.
% \ELSE
% \STATE Perform task B.
% \ENDIF
% \ENDWHILE
% \STATE \textbf{return} solution
% \end{algorithmic}
% \end{algorithm}
\section{1. Algorithm}
\label{sec:algo}
The detailed algorithm of training our proposed
CompoNeRF is shown in Algorithm ~\ref{alg:cap}.


\begin{algorithm*}[tb]
\caption{Training for CompoNeRF}\label{alg:cap}
\textbf{Input}: a pre-trained text-to-image diffusion model $\phi$, multi-object text prompt $T$ and a set of boxes for 3D scene layout.
\textbf{Output}: learned parameters of local NeRFs $\{\theta_{l,i}\}_{i=1}^{m}$ and Global MLP $\theta_g$.
\begin{algorithmic}[1]
\FOR{\text{\textbf{Iter} $=0 <$ \text{\textbf{MaxIter}}} }
\STATE  Sample $H\times W$ rays from the random camera position and add the directional prompt into $T$.
\FOR{$i=0~\text{to}~H\times W$}

\STATE  Calculate the ray-box intersection for ray $r_i$ to get $m_i$ hits.

\FOR{$j=0 $ to $m_i$}{
    \STATE  Sample $N$ points with normalized location in the $j$ hit  local frame.
    \STATE  Calculate color $\boldsymbol{C_l}$ and density $\boldsymbol{\sigma}_l$ for each point from $\theta_{l,j}$.
    \STATE  Calculate the volumetric rendering color $\hat{\boldsymbol{C_{l,i,j}}}$ of the local Frame for ray $r_i$.}
\ENDFOR
\ENDFOR
\STATE  Map all points into global locations and sort them according to the depth.
\STATE  Calculate the calibrated color $\boldsymbol{C}_{g,i}, \boldsymbol{\sigma}_{g,i}$ via Eq.~4 and Eq.~5 for each point.
\STATE  Calculate the global volumetric rendering color $\hat{\boldsymbol{C_{g,i}}}$ for each ray $r_i$.
\ENDFOR
\STATE  Generate the local view from $\{\hat{\boldsymbol{C_{l,i,j}}}\}_{i=1}^{H\times W}$ and the global view from $\{\hat{\boldsymbol{C_{g,i}}}\}_{i=1}^{H\times W}$
\STATE  Perform score distillation sampling on the local render view and the global render view.
\STATE  Update network parameters via an Adam optimizer.
\end{algorithmic} \textbf{Eng}: Decompose local NeRFs and cache them into offline dataset.
\end{algorithm*}
\section{2. Implementation Details}
\label{sec:imple}
For score distillation sampling, we use the v1-4 checkpoint of Stable Diffusion based on the latent diffusion model~\cite{rombach2022high}. 
We utilize the code-base~\cite{metzer2022latent} for 3D representation and grid encoder from Instant-NGP~\cite{muller2022instant} as our NeRF model. The global MLP consists 4 or 6 Linear layers with 64 hidden channels.
In the training loss, we set $\alpha_g=100, \alpha_l=100$, and $\beta=5e^{-4}$ if without specification.
Our 3D scenes are optimized with a batch size of 1 using the Adam~\cite{kingma2014adam} optimizer on a single RTX3090.
%
Our global frame is centered at the world origin and has a normalized side length of [-1,1]. To generate camera positions, we uniformly sample points on a hemisphere that covers the global frame with a random radius between 1.0 and 1.5.
The camera distance can also be scaled in the way discussed in the main paper. Plus,  
cameras are oriented to look toward the objects. During optimization, the camera field of view is randomly sampled between 40 and 70 degrees. At test time, the field of view is fixed at 60 degrees.
We use the Adam optimizer and perform gradient descent at a learning rate of 0.001 for 5,000 steps in simple prompts, such as ``apple and banana``, and 8,000 steps in more complex prompts for better quality. 
We follow the implementation of SJC~\cite{wang2022score} to perform the averaging implicitly, relying on the optimizer's momentum state when applying the perturb-and-average scoring strategy during training.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/sup_module.pdf}
    % \vspace{-18pt}
    \caption{Ablation study on module designs with the scene bedroom. (a) without global calibration. (b) without global text loss. (c) color-based design. (d) our density-based design.}
    \label{fig:sup_module}
    % \vspace{-12pt}
\end{figure} 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ab_calibration.pdf}
    % \vspace{-18pt}
    \caption{Multi view results with the text prompts on \cref{fig:sup_module}. Subtexts for individual NeRFs are highlighted in bold. }
    \label{fig:ab_view}
    % \vspace{-12pt}
\end{figure} 
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/dis_pose.pdf}
    % \vspace{-2pt}
    \caption{Results of using different training guidance resolutions by scaling the global frames.
    The first row is the rendering results, and the second row visualizes the rays that hit local frames.}
    \label{fig:dis_pose}
\end{figure}


\section{3. Discussion}

% Therefore, we needs to trade off the computation efficiency and rendered results quality in the 3D scene layout generation with CompoNeRF.

\noindent \textbf{More ablations on the design of composition module.}
In \cref{fig:sup_module}, we present further results from the ablation study of our composition module. As outlined in our main manuscript, our preference for a density-based approach is due to its effective and precise calibration of global density.
%
For example, the 'bedroom' scene builds upon the discussion from Fig.2(b.2) in the main paper. The complementary study in \cref{fig:sup_module}(a) demonstrates that direct global text supervision without compositional integration leads to a loss of material context, washing out the 'bed' and 'nightstand' in white. Conversely, \cref{fig:sup_module}(b) illustrates that omitting global text and relying solely on subtext supervision retains the familiar context of a 'white sheet' bed and a polished tan 'nightstand'. However, this approach introduces geometric inconsistencies, such as an overly tall nightstand and a lamp lacking a base, along with an absence of light reflection in the surrounding space.
%
The application of our composition module, depicted in \cref{fig:sup_module}(c) and 8(d), reveals that the density-based design affords enhanced control over density and, consequently, finer geometry. Instead of an empty space above the nightstand, the design aims to adjust the nightstand's height to achieve scene harmony, although it still exhibits limitations in controlling density, leading to subdued floating radiance.
%
\cref{fig:ab_view} provides a comprehensive visual comparison within the 'bedroom' context. When global calibration is absent, as seen in \cref{fig:ab_view}(a), the scene is plagued by sparse holes and a loss of color and texture detail. Neglecting the global branch entirely, as shown in \cref{fig:ab_view}(b), results in a lack of global consistency, evident in the disproportionate size of the 'nightstand' relative to the scene. Finally, the color-based solution in \cref{fig:ab_view}(c) fails to effectively correct the geometry, introducing additional artifacts. In contrast, the full model in \cref{fig:ab_view}(d) exhibits a marked improvement in these aspects.
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/sup_chess.pdf}
    \caption{
    % 1)The local NeRF also learns global context. 2) The global composition module learns stronger global semantic context. 3) The inconsistent object distribution may lead to ignoring objects in later training.
    The ablation study on the scene recomposition, showcasing the finetuning process across different iteration steps and the resulting rendering views. Individual components are labeled in \textcolor{blue}{blue} for clarity, while \textcolor{red}{red} boxes emphasize areas of contrast as discussed in the accompanying text.
    }
    \label{fig:obs}
\end{figure*}

\noindent \textbf{Optimizing Diffusion Model Guidance for Scene Resolution.}
A critical aspect of implementing our framework is the need to strike a balance between the overall scene's resolution and the rendering details. For example, when a single object is placed within a vast scene, its rendered representation may be significantly reduced, occupying only a few pixels. This small pixel footprint can limit the amount of gradient information received during backpropagation.
%
\cref{fig:dis_pose} illustrates this scenario using the same text prompt but with varying scales of the global frames, ranging from 0.3 to 0.7. The results underscore a key insight: the more pixel rays an object interacts with, the better the quality of its rendering. This finding is particularly relevant for large-scale scene rendering, where multiple local frames coexist within the same space. A small object in such a setup may receive minimal ray-box interactions, potentially leading to training inefficiencies or collapse.
%
It's also important to consider that our scene models are optimized in the latent space of the Stable Diffusion model, which has a feature resolution of 
$64\times 64$. However, we decode these latent color features into RGB images with a resolution of 
$128\times 128$. This discrepancy affects the density of rays throughout the space and, by extension, the number of objects that can be effectively rendered.
%
Therefore, reconstructing large-scale scenes using our current model settings remains a challenging issue. As demonstrated in our papers, even small-scale scenes require approximately 5 hours of processing to learn from scratch, indicating potential limitations for larger-scale applications.
% \noindent \textbf{Resolution of Diffusion Model Guidance.}
% One suggestion for applying our framework design is that the resolution of the whole scene and rendering details need to be balanced well.
% For instance, a single object is placed in a large scene. Its rendered view becomes very small, resulting in few pixels with response to receive gradients during backpropagation.
% Fig.~\ref{fig:dis_pose} shows the results using the same text prompt varying scales of the global frames vary from 0.3 to 0.7.
% It reveals that the more pixels rays interact with, the better the rendering quality. 
% This observation is important for large scene rendering, as there are multiple local frames arranged in the same space with the same ray density. Therefore, it may result in a few ray-box interactions received by a small object, leading to the training collapse. 
% Please also note that we optimize the scene model in the latent space of the Stable Diffusion model, in which the feature resolution is $64\times 64$, 
% while we decode the latent color feature into RGB images with resolution $128\times 128$, which limits the ray density of the whole space and the subsequent object numbers. 
% Therefore, it's still an tricky issue for the large scale scene reconstruction using the current model settings, since the small scenes present in our papers need around 5 hours already. It may be restrictive for the larger scale usage. 
 % $64\times 64$

\noindent \textbf{Influence of Global MLP Size on Composition Capabilities.}
The complexity of a scene directly influences the required configuration of the parameters \(\boldsymbol{\theta}_g\) within the composition module. In Figure~\ref{fig:ab_mlp_size}, we experiment with varying the number of layers in the MLPs responsible for both densities \(f_{\boldsymbol{\theta}_{g_d}}\) and color calibration \(f_{\boldsymbol{\theta}_{g_c}}\). The results indicate that an insufficiently representative MLP can fail to preserve the distinct identities of individual NeRFs. For instance, the 'white king chess' piece struggles to manifest its characteristic whiteness, leading the global calibrators to compensate inadequately by projecting a flattened representation onto the chessboard surface.
%
By increasing the number of MLP layers, we observe a notable improvement in the accurate portrayal of each object's identity and overall scene quality. For example, the contextual details of the chessboard, like its grid pattern, are rendered more clearly and naturally.
%
Based on these findings, we recommend fine-tuning this hyperparameter to align the composition module's capabilities with the specific complexity of the scene.

% \noindent \textbf{The size of global MLP decides the composition capability.}
% The parameters $\boldsymbol{\theta}_g$ in the composition module need to be adapted to the scene complexity. As shown in \cref{fig:ab_mlp_size}, we vary the number of MLP layers for both density $f_{\boldsymbol{\theta}_{g_d}}$ and color calibrators $f_{\boldsymbol{\theta}_{g_c}}$. 
% The result reveals that inadequate representation may lead to the missing identity for individual NeRFs, such as the 'white king chess' has a limited ability to depict itself as white, while the global calibrators struggle to make a compliment by adding a flattened 'white king chess' on the chess board surface. 
% %
% After increasing the MLP layers, the situation is getting better with an accurate representation of identity and enhanced quality for each NeRF, \eg, the chessboard context of the grid becomes more evident naturally. 
% %
% We recommend finetuning this hyperparamter to adjust the composition ability depending on the scene complexity. 
 
% \noindent \textbf{Don't rely too much on global composition.}
% As shown in \cref{fig:obs}, composition may introduce artifacts if the gap between target semantics and the source is large. This can judged by the comparison of visual rendering results. For instance, the target \emph{juice} has large color and geometric differences with the original \emph{wine}. On the contrast, replacing the original \emph{salad} by an \emph{apple} is much easier, since there is no careful calibration process needed. Thus,
% it will be helpful to change a more similar decomposed NeRF model, or train another from scratch to tackle this issue. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/sup_mlp_size.pdf}
    % \vspace{-18pt}
    \caption{Ablation study on the learning capability of MLP. The selected scene components are accentuated by colorful boxes. }
    \label{fig:ab_mlp_size}
    % \vspace{-12pt}
\end{figure} 

\noindent \textbf{Global Context Assimilation by Local NeRFs and Composition Module.} As illustrated in Fig.1 of the main paper, our composition module integrates global context, accommodating comprehensive semantic details such as reflections on surfaces. Despite the primary embedding of context within the composition module, local NeRFs exhibit an ability to partially learn these global attributes. For instance, Figure~\ref{fig:obs} shows that initially, the local NeRF optimization does not occur in isolation; the table, for example, bears a residual shadow from the 'french bread' in the original configuration as depicted at the upper left corner.
%
As the training progresses, these anomalies are resolved, and the local NeRFs gradually assimilate aspects of the global texture, albeit to a limited extent. For instance, while the black and white pattern of the chessboard is predominantly captured by the global composition module, the local representation of the table, highlighted in red box in their upper left corners, remains unchanged. However, as iterations advance,
%
despite there's no NeRF that is responsible for 'chess board', 
%
the global frame begins to discern it as the necessary environment and replicate the underlying 'chessboard' pattern. 
%
This reveals that NeRFs can initially embed global environmental context, while composition module can possibly merge some necessary local patterns for consistency. 

% \noindent \textbf{Both local NeRFs and the composition module learn the global context.} As we have shown in Fig. 1 of the main paper, the composition module will introduce global connections to fit the full semantic context, such as the reflection on the bed. However, we also discover that the local NeRF has learned part of it, despite most of its context being embedded in the composition module. For instance, in \cref{fig:obs}, we observe that the initial local NeRF is not independently optimized, \eg, 
% %
% the table has some visible shadow of \emph{french bread} from the original scene. 
% After some time, this inconsistent feature will be eliminated. 
% Moreover, at later training iterations, this local representation continues to learn some global texture, while it remains trivial. For example, the majority of the black and white pattern on the table is still attributed to the global composition module. Specifically, the table in the local frame magnified in red stays the same, while the global frame starts to learn the hidden \emph{chess board}. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/dis_shape.pdf}
    % \vspace{-25pt}
    \caption{\textbf{(Left)} we observe the multi-face problem, \ie, duplicated face views with geometry  collapse in all methods, even in single-object cases. \textbf{(Right)} we provide mesh as guidance instead of box layouts to solve this problem, which further proves our method's versatility and effectiveness.}
    \label{fig:dis_shape}
\end{figure}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/sup_float.pdf}
    % \vspace{-25pt}
    \caption{
Ablation study examining layout editing and the challenge of floating objects. The upper row shows the layout editing and the lower row indicates rendering views.\textbf{(Left)} Renderings exhibit floating objects due to a suboptimal layout. \textbf{(Right)} Improved outcomes following layout refinements.}
    \label{fig:layout}
\end{figure}

\noindent \textbf{Early Termination in the Scene Composition.} Our method's joint optimization aims to enhance the congruence between rendered views and multi-level text prompts. In later stages of training, the focus predominantly shifts to reinforcing global consistency as local fidelity typically reaches satisfactory levels by this point.
%
The observations from the experiment depicted in \cref{fig:obs} suggest a potential degradation in specific elements, such as the \emph{wine}, which worsens as training progresses, as evidenced by the local frames' comparison in the upper left corner. Concurrently, the global rendering depicts the \emph{wine} as nearly imperceptible. This deterioration hints at the possibility that continued optimization may inadvertently diminish the representation of certain objects.
%
Given these findings, we advocate for an early termination strategy when training models on less conventional scene compositions. This approach aims to maintain an equilibrium between rendering fidelity and semantic coherence, preventing the over-attenuation of less common scene elements.
% \noindent \textbf{Composition of unusual scene needs the early stop. } Since our joint optimization will strive to enhance the similarity between the rendering views and multi-level text prompts. The later training may mainly focus on improving the global level consistency, since the local ones are sufficiently well. 
% %
% The experiment shown in \cref{fig:obs} reveals that \emph{wine} is deteriorated with the training continues as indicated by the comparison of local frames on the upper left corner. On the other hand, the global rendering result shows \emph{wine} is nearly invisible. It may brings inaccurate predictions considering that later optimiztion would keep mute the objects. 
% %
% Therefore, we suggest for the early stop when training for less common scene composition, which strikes a balance between the rendering quality and semantics consistency.



\noindent \textbf{Addressing Multi-face Issue with Enhanced Prompts.}
Much like Latent-NeRF and SJC, our CompoNeRF framework encounters the multi-face challenge, where guidance from the Stable Diffusion model may result in conflicting facial features for certain objects, as illustrated in Figure~\ref{fig:dis_shape}. The reason lies in the fact that diffusion model does not always provide reliable guidance that aligns with the desired orientation corresponding to the camera's viewpoint during sampling.
%
To mitigate the multi-face problem, stronger constraints can be introduced to promote geometric consistency within the 3D representation. CompoNeRF incorporates mesh constraints, akin to those utilized in Latent-NeRF, offering a more detailed 3D layout compared to traditional bounding boxes. As demonstrated in Figure~\ref{fig:dis_shape}, the implementation of exact mesh constraints markedly mitigates the multi-face issue, though it may come at the expense of detail and adaptability.
%
Nevertheless, the requirement for accurate mesh input necessitates considerable manual editing, which may reduce the method's range of applications. Despite this, our approach illustrates that the 3D scene layout can be readily adapted to accommodate a broader range of input prompts. Further study is needed to solve the persistent multi-face issue in the text-to-3D tasks.
% \noindent\textbf{Multi-face Problem and Stronger Prompt.}
% % The CompoNeRF also has several limitations related to diffusion guidance.
% Similar to Latent-NeRF and SJC, the guidance generated from Stable Diffusion may produce a multi-face problem for certain objects as shown in Fig.~\ref{fig:dis_shape}.
% The diffusion model can not guarantee to generate satisfactory guidance with the desired direction along with the sampling camera pose.
% % In other words, the constraint provided by our 3D layout still can not force the diffusion model to generate satisfactory guidance for each specific object across different rendering views.
% An alternative to relieve the multi-face problem is adding stronger constraints to force the 3D representation to maintain geometric consistency.
% Our method also uses the mesh constraint, proposed by Latent-NeRF, as a more fine-grained 3D layout than the 3D box.
% Fig.~\ref{fig:dis_shape} shows that the multi-face problem can be largely relieved with the more accurate mesh constraint.
% However, accurate mesh input requires extensive editing, which reduces practical values during application.
% % However, we show that our 3D scene layout can be easily extended to more general types of input prompts.

\noindent\textbf{Layout Optimization to Address Floating Artifacts.}
The process of scene composition begins with strategically positioning NeRFs within the predefined layout.
%
An overlap of object bounding boxes is critical, as highlighted in Fig.2 of the main document, to facilitate the generation of convincing scenes.
%
In our investigations, demonstrated in \cref{fig:layout}, we identified a 'floating' issue when bounding box overlaps are absent. This issue may stem from the regularization behavior within NeRFs, where the radiance fields—specifically, the regions responsive to gradient interactions, symbolized by ellipses centered on the boxes—fail to intersect. Such non-interaction can pose challenges, as it does not provide the necessary contiguous context for the global semantics to incorporate these objects seamlessly.
%
To rectify this, one straightforward approach we recommend is the judicious repositioning of bounding boxes to introduce overlaps. For example, a slight downward adjustment of a box can instigate detectable overlaps during training, facilitating better integration.
%
This insight opens up a potential avenue of research into the interplay between layout configurations and NeRFs, offering the possibility of more nuanced control over scene dynamics without the need for explicit layout modifications.
% \noindent\textbf{Adjusting Layout to Avoid the Floating Issue.}
% The composition process is initiated by allocating NeRFs to the setup layout. 
% %
% We observe that overlap among the object bounding boxes, as indicated in Fig.2 of the main paper, is necessary for a plausible scene generation. 
% %
% For instance, our experiment in \cref{fig:layout} exemplifies the floating issue that exists without leaving the bonding box overlap. The problem may originate from the NeRF regularization, as we empirically observe that the radiance field, in other words, ray hitting that has gradient response, denoted by the eclipses centering at the boxes' centers doesn't interact with each other. This may bring troubles like the global semantics may not accommodate them accordingly as they lack the necessary contact to support their joint adjustment. 
% Facing this issue, is one of the easiest solutions as we suggest moving the boxes to create the overlap, \eg, moving the box downward a bit, to enable their overlap can be detected during the training process. 
% This can also be a new direction for people to study the relationship between layout and NeRF for more adaptable control without explicit layout refinement. 

\begin{figure*}[t]
    \centering
    % \vspace{-6pt}
    \includegraphics[width=\linewidth]{figures/sup_fail.pdf}
    \caption{The failure cases are categorized as follows: \textbf{(Left)} issues encountered during scene reconstruction, and \textbf{(Right)} challenges arising in scene editing. The outputs from Stable Diffusion selected for illustration represent the most frequently occurring types generated by the model.
 }
    \label{fig:fail}
\end{figure*}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/sup_semantic.pdf}
    \caption{
    The failure cases with their color labeled NeRFs components shown beside. The textual guidance manipulation on global/local weights are shown below.  
    }
    \label{fig:sup_semantic}
\end{figure*}
% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/sup_edit.pdf}
%     \caption{
%         xxx
%     }
%     \label{fig:sup_edit}
% \end{figure*}


\noindent \textbf{Analysis of Failure Cases in Scene Composition and Editing.}
%
Our composition module may sometimes fail to produce coherent scenes, often due to limited text description distributions within the training data of diffusion models as illustrated in \cref{fig:fail}. This can be mitigated by adjusting the loss weights governing the global and local guidance, such as \cref{fig:sup_semantic}.
%
In scene composition, the 'computer station' lacks accessories like cables and wires, the 'headphones' are misshapen, and the 'computer screen' lacks a base. Scene recomposition similarly shows the 'astronaut' and 'bed' placed together without sensible global calibration.
%
Moreover, the Stable Diffusion model's depiction of human figures often suffers from geometric distortions, potentially due to the multi-face problem, as shown in \cref{fig:sup_semantic}.
%
These failures are mostly due to uncommon layouts or the rarity of certain objects in text-to-image datasets. Repeated global text prompts on Stable Diffusion and examination of numerous samples have failed to yield images that align with our objectives. This challenge extends beyond guidance collapse, reflecting the scarcity of certain objects in the model's outputs.
%
CompoNeRF's effectiveness is inherently linked to the performance of large-scale text-to-image models, restricting its capabilities to generating primarily conventional scenes with well-defined global features.
%
To address these limitations, we can strategically adjust the weights of global textual guidance \({\alpha_g}\nabla\mathcal{L}_{\text{SDS}_g}\) and local textual guidance \({\alpha_l}\nabla\mathcal{L}_{\text{SDS}_l}\). This adjustment aims to find an equilibrium between the consistency of the overall scene and the accuracy of individual components. For example, increasing \({\alpha_g}\) enhances global consistency, as evidenced in Figure~\ref{fig:sup_semantic}. However, this can inadvertently lead to objects assimilating extraneous global context. In the 'boy riding bike' scenario, a heightened \({\alpha_g}\) may result in the 'bike' being erroneously represented as both a human figure and a bike. Similarly, in the 'breakfast' scene, amplifying global context might result in a more proportionate table, yet it complicates the distinction between the 'croissant' and its individual NeRF representation.

Ultimately, fine-tuning loss weight parameters is a delicate process that can mitigate identity issues, yet it demands careful calibration to maintain a harmony between scene integrity and the authenticity of each component. The limited representation of certain objects in pre-trained models remains a substantial obstacle, underscoring the need for further investigation into the issue of inadequate guidance in complex scene generation.

% CompoNeRF's success is thus constrained by the capabilities of large-scale text-to-image models, limiting us to creating common scenes with accurate global features.
% %
% Mitigating such failures, we can manipulate the weights of global textual guidance \({\alpha_g}\nabla\mathcal{L}_{\text{SDS}_g}\) and local textual guidance \({\alpha_l}\nabla\mathcal{L}_{\text{SDS}_l}\) to adjust the balance between overall scene consistency and individual component accuracy. For instance, increasing \({\alpha_g}\) can improve scene consistency, as seen in Figure~\ref{fig:sup_semantic}, but may also cause objects to incorporate unwanted global context.
% For instance, the 'boy riding bike' scene which shows artifacts in bike may enhance global context by doubling the $\alpha_g$. However, it may also brings other context, like 'bike' takes the role of both human and the bike. It turns out that it might not easy to distinguish between local and global context range. The 'breakfast' scene can have more resaonable output with a larger table that can support the objects on the surface with higher global context, while it also hard to depart 'croissant' into an individual NeRF.

% In conclusion, modulating loss weight parameters can help tackle identity failures, but it requires a delicate balance between scene coherence and the credibility of components. The infrequent distribution of objects in pre-trained models poses a significant challenge, indicating a need for further exploration into 'lack of sufficient guidance.'

 


\section{4. More Visualization Results}
\label{sec:vis}
We provide the multi-view qualitative results from CompoNeRF in Fig.~\ref{fig:vis1}. 
Note that we increase the resolution of the image latent features from $64 \times 64$ to $128\times128$ during inference for better results.
We also have attached video results in the supplemental materials for each case and the baseline Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score}. 
We have named each file after the text prompt used to generate the 3D asset. Please see the attached videos. 
% In addition, we included more visualization results and videos for the scene editing results.
\begin{figure*}[t]
    \centering
    \includegraphics[width=.8\linewidth]{figures/supply_2.pdf}
    % \caption{
    % More qualitative results using multi-object text prompts. 
    % }
    \label{fig:vis1}
\end{figure*}









\begin{figure*}[t]
    \centering
    \includegraphics[width=.98\linewidth]{figures/supply_1.pdf}
    \caption{
    More qualitative results using multi-object text prompts. 
    }
    \label{fig:vis1}
\end{figure*}



\clearpage
\section{Material Details}

Denote recomposition results from the source scenes with xxx.mp4 consisting of Y) 'xxx', where Y is the source scene sign.
The video attached to our supplementary materials consists of the following sections:


\begin{enumerate}
        \item Qualitative Comparison Video: 
        \begin{enumerate}
            \item (tesla.mp4) A tesla model three is running on the road.
            \item (astronaut.mp4) An astronaut is standing on the moon ground.
            \item (apple\_banana.mp4) A red apple and a yellow banana.
            \item (chess.mp4) A white king and a black queen chess piece on a chess board.
            \item (table\_wine.mp4) Glasses of wine, salad and french bread on a wooden table.
            \item (pisa\_eiffel\_opera.mp4) The sydney opera house, the Leaning tower of Pisa, and the Eiffel tower are situated in a triangle.
            \item (sphinx.mp4) The Great Sphinx of Giza is situated near the Great Pyramid in the desert.
            \item (glass\_ball.mp4) Crystal ball, Dichroic glass ball, Murano glass ball, and Solar-powered glass ball are placed on the glass table.
            \item (bed\_room.mp4)A bed is next to a nightstand with a table lamp on it in a room. 
            \item (vase\_flower.mp4) A bunch of sunflowers in a barnacle encrusted clay vase. 
        \end{enumerate}
        \item Scene Editing Video:
        \begin{enumerate}
            \item (table\_chess.mp4)
            consists of d)'white king chess', e)'glass of whine', d)'black queen chess', i)'a table lamp', and e)'a table'. 
            \item (table\_juice.mp4) consists of 'Glasses of juice', c)'apple',  e)'french bread',  e)'wooden table'.
            % \item room\_two\_stand.mp4 consists of duplication of nightstands and a bed.
            % \item room\_sunflower.mp4 consists i) bed and nightstand,  j)  bunch of sunflowers and vase.
            \item (table\_chess\_only.mp4) consists of d)'white king chess', d)'black queen chess', i)'a table lamp', and e)'a table'. 
            \item (table\_balls.mp4) consists of  h) 'Murano glass ball', h) 'Dichroic glass ball', i)'a table lamp', and e)'a table'. 
        \end{enumerate}
        \item Ablation Study Video:
        \begin{enumerate}
            \item (bed\_room\_without\_calibration.mp4) deprived of calibration module for \emph{bed room}.
            \item (bed\_room\_without\_global\_text.mp4) deprived of global branch/global text guidance for \emph{bed room}.
            \item (bed\_room\_color\_based.mp4) color-based calibration module for \emph{bed room}.
            \item (bed\_room\_full.mp4) our density-based calibration module for \emph{bed room}.
            \item (table\_wine\_color\_based.mp4) color-based calibration module for \emph{table wine}.
            \item (table\_wine\_density\_based.mp4) density-based calibration module for \emph{table wine}.
        \end{enumerate}
    \end{enumerate}

% We also extend the training and testing progress of 'table\_wine' with the subfolder 'example'. 
% The training process has different files, including 
% 'step\_{num}\_{local}\_{id}\_image', where 'num' denotes the training step, 'local' or 'global' indicates the local/global frames, and 'id' is the NeRF node index. 

% If the file names end with 'weights\_sum', it denotes the ray hit that receives gradients. 

% Our validation stage adopts a circular camera trajectory and attaches the inferred surface norms following the training process' naming convention above. 
\textbf{Training and Validation Demo}

The demonstration of the training and validation process of the 'table\_wine' scene is incorporated in a specialized subfolder titled \textit{example}.

\textbf{Training Process File Naming}

Files in the training process adhere to a specific naming convention:
\begin{itemize}
    \item \textbf{Format}: \texttt{step\_\{num\}\_\{local/global\}\_\{id\}\_image}
    \begin{itemize}
        \item \texttt{num}: Training step number.
        \item \texttt{local/global}: Indicates local or global frames in the training model.
        \item \texttt{id}: NeRF node index.
    \end{itemize}
\end{itemize}

Files ending with \texttt{weights\_sum} denote the ray hit that receives gradients, integral in image processing or neural rendering.


\textbf{Validation Stage}

The validation stage involves:
\begin{itemize}
    \item A circular camera trajectory for comprehensive model assessment.
    \item Attachment of inferred surface norms, following the training process's naming convention.
\end{itemize}