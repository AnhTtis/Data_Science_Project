%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
% \usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
\usepackage{aaai24} 
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[percent]{overpic}
\usepackage{amsmath}
\usepackage{boldline}

\newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}
\newcommand{\wrt}{\textit{w.r.t. }} 
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\crefname{figure}{Fig.}{Figs.}
\crefname{algorithm}{Alg.}{Algs.}
% \usepackage{lineno}
% \linenumbers

%
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\usepackage{booktabs}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

\usepackage{bbding}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D
Scene Layout}
\author{
    %Authors
    % All authors must be in the same font size and format.
    % Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    % AAAI Style Contributions by Pater Patel Schneider,
    % Sunil Issar,\\
    % J. Scott Penberthy,
    % George Ferguson,
    % Hans Guesgen,
    % Francisco Cruz\equalcontrib,
    % Marc Pujol-Gonzalez\equalcontrib
    Haotian Bai\textsuperscript{\rm 1},
    Yuanhuiyi Lyu\textsuperscript{\rm 1},
    Lutao Jiang\textsuperscript{\rm 1}, 
    Si Jia Li\textsuperscript{\rm 2}, 
    Haonan Lu\textsuperscript{\rm 2}, 
    Xiaodong Lin\textsuperscript{\rm 2},  
    Lin Wang\textsuperscript{\rm 1, \rm 3}$^{\dag}$\\
}

\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}VLIS LAB, AI Thrust, HKUST(GZ) \quad 
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,
    \textsuperscript{\rm 2}OPPO \quad 
    \textsuperscript{\rm 3}Dept. of CSE, HKUST\\
    % Dept. of CSE, HKUST
    % {\small Project homepage: \url{https://vlislab22.github.io/DOT/}}}

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    {\tt\small haotianwhite@outlook.com, yuanhuiyilv@hkust-gz.edu.cn, jianglutao98@gmail.com,\{lisijia, luhaonan, linxiaodong\}@oppo.com, linwang@ust.hk}
    % 1900 Embarcadero Road, Suite 101\\
    % Palo Alto, California 94303-3310 USA\\
    % email address must be in roman text type, not monospace or sans serif
    % proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse

\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it

\author {
    % Authors
    Haotian Bai\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry


\begin{document}
% \maketitle

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\vspace{-1em}
\maketitle
\vspace{-1em}
\begin{center}
\centering
% \vspace{-9pt}
% \vspace{2pt}
\begin{overpic}[width=0.9\linewidth]{figures/teaser.pdf}
\end{overpic}
% \vspace{-13pt}
% \captionof{figure}{\textbf{Left:}
% CompoNeRF generates multi-object 3D scenes based on the editable 3D scene layout that represents each object with a local NeRF associated with its spatial location (\ie, 3D box) and local text prompt.
% Also, the 3D scene layout makes the object-compositional scene generation flexibly editable (\eg, scaling, moving, duplication, or recomposition) by manipulating the 3D layout or text prompt.
% \textbf{Right}: Our approach produces the most faithful and editable scenes given the multi-object text guidance, while the Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} suffer from missing objects and semantic confusion.
% }
\captionof{figure}{\textbf{Left:}
 We introduce CompoNeRF, a method that uses boxes and texts to \textit{compose} NeRFs with global consistency.(\eg, the global reflection highlighted in red). 
 Besides its accuracy, each NeRF model can be \textit{decompose} and edited to \textit{recompose} large scenes effectively. \textbf{Right:} Despite recent efforts to develop text-to-3D generation, represented by works such as Latent-NeRF \cite{metzer2022latent} and SJC~\cite{wang2022score}, they often struggle to understand long sentences in our daily lives and usually fail to produce desired results in terms of number, type, and quality.
 }
\label{fig:teaser}
\end{center}%
}]
\let\thefootnote\relax\footnote{$^{\dag}$ Corresponding author}



% Then, the whole scene view is calibrated via a global MLP by compositing all the object views rendered from local NeRFs in the unified scene space according to the 3D layout.
% Besides, manipulation on 3D layout together with text prompts makes the editing process of object-compositional scene generation more flexible, \eg scaling, moving, duplicating, or re-composing elements within the scene.
% \textbf{Right}: Compared to approaches such as Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score}, our method produces more faithful and editable scenes when provided with multi-object text guidance. In contrast, the other approaches may suffer from missing objects or semantic confusion within the scene.

% Based on the input text, we render the entire scene view alongside each single object view. Moreover, our framework supports editable composition by replacing the objects in the scene with those trained in other scenes after fine-tuning.
%
% \textbf{Right}:  Our approach surpasses Latent-NeRF and SJC in producing visually compelling and semantically meaningful scenes in multi-object composition, where the latter struggle to  represent the correct context and quantity.}



\begin{abstract}
Recent research endeavors have shown that combining neural radiance fields (NeRFs) with pre-trained diffusion models holds great potential for text-to-3D generation.
However, a hurdle is that they often encounter guidance collapse when rendering multi-object scenes with relatively long sentences. 
Specifically, text-to-image diffusion models are inherently unconstrained, making them less competent to accurately associate object semantics with 3D structures.
To address it, we propose a novel framework, dubbed \textbf{CompoNeRF}, to explicitly incorporates an editable 3D scene layout to provide effective guidance at the object (\ie, local) and scene (\ie, global) levels. 
Firstly, we interpret the multi-object text as an editable 3D scene layout containing multiple local NeRFs associated with the object-specific 3D boxes and text prompt.
Then, we introduce a composition module to calibrate the latent features from local NeRFs, which surprisingly improves the view consistency across different local NeRFs. 
Lastly, we apply text guidance on global and local levels through their corresponding views to avoid guidance ambiguity.
Additionally, NeRFs can be decomposed and cached for composing other scenes with fine-tuning.
This way, our CompoNeRF allows for flexible scene editing and recomposition of trained local NeRFs into a new scene by manipulating the 3D layout or text prompt. 
Leveraging the open-source Stable Diffusion model, our CompoNeRF can generate faithful and editable text-to-3D results while opening a potential direction for text-guided multi-object composition via the editable 3D scene layout. Notably, our CompoNeRF can achieve at most \textbf{54\%} performance gain based on the CLIP score metric.
Code is available at \url{https://}.
\end{abstract}

% \begin{figure}[h]
%     \centering
%     % \vspace{-2pt}
%     \includegraphics[width=\linewidth]{figures/teaser.pdf}
%     % \vspace{-8pt}
%     \caption{
%     Despite the recent endeavor to develop text-to-3D generation with representative works such as Latent-NeRF\cite{metzer2022latent} and SJC\cite{wang2022score}. They normally fail to tackle lengthy sentences in our daily life, like failing to generate desired objects of various numbers, types, and lack of fertility. To solve it, we introduce CompoNeRF guided by boxes and texts to compose editable NeRF with a consistent global semantic. Besides its accuracy, we  
%     }
%     \label{fig:teaser}
%     % \vspace{-12pt}
% \end{figure}

\section{Introduction}
Recently, text-to-image generation has achieved tremendous success by coupling the vision-language pre-trained models~\cite{radford2021learning,li2022blip} with diffusion models~\cite{ho2020denoising,nichol2021improved,rombach2022high}.
These breakthroughs have also yielded far-reaching implications in text-to-3D generation ~\cite{jain2022zero,sanghi2022clip,hong2022avatarclip,JunGao2022GET3DAG,mohammad2022clip,lee2022understanding,xu2022dream3d} using powerful vision-language pre-trained models. 
More recently, several text-to-3D methods~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} have shown that matching the rendered views from the differential 3D model, such as Neural Radiance Fields (NeRFs)~\cite{mildenhall2020nerf,mip-nerf,muller2022instant}, with the learned text-to-image distribution from pre-trained diffusion model can produce remarkable results.



% The rapid development of text-image models~\cite{radford2021learning,li2022blip} has yielded far-reaching implications in text-guided image generation~\cite{ramesh2021zero,patashnik2021styleclip,ramesh2022hierarchical,saharia2022photorealistic,rombach2022high}.
% % models~\cite{ramesh2022hierarchical,saharia2022photorealistic,rombach2022high}.
% Notably,  recently text-guided image generation has achieved tremendous success when coupled with diffusion models~\cite{ho2020denoising,nichol2021improved,rombach2022high}.
% Text-guided image generation has recently gained much attention and success in both academia and industry,  
 % achieved tremendous success
% These breakthroughs have also accelerated the development of text-guided 3D generation ~\cite{jain2022zero,sanghi2022clip,hong2022avatarclip,JunGao2022GET3DAG,mohammad2022clip,lee2022understanding,xu2022dream3d} using powerful pre-trained text-image models. 
% More recently, 
% % text-conditional 3D generation methods have
% it has been shown that the diffusion model can serve as a powerful critic to optimize the underlying differential 3D representations~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score}, such as Neural Radiance Fields (NeRFs)~\cite{mildenhall2020nerf} for text-to-3D generation.

However, the textual description is often an abstract specification for a desired target 3D model or a 2D image.
Despite that the powerful diffusion models, \eg, Stable Diffusion~\cite{rombach2022high}, have been trained on billions of text-image pairs~\cite{schuhmann2022laion}, it is still a challenge to generate geometrically coherent images across different viewpoints from the text.
Moreover, the diffusion model may produce inaccurate results~\cite{feng2022training} given text containing multiple objects, resulting in missing objects or semantic confusion.
For example, Fig.~\ref{fig:intro} demonstrates that Stable Diffusion fails to maintain object identities and geometric coherence even with a simple multi-object text.
It obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash \textit{guidance collapse}, especially when rendering multi-object scenes with text prompts.
As a result, state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models can only generate part of concepts in the multi-object text shown in Fig.~\ref{fig:intro}, limiting their application for object-compositional 3D scene generation from text prompts.



% However, the learned 2D generative distribution in pre-trained diffusion model 
% Despite they have , the 2D supervision unconstrained generative capacity of the diffusion model is still considered as the major challenging issue towards comprehensive text-to-3D generation.
% across different viewpoints, given the input text prompt.
% may produce inaccurate results~\cite{feng2022training}, resulting in missing objects or semantic confusion.
% This obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash guidance collapse, especially when rendering complex scenes from multi-object texts.
% For example, Fig.~\ref{fig:intro} demonstrates that the state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models fail to maintain the correctness of object identities and the geometric coherence when presented with the simple multi-object text.
% The reason is that the text-to-image generative ability in the diffusion model remains inherently unconstrained in the 3D space. 

% Despite the impressive text-to-3D results achieved by the diffusion model, the diffusion model may produce inaccurate results~\cite{feng2022training}, resulting in missing objects or semantic confusion.
% This obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash guidance collapse, especially when rendering complex scenes from multi-object texts.
% For example, Fig.~\ref{fig:intro} demonstrates that the state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models fail to maintain the correctness of object identities and the geometric coherence when presented with the simple multi-object text.
% Although these text-to-3D models have shown , the diffusion model may produce inaccurate results~\cite{feng2022training}, resulting in missing objects or semantic confusion.
% This obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash guidance collapse, especially when rendering complex scenes from multi-object texts.
% For example, Fig.~\ref{fig:intro} demonstrates that the state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models fail to maintain the correctness of object identities and the geometric coherence when presented with the simple multi-object text.
% 
% Moreover, when given text that includes multiple objects, the diffusion model may produce inaccurate results~\cite{feng2022training}, resulting in the missing object or semantic confusion, leading to guidance collapse.
% Although text-to-3D models have shown impressive results, a hurdle for them is that they are hindered by the guidance collapse  
% An example in 
% The reason is that the text-to-image generative ability in the diffusion model remains inherently unconstrained in the 3D space. 


% Therefore, it is natural to ask whether the multi-object text guidance between 3D representations and diffusion models can be better constrained for multi-object generation.
% A solution is to make the 3D representation and rendering network object-aware by binding object-related information to particular locations in the 3D space.
% However, existing approaches~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to encode the entire scene into a single neural network, which can successfully handle object-centric scenes but is generally agnostic to the object's identity.

Therefore, it naturally raises the question:  \textit{whether the agnostic distribution of the diffusion model can accurately learn and compose all the concepts in a multi-object text for 3D scene generation.}
According to Fig.~\ref{fig:intro}, we observe that the diffusion model can more accurately generate single objects with their respective local text prompts.
This motivates us to introduce more fine-grained text guidance to tackle the guidance collapse issue in existing frameworks~\cite{metzer2022latent,wang2022score} when using multi-object text prompts.
Thus, a straightforward solution is to bind object-oriented guidance to each object, making the 3D representation and rendering pipeline object-aware.
However, existing approaches~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to use a single neural network to encode the entire scene, making it hard to incorporate the decomposed guidance during training as they are generally agnostic to the object's identity and quantity.

\begin{figure}[t]
    \centering
    % \vspace{-2pt}
    \includegraphics[width=.94\linewidth]{figures/sd.pdf}
    % \vspace{-8pt}
    \caption{
    % Comparison of our CompoNeRF with Latent-NeRF~\cite{mirzaei2022laterf} and SJC~\cite{wang2022score}.
    % It shows that using different level guidance in multi-object text can address the unconstrained generation of Stable Diffusion~\cite{rombach2022high}.
    \textbf{The guidance collapse issue} on generating multi-object scenes by Stable Diffusion.
    we observe that using both global and local text guidance can mitigate this issue.
    % and \textit{guidance collapse} issue on generating a simple multi-object scene, ``a red apple and a yellow banana".
    % unconstrained generation problem associated with Stable Diffusion~\cite{rombach2022high} in multi-object scenarios.
    % The unconstrained generation in Stable Diffusion.
    % The unconstrained generation in Stable Diffusion.
    %  poses a challenge, particularly for sentences that describe multiple objects.
    % % Stable Diffusion struggles to distinguish the context and quantity of rendering targets in 2D, which can mislead Text-to-3D generation methods. 
    % In contrast, by providing guidance from both global (scene) and local (object) levels, our object-aware modeling approach can produce more accurate and faithful scenes
    }
    \label{fig:intro}
    % \vspace{-12pt}
\end{figure}

Inspired by this observation, we propose a compositional NeRF framework, called \textbf{CompoNeRF}, interpreting multi-object text guidance as an editable 3D scene boxes with fine-grained text prompts.
The scene layout collects individual object entities from the input text and gathers their editable 3D bounding boxes as shown in Fig.~\ref{fig:teaser}.
% As shown in Fig.~\ref{fig:teaser}, the scene layout collects individual object entities from the input text, input 3D bounding boxes, and their corresponding coordinates and scales.
In CompoNeRF, each box in the 3D scene layout is modeled by a local NeRF for representation learning, and global views are rendered by compositing the learned 3D representations from local NeRFs.
However, direct composition from all local NeRFs may not ensure coherent global views without addressing the following two issues.
% Nevertheless, the direct composition from all local NeRFs may not ensure the coherent global views, because it does not tackle the following two key issues.
% directly from the composited local views as 

% The first obstacle in achieving global view consistency across multiple local NeRFs is that they are optimized independently, making it difficult to learn their global spatial or semantic correlations. 
% To address this,
\textbf{1) Independence}: capturing consistent global views across multiple local NeRFs is hard due to non-shared parameters.
To solve this, we use a global MLP to calibrate the local NeRF predictions based on their samples' global coordinates and ray directions.
The model can gradually learn global consistency across multiple objects by passing the global constraint into local NeRFs.

\textbf{2) Occlusion}: inaccurate text guidance may result from fully occluded objects due to random camera positions in the training data.
% To tackle the occlusion issue, we apply local text guidance to each individual NeRF, rendering locally and utilizing the diffusion model to provide more precise guidance for object identification despite occlusions.
% To tackle the occlusion issue, for each local NeRF, we separately apply local text guidance on the locally rendered view, as the diffusion model can provide more accurate guidance to shape the object identity despite occlusions.
To tackle the occlusion issue, we employ local text guidance on every individual NeRF-rendered view. Thanks to the use of the diffusion model, we are able to provide precise guidance for object identification even in the presence of occlusion.
% As a result, our approach ensures the global consistency, generating realistic 3D content, even in crowded scenes, 
% as demonstrated in Figure~\ref{fig:teaser}, without missing objects or ambiguity.
As a result, our approach ensures coherent and realistic 3D generation, even in crowded scenes that include multiple objects as depicted in Fig.~\ref{fig:teaser} without object missing or text ambiguity.

Moreover, scene editing is facilitated by the use of 3D scene layout, allowing flexible manipulation of text and objects, including scaling, movement, and removal.
Using a vast pre-trained content gallery, we can rapidly generate desired 3D scenes with text prompts and layouts, democratizing 3D content creation.
The comparisons of related approaches are summarized in Tab.~\ref{tab:compare}.
Additionally, previous studies fail to include a quantitative comparison, resulting in inadequate evaluation relying solely on visual comparison. We thus propose to employ the extensively recognized CLIP score~\cite{wang2022clip} to assess the efficiency of generating 3D models from text descriptions. By doing so, we provide a more comprehensive demonstration of our proficiency in producing intricate multi-object scenes.
% In addition, the editable 3D layout facilitates object editing by allowing users to modify text and manipulate objects flexibly, including moving, scaling, and duplicating. With the aid of a vast pre-trained content gallery, users can rapidly generate their desired 3D scenes using text prompts and 3D scene layouts, democratizing the process of 3D content creation. 
% we compare CompoNeRF and other editing approaches summarized in Tab.~\ref{tab:compare}.
% A solution is to make the 3D representation and rendering network object-aware by binding object-related information to particular locations in the 3D space.
% However, existing approaches~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to encode the entire scene into a single neural network, which can successfully handle object-centric scenes but is generally agnostic to the object's identity.


% To overcome these challenges,
%  we propose a novel framework, called \textbf{CompoNeRF} that allows generating and editing scenes with editable 3D layouts in a compositional way.
% Given a text prompt involving multiple objects, CompoNeRF consisting three steps to composite object (\ie, local) views:
%  1) we parse the input text into individual object entities and gather coarse 3D bounding box information for each object, as such 3D layout can be easily obtained from the input text or users;
% 2) using the generated 3D scene layout, each object entity has its own local NeRF model to produce 3D information based on object-specific text guidance; 
% 3) we construct a unified scene rendering pipeline to render the complete scene by compositing all object-specific 3D information from local NeRFs into the scene view.

% To , given a text prompt involving multiple objects, we parse the input text into individual object entities and gather coarse 3D bounding box information for each object, as such 3D layout can be easily obtained from the input text or users.

% Despite the simplicity of our framework design,
% Nevertheless, these compositional designs can not ensure learning the coherent scene (\ie, global) views directly from the composited local views if the following two issues are not tackled.
% Firstly, the scene view consistency is hard to capture across multiple local NeRFs models as the none shared parameters.
% Therefore, we further adopt a global NeRF to calibrate the local NeRF prediction using the scene coordinate as conditional input.
% With the global-level text guidance applied to the global views composited from the local views, the global consistency can be gradually learned among multiple objects.
% Secondly, some objects can be completely occluded in the scene views, caused by random camera poses during training, rendering both global and local text guidance inaccurate.
% To tackle the occlusion issue, for each local NeRF, we separately apply local text guidance on the local rendered view as the diffusion model can provide more accurate guidance to shape the object identity, as shown in Fig.~\ref{fig:intro}.
%
% %
% However, global semantics often require additional transformation and composition of individual objects. 
% For example, consider the scene in Fig.~\ref{fig:intro} containing an apple and a banana with overlapping bounding boxes. 
% When the bounding boxes of two objects overlap, they may appear merged in the final rendered image, causing ambiguity in the scene. 
% To prevent this, the global scene needs to adjust the latent expression of each object to ensure that they maintain their distinct identities. 
% Our scene representation model helps in achieving this by adjusting the object-level latent expression with the scene coordination and the full context of input text, which improves view consistency in the scene view.
% %
% We design both scene-level and object-level score distillation losses to better balance the rendering guidance within our hierarchical scene representation and rendering framework. 
% The scene-level loss helps the model to learn global semantics, such as the spatial relationship between objects and guides the model to generate a coherent scene. 
% The object-level loss enables the optimization of the representation to keep each object's identity and fits the interactions between objects based on the global semantics.

% \begin{table*}[t!]
% \centering
% \resizebox{\linewidth}{!}{ 
%     \begin{tabular}{l|c|c|c|c|c|c}
%     \toprule
%     \textbf{Methods} & \textbf{Diffusion Model} & \textbf{\textbf{3D Representation}} & \textbf{Scene Rendering} & \textbf{Input Prompt} & \textbf{Scene Editing} & \textbf{recomposition}\\  \hline
%     DreamFusion~\cite{poole2022dreamfusion} & Imagen~~\cite{sahariaphotorealistic} & Mip-NeRF 360~\cite{barron2022mip} & Object-centric & Text & T & F\\
%     Magic3D~\cite{lin2022magic3d} & eDiff-I~\cite{balaji2022ediffi} + SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-centric & Text & T & F\\
%     SJC~\cite{wang2022score} & SD~\cite{rombach2022high} & voxel radiance field & Object-centric & Text & T & F\\
%     DreamBooth3D~\cite{raj2023dreambooth3d} & DreamBooth~\cite{ruiz2023dreambooth}+DreamFusion~\cite{poole2022dreamfusion} & Mip-NeRF~\cite{JonathanTBarron2021MipNeRFAM} & Object-centric & Text+Images & T & F \\
%     Points-to-3D~\cite{yu2023points} & ControlNet~\cite{zhang2023adding}+Point-E~\cite{nichol2022point} & Instant-NGP~\cite{muller2022instant} & Object-centric & Text+Image & T & F \\
%     \hline
%     Latent-NeRF~\cite{metzer2022latent} & SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-centric & Text+Fine Shape & T & F\\
%     Ours & SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-compostional & Text+3D Coarse Boxes & T/M/S/R & T \\ \bottomrule
%     \end{tabular}
% }
% % \vspace{-7pt}
% \caption{Comparison of our method and the related works for text-to-image generation. SD denotes Stable Diffusion. For scene editing, we use T(editing object with text), M(moving object), S(scaling object), and R(removing object) for short.}
% % \vspace{-8pt}
% \label{tab:compare}
% \end{table*}

\begin{table*}[t!]
\centering
\resizebox{\linewidth}{!}{ 
    \begin{tabular}{l|c|c|c|c|c|c}
    \toprule
    % \hlineB{2.5}
    \textbf{Methods} & \textbf{Diffusion Model} & \textbf{\textbf{3D Representation}} & \textbf{Scene Rendering} & \textbf{Input Prompt} & \textbf{Scene Editing} & \textbf{recomposition}\\  \hlineB{2.5}
    DreamFusion & Imagen & Mip-NeRF 360 & Object-centric & Text & T & \XSolidBrush\\    \hline
    Magic3D & eDiff-I + SD & Instant-NGP & Object-centric & Text & T & \XSolidBrush\\    \hline
    DreamBooth3D & DreamBooth+DreamFusion & Mip-NeRF & Object-centric & Text+Images & T & \XSolidBrush \\    \hline
    Points-to-3D & ControlNet+Point-E & Instant-NGP & Object-centric & Text+Image & T & \XSolidBrush \\    \hline
    \hline
    Latent-NeRF & SD & Instant-NGP & Object-centric & Text+Fine Shape & T & \XSolidBrush \\    \hline
    SJC & SD & voxel radiance field & Object-centric & Text & T & \XSolidBrush\\    \hline
    Ours & SD & Instant-NGP & Object-compostional & Text+3D Coarse Boxes & T/M/S/R & \Checkmark \\  
    \bottomrule
     \hlineB{1.5}
    \end{tabular}
}
% \vspace{-7pt}
\caption{\textbf{Comparison of our method with the related works for text-to-image generation}. SD denotes Stable Diffusion. For scene editing, we use T(editing object with text), M(moving object), S(scaling object), and R(removing object) for short.}
% \vspace{-8pt}
\label{tab:compare}
\end{table*}

To summarize, our paper makes three key contributions:  (\textbf{I})  We solve the guidance collapse issue in multi-object 3D scene generation by integrating an editable 3D layout with multiple local NeRFs to precisely associate guidance with specific objects. Addtionally, all local NeRFs can be cached, editted and reused for composing other scenes.
(\textbf{II}) We address the issues of independence and occlusion by using composition module to calibrate the overall rendering and varying levels of text guidance to maintain the identity of individual entities while ensuring global coherence. 
(\textbf{III}) We thoroughly evaluate the effectiveness of our proposed method across various multi-object scenarios, demonstrating its ability to composite and edit 3D scenes via qualitative and quantitative analysis. We propose to use CLIP score as the metric to evaluate the similarity of the generated 3D assets to the prompt text, and our CompoNeRF achieves the best performances on various multi-object scenes.
% Our results suggest that our approach provides a compelling solution for these tasks.
% \begin{itemize}
% \item We incorporate the editable 3D layout with multiple local object NeRF to accurately associate the guidance for specific structure in text-to-3D scene generation.
% \item We design a global color calibration module and different levels text guidance procedures to balance maintaining objects' identity and global coherence.
% \item We validate the proposed method under various scenarios, showing that it offers a compelling solution for compositional scene generation and flexible editing capacity.
% \end{itemize}

\section{Related Work}
\noindent\textbf{Text-guided 3D Generative Models.}
% The field of 3D model generation has sparked numerous research that explores various 3D representations, including 3D voxel grids~\cite{gadelha20173d}, point clouds~\cite{achlioptas2018learning}, meshes~\cite{JunGao2022GET3DAG}, implicit representations~\cite{ZhiqinChen2018LearningIF}, and octree representations~\cite{MoritzIbing2022OctreeTA}.
Motivated by the success of the vision-language models,\eg,~CLIP~\cite{radford2021learning}, text-guided 3D generation has also been rapidly progressing.
Several works~\cite{jain2022zero,wang2022clip,mohammad2022clip,sanghi2022clip,xu2022dream3d} utilize the pre-trained vision-language model to provide robust alignment between image features extracted from rendered views of 3D representations and text features by minimizing feature similarity.
% Inspired by the achievements of vision-language models such as CLIP~\cite{radford2021learning}, there has been significant progress in text-guided 3D generation. Several works~\cite{jain2022zero, wang2022clip, mohammad2022clip, sanghi2022clip, xu2022dream3d} have employed pre-trained vision-language models to align image features extracted from rendered 3D views with text features by minimizing feature similarity. 
% As the pre-trained vision-language model can offer robust alignment between image and text features, 
% incorporating text guidance into the generation process has become a popular approach in recent works~\cite{jain2022zero,wang2022clip,mohammad2022clip,sanghi2022clip,xu2022dream3d}.
% They achieve text-to-3D generation by minimizing the embedding similarity between the 2D rendering results obtained from 3D representations and the input text.
% However, despite that large-scale pre-trained vision-language models can provide strong 2D guidance for 3D representation learning, their 2D rendering results may be less realistic due to limited feature details.
Though pre-trained large-scale vision-language models can offer strong 2D guidance for 3D representation learning, the quality of their 2D rendering results may be less realistic due to insufficient details in the features.
Recently, DreamFusion~\cite{poole2022dreamfusion} demonstrates remarkable capability in text-to-3D object generation by incorporating a powerful pre-trained text-to-image diffusion model~\cite{saharia2022photorealistic} through score distillation sampling.
Similarly, Magic3D~\cite{lin2022magic3d} proposes a two-stage super-resolution method to enhance generation quality with hybrid 3D representation.
Latent-NeRF~\cite{metzer2022latent} demonstrates that updating NeRF in the latent space using score distillation sampling can also produce realistic results. 
SJC~\cite{wang2022score} improves the sampling process by introducing a perturb-and-average scoring scheme to address distribution mismatching issues.
DreamBooth3D~\cite{raj2023dreambooth3d} proposes a three-stage method that utilizes DreamBooth~\cite{ruiz2023dreambooth} and DreamFusion to generate 3D object from the text and 3-6 images inputs.
Points-to-3D~\cite{yu2023points} utilizes the sparse 3D point cloud as the immediate representation to guide the 3D generative NeRF model.
% with generative diffusing models. 
% By contrast, our CompoNeRF generate multi-object 3D scenes based on the editable 3D scene layout that represents each object with a local NeRF associated through a spatial location (\eg, 3D box) and its text description.

By contrast, our CompoNeRF generates multi-object 3D scenes in an \textit{object-compositional} way by utilizing multiple local NeRFs to model the scene instead of a holistic object-centric representation.
Our editable 3D scene layout allows for editing of scenes by changing the text prompt similarly as ~\cite{poole2022dreamfusion, lin2022magic3d}. Also, it enables manipulation of the spatial arrangement of individual objects in a crowded scene, as summarized in Tab.~\ref{tab:compare}.
Moreover, the layout allows for recomposition with other off-the-shelf representations, enabling the rapid generation of new scenes.

\noindent\textbf{Neural Rendering for 3D Modeling.}
The advancement of NeRF has greatly improved the performance of neural renderers.
NeRF models~\cite{nerf,lindell2021autoint,muller2022instant,Liu2020NeuralSV,mip-nerf,ref-nerf,KaiZhang2020NeRFAA} are a family of volume rendering algorithms that utilize coordinate-based MLPs to directly predict color and opacity from the 3D position and 2D viewing direction. 
The photo-realistic synthesized views from these models have led to the widespread adoption of differential volume rendering in various applications, \eg, relighting~\cite{srinivasan2021nerv,zhang2021nerfactor}, dynamic scene reconstruction~\cite{gao2021dynamic,pumarola2021d,xian2021space,tretschk2021non}, editable scenes and avatars~\cite{liu2021neural,yang2021learning}, and surface reconstruction~\cite{azinovic2022neural,wang2021neus}.
% The resulting synthesized views exhibit photo-realistic quality, which has led to the widespread adoption of differential volume rendering in many applications, such as object and scene relighting~\cite{srinivasan2021nerv,zhang2021nerfactor}, 
% % unbounded scenes~\cite{KaiZhang2020NeRFAA,barron2022mip}, 
% dynamic scene reconstruction from videos~\cite{gao2021dynamic,pumarola2021d,xian2021space,tretschk2021non}, editable scenes, avatars~\cite{liu2021neural,yang2021learning}, and object surface reconstruction~\cite{azinovic2022neural,wang2021neus}.
These methods commonly use \textit{single} MLP to encode the entire scene, which can be ambiguous \wrt specific object identities within the scene.
Note that our method involves rendering the scene using multiple local NeRFs, and the rendering process requires compositing all the local NeRF predictions based on their spatial relationships.
% Differently, our framework improves the compositional capability of scene representation by employing multiple implicit representations to model individual objects separately. 
% In this work, we use Instant-npg~\cite{muller2022instant} as basic NeRF representation due to its fast training and inference speed.

\noindent\textbf{Object-Compositional Scene Modeling.}
The idea of generating new scenes by compositing multiple object-centric representations is a straightforward approach in scene generation~\cite{guo2020object}.
Several works~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe,ost2021neural,xu2022discoscene,song2022towards} attempt to directly decompose object representations from the scene image to perform compositional scene modeling. Based on the additional object-level information, they can be roughly categorized as semantic based~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe} and 3D layout based~\cite{ost2021neural,xu2022discoscene,song2022towards}.
% Given some object representations gallery, OSFs~\cite{guo2020object} introduces a volumetric path tracing procedure to enable rendering scenes with moving objects and lights without retraining.
% Several works~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe,ost2021neural,xu2022discoscene,song2022towards} attempt to perform compositional scene modeling by directly decomposing object representations from the scene image. These methods can be broadly categorized as either semantic-based~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe} or 3D layout-based~\cite{ost2021neural,xu2022discoscene,song2022towards}, depending on the type of additional object-level information utilized. 
% semantic based and 3D layout based.
% However, obtaining well-trained object representation is impractical for the real-world application as in most cases only the whole scene images are collected. 
% Therefore, several works focus on decomposing the object representations from the scene image directly using various object-level information, which can be roughly categorized as semantic-based~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe} and 3D layout based~\cite{ost2021neural,xu2022discoscene,song2022towards}.
Semantic based methods incorporate extra semantic information, such as segmentation labels~\cite{zhi2021place}, object instance mask~\cite{yang2021learning,wu2022object}, and features from the pre-trained vision-language model~\cite{mirzaei2022laterf}, to learn the object representations.
% For example, instance masks can guide the object decomposition process for editable scene rendering~\cite{yang2021learning} and surface reconstruction~\cite{wu2022object}.
% For example, SemanticNeRF~\cite{zhi2021place} combines an additional semantic head into NeRF to predict the semantic label for input 3D position.
% Similarly, instance masks also can be used to guide the object decomposition process for editable scene rendering \cite{yang2021learning} and surface reconstruction~\cite{wu2022object}.
% LaTeRF~\cite{mirzaei2022laterf} use feature from pre-trained model to evaluate the objectness at each 3D points and in-paint the occluded parts of the object.
% GIRAFFE~\cite{niemeyer2021giraffe} extends the generative ability of scene composition by introducing conditions latent codes for each object representation and discriminator for adversarial scene learning.
On the other hand, 3D layout-based methods directly use the 3D object coordinate information as the object guidance for learning object-specific representation and generating the full scene by compositing all object representations.
For instance, NSG~\cite{ost2021neural} and its variant~\cite{song2022towards} use a scene graph structure to model dynamic scenes, associating each object with a 3D box and a node representation.
% Similarly, DisCoScene~\cite{xu2022discoscene} adopts 3D boxes to spatially disentangle the entire scene into object-centric generative radiance fields.
% and learn the scene representation on 2D images with global-local discrimination.
While previous works focused on decomposing object entities from real-world images, our work serves as the \textit{\textbf{first}} attempt to tackle the text-to-3D generation via decomposed representations with 3D scene layout.


\section{Methodology}
% This section introduces the rendering pipeline of our proposed hierarchical compositional scene. 
% our pipeline consists of three processes, including decomposing the text into editable 3D layout, rendering the compositional views with local (object) NeRFs and global (scene) NeRF and the joint optimization on these hierarchical 3D representations.

% Note that the transformation between the object and the scene frame is defined by ${p}_o$ and ${D}_o$. 
%
% Next, we build a residual connection to add ${\sigma}_o$ and the referenced global color, and the rendering result will be used to calculate the SDS loss based on the global text.  
% Fig.~\ref{fig:framework} illustrates our pipeline, which consists of three main components, including the editable 3D scene layout based on multi-object text (Sec.~\ref{ssec:layout}), the scene rendering pipeline that composites the predictions from all local NeRFs (Sec.~\ref{ssec:render}), and the joint optimization on both local and global representation models (Sec.~\ref{sec:optimization}).
% To elaborate, our editable 3D scene layout represents a global frame of the scene by decomposing it into a set of local frames, where each is parameterized by a local NeRF, a 3D bounding box, and a corresponding local text prompt.
% For instance, the text prompt `A teddy bear and a stuffed monkey sit side by side' is interpreted as a 3D scene layout, as shown in Fig.~\ref{fig:framework}.  
% The whole 3D layout, \ie, scene frame, consists of two 3D bounding boxes, \ie local frames \#1 and \#2, with specific local text prompts, \ie, `a teddy bear' and `a stuffed monkey'. 
% %
% To render the scene view, we first calculate the ray-box intersections between the boxes and rays $({\boldsymbol{r}}_o, \boldsymbol{\phi}_d, {\boldsymbol{\theta}}_d)$, where the ${\boldsymbol{r}}_o$ is the ray origin and the $({\boldsymbol{r}}_o, \boldsymbol{\phi}_d)$ is its direction.
% Then, to infer each object's properties in local NeRFs, we sample the global points $({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g)$ in the global frame within the ray-box intersection intervals and project them into the normalized local location $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$ in the local frame.
% %
% Given the local sampling points $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$, the implicit local NeRF ${\boldsymbol{\theta}}_l$ outputs four pseudo-color channels ${\boldsymbol{C}}_l$ and density $\boldsymbol{\sigma}$, which can be used to render a local view of the local frame to match its local text prompt.
% %
% We further calibrate the predicted pseudo-color $\boldsymbol{C}_l$ from local frames by adding the global embeddings ${\boldsymbol{emb}}_g$ to improve the global view consistency.
% Then, the calibrated predictions after composition are used to reconstruct the scene view by volumetric rendering along the rays.
% %
% Lastly, the rendered views based on local and global frames are guided by score distillation sampling loss $\nabla \mathcal{L}_{\text{SDS}}$~\cite{poole2022dreamfusion} to optimize all the learnable parameters. 
To resolve the issue of guidance collapse, our core idea is to \textit{decompose the scene into reusable components and compose/recompose them into a globally consistent one}.
This enables flexible control over the generated content with simple prompts and box layouts, as illustrated in \cref{fig:teaser}.
%
Our proposed CompoNeRF offers the following advantages: 
1) \textbf{semantic coherence}: we generate 3D objects precisely with rich texture and global consistency, such as realistic light reflection on the bed.
2) \textbf{disassembly and reusability}: CompoNeRF is a collection of individually trained NeRF models that can be saved and reloaded from the off-the-shelf dataset.
3) \textbf{editability}: our method support recomposition by placing NeRF models at the arbitrary scene positions, \eg, replacing the lamp with a vase of sunflowers or adjusting object size by simply scaling their bounding boxes. 


% Furthermore, the usage of layout boxes enables more flexible control over the generated content compared with the intricate sketch shape in Latent-NeRF\cite{metzer2022latent}. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/method.pdf}
    % \vspace{-16pt}
    \caption{\textbf{Framework Overview}. Our model consists of two branches where the upper part is individual NeRFs, and the lower part denotes global calibration with our tailored composition model. The specific designs for density-based and color-based composition modules are highlighted, while the black ones are shared for the two options.
    % CompoNeRF consists of three parts: 1). The editable 3D scene layout configures the scene representations with 3D boxes and text prompts; 2).  The scene rendering includes the global calibration and the compositional process; 3). The joint optimization applies global and local text guidance on global and local render views.
    % The global frame (scene space) contains a set of local frames. Each is  represented by a local NeRF associated with a 3D box and text prompt defined by the editable 3D layout.
    % The scene view is volumetric rendered by sampling the points $({\boldsymbol{x}}_g, \boldsymbol{y}_g, \boldsymbol{z}_g)$ intersected with any local frame along the ray $(\boldsymbol{r}_o, {\boldsymbol{\phi}}_d, \boldsymbol{\theta}_d)$.
    % The sampling points are first inferred through the local NeRF with the local frame locations $({\boldsymbol{x}}_l, \boldsymbol{y}_l, \boldsymbol{z}_l)$ projected from the global location $({\boldsymbol{x}}_g, \boldsymbol{y}_g, \boldsymbol{z}_g)$.
    % And then, all the local predictions are calibrated by a global MLP with conditional input to render the scene view.
    % During the optimization, the text guidance is applied to both local views predicted by local frames only and global views predicted by the composition of all local frame predictions.
    }
    \label{fig:framework}
    % \vspace{-10pt}
\end{figure}

\subsection{Preliminaries}
% In this section, we xxx. 
Referring to individual boxes as \textit{local frames}, and the global scene space as the \textit{global frame}, we introduce both preliminaries on NeRF and diffusion here.

\label{sec:background}
\noindent \textbf{3D Representation in Latent Space.}
Our approach is built upon SoTA text-to-image model\textemdash Stable Diffusion~\cite{rombach2022high}.
To avoid heavy pixel computations, we rely on Latent-NeRF~\cite{metzer2022latent} to infer pseudo-color for each object using local NeRF.
% $\boldsymbol{\theta}_{l}$ that outputs four pseudo-color channels ${\boldsymbol{C}}$, corresponding to the four latent features that Stable Diffusion operates over, and a volume density ${\boldsymbol{\sigma}}$. 
Specifically, the representation maps a point $\boldsymbol{x}_l = \left({x}_l, {y}_l, {z}_l\right)\in [-1, 1]$ in the local frame to its corresponding volumetric density $\boldsymbol{\sigma}_l$ and emitted color $\boldsymbol{C}_l$, \ie,  $\left(\boldsymbol{C}_l, {\boldsymbol{\sigma}_l}\right)=\boldsymbol{\theta}_{_l}\left({x_l}, {y}_l, {z}_l\right)$.
The predicted pseudo-color is fed forward into the decoder of the Stable Diffusion model to obtain the final rendering result.

\noindent \textbf{Volume Rendering with Multiple Objects.}
For each local frame $j$ with NeRF parameterized as $\theta_j$, we follow original NeRF design\cite{nerf} to integrate $(\boldsymbol{C}_l, \boldsymbol{\sigma}_l)$ of   sampled points from any hit ray $r_l=(\boldsymbol{o}_l, \boldsymbol{d}_l)$ by,
{
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eq:volrend}
{\hat{\boldsymbol{C}}_l}({\boldsymbol{r}_l})=\sum_{k=1}^{N} T_k \left(1-\exp \left(-\sigma_{l, k} \delta_k\right) \right) {\boldsymbol{C}}_{l,k},
\end{equation}}
%
where $\boldsymbol{o}_l$ is the camera center that emits rays with direction $\boldsymbol{d}_l$. For a batch of rays $\boldsymbol{r}$, the light transmittance to sample $i$ is denoted as $T_k=\exp \left(-\sum_{j=1}^{k-1} \sigma_{l,j} \delta_j\right)$, 
% In fact, each local frame only has a small number of hit rays compared to the scene.
% Despite the fact that parts of rays are skipped, we observe that it is enough to represent each object accurately while maintaining short rendering times.
$\delta_k$ is the distance between adjacent samples, $\hat{\boldsymbol{C}}_l$ is the predicted color, and N is the number of samples within each ray interval. 

For consistent scene rendering, object transmittance $T_k$ must be recalculated in the global frame based on independent properties inferred from local NeRFs. Hence, we sort predictions according to their distance to $\boldsymbol{o}_g$. 
Similar to \cref{eq:volrend}, global color $\hat{\boldsymbol{C}}_g$ of ray $\boldsymbol{r}_g=(\boldsymbol{o}_g, \boldsymbol{d}_g)$ is predicted by the volumetric rendering integrating over $m$ objects,
{
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eq:multi_volrend}
{\hat{\boldsymbol{C}}_g}({\boldsymbol{r}_g})=\sum_{k=1}^{m*N} T_k \left(1-\exp \left(-\sigma_{g, k} \delta_k\right) \right) {\boldsymbol{C}}_{g,k}. 
\end{equation}}

\noindent \textbf{Score Distillation Sampling.}
To achieve text-to-3D generation, DreamFusion~\cite{poole2022dreamfusion} introduces Score Distillation Sampling (SDS) to propagate the text-to-image generative prior from diffusion model $\phi$ to the NeRF parameters $\boldsymbol{\theta}$.
During the SDS process, a noise image $\boldsymbol{X}_t$ is first generated by adding a sampled noise $\epsilon \sim \mathcal{N}(0, I)$ in noise level $t$ into a rendered view $\boldsymbol{X}$ from a NeRF.
Then, the diffusion model $\phi$ predicts the sampled noise $\epsilon_\phi\left(\boldsymbol{X}_t, t, T\right)$ given the noisy image $\boldsymbol{X}_t$, noise level $t$, and optional text prompt $T$.
Specifically, SDS computes the gradient from the distance between the predicted and added noises,
{\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eq:sds_loss}
\nabla_\theta \mathcal{L}_{\text{SDS}}(\boldsymbol{X}_t, T)=  w(t)\left(\epsilon_\phi\left(\boldsymbol{X}_t, t, T\right)-\epsilon\right),
\end{equation}}where $w(t)$ is a weighting function. 
The gradient direction generated on all rendered views is used to update $\boldsymbol{\theta}$ to match images conditioned on text prompt under diffusion prior.
% During training, gradients are propagated from the pixel gradients to the NeRF parameters and gradually change the 3D object.
We also follow SJC~\cite{wang2022score} to apply the perturb and average scoring into SDS.
Please refer to \cite{poole2022dreamfusion,wang2022score} for the complete details.

\subsection{The Proposed CompoNeRF}
\subsubsection{A. Composition Module}
CompoNeRF aims to composite multiple NeRFs to reconstruct multi-object scenes with both box and prompt guidance. 
% allowing for versatile control by modifying the box's or text's properties to define a new scene. 
% \noindent \textbf{Framework.}
Our framework, as shown in \cref{fig:framework}, applies the AABB ray intersection test algorithm to check for intersections on each box in the global frame. We then samples $\boldsymbol{x}_g$ within the ray box intervals, and project them to $\boldsymbol{x}_l$ to infer  $\left(\boldsymbol{C}_l, {\boldsymbol{\sigma}_l}\right)$ in separate NeRF models. 
%
We then utilize volume rendering to obtain rendered views for each local frame respectively. 
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/abls.pdf}
    % \vspace{-18pt}
    % \caption{Ablation study on text guidance. (a) without local SDS losses. (b) without global SDS losses. (c) vanilla SDS losses without perturb and average scoring~\cite{wang2022score}. (d) full model.}
    \caption{\textbf{The visual comparison of the impact of design choices}. \textbf{(a)} global text guidance(integrating local frames by \cref{eq:multi_volrend}) and global calibration(integrating local frames, then aligning the rendering result directly with the full text). \textbf{(b)} comparison between color-based and density-based options.}
    \label{fig:abls}
    % \vspace{-12pt}
\end{figure}
%
%
% \subsection{Editable 3D Scene Layout}
% \label{ssec:layout}
% The 3D scene layout explicitly combines language structures with 3D layouts in an editable way.
% Given the input text prompt $T$, the attribute-object pairs can be easily obtained based on user control.
% Note that the text prompt indicates the multi-object text prompt by default.
% % available for free in many structured representations, such as the constituency tree.
% As shown in Fig.~\ref{fig:framework}, we can extract multiple noun phrases with their binding attributes and map these local text prompts into corresponding regions.
% Specifically, we define the scene structure with $m$ local frames, each employs a local NeRF $\boldsymbol{\theta}_l$ as representation, the local text prompt $T_{l} \subseteq{T}$ and its spatial layout with 3D boxes $\mathbf{b} = \{\mathbf{p}, \mathbf{s}\} \in  \mathbb{R}^6$ of each object entity, where $\mathbf{p}=\{p_x, p_y, p_z\}$ refers to the center point and $\mathbf{s}=\{s_x, s_y, s_z\}$ denotes the box scale. 
% \textit{Our editable 3D layout is easy to be collected and edited with its simplicity, allowing for versatile and interactive user control by modifying the box's or text's properties to define a new scene}.
% Moreover, as depicted in Fig.~\ref{fig:teaser}, each component in a 3D scene layout can be replaced or re-composited with other trained local NeRFs, which is more friendly for flexible user editions compared with using only text prompts.
% We fine-tuned the new layout by global rendering, which enables scalable re-editing.
% Each relationship $r_k \in R$ is a triplet in a <subject-predictive object> format, where a subject node is. After we generate the scene graph from the complex prompts, we can sample the closest relationship with the 2d spatial layout as the initial 3D position. fine-tuned the new layout by global rendering, which enables scalable re-editing
%
% \subsection{Scene Rendering Pipeline}
% \label{ssec:render}
% In CompoNeRF, the scene images are rendered by a ray-casting approach following the design of NeRF.
% % Each ray to be cast is generated based on the camera pose, intrinsic, and transformation.
% The camera is defined by a pinhole camera model, casting a set of rays $(\boldsymbol{r}_o, \boldsymbol{\phi}_d, {\boldsymbol{\theta}}_d)=\boldsymbol{o}+t\boldsymbol{d}$ through each pixel on the frame of size $H \times W$, where the $\boldsymbol{r}_o \in  \mathbb{R}^3$ is the origin and the $(\boldsymbol{\phi}_d, \boldsymbol{\theta}_d)$ is the viewing direction.
% Along this ray, we sample all the points intersected with any layout box of local frames.
% For each hit sampled point, the color and volumetric density are computed through the local NeRF of the hit local frame.
% The ray color perdition is calculated by the differentiable integration applied on all the point-predicted colors and volumetric density along the ray.
%
% \noindent \textbf{Ray-box Intersection with Local Frames.}
% Given a ray $\boldsymbol{r}_i$, each box $\boldsymbol{b}_j$ of the local frame is applied with the AABB ray intersection test algorithm to check the intersections.
% When the ray $r_i$ is hit with a box $\boldsymbol{b}_j$ of the local frame, we use the entrance and exit points as near $\boldsymbol{t}_{in}$ and far $\boldsymbol{t}_{out}$ bounds to sample $N$ equidistant quadrature points, $
% \boldsymbol{t}_{i,j,n}=\frac{n-1}{N-1}\left(\boldsymbol{t}_{out}-\boldsymbol{t}_{in}\right)+\boldsymbol{t}_{in} , n \in \left[1, N\right]$
% % Despite each local frame only having a small number of hit rays compared to the scene, we observe that it is enough to represent each object accurately while maintaining short rendering times.
% Note that the coordinates of sampled points are first projected into normalized coordinates using the box scale of local frames to enable each local NeRF to learn the scale-independent representation.
% The bounding box $\mathbf{b}$ of the local frame in global coordinate can be transformed into a canonical bounding box by ${(\mathbf{b}} - \boldsymbol{p}) / \mathbf{s}$.
% Considering the rendering efficiency, we only calculate the valid points, interacted with the boxes, and set all the empty points with a constant background color.
%
% The appearance of a set object representations depends on its interaction with the scene and illumination which should be decided by the local frame location.
% To ensure the volumetric consistency, we only calibrate the emitted color with scene location, while the gradient still can be propagated.
% Since the overall color depends on both the global  positions $({x}_w, {y}_w, {z}_w)$ and ray directions $({\phi}_d, {\theta}_d)$, the global color embedding is learned based on both the positions and ray directions.
% Since the overall color depends on both the global  positions $({x}_w, {y}_w, {z}_w)$ and ray directions $({\phi}_d, {\theta}_d)$, the global color embedding is learned based on both the positions and ray directions.
After that, they would be passed on to our tailored composition Module to infer 
$\left(\boldsymbol{C}_g, {\boldsymbol{\sigma}_g}\right)$
for global rendering. 
Next, we match local and global texts with their corresponding image outputs by SDS losses. 
We also support recomposition by passing samples from cached models into $\boldsymbol{x}_l$ to continue the above process.

\textbf{Global Composition.}
Each local frame is optimized independently, causing a lack of global connections for scene composition. As shown in \cref{fig:abls}(a), we verify its necessity by dropping $\nabla \mathcal{L}_{\text{SDS}_g}$. 
%
Compared with our full model, its layout does not fit our shared sense of a room, \ie, \emph{nightstand} is usually lower than \emph{bed}; \emph{lamp} needs a base to support it. Additionally,  it lacks global consistency, such as light reflection, to make it more realistic. 
%
Therefore, we leverage the full text semantics to ensure consistent global rendering across local frames. 
%
Instead of conditioning the global rendering view with the full prompt directly, we note that global calibration is necessary for geometry and color to be learned sufficiently.
For example, we observe that geometric completeness and texture of \emph{nightstand} are not ideal. Although reflection appears around \emph{nightstand}, \emph{bed} is stripped of the light. 
%
Therefore, we opt to leverage the correlation between the rendering output of the combined NeRFs and the overall semantics to perform multi-object scene reconstruction.  

Before delving into module details, there are two choices (see \cref{fig:framework}) on the composition module design we need to elaborate on first. 
%
In \cref{fig:framework}, by taking $\boldsymbol{x}_g$ into the composition module, their inferred properties are calibrated with gradients propagated from the global SDS loss. 
However, it remains unclear whether $\boldsymbol{\sigma}_g$ should be refined or not. 
%
The trade-off on its usage is the density adjustment bringing a more reasonable layout and more geometric details that fit the global text prompt. While its potential downside is that $\boldsymbol{C}_g$ may not be optimal as $\boldsymbol{\sigma}_g$ has more uncertainty compared to $\boldsymbol{\sigma}_l$, bringing sub-optimal rendering results. 

We choose the density-based method after comparing them with the experiment shown in \cref{fig:abls}(b). 
%
Specifically, we test both designs on the scene \emph{table wine} and discover that the density-based design provides more intrinsic details(as indicated by green boxes), \eg, enriched wood grains, and a more natural shape for \emph{salad} and has much faster convergence speed. In contrast, the color-based method enhances the reflection and smooths flickering on \emph{wine cup}, (as indicated by red boxes), but it suffers from 1) sparse density, resulting in poorly generated geometry at the base of  \emph{cup} and the wood \emph{table} corner. Additionally, shadow artifacts appeared on \emph{table} when viewed up close, outweighing benefits of the color-based method.

\noindent\textbf{Network Design.}
\cref{fig:framework} depicts the network architecture of the composition module. Denote $m$ as local MLP $\{\boldsymbol{\theta}_l\}_{l=1}^{m}$ for each local frame. Then, we introduce the global MLPs including density $\boldsymbol{\theta}_{g_d}$ and $\boldsymbol{\theta}_{g_c}$ calibrators to refine $\boldsymbol{\sigma}_l$ and $\boldsymbol{C}_l$. 
%
In detail, the network design is, 
{
% \setlength\abovedisplayskip{4.5pt}
% \setlength\belowdisplayskip{4.5pt}
\begin{align}
\label{eq:g_c_d}
{\boldsymbol{\sigma}_g}  &= \alpha_d \boldsymbol{\theta}_{g_d}({\boldsymbol{\sigma}_l}) + \boldsymbol{\sigma}_l, \\  
{\boldsymbol{C}_g}  &= \alpha_c \boldsymbol{\theta}_{g_c}({\boldsymbol{C}_l},  {\boldsymbol{d}_g}) + \boldsymbol{C}_l, 
\end{align}}
%
where residual $\boldsymbol{\sigma}_l, \boldsymbol{C}_l$ assist in learning $\boldsymbol{\sigma}_g$ and $\boldsymbol{C}_g$, while $\alpha_d, \alpha_c$ balance their contribution as learnable parameters.
%
Note that the color-based omits density calibration, and simply uses the shared color refinement.



% The 3D boxes are only used for the spatial configuration of local NeRFs, while the implicit representation of local NeRFs is inferred by the canonical samples inside the local frame without considering the global relationship across different objects.
% To relieve such location-dependent effects, we further calibrate the output color and density from the local NeRF with global coordinates $({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g)$ and ray directions $\left({\boldsymbol{\phi}}_{d}, {\boldsymbol{\theta}}_{d}\right)$ as the conditional input.
% % to inject the global visual clues.
% %
% %
% Specifically, we adopt a shared MLP $\boldsymbol{\theta}_{g}$ to calibrate all the predicted object colors, that is,
% {\setlength\abovedisplayskip{4.5pt}
% \setlength\belowdisplayskip{4.5pt}
% \begin{align}
% \label{eq:MLP_dyn_2}
% {\boldsymbol{C}_g} = {\boldsymbol{C}_l} + \boldsymbol{emb}_{g} &= {\boldsymbol{C}_l} + \boldsymbol{\theta}_{g}({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g, {\boldsymbol{\phi}}_{d}, {\boldsymbol{\theta}}_{d}),
% \end{align}}
% where ${\boldsymbol{C}_l}$ is the color predicted by the local NeRF.
% Therefore, the scene color can preserve the view-consistent behavior from the original architecture and add consistency across poses for the volumetric density.
% Since the color and density values share the same latent expression in $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$, we only calibrate the emitted scene color explicitly with the scene location, as the densities of local NeRFs also are implicitly adjusted during optimization.

% \noindent \textbf{Global and Local Volumetric Rendering.}
% After compositing all the interacted points, each ray $\boldsymbol{r}_i$ collects a set sampling points by $\{\boldsymbol{t}_{i,j,n} \}_{j=1, n=1}^{m_j, N}$, where $m_j$ is the number of the hit object.
% For each sampling point, the inference results with the respective 3D representations are the local color $\boldsymbol{c}_{l}$, global color $\boldsymbol{c}_{g}$, and density $\sigma$.

% In fact, the local view $\hat{C}_{l,j}$ of single object $j$ also can be rendered by the sampled points  belongs to the same local frames as shown at Fig.~\ref{fig:framework}.
\subsubsection{B. Recomposition}
Thanks to our CompoNeRF design, we can easily reconstruct scenes using available models. Additionally, it allows us to edit the boxes with $\{\boldsymbol{\theta}_l\}_{l=1}^{m}$ before arranging them into a new layout. At \cref{fig:teaser}, the input panel allows for editing of box positions, scales, and replacement with offline models. CompoNeRF enables scene-editing functions, such as text-editing, moving, scaling, and removing, as shown at \cref{tab:compare}. Furthermore, trained NeRFs can be decomposed and cached for other scenes.
\subsubsection{C. Optimization}
After the multi-object volume rendering with \cref{eq:multi_volrend}, we match their rendering results with the corresponding text prompts, which is formulated as follows:
\label{sec:optimization}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/sota.pdf}
    % \vspace{-18pt}
    \caption{\textbf{Qualitative comparison with other text-to-3D methods using multi-object text prompts}. Case 1-3 represent easy settings with two-object compositions, while Cases 4-8 represent complex settings with more than two-object compositions. The small images next to our results depict the generated components (partially shown in Cases 4-8).}
    \label{fig:sota}
    % \vspace{-5pt}
\end{figure*}

% \begin{table*}[t!]
% \centering
% \resizebox{\textwidth}{!}
% {
% \begin{tabular}{cccccccc}
% \toprule
% Method            & \rotatebox{60}{table wine}  & \rotatebox{60}{teddy monkey} & \rotatebox{60}{computer mouse} & \rotatebox{60}{bed room}  & \rotatebox{60}{chess} & \rotatebox{60}{pisa tower} & \rotatebox{60}{astronaut} & \rotatebox{60}{tesla}  \\ \midrule
% LatentNeRF  & 21.55 & 27.38 & 17.13 & 21.86 & 31.19 & 24.31 & 27.07 & 25.16 \\
% SJC & 23.33 & 27.37 & 18.00 & 22.54 & 30.53 & \textbf{26.18 }& 27.84 & 23.55 \\
% CompoNeRF & \textbf{32.68} & \textbf{28.57}	 &\textbf{ 22.34} &\textbf{ 28.65} & \textbf{31.45} & \textbf{28.96} & 25.82 & 25.95 & 24.42 & \textbf{32.71} & \textbf{26.13 }& \textbf{26.38} & \textbf{30.98} & \textbf{33.37} \\
% \bottomrule
% \end{tabular}
% }
% \vspace{-10pt}
% \caption{Performance of our CompoNeRF in different 3D scenes. We use CLIP score \cite{parmar2023zero,zhang2023sine,wang2023imagen} as our evaluation metric, which is a common evaluation metric in text-to-image generation tasks to evaluate the similarity of the generated image to the text prompt. }
% \label{perclass}
% \end{table*}

\begin{table*}[t!]
% \scalebox{0.8}
\renewcommand{\arraystretch}{1.2}
\fontsize{4pt}{4pt}
\selectfont 
\centering
% \vspace{-8pt}
\resizebox{\textwidth}{!}
{
% \begin{tabular}{lcccccccc}
% \hline
% Method     & table\_wine    & tesla          & pyramid        & chess          & apple and banana      & astronaut      & glass\_balls   & Eiffel\_tower    \\ \hline
% LatentNeRF & 21.55          & 25.16          & 27.43          & 31.19          & 27.69          & 27.07          & 29.51          & 26.32          \\
% SJC        & 23.33          & 23.55          & 25.62          & 30.53          & 28.21          & 27.84          & 28.76          &27.41 \\
% \textbf{CompoNeRF(Ours)}     & \textbf{32.68} & \textbf{26.13} & \textbf{28.96} & \textbf{31.45} & \textbf{33.37} & \textbf{32.71} & \textbf{30.98} & \textbf{28.44}          \\ \hline
% \end{tabular}
\begin{tabular}{lcccccccc}
\hline
Method                   & Case 1         & Case 2         & Case 3         & Case 4         & Case 5         & Case 6         & Case 7         & Case 8         \\ 
\hlineB{1.1}
LatentNeRF               & 25.16          & 27.07          & 27.69          & 31.19          & 21.55          & 26.32          & 27.43          & 29.51          \\
SJC                      & 23.55          & 27.84          & 28.21          & 30.53          & 23.33          & 27.41          & 25.62          & 28.76          \\
\textbf{CompoNeRF (Ours)} & \textbf{26.13} & \textbf{32.71} & \textbf{33.37} & \textbf{31.45} & \textbf{36.06} & \textbf{28.44} & \textbf{28.96} & \textbf{30.98} \\ \hlineB{1.1}
\end{tabular}
}

% \vspace{-6pt}
\caption{\textbf{Performance comparison of our CompoNeRF in different 3D scenes}. We use CLIP score \cite{parmar2023zero,zhang2023sine,wang2023imagen} as our evaluation metric, which is a common evaluation metric in text-to-image generation tasks to evaluate the similarity of the generated image to the text prompt. }
\label{tb:perclass}
\end{table*}


% , as shown in Fig.~\ref{fig:framework}.
% For each scene described by the multi-object text prompt $T$, we
% To enhance the guidance of local representations, we use the local text prompt $T_l \subseteq T$ of a single object to optimize the local NeRFs in local views.
% The scene views $\hat{\boldsymbol{X}}_g=\{\hat{\boldsymbol{C}}_{g,i}\}_{i=1}^{H\times W}$ is obtained from the predicted pixel values of $H \times W$ rays by compositing all the ray-box interaction values.
% Similarly, the rendered view $\hat{\boldsymbol{X}}_{l,j}$ of the local frame $\boldsymbol{\theta}_j$ without compositing other objects can be calculated by $\hat{\boldsymbol{C}}_{l,j}$, as depicted in Sec.~\ref{ssec:render}.
% We use the local color instead of the globally calibrated color to obtain a local view because the local NeRF should learn the object identity unrelated to its placed position, as the position can be different during user edition.
% % Compared to cropping the local region from a global view for training, separate rendering can avoid the undesired information from other objects brought by the occlusion and resolution adjustments.
% Formally, we employ the following loss as the learning objective,
{
\small
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eqn:loss_f}
\mathcal{L}= {\alpha_g}\nabla\mathcal{L}_{\text{SDS}}(\hat{\boldsymbol{X}}_{g}, T) + {\alpha_l}\sum_{j=1}^{m} \nabla\mathcal{L}_{\text{SDS}}(\hat{\boldsymbol{X}}_{l,j}, T_{l,j}) + \beta\mathcal{L}_{\text{sparse}},\nonumber
\end{equation}
}where $T$ denotes the global text prompt, and $T_{l}$ is a subset of $T$ representing a single object. $\alpha_{g}, \alpha_{l}$, and $\beta$ are hyperparameters of loss weights.
% $\nabla \mathcal{L}_{\text{SDS}}$ is the score distillation sampling loss, as described in Sec.~\ref{sec:background}.
As suggested in~\cite{metzer2022latent}, we use $L_{\text{sparse}}$ to penalize the binary entropy of local NeRF density and reduce the floating radiance clouds. 
Moreover, it's helpful to include a directional prompt like "front view" or "side view" in our input text similar to previous works (\cite{poole2022dreamfusion,metzer2022latent}) to indicate global camera pose during training.
% Note that the global calibration in the scene frame can adaptively revise both $({C}_l, {\sigma})$ in local NeRF with $\nabla \mathcal{L}_{SDS}$ along with the back-propagating gradient.

\section{Experiments}
% \subsection{Experimental Setup}
% \noindent\textbf{Evaluation Benchmarks.}
% To address the compositional generation, we propose a new benchmark which consists of 6 our pre-defined compositional prompts and 12 natural prompts from MSCOCO~\cite{lin2014microsoft} where each contains at least two color words modifying different objects. We also switch the position of two color words to create a contrast caption similar to ~\cite{feng2022training}.
% For our benchmark, we conduct user studies to evaluate different methods based on user preferences.

% \noindent\textbf{Evaluation Metric.}
% We mainly rely on human evaluations for the visual quality and generation accuracy of compositional prompts.
% We show users two videos side by side rendered from a canonical view by two different algorithms using the same text prompt. We ask the users to select the more realistic and detailed one from Latent NeRF~\cite{metzer2022latent}, SJC~\cite{wang2022score}, and our method, respectively and indicate which 3D models demonstrate better semantic alignment or image fidelity.
% Each prompt is evaluated by 3 different users, resulting in XXX pairwise comparisons.
% We also investigate the object missing rate in our proposed benchmark from human evaluation.
\subsection{Implementation Details}
For score distillation sampling, we use the v1-4 checkpoint of Stable Diffusion based on the latent diffusion model~\cite{rombach2022high}. 
We utilize the code-base~\cite{metzer2022latent} for 3D representation and grid encoder from Instant-NGP~\cite{muller2022instant} as our NeRF model. The global MLP consists 4 or 6 Linear layers with 64 hidden channels.
In the training loss, we set $\alpha_g=100, \alpha_l=50$, and $\beta=5e^{-4}$.
Our 3D scenes are optimized with a batch size of 1 using the Adam~\cite{kingma2014adam} optimizer on a single RTX3090. 
See our \emph{suppl.} for more details.





\subsection{Qualitative Comparison}
In Fig.~\ref{fig:sota}, we present qualitative comparisons of generated 3D assets given the same multi-object text prompt with our proposed CompoNeRF method, as well as the Latent-NeRF and SJC, which are the SoTA methods  based on the same Stable Diffusion model.
We observe that our method can accurately generate complex 3D models across a diverse set of prompts with \textbf{precise object identity} and more \textbf{sensible and fertile context} than others in the showcases.
%
For example, in the simple Case 3, our method accurately generates two separate objects, a red \emph{apple} and a yellow \emph{banana}. In contrast, other methods generate a single mixed object with characteristics of both fruits.
%
In the complex Case 5, we provide a realistic scene with accurate representations of \emph{wine}, \emph{table}, \emph{salad}, \emph{bread}, and \emph{table}, including the glass reflection. However, other methods fail to generate even recognizable objects.
Note that we cannot validate the predicted results of DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d} model as they are built upon on close-sourced diffusion models.
Besides, our underlying 3D representation can also be equipped with most object-centric methods~\cite{poole2022dreamfusion,lin2022magic3d} once they are released to achieve, allowing us to achieve better single-object modeling, similar to the use of our Latent-NeRF backbone.

% CLIP score is a common evaluation metric for text-to-image generation tasks~\cite{parmar2023zero,zhang2023sine,wang2023imagen}, which is a cosine similarity score between text embeddings and image embeddings encoded by CLIP modal~\cite{radford2021learning}. 
\subsection{Quantitative Comparison}
We use CLIP score as our evaluation metric to measure the similarity of generated 3D assets to text prompt. 
The CLIP score, a widely utilized evaluation metric in text-to-image generation tasks~\cite{parmar2023zero,zhang2023sine,wang2023imagen}, is calculated as the cosine similarity between the embeddings of the text and image, both of which are encoded by the CLIP model.
For 3D assets, we calculate the CLIP score between the projected images of the 3D assets and the prompt text in different views and take the average score as the overall CLIP score. In \cref{tb:perclass}, we show quantitive comparison of the similarity between generated 3D assets and the same text prompt with the Latent-NeRF, SJC and our method across diverse scenes. As shown as \cref{tb:perclass}, we achieve the best performance in all scenes, with more significant performance improvement in complex scenes. For example, in \emph{table wine} scene, our method achieves a \textbf{54\%} gain. \textbf{The excellent performance in complex scenes highlights the effectiveness of our global calibration.}

% \noindent\textbf{Local and Global Text Guidance.}
% We conduct ablations to demonstrate the importance of introducing global and local level guidance discussed in Sec.~\ref{sec:optimization}. 
% % We show ablation with different loss settings. 
% In Fig.~\ref{fig:ab_loss}, we observe that our complete method (Ours) improves the generation accuracy of object identity and better geometry.
% Note that, without local-level SDS losses, the object frame of 'banana' in Fig.~\ref{fig:ab_loss} (a) even can not be rendered with any details, while without global-level SDS loss, the local frame of 'banana' in Fig.~\ref{fig:ab_loss} (b) can not generate the 'banana' at accurate number.
% The phenomenon is consistent with our observation that the generative ability of the pre-trained diffusion model may fail to provide accurate guidance for the multi-object text prompt.
% In Fig.~\ref{fig:ab_loss} (c), we also show the perturb and average scoring strategy~\cite{wang2022score} can generate better geometry results against the vanilla SDS losses~\cite{poole2022dreamfusion}.

% \noindent\textbf{Global Calibration.}
% We further study the global calibration by rendering a scene with glass balls made of different materials in Fig.~\ref{fig:ab_loss}.
% The rendered results show that with the global calibration procedure, the light reflection and shadow of all balls have more view consistency.
% While without global calibration, the blue ball renders shadow artifacts that are against scene coherence.

% \subsection{Quantitative evaluation of Compositional Cases}

% Missing Rate

% Failure Cases
% In Fig.~\ref{fig:sd}, we show that for some complex prompt, the Stable Diffusion can not generate accurate information and thus fail to generate faithful images. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/app.pdf}
    % \vspace{-25pt}
    \caption{\textbf{Scene editing results} from manipulations on 3D layout, text prompt and scene recomposition.}
    \label{fig:app}
    % \vspace{-12pt}
\end{figure} 

\section{Discussion}

Our proposed CompoNeRF method represents a preliminary step in handling multi-object text for text-to-3D generation. However, it unleashes its potential by composing scenes with reusable NeRF components, which also facilitates later editing. Nonetheless, we are unable to solve the multi-face problem in this paper since our box layout guidance is ambiguous for prompts with direction. In this section, we discuss these two points.


\noindent\textbf{Editable Scene Rendering and Finetuning.}
Due to the compositional capacity brought by the editable 3D scene layout, we can perform scene editing by text editing, moving, scaling, duplicating, removing the single object, and re-composting a new scene by manipulating the layout of each learned local NeRF.
Fig.~\ref{fig:app} shows our edited scene rendering results based on a pre-trained scene.
We can see that the manipulated objects are seamlessly integrated into the scene while ensuring the correct spatial relationship following the edited layout.
For text editing on a specific object, we simply change a certain part of the text prompt, \eg, 'a vase of sunflower' to 'a vase of rose' at both global and local text prompts and finetune the scene with a few steps.
When moving and scaling existing objects, we only need to adjust the box property, such as the center point and box scale.
As the duplication, removal, and recomposition, we can first input the 3D boxes and then load each box with a local text prompt from a learned local NeRF collection, \eg, copy the single \emph{nightstand} into the other at opposite locations.
Furthermore, all types of manipulation can be combined together to generate a scene with multiple user control inputs.
% Alternatively, we can further finetune the edited scene with a few finetuning steps to improve the view consistency.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/dis_shape.pdf}
    % \vspace{-25pt}
    \caption{\textbf{(Left)} we observe the multi-face problem, \ie, duplicated face views with geometry  collapse in all methods, even in single-object cases. \textbf{(Right)} we provide mesh as guidance instead of box layouts to solve this problem, which further proves our method's versatility and effectiveness.}
    \label{fig:dis_shape}
\end{figure}


\noindent\textbf{Multi-face Problem and Stronger Prompt.}
% The CompoNeRF also has several limitations related to diffusion guidance.
Similar to Latent-NeRF and SJC, the guidance generated from Stable Diffusion may produce a multi-face problem for certain objects as shown in Fig.~\ref{fig:dis_shape}.
The diffusion model can not guarantee to generate satisfactory guidance with the desired direction along with the sampling camera pose.
% In other words, the constraint provided by our 3D layout still can not force the diffusion model to generate satisfactory guidance for each specific object across different rendering views.
An alternative to relieve the multi-face problem is adding stronger constraints to force the 3D representation to maintain geometric consistency.
Our method also uses the mesh constraint, proposed by Latent-NeRF, as a more fine-grained 3D layout than the 3D box.
Fig.~\ref{fig:dis_shape} shows that the multi-face problem can be largely relieved with the more accurate mesh constraint.
However, accurate mesh input requires extensive editing, which reduces practical values during application.
% However, we show that our 3D scene layout can be easily extended to more general types of input prompts.





\section{Conclusion and Future Work}
In this work, we have proposed a multi-object text-guided compositional 3D scene generation framework, called CompoNeRF, based on an editable 3D scene layout.
The 3D scene layout interpreted the multi-object text prompt as a set of local NeRFs binding with a spatial box and object-specific text prompt.
The whole scene view is rendered by compositing local NeRFs defined in the layout.
We also designed composition module and multi-level text guidance for improving the quality of generated 3D scenes.
Despite that our flexible box layout may not be direction-aware that is unable to solve the multi-face problem, we have addressed the guidance collapse issue well in the multi-object scene reconstruction. 
Working with the large-scale Stable Diffusion model, we demonstrated that our method, the first attempt to adopt compositional NeRF design in the text-to-3D task, can generate compelling 3D models with multiple objects, comparing favorably to available concurrent works. 
Finally, we investigated an exciting application of our method for scene editing and reusing trained models for scene recomposition, identifying an avenue for future work.

% Currently, our method relies on the user input 3D layout to render a text-guided scene, which can be further optimized by adopting a learnable dynamic layout to make the compositional pipeline in an end-to-end way. 
% To mitigate the influence of unconstrained generation ability of diffusion model, we can further automatically calibrate the language information for diffusion model to generate more consistent views. 
% Besides, to achieve more realistic scene editing, it is also promising to integrate the scene lighting model into the framework in the future work.

\clearpage
\bibliography{aaai24}

\end{document}
