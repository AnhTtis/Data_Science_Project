\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[percent]{overpic}
\usepackage{caption}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage{booktabs}
\usepackage{color}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{colortbl}  
\usepackage{xcolor}
\usepackage{array}

\definecolor{hollywoodcerise}{rgb}{0.96, 0.0, 0.63}
\definecolor{lasallegreen}{rgb}{0.03, 0.47, 0.19}
\definecolor{hanpurple}{rgb}{0.32, 0.09, 0.98}
\definecolor{green(pigment)}{rgb}{0.0, 0.65, 0.31}
\definecolor{yellow}{rgb}{0.85, 0.85, 0.31}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
%232, 232, 125
 \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\hypersetup{colorlinks,linkcolor={red},citecolor={hollywoodcerise},urlcolor={red}} 
% \captionsetup{font=small}
% Include other packages here, before hyperref.
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\crefname{figure}{Fig.}{Figs.}
\crefname{algorithm}{Alg.}{Algs.}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout}

\author{
% First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
Yiqi Lin$^{1*}$ \quad Haotian Bai$^{1*}$ \quad Sijia Li$^{2}$ \quad Haonan Lu$^{2}$ \quad Xiaodong Lin$^{3}$ \quad Hui Xiong$^{1,4}$ \quad Lin Wang$^{1,4}$~$^{\dag}$\\
$^{1}$AI Thrust, HKUST(Guangzhou) \quad $^{2}$OPPO \quad $^{3}$Rutgers University \quad $^{4}$ Dept. of CSE, HKUST \\
{\tt\small ylin933@connect.hkust-gz.edu.cn \quad 
haotianwhite@outlook.com \quad \{lisijia, luhaonan\}@oppo.com} \\
{\tt\small  
Â lin@business.rutgers.edu  \quad 
\{xionghui, linwang\}@ust.hk}}

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
\centering
\begin{overpic}[width=0.92\linewidth]{figures/teaser.pdf}
\end{overpic}
\vspace{-12pt}
\captionof{figure}{\textbf{Left:}
CompoNeRF generates multi-object 3D scenes based on the editable 3D scene layout that represents each object with a local NeRF associated with its spatial location (\ie, 3D box) and local text prompt.
Also, the 3D scene layout makes the object-compositional scene generation flexibly editable (\eg, scaling, moving, duplication, or re-composition) by manipulating the 3D layout or text prompt.
\textbf{Right}: Our approach produces the most faithful and editable scenes given the multi-object text guidance, while the Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} suffer from missing objects and semantic confusion.
}
\label{fig:teaser}
\end{center}
}]
\let\thefootnote\relax\footnote{*Equal contribution \quad $^{\dag}$ Corresponding author}

%%%%%%%%% ABSTRACT
\begin{abstract} 
Recent research endeavors have shown that combining neural radiance fields (NeRFs) with pre-trained diffusion models holds great potential for text-to-3D generation.
However, a hurdle is that they often encounter guidance collapse when rendering complex scenes from multi-object texts. 
Because the text-to-image diffusion models are inherently unconstrained, making them less competent to accurately associate object semantics with specific 3D structures.
To address this issue, we propose a novel framework, dubbed \textbf{CompoNeRF}, that explicitly incorporates an editable 3D scene layout to provide effective guidance at the single object (\ie, local) and whole scene (\ie, global) levels. 
Firstly, we interpret the multi-object text as an editable 3D scene layout containing multiple local NeRFs associated with the object-specific 3D box coordinates and text prompt, which can be easily collected from users.
Then, we introduce a global MLP to calibrate the compositional latent features from local NeRFs, which surprisingly improves the view consistency across different local NeRFs. 
Lastly, we apply the text guidance on global and local levels through their corresponding views to avoid guidance ambiguity.
This way, our CompoNeRF allows for flexible scene editing and re-composition of trained local NeRFs into a new scene by manipulating the 3D layout or text prompt. Leveraging the open-source Stable Diffusion model, our CompoNeRF can generate faithful and editable text-to-3D results while opening a potential direction for text-guided multi-object composition via the editable 3D scene layout.
% Code is available at \url{https://}.
\end{abstract}

\vspace{-12pt}
%%%%%%%%% BODY TEXT
\section{Introduction}



\vspace{-4pt}
Recently, text-to-image generation~\cite{ho2020denoising,nichol2021improved,rombach2022high} has achieved tremendous success by coupling the vision-language pre-trained models~\cite{radford2021learning,li2022blip} with diffusion models~\cite{ho2020denoising,nichol2021improved,rombach2022high}.
These breakthroughs has also yielded far-reaching implications in text-to-3D generation ~\cite{jain2022zero,sanghi2022clip,hong2022avatarclip,JunGao2022GET3DAG,mohammad2022clip,lee2022understanding,xu2022dream3d} using powerful vision-language pre-trained models. 
More recently, several text-to-3D methods~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} have shown that matching the rendered views from the differential 3D model, such as Neural Radiance Fields (NeRFs)~\cite{mildenhall2020nerf,mip-nerf,muller2022instant}, with the learned text-to-image distribution from pre-trained diffusion model can achieve remarkable results.



However, the textual description is often an abstract specification for a desired target 3D model or a 2D image.
Despite that the powerful diffusion models, \eg, Stable Diffusion~\cite{metzer2022latent}, have been trained on billions of text-image pairs~\cite{schuhmann2022laion}, it is still a challenge to generate geometrically coherent images across different viewpoints from the text.
Moreover, the diffusion model may produce inaccurate results~\cite{feng2022training} given text containing multiple objects, resulting in missing objects or semantic confusion.
For example, Fig.~\ref{fig:intro} demonstrates that the Stable Diffusion fails to maintain the object identities and geometric coherence even with the simple multi-object text.
This obviously contradicts the essence of volume rendering in NeRF, leading to a hurdle~\textemdash \textit{guidance collapse}, especially when rendering complex scenes from multi-object texts.
As a result, the state-of-the-art (SoTA) Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score} models can only generate part of concepts in the multi-object text, as shown in Fig.~\ref{fig:intro}, limiting its application towards object-compositional 3D scene generation from the text prompt.


Therefore, it naturally raises the question:  \textit{whether all the concepts in the multi-object text can be accurately learned and composed from the agnostic distribution of the diffusion model for 3D scene generation.}
As shown in Fig.~\ref{fig:intro}, we observe that the diffusion model can more accurately generate single objects with their respective local text prompts.
This motivates us to introduce more fine-grained text guidance to tackle the guidance collapse issue in existing frameworks~\cite{metzer2022latent,wang2022score} when taking multi-object text prompts.
Thus, a straightforward solution is binding object-oriented guidance for each object into the particular 3D locations, making the 3D representation and rendering pipeline object-aware.
However, existing approaches~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent,wang2022score} tend to encode the entire scene into a single neural network, making it hard to incorporate the decomposed guidance during training as they are generally agnostic to the object's identity.

\begin{figure}[t]
    \centering
    \vspace{-2pt}
    \includegraphics[width=.94\linewidth]{figures/sd.pdf}
    \vspace{-8pt}
    \caption{
    The guidance collapse issue on generating the multi-object scene associated with Stable Diffusion~\cite{rombach2022high}.
    Comparison of our CompoNeRF with Latent-NeRF~\cite{mirzaei2022laterf} and SJC~\cite{wang2022score} shows that using global (scene) and local (object) text guidance can mitigate the such problem.
    }
    \label{fig:intro}
    \vspace{-12pt}
\end{figure}


To overcome these challenges, we propose a compositional NeRF framework, called \textbf{CompoNeRF}, in which the multi-object text guidance is interpreted as an editable 3D scene layout.
As shown in Fig.~\ref{fig:teaser}, the scene layout collects individual object entities from the input text and gathers their pre-defined coarse 3D bounding box information, \eg, box coordinates.
In CompoNeRF, each box in the 3D scene layout is modeled by a local NeRF for representation learning, and the complete scene (\ie, global) views are rendered by compositing all the learned 3D representations from local NeRFs.
Nevertheless, the direct composition from all local NeRFs may not ensure learning coherent global views without tackling the two following issues.

\begin{table*}[t!]
\centering
\resizebox{\linewidth}{!}{ 
    \begin{tabular}{l|c|c|c|c|c|c}
    \toprule
    \textbf{Methods} & \textbf{Diffusion Model} & \textbf{\textbf{3D Representation}} & \textbf{Scene Rendering} & \textbf{Input Prompt} & \textbf{Scene Editing} & \textbf{Re-composition}\\  \hline
    DreamFusion~\cite{poole2022dreamfusion} & Imagen~~\cite{sahariaphotorealistic} & Mip-NeRF 360~\cite{barron2022mip} & Object-centric & Text & T & \xmark\\
    Magic3D~\cite{lin2022magic3d} & eDiff-I~\cite{balaji2022ediffi} + SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-centric & Text & T & \xmark\\
    SJC~\cite{rombach2022high} & SD~\cite{rombach2022high} & voxel radiance field & Object-centric & Text & T & \xmark\\ \hline
    Latent-NeRF~\cite{metzer2022latent} & SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-centric & Text+Fine Shape & T & \xmark\\
    Ours & SD~\cite{rombach2022high} & Instant-NGP~\cite{muller2022instant} & Object-compostional & Text+3D Coarse Boxes & T/M/S/R & \cmark \\ \bottomrule
    \end{tabular}
}
\vspace{-7pt}
\caption{Comparison of our method and the related works for text-to-image generation. SD denotes Stable Diffusion. For scene editing, we use T(editing object with text), M(moving object), S(scaling object), and R(removing object) for short.}
\vspace{-8pt}
\label{tab:compare}
\end{table*}


Firstly, the global view consistency is hard to capture across multiple local NeRFs as the none shared parameters.
Therefore, we use a global MLP to calibrate the local NeRF predictions by leveraging samples' global coordinates and ray's directions as conditional input.
The model can gradually learn global consistency across multiple objects by passing the global information through the same global MLP to local NeRFs.
Secondly, certain objects in the scene may be fully occluded in the training data due to random camera positions, leading to inaccurate text guidance. 
To tackle the occlusion issue, for each local NeRF, we separately apply local text guidance on the locally rendered view, as the diffusion model can provide more accurate guidance to shape the object identity despite occlusions.
As a result, our approach ensures coherent and realistic 3D generation, even in crowded scenes, 
as demonstrated in Fig.~\ref{fig:teaser}, without missing objects or ambiguity.
Moreover, the 3D scene layout facilitates object editing by allowing users to modify text and manipulate objects flexibly, including moving, scaling, and duplicating.
Given a vast pre-trained content gallery, users can rapidly generate their desired 3D scenes using text prompts and 3D scene layouts, thereby democratizing 3D content creation.
The comparisons of editing approaches are summarized in Tab.~\ref{tab:compare}.






To summarize, our paper makes three key contributions:  (\textbf{I})  We address the guidance collapse issue in multi-object 3D scene generation by integrating an editable 3D layout with multiple local NeRFs to precisely associate guidance for specific structures.
(\textbf{II}) We tackle the global consistency and occlusion issue by introducing a global MLP to calibrate the global scene color and different levels of text guidance to maintain the identity of objects while learning the global coherence for individual entities. 
(\textbf{III}) We thoroughly evaluate the effectiveness of our proposed method across various multi-object scenarios, demonstrating its ability to generate 3D scenes compositionally and offer flexible editing capabilities.




\vspace{-4pt}
\section{Related Work}
\vspace{-4pt}
\noindent\textbf{Text-guided 3D Generative Models.}
Motivated by the success of the vision-language models,\eg,~CLIP~\cite{radford2021learning}, text-guided 3D generation has also been rapidly progressing.
Several works~\cite{jain2022zero,wang2022clip,mohammad2022clip,sanghi2022clip,xu2022dream3d} utilize the pre-trained vision-language model to provide robust alignment between image features extracted from rendered views of 3D representations and text features by minimizing feature similarity.
Though pre-trained large-scale vision-language models can offer strong 2D guidance for 3D representation learning, the quality of their 2D rendering results may be less realistic due to insufficient details in the features.
Recently, DreamFusion~\cite{poole2022dreamfusion} demonstrates remarkable capability in text-to-3D object generation by incorporating a powerful pre-trained text-to-image diffusion model~\cite{saharia2022photorealistic} through score distillation sampling.
Similarly, Magic3D~\cite{lin2022magic3d} proposes a two-stage super-resolution method to enhance generation quality with hybrid 3D representation.
Latent-NeRF~\cite{metzer2022latent} demonstrates that updating the NeRF in the latent space using score distillation sampling also can produce realistic results. 
SJC~\cite{wang2022score} improves the sampling process by introducing a perturb-and-average scoring scheme to address distribution mismatching issues.


By contrast, our CompoNeRF generates multi-object 3D scenes in an \textit{object-compositional} way by utilizing multiple local NeRFs to model the scene instead of a holistic object-centric representation.
Our editable 3D scene layout allows for editing of scenes by changing the text prompt similarly as ~\cite{poole2022dreamfusion, lin2022magic3d}. Also, it enables manipulation of the spatial arrangement of individual objects in a crowded scene, as summarized in Tab.~\ref{tab:compare}.
Moreover, the layout allows for re-composition with other off-the-shelf representations, enabling the rapid generation of new scenes.

\noindent\textbf{Neural Rendering for 3D Modeling.}
The rapid advancement of NeRF has greatly improved the performance of neural renderers.
NeRF models~\cite{nerf,lindell2021autoint,muller2022instant,Liu2020NeuralSV,mip-nerf,ref-nerf,KaiZhang2020NeRFAA} are a family of volume rendering algorithms that utilizes coordinate-based MLPs to directly predict color and opacity from the 3D position and 2D viewing direction. 
The photo-realistic synthesized views from these models have led to the widespread adoption of differential volume rendering in various applications, such as relighting~\cite{srinivasan2021nerv,zhang2021nerfactor}, dynamic scene reconstruction~\cite{gao2021dynamic,pumarola2021d,xian2021space,tretschk2021non}, editable scenes and avatars~\cite{liu2021neural,yang2021learning}, and surface reconstruction~\cite{azinovic2022neural,wang2021neus}.
Most NeRF applications commonly use \textit{single} multi-layer perceptron to encode the entire scene into parameters, which can be ambiguous \wrt the specific object identities within the scene.
Note that our method involves rendering the scene using multiple local NeRFs, and the rendering process requires compositing all the local NeRF predictions based on their spatial relationships.

\noindent\textbf{Object-Compositional Scene Modeling.}
The idea of generating new scenes by compositing multiple object-centric representations is a straightforward approach in scene generation~\cite{guo2020object}.
Several works~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe,ost2021neural,xu2022discoscene,song2022towards} attempt to directly decompose object representations from the scene image to perform compositional scene modeling. Based on the additional object-level information, they can be roughly categorized as semantic based~\cite{zhi2021place,yang2021learning,wu2022object,mirzaei2022laterf,niemeyer2021giraffe} and 3D layout based~\cite{ost2021neural,xu2022discoscene,song2022towards}.
Semantic based methods incorporate extra semantic information, such as segmentation labels~\cite{zhi2021place}, object instance mask~\cite{yang2021learning,wu2022object}, and features from the pre-trained vision-language model~\cite{mirzaei2022laterf}, to learn the object representations.
On the other hand, 3D layout based methods directly use the 3D object coordinate information as the object guidance for learning object-specific representation and generating the full scene by compositing all object representations.
For instance, NSG~\cite{ost2021neural} and its variant~\cite{song2022towards} use a scene graph structure to model dynamic scenes, associating each object with a 3D box and a node representation.
While previous works focused on decomposing object entities from real-world images, our work serves as the \textit{\textbf{first}} attempt to tackle the text-to-3D generation via decomposed representations with 3D scene layout.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/method.pdf}
    \vspace{-12pt}
    \caption{\textbf{Framework Overview}.
    CompoNeRF consists of three parts: 1). The editable 3D scene layout configures the scene representations with 3D boxes and text prompts; 2).  The scene rendering includes the global calibration and the compositional process; 3). The joint optimization applies global and local text guidance on global and local render views.
    }
    \label{fig:framework}
    \vspace{-10pt}
\end{figure*}



\vspace{-4pt}
\section{Methodology}
\vspace{-4pt}

\subsection{Overview}
Fig.~\ref{fig:framework} illustrates our pipeline, which consists of three main components, including the editable 3D scene layout based on multi-object text (Sec.~\ref{ssec:layout}), the scene rendering pipeline that composites the predictions from all local NeRFs (Sec.~\ref{ssec:render}), and the joint optimization on both local and global representation models (Sec.~\ref{sec:optimization}).
To elaborate, our editable 3D scene layout represents a global frame of the scene by decomposing it into a set of local frames, where each is parameterized by a local NeRF, a 3D bounding box, and a corresponding local text prompt.
For instance, the text prompt `A teddy bear and a stuffed monkey sit side by side' is interpreted as a 3D scene layout, as shown in Fig.~\ref{fig:framework}.  
The whole 3D layout, \ie, scene frame, consists of two 3D bounding boxes, \ie local frames \#1 and \#2, with specific local text prompts, \ie, `a teddy bear' and `a stuffed monkey'. 
%
To render the scene view, we first calculate the ray-box intersections between the boxes and rays $({\boldsymbol{r}}_o, \boldsymbol{\phi}_d, {\boldsymbol{\theta}}_d)$, where the ${\boldsymbol{r}}_o$ is the ray origin and the $({\boldsymbol{r}}_o, \boldsymbol{\phi}_d)$ is its direction.
Then, to infer each object's properties in local NeRFs, we sample the global points $({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g)$ in the global frame within the ray-box intersection intervals and project them into the normalized local location $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$ in the local frame.
%
Given the local sampling points $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$, the implicit local NeRF ${\boldsymbol{\theta}}_l$ outputs four pseudo-color channels ${\boldsymbol{C}}_l$ and density $\boldsymbol{\sigma}$, which can be used to render a local view of the local frame to match its local text prompt.
%
We further calibrate the predicted pseudo-color $\boldsymbol{C}_l$ from local frames by adding the global embeddings ${\boldsymbol{emb}}_g$ to improve the global view consistency.
Then, the calibrated predictions after composition are used to reconstruct the scene view by volumetric rendering along the rays.
%
Lastly, the rendered views based on local and global frames are guided by score distillation sampling loss $\nabla \mathcal{L}_{\text{SDS}}$~\cite{poole2022dreamfusion} to optimize all the learnable parameters. 

\vspace{-4pt}
\subsection{Preliminaries}
\label{sec:background}
\vspace{-4pt}
\noindent \textbf{3D Representation in Latent Space.}
Our approach is built upon the SoTA text-to-image model\textemdash Stable Diffusion~\cite{rombach2022high}.
To avoid heavy computation in the pixel space, we follow the Latent-NeRF~\cite{metzer2022latent} to model each object with an independent local NeRF $\boldsymbol{\theta}_{l}$ that outputs four pseudo-color channels ${\boldsymbol{C}}$, corresponding to the four latent features that Stable Diffusion operates over, and a volume density ${\boldsymbol{\sigma}}$. 
Specifically, the representation maps a point $\left({x}_l, {y}_l, {z}_l\right)\in [-1, 1]$ in the local frame to its corresponding volumetric density ${\sigma}$ and emitted color $\boldsymbol{C}_l$ to the latent features, \ie,  $\left[\boldsymbol{C}_l, {\sigma}\right]=\boldsymbol{\theta}_{_l}\left({x_l}, {y}_l, {z}_l\right)$.
The predicted pseudo-color is fed forward into the decoder of Stable Diffusion models to obtain the final rendering result.

\noindent \textbf{Score Distillation Sampling.}
To achieve text-to-3D generation, DreamFusion~\cite{poole2022dreamfusion} introduces Score Distillation Sampling (SDS) to propagate the text-to-image generative prior from diffusion model $\phi$ to the NeRF parameters $\boldsymbol{\theta}$.
During the SDS process, a noise image $\boldsymbol{X}_t$ is first generated by adding a sampled noise $\epsilon \sim \mathcal{N}(0, I)$ in noise level $t$ into a rendered view $\boldsymbol{X}$ from a NeRF.
Then, the diffusion model $\phi$ predicts the sampled noise $\epsilon_\phi\left(\boldsymbol{X}_t, t, T\right)$ given the noisy image $\boldsymbol{X}_t$, noise level $t$, and optional text prompt $T$.
Specifically, SDS computes the gradient from the difference between the predicted and added noises,
{\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eq:sds_loss}
\nabla_\theta \mathcal{L}_{\text{SDS}}(\boldsymbol{X}_t, T)=  w(t)\left(\epsilon_\phi\left(\boldsymbol{X}_t, t, T\right)-\epsilon\right),
\end{equation}}where $w(t)$ is a weighting function. 
The gradient direction generated on all the rendered views is used to update $\boldsymbol{\theta}$ to produce images that match the conditioned text prompt under diffusion prior.
We also follow the SJC~\cite{wang2022score} to apply the perturb and average scoring into the SDS process.
Please refer to \cite{poole2022dreamfusion,wang2022score} for the complete details.

\vspace{-4pt}
\subsection{Editable 3D Scene Layout}
\label{ssec:layout}
\vspace{-4pt}
The 3D scene layout explicitly combines language structures with 3D layouts in an editable way.
Given the input text prompt $T$, the attribute-object pairs can be easily obtained based on user control.
Note that the text prompt indicates the multi-object text prompt by default.
As shown in Fig.~\ref{fig:framework}, we can extract multiple noun phrases with their binding attributes and map these local text prompts into corresponding regions.
Specifically, we define the scene structure with $m$ local frames, each employs a local NeRF $\boldsymbol{\theta}_l$ as representation, the local text prompt $T_{l} \subseteq{T}$ and its spatial layout with 3D boxes $\mathbf{b} = \{\mathbf{p}, \mathbf{s}\} \in  \mathbb{R}^6$ of each object entity, where $\mathbf{p}=\{p_x, p_y, p_z\}$ refers to the center point and $\mathbf{s}=\{s_x, s_y, s_z\}$ denotes the box scale. 
\textit{Our editable 3D layout is easy to be collected and edited with its simplicity, allowing for versatile and interactive user control by modifying the box's or text's properties to define a new scene}.
Moreover, as depicted in Fig.~\ref{fig:teaser}, each component in a 3D scene layout can be replaced or re-composited with other trained local NeRFs, which is more friendly for flexible user editions compared with using only text prompts.

\vspace{-4pt}
\subsection{Scene Rendering Pipeline}
\label{ssec:render}
\vspace{-4pt}
In CompoNeRF, the scene images are rendered by a ray-casting approach following the design of NeRF.
The camera is defined by a pinhole camera model, casting a set of rays $(\boldsymbol{r}_o, \boldsymbol{\phi}_d, {\boldsymbol{\theta}}_d)=\boldsymbol{o}+t\boldsymbol{d}$ through each pixel on the frame of size $H \times W$, where the $\boldsymbol{r}_o \in  \mathbb{R}^3$ is the origin and the $(\boldsymbol{\phi}_d, \boldsymbol{\theta}_d)$ is the viewing direction.
Along this ray, we sample all the points intersected with any layout box of local frames.
For each hit sampled point, the color and volumetric density are computed through the local NeRF of the hit local frame.
The ray color perdition is calculated by the differentiable integration applied on all the point-predicted colors and volumetric density along the ray.


\noindent \textbf{Ray-box Intersection with Local Frames.}
Given a ray $\boldsymbol{r}_i$, each box $\boldsymbol{b}_j$ of the local frame is applied with the AABB ray intersection test algorithm to check the intersections.
When the ray $r_i$ is hit with a box $\boldsymbol{b}_j$ of the local frame, we use the entrance and exit points as near $\boldsymbol{t}_{in}$ and far $\boldsymbol{t}_{out}$ bounds to sample $N$ equidistant quadrature points, $
\boldsymbol{t}_{i,j,n}=\frac{n-1}{N-1}\left(\boldsymbol{t}_{out}-\boldsymbol{t}_{in}\right)+\boldsymbol{t}_{in} , n \in \left[1, N\right]$
Note that the coordinates of sampled points are first projected into normalized coordinates using the box scale of local frames to enable each local NeRF to learn the scale-independent representation.
The bounding box $\mathbf{b}$ of the local frame in global coordinate can be transformed into a canonical bounding box by ${(\mathbf{b}} - \boldsymbol{p}) / \mathbf{s}$.
Considering the rendering efficiency, we only calculate the valid points, interacted with the boxes, and set all the empty points with a constant background color.



\noindent\textbf{Global Calibration.}
The 3D boxes are only used for the spatial configuration of local NeRFs, while the implicit representation of local NeRFs is inferred by the canonical samples inside the local frame without considering the global relationship across different objects.
To relieve such location-dependent effects, we further calibrate the output color and density from the local NeRF with global coordinates $({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g)$ and ray directions $\left({\boldsymbol{\phi}}_{d}, {\boldsymbol{\theta}}_{d}\right)$ as the conditional input.
%
%
Specifically, we adopt a shared MLP $\boldsymbol{\theta}_{g}$ to calibrate all the predicted object colors, that is,
{\setlength\abovedisplayskip{4.5pt}
\setlength\belowdisplayskip{4.5pt}
\begin{align}
\label{eq:MLP_dyn_2}
{\boldsymbol{C}_g} = {\boldsymbol{C}_l} + \boldsymbol{emb}_{g} &= {\boldsymbol{C}_l} + \boldsymbol{\theta}_{g}({\boldsymbol{x}}_g, {\boldsymbol{y}}_g, {\boldsymbol{z}}_g, {\boldsymbol{\phi}}_{d}, {\boldsymbol{\theta}}_{d}),
\end{align}}
where ${\boldsymbol{C}_l}$ is the color predicted by the local NeRF.
Therefore, the scene color can preserve the view-consistent behavior from the original architecture and add consistency across poses for the volumetric density.
Since the color and density values share the same latent expression in $({\boldsymbol{x}}_l, {\boldsymbol{y}}_l, {\boldsymbol{z}}_l)$, we only calibrate the emitted scene color explicitly with the scene location, as the densities of local NeRFs also are implicitly adjusted during optimization.

\noindent \textbf{Global and Local Volumetric Rendering.}
After compositing all the interacted points, each ray $\boldsymbol{r}_i$ collects a set sampling points by $\{\boldsymbol{t}_{i,j,n} \}_{j=1, n=1}^{m_j, N}$, where $m_j$ is the number of the hit object.
For each sampling point, the inference results with the respective 3D representations are the local color $\boldsymbol{c}_{l}$, global color $\boldsymbol{c}_{g}$, and density $\sigma$.
Considering the object occlusions along the ray, we sort the predicted results according to the depth values, \ie, the distances to the camera. 
Then, the global color $\hat{\boldsymbol{C}}_g$ of ray is calculated by the volumetric rendering equation,
{\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\begin{aligned}
& {\hat{\boldsymbol{C}}_g}({r})=\sum_{k=1}^{m_j*N} T_k \left(1-\exp \left(-\sigma_k \delta_k\right) \right) {\boldsymbol{C}}_{g,k},  \\ &  \text { where }
T_k=\exp \left(-\sum_{d=1}^{k-1} \sigma_d \delta_d\right),
\end{aligned}
\end{equation}}
where $\delta$ is the distance between adjacent sampled points.
For each local NeRF $\theta_j$, we also render the local color ${\hat{\boldsymbol{C}}_l}$ of a hit ray $r_i$ from sampled points $\{t_{i,j,n} \}_{n=1}^{N}$ by,
{\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\begin{aligned}
& {\hat{\boldsymbol{C}}_l}({r})=\sum_{k=1}^{N} T_k \left(1-\exp \left(-\sigma_k \delta_k\right) \right) {\boldsymbol{C}}_{l,k}.
\end{aligned}
\end{equation}}
In fact, each local frame only has a small number of hit rays compared to the scene.
Despite the fact that parts of rays are skipped, we observe that it is enough to represent each object accurately while maintaining short rendering times.

\vspace{-4pt}
\subsection{Joint Optimization}
\label{sec:optimization}
\vspace{-4pt}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/sota.pdf}
    \vspace{-12pt}
    \caption{Qualitative comparison with other text-to-3D methods using multi-object text prompts.}
    \label{fig:sota}
    \vspace{-5pt}
\end{figure*}

For each scene described by the multi-object text prompt $T$, we optimize $m$ local NeRFs $\{\boldsymbol{\theta}_l\}_{l=1}^{m}$ according to the layout boxes and a global MLP $\boldsymbol{\theta}_g$ for global calibration.
To enhance the guidance of local representations, we use the local text prompt $T_l \subseteq T$ of single object to optimize the local NeRFs in local views, as shown in Fig.~\ref{fig:framework}.
The scene views $\hat{\boldsymbol{X}}_g=\{\hat{\boldsymbol{C}}_{g,i}\}_{i=1}^{H\times W}$ is obtained from the predicted pixel values of $H \times W$ rays by compositing all the ray-box interaction values.
Similarly, the rendered view $\hat{\boldsymbol{X}}_{l,j}$ of the local frame $\boldsymbol{\theta}_j$ without compositing other objects can be calculated by $\hat{\boldsymbol{C}}_{l,j}$, as depicted in Sec.~\ref{ssec:render}.
We use the local color instead of the globally calibrated color to obtain a local view because the local NeRF should learn the object identity unrelated to its placed position, as the position can be different during user edition.
Formally, we employ the following loss as the learning objective,
{
\small
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
\label{eqn:loss_f}
\mathcal{L}= {\alpha_g}\nabla\mathcal{L}_{\text{SDS}}(\hat{\boldsymbol{X}}_{g}, T) + {\alpha_l}\sum_{j=1}^{m} \nabla\mathcal{L}_{\text{SDS}}(\hat{\boldsymbol{X}}_{l,j}, T_{l,j}) + \beta\mathcal{L}_{\text{sparse}},\nonumber
\end{equation}
}where $T$ denotes the global text prompt, the local   text prompt $T_{l}$ is a subset of $T$ with only the single object.
The $\alpha_{g}, \alpha_{l}$, and $\beta$ are the hypeparameters of the loss weights. 
$\nabla \mathcal{L}_{\text{SDS}}$ is the score distillation sampling loss, as described in Sec.~\ref{sec:background}.
$L_{\text {sparse}}$ suggested in~\cite{metzer2022latent} penalizes the binary entropy of local NeRF density to reduce the floating radiance clouds.
For the different levels of text prompts, we both add the directional text prompt (\eg, ``front view", ``side view" regarding the global camera pose during training) to the input text prompt similar to ~\cite{poole2022dreamfusion,metzer2022latent}.


\vspace{-2pt}

\section{Experiments}

\vspace{-2pt}

\subsection{Implementation Details.}
\vspace{-2pt}
For score distillation sampling, we use the v1-4 checkpoint of Stable Diffusion based on latent diffusion model~\cite{rombach2022high}.
For 3D representation, we use the code-base provided by~\cite{metzer2022latent}, with the grid encoder from Instant-NGP~\cite{muller2022instant} as our NeRF model and a global MLP consists 6 Linear layers with 64 hidden channels.
In the training loss, we set $\alpha_g=100, \alpha_l=50$, and $\beta=5e^{-4}$.
Our 3D scenes are optimized with a batch size of 1 using the Adam~\cite{kingma2014adam} optimizer on a single RTX3090. 
Please refer to appendix for more details.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/ab_loss.pdf}
    \vspace{-8pt}
    \caption{Ablation study on text guidance. (a) without local SDS losses. (b) without global SDS losses. (c) vanilla SDS losses without perturb and average scoring~\cite{wang2022score}. (d) full model.}
    \label{fig:ab_loss}
    \vspace{-12pt}
\end{figure*} 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/ab_calibration.pdf}
    \vspace{-18pt}
    \caption{Ablation study on global calibration. (a) without global calibration. (b) full model.}
    \label{fig:ab_calibration}
    \vspace{-12pt}
\end{figure} 

\vspace{-4pt}
\subsection{Qualitative Comparison.}
\vspace{-4pt}
In Fig.~\ref{fig:sota}, we show qualitative comparisons of generated 3D assets given the same multi-object text prompt with the Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score}, which are the SoTAs based on the same Stable Diffusion model.
We observe that our method can accurately generate complex 3D models over a diverse set of prompts with more accurate object identity and more sensible structure than compared methods in the showcases.
Note that we cannot validate the predicted results of DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d} model as they are built upon on close-sourced diffusion models.
Besides, our underlying 3D representation can also be equipped with most object-centric methods~\cite{poole2022dreamfusion,lin2022magic3d} once they are released to achieve better single-object modeling, the same as the Latent-NeRF backbone used in our CompoNeRF.




\vspace{-2pt}
\subsection{Ablation Study}
\vspace{-2pt}
\noindent\textbf{Local and Global Text Guidance.}
We conduct ablations to demonstrate the importance of introducing global and local level guidance discussed in Sec.~\ref{sec:optimization}. 
% We show ablation with different loss settings. 
In Fig.~\ref{fig:ab_loss}, we observe that our complete method (Ours) improves the generation accuracy of object identity and better geometry.
Note that, without local-level SDS losses, the object frame of 'banana' in Fig.~\ref{fig:ab_loss} (a) even can not be rendered with any details, while without global-level SDS loss, the local frame of 'banana' in Fig.~\ref{fig:ab_loss} (b) can not generate the 'banana' at accurate number.
The phenomenon is consistent with our observation that the generative ability of the pre-trained diffusion model may fail to provide accurate guidance for the multi-object text prompt.
In Fig.~\ref{fig:ab_loss} (c), we also show the perturb and average scoring strategy~\cite{wang2022score} can generate better geometry results against the vanilla SDS losses~\cite{poole2022dreamfusion}.

\noindent\textbf{Global Calibration.}
We further study the global calibration by rendering a scene with glass balls made of different materials in Fig.~\ref{fig:ab_loss}.
The rendered results show that with the global calibration procedure, the light reflection and shadow of all balls have more view consistency.
While without global calibration, the blue ball renders shadow artifacts that are against scene coherence.



\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/app.pdf}
    \vspace{-18pt}
    \caption{Scene editing results from various types of manipulation on 3D layout, text prompt and scene re-composition}
    \label{fig:app}
    \vspace{-12pt}
\end{figure} 

\vspace{-2pt}
\subsection{Editable Scene Rendering and Finetuning.}
\vspace{-2pt}

Due to the compositional capacity brought by the editable 3D scene layout, we can perform scene editing by text editing, moving, scaling, duplicating, removing the single object, and re-composting a new scene by manipulating the layout of each learned local NeRF.
Fig.~\ref{fig:app} shows our edited scene rendering results based on a readily well-trained scene.
We can see that the manipulated objects are seamlessly integrated into the scene while ensuring the correct spatial relationship following the user control 3D layout.
For text editing on a specific object, we simply change a certain part of the text prompt, \eg, 'a red apple' to 'a green apple,' at both global and local levels and finetune the scene with a few steps.
When moving and scaling existing objects, we only need to adjust the box property, such as the center point and box scale.
As the duplication, removal, and re-composition, the user can first input the 3D boxes arrangement and then load each box with a local text prompt from a learned local NeRF collection, \eg, copy the single red apple box into four boxes at different locations.
Furthermore, all types of manipulation can be combined together to generate a scene with multiple user control inputs.
Alternatively, we can further finetune the edited scene with a few finetuning steps to improve the view consistency.



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/dis_shape.pdf}
    \vspace{-18pt}
    \caption{Multi-face problem and more results with different prompt.
    Note that even for the single object case, all of Latent-NeRF~\cite{metzer2022latent}, SJC~\cite{wang2022score}, and ours suffer from different extent of multi-face problem.}
    \label{fig:dis_shape}
\end{figure}

\vspace{-4pt}
\section{Discussion}
\vspace{-3pt}
Our presented CompoNeRF is yet a preliminary step in handling multi-object text for the text-to-3D generation problem.
Still, the generation quality is mainly decided by the behaviors of diffusion models.
The CompoNeRF also has several limitations related to diffusion guidance.

\noindent\textbf{Multi-face Problem and Stronger Prompt.}
Similar to most works, Latent-NeRF~\cite{metzer2022latent} and SJC~\cite{wang2022score}, the guidance generated from Stable Diffusion may produce a multi-face problem for certain objects as shown in Fig.~\ref{fig:dis_shape}.
The diffusion model can not guarantee to generate satisfactory guidance with the desired direction along with the sampling camera pose.
One alternative to relieve the multi-face problem is adding stronger constraints to force the 3D representation to maintain geometric consistency.
Our methods also use the mesh constraint, proposed in Latent-NeRF~\cite{metzer2022latent}, as a more fine-grained 3D layout than the 3D box.
Fig.~\ref{fig:dis_shape} shows that the multi-face problem can be largely relieved with the more accurate mesh constraint.
However, the accurate mesh input requires extensive user interaction, which reduces the practical values during application.
Nevertheless, we show that our 3D scene layout can be easily extended to more general types of input prompts.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/dis_pose.pdf}
    \vspace{-2pt}
    \caption{Results of using different training guidance resolutions by scaling the global frames.
    The first row is the rendering results, and the second row visualizes the rays that hit with local frame for calculating the text guidance.}
    \label{fig:dis_pose}
\end{figure}


\noindent \textbf{Resolution of Diffusion Model Guidance.}
Another issue from our framework design is that the guidance resolution of the whole scene and each object identity need to be well balanced.
When a single object is placed in a large scene, the rendered view becomes relatively smaller, resulting in a smaller number of pixels that can receive training guidance.
Fig.~\ref{fig:dis_pose} shows the results using the same text prompt while different scales of the global frames vary from 0.3 to 0.9.
It indicates that more rays interacting with the local frame can learn a better local NeRF, which is important for large scene rendering as multiple local frames are arranged in the same space while keeping relative size.
It is noted that we optimize the scene model in the latent space of the Stable Diffusion model, in which the feature resolution is $64\times 64$.
Therefore, the CompoNeRF needs to trade off the computation efficiency and rendered results quality in the 3D scene layout generation.

\vspace{-3pt}
\section{Conclusion}
\vspace{-3pt}
We proposed a multi-object text-guided compositional 3D scene generation framework, called CompoNeRF, based on an editable 3D scene layout.
The 3D scene layout interpreted the multi-object text prompt as a set of local NeRFs binding with a spatial 3D box and object-specific local text prompt.
The whole scene view is rendered by compositing all the local NeRFs defined in the layout.
We also designed global calibration and multi-level text guidance mechanisms for improving the quality of the generated 3D scene. 
Working with the large-scale Stable Diffusion model, we demonstrate that our approach can generate compelling 3D models with multiple objects, comparing favorably to available concurrent work. 
Finally, we investigate an exciting application of our framework for scene editing and reusing trained models for scene re-composition, identifying an avenue for future work.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
