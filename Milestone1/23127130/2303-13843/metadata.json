{
    "arxiv_id": "2303.13843",
    "paper_title": "CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout",
    "authors": [
        "Haotian Bai",
        "Yuanhuiyi Lyu",
        "Lutao Jiang",
        "Sijia Li",
        "Haonan Lu",
        "Xiaodong Lin",
        "Lin Wang"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2024-09-24"
    ],
    "latest_version": 4,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Text-to-3D form plays a crucial role in creating editable 3D scenes for AR/VR. Recent advances have shown promise in merging neural radiance fields (NeRFs) with pre-trained diffusion models for text-to-3D object generation. However, one enduring challenge is their inadequate capability to accurately parse and regenerate consistent multi-object environments. Specifically, these models encounter difficulties in accurately representing quantity and style prompted by multi-object texts, often resulting in a collapse of the rendering fidelity that fails to match the semantic intricacies. Moreover, amalgamating these elements into a coherent 3D scene is a substantial challenge, stemming from generic distribution inherent in diffusion models. To tackle the issue of 'guidance collapse' and further enhance scene consistency, we propose a novel framework, dubbed CompoNeRF, by integrating an editable 3D scene layout with object-specific and scene-wide guidance mechanisms. It initiates by interpreting a complex text into the layout populated with multiple NeRFs, each paired with a corresponding subtext prompt for precise object depiction. Next, a tailored composition module seamlessly blends these NeRFs, promoting consistency, while the dual-level text guidance reduces ambiguity and boosts accuracy. Noticeably, our composition design permits decomposition. This enables flexible scene editing and recomposition into new scenes based on the edited layout or text prompts. Utilizing the open-source Stable Diffusion model, CompoNeRF generates multi-object scenes with high fidelity. Remarkably, our framework achieves up to a \\textbf{54\\%} improvement by the multi-view CLIP score metric. Our user study indicates that our method has significantly improved semantic accuracy, multi-view consistency, and individual recognizability for multi-object scene generation.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13843v1",
        "http://arxiv.org/pdf/2303.13843v2",
        "http://arxiv.org/pdf/2303.13843v3",
        "http://arxiv.org/pdf/2303.13843v4"
    ],
    "publication_venue": null
}