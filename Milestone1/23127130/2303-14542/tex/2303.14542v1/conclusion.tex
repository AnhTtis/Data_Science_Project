\section{Conclusion and Future Work} \label{sec:conclusion}
In this paper, we focused on automatically generating documentation-specific code examples, a new domain in software engineering (SE) research. Initially, we investigated the feasibility and effectiveness of pre-trained model (Codex) for this task. We used source code and documentation as the input of Codex to generate code examples. Codex could generate code examples with 72.5\% passability and 82.5\% relevance with very basic setting of zero-shot learning. We also find that using error logs can be useful to further improve the passability of the examples that failed in the generation phase. Hence, this study sets up the foundation for future researches in this direction. In future, we intend to conduct more in-depth evaluation of our proposed approach. We plan our future studies from three different aspects- \textbf{1) Data:} Since the main goal of this study was to primarily verify our approach, we used a small set of data i.e., 40 Python methods and documentation. We need to study larger set of data coming from various sources and languages in the future. \textbf{2) Model:} \textit{M1.} We limited this study to zero-shot learning only. We will extend the investigation by also analyzing one/few-shot learning. \textit{M2.} Pre-trained transformer models (like GPT-3) also support fine-tuning. However, latest versions of GPT-3 models (e.g., Codex) are not available for fine-tuning. We could fine-tune GPT-3 when it is available to do so. \textit{M3.} We will investigate the effect of different parameters (associated with GPT-3) on the quality of generated examples which is not done in this study. \textit{M4.} We will leverage relevant external knowledge retrieved from online forums (such as Stack Overflow, GitHub) to improve the generated examples. \textbf{3) Evaluation:} \textit{E1.} We only used manual evaluation metrics (e.g., passability, relevance) in this study. We plan to use automatic metrics (like BLEU score, Pass@\textit{k}) in the future which would facilitate the evaluation of large scale data. However, for that we will also need a benchmark of input-output pairs (e.g., source-code, documentation$\rightarrow$code examples). \textit{E2.} We will conduct a user study (e.g., developers' survey) to evaluate the usefulness of the generated examples.


\nd\textbf{Acknowledgement.} This work was funded by Natural Sciences and Engineering Research Council of Canada, University of Calgary, Alberta Innovates, and Alberta Graduate Excellence Scholarship.