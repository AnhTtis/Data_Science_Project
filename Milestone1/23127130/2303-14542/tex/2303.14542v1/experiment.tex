% 1. intro te ekta flowchart dibo rq1 and rq2 combine kore something like:

% Method Prototype+Documetation+[External Resources] ->Gpt-3 -> Code Example for Documentation
% 2. RQ1 and RQ2 er approach e ASE paper er moto sample prompt format er ekta kore pic dibo.

% 3. RQ1 and RQ2 er Observation e partial evaluation er description and finding dibo. Ar ekta kore generated code example er  udahoron dibo.

\section{Experiment Setup}\label{subsec:model_setup}


%An overview of our approach is shown at Figure \ref{fig:workflow_doc2code}. We first employ a pre-trained model to generate code examples from corresponding source-code and documentation of any code units (e.g., method).   


% A schematic overview of our approach is shown at Figure \ref{fig:workflow_doc2code} and are described below.

% \begin{figure}[t]
%   \centering
%   %\vspace{-6mm}
%   %\hspace*{-.7cm}%
%   %%%\includegraphics[scale=.92]{new_images/Schematic Overview GPT-3 Documentation - Single Line New-cropped.pdf}
%   %%%\includegraphics[scale=.88]{new_images/Schematic Overview GPT-3 Documentation - Small - Cropped.pdf}
%   \includegraphics[scale=.4]{new_images/FlowChart_doc2codeExample.pdf}

%  % \vspace{-4mm}
%   \caption{An overview of our approach}

%   \label{fig:workflow_doc2code}
% \vspace{-6mm}
% \end{figure}



%\subsection{Model and Parameter Setup}
%\label{subsec:model_setup}
\textbf{Model.} OpenAI's GPT-3 is an auto-regressive language model with 175 billion parameters and 96 layers pre-trained on large-scale text data \cite{brown2020language}. Besides NLP, it has shown great promise in different downstream tasks of software engineering (SE). In fact, OpenAI has recently released a new descendant of GPT-3 dedicated for SE tasks named Codex \cite{chen2021evaluating}. Codex has been trained on both natural language and programming language (i.e., public code from GitHub). It has already been used to automate several SE tasks e.g., code generation \cite{finnie2022robots, trummer2022codexdb, xu2022systematic}, code summarization \cite{ahmed2022few, khan2022automatic}, code repair \cite{prenner2021automatic}, security bug-fix \cite{pearce2021can}. Therefore, we picked Codex for this study.    

%large scale ($\sim$45 TB) text data

\textbf{Prompt Engineering.} GPT-3/Codex works through prompt engineering, where the description of the task is embedded in the input (prompt) and the model (GPT-3) is expected to perform the task (e.g., text/code generation) accordingly. Prompting of GPT-3 can take place in different ways: \textit{zero-shot, one-shot, few-shot learning}. \textit{Zero-shot learning} calls for the model to produce a response without showing any examples. That means the prompt just contains the task description; no other information is provided. In contrast, \textit{one} (or \textit{few)-shot learning} involves giving one (or more than one) example(s) in the prompt. Though \textit{one} or \textit{few-shot learning} might yield better result, we have only experimented with \textit{zero-shot learning} in this exploratory study.

\textbf{Parameter Settings.}
GPT-3 involves several parameters. \textit{Temperature} and \textit{top-p} are two such parameters that control the randomness of the model's output ranging from 0 to 1. We set the temperature at a low value (0.2) while keeping top-P at 0.95 as used by Xu et al. \cite{xu2022systematic}. \href{https://beta.openai.com/docs/guides/code/best-practices}{Codex official documentation} also suggests similar setup. We keep \textit{Frequency} and \textit{Presence penalties} at their default values (0.0) that control the level of word repetition in the generated text by penalizing them based on their existing frequency and presence. In order to keep the code examples precise and simple, we select a moderately smaller \textit{max tokens size} of 256.

\section{Automatic Code Example Generation}
We present our observation based on our preliminary investigation of code example generation with pre-trained model (Codex) using source code, documentation, and error logs.

\subsection{RQ1. Contexts = Method Source Code + Documentation}
\subsubsection{Motivation}
A good code example is a vital part of documentation that helps developers to understand the usage of the underlying code unit (e.g., method) \cite{Nykaza-ProgrammersNeedsAssessmentSDKDoc-SIGDOC2002}. However, documentation of many software libraries/frameworks often lack suitable code examples which is marked to be a common documentation issue for the practitioners  \cite{Robillard-FieldStudyAPILearningObstacles-SpringerEmpirical2011a}. Hence, in this research question, we focus on documentation-specific code example generation using pre-trained model.   

\subsubsection{Approach}
While we could use any pre-trained model, we decided to evaluate Codex \cite{chen2021evaluating} for our preliminary study (as mentioned in Section \ref{subsec:model_setup}). We propose to use the source code and the documentation of a code unit (e.g., class, method) as the input (prompt) of Codex which will generate the code example accordingly. A sample input/output for our zero-shot learning is showed in Figure \ref{fig:code_example_generation} where we ask Codex to generate code example for the method {\ttfamily sklearn.metrics.cohen\_kappa\_score}. 

% \begin{figure}[t]
% \centering
%     \begin{lstlisting}[frame=single,keywordstyle=\color{black}]
% `Method Source Code:`
%     def add(x, y):
%         return x+y
        
% `Method Documentation:` 
%     Adds two numbers.
    
% `Generate a code example for the above method and documentation:`
%     ^[To be generated by Codex]^
%     \end{lstlisting}
    
% \caption{Sample prompt format for code example generation}
%     \label{fig:zero_shot_code_example}
%     \vspace{-6mm}
% \end{figure}



\begin{figure}
     \centering
     \begin{subfigure}[b]{\columnwidth}
         \centering
    \begin{lstlisting}[frame=single,keywordstyle=\color{black}]
`Method Source Code:`
    def cohen_kappa_score(y1, y2, *, labels, weights, sample_weight):
        confusion = confusion_matrix(y1, y2, labels, sample_weight)
        n_classes = confusion.shape[0]
        ... ... ... ... ... ... ... ... 
        ... ... ... ... ... ... ... ...
        ... ... ... ... ... ... ... ...
        k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)
        return 1 - k
        
`Method Documentation:` 
    This function computes Cohen's kappa, a score that expresses the 
    level of agreement between two annotators on a classification 
    problem.
    
`Generate a code example for the above method and documentation:`
    ^[To be generated by Codex]^
    
    \end{lstlisting}
    
\caption{Input}
    \label{fig:zero_shot_code_example}
     \end{subfigure}
     \hfill
     
     \begin{subfigure}[b]{\columnwidth}%{0.5\textwidth}
         \centering
        \begin{lstlisting}[frame=single]
# Import the cohen_kappa_score function 
`from sklearn.metrics import cohen_kappa_score`

# Define two annotators
`annotator_1 = [1, 1, 0, 0, 1, 1, 0, 0, 0, 0]`
`annotator_2 = [1, 1, 0, 0, 1, 0, 0, 0, 0, 0]`

# Calculate Cohen's kappa
`cohen_kappa = cohen_kappa_score(annotator_1, annotator_2)`

# Print the Cohen's kappa score
`print(cohen_kappa)`
    \end{lstlisting}
    
    \caption{Output}
    \label{fig:RQ1_example}
     \end{subfigure}
        \caption{Code example generation}
        \label{fig:code_example_generation}
        \vspace{-6mm}
\end{figure}    

While evaluation of large scale data should be the ideal goal, we experiment with a smaller number in this study to get a primary insight about the approach. To be specific, we focused on Scikit-learn (sklearn), a well-known and widely used Python library. We randomly selected 40 methods, collected their source codes and documentation, and generated code examples for them.

We plan to check three aspects of the generated code examples i.e., Passability, Relevance and Usefulness. For Passability, we run the generated code on compiler and note whether it passes (i.e., executes without error) or not. Relevance signifies whether the code example properly deals with the target method and documentation. Usefulness indicates the extent to which the generated example helps the developers. For this study, we manually analyzed the Passability and Relevance of the generated examples. We plan to analyze the Usefulness in the future. We also intend to design and employ metrics (e.g., BLEU score, Pass@\textit{k}) for automatic evaluation.


\subsubsection{Observation}
We find that passability of the generated examples is 72.5\% i.e., 29 out of 40 generated code examples were executed without any error. In terms of relevance, 82.5\% (33 out of 40) code examples successfully showed the usage of the target method. Figure \ref{fig:RQ1_example} depicts a code example for the method {\ttfamily sklearn.metrics.cohen\_kappa\_score} generated by Codex. This is fascinating how Codex understands the context of \textit{cohen's kappa coefficient} (used to measure agreement between two annotators \cite{mchugh2012interrater}) from the documentation and names the two lists as $annotator\_1$ and $annotator\_2$.   	


% \begin{figure}[t]
%     \begin{lstlisting}[frame=single]
% # Import the cohen_kappa_score function 
% `from sklearn.metrics import cohen_kappa_score`

% # Define two annotators
% `annotator_1 = [1, 1, 0, 0, 1, 1, 0, 0, 0, 0]`
% `annotator_2 = [1, 1, 0, 0, 1, 0, 0, 0, 0, 0]`

% # Calculate Cohen's kappa
% `cohen_kappa = cohen_kappa_score(annotator_1, annotator_2)`

% # Print the Cohen's kappa score
% `print(cohen_kappa)`

%     \end{lstlisting}

    
% \caption{Example of generated code example}
%     \label{fig:RQ1_example}    
%     \vspace{-3mm}
% \end{figure}



%Hence, the generated code examples are satisfactory in terms of Passability and Relevance.    





\subsection{RQ2. Contexts = Method Source Code + Doc + Error Logs}
\subsubsection{Motivation}
Whenever a code fails to execute due to compile/run time error, the developer looks at the error log first. Sometimes the developer immediately understands the mistake (based on her/his prior knowledge) and solves it, or s/he looks for a solution in online forums (e.g., Stack Overflow). Since transformer model like GPT-3/Codex is pre-trained on large amount of data collected from different sources (such as GitHUb, Stack Overflow), we anticipate that these models might be able to understand such error logs and fix the errors based on that. Hence, in this research question, we check if we can bring further improvement to the quality (i.e., passability) of the code examples (that failed in RQ1) using the error logs as input.        


\subsubsection{Approach}
%% Refer and Mimic this paper: https://arxiv.org/pdf/2004.09015.pdf. See section: 2.2 Mined NL-code Pairs.
We plan to use the log contents (i.e., error messages) produced by the compiler to fix the code examples that were unable to pass in RQ1. We incorporate the error messages with the produced code examples in the GPT-3 prompt and tell it to fix the error as showed in Figure \ref{fig:code_example_improvement}. The goal here is to evaluate GPT-3's ability to fix a faulty code (given the error messages) and thus improve the Passability of the generated code examples. We do this for the 11 code examples that failed to execute in the generation phase (RQ1) and see how many of them can be fixed by this approach.
%to fix the error as showed in Figure \ref{fig:improvement_with_log_prompt}. The goal here is to evaluate GPT-3's ability to fix a faulty codes (given the error messages) and thus improve the Passability of the generated code examples. We do this for the 11 code examples that failed to execute in the generation phase (RQ1) and see how many of them can be fixed by this approach.



% \begin{figure}[t]
% \centering
%     \begin{lstlisting}[frame=single,keywordstyle=\color{black}]
% `Code with error:`
%     x = 5
%     sum = add(x+y)
%     print(sum)

% `Error Message:` 
%     NameError: name 'y' is not defined
    
% `Correct the code based on the error message:` 
%     ^[To be generated by Codex]^
    
%     \end{lstlisting}
    
% \caption{Sample prompt format for improving code examples}
%     \label{fig:improvement_with_log_prompt}
%     \vspace{-3mm}
% \end{figure}
    


\begin{figure}
     \centering
     \begin{subfigure}[b]{\columnwidth}
         \centering
    \begin{lstlisting}[frame=single,keywordstyle=\color{black}]
`Code with error:`
    `from sklearn.pipeline import make_pipeline`
    `from sklearn.svm import SVC`
    `from sklearn.preprocessing import StandardScaler`
    
    # Create a pipeline object
    `pipe = make_pipeline(StandardScaler(), SVC(random_state=42))`
    
    # Fit the pipeline to the training data
    `pipe.fit(X_train, y_train)`
    
    # Score the accuracy on the test set
    `accuracy = pipe.score(X_test, y_test)`
    
    # Prints the accuracy
    `print('{0:.1%} test set accuracy'.format(accuracy))`

`Error Message:` 
     ~NameError: name 'X_train' is not defined~
    
`Correct the code based on the error message:` 
    ^[To be generated by Codex]^
    
    \end{lstlisting}
    
\caption{Input}
    \label{fig:input_log_prompt}
     \end{subfigure}
     \hfill
     
     \begin{subfigure}[b]{\columnwidth}%{0.5\textwidth}
         \centering
        \begin{lstlisting}[frame=single]
`from sklearn.pipeline import make_pipeline`
`from sklearn.svm import SVC`
`from sklearn.preprocessing import StandardScaler`
`from sklearn.model_selection import train_test_split`
`import numpy as np`

# Create a pipeline object
`pipe = make_pipeline(StandardScaler(), SVC(random_state=42))`

# Create train and test sets
^X, y = np.arange(20).reshape((10, 2)), range(10)^
^X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)^

# Fit the pipeline to the training data
`pipe.fit(X_train, y_train)`

# Score the accuracy on the test set
`accuracy = pipe.score(X_test, y_test)`

# Prints the accuracy
`print('{0:.1%} test set accuracy'.format(accuracy))`
    \end{lstlisting}
    
\caption{Output}
    \label{fig:output_log_prompt}
     \end{subfigure}
        \caption{Improvement using error message}
        \label{fig:code_example_improvement}
        \vspace{-6mm}
\end{figure} 

\subsubsection{Observation}
Incorporation of log contents (i.e., error messages) in the prompt immediately fixed more than half of the failed code examples (6 out of 11) from RQ1. Hence, the passability went up from 72.5\% to 87.5\%. We can see an example in Figure \ref{fig:code_example_improvement}. At first, the generated code example had an error since it was referring to some variables (such as $X\_train$) without defining them. After using the error message along with the failed code example as input (Figure \ref{fig:input_log_prompt}), the model now defines $X\_train$ and other variables that were previously undefined (highlighted with blue in Figure \ref{fig:output_log_prompt}). Thus the error got fixed. 



% \begin{figure}[t]
%     \begin{lstlisting}[frame=single]
%                                 `Before Using Error Log`@@
% `from sklearn.pipeline import make_pipeline`
% `from sklearn.svm import SVC`
% `from sklearn.preprocessing import StandardScaler`

% # Create a pipeline object
% `pipe = make_pipeline(StandardScaler(), SVC(random_state=42))`

% # Fit the pipeline to the training data
% `pipe.fit(X_train, y_train)`

% # Score the accuracy on the test set
% `accuracy = pipe.score(X_test, y_test)`

% # Prints the accuracy
% `print('{0:.1%} test set accuracy'.format(accuracy))`

% Error Message: ~NameError: name 'X_train' is not defined~
% ??
%                                  `After Using Error Log`@@
% `from sklearn.pipeline import make_pipeline`
% `from sklearn.svm import SVC`
% `from sklearn.preprocessing import StandardScaler`
% `from sklearn.model_selection import train_test_split`
% `import numpy as np`

% # Create a pipeline object
% `pipe = make_pipeline(StandardScaler(), SVC(random_state=42))`

% # Create train and test sets
% ^X, y = np.arange(20).reshape((10, 2)), range(10)^
% ^X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)^

% # Fit the pipeline to the training data
% `pipe.fit(X_train, y_train)`

% # Score the accuracy on the test set
% `accuracy = pipe.score(X_test, y_test)`

% # Prints the accuracy
% `print('{0:.1%} test set accuracy'.format(accuracy))`
%     \end{lstlisting}

    
% \caption{Improvement using error message}
%     \label{fig:RQ2_1_example}    
%     \vspace{-3mm}
% \end{figure}




% \subsection{Improving Code Example with External Resources (RQ2.2)}
% \subsubsection{Motivation}

% \subsubsection{Approach}
% \label{subsubsec:RQ2.2Approach}
% We will collect relevant external knowledge (i.e., code snippets, developers' discussion) from different sources (e.g., Stack Overflow, GitHub) and incorporate them with the input (i.e., source code, documentation) of the generative model (i.e., Codex) in an attempt for further improvement of the code examples in terms of Passability, Relevance, and Usefulness. Since all the code-snippets/responses of a QA forum (like Stack Overflow) will not actually reflect the intent of our corresponding method/documentation, we can follow the approach recommended by Yin et al. \cite{yin2018learning}) to retrieve relevant information. They trained a classifier to decide whether an NL-code pair is valid or not based on the given intent. The probability assigned by the method can serve as confidence, representing the quality of the automatically mined information. For primary evaluation, we manually searched the method prototype on Stack Overflow and retrieved relevant external information (i.e., code snippets, developers' responses) based on most up-votes, add them in the input prompt and re-generate the code examples. We do this for the examples that were found to be failed or irrelevant in RQ1 and manually evaluated the new examples again to see whether the quality has improved or not.

% %prompt as depicted in Figure \ref{} and re-generate the code examples. We do this for the examples that were found to be failed or irrelevant in RQ1 and manually evaluated the new examples again to see whether the quality has improved or not.

% %We do this only for code examples that were found to be ill-formed or poor in quality in the generation phase (RQ1). We add the external knowledge in the input prompt as depicted in Figure \ref{} and re-generate the code examples. Finally, we manually evaluated them again to see whether the quality has improved or not.

% \subsubsection{Observation}
% Among the 11 failed code examples, 8 were fixed and executed without error by the incorporation of Stack Overflow information. On the other hand, 3 out of 7 irrelevant code examples were fixed by it i.e., the new code examples properly deal with the target method. Hence, external information from Stack Overflow increased the passability from 72.5\% to 92.5\% and the relevance from 82.5\% to 90\%. We find that such improvement greatly benefits from the direct code snippets of the retrieved information.       