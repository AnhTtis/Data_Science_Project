% 1. intro te ekta flowchart dibo rq1 and rq2 combine kore something like:

% Method Prototype+Documetation+[External Resources] ->Gpt-3 -> Code Example for Documentation
% 2. RQ1 and RQ2 er approach e ASE paper er moto sample prompt format er ekta kore pic dibo.

% 3. RQ1 and RQ2 er Observation e partial evaluation er description and finding dibo. Ar ekta kore generated code example er  udahoron dibo.

% %------------------------------------------
\section{Introduction}\label{sec:introduction}
% %------------------------------------------
Modern-day rapid software development is often facilitated by the reuse of software libraries/frameworks. The learning and usage of such libraries is supported by official documentation. While good documentation helps developers to use the corresponding library function, bad
documentation can severely harm their productivity~\cite{DeSouza-DocumentationEssentialForSoftwareMaintenance-SIGDOC2005, Robillard-APIsHardtoLearn-IEEESoftware2009a,Robillard-FieldStudyAPILearningObstacles-SpringerEmpirical2011a,Aghajani-SoftwareDocPractitioner-ICSE2020, Khan-DocSmell-SANER2021,Uddin-HowAPIDocumentationFails-IEEESW2015}. It is found that developers prefer documentation that contain code examples along with textual description \cite{Forward-RelevanceSoftwareDocumentationTools-DocEng2002, DeSouza-DocumentationEssentialForSoftwareMaintenance-SIGDOC2005}. Code examples help the developers understand the usage of the given code unit (e.g., method) with less effort and time \cite{Carroll-MinimalManual-JournalHCI1987a, Shull-InvestigatingReadingTechniquesForOOFramework-TSE2000}.
 

Despite developers' constant urge for code examples in documentation, many real-world documentation does not come with complementary code examples. In fact, the lack of code examples has been identified as one of the most severe issues in documentation \cite{Robillard-FieldStudyAPILearningObstacles-SpringerEmpirical2011a}. However, studies find that developers are often unwilling towards writing documentation as they find it less productive and less rewarding \cite{parnas2011precise, mcburney2017towards}, although developers do look for code examples when they seek to learn and reuse a software library~\cite{Uddin-HowAPIDocumentationFails-IEEESW2015}. Manual efforts for generating code examples is often labouring and time-consuming. Therefore, automatically generating code examples relevant to an API documentation unit might be a significant step towards improving documentation quality and usability. 

Several works have been done on automatic code generation based on given natural language (NL) intent (i.e., NL$\rightarrow$code). Such models are usually trained on input-output pairs of NL task description and corresponding code, or learn the mapping from naturally occurring corpora collected from different sources such as Stack Overflow, GitHub, etc. \cite{allamanis2015bimodal, yin2017syntactic, rabinovich2017abstract, xu2020incorporating, alon2020structural, feng2020codebert, wang2021codet5, austin2021program, parvez2021retrieval, nijkamp2022conversational}. However, none of these existing works has directly focused on generating code examples dedicated to complement documentation. Though somewhat related to Natural-language-to-Code synthesis, automatic generation of such documentation-specific code-examples is a comparatively new research direction. 

\begin{figure}[t]
  \centering
  %\vspace{-6mm}
   %\hspace*{-.7cm}%
  %%%\includegraphics[scale=.92]{new_images/Schematic Overview GPT-3 Documentation - Single Line New-cropped.pdf}
  %%%\includegraphics[scale=.88]{new_images/Schematic Overview GPT-3 Documentation - Small - Cropped.pdf}
  \includegraphics[scale=.4]{new_images/FlowChart_doc2codeExample.pdf}

 % \vspace{-4mm}
  \caption{An overview of our approach}

  \label{fig:workflow_doc2code}
\vspace{-7mm}
\end{figure}
In this paper, we are focused on automatically generating code examples that can complement the documentation of a software library. Our goal is to include such code examples to the official documentation of a library. Hence, it is important that the produced code examples are correct, to the point, usable, and complete. It is also important that the code examples can fully address the problems specified in the documentation by taking into account the underlying development contexts. 
As such, our technique for code example generation uses contexts from different sources (such as source code, documentation, log contents) as input. Our proposed technique repeatedly uses the inputs to refine the underlying contexts, until a good quality code example is generated. An overview of our approach is shown in Figure \ref{fig:workflow_doc2code}. First, we pick a documentation that can have NL texts and the source code of a code unit (e.g., method). Second, we give this combination of source code and documentation to Machine Learning (ML) model that can produce a code example as a target. For the ML model, our initial experimentation has used the currently available pre-trained transformer model Codex \cite{chen2021evaluating}. Third, we attempt to compile the code example generated by the ML model. Fourth, if the code example generates error during the compilation, we instruct the ML model to fix and create a new code example by taking as input the error logs along with the failed code example itself. We repeat this process until the produced code example is compilable and relevant to the documentation contents. We answer two research questions to formally assess the quality of the produced code examples:


\noindent\textbf{RQ1. Can we generate good quality code examples for documentation using pre-trained models?} Transformer based pre-trained models are known for their state-of-the-art performances in several software engineering tasks such as automatic code synthesis, code repair, code summarization \cite{feng2020codebert,wang2021codet5, ahmad2021unified, phan2021cotext, khan2022automatic}. Hence, we feel motivated to explore transformer based model for the task at hand i.e., code example generation for documentation. We picked GPT-3 considering its huge success in different classification and generation tasks \cite{floridi2020gpt, chintagunta2021medically}, To be specific, we evaluated Codex which is a GPT-3 like model trained on over a dozen of programming languages like Python, Java, PHP, JavaScript, and so on \cite{chen2021evaluating}. We used the source code and the documentation of a method as input (prompt) of Codex and generated code examples in a zero-shot learning setting. Our intuition for the approach is simple and based on the naturalness of the pre-trained model. If a developer wants to write a code example for a documentation, s/he will first go through source code, understand the input/output format (i.e., parameters and return values). S/he will also refer to the corresponding documentation (if available) to get some further context. Hence, we design the input of Codex in a similar pattern.  


% Motivate GPT-3 
%       --follow from GPT-3 Documentation ASE paper

%   Motivate Use of Documentation
%   --Refer to the following papers here. We can go by saying...        We feel fascinated when knowledge of documentation has been       used to automatically win in a game in these papers:
         %% Paper-1: Learning to win by reading manuals in a Monte-Carlo framework
         %% Paper-2: Rtfm: Generalising to novel environment dynamics via reading
%   --Then mention the DocPrompting paper to say that                   documentation is used to improve code generation as well.





%\textbf{RQ2. Can we further improve the generated code examples using additional information?}

% Different studies have leveraged additional information (e.g., external knowledge retrieved from Stack Overflow and GitHub) to improve the performance of code generation tools \cite{hayati2018retrieval, xu2020incorporating,parvez2021retrieval, zhou2022docprompting}. Hence, we investigate whether we can bring any further improvement on the generated code examples using such additional information. We consider two sources of additional information: log contents produced in RQ1 and external information from different online forums (e.g., Stack Overflow). Hence, we divide this RQ in the following two sub-questions.


\noindent\textbf{RQ2. Can we further improve the generated code examples using compiler generated log contents?}
%\textbf{\textit{RQ2.1. Can we further improve using log contents produced in the generation phase?}}
Some of the generated code examples might not pass during execution because of compile time or run time error. In such cases, developers refer to compiler generated error messages in order to understand and fix their codes. Hence, we evaluate the ability of pre-trained model (Codex) to use such error messages to fix the error in the generated code examples. Using error logs as model's input for SE tasks is a new idea explored by some recent works \cite{zhang2022using, berabi2021tfix}. We provide the failed code examples and the corresponding error logs (produced by the compiler) as input of Codex and ask to fix them using prompt engineering. 


% \textbf{\textit{RQ2.2. Can we further improve using external knowledge from online forums?}}
% In this research question, we examine whether online discussion forum such as Stack Overflow, GitHub, etc. can help us to improve the generated code examples. In this study, we retrieve relevant information from Stack Overflow and incorporate them in model's input to see whether the quality of the code examples improves (compared to RQ1) or not.

While a full systematic evaluation of the proposed approach is our ultimate goal, we carry out this study as a preliminary investigation to get a heads-up about its feasibility and effectiveness. Hence, we conduct our study on 40 methods randomly selected methods from popular Python library scikit-learn. We evaluate the generated examples in terms of Passability (whether the generated example executes without error or not) and Relevance (whether the example properly deals with the targeted method and documentation). We find that Codex even with its' most basic setting (i.e., zero-shot learning) is capable of generating good quality code examples with 72.5\% passability and 82.5\% relevance. We further show that incorporation of error logs (produced by the compiler) improves the passability of the generated examples from 72.5\% to 87.5\%. 


To the best of our knowledge, this is the first study that considers the task of documentation-specific code example generation. Though further investigation is necessary, this study proves the feasibility and effectiveness of our approach. We believe, it will help the research community to explore further in this direction.   

\textbf{Replication Package.} Our code and data are shared at \url{https://github.com/disa-lab/AutomaticCodeExample_SANER23}

%Our code and data are shared at \url{https://github.com/AnonymousGit1234/CodeExampleGeneration}
