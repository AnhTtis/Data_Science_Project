\section{Automatic Detection of The Smells}
We answer the following four research questions:

\begin{enumerate}[label=RQ\arabic{*}., leftmargin=25pt]
  \item How accurate are rule-based classifiers to automatically detect the documentation smells?
  \item Can the shallow machine learning models outperform the rule-based classifiers?
  \item Can the deep machine learning models outperform the shallow learning models?
  \item Can the detection of one documentation smell support the detection another smell?
\end{enumerate}
\subsection{Performance Metrics}
We analyze and report the performance of each of the techniques developed and experimented as part of the algorithms development steps. We used 5-fold iterative stratified cross-validation for reporting performances that has been recommended for a multilabel dataset in \cite{iterative_stratified_cross_valid}. We report the performances using standard multi-label classification metrics, i.e., Exact Match Ratio, Hamming Loss \cite{a_literature_survey_on_algorithms_for_multilabel}. We also report separate Accuracy, Precision, Recall, and F1-score for each class.

\begin{inparaenum}[(a)]
\item\bf{Iterative Stratified Cross-Validation}
Traditional $k$-fold cross-validation is a statistical method of evaluating machine learning algorithms which divides data into $k$ equally sized folds and runs for $k$ iterations \cite{traditional_cross_validation_paper}. In each iteration, each of the $k$ folds is used as the held-out set for validation while the remaining $k-1$ folds are used as training sets. Stratified cross-validation is used to make sure that each fold is an appropriate representative of the original data by producing folds where the proportion of different classes is maintained \cite{stratified_cross_validation_paper}. However, stratification is not sufficient for multi-label classification problems as the number of distinct labelsets (i.e., different combinations of labels) is often quite large. For example, there can be 32 combinations of labels in our study as there are 5 types of documentation smells. In such cases, original stratified $k$-fold cross-validation is impractical since most groups might consist of just a single example. Iterative stratification, proposed by \cite{iterative_stratified_cross_valid}, solves this issue by employing a greedy approach of selecting the rarest groups first and adding them to the smallest folds while splitting. Hence, we used this iterative stratified cross-validation in our experiment. 

\item\bf{Exact Match Ratio}
In multi-label classification, prediction for an instance is a set of labels. Hence, it has a notion of partially correct prediction (along with fully correct and incorrect). The Exact Match Ratio ($EMR$) ignores the partially correct prediction and considers only those predictions as correct which exactly match the true label sets.

\begin{equation}
EMR  = \frac{1}{N} \sum_{i=1}^{N} I (L_i^{pred} = L_i^{true})
\label{eq:EMR}
\end{equation} %&

{$I$ is the indicator function. Its' value is 1 if the prediction, $L_i^{pred}$ matches the actual label, $L_i^{true}$, and 0 otherwise. $N$ is the total number of data.
}


\item\bf{Hamming Loss}
Hamming Loss ($HL$) is the fraction of labels in labelsets that are incorrectly predicted, i.e., the fraction of the wrong labels to the total number of labels \cite{hamming_loss_paper}. It takes the notion of partially correct prediction into account.  
\begin{equation}
HL  = \frac{1}{N\cdot L} \sum_{i=1}^{N} \sum_{j=1}^{L} xor (y_{i,j}^{pred}, y_{i,j}^{true}) \label{eq:HL}
\end{equation}

{$N$ is the total number of data, $L$ is the total number of labels.}

\item\bf{Accuracy}
Accuracy ($A$) is the number of correctly predicted instances out of all the instances. We reported accuracy separately for each class.
\begin{equation}
A = \frac{TP+TN}{TP+FN+TN+FP}
\label{eq:acc}
\end{equation}

{$TP$ is the number of true positives, $FN$ is the number of false
negatives, $FP$ is the number of false positives, $TN$ is the number of true
negatives.}


\item\bf{Precision}
Precision ($P$) is the ratio between the number of correctly predicted instances and all the predicted instances for a given class.
\begin{equation}
P  = \frac{TP}{TP+FP} 
\label{eq:precision}
\end{equation} %&


\item\bf{Recall}
Recall ($R$) represents the ratio of the number of correctly predicted instances and all instances belonging to a given class.

\begin{equation}
 R = \frac{TP}{TP+FN}
 \label{eq:recall}
 \end{equation} 
 

\item\bf{F1-score}
F1-score is the harmonic mean of precision and recall.
\begin{equation}
F1 = \frac{2\cdot P\cdot R}{P+R}
\label{eq:f-score}    
\end{equation}

\end{inparaenum}

\subsection{ How accurate are rule-based classifiers to automatically detect the documentation smells? (RQ1)}
\subsubsection{Rule-based Metrics}
%\subsubsection{Approach}
%\gias{discuss the studied features, put the feature summaries in a table}
%discuss the features, group the features into a set of feature dimensions.


\begin{inparaenum}[(a)]
\item\bf{Documentation Length.} We use the length of every documentation in order to capture the extensiveness of the bloated documentations.

\item\bf{Readability Metrics.} We measure Flesch readability metrics for the documentations and use them as a feature to analyze the understandability of documentation. This feature might be useful to detect tangled documentations.

\item\bf{Number of Acronyms and Jargons.} Since acronyms and jargons increase the complexity of a reading passage, we use the number of acronyms and jargon in every documentation to detect the tangled documentation by estimating the understandability.

\item\bf{Number of URLs.} As URLs are hints of possible fragmentations in the documentation, we use the number of URLs to capture these smells.

\item\bf{Number of function, class and package name mentioned.} We use the number of functions, classes, packages mentioned in documentation to capture excess structural information and fragmentation.

\item\bf{Edit Distance.} From the definition of lazy documentation, it can be said that the edit distance of a lazy documentation and its’ unit definition (method prototype) will be comparatively small. So we calculate the edit distance between the documentation description and method prototype and use it as a feature for our machine learning models.

\item\bf{Average Cosine Similarity.} We calculate the avg. cosine similarity of the documentations’ description. For this, we calculate the cosine similarities between each possible pair of that documentation and take their avg value for normalization. This feature will help to detect redundancy in the documentations.
\end{inparaenum}

\subsubsection{Results}
discuss the results by varying the metric thresholds

\subsection{How accurate are shallow machine learning models to automatically detect the documentation smells? (RQ2)}
\subsubsection{Shallow Learning Models}
We applied both traditional and deep learning algorithms to detect documentation smells. We employed different decomposition approaches (i.e., One-Vs-Rest, Label Powerset, Classifier Chains)~\cite{multilabel_decomp_1_MultilabelClassificationAnOverview,multilabel_decomp_2_AReviewOnMultilabelLearning,multilabel_decomp_3_ATutorialOnMultilabelClassification,multilabel_decomp_4_LearningFromMultilabelData} with Support Vector Machine (SVM) \cite{SVM_SupportVectorNetworks} as the base estimator for multi-label classification of documentation smells. We chose SVM since it has been successfully used with these decomposition approaches for multi-label classification in previous studies \cite{SVM_multi_1_AKernelMethodForMultilabelled, SVM_multi_2_TextCategorizationWithSVM}. SVM has also shown great performance in text classification tasks \cite{svm_text_classification_1, svm_text_classification_2}. We also evaluated adapted approaches like ML-$k$NN \cite{MLkNN_ALazyLearningApproach}, and deep learning models like Bi-LSTM \cite{bi_lstm_BidirectionalRecurrentNeuralNetworks}, and BERT \cite{bert_base_BertPretrainingOfDeepBidirectionalTransformers}. 

%\subsubsection{OVR-SVM}
%\label{subsubsec:ovr_svm}
\begin{inparaenum}[(a)]
\item\bf{One-Vs-Rest} is a heuristic method that works by decomposing the multi-label classification problem into multiple independent binary classification problems (one per class) \cite{multilabel_decomp_1_MultilabelClassificationAnOverview, multilabel_decomp_3_ATutorialOnMultilabelClassification}. It trains a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. All the independent classifiers then separately give individual class predictions for unseen data. We evaluated One-Vs-Rest (OVR) approach with SVM (as the base estimator) for detecting documentation smells. One-Vs-Rest support vector machine (OVR-SVM) has been successfully applied in several problems \cite{SVM_multi_1_AKernelMethodForMultilabelled, SVM_multi_2_TextCategorizationWithSVM, multilabel_decomp_1_MultilabelClassificationAnOverview}. We used RBF kernel for the SVM classifiers as recommended by earlier works \cite{svm_rbf_1_AComparisonStudyOfDifferentKernelFunctions, svm_rbf_2_APracticalGuideToSVM}.

%\subsubsection{LPS-SVM}
%\label{subsubsec:lps_svm}
\item\bf{Label Power Set (LPS)} treats every combination of labels as a new class and approaches in a multiclass classification manner \cite{LearningMultiLabelSceneClassification}. As a result, this method has high computational complexity. However, it is capable of taking label correlation into account. We used SVM (with RBF kernel) as the base estimator of the Label Power Set method \cite{SVM_multi_1_AKernelMethodForMultilabelled, SVM_multi_2_TextCategorizationWithSVM}.


%\subsubsection{CC-SVM}
%\label{subsubsec:cc_svm}
\item\bf{Classifier Chains (CC)} constructs a chain of binary classifiers, where every classifier uses the predictions of all the previous classifiers of the chain \cite{CC_1_ClassifierChainsForMultilabel, CC_2_ClassifierChainsForMultilabel}. This way the method can take label correlations into account. We constructed a Classifier Chain (CC) of SVMs and evaluated its’ performance for documentation smell detection.



%\subsubsection{ML-$k$NN}
%\label{subsubsec:ml_knn}
\item\bf{Multi-label $k$ Nearest Neighbors (ML-$k$NN)} is derived from the traditional $k$-nearest neighbor ($k$NN) algorithm \cite{MLkNN_ALazyLearningApproach}. It finds the $k$ nearest neighborhood of an input instance using $k$NN, then uses Bayesian inference to determine the label set of the instance. We studied this method because it has been reported to achieve considerable performance for different multi-label classification tasks in previous studies \cite{MLkNN_ALazyLearningApproach, mlknn_MultilabelTextClassificationUsingSemanticFeatures}. We used ML-$k$NN with the number of nearest neighbors, $K$ = 10 during our experiment as recommended by \cite{mlknn_MultilabelTextClassificationUsingSemanticFeatures}.

\end{inparaenum}

\subsubsection{Studied Features} In this subsection, we describe the features that we used to train our SVM and \it{k}NN-based models (see section \ref{subsubsec:ovr_svm} to \ref{subsubsec:ml_knn}) for documentation smell detection.

use two types of features: the rule-based metrics, bag-of-words (as a proxy of word embeddings which will be used in BERT/LSTM - you need to 
ensure that reviewers are convinced that you have looked at each option before claiming BERT is a the best classifier)

%%%% Table %%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%% Table %%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\subsubsection{Results}

Table \ref{table_overall_performance} represents the performance of the ML models for detecting documentation smell as a multi-label classification problem. We observe that OneVsRest (OVR) and Classifier Chain (CC) achieved equal results. CC-based models are generally superior to OVR-based models because of the capability of capturing label correlation. Since the labels (types) of the documentation smells are not correlated, the CC-based SVM could not exhibit higher performance than the OVR-based SVM. However, Label Powerset (LPS) achieved lower performance than the OVR and CC-based models. To show better results, LPS-based SVM would require sufficient instances for all 32 classes of 5 smell types. Deep learning-based models like Bi-LSTM and BERT, on the other hand, outperformed the SVM and $k$NN-based models by achieving higher exact match ratios. Although Bi-LSTM showed higher Hamming loss, BERT achieved similar to the SVM and $k$NN-based models. Therefore, in terms of Hamming loss, deep learning-based models did not show any improvement. However, considering both exact match ratio and Hamming loss as the performance metrics, we can deduce that BERT is the most suitable of all the models experimented in detecting the documentation documentation smell.
\begin{table}[t]
\caption{Shallow models on the overall smell types detection}
\centering
%\begin{center}
%\scalebox{1.0}
%\resizebox{\columnwidth}{!} %new add
%{
%\renewcommand*{\arraystretch}{2}
\begin{tabular}{lrr}
\toprule
\textbf{Model}   & \textbf{Exact Match Ratio} & \textbf{Hamming Loss} \\ \midrule
\textbf{Rule-based} & .17                        & .35       \\ %\hline
\textbf{OVR-SVM} & .50                        & \textbf{.14}          \\ %\hline
\textbf{LPS-SVM} & .48                        & .15                   \\ %\hline
\textbf{CC-SVM}  & .50                        & \textbf{.14}          \\ %\hline
\textbf{ML-$k$NN}  & .47                        & .15                   \\ %\hline
%\textbf{Bi-LSTM} & .80                        & .20                   \\ %\hline
%\textbf{BERT}    & \textbf{.84}               & .15                   \\ 
\bottomrule
\end{tabular}
%}
\label{table_overall_performance}
%\end{center}
\end{table}
\begin{table*}[t]
\caption{Performance on different type of documentation smells}
%\begin{center}
%\scalebox{1.0}
%{
%\renewcommand*{\arraystretch}{3}
\begin{tabular}{lrrrr|rrrr|rrrr|rrrr|rrrr}
%                                       & \multicolumn{20}{c}{}                                         \\ \cmidrule{2-21} 
\multicolumn{1}{l}{}                  & \multicolumn{4}{c}{\textbf{Bloated}}                                                                     & \multicolumn{4}{c}{\textbf{Lazy}}                                                                        & \multicolumn{4}{c}{\textbf{Excess   Struct}}                                                             & \multicolumn{4}{c}{\textbf{Tangled}}                                                                     & \multicolumn{4}{c}{\textbf{Fragmented}}                                                                  \\ 
\midrule
{\textbf{Model}}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}  \\ 
\midrule
{\textbf{Rule-based}} & {.76} & {.67} & {.86} &{.75} & {.57} & {.69} & {.70} &{.69} & {.74} & {.68} & {.74} &{.71} & {.51} & {.40} & {.38} &{.39} & {.61} & {.57} & {.58} &{.58} \\ %\hline
{\textbf{OVR-SVM}} & {\textbf{.97}} & {.88} & {.89} &{.89} & {.93} & {.85} & {.92} &{.88} & {\textbf{.77}} & {.48} & {.23} &{.31} & {\textbf{.85}} & {.72} & {.54} &{.62} & {\textbf{.76}} & {\textbf{.77}} & {.44} &{.56} \\ %\hline
{\textbf{LPS-SVM}} & {.96} & {.90} & {.77} &{.83} & {.94} & {.84} & {.96} &{.89} & {\textbf{.77}} & {.50} & {.24} &{.33} & {.83} & {.71} & {.42} &{.54} & {.73} & {.64} & {.49} &{.56} \\ %\hline
{\textbf{CC-SVM}}  & {\textbf{.97}} & {.88} & {.90} &{.89} & {.93} & {.83} & {.93} &{.87} & {\textbf{.77}} & {.49} & {.21} &{.29} & {\textbf{.85}} & {.72} & {.54} &{.62} & {\textbf{.76}} & {.75} & {.45} &{.56} \\ %\hline
{\textbf{ML-$k$NN}}  & {.95} & {.77} & {.82} &{.80} & {.91} & {.83} & {.84} &{.84} & {.76} & {.46} & {.39} &{.42} & {.82} & {.65} & {.55} &{.60} & {\textbf{.76}} & {.65} & {.64} &{.65} \\ %\hline
\bottomrule
%{\textbf{Bi-LSTM}} & {.92} & {.92} & {.92} &{.91} & {.89} & {.90} & {.89} &{.90} & {.76} & {.72} & {\textbf{.76}} &{.73} & {.78} & {.74} & {.78} &{.74} & {.67} & {.64} & {.67} &{.63} \\ %\hline
%{\textbf{BERT}}    & {.93} & {\textbf{.93}} & {\textbf{.93}} &{\textbf{.93}} & {\textbf{.97}} & {\textbf{.97}} & {\textbf{.97}} &{\textbf{.97}} & {.76} & {\textbf{.75}} & {\textbf{.76}} &{\textbf{.76}} & {.83} & {\textbf{.83}} & {\textbf{.83}} &{\textbf{.83}} & {.75} & {.75} & {\textbf{.75}} &{\textbf{.75}} \\ 
%\hline
\end{tabular}
%}
\label{table_individual_performance}
%\end{center}
\end{table*}
%%%% Table %%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\subsubsection{Results}
Table \ref{table_individual_performance} shows the performance of the ML models
on each type of documentation smells. Although SVM and $k$NN-based models
achieved slightly better accuracy than Bi-LSTM and BERT, the later models
outperformed SVM and $k$NN-based models in F1-score. This behavior is due to the
unequal distribution of the smell types over the dataset. SVM and $k$NN-based
models produced more false-negative results because the number of positive
instances for an individual smell type is significantly lower than the number of
negative instances for that type. As a result, SVM and $k$NN-based models showed
quite low recalls for some types (Excess Struct, Tangled, and Fragmented) and
consequently resulted in low F1-scores. 
%\subsubsection{Feautre Importance Analysis}
\gias{TODO: do the feature-based analysis here, i.e., ablation study on the shallow learning models as discussed}

\subsection{Can the deep machine learning models outperform the shallow learning models? (RQ3)}
\subsubsection{Deep Learning Models}
%\subsubsection{Bi-LSTM}
Bidirectional LSTM is more capable of exploiting contextual information than the unidirectional LSTM \cite{bilstm_FramewisePhonemeClassificationWithBiLSTM}. Hence, the Bi-LSTM network can detect the documentation smell by capturing the information of the API documentations from both directions. We constructed a Bi-LSTM model with 300 hidden states and initialized it using the pre-trained GloVe embedding \cite{Glove_GlobalVectorsForWordRepresentation} of 100 dimensions. We used ADAM optimizer \cite{adam_optimizer} with an initial learning rate of 0.001. We trained the model with batch size 256 over 10 epochs.


%\subsubsection{BERT}
Considering the great success of BERT (Bidirectional Encoder Representations from Transformers) in various natural language processing and text classification tasks \cite{bert_success_1,bert_success_2,bert_success_3,bert_success_4,bert_success_5,bert_success_6,bert_success_7}, we feel motivated to evaluate its’ performance in documentation smell detection. BERT is a  pre-trained model which was designed to learn contextual word representations of unlabeled texts \cite{bert_base_BertPretrainingOfDeepBidirectionalTransformers}. We used BERT-Base for this study which has 12 layers with 12 attention heads and 110 million parameters. We trained it on our labeled dataset for 10 epochs with a mini-batch size of 32. We used early-stop to avoid overfitting \cite{early_stop_bert_1} and considered validation loss as the metric of the early-stopping \cite{early_stop_bert_2}. The maximum length of the input sequence was set to 256. We used AdamW optimizer \cite{adam_w} with the learning rate set to 4e\textsuperscript{-5}, ß1 to 0.9, ß2 to 0.999, and epsilon to 1e\textsuperscript{-8} \cite{bert_base_BertPretrainingOfDeepBidirectionalTransformers, bert_fine_tuning}. We used binary cross-entropy to calculate the loss \cite{binary_cross_AreLossFunctionSame}. 


\subsubsection{Studied Features} talk about word-embeddings

\subsubsection{Results}
On the other hand, Bi-LSTM and BERT
focused on capturing generalized attributes for each smell type and as a result,
achieved impressive performance in all the performance metrics. Overall, BERT
showed promising results among all the models in accurately detecting each of
the individual documentation smells.
\begin{table}
\begin{tabular}{lrr}
\toprule
\textbf{Model}   & \textbf{Exact Match Ratio} & \textbf{Hamming Loss} \\ \midrule

%\textbf{Rule-based} & .17                        & .35       \\ %\hline
%\textbf{OVR-SVM} & .50                        & \textbf{.14}          \\ %\hline
%\textbf{LPS-SVM} & .48                        & .15                   \\ %\hline
%\textbf{CC-SVM}  & .50                        & \textbf{.14}          \\ %\hline
%\textbf{ML-$k$NN}  & .47                        & .15                   \\ %\hline
\textbf{Bi-LSTM} & .80                        & .20                   \\ %\hline
\textbf{BERT}    & \textbf{.84}               & .15                   \\ %\hline
\bottomrule
\end{tabular}
%}
\label{table_overall_performance}
%\end{center}
\end{table}

% \begin{table*}[!ht]
% \caption{Performance on different type of documentation smells}
% \begin{center}
% \scalebox{1.0}
% {
% %\renewcommand*{\arraystretch}{3}
% \begin{tabular}{ccccccccccccccccccccc}
%                                        & \multicolumn{20}{c}{}                                         \\ \cline{2-21} 
%{}                  & \multicolumn{4}{c|}{\textbf{Bloated}}                                                                     & \multicolumn{4}{c|}{\textbf{Lazy}}                                                                        & \multicolumn{4}{c|}{\textbf{Excess   Struct}}                                                             & \multicolumn{4}{c|}{\textbf{Tangled}}                                                                     & \multicolumn{4}{c|}{\textbf{Fragmented}}                                                                  \\ \hline
% {\textbf{Model}}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}  \\ \hline
% %{\textbf{Rule-based}} & {.76} & {.67} & {.86} &{.75} & {.57} & {.69} & {.70} &{.69} & {.74} & {.68} & {.74} &{.71} & {.51} & {.40} & {.38} &{.39} & {.61} & {.57} & {.58} &{.58} \\ %\hline
% %{\textbf{OVR-SVM}} & {\textbf{.97}} & {.88} & {.89} &{.89} & {.93} & {.85} & {.92} &{.88} & {\textbf{.77}} & {.48} & {.23} &{.31} & {\textbf{.85}} & {.72} & {.54} &{.62} & {\textbf{.76}} & {\textbf{.77}} & {.44} &{.56} \\ %\hline
% %{\textbf{LPS-SVM}} & {.96} & {.90} & {.77} &{.83} & {.94} & {.84} & {.96} &{.89} & {\textbf{.77}} & {.50} & {.24} &{.33} & {.83} & {.71} & {.42} &{.54} & {.73} & {.64} & {.49} &{.56} \\ %\hline
% %{\textbf{CC-SVM}}  & {\textbf{.97}} & {.88} & {.90} &{.89} & {.93} & {.83} & {.93} &{.87} & {\textbf{.77}} & {.49} & {.21} &{.29} & {\textbf{.85}} & {.72} & {.54} &{.62} & {\textbf{.76}} & {.75} & {.45} &{.56} \\ %\hline
% %{\textbf{ML-$k$NN}}  & {.95} & {.77} & {.82} &{.80} & {.91} & {.83} & {.84} &{.84} & {.76} & {.46} & {.39} &{.42} & {.82} & {.65} & {.55} &{.60} & {\textbf{.76}} & {.65} & {.64} &{.65} \\ %\hline
% {\textbf{Bi-LSTM}} & {.92} & {.92} & {.92} &{.91} & {.89} & {.90} & {.89} &{.90} & {.76} & {.72} & {\textbf{.76}} &{.73} & {.78} & {.74} & {.78} &{.74} & {.67} & {.64} & {.67} &{.63} \\ %\hline
% {\textbf{BERT}}    & {.93} & {\textbf{.93}} & {\textbf{.93}} &{\textbf{.93}} & {\textbf{.97}} & {\textbf{.97}} & {\textbf{.97}} &{\textbf{.97}} & {.76} & {\textbf{.75}} & {\textbf{.76}} &{\textbf{.76}} & {.83} & {\textbf{.83}} & {\textbf{.83}} &{\textbf{.83}} & {.75} & {.75} & {\textbf{.75}} &{\textbf{.75}} \\ \hline
% \end{tabular}
% }
% \label{table_individual_performance}
% \end{center}
% \end{table*}
\begin{table*}[t]
\caption{Performance on different type of documentation smells}
%\begin{center}
%\scalebox{1.0}
%{
%\renewcommand*{\arraystretch}{3}
\begin{tabular}{lrrrr|rrrr|rrrr|rrrr|rrrr}
%                                       & \multicolumn{20}{c}{}                                         \\ \cmidrule{2-21} 
\multicolumn{1}{l}{}                  & \multicolumn{4}{c}{\textbf{Bloated}}                                                                     & \multicolumn{4}{c}{\textbf{Lazy}}                                                                        & \multicolumn{4}{c}{\textbf{Excess   Struct}}                                                             & \multicolumn{4}{c}{\textbf{Tangled}}                                                                     & \multicolumn{4}{c}{\textbf{Fragmented}}                                                                  \\ 
\midrule
{\textbf{Model}}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}   & {A}   & {P}   & {R}   &{F1}  \\ 
\midrule
{\textbf{Bi-LSTM}} & {.92} & {.92} & {.92} &{.91} & {.89} & {.90} & {.89} &{.90} & {.76} & {.72} & {\textbf{.76}} &{.73} & {.78} & {.74} & {.78} &{.74} & {.67} & {.64} & {.67} &{.63} \\ %\hline
{\textbf{BERT}}    & {.93} & {\textbf{.93}} & {\textbf{.93}} &{\textbf{.93}} & {\textbf{.97}} & {\textbf{.97}} & {\textbf{.97}} &{\textbf{.97}} & {.76} & {\textbf{.75}} & {\textbf{.76}} &{\textbf{.76}} & {.83} & {\textbf{.83}} & {\textbf{.83}} &{\textbf{.83}} & {.75} & {.75} & {\textbf{.75}} &{\textbf{.75}} \\ 
\bottomrule
\end{tabular}
%}
\label{table_individual_performance}
%\end{center}
\end{table*}
\gias{TODO. Now report some misclassified cases of BERT per documentation smell and explain why the misclassification occurred.}

\subsection{Can the detection of one documentation smell support the detection another smell? (RQ4)}
In multi-label learning, the labels might be interdependent and correlated \cite{label_correlation_paper_1}. We used Phi Coefficients to determine such interdependencies and correlations between different documentation smells. The Phi Coefficient is a measure of association between two binary variables \cite{phi_coefficient_paper}. It ranges from -1 to +1, where ±1 indicates a perfect positive or negative correlation, and 0 indicates no relationship. We report the Phi Coefficients between each pair of labels in Figure \ref{fig:label_correlation}. We find that there is almost no correlation between `Fragmented’ and any other smell (except `Lazy’). By definition, the information of fragmented documentation is scattered in many sections or pages. Hence, it has little to do with smells like `Bloated’, `Excess Structural Information’, `Tangled’. We also observe that there is a weak positive correlation (+0.2 to +0.4) among the `Bloated’, `Excess Structural Information’, and `Tangled’ smells. One possible reason might be that if a documentation is filled with complex and unorganized information (Tangled), and unnecessary structural information (Excess Structural Information), it might be prone to become bloated as well. On the other hand, `Lazy’ smell has a weak negative correlation (-0.2 to -0.3) with all other groups since these kinds of documentation are often too small to contain other smells. However, none of these coefficients is high enough to imply a strong or moderate correlation between any pair of labels. Hence, it justifies our catalog by indicating that all the smells that we proposed are sufficiently unique in nature.

\begin{figure}[t]%[!htb]
  \centering
  %\vspace{-6mm}
   \hspace*{-.7cm}%
  \includegraphics[scale=.5]{new_images/label_correlation.png}
 % \vspace{-4mm}
  \caption{Correlation of different documentation smells. Red, Blue, and Gray mean positive, negative, and no correlation respectively. Intensity of color indicates the level of correlation.}

  \label{fig:label_correlation}
%\vspace{-4mm}
\end{figure}


% 
% \subsection{Ablation Study (RQ4)}
% BERT - epoch change
% SVM - feature change.



%\subsection{How accurate is each feature dimension?}
%answer RQ

%\subsection{How well does hypertuning work in the ML models?}
%answer RQ.
