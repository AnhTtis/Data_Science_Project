\subsection{Datasets}
\myparagraph{MOT17 dataset \cite{milan2016mot16}} has videos captured by both moving and stationary cameras from various viewpoints at different frame rates. It contains 7 training videos and 7 test videos. We use the first half of each video in the training set of MOT17 for training and the last half for validation following \cite{zhou2020tracking}. MOT17 provides both ``public detection'' and ``private detection'' protocols. Public detectors include DPM \cite{felzenszwalb2008discriminatively}, Faster R-CNN \cite{ren2015faster} and SDP \cite{yang2016exploit}. For the private detection setting, we train on the combination of CrowdHuman dataset \cite{shao2018crowdhuman} and MOT17 half training set following \cite{zhou2020tracking,sun2020transtrack,zeng2021motr,wu2021track} in ablation studies. We add Cityperson \cite{zhang2017citypersons} and ETHZ \cite{ess2008mobile} for training following \cite{wang2020towards,zhang2020fairmot,liang2020rethinking} when testing on the test set of MOT17. 

\myparagraph{MOT20 dataset \cite{dendorfer2020mot20}} capture the videos in very crowded scenes so there is a lot of occlusion happening. The average pedestrian in a frame is much larger than that of MOT17 (139 \textit{vs.} 33). MOT20 contains 4 training videos and 4 test videos with longer video length. It also provides public detection results from Faster-RCNN. We only use CrowdHuman dataset and the training set of MOT20 for training under the private detection setting. 

\myparagraph{HiEve dataset \cite{lin2020human}} is a large-scale human-centric dataset focusing on crowded and complex events. It contains longer average trajectory length, bringing more difficulty for human tracking tasks. HiEve captures videos on 30+ different scenarios including subway station, street, and dining hall, making the tracking problem a more challenging task. It contains 19 training videos and 13 test videos. We combine CrowdHuman and the training set of HiEve for training. 

\myparagraph{BDD100K dataset \cite{yu2020bdd100k}} is the largest 2D driving video dataset and the dataset splits of the 2D MOT task are 1400 videos for training, 200 videos for validation and 400 videos for testing. It needs to track objects of 8 classes and contains cases of large camera motion. We combine the training sets of the detection task and the 2D MOT task for training. 

\myparagraph{nuScenes dataset \cite{caesar2020nuscenes}} is a large scale 3D object detection and tracking dataset. It is the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. The tracking task contains 3D annotations with 7 object classes. nuScenes comprises 1000 scenes, including 700 training videos, 150 validation videos and 150 test videos. For each sequence, only the key frames sampled at 2 FPS are annotated. There are about 40 key frames per camera in a sequence. We only use the key frames in each sequence for tracking. 

\subsection{Metrics}
\myparagraph{2D MOT. } We use the CLEAR metrics \cite{bernardin2008evaluating}, including MOTA, FP, FN, IDs, \textit{etc.}, IDF1 \cite{ristani2016performance} and HOTA \cite{luiten2021hota} to evaluate different aspects of the tracking performance. MOTA is computed based on FP, FN and IDs as follows:
\begin{equation}
    MOTA=1-\frac{IDS+FP+FN}{GT},
\end{equation}
where GT stands for the number of ground-truth objects. Considering the amount of FP and FN are larger than IDs, MOTA focuses more on the detection performance. IDF1 evaluates the identity preservation ability and focus more on the association performance. HOTA is a very recently proposed metric which explicitly balances the effect of performing accurate detection, association and localization. For BDD100K dataset, there are some multi-class metrics such as mMOTA and mIDF1. mMOTA / mIDF1 is computed by averaging the MOTA / IDF1 of all the classes. 

\myparagraph{3D MOT. } The nuScenes tracking benchmark adopts Average Multi-Object Tracking Accuracy (AMOTA) \cite{weng20203d} as the main metric, which averages the MOTA metric over different recall thresholds to reduce the effect of detection confidence thresholds. sMOTA$_r$ \cite{weng20203d} augments MOTA by a term to adjust for the respective recall and guarantees that the sMOTA$_r$ values range from 0.0 to 1.0:
\begin{align}
\begin{split}
    & sMOTA_r=\\
    &\operatorname{max}(0,1-\frac{IDS_r+FP_r+FN_r-(1-r)P}{rP}),
\end{split}
\end{align}
Then, 40-point interpolation is utilized to compute the AMOTA metric:
\begin{equation}
    AMOTA=\frac{1}{|\mathcal{R}|}\sum_{r \in \mathcal{R}}sMOTA_r
\end{equation}
However, we identify in experiments that it still needs to select a suitable detection score threshold as the false positives may mislead the association results. 