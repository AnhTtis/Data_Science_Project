In this section, we briefly review the existing works which are related to our topic including 2D object detection, 3D object detection, 2D multi-object tracking, and 3D multi-object tracking. We also discuss the relationship between these tasks.

\subsection{2D Object Detection}
2D object detection aims at predicting bounding boxes from image input. It is one of the most active topics in computer vision and it is the basis of multi-object tracking. With the rapid development of object detection \cite{ren2015faster,he2017mask,redmon2018yolov3,lin2017focal,cai2018cascade,sun2021sparse,peize2020onenet,wang2022yolov7}, more and more multi-object tracking methods begin to utilize more powerful detectors to obtain higher tracking performance. The one-stage object detector RetinaNet \cite{lin2017focal} begins to be adopted by several methods such as \cite{lu2020retinatrack,peng2020chained}. The anchor-free detector CenterNet \cite{zhou2019objects} is the most popular detector adopted by most methods \cite{zhou2020tracking,zhang2020fairmot,wu2021track,zheng2021improving,wang2020joint,tokmakov2021learning,wang2021multiple} for its simplicity and efficiency. The YOLO series detectors \cite{redmon2018yolov3,bochkovskiy2020yolov4,ge2021yolox} are also adopted by a large number of methods \cite{wang2020towards,liang2020rethinking,liang2021one,chu2021transmot,unicorn,zhang2022robust} for its excellent balance of accuracy and speed. Recently, transformer-based detectors \cite{carion2020end,zhu2020deformable,meng2021conditional} are utilized by some trackers \cite{sun2020transtrack,meinhardt2022trackformer,zeng2021motr} for their elegant end-to-end framework. We adopt YOLOX \cite{ge2021yolox} as our 2D object detector for its high efficiency. 

The MOT17 dataset \cite{milan2016mot16} provides detection results obtained by popular detectors such as DPM \cite{felzenszwalb2008discriminatively}, Faster R-CNN \cite{ren2015faster} and SDP \cite{yang2016exploit}. A large number of multi-object tracking methods \cite{xu2019spatial,chu2019famnet,bergmann2019tracking,chen2018real,zhu2018online,braso2020learning,hornakova2020lifted} focus on improving the tracking performance based on these given detection results. We also evaluate our tracking algorithm under this public detection setting. 

\subsection{3D Object Detection}
3D object detection aims at predicting three-dimensional rotated bounding boxes from images or LiDAR input. It is an indispensable component of 3D multi-object tracking as the quality of the predicted 3D bounding boxes plays an important role in the tracking performance.

LiDAR-based 3D object detection methods \cite{zhou2018voxelnet,yan2018second,lang2019pointpillars,shi2019pointrcnn,yin2021center,du2021ago,liu2022spatial} achieve impressive performance because the point clouds retrieved from LiDAR sensors contain accurate 3D structural information. However, the high cost of LiDAR limits its widespread application. 

Alternatively, recent progress in camera-based methods makes low-cost mobility widely available and thus the camera-based approaches have received increasing attention due to their low cost and rich context information. 
Due to a lack of accurate depth, the monocular 3D object detection methods \cite{chen20153d,chen2016monocular,wang2021fcos3d,zou2021devil,zhang2021objects,ye2022rope3d} directly infer geometric knowledge via deep neural networks or investigate pixel-wise depth estimation distribution to convert the image into pseudo-LiDAR points \cite{reading2021categorical,ye2020monocular, wang2019pseudo,weng2019monocular,you2019pseudo,vianney2019refinedmpl}. 

Multi-camera 3D object detection \cite{philion2020lift,li2022bevformer,wang2022detr3d,liu2022petr,li2022bevdepth,xiong2023cape} is trending and drawing extensive attention by learning powerful representations in bird's-eye-view (BEV), which is straightforward due to its unified representation and easy adaptation for downstream tasks such as future prediction and planning. Thus, the vision-centric multi-view BEV perception approaches significantly narrow the performance gap between camera-based and LiDAR-based methods. 

%Taking the pros of each modality into consideration, multi-modality fusion-based methods fully explore the data-level\cite{vora2020pointpainting,wang2021pointaugmenting} and feature-level fusion\cite{huang2020epnet,chen2022futr3d,bai2022transfusion,li2022deepfusion,liu2022bevfusion} to preserve both geometric and semantic information and thus achieve the overwhelming performance. 

LiDAR-based detectors are popular choices for 3D MOT. PointRCNN \cite{shi2019pointrcnn} and CenterPoint \cite{yin2021center} are adopted by many 3D MOT methods \cite{weng20203d,yin2021center,benbarka2021score,pang2021simpletrack,wang2021immortal} for their simplicity and effectiveness. Recently, image-based 3D object detectors \cite{kundu20183d,wang2022detr3d,li2022bevdepth} begin to be adopted by some 3D trackers such as \cite{hu2022monocular,zhang2022mutr3d,yang2022quality} because image information can provide appearance cues for tracking. Our tracking framework is modality-agnostic and thus can be easily incorporated with various 3D object detectors.


\subsection{2D Multi-Object Tracking}
Data association is the core of multi-object tracking, which first computes the similarity between tracklets and detection boxes and leverages different strategies to match them according to the similarity. 

In 2D MOT, image information serves a basic role to compute similarity. Location, motion and appearance are useful cues for data association. SORT \cite{bewley2016simple} combines location and motion cues in a very simple way. It first adopts Kalman filter \cite{kalman1960new} to predict the location of the tracklets in the new frame and then computes the IoU between the detection boxes and the predicted boxes as the similarity. Other methods \cite{zhou2020tracking,sun2020transtrack,wu2021track,shuai2021siammot} design networks to learn object motions and achieve more robust results in cases of large camera motion or low frame rate. Location and motion similarities are both accurate in the short-term association, while appearance similarity is helpful for the long-term association. An object can be re-identified using appearance similarity after being occluded for a long period of time. Appearance similarity can be measured by the cosine similarity of the Re-ID features. DeepSORT \cite{wojke2017simple} adopts a stand-alone Re-ID model to extract appearance features from the detection boxes. Recently, joint detection and Re-ID models \cite{wang2020towards,zhang2020fairmot,lu2020retinatrack,zhang2021voxeltrack,pang2021quasi,zhou2022global,xu2021segment} becomes more and more popular because of their simplicity and efficiency. 

After similarity computation, matching strategy assigns identities for the objects. This can be done by Hungarian Algorithm \cite{kuhn1955hungarian} or greedy assignment \cite{zhou2020tracking}. SORT \cite{bewley2016simple} matches the detection boxes to the tracklets by matching once. DeepSORT \cite{wojke2017simple} proposes a cascaded matching strategy that first matches the detection boxes to the most recent tracklets and then to the lost ones. MOTDT \cite{chen2018real} first utilizes appearance similarity to match and then utilizes IoU similarity to match the unmatched tracklets. QDTrack  \cite{pang2021quasi} turns the appearance similarity into probability by a bi-directional softmax operation and adopts a nearest neighbor search to accomplish matching. Attention mechanism \cite{vaswani2017attention} can directly propagate boxes between frames and perform association implicitly. Recent methods such as \cite{meinhardt2022trackformer,zeng2021motr,zhao2022tracking} propose track queries to predict the location of the tracked objects in the following frames. The matching is implicitly performed in the attention interaction process without using Hungarian Algorithm.

% Tracking can also adopted to help obtain more accurate detection results. Some methods \cite{sanchez2016online,zhu2018online,chu2019famnet,chu2019online,chu2021transmot,chen2018real} utilize single object tracking (SOT) \cite{bertinetto2016fully} or Kalman filter \cite{kalman1960new}  to predict the location of the tracklets in the following frame and fuse the predicted boxes with the detection boxes to enhance the detection results. Other methods \cite{zhang2018integrated,liang2021one} leverage tracked boxes in the previous frames to enhance feature representation of the following frame. Recently, Transformer-based \cite{vaswani2017attention,dosovitskiy2020vit,wang2021pvt,liu2021swin} detectors \cite{carion2020end,zhu2020deformable} are adopted by several methods \cite{sun2020transtrack,meinhardt2022trackformer,zeng2021motr,cai2022memot} for its strong ability to propagate boxes between frames. These methods utilize the temporal information to obtain more coherent detection results. Our method also utilize the similarity with tracklets to strength the reliability of detection boxes. 

Most 2D MOT methods focus on how to design better association strategies. However, we argue that the way detection boxes are utilized determines the upper bound of data association. After obtaining the detection boxes by various detectors, most methods \cite{wang2020towards,zhang2020fairmot,pang2021quasi,lu2020retinatrack} only keep the high score boxes by a threshold, \ie 0.5, and use those boxes as the input of data association. This is because the low score boxes contain many backgrounds which harm the tracking performance. However, we observe that many occluded objects can be correctly detected but have low scores. To reduce missing detections and keep the persistence of trajectories, we keep all the detection boxes and associate across every one of them. We focus on how to make full use of detection boxes from high scores to low ones in the association process. 

\subsection{3D Multi-Object Tracking}
3D MOT shares many commonalities with 2D MOT, \ie data association. Most 3D MOT methods follow the tracking-by-detection paradigm, which first detects objects and then associates them across time. Compared with 2D MOT, the location and motion cues utilized in 3D MOT are more accurate and reliable because they contain depth information. For example, when two pedestrians come across each other, the 2D IoU obtained from the image plane is large and it is difficult to distinguish them via the 2D location. In 3D scenarios, the two pedestrians can be easily separated according to their differences in depth. 

LiDAR-based 3D trackers tend to utilize location and motion cues to compute similarity. Similar to SORT \cite{bewley2016simple}, AB3DMOT \cite{weng20203d} provides a simple baseline and a new evaluation metric for 3D MOT, which adopts Kalman filter as motion model and use 3D IoU between detections and tracklets for association. CenterPoint \cite{yin2021center} extends the center-based tracking paradigm in CenterTrack \cite{zhou2020tracking} to 3D. It utilizes the predicted object velocity as the constant velocity motion model and shows effectiveness under abrupt motions. Following works \cite{benbarka2021score,chiu2021pro,pang2021simpletrack,wang2021immortal} focus on the improvement of association metrics and life cycle management. QTrack \cite{yang2022quality} estimates the quality of predicted object attributes and proposes a quality-aware association strategy for more robust association. 

Visual appearance features extracted from images can further enhance the long-term association in 3D MOT. QD3DT \cite{hu2022monocular} proposes a quasi-dense similarity learning of appearance features following QDTrack \cite{pang2021quasi} in 2D MOT to handle the object reappearance problems. GNN3DMOT \cite{weng2020gnn3dmot} integrates motion and appearance features and learns the interaction between them via graph neural networks. TripletTrack \cite{marinello2022triplettrack} proposes local object feature embeddings to encode information about the visual appearance and monocular 3D object characteristics to achieve more robust performance in case of occlusions and missing detections. Recent progress in Transformer \cite{vaswani2017attention} makes it appealing in 3D MOT. Following MOTR \cite{zeng2021motr} in 2D MOT, camera-based 3D tracker MUTR3D \cite{zhang2022mutr3d} utilizes Transformer to learn 3D representations with 2D visual information and propagate the 3D bounding boxes across time in an end-to-end manner. 

We only utilize the motion cues to perform data association for a simpler unification of 2D and 3D MOT. We propose an integrated motion prediction strategy based on the detected velocity and Kalman filter. The smooth location predicted by Kalman filter is adopted for long-term association, which plays a similar role to deal with the reappearance problems as appearance features.