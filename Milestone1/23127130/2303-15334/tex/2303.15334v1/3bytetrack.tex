\begin{figure*}
\centering
\includegraphics[width=0.95\textwidth]{image/pipeline.pdf}
\caption{Overview of our unified 2D and 3D MOT framework \modelname. The input can be images, multi-view images, or point clouds. We first adopt a 2D or 3D object detector to obtain the detection boxes. In the first frame, we initialize all the high-score detection boxes as tracklets. In the following frames, we first associate the high-score detection boxes with all the tracklets and then associate the low-score detection boxes with the unmatched tracklets. The associations are performed between the detection boxes and the tracklets after motion prediction. The final outputs are the updated trajectories in each frame. }
\label{fig:pipeline}
\vspace{-3mm}
\end{figure*}


We solve the problems of 2D and 3D MOT in a simple and unified framework. It contains three parts: object detection, motion prediction, and data association. We first introduce how we phrase the problems of 2D MOT and 3D MOT in Sec.~\ref{subsec:problem}. Then we list some 2D and 3D detectors along with the basic Kalman filter motion model in Sec.~\ref{subsec:preliminary}. In Sec.~\ref{subsec:motion}, we introduce the complementary 3D motion prediction strategy proposed specially for 3D MOT. Finally, we elaborate on the core steps of the proposed detection-driven hierarchical data association strategy and how it alleviates the problem of object missing and fragmented trajectories in Sec.~\ref{subsec:asso}. The overview of the tracking framework is shown in Fig.~\ref{fig:pipeline}.


\subsection{Problem Formulation}
\label{subsec:problem}
\myparagraph{Multi-object tracking. } The goal of multi-object tracking is to estimate object trajectories in videos. Suppose we are going to obtain $L$ trajectories $\mathbb{S} = \{s^1,s^2,...,s^L\}$ in a video. Each trajectory $s^i=\{\mathbf{b}^i_{t_1},\mathbf{b}^i_{t_1+1},...,\mathbf{b}^i_{t_2}\}$ contains the location information of an object within a time period, \ie from frame $t_1$ to frame $t_2$, where the object appears. In 2D MOT, the location of object $i$ at frame $t$ can be represented as ${\mathbf{b}^i_{t}}_{2d}=[x_1,y_1,x_2,y_2] \in \mathbb{R}^4$, where $(x_1,y_1)$, $(x_2,y_2)$ are the top-left and bottom-right coordinates of the 2D object bounding box in the image plane. In 3D MOT, the tracking procedure is usually performed in the 3D world coordinates. The 3D location of object $i$ at frame $t$ can be denoted as ${\mathbf{b}^i_{t}}_{3d}=[x,y,z,\theta,l,w,h] \in \mathbb{R}^7$, where $(x,y,z)$ are the 3D world locations of the object center, $\theta$ is the object orientation and $(l,w,h)$ are the object dimensions.

\myparagraph{Data association. } We follow the popular \textit{tracking-by-detection} paradigm in multi-object tracking, which first detects objects in individual video frames and then associates them between frames and forms trajectories over time. Suppose we have $M$ detections and $N$ history trajectories at frame $t$, our goal is to assign each detection to one of the trajectories, which has the same identity in an entire video. Let $\mathbb{A}$ denote the space which consists of all possible associations (or matchings). Under the setting of multi-object tracking, each detection matches at most one trajectory and each trajectory matches at most one detection. We define the space $\mathbb{A}$ as follow:
\begin{align}
\label{eq:space}
    \mathbb{A} &= \Big \{A = \Big (m_{ij}\Big )_{i\in \mathbb{M},j\in \mathbb{N}} \; \Big | \; m_{ij} \in \{0,1\} \\
               & \land \; \sum_{i=0}^{M} m_{ij} \leqslant 1, \forall j \in \mathbb{N} \\
               & \land \; \sum_{j=0}^{N} m_{ij} \leqslant 1, \forall i \in \mathbb{M} \Big \},
\end{align}
where $\mathbb{M}=\{1,2,...,M\}$, $\mathbb{N}=\{1,2,...,N\}$ and $A$ is one possible matching of the entire $M$ detections and $N$ trajectories. When the $i_{th}$ detection is matched to the $j_{th}$ trajectory, then $m_{ij}=1$. Let $\mathbf{d}^1_t,...,\mathbf{d}^M_t$ and $\mathbf{h}^1_t,...,\mathbf{h}^N_t$ be the locations of all $M$ detections and $N$ trajectories at frame $t$, respectively. We compute a similarity matrix $\mathbf{S}_t \in \mathbb{R}^{M\times N}$ between all the detections and trajectories as follows:
\begin{equation}
\label{eq:sim}
    \mathbf{S}_t(i,j) = \operatorname{sim}(\mathbf{d}_t^i,\mathbf{h}_t^j),
\end{equation}
where the similarity can be computed by some spatial distances between the detections and the trajectories such as IoU or L2 distance. Our goal is to obtain the optimal matching $A^*$ where the total similarities (or scores) between the matched detections and trajectories is the highest:
\begin{equation}
\label{eq:optimal}
    A^* = \mathop{\arg\max}\limits_{m_{ij}=1}\sum_{i\in\mathbb{M},j\in\mathbb{N}}\mathbf{S}_t(i,j),
\end{equation}

\subsection{Preliminary}
\label{subsec:preliminary}
\myparagraph{2D object detector. } We adopt YOLOX \cite{ge2021yolox} as our 2D object detector. YOLOX is an anchor-free detector equipped with advanced detection techniques, \ie, a decoupled head, and the leading label assignment strategy SimOTA derived from OTA \cite{ge2021ota}. It also adopts strong data augmentations such as mosaic \cite{bochkovskiy2020yolov4} and mix-up \cite{zhang2017mixup} to further enhance the detection performance. YOLOX achieves an excellent balance of speed and accuracy compared with other modern detectors \cite{zhang2022dino,liu2021swin} and is appealing in real applications. 

\myparagraph{Camera-based 3D object detector. } We follow the multi-camera 3D object detection setting, which shows advantages over monocular methods by learning powerful and unified representations in bird's-eye-view (BEV). We utilize PETRv2 \cite{liu2022petrv2} as our camera-based 3D object detector. It is built upon PETR \cite{liu2022petr}, which extends the transformer-based 2D object detector DETR \cite{carion2020end} to multi-view 3D setting by encoding the position information of 3D coordinates into image features. PETRv2 utilizes the temporal information of previous frames to boost detection performance. 

\myparagraph{LiDAR-based 3D object detector. } We adopt CenterPoint \cite{yin2021center} and TransFusion-L \cite{bai2022transfusion} as our LiDAR-based 3D object detectors. CenterPoint utilizes a keypoint detector to find the centers of objects, and simply regresses to other 3D attributes. It also refines these 3D attributes using additional point features on the object in a second stage. TransFusion-L consists of convolutional backbones and a detection head based on a transformer decoder. It predicts 3D bounding boxes from a LiDAR point cloud using a sparse set of object queries. 

\myparagraph{Basic motion model. } We utilize Kalman filter \cite{kalman1960new} as our basic motion model for both 2D and 3D MOT. Similar to \cite{wojke2017simple}, we define an eight-dimensional state space $(u,v,a,b,\dot{u},\dot{v},\dot{a},\dot{b})$ in 2D tracking scenario, where $\mathbf{P}^{2d}=(u,v,a,b)$ are the 2D bounding box center position, the aspect ratio (width / height) and the bounding box height. $\mathbf{V}^{2d}=(\dot{u},\dot{v},\dot{a},\dot{b})$ are the respective velocities in the image plane. In 3D tracking scenario, we follow \cite{weng20203d} to define a ten-dimensional state space $(x,y,z,\theta,l,w,h,\dot{x},\dot{y},\dot{z})$, where $\mathbf{P}^{3d}=(x,y,z)$ is the 3D bounding box center position, $(l,w,h)$ is the object's size, $\theta$ is the object orientation and $\mathbf{V}^{3d}=(\dot{x},\dot{y},\dot{z})$ are the respective velocities in the 3D space. Different from \cite{weng20203d}, we define the state space in the 3D world coordinates to eliminate the effects of ego-motion. We directly adopt a standard Kalman filter with constant velocity motion and linear observation model. The motion prediction process at frame $t+1$ in 2D and 3D tracking scenario can be denoted as follows:
% \begin{equation}
% \begin{split}
%     \label{eq:predict}
%     u_{t+1}=u_{t}+\dot{u}_{t} \qquad v_{t+1}=v_{t}+\dot{v}_{t} \\
%     a_{t+1}=a_{t}+\dot{a}_{t} \qquad b_{t+1}=b_{t}+\dot{b}_{t},
% \end{split}
% \end{equation}
% It is similar in 3D tracking scenario:
% \begin{equation}
% \begin{split}
%     \label{eq:predict}
%     x_{t+1}=x_{t}+\dot{x}_{t} \quad
%     y_{t+1}=y_{t}+\dot{y}_{t} \quad
%     z_{t+1}=z_{t}+\dot{z}_{t}.
% \end{split}
% \end{equation}
\begin{equation}
\begin{split}
    \label{eq:predict}
    \mathbf{P}^{2d}_{t+1} = \mathbf{P}^{2d}_{t} + \mathbf{V}^{2d}_{t} \\
    \mathbf{P}^{3d}_{t+1} = \mathbf{P}^{3d}_{t} + \mathbf{V}^{3d}_{t}
\end{split}
\end{equation}
The updated state of each trajectory is the weighted average of the trajectory and the matched detection (or observation). The weights are determined by the uncertainty of both the trajectory and the matched detection following the Bayes rule.

\subsection{Complementary 3D Motion Prediction}
\label{subsec:motion}
We propose a complementary 3D motion prediction strategy to address the abrupt motions and short-term object disappearing problems in the driving scenarios. Specifically, we adopt the detected velocity for short-term association and the Kalman filter for long-term association.

%For example, the velocities of vehicles change drastically when cornering and braking occur. Linear motion models such as Kalman filter \cite{kalman1960new} have difficulty handling these situations. Another case is that pedestrians tend to get lost for a period of time when occlusion happens. Re-associating these targets needs long-term motion prediction utilizing history information.  

In 3D scenarios, modern detectors \cite{li2022bevformer,liu2022petrv2,yin2021center} have the ability to predict accurate short-term velocity via temporal modeling. Kalman filter models the smooth long-term velocity via state updating based on historical information. We maximize the strengths of both motion models through a bilateral prediction strategy. We adopt the Kalman filter to perform forward prediction and adopt the detected object velocity to perform backward prediction. The backward prediction is responsible for the short-term association of alive tracklets while the forward prediction is leveraged for the long-term association of lost tracklets. Fig.~\ref{fig:motion} illustrates the complementary motion prediction strategy. 

Suppose we have $M$ detections $\mathbf{D}^t \in \mathbb{R}^{M \times 7}$ and the detected object velocity $\mathbf{V}^t \in \mathbb{R}^{M \times 2}$ in both $x$ and $y$ directions at frame $t$. The 
backward prediction can be computed as follows:
\begin{equation}
\label{eq:back}
\hat{\mathbf{D}}^{t-1}_x=\mathbf{D}^t_x - \mathbf{V}^t_x \qquad
\hat{\mathbf{D}}^{t-1}_y=\mathbf{D}^t_y - \mathbf{V}^t_y,
\end{equation}
Suppose there are $N$ tracklets $\mathbf{T}^{t-1} \in \mathbb{R}^{N \times 7}$ at frame $t-1$, We adopt the Kalman filter described in Sec.~\ref{subsec:preliminary} to perform forward prediction as follows:
\begin{equation}
\begin{split}
    \label{eq:forward}
    \mathbf{T}^{t}_{x,y,z}=\mathbf{T}^{t-1}_{x,y,z} + \dot{\mathbf{T}}^{t-1}_{x,y,z},
\end{split}
\end{equation}
where $\dot{\mathbf{T}}^{t-1}_{x,y,z}$ is the smoothed velocity in $x$, $y$ and $z$ directions computed by the Kalman filter in Sec.~\ref{subsec:preliminary}. 

We utilize the unified 2D and 3D data association strategy introduced in Sec.~\ref{subsec:asso} after the bilateral prediction. In the first association, the similarity $\mathbf{S}_t \in \mathbb{R}^{M\times N}$ is computed between the detection results $\mathbf{D}^{t-1}$ from backward prediction and the tracklets $\mathbf{T}^{t-1}$ as follows:
\begin{equation}
\label{eq:sim3d}
    \mathbf{S}_{t}(i,j) = \operatorname{GIoU}(\mathbf{D}_i^{t-1},\mathbf{T}_j^{t-1}),
\end{equation}
We adopt 3D GIoU \cite{rezatofighi2019generalized} as the similarity metric to address the occasional non-overlapping problem between the detection boxes and the tracklet boxes. We use Hungarian algorithm \cite{kuhn1955hungarian} to complete the identity assignment based on $\mathbf{S}_t$.
After the association, the matched detections $\mathbf{D}^t_{match}$ are utilized to update the matched tracklets $\mathbf{T}^t_{match}$ following the standard Kalman filter updating rule. The forward prediction strategy in Eq.~\ref{eq:forward} plays an important role when a tracklet gets lost, \ie with no matched detections. When the lost object reappears in the following frames, it can be re-associated via the similarity with the predicted location, which is also called track rebirth. The second association in Algorithm~\ref{algo:byte} follows the same procedure as the first association. 

We adopt the detection scores to further enhance motion prediction by adaptively updating the measurement uncertainty matrix $\hat{\mathbf{R}}_t^j$ in the Kalman filter of trajectory $j$ at frame $t$ as follows:
\begin{equation}
\label{eq:update}
    \hat{\mathbf{R}}_t^j = \alpha (1-s^j_t)^2 \mathbf{R}_t^j ,
\end{equation}
where $s^j_t$ is the detection score of trajectory $j$ at frame $t$ and $\alpha$ is a hyperparameter that controls the magnitude of the uncertainty. By plugging the detection score into the uncertainty matrix, we make the Kalman filter more robust to detections of different qualities. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.98\linewidth]{image/motion.pdf}
	\caption{Illustration of the complementary motion prediction strategy. Boxes in orange represent the detection results. Boxes in blue represent the predicted location by the Kalman filter. Dashed boxes in green represent the location of backward prediction by the detected velocity. }
	\label{fig:motion}
	\vspace{-5mm}
\end{figure}


\subsection{Unified 2D and 3D Data Association}
\label{subsec:asso}
We propose a simple, effective, and unified data association method for 2D and 3D MOT. Different from previous methods \cite{wang2020towards,zhang2020fairmot,pang2021quasi,weng20203d,yin2021center}, which only keep the high-score detection boxes, we keep every detection box and separate them into high-score ones and low-score ones. The whole pipeline of our detection-driven hierarchical data association strategy is shown in Figure~\ref{fig:pipeline}. 

\myparagraph{Overview. }In the first frame of the video, we initialize all the detection boxes as tracklets. In the following frames, we first associate the high-score detection boxes with the tracklets. Some tracklets get unmatched because they do not match an appropriate high-score detection box, which usually happens when occlusion, motion blur, or size change occurs. We then associate the low-score detection boxes and these unmatched tracklets to recover the objects in low-score detection boxes and filter out the background, simultaneously. The pseudo-code of \modelname is shown in Algorithm~\ref{algo:byte}. 


\begin{algorithm}[!h]
\SetAlgoLined
\DontPrintSemicolon
\SetNoFillComment
\footnotesize
\KwIn{A video sequence $\texttt{V}$; object detector $\texttt{Det}$; detection score threshold {$\tau$}}
\KwOut{Tracks $\mathcal{T}$ of the video}

Initialization: $\mathcal{T} \leftarrow \emptyset$\;
\For{frame $f_k$ in $\texttt{V}$}{
	\tcc{Figure 3(a)}
	\tcc{predict detection boxes \& scores}
	$\mathcal{D}_k \leftarrow \texttt{Det}(f_k)$ \;
	$\mathcal{D}_{high} \leftarrow \emptyset$ \;
	\textcolor{codegreen}{$\mathcal{D}_{low} \leftarrow \emptyset$} \;
	\For{$d$ in $\mathcal{D}_k$}{
	\If{$d.score > \tau$}{
	$\mathcal{D}_{high} \leftarrow  \mathcal{D}_{high} \cup \{d\}$ \;
	}
	\Else{
	\textcolor{codegreen}{$\mathcal{D}_{low} \leftarrow  \mathcal{D}_{low} \cup \{d\}$ \;
	}}
	}
	
    \BlankLine	
	\BlankLine
	\tcc{predict new locations of tracks}
	\For{$t$ in $\mathcal{T}$}{
	$t \leftarrow \texttt{MotionPredictor}(t)$ \;
	}
	
    \BlankLine
    \BlankLine
	% \tcc{Figure 3(b)}
	\tcc{first association}
	Associate $\mathcal{T}$ and $\mathcal{D}_{high}$ using \texttt{Similarity\#1}\;
	$\mathcal{D}_{remain} \leftarrow \text{remaining object boxes from } \mathcal{D}_{high}$ \;
	$\mathcal{T}_{remain} \leftarrow \text{remaining tracks from } \mathcal{T}$ \;
	
	\BlankLine
	\BlankLine
	% \tcc{Figure 3(c)}
    \tcc{second association}
	\textcolor{codegreen}{
	Associate $\mathcal{T}_{remain}$ and $\mathcal{D}_{low}$ using \texttt{Similarity\#2}\;}
	\textcolor{codegreen}{
	$\mathcal{T}_{re-remain} \leftarrow \text{remaining tracks from } \mathcal{T}_{remain}$ \;}
	

    \BlankLine
	\BlankLine
	\tcc{delete unmatched tracks}
	$\mathcal{T} \leftarrow \mathcal{T} \setminus \mathcal{T}_{re-remain}$ \;
	
    \BlankLine
	\BlankLine
	\tcc{initialize new tracks}
    \For{$d$ in $\mathcal{D}_{remain}$}{
	$\mathcal{T} \leftarrow  \mathcal{T} \cup \{d\}$ \;
	}
}
Return: $\mathcal{T}$
\caption{Pseudo-code of \modelname.}
\algorithmfootnote{Track rebirth~\cite{wojke2017simple,zhou2020tracking} is not shown in the algorithm for simplicity. In \textcolor{codegreen}{green} is the key of our method. }
\label{algo:byte}
\end{algorithm}

% \vspace{-5mm}

\myparagraph{Input. }The input of \modelname is a video sequence $\texttt{V}$, along with an object detector $\texttt{Det}$. We also set a detection score threshold $\tau$. The output is the tracks $\mathcal{T}$ of the video and each track contains the bounding box and identity of the object in each frame. 


\myparagraph{Detection boxes. }For each frame in the video, we predict the detection boxes and scores using the detector $\texttt{Det}$. We separate all the detection boxes into two parts $\mathcal{D}_{high}$ and $\mathcal{D}_{low}$ according to the detection score threshold $\tau$. For the detection boxes whose scores are higher than $\tau$, we put them into the high-score detection boxes $\mathcal{D}_{high}$. For the detection boxes whose scores are lower than $\tau$, we put them into the low-score detection boxes $\mathcal{D}_{low}$ (Line 3 to 13 in Algorithm~\ref{algo:byte}). 

\myparagraph{Motion prediction. }After separating the low-score detection boxes and the high-score detection boxes, we predict the new locations in the current frame of each track in $\mathcal{T}$ (Line 14 to 16 in Algorithm~\ref{algo:byte}) For 2D MOT, we directly adopt the Kalman filter for motion prediction. For 3D MOT, we utilize the complementary motion prediction strategy introduced in Sec.~\ref{subsec:motion}. 


\myparagraph{High-score boxes association. }The first association is performed between the high-score detection boxes $\mathcal{D}_{high}$ and all the tracks $\mathcal{T}$ (including the lost tracks $\mathcal{T}_{lost}$). \texttt{Similarity\#1} can be computed by the spatial distance such as IoU between the detection boxes $\mathcal{D}_{high}$ and the predicted box of tracks $\mathcal{T}$. Then, we adopt Hungarian Algorithm \cite{kuhn1955hungarian} to finish the matching based on the similarity. We keep the unmatched detections in $\mathcal{D}_{remain}$ and the unmatched tracks in $\mathcal{T}_{remain}$ (Line 17 to 19 in Algorithm~\ref{algo:byte}). 

The whole pipeline is highly flexible and can be compatible to other different association methods. For example, when it is combined with FairMOT~\cite{zhang2020fairmot}, Re-ID feature is added into \texttt{* first association *} in Algorithm~\ref{algo:byte}, others are the same. In the experiments of 2D MOT, we apply the association method to 9 different state-of-the-art trackers and achieve notable improvements on almost all the metrics.


\myparagraph{Low-score boxes association. }The second association is performed between the low-score detection boxes $\mathcal{D}_{low}$ and the remaining tracks $\mathcal{T}_{remain}$ after the first association. We keep the unmatched tracks in $\mathcal{T}_{re-remain}$ and just delete all the unmatched low score detection boxes since we view them as background. (line 20 to 21 in Algorithm~\ref{algo:byte}). We find it important to use IoU alone as the \texttt{Similarity\#2} in the second association because the low-score detection boxes usually contain severe occlusion or motion blur and the appearance features are not reliable. Thus, when applied to other Re-ID-based trackers \cite{wang2020towards,zhang2020fairmot,pang2021quasi}, we do not adopt appearance similarity in the second association. 

\myparagraph{Track rebirth. }After the association, the unmatched tracks will be deleted from the tracklets. We do not list the procedure of track rebirth \cite{wojke2017simple,chen2018real,zhou2020tracking} in Algorithm~\ref{algo:byte} for simplicity. Actually, it is necessary for the long-term association to preserve the identity of the tracks. For the unmatched tracks $\mathcal{T}_{re-remain}$ after the second association, we put them into $\mathcal{T}_{lost}$. For each track in $\mathcal{T}_{lost}$, only when it exists for more than a certain number of frames, \ie 30, we delete it from the tracks $\mathcal{T}$. Otherwise, we remain the lost tracks $\mathcal{T}_{lost}$ in $\mathcal{T}$ (Line 22 in Algorithm~\ref{algo:byte}). Finally, we initialize new tracks from the unmatched high-score detection boxes $\mathcal{D}_{remain}$ after the first association (Line 23 to 27 in Algorithm~\ref{algo:byte}). The output of each individual frame is the bounding boxes and identities of the tracks $\mathcal{T}$ in the current frame. 


% Fig.~\ref{fig:ass} provides a more vivid depiction. In frame $t_1$, we initialize three different tracklets as their scores are all higher than  0.5. However, in frame $t_2$ and frame $t_3$ when occlusion happens, red tracklet's corresponding detection score becomes lower \ie 0.8 to 0.4 and then 0.4 to 0.1. These detection boxes are eliminated by the thresholding mechanism and the red tracklet disappears accordingly. Nevertheless, if we take every detection box into consideration, more false positives will be introduced immediately, \eg, the most right box in frame $t_3$ of Fig.~\ref{fig:ass} (a). To the best of our knowledge, very few methods \cite{khurana2020detecting,tokmakov2021learning} in MOT are able to handle this detection dilemma.

% Fig.~\ref{fig:ass} (b) is exactly the results after the first association between the high score detection boxes and all the tracklets. Then, we perform the second association between the unmatched tracklets, \ie the tracklet in red box, and the low score detection boxes using the same motion similarity. Fig.~\ref{fig:ass} (c) shows the results after the second association. The occluded person with low detection scores is matched correctly to the previous tracklet and the background (in the right part of the image) is removed. As shown in Fig.~\ref{fig:ass} (c), two low score detection boxes are matched to the tracklets by the motion model's predicted boxes, and thus the objects are correctly recovered. At the same time, the background box is removed since it has no matched tracklet.

% \begin{figure}
% \centering
% \includegraphics[width=0.3\textwidth]{image/det_score.pdf}
% \caption{Detection score and occlusion rate of an object when occlusion occurs. }
% \label{fig:score}
% \vspace{-4mm}
% \end{figure}

% In Fig.~\ref{fig:score}, we plot the detection score and occlusion rate of a pedestrian and find that the detection score drops when the occlusion ratio increases. When an occlusion case happens, the score first decreases and then increases, as the pedestrian is occluded first and then re-appears. This inspires that we first associate the high score boxes to the tracklets. If a tracklet does not match any of the high score boxes, it is highly possible to be occluded and the detection score drops. Then, we associate it to the low score boxes to track the occluded target. For those FPs in the low score boxes, no tracklets match them. Thus, we throw them away. This is the key why our data association algorithm works. 
\myparagraph{Discussion. }We empirically find that the detection score drops when the occlusion ratio increases. When an occlusion case happens, the score first decreases and then increases, as the pedestrian is occluded first and then re-appears. This inspires us to first associate the high-score boxes with the tracklets. If a tracklet does not match any of the high score boxes, it is highly possible to be occluded and the detection score drops. Then, we associate it with the low-score boxes to track the occluded target. For those false positives in the low-score boxes, no tracklets match them. Thus, we throw them away. This is the key point that our data association algorithm works.


