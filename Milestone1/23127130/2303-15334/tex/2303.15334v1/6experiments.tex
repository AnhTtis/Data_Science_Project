\subsection{Implementation Details}
\label{subsec:detail}
\myparagraph{2D MOT. } The detector is YOLOX \cite{ge2021yolox} with YOLOX-X as the backbone and COCO-pretrained model \cite{lin2014microsoft} as the initialized weights. For MOT17, the training schedule is 80 epochs on the combination of MOT17, CrowdHuman, Cityperson and ETHZ. For MOT20 and HiEve, we only add CrowdHuman as additional training data. For BDD100K, we do not use additional training data and only train 50 epochs. The input image size is 1440 $\times$ 800 and the shortest side ranges from 576 to 1024 during multi-scale training. The data augmentation includes Mosaic \cite{bochkovskiy2020yolov4} and Mixup \cite{zhang2017mixup}. The model is trained on 8 NVIDIA Tesla V100 GPU with batch size of 48. The optimizer is SGD with weight decay of $5\times10^{-4}$ and momentum of 0.9. The initial learning rate is $10^{-3}$ with 1 epoch warm-up and cosine annealing schedule. The total training time is about 12 hours. Following \cite{ge2021yolox}, FPS is measured with FP16-precision \cite{micikevicius2017mixed} and batch size of 1 on a single GPU.

For the tracking part, the default detection score threshold $\tau$ is 0.6, unless otherwise specified. For the benchmark evaluation of MOT17, MOT20 and HiEve, we only use IoU as the similarity metrics. In the linear assignment step, if the IoU between the detection box and the tracklet box is smaller than 0.2, the matching will be rejected. For the lost tracklets, we keep it for 30 frames in case it appears again. For BDD100K, we use UniTrack \cite{wang2021different} as the Re-ID model. In ablation study, we use FastReID \cite{he2020fastreid} to extract Re-ID features for MOT17. 

\myparagraph{3D MOT. } For the camera modality setting, we adopt PETRv2 \cite{liu2022petrv2} with VoVNetV2 backbone \cite{lee2020centermask} as the detector. The input image size is 1600 $\times$ 640. The detection query number is set as 1500. It is trained with the pretrained model FCOS3D \cite{wang2021fcos3d} on validation dataset and with DD3D \cite{park2021pseudo} pretrained model on the test dataset as initialization. The optimizer is AdamW \cite{loshchilov2017decoupled} with a weight decay of 0.01. The learning rate is initialized with $2.0 \times 10^{-4}$ and decayed with cosine annealing policy. The model is trained on 8 Tesla A100 GPUs with a batch size of 8 for 24 epochs. For the LiDAR modality setting, we utilize TransFusion-L with VoxelNet \cite{zhou2018voxelnet} 3D backbone as the detector for the nuScenes test set. The model is trained for 20 epochs with the LiDAR point clouds as input. For the results on the nuScenes validation set, we adopt CenterPoint \cite{yin2021center} with VoxelNet backbone as the detector. The same detection results are used for fair comparisons with other 3D MOT methods \cite{yin2021center,weng20203d,chiu2021pro,benbarka2021score}. 

For the tracking part, the detection score threshold $\tau$ is 0.2 for PETRv2 and CenterPoint. The detection score of TransFusion-L is lower than other detectors because it is computed as the geometric average of the heatmap score and the classification score, so we set the threshold $\tau$ as 0.01 for TransFusion-L. We set different 3D GIoU thresholds for different object classes as they have different sizes and velocities. Concretely, we set -0.7 for bicycle, -0.2 for bus, -0.1 for car, -0.5 for motorcycle, -0.7 for pedestrian, -0.4 for trailer and -0.1 for truck. Similar to 2D MOT, we keep the lost tracklets for 30 frames in case it appears again. The hyperparameter $\alpha$ for updating the measurement uncertainty matrix in Eq.~\ref{eq:update} is 100 for camera-based method and 10 for LiDAR-based method. 

\subsection{2D MOT}
We evaluate ByteTrack under the 2D MOT setting in this section. We first present some experimental results of ablation analysis on MOT17 datasets. Then, we compare ByteTrack with other trackers on 4 different benchmarks. We denote the hierarchical data association strategy of ByteTrack as BYTE in the following paragraphs for simplicity. 

\subsubsection{Ablation Studies}
\myparagraph{Similarity analysis. }
We choose different types of similarity for the first association and the second association of BYTE. The results are shown in Table~\ref{table_sim}. We can see that either IoU or Re-ID can be a good choice for \texttt{Similarity\#1} on MOT17. IoU achieves better MOTA and IDs while Re-ID achieves higher IDF1. It is important to utilize IoU as \texttt{Similarity\#2} in the second association because the low score detection boxes usually contains severe occlusion and thus Re-ID features are not reliable. From Table~\ref{table_sim} we can find that using IoU as \texttt{Similarity\#2} increases 0.8 MOTA compared to Re-ID, which indicates that Re-ID features of the low score detection boxes are not reliable. We choose to use IoU as similarity in both associations. It is also more unified with the 3D tracking framework.

\begin{table}[!h]
\caption{Comparison of different type of similarity metrics used in the first association and the second association on the MOT17 validation set. The best results are shown in \textbf{bold}.}
\begin{center}
{\input{tables/sim}}
\end{center}
\label{table_sim}
\vspace{-5mm}
\end{table}


\myparagraph{Comparisons with other association methods.}
We compare BYTE with other popular association methods including SORT \cite{bewley2016simple}, DeepSORT \cite{wojke2017simple} and MOTDT \cite{chen2018real} on the validation set of MOT17. Results are shown in Table~\ref{table_ass}. 

SORT can be seen as our baseline method because both methods only adopt Kalman filter to predict the object motion.  We can find that BYTE improves the MOTA metric of SORT from 74.6 to 76.6, IDF1 from 76.9 to 79.3 and decreases IDs from 291 to 159. This highlights the importance of the low score detection boxes and proves the ability of BYTE to recover object boxes from low score one.

DeepSORT utilizes additional Re-ID models to enhance the long-range association. We surprisingly find BYTE also has additional gains compared with DeepSORT. This suggests a simple Kalman filter can perform long-range association and achieve better IDF1 and IDs when the detection boxes are accurate enough. We note that in severe occlusion cases, Re-ID features are vulnerable and may lead to identity switches, instead, motion model behaves more reliably.


\begin{table}[t]
\caption{Comparison of different data association methods on the MOT17 validation set. The best results are shown in \textbf{bold}. }
\begin{center}
{\input{tables/ass}}
\end{center}
\label{table_ass}
\vspace{-2mm}
\end{table}


MOTDT integrates motion-guided box propagation results along with detection results to associate unreliable detection results with tracklets. Although sharing the similar motivation, MOTDT is behind BYTE by a large margin. We explain that MOTDT uses propagated boxes as tracklet boxes, which may lead to locating drifts in tracking. Instead, BYTE uses low-score detection boxes to re-associate those unmatched tracklets, therefore, tracklet boxes are more accurate.

% Table~\ref{table_ass} also shows the results on BDD100K dataset. BYTE also outperforms other association methods by a large margin. Kalman filter fails in autonomous driving scenes and it is the main reason for the low performance of SORT, DeepSORT and MOTDT. Thus, we do not use Kalman filter on BDD100K. Additional off-the-shelf Re-ID models greatly improve the performance of BYTE on BDD100K. 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{image/threshold.pdf}
	\caption{Comparison of the performances of BYTE and SORT under different detection score thresholds. The results are from the validation set of MOT17. }
	\label{fig:threshold}
	%\vspace{-3mm}
\end{figure}

\begin{figure*}[!htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{image/bar.pdf}
	\caption{Comparison of the number of TPs and FPs in all low score detection boxes and the low score tracked boxes obtained by BYTE. The results are from the validation set of MOT17. }
	\label{fig:bar}
	\vspace{-3mm}
\end{figure*}



\myparagraph{Robustness to detection score threshold.}
The detection score threshold $\tau$ is a sensitive hyper-parameter and needs to be carefully tuned in the task of multi-object tracking. We change it from 0.2 to 0.8 and compare the MOTA and IDF1 score of BYTE and SORT. The results are shown in Fig.~\ref{fig:threshold}. From the results we can see that BYTE is more robust to the detection score threshold than SORT. This is because the second association in BYTE recovers the objects whose scores are lower than $\tau$, and thus considers almost every detection box regardless of the change of $\tau$. 


% \begin{table}[htpb]
% \caption{Results of applying BYTE to 9 different SOTA trackers on the MOT17 validation set. ``K'' is short for Kalman filter. In green are the improvements of at least \textcolor{codegreen}{+\textbf{1.0}} point. }
% \begin{center}
% \scalebox{1.0}{\input{tables/app}}
% \end{center}
% \label{table_app}
% \vspace{-6mm}
% \end{table}

\myparagraph{Analysis on low score detection boxes.}
To prove the effectiveness of BYTE, we collect the number of TPs and FPs in the low score boxes obtained by BYTE. We use the half training set of MOT17 and CrowdHuman for training and evaluate on the half validation set of MOT17. First, we keep all the low score detection boxes whose scores range from $\tau_{low}$ to $\tau_{high}$ and classify the TPs and FPs using ground truth annotations. Then, we select the tracking results obtained by BYTE from low score detection boxes. The results of each sequence are shown in Fig.~\ref{fig:bar}. We can see that BYTE obtains notably more TPs than FPs from the low score detection boxes even though some sequences (\ie MOT17-02) have much more FPs in all the detection boxes. The obtained TPs notably increases MOTA from 74.6 to 76.6 as is shown in Table~\ref{table_ass}. 





% \myparagraph{Applications on other trackers.}
% We apply BYTE on 9 different state-of-the-arts trackers, including JDE \cite{wang2020towards}, CSTrack \cite{liang2020rethinking}, FairMOT \cite{zhang2020fairmot}, TraDes \cite{wu2021track}, QDTrack  \cite{pang2021quasi}, CenterTrack \cite{zhou2020tracking}, Chained-Tracker \cite{peng2020chained}, TransTrack \cite{sun2020transtrack} and MOTR \cite{zeng2021motr}. Among these trackers, JDE, CSTrack, FairMOT, TraDes adopt a combination of motion and Re-ID similarity. QDTrack  adopts Re-ID similarity alone. CenterTrack and TraDes predict the motion similarity by the learned networks. Chained-Tracker adopts the chain structure and outputs the results of two consecutive frames simultaneously and associate in the same frame by IoU. TransTrack and MOTR adopt the attention mechanism to propagate boxes among frames. Their results are shown in the first line of each tracker in Table~\ref{table_app}.To evaluate the effectiveness of BYTE, we design two different modes to apply BYTE to these trackers. 

% \begin{itemize}
%     \item The first mode is to insert BYTE into the original association methods of different trackers, as is shown in the second line of the results of each tracker in Table~\ref{table_app}. Take FairMOT\cite{zhang2020fairmot} for example, after the original association is done, we select all the unmatched tracklets and associate them with the low score detection boxes following the \texttt{* second association *} in Algorithm~\ref{algo:byte}. Note that for the low score objects, we only adopt the IoU between the detection boxes and the tracklet boxes after motion prediction as the similarity. We do not apply the first mode of BYTE to Chained-Tracker because we find it is difficult to implement in the chain structure. 
    
%     \item The second mode is to directly use the detection boxes of these trackers and associate using the whole procedure in Algorithm~\ref{algo:byte}, as is shown in the third line of the results of each tracker in Table~\ref{table_app}. 
    
% \end{itemize}

% We can see that in both modes, BYTE can bring stable improvements over almost all the metrics including MOTA, IDF1 and IDs. For example, BYTE increases CenterTrack by 1.3 MOTA and 9.8 IDF1, Chained-Tracker by 1.9 MOTA and 5.8 IDF1, TransTrack by 1.2 MOTA and 4.1 IDF1. The results in Table~\ref{table_app} indicate that BYTE has strong generalization ability and can be easily applied to existing trackers. 

% \myparagraph{Tracklet interpolation. } We notice that there are some fully-occluded pedestrians in MOT17, whose visible ratio is 0 in the ground truth annotations. Since it is almost impossible to detect them by visual cues, we obtain these objects by tracklet interpolation. 

% Suppose we have a tracklet $T$, its tracklet box is lost due to occlusion from frame $t_1$ to $t_2$. The tracklet box of $T$ at frame $t_1$ is $B_{t_1} \in \mathbb{R}^4$ which contains the top left and bottom right coordinate of the bounding box. Let $B_{t_2}$ represent the tracklet box of $T$ at frame $t_2$. We set a hyper-parameter $\sigma$ representing the max interval we perform tracklet interpolation, which means tracklet interpolation is performed when $t_2 - t_1 \leq \sigma$, . The interpolated box of tracklet $T$ at frame t can be computed as follows:
% \begin{equation}
%     B_t = B_{t_1} + (B_{t_2} - B_{t_1}) \frac{t - t_1}{t_2 - t_1},
% \end{equation}
% where $t_1 < t < t_2$. 

% As shown in Table~\ref{table_inter}, tracklet interpolation can improve MOTA from 76.6 to 78.3 and IDF1 from 79.3 to 80.2, when $\sigma$ is 20. Tracklet interpolation is an effective post-processing method to obtain the boxes of those fully-occluded objects. We leverage tracklet interpolation in the test sets of MOT17 \cite{milan2016mot16}, MOT20 \cite{dendorfer2020mot20} and HiEve \cite{lin2020human} under the private detection protocol.  

% \begin{table}[h]
% \caption{Comparison of different interpolation intervals on the MOT17 validation set. The best results are shown in \textbf{bold}.}
% \begin{center}
% {\input{tables/inter}}
% \end{center}
% \label{table_inter}
% \vspace{-3mm}
% \end{table}


\begin{table}[t]
\caption{Comparison of the state-of-the-art methods under the “private detector” protocol on MOT17 test set. The best results are shown in \textbf{bold}. MOT17 contains rich scenes and half of the sequences are captured with camera motion. ByteTrack ranks 1st among all the trackers on the leaderboard of MOT17. It also has the highest running speed among all trackers.}
\begin{center}
\scalebox{1.0}{\input{tables/mot17}}
\end{center}
\label{table_mot17}
\vspace{-2mm}
\end{table}

\begin{table}[t]
\caption{Comparison of the state-of-the-art methods under the “private detector” protocol on MOT20 test set. The best results are shown in \textbf{bold}. The scenes in MOT20 are much more crowded than those in MOT17. ByteTrack ranks 1st among all the trackers on the leaderboard of MOT20. It also has the highest running speed among all trackers. }
\begin{center}
\scalebox{1.0}{\input{tables/mot20}}
\end{center}
\label{table_mot20}
\vspace{-3mm}
\end{table}



\begin{table}[t]
\caption{Comparison of the state-of-the-art methods under the “private detector” protocol on HiEve test set. The best results are shown in \textbf{bold}. HiEve has more complex events than MOT17 and MOT20. ByteTrack ranks 1st among all the trackers on the leaderboard of HiEve. }
\begin{center}
\scalebox{1.0}{\input{tables/hie}}
\end{center}
\label{table_hie}
\vspace{-5mm}
\end{table}


\subsection{Benchmark Evaluation}
We compare ByteTrack with the state-of-the-art trackers on the test set of MOT17, MOT20, HiEve and BDD100K under the private detection protocol in Table~\ref{table_mot17}, Table~\ref{table_mot20}, Table~\ref{table_hie} and Table~\ref{table_bdd} respectively. 
% We also evaluate ByteTrack on the test set of MOT17 and MOT20 under the public detection protocol in Table~\ref{table_mot17pub} and Table~\ref{table_mot20pub}. 

\myparagraph{MOT17.}
ByteTrack ranks 1st among all the trackers on the leaderboard of MOT17. Not only does it achieve the best accuracy (\ie 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA), but also runs with highest running speed (30 FPS). It outperforms the second-performance tracker \cite{yang2021remot} by a large margin (\ie +3.3 MOTA, +5.3 IDF1 and +3.4 HOTA). Also, we use less training data than many high performance methods such as \cite{zhang2020fairmot,liang2020rethinking,wang2021multiple,liang2021one} (29K images vs. 73K images). It is worth noting that we only leverage the simplest similarity computation method Kalman filter in the association step compared to other methods \cite{zhang2020fairmot,liang2020rethinking,pang2021quasi,wang2020joint,zeng2021motr,sun2020transtrack} which additionally adopt Re-ID similarity or attention mechanisms. All these indicate that ByteTrack is a simple and strong tracker. 

%We also evaluate ByteTrack under the public detection protocol. Following the public detection filtering strategy in Tracktor \cite{bergmann2019tracking} and CenterTrack \cite{zhou2020tracking}, we only initialize a new trajectory when its IoU with a public detection box is larger than 0.8. We do not use tracklet interpolation under the public detection protocol. As is shown in Table~\ref{table_mot17pub}, ByteTrack outperforms other methods by a large margin on MOT17. For example, it outperforms SiamMOT by 1.5 points on MOTA and 6.7 points on IDF1. The results under public detection protocol further indicate the effectiveness of our association method by eliminating the effect of the detection performance. 


% \begin{table}[t]
% \caption{Comparison of the state-of-the-art methods under the “public detection” protocol on MOT17 test set. The best results are shown in \textbf{bold}. }
% \begin{center}
% \scalebox{1.0}{\input{tables/mot17_pub}}
% \end{center}
% \label{table_mot17pub}
% \vspace{-3mm}
% \end{table}


% \begin{table}[t]
% \caption{Comparison of the state-of-the-art methods under the “public detection” protocol on MOT20 test set. The best results are shown in \textbf{bold}. }
% \begin{center}
% \scalebox{1.0}{\input{tables/mot20_pub}}
% \end{center}
% \label{table_mot20pub}
% \vspace{-3mm}
% \end{table}



\myparagraph{MOT20.}
Compared with MOT17, MOT20 has much more crowded scenarios and occlusion cases. The average number of pedestrians in an image is 170 in the test set of MOT20. ByteTrack also ranks 1st among all the trackers on the leaderboard of MOT20 and outperforms other trackers by a large margin on almost all the metrics. For example, it increases MOTA from 68.6 to 77.8, IDF1 from 71.4 to 75.2 and decreases IDs by 71\% from 4209 to 1223. It is worth noting that ByteTrack achieves extremely low identity switches, which further indicates that associating every detection boxes is very effective under occlusion cases. 

%Table~\ref{table_mot20pub} shows the results under the public detection protocol. ByteTrack also outperforms existing results by a large margin. For example, it outperforms TMOH \cite{stadler2021improving} by 6.9 points on MOTA, 9.0 points on IDF1, 7.5 points on HOTA and reduce the identity switches by three quarters, which further indicates the effectiveness of our association method under crowded scenarios. 



\myparagraph{Human in Events.}
Compared with MOT17 and MOT20, HiEve contains more complex events and more diverse camera views. We train ByteTrack on CrowdHuman dataset and the training set of HiEve. ByteTrack also ranks 1st among all the trackers on the leaderboard of HiEve and outperforms other state-of-the-art trackers by a large margin. For example, it increases MOTA from 40.9 to 61.3 and IDF1 from 45.1 to 62.9. The superior results indicate that ByteTrack is robust to complex scenes. 

\begin{table}[t]
\caption{Comparison of the state-of-the-art methods on BDD100K test set. The best results are shown in \textbf{bold}. ByteTrack ranks 1st among all the trackers on the leaderboard of BDD100K. The methods denoted by * are the ones reported on the leaderboard of BDD100K. }
\begin{center}
\scalebox{1.0}{\input{tables/bdd100k}}
\end{center}
\label{table_bdd}
\vspace{-2mm}
\end{table}

\myparagraph{BDD100K.}
BDD100K is a multi-category tracking dataset in autonomous driving scenes. The challenges include low frame rate and large camera motion. We utilize a simple ResNet-50 ImageNet classification model from UniTrack \cite{wang2021different} to compute appearance similarity. ByteTrack ranks first on the leaderboard of BDD100K. It increases mMOTA from 36.6 to 45.5 on the validation set and 35.5 to 40.1 on the test set, which indicates that ByteTrack can also handle the challenges in autonomous driving scenes.

\myparagraph{Qualitative results. } We show some visualization results of difficult cases which ByteTrack is able to handle in Fig.~\ref{fig:vis}. We select 6 sequences from the half validation set of MOT17 and  generate the visualization results using the model with 76.6 MOTA and 79.3 IDF1. The difficult cases include occlusion (\ie MOT17-02, MOT17-04, MOT17-05, MOT17-09, MOT17-13), motion blur (\ie MOT17-10, MOT17-13) and small objects (\ie MOT17-13). The pedestrian in the middle frame with red triangle has low detection score, which is obtained by our association method. The low score boxes not only decrease the number of missing detections, but also play an important role for long-range association. As we can see from all these difficult cases, ByteTrack does not bring any identity switch and preserve the identity effectively. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.9\linewidth]{image/vis_big.pdf}
	\caption{Visualization results of ByteTrack under the 2D MOT setting. We select 6 sequences from the validation set of MOT17 and show the effectiveness of ByteTrack to handle difficult cases such as occlusion and motion blur. The yellow triangle represents the high score box and the red triangle represents the low score box. The same box color represents the same identity. }
	\label{fig:vis}
	\vspace{-4mm}
\end{figure}

\subsection{3D MOT}
We evaluate \modelname under the 3D MOT setting in this section. We first present some experimental results of ablation analysis on the nuScenes dataset. Then, we compare \modelname with other trackers on the validation set and test set of nuScenes. All the experiments are conducted under both camera and LiDAR modalities to verify the generalization ability of \modelname. 

\subsubsection{Ablation Studies}
We present some results of ablation experiments on the design choices of each component such as complementary motion prediction and BYTE data association strategies in this section. We also discuss the effect of hyperparameters including the detection score threshold and the matching threshold. We adopt the detection results from PETRv2 \cite{liu2022petrv2} and CenterPoint \cite{yin2021center} under the camera modality and the LiDAR modality, respectively. 

\myparagraph{Complementary motion prediction. } As mentioned in Sec.~\ref{subsec:motion}, we propose a complementary motion prediction strategy consisting of the bilateral prediction of the Kalman filter and the detected velocity. We evaluate different motion prediction strategies in Table~\ref{table_motion_3d}. We can see that the detected velocity outperforms the Kalman filter motion model by 0.9 MOTA under the camera modality while the Kalman filter surpasses the detected velocity by 0.2 MOTA under the LiDAR modality. This is because LiDAR-based detectors provide more accurate detection results which help the Kalman filter obtain more reliable estimations. The complementary motion prediction achieves higher AMOTA and lower IDS than both the Kalman filter and the detected velocity, which indicates that the detected velocity is suitable for short-term association and the Kalman filter works for long-term association. When we further adaptively update the observation uncertainty matrix in Kalman filter based on the detection confidence following Eq.~\ref{eq:update}, the AMOTA metric is increased to 53.9 and 72.0 under the two modalities, which illustrates that detection helps motion prediction.


\myparagraph{Data association strategy. } We propose a detection-driven hierarchical data association strategy in Algorithm~\ref{algo:byte} of Sec.~\ref{subsec:asso}. We evaluate the effectiveness of the association strategy in Table~\ref{table_ass_3d}. For each modality, the first line marked with ``wo / BYTE'' in Table~\ref{table_ass_3d} means we only perform the high score detection boxes association in  Algorithm~\ref{algo:byte}. The second line marked with ``w / BYTE'' means we adopt the whole process of Algorithm~\ref{algo:byte} including both the high score and low score detection boxes associations. We can see that the low score detection boxes association boosts the 3D tracking performance by 0.3 AMOTA under the camera modality and 0.4 AMOTA under the LiDAR modality. The experimental results illustrate that the detection-driven hierarchical data association strategy is effective under both 2D and 3D settings by mining true objects in the low-score detection boxes. 


\myparagraph{Hyperparameter search. } The detection score threshold and the GIoU matching score threshold are two important hyperparameters in the entire tracking framework. From the left part of Fig.~\ref{fig:hyper} we can see that a lower detection score threshold tends to achieve higher AMOTA as AMOTA needs high recall of objects. However, simply decreasing the detection score threshold may bring a large number of wrong associations which harm the tracking performance. We perform grid search and find that the optimal detection score threshold is 0.25 for camera-based detectors and 0.2 for LiDAR-based detectors. The right part of Fig.~\ref{fig:hyper} shows how the matching score threshold affects AMOTA. We find that the optimal GIoU threshold is around -0.5 under both the LiDAR and Camera settings. We also observe that objects of different classes need different matching score thresholds because they have different sizes and velocities. The dashed line shows the performance of using optimal thresholds for each class, which brings about 0.5 AMOTA improvements. The optimal GIoU threshold for each class is listed in Sec.~\ref{subsec:detail}. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.98\linewidth]{image/hyper.pdf}
	\caption{Comparison of the tracking performance under different detection score thresholds and matching score thresholds. The dashed line represents using the optimal matching score threshold for each class. }
	\label{fig:hyper}
% 	\vspace{-2mm}
\end{figure}

\begin{table}[t]
\caption{Comparison of different motion prediction strategies on nuScenes validation set. We evaluate using both camera and LiDAR modalities. ``DV'' is short of detected velocity. }
\begin{center}
\scalebox{1.0}{\input{tables/motion_3d}}
\end{center}
\label{table_motion_3d}
\vspace{-4mm}
\end{table}


\begin{table}[t]
\caption{Comparison of different data association strategies on the validation set of nuScenes using both camera and LiDAR modalities. }
\begin{center}
\scalebox{1.0}{\input{tables/ass_3d}}
\end{center}
\label{table_ass_3d}
\vspace{-4mm}
\end{table}


\subsubsection{Benchmark Evaluation}
We compare \modelname with the state-of-the-art trackers on the validation set and test set of nuScenes under both camera and LiDAR modalities in Table~\ref{table_nu_camera} and Table~\ref{table_nu_lidar} respectively. 

\myparagraph{Camera modality. } We directly adopt the detection results from PETRv2 \cite{liu2022petrv2} as input. \modelname ranks 1st among all the camera-based methods on both the validation set and the test set of the nuScenes dataset. As is shown in Table~\ref{table_nu_camera}, it achieves 54.2 AMOTA and 696 IDS on the validation set, which surpasses the second one QTrack \cite{yang2022quality} by 3.1 AMOTA and half the number of identity switches. On the test set, it greatly increases AMOTA from 51.9 to 56.4 and decreases IDS by 68\% from 2204 to 704. The low number of identity switches indicates the effectiveness of our tracking algorithm. It also outperforms some LiDAR-based methods such as \cite{weng20203d,chiu2021pro} in Table~\ref{table_nu_lidar}, which significantly narrows the performance gap between camera-based and LiDAR-based methods.

\begin{table}[t]
\caption{Comparison of the state-of-the-art camera-based methods on the nuScenes dataset. The best results are shown in \textbf{bold}. \modelname ranks 1st among all the camera-based trackers on the leaderboard of nuScenes. }
\begin{center}
\scalebox{1.0}{\input{tables/nu_camera}}
\end{center}
\label{table_nu_camera}
\vspace{-3mm}
\end{table}

\myparagraph{LiDAR modality. } We utilize the publicly available CenterPoint \cite{yin2021center} and TransFusion-L \cite{bai2022transfusion} detection results as input on the validation set of nuScenes. For fair comparisons, we list the results using the same CenterPoint detection as all other methods \cite{weng2020ab3dmot,chiu2021pro,yin2021center,benbarka2021score,zaech2022learnable,pang2021simpletrack,wang2021immortal} on the validation set. From Table~\ref{table_nu_lidar} we can see that \modelname* outperforms the second one Immortal \cite{wang2021immortal} by 2.2 AMOTA, 2.3 MOTA and 52\% IDS using the same detection results, which illustrates the advantages of our data association and motion prediction strategies. When equipped with powerful TransFusion-L detection results, it further increases the AMOTA metric to 75.0 and surpasses other methods by a large margin. On the test set of nuScenes, we adopt the same detection results as TransFusion-L. \modelname outperforms TransFusion-L by 1.5 AMOTA and 45\% IDS. The improvements come entirely from our tracking algorithm. \modelname is the first LiDAR-based method which achieves 70 AMOTA and set the new state-of-the-art result on nuScenes. 

\begin{table}[t]
\caption{Comparison of the state-of-the-art LiDAR-based methods on the nuScenes dataset. The best results are shown in \textbf{bold}. \modelname ranks 1st among all the LiDAR-based trackers on the leaderboard of nuScenes. * represents that we use the same CenterPoint \cite{yin2021center} detection as all other methods on the validation set. }
\begin{center}
\scalebox{1.0}{\input{tables/nu_lidar}}
\end{center}
\label{table_nu_lidar}
\vspace{-0mm}
\end{table}


\begin{figure}[!h]
	\centering
	\includegraphics[width=0.9\linewidth]{image/cam_vis.pdf}
	\caption{Visualization results of \modelname under the camera 3D MOT setting. We select one sequence from nuScenes validation set and show the results from 6 different cameras. The same box color represents the same identity. The arrow represents the direction and magnitude of the object velocity. }
	\label{fig:cam_vis}
	\vspace{-0mm}
\end{figure}

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.9\linewidth]{image/lidar_vis.pdf}
	\caption{Visualization results of \modelname under the LiDAR 3D MOT setting. We select 4 sequences from nuScenes validation set. Colors and arrows have similar meanings with those under the camera setting. }
	\label{fig:lidar_vis}
	\vspace{-0mm}
\end{figure}

\myparagraph{Qualitative results. } We show some visualization results of difficult cases which \modelname is able to handle under the camera and LiDAR 3D MOT settings in Fig.~\ref{fig:cam_vis} and Fig.~\ref{fig:lidar_vis} respectively. The difficult cases include occlusion (\ie FONT and FONT\_RIGHT cameras in Fig.~\ref{fig:cam_vis}, SEQ 4 in Fig.~\ref{fig:lidar_vis}) and abrupt motion (\ie FONT\_LEFT camera in Fig.~\ref{fig:cam_vis}, all the sequences in Fig.~\ref{fig:lidar_vis}).  As we can see from all these difficult cases, \modelname does not bring any identity switch and preserve the identity effectively.

