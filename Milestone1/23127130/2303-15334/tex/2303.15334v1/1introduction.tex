\IEEEPARstart{T}{}his work addresses the problem of 2D and 3D multi-object tracking (MOT). Both 2D and 3D multi-object tracking \cite{milan2013continuous,bae2014robust,bewley2016simple,wojke2017simple,luo2018fast,baser2019fantrack,weng20203d} have been longstanding tasks in computer vision. The goal is to estimate the trajectories for objects of interest either in the 2D image plane or in the 3D world coordinates. Successfully solving the problem can benefit many applications such as autonomous driving and intelligent transportation.

2D multi-object tracking and 3D multi-object tracking are inherently intertwined. Both tasks have to localize the objects and obtain object correspondence across different frames. However, they have been independently addressed by researchers from different areas because the input data come from different modalities. 2D MOT is carried out on the image plane and image information serves as an important cue for object correspondence. Appearance-based trackers \cite{bae2014robust,wojke2017simple,zhang2020fairmot,pang2021quasi} extract object appearance features from images and then compute the feature distance as correspondence. 3D MOT is usually performed in the world system which contains the depth information. It is easier to distinguish different objects by spatial similarity such as 3D Intersection over Union (IoU) \cite{weng20203d,weng2020ab3dmot} or point distance \cite{yin2021center}. Fig.~\ref{fig:teasing_3d} shows the visualization of 2D MOT and 3D MOT.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{image/teasing_3d.pdf}
\caption{Illustration of 2D multi-object tracking and 3D multi-object tracking. The first row shows the visualization of 2D MOT, which is performed on the image plane. The second row and the third row show the visualization of 3D MOT from the multi-view images and the Bird's Eye View (BEV) of the LiDAR point clouds, respectively. The same colors represent the same object identities.}
\label{fig:teasing_3d}
\vspace{-5mm}
\end{figure}

We solve the tasks of 2D and 3D MOT with three modules, \textit{i.e.}, detection, motion prediction, and data association. First, an object detector generates 2D / 3D detection boxes and scores. In the beginning frame, the detected objects are initialized as trajectories (or tracklets). Then, a motion predictor such as the Kalman filter \cite{kalman1960new} predicts the location of the tracklets in the following frame. Motion prediction is easy to realize on both image planes and the 3D world space. Finally, the detection boxes are associated with the predicted location of tracklets according to some spatial similarities. 

Detection is the basis of the entire MOT framework. Due to the complex scenarios in videos, detectors are prone to make imperfect predictions. High-score detection boxes usually contain more true positives than low-score ones. However, simply eliminating all low-score boxes is sub-optimal since low-score detection boxes sometimes indicate the existence of objects, \eg the occluded objects. Filtering out these objects causes irreversible errors for MOT and brings non-negligible missing detection and fragmented trajectories, as shown in Row (b) of Fig.~\ref{fig:ass}. 


\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{image/motivation_2d.pdf}
\caption{Examples of our method that associates every detection box. (a) shows all the detection boxes with their scores. (b) shows the tracklets obtained by previous methods that associate detection boxes whose scores are higher than a threshold, \ie 0.5. The same box color represents the same identity. (c) shows the tracklets obtained by our method. The dashed boxes represent the predicted box of the previous tracklets using Kalman filter. The two low-score detection boxes are correctly matched to the previous tracklets based on the large IoU. The number colored in yellow denotes the score of the box.}
\label{fig:ass}
\vspace{-3mm}
\end{figure}

To address the problem of missing detection and fragmented trajectories caused by eliminating low-score boxes, we propose a detection-driven hierarchical data association strategy. It makes full use of detection boxes from high scores to low ones. We identify that the motion similarity between the detection boxes and tracklets provides a strong cue to distinguish the objects from the background in low-score detection boxes. We first associate the high-score detection boxes with the tracklets based on the motion similarity. Similar to \cite{bewley2016simple}, we adopt the Kalman filter to predict the location of the tracklets in the new frame. The similarity can be computed by the 2D or 3D IoU of the predicted box and the detection box. Then, we perform the second association between the unmatched tracklets and the low-score detection boxes using the same motion similarity to recover the true objects and remove the background. The association result is shown in line (c) of Fig.~\ref{fig:ass}.

In 3D MOT especially the driving scenarios, object abrupt motions and short-term disappearing caused by occlusion or blur bring about identity switches. Different from 2D MOT, it is easier for trackers to predict accurate velocities in the world coordinate. We propose a complementary 3D motion prediction strategy to address the problem of object abrupt motions and short-term disappearing. More precise motion prediction tends to achieve more reliable association results and brings gain to the tracking performance. Previous works either use the detected velocities \cite{yin2021center,chen2022polar} or Kalman filter \cite{weng20203d,weng2020ab3dmot} for motion prediction. However, detected velocities have difficulty in performing long-term association as it lacks historical motion information. On the other hand, the Kalman filter produces smoother motion prediction as it leverages historical information. But when encountering abrupt and unpredictable motions or low frame rate videos, it fails to predict accurate locations. We propose a complementary motion prediction method by combining the detected object velocity with Kalman filter. Specifically, we utilize the detected velocity to perform short-term association, which is more robust to abrupt motions. We adopt the Kalman filter to predict a smoother location for each tracklet in each frame. When short-term disappearing happens, Kalman filter can maintain the object location and perform long-term association when the object reappears again. 

In summary, we propose \modelname to solve the 2D and 3D MOT problems. This builds upon our initial work ByteTrack \cite{zhang2022bytetrack}, named for each detection box is a basic unit of the tracklet, as a byte in the computer program. ByteTrack focuses on how to leverage the low-score detection boxes to reduce true object missing and fragmented trajectories in the data association strategy. The key contributions and extended results of \modelname are highlighted below. 

\myparagraph{Unified 2D and 3D Data Association.} We propose a unified data association strategy to solve the 2D and 3D MOT problems. It mines the true objects in low-score detection boxes and alleviates the problems of object missing and fragmented trajectories. Furthermore, it is non-parametric and can be combined with various detectors, making it appealing in real applications.

\myparagraph{Complementary 3D Motion Prediction.} We propose a complementary 3D motion prediction strategy to deal with the challenges of abrupt motions and short-term disappearing of objects. Specifically, we utilize the object velocity predicted by the detector to perform short-term association, which is more robust to abrupt motions. We also adopt the Kalman filter to predict a smoother location for each tracklet and perform long-term association when objects lost and reappear. 

\myparagraph{Thorough Experiments on 3D MOT Benchmarks under Different Modalities. } We conduct detailed experiments on the large-scale nuScenes dataset. The detection-driven hierarchical data association and integrated 3D motion prediction strategies are verified in 3D scenarios. \modelname achieves state-of-the-art performance on the nuScenes tracking task under both camera and LiDAR settings. 

