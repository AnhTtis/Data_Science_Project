{
    "arxiv_id": "2303.10058",
    "paper_title": "No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier",
    "authors": [
        "Zexi Li",
        "Xinyi Shang",
        "Rui He",
        "Tao Lin",
        "Chao Wu"
    ],
    "submission_date": "2023-03-17",
    "revised_dates": [
        "2023-03-20"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.CV"
    ],
    "abstract": "Data heterogeneity is an inherent challenge that hinders the performance of federated learning (FL). Recent studies have identified the biased classifiers of local models as the key bottleneck. Previous attempts have used classifier calibration after FL training, but this approach falls short in improving the poor feature representations caused by training-time classifier biases. Resolving the classifier bias dilemma in FL requires a full understanding of the mechanisms behind the classifier. Recent advances in neural collapse have shown that the classifiers and feature prototypes under perfect training scenarios collapse into an optimal structure called simplex equiangular tight frame (ETF). Building on this neural collapse insight, we propose a solution to the FL's classifier bias problem by utilizing a synthetic and fixed ETF classifier during training. The optimal classifier structure enables all clients to learn unified and optimal feature representations even under extremely heterogeneous data. We devise several effective modules to better adapt the ETF structure in FL, achieving both high generalization and personalization. Extensive experiments demonstrate that our method achieves state-of-the-art performances on CIFAR-10, CIFAR-100, and Tiny-ImageNet.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10058v1"
    ],
    "publication_venue": "preprint"
}