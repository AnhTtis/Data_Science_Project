\section{Introduction}
Deep neural networks (DNNs) have achieved phenomenal success in image analysis and comparable human performance in many semantic segmentation tasks. However, based on the assumption of DNNs, the training and testing data of the network should come from the same probability distribution~\cite{valiant1984theory}. The generalization ability of DNNs on unseen domains is limited. The lack of generalizability hinders the further implementation of DNNs in real-world scenarios. 

\par
Ultrasound (US), as one of the most popular means of medical imaging, is widely used in daily medical practice to diagnose internal organs, such as vascular structures. Compared to other imaging methods, e.g., computed tomography (CT) and magnetic resonance imaging (MRI), US shows its advantages in terms of being radiation-free and portable. However, the US image quality is operator-dependent and sensitive to inter-machine and inter-patient variations. Therefore, the performance of the US segmentation network is often decayed due to the domain shift caused by the inconsistency between the training and test data.
% Addressing the performance degradation caused by domain shift can further elevate the acceptance of DNNs in medical applications and facilitate the automatic measurements and diagnosis in clinical practice.
% Addressing the performance degradation caused by domain shift can further elevate the acceptance of DNNs in medical applications and help doctors automatically get the measurements for diagnosis, e.g., the diameter of vessels.

% To tackle this problem, there are three main folders of approaches in previous publications: (1) data augmentation, (2) image-level domain adaptation, and (3) feature disentanglement.

\par
\subsubsection{Data Augmentation} One of the most common ways of improving the generalization ability of DNNs is to increase the variability of the dataset~\cite{zhang2021understanding}. However, in most clinical cases, the number of data is limited. Therefore, data augmentation is often used as a feasible method to increase diversity. Zhang~\emph{et al.} proposed BigAug~\cite{zhang2020generalizing}, a deep stacked transformation method for 3D medical image augmentation. By applying a wide variety of augmentation methods to the single source training data, they showed the trained network is able to increase its performance on unseen domains. In order to take the physics of US into consideration, Tirindelli~\emph{et al.} proposed a physics-inspired augmentation method to generate realistic US images~\cite{tirindelli2021rethinking}. 
% Instead of randomly selecting the augmentation parameters, Lee~\emph{et al.} implemented a grid search policy to look for the optimized hyperparameters which achieves a better performance than the conventional augmentation strategy~\cite{lee2021principled}.

\subsubsection{Image-Level Domain Adaptation} In order to make the network generalizable to target domains that are different from the source domain, the most intuitive way is to transfer the image style between the target and source domains. The work from Chen~\emph{et al.} achieved impressive segmentation results in MRI to CT adaptation by applying both image and feature level alignment~\cite{chen2020unsupervised}. To increase the robustness of segmentation networks for US images, Yang~\emph{et al.} utilized a rendering network to unify the image styles of training and test data so that the model is able to perform equally well on different domains~\cite{yang2018generalizing}. Velikova~\emph{et al.} extended this idea by defining a common anatomical CT-US space so that the labeled CT data can be exploited to train a segmentation network for US images~\cite{velikova2022cactuss}.
% Unlike the application scenarios of data augmentation, the target domain is known in the case of domain adaptation method. Normally

\subsubsection{Feature Disentanglement} Instead of solving the domain adaptation problem directly at the image-level, many researchers focused on disentangling the features in latent space, forcing the network to learn the shared statistical shape model across different domains~\cite{bengio2013representation}. One way of realizing this is through adversarial learning~\cite{huang2018multimodal}\cite{lee2018diverse}\cite{ning2021new}\cite{zhao2022multi}.
% Inspired by, Ning~\emph{et al.} proposed a bidirectional domain adaptation segmentation network between CT and MRI by using two encoders to extract the domain-sharing and domain-specific features separately~\cite{ning2021new}. In the US scenarios, Zhao~\emph{et al.} adopted the adversarial learning concept to decouple the domain-distinct and domain-universal features to solve the domain shift problem between 2D US and contrast enhanced US images~\cite{zhao2022multi}. 
However, adversarial learning optimization remains difficult and unstable in practice~\cite{lezama2018overcoming}. A promising solution for decoupling latent representations is to minimize a metric that can explicitly measure the shared information between different features. 
Mutual information (MI), which measures the amount of shared information between two random variables~\cite{kraskov2004estimating}, suits this demand. 
Previous researches have exploited its usage in increasing the generalizability for classification networks when solving the vision recognition~\cite{peng2019domain}\cite{liu2021mutual} and US image classification~\cite{meng2020mutual} problems.
% Peng~\emph{et al.} combined both adversarial learning concept and MI-based method to increase the generalizability of the classification network for objects and digits identification in public computer vision database~\cite{peng2019domain}. For a similar vision recognition task, Liu~\emph{et al.} proposed a network structure purely based on MI minimization to achieve the representation disentanglement~\cite{liu2021mutual}. In the medical application field, Meng~\emph{et al.} utilized the MI and achieved a state-of-the-art result on the task of fetal US classification in unseen domains~\cite{meng2020mutual}. 
% However, the potentiality of implementing MI in segmentation networks is still unexplored.
In this study, we investigate the effective way to integrate MI into a segmentation network in order to improve the adaptiveness on unseen images. 
% However, the challenges of modelling MI for introducing disentanglement in segmentation network is unexplored.

\par
In order to solve the performance drop caused by the domain shift in segmentation networks, the aforementioned methods require a known target domain, e.g., CT~\cite{chen2020unsupervised}, MRI~\cite{ning2021new}, contrast enhanced US~\cite{zhao2022multi}. However, compared to MRI and CT, the image quality of US is more unstable and unpredictable. It is frequently observed that the performance of a segmentation network decreases dramatically for the US images acquired from a different machine or even with a different set of acquisition parameters. In such cases, it is impractical to define a so-called target US domain. 
Here we introduce MI-SegNet, an MI-based segmentation network, to address the domain shift problem in US image segmentation. Specifically, the proposed network extracts the disentangled domain (image style) and anatomical (shape) features from US images. The segmentation mask is generated based on the anatomical features, while the domain features are explicitly excluded. Thereby, the segmentation network is able to understand the statistical shape model of the target anatomy and generalize to different unseen scenarios. The ablation study shows that the proposed MI-SegNet is able to increase the generalization ability of the segmentation network in unseen domains.

% Our main contributions are: We introduce MI-SegNet, which utilizes MI as a metric to implicitly disentangle the extracted anatomical and domain features from US images. The anatomical features are used to generate the segmentation mask. In contrast, the domain features and the anatomical features are combined to reconstruct the original US images to ensure that all information is preserved. The feature decoupling is further strengthened by a framework based on data transformation and cross-reconstruction.
% \begin{itemize}
%   \item We tackle the segmentation performance decline caused by domain shifts in US images where there is no information about the target domains. This scenario is more realistic to the actual clinical routine in the sense that the US image styles are unpredictable. 

%   \item We introduce MI-SegNet, which utilizes MI as a metric to implicitly disentangle the extracted anatomical and domain features from US images. The anatomical features are used to generate the segmentation mask. In contrast, the domain features and the anatomical features are combined to reconstruct the original US images to ensure that all information is preserved. The feature decoupling is further strengthened by a framework based on data transformation and cross-reconstruction.
  
%   \item We show that the proposed MI-SegNet is able to increase the generalization ability of the segmentation network in unseen domains. We also investigate the performance of the trained network in downstream tasks when only 5\% of the test dataset is provided for adaptation. The proposed framework demonstrates better results compared to other network structures.
  
%   \item We discuss the particular importance of domain adaptation in the US. Due to the fact of the low speed of sound compared to light and X-ray, the complexity of US imaging and its dependency on many parameters in beamforming is more remarkable than optical imaging, X-ray, and CT. Therefore, the 
  
  
% the complexity of US and its dependency on many parameters is 
% Compared to optimal, X-ray, and CT due to the fact of the low speed of sound related to optical and X-ray. Therefore, the complexity of US imaging and its dependency on many parameters in beamforming makes 
% Because of the speed of x-ray in the tissue is much higher than the speed of sound, the complexity of US and its dependency on 
% Therefore, the complexity of US imaging and its dependency on many parameters including b-forming and gains

% \end{itemize}

\section{Method}
% Our goal is to train a segmentation network that is able to generalize to different unknown domains and serve as a good pre-trained model for domain adaptations while the training dataset only contains images from one single domain. In order to achieve this, the training framework should be designed to encourage the model to focus on the shape of the segmentation target rather than the background or appearance of the images. Following this concept of design, we propose MI-SegNet (Fig.~\ref{Fig:Network}), where two encoders encode the anatomical ($f_a$) and domain ($f_d$) features from the input image separately. The MI between $f_a$ and $f_d$ is minimized to realize the feature disentanglement. To strengthen the context-understanding ability of the segmentation network, disentangled non-local (DNL) blocks are implemented in the skip connections. Besides, to ensure that all the information of the input image is covered, $f_a$ and $f_d$ are combined and passed to a generator network to reconstruct the original image. 

Our goal is to train a segmentation network that can generalize to unseen domains and serve as a good pre-trained model for downstream tasks, while the training dataset only contains images from a single domain. In order to achieve this, the training framework should be designed to encourage the model to focus on the shape of the segmentation target rather than the background or appearance of the images. Following this concept of design, we propose MI-SegNet. During the training phase, a parameterised data transformation procedure is undertaken for each training image ($x$). Two sets of parameters are generated for spatial ($a_1, a_2$) and domain ($d_1, d_2$) transformation respectively. For individual input, four transformed images ($x_{a_1d_1}, x_{a_2d_2}, x_{a_1d_2}, x_{a_2d_1}$) are created according to the four possible combinations of the spatial and domain configuration parameters. Two encoders ($E_a, E_d$) are applied to extract the anatomical features ($f_{a_1}, f_{a_2}$) and domain features ($f_{d_1}, f_{d_2}$) separately. The mutual information between the extracted anatomical features and the domain features from the same image is computed using mutual information neural estimator (MINE)~\cite{belghazi2018mutual} and minimized during training. Only the anatomical features are used to predict segmentation masks ($m_1, m_2$). The extracted anatomical and domain features are then combined and fed into the generator network ($G$) to reconstruct the images ($\widehat{x}_{a_1d_1}, \widehat{x}_{a_1d_2}, \widehat{x}_{a_2d_1}, \widehat{x}_{a_2d_2}$) accordingly. Because the images are transformed explicitly, it is possible to provide direct supervision to the reconstructed images. Notably, only two of the transformed images ($x_{a_1d_1}, x_{a_2d_2}$) are fed into the network, while the other two ($x_{a_1d_2}, x_{a_2d_1}$) are only used as ground truth for reconstructions.
% \begin{figure}
% \includegraphics[width=\textwidth]{images/oldNetwork.PNG}
% \caption{The structure of the proposed MI-SegNet} \label{Fig:oldNetwork}
% \end{figure}
\begin{figure}
\includegraphics[width=\textwidth]{images/Network.PNG}
\caption{
% The network structure with cross reconstruction. The extracted anatomical and domain features of two images are paired in all four possible combinations to reconstruction images with different shapes and styles accordingly.
Network structure of MI-SegNet. The green and blue arrows represent the data flow of the first ($x_{a_1d_1}$) and the second input image ($x_{a_2d_2}$), respectively.
} \label{Fig:Network}
\end{figure}

\subsection{Mutual Information}
In order to decouple the anatomical and domain features intuitively, a metric that can evaluate the dependencies between two variables is needed. Mutual information, by definition, is a metric that measures the amount of information obtained from a random variable by observing another random variable. The MI is defined as the Kullback-Leibler (KL) divergence between the joint distribution and the product of marginal distributions of random variables $f_a$ and $f_d$:
\begin{equation}\label{Eq:MIDefinition}
\mathcal{MI}(f_a;f_d) = \mathcal{KL}(p(f_a,f_d)\|p(f_a)\otimes p(f_d))
\end{equation}
where $p(f_a,f_d)$ is the joint distribution and $p(f_a)\otimes p(f_d)$ is the product of the marginal distributions. Based on the Donsker-Varadhan representation~\cite{DonskerVaradhan1983}, the lower bound of MI can be represented as:
\begin{equation}\label{Eq:DonskerVaradhan}
\mathcal{MI}(f_a;f_d)  \geq E_{p(f_a,f_d)}[\mathcal{T}(f_a,f_d)]-\log(E_{p(f_a)\otimes p(f_d)}[e^{\mathcal{T}(f_a,f_d)}])
\end{equation}
where $\mathcal{T}$ is any arbitrary given continuous function. By replacing $\mathcal{T}$ with a neural network $\mathcal{T}_{\theta_{MINE}}$ and applying Monte Carlo method~\cite{peng2019domain}, the lower bound can be calculated as:
\begin{equation}\label{Eq:MonteCarlo}
\widehat{\mathcal{MI}(f_a;f_d)}=\frac{1}{N}\sum_{i=1}^N \mathcal{T}_{\theta_{MINE}}(f_a,f_d) - \log \frac{1}{N} \sum_{i=1}^N e^{\mathcal{T}_{\theta_{MINE}}(f_a',f_d')}
\end{equation}
where $(f_a,f_d)$ are drawn from the joint distribution and $(f_a',f_d')$ are drawn from the product of marginal distributions. By updating the parameters $\theta_{MINE}$ to maximize the lower bound expression in Eq.~\ref{Eq:MonteCarlo}, a loose estimation of MI is achieved, also known as MINE~\cite{belghazi2018mutual}. 

To force the anatomical and domain encoders to extract decoupled features, the MI is served as a loss to update the weights of these two encoder networks. The loss is defined as:
\begin{equation}\label{Eq:MILoss}
\mathcal{L}_{MI} = \widehat{\mathcal{MI}(f_a;f_d)}
\end{equation}

\subsection{Image Segmentation and Reconstruction}\label{section:ImageRec}
% The segmentation part is accomplished by an anatomical encoder and a segmentor network, where the anatomical encoder
To make the segmentation network independent of the domain information, 
% the segmentor only takes the anatomical features extracted from the input images to generate the segmentation mask.
the domain features are excluded when generating the segmentation mask.
% To precisely segment objects from images, a combined loss function is often resulting better performance [\todo{asdasd}]. 
Here, the segmentation loss $\mathcal{L}_{seg}$ is defined in the combined form of dice loss $\mathcal{L}_{dice}$ and binary cross-entropy loss $\mathcal{L}_{bce}$.  
% Based on the two different sets of spatial transformation parameters ($a_1, a_2$), the two augmented images are anatomically different. The ground truth label is also transformed accordingly to calculate the segmentation loss based on Eq.~\ref{Eq:SegmentationLoss}
\begin{equation}\label{Eq:SegmentationLoss}
\begin{split}
\mathcal{L}_{seg} &= \mathcal{L}_{dice} + \mathcal{L}_{bce}\\
&= 1 - \frac{1}{N}\sum_{n=1}^N \frac{2 l_n m_n + s}{l_n + m_n +s} -\frac{1}{N}\sum_{n=1}^N (l_n \log m_n + (1-l_n)\log (1-m_n))
\end{split}
\end{equation}
where $l$ is the ground truth label, $m$ represents the predicted mask, $s$ is added to ensure the numerical stability, and $N$ is the mini batch size.

% \subsection{Image Reconstruction}
To ensure that the extracted anatomical and domain features can contain all the information of the input image, a generator network is used to reconstruct the image based on both features. The reconstruction loss is then defined as:
\begin{equation}\label{Eq:RecLoss}
\mathcal{L}_{rec} = \frac{1}{N}\sum_{n=1}^N \frac{1}{wh} (x_n-\widehat{x}_n)^2
\end{equation}
where $x_n$ is the ground truth image, $\widehat{x}_n$ is the reconstructed image, $w$ and $h$ are the width and height of the image in pixel accordingly.

\subsection{Data Transformation}
Since the training dataset only contains images from one single domain, it is necessary to enrich the diversity of the training data so that overfitting can be prevented and the generalization ability is increased. The transformation methods are divided into two categories, domain and spatial transformations. Each transformation ($T$) is controlled by two parameters, probability ($p$) and magnitude ($\lambda$).

\subsubsection{Domain Transformations} aim to transfer the single domain images to different domain styles. Five types of transformation methods are involved in this aspect, i.e., \textit{blurriness}, \textit{sharpness}, \textit{noise level}, \textit{brightness}, and \textit{contrast}. The implementations are identical to~\cite{zhang2020generalizing}.
% For blurriness, a 2D Gaussian filter is applied with $\lambda$, ranging between $[0.25,1.5]$, representing the standard deviation of the Gaussian kernel. The sharpness of the image is manipulated based on unsharp masking technique, while the $\lambda$ is in the range of $[10,30]$. To make the model robust against noise, a rayleigh distribution is added to the original images with scale parameter ($\lambda$) ranges between $[20,50]$. The brightness of the image is transformed in the range of $[-25,25]$. The contrast is changed based on gamma correction, where $\lambda$ stands for the gamma value ranging between $[0.5,3]$. 
The possibility of all the domain transformations are empirically set to $10\%$.

\subsubsection{Spatial Transformations} mainly consist of two parts, \textit{crop} and \textit{flip}. For cropping, a window with configurable sizes ($[0.7,0.9]$ of the original image size) is randomly masked on the original image. Then the cropped area is resized to the original size to introduce varying shapes of anatomy. Here $\lambda$ controls the size and the position of the cropping window. Besides cropping, horizontal flipping is also involved. Unlike domain transformations, the labels are also transformed accordingly by the same spatial transformation. The probability ($p$) of flipping is $5\%$, while the $p$ for cropping is $50\%$ to introduce varying anatomy sizes.The images are then transformed in a stacked way:
\begin{equation}\label{Eq:DataAugmentation}
x_{aug} = T^{(P[n],\Lambda[n])}(T^{(P[n-1],\Lambda[n-1])}\cdots (T^{(P[1],\Lambda[1])}(x)))
\end{equation}
where $n=7$ represents the seven different transformation methods involved in our work, $\Lambda=[\lambda_n,\lambda_{n-1},\cdots, \lambda_1]$ represents the magnitude parameter, and $P=[p_n,p_{n-1},\cdots,p_1]$ contains all the probability parameters for each transformations. In our setup, $\Lambda$ and $P$ can be further separated into $a=[\Lambda_a;P_a]$ and $d=[\Lambda_d;P_d]$ for spatial and domain transformations respectively.
% \begin{figure}
% \includegraphics[width=\textwidth]{images/Network.PNG}
% \caption{The network structure with cross reconstruction. The extracted anatomical and domain features of two images are paired in all four possible combinations to reconstruction images with different shapes and styles accordingly.} \label{Fig:Network}
% \end{figure}


\subsection{Cross Reconstruction}\label{section:CrossRec}

% The MI loss indeed forces the two representations to have minimal shared information. However, the minimization of MI between the anatomical and domain features cannot necessarily make both features contain the respective information. By observing the distributions of anatomical and domain feature spaces, we found that the network goes into local optimums frequently, where the domain features are kept constant, and all the information is stored in the anatomical features. Because there is no information in the domain features, the MI between two representations is thus approaching zero. However, this is not our original intention. As a result, we optimized MI-SegNet by introducing cross reconstruction strategy (Fig.~\ref{Fig:Network}).
According to experimental findings, the MI loss indeed forces the two representations to have minimal shared information. However, the minimization of MI between the anatomical and domain features cannot necessarily make both features contain the respective information. The network goes into local optimums frequently, where the domain features are kept constant, and all the information is stored in the anatomical features. Because there is no information in the domain features, the MI between two representations is thus approaching zero. However, this is not our original intention. As a result, cross reconstruction strategy is introduced to tackle this problem.
% During the training phase, two sets of augmentation parameters are generated for spatial ($a_1, a_2$) and domain ($d_1, d_2$) transformations. For each training image, four augmented images ($x_{a_1d_1}, x_{a_2d_2}, x_{a_1d_2}, x_{a_2d_1}$) are created according to the four possible combinations of the spatial and domain configuration parameters. The extracted anatomical and domain features are then combined and fed into the generator network ($G$) to reconstruct the images ($\widehat{x}_{a_1d_1}, \widehat{x}_{a_1d_2}, \widehat{x}_{a_2d_1}, \widehat{x}_{a_2d_2}$) accordingly. Because the images are augmented traceably, it is possible to provide direct supervision to the reconstructed images. 
The cross reconstruction loss will punish the behavior of summarizing all the information into one representation. Thus, it can force each encoder to extract informative features accordingly and prevent the whole network from going into the local optimums. 
The training is conducted in an alternating way, and the detailed process is shown in Algorithm~\ref{algorithm_network_update}.
\begin{algorithm}
\caption{Training process of MI-SegNet with cross reconstruction}~\label{algorithm_network_update}
\begin{algorithmic}[1]
\State $\theta \gets$ initialize network parameters
\Repeat
    \State ($x$, $l$) $\gets$ random mini-batch
    \State ($x_{a_1d_1}$, $x_{a_2d_2}$, $x_{a_1d_2}$, $x_{a_2d_1}$, $l_1$, $l_2$) $\gets$ data augmentation
    \State $f_{a_1}, f_{d_1} \gets E_a(x_{a_1 d_1}), E_d(x_{a_1 d_1})$; $f_{a_2}, f_{d_2} \gets E_a(x_{a_2 d_2}), E_d(x_{a_2 d_2})$
    \State $m_1,m_2 \gets Seg(f_{a_1}),Seg(f_{a_2})$
    \State $\widehat{x}_{a_1d_1},\widehat{x}_{a_2d_2},\widehat{x}_{a_1d_2},\widehat{x}_{a_2d_1}\gets G(f_{a_1}, f_{d_1}),G(f_{a_2}, f_{d_2}),G(f_{a_1}, f_{d_2}),G(f_{a_2}, f_{d_1})$
    % \State $\widehat{x}_{a_1d_1}\gets G(f_{a_1}, f_{d_1})$, $\widehat{x}_{a_2d_2}\gets G(f_{a_2}, f_{d_2})$, $\widehat{x}_{a_1d_2}\gets G(f_{a_1}, f_{d_2})$, $\widehat{x}_{a_2d_1}\gets G(f_{a_2}, f_{d_1})$
    \State Calculate losses:
    \State $[-\widehat{\mathcal{MI}(f_{a_1};f_{d_1})},-\widehat{\mathcal{MI}(f_{a_2};f_{d_2})}]\gets$ Eq.~(\ref{Eq:MonteCarlo})
    \State $[\mathcal{L}_{MI}(f_{a_1},f_{d_1}),\mathcal{L}_{MI}(f_{a_2},f_{d_2})]\gets$ Eq.~(\ref{Eq:MILoss})
    \State $[\mathcal{L}_{seg}(l_1,m_1),\mathcal{L}_{seg}(l_2,m_2)]\gets$ Eq.~(\ref{Eq:SegmentationLoss})
    \State $[\mathcal{L}_{rec}(x_{a_id_j},\widehat{x}_{a_id_j})],~i,j=1,2\gets$ Eq.~(\ref{Eq:RecLoss})
    % \State $[\mathcal{L}_{rec}(x_{a_1d_1},\widehat{x}_{a_1d_1}),\mathcal{L}_{rec}(x_{a_2d_2},\widehat{x}_{a_2d_2}),\mathcal{L}_{rec}(x_{a_1d_2},\widehat{x}_{a_1d_2}),\mathcal{L}_{rec}(x_{a_2d_1},\widehat{x}_{a_2d_1})]\gets$ Eq.~(\ref{Eq:RecLoss})
    
    \State Update parameters:
    \State $\theta_{MINE}\gets \bigtriangledown(-\widehat{\mathcal{MI}(f_{a_1};f_{d_1})} -\widehat{\mathcal{MI}(f_{a_2};f_{d_2})})$
    \State $(\theta_{E_a}, \theta_{E_d}, \theta_{G})\gets \bigtriangledown \left( \sum_{i=1}^2 \sum_{j=1}^2 \mathcal{L}_{rec}(x_{a_id_j},\widehat{x}_{a_id_j}) \right)$
    \State $(\theta_{E_a}, \theta_{E_d})\gets \bigtriangledown \left(\mathcal{L}_{MI}(f_{a_1},f_{d_1})+\mathcal{L}_{MI}(f_{a_2},f_{d_2}) \right)$
    \State $(\theta_{E_a}, \theta_{Seg})\gets \bigtriangledown \left(\mathcal{L}_{seg}(l_1,m_1)+\mathcal{L}_{seg}(l_2,m_2) \right)$
\Until{end}
\end{algorithmic}
\end{algorithm}
\section{Experiments}

\subsection{Implementation Details}\label{section:implementation}
% The anatomical encoder ($E_a$) is modified from ResNet~\cite{he2016deep} while the domain encoder ($E_d$) is adapted from SonoNet~\cite{baumgartner2017sononet}. The dimension of both anatomical and domain features are set to $32$, and the value of each elements are constrained within $[0,1]$. The MINE consists of multiple fully connected layers with ELU as activation function. The segmentor ($Seg$) and generator ($G$) networks have the same structure with multiple up-sampling and convolution layers. The DNL blocks are applied to the skip connections between $E_a$ and $Seg$. We use Adam optimizer with a learning rate of $10^{-4}$ to optimize $\theta_{MINE}$ for MINE and the learning rate is set to $10^{-5}$ to update the weights of encoders ($\theta_{E_a},\theta_{E_d}$), segmentor ($\theta_{Seg}$), and generator ($\theta_G$). The training is carried out on a single GPU (Nvidia TITAN Xp) with 12GB memory. The detailed structure and the complete codes can be found at \url{https://github.com/starbucks-drinker/MI-SegNet}.
The training dataset consists of $2107$ carotid US images of one adult acquired using Siemens Juniper US Machine (ACUSON Juniper, SIEMENS AG, Germany) with a system-predefined "Carotid" acquisition parameter. The test dataset consists of (1) ValS: $200$ carotid US images which are left out from the training dataset, (2) TS1: $538$ carotid US images of $15$ adults from Ultrasonix device, (3) TS2: $433$ US images of $2$ adults and one child from Toshiba device, and (4) TS3: $540$ US images of $6$ adults from Cephasonics device (Cephasonics, California, USA). TS1 and TS2 are from a public database of carotid artery~\cite{vriha2013novel}. Notably, due to the absence of annotations, the publicly accessed images were also annotated by ourselves under the supervision of US experts. 
% ValS and TS3 were recorded and carefully annotated with the joint force of our clinical partners. 
The acquisition was performed within the Institutional Review Board Approval by the Ethical Commission of the Technical University of Munich (reference number 244/19 S).
% , having the volunteer signed an informed consent. 
All the images are resized to $256\times256$ for training and testing.

We use Adam optimizer with a learning rate of $1\times10^{-4}$ to optimize all the parameters. The training is carried out on a single GPU (Nvidia TITAN Xp) with 12GB memory.

\subsection{Performance Comparison on Unseen Datasets}
In this section, we compare the performance of the proposed MI-SegNet with other state-of-art segmentation networks. All the networks are trained on the same dataset described in Section~\ref{section:implementation} with $200$ episodes. 

\subsubsection{Without Adaptation}\label{section:WithoutFineTuning}: The trained models are then tested directly on 4 different datasets described in Section~\ref{section:implementation} without further training or adaptation on the unseen domains. The dice score (DSC) is applied as the evaluation metrics. The results are shown in Table~\ref{tab1}.

\begin{table}
\caption{Performance comparison of the proposed MI-SegNet with different segmentation networks on the US carotid artery datasets without adaptation.}\label{tab1}
  \begin{tabular}{L{0.30\textwidth} C{0.17\textwidth}| C{0.17\textwidth}| C{0.17\textwidth}| C{0.17\textwidth}}
    \toprule
    \multirow{2}{*}{Method}&\multicolumn{4}{c}{DSC}\\
    \cmidrule{2-5}
     & ValS & TS1 & TS2 & TS3\\
    % \multirow{2}{*}{Method} &
    %   \multicolumn{2}{c|}{ValS} &
    %   \multicolumn{2}{c|}{TS1} &
    %   \multicolumn{2}{c|}{TS2} &
    %   \multicolumn{2}{c}{TS3}\\
    %   \cmidrule{2-9}
    %   & {Dice} & {HD} & {Dice} & {HD} & {Dice} & {HD} & {Dice} & {HD} \\
    \midrule
    UNet~\cite{ronneberger2015u}            & 0.920$\pm$0.080 & 0.742$\pm$0.283 & 0.572$\pm$0.388 & 0.529$\pm$0.378\\
    GLFR~\cite{song2022global}              & 0.927$\pm$0.045 & 0.790$\pm$0.175 & 0.676$\pm$0.272 & 0.536$\pm$0.347\\
    Att-UNet~\cite{schlemper2019attention}  & \textbf{0.932$\pm$0.046} & 0.687$\pm$0.254 & 0.602$\pm$0.309 & 0.438$\pm$0.359\\
    MedT~\cite{valanarasu2021medical}       & 0.875$\pm$0.056 & 0.674$\pm$0.178 & 0.583$\pm$0.303 & 0.285$\pm$0.291\\
    MI-SegNet w/o $\mathcal{L}_{MI}$        & 0.928$\pm$0.057 & 0.768$\pm$0.217 & 0.627$\pm$0.346 & 0.620$\pm$0.344 \\
    MI-SegNet w/o cross rec.                & 0.921$\pm$0.050 & 0.790$\pm$0.227 & 0.662$\pm$0.309 & 0.599$\pm$0.344 \\
    MI-SegNet                               & 0.928$\pm$0.046 & \textbf{0.821$\pm$0.146} & \textbf{0.725$\pm$0.215} & \textbf{0.744$\pm$0.251}\\
    \bottomrule
  \end{tabular}
\end{table}
% \begin{table}
% \caption{Performance comparison of the proposed MI-SegNet with different segmentation networks on the US carotid artery datasets without fine-tuning.}\label{tab1}
%   \begin{tabular}{L{0.16\textwidth} C{0.1\textwidth} C{0.1\textwidth}| C{0.1\textwidth} C{0.1\textwidth}| C{0.1\textwidth} C{0.1\textwidth}| C{0.1\textwidth} C{0.1\textwidth}}
%     \toprule
%     \multirow{2}{*}{Method} &
%       \multicolumn{2}{c|}{ValS} &
%       \multicolumn{2}{c|}{TS1} &
%       \multicolumn{2}{c|}{TS2} &
%       \multicolumn{2}{c}{TS3}\\
%       \cmidrule{2-9}
%       & {Dice} & {F1} & {Dice} & {F1} & {Dice} & {F1} & {Dice} & {F1} \\
%       \midrule
%     UNet~\cite{ronneberger2015u}            & 0.920 & 0.921 & 0.741 & 0.742 & 0.572 & 0.569 & 0.529 & 0.527\\
%     GLFR~\cite{song2022global}           & 0.927 & 0.937 & 0.790 & 0.791 & 0.676 & 0.676 & 0.536 & 0.534\\
%     Att-UNet~\cite{schlemper2019attention}  & 0.932 & 0.932 & 0.687 & 0.688 & 0.608 & 0.601 & 0.438 & 0.437\\
%     MedT~\cite{valanarasu2021medical}       & 0.875 & 0.883 & 0.674 & 0.685 & 0.582 & 0.581 & 0.285 & 0.282 \\
%     MI-SegNet                 & 0.933 & 0.934 & \textbf{0.843} & \textbf{0.846} & \textbf{0.771} & \textbf{0.779} & \textbf{0.689} & \textbf{0.692}\\
%     \bottomrule
%   \end{tabular}
% \end{table}
% In the validation dataset (ValS), the MI-SegNet without cross reconstruction loss performs the best with a dice score of $0.939$. As discussed in Section~\ref{section:CrossRec}, if only the normal reconstruction loss is considered, then most likely both anatomical and domain information are contained in the anatomical features. The network is then overfitted and performs poorly on the unseen dataset (TS1, TS2, and TS3) in comparison to MI-SegNet with cross reconstruction. 
Compared to the performance on ValS, all networks demonstrate a performance degradation on unseen datasets (TS1, TS2, and TS3). In order to validate the effectiveness of the MI loss as well as the cross reconstruction design, two ablation networks (MI-SegNet w/o $\mathcal{L}_{MI}$ and MI-SegNet w/o cross rec.) are introduced here for comparison. 
The results on TS1 are the best among all three unseen datasets while the scores on TS3 are the worst for most networks, which indicates that the domain similarity between the source and target domain decreases accordingly from TS1 to TS3.
Fig.~\ref{Fig:VisualComparison} shows the predicted segmentation masks using different network structures.
The MI-SegNet performs the best among others on all three unseen datasets, which showcases the high generalization ability of the proposed framework. 

\begin{figure}[h]
\includegraphics[width=\textwidth]{images/Segmentation_Performance.png}
\caption{
Visual comparison between MI-SegNet and different segmentation networks on the US carotid artery datasets without adaptation. For each row, we show the input US image and its ground truth, and the corresponding output of each network. Red, pink and green regions represent the false negative, false positive and true positive, respectively.
}\label{Fig:VisualComparison}
\end{figure}

\subsubsection{After Adaptation}\label{section:afterFineTuning}: Although the proposed network achieves the best scores when applied directly to unseen domains, performance decay still occurs. Using it directly to unseen datasets with degraded performance is not practical. As a result, adaptation on the target domain is needed. The trained models in Section~\ref{section:WithoutFineTuning} are further trained with $5\%$ data of each unseen test dataset. The adapted models are then tested on the rest $95\%$ of each test dataset. Notably, for the MI-SegNet only the anatomical encoder and segmentor are involved in this adaptation process, which means the network is updated solely based on $\mathcal{L}_{seg}$.

\begin{table}
\caption{Performance comparison of the proposed MI-SegNet with different segmentation networks after adaptation when 5\% data of each test set is involved in the adaptation process.}\label{tab2}
  \begin{tabular}{L{0.24\textwidth} C{0.24\textwidth}| C{0.24\textwidth}| C{0.24\textwidth}}
    \toprule
    \multirow{2}{*}{Method}&\multicolumn{3}{c}{DSC}\\
    \cmidrule{2-4}
     & TS1 & TS2 & TS3\\
    % \multirow{2}{*}{Method} &
    %   \multicolumn{2}{c|}{ValS} &
    %   \multicolumn{2}{c|}{TS1} &
    %   \multicolumn{2}{c|}{TS2} &
    %   \multicolumn{2}{c}{TS3}\\
    %   \cmidrule{2-9}
    %   & {Dice} & {HD} & {Dice} & {HD} & {Dice} & {HD} & {Dice} & {HD} \\
    \midrule
    UNet~\cite{ronneberger2015u}            & $0.890\pm0.183$ & $0.707\pm0.328$ & $0.862\pm0.139$\\
    GLFR~\cite{song2022global}              & $0.915\pm0.154$ & $0.875\pm0.099$ & $0.907\pm0.049$\\
    Att-UNet~\cite{schlemper2019attention}  & $0.916\pm0.117$ & $0.876\pm0.145$ & $0.893\pm0.087$\\
    MedT~\cite{valanarasu2021medical}       & $0.870\pm0.118$ & $0.837\pm0.137$ & $0.795\pm0.170$\\
    MI-SegNet                               & $\mathbf{0.919\pm0.095}$ & $\mathbf{0.881\pm0.111}$ & $\mathbf{0.916\pm0.061}$\\
    \bottomrule
  \end{tabular}
\end{table}

The intention of this experiment is to validate whether the proposed network can serve as a good pre-trained model for the downstream task. A well-trained pre-trained model, which can achieve good results when only a limited amount of annotations is provided, has the potential to release the burden of manual labeling and adapts to different domains with few annotations.
% A good pre-trained model here is defined as a model that is able to converge quickly when only a very limited number of data is provided. 
% As a result, it has the potential to release the burden of manual labelling and adapt to different domains within few fine-tuning episodes. 
Table~\ref{tab2} shows that the MI-SegNet performs the best on all test datasets.
% Since both MI-SegNet have the same structures for segmentation part ($E_a$ & $Seg$) and 
% Based on the segmentation results, the MI-SegNet performs the best among all other segmentation networks in all three unseen datasets.
% Although the scores after adaptation are still not comparable with the scores (Dice: $0.942$) achieved in the validation dataset (ValS), the scores of the adapted models are all around $0.90$, which we consider as an acceptable precision for carotid lumen segmentation in clinical practice.
However, the difference is not that significant as in Table~\ref{tab1} when no data is provided for the target domain. This is partially due to the fact that carotid artery is a relatively easy anatomy for segmentation. It is observed that when more data (10\%) is involved in the adaptation process GLFR and Att-UNet tend to outperform the others and it can be therefore expected when the data size further increases all the networks will perform equally well on each test set.

\section{Discussion and Conclusion}
In this paper, we discuss the particular importance of domain adaptation for US images. Due to the low speed of sound compared to light and X-ray, the complexity of US imaging and its dependency on many parameters are more remarkable than optical imaging, X-ray, and CT. Therefore, the performance decay caused by the domain shift is a prevalent issue when applying DNNs in US images. To address this problem, a MI-based disentanglement method is applied to increase the generalization ability of the segmentation networks for US image segmentation.
The ultimate goal of increasing the generalizability of the segmentation network is to apply the network to different unseen domains directly without any adaptation process. However, from the authors' point of view, training a good pre-trained model that can be adapted to an unseen dataset with minimal annotated data is still meaningful. As demonstrated in Section~\ref{section:afterFineTuning}, the proposed model also shows the best performance in the downstream adaptation tasks. Currently, only the conventional image transformation methods are involved. In the future work, more realistic and US specific image transformations could be implemented to strengthen the feature disentanglement.

% One limitation of the proposed framework is that it can only generalize to anatomies with a regular shape. For plaques or tumors segmentation, where the contours of the targets are irregular, it will most likely fail to achieve a high generalization ability. In such cases, more annotations or prior knowledge should be provided to help the network better understand the scenarios.
