{
    "arxiv_id": "2303.12649",
    "paper_title": "MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization",
    "authors": [
        "Yuan Bi",
        "Zhongliang Jiang",
        "Ricarda Clarenbach",
        "Reza Ghotbi",
        "Angelos Karlas",
        "Nassir Navab"
    ],
    "submission_date": "2023-03-22",
    "revised_dates": [
        "2023-09-11"
    ],
    "latest_version": 2,
    "categories": [
        "eess.IV",
        "cs.CV"
    ],
    "abstract": "Generalization capabilities of learning-based medical image segmentation across domains are currently limited by the performance degradation caused by the domain shift, particularly for ultrasound (US) imaging. The quality of US images heavily relies on carefully tuned acoustic parameters, which vary across sonographers, machines, and settings. To improve the generalizability on US images across domains, we propose MI-SegNet, a novel mutual information (MI) based framework to explicitly disentangle the anatomical and domain feature representations; therefore, robust domain-independent segmentation can be expected. Two encoders are employed to extract the relevant features for the disentanglement. The segmentation only uses the anatomical feature map for its prediction. In order to force the encoders to learn meaningful feature representations a cross-reconstruction method is used during training. Transformations, specific to either domain or anatomy are applied to guide the encoders in their respective feature extraction task. Additionally, any MI present in both feature maps is punished to further promote separate feature spaces. We validate the generalizability of the proposed domain-independent segmentation approach on several datasets with varying parameters and machines. Furthermore, we demonstrate the effectiveness of the proposed MI-SegNet serving as a pre-trained model by comparing it with state-of-the-art networks.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12649v1",
        "http://arxiv.org/pdf/2303.12649v2"
    ],
    "publication_venue": "Accepted by MICCAI 2023"
}