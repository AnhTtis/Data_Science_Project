\section{Discussion}
% \paragraph{Conclusions}
In this paper we proposed the Alias-Free Convnet, which for the first time, is guaranteed to eliminate any aliasing effects in the model, to ensure the output is invariant to any input shifts (even sub-pixel ones), and to ensure the internal representations are equivariant to any shifts (even sub-pixel ones). 
We demonstrate this numerically and show this leads to (certified) high performance under adversarial shift-based attacks --- in contrast to existing models which degrade in performance. 
However, this comes at a cost, such as a 1.08\% reduction in standard test accuracy (as methods that increase robustness often reduce accuracy). 
% We discuss additional limitations in \cref{sec:limitaions-append}.  

\subsection{Accuracy}
Although our model has improved robustness in comparison to the baseline model and other shift-invariance methods, it has a lower accuracy on the original test set. 
This result makes sense since robustness often comes at the cost of accuracy. 
Specifically, the property of perfect shift-invariance is architecturally forced in our model, in contrast to other CNNs where it can be violated. This may seem as a reduction of the hypothesis set.
However, in \Cref{table:aal-modifcations-accuracy} we observe that the highest drop in accuracy does not occur at the last modification, where the model becomes shift-invariant, but rather as a result of modifying the Normalization Layer to be alias-free.
Although the proposed alias-free normalization layer was designed to remain similar to the original model LayerNorm, other alias-free normalization methods may exist and lead to a higher accuracy than ours, without hurting robustness. 
Another drop in accuracy occurs as a result of replacing GeLU in polynomial activation, which surprisingly does not happen in the regular setting (models without cyclic convolutions); a polynomial activation in a cyclic setting leads to a reduction of 0.4\% in accuracy (\cref{table:aal-modifcations-accuracy}) while in a non-cyclic setting it leads to a reduction of 0.1\% only (\cref{table:polynomial-vit-results}). 
It implies that additional hyper-parameter tuning might help in the recovery of this accuracy drop.
%In addition, we note again that the modifications in our model effectively remove non-linearities from the baseline model, and that ConvNeXt-AFC is practically a polynomial of the input with a degree which is the depth of the convnet multiplied by two.  
In addition, we note again that the modifications in our model effectively remove non-linearities from the baseline model, and that ConvNeXt-AFC is practically a polynomial of the input with a degree that is an exponent of the convnet's depth.  
Thus, using a wider or deeper convnet might help in closing the accuracy gap.
% To test the hypothesis that accuracy reduction is a result of the Layer Norm modification, we test a model for which this is the only change. \daniel{where are the results?}
% We leave this kind of normalization layer search as future work.
% We leave examining these possible solutions as future work.



% \subsection{Limitations} \label{sec:limitaions-append}

\subsection{Runtime performance}
Although the AFC model has only a small amount of additional parameters in the polynomial activation function, it has a higher computation cost than the baseline (see \cref{table:baenchmark-results}). The main reason for that is that while the activation function in the baseline model is a single pointwise operation, our activation requires upsampling and downsampling which are rather expensive. This issue has been addressed in a previous study \citep{Karras2021Alias-FreeNetworks}, where a similar scheme for partially alias-free activation has been used. They combined all the required operations to a single CUDA kernel which (according to them) led to a speed-up of at least x20 over native PyTorch implementation, and in total to  x10 speed-up in training time.
Our model training time is only 5 times higher than the baseline training time, therefore it is reasonable to assume such efficient implementation may significantly reduce the training time gap.


\begin{table}[h]
\caption{\textbf{Training and evaluation performance.} Train time was measured on Nvidia A6000 x 8 using the maximal possible batch-size per model due to memory constraints. Evaluation time was measured on a single A6000 with batch-size 256.}
  \label{table:baenchmark-results}

  \centering
\resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lll}
    \toprule

Model & Train time [hours] & Eval time [ms per sample] \\ 
\midrule

ConvNeXt-Baseline \cite{Liu2022A2020s}           & 84        &  1.39  \\
ConvNeXt-APS   \cite{Chaman2020TrulyNetworks}    & 93     &   1.56  \\
ConvNeXt-AFC  (ours)                          & 418    &  9.16 \\
\bottomrule
  \end{tabular}
  }
\end{table}

\subsection{Image translations}
\subsubsection{Circular translations}
The guaranteed robustness in the AFC model is limited to circular shifts, similarly to previous work \citep{Chaman2020TrulyNetworks}. 
Applying this kind of translation on a finite image causes edge artifacts and creates an unrealistic image (e.g., see \cref{fig:circular_crop_shifts}).
Although our model has improved robustness even in translations of the frame with respect to the scene (\cref{fig:crop-shift-bilinear}), which may seem as a more practical setting, information-loss makes guaranteed robustness impossible --- for example, consider an image in which a translation cause the classified object to get out of the frame. 
Although the certified robustness may seem not applicable, circular shifts can actually be practically relevant. 
For example, shifting an object over a constant (i.e., uniform) background will seem identical to a circular translation of the entire frame. 
This setting may be relevant in face recognition tasks and medical imaging (see \cref{fig:medical_shifts}). 
In addition, horizontal circular shifts are relevant for panoramic (360$^{\circ}$) cameras, e.g.~in autonomous cars (see \cref{fig:panoram_shifts}).


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/attack_samples_medical_original.pdf}
    \caption{Original image}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/attack_samples_medical_circ_shift.pdf}
    \caption{Circular shifted image}
    \end{subfigure}
\caption{
 \textbf{ Circular translation of a retinal image.} (a) Original Image \citep{Staal2004Ridge-BasedRetina}. (b) Circular translated image. The retinal image has a uniform background, hence circular shift is equivalent to a translation of the object within the image (i.e.~``crop-shift'').
  }
\label{fig:medical_shifts}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/attack_samples_panorama_original.pdf}
    \caption{Original image}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/attack_samples_panorama_shift.pdf}
    \caption{Circular shifted image}
    \end{subfigure}
\caption{ \textbf{Circular translation of a panoramic image.} (a) Original Image \citep{Armeni2017JointUnderstanding}. (b) Circular translated image, representing a translation of the camera with respect to the scene, without any edge artifacts.
    }
\label{fig:panoram_shifts}
\end{figure}



\subsubsection{Interpolation kernel}
In \Cref{sec:methods} we proved that AFC is robust even to sub-pixel shifts. %, looking at the input at the continuous domain. 
%From Digital signal perspective, as reflects in our experiments,
%(where we do not have access to continuous signals but rather to digital images)
Our robustness guarantees assume the digital image processed by the network corresponds to point-wise samples of a continuous-space image that had been convolved with a perfect anti-aliasing filter prior to sampling (though, empirically, our method performs well with other types of interpolation kernels, e.g. \cref{fig:crop-shift-bilinear}). %1/2T band-limited function we retrieve using sinc interpolation (i.e.~our upsample implementation, \cref{algo:Up-sample}). 
Although this may seem like a serious limitation, it is in fact the standard setting in any imaging system. %as the point-spread function (PSF) of the camera may differ from the sinc kernel. since upsampling is often performed with smaller interpolation kernels (e.g.~bilinear, bicubic), we argue this is actually the required method in imaging systems:   
Indeed, in any optical imaging system, the image impinging on the detector corresponds to the continuous scene convolved with a low-pass filter. 
This low-pass filter completely zeros out all frequencies above a cutoff frequency that is inversely proportional to the diameter of the aperture 
\citep{Goodman1969IntroductionOptics}. Thus, while in many domains perfect low-pass filtering is challenging to achieve, in Optics, this diffraction limit is in fact impossible to avoid. 
Cameras are designed such that the cutoff frequency of the aperture corresponds to the Nyquist frequency of the CCD array. 
In systems where the aperture can be modified by the user, the CCD array is typically adjusted to the minimal aperture width and an additional anti-aliasing filter element is inserted in front of the CCD \citep{Schoberl2010DimensioningCameras}, so that the Nyquist condition is still met for any chosen aperture diameter within the allowed range. It should be noted, however, that the digital image captured by the sensor typically undergoes a series of nonlinear operations within the image signal processor (ISP) of the camera.
This implies that shift equivariance may be lost already at the camera level, before the image reaches our convnet. Addressing these effects is beyond the scope of our work. 

%Had cameras not been designed this way, images captured by them would exhibit Moir√© patterns.

\subsection{Polynomial activation functions}
Polynomial activation functions are relatively cheap to compute, which might motivate their use in the future, regardless of their advantage in the context of aliasing-free convnets. 
Despite this, the high curvature of the polynomial activation seems a barrier to its usage in a wider variety of architectures. 
To achieve good performance on CIFAR, \citet{Gottemukkula2019POLYNOMIALFUNCTIONS} implemented in ResNet \citep{He2015DeepRecognition} a stable version of polynomial activation function, by scaling the pre-activation by the $L_1$ of the layer, which bounds the maximal input element (note that this scaling technique is not aliasing-free and thus was not useful for our purposes). 
We observe this scaling is unnecessary in architectures with sparser usage of activation functions, such as ConvNeXt and ViT; in \Cref{table:polynomial-vit-results}, we show preliminary results suggesting a polynomial activation may be a reasonable substitute for the widely spread GeLU in Transformer-based architectures as well as convnets. 

\subsection{Future work}
This study shows how an aliasing-free convnet can be used to build an image classifier with certified shift robustness. 
Yet, the applications of such convnet are not limited to that purpose; our method can be applied in other domains in which aliasing has been shown to be damaging, such as generative models \citep{Karras2021Alias-FreeNetworks}. 
Furthermore, our method guarantees shift-equivariant internal representation, a stronger property than shift-invariance. 
Future work may examine the importance of this property in other tasks. For example, our method can be naturally expanded to construct a shift-equivariant convnets for segmentation. 

\subsubsection*{Acknowledgments}
 The research of DS was funded by the European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of Schmidt Career Advancement Chair in AI. TM was supported by grant 2318/22 from the Israel Science Foundation and by the Ollendorff Center of the Viterbi Faculty of Electrical and Computer Engineering at the Technion.