\section{Methods}
\label{sec:methods}
\subsection{Shift invariance and equivariance}
Let $\tau_{\Delta}:L^2(\mathbb{R}^2)\to L^2(\mathbb{R}^2)$ be the translation operator, which shifts a continuous-domain two-dimensional signal by $\Delta\in\mathbb{R}^2$. An operator $f:L^2(\mathbb{R}^2)\to L^2(\mathbb{R}^2)$ is said to be \emph{shift-equivariant} if it commutes with $\tau_{\Delta}$ for every $\Delta$. Namely,
\[
% f \circ \tau = \tau \circ f
f \left(  \tau_{\Delta} \left( x \right) \right) = \tau_{\Delta} \left( f \left( x \right) \right)
\]
for every $x\in L^2(\mathbb{R}^2)$ and every $\Delta\in\mathbb{R}^2$. \\
An operator $f:L^2(\mathbb{R}^2)\to\mathbb{R}^d$ is said to be \emph{shift-invariant} if its output is invariant to translation of its input, i.e.
\[
f \left(  \tau_{\Delta} \left( x \right) \right) =  f \left( x \right)\,.
\]

The definitions of equivariance and invariance to translations naturally transfer to discrete-domain signals in $L^2(\mathbb{Z}^2)$ and integer shifts $\Delta\in\mathbb{Z}^2$. 
To simplify notations, from now on we will not specify the domain over which operators are defined, and will also omit the subscript $\Delta$ from $\tau$, whenever the meaning is clear from the context.

CNN architectures for classification commonly comprise a \emph{Feature Extractor}, which is mainly composed of convolution layers, and a \emph{Classifier}, which is typically composed of a linear layer and 
a softmax activation.
A sufficient condition for the model to be shift-invariant is that the \emph{Classifier} be shift-invariant, and the \emph{Feature Extractor} be shift-equivariant. 
This is because the composition of a shift-equivariant $f$ and a shift-invariant $g$ yields a shift-invariant function, as
\[
g\left(f\left(\tau\left(x\right)\right)\right)=g\left(\tau\left(f\left(x\right)\right)\right)=g\left(f\left(x\right)\right)\,.
\]
For our discussion, we assume that the \emph{Classifier} is shift-invariant as its inputs are the spatially-averaged channels.
% \todo{We got a comment about this in the review, the classifier is actually shift-invariant on the channels axis because it has a single value per channel...}
However, the \emph{Feature Extractor} part of CNNs commonly includes also downsampling layers. 
The spatial dimensions of the output of such layers are smaller than the spatial dimensions of their input. 
Therefore, for such layers, shift-equivariance is not a desired property. 
Indeed, when shifting an image by 2 pixels at the input of a layer that performs downsampling by a factor of 2, we expect the output image to shift by only 1 pixel, not 2.
Even worse, when shifting an image by only 1 pixel, it is not clear how precisely the output should shift.
In order to extend the discussion to include these networks, here we consider equivariance w.r.t.~the continuous domain. To simplify the exposition, let us present the definitions for 1D signals, where `discrete' and `continuous' will refer to the signal index we use. 
Namely, a discrete signal $x[n]$ is defined over $n\in\mathbb{Z}$ while a continuous signal $x(t)$ is defined over $t\in\mathbb{R}$.

\begin{definition}[Fractional translation for discrete signals] \label{def:fracShift}
Let  $x[n]$ be a discrete-domain signal and let $\Delta\in\mathbb{R}$ be a (possibly non-integer) shift. 
Then the translation operator $\tau_\Delta$ is defined by $\tau_{\Delta}(x)[n]=z(nT+\Delta)$, where $z(t)$ is the unique $1/2T$-bandlimitted continuous-domain signal satisfying $x[n]=z(nT)$.
\end{definition}

Note that the uniqueness of $z(t)$ in \cref{def:fracShift} is guaranteed by the Nyquist theorem. 
It is also easily verified that this definition does not depend on $T$. 
Equipped with this definition, we can define the following. 

\begin{definition}[shift-equivariance w.r.t.~the cont.~domain]
\label{def:shift-equiv}
An operator $f$ operating on discrete signals is said to be shift-equivariant w.r.t.~the continuous domain if it commutes with fractional shifts. 
Namely, $f (  \tau_{\Delta} ( x ) ) = \tau_{\Delta} ( f ( x ) )$ for every $x\in L^2(\mathbb{Z})$ and every $\Delta\in\mathbb{R}$.

\end{definition}
Similarly, we can define the following.
\begin{definition}[shift-invariance w.r.t.~the cont.~domain]
\label{def:shift-inv}
An operator $f$ operating on discrete signals is said to be shift-invariant w.r.t.~the continuous domain if it is invariant to fractional shifts of its input. 
Namely, $f (  \tau_{\Delta} ( x ) ) = f ( x )$ for every $x\in L^2(\mathbb{Z})$ and every $\Delta\in\mathbb{R}$.
\end{definition}

An important observation is the following.
\begin{proposition} \label{prop:cnn_equivariance_invariance}
In a network comprised of a Feature Extractor and a Classifier, if the Feature Extractor ends with a global average pooling layer, then shift-equivariance w.r.t.~the continuous domain of the Feature Extractor implies shift-invariance  w.r.t.~the continuous domain of the entire model.
\end{proposition}

Indeed, in this case, the Classifier's input is only dependent on the average of the Feature Extractor, which is shift-invariant.
The last statement stems from the fact that when shifting the input of an operator that is shift-equivariant w.r.t.~the continuous domain, the output must be a faithful translated discrete representation of the same continuous signal. 
Namely, there exists some $1/2T$-bandlimited continuous signal $\tilde{f}( t )$  such that $f  (x ) [ n ] = \tilde{f} (nT)$ and  $f ( \tau ( x ) ) [ n ] = \tilde{f} (nT + \Delta ) $. 
Thus, the averages of $f ( x ) [ n]$ and $f ( \tau ( x ) ) [ n ]$ are both equal to the ``DC component'' of $\tilde{f}$, and therefore must be equal.

In order to examine the property of equivariance w.r.t.~continuous domain of CNNs, we shall look at the discrete signal that propagates in a CNN as a representation of a continuous signal, and at each layer as a representation of a continuous operation on the continuous signal.
As shown by \citet{Karras2021Alias-FreeNetworks}, aliasing in the discrete representation prevents shift-invariance of CNNs since it decouples the discrete signal from its continuous equivalent. 
In contrast, they have shown that alias-free operations preserve shift-equivariance w.r.t.~continuous domain, and lead to shift-invariant CNNs.
There, \citet{Karras2021Alias-FreeNetworks} have shown that convolutions and downsamplers which are properly treated using low-pass filters (LPFs), are indeed alias-free and thus shift-equivariant w.r.t.~the continuous domain. 
In addition, they proposed a method to reduce the implicit aliasing of non-linearities which we describe next.

% Non-linearities cause aliasing in discrete signals since, in the continuous domain, they induce new frequencies that may violate the Nyquist condition, so the discrete signal can no longer faithfully represent the continuous signal.
In the continuous domain, pointwise non-linearities may induce indefinitely high new frequencies. 
Applying a pointwise non-linearity in the discrete domain is equivalent to sampling a continuous signal after applying the pointwise non-linearity --- which may break the Nyquist condition and cause aliasing. This implies that pointwise nonlinearities applied in the discrete domain are generally not shift-invariant w.r.t.~the continuous domain. 
Using upsampling before the non-linearity may solve this problem since it increases the frequency support that does not cause aliasing.
However, this approach cannot generally prevent aliasing, since the new frequencies generated by non-linear operations can be arbitrarily high. 
For example, the outputs of non-differentiable operations such as ReLU can have infinite support in the frequency domain, thus aliasing will be induced for every finite upsampling factor.

In this study, we propose replacing non-linear operations with a band-limited preserving alternative --- polynomial functions. 
The proposed scheme for an aliasing-free polynomial function of degree $d$ is defined in \Cref{algo:af-poly}.
In this algorithm, $\mathrm{Upsample}_{z}$ performs upsampling by a factor $z$ (i.e.~resampling the input continuous signal at a $z\times$ larger sampling frequency), $\mathrm{LPF}_{z}$ is an ideal low-pass filter with cut-off $z$, $\mathrm{Downsample}_{z}$ performs downsampling by a factor $z$ (i.e.~dividing the sample frequency by $z$), and 
\begin{equation}
    \mathrm{Poly}_d(x)=\sum_{i=0}^d a_ix^i \,.
\end{equation}


% \begin{algorithm}[tb]
%     \caption{Alias-free polynomial activation}
%     \label{algo:af-poly}
% \begin{algorithmic}
%     \STATE {\bfseries Input:} $x$ - input signal, $\mathrm{Poly}_{d}$ - polynomial of degree $d$.\\
%     %\daniel{define here what is $d$. the bandwidth of $x$?}
%     \STATE $x_{\mathrm{up}} \gets \mathrm{Upsample}_{\frac{d+1}{2}}\left(x\right)$\\
%     \STATE $y_{\mathrm{poly}} \gets \mathrm{Poly}_{d} \left( x_{\mathrm{up}} \right)$ \\
%     \STATE $y_{\mathrm{LPF}} \gets \mathrm{LPF}_{\frac{2}{d+1}}\left(y_{\mathrm{poly}}\right)$\\
%     \STATE $y \gets \mathrm{Downsample}_{\frac{d+1}{2}}\left(y_{{\mathrm{LPF}}}\right)$\\
%     \STATE {\bfseries Output:} $y$
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}[h!]
    \caption{Alias-free polynomial activation}
    \label{algo:af-poly}
\begin{algorithmic}
    \item {\bfseries Input:} $x$ - input signal, $\mathrm{Poly}_{d}$ - polynomial of degree $d$.
    \item $x_{\mathrm{up}} \gets \mathrm{Upsample}_{\frac{d+1}{2}}\left(x\right)$
    \item $y_{\mathrm{poly}} \gets \mathrm{Poly}_{d} \left( x_{\mathrm{up}} \right)$ 
    \item $y_{\mathrm{LPF}} \gets \mathrm{LPF}_{\frac{2}{d+1}}\left(y_{\mathrm{poly}}\right)$
    \item $y \gets \mathrm{Downsample}_{\frac{d+1}{2}}\left(y_{{\mathrm{LPF}}}\right)$
    \item {\bfseries Output:} $y$
\end{algorithmic}
\end{algorithm}


The practical implementations of the operations above are described in \Cref{sec:implementation} and \Cref{sec:implementation-appendix}.
Our contribution to the general framework that has been presented by \citet{Karras2021Alias-FreeNetworks} is the usage of polynomial activations, which extends the frequency bandwidth in a limited fashion, unlike other non-linearities. 
Hence, by using appropriate upsampling as in \Cref{algo:af-poly}, aliasing can be avoided, as described in \Cref{fig:poly-alias}. 
Specifically, in \Cref{sec:shift-invariance-proof} we prove  the following.

\begin{proposition}\label{prop:af-poly-invariance}
    The operator defined by \Cref{algo:af-poly} is shift-equivariant w.r.t.~the continuous domain.
\end{proposition}

By combining \cref{prop:cnn_equivariance_invariance} and \cref{prop:af-poly-invariance} with the shift-equivariance the other layers (as described above), we conclude the network output is shift-invariant. 
Next, we describe the proposed process of non-linearities in the frequency domain, which is additionally demonstrated in \Cref{fig:poly-alias}.
In the first step, the input $x$ is upsampled, leading to a contraction of its support in the frequency domain (\cref{fig:poly-alias}{(b)}). 
Effectively, it expands the range of allowed new frequencies generated by the following non-linearity (\cref{fig:poly-alias}{(c1)}). Then, a low-pass filter is applied in order to prevent aliasing in the following downsampling layer (\cref{fig:poly-alias}{(d1), (e1)}).
Overall, for an upsampling factor that is appropriate for the frequency expansion of the polynomial, the effective frequencies for the output are not being overlapped at any of the steps, thus aliasing is prevented.
However, in the case of non-linearities that do not preserve the band-limited property, upsampling cannot prevent the frequency overlap in (\cref{fig:poly-alias}(c2)).



\begin{figure}[ht]
    \centering
    % \includegraphics[width=0.75\linewidth]{images/convnext_modifications.pdf}
    \includegraphics[width=0.75\linewidth]{images/modification-png-short5.pdf}
    
    \caption{\textbf{ConvNeXt baseline architecture vs AFC modifications.}
    D-conv: depthwise convolution $7 \times 7$, P-Conv: pointwise convolution, Strided-conv: convolution $4 \times 4$, stride $4$.
    LN: Layer Norm, AF-LN: Alias free Layer Norm, Poly: Polynomial activation. Up x2: Upsample x2, LPF: ideal LPF with cutoff 0.5, Down x2: Downsample x2. Detailed explanations about BlurPool, Poly and LPF-Poly activations can be found in \Cref{sec:implementation}.}
    \label{fig:model-modifications}    
\end{figure}


\begin{figure}[ht]
   \centering
   \includegraphics[width=0.98\linewidth]{images/plot8.pdf}

   \caption{\textbf{A demonstration of the proposed non-linearities in the frequency domain.} The top plot at each panel represents the signal in the continuous domain, and the bottom represents the discrete domain.
   Where the input (a) is upsampled it shrinks its frequency response, expanding the allowed frequencies (b).
   Applying the polynomial activation expands the frequency response support by as factor $d$, without causing aliasing in the relevant frequencies (c.1). Thus, the discrete signal remains a faithful representation of the continuous signal after applying LPF (d1) and downsample back to the same spatial size (d2). 
   However, applying GeLU expands the support infinitely (c.2). This leads to an aliasing effect --- interference in the relevant frequencies marked in red in (c2). This causes the discrete signal not to be a correct representation of the continuous one, after LPF (d2) and downsampling (e2).}
   \label{fig:poly-alias}   
  
\end{figure}

