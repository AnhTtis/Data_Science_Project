\section{Limitations} \label{sec:limitaions-append}
\subsection{Accuracy}
Despite our model has improved robustness in comparison to the baseline model and other shift-invariance methods, it has lower accuracy on the original test set. 
This result might make sense since robustness often comes at the cost of accuracy. Specifically, the property of perfect shift-invariance is architecturally forced in our model, in contrast to other CNNs where it can be violated. This may seem as a reduction of the hypothesis set.

However, we observe that the highest drop in accuracy does not occur at the last modification, where the model becomes shift-invariant, but rather as a result of changing the Normalization Layer. 
We note that a modification of the Layer Norm is required to make an alias-free ConvNext. However, other alias-free normalization methods may exist and lead to a higher accuracy than ours, without hurting robustness. 

Another drop in accuracy occurs as a result of replacing GeLU in polynomial activation, which surprisingly does not happen in the regular setting (models without cyclic convolutions). Specifically, a polynomial activation in a non-cyclic setting leads to a reduction of 0.4\% in accuracy (see Tab. 2 in the paper) while in a non-cyclic setting it leads to a reduction of 0.1\% only (see Tab. 3 in the paper). It implies that additional hyper-parameter tuning might help in the recovery of this accuracy drop.

In addition, we note again that the modifications in our model effectively remove non-linearities from the baseline model, and that ConvNeXt-AFC is practically a polynomial of the input with a degree which is the depth of the convnet multiplied by two.  
This implies that using a wider or deeper convnet might help in closing the accuracy gap.
% To test the hypothesis that accuracy reduction is a result of the Layer Norm modification, we test a model for which this is the only change. \daniel{where are the results?}
% We leave this kind of normalization layer search as future work.
We leave examining these possible solutions as future work.

\subsection{Runtime performance}
Our model has a higher computation cost than the baseline (see \cref{table:baenchmark-results}). The main reason for that is that while the activation function in the baseline model is a single pointwise operation, our activation requires upsampling and downsampling which are rather expensive. This issue has been addressed in a previous study \citep{Karras2021Alias-FreeNetworks}, where a similar scheme for partially alias-free activation has been used. They combined all the required operations to a single CUDA kernel which (according to them) led to a speed-up of at least x20 over native PyTorch implementation, and in total to  x10 speed-up in training time.
Our model training time is only 5 times higher than the baseline training time, therefore it is reasonable to assume such efficient implementation may significantly reduce the training time gap.


\begin{table}[h]
\caption{Training and evaluation performance. Train time was measured on Nvidia A6000 x 8 using the maximal possible batch-size. Evaluation time was measured on a single A6000 with batch-size 256.}
  \label{table:baenchmark-results}

  \centering
\resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lll}
    \toprule

Model & Train time [hours] & Eval time [ms per sample] \\ 
\midrule

ConvNeXt-baseline \cite{Liu2022A2020s}           & 84        &  1.39  \\
ConvNeXt-APS   \cite{Chaman2020TrulyNetworks}    & 93     &   1.56  \\
ConvNeXt-AFC  (ours)                          & 418    &  9.16 \\
\bottomrule
  \end{tabular}
  }
\end{table}


