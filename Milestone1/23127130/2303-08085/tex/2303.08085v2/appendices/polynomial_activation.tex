\section{Polynomial activation function} \label{sec:poly-append}
\subsection{Coefficients initialization}
The Polynomial activation function is a point-wise polynomial:
\begin{equation}
    \mathrm{Poly}_2(x)=a_0+a_1x+a_2x^2 \,,
\end{equation}
where the coefficients $\{a_0,a_1,a_2\}$ are trainable parameters, which are shared per-channel.
They were initialized by fitting this function to the GeLU, as proposed by \cite{Gottemukkula2019POLYNOMIALFUNCTIONS}, to function as an approximation to the original activation function ConvNeXt works well with. This initialization gives the function presented in \Cref{fig:plot_poly_channels}.
Yet, the activation function may converge to a completely different function by the end of the training. 
Moreover, it may differ significantly between different layers and between different channels in the same layer. 
\Cref{fig:plot_poly_channels} shows the final activation function for five different channels in the first block of a trained model. 



\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/plot_gelu_poly_init.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/plot_poly_channels.pdf}
    \end{subfigure}

\caption{
 \textbf{ The $\mathrm{Poly}_{2}(x)$ activation function.} In the left panel, we see how the coefficients are initialized to fit GeLU in range $\left[ \sqrt{2},\ -\sqrt{2} \right]$ (dashed lines).
  In the right panel we see how, in the trained model, the activation function may change significantly and converge to a different function in each channel.
  }
% \label{performace_steps_var_0.05}
\label{fig:plot_poly_channels}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.45\linewidth]{images/plot_poly_c_search.pdf}
\caption{
  \textbf{Test accuracy on ImageNet200, non-cyclic convolutions, GeLU and polynomial ConvNeXt with different scales.} Scaling with $c = 7$ gave the best results in the scope of our search.}
\label{fig:polynomial-scale-results-imnet200}
\end{figure}



\subsection{Activation scaling}
During our experiments, we found out that scaling the inputs and outputs of the activation function, regardless of the coefficients themselves, may change the final model results.
Thus, we used the activation function:
\begin{equation}
    \mathrm{Poly}_{c} \left( x \right) =  c \mathrm{Poly}_2(c x)
\end{equation}
and were looking for the optimal scale $c$. 
This scaling factor can effectively be seen as a scaling factor of the weights initialization of the pointwise convolution layers before and after the activation function. 
In \cref{fig:polynomial-scale-results-imnet200} we ran a scan over different scale factors and compared the results of the non-cyclic convolution polynomial model, without additional alias-free modifications (ConvNeXt-Tiny), and on the final model (ConvNeXt-AFC). 
The scan was run on ``ImageNet200'', a subset of ImageNet consisting of 200 classes.
We compare the results to the results of non-cyclic convolution GeLU model. 
In the scope of our search, the best result was achieved with scale $c = 7$ for ConvNeXt-Tiny and scale $c=4$ for ConvNeXt-AFC. 
We used scale $c=7$ for the rest of the polynomial models, as it achieved slightly better results on the full dataset.
% . We ran a smaller search to verify the best scale on ImageNet 200 ($c = 7$) also leads to a reasonable result for the full ImageNet \cref{table:polynomial-scale-results-imnet}. 



% \begin{table}
%   \centering
%     \begin{tabular}{lll}
%         \toprule
%         Model &  &Test acc. \\ 
%         \midrule
%         ConvNeXt-baseline (GeLU) & &  81.48 \\
%         \midrule
%         ConvNeXt-polynomial & $c = 1$          & 81.12 \\
%         ConvNeXt-polynomial & $c = 2$          & 81.93 \\
%         ConvNeXt-polynomial & $c = 3$          & 82.57 \\
%         ConvNeXt-polynomial & $c = 4$          & 82.49 \\
%         ConvNeXt-polynomial & $c = 5$          & 82.7 \\
%         ConvNeXt-polynomial & $c = 6$          & 82.44 \\
%         ConvNeXt-polynomial & \textbf{$c = 7$} & \textbf{82.92} \\
%         ConvNeXt-polynomial & $c = 9$          & 81.28 \\
%         ConvNeXt-polynomial & $c = 11$         & 80.53 \\
%         \bottomrule
%     \end{tabular}
% \caption{Test accuracy on ImageNet200, non-cyclic convolutions, GeLU and polynomial ConvNeXt with different scales. Scaling with $c = 7$ gave the best results in the scope of our search.}
%   \label{table:polynomial-scale-results-imnet200}
% \end{table}

% \begin{table}
%   \centering
%     \begin{tabular}{lll}
%         \toprule
%         Model &  &Test acc. \\ 
%         \midrule
%         ConvNeXt-baseline (GeLU) & &  82.11 \\
%         \midrule
%         ConvNeXt-polynomial & $c = 4$          & 81.76 \\
%         ConvNeXt-polynomial & \textbf{$c = 7$} & \textbf{81.98} \\
%         \bottomrule
%   \end{tabular}
% \caption{Test accuracy on ImageNet, polynomial ConvNeXt with different scale.}
%   \label{table:polynomial-scale-results-imnet}
% \end{table}

% \subsection{Aliasing}
% We note that the polynomial coefficients do not change that bandwidth expansion rate, which is only dependent on the polynomial degree.
% This is 



% \begin{table}%
%   \centering
%   \subfloat[][]{\adjustbox{raise=-5pc}}{
%     \begin{tabular}{lll}
%         \toprule
%         Model &  &Test acc. \\ 
%         \midrule
%         ConvNeXt-baseline (GeLU) & &  82.11 \\
%         \midrule
%         ConvNeXt-polynomial & $c = 4$          & 81.76 \\
%         ConvNeXt-polynomial & \textbf{$c = 7$} & \textbf{81.98} \\
%         \bottomrule
%   \end{tabular}

%   }%
%   % \qquad
%   \subfloat[][]{
%         \begin{tabular}{lll}
%             \toprule
%             Model &  &Test acc. \\ 
%             \midrule
%             ConvNeXt-baseline (GeLU) & &  81.48 \\
%             \midrule
%             ConvNeXt-polynomial & $c = 1$          & 81.12 \\
%             ConvNeXt-polynomial & $c = 2$          & 81.93 \\
%             ConvNeXt-polynomial & $c = 3$          & 82.57 \\
%             ConvNeXt-polynomial & $c = 4$          & 82.49 \\
%             ConvNeXt-polynomial & $c = 5$          & 82.7 \\
%             ConvNeXt-polynomial & $c = 6$          & 82.44 \\
%             ConvNeXt-polynomial & \textbf{$c = 7$} & \textbf{82.92} \\
%             ConvNeXt-polynomial & $c = 9$          & 81.28 \\
%             ConvNeXt-polynomial & $c = 11$         & 80.53 \\
%             \bottomrule
%           \end{tabular}
%   }
%   \caption{Here are some tables in a \texttt{table} environment.}%
%   \label{tbl:table}%
% \end{table}