\section{Implementation}
\label{sec:implementation}
We propose an Alias-Free Convnet (AFC), based on the ConvNeXt architecture \citep{Liu2022A2020s}, which has been shown to achieve state-of-the-art results in image classification tasks. 
We modify the layers which suffer from aliasing (as described in \cref{fig:model-modifications}) so that the convnet is completely free of aliasing. 
% Thus, as we demonstrate in \Cref{sec:experiments}, our model's feature extractor is perfectly shift-equivariant while the classifier is perfectly shift-invariant --- even for sub-pixel shifts.
The theoretical derivation in \Cref{sec:methods} assumes infinite-length discrete signals, hence cannot be directly applied in practical systems. 
However, it can be naturally used by limiting the discussion to circular translations, which implies that the continuous signals are periodic. 
In this case, the theoretical results from \Cref{sec:methods} can be equivalently attained with finite-length signals using our following implementation.

\paragraph{Convolution}
We use circular convolutions to meet the periodic signal assumption, as described above.
This is practically done by replacing zero padding with circular padding, similarly to \citet{Chaman2020TrulyNetworks}.

\paragraph{BlurPool}
Similarly to the model presented by \citet{Zhang2019MakingAgain}, we separate strided convolutions into linear convolution and downsampling operations. 
The downsampling operation is replaced by BlurPool, which applies sub-sampling after low-pass filtering. 
Instead of implementing a low-pass filter using convolutions with custom fixed kernels, we implement an ``ideal low-pass filter'' by truncating high frequencies in the Fourier domain. 
Specifically, we transform the input to the Fourier domain using Pytorch FFT kernel \citep{NEURIPS2019_9015}, zero out the relevant frequencies, and transform it back to the spatial domain. 
This is an efficient implementation of downsampling after applying multiplication with filter $H^{2D}$ in DFT domain, which is defined as 
\begin{equation}
H^{2D} = HH^{T} \,,
\end{equation}
where for stride $s$ and spatial-domain size $N \times N$ , $H$ is defined as
\begin{equation}
H[k]=\begin{cases}
1, & 0\leq k<\frac{N}{2s}\,, \\
0, & \frac{N}{2s}\leq k\leq\frac{3N}{2s}\,,  \\
1, & \frac{3N}{2s}<k\leq N-1\,.
\end{cases} 
\end{equation}
%
A derivation of this filter can be found in \Cref{sec:implementation-appendix}.

\paragraph{Activation function}
We replace the original GeLU activation with a polynomial function of degree 2, whose coefficients are trainable parameters, per channel:
\begin{equation}
    \mathrm{Poly}_2(x)=a_0+a_1x+a_2x^2 \,.
\end{equation}
The coefficients $\{a_0,a_1,a_2\}$ are initialized by fitting this function to the GeLU, as proposed by \citet{Gottemukkula2019POLYNOMIALFUNCTIONS}. 
All activation functions are wrapped according to the alias-free technique presented in \Cref{algo:af-poly}.
Generally, replacing the activation function in a Deep Neural Network may change the scale of the propagated activation, thus requiring adjusting the weight initialization. 
In our experiments the activation scale had a large impact on the achieved accuracy, thus searching for an appropriate scale factor was required.
Details regarding the activation tuning can be found in \cref{sec:poly-append}.
Overall, in our case (polynomial activation in ConvNeXt) using the appropriate scale seemed to recover most or all of the lost accuracy.
%  Generally speaking, the method of upsample before nonlinearity approximates operating the nonlinearity in the continuous domain, where it does not cause aliasing. We show that in the case of bandwidth-limited activation (such as polynomial) this method eliminates the aliasing effect.
% In order to solve the implicit aliasing in non linearities, we performed upsampling before the non linearity, Then performed BlurPool in order to return to the same signal size. When using Polynomial activations, this methodology is aliasing free due to the limited bandwidth of polynomials. 
% Note that for some models applying polynomial activations cause training divergence due to polynomial in stability for inputs with high dynamic range. We saw that using the same methodology with ReLU gave reasonable improvement comparing the regular activation in terms of shift invariance.

\paragraph{Normalization}
ConvNeXt model implementation uses a variation of LayerNorm, which centers and scales each pixel according to its mean and standard deviation over channels, respectively. 
The scaling operation requires the multiplication of each pixel with a different scalar which, like other point-wise non-linearities, is not alias-free. 
We construct an alias-free alternative by using scaling per layer instead of scaling per pixel, i.e.~all pixels are scaled by the standard deviation of the layer, which is shift-equivariant w.r.t.~the continuous domain.
Although eliminating aliasing effects, this modification caused a small reduction in the model accuracy, as we shall see later. 
We hypothesize this reduction results from the ``normalization-per-pixel'' operation functioning as an additional non-linearity, which enlarges the model capacity. 
Yet, this modification is required for the property of shit-equivariance w.r.t. continuous domain, which leads to an overall improvement in terms of robustness to sub-pixel image translations, as shown in \Cref{sec:experiments}.
% This property can be achieved with different kind of normalization layers as well which may not lead to such reduction, hence we leave this investigation for future work.

% \textbf{Convolution separability}
% We could leverage the separation of ConvNext blocks to point-wise and depth-wise convolutions in order to reduce the additional complexity of the activation implementation: We moved the wrapping upsample and downsample operations of the activations to before and after depthwise convolutions. This reduced the number of FFT transformed channels by 4 \hagay{check how to write it}, at the lower cost of applying depth-wise convolutions on larger channels (4 times bigger elements) \hagay{check how to write it}.


\paragraph{First downsample layer}
Unlike other CNN architectures that were examined in the context of aliasing prevention, ConvNeXt does not have a non-linearity before the first downsampling layer. 
Thus, due to the commutativity of convolutions with the LPFs, we cannot replace the first downsampling operation with BlurPool --- since this is equivalent to applying a low-pass filter directly on the input, effectively reducing its resolution. 
Such composition may prevent the model from using high-frequency features and lead to a reduction in the model's accuracy.
To solve this problem, we add an additional activation function before the first BlurPool. 
For computation efficiency, instead of using the regular scheme which requires upsampling before the activation, we replace the usual activation  $\mathrm{Poly}_2(x)$ with
\begin{equation}
    \mathrm{LPFPoly_2}(x)= a_0 + a_1 x + a_2 x \cdot \mathrm{LPF}_{\frac{3}{4}}\left(x \right) \,.
\end{equation}
This modification of the polynomial activation leads to a smaller increase of the signal bandwidth. 
Thus, it does not require upsampling to avoid aliasing when it is followed by an LPF, as in the first BlurPool.
Specifically, since it is followed by a BlurPool with a cutoff $1/4$, The maximally allowed cutoff for the LPF-Poly's filter is $3/4$.
More details on this activation function can be found in \Cref{sec:lpf-poly-append}.
% \daniel{At least give the cut-off frequency here. Anyway we defined the LPF function with a cutoff parameter.}
