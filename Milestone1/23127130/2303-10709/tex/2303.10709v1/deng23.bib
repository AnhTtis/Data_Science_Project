@STRING{aaai    = {Proc.~of the Conference on Advancements of Artificial Intelligence (AAAI)} }
@STRING{aaaiold = {Proc.~of the National Conference on Artificial Intelligence (AAAI)} }
@STRING{aamas   = {Proceedings of the International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)}}
@STRING{ac      = {IEEE Trans. on Automatic Control} }
@STRING{acc     = {Proc.~of the IEEE American Control Conference (ACC)} }
@STRING{accv    = {Proc.~of the Asian Conf.~on Computer Vision (ACCV)} }
@STRING{acmcs   = {ACM Computing Surveys} }
@STRING{acmgraphics={ACM Transactions on Graphics} }
@STRING{acmsuist={ACM Symposium on User Interface Software and Technology} }
@STRING{acra    = {Proc.~of the Australasian Conf.~on Robotics and Automation (ACRA)} }
@STRING{addison = {Addison-Wesley Publishing Inc.} }
@STRING{advancedrobotics={Advanced Robotics} }
@STRING{agsys    = {Agricultural Systems} }
@STRING{ai      = {Artificial Intelligence} }
@STRING{ams     = {Proc.~of Autonome Mobile Systeme} }
@STRING{annurevcontrol = {Annual Review of Control, Robotics, and Autonomous Systems} }
@STRING{ar      = {Autonomous Robots} }
@STRING{arpb = {Annual review of plant biology}}
@STRING{arxiv   = {arXiv preprint} }
@STRING{biosyseng={Biosystems Engineering} }
@STRING{bmcbio = {BMC Bioinformatics} }
@STRING{bmvc    = {Proc.~of British Machine Vision Conference (BMVC)} }
@STRING{case    = {Proc.~of the International Conf.~on Automation Science and Engineering (CASE)} }
@STRING{cacm    = {Communications of the ACM} }
@STRING{ccvw    = {Proc.~of the Croation Computer Vision Workshop (CCVW)} }
@STRING{cdc     = {Proceedings of the Conference on Decision Making and Control (CDC)}}
@STRING{cea     = {Computers and Electronics in Agriculture} }
@STRING{cigr    = {Proc.~of the International Conf.~of Agricultural Engineering (CIGR)} }
@STRING{cira    = {Proc.~of the IEEE Intl.~Symp. on Computer Intelligence in Robotics and Automation (CIRA)} }
@STRING{cob = {Current opinion in biotechnology}}
@STRING{cogsys  = {Proc.~of the Intl.~Conf.~on Cognitive Systems (CogSys)} }
@STRING{corl   = {Proc.~of the Conf.~on Robot Learning (CoRL)} }
@STRING{crews   = {Proc.~of the SPIE Conf.~on Reconnaissance and Electronic Warfare System} }
@STRING{cvgip   = {Computer vision, graphics, and image processing} }
@STRING{cviu    = {Journal of Computer Vision and Image Understanding (CVIU)} }
@STRING{cvpr    = {Proc.~of the IEEE/CVF Conf.~on Computer Vision and Pattern Recognition (CVPR)} }
@STRING{cvprold = {Proc.~of the IEEE Conf.~on Computer Vision and Pattern Recognition (CVPR)} }
@STRING{cvprws    = {Proc.~of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition Workshops} }
@STRING{cvvt    = {Proc.~of the Intl.~Workshop on Computer Vision in Vehicle Technology (CVVT)} }
@STRING{dagm    = {Proc.~of the Symposium of the German Association for Pattern Recognition (DAGM)} }
@STRING{dagstuhl= {Proc.~of the Dagstuhl Seminar} }
@STRING{dars    = {Distributed Autonomous Robotic Systems}}
@STRING{dgpf    = {Proc.~of the Conf.~of the German Society for Photogrammetry, Remote Sensing and Geoinformation (DGPF)} }
@STRING{eccv    = {Proc.~of the Europ.~Conf.~on Computer Vision (ECCV)} }
@STRING{eccvws    = {Proc.~of the Europ.~Conf.~on Computer Vision Workshops}}
@STRING{ecmr    = {Proc.~of the Europ.~Conf.~on Mobile Robotics (ECMR)} }
@STRING{emav    = {Proc.~of the European Micro Aerial Vehicle Conference} }
@STRING{esa     = {Expert Systems with Applications} }
@STRING{etfa    = {Proc.~of the IEEE Int.~Conf.~on Emerging Technologies Factory Automation (ETFA)} }
@STRING{euros   = {Proc.~of the Europ.~Robotics Symp. (EUROS)} }
@STRING{fntr    = {Foundations and Trends in Robotics} }
@STRING{fps = {Frontiers in plant science} }
@STRING{gcpr    = {Proc.~of the German Conf.~on Pattern Recognition (GCPR)} }
@STRING{grsl    = {IEEE Geoscience and Remote Sensing Letters} }
@STRING{humanoids={Proc.~of the IEEE Intl.~Conf.~on Humanoid Robots} }
@STRING{ias     = {Proc. of Int.~Conf.~on Intelligent Autonomous Systems (IAS)} }
@STRING{icar    = {Proc.~of the Int.~Conf.~on Advanced Robotics (ICAR)} }
@STRING{icces   = {Proc.~of the Intl.~Conf.~on Computer Engineering Systems (ICCES)} }
@STRING{iccv    = {Proc.~of the IEEE/CVF Intl.~Conf.~on Computer Vision (ICCV)} }
@STRING{iccvold = {Proc.~of the IEEE Intl.~Conf.~on Computer Vision (ICCV)} }
@STRING{iccvws  = {Proc.~of the Int.~Conf.~on Computer Vision Workshops} }
@STRING{iciap   = {Proc.~of the Intl.~Conf.~on Image Analysis and Processing (ICIAP)} }
@STRING{icif    = {Proc.~of the Int.~Conf.~on Information Fusion} }
@STRING{icip    = {Proc.~of the IEEE Intl.~Conf.~on Image Processing (ICIP)} }
@STRING{iclr    = {Proc.~of the Int.~Conf.~on Learning Representations (ICLR)}}
@STRING{icme    = {Proc.~of the IEEE Intl.~Conf.~on Multimedia and Expo}}
@STRING{icml    = {Proc.~of the Int.~Conf.~on Machine Learning (ICML)} }
@STRING{icpr    = {Proc.~of the Intl.~Conf.~on Pattern Recognition (ICPR)} }
@STRING{icra    = {Proc.~of the IEEE Intl.~Conf.~on Robotics \& Automation (ICRA)} }
@STRING{icuas   = {Proc.~of the Intl.~Conf.~on Unmanned Aircraft Systems (ICUAS)} }
@STRING{ieeepress={IEEE Computer Society Press} }
@STRING{ifac    = {IFAC-PapersOnLine}}
@STRING{ijabe     = {Int.~J.~Agric \& Biol.~Eng.} }
@STRING{ijars   = {Intl.~Journal of Advanced Robotic Systems} }
@STRING{ijcai   = {Proc.~of the Intl.~Conf.~on Artificial Intelligence (IJCAI)} }
@STRING{ijcv    = {Intl.~Journal~of Computer Vision (IJCV)} }
@STRING{ijgi    = {Intl.~Journal of Geo-Information} }
@STRING{ijhr    = {The Int.~Journal of Humanoid Robotics (IJHR)} }
@STRING{ijrr    = {Intl.~Journal~of Robotics Research (IJRR)} }
@STRING{ijrs = {International Journal of Remote Sensing}}
@STRING{ijsip   = {Int. Journal of Signal Processing, Image Processing and Pattern Recognition} }
@STRING{imvip   = {Proc.~of the Irish Machine Vision and Image Processing Conference (IMVIP)} }
@STRING{iros    = {Proc.~of the IEEE/RSJ Intl.~Conf.~on Intelligent Robots and Systems (IROS)} }
@STRING{iser    = {Proc.~of the Intl.~Sym.~on Experimental Robotics (ISER)} }
@STRING{isic    = {Proc.~of the Intl.~Symp. on Intelligent Control (ISIC)}}
@STRING{ismar   = {Proc.~of the Intl.~Symposium~on Mixed and Augmented Reality (ISMAR)} }
@STRING{isprsannals={ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences} }
@STRING{isprsarchives={ISPRS Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences} }
@STRING{isrr    = {Proc.~of the Intl.~Symposium~on Robotic Research (ISRR)} }
@STRING{itsc    = {Proc.~of the IEEE Intl.~Conf.~on Intelligent Transportation Systems (ITSC)} }
@STRING{iv      = {Proc.~of the IEEE Vehicles Symposium (IV)} }
@STRING{ivc     = {Journal on Image and Vision Computing (IVC)} }
@STRING{jaer    = {Journal of Agricultural Engineering Research} }
@STRING{jair    = {Journal of Artificial Intelligence Research (JAIR)} }
@STRING{jbe     = {ASME Journal of Basic Engineering} }
@STRING{jfr     = {Journal of Field Robotics (JFR)} }
@STRING{jirs    = {Journal of Intelligent and Robotic Systems (JIRS)} }
@STRING{jmiv    = {Journal of Mathematical Imaging and Vision} }
@STRING{jmlr = {Journal on Machine Learning Research~(JMLR)}}
@STRING{jmp     = {Mathematical Programming}}
@STRING{joe     = {IEEE Journal of Oceanic Engineering} }
@STRING{jprs    = {ISPRS Journal of Photogrammetry and Remote Sensing (JPRS)} }
@STRING{jra     = {IEEE Journal of Robotics and Automation} }
@STRING{jras    = {Journal on Robotics and Autonomous Systems (RAS)} }
@STRING{jvcir   = {Journal of Visual Communication and Image Representation~(JVCIR)} }
@STRING{lsno    = {Large-Scale Nonlinear Optimization}}
@STRING{m2rsm = {Proc.~of the IEEE Intl.~Workshop on Multi-Platform/Multi-Sensor Remote Sensing and Mapping}}
@STRING{mcg     = {Proc.~of the Intl.~Conf.~on Machine Control and Guidance (MCG)} }
@STRING{mirage  = {Proc.~of the Intl.~Conf.~on Computer Vision/Computer Graphics Collaboration Techniques and Applications (MIRAGE)} }
@STRING{mitpress= {MIT Press} }
@STRING{ml      = {Machine Learning} }
@STRING{mobicom = {Proc.~of the {ACM} Intl.~Conf.~on Mobile Computing and Networking (MobiCom)} }
@STRING{mor     = {Mathematics of Operations Research} }
@STRING{mpc     = {Mathematical Programming Computation}}
@STRING{mva     = {Proc.~of the IAPR Conf.~on Machine Vision Applications (MVA)} }
@STRING{nas     = {National Academy of Sciences}}
@STRING{neurips = {Proc.~of the Conference on Neural Information Processing Systems (NeurIPS)} }
@STRING{neuroc = {Neurocomputing}}
@STRING{nips    = {Proc.~of the Advances in Neural Information Processing Systems (NIPS)} }
@STRING{nipsws  = {Proc.~of the Advances in Neural Information Processing Systems Workshops} }
@STRING{npl = {Neural processing letters} }
@STRING{nrlq = {Naval research logistics quarterly} }
@STRING{oceans  = {Proc.~of OCEANS MTS/IEEE Conference and Exhibition} }
@STRING{paa     = {Pattern Analysis and Applications} }
@STRING{pagri   = {Precision Agriculture} }
@STRING{pami    = {IEEE Trans.~on Pattern Analysis and Machine Intelligence (TPAMI)} }
@STRING{pcv     = {Proc.~of the ISPRS Conference on Photogrammeric Computer Vision (PCV)} }
@STRING{pers    = {Photogrammetric Engineering and Remote Sensing (PE\&RS)} }
@STRING{pfg     = {Photogrammetrie -- Fernerkundung -- Geoinformation (PFG)} }
@STRING{phowo   = {Proc.~of the Photogrammetric Week (PhoWo)} }
@STRING{pia     = {Proc.~of the ISPRS Conference on Photogrammeric Image Analysis (PIA)} }
@STRING{pieee   = {Proceedings of the IEEE} }
@STRING{plantphysiology = {Plant Physiology} }
@STRING{plosone = {PLOS ONE} }
@STRING{pnas    = {Proceedings of the National Academy of Sciences}}
@STRING{pr      = {Pattern Recognition} }
@STRING{prl     = {Pattern Recognition Letters} }
@STRING{ral     = {IEEE Robotics and Automation Letters (RA-L)} }
@STRING{ram     = {IEEE Robotics and Automation Magazine (RAM)} }
@STRING{ras     = {Journal on Robotics and Autonomous Systems (RAS)} }
@STRING{rasmag  = {IEEE Robotics and Automation Magazine} }
@STRING{rs      = {Remote Sensing} }
@STRING{rss     = {Proc.~of Robotics: Science and Systems (RSS)} }
@STRING{rssbook = {Robotics: Science and Systems} }
@STRING{scia    = {Proc.~of the Scandinavian Conference on Image Analysis} }
@STRING{sensors = {Sensors} }
@STRING{siam    = {Society for Industrial and Applied Mathematics (SIAM)}}
@STRING{sice    = {Proc.~of the Annual Conference of the Society of Instrument and Control Engineers (SICE)} }
@STRING{siggraph    = {Proc.~of the Intl.~Conf.~on Computer Graphics and Interactive Techniques (SIGGRAPH)} }
@STRING{sirev   = {SIAM Review (SIREV)}}
@STRING{smc     = {Proc.~of the IEEE Intl.~Conf.~on Systems, Man, and Cybernetics (SMC)} }
@STRING{snowbird= {Proc.~of the Learning Workshop (Snowbird)} }
@STRING{soave   = {Proc.~of the Workshop on Self-Organization of AdaptiVE behavior (SOAVE)} }
@STRING{spiesdvrs={Proc.~of SPIE Stereoscopic Displays and Virtual Reality Systems} }
@STRING{spiev   = {Proc.~of SPIE Videometrics} }
@STRING{springer= {Springer Verlag} }
@STRING{springerstaradvanced={STAR Springer Tracts in Advanced Robotics} }
@STRING{ssns = {Sensors, Systems, and Next-Generation Satellites}}
@STRING{talg    = {ACM Transactions on Algorithms (TALG)}}
@STRING{tarj    = {The Australian Rangeland Journal} }
@STRING{tgrs = {IEEE Trans.~on Geoscience and Remote Sensing}}
@STRING{threedv = {Proc.~of the Intl.~Conf.~on 3D Vision (3DV)} }
@STRING{tip     = {IEEE Trans.~on Image Processing} }
@STRING{tits    = {IEEE Trans.~on Intelligent Transportation Systems (ITS)} }
@STRING{titsmag = {IEEE Trans.~on Intelligent Transportation Systems Magazine} }
@STRING{tiv     = {IEEE Trans.~on Intelligent Vehicles} }
@STRING{tog     = {ACM Trans.~on Graphics (TOG)} }
@STRING{tpami   = {IEEE Trans.~on Pattern Analysis and Machine Intelligence (TPAMI)} }
@STRING{tra     = {IEEE Trans.~on Robotics and Automation} }
@STRING{tro     = {IEEE Trans.~on Robotics (TRO)} }
@STRING{tvcg     = {IEEE Trans.~on Visualization and Computer Graphics} }
@STRING{uai     = {Proc.~of the Conf.~on Uncertainty in Artificial Intelligence (UAI)} }
@STRING{uavg    = {Proc.~of the Intl.~Conf.~on Unmanned Aerial Vehicles in Geomatics} }
@STRING{uust    = {Proc.~of the Intl.~Symp.~on Unmanned Untethered Submersible Technology} }
@STRING{vc      = {The Visual Computer (VC)} }
@STRING{visapp    = {Proc.~of the Intl.~Conf.~of Computer Vision Theory and Applications (VISAPP)} }
@STRING{vmv     = {Proc.~of Vision, Modeling, Visualization (VMV)} }
@STRING{wacv    = {Proc.~of the IEEE Winter Conf.~on Applications of Computer Vision (WACV)} }
@STRING{wafr    = {Intl.~Workshop on the Algorithmic Foundations of Robotics (WAFR)} }
@STRING{wcica     = {Proc.~of the World Congress on Intelligent Control and Automation} }


@misc{Authors14,
 author = {Full Author Name},
 title = {The Frobnicatable Foo Filter},
 note = {Face and Gesture  submission ID 324. Supplied as additional material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {Full Author Name},
 title = {Frobnication Tutorial},
 note = {Supplied as additional material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {Alvin Alpher},
title = {Frobnication},
journal = {Journal of Foo},
volume = 12, 
number = 1, 
pages = {234--778}, 
year = 2002
}

@article{Alpher03,
author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe},
title = {Frobnication Revisited},
journal = {Journal of Foo},
volume = 13, 
number = 1, 
pages = {234--778}, 
year = 2003
}

@article{Alpher04,
author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe and Gavin Gamow},
title = {Can a Machine Frobnicate?},
journal = {Journal of Foo},
volume = 14, 
number = 1, 
pages = {234--778}, 
year = 2004
}

@inproceedings{zhong2023icra,
  title={SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit NEural Representations},
  author={Zhong, Xingguang and Pan, Yue and Behley, Jens and Stachniss, Cyrill},
  booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
  year={2023}
}

@article{vizzo2022sensors,
  author         = {Vizzo, Ignacio and Guadagnino, Tiziano and Behley, Jens and Stachniss, Cyrill},
  title          = {VDBFusion: Flexible and Efficient TSDF Integration of Range Sensor Data},
  journal        = sensors,
  url            = {https://www.mdpi.com/1424-8220/22/3/1296/pdf},
  volume         = {22},
  year           = {2022},
  number         = {3},
  article-number = {1296},
  issn           = {1424-8220},
  abstract       = {Mapping is a crucial task in robotics and a fundamental building block of most mobile systems deployed in the real world. Robots use different environment representations depending on their task and sensor setup. This paper showcases a practical approach to volumetric surface reconstruction based on truncated signed distance functions, also called TSDFs. We revisit the basics of this mapping technique and offer an approach for building effective and efficient real-world mapping systems. In contrast to most state-of-the-art SLAM and mapping approaches, we are making no assumptions on the size of the environment nor the employed range sensor. Unlike most other approaches, we introduce an effective system that works in multiple domains using different sensors. To achieve this, we build upon the Academy-Award-winning OpenVDB library used in filmmaking to realize an effective 3D map representation. Based on this, our proposed system is flexible and highly effective and, in the end, capable of integrating point clouds from a 64-beam LiDAR sensor at 20 frames per second using a single-core CPU. Along with this publication comes an easy-to-use C++ and Python library to quickly and efficiently solve volumetric mapping problems with TSDFs.},
  doi            = {10.3390/s22031296}
}

@inproceedings{vizzo2021icra,
author    = {Ignacio Vizzo and Xieyuanli Chen and Nived Chebrolu and Jens Behley and Cyrill Stachniss},
title     = {{Poisson Surface Reconstruction for LiDAR Odometry and Mapping}},
booktitle = {Proc.~of the IEEE Intl.~Conf.~on Robotics \& Automation (ICRA)},
codeurl   = {https://github.com/PRBonn/puma/},
year      = 2021,
}
Xieyuanli Chen, Andres Milioto, Emanuele Palazzolo, Philippe Giguère, Jens Behley, Cyrill Stachniss
@inproceedings{chen2019iros,
author = {Xieyuanli Chenn and Andres Milioto and Emanuele Palazzolo and Philippe Giguère and Jens Behley and  Cyrill Stachniss},
title = {{SuMa++: Efficient LiDAR-based Semantic SLAM}},
booktitle = {Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)},
year = 2019
}



@article{liu2020nips,
  title={Neural sparse voxel fields},
  author={Liu, Lingjie and Gu, Jiatao and Zaw Lin, Kyaw and Chua, Tat-Seng and Theobalt, Christian},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15651--15663},
  year={2020}
}

@inproceedings{azinovic2022cvpr,
  title={Neural rgb-d surface reconstruction},
  author={Azinovi{\'c}, Dejan and Martin-Brualla, Ricardo and Goldman, Dan B and Nie{\ss}ner, Matthias and Thies, Justus},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6290--6301},
  year={2022}
}

@inproceedings{zhu2022cvpr,
  title={Nice-slam: Neural implicit scalable encoding for slam},
  author={Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Xu, Weiwei and Bao, Hujun and Cui, Zhaopeng and Oswald, Martin R and Pollefeys, Marc},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12786--12796},
  year={2022}
}



@inproceedings{klingensmith2015rss,
  title={Chisel: Real Time Large Scale 3D Reconstruction Onboard a Mobile Device using Spatially Hashed Signed Distance Fields.},
  author={Klingensmith, Matthew and Dryanovski, Ivan and Srinivasa, Siddhartha S and Xiao, Jizhong},
  booktitle={Robotics: science and systems},
  volume={4},
  number={1},
  year={2015},
  organization={Citeseer}
}

@article{ortiz2022arxiv,
  title={isdf: Real-time neural signed distance fields for robot perception},
  author={Ortiz, Joseph and Clegg, Alexander and Dong, Jing and Sucar, Edgar and Novotny, David and Zollhoefer, Michael and Mukadam, Mustafa},
  journal={arXiv preprint arXiv:2204.02296},
  year={2022}
}



@article{wang2021arXiv,
  title={NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction},
  author={Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
  journal={arXiv preprint arXiv:2106.10689},
  year={2021}
}

@article{shi2022city,
  title={City-scale Incremental Neural Mapping with Three-layer Sampling and Panoptic Representation},
  author={Shi, Yongliang and Yang, Runyi and Li, Pengfei and Wu, Zirui and Zhao, Hao and Zhou, Guyue},
  journal={arXiv preprint arXiv:2209.14072},
  year={2022}
}



@inproceedings{yang2022ismar,
  title={Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation},
  author={Yang, Xingrui and Li, Hai and Zhai, Hongjia and Ming, Yuhang and Liu, Yuqian and Zhang, Guofeng},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  pages={499--507},
  year={2022},
}

Milad Ramezani, Yiduo Wang, Marco Camurri, David Wisth, Matias Mattamala, Maurice Fallon
  @inproceedings{ramezani2020iros,
  title     = {The Newer College Dataset: Handheld LiDAR, Inertial and Vision with Ground Truth},
  author    = {Milad Ramezani and Yiduo Wang and Marco Camurri and David Wisth and Matias Mattamala and Maurice Fallon},
  booktitle = iros,
  year      = {2020}
}

%Andreas Geiger; Philip Lenz; Raquel Urtasun
@inproceedings{geiger2012cvpr,
  author    = {Andreas Geiger and Philip Lenz and Raquel Urtasun},
  title     = {{Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite}},
  booktitle = cvprold,
  year      = {2012},
  keywords  = {Autonomous Driving, Dataset, Classification, SLAM, Visual Odometry},
  url       = {http://www.cvlibs.net/publications/Geiger2012CVPR.pdf}
}

@inproceedings{mescheder2019cvpr,
  title={Occupancy networks: Learning 3d reconstruction in function space},
  author={Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
  booktitle=cvpr,
  year={2019}
}

@article{lim2021ral,
    title={Patchwork: Concentric Zone-based Region-wise Ground Segmentation with Ground Likelihood Estimation Using a 3D LiDAR Sensor},
    author={Lim, Hyungtae and Minho, Oh and Myung, Hyun},
    journal={IEEE Robotics and Automation Letters},
    year={2021}
}

@article{vizzo2023ral,
  author    = {Vizzo, Ignacio and Guadagnino, Tiziano and Mersch, Benedikt and Wiesmann, Louis and Behley, Jens and Stachniss, Cyrill},
  title     = {{KISS-ICP: In Defense of Point-to-Point ICP -- Simple, Accurate, and Robust Registration If Done the Right Way}},
  journal   = {IEEE Robotics and Automation Letters (RA-L)},
  pages     = {1-8},
  doi       = {10.1109/LRA.2023.3236571},
  volume    = {8},
  number    = {2},
  year      = {2023},
  codeurl   = {https://github.com/PRBonn/kiss-icp},
}

@inproceedings{behley2018rss, 
		author = {Jens Behley and Cyrill Stachniss},
		title  = {Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments},
		booktitle = {Proc.~of Robotics: Science and Systems~(RSS)},
		year = {2018}  
}
P.J. Besl; 
@article{besl1992pami,
  author   = {P.J. Besl and Neil D. McKay},
  title    = {{A Method for Registration of 3D Shapes}},
  journal  = pami,
  year     = {1992},
  volume   = {14},
  number   = {2},
  pages    = {239--256},
  url      = {http://www-evasion.inrialpes.fr/people/Franck.Hetroy/Teaching/ProjetsImage/2007/Bib/besl_mckay-pami1992.pdf},
  keywords = {Registration},
  abstract = {The authors describe a general-purpose, representation-independent method for the accurate and computationally efficient registration of 3-D shapes including free-form curves and surfaces. The method handles the full six degrees of freedom and is based on the iterative closest point (ICP) algorithm, which requires only a procedure to find the closest point on a geometric entity to a given point. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric, and the rate of convergence is rapid during the first few iterations. Therefore, given an adequate set of initial rotations and translations for a particular class of objects with a certain level of 'shape complexity', one can globally minimize the mean-square distance metric over all six degrees of freedom by testing each initial registration. One important application of this method is to register sensed data from unfixtured rigid objects with an ideal geometric model, prior to shape inspection. Experimental results show the capabilities of the registration algorithm on point sets, curves, and surfaces.}
}


@inproceedings{segal2009rss,
  author    = {Aleksandr Segal and Dirk Hähnel and Sebastian Thrun},
  title     = {{Generalized-ICP}},
  booktitle = rss,
  year      = {2009},
  abstract  = {In this paper we combine the Iterative Closest Point (ICP') and 'point-to-plane ICP' algorithms into a single probabilistic framework. We then use this framework to model locally planar surface structure from both scans instead of just the "model" scan as is typically done with the point-to-plane method. This can be thought of as 'plane-to-plane'. The new approach is tested with both simulated and real-world data and is shown to outperform both standard ICP and point-to-plane. Furthermore, the new approach is shown to be more robust to incorrect correspondences, and thus makes it easier to tune the maximum match distance parameter present in most variants of ICP. In addition to the demonstrated performance improvement, the proposed model allows for more expressive probabilistic models to be incorporated into the ICP framework. While maintaining the speed and simplicity of ICP, the Generalized-ICP could also allow for the addition of outlier terms, measurement noise, and other probabilistic techniques to increase robustness.}
}

@inproceedings{nubert2021icra,
  title={Self-supervised Learning of LiDAR Odometry for Robotic Applications},
  author={Nubert, Julian and Khattak, Shehryar and Hutter, Marco},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2021},
  organization={IEEE}
}

@InProceedings{wang2021cvpr,
    author    = {Wang, Guangming and Wu, Xinrui and Liu, Zhe and Wang, Hesheng},
    title     = {PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical Embedding Mask Optimization},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {15910-15919}
}

@inproceedings{li2019cvpr,
  title={Lo-net: Deep real-time lidar odometry},
  author={Li, Qing and Chen, Shaoyang and Wang, Cheng and Li, Xin and Wen, Chenglu and Cheng, Ming and Li, Jonathan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8473--8482},
  year={2019}
}

@inproceedings{wang2019iros,
  title={Deeppco: End-to-end point cloud odometry through deep parallel neural network},
  author={Wang, Wei and Saputra, Muhamad Risqi U and Zhao, Peijun and Gusmao, Pedro and Yang, Bo and Chen, Changhao and Markham, Andrew and Trigoni, Niki},
  booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3248--3254},
  year={2019},
  organization={IEEE}
}


@inproceedings{mildenhall2020eccv,
  title    = {{NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis}},
  author   = {Ben Mildenhalland Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
  booktitle  = eccv,
  year     = {2020},
  url      = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460392.pdf},
  abstract = {We  present  a  method  that  achieves  state-of-the-art  resultsfor synthesizing novel views of complex scenes by optimizing an under-lying  continuous  volumetric  scene  function  using  a  sparse  set  of  inputviews.  Our  algorithm  represents  a  scene  using  a  fully-connected  (non-convolutional) deep network, whose input is a single continuous 5D coor-dinate (spatial location $(x,y,z)$ and viewing direction $(\theta,\phi)$) and whoseoutput  is  the  volume  density  and  view-dependent  emitted  radiance  atthat spatial location. We synthesize views by querying 5D coordinatesalong camera rays and use classic volume rendering techniques to projectthe output colors and densities into an image. Because volume renderingis naturally differentiable, the only input required to optimize our repre-sentation is a set of images with known camera poses. We describe how toeffectively optimize neural radiance fields to render photorealistic novelviews of scenes with complicated geometry and appearance, and demon-strate results that outperform prior work on neural rendering and viewsynthesis. View synthesis results are best viewed as videos, so we urgereaders to view our supplementary video for convincing comparisons.}
}

@InProceedings{sucar2021iccv,
    author    = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J.},
    title     = {iMAP: Implicit Mapping and Positioning in Real-Time},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {6229-6238}
}

@inproceedings{zhang2014rss,
  author    = {Ji Zhang and Sanjiv Singh},
  title     = {{LOAM: Lidar Odometry and Mapping in Real-time}},
  booktitle = rss,
  year      = {2014},
  abstract  = {We propose a real-time method for odometry and mapping using range measurements from a 2-axis lidar moving in 6-DOF. The problem is hard because the range measurements are received at different times, and errors in motion estimation can cause mis-registration of the resulting point cloud. To date, coherent 3D maps can be built by off-line batch methods, often using loop closure to correct for drift over time. Our method achieves both low-drift and low-computational complexity without the need for high accuracy ranging or inertial measurements. The key idea in obtaining this level of performance is the division of the complex problem of simultaneous localization and mapping, which seeks to optimize a large number of variables simultaneously, by two algorithms. One algorithm performs odometry at a high frequency but low fidelity to estimate velocity of the lidar. Another algorithm runs at a frequency of an order of magnitude lower for fine matching and registration of the point cloud. Combination of the two algorithms allows the method to map in real-time. The method has been evaluated by a large set of experiments as well as on the KITTI odometry benchmark. The results indicate that the method can achieve accuracy at the level of state of the art offline batch methods.},
  timestamp = {2017.08.27},
  url       = {proceedings:zhang2014rss.pdf}
}



@article{1989Using,
  title={Using Occupancy Grids for Mobile Robot Perception and Navigation},
  author={ Alberto Elfes },
  journal={Computer},
  year={1989},
}

@inproceedings{lorensen1987siggraph,
  title     = {{Marching Cubes: a High Resolution 3D Surface Construction Algorithm}},
  author    = {William E. Lorensen and  Harvey E. Cline},
  booktitle = siggraph,
  pages     = {163--169},
  year      = {1987},
  url       = {http://fab.cba.mit.edu/classes/S62.12/docs/Lorensen_marching_cubes.pdf}
}

@inproceedings{izadi2011acmsuist,
  title     = {{KinectFusion: Real-time 3D Reconstruction and Interaction using a Moving Depth Camera}},
  author = {Izadi, Shahram and Kim, David and Hilliges, Otmar and Molyneaux, David and Newcombe, Richard and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Freeman, Dustin and Davison, Andrew and Fitzgibbon, Andrew},
  pages     = {559--568},
  year      = {2011},
  url       = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/kinectfusion-uist-comp.pdf}
}

@inproceedings{chen2021icra,
author = {Xieyuanli Chen and Ignacio Vizzo and Thomas L{\"a}be and Jens Behley and Cyrill Stachniss},
title = {{Range Image-based LiDAR Localization for Autonomous Vehicles}},
booktitle = {Proc.~of the IEEE Intl.~Conf.~on Robotics \& Automation (ICRA)},
year = 2021
}

@inproceedings{chen2020iros,
author = {Xieyuanli Chen and  Thomas L{\"a}be and Lorenzo Nardi and Jens Behley and  Cyrill Stachniss},
title = {{Learning an Overlap-based Observation Model for 3D LiDAR Localization}},
booktitle = {Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)},
year = {2020}
}

@article{wiesmann2021ral,
author = {Louis Wiesmann and Andres Milioto and Xieyuanli Chen and Cyrill Stachniss and Jens Behley},
title = {{Deep Compression for Dense Point Cloud Maps}},
journal = {IEEE Robotics and Automation Letters (RA-L)},
volume = 6,
issue = 2,
pages = {2060-2067},
doi = {10.1109/LRA.2021.3059633},
year = 2021
}

@inproceedings{curless1996siggraph,
  title        = {{A Volumetric Method for Building Complex Models from Range Images}},
  author       = {Curless, Brian, and Marc Levoy},
  booktitle    = siggraph,
  pages        = {303--312},
  year         = {1996},
  organization = {ACM},
  url          = {http://papers.cumincad.org/data/works/att/2ca3.content.pdf}
}

@conference {pfister2000siggraph,
    title={Surfels-Surface Elements as Rendering Primitives},
    author={Hanspeter Pfister and Matthias Zwicker and Jeroen van Baar and Markus Gross},
    booktitle={ACM Transactions on Graphics (Proc. ACM SIGGRAPH)},
    year={2000},
    month={7/2000},
    pages={335--342}
}

@article{kolluri2008talg,
  author   = {Kolluri, Ravikrishna},
  journal  = talg,
  title    = {Provably good moving least squares},
  year     = {2008},
  number   = {2},
  pages    = {18},
  volume   = {4},
  comment  = {This is the mehtod used in IMLS-SLAM},
  keywords = {3D Surface Reconstruction},
  url      = {http://graphics.berkeley.edu/papers/Kolluri-PGM-2005-08/Kolluri-PGM-2005-08.pdf}
}
Kazhdan, Michael, Matthew Bolitho, and Hugues Hoppe.
@inproceedings{kazhdan2006eg,
  title     = {Poisson surface reconstruction},
  author    = {Kazhdan Michael and Matthew Bolitho and Hugues Hoppe.},
  booktitle = {Proceedings of the fourth Eurographics symposium on Geometry processing},
  volume    = {7},
  year      = {2006},
  url       = {https://people.engr.tamu.edu/schaefer/teaching/689_Fall2006/poissonrecon.pdf},
  abstract  = {We show that surface reconstruction from oriented points can be cast as a spatial Poisson problem. This Poissonformulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, andis therefore highly resilient to data noise. Unlike radial basis function schemes, our Poisson approach allows ahierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparselinear system. We describe a spatially adaptive multiscale algorithm whose time and space complexities are pro-portional to the size of the reconstructed model. Experimenting with publicly available scan data, we demonstratereconstruction of surfaces with greater detail than previously achievable.}
}

@article{kazhdan2013acmgraphics,
  author   = {Kazhdan Michael and Hugues Hoppe},
  journal  = acmgraphics,
  title    = {Screened poisson surface reconstruction},
  year     = {2013},
  number   = {3},
  pages    = {1--13},
  volume   = {32},
  keywords = {3D Surface Reconstruction},
  url      = {https://www.cs.jhu.edu/~misha/MyPapers/ToG13.pdf}
}

@article{vespa2018ral,
  author   = {Vespa, Emanuele and Nikolay Nikolov and Marius Grimm and Luigi Nardi and Paul HJ Kelly and Stefan Leutenegger},
  title    = {Efficient Octree-Based Volumetric SLAM Supporting Signed-Distance and Occupancy Mapping},
  journal  = ral,
  year     = 2018,
  keywords = {Mapping, SLAM, Visual-Based Navigation},
  volume   = {3},
  number   = {2},
  pages = {1144--1151},
  abstract = {We present a dense volumetric SLAM framework that uses an octree representation for efficient fusion and rendering of either a truncated signed distance field (TSDF) or occupancy map. The primary aim of this work is to use one single representation of the environment that can be used not only for robot pose tracking, and high-resolution mapping, but seamlessly for planning. We show that our highly efficient octree representation of space fits SLAM and planning purposes in a real-time control loop. In a comprehensive evaluation, we demonstrate dense SLAM accuracy and runtime performance on-par with flat hashing approaches when using TSDF-based maps, and considerable speed-ups when using occupancy mapping compared to standard occupancy maps frameworks. Our SLAM system can run at 10-40 Hz on a modern quadcore CPU, without the need for massive parallelisation on a GPU. We furthermore demonstrate a probabilistic occupancy mapping as an alternative to TSDF mapping in dense SLAM and show its direct applicability to online motion planning, using the example of Informed RRT*.}
}

@article{meagher1980techreport,
  author  = {Donald Meagher},
  year    = {1980},
  journal = {Technical Report},
  title   = {{Octree Encoding: A New Technique for the Representation, Manipulation and Display of Arbitrary 3-D Objects by Computer}}
}

@InProceedings{takikawa2021cvpr,
    author    = {Takikawa, Towaki and Litalien, Joey and Yin, Kangxue and Kreis, Karsten and Loop, Charles and Nowrouzezahrai, Derek and Jacobson, Alec and McGuire, Morgan and Fidler, Sanja},
    title     = {Neural Geometric Level of Detail: Real-Time Rendering With Implicit 3D Shapes},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {11358-11367}
}

@inproceedings{dai2017cvpr,
    title={ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes},
    author={Dai, Angela and Chang, Angel X. and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
    booktitle = {Proc. Computer Vision and Pattern Recognition (CVPR), IEEE},
    year = {2017}
}

@article{change20173dv,
  title={Matterport3D: Learning from RGB-D Data in Indoor Environments},
  author={Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
  journal={International Conference on 3D Vision (3DV)},
  year={2017}
}


@article{mildenhall2019tog,
  title={Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines},
  author={Ben Mildenhall and Pratul P. Srinivasan and Rodrigo Ortiz-Cayon and Nima Khademi Kalantari and Ravi Ramamoorthi and Ren Ng and Abhishek Kar},
  journal={ACM Transactions on Graphics (TOG)},
  year={2019}
}


        
@INPROCEEDINGS {jensen2014cvpr,
author = {Rasmus Jensen and Anders Dahl and George Vogiatzis and Engil Tola and Henrik Aanaes},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Large Scale Multi-view Stereopsis Evaluation},
year = {2014},
volume = {},
issn = {1063-6919},
pages = {406-413},
keywords = {three-dimensional displays;surface reconstruction;image reconstruction;cameras;robot vision systems;accuracy},
doi = {10.1109/CVPR.2014.59},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2014.59},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@article{Knapitsch2017acm,
author = {Knapitsch, Arno and Park, Jaesik and Zhou, Qian-Yi and Koltun, Vladlen},
title = {Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073599},
doi = {10.1145/3072959.3073599},
month = {jul},
articleno = {78},
numpages = {13},
keywords = {structure from motion, image-based reconstruction, large-scale scene reconstruction, multi-view stereo}
}

@inproceedings{yen2020iros,
  title={{iNeRF}: Inverting Neural Radiance Fields for Pose Estimation},
  author={Lin Yen-Chen and Pete Florence and Jonathan T. Barron and Alberto Rodriguez and Phillip Isola and Tsung-Yi Lin},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems ({IROS})},
  year={2021}
}

@article{wangzi2021arXiv,
  title={Ne{RF}$--$: Neural Radiance Fields Without Known Camera Parameters},
  author={Zirui Wang and Shangzhe Wu and Weidi Xie and Min Chen and Victor Adrian Prisacariu},
  journal={arXiv preprint arXiv:2102.07064},
  year={2021}
}

,  
@inproceedings{Lindell2021cvpr,
  title={AutoInt: Automatic Integration for Fast Neural Volume Rendering},
  author={David B. Lindell and Julien N. P. Martel and Gordon Wetzstein},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognitio (CVPR)},
  year={2021},
}

@inproceedings{yu_and_fridovichkeil2021cvpr,
      title={Plenoxels: Radiance Fields without Neural Networks},
      author={{Sara Fridovich-Keil and Alex Yu} and Matthew Tancik and Qinhong Chen and Benjamin Recht and Angjoo Kanazawa},
      year={2022},
      booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognitio (CVPR)},
}

@inproceedings{yu2020cvpr,
      title={{pixelNeRF}: Neural Radiance Fields from One or Few Images},
      author={Alex Yu and Vickie Ye and Matthew Tancik and Angjoo Kanazawa},
      year={2021},
      booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognitio (CVPR)},
}


@inproceedings{Chen2021cvpr,
  title={Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo},
  author={Chen, Anpei and Xu, Zexiang and Zhao, Fuqiang and Zhang, Xiaoshuai and Xiang, Fanbo and Yu, Jingyi and Su, Hao},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={14124--14133},
  year={2021}
}

@InProceedings{Jain2021iccv,
  author = {Jain, Ajay and Tancik, Matthew and Abbeel, Pieter},
  title = {Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month = {October},
  year = {2021},
  pages = {5885-5894}
}


@article{kaizhang2020arXiv,
    author    = {Kai Zhang and Gernot Riegler and Noah Snavely and Vladlen Koltun},
    title     = {NeRF++: Analyzing and Improving Neural Radiance Fields},
    journal   = {arXiv:2010.07492},
    year      = {2020},
}

@inproceedings{xie20213dv,
  author      = {Xie, Christopher and Park, Keunhong and Martin-Brualla, Ricardo and Brown, Matthew},
  title       = {FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling},
  booktitle   = {International Conference on 3D Vision (3DV)},
  year        = {2021},
}



@INPROCEEDINGS{ma2022cvpr,
  author={Ma, Baorui and Liu, Yu-Shen and Zwicker, Matthias and Han, Zhizhong},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Surface Reconstruction from Point Clouds by Learning Predictive Context Priors}, 
  year={2022},
  volume={},
  number={},
  pages={6316-6327},
  doi={10.1109/CVPR52688.2022.00622}}

@article{Rusinkiewicz20013di,
  title={Efficient variants of the ICP algorithm},
  author={Szymon M. Rusinkiewicz and Marc Levoy},
  journal={Proceedings Third International Conference on 3-D Digital Imaging and Modeling},
  year={2001},
  pages={145-152}
}
