\section{Validation of Results} \label{sec:rw-kb}

\ignore{
\begin{table*}[h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccccccc}
& & & & & & & & & & \multicolumn{2}{c}{\textcolor{blue}{In-db}} & \multicolumn{2}{c}{\textcolor{violet}{In-memory}} \\
\toprule
Family & Name & \nrule & \nfact & \texttt{n-pred} & \arity & \nshape & \tparse & \tgraph & \tcomp & \textcolor{blue}{\tshape} & \textcolor{blue}{\ttotal} & \textcolor{violet}{\tshape}  & \textcolor{violet}{\ttotal} \\
\midrule
\multirow{3}{*}{Deep} & \texttt{deep-100} & 4,241 & \multirow{3}{*}{1,000} & \multirow{3}{*}{1,299} & \multirow{3}{*}{4} & \multirow{3}{*}{1,000} & 214 & 90 & 10 & \multirow{3}{*}{6,641} & 6,957 & 447 & \HL{763} 
\\%\cline{2-3}\cline{8-10}\cline{12-12}
& \texttt{deep-200} & 4,541 &  &  & & & 265 & 116 & 9 & & 7,033 & 447 & \HL{839}
\\%\cline{2-3}\cline{8-10}\cline{12-12}
& \texttt{deep-300} & 4,841 &  & & & & 234 & 100 & 11 & & 6,986 & 500 & \HL{846}
\\\midrule
\multirow{4}{*}{LUBM} & \lubmone & \multirow{4}{*}{137} & 99,547 & \multirow{4}{*}{104} & \multirow{4}{*}{1.45 [1,2]} & \multirow{4}{*}{30} & 84 & 10 & 1 & 221 & \HL{318} & 2,724 & 2,820\\%\cline{1-1}\cline{3-3}\cline{7-11}
& \lubmtwo &  & 1,272,575 &  &  &  & 46 & 10 & 1 & 830 & \HL{889} & 10,943 & 11,002\\%\cline{1-1}\cline{3-3}\cline{7-11}
& \lubmthree &  & 13,405,381 &  &  & & 45 & 11 & 1 & 6,396 & \HL{6,454} & 70,131 & 70,189 \\%\cline{1-1}\cline{3-3}\cline{7-11}
& \lubmfour &  & 133,573,854 &  & &  & 43 & 231 & 80 & 65,578 & 
\HL{65,932} & 854,015 & 854,369\\
\midrule
\multirow{2}{*}{\ibench} & \ibenchont & 785 & 2,146,490 & 662 & 3.78 [1,11] & 245 & 179 & 35 & 8 & 11,726 & \HL{11,949} & 15,761 & 
15,984\\
& \ibenchstd & 231 & 1,109,037 & 287
 & 3.76 [1,10] & 129 & 78 & 18 & 7 & 4,991 & \HL{5,096} & 7,379 & 7,484\\
\bottomrule
\end{tabular}}
\vspace{0.3cm}
\caption{A summary of the results for checking the chase termination for real-world knowledge bases. The time parameters are in ms.}
\vspace{-3mm}
\label{tab:real}
\end{table*}}


In Sections~\ref{sec:slinear-ex} and~\ref{sec:linear-ex}, we have presented an experimental evaluation of the algorithms $\mathsf{IsChaseFinite[SL]}$ and $\mathsf{IsChaseFinite[L]}$ using synthetically generated databases and sets of TGDs. 
%Our intention was to stress test those algorithms with aim of understanding which input parameters affect their performance, clarifying whether they can be applied in practice, and revealing their performance limitations. 
In this final section, we use databases and sets of TGDs that are available in the literature with the aim of validating the main outcome of the stress test analysis performed using the synthetic scenarios. 
%In what follows, we discuss only $\mathsf{IsChaseFinite[L]}$ since the 

\subsection{Adopted Scenarios}\label{sec:kb}

We considered three families of databases and sets of TGDs that are briefly discussed below. They are also summarized in Table~\ref{tab:param}, where we report some statistics about (i) the underlying schema, i.e., number of predicates (\texttt{n-pred}) and arity of predicates (\texttt{arity}), (ii) the size of the database, i.e., number of atoms (\nfact) and number of shapes (\nshape), and (iii) the number of TGDs (\nrule).


%\begin{itemize}%[leftmargin=*]

\medskip

\noindent 
\textbf{Deep.}
%\item \textbf{Deep:} 
This family collects sets of simple-linear TGDs that are at the same time weakly-acyclic~\cite{BKMMPST17}. It has been developed to test scenarios with a large number of chase applications and large source instances, as well as a significant number of source-to-target TGDs and target TGDs in a data exchange setting.

\medskip

\noindent 
\textbf{LUBM.}
%\item \textbf{LUBM:} 
This is a popular benchmark consisting of an ontology modelled using the central Description Logic (DL) EL, called Univ-Bench, and a data generator, called UBA, for generating synthetic data over the vocabulary of Univ-Bench~\cite{guo2005lubm}. 
%It can also be used to generate complex TGDs to test the chase procedure~\cite{benedikt2017benchmarking}. 
We use four members of the LUBM family, namely \lubmone, \lubmtwo, \lubmthree, and \lubmfour.
%
Let us clarify that the DL axioms occurring in Univ-Bench can be easily converted into TGDs with one atom in the head that do not repeat variables in an atom. However, not all the axioms lead to linear TGDs, and thus, we kept only those that can be converted into linear TGDs (which are also simple-linear).
%These are DL-Lite knowledge bases. For our experiments, we convert them to linear tgds by creating multiple tgds for tgds with multiple heads, which does not impact the chase w.r.t. its termination. 
% Removing the rules with multiple atoms in their bodies, dllite r, approximating them to dllite fragment EL, exlain in more detail about teminatiion presevtaion with the change

\medskip

\noindent 
\textbf{\ibench.}
 %\item \textbf{\ibench:} 
 This is a framework for generating dependencies such as TGDs with tuning parameters that can control a wide range of properties~\cite{arocena2015ibench}. For our experiments, we use the following sets of simple-linear TGDs generated using \ibench: (i) STB-128, derived from an earlier STBenchmark~\cite{ATV08}, and is the smaller scenario of the family, and (ii) ONT-256, a scenario that has several times larger source instances.
%
For both of those sets of TGDs, we used the databases from~\cite{BKMMPST17} that were generated using the data generator in~\cite{barbosa2002toxgene} and consists of 1000 tuples per source relation. 
%
%Note that both STD-128 and ONT-256 consist of simple-linear TGDs, but we use them to also evaluate $\mathsf{IsChaseFinite[L]}$.
%However, in our experiments, we use them to evaluate the algorithm $\mathsf{IsChaseFinite[L]}$ for linear TGDs. We do not experiment with apply the graph-based algorithm for simple-linear tgds as it runs in few ms for all the ontologies in this category. 
%\end{itemize}

\medskip


Let us remark that in what follows we discuss our experimental evaluation of the algorithm $\mathsf{IsChaseFinite[L]}$ for linear TGDs.
%
Concerning the algorithm $\mathsf{IsChaseFinite[SL]}$ for simple-linear TGDs, there is not much to discuss other than the fact that it runs in a few milliseconds for all the scenarios discussed above. This is a confirmation that for simple-linear TGDs, checking for the finiteness of the chase instance can be done very efficiently.


\begin{table}[t]
	\centering
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{llccccc}
			\toprule
			Family & Name & \texttt{n-pred} & \texttt{arity} & \nfact & \nshape & \nrule \\
			\midrule
			\multirow{3}{*}{Deep} & \texttt{Deep-100} & \multirow{3}{*}{1299} & \multirow{3}{*}{4} & \multirow{3}{*}{1000} & \multirow{3}{*}{1000} & 4241
			\\%\cline{2-3}\cline{8-10}\cline{12-12}
			& \texttt{Deep-200} &  &  &  & & 4541
			\\%\cline{2-3}\cline{8-10}\cline{12-12}
			& \texttt{Deep-300} &  &  & & & 4841
			\\\midrule
			\multirow{4}{*}{LUBM} & \lubmone & \multirow{4}{*}{104} & \multirow{4}{*}{[1,2]}  & 99547 & \multirow{4}{*}{30} & \multirow{4}{*}{137}\\%\cline{1-1}\cline{3-3}\cline{7-11}
			& \lubmtwo &  &  & 1272575 &  &\\%\cline{1-1}\cline{3-3}\cline{7-11}
			& \lubmthree &  &  & 13405381 &  & \\%\cline{1-1}\cline{3-3}\cline{7-11}
			& \lubmfour &  &  & 133573854 & & \\
			\midrule
			\multirow{2}{*}{\ibench} & \ibenchstd & 287 & [1,10] & 1109037
			& 129 & 231\\
			& \ibenchont & 662 & [1,11] & 2146490 & 245 & 785\\
			\bottomrule
	\end{tabular}}
	%\vspace{-0.1cm}
	\medskip
	\caption{The families Deep, LUMB, and iBench.}
	%\vspace{-3mm}
	\label{tab:param}
\end{table}

\ignore{
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|l||c|c|c|c|c}
\toprule
Family & Name & \nrule & \nfact & \texttt{n-pred} & \texttt{arity} & \nshape \\
\midrule
\multirow{3}{*}{Deep} & \texttt{deep-100} & 4,241 & \multirow{3}{*}{1,000} & \multirow{3}{*}{1,299} & \multirow{3}{*}{4} & \multirow{3}{*}{1,000} 
\\%\cline{2-3}\cline{8-10}\cline{12-12}
& \texttt{deep-200} & 4,541 &  &  & & 
\\%\cline{2-3}\cline{8-10}\cline{12-12}
& \texttt{deep-300} & 4,841 &  & & & 
\\\midrule
\multirow{4}{*}{LUBM} & \lubmone & \multirow{4}{*}{137} & 99,547 & \multirow{4}{*}{104} & \multirow{4}{*}{1.45 [1,2]} & \multirow{4}{*}{30}\\%\cline{1-1}\cline{3-3}\cline{7-11}
& \lubmtwo &  & 1,272,575 &  &  &\\%\cline{1-1}\cline{3-3}\cline{7-11}
& \lubmthree &  & 13,405,381 &  &  & \\%\cline{1-1}\cline{3-3}\cline{7-11}
& \lubmfour &  & 133,573,854 &  & & \\
\midrule
\multirow{2}{*}{\ibench} & \ibenchont & 785 & 2,146,490 & 662 & 3.78 [1,11] & 245\\
& \ibenchstd & 231 & 1,109,037 & 287
 & 3.76 [1,10] & 129\\
\bottomrule
\end{tabular}}
%\vspace{-0.1cm}
\medskip
\caption{The families Deep, LUMB, and iBench.}
%\vspace{-3mm}
\label{tab:param}
\end{table}
}


  
\subsection{Experimental Evaluation} 
We run the algorithm $\mathsf{IsChaseFinite[L]}$ with all the scenarios discussed above and the experimental results are summarized in Table~\ref{tab:real}. Note that \ttotal refers to the end-to-end runtime for checking the finiteness of the chase, i.e, \ttotal = \tparse + \tgraph + \tcomp + \tshape. 
%
We highlight in a box the best end-to-end runtime obtained by considering either the in-memory or the in-database implementation for finding the database shapes. 
%The obtained times confirm that, in general, the finiteness of the chase can be efficiently checked (in the order of seconds) when we focus on linear TGDs. It also confirms (see scenario \lubmfour) that the bottleneck is finding the database shapes.


%
%As we explained in Section~\ref{sec:kb}, we apply a pre-processing step that converts the real-world knowledge bases to sets of linear tgds for which we use Algorithm~\ref{alg:dterm} and dynamic simplification to check the chase termination. 
The parsing time (\tparse) is insignificant in all scenarios as the number of rules (\nrule), which is the main parameter that impacts the parsing time, is at most 4000. The time to build the dependency graph (\tgraph) and the time to find the special SCCs (\tcomp) are also negligible. This due to the limited number of shapes (\nshape) in these scenarios, which means that the dynamic simplification that uses those shapes does not construct a large set of simplified TGDs, and thus, the induced dependency graph is rather small. 
%
It is evident from Table~\ref{tab:real} that the most costly task in all scenarios is finding the database shapes, which is consistent with what we observed in Section~\ref{sec:slinear-ex}. 


We report the time to find the shapes (\tshape) for both the in-memory and the in-database implementations of the procedure $\mathsf{FindShape}$. The in-memory implementation is faster for the Deep family since the underlying database has many singleton relations, i.e., relations with only one tuple, and thus, loading the tuples and finding the shapes can be done efficiently. On the other hand, the in-database implementation takes more time because it runs one query per relation, which results in many queries. 
%
For the LUBM and \ibench families, the in-database implementation is faster as it finds the shapes by running a few queries due to the small number of predicates. Note that, in general, the runtime of the in-database implementation increases with the number of tuples in the relations, which is evident from the LUBM scenarios. 
%
As a result, finding the shapes for \lubmfour takes a significant time. However, it is still much lower than the time for finding the database shapes using the in-memory implementation that requires loading many tuples.



\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccccccc}
& & & & \multicolumn{2}{c}{\textcolor{blue}{In-db}} & \multicolumn{2}{c}{\textcolor{violet}{In-memory}} \\
\toprule
Name & \tparse & \tgraph & \tcomp & \textcolor{blue}{\tshape} & \textcolor{blue}{\ttotal} & \textcolor{violet}{\tshape}  & \textcolor{violet}{\ttotal} \\
\midrule
\texttt{Deep-100} & 214 & 90 & 10 & \multirow{3}{*}{6,641} & 6,957 & 447 & \HL{763} 
\\
\texttt{Deep-200} & 265 & 116 & 9 & & 7,033 & 447 & \HL{839}
\\
\texttt{Deep-300} & 234 & 100 & 11 & & 6,986 & 500 & \HL{846}
\\\midrule
\lubmone & 84 & 10 & 1 & 221 & \HL{318} & 2,724 & 2,820\\
\lubmtwo & 46 & 10 & 1 & 830 & \HL{889} & 10,943 & 11,002\\
\lubmthree & 45 & 11 & 1 & 6,396 & \HL{6,454} & 70,131 & 70,189\\
\lubmfour & 43 & 231 & 80 & 65,578 & 
\HL{65,932} & 854,015 & 854,369\\
\midrule
\ibenchstd & 78 & 18 & 7 & 4,991 & \HL{5,096} & 7,379 & 7,484\\
\ibenchont & 179 & 35 & 8 & 11,726 & \HL{11,949} & 15,761 & 
15,984\\
\bottomrule
\end{tabular}}
\medskip
\caption{Runtime of $\mathsf{IsChaseFinite[L]}$ in milliseconds.}
\label{tab:real}
\end{table}



\subsection{Discussion}\label{sec:discussion-real}

It is fair to conclude that the experimental evaluation performed in this section confirms the main outcome of the analysis for the algorithm $\mathsf{IsChaseFinite[L]}$ performed in Section~\ref{sec:linear-ex}.
%
In particular, we observe that indeed the costly task is finding the database shapes, whereas the time taken by the db-independent component is negligible.
%
Moreover, we see that checking for the finiteness of the chase instance can be done rather efficiently in practice. In particular, for sets consisting of thousands of TGDs such as the Deep scenarios, and millions of facts such as \lubmtwo, it takes less than a second.
%
For schemas with a large number of predicates (e.g., \ibenchstd and \ibenchont) and databases with a large numbers of atoms (e.g., \lubmthree), it takes less than 10 seconds.
%
Finally, for very large databases with hundreds of millions of atoms such as \lubmfour, it takes around a minute, which is a reasonable time taking into account the actual size of the input.

%An important property of our proposed chase algorithm using dynamic simplification is that it is divided into two main separate costly tasks: finding shapes and checking termination for simplified rules. The isolation of these two tasks facilitates the implementation of optimization ideas to improve each task separately. While we implemented the in-db and in-memory algorithms for finding the shapes, they can be easily replaced with other techniques depending on the application, e.g., a possible solution is to materialize and incrementally keep updated the shapes in a database for faster computation. 

Another interesting takeaway from the experimental evaluation of this section is that there is no clear way to go regarding the implementation of $\mathsf{FindShape}$ among the in-memory and the in-database options.
%
In particular, the in-memory implementation is preferred when there are a few tuples per relation in the database, whereas the in-database implementation performs better when the underlying schema has a few predicates of small arity. For schemas with many predicates, each of which has many tuples in the input database, both implementations require significant time, and the offline computation of the database shapes might be preferred.


