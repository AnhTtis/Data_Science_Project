\section{Evaluation for Simple-Linear TGDS}\label{sec:slinear-ex}

We start with the experimental evaluation of $\mathsf{IsChaseFinite[SL]}$, which is depicted in Algorithm~\ref{alg:slinear}. Towards a refined analysis, we are going to break down its end-to-end runtime, which we denote by \ttotal, into the following three time parameters:

\begin{itemize}
    \item \tparse: time to parse the TGDs from an input file,
    \item \tgraph: time to build the dependency graph $G$ of the input set of TGDs, and
    \item \tcomp: time to find the special SCCs in the graph $G$.
\end{itemize}

\noindent In the rest of the section, we explain how we generate the sets of simple-linear TGDs that are used in our experimental evaluation, and then present our experimental results and discuss the take-home messages. But let us first give a couple of clarification remarks.

\medskip

\noindent 
\textbf{Remark 1.} In our analysis, we neglect the time taken by the procedure $\mathsf{Supports}$, which, as explained in Section~\ref{sec:support}, consists of two steps: (1) find the predicates occurring in the input database, and (2) traverse the dependency graph starting from the positions in the special SCCs in the reverse order to reach the positions of the relations computed in the first step.
%
Step (1) is performed by running a fast query on the catalog of the DBMS storing the input database, and can be safely ignored as it does not impact the rest of the algorithm. 
%
Concerning step (2), the time to traverse the dependency graph is negligible compared to the time needed to find the special SCCs, which is already in the order of milliseconds.
%
Summing up, the procedure $\mathsf{Supports}$ takes insignificant time compared to the rest of the algorithm, which we can simply ignore without affecting our analysis for $\mathsf{IsChaseFinite[SL]}$. Therefore, in our experiments, we assume that all the predicates used by the set of simple-linear TGDs occur in the database, and thus, all the positions in the special SCCs are trivially supported. 
%
This in turn simplifies our experiments as they can be conducted using a very simple database that can be induced by the set $\dep$ of simple-linear TGDs, denoted $D_\dep$, without using our data generator discussed above. In fact, $D_\dep$ has an atom $R(c_1,\ldots,c_n)$, where $c_1,\ldots,c_n$ are distinct constants, for each predicate $R \in \sch{\dep}$.


\medskip

\noindent 
\textbf{Remark 2.} The parsing time (\tparse) clearly depends on the parser's performance, and is not so crucial for evaluating the performance of $\mathsf{IsChaseFinite[SL]}$. However, we include it in our analysis in order to have an accurate figure for the end-to-end runtime of the algorithm, and also compare it with the other two time parameters.



\ignore{\begin{table}[h]
\centering
%\resizebox{\textwidth}{!}{
\begin{tabular}{lc}
\toprule
Parameter & Description \\
\midrule
\ttotal & Total end-to-end runtime
\\
\tparse & Time for parsing a set of tgds \\
\tgraph & Time for building a dependency graph \\
\tcomp & Time for finding special SCCs \\
\nrules &  The number of tgds in a set of tgds\\
\nshape & Time for parsing a set of tgds 
%\midrule
\bottomrule
\end{tabular}
%}
\vspace{0.3cm}
\caption{Evaluation parameters}
\vspace{-3mm}
\label{tab:params}
\end{table}}




\subsection{Generating Simple-Linear TGDs}

We now discuss how the sets of simple-linear TGDs used in our experiments are generated.
%
To systematically generate a representative family of sets of TGDs, without favouring any of the two key parameters, namely the size of the underlying schema and the number of TGDs, we consider three {\em predicate profiles} consisting of sets of TGDs that mention [5,200], [200,400], and [400,600] predicates of arity between 1 and 5, and we further consider three {\em TGD profiles} consisting of sets of TGDs with [1,333K], [333K,666K], and [666K,1M] TGDs. Note that our choice to fix the arity of the predicates between 1 and 5 is consistent with what we observe in real-life scenarios, where the arity is typically small.
%
The combination of those predicate and TGD profiles gives rise to nine {\em combined profiles} consisting of sets of TGDs with similar syntactic properties. 
%
For example, the combined profile obtained from the predicate profile [200,400] and the TGD profile [333K,666K] consists of sets of TGDs $\dep$ such that $200 \leq \sch{\dep} \leq 400$, each predicate of $\sch{\dep}$ has arity between 1 and 5, and $333{\rm K} \leq |\dep| \leq 666{\rm K}$.
% 
For our experiments, we generated 100 sets of TGDs for each of the nine combined profiles, totalling 900 sets of simple-linear TGDs. This was done as follows.
%
We have first constructed the underlying schema $\ins{S}$ by generating 1000 predicates, while their arities were randomly selected from the range [1,5]. Then, for the combined profile induced by the predicate profile $[x,y]$ and the TGD profile $[z,w]$, we have generated 100 sets of simple-linear TGDs by repeatedly executing our TGD generator with input the schema $\ins{S}$ and the tuple of values for the tuning parameters $(\mi{ssize},1,5,\mi{tsize},\mathsf{SL})$, where $\mi{ssize}$ and $\mi{tsize}$ were randomly chosen from the range of values $[x,y]$ and $[z,w]$, respectively.




%This large schema is the one that we give as an input to the TGD generator.

%As discussed in Section~\ref{sec:synth-rule}, the TGD generator {\color{red} selects a subset $\ins{S}'$ of $\ins{S}$ that is consistent with the predicate profile}, and then generates a set of TGDs that its size is consistent with the TGD profile.

%%% do we need to justify with these profiles?



\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{scatter-slinear-total-d.pdf}
		\caption{\ttotal}
		\label{fig:slinear-ttotal}
	\end{subfigure}
	\hfill
	\medskip
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{scatter-slinear-parse-graphcomponent-d.pdf}
		\caption{\tparse vs. \tgraph+\tcomp}
		\label{fig:slinear-parse-graphcomponent}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{scatter-slinear-graphcomponent-d.pdf}
		\caption{\tgraph + \tcomp}
		\label{fig:slinear-graphcomponent}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{scatter-slinear-graph-component-d.pdf}
		\caption{\tgraph vs. \tcomp}
		\label{fig:slinear-graph-component}
	\end{subfigure}
	\caption{Runtime of $\mathsf{IsChaseFinite[SL]}$.}
	\label{fig:slinear-time}
\end{figure}


\subsection{Experimental Evaluation}

% Adding two figures that include parsing time

% Discussion summary: 1- Parsing is the dominant time, 2- Times are linear with #rules as the main parameter, 3- Building the graph takes more time compared with processing the graph, 4- 



The algorithm $\mathsf{IsChaseFinite[SL]}$ was run for each one of the 900 sets of TGDs of the combined profiles discussed above. Recall that the input database is induced by the input set of TGDs, i.e., for the set of TGDs $\dep$, the database is $D_\dep$.
%
The scatter plots in Figure~\ref{fig:slinear-time} show the runtime of $\mathsf{IsChaseFinite[SL]}(D_\dep,\dep)$, for each set $\dep$ of TGDs from the combined profiles. In particular, each point in the plots corresponds to one of the 900 sets of TGDs. 
%
Figure~\ref{fig:slinear-ttotal} shows the total runtime (\ttotal) for sets of TGDs with various sizes (\nrule). Figure~\ref{fig:slinear-parse-graphcomponent} breaks down \ttotal into the time to parse the TGDs (\tparse) and the time to build their dependency graph and find the special SCCs (\tgraph + \tcomp). Figure~\ref{fig:slinear-graphcomponent} zooms in \tgraph + \tcomp, which are shown separately in Figure~\ref{fig:slinear-graph-component}. 


It is evident from the above scatter plots that the time parameters \tparse and \tgraph increase linearly as long as we increase \nrule, whereas \tcomp increases very slowly.
%
Let us remark that we have not observed such a linear relationship (in fact, we have not observed any correlation) between the time parameters \tparse and \tgraph, and the number of predicates of the underlying schema.
%
The linear relationship between \tparse and \nrule is because parsing each TGD takes constant time since the arity of the predicates falls in the limited range [1,5], and each TGD has one atom in its body and one atom in its head. Note that allowing multi-heads will not change this since, as discussed in Section~\ref{sec:setting}, the number of head-atoms is negligible compared to the number of TGDs.
%
The linear relationship with \tgraph (as shown in Figure~\ref{fig:slinear-graph-component}) is because the algorithm iterates over the TGDs and spends constant time to process each TGD and update the graph by adding new nodes and edges. Again, since the arity of the predicates falls in [1,5], and each TGD has one atom in its body and one atom in its head, the number of nodes and edges added in the dependency graph due to a certain TGD is in a small fixed range, and thus, the time to update the graph w.r.t. each TGD is constant.
%
The fact that \tcomp increases very slowly is because finding the special SCCs solely depends on the dependency graph, which is in general much smaller than the set of TGDs, while Tarjan's algorithm is quite efficient that runs in linear time in the size of the underlying graph.



%\mostafa{Explain the two figures for simple-linear.}

\subsection{Take-home Messages} \label{sec:discussion-simple}

The main takeaway from the experimental results for simple-linear TGDs is that the primary parameter impacting the runtime of $\mathsf{IsChaseFinite[SL]}$ is the number of TGDs (\nrule), and we have also observed that the algorithm is very fast even for extremely large sets of TGDs. In fact, most of the end-to-end runtime is spent on parsing (\tparse) and building the dependency graph (\tgraph), whereas the time to the find special SCCs (\tcomp) is insignificant compared to \tparse and \tgraph. To be more precise, \tparse is much larger than \tgraph, and it actually takes most of the total end-to-end runtime of the algorithm. This illustrates the effectiveness of $\mathsf{IsChaseFinite[SL]}$ as the actual check for the finiteness of the chase instance for large sets of TGDs is much faster than even reading and parsing the TGDs from the input file.


%%%%%%%%%%%%%%%%
%%% LINEAR TGDs %%%
%%%%%%%%%%%%%%%%


\section{Evaluation for Linear TGDs} \label{sec:linear-ex}

We now proceed with the evaluation of $\mathsf{IsChaseFinite[L]}$, depicted in Algorithm~\ref{alg:dterm}. Differently from $\mathsf{IsChaseFinite[SL]}$, where the input database did not play any crucial role, we now have a component that heavily relies on the database, that is, the procedure that computes the database shapes, which is part of dynamic simplification.
%
In other words, we have the {\em database-dependent component} of $\mathsf{IsChaseFinite[L]}$, that is, find the database shapes, and the {\em database-independent component}, that is, simplify the given set of linear TGDs by using the database shapes, build the dependency graph of the simplified set of TGDs, and find the special SCCs in this graph.
%
We claim that these two components, which from now on we call db-dependent and db-independent, respectively, should be evaluated separately as their runtime is impacted by different parameters.
%
Concerning the db-dependent component, it is obvious that it is only affected by the database, whereas the set of TGDs plays no role. 
%
On the other hand, although it is clear that the db-independent component is affected by the set of TGDs, it is not straightforward to see that it is not affected by the input database since it operates on a dynamically simplified set of TGDs. Interestingly, we experimentally confirm below that this is indeed the case.
%
Consequently, towards a refined analysis of the algorithm $\mathsf{IsChaseFinite[L]}$, we are going to consider the following four time parameters:
\begin{itemize}
	\item \tshape: time to find the database shapes,
	\item \tparse: time to parse the TGDs from an input file,
	\item \tgraph: time to build the dependency graph $G$ of the simplified version of the input set of TGDs (including the time for the simplification using the database shapes), and
	\item \tcomp: time to find the special SCCs in the graph $G$.
\end{itemize}
Clearly, \tshape refers to the runtime of the db-dependent component, whereas \tparse + \tgraph + \tcomp, which we denote by \ttotal, refers to the end-to-end runtime of the db-independent component.
%
In the rest of the section, we explain how we generate the databases and the sets of linear TGDs that are used in our experimental evaluation, confirm that the db-independent component is not affected by the input database, and then present our experimental results for the two components of $\mathsf{IsChaseFinite[L]}$ and discuss the take-home messages.
%
Note that, as stated in Remark 2 in Section~\ref{sec:slinear-ex}, although the parsing time is not crucial for evaluating the performance of the db-independent component, we consider it in order to have an accurate figure for the end-to-end runtime, and also compare it with \tgraph and \tcomp.




%Recall that it dynamically simplifies the given set of linear TGDs, builds the dependency graph $G$ of the simplified set of TGDs, and finally checks for the existence of a special SCC in $G$.
%
%The time for dynamic simplification can be divided into the time for finding the shapes, which we refer to by \tshape, and the time for chasing for simplification. Chasing for simplification is done along with building the dependency graph for the simplified tgds and is considered in \tgraph. Therefore, we can break down the end-to-end runtime of Algorithm~\ref{alg:dterm} into \tparse, \tgraph, \tcomp, and \tshape. To be consistent with our experimental evaluation for simple-linear tgds, we use \ttotal to refer to \tparse+\tgraph+\tcomp. We also use \nshape, the number of shapes in the extensional database, to analyze our algorithm for finding the shapes. Unlike our analysis in Section~\ref{sec:slinear-ex}, in this section, we consider databases of various sizes because dynamic simplification and building the dependency graphs are both impacted by the number of shapes in the database, which depends on the size of the databases.

\ignore{\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{newfigs/shapes-time-5.pdf}
         \caption{Predicate profile [5,200]}
         \label{fig:test_computers_sub}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{newfigs/shapes-time-200.pdf}
         \caption{Predicate profile [200,400]}
         \label{fig:valid_computers_sub}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{newfigs/shapes-time-400.pdf}
         \caption{Predicate profile [400,600]}
         \label{fig:train_computers_sub}
     \end{subfigure}
        \caption{The time to generate shapes for databases with varying sizes and different predicate profiles}
        \label{fig:computers_sub graphs}
\end{figure*}


\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{newfigs/shapes-5.pdf}
         \caption{Predicate profile [5,200]}
         \label{fig:test_computers_sub}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=0.88\textwidth]{newfigs/shapes-time-200.pdf}
         \caption{Predicate profile [200,400]}
         \label{fig:valid_computers_sub}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=0.85\textwidth]{newfigs/shapes-time-400.pdf}
         \caption{Predicate profile [400,600]}
         \label{fig:train_computers_sub}
     \end{subfigure}
        \caption{The number of shapes for databases with varying sizes and different predicate profiles}
        \label{fig:computers_sub graphs}
\end{figure*}}




%\subsubsection{Impact of Program Size}


\ignore{\begin{figure}[H]
    %\includegraphics[width=\textwidth]{figures/db-time-graph-processing.png}
    \includegraphics[width=0.3\textwidth]{figures/db-time-graph-processing.png}
    \caption{The time spent on building and processing the dependency graph (t-graph + t-comp) for the database with various sizes}
    \label{fig:test}
\end{figure}}



%plots linear:
%1- type:boxplot x:dbsize y:t_shape, 1 plot per predicate_profile (3 charts)
%2- type:scatter x:rules y:t_graph+t_component
%3- type:scatter x:rules y_1:t_graph y_2:t_components (y_1 and y_2 with different colors)
%4- type:boxplot x:dbsize y:n_shapes, 1 plot per predicate_profile (3 charts)
%5, type:boxplot x:dbsize y:t_graph+t_component

%plots simple-linear:
%1- type:scatter x:rules y:t_graph+t_component
%2- type:scatter x:rules y_1:t_graph y_2:t_components (y_1 and y_2 with different colors)



%\subsection{Chase Termination for Other Ontalogies} \label{sec:real-data}

%So far we conducted our experiments using synthetic ontologies to stress on performance of the our chase-termination algorithm. In this section, we use real ontologies, as opposed to synthetic ontologies, with their data and rules extracted from different applications domains. Table~\ref{tab:real} summarizes some important statistics about these ontologies.

%\multirow{3}{*}{1,000}

\subsection{Generating Databases and Linear TGDs}

% 1- differentiating between the chase and its termination algorithm (addressed)
% 2- 8.2 explain why we did not discuss the linear relationship for simple linear and t-comp (addressed)
% 3- random generation of records and the issue with increasing the number of shapes vs. db size (addressed)
% 4- Explaining the preparation of the real-world knowledge bases and then clarifying that we use the algorithm for linear tgds (worst case)
% 5- running the algorithm for simple-linear is very fast
% 6- offline approach for computing shapes and maintaining them (materializing shapes and maintaining them), incremental maintenance of the shapes

The goal is to generate a family of pairs of the form $(D,\dep)$, where $D$ is a database and $\dep$ a set of linear TGDs, that will serve as the input to $\mathsf{IsChaseFinite[L]}$ during the experimental evaluation.
%
To this end, we first constructed a very large database, which we dub $D^\star$, by using our data generator. In particular, we called the data generator with input $(1000,1,5,500{\rm K},500{\rm K})$, and obtained $D^\star$ that mentions 1000 predicates of arity between 1 and 5, and each such predicate has 500K tuples, resulting in a very large database with 500M tuples in total.
%
We further devised views over $D^\star$ that allow us to define on-demand virtual databases with 1K, 50K, 100K, 250K, and 500K tuples per predicate, resulting in databases with 1M, 50M, 100M, 250M, and 500M tuples in total, respectively. Those views are actually implemented as a simple SQL query that simply keeps the first 1K, 50K, 100K, 250K, and 500K tuples per predicate, respectively. Note that the tuples in $D^\star$ are lexicographically sorted, which means that the different shapes in a relation of $D^\star$ are evenly distributed. This is turn ensures that the virtual databases defined via the views have a variety of shapes, which is crucial for our purposes.
%
Having the database $D^\star$ and the database views in place the desired family of pairs was generated as described below.


For each one of the nine combined profiles used in the generation of simple-linear TGDs in Section~\ref{sec:slinear-ex}, we generated 5 sets of linear TGDs, totalling 45 sets. In particular, for the combined profile induced by the predicate profile $[x,y]$ and the TGD profile $[z,w]$, we have generated 5 sets of linear TGDs by repeatedly executing our TGD generator with input the schema $\{R \mid R(\bar c) \in D^\star\}$, i.e., the 1000 predicates occurring in $D^\star$, and the tuple of values for the tuning parameters $(\mi{ssize},1,5,\mi{tsize},\mathsf{L})$, where $\mi{ssize}$ and $\mi{tsize}$ were randomly chosen from the range of values $[x,y]$ and $[z,w]$, respectively. Let $\dep^\star$ be the family that collects the 45 generated sets of linear TGDs.
%
Then, for each set $\dep \in \dep^\star$ of linear TGDs, by exploiting the database views discussed above, we obtained five virtual databases of varying size (1K, 50K, 100K, 250K, and 500K tuples per predicate), denoted $D_{\dep}^{1}$, $D_{\dep}^{50}$, $D_{\dep}^{100}$, $D_{\dep}^{250}$, and $D_{\dep}^{500}$, respectively, leading to five pairs.
%
Summing up, we generated the family
\[
\left\{\left(D_{\dep}^{s},\dep\right) \mid \dep \in \dep^\star \text{ and } s \in \{1,50,100,250,500\}\right\}
\]
consisting of 225 pairs that will serve as the input to $\mathsf{IsChaseFinite[L]}$.



%We then used five database views over $D^\star$ to define five virtual databases with 1K, 50K, 100K, 250K, and 500K tuples per predicate, resulting in databases with 1M, 50M, 100M, 250M, and 500M tuples in total, respectively. These virtual databases are used to stress-test the db-dependent component (i.e., the procedure $\mathsf{FindShapes}$) and analyze how its runtime is affected as we increase the size of the input database.

%To study the impact of the extensional database size on dynamic simplification and the termination algorithm for linear-tgds, we use the data generator and the large shared schema with 1,000 relations to generate a database instance with 500k records per relation, totaling 500m records in the database. Similar to the experiments for simple-linear tgds, the database provides a shared schema for all the synthetic sets of linear tgds and their extensional data in this section. To vary the size of the extensional data for the sets of tgds, we use five database views to define five virtual databases with 1k, 50k, 250k, and 500k records per relation (with database sizes 1m, 50m, 250m, and 500m). The databases are used to stress-test dynamic simplification and the algorithm for checking the chase termination and studying the runtime for varying database sizes.

%%Now, for the sets of linear TGDs used in our experiments, for each one of the nine combined profiles used in the generation of simple-linear TGDs in Section~\ref{sec:slinear-ex}, we generated 5 sets of linear TGDs, totalling 45 sets. In particular, for the combined profile induced by the predicate profile $[x,y]$ and the TGD profile $[z,w]$, we have generated 5 sets of linear TGDs by repeatedly executing our TGD generator with input the schema $\{R \mid R(\bar c) \in D^\star\}$, i.e., the 1000 predicates occurring in $D^\star$, and the tuple of values for the tuning parameters $(\mi{ssize},1,5,\mi{tsize},\mathsf{L})$, where $\mi{ssize}$ and $\mi{tsize}$ were randomly chosen from the range of values $[x,y]$ and $[z,w]$, respectively.

%%Summing up, we obtained 5 databases of varying size that will be used to experimentally evaluate the db-dependent component of $\mathsf{IsChaseFinite[L]}$. Furthermore, by combing those 5 databases with the 45 sets of linear TGDs, we obtain 225 pairs consisting of a database and a set of linear TGDs, which will be used to experimentally evaluate the db-independent component of $\mathsf{IsChaseFinite[L]}$.


%For generating synthetic linear tgds, we consider the same nine profiles we used for simple-linear tgds. We generate five sets of tgds in each of the nine categories, totaling 45 sets. All these sets of tgds share the same schema with the 1,000 predicates in the shared database schema, where each set may use a subset of the schema. The 45 ontologies and the five databases create a pool of 225 knowledge bases that allow us to study different performance parameters of the termination algorithms for linear tgds. 


\begin{figure*}[h!]
	\begin{minipage}{.74\textwidth}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{shape-5.pdf}
		\vspace{-5mm}
		\caption{[5,200]}
		\label{fig:shape-5}
	\end{subfigure}
	\hspace{0.5cm}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{shape-200.pdf}
		\vspace{-5mm}
		\caption{[200,400]}
		\label{fig:shape-200}
	\end{subfigure}
	\hspace{0.1cm}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{shape-400.pdf}
		\vspace{-5mm}
		\caption{[400,600]}
		\label{fig:shape-400}
	\end{subfigure}
	\vspace{-3mm}
	\caption{Number of Shapes.}
	\label{fig:nshapes}
	\end{minipage}
	%
	%
	%
	\begin{minipage}{.74\textwidth}
		\centering
		\begin{subfigure}[b]{0.3\textwidth}
			\centering
			\includegraphics[width=\textwidth]{shape-time-5.pdf}
			\vspace{-5mm}
			\caption{[5,200]}
			\label{fig:shape-time-5}
		\end{subfigure}
		\hspace{0.5cm}
		\begin{subfigure}[b]{0.3\textwidth}
			\centering
			\includegraphics[width=\textwidth]{shape-time-200.pdf}
			\vspace{-5mm}
			\caption{[200,400]}
			\label{fig:shape-time-200}
		\end{subfigure}
		\hspace{0.1cm}
		\begin{subfigure}[b]{0.3\textwidth}
			\centering
			\includegraphics[width=\textwidth]{shape-time-400.pdf}
			\vspace{-5mm}
			\caption{[400,600]}
			\label{fig:shape-time-400}
		\end{subfigure}
		\vspace{-3mm}
		\caption{Runtime of $\mathsf{FindShapes}$ (in-memory implementation).}
		\label{fig:tshapes}
	\end{minipage}
		%
	%
	%
	\begin{minipage}{.74\textwidth}
		\centering
		\begin{subfigure}[b]{0.3\textwidth}
			\centering
			\includegraphics[width=\textwidth]{shape-time-5-q.pdf}
			\vspace{-5mm}
			\caption{[5,200]}
			\label{fig:shape-time-5-q}
		\end{subfigure}
		\hspace{0.5cm}
		\begin{subfigure}[b]{0.3\textwidth}
			\centering
			\includegraphics[width=\textwidth]{shape-time-200-q.pdf}
			\vspace{-5mm}
			\caption{[200,400]}
			\label{fig:shape-time-200-q}
		\end{subfigure}
		\hspace{0.1cm}
		\begin{subfigure}[b]{0.3\textwidth}
			\centering
			\includegraphics[width=\textwidth]{shape-time-400-q.pdf}
			\vspace{-5mm}
			\caption{[400,600]}
			\label{fig:shape-time-400-q}
		\end{subfigure}
		\vspace{-3mm}
		\caption{Runtime of $\mathsf{FindShapes}$ (in-database implementation).}
		\label{fig:tshapes-q}
	\end{minipage}
\end{figure*}


\subsection{Experimental Evaluation}
%

Before delving into the evaluation of the two components of the algorithm $\mathsf{IsChaseFinite[L]}$, let us first confirm that indeed the db-independent component is not affected by the input database, which in turn justifies our decision to separately evaluate the two components as their runtime is impacted by different parameters. 


\medskip

\noindent 
\textbf{Separate the Two Components.} The figure below depicts the average time over all generated pairs, consisting of a database $D_{\dep}^{s}$ of a certain size $s \in \{1,50,100,250,500\}$ and a set $\dep$ of linear TGDs, for building the dependency graph of the dynamically simplified version of $\dep$ using the shapes of $D_{\dep}^{s}$ and finding the special SCCs:

%\smallskip

\centerline{\includegraphics[width=.23\textwidth]{shape-db-time.pdf}}

%\smallskip

\noindent Interestingly, it confirms that the database size does not impact the time to build the dependency graph and find the special SCCs; thus, it does not impact the end-to-end runtime of the db-independent component, as claimed above.
%
This can be explained by the fact that the number of shapes in a database increases very slowly as we increase the size of the database; this is illustrated in Figure~\ref{fig:nshapes}. In particular, the bar plots in Figure~\ref{fig:nshapes} show the average number of shapes over all databases $D_{\dep}^{s}$ of a certain size $s$, where each plot corresponds to a certain predicate profile $[x,y]$, i.e., $\dep$ falls in the predicate profile $[x,y]$. It is clear that the number of shapes increases as we increase the size of the database, which is somehow expected as an increase in the database size leads to more tuples that are likely to induce more unique shapes. The interesting outcome, however, is the fact that this increase is very slow, which should be attributed to the fact that many tuples are likely to induce the same shape. 
%
Moreover, a new shape gives rise to only a few simplified TGDs, and it does not significantly affect the time for building and processing the dependency graph.


Let us finally observe that, by comparing the three bar plots, it is clear that the number of predicates, reflected in the predicate profile, impacts the number of shapes, which is rather expected as with more predicates there would be more shapes. This means that the number of predicates of the underlying schema is a parameter that affects the number of shapes, which explains why in our analysis above we had to separately consider the three predicate profiles.








\medskip

\noindent 
\textbf{Evaluation of the DB-dependent Component.}
%
We run the procedure $\mathsf{FindShapes}$ for each one of the databases $D_{\dep}^{s}$, where $\dep \in \dep^\star$ and $s \in \{1,50,100,250,500\}$; 225 executions in total. Recall that we have two kinds of implementations for the procedure $\mathsf{FindShapes}$, namely in-memory and in-database (see Section~\ref{sec:d-simplification}).
%
The bar plots in Figures~\ref{fig:tshapes} and~\ref{fig:tshapes-q} show the average runtime over all databases $D_{\dep}^{s}$ of a certain size $s$ for finding the shapes in the case of the in-memory and in-database implementation, respectively, where each plot corresponds to a certain predicate profile. 
%Due to space constraints, we do not report the analogous plots for the in-memory implementation. 
Note that for both implementations we observed a similar trend, with the in-database implementation outperforming the in-memory one. 
%Having said that, in our experiments with existing databases and sets of TGDs, performed in Section~\ref{sec:rw-kb}, we have seen exceptions where the in-memory implementation runs faster.


It is evident from the bar plots in Figure~\ref{fig:tshapes-q} that the time to find the shapes increases while the database size increases, which is not surprising since, as discussed above, the number of shapes increases while the database size increases. Observe, however, that the time to find the shapes grows much faster than the actual number shapes, which should be attributed to the fact that for finding the shapes we actually need to scan the whole database.
%
Let us finally observe that, by comparing the three bar plots, it is apparent that the number of predicates, reflected in the underlying predicate profile, also impacts the time to find the shapes, which explains why we had to analyze each predicate profile separately.


%This outcome is not surprising since, as shown in Figure~\ref{fig:nshapes}, the actual number of shapes (\nshape) is similarly affected by the database size. In particular, the bar plots in Figure~\ref{fig:nshapes} show the average number of shapes over all databases $D_{\dep}^{s}$ of a certain size $s$, where each plot corresponds to a certain predicate profile. It is clear that the number of shapes increases as we increase the size of the database, which is somehow expected as an increase in the database size leads to more tuples that are likely to induce more unique shapes. Moreover, by comparing the three bar plots, it is clear that the number of predicates, reflected in the predicate profile, impacts the number of shapes. This was expected as with more predicates there would be obviously more shapes.




%%%%%%%%%%%%%

%The bar plots in Figures~\ref{fig:nshapes} and~\ref{fig:tshapes-q} describe the performance of the in-db algorithm for finding the shapes in Section~\ref{sec:shapes}. Here, we only report the time for the in-db algorithm because both have a similar trend. Still, the in-db algorithm outperforms the in-memory algorithm (see Figure~\ref{fig:tshapes} in the appendix for the results of the in-memory algorithm). Note that the in-db algorithm is not always superior; we show exceptions in real-world knowledge bases in Section~\ref{sec:rw-kb} where the in-db algorithm returns faster. The bar plots in Figure~\ref{fig:nshapes} show the average number of shapes (\nshape) in knowledge bases with varying database sizes, where each plot reports \nshape for a predicate profile. Looking at the bars in each plot, one can conclude that the number of shapes grows with the database size. This is not surprising as an increase in the database size leads to more records that are likely to make more unique shapes in the database. Comparing the three bar plots, we observe that the number of predicates reflected in predicate profiles also impacts the number of shapes. This was expected as with more predicates, there would be more shapes in the database.



%The bar plots in Figures~\ref{fig:nshapes} and~\ref{fig:tshapes-q} describe the performance of the in-db algorithm for finding the shapes in Section~\ref{sec:shapes}. Here, we only report the time for the in-db algorithm because both have a similar trend. Still, the in-db algorithm outperforms the in-memory algorithm (see Figure~\ref{fig:tshapes} in the appendix for the results of the in-memory algorithm). Note that the in-db algorithm is not always superior; we show exceptions in real-world knowledge bases in Section~\ref{sec:rw-kb} where the in-db algorithm returns faster. The bar plots in Figure~\ref{fig:nshapes} show the average number of shapes (\nshape) in knowledge bases with varying database sizes, where each plot reports \nshape for a predicate profile. Looking at the bars in each plot, one can conclude that the number of shapes grows with the database size. This is not surprising as an increase in the database size leads to more records that are likely to make more unique shapes in the database. Comparing the three bar plots, we observe that the number of predicates reflected in predicate profiles also impacts the number of shapes. This was expected as with more predicates, there would be more shapes in the database.

%The bar plots in Figure~\ref{fig:tshapes-q} are similar to those in Figure~\ref{fig:nshapes}, but report the average time to find the shapes, \tshape. The plots support the results about the number of shapes and confirm that increasing the database size increases the time to find the shapes as there will be more records to load and examine for finding unique shapes. Also, having more predicates will increase the time to find the shapes for each predicate.




\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{scatter-linear-total-d3.pdf}
		\caption{\ttotal}
		\label{fig:linear-total}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{scatter-linear-parse-graphcomponent-d3.pdf}
		\caption{\tparse vs. \tgraph+\tcomp}
		\label{fig:linear-parse-graphcomponent-d3}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{scatter-linear-graph-component-d3.pdf}
		\caption{\tgraph vs. \tcomp}
		\label{fig:linear-graph-component-d3}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{scatter-linear-component-d3.pdf}
		\caption{\tcomp}
		\label{fig:linear-graphcomponent-d3}
	\end{subfigure}
	\caption{Runtime of the db-independent component.}
	\label{fig:linear-time}
\end{figure}


\medskip

\noindent 
\textbf{Evaluation of the DB-independent Component.}
%
The scatter plots in Figure~\ref{fig:linear-time} show the runtime of the db-independent component of the algorithm $\mathsf{IsChaseFinite[L]}$ when executed with input $(D_{\dep}^{s},\dep)$, for each set $\dep \in \dep^\star$ falling in the predicate profile [400,600] and $s \in \{1,50,100,250,500\}$. In particular, each point in the plots corresponds to a pair $(D_{\dep}^{s},\dep)$. 
%
Let us stress that, unlike the analogous Figure~\ref{fig:slinear-time} for simple-linear TGDs, we focus on a particular predicate profile since otherwise we do not obtain any trend of the runtime w.r.t. the number of TGDs. In other words, the apparent linear trend observed in those plots only holds for sets of TGDs from the same predicate profile. 
%
This is because the number of predicates of the underlying schema impacts the number of shapes, which in turn affects the process of dynamic simplification and the size of the dependency graph, and thus, the time parameters \tgraph and \tcomp are impacted. This explains why we had to analyze each predicate profile separately. 
%
For the sake of readability, the analogous plots for the predicate profiles [5,200] and [200,400] are presented and discussed in the appendix.
%
Figure~\ref{fig:linear-total} shows the total runtime (\ttotal) for sets of TGDs with various sizes (\nrule). Figure~\ref{fig:linear-parse-graphcomponent-d3} breaks down \ttotal into the time to parse the TGDs (\tparse) and the time to build their dependency graph and find the special SCCs (\tgraph + \tcomp), whereas Figure~\ref{fig:linear-graph-component-d3} shows separately \tgraph and \tcomp. Figure~\ref{fig:linear-graphcomponent-d3} zooms in \tcomp.

%Figure~\ref{fig:linear-graphcomponent-d3} zooms in \tgraph + \tcomp, which are shown separately in Figure~\ref{fig:linear-graph-component-d3}. 


It is evident from the above scatter plots that the time parameters \tparse and \tgraph increase linearly as long as we increase \nrule, whereas \tcomp increases very slowly. This is essentially what we have observed for simple-linear TGDs in Figure~\ref{fig:slinear-time}, with the key difference that the time needed to parse the TGDs (\tparse) is now much less compared to the time for building the dependency graph and finding the special SCCs (\tgraph + \tcomp). It should not be forgotten, however, that for linear TGDs we need to focus on a single predicate profile in order to get these linear trends; otherwise, if we consider all the predicate profiles at once, there is no trend that can be observed. Note also that the absolute running time increases compared to the case of simple-linear TGDs.

%Figure~\ref{fig:linear-component-d3} shows the time to find special SCCs (\tcomp) in the dependency graphs of linear tgds and demonstrates a linear relationship between the number of tgds and \tcomp for the sets tgds in the predicate profile [400,600]. We observe a similar relationship for the sets of tgds in the other predicate profiles, [5,200] and [200,400]. Like other time parameters, this relationship only exists within each predicate profile due to the impact of the number of predicates on the number of nodes and edges in the dependency graphs and the time to find special SCCs. Note that this linear correlation also exists for simple-linear tgds when looking at the sets of tgds in each predicate profile. Like linear-tgds, the linear relationship between \nrule and \tcomp is only within each predicate profile. See Appendix~\ref{sec:additoinal} for more results about predicate profiles [5,200] and [200,400], all sets of tgds independent of any profile, and \tcomp for simple-linear tgds. We did not discuss this in Section~\ref{sec:slinear} as it is unlike the linear relationships between \nrule and other time parameters for the graph-based algorithm that is both within and also across the profiles.



\ignore{
\begin{figure*}[h]
    \centering
     \begin{minipage}{0.23\textwidth}
        \centering%\vspace{-2mm}
        \includegraphics[width=\textwidth]{figs/scatter-linear-component-d3.pdf}
        \vspace*{-8mm}
        \caption{The time for finding special SCCs (linear tgds)}
        \label{fig:linear-component-d3}
    \end{minipage}
    \begin{minipage}{.74\textwidth}
         \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/shape-5.pdf}
         \vspace{-5mm}
         \caption{[5,200]}
         \label{fig:shape-5}
     \end{subfigure}
     \hspace{0.5cm}
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/shape-200.pdf}
         \vspace{-5mm}
         \caption{[200,400]}
         \label{fig:shape-200}
     \end{subfigure}
     \hspace{0.1cm}
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/shape-400.pdf}
         \vspace{-5mm}
         \caption{[400,600]}
         \label{fig:shape-400}
     \end{subfigure}
     \vspace{-3mm}
     \caption{The number of shapes vs. the database size in different predicate profiles}
        \label{fig:nshapes}
    \end{minipage}\hspace{0.2cm}
\end{figure*}


\begin{figure*}[h]
    \centering
    \begin{minipage}{0.23\textwidth}
        \centering%\vspace{-2mm}
        \includegraphics[width=\textwidth]{figs/shape-db-time.pdf}
        \vspace*{-8mm}
        \caption{Building and processing dependency graphs}
        \label{fig:db-time}
    \end{minipage}
    \begin{minipage}{.74\textwidth}
         \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/shape-time-5-q.pdf}
         \vspace{-5mm}
         \caption{[5,200]}
         \label{fig:shape-time-5-q}
     \end{subfigure}
     \hspace{0.5cm}
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/shape-time-200-q.pdf}
         \vspace{-5mm}
         \caption{[200,400]}
         \label{fig:shape-time-200-q}
     \end{subfigure}
     \hspace{0.1cm}
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/shape-time-400-q.pdf}
         \vspace{-5mm}
         \caption{[400,600]}
         \label{fig:shape-time-400-q}
     \end{subfigure}
     \vspace{-3mm}
     \caption{The time to find shapes vs. database size in different predicate profiles (in-db)}
        \label{fig:tshapes-q}
    \end{minipage}\hspace{0.2cm}
\end{figure*}
}

%It is important to note that the plots in Figure~\ref{fig:linear-time} are for sets of tgds with [400,600] predicates. This is because the linear relationships in these plots only hold for the sets of tgds within the predicate profile. We observed similar results in the predicate profiles [5,200] and [200,400], although the linear relationship is less evident for the knowledge bases in [5,200] as we explain in Appendix~\ref{sec:additoinal}. The experimental results in Appendix~\ref{sec:additoinal} also show that the linear relationship does not hold when considering all predicate profiles together. This is because the number of predicates significantly impacts the time parameters. For \tgraph, the number of predicates decides the number of shapes and directly impacts the number of simplified tgds that affect \tgraph, as we demonstrated in Figure~\ref{fig:linear-graph-component-d3}. 

%The parsing time, \tparse, is the only parameter that is linearly correlated with \nrule within each predicate profile (shown in Figure~\ref{fig:linear-parse-graphcomponent-d3}) and also among all knowledge bases (see Appendix~\ref{sec:additoinal} for more detail). This is because the number of predicates does not impact the time to parse each tgd.

%Figure~\ref{fig:linear-component-d3} shows the time to find special SCCs (\tcomp) in the dependency graphs of linear tgds and demonstrates a linear relationship between the number of tgds and \tcomp for the sets tgds in the predicate profile [400,600]. We observe a similar relationship for the sets of tgds in the other predicate profiles, [5,200] and [200,400]. Like other time parameters, this relationship only exists within each predicate profile due to the impact of the number of predicates on the number of nodes and edges in the dependency graphs and the time to find special SCCs. Note that this linear correlation also exists for simple-linear tgds when looking at the sets of tgds in each predicate profile. Like linear-tgds, the linear relationship between \nrule and \tcomp is only within each predicate profile. See Appendix~\ref{sec:additoinal} for more results about predicate profiles [5,200] and [200,400], all sets of tgds independent of any profile, and \tcomp for simple-linear tgds. We did not discuss this in Section~\ref{sec:slinear} as it is unlike the linear relationships between \nrule and other time parameters for the graph-based algorithm that is both within and also across the profiles.   


%Figure~\ref{fig:db-time} demonstrates the average time to build the dependency graph and to find special SCCs (\tgraph+\tcomp) for linear tgds and extensional databases of various sizes. It confirms that the database size does not impact the \tgraph+\tcomp. This can be explained by the fact that the number of shapes in a database increases very slowly with an increase in the database size, as many records are likely to share the same shape. Additionally, a new shape only creates a few simplified tgds that does not cause a significant increase in building and processing the dependency graph.

\subsection{Take-home Messages}\label{sec:discussion-linear}


The main takeaway is that the algorithm $\mathsf{IsChaseFinite[L]}$ consists of two components that are of different nature in the sense that their runtime is impacted by different parameters of the input. 
%
On the one hand, we have the db-dependent component, which is responsible for finding the database shapes, that is only affected by the size of the database.
%
On the other hand, we have the db-independent component, that is, simplify the given set of linear TGDs by using the database shapes, build the dependency graph of the simplified set of TGDs, and find the special SCCs in this graph, whose runtime is primarily affected by the number of TGDs (\nrule). Having said that, we have also observed that the number of predicates also affects the runtime of the db-independent component since it impacts the number of shapes, which in turn affects the process of dynamic simplification and the size of the dependency graph.
%
We conclude by observing that the total runtime of $\mathsf{IsChaseFinite[L]}$ is quite reasonable, which should be seen as a strong evidence that fast checking for the finiteness of the chase instance in the case of linear TGDs is not an unrealistic goal. Note that most of the total end-to-end runtime of the algorithm is spent on finding database shapes, which indicates that our future efforts should be concentrated on improving the db-dependent component.


%This is a key difference compared to $\mathsf{IsChaseFinite[SL]}$ for simple-linear TGDs, where most of the runtime is spent on parsing the set of TGDs.