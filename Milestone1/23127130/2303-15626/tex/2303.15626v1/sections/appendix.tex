\appendix

\section{Generative models}\label{app:architectures}

In this appendix, we briefly introduce the generative models used in this study. We also summarize their hyperparameters in Tab.~\ref{tab:hyperparams}.

\subsection{Quantum Circuit Born Machines (QCBMs)}

Quantum circuit Born machines (QCBMs) are a class of expressive quantum generative models based on parametrized quantum circuits (PQCs)~\cite{Benedetti2019}. Starting from a fixed initial qubit configuration $\ket{\bm{0}}^{\otimes N}$, we apply a unitary $U(\bm{\theta})$ on top of this state. Projective measurements are performed at the end of the circuit to get configurations that are sampled, according to Born's rule, from the squared modulus of the PQC's wave function. One can use different topologies that describe the qubit connectivity to model the unitary $U(\bm{\theta})$. Our study uses the line topology, where each qubit is connected to its nearest neighbors in a 1D chain configuration~\cite{Benedetti2019,benedetti2019parameterized}. More details can be found in Ref.~\cite{Gili2022}. 

To train our QCBM, we compute the KL divergence between the softmax training distribution $P_{\text{train}}$ (see Eq.~\eqref{eq:softmax_train_prob}) and the QCBM probability distribution $P_{\text{QCBM}}$ as 
\begin{equation*}
    \text{KL}(P_{\text{train}} || P_{\text{QCBM}}) = \sum_{\bm{x}} P_{\text{train}}(\bm{x}) \log \left( \frac{P_{\text{train}}(\bm{x})}{\max(\delta,P_{\text{QCBM}}(\bm{x}))} \right),
\end{equation*}
where $\delta = 10^{-8}$ is a regularization factor to avoid numerical instabilities. The optimization of the parameters $\bm{\theta}$ is performed using a gradient-free method called CMA-ES optimizer~\cite{hansen2016cma, hansen2019pycma} after randomly initializing the parameters of the PQC. In our simulations, we used $8$ layers, which provided the best utility $U$ compared to $2, 4$ and $6$ layers with line topology.
For the QCBM and the following classical generative models, the optimal hyperparameters are obtained with a grid search with the condition of getting the lowest utility after $100$ training iterations.

We have also performed simulations with the all-to-all topology~\cite{Benedetti2019}; however, they lead to sub-optimal performances compared to the line topology for trainability reasons at the scale of our experiments. In Ref.~\cite{Gili2022}, we did not observe this issue for an all-to-all topology at the scale of $12$ qubits. To further improve the trainability of QCBMs, there is a potential for using pre-training of QCBMs with tensor networks as suggested in Ref.~\cite{rudolph2022synergistic}. As a final note, since the exact computation of $P_{\text{QCBM}}$ is not tractable for large system sizes, one could consider the use of sample-based cost functions such as Maximum Mean Discrepancy (MMD) for a large number of qubits~\cite{liu2018differentiable}. Extending this study to an experimental demonstration on currently available quantum devices is also crucial to evaluate the impact of noise on the model's performance.

\subsection{Recurrent Neural Networks (RNNs)}

Recurrent Neural Networks (RNNs) are unique architectures traditionally used for applications in natural language processing such as machine translation and speech recognition~\cite{lipton2015RNN, Bengio-Book}. They are known for their ability to simulate Turing machines~\cite{RNNTuring} and for their capability of being universal approximators~\cite{Shafer2006}. RNNs take advantage of the probability chain rule to generate uncorrelated samples autoregressively as follows:
\begin{align}
    P&_{\rm RNN}(\sigma_1, \sigma_2, \ldots, \sigma_N) = \nonumber\\
    &P_{\rm RNN}(\sigma_1) P_{\rm RNN}(\sigma_2 | \sigma_1) \ldots P_{\rm RNN}(\sigma_N | \sigma_1, \ldots \sigma_{N-1}).
    \label{eq:chain_rule}
\end{align}
Here $(\sigma_1, \sigma_2, \ldots, \sigma_N)$ is a configuration of $N$ bits where $\sigma_i = 0, 1$. Each conditional $P_{\rm RNN}(\sigma_i | \sigma_1, \ldots \sigma_{i-1})$ is computed using a Softmax layer as:
\begin{equation}
     P_{\rm RNN}(\sigma_i | \sigma_{<i}) = \text{Softmax} \left ( U \bm{h}_i + \bm{c} \right ) \cdot \bm{\sigma}_i.
    \label{eq:softmax_layer}
\end{equation}
$\bm{\sigma}_i$ is a one-hot encoding of $\sigma_i$ and `$\cdot$' is the dot product operation. The weights $U$ and the biases $\bm{c}$ are the parameters of this Softmax layer. The vector $\bm{h}_i$ is called the memory state with a tunable size $d_h$, which we call the hidden dimension. For the simplest (vanilla) RNN, $\bm{h}_i$ is computed using the following recursion relation:
\begin{equation}
    \bm{h}_i = f(W \bm{h}_{i-1} + V \bm{\sigma}_{i-1} + \bm{b}),
\end{equation}
where $W$, $V$ and $\bm{b}$ are trainable parameters and $f$ is a non-linear activation function. Furthermore, $\bm{\sigma}_{0}, \bm{h}_0$ are initialized to zero. Typically, vanilla RNNs suffer from the vanishing gradient problem, which makes their training a challenging task~\cite{Pascanu2012}. To mitigate this limitation, more advanced versions of RNN cells have been devised, namely, the Gated Recurrent Unit (GRU)~\cite{GRU2014, lipton2015RNN}. In our experiments, we use the PyTorch implementation of the GRU unit cell~\cite{NEURIPS2019_9015}.

Similarly to the QCBM, we minimize the KL divergence between the RNN distribution~\eqref{eq:chain_rule} and the training distribution~\eqref{eq:softmax_train_prob} without a regularization factor $\delta$. For the optimization, we use Adam optimizer~\cite{Kingma2014}. To find the best hyperparameters that provided the lowest utility, we have conducted a grid search with the learning rates $\eta = 10^{-4}, 10^{-3}, 10^{-2}$ and the hidden dimensions $d_h = 8,16,32,64,128$. We find that $\eta = 10^{-3}$ and $d_h = 32$ are optimal for both the two values of the training data portion $\epsilon$.

\subsection{Transformers (TFs)}

Transformers (TFs) are attention models that have sparked exciting advances in natural language processing~\cite{Transformer2017}, namely in applications related to language translation, text summarization, and language modeling. Similarly to the RNN, TFs can have the autoregressive property, and they have been proposed in the literature as a solution to the vanishing gradient problem in RNNs~\cite{Pascanu2012}. The attention mechanism in TFs allows handling long-range correlations compared to traditional RNNs~\cite{Transformer2017, Huitao2019}. TFs can also process long data sequences compared to traditional neural network architectures.

In our simulations, we use the traditional implementation of TF decoders in Ref.~\cite{Transformer2017} without using a TF encoder in a similar fashion to the RNN. First, we embed our inputs using a multilayer perceptron (MLP) with a Leaky ReLU activation, and then we add positional encoding. Next, we use a one-layered TF with one attention head. Additionally, the outputs are passed to a two-layer feed-forward neural network with a ReLU activation in the hidden layer. To reduce the search space, the size of the hidden dimension in the feed-forward neural network is chosen to be the same as the size of the embedding dimension, which we also denote as $d_h$~\cite{Transformer2017}. The latter is fine-tuned in the range of possibilities $d_h = 8, 16, 32, 64, 128$ along with a range of learning rates $\eta = 10^{-4}, 10^{-3}, 10^{-2}$. For the training, we minimize the KL divergence between the TF probability distribution and the re-weighted training distribution~\eqref{eq:softmax_train_prob}. We find that $d_h = 64$ and $\eta = 10^{-2}$ provided the lowest utilities for both values of the training data portion $\epsilon$.


\subsection{Variational Autoencoders (VAEs)}

Variational autoencoders (VAEs) are approximate likelihood generative models that can learn to represent and reconstruct data~\cite{goodfellow2016nips}. Initially, a VAE encoder maps an input data point to a latent representation $\bm{z}$. Next, a VAE decoder uses the latent representation to reconstruct the original input data point. Finally, the VAE compares the reconstruction and the initial input data point and is trained until the reconstruction error is as small as  possible. In this case, the VAE cost function corresponds to the evidence lower bound (ELBO) of the negative log-likelihood~\cite{CyclicalVAE2019}.

In our study, the encoder and the decoder are built as an MLP with two hidden layers with a CELU ($\alpha = 2$) activation~\cite{CELU2017}. The encoder is followed by two separate MLP layers for the purpose of implementing the reparametrization trick~\cite{kingma2013auto,goodfellow2016nips}. To reduce the search space of hyperparameters, the size of these hidden layers is taken to be the same as the size of the latent representation. The latter is chosen from the range $8, 16, 32, 64, 128$. For the optimization, we explore different learning rates $\eta = 10^{-4}, 10^{-3}, 10^{-2}$. We also note that the temperature $\beta$ was not added to the training of VAEs~\cite{higgins2017betavae}. After conducting a grid search, we find that a latent dimension of $128$ and a learning rate $\eta = 10^{-3}$ are the optimal hyperparameters.


\subsection{Generative Adversarial Networks (GANs)}

Generative adversarial networks (GANs) are a class of neural networks that consists of a generator and a discriminator~\cite{goodfellow2014generative, goodfellow2016nips}. The generator is optimized based on a dataset, while the discriminator is trained to distinguish between real and generated data points. The two networks are trained together until reaching Nash equilibrium, where the generator produces data that is very similar to the original dataset while the discriminator becomes much better at distinguishing real from generated data.

In our study, we use Wasserstein GANs (WGAN)~\cite{arjovsky2017wasserstein}, which are a variant of GANs with a loss function called the Wasserstein loss (which is also known as the Earth Mover's distance). For the optimization of the loss function, we use Adam optimizer. An interesting feature about WGANs is that they are less susceptible to mode collapse. They can also be used for a wide range of applications, including audio synthesis, text generation, and image generation, and a wide range of areas of science~\cite{Dash_2021}.

In our numerical implementation, we choose the generator and discriminator as two feed-forward neural networks with two hidden layers with CELU $(\alpha = 2)$ activation~\cite{CELU2017}. For simplicity, we choose the size of the gaussian prior to being the same as the width of the hidden layers. We fine-tune the hyperparameters with a grid search on the widths $8, 16, 32, 64, 128$ and the learning rates $\eta = 10^{-4}, 10^{-3}, 10^{-2}$. We find that a prior size of $8$ and $\eta = 10^{-2}$ are optimal for the data portion $\epsilon = 0.01$, whereas a prior size of $128$ with $\eta = 10^{-3}$ works best for $\epsilon = 0.001$. 

\begin{table*}[p]
    \centering
    \footnotesize
    \begin{tabular}{|c|c|c|}\hline
       Generative model & Hyperparameter & Value \\\hline
      \multirow{4}{*}{QCBM} & Number of layers & $8$ \\
      &  Circuit topology & Line topology \\
       & Optimizer & CMA-ES optimizer with $\sigma_0 = 0.1$ \\
      & Initialization & Random initialization between $-\pi/2$ and $\pi/2$ \\
      & Total number of parameters & 256
       \\          
       \hline
      \multirow{6}{*}{RNN} & Architecture & GRU \\
      &  Number of layers & $1$ \\
       & Optimizer & Adam optimizer \\
       & Learning rate & $10^{-3}$ \\
      & Number of hidden units & $32$ \\
      & Total number of parameters & 3456
       \\    
       \hline   
      \multirow{7}{*}{TF} & Number of layers & $1$ \\
       & Number of attention heads & $1$ \\
       & Embedding dimension size & $64$ \\
      & Size of FFNN output & $64$ \\   
       & Optimizer & Adam optimizer \\
      & Learning rate & $10^{-3}$ \\
      & Total number of parameters & 25538
       \\        
       \hline   
      \multirow{12}{*}{VAE} & Prior size & $128$ \\
       & Encoder architecture & FFNN with CELU ($\alpha = 2$) activation\\
       & Encoder hidden layers width & $128$ \\
       & Number of hidden layers of encoder net & $2$ \\
       & Decoder architecture & FFNN with CELU ($\alpha = 2$) activation \\
       & Decoder hidden layers width & $128$ \\
       & Number of hidden layers of decoder net & $2$ \\
       & Temperature $1/\beta$ & $0.0$ \\
       & Optimizer & Adam optimizer \\
      & Learning rate & $10^{-3}$ \\
      & Total number of parameters & 87828
       \\      
       \hline          
      \multirow{11}{*}{WGAN $(\epsilon = 0.01)$} & Prior size & $8$ \\
       & Generator architecture & FFNN with CELU ($\alpha = 2$) activation\\
       & Generator hidden layers width & $8$ \\
       & Number of hidden layers of generator net & $2$ \\
       & Discriminator architecture & FFNN with CELU ($\alpha = 2$) activation \\
       & Discriminator hidden layers width & $8$ \\
       & Number of hidden layers of discriminator net & $2$ \\
       & Optimizer & Adam optimizer \\
       & Gradient regularization & 10.0 \\ 
      & Learning rate & $10^{-2}$ \\
      & Total number of parameters & 573
       \\
       \hline          
      \multirow{11}{*}{WGAN $(\epsilon = 0.001)$} & Prior size & $128$ \\
       & Generator architecture & FFNN with CELU ($\alpha = 2$) activation \\
       & Generator hidden layers width & $128$ \\
       & Number of hidden layers of generator net & $2$ \\
       & Discriminator architecture & FFNN with CELU ($\alpha = 2$) activation \\
       & Discriminator hidden layers width & $128$ \\
       & Number of hidden layers of discriminator net & $2$ \\
       & Optimizer & Adam optimizer \\
       & Gradient regularization & 10.0 \\
       & Learning rate & $10^{-3}$ \\
     & Total number of parameters & 54933
       \\
       \hline          
    \end{tabular}    
    \caption{Hyperparameters used to obtain the results reported in this study. FFNN stands for a feed-forward neural network. We also report the total number of parameters for each model, where the QCBM has the lowest number of variational parameters.}
    \label{tab:hyperparams}
\end{table*}

\section{Pre-generalization and validity-based generalization results}\label{app:generalization_metrics}

The focus of pre-generalization is to check whether a generative model can generate samples outside the training data. This can be done by generating a set of queries $\mathcal{G} = \{ \bm{x}^{(1)}, \bm{x}^{(2)}, \ldots, \bm{x}^{(Q)} \}$ with size $Q$ to compute the ratio:
\begin{equation*}
    E = \frac{|G_{\text{new}}|}{Q},
\end{equation*}
where $G_{\text{new}}$ is the set of unseen (i.e., out-of-training) samples among the set of generated queries $\mathcal{G}$. This metric is called {\it exploration}, and the larger its value, the more our model of interest can \emph{explore} different configurations outside the training data.

Given a set of constraints, it is also desirable that a generative model generates queries that satisfy these constraints, which are typical in combinatorial optimization problems. In this case, these queries are called valid. We can define the first validity-based metric, called the {\it generalization rate}:
\begin{equation*}
    R = \frac{|G_{\text{sol}}|}{Q},
\end{equation*}
where $G_{\text{sol}}$ is the set of valid and unseen queries. This metric quantifies the likelihood of our generated samples that are both unseen and valid, and it can be renormalized to $1$~\cite{Gili2022}. In this case, we can define the normalized rate as:
\begin{equation*}
    \tilde{R} = \frac{R}{1-\epsilon}.
\end{equation*}
Additionally, we can define the {\it generalization fidelity} as
\begin{equation*}
    F = \frac{|G_{\text{sol}}|}{|G_{\text{new}}|},
\end{equation*}
which quantifies the likelihood of a generative model to produce valid unseen samples out of the generated unseen samples, rather than invalid ones.~\footnote{This metric is not to confuse with the fidelity measure between two probability distributions or two quantum states.}.  

In some applications~\cite{nica2022evaluating}, it is also important that a generative model provides a diverse set of unseen and valid samples. For this reason, we can define a third metric called the {\it generalization coverage} defined as:
\begin{equation*}
    C = \frac{|g_{\text{sol}}|}{|S| - T} = \frac{|g_{\text{sol}}|}{|S|(1 - \epsilon)},
\end{equation*}
where $g_{\text{sol}}$ is the set of unique queries that are both unseen and valid. This metric allows quantifying the diversity of solutions as opposed to the fidelity and the rate. The coverage can also be renormalized to $1$ by defining a normalized coverage:
\begin{equation*}
    \tilde{C} = \frac{C}{1-(1-1/(|S|(1-\epsilon))^{Q}} \approx \frac{|g_{\text{sol}}|}{Q},
\end{equation*}
where the denominator corresponds to the ideal expected value of the coverage~\cite{gili2022evaluating} and the approximation holds in the regime $Q \ll |S|(1-\epsilon)$. The latter is typical when we have a relatively large number of binary variables. We highlight that it is not necessary to know $|S|$ (or $\epsilon$) in order to compute the coverage: since we are interested in using these metrics to compare models, one can simply utilize coverage ratios among models thus avoiding the need of having prior knowledge about the size of the solution space. Lastly, we specify that we only use the first track T1 to compute the validity-based metrics, since there is no cost involved in the calculation, which makes the second track T2 not applicable.

In this section, we also present the results from the pre-generalization and the validity-based generalization metrics to provide an additional perspective on the results in the main text. In Fig.~\ref{fig:validity_metrics}, we show the exploration $E$, normalized rate $\tilde{R}$, the fidelity $F$ and the normalized coverage $\tilde{C}$ for a data portion $\epsilon = 0.01$ in panel (a) and for $\epsilon  = 0.001$ in panel (b). A common feature of the results is that QCBMs are competitive with TFs and RNNs on the validity metrics and superior to VAEs and WGANs. Besides the quality-based generalization results in the main text, these observations favor the QCBMs' ability to provide the best compromise for validity-based and quality-based generalization compared to the other generative models. The observation of VAEs being less efficient at this generalization level could be explained by the mode collapse that occurs at zero temperature. Using a finite temperature $1/\beta$ in future studies could be helpful to mitigate this limitation~\cite{higgins2017betavae}. We also observe that the WGAN has a similar behavior to the VAE, which can also be related to mode-collapse.

\begin{figure*}[htb]
    \centering
    \includegraphics[width = \linewidth]{figures/validity_generalization.pdf}
    \caption{\textbf{Validity-based generalization comparison between QCBMs, RNNs, TFs, WGANs and VAEs} for a system size $N_{\text{var}} = 20$. Here we plot the exploration $E$, normalized rate $\tilde{R}$, fidelity $F$ and normalized coverage $\tilde{C}$ against the number of training steps for $\epsilon = 0.01$ in panel (a), and for $\epsilon = 0.001$ in panel (b). The models are trained using $N_{\text{seeds}} = 10$ random seeds, and the outcomes of the metrics are average over these seeds with errors bar estimated as the standard deviation, which can be computed for each metric as $\sqrt{\text{Var}/N_{\text{seeds}}}$.}
    \label{fig:validity_metrics}
\end{figure*}


\section{Additional quality-based generalization results}
\label{app:additional_data}

In this appendix, we report the best values found by our generative models throughout the training shown in Fig.~\ref{fig:quality_metrics}. The error bars are computed by averaging over the outcome of $10$ different training seeds. For $\epsilon = 0.001$, we report the best performances for each metric during the training in Tab.~\ref{tab:best_result_eps1e-3}. 

\begin{table*}[]
\begin{tabular}{cccc}
\hline
{\color[HTML]{000000} \textbf{T1}}   & {\color[HTML]{000000} $\boldsymbol{C_q}$}              & {\color[HTML]{000000} \textbf{MV}}                & {\color[HTML]{000000} \textbf{U}}                  \\ \hline
{\color[HTML]{000000} QCBM} & {\color[HTML]{000000} \textbf{7e-4}}      & {\color[HTML]{000000} \textbf{-19}}      & {\color[HTML]{000000} \textbf{-17.30(8)}} \\ \hline
{\color[HTML]{000000} RNN}  & {\color[HTML]{000000} \textbf{7e-4}}      & {\color[HTML]{000000} \textbf{-19}}      & {\color[HTML]{000000} -15.03(13)}         \\ \hline
{\color[HTML]{000000} TF}   & {\color[HTML]{000000} \textbf{6.9(1)e-4}} & {\color[HTML]{000000} \textbf{-18.9(1)}} & {\color[HTML]{000000} -16.07(42)}         \\ \hline
{\color[HTML]{000000} VAE}  & {\color[HTML]{000000} 6.7(1)e-4}          & {\color[HTML]{000000} \textbf{-19}}      & {\color[HTML]{000000} -16.46(7)}          \\ \hline
{\color[HTML]{000000} WGAN} & {\color[HTML]{000000} 6.4(2)e-4}          & {\color[HTML]{000000} \textbf{-19}}      & {\color[HTML]{000000} -15.21(13)}         \\ \hline \\ \hline
{\color[HTML]{000000} \textbf{T2}}   & {\color[HTML]{000000} $\boldsymbol{C_q}$} & {\color[HTML]{000000} \textbf{MV}} & {\color[HTML]{000000} \textbf{U}} \\ \hline
{\color[HTML]{000000} QCBM} & \textbf{0.04}                & -16              & \textbf{-14.60(5)}       \\ \hline
{\color[HTML]{000000} RNN}  & 0.005(2)           & -11.5(5)        & -10.94(46)               \\ \hline
{\color[HTML]{000000} TF}   & 0.024(4)          & -14.5(4)         & 12.98(39)                \\ \hline
{\color[HTML]{000000} VAE}  & 0.037(1)                     & \textbf{-16.2(2)}         & \textbf{-14.58(7)}                \\ \hline
{\color[HTML]{000000} WGAN} & 0.036(1)                     & -16             & -14.14(12)               \\ \hline
\end{tabular}

\caption{A summary of the best values of quality coverage ($C_q$), minimum value ($MV$) and utility ($U$) for the data portion $\epsilon = 0.001$ from Fig.~\ref{fig:quality_metrics} and for the generative models (QCBM, RNN, TF, VAE and WGAN). Values in bold correspond to the best performance among the different models. Furthermore, the digits in parentheses correspond to the uncertainty over the last digit of the reported numbers. For track T1, the QCBM is the winner, whereas for track T2, QCBM is competitive with the VAE.}
\label{tab:best_result_eps1e-3}
\end{table*}


\begin{table*}[]
\begin{tabular}{cccc}
\hline
{\color[HTML]{000000} \textbf{T1}}   & {\color[HTML]{000000} $\boldsymbol{C_q}$}              & {\color[HTML]{000000} \textbf{MV}}                & {\color[HTML]{000000} \textbf{U}}                  \\ \hline
{\color[HTML]{000000} QCBM} & {\color[HTML]{000000} \textbf{7e-4}}      & {\color[HTML]{000000} \textbf{-19}}      & {\color[HTML]{000000} -18.19(7)} \\ \hline
{\color[HTML]{000000} RNN}  & {\color[HTML]{000000} \textbf{7e-4}}      & {\color[HTML]{000000} \textbf{-19}}      & {\color[HTML]{000000} -15.01(12)}         \\ \hline
{\color[HTML]{000000} TF}   & {\color[HTML]{000000} 6.7(2)e-4} & {\color[HTML]{000000} \textbf{-18.8(2)}} & {\color[HTML]{000000} -16.12(26)}         \\ \hline
{\color[HTML]{000000} VAE}  & {\color[HTML]{000000} \textbf{7e-4}}               & {\color[HTML]{000000} \textbf{-19}}      & {\color[HTML]{000000} \textbf{-19}}                \\ \hline
{\color[HTML]{000000} WGAN} & {\color[HTML]{000000} \textbf{7e-4}}               & {\color[HTML]{000000} \textbf{-19}}      & {\color[HTML]{000000} -17.86(46)}        \\ \hline \\ \hline
{\color[HTML]{000000} \textbf{T2}}   & {\color[HTML]{000000}$\boldsymbol{C_q}$} & {\color[HTML]{000000} \textbf{MV}} & {\color[HTML]{000000} \textbf{U}} \\ \hline
{\color[HTML]{000000} QCBM} & 0.038(1)            & -15.8(1)       & -14.52(16)      \\ \hline
{\color[HTML]{000000} RNN}  & 0.015(2)           & -13.5(2)         & -12.78(15)               \\ \hline
{\color[HTML]{000000} TF}   & 0.027(5)            & -14.7(5)        & -13.74(41)               \\ \hline
{\color[HTML]{000000} VAE}  & \textbf{0.05}                         & -17            & -15.58(5)                \\ \hline
{\color[HTML]{000000} WGAN} & \textbf{0.051(2)}                     & \textbf{-17.7(3)}         & \textbf{-15.96(19)}               \\ \hline
\end{tabular}
\caption{A report of the numerical values as in Tab.~\ref{tab:best_result_eps1e-3} for the data portion $\epsilon = 0.01$ reported in Fig.~\ref{fig:quality_metrics}. Values in bold correspond to the best performance among the different models. For track T1, VAE is the winner, whereas, for track T2, WGAN is superior compared to the other models.}
\label{tab:best_result_eps1e-2}
\end{table*}
