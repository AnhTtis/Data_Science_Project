\section{Methods}\label{s:methods}
\begin{figure*}[htb]
\includegraphics[width=0.95\linewidth]{figures/race_and_runners.pdf}
\caption{\textbf{The practical quantum advantage (PQA) race: a sports analogy}. In panel (a), each runner (generative model) is characterized by (some of) its strengths and weaknesses, namely: training efficiency, sampling efficiency, and expressive power. Note that the power bars are indicative, and that is far from trivial to determine, but some insights can be obtained from intuition from the theoretical characterization of some of the models, e.g., via computational quantum advantage papers, or known properties or highlights for each model. A complete characterization of the runner can be used to identify the odds-on favorite, independent of the specific race context. In panel (b), the different runners are embedded into a context (i.e., `the real-world application setting') represented as a concrete instance of a hurdles race. They all run the same race, but they see the hurdles differently according to their strengths and weaknesses. The runners can compete on different tracks, for instance, on shorter or longer tracks. For the PQA race to be well defined, it is necessary to clearly state what track is taken under examination. In this study, we propose two tracks, motivated by the limitation of sampling or cost evaluation budget. Once the track is selected, we can evaluate runners using different criteria: application-driven metrics need to be defined to fully characterize the race. Our evaluation criterion is the quality-based generalization, with appropriate metrics defined in Sec.~\ref{ss:metrics} (see also Fig.~\ref{f:lenses} for further specific details).}
\label{f:sports-analogy}
\end{figure*}
\subsection{Generalization Metrics}\label{ss:metrics}

The evaluation of unsupervised generative models is a challenging task, especially when one aims to compare different models in terms of generalization. In this work, we focus on discrete probability distributions of bitstrings where an unambiguous definition of generalization is possible~\cite{gili2022evaluating}. Here we start from the framework provided in Ref.~\cite{gili2022evaluating} that puts different generative models on an equal footing and allows us to assess the generalization performances of each generative model from a practical perspective.

In this framework, we assume that we are given a solution space $S$ that corresponds to the set of bitstrings that satisfy a constraint or a set of constraints, such that $|S| \leq 2^{N_{\text{var}}}$ where $N_{\text{var}}$ is the number of binary variables in a bitstring. A typical example is the portfolio optimization problem, where there is a constraint on the number of assets to be included in a portfolio. Additionally, we assume that we are given a training dataset $\mathcal{D}_{\text{train}} = \{ \bm{x}^{(1)}, \bm{x}^{(2)}, \ldots, \bm{x}^{(T)} \}$, where $T = \epsilon |S|$ and $\epsilon$ is a tunable parameter that controls the size of the training dataset such that $0 < \epsilon \leq 1$.

The metrics provided in Ref.~\cite{gili2022evaluating} allow probing different features of generalization. Notably, there are three main pillars of generalization: (1) pre-generalization, (2) validity-based generalization, and (3) quality-based generalization. In the main text, we focus on quality-based generalization and provide details about pre-generalization and validity-based generalization in App.~\ref{app:generalization_metrics}.

In typical real-world applications, it is desirable to generate high-quality samples that have a low cost $c$ compared to what has been seen in the training dataset. In the quality-based generalization framework, we can define the {\it minimum value} as:
\begin{equation*}
    \text{MV} = \min_{\bm{x} \in G_{\text{sol}}} c(\bm{x}),
\end{equation*}
which corresponds to the lowest cost in a given set of unseen and valid queries $G_{\text{sol}}$, which we obtain after generating a set of queries $\mathcal{G} = \{ \bm{x}^{(1)}, \bm{x}^{(2)}, \ldots, \bm{x}^{(Q)} \}$ from a generative model of interest. In our terminology, a sample $\bm{x}$ is valid if $\bm{x} \in S$ and it is considered unseen if $\bm{x} \notin D_{\text{train}}$.

To avoid the chance effect of using the minimum, we can average over different random seeds. We can also define the {\it utility} that circumvents the use of the minimum through:
\begin{equation*}
    U = \langle c(\bm{x}) \rangle_{\bm{x} \in P_5},
\end{equation*}
where $P_5$ corresponds to the set of the $5\%$ lowest-cost samples obtained from $G_{\text{sol}}$. The averaging effect allows us to ensure that a low cost was not obtained by chance.

In quality-based generalization, it is also valuable to have a diverse set of samples that have high quality. To quantify this desirable feature, we define the {\it quality coverage} as
\begin{equation*}
    C_q = \frac{|g_{\text{sol}}(c < \min_{\bm{x} \in \mathcal{D}_{\text{train}}} c(\bm{x}))|}{Q},
\end{equation*}
where $g_{\text{sol}}(c < \min_{\bm{x} \in \mathcal{D}_{\text{train}}} c(\bm{x}))$ corresponds to the set of unique valid and unseen samples that have a lower cost compared to the minimal cost in the training data. The choice of the values of the number of queries $Q$ depends on the tracks/rules of comparison presented in Sec.~\ref{sec:PQA}.

\subsection{Defining practical quantum advantage}
\label{sec:PQA}
In this work, we refer to \emph{practical quantum advantage} (PQA) as the ability of a quantum system to perform a useful task - where `useful' can refer to a scientific, industrial, or societal use - with performance that is faster or better than what is enabled by any existing classical system~\cite{alsing2022accelerating, QuantumUtility23}. We highlight that this concept differs from the \emph{computational quantum advantage} notion (originally introduced as quantum supremacy), which refers instead to the capability of quantum machines to outperform classical computers, providing a speedup in solving a given task, which would otherwise be classically unsolvable, even using the best classical machine and algorithm~\cite{Preskill2018, boixo2018characterizing, bouland2019complexity, Huang2022}. While the latter definition is focused on provable speedup, the former practical notion aims at showing an advantage, either temporal or qualitative (or both), in the context of a real-world task. 

In computational quantum complexity, demonstrating a quantum advantage often boils down to proving a theoretical result that rules out a possibility for any classical algorithm to outperform a certain classical algorithm. PQA can be defined differently based on the practical aspects of a problem of interest and the availability of classical algorithms for the specific task at hand. Here we take inspiration from Ref.~\cite{Ronnow25072014}, to define four different types of PQA.

The first version, which we refer to as {\it provable PQA} (PrPQA) has the ultimate goal of demonstrating the superiority of a quantum algorithm with respect to the best classical algorithm, where the proof is backed up by complexity theory arguments. One example of such a milestone would be to extend the quantum supremacy experiment~\cite{Google2019supremacy, liu2021redefining} to practical and commercially relevant use cases. Most likely, this scenario will require fault-tolerant quantum devices and a key practical setting is still missing for ML use cases. Examples of this would be to show a realization of Shor's algorithm at scale, although backed to some extent by complexity theory arguments. Even in that case, it is not proven that a polynomial classical algorithm exists. To the best of our knowledge, the equivalent of Shor's algorithm in the context of real-world ML tasks, i.e., useful enough to be included in the definition of {\it provable} PQA provided above, is still missing. Since we do not yet have either the theoretical or the hardware capabilities for such an ambitious goal, we focus here on the following three classes, which might be more reachable with near- and medium-term quantum devices. We define \textit{robust PQA} (RPQA) as a practical advantage of a quantum algorithm compared to the best available classical algorithms. An RPQA can be short-lived when a better classical algorithm is potentially developed after an RPQA has been established.

However, on some occasions, there is no clear consensus about the status of the best available classical algorithm as it depends on each scientific community. To go around that, we can conduct a comparison with a state-of-the-art classical algorithm or a set of classical algorithms. If there is a quantum advantage in this case, we can refer to it as {\it potential PQA} (PPQA). Within this scenario, a genuine attempt to compare against the best-known classical algorithms has to be conducted with the possibility that a PPQA is short-lived with the development or discovery of more powerful and advanced classical algorithms. A weaker scenario corresponds to the case where we promote a classical algorithm to its quantum counterpart to investigate whether quantum effects are useful. A quantum advantage in this scenario is an example of {\it limited PQA} (LPQA). A potential case is a comparison between a restricted Boltzmann machine~\cite{RBM_Hinton} and a quantum Boltzmann machine~\cite{Amin2016}. In this study, we are pushing the search for PQA beyond the LPQA scenario to a PPQA, with the hope to include a more comprehensive list of the best available classical models in our comparison in future studies.

A significant difference between our definitions and the types of quantum speedup provided in Ref.~\cite{Ronnow25072014} is that, for the PQA case, we do not require a scaling advantage as a function of the problem size. The main reason is that industry or application-relevant problems rarely vary in size, and some of them have a very well-defined unique size. For instance, the problem of portfolio optimization is usually defined over a specific asset universe of fixed size, such as the S \& P 500 market index, which involves an optimization over $N=500$ variables. As long as we have a quantum algorithm that performs better than any of the available classical solutions at the right problem size and under the exact real-world conditions, this is already of commercial value and qualifies for PQA.

In this study, we consider different generative models and let them compete for PPQA, and for this `race' to be well-defined, it is essential to establish its rules first. When searching for any of the variants of PQA in generative modeling, we argue that generalization, as defined and equipped with quantitative metrics in Ref.~\cite{gili2022evaluating}, is an essential evaluation criterion that should be used to establish whether quantum-circuit-based or quantum-inspired models have a better performance over classical ones. A fair assessment of generative models' performance consists in measuring their ability to generate novel high-scoring solutions for a given task of interest. 

To illustrate our approach, we propose a simple sports analogy. Let us consider a hurdles race, where different runners are competing against each other. Each generative model can thus be seen as a runner in such a race. Each contender has their strengths and weaknesses, which make them see hurdles differently, and which can be quantitatively analyzed and gathered to produce a full characterization of their potential performance in the race (see Fig.~\ref{f:sports-analogy}(a)). Thus, one can aim to investigate relevant model features and determine whether they constitute a strength for the model under examination. For instance, when analyzing a quantum model, one could consider its expressive power as a strength, and its training efficiency as a weakness, and vice versa for a classical model (at least as a first-order approximation). One possibility to get a more complete intuition of this characterization is to leverage the results from the \emph{computational quantum advantage} studies on synthetic datasets.

From a full characterization of all the runners, one can establish the odds-on favorite to win the race, i.e., the fastest contender. However, hurdles races take place in a specific concrete context, for instance, with given wind and track surface conditions, which affect the competition outcome significantly (see Fig.~\ref{f:sports-analogy}(b)). The PQA approach takes this concrete context into account when evaluating the contenders, who are analyzed not only `in principle' but also embedded in a specific context. For example, the track field's length is crucial for the evaluation since different runners can perform differently if the `rules of the game' are modified. The conditions of the race affect the runners' performance, which is equivalent to say that generative models are affected by factors such as the type and size of the dataset, the ground truth distribution to be learned, etc. Each instance of a generative modeling task is unique, just as the conditions for every day of the competitions could be unique. As such, the tracks and the race conditions must be specified before the competition happens, to clarify the precise setting where the search for PQA (or, in our study, for PPQA) takes place.

Lastly, we argue that, when evaluating performance in a concrete instance of a race on a given track, the measure of success for an athlete might not necessarily be attributed to the maximum speed. For instance, what matters to win a race can be the highest speed reached throughout the race, the optimal trajectory, the best technical execution of jumps, etc. Outside the analogy, for a practical implementation of a task, other factors than the speedup are likely needed to be taken into account to judge if quantum advantage has occurred. Quality-based generalization is one of these playgrounds. Although validity-based generalization is also interesting in many use cases, we focus on quality-based generalization. The latter is particularly relevant when considering combinatorial optimization problems, as suggested by the generative enhanced optimization (GEO) framework~\cite{alcazar2021enhancing}. This reference introduces a connection between generative models and optimization and  we take inspiration from this, which is in and of itself a new perspective on a family of commercially valuable use cases for generative models beyond large language models and image generation, but that is not fully appreciated yet by the ML community. Remarkably, quality-based generalization turns out to be paramount when the generative modeling task under examination is linked to an optimization problem or whenever a suitable cost/score map can be associated with the samples. It is thus desirable to learn to generate solutions with the lowest possible cost, at least lower (i.e., of better quality) compared to the available costs in a training dataset. The utility, the minimum value, and the quality coverage have been introduced precisely to quantify this capability. However, these metrics can be computed in different ways according to the main features of a specific use case, i.e., based on the track field defining the rules of the game. In Sec.~\ref{s:rules}, we propose two distinct `track fields' that give us two different lenses, according to which we conduct a comparison of generative models toward PPQA in an optimization context that takes the resource bottlenecks of the specific use case into consideration. 


\subsection{Competition details}\label{s:rules}

\begin{figure*}[htb]
\centering
  \includegraphics[width=0.8\linewidth]{figures/flowchart.pdf}%
\caption{\textbf{An illustration of the scheme used for training and assessing the quality-based generalization of our generative models}. Given the training dataset with size $|D_{\text{train}}| = \epsilon |S|$, sampled from the \textit{Evens} distribution where $|S| = 2^{N_{\text{var}}-1}$, we choose different generative models, and select the track we want to compare them on (i.e., select the ``rules of the game" used to probe the generative models). We then train and fine-tune them using the chosen dataset. After this step, we estimate the quality-based metrics $C_q$, $MV$, and $U$ using the selected track, T1 or T2, to assess the quality of the queries generated by each model. In the first track T1, we use $Q = 10^4$ queries to estimate our metrics, whereas, for the second track T2, we require $Q_u = 10^2$ unique and valid samples at most to compute our metrics. We also choose different values of the data portion $\epsilon$ to investigate its influence on the generalization of each generative model. For a fair comparison, we use the same training budget $N_{\text{epochs}} = 1000$. Additionally, we use $N_{\text{seeds}} = 10$ different initializations for each generative model to obtain error bars on metrics.}
\label{f:lenses}
\end{figure*}

In our study, we compare several quantum and state-of-the-art classical generative models. On the quantum side, we use quantum circuit Born machines (QCBMs)~\cite{Benedetti2019} that are trained with a gradient-free optimization technique. On the classical side, we use Recurrent Neural Networks (RNN)~\cite{GRU2014}, Transformers (TF)~\cite{Transformer2017}, Variational Autoencoders (VAE)~\cite{rolfe2016discrete}, and  Wasserstein Generative Adversarial Networks (WGAN)~\cite{goodfellow2016nips}. More details about these models and their characteristics along with their hyperparameters are explained in App.~\ref{app:architectures}.

As a test bed, and to illustrate a concrete realization of our framework, we choose a re-weighted version of the \emph{Evens} (also known as parity) distribution where each bitstring with a non-zero probability has an even number of ones~\cite{Gili2022}. In this case, the size of the solution space, for $N_{\text{var}}$ binary variable, is given by $|S| = 2^{N_{\text{var}}-1}$. Furthermore, we choose a synthetic cost, called the negative separation cost $c$~\cite{Gili2022}, which is defined as the negative of the largest separation between two $1$ in a bitstring, i.e., $c(\bm{x})=-(z + 1)$, where $z$ is the largest number of consecutive zeros in the bitstring $\bm{x}$. For instance, $c(\text{`}11100011\text{'}) = -4$, $c(\text{`}10110011\text{'}) = -3$, and $c(\text{`}11111111\text{'}) = -1$. 

Given this cost function, we can define our \emph{re-weighted} training distribution $P_{\text{train}}$ over the training data, such that:
\begin{equation}
    P_{\text{train}}(\bm{x}) = \frac{\exp(-\beta c(\bm{x}))}{\sum_{\bm{y} \in D_{\text{train}}} \exp(-\beta c(\bm{y}))},
    \label{eq:softmax_train_prob}
\end{equation}
with inverse temperature $\beta \equiv \hat{\beta}/2$, where $\hat{\beta}$ is defined as the standard deviation of the scores $c$ in the training set. If a data point $\bm{x} \notin \mathcal{D}_{\text{train}}$, then we assign $P_{\text{train}}(\bm{x}) = 0$. The re-weighting procedure applied to the training data encourages our trained models to generate samples with low costs, with the hope that we sample unseen configurations that have a lower cost than the costs seen in the training set~\cite{alcazar2021enhancing}. To achieve the latter, it is crucial that the KL divergence between the generative model distribution and the training distribution does not tend to zero during the training to avoid memorizing the training data~\cite{Gili2022}. It is important to note that it is not mandatory to apply the re-weighting of the samples as part of the generative modeling task. However, the re-weighting procedure in Eq.~\ref{eq:softmax_train_prob} has been shown to help in finding high quality samples~\cite{alcazar2021enhancing,gili2022evaluating,Gili2022,Lopez-Piqueres2022}. Since all the models will be evaluated in their capabilities to generate low-cost and diverse samples, as dictated by the evaluation criteria $C_q$, $MV$, and $U$, we used the re-weighted dataset to train all the generative models studied here. In reality, the bare training set consists of $T$ data points with their respective cost values $c$ and any other transformation could be applied to facilitate the generation of high-quality samples. 

In our simulations, we choose $N_{\text{var}} = 20$ as the size of each bitstring, and we train our generative models for two training set sizes corresponding to $\epsilon = 0.001$ and $\epsilon = 0.01$ (see Fig.~\ref{f:lenses}). We choose the training data for the two different epsilons, such that we have the same minimum cost of $-12$ for the two datasets. The purpose of this constraint is to rule out the effect of the minimum seen cost in our experiments. We have selected these small epsilon values to probe the model's capabilities to successfully train and generalize in this scarce-data regime.

We focus our attention on evaluating quality-based generalization for the aforementioned generative models (the `runners') using two different competition rules (the `tracks'). These two tracks described next are motivated, respectively, by the sampling budget and the difficulty of evaluating a cost function, which are common bottlenecks affecting real-world tasks. Specifically:
\begin{itemize}
    \item Track 1 (T1): there is a fixed budget of queries $Q$ generated by the generative model to compute $C_q$, $MV$ and $U$ for the purpose of establishing the most advantageous models. This criterion is appropriate in the case where it is cheap to compute the cost associated with samples while only having access to a limited sampling budget. For instance, a definition of PPQA based on T1 can be used in the case of generative models requiring expensive resources for sampling, such as QCBMs executed on real-world quantum computers. Here, one aims to reduce the number of measurements as much as possible while still being able to see an advantage in the quality of the generated solutions.
    
    \item Track 2 (T2): there is a fixed budget $Q_u$ of unique, unseen and valid samples to compute the quality coverage, the utility and the minimum value. This approach implies the ability of sampling from the trained models repeatedly to get up to $Q_u$ unique, unseen and valid queries. Note that some models might never get to the \emph{target $Q_u$}, for instance, if they suffer from mode collapse. In this case, the metrics can be computed using the \emph{reached} $\tilde{Q}_u$. This track is motivated by a class of optimization problems where the cost evaluation is expensive. Examples of such scenarios include molecule design and drug discovery that involve clinical trials. In these settings, the cost function is expensive to compute. This track is aimed to provide a proxy reflecting these real-world use cases. In this case, one aims to avoid excessive evaluations of the cost function, i.e., for repeated samples.
\end{itemize}

Regarding the sampling budget, we use $Q = 10^4$ configurations to estimate our quality metrics for track T1. From the perspective of track T2, we sample until we obtain $Q_u = 10^2$ unique configurations that are used to compute our quality-based metrics\footnote{We checked how many samples batches are needed, and we observed that $Q = 10^4$ is enough to extract $Q_u = 10^2$ unique configurations for all the generative models in our study.}. Our metrics are averaged over $10$ random seeds for each model while keeping the same data for each portion $\epsilon$. For a fair comparison between the generative models, we conduct a hyperparameters grid search using Optuna~\cite{optuna_2019}, and we extract the best generative model that allows obtaining the lowest utility after $100$ training steps. Note that, in order to carry out the hyperparameters tuning process, one could also utilize $MV$, $C_q$, or any appropriate combination of the three metrics. Additionally, as a fair training budget, we train all our generative models for $N_{\text{epochs}} = 1000$ steps. We compute our quality-based generalization metrics for tracks T1 and T2 after each 100 training steps. We do not include this sampling cost in the evaluation budget ($Q$ or $Q_u$), as in this study we are not focusing on the training efficiency of these models, so we allow potentially unlimited resources for the training process. However, for a more realistic setting, the sampling budget could be customized to keep the training requirements into account. For clarity, Fig.~\ref{f:lenses} provides a schematic representation of our methods. The hyperparameters of each architecture and the parameters count are detailed in Tab.~\ref{tab:hyperparams}. 


