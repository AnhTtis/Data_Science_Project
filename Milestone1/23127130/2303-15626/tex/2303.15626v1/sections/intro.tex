\section{Introduction}\label{s:intro}

Generative modeling has become more widely popular with its remarkable success in tasks related to image generation and text synthesis, as well as machine translation~\cite{LeCun-Nature-2015, Transformer2017, Ramesh_2021, DiffusionModels2022, team2022chatgpt, ouyang2022training}, making this field a promising avenue to demonstrate the power of quantum computers and to reach the paramount milestone of \emph{practical quantum advantage (PQA)}~\cite{PerdomoOrtiz2017}. The most desirable feature in any machine learning (ML) model is \emph{generalization}, and as such, this property should be considered to assess its performance in search of PQA. However, the definition of this property in the domain of generative modeling can be cumbersome, and it is yet an unresolved question for the case of arbitrary generative tasks~\cite{alaa2021faithful}. Its definition can take on different nuances depending on the area of research, such as in computational learning theory~\cite{Vapnik99} or other practical approaches~\cite{zhao2018bias,nica2022evaluating}. Ref.~\cite{gili2022evaluating} defines an unambiguous framework for generalization on discrete search spaces for practical tasks. This approach puts all generative models on an equal footing since it is sample-based and does not require knowledge of the exact likelihood, therefore making it a model-agnostic and tractable evaluation framework. This reference also demonstrates footprints of a quantum-inspired advantage of Tensor Network Born Machines~\cite{han2018unsupervised} compared to Generative Adversarial Networks~\cite{goodfellow2016nips}. 

Remarkably, there is still a lack of a concrete quantitative comparison between quantum generative models and a broader class of classical state-of-the-art generative models, in search of PQA. In particular, quantum circuit Born machines (QCBMs)~\cite{Benedetti2019} have not been compared up-to-date with other classical generative models in terms of generalization, although they have been shown recently for their ability to generalize~\cite{Gili2022}. In this paper, we aim to bridge this gap and provide the first numerical comparison, to the best of our knowledge, between quantum and classical state-of-the-art generative models in terms of generalization. 

In this comparison, these models compete for PQA. For this `race' to be well-defined, it is essential to establish its rules first. Indeed, a clear-cut definition of PQA is not present in the relevant literature so far, especially when it comes to challenging ML applications such as generative modeling, or in general, to practical ML.

Previous works emphasize either computational quantum advantage, or settings that are not relevant from a real-world perspective, or scenarios that use data sets that give an advantage to the quantum model from the start (and also bear no relevance to a real-world setting)~\cite{Havlicek2019, boixo2018characterizing, Google2019supremacy, bouland2019complexity, Madsen2022}. One potential exception would be the case of Ref.~\cite{Huang2022}, which showed an advantage for a quantum ML model in a practical setting. However, besides the unresolved challenge of relying on quantum-loaded data, it is still unclear if it would be relevant to some concrete real-world and large-scale applications, although the authors mention some potential applications in the domain of quantum sensing.

We acknowledge as well previous works that have attempted or proposed ways to perform model comparisons, within generative models and beyond. For instance, a recent work~\cite{VarBench2023} has developed a novel metric for assessing the quality of variational calculations for different classical and quantum models on an equal footing. Another recent study~\cite{riofrio2023performance} proposes a detailed analysis that systematically compares generative models in terms of the quality of training to provide insights on the advantage of their adoption by quantum computing practitioners, although without addressing the question of generalization. In another recent work~\cite{QuantumUtility23}, the authors propose the generic notion of quantum utility, a measure for quantifying effectiveness and practicality, as an index for PQA, but this work differs from our study in the sense that PQA is defined in a broad perspective as the ability of a quantum device to be either faster, more accurate or demanding less energy compared to classical machines with similar characteristics in a certain task of interest. Others have emphasized quantum simulation as one of the prominent opportunities for PQA~\cite{daley2022practical}. In our paper, we share the long-term goal of identifying practical use cases for which quantum computing has the potential to bring an advantage. However, our work is focused on generative models and their generalization capabilities, which is the gold standard to measure the performance of generative ML models in real-world use cases. 

In summary, the goal of this framework and of this study is to set the stage for a quantitative race between different state-of-the-art classical and quantum generative models in terms of generalization in search of PQA, uncovering the strengths and weaknesses of each model under realistic ``race conditions" (see Fig.~\ref{f:sports-analogy}). These competition rules are defined in advance before the fine-tuning of each model and dictated by the desired outcome from real-world motivated metrics and limitations, making our framework  application and/or commercially relevant from the start. Hence, we consider this formalization to be one of the main contributions of this work. This focus is motivated by the growing interest of the scientific and business community in showcasing the value of quantum strategies compared to conventional algorithms, and provides a common ground for a fair comparison based on relevant properties.

This paper is structured as follows. In Sec.~\ref{s:methods}, we provide details about the metrics we use to compare our generative models. We also contribute to the paramount yet unanswered question of better defining PQA in the scope of generalization by formalizing several types of PQA and the specific rules for the competition. In Sec.~\ref{s:results}, we show that QCBMs are competitive with the other classical state-of-the-art generative models and provide the best compromise for the requirements of the generalization framework we are adopting. Remarkably, we demonstrate that QCBMs perform well in the low-data regime, which constitutes a bottleneck for deep learning models~\cite{Deep_learning_appraisal2018, zhang2018strategy, Austin2020} and which we believe to be a promising setting for PQA.






