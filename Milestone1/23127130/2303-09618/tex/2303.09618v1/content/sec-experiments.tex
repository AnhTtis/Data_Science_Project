

\vspace{-.2em}
\section{Experiments}\label{sec:exp}
\vspace{-.3em}


\begin{figure}[t]
\begin{center}
\includegraphics[ width=1\linewidth]{Figures/plot_instructpix2pix_v1_wo_w_rw.pdf}
\end{center}
\vspace{-1.5em}
   \caption{Comparisons between InstructPix2Pix, HIVE  without human feedback, and HIVE with human feedback. It plots tradeoffs between consistency with the input image and consistency with the edit. The higher the better. For all methods, we adopt the same parameters as that in \cite{brooks2022instructpix2pix}.}
\label{fig:plot_instructpix2pix_vs_v1_wofeedback}
\vspace{-1.7em}
\end{figure}

This section presents the experimental results and ablation studies of HIVE's technical choices, demonstrating the effectiveness of our method. For a fair comparison, we adopt the default guidance scale parameters in InstrcutPix2Pix. Through our experiments, we discovered that the conditional reward loss performs slightly better than the weighted reward loss, and therefore, we present our results based on the conditional reward loss.  The detailed comparisons can be found in Sec.~\ref{sec:ablation} and Appendix~\ref{sec:appendix_ablation}.


We evaluate our method using two datasets: a synthetic evaluation dataset with 15,652 image pairs from \cite{brooks2022instructpix2pix} and a self-collected 1K evaluation dataset with real image-instruction pairs. For the synthetic dataset, we follow InstructPix2Pix's quantitative evaluation metric and plot the trade-offs between CLIP image similarity and directional CLIP similarity\cite{Gal2022stylegannana}. For the 1K dataset, we conduct a user study where for each instruction, the images generated by competing methods are reviewed and voted by three human annotators, and the winner is determined by majority votes.








\begin{figure}[t]
	\centering
	\subfigure[\footnotesize{InstructPix2Pix vs HIVE w/o feedback}]{
		\begin{minipage}[b]{0.22\textwidth}
			\includegraphics[width=1\textwidth]{Figures/user_study_instructpix2pix_vs_hive_wo_feedback.pdf}
		\end{minipage}
		\label{fig:userstudy_instructpix2pix_vs_hive_wo_hf}
	}
    \subfigure[\footnotesize{HIVE~w/o~vs~w/~feedback}]{
    	\begin{minipage}[b]{0.22\textwidth}
   		\includegraphics[width=1\textwidth]{Figures/user_study_hive_w_wo_feedback.pdf}
    	\end{minipage}
	\label{fig:userstudy_hive_with_without_hf}
    }
    \vspace{-1em}
	\caption{User study of comparison between (a) InstructPix2Pix vs HIVE without human feedback and (b) HIVE without and with human feedback. HIVE without human feedback obtains 30\% more votes than InstructPix2Pix. HIVE with human feedback obtains 10.8\% more votes than that without human feedback.}
    \label{fig:userstudy_v1}
\vspace{-1em}
\end{figure}



\subsection{Baseline Comparisons}
\label{sec:exp_baseline}

We perform experiments with the same setup as InstructPix2Pix, where stable diffusion (SD) v1.5 is adopted. We compare three models: InstructPix2Pix, HIVE without human feedback, and HIVE with human feedback. We report the quantitative results on the synthetic evaluation dataset in Fig.~\ref{fig:plot_instructpix2pix_vs_v1_wofeedback}. We observe that HIVE without human feedback improves notably over InstructPix2Pix (blue curve vs. green curve). Moreover, human feedback further boosts the performance of HIVE (red curve vs blue curve) by a large margin. In other words, with the same directional similarity value, HIVE with human feedback obtains better image consistency than that without feedback. 

 To test the effectiveness of HIVE on real-world images, we report the user study results on the 1K evaluation dataset. As shown in Fig.~\ref{fig:userstudy_instructpix2pix_vs_hive_wo_hf}, HIVE without human feedback gets around 30\% more votes than the InstructPix2Pix. The result is consistent with the user study on the synthetic dataset. We also demonstrate the user study outcome between HIVE without and with human feedback in Fig.~\ref{fig:userstudy_hive_with_without_hf}. The user study indicates similar conclusions to the consistency plot, where the model with human feedback gets 10.8\% more favorites than the model without human feedback. 


 In Fig.~\ref{fig:result_official_v1_rw}, we present representative edits that demonstrate the effectiveness of HIVE. The results show that while HIVE can partially learn editing instructions without human feedback, the reward model leads to better alignment between instruction and the edited image. For example, in the third row, HIVE without human feedback generates a door-like object, but with the guidance of human feedback, the generated door matches human perception better. 
 
 
We conducted another user study to assess the image quality of the edited images and found that HIVE with human feedback received the highest number of votes (as shown in Fig.~\ref{fig:user_study_image_quality}). For example, in the last row of Fig.~\ref{fig:result_official_v1_rw}, the beer glass generated by InstructPix2Pix and HIVE without human feedback appears misleading or unfinished to human reviewers. We think that the reason is annotators' preferences are diverse, which leads to consistent and robust evaluation. Therefore, the image quality benefits from it. Additionally, our visual analysis of the results (Fig.~\ref{fig:result_preserve}) indicates that the HIVE model with human feedback tends to preserve the remaining part of the original image that is not instructed to be edited, while the models without human feedback lead to excessive image editing more often. For instance, in the first example of Fig.~\ref{fig:result_preserve}, HIVE with human feedback blends a pond naturally into the original image. While InstructPix2Pix and HIVE without human feedback fulfill the same instruction, but at the same time, alter the uninstructed part of the original background.




\begin{figure}[t]
\begin{center}
\includegraphics[ width=0.99\linewidth]{Figures/result_preserve.pdf}
\end{center}
    \vspace{-2em}
   \caption{Human feedback tends to help HIVE avoid unwanted excessive image modifications.}
\label{fig:result_preserve}
\vspace{-1em}
\end{figure}


 \begin{figure}[t]
\begin{center}
\includegraphics[ width=0.53\linewidth]{Figures/user_study_image_quality.pdf}
\end{center}
\vspace{-2em}
   \caption{Image quality comparisons between InstructPix2Pix, HIVE without human feedback, and HIVE with human feedback. HIVE with HF obtains the most votes from annotators.}
\label{fig:user_study_image_quality}
\vspace{-1.7em}
\end{figure}
























\vspace{-.3em}
\subsection{Ablation Study}
\label{sec:ablation}


\myparagraph{Weighted Reward and Condition Reward Loss.} We perform user study on HIVE with these two losses individually. As shown in Fig.~\ref{fig:userstudy_rewardloss}, these two losses obtain very similar human preferences on the evaluation dataset. More comparisons are in Appendix~\ref{sec:appendix_ablation}.

\begin{figure}[t]
\begin{center}
\includegraphics[ width=0.99\linewidth]{Figures/plot_instructpix2pix_v1_vs_v2.pdf}
\end{center}
    \vspace{-2.0em}
   \caption{InstructPix2Pix with SD v1.5 and v2.1. The higher the better.}
\vspace{-1em}
\label{fig:plot_instructpix2pix_v1_v2}
\end{figure}


\begin{figure}[t]
	\centering
	\subfigure[HIVE with weighted reward loss]{
		\begin{minipage}[b]{0.22\textwidth}
			\includegraphics[width=1\textwidth]{Figures/user_study_HIVE_reward_loss.pdf}
		\end{minipage}
		\label{fig:userstudy_HIVE_rewardloss}
	}
    \subfigure[HIVE with condition reward loss]{
    	\begin{minipage}[b]{0.22\textwidth}
   		\includegraphics[width=1\textwidth]{Figures/user_study_hive_w_wo_feedback.pdf}
    	\end{minipage}
	\label{fig:userstudy_HIVE_conditionloss}
    }
    \vspace{-1.4em}
	\caption{User study of pairwise comparison between (a) HIVE with weighted reward loss and (b) HIVE with condition reward loss. The human preferences are very close to each other.}
    \label{fig:userstudy_rewardloss}
\end{figure}

\myparagraph{SD v1.5 and v2.1.} Considering the recent progress of stable diffusion, we upgrade the backbone of stable diffusion from v1.5 to the latest version v2.1, where OpenCLIP text encoder \cite{schuhmann2022laion5B} replaces the CLIP text encoder \cite{radford2021learning}, and expect to see the backbone upgrading benefits. The quantitative consistency plot in Fig.~\ref{fig:plot_v1_vs_V2} on the synthetic evaluation dataset confirms our assumption and shows that SD v2.1 improves performance over SD v1.5 by a small margin. Our user study in Fig.~\ref{fig:userstudy_v1_v2} using human feedback indicates a similar conclusion.  We compare InstructPix2Pix v1.5 with v2.1 as well. An interesting observation is that we train InstructPix2Pix with SD v2.1 and show in Fig.~\ref{fig:plot_instructpix2pix_v1_v2} that its improvement over SD v1.5 is larger than HIVE. 












\myparagraph{Failure Cases.} We summarize representative failure cases in Fig. \ref{fig:result_fail}.
First, some instructions cannot be understood. In the upper left example in Fig. \ref{fig:result_fail}, the prompt ``zoom in'' or similar instructions can rarely be successful. We believe the root cause is current training data generation method fails to generate image pairs with this type of instruction. Second, counting and spatial reasoning are common failure cases (see the upper right example in Fig. \ref{fig:result_fail}). We find that the instruction ``one'', ``two'', or ``on the right'' can lead to many undesired results.
Third, the object understanding sometimes is wrong. In the bottom left example, the red color is changed on the wrong object. This is a common error.
Other ablation studies can be found in Appendix~\ref{sec:appendix_ablation}.

\begin{figure}[t]
\begin{center}
\includegraphics[ width=0.99\linewidth]{Figures/plot_v1_vs_v2.pdf}
\end{center}
 \vspace{-2em}
   \caption{Comparisons between HIVE (no human feedback) with SD v1.5 and v2.1. The higher the better.}
\label{fig:plot_v1_vs_V2}
\vspace{-1em}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[ width=0.53\linewidth]{Figures/user_study_HIVE_v1_v2.pdf}
\end{center}
 \vspace{-1.8em}
   \caption{User study between HIVE with SD v1.5 and v2.1 with human feedback. SD v2.1 obtains more votes.}
   \vspace{-1em}
\label{fig:userstudy_v1_v2}
\end{figure}


\begin{figure}[t]
\begin{center}
\includegraphics[ width=0.97\linewidth]{Figures/result_failure_cases.pdf}
\end{center}
 \vspace{-2em}
   \caption{Failure examples.}
\label{fig:result_fail}
\vspace{-1.2em}
\end{figure}






