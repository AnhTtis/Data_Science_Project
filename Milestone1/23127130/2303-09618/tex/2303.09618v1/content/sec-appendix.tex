\begin{center}
\Large
\textbf{Appendix}
\end{center}


\section{Data Collection and User Study}
\label{sec:appendix_dataset}

In the evaluation steps, we collect real-world images with instructions using Amazon Mechanical Turk (Mturk) \footnote{https://www.mturk.com}. We randomly collect 200 real-world images. Then we ask Mturk annotators to write five instructions for each image, and encourage them to have wild imaginations and diversify the instruction types. We encourage annotators to not be limited to making the image realistic. For example, annotators can write ``add a horse in the sky''. A screenshot of the interface is illustrated in Fig.~\ref{fig:mturk_write_instruction}. We analyze the top five verbs and nouns in the evaluation dataset. It is shown in Fig.~\ref{fig:verb_evaluation} that the verbs ``add'', ``change'', ``make'', ``remove'' and ``put'' make up around 85\% of all verbs, which means that the editing instruction verbs have a long-tail distribution. In contrast, the distribution of nouns in Fig.~\ref{fig:noun_evaluation} is close to uniform, where the top five nouns represent only around 20\% of all nouns.


\begin{figure}[b]
\begin{center}
\includegraphics[ width=0.99\linewidth]{Figures/mturk_write_instructions.png}
\end{center}
    \vspace{-2em}
   \caption{Mturk writing editing instructions interface: write five instructions per image.}
\label{fig:mturk_write_instruction}
\end{figure}

In user studies, we use Mturk to ask annotators to evaluate edited images. A screenshot of the interface is shown in Fig.~\ref{fig:mturk_select_image}. The annotators are provided with the original image, two edited images, and the editing instruction. They are asked to select the better edited image. The third option indicates that the edited images are equally good or equally bad. We ask three annotators to label one data sample, and use the majority votes to determine the results. We shuffle the edited images to avoid choosing the left image over the right and vice versa.

\begin{figure}[b]
\begin{center}
\includegraphics[ width=0.99\linewidth]{Figures/mturk_select_best_image.png}
\end{center}
    \vspace{-2em}
   \caption{Mturk labeling interface: select the better edited image.}
\label{fig:mturk_select_image}
\end{figure}


\begin{figure}
	\centering
	\subfigure[\footnotesize{Top five verbs}]{
		\begin{minipage}[b]{0.4\textwidth}
			\includegraphics[width=1\textwidth]{Figures/stat_verb_evaluation.pdf}
		\end{minipage}
		\label{fig:verb_evaluation}
	}
    \subfigure[\footnotesize{Top five nouns}]{
    	\begin{minipage}[b]{0.4\textwidth}
   		\includegraphics[width=1\textwidth]{Figures/stat_noun_evaluation.pdf}
    	\end{minipage}
	\label{fig:noun_evaluation}
    }
    \vspace{-1em}
	\caption{Top five verbs and nouns in the evaluation dataset.}
    \label{fig:stat_noun_verb_evaluation}
\vspace{-1em}
\end{figure}


\section{Implementation Details}
\subsection{Instructional Supervised Training}
\label{sec:appendix_instructional_supervised_training}

We use pre-trained stable diffusion models as the initial checkpoint to start instructional supervised training. We train HIVE on 40GB NVIDIA A100 GPUs for 500 epochs. We use the learning rate of $10^{-4}$ and the image size of 256. In the inference, we use 512 as the default image resolution. 

\subsection{Human Feedback Rewards Learning}
\label{sec:appendix_rm}
\begin{figure}[b]
\begin{center}
\includegraphics[ width=0.99\linewidth]{Figures/method_weighted_reward_loss.pdf}
\end{center}
    \vspace{-2em}
   \caption{Overall architecture of HIVE. Different from Fig. \ref{fig:method}, in the third step, we use weighted reward loss instead of condition reward loss to fine-tune the diffusion model.}
\label{fig:method_weighted}
\end{figure}

As shown in Fig.~\ref{fig:reward_model_architecture}, the reward model takes in an input image $c_I$, a text instruction $c_E$, and an edited image $\tilde{x}$ and outputs a scalar value. Inspired by the recent work on the vision-language model, especially BLIP 
 \cite{li2022blip}, we employ a visual transformer \cite{dosovitskiy2020vit} as our image encoder and an image-grounded text encoder as the multimodal encoder for images and text. Finally, we set a linear layer on top of the image-grounded text encoder to map the multimodal embedding to a scalar value.

(1) \textbf{Visual transformer}. We encode both the input image $c_I$ and edited image $\tilde{x}$ with the same visual transformer. Then we obtain the joint image embedding by concatenating the two image embeddings $vit(c_I)$, $vit(\tilde{x})$.

(2) \textbf{Image-grounded text encoder}. The image-grounded text encoder is a multimodal encoder that inserts one additional cross-attention layer between the self-attention layer and the feed-forward network for each transformer block of BERT \cite{devlin2018bert}. The additional cross-attention layer incorporates visual information into the text model. The output embedding of the image-grounded text encoder is used
as the multimodal representation of the ($c_I$, $c_E$, $\tilde{x}$) triplet.

We gather a dataset comprising 3,634 images for the purpose of ranking. For each image, we generate five variant edited images, and ask an annotator to rank images from best to worst. Additionally, we ask annotators to indicate if any of the following scenarios apply: (1) all edited images are edited but none of them follow the instruction; (2) all edited images are visually the same as the original image; (3) all images are edited beyond the scope of instruction; (4) edited images have harmful content containing sex, violence, porn, etc; and (5) all edited images look similar to each other. We compare training reward models by filtering some/all of these options.

We note that a considerable portion of the collected data falls under at least one of the aforementioned categories, indicating that even for humans, ranking these images is challenging. As a result, we only use the data that did not include any non-rankable options in the reward model training. From a pool of 1,412 images, we select 1,285 for the training set, while the remaining images were used for the validation set. The reward model is trained on a dataset of comparisons between multiple model outputs on the same input. Each comparison sample contains an input image, an instruction, five edited versions of the image, and the corresponding rankings. We divide the dataset into training and validation sets based on the distribution of the corresponding instructions.

We apply the method in Sec. \ref{sec:rewardslearning} on the reward data to develop a reward model. We initialize the reward model from the pre-trained BLIP, which was trained on paired images and captions using three objectives: image-text contrastive learning, image-text matching, and masked language modeling. Although there is a domain gap between BLIP's pre-training data and our reward data, where the captions in BLIP's data describe a single image, and the instructions in our data refer to the difference between image pairs. We hypothesized that leveraging the learned alignment between text and image in BLIP could enhance the reward model's ability to comprehend the relationship between the instruction and the image pairs.

The reward model is trained using 4 A100 GPUs for 10 epochs, employing a learning rate of $10^{-4}$ and weight decay of 0.05. The image encoder's and multimodal encoder's last layer outputs are utilized as image and multimodal representations, respectively. The encoders' final layer is the only  fine-tuned component.

We use the trained reward model to generate a reward score on our training data. We perform two experiments. The first experiment takes the exponential rewards as weights and fine-tunes the diffusion model with \textbf{weighted reward loss} as described in Sec. \ref{sec:hfdf}. See Fig. \ref{fig:method_weighted} for the visualization of the method. The second experiment transforms the rewards to text prompts and fine-tunes the diffusion model with the \textbf{condition reward loss} as described in Sec. \ref{sec:hfdf}. The method is introduced in Fig.~\ref{fig:method}. We compare those two experiment settings, and results can be found in Sec.~\ref{sec:appendix_rewards_exp}. 



\section{Reward Maximization for Diffusion-Based Generative Models}
\subsection{Discussion on On-Policy based Reward Maximization for Diffusion Models}
\label{sec:appendix_sampling_methods}
Directly adapting on-policy RL methods to the current training pipeline might be computationally expensive, but we do not conclude that sampling-based approaches are not doable for diffusion models. 
We consider developing more scalable sampling-based methods as future work. 

We start the sampling methods derivation with the following objective:
\begin{align}
    J(\theta):= \max_{\pi_\theta}\E_{c \sim p_c}\Big[\E_{\tgx \sim \pi_{\theta}(\cdot | c)}\left[\mathcal{R}_{\phi}(\tgx, c)\right] - \eta \mathrm{KL}[p_{\mathcal{D}}(\tgx | c) ||  \pi_{\theta}(\tgx | c)]\Big]\,, \label{eq:reverse_obj}
\end{align}
where $p_c(c)p_{\mathcal{D}}(\tgx | c)$ is the joint distribution of the condition and edited images pair, 
and $\pi_{\theta}$ denotes the policy or the diffusion model we want to optimize.
Note that $p_{\mathcal{D}}(\tgx | c)$ and  $\pi(\tgx | c)$ are swaped compared with the objective in Eq.~\eqref{equ:rl_obj}. 
The second term in Eq.~\eqref{eq:reverse_obj}, is the \textit{KL Minimization} formula for maximum likelihood estimation, equivalent to the loss of diffusion models. 
We represent the policy $\pi_{\theta}$ via the \textit{reverse process} of a conditional diffusion model:
\begin{align*}
    \pi_\theta(\tgx | c):= p_{\theta}(\tgx^{0:T} | ~c) = p_{0}(\tgx^{T})\prod_{t=1}^{T}p_{\theta}(\tgx^{t-1}| \tgx^{t}; c)\,,
\end{align*}
where $p_{0}(\tgx^{T}):=\mathcal{N}(\tgx^{T}, \bf{0}; \bf{{I}}) $, and $p_{\theta}(\tgx^{t-1}| \tgx^{t}; c):= \mathcal{N}(\tgx^{t} | \mu_{\theta}(\tgx_{t}, t), \sigma_t^2) \bf{\mathrm{I}}$ is a Gaussian distribution, whose parameters are defined by score function $\epsilon_\theta$ and stepsize of noise scalings. 
So we can get a edited image sample $\tgx^{0}$ by running a reverse diffusion chain: 
\begin{align*}
    \tgx^{t-1} | \tgx^{t} = \frac{1}{\sqrt{\alpha_t}}\left(\tgx^t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}\epsilon_{\theta}(\tgx^t,c, t)\right) + \sigma_t \pmb{z}_t, ~~\pmb{z}\sim \mathcal{N}(\textbf{0}, \textbf{I}), \text{for}~~ t=T,\ldots,1\,,
\end{align*}
and $\tgx^{T} \sim \mathcal{N}(\textbf{0}, \textbf{I})$. 

As a result, the reverse diffusion process can be viewed as a black box function defined by $\epsilon_\theta$ and noises $\pmb{\epsilon}:= (\pmb{z}_T, \ldots, \pmb{z}_1, \tgx^T)$, which we can view as a \textit{shared parameter network with noises}. And for each layer, we can view the parameter is the score function $\epsilon_\theta$. 
Define the network as 
\begin{align*}
    \tgx^{0} := f(c, \pmb{\epsilon}; \theta)\,,~~ \pmb{\epsilon} \sim p_{\mathrm{noise}}(\cdot), c\sim p_{c}(\cdot)\,,
\end{align*}
where we can rewrite the first term as 
\begin{align*}
    \E_{c \sim \mathcal{D}, \pmb{\epsilon} \sim p_{\mathrm{noise}}(\cdot)}[\mathcal{R}_{\phi}(f(c, \pmb{\epsilon}; \theta), c)]\,,
\end{align*}
and we can optimize the parameter $\theta$ with path gradient if $\mathcal{R}_{\cdot}$ is differentiable with path gradient.
Similarly, suppose we want to optimize the first term via PPO. In that case, the main technical difficulty is to estimate $\nabla_{\theta}\log\pi_{\theta}(\tgx| c)$, which can be estimated with the following derivation:
\begin{align*}
\nabla_{\theta}\log \pi_{\theta}(\tgx| c) = \nabla_{\theta}\log p_{\theta}(\tgx^{0:T} | ~c) = \sum_{t=1}^{T}\nabla_{\theta}\log p_{\theta}(\tgx^{t-1}| \tgx^{t}; c)\,.
\end{align*}
Note that for both the end-to-end path gradient method and PPO we require to sample the reverse chain from $\tgx^{T}$ to $\tgx^{0}$, thus we can estimate $\nabla_{\theta}\log \pi(\tgx | c)$ using the empirical samples $\tgx^{0:T}$.

For the above two methods, to perform one step policy gradient update, we need to run the whole reverse chain to get an edited image sample $\tgx^{0}$ to estimate the parameter gradient for the first term. 
As a result, the computational cost is the number of diffusion steps more extensive than the supervised fine-tuning 
cost. Now we need more than two days to fine-tune the stable diffusion model, so for standard LDM, where the number of steps is 1000, we can not finish the training within an acceptable training time. 
Even if we can use some fast sampling methods such as DDIM or variance preserve (VP) based noise scaling, the diffusion steps are still more than 5 or 10. 
Further, we haven't seen any previous work using such noise scaling to fine-tune stable diffusion. As a result, we think naive sampling methods might have high risk to obtain similar performance, compared with our current offline RL based approaches. 


























\subsection{Derivation for Eq.~\eqref{equ:optim_q}}
\label{sec:appendix_derivation}
Take a functional view of Eq.~\eqref{equ:optim_q}, and differentiate $J(\rho)$ w.r.t $\rho$, we get 
\begin{align*}
    \frac{\partial J(\rho)}{\partial \rho}= \mathcal{R}_{\phi}(\tgx |c) - \eta \left(\log\rho(\tgx|c)  + 1 - \log p(\tgx | c)\right)\,.
\end{align*}
Setting $\frac{\partial J(\rho)}{\partial \rho} = 0$ gives us
\begin{align*}
    \log \rho(\tgx | c) &= \frac{1}{\eta}\mathcal{R}_{\phi}(\tgx |c) + \log p(\tgx |c)  - 1\,,\\
    \rho(\tgx | c) & \propto p(\tgx | c)\exp\left(\mathcal{R}_{\phi}(\tgx , c) / \eta\right)\,.
\end{align*}
Thus we can get the optimal $\rho^{*}(\tgx | c)$.


\section{Additional Ablation Study}
\label{sec:appendix_ablation}

\begin{figure}
\begin{center}
\includegraphics[ width=0.5\linewidth]{Figures/stats_cycle_consistency.pdf}
\end{center}
    \vspace{-1.8em}
   \caption{Cycle consistency top five augmentations.}
\vspace{-1em}
\label{fig:stat_cycle_consistency}
\end{figure}



\begin{figure}
\begin{center}
\includegraphics[ width=0.5\linewidth]{Figures/plot_v1_vs_wocycle.pdf}
\end{center}
    \vspace{-1.8em}
   \caption{HIVE with and without cycle consistency.}
\vspace{-1em}
\label{fig:plot_v1_vs_wocycle}
\end{figure}

\subsection{Weighted Reward and Conditional Reward Losses}
\label{sec:appendix_rewards_exp}

We compare the weighted reward loss and conditional reward loss on the synthetic evaluation dataset. As shown in Fig.~\ref{fig:plot_reward_losses}, the performances of these two losses are close to each other, while the conditional reward loss is slightly better. Therefore we adopt the conditional reward loss in all our experiments.


\begin{figure}
\begin{center}
\includegraphics[ width=0.5\linewidth]{Figures/plot_reward_losses.pdf}
\end{center}
    \vspace{-1.8em}
   \caption{HIVE with weighted reward loss and conditional reward loss.}
\vspace{-1em}
\label{fig:plot_reward_losses}
\end{figure}

\subsection{Cycle Consistency}

We analyze the impact of cycle consistency augmentation in Sec.~\ref{sec:cycleconsistency}. The top five augmentations in the cycle consistency are demonstrated in Fig.~\ref{fig:stat_cycle_consistency}. As shown in Fig.~\ref{fig:plot_v1_vs_wocycle}, the cycle consistency augmentation improves the performance of HIVE by a notable margin.




\begin{figure}
\begin{center}
\includegraphics[ width=0.5\linewidth]{Figures/plot_v1_different_percent.pdf}
\end{center}
    \vspace{-1.8em}
   \caption{HIVE with different training data size.}
\vspace{-1em}
\label{fig:plot_v1_different_datasize}
\end{figure}

\subsection{Training with Less Data}

We analyze the effect of the training data size. We compare HIVE with SD v1.5 at four training dataset size ratios: 100\%, 50\%, 30\% and 10\%. As shown in Fig. \ref{fig:plot_v1_different_datasize}, significantly decreasing the size of the dataset, \eg 10\% data, leads to worse ability to perform large image edits. On the other hand, reasonable decreasing dataset size  can result in a similar yet slightly worse performance \eg 50\% data.


\subsection{Additional Visualized Results}

We illustrate additional visualized results in Fig.~\ref{fig:res_moreresults}.

\begin{figure}
\begin{center}
\includegraphics[ width=0.99\linewidth]{Figures/result_moreresults.pdf}
\end{center}
    \vspace{-2em}
   \caption{Additional editing results.}
\label{fig:res_moreresults}
\end{figure}

