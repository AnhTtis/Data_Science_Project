\vspace{-.5em}
\section{Methodology}
\vspace{-.3em}
In this section, we introduce the new datasets we collected in Sec.~\ref{sec:dataset}, and explain the three major steps of HIVE in the rest of the section.  
Concretely, we introduce the instructional supervised training in Sec.~\ref{sec:instructional_supervised_training},
 and describe how to train a reward model to score edited images in Sec.~\ref{sec:rewardslearning}, then present two scalable fine-tuning methods to align diffusion models with human feedback in Sec.~\ref{sec:policy_update}.

\vspace{-.3em}
\subsection{Dataset}
\vspace{-.3em}
\label{sec:dataset}

\myparagraph{Instructional Edit Training Dataset.} We follow the same method of \cite{brooks2022instructpix2pix} to generate the training dataset. We collect 1K images and their corresponding captions. We ask three annotators to write three instructions and corresponding edited captions based on the collected input captions. Therefore, we obtain 9K prompt triplets: input caption, instruction, and edited caption. We fine-tune GPT-3 \cite{Brown2020GPT3} with OpenAI API v0.25.0 \cite{openaiapi} with them. 
We use the fine-tuned GPT-3 to generate five instructions and edited captions per input image-caption pair in Laion-Aesthetics V2 \cite{schuhmann2022laion5B}. We observe that the captions from Laion are not always visually descriptive, so we use BLIP \cite{li2022blip} to generate more diverse types of image captions. Later stable diffusion based Prompt-to-Prompt \cite{hertz2022prompt} is adopted to generate paired images. In addition, we design a cycle-consistent augmentation method (Sec.~\ref{sec:cycleconsistency}) to generate additional training data. We generate 1.04M training triplets in total. 
Combining the 281K training data from \cite{brooks2022instructpix2pix}, we obtain 1.32M training image pairs along with instructions.

\myparagraph{Reward Fine-tuning Dataset.} We collect 3.6K image-instruction pairs for the task of reward fine-tuning. Among them, 1.6K image-instruction pairs are manually collected, and the rest are from Laion-Aesthetics V2 with GPT-3 generated instructions. We use this dataset to ask annotators to rank various model outputs.

\myparagraph{Evaluation Dataset.} We use two evaluation datasets: the test dataset in \cite{brooks2022instructpix2pix} for quantitative evaluation and a new 1K dataset collected for the user study. The quantitative evaluation dataset is generated following the same method as the training dataset, which means that the dataset does not contain real images. Our collected 1K dataset contains 200 real images, and each image is annotated with five human-written instructions. More details of annotation tooling, guidelines, and analysis are in Appendix~\ref{sec:appendix_dataset}.

\subsection{Instructional Supervised Training}
\label{sec:instructional_supervised_training}
We follow the instructional fine-tuning method in \cite{brooks2022instructpix2pix} with two major upgrades on dataset curation (Sec.~\ref{sec:dataset}) and cycle consistency augmentation (Sec.~\ref{sec:cycleconsistency}). A pre-trained stable diffusion model \cite{rombach2022high} is adopted as the backbone architecture. 
 In instructional supervised training,  the stable diffusion model has two conditions $c = \left[ c_I, c_E \right]$, where $c_E$ is the editing instruction, and $c_I$ is the latent space of the original input image.
 In the training process, a pre-trained auto-encoder \cite{kingma2013autoencoding} with encoder $\mathcal{E}$ and decoder $\mathcal{D}$ is used to convert between edited image $\tgx$ and its latent representation $z=\mathcal{E}(\tgx)$. 
 The diffusion process is composed of an equally weighted sequence of denoising autoencoders $\epsilon_{\theta}(z_t, t, c)$, $t = 1, \cdots , T$, which are trained to predict a denoised variant of their input $z_t$, a noisy version of $z$. The objective of instructional supervised training is:
 \vspace{-.9em}
\begin{equation*}\textstyle
\resizebox{0.8\linewidth}{!}{%
$
L = \mathbb{E}_{\mathcal{E}(\tgx), c, \epsilon \sim \mathcal{N}(0, 1), t }\Big[ \Vert \epsilon - \epsilon_\theta(z_{t}, t, c) \Vert_{2}^{2}\Big]\,.
$%
}
\vspace{-.5em}
\end{equation*}
\vspace{-.4em}
\subsubsection{Cycle Consistency Augmentation}
\vspace{-.2em}
\label{sec:cycleconsistency}

Cycle consistency is a powerful technique that has been widely applied in image-to-image generation~\cite{zhu2017unpaired,isola2017image}. It involves coupling and inverting bi-directional mappings of two variables $X$ and $Y$, $G: X \rightarrow Y$ and $F: Y \rightarrow X$, such that $F(G(X)) \approx X$ and vice versa. This approach has been shown to enhance generative mapping in both directions.

While Instructpix2pix~\cite{brooks2022instructpix2pix}  considers instructional image editing as a single-direction mapping, we propose adding cycle consistency. Our approach involves a forward-pass editing step, $F: x \stackrel{inst}{\longrightarrow} \tgx$. We then introduce instruction reversion to enable a reverse-pass mapping, $R: \tgx \stackrel{\sim inst}{\longrightarrow} x$. In this way, we could close the loop of image editing as: $x \stackrel{inst}{\longrightarrow} \tgx \stackrel{\sim inst}{\longrightarrow} x$, \eg ``add a dog'' to ``remove the dog''.

To ensure the effectiveness of this technique, we need to separate invertible and non-invertible instructions from the dataset.
We devised a rule-based method that combines speech tagging and template matching. We found that most instructions adhere to a particular structure, with the verb appearing at the start, followed by objects and prepositions. Thus, we grammatically tagged all instructions using the Natural Language Toolkit (NLTK)~\footnote{https://www.nltk.org/}. We identified all invertible verbs and pairing verbs, and also analyzed the semantics of the objects and the prepositions used. By summarizing invertible instructions in predefined templates, we matched desired instructions. Our analysis revealed that 29.1\% of the instructions in the dataset were invertible. We augmented this data to create more comprehensive training data, which facilitated cycle consistency. For more information, see Appendix~\ref{sec:appendix_instructional_supervised_training}.

\subsection{Human Feedback Reward Learning}
\label{sec:rewardslearning}

The second step of HIVE is to learn a reward function $\mathcal{R}_{\phi}(\tilde{\gx}, \tc)$, which takes the original input image, the text instruction condition $c = \left[\ci, \ce \right]$, and the edited image $\tilde{\gx}$ that is generated by the fine-tuned stable diffusion as input, and outputs a scalar that reflects human preference.

Unlike InstructGPT which only takes text as input, our reward model $\mathcal{R}_{\phi}(\tilde{\gx}, \tc)$
needs to measure the alignment between instructions and the edited images.
To address the challenge, we present a reward model architecture in Fig.~\ref{fig:reward_model_architecture},
which leverages pre-trained vision-language models such as BLIP \cite{li2022blip}.
More specifically, the reward model employs an image-grounded text encoder as the multi-modal encoder to take the joint image embedding and the text instruction as input and produce a multi-modal embedding. A linear layer is then applied to the multi-modal embedding to map it to a scalar value. More details are in Appendix~\ref{sec:appendix_rm}.


With the specifically designed network architecture, 
we train the reward function $\mathcal{R}_{\phi}(\tilde{\gx}, \tc)$ with our collected reward fine-tuning dataset $\mathcal{D}_\mathrm{human}$ induced in Sec.~\ref{sec:dataset}. 
For each input image $\ci$ and instruction $\ce$ pair,
we have $K$ edited images $\{\tilde{\gx}\}_{k=1}^{K}$ ranked by human annotators,
and denote the human preference of edited image $\tgx_i$ over $\tgx_j$  by  $\tgx_i \succ \tgx_j $. 
Then we can follow the Bradley-Terry model
of preferences \cite{bradley1952rank,Ouyang2022instructgpt} to define the pairwise loss function:
\vspace{-.5em}
\begin{equation*}\textstyle
\resizebox{0.9\linewidth}{!}{%
$
 \ell_{\mathrm{RM}}(\phi) := - \sum_{\tgx_i \succ \tgx_j}\log \left[\frac{\exp(\mathcal{R}_{\phi}(\tgx_i, c))}{\sum_{k=i, j}\exp(\mathcal{R}_{\phi}(\tgx_k, c))}\right]\,,
$%
}
\end{equation*}
where $(i, j)\in [1\ldots K]$ and we can get $K \choose 2$ pairs of comparison for each condition $c$.
Similar to \cite{Ouyang2022instructgpt}, we put all the $K \choose 2$ pairs for each condition $c$ in a single batch to learn the reward functions. 
We provide a detailed reward model training discussion in Appendix~\ref{sec:appendix_rm}.

\begin{figure}
\begin{center}
\includegraphics[ width=0.99\linewidth]{Figures/reward_model_architecture.pdf}
\end{center}
\vspace{-1.7em}
   \caption{Model architecture for reward $R(\tgx, c)$. Here the reward model evaluates human preference for an edited image of a hand selecting an orange compared to the original input  image of the hand selecting an apple. The input to the reward model includes both images and a text instruction. The output is a score indicating the degree of preference for the edited image based on the input image and instruction.}
\label{fig:reward_model_architecture}
\end{figure}

\subsection{Human Feedback based Model Fine-tuning}\label{sec:hfdf}
With the learned reward function $\mathcal{R}_{\phi}(c, \tgx)$, 
the next step is to improve the instructional supervised training model
by reward maximization. As a result, we can obtain an instructional diffusion model that aligns with human preferences. 

To address the difficulty of sampling-based methods as we discussed previously (also in Appendix~\ref{sec:appendix_sampling_methods}), we adapt offline RL techniques \cite{levine2020offline,peng2019advantage,chen2021decision,janner2021sequence} to fine-tune diffusion models, which allows us to align diffusion models with human feedback under 
acceptable training time and cost.

The RL fine-tuning techniques we present can be utilized for latent and pixel-based diffusion models. 
For simplicity, we introduce our methods for pixel-based generative diffusion models.
With an input image and editing instruction condition $c=[\ci, \ce]$, 
we define the edited image data distribution generated by the instructional supervised diffusion model as $p(\tgx|~c)$,
and the edited image data distribution generated by the current diffusion model we want to optimize as $\rho(\tgx | ~c)$\,,
then under the pessimistic principle of offline RL, we can optimize $\rho$ by the following objectives:
\vspace{-.5em}
\begin{align}\textstyle
    J(\rho) := \max_{\rho} \E_{c}\big[&\E_{\tgx \sim \rho(\cdot | c)}[\mathcal{R}_{\phi}(\tgx, c)] -\notag \\
        & ~~~~~~~~~~~~\eta \mathrm{KL}(\rho(\tgx | c) ||p(\tgx | c) ) \big]\,, \label{equ:rl_obj}
\end{align}
where $\eta$ is a hyper-parameter.
The first term in Eq.~\eqref{equ:rl_obj} is the standard reward maximization in RL, and the second term is a regularization to stabilize learning, which is a widely used 
technique in offline RL \cite{kumar2020conservative}, and also adopted for PPO fine-tuning of InstructGPT (\textit{a.k.a} ``PPO-ptx'') \cite{Ouyang2022instructgpt}.

To avoid using sampling-based methods to optimize $\rho$, we can
differentiate $J(\rho)$ \textit{w.r.t} $\rho(\tgx | c )$ and solve for the optimal $\rho^{*}(\tgx | c)$, resulting the following expression for the optimal solution of Eq.~\eqref{equ:rl_obj}:
\vspace{-.5em}
\begin{align}
    \rho^{*}(\tgx | c) \propto p(\tgx |c)\exp\left(\mathcal{R}_{\phi}(\tgx ,c) / \eta\right)\,, \label{equ:optim_q}
\end{align}
or $\rho^{*}(\tgx | c) = \frac{1}{Z(c)}p(\tgx |c)\exp\left(\mathcal{R}_{\phi}(\tgx ,c) / \eta\right)$, with $Z(c) = \int p(\tgx |c)\exp\left(\mathcal{R}_{\phi}(\tgx ,c) / \eta\right) d\tgx$ being the partition function.  A detailed derivation is in Appendix~\ref{sec:appendix_derivation}.


\myparagraph{Weighted Reward Loss.} The optimal target distribution $\rho^{*}(\tgx |c)$ in Eq.~\eqref{equ:optim_q}  can be viewed as an exponential reward-weighted distribution for $p(\tgx | c)$. 
Moreover, we have already obtained the empirical edited image data drawn from $p(\tgx |c)$ when constructing the instructional editing dataset, 
and we can view the exponential reward weighted edited image $\tgx$ from the instructional editing dataset as an empirical approximation of samples drawn from $\rho^{*}(\tgx | c)$. Formally, we can fine-tune a diffusion model thus it generates data from $\rho^{*}(\tgx | c)$, resulting in the weighted reward loss:
\begin{equation*}\textstyle
\resizebox{0.95\linewidth}{!}{%
$
  \ell_\mathrm{WR}(\theta) := \E_{\mathcal{E}(\tgx), c, \epsilon \sim \mathcal{N}(0, 1), t }\left[\omega(\tgx, c)\cdot \left\Vert \epsilon - \epsilon_\theta(z_{t}, t, c) \right\Vert_{2}^{2}\right]\,,
$%
}
\end{equation*}
with $\omega(\tgx, c) = \exp\left(\mathcal{R}_{\phi}(\tgx, c) / \eta\right) $ being the \textit{exponential reward weight} for edited image $\tgx$ and condition $c$. 
Different from RL literature \cite{peters2010relative,peng2019advantage} using exponential reward or advantage weights to learn a policy function,
our weighted reward loss is derived for fine-tuning stable diffusion.

\myparagraph{Condition Reward Loss.} We can also leverage the control-as-inference perspective of RL \cite{levine2018reinforcement} 
to transform Eq.~\eqref{equ:optim_q} to a conditional reward expression, thus we can directly view the reward as a conditional label to fine-tune diffusion models. 
Similar to \cite{levine2018reinforcement}, we introduce a new binary variable $R^{*}$ indicating whether human prefers the edited image or not, where $R^{*} = 1$ denotes that human prefers the edited image, and $R^{*} = 0$ denotes that human does not prefer, thus we have $p(R^{*} = 1 ~| ~\tgx, c) \propto \exp\left(\mathcal{R}_{\phi}(\tgx ,c)\right)$.
Together with Eq.~\eqref{equ:optim_q}, and applying Bayes rules gives us the following derivation: 
\vspace{-1em}
\begin{align*}\textstyle
    p(\tgx |c)&\exp\left(\mathcal{R}_{\phi}(\tgx ,c) / \eta\right) := q(\tgx | c) \left(p(R^{*} = 1 ~| ~\tgx, c)\right)^{1 / \eta}  \\
    & = p(\tgx | c) \left(\frac{p(\tgx|~R^{*} = 1, c)p(R^{*} = 1|~c)}{p(\tgx | c)}\right)^{1 / \eta} \\
    & \propto  p(\tgx | c)^{1 - 1  / \eta}p(\tgx|~R^{*} = 1, c)^{1 / \eta}\,,
\end{align*}
where we drop $p(R^{*} = 1|~c)$ since it is a constant w.r.t $\tgx$. 
We can now view the reward for each edited image as an additional condition. 
Define the new condition $\tilde{c}=[\ci, \ce, c_{R}]$, with $c_{R}$ as the reward label, we can fine-tune the diffusion model with the condition reward loss:
\vspace{-.5em}
\begin{align*}\textstyle
    \ell_\mathrm{CR}(\theta) = \mathbb{E}_{\mathcal{E}(x), \tilde{c}, \epsilon \sim \mathcal{N}(0, 1), t }\Big[ \Vert \epsilon - \epsilon_\theta(z_{t}, t, \tilde{c}) \Vert_{2}^{2}\Big]\,.
\end{align*}



\begin{figure*}[t]
\begin{center}
\includegraphics[ width=0.99\linewidth]{Figures/result_official_vs_v1_vs_rw.pdf}
\end{center}
\vspace{-2.5em}
   \caption{Comparisons between InstructPix2Pix, HIVE without human feedback and HIVE with human feedback. HIVE with human feedback can boost performance by understanding the instruction correctly.}
\label{fig:result_official_v1_rw}
\vspace{-1em}
\end{figure*}

 We quantize the reward into five categories, based on the quantile of the empirical reward distribution of the training dataset, and convert the reward value into a text prompt. For instance, if the reward value of a training pair lies in the bottom 20\% of the reward distribution of the dataset, then we convert the reward value as a text prompt condition $c_{R}:=$\textit{``The image quality is one out of five''}. 
 And during the inference time to generate edited images, we fix the text prompt as $c_{R}:=$\textit{``The image quality is five out of five''}, indicating we want the generated edited images with the highest reward. 
 We empirically find this technique improves the stability of fine-tuning.
 



\label{sec:policy_update}




