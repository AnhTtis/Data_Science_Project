\section{Conclusion and Discussion}
In our paper, we introduce a novel framework called HIVE that enables instructional image editing with human feedback. Our framework integrates human feedback, which is quantified as reward values, into the diffusion model fine-tuning process. We design two variants of the approach and both of them improve performance over previous state-of-the-art instructional image editing methods.
Our work demonstrates instructional image editing with human feedback is a variable approach to align image generation with human preference, thus unlocking new opportunities and potential to scale up the model capabilities towards more powerful applications such as conversational image editing. 
While our method demonstrates impressive performance, we have also identified failure scenarios, as discussed in Sec.~\ref{sec:ablation}. In addition, it is possible that our trained model inherits bias and suffers from harmful content from pre-trained foundation models such as Stable Diffusion, GPT3 and BLIP. These limitations should be taken into consideration when interpreting our results, and we expect red teaming with human feedback to mitigate some of the risks in future work.