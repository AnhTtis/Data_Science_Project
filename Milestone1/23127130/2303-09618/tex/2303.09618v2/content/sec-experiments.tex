

\vspace{-.2em}
\section{Experiments}\label{sec:exp}
\vspace{-.3em}



This section presents the experimental results and ablation studies of HIVE's technical choices, demonstrating the effectiveness of our method. We adopt the default guidance scale parameters in InstrcutPix2Pix for a fair comparison. Through our experiments, we discovered that the conditional reward loss performs slightly better than the weighted reward loss, and therefore, we present our results based on the conditional reward loss.  The detailed comparisons can be found in Sec.~\ref{sec:ablation} and Appendix~\ref{sec:appendix_rewards_exp}.


\begin{figure}[t]
\begin{center}
\includegraphics[ width=1\linewidth]{Figures/plot_instructpix2pix_v1_wo_w_rw.pdf}
\end{center}
\vspace{-1.8em}
   \caption{Comparisons between IP2P-Official, IP2P-Ours, and HIVE. It plots tradeoffs between consistency with the input image and consistency with the edit. The higher the better. For all methods, we adopt the same parameters as that in \cite{brooks2022instructpix2pix}.}
\label{fig:plot_instructpix2pix_vs_v1_wofeedback}
\vspace{-1em}
\end{figure}

We evaluate our method using two datasets: a synthetic evaluation dataset with 15,652 image pairs from \cite{brooks2022instructpix2pix} and a self-collected 1K evaluation dataset with real image-instruction pairs. For the synthetic dataset, we follow InstructPix2Pix's quantitative evaluation metric and plot the trade-offs between CLIP image similarity and directional CLIP similarity~\cite{Gal2022stylegannana}. For the 1K dataset, we conduct a user study where for each instruction, the images generated by competing methods are reviewed and voted by three human annotators, and the winner is determined by majority votes.




\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
         \hspace{-0.5em}\includegraphics[width=.23\textwidth]{Figures/user_study_instructpix2pix_vs_hive_wo_feedback.pdf}  &  
         \hspace{-1em}\includegraphics[width=.23\textwidth]{Figures/user_study_hive_w_wo_feedback.pdf}  \\
         \small{IP2P-Official vs IP2P-Ours} & \small{IP2P-Ours vs HIVE} \\
       
    \end{tabular}
    
    \vspace{-1em}
    \caption{User study of comparison between (a) IP2P-Official vs IP2P-Ours and (b) IP2P-Ours and HIVE. IP2P-Ours obtains 30\% more votes than IP2P-Official. HIVE obtains 25\% more votes than that IP2P-Ours.}
    \label{fig:userstudy_instructpix2pix_vs_hive}
    \vspace{-4mm}
\end{figure}







\subsection{Baseline Comparisons}
\label{sec:exp_baseline}

We perform experiments with the same setup as InstructPix2Pix, where stable diffusion (SD) v1.5 is adopted. We compare three models: InstructPix2Pix official model (IP2P-Official), InstructPix2Pix using our data (IP2P-Ours) \footnote{It is the same to HIVE without human feedback.}, and HIVE. We report the quantitative results on the synthetic evaluation dataset in Fig.~\ref{fig:plot_instructpix2pix_vs_v1_wofeedback}. We observe that IP2P-Ours improves notably over IP2P-Official (blue curve vs. green curve). Moreover, human feedback further boosts the performance of HIVE (red curve vs blue curve) over IP2P-Ours by a large margin. In other words, with the same directional similarity value, HIVE obtains better image consistency than InstructPix2Pix. 

 To test the effectiveness of HIVE on real-world images, we report the user study results on the 1K evaluation dataset. We use ``Tie'' to represent that users think results are equally good or equally bad. As shown in Fig.~\ref{fig:userstudy_instructpix2pix_vs_hive}(a), IP2P-Ours gets around 30\% more votes than the IP2P-Official. The result is consistent with the user study on the synthetic dataset. We also demonstrate the user study outcome between HIVE and IP2P-Ours in Fig.~\ref{fig:userstudy_instructpix2pix_vs_hive}(b). The user study indicates similar conclusions to the consistency plot, where HIVE gets around 25\% more favorites than IP2P-Ours. 


 In Fig.~\ref{fig:result_official_v1_rw}, we present representative edits that demonstrate the effectiveness of HIVE. The results show that while using more data can partially improve editing instructions without human feedback, the reward model leads to better alignment between instruction and the edited image. For example, in the second row, IP2P-Ours generates a door-like object, but with the guidance of human feedback, the generated door matches human perception better. In the fourth row, the example of which is from the failure examples in \cite{brooks2022instructpix2pix}, HIVE can locate the tie and change its color correctly.
 
 

Additionally, our visual analysis of the results (Fig.~\ref{fig:result_preserve}) indicates that the HIVE model tends to preserve the remaining part of the original image that is not instructed to be edited, while IP2P-Ours leads to excessive image editing more often. For instance, in the first example of Fig.~\ref{fig:result_preserve}, HIVE blends a pond naturally into the original image. The two InstructPix2Pix models fulfill the same instruction, however, at the same time, alter the uninstructed part of the original background.




\begin{figure}[t]
\begin{center}
\includegraphics[ width=0.99\linewidth]{Figures/result_preserve.pdf}
\end{center}
    \vspace{-2.5em}
   \caption{Human feedback tends to help HIVE avoid unwanted excessive image modifications.}
\label{fig:result_preserve}
\vspace{-1em}
\end{figure}














\vspace{-.3em}
\subsection{Ablation Study}
\label{sec:ablation}


\myparagraph{Weighted Reward and Condition Reward Loss.} We perform user study on HIVE with these two losses individually. As shown in Fig.~\ref{fig:userstudy_rewardloss}, these two losses obtain similar human preferences on the evaluation dataset. More comparisons are in Appendix~\ref{sec:appendix_ablation}.




\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
         \hspace{-.5em}\includegraphics[width=.23\textwidth]{Figures/user_study_HIVE_reward_loss.pdf}&  
         \hspace{-1em}\includegraphics[width=.23\textwidth]{Figures/user_study_hive_w_wo_feedback.pdf}   \\
         \footnotesize{HIVE with weighted reward loss} & \hspace{-2em}  \footnotesize{HIVE with condition reward loss} \\
       
    \end{tabular}
    
    \vspace{-1em}
    \caption{User study of pairwise comparison between (a) HIVE with weighted reward loss and (b) HIVE with condition reward loss. The human preferences are close to each other.}
    \label{fig:userstudy_rewardloss}
    \vspace{-1.25em}
\end{figure}


\myparagraph{Cycle Consistency} We analyze the impact of it which is introduced in Sec.~\ref{sec:cycleconsistency}. The top five augmentations in the cycle consistency are demonstrated in Fig.~\ref{fig:cycle_consistency}(a). We perform evaluation on both synthetic dataset and the 1K evaluation dataset. The user study in Fig.~\ref{fig:cycle_consistency}(b) shows that the cycle consistency augmentation improves the performance of HIVE by a notable margin. 


\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
         \hspace{-.5em}\includegraphics[width=.23\textwidth]{Figures/stats_cycle_consistency.pdf}&  
         \hspace{-1em}\includegraphics[width=.23\textwidth]{Figures/user_study_hive_cycle.pdf}   \\
         \footnotesize{Top five augmentations.} & \hspace{-2em}  \footnotesize{User study of cycle consistency.} \\
       
    \end{tabular}
    
    \vspace{-1em}
    \caption{Cycle consistency analysis.}
    \label{fig:cycle_consistency}
    \vspace{-1.25em}
\end{figure}






\myparagraph{Success Rate on Verbs} It is observed that five verbs take around 85\% of all verbs, where details can be found in Sec.~\ref{sec:appendix_dataset}. We compare HIVE with IP2P-Ours on these five verbs, and report the success rate of these two methods on these verbs. It is seen in Fig.~\ref{fig:stat_verb_success} that HIVE improves the most on ``add'' from 23.5\% to 28.7\%. 



\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
         \hspace{-0.5em}\includegraphics[width=.23\textwidth]{Figures/stat_ip2p_verb_success.pdf}&  
         \hspace{-1em}\includegraphics[width=.23\textwidth]{Figures/stat_hive_verb_success.pdf}   \\
         \small{IP2P-Ours} & \small{HIVE} \\
       
    \end{tabular}
    
    \caption{Success rate of IP2P-Ours and HIVE on top five verbs.}
    \label{fig:stat_verb_success}
\end{figure}




\myparagraph{Other Baselines.} To test the effectiveness of HIVE, we experiment two additional baselines. In Fig. \ref{fig:userstudy_rewardscore_SDversions}(a), we upgrade the backbone of stable diffusion from v1.5 to v2.1. We observe that the upgraded backbone slightly improves the results. 
In Fig. \ref{fig:userstudy_rewardscore_SDversions}(b), we directly use the reward scalar instead of the reward prompt as the condition for training, and the condition on the highest reward scalar for generating the image.
We adopt the user study to compare it (named HIVE-reward) with HIVE.  HIVE obtains 25.8 \% more votes than the baseline model conditioned on the reward score. This is mainly because directly conditioning on the highest reward might cause overfiting.




\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
         \hspace{-.5em}\includegraphics[width=.23\textwidth]{Figures/user_study_HIVE_v1_v2.pdf}&  
         \hspace{-1em}\includegraphics[width=.23\textwidth]{Figures/user_study_HIVE_condition_reward.pdf}   \\
         \footnotesize{HIVE SD v1.5 and v2.1} & \hspace{-2em}  \footnotesize{HIVE-Reward vs. HIVE} \\
       
    \end{tabular}
    
    \vspace{-1em}
    \caption{User study of pairwise comparison between (a) HIVE with SD v1.5 and v2.1 and (b) HIVE conditioning on reward score and HIVE. The human preferences are very close to each other.}
    \label{fig:userstudy_rewardscore_SDversions}
    \vspace{-1.25em}
\end{figure}








\myparagraph{Failure Cases and Limitations.} We summarize representative failure cases in Fig. \ref{fig:result_fail}.
First, some instructions cannot be understood. In the upper left example in Fig. \ref{fig:result_fail}, the prompt ``zoom in'' or similar instructions can rarely be successful. We believe the root cause is current training data generation method fails to generate image pairs with this type of instruction. Second, counting and spatial reasoning are common failure cases (see the upper right example in Fig. \ref{fig:result_fail}). We find that the instruction ``one'', ``two'', or ``on the right'' can lead to many undesired results.
Third, the object understanding sometimes is wrong. In the bottom left example, the red color is changed on the wrong object. This is a common error in HIVE, where instructed edited objects are wrongly recognized. 

We find some other limitations as well. One limitation of HIVE is that it cannot bring benefits to the cases where all outputs by the model without human feedback obtain the same wrong results. In such cases, user preferences cannot always be beneficial to the results. We believe that improving the data as well as the base model is an important step in the future. Another limitation is that compared to Prompt-to-Prompt \cite{hertz2022prompt}, which is used to generate our training data, HIVE sometimes leads to some unstructured change in the image. We think that it is because of the limitation of the current training data. Instructed editing can have more diverse and ambiguous scenarios than traditional image editing problems. Using GPT-3 to finetune prompts to generate the training data is limited by the model and the labeled data.
More ablation studies are in Appendix~\ref{sec:appendix_ablation}.





\begin{figure}[t]
\begin{center}
\includegraphics[ width=0.97\linewidth]{Figures/result_failure_cases.pdf}
\end{center}
 \vspace{-2em}
   \caption{Failure examples.}
\label{fig:result_fail}
\vspace{-1.2em}
\end{figure}




