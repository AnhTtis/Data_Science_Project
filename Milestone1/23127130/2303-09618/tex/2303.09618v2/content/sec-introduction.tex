\section{Introduction}



\begin{figure*}
    \includegraphics[width=\linewidth]{Figures/method.pdf}
    \vspace{-2.5em}
    \caption{Overall architecture of HIVE. The first step is to train a baseline HIVE without human feedback. In the second step, we collect human feedback to rank variant outputs for each image-instruction pair, and train a reward model to learn the rewards. In the third step, we fine-tune diffusion models by integrating the estimated rewards.}
    \label{fig:method}
    \vspace*{-3.9mm}
\end{figure*}


State-of-the-art (SOTA) text-to-image generative models have shown impressive performance in terms of both image quality and alignment between 
output images and captions
\cite{alayrac2022flamingo,rombach2022high,ramesh2022hierarchical}. Thanks to the impressive generation abilities of these models, \textit{instructional image editing} has emerged as one of the most promising application scenarios for content generation \cite{brooks2022instructpix2pix}. 
Different from traditional image editing \cite{avrahami2022blended,hertz2022prompt,wallace2022edict,liew2022magicmix,hertz2022prompt,wallace2022edict},
where both the input and the edited caption are needed, \textit{instructional image editing} only requires human-readable instructions. 
For instance, classic image editing approaches require an input caption ``a dog is playing a ball'', and an edited caption ``a cat is playing a ball''. In contrast, instructional image editing only needs editing instruction such as ``change the dog to a cat''. 
This experience mimics \textit{how humans naturally perform image editing}.


Instructional image editing was first proposed in InstructPix2Pix \cite{brooks2022instructpix2pix}, which fine-tunes a pre-trained stable diffusion \cite{rombach2022high} by curating a triplet of the original image, instruction, and edited image, 
with the help of GPT-3 \cite{Brown2020GPT3} and Prompt-to-Prompt image editing \cite{hertz2022prompt}. 
Though achieving promising results,
the training data generation process of InstructPix2Pix lacks explicit alignment between editing instructions and edited images.

Consequently, the modified images may only align to a certain extent with the editing instructions, as shown in the second column of Fig. \ref{fig:result_official_v1_rw}. Furthermore, since these editing instructions are provided by human users, it's crucial that the final edited images accurately reflect the users' true intentions and preferences. Typically, humans prefer to make selective changes to the original images, which are usually not factored into the training data or objectives of InstructPix2Pix \cite{brooks2022instructpix2pix}. Considering this observation and the recent successes of ChatGPT \cite{chatGPT}, we propose to refine the stable diffusion process with human feedback. This adjustment aims to ensure that the edited images more closely correspond to editing instructions provided by humans.



For large language models (LLMs) such as InstructGPT \cite{chatGPT,Ouyang2022instructgpt}, 
we often first learn a reward function to reflect what humans care about or prefer on the generated text output, and then leverage reinforcement learning (RL) algorithms such as proximal policy optimization (PPO) \cite{schulman2017ppo} to fine-tune the models.
This process is often referred to as \textit{reinforcement learning with human feedback} (RLHF).
Leveraging RLHF to fine-tune diffusion-based generative models, however,  remains challenging.
Applying on-policy algorithms (\eg,PPO) to maximize rewards during the fine-tuning process can be prohibitively expensive due to the hundreds or thousands of denoising steps required for each sampled image. 
Moreover, even with fast sampling methods \cite{song2020denoising,xiao2021tackling,karras2206elucidating,lu2022dpm}, 
it is still challenging to back-propagate the gradient signal to the parameters of the U-Net. \footnote{ We present a rigorous discussion on the difficulty in Appendix~\ref{sec:appendix_sampling_methods}.}



To address the technical issues described above, 
we propose Harnessing \textbf{H}uman Feedback for \textbf{I}nstructional \textbf{V}isual \textbf{E}diting (\textbf{HIVE}), which allows us to fine-tune diffusion-based generative models with human feedback.
As shown in Fig. \ref{fig:method}, HIVE consists of three steps:



\textbf{1)}~ We perform instructional supervised fine-tuning on the dataset that combines our newly collected 1.1M training data and the data from InstructPix2Pix. Since observing failure cases and suspecting the grounding visual components from image to instruction is still a challenging problem, we collect 1.1M training data.


\textbf{2)}~ 
For each input image and editing instruction pair, we ask human annotators to rank variant outputs of the fine-tuned model from step 1, which gives us a reward learning dataset. 
Using the collected dataset, we then train a reward model (RM) that reflects human preferences. 

\textbf{3)}~ We estimate the reward for each training data used in step 1, and integrate the reward to perform human feedback diffusion model finetuning using our proposed objectives presented in Sec.~\ref{sec:hfdf}.


Our main contributions are summarized as follows:

\noindent $\bullet$~ To tackle the technical challenge of fine-tuning diffusion models using human feedback, we introduce two scalable fine-tuning approaches in Sec.~\ref{sec:hfdf}, which are computationally efficient and offer similar costs compared with supervised fine-tuning. Moreover, we empirically show that human feedback is an essential component to boost the performance of instructional image editing models. 

\noindent $\bullet$~ To explore the fundamental ability of instructional editing, we create a new dataset for HIVE including three sub-datasets: a new 1.1M \textbf{training dataset},  a 3.6K \textbf{reward dataset} for rewards learning, and a 1K \textbf{evaluation dataset}. 

\noindent  $\bullet$~ To increase the diversity of the data for training, we introduce cycle consistency augmentation based on the inversion of editing instruction. Our dataset has been enriched with one pair of data for bi-directional editing. 









 

