 \section{Related Work}

\myparagraph{Text-To-Image Generation.} Text-to-image generative models have achieved tremendous success in the past decade. Generative adversarial nets (GANs) \cite{Goodfellow2014GAN} is one of the fundamental methods that dominated the early-stage works \cite{reed2016generative,han2017stackgan,xu2018Attngan}.
Recently, diffusion models \cite{sohl15deep,ho2020denoising,song2019generative,song2020denoising} have achieved state-of-the-art text-to-image generation performance. \cite{dhariwal2021diffusion,nichol2021glide,ramesh2021zero,ramesh2022hierarchical,saharia2022photorealistic,yu2022scaling,rombach2022high,li2023gligen}.  
As a result, instead of training a text-to-image model from scratch,  our work focuses on \textit{fine-tuning} existing stable diffusion model \cite{rombach2022high}, by leveraging additional human feedback. 


\myparagraph{Image Editing.}
Similarly, 
diffusion models based image editing methods, \eg SDEdit \cite{meng2021sdedit}, BlendedDiffusion \cite{avrahami2022blended}, BlendedLatentDiffusion \cite{avrahami2022blendedlatent}, DiffusionClip \cite{Kim_2022_diffusionclip}, EDICT \cite{wallace2022edict} or MagicMix \cite{liew2022magicmix}, have garnered significant attention in recent years.
To leverage a pre-trained image-text representation (\eg, CLIP \cite{radford2021learning}, BLIP \cite{li2022blip}) and text-to-image diffusion based pre-trained models \cite{ramesh2022hierarchical,saharia2022photorealistic,rombach2022high},
most existing works focus on text-based localized editing \cite{bar2022text2live,meng2022sdedit,hertz2022prompt}.
Prompt-to-Prompt \cite{hertz2022prompt} edits the cross-attention layer in Imagen and stable diffusion to control the similarity of image and text prompt.
ControlNet~\cite{zhang2023adding} and UniControl~\cite{qin2023unicontrol} adopt controllable conditions to control image editings.
Recently, InstructPix2Pix \cite{brooks2022instructpix2pix} tackle the problem via a different approach, requiring only human-readable editing instruction to perform image editing. 
Our work follows the same direction as InstructPix2Pix\cite{brooks2022instructpix2pix} and leverages human feedback to address the misalignment between editing instructions and resulting edited images.





\myparagraph{Learning with Human Feedback.}
Incorporating human feedback into the learning process can be a highly effective way to enhance performance across various tasks
 such as fine-tuning LLMs \cite{chatGPT,Bai2022training,Scheurer2022training,Ouyang2022instructgpt,stiennon2020learning}, robotic simulation \cite{christiano2017deepreinforcement,Ibarz2018reward}, computer vision \cite{pinto2023tuning}, and to name a few.
Many existing works leverage PPO \cite{schulman2017ppo} to align to human feedback, however on-policy RL algorithms are not suitable for diffusion-based model fine-tuning (See more discussion in Appendix \ref{sec:appendix_sampling_methods}).

Simultaneously, several concurrent works \cite{wu2023better,xu2023imagereward,Lee2023aligning} study the text-to-image generation problem using human feedback.
 ReFL \cite{xu2023imagereward} investigates how to back-propagate the reward signal to random latter denoising step in the diffusion process, while \cite{wu2023better} explores how to design fine-grained
 human preference score to improve the generation quality. 
\cite{Lee2023aligning} leverages human feedback to align text-to-image generation, where they naively view reward as weights to perform maximum likelihood training. 
Different from the above works, our work tackles the problem of \textit{instructional image editing}, where there are \textit{little or even no ground truth data} for the alignment between \textit{human-readable} editing instructions and {edited images}. 
In addition, the conditions on both image input and instructions make the human feedback more valuable than standard text-to-image tasks, since the conditions make the training harder than standard text-to-image tasks.


 
