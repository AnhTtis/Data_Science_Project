@INPROCEEDINGS{9541509,  author={Hamadouche, Anis and Wu, Yun and Wallace, Andrew M. and Mota, João F. C.},  booktitle={2021 Sensor Signal Processing for Defence Conference (SSPD)},   title={Approximate Proximal-Gradient Methods},   year={2021},  volume={},  number={},  pages={1-6},  doi={10.1109/SSPD51364.2021.9541509}}

@ARTICLE{9547812,  author={Dhingra, Neil K. and Khong, Sei Zhen and Jovanović, Mihailo R.},  journal={IEEE Transactions on Automatic Control},   title={A Second Order Primal-Dual Method for Nonsmooth Convex Composite Optimization},   year={2022},  volume={67},  number={8},  pages={4061-4076},  doi={10.1109/TAC.2021.3115449}}


@inproceedings{Martinet1970RegularisationDV,
  title={R'egularisation d''in'equations variationnelles par approximations successives},
booktitle={Recherche Operationnelle},
  author={Bernard Martinet},
volume={4},  number={3},  pages={154-158},   year={1970},
}

@article{article3,
author = {Taylor, Adrien and Hendrickx, Julien and Glineur, François},
year = {2018},
month = {08},
pages = {},
title = {Exact Worst-Case Convergence Rates of the Proximal Gradient Method for Composite Convex Minimization},
volume = {178},
journal = {Journal of Optimization Theory and Applications},
doi = {10.1007/s10957-018-1298-1}
}
@article{Yang2014AnEP,
  title={An efficient primal dual prox method for non-smooth optimization},
  author={Tianbao Yang and Rong Jin and Mehrdad Mahdavi and Shenghuo Zhu},
  journal={Machine Learning},
  year={2014},
  volume={98},
  pages={369-406}
}

@article{doi:10.1137/080716542,
author = {Beck, Amir and Teboulle, Marc},
title = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems},
journal = {SIAM Journal on Imaging Sciences},
volume = {2},
number = {1},
pages = {183-202},
year = {2009},
doi = {10.1137/080716542}
}

@article{Parikh2014ProximalA,
  title={Proximal Algorithms},
  author={Neal Parikh and Stephen P. Boyd},
  journal={Found. Trends Optim.},
  year={2014},
  volume={1},
  pages={127-239}
}

@article{YANG2022110551,
title = {Proximal ADMM for nonconvex and nonsmooth optimization},
journal = {Automatica},
volume = {146},
pages = {110551},
year = {2022},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2022.110551},
author = {Yu Yang and Qing-Shan Jia and Zhanbo Xu and Xiaohong Guan and Costas J. Spanos},
keywords = {Distributed nonconvex and nonsmooth optimization, Proximal ADMM, Bounded Lagrangian multipliers, Global convergence, Smart buildings},
abstract = {By enabling the nodes or agents to solve small-sized subproblems to achieve coordination, distributed algorithms are favored by many networked systems for efficient and scalable computation. While for convex problems, substantial distributed algorithms are available, the results for the more broad nonconvex counterparts are extremely lacking. This paper develops a distributed algorithm for a class of nonconvex and nonsmooth problems featured by (i) a nonconvex objective formed by both separate and composite components regarding the decision variables of interconnected agents, (ii) local bounded convex constraints, and (iii) coupled linear constraints. This problem is directly originated from smart buildings and is also broad in other domains. To provide a distributed algorithm with convergence guarantee, we revise the powerful alternating direction method of multiplier (ADMM) method and proposed a proximal ADMM. Specifically, noting that the main difficulty to establish the convergence for the nonconvex and nonsmooth optimization with ADMM is to assume the boundness of dual updates, we propose to update the dual variables in a discounted manner. This leads to the establishment of a so-called sufficiently decreasing and lower bounded Lyapunov function, which is critical to establish the convergence. We prove that the method converges to some approximate stationary points. We besides showcase the efficacy and performance of the method by a numerical example and the concrete application to multi-zone heating, ventilation, and air-conditioning (HVAC) control in smart buildings.}
}
@inproceedings{inproceedings,
author = {Duchi, John and Shalev-Shwartz, Shai and Singer, Yoram and Tewari, Ambuj},
year = {2010},
month = {12},
pages = {14-26},
title = {Composite Objective Mirror Descent.}
}
@inproceedings{Hu2009AcceleratedGM,
  title={Accelerated Gradient Methods for Stochastic Optimization and Online Learning},
  author={Chonghai Hu and James Tin-Yau Kwok and Weike Pan},
  booktitle={NuerIPS},
  year={2009}
}
@article{Ghadimi2012OptimalSA,
  title={Optimal Stochastic Approximation Algorithms for Strongly Convex Stochastic Composite Optimization I: A Generic Algorithmic Framework},
  author={Saeed Ghadimi and Guanghui Lan},
  journal={SIAM J. Optim.},
  year={2012},
  volume={22},
  pages={1469-1492}
}
@inproceedings{Chen2012OptimalRD,
  title={Optimal Regularized Dual Averaging Methods for Stochastic Optimization},
  author={X. Chen and Qihang Lin and Javier F. Pena},
  booktitle={NuerIPS},
  year={2012}
}
@article{doi:10.1137/140990309,
author = {Hong, Mingyi and Luo, Zhi-Quan and Razaviyayn, Meisam},
title = {Convergence Analysis of Alternating Direction Method of Multipliers for a Family of Nonconvex Problems},
journal = {SIAM Journal on Optimization},
volume = {26},
number = {1},
pages = {337-364},
year = {2016},
doi = {10.1137/140990309},
eprint = {https://doi.org/10.1137/140990309}
,abstract = { The alternating direction method of multipliers (ADMM) is widely used to solve large-scale linearly constrained optimization problems, convex or nonconvex, in many engineering fields. However there is a general lack of theoretical understanding of the algorithm when the objective function is nonconvex. In this paper we analyze the convergence of the ADMM for solving certain nonconvex consensus and sharing problems. We show that the classical ADMM converges to the set of stationary solutions, provided that the penalty parameter in the augmented Lagrangian is chosen to be sufficiently large. For the sharing problems, we show that the ADMM is convergent regardless of the number of variable blocks. Our analysis does not impose any assumptions on the iterates generated by the algorithm and is broadly applicable to many ADMM variants involving proximal update rules and various flexible block selection rules. }
}
@INPROCEEDINGS{9287537,  author={Lopez-Ramos, Luis M. and Beferull-Lozano, Baltasar},  booktitle={2020 28th European Signal Processing Conference (EUSIPCO)},   title={Online Hyperparameter Search Interleaved with Proximal Parameter Updates},   year={2021},  volume={},  number={},  pages={2085-2089},  doi={10.23919/Eusipco47968.2020.9287537}}
@article{article7,
author = {Liang, Jingwei and Poon, Clarice},
year = {2022},
month = {07},
pages = {1-51},
title = {Variable Screening for Sparse Online Regression},
journal = {Journal of Computational and Graphical Statistics},
doi = {10.1080/10618600.2022.2099872}
}
@ARTICLE{7296710,  author={Ditzler, Gregory and Roveri, Manuel and Alippi, Cesare and Polikar, Robi},  journal={IEEE Computational Intelligence Magazine},   title={Learning in Nonstationary Environments: A Survey},   year={2015},  volume={10},  number={4},  pages={12-25},  doi={10.1109/MCI.2015.2471196}}
@INPROCEEDINGS{9287550,  author={Shafipour, Rasoul and Mateos, Gonzalo},  booktitle={2020 28th European Signal Processing Conference (EUSIPCO)},   title={Online proximal gradient for learning graphs from streaming signals},   year={2021},  volume={},  number={},  pages={865-869},  doi={10.23919/Eusipco47968.2020.9287550}}
@article{JOULANI2020108,
title = {A modular analysis of adaptive (non-)convex optimization: Optimism, composite objectives, variance reduction, and variational bounds},
journal = {Theoretical Computer Science},
volume = {808},
pages = {108-138},
year = {2020},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2019.11.015},
author = {Pooria Joulani and András György and Csaba Szepesvári},
keywords = {Online learning, Stochastic optimization, Adaptive algorithms, Regret bound, Follow the regularized leader, Mirror descent},
abstract = {Recently, much work has been done on extending the scope of online learning and incremental stochastic optimization algorithms. In this paper we contribute to this effort in two ways: First, based on a generalization of Bregman divergences and a generic regret decomposition, we provide a self-contained, modular analysis of the two workhorses of online learning: (general) adaptive versions of Mirror Descent (MD) and the Follow-the-Regularized-Leader (FTRL) algorithms. The analysis is done with extra care so as not to introduce assumptions not needed in the proofs and allows to combine, in a straightforward way, different algorithmic ideas (e.g., adaptivity, optimism, implicit updates, variance reduction) and learning settings (e.g., strongly convex or composite objectives). This way we are able to reprove, extend and refine a large body of the literature, while keeping the proofs concise. The second contribution is a by-product of this careful analysis: We present algorithms with improved variational bounds for smooth, composite objectives, including a new family of optimistic MD algorithms with only one projection step per round. Furthermore, we provide a simple extension of adaptive regret bounds to a class of practically relevant non-convex problem settings (namely, star-convex loss functions and their extensions) with essentially no extra effort.}
}

@ARTICLE{2022arXiv220209338M,
       author = {{Meyers}, Bennet E. and {Boyd}, Stephen P.},
        title = "{Signal Decomposition Using Masked Proximal Operators}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
         year = 2022,
          eid = {arXiv:2202.09338},
        pages = {arXiv:2202.09338}
}

@inproceedings{6483273,  author={Chen, Annie I. and Ozdaglar, Asuman},  booktitle={2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},   title={A fast distributed proximal-gradient method},   year={2012},  volume={},  number={},  pages={601-608},  doi={10.1109/Allerton.2012.6483273}}

@article{Alghunaim2021DecentralizedPG,
  title={Decentralized Proximal Gradient Algorithms With Linear Convergence Rates},
  author={Sulaiman A. Alghunaim and Ernest K. Ryu and K. Yuan and Ali H. Sayed},
  journal={IEEE Transactions on Automatic Control},
  year={2021},
  volume={66},
  pages={2787-2794}
}

@article{RN74,
   author = {Dixit, Rishabh and Bedi, Amrit Singh and Tripathi, Ruchi and Rajawat, Ketan},
   title = {Online Learning With Inexact Proximal Online Gradient Descent Algorithms},
   journal = {IEEE Transactions on Signal Processing},
   volume = {67},
   number = {5},
   pages = {1338-1352},
   ISSN = {1053-587X
1941-0476},
   DOI = {10.1109/tsp.2018.2890368},
   year = {2019},
   type = {Journal Article}
}

@INPROCEEDINGS{5495870,  author={Murakami, Yukihiro and Yamagishi, Masao and Yukawa, Masahiro and Yamada, Isao},  booktitle={2010 IEEE International Conference on Acoustics, Speech and Signal Processing},   title={A sparse adaptive filtering using time-varying soft-thresholding techniques},   year={2010},  volume={},  number={},  pages={3734-3737},  doi={10.1109/ICASSP.2010.5495870}}
@INPROCEEDINGS{5947303,  author={Yamagishi, Masao and Yukawa, Masahiro and Yamada, Isao},  booktitle={2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Acceleration of adaptive proximal forward-backward splitting method and its application to sparse system identification},   year={2011},  volume={},  number={},  pages={4296-4299},  doi={10.1109/ICASSP.2011.5947303}}
@article{Yamamoto2012AdaptivePF,
  title={Adaptive proximal forward-backward splitting for sparse system identification under impulsive noise},
  author={Takayuki Yamamoto and Masao Yamagishi and Isao Yamada},
  journal={2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)},
  year={2012},
  pages={2620-2624}
}

@ARTICLE{9178808,
  author={Li, Yan and Cao, Xiaofeng and Chen, Honghui},
  journal={IEEE Access}, 
  title={Fully Projection-Free Proximal Stochastic Gradient Method With Optimal Convergence Rates}, 
  year={2020},
  volume={8},
  number={},
  pages={165904-165912},
  doi={10.1109/ACCESS.2020.3019885}}

@article{10.5555/1577069.1755882,
author = {Duchi, John and Singer, Yoram},
title = {Efficient Online and Batch Learning Using Forward Backward Splitting},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {We describe, analyze, and experiment with a framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we first perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the first phase. This view yields a simple yet effective algorithm that can be used for batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as l1. We derive concrete and very simple algorithms for minimization of loss functions with l1, l2, l22, and l∞ regularization. We also show how to construct efficient algorithms for mixed-norm l1/lq regularization. We further extend the algorithms and give efficient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in a series of experiments with synthetic and natural data sets.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {2899–2934},
numpages = {36}
}
@article{Yu2020ALC,
  title={A Low Complexity Algorithm with ${O}(\sqrt{T})$ Regret and ${O}(1)$ Constraint Violations for Online Convex Optimization with Long Term Constraints},
  author={Hao Yu and Michael J. Neely},
  journal={J. Mach. Learn. Res.},
  year={2020},
  volume={21},
  pages={1-24}
}

@inproceedings{Schmidt2011ConvergenceRO,
author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
title = {Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization},
year = {2011},
isbn = {9781618395993},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term. We show that both the basic proximal-gradient method and the accelerated proximal-gradient method achieve the same convergence rate as in the error-free case, provided that the errors decrease at appropriate rates. Using these rates, we perform as well as or better than a carefully chosen fixed error level on a set of structured sparsity problems.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1458–1466},
numpages = {9},
location = {Granada, Spain},
}

@ARTICLE{9070199,  author={Yuan, Deming and Hong, Yiguang and Ho, Daniel W. C. and Xu, Shengyuan},  journal={IEEE Transactions on Automatic Control},   title={Distributed Mirror Descent for Online Composite Optimization},   year={2021},  volume={66},  number={2},  pages={714-729},  doi={10.1109/TAC.2020.2987379}}

@article{Salzo2011InexactAA,
author = {Salzo, Saverio and Villa, Silvia},
year = {2012},
month = {01},
pages = {1167-1192},
title = {Inexact and accelerated proximal point algorithms},
volume = {19},
journal = {Journal of Convex Analysis}
}

@article{2020strongly,
author = {Dixit, Rishabh and Bedi, Amrit and Rajawat, Ketan},
year = {2020},
month = {10},
pages = {1-1},
title = {Online Learning Over Dynamic Graphs via Distributed Proximal Gradient Algorithm},
volume = {PP},
journal = {IEEE Transactions on Automatic Control},
doi = {10.1109/TAC.2020.3033712}
}

@ARTICLE{8950383,  author={Yi, Xinlei and Li, Xiuxian and Xie, Lihua and Johansson, Karl H.},  journal={IEEE Transactions on Signal Processing},   title={Distributed Online Convex Optimization With Time-Varying Coupled Inequality Constraints},   year={2020},  volume={68},  number={},  pages={731-746},  doi={10.1109/TSP.2020.2964200}}

@ARTICLE{6698348,  author={Tao, Qing and Gao, Qian-Kun and Chu, De-Jun and Wu, Gao-Wei},  journal={IEEE Transactions on Neural Networks and Learning Systems},   title={Stochastic Learning via Optimizing the Variational Inequalities},   year={2014},  volume={25},  number={10},  pages={1769-1778},  doi={10.1109/TNNLS.2013.2294741}}

@ARTICLE{7390084,  author={Xue, Wei and Zhang, Wensheng},  journal={IEEE Transactions on Neural Networks and Learning Systems},   title={Learning a Coupled Linearized Method in Online Setting},   year={2017},  volume={28},  number={2},  pages={438-450},  doi={10.1109/TNNLS.2016.2514413}}
%ogm background
@ARTICLE{9165951,  author={Nutalapati, Mohan Krishna and Bedi, Amrit Singh and Rajawat, Ketan and Coupechoux, Marceau},  journal={IEEE Transactions on Signal Processing},   title={Online Trajectory Optimization Using Inexact Gradient Feedback for Time-Varying Environments},   year={2020},  volume={68},  number={},  pages={4824-4838},  doi={10.1109/TSP.2020.3015276}}

@ARTICLE{allocation,  author={Chen, Tianyi and Ling, Qing and Giannakis, Georgios B.},  journal={IEEE Transactions on Signal Processing},   title={An Online Convex Optimization Approach to Proactive Network Resource Allocation},   year={2017},  volume={65},  number={24},  pages={6350-6364},  doi={10.1109/TSP.2017.2750109}}

@INPROCEEDINGS{9500493,  author={Wang, Tianyu and Yu, Wentao and Wang, Shaowei},  booktitle={Proceedings of IEEE International Conference on Communications},   title={Inter-Slice Radio Resource Management via Online Convex Optimization},   year={2021},  volume={},  number={},  pages={1-6},  doi={10.1109/ICC42927.2021.9500493}}

@INPROCEEDINGS{6130318,  author={Graber, Gottfried and Pock, Thomas and Bischof, Horst},  booktitle={2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)},   title={Online 3D reconstruction using convex optimization},   year={2011},  volume={},  number={},  pages={708-711},  doi={10.1109/ICCVW.2011.6130318}}


@article{bubeck2011introduction,
author = {Hazan, Elad},
year = {2016},
month = {01},
pages = {157-325},
title = {Introduction to Online Convex Optimization},
volume = {2},
journal = {Foundations and Trends in Optimization},
doi = {10.1561/2400000013}
}

@inproceedings{Zinkevich2003,
author = {Zinkevich, Martin},
title = {Online Convex Programming and Generalized Infinitesimal Gradient Ascent},
year = {2003},
isbn = {1577351894},
pages = {928–935},
numpages = {8},
location = {Washington, DC, USA},
booktitle= {ICML}
}

@ARTICLE{9184135,  author={Li, Xiuxian and Yi, Xinlei and Xie, Lihua},  journal={IEEE Transactions on Automatic Control},   title={Distributed Online Optimization for Multi-Agent Networks With Coupled Inequality Constraints},   year={2021},  volume={66},  number={8},  pages={3575-3591},  doi={10.1109/TAC.2020.3021011}}

@ARTICLE{9525208,  author={Li, Xiuxian and Yi, Xinlei and Xie, Lihua},  journal={IEEE Transactions on Control of Network Systems},   title={Distributed Online Convex Optimization With an Aggregative Variable},   year={2022},  volume={9},  number={1},  pages={438-449},  doi={10.1109/TCNS.2021.3107480}}

@article{Chang2020UnconstrainedOO,
  title={Unconstrained Online Optimization: Dynamic Regret Analysis of Strongly Convex and Smooth Problems},
  author={Ting-Jui Chang and Shahin Shahrampour},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.03912}
}

@article{Zhao2020DynamicRO,
  title={Dynamic Regret of Convex and Smooth Functions},
  author={Peng Zhao and Yu-Jie Zhang and Lijun Zhang and Zhi-Hua Zhou},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.03479}
}
@ARTICLE{2021pgmDynamicRegret,
  author={Zhao, Yawei and Qiu, Shuang and Li, Kuan and Luo, Lailong and Yin, Jianping and Liu, Ji},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Proximal Online Gradient Is Optimum for Dynamic Regret: A General Lower Bound}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  doi={10.1109/TNNLS.2021.3087579}}

@article{DBLP:journals/corr/RakhlinS13,
author = {Rakhlin, Alexander and Sridharan, Karthik},
year = {2013},
month = {11},
pages = {},
title = {Optimization, Learning, and Games With Predictable Sequences},
journal = {Advances in Neural Information Processing Systems}
}

@article{gradualvar,
author = {Chiang, Chao-Kai and Yang, Tianbao and Lee, Chia-Jung and Mahdavi, Mehrdad and Lu, Chi-Jen and Jin, Rong and Zhu, Shenghuo},
year = {2012},
month = {01},
pages = {},
title = {Online Optimization with Gradual Variations},
volume = {23},
journal = {Journal of Machine Learning Research}
}

@ARTICLE{2020arXiv200703479Z,
       author = {{Zhao}, Peng and {Zhang}, Yu-Jie and {Zhang}, Lijun and {Zhou}, Zhi-Hua},
        title = "{Dynamic Regret of Convex and Smooth Functions}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2020,
        month = jul,
          eid = {arXiv:2007.03479},
        pages = {arXiv:2007.03479},
archivePrefix = {arXiv},
       eprint = {2007.03479},
 primaryClass = {cs.LG},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2020arXiv200605876Z,
       author = {{Zhao}, Peng and {Zhang}, Lijun},
        title = "{Improved Analysis for Dynamic Regret of Strongly Convex and Smooth Functions}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2020,
        month = jun,
          eid = {arXiv:2006.05876},
        pages = {arXiv:2006.05876},
archivePrefix = {arXiv},
       eprint = {2006.05876},
 primaryClass = {cs.LG},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{10.5555/3294771.3294841,
author = {Zhang, Lijun and Yangt, Tianbao and Yi, Jinfeng and Jin, Rong and Zhou, Zhi-Hua},
title = {Improved Dynamic Regret for Non-Degenerate Functions},
year = {2017},
isbn = {9781510860964},
address = {Red Hook, NY, USA},
booktitle = {NeurIPS},
pages = {732–741},
numpages = {10},
location = {Long Beach, California, USA},
}

@inproceedings{10.5555/3042817.3042884,
author = {Hall, Eric C. and Willett, Rebecca M.},
title = {Dynamical Models and Tracking Regret in Online Convex Programming},
year = {2013},
publisher = {JMLR.org},
pages = {I–579–I–587},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@article{CAO2021109676,
title = {Decentralized online convex optimization based on signs of relative states},
journal = {Automatica},
volume = {129},
pages = {109676},
year = {2021},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2021.109676},
author = {Xuanyu Cao and Tamer Başar}}

@ARTICLE{9325943,  author={Kalhan, Deepak S. and Singh Bedi, Amrit and Koppel, Alec and Rajawat, Ketan and Hassani, Hamed and Gupta, Abhishek K. and Banerjee, Adrish},  journal={IEEE Transactions on Signal Processing},   title={Dynamic Online Learning via Frank-Wolfe Algorithm},   year={2021},  volume={69},  number={},  pages={932-947},  doi={10.1109/TSP.2021.3051871}}

@inproceedings{Zhang2018AdaptiveOL,
author = {Zhang, Lijun and Lu, Shiyin and Zhou, Zhi-Hua},
title = {Adaptive Online Learning in Dynamic Environments},
year = {2018},
address = {Red Hook, NY, USA},
abstract = {In this paper, we study online convex optimization in dynamic environments, and aim to bound the dynamic regret with respect to any sequence of comparators. Existing work have shown that online gradient descent enjoys an O(√T(1 + PT)) dynamic regret, where T is the number of iterations and PT is the path-length of the comparator sequence. However, this result is unsatisfactory, as there exists a large gap from the Ω(√T(1 + PT)) lower bound established in our paper. To address this limitation, we develop a novel online method, namely adaptive learning for dynamic environment (Ader), which achieves an optimal O(√T(1 + PT)) dynamic regret. The basic idea is to maintain a set of experts, each attaining an optimal dynamic regret for a specific path-length, and combines them with an expert-tracking algorithm. Furthermore, we propose an improved Ader based on the surrogate loss, and in this way the number of gradient evaluations per round is reduced from O(log T) to 1. Finally, we extend Ader to the setting that a sequence of dynamical models is available to characterize the comparators.},
booktitle = {NeurIPS},
pages = {1330–1340},
numpages = {11},
location = {Montr\'{e}al, Canada}
}

@ARTICLE{9468952,  author={Li, Jueyou and Li, Chaojie and Yu, Wenwu and Zhu, Xiaomei and Yu, Xinghuo},  journal={IEEE Transactions on Network Science and Engineering},   title={Distributed Online Bandit Learning in Dynamic Environments Over Unbalanced Digraphs},   year={2021},  volume={8},  number={4},  pages={3034-3047},  doi={10.1109/TNSE.2021.3093536}}

@ARTICLE{2015Hall,  author={Hall, Eric C. and Willett, Rebecca M.},  journal={IEEE Journal of Selected Topics in Signal Processing},   title={Online Convex Optimization in Dynamic Environments},   year={2015},  volume={9},  number={4},  pages={647-662},  doi={10.1109/JSTSP.2015.2404790}}
@article{2016Yang,
author = {Yang, Tianbao and Zhang, Lijun and Jin, Rong and Yi, Jinfeng},
year = {2016},
month = {05},
pages = {},
title = {Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient}
}

@ARTICLE{9722367,  author={Eshraghi, Nima and Liang, Ben},  journal={IEEE Control Systems Letters},   title={Dynamic Regret of Online Mirror Descent for Relatively Smooth Convex Cost Functions},   year={2022},  volume={6},  number={},  pages={2395-2400},  doi={10.1109/LCSYS.2022.3155067}}

@INPROCEEDINGS{2016Mokhtari,  author={Mokhtari, Aryan and Shahrampour, Shahin and Jadbabaie, Ali and Ribeiro, Alejandro},  booktitle={2016 IEEE 55th Conference on Decision and Control (CDC)},   title={Online optimization in dynamic environments: Improved regret rates for strongly convex problems},   year={2016},  volume={},  number={},  pages={7195-7201},  doi={10.1109/CDC.2016.7799379}}

@article{2015Besbes,
author = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
year = {2015},
month = {09},
pages = {1227-1244},
title = {Non-Stationary Stochastic Optimization},
volume = {63},
journal = {Operations Research},
doi = {10.1287/opre.2015.1408}
}
@INPROCEEDINGS{9654953,  author={Bastianello, Nicola and Dall’Anese, Emiliano},  booktitle={2021 European Control Conference (ECC)},   title={Distributed and Inexact Proximal Gradient Method for Online Convex Optimization},   year={2021},  volume={},  number={},  pages={2432-2437},  doi={10.23919/ECC54610.2021.9654953}}

@article{Dixit2021OnlineLO,
  title={Online Learning Over Dynamic Graphs via Distributed Proximal Gradient Algorithm},
  author={Rishabh Dixit and A. S. Bedi and Ketan Rajawat},
  journal={IEEE Transactions on Automatic Control},
  year={2021},
  volume={66},
  pages={5065-5079}
}

@INPROCEEDINGS{2020Ajalloeian,  author={Ajalloeian, Amirhossein and Simonetto, Andrea and Dall’Anese, Emiliano},  booktitle={2020 American Control Conference (ACC)},   title={Inexact Online Proximal-gradient Method for Time-varying Convex Optimization},   year={2020},  volume={},  number={},  pages={2850-2857},  doi={10.23919/ACC45564.2020.9147467}}

@INPROCEEDINGS{7364091,  author={Hadgu, Asmelash Teka and Nigam, Aastha and Diaz-Aviles, Ernesto},  booktitle={2015 IEEE International Conference on Big Data (Big Data)},   title={Large-scale learning with AdaGrad on Spark},   year={2015},  volume={},  number={},  pages={2828-2830},  doi={10.1109/BigData.2015.7364091}}

@INPROCEEDINGS{8852239,  author={Bock, Sebastian and Weiß, Martin},  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},   title={A Proof of Local Convergence for the Adam Optimizer},   year={2019},  volume={},  number={},  pages={1-8},  doi={10.1109/IJCNN.2019.8852239}}

%numerical

@article{Chen2010RegularizedLA,
  title={Regularized Least-Mean-Square Algorithms},
  author={Yilun Chen and Yuantao Gu and Alfred O. Hero},
  journal={arXiv: Methodology},
  year={2010}
}

@ARTICLE{5773043,  author={Negahban, Sahand N. and Wainwright, Martin J.},  journal={IEEE Transactions on Information Theory},   title={Simultaneous Support Recovery in High Dimensions: Benefits and Perils of Block $\ell _{1}/\ell _{\infty} $-Regularization},   year={2011},  volume={57},  number={6},  pages={3841-3863},  doi={10.1109/TIT.2011.2144150}}

@article{LIANG2022268,
title = {Kernel-based online regression with canal loss},
journal = {European Journal of Operational Research},
volume = {297},
number = {1},
pages = {268-279},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2021.05.002},
author = {Xijun Liang and Zhipeng Zhang and Yunquan Song and Ling Jian},
keywords = {Data science, Regression, Nonconvex optimization, Regret bound, Noisy label},
abstract = {Typical online learning methods have brought fruitful achievements based on the framework of online convex optimization. Meanwhile, nonconvex loss functions also received numerous attentions for their merits of noise-resiliency and sparsity. Current nonconvex loss functions are typically designed as smooth for the ease of designing the optimization algorithms. However, these loss functions no longer have the property of sparse support vectors. In this work, we focus on regression with a special type of nonconvex loss function (i.e., canal loss), and propose a kernel-based online regression algorithm, n̲oise-r̲esilient o̲nline r̲egression (NROR), to deal with the noisy labels. The canal loss is a type of horizontally truncated loss and has the merit of sparsity. Although the canal loss is nonconvex and nonsmooth, the regularized canal loss has a property similar to convexity which is called strong pseudo-convexity. Furthermore, the sublinear regret bound of NROR is proved under certain assumptions. Experimental studies show that NROR achieves low prediction errors in terms of mean absolute error and root mean squared error on the datasets of heavy noisy labels. Particularly, we check whether the convergence assumption strictly holds in practice and find that the assumptions required for convergence are rarely violated, and the convergence rate is not affected.}
}

@article{Karbowski2022DistributedOR,
  title={Distributed Online Risk Assessment in the National Cyberspace},
  author={Andrzej Karbowski},
  journal={Electronics},
  year={2022}
}
@INPROCEEDINGS{7364091,  author={Hadgu, Asmelash Teka and Nigam, Aastha and Diaz-Aviles, Ernesto},  booktitle={2015 IEEE International Conference on Big Data (Big Data)},   title={Large-scale learning with AdaGrad on Spark},   year={2015},  volume={},  number={},  pages={2828-2830},  doi={10.1109/BigData.2015.7364091}}

@INPROCEEDINGS{8852239,  author={Bock, Sebastian and Weiß, Martin},  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},   title={A Proof of Local Convergence for the Adam Optimizer},   year={2019},  volume={},  number={},  pages={1-8},  doi={10.1109/IJCNN.2019.8852239}}
@article{Zhou2020ARB,
  title={A Randomized Block-Coordinate Adam online learning optimization algorithm},
  author={Yangfan Zhou and Mingchuan Zhang and Junlong Zhu and Ruijuan Zheng and Qingtao Wu},
  journal={Neural Computing and Applications},
  year={2020},
  pages={1-14}
}
@INPROCEEDINGS{9264510,  author={Shen, Xiuyu and Fang, Runyue and Li, Dequan and Wu, Xiongjun},  booktitle={2020 IEEE 16th International Conference on Control \& Automation (ICCA)},   title={A Distributed Adaptive Moment Estimation Method With Dynamic Bound For Online Optimization},   year={2020},  volume={},  number={},  pages={684-689},  doi={10.1109/ICCA51439.2020.9264510}}