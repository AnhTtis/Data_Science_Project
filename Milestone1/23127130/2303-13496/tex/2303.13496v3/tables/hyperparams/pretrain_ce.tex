\begin{table}[!htb]
    \begin{center}
        \centering
        \resizebox{0.8\linewidth}{!}{
            \begin{tabular}{l|c}
                \bf Setting & \bf Value  \\
                \midrule
                Epochs & 1 \\
                Batch size & 8192 \\
                Optimizer & AdamW~\cite{loshchilov2017decoupled} \\
                Learning rate: \\
                \quad Schedule & Linear \\
                \quad Peak & 4e-4 \\
                \quad Warmup Schedule & Linear \\
                \quad Warmup Fraction & 5\% \\
                Weight decay & 0.1 \\
                Optimizer Momentum & $\beta_1=0.9,\beta_2=0.999$ \\
                Augmentations: \\
                \quad {\tt RandomResizedCrop} \\
                \qquad {\tt size} & 224px \\
                \qquad {\tt scale} & [0.08, 1.00] \\
                \qquad {\tt ratio} & [0.75, 1.33] \\
                \qquad {\tt interpolation} & Bicubic \\
                \quad {\tt RandomHorizontalFlip} & $p=0.5$ \\
                \quad {\tt Normalize} \\
            \end{tabular}
        }
    \end{center}
    \vspace{-0.15in}
    \caption{
        \textbf{\ce Pretraining hyperparameters.} We follow the settings from \cite{singh2022revisiting} without any modifications.
        For \vitTwoB we were able to train with the same hyperparameters, but ultimately reduced the learning rate to 1e-4, enabled
        gradient clipping to 1.0 norm, and set AdamW's $\beta_2$ to 0.95 for improved training stability.
    }
    \label{tab:ce_pretrain_settings}
\end{table}

