
\begin{table}[!htb]
    \begin{center}
        \centering
        \resizebox{0.8\linewidth}{!}{
            \begin{tabular}{l|c}
                \bf Setting & \bf Value  \\
                \midrule
                Epochs & 1 \\
                Batch size & 32768 \\
                Optimizer & AdamW~\cite{loshchilov2017decoupled} \\
                Learning rate: \\
                \quad Schedule & Cosine \\
                \quad Peak & 2e-4 \\
                \quad Warmup Schedule & Linear \\
                \quad Warmup Fraction & 4\% \\
                Weight decay & 0.1 \\
                Optimizer Momentum & $\beta_1=0.9,\beta_2=0.98$ \\
                Loss: \\
                \quad {\tt CLIP~\cite{radford2021learning}} \\
                \quad {\tt LabelSmoothing}~\cite{szegedy2016rethinking} & 0.1 \\
                Augmentations: \\
                \quad {\tt RandomResizedCrop} \\
                \qquad {\tt size} & 224px \\
                \qquad {\tt scale} & [0.9, 1.00] \\
                \qquad {\tt ratio} & [0.75, 1.33] \\
                \qquad {\tt interpolation} & Bicubic \\
                \quad {\tt RandomHorizontalFlip} & $p=0.5$ \\
                \quad {\tt Normalize} \\
            \end{tabular}
        }
    \vspace{-0.15in}
    \end{center}
    \caption{
        \textbf{\lit training hyperparameters} for \cref{tab:zero_shot}. 
        For ablations with \xlmr Base we used a higher learning rate of 1e-3 with a
        dropout of 0.1.
    }
    \label{tab:lit_settings}
\end{table}
