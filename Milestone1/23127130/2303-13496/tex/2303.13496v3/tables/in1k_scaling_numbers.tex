\begin{table}[H]
    \centering
    \setlength{\tabcolsep}{3pt}
    \resizebox{0.85\linewidth}{!}{
    \begin{tabu}{c|ccccc}
        \centering
        \bf Method & \bf \vitB & \bf \vitL & \bf \vitH & \bf \vitTwoB & \bf \vitSixB \\
        \midrule
        \multicolumn{6}{l}{\emph{\mae pretrained on \igSizeShort}} \\
        Linear 224px & 56.5 & 65.1 & 69.6 & 76.1 & 78.2 \\
        Finetune 224px & 83.5 & 86.1 & 87.4 & 87.8 & 88.3 \\
        \midrule
        \multicolumn{6}{l}{\emph{\oursShort (\ours) pretrained on \igSizeShort}} \\
        Linear 224px & 82.8 & 86.0 & 87.0 & 88.1 & 88.6 \\
        Finetune 518px & 86.4 & 88.8 & 89.3 & 89.7 & 90.1 \\
    \end{tabu}%
    }
    \caption{
        \textbf{\mae and \oursShort \inetOneKShort performance across all model scales.} We show the linear and finetuned performance on \inetOneK for \mae and \oursShort pretraining 
        on \igSize. Both \mae \prept and \oursShort scale well, starting from 86M parameters, up to 6.5B parameters.
        Using frozen features and when finetuned, \oursShort models are some of the strongest models on \inetOneK at all size scales.
    }
    \label{tab:overall_in1k_scaling_numbers}
\end{table}
