
\subsection{Datasets and training details}

\input{tables/eval_datasets}

\noindent \textbf{Pretraining dataset.} We use \igSize(\igSizeShort) a billion-scale multi-label dataset
sourced from \ig (\igShort). 
This multi-label dataset contains 28K classes and 3B unique images, resampled to 5B total images,
and was produced by running the dataset generation pipeline from \swag~\cite{singh2022revisiting} without modification.
Compared to~\cite{singh2022revisiting}, our
version of the dataset has 16\% fewer images (3.0B \vs 3.6B), but we were able to reproduce the results
from~\cite{singh2022revisiting} with our version.
We obtain labels using an automated process wherein we first obtain hashtags from the associated image captions, and
then map the hashtags to WordNet synsets following~\cite{singh2022revisiting}.
After this processing, we get the weakly labeled \igShort dataset that contains images and their associated labels.



\par \noindent \textbf{Evaluation datasets.}
We evaluate \ours on a variety of different downstream visual recognition tasks.
To evaluate our model on image classification, we use the standard \inetOneK~\cite{ILSVRC15} (\inetOneKShort) dataset,
and also the long-tailed and fine-grained \inat~\cite{iNaturalist} (\inatShort) dataset.
For object detection and segmentation, we use the popular \cocoShort~\cite{lin2014microsoft} dataset, and also \lvisShort
\cite{gupta2019lvis}, a large vocabulary dataset for long tailed object recognition.
We evaluate video classification performance using two popular action recognition datasets, \kinetics~\cite{kay2017kinetics}
(\kineticsShort) and \sthsth~\cite{goyal2017something} (\sthsthShort). For zero-shot transfer, we evaluate on \inetOneKShort
and \food~\cite{bossard2014food} (\foodShort).
We also evaluate the robustness of our models on test sets which overlap with \inetOneKShort classes, specifically
\inetvTwo~\cite{recht2019imagenet} (\inetvTwoShort), \inetReal~\cite{beyer2020we} (\inetRealShort),
and \objectNet~\cite{barbu2019objectnet} (\objectNetShort). 
Please see~\cref{tab:eval_datasets} for more details.


\par \noindent \textbf{\mae pretraining details.}
We follow \cite{he2021masked} to train \mae models on \igSizeShort without using any labels.
We mask 75\% of the image for this training and train the model for 1 epoch over the dataset. We follow the same hyperparameters
used in \cite{he2021masked} for pretraining on \inetOneKShort.

\par \noindent \textbf{Supervised pretraining details.}
We train with a supervised cross-entropy loss on \igSizeShort using the hashtags as labels.
This model is trained by default with random weight initialization and we use the training hyperparameters from~\cite{singh2022revisiting}.
\par \noindent \textbf{Using \prept.}
When using \prept, we first train a model from scratch using \mae on the \igShort dataset.
We then use the weights of the \mae encoder and perform supervised pretraining using the cross-entropy loss as described above.
We reuse the same hyperparameters and training details as
\cite{singh2022revisiting}, \ie \emph{there is no hyperparameter search needed} for \ours, and we train for 1 epoch on
\igSizeShort. 


\par \noindent \textbf{Zero-shot training and evaluation details.}
To impart zero shot understanding capabilities to our models, we use the \lit approach from~\cite{zhai2022lit}.
For \lit, we use the original \emph{(image, caption)} pairs from the \igSizeShort dataset.
We \emph{freeze} the image encoder, and train a text encoder to encode the image captions and match the text embeddings to the associated image embedding using a \clip loss~\cite{radford2021learning}.
We train the text encoder for 1 epoch.
For evaluation, we follow~\cite{radford2021learning} -- we use the text encoder to compute embeddings from the templated text descriptions of classes and use the cosine similarity of the image and text embeddings as the classification score.

For full training details and hyperparameters, please refer to \cref{app:pretraining_details}.


