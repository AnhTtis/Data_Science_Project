\section{Setup}

Our goal is to empirically study the effectiveness of self-supervised pre-pretraining as a precursor to billion scale weakly supervised pretraining for representation learning.
Given the simplicity and efficiency of Masked AutoEncoding (MAE)~\cite{he2021masked},
we leverage it as the self-supervised pre-pretraining approach. %
Our study shows that MAE scales with the size of the pretraining dataset and model size, and combining it with weak supervision improves large scale vision models.
Additionally, such a combination leads to faster convergence and is a simple, scalable way to learn visual representations at scale.
We describe our setup and the approaches in detail next.


\par \noindent \textbf{Architecure.}
We use the Vision Transformer (\vit)~\cite{dosovitskiy2020image} architecture as the visual encoder for all our experiments.
ViTs employ minimal vision-specific inductive biases combined with the standard transformer architecture~\cite{vaswani2017attention}, and yet have emerged as an architecture of choice for a wide variety of visual and multimodal recognition tasks~\cite{zhai2022scaling,arnab2021vivit,girdhar2023omnimae}.
We train \vit models
at various scales in terms of number of parameters, including \vitB (86M), \vitL (307M), and \vitH (632M). We also train on larger 1.9B and 6.5B parameter \vit models, which
we call \vitTwoB and \vitSixB, respectively (\cref{tab:app_model_architectures}).
As is common practice \cite{dosovitskiy2020image,zhai2022scaling}, we train models of sizes \vitB, \vitL with a patch size
of 16 and larger models with a patch size of 14. We pretrain with a $224\times{}224$ resolution for all models. 

\input{tables/model_architectures.tex}
\par \noindent \textbf{Pre-pretraining (\mae)}~\cite{he2021masked} learns visual representations from image datasets without using any labels. %
We choose this approach as it is simple to implement and scales very effectively with large \vit model sizes due to patch dropping as described next. %
\mae randomly masks 75\% of an image %
and trains the model to reconstruct the masked input image by minimizing the pixel reconstruction error.
The target pixel values for a given patch are normalized by the mean and standard deviation of all pixels in it.
Coupled with the \vit architecture, \mae can be trained by only processing the 25\% unmasked image patches.
A separate, smaller, decoder is then used to reconstruct the missing part of the input.
This asymmetrical design makes training the encoder extremely efficient, allowing for scaling visual encoder sizes.


\par \noindent \textbf{Weakly-supervised pretraining (\ce)} leverages images with associated `weak' supervision for training models.
In particular, we focus on internet images and use their associated text information as supervision.
We convert the text into a discrete set of labels, specifically leveraging hash-tag information~\cite{mahajan2018exploring,singh2022revisiting,ghadiyaram2019large}.
We then use a multi-label classification loss to train models.
We refer to this method as \ce. %


\par \noindent \textbf{\ours}, or \textbf{\oursShort} for short,
first trains the encoder %
using the \mae self-supervised method using only the images. %
This pre-pretraining stage initializes the model while simultaneously being computationally efficient because of the masking used in \mae.
In the second stage, we pretrain the encoder using both the image and associated weak supervision. %
This combination outperforms using either strategy in isolation, \ie, an \mae model or a weakly supervised model trained from scratch.


