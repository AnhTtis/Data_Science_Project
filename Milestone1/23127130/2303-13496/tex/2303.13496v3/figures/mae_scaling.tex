

\begin{figure*}[!t]
\begin{center}
  \captionsetup{type=figure}
    \begin{subfigure}{.245\textwidth}
        \include{plots/mae_scaling_ft_in1k}
    \end{subfigure} \hfill
    \begin{subfigure}{.245\textwidth}
        \include{plots/mae_scaling_ft_inat18}
    \end{subfigure} \hfill
    \begin{subfigure}{.245\textwidth}
        \include{plots/mae_scaling_lvis}
    \end{subfigure} \hfill
    \begin{subfigure}{.245\textwidth}
        \include{plots/mae_scaling_coco}
    \end{subfigure}
    \ifarxiv
        \vspace{-0.23in}
    \else
        \vspace{-0.4in}
    \fi
    \caption{\textbf{Scaling MAE with model and dataset size}. We plot \mae's performance when pretrained on \inetOneK or \igSize and
    finetuned on downstream tasks. \mae scales to billion parameters sized models using just \inetOneKShort pretraining.
    Larger models show improved scaling behavior when pretrained with the much larger \igSizeShort dataset.
    Tabulated results in Appendix \cref{tab:mae_scaling_numbers}.
    \inetOneKShort and \inatShort results are finetuned at 224px resolution.
    For \cocoShort and \lvisShort, \mae pretrained on \inetOneKShort for \vitTwoB is missing as training at that scale was unstable, and \vitSixB results are skipped due to compute limitations.
    }
    \label{fig:mae_scaling}
\end{center}
\end{figure*}
