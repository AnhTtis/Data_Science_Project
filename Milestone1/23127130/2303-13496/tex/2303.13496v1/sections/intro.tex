\section{Introduction}
\label{sec:intro}

\input{figures/mae_vs_ce_vs_mae_ce_radar_ig.tex}

The pretrain-then-finetune paradigm in visual recognition has enabled high performance visual recognition models across a range of tasks such as image classification~\cite{singh2022revisiting,mahajan2018exploring,radford2021learning}, video action recognition~\cite{girdhar2023omnimae,feichtenhofer2022masked,ghadiyaram2019large}, object detection~\cite{caron2020unsupervised,zhou2021ibot}, 3D \etc.
Typically, pretraining consists of training a model using a pretraining task on large scale data.
The resulting pretrained models learn general purpose visual representations that can be used for a range of target tasks, often with limited labeled data, by transfer learning.

In this paper, we show that an initial stage of \prept before the standard pretraining task can improve vision models across a variety of different tasks.
Our method combines two common pretraining tasks in vision: (1) weakly supervised pretraining that uses weak, often noisy, signals such as text or image hashtags as supervision, and (2) self-supervised pretraining that only uses the data without additional supervision.
Both forms of pretraining start training with a randomly initialized model and have proven effective at learning general purpose vision models.
While there have been attempts to combine both these forms of pretraining~\cite{mu2022slip, singh2022flava}, they are typically used independently in the pretrain-then-finetune two stage paradigm~\cite{dosovitskiy2020image, zhai2022scaling,singh2022revisiting}.

In this work we explore the combination of self- and weakly-supervised learning in a simple \textit{pre}-pretraining framework, as follows. 
We first begin with the Masked Autoencoder (\mae)~\cite{he2021masked} self-supervised learning technique to \pptrain vision models without using any labels.
After initializing from the pre-pretrained model, we use standard weakly supervised pretraining on billions of images with noisy labels.
We perform a large-scale empirical study to measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video recognition, object detection, low-shot classification and zero-shot recognition. 
Our study reveals that \pptrain initialization improves the performance for the weakly supervised models, and this improvement holds even at billion scale weakly labeled data, and across vision tasks (\cref{fig:mae_vs_ce_vs_mae_ce_ig}).
It also improves the model convergence during pretraining, leading to an efficient way of training large scale vision models.
\Pptrain further enjoys the computational efficiency of the MAE approach, making it simple and scalable.
Finally, we show that by using \pptraining, \emph{both} self-supervised learning and weakly supervised learning can be combined for improved model performance for billion-scale data.

\Pptrain is related to `intermediate finetuning'~\cite{bao2021beit, liu2022swin} which introduces a stage \emph{after} pretraining to better align the pretrained features with the downstream task using labeled data.
In contrast, \pptrain serves as a better way to initialize a model \emph{before} pretraining.
Since we leverage MAE for \pptraining, we do not need additional information or labels for this stage and can re-use the pretraining data.
This makes \pptrain convenient and simple to use with existing pretraining datasets.

Our study on large-scale \pptraining reveals that model initialization plays a significant role, even for web-scale pretraining, and \prept is a simple and promising technique in that direction. In particular, we show that
 (i) \mae not only scales with model size as shown in~\cite{he2021masked}, but \textit{also} with the size of the training \textit{data} (\cref{fig:mae_scaling}).
(ii) \Prept improves both the model convergence and the final downstream performance for different sized models (millions to billions of parameters) trained on different sized datasets (millions to billions of labels).
(iii) Using \prept combines the benefits of both self-supervised learning and large scale weakly-supervised learning, and our models achieve excellent performance on a variety of different visual recognition tasks (\cref{fig:mae_vs_ce_vs_mae_ce_ig}). 
Most prominently, our model sets new state-of-the-art results on \inat image classification (91.3\%), 1-shot \inetOneK image classification (62.1\%),
and zero-shot transfer on \food (96.0\%).



















