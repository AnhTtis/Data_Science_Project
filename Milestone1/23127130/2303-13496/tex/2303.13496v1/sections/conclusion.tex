\section{Conclusion}
\label{sec:conclusion}

We introduced \prept which is an initial stage in the standard pretrain-then-finetune paradigm.
\Prept uses \mae and thus, does not need additional supervision and can be conveniently added to web-scale training pipelines.
We show that \prept improves downstream performance on multiple different recognition tasks, improves model convergence, and is overall more efficient than standard weakly-supervised pretraining.
Our self-supervised \prept improves results for models trained with billions of labels, showing that it is a scalable technique that matters even at web-scale.
The benefits of using \prept hold across varying model sizes, and different pretraining data distributions showing that it is a robust technique.
Finally, \prept naturally and successfully combines the two most common pretraining strategies -- self-supervised and weakly-supervised learning.
Our results suggest that model initialization plays a significant role in the final performance, training dynamics \etc even for web-scale training with billions of parameter updates and labels, and should be further investigated.
