



\subsection{Scaling \mae pretraining to large data}
\label{sec:mae-scale}

Since our \prept uses \mae in the very first stage, we first study how \mae behaves on the large scale \igSizeShort dataset.
We compare the performance of \mae pretraining on the large scale \igSizeShort with the original \mae~\cite{he2021masked} models 
trained on \inetOneKShort for 1600 epochs.
We train models of varying sizes, from \vitB to \vitH as in~\cite{he2021masked}.
To test the scaling behavior further, we also train \mae on the \vitTwoB model with 2B parameters.
We measure the performance of the resulting models in
\cref{fig:mae_scaling} on four different vision tasks.

We observe that using the \igSizeShort data provides consistent gains over \inetOneKShort for all vision tasks, and the gain increases for larger models.
These experiments show that \mae scales with the size of the pretraining dataset, and benefits from using billions of images from \igSizeShort.
He \etal~\cite{he2021masked}'s findings were limited to the fact that \mae scales with the size of the model, and thus our findings on \mae scaling with the size of the pretraining data are complementary to theirs.

Our \vitTwoB model pretrained on \igSizeShort improves upon the best results from \cite{he2021masked} on image classification,
attaining 88.4\% on \inetOneKShort (+0.6\%) and 89.3\% on \inatShort (+2.5\%) at 518$\times$518 resolution. 
The gains on detection are equally encouraging, with our \vitTwoB
reaching 53.6 AP\textsuperscript{box} on \lvisShort  (+2.1 over \cite{he2021masked}) and 59.8 AP\textsuperscript{box} on \cocoShort
(+1.2 over \cite{he2021masked}).
Lastly, we highlight the simplicity of our setup, since we use the same hyperparameters as~\cite{he2021masked} to train \mae on 
\igSizeShort, and also for the largest \vitTwoB model, without encountering any training instabilities or needing any extra tweaks.


\subsection{\mae \prept}
\label{sec:pretraining}


\begin{figure*}
    \centering
    \begin{minipage}[t]{.29\textwidth}
        \centering
        \input{figures/mae_for_wsl_model_scaling.tex}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.39\textwidth}
        \centering
        \input{figures/mae_vs_ce_epochs.tex}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.29\textwidth}
        \centering
        \input{figures/mae_for_wsl_flops_scaling.tex}
    \end{minipage}
\end{figure*}

\input{tables/image_and_video_classification.tex}

Given the promising aspects of \mae as a pretraining approach from~\cref{sec:mae-scale}, specifically that \mae (i) trains with larger models and datasets
without needing any tuning (ii) shows gains when scaling model and / or dataset size (iii) is efficient to train, we investigate it
as a \preptemph approach for supervised pretraining (\ce).

\cref{fig:mae_vs_ce_vs_mae_ce_ig} shows the performance of a \vitL with \mae pretraining, supervised pretraining (\ce), or
\mae \prept followed by supervised pretraining (\ours). We see that \mae and \ce have different strengths. \mae has strong performance for object detection, and full finetuned image classification. However, \mae underperforms on tasks where the model
is not finetuned, such as linear
classifiers, zero-shot, or low-shot classification -- situations where \ce performs better. For
these evaluations \mae lags behind \ce by more than 10 points, which is why the results for \mae are not visible in~\cref{fig:mae_vs_ce_vs_mae_ce_ig}.
For video classification, \mae performs significantly better than \ce on \sthsthShort, but lags behind it on \kineticsShort.

\ours outperforms either of \mae or \ce pretraining on most evaluations, across image classification, video recognition, zero-shot evaluation, object detection, \etc.
Given that all baselines are trained at billion-scale, these results show that \ours is a simple yet promising strategy to improve performance while requiring no extra data or tuning.

\noindent Next, we ablate the key aspects of \ours.

\input{figures/mae_vs_ce_vs_mae_ce_radar_in21k.tex}

\par \noindent \textbf{Effect of model size.}
In~\cref{fig:mae_for_wsl_model_scaling} we study the effect of \prept compared to random initialization for different model sizes.
After initialization, all models are pretrained using \ce on the \igSizeShort dataset, and we measure the transfer performance on \inetOneKShort using linear probing.
We observe that \mae \prept gives consistent gains over the \ce baseline across all model sizes, ranging from 86M to 2B parameters.
The gains over the \ce baseline \emph{increase} for larger model sizes showing that \prept shows promising scaling behavior with model sizes.
Notably, a 2B \ours model outperforms a larger 6.5B \ce model.

\par \noindent \textbf{Number of \prept epochs.}
We vary the number of \mae \prept and the number of \ce pretraining epochs to understand their effect on the final recognition performance.
We study this in~\cref{fig:mae_vs_ce_epochs}.

\Prept improves results over the standard pretraining (random initialization, w/o \prept), and provides large gains with fewer \ce pretraining epochs.
\Prept also leads to faster convergence since even a small amount of \prept for 0.1 epochs provides improvements.
Increasing the epochs of \prept provide a larger improvement, and the gains saturate at 1 epoch of \prept.
Finally, \prept's gains do not diminish even after 4 epochs of \ce (20 billion samples) showing the value of \prept at scale.
We also note that these gains are independent of the evaluation protocol, and we observed them with full finetuning on \inetOneKShort as well.


\par \noindent \textbf{Training efficiency.}
\cref{fig:mae_for_wsl_flops_scaling} shows a comparison between \ce and \ours when comparing training FLOPs.
For the same training FLOPs, \ours achieves better transfer performance compared to \ce, and is up to $2\times$ more efficient.
\Prept's training efficiency holds over a large $10\times$ compute window.

\par \noindent \textbf{Different datasets for \prept.}
We also evaluate the performance of \ours when \prept \mae on the much smaller \inetOneK dataset.
\begin{center}
    \resizebox{0.7\linewidth}{!}{
            \begin{tabu}{ll|c|cc}
                \makecell[c]{\bf \mae \\ \bf Dataset} & \makecell[c]{\bf \ce \\ \bf Dataset} & \bf Arch. & \bf \inetOneKShort & \bf \inatShort \\
                \midrule
                \igSizeShort & \igSizeShort & \vitH 
                & 89.3 %
                & 90.5 %
                \\
                \inetOneKShort & \igSizeShort & \vitH 
                & 89.4 %
                & 90.5 %
                \\

            \end{tabu}
    }
\end{center}

\noindent The results show that \prept is just as effective with a small (1M images) dataset that has a different image distribution than the pretraining dataset.
Practically, this allows reusing \mae pretrained models without additional work.

\par \noindent \textbf{Different datasets for \prept and pretraining.}
We investigate the effect of the dataset used in \prept and pretraining by using \inetFullShort~\cite{deng2009imagenet} for all methods, including \mae and \ours.
Compared to \igSizeShort, \inetFullShort is more curated, smaller (14M images), and has cleaner labels (21K classes), where each image is labeled with one class from the WordNet synsets~\cite{miller1995wordnet}.
For evaluating
zero shot performance, we use the \pmd~\cite{singh2022flava} dataset for \lit training.
For full details about the training
hyperparameters, we refer to \cref{app:pretraining_details}.

\cref{fig:mae_vs_ce_vs_mae_ce_in21k} compares the performance of \mae, \ce and \ours when pretrained on this \inetFullShort
dataset.
We notice a similar trend as when pretraining on \igSizeShort where \ours outperforms both \mae and \ce.
This shows that \mae \prept works with datasets of different scales and distributions.












