\section*{Appendix}
\label{sec:appendix}



\section{Pretraining details}
\label{app:pretraining_details}

{\bf \noindent Model architectures.} To ensure we were able to scale models out further than \vitH, and that the models can be
easily scaled beyond a few billion parameters, we decided to scale models along the same lines as GPT-3~\cite{brown2020language},
which has proven to be successful for NLP.
We share the exact settings for all the models discussed in the paper in \cref{tab:app_model_architectures}.

\input{tables/model_architectures.tex}

{\bf \noindent \mae pretraining.} We make no changes from He \etal~\cite{he2021masked} for \mae pretraining.
We utilize the same decoder dimensions as well -- 8 layers, 512 dimension, and 16 heads. Training hyperparameters are shared in
\cref{tab:mae_pretrain_settings}.


\input{tables/hyperparams/pretrain_mae.tex}
\input{tables/hyperparams/pretrain_ce.tex}
\input{tables/hyperparams/lit.tex}
\input{tables/hyperparams/pretrain_ce_in21k.tex}


{\bf \noindent \ce and \ours pretraining.}
We note that large scale \ce pretraining is quite robust to the choice of training hyperparameters, including the choice of
(i) a single \vs two layer MLP head, (ii) softmax cross-entropy \vs binary cross-entropy training loss, (iii) additional training
augmentations like mixup~\cite{zhang2017mixup}, CutMix~\cite{yun2019cutmix}, \etc, (iv) learning rate over a $\sim5\times$ range.
Most of these choices seem to affect results at small scales, like 0.1 epochs over \igSizeShort (500 million samples seen), but the
differences dissipate when training for a full epoch (5 billion samples seen). Our training hyperparameters are shared in
\cref{tab:ce_pretrain_settings}, and we use these settings for both \ce and \ours.
We follow Singh \etal~\cite{singh2022revisiting} for the training setup and hyperparameters for consistency and easier comparisons.
For a label vocabulary $C$, and an image $img$ with labels $L_{img} \in \{0, 1\}^{|C|}$, we utilize softmax cross-entropy,
where our label is normalized to a probability
distribution, $L_{im} / \sum_{c \in C}{L_{im}^c}$, and the model output is passed to a softmax followed by a cross-entropy loss
\cite{singh2022revisiting, mahajan2018exploring}.
We attach a two layer MLP head to the trunk -- \texttt{\small 
    Linear(embed, embed), Tanh(), Linear(embed, classes)}.



{\bf \noindent \lit training.} We follow \lit to train a text encoder on \ig captions.
We sanitize the captions and also remove the pound signs in front of hashtags.
We use a context length (number of tokens) of 100 per caption. Following \clip, we chose the embedding dimension
for aligning the encoders to be 512, 768 and 1024 for \vitB, \vitL and \vitH, respectively, and set it to 2048 for \vitTwoB. 
We use a pretrained \xlmr~\cite{conneau2020unsupervised} text encoders, with the Base size (270M parameters) for ablations and
Large (550M parameters) for the results in \cref{tab:zero_shot}. Our \lit training hyperparameters are shared in 
\cref{tab:lit_settings}.



{\bf \noindent \inetFull pretraining.}
In \inetFullShort each image is labeled with one class, but the classes are based on WordNet synsets~\cite{miller1995wordnet} which are hierarchical in nature.
Some images in this dataset are duplicated across
more than one class -- we deduplicate the images by hashing the image contents, and
convert the dataset to a multi-label dataset.
We disregard the class hierarchy amongst the labels and treat them independently.

For \mae pretraining we again follow \cite{he2021masked} and use the hyperparameters in \cref{tab:mae_pretrain_settings} and train
for 160 epochs over the dataset.
For \ce (and \ours) pretraining on \inetFull, we use the hyperparameters from Steiner \etal\cite{steiner2021train}, with a few minor
differences. We train for 90 epochs over the dataset,
and select the augmentation setting \textit{medium2} of the paper. %
We train with a softmax cross-entropy loss similar to \igSizeShort pretraining, but utilize a head with just a single linear layer.
Full hyperparameters are in \cref{tab:pretrain_details_inet21k}.


For \lit finetuning of models pretrained on \inetFullShort, we follow a similar setup and hyperparameters used for \igSizeShort (\cref{tab:lit_settings}),
and train for 20 epochs on \pmd~\cite{singh2022flava}.
Unlike \igSize dataset which has 5 billion image-text pairs, \pmd has only about 70 million image-text pairs, so we train for multiple epochs on this dataset.



\section{Transfer learning details}
\label{app:transfer_details}

\input{tables/hyperparams/image_ft_mae.tex}
\input{tables/hyperparams/image_ft_ce.tex}
\input{tables/hyperparams/video_ft.tex}
\input{tables/hyperparams/lowshot.tex}
\input{tables/hyperparams/detection_lvis.tex}
\input{tables/hyperparams/detection_coco.tex}



{\bf \noindent Image classification details.} We finetune models pretrained with \mae, \ce and \ours by attaching a single linear
layer on \inetOneKShort and \inatShort. We finetune models at either $224\times{}224$ resolution or $518\times{}518$ resolution (or
$512\times{}512$ for models which use a patch size of 16) and use the same hyperparameters at all resolutions.
The hyperparameters for high resolution finetuning are shared in \cref{tab:mae_image_ft_settings} for \mae models
and in \cref{tab:ce_image_ft_settings} for models pretrained with \ce or \ours.



{\bf \noindent Video classification details.}
For video finetuning, we sample 32 frames out of 2.7 second clips for \kinetics and 4 second clips for \sthsth, and train the
models at $224\times{}224$ resolution. We convert the
input into patches of size $2\times{}16\times{}16$, akin to MAE approaches applied to video
models~\cite{feichtenhofer2022masked,tong2022videomae,girdhar2023omnimae}. We initialize the video models with
weights from our inflated~\cite{carreira2017quo} pretrained image models. \cref{tab:video_ft_settings}
contains the finetuning hyperparameters for both \kineticsShort and \sthsthShort.



{\bf \noindent Low-shot image classification details.}
We adapt all \ours and \swag models with VPT~\cite{jia2022visual} with the same hyperparameters -- 
8 tokens of size 192 per self attention layer, as this proved to be a reasonable default setting.
The full settings for training our models with VPT are described in \cref{tab:hyperparameters_lowshot}.

We also attempted to train \clip and \openclip models with Adapters, but however noticed that they 
do not work as great with VPT as they do with logistic regression, despite sweeping a range of different hyperparameters.
We ultimately adopt the logistic regression protocol of \msn~\cite{assran2022masked} for \clip and \openclip, and
also for \dino and \msn.
For \mae low-shot evaluations, we noticed that neither Adapters nor logistic regression led to great results,
something already seen in \msn~\cite{assran2022masked}. 
We therefore follow \msn and finetune \mae in low-shot settings, but improve upon the results published in 
\cite{assran2022masked} for \mae significantly.
All these results are reported in \cref{tab:low_shot_image_classification}.

{\bf \noindent Zero-shot transfer details.} For evaluation of our \lit models,
we follow the zero-shot evaluation strategy proposed in \clip~\cite{radford2021learning}. We leverage the prompt templates 
and class names introduced in \clip for \inetOneK and
\food. We compute the cosine similarity between the query image and all the generated prompts for each class.
While doing so, we also take advantage of the class prompt ensembling approach introduced
in \clip by considering multiple prompt templates for each class.

{\bf \noindent Detection and segmentation details.}
We train all of our models with the Cascade Mask-RCNN~\cite{he2017maskrcnn} framework and
use the \vitDet~\cite{li2022vitdet} architecture to leverage our \vit models within this framework.

For our \mae models trained on \igShort data, we use the hyperparameters of \mae trained on \inetOneKShort from \cite{li2022vitdet}.
For the large \vitTwoB model, we use settings similar to \vitH and only change the layer decay to be the same as \vitL,
since both those models have $24$ transformer layers.

For \ce and \ours pretraining, we adapt the parameters used for \mae pretraining.
The most salient change is a modification of the layer decay scheme of \mae to cap it at a minimum value of $0.1$,
allowing \ce pretrained models to update the initial layers to align better for detection and segmentation tasks.
This was particularly useful for instance segmentation.

The hyperparameters used for training our detection and instance segmentation models are available
in \cref{tab:detection_parameters_lvis} for \lvisShort and \cref{tab:detection_parameters_coco} for \cocoShort.


\section{Additional Results and Full Tables}

\input{figures/mae_scaling_lin_in1k}
\input{tables/mae_scaling_and_radar_numbers.tex}
\input{tables/prepretrain_datasets.tex}
\input{tables/linear_numbers.tex}
\input{tables/detection_tide_all.tex}

