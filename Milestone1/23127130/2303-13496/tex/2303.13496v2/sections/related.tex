\section{Related Work}
\label{sec:related}


{\noindent \bf Supervised pretraining} of transferrable representations on large labeled datasets~\cite{deng2009imagenet,kay2017kinetics,ridnik2021imagenet} and employing them for downstream recognition tasks, has emerged as a powerful approach in computer vision.
It has spurred rapid progress on various tasks including image classification~\cite{donahue2014decaf,esc,razavian2014features}, object detection/segmentation~\cite{girshick2013rcnn,ren2015faster}, image captioning~\cite{li2020oscar,yu2022coca} and video action recognition~\cite{simonyan2014two,carreira2017quo,fan2021multiscale}.
While useful, such representations are often limited by the scale and diversity of the supervision in the pretraining datasets. Hence,
recent work has probed the effectiveness, robustness, and fairness of these representations
~\cite{abnar2021exploring,kornblith2019better,de2019does,recht2019imagenet,shankar2021image,taori2020measuring,idrissi2022imagenet}.



{\noindent \bf Self-supervised pretraining} is a promising alternative to learn these representation without relying on large well-labeled datasets.
Initial works focused on reconstructions methods~\cite{vincent2010stacked} before moving to other pretraining tasks
such as solving jigsaw puzzles~\cite{noroozi2016unsupervised},
constrastive learning~\cite{he2019moco,chen2020simple} or joint embedding
approaches~\cite{grill2020bootstrap,caron2020unsupervised,caron2021emerging, zhou2021ibot,assran2022masked,assran2023self}. %
With the advent of Vision Transformers \cite{dosovitskiy2020image}, approaches based on reconstructions such as \cite{bao2021beit,xie2021simmim,he2021masked} got renewed interest for their simplicity and state of the art performance.
Of particular interest to us is MAE~\cite{he2021masked} for its state of the art performance on many transfer tasks~\cite{he2021masked,li2022exploring,girdhar2023omnimae,tong2022videomae,feichtenhofer2022masked} and its computational efficiency.
Given the lack of supervision during pretraining, these representations often require significant finetuning to align to downstream tasks.

{\bf \noindent Weakly supervised pretraining (\ce)} is a middle-ground between supervised and self-supervised pretraining. Instead of ignoring annotations completely as in self-supervised pretraining, or requiring exhaustive labels as in supervised pretraining, \ce
relies on the large quantity of ``free'' annotation available on the internet. These annotations occur as image-text
pairs~\cite{radford2021learning,schuhmann2022laion}, where the text can additionally be processed to produce pseudo labels. %
Of particular interest to us is the latter, \ie
approaches which leverage multi-label classification on noisy labels~\cite{zhai2022scaling,mahajan2018exploring,singh2022revisiting,ghadiyaram2019large}
which have shown state of the art fine-tuning performance, and at the same time can be adapted using image-text data
to gain zero-shot capabilities~\cite{zhai2022lit}.
In this work, we explore \ce in conjunction with self-supervised \prept, and show faster convergence and stronger performance.


