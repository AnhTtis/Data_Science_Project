\subsection{Transfer Evaluation}
\label{sec:results}

We compare with \sota research on image and video classification, detection and segmentation, low-shot image classification, zero shot 
transfer, robustness analysis.



\input{tables/low_and_zero_shot_classification.tex}
\input{tables/detection_segmentation_lvis}
\input{tables/detection_segmentation_coco}

{\bf \noindent \inetOneK image classification.}
\cref{tab:image_classification} shows the performance of different methods on \inetOneKShort.
\ours gets the best performance for a \vitH sized model (89.3\%).
As we report in the appendix, \ours also achieves the best performance for \vitL (88.8\%).
Recent methods such as \scaleViT~\cite{zhai2022scaling} are better on \inetOneKShort and we hypothesize that this gap stems mainly from the differences in the pretraining datasets (\igSizeShort \vs \jftThreeB).
We also compute linear performance using frozen features on \inetOneKShort at 224px resolution. Our models produce strong
features which outperform other methods with \ce objectives. They also surpass the performance of the self-supervised 
\dinovTwo~\cite{oquab2023dinov2} model optimized to produce strong frozen representations:

\begin{center}
    \resizebox{0.8\linewidth}{!}{
        \begin{tabu}{llcH|c}
            \bf Method & \bf Dataset & \bf Arch. & \bf Res. & \bf \inetOneKShort Linear \\
            \midrule
            \swag~\cite{singh2022revisiting} & IG-3.6B & \vitH & 224 & 85.8 \\
            \openclip~\cite{ilharco_gabriel_2021_5143773} & \laionTwoB & \vitG & 224 & 86.2 \\
            \dinovTwo~\cite{oquab2023dinov2} & LVD-142M & ViT-g & 224 & 86.5 \\
            \midrule
            \ours & \igSizeShort & \vitH & 224 & 87.0 \\
            \ours & \igSizeShort & \vitTwoB & 224 & \bf 88.1 \\ 
    

        \end{tabu}
    }
\end{center}


{\bf \noindent Robustness for image classification.}
We evaluate the robustness of our models finetuned on \inetOneKShort on
additional test sets in~\cref{tab:image_classification} and notice that, despite \ours being $\sim$1\% behind \scaleViT on \inetOneKShort, it is significantly
more robust and generalizes better
on these additional test sets -- \ours gets the highest reported performance on \inetReal and \objectNet for \inetOneKShort
finetuned models.

{\bf \noindent Generalization in image classification.}
We evaluate the generalization of our model on additional fine-grained image classification using \inat.
\inatShort is a challenging long-tailed and fine-grained dataset with images of multiple species of visually similar plants and animals.
Our \vitTwoB sets a new \sota result on \inatShort (+4.5\% over~\cite{he2021masked})


{\bf \noindent Video classification.}
In~\cref{tab:video_classification} we investigate how \ours's pretraining transfers to video action classification on \kineticsShort and \sthsthShort. \ours is competitive with \sota
methods, including ones that pretrain on videos, whereas our models are only pretrained on images. Specifically,
our \vitL gets the highest reported performance on both video datasets.
For all video finetuning, we use relative position 
embeddings~\cite{fan2021multiscale}, which improves our performance by 0.6\% on \kineticsShort for a \vitL.
Overall, the results indicate the promise of \mae \prept for building strong video understanding models.

{\bf \noindent Low-shot image classification.}
We evaluate the label efficiency of our models using a few examples per class for finetuning.
We use two datasets, \inetOneKShort and \inatShort, with $K$ shots (labeled examples per class), $K \in \{1, 5, 10\}$.
For \inatShort, as some classes have less than $K$ images, we adapt our setting to consider \textit{at most} $K$ shots.
For each value of $K$, we generate 5 splits of the original dataset using 5 different random seeds and report 
the mean top-1 accuracy.

We evaluate two protocols for low-shot finetuning -- linear classifiers and Adapters~\cite{houlsby2019parameter}, both 
of which keep the entire model parameters frozen and introduce a few trainable parameters.
We evaluated multiple Adapters proposed for ViTs -- LoRA~\cite{hu2022lora}, 
AdaptFormer~\cite{chen2022adaptformer}, and VPT~\cite{jia2022visual}. We found that VPT performed the best while being 
robust to the choice of hyperparameters, and outperforms linear classifiers for our models.
For other works, we report with the best protocol. Full details in \cref{app:transfer_details}.

\cref{tab:low_shot_image_classification} shows a comparison with \sota methods on low-shot \inetOneKShort and 
\inatShort, including foundational and self-supervised models.
Our models show impressive low-shot performance on both \inetOneKShort and \inatShort, reaching 83.7\% and 80.3\% 
top-1 accuracy with only 10 labeled examples per class, and reaching the highest reported performance with just one 
labeled example per class on \inetOneKShort of 62.1\%.

{\bf \noindent Zero-shot transfer.}
Strong foundational models are expected to also have a good open world understanding of visual concepts.
To equip our pretrained vision encoders with such capabilities, we utilize \lit~\cite{zhai2022lit}. 
We initialize the text encoder from an \xlmr Large~\cite{conneau2020unsupervised} model.
\cref{tab:zero_shot} shows the zero-shot
transfer performance of our models on \inetOneKShort, and \foodShort. Our \vitTwoB attains 81.9\% accuracy on 
\inetOneKShort, outperforming a similarly sized \openclip model. Our results lag behind other works which pretrain 
on datasets such as \jftThreeB and ALIGN, highlighting the importance of the pretraining dataset for performance. 
This data-advantage is also observed in the finetuning results discussed in \cref{tab:image_classification}, where 
\jftThreeB provides best \inetOneKShort accuracy. On \food we attain the highest zero-shot transfer accuracy of 96.2\%. 
The performance on the two datasets is not well correlated, further demonstrating the impact the pretraining 
dataset can have on a particular zero-shot task. 

{\bf \noindent Detection and segmentation.}
Next, we evaluate models on detection and instance segmentation, on the \lvisShort~\cite{gupta2019lvis} (\cref{tab:detection_segmentation_lvis}) 
and \cocoShort~\cite{lin2014microsoft} datasets (\cref{tab:detection_segmentation_coco}).
We use the Cascade Mask R-CNN framework~\cite{he2017maskrcnn}, with the \vitDet~\cite{li2022vitdet} architecture, and 
initialize the backbone with our pretrained models.
For finetuning our models, we start with the hyperparameters from~\cite{li2022vitdet} and adapt them for our models, 
full details are in~\cref{app:transfer_details}.
We also perform a system level comparison with other \sota works, but note that drawing meaningful conclusions out of this 
is difficult, on account
of the multiple differences in the detection frameworks, model architectures, and datasets.  

On both benchmarks, \ours considerably outperforms the weakly supervised \swag~\cite{singh2022revisiting} model, demonstrating the benefit of our 
additional \prept stage. On the long-tailed \lvisShort dataset, \ours outperforms \mae \inetOneKShort pretraining on detection 
AP~\cite{li2022vitdet}, 
but lags slightly behind on \cocoShort. \mae's strong performance on detection using both \inetOneKShort 
and \igSizeShort (\cref{fig:mae_scaling}) can potentially be explained by the fact that it is trained to reproduce images with 75\% 
masking, whereas \ce is only trained to predict one or a few salient objects in an image.
Lastly, methods which use additional detection data, such as \objectsThreeSixFive~\cite{shao2019objects365} or 
FLOD-9M~\cite{yuan2021florence}, have strong detection performance. 

\input{figures/ce_vs_maece_detection.tex}
\input{figures/detection_analysis_all.tex}
\noindent \textbf{Analyzing detection performance.}
We further inspect the benefits of \prept for detection in~\cref{fig:ce_vs_maece_detection}.
Unlike other tasks like image / video classification, scaling model size using \ce pretraining does \emph{not} improve 
detection performance.
However, adding \mae \prept provides consistent gains and allows \ce to scale with model size.

We dissect the performance of \vitL models trained with \mae, \ce, and \ours in \cref{fig:detection_analysis_all}
based on the size of the object, or by the frequency of the object's class.
We observe that \ours performs better than \ce on all tasks -- detecting rare to frequent classes across small to large object sizes.
It improves over \mae at detecting rare objects, presumably because of the diversity in the \igSizeShort labels.














