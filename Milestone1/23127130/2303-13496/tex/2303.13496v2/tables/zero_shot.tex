\begin{table}[H]
    \centering
    \setlength{\tabcolsep}{3pt}
    \resizebox{\linewidth}{!}{
    \begin{tabu}{llcHc|c|cH}
        \centering
        \bf Method & \bf Dataset & \bf Arch. & \bf Text Enc. & \bf Res. & \bf \inetOneKShort & \bf \foodShort & \bf \petsShort \\
        \midrule
        \multicolumn{6}{l}{\em Results with different pretraining datasets} \\
        \clip~\cite{radford2021learning} & \wit & ViT-L/14 & & 336 & 76.2 & 93.8 & 93.5  \\
        \openclip~\cite{ilharco_gabriel_2021_5143773} & \laionTwoB & \vitH & & 224 & 78.0 & 92.5 & 94.4 \\
        \openclip~\cite{ilharco_gabriel_2021_5143773} & \laionTwoB & \vitG & & 224 & 80.1 & 92.9 & 95.2 \\
        \florence~\cite{yuan2021florence} & \florenceDataset & \florenceModel &  & 384 & 83.7 & 95.1 & 95.9 \\
        \makecell[l]{\scaleViT~\cite{zhai2022scaling} \\ + \lit~\cite{zhai2022lit}} & \makecell[l]{\jftThreeB \\ + \alignThreeSixB} & \vitL & \vitH & 224 & 80.8 & -- & -- \\
        \makecell[l]{\scaleViT~\cite{zhai2022scaling} \\ + \lit~\cite{zhai2022lit}} & \makecell[l]{\jftThreeB \\ + \alignThreeSixB} & \vitg & \vitg & 288 & 85.2 & -- & --\\
        CoCa~\cite{yu2022coca} & \makecell[l]{\jftThreeB + \\ \alignOriginal} & \multicolumn{2}{c}{\CocaModel} & 576 & \textbf{86.3} & -- & --\\
        \midrule
        \ours & \igSizeShort & \vitH & ? & 224 & 80.8 & 95.8 & -- \fixme{!!} \\ %
        \ours & \igSizeShort & \vitTwoB & ? & 224 & 81.9 & \textbf{96.2} & -- \\ %
    \end{tabu}%
    }
    \caption{\textbf{Zero shot image classification results.} We evaluate zero-shot transfer on \inetOneKShort and \food. 
    Our models push the \sota on \foodShort, while being competitive on \inetOneKShort. 
    The best performing models on \inetOneKShort train on \jftThreeB and ALIGN, and the performance on the two datasets is not
    well correlated, exemplifying the impact of pretraining dataset choice on zero-shot transfer performance.
    }
    \label{tab:zero_shot}
\end{table}
