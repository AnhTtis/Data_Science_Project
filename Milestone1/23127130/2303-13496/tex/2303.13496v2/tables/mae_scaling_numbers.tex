\begin{table}[H]
    \centering
    \setlength{\tabcolsep}{3pt}
    \resizebox{\linewidth}{!}{
    \begin{tabu}{c|cc|cccc}
        \centering
        \bf Arch. & \bf \makecell[c]{\inetOneKShort \\ Top-1} & \bf \makecell[c]{\inatShort \\ Top-1} & \bf \makecell[c]{\lvisShort \\ \bf AP\textsuperscript{box}} & \bf \makecell[c]{\lvisShort \\ AP\textsuperscript{mask}} & \bf \makecell[c]{\cocoShort \\ \bf AP\textsuperscript{box}} & \bf \makecell[c]{\cocoShort \\ AP\textsuperscript{mask}} \\
        \midrule
        \multicolumn{4}{l}{\emph{\inetOneKShort pretraining}} \\
        \vitB & 83.5& 75.0 & 43.0& 38.9 & 54.0 & 46.7 \\
        \vitL & 86.0& 80.2 & 49.2& 44.5 & 57.6 & 50.0 \\
        \vitH & 86.9& 82.8 & \bf 51.5& \bf 46.6 & \bf 58.7 & \bf 51.0 \\
        \vitTwoB & \bf 87.4 & \bf 84.5 & --\textsuperscript{\textdagger} & --\textsuperscript{\textdagger} & --\textsuperscript{\textdagger} & --\textsuperscript{\textdagger} \\
        \midrule
        \multicolumn{4}{l}{\emph{\igSizeShort pretraining}} \\
        \vitB & 83.5 & 74.7 & 42.9 & 38.8 & 53.8 & 46.5 \\
        \vitL & 86.1 & 80.7 & 49.0 & 44.5 & 58.0 & 50.3 \\
        \vitH & 87.4 & 84.0 & 52.7 & 47.5 & 59.1 & 51.2 \\
        \vitTwoB & \bf 87.8 & \bf 85.6 & \bf 53.6 & \bf 48.6 & \bf 59.9 & \bf 52.0\\
    \end{tabu}%
    }
    \caption{
        \textbf{Scaling \mae with model and dataset size.} We show the performance in tabular form of \mae pretraining on \inetOneK 
        and \igSize, earlier shown in \cref{fig:mae_scaling}. \mae scales with both data and model size.
        \inetOneKShort and \inatShort finetuning results are at 224px resolution.
        \textsuperscript{\textdagger}Training was unstable.
    }
    \label{tab:mae_scaling_numbers}
\end{table}
