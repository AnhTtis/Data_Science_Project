%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.

\documentclass[sigconf,natbib=true,anonymous=false]{acmart}

% \usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{makecell}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{soul} % underline
\usepackage{tikz}
\usepackage{multirow}

\newcommand{\thai}[1]{{\color{blue}{[Thai: {\normalsize   #1}]}}}
\newcommand{\lee}[1]{{\color{red}{[Dongwon: {\normalsize   #1}]}}}
\newcommand\tab[1][0.5cm]{\hspace*{#1}}

%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


\newcommand{\name}{{\sf NoisyHate}}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{{\name}: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\newcommand\copyrighttext{%
  \footnotesize \textcolor{red}{\textbf{DISCLAIMER! THIS PAPER CONTAINS EXAMPLE TEXTS THAT ARE OFFENSIVE IN NATURE}}}
\newcommand\copyrightnotice{%
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south,yshift=20pt] at (current page.south) {{\copyrighttext}};
\end{tikzpicture}%
}

\author{Yiran Ye}
\email{yby5204@psu.edu}
\affiliation{%
  \institution{Penn State University}
  \city{University Park}
  \state{PA}
  \country{USA}
}

\author{Thai Le}
\email{thaile@olemiss.edu}
\affiliation{%
  \institution{University of Mississippi}
  \city{Oxford}
  \state{MS}
  \country{USA}
}

\author{Dongwon Lee}
\email{dongwon@psu.edu}
\affiliation{%
  \institution{Penn State University}
  \city{University Park}
  \state{PA}
  \country{USA}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

Online texts with toxic contents are a clear threat to the users on social media in particular and society in general.
%that might cause cyber harassment. 
Although many platforms have adopted various measures (e.g., machine learning based hate-speech detection systems) to diminish their effect,  toxic content writers have also attempted to evade such measures by using cleverly modified toxic words, so-called {\em human-written text perturbations}. Therefore, to help build AI-based detection to recognize those perturbations, prior methods have developed sophisticated techniques to generate diverse adversarial samples. However, we note that \ul{these algorithms-generated perturbations do not necessarily capture all the traits of human-written perturbations.}
%However, there is still a gap between those machine-generated perturbations and human-written perturbations. 
Therefore, in this paper, we introduce a benchmark test set of human-written perturbations, named as {\name}, created from real perturbations written by human users on various social platforms, for helping develop better toxic speech detection models. 
%We also recruited a group of workers to evaluate the quality of this test set and dropped low-quality samples. 
Meanwhile, to check if our perturbation can be normalized to its clean version, we applied spell corrector algorithms on this dataset. Finally, we test this data on state-of-the-art language models, such as BERT and RoBERTa, and black box APIs, such as Perspective API, to demonstrate the adversarial attack with real human-written perturbations is still effective.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002978.10003029.10003032</concept_id>
       <concept_desc>Security and privacy~Social aspects of security and privacy</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010179.10010186</concept_id>
       <concept_desc>Computing methodologies~Language resources</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003260.10003261.10003376</concept_id>
       <concept_desc>Information systems~Social tagging</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Security and privacy~Social aspects of security and privacy}
\ccsdesc[300]{Computing methodologies~Language resources}
\ccsdesc[500]{Information systems~Social tagging}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{toxicity detection, social media, adversarial attack, crowd-sourcing}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
\copyrightnotice

\section{Introduction}\label{sec:intro}

Hate speech, a conscious and willful public statement intended to defame a group of people, is always a threat on social media platforms that might cause cyberbullying. Many popular social media platforms, such as Twitter and Reddit, published specific hateful conduct policies and applied machine learning algorithms for hate speech detection to diminish its effects. Meanwhile, they also provide advanced muting options so that users can block the words they do not intend to see, such as those provided by Facebook and Twitter. However, trolls sometimes use another spelling of a potentially offensive word, such as text perturbation, to evade such hate speech detection systems. At the same time, a human being can still understand the meaning. These text perturbations have been produced in lots of different ways. One might use \textit{visually similar} characters to replace the original alphabetical characters. For example, ``ni66er" is commonly used to perturb the word ``nigger". Another perturbation strategy trolls are likely to use \textit{repeating or removing} certain characters in a word, e.g., ``bitch"$\rightarrow$``bitttch," ``stupid"$\rightarrow$``stupd." Other approaches to produce text perturbations include \textit{placeholder} (``shit"$\rightarrow$``sh\_t"), \textit{lower-upper-case}(``democrats"$\rightarrow$``democRATs"), and the combination from any of the above approaches. These various text perturbations could be a problem that nullifies the effects of safeguard machine learning algorithms.

Existing works have developed many frameworks to automatically generate text perturbations to benchmark the robustness of machine learning algorithms~\cite{zeng2020openattack,morris2020textattack}. These frameworks often borrow several adversarial attack algorithms from NLP literature~(\cite{yuan2019adversarial,alzantot2018generating,li2018textbugger,zang2020word,li2020bert,gao2018black,ren2019generating,garg2020bae}). However, there is still a gap between those machine-generated perturbations and human-written perturbations~\cite{le2022perturbations}. Instead of augmenting datasets with machine-generated perturbations and using them for evaluating the robustness of machine learning algorithms, we suggest augmenting those datasets with human-written perturbations. Evaluating machine learning models using datasets augmented with human-written perturbations is more practical as it better reflects real-life scenarios. Hence, this paper proposes a benchmark dataset for the toxic speech detection task that contains diverse human-written perturbations online. The contributions of our work can be summarized as follows:

\renewcommand{\tabcolsep}{3pt}
\begin{table*}[hbt!]
    \centering
    \footnotesize
    \begin{tabular}{lccc}
        \toprule
        \multirow{2}{*}{\textbf{Original Sentence}} & \multirow{2}{*}{\textbf{Generated Adversarial Sample}} & \textbf{Cited Perspective} & \textbf{Perspective} \\
        {} & {} & \textbf{API score} & \textbf{API score in 2022} \\
        \cmidrule(lr){1-4}
        I think the 1 million sales is total \textbf{bullshit} though. & \makecell[l]{ I think the 1 million sales is total \textbf{bullshkt} though.} & 0.79 → 0.07 \cite{gil2019white} & 0.55 \\
        \cmidrule(lr){1-4}
        \makecell[l]{Climate change is happening and it’s not changing \\ in our favor. If you think differently you’re an \textbf{idiot}} & \makecell[l]{Climate change is happening and it’s not changing \\ in our favor. If you think differently you’re an \textbf{idiiot}} & 0.84 → 0.20 \cite{hosseini2017deceiving} & 0.88 \\
        \cmidrule(lr){1-4}
        Anyone who voted for Trump is a \textbf{moron}  & \makecell[l]{ Anyone who voted for Trump is a \textbf{mo.ron}} & 0.80 → 0.13 \cite{hosseini2017deceiving} & 0.86 \\
        \cmidrule(lr){1-4}
        \makecell[l]{The irony of the crowd booing the women until \textbf{corbyn} \\ gives them permission to cheer. Talk about deluded. Are \\ the corbyn supporters really this stupid.} & \makecell[l]{The irony of the crowd booing the women until \textbf{čör$\beta$yn} \\ gives them permission to cheer talk about deluded Are \\ the corbyn supporters really this stupid. } & 0.92 → 0.27 \cite{brown2019acoustic} & 0.74 \\
        \bottomrule
    \end{tabular}
    \caption{Comparison between old Perspective API score and current Perspective API score towards the same attack}
    \vspace{-20pt}
    \label{tab:perspectiveattack}
\end{table*}

\begin{itemize}
  \item We introduce a novel benchmark test set, {\name}, with online human-written perturbations for toxic speech detection models. This dataset is derived from the popular Jigsaw dataset~\footnote{\url{https://jigsaw.google.com/}} with toxicity labels and identity annotations.
  \item We test {\name} dataset with several spell checkers and show that it is worth developing a better normalization tool targeting these online human-written perturbations.
  \item Our evaluation with state-of-the-art (SOTA) language models and the commercial toxic detection \textit{Perspective API}~\footnote{\url{https://perspectiveapi.com/}} on {\name} reveals that there are still room for improving the robustness of these models on predicting texts with human-written perturbations.
\end{itemize}




\section{Related Works}

We briefly go through two research areas closely related to our works: text perturbation generation algorithms and toxic speech detection.

\subsection{Text Perturbation Generation}
\subsubsection{Machine Generated Text Perturbations}



In literature, two major approaches are used to generate adversarial text sample: spelling modification \cite{bhalerao2022data, li2018textbugger, gao2018black, eger2019text, hosseini2017deceiving} and close words substitution \cite{alzantot2018generating, ribeiro2018semantically, sato2018interpretable, jin2020bert}. The spelling modification approach usually involves deleting, inserting, swapping, and replacing certain characters in a word. Bhalerao1 et al. \cite{bhalerao2022data} proposed a tool named Continuous Word2Vec (CW2V) to perturb text with the following rules: Fake punctuation (``like"$\rightarrow$``l.i.k.e"), Neighboring key (``like"$\rightarrow$``lime"), Random spaces (``like"$\rightarrow$``l ike"), Transposition (``share"$\rightarrow$``sahre"), and Vowel repetition and deletion (``like"$\rightarrow$``likee"). Other than changing the word's spelling directly, the second approach aims to replace the entire word with other regular English words to attack text classification models. Ribeiro et al.'s work \cite{ribeiro2018semantically} demonstrated the effectiveness of the attack with semantically equivalent adversarial rules (SEARs) on machine comprehension, visual question answering, and sentiment analysis tasks. SEARs are simple universal replacement rules intending to convert the target word into its semantically identical pairs (``what"$\rightarrow$``which", ``what is"$\rightarrow$``what's"). Alzantot et al. \cite{alzantot2018generating} also introduced an adversarial attach method that replaces the target word with its top k nearest neighbors based on the distance in the GloVe embedding space. To make the perturbation types more diverse, Li et al. \cite{li2018textbugger} developed TEXTBUGGER that applied both the spelling modification and close word substitution approaches. Nevertheless, experiment \cite{le2022perturbations} involving human evaluation reveals that the distance between the perturbations generated by TEXTBUGGER and real Human-written Text Perturbations still exists. Moreover, since these adversarial samples are produced based on some known vulnerabilities of a target model, how well they can help the hate-speech detection model prevent real-world attack are yet to be explored.


\subsubsection{Human-written Text Perturbations}

\textsc{CrypText}~\cite{le2023cryptext} is a platform that retrievals human-written perturbations directly from social media, such as Reddit, and provides visualization on the trend of those perturbations. Meanwhile, it also offers an interface to perturb the user-inputted sentences randomly. Due to this Randomness, some insignificant words might be perturbed occasionally. For example, given a sentence, ``I hate those stupid vegans", a perturbation on ``I" or ``those" might be insufficient to help this sentence evade the hate speech detection system. Moreover, the \textsc{CrypText} is using the \textsc{Anthro} algorithm~\cite{le2022perturbations} to cluster the words and their perturbations in social media based on their sound and spelling composition (e.g., leading characters, vowels and consonants, and visually similar characters). Therefore, some different standard English words with the same pronunciation, such as ``maim" and ``mam," will be treated as each other's perturbation. Hence, the randomly picked perturbation might not fit the context well. 



\subsection{Toxic Speech Detection}

Barbieri et al. \cite{barbieri2020tweeteval} applied state-of-the-art models, such as BERT, RoBERTa, LSTM, and SVM, on the TweetEval data set and reported that the best Macro F1 score on the test set is 0.829 (RoBERTa-Retrained) for offensive speech classification. Mathew et al. \cite{mathew2021hatexplain} introduced another dataset named HateXplain, and also trained language models, including BERT and BiRNN, on this dataset. According to their report, BERT demonstrated the best Macro F1 result, 0.687. Perspective API \cite{perspectiveapi}, created by Jigsaw and Google team, is one of the most famous black box toxic content detection systems. According to their website, their model was trained on millions of comments from various sources, including online forums such as Wikipedia and The New York Times, across various languages. Researchers also studied adversarial attacks targeting Perspective API \cite{jain2018adversarial, hosseini2017deceiving, gil2019white, brown2019acoustic}. However, as the version iterates, Perspective API developed defensive strategies to thwart these machine-generated attacks. Table \ref{tab:perspectiveattack} presents the outcomes of today's Perspective API when facing previous effective attacks. It is still worthwhile to investigate the performance of these models when subjected to human-written perturbations.


\begin{figure*}
\centerline{\includegraphics[width=1.0\textwidth]{process.png}}
\caption{Overall curation pipeline of {\name} dataset. This pipeline has three steps: (1) Data sourcing and cleaning from the original Jigsaw dataset (Section \ref{sec:step1}), (2) Sentence perturbation with human-written perturbations via pseudo-random sampling (Section \ref{sec:step2}) and (3) Human evaluation via crowd-sourcing to validate the quality of the perturbed sentences (Section \ref{sec:step3})}
\label{fig:pipeline}
\vspace{10pt}
\end{figure*}


% and in this paper we use five classes to cluster different perturbation types based on the method they were used to be produced: repeat\_char, abbr, special\_char, lower\_upper\_case, and interesting\_lower\_upper\_case. 


\section{{\name} Dataset}

\subsection{Overview and Usage}
This section introduces the transformation process of our dataset from the Jigsaw dataset's original texts to its current version. This transformation process includes three consecutive steps, namely (1) data pre-processing, (2) sentence perturbation using human-written perturbations, and (3) human evaluation via crowd-sourcing. Figure \ref{fig:pipeline} reveals the overall pipeline of this procedure. Our data and the source code for all the following steps can be found at our repository page~\footnote{\url{https://github.com/YiranYe/toxic-detection-testset}}. One can also use programming scripts to access our data through Hugging Face's dataset repository~\footnote{\url{https://huggingface.co/datasets}}. Figure \ref{fig:python_code} demonstrates the code snippets to retrieve and load the {\name} into a table format using Python programming. This semi-automatic pipeline will also provide other researchers with resources such as user-study designs and interfaces to curate benchmark datasets with human-written perturbations in domains other than toxic text detection.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{code.png}
  \caption{Python code for loading the {\name} datasets into a table using the Hugging Face API}
\label{fig:python_code}
% \vspace{-5pt}
\end{figure}

\subsection{Data Source and Cleaning}\label{sec:step1}
Jigsaw is a famous toxic speech classification dataset containing approximately 2 million public comments from the Civil Comments platform. In addition to the toxic score labels for toxicity classification, the Jigsaw dataset also provides several toxicity sub-type dimensions which indicate particular comment's target groups, such as male, female, black, and Asian. Due to these prolific identity annotations and significant data volume, we adopt this dataset as our raw data source. Since the dataset has been used as the standard benchmark dataset for content moderation tasks, this adoption will also help reduce the entry barrier in adopting {\name} from the community.

Since the comments from the Jigsaw dataset contain a lot of special characters, emojis, and informal languages, data cleaning was necessary to ensure data quality. Following a typical text processing pipeline, we removed duplicated texts, special characters, special punctuation, hyperlinks, and numbers. Since we only focused on English texts, sentences containing non-standard English words were filtered out. 13,1982 texts remained after this step.

\begin{algorithm}[t]
	\caption{Perturbing Process}
	\begin{algorithmic}[1]
	    \Require Clean Sentence $S_{clean} = [w_{1},w_{2},...,w_{n}]$, 
     Bert model $B$, 
     Perturbations Generated by the ANTHOR Algorithm $Dict:(w,[p_{1},p_{2},...,p_{t}])$, 
     Output Size $k$, Threshold $\Theta$
	    \Ensure Perturbed Sentence list $\mathbb{S}_{pert}=[S_{1},S_{2},...,S_{k}]$
	    \State $\mathbb{S}_{pert}=[]$
		\State $Score_{origin}=B(S_{clean})$ 
  %\Comment Calculate the original toxic score 
		\For {$w_{i}$ in $S_{clean}$}
    		\State $Score_{mask_{i}}=B([w_{1},w_{2},...,w_{i-1},w_{i+1},...,w_{n}])$ 
      %\Comment Calculate the toxic score with mask on $w_{i}$
    		\If {$abs(Score_{origin}-Score_{mask_{i}})>\Theta$}
        		\For {$p_{j}$ in $random.sample(Dict_{hard}[w_{i}],k)$}
        		    \State $\mathbb{S}_{pert}$ += $[w_{1},w_{2},...,w_{i-1},p_{j},w_{i+1},...,w_{n}]$
        		\EndFor
    		\EndIf
    		
		\EndFor
	\end{algorithmic} 
	% he is a fucking gay -> [he is a fucking, he is a gay]
	%sampling a subset such that one perturbation do not appear too many times -> decrease theta
	%sampling with probability as the abs in score difference -> so if the difference in score is high => more chance to be selected
	%np.random.sample(list, p=XXX)
	\label{alg:perturb}
\end{algorithm} 

\subsection{Sentence Perturbation}\label{sec:step2}
In this step, we aim to perturb each of the texts resulting from the previous step with human-written perturbations. Algorithm \ref{alg:perturb} is the pseudo-code of this process. Since we focus on benchmarking toxic text detection tasks, it is practical and meaningful to perturb only a few critical words within a sentence. To do this, we first train a proxy toxic detection model and utilize it to approximate the importance of each word to toxicity detection. In particular, we first fine-tune a BERT model~\cite{devlin2018bert} on the Jigsaw dataset. Then, we enumerate and mask every word in each sentence and observe how much confidence of the trained model changes after the such masking operation (Algorithm \ref{alg:perturb}, Line 4--Line 9). A candidate word is selected to be perturbed at every enumeration step if masking decreases the proxy model's confidence more than a pre-determined threshold (Algorithm \ref{alg:perturb}, Line 5). 

A sentence might have more than one crucial words. However, perturbing all of them will increase the chance that the perturbed sentence will be discarded in the crowd-sourcing step. This can happen because even one poorly perturbed word can lead to the discard of the whole sentence by the crowd-sourced workers. Thus, to maximize the number of sentences that remained at the end, we take a conservation measure and only perturb the most important word in a given sentence. Then, we utilize the \textsc{Anthro} algorithm~\cite{le2022perturbations}, which applies a customized version of the Soundex algorithm that can provide the cluster of online human-written perturbations for a a given word, to retrieve the perturbations from social media on the selected important words. 

Different types of human-written perturbations exist according to the perturbation strategies that curate them. To categorize all the perturbations used in this step, we classify them into give different perturbation strategies. Table \ref{tab:types} presents the definition of those types and examples. They are (1) repeating characters (RepeatChar), (2) using abbreviation by removing one or several characters (Abbr), (3) using non-English or special characters (SpecialChar), (4) using mixed cased characters (MixedCase) and (5) using mixed cased characters with an additional layer of meaning (e.g., ``republicans"$\rightarrow$``repubLIEcans") (MixedCase+). Due to the imbalanced frequency distribution among different perturbation strategies, \textsc{Anthro} algorithm~\cite{le2022perturbations} is biased to a specific type of perturbation such as RepeatChar. Hence, To increase the diversity of different perturbation types in {\name} dataset, we apply a pseudo-random strategy to give a higher chance for perturbation types with less frequency to be sampled. We utilized this procedure with ten random seeds and chose the best distribution entropy. This step results in 2,120 sentences.


\renewcommand{\tabcolsep}{1.5pt}
\begin{table}
    \centering
    % \footnotesize
    \begin{tabular}{lcr}
        \toprule
        \textbf{Type} & \textbf{Description} & \textbf{Example} \\
        \cmidrule(lr){1-3}
        RepeatChar & \makecell[l]{ 
            repeat several characters
            } & stupid{$\rightarrow$}stuppppid \\
        \cmidrule(lr){1-3}
        Abbr & \makecell[l]{ 
            delete several characters
            } & stupid{$\rightarrow$}stupd\\
        \cmidrule(lr){1-3}
        SpecialChar & \makecell[l]{ 
            replace several characters\\ 
            with Non-English characters
            } & \makecell[r]{stupid{$\rightarrow$}5tupid \\ stupid{$\rightarrow$}st*pid}\\
        \cmidrule(lr){1-3}
        MixedCase & \makecell[l]{ 
            replace several characters\\ 
            with their upper cases
            } & stupid{$\rightarrow$}sTuPId\\
        \cmidrule(lr){1-3}
        MixedCase+ & \makecell[l]{ 
            replace several characters\\ 
            with their upper cases, while \\
            these characters can be \\
            combined into a new word
            } & stupid{$\rightarrow$}stuPiD\\
        \bottomrule
    \end{tabular}
    \caption{We categorize all human-written perturbations to five different groups according to five perturbation strategies that curate them.}
    \vspace{-20pt}
    \label{tab:types}
\end{table}
% \begin{figure}
% \centerline{\includegraphics[width=\linewidth]{figs/Perturbation Type Frequency Pie Chart Before.png}}
% \caption{Perturbation Type Frequency Pie Chart Before Human Evaluation}
% \label{fig:type_dist_before}
% \vspace{-5pt}
% \end{figure}

\begin{figure}[hbt!]
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{distribution.png}
         \caption{Percentage of perturbation remained across all five perturbation types.}
         \label{fig:a}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{distribution2.png}
         \caption{Perturbation type distribution}
         \label{fig:b}
     \end{subfigure}
        \caption{Comparison of distribution of different perturbation categories \textit{before and after} validated by human workers (Section \ref{sec:step3}). ``Raw" refers to the original data we send to MTurk workers, and ``preserved" refers to the number/percentage saved after human evaluation.}
        \label{fig:type_dist}
\end{figure}

\renewcommand{\tabcolsep}{5pt}
\begin{table*}[hbt!]
    \centering
    \begin{tabular}{lrrrrrr}
        \toprule
        \textbf{Spell Corrector} & \textbf{RepeatChar} & \textbf{Abbr} & \textbf{SpecialChar} & \textbf{MixedCase} & \textbf{MixedCase+} & \textbf{Overall Accuracy} \\
        \cmidrule(lrrrrrr){1-7}
        Google SerpApi & \textbf{0.698} & \textbf{0.751} & 0.322 & 0.934 & 0.954 & \textbf{0.755}\\
        Bing API       & 0.406 & 0.533 & 0.426 & 0.943 & \textbf{0.963} & 0.633\\
        pyspellchecker & 0.544 & 0.480 & \textbf{0.809} & \textbf{0.948} & 0.954 & 0.728 \\
        NeuSpell       & 0.294 & 0.550 & 0.448 & 0.867 & 0.931 & 0.583 \\
        \bottomrule
    \end{tabular}
    \caption{Perturbation Normalization Accuracy}
    \label{tab:spellcorrector}
\end{table*}

\subsection{Human Evaluation}\label{sec:step3}
With the dataset of perturbed sentences prepared in Section \ref{sec:step2}, we now proceed to the human evaluation step that involves Amazon Mechanical Turk (MTurk) workers in judging the generated perturbation's quality. This step is necessary to ensure that the select perturbations in the previous step (Section \ref{sec:step2}) are appropriate to the sentences' contexts. Before MTurk workers work on their tasks, we provide brief training to the workers. To do this, we display a guideline in Table \ref{tab:instruction} (Appendix) to explain the definition of human-generated perturbation and provide examples of both high-quality and low-quality perturbations. This training phase has been suggested to warrant high-quality responses from the human worker, especially for labeling tasks~\cite{clark2021all}. Each MTurk worker is then presented with a pair of a perturbed sentences and its clean version and is asked to determine the quality of the perturbed one (Figure \ref{fig:interface}, Appendix).

We recruited five different workers from the North America region through five assignments to assess each pair. A five-second countdown timer was also set for each task to ensure workers spent enough time on it. To ensure the quality of their responses, we designed an attention question that asks them to click on the perturbed word in the given sentences before they provide their quality ratings (Figure \ref{fig:interface}, Appendix). Workers who cannot correctly identify the perturbation's location in the given sentence will be blocked for future batches. We aimed to pay the workers at an average rate of \$10 per hour, which is well above the federal minimum wage (\$7.25 per hour). The payment of each task was estimated by the average length of the sentences, which totals around 25 words per pair, and the average reading speed of native speakers is around 228 words per minute~\cite{trauzettel2012standardized}. 
% Hence, for each task, we paid $ \$10/(60*228/25) \approx \$0.02 $. 

By removing tasks that failed to identify the location of the perturbed words accurately, the data was reduced from 2120 to 1707. Subsequently, approximately 78.4\% (1339) of the remaining data were deemed high-quality perturbations and retained for further analysis. Figure \ref{fig:type_dist} presents the distribution of high-quality perturbations across five categories. MixedCase+ category (e.g., ``republicans"{$\rightarrow$}``repubLIEcans") had the highest retention rate, with 88.8\% of perturbed sentences retained. Conversely, MTurk workers discarded more than 30\% of perturbed sentences containing abbreviation perturbations or special characters—the outcome of our study aligned with our initial expectations. The generation of MixedCase+ perturbations typically requires human insights and understanding of outside contexts that are not accessible by machines, which then makes it more acceptable to humans. In contrast, both abbreviation-based perturbations (Abbr) and perturbations utilizing special characters (SpecialChar) result in character loss, making it more challenging for humans to associate the perturbed sentence with its clean version.

% \renewcommand{\tabcolsep}{5pt}
% \begin{table*}[tb]
%     \centering
%     \footnotesize
%     \begin{tabular}{lr}
%         \toprule
%         \textbf{Perturbation Type} & \textbf{Description} & \textbf{Example} \\
%         \cmidrule(lr){1-2}
%         repeat\_char & \makecell[l]{ 
%             Perturbation that is generated by repeating several characters\\ in its original clean version.
%             } & Stupid -> Stuppppid \\
%         \cmidrule(lr){1-2}
%         abbr & Perturbation that is generated by deleting several characters in its original clean version. & Stupid -> Stupd\\
%         \cmidrule(lr){1-2}
%         special\_char & Perturbation that is generated by deleting several characters in its original clean version. & Stupid -> Stupd \\
%         \cmidrule(lr){1-2}
%         lower\_upper\_case & Perturbation that is generated by deleting several characters in its original clean version. & Stupid -> Stupd \\
%         \cmidrule(lr){1-2}
%         interesting\_lower\_upper\_case & Perturbation that is generated by deleting several characters in its original clean version. & Stupid -> Stupd \\
%         \bottomrule
%     \end{tabular}
%     \caption{Perturbation Types}
%     \label{tab:example}
% \end{table*}


\section{Generalizability to other Tasks}

The workflow presented in this study can be applied to curate datasets of various NLP tasks. Specifically, this section will focus on two case studies, sentiment analysis, and machine translation, and briefly discuss the importance and feasibility of integrating the proposed pipeline into these tasks.

\begin{itemize}
  \item \textit{Sentiment Analysis.} Sentiment analysis uses models to identify the emotional tone, opinion, or attitude expressed in a given input sentence. Sentiment analysis is frequently utilized to monitor social media platforms to understand public opinion about a particular topic, brand, or event. However, since human-written perturbations are present on social media platforms such as Reddit~\cite{le2022perturbations}, the accuracy of a sentiment analysis model can be demised by such perturbations in real-world settings. To tackle this, our proposed pipeline can be extended to generate or synthesize difficult training examples to improve existing sentiment analysis models or benchmark their robustness.
  \item \textit{Machine Translation.} Machine Translation is a valuable tool that facilitates cross-cultural communication and enhances understanding on social media platforms. It enables non-native speakers to connect with individuals from diverse cultural backgrounds and gain insights into global issues and perspectives. Nonetheless, human-written perturbations can pose challenges for translators and contribute to language barriers for non-native speakers. This is particularly true when such individuals cannot relate the perturbation to its original word. Our pipeline can be extended to validate the performance of the machine translation model on noisy text.
\end{itemize}

The users can also use our pipeline to add human-written perturbations to another source dataset different from the Jigsaw dataset. First, the pipeline can be applied to clean the data and then use state-of-the-art language models to find the important words. Two approaches can then be employed to obtain related perturbations for these words. The first approach is to directly use the API provided by \textsc{CrypText} \cite{le2023cryptext}, which utilize \textsc{Anthro}~\cite{le2022perturbations} behind the scene. By inputting a given word into the API, it returns a set of human-written perturbations of that word. Alternatively, suppose we want to restrict the perturbations to only those in a selected corpus. In that case, we can implement the second approach, which clusters perturbations of words in the given corpus using the \textsc{Anthro} algorithm~\cite{le2022perturbations}. Finally, crowd-sourcing can be employed to filter out low-quality data using the provided user-study training examples and designs (Table \ref{tab:instruction}, Figure \ref{fig:interface}, Appendix)


\section{Baseline}
This section will demonstrate some of the baseline performances on {\name}'s dataset on two natural language processing tasks that correspond to two research questions.

\noindent \textbf{RQ. 1.} \textit{Can {\name}'s perturbed sentence be restored to its clean version by some normalization algorithms such as misspelling corrector?} Answers to this question can demonstrate the novelty of the proposed human-written perturbations.\\
\textbf{RQ. 2.} \textit{Can {\name}'s perturbed sentence effectively attack state-of-the-art language-model-based and commercial toxic detection models?} Answers to this question can demonstrate the potential impact of {\name} dataset on improving future toxic text detection models.

\subsection{Perturbations Normalization}

To answer the first question, we select one word-level spell corrector: pyspellchecker \cite{pyspellchecker}, one open source deep learning model-based spell corrector: NeuSpell \cite{jayanthi-etal-2020-neuspell}, and two commercial APIs: Google Search SerpApi \cite{googlespell} and Bing Spell Checker API \cite{bingspell}. We detailed these spell correctors below. 


\begin{figure*}[hbt!]
     \centering
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{accuracy1.png}
         \caption{Perspective API-2022 (AUC\_diff=0.0568)}
         \label{fig:pers}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{accuracy3.png}
         \caption{BERT-TweetEval (AUC\_diff=0.15133)}
         \label{fig:bert}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{accuracy2.png}
         \caption{RoBERTa-TweetEval (AUC\_diff=0.21178)}
         \label{fig:roberta}
     \end{subfigure}
        \caption{Model Accuracy vs Threshold Curve}
        \label{fig:understanding}
    \vspace{-10pt}
\end{figure*}

\begin{itemize}
  \item \textit{pyspellchecker.} pyspellchecker, the python package, is a word-level spell checker that returns the target word $w$'s most likely permutations within a predefined Levenshtein Distance. This likelihood for a candidate permutation $c$ is calculated based on the multiplication of its word frequency, $P(c)$, and the probability that the $w$ is typed when the author meant to type the candidate $c$, $P(w|c)$.
  \item \textit{NeuSpell.} NeuSpell is an open-source toolkit for sentence-level spelling correction. It contains ten language models, including CNN-LSTM, Nested-LSTM and BERT. We test all these models on our dataset and select the model with the best performance.
  \item \textit{Google Search SerpApi} Google Search provides ``Did you mean?" suggestions when a user input a sentence with spelling errors in its search bar. SerpApi extracts these suggestions and returns them as a spell-checker API.
  \item \textit{Bing Spell Checker API.} The Bing Spell Checker API utilizes statistical machine translation and machine learning algorithms to deliver precise and contextual corrections. According to their website, it can recognize and normalize slang, informal language, and different words with the same sounds (``see" and ``sea").
\end{itemize}

Since {\name}'s perturbed sentences contain lowercase and uppercase characters. At the same time, our clean set only has lowercase characters; some spell correctors might choose to retain the letter case (e.g., ``Stupiid" will be corrected as ``Stupid" instead of ``stupid"), a corrected word with uppercase characters might be treated as failure even if it has the same spelling as the clean word. In this case, we convert the corrected word into lowercase before comparison. Noticeably, this operation might reduce the toxicity of certain words (``democRATs"{$\rightarrow$}``democrats"). Table \ref{tab:spellcorrector} summarizes the accuracy of these spell checkers on {\name}. One of the commercial APIs, Google Search, presents the best result. However, the word-level spell corrector, pyspellchecker, has the second-highest accuracy. One possible explanation for this observation is that, for an inputted non-English word, the pyspellchecker always offered its best guess even if the calculated probability of typos was low, while other tools remained conservative with a small confidence value. 
% In practice, one sentence could contain more than one perturbation. 
In practice, these spell checkers might encounter more complex and novel perturbations. This result calls for continuous improvement of normalization tools targeting human-written perturbations online.

\subsection{Perturbations Understanding}

For \textbf{RQ. 2.}, we tested the BERT model~\cite{devlin2018bert}, RoBERTa model~\cite{liu2019roberta} and also the Perspective API on {\name} dataset. Then, we compared their performance when tested on clean and perturbed sentences. Since all of the data in our {\name} dataset are positive examples (i.e., toxicity$\geq$0.5 in the original Jigsaw dataset), the model's classification accuracy depends on a pre-defined decision threshold $t$. Typically, $t{=}k$ means that the input will be considered toxic if the confidence of a model's prediction for this data is larger than $k$, and vice versa. To better capture the performance differences among different threshold values, we plot the model accuracy vs. threshold curve as shown in Figure \ref{fig:understanding}.

According to Figure \ref{fig:understanding}, suppose we choose $t{=}0.5$; the RoBERTa-TweetEval model has the best performance ($Acc_{t{=}0.5}{=}0.915$) on the clean set while the Perspective API has the worst performance ($Acc_{t{=}0.5}{=}0.814$). However, the perspective API achieves the best overall robustness compared to other models, according to Figure \ref{fig:understanding}. In the perturbed dataset, the accuracy of the Perspective API's classification when $t{=}0.5$ is 0.672, it is much higher than the second best model, BERT model($Acc_{t{=}0.5}{=}0.557$).


\section{Conclusions and future work}

This work presents a novel benchmark dataset with annotated toxicity labels and identity information. This dataset consists of clean data and its corresponding perturbed version with online human-written text perturbations. It allows researchers to evaluate the effectiveness of their proposed toxic content detection models in the face of real-world human-generated attacks. Furthermore, the diverse types of perturbations in the dataset pose a challenge for perturbation normalization algorithms. However, the toxicity of a sentence can evolve. For instance, words that may have been considered neutral several decades ago, such as "retarded" may be perceived as toxic in contemporary times. Also, people might generate more interesting perturbations in the future. It depends on some events, online memes, and even TikTok videos. In order to address this issue, we aim to develop a model that can distinguish high-quality perturbations based on the annotated data we collected from Amazon MTurk. This model will enable us to automate the perturbation process, ensuring that the perturbations remain timely and relevant.

\appendix
\section*{Appendix}
Table \ref{tab:instruction} demonstrates the guideline and examples used for training MTurk workers for the human evaluation step (Section \ref{sec:step3}). Figure \ref{fig:interface} also illustrates the interactive user-study designs employed to validate the quality of perturbed sentences.

\renewcommand{\tabcolsep}{5pt}
\begin{table*}[h]
    \centering
    \footnotesize
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ll}
        \toprule
        \textbf{Section} & \textbf{Instruction} \\
        \cmidrule(lr){1-2}
        \textbf{Definition} & \makecell[l]{
                                \textbf{What is a human written text perturbation?}\\ 
                                A human written text perturbation is an alternative of a standard English word that is usually \\ 
                               invented by netizens. The purpose of a perturbation can be:\\ 
                               \tab 1. avoiding platforms’ filtering: ass \(\rightarrow\) a55; Russian \(\rightarrow\) Russ!an\\ 
                               \tab 2. visual attraction: bro \(\rightarrow\) brooooooo; porn \(\rightarrow\) pornnnnnn\\ 
                               \tab 3. memes: republican + lie \(\rightarrow\) repubLIEcan; democrat \(\rightarrow\) democRAT
                            } \\
        \cmidrule(lr){1-2}
        \textbf{Examples} & \makecell[l]{
                                \textbf{Perturbations that are \textcolor{green}{LIKELY} to be written by human:}\\ 
                                \tab Example 1:\\ 
                                \tab \tab Clean Sentence: keeping \ul{russian} invaders warm in these frigid temperatures\\ 
                                \tab \tab Perturbed Sentence: keeping \ul{russ!an} invaders warm in these frigid temperatures\\ 
                                \tab Example 2:\\ 
                                \tab \tab Clean Sentence: our \ul{country} is a cesspool\\ 
                                \tab \tab Perturbed Sentence: our \ul{countrrry} is a cesspool\\ 
                                \textbf{Perturbations that are \textcolor{red}{UNLIKELY} to be written by human:}\\ 
                                \tab Example 1:\\ 
                                \tab \tab Clean Sentence: \ul{damn} you beat me to it\\ 
                                \tab \tab Perturbed Sentence: \ul{dawn} you beat me to it\\ 
                                \tab Example 2:\\
                                \tab \tab Clean Sentence: they were pretty \ul{cool} to me when i was there\\ 
                                \tab \tab Perturbed Sentence: they were pretty \ul{coll} to me when i was there
                            } \\
        \bottomrule
    \end{tabular}
    }
    \caption{Guideline for MTurk Workers}
    \label{tab:instruction}
\end{table*}

\begin{figure*}[h]
\centerline{\includegraphics[width=\linewidth]{figs/mturk_ui.png}}
\caption{Human evaluation Interface: a clean-perturbed word pair will be highlighted when the worker moves the mouse cursor over one of them. By clicking the highlighted word, the worker commits that this is the identified clean-perturbed pair.}
\label{fig:interface}
\end{figure*}

% \clearpage
% \newpage
% \newpage
\clearpage
\clearpage
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{custom}

\end{document}
\endinput
%%

% 1. biased experiment with two models + figures
2. generalize ability [sentiment analysis + question answer]
% 3. python code to load our data
% 4. change figures, caption, color, figure 2&3 larger 
% 5. add appendix with highlight mouse over 
% 6. add clamier
%% End of file `sample-authordraft.tex'.
