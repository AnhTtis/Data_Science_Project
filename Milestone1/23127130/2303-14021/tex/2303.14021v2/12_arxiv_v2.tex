\documentclass[english, 10pt, a4paper]{article}
\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage{multirow}
\usepackage[hyphens]{url}
% Packages and macros go here
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage[toc,page]{appendix}
\usepackage{algorithmic}
\ifpdf
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi

% \usepackage{lmodern}
% \renewcommand{\familydefault}{\sfdefault}
% \usepackage{geometry}
% \geometry{
% left=40mm,
% top=30mm,
% right=40mm,
% bottom=30mm
% }

%%%% LINE SPACING
% \renewcommand{\baselinestretch}{1} 


% Prevent itemized lists from running into the left margin inside theorems and proofs
\usepackage{enumitem}
\setlist[enumerate]{leftmargin=.5in}
\setlist[itemize]{leftmargin=.5in}

% Add a serial/Oxford comma by default.
\newcommand{\autoreflastconjunction}{, and~}

\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry} % marges
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{mwe,tikz}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathabx}
\usepackage{color}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage[ruled]{algorithm2e}
\usepackage{subfigure}
%\usepackage{subcaption}
\usepackage{float}
\usepackage[most]{tcolorbox}
\usepackage{xr-hyper ,hyperref}
% \usepackage[shortlabels]{enumitem}
% \usepackage{bbm}
\usepackage{xspace}
\usepackage{regexpatch}
\usepackage{colortbl}%
% \usepackage{enumitem}
\usepackage{makecell}
\usepackage{microtype}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{calc}
%-------------------------------------------
% DEFINITIONS PRELIMINAIRES /  RACCOURCIS NOTATIONS
\usepackage{graphicx}

\def\XS{\xspace}
\DeclareMathAlphabet{\mathb}{OML}{cmm}{b}{it}
\def\sbm#1{\ensuremath{\mathb{#1}}}                % Style gras italique (necessite amsmath)           
\def\sbmm#1{\ensuremath{\boldsymbol{#1}}}          % Style gras italique (necessite amsmath)           
\def\sdm#1{\ensuremath{\mathrm{#1}}}               % Style droit en math
\def\sbv#1{\ensuremath{\mathbf{#1}}}               % Style gras droit
\def\scu#1{\ensuremath{\mathcal{#1\XS}}}           % Style cursif
\def\scb#1{\ensuremath{\boldsymbol{\mathcal{#1}}}} % Style gras cursif
\def\sbl#1{\ensuremath{\mathbbm{#1}}}              % Style blackboard (necessite bbm)

\newcommand{\N}{\mathbb{N}}		% N double barre (entiers)
\newcommand{\Z}{\mathbb{Z}}		% Z double barre (relatifs)
\newcommand{\Q}{\mathcal{Q}}		% Q double barre (rationnels)
\newcommand{\R}{\mathbb{R}}		% R double barre (réels)
\renewcommand{\P}{\mathbb{P}}		% P double barre (proba)
\newcommand{\E}{\mathbb{E}}		% E double barre (esperance)
\newcommand{\B}{\mathcal{B}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\var}{\mathbb{V}}		% V double barre (variance)
\newcommand{\C}{\mathbb{C}}		% C double barre (complexes)
\newcommand{\K}{\mathbb{K}}		% K double barre

\newcommand\innerprod[2]{\left\langle #1\,|\, #2 \right\rangle}
\newcommand{\fonction}[5]{\begin{array}[t]{lrcl}
#1: & #2 & \longrightarrow & #3 \\
    & #4 & \longmapsto & #5 \end{array}}	% fonction
\newcommand{\afonction}[4]{\begin{array}{ccc} #1 & \longrightarrow & #2 \\ #3 & \longmapsto & #4 \end{array}}		% fonction sans nom
\newcommand{\fonc}[3]{#1:  #2  \rightarrow  #3}					% fonction sans 2ème ligne
\newcommand{\syst}[1]{\left \{ \begin{array}{l} #1 \end{array} \right. \kern-\nulldelimiterspace}	% système
\newcommand{\prox}{\text{\normalfont prox}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\epsprox}{\varepsilon\text{\normalfont-prox}}
% \newcommand{\argmin}{\text{\normalfont argmin}}
% \newcommand{\argmax}{\text{\normalfont argmax}}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\dom}{\text{\normalfont dom}\,}
\newcommand{\graph}{\text{\normalfont graph}\,}
\newcommand{\epi}{\text{\normalfont epi}\,}
\newcommand{\cl}{\text{\normalfont cl}\,}
\newcommand{\ri}{\text{\normalfont ri}\,}
\newcommand{\aff}{\text{\normalfont aff}\,}
\newcommand{\inter}{\text{\normalfont int}\,}
\newcommand{\HRule}{\rule{\linewidth}{1.5mm}}
\newcommand{\minimize}[2]{\ensuremath{\underset{\substack{{#1}}}{\mathrm{minimize}}\;\;#2 }}
\newcommand{\maximize}[2]{\ensuremath{\underset{\substack{{#1}}}{\mathrm{maximize}}\;\;#2 }}
\newcommand{\argmind}[2]{\ensuremath{\underset{\substack{{#1}}}%
{\mathrm{argmin}}\;\;#2 }}
\newcommand{\proj}{\ensuremath{\text{\rm proj}}}
\newcommand\blue[1]{{\color{blue}#1}}
  \newcommand{\myrowcolour}{\rowcolor[gray]{0.925}}
\newcommand\ewa[1]{{\color{red}[Ewa: #1]}}
\newcommand\lele[1]{{\color{purple}[Lele: #1]}}
\newcommand\hung[1]{{\color{blue}[Hung: #1]}}
\newcommand\gio[1]{{\color{teal}[Gio: #1]}}
\newenvironment{mylist}{\begin{list}{$\rhd$}{}}{\end{list}}

% POUR LES ALGOS
\makeatletter
\newlength{\algorithmboxrule}
\setlength{\algorithmboxrule}{0pt}
\newcommand{\algorithmboxcolor}{white}
\xpatchcmd*{\algocf@caption@boxruled}{0.0pt}{2\algorithmboxrule}{}{}
\xpatchcmd*{\algocf@caption@boxruled}{\vrule}{\vrule width \algorithmboxrule}{}{}
\xpatchcmd{\algocf@caption@boxruled}{\hrule}{\hrule height \algorithmboxrule}{}{}
\xpretocmd{\algocf@caption@boxruled}{\color{\algorithmboxcolor}}{}{}
\setlength{\fboxsep}{\dimexpr\fboxsep+\fboxrule-\algorithmboxrule}
\setlength{\fboxrule}{\algorithmboxrule}
\makeatother

% ENVIRONNEMENTS
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newcommand{\assumptionautorefname}{Assumption}
\newcommand{\conditionautorefname}{Condition}
\newcommand{\propositionautorefname}{Proposition}
\newcommand{\corollaryautorefname}{Corollary}
\newcommand{\lemmaautorefname}{Lemma}
\newcommand{\definitionautorefname}{Definition}
\newcommand{\remarkautorefname}{Remark}
\newcommand{\exampleautorefname}{Example}
\renewcommand{\sectionautorefname}{Section}
% \newcommand{\algorithmautorefname}{Algorithm}
% \newcommand{\sectionautorefname}{Section}
\allowdisplaybreaks
\definecolor{shadecolor}{rgb}{0.8,0.8,0.8}

% Sets running headers as well as PDF title and authors

% Title. If the supplement option is on, then "Supplementary Material"
% is automatically inserted before the title.
\title{Forward-Backward algorithms for weakly convex problems}

% Authors: full names plus addresses.
% \title{{Inexact Forward-Backward algorithm for problems involving weakly convex functions}\thanks{Submitted to the editors DATE.
% \funding{European Union's Horizon 2020 research and innovation programme under the Marie SKLodowska-Curie grant agreement No 861137.}}}


\author{Ewa Bednarczuk\thanks{Warsaw University of Technology,  00-662 Warsaw, Koszykowa 75, Poland} \thanks{Systems Research Institute, PAS,  01-447 Warsaw, Newelska 6, Poland}
\and Giovanni Bruccola\footnotemark[2] \and Gabriele Scrivanti\thanks{Université Paris-Saclay, Inria, CentraleSupélec, CVN, 3 Rue Joliot Curie, 91190, Gif-Sur-Yvette, France.} 
\and The Hung Tran\footnotemark[2]}

\usepackage{amsopn}

\date{}
\begin{document}
\maketitle

\begin{abstract} 
We investigate the convergence properties of exact and inexact forward-backward algorithms for the minimisation of the sum of two weakly convex functions defined on a Hilbert space, where one has a Lipschitz-continuous gradient. We show that the exact forward-backward algorithm converges strongly to a global solution, provided that the objective function satisfies a sharpness condition. For the inexact forward-backward algorithm, the same assumptions ensure that the distance from the iterates to the solution set approaches a positive threshold depending on the accuracy level of the proximal computations. As an application of the considered setting, we provide numerical experiments related to feasibility problems involving a sphere and a closed convex set, source localisation and discrete tomography.
\end{abstract}
\vspace{0.4cm} 


\textbf{Keywords:} {
weakly convex functions, sharpness condition, forward-backward algorithm, inexact forward-backward algorithm, $\rho$-criticality, proximal subgradient, proximal operator}


% REQUIRED
\textbf{MSC codes:}
90C30, 90C26,  90C51, 65K10, 52A01


\section{Scope of this Work}
Given a Hilbert space $\X$, we consider a problem of the form
\begin{equation}
\label{eq: ourproblem0}
    \minimize{x\in\X}{F(x) + G(x)}
\end{equation}
where
  function $\fonc{F}{\X}{(-\infty,+\infty]}$ is weakly convex with modulus $\rho_F\geq 0$ ($\rho_F$-weakly convex) and function ${\fonc{G}{\X}{\R}}$ is {Fr\'echet} 
 differentiable with a $L_G$-Lipschitz continuous {(Fr\'echet) gradient $\nabla G$}. 
 
 From a theoretical viewpoint, weakly convex functions have been investigated on different levels of generality, \emph{e.g.\ } \cite{jourani1996Subdifferentiability, rolewicz1993globalization, vial1983strong}. In applications, weakly convex functions appear \emph{e.g.\ }in robust phase retrieval and matrix factorization. More examples can be found in  \cite{Davis2018SubgradientMethodsSharpWeaklyConvex}, where finite-dimensional instances of problem \eqref{eq: ourproblem0} with $F$ being convex are investigated together with a number of large-scale applications.
 
%Under these assumption, 
As function $G$ is  $L_G$-weakly convex (see \autoref{prop:suff-weak_convexity}), we can reformulate problem \eqref{eq: ourproblem0} as % \begin{equation}
% \label{eq: ourproblem2}
% \begin{split}
%     & \minimize{x\in\X}{f(x) -\frac{\rho_g}{2}\|x\|^2+\frac{\rho_g}{2}\|x\|^2+\ g(x)}\\
%     & \minimize{x\in\X}{(f(x) -\frac{\rho_g}{2}\|x\|^2)+(\frac{\rho_g}{2}\|x\|^2+\ g(x))}\\
%     & \minimize{x\in\X}{F(x) +G(x),\ \ \text{ }F(x):=f(x) -\frac{\rho_g}{2}\|x\|^2, G(x):=\frac{\rho_g}{2}\|x\|^2+\ g(x)}
% \end{split}
% \end{equation}
\begin{equation}
\label{eq: ourproblem}
\begin{split}
    %(\forall x \in \X)\quad \quad F(x) + G(x) = 
    %{f(x) -\frac{\rho_g}{2}\|x\|^2+\frac{\rho_g}{2}\|x\|^2+\ g(x)}\\
 \minimize{x\in\X}  \underbrace{ {\left(F(x) -\frac{L_G}{2}\|x\|^2\right)}}_{f(x)}+\underbrace{\left(\frac{L_G}{2}\|x\|^2+\ G(x)\right)}_{g(x)}
    %& \minimize{x\in\X}{F(x) +G(x),\ \ \text{ }F(x):=f(x) -\frac{\rho_g}{2}\|x\|^2, G(x):=\frac{\rho_g}{2}\|x\|^2+\ g(x)}
   % = f(x) +  g(x)
\end{split}
\end{equation}
where $\fonc{f}{\X}{(-\infty,+\infty]}$ is $\rho$-weakly convex  with $\rho=\left(\rho_F+L_G\right)$, $\fonc{ g}{\X}{\R}$ is convex and differentiable with a $L_g$-Lipschitz continuous gradient with $L_g=2L_G$, \emph{i.e.\ }the objective $f+g$ is $\rho$-weakly convex.

We apply Problem \eqref{eq: ourproblem} to model and solve the feasibility problem of finding the intersection of a closed convex set and a sphere in Hilbert spaces by the Forward-Backward (FB) iterative scheme defined in \eqref{eq: FB upate : 2} below. When the closed convex set is just a line, a non-convex Douglas-Rachford algorithm converges to the intersection of the line and the unit sphere, see
\cite{borwein2011douglas,benoist2015douglas}. Our experiments in 
 \autoref{sec:exact_exp} show that the forward-backward \eqref{eq: FB upate : 2} outperforms the Douglas-Rachford in finite $n$-dimensional space. 
We also consider more general feasibility problems, which appear in different applications such as source localization, \cite{beck2008exact}, phase retrieval, \cite{luke2019optimization}, and discrete tomography \cite{SCHULE2005_DiscreteTomography, Kadu2029ConvexBinaryTomography}. 
When the feasibility problem involves multiple spheres, an exact computation of the proximal step is not available and we employ an inexact version of the forward-backward in our numerical tests, see \autoref{sec:FBsep}.

 
{ 

The main scope of this paper is to analyse {the convergence properties of }the inexact forward-backward splitting {when applied to problem \eqref{eq: ourproblem}}, with step size $\alpha_t>0$} and accuracy level $\varepsilon_t\ge0$
    % \begin{equation}
    % \label{eq: FB upate}
    % x_{t+1} = \varepsilon_{t}\text{-}\prox_{\alpha_t f}(x_t - \alpha_t \nabla g(x_t))
    % \end{equation}
    % or
     \begin{equation}
    \label{eq: FB upate : 2}
    x_{t+1} \in \varepsilon_{t}\text{-}\prox_{\alpha_t f}(x_t - \alpha_t \nabla g(x_t)).
    \end{equation}
   
In the finite-dimensional setting, there exist algorithms which are devoted to solving problems that involve weakly convex functions, see \emph{e.g.\ }\cite{Atenas2023Unified, bayram2015convergence, davis2019stochastic,Davis2018SubgradientMethodsSharpWeaklyConvex}. {In particular, the authors in \cite{bayram2015convergence} analyse the convergence of the FB splitting under the assumption that the smooth function $g$ is $\rho$-strongly convex, implying that the overall objective $f+g$ is convex.}

{In the present paper, we 
exploit a sharpness condition (see \autoref{def:sharpness} below}) to investigate the strong convergence of the scheme   \eqref{eq: 
FB upate : 2} (see \autoref{prop:convergence_convex:new} and \autoref{theo:Et constant convergence}). There has been a rising number of works which use sharpness to discuss the global convergence of splitting algorithms, such as \cite{Li2021_WeaklyCOnvexOptimization,li2019incremental}. 
We take into account a possibly inexact proximal step for the weakly convex function. The exact proximal step is also considered in {\cite[Section 5]{bohm2021variable}} to infer a complexity bound for the exact forward-backward algorithm. {We use the concept of $\varepsilon$-proximal operator introduced in \cite{bednarczuk2022calculus}, see also \autoref{def:eps_prox} below. Its properties, expressed in terms of global proximal ${\varepsilon\text{-subdifferential}}$s, are investigated in {\cite[Proposition 4, Proposition 5]{bednarczuk2022calculus}}, where a comparison with other concepts of $\varepsilon$-proximal operator in the convex case is provided.
The concept of an inexact proximal operator and its computational tractability lie at the core of many proximal methods referring to fully convex case, }where both $F$ and $G$ are convex \cite{barre2022principled,Millan2019InexactProximal,  salzo2012inexact}.


\subsection{Related Works}
\phantom{ciao}\\
\paragraph{The FB Algorithm}The FB algorithm (or proximal gradient algorithm) belongs to the class of splitting methods \cite{han1988parallel,lions1979splitting} whose aim is to minimise the sum of a smooth function and a non-smooth one. 
%By taking a gradient step on the smooth function and the proximal step on the non-smooth one, it is only needed to access each function separately. 
This kind of algorithm has been well studied and understood in Hilbert spaces for the convex case \cite{han1988parallel, combettes2005signal}. 
Many variants of FB have been proposed recently \emph{e.g.\ }inertial FB \cite{bello2022fista,lorenz2015inertial,bonettini2023abstract}, to accelerate the algorithm, variable metric FB  \cite{combettes2014variable} to improve convergence, or FB in Banach spaces, \cite{guan2022forward}.


When the convexity hypothesis is relaxed, two main drawbacks arise. Firstly, the convergence to global minimiser cannot be easily guaranteed any longer. Secondly, the lack of convexity of the non-smooth function invalidates the fact that the corresponding proximal operator is single-valued.
 In particular, in \cite{Bonettini_2017}, a line search-based inexact proximal gradient scheme is designed for problems where function $G$ is not necessarily convex, whereas $F$ is convex and its proximal map can be approximated using the strategy proposed in \cite{salzo2012inexact}. Therefore, we highlight that in 
\autoref{alg:epsFB} we take the proximal step on the weakly convex function $f$, while the function $g$ is Fr\'echet differentiable with a $L_g$-Lipschitz continuous gradient and also convex, by the manipulation in \eqref{eq: ourproblem}.\\

\paragraph{Convergence under error bound conditions} In the general non-convex case, the seminal works \cite{attouch2013convergence, liu2017further} illustrate that the convergence to a local minimum can be ensured provided that the objective function satisfies the Kurdyka-{\L}ojasiewicz (KL) inequality \cite{Kurdyka1998GradientsOfFunctions,Lojasiewicz1963ProprieteTopologique,Lojasiewicz1993GeometrieSemiEtSousAnalytique}. 
 The attractiveness of the KL inequality comes from the fact that, in finite-dimensional spaces, it is satisfied by, among others, subanalytic and semialgebraic functions, which are wide classes covering both convex and non-convex functions appearing in most applications.
  Given its considerable impact on several fields of applied mathematics, the authors in \cite{bolte2010characterizations} proposed a characterisation of the KL property in a non-smooth and infinite dimensional setting.

%In \autoref{sec:sharpness}, we discuss the relations between sharpness and KL in general Hilbert spaces.
In  finite-dimensional 
settings, \cite{Bai2022Equivalence} shows the equivalence, for weakly convex functions, between local sharpness, KL inequality and subdifferential error bound around a given critical point. In a recent work \cite{Atenas2023Unified}, the authors assume the subdifferential error bound to obtain the convergence of a FB scheme for the sum of a convex and a smooth functions to a stationary point.
Meanwhile, in our setting of a general Hilbert space, we consider the sharpness condition with respect to the set of global minimisers of the sum of a weakly convex and a smooth function.\\ 


\paragraph{Proximal Subdifferential}  
     Focusing on the class of weakly convex functions allows us to adopt a specific form of the general Fréchet subdifferential, namely
     %which is known as 
     \textit{proximal subdifferential},  %There exists a vast literature devoted to proximal subdifferentials, 
     see {\em e.g.}\  in the finite-dimensional case, the monograph by Rockafellar and Wets \cite{RockWets1998Variational}, in Hilbert spaces the work by Bernard and Thibault \cite{Bernard2005_ProxRegular} and the book by Clarke \textit{et al.}\ \cite{clarke2008nonsmooth}. 
    In particular, we make use of the fact that weakly convex functions 
    %(among others) 
    enjoy a so-called \emph{globalisation property} \cite{jourani1996Subdifferentiability, rolewicz1993globalization}, which states that the proximal subgradient inequality holds globally (see \autoref{def:paraconvexity} and \autoref{def:eps:prox_subdiff}). 
    %Finally, we highlight that for $\rho$-weakly convex functions, the proximal subdifferential coincides with the Clarke subdifferential (see {\cite[Theorem 3.1]{jourani1996Subdifferentiability}} ).
    \\




\paragraph{Outline} The work is organised as follows.
In \autoref{sec:Preliminaries}, we provide some preliminary facts on weakly convex functions and proximal $\varepsilon$-subdifferentials. In \autoref{sec:inexact}, we show the relation between proximal operators and proximal subdifferentials of weakly convex functions. In \autoref{sec:sharpness}, we introduce the concept of sharpness and investigate its relation with critical points and with points satisfying the KL property for weakly convex functions. In 
    \autoref{sec:inexact FB convergence} we present in \autoref{alg:epsFB} the inexact FB algorithm and prove the descending behaviour of the objective values for iterates generated by \autoref{alg:epsFB}. In \autoref{sec:convergence:analysis}, we show the monotonic decrease of the distance from the iterates to the solution set in the case of inexact proximal computations (see \autoref{theo:Et constant convergence}) and, in \autoref{sec:convergence:exact}, we show the local strong convergence to a global solution of the whole sequence with linear rate in the case of exact proximal computations (see \autoref{prop:convergence_convex:new}).
In \autoref{sec:feasibility}, \autoref{sec:FBsep} and \autoref{sec:discrete_tomography}, we apply the considered setting to model several feasibility problems and report on numerical simulations for the proposed FB scheme in the context of source localisation and binary tomography problems.

\section{Preliminaries}\label{sec:Preliminaries}

 We start with the  definition of $\rho$-weak convexity (also known as $\rho$-paraconvexity, see \cite{jourani1996Subdifferentiability,Rolewicz1979paraconvex} or $\rho$-semiconvexity, see \cite{cannarsa2004semiconcave}).

\begin{definition}[\textit{$\rho$-weak convexity}]\label{def:paraconvexity}
Let $\mathcal{X}$ be a Hilbert space. A function $\fonc{h}{\mathcal{X}}{(-\infty,+\infty]}$ is 
%said to be 
$\rho$-weakly convex if there exists a constant $\rho\ge0$ such that for $\lambda\in[0,1]$ the following inequality holds:
\begin{flalign}
 \quad (\forall (x,y)\in \mathcal{X}^2) && h(\lambda x + (1-\lambda)y) \leq  \lambda h(x) + (1-\lambda) h(y) + \lambda(1-\lambda)\frac{\rho}{2}\|x-y\|^{2}. &&&
\end{flalign}
We refer to $\rho$ as the \textit{modulus of weak convexity} of the function $h$. 
\end{definition}
Weakly convex functions can be equivalently characterised in the following manner.
\begin{proposition}[{\cite[Proposition 1.1.3]{cannarsa2004semiconcave}}]\label{prop:cannarsa}
    Let $\mathcal{X}$ be a Hilbert space. Function $\fonc{h}{\mathcal{X}}{(-\infty,+\infty]}$ is $\rho$-weakly convex if and only if  $h + \frac{\rho}{2}\|\cdot\|^2$ is a convex function. 
\end{proposition}
\sloppy In analogy to the convex case, for any $\rho\ge 0$, we  use the notation $\Gamma_\rho (\X)$ to indicate the class of proper lower-semicontinuous $\rho$-weakly convex functions defined on $\X$ with values in $(-\infty, + \infty]$. In particular, $\Gamma_0 (\X)$ denotes  the class of proper lower-semicontinuous convex functions defined on $\X$ with values in $(-\infty, + \infty]$. Further characterisations of weak convexity can be found in \cite{bednarczuk2022calculus} and the references therein.

{\begin{example}\label{ex: weak convexity}
   \sloppy Let $\fonc{f}{\R}{(-\infty,+\infty)}$ be defined as $f(x): = |(x-a)(x-b)|$, where ${(a,b)\in\{(a,b)\in \R^2\,|\,a<b\}}$. Function $f$ is $\rho$-weakly convex with $\rho=2$  by 
 %virtue of 
 \autoref{prop:cannarsa}.
 %. As a matter of fact, 
 For every $x\in \R$, it is easy to verify that ${f(x)+ x^2 = \max\{2x^2 -(a+b)x + ab, (a+b)x - ab\}}$
% \begin{equation}
% \begin{aligned}
%      f(x)+ x^2 &= \begin{cases}
%         x^2 -(a+b)x + ab + x^2 &\text{\,\emph{if}\,\,} x\leq a \text{\,\emph{and}\,\,} x \geq b\\
%         -x^2 +(a+b)x - ab  + x^2 &\text{\,\emph{if}\,\,} a\leq x\leq b\\
%     \end{cases}\\
%        & = \max\{2x^2 -(a+b)x + ab, (a+b)x - ab\},
% \end{aligned}
% \end{equation}
\emph{i.e.\ }function $x\mapsto  f(x)+ x^2$ is the point-wise maximum of convex functions, hence it is itself convex. 
\end{example}
Function $f$ from \autoref{ex: weak convexity}, %is an example of a non-smooth weakly convex function. 
despite its simplicity,  
can be used 
%as a modelling function 
in various applications, {\em e.g.}  in feasibility problems and in
 variational problems as 
a penalty function to promote integer solutions, 
 see \autoref{sec:feasibility}, \autoref{sec:FBsep} and \autoref{sec:discrete_tomography} .\\



%The following proposition 
Below, we provide a characterisation of weak convexity for  Fr\'echet differentiable functions.
\begin{proposition} 
\label{prop:subdiff}
Let $\X$ be a Hilbert space and let $h:\X\rightarrow(-\infty,+\infty]$ be  Fr\'echet differentiable on a convex set $U\subset\X$. Then, the following are equivalent:
\begin{description}
\item \textit{(i)}  $h$ is $\rho$-weakly convex on $U$;
\item \textit{(ii)} for every $x\in U$
\begin{flalign} 
\label{eq:descent_ineq}
\quad (\forall\ y\in \X)&&h(y)\ge h(x)+\langle \nabla h(x)\ |\ y-x\rangle-\frac{\rho}{2}\|x-y\|^{2}. &&&
\end{flalign}
\end{description}
\end{proposition}
\begin{proof}
By \autoref{prop:cannarsa} and {\cite[Proposition 17.7]{Bauschke2017}}, for all $x\in U$
\begin{equation} 
	(i) \text{ holds}\begin{array}[t]{l} 
	\Leftrightarrow \ (\forall\ y\in \X) \quad h(y)+\frac{\rho}{2}\|y\|^{2}\ge h(x)+\frac{\rho}{2}\|x\|^{2}+\langle \nabla h(x)+\rho x\ |\ y-x\rangle\\
			\Leftrightarrow \ (\forall\ y\in \X)\quad h(y)\ge h(x)+\langle \nabla h(x)\ |\ y-x\rangle-\frac{\rho}{2}\|x\|^{2}+\langle \rho x\ |\ y\rangle-\frac{\rho}{2}\|y\|^{2}\\
			\Leftrightarrow \ (\forall\ y\in \X)\quad h(y)\ge h(x)+\langle \nabla h(x)\ |\ y-x\rangle-\frac{\rho}{2}\|y-x\|^{2}\\
				\Leftrightarrow\ (ii) \text{  holds}.
	\end{array}
\end{equation}
\end{proof}}
%We report 
The following result, 
%commonly 
known as \textit{Descent Lemma}, 
%that allows to 
provides sufficient conditions for weak convexity.
%holds for sufficiently regular functions defined on a Hilbert space.

\begin{lemma}[{\cite[Lemma 2.64]{Bauschke2017}}]\label{lem:descent_lemma:BC}Let $\X$ be a Hilbert space, let $U$ be a non-empty open convex subset of $\X$. Let $\fonc{h}{U}{\R}$ be a Fréchet differentiable function with  $\nabla h$ being  $L_h$-Lipschitz continuous on $U$. Let $x$ and $y$ be in $U$. The following holds.
\begin{enumerate}[label=(\roman*)]
\setlength{\itemindent}{+.3in}
\item $|h(x) - h(y) - \langle x-y|\nabla h(y)\rangle |\leq \frac{L_h}{2}\|x-y\|^2$.
    \item $|\langle x-y|\nabla h(x)-\nabla h(y)\rangle |\leq {L_h}\|x-y\|^2$.
\end{enumerate}
\end{lemma}
{The following proposition is an immediate consequence of the Descent Lemma and it proves that in problem \eqref{eq: ourproblem} we are minimizing the sum of two weakly convex functions.
\begin{proposition} 
\label{prop:suff-weak_convexity}
Let $\X$ be a Hilbert space.
%, let $U$ be a non-empty open convex subset of $\X$.
Let $\fonc{h}{\X}{\R}$ be a Fréchet differentiable function such that $\nabla h$ is $L_h$-Lipschitz continuous on $\X$. Then $h$ is $L_{h}$-weakly convex on $\X$.
\end{proposition}
\begin{proof}
    By Descent Lemma, for all $x,y\in\X$ 
    $$
    -\frac{L_{h}}{2}\|x-y\|^{2}\le h(y)-h(x)-\langle y-x\ |\ \nabla h(x)\rangle,
    $$
    which, by Proposition \autoref{prop:subdiff}, is equivalent to  $L_{h}$-weak convexity of $h$ on $\X$.
\end{proof}
%This proves that, in fact,  problem \eqref{eq: ourproblem}, is that of minimizing the sum of two weakly convex functions.\\
}
%In this work, we will consider $\rho$-weakly convex functions, which are proper lower-semicontinuous. 

\sloppy For a function ${\fonc{h}{\X}{(-\infty,+\infty]}}$, the effective domain, $\dom h$,  is defined as ${\dom h := \{x\in \X\,|\, h(x) < +\infty\}}$ and  we  say that $h$ is \textit{proper} whenever $\dom h \neq \emptyset $. For a set set-valued mapping $M: \X \rightrightarrows \X$, 
%we  use the notation $\dom M$ to indicate the set
% \begin{equation*}
   ${ \dom M := \{x\in \X\,|\, M(x) \neq \emptyset\}.}$
% \end{equation*}
We define the \textit{global proximal {$\varepsilon$-subdifferential}} as follows.

\begin{definition}[Global proximal ${\varepsilon\text{-subdifferential}}$]\label{def:eps:prox_subdiff}
 Let $\varepsilon\ge0$. Let $\mathcal{X}$ be a Hilbert space. The global proximal ${\varepsilon\text{-subdifferential}}$ of a function $\fonc{h}{\X}{(-\infty,+\infty]}$ at $x_{0}\in\dom\,h$ for $C \geq 0$ is defined as 
 \begin{equation}
 \label{eq:def_prox_subdiff_eps}
     \partial^{\,\varepsilon}_{C} h (x_0) = \left\{v\in\X\,|\,  h(x)-h(x_{0})\ge\langle v\ |\  x-x_{0}\rangle-C\|x-x_{0}\|^{2}-\varepsilon\ \ \forall x\in\X\right\}.
 \end{equation}
 For $\varepsilon=0$, we have
 \begin{equation}
\label{eq:def_prox_subdiff_0}
    \partial_{C} h(x_0) = \{v\in\mathcal{X} \,|\, h(x)\geq h(x_0) + \langle v\ |\ x-x_0\rangle  - C\|x-x_0\|^{2},\;\forall x\in \mathcal{X}\}.
\end{equation}

%In view of \eqref{eq:def_prox_subdiff_0}, $\partial_{0} h(x)$ denotes the subdifferential in the sense of convex analysis. 
The elements of $\partial^{\,\varepsilon}_{C} h(x)$ are called proximal $\varepsilon$-subgradients. %The notation $\partial^{\, \varepsilon} h(x)$ is used when the constant $C$ in \eqref{eq:def_prox_subdiff_eps} is inessential, \emph{\emph{i.e.\ }\ }$v\in\partial^{\, \varepsilon} h(x)$ means that there exists $C\geq 0$ such that $v\in\partial_{C}^{\, \varepsilon} h(x)$.\ewa{are we using this latter notation?} \lele{does not seem so}
\end{definition}

{By {\cite[Proposition 3.1]{jourani1996Subdifferentiability}}, for a function $h\in \Gamma_\rho(\X)$, the global proximal subdifferential $\partial_{{\rho}/{2}} h(x_0)$ defined by \eqref{eq:def_prox_subdiff_0} coincides with the set of local proximal subgradients which satisfy the proximal subgradient inequality
\eqref{eq:def_prox_subdiff_0}  locally in a neighbourhood of $x_{0}$. 
%Moreover, when $h$ is $\rho$-weakly convex, by {\cite[Theorem 3.1]{jourani1996Subdifferentiability}}, the proximal subdifferential $\partial_{\rho/2}h(x_{0})$ coincides with the Clarke subdifferential $\partial_{c}h(x_{0})$ (see also \cite{Atenas2023Unified}). 
An extensive study of local proximal subdifferentials in Hilbert spaces can be found in 
%Chapter 2 of the book by Clarke, Stern, Ledyaiev, Wolenski - non-smooth Analysis and Control Theory 
{\cite[Chapter 2]{clarke2008nonsmooth}} and in \cite{ Bernard2005_ProxRegular}.
}

% For any set set-valued mapping $M: \X \rightrightarrows \X$, we will use the notation $\dom M$ to indicate the set
% \begin{equation}
%     \dom M := \{x\in \X\,|\, M(x) \neq \emptyset\}.
% \end{equation}
% while for a function $\fonc{f}{\X}{(-\infty,+\infty]}$, the notation $\dom f$ will indicate the set
% \begin{equation}
%     \dom f := \{x\in \X\,|\, f(x) < +\infty\}
% \end{equation}
% \ewa{the domain of $f$  should be defined before subdifferential}


\begin{proposition}[{ \cite[Proposition 2]{bednarczuk2022calculus}}]
\sloppy Let $\X$ be a Hilbert space and $h\in\Gamma_{\rho}(\X)$ with $\rho\geq 0$. Then 

   ${\dom \partial_{\rho/2} h \subset \dom h}$
and for every $\varepsilon> 0$ {we have the equality ${\dom \partial_{\rho/2}^{\,\varepsilon} h = \dom h}$.}
\end{proposition}


\section{Proximal Operators}\label{sec:inexact}

\begin{definition}[Proximal Map]\label{def:prox} Let $\X$ be a Hilbert space and $\fonc{h}{\X}{(-\infty,+\infty]}$ be a proper $\rho$-weakly convex function on $\X$. For any $\alpha>0$, the set-valued mapping
$\prox_{\alpha h}:  \X  \rightrightarrows  \X $
defined as
\begin{flalign}\label{eq:proximal_map}
 \quad\left(\forall y \in \X \right) &&   \prox_{\alpha h}(y) := \underset{x\in \X}{\argmin}\left\{h(x) + \frac{1}{2\alpha}\|x-y\|^2\right\} &&
\end{flalign}
is called \textit{proximal map} of $h$ at $x$ with respect to parameter $\alpha$.
In general, $\dom \prox_{\alpha h}$ could be empty.
When $1/\alpha> \rho$, or when $h$ is bounded from below,   $\dom \prox_{\alpha h}:= \{x\in \X\,|\, \prox_{\alpha h}(x) \neq \emptyset\}=\X$. And, if $1/\alpha> \rho$,  $\prox_{\alpha h}$ is  single-valued.
\end{definition}

% \subsection{Inexact Proximal Operators}
%In practical contexts, 
It is not always possible to rely on a closed-form expression for the proximity operator of a function (we refer to the web page \cite{proxpage} for a list of examples where these explicit forms are available). 
  %A simple example is represented by the proximal operator of the sum of two or multiple functions. 
%In \cite{Pustelnik2017_ProxOfASum} and in the references therein, the authors provide sufficient conditions for the equality $\prox_{h_1+h_2} = \prox_{h_1}\circ \prox_{h_2}$ to hold in the convex case. 
In general, the computation of the proximal map is an independent optimisation problem.
 It follows that the proximal map might be known up to a certain accuracy only, and %in the framework of proximal splitting methods, 
 %it is important to carry out 
 in our convergence analysis we take this fact into account. 
 
 We recall the concepts of $\varepsilon$-solution for an optimisation problem and $\varepsilon$-proximal point.


\begin{definition}[$\varepsilon$-solution]\label{def:epssol} Let $\X$ be a Hilbert space and $\fonc{h}{\X}{(-\infty,+\infty]}$ be a proper function  bounded from below,  $\varepsilon\geq0$ The element $x_\varepsilon$ is  an $\varepsilon$-\textit{solution} to the minimisation problem
 \begin{equation}
     \minimize{x\in \X}{h(x)}
 \end{equation}
 if $h(x_\varepsilon) \leq h(x) + \varepsilon$ for all $x \in \X.$
 %the following condition is satisfied
 %\begin{flalign}
   %\quad \left(\forall x \in \X\right) && h(x_\varepsilon) \leq h(x) + \varepsilon. &&&
 %\end{flalign}
\end{definition}

\begin{definition}[$\varepsilon$-proximal point]\label{def:eps_prox} Let $\X$ be a Hilbert space, $h:\X\rightarrow(-\infty,+\infty]$ be proper and bounded from below, $y\in \X$ and $\alpha>0$, $\varepsilon \geq 0$. Any $\varepsilon$-solution, to the proximal minimisation problem 
\begin{equation} 
\label{eq:eps_prox}
\minimize{x\in\X}{ h(x)+\frac{1}{2\alpha}\|x-y\|^{2}}
\end{equation}
 is  an $\varepsilon$-proximal point for $h$ at $y$ with respect to $\alpha$.  The set of all $\varepsilon$-proximal points of $h$ at $y$ is denoted as
\begin{equation}
    \varepsilon\text{-}\prox_{\alpha h}(y) := \left\{x\in \X\,|\, x\  \text{is\;a\;}\varepsilon\text{-solution\;of\;\eqref{eq:eps_prox}} \right\}.
\end{equation}
\end{definition}
In the convex case, the above definition of an inexact proximal point coincides with the definition in {\cite[Equation (2.15)]{villa2013accelerated}} and the convergence analysis of an inexact forward-backward algorithm  {\cite[Equation (2.15)]{villa2013accelerated}} is proposed in {\cite[Appendix A]{villa2013accelerated}}.\\


%To proceed with our analysis, we consider the following notion of $\varepsilon$-critical point:

\begin{definition}[$\varepsilon$-critical point]\label{def:crit} Let $\X$ be a Hilbert space, ${\fonc{h}{\X}{ (-\infty,+\infty]}}$  be a proper function and $\varepsilon\ge0$, $C\geq 0$.
 A point  $x\in\dom \partial _C^{\,\varepsilon} h $ is  a $C\text{-}\varepsilon$-\textit{critical point} for $h$ if $0\in\partial^{\,\varepsilon}_C h(x)$. The set of $C\text{-}\varepsilon$-critical points is identified as 
\begin{equation}
    \varepsilon \text{-} \operatorname{crit}_C \, h := \{x\in\X\,|\,0\in \partial^{\,\varepsilon}_C h (x) \}.
\end{equation}
\end{definition}



For a function $h\in\Gamma_{\rho}(\X)$, we  consider $C=\rho/2$. 
In order to relate the inexact proximal points of $h\in \Gamma_\rho(\X)$ with $\partial_{\rho/2}^{\, \varepsilon} h$, we use the following lemma.






\begin{lemma}[{\cite[Corollary 1]{bednarczuk2022calculus}}]
\label{cor:proxsub:2}	Let $\X$ be a Hilbert space.
Let $h\in\Gamma_\rho(\X)$ and  $\varepsilon\ge 0$, $\alpha>0$.
If $x_{\varepsilon} \in \varepsilon\operatorname{-prox}_{\alpha h}(y)$, then for any  $e \in \X $ with $\frac{\|e\|^2}{2\alpha}\leq \varepsilon$, we have
\begin{equation}\label{eq:eps:prox_inclusion:2}
\frac{y - x_\varepsilon - e}{\alpha } \in \partial_{ \rho/2}^{\,\varepsilon} h(x_\varepsilon).
\end{equation}
\end{lemma}
  
 \autoref{cor:proxsub:2}  is used in \autoref{sec:inexact FB convergence}, where we investigate the convergence of the inexact scheme.

\begin{remark}
 By   \autoref{def:crit} with $\varepsilon=0$ and {\cite[Theorem 2]{bednarczuk2022calculus}}, for any $x_0\in \prox_{\alpha h}(y) $, we obtain 
\begin{equation}
    0\in \partial_{\rho/2}(h + \frac{1}{2\alpha}\|\cdot - y\|^2)(x_0) = \partial_{\rho/2} h(x_0) + \frac{x_0-y}{\alpha},
\end{equation}
hence {${(y-x_0)}/{\alpha}\in \partial_{\rho/2} h(x_0).$}
\end{remark}




\section{Sharpness and criticality}\label{sec:sharpness}
{In this section, we analyse the relations between the solution set $S$ and $\varepsilon$-critical points of the  problem
\begin{equation}
\minimize{x\in \X}{} h(x)
\end{equation}
in the case the objective function $h$ satisfies the following sharpness condition.}
  

\begin{definition}[Sharpness, {\cite{Polyak1979,burke1993WeakSharpMinima}}] 
\label{def:sharpness}
Let $\X$ be a Hilbert space and $\fonc{h}{\X}{(-\infty,+\infty]}$ be a function with a nonempty solution set, ${S:=\argmin_{x\in\X}h(x)}\neq\emptyset$. Function $h$  satisfies the sharpness condition locally if there 
exist $\mu >0$ and $\delta>0$ such that
\begin{flalign}
    \label{eq:sharpness:local:def}
       \quad \left(\forall x\in B(S,\delta)\right)&& h(x) - \inf_{x\in \X} h(x) \geq \mu \dist (x,S).&&
\end{flalign}

If \eqref{eq:sharpness:local:def} is satisfied for every $\delta>0$, then the sharpness condition  holds globally and it reads as
\begin{flalign}
    \label{eq:sharpness:def}
   %\tag{Sh_{glob}}
    % \begin{split}
       \quad \left(\forall x\in \X\right)&& h(x) - \inf_{x\in\X} h(x) \geq \mu \dist (x,S).&&
\end{flalign}

\end{definition}

For any $x\in\X$, ${B}(x,\delta)$ denotes the open ball with centre  $x$ and radius $\delta>0$ and ${\overline{B}(x,\delta)}$ its closure, while for a set $C\subset \X$, $B(C,\delta) = \bigcup_{x\in C} B(x,\delta)$ and $\overline{B}(C,\delta)$ its closure.\\

The following elementary example shows that there exist weakly convex globally sharp functions (see also \autoref{sec:feasibility} devoted to the feasibility problem).



\begin{example}\label{ex:sharpness}
\sloppy The function $f$ from \autoref{ex: weak convexity} is globally sharp with solution set $S=\{a,b\}$. Take ${\delta = \left|\frac{a-b}{2} \right|}$ so that ${\overline{B}(a,\delta)}\cap {\overline{B}(b,\delta)} = \{\frac{a+b}{2}\}$. If $x = \frac{a+b}{2}$, 
then
$
   {f(x) =  |-\left(\frac{a-b}{2}\right)\left(\frac{a-b}{2}\right) | = |\frac{a-b}{2} | \dist(x,S).}
$
If $x\in B(a,\delta)$, 
%so $x\notin B(b,\delta)$, and 
then ${f(x) \geq \delta |x-a| = \delta \dist (x,S)}$. If $x\in B(b,\delta)$, then 
%$x\notin B(a,\delta)$, and 
${ f(x) \geq \delta |x-b| = \delta\dist (x,S)}$. If $x\notin B(a,\delta)$ and $x\notin B(b,\delta)$, then $f(x) \geq  \delta |x-a|$ and $f(x) \geq \delta |x-b|$.
Hence, for every $x\in\R$m
  ${f(x) \geq \delta \min\{ |x-a|, |x-b|\} =  \delta \dist (x,S)}$, $f$ is {globally sharp} with constant  { $\delta>0$} for  $\delta \leq\left |\frac{a-b}{2}\right |$.
\end{example}



Below, we investigate the position of the $\varepsilon$-critical points of $h$ which are not in $S$.
{ 
\begin{proposition}[Position of solutions relative to $C$-$\varepsilon$-critical points]
\label{prop:eps:statpoint}
Let $\X$ be a Hilbert space and ${\fonc{h}{\X}{(-\infty,+\infty]}}$ be a proper function on $\X$.  
Let 
$S = \argmin_{x\in\X} h(x)$ and
$h$ satisfy the sharpness condition \eqref{eq:sharpness:def} with a constant $\mu>0$. {
For any $0\leq \varepsilon< \mu^2/4C$ and any $x \in\varepsilon\operatorname{-crit}_C\;(h)\setminus S $, $C\geq 0$, 
\begin{equation} 
\label{eq:rings}
\text{either \ } \dist(x,S) \geq \tau_2  \text{  or  } 0< \dist(x,S) \leq \tau_1(\varepsilon),
\end{equation}
where \begin{equation}
\label{eq: t12}
    \tau_{1}(\varepsilon): = \frac{\mu - \sqrt{\mu^2 - 4 C\varepsilon}} {2C} \qquad  \text{and}\qquad\tau_{2}(\varepsilon): = \frac{\mu + \sqrt{\mu^2 - 4 C\varepsilon}} {2C}.
\end{equation}
}
\end{proposition}
\begin{proof}
Let $x \in\varepsilon\operatorname{-crit}_C\;(h)\setminus S $. By \autoref{def:crit}, $0\in \partial_{C}^{\varepsilon} h(x)$,
which means that
\begin{flalign}
\quad\left(\forall y \in \X\right) && \qquad h(x) - h(y) \leq C\|x-y\|^2 + \varepsilon. &&&
\end{flalign}
%In particular, when $y=\overline{x}\in S$, 
By the global sharpness of $h$, 
\begin{flalign}
\quad (\forall \overline{x}\in S) && \mu \dist(x,S) \leq C\|x-\overline{x}\|^2 + \varepsilon. &&&
\end{flalign}
By taking the infimum in the right-hand side over all $\overline{x}\in S$ we obtain the inequality
\begin{equation}
    \label{eq:eps:quadratic_equation}
    \mu \operatorname{dist}(x,S) \leq C\operatorname{dist}^2(x,S) + \varepsilon.
\end{equation}
which is  quadratic  with respect to $\dist(x,S)$
%we obtain from \eqref{eq:eps:quadratic_equation} that
and  either $ \dist(x,S) \geq \tau_2(\varepsilon)  $ or $0< \dist(x,S) \leq \tau_1(\varepsilon)$.
\end{proof}
}


\begin{remark} 
\label{rem:local_sharp}
When the global sharpness \eqref{eq:sharpness:def} is replaced in \autoref{prop:eps:statpoint} by the local sharpness \eqref{eq:sharpness:def}, then \eqref{eq:rings} remains unchanged provided $\delta>\tau_{2}(\varepsilon)$.
\end{remark}

In view of \autoref{prop:eps:statpoint}, given $0\le\varepsilon<\mu^2/C$, %the solution set $S$ and 
for any $C$-$\varepsilon$-critical point $x\in \X$ of $h$ which does not belong to $S$, either the point $x$ is  in a neighbourhood of $S$ of radius $\tau_1(\varepsilon)$ or its distance from $S$ is bounded from below by a positive quantity $\tau_2(\varepsilon)$. 

\begin{remark} 
\label{rmk:statpoint}
When $\varepsilon=0$, for a ${C}$-critical point $x$ that does not belong to $S$, by \autoref{prop:eps:statpoint} we obtain
% \begin{equation}\label{eq:DavisInequality}
%     \frac{\mu}{C} \leq \dist(x,S)
% \end{equation}
% or, equivalently,
\begin{equation}\label{eq:DavisInequality1}
    \operatorname{crit}_C (h) \setminus S\subset \left\{ x\in \X\ |\ \frac{\mu}{C} \leq \dist(x,S)\right\},
\end{equation}
% % \end{itemize}
which coincides with {\cite[ Lemma 3.1]{Davis2018SubgradientMethodsSharpWeaklyConvex}}, that is stated for $\rho$-weakly convex functions and $C=\rho / 2$.
\end{remark}

\autoref{prop:eps:statpoint}, in the form considered in \autoref{rmk:statpoint}, is illustrated in the following example, 
% where inequality \eqref{eq:DavisInequality} is attained.

{
\begin{example}\label{ex:critical_points}
Let us consider function $f$ from \autoref{ex: weak convexity}. We notice that $x = \frac{a+b}{2}$ is a $(\rho/2)$-$\varepsilon$-critical point in the sense of \autoref{def:crit} for $\rho = 2$ and $\varepsilon=0$. %We start
%by rewriting $ f$ in the following manner: 
Observe that, for every $x\in\R$,
\begin{equation*}
\begin{aligned}
  f(x) &= \max \{(x-a)(x-b), -(x-a)(x-b)\} = \max \{x^2 - (a+b)x + ab, -x^2-ab + (a+b)x\}
\end{aligned}\end{equation*}
which implies that, for every $x\in\R$, $ f(x) \geq  -x^2-ab + (a+b)x$.
We now notice that, for any $(a,b)\in\R^2$, the identity $ {(a-b)^2}/{4} -  {(a+b)^2}/{4} = -ab $ holds. It follows that
% \begin{equation*}
%     \frac{(a-b)^2}{4} -   \frac{(a+b)^2}{4} = \left( \frac{a-b}{2} + \frac{a+b}{2}\right) \left(\frac{a-b}{2} - \frac{a+b}{2}\right) = -ab
% \end{equation*} 
\begin{flalign*}
 \quad   (\forall x \in \R) &&  f(x) \geq  -x^2 + (a+b)x + \frac{(a-b)^2}{4} -   \frac{(a+b)^2}{4}  = -\left( x - \frac{a+b}{2}\right)^2 + \frac{(a-b)^2}{4}.&&&
\end{flalign*}
Since $ f\left( \frac{a+b}{2}\right) = \frac{(a-b)^2}{4}$, we get $f(x) -  f\left( \frac{a+b}{2}\right)  \geq    -\left( x - \frac{a+b}{2}\right)^2$ for $x\in\R$, 
%the inequality above reads as
%\begin{flalign*}
 %\quad   (\forall x \in \R) &&  f(x) -  f\left( \frac{a+b}{2}\right)  \geq    -\left( x - \frac{a+b}{2}\right)^2  &&&
%\end{flalign*}
which, according to \autoref{def:eps_prox}, implies that $0 \in \partial_{\rho/2}^{\,\varepsilon}  f \left(\frac{a+b}{2}\right)$ for every $\varepsilon\geq 0 $ and  $\rho = 2$. In addition, considering the fact that function $ f$ satisfies the global sharpness condition with constant $\mu = \left| \frac{a-b}{2} \right|$ (as shown in \autoref{ex:sharpness}), we have 
% inequality \eqref{eq:DavisInequality} is satisfied at $\overline{x} = \frac{a+b}{2}$:
\begin{equation*}
    \frac{2\mu}{\rho} = \frac{2 \left| \frac{a-b}{2} \right|}{2} = \left| \frac{a-b}{2} \right|  = \dist\left( \frac{a+b}{2},S\right).
\end{equation*}
\end{example}

For a weakly convex function $h\in \Gamma_\rho (\X)$, 
and the set $[h=0]=\{x\in\X\,|\, h(x) =0\}$, we  define the  Kurdyka-{\L}ojasiewicz inequality as in \cite{bolte2010characterizations}. For a given ${r_0}>0$, the set $\mathcal{K}(0,{r_0})$ represents the class of \emph{desingularisation functions}, which is defined as
\begin{equation}
    \mathcal{K}(0,{r_0}) = \{\varphi \in \mathcal{C}[0,{r_0})\cap\mathcal{C}^1(0,{r_0}) \,|\, \varphi(0) = 0\text{\;and\;}\phi'(r)>0 \;\forall r \in (0,{r_0})\}.
\end{equation}
\begin{definition}[Kurdyka-{\L}ojasiewicz Inequality]
Let $\delta_0, r_0>0$, {$x^* \in [h=0]$} and $\mathcal{K}(0,r_0)$ be a class of desingularisation functions. We say that the Kurdyka-{\L}ojasiewicz inequality holds at $x^*$ if there exist $r\in (0,r_0)$ and a function $\varphi\in \mathcal{K}(0,r_0)$ such that 
\begin{equation}
    \label{eq:KL:loc}
    %\tag{$K{\text{\L}}_{\text{loc}}$}
   \inf\{\|z\| \ |\ z\in \partial_{\rho/2}(\varphi\circ h)(x)\} \ge 1,\ \ \forall\,x\in \overline{B}(x^*, \delta_0)\cap [ 0 < h \le r],
\end{equation}
where $[ 0 < h \le r] = \{x\in\X\,| 0< h(x) \leq r\} $.
\end{definition}


%{F the rest of this section,   $h$ is a non-negative function with non-empty set of minimisers $S$.} 
We close this section by addressing the question of the relationship between \eqref{eq:KL:loc} and \eqref{eq:sharpness:local:def}. 
%for   a non-negative function $h$ with a non-empty set of minimisers $S$.} The following lemma holds.


\begin{lemma} 
\label{thm:sh_versus_kl}
    Let $\X$ be a Hilbert space and $h\in \Gamma_{\rho}(\X)$, $h\geq 0$, $S = \argmin_{x\in\X} h(x) = [h\leq 0]\neq\emptyset$. Let $r_0 >0$. Then 
    \begin{equation}
     \inf_{x \in [0< h<r_0]}\dist(\partial_{\rho/2} h(x),0) = \inf_{0\leq r < r_0} \inf_{x\in[r < h< r_0]} \frac{h(x) - r}{\dist(x, [h\leq r])}. % \leq \inf_{x\in[0 < h< \beta]} \frac{h(x)}{\dist(x, [h\leq 0])} \leq \frac{h(x)}{\dist(x,[h\leq 0])} 
    \end{equation}
%    for all $x \in [0<h<r_0]$.
\end{lemma}.
\begin{proof}
The proof follows from {\cite[Theorem 2.1]{aze2004characterizations}} and {\cite[Remark 12.(ii)]{bolte2010characterizations}}
\end{proof}

\begin{proposition} 
\label{cor:sh_versus_kl} 
Let $\X$ be a Hilbert space and $h\in \Gamma_{\rho}(\X)$, $h\geq 0$. Let $x^{*}\in S = \argmin_{x\in\X} h(x) = [h\leq 0]\neq\emptyset$.
Consider the following facts:
\begin{description} 
 \item \textit{(i)} there exist $r_1>0$, $\delta_1>0$, $\mu>0$ such that for every $x\in  B(x^*, \delta_1)\cap [ 0 < h(x) < r_1]$
 \begin{equation}\label{eq:prop:sh_loc}
     % \quad \left(\forall x\in \mathcal{B}(S,\delta)\right)&&
     h(x) - \inf_{x\in \X} h(x) \geq \mu \dist (x,S);
 \end{equation}
 % \eqref{eq:sharpness:local:def} 
 \item \textit{(ii)} \sloppy there exist $\overline r>0$, $r_2\in(0,\overline{r})$, $\delta_2>0$ and $\varphi\in\mathcal{K}(0,\overline{r})$ such that %\eqref{eq:KL:loc} holds at $x^{*}$
 for every
${x\in \overline{B}(x^*, \delta_2)\cap [0 < h(x) \le r_2]}$
 \begin{equation}\label{eq:prop:kl_loc}
     \inf\{\|z\| \ |\ z\in \partial_{\rho/2}(\varphi\circ h)(x)\} \ge 1.
 \end{equation}
\end{description}
Then $(i)\implies (ii)$. If $h$ is continuous and $\varphi = \frac{1}{\mu}\operatorname{Id} $ for some $\mu>0$ (where  $\operatorname{Id}$ is the identity operator over $\X$), then $(ii) \implies (i)$.
 \end{proposition} 


}


\begin{proof}
    See Appendix \ref{app:proof_of_equivalence}.
\end{proof}
\begin{remark}
    If $h$ satisfies \eqref{eq:KL:loc} at $x^*\in S$, then, by \autoref{cor:sh_versus_kl}, there exist $\eta,r_0 >0$ such that
\begin{equation}
    (\forall x\in B(x^*,\eta)\cap [0<h<r_0])\qquad h(x)-h(x^*) \geq c\dist(x,S).
\end{equation}
Combining this with \eqref{eq:KL:loc}, we have 
\begin{equation}
    \dist(0,\partial_{\rho/2} h(x)) \geq c (h(x)-h(x^*)) \geq c^2 \dist (x,S),
\end{equation}
for all $x\in B(x^*,\eta)\cap [0<h<r_0]$, which is the subdifferential error bound c.f. \cite[Theorem 3.11]{Kruger2019HlderEB}.
\end{remark}

\section{The Inexact FB Algorithm} \label{sec:inexact FB convergence}

%Let us give the basic setting for the minimization of problem 
%\eqref{eq: ourproblem}. 
\autoref{assu:problem} summarises the standing assumptions on functions $f$ and $g$, which are made in all the results of the subsequent sections.
\begin{assumption}
    \label{assu:problem} Let the following facts hold:
    \begin{enumerate}[parsep=0cm, itemsep=0cm, topsep=0cm, label=(\roman*)]
    \setlength{\itemindent}{+.3in}
        \item function $f\in\Gamma_\rho (\X)$ is bounded from below;
        \item function $g\in \Gamma_0 (\X)$ is {Fr\'echet} differentiable on $\X$ with a $L_g$-Lipschitz continuous gradient;
        %\item $\dom f \subset \dom g$; %$\dom f \cap \dom g \neq \emptyset$; \lele{ ensures the well definition the iterates}
        \item the set $S = \argmin_{x\in\X}(f+g)(x)$ is non-empty.
    \end{enumerate}
\end{assumption}

We discuss the following forward-backward scheme:\\

\begin{algorithm}[H]\caption{Inexact Forward-Backward}\label{alg:epsFB}
\textbf{Initialise:} $x_0 \in \dom (f+g)$\\
\textbf{Set:} $(\varepsilon_t)_{t\in\N}$ non-negative, $(\alpha_t)_{t\in\N}$ positive; \\
\textbf{For}{ $t=1,2,\dots$ {\normalfont\textbf{until} convergence}}{
\begin{align}
y_{t}\;&=x_{t}-\alpha_{t}\nabla g(x_{t}) \\
x_{t+1}\;&\in{\varepsilon_{t}}\text{-prox}_{\alpha_t f}(y_t) %\text{using an {oracle}}
\end{align}
}
\end{algorithm}
\vspace{0.5cm}


 In the analysis below, we do not tackle the problem of how to compute the $\varepsilon\operatorname{-}$proximal point of  $f$ at any given $y$. Instead, we assume the existence of an oracle which provides us with an $\varepsilon$-proximal point. For instance, see Appendix \ref{app:inexprox}.  %Recall that we defined $\varepsilon$-proximal points as $\varepsilon$-solutions of the corresponding optimisation problem, see \autoref{def:eps_prox}. 


By applying \autoref{cor:proxsub:2}, we investigate the decreasing behaviour of the objective function in \eqref{eq: ourproblem} for a sequence generated by \autoref{alg:epsFB}. 

% \lele{this generalises \cite[Proposition 7]{bayram2015convergence}}
\begin{proposition}
\label{prop:eps:FB estimate g-convex}
Let $\X$ be a Hilbert space and $\fonc{f,g}{\X}{(-\infty,+\infty]}$ satisfy \autoref{assu:problem}.
%Let $f\in \Gamma_\rho(\X)$ and
% $\fonc{g}{\X}{(-\infty,+\infty]}$ be proper, convex and %differentiable on its domain with $L_g$-Lipschitz continuous gradient, ${\dom f \cap \dom g \neq \emptyset}$. 
Let $\left(x_t\right)_{t\in\N}$ be the sequence generated by \autoref{alg:epsFB}. Then, for any $x\in \X$, 
%the following estimate holds:
\begin{equation}\label{eq:eps:FB_estimate}
\begin{aligned}
    (f+g)(x)   - (f+g)&(x_{t+1}) \geq \innerprod{\frac{x_t - x_{t+1}}{\alpha_t}}{x-x_{t+1}} \\
    &  - \frac{\rho}{2}\|x-x_{t+1}\|^2 - \frac{L_g}{2}\|x_{t}- x_{t+1}\|^2    
    - \varepsilon_t - \sqrt{\frac{2\varepsilon_t}{\alpha_t}}\| x - x_{t+1}\|.\\
    \end{aligned}
\end{equation}
Moreover,  when $2/\alpha_t > \rho+1 +L_g$, for all $t\in\mathbb{N}$
\begin{equation}
\label{eq:decrease}
     \left(f+g\right)\left(x_t\right)-\left(f+g\right)\left(x_{t+1}\right) >  - \varepsilon_t - \sqrt{\frac{2\varepsilon_t}{\alpha_t}}\| x - x_{t+1}\|.
\end{equation}%}
%when $2/\alpha_t > \rho+1 +L_g$.
\end{proposition}

\begin{proof}
By \autoref{alg:epsFB} and \autoref{cor:proxsub:2}, for any $t\in\mathbb{N}$, there exists $e_t\in \X$,  $\frac{\|e_t\|^2}{2\alpha_t}\leq \varepsilon_t$, such that
\begin{equation}
    \frac{x_t - \alpha_t \nabla g(x_t) - x_{t+1} - e_t}{\alpha_t} \in \partial_{\rho/2}^{\,\varepsilon_t} f(x_{t+1}).
\end{equation}
By definition of proximal ${\varepsilon\text{-subdifferential}}$,  for any $x\in\X$
\begin{align}\label{eq:inequality01}
    f\left(x\right)-f\left(x_{t+1}\right)	&\geq\left\langle \frac{x_{t}-x_{t+1}}{\alpha_{t}}-\nabla g\left(x_{t}\right) | x-x_{t+1}\right\rangle -\frac{\rho}{2}\left\Vert x-x_{t+1}\right\Vert ^{2} - \left\langle \frac{e_t}{\alpha_t} |  x - x_{t+1}\right\rangle -\varepsilon_t.
\end{align}
By the convexity of $g$ and Descent Lemma, we obtain
\begin{equation}
\begin{aligned}\label{eq:inequality00}
    \left\langle -\nabla g\left(x_{t}\right)\ |\ x-x_{t+1}\right\rangle 
    &\geq g\left(x_{t+1}\right) - g(x) 
    -\frac{L_{g}}{2}\left\Vert x_{t}-x_{t+1}\right\Vert ^{2}.
    \end{aligned}
\end{equation}
On the other side,  by applying the Cauchy-Schwarz inequality, for any $x\in\X$
\begin{equation}\label{eq:inequality2}
    \left\langle \frac{e_t}{\alpha_t}\ |\  x - x_{t+1}\right\rangle \leq \frac{1}{\alpha_t}\|e_t\|\| x - x_{t+1}\|% \leq \frac{1}{2\alpha_t}\left( \|e_t\|^2 + \|x - x_{t+1}\|^2\right) \leq \varepsilon_t + \frac{1}{2\alpha_t}\|x- x_{t+1}\|^2.
    \leq \frac{1}{\alpha_t}\sqrt{2\alpha_t\varepsilon_t }\| x - x_{t+1}\| \leq \sqrt{\frac{2\varepsilon_t}{\alpha_t}}\| x - x_{t+1}\|.
\end{equation}
By plugging \eqref{eq:inequality00} and \eqref{eq:inequality2} into \eqref{eq:inequality01} we obtain \eqref{eq:eps:FB_estimate}.% for any $x\in\X$
% \begin{equation}
% \begin{aligned}
% \label{eq:inequality_f_g}
%     &(f+g)(x) - (f+g)(x_{t+1}) \\
%     &\geq \innerprod{\frac{x_t - x_{t+1}}{\alpha_t}}{x-x_{t+1}}  - \frac{\rho}{2}\|x-x_{t+1}\|^2 - \frac{L_g}{2}\|x_{t}- x_{t+1}\|^2 - \varepsilon_t - \sqrt{\frac{2\varepsilon_t}{\alpha_t}}\| x - x_{t+1}\|.\\
%     \end{aligned}
% \end{equation}

For the second part, since \eqref{eq:eps:FB_estimate} holds for any $x\in \X$, substituting $x=x_t$ yields
\begin{align}
    \left(f+g\right)\left(x_t\right)-\left(f+g\right)\left(x_{t+1}\right)&\geq \left(\frac{1}{\alpha_t}-\frac{\rho}{2}-\frac{ L_{g}}{2}\right) \left\Vert x_{t}-x_{t+1}\right\Vert ^{2} - \varepsilon_t - \sqrt{\frac{2\varepsilon_t}{\alpha_t}}\| x_t - x_{t+1}\|.
\end{align}
The assumption $2/\alpha_t > \rho +L_g$ yields
\begin{equation}
     \left(f+g\right)\left(x_t\right)-\left(f+g\right)\left(x_{t+1}\right)>  - \varepsilon_t - \sqrt{\frac{2\varepsilon_t}{\alpha_t}}\| x_t - x_{t+1}\|.
\end{equation}
\end{proof}

\begin{remark}\label{rem:FB estimate g-convex}
When $\varepsilon_t=0$ for every $t\in\N$, inequality \eqref{eq:decrease} from \autoref{prop:eps:FB estimate g-convex} reduces to
\begin{equation}\label{eq:FB estimate g-convex}
     \left(f+g\right)\left(x_t\right)-\left(f+g\right)\left(x_{t+1}\right)> 0,
\end{equation}
 \emph{i.e.\ }the objective values diminish in a strictly monotone way {(even if both $f$ and $g$ are weakly convex) (for finite-dimensional setting c.f. {\cite[Proposition 7]{bayram2015convergence}})}. If 
 %the objective function is lower bounded,
 $\inf( f+g)>-\infty$, then $(f+g)(x_t)$  converges as $t$ goes to infinity. In addition, we observe that the restriction that we have on the step size 
$\alpha_t < \dfrac{2}{L_g + \rho}$
generalises the one of the convex case (\emph{i.e.\ }when $\rho=0$) which is $\alpha_t < 2 / L_g$  (see \cite{combettes2005signal}).
\end{remark}

From now on, we consider fixed  {$\alpha_{t}=\alpha$ and $\varepsilon_{t}=\varepsilon$ for  $t\in\mathbb{N}$}. 
We adopt the following assumption.


\begin{assumption}\label{assu:conditions}
 We choose $\varepsilon \geq 0$ and $\alpha >0$ satisfying the following inequalities:
\begin{align} 
\label{eq:cond:epsilon}&\varepsilon<\frac{\mu^{2}}{2\left(\rho+1\right)}\min\left \{\frac{1}{L_g+1},\frac{1}{\rho+2}\right\},\\ %\\
\label{eq:cond:alpha:1}&\frac{2\varepsilon (\rho+1)}{\mu^2 -2\varepsilon(\rho+1)} \le \alpha < \min\left\{\frac{1}{L_g}, \frac{1}{\rho+1}\right\}.
%\label{eq:cond:alpha:2}\alpha &\geq \frac{2\varepsilon (\rho+1)}{\mu^2 -2\varepsilon(\rho+1)}\\
% \label{eq:cond:alpha:1} &\frac{1}{L_g}\geq \alpha.
\end{align}
\end{assumption}
The following quantities {play an important role in the convergence analysis in \autoref{sec:convergence:analysis}}:
\begin{align}
   % \label{eq:E minus:c}
    E^{-} := \frac{\mu-\sqrt{\mu^{2}-2\varepsilon_{}\left(\rho+1\right)\frac{\alpha_{}+1}{\alpha_{}}}}{\rho+1}\text{\quad and\quad}
   % \label{eq:E plus:c}
    E^{+} := \frac{\mu+\sqrt{\mu^{2}-2\varepsilon_{}\left(\rho+1\right)\frac{\alpha+1}{\alpha_{}}}}{\rho+1}. 
\end{align}

\begin{remark}
\label{rem:tubeE}
%We notice that the following inequalities holds: 
% {\sloppy We have}
%\begin{equation}
   We have $\tau_1(\varepsilon)\leq E^- < E^+ \leq \tau_2(\varepsilon)$,
%\end{equation}
{ $\tau_1(\varepsilon)$, where $\tau_2(\varepsilon)$ are as in %\linebreak
\autoref{prop:eps:statpoint}.}
\end{remark}

\begin{remark}

When $\varepsilon=0$, \autoref{assu:conditions} simplifies to 
  \begin{flalign*}
&& \alpha < \min \left\{ \frac{1}{L_g} , \frac{1}{\rho+1}\right\}. &&&
\end{flalign*}
This implies $2 > \alpha(\rho +1  + L_g)$
%\begin{flalign*}
%&&   2 > \alpha(\rho +1  + L_g) &&&
%\end{flalign*}
which  
%satisfies 
{coincides with } the condition in {\eqref{eq:decrease}} in \autoref{prop:eps:FB estimate g-convex}.
%ensuring that
%the sequence of objective values %$\left((f+g)(x_t)\right)_{t\in\N}$ is monotonically decreasing. 
To conclude, we observe that in the case of exact proximal computations ($\varepsilon=0$, {\emph{c.f} \autoref{rmk:statpoint}} ), 
\begin{equation*}
E^- = \tau_1(0)= 0 \quad \text{and}\quad E^+ = \tau_2(0)  = \frac{2\mu}{\rho}.
\end{equation*}
%{\emph{c.f} \autoref{rmk:statpoint}}.
\end{remark}

\section{Convergence analysis}\label{sec:convergence:analysis}

The aim of this section is to prove the following result.


\begin{theorem} \label{theo:Et constant convergence}
 Let $\X$ be a Hilbert space, $\fonc{f,g}{\X}{(-\infty,+\infty]}$ satisfy \autoref{assu:problem} and  $f+g$ satisfy the sharpness condition \eqref{eq:sharpness:def} with constant $\mu>0$. Assume that $\varepsilon\geq0$, $\alpha>0$ are chosen according \autoref{assu:conditions}, 
and $\left(x_t\right)_{t\in\N}$ is  generated by \autoref{alg:epsFB}. 
The following  hold true.
\begin{description} 
\item[--] If there exists $t_0 \in \N$ such that 
%the following holds 
\begin{equation}\label{eq:assuA}
E^-<\operatorname{dist}(x_{t_0},S) < E^+,
\end{equation}
then there exists a constant $\zeta >1$ such that
\begin{flalign}\label{eq:theo:conv_rate}
    \quad(\forall t\geq t_0)&& \dist^2(x_{t},S)-(E^-)^2 \leq \left(\frac{1}{\zeta}\right)^{t-t_0}( \dist^2(x_{t_0},S)-(E^-)^2). &&&
\end{flalign}
%implying 
Consequently,
       $\limsup_{t\to\infty} \dist(x_{t},S) \leq E^-.$
\item[--] If 
\begin{flalign}\label{eq:assu1} \quad(\forall t\geq t_0)&& E^- < \operatorname{dist}(x_{t},S) < E^+,&&&
\end{flalign}
then 
%\begin{equation}
    $\lim_{t\to +\infty} \dist(x_t,S) = E^-.$
%\end{equation}
\end{description}
\end{theorem}

The following two lemmas are used in the proof of \autoref{theo:Et constant convergence}.

\begin{lemma}\label{lem:inthetube:const}
 
 %Under the assumptions 
Let the assumptions of \autoref{theo:Et constant convergence} be satisfied. We have the following:
%we have 
\begin{equation}
\label{eq:dist:xtplus}
(\exists t_0\in\N)\quad  \dist\left(x_{t_0},S\right)\leq
E^+ \Rightarrow \quad (\forall t\geq t_0) \quad \dist\left(x_{t},S\right)\leq
E^+.
\end{equation}
% then 
% %for every $t\geq t_0$ we have
% \begin{flalign}
% \label{eq:dist:xtplus:2}
% \quad (\forall t\geq t_0) && \text{dist}\left(x_{t},S\right)\leq
% E^+. &&&
% \end{flalign}
% Moreover, if for a certain $t_0\in \N$ we have
\begin{equation}
\label{eq:dist:xtminus}
(\exists t_0 \in\N) \quad \dist\left(x_{t_0},S\right)\leq
E^- \Rightarrow \quad (\forall t\geq t_0) \quad \dist\left(x_{t},S\right)\leq
E^-. 
\end{equation}
% then 
% %for every $t\geq t_0$ we have
% \begin{flalign}
% \label{eq:dist:xtminus:2}
% \quad (\forall t\geq t_0) && \text{dist}\left(x_{t},S\right)\leq
% E^-. &&&
% \end{flalign}
\end{lemma}


\begin{proof}
See \autoref{sec:lem:inthetube:const}.
\end{proof}




\begin{lemma}
\label{prop:estimation with Et}
{ 
Under the assumptions of \autoref{theo:Et constant convergence},
if, for some $ t_0\in \N$, we have 
\begin{equation}
% \label{eq:dist:xtplus}
\text{dist}\left(x_{t_0},S\right)\leq
E^+,
\end{equation}
then for every $t\geq t_0$ there exists $\zeta_{t+1}\geq 1$ such that 
\begin{equation}
\label{eq:eps:contraction}
    \zeta_{t+1} \left(\operatorname{dist}^2 (x_{t+1},S) -(E^-)^2\right) \leq  \operatorname{dist}^2 (x_{t},S)-(E^-)^2.
\end{equation}
Consequently, $\dist(x_{t+1},S) \leq \dist(x_t,S)$ provided $\dist(x_{t+1},S) >E^-$.}
\end{lemma}





\begin{proof}
See \autoref{sec:prop:estimation with Et}.
\end{proof}
\autoref{lem:inthetube:const} and \autoref{prop:estimation with Et} hold true when 
%the sharpness condition 
\eqref{eq:sharpness:def} is replaced by \eqref{eq:sharpness:local:def} for a sufficiently large $\delta>0$.
{
\begin{proof}[\textbf{Proof of \autoref{theo:Et constant convergence}}]



By \eqref{eq:eps:contraction},
%in \autoref{prop:estimation with Et}  and from the fact that $E^-$ is constant for every $t$, 
there exists $\zeta_{t+1}\geq 1$ such that
\begin{flalign}
 \quad (\forall t\geq t_0)&&   \zeta_{t+1} \left(\operatorname{dist}^2 (x_{t+1},S) -(E^-)^2\right) \leq  \operatorname{dist}^2 (x_{t},S)-(E^-)^2.&&&
\end{flalign}
% where {$\zeta_{t+1} := 1-\alpha \rho -\alpha+ \frac{2\alpha \mu}{\operatorname{dist}(x_{t+1},S)+ E^-}\geq 1.$}
From this, we infer that, for every $t\geq t_0$, the following holds:
\begin{equation}\label{eq:inequality_prod}
\begin{aligned}
\operatorname{dist}^2 (x_{t+1},S) -(E^-)^2 &\leq  \frac{1}{\zeta_{t+1}}\left(\operatorname{dist}^2 (x_{t},S)-(E^-)^2 \right) \leq  \left(\prod_{s=t_0}^{t}\frac{1}{\zeta_{s+1}}\right)\left(\operatorname{dist}^2 (x_{t_0},S)-(E^-)^2 \right).
   % & < \left(\frac{1}{\zeta}\right)^{t+1}\left(\operatorname{dist}^2 (x_{t_0},S)-(E^-)^2 \right).
\end{aligned}\end{equation}
%Now, we discuss  the LHS in \eqref{eq:inequality_prod}. 
Denote by $t_1$, the first iterate $t_1 > t_0$ such that $\dist(x_{ t_1},S) < E^-$.
By \autoref{lem:inthetube:const}, $\dist(x_t,S)\leq E^-$, for every $t\geq t_1$, 
 %we have $\dist(x_t,S)\leq E^-$, 
 which implies that 
 %for every $t\geq t_1$ 
 the LHS in \eqref{eq:inequality_prod} is non-positive,
\begin{flalign}\label{eq:conv:-}
\quad\left( t\geq t_1\right)&& \dist^2(x_{t},S) - (E^-)^2 \leq 0. &&& \end{flalign}
On the other side, by our choice,
$\dist(x_t,S) > E^-$
for 
%every $t$ such that 
$t_0\leq t<  t_1$.  
%we have $\dist(x_t,S) > E^-$.
By \autoref{prop:estimation with Et}, for every $  t_0< t<  t_1$, we have
%\begin{flalign}
 $\operatorname{dist}(x_{t},S) \leq \operatorname{dist}(x_{t-1},S)$,
%\end{flalign}
implying that
\begin{flalign}\label{eq:dist:ineq} 
&&
    \operatorname{dist}(x_{t},S) \leq 
 \operatorname{dist}(x_{t-1},S) \leq \dots \leq \operatorname{dist}(x_{t_0},S). &&&
\end{flalign}
We can therefore define a lower-bound for $\zeta_{t}$ as follows, for $  t_0\leq t<  t_1$
\begin{equation}\label{eq:zetas}
\begin{aligned}
    \zeta_{t} &= 1-\alpha \rho -\alpha+ \frac{2\alpha \mu}{\operatorname{dist}(x_{t},S)+ E^-} \geq  1-\alpha \rho -\alpha+ \frac{2\alpha \mu}{\operatorname{dist}(x_{t_0},S)+ E^-} = \zeta_{t_0} \\
    &\quad> 1-\alpha \rho -\alpha+ \frac{2\alpha \mu}{E^+ + E^-} = 1.
\end{aligned}
\end{equation}
%where the first inequality stems from \eqref{eq:dist:ineq}, 
%(we include the equality to take into account the case $t=t_0$), 
%the second from \eqref{eq:assuA} and the last equality from the definition of $E^+$ and $E^-$. 
The product in the RHS of \eqref{eq:inequality_prod} can be estimated %by using $\zeta_{t_0}$ 
as follows
\begin{equation}
     \left(\prod_{s=t_0}^{t}\frac{1}{\zeta_{s+1}}\right) \leq  \left(\frac{1}{\zeta_{t_0}}\right)^{t-t_0+1},
\end{equation}
implying that 
 \begin{flalign}\label{eq:conv:+}
 \quad \left (  t_0\leq t<  t_1 \right) &&    \dist^2(x_{t},S) - (E^-)^2 \leq \left(\frac{1}{\zeta_{t_0}}\right)^{t-t_0}\left(\operatorname{dist}^2 (x_{t_0},S)-(E^-)^2 \right).   &&&
 \end{flalign}
 To conclude, we combine  \eqref{eq:conv:-} (for $t > {t_1}$) with \eqref{eq:conv:+} (for $  t_0\leq t<  t_1$) to obtain
 %into the following inequality that holds 
 %for every $t\geq t_0$ 
 %by taking as an upper bound the positive upper bound in \eqref{eq:conv:+}:
 \begin{flalign}\label{eq:convergence_inequality}
\quad (\forall t\geq t_0)   && \dist^2(x_{t},S) - (E^-)^2 \leq \left(\frac{1}{\zeta_{t_0}}\right)^{t-t_0}\left(\operatorname{dist}^2 (x_{t_0},S)-(E^-)^2 \right). &&& 
 \end{flalign}
 Passing to the $\limsup$ as $t\to \infty$ we conclude that
 \begin{equation}
     \begin{split}
         \limsup_{t\to\infty} \left(\dist^2(x_{t},S) - (E^-)^2 \right)\leq \limsup_{t\to\infty} \left(\frac{1}{\zeta_{t_0}}\right)^{t-t_0}\left(\operatorname{dist}^2 (x_{t_0},S)-(E^-)^2 \right) \\
          = \left(\operatorname{dist}^2 (x_{t_0},S)-(E^-)^2 \right) \lim_{t\to\infty} \left(\frac{1}{\zeta_{t_0}}\right)^{t-t_0}= 0,
     \end{split}
 \end{equation}
where the last equality is due to the fact that $\zeta_{t_0}>1$ as shown in \eqref{eq:zetas}.  We then conclude that
 \begin{equation}
       \limsup_{t\to\infty} \dist(x_{t},S) \leq E^-.
 \end{equation}
 
To show the second part of the statement, we notice that by combining \eqref{eq:assu1} %holds for all $t\geq t_0$, then combining it 
with \eqref{eq:theo:conv_rate}, we get
\begin{flalign}\label{eq:dist_e_}
    \quad(\forall t\geq t_0)&& 0 <\dist^2(x_{t},S)-(E^-)^2 \leq \left(\frac{1}{\zeta}\right)^{t-t_0}( \dist^2(x_{t_0},S)-(E^-)^2),&&&
\end{flalign} 
with the RHS non-negative and monotonically decreasing to $0$ as $t\to\infty$. By \eqref{eq:assu1}, the LHS is positive, and \eqref{eq:dist_e_} implies that
\begin{equation*}
    \lim_{t\to \infty} \operatorname{dist}(x_{t},S)  = E^-,
\end{equation*}
which concludes the proof.
\end{proof}



 %\autoref{theo:Et constant convergence} describes the behaviour of $(\dist(x_t,S))_{t\in\N}$ when \eqref{eq:assu1} holds. Moreover,  
 Condition \eqref{eq:assu1} of \autoref{theo:Et constant convergence}  allows us to obtain the strong convergence of the sequence $(x_t)_{t\in\N}$.
 %as shown in the following corollary.
\begin{corollary}\label{cor:eps:strong_conv}
    Under the assumptions of \autoref{theo:Et constant convergence}, if \eqref{eq:assu1} holds, then $(x_t)_{t\in\N}$ converges. 
\end{corollary}
\begin{proof}
By \eqref{eq:eps distance square estimate with xi}, (see \autoref{sec:prop:estimation with Et}),
there exists $\zeta_{t+1}>1$ such that
\begin{equation}
\zeta_{t+1} \left(\operatorname{dist}^2 (x_{t+1},S) -(E^-)^2\right) +(1-\alpha L_g) \Vert x_t -x_{t+1}\Vert^2 \leq  \operatorname{dist}^2 (x_{t},S)-(E^-)^2. 
\label{eq: dist const param with xt1}
\end{equation}
By \eqref{eq:assu1}, the first term of the LHS of \eqref{eq: dist const param with xt1} is positive, which implies
\begin{equation}
(1-\alpha L_g) \Vert x_t -x_{t+1}\Vert^2 \leq  \operatorname{dist}^2 (x_{t},S)-(E^-)^2. 
\end{equation}
By  \autoref{theo:Et constant convergence}, the RHS can be estimated with a constant $\zeta >1$
\begin{equation}
(1-\alpha L_g) \Vert x_t -x_{t+1}\Vert^2 \leq \left(\frac{1}{\zeta}\right)^{t-t_0} 
 \left[\operatorname{dist}^2 (x_{t_0},S)-(E^-)^2 \right].
 \label{eq: xt const param zeta}
\end{equation}
As both sides of \eqref{eq: xt const param zeta} are non-negative for all $t\geq t_0$, %taking the square root and summing over $t$ to some $T>t_0$ yields
\begin{equation}\label{eq:sum}
\sqrt{1-\alpha L_g} \sum_{t\geq t_0}^T\Vert x_t -x_{t+1}\Vert \leq 
 \sqrt{\operatorname{dist}^2 (x_{t_0},S)-(E^-)^2 } \sum_{t\geq t_0}^T \left(\frac{1}{\sqrt{\zeta}}\right)^{t-t_0}.
\end{equation}
%Since  $\zeta>1$, 
We  have $\sqrt{\zeta}>1$
%. By assumption, 
and $\sqrt{1-\alpha L_g}>0$. When  $T\to\infty$, the RHS of \eqref{eq:sum} is finite, which implies that 
\begin{equation}
\sum_{t\geq t_0}^{\infty} \Vert x_t -x_{t+1}\Vert < +\infty.
\end{equation}
By virtue of {\cite[Theorem 1]{Bolte2014_Proximal}}, we conclude that $(x_t)_{t\in\N}$ is a Cauchy sequence so it converges.
\end{proof}



 

\section{Convergence to global solutions in the exact case}\label{sec:convergence:exact}


 In the case of exact proximal computations we can improve \autoref{theo:Et constant convergence} by admitting a larger upper bound for the step size $\alpha$. \autoref{lem:inthetube:const} and \autoref{prop:estimation with Et} take the form presented in the following lemma. 
\begin{lemma}
\label{cor:1st result of inexact case}
 Let $\X$ be a Hilbert space, $\fonc{f,g}{\X}{(-\infty,+\infty]}$ satisfy \autoref{assu:problem} and  $f+g$ satisfy the sharpness condition \eqref{eq:sharpness:def} with constant $\mu>0$. Let $\left(x_t\right)_{t\in\N}$ be  generated by \autoref{alg:epsFB} with $\varepsilon = 0$ and  the step size  $\alpha>0$ satisfying the condition
\begin{flalign*}
\quad (\forall t\in\N)&& \alpha < \min \left\{ \frac{1}{L_g} , \frac{1}{\rho}\right\}. &&&
\end{flalign*}
For every $t\geq 0$, either $\operatorname{dist}(x_{t+1},S)=0$ or 
%the following inequality 
%\begin{equation*}
$\zeta_{t+1} \operatorname{dist}^2(x_{t+1},S) \leq \operatorname{dist}^2(x_t,S)$,
%\end{equation*}
where 
\begin{equation}
\label{eq:xi_exact}
\zeta_{t+1} =  1  - \alpha \rho +\frac{2\alpha \mu}{\dist (x_{t+1},S)}.
\end{equation}
Moreover, if $\dist(x_{t_0},S) <\frac{2\mu}{\rho}$, for some $t_0\in\N$,  then    %following condition is satisfied for all $t\geq t_0$.

\begin{equation}\label{eq:Inequality2murho}
   \dist(x_{t},S) < \frac{2\mu}{\rho},
\end{equation}
and $\zeta_{t+1} > 1$ for all $t\geq t_0$.
\end{lemma}
\begin{proof}
%See Appendix \ref{app:cor:1st result of inexact case}
%\ref{cor:1st result of inexact case}
 The proof follows the same steps as Lemma 6.1 starting from \eqref{eq:eps:FB_estimate} of Proposition 5.1. Notice that there is no $\varepsilon$ term in this case.
\end{proof}
In the exact case, with 
%it is possible to consider a 
%step size 
%varying in a range 
$\alpha_t \in [\underline{\alpha}, \overline{\alpha}]\subset \left(0, \min\left\{\frac{1}{\rho}, \frac{1}{L_g}\right\}\right)$ and obtain a similar result as %the one in 
\autoref{theo:Et constant convergence}. 


\begin{proposition}[\textbf{Strong convergence of the sequence in the exact case}]\label{prop:convergence_convex:new}
  Let $\X$ be a Hilbert space and $\fonc{f,g}{\X}{(-\infty,+\infty]}$ satisfy \autoref{assu:problem} and the function $f+g$ satisfy the sharpness condition \eqref{eq:sharpness:def} with constant $\mu>0$. Let $(x_t)_{t\in\N}$ be  generated by \autoref{alg:epsFB} with $\varepsilon=0$ and  step sizes ${\alpha_t \in [\underline{\alpha},\overline{\alpha}]\subset (0, \min\{\frac{1}{\rho}, \frac{1}{L_g}\})}$. Assume  there exists a $t_0\in \N$ such that $\operatorname{dist}(x_{t_0},S)<\frac{2\mu}{\rho}$.
 %\begin{flalign}&& \operatorname{dist}(x_{t_0},S)<\frac{2\mu}{\rho}. &&& \end{flalign}
%  the starting point $x_0\in \X$ chosen so as to have $\operatorname{dist}(x_0,S)<\frac{2\mu}{\rho}$.
%Then, for every $t\geq t_0$ there exists $\zeta>1$ such that
% \begin{equation}\label{eq:exact:contraction}
%   \zeta  \operatorname{dist}^2(x_{t},S) \leq \operatorname{dist}^2(x_{t+1},S) 
% \end{equation}
%  and
Then 
%the following holds 
\begin{equation}\label{eq:dist_to_zero}
  \lim_{t\to \infty} \operatorname{dist}(x_{t+1},S)=0
\end{equation}
 and the sequence $(x_t)_{t\in\N}$ converges strongly to a point $x^*\in S$.
\end{proposition}
\begin{proof}
By \autoref{cor:1st result of inexact case},  for every $t\geq t_0$, there exists $\xi_{t+1} =  1 -\alpha_t \rho +\frac{2\alpha_t \mu}{\dist (x_{t+1},S)}> 1$ such that 
\begin{equation}\label{eq:ineq:xi:exact}
    \xi_{t+1} \dist^2(x_{t+1},S) \leq \dist^2(x_{t},S) . 
\end{equation}
Given the lower-bound for $\alpha_t$, we now define
\begin{equation}
\label{eq:zeta_uniform}\xi:=1+\underline{\alpha}\left(\frac{2\mu}{\dist(x_{t_0},S)}-\rho\right)>1,
\end{equation}
which yields the following inequality for every $t\geq t_0$
\begin{align}
\xi_{t+1} & =1+\alpha_{t}\left(\frac{2\mu}{\operatorname{dist}\left(x_{t+1},S\right)}-\rho\right)
  >1+\alpha_{t}\left(\frac{2\mu}{\dist(x_{t_0},S)}-\rho\right) \geq1+\underline{\alpha}\left(\frac{2\mu}{\dist(x_{t_0},S)}-\rho\right)=\xi.
\end{align}
Combining the latter with \eqref{eq:ineq:xi:exact}, we have that for every $t\geq t_0$
\begin{equation}
% \operatorname{dist}^2\left(x_{t},S\right)\ge \xi_{t+1}\operatorname{dist}^2\left(x_{t+1},S\right)> \xi\left(\underline{\alpha}\right) \operatorname{dist}^2\left(x_{t+1},S\right),
\xi\operatorname{dist}^2\left(x_{t+1},S\right) < \xi_{t+1}\operatorname{dist}^2\left(x_{t+1},S\right) \leq \operatorname{dist}^2\left(x_{t},S\right)
\end{equation}
hence the following \emph{contraction} inequality holds
\begin{equation}
\label{eq: linear convergence}
\operatorname{dist}^2\left(x_{t+1},S\right) < \frac{1}{\xi}\operatorname{dist}^2\left(x_{t},S\right),
\end{equation}
which by recursiveness yields that for every $t\geq t_0$
\begin{equation}
\label{eq: linear convergence:2}
\operatorname{dist}^2\left(x_{t+1},S\right) < \left(\frac{1}{\xi}\right)^{t+1-t_0}\operatorname{dist}^2\left(x_{t_0},S\right).
\end{equation}
For $t\to \infty$ we obtain \eqref{eq:dist_to_zero} and this concludes the proof of the first part of the statement.\\

To show the strong convergence of  $(x_t)_{t\in\N}$, notice that, in the case of exact proximal computation, \eqref{eq:eps:FB_estimate} from \autoref{prop:eps:FB estimate g-convex} can be further estimated 
\begin{align*}
%\label{eq:ineq_without_eta_exact}
\left(f+g\right)\left(\overline{x}\right)-\left(f+g\right)\left(x_{t+1}\right) 
   \geq  \left(\frac{1}{2\alpha } - \frac{L_g}{2}\right)\| x_t - x_{t+1}\|^2 +\left(\frac{1}{2\alpha } -\frac{\rho}{2}\right)\left\Vert \overline{x} -x_{t+1}\right\Vert ^{2} -\frac{1}{2\alpha }\|x_t-\overline{x}\|^2,    %\end{multline}
   \end{align*}
   with $\overline{x}\in S$. Following the proof of \autoref{lem:inthetube:const}, we estimate the LHS by \eqref{eq:sharpness:local:def}

\begin{align*}
   -\mu \operatorname{dist}(x_{t+1},S) + \frac{1}{2\alpha_{t}}\operatorname{dist}^2(x_t,S) \geq\left(\frac{1}{2\alpha_{t}}-\frac{\rho}{2}\right)\operatorname{dist}^2(x_{t+1},S)
    +\left(\frac{1}{2\alpha_{t}}-\frac{L_{g}}{2}\right)\left\Vert x_{t}-x_{t+1}\right\Vert ^{2}.
%\end{aligned}\end{equation}
\end{align*}
Multiplying  both sides by $2\alpha_t$ we get
\begin{equation}\label{eq:the_inequality_above}
\begin{aligned}
   -2\alpha_t\mu \operatorname{dist}(x_{t+1},S) + \operatorname{dist}^2(x_t,S) \geq\left({1}-{\rho}\alpha_t\right)\operatorname{dist}^2(x_{t+1},S)
    +\left(1-{L_{g}}\alpha_t\right)\left\Vert x_{t}-x_{t+1}\right\Vert ^{2}.
\end{aligned}\end{equation}
Let now $t\geq t_0$. From \eqref{eq:the_inequality_above} we infer
\begin{align}
\label{eq:ineq:exactcase}
&\left(1-\alpha_{t}L_{g}\right)\left\Vert x_{t}-x_{t+1}\right\Vert ^{2}  \leq \operatorname{dist}^2(x_{t},S)-\operatorname{dist}^2(x_{t+1},S)+\underbrace{\alpha_{t}\rho \operatorname{dist}^2(x_{t+1},S)-2\mu\alpha_{t}\operatorname{dist}(x_{t+1},S)}_{<0}
% & \leq \operatorname{dist}^2(x_{t},S)
\end{align}
where for every $t\geq t_0$, by virtue of \autoref{cor:1st result of inexact case} we have
\begin{equation}
    \alpha_t\operatorname{dist}(x_{t+1},S)\left(\rho\operatorname{dist}(x_{t+1},S) - 2\mu\right) <0.
\end{equation}
Since all the terms depending on $x_{t+1}$ in the RHS of \eqref{eq:ineq:exactcase} are negative, 
%we infer the following upper bound for $\left(1-\alpha_{t}L_{g}\right)\left\Vert x_{t}-x_{t+1}\right\Vert ^{2}$ in the form
\begin{flalign}
\quad (t\geq t_0) && \left(1-\alpha_{t}L_{g}\right)\left\Vert x_{t}-x_{t+1}\right\Vert ^{2} \leq  \operatorname{dist}^2(x_{t},S)&&&
\end{flalign}
Given the upper-bound for $\alpha_t $, %we have that  
% \begin{flalign}
%   \quad  (\forall t \in\N) && (1- \overline\alpha L_g) \leq (1- \alpha_t L_g) &&&
% \end{flalign}
\sloppy we obtain the following lower bound for $\left(1-\alpha_{t}L_{g}\right)\left\Vert x_{t}-x_{t+1}\right\Vert ^{2}$:
\begin{flalign}
\quad(\forall t \in\N) && \left(1-\overline\alpha L_{g}\right)\left\Vert x_{t}-x_{t+1}\right\Vert ^{2} \leq  \left(1-\alpha_{t}L_{g}\right)\left\Vert x_{t}-x_{t+1}\right\Vert ^{2}.&&&
\end{flalign}
By taking $\xi>1$ from \eqref{eq:zeta_uniform},  for every $t> t_0$ we have
\begin{equation}
\left\Vert x_{t}-x_{t+1}\right\Vert \leq\frac{\operatorname{dist}(x_{t},S)}{\sqrt{1-\alpha_{t}L_{g}}}\leq \frac{\operatorname{dist}(x_{t},S) }{\sqrt{1-\overline{\alpha}L_g}}< \frac{1 }{\sqrt{1-\overline{\alpha}L_g}}\left(\frac{1}{\sqrt{\xi}}\right)^{t-t_0}\operatorname{dist}(x_{t_0},S),
\end{equation}
where the last inequality is a consequence of \eqref{eq: linear convergence:2}. In conclusion, 
%we obtain 
\begin{equation}
    \sum_{t=0}^{\infty} \|x_{t}- x_{t+1}\| < +\infty
\end{equation}
 which means that $(x_{t})_{t\in\N}$ is a Cauchy sequence, hence it converges to some $x^*\in\X$ (see {\cite[Theorem 1]{Bolte2014_Proximal}}). Eventually,  $x^*\in S$ because $S$ is closed 
 %(as it is the level set of a lower semicontinuous function) 
 and $\operatorname{dist}(x_t,S)\rightarrow 0$ as $t\rightarrow +\infty$.
\end{proof}
}
{
\begin{remark}[Strong Convergence in the exact convex case]
Under the sharpness condition  \eqref{eq:sharpness:def}, for convex function ($\rho=0$) with exact proximal calculation ($\varepsilon=0$), starting from \eqref{eq:distance with eta and eps} one can arrive at 
\begin{equation*}
    2\alpha\mu \dist(x_{t+1},S) \leq \dist^2(x_t,S) - \dist^2(x_{t+1},S).
\end{equation*}
Taking the sum for $t=0,\dots,T$ yields
\begin{equation*}
    2\alpha\mu \sum_{t=0}^{T}\dist(x_{t+1},S) \leq \sum_{t=0}^{T}\left(\dist^2(x_t,S) - \dist^2(x_{t+1},S)\right) = \dist^2(x_0,S).
\end{equation*}
Hence, for $T\to \infty$, 
\begin{equation*}
    2\alpha\mu \sum_{t=0}^{\infty}\dist(x_{t+1},S) \leq \dist^2(x_0,S) < +\infty,
\end{equation*}
which implies $\dist(x_t,S) \to 0$ as $t\to\infty$. In conclusion, we obtain   \cite[Theorem 3.4(d)]{combettes2005signal}. %which is the strong convergence of the sequence $(x_t)_{t\in\N}$ to a solution.
\end{remark}
}
\section{Feasibility Problem}\label{sec:feasibility}

% One of these schemes is 
% \begin{equation*}
%     \minimize{x\in \X}\,\frac{\dist^2(x,Q)}{2}+\frac{\dist^2(x,C)}{2}.
% \end{equation*}

We show that the feasibility problem for a sphere and a closed convex set $C\subset \X$ can be modelled as 
\begin{equation}
    \label{FP0}
    \tag{FP}
    \minimize{x\in \X}\,f(x)+\frac{\dist^2(x,C)}{2},
\end{equation}
where $f$ is a weakly convex function.
The problem \eqref{FP0} is an instance of
\eqref{eq: ourproblem} and satisfies the assumptions of \autoref{theo:Et constant convergence} and \autoref{prop:convergence_convex:new}. In particular, the objective of \eqref{FP0} is sharp.

We denote the unit sphere centred in the origin r with $\mathcal{S}$. 
% Gabriele's example shows that the proof in Feasibility.tex is somehow wrong, as we can see from the following image.
%  \begin{minipage}[c]{0.47\textwidth}
%     \includegraphics[width=0.4\textwidth]{Feaswrong.png}
%     \label{fig:test}
%   \end{minipage}\hfill
%   \begin{minipage}[c]{0.5\textwidth}
% $Dist(x,S)$ in blue, $Dist(x,S) +2\|x\|^2$ in red.
% \label{fig:Feaswrong}
%   \end{minipage}
 The projection of $x\in \X\setminus \{0\}$ onto the unit sphere $\mathcal{S}$ equals $x/\|x\|$. To model our feasibility problem, we define the function $f:\X\rightarrow \R$ as
{
\begin{flalign*}
 \quad   \left(\forall x \in \X\right)&& f(x) := \begin{cases}
        |\|x\|^2-\|\frac{x}{\|x\|}\|^2| &\text{if}\,\, x\neq 0\\
        1 &\text{if}\,\, x = 0
    \end{cases}  \Rightarrow\,f(x) := |\|x\|^2-1|,&&
\end{flalign*}}
and we highlight that $\min_{x\in\X}\, f(x)=0$ and $\argmin_{x\in\X}\, f(x)=\mathcal{S}$


\begin{lemma}\label{lem:weak}
    \label{lem: f wc} Function 
    $f$ is $2$-weakly convex.
\end{lemma}

\begin{proof} { By \autoref{ex: weak convexity},}
 for every $x\in\X$, ${f(x)+\|x\|^2 =\max\{2\|x\|^2-1,1\}}$, \emph{i.e.\ }$f$ is convex.
 %as the pointwise maximum of convex functions. %\autoref{fig:comparisons:2} shows the plot of function $f + \rho\|x\|^2$ for different choices of $\rho$.
\end{proof}

%The following Lemmas shows that $f$ is weakly convex and sharp and 
\autoref{fig:comparisons} illustrates a relation between function $f$ and the distance from $\mathcal{S}$ when $\X = \R$.

\begin{lemma}
    \label{lem: f wc sharp} Function
    $f$  satisfies the sharpness condition with constant $\mu=1$.
\end{lemma}
\begin{proof}
    % \begin{flalign}
    % \quad (\forall x \in \X ) &&    \dist(x, \mathcal{S}) = |\|x\|-1 &&&
    % \end{flalign}
  See Appendix \ref{app:proof_of_lemma_sharp}. 
\end{proof}
% \begin{figure}
%     \centering
%    \begin{tikzpicture}
% \draw[->] (-2, 0) -- (2, 0) node[right] {$x$};
%   \draw[->] (0, -0.5) -- (0, 2) node[above] {$y$};
%   \draw[scale=1, domain=-2:2, smooth, variable=\x, blue] plot ({\x}, {abs(\x*\x - 1)});
%   \draw[scale=1, domain=-2:2, smooth, variable=\x, red]  plot ({\x},{abs(abs(\x) - 1)});
% \end{tikzpicture}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure}






\begin{figure}
\centering
\resizebox{0.25\textheight}{!}{
\begin{tikzpicture}
\begin{axis}[
    axis lines = center,
    % xlabel = $\|x\|$,
    %ylabel = {$f(x)$},
    legend style={at={(0.5,1.2)}, anchor=north,legend columns=-1}
]

\addplot [
    domain=-2:2, 
    samples=300, 
    color=black,
    ]
    {abs(x^2 -1)};
\addlegendentry{$|\|x\|^2 -1|$}
\addplot [dashed,
    domain=-2:2, 
    samples=300, 
    color=black,
]
{abs(abs(x)-1)};
\addlegendentry{$|\|x\| -1|$}
\end{axis}
\end{tikzpicture}}
\caption{Comparison between function $f$ (continuous line) and the distance from $S$ (dashed line) for $\X = \R$.}
    \label{fig:comparisons}
\end{figure}



The  properties of the function $\fonc{g}{\X}{\R}$,  $g:=\frac{\dist^2(\cdot,C)}{2} $, 
 are summarised in the following lemma.

\begin{lemma}[{\cite[Corollary 12.30, Example 13.5]{Bauschke2017}}]\label{lem:dist}
    Let $C\subseteq \X$ be a convex set. Then 
    \begin{flalign}\label{eq:dist_func:diff_conv}
 \quad\left(\forall x \in \X\right) &&  
    g(x) = \frac{\dist^2(x,C)}{2} = \frac{\|x\|^2}{2} - \left( \frac{\|\cdot\|^2}{2} + \iota_C\right)^*(x), &&&
\end{flalign}
{ where $\left( \frac{\|\cdot\|^2}{2} + \iota_C\right)^*$ is the Fenchel conjugate of function $ \frac{\|\cdot\|^2}{2} + \iota_C$ {\cite[Definition 13.1]{Bauschke2017}}}. If, in addition, $C$ is closed, then  $\dist^2(\cdot,C)$ is Fréchet differentiable on $\X$ and 
    \begin{flalign}  \quad\left(\forall x \in \X\right) &&\nabla \frac{\dist^2(x,C)}{2} = x - \proj_C(x).&&&
    \end{flalign}
\end{lemma}
 %We highlight
 Note that $\nabla g$ is Lipschitz continuous 
 %as the sum of two Lipschitz continuous functions. With the explicit form of $f$ and $g$, 
 and the feasibility problem (FP) becomes
\begin{equation}
\label{FP}
\tag{FP0}
\minimize{x\in\X}\, |\|x\|^2-1|+\frac{\dist^2(x,C)}{2},
\end{equation}
{where the objective function is non-convex. According to \autoref{lem:dist}, the problem can be recast as 
\begin{equation}
    \minimize{x\in\X}\, |\|x\|^2-1|+ \frac{\|x\|^2}{2} - \left( \frac{\|\cdot\|^2}{2} + \iota_C\right)^*(x),
\end{equation}
where  $x\mapsto |\|x\|^2-1|+ \frac{\|x\|^2}{2} $ is $1$-weakly convex by \autoref{lem:weak} and function ${x\mapsto- \left( \frac{\|\cdot\|^2}{2} + \iota_C\right)^*(x)}$ is the negative of a conjugate function, 
hence concave 
%since a Fenchel conjugate function is always convex 
\cite[Proposition 13.13]{Bauschke2017}).\\ }
\begin{example}
\label{ex: prox calculus}
    We give an explicit form of proximal operator for the function $f = |\Vert \cdot\Vert^2 -1|$. Consider $0<\alpha <1/2$ and $y \in \X$. Our aim is to solve the minimisation problem
    \begin{equation}
        \argmin_{x\in\X} f(x) +\frac{1}{2\alpha} \Vert x-y\Vert^2.
    \end{equation}
    Let $x\in \X$ be its unique minimiser. 
    %We have the following:
 If $\Vert x\Vert >1$, then $x= \frac{y}{2\alpha +1}$; if $\Vert x\Vert <1$, then $x = \frac{y}{1-2\alpha}$; if $\Vert x\Vert =1$, then for any $x\in X$ such that $\Vert x\Vert=1$, we have
        \begin{equation}
            f(x)+\frac{1}{2\alpha} \Vert x-y\Vert^2 =\frac{1}{2\alpha} \Vert x-y\Vert^2 \geq \frac{(\Vert y\Vert -1)^2}{2\alpha}.
        \end{equation}
        Hence $x = \frac{y}{\Vert y\Vert}$ when $\Vert y\Vert\neq 0$, otherwise we can take any $x$ in the unit sphere. In conclusion, 
\begin{flalign}\label{eq:prox_of_f}
\quad(\forall y \in \X)&&
    \prox_{\alpha f} (y) = \begin{cases}
        \frac{y}{1+2\alpha} & \text{ if } \Vert y \Vert >1+2\alpha\\
        \frac{y}{1-2\alpha} & \text{ if } \Vert y \Vert <1-2\alpha\\
        \frac{y}{\Vert y\Vert} & \text{ otherwise. } 
    \end{cases}&&&
\end{flalign}
\end{example}

The sharpness of $f+g$, 
%which is the core of our convergence result, 
holds by virtue of the following theorem, where $h_1 = f$ and $h_2 = g$.
\begin{theorem}\label{theo:sharpness 1}
    Let functions $\fonc{h_1,h_2}{\X}{\R}$ be proper and let the following assumptions be satisfied:
    \setlist[enumerate,1]{label={(\roman*)}}
    \begin{enumerate}
    \setlength{\itemindent}{+.3in}
        \item the global minimiser set $S$ of $h_1+h_2$ is equal to (or contained in) $S_1\cap S_2$, if both non-empty or $S\subset S_1$ where $S_1,S_2$ are the sets of the minimisers of $h_1$ and $h_2$ respectively;
        \item the optimal value of $h_1+h_2$ is zero, $v_{opt} =0$;
        \item $h_1(x)\geq 0$, $h_2(x) \geq 0$ for all $x$ in a neighborhood of $S$;
        \item $h_1$ is sharp with respect to $S$ locally or globally with constant $\mu>0$ and ${\inf_{x\in \X} h_1(x) =0}$.
    \end{enumerate}
    Then $h_1+h_2$ is locally (globally) sharp.
\end{theorem}
\begin{proof}
By \emph{(iv)} there exists $\delta_1> 0$ such that 
% \begin{flalign*}
%   \quad\left(\forall  x\in B(S,\delta_1)\right) && h_1(x) \geq \mu \dist (x,S).&&&
% \end{flalign*}
for every $x\in B(S,\delta_1)$ we have $h_1(x) \geq \mu \dist (x,S).$
By $(iii)$, there exists $\delta_2> 0$ such that for every $ x\in B(S,\delta_2)$ function $h_2$ is non-negative, hence by taking $\delta=\min\{\delta_1,\delta_2\}$ we have 
\begin{flalign*}
\quad  \left(\forall  x\in B(S,\delta)\right) && h_1(x) + h_2(x) \geq \mu \dist (x,S).&&&
\end{flalign*}
By $(i)$ and $(ii)$, the above inequality corresponds to the local sharpness of function $h_1+h_2$.
\end{proof}

\subsection{Numerical simulation}
\subsubsection{Exact FB simulation}
\label{sec:exact_exp}
The first simple experiment that we show consists in finding the intersection of a unit sphere in $\R^n$ and a line $L$ \emph{i.e.\ }
\begin{equation}
\label{easy_FP}
    \text{find }z\in \mathbb{R}^n,\ \ z\in \mathcal{S}\cap L.
\end{equation}
The line $L$ is defined as
$
L:=\{\overline{x}+\lambda e_2\ |\ \lambda\in \mathbb{R} \},
$
where $\overline{x}\in \mathbb{R}^n$, with all components equal zero except for the first component which is equal to $0.5$, and $e_2\in \mathbb{R}^n$, with all components equal to zero except for the second components equal to 1.

Problem \eqref{easy_FP}, modelled as  \eqref{FP}, has been solved in \cite{benoist2015douglas} with the Douglas-Rachford algorithm.
% \hung{Consider to remove:
% $\mathcal{S}$ in $\mathbb{R}^n$,
We applied the FB and the Douglas-Rachford algorithm for different starting points and for different dimensions $n$. 
The step size of the FB was set to $\alpha=0.499$ (notice that the step size must satisfy $0< a < 0.5$). 
We stopped both algorithms when the distance between two successive iterations satisfied $\|x_t-x_{t+1}\|\le 10^{-10}$.
Every entry of the starting points $z_0$ was calculated by a pseudo-random uniform distribution between $(0,1)$ with different seeds (we only show the results for the seed $8$, as no significant difference in the behaviour of the algorithms was observed by changing the seeds).

 \autoref{fig: DR_FB} shows that, as the dimension $n$ of the space increases, the FB outperforms the Douglas-Rachford method in the minimization of problem.

\begin{figure}[h]
\label{fig: DR_FB}
    \centering
    \begin{minipage}{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DtS_n=2_seed=8ok.png} % first figure itself
    \end{minipage}\hfill
    \begin{minipage}{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DtS_n=1000_seed=8ok.png} % first figure itself
    \end{minipage}\hfill
    \begin{minipage}{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DtS_n=10000_seed=8ok.png} % second figure itself
    \end{minipage}
    \caption{Comparison between DR and FB for the feasibility problem of a sphere and a line (seed=8). From left to right, n=2, 1000, 10000. }
\end{figure}
\subsubsection{Inexact FB simulation}
{
 In this example, we exploit the inexact FB to find a point at the intersection of three circles and a line in $\R^2$. 
 The inexact proximal calculation can be performed using Scipy's built-in BFGS method to solve the smooth surrogate problem defined in Appendix \ref{app:inexprox}. All the calculations are done in Python with the Scipy package.
 % of a function $F$ as the one appearing in \eqref{eq:source}, we employed the strategy outlined in Appendix \ref{inexprox} and used 
 
 
In this setting, we have $\mu = 1$, $L_g=1$ and we consider $\rho = 3$, so that inexactness parameter needs to satisfy the condition $\varepsilon< 0.025$ -- see \eqref{eq:cond:epsilon}. We test for various values of \linebreak $\varepsilon\in \{0.0249, 0.01, 0.005, 0.0025, 0.001\}$ to analyse the algorithm's behaviour depending on different levels of accuracy.
 \autoref{fig:3circles} (left) illustrates the configuration we considered along with the trajectories of the estimated sequences $x_t$ for different starting points $x_0$. \autoref{fig:3circles} (right) shows the distance from the iterates $x_t$ to the ground truth $x^*$ over 20000 iterations for $x_0 =(5,5)$. All choices of $\varepsilon$ yield the same global behaviour for $\|x_t-x^*\|$, which monotonically decreases until a certain distance from the solution set is reached, exactly as stated in \autoref{theo:Et constant convergence}. Moreover, the convergence is faster as $\varepsilon$ becomes smaller: after five iterations, the sequence generated with $\varepsilon=0.001$ is approximately three orders of magnitude closer to the ground truth than the sequence generated with $\varepsilon=0.0249$. 
\begin{figure}
    \centering
    \includegraphics[height = 2.3in]{3CIRCLES_NEW_INIT_correctedgrad.png}
    % \includegraphics[width=0.48\textwidth]
    % {3CIRCLES_NEW.png}
    \includegraphics[height = 2.3in]{3_CIRCLES_corrrected_grad.png}
    \caption{Feasibility problem between three circles and a line: Trajectories of the sequences generated at different starting points (left); distance from the iterates to the solution set for different levels of accuracy $\varepsilon$ (right).}
    \label{fig:3circles}
\end{figure}
}

\section{Feasibility problem with separable functions}
\label{sec:FBsep}
In this section we consider the Hilbert space $\X = \bigtimes_{i=1}^n \X_i$ where $\X_i$ is a Hilbert space,  $i=1,\dots,n$. 
The inner product on $\X$ is defined as
 $\langle x\ |\ y \rangle=\sum\limits_{i=1}^n\langle x_i\ |\ y_i\rangle_i$ for  $(x,y)\in \X\times \X$, and  $\|\cdot\|_{\X} = \sqrt{\sum_{i=1}^n \|\cdot\|_i^2}$ where $\|\cdot\|_i$ is the norm of $\X_i$.\\

Phase retrieval and Source localization problems can be modelled by feasibility problems,
\begin{equation}
\label{eq: lukeproblem}
  \text{find}\, x\in \bigcap\limits_{j=0}^m C^j,
\end{equation}
where each $C^j\subset \X_i$ is defined as
$
    C^j := \bigtimes_{i=1}^n C_i^j = \bigtimes_{i=1}^n \{x\in\X_i \,|\, \|x-c_i^j\| = r_i^j\}\subset \X_i,
$
for $m,n\in\N$ and given $c_i^j \in \X_i, b^j_i \in (0,+\infty)$. The set $C^0\subset \X$ represents qualitative constraint and may be convex. See \cite{luke2019optimization} for further details on this modelisation. %Denoting $\X = \bigtimes_{i=1}^n\X_i$, with  the inner product as
% $\langle x\ |\ y \rangle=\sum\limits_{i=1}^n\langle x_i\ |\ y_i\rangle_i$ for any $(x,y)\in \X\times \X$, and the norm as $\|\cdot\|_{\X} = \sqrt{\sum_{i=1}^n \|\cdot\|_i^2}$ where $\|\cdot\|_i$ is the norm of $\X_i$.
 
%Each set $C_i^j$ can be modeled by the following function 

For  $i=1,...,n$, $j=1,...,m$,  a point $\overline{x}_i \in C_i^j$
if and only if $f_i^j(\overline{x}_i)=0$, where ${f_i^j:\X_i\rightarrow [0, +\infty]}$,
\begin{flalign}\label{eq:f_j}
\quad(\forall x_i\in \X_i) &&f_i^j(x)=|\|x_i-c_i^j\|^2_i-(r_i^j)^2|. &&&
\end{flalign}
The functions $f_i^j $,  $i=1,...,n$,  $j=1,...,m$, are $\rho$-weakly convex with $\rho = 2$ and sharp with constant $\mu_i=1$ by analogous arguments as in \autoref{sec:feasibility}. For  $j=1,...,m$, let $F^j:\X\rightarrow [0,+\infty]$, 
\begin{flalign}
\label{eq: source_loc_fun}
(\forall x \in \X) && F^j(x):=\sum\limits_{i=1}^n f_i^j(x). &&&
\end{flalign}
A point $\overline x \in C^j$ if and only if  
$
F^j(\overline x)=\sum\limits_{i=1}^nf_i(\overline x_i)= 0
$ \emph{i.e.\ } $f_i^j(\overline x)=0$, for all $i\in \{1,...,n\}$. The function $F^j$ is weakly convex and sharp thanks to its separable structure. The moduli of weak convexity and sharpness are inherited from $f_i^j$, which are $\rho=\rho_i$ and $\mu =\mu_i$.\\

\paragraph{Source Localization} For these problems,  $m\leq 1000$, $n=1$ and $\X = \R^2 $ or $\X = \R^3$ (we omit the index $"i"$).
%The locations of each sensor are described by $c_j$ while $r^j$ is the communication data between each sensor and the source. 
The aim is to locate the source of a signal, knowing the location $c_j$ of the $m$ sensors and the registered distance $r_j$. When the intersection is nonempty, problem \eqref{eq: lukeproblem}  is equivalent to 
\begin{equation}\label{eq:source} \text{find}\ x\in \argmin_{z\in C_0}\  F(x):=\sum\limits_{j=1}^{m} F^j(x) = \sum\limits_{j=1}^{m}|\|x-c^j\|^2-(r^j)^2|,
\end{equation}
where $F$ is weakly convex
%as the sum of weakly convex and sharp functions, 
by \autoref{theo:sharpness 1}.  The problem  \eqref{eq:source} can be rewritten as the following feasibility problem 
\begin{equation}\tag{FP1}
    \label{eq: lukeproblem2}
    \argmin_{x\in \X}\ \ F(x)+\frac{\dist^2(x,C_0)}{2},
\end{equation}
which is an instance of problem 
\eqref{eq: ourproblem}, when $C_0$ is convex.

In real-life source localization problems, measurements might be affected by a certain degree of uncertainty and we replace the function \eqref{eq: source_loc_fun}  in \eqref{eq:source} with a function $\overline{F}^j:\X\times\mathbb{R}^m\rightarrow [0,+\infty]$, 
\begin{flalign}
\label{eq: source_loc_fun_inex}
(\forall (x,\delta) \in \X\times\mathbb{R}^m) && \overline{F}^j(x,\delta):=\sum\limits_{j=1}^m \overline{f}^j(x,\delta^j)=\sum\limits_{j=1}^m|\|x-c^j\|^2-(r^j)^2-\delta^j|. &&&
\end{flalign}
Function $\overline{F}^j$ is weakly convex and (globally) sharp, as we show in the following remark.%for all $(x,\varepsilon)\in\{(x,\varepsilon)\ |\ x\in\X, (r^j)^2-\varepsilon^j \ge0  \}$ 
\begin{remark}
   Function $\overline{f}:\X \times \R\to \R$, with $f(x,y) = |\|x\|^2-y|$ is sharp. For every fixed $ y \in (0,+\infty)$, we denote by $S(y)=\left\{w\in\X\,|\,{y}=\|w\|^2\right\} \subset \X$ the set of global minimisers of $\overline{f}(\cdot,y)$. By \autoref{lem: f wc sharp}, for every $y\in[0,+\infty)$, $\overline{f}(\cdot,y)$ is sharp \emph{i.e.\ }
$$\left(\forall x \in \X\right) \qquad \qquad \overline{f}(x, y) \geq \dist(x, S(y)).$$
Moreover, the set $S(y)$ can be embedded into $\X\times[0,+\infty) $
\begin{equation}
    S(y)\times\{y\} \subset \bigcup_{y\in[0,+\infty)} S(y)\times \{y\}= \{(x,y) \in\X\times [0,+\infty): \Vert x\Vert^2 = y\}:=S
\end{equation}
which is the set of global minimisers of $\overline{f}$.
It then follows that 
\begin{flalign}
\left(\forall (x,y) \in \X \times [0,+\infty)\right) && \overline{f}(x, y)\geq \dist(x, S(y)) = \dist((x,y), S(y)\times\{y\})
\geq \dist((x,y), S)&&&
\end{flalign}
and the last inequality yields the sharpness of function $\overline{f}$. 
The same reasoning can be applied to prove the sharpness of the function $\overline{f}:\X \times \R\to \R$ $f(x,y) = |\|x-c\|^2 - (y+r^2)|$ for given  $(c,r)\in\X\times \R$, which vanishes on the set $\left\{(x,y)\in \X \times \R\, | \,\|x-c\|^2 = y+r^2\right\} \subset \X\times [-r^2,+\infty)$.
 
\end{remark}


The source localisation problem becomes 
\begin{equation}\label{eq:source2} \text{find}\ (x,\delta)\in \argmin_{(x,\delta)\in \X \times C_0}\  \overline {F}(x,\delta):=\sum\limits_{j=1}^{m} \overline{F}^j(x) = \sum\limits_{j=1}^{m}|\|x-c^j\|^2-(r^j)^2-\delta^j|,
\end{equation}
where the convex constraint $ C_0:=\{\delta\in \mathbb{R}^m\ |\  \delta \in [\underline\delta, \overline{\delta}]\}, $
% \begin{equation}
%     \label{eq:c0_source_loc}
%     C_0:=\{\delta\in \mathbb{R}^m\ |\  \delta \in [\underline\delta, \overline{\delta}]\}, 
% \end{equation}
ensures that the exact location of the source is in the $\argmin$ of \eqref{eq:source2}. 
This is an advantage over other models 
%for source localization problems 
with measurement uncertainty, where the optimal solution only approximates the true location of the source (see \cite{beck2008exact}). \\

\paragraph{Phase Retrieval} The problem is to recover the original signal affected by the measurement error $c_i^j$ from observation data $r_i^j$. The dimensions of the problem are  $d=2,m$ ranges from $1$ to $10$ while $n$ can be up to $10000$. When $m=1$, we omit the index $"j"$ and the phase retrieval problem is equivalent to the feasibility problem of one sphere and a convex set in a product space. The phase retrieval problem for $m=1$ can be cast as
\begin{equation}
    \label{FP2}
    \tag{FP2}
    \minimize{x\in \X}\,F(x)+\frac{\dist^2(x,C_0)}{2}
\end{equation}
and \eqref{FP2} can be solved by \autoref{alg:epsFB}. In particular, since $F$ is a separable sum of functions of the form $f_i$, the proximity operator of $F$ is defined as
\begin{flalign}
\quad(\forall y\in \X) &&   \prox_{\alpha F}(y) = \bigtimes_{i=1}^n \prox_{\alpha f_i}(y^i), &&&
\end{flalign}
where each $\prox_{\alpha f_i}$ can be easily inferred from \eqref{eq:prox_of_f} in \autoref{ex: prox calculus}.

\subsection{Inexact FB for source localization}

We exploit the inexact FB to solve \eqref{eq:source2} to find the optimal solution of the source localization problem with measurement uncertainty \cite[Example 1]{beck2008exact}. Here $\X=\mathbb{R}^2$. 
Starting from points generated with a pseudo-random uniform distribution and different seeds, we use the stopping criterion $\|(x,\varepsilon)_n-(x,\varepsilon)_{n+1}\|\le 10^{-10}$. The exact solution is $(-2,3)$ and we show the approximated results obtained  in \cite{beck2008exact}.
\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c|c| }
 \hline
 \textbf{Seed} & \textbf{Iteration Num.}& \textbf{Output} \\
 \hline
 1 & 31 & $(-2.0166,  2.9687)$ \\ 
 \hline
 3 & 26 &$(-2.0414,   2.9801)$ \\ 
 \hline
\end{tabular}
\end{center}
\vspace{0.5cm}
\begin{center}
\begin{tabular}{ |c|c|c|c|c| }
 \hline
  \textbf{Method}& R-LS & SDR&  SR-LS&USR-LS\\ 
 \hline
 \textbf{Output} & $(-1.9907, 3.0474)$ &$(-1.7718, 3.36655)$& $(-2.018, 2.9585)$&$(-1.3450, 2.6238)$ \\ 
 \hline
\end{tabular}
\end{center}
\vspace{0.5cm}
\caption{First table: results with our approach for two different seeds. Second table: results from \cite{beck2008exact}.}
\end{table}


%\begin{center}
%\begin{tabular}{ |c|c|c|c| } 
%\hline
%\textbf{Seed} & $n$ & %\textbf{Iteration Num.} & %\textbf{x_{n+1}} \\ 
%\hline
%1 & 31 & $(-2.01663911,  2.96866688)$ \\ 
%3& &26& $(-2.0414224   2.98006106)$\\
%\hline
%\end{tabular}
%\end{center}

 \section{Discrete Tomography}
\label{sec:discrete_tomography}
% \paragraph{A sharp weakly convex function to promote binary solutions}
Let $\X  = \mathbb{R}^n$. Let us consider the function $\fonc{F}{\mathbb{R}^n}{\mathbb{R}}$ defined as
\begin{flalign}\label{eq:binaryfunction}
 \quad \left(\forall x\in \X\right) &&   F({x}) = {\sum_{i=1}^n |x_i^2-1|}, &&&
\end{flalign}
which is the sum of functions $f_i^j$ from \eqref{eq:f_j} for $j = 1$, $\X_i=\R$, $c_i=0$ and $r_i=1$ for every $i=1,\dots,n$. \sloppy By \autoref{lem: f wc sharp}, $F$ is a (globally) sharp and weakly convex function with the set of minimisers ${S = \{x=(x_1,\dots,x_n)\in\R^n\,|\, x_i \in\{-1,1\} \;\text{for}\; i=1,\dots,n\} = \{-1,1\}^n}$,
%and for this reason, this f
and can be seen as a non-smooth counterpart to the function 
 ${ x\mapsto  {\sum_{i=1}^n x_i^2(x_i-1)^2} }$,  $x\in\R^n$,
 proposed in \cite{Xu2021DoublyGraduated} to promote binary integer solutions (with values in $\{0,1\}^n$) in Markov Random Field approaches. To the best of our knowledge, this is the first work where function $F$ is used in such a context.

%\paragraph{Binary Tomography}
Binary Tomography (BT) is a special case of Discrete Tomography (DT) which aims at reconstructing binary images starting from a limited number of their projections \cite{SCHULE2005_DiscreteTomography, Kadu2029ConvexBinaryTomography}. 
%With (DT), images can be reconstructed relying on a much smaller data set than for the Computerized Tomography (CT). 
%The problem can be mathematically cast as follows:
The image $\overline x \in \R^n$ is represented as a grid of $n=n_1\times n_2$ pixels taking values $x_j \in \{-1,1\}$ for $j=1,\dots,n$. The projections $y_i$ for $i=1,\dots,m$ are linear combinations of the pixels along $m$ directions. The linear transformation from image to projections is modelled as 
\begin{equation}
    y=\mathbf{A}\overline x + \omega
\end{equation}
where $x_j$ denotes the value of the image in the $j$-th cell, $y_i$ is the weighted sum of the image along the $i$-th ray, the element $\mathbf{A}_{i,j}$ of matrix $\mathbf{A}\in\R^{m\times n}$ is proportional to the length of the $i$-th ray in the $j$-th cell and finally $\omega\in\R^n$ is an additive noise with Gaussian distribution and standard deviation $\sigma>0$. In general, the projection matrix $\mathbf A$ has a low rank, meaning that $\operatorname{rank}(\mathbf A) < \min\{ n,m\}$ \cite{Kadu2029ConvexBinaryTomography}. We cast the problem as
\begin{equation}\label{eq:binary1}
    \text{Find}\quad x\in\{-1,1\}^n\quad\text{such\,that}\quad \|y-\mathbf Ax\|_2 \leq \theta
\end{equation}
for some $\theta>0$, which can be reformulated as
\begin{equation}\label{eq:discrete_tomography_objective}
    \minimize{{x\in {C}}}  F(x) \quad \text{where}\quad {C} = \{x\in\R^n\,|\,\|\mathbf Ax-y\|_2\leq \theta\}
\end{equation}
with $F$ defined as in \eqref{eq:binaryfunction} and the constraint  set ${C}$, represented by the function
  $\dist^2(\cdot,\overline{B}(y,\theta))/2$. In conclusion, we have
\begin{equation}\label{eq:discrete_tomography_objective_final}
    \minimize{{x\in\R^n}}  F(x)  + \frac{\dist^2(\mathbf Ax,\overline{B}(y,\theta))}{2}.
\end{equation}
The sharpness of the objective function is ensured by \autoref{theo:sharpness 1}, assuming that the set ${S}\cap C \neq \emptyset$ and equivalently $S \cap \argmin \frac{\dist^2(\mathbf Ax,\overline{B}(y,\theta))}{2}  \neq \emptyset$ (for an adequate value of $\theta$, the original signal $\overline{x}$ belongs to the intersection). Clearly, for every $x\in \R^n$, $\iota_{\overline{B}(y,\theta)}(x) = \iota_{\overline{B}(0,\theta)}(x-y)$ and 
  \begin{equation}
      \proj_{\overline{B}(y,\theta)}(x) = y + \proj_{\overline{B}(0,\theta)}(x-y) = y + \left( (x-y) - \prox_{\theta \|\cdot\|_2}(x-y)\right) = x - \prox_{\theta \|\cdot\|_2}(x-y).
  \end{equation}
  It follows that, by \autoref{lem:dist}, for every $x\in\R^n$, $\nabla\left( \frac{\dist^2(\mathbf Ax,\overline{B}(y,\theta))}{2}\right) =  \mathbf A^\top\left(  \prox_{\theta \|\cdot\|_2}(\mathbf Ax - y)\right)$
   and $\|\mathbf A\|^2$ is a Lipschitz constant for this gradient.

\begin{remark}
    The model in \eqref{eq:binary1} and \eqref{eq:discrete_tomography_objective} (for appropriate choices of matrix $\mathbf A\in\R^{m\times n}$, vector $y\in\R^m$ and possible additional convex constraints) could be applied to other Binary Quadratic Programs (a special class of QCQP problems having the equality constraint $x_i^2=1$ for every $i\in\{1, \dots, n\}$ ) arising in Computer Vision, such as Graph Bisection, Graph Matching and Image Co-Segmentation (see \cite[Table 2]{Wang2016_BQ} and the references therein).
\end{remark}
% { \color{purple}
\paragraph{Numerical tests} For our simulations we used the MATLAB codes and data from \cite{Kadu2029ConvexBinaryTomography, githubpage}. We reproduced the same synthetic setting: we considered four phantoms (\emph{Apple}, \emph{Lizard}, \emph{Bell}, \emph{Bird}), corresponding to binary images of size 64 x 64 pixels. Operator $\mathbf A$ models an X-ray tomographic scan with 64 detectors and a parallel beam acquisition geometry with four angles (0°, 50°, 100°, 150°). We set $\sigma=0.01$ for the additive noise and $\theta = 10(64\sigma)^2$ in the constraint set $\overline{B}(y,\theta)$. \autoref{fig:discrete_tomography} illustrates, from left to right, the original image and the reconstructions obtained with the Least Squares QR method (LSQR), with the Truncated Least Squares QR method (TLSQR), with the DUAL method proposed in \cite{Kadu2029ConvexBinaryTomography} and finally with \autoref{alg:epsFB} applied to \eqref{eq:discrete_tomography_objective_final}, which we refer to as Continuously Relaxed Binary Tomography (CRBT). All methods are initialised with a vector of zeros.


\begin{figure}[htb]
    \centering
    \begin{tabular}{p{0.16\textwidth}p{0.16\textwidth}p{0.16\textwidth}p{0.16\textwidth}p{0.16\textwidth}}
    \phantom{ccc} ORIG & \phantom{cci} LSQR & \phantom{cc} TLSQR & \phantom{cci} DUAL & \phantom{cci}{CRBT} \\
  \multicolumn{5}{c}{\centering
    \includegraphics[width=0.9\textwidth,trim={0 0 0 13},clip]{apple.png}}\\
   \multicolumn{5}{c}{ \includegraphics[width=0.9\textwidth,trim={0 0 0 13},clip]{lizzard.png}}\\
   \multicolumn{5}{c}{ \includegraphics[width=0.9\textwidth,trim={0 0 0 13},clip]{bell.png}}\\
   \multicolumn{5}{c}{ \includegraphics[width=0.9\textwidth,trim={0 0 0 13},clip]{bird.png}}\\
      \end{tabular}
       \caption{comparisons between different solutions. From left to right: original image, least squares solution, thresholded least squares solution, solution obtained with the dual method proposed in \cite{Kadu2029ConvexBinaryTomography}, solution obtained with our model.}
    \label{fig:discrete_tomography}
\end{figure}
\section{Conclusions}
{We investigate the convergence properties of the exact and the inexact forward-backward algorithms for the minimisation of a function that is expressed as the sum of a weakly convex function and a smooth function with Lipschitz continuous gradient. In the inexact case, we inferred a convergence result that relies on the hypothesis that the accuracy level $\varepsilon> 0$ for the inexact proximal computation is kept constant throughout all the iterations. 
%To carry out our analysis, we exploited the notion of proximal $\varepsilon$-subdifferential. 
We successfully applied the analysed method to feasibility problems in the context of source localisation and binary tomography.
It will be interesting, in a future work, to extend our results so as to take into account non-constant accuracy levels $\varepsilon_t$.\\  } 
\paragraph{Funding} This work was funded by the European Union's Horizon 2020 research and innovation program under the Marie Sk{\l}odowska-Curie grant agreement No 861137. This work represents only the authors' view and the European Commission is not responsible for any use that may be made of the information it contains.

\bibliographystyle{acm}
\bibliography{references}

\begin{appendices}
\section{Proof of \autoref{cor:sh_versus_kl}}
\label{app:proof_of_equivalence}
The following theorem is inspired by {\cite[Theorem 5.2]{studniarski1999weak}},  originally in the finite-dimensional setting which can be easily adapted to the general Hilbert space setting.
\begin{theorem} \label{thm:stud_ward}
Let $\X$ be a Hilbert space and $h:\X\rightarrow\mathbb{R}$ be a continuous $\rho$-weakly convex function, $ h\geq 0 $,  $x^{*}\in S = \argmin_{x\in\X} h(x) = [h\leq 0]\neq\emptyset$.  Let $\delta_{0}>0$ and $r_0>0$. If there exists $\mu>0$ such that for every $x\in \overline{B}(x^{*},\delta_{0})\cap[0<h\leq r_0]$
\begin{flalign}
    \label{eq:kl1}
 \quad \left(\forall  z\in\partial_{\rho} h(x)\right) &&  \|z\|\ge \mu &&&
\end{flalign}
then there exists $\eta>0$ such that for every $ x\in {B}(x^{*},\eta)\cap[0<h<r_0]$
\begin{equation}
    \label{eq:sh1}
    \ \ h(x)-h(x^{*})\ge \mu\ \dist(x,{S})
\end{equation}
\end{theorem}


\begin{proof}[Proof of \autoref{cor:sh_versus_kl}]

\textbf{($(i) \implies (ii)$) } We assume \eqref{eq:prop:sh_loc} holds for all ${x\in  B(x^*, \delta_1)\cap [ 0 < h(x) < r_1]}$. For any $0 < \delta < \delta_1$ we consider $\overline{h} = h + \iota_{\overline{B}(x^*,\delta)}$. By \autoref{thm:sh_versus_kl}, we have
   \begin{equation}
     \inf_{x \in [0< \overline h<r_1]}\dist(\partial_{\rho/2} \overline h(x),0) = \inf_{0\leq r < r_1} \inf_{x\in[r <\overline  h< r_1]} \frac{ \overline h(x) - r}{\dist(x, [\overline h\leq r])} % \leq \inf_{x\in[0 < h< \beta]} \frac{h(x)}{\dist(x, [h\leq 0])} \leq \frac{h(x)}{\dist(x,[h\leq 0])} 
    \end{equation}
For the RHS we have
  \begin{equation}
   \operatorname{RHS} = \inf_{0\leq r < r_1} \inf_{x\in[r <  h< r_1] \cap \overline{B}(x^*,\delta)} \frac{  h(x) - r}{\dist(x, [ h\leq r])}
   \geq  \inf_{0\leq r < r_1} \inf_{x\in[r <  h< r_1] \cap {B}(x^*,\delta_1)} \frac{  h(x) - r}{\dist(x, [ h\leq r])}
   % \leq \inf_{x\in[0 < h< \beta]} \frac{h(x)}{\dist(x, [h\leq 0])} \leq \frac{h(x)}{\dist(x,[h\leq 0])} 
    \end{equation}
    % and if $SH_loc$ holds, then $RHS > \mu$
   %  and in particular we have
   %  \begin{equation}
   % RHS \geq  \inf_{0\leq r < r_1} \inf_{x\in[r <  h< r_1] \cap {B}(x^*,\delta_1)} \frac{  h(x) - r}{\dist(x, [ h\leq r])}. % \leq \inf_{x\in[0 < h< \beta]} \frac{h(x)}{\dist(x, [h\leq 0])} \leq \frac{h(x)}{\dist(x,[h\leq 0])} 
   %  \end{equation}
    because $\overline{B}(x^*, \delta) \subset{B}(x^*, \delta_1) $.
On the other side, by {\cite[Theorem 2]{bednarczuk2022calculus}}, the LHS is equivalent to 
 \begin{equation}
   \operatorname{ LHS} =  \inf_{x \in [0< h<r_1]}\dist(\partial_{\rho/2}  h(x) + N_{\overline{B} (x^*,\delta)}x,0).
    \end{equation}
where $N_{\overline{B} (x^*,\delta)}$ is the Normal Cone in the sense of convex analysis. For any $0<\delta_2 < \delta$ and for any $0<r_2<r_1$ we have
\begin{equation}
   \operatorname{ LHS} \leq   \inf_{x \in [0< h\leq r_2] \cap \overline B(x^*,\delta_2)}\dist(\partial_{\rho/2}  h(x)+ N_{\overline{B}(x^*,\delta)} x ,0) = \inf_{x \in [0< h\leq r_2] \cap \overline B(x^*,\delta_2)}\dist(\partial_{\rho/2}  h(x),0)
    \end{equation}
    where the last equality stems from the fact that  $N_{\overline{B}(x^*,\delta)}x=0$ for every $x \in B(x^*,\delta)$. In conclusion we obtain
    \begin{equation}
  \inf_{x \in [0< h\leq r_2] \cap \overline B(x^*,\delta_2)}\dist(\partial_{\rho/2}  h(x),0) \geq \inf_{0\leq r < r_1} \inf_{x\in[r <  h<  r_1] \cap {B}(x^*,\delta_1)} \frac{  h(x) - r}{\dist(x, [ h\leq r])}
    \end{equation}   
 %     \begin{equation}
 % \inf_{x \in [0< h\leq r_2] \cap \overline B(x^*,\delta_2)}\dist(\partial_{\rho}  h(x),0)  \geq  \inf_{x \in [0< h<r_1] \cap \overline B(x^*,\delta_2)}\dist(\partial_{\rho}  h(x),0) 
 %    \end{equation}
and by \eqref{eq:prop:sh_loc} we get
\begin{equation}
\inf_{0\leq r < r_1} \inf_{x\in[r <  h<  r_1] \cap {B}(x^*,\delta_1)} \frac{  h(x) - r}{\dist(x, [ h\leq r])} \geq \mu>0,
    \end{equation} 
which implies
%\begin{equation}
  $\inf_{x \in [0< h\leq  r_2] \cap \overline B(x^*,\delta_2)}\dist(\partial_{\rho/2}  h(x),0) \geq \mu >0$,
%\end{equation}
which is \eqref{eq:prop:kl_loc} with function $\varphi = \frac{1}{\mu}\operatorname{Id}$.\\

\textbf{($(i) \implies (ii)$) } The second part of the assertion follows from \autoref{thm:stud_ward}.
\end{proof}


\section{Proof of \autoref{lem:inthetube:const}}\label{sec:lem:inthetube:const}
%Let us present an auxiliary result about the behavior of $\dist(x_t,S)$ which is needed in the proof of \autoref{lem:inthetube:const}

\begin{proof}
{
Take an arbitrary but fixed $t\in\mathbb{N}$ and let $\operatorname{dist}(x_{t+1},S)>0$.
We take any $\overline{x}\in S$  and  apply \eqref{eq:eps:FB_estimate} from \autoref{prop:eps:FB estimate g-convex} for $x=\overline{x}$, which yields the following inequality:
\begin{equation}\label{eq:eps:FB_estimate:2}
\begin{aligned}
    (f+g)(\overline{x})  - (f+g)&(x_{t+1}) \geq \innerprod{\frac{x_t - x_{t+1} }{\alpha }}{\overline{x}-x_{t+1}} \\
    &  - \frac{\rho}{2}\|\overline{x}-x_{t+1}\|^2 - \frac{L_g}{2}\|x_{t}- x_{t+1}\|^2 - \varepsilon  - \sqrt{\frac{2\varepsilon }{\alpha }}\| \overline{x} - x_{t+1}\|.\\
    \end{aligned}
\end{equation}
%where $\operatorname{dist}(x_{t+1},S)>0$.
{ By using the identity
\begin{flalign*}
    \quad\left(\forall (w,y,z)\in\X^3\right) &&   \innerprod{w-y}{y-z} = \frac{1}{2}\|w-z\|^2 - \frac{1}{2}\|w-y\|^2 - \frac{1}{2}\|y-z\|^2, &&&
\end{flalign*}
which follows from \cite[Lemma 2.12]{Bauschke2017}, we obtain}
\begin{equation}\label{eq:ineq_without_eta}
\begin{aligned}
&\left(f+g\right)\left(\overline{x}\right)-\left(f+g\right)\left(x_{t+1}\right) \geq  \frac{1}{2\alpha }\left( -\|x_t-\overline{x}\|^2 +\|x_{t+1}- \overline{x}\|^2  + \| x_t - x_{t+1}\|^2\right) \\
    & \qquad-\frac{\rho}{2}\left\Vert \overline{x} -x_{t+1}\right\Vert ^{2}-\frac{L_{g}}{2}\left\Vert x_{t}-x_{t+1}\right\Vert ^{2} - \varepsilon  - \sqrt{\frac{2\varepsilon }{\alpha }}\left\Vert \overline{x}-x_{t+1}\right\Vert  \\
    & \qquad= \left(\frac{1}{2\alpha } - \frac{L_g}{2}\right)\| x_t - x_{t+1}\|^2 +\left(\frac{1}{2\alpha } -\frac{\rho}{2}\right)\left\Vert \overline{x} -x_{t+1}\right\Vert ^{2}-\frac{1}{2\alpha }\|x_t-\overline{x}\|^2 - \varepsilon   - \sqrt{\frac{2\varepsilon }{\alpha }}\left\Vert \overline{x}-x_{t+1}\right\Vert  .\\
    \end{aligned}
    \end{equation}
% Notice that for every $\eta>0$, the following inequality (known as Young's Inequality) holds
%  \begin{flalign}\label{eq:youngs}
%     \quad (\forall (u,v)\in [0,+\infty)^2)&& uv \leq \frac{1}{2} \left(\eta u^2 + \frac{1}{\eta}v^2 \right). &&
%  \end{flalign} Hence, at iteration $t$, we use {the constant $\eta >0$ } to estimate the last term in \eqref{eq:ineq_without_eta} according to \eqref{eq:youngs} {with $v=\sqrt{\frac{1}{\alpha }}\left\Vert \overline{x}-x_{t+1}\right\Vert$ and $u=\sqrt{2\varepsilon }$}, which yields
Using Young's Inequality, we have
\begin{equation}\label{eq:ineqeta}
     - \sqrt{\frac{2\varepsilon }{\alpha }}\left\Vert \overline{x}-x_{t+1}\right\Vert  \geq  - \frac{1 }{2 }\|\overline{x}- x_{t+1}\|^2 - \frac{\varepsilon }{\alpha }.
  \end{equation}
Plugging \eqref{eq:ineqeta} back into \eqref{eq:ineq_without_eta} yields
% , by combining \eqref{eq:ineq_without_eta} and \eqref{eq:ineqeta}, we obtain
\begin{equation}\label{eq:ineq_with_eta}
\begin{aligned}
&\qquad\qquad \left(f+g\right)\left(\overline{x}\right)-\left(f+g\right)\left(x_{t+1}\right) \geq \\
    &  \left(\frac{1}{2\alpha } - \frac{L_g}{2}\right)\| x_t - x_{t+1}\|^2 +\left(\frac{1}{2\alpha } -\frac{\rho+1}{2} \right)\left\Vert \overline{x} -x_{t+1}\right\Vert ^{2}%\\&\qquad\qquad
    -\frac{1}{2\alpha }\|x_t-\overline{x}\|^2 - \frac{\alpha+1}{\alpha}\varepsilon  .\\
    \end{aligned}
    \end{equation}
% \begin{equation}
% \begin{aligned}
%     &\left(f+g\right)\left(\overline{x}\right)-\left(f+g\right)\left(x_{t+1}\right) \\
%     &\geq  -\frac{1}{2\alpha_t}\|x_t-\overline{x}\|^2 +\left(\frac{1}{2\alpha_t} -\frac{\rho}{2} - \frac{\eta_{t}}{2\alpha_{t}} \right) \left\Vert \overline{x} -x_{t+1}\right\Vert ^{2} - \varepsilon_t - \frac{\varepsilon_t}{\eta_{t}}.\\
%     \end{aligned}
%     \end{equation}
We then estimate the LHS of \eqref{eq:ineq_with_eta} by the assumption of sharpness on $f+g$
 \begin{align*}
   -\mu \dist(x_{t+1},S) &\geq   \left(\frac{1}{2\alpha } -\frac{\rho+1}{2} \right) \left\Vert \overline{x} -x_{t+1}\right\Vert ^{2} - \frac{1}{2\alpha }\|x_t-\overline{x}\|^2  +\left(\frac{1}{2\alpha } - \frac{L_g}{2}\right)\| x_t - x_{t+1}\|^2 - \frac{\alpha+1}{\alpha}\varepsilon.
 \end{align*}   
By using \eqref{eq:cond:alpha:1} from \autoref{assu:conditions} and the fact that $\overline{x}\in S$ can be taken arbitrarily, 
%first we further estimate the RHS by dropping the positive term $\left(\frac{1}{2\alpha } - \frac{L_g}{2}\right)\| x_t - x_{t+1}\|^2$. Then, 
we choose $\overline{x}\in S$ so that 
%we have 
 $$\|x_t - \overline{x}\|^2 \geq \dist^2(x_{t},S)\qquad \text{and} \qquad \|x_{t+1}- \overline{x}\|^2 \geq \dist^2(x_{t+1},S)$$
 which yields
 \begin{align*}
   -\mu \dist(x_{t+1},S) &\geq   \left(\frac{1}{2\alpha } -\frac{\rho+1}{2} \right) \dist^2(x_{t+1},S) - \frac{1}{2\alpha }\dist^2(x_{t},S) - \frac{\alpha+1}{\alpha}\varepsilon
 \end{align*}   
 %and
 % $\frac{1}{2\alpha }\|x_t-\overline{x}\|^2$ to the LHS of the inequality, we observe that both terms involving $\overline{x}\in S$ have a positive sign
 % \begin{equation*}
 % \begin{aligned}
 % &\frac{1}{2\alpha }\|x_t-\overline{x}\|^2 -\mu \dist(x_{t+1},S) \geq \\
 %  &\qquad\left(\frac{1}{2\alpha } -\frac{\rho}{2} - \frac{\eta }{2\alpha } \right) \left\Vert \overline{x} -x_{t+1}\right\Vert ^{2} +\left(\frac{1}{2\alpha } - \frac{L_g}{2}\right)\| x_t - x_{t+1}\|^2 - \varepsilon  - \frac{\varepsilon }{\eta }. \\
 %  \end{aligned}
 % \end{equation*} 
 % This allows us to take the infimum over $\overline{x}\in S$ in both sides of the inequality, yielding the following inequality involving the distance to the solution set
 % \begin{equation}
 % \label{eq:distance with eta and Lg}
 % \begin{aligned}
 %   &\frac{1}{2\alpha }\dist^2(x_t,S) -\mu \dist(x_{t+1},S) \geq \\ &\qquad \left(\frac{1}{2\alpha } -\frac{\rho}{2} - \frac{\eta }{2\alpha } \right) \dist^2(x_{t+1},S) +\left(\frac{1}{2\alpha } - \frac{L_g}{2}\right)\| x_t - x_{t+1}\|^2 - \varepsilon  - \frac{\varepsilon }{\eta }
 %   \end{aligned}
 % \end{equation}
% Rearranging the terms in the inequality above, we obtain
  \begin{equation}
  \label{eq:distance with eta and eps}
  \begin{split}
      \iff \dist^2(x_t,S) \geq \left(1 -\alpha  \rho - \alpha  \right) \dist^2(x_{t+1},S) +2\alpha  \mu \dist (x_{t+1},S) 
        -2(\alpha+1)\varepsilon
      % +(1-\alpha L_g) \Vert x_t -x_{t+1}\Vert^2 .
  \end{split}
 \end{equation}

Let now $t=t_0$. By hypothesis \eqref{eq:dist:xtplus}, we have 
\begin{equation}\label{eq:ineq:discriminant}
\begin{aligned}
\left(1-\alpha \rho-\alpha \right)\text{dist}^{2}\left(x_{t+1},S\right)+2\alpha \mu\text{dist}\left(x_{t+1},S\right)-2(\alpha+1)\varepsilon \leq (E^+)^2\\
% \leq\left(\frac{\alpha \mu+\sqrt{\alpha ^{2}\mu^{2}-2\alpha \left(\varepsilon +\frac{\varepsilon }{\eta }\right)\left(\alpha \rho+\eta \right)}}{\alpha \rho+\eta }\right)^{2}.
\end{aligned}
\end{equation}
which is a second degree inequality with respect  $\dist(x_{t+1},S)$.
he discriminant has the form %(see \autoref{sec:appendix:discriminant})
\begin{equation}
    \Delta =\left(\frac{\mu+\left(1-\alpha\left(\rho+1\right)\right)\sqrt{\mu^{2}-2\varepsilon\left(\rho+1\right)\frac{\alpha+1}{\alpha}} }{1+\rho}\right)^{2}  \geq0.
\end{equation}
Since $\Delta\geq 0$, the solutions are within the following range
\begin{align}\label{eq:discriminant}
\frac{-\alpha \mu-\sqrt{\Delta}}{1-\alpha \rho-\alpha } & \leq \dist \left(x_{t+1},S\right)\leq\frac{-\alpha \mu+\sqrt{\Delta}}{1-\alpha \rho-\alpha }  =\frac{\mu+\sqrt{\mu^{2}-2\varepsilon\left(\rho+1\right)\frac{\alpha+1}{\alpha}} }{1+\rho} = E^+
\end{align}
% {\color{purple} \textbf{remove} where the lower bound is clearly negative, while the upper bound reads as 
% \begin{equation}
% \begin{aligned}
% \frac{-\alpha \mu+\sqrt{\Delta}}{1-\alpha \rho-\eta } & = \frac{\alpha \mu+\sqrt{\alpha ^{2}\mu^{2}-2\alpha \left(\varepsilon +\frac{\varepsilon }{\eta }\right)\left(\alpha \rho+\eta \right)}}{\left(\alpha \rho+\eta \right)}
% \end{aligned}
% \end{equation}}
% (see \autoref{sec:appendix:discriminant}). 
In conclusion, since the distance from a set is a non-negative value, we obtain
\begin{equation}
0\leq \dist \left(x_{t+1},S\right)\leq
% \frac{\alpha \mu+\sqrt{\alpha ^{2}\mu^{2}-2\alpha \left(\varepsilon +\frac{\varepsilon }{\eta }\right)\left(\alpha \rho+\eta \right)}}{\left(\alpha \rho+\eta \right)}=
E^+.
\end{equation}
which by induction proves \eqref{eq:dist:xtplus}. 
The same reasoning can be applied to obtain \eqref{eq:dist:xtminus}. %: see \autoref{sec:appendix:discriminant}.
}
\end{proof}

\section{Proof of \autoref{prop:estimation with Et}}\label{sec:prop:estimation with Et}
\begin{proof}[Proof of \autoref{prop:estimation with Et}]
{ The quantities $E^+$ and $E^-$ are roots of the quadratic equation
\begin{equation}\label{eq:equation_Et}
    (E^-)^2 = (1-\alpha \rho -\alpha)(E^-)^2  +2\alpha \mu E^- -2(\alpha+1) \varepsilon .
\end{equation}
}
 Subtracting \eqref{eq:equation_Et} from \eqref{eq:distance with eta and eps},
 %(from the proof of \autoref{lem:inthetube:const}
 we obtain the following inequality that holds for every $t\in\N$
 \begin{equation}
\begin{aligned}
    &\operatorname{dist}^2(x_{t },S)-(E^-)^2\geq \\
    &  (1-\alpha \rho-\alpha)(\operatorname{dist}^2(x_{t +1},S)-(E^-)^2) +2\alpha \mu (\operatorname{dist}(x_{t +1},S)-E^-)+(1-\alpha L_g) \Vert x_t -x_{t+1}\Vert^2\\
    &= (1-\alpha \rho -\alpha+ \frac{2\alpha \mu}{\operatorname{dist}(x_{t +1},S)+ E^-})\left(\operatorname{dist}^2(x_{t +1},S)-(E^-)^2\right)+(1-\alpha L_g) \Vert x_t -x_{t+1}\Vert^2.
    \end{aligned}\end{equation}
    Then, 
    \begin{align} \label{eq:eps distance square estimate with xi}
    \operatorname{dist}^2(x_{t },S)-(E^-)^2 \geq \zeta_{{t }+1} \left(\operatorname{dist}^2(x_{t +1},S)-(E^-)^2\right)+(1-\alpha L_g) \Vert x_t -x_{t+1}\Vert^2,
\end{align}
where 
${\zeta_{t +1} := 1-\alpha \rho -\alpha+ \dfrac{2\alpha \mu}{\operatorname{dist}(x_{t +1},S)+ E^-}  \geq 0 }$
is  non-negative  by \autoref{assu:conditions}.
By dropping the term $(1-\alpha L_g) \Vert x_t -x_{t+1}\Vert^2$ (positive by \eqref{eq:cond:alpha:1}), we have
\begin{align} \label{eq:eps distance square estimate with xi 2}
    \operatorname{dist}^2(x_{t },S)-(E^-)^2 \geq \zeta_{{t }+1} \left(\operatorname{dist}^2(x_{t +1},S)-(E^-)^2\right).
\end{align}
Now we show that at iteration $t\geq t_0$, we have $\zeta_{t+1}\geq 1$. Notice that  $\zeta_{t+1}\geq 1$  whenever 
%\begin{align}
    $\frac{2\alpha \mu}{\operatorname{dist}(x_{t +1},S)+ E^-} \geq \alpha \rho +\alpha.$
    %\end{align}
    Equivalently,
    \begin{align}
    \operatorname{dist}(x_{t +1},S) & \leq \frac{2 \mu}{\rho + 1} - E^-  =\frac{\mu +\sqrt{\mu^{2}-2\varepsilon(\rho+1)\frac{\alpha+1}{\alpha} }}{\rho+1} = E^+.
\end{align}
{By assumption, there exists $t_0$ such that $\dist(x_{t_0},S)\leq E^+$ and, by \autoref{lem:inthetube:const}, this implies that \linebreak ${\dist(x_{t+1},S)\leq E^+}$. In conclusion,   for every $t\geq t_0$ the condition $\zeta_{t+1} \geq 1$ holds.
}
\end{proof}

\iffalse
\section{Proof of \autoref{cor:1st result of inexact case}}\label{app:cor:1st result of inexact case}

\begin{proof}
In the case of exact proximal computation, \eqref{eq:eps:FB_estimate} from \autoref{prop:eps:FB estimate g-convex} can be further estimated 
\begin{multline}\label{eq:ineq_without_eta_exact}
\left(f+g\right)\left(\overline{x}\right)-\left(f+g\right)\left(x_{t+1}\right) 
   \geq  \left(\frac{1}{2\alpha } - \frac{L_g}{2}\right)\| x_t - x_{t+1}\|^2 +\left(\frac{1}{2\alpha } -\frac{\rho}{2}\right)\left\Vert \overline{x} -x_{t+1}\right\Vert ^{2} -\frac{1}{2\alpha }\|x_t-\overline{x}\|^2.
    \end{multline}    
    Following the proof of \autoref{lem:inthetube:const}, we estimate the LHS of \eqref{eq:ineq_without_eta_exact} by the sharpness assumption 
 \begin{multline}\label{eq:exact_sharpness_distance}
   -\mu \dist(x_{t+1},S) \geq \left(\frac{1}{2\alpha } - \frac{L_g}{2}\right)\| x_t - x_{t+1}\|^2 +\left(\frac{1}{2\alpha } -\frac{\rho}{2}\right)\left\Vert \overline{x} -x_{t+1}\right\Vert ^{2} -\frac{1}{2\alpha }\|x_t-\overline{x}\|^2.
\end{multline}  
By using the assumption that $\left(\frac{1}{\alpha } - {L_g}\right)>0$ and the fact that $\overline{x}\in S$ can be taken arbitrarily, we obtain 
 \begin{equation}
   -\mu \dist(x_{t+1},S) \geq \left(\frac{1}{2\alpha } -\frac{\rho}{2}\right)\dist^2(x_{t+1},S) -\frac{1}{2\alpha }\dist^2(x_{t},S), 
\end{equation}  
 \begin{equation}
   -2\alpha\mu \dist(x_{t+1},S) \geq \left(1 -\alpha\rho\right)\dist^2(x_{t+1},S) -\dist^2(x_{t},S).
\end{equation} 

Consider that, for a certain $t=t_0$, the inequality $\dist(x_{t_0},S)< \frac{2\mu}{\rho}$ holds. It follows that
\begin{equation}
     \left(1 -\alpha\rho\right)\dist^2(x_{t_0+1},S) +2\alpha\mu \dist(x_{t_0+1},S) -\left(\frac{2\mu}{\rho}\right)^2 \leq 0
\end{equation} 
To solve the above second degree inequality in  $\dist(x_{t_0+1},S)$: we  show that its discriminant $\Delta$ is positive and 
\begin{equation}
    \Delta = \alpha^2\mu^2 + (1-\alpha\rho)\left(\frac{2\mu}{\rho}\right)^2 = \frac{\rho^2\alpha^2\mu^2 - 4\alpha\rho\mu^2 + 4\mu^2}{\rho^2} = \left( \frac{2\mu -\alpha\rho\mu}{\rho}\right)^2 = \left( \frac{2\mu(1 -\alpha\rho)}{\rho}\right)^2>0.
\end{equation}
It then follows that 
\begin{equation}\label{eq:inthetubexact}
\dist(x_{t_0+1},S)< \frac{-\alpha\mu + \sqrt{\Delta}}{(1-\alpha\rho)} = \frac{-\alpha\mu + \frac{2\mu-\alpha\rho\mu}{\rho}}{1-\alpha\rho} = \frac{-2\alpha\rho\mu + 2\mu}{(1-\alpha\rho)\rho} = \frac{2\mu}{\rho}\end{equation}
and by induction for all $t\geq t_0$, $\dist(x_{t},S)<\frac{2\mu}{\rho}$, which is exactly \eqref{eq:Inequality2murho}.\\


Assume now that, for some $t\in\N$, $\dist(x_{t+1},S)>0$. By putting $\zeta_{t+1}= 1-\alpha\rho + \frac{2\alpha\mu}{\dist(x_{t+1},S)}>0$, we get
\begin{equation}
    \zeta_{t+1}\dist^2(x_{t+1},S)\leq \dist^2(x_{t},S).
\end{equation}
We now need to show that if $t\geq t_0$ we have $\zeta_{t+1}> 1$: this holds whenever

\begin{equation}
    -\alpha\rho + \frac{2\alpha\mu}{\dist(x_{t+1},S)} >0 \iff  {\dist(x_{t+1},S)} < \frac{2\mu}{\rho},
\end{equation}
which is exactly what we showed in \eqref{eq:inthetubexact}.
\end{proof}
\fi

\section{Proof of \autoref{lem: f wc sharp}}\label{app:proof_of_lemma_sharp}
\begin{proof}
 For  $x\in\X$, the distance to the sphere $\mathcal{S}$  is given as ${x}\mapsto{|\|x\|-1 |}$.
     We show that for $\mu=1$ we have
\begin{flalign*}
 \quad(\forall x \in \X)&&    |\|x\|^2 - 1| \geq \mu \dist(x,S).  &&&
    \end{flalign*}
% By applying the inverse triangle inequality we can further estimate the RHS as
% \begin{equation}
% \begin{aligned}
%     |\|x\|^2 - 1| \geq \mu \|x - s\| \geq \mu |  \|x\| - \|s\|| = \mu |  \|x\| - 1|
%     \end{aligned}
% \end{equation}
If $x=0$, then $\dist(x,\mathcal{S}) = 1$ and the inequality is satisfied for $\mu=1$. Otherwise, for every $x\in\X\setminus 0$ we have
\begin{equation*}
\min_{s\in \mathcal{S}} \|x-s\|  = 
    \|x- \frac{x}{\|x\|}\| = \| x \left(1 - \frac{1}{\|x\|}\right)\| = \|x\| \left| 1- \frac{1}{\|x\|}\right| = \|x\| \frac{1}{\|x\|} \left | \|x\| - 1\right| = |\|x\|-1|.
 \end{equation*}
% where the third equality stems from the absolute homogeneity of a norm.
Consider $ \fonc{\tilde f}{\R}{\R}$ defined as ${ \sigma \mapsto |\sigma^2-1|}$, \emph{i.e.\ }  the function from \autoref{ex: weak convexity} with  $(a,b)=(-1,1)$. The following inequality is satisfied for every $\delta \leq 1$ (as illustrated in \autoref{fig:comparisons})
\begin{flalign*}
\quad(\forall \sigma \in\R)  && \delta | |\sigma|-1| = \delta \min\{ |\sigma - 1|,  |\sigma +1| \}\leq  |\sigma^2 -1 |. &&&
\end{flalign*}
% We notice that if we restict to the positive semiline we obtain
% \begin{flalign}
% (\forall \sigma \in (0,+\infty))  && \delta |\sigma - 1| \leq  |\sigma^2 -1 |. &&&
% \end{flalign}
where the last inequality holds by the sharpness of function $\tilde f$ (see \autoref{ex:sharpness}). Choosing $\sigma = \|x\|$ we obtain
\begin{flalign*}
 \quad(\forall x \in \X\setminus 0)&&   \delta| \|x\| - 1| \leq   |\|x\|^2 - 1|. &&&
\end{flalign*}
Thus,   $\mu\dist(x,\mathcal{S})\leq   f(x)$, for  $\mu \in(0, 1]$, for $x \in \X$
%\begin{flalign*}
% \quad(\forall x \in \X)&&   \mu\dist(x,%\mathcal{S})\leq   f(x) &&&
%\end{flalign*}
which implies the global sharpness of  $f$ with  $\mu=1$.
\end{proof}

\section{Inexact Proximal Computation}\label{app:inexprox}
{

%To compute inexact proximal points, we rely on the strategy based on the definition of \emph{$\varepsilon$-proximal points} 
%as \emph{$\varepsilon$-solutions} of the corresponding optimisation problem 
%(see \autoref{def:eps_prox}). \\

For any strongly convex function $h$ and its unique minimiser $\overline{x}$,  
%the  level set
\begin{equation}
    \operatorname{lev}_{\leq \varepsilon + h(\overline{x})} h : = \{x\in\X\,|\, h(x) \leq \varepsilon + h(\overline{x})\}
\end{equation}
is the set of  $\varepsilon$-solutions of $h$.
%, $\varepsilon\geq 0$.
\sloppy If, for some $y\in\X$, and $f$ $\rho$-weakly convex with $\alpha<\frac{1}{\rho}$, for every $x\in\X$, function $h$ is defined as 
 ${h(x): = f(x) + \frac{1}{2\alpha}\|x-y\|^2}$,
then
$
    \varepsilon\operatorname{-prox}_{\alpha f}(y) =  \operatorname{lev}_{\leq \varepsilon + h(\overline{x})}h$,
where $\overline{x}$ is the exact proximal point of $f$ at $y$.

\begin{lemma}
Let $\fonc{h,\,\widehat{h}}{\X}{(-\infty,+\infty]}$ be two strongly convex functions with minimisers $\overline{x}\in\X$ and $\widehat{x}\in\X$ respectively. Assume that for every $\varepsilon\geq 0$ 
\begin{equation}\label{eq:ineq}
    (\forall x \in \X)\qquad \qquad h(x) \leq \widehat{h}(x) \leq h(x) + \varepsilon.
\end{equation}
Then, the unique minimiser $\widehat{x}$ of $\widehat{h}$ belongs to $ \operatorname{lev}_{\leq \varepsilon + h(\overline{x})}h$.\\
\end{lemma}
\begin{proof}
By contradiction,  assume that $\widehat{x}\notin \operatorname{lev}_{\leq \varepsilon + h(\overline{x})}h$, \emph{i.e.\ }, $h(\widehat{x}) > \varepsilon  + h(\overline{x}).$ By assumption,  for $x=\widehat{x}$, equation \eqref{eq:ineq} reads as
$
    h(\widehat{x}) \leq \widehat{h}(\widehat{x}) \leq h(\widehat{x}) + \varepsilon
$
which combined with our hypothesis implies
$
    \varepsilon + h(\overline{x}) < \widehat{h}(\widehat{x}).
$
On the other side, for $x=\overline{x}$, equation \eqref{eq:ineq} reads as
$
    h(\overline{x}) \leq \widehat{h}(\overline{x}) \leq h(\overline{x}) + \varepsilon
$,
which implies $
     \widehat{h}(\overline{x}) \leq \varepsilon + h(\overline{x}).$ This induces
    $ \widehat{h}(\overline{x}) \leq \varepsilon + h(\overline{x}) < \widehat{h}(\widehat{x})$
which is a contradition because $\widehat{x}$ is the unique minimiser of $\widehat{h}$.
\end{proof}

\begin{example}
    Let 
    %us consider function $f$ from \autoref{ex: weak convexity} 
     $(a_1,\dots,a_n), (b_1,\dots,b_n)\in\R^n$ be  pair of vectors. The function $\fonc{F}{\R}{\R}$ defined, for every $x\in\R^n$, as
% \begin{flalign}
%     (\forall x \in \R) &&\qquad F(x) := \sum_{i=1}^n |(x-a_i)(x-b_i)| &&&
% \end{flalign}
$F(x) := \sum_{i=1}^n |(x-a_i)(x-b_i)|$ 
lacks an explicit formula for its proximal operator. Consider the function $\fonc{\widehat{F}}{\R^n}{\R}$ defined, for every $x\in\R^n$ as 
% \begin{flalign}
%     (\forall x \in \R) && \widehat{F}(x) = \sum_{i=1}^n \sqrt{((x-a_i)(x-b_i))^2 + \varepsilon_i^2} &&&
% \end{flalign}
$\widehat{F}(x) = \sum_{i=1}^n \sqrt{((x-a_i)(x-b_i))^2 + \varepsilon_i^2} $
for given $\varepsilon_i>0$, $i=1,\dots,n$. It is  easy to see that for every $x\in\R^n$
\begin{equation}
    F(x) \leq \widehat{F}(x) \leq \sum_{i=1}^n \left(|((x-a_i)(x-b_i))| + \varepsilon_i\right) \leq F(x) + \sum_{i=1}^n \varepsilon_i.
\end{equation}
Functions $h:=F + \frac{1}{2\alpha}\|\cdot - y\|^2$ and $\widehat{h}: =\widehat{F}  + \frac{1}{2\alpha}\|\cdot - y\|^2$ satisfy \eqref{eq:ineq} with $\varepsilon=\sum_{i=1}^n\varepsilon_i$ and by solving the surrogate smooth and strongly convex problem
\begin{equation}\label{eq:surrogate_smooth_problem}
    \argmin_{x\in\R^n} \widehat F(x) + \frac{1}{2\alpha}\|x-y\|^2
\end{equation}
we obtain a point in $\varepsilon\operatorname{-prox}_{\alpha F}(y)$. In particular, solving \eqref{eq:surrogate_smooth_problem} corresponds to solving $n$ independent smooth and strongly convex 1-dimensional problems.
\end{example}}
\end{appendices}

\end{document}


