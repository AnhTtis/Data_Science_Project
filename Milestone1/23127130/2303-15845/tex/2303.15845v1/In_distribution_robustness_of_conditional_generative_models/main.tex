\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{geometry}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz,url}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma} 
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{question}[theorem]{Question}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}

\newtheorem{example}[theorem]{Example}

\newcommand\E{\mathbb{E}}
\newcommand\R{\mathbb{R}}
\newcommand\dx{\mathrm{d}}
\newcommand\supp{\mathrm{supp}~}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\tT}{\mathrm{T}}

%------------------------------------------------------
\title{Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems}
\author{Fabian Altekr\"uger, Paul Hagemann and Gabriele Steidl}

\begin{document}

\maketitle


\begin{abstract}
    Conditional generative models became a very powerful tool to sample from Bayesian inverse problem posteriors. 
     It is well-known in classical Bayesian literature that posterior measures are quite robust with respect to perturbations of both the prior measure and the negative log-likelihood, which includes perturbations of the observations. However, to the best of our knowledge, the robustness of conditional generative models with respect to perturbations of the observations has not been investigated yet. In this paper, we prove for the first time that appropriately learned conditional generative models provide robust results for single observations.
\end{abstract}

%----------------------------------------------------------
\section{Introduction}
%----------------------------------------------------------
Initiated by \cite{adv_attacks}, 
the vulnerability of deep neural networks (NNs) to adversarial attacks 
has been shown in many papers, see for instance
\cite{Carlini2020,OMMF2020,adv_survey}.
The vast majority of the literature is concerned 
with classification and related tasks like image segmentation. Here, gradient-based information is typically used in order to cross the discontinuous decision boundary of the classifier.


In this paper, we are interested in the solution of inverse problems by Bayesian methods.
In \cite{PNU2023}, it was shown that for Gaussian noise and a convex negative log prior, 
the maximum a-posteriori (MAP) estimation is stable with respect to the observations.
This is no longer true for non-convex log priors, see our motivating example in the appendix.
Concerning the robustness of end-to-end NN architectures, e.g., when learning the NN 
with a parameter constrained quadratic loss function 
between the true data and their NN reconstruction from corresponding observations,
there exist ambivalent results in the literature. 
Antun et al. \cite{Antun_2020} observed that deep learning for inverse problems comes with instabilities
in the sense that ,,tiny, almost undetectable  perturbations, both in the image and sampling domain, may 
result in severe artifacts in the reconstruction'', while
Genzel et al. \cite{Genzel_2023} attested in their comprehensive tests that ,, deep-learning-based methods are at least as
robust as TV minimization with respect to adversarial noise''. The authors of \cite{GCM2022} showed experimentally the sensitivity of NN to perturbations for the inverse problem of image deblurring.

We are not interested in end-to-end learning methods, but rather in learning the whole posterior distribution in Bayesian inverse problems by conditional generative NNs as proposed, e.g., in \cite{adler_deep,ardizzone2019guided, cond_score, Hagemann_2022}. 
Addressing the posterior measure instead of end-to-end reconstructions has several advantages as illustrated in our example in the appendix.
In particular, (samples of) the posterior can be used to provide additional 
information on the reconstructed data, for example on their uncertainty.
Further, several robustness guarantees on the posterior were proved in the literature. One of the first results in the direction of stability was obtained in \cite{stuart_2010} with respect to the Hellinger distance, see also \cite{DS2017}. 
A very related question instead of perturbed observations concerns the approximations of forward maps, which was investigated in \cite{MX2009}. Furthermore, different prior measures or different metrics on the space of probability measures were considered in \cite{Hosseini2017, HN2017,Sullivan_2017}. Two recent works \cite{latz_wellposedness,Sprungk_2020} investigated the (Lipschitz) continuity of the posterior measures with respect to a multitude of metrics, where \cite{latz_wellposedness} focused on the well-posedness of the Bayesian inverse problem and \cite{Sprungk_2020}  on the local Lipschitz continuity. Our paper is based on the findings in \cite{Sprungk_2020}, but relates them with conditional generative NNs that aim to learn the posterior.

More precisely, in many machine learning papers, 
the following idea is pursued in order to solve inverse problems simultaneously for all observations $y$: 
Consider a family of generative models $G_\theta(y,\cdot)$ with parameters $\theta$, 
which are supposed to map a latent distribution, like the standard Gaussian one, to the absolutely continuous posteriors 
$P_{X|Y=y}$, i.e., $G_{\theta}(y,\cdot)_{\#} P_Z \approx P_{X|Y=y}$. 
In order to learn such a conditional generative model, usually a loss of the form 
$$
L(\theta) \coloneqq \mathbb{E}_{y \sim P_Y} [D (P_{X|Y=y}, G_\theta(y,\cdot)_{\#} P_Z)]
$$
is chosen with some ,,distance'' $D$ between measures 
like the Kullback-Leibler (KL) divergence $D = \text{KL}$ used in \cite{ardizzone2019guided}
or the Wasserstein-1 distance $D = W_1$ appearing, e.g., in the framework of (conditional) Wasserstein generative adversarial networks (GANs) \cite{adler_deep,arjovsky2017wasserstein, liu2021wasserstein}. Also conditional diffusion models \cite{eq_diff,song2021scorebased,tashiro2021csdi} fit into this framework. Here De Bortoli \cite{debortoli2022convergence}
showed that the standard score matching diffusion loss also optimizes the Wasserstein distance between the target and predicted distribution. 

However, in practice we are usually interested in the reconstruction quality from a single or just a few measurements
which are null sets with respect to $P_Y$. 
In this paper, we are interested in the important question, whether there exist any guarantees for the NN output to
be close to the posterior for one specific measurement $\tilde{y}$.
Our main result in Theorem \ref{thm:posterior_bound} 
shows that for a NN learned such that the loss becomes small in the Wasserstein-1 distance, say
$L(\theta) < \varepsilon$, 
the distance $W_1(P_{X|Y=\tilde y}, G_\theta(\tilde y,\cdot)_{\#} P_Z)$ becomes also small for the single observation $\tilde y$.
More precisely, we get the bound
$$W_1(P_{X|Y=\tilde y}, G_\theta(\tilde y,\cdot)_{\#} P_Z) \le C \varepsilon^\frac{1}{n+1},$$ 
where $C$ is a constant and $n$ is the dimension of the observations.
To the best of our knowledge, this is the first estimate given in this direction. 


We like to mention that in contrast to our paper, 
where we assume that samples are taken from the distribution for which the NN was learned,
the authors of \cite{robust_cond_ood} observed that conditional normalizing flows 
are unstable when feeding them out-of-distribution observations. 
This is not too surprising given some literature on the instability of (conditional) normalizing flows \cite{understanding_and_mitigating,norm_flows_ood}.


\paragraph{Outline of the paper.}
The main theorem is shown in Section~\ref{sec:main}.
In Section~\ref{sec:cond_gen_models}, we discuss the dependence of our derived bound on the training loss for different conditional generative models.
In Appendix~\ref{app:example},
we illustrate by a simple example with a Gaussian mixture prior and Gaussian noise,
why posterior distributions can be expected to be more stable than maximum a-posteriori (MAP) estimations and have more desirable properties than minimum mean squared error (MMSE) estimations.

%----------------------------------------------------------
\section{Pointwise Robustness of Conditional Generative NNs} \label{sec:main}
%----------------------------------------------------------
Let $X \in \R^m$ be a continuous random variable with law $P_X$  determined by its density function $p_X$
and $f \colon \mathbb R^m \rightarrow \mathbb R^n$ a measurable function.
We consider a Bayesian inverse problem 
\begin{align} \label{inverse_prob}
Y = \mathrm{noisy} (f(X))  
\end{align}
where "noisy" describes the underlying noise model. A typical choice is additive Gaussian noise, resulting in
\begin{align}
Y = f(X) + \Xi, \quad \Xi \sim \mathcal N(0, \sigma^2 I_n).
\end{align}
Let $G_\theta = G \colon \R^n \times \R^d \to \R^m$ be a conditional generative model 
trained to approximate the posterior distribution $P_{X|Y=y}$ using the latent random variable $Z \in \R^d$.
We will assume that all appearing measures are absolutely continuous. 
In particular, the posterior density is related via Bayes' theorem through the prior $p_X$ and the likelihood $p_{Y|X=x}$ as
$$p_{X|Y=y} \propto p_{Y|X=x} p_X,$$
where $\propto$ means quality up to a multiplicative normalization constant.
Further, we assume that the negative log-likelihood $- \log p_{Y|X=x}$ is bounded from below with respect to $x$, 
i.e., $\inf_x - \log p_{Y|X=x} > - \infty$. In particular, this includes 
mixtures of additive and multiplicative noise $Y = f(X) + \Xi_1 + \Xi_2 f(X)$, if $X$, $\Xi_1$ and $\Xi_2$ are independent, or log-Poisson noise commonly arising in computerized tomography.

We will use the Wasserstein-1 distance \cite{villani2009optimal}, which is a metric on the space of probability measures with finite expectation and is defined 
 for measures $\mu$ and $\nu$ on the space $\R^m$ as
 \begin{align}
W_1(\mu,\nu) = \inf_{\pi \in \Pi (\mu, \nu)} \int_{\R^m \times \R^m} \Vert x - y \Vert d\pi(x,y),
\end{align}
where $\Pi (\mu,\nu)$ contains all measures on $\R^m \times \R^m$ with $\mu$ and $\nu$ as its marginals.
The Wasserstein distance can be also rewritten by its dual formulation \cite[Remark 6.5]{villani2009optimal} as
\begin{align} \label{eq:w1_dual}
W_1(\mu, \nu) = \max_{\mathrm{Lip}(\varphi) \leq 1} \int \varphi(x) \dx (\mu-\nu)(x).
\end{align}


First, we show the Lipschitz continuity of our generating measures $G(y,\cdot)_\# P_Z$ with respect to $y$.

\begin{lemma}[Lipschitz continuity of generator] \label{gen_cont}
For any parameterized family of generative models $G$ with 
$\sup_y \Vert \nabla_y G(y,z) \Vert \leq L$ for all $z \in \R^d$ for some $L > 0$, it holds 
$$W_1(G(y_1,\cdot)_{\#} P_Z, G(y_2,\cdot)_{\#} P_Z) \leq L \Vert y_1-y_2 \Vert$$ for all $y_1,y_2 \in \R^n$.
\end{lemma}

\begin{proof}
We use the mean value theorem which yields for every $z \in \R^d$ and all $y_1,y_2 \in \R^n$
\begin{align*}
\Vert G(y_1,z) - G(y_2,z) \Vert 
&= \Big\Vert \int_0^1 \nabla_y G (y_1 + t(y_2 - y_1),z) ( y_1 - y_2) \dx t  \Big\Vert\\
&\le  \int_0^1 \Vert \nabla_y G (y_1 + t(y_2 - y_1),z) \Vert \dx t   \Vert y_1 - y_2   \Vert \\
&\le L \Vert y_1 - y_2  \Vert.
\end{align*}
Next, we apply the dual formulation of the Wasserstein-1 distance to estimate
\begin{align*}
W_1(G(y_1,\cdot)_{\#} P_Z, G(y_2,\cdot)_{\#} P_Z)  &= \max_{\mathrm{Lip}(\varphi) \le 1} \E_{z \sim P_Z} [\varphi ( G (y_1,z)) -  \varphi (G(y_2,z))] \\
&\le \max_{\mathrm{Lip}(\varphi) \le 1} \E_{z \sim P_Z} [\vert \varphi ( G (y_1,z) )  - \varphi ( G(y_2,z)) \vert] \\
&\le \E_{z \sim P_Z} [\Vert G(y_1,z) - G(y_2,z) \Vert] \\
&\le L \Vert y_1 - y_2 \Vert. 
\end{align*}
\end{proof}

\begin{remark}
 In practice, the assumption that $\sup_y \Vert \nabla_y G(y, z) \Vert \leq L$ for all $z \in \R^d$ is satisfied by most common generative NNs as 
GANs, normalizing flows, variational autoencoders and even diffusion models (when using their equivalent ODE formulation \cite{song2021maximum}). 
Only truly stochastic jump steps such as MCMC layers which may appear, e.g., in stochastic normalizing flows \cite{HHS2023,WKN2020}
are not included in our framework. 
\end{remark}

By the following lemma, which is just \cite[Corollary 19]{Sprungk_2020} for Euclidean spaces, 
the local Lipschitz continuity of the posterior distribution
with respect to the Wasserstein-1 distance is guaranteed.

\begin{lemma}[Local Lipschitz continuity of the posterior] \label{post_cont}
Let the forward operator $f$ and the likelihood $p_{Y|X=x}$ in \eqref{inverse_prob} be measurable. 
Assume that there exists a function $M \colon[0,\infty)\times \mathbb{R} \to [0,\infty )$ 
which is monotone in the first component and non-decreasing in the second component 
such that for all $y_1,y_2 \in \R^n$ with $\Vert y_1 \Vert , \Vert y_2 \Vert \le r$ for $r>0$ 
and for all $x \in \R^m$ it holds
\begin{align} \label{eq:likelihood_bounded}
\vert \log p_{Y|X=x}(y_2) - \log p_{Y|X=x}(y_1) \vert \le M (r,\Vert x \Vert ) \Vert y_1 - y_2 \Vert.
\end{align}
Furthermore, assume that $M(r,\Vert \cdot \Vert ) \in L^2_{P_X} (\R^m, \mathbb{R})$. 
Then, for any $r > 0$ there exists a constant $C_r < \infty$ such that for all $y_1,y_2 \in \R^n$ 
with $\Vert y_1 \Vert , \Vert  y_2 \Vert \le r$ we have
\begin{align*}
W_1(P_{X|Y=y_1}, P_{X|Y=y_2}) \le C_r \Vert y_1-y_2 \Vert.
\end{align*}
\end{lemma}
The Lipschitz constants of the family of generative models $G^{\varepsilon}$ and the posterior distributions $P_{X|Y=y}$ can be related to each other under some convergence assumptions. Let the assumptions of Lemma~\ref{post_cont} be fulfilled, assume further that $$\lim_{\varepsilon \rightarrow 0} G^{\varepsilon}(y,\cdot)_{\#} P_Z = P_{X|Y=y}$$
with respect to the $W_1$-distance and consider observations $y_1,y_2 \in \R^n$ with $\Vert y_1, \Vert, \Vert y_2 \Vert \le r$.
Then, by the triangle inequality it holds
\begin{align*}
\lim_{\varepsilon \rightarrow 0} W_1(G^{\varepsilon}(y_1,\cdot)_{\#}P_Z,G^{\varepsilon}(y_2,\cdot)_{\#}P_Z) 
&\leq \lim_{\varepsilon \rightarrow 0}  W_1(G^{\varepsilon}(y_1,\cdot)_{\#}P_Z,P_{X|Y = y_1}) +
W_1(P_{X|Y = y_1},P_{X|Y = y_2}) \\
&\quad +  W_1(P_{X|Y = y_2}, G^{\varepsilon}(y_2,\cdot)_{\#}P_Z) \\
&= W_1(P_{X|Y = y_1},P_{X|Y = y_2}) \\
&\leq C_r \Vert y_1 - y_2 \Vert.
\end{align*}
Hence, under the assumption of convergence, we expect the Lipschitz constant of our conditional generative models to behave similar to the one of the posterior distribution. 

\begin{remark}
The assumption \eqref{eq:likelihood_bounded} is for instance fulfilled 
for additive Gaussian noise $\Xi \sim \mathcal{N}(0, \sigma^2 \mathrm{Id})$. 
In this case 
\begin{align*}
-\log p_{Y|X=x}(y)= 
\frac{n}{2} \log (2 \pi \sigma^2)+ \frac{1}{2\sigma^2} \Vert y - f(x) \Vert^2.
\end{align*}
Hence $-\log p_{Y|X=x}(y)$
is differentiable with respect to $y$ and we get local Lipschitz continuity of the negative log-likelihood. 
\end{remark}

Now we can prove our main theorem
which ensures  pointwise bounds on the distance between posterior and generated measure, if the training loss becomes small. In particular, the bound depends on the Lipschitz constant of the conditional generator with respect to the observation, the Lipschitz constant of the inverse problem, the training loss and the probability of the considered observation $\Tilde{y}$. We want to highlight that the bound depends on the evidence $p_Y(\tilde y)$ of an observation $\tilde y$ and indicates that we generally cannot expect a good pointwise estimate for out-of-distribution observations, i.e., $p_Y(y) \approx 0$. This is in agreement with the empirical results presented in \cite{robust_cond_ood}.

\begin{theorem} \label{thm:posterior_bound}
Let the forward operator $f$ and the likelihood $p_{Y|X = x}$ in \eqref{inverse_prob} fulfill the assumptions of Lemma \ref{post_cont}.
Assume that we have trained a family of generative models $G$ which fulfills 
$\sup_y \Vert \nabla_y G(y,z) \Vert \leq L$ for all $z \in \R^d$ and some $L > 0$, 
such that 
\begin{align} \label{eq:expectation_W1}
\mathbb{E}_{y \sim P_Y}  [ W_1(P_{X|Y=y}, G(y,\cdot)_{\#}P_Z) ] \leq \varepsilon
\end{align}
for some $\varepsilon > 0$. 
Let $\tilde{y} \in \R^n$ be an observation with $p_Y(\Tilde{y}) = a > 0$. 
Further, assume that $y \mapsto p_Y(y)$ is differentiable with $\Vert \nabla p_Y(y) \Vert \leq K$ for $K>0$ and all $y \in \R^n$. 
Then we have for 
$\varepsilon 
\le 
\left( \frac{a}{2K}\right)^{n+1} \frac{(L+C_{\Vert \tilde y \Vert + \frac{a}{2 K}})S_n a}{2n}$
that
\begin{align} \label{estimate1}
W_1(P_{X|Y=\tilde{y}}, G(\tilde{y},\cdot)_{\#}P_Z) &
\le 
(L+  C_{\Vert \tilde y \Vert + \frac{a}{2 K}}) ^{1-\frac{1}{n+1}} (1 + \frac1n) \left( \frac{2n}{S_n a}\right)^\frac{1}{n+1} \varepsilon^\frac{1}{n+1},
\end{align}
where $S_n \coloneqq \pi^{\frac{n}{2}} / \Gamma (\frac{n}{2} + 1)$ and 
$C_\bullet$ is the Lipschitz constant from Lemma \ref{post_cont}.
If $\varepsilon \le 1$, it also holds
\begin{align} \label{estimate}
W_1(P_{X|Y=\tilde{y}}, G(\tilde{y},\cdot)_{\#}P_Z) \leq (L + C_{\Vert \tilde y \Vert 
+ \frac{a}{2 K}}) \frac{\varepsilon^{\frac{1}{n+1}}a}{2 K}  + \frac{2 \varepsilon^{\frac{1}{n+1}}}{S_n (\frac{a}{2 K})^n a}.
\end{align}

\end{theorem}

\begin{proof}
Let $ 0 < r \le \frac{a}{2 K}$. 
Then, for $y \in B_r(\tilde{y})$, there exists by the mean value theorem some $\xi \in \overline{y \tilde y}$ such that
\begin{align*}
|p_Y(y) - p_Y(\tilde{y}) | \le \Vert \nabla p_Y(\xi) \Vert  \Vert y - \Tilde{y} \Vert \le K r \le  \frac{a}{2}.
\end{align*}
Consequently, each $y \in B_r(\tilde{y})$ has at least probability $p_Y(y) \geq \frac{a}{2}$. 
Moreover, by the volume of the $n$-dimensional ball it holds that 
\begin{align}
P_Y(B_r(\tilde{y})) = \int_{B_r (\tilde y)}  p_Y(y) \dx y 
\ge  \frac{\pi^{\frac{n}{2}}}{\Gamma (\frac{n}{2} + 1)} r^n \frac{a}{2} = S_n r^n \frac{a}{2}.
\end{align}
Now we claim that there exists $\widehat y \in B_r(\tilde{y})$ with 
\begin{align} \label{eq:bound_W1_yhat}
W_1(P_{X|Y=\widehat y}, G(\widehat y, \cdot)_{\#}P_Z) \leq \frac{2 \varepsilon}{S_n r^n a}.
\end{align}
If this would not be the case, this would imply a contradiction to \eqref{eq:expectation_W1} by
\begin{align*}
\mathbb{E}_{y \sim P_Y}  [ W_1(P_{X|Y=y}, G(y,\cdot)_{\#}P_Z) ]  &= \int_{\mathbb{R}^n} W_1(P_{X|Y=y}, G(y,\cdot)_{\#}P_Z) \dx P_Y(y)  \\
&\geq \int_{B_r({\Tilde{y}})} W_1(P_{X|Y=y}, G(y,\cdot)_{\#}P_Z) \dx P_Y(y) \\
&> \int_{B_r({\Tilde{y}})} \frac{2 \varepsilon}{S_n r^n a} \dx P_Y(y) \\
&= P_Y(B_r(\Tilde{y})) \frac{2 \varepsilon}{S_n r^n a} 
\ge \varepsilon.
\end{align*}
Next, we show the local Lipschitz continuity of $y \mapsto W_1(P_{X|Y=y}, G(y,\cdot)_{\#}P_Z)$ on $B_r (\tilde{y})$ 
by combining Lemma~\ref{gen_cont} and Lemma~\ref{post_cont}. 
Let $y_1,y_2 \in B_r(\tilde y)$, so that  $\Vert y_1 \Vert, \Vert y_2 \Vert \le \Vert \tilde y \Vert + r$.
Let $L>0$ be the Lipschitz constant from Lemma~\ref{gen_cont} 
and $C_{\Vert \tilde y \Vert + r}$  the local Lipschitz constant from Lemma~\ref{post_cont}. 
Using the triangle inequality and its reverse, we get
\begin{align} \label{eq:localLipschitz_objective}
&\quad| W_1(P_{X|Y=y_1}, G(y_1,\cdot)_{\#} P_Z) - W_1(P_{X|Y=y_2}, G(y_2,\cdot)_{\#} P_Z) |  \nonumber \\
&\le | W_1(P_{X|Y=y_1}, G(y_1,\cdot)_{\#} P_Z) - W_1(P_{X|Y=y_1}, G(y_2,\cdot)_{\#} P_Z) | \nonumber \\ 
&\quad + | W_1(P_{X|Y=y_1}, G(y_2,\cdot)_{\#} P_Z) - W_1(P_{X|Y=y_2}, G(y_2,\cdot)_{\#} P_Z) | \\ 
&\le W_1(G(y_1,\cdot)_{\#} P_Z, G(y_2,\cdot)_{\#} P_Z) + W_1(P_{X|Y=y_1}, P_{X|Y=y_2}) \nonumber \\
&\le (L + C_{\Vert \tilde y \Vert + r}) \Vert y_1 - y_2 \Vert \nonumber .
\end{align}
Combination of the results in \eqref{eq:bound_W1_yhat} and \eqref{eq:localLipschitz_objective} yields the estimate
\begin{align*}
W_1(P_{X|Y=\tilde{y}}, G(\tilde{y},\cdot)_{\#}P_Z) &\le |W_1(P_{X|Y=\tilde{y}}, G(\tilde{y},\cdot)_{\#}P_Z) - W_1(P_{X|Y=\widehat y}, G(\widehat y,\cdot)_{\#}P_Z) | \\
&\quad + | W_1(P_{X|Y=\widehat y}, G(\widehat y,\cdot)_{\#}P_Z) | \\
& \leq (L + C_{\Vert \tilde y \Vert + r}) r + \frac{2 \varepsilon}{S_n r^n a} \\
& \leq (L + C_{\Vert \tilde y \Vert + \frac{a}{2 K}}) r + \frac{2 \varepsilon}{S_n r^n a}.
\end{align*}
The radius $r$, for which the right-hand side becomes minimal, is given by 
$$r = \Big(\frac{2 n \varepsilon}{(L + C_{\Vert \tilde y \Vert + \frac{a}{2 K}}) S_n a} \Big)^{\frac{1}{n+1}}.$$
Plugging this in, we get \eqref{estimate1}.
However, we need that $r \le \frac{a}{2K}$ which implies 
$$
\varepsilon 
\le 
\left( \frac{a}{2K}\right)^{n+1} \frac{(L+C_{\Vert \tilde y \Vert + \frac{a}{2 K}})S_n a}{2n}.
$$
 On the other hand, if $\varepsilon \le 1$, we can choose
$r = \varepsilon^{\frac{1}{n+1}} \frac{a}{2K} \le \frac{a}{2K}$ which results in \eqref{estimate} and has the same asymptotic rate.
\end{proof}
\begin{remark}
We can get rid of the dimension scaling $\varepsilon^{\frac{1}{n+1}}$ by choosing the radius as $r = \frac{a}{2K}$, which yields
\begin{align*}
W_1(P_{X|Y=\tilde{y}}, G(\tilde{y},\cdot)_{\#}P_Z) \leq (L + C_{\Vert \tilde y \Vert + \frac{a}{2 K}}) \frac{a}{2K} + \frac{2 \varepsilon}{S_n (\frac{a}{2 K})^n a}.
\end{align*}
This comes at the disadvantage that the first term is constant with respect to $\varepsilon$.
\end{remark}


The following corollary provides a characterization of a perfect generative model. 
If the expectation \eqref{eq:expectation_W1} goes to zero, then for all $y \in \R^n$ with $p_Y (y) > 0$ 
the posteriors $P_{X|Y=y}$ get predicted correctly.

\begin{corollary}
\label{cor:zero}
Let the assumptions of Lemma~\ref{gen_cont} and Lemma~\ref{post_cont} hold true. Let $p_Y$ be differentiable with $\Vert \nabla p_Y(y) \Vert \le K$ for some $K>0$ and all $y \in \R^n$. Consider a family of generative networks $(G^\varepsilon)_{\varepsilon >0}$ fulfilling 
\begin{align}
\mathbb{E}_{y \sim P_Y}  [ W_1(P_{X|Y=y}, G^{\varepsilon}(y,\cdot)_{\#}P_Z) ] \le \varepsilon
\end{align}
and assume that the Lipschitz constants $L^\varepsilon$ of $G^\varepsilon$ from Lemma~\ref{gen_cont} are bounded by some $L < \infty$. Then for all observations $y \in \R^n$ with $p_Y(y) > 0$ it holds 
\begin{align}
W_1(P_{X|Y=y}, G^{\varepsilon}(y,\cdot)_{\#}P_Z) \rightarrow 0 \quad \text{as} ~ \varepsilon \to 0.
\end{align}
\end{corollary}

\begin{proof}
We can assume that $\varepsilon \le 1$, then the statement follows immediately from Theorem~\ref{thm:posterior_bound}.
\end{proof}

\section{Conditional Generative Models} \label{sec:cond_gen_models}
In this section, we discuss whether the main assumption, namely that the averaged Wasserstein distance $\mathbb{E}_{y \sim P_Y}  [ W_1(P_{X|Y=y}, G(y,\cdot)_{\#}P_Z) ]$ 
in \eqref{eq:expectation_W1} becomes small,
is reasonable for different conditional generative models. Therefore we need to relate the typical choices of training loss with the Wasserstein distance. 

\subsection{Conditional Normalizing Flows}
\label{sec:cond_flow}
Conditional normalizing flows \cite{wpp_flows,graz_inc,ardizzone2019guided, winkler_cond_flows} are a family of normalizing flows parameterized by a condition, which in our case is the observation $y$. The aim is to learn a network $\mathcal{T} \colon \R^n \times \R^m \to \R^m$ such that $\mathcal{T}(y,\cdot)$ is a diffeomorphism and $\mathcal{T}(y,\cdot)_{\#}P_Z \approx  P_{X|Y=y}$ for all $y \in \R^n$, where $\approx$ means that two distributions are similar in some proper distance or divergence.

This can be done via minimizing the expectation on $Y$ of the \textit{forward} KL divergence $\mathbb{E}_{y \sim P_Y} [\mathrm{KL}(P_{X|Y=y},\mathcal{T}(y,\cdot)_{\#}P_Z)]$, which is equal, up to a constant, to
$$\mathbb{E}_{x \sim P_X, y \sim P_Y}[-\log p_Z(\mathcal{T}^{-1}(y,x))-\log (|\operatorname{det}D \mathcal{T}^{-1}(y,x)|)],$$ where the inverse is meant with respect to the second component, see \cite{Hagemann_2022} for more details. Training a network using the forward KL has many desirable properties like a mode-covering behaviour of $\mathcal{T}(y,\cdot)_{\#}P_Z$.  
Now conditional normalizing flows are trained using the KL divergence, while the theoretical bound in Section~\ref{sec:main} relies on the metric properties of the Wasserstein-1 distance.
Thus we need to show that we can ensure a small $\varepsilon$ in \eqref{eq:expectation_W1} when training the conditional normalizing flow as proposed.
Following \cite[Theorem 4]{on_choosing_and}, we can bound the Wasserstein distance by the total variation distance, which in turn is bounded by KL via Pinsker's inequality \cite{pinsker1963information}, i.e., 
\begin{align*}
\mathbb{E}_{y \sim P_y} [W_1((P_{X|Y=y},\mathcal{T}(y,\cdot)_{\#}P_Z)^2] &\leq
C\  \mathbb{E}_{y \sim P_Y} [\mathrm{TV}((P_{X|Y=y} -\mathcal{T}(y,\cdot)_{\#}P_Z)^2] \\
&\leq \frac{C}{\sqrt{2}}\mathbb{E}_{y \sim P_Y}[ \mathrm{KL}((P_{X|Y=y},\mathcal{T}(y,\cdot)_{\#}P_Z)
],
\end{align*}
where $C$ is a constant depending on the support of the probability measures. However, by definition  $\mathrm{supp}(\mathcal{T}(y,\cdot)_{\#}P_Z) = \R^m$. By \cite[Lemma 4]{ADHHMS2022} the density $p_{\mathcal{T}(y,\cdot)_{\#}P_Z}$ decays exponentially. Therefore, we expect in practice that the Wasserstein distance becomes small if the KL vanishes even though \cite[Theorem 4]{on_choosing_and} is not applicable.


\subsection{Conditional Wasserstein GANs}
In Wasserstein GANs \cite{arjovsky2017wasserstein}, a generative adversarial network approach is taken in order to sample from a target distribution. For this, the dual formulation \eqref{eq:w1_dual} is used in order to calculate the Wasserstein distance between measures $P_X$ and $P_Y$. Then the 1-Lipschitz function is reinterpreted as a discriminator in the GAN framework \cite{GPMXWOCB2014}. If the corresponding minimizer in the space of 1-Lipschitz functions can be found, then optimizing the adversarial Wasserstein GAN loss directly optimizes the Wasserstein distance. The classical Wasserstein GAN loss for a target measure $\mu$ and a generator $G\colon\R^d \to \R^m$ is given by
$$\min_\theta  \max_{\mathrm{Lip}(\varphi) \leq 1} \mathbb{E}_{x \sim P_X, z \sim P_Z}[\varphi(x) - f(G(z))],$$
where $d \in \mathbb N$ is the dimension of the latent space.

The Wasserstein GAN framework can be extended to conditional Wasserstein GANs \cite{adler_deep,liu2021wasserstein} for solving inverse problems. For this, we aim to train generators $G\colon \R^n \times \R^d \to \R^m$ and average with respect to the observations
$$ L(\theta) = \mathbb{E}_{y \sim P_y} \big[ \max_{\mathrm{Lip}(\varphi_y) \leq 1}  \mathbb{E}_{x \sim P_{X|Y=y}, z \sim P_Z}[\varphi_y(x) - \varphi_y(G(y,z))] \big].$$
Hence minimizing this loss (or a variant of it) directly  enforces a small $\varepsilon$ in assumption \eqref{eq:expectation_W1}.

\subsection{Conditional Diffusion Models}
In diffusion models, a forward SDE, which maps  a data distribution to an approximate Gaussian distribution is considered \cite{song2021maximum,song2020score}. Then the theory of reverse SDEs \cite{ANDERSON1982313} allows to sample from the data distribution by learning the score $\nabla \log p_t(x)$, where $p_t(x)$ is the path density of the forward SDE. 
The forward SDE usually reads 
$$dX_t = - \alpha X_t \dx t + \sqrt{2 \alpha} \dx W_t,$$
while the reverse SDE is given by 
$$dY_t = - \alpha Y_t \dx t - 2\ \nabla \log p_t(x) \dx t + \sqrt{2 \alpha} \dx \tilde{W}_t,$$
where $\alpha \in \R$ describes the schedule of the SDE.
However, the path density $p_t(x)$ is usually intractable, so that the score $\nabla \log p_t(x)$ is learned with a NN $s_\theta\colon [0,T]\times \R^m \to \R^m$ such that $s_{\theta}(t,x) \approx \nabla \log p_t(x)$ for all $t \in [0,T]$ and $x \in \R^m$.
This can be ensured using the so-called score matching loss \cite{song2020score} defined by
$$\min_\theta \mathbb{E}_{t \sim U([0,T]),x \sim P_{X_t}}\left[\Vert s_{\theta}(t,x) -\nabla \log p_t(x)\Vert^2\right].$$
In order to solve inverse problems, we can consider a conditional reverse SDE
\begin{align*}
dY_t = - \alpha Y_t \dx t - 2\ \nabla \log p_t(x|y) \dx t + \sqrt{2 \alpha} \dx \tilde{W}_t,
\end{align*}
where $p_t(x|y)$ is the conditional path density given an observation $y \in \R^n$. 
Consequently, we consider conditional diffusion models, where a NN $s_\theta \colon \R^n \times [0,T] \times \R^m \to \R^m$ is learned to approximate $s_{\theta}(y,t,x) \approx \nabla \log p_t(x|y)$ for all $t \in [0,T]$, $x \in \R^m$ and all observations $y \in \R^m$. Then the score matching loss for conditional diffusion models is given by \cite[Theorem 1]{cond_score} as
\begin{align} \label{eq:loss_score}
L(\theta) = \mathbb{E}_{y \sim P_Y} \big[ \mathbb E_{t \sim U([0,T]) ,x \sim P_{X_t|Y=y}} [\Vert s_{\theta}(y,t,x) -\nabla \log p_t(x|y)\Vert^2 ].
\end{align}
Denote by $\Tilde{Y}$ the solution to the approximated SDE starting at $\tilde{Y}_0 \approx P_Z$ and $\tilde Y^y$ the solution of the approximated SDE conditioned on an observation $y \in \R^n$.
Then we can use the bound derived in \cite[Theorem 2]{pidstrigach2023infinitedimensional} which gives
$$\mathbb{E}_{y \sim P_Y}[W_2(P_{X|Y=y}, P_{\tilde{Y}_T^y})] \leq \mathbb{E}_{y \sim P_Y} \big[C\ W_2\big(P_{X^y_T}, \mathcal{N}(0,\mathrm{Id})\big)\big] + T L(\theta) ,$$
where $C$ is a constant depending on the length of the interval $T$ and the Lipschitz constant of the conditional score $\nabla \log p_t(x|y).$
Finally, H\"olders inequality yields for the Wasserstein-1 distance
$$\mathbb{E}_{y \sim P_Y}[W_1(P_{X|Y=y}, P_{\tilde{Y}_T^y})] \leq \mathbb{E}_{y \sim P_Y} \big[C\ W_2\big(P_{X^y_T}, \mathcal{N}(0,\mathrm{Id})\big)\big] + T L(\theta).$$
Hence, when training the conditional diffusion model by minimizing \eqref{eq:loss_score} we also ensure that \eqref{eq:expectation_W1} becomes small. For more in depth discussion with less restrictive assumptions on the score, see also \cite{debortoli2022convergence}.

\subsection{Conditional Variational Autoencoder}

Variational Autoencoder (VAE) \cite{kingma2013auto} aim to approximate a distribution $P_X$ by learning a stochastic encoder $E_\phi \colon \R^m \to \R^d \times \R^{d,d}$ determining parameters of the normal distribution $(\mu_\phi(x),\Sigma_\phi(x))$ for $x$ sampled from $P_X$  and pushing $P_X$ to a latent distribution $P_Z$ with density $p_Z$ of dimension $d \in \mathbb N$. In the reverse direction, a stochastic decoder $D_\theta \colon \R^d \to \R^m \times \R^{m,m}$ determines parameters of the normal distribution $(\mu_\theta(z),\Sigma_\theta(z))$ for $z \in \R^d$  and pushes $P_Z$ back to $P_X$. By definition, the densities of $E_\phi$ and $D_\theta$ are given by $q_\phi(z|x) = \mathcal{N}(z;\mu_\phi(x),\Sigma_\phi(x))$ and $p_\theta(x|z) = \mathcal{N}(x;\mu_\theta(z),\Sigma_\theta(z))$, respectively.
These networks are trained by minimizing the so-called evidence lower bound (ELBO) 
\begin{align*}
\mathrm{ELBO}(\theta,\phi) =  - \mathbb E_{x \sim P_X}\big[ \mathbb E_{z \sim q_\phi(\cdot|x)} [\log (p_\theta(x|z)p_Z(z)) - \log(q_\phi(z|x)))] \big].
\end{align*}
By \cite[Theorem 4.1]{HHS2023}, the loss $L(\theta,\phi)$ is related to KL by
\begin{align}
\text{KL}(P_X,{D_\theta}_{\#}P_Z) \le \mathrm{ELBO}(\theta,\phi).
\end{align}

We can solve inverse problems by extending VAEs to conditional VAEs \cite{lim2018molecular,SLY2015} and aim to approximate the posterior distribution $P_{X|Y=y}$ for a given observation $y \in \R^n$. The conditional stochastic encoder $E_\phi \colon \R^n \times \R^m \to \R^d \times \R^{d,d}$ and conditional stochastic decoder $D_\theta \colon \R^n \times \R^d \to \R^m \times \R^{m,m}$ are trained by
\begin{align}
L(\theta,\phi) = \mathbb E_{y \sim P_Y} \big[ - \mathbb E_{x \sim P_{X|Y=y}}\big[ \mathbb E_{z \sim q_\phi(\cdot|y,x)} [\log (p_\theta(x|y,z)p_Z(z)) - \log(q_\phi(z|y,x)))] \big] \big].
\end{align}
By the same argument as above, the KL can be bounded by
\begin{align}
\mathbb E_{y \sim P_Y} [\text{KL}(P_{X|Y=y},D_{\theta}(y,\cdot)_{\#}P_Z)] \le L(\theta,\phi)
\end{align}
and, using similar arguments as in Section~\ref{sec:cond_flow}, we get the estimate
\begin{align}
\mathbb E_{y \sim P_Y} [W_1(P_{X|Y=y},D_\theta(y,\cdot)_{\#}P_Z)^2] \le \frac{C}{\sqrt{2}} L(\theta,\phi).
\end{align}

%-------------------------------------------------------------------------------
\section{Conclusion}
%-------------------------------------------------------------------------------

We showed a pointwise stability guarantee of the Wasserstein distance between the posterior $P_{X|Y=y}$ of a Bayesian inverse problem and the learned distribution $G(y,\cdot)_{\#}P_Z$ of a conditional generative model $G$ under certain assumptions. In particular, the pointwise bound depends on the Lipschitz constant of the conditional generator with respect to the observation, the Lipschitz constant of the inverse problem, the training loss with respect to the Wasserstein distance and the probability of the considered observation. 

 The required training accuracy of the bound depends on the Wasserstein-1 distance between the target distribution and the learned distribution. However, some conditional networks as the conditional normalizing flow are not trained to minimize the Wasserstein-1 distance. Consequently, a direct dependence of the bound on the training accuracy with respect to the KL divergence would be helpful. Furthermore, our bound is a worst case bound and is not always practical if the constants are large. It would be interesting to check whether tightness of the bound can be shown for some examples.  

\appendix
%-------------------------------------------------------------------------------
\section{Example on the Robustness of the MAP and Posterior}\label{app:example}
%-------------------------------------------------------------------------------
We like to provide an example that illustrates the stability of the posterior distribution in contrast to the
MAP estimator and highlights the role of the MMSE estimator.

By the following lemma, see, e.g., \cite{GFO2017,HHS2023},
the posterior of a Gaussian mixture model given observations from a linear forward operator corrupted by white Gaussian noise
can be computed analytically.

%--------------------------------------------------------
\begin{lemma} \label{lem:mm}
Let $X \sim \sum_{k=1}^K w_k \mathcal N(m_k,\Sigma_k) \in \mathbb R^m$ be a Gaussian mixture random variable.
Suppose that 
$$
Y=A X+\Xi,
$$
where
$A: \R^m \rightarrow \R^n$ 
is a linear operator and $\Xi \sim N(0,\sigma^2 I_n)$. Then the posterior is also a Gaussian mixture
$$
P_{X|Y=y} \propto \sum_{k=1}^K \tilde w_k \mathcal N(\cdot|\tilde m_k,\tilde \Sigma_k)
$$
with
$$
\tilde \Sigma_k \coloneqq (\tfrac{1}{\sigma^2}A^\tT A+\Sigma_k^{-1})^{-1},
\qquad 
\tilde m_k \coloneqq \tilde\Sigma_k (\tfrac1{b^2}A^\tT y+\Sigma_k^{-1}\mu_k)
$$
and
$$
\tilde w_k \coloneqq w_k \exp\left(\frac12 (\tilde m_k \tilde \Sigma_k^{-1} \tilde m_k - m_k \Sigma_k^{-1} m_k)\right).
$$
\end{lemma}


Now, for some small $\varepsilon > 0$ we consider  
the random variable $X \in \mathbb R$ with  simple prior distribution 
$$P_X = \frac{1}{2} \mathcal{N}(-1,\varepsilon^2) + \frac{1}{2} \mathcal{N}(1,\varepsilon^2)$$
and observations from
$Y=X+\Xi$ 
with noise $\Xi \sim \mathcal{N}(0,\sigma^2)$. 
The  MAP estimator is given by
\begin{align}
x_{\mathrm{MAP}}(y) &\in \argmax_x p_{X|Y=y} (x) \\
&=
\argmin_x \frac{1}{2 \sigma^2} ( y - x )^2 - \log  \big( \frac{1}{2} (  e^{-\frac{1}{2 \varepsilon^2} ( x - 1 )^2} 
+  e^{-\frac{1}{2 \varepsilon^2} ( x + 1 )^2} )\big) \\
&= 
\argmin_x \frac{1}{2 \sigma^2} ( y - x )^2  + \frac{1}{2 \varepsilon^2} (x^2 + 1) - \log \left( \cosh \left(\frac{x}{\varepsilon^2} \right) \right).
\end{align}
The above minimization problem has a unique global minimizer for $y \not = 0$ which we computed numerically.
Figure \ref{fig:map_mmse_estimator} (top) shows the plot of the function $x_{\mathrm{MAP}}(y)$ for $\varepsilon^2 = 0.05^2$
and different values of $\sigma$. Clearly, small perturbations of $y$ near zero lead to qualitatively completely different $x$-values,
where a smaller noise level $\sigma$ lowers the distance between the values $x_{\mathrm{MAP}}(y)$ for $y>0$ and $y<0$.
In other words, the MAP estimator is not robust with respect to perturbations of the observations near zero.

\begin{figure*}[t]
\centering
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{imgs/bayes_cont/xMAP_0.01.pdf}
\end{subfigure}%
\hspace{0.2cm}
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{imgs/bayes_cont/xMAP_0.1.pdf}
\end{subfigure}%
\hspace{0.2cm}
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{imgs/bayes_cont/xMAP_0.3.pdf}
\end{subfigure}%

\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{imgs/bayes_cont/xMMSE_0.01.pdf}
  \caption*{$\sigma^2 = 0.01$}
\end{subfigure}%
\hspace{0.2cm}
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{imgs/bayes_cont/xMMSE_0.1.pdf}
  \caption*{$\sigma^2 = 0.1$}
\end{subfigure}%
\hspace{0.2cm}
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{imgs/bayes_cont/xMMSE_0.3.pdf}
  \caption*{$\sigma^2 = 0.3$}
\end{subfigure}%
\caption{
The MAP estimator (top) and the MMSE estimator (bottom) with respect to the observation $y$ for $\varepsilon^2 = 0.05^2$  and
different noise levels $\sigma^2$.
} \label{fig:map_mmse_estimator}
\end{figure*}

In contrast, using Lemma \ref{lem:mm}, we can compute the posterior 
\begin{align}
P_{X|Y=y} = \frac{1}{\tilde w_1 + \tilde w_2} (\tilde w_1 \mathcal{N}(\cdot | \tilde m_1 , \tilde \sigma^2) + \tilde w_2 \mathcal{N}(\cdot | \tilde m_2 \tilde \sigma^2) )
\end{align}
with 
\begin{align}
\tilde \sigma^2 &= \frac{\sigma^2 \varepsilon^2}{\sigma^2 + \varepsilon^2}, \quad 
\tilde m_1 = \frac{\varepsilon^2 y + \sigma^2}{\varepsilon^2 + \sigma^2}, \quad \tilde m_2 = \frac{\varepsilon^2 y - \sigma^2}{\varepsilon^2 + \sigma^2},
\\
\tilde w_1 &= \frac{1}{2 \varepsilon} \exp \Big(\frac{1}{2\varepsilon^2} \Big( \frac{(\varepsilon^2 y + \sigma^2)^2}{\sigma^2  (\varepsilon^2 + \sigma^2)} - 1 \Big) \Big), \quad 
\tilde w_2 = \frac{1}{2 \varepsilon} \exp \Big(\frac{1}{2\varepsilon^2} \Big( \frac{(\varepsilon^2 y - \sigma^2)^2}{\sigma^2  (\varepsilon^2 + \sigma^2)} - 1 \Big) \Big) .
\end{align}
Then the MMSE estimator is given by the expectation value of the posterior
\begin{align}
x_{\mathrm{MMSE}}(y) 
&= \argmin_{T} \mathbb E_{(x,y) \sim P_{(X,Y)}} \|x - T(y)\|^2  =  \mathbb E[X|Y=y] \label{loss_MMSE}\\
&= \int_{\mathbb R} x p_{X|Y = y} (x) \, \text{d} x\\
&= \frac{1}{\tilde w_1 + \tilde w_2} (\tilde w_1 \tilde m_1 + \tilde w_2 \tilde m_2)\\
&=
\frac{1}{\tilde w_1 + \tilde w_2}\frac{1}{\varepsilon ( \varepsilon^2 + \sigma^2)} e^{\frac{\varepsilon^2 y^2 - \sigma^2}{2 \sigma^2 (\varepsilon^2 + \sigma^2)} } \big(\varepsilon^2 y \cosh (\frac{y}{\varepsilon^2 + \sigma^2}) + \sigma^2 \sinh ( \frac{y}{\varepsilon^2 + \sigma^2}) \big).
\end{align}
In Figure \ref{fig:map_mmse_estimator} (bottom), we see that the MMSE estimator shows a smooth transition in particular for larger noise levels,
meaning that the estimator is robust against small perturbations of the observation near zero.
The posterior is plotted for four different small values of $y$ in Figure \ref{fig:posterior_density}.
The red curves show the graphs of the corresponding density functions.
Obviously, these curves change smoothly with respect to $y$, 
i.e., we observe a continuous behavior of the posterior also with respect to observations near zero.
Therefore, sampling from the posterior distributions $P_{X|Y = y}$ appears to be robust to perturbations of $y$.
Having different samples from the posterior at $Y=y$, we can obtain a more circumvent overview on the original data
than just taking their mean value represented by the MMSE estimator into account.
As can be seen in the figure, for fixed $y$ the MMSE estimator delivers just an averaged value, 
which is, in contrast to the non robust MAP estimator, not the one with highest probability.
Note that in case of a Gaussian prior $X \sim \mathcal N (m, \Sigma)$ in $\mathbb R^m$ and white Gaussian noise, 
the MAP and MMSE estimators coincide.


\begin{figure*}
\centering
\begin{subfigure}{.245\textwidth}
  \includegraphics[width=\linewidth]{imgs/bayes_cont/post_-0.05.pdf}
  \caption*{$y=-0.05$}
\end{subfigure}%
\hfill
\begin{subfigure}{.245\textwidth}
  \includegraphics[width=\linewidth]{imgs/bayes_cont/post_-0.01.pdf}
    \caption*{$y=-0.01$}
\end{subfigure}%
\hfill
\begin{subfigure}{.245\textwidth}
  \includegraphics[width=\linewidth]{imgs/bayes_cont/post_0.01.pdf}
    \caption*{$y=0.01$}
\end{subfigure}%
\hfill
\begin{subfigure}{.245\textwidth}
  \includegraphics[width=\linewidth]{imgs/bayes_cont/post_0.05.pdf}
    \caption*{$y=0.05$}
\end{subfigure}%
\caption{Posterior density (red), MAP estimator (blue) and MMSE estimator (green) for different observations $y=-0.05,-0.01,0.01,0.05$ 
(from left to right). While the MAP estimator is discontinuous with respect to the observation $y$,  the posterior density 
is continuous with respect to y. 
The MMSE estimator gives just the expectation value of the posterior which is, in contrast to MAP,  
in  not the value with highest probability.
} \label{fig:posterior_density}
\end{figure*}




\subsection*{Acknowledgement}
P.H. acknowledges funding by the German Research Foundation (DFG)  
within the project of the DFG-SPP 2298 ,,Theoretical Foundations of Deep Learning''
and 
F.A. within project
EF3-7 of Germany‘s
Excellence Strategy – The Berlin Mathematics Research Center MATH+.
The authors want to thank B. Sprungk for posing the question considered in this paper, namely
if there are guarantees that conditional generative NNs
work well for single observations.
Moreover, many thanks to J. Hertrich for fruitful discussions and for suggesting the example in the appendix.

\bibliographystyle{abbrv}
\bibliography{refs}
\end{document}
