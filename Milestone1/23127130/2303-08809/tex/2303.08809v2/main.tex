% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass[9pt]{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{hyperref} 
\usepackage{booktabs}
\usepackage{amssymb,xcolor,stackengine,graphicx}
\usepackage{multirow}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
%\title{Unsupervised Constituency Parsing of Spoken Sentences}
%\title{Investigating Unsupervised Approaches to Constituency Parsing on Spoken Sentences}
\title{Cascading and Direct Approaches to Unsupervised \\Constituency Parsing on Spoken Sentences}
%
% Single address.
% ---------------
% \name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
% \address{Author Affiliation(s)}

\name{Yuan Tseng$^{1}$ \qquad Cheng-I Jeff Lai$^{2}$ \qquad Hung-yi Lee$^{1}$}% \qquad Lin-shan Lee$^{1}$}
  
\address{$^{1}$ National Taiwan University \qquad $^{2}$ MIT CSAIL 
\\ \texttt{r11942082@ntu.edu.tw} }

\begin{document}
\ninept
%
\maketitle
%
% \begin{abstract}
%     We present the first study on unsupervised methods of obtaining syntactic structure from spoken sentences in the form of constituency parse trees, where nodes are spans of audio that corresponds to constituents, given that only raw speech and unpaired textual data is available. We consider two main approaches: First, a cascaded approach of using unsupervised automatic speech recognition (ASR) and a parser trained on text to infer parse trees from ASR transcripts. Second, a less resource-intensive approach of splitting speech input into sequences of word-level segments, and then training a parser on word-level continuous speech embeddings. We show that the second approach is not only more robust towards ASR errors, but also functions at reasonable accuracy without good word boundaries. We also present analyses exploring the various linguistic structures learned by the two approaches.
% \end{abstract}
\begin{abstract}
    Past work on unsupervised parsing is constrained to written form.
    In this paper, we present the first study on \emph{unsupervised spoken constituency parsing} given unlabeled spoken sentences and unpaired textual data. 
    The goal is to determine the spoken sentences' hierarchical syntactic structure in the form of constituency parse trees, such that each node is a span of audio that corresponds to a constituent.
    We compare two approaches: (1) cascading an unsupervised automatic speech recognition (ASR) model and an unsupervised parser to obtain parse trees on ASR transcripts, and
    (2) direct training an unsupervised parser on continuous word-level speech representations. 
    This is done by first splitting utterances into sequences of word-level segments, and aggregating self-supervised speech representations within segments to obtain segment embeddings.
    We find that separately training a parser on the unpaired text and directly applying it on ASR transcripts for inference produces better results for unsupervised parsing.
    Additionally, our results suggest that accurate segmentation alone may be sufficient to parse spoken sentences accurately. 
    Finally, we show the direct approach may learn head-directionality correctly for both head-initial and head-final languages without any explicit inductive bias. 
\end{abstract}
%
\begin{keywords}
Unsupervised constituency parsing, 
unsupervised word segmentation, self-supervised speech representations
\end{keywords}
%
%\vspace{-2mm}
\section{Introduction}
\label{sec:intro}
%\vspace{-2mm}
% Children are able to learn high-level linguistic aspects of their native language without explicit supervision—such as syntax and grammar—from listening and reading. However, most computational models of grammar induction only function on textual inputs \cite{DUPOUX201843}, operating under the assumption that transcriptions are either available or able to be determined accurately with automatic speech recognition (ASR). 
Unsupervised constituency parsing is a long-standing research challenge in natural language processing \cite{10.5555/864689,klein-manning-2004-corpus,bod-2006-subtrees,shen2018neural} that aims to automatically determine the syntactic constituent structure of sentences without access to any training labels.
%It is relevant to the fundamental question of understanding how language is acquired without expert supervision. 
It sheds light on how children are able to learn high-level linguistic information, such as syntax and grammar, without expert supervision.
Additionally, constituency parse trees have also been shown to improve and provide greater interpretability to a variety of downstream tasks such as semantic role labeling \cite{strubell2018linguistically}, word representation learning \cite{kuncoro2020syntactic}, and speech synthesis \cite{Guo2019,tyagi20_interspeech,song2021}. 
% It is relevant to the fundamental question of how language acquisition is done at an early age without supervisory feedback
% With the success of supervised neural networks in natural language processing, research in unsupervised parsing is gaining momentum, out of a desire to model linguistic structure without large amounts of human-labelled data [2]. 

% Most research in syntactic parsing focuses on textual sentences. 
% For spoken sentences, they are usually first transcribed into text with an off-the-shelf ASR model, and syntactic structure is derived from the transcriptions with a separate parser.
\begin{figure*}[!htp]
  \centering
  \includegraphics[width=0.65\linewidth]{figs/diora_v5.png}
  \caption{
  Diagram of our proposed direct approach to unsupervised spoken constituency parsing, using only raw speech and unpaired text.
  Textual transcripts of the input sentence are only shown for illustrative purposes.
  }
  \label{fig:weight_line}
\end{figure*}

To the best of our knowledge, syntactic parsing on speech was done exclusively in a supervised manner, using paired text transcripts and syntactic labels for training. 
However, these approaches cannot be applied to low-resource languages without paired data. 
This motivates us to explore a more realistic setting where only raw speech and limited unpaired textual data are available.
% This means that either speech recognition should also be performed in an unsupervised manner with raw speech and unpaired text, or parsers should be able to directly infer syntactic structure from spoken input. 
% The most successful unsupervised speech recognition systems operate at a phonetic level, and word-level transcripts are obtained with a given lexicon. This necessitates separately training speech recognition models and parsers on different criteria, which means that any ASR errors made are irrecoverable. This raises the question: How well does the cascaded approach function under unsupervised settings?

% On the other hand, continuous speech is known to carry acoustic/prosodic information that benefits constituency parsing \cite{kahn-etal-2005-effective,dreyer2007exploiting}. The locations of pauses in sentences correlate with syntactic structure \cite{GROSJEAN197958}, and listeners also use prosodic information to resolve ambiguity \cite{price1991use}.
%Transcribing speech into text removes the possibility to benefit from this additional information. 
% This raises a second question: Can we directly parse spoken utterances without an intermediate text phase to benefit from additional information and potentially reach more globally optimized performance?

% Additionally, spoken sentences may contain disfluencies, filled pauses (“uh”, “um”), and other artifacts that do not adhere to the grammar in conventional parsers. Previous works in constituency and dependency parsing attempt to detect and remove these artifacts before parsing [9], model speech repairs [10], or jointly perform disfluency detection and parsing [11,12], to varying degrees of success.
% This motivates us to explore different approaches to unsupervised spoken syntactic parsing, without using any supervisory data in the form of speech-text pairs or syntax tree-text pairs.

With no speech-text pairs or tree-text pairs, the first approach to unsupervised spoken constituency parsing is to cascade unsupervised ASR with an unsupervised parser. We compare training the parser on limited unpaired text and ASR transcripts and find that training on ASR transcripts does not help the model parse ASR transcripts of the same domain.

We also propose a framework to directly parse spoken input without any intermediate textual form.
First, we split an utterance into word-level segments, and transform each segment into a continuous embedding. Then we directly use this sequence of segment embeddings as input for our unsupervised parser.
We refer to this as the direct approach.
% We empirically show that direct systems perform comparably to cascade systems with similarly accurate segmentation three-stage direct system builds on recent advances in self-supervised speech representation learning and unsupervised constituency parsing models on text to infer parse trees from segments of speech, without a potentially erroneous intermediate stage of word discovery. We show that direct systems are able to discover how English and Korean parse trees tend to branch towards the right and the left, respectively.

\vspace{1mm} \noindent \textbf{Contributions.}
(1) We perform the first investigation of unsupervised constituency parsing on spoken sentences using only raw speech and unpaired text. 
(2) We demonstrate that for parsing ASR transcripts, training on limited unpaired text is still better than training on ASR transcripts, and we quantify the effects of ASR errors on unsupervised parsing.
(3) We propose a framework to directly parse continuous speech without intermediate lexical unit discovery.
(4) We show that our direct approach may induce parse trees with the correct branching direction for different spoken languages.
% Our framework is applicable to different spoken language processing tasks.
% Finally, we show that the speech representations extracted by self-supervised pretrained models can be used for syntactic applications.

% \begin{figure*}[t]
%   \centering
%   \includegraphics[scale=0.3]{pic.png}
%   \vspace{-0.5cm}
%   \caption{Illustration of our two approaches.}
%   \label{fig:weight_line}
% \end{figure*}
% \vspace{-3mm}
\section{RELATED WORK}
\label{sec:format}
% \vspace{-3mm}
\subsection{Unsupervised Constituency Parsing}
\label{ssec:subhead}
% \vspace{-3mm}
Previous studies in unsupervised constituency parsing focus on obtaining constituency tree structures from large unannotated text corpora, usually by encouraging neural models to follow syntactic structure \cite{shen2018neural,shen2018ordered,kim2019unsupervised}, or parameterizing linguistic models with neural networks \cite{drozdov-etal-2019-unsupervised-latent,zhu-etal-2020-return,yang-etal-2021-neural}.

A recent line of work in visually-grounded grammar induction leverages paired images to improve unsupervised constituency parsing \cite{shi-etal-2019-visually,zhao2020visually,wan2021unsupervised}. We consider AV-NSL \cite{anonymous2023textless} in particular to be most relevant to our work, as they extend this approach to audio-visual learning and attempt to learn constituency parse trees from raw speech and image pairs. Unlike AV-NSL, our work does not rely on paired visual grounding data.

% \vspace{-3mm}
\subsection{Parsing Speech with Supervision}
\label{ssec:subhead}
% \vspace{-3mm}
% Even under the supervised setting, research in syntactic parsing on speech often unrealistically assumes the availability of human-labelled transcripts. 
Past works on syntactic parsing of speech address topics such as disfluency detection \cite{charniak2001edit,honnibal2014joint}, or incorporating prosodic cues \cite{kahn2005effective,dreyer07_interspeech}. However, most previous works require oracle transcripts, which is an unrealistic setting.
Yoshikawa et al. \cite{yoshikawa-etal-2016-joint} shows that it is possible to build a supervised dependency parser that jointly detect disfluencies and ASR errors, and Pupier et al. \cite{pupier22_interspeech} build an end-to-end supervised dependency parser for French that jointly predicts transcription and dependency tree from raw speech signals. Both works show an improvement over cascading baseline systems.

Additionally, prosodic features are shown to be closely related to syntax \cite{GROSJEAN197958,price1991use} and beneficial for both constituency parsing \cite{huang-harper-2010-appropriately,tran2018parsing} and dependency parsing \cite{ghaly2020using}, even under the presence of ASR errors \cite{tran21_interspeech}. These works motivate us to explore ways of using speech features to improve unsupervised constituency parsing of spoken data.

% \vspace{-3mm}
\subsection{Unsupervised Spoken Language Modeling}
\label{ssec:subhead}
% \vspace{-3mm}
Unsupervised spoken language modeling \cite{lakhotia-etal-2021-generative} aims to learn a spoken language model that simultaneously learns different levels of linguistic structure from raw speech signals with little or no textual data. 
The ZeroSpeech Challenge 2021 \cite{nguyen2020zero,dunbar2021zero} proposes to evaluate such a model at the acoustic, lexical, syntactic, and semantic levels. They find that while current spoken language models excel at the acoustic and lexical levels, higher levels of linguistic structure are much more difficult to model. They only require their models to be able to determine how grammatical a sentence is, while our work aims to solve the more challenging problem of producing the exact constituency structure of a sentence.

\vspace{-1mm}
\section{METHOD}
\label{sec:pagestyle}
\vspace{-1mm}
\subsection{Background}
\label{ssec:subhead}
% \vspace{-2mm}

Constituency parsing is usually formulated under the binary setting in order to reduce computation complexity. \cite{10.5555/1214993}
This setting entails that for a sentence with $n$ words $\{{x}_{1}, {x}_{2}, ...,  {x}_{n}\}$, each constituent spanning $x_{i:j}$ is composed of two constituents spanning $x_{i:k}$ and $x_{k+1:j}$ for some $k$ such that $i \leq k < j$. 
Our unsupervised parser follows the chart-based Deep Inside-Outside Recursive Autoencoder (DIORA) framework proposed by \cite{drozdov-etal-2019-unsupervised-latent} to produce binary parse trees without using any syntactically labeled data.  

\vspace{-2mm}
% \noindent \textbf{Chart-based Constituency Parsing}
\subsubsection{Chart-based Constituency Parsing}
\label{sssec:chartbased}
Chart-based parsers find the optimal tree out of all valid binary parse trees by filling the upper-triangular portion of an $n \times n$ chart with a score $s_{i,j}$ for each cell. For $1 \leq i < j \leq n $, the score in cell $(i,j)$ represents how likely the span $x_{i:j}$ is a constituent. The CKY dynamic programming algorithm \cite{kasami1966efficient,younger1967recognition} is then used to determine the parse tree with the highest total score. 

\vspace{-2mm}
\subsubsection{Unsupervised Parser Architecture: DIORA}
\label{sssec:diora}
DIORA consists of an encoder and a decoder, and operates similarly to masked language models. 
The framework recursively encodes all but one of the words from the input sentence as a vector, and optimizes that vector to reconstruct the missing word. 
The core assumption is that the most efficient weights to produce such an encoding can be used as scores for chart-based constituency parsing.
DIORA initially represents the input sentence with pretrained ELMo character embeddings, but subsequent work \cite{wan2021unsupervised} shows that the framework can also be used with randomly initialized word embeddings.

% \vspace{-2mm}
\subsection{Cascading Parsing with Unsupervised ASR}
\label{ssec:subhead}
% \vspace{-2mm}
A straightforward approach to unsupervised spoken constituency parsing is to obtain word-level transcripts from unsupervised ASR, then represent each word with a randomly initialized vector. 
An unsupervised parser can then produce parse trees using these vector sequences as input. 

Our unsupervised ASR system adopts the wav2vec-U framework \cite{baevski2021unsupervised}.
wav2vec-U first phonemizes unpaired text data, then performs a series of preprocessing steps on unlabeled speech to produce higher-level features with length similar to phoneme sequences. 
They use adversarial training to train a model to predict phoneme sequences from speech features. 
A weighted finite-state transducer (WFST) trained on the unpaired text data is then used to decode the output into words. Further improvements are be obtained through Hidden Markov Model (HMM) self-training.
% which refers to using the predicted phoneme sequences as pseudo-labels to train an additional hidden Markov model (HMM). 
The phoneme output of the HMM achieves a lower phone error rate and can be decoded into more accurate word-level transcripts. 

% Silence is first removed from the speech data with an unsupervised voice activity detection system; then initial speech features are extracted with a pre-trained self-supervised speech model, wav2vec 2.0, and further processed with three steps: dimension reduction using principal component analysis, k-means clustering to merge adjacent frames that match the same cluster center, and mean pooling on the time axis to further reduce sequence length. Unpaired text is phonemeized with an off-the-shelf grapheme-to-phoneme toolkit using a given lexicon, and silence tokens are randomly inserted back to the phoneme sequence to compensate for imperfect silence removal in the speech preprocessing step. 

% A single CNN layer is used as a generator that takes the preprocessed speech segment features as input, and outputs a probability distribution for each segment. With consecutive segments that match the same phoneme, one segment is sampled in the final output sequence. The generator is adversarially trained against a two-layer CNN discriminator that alternates between taking generator output and phonemized text as input, where phonemized text is represented as a sequence of one-hot vectors.

% After greedily decoding generator output into phoneme sequences, a weighted finite state transducer (WFST) trained on the unpaired text data is used to decode the output into words. 

% In order to infer constituency structure from continuous embeddings, we takes advantage of the unsupervised parser framework proposed in that only required word-level embeddings for input. In the cascade scenario, word embeddings are randomly initialized 1024-dimensional vectors, whereas or continuous embeddings aggregated from frame-level representation of speech segments in the integrated approach. We describe our parser framework and the two methods of acquiring word-level embeddings in detail below. 
% \vspace{-2mm}
\subsection{Direct Parsing on Speech Segments}
\label{ssec:subhead}
% \vspace{-2mm}
% In order to allow our parser to potentially benefit from continuous information in speech, we do not discretize our features, and instead 

The direct parsing approach extends the DIORA framework by training on continuous word-level speech embeddings instead of ELMo embeddings. 
Using continuous segment representations allows our parser to benefit from continuous information in speech, in comparison to the discretization approach recently proposed in spoken language modeling \cite{lakhotia-etal-2021-generative}. 
This design choice is supported by AV-NSL \cite{anonymous2023textless}, which finds that continuous segment representations outperforms discrete representations for audio-visual parsing.

From each spoken utterance, we prepare (1) frame-level features, and (2) word-level segments.
Frame-level features can extracted from a pretrained self-supervised speech model such as XLSR-53 \cite{baevski2021unsupervised}, and word-level segments can be determined with unsupervised word segmentation models \cite{bhati21_interspeech,kamper2022word}.
Mirroring AV-NSL, we represent each segment with a continuous embedding parameterized by a simple weighted average of frame-level features. 
Weights are determined by a learnable two-layer MLP that is jointly optimized with the parser. 
This sequence of word-level segment embeddings is then directly used as input for our parser, then jointly optimized with the reconstruction loss proposed in DIORA \cite{drozdov-etal-2019-unsupervised-latent}.

% \begin{table}[htb]
% \centering
% %\setlength\tabcolsep{4pt} 
% \begin{tabular}{cccc}
% \toprule
% \multirow{2}{*}{ } & \multirow{2}{*}{w/ self training} & \multirow{2}{*}{w/o self training}  \\ 
% \\
% \midrule
% Training set    & 13.15 & 25.28 \\ %& \multicolumn{2}{c}{24.73} \\
% Validation set  & \textcolor{red}{13.24} & \textcolor{red}{23.51} \\ %& \multicolumn{2}{c}{56.99} \\
% Testing set     & \textcolor{red}{13.08} & \textcolor{red}{23.41} \\ %& \multicolumn{2}{c}{33.14} \\
% \bottomrule
% \end{tabular}
% \caption{WER of the two unsupervised ASR models}
% \label{table:1}
% \end{table}

% \section{Experiments}
% \label{sec:typestyle}

% \begin{table*}[t]
% \centering
% %\setlength\tabcolsep{4pt} 
% \begin{tabular}{cccc}
% \toprule
% \multirow{2}{*}{Training Setting} & \multirow{2}{*}{Parser Input} & \multicolumn{2}{|c}{$F_1$} \\ \cline{3-4} 
% &  & \multicolumn{1}{|c}{Sup. Sel.} & Unsup. Sel. \\
% \midrule
% \multicolumn{1}{l}{\textit{Rule-based}} \\
% Left branching                      & text & 24.68 & - \\ %& \multicolumn{2}{c}{24.73} \\
% Right branching                     & text & 57.11 & - \\ %& \multicolumn{2}{c}{56.99} \\
% Random                              & text & 33.25 & - \\ %& \multicolumn{2}{c}{33.14} \\
% \midrule
% \multicolumn{1}{l}{\textit{Topline}} \\
% Entire train set                    & text & \textbf{59.02} $\pm$ 2.53 & 57.15 $\pm$ 2.09 \\
% \midrule
% \multicolumn{1}{l}{\textit{Cascasde with unsupervised ASR}} \\
% \textcolor{red}{Unpaired text}                       & text & 58.84 $\pm$ 2.11 & 57.66 $\pm$ 2.59 \\
% ASR-ST text (13.15\% WER)           & text & 54.54 $\pm$ 2.32 & 53.14 $\pm$ 2.51 \\
% ASR text (25.28\% WER)              & text & 51.46 $\pm$ 0.67 & 49.17 $\pm$ 0.57 \\
% \midrule 
% \midrule
% \multicolumn{1}{l}{\textit{Topline}} \\
% XLSR-53 + oracle segmentation       & speech & 57.11 $\pm$ 0.00 & 57.11 $\pm$ 0.00 \\
% \midrule 
% \multicolumn{1}{l}{\textit{Speech + Word Segmentation}} \\
% % wav2vec 2.0 + oracle segmentation   & speech & 54.20 $\pm$ 3.00 & 52.50 $\pm$ 5.19 \\
% % wav2vec 2.0 + better ASR            & speech & 55.39 $\pm$ 2.52 & 53.33 $\pm$ 6.64 \\
% % wav2vec 2.0 + ASR                   & speech & 51.77 $\pm$ 4.47 & 48.65 $\pm$ 7.61 \\
% XLSR-53 + ASR-ST                    & speech & 55.71 $\pm$ 2.81 & 55.16 $\pm$ 3.90 \\
% XLSR-53 + ASR                       & speech & 55.84 $\pm$ 2.54 & 54.43 $\pm$ 5.35 \\
% \midrule 
% \multicolumn{1}{l}{\textit{Speech only}} \\
% % wav2vec 2.0 + fixed length segmentation     & speech & 55.01 $\pm$ 3.29 & 51.34 $\pm$ 7.65 \\
% XLSR-53 + fixed length segmentation         & speech & 56.06 $\pm$ 2.10 & 55.38 $\pm$ 3.47 \\
% \midrule 
% \midrule 
% \multicolumn{1}{l}{\textit{Concat. speech and text}} \\
% XLSR-53 + oracle seg. and text       & speech & 57.11 $\pm$ 0.00 & 57.11 $\pm$ 0.00 \\
% \bottomrule
% \end{tabular}
% \caption{Parsing capability of systems, as measured by corpus-level $F_1$ score on test set of SpokenCOCO when provided oracle segmentation/transcripts}
% \label{table:2}
% \end{table*}
% \vspace{-3mm}
\section{Experiments}
\label{sec:exps}
% \vspace{-3mm}
\subsection{Datasets, Preprocessing, and Hyperparameters}
\label{ssec:subhead}
% \vspace{-2mm}
Experiments are mainly conducted on the SpokenCOCO dataset \cite{hsu-etal-2021-text}, 
% to allow better comparison with previous work \cite{wan2021unsupervised,anonymous2023textless}.
a 742h English read-speech dataset produced by 2.3k speakers reading the captions in MSCOCO \cite{lin2014microsoft}. 
Each image in MSCOCO corresponds to 5 captions on average. 
Following \cite{zhao2020visually}, we use the spoken captions of the 83k/5k/5k image split for training, validation, and testing respectively. The textual captions of the remaining 31k images are used as unpaired text data for unsupervised ASR. 
We focus on the more practical setting of unsupervised spoken constituency parsing using speech and unpaired text data only, hence we do not utilize the image data. 

Additional experiments in Korean are done on the Zeroth-Korean corpus\footnote{\href{https://github.com/goodatlas/zeroth}{https://github.com/goodatlas/zeroth}}, which contains 51.6hrs of audio spoken by 105 speakers for training, and 1.6hrs by 10 different speakers for testing.
We use the utterances of 10 speakers in the original training set for validation.

We note that due to a lack of labelled speech data, we are limited to experimenting on high-resource languages.
Following \cite{shi-etal-2019-visually, zhao2020visually}, ground-truth parse trees are obtained from the outputs of an off-the-shelf parser \cite{kitaev-klein-2018-constituency} on the normalized text captions. 
Punctuation is removed from the trees, and we run forced alignment using the Montreal Forced Aligner \cite{mcauliffe17_interspeech} to obtain oracle word boundaries.

For all experiments, we use the same hyperparameters as the randomly initialized DIORA experiment in \cite{wan2021unsupervised}, with a batch size of 32 and learning rate of $5e-3$. We perform unsupervised model selection with the reconstruction loss of DIORA on the validation set. Our cascading and direct systems are trained for 10 epochs and 2000 batches respectively, as we found our direct systems to converge much faster. Further details are available in our training code\footnote{\href{https://github.com/roger-tseng/speech-parsing}{https://github.com/roger-tseng/speech-parsing}}.

% \vspace{-3mm}
\subsection{Evaluation}
\label{ssec:subhead}
% \vspace{-2mm}
Unsupervised constituency parsing on text is typically evaluated with $F_1$ score of constituents, where a match is only counted if a predicted constituent and a oracle constituent consist of the exact same words. However, erroneous word segmentation or ASR may introduce mismatch in the number of word-level leaves between model predictions and ground truth parse trees. Therefore, we match the constituents first by calculating an alignment between our word-level segments and oracle text, similar to SParseval \cite{roark2006sparseval}. 

We use forced alignment to determine the spans of audio that correspond to each word in the oracle sentence. 
We then compute the optimal one-to-one mapping that maximizes total span overlap between oracle segmentation and our proposed word segmentations\footnote{This mapping is determined via bipartite weight mapping, where the nodes are speech segments and the weights are given by the overlap duration across nodes.}. 
This allows us to first match nodes between predicted and ground truth parse trees, and calculate an $F_1$ score that jointly considers segmentation and parsing performance. We include whole sentence spans in our evaluation, in order to compare to AV-NSL \cite{anonymous2023textless}.
%Since the calculated one-to-one matching may be sub-optimal, we also evaluate the parsing capability of our systems alone, by providing ground truth segmentation/transcript during testing for our direct/cascade systems respectively.
For all experiments, we evaluate fully unsupervised parsing with this proposed $F_1$ score, and report the average and standard deviation of corpus-level $F_1$ of the best model before convergence over five different random seeds. 
% \vspace{-3mm}
\subsection{Results of Cascading Systems}
\label{ssec:subhead}
% \vspace{-2mm}
% \begin{table}[htb]
% \centering
% %\setlength\tabcolsep{4pt} 
% \begin{tabular}{ccc}
% \toprule
% \multirow{2}{*}{ } & AST-ST & ASR\\ 
% & (w/ self training) & (w/o self training)  \\ 
% \midrule
% Training set    & 13.15 & 25.28 \\ %& \multicolumn{2}{c}{24.73} \\
% Validation set  & \textcolor{red}{13.24} & \textcolor{red}{23.51} \\ %& \multicolumn{2}{c}{56.99} \\
% Testing set     & \textcolor{red}{13.08} & \textcolor{red}{23.41} \\ %& \multicolumn{2}{c}{33.14} \\
% \bottomrule
% \end{tabular}
% \caption{WER of the two unsupervised ASR models}
% \label{table:1}
% \end{table}

We train two unsupervised ASR models to observe how varying accuracy of ASR may affect parsing performance.
The two models are trained with and without self-training following the original setup of wav2vec-U. 
They are denoted as ASR-ST and ASR respectively.
% For the cascade systems, we use the unpaired text data alone to build vocabularies and 4-gram WFST language models. 
We use a 100-hour subset of speech from the training set, and 150k unpaired text sentences as our training data. 
% Word-level transcriptions are decoded from phoneme output sequences with the WFST trained on unpaired text. 
% The trained models are then used to infer ASR transcripts for the entire SpokenCOCO dataset.
Word-level transcriptions for the entire SpokenCOCO dataset are decoded from the phoneme output sequences of the ASR models. Word error rate of training set transcripts is 13.15\% and 28.25\% for AST-ST and ASR.

The training set transcripts are then used to train our parser. We follow \cite{zhao2020visually} and use the 10,000 most commonly occurring words in their respective training set transcripts as the vocabulary set for the parser.
Since we assume the availability of unpaired text data in the cascade scenario, we also consider training a parser from the unpaired text data alone. 

% \begin{table}[htb]
% \centering
% %\setlength\tabcolsep{4pt} 
% \begin{tabular}{ccc}
% \toprule
% \multirow{2}{*}{Training Setting} & \multicolumn{2}{|c}{$F_1$} \\ \cline{2-3} 
%  & \multicolumn{1}{|c}{Sup. Sel.} & Unsup. Sel. \\
% \midrule
% \multicolumn{1}{l}{\textit{Topline}} \\
% Entire train set        & \textbf{59.02} $\pm$ 2.53 & 57.15 $\pm$ 2.09 \\
% \midrule
% \midrule
% Unpaired text           & 44.62 $\pm$ 1.57 & 44.08 $\pm$ 1.64 \\
% ASR-ST text             & 41.63 $\pm$ 1.75 & 40.53 $\pm$ 1.65 \\ %& \multicolumn{2}{c}{24.73} \\
% \midrule
% Unpaired text           & 35.80 $\pm$ 1.23 & 34.97 $\pm$ 1.32 \\
% ASR text                & 32.13 $\pm$ 0.37 & 31.01 $\pm$ 1.17 \\ %& \multicolumn{2}{c}{56.99} \\
% \bottomrule
% \end{tabular}
% \caption{WER of the two unsupervised ASR models}
% \label{table:1}
% \end{table}

\begin{table}[htb]
\centering
\renewcommand{\arraystretch}{1.1}
% \setlength\tabcolsep{4pt} 
\begin{tabular}{lcccc}
\toprule
%  & Training split & Training & Testing & $F_1$ \\ 
\multirow{2}{*}{} & \multirow{2}{*}{Training split} & \multirow{2}{*}{Training} & \multirow{2}{*}{Testing} & \multirow{2}{*}{$F_1$} \\ 
\\
\midrule
\midrule
(A) & Entire train set        & oracle & oracle & 57.15 $\pm$ 2.09 \\
\midrule
(B) & Unpaired text           & oracle & ASR-ST & 44.08 $\pm$ 1.64 \\
(C) & Entire train set        & ASR-ST & ASR-ST & 40.53 $\pm$ 1.65 \\ %& \multicolumn{2}{c}{24.73} \\
\midrule
(D) & Unpaired text           & oracle & ASR & 34.97 $\pm$ 1.32 \\
(E) & Entire train set        & ASR & ASR & 31.01 $\pm$ 1.17 \\ %& \multicolumn{2}{c}{56.99} \\
\bottomrule
\end{tabular}
\caption{$F_1$ score of our casacading systems. 
The leftmost column lists whether training data comes from the unpaired text split or the training split.
We also list the $F_1$ score obtained by training and testing our parser on oracle transcripts in row (A) as a topline.
}
\label{table:1}
\end{table}
% Table 1 shows the scores for our cascaded systems of unsupervised ASR and unsupervised parsing. 
% We also list the $F_1$ score obtained by training and testing our parser on oracle transcripts in the first row as a topline.

% Consistent with previous work \cite{wan2021unsupervised}, unsupervised constituency parsing on text determines a general tree structure without pretrained word embeddings.

\vspace{1mm} \noindent \textbf{Effect of ASR errors on parsing.}
When comparing results across different blocks, we can see that parsing accuracy is heavily degraded when ASR errors are present. Additionally, one might expect that training a parser on ASR transcripts would allow it to better handle text with ASR errors during inference. However, by comparing rows (B)/(C) and rows (D)/(E), we see that training our parser on unpaired oracle text is consistently better than training on ASR transcripts. 
We note that this occurs in spite of the training set being nearly 3x larger than the unpaired text set. 
% We find that training a parser on oracle despite being of the same domain and trained on nearly 3x more sentences, the parser trained on ASR texts shows decreased accuracy compared to the parser trained on ground truth text.
We hypothesize that this is partially caused by the decoding process of wav2vec-U. Uncommon words rarely get decoded by the WFST language model. As a result, the ASR transcripts contain fewer types of words compared to the original vocabulary, and parser trained on imperfect transcripts deal with more out-of-vocabulary words. The training set transcripts from the AST-ST model only use 8.2k words out of the original 16k words present in the ground truth captions.
% \vspace{-3mm}
\subsection{Results of Direct Systems}
\label{ssec:subhead}
% \vspace{-2mm}
% \begin{table}[htb]
% \centering
% %\setlength\tabcolsep{4pt} 
% \begin{tabular}{ccc}
% \toprule
% \multirow{2}{*}{Training Setting} & \multicolumn{2}{|c}{$F_1$} \\ \cline{2-3} 
% & \multicolumn{1}{|c}{Sup. Sel.} & Unsup. Sel. \\
% \midrule
% \multicolumn{1}{l}{\textit{Rule-based}} \\
% Right branching                     & x & - \\ %& \multicolumn{2}{c}{56.99} \\
% Random                              & x & - \\ %& \multicolumn{2}{c}{33.14} \\
% \midrule
% \multicolumn{1}{l}{\textit{Cascasde}} \\
% Unpaired text                       & x $\pm$ x & x $\pm$ x \\
% ASR text (13.15\% WER)              & x $\pm$ x & x $\pm$ x \\
% ASR text (25.28\% WER)              & x $\pm$ x & x $\pm$ x \\
% \midrule 
% \multicolumn{1}{l}{\textit{Speech + Segmentation}} \\
% % wav2vec 2.0 + oracle segmentation   & speech & 54.20 $\pm$ 3.00 & 52.50 $\pm$ 5.19 \\
% % wav2vec 2.0 + better ASR            & speech & 55.39 $\pm$ 2.52 & 53.33 $\pm$ 6.64 \\
% % wav2vec 2.0 + ASR                   & speech & 51.77 $\pm$ 4.47 & 48.65 $\pm$ 7.61 \\
% XLSR-53 + oracle seg.               & 39.48 $\pm$ 2.78 & 36.50 $\pm$ 4.07 \\
% XLSR-53 + better ASR                & 36.91 $\pm$ 3.74 & 34.55 $\pm$ 6.01 \\
% XLSR-53 + ASR                       & 39.39 $\pm$ 1.86 & 35.24 $\pm$ 3.98 \\
% \midrule 
% \multicolumn{1}{l}{\textit{Speech only}} \\
% % wav2vec 2.0 + fixed length segmentation     & speech & 55.01 $\pm$ 3.29 & 51.34 $\pm$ 7.65 \\
% XLSR-53 + fixed seg.       & x $\pm$ x & x $\pm$ x \\
% \bottomrule
% \end{tabular}
% \caption{\textcolor{red}{Fully unsupervised parsing results on test set of SpokenCOCO}}
% \label{table:3}
% \end{table}

For direct systems, only speech features and word boundaries are required. 
We extract frame-level speech features from the 14\textsuperscript{th} layer of XLSR-53 \cite{conneau21_interspeech}, a publicly available wav2vec 2.0 model pretrained on 53 languages.
For word boundaries, we naively split all utterances into 0.5-second segments, to encompass approximately one word in each segment\footnote{As a reference, average word length in SpokenCOCO is about 0.4 seconds, see Appendix of \cite{hsu-etal-2021-text}.}.

Since this method of segmentation is very inaccurate, the parsing results are similarly poor (Table~\ref{table:2} row (D)).
However, when provided with ground truth segmentation during testing (Table~\ref{table:2} row (C)), the parser trained on fixed length segments is able to achieve a performance similar to the parser trained on ground truth. 
This suggests that our direct system is limited by segmentation accuracy during inference. 
%We find speech-only unsupervised word segmentation models to be insufficiently accurate for inference, so we used boundaries determined via during inference time, and explore the effects of using different segmentation methods during training.

% \section{DISCUSSION}
% \label{sec:majhead}
\vspace{-2mm}
\subsection{A Hybrid Approach: Segmenting speech with word boundaries determined by unsupervised ASR}
\label{ssec:subhead}
% \vspace{-3mm}
\begin{table}[htb]
\renewcommand{\arraystretch}{1.1}
\centering
\setlength\tabcolsep{4pt} 
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{} & \multirow{2}{*}{Approach} & \multicolumn{2}{c}{Segmentation} & \multirow{2}{*}{$F_1$} \\ 
& & Training & Testing \\
\midrule
\midrule
(A) & AV-NSL & ground truth & ground truth     & 55.51 \\
(B) & Ours    & ground truth & ground truth     & 57.11 $\pm$ 0.00 \\
\midrule
(C) & Direct & every 0.5 sec. & ground truth   & 57.10 $\pm$ 0.01 \\
(D) & Direct & every 0.5 sec. & every 0.5 sec. & 3.88 $\pm$ 0.00 \\
\midrule
(E) & Hybrid & AST-ST       & AST-ST       & 40.44 $\pm$ 1.72 \\
(F) & Hybrid & ASR          & ASR          & 28.49 $\pm$ 0.57 \\
\bottomrule
\end{tabular}
\caption{$F_1$ score of direct and hybrid systems.
We include $F_1$ scores obtained by training and testing using oracle segmentation in rows (A) and (B) as toplines.
}
\label{table:2}
\end{table}
% \vspace{-3mm}

\noindent 
We compare our systems with AV-NSL under oracle segmentation settings in rows (A) and (B).
We find that our parser outperforms AV-NSL despite not using any visual grounding information.
This suggests that the DIORA framework may be better suited for unsupervised spoken constituency parsing.

We experimented with a speech-only unsupervised word segmentation method \cite{fuchs22_interspeech}, but found it to be suboptimal. Therefore, we consider a hybrid approach that uses forced alignment to obtain word boundaries from unsupervised ASR transcripts.
We find that when word boundaries are sufficiently accurate, using word boundaries alone can achieve similar accuracy to cascading systems, as shown in Table~\ref{table:1} row (C) and Table~\ref{table:2} row (E).
This implies that accurate word segmentation is necessary for unsupervised constituency parsing from speech, which aligns with the findings in AV-NSL.

% We consider both supervised and unsupervised model selection to better gauge model performance, where unsupervised model selection is determined by the reconstruction loss on the validation set.

% Since our constituent matching scheme for imperfect word transcriptions may be inaccurate, we report the $F_1$ scores of all systems when given accurate word transcriptions/ boundaries during inference time in Table 2, in order to accurately compare parsing accuracy.

\vspace{-3mm}
\subsection{On the Inductive Bias and Trivial Tree Structure}
% \subsection{Direct systems tend to output trivial trees}
\label{ssec:subhead}
% \vspace{-2mm}
% Furthermore, we in a low-resource scenario where unsupervised ASR fails, speech-only word segmentation may be able to provide sufficient information for unsupervised parsing.
Due to the head-initial property of English \cite{baker2008atoms}, constituency parse trees tend to be right-branching, especially if punctuation is removed.
On the other hand, for head-final languages such as Japanese and Korean, trees are left-branching instead.

In our direct and hybrid systems, we observe that our models tend to converge to producing right-branching trees on SpokenCOCO\footnote{We note that right-branching is a difficult baseline even for parsers trained on oracle text. As shown in Table~\ref{table:1} row (A) and \cite{wan2021unsupervised}, unsupervised text parsers only marginally improve on right-branching trees.}. 
It is worthwhile to note that our framework does not apply any inductive bias that encourages the model to favor right-branching trees; hence, it is non-trivial for such a phenomenon to emerge. 
We hypothesize that our systems learn a language's branching direction from continuous spoken input without supervision. 
% This occurs for all segmentation methods, including the textless setting where segmentation is done every 0.5 seconds. 
% When provided with ground truth word boundaries, our parser converges to right-branching trees every run.

We empirically verify this claim by conducting experiments on Korean, a primarily left-branching language. 
Over 5 runs with different random seeds, 3 runs converge towards producing some left-branching structures (Fig.~\ref{fig:res}),  supporting our hypothesis. 
% Despite being trivially trained on segments of fixed length, this shows that our direct approach is still capable of discovering the branching directionality of different languages.

\begin{table}[htb]
% \renewcommand{\arraystretch}{1.1}
\centering
%\setlength\tabcolsep{4pt} 
\begin{tabular}{cccc}
\toprule
\multirow{2}{*}{} & \multirow{2}{*}{English} & \multirow{2}{*}{Korean}  \\ 
\\
\midrule
\midrule
\multicolumn{1}{l}{\textit{Rule-based}} \\
Left branching              & 24.68 & 27.15 \\ %& \multicolumn{2}{c}{24.73} \\
Right branching             & 57.11 & 7.60 \\ %& \multicolumn{2}{c}{56.99} \\
% Random                      & 33.25 & 23.15 \\ %& \multicolumn{2}{c}{33.14} \\
\midrule 
\multicolumn{1}{l}{\textit{Speech only}} \\
0.5 sec. segmentation       & 57.10 $\pm$ 0.01 & 18.53 $\pm$ 8.99 \\
\bottomrule
\end{tabular}
\caption{Results of direct systems trained on English (right-branching) and Korean (left-branching), respectively, using the same setting as Table~\ref{table:2} row (C).}
\label{table:r}
\end{table}
\vspace{-3mm}
% \subsubsection{Sub-subheadings}
% \label{sssec:subsubhead}

% Sub-subheadings, as in this paragraph, are discouraged. However, if you
% must use them, they should appear in lower case (initial word
% capitalized) and start at the left margin on a separate line, with paragraph
% text beginning on the following line.  They should be in italics.

% In LaTeX, to start a new column (but not a new page) and help balance the
% last-page column lengths, you can use the command ``$\backslash$pagebreak'' as
% demonstrated on this page (see the LaTeX source below).

% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
\begin{figure}[htb]

\begin{minipage}[b]{1.05\linewidth}
  \centering
  \centerline{\includegraphics[width=8.0cm]{figs/gen.png}}
%  \vspace{1.5cm}
  \centerline{(a) Generated parse tree}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{1.05\linewidth}
  \centering
  \centerline{\includegraphics[width=8.0cm]{figs/gt.png}}
%  \vspace{1.5cm}
  \centerline{(b) Oracle parse tree}\medskip
\end{minipage}
\vspace{-6mm}
\caption{A sample pair of oracle and generated Korean parse trees. Only textual transcripts are shown for ease of visualization.}
\label{fig:res}
%
\end{figure}
\vspace{-6mm}

% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak
\section{CONCLUSION}
\label{sec:conclusioni}
% \vspace{-3mm}
The work investigates cascading and direct approaches to perform constituency parsing on speech input, while only requiring raw speech and unpaired data. 
For cascading systems, we empirically show that parsers trained on ASR transcripts do not parse ASR transcripts better than parsers trained on unpaired text.
For direct and hybrid systems, our results suggest that using segmentation alone may be sufficient to produce unsupervised parse trees.

For future work, we expect to extend our system to end-to-end training to jointly optimize word segmentation and parsing. 
Additionally, we also plan to investigate whether unsupervised spoken constituency parsing can be improve other speech processing tasks under low-resource scenarios, such as text-to-speech, spoken question answering, or spoken content retrieval.

\vspace{2mm} 
\noindent
\textbf{Acknowledgement.} 
We thank the helpful discussions with Freda Shi, Shang-Wen Li, Ali Elkahky, and Abdelrahman Mohamed. We also thank the National Center for High-performance Computing (NCHC) of National Applied Research Laboratories (NARLabs) in Taiwan for providing computational and storage resources.



\vfill\pagebreak
% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\small{
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}
}

\end{document}
