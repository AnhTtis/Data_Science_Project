@inproceedings{strubell2018linguistically,
  title={Linguistically-Informed Self-Attention for Semantic Role Labeling},
  author={Strubell, E. and others},
  booktitle={EMNLP},
  year={2018}
}
@article{kuncoro2020syntactic,
  title={Syntactic Structure Distillation Pretraining for Bidirectional Encoders},
  author={Kuncoro, A. and others},
  journal={TACL},
  year={2020}
}
@inproceedings{Guo2019,
  author={H. Guo and others},
  title={{Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS}},
  year=2019,
  booktitle={Interspeech},
  doi={10.21437/Interspeech.2019-2167},
  url={http://dx.doi.org/10.21437/Interspeech.2019-2167}
}
@inproceedings{tyagi20_interspeech,
  author={S. Tyagi and others},
  title={{Dynamic Prosody Generation for Speech Synthesis Using Linguistics-Driven Acoustic Embedding Selection}},
  year=2020,
  booktitle={Interspeech},
  doi={10.21437/Interspeech.2020-1411}
}
@INPROCEEDINGS{song2021,  
author={Song, C. and others},  
booktitle={ICASSP},   
title={Syntactic Representation Learning For Neural Network Based TTS with Syntactic Parse Tree Traversal},   
year={2021},  
volume={},  
number={},  
doi={10.1109/ICASSP39728.2021.9414671}
}
@article{kasami1966efficient,
  title={An efficient recognition and syntax-analysis algorithm for context-free languages},
  author={Kasami, T.},
  journal={Coordinated Science Laboratory Report},
  year={1966},
}
@article{younger1967recognition,
  title={Recognition and parsing of context-free languages in time n3},
  author={Younger, D. H},
  journal={Information and control},
  year={1967},
  publisher={Elsevier}
}
@book{10.5555/1214993,
author = {Jurafsky, D. and others},
title = {Speech and Language Processing (2nd Edition)},
year = {2009},
isbn = {0131873210},
publisher = {Prentice-Hall, Inc.},
address = {USA}
}
@inproceedings{
shen2018neural,
title={Neural Language Modeling by Jointly Learning Syntax and Lexicon},
author={Y. Shen and others},
booktitle={ICLR},
year={2018},
url={https://openreview.net/forum?id=rkgOLb-0W},
}
@inproceedings{
shen2018ordered,
title={Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks},
author={Y. Shen and others},
booktitle={ICLR},
year={2019},
url={https://openreview.net/forum?id=B1l6qiR5F7},
}
@inproceedings{kim2019unsupervised,
  title={Unsupervised Recurrent Neural Network Grammars},
  author={Kim, Y. and others},
  booktitle={NAACL-HLT},
  year={2019}
}
@inproceedings{wang2019tree,
  title={Tree Transformer: Integrating Tree Structures into Self-Attention},
  author={Wang, Y. and others},
  booktitle={EMNLP-IJCNLP},
  year={2019}
}
@inproceedings{kim2019compound,
  title={Compound Probabilistic Context-Free Grammars for Grammar Induction},
  author={Kim, Y. and others},
  booktitle={ACL},
  year={2019}
}
@article{zhu-etal-2020-return,
    title = "The Return of Lexical Dependencies: Neural Lexicalized {PCFG}s",
    author = "Zhu, H.  and
      others",
    journal = "TACL",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.42",
    doi = "10.1162/tacl_a_00337",
    abstract = "In this paper we demonstrate that context free grammar (CFG) based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either constituents or dependencies. Previous approaches to marry these two disparate syntactic formalisms (e.g., lexicalized PCFGs) have been plagued by sparsity, making them unsuitable for unsupervised grammar induction. However, in this work, we present novel neural models of lexicalized PCFGs that allow us to overcome sparsity problems and effectively induce both constituents and dependencies within a single model. Experiments demonstrate that this unified framework results in stronger results on both representations than achieved when modeling either formalism alone.1",
}
@inproceedings{yang-etal-2021-neural,
    title = "Neural Bi-Lexicalized {PCFG} Induction",
    author = "Yang, S.  and
      others",
    booktitle = "ACL-IJCNLP",
    year = "2021",
    url = "https://aclanthology.org/2021.acl-long.209",
    doi = "10.18653/v1/2021.acl-long.209",
    abstract = "Neural lexicalized PCFGs (L-PCFGs) have been shown effective in grammar induction. However, to reduce computational complexity, they make a strong independence assumption on the generation of the child word and thus bilexical dependencies are ignored. In this paper, we propose an approach to parameterize L-PCFGs without making implausible independence assumptions. Our approach directly models bilexical dependencies and meanwhile reduces both learning and representation complexities of L-PCFGs. Experimental results on the English WSJ dataset confirm the effectiveness of our approach in improving both running speed and unsupervised parsing performance.",
}
@inproceedings{zhao2020visually,
  title={Visually Grounded Compound PCFGs},
  author={Zhao, Y. and others},
  booktitle={EMNLP},
  year={2020}
}
@inproceedings{wan2021unsupervised,
  title={Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling},
  author={Wan, B. and others},
  booktitle={ICLR},
  year={2021}
}
@misc{
anonymous2023textless,
title={Textless Phrase Structure Induction from Visually-Grounded Speech},
author={Lai, C. I. and others},
year={2023},
url={https://openreview.net/forum?id=0c2SbGJ3Lt}
}
@inproceedings{drozdov-etal-2019-unsupervised-latent,
    title = "Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders",
    author = "Drozdov, A. and others",
    booktitle = "NAACL-HLT",
    year = "2019",
    url = "https://aclanthology.org/N19-1116",
    doi = "10.18653/v1/N19-1116",
    abstract = "We introduce the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence. During training we use dynamic programming to consider all possible binary trees over the sentence, and for inference we use the CKY algorithm to extract the highest scoring parse. DIORA outperforms previously reported results for unsupervised binary constituency parsing on the benchmark WSJ dataset.",
}
@inproceedings{huang-harper-2010-appropriately,
    title = "Appropriately Handled Prosodic Breaks Help {PCFG} Parsing",
    author = "Huang, Z.  and others",
    booktitle = "NAACL-HLT",
    year = "2010",
    url = "https://aclanthology.org/N10-1005",
}
@inproceedings{tran2018parsing,
  title={Parsing Speech: a Neural Approach to Integrating Lexical and Acoustic-Prosodic Information},
  author={Tran, T. and others},
  booktitle={NAACL-HLT},
  year={2018}
}
@inproceedings{ghaly2020using,
  title={Using Prosody to Improve Dependency Parsing},
  author={Ghaly, H. and others},
  booktitle={International Conference on Speech Prosody},
  year={2020}
}
@inproceedings{fuchs22_interspeech,
  author={T. Fuchs and others},
  title={{Unsupervised Word Segmentation using K Nearest Neighbors}},
  year=2022,
  booktitle={Interspeech},
  doi={10.21437/Interspeech.2022-11474}
}
@techreport{10.5555/864689,
author = {Carroll, G. and others},
title = {Two Experiments on Learning Probabilistic Dependency Grammars from Corpora},
year = {1992},
publisher = {Brown University},
address = {USA},
abstract = {We present a scheme for learning probabilistic dependency grammars from positive training examples plus constraints on rules. In particular, we present the results of two experiments. The first, in which the constraints were minimal, was unsuccessful. The second, with significant constraints, was successful within the bounds of the task we had set.}
}
@inproceedings{10.3115/981967.981984,
author = {Pereira, F. and others},
title = {Inside-Outside Reestimation from Partially Bracketed Corpora},
year = {1992},
url = {https://doi.org/10.3115/981967.981984},
doi = {10.3115/981967.981984},
abstract = {The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modeling of hierarchical structure than the original one. In particular, over 90% test set bracketing accuracy was achieved for grammars inferred by our algorithm from a training set of handparsed part-of-speech strings for sentences in the Air Travel Information System spoken language corpus. Finally, the new algorithm has better time complexity than the original one when sufficient bracketing is provided.},
booktitle = {ACL},
numpages = {8},
}
@inproceedings{klein-manning-2002-generative,
    title = "A Generative Constituent-Context Model for Improved Grammar Induction",
    author = "Klein, D. and others",
    booktitle = "ACL",
    year = "2002",
    url = "https://aclanthology.org/P02-1017",
    doi = "10.3115/1073083.1073106",
}
@inproceedings{klein-manning-2004-corpus,
    title = "Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency",
    author = "Klein, D.  and others",
    booktitle = "ACL",
    year = "2004",
    url = "https://aclanthology.org/P04-1061",
    doi = "10.3115/1218955.1219016",
}
@inproceedings{bod-2006-subtrees,
    title = "An All-Subtrees Approach to Unsupervised Parsing",
    author = "Bod, R.",
    booktitle = "COLING-ACL",
    year = "2006",
    url = "https://aclanthology.org/P06-1109",
    doi = "10.3115/1220175.1220284",
}
@article{honnibal2014joint,
  title={Joint incremental disfluency detection and dependency parsing},
  author={Honnibal, M. and others},
  journal={TACL},
  year={2014},
  publisher={MIT Press}
}
@inproceedings{charniak2001edit,
  title={Edit detection and parsing for transcribed speech},
  author={Charniak, E. and Johnson, M.},
  booktitle={NAACL},
  year={2001}
}
@inproceedings{kahn2005effective,
  title={Effective use of prosody in parsing conversational speech},
  author={Kahn, J. G. and others},
  booktitle={HLT/EMNLP},
  year={2005}
}
@inproceedings{hale2006pcfgs,
  title={PCFGs with syntactic and prosodic indicators of speech repairs},
  author={Hale, J. and others},
  booktitle={COLING-ACL},
  year={2006}
}
@inproceedings{dreyer07_interspeech,
  author={M. Dreyer and others},
  title={{Exploiting prosody for PCFGs with latent annotations}},
  year=2007,
  booktitle={Interspeech 2007},
  doi={10.21437/Interspeech.2007-216}
}
@inproceedings{yoshikawa-etal-2016-joint,
    title = "Joint Transition-based Dependency Parsing and Disfluency Detection for Automatic Speech Recognition Texts",
    author = "Yoshikawa, M.  and others",
    booktitle = "EMNLP",
    year = "2016",
    url = "https://aclanthology.org/D16-1109",
    doi = "10.18653/v1/D16-1109",
}
@inproceedings{pupier22_interspeech,
  author={A. Pupier and others},
  title={{End-to-End Dependency Parsing of Spoken French}},
  year=2022,
  booktitle={Interspeech},
  doi={10.21437/Interspeech.2022-381}
}
@article{GROSJEAN197958,
title = {The patterns of silence: Performance structures in sentence production},
journal = {Cognitive Psychology},
year = {1979},
issn = {0010-0285},
doi = {https://doi.org/10.1016/0010-0285(79)90004-5},
url = {https://www.sciencedirect.com/science/article/pii/0010028579900045},
author = {F. Grosjean and others},
abstract = {The pauses produced by speakers while reading familiar material were used to obtain hierarchical sentence structures. Identical structures were obtained from parsing, indicating that the performance structures of sentences are not task specific. The linguistic surface structure of a sentence is a good predictor of the pause durations. However, speakers also revealed a tendency to place pauses between segments of equal length. A simple cyclical model combining, for each pause location, an index of linguistic complexity and a measure of the distance to the midpoint of the segment, accounts for 72% of the pause time variance as opposed to 56% for the linguistic index alone. The generality of the model is shown by its good prediction of the pause durations obtained in unrelated studies in English and American Sign Language.}
}
@inproceedings{price1991use,
  title={The Use of Prosody in Syntactic Disambiguation},
  author={Price, P. and others},
  booktitle={Workshop on Speech and Natural Language},
  year={1991}
}
@inproceedings{tran21_interspeech,
  author={T. Tran and others},
  title={{Assessing the Use of Prosody in Constituency Parsing of Imperfect Transcripts}},
  year=2021,
  booktitle={Interspeech}
}
@inproceedings{dreyer2007exploiting,
  title={Exploiting prosody for PCFGs with latent annotations.},
  author={Dreyer, M. and Shafran, I.},
  booktitle={Interspeech},
  year={2007},
}
@article{lakhotia-etal-2021-generative,
    title = "On Generative Spoken Language Modeling from Raw Audio",
    author = "Lakhotia, K. and others",
    journal = "TACL",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.79",
    doi = "10.1162/tacl_a_00430",
    abstract = "Abstract We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo- text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder- dependent way, and that some combinations approach text-based systems.1",
}
@article{dunbar2021zero,
  title={The zero resource speech challenge 2021: Spoken language modelling},
  author={Dunbar, E. and others},
  journal={arXiv},
  year={2021}
}
@inproceedings{nguyen2020zero,
author={Nguyen, T. A. and others},
year={2020},
title= {The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling},
booktitle={NeurIPS SAS Workshop},
}
@inproceedings{hsu-etal-2021-text,
    title = "Text-Free Image-to-Speech Synthesis Using Learned Segmental Units",
    author = "Hsu, W. N.  and
      others",
    booktitle = "ACL-ICJNLP",
    year = "2021",
    url = "https://aclanthology.org/2021.acl-long.411",
    doi = "10.18653/v1/2021.acl-long.411",
    abstract = "In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe. We investigate several different intermediate speech representations, and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text.",
}
@inproceedings{conneau21_interspeech,
  author={A. Conneau and others},
  title={{Unsupervised Cross-Lingual Representation Learning for Speech Recognition}},
  year=2021,
  booktitle={Interspeech},
}
@inproceedings{roark2006sparseval,
  title={SParseval: Evaluation metrics for parsing speech},
  author={Roark, B. and others},
  booktitle={LREC},
  year={2006}
}
@article{baevski2021unsupervised,
  title={Unsupervised speech recognition},
  author={Baevski, A. and others},
  journal={NeurIPS},
  year={2021}
}
@inproceedings{bhati21_interspeech,
  author={S. Bhati and others},
  title={{Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation}},
  year=2021,
  booktitle={Proc. Interspeech},
  doi={10.21437/Interspeech.2021-1874}
}
@article{kamper2022word,
  title={Word Segmentation on Discovered Phone Units with Dynamic Programming and Self-Supervised Scoring},
  author={Kamper, H.},
  journal={arXiv preprint arXiv:2202.11929},
  year={2022}
}
@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, T. Y. and others},
  booktitle={ECCV},
  year={2014},
}
@inproceedings{shi-etal-2019-visually,
    title = "Visually Grounded Neural Syntax Acquisition",
    author = "Shi, H. and others",
    booktitle = "ACL",
    year = "2019",
    url = "https://aclanthology.org/P19-1180",
    doi = "10.18653/v1/P19-1180",
    abstract = "We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without any explicit supervision. The model learns by looking at natural images and reading paired captions. VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. We define concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F1 scores against gold parse trees. We find that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data. We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists. Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches.",
}
@inproceedings{kitaev-klein-2018-constituency,
    title = "Constituency Parsing with a Self-Attentive Encoder",
    author = "Kitaev, N.  and others",
    booktitle = {ACL},
    year = "2018",
    url = "https://aclanthology.org/P18-1249",
    doi = "10.18653/v1/P18-1249",
    abstract = "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
}
@inproceedings{mcauliffe17_interspeech,
  author={M. McAuliffe and others},
  title={{Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi}},
  year=2017,
  booktitle={Interspeech},
}
@book{baker2008atoms,
  title={The atoms of language: The mind's hidden rules of grammar},
  author={Baker, M. C.},
  year={2008},
  publisher={Basic books}
}