
\documentclass[ejs, noshowframe]{imsart}

%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb,mathtools}
% \RequirePackage[numbers]{natbib}
\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{graphicx}

% \usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,  bm}
\usepackage{algorithm, algpseudocode}
\usepackage{tikz}
% \usepackage{setspace}
\usepackage{multirow}

\input{shortcuts}

\arxiv{2303.11786}
\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothesis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}

\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{alg}[thm]{Algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{definition}            %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}{Remark}
\newtheorem*{example}{Example}
\newtheorem*{fact}{Fact}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Case use \theoremstyle{remark}       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{remark}
\newtheorem{case}{Case}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlocaldefs

\begin{document}
\begin{frontmatter}

\title{Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure}
%\title{A sample article title with some additional note\thanksref{t1}}
\runtitle{Skeleton Regression}
%\thankstext{T1}{A sample additional note to the title.}

\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only one address is permitted per author. %%
%% Only division, organization and e-mail is %%
%% included in the address.                  %%
%% Additional information can be included in %%
%% the Acknowledgments section if necessary. %%
%% ORCID can be inserted by command:         %%
%% \orcid{0000-0000-0000-0000}               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[A]{\fnms{Zeyu}~\snm{Wei}\ead[label=e1]{zwei5@uw.edu} \orcid{0000-0003-1614-4458}} \and
\author[A]{\fnms{Yen-Chi}~\snm{Chen}\ead[label=e2]{yenchic@uw.edu}\orcid{0000-0002-4485-306X}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address[A]{Department of Statistics,
University of Washington\printead[presep={,\ }]{e1,e2}}

% \address[B]{Department,
% University or Company Name\printead[presep={,\ }]{e2,e3}}
% \runauthor{F. Author et al.}
\end{aug}


\begin{abstract}
We introduce a new regression framework designed to deal with large-scale, complex data that lies around a low-dimensional manifold with noises. 
Our approach first constructs a graph representation, referred to as the \textit{skeleton}, to capture the underlying geometric structure. 
We then define metrics on the skeleton graph and apply nonparametric regression techniques, along with feature transformations based on the graph, to estimate the regression function.
We also discuss the limitations of some nonparametric regressors with respect to the general metric space such as the skeleton graph.
The proposed regression framework suggests a novel way to deal with data with underlying geometric structures and provides additional advantages in handling the union of multiple manifolds, additive noises, and noisy observations.
We provide statistical guarantees for the proposed method and demonstrate its effectiveness through simulations and real data examples. 
% We also discussed the feasibility and the issue of edge directions when fitting splines of some derivative smoothness on graphs.


\end{abstract}

\begin{keyword}[class=MSC]
\kwd[Primary ]{62R99}
\kwd{62G08}
\kwd{62H99}
\kwd[; secondary ]{62-04}
\end{keyword}

\begin{keyword}
\kwd{nonparametric regression}
\kwd{geometric data analysis}
\kwd{graph}
\kwd{kernel regression}
\kwd{spline}
\end{keyword}

\end{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please use \tableofcontents for articles %%
%% with 50 pages and more                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\tableofcontents




\section{Introduction}

% \begin{figure}
% \centering
% \includegraphics[width=4.5cm]{figures/Yinyang_d200plot_KM.jpeg}
% \includegraphics[width=4.5cm]{figures/Yinyang_d200plot_SC.jpeg}\\
% \includegraphics[width=4.5cm]{figures/Yinyang_d200plot_MS.jpeg}
% \includegraphics[width=4.5cm]{figures/Yinyang_d200plot_Voron.jpeg}
% \caption{Yinyang Data with dimension 200. The bottom-right is clustering result with Skeleton clustering framework and using the newly proposed Voronoi density similarity measure.}
% \label{fig::ex01}
% \end{figure}



~~~~ Many data nowadays are geometrically structured that the covariates lie around a low-dimensional manifold embedded inside a large-dimensional vector space. 
Among many geometric data analysis tasks, the estimation of functions defined on manifolds has been extensively studied in the statistical literature. 
A classical approach to explicitly account for geometric structure takes two steps: map the data to the tangent plane or some embedding space and then run regression methods with the transformed data.
This approach is pioneered by the Principle Component Regression (PCR) \cite{PCR} and the Partial Least Squares (PLS) \cite{Wold1975}.
\cite{Aswani2011} innovatively relates the regression coefficients to exterior derivatives.  
They propose to learn the manifold structure through local principal components and then constrain the regression to lie close to the manifold by solving a weighted least-squares problem with Ridge regularization.
\cite{Cheng2013} present the Manifold Adaptive Local Linear Estimator for the Regression (MALLER) that performs the local linear regression (LLR) on a tangent plane estimate. 
However, because those methods directly exploit the local manifold structures in an exact sense, they are not robust to variations in the covariates that perturb them away from the true manifold structure.

Many other manifold estimation approaches exist in the statistical literature.
\cite{Guhaniyogi2016} utilize random compression of the feature vector in combination with Gaussian process regression.
\cite{Yuchen2013} follows a divide-and-conquer approach that computes an independent kernel Ridge regression estimator for each randomly partitioned subset and then aggregates.
Other nonparametric regression approaches such as kernel machine learning \citep{Bernhard2002}, manifold regularization \citep{belkin06a}, and the spectral series approach \citep{Lee2016} also account for the manifold structure of the data.
More recently, \cite{Green2021} proposes the Principal Components Regression with Laplacian-Eigenmaps (PCR-LE) that projects data onto the eigenvectors output by Laplacian Eigenmaps and provides the rates of convergence of such nonparametric regression method over Sobolev spaces. 
However, those methods still suffer from the curse of dimensionality with large-dimensional covariates.

% \revise{Geometric data is also attracting attention from the machine learning field. A flourishing direction of research incorporates geometric information into deep neural models and is termed Geometric Deep Learning \cite{GeometricDL, Peter2018,  Bronstein2021}. Under this general blueprint, one approach encodes geometric information through graphs and performs learning tasks with the Graph Neural Networks (GNNs)  \cite{GRAND,Petar2018graph, Chamberlain2021, xu2018how, Bouritsas2021}. 
% In particular, \cite{Yue2019} dynamically builds neighborhood graphs from point clouds and aggregates edge features through layers for classification and segmentation tasks.
% \cite{DGM} learns the probabilistic latent graphs in the deep learning architecture for optimal classification.
% However, the learning tasks focused on by those works are classification and segmentation, while in this work we focus on regression.
% }

In addition to data with manifold-based covariates, manifold learning has been applied to other types of manifold-related data. 
\cite{Marzio2014} develop nonparametric smoothing for regression when both the predictor and the response variables are defined on a sphere.
\cite{Zhang2019} deal with the presence of grossly corrupted manifold-valued responses.
\cite{Lin2020} address data with functional predictors that reside on a finite-dimensional manifold with contamination. 
In this work, we focus on manifold-based covariates and may incorporate other types of manifold-related data in the future.
% \cite{Burnaev2021} a recent survey on manifold estimation

The main goal of this work is to estimate a scalar response with covariates lying around some manifold structures in a way that utilizes the geometric structure and bypasses the curse of dimensionality.
This is achieved by proposing a new framework that combines graphs and nonparametric regression techniques. 
Our framework follows the two-step idea: first, we learn a graph representation, which we call the \textit{skeleton}, of the manifold structure based on the methods from \citet{skelclus} and project the covariates onto the skeleton. Then we apply different nonparametric regression methods with the skeleton-projected covariates. We give brief descriptions of the relevant nonparametric regression methods below.

Kernel smoothing is a widely used technique that estimates the regression function as locally weighted averages with the kernel as the weighting function. Pioneered by the famous Nadarayaâ€“Watson estimator from \cite{Nadaraya1964} and \cite{Watson1964}, this technique has been widely used and extended by recent works \citep{Fan1992, Hastie1993, Fan1996, Kpotufe2017}. 
Splines \citep{ESL, Friedman1991} are popular nonparametric regression constructs that take the derivative-based measure of smoothness into account when fitting a regression function.
Moreover, k-Nearest-Neighbors (kNN) regression \citep{Altman1992, ESL} has a simple form based on a distance metric but is powerful and widely used in many applications. These techniques are incorporated into our proposed regression framework.

In recent years, many nonparametric regression techniques have been shown to adapt to the manifold structure of the data, with convergence rates that depend only on the intrinsic dimension of the data space. Specifically, the classical kNN and kernel regressor have been shown to be manifold-adaptive with proper parameter tuning procedures \citep{Kpotufe2009,  Kpotufe2011, Kpotufe2013, Kpotufe2017}, while recent methods like the Spectral Series regression and PCR-LE also enjoy this property \citep{Green2021}.
The proposed regression framework in this work also adapts to the manifold, as the nonparametric regression models fitted on a graph are dimension-independent. This framework has several additional advantages such as the ability to account for predictors from distinct manifolds and being robust to additive noise and noisy observations.

\begin{figure}
% \captionsetup{skip=1pt}
\centering
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Twomoon_data.jpeg} 
        \caption{Data}
    \end{subfigure}
        \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Twomoon_knots.jpeg}
        \caption{Knots}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Twomoon_skelvor.jpeg}
        \caption{Skeleton}
    \end{subfigure}\\
        \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Twomoon_skelkernel.jpeg} 
        \caption{S-Kernel Regression}
    \end{subfigure}
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Twomoon_lspline.jpeg} 
        \caption{Linear Spline}
    \end{subfigure}
\caption{Skeleton Regression illustrated by data with covariates having the shape of two moons in a 2D space.}
\label{fig::ex1}
\end{figure}

% \citet{Meier2008} considered high-dimensional generalized additive regression model and proposed a new penalty that combines sparsity and smoothness considerations. A computationally efficient algorithm has been provided with provable numerical convergence for finding the optimum of a penalized likelihood function.

% This contamination issue is not encountered in the situation studied by \cite{Cheng2013} and has been considered only for linear regression of multivariate data by \cite{Aswani2011} and \citet{Wainwright2012}
%  work on applications involving noisy and/or missing data and possible dependence in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing and/or dependent data \citet{Wainwright2012}.  \citet{Wainwright2012}  prove that a simple algorithm based on projected gradient descent will converge in polynomial time to a small neighborhood of the set of all global minimizers. They bound the statistical error, showing that it has the same scaling as the minimax rates for the classical cases of perfectly observed and independently sampled covariates.
% \citet{Loh2012} also considered high-dimensional linear regression and established minimax rates of recovering sparse coefficient vector under additive noise and missing data cases.


\emph{Outline.} 
We start by presenting the procedures of the skeleton regression framework in section \ref{sec::framework}.
In section \ref{sec::regression}, we apply nonparametric regression techniques to the constructed skeleton graph along with theoretical justifications.
% In section \ref{sec::skeletoncons}, we describe the construction of the skeleton. 
% In section \ref{sec::skelkernel}, we apply kernel regression with the geodesic distances on the skeleton.
% In section \ref{sec::lspline}, we fit linear spline on the skeleton structure. 
% We also discuss the feasibility of fitting higher-order splines and the accompanying issues with more details in Appendix \ref{sec::higherspline}.
In section \ref{sec::simulation}, we present some simulation results for skeleton regression and demonstrate the effectiveness of our method on real datasets in Section \ref{sec::real}. 
In section \ref{sec::conclusion}, we conclude the paper and point out some directions for future research.

% In section \ref{sec::higherspline} we discuss the feasibility of fitting higher-order splines on the skeleton. We also raise the issue that odd-degree derivatives are directional and hence are dependent on the definition of edge directions, a problem ignored in previous graph-based model literature.

%---------------------------------------------------------------------
\section{Skeleton Regression Framework} \label{sec::framework}

% {\color{magenta}YC: a concept we should mention is fixed-design--the covariates are non-random in our case. In this case, we do not need to worry about the randomness of the skeleton because the covariates are fixed.}

~~~~In this section, we introduce the skeleton regression framework. 
% random independent samples $(X_1, Y_1), \dots, (X_n, Y_n)$, with $X_j \in \calX \subseteq \RR^d$ being the covariates and $Y_j \in \RR$ being the response for $j = 1, \dots, n$, 
Given design vectors $\set{\bx_i}_{i=1}^n$ where $\bx_i \in \calX \subseteq \RR^d$ for each $i$ and the corresponding responses $\set{Y_i}_{i=1}^n$ in $\RR$, 
a traditional regression approach is to estimate the regression function $m(\bx) = \EE(Y|X = \bx)$.
However, the ambient dimension $d$ can be large while the covariates are distributed around some low-dimensional manifold structures. In this case, $\calX$ can be the union of several disjoint components with different manifold structures, and the regression function can have discontinuous changes from one component to another. 
To handle such geometrically structured data, we approach the regression task by first representing the sample covariate space with a graph, which we call the \textit{skeleton}, to summarize the manifold structures. We then focus on the regression function over the skeleton graph, which incorporates the covariate geometry in a dimension-independent way.

% In this work, we use the methods in \citet{skelclus} to construct the skeleton, while it can be constructed with other approaches and tuned with subject matter knowledge. 
We illustrate our regression framework on the simulated Two Moon data in Figure \ref{fig::ex1}. The covariates of the Two Moon data consist of two $2$-dimensional clumps with intrinsically 1-dimensional curve structure, and the regression response increases polynomially with the angle and the radius (Figure \ref{fig::ex1} (a)). 
We construct the skeleton presentation to summarize the geometric structure (Figure \ref{fig::ex1} (b,c) ) and project the covariates onto the skeleton.
The regression function on the skeleton is estimated using kernel smoothing (Section \ref{sec::skelkernel}, illustrated in Figure \ref{fig::ex1} (d) ) and linear spline (Section \ref{sec::lspline}, illustrated in Figure \ref{fig::ex1} (e)).
% and some additional nonparametric methods (Section \ref{sec::higherspline}).  
%We provide some consistency guarantees and computational tricks for those regression methods. 
The estimated regression function can be used to predict new projected covariates. We summarize the overall procedure in Algorithm \ref{alg::Skelreg}.

\begin{algorithm}
\caption{Skeleton Regression Framework}
\label{alg::Skelreg}

\begin{algorithmic}
\State \textbf{Input:} 
Observations $(\bx_1, Y_1), \dots, (\bx_n, Y_n)$.
\State 1. {\bf Skeleton Construction.} Construct a data-driven skeleton representation of the covariates preferably assisted with subject knowledge.
\State 2. {\bf Data Projection.} 
Project the covariates onto the skeleton.
\State 3. {\bf Skeleton Regression Function Estimation.}
Fitting regression function on the skeleton using nonparametric techniques such as kernel smoothing (Section \ref{sec::skelkernel}), k-Nearest Neighbor (Section \ref{sec::SkNN}), and linear spline (Section \ref{sec::lspline}). 
% or additional methods (Section \ref{sec::higherspline})
\State 4. {\bf Prediction.}
Project new covariates onto the skeleton and use the estimated regression function for prediction.
%\State \textbf{Output:} 
\end{algorithmic}

\end{algorithm}
 
 





\subsection{Skeleton Construction}
\label{sec::skeletoncons}

%\subsection{Knots and Edges}
%\label{sec::knotsedges}


~~~~A skeleton is a graph constructed from the sample space representing regions of interest. 
From a statistical perspective, a region is of interest if it encompasses a sufficient measure of probability distribution.
For given covariate space $\calX \subseteq \RR^d$,
let $\calV = \{V_j \in \RR^d : j =1 , \dots k\}$ 
be a collection of points of interest
and $E$ be a set of edges connecting points in $\calV$
such that an edge $e_{j\ell}\in E$ 
if the region between $V_j$ and $V_\ell$ is also of interest. 
The tuple $({\cal V}, E)$ together forms 
a graph that represents the focused regions in the sample space.
Notably, different from common graph-based regression approaches that take each sample covariate as a vertex, the set $\calV$ takes representative points of the covariate space and has size $k \ll n$ where $n$ is the sample size.
Moreover, the points on the edges are also part of the analysis as belonging to the regions of interest, which is different from the usual knot-edge graph.
While the graph $({\cal V}, E)$ contains the region
of interest, 
it is not easy to work with this graph directly. 
Thus, we introduce the concept of the skeleton induced by this graph.

Let $\calE = \{tV_j  + (1-t) V_\ell: t \in (0,1),  e_{j\ell} \in E \}$
be the collection of line segments induced by the edge set $E$. 
We define the skeleton of $({\cal V}, E)$
as $\calS = \calV \cup \calE $, i.e.,
$\calS$ is the points of interest and the associated line segments
representing the regions of interest. 
Clearly, $\calS$ is a collection of one-dimensional line segments
and zero-dimensional points so it is independent of the ambient dimension $d$, but the physical location of $\calS$ is meaningful as representing the region of interest. 
The idea of skeleton regression is to build a regression model
on the skeleton $\calS$. 



%~~~~For given covariate space $\calX \subseteq \RR^d$, we construct a skeleton graph  with $k$ knots $\calV = \{V_j \in \RR^d : j =1 , \dots k\}$ 
%and the set of connected edges $\calE = \{tV_j  + (1-t) V_\ell: t \in (0,1),  V_j \text{ connected to } V_\ell \}$. 
%For the parametrization, we have $t \in (0,1)$ to exclude the knots. Notably, the knots and edges in our framework, different from the usual graphs, have physical locations in the ambient space.
%We denote the skeleton as $\calS = \calV \cup \calE $. 

\subsubsection{A data-driven approach to construct skeleton}

~~~~The skeleton should ideally be constructed based on the analyst's judgment or prior knowledge of the focus regions. 
However, this information may be unavailable and we have to construct a skeleton from the data.
In this section, we give a brief description of a data-driven approach proposed in \cite{skelclus} that constructs the skeleton to represent high-density regions. 
%The skeleton graph is constructed to give an approximate representation of the data structure and many existing prototype-based methods can be used for this purpose. 
The method constructs knots as the centers from the $k$-means clustering with a large number of centers \footnote{By default $[\sqrt{n}]$. We explore the effect of choosing different numbers of knots with empirical results.}.
% Given the knots, the edges are constructed by approximate Delaunay triangulation. 
% The exact Delaunay triangulation \cite{Delaunay1934} has an edge between two knots if their corresponding Voronoi cells share a common boundary \cite{voronoi1908recherches}. 
% The Voronoi cell $\CC_{j}$ associated with a knot $c_{j}$ is the set of points in $\calX$ whose distance to $c_{j}$ is the smallest compared to other knots that
% \begin{align}
%     \CC_j = \{x \in \calX: \norm{x, c_j} \leq d(x, c_\ell) \ \  \forall l \neq j\},
% \end{align}
% where $||.||$ denotes the Euclidean norm.
% Exact Delaunay triangulation can be computationally heavy, and as in \cite{skelclus}, for approximation, 
% Then an edge between two knots $V_i, V_j$ is connected if there is at least one covariate point whose two nearest neighbors are $V_i, V_j$. 
The edges are connected by examining the sample 2-Nearest-Neighbor (2-NN) region of a pair of knots $(V_j,V_\ell)$  (see Figure \ref{fig::2nn}) defined as
\begin{align}
    B_{j\ell} = \{ X_m, m=1,\dots,n :\norm{x- V_i} > \max\{ \norm{x-V_j}, \norm{x- V_\ell} \}, \forall i \neq j, \ell \},
\label{eq::2NNregion}
\end{align}
where $||.||$ denotes the Euclidean norm, and an edge between $V_j$ and $V_\ell$ is added if $B_{j\ell}$ is non-empty. 
The method can further prune edges or segment the skeleton by using hierarchical clustering with respect to the Voronoi Density weights defined as
$S_{j\ell}^{VD} = \frac{\frac{1}{n}\abs{B_{j\ell}}}{\norm{V_j - V_\ell}}.$
We provide more details about this approach in Appendix \ref{ref::skelconsVoron}.

% \YC{We should provide a short summary of the Voronoi density skeleton construction in the appendix.}


\begin{figure}
% \captionsetup{skip=1pt}
\centering
\includegraphics[height=5cm]{figures/2nnRegion_shaded.jpeg}
\caption{Orange shaded area illustrates the 2-NN region between knots $1$ and $2$.}
\label{fig::2nn}
\end{figure}


\begin{remark}
The idea of using the $k$-means algorithm to divide data into cells
and perform analysis based on the cells has been proposed
in the literature for fast computation.
%for fast computation has been applied in many machine learning realms. 
\cite{InvertedFiles}, when carrying out an approximate nearest neighbor search, proposed to divide the data into Voronoi cells by $k$-means and do a neighbor search only in the same or some nearby cells. \cite{InvertedMultiIndex} adopted the Product Quantization technique to construct cell centers for high-dimensional data as the Cartesian product of centers from sub-dimensions. 
% $k$-means algorithm can be slow for large-scale data, but \cite{johnson2019billion} has implemented the $k$-means algorithm efficiently on the GPU base, which dramatically improves the calculation speed of the algorithm. 
\end{remark}


% The Edge weights are measured by Voronoi density proposed in \citet{skelclus}.
% Connected with the approximate triangulation above, the \emph{Voronoi density (VD)} measures the edge weight of  $(c_j,c_\ell)$ with
% \begin{align}
%     S_{j\ell}^{VD} = \frac{\PP(A_{j\ell})}{\|c_j - c_\ell \|}.
% \end{align}
% Namely, we divide the probability of overlapping regions by the mutual Euclidean distance.
% The division of the distance adjusts for the fact that 2-NN regions have different sizes and provide more weights to edges between knots that are close to each other. 

% In practice we estimate $ S_{j\ell}^{VD}$ by a sample average. 
% Specifically, the numerator $\PP(A_{j\ell}) $ is estimated by $\hat{P}_n (A_{j\ell}) = \frac{1}{n}\sum_{i=1}^n I(X_i\in A_{j\ell})$
% and the  estimator for the VD is
% \begin{align}
%     \hat{S}_{j\ell}^{VD} &= \frac{\hat{P}_n ({A}_{j\ell})}{\|{c}_j - {c}_\ell\|}.
% \end{align}

\subsection{Skeleton-Based Distance}

~~~~One of the advantages of the physically located skeleton is that it allows for a natural definition of the skeleton-based distance function $d_\calS(.,.): \calS \times \calS \to \RR^{+}\cup \{\infty\}$. 
Let $\bs_j, \bs_\ell \in \calS$ be two arbitrary points on the skeleton and note that, different from the usual geodesic distance on a graph, in our framework $\bs_j, \bs_\ell$ can be on the edges. We measure the skeleton-based distance between two skeleton points as the graph path length as defined below:

\begin{itemize}
    \item If $\bs_j, \bs_\ell$ are disconnected that they belong to two disjoint components of $\calS$, we define
    \begin{align}
    d(\bs_j, \bs_\ell) = \infty
    \label{eq::graphdist}
    \end{align} 
    \item If $\bs_j$ and $\bs_\ell$ are on the same edge, we define the skeleton distance as their Euclidean distance that
\begin{align}
    d_\calS(\bs_j, \bs_\ell) = ||\bs_j - \bs_\ell|| 
    \label{eq::graphdist0}
\end{align}
    \item For $\bs_j$ and $\bs_\ell$ on two different edges that share a knot $V_0$, the skeleton distance is defined as
    \begin{align}
    d_\calS(\bs_j, \bs_\ell) = ||\bs_j - V_{0}|| + ||\bs_\ell - V_{0}||
    \label{eq::graphdist1}
\end{align}
    \item Otherwise, let knots $V_{i(1)}, \dots, V_{i(m)}$ be the vertices on a path connecting $\bs_j, \bs_\ell$, where $V_{i(1)}$ is one of the two closest knots of $\bs_j$ and $V_{i(m)}$ is the other closest knots of $\bs_\ell$. We add the edge lengths of the in-between knots to the distance that 
\begin{align}
\begin{split}
    d_\calS(\bs_j, \bs_\ell) &= ||\bs_j - V_{i(1)}|| + ||\bs_\ell - V_{i(m)}|| + \sum_{p=1}^{m-1}\norm{V_{i(p)}, V_{ i(p+1)}}
\end{split}
\label{eq::graphdist2}
\end{align}
and we use the shortest path length if there are multiple paths connecting $\bs_j$ and $ \bs_\ell$.
\end{itemize}

An example illustrating the skeleton-based distance is shown in Figure \ref{fig::skeldist}.
Like the shortest path (geodesic) distance that makes a usual knot-edge graph into a metric space, the skeleton-based distance is also a metric on the skeleton graph. 
In the following sections, we will discuss methods to perform regression on space only with the defined metric.


\begin{figure}[ht]
% \captionsetup{skip=1pt}
\centering
\includegraphics[height=4cm]{figures/skelregdistraw.png}
\caption{Illustration of skeleton-based distance. Let $C_1, C_2, C_3, C_4$ be the knots, and let $S_2,S_3,S_4$ be the mid-point on the edges $E_{12},E_{23},E_{34}$ respectively. Let $S_1$ bet the midpoint between $C_1$ and $S_2$ on the edge. Let $d_{ij} = \norm{C_i - C_j}$ denotes the length of the edge $E_{ij}$. $d_\calS(S_1,S_2) = \frac{1}{4} d_{12}$ illustrated by the blue path.  $d_\calS(S_2,S_3) = \frac{1}{2} d_{12} + \frac{1}{2} d_{23}$ illustrated by the green path. $d_\calS(S_2,S_4) = \frac{1}{2} d_{12} + d_{23} + \frac{1}{2} d_{34}$ illustrated by the orange path.}
\label{fig::skeldist}
\end{figure}



%
% Note that in skeleton regression framework $w_{j\ell} =\norm{c_j - c_\ell}$, but the defined skeleton distance can be applied to general edge weights where partial weights are meaningful.

\begin{remark}
We may view the skeleton-based distance as an approximation of the geodesic distance on the underlying data manifold. Moreover, to make a stronger connection to the manifold structure, it is possible to define edge lengths through local manifold learning techniques that have better approximations to the local manifold structure. However, using more complex local edge weights can pose additional challenges for the data projection step described in the next section and we leave this as a future direction.
%but the theoretical justification of this connection is hard due to the randomness of how such knots are chosen.
\end{remark}


\subsection{Data Projection}
\label{sec:dataProjection}

~~~~For the next step, we project the sample covariates onto the constructed skeleton.
For given covariate $\bx$, let $I_1(\bx), I_2(\bx) \in \{1,\dots,k\}$ be the index of its closest and second closest knots in terms of the Euclidean metric. 
We define the projection function $\Pi(.): \calX \to \calS$ for $\bx \in \calS$ as (illustrated in Figure \ref{fig::skelproject}):
\begin{itemize}
    \item[Case I: ] If $V_{I_1(\bx)}$ and $V_{I_2(\bx)}$ are not connected, $\bx$ is projected onto the closest knot that
    $\Pi(\bx) = V_{I_1(\bx)}$ 
    \item[Case II: ] If $V_{I_1(\bx)}$ and $V_{I_2(\bx)}$ are connected, $\bx$ is projected with the Euclidean metric onto the line passing through $V_{I_1(\bx)}$ and $V_{I_2(\bx)}$ that, let $t = \frac{\left(\bx - V_{I_1(\bx)}\right)^T\cdot \left(V_{I_2(\bx)}-V_{I_1(\bx)}\right)}{\norm{V_{I_2(\bx)}-V_{I_1(\bx)}}^2}$ be the projection proportion,
    \begin{align}
    \Pi(\bx)  = V_{I_1(\bx)} + \left(V_{I_2(\bx)}-V_{I_1(\bx)}\right) \cdot 
    \begin{cases}
    0, \text{ if } t <0\\
    1, \text{ if } t>1\\
    t, \text{ otherwise}
    \end{cases}
    \end{align}
    where we constrain the covariates to be projected onto the closest edge.
\end{itemize}


\begin{figure}[ht]
% \captionsetup{skip=1pt}
\centering
\includegraphics[height=3cm]{figures/skelregproject.png}
\caption{Illustration of projection to the skeleton. The skeleton structure is given by the black dots and lines. Data point $X_1$ is projected to $S_1$ on the edge between $C_1$ and $C_2$. Data point $X_2$ is projected to knot $C_2$.}
\label{fig::skelproject}
\end{figure}


Note that with the projection defined above, a non-trivial volume of points can be projected onto the knots of the skeleton graph as belonging to Case I or due to the truncation in Case II. 
This adds complexities to the theoretical analysis of the proposed regression framework and leads to our separate analysis of the different domains of the graph in Section \ref{sec:kernelConsistent}.



\section{Skeleton Nonparametric Regression}
\label{sec::regression}

% {\color{magenta}I reversed the logic here to make it smooth.
% Also, there are a lot of notation inconsistencies. I tried
% to clarify them but please double-check.}
~~~~Covariates are mapped onto the skeleton after the data projection step and are equipped with the skeleton-based distances. 
In this section, we apply nonparametric
regression techniques to the skeleton graph with projected data points. 
%Note that we only have the metric defined on the skeleton graph but there is no nice structures like vectors or inner products as in the Euclidean space or the Hilbert spaces, which imposes constraints on applicable regression methods.
We study three feasible nonparametric approaches: the skeleton-based kernel regression (S-Kernel), the skeleton-based k-nearest-neighbor method (S-kNN), and the linear spline on the skeleton (S-Lspline).
At the end of this section, we discuss the challenges of applying some other nonparametric regression methods in the setting of skeleton graphs.


\subsection{Skeleton Kernel Regression}
\label{sec::skelkernel}
~~~~
We start by adopting kernel smoothing to the skeleton graph. 
Let $\bs_1,\cdots, \bs_n$ be the projections on the skeleton
from $\bx_1,\cdots, \bx_n$, i.e., $\bs_i = \Pi(\bx_i)$. 
With the skeleton-based distances, the skeleton kernel regression makes a prediction at the location
$\bs \in \calS$ as
\begin{align}
    \hat{m}(\bs) = \frac{\sum_{i=1}^N K(d_\calS(\bs_i, \bs)/h) Y_i}{\sum_{j=1}^N K(d_\calS(\bs_j, \bs)/h)},
\end{align}
where $K(\cdot) \geq 0$ is a smoothing kernel
such as the Gaussian kernel and $h>0$
is the smoothing bandwidth that controls the amount of smoothing.
In practice, we choose $h$ by cross-validation.
Essentially, the estimator $\hat{m}(\bs)$ is the kernel regression applied to a general metric space (skeleton) rather than the usual Euclidean space.
Notably, the kernel function calculation only depends on the skeleton distances and hence is independent of neither the ambient dimension of the original input nor the intrinsic dimension of the manifold structure.
% {\color{magenta}YC: we often define $K$ rather than $K_h$
% so that the kernel function stays invariant--as your assumptions K is written. Please change notations accordingly.}

It should be noted that $\hat{m}(\bs)$ only makes predictions on the skeleton $ \calS$. 
If we are interested in predicting the outcome 
at any arbitrary point $\bx\in\calX$, 
the prediction will be based on the projected point, i.e.,
$
\hat m(\bx) = \hat m\left(\Pi(\bx)\right), 
$
where $\Pi(\bx) \in \calS.$
Because of the above projection property, one can think of the skeleton kernel regression as an estimator
to the following skeleton-projected regression function
\begin{align}
    m_\calS(\bs) = \EE(\bY|\Pi(\bX) = \bs), \bs \in \calS.
\end{align}
We study the convergence of $\hat{m}(\bs)$ to $m_\calS(\bs)$ in what follows.

\begin{remark}
    Admittedly, the projection of the covariates onto the skeleton as described in Section \ref{sec:dataProjection} introduces the projection error between the true regression function $m(\bx) = \EE(Y|X = \bx)$ and the skeleton-projected regression function. 
    Bounding this projection error involves not only a precise characterization of the underlying manifolds and the data distribution around them but also the physical locations of the skeleton relative to the local manifold structure. 
    Due to such complexity, a theoretical result bounding the projection error under some general conditions requires careful formulation (despite that results are straightforward for particular cases such as having covariates exactly on a 1D circular segment).
    We leave the in-depth analysis of the projection as future work and focus on generalizing the nonparametric regression methods to the skeleton graph in this work.
\end{remark}



%In this section, we apply kernel smoothing to the skeleton-projected covariates based on the skeleton-based distances. 
%Instead of estimating the regression function defined on $\calX$, we estimate the \textit{projected} regression function 
%\begin{align}
%    m_\calS(\bs) = \EE(y|\pi(\bx) = \bs), \bs \in \calS 
%\end{align} 
%on the skeleton domain $\calS$.  
%Let $K_h(.) = K(./h)$ be a non-negative kernel function with bandwidth $h > 0$ and let $\bs_1, \dots, \bs_n$ denote the skeleton-projected covariates that $\bs_i = \Pi(\bx_i)$ for $i = 1,\dots,n$, the corresponding skeleton-based kernel (S-kernel) regressor for a point $\bs \in \calS$ is
%\begin{align}
%    \hat{m}(\bs) = \frac{\sum_{j=1}^N K_h(d_\calS(\bs_j, \bs)) Y_j}{\sum_{j=1}^N K_h(d_\calS(\bs_j, \bs))} 
%\end{align}
%An example kernel function  is the Gaussian kernel that
%\begin{align}
%    K_h(d_\calS(\bs_j, \bs_\ell)) = \exp\bigg(- \frac{d_\calS(\bs_j, \bs_\ell)^2}{h^2}\bigg)
%\end{align}



% In practice we use skeleton-based truncated Gaussian kernel. Specifically, let $V_p,V_q$ be the two closest knots of the point $\bx$. If $V_p$ and $V_q$ are connected in the skeleton, then we constrain the Gaussian kernel to the Two-Nearest-Neighbor region $A_{pq}$ of $V_p,V_q$ (see Equation \ref{eq::2NNregion}) that
% \begin{align*}
%     K_h(\Pi(\bx), \bs) = 
%         \begin{cases}
%       \exp\bigg(- \frac{d_\calS(\Pi(\bx_j), \bs)^2}{h^2}\bigg) & \text{if } \bs \in A_{pg}\\
%       0 & \text{otherwise}
%     \end{cases} 
% \end{align*}
%Notably, the kernel function calculation only depends on the skeleton distances and hence is independent of neither the ambient dimension of the original input nor the intrinsic dimension of the manifold structure.
%The smoothing bandwidth $h$ can be chosen by cross-validation. 


\subsubsection{Consistency of S-Kernel Regression}
\label{sec:kernelConsistent}
% \subsubsection{Continuous Edge \& Discrete Knots Formulation}

% {\color{magenta}YC: one important thing about the theoretical results
% is the discussion. We need a discussion
% on every theoretical result to interpret it or discuss its
% property and how it is similar to or different from other work.}


%~~~~In this section, we present the convergence result for the S-kernel regressor.
~~~~
Our analysis assumes that the skeleton is fixed and given and focuses on the estimation of the regression function.
To evaluate the estimation error, we must first impose some concepts of distribution on the skeleton. 
However, due to the covariate projection procedure, the probability measures on the knots and edges are different, and we analyze them separately. On an edge, the domain of the projected regression function varies in one dimension, resulting in a standard univariate problem for estimation.
% We start with the formulation of the regression problem. For given covariate space $\calX \subseteq \RR^d$, the skeleton graph has $\calV = \{V_j \in \RR^d : j =1 , \dots k\}$ the set of $k$ knots and
% $\calE = \{tV_j  + (1-t) V_\ell: t \in (0,1),  w_{j\ell} > 0\}$ the set of connected edges with nonzero weights $w_{j\ell}$. Note that for the parametrization we have $t \in (0,1)$ to exclude the knots.
% We denote the skeleton as $\calS = \calV \cup \calE $. 
%can connect to multiple edges and the degrees of the knots vary, making the domain at the knots not suited under a 1-dimensional characterization. Therefore, we formulate the regression problem differently for edges and knots.}
%In particular, we impose the one-dimensional Lebesgue measure with respect to the parametrization $t$ as in the definition of $\calE$ in Section \ref{sec::skeletoncons}, and the true projected regression model for a point $\bs$ on edge $(V_j, V_\ell)$ is 
%\begin{align*}
%    m_\calS(\bs) = m_{j\ell}(\bs) = m_{j\ell}(t)
%\end{align*}
%where $t$ is the parametrization for $\bs$.
% Further, we assume the density function $g(t)$ and the projected regression function $m_\calS(t)$ are Lipschitz continuous on the edges. 
For the case of knots, a nontrivial region of the covariate space can be projected onto a knot, leading to a nontrivial probability mass at the knot. 

%Hence, we assign discrete counting measure on each knot,
%and the true projected regression model at $\bs \in \calV$ is a constant function that
%\begin{align*}
%    m_\calS(\bs) = M_j, \bs = V_j.
%\end{align*}
%\revise{Note that uniform convergence cannot be established under this formulation because of the probability mass on the knots.}


For simplicity, we write
$ K_h(\bs_j, \bs_\ell) \equiv  K(d_\calS(\bs_j, \bs_\ell)/h)$
for $\bs_j, \bs_\ell \in \calS$.
Let $\calB(\bs, h) = \set{\bs' \in \calS: d_\calS(\bs', \bs) < h} $ be the ball on skeleton centered at the point $\bs \in \calS$ with radius $h$.
% {\color{magenta}YC: The support
% is restricted to the skeleton? And the distance should be replaced by skeleton distance I think.}
% Let $\mu$ be the one-dimensional Lebesgue measure.
% and note that, as $h \to 0$, $\mu(\calB(\bs, h)) \to 0$ for every $\bs \in \calS$. 
We can decompose the kernel regression estimator into edge parts and knot parts as
\begin{align} \label{eq::kerneldecomp}
\begin{split}
    &\hat{m}(\bs) = \frac{\sum_{j=1}^n Y_j  K_h(\bs_j, \bs) }{\sum_{j=1}^n K_h(\bs_j, \bs)} \\
    &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calE)  + \frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calV )}{\frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs)I(\bs_j \in \calE ) + \frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs) I(\bs_j \in \calV ) } \\
    &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j  I(\bs_j =\bs)}{\frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n  I(\bs_j =\bs) }
\end{split}
\end{align}
In the last line, we emphasize that the knots and edges in the kernel estimator have a meaningful contribution only within the support of the kernel function. We inspect the different domain cases separately in the following sections.

% {\color{magenta}YC: this part requires a lot of revision. 
% We have not yet defined the density $g(\bs)$.
% The ball $B(\bs, h)$ is also not well-defined since
% we are restricting it to the skeleton.
% Also, the assumption (K) is for the kernel function,
% not for the point of interest.

% I think the density on the skeleton edge is:
% $g(\bs) = \lim_{r\rightarrow 0 }\frac{P(\Pi_\calS(X)\in \calB(\bs , r))}{2r}$.
% This is known as the 1-Hausdorff density.

% Note that $g(\bs) = \infty$ if $\bs$ is at a knot point
% that has a probability mass.

% }

% \YC{Need to be very careful when using $\bS$ vs $\bs$. Similarly for $\bX$ vs $\bx$.}

For the model and assumptions, we let $Y_j =  m_\calS(\bS_j)+U_j, \bS_j \in \calS$, and $\EE(U_j|\bS_j) = 0$ almost surely. 
Let $\sig^2(\bs) = \EE(U_j^2 | \bS_j = \bs)$. 
Let the density on the skeleton edge be defined as the 1-Hausdorff density that
$g(\bs) = \lim_{r\downarrow 0 }\frac{P(\bS\in \calB(\bs, r))}{2r}$.
Note that $g(\bs) = \infty$ if $\bs$ is at a knot point
that has a probability mass.
We consider the following assumptions: 
\begin{itemize}
    \item[\textbf{A1}] $\sig^{2}(\bs)$ is continuous and  uniformly bounded.
    \item[\textbf{A2}] The skeleton edge density function $g(\bs) > 0$ and are bounded and Lipschitz continuous for $\bs \in \calE$.
    \item[\textbf{A3}] $m_\calS(\bs) g(\bs)$ is bounded and Lipschitz continuous for $\bs \in \calE$.
    \item[\textbf{K}] The kernel function has compact support and satisfies $\int K(x) dx = 1$, $\int K^2(x) dx < \infty$, $\int xK(x) d x = 0$,  and $\int x^2 K(x) dx < \infty$ 
    % {\color{magenta}YC: I don't think you need so many additional assumptions. This is way beyond the usual kernel assumption. You should just need compact support, symmetric, integrated into 1, and bounded 2nd moment.}
\end{itemize}
Conditions A1 and K are general assumptions that are commonly made in kernel regression analysis.
A2 and A3 are mild conditions that can be sufficiently implied by the boundedness and Lipschitz continuity of the density and regression function in the ambient space along with non-overlapping knots that the area of the orthogonal complements have Lipschitz changes.
We do not assume the second-order smoothness commonly required for kernel regression because requiring higher-order derivative smoothness would necessitate specifying directions on the graph, which may present difficulties in model formulation. 
We include further discussions on formulating the derivatives on the skeleton in Section \ref{sec::otherParametric}.
% (see Section \ref{sec::higherspline} and Appendix \ref{sec::directions} for discussions and examples).

% {\color{magenta}YC: I think the notation $\calB(\bs, h)$ might be misleading. What you meant I guess is:
% $$
% I(\bs_j \in \calV \cap  \calB(\bs, h)) = I(\bs_j \in \calV: d_\calS(\bs_j, \bs)\leq h).
% $$
% Please clarify this.}



\subsubsection{Convergence of the Edge Point}

~~~~We first look at an edge point $\bs \in E_{j\ell} \in \calE$. 
In this case, as $n\to \infty, h\to 0$, for sufficiently large $n$, we have $\calB(\bs, h) \subset E_{j\ell}$, and the skeleton distance is the $1$-dimensional Euclidean distance for any point within the support. Therefore, we have a convergence rate similar to the $1$-dimensional kernel regression estimator \citep{Bierens1983, wasserman2006all, Chen2017}.


\begin{thm}[Consistency on Edge Points]
Let $\bs \in \calE$ be a point on the edge. Assume conditions (A1-3) hold for all points in $\calE \cap  \calB(\bs, h)$ and (K) for the kernel function. When $n \to \infty$, $h\rightarrow0$, $nh\rightarrow\infty$, we have
\begin{align}
    \abs{\hat{m}_n(\bs) - m_\calS(\bs)} = O(h) + O_p\bigg(\sqrt{\frac{1}{n h}}\bigg) 
\end{align}
\label{thm::edge}
\end{thm}

% Note that, if condition A3 is replaced with the usual second-order derivative smoothness, we have the bias to be of rate $O(h^2)$, but with weaker Lipschitz continuity condition in A3 we have the bias to have rate $O(h)$. 
We leave the proof in Appendix \ref{sec::contproof}.
Theorem \ref{thm::edge} gives the convergence rate for a point on the edge of the constructed skeleton. 
%Note that, 
The convergence rate at the bias is $O(h)$, which is
the usual rate when we only have Lipschitz smoothness (A2) of $m_{\calS}$.
One may be wondering if we can obtain a faster rate such as $O(h^2)$
if we assume higher-order smoothness of $m_{\calS}$. 
While it is possible to obtain a faster rate if we have a higher-order smoothness, we note that this assumption will not be reasonable on the skeleton
because $m_{\bS}(\bs) = \EE(Y|\Pi(X) = \bs)$ is defined via projection.
The region being projected onto $\bs$ is continuously changing
and may not be differentiable due to the boundary of Voronoi cells.
Therefore, the Lipschitz continuity (A2) is reasonable while
higher-order smoothness is not.
%as we assume a Lipschitz continuity condition in A3 rather than the derivative smoothness used in classical kernel smoothing analysis, the bias term in Theorem \ref{thm::edge} has rate $O(h)$ rather than the usual $O\left(h^2\right)$ rate.




\subsubsection{Convergence of the Knots with Nonzero Mass}
~~~~We then look at the knots with nonzero probability mass that $\bs \in \calV$ with $p(\bs) > 0$, where we use $p(\bs)$ to denote the probability mass on a knot. 
This case mainly occurs for knots with degree $1$ on the skeleton graph, when a non-trivial region of points is projected onto such knots. For example, refer to knot C2 in Figure \ref{fig::skelproject}. 
% The consistency result is presented in Theorem \ref{thm::knotconsistency}.

\begin{thm}[Consistency on Knots with Nonzero Mass]
Let $\bs \in \calV$ be a point at a knot and the probability mass at $\bs$ be $P(\Pi_\calS(X)=\bs) \equiv p(\bs) > 0$ and assume $\sig^2(\bs)$ bounded. Also, assume conditions (A1-3) hold for all points in $\calE \cap  \calB(\bs, h)$ and (K) for the kernel function. When $n\to \infty$, $h\rightarrow0$, and $nh\rightarrow\infty$,
we have
\begin{align}
    \abs{\hat{m}(\bs) - m_\calS(\bs)} = O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right)
\end{align}
\label{thm::knotconsistency}
\end{thm}
Theorem \ref{thm::knotconsistency} gives the convergence result for a knot point with a nontrivial mass of the skeleton. 
The bias term $O(h)$ comes from the influence of nearby edge points.
For the stochastic variation part, instead of having the  $O_p\left(\sqrt{\frac{1}{n h}}\right)$ rate as the usual kernel regression and in Theorem \ref{thm::edge}, we have $O_p\left(\sqrt{\frac{1}{n}}\right)$ rate which comes from averaging the observations projected onto the knots. The proof of Theorem \ref{thm::knotconsistency} is provided in Appendix \ref{sec::knotproof}.


\subsubsection{Convergence of the Knots with Zero Mass}
~~~~We now look at a knot point $\bs \in \calV$ with no probability mass that $p(\bs) = 0$. 
This can be the case for a knot with a degree larger than $1$ like knot C3 in Figure \ref{fig::skelproject}. Since we define edge sets excluding the knots, there will be no density as well as no probability mass at $\bs$. Note that, with some reformulation, degree $2$ knots can be parametrized together with the two connected edges and, under the appropriate assumptions, Theorem \ref{thm::edge} applies, giving consistency estimation with $O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right) $ rate. However, density cannot be extended directly to knots with a degree larger than $2$, but the kernel estimator still converges to some limits as presented in the Proposition below.
% However, we can make sense of this case by thinking of this point as part of the connecting edges and therefore impose some density $g(\bs) > 0$ and some regression value $m(\bs)$ on the knot  $V_\ell$. If the limiting density values and regression values from all the connected edges agree, then $g(\bs)$ and $m(\bs)$ are well-defined, and the conditions A1-4 are satisfied given those conditions are satisfied on all the connected edges. Consequently the same convergence analysis as in Theorem \ref{thm::edge} applies, giving $O(h) + O_p\big(\sqrt{\frac{1}{n h}}\big) $ rate.
\begin{prop}
\label{prop::zeroknot}
Let $\bs \in \calV$ be a point at a knot
such that the probability mass at $\bs$ be $P(\Pi_\calS(X)=\bs) \equiv p(\bs) = 0$. 
Assume conditions (A1-3) hold for all points in $\calE \cap  \calB(\bs, h)$ and (K) for the kernel function.
%For $\bs \in \calV$ a knot point, assume conditions A1-3 hold for all points in $\calE \cap  \calB(\bs, h)$ and let the probability mass at $\bs$ be $p(\bs) = 0$. 
%We assume condition K for the kernel function.
Let $\calI$ collect the indexes of edges with one knot being $\bs$.
For $\ell \in \calI$ and edge $E_\ell$ connects $\bs$ and $V_\ell$,
let $g_\ell(t) = g((1-t)\bs + t V_\ell)$ and $g_\ell(0) = \lim_{x\downarrow 0} g_{\ell}(x)$.
Let  $m_\ell(t) = m_\calS( (1-t)\bs + t V_\ell)$ and $m_\ell(0) = \lim_{t \downarrow 0} m_\ell(t)  $. 
When $n\to \infty$, $h\rightarrow0$, and $nh\rightarrow\infty$, we have
% Denote $\sigma_\ell^2(t) = \EE(|U_j|^2 | \bs_j = (1-t) \bs + t V_\ell )$ and $\sigma_\ell^2(0) = \lim_{t \downarrow 0} \sigma_\ell^2(t) $.
\begin{align}
    \hat{m}(\bs) 
    &= \frac{ \sum_{\ell \in \calI}   m_\ell(0) g_\ell(0)  }{ \sum_{\ell \in \calI}   g_\ell(0) } + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right).
\end{align}
%{\color{magenta}YC: the point $0$ should be replaced by
%the location of knot $\mathbf{s}$, right?}
\end{prop}

Proposition \ref{prop::zeroknot} shows that, under proper conditions, the skeleton kernel estimator on a zero-mass knot converges to the weighted average of the limiting regression values of the connected edges, and the convergence rate is the same as the edge points shown in Theorem \ref{thm::edge}.
The proof is included in Appendix \ref{sec::zeroknotproof}.


\begin{remark}
The domain $\calS$ of the regression function can be seen as bounded, and hence the boundary bias issue can arise. 
The true manifold structure's boundary can be different from the boundary of the skeleton graph, making the consideration of the boundary more complicated.
However, the boundary of the skeleton is the set of degree $1$ knots, and, under our formulation, knots have discrete measures, so the consideration of boundary bias may not be necessary for the proposed formulation.
However, some boundary corrections can potentially improve the empirical performance and we leave it for future research.
\end{remark}




%\YC{This will not reduce to the variance in Euclidean space... 
%If we have a metric $d$, the variance should be 
%$$
%\hat \sigma^2 = \frac{2}{n (n-1)}\sum_{i\neq j} d^2_{\calS}(\bs_i, \bs_j).$$
%Suppose the skeleton can be decomposed into $\calS_1,\cdots, \cal_M$, $M$ connected components. 
%Then for each $m$, we denote $I_m$ be the indices
%of observations projected to $\calS_m$ and $n_m = |I_m|$
%be the number of such observations.
%Then we can define the 
%variance of component $\calS_m$ as 
%$$
%\hat \sigma^2_m = \frac{2}{n_m (n_m-1)}\sum_{i\neq j\in I_m} d^2_{\calS}(\bs_i, \bs_j).
%$$
%}



\subsection{Skeleton kNN regression}
\label{sec::SkNN}

% {\color{magenta}Add a description on skeleton kNN regression.}
~~~~
The $k$-Nearest Neighbor (kNN) method
can be easily applied to the skeleton
using the distance on the skeleton.
For a given point on the skeleton at $\bs \in \calS$, 
we define the distance to the k-th nearest observation
on the skeleton as 
\begin{align}
    R_k(\bs) = \min\left\{r>0: \sum_{i=1}^n I(d_\calS(\bs_i, \bs)\leq r)\geq k\right\}.
\end{align}
Note that it is possible to have multiple
observations being the $k$-th nearest observation
due to observations being projected to the vertices.
In this case, we can either randomly choose
from them or consider all of them.
Here we include all of them in the calculation.
The skeleton-based $k$NN regression (S-kNN) 
predicts the value of outcome at $\bs$ as
\begin{align}
    \hat{m}_{SkNN}(\bs) = \frac{\sum_{i=1}^k Y_{i} I(d_\calS(\bs_i, \bs)\leq R_k(\bs))}{\sum_{j=1}^k I(d_\calS(\bs_j, \bs)\leq R_k(\bs))}.
\end{align}

Different from the usual kNN regressor with the covariates $\bx_1, \dots, \bx_n$, which selects neighbors through Euclidean distance in the ambient space, the S-kNN regressor chooses neighbors with skeleton-based distances after projection onto the skeleton graph.
Measuring proximity with the skeleton can improve the regression performance when the dimension of the covariates is large, which we empirically show in Section \ref{sec::simulation}.



%let $\bs_{i(1)}, \dots, \bs_{i(k)}$ be the $k$ nearest projected covariates in terms of the skeleton-based distance with $\calI \equiv \set{i(1), \dots, i(k)} \subset \set{1, \dots, n}$ the corresponding indices, such that
%$d_\calS(\bs_\ell, \bs ) \leq d_\calS(\bs_j, \bs )$ for any $\ell \in \calI $ and $j \in \set{1, \dots, n}\backslash\calI$.
%Then we have the skeleton-based $k$-nearest neighbors regressor (S-kNN) to be
%\begin{align}
%    \hat{m}_{SkNN}(\bs) = \frac{\sum_{\ell=1}^k Y_{i(\ell)}}{k},
%\end{align}


\begin{remark}
It is well known that the usual $k_n$NN regressor can be consistent if we let $k_n$ grow as a function of the sample size $n$, and under appropriate assumptions, \cite{DistributionFreeThoery} give the convergence rate of the $k_n$NN estimate $m_n$ to the true function $m$ as 
\begin{align*}
\mathbb{E}\left\|m_n-m\right\|^2 \leq \frac{\sigma^2}{k_n}+c_1 \cdot C^2\left(\frac{k_n}{n}\right)^{2 /d}
\end{align*}
Later, \cite{Kpotufe2011} has shown that the convergence rate of $k$NN regressor depends on the intrinsic dimension. 
We expect a similar result with $d=1$ rate for the skeleton $k$NN
regression at an edge point.
%The previous analysis on $k$NN regressor depends on some expansion rates of the local neighborhood, but, in the case of the skeleton, two different systems of measures are imposed on edges and knots and the local regions can be complicated if both knots and edges are involved.
\end{remark}

\subsection{Linear Spline Regression on Skeleton}

\label{sec::lspline}

% {\color{magenta}I rephrase this part a bit to make it clearer.}

% While the smoothing spline does not work on the skeleton,
% the linear spline method turns out to be applicable
% to the skeleton. 

~~~~In this section, we propose a skeleton-based linear spline model (S-Lspline) for regression estimation.
By construction, this approach results in a continuous model across the graph.
Moreover, we show that the skeleton-based linear spline corresponds to an elegant parametric regression model on the skeleton.
% We construct a linear model on each edge of the graph while ensuring that the predicted values agree on shared vertices, resulting in a continuous model across the graph. 
As the skeleton $\calS$
can be decomposed into the edge component $\mathcal{E}$
and the knot component $\mathcal{V}$, the linear spline regression on the skeleton can be written as the following constrained model:

\begin{equation}
\begin{aligned}
f: \calS\,\,\rightarrow \,\,\mathbb{R} \ \
\mbox{  such that }&\mbox{1. $f(x)$ is linear on $x\in\mathcal{E}$,}\\
&\mbox{2. $f(x)$ is continuous at $x\in \mathcal{V}$.}
\end{aligned}
\label{eq::LS_original}
\end{equation}
While solving the above constrained problem may not be easy,
we have the following elegant representer theorem
showing that a linear spline on the skeleton can be uniquely characterized by the values on each knot. 

\begin{theorem}[Linear spline representer theorem]
\label{thm::spline}
Any function satisfying equation \eqref{eq::LS_original}
can be characterized by $\{f(v): v\in\mathcal{V}\}$
and for $x\in\mathcal{E}$, $f(x)$ is linear interpolation
between the values on the two knots on the edge that $x$ belongs to.
\end{theorem}

\begin{proof}
Let $f$ be a function satisfying equation \eqref{eq::LS_original}. 
By construct, $f$ is linear for $x\in\calE$ and is continuous at
$x\in \calV$.
Let $V_j$ and $V_\ell$ be two knots that share an edge
and let $E_{j\ell} =\{x = t V_j + (1-t) V_\ell: t\in(0,1)\}$
be the shared edge segment. 
For any $x\in\calE$, there exists a pair $(V_j,V_\ell)$
such that $x\in E_{j\ell}$.
Because $f$ is linear in $E_{j\ell}$, 
$f$ can be uniquely characterized by the pairs $(f(e_1), e_1), (f(e_2), e_2)$ for two distinct points $e_1,e_2 \in \bar E_{j\ell}$,
where $\bar E_{j\ell} =\{x = t V_j + (1-t) V_\ell: t\in[0,1]\}$
is the closure of $E_{j\ell}.$
Thus, we can pick $e_1 = V_j$ and $e_2 =V_\ell$, which implies
that $f$ on the segment $E_{j\ell}$ is parameterized 
by $f(v_j)$ and $f(V_\ell)$, the values on the two knots. 

By applying this procedure to every edge segment,
we conclude that 
any function satisfying the first condition in \eqref{eq::LS_original}
can be characterized by the values of the knots. 
The second condition in \eqref{eq::LS_original} will require
that every knot has one consistent value. 
As a result, any function $f$ satisfying \eqref{eq::LS_original}
can be uniquely characterized by the values on the knot $\{f(x): x\in \calV\}$
and $f(x)$ will be a linear interpolation when $x\in \calE$. 

\end{proof}

%Notably, a linear function can be uniquely parameterized by
%the value of two distinct points. 
%With this insight, any linear spline model on a skeleton
%is uniquely determined by the values on each knot.
%Thus, we only need to estimate the predicted values on each knot.

%In more details, consider a linear function $f(t) = \alpha + \beta t$.
%This linear function is parameterized by $(\alpha, \beta)$.  
%However, if we pick any two points, say $t=0,1$, and determine
%the values $f(0)$, $f(1)$, the linear function can be parameterized as 
%$f(t) = f(0) + t(f(1)-f(0))$
%and $\alpha = f(0), \beta = f(1)-f(0)$. 

%which is equivalent to $f(t) = f(0) + (f(1) - f(0))t$ parametrized by $f(0) = \alpha$ and $f(1) = \alpha + \beta$. Also, fitting exactly one value to each knot guarantees the continuity of the model at the knots as required by the linear spline model. As a result, the linear spline model can be parameterized by the values on each knot.
Using Theorem~\ref{thm::spline},
we only need to determine the values on the knots. 
Let $\bbeta \in \mathbb{R}^k$
be the values of the skeleton linear spline model on each knot
with $k = \abs{\calV}$ being the number of knots.
As is argued previously, the spline model is parameterized by $\bbeta$,
so we only need to estimate $\bbeta$ from the data. 
Given $\bbeta$, the predicted value of each $\by_i$ is a linear interpolation
depending on the projected location of each $\bx_i$.

To derive an analytic form of $\by_i$,
we introduce a transformed covariate matrix 
$\bZ = (\bz_1, \dots, \bz_n)^T \in \bR^{n\times k}$ as follows:
%With the values-on-knots parametrization, we can fit the S-Lspline model through ordinary least squares with a graph-transformed $n \times v$ covariate matrix $\bZ = (\bz_1, \dots, \bz_n)^T$  where $v = \abs{\calV}$ is the number of knots and $\bz_j$ is the length $v$ transformed data vector for $\bx_j \in \calX$. 
%The covariates are transformed in the following way:
\begin{enumerate}
    \item If $\bx_i$ is projected onto a vertex that $\bs_i = V_j$ for some $j$, then
    \begin{align*}
        \bz_{ij'}=I(j'=j).
    \end{align*}
    %the transformed data $\bz_{ij}=1$ and $\bz_{ij'} = 0$ for $j'\neq j$. 
    \item If $\bx_i$ is projected onto an edge between knots $V_j$ and $V_\ell$, then 
    \begin{align*}
    \bz_{ij} = \frac{||\bs_i - V_j||}{||V_j - V_\ell||},
    \quad \bz_{i\ell} = \frac{||\bs_i - V_\ell||}{||V_j - V_\ell||}, \quad \text{ and } \bz_{ij'}=0 \text{ for } j'\neq j,\ell.
    \end{align*}
    %$\bz_i$ has value $\frac{||\bs_i - V_j||}{||V_j - V_\ell||}$ on the $\ell$-th entry, value $\frac{||\bs_i - V_\ell||}{||V_j - V_\ell||}$ on the $j$-th entry, and $0$ on all other entries. 
    
\end{enumerate}
With the above feature transform, the predicted value of $\by_i$ by the S-Lspline model is
\begin{align}
    \hat \by_i = \bbeta^T \bz_i.
\end{align}

To see this,
if $\bx_i$ is projected onto a vertex that $\bs_i = V_j$ for some $j$, the linear model with transformed covariates gives $\bbeta^T\bz_i = \bbeta_j$, the predicted value on vertex $V_j$.
In the case where $\bx_i$ is projected onto an edge between knots $V_j$ and $V_\ell$,
let $\bbeta_j$ and $\bbeta_\ell$ be the corresponding predicted values at $V_j$ and $V_\ell$, and the linear interpolation between $\bbeta_\ell$ and $\bbeta_j$ at $\bs_i$ can be written as
\begin{align*}
    \bbeta_j + \frac{||\bs_i - V_j||}{||V_j - V_\ell||} \cdot \left(\bbeta_\ell - \bbeta_j\right) = \frac{||\bs_i - V_\ell||}{||V_j - V_\ell||} \cdot \bbeta_j + \frac{||\bs_i - V_j||}{||V_j - V_\ell||} \cdot \bbeta_\ell = \bbeta^T\bz_i.
\end{align*}
To estimate $\bbeta$, we can apply the least squares procedure to get:
\begin{align*}
\hat \bbeta &= {\sf argmin}_{\bbeta} \sum_{i=1}^n (\by_i-\hat \by_i)^2\\
& = {\sf argmin}_{\bbeta} \sum_{i=1}^n ( \by_i- \bbeta^T \bz_i)^2.
\end{align*}
So it becomes a linear regression model
and the solution can be elegantly written as
\begin{align*}
    \hat{\bbeta} = (\bZ^T \bZ)^{-1} \bZ \by.
\end{align*}
Note that in a sense, the above procedure can be viewed
as placing a linear model 
\begin{align*}
    \mathbb{E}(\by|\bX) = \bbeta^T \bZ,
\end{align*}
where $\bZ$ is a transformed covariate matrix from $\bX$.
%With the above procedure,we transform $\bX$ into $\bZ$. 
%Let $\hat{\by}$ be the length $v$ vector of predicted values on all the knots.
%If $\bx_i$ is projected onto a vertex that $\bs_i = V_j$ for some $j$, the linear model with transformed covariates gives $\bz_i^T \hat{\by} = \hat{y}_j$, the predicted value on vertex $V_j$.
%If $\bx_i$ is projected onto an edge between knots $V_j$ and $V_\ell$,
%let $\hat{y}_j$ and $\hat{y}_\ell$ be the corresponding predicted values at $V_j$ and $V_\ell$, and the linear interpolation between $\hat{y}_\ell$ and $\hat{y}_j$ at $\bs_i$ can be written as
%\begin{align}
%    \hat{y}_j + \frac{||\bs_i - V_j||}{||V_j - V_\ell||} \cdot \left(\hat{y}_\ell - \hat{y}_j\right) = \frac{||\bs_i - V_\ell||}{||V_j - V_\ell||} \cdot \hat{y}_j + \frac{||\bs_i - V_j||}{||V_j - V_\ell||} \cdot \hat{y}_\ell = \bz_i^T \hat{\by}
%\end{align}
%Consequently, the S-Lspline model in matrix form can be written as 
%\begin{align}
%    \mathbb{E}(\by|\bZ) = \bbeta^T \bZ
%\end{align}
% {\color{magenta}YC: The formal regression model is the conditional mean; avoid writing something without conditional expectation.}
%for $\bbeta$ the $v\times 1$ column vector of coefficients with each coefficient $\beta_j = \hat{y}_j$ representing the predicted value on the corresponding knot. 
%To estimate the parameter $\bbeta$, we use the least square method, which leads to
%\begin{align}
%    \hat{\bbeta} = (\bZ^T \bZ)^{-1} \bZ \by
%\end{align}
Note that the S-Lspline model with the graph-transformed covariates does not include an intercept.
%New prediction can be carried out the same way by first transforming the new covariates and then use the estimated coefficients for predicted values.

\begin{remark}
%The parametrization can be testified by the sufficient condition on the degrees of freedom. 
An alternative justification of the value-on-knots parameterization is to calculate the degree of freedom. On each graph, the sum of the vertex degrees is twice the number of edges since each edge is counted from both ends. Let $e$ be the number of edges in the graph, let $v$ be the number of vertices, and let $r$ be the sum of all the vertex degrees, we have $r = 2e$. For the S-Lspline model, we construct a linear model with $2$ free parameters for each edge, and thus without any constraints, the total number of degrees of freedom is $2e$. For each vertex $V_i$ with degree $r_i$, the continuity constraint imposes $r_i - 1$ equations, and as a result, the continuity constraints consume a total of $\sum_{i=1}^v r_i-1 = r - v$ degrees of freedom. Combining it, we have $2e - (r -v) = v$ degrees of freedom, which matches the degrees of freedom given by the parametrization of values on the knots.
\end{remark}

% \begin{remark}
% A natural idea is to extend the linear spline model to a higher-order spline on the skeleton. However, higher-order spline models may encounter various issues. First, to guarantee the required degree of smoothness on the skeleton graph, we may need polynomials with higher degrees than those used in Euclidean cases. Second, the odd-degree derivatives are directional and thus depend on the direction of edges, which leads to different models on a graph when there are different edge directions. Discussions on higher-order splines on graphs are beyond the scope of this paper and will be considered as future work.
% \end{remark}

\subsubsection{Regularized Linear Spline Method}
\label{sec:penalLspline}
~~~~Given the formulation of the S-Lspline as a linear regression with transformed data, it is natural to incorporate penalization with this method. In this section, we introduce penalization into the S-Lspline method by making connections to the literature about regularization on graphs, with a particular focus on graph Laplacian smoothing by \cite{GraphLaplacianSmoothing} and graph trend filtering by \cite{Wang2016}.

Let $B$ be the (unoriented) incidence matrix of the skeleton graph that 
\begin{align*}
    B_{ij} =
    \begin{cases}
    1 & \text{if vertex $v_i$ is incident with edge $e_j$ },\\
    0 & \text{otherwise.}
\end{cases}
\end{align*}
for $i = 1, \dots, k$ where $k$ is the number of knots and $j = 1, \dots, m$ where $m$ is the number of edges in the skeleton graph.
Let $L$ denote the Laplacian matrix that $L = D - A = B B^T$ where $D$ is the degree matrix and $A$ is the adjacency matrix of the skeleton graph.
The $q$-th order trend filtering matrix, for $q \in \set{0,1,2,\dots}$,  is defined as
\begin{align*}
    \Delta^{(q+1)} = \begin{cases}
    L^{\frac{q+1}{2}} & \text{for odd }q,\\
    B L^{q/2} & \text{for even }q.
\end{cases}
\end{align*}

The $q$-th order Laplacian smoothing can be taken as the $L_2$ penalty with the trend filtering matrix, and we have the regularized problem to be 
\begin{align*}
     {\sf argmin}_{\bbeta} \norm{\bY - \bZ \bbeta}_2^2 + \lam \norm{\Delta^{(k+1)} \bbeta}_2 
\end{align*}
where $\norm{\Delta^{(k+1)} \bbeta}_2  = \bbeta^T L^{k+1} \bbeta$ for Laplacian matrix $L$, and $\bZ$ the transformed covariate matrix from $\bX$.
This can be solved as a Generalized Ridge problem\footnote{Generally if the penalty matrix $L^{k+1} $ is positive definite, the generalized penalty is a non-degenerated quadratic form in $\beta$, and hence strictly convex. The analytical solution is then
\begin{align*}
    \hat \beta = \brac{X^T X + \lam L^{k+1}}^{-1} ,\brac{X^T Y}
\end{align*}
However, the Laplacian matrix is only positive semi-definite, and therefore the loss function need not be strictly convex. Some work suggests adding $\norm{\bbeta}_2^2$ as an additional penalty to address this, but we do not implement that to be consistent with the trend filtering penalization.}. 

The Trend Filtering regularization similarly applies a $L_1$ penalty and the problem becomes
\begin{align*}
     {\sf argmin}_{\bbeta} \norm{\bY - \bZ \bbeta}_2^2 + \lam \norm{\Delta^{(k+1)} \bbeta}_1.
\end{align*}
We follow \cite{SolutionPathLasso} to get the solution to the generalized Lasso problem. We include the algorithm in Appendix \ref{sec:lassoSolution} for completeness.
Empirically, we observe that penalization does not improve the regression results of the S-Lspline model (see Appendix \ref{sec:penalSplineSims}). 
To account for this, note that the skeleton graph is a summarizing presentation of the data with a concise structure, and the S-Lspline method assumes a simple piecewise linear model on the skeleton which inherits the simple geometric structure and is not a complex model in nature, and hence adding penalization does not improve the performance of this method.



\subsection{Challenges of Other Nonparametric Regression}
\label{sec::otherParametric}
~~~~In this section, we discuss the challenges when applying other nonparametric regression methods to the skeleton. 
Particularly, the skeleton graph is only equipped with a metric and 
does not have a well-defined inner product or orientation,
which makes many conventional approaches not directly applicable.
%lacks many nice structures compared to Euclidean space or Hilbert space, which requires additional considerations when adopting some nonparametric methods.



\subsubsection{Local polynomial regression}

% {\color{magenta}Can we do local polynomial regression? 
% I don't think we can do it. And here we need to explain why we
% cannot do it. }

%The kernel estimator described in Section \ref{sec::skelkernel} can be understood as introducing kernel weights 
%for localization in an intercept-only linear model.
~~~~Local polynomial regression \cite{fan2018local}
is a common generalization of the kernel regression that
tries to improve the kernel regression estimator by using higher-order polynomials as local approximations to the regression function. 
In the Euclidean space, a $p$-th order local polynomial regression aims to choose $\beta(\bx)$ via minimizing
\begin{align}
    \sum_{i=1}^n \sbrac{Y_i - \sum_{j = 0}^p \beta_j (\bx_i - \bx)^j}^2 K \brac{\frac{\bx_i - \bx}{h}}
\end{align}
and predict $m(\bx)$ via $\hat \beta(\bx)$, the first element in the minimizer.
Note that when $p=1$, one can show that this is equivalent to the kernel regression.

Unfortunately, the local polynomial regression cannot be easily adapted to the skeleton because the polynomial $(\bx_i - \bx)^j$ requires a well-defined orientation, which is ill-defined at a knot (vertex). 
%To adopt this approach to the skeleton graph, we need to first define $(\bs_i -\bs)$ for $\bs_i, \bs \in \calS$ and the corresponding polynomials.
%However, as we only have metric on $\calS$, the difference cannot be naturally defined as in vector spaces. 
Directly replacing $(\bs_i -\bs)$ with the distance $d_\calS (\bs_i -\bs)$ will make all the polynomials to be non-negative, which will be problematic for odd orders. 
Unless in some special skeletons such as a single chain structure,
the local polynomial regression cannot be directly applied.

%Given the kernel function is constrained on an edge with a clearly defined direction, the local polynomial model can be constructed, but additional considerations are needed around the knots.

\subsubsection{Higher-Order Spline}
%~~~~With the skeleton-based linear spline model described in Section 

~~~~In Section \ref{sec::lspline}, we introduce the linear spline model.
One may be curious about the possibility of using a higher-order spline (enforcing higher-order smoothness on knots; see, e.g., Chapter 5.5 of \cite{wasserman2006all}).
% It is a natural thought to generalize the model with higher-order splines that ensure higher-order smoothness of the model.
Unfortunately, the higher-order spline is generally not applicable to the skeleton
because a higher-order spline requires derivatives and the concept of a derivative may be ill-defined on a knot because of the lack of orientation.
To see this, consider a knot with three edges connecting to it. 
There is no simple definition of derivative at this knot unless we specify
the orientation of these three edges.
%needs a more careful treatment on the skeleton graph. 

One possible remedy is to introduce an orientation for every edge.
This could be done by ordering the knots first and, for every edge,
the orientation is always from a lower index vertex to the higher index vertex. 
With this orientation, it is possible to create a higher-order spline
on the skeleton
but the result will depend on the orientation we choose.

%First, for a knot with a degree larger than or equal to $3$, we need a quantity like the total derivative to ensure a smooth change from the knot to all of its connected edges. 
%Moreover, odd-degree derivatives are directional but many graphs, including the skeleton graph, do not have built-in directions. 
%Empirically, different edge directions do lead to different spline models on the graph and do give different empirical performances.

Even with edge directions provided and the derivatives on the skeleton defined, higher-order spline on the skeleton can be prone to overfitting.
Classical spline methods use degree $p+1$ polynomial functions to achieve continuity at $p$-th order derivative. For example, univariate cubic splines use polynomials up to degree $3$ to ensure the second-order smoothness of the regression function at each knot. 
However, on a graph, degree $p+1$ polynomial functions may fail to achieve continuity at $p$-th order derivative, and on complete graphs, which is the worst case, $2p+1$ degree polynomials are needed instead.


\subsubsection{Smoothing Spline}
% {\color{magenta}Can we do smoothing spline regression? 
% I don't think we can do it. And here we need to explain why we
% cannot do it. }
~~~~Smoothing spline \cite{wang2011smoothing, wahba1975smoothing} is another popular approach
for curve-fitting that attempts to 
find a smooth curve that minimizes the square loss in the prediction
with a penalty on the curvature (second or higher-order derivatives).

The major difficulty of this method is that the concept of a \emph{smooth} function
is ill-defined at a knot even if we have a well-defined orientation.
In fact, the `linear function' is not well-defined in general on a skeleton's knot.
To see this, consider a knot $V_0$ with three edges $e_1,e_2,e_3$ connecting to $V_1,V_2,V_3$, respectively.
%Suppose the orientation is $V_1\rightarrow V_0, V_0\rightarrow V_2, V_0\rightarrow V_3$.
Suppose we have a linear function $f_0$ and
$f_0$ is linearly increasing on paths $V_1-V_0-V_2$
and $V_1-V_0- V_3$. 
However, on the path $V_2-V_0-V_3$, the function $f_0$ will be 
decreasing ($V_2-V_0$) and then increasing ($V_0-V_3$), leading to a non-smooth structure.


%Unless we introduce an orientation, we cannot 



%Smoothing spline balances the goodness of fit and a derivative-based measure of smoothness of the estimator, and hence faces similar challenges as the higher-order spline when adapted to the skeleton graph.  
%Particularly, the well-defined directions of the edges along with careful considerations around the knots are needed to make sense of the derivatives on the skeleton.

\subsubsection{Orthonormal Basis and Tree}

~~~~Orthonormal basis approach (see, e.g., Chapter 8 of \cite{wasserman2006all}) uses a set of orthonormal basis functions to approximate the regression function. 
In general, it is unclear how to find a good orthonormal basis 
for a skeleton unless the skeleton is simply a circle or a chain. 

Having said that, it is possible to construct an orthonormal basis
borrowing the idea from wavelets \citep{torrence1998practical}.
The key idea is that the skeleton is a measurable set
that we can measure its (one-dimensional) volume. 
Thus, we can partition the skeleton $\calS$ into two equal-volume sets $A_1, A_2$. Note that the resulting sets $A_1, A_2$ are not necessarily skeletons because we may cut an edge into two pieces. 
For each set $A_j$, we can further partition it again into equal volume sets $A_{j,1}, A_{j,2}$. 
And we can repeat this dyadic procedure to create many equal-volume subsets.
We then define a basis as follows:
\begin{align*}
f_0(s) &= 1,\\
f_1(s) & =  I\brac{s\in A_1} -  I\brac{s\in A_2}\\
f_2(s) & =  I(s\in A_{1,1}) -  I(s\in A_{1,2})\\
f_3(s) & =  I(s\in A_{2,1}) -  I(s\in A_{2,2})\\
\vdots
\end{align*}
After normalization, this set of functions forms
an orthonormal basis.
With this basis, it is possible to fit an orthonormal basis on the skeleton. 
However, the above construction creates the partition arbitrarily. 
The fitting result depends 
on the particular partition we use to generate the basis
and it is unclear how to pick a reasonable partition in practice. 


The regression tree \cite{breiman2017classification, loh2014fifty} is a popular idea in nonparametric regression that fits the data via creating a tree of partitioning the whole sample space 
whose leaves represent a subset of the sample space
and predicts the response using a single parameter at each leaf (region). 
This idea could be applied to the skeleton using a similar
procedure as the construction of an orthonormal basis
that we keep splitting a region into two subsets (but
we do not require the two subsets to be of equal size). 
However, unlike the usual regression tree (in Euclidean space) that
the split of two regions is often at a threshold at one coordinate,
the split of a skeleton may not be easily represented
as the skeleton is just a connected subregion of Euclidean space.
Therefore, similar to the orthonormal basis, 
regression tree may be used in skeleton regression, but
there is no simple and principled way to create
a good partition.




%The orthonormal basis approach approximates the regression function with a set of orthonormal functions. 
%However, the usual orthonormal basis in Euclidean space cannot be directly adopted to the skeleton. 
%It can be possible to define an orthonormal basis on the skeleton with wavelet constructs, but this is beyond the scope of this work.


\section{Simulations}	\label{sec::simulation}
~~~~In this section, we use simulated data to evaluate the performance of the proposed skeleton regression framework. 
\footnote{R implementation of the proposed skeleton regression methods can be accessed at \url{https://github.com/JerryBubble/skeletonMethods} and Python implementation can be accessed at \url{https://pypi.org/project/skeleton-methods/}.}
We first demonstrate an example with the intrinsic domain composed of several disconnected components, which we call the Yinyang data (Section \ref{sec::Yinyang}). Then, we add noisy observations to the Yinyang data (Section \ref{sec::NoisyYinyang}) to show the effectiveness of our method in handling noisy data points. Moreover, we present an example where the domain is a continuous manifold with a Swiss roll shape (Section \ref{sec::SwissRoll}). In all the simulations in this section, there are random perturbations in the intrinsic dimensions, and we add random Gaussian variables as covariates to increase the ambient dimension.


\subsection{Analysis Procedure}
\label{sec:analysisProcedure}
~~~~We apply the following analysis procedure for all the simulations in this section.
We randomly generate the dataset for $100$ times, and, on each dataset, we use $5$-fold cross-validation to calculate the sum of squared errors (SSE) as the performance assessment. 
We use the skeleton construction method described in Section \ref{sec::skeletoncons} to construct skeletons with varying numbers of knots on each training set. 
In this section, we present results where the construction procedure cuts the skeleton into a given number of disjoint components according to the Voronoi Density weights (Section \ref{sec::skeletoncons}).
We also empirically tested using different cuts to get skeleton structures with different numbers of disjoint components under the same number of knots and noticed little change in the squared error performance (see Appendix \ref{sec::extraSim}). 

% \YC{how do you determine the edge directions in the graph? randomly generated the index and pointing from low-index to high-index?} 
% \JW{Yes}

We evaluate the skeleton-based nonparametric regressors introduced in Section \ref{sec::regression}: skeleton kernel regression (S-Kernel), $k$-NN regressor using skeleton-based distance (S-kNN), and the skeleton spline model(S-Lspline). For S-Kernel and S-kNN methods, To simplify the calculation, we only compute the skeleton-based distances between points in the same or neighboring Voronoi cells. That is the skeleton-based distance between a pair of points is calculated when they share at least one knot from their respective set of two closest knots.
For the S-Lspline method, we include the results without additional penalization in this section. We compare the empirical performance of the S-Lspline method with various penalizations discussed in Section \ref{sec:penalLspline} on the simulated datasets and present the results in Appendix \ref{sec:penalSplineSims}, and we observe that incorporating penalization terms does not improve the empirical performance of the S-Lspline method.
% and Appendix \ref{sec::highsplineparam}, where for edge directions of higher-order derivatives we randomly index the knots and let edges pointing from low-index knots to high-index knots. 

For comparisons, we apply the classical k-nearest-neighbors regression based on Euclidean distances (kNN).
% with varying numbers of neighbors. 
For penalization regression methods, we test Lasso and Ridge regression.
% with different penalization coefficients are tested. 
Among the recent manifold and local regression methods, we include the Spectral Series approach \citep{Lee2016} with the radial kernel (SpecSeries) for its superior performance\footnote{The Spectral Series approach demonstrates similar empirical performance as the kernel machine learning methods with regularization in RKHS as in \cite{Lee2016}.} and readily available R implementation \footnote{\url{https://projecteuclid.org/journals/supplementalcontent/10.1214/16-EJS1112/supzip_1.zip}}. 
% The number of basis functions in SpecSeries is auto-tuned with the validation data of each fold, which can be advantageous, and we test different bandwidth of the kernel used. 
For kernel machine learning approaches, we include the Divide-and-Conquer Kernel Ridge Regression (Fast-KRR) method as in \cite{Yuchen2013}. For Fast-KRR, we set the penalization hyperparameter $\lambda = 1/n$ and set the number of random partitions $m = \sqrt{n}$ where $n$ is the size of the training sample, and use the radial kernel where the best bandwidth $\sigma$ is given by grid search. 

For the simulations presented in this section, we add random Gaussian variables to create settings with a large ambient dimension of $1000$ to demonstrate that the proposed skeleton regression framework is robust under such challenging scenarios. 
For completeness, we also include the simulation results on low-dimensional data settings in Appendix \ref{sec::extraSim}, and the skeleton-based regression methods also show competitive performance in such settings.
% The SpecSeries approach in this setting has performance similar to the Lasso regression and did not improve much on the regression results utilizing information about the underlying manifold structure, possibly due to the large number of noisy dimensions.


\subsection{Yinyang Data}
\label{sec::Yinyang}

~~~~The covariate space of Yinyang data is intrinsically composed of $5$ disjoint structures of different geometric shapes and different sizes: a large ring of $2000$ points, two clumps each with $400$ points (generated with the \texttt{shapes.two.moon} function with default parameters in the \texttt{clusterSim} library in R \citep{clusterSim}), and two 2-dimensional Gaussian clusters each with $200$ points (Figure \ref{fig::YinyangData} left). Together there are a total of $3200$ observations. 
Note that the intrinsic structures of the components are curves and points, and, with perturbations, the generated covariates do not lay exactly on the corresponding manifold structures.
The responses are generated from a trigonometric function on the ring and constant functions on the other structures with random Gaussian error(Figure \ref{fig::YinyangData} right). That is, let $\epsilon \sim N(0,0.01)$ and let $\theta$ be the angle of the covariates, then
\begin{align*}
\label{eq::YinyangResponse}
    Y = \epsilon +\begin{cases}
  \sin(\theta*4) + 1.5& \text{for points on the outer ring} \\
  0& \text{for points on the bottom-right Gaussian cluster}\\
  1& \text{for points on the right clump}\\
  2& \text{for points on the left clump}\\
  3& \text{for points on the upper-left Gaussian cluster}
    \end{cases}
\end{align*}
To make the task more challenging with the presence of noisy variables, we add independent and identically distributed random $N(0,0.01)$ variables to the generated covariates. In this section, we increase the dimension of the covariates to a total of $1000$ with those added Gaussian variables.
% Empirically we find the advantage of the proposed skeleton regression methods is more profound when there are more noisy dimensions, and we include such simulation results in Appendix \ref{sec::extraSim}.

% \YC{please write down how $Y$ is generated!}


For the Yinyang data, we cut the skeleton into $5$ disjoint components during the skeleton construction process according to the Voronoi Density weights.
We take the median, 5th percentile, and 95th percentile of the 5-fold cross-validation Sum of Squared Errors (SSEs) for each parameter setting of each method on the 100 datasets. We present the smallest median SSE for each method in Table \ref{table:Yinyangd1000} along with the corresponding best parameter setting.


\begin{figure}
% \captionsetup{skip=1pt}
\centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Yinyang_data2D.jpeg} 
    \end{subfigure}
        \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Yinyang_data3D.jpeg}
    \end{subfigure}
    \vspace{-1em}
\captionof{figure}{Yinyang Regression Data}
\label{fig::YinyangData}
\vspace{2em}

%%%%%%%%%%%%%%%%%%%%%%
\begin{tabular}{c l c l} 
 \hline
 Method & Median SSE ($5$\%, $95$\%) & nknots & Parameter \\ [1ex] 
 \hline
 kNN &204.5 (192.3, 221.9) & - & neighbor=18 \\ 
 Ridge & 2127.0 (2100.2, 2155.2) & & $\lambda  = 7.94$\\
 Lasso & 1556.8 (1515.4, 1607.9) & & $\lambda = 0.0126$\\
 SpecSeries & 1506.4 (1469.1,1555.6) & - &bandwidth = 2\\
 Fast-KRR & 2404.0 (2370.0, 2440.2) & - &$\sigma$ = 0.1\\
 S-Kernel & 91.6 (81.6, 103.5) & 38 & bandwidth = 4 $r_{hns}$ \\
 S-kNN & 92.7  (84.5, 102.8) & 38 & neighbor = 36 \\
 S-Lspline & 94.4 (87.7, 103.2) & 38 & -  \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on Yinyang $d=1000$ data. The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$th percentile and $95$th percentile of the SSEs from the given parameter settings are reported in brackets.}
\label{table:Yinyangd1000}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%

\centering
\includegraphics[width=\textwidth]{figures/Yinyang1000_knots.jpeg}
\captionof{figure}{Yinyang $d = 1000$ data regression results with varying number of knots. The median SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::Yinyangd1000Numknots}

\end{figure}



We observed that all the skeleton-based methods (S-Kernel, S-kNN, and S-Lspline) perform better than the standard kNN in this setting. That is, the skeleton better captures the geometric structures of the data and improves the downstream regression performance.
The three skeleton-based methods have similar performance on this simulated Yinyang data, but S-Lspline method can be preferred in this case in terms of computation as it does not require the skeleton-based distance computations.
The spectral method SpecSeries and the kernel machine learning approach Fast-KRR both perform worse than the classical kNN. The underlying data structure being comprised of multiple disconnected components in this case can diminish the power of such manifold learning methods.
Ridge and Lasso regression, despite the regularization effect, resulted in relatively high SSEs. Therefore, the skeleton regression framework has the empirical advantage when dealing with covariates that lie around manifold structures.
% The advantage of skeleton-based methods is manifested more if the number of noisy variables in the input vector gets larger (see Appendix \ref{sec::Yinyang1000}).

In Figure \ref{fig::Yinyangd1000Numknots}, we present the median SSE of the S-Lspline, S-Kernel, and S-kNN methods on skeletons with various numbers of knots. The vertical dashed line indicates $[\sqrt n] = 51$ knots as suggested by the empirical rule, where $n$ is the training sample size.
The empirical rule seems to produce satisfactory results in this simulation study, roughly identifying the ``elbow'' position, but it's advised to use cross-validation for fine-tuning in practice.



% The fitted values given by S-Lspline, S-Kernel, and S-kNN methods are illustrated in Figure \ref{fig::Yinyangd100fit}. For each method, we illustrate the fitted values from the best SSE model. We see that all the methods fit well.

% \begin{figure}[ht]
% \captionsetup{skip=1pt}
% \centering
%     \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Yinyangd100_fitLspline.jpeg} 
%         \caption{Linear Spline}
%     \end{subfigure}
%         \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Yinyangd100_fitskelkernel.jpeg}
%         \caption{Skeleton Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Yinyangd100_fitskelknn.jpeg}
%         \caption{Skeleton KNN}
%     \end{subfigure}
% \caption{Yinyang Regression fitted points $d = 100$ with varying number of knots results}
% \label{fig::Yinyangd100fit}
% \end{figure}

%------------------------------------------------------------------------------------------------------------------------


\subsection{Noisy Yinyang Data}
\label{sec::NoisyYinyang}
~~~~To show the robustness of the proposed skeleton-based regression methods, we add $800$ noisy observations to the Yinyang data in Section \ref{sec::Yinyang} ($20\%$ of a total of $4000$ observations). 
The first two dimensions of the noisy covariates are uniformly sampled from the $2$-dimensional square $[-3.5,3.5]\times [-3.5,3.5]$ and independent random normal $N(0,0.01)$ variables are added to make the covariates $1000$-dimensional in total. 
The responses of the noisy points are set as $1.5 + \epsilon$ with $\epsilon \sim N(0,0.01)$, while the responses on the Yinyang covariates are generated the same as in Equation \ref{eq::YinyangResponse}. 
The first two dimensions of the Noisy Yinyang covariates are plotted in Figure \ref{fig::NoiseYinyangData} left and the $Y$ values against the first two dimensions of the covariates are illustrated in Figure \ref{fig::NoiseYinyangData} right.

\begin{figure}
% \captionsetup{skip=1pt}
\centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/NoiseYinyang_data2D.jpeg} 
    \end{subfigure}
        \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/NoiseYinyang_data3D.jpeg}
    \end{subfigure}
\vspace{-1em}
\captionof{figure}{Noisy Yinyang Regression Data}
\label{fig::NoiseYinyangData}
\vspace{2em}
%%%%%%%%%%%%%%%%%%
\centering
\begin{tabular}{c l c l} 
 \hline
 Method & Median SSE ($5$\%, $95$\%)  & Number of knots & Parameter \\ [1ex] 
 \hline
 kNN & 440.8 (420.4, 463.0) & -  & neighbor=18 \\ 
 Ridge & 2139.1 (2102.6, 2171.1) &- & $\lambda  = 6.31$\\
 Lasso &  2029.2 (1988.7, 2071.0) &- & $\lambda = 0.02$\\
 SpecSeries & 1532.0 (1490.7, 1563.2) & - & bandwidth = $2$\\
 Fast-KRR & 2584.6 (2556.3, 2624.5) &- & $\sigma$ =0.1\\
 S-Kernel & 313.5 (293.2, 331.1) & 28 & bandwidth = 2 $r_{hns}$ \\
 S-kNN & 352.9 (332.4, 376.7) & 28 & neighbor = 15 \\
 S-Lspline & 376.5 (354.3, 399.2) & 57 &  - \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on Noisy Yinyang $d=1000$ data.The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$ percentile and $95$ percentile of the SSEs from the given parameter settings are reported in brackets.}
\label{table:NoiseYinyangd1000}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
\includegraphics[width=\textwidth]{figures/NoiseYinyang1000_knots.jpeg}
\captionof{figure}{Noisy Yinyang $d = 1000$ data regression results with varying number of knots. The median SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::NoiseYinyangd1000Numknots}

\end{figure}

% \YC{what do you mean by "do not cut the constructed skeleton"?}
% \JW{The number of disconnected components in the skeleton is 1.}

To evaluate the robustness of the proposed skeleton-based regression methods, we randomly generate the Noisy Yinyang data 100 times and follow the analysis procedure as in Section \ref{sec:analysisProcedure}, except that we leave the skeleton to be a fully connected graph. We also took the median, 5th percentile, and 95th percentile of the 5-fold cross-validation SSEs for each parameter setting of each method on the 100 datasets. The smallest median SSE for each method is reported in Table \ref{table:NoiseYinyangd1000} along with the corresponding best parameter setting.

It can be observed that all the skeleton-based regression methods outperform the standard kNN approach, which indicates that the skeleton regression framework can capture the data structure in the presence of noisy observations and give good regression performance.
Among the skeleton-based methods, the S-Kernel has the best performance, and kernel smoothing can be a helpful nonparametric technique to deal with noisy observations.
The SpecSeries, Fast-KRR, Ridge, and Lasso regressions again fail to provide good performance on this simulated dataset.
The advantage of the skeleton regression framework is more manifesting with noisy observations.

In Figure \ref{fig::Yinyangd1000Numknots}, we plot the median SSE of the skeleton-based methods on skeletons with different numbers of knots. Using the empirical rule to construct a skeleton with $[\sqrt{3200}] = 57$ knots results in good regression performance and approximately identifies the ``elbow'' position in Figure \ref{fig::Yinyangd1000Numknots}. However, for some skeleton-based methods, using a number of knots larger than that given by the empirical rule leads to better regression performance.
This improvement is related to the phenomenon observed in \citet{skelclus} that when dealing with noisy observations, it's better to have a skeleton with more knots and cut the skeleton into more disjoint components in order to have a cleaner representation of the key manifold structures.
Therefore, when facing data with noisy feature vectors, it's advised to empirically tune the number of knots favoring larger values.


% We again illustrate the fitted values given by the best S-Lspline, S-Kernel, and S-kNN models in Figure \ref{fig::NoiseYinyangd100fit}. The fitted values all follow the true regression function well, but all show a tendency to be shifted toward the origin. This is caused by the added noisy observations which have regression values all around $0$.

% \begin{figure}[ht]
% \captionsetup{skip=1pt}
% \centering
%     \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/NoiseYinyangd100_fitLspline.jpeg} 
%         \caption{Linear Spline}
%     \end{subfigure}
%         \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/NoiseYinyangd100_fitskelkernel.jpeg}
%         \caption{Skeleton Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.27\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/NoiseYinyangd100_fitskelknn.jpeg}
%         \caption{Skeleton KNN}
%     \end{subfigure}
% \caption{Noisy Yinyang Regression fitted points $d = 100$ with varying number of knots results}
% \label{fig::NoiseYinyangd100fit}
% \end{figure}


%----------------------------------------------------------------------------------------

\subsection{SwissRoll Data}
\label{sec::SwissRoll}
~~~~The intrinsic components of the covariates in Yinyang data are all well-separated, which, admittedly, can give an advantage to skeleton-based methods. 
Moreover, the intrinsic dimensions of the structural components for Yinyang data covariates are all lower than or equal to $1$ and can be straightforwardly represented by knots and line segments, potentially giving another advantage to skeleton-based methods.
To address such concerns, we present another simulated data which has covariates lying around a Swill Roll shape (Figure \ref{fig::SwissRollData} left), an intrinsically $2$-dimensional manifold in the $3$-dimensional Euclidean space. 
To make the density on the Swill Roll manifold balanced, we sample points inversely proportional to the radius of the roll in the $X_1 X_3$ plane. 
Specifically, let $u_1, u_2$ be independent random variables from $\text{Uniform}(0,1)$ and let the angle in the $X_1 X_3$ plane be generated as $\theta_{13} = \pi 3^{u_1}$. 
Then for the first $3$ dimensions of the covariates we have
\begin{align*}
  X_1 = \theta_{13} \cos(\theta_{13}), \ \ X_2 = 4 u_2, \ \ X_3 =  \theta_{13} \sin(\theta_{13}) 
\end{align*}
The true response has a polynomial relationship with the angle on the manifold if the $X_2$ value of the point is within some range. 
Let $\Tilde{\theta}_{13} = \theta_{13} - 2 \pi$, and let $\epsilon \sim N(0, 0.3)$. Then we set
\begin{align*}
    Y =  0.1\times \Tilde{\theta}_{13}^3 \times \left[ I(X_2<\pi) + I( 2\pi < X_2<3\pi) \right] + \epsilon
\end{align*}
The response versus the angle $\theta_{13}$ and $X_2$ is demonstrated in Figure \ref{fig::SwissRollData} right. Independent random Gaussian variables from $N(0,0.1)$ are added to make the covariates $1000$-dimensional in total, and $2000$ observations are sampled to make the Swiss Roll dataset.


\begin{figure}

\centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/SwissRoll_X.jpeg} 
    \end{subfigure}
        \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/SwissRoll_Y.jpeg}
    \end{subfigure}
\captionof{figure}{SwissRoll Regression Data}
\label{fig::SwissRollData}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{2em}
\begin{tabular}{c l c l} 
 \hline
 Method & Median SSE ($5$\%, $95$\%)  & nknots & Parameter \\ [1ex] 
 \hline
 kNN & 648.5 (607.1, 696.0) & - & neighbor=12 \\ 
 Ridge &  1513.7 (1394.4, 1616.2) & -& $\lambda  = 2.0$\\
 Lasso &  1191.4 (1106.7, 1260.7) & -& $\lambda = 0.032$\\
 SpecSeries & 1166.5 (1081.4, 1238.8) &- & bandwidth = $2.0$\\
 Fast-KRR & 1503.5 (1403.2, 1592.9) & - & $\sigma = 0.1$ \\
 S-Kernel & 458.2 (409.0, 511.8) & 30 & bandwidth = 2 $r_{hns}$ \\
 S-kNN & 474.7 (417.6, 553.4) & 30 & neighbor = 18 \\
 S-Lspline & 569.8 (519.5, 645.8) & 60 & $\lambda = 0$ \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on the Swiss Roll $d=1000$ data. The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$ percentile and $95$ percentile of the SSEs from the given parameter settings are reported in brackets.}
\label{table:Swissd1000}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
\includegraphics[width=\textwidth]{figures/SwissRoll1000_knots.jpeg}
\captionof{figure}{SwissRoll $d = 1000$ data regression results with varying number of knots. The median SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::SwissRolld1000}

\end{figure}

We follow the same analysis procedures as in Section \ref{sec:analysisProcedure} with the skeletons constructed to be fully connected graphs without additional graph cuts. We took the median, 5th percentile, and 95th percentile of the 5-fold cross-validation SSEs across each parameter setting for each method on the 100 datasets, and reported the smallest median SSE for each method along with the corresponding best parameter setting in Table \ref{table:Swissd1000}.

All the proposed skeleton-based methods have better performance than the standard kNN regressor, while the S-Kernel method had the best performance in terms of SSE.
Particularly, the methods that utilize the skeleton-based distances, S-Kernel and S-kNN, have significantly better performance compared to the S-Lspline method which only utilizes the knot-edge structure of the skeleton graph.
Intuitively, the skeleton-based distances are good approximations to the geodesic distances on the manifold and hence lead to improvements in the regression performance.
The spectral and penalization approaches do not demonstrate good performance on this simulated data.
Therefore, the proposed skeleton regression framework can also be powerful for data on connected, multi-dimensional manifolds.


By plotting the median SSE under skeletons with a varying number of knots in Figure \ref{fig::SwissRolld1000}, we observed that the best performance for all the skeleton-based methods is achieved with the number of knots larger than $[\sqrt{1600}] = 40$ knots. Given the intrinsic structure of the Swiss Roll input space is a $2$D plane, having more knots on the plane can give a better representation of the data structure and, therefore, lead to better prediction accuracy. We conjecture that the optimal number of knots should depend on the intrinsic dimension of the covariates, and we plan to discuss this further in future work. However, it's recommended to use cross-validation to choose the number of knots in practice.


% \begin{figure}[ht]
% \captionsetup{skip=1pt}
% \centering
%     \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Swissd100_nknot70_lsplinefit.jpeg} 
%         \caption{Linear Spline}
%     \end{subfigure}
%         \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Swissd100_nknot160_bandrate6_skelkernelfit.jpeg}
%         \caption{Skeleton Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Swissd100_nknot160_k18_skelnnfit.jpeg}
%         \caption{Skeleton KNN}
%     \end{subfigure}
% \caption{SwissRoll Regression fitted points $d = 100$ with varying number of cuts, number of knots fixed to be $60$}
% \label{fig::SwissRolld100fit}
% \end{figure}




\section{Real Data}	\label{sec::real}

~~~~In this section, we present analysis results on two real datasets.
We first predict the rotation angles of an object in a sequence of images taken from different angles (Section \ref{sec::cup}). 
For the second example, we study the galaxy sample from the Sloan Digital Sky Survey (SDSS) to predict the spectroscopic redshift (Section \ref{sec::sdss}), a measure of distance from a galaxy to Earth.



\subsection{Cup Images Data}
\label{sec::cup}
~~~~
This dataset consists of $72$ gray-scale images of size $128\times 128$ pixels taken from the COIL-20 processed dataset \citep{COIL20}. They are 2D projections of a 3D cup obtained by rotating the object by $72$ equispaced angles on a single axis.
Several examples of the images are given in Figure \ref{fig::cup}. 

The response in this dataset is the angle of rotation. However, this response has a circular nature where degree 0 is the same as degree 360. To avoid this issue, we removed the last 8 images from the sequence, only using the first 64 images. As a result, our dataset consists of 64 samples from a 1-dimensional manifold embedded in $\RR^{16384}$ along with scalar values representing the angle of rotation.
To assess the performance of each method, we use leave-one-out cross-validation, that, in each iteration, one image is taken out of the dataset and the regression methods are fitted to the remaining images to estimate the angle of the left-out image. 


\begin{figure}
\centering
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__0.png} 
    \end{subfigure}
        \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__12.png}
    \end{subfigure}
            \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__24.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__36.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__48.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__60.png}
    \end{subfigure}
\captionof{figure}{A part of the cup images from the COIL-20 processed dataset. Each image
is of size $128\time 128$ pixels.}
\label{fig::cup}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\centering
\begin{tabular}{c c l} 
 \hline
 Method & SSE &  Parameter \\ [1ex] 
 \hline
 kNN & 1147.2 & neighbor=3 \\ 
 Ridge& -&-\\
 Lasso& -&-\\
 SpecSeries& -&-\\
 Fast-KRR & -&-\\
 S-Kernel & 1735.0 &  bandwidth = 2$r_{hns}$ \\
 S-kNN & 2068.8 & neighbor = 2\\
 S-Lspline &  1073.4  &- \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on cup images data from COIL-20. The best SSE from each method is listed with the corresponding parameters used.}
\label{table:cup}

\end{figure}

Similarly to the simulation studies, we use the skeleton construction method with Voronoi weights in \citet{skelclus} to construct the skeleton on the training set. 
In practice, we found that a small number of knots can still lead to loops in the constructed skeleton structure, and, after some tuning, we fit $2[\sqrt{n}] = 16$ knots to each training set. Additionally, since the underlying manifold should be one connected structure, we do not cut the constructed skeleton structure in this experiment. 
Due to the high-dimensional nature of the data, Ridge regression, Lasso regressions, and the Spectral Series approach failed to run with the implementations in R. 
The best result from each method is listed in Table \ref{table:cup} along with the corresponding parameters.


We observe that the S-Lspline method gives outstanding performance on this real data, outperforming the kNN regressor, while the other skeleton-based methods also demonstrate good performance. 
The lightening conditions of this series of images do not vary much by the rotation angle, which poses challenges to the similarity calculations based on the Euclidean distance and hence limits the performance of the classical kNN method. 
% We also note that the S-Cspline method gives an extraordinarily bad performance and indicates extra caution is needed for higher-order splines.
Note that the S-Kernel and S-kNN methods depend on the skeleton-based distances between data points while the S-Lspline methods do not, and hence the difference between such methods may imply that, although the skeleton graph can capture the data structure which leads to the good performance of the S-Lspline method, the skeleton-based distances can give inaccurate relations between data points compared to the true underlying data structure.
However, the skeleton graph still provides information about the data structure as the S-Lspline method has good performance with the simple piecewise linear model assumption on the skeleton.
% Particularly, the projection of this large-dimensional data onto the skeleton can be unstable, leading to diminished performance of methods utilizing the skeleton-based distances.
% In Appendix \ref{sec::extraReal}, we provide some additional empirical results on images from the COIL-20 data.




%------------------------------------------------------
\subsection{SDSS Data} \label{sec::sdss}

~~~~In this section, we applied the skeleton regression to a galaxy sample of size $5000$, taken from a random subsample of the Sloan Digital Sky Survey (SDSS), data release 12 \citep{york2000sloan, alam2015eleventh}. We repeat the random data subsampling for 100 times to get 100 different datasets. One dataset consists of $5$ covariates measuring apparent magnitudes of galaxies from images taken using $5$ photometric filters. These covariates can be understood as the color of a galaxy and are inexpensive to obtain. The response variable is the spectroscopic redshift, which is a very costly but accurate measurement of the distance
to the Earth. %but it provides information on the actual distance from a galaxy to the Earth.
It is known that the $5$ photometric color measurements are correlated with
the spectroscopic redshift. So the goal is to use the photometric
information to predict the redshift; this is known as 
the clustering redshift problem in Astronomy literature \citep{morrison2017wizz, rahman2015clustering}.

We construct the skeleton with the same method in the simulation studies. The resulting skeleton graph is shown in Figure \ref{fig::SDSScolor}. 
In the left panel of Figure \ref{fig::SDSScolor}, we color the knots by their predicted redshift values according to the S-Lspline method and color the edges by the average predicted values of the two connected knots. 
For comparison, we color the knots and edges using the true values in the right panel of Figure \ref{fig::SDSScolor}. 
The predictions given by S-Lspline are very close to the true values. 

\begin{figure}
\centering
    \begin{subfigure}[t]{0.38\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/SDSS5000_Lspline.jpeg} 
        \caption{S-Lspline}
    \end{subfigure}
        \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/SDSS5000_True.jpeg}
        \caption{True Value}
    \end{subfigure}
\captionof{figure}{SDSS Skeleton Colored by values predicted by S-Lspline (left) and by true values (right).}
\label{fig::SDSScolor}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%
\centering
\begin{tabular}{c l c l} 
 \hline
 Method & SSE & nknots &  Parameter \\ [0.5ex] 
 \hline
 kNN & 58.6 ( 46.7, 79.1) &  &neighbor=6 \\ 
 Ridge &868.4 (771.8, 984.5) & & $\lam = 0.001 $\\
 Lasso & 861.7 (750.3, 993.8) & & $\lam = 0.0013 $\\
 SpecSeries& 73.0 (54.1, 114.0) & &  bandwidth = 5\\
 Fast-KRR & 312.3 (242.5, 396.9) & & $\sigma = 0.1$\\
 S-Kernel & 78.6 (71.5, 92.4) &  126 &  bandwidth = $4 r_{hns}$ \\
 S-kNN& 83.1 (73.9, 98.7) & 126 & neighbor = 9\\
 S-Lspline &  75.9 (69.0, 90.4) & 126 & $\lambda$ = 0 \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on SDSS data. The best SSE from each method is listed with the corresponding parameters used. The 5 percentile and 95 percentile of the SSEs from the 100 runs are reported in brackets.}
\label{table:SDSS}
\end{figure}

For completeness, we We perform the same analysis as in Section \ref{sec::simulation} by comparing the 5-fold cross-validation SSEs of different regression methods on this dataset and include results in Table \ref{table:SDSS}. 
The classical kNN shows superior performance on this dataset, which can imply that the kNN method adapts nicely to the complex structure of the data. 
Notably, the Spectral Series regression shows good performance in this low-dimensional setting.
However, note that the Spectral Series regression has a large variation in its performance ranging over the different subsampled datasets, with SSEs a 5 percentile of 54.1 to 95 percentile of 114.0. 
Overall, kNN and SpecSeries methods work well in this data and both methods can adapt to the underlying manifold, while the skeleton-based regression methods also show comparable results.
The Fast-KRR approach demonstrates performance better than the usual Ridge and Lasso regression, demonstrating the effectiveness of kernel tricks in this setting.
While skeleton approaches do not provide the best prediction accuracy,
the skeleton structure obtained in Figure~\ref{fig::SDSScolor}
shows a clear one-dimensional structure in the underlying covariate distribution and an approximate monotone trend in the response. 
Thus, even if our method does not provide the best prediction accuracy, the skeleton itself can be used as a tool to investigate the structure of the covariate distribution, which can be valuable for practitioners.



% Despite the fact that our skeleton-based methods do not show superior performance on this particular dataset, the skeleton representation does reveal the manifold structure in the covariate space and an approximate monotone trend in the response. 
% The clean manifold structure of the data and the small number of covariates may explain the superior performance of kNN and SpecSeries in this case.


%e helpful for further scientific investigations.



%-------------------------------------------------------------------------
\section{Conclusion}	
\label{sec::conclusion}

~~~~In this work, we introduce the skeleton regression framework to handle regression problems with manifold-structured inputs. We generalize the nonparametric regression techniques such as kernel smoothing and splines onto graphs. 
Our methods provide accurate and reliable prediction performance and are capable of recovering the underlying manifold structure of the data.
Both theoretical and empirical analyses are provided to illustrate the effectiveness of the skeleton regression procedures.

In what follows, we describe some possible future directions:
\begin{itemize}

\item {\bf Generalizing skeleton graphs to a simplicial complex.}
From a geometric perspective, the skeleton graph constructed in this work only focuses on $0$-simplices (points) and $1$-simplices (line segments). Additional geometric information can be encoded using higher-dimensional simplices. Recent research in deep learning has explored the use of simplicial complices for tasks such as clustering and segmentation \citep{GeometricDL, MPSN2021}. Higher-dimensional simplicies
offer a finer approximation to the covariate distribution
but have a higher computational cost and a more complex model.
Thus, it is unclear if using a higher-dimensional simplex 
will lead to better prediction accuracy.
We will explore
the possibility of extending skeleton graphs to the skeleton complex in the future. 
%in this work to include simplicial sets, which are natural higher-dimensional generalizations of directed graphs.

\item {\bf Nonparametric smoothers on graphs.} The kernel regression and spline regression
are not the only possibilities for performing nonparametric smoothing on graphs.
%Previous works have applied nonparametric regression estimators to graphs. 
For example, \citet{Wang2016} generalized the concept of trend filtering \citep{Kim2009, Tibshirani2014} to graphs and compared it to Laplacian smoothing and Wavelet smoothing. In contrast to our work, these regression estimators for graphs are applied to data where both the inputs and responses are located on the vertices of a given graph. As a result, these graph smoothers, which include different regularizations, can only fit values on the vertices and do not model the regression function on the edges (\citet{Wang2016} mentioned the possibility of linear interpolation with trend filtering).

It is possible to generalize these methods to the skeleton by constructing responses on the knots in the skeleton graph as the mean values of the corresponding Voronoi cell, and then graph smoothers can be applied. Some interpolation methods can again be used to predict the responses on the edge, and this can lead to another skeleton-based regression estimator.

% Laplacian smoothing:
% \begin{align*}
%     \min_{\be \in \RR^n} ||y - \be||_2^2 + \lam \be^T L \be
% \end{align*}
% The usual spline regressions can all be framed as Laplacian smoothing problem.

% Wavelet smoothing:
% \begin{align*}
%     \min_{\theta \in \RR^n} ||y - W \theta||^2 + \lam ||\theta||_1
% \end{align*}
% where $W$ is wavelet basis over the graph

% The $k$the order graph trend filtering (GTF) estimate is given by
% \begin{align*}
%     \hat{\be} = \argmin_{\be \in \RR^n} \frac{1}{2} ||y - \be||_2^2 + \lam ||\Delta^{(k+1)} \be||_1
% \end{align*}
% where $\Delta^{(k+1)}$ is graph difference operator of order $k+1$. Defined that $\Delta^{(1)}$ is the oriented incidence matrix of the graph that 
% \begin{align*}
% \begin{matrix}
% \Delta^{(1)}_{\ell} = &
%         &(0,\ldots,0,&1,&0,\ldots,0,&-1,&0,\dots,0)\\
%         &&&\uparrow&     &\uparrow& \\
%         &&&i&            &j& \\
% \end{matrix}
% \end{align*}
% where the $\ell$-th edge in graph $G$, with orientation, goes from node $i$ to $j$. Then for higher order $k\geq 0$ we have
% \begin{align*}
%       \Delta^{(k+1)} =
%     \begin{cases}
%       (\Delta^{(1)})^T \Delta^{(k)} = L^{k+1/2} & \text{for odd $k$}\\
%       \Delta^{(1)} \Delta^{(k)} = D L^{k/2} & \text{for even $k$}
%     \end{cases}    
% \end{align*}
% where $D = \Delta^{(1)}$ and note that the graph Laplacian matrix $L = D^T D$

% In comparison, the $k$th order graph Laplacian smoothing has penalty term
% \begin{align*}
%     \lam \be^T L^{k+1} \be = \Vert \Delta^{(k+1)} \be \Vert_{2}^2
% \end{align*}
% and hence the difference is the choice of $\ell_1$ or $\ell_2$ norm. Trend filtering on graphs has a level of local adaptivity unmatched by the usual graph smoothers.


\item {\bf Time-varying covariates and responses.} A possible avenue for future research is to extend the skeleton regression framework to handle time-varying covariates and responses. Specifically, covariates collected at different times could be used together to construct knots in a skeleton. The edges in the skeleton can change dynamically according to the covariate distribution at different times, providing insight into how the covariate distributions have evolved. Additionally, representing the regression function on the skeleton would make it simple to visualize how the function changes over time.

\item {\bf Streaming data and online skeleton update.}
As streaming data becomes increasingly common, a potential area of future research is to investigate methods for updating the skeleton structure and its regression function in a real-time or online fashion. Reconstructing the entire skeleton can be computationally costly, but local updates to edges and knots can be more efficient. We plan to explore ways to develop a simple yet reliable method for updating the skeleton in the future.

%tasks are in the online learning setting that data is fed in sequentially and the predictor needs to be updated at each step. As skeleton can be computational costly to construct (see Section \ref{sec::complexity}), future research can focus on efficient ways to update the skeleton in the online learning setting and hence update the skeleton-based regressors in a fast manner.

\end{itemize}



\newpage
\begin{center}
    {\huge Appendices}
\end{center}
\begin{appendix}

% \addcontentsline{toc}{section}{Appendices}
% \renewcommand{\thesubsection}{\Alph{subsection}}

\section{Skeleton Construction with Voronoi Density}
\label{ref::skelconsVoron}
~~~~
In this section, we provide a more detailed description of the procedures for constructing the skeleton and computing the density-aided edge weight called the Voronoi density, following the work in \cite{skelclus}.

\subsection{Knots Construction}	
\label{sec::knots}
~~~~
The knots in the skeleton serve as reference points within the data, allowing us to focus our attention from the overall data to these specific locations of interest. 
We utilize the $k$-means algorithm with a relatively large value of a number of knots $k$ to create these knots in a data-driven way. 
The number of knots is a crucial parameter in this procedure as it governs the trade-off between the summarizing power of the representation and the preservation of information. 
Empirical evidence from \cite{skelclus} suggests that setting $k$ to around $\sqrt{n}$ can be a helpful reference rule, while the dimensionality of the data should be taken into consideration when choosing $k$.

In practice, since the $k$-means algorithm may not always find the global optimum, we repeat it $1,000$ times with random initial points and select the result corresponding to the optimal objective. 
We also advise pruning knots with only a small number of with-in-cluster observations. Additionally, it can be helpful to preprocess or denoise the data by removing observations in low-density areas to address issues that could arise for $k$-means clustering.


\subsection{Edges Construction}	\label{sec::edge}
~~~~
We denote the given knots as $c_1,\cdots, c_k$ and represent their collection as $\calC = {c_1,\cdots, c_k}$. An edge is added between two knots if they are neighbors, which is determined by whether their corresponding Voronoi cells share a common boundary. The Voronoi cell associated with a knot $c_j$ is defined as the set of points in $\calX$ whose distance to $c_j$ is the smallest among all knots. 
That is, 
\begin{align}
    \CC_j = \{x \in \calX: d(x, c_j) \leq d(x, c_\ell) \ \  \forall \ell \neq j\},
\end{align}
where $d(x,y)$ is the usual Euclidean distance.
We add an edge between knots $(c_i,c_j)$ if their Voronoi cells have a non-empty intersection. This graph is referred to as the Delaunay triangulation of $\calC$, denoted as $DT(\calC)$. 

Although the Delaunay triangulation graph is conceptually intuitive, the computational complexity of the exact Delaunay triangulation algorithm has an exponential dependence on the ambient dimension $d$, making it unfavorable for multivariate or high-dimensional data settings. To overcome this issue, we approximate the Delaunay triangulation with $\hat{DT}(\calC)$ by examining the 2-nearest knots of the sample data points. We query the two nearest knots for each data point and add an edge between $c_i, c_j$ if there is at least one data point whose two nearest neighbors are $c_i, c_j$. The computational complexity of this sample-based approximation depends linearly on the dimension $d$, making it suitable for high-dimensional settings.


\subsection{Voronoi Density}	\label{sec::VD}

~~~~The Voronoi density (VD) measures the similarity between a pair of knots $(c_j,c_\ell)$ based on the number of observations whose 2-nearest knots are $c_j$ and $c_\ell$. We first define the Voronoi density based on the underlying probability measure and then introduce its sample analog. Given a metric $d$ on $\RR^d$, the 2-Nearest-Neighbor (2-NN) region of a pair of knots $(c_j,c_\ell)$ is defined in Equation \ref{eq::2NNregion} as
\begin{align*}
    B_{j\ell} = \{ X_m, m=1,\dots,n :\norm{x- V_i} > \max\{ \norm{x-V_j}, \norm{x- V_\ell} \}, \forall i \neq j, \ell \}.
\end{align*}
Figure~\ref{fig::2nn} provides an illustration of an example 2-NN region of a pair of knots.
If two knots $c_j, c_\ell$ are in a connected high-density region, then we expect the 2-NN region of $c_j, c_\ell$ to have a high probability measure. Therefore, the probability $\PP(B_{j\ell}) = P(X_1 \in B_{j\ell})$ can measure the association between $c_j$ and $c_\ell$. Based on this insight, the Voronoi density measures the edge weight of $(c_j,c_\ell)$ as
\begin{align}
S_{j\ell}^{VD} = \frac{\PP(B_{j\ell})}{|c_j - c_\ell |}.
\end{align}
The Voronoi density adjusts for the fact that 2-NN regions have different sizes by dividing the probability of the in-between region by the mutual Euclidean distance. 

In practice, we estimate $S_{j\ell}^{VD}$ by a sample average. The numerator $\PP(B_{j\ell}) $ is estimated by $\hat{P}_n (B_{j\ell}) = \frac{1}{n}\sum_{i=1}^n I(X_i\in B_{j\ell})$, and the final estimator for the VD is:

\begin{align}
\hat{S}_{j\ell}^{VD} &= \frac{\hat{P}_n ({B}_{j\ell})}{|{c}_j - {c}_\ell|}.
\end{align}

Calculating the Voronoi density is fast. The numerator, which only depends on 2-nearest-neighbors calculation, can be computed efficiently by the k-d tree algorithm. For high-dimensional space, space partitioning search approaches like the k-d tree can be inefficient, but a direct linear search still gives a short run-time.


\subsection{Graph Segmentation}	\label{sec::segmenting}


~~~~After obtaining the weighted skeleton graph, it can be helpful to prune certain edges that are not of interest or segment the skeleton into disconnected components. 
The edge weights defined above can be utilized to achieve this. 
We start by first converting the edge weights into dissimilarity measures. Specifically, let ${s_{ij}}{i\neq j}$ be the edge weights, where only connected pairs can take non-zero entries, and let $s{\max} = \max_{i \neq j} s_{ij}$.
We then define the corresponding dissimilarities as $d_{ij} = 0$ if $i = j$, and $d_{ij} = s_{\max} - s_{ij}$ otherwise. 
Next, we apply hierarchical clustering using these distances. The choice of linkage criterion for hierarchical clustering depends on the underlying geometric structure of the data. Single linkage is recommended when the components are well-separated, while average linkage works better when there are overlapping clusters of approximately spherical shapes. 
To determine the resulting segmented skeleton graph, dendrograms can be useful in displaying the clustering structure at different resolutions, and analysts can experiment with different numbers of final clusters and choose a cut that preserves meaningful structures based on the dendrograms. However, it is important to note that the presence of noisy data points may require a larger number of final clusters $S$ to achieve better clustering results. 

\section{Computational Complexity}	\label{sec::complexity}
~~~~In this section, we briefly analyze the computational costs of the proposed skeleton regression framework. 
The first main computational burden of the proposed regression procedure is at the skeleton construction step. \citet{skelclus} has provided the computational analysis on this. In particular, when constructing knots, the $k$-means algorithm of Hartigan and Wong \cite{Hartigan1979} has time complexity $O(ndkI)$, where $n$ is the number of points, $d$ is the dimension of the data, $k$ is the number of clusters for $k$-means, and $I$ is the number of iterations needed for convergence. For the edge construction step, the approximate Delaunay Triangulation only depends on the 2-NN neighborhoods,  and the k-d tree algorithm for the 2-nearest knot search gives the worst-case complexity of $O(nd k^{(1-1/d)})$.  For the edge weights with Voronoi density, the numerator can be computed directly from the $2$-NN search without additional computation, and the denominators as pairwise distances between knots can be computed with the worst-case complexity of $O(dk^2)$.

Given the skeleton, we then project original feature vectors onto the skeleton, which is not very time-consuming. Finding the edge to project on depends on identifying the two nearest knots, which is provided in the skeleton construction step. The projection takes inner product computations and takes $O(nd)$ for all the covariates.

The next computational task is to calculate the skeleton-based distance between points on the skeleton. 
Note that this step is not needed for the S-Lspline method but is necessary for S-Kernel and S-kNN.
To find the shortest path on a graph between two faraway knots, the general version of Dijkstra's algorithm \cite{Dijkstra1959} takes $ \Theta (|\calE|+|\calV|^{2})=\Theta (k^{2})$ for each run. However, in practice, we don't need the $\frac{n(n-1)}{2}$ pairwise distances between all the projected points as the skeleton-based regressors proposed can perform with distances in local neighborhoods, which do not require path-finding algorithm for the skeleton-distance calculation.

With all the pairwise skeleton-based distances between projected feature points given, the S-kernel estimate at one point takes $n_{loc}$ kernel weights computation where $n_{loc}$ refers to the local support of the kernel function. S-Lspline takes $O(n)$ time to transform the data and then a single run of matrix multiplication and inversion to get the coefficients.

\section{Proofs}	\label{sec::proof}



% \YC{You have to rewrite this proof entirely.
% This is not a proof of the two theorems. }

\subsection{Kernel Regression: Convergence on Edge Point (Theorem \ref{thm::edge})} \label{sec::contproof}

% A dimension $d$ result is provided, but note that in the skeleton regression framework, we have $d=1$ when applying kernel regression on the skeleton.

% {\color{magenta}The point $x$ is ill-defined here!
% The theorem statement use $s$ and $S_i$.
% You need to write the proof in terms of the metric space rather
% than the usual 1D case.

% A useful tip:
% We are in the asymptotic regime that $h\rightarrow0 $
% and assume that we use a compact support kernel. 
% In this case, for any point $s$ in an edge $e$, 
% any observations $S_j\neq e$ will have 
% $K(d(s,S_j)/h) = 0$.

% You also need to define $\hat g(x)$.
% }

\begin{proof}
Let $\calB(\bs, h) \subset \calS$ be the support for the kernel function $K_h(.)$ at point $\bs \in \calS$ with bandwidth $h$.
For an edge point $\bs \in E_{j\ell} \in \calE$, where $\calE$ is the overall set of edges defined as open sets. As $n\to \infty, h\to 0$, for sufficiently large $n$, by the property of an open set, we have
$$\calB(\bs, h) \subset E_{j\ell} $$ 
and by our definition of skeleton distance, for two points $\bs, \bs' \in E_{j\ell}$ on the same edge in the skeleton, $d_\calS(\bs, \bs') = \norm{\bs-\bs'}$ where $\norm{.}$ denotes the Euclidean distance and is 1-dimensional as parametrized on the same edge. Also we have $$K_h(\bs_j, \bs_\ell) \equiv  K(d_\calS(\bs_j, \bs_\ell)/h) = K(\norm{\bs_j-\bs_\ell}/h) = K\left(\frac{\bs_j-\bs_\ell}{h}\right)$$

Consequently, the skeleton-based kernel regression estimator reduces to 
\begin{align}
    \hat{m}_n(\bs) = \frac{\frac{1}{n h}\sum_{j=1}^n Y_j K(\frac{\bs_j- \bs}{h})  }{\frac{1}{n h}\sum_{j=1}^n K(\frac{\bs_j- \bs}{h})}
\end{align}
and we can use the classical asymptotic results for kernel regression in the continuous case \cite{Bierens1983, wasserman2006all,Chen2017}.

% \JW{The edge points that do not have all the support of kernel on the same edge can be classified into two types. The first type is those near a knot with degree 1. Those are the end knots, and by the construction of the skeleton, there is a non-vanishing share of data points that are projected onto those end knots, and the bias hence is also non-vanishing. We may only be able to admit this case.\\
% Another type of problematic edge point are near knots with 2 or more degrees. The kernel support of those points includes the knot, which has a probability $0$ share of original covariates projected onto and hence can be ignored, and the edge points from the other connected edges. Note the maximum degree into the skeleton to be $m$, which is fixed as the skeleton structure is given, then $\PP(\bs_j \in \calV \cap  \calB(\bs, h)) = O(h)$, and we have 
% \begin{align*}
%     \hat{m}(\bs) = \frac{\Tilde{m}(\bs) \hat{g}(\bs) + O(h)}{\hat{g}(\bs) + O(h)} = \frac{\Tilde{m}(\bs) \hat{g}(\bs) }{\hat{g}(\bs)}+ O(h)
% \end{align*}
% where $\Tilde{m}(\bs)$ is the reduced form above. Not sure whether including this case is helpful or not.
% \begin{align}
% \begin{split}
%     \hat{m}(\bs) &= \frac{\sum_{j=1}^n Y_j  K_h(\bs_j, \bs) }{\sum_{j=1}^n K_h(\bs_j, \bs)} \\
%     &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calE)  + \frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calV )}{\frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs)I(\bs_j \in \calE ) + \frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs) I(\bs_j \in \calV ) } \\
%     &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h))}{\frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h)) }
% \end{split}
% \end{align}
% }

% $\Tilde{K_{h}}(\bs_j, \bs) = K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h))$  

Let $\hat{g}_n(\bs) = \frac{1}{nh}\sum_{j=1}^n K\left(\frac{\bs_j-\bs}{h}\right)$. 
We express the difference as
\begin{equation}
\begin{aligned}
     \hat{m}_n(\bs) - m_\calS(\bs) &= \frac{[\hat{m}_n(\bs) - m_\calS(\bs)] \hat{g}_n(\bs)}{\hat{g}_n(\bs)} = \frac{\frac{1}{n h}\sum_{j=1}^n [Y_j - m_\calS(\bs)] K(\frac{\bs_j- \bs}{h})  }{\frac{1}{n h}\sum_{j=1}^n K(\frac{\bs_j-\bs}{h})}
\end{aligned}
\label{eq::decom}
\end{equation}
and we analyze the denominator and numerator below.

Let $g(\bs)$ be the density at point $\bs$ on the skeleton.
For the denominator, we start with the bias:
\begin{align*}
    \abs{\EE \hat{g}_n(\bs) - g(\bs)} &= \abs{\frac{1}{h} \int K\bigg(\frac{\bs - y}{h}\bigg) g(y) d y - g(x) \int K(y) dy}\\
    &= \abs{\int K(z) [g(\bs - h z) - g(\bs)] dz}\\
    &\leq \int K(z) C_1 \abs{h z} dz = C_1 h \int K(z)  \abs{ z} dz = O(h),
\end{align*}
where $C_1$ is the Lipschitz constant of the density function.
%where we used $\int K(z) dz = 1$, the Lipschitz constant $C_1$ of the density function, and that $\int K(z)  \abs{ z} dz$ is bounded. 
For the variance, we have
\begin{align*}
    {\text Var}\left( \hat{g}_n(\bs) \right) &\leq \frac{1}{n h^2} \int K^2\bigg(\frac{\bs - y}{h}\bigg)g(y) dy\\
    &= \frac{1}{n h} \int K^2(z) g(\bs - h z) dz\\
    &\leq \frac{1}{n h} \int K^2(z) [g(\bs) + C_1 \abs{h z} ] dz\\
    &= \frac{1}{n h}\left[ g(\bs) \int K^2(z) dz + C_1 h \int K^2(z) \abs{ z}  dz \right]\\
    &= \frac{1}{n h} g(\bs) \int K^2(z) dz  + o\left(\frac{1}{n h}\right).
\end{align*}

Putting it all together, we have
\begin{align*}
    \abs{\hat{g}_n(\bs) - g(\bs)} = O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right).
\end{align*}
Note that we only assume Lipschitz continuity and hence have the bias of rate $O(h)$ rather than the usual $O(h^2)$ rate with second-order smoothness. 
Higher-order smoothness of $g$ may not improve the overall
estimation rate due to the fact that
we only have Lipschitz continuity of the regression function.
%The reason is that, with Lipschitz continuity of the regression function, requiring more continuity on the density does not improve the overall estimation error rate.


% \begin{align*}
%     \EE \hat{g}(\bs) &= h^{-1} \int K\left(\frac{\bs - z}{h}\right)g(z) dz  = \int K\left(z'\right) g(\bs - h z') g(z')\\
%     &\to g(\bs) \int K(z) d z = g(\bs)
% \end{align*}
% where for the second equality we use the change of variable setting $z' = \frac{\bs - z}{h}$ and the limit holds by the Bounded Convergence Theorem.

Now we analyze the numerator of equation \eqref{eq::decom}.
We start with the decomposition
\begin{align*}
    &[\hat{m}_n(\bs) - m_\calS(\bs)] \hat{g}(\bs) =  \underbrace{\frac{1}{n h} \sum_{j=1}^n U_j K\bigg(\frac{\bs - \bs_j}{h}\bigg)}_{q_1(\bs)} +  \\
    & \underbrace{\frac{1}{n} \sum_{j=1}^n \bigg\{ [m_\calS(\bs_j) - m(\bs)]K\bigg(\frac{\bs - \bs_j}{h}\bigg) \frac{1}{h} - \EE\bigg[ [m_\calS(\bs_j) - m_\calS(\bs)]K\bigg(\frac{\bs - \bs_j}{h}\bigg) \frac{1}{h} \bigg]  \bigg\} }_{q_2(\bs)} +\\
    &  \underbrace{ \frac{1}{n} \sum_{j=1}^n \EE\bigg[ [m_\calS(\bs_j) - m_\calS(\bs)]K\bigg(\frac{\bs - \bs_j}{h}\bigg) \frac{1}{h} \bigg]}_{q_3(\bs)}.
\end{align*}


First, we show that
\begin{align*}
    q_1(\bs)  =  O_p\left(\sqrt{\frac{1}{n h} } \right).
\end{align*}
% \begin{align*}
%     \sqrt{n h} q_1(\bs) \to_d N(0, \sig_u^2(\bs) g(\bs) \int K(z)^2 dz )
% \end{align*}
Let
\begin{align*}
    v_{n,j} (\bs) = U_j K\bigg(\frac{\bs - \bs_j}{h}\bigg)\frac{1}{\sqrt{h}}
\end{align*}
and we have
\begin{align*}
     \sqrt{n h} q_1(\bs) = \frac{1}{\sqrt{n}} \sum_{j=1}^n v_{n,j}(\bs).
\end{align*}
Thus, its mean is
\begin{align*}
    \EE v_{n,j}(\bs) &= \EE\left\{ U_j K\bigg(\frac{\bs - \bs_j}{h}\bigg)\frac{1}{\sqrt{h}} \right\}
    = 0
\end{align*}
and the variance is
\begin{align*}
    \EE [v_{n,j}(\bs)^2] &= \EE U_j^2 K\bigg(\frac{\bs - \bs_j}{h}\bigg)^2\frac{1}{h} = \int \sig_u^2(\bs - h z) g(\bs - h z) K(z)^2 dz\\
    &\to \sig_u^2(\bs) g(\bs) \int K(z)^2 dz = O(1),
\end{align*}
where for the second equality we use the change of variable and by assumption, we have $\int K(z)^2 dz <\infty$. Therefore, 
\begin{align*}
    q_1(\bs)  =  O_p\left(\sqrt{\frac{1}{n h }} \right).
\end{align*}

For the second term, note that $\EE (q_2(\bs)) = 0$ and the variance is
\begin{align*}
    \EE \left[\sqrt{n h} q_2(\bs)\right]^2 &= \int [m_\calS(\bs - h z) - m_\calS(\bs)]^2 g(\bs - h z) K(z)^2 dz \\
    &\ \ - h \bigg\{ \int [m_\calS(\bs - h z) - m_\calS(\bs)] g(\bs - h z) K(z) dz  \bigg\}^2\\
    &\to 0
\end{align*}
when $h\rightarrow0$, and hence,
\begin{align*}
    q_2(\bs)  =  o_p\left(\sqrt{\frac{1}{n h} } \right).
\end{align*}


% {\color{magenta}We only need to get the convergence rate.
% There is no need to use the central limit theorem.
% One of the simplest ways to get the rate
% is to compute the asymptotic variance.}

For the last term, note that we have 
\begin{align*}
    q_3(\bs) &= \int[m_\calS(\bs - h z) - m_\calS(\bs)]g(\bs - h z) K(z)  dz \\
    &=\int [ m_\calS(\bs - h z)g(\bs - h z) - m_\calS(\bs) g(\bs) ] K(z) dz\\
    & \ \ \ \   - m_\calS(\bs) \int [g(\bs - h z) - g(\bs) ] K(z) dz\\
    &\leq C_1 h \int |z| K(z)dz + C_2 h \int |z| K(z) dz
\end{align*}
where $C_1$ is the Lipschitz constant for $m(\bs)g(\bs)$ and $C_2$ is the Lipschitz constant for $g(\bs)$. Therefore, 
\begin{align*}
     q_3(\bs) = O(h)
\end{align*}

Putting all three terms together, $[\hat{m}(\bs) - m(\bs)] \hat{g}(\bs)  = O(h) + O_p\left(\sqrt{\frac{1}{n h} } \right)$. 
As a result, equation \eqref{eq::decom} becomes
\begin{align*}
    \hat{m}_n(\bs) - m_\calS(\bs) &= \frac{[\hat{m}_n(\bs) - m_\calS(\bs)] \hat{g}(\bs)}{\hat{g}(\bs)} = \frac{O(h) + O_p\left(\sqrt{\frac{1}{n h} } \right)}{ g(\bs) + O(h) + O_p\left(\sqrt{\frac{1}{n h} } \right)}\\
    &= O(h) + O_p\left(\sqrt{\frac{1}{n h} } \right)
\end{align*}
by the Taylor expansion of the fraction. 

% Note that, with second-order smoothness of the density function and regression function, we can have the bias to be of rate $O(h^2)$.
\end{proof}




\subsection{Kernel Regression: Convergence on Knot with Zero Mass (Proposition \ref{prop::zeroknot})}
\label{sec::zeroknotproof}

% \YC{I do not change $h$ to $h$; plz do it before submission.}


For the ease of proof, we first prove
Proposition \ref{prop::zeroknot}
and then prove Theorem \ref{thm::knotconsistency}.

\begin{proof}
Let $\bs \in \calV$ be a knot with no mass, i.e.,
$P(\bs_j = \bs) = 0$. 
The kernel regression can be decomposed
as
\begin{align*}
    \hat{m}(\bs)
     % &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h))}{\frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h)) }\\
    &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j  I(\bs_j =\bs)}{\frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n  I(\bs_j =\bs) }\\
    &= \frac{\eps_{1,n}(\bs) + \nu_{1,n}(\bs) }{\eps_{2,n}(\bs) + \nu_{2,n}(\bs) }.
\end{align*}

Because $\bs$ is a point without probability mass,
$\nu_{1,n}(\bs)=\nu_{2,n}(\bs) =0$,
so the above can be further reduced to
\begin{align*}
    \hat{m}(\bs)
     &= \frac{\frac{1}{n h}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  }{\frac{1}{n h}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h))}.
\end{align*}

However, different from the case on edges, the support of the kernel intersects with multiple edges even when $h\rightarrow0$, so we study the contribution of each edge individually. 
Note that when $h\rightarrow0$, the only knot that exists in the intersection $\calB(\bs, h)\cap \calE$ is $\bs$.
So we only need to consider contributions of edges 
adjacent to $\bs$.

Let $\calI$ collect all the edge indices with one knot being $\bs$, i.e., $\ell \in \calI$ implies that there is an edge
between $\bs$ and $\bv_\ell \in \calV$.
Let $E_\ell$ be the edge connecting $\bs$ and $\bv_\ell$.
The indicator function $I(\bs_j \in \calE \cap  \calB(\bs, h)) = \sum_{\ell\in\calI}I(\bs_j \in E_\ell \cap  \calB(\bs, h)).$
With this, we can rewrite $\hat m(\bs)$ as 
\begin{equation*}
\begin{aligned}
    \hat m(\bs) &= \frac{ \sum_{\ell\in  \calI} \frac{1}{n h}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in E_\ell \cap  \calB(\bs, h))  }{\sum_{\ell\in \calI}\frac{1}{n h}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in E_\ell \cap  \calB(\bs, h))}\\
    & = \frac{\sum_{\ell\in \calI} \hat m_{n,\ell}(\bs) \hat g_{n,\ell}(\bs)}{\sum_{\ell\in\calI} \hat g_{n,\ell}(\bs)}.
    \end{aligned}
\end{equation*}
where
\begin{align*}
    \hat{g}_{n, \ell}(\bs) &= \frac{1}{n h}\sum_{j=1}^n K\left(\frac{\bs_j-\bs}{h}\right)I(\bs_j \in E_\ell \cap  \calB(\bs, h)),\\
\hat{m}_{n,\ell}(\bs) \cdot \hat{g}_{n, \ell}(\bs) &= \frac{1}{n h}\sum_{j=1}^n Y_j K\left(\frac{\bs_j-\bs}{h}\right) I(\bs_j \in E_\ell \cap  \calB(\bs, h)).
\end{align*}
Thus, we will analyze $\hat g_{n,\ell}(\bs)$
and $\hat m_{n,\ell}(\bs) \hat g_{n,\ell}(\bs)$. 
%Both cases are similar to the case on edges
%except that the point of evaluation is on the boundary. 
%So we have 
%\begin{align*}
%g_{n,\ell}(\bs)& = \frac{1}{n h}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in E_\ell \cap  \calB(\bs, h))\\
%& = \frac{1}{n h}\sum_{j=1}^n K\left(\frac{\bs_j-\bs}{h}\right)I(\bs_j \in E_\ell \cap  \calB(\bs, h))\\
%& = \frac{1}{2}g_\ell(0) + O(h) + O_P\left(\sqrt{\frac{1}{nh}}\right),
%\end{align*}
%where $g_\ell(0) = \lim_{h\rightarrow0}\frac{1}{h} P(S_j \in E_\ell \cap  \calB(\bs, h))$.
%Note that the term $\frac{1}{2}g_\ell(0)$
%comes from the fact that $\bs$ is on the boundary
%of $E_\ell$,
%so for a symmetric kernel $K(.)$,
%the integral is only on one side, leading to
%a factor of $\frac{1}{2}$.
%Similarly, 
%\begin{align*}
%\hat m_{n,\ell}(\bs) \hat g_{n,\ell}(\bs)&=\frac{1}{n h}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in E_\ell \cap  \calB(\bs, h))\\
%& = \frac{1}{n h}\sum_{j=1}^n Y_j K\left(\frac{\bs_j-\bs}{h}\right)I(\bs_j \in E_\ell \cap  \calB(\bs, h))\\
%& = \frac{1}{2}m_\ell(0)g_\ell(0) + O(h) + O_P\left(\sqrt{\frac{1}{nh}}\right),
%\end{align*}
%where $m_\ell(0) =  \lim_{\bx \in E_\ell, \bx\rightarrow \bs} m(\bx)$.
%}
%\YC{You need to define $K_{h}(t,0)$ for a univariate $t$. }
%Let $E_\ell$ be an edge between knot $\bv_\ell$ and $\bs$ such that $E_\ell \cap  \calB(\bs, h))$ is not empty. 
For a point $\bs_j$ on the edge $E_\ell$, we can reparamterize it as $\bs_j =  T_j \bv_\ell  + (1-T_j) \bs$ for some  $T_j \in (0,1)$. 
The location $\bs$ corresponds to the case $T_j = 0$
and any $\bs_j\in E_\ell$ will be mapped to $T_j>0.$
With this reparameterization, 
we can write
\begin{align*}
    \hat{g}_{n, \ell}(\bs) 
    &= \frac{1}{n h}\sum_{j=1}^n K\left(\frac{T_j}{h}(\bv_\ell-\bs)\right)I(\bs_j \in E_\ell \cap  \calB(\bs, h)),\\
\hat{m}_{n,\ell}(\bs) \cdot \hat{g}_{n, \ell}(\bs) 
&= \frac{1}{n h}\sum_{j=1}^nY_j K\left(\frac{T_j}{h}(\bv_\ell-\bs)\right)I(\bs_j \in E_\ell \cap  \calB(\bs, h)).
\end{align*}

To study the limiting behavior when $h \rightarrow0$,
let $g_\ell(t) = g((1-t)\bs + t \bv_\ell)$, $g_\ell(0) = \lim_{x\downarrow 0} g_{\ell}(x)$;
$m_\ell(t) = m_\calS( (1-t)\bs + t \bv_\ell)$, $m_\ell(0) = \lim_{t \downarrow 0} m_\ell(t)  $;
and
$\sigma_\ell^2(t) = \EE(|U_j|^2 | \bs_j = (1-t) \bs + t \bv_\ell )$ , $\sigma_\ell^2(0) = \lim_{t \downarrow 0} \sigma_\ell^2(t) $.
% \YC{I added the following sentence to 
% explain how we get the integral.}
Then with the new notations, we can write 
\begin{align*}
\EE(f(T_j(\bv_\ell-\bs))I(\bs_j \in E_\ell \cap  \calB(\bs, h))) &= \EE(f(\bs_j-s)I(\bs_j \in E_\ell \cap  \calB(\bs, h)))\\
    &=\int_{t>0}f(t) g_\ell(t)dt
\end{align*}
for any integrable function $f$.
The bias of the denominator can be written as
\begin{align*}
    \abs{\EE \hat{g}_{n,\ell}(\bs) -\frac{1}{2} g_\ell(0)} &= \abs{\frac{1}{h} \int_{t>0} K\bigg(\frac{t}{h}\bigg) g_\ell(t) d t  - g_\ell(0) \int_{z>0} K(z)}\\
    &= \abs{\int_{z>0} K(z) [g_\ell(h z) -  g_\ell(0)] dz } \\
    &\leq \int_{z>0} K(z) C_1 h z  dz \\
    &=  C_1 h  \int_{z>0} K(z) z dz = O(h).
\end{align*}
For stochastic variation, we have
\begin{align*}
    {\text Var}\left( \hat{g}_{n,\ell}(\bs) \right) &\leq \frac{1}{n h^2} \int_{t>0} K^2\bigg(\frac{t}{h}\bigg)g_\ell(t) dt\\
    &= \frac{1}{n h} \int_{z>0} K^2(z) g(h z) dz\\
    &\leq \frac{1}{n h} \int_{z>0} K^2(z) [g(0) + C_1 \abs{h z} ] dz\\
    &= \frac{1}{n h}\left[ g(0) \int_{z>0} K^2(z) dz + C_1 h \int_{z>0} K^2(z) \abs{ z}  dz \right]\\
    &= O\left(\frac{1}{n h}\right).
\end{align*}
%Let $\calI$ collect all the edge indexes with one knot being $\bs$ and together we have
Thus, 
\begin{align*}
    \hat{g}_n(\bs) = \sum_{\ell \in \calI} \hat{g}_{n,\ell}(\bs) = \frac{1}{2} \sum_{\ell \in \calI}   g_\ell(0) + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right)
\end{align*}



For the numerator, 
\begin{align*}
    \hat{m}_{n,\ell}(\bs) \hat{g}_{n,\ell}(\bs) &=  \underbrace{\frac{1}{n h} \sum_{j=1}^n U_j K\bigg(\frac{t_j}{h}\bigg)I(\bs_j \in E_\ell \cap  \calB(\bs, h))}_{Q_1} \\
    &\qquad+  \underbrace{\frac{1}{n h} \sum_{j=1}^n   m_\calS(\bs_j) K\bigg(\frac{t_j}{h}\bigg)I(\bs_j \in E_\ell \cap  \calB(\bs, h))}_{Q_2},
\end{align*}
where $U_j  = Y_j - m_\calS(\bs_j)$.
% By model definition of $U_j$ we again have $\EE Q_1 = 0$. For the variation of $Q_1$,  
% % \YC{conditional independence?}
Using the fact that $\EE(U_j|\bs_j) = 0$,
$\EE(Q_1)=0$, and the variance is
\begin{align*}
    {\text Var}(Q_1) &\leq \frac{1}{n h^2 } \int_{t > 0} \sig_\ell^2(t) K^2\bigg(\frac{t}{h}\bigg) g_\ell(t) dt \\
    &= \frac{1}{n h } \int_{z > 0} \sig_\ell^2(h z) K^2(z) g_\ell(h z) dz\\
    &= \frac{1}{n h } \int_{z > 0} \sig_\ell^2(0) K^2(z) g_\ell(0) dz + O\left(\frac{1}{n h}\right) = O\left(\frac{1}{n h}\right) .
\end{align*}
For $Q_2$, we have
\begin{align*}
    \abs{\EE (Q_2) - \frac{m_\ell(0) g_\ell(0)}{2}  } &= \abs{\frac{1}{h}\int_{t>0} m_\ell( t) K(t/h) g(t)  dt - m_\ell(0) g_\ell(0) \int_{z>0}  K(z)  dz } \\
    &=\abs{\int_{z>0} m_\ell( h z) K(z) g_\ell(h z)  dz - m_\ell(0) g_\ell(0) \int_{z>0}  K(z)  dz } \\
    &\leq \int_{z>0} \bigg\{ \big[m_\ell(0)+ C_2 h z\big]  \big[g_\ell(0) + C_1 h z\big] - m_\ell(0) g_\ell(0)\bigg\} K(z)  dz \\
    &\leq [C_1 m_\ell(0)+C_2g_\ell(0)] h \int_{z>0}  K(z) z dz + o(h)=  O(h).
\end{align*}
The variance of $Q_2$ is bounded via
\begin{align*}
    {\text Var}(q_2) &\leq \frac{1}{n h^2 } \int_{t > 0} m_\ell^2(t) K^2\bigg(\frac{t}{h}\bigg) g_\ell(t) dt\\
    &= \frac{1}{n h} \int_{z > 0} m_\ell^2(h z) K^2(z) g_\ell(h z) dz\\
    &\leq \frac{1}{n h} \int_{z>0} \left\{m_\ell(0)+ C_2 \abs{h z}\right\}^2  K^2(z) \left\{g_\ell(0) + C_1 \abs{h z}\right\}  dz \\
    &= \frac{1}{n h} \left\{ m^2_\ell(0) g_\ell(0) \int_{z>0} z K^2(z)   dz  + O(h)\right\}\\
    &= O\left(\frac{1}{n h}\right) 
\end{align*}
Putting the terms $Q_1$ and $Q_2$ together, we have
\begin{align*}
    \hat{m}_{n,\ell}(\bs) \hat{g}_{n,\ell}(\bs) = \frac{1}{2} m_\ell(0) g_\ell(0) + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right).
\end{align*}
As a result, we conclude that
\begin{align*}
    \hat{m}(\bs) &= \frac{\sum_{\ell \in \calI}\hat{m}_{n,\ell}(\bs)  \hat{g}_{n,\ell}(\bs)}{\sum_{\ell \in \calI} \hat{g}_{n,\ell}(\bs) }\\
    &= \frac{ \frac{1}{2} \sum_{\ell \in \calI}   m_\ell(0) g_\ell(0) + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right) }{\frac{1}{2} \sum_{\ell \in \calI}   g_\ell(0) + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right)} \\
    &=  \frac{ \frac{1}{2} \sum_{\ell \in \calI}   m_\ell(0) g_\ell(0)  }{\frac{1}{2} \sum_{\ell \in \calI}   g_\ell(0) } + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right)\\
    &= \frac{ \sum_{\ell \in \calI}   m_\ell(0) g_\ell(0)  }{ \sum_{\ell \in \calI}   g_\ell(0) } + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right),
\end{align*}
which completes the proof.
\end{proof}



\subsection{Kernel Regression: Convergence on Knot with Nonzero Mass (Theorem \ref{thm::knotconsistency})}
\label{sec::knotproof}



\begin{proof}

Let $\bs \in \calV$ be a point where $P(\bs_j =\bs) = p(\bs)>0$.
Recall that the kernel regression can be expressed as
\begin{align*}
    \hat{m}(\bs)
     % &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h))}{\frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h)) }\\
    &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j  I(\bs_j =\bs)}{\frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n  I(\bs_j =\bs) }\\
    &= \frac{\eps_{1,n}(\bs) + \nu_{1,n}(\bs) }{\eps_{2,n}(\bs) + \nu_{2,n}(\bs) }.
\end{align*}

% {\color{magenta}Replace $x$ by $\bs$. Also, you need to define $p(\bs)$. Also need to explain why $\epsilon_{2,n}(\bs)\rightarrow 0$.}

% We have
% $\eps_{2,n}(\bs) \to 0$, $\nu_{2,n}(\bs) \to_p p(\bs)$, 
% and hence the denominator as a whole converge in probability to $p(\bs)$. 
% Similarly we can have the estimator as a whole converge in probability to $m(\bs) = M_\ell$ where $\bv_\ell = \bs$.


% In the numerator, for bias, the edge component will give $O(h)$ rate, but no bias from the discrete component.
% For stochastic variation, the edge component will give $O_p(\sqrt{\frac{1}{n h}})$ rate, and the discrete component will give $O_p(\sqrt{\frac{1}{n}})$.

% Note that here we have $\bs \in \calV$, and hence $\PP\left(\calE \cap  \calB(\bs, h) \right) \leq g_{\max} \times h = O(h)$ where $g_{\max}$ is the maximum density value on the skeleton. 
We look at each term individually and note that we have the edge components terms identical to the proof of  Proposition \ref{prop::zeroknot}, so 
\begin{align*}
    \eps_{1,n}(\bs) &= h \left\{ \sum_{\ell \in \calI}   m_\ell(0) g_\ell(0)   + O(h)+ O_p\left(\sqrt{\frac{1}{n h}}\right) \right\} =  O(h)+ O_p\left(\sqrt{\frac{h}{n }}\right), \\
    \eps_{2,n}(\bs) &= h \left\{ \sum_{\ell \in \calI}   g_\ell(0) + O(h)+ O_p\left(\sqrt{\frac{1}{n h}}\right) \right\} = O(h)+ O_p\left(\sqrt{\frac{h}{n }}\right).
\end{align*}

For the terms on the knots, they are just a sample average,
so
\begin{align*}
    \nu_{2,n}(\bs)  = p(\bs)+ O_p\left(\sqrt{\frac{1}{n}}\right)
\end{align*}
and similarly
\begin{align*}
        \nu_{1,n}(\bs) &= \frac{1}{n}\sum_{j=1}^n \left(m_\calS(\bs\right) + U_j)  I(\bs_j =\bs)\\
        &= m_\calS(\bs) p(\bs) + O_p\left(\sqrt{\frac{1}{n}}\right).
\end{align*}

With the fact that $O_p\left(\sqrt{\frac{1}{n }}\right)$ dominates $O_p\left(\sqrt{\frac{h}{n }}\right)$, 
we conclude
\begin{align*}
    \hat{m}(\bs) &= \frac{O(h)+ O_p\left(\sqrt{\frac{ h}{n}}\right) + m_\calS(\bs)p(\bs) + O_p\left(\sqrt{\frac{1}{n}}\right)}{ O(h)+ O_p\left(\sqrt{\frac{h}{n }}\right) + p(\bs) + O_p\left(\sqrt{\frac{1}{n}}\right)}\\
    &= \frac{O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right) }{ O(h)+ O_p\left(\sqrt{\frac{1}{n}}\right) + p(\bs) } + \frac{m_\calS(\bs)p(\bs) }{ O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right) + p(\bs) }\\
    &= \frac{O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right) }{ p(\bs) } + O\left[\left(\frac{O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right) }{ p(\bs) }\right)^2\right]\\
    &\ \ \ \ + m_\calS(\bs)p(\bs) \left\{ \frac{1}{p(\bs)} + \frac{O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right)}{ p(\bs)^2}  \right\}\\
    &= m_\calS(\bs) +  O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right),
\end{align*}
which completes the proof.

% we first look at the denominator that 
% \begin{align*}
%     \frac{1}{\eps_{2,n}(\bs) + \nu_{2,n}(\bs) } &= \frac{1}{ \nu_{2,n}(\bs) } - \frac{\eps_{2,n}(\bs)}{\nu_{2,n}(\bs)^2} + \frac{1}{\xi_{2,n}(\bs)^3} \eps_{2,n}(\bs)^2\\
%     & = \frac{1}{ \nu_{2,n}(\bs) } + \frac{O(h)+ O_p\left(\sqrt{\frac{1}{n h}}\right)}{O_p\left(\sqrt{\frac{1}{n}}\right)}
% \end{align*}
% for $\xi_{2,n}(\bs)$ between $\nu_{2,n}(\bs)$ and $\eps_{2,n}(\bs) + \nu_{2,n}(\bs) $.
% So the later terms are $O_p(\frac{h}{n})$ and can be ignored. 

\end{proof}


% \subsection{Theory for S-kNN Regressor}

% For consistency,
% it is well known that we need k to grow as a function of the sample size n.

% \cite{DistributionFreeThoery} Theorem 6.1 states the consistency result that
% \begin{thm}
% If $k_n \rightarrow \infty, k_n / n \rightarrow 0$, then the $k_n$-NN regression function estimate is weakly consistent for all distributions of $(X, Y)$ where ties occur with probability zero and $\mathbf{E} Y^2<\infty$, i.e.,
% $$
% \lim _{n \rightarrow \infty} \mathbf{E}\left\{\int\left(m_n(x)-m(x)\right)^2 \mu(d x)\right\}=0
% $$
% for all distributions of $(X, Y)$ with $\mathbf{E} Y^2<\infty$.
% \end{thm}

% Theorem 6.2 states the rate of convergence result that
% \begin{thm}
%     Theorem 6.2. Assume that $X$ is bounded,
% \begin{align*}
% \sigma^2(x)=\operatorname{Var}(Y \mid X=x) \leq \sigma^2 \quad\left(x \in \mathcal{R}^d\right)
% \end{align*}
% and
% \begin{align*}
% |m(x)-m(z)| \leq C\|x-z\| \quad\left(x, z \in \mathcal{R}^d\right) .
% \end{align*}
% Also assume that there exist $\epsilon_0$, a nonnegative function $g$ such that for all $x \in \calR$ and $0 < \epsilon < \epsilon_0$. For $S_{x, \eps}$ closed ball centered at $x$ with radius $\eps$, we have
% \begin{align*}
%     \mu\brac{S_{x, \eps}} > g(x) \eps^d
% \end{align*}
% and
% \begin{align*}
%     \int \frac{1}{g(x)^2} \mu(dx ) < \infty
% \end{align*}
% Let $m_n$ be the $k_n-N N$ estimate. Then
% \begin{align*}
% \mathbf{E}\left\|m_n-m\right\|^2 \leq \frac{\sigma^2}{k_n}+c_1 \cdot C^2\left(\frac{k_n}{n}\right)^{2 /d},
% \end{align*}
% thus for $k_n=c^{\prime}\left(\sigma^2 / C^2\right)^{1/3} n^{\frac{2}{3}}$,
% \begin{align*}
% \mathbf{E}\left\|m_n-m\right\|^2 \leq c^{\prime \prime} \sigma^{\frac{4}{3}} C^{\frac{2 }{3}} n^{-\frac{2}{3} .} .
% \end{align*}
% \end{thm}

% ~\\
% The key step for the rate is to prove that under the conditions
% \begin{align*}
% \mathbf{E}\left\{\left\|X_{(1, n)}(X)-X\right\|^2\right\} \leq \frac{\tilde{c}}{n^{2 / d}} .
% \end{align*}
% The assumption implies that for almost all $x \bmod \mu$ and $\epsilon_0<\epsilon<L$,
% \begin{align*}
% \mu\left(S_{x, \mathrm{c}}\right) \geq \mu\left(S_{x, \epsilon_0}\right) \geq g(x) \epsilon_0^d \geq g(x)\left(\frac{\epsilon_0}{L}\right)^d \epsilon^d,
% \end{align*}
% hence we can assume w.l.o.g. that the condition holds for all $0<\epsilon<L$. In this case, we get, for fixed $L>\epsilon>0$,
% \begin{align*}
% \begin{aligned}
% \mathbf{P}\left\{\left\|X_{(1, n)}(X)-X\right\|>\epsilon\right\} & =\mathbf{E}\left\{\left(1-\mu\left(S_{X, \epsilon}\right)\right)^n\right\} \\
% & \leq \mathbf{E}\left\{e^{-n \mu\left(S_{X, \epsilon}\right)}\right\} \\
% & \leq \mathbf{E}\left\{e^{-n g(X) \epsilon^d}\right\},
% \end{aligned}
% \end{align*}
% therefore,
% \begin{align*}
% \begin{aligned}
% \mathbf{E}\left\{\left\|X_{(1, n)}(X)-X\right\|^2\right\} & =\int_0^{L^2} \mathbf{P}\left\{\left\|X_{(1, n)}(X)-X\right\|>\sqrt{\epsilon}\right\} d \epsilon \\
% & \leq \int_0^{L^2} \mathbf{E}\left\{e^{-n g(X) c^{d / 2}}\right\} d \epsilon \\
% & \leq \iint_0^{\infty} e^{-n g(x) c^{d / 2}} d \epsilon \mu(d x) \\
% & =\int \frac{1}{n^{2 / d} g(x)^{2 / d}} \int_0^{\infty} e^{-z^{d / 2}} d z \mu(d x) \\
% & =\frac{\tilde{c}}{n^{2 / d}} .
% \end{aligned}
% \end{align*}

% \revise{Overall, the theory works if we figure out how to adopt this to the measures on the skeleton. However, the case is a bit trickier with the knot points with nonzero mass, as all points projected to the knot have the same skeleton-based distance. The essence will still be down to the analog of the rate of convergence of the nearest neighbor distance, which is $0$ in this case, and with model assumptions that $Y = m_\calS(x) + \eps$ for $x \in \calS$ we can just gather all the errors into the noise part.}

\subsection{Dual Path Algorithm for Generalized Lasso Problem}
\label{sec:lassoSolution}

For the generalized Lasso problem:
\begin{align*}
     {\sf minimize}_{\bbeta} \norm{y - X \beta}_2^2 + \lam \norm{ D \beta}_1
\end{align*}
If $D$ is invertible or the matrix $D$ has dimension $m \times p$ with $\text{rank}(D) = m$, this can be converted into a standard Lasso problem by setting $\theta = D \beta$, and the problem reduces to
\begin{align*}
     {\sf minimize}_{\bbeta} \norm{y - X D^{-1} \theta}_2^2 + \lam \norm{ \theta}_1
\end{align*}
However, this is not the case for the incidence matrix with the number of edges larger than the number of nodes.
Hence, we turn to the Lagrange dual problem. Let $\text{rank}(D) = m$, then we want to solve
\begin{align*}
    \text{minimize}_{u \in \RR^m } \frac{1}{2} \brac{X^T y - D^T u}\brac{X^T X}^{+} \brac{X^T y - D^T u}\\
    \text{subject to} \norm{u}_{\infty} \leq \lambda, D^T u \in \text{row}(X)
\end{align*}
We then follow the dual path algorithm by \cite{SolutionPathLasso}.
For notation, use $A^+$ to denote the Moore-Penrose pseudo-inverse of matrix $A$, and use subscript $-\calB$ to index over all rows or coordinates except those in set $\calB$. The algorithm is described in Algorithm \ref{alg::lassoPath}.

\begin{algorithm}
\caption{Dual path algorithm for generalized Lasso problem}
\label{alg::lassoPath}
Start with $k = 0, \lam_0 = \infty, \calB = \emptyset, s = \emptyset$.
While $\lam_k > 0$:
\begin{algorithmic}
\State 1. Compute a solution at $\lam_k$ by least squares as
\begin{align}
    \hat{u}_{\lam_k, -\calB} = \brac{D_{-\calB} D_{-\calB}^T}^{+} D_{-\calB}\brac{y - \lam_k D_{\calB}^T s}
\end{align}
\State 2. Compute the next hitting time $h_{k+1}$ by
\begin{align}
    t_i^{(hit)} = \frac{\sbrac{ \brac{D_{-\calB} D_{-\calB}^T}^+ D_{-\calB} y}_i}{\sbrac{ \brac{D_{-\calB} D_{-\calB}^T}^+ D_{-\calB} D_{-\calB}^T s}_i \pm 1 }
\end{align}
where only one of $+1$ or $-1$ will yield a value in $\sbrac{0, \lam_k}$, and this is the ``hitting time'' of coordinate $i$. Hence the next hitting time is
\begin{align}
    h_{k+1} = \max_i t_i^{(hit)}
\end{align}
\State 3. Compute the next leaving time $\ell_{k+1}$ by first defining 
\begin{align}
    c_i &= s_i \cdot \sbrac{D_{\calB} \sbrac{I - D_{-\calB}^T \brac{D_{-\calB} D_{-\calB}^T}^+ D_{-\calB} } y }_i,\\
    d_i &=  s_i \cdot \sbrac{D_{\calB} \sbrac{I - D_{-\calB}^T \brac{D_{-\calB} D_{-\calB}^T}^+ D_{-\calB} } D_{\calB}^T s }_i
\end{align}
and then the leaving time of the $i$th boundary coordinate is
\begin{align}
    t_i^{(leave)} = 
    \begin{cases}
        c_i / d_i, \text{if } c_i <0 \text{ and } d_i <0,\\
        0, \text{otherwise}
    \end{cases}
\end{align}
Therefore, the next leaving time is
\begin{align}
    \ell_{k+1} = \max_i t_i^{(leave)}
\end{align}

\State 4. Set $\lam_{k+1} = \max \set{h_{k+1}, \ell_{k+1}}$. If $h_{k+1} > \ell_{k+1}$, then add the hitting coordinate to $\calB$ and its sign to $s$, otherwise remove the leaving coordinate to $\calB$ and its sign from $s$. Set $k = k+1$.
\end{algorithmic}
\end{algorithm}




% \section{Preliminary Theory on Skeleton Projection}
% ~~~~An essential step in the proposed regression framework is the projection onto the constructed skeleton. In this section, we provide some preliminary theoretical analysis of this projection step. 


% \subsection{Characterizing Skeleton Projection}
% ~~~~To begin with, for simplification for analysis, we let all the data points be lying exactly on an underlying manifold $\calM$ with intrinsic dimension $q$ and let $d_\calM(.,.)$ denote the geodesic distance between two points on $\calM$.
% Note that for the empirical results, we allow intrinsic noise in the data structure that the data points are lying around the manifolds rather than exactly on the manifold, and the ability to deal with such data is an advantage of the proposed regression framework. 
% The exact manifold assumption is made here for ease of theoretical analysis.
% To make a distinction, we use $\norm{\bx - \by}$ to denote the Euclidean distance between two points in the ambient space $\bx, \by \in \calX \subseteq \RR^d$.
% Also, for notation we have $\Pi^{-1}: \calS \to \RR^d$ that the reverse projection gives the normal space to a point on the skeleton. 
% For $j = 1, \dots, k$, let $\calM_j = \calC_j \cap \calM$ be the $j$-th segmentation of the manifold within the ambient space Voronoi cells $\calC_j = \set{\bx \in \calX | d_\calX(\bx, V_j) < d_\calX(\bx, V_\ell), \ell \neq j}$. 
% We let $\Bar{\calA}$ denote the closure of a set $\calA$.
% % (Here we have the Voronoi cells to be open so we need to take closure to work with compact submanifolds. The boundaries are double counted, but will not affect the analysis. )  
% Denote the diameter of $\calM_j$ as $Diam_j= \sup_{a, b \in \calM_j} d_{\calM}(a,b)$ and let the reach of $\calM$ be $\tau = \sup\{t > 0: \forall x \in \RR^d , \exists! y \in \calM  \ \ s.t. \ \ dist(x, \calM) = \norm{x - y}\}$.
% Intuitively, you can roll a ball with radius $\tau$ freely along the manifold.

% \begin{itemize}
%     \item[\textbf{M1}] (Local Manifold Structure) Assume each $\bar{\calM_j}, \ \ j = 1, \dots, k$, is a compact, connected, $q_j$-dimensional Riemannian
% submanifold with $q_j \leq q$ embedded in $\RR^d$ for some $0<q\leq d$. 
% Denote the diameter of $\calM_j$ as $Diam_j$ and the reach of $\calM$ be $\tau$ and assume that $Diam_j < \tau$.
% % {\color{magenta}YC: Need to define sectional curvature, reach, and diameter.}
% \end{itemize}

% % The union of all $\calM_j$ is a rectifiable set.
% \textbf{M1} indicates that locally within each Voronoi cell, the underlying manifold satisfies usual regularity conditions and that locally the closest point is well-defined.
% For a large enough number of segments $k$, the local regions can be small and the local manifold structure within each segmentation can be well-structured. 
% % {\color{magenta}YC: Not sure why we can argue that this is a reasonable assumption. We need more detailed argument.}



% \begin{itemize}
%     \item[\textbf{B1}] (Bounding Diameter of Local Manifold ) For each $j = 1, \dots, k$, there exists a constant $C_1$ such that 
%     \begin{align*}
%         Diam_j  \leq \frac{C_1}{k^{1/q}}
%     \end{align*}
% \end{itemize}
% % Then, for each $j = 1, \dots, k$, there exists a constant $C_1$ such that 
% % \begin{align*}
% %     \sup_{\by \in \calM_j} d_{\calM}(\by, \Tilde{V}_j)  \leq \frac{C_1}{k^{1/q}}
% % \end{align*}

% Assumption B1 states that the diameter of the segments on the manifold according to the geodesic distance decreases with rates depending on the intrinsic dimension. 
% This rate makes sense if the manifold is segmented in a balanced way that each local submanifold has volume at rate $O(1/k)$.

% \begin{remark}
%     We employ $k$-means to perform data segmentation in the proposed framework for its empirical performance and existing theoretical guarantees. Previous works have viewed the $k$-means clustering objective as equivalent to finding a measure $\Tilde{\mu}_k$ supported on at most $k$ points such that $W_2\brac{\mu, \Tilde{\mu}_k}$ is small and have provided concentration properties in this regard. 
% \cite{LearningProbabilityOTMetric} shows that for sufficiently large $k$ and $\calX$ a compact, smooth $d$-dimensional manifold, there exists constants $C$ and $C'$ and a measure $\Tilde{\mu}_k$ such that 
% \begin{align*}
%     W_2\brac{\mu, \Tilde{\mu}_k} \leq C \tau k^{-1/d} \text{ with probability } 1 - e^{-\tau^2}
% \end{align*}
% on the basis of $n = C' k^{2+4/d}$ samples.
% \cite{ConvergenceinWasserstein} further shows that, in high dimensions with $d > 4$, the empirical measure $\hat{\mu}_k$ satisfies
% \begin{align*}
%     \EE W_2\brac{\mu, \hat{\mu}_k} \leq C_2 k^{-1/s}
% \end{align*}
% for any $s > d$ and $C^{''}$ a constant and that
% \begin{align*}
%     W_2\brac{\mu, \hat{\mu}_k} \leq C \tau k^{-1/s} \text{ with probability } 1 - e^{-\tau^4}
% \end{align*}
% that clustering on the basis of $k$ i.i.d.
% samples from $\mu$ is asymptotically optimal.
% \end{remark}


% \begin{prop}[Fr\'echet Mean and Knot Distance]
% \label{prop:distBetweenMeans}
% Assume conditions \textbf{M1} and \textbf{B1} hold.
% For arbitrary $q$-Hausdorff measure on $\calM_j$, let $\Tilde{V}_j$ be the Fr\'echet mean of $\calM_j$ with respect to the geodesic distance on the manifold. 
% Let the knots $V_j$ be constructed within the convex hull of data points in $\calC_j$. 
% For every $j = 1, \dots, k$, we have
%     \begin{align}
%         \norm {\Tilde{V}_j - V_j} \leq \frac{C_1}{k^{1/q}}
%     \end{align}
% where $C_1$ is the same constant as in Condition \textbf{B1}.
% \end{prop}


% \begin{proof}[Proof of Proposition \ref{prop:distBetweenMeans}]
% Let $Conv(\calM_j)$ be the convex hull of $\calM$ in the ambient space and note that
% \begin{align*}
%     Diam(Conv(\calM_j)) = Diam(\calM_j) \leq \frac{C_1}{k^{1/q}}
% \end{align*}
%     The knot $V_j$ given by the $k$-Means algorithm is a convex linear combination of points from $\calM_j$ and hence $V_j \in Conv(\calM_j)$, and as $\Tilde{V}_j \in \calM \subset Conv(\calM_j)$, hence
% \begin{align*}
%     \norm{\Tilde{V}_j - V_j} \leq Diam_\calX(Conv(\calM_j)) \leq \frac{C_1}{k^{1/q}}
% \end{align*}
% \end{proof}

% The above proposition ensures that the knots we constructed are not far from the Fr\'echet mean centroid on the manifold.
% A direct corollary by triangular inequality is that
% \begin{align*}
%     \sup_{\bx \in \calX} \norm{\bx - \Pi(\bx)} \leq \norm{\bx, V_{j(\bx)}} \leq d_{\calM}(\bx, \Tilde{V}_{j(\bx)}) + \norm{\Tilde{V}_{j(\bx)},V_{j(\bx)} }  \leq \frac{2 C_1}{k^{1/q}}
% \end{align*}
% so the projection distance is bounded with the rate decreasing with respect to the intrinsic dimension.
% This is a loose bound and note that there is no particularity about $\Tilde{V}_j$ that the distance between $V_j$ and any point in $\calM_j$ can achieve this same rate.
% Potentially the bound can be improved with some additional conditions and to argue for the specialty of the Fr\'echet mean, and we leave this as future work.

% % \begin{itemize}
% %     \item[\textbf{D1}] (Distribution on Manifold ) Assume the distribution of data points on $\calM_j$ follows a uniform $q$-Hausdorff measure. 
% % \end{itemize}




% % Then we want to work with a more general manifold which is not a space form that the curvature be different from point to point.
% % However, by the definition of reach $\tau$, 


% % \cite{EstimateReach} Proposition 2.7. 
% % Let $\calM \subset \RR^D$ be a connected closed $d$-dimensional manifold,
% % and let $\QQ$ be a probability distribution with support $\calM$. Assume that $\QQ$ has a density f with respect to the Hausdorff measure on $\calM$ such that $\inf_{x\in \calM} f(x) \geq f_{min} > 0$. Then
% % \begin{align*}
% %     \tau_\calM^d \leq \frac{C_d}{f_{min}} ,
% % \end{align*}
% % for some constant $C_d > 0$ depending only on $d$.
% % Also
% % \begin{align*}
% %     Diam(\calM) \leq \frac{C_d^{'}}{\tau_\calM^{d-1} f_{min}}
% % \end{align*}


% % Corollary 4.1. Let $\calM$ be a submanifold with reach $\tau_\calM$ and $\calY \subset \calX \subset \calM$ be two
% % nested subsets. Then $\tau_\calY \geq \tau_\calX \geq \tau_\calM$.

% % \begin{itemize}
% %     \item[\textbf{M2}] (Diffeomorphism) Let $T_{\Tilde{V}_j}\brac{\calM_j}$ be the tangent plane to $\calM_j$ at $\Tilde{V}_j$. 
% %     For each $j$, we assume that there is a subset $\calT_j \subset T_{\Tilde{V}_j}\brac{\calM_j}$ and Euclidean projection map $f_j: \calM_j \to \calT_j$ a diffeomorphism.
% % \end{itemize}

% % \begin{lem}
% % \label{lem:perpendicular}
% %     Under assumptions \textbf{M1}, \textbf{M2}, and \textbf{D1}, let $\dot V_j$ be the Euclidean mean in $\RR^d$  of $\calM_j$(minimizing $\int \norm{\bx - \dot V_j}^2 d \mu $ for the $\mu$ uniform $q$-Hausdorff measure on $\calM_j$)  and $\Tilde{V}_j$ the Fr\'echet mean (minimizing $\int d_{\calM} (\bx , \Tilde{V}_j)^2 d \mu $ ).  Then we have
% %     $(\dot V_j - \Tilde{V}_j) \perp T_{\Tilde{V}_j}\brac{\calM_j}$.
% % \end{lem}

% % \begin{proof}[Lemma \ref{lem:perpendicular}]
% %     By diffeomorphism, $\Tilde{V}_j$ is also the Euclidean mean of $\calT_j$.
% %     By the property of Euclidean projection, the projection of $\dot V_j$ to $T_{\Tilde{V}_j}\brac{\calM_j}$ is at $\Tilde{V}_j$, and hence $(\dot V_j - \Tilde{V}_j) \perp T_{\Tilde{V}_j}\brac{\calM_j}$.
% % \end{proof}


% % \begin{conjecture}
% %      Under assumptions \textbf{M1}, \textbf{M2}, \textbf{B1}, and \textbf{D1}, let $\kappa_j$ be the first principle curvature (maximum curvature) of $\calM_j$, then we have $\norm{\dot V_j - \Tilde{V}_j} \leq \kappa_j Diam_j \leq \frac{\kappa_j C_1}{k^{1/q}}$
% % \end{conjecture}


% % \JW{This is not an improvement in the rate compared to the example of circle. 
% % I suspect we need constant or bounded curvature stuffs, which may lead to the space form concepts.
% % However, we can just do a more general example on $q$-sphere to make the point that the knot should be close to the Fr\'echet mean.}

% % {\color{magenta}YC: Not working? then we have to delete this lemma
% % entirely later if we cannot prove it.}
% % \begin{lem} (Not working)
% %      For any $\bz \in \calM_j$ for $j = 1, \dots, k$, we have 
% %      \begin{align*}
% %          \Pi(\bz) \in \calC_j
% %      \end{align*}
% % \end{lem}
% % \begin{proof}
% %     By the definition of projection onto the skeleton, this trivially holds if $\bz$ is projected onto a knot. 
% %     For $\bz$ projected onto an edge in the skeleton, the edge must be connecting $V_j$ and the second closest knot from $\bz$, which, without loss of generality, we denote as $V_\ell$. 
% %     Since $\bz \in \calM_j$, by definition we have $\norm{V_j - \bz} < \norm{V_\ell - \bz}$, and as we are doing orthogonal projection from $\bz$ onto the edge, we have $\norm{V_j - \Pi(\bz)} < \norm{V_\ell - \Pi(\bz)}$ that the projection is on the edge closer to $V_j$.
    
% % \textit{    However, an issue is that, in some cases, the edge between $V_j$ and $V_\ell$ may not be contained in the two Voronoi cells but crosses another Voronoi cell. E.g. imagine a pretty flat triangle and the Voronoi cells given by the three vertices.
% % Good news is that we can work around this and the lemma needed is the next.}
% % \end{proof}

% By projecting onto the skeleton large dimensional space is essentially projected onto 1-dimensional and 0-dimensional structures, and the next lemma characterizes the projection sets.

% \begin{lem}
% \label{lem:projection}
%     For any $\bz \in \calM_j$ for $j = 1, \dots, k$, we have  
%         \begin{align*}
%         \Pi^{-1}(\Pi(\bz)) \subseteq \calC_j 
%     \end{align*} 
%     and hence
%     $\Pi^{-1}(\Pi(\bz)) \cap \calM \subseteq \calM_j $, 
%     where $\Pi^{-1}: \calS \to \calX $ such that $\Pi^{-1}(\bs) = \set{\bx \in \calX \vert  \Pi(\bz) = \bs}$ for $\bs \in \calS$.

%     % {\color{magenta}YC: we need to change notations. $\Pi^{-1}$ was
%     % defined as inverse projection on $\calX$. Perhaps $\Pi^{-1}_{\calM}?$}
% \end{lem}

% % {\color{magenta}YC: please add some comments on the interpretation of the Lemma.}
% This lemma essentially shows that the points that can be projected onto the same point on the skeleton belong to the same Voronoi cell, and therefore are located in the same manifold $\calM_j$.

 
% \begin{proof}[Proof of Lemma \ref{lem:projection}]
%     By the definition of projection onto the skeleton, we discuss the two cases of projections separately.
    
%     If $\bz$ is projected onto the knot $V_j$, then as only points with the closest knot being $V_j$ can be projected onto $V_j$, trivially we have $ \Pi^{-1}(V_j) \subseteq \calC_j$. 
    
%     For $\bz$ projected onto an edge in the skeleton, the edge must be connecting $V_j$ and the second closest knot from $\bz$, which, without loss of generality, we denote as $V_\ell$. 
%     Since $\bz \in \calM_j$, by definition we have $\norm{V_j - \bz} \leq \norm{V_\ell - \bz}$, and as we are doing orthogonal projection from $\bz$ onto the edge, we have $\norm{V_j - \Pi(\bz)} \leq \norm{V_\ell - \Pi(\bz)}$ that the projection is on the edge closer to $V_j$.
%     Although $\Pi(\bz)$ on the edge may not be contained in $V_j$ in some special cases, we still have $\Pi^{-1}(\Pi(\bz)) \subseteq \calM_j $ as argued below.
%     Further, we have  $\norm{V_j - \by} \leq \norm{V_\ell - \by}$ for all $\by \in \Pi^{-1}(\Pi(\bz))$.
%     Also note that, by the definition of skeleton projection, for a point $\by$ to be projected onto the edge connecting $V_j$ and $V_\ell$, the closest knot from $\by$ must be either $V_j$ or $V_\ell$.
%     Combining above, we have for all $\by \in \Pi^{-1}(\Pi(\bz))$, $\norm{V_j - \by} \leq \norm{V_i - \by}, i \neq j$.
%     Hence $\Pi^{-1}(\Pi(\bz)) \subseteq \calM_j $.
    


% \end{proof}


% \subsection{Bounding Projection Error}
% In this section, we provide a bound on the projection error between the true regression function 
% \begin{align*}
%     m(\bx) = \EE(\bY|\bX = \bx), \bx \in \calX
% \end{align*}
% and the skeleton-projected regression function \begin{align*}
%     m_\calS(\bs) = \EE\brac{\bY| \bX \in \Pi^{-1}(\bs)}, \bs \in \calS
% \end{align*}
% where $\Pi^{-1}(\bs) = \set{\bx \in \calX | \Pi(\bx) = \bs}$.

% \begin{itemize}
%     \item[\textbf{L1}] (Smoothness of the Regression Function ) The true regression function is Lipschitz continuous with respect to the geodesic distance on the manifold. That is, there exists a constant $L$ such that, for arbitrary $j$ in $1, \dots, k$, for any $\bx, \bz \in \calM$,
%     \begin{align}
%         \abs{m(\bx) - m(\bz)} \leq L \cdot d_\calM(\bx, \bz).
%     \end{align}
% \end{itemize}




% \begin{prop}[Projection Error Bound for Data Projected to Knot]
% \label{thm:projErrorKnot}
% Assume conditions \textbf{M1}, \textbf{B1}, and \textbf{L1} and assume all the data points are lying on the manifold $\calM$. 
% Then we have
% \begin{align}
%     \sup_{\bx \in \calX} \abs{m(\bx) - m_{\calS}(\bx)} \leq L  \frac{C_1}{k^{1/q}} 
% \end{align}
    
% \end{prop}

% \begin{proof}
% For $\bz \in \calM$, with a bit abuse of notation we let $m_{\calS}(\bz) := m_{\calS}\brac{\Pi(\bz)} $.
%  and by the tower rule of expectation, we have
% \begin{align*}
%     m_{\calS}\brac{\Pi(\bz)} & = \EE\brac{\bY| \Pi^{-1}(\Pi(\bz))}\\
%     & = \EE \sbrac{ \EE\brac{\bY| \bX \in \Pi^{-1}(\Pi(\bz)),  \bX = \bx } \vert   \bX \in \Pi^{-1}(\Pi(\bz))}\\
%     & = \EE \sbrac{ m(\bx) \vert \bX \in \Pi^{-1}(\Pi(\bz))}
% \end{align*}

% Without loss of generality, we let $\bz \in \calM_j \subset \calC_j$.
% By Lemma \ref{lem:projection}, we have $\Pi^{-1}(\Pi(\bz)) \subset \calM_j $. 
% Therefore, 
% \begin{align*}
%     \abs{m(\bx) - m_{\calS}(\bx)} &\leq \sup_{\bz \in \Pi^{-1}(\Pi(\bx))} \abs{m(\bx) - m(\bz)}\\
%     &\leq L \cdot \sup_{\bz \in \Pi^{-1}(\Pi(\bx))} d_\calM \brac{m(\bx) - m(\bz)}\\
%     &\leq L \cdot Diam_{j} \leq L  \frac{C_1}{k^{1/q}} 
% \end{align*}

% and hence 
% \begin{align*}
%     \sup_{\bx \in \calX} \abs{m(\bx) - m_{\calS}(\bx)} \leq L  \frac{C_1}{k^{1/q}} 
% \end{align*}

% \end{proof}

% Theorem \ref{thm:projErrorKnot} provides a bound with the rate depending on the intrinsic dimension for the projections error between the true regression function on the manifold and the projected regression function on the skeleton.
% Note that this bound also applies to points projected onto the skeleton edges, but generally, there can be some improvement in the convergence rate for edge points.


% % {\color{magenta}YC: We need to add more discussion
% % and comments on the projection error
% % and perhaps
% % combine this with 
% % the estimating error to obtain a final bound in case
% % if someone is interested in estimating the true regression function.
% % We should comment that while we lose some efficiency, the projection
% % error depends only on the dimension of the manifold. }


% \begin{lem}[Submanifold Dimension]
% \label{lem:intersection}
% Assume condition \textbf{M1}.
% For $\bz \in \calM_j$ such that $\Pi(\bz) \in \calE_{j \ell}$ that $\bz$ is projected onto the edge $j\ell$ of the skeleton. 
% Let $N_{\bx} \brac{\calM_j}$ denotes the normal space to $\calM_j$ at $\bx \in \calM_j$ and let $N_{\Pi(\bz)}\brac{\calE_{j \ell}}$ be the subspace normal to the line extending from the edge $\calE_{j \ell}$.
% Assume for every $j, \ell$ and every $\bx \in \calM_j \cap \Pi^{-1}\brac{\Pi\brac{\bz}}$, we have $ N_{\bx} (\calM_j) \cap N_{\bx} \brac{\Pi^{-1}(\Pi(\bz))} = \{ 0\}$. 
% Then we have $\Pi^{-1}(\Pi(\bz)) \cap \calM_j$ to be a $(q-1)$-dimensional submanifold of $\calM_j$.
% \end{lem}

% \begin{proof}
% Note that $\Pi^{-1}(\Pi(\bz)) = N_{\Pi(\bz)}\brac{\calE_{j \ell}}$. Then the lemma follows from tangent space arguments.
% $\calM_j$ and $\Pi^{-1}(\Pi(\bz))$ respectively have codimensions $d-q$ and $1$.
% So $\Pi^{-1}(\Pi(\bz))$  is defined (locally) as the set of zeros of a differentiable function $f_2: U \to \RR^{n-1}$
%  defined on an open neighbourhood $U$ of $\bx$ with surjective derivative $df_2$. 
%  The kernel of $df_2$ is the tangent space to $\Pi^{-1}(\Pi(\bz))$  which is l dimensional. 
%  If we now look at $f_2 \restriction \calM_j$, then $d(f_2 \restriction M_1):T_x(M_1)\rightarrow \mathbb{R}^{n-l}
% $ is again surjective since the kernel of $df_2$ is the tangent space to $N_{\Pi(\bz)}\brac{\calE_{j \ell}}$. 
% The intersection of the two tangent spaces has dimension $q+1-d$ 
% so the rank nullity theorem gives the dimension of the image as $q - (q+1-d) = d-1$.
% Thus the zero set of $f_2 \restriction M_1
% $ is the a submanifold of $\calM_j$ and is equal to $\Pi^{-1}(\Pi(\bz)) \cap \calM_j$ with dimension $(q-1)$.
% \end{proof}

% Note that the assumption in Lemma \ref{lem:intersection} is essentially saying that the edges connected to the knot $V_j$ are not within the normal bundle of the manifold $\calM_j$. With a carefully constructed skeleton, this should not be an issue.
% Lemma \ref{lem:intersection} shows that, generally, the set of points projected onto a point on the skeleton edge is generally of 1 dimension lower than the intrinsic dimension of the manifold. However, further assumptions and analysis are needed to translate this lower-dimensional space into a lower convergence rate.
% We show below a special case that for the underlying data structure to be a 1D manifold, the edge point on the skeleton has zero projection error, and leave the general theory comparing the rate of edge point to the knot point as future work.

% % For improved bound, we need the following assumption, which may not hold in general.
% % \begin{itemize}
% %     \item[\textbf{B2}] (Bounding Diameter of Submanifold) For each $j = 1, \dots, k$, and for every $\bs \in \calE \cap \calC_j$, there exists a constant $C_2$ such that 
% %     \begin{align*}
% %         Diam\brac{\Pi^{-1} \brac{\bs} \cap \calM_j}  \leq \frac{C_2}{k^{1/(q-1)}}
% %     \end{align*}
% % \end{itemize}

% % \JW{Can again try to argue through the volume of the submanifold compared to the original manifold to justify this.
% % However, this may not be that straightforward as the diameter of the submanifold may not be smaller than the original manifold.
% % For instance, for a sphere, a subspace passing through the center always has the same diameter as the sphere.
% % The way to make it sort of more reasonable is the case that all such submanifolds given by $\Pi^{-1}$ have a relatively similar volume, and the total edge length within a neighborhood is comparable to the diameter of the manifold so that $Vol\brac{\text{submanifold}} \times \text{edgeLength} \approx 1/k$, and then relating to condition \textbf{B1} we have the rate in \textbf{B2}.
% % }

% % \begin{theorem}[Projection Error Bound for Data Projected onto Edge]
% % Assume conditions \textbf{M1}, \textbf{B2}, and \textbf{L1} and the conditions in Lemma \ref{lem:intersection} hold. 
% % Then we have
% % \begin{align}
% %     \sup_{\bx \in \calX} \abs{m(\bx) - m_{\calS}(\bx)} \leq L  \frac{C_2}{k^{1/(q-1)}} 
% % \end{align}
    
% % \end{theorem}


% % \vspace{4em}

% % \textit{Try to generalize to points with noises.}

% % We then provide a more general version of the theorem with data points lying around the manifold that each data point $\bx_i = \Pi^{-1}(\bx_i) + \bm{\epsilon}_i$ where $\Pi^{-1}(\bx_i) \in \calM$ is the closest point on the manifold to $\bx$ (deviation from the manifold is smaller than reach so uniqueness of this is not a problem) and the error part $\bm{\epsilon}_i \in \RR^d$ for $i = 1, \dots, N$.
% % And the true regression function is defined on the manifold that $m(\bx) = m\brac{\Pi^{-1}(\bx)}$.

% % Let $\bu = \Pi^{-1}(\bx)$.
% % To follow the similar argument from the previous theorem, the easiest approach is to control that $\bu \in \calM_j$.
% % However, such precise control is not easy. For $\bu$ close to the boundary of the Voronoi cell, adding a random error can easily make it into another Voronoi cell.
% % Assuming the errors are normal to the manifold doesn't help much with this issue because normal to the manifold is different from normal to the skeleton edge and hence does not provide a guarantee.

% % More generally, for a bit loose of control, want to have
% %     \begin{align*}
% %         \sup_{\bz \in \Pi^{-1}(\Pi(\bx))} d_\calM \brac{m(\bu) - m(\bz)} \leq 2 \max_j Diam_j
% %     \end{align*}
% % For this we need the length of the error to be smaller in a way that doesn't extend to more than 1 other Voronoi cell. 
% % However this is difficult to control for. For space with high dimension $d$, at most $2^d$ Voronoi cells can intersect at a point, and for any data point around the intersection, a small perturbation can easily send the data point to a cell that only intersects the current cell at that intersection point. Therefore, it is difficult to characterize point after adding error in the ambient space, unless we make strong assumptions.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Projection Error for 1D Manifold}
% ~~~~In this section, we look at the special case that the underlying manifold $\calM$ has Hausdorff dimension $q = 1$ and provide the bounds for the projection errors onto knot points and edge points. 

% \begin{cor}[Projection Error onto Knots for Data on 1D Manifold]
% \label{thm:projKnot1D}
% Assume conditions \textbf{M1}, \textbf{B1}, and \textbf{L1} and assume all the data points are lying on the manifold $\calM$ with Hausdorff dimension $1$. 
% Then we have
% \begin{align}
%     \sup_{\bx \in \calX} \abs{m(\bx) - m_{\calS}(\bx)} \leq L  \frac{C_1}{k} 
% \end{align}
    
% \end{cor}
% This follows directly from Proposition \ref{thm:projErrorKnot} with $q = 1$.
% Note that, with $k = \sqrt{n}$, the projections error between the true regression function on the manifold and the projected regression function on the skeleton has rate $O\brac{n^{-1/2}}$.

% % \begin{proof}
% % For $\bz \in \calM$, with a bit abuse of notation we let $m_{\calS}(\bz) := m_{\calS}\brac{\Pi(\bz)} $.
% %  and by the tower rule of expectation, we have
% % \begin{align*}
% %     m_{\calS}\brac{\Pi(\bz)} & = \EE\brac{\bY| \Pi^{-1}(\Pi(\bz))}\\
% %     & = \EE \sbrac{ \EE\brac{\bY| \bX \in \Pi^{-1}(\Pi(\bz)),  \bX = \bx } \vert   \bX \in \Pi^{-1}(\Pi(\bz))}\\
% %     & = \EE \sbrac{ m(\bx) \vert \bX \in \Pi^{-1}(\Pi(\bz))}
% % \end{align*}

% % Without loss of generality, we let $\bz \in \calM_j \subset \calC_j$.
% % By Lemma \ref{lem:projection}, we have $\Pi^{-1}(\Pi(\bz)) \subset \calM_j $. 
% % Therefore, 
% % \begin{align*}
% %     \abs{m(\bx) - m_{\calS}(\bx)} &\leq \sup_{\bz \in \Pi^{-1}(\Pi(\bx))} \abs{m(\bx) - m(\bz)}\\
% %     &\leq L \cdot \sup_{\bz \in \Pi^{-1}(\Pi(\bx))} d_\calM \brac{m(\bx) - m(\bz)}\\
% %     &\leq L \cdot Diam_{j} \leq L  \frac{C_1}{k} 
% % \end{align*}

% % and hence 
% % \begin{align*}
% %     \sup_{\bx \in \calX} \abs{m(\bx) - m_{\calS}(\bx)} \leq L  \frac{C_1}{k} 
% % \end{align*}

% % \end{proof}


% Then we show that, on the 1D manifold, projection onto edge point enjoys zero error, which is lower error compared to the projection onto a knot point.

% \begin{prop}[Zero Projection Error for 1D Edge Point]
% \label{thm:projEdge1D}
% Assume the data manifold has Hausdorff dimension $1$, and conditions \textbf{M1} and the conditions in Lemma \ref{lem:intersection} hold. 
% Then for $\Pi^{-1}\brac{\calE} \equiv \set{\bx \in \calX : \Pi(\bx) \in \calE}$, we have
% \begin{align}
%     \sup_{\bx \in \Pi^{-1}\brac{\calE}} \abs{m(\bx) - m_{\calS}(\bx)} = 0
% \end{align}
    
% \end{prop}

% \begin{proof}
%     By Lemma \ref{lem:intersection}, for a 1D manifold, the projection set of an edge point has dimension $q-1 = 0$, so can only be composed of points.
%     Also by assumption \textbf{M1} each submanifold has diameter smaller than the reach so that the closest point is unique. 
%     Therefore, if the data point is projected onto an edge of the skeleton, then that projection is unique and has a one-to-one correspondence between the manifold point and the edge point.
% \end{proof}

% Intuitively, for a data point on the 1D manifold that is projected onto an edge of the skeleton, such projection is bijective and there is no loss in information with the projection.
% Particularly, compared to Proposition \ref{thm:projKnot1D} when bounds the projection error onto knots of the skeleton, when bounding the projection error from 1D manifold onto the skeleton edge, Proposition \ref{thm:projEdge1D} does not depend on condition \textbf{B1} which assumes the shrinking rate on the size of the submanifolds and condition \textbf{L1} which imposes smoothness on the true regression function.

% \begin{remark}
%     To gather the previous propositions into an overall thoerem for the projection error for data on 1D manifold, we need to measure the set of points projected onto the skeleton knots. 
%     However, we need more precise characterization of the skeleton graph for this purpose.
%     Briefly, for a knot with degree $0$ or $1$, we can bound the measure of the corresponding knot projection set by the volume of the submanifold as $O(1/k)$, but then we need the count of how many knots are of degree $0$ or $1$.
%     For a knot with degree greater than or equal to $2$, then the knot projection set depends on the principle angles between the edges in the ambient space and the shape of the local manifold. 
%     Due to such challenges, we leave the general result on this as future work.
% \end{remark}

% % \begin{lem}[Measure of Knot Projection Set]
% % Assume the data manifold has Hausdorff dimension $1$, and conditions \textbf{M1}, \textbf{D1}, \textbf{B2}.
% % Moreover, 

% % Then 
% % \begin{align}
% %     \mu_\calM \brac{\Pi^{-1}\brac{\calV} } \leq C k^{-1}
% % \end{align}
    
% % \end{lem}

% % \begin{proof}
% %     For knots $V_j$ with degree 1, then some proportion of $\calM_j$ will be projected onto the knot, with bound to be $\mu_\calM\brac{\calM_j} = \frac{1}{k}$. 
% %     \JW{Need assumption on the count of those}
    
% %     For connection knots, $d(\bx, V_j) = o(1/k)$
% % \end{proof}

% \subsection{Example of 1D Circular Manifold in 2D Euclidean Space}

% ~~~~Facing the challenges for a general theory as discussed above, here we look at a particular example that the manifold is a 1D circular segment embedded in the 2D Euclidean space.
% In particular, let $\calM_j$ be a segment of the $1$D circle with radius $r$ embedded in $\RR^2$ satisfying condition \textbf{M1}, so that segment is smaller than a half circle. See Figure $\ref{fig:circleExamle}$ for illustration.

% \begin{figure}
%     \centering
%     \includegraphics[height=4cm]{figures/circleExample.png}
%     \caption{Example of the difference between the knot $C$ and the Fr\'echet mean $F$ on a circular segment.}
%     \label{fig:circleExamle}
% \end{figure}


% We first analyze the distance between the knot and the circular manifold.
% Under this setup, we know the  Fr\'echet mean (F) is the midpoint of the curve, and the knot $C$ is on the line bisecting the curve. 
% Let $\ell = diam_j$ be the length of the curve, and we know half of the angle for the circular sector is $\theta = \frac{\ell/2}{r}$.
% Then the distance from the Fr\'echet mean (F) to the line segment between the two endpoints is 
% \begin{align*}
% d(F,A) =  r - r \cos \theta = r \brac{ 1 - 1 + \frac{\theta^2}{2} - \frac{\theta^4}{24} + \dots} \leq r \frac{\theta^2}{2}  = \frac{\ell^2}{8 r}.
% \end{align*}
% Therefore, 
% \begin{align*}
%     d(F, C) = \frac{1}{2} d(F,A) \leq \frac{\ell^2}{16 r}
% \end{align*}
% that the distance decreases at the rate of the square of the diameter.
% Then bounding the diameter with the rate in Assumption \textbf{B1} we have the following result:
% \begin{prop}[Knot to Circular Manifold Distance]
%     In this circular manifold setting and under assumption \textbf{M1, B1}, we have 
% \begin{align*}
%     d(F, C) \leq \frac{C_1^{'}}{k^{2/q}}.
% \end{align*}
% \end{prop}
% Note that this is the square of the rate as in Proposition \ref{prop:distBetweenMeans} and only applies to the Fr\'echet mean. Hence, under appropriate conditions, a general result with the format in Proposition \ref{prop:distBetweenMeans} can be possible.


% Then we analyze the volume of the set of points on the circular manifold that are projected onto the skeleton knots rather than edges. 
% For this analysis we provide another illustration as shown in Figure \ref{fig:knotProjSetExample}. 
% Let the submanifolds be the circular segments between GB, BD, and DE, the knots of the submanifold are M, C, and Q, and the skeleton edges are line segments between MC and CQ. 
% Line $IC \perp CM$ and intersect the manifold at $I$.
% Line $JC \perp CQ$ and intersect the manifold at $J$. 
% The set of points on the manifold that are projected onto the knot $C$ is the circular segment between $I$ and $J$.

% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}{0.5\textwidth}
%     \includegraphics[width=\textwidth]{figures/knotProjSetExample.png}
%     \caption{Illustration of the knot project set of a circular segment. 
%     The submanifolds are the circular segments between BD, GB, and DE (colored solid blue).
%     The Skeleton is colored in red.}
%     \label{fig:knotProjSetExample1}
%     \end{subfigure}
%     \hfill
%     \\
%     \begin{subfigure}{0.4\textwidth}
%     \includegraphics[width=\textwidth]{figures/knotProjSetExample2.png}
%     \caption{Illustration for the calculation of knot project set.}
%     \label{fig:knotProjSetExample2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.4\textwidth}
%     \includegraphics[width=\textwidth]{figures/knotProjSetExample3.png}
%     \caption{Illustration for the calculation of knot project set.}
%     \label{fig:knotProjSetExample3}
%     \end{subfigure}
%     \caption{Illustration of the knot project set of a circular segment.}
%     \label{fig:knotProjSetExample}
% \end{figure}

% To calculate the length of this knot projection set, we start with some notations and give a more detailed illustration in Figure \ref{fig:knotProjSetExample2}. 
% Let the radius for the circular manifold (such as $d(F,A)$) given to be $r$, let the distance between the knot $C$ and the circular manifold $d(F,C) \coloneq e$, and let the angle $\angle FCI \coloneq \theta$.
% To find the curve length  $\ell_{FI}$ between $F, I$, we need the angle of $\angle FAI \coloneq \alpha$, and denote the angle $\angle AIC \coloneq \beta$.
%     Trivially we have $\alpha + \beta = \theta$.
%     Then by the law of sines, we know
%     \begin{align*}
%         \frac{d(A,I)}{\sin(\angle ACI)} = \frac{d(A,C)}{\sin(\angle AIC)}
%     \end{align*}
%     which leads to
%     \begin{align*}
%         \beta = \arcsin \sbrac{ \frac{r-e}{r}\sin(\pi-\theta)} = \arcsin \sbrac{ \frac{r-e}{r}\sin(\theta)}
%     \end{align*}
%     and therefore
%     \begin{align*}
%         \alpha & = \theta - \beta = \theta - \arcsin \sbrac{ \frac{r-e}{r}\sin(\theta)}
%     \end{align*}
%     By the Taylor expansion of $\arcsin(x) = x + \frac{x^3}{6} + \frac{3 x^5}{40} + \dots \geq x$, and $\sin \theta \geq \theta - \frac{\theta^3}{6}$, we know the curve length
%     \begin{align*}
%         \ell_{FI} &= r \cdot \alpha = r \cdot \sbrac{ \theta - \arcsin \sbrac{ \frac{r-e}{r}\sin(\theta)} }\\
%         & \leq r \cdot \sbrac{ \theta -  \frac{r-e}{r}\sin(\theta) }\\
%         & \leq r \cdot \sbrac{ \theta -  \frac{r-e}{r} \brac{\theta - \frac{\theta^3}{6}} }  = e \theta + (r-e) \frac{\theta^3}{6}
%     \end{align*}

%     % \JW{Plug in $\sin \theta \approx \theta$ for small $\theta$, we have the above to be $e \cdot \theta$, which makes sense and can decrease in $e$, which by the above remark we have $e \leq \frac{C'}{k^{2}}$.
%     % Then to complete the argument, particularly for the second term, we need an argument that $\theta$ is decreasing with a rate at least $O\brac{k^{-2/3}}$. }

%     To bound the angle $\angle FCI \coloneq \theta$, let $\ell_{max} = \max_j \set{diam_j} = O\brac{k^{-1}}$, and let $\gamma =  Reach(\calM)$.
%     For an illustration of angle bound refer to Figure \ref{fig:knotProjSetExample3}.
%     The angle 
%     \begin{align*}
%         \angle BAD = \frac{\ell_{BD}}{r} = \frac{diam_j}{r} \leq \frac{\ell_{max}}{\gamma}
%     \end{align*}
%     and this bound similarly holds for the angle $\angle GAB$.
%     Note that the knots are on the angular bisecting line of the corresponding circular segments, and therefore we have
%     \begin{align*}
%         \angle MAC \leq \frac{\ell_{max}}{\gamma}
%     \end{align*}
%     As $\theta \coloneq \angle ICF$ and let $\angle NMK = \eta \geq 0$, and we know that
%     \begin{align*}
%         \brac{\pi/2-\theta} + \brac{\pi/2 -\eta} + \angle MAC = \pi
%     \end{align*}
%     and hence
%     \begin{align*}
%         \theta + \eta = \angle MAC
%     \end{align*}
%     For a loose bound we have
%     \begin{align*}
%         \theta \leq \angle MAC \leq \frac{\ell_{max}}{\gamma} \leq \frac{C''}{\gamma} k^{-1}
%     \end{align*}


%     Combining with the above, we have the curve length between $F, I$
%     \begin{align*}
%         \ell_{FI}&\leq e \theta + (r-e) \frac{\theta^3}{6} = O\brac{k^{-2}}\times O\brac{k^{-1}} + O(1) \times O(k^{-3}) = O(k^{-3})
%     \end{align*}
%     This bound also applies to the length of the circular segment $\ell_{FJ}$.
%     Therefore, the measure of the set of points projected onto knot $C$ is 
%     \begin{align*}
%         \ell_{IJ} = \ell_{FJ} + \ell_{FI} = O(k^{-3}) + O(k^{-3}) = O(k^{-3}).
%     \end{align*}

%     \begin{prop}[Knot Projection Set for 1D circular Manifold]
%     \label{thm:knotProjSet1Dcircle}
%     Let the underlying manifold be a 1D circular segment. Let the submanifolds partition the circular segment and the skeleton graph has a chain-like structure.
%     Assume conditions \textbf{M1, B1} hold. Then for any knot $V$ with degree $2$, we have
%     \begin{align*}
%         \mu_\calM \brac{\Pi^{-1}\brac{V}} = O(k^{-3}).
%     \end{align*}
        
%     \end{prop}

% That is, the measure of the projection set of a in-between knot decreases with a rate of the cubic of $k$, which is fast.
% Then, as a corrolary to Proposition \ref{thm:knotProjSet1Dcircle}, we can bound the total measure of all the knot-projecting points.
% % with the Proposition \ref{thm:projKnot1D} and Proposition \ref{thm:projEdge1D} in the previous section, we can have a result on the total projection error for this particular 1D circular segment case.

% \begin{cor}
%     Let the underlying manifold be a 1D circular segment. Let the submanifolds partition the circular segment and the skeleton graph has a chain-like structure.
%     Assume conditions \textbf{M1, B1} hold. Then we have
%     \begin{align*}
%         \mu_\calM\brac{\cup_{V \in \calV} \Pi^{-1}\brac{V}} = O\brac{k^{-1}}.
%     \end{align*}
% \end{cor}

% \begin{proof}
%     Overall, by Proposition \ref{thm:knotProjSet1Dcircle}, the set of points projected onto any connecting knot has a measure smaller than or equal to $(k-2) \times O(k^{-3}) = O(k^{-2})$.

%     For end knots (with degree equal to $1$, or $0$), we can bound the measure of the set of points projected onto one of these knots by the measure of the corresponding submanifold, with rate $O\brac{k^{-1}}$. 
%     In this case, we have the number of end knots to be $2$, which is $O(1)$.
%     Together, we have the total measure of all the knot-projecting points to be bounded by
%     \begin{align*}
%         O(k) \times O(k^{-3}) + O(1) \times O\brac{k^{-1}} = O\brac{k^{-1}}
%     \end{align*}
%     which diminishes as $k$ increases.

% \end{proof}

% That is, in this 1D circular manifold case, the total measure of knot-projecting point is only a small proportion of the total measure of the data space and  decreases with the number of knots $k$. 




% \begin{remark}[Generalize to High-Dim (Loose Discussion)]
% More generally, for $\calM_j$ a subspace on the $q$-sphere with diameter $\ell < 2\pi r$ where $r$ is the radius of the sphere, we can reduce such a case to the example above.
% We let the diameter-attaining pair of points be $G, H \in \Bar{\calM_j}$ similar to as in Figure \ref{fig:circleExamle}. 
% Again we let A be the midpoint of the line segment between G and H, and let F be the midpoint on the geodesic curve between G and H. 
% Note that the line given by AF is perpendicular to the line GH and passes the center of the sphere. 
% However, in this more general case, the Euclidean mean point C may not be on the line segment AF.
% However, there exists a point $C'$ on the sphere that the distance between C and $C'$ is bounded above by the distance between A and F.
% To see this, note that C is within the convex hull of $\calM_j$ and hence let the line passing through the center and C intersect the sphere at $C' \in \calM_j$.
% Also, C is within the space between the sphere and the subspace spanned by rotation GH with respect to AF, and hence CC' has a length smaller than AF.
% With the calculation in the previous example, we have $d(F,A) \leq \frac{\ell^2}{8 r}$ so we still enjoy the quadratic rate.
% \end{remark}


% \begin{thm}[Projection Error Bound for Data around the Manifold]
% Assume conditions \textbf{M1}, \textbf{B1}, and \textbf{L1} . 
% Then we have
% \begin{align}
%     \sup_{\bx \in \calX} \abs{m(\bx) - m_{\calS}(\bx)} \leq L  \frac{C_1}{k^{1/q}} 
% \end{align}
    
% \end{thm}

% \vspace{2em}

% \textbf{Naive Approach}

% To start with, we assume the knots set $\calV \in \calX$ and the ambient space $\calX $ is convex, and therefore the edge set, as convex combinations of knots, are also in $\calX$.

% \begin{itemize}
%     \item[\textbf{B1}] (Bounded radius of Voronoi cells) Let $\calC_j = \set{\bx \in \calX: d_{\calX}(\bx, V_j) \geq \max_{j \neq \ell} \set{d_{\calX}(\bx, V_\ell)} }$ be the Voronoi cell corresponding to knot $V_j$ where points on the boundary of the Voronoi cells are assigned randomly. There exists a constant $C_1$ such that, for every $j$,
%     \begin{align*}
%         \sup_{\bx \in \calC_j} d_{\calX}(\bx, V_j)  \leq \frac{C_1}{k^{1/d}}
%     \end{align*}
% \end{itemize}

% Assumption B1 requires the radius of the Voronoi cells to be decaying at rate $k^{-1/d}$. In the case when the number of knots $k$ is chosen at rate $\Theta(\sqrt{n})$, then the rate of decay for the radius of the Voronoi cells is $n^{-\frac{1}{2d}}$.

% \begin{lemma}[Projection Distance Bound]
%     Given assumption B1, let $V_{j(\bx)}$ be the knot of the Voronoi cell such that $\bx \in \calC_j$, we have
%     \begin{align*}
%         \sup_{\bx \in \calX} d_{\calX}(\bx, \Pi(\bx)) \leq d_{\calX}(\bx, V_{j(\bx)}) \leq \frac{C_1}{k^{1/d}}
%     \end{align*}
% \end{lemma}

% \begin{itemize}
%     \item[\textbf{B2}] (Smoothness of Regression Function) The regression function over the ambient space is Lipschitz continuous.
% \end{itemize}

% \begin{theorem}[Naive Projection Error Bound] There exists a constant $C_3$ such that, for every point $\bx \in \calX$,
%     \begin{align*}
%         \sup_{\bx \in \calX} \abs{m(\bx) - m_{\calS}(\bx)} \leq \frac{C_3}{k^{1/d}}
%     \end{align*}
% \end{theorem}

% Note that, in the case when the number of knots $k$ is chosen at rate $\Theta(\sqrt{n})$, the rate of decay for the naive projection error bound is $n^{-\frac{1}{2d}}$, which can be slow.

% The bound can be improved by replacing the ambient dimension $d$ with the intrinsic dimension $\Tilde{d}$ if we believe the data points are lying around (or one) an underlying manifold and strengthen the previous assumptions. 
% Lying around the manifold is more beneficial to our workflow as, if it is on the manifold, then exact manifold recovery algorithms with theoretical guarantee definitely have the advantage.



% \vspace{2em}

% \textit{\textbf{Approve involving the characterizations of the location of the knots in the ambient space posts a bunch of challenges and we try to work around it.}}
% \begin{itemize}
%     \item[\textbf{B1'}] (Bounded intrinsic radius of Voronoi cells) There exists a constant $C_1$ such that, for every $j$,
%     \begin{align*}
%         \sup_{\bx \in \calC_j} d_{\calX}(\bx, V_j)  \leq \frac{C_1}{k^{1/\Tilde{d}}}
%     \end{align*}
%     or for another version
%     \begin{align*}
%         \sup_{\bx \in \calC_j} d_{\calX}(\bx, \Pi(\bx))  \leq \frac{C_1}{k^{1/\Tilde{d}}}
%     \end{align*}    
% \end{itemize}
% \textit{This definitely requires justification from a geometric perspective, potentially involving conditions about curvature and reach. 
% A critical point is that, even if all the data points are on the manifold, the knots given by k-Means may not be on the manifold but in the ambient space, and the edge connecting two nearby knots where data points are projected onto is also in the ambient space and its relation to the manifold cannot be guaranteed (such as tangential).
% Definitely more considerations are needed.
% }


% We again want to bound
% \begin{align*}
%     \sup_{\bx \in \calX} \abs{m(\bx) - m_{\calS}(\bx)} 
% \end{align*}
% In this setting, the true regression function should be defined on the manifold that $m(\bx) = m (\Pi^{-1} (\bx)) = m_{\calM} (\Pi^{-1} (\bx))$ and that $\bx = \Pi^{-1} (\bx) + \epsilon$ where the projection onto the manifold is according to the normal bundle. \textit{For simplicity for now assume the random error $\epsilon$ is normal to the manifold and hence does not change the projection.}

% \begin{lemma}[Value of Regression function on Skeleton]
%     We have
%     \begin{align*}
%         m_\calS(\bx) = m_\calM( \by), \Pi_{\calS}(\by) = \Pi_{\calS}(\bx) \text{ for } \by \in \calM
%     \end{align*}
% \end{lemma}

% In this sense, we want to bound
% \begin{align*}
%     \sup_{\bx \in \calX} \abs{m (\Pi^{-1} (\bx)) - m(\by)},  \Pi_{\calS}(\by) = \Pi_{\calS}(\bx)
% \end{align*}



%\revise{
%\subsection{Choice of Bandwidth} \label{sec::bandwidth}
%~~~~A key parameter for the kernel regressor is the smoothing bandwidth $h$. In practice the best performance is usually achieved by picking $h$ through cross-validation. To inform the bandwidth values to test, we adopt some rule of thumb bandwidths as the base and apply different scaling factors for parameter tuning. This is to provide some reasonable bandwidth choices with limited computational resources. Empirically we observe that some scaling of the base bandwidth can give better regression performances, and fine-tuning bandwidth with cross-validation can be beneficial if computation allows.
%}

%\begin{comment}
%\subsubsection{Initialization for Cross-Validation}
%\cite{li2007nonparametric} proposed to use $h = \sig_x n^{-1/5}$ for kernel regression, which is similar to the Silverman's rule of thumb bandwidth \cite{silverman1986density} for density estimation that $h = \left(\frac{4}{3}\right)^{1/5} \hat{\sig}_x n^{-1/5}$, where $\sig_x$ and $\hat{\sig}_x$ are the true and estimated standard deviation for the covariates. However, the skeleton-projected covariates, different from the original $\RR^d$-based covariates, do not have a clear definition of mean and variance. So we introduce a simple way to measure the variability of covariates on the skeleton as replacements for the usual standard deviation. \\
%\ \\
%The skeleton can be compressed as a $1$-dimensional structure by picking a point $\bs \in \calS$ as the origin and calculate the skeleton-based distances between $\bs$ and all the skeleton-projected covariates $\bs_\ell, \ell = 1,\dots, n$. Based on this vector of skeleton distances, let $\Bar{d}_\calS(\bs) = \frac{1}{n}\sum_{j=1}^n d_{\calS}(\bs, \bs_j)$ be the average skeleton distance to $\bs$. We define a quantity like the standard deviation that
%\begin{align*}
%    \hat{\sig}_{\bs} = \sqrt{\frac{1}{n} \sum_{j = 1}^n  \left(d_{\calS}(\bs, \bs_j) -\Bar{d}_\calS(\bs) \right)^2}.
%\end{align*}
%However, $\hat{\sig}_{\bs}$ can be different with different $\bs$ as the origin. To accommodate for this, we can get the $n\times n$ matrix of skeleton distances between each observed $\bs_j$, compute $\hat{\sig}_{\bs_j}$ for each row, and define the average as the overall variability measure for the skeleton-based covariates. But the computation of the $n\times n$ matrix of skeleton-based distances can be computation expensive (the computation of S-kernel regressor for new predictions only requires the skeleton-based distance between the new test observations and the training points, and hence the skeleton-distances between the training samples are extra calculations only used for the bandwidth). Therefore, to save some computation, we instead compute the $k \times n$ skeleton distance matrix between the knots and the skeleton-projected training covariates, get the $\hat{\sig}_{V_j}$ for each knot, and use the average as variability of skeleton-based covariates. Formally, we let
%\begin{align} \label{eq::skelsd}
%    \hat{\sig}_\calS = \frac{1}{k } \sum_{i = 1}^k \hat{\sig}_{V_i} =  \frac{1}{k} \sum_{i = 1}^k \sqrt{\frac{1}{n} \sum_{j = 1}^n  \left(d_{\calS}(V_i, \bs_j) -\Bar{d}_\calS(V_i)  \right)^2}
%\end{align}
%Note that in case there are disconnected components in the skeleton, observations in different components have infinite skeleton-based distances, and we ignore the infinite values in the standard deviation calculation. In other words, the standard deviation at each skeleton point is only calculated within the same component.
%The base bandwidth used in this work is 
%\begin{align} \label{eq::baseBand}
%    h_{hns} = \left(\frac{4}{3}\right)^{1/5} \cdot \hat{\sig}_\calS \cdot n^{-1/5}
%\end{align}\\
%The above discussion on choosing the base bandwidth is to inform a starting point for the cross-validation bandwidth choice procedure and, with limited computational resource, to offer some reasonable bandwidth choices without much parameter tuning. More comprehensive cross-validation procedure to fine tune the bandwidth can potentially lead to better performance results than the procedure used in this work.
%\ \\
%\begin{remark}
%In practice, the bandwidth can also be chosen in an adaptive way. For each prediction, the skeleton-based distances between that test covariate and all the training points are required to compute the S-kernel regressor, and the standard deviation of this vector can be used to get the base bandwidth. However, this will make the base bandwidth at each prediction point different and hence adaptive. The adaptive way saves the extra skeleton distance calculations between the training points and can take less time. Empirically we observe that the global bandwidth proposed in Equation \ref{eq::baseBand} and the adaptive bandwidth have similar performances while the global bandwidth has slightly better performances on our data examples in Section \ref{sec::simulation} and \ref{sec::real}. Therefore all the S-kernel regressor results presented in this work choose bandwidth with cross-validation with the base given by the global bandwidth in Equation \ref{eq::baseBand}.
%\end{remark}


\section{Additional Simulation Results}
\label{sec::extraSim}

\subsection{Vary Skeleton Cuts}
~~~~In this section, we examine the effect of cutting the skeleton into various numbers of disjoint components on the final regression performance. 
We use the same simulated datasets from Section \ref{sec::simulation}, including Yinyang data, Noisy Yinyang data, and SwissRoll data. 
The analysis procedure is mainly the same, where we use $5$-fold cross-validation SSE to evaluate the regression results for each dataset and repeat the process 100 times with randomly generated datasets. 
The main difference is that, during the skeleton construction step, we segment the skeleton graph into different disjoint components using single-linkage hierarchical clustering with respect to the Voronoi Density weights, as outlined in Section \ref{sec::skeletoncons}. 
We then fit and evaluate the skeleton-based regression methods on the skeletons that have been differently cut. 
% present some additional simulation results using the proposed skeleton regression framework, and we mainly 

% \YC{I don't think we need to 
% show the figures of Q-spline and C-spline
% since they are often not as good as the L-spline. 
% As for the kernel and kNN,
% you should not display all the smoothing bandwidth/NN.
% Probably just pick 3-5 of them.
% The figures look very chaotic...}


% \YC{Also, did you repeat the experiment multiple times?
% We need to repeat it at least 100 times and display the error bars
% of the SSE under different cuts. }

 
\subsubsection{Vary Skeleton Cuts for Yinyang Data}
~~~~In this section, we investigate how cutting the skeleton into different numbers of disjoint components affects the performance of skeleton-based methods using the Yinyang data (from Section \ref{sec::Yinyang}). We randomly generate 1000-dimensional Yinyang data 100 times and use $5$-fold cross-validation to calculate the SSE on each dataset. We fit the skeleton-based methods in the same manner as in Section \ref{sec::simulation}, with the exception that the number of knots is fixed at 38 and we cut the initial graph into various numbers of disjoint components (ranging from 1 to 25) when constructing the skeleton. The median 5-fold cross-validation SSEs across the 100 datasets for different numbers of disjoint components are plotted in Figure \ref{fig::Yinyangd1000cut}.

Our results show that the S-Lspline method is sensitive to changes in the skeleton structure. In the case of Yinyang data, since there are 5 true disjoint structures in the covariate space, a cut of 5 results in the best regression performance. By design, S-Lspline regressors may incorporate unrelated information from one structure to another when an edge connects two structurally different areas, thus leading to a decline in the regression performance. For future research, incorporating edge weights into the S-Lspline regressor may help to mitigate the interference between different structures. The S-Kernel regressor also achieves optimal performance when the skeleton is segmented into 5 disjoint components. Skeleton-based kernel regression methods exhibit large changes in performance as the skeleton segmentation changes when the bandwidth is large. This is understandable as larger bandwidths allow more information from large distances, which are more likely to be non-informative as the segmentation changes. On the other hand, the S-kNN regressor has the best regression performance when the skeleton is left as a fully connected graph. This may be due to the locally adaptive nature of the k-nearest-neighbor method that ensures regression results are accurate as long as local neighborhoods are identified accurately.

\begin{figure}
% \captionsetup{skip=1pt}
\centering
\includegraphics[width = \textwidth]{figures/Yinyang1000_cuts.jpeg}
\caption{Yinyang $d = 1000$ data skeleton regression results with the number of knots fixed as $38$ but segmented into varying numbers of disjoint components. The median SSE across the $100$ simulated datasets with each given parameter setting is plotted. 
% \YC{Does this mean that the few cut seems to be always better?
% For kernel}
}
\label{fig::Yinyangd1000cut}
\end{figure}
%   \begin{subfigure}[t]{0.26\textwidth}
%         \centering
% \includegraphics[width=\linewidth]{figures/Yinyang100_ncut_skelLspline.jpeg} 
%         \caption{S-Lspline}
%     \end{subfigure}
% %         \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/Yinyang100_ncut_skelQspline.jpeg} 
% %         \caption{S-Qspline}
% %     \end{subfigure}
% %     \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/Yinyang100_ncut_skelCspline.jpeg} 
% %         \caption{S-Cspline}
% %     \end{subfigure}\\
%         \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Yinyang100_ncut_skelkernel.jpeg}
%         \caption{S-Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Yinyang100_ncut_skelknn.jpeg}
%         \caption{S-kNN}
%     \end{subfigure}


\subsubsection{Vary Skeleton Cuts for Noisy Yinyang Data}
~~~~We then evaluate the performance of the skeleton-based regression methods on the Noisy Yinyang data (from Section \ref{sec::NoisyYinyang}) when the skeletons are constructed with different numbers of disjoint components. Similarly, we randomly generate 1000-dimensional Noisy Yinyang data 100 times and use $5$-fold cross-validation to calculate the sum of squared errors (SSE) on each dataset. We fix the number of knots to be 71 and construct skeletons with different numbers of disjoint components. The median 5-fold cross-validation SSEs across the 100 datasets for different numbers of disjoint components are plotted in Figure \ref{fig::NoiseYinyangd1000cut}.

\begin{figure}
% \captionsetup{skip=1pt}
\centering
\includegraphics[width = \textwidth]{figures/NoiseYinyang1000_cuts.jpeg}
\caption{Noise Yinyang $d = 1000$ data skeleton regression results with the number of knots fixed as $38$ but segmented into varying numbers of disjoint components. The median SSE across the $100$ simulated datasets with each given parameter setting is plotted. }
\label{fig::NoiseYinyangd1000cut}
\end{figure}

With the presence of noise, the S-Lspline method does not show significant variations in performance when the skeleton graph is cut into different disjoint components. The best regression result is obtained when the graph is left as a fully connected graph.
In contrast, the performance of the S-kernel method varies with the number of disjoint components. The best results, regardless of the bandwidth, are obtained when the skeleton is segmented into around 13 components, which is larger than the true number of 5 components in the data.
Lastly, the S-kNN method demonstrates an increase in SSE with an increase in the number of disjoint components.


% \begin{figure}
% % \captionsetup{skip=1pt}
% \centering
%     \begin{subfigure}[t]{0.26\textwidth}
%         \centering
% \includegraphics[width=\linewidth]{figures/NoiseYinyang100_ncut_skelLspline.jpeg} 
%         \caption{S-Lspline}
%     \end{subfigure}
% %         \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/NoiseYinyang100_ncut_skelQspline.jpeg} 
% %         \caption{S-Qspline}
% %     \end{subfigure}
% %     \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/NoiseYinyang100_ncut_skelCspline.jpeg} 
% %         \caption{S-Cspline}
% %     \end{subfigure}\\
%         \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/NoiseYinyang100_ncut_skelkernel.jpeg}
%         \caption{S-Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/NoiseYinyang100_ncut_skelknn.jpeg}
%         \caption{S-kNN}
%     \end{subfigure}
% \caption{Noisy Yinyang Regression fitted points $d = 1000$ with varying number of cuts results, with number of knots fixed as $99$.
% }
% \label{fig::NoiseYinyangd1000cut}
% \end{figure}




\subsubsection{Vary Skeleton Cuts for SwissRoll data}
~~~~In this section, we evaluate the performance of skeleton-based methods on SwissRoll data (from Section \ref{sec::SwissRoll}) with skeletons cut into different numbers of disjoint components. Similarly, we randomly generate 1000-dimensional SwissRoll data 100 times and use $5$-fold cross-validation to calculate the sum of squared errors (SSE) on each dataset. We fix the number of knots to be 70 and construct skeletons with different numbers of disjoint components. The median 5-fold cross-validation SSEs across the 100 datasets for different numbers of disjoint components are plotted in Figure \ref{fig::SwissRolld1000cut}.

We find that the S-Lspline regressor is sensitive to changes in the skeleton structure, with the best regression results obtained when the skeleton is constructed as one connected graph. This makes sense as the covariates lie on one connected manifold.
The S-Kernel regressor also performs best on the fully connected skeleton. After an initial increase in SSE as the number of disjoint components increases, the SSE of the S-kernel regressor remains relatively stable.
The S-kNN regressor also achieves the best regression performance when the skeleton is left as a fully connected graph.
Overall, the SSE of the S-kNN regressor increases with the number of disjoint components, but for a small number of neighbors, there can be a decrease in SSE when the skeleton is cut into more disjoint components. One possible explanation is that, as the response function has discontinuous changes, segmenting the covariate space into more fragments can improve estimation in regions where the response changes abruptly.


\begin{figure}
% \captionsetup{skip=1pt}
\centering
\includegraphics[width = \textwidth]{figures/SwissRoll1000_cuts.jpeg}
\caption{SwissRoll $d = 1000$ data skeleton regression results with the number of knots fixed as $70$ but segmented into varying numbers of disjoint components. The median SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::SwissRolld1000cut}
\end{figure}

%     \begin{subfigure}[t]{0.26\textwidth}
%         \centering
% \includegraphics[width=\linewidth]{figures/Swissd100_ncut_lspline.jpeg} 
%         \caption{S-Lspline}
%     \end{subfigure}
% %         \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/Swissd100_ncut_qspline.jpeg} 
% %         \caption{S-Qspline}
% %     \end{subfigure}
% %     \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/Swissd100_ncut_cspline.jpeg} 
% %         \caption{S-Cspline}
% %     \end{subfigure}\\
%         \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Swissd100_ncut_skelkernel.jpeg}
%         \caption{S-Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Swissd100_ncut_skelnn.jpeg}
%         \caption{S-kNN}
%     \end{subfigure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Penalized S-Lspline Empirical Results}
\label{sec:penalSplineSims}

~~~~In this section, we present the results of the S-Lspline regression with penalizations as introduced in Section \ref{sec:penalLspline}.
We use the same datasets as the simulations in Section \ref{sec::simulation} and follow the analysis procedure but with the regression methods to be the S-Lspline method with different types of penalties and varying penalization parameters $\lambda$.
Empirically, we observe that penalization does not improve the regression results of the S-Lspline model. 
This can be due to that the skeleton graph is already a summarizing presentation of the data with a concise structure, and the S-Lspline model based on the skeleton inherits the representational power and is not complex in nature, and hence penalization does not improve much for this method.

The results of the penalized S-Lspline methods on the Yinyang $d=1000$ data are summarized in Table \ref{table:Yinyangd1000PenalLspline} with the plots illustrating the effect of a varying number of knots and varying penalty parameters shown in Figure \ref{fig::Yinyangd1000PenalLspline}.

\begin{figure}
% \captionsetup{skip=1pt}
%%%%%%%%%%%%%%%%%%%%%%
\begin{tabular}{l l c l} 
 \hline
 Method & Median SSE ($5$\%, $95$\%)  & lambda & nknots \\ [1ex] 
 \hline
  No Penalization& 110.4  (105.6,  118.5)  & - & 38 \\ 
  Laplacian Smoothing Order 0 & 110.4 (105.6, 118.5)  & 0.001 & 38 \\ 
 Laplacian Smoothing Order 1 & 111.2 (104.4, 117.8) & 0.001 & 38\\
 Laplacian Smoothing Order 2 & 110.2 (104.7, 118.9) & 0.001& 38\\
 Trend Filtering Order 0 & 110.6 (105.6, 118.9) & 0.001 &38\\
 Trend Filtering Order 1 & 111.3 (104.4, 118.0) & 0.001 & 38 \\
  Trend Filtering Order 2 & 110.3 (104.7, 120.5) & 0.001 & 38 \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{S-Lspline regression results on Yinyang $d=1000$ data with varying penalties and parameters. The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$th percentile and $95$th percentile of the SSEs from the given parameter settings are reported in brackets.}
\label{table:Yinyangd1000PenalLspline}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%

\centering
\includegraphics[width=\textwidth]{figures/Yinyang_penalRidge.jpeg}\\
\includegraphics[width=\textwidth]{figures/Yinyang_penalLasso.jpeg}
\captionof{figure}{S-Lspline regression results on Yinyang $d=1000$ data with varying penalties and parameters. The median SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::Yinyangd1000PenalLspline}

\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The results of the penalized S-Lspline methods on the Noisy Yinyang $d=1000$ data are summarized in Table \ref{table:NoiseYinyangd1000PenalLspline} with plots in Figure \ref{fig::NoiseYinyangd1000PenalLspline}. We observe that adding penalization does not improve the regression results. 

\begin{figure}
%%%%%%%%%%%%%%%%%%%%%%
\begin{tabular}{l l c l} 
 \hline
 Method & Median SSE ($5$\%, $95$\%)  & lambda & nknots \\ [1ex] 
 \hline
  No penalization & 375.0 (354.7, 396.3) & - &  57  \\ 
 Laplacian Smoothing Order 0 & 377.0 (355.1,394.4)  & 0.001 & 42 \\ 
 Laplacian Smoothing Order 1 & 375.0 (354.7, 396.3) & 0.001 & 57\\
 Laplacian Smoothing Order 2 & 377.2 (355.9, 403.3) & 0.001& 71\\
 Trend Filtering Order 0 & 377.0 (355.1, 394.4) & 0.003 &42\\
 Trend Filtering Order 1 & 375.0 (354.7, 398.0) & 0.001 & 57 \\
  Trend Filtering Order 2 & 378.0,  (355.9, 399.0) & 0.001 & 42 \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{S-Lspline regression results on Noisy Yinyang $d=1000$ data with varying penalties and parameters. The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$th percentile and $95$th percentile of the SSEs from the given parameter settings are reported in brackets.}
\label{table:NoiseYinyangd1000PenalLspline}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
\includegraphics[width=\textwidth]{figures/NoiseYinyang_penalRidge.jpeg}\\
\includegraphics[width=\textwidth]{figures/NoiseYinyang_penalLasso.jpeg}
\captionof{figure}{S-Lspline regression results on Noisy Yinyang $d=1000$ data with varying penalties and parameters. The median SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::NoiseYinyangd1000PenalLspline}

\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The results of the penalized S-Lspline methods on the Swill Roll $d=1000$ data are summarized in Table \ref{table:SwissRolld1000PenalLspline} with plots in Figure \ref{fig::SwissRolld1000PenalLspline}. Including penalization terms for the linear spline model does not improve the regression results in this setting. 

\begin{figure}
%%%%%%%%%%%%%%%%%%%%%%
\begin{tabular}{l l c l} 
 \hline
 Method & Median SSE ($5$\%, $95$\%)  & lambda & nknots \\ [1ex] 
 \hline
  No penalization & 572.7 (521.0, 640.9) & - &  60  \\ 
 Laplacian Smoothing Order 0 & 573.4 (521.4, 632.6) & 0.001 & 60 \\ 
 Laplacian Smoothing Order 1 & 583.7 (524.7, 633.1) & 0.001 & 60\\
 Laplacian Smoothing Order 2 & 573.1 (521.5, 641.3) & 0.001& 60\\
 Trend Filtering Order 0 & 580.2 (524.6, 685.7)  & 0.01 &60\\
 Trend Filtering Order 1 & 583.7,  (524.6, 633.0) & 0.001 & 60 \\
  Trend Filtering Order 2 & 574.4,  (522.0,  657.1) & 0.001 & 60 \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{S-Lspline regression results on Swiss Roll $d=1000$ data with varying penalties and parameters. The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$th percentile and $95$th percentile of the SSEs from the given parameter settings are reported in brackets.}
\label{table:SwissRolld1000PenalLspline}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
\includegraphics[width=\textwidth]{figures/SwissRoll_penalRidge.jpeg}\\
\includegraphics[width=\textwidth]{figures/SwissRoll_penalLasso.jpeg}
\captionof{figure}{S-Lspline regression results on Swill Roll $d=1000$ data with varying penalties and parameters. The median SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::SwissRolld1000PenalLspline}

\end{figure}

Overall, we observe that adding penalization terms to the linear spline model based on the skeleton graph has minimal effect on the regression performance, and having no penalization actually gives the best results although by a very tiny margin.
We also test the penalized versions of the S-Lspline method on the real data examples (COIL-20 and SDSS datasets) and similarly observe that having penalization terms only leads to minimal effects on the regression performance much and the vanilla version of the S-Lspline can give the best result for most of the times.


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Low Dimensional Simulation Data Examples}
For the simulations in Section \ref{sec::simulation}, we add random Gaussian variables to make the datasets have a large dimension, and illustrate that the skeleton regression framework can have advantages in dealing with such large-dimensional datasets. 
In this section, we look instead at the low-dimensional settings where the datasets do not have noisy variables. We show that the skeleton-based regression methods can also have competitive performance in such settings compared to existing regression approaches.

\subsubsection{Yinyang Data d = 2}
\label{sec::Yinyangd2}

~~~~We use the same data generation mechanism as in Section \ref{sec::Yinyang} but without additional noisy variables (having $d = 2$).
We follow the same analysis procedure as in Section \ref{sec::Yinyang} and take the median, 5th percentile, and 95th percentile of the 5-fold cross-validation Sum of Squared Errors (SSEs) for each parameter setting of each method over the 100 simulated datasets. We present the smallest median SSE for each method in Table \ref{table:Yinyangd2} along with the corresponding best parameter setting. The plots illustrating the effect of varying the numbers of knots are included in Figure \ref{fig::Yinyangd2Numknots}.


\begin{figure}
%%%%%%%%%%%%%%%%%%%%%%
\begin{tabular}{c l c l} 
 \hline
 Method & Median SSE ($5$\%, $95$\%) & nknots & Parameter \\ [1ex] 
 \hline
 kNN & 60.1 (57.1, 63.0) & - & neighbor=36 \\ 
 Ridge & 1355.3  (1312.0,  1392.5) & & $\lambda  = 0.001$\\
 Lasso & 1354.8 (1311.4, 1391.9) & & $\lambda = 0.001$\\
 SpecSeries & 71.2  (67.2, 74.1) & - &bandwidth = 0.1\\
 Fast-KRR & 115.9 (108.4, 124.2) & - & $\sigma$ = 1\\
 S-Kernel & 60.4 (57.3,  64.7) & 63 & bandwidth = 4 $r_{hns}$ \\
 S-kNN & 60.9 (58.0, 63.9) & 76 & neighbor = 36 \\
 S-Lspline & 60.0 (57.0, 63.7) & 63 & -  \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on Yinyang $d=2$ data. The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$th percentile and $95$th percentile of the SSEs from the given parameter settings are reported in brackets.}
\label{table:Yinyangd2}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%

\centering
\includegraphics[width=\textwidth]{figures/Yinyang2_knots.jpeg}
\captionof{figure}{Yinyang $d = 2$ data regression results with varying number of knots. The median SSE across the $2$ simulated datasets with each given parameter setting is plotted.}
\label{fig::Yinyangd2Numknots}

\end{figure}


We observe that all the skeleton-based methods (S-Kernel, S-kNN, and S-Lspline) have performance comparable to the standard kNN in this setting. Ridge and Lasso regression, despite the regularization effect, resulted in relatively high SSEs. 
The SpecSeries method as a spectral approach and the Fast-KRR method as a kernel machine learning approach have improved performance compared to the classical Ridge and Lasso penalization regression methods, but do not give the top performances. This can be due to the underlying data structure being comprised of multiple disconnected components which diminishes the power of such manifold learning methods.
Therefore, the skeleton regression framework also gives competitive performance in datasets without noisy variables, but the advantage of skeleton-based methods is manifested more if the number of noisy variables in the input vector gets larger (see Appendix \ref{sec::Yinyang}).




%------------------------------------------------------------------------------------------------------------------------


\subsubsection{Noisy Yinyang Data d = 2}
\label{sec::NoisyYinyangd2}
~~~~We follow the same data generation mechanism as in Section \ref{sec::NoisyYinyang} to get Noisy Yinyang data without the additional variable dimensions and only have $d = 2$. 
We follow the same analysis procedure as in Section \ref{sec::NoisyYinyang} and take the median, 5th percentile, and 95th percentile of the 5-fold cross-validation Sum of Squared Errors (SSEs) for each parameter setting of each method over the 100 simulated datasets. We present the smallest median SSE for each method in Table \ref{table:NoiseYinyangd2} along with the corresponding best parameter setting, with the plots illustrating the effect of varying the numbers of knots included in Figure \ref{fig::Yinyangd1000Numknots}.


\begin{figure}
\centering
\begin{tabular}{c l c l} 
 \hline
 Method & Median SSE ($5$\%, $95$\%)  & Number of knots & Parameter \\ [1ex] 
 \hline
 kNN & 228.5 (213.0,  244.3) & -  & neighbor=6 \\ 
 Ridge & 1938.5  (1906.0, 1973.0) &- & $\lambda  = 0.005$\\
 Lasso &  1938.6 (1905.8, 1972.9) &- & $\lambda = 0.0016$\\
 SpecSeries & 243.9 (229.0, 259.0) & - & bandwidth = $0.1$\\
 Fast-KRR & 497.8 (475.3, 520.6) & - & $sigma$ = 1 \\
 S-Kernel & 239.0 (223.2, 254.4) & 226 & bandwidth = 4 $r_{hns}$ \\
 S-kNN & 250.7 (234.4,  264.7) & 226 & neighbor = 9 \\
 S-Lspline & 234.3 (218.5 249.8) & 226 &  - \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on Noisy Yinyang $d=2$ data.The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$ percentile and $95$ percentile of the SSEs from the given parameter settings are reported in brackets.}
\label{table:NoiseYinyangd2}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
\includegraphics[width=\textwidth]{figures/NoiseYinyang2_knots.jpeg}
\captionof{figure}{Noisy Yinyang $d = 1000$ data regression results with varying number of knots. The median SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::NoiseYinyangd2Numknots}

\end{figure}

% \YC{what do you mean by "do not cut the constructed skeleton"?}
% \JW{The number of disconnected components in the skeleton is 1.}

We observe similar patterns as shown in the Yinyang data that all the skeleton-based methods (S-Kernel, S-kNN, and S-Lspline) have performance comparable to the standard kNN in this setting. Ridge and Lasso regression methods result in relatively high SSEs, while the SpecSeries method and the Fast-KRR method have improved performance compared to the classical Ridge and Lasso penalization regression methods. 
Notably, the Spectral Series regression gives a top-level performance in this setting, and this can be due to the noisy observations adding a uniform density to the data space and making the structures in the data connected.



%----------------------------------------------------------------------------------------

\subsubsection{SwissRoll Data d = 3}
\label{sec::SwissRolld3}
~~~~We follow the same data generation mechanism as in Section \ref{sec::SwissRoll} but with $d = 3$ so that no random normal variables are added as additional features.
We present the smallest median SSE for each method in Table \ref{table:Swissd3} along with the corresponding best parameter setting, and the plots illustrating the effect of varying the numbers of knots are included in Figure \ref{fig::Yinyangd1000Numknots}.


\begin{figure}

\begin{tabular}{c l c l} 
 \hline
 Method & Median SSE ($5$\%, $95$\%)  & nknots & Parameter \\ [1ex] 
 \hline
 kNN & 309.1 (281.5, 342.1) & - & neighbor=6 \\ 
 Ridge &  1123.8  (1054.6,  1202.4) & -& $\lambda  = 0.00126$\\
 Lasso &  1123.3 (1053.9, 1201.3) & -& $\lambda = 0.0025$\\
 SpecSeries & 331.8 (307.3, 351.8) &- & bandwidth = $0.1$\\
 Fast-KRR & 563.8 (533.0, 598.6) &- & $\sigma = 1$\\
 S-Kernel & 348.1 (345.5, 365.3) & 320 & bandwidth = 8 $r_{hns}$ \\
 S-kNN & 363.5 (361.6, 406.7) & 320 & neighbor = 9 \\
 S-Lspline & 368.9 (329.4, 409.1) & 160 & - \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on SwissRoll $d=3$ data. The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$ percentile and $95$ percentile of the SSEs from the given parameter settings are reported in brackets.}
\label{table:Swissd3}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
\includegraphics[width=\textwidth]{figures/SwissRoll3_knots.jpeg}
\captionof{figure}{SwissRoll $d = 3$ data regression results with varying number of knots. The median SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::SwissRolld3}

\end{figure}

The kNN method has the best performance in this setting, and the skeleton-based methods have comparable performance.
Note that the Spectral Series approach has performance slightly better than the skeleton-based methods in this case, which does corroborate its effectiveness in dealing with connected and smooth manifolds.



% \section{Additional Real Data Examples}
% \label{sec::extraReal}

% ~~~~In this section, we present results on some additional examples from the COIL-20 dataset \cite{COIL20}, following the same procedure as in Section \ref{sec::cup}.
% Each dataset consists of $72$ gray-scale images of size $128\times 128$ pixels as 2D projections of a 3D object obtained through rotating the object by $72$ equispaced angles on a single axis.
% The response is the angle of rotation, and to avoid the circular response issue, we remove the last $8$ images from the sequence and only use the first $64$ images from each dataset. 
% We use leave-one-out cross-validation to assess the performance of each method.

% \subsection{Lucky Cat Data} \label{sec::luckycat}

% ~~~~This dataset consists of gray-scale images from rotating a lucky cat by equispaced angles on a single axis.
% Several examples of the images are given in Figure \ref{fig::luckycat}. 
% Following the same analysis procedure from Section \ref{sec::cup}, we use leave-one-out cross-validation and the best SSE from each method is listed in Table \ref{table:luckycat} along with the corresponding parameters.

% \begin{figure}
% \centering
%     \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj4__0.png} 
%     \end{subfigure}
%         \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj4__12.png}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj4__24.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj4__36.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj4__48.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj4__60.png}
%     \end{subfigure}
% \captionof{figure}{A part of the lucky cat images from the COIL-20 processed dataset. Each image
% is of size $128\time 128$ pixels.}
% \label{fig::luckycat}
% \vspace{2em}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \centering
% \begin{tabular}{c l l} 
%  \hline
%  Method & SSE &  Parameter \\ [1ex] 
%  \hline
%  kNN & 175.0 & neighbor=2 \\ 
%  Ridge& -&-\\
%  Lasso& -&-\\
%  SpecSeries& -&-\\
%  Fast-KRR & -&-\\
%  S-Kernel & 551.4 &  bandwidth = 0.125$r_{hns}$ \\
%  S-kNN& 456.3 & neighbor = 2\\
%  S-Lspline &  338.1  & - \\[1ex] 
%  \hline
% \end{tabular}
% \captionof{table}{Regression results on Lucky Cat data from COIL-20. The best SSE from each method is listed with the corresponding parameters used.}
% \label{table:luckycat}

% \end{figure}


% We observe that the kNN method gives outstanding performance on this real-world data. 
% The salient regions of black regions of the lucky cat make Euclidean distance between the pixel vectors reliable for this series of images, and hence the kNN method can always correctly identify the two closest neighboring images and has the correct predictions and only give out errors at the beginning and ending parts of the sequence of images.
% Among the skeleton-based methods, the S-Lspline method gives the best SSE result, while the S-Kernel method using kernel smoothing on the skeleton-based distance gives a result worse than the other methods, which is similarly observed for the cup data in Section \ref{sec::cup}.


% We also note that the S-Cspline method gives an extraordinarily bad performance and indicates extra caution is needed for higher-order splines.


%---------------------------------------------------------

% \subsubsection{Piggy Bank Images Data}
% We look at another sequence of images taken around a piggy bank, with some examples in Figure \ref{fig::pigbank}.
% The best SSE from each method is listed in Table \ref{table:pigbank} along with the corresponding parameters. 
% We see that the S-Lspline method gives the best performance in terms of SSE, while the usual kNN regressor also performs well on this data.

% \begin{figure}
% \captionsetup{skip=1pt}
% \centering
%     \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__0.png} 
%     \end{subfigure}
%         \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__12.png}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__24.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__36.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__48.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__60.png}
%     \end{subfigure}
% \caption{A part of the piggy bank images from the COIL-20 processed dataset. Each image
% is of size $128\time 128$ pixels.}
% \label{fig::pigbank}
% \end{figure}

% \begin{table}
% \centering
% \begin{tabular}{||c c c||} 
%  \hline
%  Method & SSE &  Parameter \\ [0.5ex] 
%  \hline\hline
%  kNN & 888.9 & neighbor=3 \\ 
%  Ridge& -&-\\
%  Lasso& -&-\\
%  SpecSeries& -&-\\
%  S-Kernel & 2948.1 &  bandwidth = 2$r_{hns}$ \\
%  S-kNN& 5050.0 & nneighbor = 6\\
%  S-Lspline &  1251.1  &- \\[1ex] 
%  \hline
% \end{tabular}
% \caption{Regression results on piggy bank images from COIL-20. The best SSE from each method is listed with the corresponding parameters used.}
% \label{table:pigbank}
% \end{table}




%-------------------------------------------------------------
% \subsection{Sauce Box Image Data}
% ~~~~We look at another sequence of images taken around a sauce box, with some example images in Figure \ref{fig::sauce}.
% The best SSE from each method is listed in Table \ref{table:sauce} along with the corresponding parameters. 
% In this case, the usual kNN regressor gives the best performance in terms of SSE, while the S-Lspline method gives satisfactory results. 
% The good performance of the kNN regressor can be due to the distinctive marks on the box, which makes neighbor search through Euclidean distance on the vectorized image inputs effective.
% However, the S-Lspline method explicitly models the latent structure of the data and can give a more structured model of the response.
% Among the skeleton-based methods, the S-Kernel and S-kNN methods using the skeleton-based distance give worse results, which is similarly observed for the cup data in Section \ref{sec::cup}.


% \begin{figure}
% \centering
%     \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj20__0.png} 
%     \end{subfigure}
%         \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj20__12.png}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj20__24.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj20__36.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj20__48.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj20__60.png}
%     \end{subfigure}
% \captionof{figure}{A part of the sauce images from the COIL-20 processed dataset. Each image
% is of size $128\time 128$ pixels.}
% \label{fig::sauce}
% \vspace{2em}
% %%%%%%%%%%%%%%%%%%%%%%%%%%
% \centering
% \begin{tabular}{c c l} 
%  \hline
%  Method & SSE &  Parameter \\ [1ex] 
%  \hline
%  kNN & 931.3 & neighbor=2 \\ 
%   Ridge& -&-\\
%  Lasso& -&-\\
%  SpecSeries& -&-\\
%  Fast-KRR & -&-\\
%  S-Kernel & 1996.3 &  bandwidth = 0.5$r_{hns}$ \\
%  S-kNN& 2450.0 & neighbor = 1\\
%  S-Lspline &  1220.1  &- \\[1ex] 
%  \hline
% \end{tabular}
% \captionof{table}{Regression results on sauce images data from COIL-20. The best SSE from each method is listed with the corresponding parameters used.}
% \label{table:sauce}

% \end{figure}


\end{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements                         %%
%% should be provided in the                %%
%% Acknowledgements section.                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{acks}[Acknowledgments]
%The authors would like to thank the anonymous referees, an Associate
%Editor and the Editor for their constructive comments that improved the
%quality of this paper.

%\end{acks}

% Acknowledgements should go at the end, before appendices and references




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Funding information, if any,             %%
%% should be provided in the                %%
%% funding section.                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{funding}
Jerry Wei is supported by NSF Grant DMS - 2112907.
Yen-Chi Chen is supported by NSF DMS-195278, 2112907, 2141808, and NIH U24-AG072122.
\end{funding}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary Material, including data   %%
%% sets and code, should be provided in     %%
%% {supplement} environment with title      %%
%% and short description. It cannot be      %%
%% available exclusively as external link.  %%
%% All Supplementary Material must be       %%
%% available to the reader on Project       %%
%% Euclid with the published article.       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{supplement}
\stitle{Code for Empirical Results}
\sdescription{We include the essential R code files and data files in the code supplement.
We also include a convenient R package implementing the skeleton regression methods.
The R package can also be found online at \url{https://github.com/JerryBubble/skeletonMethods} with instructions and examples, and the Python package implementation can be found at \url{https://pypi.org/project/skeleton-methods/}.}
\end{supplement}
% \begin{supplement}
% \stitle{Title of Supplement B}
% \sdescription{Short description of Supplement B.}
% \end{supplement}


%%%%% Reference-------------------------------------------------------
\newpage
\bibliographystyle{imsart-nameyear}
\bibliography{skelreg.bib}


\end{document}