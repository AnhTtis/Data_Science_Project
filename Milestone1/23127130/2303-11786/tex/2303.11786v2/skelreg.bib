%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/
@article{ConvergenceinWasserstein,
author = {Jonathan Weed and Francis Bach},
title = {{Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance}},
volume = {25},
journal = {Bernoulli},
number = {4A},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
pages = {2620 -- 2648},
keywords = {Optimal transport, quantization, Wasserstein metrics},
year = {2019},
doi = {10.3150/18-BEJ1065},
URL = {https://doi.org/10.3150/18-BEJ1065}
}

@inproceedings{LearningProbabilityOTMetric,
 author = {Canas, Guillermo and Rosasco, Lorenzo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Probability Measures with respect to Optimal Transport Metrics},
 volume = {25},
 year = {2012}
}

@article{EstimateReach,
author = {Eddie Aamari and Jisu Kim and Fr{\'e}d{\'e}ric Chazal and Bertrand Michel and Alessandro Rinaldo and Larry Wasserman},
title = {{Estimating the reach of a manifold}},
volume = {13},
journal = {Electronic Journal of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {1359 -- 1399},
keywords = {Geometric inference, minimax risk, reach},
year = {2019},
doi = {10.1214/19-EJS1551},
URL = {https://doi.org/10.1214/19-EJS1551}
}




@InProceedings{GraphLaplacianSmoothing,
author="Smola, Alexander J.
and Kondor, Risi",
editor="Sch{\"o}lkopf, Bernhard
and Warmuth, Manfred K.",
title="Kernels and Regularization on Graphs",
booktitle="Learning Theory and Kernel Machines",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="144--158",
abstract="We introduce a family of kernels on graphs based on the notion of regularization operators. This generalizes in a natural way the notion of regularization and Greens functions, as commonly used for real valued functions, to graphs. It turns out that diffusion kernels can be found as a special case of our reasoning. We show that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators.",
isbn="978-3-540-45167-9"
}

@article{SolutionPathLasso,
author = {Ryan J. Tibshirani and Jonathan Taylor},
title = {{The solution path of the generalized lasso}},
volume = {39},
journal = {The Annals of Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {1335 -- 1371},
keywords = {Degrees of freedom, Lagrange dual, LARS, Lasso, path algorithm},
year = {2011},
doi = {10.1214/11-AOS878},
URL = {https://doi.org/10.1214/11-AOS878}
}


%% Saved with string encoding Unicode (UTF-8)
@inproceedings{DelaunayComplexity,
	abstract = {We show that the Delaunay triangulation of a set of points distributed nearly uniformly on a polyhedron (not necessarily convex) of dimension p in d-dimensional space is O(n(d-1)/p). For all 2 ≤ p ≤ d - 1, this improves on the well-known worst-case bound of O(n⌈d/2⌉).},
	address = {USA},
	author = {Amenta, Nina and Attali, Dominique and Devillers, Olivier},
	booktitle = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
	isbn = {9780898716245},
	location = {New Orleans, Louisiana},
	numpages = {8},
	pages = {1106--1113},
	publisher = {Society for Industrial and Applied Mathematics},
	series = {SODA '07},
	title = {Complexity of Delaunay Triangulation for Points on Lower-Dimensional Polyhedra},
	year = {2007}}
 
 @article{Chazelle1993,
	abstract = {We present a deterministic algorithm for computing the convex hull of n points in Ed in optimal O(n log n+n{\{}down left corner{\}}d/2{\{}down right corner{\}} ) time. Optimal solutions were previously known only in even dimension and in dimension 3. A by-product of our result is an algorithm for computing the Voronoi diagram of n points in d-space in optimal O(n log n+n{\{}top left corner{\}}d/2{\{}top right corner{\}} ) time. {\textcopyright} 1993 Springer-Verlag New York Inc.},
	author = {Chazelle, Bernard},
	doi = {10.1007/BF02573985},
	file = {:C$\backslash$:/Users/Owner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chazelle - 1993 - An optimal convex hull algorithm in any fixed dimension.pdf:pdf},
	journal = {Discrete {\&} Computational Geometry},
	keywords = {Combinatorics,Computational Mathematics and Numerical Analysis},
	mendeley-groups = {ImprovedKMeans},
	month = {dec},
	number = {1},
	pages = {377--409},
	publisher = {Springer-Verlag},
	title = {{An optimal convex hull algorithm in any fixed dimension}},
	year = {1993},
	Bdsk-Url-1 = {https://link.springer.com/article/10.1007/BF02573985},
	Bdsk-Url-2 = {https://doi.org/10.1007/BF02573985}}
 
@article{loh2014fifty,
  title={Fifty years of classification and regression trees},
  author={Loh, Wei-Yin},
  journal={International Statistical Review},
  volume={82},
  number={3},
  pages={329--348},
  year={2014},
  publisher={Wiley Online Library}
}

@book{breiman2017classification,
  title={Classification and regression trees},
  author={Leo Breiman and Jerome Friedman and R.A. Olshen and Charles J. Stone},
  year={1984},
  publisher={ Chapman and Hall/CRC}
}

@article{torrence1998practical,
  title={A practical guide to wavelet analysis},
  author={Torrence, Christopher and Compo, Gilbert P},
  journal={Bulletin of the American Meteorological society},
  volume={79},
  number={1},
  pages={61--78},
  year={1998},
  publisher={American Meteorological Society}
}

@article{wahba1975smoothing,
  title={Smoothing noisy data with spline functions},
  author={Wahba, Grace},
  journal={Numerische mathematik},
  volume={24},
  number={5},
  pages={383--393},
  year={1975},
  publisher={Springer}
}

@book{wang2011smoothing,
  title={Smoothing splines: methods and applications},
  author={Wang, Yuedong},
  year={2011},
  publisher={CRC press}
}

@book{wasserman2006all,
  title={All of nonparametric statistics},
  author={Wasserman, Larry},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@book{fan2018local,
  title={Local polynomial modelling and its applications},
  author={Fan, Jianqing and Gijbels, Irene},
  year={2018},
  publisher={Routledge}
}

@article{alam2015eleventh,
  title={The eleventh and twelfth data releases of the Sloan Digital Sky Survey: final data from SDSS-III},
  author={Alam, Shadab and Albareti, Franco D and Prieto, Carlos Allende and Anders, Friedrich and Anderson, Scott F and Anderton, Timothy and Andrews, Brett H and Armengaud, Eric and Aubourg, {\'E}ric and Bailey, Stephen and others},
  journal={The Astrophysical Journal Supplement Series},
  volume={219},
  number={1},
  pages={12},
  year={2015},
  publisher={IOP Publishing}
}

@article{york2000sloan,
  title={The sloan digital sky survey: Technical summary},
  author={York, Donald G and Adelman, J and Anderson Jr, John E and Anderson, Scott F and Annis, James and Bahcall, Neta A and Bakken, JA and Barkhouser, Robert and Bastian, Steven and Berman, Eileen and others},
  journal={The Astronomical Journal},
  volume={120},
  number={3},
  pages={1579},
  year={2000},
  publisher={IOP Publishing}
}

@article{rahman2015clustering,
  title={Clustering-based redshift estimation: comparison to spectroscopic redshifts},
  author={Rahman, Mubdi and M{\'e}nard, Brice and Scranton, Ryan and Schmidt, Samuel J and Morrison, Christopher B},
  journal={Monthly Notices of the Royal Astronomical Society},
  volume={447},
  number={4},
  pages={3500--3511},
  year={2015},
  publisher={Oxford University Press}
}

@article{morrison2017wizz,
  title={The-wiZZ: Clustering redshift estimation for everyone},
  author={Morrison, Christopher B and Hildebrandt, Hendrik and Schmidt, Samuel J and Baldry, Ivan K and Bilicki, Maciej and Choi, Ami and Erben, Thomas and Schneider, Peter},
  journal={Monthly Notices of the Royal Astronomical Society},
  volume={467},
  number={3},
  pages={3576--3589},
  year={2017},
  publisher={Oxford University Press}
}

@InProceedings{MPSN2021,
  title = 	 {Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks},
  author =       {Bodnar, Cristian and Frasca, Fabrizio and Wang, Yuguang and Otter, Nina and Montufar, Guido F and Li{\'o}, Pietro and Bronstein, Michael},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1026--1037},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/bodnar21a/bodnar21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/bodnar21a.html},
  abstract = 	 {The pairwise interaction paradigm of graph machine learning has predominantly governed the modelling of relational systems. However, graphs alone cannot capture the multi-level interactions present in many complex systems and the expressive power of such schemes was proven to be limited. To overcome these limitations, we propose Message Passing Simplicial Networks (MPSNs), a class of models that perform message passing on simplicial complexes (SCs). To theoretically analyse the expressivity of our model we introduce a Simplicial Weisfeiler-Lehman (SWL) colouring procedure for distinguishing non-isomorphic SCs. We relate the power of SWL to the problem of distinguishing non-isomorphic graphs and show that SWL and MPSNs are strictly more powerful than the WL test and not less powerful than the 3-WL test. We deepen the analysis by comparing our model with traditional graph neural networks (GNNs) with ReLU activations in terms of the number of linear regions of the functions they can represent. We empirically support our theoretical claims by showing that MPSNs can distinguish challenging strongly regular graphs for which GNNs fail and, when equipped with orientation equivariant layers, they can improve classification accuracy in oriented SCs compared to a GNN baseline.}
}

@article{Peter2018,
title	= {Relational inductive biases, deep learning, and graph networks},
author	= {Peter Battaglia and Jessica Blake Chandler Hamrick and Victor Bapst and Alvaro Sanchez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andy Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Jayne Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
year	= {2018},
URL	= {https://arxiv.org/pdf/1806.01261.pdf},
journal	= {arXiv}
}


@ARTICLE{Bouritsas2021,
  author={Bouritsas, Giorgos and Frasca, Fabrizio and Zafeiriou, Stefanos P and Bronstein, Michael},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting}, 
  year={2022},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TPAMI.2022.3154319}}
  
@inproceedings{
xu2018how,
title={How Powerful are Graph Neural Networks?},
author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ryGs6iA5Km},
}

@inproceedings{Chamberlain2021,
 author = {Chamberlain, Benjamin and Rowbottom, James and Eynard, Davide and Di Giovanni, Francesco and Dong, Xiaowen and Bronstein, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1594--1609},
 publisher = {Curran Associates, Inc.},
 title = {Beltrami Flow and Neural Diffusion on Graphs},
 url = {https://proceedings.neurips.cc/paper/2021/file/0cbed40c0d920b94126eaf5e707be1f5-Paper.pdf},
 volume = {34},
 year = {2021}
}


@inproceedings{Petar2018graph,
title={Graph Attention Networks},
author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJXMpikCZ},
}

@misc{Bronstein2021,
  doi = {10.48550/ARXIV.2104.13478},
  url = {https://arxiv.org/abs/2104.13478},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computational Geometry (cs.CG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{GRAND,
  title = 	 {GRAND: Graph Neural Diffusion},
  author =       {Chamberlain, Ben and Rowbottom, James and Gorinova, Maria I and Bronstein, Michael and Webb, Stefan and Rossi, Emanuele},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1407--1418},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/chamberlain21a/chamberlain21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/chamberlain21a.html},
  abstract = 	 {We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks.}
}


@ARTICLE{GeometricDL,  author={Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},  journal={IEEE Signal Processing Magazine},   title={Geometric Deep Learning: Going beyond Euclidean data},   year={2017},  volume={34},  number={4},  pages={18-42},  doi={10.1109/MSP.2017.2693418}}


@ARTICLE{DGM,  author={Kazi, Anees and Cosmo, Luca and Ahmadi, Seyed-Ahmad and Navab, Nassir and Bronstein, Michael},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Differentiable Graph Module (DGM) for Graph Convolutional Networks},   year={2022},  volume={},  number={},  pages={1-1},  doi={10.1109/TPAMI.2022.3170249}}

@article{Yue2019,
author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
title = {Dynamic Graph CNN for Learning on Point Clouds},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3326362},
doi = {10.1145/3326362},
abstract = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS.},
journal = {ACM Trans. Graph.},
month = {oct},
articleno = {146},
numpages = {12},
keywords = {segmentation, Point cloud, classification}
}

@misc{Chen2017,
  doi = {10.48550/ARXIV.1704.03924},
  author = {Chen, Yen-Chi},
  
  keywords = {Methodology (stat.ME), Other Statistics (stat.OT), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Tutorial on Kernel Density Estimation and Recent Advances},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{li2007nonparametric,
  title={Nonparametric Econometrics: Theory and Practice},
  author={Li, Q. and Racine, J.S.},
  isbn={9780691121611},
  lccn={2006044989},
  url={https://books.google.com/books?id=BI\_PiWazY0YC},
  year={2007},
  publisher={Princeton University Press}
}


@inproceedings{clusterSim,
	author = {Marek Walesiak and Andrzej Dudek},
	booksubtitle = {Proceedings of the 35th International Business Information Management Association Conference (IBIMA), 1-2 April 2020, Seville, Spain},
	booktitle = {Education Excellence and Innovation Management: A 2025 Vision to Sustain Economic Development During Global Challenges},
	editor = {Khalid S. Soliman},
	isbn = {978-0-9998551-4-1},
	pages = {325-340},
	publisher = {International Business Information Management Association (IBIMA)},
	title = {The Choice of Variable Normalization Method in Cluster Analysis},
	year = {2020}}

@book{silverman1986density,
	author = {Silverman, Bernard W},
	publisher = {CRC press},
	title = {Density estimation for statistics and data analysis},
	volume = {26},
	year = {1986}}
	

@article{skelclus,
author = {Zeyu Wei and Yen-Chi Chen},
title = {Skeleton Clustering: Dimension-Free Density-Aided Clustering},
journal = {Journal of the American Statistical Association},
volume = {0},
number = {ja},
pages = {1-30},
year  = {2023},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2023.2174122},
eprint = { 
    
        https://doi.org/10.1080/01621459.2023.2174122
}
}
	

@InProceedings{Green2021,
  title = 	 { Minimax Optimal Regression over Sobolev Spaces via Laplacian Regularization on Neighborhood Graphs },
  author =       {Green, Alden and Balakrishnan, Sivaraman and Tibshirani, Ryan},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2602--2610},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/green21a/green21a.pdf},
  abstract = 	 { In this paper we study the statistical properties of Laplacian smoothing, a graph-based approach to nonparametric regression. Under standard regularity conditions, we establish upper bounds on the error of the Laplacian smoothing estimator \smash{$\widehat{f}$}, and a goodness-of-fit test also based on \smash{$\widehat{f}$}. These upper bounds match the minimax optimal estimation and testing rates of convergence over the first-order Sobolev class $H^1(\mathcal{X})$, for $\mathcal{X} \subseteq \mathbb{R}^d$ and $1 \leq d &lt; 4$; in the estimation problem, for $d = 4$, they are optimal modulo a $\log n$ factor. Additionally, we prove that Laplacian smoothing is manifold-adaptive: if $\mathcal{X} \subseteq \mathbb{R}^d$ is an $m$-dimensional manifold with $m &lt; d$, then the error rate of Laplacian smoothing (in either estimation or testing) depends only on $m$, in the same way it would if $\mathcal{X}$ were a full-dimensional set in $\mathbb{R}^m$. }
}


@INPROCEEDINGS{Bertin-Mahieux2011,
  author = {Thierry Bertin-Mahieux and Daniel P.W. Ellis and Brian Whitman and Paul Lamere},
  title = {The Million Song Dataset},
  booktitle = {{Proceedings of the 12th International Conference on Music Information
	Retrieval ({ISMIR} 2011)}},
  year = {2011},
  owner = {thierry},
  timestamp = {2010.03.07}
}	

@article{Wainwright2012,
   author = {P J Wainwright},
   doi = {10.1214/12-AOS1018},
   issue = {3},
   journal = {The Annals of Statistics},
   keywords = {M-estimation Disciplines Disciplines Statistics and Probability,high-dimensional statistics,missing data,nonconvexity,regularization,sparse linear regression},
   pages = {1637-1664},
   title = {High-Dimensional Regression With Noisy and Missing Data: Provable Guarantees With Nonconvexity},
   volume = {40},
   year = {2012},
}

@article{Bailer-jones,
author = {Bailer-jones, Coryn},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/basis_expansions.pdf:pdf},
mendeley-groups = {Skelreg},
title = {{Last time ... This week Bias-Variance decomposition Bias-variance trade-off Bias and variance Basis expansions Splines ( one-dimensional ) Idea behind splines is that region to fit is split into subregions}}
}

@article{Statistics2008,
author = {Statistics, Mathematical},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/ConsistentNonparametricRegression.pdf:pdf},
mendeley-groups = {Skelreg},
number = {4},
pages = {595--620},
title = {{Consistent Nonparametric Regression Author ( s ): Charles J . Stone Source : The Annals of Statistics , Vol . 5 , No . 4 ( Jul ., 1977 ), pp . 595-620 Published by : Institute of Mathematical Statistics Stable URL : http://www.jstor.org/stable/2958783}},
volume = {5},
year = {2008}
}
@book{DistributionFreeThoery,
   author = {László Györfi and Michael Kohler and Adam Krzyżak and Harro Walk},
   city = {New York, NY},
   doi = {10.1007/B97848},
   isbn = {978-0-387-95441-7},
   publisher = {Springer New York},
   title = {A Distribution-Free Theory of Nonparametric Regression},
   url = {http://link.springer.com/10.1007/b97848},
   year = {2002},
}


@article{Lin2017,
abstract = {We propose an extrinsic regression framework for modeling data with manifold valued responses and Euclidean predictors. Regression with manifold responses has wide applications in shape analysis, neuroscience, medical imaging, and many other areas. Our approach embeds the manifold where the responses lie onto a higher dimensional Euclidean space, obtains a local regression estimate in that space, and then projects this estimate back onto the image of the manifold. Outside the regression setting both intrinsic and extrinsic approaches have been proposed for modeling iid manifold-valued data. However, to our knowledge our work is the first to take an extrinsic approach to the regression problem. The proposed extrinsic regression framework is general, computationally efficient, and theoretically appealing. Asymptotic distributions and convergence rates of the extrinsic regression estimates are derived and a large class of examples is considered indicating the wide applicability of our approach. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1508.02201},
author = {Lin, Lizhen and {St. Thomas}, Brian and Zhu, Hongtu and Dunson, David B.},
doi = {10.1080/01621459.2016.1208615},
eprint = {1508.02201},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/Extrinsic Local Regression on Manifold Valued Data.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Convergence rate,Differentiable manifold,Geometry,Local regression,Object data,Shape statistics},
mendeley-groups = {Skelreg},
number = {519},
pages = {1261--1273},
publisher = {Taylor & Francis},
title = {{Extrinsic Local Regression on Manifold-Valued Data}},
url = {https://doi.org/10.1080/01621459.2016.1208615},
volume = {112},
year = {2017}
}
@article{Fan1992,
author = {Fan, Jianqing and Fan, Jianqing},
keywords = {boundary effects,kernel estimator,linear smoother,local linear regression,minimax efficiency},
number = {420},
pages = {998--1004},
title = {{Design-adaptive Nonparametric Regression}},
journal={Journal of the American Statistical Association},
volume = {87},
year = {1992}
}
@article{Fan1996,
abstract = {A decisive question in nonparametric smoothing techniques is the choice of the bandwidth or smoothing parameter. The present paper addresses this question when using local polynomial approximations for estimating the regression function and its derivatives. A fully-automatic bandwidth selection procedure has been proposed by Fan and Gijbels (1995a), and the empirical performance of it was tested in detail via a variety of examples. Those experiences supported the methodology towards a great extend. In this paper we establish asymptotic results for the proposed variable bandwidth selector. We provide the rate of convergence of the bandwidth estimate, and obtain the asymptotic distribution of its error relative to the theoretical optimal variable bandwidth. These asymptotic properties give extra support to the proposed bandwidth selection procedure. It is also demonstrated how the proposed selection method can be applied in the density estimation setup. Some examples illustrate this application.},
author = {Fan, Jianqing and Gijbels, Ir{\`{e}}ne and Hu, Tien Chung and Huang, Li Shan},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/Fan1996.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {Assessment of bias and variance,Asymptotic normality,Binning,Density estimation,Local polynomial fitting,Variable bandwidth selector},
mendeley-groups = {Skelreg},
number = {1},
pages = {113--127},
title = {{A study of variable bandwidth selection for local polynomial regression}},
volume = {6},
year = {1996}
}
@book{,
abstract = {Cosmetic products are generally formulated as emulsions, ointments, solutions or powders containing active ingredients. According to EU legislation, a cosmetic product is "any substance or preparation intended to be placed in contact with the various external parts of the human body with a view exclusively or mainly to cleaning, perfuming them, changing their appearance, and/or correcting body odors and/or protecting them or keeping them in good conditions". However, science advancement in both active carriers and ingredients has streamlined the process through which many cosmetic products by their delivery systems can induce modifications on the skin physiology. This is the reason why Reed and Kligman redefined these products as "cosmeceuticals", which refers to the combination of cosmetics and pharmaceuticals. Until recently, the term of cosmeceuticals has not had legal significance. The so-called cosmeceuticals, in fact, may induce modifications on the skin physiology, modifying, for example, transepidermal water loss, keratinocytes cohesion and turnover, modulating the inflammatory cascade, and/or altering the surface microbiota by the activity of the preservatives content. For these reasons, they are claimed to have medical or drug-like benefits. Naturally, their effectiveness on minor skin disorders or mild skin abnormalities has to be shown by in vitro and in vivo studies. On the other hand, their formulations contain emulsifiers, preservatives, and other chemicals which, by their cumulative use, may provoke side effects, such as allergic and/or sensitization phenomena. Moreover, many ingredients and packaging for such products are not biodegradable. In this study, we would like to introduce an innovative category of cosmeceuticals made by biodegradable nonwoven tissues. These cosmeceutical tissues, produced through the use of natural fibers, may bind different active ingredients and therefore become effective as antibacterial, anti-inflammatory, sun-protective, whitening, or anti-aging products, depending on the ingredient(s) used. Differently from the usual cosmetics, they do not contain preservatives, emulsifiers, colors, and other chemicals. They can be applied as dried tissue on wet skin, remaining in loco for around 30 min, slowly releasing the active ingredients entrapped into the fibers. It is interesting to underline that the tissue, acting as a carrier, has its own effectiveness via chitin and lignin polymers with an antibacterial and anti-inflammatory activity. When hydrolyzed by the human microbiota enzymes, they give rise to ingredients used as cell nourishment or energy. This paper will review part of the scientific research results, supporting this new category of biodegradable cosmetic products known as facial mask sheets.},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/Herman_J. Bierens_TopicsinAdvancedEconometrics.pdf:pdf},
isbn = {9780521419000},
mendeley-groups = {Skelreg},
title = {{No Titleالبترول}}
}
@article{Buja2007,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy bynhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {Buja, Andreas and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1214/aos/1176347115},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/LinearSmootherAdditiveModels.pdf:pdf},
issn = {2168-8966},
journal = {The Annals of Statistics},
mendeley-groups = {Skelreg},
number = {2},
pages = {453--510},
title = {{Linear Smoothers and Additive Models}},
volume = {17},
year = {2007}
}
@article{Gijbels1992,
author = {Gijbels, Jianqing Fan and Irene},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/LocalLinearRegressionSmoothers.pdf:pdf},
journal = {The Annals of Statistics},
mendeley-groups = {Skelreg},
number = {4},
pages = {2008--2036},
title = {{Variable Bandwidth and Local Linear Regression Smoothers Author ( s ): Jianqing Fan and Irene Gijbels Source : The Annals of Statistics , Vol . 20 , No . 4 , ( Dec ., 1992 ), pp . 2008-2036 Published by : Institute of Mathematical Statistics Stable URL :}},
volume = {20},
year = {1992}
}
@article{Wand2001,
abstract = {This book, and the associated software, have grown out of the author's work in the ﬁeld of local regression over the past several years. The book is designed to be useful for both theoretical work and in applications. Most chapters contain distinct sections introducing methodology, computing and practice, and theoretical results. The methodological and practice sections should be accessible to readers with a sound background in statistical methods and in particular regression, for example at the level of Draper and Smith (1981). The theoretical sections require a greater understanding of calculus, matrix algebra and real analysis, generally at the level found in advanced undergraduate courses. Applications are given from a wide variety of ﬁelds, ranging from actuarial science to sports. The extent, and relevance, of early work in smoothing is not widely appreciated, even within the research community. Chapter 1 attempts to redress the problem. Many ideas that are central to modern work on smoothing: local polynomials, the bias-variance trade-oﬀ, equivalent kernels, likelihood models and optimality results can be found in literature dating to the late nineteenth and early twentieth centuries. The core methodology of this book appears in Chapters 2 through 5. These chapters introduce the local regression method in univariate and multivariate settings, and extensions to local likelihood and density estimation. Basic theoretical results and diagnostic tools such as cross validation are introduced along the way. Examples illustrate the implementation of the methods using the locfit software. The remaining chapters discuss a variety of applications and advanced topics: classiﬁcation, survival data, bandwidth selection issues, computa- tion and asymptotic theory. Largely, these chapters are independent of each other, so the reader can pick those of most interest. Most chapters include a short set of exercises. These include theoretical results; details of proofs; extensions of the methodology; some data analysis examples and a few research problems. But the real test for the methods is whether they provide useful answers in applications. The best exercise for every chapter is to ﬁnd datasets of interest, and try the methods out! The literature on mathematical aspects of smoothing is extensive, and coverage is necessarily selective. I attempt to present results that are of most direct practical relevance. For example, theoretical motivation for standard error approximations and conﬁdence bands is important; the reader should eventually want to know precisely what the error estimates represent, rather than simply asuming software reports the right answers (this applies to any model and software; not just local regression and locfit!). On the other hand, asymptotic methods for boundary correction receive no coverage, since local regression provides a simpler, more intuitive and more general approach to achieve the same result. Along with the theory, we also attempt to introduce understanding of the results, along with their relevance. Examples of this include the discussion of non-identiﬁability of derivatives (Section 6.1) and the problem of bias estimation for conﬁdence bands and bandwidth selectors (Chapters 9 and 10).},
author = {Wand, M. P},
doi = {10.1198/jasa.2001.s373},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/LocalRegressionBook_1999.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
mendeley-groups = {Skelreg},
number = {453},
pages = {339--355},
title = {{Local Regression and Likelihood}},
volume = {96},
year = {2001}
}
@article{Aanjaneya2012,
abstract = {Many real-world data sets can be viewed of as noisy samples of special types of metric spaces called metric graphs. 19 Building on the notions of correspondence and Gromov-Hausdorff distance in metric geometry, we describe a model for such data sets as an approximation of an underlying metric graph. We present a novel algorithm that takes as an input such a data set, and outputs a metric graph that is homeomorphic to the underlying metric graph and has bounded distortion of distances. We also implement the algorithm, and evaluate its performance on a variety of real world data sets. {\textcopyright} 2012 World Scientific Publishing Company.},
author = {Aanjaneya, Mridul and Chazal, Frederic and Chen, Daniel and Glisse, Marc and Guibas, Leonidas and Morozov, Dmitriy},
doi = {10.1142/S0218195912600072},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/MetricGraphReconstruct.pdf:pdf},
issn = {02181959},
journal = {International Journal of Computational Geometry and Applications},
keywords = {Reconstruction,inference,metric graph,noise},
mendeley-groups = {Skelreg},
number = {4},
pages = {305--325},
title = {{Metric graph reconstruction from noisy data}},
volume = {22},
year = {2012}
}
@article{TeunterRHandDuncan2006,
abstract = {I argue that the impact of context on organizational behavior is not sufficiently recognized or appreciated by researchers. I define context as situational opportunities and constraints that affect the occurrence and meaning of organizational behavior as well as functional relationships between variables, and I propose two levels of analysis for thinking about context?one grounded in journalistic practice and the other in classic social psychology. Several means of contextualizing research are considered.},
author = {{Teunter, RH and Duncan}, L},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/MorseSmaleRegression.pdf:pdf},
isbn = {0305624042000},
journal = {Academy of Management Review},
keywords = {at estimating the unobservable,classical test theory,fundamentally,information,it concerns using observable,measurement,questionnaire items,reliability,such as scores on,thus,to garner insights into,variables of interest},
mendeley-groups = {Skelreg},
number = {2},
pages = {386--408},
title = {{Linked references are available on JSTOR for this article}},
volume = {31},
year = {2006}
}
@article{Tibshirani2019,
author = {Tibshirani, Ryan and Wasserman, Larry},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/NonparReg.pdf:pdf},
mendeley-groups = {Skelreg},
title = {{Nonpar2019}},
year = {2019}
}
@book{Dold1979,
author = {Dold, Edited A and Eckmann, B},
booktitle = {Lecture Notes in Mathematics},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/SmoothingTechniquesforCurveEstimation .pdf:pdf},
isbn = {0387097066},
mendeley-groups = {Skelreg},
title = {{Lecture Notes in Mathematics Smoothing Techniques for Curve Estimation}},
year = {1979}
}
@article{Li2021,
abstract = {Over the past two decades, we have seen an increased demand for 3D visualization and simulation software in medicine, architectural design, engineering, and many other areas, which have boosted the investigation of geometric data analysis and raised the demand for further advancement in statistical analytic approaches. In this paper, we propose a class of spline smoother appropriate for approximating geometric data over 3D complex domains, which can be represented in terms of a linear combination of spline basis functions with some smoothness constraints. We start with introducing the tetrahedral partitions, Barycentric coordinates, Bernstein basis polynomials, and trivariate spline on tetrahedra. Then, we propose a penalized spline smoothing method for identifying the underlying signal in a complex 3D domain from potential noisy observations. Simulation studies are conducted to compare the proposed method with traditional smoothing methods on 3D complex domains.},
archivePrefix = {arXiv},
arxivId = {2106.04255},
author = {Li, Xinyi and Yu, Shan and Wang, Yueying and Wang, Guannan and Wang, Li},
eprint = {2106.04255},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/Spline3DGeometric.pdf:pdf},
keywords = {and phrases,complex domain,nonparametric smoothing,penalized splines,splines,tetrahedra partitions,trivariate},
mendeley-groups = {Skelreg},
pages = {1--44},
title = {{Spline Smoothing of 3D Geometric Data}},
url = {http://arxiv.org/abs/2106.04255},
year = {2021}
}
@article{Wang2016,
abstract = {We introduce a family of adaptive estimators on graphs, based on penalizing the ℓ1 norm of discrete graph differences. This generalizes the idea of trend filtering (Kim et al., 2009; Tibshirani, 2014), used for univariate nonparametric regression, to graphs. Analogous to the univariate case, graph trend filtering exhibits a level of local adaptivity unmatched by the usual ℓ2-based graph smoothers. It is also defined by a convex minimization problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We demonstrate the merits of graph trend filtering through both examples and theory.},
archivePrefix = {arXiv},
arxivId = {1410.7690},
author = {Wang, Yu Xiang and Sharpnack, James and Smola, Alexander J. and Tibshirani, Ryan J.},
eprint = {1410.7690},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/trendFilteringGraph.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Fused lasso,Graph smoothing,Local adaptivity,Total variation denoising,Trend filtering},
mendeley-groups = {Skelreg},
pages = {1--41},
title = {{Trend filtering on graphs}},
volume = {17},
year = {2016}
}
@article{JennenSteinmetz1988,
abstract = {There exists a variety of nonparametric regression estimators, with cubic smoothing splines, k-nearest-neighbor (k-NN) estimators, and various types of kernel estimators among the most popular. A class of kernel estimators with local bandwidth depending on the density of the design points is introduced, where the degree of design adaptation may be expressed by a single parameter $\alpha$ in [0, 1]. The adaptation to the design is such that the bandwidth is made larger, to a degree depending on $\alpha$, where the design is thin. Special values of this parameter correspond approximately to the ordinary (fixed-width) kernel estimator, the smoothing spline, and the k-NN estimator. Hence this method offers a synthesis of some classical methods. The same method allows estimation of derivatives by using appropriate kernels. The influence of this degree of design adaptation on the integrated mean squared error is investigated. There is no uniformly optimal solution. The optimal solution depends in a complex way on design and underlying regression function, but there exists a minimax optimal solution given as the fixed-width kernel smoother. In an empirical investigation, the fixed-width smoother or some mildly design-adaptive estimators performed well. Essentially, two different weighting schemes for kernel estimation have been introduced in the literature, one traditionally associated with the fixed-design model (the Priestley–Chao–type estimator) and one with the random-design model (the Nadaraya–Watson–type estimator). The derivation of bias and variance for random design for the former allows a more extensive comparison of the two types of kernel estimators. {\textcopyright} 1976 Taylor & Francis Group, LLC.},
author = {Jennen–Steinmetz, Christine and Gasser, Theo},
doi = {10.1080/01621459.1988.10478705},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/UnifyingApproachNonparametricRegressionEstimation.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {K-nearest-neighbor estimator,Kernel estimator,Minimax optimality,Smoothing spline,Variable bandwidth},
mendeley-groups = {Skelreg},
number = {404},
pages = {1084--1089},
title = {{A unifying approach to nonparametric regression estimation}},
volume = {83},
year = {1988}
}
@article{Muller1987,
abstract = {Weighted local regression, a popular technique for smoothing scatterplots, is shown to be asymptotically equivalent to certain kernel smoothers. Since both methods are local weighted averages of the data, it is proved that in the fixed design regression model, given a weighted local regression procedure with any weight function, there is a corresponding kernel method such that the quotients of weights distributed by both methods tend uniformly to 1 as the number of observations increases to infinity. It is demonstrated by examples that in some instances the weights are nearly the same for both methods, even for small samples. The asymptotic equivalence allows the derivation of the leading terms of the mean squared error and of the local limit distribution for weighted local regression. Further, a close correspondence is found between the orders of the polynomial to be locally fitted in weighted local regression and the order (number of vanishing moments) of the kernel employed in the kernel smoother and between the shapes of the kernel and the weight function. {\textcopyright} 1976 Taylor & Francis Group, LLC.},
author = {M{\"{u}}ller, Hans Georg},
doi = {10.1080/01621459.1987.10478425},
file = {:C\:/Users/Owner/OneDrive - UW/SkeletonReg/ref/WeightedLocalRegressionandKernelMethodsforNonparametricCurve Fitting.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Asymptotic normality,Gauss–Markov theory,Kernel function,Mean squared error,Nonparametric regression,Smoothing scatterplots,Weight function},
mendeley-groups = {Skelreg},
number = {397},
pages = {231--238},
title = {{Weighted local regression and kernel methods for nonparametric curve fitting}},
volume = {82},
year = {1987}
}
@article{Loh2012,
abstract = {Missing and corrupted data are ubiquitous in many science and engineering domains. We analyze the information-theoretic limits of recovering sparse vectors under various models of corrupted and missing data. In particular, consider a high-dimensional linear regression model y = X $\beta$* + $\epsilon$, where y ∈ ℝ n is the response vector, X ∈ ℝ nXp is a random design matrix with p ≫ n and rows distributed i.i.d. as N(0, $\Sigma$ x), $\beta$* $\epsilon$ ℝ p is the unknown regression vector, and $\epsilon$ ∼ N(0,$\sigma$ $\epsilon$2I) is independent additive noise. Whereas a traditional approach assumes that the covariates X are fully observed, we assume only that a corrupted version Z is observed. Our main contribution is to establish minimax rates of convergence for estimating $\beta$* in squared ℓ 2-loss, assuming $\beta$* is k-sparse. Our upper and lower bounds in both additive noise and missing data cases scale as k log(p/k)/n, with prefactors depending only on the corruption and/or missing pattern of the data. {\textcopyright} 2012 IEEE.},
author = {Loh, Po Ling and Wainwright, Martin J.},
doi = {10.1109/ISIT.2012.6283989},
isbn = {9781467325790},
journal = {IEEE International Symposium on Information Theory - Proceedings},
mendeley-groups = {Skelreg},
pages = {2601--2605},
title = {{Corrupted and missing predictors: Minimax bounds for high-dimensional linear regression}},
year = {2012}
}
@article{Meier2008,
abstract = {We propose a new sparsity-smoothness penalty for high-dimensional generalized additive models. The combination of sparsity and smoothness is crucial for mathematical theory as well as performance for finite-sample data. We present a computationally efficient algorithm, with provable numerical convergence properties, for optimizing the penalized likelihood. Furthermore, we provide oracle results which yield asymptotic optimality of our estimator for high dimensional but sparse additive models. Finally, an adaptive version of our sparsity-smoothness penalized approach yields large additional performance gains.},
archivePrefix = {arXiv},
arxivId = {0806.4115v4},
author = {Meier, Lukas and van de Geer, Sara and B{\"{u}}hlmann, Peter},
doi = {10.1214/09-AOS692},
eprint = {0806.4115v4},
file = {:C\:/Users/Owner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meier, van de Geer, B{\"{u}}hlmann - 2008 - High-dimensional additive modeling.pdf:pdf},
journal = {Annals of Statistics},
keywords = {Group lasso,Model selection,Nonparametric regression,Oracle inequality,Penalized likelihood,Sparsity,oracle inequality,penalized likelihood,sparsity},
mendeley-groups = {Skelreg},
month = {jun},
number = {6 B},
pages = {3779--3821},
title = {{High-dimensional additive modeling}},
url = {http://arxiv.org/abs/0806.4115 http://dx.doi.org/10.1214/09-AOS692},
volume = {37},
year = {2008}
}


@article{Watson1964,
 ISSN = {0581572X},
 URL = {http://www.jstor.org/stable/25049340},
 abstract = {Few would deny that the most powerful statistical tool is graph paper. When however there are many observations (and/or many variables) graphical procedures become tedious. It seems to the author that the most characteristic problem for statisticians at the moment is the development of methods for analyzing the data poured out by electronic observing systems. The present paper gives a simple computer method for obtaining a "graph" from a large number of observations.},
 author = {Geoffrey S. Watson},
 journal = {Sankhyā: The Indian Journal of Statistics, Series A (1961-2002)},
 number = {4},
 pages = {359--372},
 publisher = {Springer},
 title = {Smooth Regression Analysis},
 volume = {26},
 year = {1964}
}

@article{Nadaraya1964,
   abstract = {A study is made of certain properties of an approximation to the regression line on the basis of sampling data when the sample size increases unboundedly.},
   author = {E. A. Nadaraya},
   doi = {10.1137/1109020},
   issn = {0040-585X},
   issue = {1},
   journal = {http://dx.doi.org/10.1137/1109020},
   month = {7},
   pages = {141-142},
   publisher = {Society for Industrial and Applied Mathematics},
   title = {On Estimating Regression},
   volume = {9},
   year = {1964},
}

@article{Hastie1993,
   author = {Trevor Hastie and Clive Loader},
   doi = {10.1214/SS/1177011005},
   issn = {0883-4237},
   issue = {2},
   journal = {https://doi.org/10.1214/ss/1177011005},
   month = {5},
   pages = {139-143},
   publisher = {Institute of Mathematical Statistics},
   title = {[Local Regression: Automatic Kernel Carpentry]: Rejoinder},
   volume = {8},
   year = {1993},
}

@book{ESL,
   author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
   city = {New York, NY},
   doi = {10.1007/978-0-387-84858-7},
   isbn = {978-0-387-84857-0},
   publisher = {Springer New York},
   title = {The Elements of Statistical Learning},
   url = {http://link.springer.com/10.1007/978-0-387-84858-7},
   year = {2009},
}

@article{Friedman1991,
   abstract = {A new method is presented for flexible regression modeling of high dimensional data. The model takes the form of an expansion in product spline basis functions, where the number of basis functions as well as the parameters associated with each one (product degree and knot locations) are automatically determined by the data. This procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties. Unlike recursive partitioning, however, this method produces continuous models with continuous derivatives. It has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables. In addition, the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions.},
   author = {Jerome H. Friedman},
   doi = {10.1214/AOS/1176347963},
   issn = {0090-5364},
   issue = {1},
   journal = {https://doi.org/10.1214/aos/1176347963},
   keywords = {62H30,62J02,65D07,65D10,65D15,68T05,68T10,90A19,93C35,93E11,93E14,AID,CART,Nonparametric multiple regression,multivariable function approximation,multivariate smoothing,recursive partitioning,splines,statistical learning neural networks},
   month = {3},
   pages = {1-67},
   publisher = {Institute of Mathematical Statistics},
   title = {Multivariate Adaptive Regression Splines},
   volume = {19},
   year = {1991},
}

@article{Altman1992,
   abstract = {Nonparametric regression is a set of techniques for estimating a regression curve without making strong assumptions about the shape of the true regression function. These techniques are therefore useful for building and checking parametric models, as well as for data description. Kernel and nearest-neighbor regression estimators are local versions of univariate location estimators, and so they can readily be introduced to beginning students and consulting clients who are familiar with such summaries as the sample mean and median. © 1992 American Statistical Association.},
   author = {N. S. Altman},
   doi = {10.1080/00031305.1992.10475879},
   issn = {15372731},
   issue = {3},
   journal = {American Statistician},
   keywords = {Confidence intervals,Local linear regression,Model building,Model checking,Smoothing},
   pages = {175-185},
   title = {An introduction to kernel and nearest-neighbor nonparametric regression},
   volume = {46},
   year = {1992},
}


@article{Kpotufe2017,
  author  = {Samory Kpotufe and Nakul Verma},
  title   = {Time-Accuracy Tradeoffs in Kernel Prediction: Controlling Prediction Quality},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {44},
  pages   = {1-29},
  url     = {http://jmlr.org/papers/v18/16-538.html}
}

@article{Kpotufe2009,
   abstract = {It was recently shown that certain nonparametric regressors can escape the curse of dimensionality when the intrinsic dimension of data is low ([1, 2]). We prove some stronger results in more general settings. In particular, we consider a regres-sor which, by combining aspects of both tree-based regression and kernel regression , adapts to intrinsic dimension, operates on general metrics, yields a smooth function, and evaluates in time O(log n). We derive a tight convergence rate of the form n −2/(2+d) where d is the Assouad dimension of the input space.},
   author = {Samory Kpotufe},
   journal = {Advances in Neural Information Processing Systems},
   title = {Fast, smooth and adaptive regression in metric spaces},
   volume = {22},
   year = {2009},
}

@article{Kpotufe2011,
   abstract = {Many nonparametric regressors were recently shown to converge at rates that
depend only on the intrinsic dimension of data. These regressors thus escape
the curse of dimension when high-dimensional data has low intrinsic dimension
(e.g. a manifold). We show that k-NN regression is also adaptive to intrinsic
dimension. In particular our rates are local to a query x and depend only on
the way masses of balls centered at x vary with radius. Furthermore, we show a simple way to choose k = k(x) locally at any x so as
to nearly achieve the minimax rate at x in terms of the unknown intrinsic
dimension in the vicinity of x. We also establish that the minimax rate does
not depend on a particular choice of metric space or distribution, but rather
that this minimax rate holds for any metric space and doubling measure.},
   author = {Samory Kpotufe},
   isbn = {9781618395993},
   journal = {Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011},
   month = {10},
   title = {k-NN Regression Adapts to Local Intrinsic Dimension},
   url = {https://arxiv.org/abs/1110.4300v1},
   year = {2011},
}

@article{Kpotufe2013,
   abstract = {We present the first result for kernel regression where the procedure adapts locally at a point x to both the unknown local dimension of the metric space X and the unknown Hölder-continuity of the regression function at x. The result holds with high probability simultaneously at all points x in a general metric space X of unknown structure.},
   author = {Samory Kpotufe and Vikas K Garg},
   journal = {Advances in Neural Information Processing Systems},
   title = {Adaptivity to Local Smoothness and Dimension in Kernel Regression},
   volume = {26},
   year = {2013},
}


@article{Tibshirani2014,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/43556281},
 abstract = {We study trend filtering, a recently proposed tool of Kim et al. [SIAM Rev. 51 (2009) 339-360] for nonparametric regression. The trend filtering estimate is defined as the minimizer of a penalized least squares criterion, in which the penalty term sums the absolute kth order discrete derivatives over the input points. Perhaps not surprisingly, trend filtering estimates appear to have the structure of kth degree spline functions, with adaptively chosen knot points (we say "appear" here as trend filtering estimates are not really functions over continuous domains, and are only defined over the discrete set of inputs). This brings to mind comparisons to other nonparametric regression tools that also produce adaptive splines; in particular, we compare trend filtering to smoothing splines, which penalize the sum of squared derivatives across input points, and to locally adaptive regression splines [Ann. Statist. 25 (1997) 387-413], which penalize the total variation of the kth derivative. Empirically, we discover that trend filtering estimates adapt to the local level of smoothness much better than smoothing splines, and further, they exhibit a remarkable similarity to locally adaptive regression splines. We also provide theoretical support for these empirical findings; most notably, we prove that (with the right choice of tuning parameter) the trend filtering estimate converges to the true underlying function at the minimax rate for functions whose kth derivative is of bounded variation. This is done via an asymptotic pairing of trend filtering and locally adaptive regression splines, which have already been shown to converge at the minimax rate [Ann. Statist. 25 (1997) 387-413]. At the core of this argument is a new result tying together the fitted values of two lasso problems that share the same outcome vector, but have different predictor matrices.},
 author = {Ryan J. Tibshirani},
 journal = {The Annals of Statistics},
 number = {1},
 pages = {285--323},
 publisher = {Institute of Mathematical Statistics},
 title = {ADAPTIVE PIECEWISE POLYNOMIAL ESTIMATION VIA TREND FILTERING},
 volume = {42},
 year = {2014}
}

@article{Kim2009,
   abstract = {The problem of estimating underlying trends in time series data arises in a variety of disciplines. In this paper we propose a variation on Hodrick–Prescott (H-P) filtering, a widely used method fo...},
   author = {Seung Jean Kim and Kwangmoo Koh and Stephen Boyd and Dimitry Gorinevsky},
   doi = {10.1137/070690274},
   issn = {00361445},
   issue = {2},
   journal = {http://dx.doi.org/10.1137/070690274},
   keywords = {$\ell_1$ regularization,37M10,62P99,Hodrick–Prescott filtering,detrending,feature selection,piecewise linear fitting,sparse signal recovery,time series analysis,trend estimation},
   month = {5},
   pages = {339-360},
   publisher = {Society for Industrial and Applied Mathematics},
   title = {$\ell_1$ Trend Filtering},
   volume = {51},
   year = {2009},
}

  @Article{igraph,
    title = {The igraph software package for complex network research},
    author = {Gabor Csardi and Tamas Nepusz},
    journal = {InterJournal},
    volume = {Complex Systems},
    pages = {1695},
    year = {2006},
    url = {https://igraph.org},
  }
  
@ARTICLE{Dijkstra1959,
    author = {E. W. Dijkstra},
    title = {A Note on Two Problems in Connexion with Graphs},
    journal = {NUMERISCHE MATHEMATIK},
    year = {1959},
    volume = {1},
    number = {1},
    pages = {269--271}
}

@article{Hartigan1979,
	Abstract = {The K-means clustering algorithm is described indetail by Hartigan(1975). An efficient version of the algorithm is presented here.$\backslash$nThe aim of the K-means algorithm is to divide M points in N dimensions into K clusters so that the within-cluster sum of squares is minimized. It is not practical to require that the solution has minimal sum of squares against all partitions except when M,N are small and K = 2. We seek instead "local" optima, solution such that no movement of a point from one cluster to another will reduce the within cluster sum of squares.},
	Author = {Hartigan, J. A. and Wong, M. A.},
	Doi = {10.2307/2346830},
	Issn = {00359254},
	Journal = {Applied Statistics},
	Mendeley-Groups = {ImprovedKMeans},
	Number = {1},
	Pages = {100},
	Publisher = {JSTOR},
	Title = {{Algorithm AS 136: A K-Means Clustering Algorithm}},
	Volume = {28},
	Year = {1979},
	Bdsk-Url-1 = {https://doi.org/10.2307/2346830}}


@article{Cheng2013,
author = { Ming-Yen   Cheng  and  Hau-Tieng   Wu },
title = {Local Linear Regression on Manifolds and Its Geometric Interpretation},
journal = {Journal of the American Statistical Association},
volume = {108},
number = {504},
pages = {1421-1434},
year  = {2013},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2013.827984},
eprint = { 
        https://doi.org/10.1080/01621459.2013.827984
    
}

}

@article{Aswani2011,
author = {Anil Aswani and Peter Bickel and Claire Tomlin},
title = {{Regression on manifolds: Estimation of the exterior derivative}},
volume = {39},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {48 -- 81},
keywords = {collinearity, Manifold, Model selection, Nonparametric regression, regularization},
year = {2011},
doi = {10.1214/10-AOS823},
URL = {https://doi.org/10.1214/10-AOS823}
}


@article{PCR,
author = { William F.   Massy },
title = {Principal Components Regression in Exploratory Statistical Research},
journal = {Journal of the American Statistical Association},
volume = {60},
number = {309},
pages = {234-256},
year  = {1965},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1965.10480787},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1965.10480787
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1965.10480787
    
}

}

@article{Wold1975,
   abstract = {The NIPALS approach is applied to the ‘soft’ type of model that has come to the fore in sociology and other social sciences in the last five or ten years, namely path models that involve latent variables which serve as proxies for blocks of directly observed variables. Such models are seen as hybrids of the ‘hard’ models of econometrics where all variables are directly observed (path models in the form of simultaneous equations systems) and the ‘soft’ models of psychology where the human mind is described in terms of latent variables and their directly observed indicators. For hybrid models that involve one or two latent variables the NIPALS approach has been developed in [38], [41] and [42]. The present paper extends the NIPALS approach to path models with three or more latent variables. Each new latent variable brings a rapid increase in the pluralism of possible model designs, and new problems arise in the parameter estimation of the models. Iterative procedures are given for the point estimation of the parameters. With a view to cases when the iterative estimation does not converge, a device of range estimation is developed, where high profile versus low profile estimates give ranges for the parameter estimates.},
   author = {Herman Wold},
   doi = {10.1017/S0021900200047604},
   issn = {0021-9002},
   issue = {S1},
   journal = {Journal of Applied Probability},
   pages = {117-142},
   publisher = {Cambridge University Press (CUP)},
   title = {Soft Modelling by Latent Variables: The Non-Linear Iterative Partial Least Squares (NIPALS) Approach},
   volume = {12},
   year = {1975},
}


@article{Marzio2014,
author = {Marco Di Marzio and Agnese Panzera and Charles C. Taylor},
title = {Nonparametric Regression for Spherical Data},
journal = {Journal of the American Statistical Association},
volume = {109},
number = {506},
pages = {748-763},
year  = {2014},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2013.866567},

URL = { 
        https://doi.org/10.1080/01621459.2013.866567
    
},
eprint = { 
        https://doi.org/10.1080/01621459.2013.866567
    
}

}


@article{Lee2016,
author = {Ann B. Lee and Rafael Izbicki},
title = {{A spectral series approach to high-dimensional nonparametric regression}},
volume = {10},
journal = {Electronic Journal of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {423 -- 463},
keywords = {data-driven basis, eigenmaps, high-dimensional inference, manifold learning, Mercer kernel, orthogonal series regression},
year = {2016},
doi = {10.1214/16-EJS1112},
URL = {https://doi.org/10.1214/16-EJS1112}
}

@article{belkin06a,
  author  = {Mikhail Belkin and Partha Niyogi and Vikas Sindhwani},
  title   = {Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples},
  journal = {Journal of Machine Learning Research},
  year    = {2006},
  volume  = {7},
  number  = {85},
  pages   = {2399-2434},
  url     = {http://jmlr.org/papers/v7/belkin06a.html}
}


@InProceedings{Yuchen2013,
  title = 	 {Divide and Conquer Kernel Ridge Regression},
  author = 	 {Zhang, Yuchen and Duchi, John and Wainwright, Martin},
  booktitle = 	 {Proceedings of the 26th Annual Conference on Learning Theory},
  pages = 	 {592--617},
  year = 	 {2013},
  editor = 	 {Shalev-Shwartz, Shai and Steinwart, Ingo},
  volume = 	 {30},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Princeton, NJ, USA},
  month = 	 {12--14 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v30/Zhang13.pdf},
  url = 	 {https://proceedings.mlr.press/v30/Zhang13.html}
}

@article{Guhaniyogi2016,
  author  = {Rajarshi Guhaniyogi and David B. Dunson},
  title   = {Compressed Gaussian Process for Manifold Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {69},
  pages   = {1-26},
  url     = {http://jmlr.org/papers/v17/14-230.html}
}

@ARTICLE{Zhang2019,
  author={Zhang, Xiaowei and Shi, Xudong and Sun, Yu and Cheng, Li},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Multivariate Regression with Gross Errors on Manifold-Valued Data}, 
  year={2019},
  volume={41},
  number={2},
  pages={444-458},
  doi={10.1109/TPAMI.2017.2776260}}
  
  @article{Meila2018,
  author    = {Marina Meila and
               Samson Koelle and
               Hanyu Zhang},
  title     = {A regression approach for explaining manifold embedding coordinates},
  journal   = {CoRR},
  volume    = {abs/1811.11891},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.11891},
  eprinttype = {arXiv},
  eprint    = {1811.11891},
  timestamp = {Fri, 30 Nov 2018 14:31:38 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-11891.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Lin2020,
    author = {Lin, Zhenhua and Yao, Fang},
    title = "{Functional regression on the manifold with contamination}",
    journal = {Biometrika},
    volume = {108},
    number = {1},
    pages = {167-181},
    year = {2020},
    month = {07},
    abstract = "{We propose a new method for functional nonparametric regression with a predictor that resides on a finite-dimensional manifold, but is observable only in an infinite-dimensional space. Contamination of the predictor due to discrete or noisy measurements is also accounted for. By using functional local linear manifold smoothing, the proposed estimator enjoys a polynomial rate of convergence that adapts to the intrinsic manifold dimension and the contamination level. This is in contrast to the logarithmic convergence rate in the literature of functional nonparametric regression. We also observe a phase transition phenomenon related to the interplay between the manifold dimension and the contamination level. We demonstrate via simulated and real data examples that the proposed method has favourable numerical performance relative to existing commonly used methods.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asaa041},
    url = {https://doi.org/10.1093/biomet/asaa041},
    eprint = {https://academic.oup.com/biomet/article-pdf/108/1/167/36441008/asaa041.pdf},
}

@article{Sober2021,
   abstract = {peerReviewed},
   author = {Barak Sober and Yariv Aizenbud and David Levin},
   doi = {10.1016/J.CAM.2020.113140},
   issn = {03770427},
   journal = {Journal of Computational and Applied Mathematics},
   keywords = {approksimointi,dimension reduction,funktiot,high dimensional approximation,manifold learning,monistot,moving least,numeerinen analyysi,of,out,regression over manifolds,sample extension,squares},
   month = {2},
   publisher = {Elsevier BV},
   title = {Approximation of functions over manifolds : A Moving Least-Squares approach},
   volume = {383},
   url = {https://jyx.jyu.fi/handle/123456789/77962},
   year = {2021},
}

@article{Burnaev2021,
   abstract = {Predictive Modeling problems deal with high-dimensional data; however, the curse of dimensionality presents an obstacle to the use of many methods for their solutions. In many applications, real-world data occupy only a very small part of high-dimensional observation space whose intrinsic dimension is essentially lower than that of the space. A popular model for such data is the manifold model, according to which the data lie on an unknown low-dimensional manifold (Data Manifold) embedded in the ambient high-dimensional space. Predictive modeling problems, which are studied under this assumption, are called manifold estimation problems. The general goal of such problems is to identify the low-dimensional structure of multidimensional data from a given dataset. If dataset points are sampled according to an unknown probability measure on the data manifold, there is a need to model manifolds when solving various machine learning problems. We provide a short survey of such problems and outline some approaches to their solution.},
   author = {E. V. Burnaev and A. V. Bernstein},
   doi = {10.1134/S106422692106005X},
   issn = {1555-6557},
   issue = {6},
   journal = {Journal of Communications Technology and Electronics 2021 66:6},
   keywords = {Communications Engineering,Networks,predictive modeling},
   month = {6},
   pages = {754-763},
   publisher = {Springer},
   title = {Manifold Modeling in Machine Learning},
   volume = {66},
   url = {https://link.springer.com/article/10.1134/S106422692106005X},
   year = {2021},
}


@book{Bernhard2002,
   abstract = {In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs----kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.<br /> <br /> <i>Learning with Kernels</i> provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
   author = {Bernhard Schölkopf and Alexander J. Smola},
   isbn = {9780262194754},
   pages = {626},
   title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond Adaptive computation and machine learning},
   year = {2002},
   publisher={The MIT Press}
}

@article{voronoi1908recherches,
	Author = {Voronoi, G},
	Journal = {J. reine angew. Math},
	Pages = {198--287},
	Title = {Recherches sur les parall{\'e}lo{\`e}dres primitives},
	Volume = {134},
	Year = {1908}}
	
@article{Delaunay1934,
	Author = {Delaunay, B.},
	Journal = {Bulletin de l'Acad\'emie des Sciences de l'URSS. Classe des sciences math\'ematiques et na},
	Pages = {793--800},
	Title = {Sur la sph\`ere vide. A la m\'emoire de Georges Vorono\"\i},
	Volume = {6},
	Year = {1934}}
	
	@article{Bierens1983,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2288140},
 abstract = {In this article we prove uniform consistency of kernel estimators of a multivariate regression function under various assumptions on the distribution of the data. In addition to the usual assumptions that the data are iid and that the distribution of the regressors is absolutely continuous, we consider the cases that some regressors are discrete and the data are either stationary φ-mixing themselves or generated by a class of functions of one-sided infinite stationary φ-mixing sequences. Moreover, we demonstrate the performance of the kernel estimation method under these generalized conditions by a numerical example.},
 author = {Herman J. Bierens},
 journal = {Journal of the American Statistical Association},
 number = {383},
 pages = {699--707},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Uniform Consistency of Kernel Estimators of a Regression Function Under Generalized Conditions},
 volume = {78},
 year = {1983}
}


@techreport{COIL20,
   author = {S. A. Nene and S. K. Nayar and H. Murase},
   month = {2},
   title = {Columbia Object Image Library (COIL-20)},
   url = {https://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php},
   year = {1996},
   institution = {Columbia University}
}


@article{ANNbench2018,
   author = {Martin Aumüller and Erik Bernhardsson and Alexander Faithfull},
   doi = {10.1016/J.IS.2019.02.006},
   issn = {03064379},
   journal = {Information Systems},
   keywords = {Benchmarking,Evaluation,Nearest neighbor search},
   month = {1},
   publisher = {Elsevier Ltd},
   title = {ANN-Benchmarks: A benchmarking tool for approximate nearest neighbor algorithms},
   volume = {87},
   year = {2020},
}

@inproceedings{avq_2020,
  title={Accelerating Large-Scale Inference with Anisotropic Vector Quantization},
  author={Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
  booktitle={International Conference on Machine Learning},
  year={2020},
  URL={https://arxiv.org/abs/1908.10396}
}
@article{johnson2019billion,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@INPROCEEDINGS{InvertedFiles,
  author={Sivic and Zisserman},
  booktitle={Proceedings Ninth IEEE International Conference on Computer Vision}, 
  title={Video Google: a text retrieval approach to object matching in videos}, 
  year={2003},
  volume={},
  number={},
  pages={1470-1477 vol.2},
  doi={10.1109/ICCV.2003.1238663}}

@INPROCEEDINGS{InvertedMultiIndex,
  author={Babenko, Artem and Lempitsky, Victor},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={The inverted multi-index}, 
  year={2012},
  volume={},
  number={},
  pages={3069-3076},
  doi={10.1109/CVPR.2012.6248038}}