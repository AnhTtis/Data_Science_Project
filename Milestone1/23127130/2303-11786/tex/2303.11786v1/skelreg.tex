\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
\usepackage[abbrvbib, preprint]{jmlr2e}

% \usepackage{jmlr2e}

% \usepackage[font=small]{caption}
\usepackage[font=small]{subcaption}
\usepackage{amsmath,  bm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{multirow}

% Definitions of handy macros can go here

\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{alg}[thm]{Algorithm}

\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\brac}[1]{\left(#1\right)}
\newcommand{\abrac}[1]{\left\langle#1\right\rangle}
\newcommand{\sbrac}[1]{\left[#1\right]}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\Real}{\mathbb R}
\newcommand{\eps}{\varepsilon}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\lam}{\lambda}
\newcommand{\Lam}{\Lambda}
\newcommand{\gam}{\gamma}
\newcommand{\sig}{\sigma}
\newcommand{\ze}{\zeta}
\newcommand{\Om}{\Omega}
\newcommand{\del}{\delta}
\newcommand{\Del}{\Delta}
\def\dlim{\displaystyle\lim}

\newcommand{\AAA}{{\mathbb A}}
\newcommand{\BB}{{\mathbb B}}
\newcommand{\CC}{{\mathbb C}}
\newcommand{\DD}{{\mathbb D}}
\newcommand{\EE}{{\mathbb E}}
\newcommand{\FF}{{\mathbb F}}
\newcommand{\GG}{{\mathbb G}}
\newcommand{\HH}{{\mathbb H}}
\newcommand{\II}{{\mathbb I}}
\newcommand{\JJ}{{\mathbb J}}
\newcommand{\KK}{{\mathbb K}}
\newcommand{\LL}{{\mathbb L}}
\newcommand{\MM}{{\mathbb M}}
\newcommand{\NN}{{\mathbb N}}
\newcommand{\OO}{{\mathbb O}}
\newcommand{\PP}{{\mathbb P}}
\newcommand{\QQ}{{\mathbb Q}}
\newcommand{\RR}{{\mathbb R}}
\newcommand{\SSS}{{\mathbb S}}
\newcommand{\TT}{{\mathbb T}}
\newcommand{\UU}{{\mathbb U}}
\newcommand{\VV}{{\mathbb V}}
\newcommand{\WW}{{\mathbb W}}
\newcommand{\XX}{{\mathbb X}}
\newcommand{\YY}{{\mathbb Y}}
\newcommand{\ZZ}{{\mathbb Z}}


\newcommand{\im}{{\rm Im}\,}
\newcommand{\re}{{\rm Re}\,}
\newcommand{\dist}{\hbox{ \rm dist}}
\newcommand{\Span}{\hbox{Span}}

\newcommand{\pDisk}{{\sf pDisk}}
\newcommand{\Disk}{{\sf Disk}}
\newcommand{\argmin}{{\sf argmin}}

\newcommand{\calA}{{\mathcal A}}
\newcommand{\calB}{{\mathcal B}}
\newcommand{\calC}{{\mathcal C}}
\newcommand{\calD}{{\mathcal D}}
\newcommand{\calE}{{\mathcal E}}
\newcommand{\calF}{{\mathcal F}}
\newcommand{\calG}{{\mathcal G}}
\newcommand{\calH}{{\mathcal H}}
\newcommand{\calI}{{\mathcal I}}
\newcommand{\calJ}{{\mathcal J}}
\newcommand{\calK}{{\mathcal K}}
\newcommand{\calL}{{\mathcal L}}
\newcommand{\calM}{{\mathcal M }}
\newcommand{\calN}{{\mathcal N}}
\newcommand{\calO}{{\mathcal O}}
\newcommand{\calP}{{\mathcal P}}
\newcommand{\calQ}{{\mathcal Q}}
\newcommand{\calR}{{\mathcal R}}
\newcommand{\calS}{{\mathcal S}}
\newcommand{\calT}{{\mathcal T}}
\newcommand{\calU}{{\mathcal U}}
\newcommand{\calV}{{\mathcal V}}
\newcommand{\calW}{{\mathcal W}}
\newcommand{\calX}{{\mathcal X}}
\newcommand{\calY}{{\mathcal Y}}
\newcommand{\calZ}{{\mathcal Z}}


\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bA}{\bm{A}}
\newcommand{\bE}{\bm{E}}
\newcommand{\bH}{\bm{H}}
\newcommand{\bq}{\bm{q}}
\newcommand{\bR}{\bm{R}}
\newcommand{\br}{\bm{r}}
\newcommand{\bS}{\bm{S}}
\newcommand{\bs}{\bm{s}}
\newcommand{\bU}{\bm{U}}
\newcommand{\bv}{\bm{v}}
\newcommand{\bV}{\bm{V}}
\newcommand{\bz}{\bm{z}}
\newcommand{\bZ}{\bm{Z}}
\newcommand{\bu}{\bm{u}}
\newcommand{\by}{\bm{y}}
\newcommand{\bX}{\bm{X}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bY}{\bm{Y}}

\newcommand\reach{{\sf reach}}
\newcommand\Quantile{{\sf Quantile}}
\newcommand\BC{\mathbf{BC}}

\newcommand{\Set}[1]{\left\{{#1}\right\}}       
\newcommand{\st}{:\;}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}


%%%% new version of enumerate with less spacing
\newenvironment{enum}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

% comment macro
\def\YC#1{\textcolor[RGB]{152,10,100}{#1}}
\def\JW#1{\textcolor[RGB]{2,102,180}{[#1]}}
\def\revise#1{{\color{cyan}#1}}


%

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{23}{2022}{1-48}{}{}{}{Zeyu Wei and Yen-Chi Chen}

% Short headings should be running head and authors last names

\ShortHeadings{Skeleton Regression}{Wei and Chen}
\firstpageno{1}

\begin{document}

\title{Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure} 

\author{\name Zeyu Wei \email zwei5@uw.edu \\
       \addr Department of Statistics\\
       University of Washington\\
       Seattle, WA 98195-4322, USA
       \AND
       \name Yen-Chi Chen \email yenchic@uw.edu \\
       \addr Department of Statistics\\
       University of Washington\\
       Seattle, WA 98195-4322, USA}

\editor{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
We introduce a new regression framework designed to deal with large-scale, complex data that lies around a low-dimensional manifold. 
Our approach first constructs a graph representation, referred to as the \textit{skeleton}, to capture the underlying geometric structure. 
We then define metrics on the skeleton graph and apply nonparametric regression techniques, along with feature transformations based on the graph, to estimate the regression function.
In addition to the included nonparametric methods, we also discuss the limitations of some nonparametric regressors with respect to the general metric space such as the skeleton graph.
The proposed regression framework allows us to bypass the curse of dimensionality and provides additional advantages that it can handle the union of multiple manifolds and is robust to  additive noise and noisy observations.
We provide statistical guarantees for the proposed method and demonstrate its effectiveness through simulations and real data examples. 
% We also discussed the feasibility and the issue of edge directions when fitting splines of some derivative smoothness on graphs.


\end{abstract}

\begin{keywords}
  manifold learning, nonparametric regression, kernel regression, spline
\end{keywords}


\section{Introduction}

% \begin{figure}
% \centering
% \includegraphics[width=4.5cm]{figures/Yinyang_d200plot_KM.jpeg}
% \includegraphics[width=4.5cm]{figures/Yinyang_d200plot_SC.jpeg}\\
% \includegraphics[width=4.5cm]{figures/Yinyang_d200plot_MS.jpeg}
% \includegraphics[width=4.5cm]{figures/Yinyang_d200plot_Voron.jpeg}
% \caption{Yinyang Data with dimension 200. The bottom-right is clustering result with Skeleton clustering framework and using the newly proposed Voronoi density similarity measure.}
% \label{fig::ex01}
% \end{figure}



~~~~ Many data nowadays are geometrically structured that the covariates lie around a low-dimensional manifold embedded inside a large-dimensional vector space. 
Among many geometric data analysis tasks, the estimation of functions defined on manifolds has been extensively studied in the statistical literature. 
A classical approach to explicitly account for geometric structure takes two steps: map the data to the tangent plane or some embedding space and then run regression methods with the transformed data.
This approach is pioneered by the Principle Component Regression (PCR) \citep{PCR} and the Partial Least Squares (PLS) \citep{Wold1975}.
\cite{Aswani2011} innovatively relate the regression coefficients to exterior derivatives.  
They propose to learn the manifold structure through local principal components and then constrain the regression to lie close to the manifold by solving a weighted least-squares problem with Ridge regularization.
\cite{Cheng2013} present the Manifold Adaptive Local Linear Estimator for the Regression (MALLER) that performs the local linear regression (LLR) on a tangent plane estimate. 
However, because those methods directly exploit the local manifold structures in an exact sense, they are not robust to variations in the covariates that perturb them away from the true manifold structure.

Many other manifold estimation approaches exist in the statistical literature.
\cite{Guhaniyogi2016} utilize random compression of the feature vector in combination with Gaussian process regression.
\cite{Yuchen2013} follows a divide-and-conquer approach that computes an independent kernel Ridge regression estimator for each randomly partitioned subset.
Other nonparametric regression approaches such as kernel machine learning \citep{Bernhard2002}, manifold regularization \citep{belkin06a}, and the spectral series approach \citep{Lee2016} also account for the manifold structure of the data. However, those methods still suffer from the curse of dimensionality with large-dimensional covariates.

% \revise{Geometric data is also attracting attention from the machine learning field. A flourishing direction of research incorporates geometric information into deep neural models and is termed Geometric Deep Learning \citep{GeometricDL, Peter2018,  Bronstein2021}. Under this general blueprint, one approach encodes geometric information through graphs and performs learning tasks with the Graph Neural Networks (GNNs)  \citep{GRAND,Petar2018graph, Chamberlain2021, xu2018how, Bouritsas2021}. 
% In particular, \cite{Yue2019} dynamically builds neighborhood graphs from point clouds and aggregates edge features through layers for classification and segmentation tasks.
% \cite{DGM} learns the probabilistic latent graphs in the deep learning architecture for optimal classification.
% However, the learning tasks focused on by those works are classification and segmentation, while in this work we focus on regression.
% }

In addition to data with manifold-based covariates, manifold learning has been applied to other types of manifold-related data. 
\cite{Marzio2014} develop nonparametric smoothing for regression when both the predictor and the response variables are defined on a sphere.
\cite{Zhang2019} deal with the presence of grossly corrupted manifold-valued responses.
\citet{Green2021} proposes the Principal Components Regression with Laplacian-Eigenmaps (PCR-LE) that projects responses onto the eigenvectors output by Laplacian Eigenmaps.
\cite{Lin2020} address data with functional predictors that reside on a finite-dimensional manifold with contamination. 
In this work, we focus on manifold-based covariates and may incorporate other types of manifold-related data in the future.
% \cite{Burnaev2021} a recent survey on manifold estimation

The main goal of this work is to estimate scalar responses on manifold-structured covariates in a way that bypasses the curse of dimensionality.
This is achieved by proposing a new framework that utilizes graphs and nonparametric regression techniques. 
Our framework follows the two-step idea: first, we learn a graph representation, which we call the \textit{skeleton}, of the manifold structure based on the methods from \citet{skelclus} and project the covariates onto the skeleton. Then we apply different nonparametric regression methods to the skeleton-projected data. We give brief descriptions of the relevant nonparametric regression methods below.

Kernel smoothing is a widely used technique that estimates the regression function as locally weighted averages with the kernel as the weighting function. Pioneered by \citet{Nadaraya1964} and \citet{Watson1964} with the famous Nadarayaâ€“Watson estimator, this technique has been widely used and extended by recent works (\cite{Fan1992}, \cite{Hastie1993}, \cite{Fan1996}, \cite{Kpotufe2017}). 
Splines (\cite{ESL}, \cite{Friedman1991}) are popular nonparametric regression constructs that take the derivative-based measure of smoothness into account when fitting a regression function.
Moreover, k-Nearest-Neighbors (kNN) regression \citep{Altman1992, ESL} has a simple form but is powerful and widely used in many applications. These techniques are incorporated into our proposed regression framework.

In recent years, many nonparametric regression techniques have been shown to adapt to the manifold structure of the data, with convergence rates that depend only on the intrinsic dimension of the data space. Specifically, the kNN regressor and the kernel regressor have been shown to be manifold-adaptive with proper parameter tuning procedures \citep{Kpotufe2009,  Kpotufe2011, Kpotufe2013, Kpotufe2017}.
The proposed regression framework in this work also adapts to the manifold, as the nonparametric regression models fitted on a graph are dimension-independent. This framework has several additional advantages such as the ability to account for predictors from distinct manifolds and being robust to additive noise and noisy observations.

\begin{figure}
% \captionsetup{skip=1pt}
\centering
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Twomoon_data.jpeg} 
        \caption{Data}
    \end{subfigure}
        \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Twomoon_knots.jpeg}
        \caption{Knots}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Twomoon_skelvor.jpeg}
        \caption{Skeleton}
    \end{subfigure}\\
        \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Twomoon_skelkernel.jpeg} 
        \caption{S-Kernel Regression}
    \end{subfigure}
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Twomoon_lspline.jpeg} 
        \caption{Linear Spline}
    \end{subfigure}
\caption{Skeleton Regression illustrated by Two Moon Data (d=2).}
\label{fig::ex1}
\end{figure}

% \citet{Meier2008} considered high-dimensional generalized additive regression model and proposed a new penalty that combines sparsity and smoothness considerations. A computationally efficient algorithm has been provided with provable numerical convergence for finding the optimum of a penalized likelihood function.

% This contamination issue is not encountered in the situation studied by \cite{Cheng2013} and has been considered only for linear regression of multivariate data by \cite{Aswani2011} and \citet{Wainwright2012}
%  work on applications involving noisy and/or missing data and possible dependence in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing and/or dependent data \citet{Wainwright2012}.  \citet{Wainwright2012}  prove that a simple algorithm based on projected gradient descent will converge in polynomial time to a small neighborhood of the set of all global minimizers. They bound the statistical error, showing that it has the same scaling as the minimax rates for the classical cases of perfectly observed and independently sampled covariates.
% \citet{Loh2012} also considered high-dimensional linear regression and established minimax rates of recovering sparse coefficient vector under additive noise and missing data cases.


\emph{Outline.} 
We start by presenting the procedures of the skeleton regression framework in section \ref{sec::framework}.
In section \ref{sec::regression}, we apply nonparametric regression techniques to the constructed skeleton graph along with theoretical justifications.
% In section \ref{sec::skeletoncons}, we describe the construction of the skeleton. 
% In section \ref{sec::skelkernel}, we apply kernel regression with the geodesic distances on the skeleton.
% In section \ref{sec::lspline}, we fit linear spline on the skeleton structure. 
% We also discuss the feasibility of fitting higher-order splines and the accompanying issues with more details in Appendix \ref{sec::higherspline}.
In section \ref{sec::simulation}, we present some simulation results for skeleton regression and demonstrate the effectiveness of our method on real datasets in Section \ref{sec::real}. 
In section \ref{sec::conclusion}, we conclude the paper and point out some directions for future research.
\footnote{R implementation of the proposed methods can be accessed at \url{https://github.com/JerryBubble/skeletonMethods} and Python implementation can be accessed at \url{https://pypi.org/project/skeleton-methods/}.}

% In section \ref{sec::higherspline} we discuss the feasibility of fitting higher-order splines on the skeleton. We also raise the issue that odd-degree derivatives are directional and hence are dependent on the definition of edge directions, a problem ignored in previous graph-based model literature.

%---------------------------------------------------------------------
\section{Skeleton Regression Framework} \label{sec::framework}

% {\color{magenta}YC: a concept we should mention is fixed-design--the covariates are non-random in our case. Under this case, we do not need to worry about the randomness of the skeleton because the covariates are fixed.}

~~~~In this section, we introduce the skeleton regression framework. 
% random independent samples $(X_1, Y_1), \dots, (X_n, Y_n)$, with $X_j \in \calX \subseteq \RR^d$ being the covariates and $Y_j \in \RR$ being the response for $j = 1, \dots, n$, 
Given design vectors $\set{\bx_i}_{i=1}^n$ where $\bx_i \in \calX \subseteq \RR^d$ for each $i$ and the corresponding responses $\set{Y_i}_{i=1}^n$ in $\RR$, 
a traditional regression approach is to estimate the regression function $m(\bx) = \EE(Y|X = \bx)$.
However, the ambient dimension $d$ can be large while the covariates are distributed on a low-dimensional manifold structure. In this case, $\calX$ can be the union of several disjoint components with different manifold structures, and the regression function can have discontinuous changes from one component to another. 
To handle such manifold-structured data, we approach the regression task by first representing the sample covariate space with a graph, which we call the skeleton, to summarize the manifold structures. We then focus on the regression function over the skeleton graph, which incorporates the covariate geometry in a dimension-independent way.

% In this work, we use the methods in \citet{skelclus} to construct the skeleton, while it can be constructed with other approaches and tuned with subject matter knowledge. 
We illustrate our regression framework on simulated TwoMoon data in Figure \ref{fig::ex1}. The covariates of the TwoMoon data consist of two $2$-dimensional clumps with intrinsically 1-dimensional curve structure, and the regression response increases polynomially with the angle and the radius (Figure \ref{fig::ex1} (a)). 
We construct the skeleton presentation to summarize the geometric structure (Figure \ref{fig::ex1} (b,c) ) and project the covariates onto the skeleton.
The regression function on the skeleton is estimated using kernel smoothing (Section \ref{sec::skelkernel}, illustrated in Figure \ref{fig::ex1} (d) ) and linear spline (Section \ref{sec::lspline}, illustrated in Figure \ref{fig::ex1} (e)).
% and some additional nonparametric methods (Section \ref{sec::higherspline}).  
%We provide some consistency guarantees and computational tricks for those regression methods. 
The estimated regression function can be used to predict new projected covariates. We summarize the overall procedure in Algorithm \ref{alg::Skelreg}.

\begin{algorithm}
\caption{Skeleton Regression}
\label{alg::Skelreg}
\begin{algorithmic}
\State \textbf{Input:} 
Observations $(\bx_1, Y_1), \dots, (\bx_n, Y_n)$.
\State 1. {\bf Skeleton Construction.} Construct a data-driven skeleton representation of the covariates preferably assisted with subject knowledge.
\State 2. {\bf Data Projection.} 
Project the covariates onto the skeleton.
\State 3. {\bf Skeleton Regression Function Estimation.}
Fitting regression function on the skeleton using nonparametric techniques such as kernel smoothing (Section \ref{sec::skelkernel}), k-Nearest Neighbor (Section \ref{sec::SkNN}), and linear spline (Section \ref{sec::lspline}). 
% or additional methods (Section \ref{sec::higherspline})
\State 4. {\bf Prediction.}
Project new covariates onto the skeleton and use the estimated regression function for prediction.

%\State \textbf{Output:} 
\end{algorithmic}
\end{algorithm}
 
 





\subsection{Skeleton Construction}
\label{sec::skeletoncons}

%\subsection{Knots and Edges}
%\label{sec::knotsedges}


~~~~A skeleton is a low-dimensional subset of the sample space
representing regions of interest that 
admits a graph representation. 
For given covariate space $\calX \subseteq \RR^d$,
let $\calV = \{V_j \in \RR^d : j =1 , \dots k\}$ 
be a collection of points of interest
and $E$ be a set of edges connecting points in $\calV$
such that an edge $e_{j\ell}\in E$ 
if the region between $V_j$ and $V_\ell$ is also of interest. 
The tuple $({\cal V}, E)$ together forms 
a graph that represents the focused regions in the sample space.
Notably, different from common graph-based regression approaches that take each sample covariate as a vertex, the set $\calV$ takes representative points of the covariate space and  has size $k \ll n$ where $n$ is the sample size.
Moreover, the points on the edges are also part of the analysis as belonging to the regions of interest, which is different from the usual knot-edge graph.
While the graph $({\cal V}, E)$ contains the region
of interest, 
it is not easy to work with this graph directly. 
Thus, we introduce the concept of the skeleton induced by this graph.

Let $\calE = \{tV_j  + (1-t) V_\ell: t \in (0,1),  e_{j\ell} \in E \}$
be the collection of line segments induced by the edge set $E$. 
We define the skeleton of $({\cal V}, E)$
as $\calS = \calV \cup \calE $, i.e.,
$\calS$ is the points of interest and the associated line segments
representing the regions of interest. 
Clearly, $\calS$ is a collection of one-dimensional line segments
and zero-dimensional points so it is independent of the ambient dimension $d$, but the physical location of $\calS$ is meaningful as representing the region of interest. 
The idea of skeleton regression is to build a regression model
on the skeleton $\calS$. 



%~~~~For given covariate space $\calX \subseteq \RR^d$, we construct a skeleton graph  with $k$ knots $\calV = \{V_j \in \RR^d : j =1 , \dots k\}$ 
%and the set of connected edges $\calE = \{tV_j  + (1-t) V_\ell: t \in (0,1),  V_j \text{ connected to } V_\ell \}$. 
%For the parametrization, we have $t \in (0,1)$ to exclude the knots. Notably, the knots and edges in our framework, different from the usual graphs, have physical locations in the ambient space.
%We denote the skeleton as $\calS = \calV \cup \calE $. 

\subsubsection{A data-driven approach to construct skeleton}

~~~~The skeleton should ideally be constructed based on the analyst's judgment or prior knowledge of the region  of interest. 
However, this information may be unavailable and we have to construct a skeleton from the data.
In this section, we give a brief description of a data-driven approach proposed in \citet{skelclus} that constructs the skeleton to represent high-density regions. 
%The skeleton graph is constructed to give an approximate representation of the data structure and many existing prototype-based methods can be used for this purpose. 
The method constructs knots as the centers from the $k$-means clustering with a large number of knots \footnote{By default $[\sqrt{n}]$. We explore the effect of choosing different numbers of knots with empirical results.}.
% Given the knots, the edges are constructed by approximate Delaunay triangulation. 
% The exact Delaunay triangulation \cite{Delaunay1934} has an edge between two knots if their corresponding Voronoi cells share a common boundary \citep{voronoi1908recherches}. 
% The Voronoi cell $\CC_{j}$ associated with a knot $c_{j}$ is the set of points in $\calX$ whose distance to $c_{j}$ is the smallest compared to other knots that
% \begin{align}
%     \CC_j = \{x \in \calX: \norm{x, c_j} \leq d(x, c_\ell) \ \  \forall l \neq j\},
% \end{align}
% where $||.||$ denotes the Euclidean norm.
% Exact Delaunay triangulation can be computationally heavy, and as in \cite{skelclus}, for approximation, 
% Then an edge between two knots $V_i, V_j$ is connected if there is at least one covariate point whose two nearest neighbors are $V_i, V_j$. 
The edges are connected by examining the sample 2-Nearest-Neighbor (2-NN) region of a pair of knots $(V_j,V_\ell)$  (see Figure \ref{fig::2nn})
\begin{align}
    B_{j\ell} = \{ X_m, m=1,\dots,n :\norm{x- V_i} > \max\{ \norm{x-V_j}, \norm{x- V_\ell} \}, \forall i \neq j, \ell \},
\label{eq::2NNregion}
\end{align}
where $||.||$ denotes the Euclidean norm, and an edge between $V_j$ and $V_\ell$ is added if $B_{j\ell}$ is non-empty. 
The method can further prune edges or segment the skeleton by using hierarchical clustering with respect to the Voronoi Density weights defined as
$S_{j\ell}^{VD} = \frac{\frac{1}{n}\abs{B_{j\ell}}}{\norm{V_j - V_\ell}}.$
We provide more details about this approach in Appendix \ref{ref::skelconsVoron}.

% \YC{We should provide a short summary of the Voronoi density skeleton construction in the appendix.}


\begin{figure}
% \captionsetup{skip=1pt}
\centering
\includegraphics[height=4cm]{figures/2nnRegion_shaded.jpeg}
\caption{Orange shaded area illustrates the 2-NN region of knots $1,2$.}
\label{fig::2nn}
\end{figure}


\begin{remark}
The idea of using the $k$-means algorithm to divide data into cells
and perform analysis based on the cells has been proposed
in the literature for fast computation.
%for fast computation has been applied in many machine learning realms. 
\cite{InvertedFiles}, when carrying out an approximate nearest neighbor search, proposed to divide the data into Voronoi cells by $k$-means and do a neighbor search only in the same or some nearby cells. \cite{InvertedMultiIndex} adopted the Product Quantization technique to construct cell centers for high-dimensional data as the Cartesian product of centers from sub-dimensions. 
% $k$-means algorithm can be slow for large-scale data, but \cite{johnson2019billion} has implemented the $k$-means algorithm efficiently on the GPU base, which dramatically improves the calculation speed of the algorithm. 
\end{remark}


% the Edge weights are measured by Voronoi density proposed in \citet{skelclus}.
% Connected with the approximate triangulation above, the \emph{Voronoi density (VD)} measures the edge weight of  $(c_j,c_\ell)$ with
% \begin{align}
%     S_{j\ell}^{VD} = \frac{\PP(A_{j\ell})}{\|c_j - c_\ell \|}.
% \end{align}
% Namely, we divide the probability of overlapping region by the mutual Euclidean distance.
% The division of the distance adjusts for the fact that 2-NN regions have different sizes and provides more weights to edges between knots that are close to each other. 

% In practice we estimate $ S_{j\ell}^{VD}$ by a sample average. 
% Specifically, the numerator $\PP(A_{j\ell}) $ is estimated by $\hat{P}_n (A_{j\ell}) = \frac{1}{n}\sum_{i=1}^n I(X_i\in A_{j\ell})$
% and the  estimator for the VD is
% \begin{align}
%     \hat{S}_{j\ell}^{VD} &= \frac{\hat{P}_n ({A}_{j\ell})}{\|{c}_j - {c}_\ell\|}.
% \end{align}

\subsection{Skeleton-Based Distance}

~~~~One of the advantages of the physically-located skeleton is that it allows for a natural definition of the skeleton-based distance function $d_\calS(.,.): \calS \times \calS \to \RR^{+}\cup \{\infty\}$. 
Let $\bs_j, \bs_\ell \in \calS$ be two arbitrary points on the skeleton and note that, different from the usual geodesic distance on a graph, in our framework $\bs_j, \bs_\ell$ can be on the edges. We measure the skeleton-based distance between two skeleton points as the graph path length as defined below:

\begin{itemize}
    \item If $\bs_j, \bs_\ell$ are disconnected that they belong to two disjoint components of $\calS$, we define
    \begin{align}
    d(\bs_j, \bs_\ell) = \infty
    \label{eq::graphdist}
    \end{align} 
    \item If $\bs_j$ and $\bs_\ell$ are on the same edge, we define the skeleton distance as their Euclidean distance that
\begin{align}
    d_\calS(\bs_j, \bs_\ell) = ||\bs_j - \bs_\ell|| 
    \label{eq::graphdist0}
\end{align}
    \item For $\bs_j$ and $\bs_\ell$ on two different edges that share a knot $V_0$, the skeleton distance is defined as
    \begin{align}
    d_\calS(\bs_j, \bs_\ell) = ||\bs_j - V_{0}|| + ||\bs_\ell - V_{0}||
    \label{eq::graphdist1}
\end{align}
    \item Otherwise, let knots $V_{i(1)}, \dots, V_{i(m)}$ be the vertices on a path connecting $\bs_j, \bs_\ell$, where $V_{i(1)}$ is one of the two closest knots of $\bs_j$ and $V_{i(m)}$ is the other closest knots of $\bs_\ell$. We add the edge lengths of the in-between knots to the distance that 
\begin{align}
\begin{split}
    d_\calS(\bs_j, \bs_\ell) &= ||\bs_j - V_{i(1)}|| + ||\bs_\ell - V_{i(m)}|| + \sum_{p=1}^{m-1}\norm{V_{i(p)}, V_{ i(p+1)}}
\end{split}
\label{eq::graphdist2}
\end{align}
and we use the shortest path length if there are multiple paths connecting $\bs_j$ and $ \bs_\ell$.
\end{itemize}

An example illustrating the skeleton-based distance is shown in Figure \ref{fig::skeldist}.
Like the shortest path (geodesic) distance that makes a usual knot-edge graph into a metric space, the skeleton-based distance is also a metric on the skeleton graph. 
In the following sections, we will discuss methods to perform regression on space only with the defined metric.


\begin{figure}[ht]
% \captionsetup{skip=1pt}
\centering
\includegraphics[height=4cm]{figures/skelregdistraw.png}
\caption{Illustration of skeleton-based distance. Let $C_1, C_2, C_3, C_4$ be the knots, and let $S_2,S_3,S_4$ be the mid-point on the edges $E_{12},E_{23},E_{34}$ respectively. Let $S_1$ bet the midpoint between $C_1$ and $S_2$ on the edge. Let $d_{ij} = \norm{C_i - C_j}$ denotes the length of the edge $E_{ij}$. $d_\calS(S_1,S_2) = \frac{1}{4} d_{12}$ illustrated by the blue path.  $d_\calS(S_2,S_3) = \frac{1}{2} d_{12} + \frac{1}{2} d_{23}$ illustrated by the green path. $d_\calS(S_2,S_4) = \frac{1}{2} d_{12} + d_{23} + \frac{1}{2} d_{34}$ illustrated by the orange path.}
\label{fig::skeldist}
\end{figure}



%
% Note that in skeleton regression framework $w_{j\ell} =\norm{c_j - c_\ell}$, but the defined skeleton distance can be applied to general edge weights where partial weights are meaningful.

\begin{remark}
We may view the skeleton-based distance as an approximation of the geodesic distance on the underlying data manifold. Moreover, to make a stronger connection to the manifold structure, it is possible to define edge lengths through local manifold learning techniques that have better approximations to the local manifold structure. However, using more complex local edge weights can pose additional challenges for the data projection step described in the next section and we leave this as a future direction.
%but the theoretical justification of this connection is hard due to the randomness of how such knots are chosen.
\end{remark}


\subsection{Data Projection}

~~~~For the next step, we project the sample covariates onto the constructed skeleton.
For given covariate $\bx$, let $I_1(\bx), I_2(\bx) \in \{1,\dots,k\}$ be the index of its closest and second closest knots in terms of the Euclidean metric. 
We define the projection function $\Pi(.): \calX \to \calS$ for $\bx \in \calS$ as (illustrated in Figure \ref{fig::skelproject}):
\begin{itemize}
    \item[Case I: ] If $V_{I_1(\bx)}$ and $V_{I_2(\bx)}$ are not connected, $\bx$ is projected onto the closest knot that
    $\Pi(\bx) = V_{I_1(\bx)}$ 
    \item[Case II: ] If $V_{I_1(\bx)}$ and $V_{I_2(\bx)}$ are connected, $\bx$ is projected with the Euclidean metric onto the line passing through $V_{I_1(\bx)}$ and $V_{I_2(\bx)}$ that, let $t = \frac{\left(\bx - V_{I_1(\bx)}\right)^T\cdot \left(V_{I_2(\bx)}-V_{I_1(\bx)}\right)}{\norm{V_{I_2(\bx)}-V_{I_1(\bx)}}^2}$ be the projection proportion,
    \begin{align}
    \Pi(\bx)  = V_{I_1(\bx)} + \left(V_{I_2(\bx)}-V_{I_1(\bx)}\right) \cdot 
    \begin{cases}
    0, \text{ if } t <0\\
    1, \text{ if } t>1\\
    t, \text{ otherwise}
    \end{cases}
    \end{align}
    where we constrain the covariates to be projected onto the closest edge.
\end{itemize}


\begin{figure}[ht]
% \captionsetup{skip=1pt}
\centering
\includegraphics[height=3cm]{figures/skelregproject.png}
\caption{Illustration of projection to the skeleton. The skeleton structure is given by the black dots and lines. Data point $X_1$ is projected to $S_1$ on the edge between $C_1$ and $C_2$. Data point $X_2$ is projected to knot $C_2$.}
\label{fig::skelproject}
\end{figure}


Note that with the projection defined above, a non-trivial volume of points can be projected onto the end knots of the skeleton graph as belonging to Case I or due to the constraining in Case II. 
This adds complexities to the theoretical analysis of the proposed regression framework and leads to our separate analysis of the different domains of the graph in Section \ref{sec:kernelConsistent}.



\section{Skeleton Nonparametric Regression}
\label{sec::regression}

% {\color{magenta}I reversed the logic here to make it smooth.
% Also, there are a lot of notation inconsistency. I tried
% to clarify them but please double check.}
~~~~Covariates are mapped to locations on the skeleton after the data projection step and are equipped with the skeleton-based distances. 
In this section, we apply nonparametric
regression techniques to the skeleton graph with projected data points. 
%Note that we only have the metric defined on the skeleton graph but there is no nice structures like vectors or inner products as in the Euclidean space or the Hilbert spaces, which imposes constraints on applicable regression methods.
We study three feasible nonparametric approaches: the skeleton-based kernel regression (S-Kernel), the skeleton-based k-nearest-neighbor method (S-kNN), and the linear spline model (S-Lspline).
At the end of this section, we discuss the challenges of applying some other nonparametric regression methods in the setting of skeleton graph.


\subsection{Skeleton Kernel Regression}
\label{sec::skelkernel}
~~~~
We start by adopting kernel smoothing to the skeleton graph. 
Let $\bs_1,\cdots, \bs_n$ be the projections on the skeleton
from $\bx_1,\cdots, \bx_n$, i.e., $\bs_i = \Pi(\bx_i)$. 
With the skeleton-based distances, the skeleton kernel regression makes a prediction at the location
$\bs \in \calS$ as
\begin{align}
    \hat{m}(\bs) = \frac{\sum_{i=1}^N K(d_\calS(\bs_i, \bs)/h) Y_i}{\sum_{j=1}^N K(d_\calS(\bs_j, \bs)/h)},
\end{align}
where $K(\cdot) \geq 0$ is a smoothing kernel
such as the Gaussian kernel and $h>0$
is the smoothing bandwidth that controls the amount of smoothing.
In practice, we choose $h$ by cross-validation.
Essentially, the estimator $\hat{m}(\bs)$ is the
usual kernel regression applied to a general metric space (skeleton) rather than the usual Euclidean space.
Notably, the kernel function calculation only depends on the skeleton distances and hence is independent of neither the ambient dimension of the original input nor the intrinsic dimension of the manifold structure.
% {\color{magenta}YC: we often define $K$ rather than $K_h$
% so that the kernel function stays invariant--as your assumptions K is written. Please change notations accordingly.}

It should be also noted that $\hat{m}(\bs)$ only makes prediction on 
$\bs \in \calS$. 
If we are interested in predicting the outcome 
at any arbitrary point $\bx\in\calX$, 
the prediction will be based on the projected point, i.e.,
$
\hat m(\bx) = \hat m\left(\Pi(\bx)\right), 
$
where $\Pi(\bx) \in \calS.$
Because of the above projection property, one can think of the skeleton kernel regression as an estimator
to the following skeleton-projected regression function
\begin{align}
    m_\calS(\bs) = \EE(\bY|\Pi(\bX) = \bs), \bs \in \calS.
\end{align}
We study the convergence of $\hat{m}(\bs)$ to $m_\calS(\bs)$ in what follows.





%In this section, we apply kernel smoothing to the skeleton-projected covariates based on the skeleton-based distances. 
%Instead of estimating the regression function defined on $\calX$, we estimate the \textit{projected} regression function 
%\begin{align}
%    m_\calS(\bs) = \EE(y|\pi(\bx) = \bs), \bs \in \calS 
%\end{align} 
%on the skeleton domain $\calS$.  
%Let $K_h(.) = K(./h)$ be a non-negative kernel function with bandwidth $h > 0$ and let $\bs_1, \dots, \bs_n$ denote the skeleton-projected covariates that $\bs_i = \Pi(\bx_i)$ for $i = 1,\dots,n$, the corresponding skeleton-based kernel (S-kernel) regressor for a point $\bs \in \calS$ is
%\begin{align}
%    \hat{m}(\bs) = \frac{\sum_{j=1}^N K_h(d_\calS(\bs_j, \bs)) Y_j}{\sum_{j=1}^N K_h(d_\calS(\bs_j, \bs))} 
%\end{align}
%An example kernel function  is the Gaussian kernel that
%\begin{align}
%    K_h(d_\calS(\bs_j, \bs_\ell)) = \exp\bigg(- \frac{d_\calS(\bs_j, \bs_\ell)^2}{h^2}\bigg)
%\end{align}



% In practice we use skeleton-based truncated Gaussian kernel. Specifically, let $V_p,V_q$ be the two closest knots of the point $\bx$. If $V_p$ and $V_q$ are connected in the skeleton, then we constrain the Gaussian kernel to the Two-Nearest-Neighbor region $A_{pq}$ of $V_p,V_q$ (see Equation \ref{eq::2NNregion}) that
% \begin{align*}
%     K_h(\Pi(\bx), \bs) = 
%         \begin{cases}
%       \exp\bigg(- \frac{d_\calS(\Pi(\bx_j), \bs)^2}{h^2}\bigg) & \text{if } \bs \in A_{pg}\\
%       0 & \text{otherwise}
%     \end{cases} 
% \end{align*}
%Notably, the kernel function calculation only depends on the skeleton distances and hence is independent of neither the ambient dimension of the original input nor the intrinsic dimension of the manifold structure.
%The smoothing bandwidth $h$ can be chosen by cross-validation. 


\subsubsection{Consistency of S-Kernel Regression}
\label{sec:kernelConsistent}
% \subsubsection{Continuous Edge \& Discrete Knots Formulation}

% {\color{magenta}YC: one important thing about the theoretical results
% is the discussion. We need a discussion
% on every theoretical result to interpret it or discuss its
% property and how it is similar to or different from other work.}


%~~~~In this section, we present the convergence result for the S-kernel regressor.
~~~~
Our analysis assumes that the skeleton is fixed and given and focuses on the estimation of the regression function.
To evaluate the estimation error, we must first impose some concepts of distribution on the skeleton. However, due to the covariate projection procedure, the probability measures on the knots and edges are different. Therefore, we analyze them separately. On an edge, the domain of the projected regression function varies in one dimension, resulting in a standard univariate problem for estimation.
% We start with the formulation of the regression problem. For given covariate space $\calX \subseteq \RR^d$, the skeleton graph has $\calV = \{V_j \in \RR^d : j =1 , \dots k\}$ the set of $k$ knots and
% $\calE = \{tV_j  + (1-t) V_\ell: t \in (0,1),  w_{j\ell} > 0\}$ the set of connected edges with nonzero weights $w_{j\ell}$. Note that for the parametrization we have $t \in (0,1)$ to exclude the knots.
% We denote the skeleton as $\calS = \calV \cup \calE $. 
%can connect to multiple edges and the degrees of the knots vary, making the domain at the knots not suited under a 1-dimensional characterization. Therefore, we formulate the regression problem differently for edges and knots.}
%In particular, we impose the one-dimensional Lebesgue measure with respect to the parametrization $t$ as in the definition of $\calE$ in Section \ref{sec::skeletoncons}, and the true projected regression model for a point $\bs$ on edge $(V_j, V_\ell)$ is 
%\begin{align*}
%    m_\calS(\bs) = m_{j\ell}(\bs) = m_{j\ell}(t)
%\end{align*}
%where $t$ is the parametrization for $\bs$.
% Further, we assume the density function $g(t)$ and the projected regression function $m_\calS(t)$ are Lipschitz continuous on the edges. 
For the case of knots, a nontrivial region of the covariate space can be projected onto a knot, leading to a nontrivial probability mass at the knot. 

%Hence, we assign discrete counting measure on each knot,
%and the true projected regression model at $\bs \in \calV$ is a constant function that
%\begin{align*}
%    m_\calS(\bs) = M_j, \bs = V_j.
%\end{align*}
%\revise{Note that uniform convergence cannot be established under this formulation because of the probability mass on the knots.}


For simplicity, we write
$ K_h(\bs_j, \bs_\ell) \equiv  K(d_\calS(\bs_j, \bs_\ell)/h)$
for $\bs_j, \bs_\ell \in \calS$.
Let $\calB(\bs, h) = \set{\bs' \in \calS: d_\calS(\bs', \bs) < h} $ be the ball on skeleton centered at the point $\bs \in \calS$ with radius $h$.
% {\color{magenta}YC: The support
% is restricted to the skeleton? And the distance should be replaced by skeleton distance I think.}
% Let $\mu$ be the one-dimensional Lebesgue measure.
% and note that, as $h \to 0$, $\mu(\calB(\bs, h)) \to 0$ for every $\bs \in \calS$. 
We can decompose the kernel regression estimator into edge parts and knot parts as
\begin{align} \label{eq::kerneldecomp}
\begin{split}
    \hat{m}(\bs) &= \frac{\sum_{j=1}^n Y_j  K_h(\bs_j, \bs) }{\sum_{j=1}^n K_h(\bs_j, \bs)} \\
    &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calE)  + \frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calV )}{\frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs)I(\bs_j \in \calE ) + \frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs) I(\bs_j \in \calV ) } \\
    &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h))}{\frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h)) }
\end{split}
\end{align}
In the last line, we emphasize that the knots and edges in the kernel estimator have a meaningful contribution only within the support of the kernel function. We inspect the different domain cases separately in the following sections.

% {\color{magenta}YC: this part requires a lot of revision. 
% We have not yet defined the density $g(\bs)$.
% The ball $B(\bs, h)$ is also not well-defined since
% we are restricting it to the skeleton.
% Also, the assumption (K) is for the kernel function,
% not for the point of interest.

% I think the density on skeleton edge is:
% $g(\bs) = \lim_{r\rightarrow 0 }\frac{P(\Pi_\calS(X)\in \calB(\bs , r))}{2r}$.
% This is known as the 1-Hausdorff density.

% Note that $g(\bs) = \infty$ if $\bs$ is at a knot point
% that has a probability mass.

% }

% \YC{Need to be very careful when using $\bS$ vs $\bs$. Similarly for $\bX$ vs $\bx$.}

For the model and assumptions, we let $Y_j =  m_\calS(\bS_j)+U_j, \bS_j \in \calS$, and $\EE(U_j|\bS_j) = 0$ almost surely. 
Let $\sig^2(\bs) = \EE(U_j^2 | \bS_j = \bs)$. 
Let the density on the skeleton edge be defined as the 1-Hausdorff density that
$g(\bs) = \lim_{r\downarrow 0 }\frac{P(\bS\in \calB(\bs, r))}{2r}$.
Note that $g(\bs) = \infty$ if $\bs$ is at a knot point
that has a probability mass.
We consider the following assumptions: 
\begin{itemize}
    \item[\textbf{A1}] $\sig^{2}(\bs)$ is continuous and  uniformly bounded.
    \item[\textbf{A2}] The skeleton edge density function $g(\bs) > 0$ and are bounded and Lipschitz continuous for $\bs \in \calE$.
    \item[\textbf{A3}] $m_\calS(\bs) g(\bs)$ is bounded and Lipschitz continuous for $\bs \in \calE$.
    \item[\textbf{K}] The kernel function has compact support and satisfies $\int K(x) dx = 1$, $\int K^2(x) dx < \infty$, $\int xK(x) d x = 0$,  and $\int x^2 K(x) dx < \infty$ 
    % {\color{magenta}YC: I don't think you need so many additional assumptions. This is way  beyond the usual kernel assumption. You should just need compact support, symmetric, integrated into 1, and bounded 2nd moment.}
\end{itemize}
Conditions A1 and K are general assumptions that are commonly made in kernel regression analysis.
A2 and A3 are mild conditions that can be sufficiently implied by the boundedness and Lipschitz continuity of the density and regression function in the ambient space along with non-overlapping knots that the area of the orthogonal complements have Lipschitz changes.
We do not assume the second-order smoothness commonly required for kernel regression because requiring higher-order derivative smoothness would necessitate specifying directions on the graph, which may present difficulties in model formulation. 
We include further discussions about derivatives on the skeleton in Section \ref{sec::otherParametric}.
% (see Section \ref{sec::higherspline} and Appendix \ref{sec::directions} for discussions and examples).

% {\color{magenta}YC: I think the notation $\calB(\bs, h)$ might be misleading. What you meant I guess is:
% $$
% I(\bs_j \in \calV \cap  \calB(\bs, h)) = I(\bs_j \in \calV: d_\calS(\bs_j, \bs)\leq h).
% $$
% Please clarify this.}



\subsubsection{Convergence of the Edge Point}

~~~~We first look at an edge point $\bs \in E_{j\ell} \in \calE$. 
In this case, as $n\to \infty, h\to 0$, for sufficiently large $n$, we have $\calB(\bs, h) \subset E_{j\ell}$, and the skeleton distance is the $1$-dimensional Euclidean distance for any point within the support. Therefore, we have a convergence rate similar to the $1$-dimensional kernel regression estimator \citep{Bierens1983, wasserman2006all, Chen2017}.


\begin{thm}[Consistency on Edge Points]
For $\bs \in \calE$ an edge point, assume conditions A1-3 and K hold for all points in $\calE \cap  \calB(\bs, h)$, as $n \to \infty$, $h\rightarrow0$, $nh\rightarrow\infty$,
\begin{align}
    \abs{\hat{m}_n(\bs) - m_\calS(\bs)} = O(h) + O_p\bigg(\sqrt{\frac{1}{n h}}\bigg) 
\end{align}
\label{thm::edge}
\end{thm}

% Note that, if condition A3 is replaced with the usual second-order derivative smoothness, we have the bias to be of rate $O(h^2)$, but with weaker Lipschitz continuity condition in A3 we have the bias to have rate $O(h)$. 
We leave the proof in Appendix \ref{sec::contproof}.
Theorem \ref{thm::edge} gives the convergence rate for a point on the edge of the constructed skeleton. 
%Note that, 
The convergence rate at the bias is $O(h)$, which is
the usual rate when we only have Lipschitz smoothness (A2) of $m_{\calS}$.
One may be wondering if we can obtain a faster rate such as $O(h^2)$
if we assume higher-order smoothness of $m_{\calS}$. 
While it is possible to obtain a faster rate if we have a higher-order smoothness, this assumption will not be reasonable
because $m_{\bS}(\bs) = \EE(Y|\Pi(X) = \bs)$ is defined via a project.
The region being projected onto $\bs$ is continuously changing
and may not be differentiable due to the boundary of Voronoi cells.
Therefore, the Lipschitz continuity (A2) is reasonable while
higher-order smoothness is not.
%as we assume a Lipschitz continuity condition in A3 rather than the derivative smoothness used in classical kernel smoothing analysis, the bias term in Theorem \ref{thm::edge} has rate $O(h)$ rather than the usual $O\left(h^2\right)$ rate.




\subsubsection{Convergence of the Knots with Nonzero Mass}
~~~~We then look at the knots with nonzero probability mass that $\bs \in \calV$ with $p(\bs) > 0$, where we use $p(\bs)$ to denote the probability mass on a knot. 
This case mainly occurs for knots with degree $1$ on the skeleton graph, when a non-trivial region of points is projected onto such knots. For example, refer to knot C2 in Figure \ref{fig::skelproject}. 
% The consistency result is presented in Theorem \ref{thm::knotconsistency}.

\begin{thm}[Consistency on Knots with Nonzero Mass]

For $\bs \in \calV$ a knot point, let the probability mass at $\bs$ be $P(\Pi_\calS(X)=\bs) \equiv p(\bs) > 0$ and assume $\sig^2(\bs)$ bounded. Also assume conditions A1-3 and K hold for all edge points in $\calE \cap  \calB(\bs, h)$. We have,  as $n\to \infty$, $h\rightarrow0$, and $nh\rightarrow\infty$,
\begin{align}
    \abs{\hat{m}(\bs) - m_\calS(\bs)} = O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right)
\end{align}
\label{thm::knotconsistency}
\end{thm}
Theorem \ref{thm::knotconsistency} gives the convergence result for a knot point with a nontrivial mass of the skeleton. 
The bias term $O(h)$ comes from the influence of nearby edge points.
For the stochastic variation part, instead of having the  $O_p\left(\sqrt{\frac{1}{n h}}\right)$ rate as the usual kernel regression and in Theorem \ref{thm::edge}, we have $O_p\left(\sqrt{\frac{1}{n}}\right)$ rate which comes from averaging the observations projected onto the knots. The proof of Theorem \ref{thm::knotconsistency} is provided in Appendix \ref{sec::knotproof}.


\subsubsection{Convergence of the Knots with Zero Mass}
~~~~We now look at a knot point $\bs \in \calV$ with no probability mass that $p(\bs) = 0$. 
This is can be the case for a knot with a degree larger than $1$ like knot C3 in Figure \ref{fig::skelproject}. Since we define edge sets excluding the knots, there will be no density as well as no probability mass at $\bs$. Note that, with some reformulation, degree $2$ knots can be parametrized together with the two connected edges and, under the appropriate assumptions, Theorem \ref{thm::edge} applies, giving consistency estimation with $O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right) $ rate. However, density cannot be extended directly to knots with a degree larger than $2$, but the kernel estimator still converges to some limits as presented in the Proposition below.
% However, we can make sense of this case by thinking of this point as part of the connecting edges and therefore impose some density $g(\bs) > 0$ and some regression value $m(\bs)$ on the knot  $V_\ell$. If the limiting density values and regression values from all the connected edges agree, then $g(\bs)$ and $m(\bs)$ are well-defined, and the conditions A1-4 are satisfied given those conditions are satisfied on all the connected edges. Consequently the same convergence analysis as in Theorem \ref{thm::edge} applies, giving $O(h) + O_p\big(\sqrt{\frac{1}{n h}}\big) $ rate.
\begin{prop}
\label{prop::zeroknot}
For $\bs \in \calV$ a knot point, assume conditions A1-3 hold for all points in $\calE \cap  \calB(\bs, h)$ and let the probability mass at $\bs$ be $p(\bs) = 0$. 
We assume condition K for the kernel function.
Let $\calI$ collect the indexes of edges with one knot being $\bs$.
For $\ell \in \calI$ and edge $E_\ell$ connects $\bs$ and $V_\ell$,
let $g_\ell(t) = g((1-t)\bs + t V_\ell)$ and $g_\ell(0) = \lim_{x\downarrow 0} g_{\ell}(x)$.
Let  $m_\ell(t) = m_\calS( (1-t)\bs + t V_\ell)$ and $m_\ell(0) = \lim_{t \downarrow 0} m_\ell(t)  $. 
We have,  as $n\to \infty$, $h\rightarrow0$, and $nh\rightarrow\infty$,
% Denote $\sigma_\ell^2(t) = \EE(|U_j|^2 | \bs_j = (1-t) \bs + t V_\ell )$ and $\sigma_\ell^2(0) = \lim_{t \downarrow 0} \sigma_\ell^2(t) $.
\begin{align}
    \hat{m}(\bs) 
    &= \frac{ \sum_{\ell \in \calI}   m_\ell(0) g_\ell(0)  }{ \sum_{\ell \in \calI}   g_\ell(0) } + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right).
\end{align}
%{\color{magenta}YC: the point $0$ should be replaced by
%the location of knot $\mathbf{s}$, right?}
\end{prop}

Proposition \ref{prop::zeroknot} shows that, under proper conditions, the skeleton kernel estimator on a zero-mass knot converges to the weighted average of the limiting regression values of the connected edges, and the convergence rate is the same as the edge points shown in Theorem \ref{thm::edge}.
The proof is included in Appendix \ref{sec::zeroknotproof}.



% \YC{I was not very sure about this part on the boundary bias.}

\begin{remark}
The domain $\calS$ of the regression function can be seen as bounded, and hence the boundary bias issue can arise. 
The true manifold structure's boundary can be different from the boundary of the skeleton graph, making the consideration of the boundary more complicated.
However, the boundary of the skeleton is the set of degree $1$ knots, and, under our formulation, knots have discrete measures, so the consideration of boundary bias may not be necessary for the proposed formulation.
However, some boundary corrections can potentially improve the empirical performance and we leave it for future research.
\end{remark}




%\YC{This will not reduce to the variance in Euclidean space... 
%If we have a metric $d$, the variance should be 
%$$
%\hat \sigma^2 = \frac{2}{n (n-1)}\sum_{i\neq j} d^2_{\calS}(\bs_i, \bs_j).$$
%Suppose the skeleton can be decomposed into $\calS_1,\cdots, \cal_M$, $M$ connected components. 
%Then for each $m$, we denote $I_m$ be the indices
%of observations projected to $\calS_m$ and $n_m = |I_m|$
%be the number of such observations.
%Then we can define the 
%variance of component $\calS_m$ as 
%$$
%\hat \sigma^2_m = \frac{2}{n_m (n_m-1)}\sum_{i\neq j\in I_m} d^2_{\calS}(\bs_i, \bs_j).
%$$
%}

%\revise{
%\subsection{Choice of Bandwidth} \label{sec::bandwidth}
%~~~~A key parameter for the kernel regressor is the smoothing bandwidth $h$. In practice the best performance is usually achieved by picking $h$ through cross-validation. To inform the bandwidth values to test, we adopt some rule of thumb bandwidths as the base and apply different scaling factors for parameter tuning. This is to provide some reasonable bandwidth choices with limited computational resources. Empirically we observe that some scaling of the base bandwidth can give better regression performances, and fine-tuning bandwidth with cross-validation can be beneficial if computation allows.
%}

%\begin{comment}
%\subsubsection{Initialization for Cross-Validation}
%\cite{li2007nonparametric} proposed to use $h = \sig_x n^{-1/5}$ for kernel regression, which is similar to the Silverman's rule of thumb bandwidth \citep{silverman1986density} for density estimation that $h = \left(\frac{4}{3}\right)^{1/5} \hat{\sig}_x n^{-1/5}$, where $\sig_x$ and $\hat{\sig}_x$ are the true and estimated standard deviation for the covariates. However, the skeleton-projected covariates, different from the original $\RR^d$-based covariates, do not have a clear definition of mean and variance. So we introduce a simple way to measure the variability of covariates on the skeleton as replacements for the usual standard deviation. \\
%\ \\
%The skeleton can be compressed as a $1$-dimensional structure by picking a point $\bs \in \calS$ as the origin and calculate the skeleton-based distances between $\bs$ and all the skeleton-projected covariates $\bs_\ell, \ell = 1,\dots, n$. Based on this vector of skeleton distances, let $\Bar{d}_\calS(\bs) = \frac{1}{n}\sum_{j=1}^n d_{\calS}(\bs, \bs_j)$ be the average skeleton distance to $\bs$. We define a quantity like the standard deviation that
%\begin{align*}
%    \hat{\sig}_{\bs} = \sqrt{\frac{1}{n} \sum_{j = 1}^n  \left(d_{\calS}(\bs, \bs_j) -\Bar{d}_\calS(\bs) \right)^2}.
%\end{align*}
%However, $\hat{\sig}_{\bs}$ can be different with different $\bs$ as the origin. To accommodate for this, we can get the $n\times n$ matrix of skeleton distances between each observed $\bs_j$, compute $\hat{\sig}_{\bs_j}$ for each row, and define the average as the overall variability measure for the skeleton-based covariates. But the computation of the $n\times n$ matrix of skeleton-based distances can be computation expensive (the computation of S-kernel regressor for new predictions only requires the skeleton-based distance between the new test observations and the training points, and hence the skeleton-distances between the training samples are extra calculations only used for the bandwidth). Therefore, to save some computation, we instead compute the $k \times n$ skeleton distance matrix between the knots and the skeleton-projected training covariates, get the $\hat{\sig}_{V_j}$ for each knot, and use the average as variability of skeleton-based covariates. Formally, we let
%\begin{align} \label{eq::skelsd}
%    \hat{\sig}_\calS = \frac{1}{k } \sum_{i = 1}^k \hat{\sig}_{V_i} =  \frac{1}{k} \sum_{i = 1}^k \sqrt{\frac{1}{n} \sum_{j = 1}^n  \left(d_{\calS}(V_i, \bs_j) -\Bar{d}_\calS(V_i)  \right)^2}
%\end{align}
%Note that in case there are disconnected components in the skeleton, observations in different components have infinite skeleton-based distances, and we ignore the infinite values in the standard deviation calculation. In other words, the standard deviation at each skeleton point is only calculated within the same component.
%The base bandwidth used in this work is 
%\begin{align} \label{eq::baseBand}
%    h_{hns} = \left(\frac{4}{3}\right)^{1/5} \cdot \hat{\sig}_\calS \cdot n^{-1/5}
%\end{align}\\
%The above discussion on choosing the base bandwidth is to inform a starting point for the cross-validation bandwidth choice procedure and, with limited computational resource, to offer some reasonable bandwidth choices without much parameter tuning. More comprehensive cross-validation procedure to fine tune the bandwidth can potentially lead to better performance results than the procedure used in this work.
%\ \\
%\begin{remark}
%In practice, the bandwidth can also be chosen in an adaptive way. For each prediction, the skeleton-based distances between that test covariate and all the training points are required to compute the S-kernel regressor, and the standard deviation of this vector can be used to get the base bandwidth. However, this will make the base bandwidth at each prediction point different and hence adaptive. The adaptive way saves the extra skeleton distance calculations between the training points and can take less time. Empirically we observe that the global bandwidth proposed in Equation \ref{eq::baseBand} and the adaptive bandwidth have similar performances while the global bandwidth has slightly better performances on our data examples in Section \ref{sec::simulation} and \ref{sec::real}. Therefore all the S-kernel regressor results presented in this work choose bandwidth with cross-validation with the base given by the global bandwidth in Equation \ref{eq::baseBand}.
%\end{remark}


\subsection{Skeleton kNN regression}
\label{sec::SkNN}

% {\color{magenta}Add a description on skeleton knn regression.}
~~~~
The $k$-Nearest Neighbor (kNN) method
can be easily applied to the skeleton
using the distance on the skeleton.
For a given point on the skeleton at $\bs \in \calS$, 
we define the distance to the k-th nearest observation
on the skeleton as 
\begin{align}
    R_k(\bs) = \min\left\{r>0: \sum_{i=1}^n I(d_\calS(\bs_i, \bs)\leq r)\geq k\right\}.
\end{align}
Note that it is possible to have multiple
observations being the $k$-th nearest observation
due to observations being projected to the vertices.
In this case, we can either randomly choose
from them or consider all of them.
Here we include all of them in the calculation.
The skeleton-based $k$NN regression (S-kNN) 
predicts the value of outcome at $\bs$ as
\begin{align}
    \hat{m}_{SkNN}(\bs) = \frac{\sum_{i=1}^k Y_{i} I(d_\calS(\bs_i, \bs)\leq R_k(\bs))}{\sum_{j=1}^k I(d_\calS(\bs_j, \bs)\leq R_k(\bs))}.
\end{align}

Different from the usual kNN regressor with the covariates $\bx_1, \dots, \bx_n$, which selects neighbors through Euclidean distance in the ambient space, the S-kNN regressor chooses neighbors with skeleton-based distances after projection onto the skeleton graph.
Measuring proximity with the skeleton can improve the regression performance when the dimension of the covariates is large, which we empirically show in Section \ref{sec::simulation}.



%let $\bs_{i(1)}, \dots, \bs_{i(k)}$ be the $k$ nearest projected covariates in terms of the skeleton-based distance with $\calI \equiv \set{i(1), \dots, i(k)} \subset \set{1, \dots, n}$ the corresponding indices, such that
%$d_\calS(\bs_\ell, \bs ) \leq d_\calS(\bs_j, \bs )$ for any $\ell \in \calI $ and $j \in \set{1, \dots, n}\backslash\calI$.
%Then we have the skeleton-based $k$-nearest neighbors regressor (S-kNN) to be
%\begin{align}
%    \hat{m}_{SkNN}(\bs) = \frac{\sum_{\ell=1}^k Y_{i(\ell)}}{k},
%\end{align}


\begin{remark}
It is well known that the usual $k_n$NN regressor can be consistent if we let $k_n$ grow as a function of the sample size $n$, and under appropriate assumptions, \cite{DistributionFreeThoery} give the convergence rate of the $k_n$NN estimate $m_n$ to the true function $m$ as 
\begin{align*}
\mathbb{E}\left\|m_n-m\right\|^2 \leq \frac{\sigma^2}{k_n}+c_1 \cdot C^2\left(\frac{k_n}{n}\right)^{2 /d}
\end{align*}
Later, \cite{Kpotufe2011} has shown that the convergence rate of $k$NN regressor depends on the intrinsic dimension. 
We  expect a similar result with $d=1$ rate for the skeleton $k$NN
regression at an edge point.
%The previous analysis on $k$NN regressor depends on some expansion rates of the local neighborhood, but, in the case of the skeleton, two different systems of measures are imposed on edges and knots and the local regions can be complicated if both knots and edges are involved.
\end{remark}

\subsection{Linear Spline Regression on Skeleton}

\label{sec::lspline}

% {\color{magenta}I rephrase this part a bit to make it clearer.}

% While the smoothing spline does not work on the skeleton,
% the linear spline method turns out to be applicable
% to the skeleton. 

~~~~In this section, we propose a skeleton-based linear spline model (S-Lspline) for regression estimation.
By construction, this approach results in a continuous model across the graph.
Moreover, we show that the skeleton-based linear spline corresponds to an elegant parametric regression model on the skeleton.
% We construct a linear model on each edge of the graph while ensuring that the predicted values agree on shared vertices, resulting in a continuous model across the graph. 
As the skeleton $\calS$
can be decomposed into the edge component $\mathcal{E}$
and the knot component $\mathcal{V}$, the linear spline regression on the skeleton can be written as the following constrained model:

\begin{equation}
\begin{aligned}
f: \calS\,\,\rightarrow \,\,\mathbb{R} \ \
\mbox{  such that }&\mbox{1. $f(x)$ is linear on $x\in\mathcal{E}$,}\\
&\mbox{2. $f(x)$ is continuous at $x\in \mathcal{V}$.}
\end{aligned}
\label{eq::LS_original}
\end{equation}
While solving the above constrained problem may not be easy,
we have the following elegant representer theorem
showing that a linear spline on the skeleton can be uniquely characterized by the values on each knot. 

\begin{theorem}[Linear spline representer theorem]
\label{thm::spline}
Any function satisfying equation \eqref{eq::LS_original}
can be characterized by $\{f(v): v\in\mathcal{V}\}$
and for $x\in\mathcal{E}$, $f(x)$ is linear interpolation
between the values on the two knots on the edge that $x$ belongs to.
\end{theorem}

\begin{proof}
Let $f$ be a function satisfying equation \eqref{eq::LS_original}. 
By construct, $f$ is linear for $x\in\calE$ and is continuous at
$x\in \calV$.
Let $V_j$ and $V_\ell$ be two knots that share an edge
and let $E_{j\ell} =\{x = t V_j + (1-t) V_\ell: t\in(0,1)\}$
be the shared edge segment. 
For any $x\in\calE$, there exists a pair $(V_j,V_\ell)$
such that $x\in E_{j\ell}$.
Because $f$ is linear in $E_{j\ell}$, 
$f$ can bee uniquely characterized by the pairs $(f(e_1), e_1), (f(e_2), e_2)$ for two distinct points $e_1,e_2 \in \bar E_{j\ell}$,
where $\bar E_{j\ell} =\{x = t V_j + (1-t) V_\ell: t\in[0,1]\}$
is the closure of $E_{j\ell}.$
Thus, we can pick $e_1 = V_j$ and $e_2 =V_\ell$, which implies
that $f$ on the segment $E_{j\ell}$ is parameterized 
by $f(v_j)$ and $f(V_\ell)$, the values on the two knots. 

By applying this procedure to every edge segment,
we conclude that 
any function satisfying the first condition in \eqref{eq::LS_original}
can be characterized by the values on the knots. 
The second condition in \eqref{eq::LS_original} will require
that every knot has one consistent value. 
As a result, any function $f$ satisfying \eqref{eq::LS_original}
can be uniquely characterized by the values on the knot $\{f(x): x\in \calV\}$
and $f(x)$ will be a linear interpolation when $x\in \calE$. 

\end{proof}

%Notably, a linear function on can be uniquely parameterized by
%the value of two distinct points. 
%With this insight, any linear spline model on a skeleton
%is uniquely determined by the values on each knot.
%Thus, we only need to estimate the predicted values on each knot.

%In more details, consider a linear function $f(t) = \alpha + \beta t$.
%This linear function is parameterized by $(\alpha, \beta)$.  
%However, if we pick any two points, say $t=0,1$, and determine
%the values $f(0)$, $f(1)$, the linear function can be parameterized as 
%$f(t) = f(0) + t(f(1)-f(0))$
%and $\alpha = f(0), \beta = f(1)-f(0)$. 

%which is equivalent to $f(t) = f(0) + (f(1) - f(0))t$ parametrized by $f(0) = \alpha$ and $f(1) = \alpha + \beta$. Also, fitting exactly one value to each knot guarantees the continuity of the model at the knots as required by the linear spline model. As a result, the linear spline model can be parameterized by the values on each knot.
Using Theorem~\ref{thm::spline},
we only need to determine the values on the knots. 
Let $\bbeta \in \mathbb{R}^k$
be the values of skeleton linear spline model on each knot
with $k = \abs{\calV}$ being the number of knots.
As is argued previously, the spline model is parameterized by $\bbeta$,
so we only need to estimate $\bbeta$ from the data. 
Given $\bbeta$, the predicted value of each $\by_i$ is a linear interpolation
depending on the projected location of each $\bx_i$.

To derive an analytic form of $\by_i$,
we introduce a transformed covariate matrix 
$\bZ = (\bz_1, \dots, \bz_n)^T \in \bR^{n\times k}$ as follows:
%With the values-on-knots parametrization, we can fit the S-Lspline model through ordinary least squares with a graph-transformed $n \times v$ covariate matrix $\bZ = (\bz_1, \dots, \bz_n)^T$  where $v = \abs{\calV}$ is the number of knots and $\bz_j$ is the length $v$ transformed data vector for $\bx_j \in \calX$. 
%The covariates are transformed in the following way:
\begin{enumerate}
    \item If $\bx_i$ is projected onto a vertex that $\bs_i = V_j$ for some $j$, then
    \begin{align}
        \bz_{ij'}=I(j'=j).
    \end{align}
    %the transformed data $\bz_{ij}=1$ and $\bz_{ij'} = 0$ for $j'\neq j$. 
    \item If $\bx_i$ is projected onto an edge between knots $V_j$ and $V_\ell$, then 
    \begin{align}
    \bz_{ij} = \frac{||\bs_i - V_j||}{||V_j - V_\ell||},
    \quad \bz_{i\ell} = \frac{||\bs_i - V_\ell||}{||V_j - V_\ell||}, \quad \text{ and } \bz_{ij'}=0 \text{ for } j'\neq j,\ell.
    \end{align}
    %$\bz_i$ has value $\frac{||\bs_i - V_j||}{||V_j - V_\ell||}$ on the $\ell$-th entry, value $\frac{||\bs_i - V_\ell||}{||V_j - V_\ell||}$ on the $j$-th entry, and $0$ on all other entries. 
    
\end{enumerate}
With the above feature transform, the predicted value of $\by_i$ by the S-Lspline model is
\begin{align}
    \hat \by_i = \bbeta^T \bz_i.
\end{align}

To see this,
if $\bx_i$ is projected onto a vertex that $\bs_i = V_j$ for some $j$, the linear model with transformed covariates gives $\bbeta^T\bz_i = \bbeta_j$, the predicted value on vertex $V_j$.
In the case where $\bx_i$ is projected onto an edge between knots $V_j$ and $V_\ell$,
let $\bbeta_j$ and $\bbeta_\ell$ be the corresponding predicted values at $V_j$ and $V_\ell$, and the linear interpolation between $\bbeta_\ell$ and $\bbeta_j$ at $\bs_i$ can be written as
\begin{align}
    \bbeta_j + \frac{||\bs_i - V_j||}{||V_j - V_\ell||} \cdot \left(\bbeta_\ell - \bbeta_j\right) = \frac{||\bs_i - V_\ell||}{||V_j - V_\ell||} \cdot \bbeta_j + \frac{||\bs_i - V_j||}{||V_j - V_\ell||} \cdot \bbeta_\ell = \bbeta^T\bz_i.
\end{align}
To estimate $\bbeta$, we can apply the least squared procedure to get:
\begin{align}
\hat \bbeta &= {\sf argmin}_{\bbeta} \sum_{i=1}^n (\by_i-\hat \by_i)^2\\
& = {\sf argmin}_{\bbeta} \sum_{i=1}^n ( \by_i- \bbeta^T \bz_i)^2.
\end{align}
So it becomes a linear regression model
and the solution can be elegantly written as
\begin{align}
    \hat{\bbeta} = (\bZ^T \bZ)^{-1} \bZ \by.
\end{align}
Note that in a sense, the above procedure can be viewed
as placing a linear model 
\begin{align}
    \mathbb{E}(\by|\bX) = \bbeta^T \bZ,
\end{align}
where $\bZ$ is a transformed covariate matrix from $\bX$.
%With the above procedure,we transform $\bX$ into $\bZ$. 
%Let $\hat{\by}$ be the length $v$ vector of predicted values on all the knots.
%If $\bx_i$ is projected onto a vertex that $\bs_i = V_j$ for some $j$, the linear model with transformed covariates gives $\bz_i^T \hat{\by} = \hat{y}_j$, the predicted value on vertex $V_j$.
%If $\bx_i$ is projected onto an edge between knots $V_j$ and $V_\ell$,
%let $\hat{y}_j$ and $\hat{y}_\ell$ be the corresponding predicted values at $V_j$ and $V_\ell$, and the linear interpolation between $\hat{y}_\ell$ and $\hat{y}_j$ at $\bs_i$ can be written as
%\begin{align}
%    \hat{y}_j + \frac{||\bs_i - V_j||}{||V_j - V_\ell||} \cdot \left(\hat{y}_\ell - \hat{y}_j\right) = \frac{||\bs_i - V_\ell||}{||V_j - V_\ell||} \cdot \hat{y}_j + \frac{||\bs_i - V_j||}{||V_j - V_\ell||} \cdot \hat{y}_\ell = \bz_i^T \hat{\by}
%\end{align}
%Consequently, the S-Lspline model in matrix form can be written as 
%\begin{align}
%    \mathbb{E}(\by|\bZ) = \bbeta^T \bZ
%\end{align}
% {\color{magenta}YC: The formal regression model is the conditional mean; avoid writing something without conditional expectation.}
%for $\bbeta$ the $v\times 1$ column vector of coefficients with each coefficient $\beta_j = \hat{y}_j$ representing the predicted value on the corresponding knot. 
%To estimate the parameter $\bbeta$, we use the least square method, which leads to
%\begin{align}
%    \hat{\bbeta} = (\bZ^T \bZ)^{-1} \bZ \by
%\end{align}
Note that the S-Lspline model with the graph-transformed covariates does not include an intercept.
%New prediction can be carried out the same way by first transforming the new covariates and then use the estimated coefficients for predicted values.

\begin{remark}
%The parametrization can be testified by the sufficient condition on the degrees of freedom. 
An alternative justification of the value-on-knots parameterization is to calculate the degree of freedom. On each graph, the sum of the vertex degrees is twice the number of edges since each edge is counted from both ends. Let $e$ be the number of edges in the graph, let $v$ be the number of vertices, and let $r$ be the sum of all the vertex degrees, we have $r = 2e$. For the S-Lspline model, we construct a linear model with $2$ free parameters for each edge, and thus without any constraints, the total number of degrees of freedom is $2e$. For each vertex $V_i$ with degree $r_i$, the continuity constraint imposes $r_i - 1$ equations, and as a result, the continuity constraints consume a total of $\sum_{i=1}^v r_i-1 = r - v$ degrees of freedom. Combining it, we have $2e - (r -v) = v$ degrees of freedom, which matches the degrees of freedom given by the parametrization of values on the knots.
\end{remark}

% \begin{remark}
% A natural idea is to extend the linear spline model to a higher-order spline on the skeleton. However, higher-order spline models may encounter various issues. First, to guarantee the required degree of smoothness on the skeleton graph, we may need polynomials with higher degrees than those used in Euclidean cases. Second, the odd-degree derivatives are directional and thus depend on the direction of edges, which leads to different models on a graph when there are different edge directions. Discussions on higher-order splines on graphs are beyond the scope of this paper and will be considered as future work.
% \end{remark}



\subsection{Challenges of Other Nonparametric Regression}
\label{sec::otherParametric}
~~~~In this section, we discuss the challenges when applying other nonparametric regression methods to the skeleton. 
Particularly, the skeleton graph is only equipped with a metric and 
does not have a well-defined inner product or orientation,
which makes many conventional approaches not directly applicable.
%lacks many nice structures compared to Euclidean space or Hilbert space, which requires additional considerations when adopting some nonparametric methods.



\subsubsection{Local polynomial regression}

% {\color{magenta}Can we do local polynomial regression? 
% I don't think we can do it. And here we need to explain why we
% cannot do it. }

%The kernel estimator described in Section \ref{sec::skelkernel} can be understood as introducing kernel weights 
%for localization in an intercept-only linear model.
~~~~Local polynomial regression \citep{fan2018local}
is a common generalization of the kernel regression that
tries to improve the kernel regression estimator by using higher-order polynomials as local approximations to the regression function. 
In the Euclidean space, a $p$-th order local polynomial regression aims to choose $\beta(\bx)$ via minimizing
\begin{align}
    \sum_{i=1}^n \sbrac{Y_i - \sum_{j = 0}^p \beta_j (\bx_i - \bx)^j}^2 K \brac{\frac{\bx_i - \bx}{h}}
\end{align}
and predict $m(\bx)$ via $\hat \beta(\bx)$, the first element in the minimizer.
Note that when $p=1$, one can show that this is equivalent to the kernel regression.

Unfortunately, the local polynomial regression cannot be easily adapted to the skeleton because the polynomial $(\bx_i - \bx)^j$ requires a well-defined orientation, which is ill-defined at a knot (vertex). 
%To adopt this approach to the skeleton graph, we need to first define $(\bs_i -\bs)$ for $\bs_i, \bs \in \calS$ and the corresponding polynomials.
%However, as we only have metric on $\calS$, the difference cannot be naturally defined as in vector spaces. 
Directly replacing $(\bs_i -\bs)$ with the distance $d_\calS (\bs_i -\bs)$ will make all the polynomials to be non-negative, which will be problematic for odd orders. 
Unless in some special skeletons such as a single chain structure,
the local polynomial regression cannot be directly applied.

%Given the kernel function is constrained on an edge with a clearly-defined direction, the local polynomial model can be constructed, but additional considerations are needed around the knots.

\subsubsection{Higher-Order Spline}
%~~~~With the skeleton-based linear spline model described in Section 

~~~~In Section \ref{sec::lspline}, we introduce the linear spline model.
One may be curious about the possibility of using a higher-order spline (enforcing higher-order smoothness on knots; see, e.g., Chapter 5.5 of \cite{wasserman2006all}).
%it is a natural thought to generalize the model with higher-order splines that ensure higher-order smoothness of the model.
Unfortunately, the higher-order spline is generally not applicable to skeleton
because a higher-order spline requires derivatives and the concept of derivative
may be ill-defined on a knot because of the lack of orientation.
To see this, consider a knot with three edges connecting to it. 
There is no simple definition of derivative at this knot unless we specify
the orientation of these three edges.
%needs a more careful treatment on the skeleton graph. 

One possible remedy is to introduce an orientation for every edge.
This could be done by ordering the knots first and for every edge,
the orientation is always from a lower index vertex to the higher index vertex. 
With this orientation, it is possible to create a higher-order spline
on the skeleton
but the result will depend on the orientation we choose.

%First, for a knot with a degree larger than or equal to $3$, we need a quantity like the total derivative to ensure a smooth change from the knot to all of its connected edges. 
%Moreover, odd-degree derivatives are directional but many graphs, including the skeleton graph, do not have built-in directions. 
%Empirically, different edge directions do lead to different spline models on the graph and do give different empirical performances.

Even with edge directions provided and the derivatives on the skeleton defined, higher-order spline on the skeleton can be prone to overfitting.
Classical spline methods use degree $p+1$ polynomial functions to achieve continuity at $p$-th order derivative. For example, univariate cubic splines use polynomials up to degree $3$ to ensure the second-order smoothness of the regression function at each knot. 
However, on a graph, degree $p+1$ polynomial functions may fail to achieve continuity at $p$-th order derivative, and on complete graphs, which is the worst case, $2p+1$ degree polynomials are needed instead.


\subsubsection{Smoothing Spline}
% {\color{magenta}Can we do smoothing spline regression? 
% I don't think we can do it. And here we need to explain why we
% cannot do it. }
~~~~Smoothing spline \citep{wang2011smoothing, wahba1975smoothing} is  another popular approach
for curve-fitting that attempts to 
find a smooth curve that minimizes the square loss in the prediction
with a penalty on the curvature (second or higher-order derivatives).

The major difficulty of this method is that the concept of a \emph{smooth} function
is ill-defined at a knot even if we have a well-defined orientation.
In fact, the `linear function' is not well-defined in general on a skeleton's knot.
To see this, consider a knot $V_0$ with three edges $e_1,e_2,e_3$ connecting to $V_1,V_2,V_3$, respectively.
%Suppose the orientation is $V_1\rightarrow V_0, V_0\rightarrow V_2, V_0\rightarrow V_3$.
Suppose we have a linear function $f_0$ and
$f_0$ is linearly increasing on paths $V_1-V_0-V_2$
and $V_1-V_0- V_3$. 
However, on the path $V_2-V_0-V_3$, the function $f_0$ will be 
decreasing ($V_2-V_0$) and then increasing ($V_0-V3$), leading to a non-smooth structure.


%Unless we introduce an orientation, we cannot 



%Smoothing spline balances the goodness of fit and a derivative-based measure of smoothness of the estimator, and hence faces similar challenges as the higher-order spline when adopting to the skeleton graph.  
%Particularly, the well-defined  directions of the edges along with careful considerations around the knots are needed to make sense of the derivatives on the skeleton.

\subsubsection{Orthonormal Basis and Tree}

~~~~Orthnormal basis approach (see, e.g., Chapter 8 of \cite{wasserman2006all}) uses a set of orthonormal basis functions to approximate
the  regression function. 
In general, it is unclear how to find a good orthonormal basis 
for a skeleton unless the skeleton is simply a circle or a chain. 

Having said that, it is possible to construct an orthonormal basis
borrowing the idea from wavelets \citep{torrence1998practical}.
The key idea is that the skeleton is a measurable set
that we can measure its (one-dimensional) volume. 
Thus, we can partition the skeleton $\calS$ into two equal-volume sets $A_1,A_2$. Note that the resulting sets $A_1,A_2$ are not necessarily skeletons because we may cut an edge into two pieces. 
For each set $A_j$, we can further partition it again into equal volume sets $A_{j,1}, A_{j,2}$. 
And we can repeat this dyadic procedure to create many equal-volume subsets.
We then define a basis as follows:
\begin{align*}
f_0(s) &= 1,\\
f_1(s) & =  I\brac{s\in A_1} -  I\brac{s\in A_2}\\
f_2(s) & =  I(s\in A_{1,1}) -  I(s\in A_{1,2})\\
f_3(s) & =  I(s\in A_{2,1}) -  I(s\in A_{2,2})\\
\vdots
\end{align*}
After normalization, this set of functions forms
an orthonormal basis.
With this basis, it is possible to fit an orthonormal basis
on the skeleton. 

However, the above construction creates the partition arbitrarily. 
The fitting result depends 
on the particular partition we use to generate the basis
and it is unclear how to pick a reasonable partition in practice. 


The regression tree \citep{breiman2017classification, loh2014fifty} is a popular idea in nonparametric regression that fits the data via creating a tree of partitioning the whole sample space 
whose leaves represent a subset of the sample space
and predicts the response using a single parameter at each leave (region). 
This idea could be applied to the skeleton using a similar
procedure as the construction of an orthonormal basis
that we keep splitting a region into two subsets (but
we do not require the two subsets to be of equal size). 
However, unlike the usual regression tree (in Euclidean space) that
the split of two regions is often at a threshold at one coordinate,
the split of a skeleton may not be easily represented
as the skeleton  is just a connected subregion of Euclidean space.
Therefore, similar to the orthonormal basis, 
regression tree may be used in skeleton regression, but
there is no simple and principled way to create
a good partition.




%The orthonormal basis approach approximates the regression function with a set of orthonormal functions. 
%However, the usual orthonormal basis in Euclidean space cannot be directly adopted to the skeleton. 
%It can be possible to define an orthonormal basis on the skeleton with wavelet constructs, but this is beyond the scope of this work.


\section{Simulations}	\label{sec::simulation}
~~~~In this section, we use simulated data to evaluate the performance of the proposed skeleton regression framework. We first demonstrate an example with the intrinsic domain composed of several disconnected components, which we call the Yinyang data (Section \ref{sec::Yinyang}). Then, we add noisy observations to the Yinyang data (Section \ref{sec::NoisyYinyang}) to show the effectiveness of our method in handling noises. Moreover, we present an example where the domain is a continuous manifold with a Swiss roll shape (Section \ref{sec::SwissRoll}). In all the simulations in this section, there are random perturbations in the intrinsic dimensions, and we add random Gaussian variables as covariates to increase the ambient dimension.

\subsection{Yinyang Data}
\label{sec::Yinyang}

~~~~The covariate space of Yinyang data is intrinsically composed of $5$ disjoint structures of different geometric shapes and different sizes: a large ring of $2000$ points, two clumps each with $400$ points (generated with the \texttt{shapes.two.moon} function with default parameters in the \texttt{clusterSim} library in R \citep{clusterSim}), and two 2-dimensional Gaussian clusters each with $200$ points (Figure \ref{fig::YinyangData} left). Together there are a total of $3200$ observations. 
Note that the intrinsic structures of the components are curves and points, and, with perturbations, the generated covariates do not lay exactly on the corresponding manifold structures.
The responses are generated from a trigonometric function on the ring and constant functions on the other structures with random Gaussian error(Figure \ref{fig::YinyangData} right). That is, let $\epsilon \sim N(0,0.01)$ and let $\theta$ be the angle of the covariates, then
\begin{align}
\label{eq::YinyangResponse}
    Y = \epsilon +\begin{cases}
  \sin(\theta*4) + 1.5& \text{for points on the outer ring} \\
  0& \text{for points on the bottom-right Gaussian cluster}\\
  1& \text{for points on the right clump}\\
  2& \text{for points on the left clump}\\
  3& \text{for points on the upper-left Gaussian cluster}
    \end{cases}
\end{align}
To make the task more challenging with the presence of noisy variables, we add independent and identically distributed random $N(0,0.01)$ variables to the generated covariates. In this section, we increase the dimension of the covariates to a total of $1000$ with those added Gaussian variables.
% Empirically we find the advantage of the proposed skeleton regression methods is more profound when there are more noisy dimensions, and we include such simulation results in Appendix \ref{sec::extraSim}.

% \YC{please write down how $Y$ is generated!}


We randomly generate the dataset for $100$ times, and on each dataset we use $5$-fold cross-validation to calculate the sum of squared errors (SSE) as the performance assessment. For each fold, there are $2560$ training samples. 
We use the skeleton construction method described in Section \ref{sec::skeletoncons} to construct skeletons with varying numbers of knots on each training set. The construction procedure also cuts each skeleton into $5$ disjoint components according to the Voronoi Density weights (Section \ref{sec::skeletoncons}).
We also empirically tested using different cuts to get skeleton structures with different numbers of disjoint components under the same number of knots and noticed little change in the squared error performance (see Appendix \ref{sec::extraSim}). 

% \YC{how do  you determine the edge directions in the graph? randomly generated the index and pointing from low-index to high-index?} 
% \JW{Yes}

We evaluate the skeleton-based nonparametric regressors introduced in Section \ref{sec::regression}: skeleton kernel regression (S-kernel), $k$-NN regressor using skeleton-based distance (S-kNN), and the skeleton spline model(S-Lspline).
% and Appendix \ref{sec::highsplineparam}, where for edge directions of higher-order derivatives we randomly index the knots and let edges pointing from low-index knots to high-index knots. 
For comparisons, we apply the classical k-Nearest-Neighbors regression based on Euclidean distances (kNN).
% with varying numbers of neighbors. 
For penalization regression methods, we test Lasso and Ridge regression.
% with different penalization coefficients are tested. 
Among the recent manifold and local regression methods, we include the Spectral Series approach with the radial kernel (SpecSeries) for its superior performance and readily available R implementation \footnote{\url{https://projecteuclid.org/journals/supplementalcontent/10.1214/16-EJS1112/supzip_1.zip}}. 
% The number of basis functions in SpecSeries is auto-tuned with the validation data of each fold, which can be advantageous, and we test different bandwidth of the kernel used. 
We take the median, 5th percentile, and 95th percentile of the 5-fold cross-validation Sum of Squared Errors (SSEs) for each parameter setting of each method on the 100 datasets. We present the smallest median SSE for each method in Table \ref{table:Yinyangd1000} along with the corresponding best parameter setting.


\begin{figure}
% \captionsetup{skip=1pt}
\centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Yinyang_data2D.jpeg} 
    \end{subfigure}
        \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Yinyang_data3D.jpeg}
    \end{subfigure}
    \vspace{-1em}
\captionof{figure}{Yinyang Regression Data}
\label{fig::YinyangData}
\vspace{2em}

%%%%%%%%%%%%%%%%%%%%%%
\begin{tabular}{||c l c l||} 
 \hline
 Method & Medium SSE ($5$\%, $95$\%) & nknots & Parameter \\ [0.5ex] 
 \hline\hline
 kNN &204.5 (192.3, 221.9) & - & neighbor=18 \\ 
 Ridge & 2127.0 (2100.2, 2155.2) & & $\lambda  = 7.94$\\
 Lasso & 1556.8 (1515.4, 1607.9) & & $\lambda = 0.0126$\\
 SpecSeries & 1506.4 (1469.1,1555.6) & - &bandwidth = 2\\
 S-Kernel & 112.8 (102.0, 121.7) & 38 & bandwidth = 6 $r_{hns}$ \\
 S-kNN & 139.6 (129.6,148.7) & 38 & neighbor = 36 \\
 S-Lspline & 95.8 (88.6, 102.6) & 38 &-  \\[1ex] 
 \hline
\end{tabular}
\label{table:Yinyangd1000}
\captionof{table}{Regression results on Yinyang $d=1000$ data. The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$th percentile and $95$th percentile of the SSEs from the given parameter settings are reported in brackets.}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%

\centering
\includegraphics[width=\textwidth]{figures/Yinyang1000_knots.jpeg}
\captionof{figure}{Yinyang $d = 1000$ data regression results with varying number of knots. The medium SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::Yinyangd1000Numknots}

\end{figure}



We observed that all the skeleton-based methods (S-Kernel, S-kNN, and S-Lspline) performed better than the standard kNN in this setting. The SpecSeries approach performed worse than the classical kNN, and only slightly better than the Lasso regression. Ridge and Lasso regression, despite the regularization effect, resulted in relatively high SSEs. Therefore, the skeleton regression framework is beneficial in dealing with covariates that lie around manifold structures.
% The advantage of skeleton-based methods is manifested more if the number of noisy variables in the input vector gets larger (see Appendix \ref{sec::Yinyang1000}).

In Figure \ref{fig::Yinyangd1000Numknots}, we present the median SSE of the S-Lspline, S-Kernel, and S-kNN methods on skeletons with various numbers of knots. The vertical dashed line indicates $[\sqrt n] = 51$ knots as suggested by the empirical rule, where $n$ is the training sample size.
The empirical rule seems to produce satisfactory results in this simulation study, roughly identifying the "elbow" position, but it's advised to use cross-validation for fine-tuning in practice.



% The fitted values given by S-Lspline, S-Kernel, and S-kNN methods are illustrated in Figure \ref{fig::Yinyangd100fit}. For each method we illustrate the fitted values from the best SSE model. We see that all the methods fit well.

% \begin{figure}[ht]
% \captionsetup{skip=1pt}
% \centering
%     \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Yinyangd100_fitLspline.jpeg} 
%         \caption{Linear Spline}
%     \end{subfigure}
%         \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Yinyangd100_fitskelkernel.jpeg}
%         \caption{Skeleton Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Yinyangd100_fitskelknn.jpeg}
%         \caption{Skeleton KNN}
%     \end{subfigure}
% \caption{Yinyang Regression fitted points $d = 100$ with varying number of knots results}
% \label{fig::Yinyangd100fit}
% \end{figure}

%------------------------------------------------------------------------------------------------------------------------


\subsection{Noisy Yinyang Data}
\label{sec::NoisyYinyang}
~~~~To show the robustness of the proposed skeleton-based regression methods, we add $800$ noisy observations to the Yinyang data in Section \ref{sec::Yinyang} ($20\%$ of a total of $4000$ observations). 
The first two dimensions of the noisy covariates are uniformly sampled from the $2$-dimensional square $[-3.5,3.5]\times [-3.5,3.5]$ and independent random normal $N(0,0.01)$ variables are added to make the covariates $1000$-dimensional in total. 
The responses of the noisy points are set as $1.5 + \epsilon$ with $\epsilon \sim N(0,0.01)$, while the responses on the Yinyang covariates are generated the same as in Equation \ref{eq::YinyangResponse}. 
The first two dimensions of the Noisy Yinyang covariates are plotted in Figure \ref{fig::NoiseYinyangData} left and the $Y$ values against the first two dimensions of the covariates are illustrated in Figure \ref{fig::NoiseYinyangData} right.

\begin{figure}
% \captionsetup{skip=1pt}
\centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/NoiseYinyang_data2D.jpeg} 
    \end{subfigure}
        \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/NoiseYinyang_data3D.jpeg}
    \end{subfigure}
\vspace{-1em}
\captionof{figure}{Noisy Yinyang Regression Data}
\label{fig::NoiseYinyangData}
\vspace{2em}
%%%%%%%%%%%%%%%%%%
\centering
\begin{tabular}{||c l c l||} 
 \hline
 Method & Medium SSE ($5$\%, $95$\%)  & Number of knots & Parameter \\ [0.5ex] 
 \hline\hline
 kNN & 440.8 (420.4, 463.0) & -  & neighbor=18 \\ 
 Ridge & 2139.1 (2102.6, 2171.1) &- & $\lambda  = 6.31$\\
 Lasso &  2029.2 (1988.7, 2071.0) &- & $\lambda = 0.02$\\
 SpecSeries & 1532.0 (1490.7, 1563.2) & - & bandwidth = $2$\\
 S-Kernel & 385.7 (365.2, 406.0) & 57 & bandwidth = 6 $r_{hns}$ \\
 S-kNN & 417.6 (396.1, 440.6) & 71 & neighbor = 36 \\
 S-Lspline & 377.7 (358.1, 398.9) & 71 &  - \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on Noisy Yinyang $d=1000$ data.The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$ percentile and $95$ percentile of the SSEs from the given parameter settings are reported in bracket.}
\label{table:NoiseYinyangd1000}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
\includegraphics[width=\textwidth]{figures/NoiseYinyang1000_knots.jpeg}
\captionof{figure}{Noisy Yinyang $d = 1000$ data regression results with varying number of knots. The medium SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::NoiseYinyangd1000Numknots}

\end{figure}

% \YC{what do you mean by "do not cut the constructed skeleton"?}
% \JW{The number of disconnected components in the skeleton is 1.}

To evaluate the robustness of the proposed skeleton-based regression methods, we randomly generated the Noisy Yinyang data 100 times, and followed the same analysis procedure as in Section \ref{sec::Yinyang}, except that we left the skeleton that we fit our regression estimators on to be a fully connected graph. We also took the median, 5th percentile, and 95th percentile of the 5-fold cross-validation SSEs for each parameter setting of each method on the 100 datasets. The smallest median SSE for each method is reported in Table \ref{table:NoiseYinyangd1000} along with the corresponding best parameter setting.
It can be seen that all the skeleton-based regression methods outperform the standard kNN and the SpecSeries approach.
The Ridge and Lasso regressions again fail to provide good performance on this simulated dataset.

In Figure \ref{fig::Yinyangd1000Numknots}, we plot the median SSE of the skeleton-based methods on skeletons with different numbers of knots. Using the empirical rule to construct a skeleton with $[\sqrt{3200}] = 57$ knots results in good regression performance and approximately identifies the "elbow" position in Figure \ref{fig::Yinyangd1000Numknots}. However, for some skeleton-based methods, using a number of knots larger than that given by the empirical rule leads to better regression performance.
This improvement is related to the phenomenon observed in \citet{skelclus} that when dealing with noisy observations, it's better to have a skeleton with more knots and cut the skeleton into more disjoint components in order to have a cleaner representation of the key manifold structures.
Therefore, when facing data with noisy feature vectors, it's advised to empirically tune the number of knots favoring larger values.


% We again illustrate the fitted values given by the best S-Lspline, S-Kernel, and S-kNN models in Figure \ref{fig::NoiseYinyangd100fit}. The fitted values all follow the true regression function well, but all show a tendency to be shifted toward the origin. This is caused by the added noisy observations which have regression values all around $0$.

% \begin{figure}[ht]
% \captionsetup{skip=1pt}
% \centering
%     \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/NoiseYinyangd100_fitLspline.jpeg} 
%         \caption{Linear Spline}
%     \end{subfigure}
%         \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/NoiseYinyangd100_fitskelkernel.jpeg}
%         \caption{Skeleton Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.27\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/NoiseYinyangd100_fitskelknn.jpeg}
%         \caption{Skeleton KNN}
%     \end{subfigure}
% \caption{Noisy Yinyang Regression fitted points $d = 100$ with varying number of knots results}
% \label{fig::NoiseYinyangd100fit}
% \end{figure}


%----------------------------------------------------------------------------------------

\subsection{SwissRoll Data}
\label{sec::SwissRoll}
~~~~The intrinsic components of the covariates in Yinyang data are all well-separated, which, admittedly, can give an advantage to skeleton-based methods. 
Moreover, the intrinsic dimensions of the structural components for Yinyang data covariates are all lower than or equal to $1$ and can be straightforwardly represented by knots and line segments, potentially giving another advantage to skeleton-based methods.
To address such concerns, we present another simulated data which has covariates lying around a Swill Roll shape (Figure \ref{fig::SwissRollData} left), an intrinsically $2$-dimensional manifold in the $3$-dimensional Euclidean space. 
To make the density on the Swill Roll manifold balanced, we sample points inversely proportional to the radius of the roll in the $X_1 X_3$ plane. 
Specifically, let $u_1, u_2$ be independent random variables from $\text{Uniform}(0,1)$ and let the angle in the $X_1 X_3$ plane be generated as $\theta_{13} = \pi 3^{u_1}$. 
Then for the first $3$ dimensions of the covariates we have
\begin{align}
  X_1 = \theta_{13} \cos(\theta_{13}), \ \ X_2 = 4 u_2, \ \ X_3 =  \theta_{13} \sin(\theta_{13}) 
\end{align}
The true response has a polynomial relationship with the angle on the manifold if the $X_2$ value of the point is within some range. 
Let $\Tilde{\theta}_{13} = \theta_{13} - 2 \pi$, and let $\epsilon \sim N(0, 0.3)$. Then we set
\begin{align}
    Y =  0.1\times \Tilde{\theta}_{13}^3 \times \left[ I(X_2<\pi) + I( 2\pi < X_2<3\pi) \right] + \epsilon
\end{align}
The response versus the angle $\theta_{13}$ and $X_2$ is demonstrated in Figure \ref{fig::SwissRollData} right. Independent random Gaussian variables from $N(0,0.1)$ are added to make the covariates $1000$-dimensional in total, and $2000$ observations are sampled to make the Swiss Roll dataset.


\begin{figure}

\centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/SwissRoll_X.jpeg} 
    \end{subfigure}
        \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/SwissRoll_Y.jpeg}
    \end{subfigure}
\captionof{figure}{SwissRoll Regression Data}
\label{fig::SwissRollData}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{2em}
\begin{tabular}{||c l c l||} 
 \hline
 Method & Medium SSE ($5$\%, $95$\%)  & nknots & Parameter \\ [0.5ex] 
 \hline\hline
 kNN & 648.5 (607.1, 696.0) & - & neighbor=12 \\ 
 Ridge &  1513.7 (1394.4, 1616.2) & -& $\lambda  = 2.0$\\
 Lasso &  1191.4 (1106.7, 1260.7) & -& $\lambda = 0.032$\\
 SpecSeries & 1166.5 (1081.4, 1238.8) &- & bandwidth = $2.0$\\
 S-Kernel & 588.7 (527.0, 653.7) & 70 & bandwidth = 4 $r_{hns}$ \\
 S-kNN & 614.7 (561.2, 692.6) & 70& neighbor = 27 \\
 S-Lspline & 578.6 (508.0, 629.6) & 60 & - \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on SwissRoll $d=1000$ data. The smallest medium 5-fold cross-validation SSE from each method is listed with the corresponding parameters used. The $5$ percentile and $95$ percentile of the SSEs from the given parameter settings are reported in brackets.}
\label{table:Swissd1000}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
\includegraphics[width=\textwidth]{figures/SwissRoll1000_knots.jpeg}
\captionof{figure}{SwissRoll $d = 1000$ data regression results with varying number of knots. The medium SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::SwissRolld1000}

\end{figure}

We randomly generated the data 100 times and used the same analysis procedures as in Section \ref{sec::Yinyang}. We took the median, 5th percentile, and 95th percentile of the 5-fold cross-validation SSEs across each parameter setting for each method on the 100 datasets, and reported the smallest median SSE for each method along with the corresponding best parameter setting in Table \ref{table:Swissd1000}.
All the proposed skeleton-based methods have better performance than the standard kNN regressor, while the S-Lspline method had the best performance in terms of SSE. The SpecSeries approach in this setting has performance similar to the Lasso regression and did not improve much on the regression results utilizing information about the underlying manifold structure, possibly due to the large number of noisy dimensions.
Therefore, the proposed skeleton regression framework can also be powerful for data on connected, multi-dimensional manifolds.


By plotting the median SSE under skeletons with a varying number of knots in Figure \ref{fig::SwissRolld1000}, we observed that the best performance for all the skeleton-based methods is achieved with the number of knots larger than $[\sqrt{1600}] = 40$ knots. Given the intrinsic structure of the Swiss Roll input space is a $2$D plane, having more knots on the plane can give a better representation of the data structure and, therefore, lead to better prediction accuracy. We conjecture that the optimal number of knots should depend on the intrinsic dimension of the covariates, and we plan to discuss this further in future work. However, it's recommended to use cross-validation to choose the number of knots in practice.


% \begin{figure}[ht]
% \captionsetup{skip=1pt}
% \centering
%     \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Swissd100_nknot70_lsplinefit.jpeg} 
%         \caption{Linear Spline}
%     \end{subfigure}
%         \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Swissd100_nknot160_bandrate6_skelkernelfit.jpeg}
%         \caption{Skeleton Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Swissd100_nknot160_k18_skelnnfit.jpeg}
%         \caption{Skeleton KNN}
%     \end{subfigure}
% \caption{SwissRoll Regression fitted points $d = 100$ with varying number of cuts, number of knots fixed to be $60$}
% \label{fig::SwissRolld100fit}
% \end{figure}


\section{Real Data}	\label{sec::real}

~~~~In this section, we present analysis results on two real datasets.
We first predict the rotation angles of an object in a sequence of images taken from different angles (Section \ref{sec::luckycat}). 
For the second example, we study the galaxy sample from the Sloan Digital Sky Survey (SDSS) to predict the spectroscopic redshift (Section \ref{sec::sdss}), a measure of distance from a galaxy to earth.

\subsection{Lucky Cat Data} \label{sec::luckycat}

~~~~This dataset consists of $72$ gray-scale images of size $128\times 128$ pixels taken from the COIL-20 processed dataset \citep{COIL20}. They are 2D projections of a 3D lucky cat obtained by rotating the object by $72$ equispaced angles on a single axis.
Several examples of the images are given in Figure \ref{fig::luckycat}. 
\begin{figure}
\centering
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj4__0.png} 
    \end{subfigure}
        \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj4__12.png}
    \end{subfigure}
            \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj4__24.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj4__36.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj4__48.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj4__60.png}
    \end{subfigure}
\captionof{figure}{A part of the lucky cat images from the COIL-20 processed dataset. Each image
is of size $128\time 128$ pixels.}
\label{fig::luckycat}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
\begin{tabular}{||c c c||} 
 \hline
 Method & SSE &  Parameter \\ [0.5ex] 
 \hline\hline
 kNN & 888.9 & neighbor=9 \\ 
 Ridge& -&-\\
 Lasso& -&-\\
 SpecSeries& -&-\\
 S-Kernel & 1205.9 &  bandwidth = 4$r_{hns}$ \\
 S-kNN& 2604.2 & neighbor = 6\\
 S-Lspline &  338.1  &- \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on LuckyCat data from COIL-20. The best SSE from each method is listed with the corresponding parameters used.}
\label{table:luckycat}

\end{figure}
The response in this dataset is the angle of rotation. However, this response has a circular nature where degree 0 is the same as degree 360. To avoid this issue, we removed the last 8 images from the sequence, only using the first 64 images. As a result, our dataset consists of 64 samples from a 1-dimensional manifold embedded in $\RR^{16384}$ along with scalar values representing the angle of rotation.

To assess the performance of each method, we use leave-one-out cross-validation.
%In each iteration, one image is taken out of the dataset, and skeleton regression methods and other regression methods for comparison are fitted to the remaining images to estimate the angle of the left-out image. 
Similarly to the simulation studies, we use the skeleton construction method with Voronoi weights in \citet{skelclus} to construct the skeleton on the training set. 
In practice, we found that a small number of knots can still lead to loops in the constructed skeleton structure, and, after some tuning, we fit $2[\sqrt{n}] = 16$ knots to each training set. Additionally, since the underlying manifold should be one connected structure, we do not cut the constructed skeleton structure in this experiment. 
Due to the high-dimensional nature of the data, Ridge regression, Lasso regressions, and the Spectral Series approach failed to run with the implementations in R. 
The best SSE from each method is listed in Table \ref{table:luckycat} along with the corresponding parameters.
We observed that the S-Lspline method gives outstanding performance on this real-world data, significantly outperforming the kNN regressor.
% We also note that the S-Cspline method gives an extraordinarily bad performance and indicates extra caution needed for higher-order splines.


%------------------------------------------------------
\subsection{SDSS Data} \label{sec::sdss}

~~~~In this section, we applied the skeleton regression to a galaxy sample of size $5000$, taken from a random subsample of the Sloan Digital Sky Survey (SDSS), data release 12 \citep{york2000sloan, alam2015eleventh}. The dataset consists of $5$ covariates measuring apparent magnitudes of galaxies from images taken using $5$ photometric filters. These covariates can be understood as the color of a galaxy and  are inexpensive to obtain. The response variable is the spectroscopic redshift, which is a very costly but accurate measurement of the distance
to the earth. %but it provides information on the actual distance from a galaxy to the earth.
It is known that the $5$ photometric color measurements are correlated with
the spectroscopic redshift. So the goal is to use the photometric
information to predict the redshift; this is known as 
the clustering redshift problem in Astronomy literature \citep{morrison2017wizz, rahman2015clustering}.

We construct the skeleton with the method in \citet{skelclus} and fit the S-Lspline model. We color the knots by their predicted redshift values and color the edges by the average predicted values of the two connected knots. The resulting skeleton graph is shown in the left panel of Figure \ref{fig::SDSScolor}. For comparison, we color the knots and edges using the true values in the right panel of Figure \ref{fig::SDSScolor}. 
The predictions given by S-Lspline are very close to the true values. 
For completeness, we also include results from 
other approaches in Table \ref{table:SDSS}. 
While skeleton approaches do not provide the best prediction accuracy,
the skeleton structure obtained in Figure~\ref{fig::SDSScolor}
shows a clear one-dimensional structure in the underlying covariate distribution.
This explains why kNN and SpecSeries methods work well in this data (
both methods can adapt to the underlying manifold data).
Thus, even if 
our method does not provide the best prediction accuracy,
the skeleton itself can be used as a tool to investigate the 
structure of the covariate distribution, which can be valuable for practitioners.


\begin{figure}
\centering
    \begin{subfigure}[t]{0.38\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/SDSS5000_Lspline.jpeg} 
        \caption{S-Lspline}
    \end{subfigure}
        \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/SDSS5000_True.jpeg}
        \caption{True Value}
    \end{subfigure}
\captionof{figure}{SDSS Skeleton Colored by values predicted by S-Lspline (left) and by true values (right).}
\label{fig::SDSScolor}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%
\centering
\begin{tabular}{||c c c||} 
 \hline
 Method & SSE &  Parameter \\ [0.5ex] 
 \hline\hline
 kNN & 67.8 & neighbor=12 \\ 
 Ridge &870.3 & $\lam = 0.001 $\\
 Lasso &882.7 & $\lam = 0.001 $\\
 SpecSeries& 66.6& bandwidth = 2\\
 S-Kernel & 90.6 &  bandwidth = $10 r_{hns}$ \\
 S-kNN& 95.8 & neighbor = 39\\
 S-Lspline &  89.6  &- \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on SDSS data. The best SSE from each method is listed with the corresponding parameters used.}
\label{table:SDSS}
\end{figure}

We perform the same analysis as in Section \ref{sec::simulation} by comparing the 5-fold cross-validation SSEs of different regression methods on this dataset. Despite the fact that our skeleton-based methods do not show superior performance on this particular dataset, the skeleton representation does reveal the  manifold structure in the covariate space and an approximate monotone trend in the response. 
The clean manifold structure of the data and the small number of covariates may explain the superior performance of kNN and SpecSeries in this case.


%e helpful for further scientific investigations.



%-------------------------------------------------------------------------
\section{Conclusion}	
\label{sec::conclusion}

~~~~In this work, we introduce the skeleton regression framework to handle regression problems with manifold-structured inputs. We generalize the nonparametric regression techniques such as kernel smoothing and splines onto graphs. 
Our methods provide accurate and reliable prediction performance and are capable of recovering the underlying manifold structure of the data.
Both theoretical and empirical analyses are provided to illustrate the effectiveness of the skeleton regression procedures.

In what follows, we describe some possible future directions:
\begin{itemize}

\item {\bf Generalizing skeleton graphs to simplicial complex.}\\
From a geometric perspective, the skeleton graph constructed in this work only focuses on $0$-simplices (points) and $1$-simplices (line segments). Additional geometric information can be encoded using higher-dimensional simplices. Recent research in deep learning has explored the use of simplicial complices for tasks such as clustering and segmentation \citep{GeometricDL, MPSN2021}. Higher-dimensional simplicies
offer a finer approximation to the covariate distribution
but have a higher computational cost and a more complex model.
Thus, it is unclear if using a higher-dimensional simplex 
will lead to a better prediction accuracy.
We will explore
the possibility of extending skeleton graphs to skeleton complex in the future. 
%in this work to include simplicial sets, which are natural higher-dimensional generalizations of directed graphs.

\item {\bf Nonparametric smoothers on graphs.}\\
The kernel regression and spline regression
are not the only possibilities to perform nonparametric smoothing 
on graphs.
%Previous works have applied nonparametric regression estimators to graphs. 
For example, \citet{Wang2016} generalized the concept of trend filtering \cite{Kim2009, Tibshirani2014} to graphs and compared it to Laplacian smoothing and Wavelet smoothing. In contrast to our work, these regression estimators for graphs are applied to data where both the inputs and responses are located on the vertices of a given graph. As a result, these graph smoothers, which include different regularizations, can only fit values on the vertices, and do not model the regression function on the edges (\citet{Wang2016} mentioned the possibility of linear interpolation with the trend filtering).

It is possible to generalize these methods to skeleton by constructing responses on the knots in the skeleton graph as the mean values of the corresponding Voronoi cell, and then graph smoothers can apply. Some interpolation methods can again be used to predict the responses on the edge, and this can lead to another skeleton-based regression estimator.

% Laplacian smoothing:
% \begin{align*}
%     \min_{\be \in \RR^n} ||y - \be||_2^2 + \lam \be^T L \be
% \end{align*}
% The usual spline regressions can all be framed as Laplacian smoothing problem.

% Wavelet smoothing:
% \begin{align*}
%     \min_{\theta \in \RR^n} ||y - W \theta||^2 + \lam ||\theta||_1
% \end{align*}
% where $W$ is wavelet basis over the graph

% The $k$the order graph trend filtering (GTF) estimate is given by
% \begin{align*}
%     \hat{\be} = \argmin_{\be \in \RR^n} \frac{1}{2} ||y - \be||_2^2 + \lam ||\Delta^{(k+1)} \be||_1
% \end{align*}
% where $\Delta^{(k+1)}$ is graph difference operator of order $k+1$. Defined that $\Delta^{(1)}$ is the oriented incidence matrix of the graph that 
% \begin{align*}
% \begin{matrix}
% \Delta^{(1)}_{\ell} = &
%         &(0,\ldots,0,&1,&0,\ldots,0,&-1,&0,\dots,0)\\
%         &&&\uparrow&     &\uparrow& \\
%         &&&i&            &j& \\
% \end{matrix}
% \end{align*}
% where the $\ell$-th edge in graph $G$, with orientation, goes from node $i$ to $j$. Then for higher order $k\geq 0$ we have
% \begin{align*}
%       \Delta^{(k+1)} =
%     \begin{cases}
%       (\Delta^{(1)})^T \Delta^{(k)} = L^{k+1/2} & \text{for odd $k$}\\
%       \Delta^{(1)} \Delta^{(k)} = D L^{k/2} & \text{for even $k$}
%     \end{cases}    
% \end{align*}
% where $D = \Delta^{(1)}$ and note that the graph Laplacian matrix $L = D^T D$

% In comparison, the $k$th order graph Laplacian smoothing has penalty term
% \begin{align*}
%     \lam \be^T L^{k+1} \be = \Vert \Delta^{(k+1)} \be \Vert_{2}^2
% \end{align*}
% and hence the difference is the choice of $\ell_1$ or $\ell_2$ norm. Trend filtering on graphs has a level of local adaptivity unmatched by the usual graph smoothers.


\item {\bf Time-varying covariates and responses.}\\
A possible avenue for future research is to extend the skeleton regression framework to handle time-varying covariates and responses. Specifically, covariates collected at different time could be used together to construct knots in a skeleton. The edges in the skeleton can change dynamically according to the covariate distribution at different times, providing insight into how the covariate distributions have evolved. Additionally, representing the regression function on the skeleton would make it simple to visualize how the function changes over time.

\item {\bf Streaming data and online skeleton update.}
As streaming data becomes increasingly common, a potential area of future research is to investigate methods for updating the skeleton structure and its regression function in a real-time or online fashion. Reconstructing the entire skeleton can be computationally costly, but local updates to edges and knots can be more efficient. We plan to explore ways to develop a simple yet reliable method for updating the skeleton in the future.

%tasks are in the online learning setting that data is fed in sequentially and the predictor needs to be updated at each step. As skeleton can be computational costly to construct (see Section \ref{sec::complexity}), future research can focus on efficient ways to update the skeleton in the online learning setting and hence update the skeleton-based regressors in a fast manner.

\end{itemize}



% Acknowledgements should go at the end, before appendices and references

\acks{YC is supported by NSF DMS-195278, 2112907, 2141808 and NIH U24-AG072122. JW is supported by NSF DMS - 2112907.}


%%%%% Reference-------------------------------------------------------
\newpage
\bibliography{skelreg.bib}



\newpage
\appendix
\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}
\renewcommand{\thesubsection}{\Alph{subsection}}

\subsection{Skeleton Construction with Voronoi Density}
\label{ref::skelconsVoron}
~~~~
In this section, we provide a more detailed description of the procedures for constructing the skeleton and computing the density-aided edge weight called the Voronoi density, following the work in \cite{skelclus}.

\subsubsection{Knots Construction}	
\label{sec::knots}
~~~~
The knots in the skeleton serve as reference points within the data, allowing us to focus our attention from the overall data to these specific locations of interest. 
We utilize the $k$-means algorithm with a relatively large value of a number of knots $k$ to create these knots in a data-driven way. 
The number of knots is a crucial parameter in this procedure as it governs the trade-off between the summarizing power of the representation and the preservation of information. 
Empirical evidence from \cite{skelclus} suggests that setting $k$ to around $\sqrt{n}$ can be a helpful reference rule, while the dimensionality of the data should be taken into consideration when choosing $k$.

In practice, since the $k$-means algorithm may not always find the global optimum, we repeat it $1,000$ times with random initial points and select the result corresponding to the optimal objective. 
We also advise pruning knots with only a small number of with-in-cluster observations. Additionally, it can be helpful to preprocess or denoise the data by removing observations in low-density areas to address issues that could arise for $k$-means clustering.


\subsubsection{Edges Construction}	\label{sec::edge}
~~~~
We denote the given knots as $c_1,\cdots, c_k$ and represent their collection as $\calC = {c_1,\cdots, c_k}$. An edge is added between two knots if they are neighbors, which is determined by whether their corresponding Voronoi cells share a common boundary. The Voronoi cell associated with a knot $c_j$ is defined as the set of points in $\calX$ whose distance to $c_j$ is the smallest among all knots. 
That is, 
\begin{align}
    \CC_j = \{x \in \calX: d(x, c_j) \leq d(x, c_\ell) \ \  \forall \ell \neq j\},
\end{align}
where $d(x,y)$ is the usual Euclidean distance.
We add an edge between knots $(c_i,c_j)$ if their Voronoi cells have a non-empty intersection. This graph is referred to as the Delaunay triangulation of $\calC$, denoted as $DT(\calC)$. 

Although the Delaunay triangulation graph is conceptually intuitive, the computational complexity of the exact Delaunay triangulation algorithm has an exponential dependence on the ambient dimension $d$, making it unfavorable for multivariate or high-dimensional data settings. To overcome this issue, we approximate the Delaunay triangulation with $\hat{DT}(\calC)$ by examining the 2-nearest knots of the sample data points. We query the two nearest knots for each data point and add an edge between $c_i, c_j$ if there is at least one data point whose two nearest neighbors are $c_i, c_j$. The computational complexity of this sample-based approximation depends linearly on the dimension $d$, making it suitable for high-dimensional settings.


\subsubsection{Voronoi Density}	\label{sec::VD}

~~~~The Voronoi density (VD) measures the similarity between a pair of knots $(c_j,c_\ell)$ based on the number of observations whose 2-nearest knots are $c_j$ and $c_\ell$. We first define the Voronoi density based on the underlying probability measure and then introduce its sample analog. Given a metric $d$ on $\RR^d$, the 2-Nearest-Neighbor (2-NN) region of a pair of knots $(c_j,c_\ell)$ is defined in Equation \ref{eq::2NNregion} as
\begin{align*}
    B_{j\ell} = \{ X_m, m=1,\dots,n :\norm{x- V_i} > \max\{ \norm{x-V_j}, \norm{x- V_\ell} \}, \forall i \neq j, \ell \}.
\end{align*}
Figure~\ref{fig::2nn} provides an illustration of an example 2-NN region of a pair of knots.
If two knots $c_j, c_\ell$ are in a connected high-density region, then we expect the 2-NN region of $c_j, c_\ell$ to have a high probability measure. Therefore, the probability $\PP(B_{j\ell}) = P(X_1 \in B_{j\ell})$ can measure the association between $c_j$ and $c_\ell$. Based on this insight, the Voronoi density measures the edge weight of $(c_j,c_\ell)$ as
\begin{align}
S_{j\ell}^{VD} = \frac{\PP(B_{j\ell})}{|c_j - c_\ell |}.
\end{align}
The Voronoi density adjusts for the fact that 2-NN regions have different sizes by dividing the probability of the in-between region by the mutual Euclidean distance. 

In practice, we estimate $S_{j\ell}^{VD}$ by a sample average. The numerator $\PP(B_{j\ell}) $ is estimated by $\hat{P}_n (B_{j\ell}) = \frac{1}{n}\sum_{i=1}^n I(X_i\in B_{j\ell})$, and the final estimator for the VD is:

\begin{align}
\hat{S}_{j\ell}^{VD} &= \frac{\hat{P}_n ({B}_{j\ell})}{|{c}_j - {c}_\ell|}.
\end{align}

Calculating the Voronoi density is fast. The numerator, which only depends on 2-nearest-neighbors calculation, can be computed efficiently by the k-d tree algorithm. For high-dimensional space, space partitioning search approaches like the k-d tree can be inefficient, but a direct linear search still gives a short run-time.


\subsubsection{Graph Segmentation}	\label{sec::segmenting}


~~~~After obtaining the weighted skeleton graph, it can be helpful to prune certain edges that are not of interest or segment the skeleton into disconnected components. 
The edge weights defined above can be utilized to achieve this. 
We start by first converting the edge weights into dissimilarity measures. Specifically, let ${s_{ij}}{i\neq j}$ be the edge weights, where only connected pairs can take non-zero entries, and let $s{\max} = \max_{i \neq j} s_{ij}$.
We then define the corresponding dissimilarities as $d_{ij} = 0$ if $i = j$, and $d_{ij} = s_{\max} - s_{ij}$ otherwise. 
Next, we apply hierarchical clustering using these distances. The choice of linkage criterion for hierarchical clustering depends on the underlying geometric structure of the data. Single linkage is recommended when the components are well-separated, while average linkage works better when there are overlapping clusters of approximately spherical shapes. 
To determine the resulting segmented skeleton graph, dendrograms can be useful in displaying the clustering structure at different resolutions, and analysts can experiment with different numbers of final clusters and choose a cut that preserves meaningful structures based on the dendrograms. However, it is important to note that the presence of noisy data points may require a larger number of final clusters $S$ to achieve better clustering results. 

\subsection{Computational Complexity}	\label{sec::complexity}
~~~~In this section, we briefly analyze the computational costs of the proposed skeleton regression framework. 
The first main computational burden of the proposed regression procedure is at the skeleton construction step. \citet{skelclus} has provided the computational analysis on this. In particular, when constructing knots, the $k$-means algorithm of Hartigan and Wong \citep{Hartigan1979} has time complexity $O(ndkI)$, where $n$ is the number of points, $d$ is the dimension of the data, $k$ is the number of clusters for $k$-means, and $I$ is the number of iterations needed for convergence. For the edge construction step, the approximate Delaunay Triangulation only depends on the 2-NN neighborhoods,  and the k-d tree algorithm for the 2-nearest knot search gives the worst-case complexity of $O(nd k^{(1-1/d)})$.  For the edge weights with Voronoi density, the numerator can be computed directly from the $2$-NN search without additional computation and the denominators as pairwise distances between knots can be computed with the worst-case complexity of $O(dk^2)$.

Given the skeleton, we then project original feature vectors onto the skeleton, which is not much time-consuming. Finding the edge to project onto depends on identifying the two nearest knots, which is provided in the skeleton construction step. Projection is taking inner product computations and takes $O(nd)$ for all the covariates.

The next computational task is to calculate the skeleton-based distance between points on the skeleton. To find the shortest path on a graph between two faraway knots, the general version of Dijkstra's algorithm \citep{Dijkstra1959} takes $ \Theta (|\calE|+|\calV|^{2})=\Theta (k^{2})$ for each run. However, in practice, we don't need the $\frac{n(n-1)}{2}$ pairwise distances between all the projected points as the skeleton-based regressors proposed can perform with distances in local neighborhoods, which do not require path-finding algorithm for the skeleton-distance calculation.

With all the pairwise skeleton-based distances between projected feature points given, the S-kernel estimate at one point takes $n_{loc}$ kernel weights computation where $n_{loc}$ refers to the local support of the kernel function. S-Lspline takes $O(n)$ time to transform the data and then a single run of matrix multiplication and inversion to get the coefficients.

\subsection{Proofs}	\label{sec::proof}



% \YC{You have to rewrite this proof entirely.
% This is not a proof of the two theorems. }

\subsubsection{Kernel Regression: Convergence on Edge Point (Theorem \ref{thm::edge})} \label{sec::contproof}

% A dimension $d$ result is provided, but note that in the skeleton regression framework we have $d=1$ when applying kernel regression on the skeleton.

% {\color{magenta}The point $x$ is ill-defined here!
% The theorem statement use $s$ and $S_i$.
% You need to write the proof in terms of the metric space rather
% than the usual 1D case.

% A useful tip:
% We are in the asymptotic regime that $h\rightarrow0 $
% and assume that we use a compact support kernel. 
% In this case, for any point $s$ in an edge $e$, 
% any observations $S_j\neq e$ will have 
% $K(d(s,S_j)/h) = 0$.

% You also need to define $\hat g(x)$.
% }

\begin{proof}
Let $\calB(\bs, h) \subset \calS$ be the support for the kernel function $K_h(.)$ at point $\bs \in \calS$ with bandwidth $h$.
For an edge point $\bs \in E_{j\ell} \in \calE$, where $\calE$ is the overall set of edges defined as open sets. As $n\to \infty, h\to 0$, for sufficiently large $n$, by the property of an open set, we have
$$\calB(\bs, h) \subset E_{j\ell} $$ 
and by our definition of skeleton distance, for two points $\bs, \bs' \in E_{j\ell}$ on the same edge in the skeleton, $d_\calS(\bs, \bs') = \norm{\bs-\bs'}$ where $\norm{.}$ denotes the Euclidean distance and is 1-dimensional as parametrized on the same edge. Also we have $$K_h(\bs_j, \bs_\ell) \equiv  K(d_\calS(\bs_j, \bs_\ell)/h) = K(\norm{\bs_j-\bs_\ell}/h) = K\left(\frac{\bs_j-\bs_\ell}{h}\right)$$

Consequently, the skeleton-based kernel regression estimator reduces to 
\begin{align}
    \hat{m}_n(\bs) = \frac{\frac{1}{n h}\sum_{j=1}^n Y_j K(\frac{\bs_j- \bs}{h})  }{\frac{1}{n h}\sum_{j=1}^n K(\frac{\bs_j- \bs}{h})}
\end{align}
and we can use the classical asymptotic results for kernel regression in the continuous case \citep{Bierens1983, wasserman2006all,Chen2017}.

% \JW{The edge points that do not have all the support of kernel on the same edge can be classified into two types. The first type is those near a knot with degree 1. Those are the end knots, and by construction of the skeleton, there is a non-vanishing share of data points that are projected onto those end knots, and the bias hence is also non-vanishing. We may only be able to admit this case.\\
% Another type of problematic edge points are near knots with 2 or more degree. The kernel support of those points include the knot, which has probability $0$ share of original covariates projected onto and hence can be ignored, and the edge points from the other connected edges. Note the maximum degree in to skeleton to be $m$, which is fixed as the skeleton structure is given, then $\PP(\bs_j \in \calV \cap  \calB(\bs, h)) = O(h)$, and we have 
% \begin{align*}
%     \hat{m}(\bs) = \frac{\Tilde{m}(\bs) \hat{g}(\bs) + O(h)}{\hat{g}(\bs) + O(h)} = \frac{\Tilde{m}(\bs) \hat{g}(\bs) }{\hat{g}(\bs)}+ O(h)
% \end{align*}
% where $\Tilde{m}(\bs)$ is the reduced form above. Not sure whether including this case is helpful or not.
% \begin{align}
% \begin{split}
%     \hat{m}(\bs) &= \frac{\sum_{j=1}^n Y_j  K_h(\bs_j, \bs) }{\sum_{j=1}^n K_h(\bs_j, \bs)} \\
%     &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calE)  + \frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calV )}{\frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs)I(\bs_j \in \calE ) + \frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs) I(\bs_j \in \calV ) } \\
%     &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j K_h(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h))}{\frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n K_h(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h)) }
% \end{split}
% \end{align}
% }

% $\Tilde{K_{h}}(\bs_j, \bs) = K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h))$  

Let $\hat{g}_n(\bs) = \frac{1}{nh}\sum_{j=1}^n K\left(\frac{\bs_j-\bs}{h}\right)$. 
We express the difference as
\begin{equation}
\begin{aligned}
     \hat{m}_n(\bs) - m_\calS(\bs) &= \frac{[\hat{m}_n(\bs) - m_\calS(\bs)] \hat{g}_n(\bs)}{\hat{g}_n(\bs)} = \frac{\frac{1}{n h}\sum_{j=1}^n [Y_j - m_\calS(\bs)] K(\frac{\bs_j- \bs}{h})  }{\frac{1}{n h}\sum_{j=1}^n K(\frac{\bs_j-\bs}{h})}
\end{aligned}
\label{eq::decom}
\end{equation}
and we analyze the denominator and numerator below.

Let $g(\bs)$ be the density at point $\bs$ on the skeleton.
For the denominator, we start with the bias:
\begin{align*}
    \abs{\EE \hat{g}_n(\bs) - g(\bs)} &= \abs{\frac{1}{h} \int K\bigg(\frac{\bs - y}{h}\bigg) g(y) d y - g(x) \int K(y) dy}\\
    &= \abs{\int K(z) [g(\bs - h z) - g(\bs)] dz}\\
    &\leq \int K(z) C_1 \abs{h z} dz = C_1 h \int K(z)  \abs{ z} dz = O(h),
\end{align*}
where $C_1$ is the Lipschitz constant of the density function.
%where we used $\int K(z) dz = 1$, the Lipschitz constant $C_1$ of the density function, and that $\int K(z)  \abs{ z} dz$ is bounded. 
For the variance, we have
\begin{align*}
    {\text Var}\left( \hat{g}_n(\bs) \right) &\leq \frac{1}{n h^2} \int K^2\bigg(\frac{\bs - y}{h}\bigg)g(y) dy\\
    &= \frac{1}{n h} \int K^2(z) g(\bs - h z) dz\\
    &\leq \frac{1}{n h} \int K^2(z) [g(\bs) + C_1 \abs{h z} ] dz\\
    &= \frac{1}{n h}\left[ g(\bs) \int K^2(z) dz + C_1 h \int K^2(z) \abs{ z}  dz \right]\\
    &= \frac{1}{n h} g(\bs) \int K^2(z) dz  + o\left(\frac{1}{n h}\right).
\end{align*}

Putting it altogether, we have
\begin{align*}
    \abs{\hat{g}_n(\bs) - g(\bs)} = O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right).
\end{align*}
Note that we only assume Lipschitz continuity and hence has the bias of rate $O(h)$ rather than the usual $O(h^2)$ rate with second order smoothness. 
Higher-order smoothness of $g$ may not improve the overall
estimation rate due to the fact that
we only have Lipschitz continuity of the regression function.
%The reason is that, with Lipschitz continuity of the regression function, requiring more continuity on the density does not improve the overall estimation error rate.


% \begin{align*}
%     \EE \hat{g}(\bs) &= h^{-1} \int K\left(\frac{\bs - z}{h}\right)g(z) dz  = \int K\left(z'\right) g(\bs - h z') g(z')\\
%     &\to g(\bs) \int K(z) d z = g(\bs)
% \end{align*}
% where for the second equality we use the change of variable setting $z' = \frac{\bs - z}{h}$ and the limit holds by the Bounded Convergence Theorem.

Now we analyze the numerator of equation \eqref{eq::decom}.
We start with the decomposition
\begin{align*}
    [\hat{m}_n(\bs) - m_\calS(\bs)] \hat{g}(\bs) &=  \underbrace{\frac{1}{n h} \sum_{j=1}^n U_j K\bigg(\frac{\bs - \bs_j}{h}\bigg)}_{q_1(\bs)} \\
    &+  \underbrace{\frac{1}{n} \sum_{j=1}^n \bigg\{ [m_\calS(\bs_j) - m(\bs)]K\bigg(\frac{\bs - \bs_j}{h}\bigg) \frac{1}{h} - \EE\bigg[ [m_\calS(\bs_j) - m_\calS(\bs)]K\bigg(\frac{\bs - \bs_j}{h}\bigg) \frac{1}{h} \bigg]  \bigg\} }_{q_2(\bs)}\\
    &+  \underbrace{ \frac{1}{n} \sum_{j=1}^n \EE\bigg[ [m_\calS(\bs_j) - m_\calS(\bs)]K\bigg(\frac{\bs - \bs_j}{h}\bigg) \frac{1}{h} \bigg]}_{q_3(\bs)}.
\end{align*}


First, we show that
\begin{align*}
    q_1(\bs)  =  O_p\left(\sqrt{\frac{1}{n h} } \right).
\end{align*}
% \begin{align*}
%     \sqrt{n h} q_1(\bs) \to_d N(0, \sig_u^2(\bs) g(\bs) \int K(z)^2 dz )
% \end{align*}
Let
\begin{align*}
    v_{n,j} (\bs) = U_j K\bigg(\frac{\bs - \bs_j}{h}\bigg)\frac{1}{\sqrt{h}}
\end{align*}
and we have
\begin{align*}
     \sqrt{n h} q_1(\bs) = \frac{1}{\sqrt{n}} \sum_{j=1}^n v_{n,j}(\bs).
\end{align*}
Thus, its mean is
\begin{align*}
    \EE v_{n,j}(\bs) &= \EE\left\{ U_j K\bigg(\frac{\bs - \bs_j}{h}\bigg)\frac{1}{\sqrt{h}} \right\}
    = 0
\end{align*}
and the variance is
\begin{align*}
    \EE [v_{n,j}(\bs)^2] &= \EE U_j^2 K\bigg(\frac{\bs - \bs_j}{h}\bigg)^2\frac{1}{h} = \int \sig_u^2(\bs - h z) g(\bs - h z) K(z)^2 dz\\
    &\to \sig_u^2(\bs) g(\bs) \int K(z)^2 dz = O(1),
\end{align*}
where for the second equality we use the change of variable and by assumption we have $\int K(z)^2 dz <\infty$. Therefore, 
\begin{align*}
    q_1(\bs)  =  O_p\left(\sqrt{\frac{1}{n h }} \right).
\end{align*}

For the second term, note that $\EE (q_2(\bs)) = 0$ and the variance is
\begin{align*}
    \EE \left[\sqrt{n h} q_2(\bs)\right]^2 &= \int [m_\calS(\bs - h z) - m_\calS(\bs)]^2 g(\bs - h z) K(z)^2 dz \\
    &\ \ - h \bigg\{ \int [m_\calS(\bs - h z) - m_\calS(\bs)] g(\bs - h z) K(z) dz  \bigg\}^2\\
    &\to 0
\end{align*}
when $h\rightarrow0$, and hence,
\begin{align*}
    q_2(\bs)  =  o_p\left(\sqrt{\frac{1}{n h} } \right).
\end{align*}


% {\color{magenta}We only need to  get the convergence rate.
% There is no need to use the central limit theorem.
% One of the simplest way to get the rate
% is to compute the asymptotic variance.}

For the last term, note that we have 
\begin{align*}
    q_3(\bs) &= \int[m_\calS(\bs - h z) - m_\calS(\bs)]g(\bs - h z) K(z)  dz \\
    &=\int [ m_\calS(\bs - h z)g(\bs - h z) - m_\calS(\bs) g(\bs) ] K(z) dz - m_\calS(\bs) \int [g(\bs - h z) - g(\bs) ] K(z) dz\\
    &\leq C_1 h \int |z| K(z)dz + C_2 h \int |z| K(z) dz
\end{align*}
where $C_1$ is the Lipschitz constant for $m(\bs)g(\bs)$ and $C_2$ is the Lipschitz constant for $g(\bs)$. Therefore, 
\begin{align*}
     q_3(\bs) = O(h)
\end{align*}

Putting all three terms together, $[\hat{m}(\bs) - m(\bs)] \hat{g}(\bs)  = O(h) + O_p\left(\sqrt{\frac{1}{n h} } \right)$. 
As a result, equation \eqref{eq::decom} becomes
\begin{align*}
    \hat{m}_n(\bs) - m_\calS(\bs) &= \frac{[\hat{m}_n(\bs) - m_\calS(\bs)] \hat{g}(\bs)}{\hat{g}(\bs)} = \frac{O(h) + O_p\left(\sqrt{\frac{1}{n h} } \right)}{ g(\bs) + O(h) + O_p\left(\sqrt{\frac{1}{n h} } \right)}\\
    &= O(h) + O_p\left(\sqrt{\frac{1}{n h} } \right)
\end{align*}
by Taylor expansion of the fraction. 

% Note that, with second order smoothness of the density function and regression function, we can have the bias to be of rate $O(h^2)$.
\end{proof}




\subsubsection{Kernel Regression: Convergence on Knot with Zero Mass (Proposition \ref{prop::zeroknot})}
\label{sec::zeroknotproof}

% \YC{I do not change $h$ to $h$; plz do it before submission.}


For the ease of proof, we first prove
Proposition \ref{prop::zeroknot}
and then prove Theorem \ref{thm::knotconsistency}.

\begin{proof}
Let $\bs \in \calV$ be a knot with no mass, i.e.,
$P(\bs_j = \bs) = 0$. 
The kernel regression can be decomposed
as
\begin{align*}
    \hat{m}(\bs)
     &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h))}{\frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h)) }\\
    &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j  I(\bs_j =\bs)}{\frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n  I(\bs_j =\bs) }\\
    &= \frac{\eps_{1,n}(\bs) + \nu_{1,n}(\bs) }{\eps_{2,n}(\bs) + \nu_{2,n}(\bs) }.
\end{align*}

Because $\bs$ is a point without probability mass,
$\nu_{1,n}(\bs)=\nu_{2,n}(\bs) =0$,
so the above can further reduce to
\begin{align*}
    \hat{m}(\bs)
     &= \frac{\frac{1}{n h}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  }{\frac{1}{n h}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h))}.
\end{align*}

However, different from the case on edges, the support of the kernel intersects with multiple edges even when $h\rightarrow0$, so we study the contribution of each edge individually. 
Note that when $h\rightarrow0$, the only knot that exists in the intersection $\calB(\bs, h)\cap \calE$ is $\bs$.
So we only need to consider contributions of edges 
adjacent to $\bs$.

Let $\calI$ collect all the edge indices with one knot being $\bs$, i.e., $\ell \in \calI$ implies that there is an edge
between $\bs$ and $\bv_\ell \in \calV$.
Let $E_\ell$ be the edge connecting $\bs$ and $\bv_\ell$.
The indicator function $I(\bs_j \in \calE \cap  \calB(\bs, h)) = \sum_{\ell\in\calI}I(\bs_j \in E_\ell \cap  \calB(\bs, h)).$
With this, we can rewrite $\hat m(\bs)$ as 
\begin{equation*}
\begin{aligned}
    \hat m(\bs) &= \frac{ \sum_{\ell\in  \calI} \frac{1}{n h}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in E_\ell \cap  \calB(\bs, h))  }{\sum_{\ell\in \calI}\frac{1}{n h}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in E_\ell \cap  \calB(\bs, h))}\\
    & = \frac{\sum_{\ell\in \calI} \hat m_{n,\ell}(\bs) \hat g_{n,\ell}(\bs)}{\sum_{\ell\in\calI} \hat g_{n,\ell}(\bs)}.
    \end{aligned}
\end{equation*}
where
\begin{align*}
    \hat{g}_{n, \ell}(\bs) &= \frac{1}{n h}\sum_{j=1}^n K\left(\frac{\bs_j-\bs}{h}\right)I(\bs_j \in E_\ell \cap  \calB(\bs, h)),\\
\hat{m}_{n,\ell}(\bs) \cdot \hat{g}_{n, \ell}(\bs) &= \frac{1}{n h}\sum_{j=1}^n Y_j K\left(\frac{\bs_j-\bs}{h}\right) I(\bs_j \in E_\ell \cap  \calB(\bs, h)).
\end{align*}
Thus, we will analyze $\hat g_{n,\ell}(\bs)$
and $\hat m_{n,\ell}(\bs) \hat g_{n,\ell}(\bs)$. 
%Both cases are similar to the case on edges
%except that the point of evaluation is on the boundary. 
%So we have 
%\begin{align*}
%g_{n,\ell}(\bs)& = \frac{1}{n h}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in E_\ell \cap  \calB(\bs, h))\\
%& = \frac{1}{n h}\sum_{j=1}^n K\left(\frac{\bs_j-\bs}{h}\right)I(\bs_j \in E_\ell \cap  \calB(\bs, h))\\
%& = \frac{1}{2}g_\ell(0) + O(h) + O_P\left(\sqrt{\frac{1}{nh}}\right),
%\end{align*}
%where $g_\ell(0) = \lim_{h\rightarrow0}\frac{1}{h} P(S_j \in E_\ell \cap  \calB(\bs, h))$.
%Note that the term $\frac{1}{2}g_\ell(0)$
%comes from the fact that $\bs$ is on the boundary
%of $E_\ell$,
%so for a symmetric kernel $K(.)$,
%the integral is only on one side, leading to
%a factor of $\frac{1}{2}$.
%Similarly, 
%\begin{align*}
%\hat m_{n,\ell}(\bs) \hat g_{n,\ell}(\bs)&=\frac{1}{n h}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in E_\ell \cap  \calB(\bs, h))\\
%& = \frac{1}{n h}\sum_{j=1}^n Y_j K\left(\frac{\bs_j-\bs}{h}\right)I(\bs_j \in E_\ell \cap  \calB(\bs, h))\\
%& = \frac{1}{2}m_\ell(0)g_\ell(0) + O(h) + O_P\left(\sqrt{\frac{1}{nh}}\right),
%\end{align*}
%where $m_\ell(0) =  \lim_{\bx \in E_\ell, \bx\rightarrow \bs} m(\bx)$.
%}
%\YC{You need to define $K_{h}(t,0)$ for a univariate $t$. }
%Let $E_\ell$ be an edge between knot $\bv_\ell$ and $\bs$ such that $E_\ell \cap  \calB(\bs, h))$ is not empty. 
For a point $\bs_j$ on the edge $E_\ell$, we can reparamterize it as $\bs_j =  T_j \bv_\ell  + (1-T_j) \bs$ for some  $T_j \in (0,1)$. 
The location $\bs$ corresponds to the case $T_j = 0$
and any $\bs_j\in E_\ell$ will be mapped to $T_j>0.$
With this reparameterization, 
we can write
\begin{align*}
    \hat{g}_{n, \ell}(\bs) 
    &= \frac{1}{n h}\sum_{j=1}^n K\left(\frac{T_j}{h}(\bv_\ell-\bs)\right)I(\bs_j \in E_\ell \cap  \calB(\bs, h)),\\
\hat{m}_{n,\ell}(\bs) \cdot \hat{g}_{n, \ell}(\bs) 
&= \frac{1}{n h}\sum_{j=1}^nY_j K\left(\frac{T_j}{h}(\bv_\ell-\bs)\right)I(\bs_j \in E_\ell \cap  \calB(\bs, h)).
\end{align*}

To study the limiting behavior when $h \rightarrow0$,
let $g_\ell(t) = g((1-t)\bs + t \bv_\ell)$, $g_\ell(0) = \lim_{x\downarrow 0} g_{\ell}(x)$;
$m_\ell(t) = m_\calS( (1-t)\bs + t \bv_\ell)$, $m_\ell(0) = \lim_{t \downarrow 0} m_\ell(t)  $;
and
$\sigma_\ell^2(t) = \EE(|U_j|^2 | \bs_j = (1-t) \bs + t \bv_\ell )$ , $\sigma_\ell^2(0) = \lim_{t \downarrow 0} \sigma_\ell^2(t) $.
% \YC{I added the following sentence to 
% explain how we get the integral.}
Then with the new notations, we can write 
\begin{align*}
\EE(f(T_j(\bv_\ell-\bs))I(\bs_j \in E_\ell \cap  \calB(\bs, h))) &= \EE(f(\bs_j-s)I(\bs_j \in E_\ell \cap  \calB(\bs, h)))\\
    &=\int_{t>0}f(t) g_\ell(t)dt
\end{align*}
for any integrable function $f$.
The bias of the denominator can be written as
\begin{align*}
    \abs{\EE \hat{g}_{n,\ell}(\bs) -\frac{1}{2} g_\ell(0)} &= \abs{\frac{1}{h} \int_{t>0} K\bigg(\frac{t}{h}\bigg) g_\ell(t) d t  - g_\ell(0) \int_{z>0} K(z)}\\
    &= \abs{\int_{z>0} K(z) [g_\ell(h z) -  g_\ell(0)] dz } \\
    &\leq \int_{z>0} K(z) C_1 h z  dz \\
    &=  C_1 h  \int_{z>0} K(z) z dz = O(h).
\end{align*}
For stochastic variation, we have
\begin{align*}
    {\text Var}\left( \hat{g}_{n,\ell}(\bs) \right) &\leq \frac{1}{n h^2} \int_{t>0} K^2\bigg(\frac{t}{h}\bigg)g_\ell(t) dt\\
    &= \frac{1}{n h} \int_{z>0} K^2(z) g(h z) dz\\
    &\leq \frac{1}{n h} \int_{z>0} K^2(z) [g(0) + C_1 \abs{h z} ] dz\\
    &= \frac{1}{n h}\left[ g(0) \int_{z>0} K^2(z) dz + C_1 h \int_{z>0} K^2(z) \abs{ z}  dz \right]\\
    &= O\left(\frac{1}{n h}\right).
\end{align*}
%Let $\calI$ collect all the edge indexes with one knot being $\bs$ and together we have
Thus, 
\begin{align*}
    \hat{g}_n(\bs) = \sum_{\ell \in \calI} \hat{g}_{n,\ell}(\bs) = \frac{1}{2} \sum_{\ell \in \calI}   g_\ell(0) + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right)
\end{align*}



For the numerator, 
\begin{align*}
    \hat{m}_{n,\ell}(\bs) \hat{g}_{n,\ell}(\bs) &=  \underbrace{\frac{1}{n h} \sum_{j=1}^n U_j K\bigg(\frac{t_j}{h}\bigg)I(\bs_j \in E_\ell \cap  \calB(\bs, h))}_{Q_1} \\
    &\qquad+  \underbrace{\frac{1}{n h} \sum_{j=1}^n   m_\calS(\bs_j) K\bigg(\frac{t_j}{h}\bigg)I(\bs_j \in E_\ell \cap  \calB(\bs, h))}_{Q_2},
\end{align*}
where $U_j  = Y_j - m_\calS(\bs_j)$.
% By model definition of $U_j$ we again have $\EE Q_1 = 0$. For variation of $Q_1$,  
% % \YC{conditional independence?}
Using the fact that $\EE(U_j|\bs_j) = 0$,
$\EE(Q_1)=0$, and the variance is
\begin{align*}
    {\text Var}(Q_1) &\leq \frac{1}{n h^2 } \int_{t > 0} \sig_\ell^2(t) K^2\bigg(\frac{t}{h}\bigg) g_\ell(t) dt \\
    &= \frac{1}{n h } \int_{z > 0} \sig_\ell^2(h z) K^2(z) g_\ell(h z) dz\\
    &= \frac{1}{n h } \int_{z > 0} \sig_\ell^2(0) K^2(z) g_\ell(0) dz + O\left(\frac{1}{n h}\right) = O\left(\frac{1}{n h}\right) .
\end{align*}
For $Q_2$, we have
\begin{align*}
    \abs{\EE (Q_2) - \frac{m_\ell(0) g_\ell(0)}{2}  } &= \abs{\frac{1}{h}\int_{t>0} m_\ell( t) K(t/h) g(t)  dt - m_\ell(0) g_\ell(0) \int_{z>0}  K(z)  dz } \\
    &=\abs{\int_{z>0} m_\ell( h z) K(z) g_\ell(h z)  dz - m_\ell(0) g_\ell(0) \int_{z>0}  K(z)  dz } \\
    &\leq \int_{z>0} \bigg\{ \big[m_\ell(0)+ C_2 h z\big]  \big[g_\ell(0) + C_1 h z\big] - m_\ell(0) g_\ell(0)\bigg\} K(z)  dz \\
    &\leq [C_1 m_\ell(0)+C_2g_\ell(0)] h \int_{z>0}  K(z) z dz + o(h)=  O(h).
\end{align*}
The varaince of $Q_2$ is bounded via
\begin{align*}
    {\text Var}(q_2) &\leq \frac{1}{n h^2 } \int_{t > 0} m_\ell^2(t) K^2\bigg(\frac{t}{h}\bigg) g_\ell(t) dt\\
    &= \frac{1}{n h} \int_{z > 0} m_\ell^2(h z) K^2(z) g_\ell(h z) dz\\
    &\leq \frac{1}{n h} \int_{z>0} \left\{m_\ell(0)+ C_2 \abs{h z}\right\}^2  K^2(z) \left\{g_\ell(0) + C_1 \abs{h z}\right\}  dz \\
    &= \frac{1}{n h} \left\{ m^2_\ell(0) g_\ell(0) \int_{z>0} z K^2(z)   dz  + O(h)\right\}\\
    &= O\left(\frac{1}{n h}\right) 
\end{align*}
Putting the terms $Q_1$ and $Q_2$ together, we have
\begin{align*}
    \hat{m}_{n,\ell}(\bs) \hat{g}_{n,\ell}(\bs) = \frac{1}{2} m_\ell(0) g_\ell(0) + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right).
\end{align*}
As a result, we conclude that
\begin{align*}
    \hat{m}(\bs) &= \frac{\sum_{\ell \in \calI}\hat{m}_{n,\ell}(\bs)  \hat{g}_{n,\ell}(\bs)}{\sum_{\ell \in \calI} \hat{g}_{n,\ell}(\bs) }\\
    &= \frac{ \frac{1}{2} \sum_{\ell \in \calI}   m_\ell(0) g_\ell(0) + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right) }{\frac{1}{2} \sum_{\ell \in \calI}   g_\ell(0) + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right)} \\
    &=  \frac{ \frac{1}{2} \sum_{\ell \in \calI}   m_\ell(0) g_\ell(0)  }{\frac{1}{2} \sum_{\ell \in \calI}   g_\ell(0) } + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right)\\
    &= \frac{ \sum_{\ell \in \calI}   m_\ell(0) g_\ell(0)  }{ \sum_{\ell \in \calI}   g_\ell(0) } + O(h) + O_p\left(\sqrt{\frac{1}{n h}}\right),
\end{align*}
which completes the proof.
\end{proof}



\subsubsection{Kernel Regression: Convergence on Knot with Nonzero Mass (Theorem \ref{thm::knotconsistency})}
\label{sec::knotproof}



\begin{proof}

Let $\bs \in \calV$ be a point where $P(\bs_j =\bs) = p(\bs)>0$.
Recall that the kernel regression can be expressed as
\begin{align*}
    \hat{m}(\bs)
     &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h))}{\frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs) I(\bs_j \in \calV \cap  \calB(\bs, h)) }\\
    &= \frac{\frac{1}{n}\sum_{j=1}^n Y_j K_{h}(\bs_j, \bs) I(\bs_j \in \calE \cap  \calB(\bs, h))  + \frac{1}{n}\sum_{j=1}^n Y_j  I(\bs_j =\bs)}{\frac{1}{n}\sum_{j=1}^n K_{h}(\bs_j, \bs)I(\bs_j \in \calE \cap  \calB(\bs, h)) + \frac{1}{n}\sum_{j=1}^n  I(\bs_j =\bs) }\\
    &= \frac{\eps_{1,n}(\bs) + \nu_{1,n}(\bs) }{\eps_{2,n}(\bs) + \nu_{2,n}(\bs) }.
\end{align*}

% {\color{magenta}Replace $x$ by $\bs$. Also, you need to define $p(\bs)$. Also need to explain why $\epsilon_{2,n}(\bs)\rightarrow 0$.}

% We have
% $\eps_{2,n}(\bs) \to 0$, $\nu_{2,n}(\bs) \to_p p(\bs)$, 
% and hence the denominator as a whole converge in probability to $p(\bs)$. 
% Similarly we can have the estimator as a whole converge in probability to $m(\bs) = M_\ell$ where $\bv_\ell = \bs$.


% In the numerator, for bias, the edge component will give $O(h)$ rate, but no bias from the discrete component.
% For stochastic variation, the edge component will give $O_p(\sqrt{\frac{1}{n h}})$ rate, and the discrete component will give $O_p(\sqrt{\frac{1}{n}})$.

% Note that here we have $\bs \in \calV$, and hence $\PP\left(\calE \cap  \calB(\bs, h) \right) \leq g_{\max} \times h = O(h)$ where $g_{\max}$ is the maximum density value on the skeleton. 
We look at each term individually and note that we have the edge components terms identical to the proof of  Proposition \ref{prop::zeroknot}, so 
\begin{align*}
    \eps_{1,n}(\bs) &= h \left\{ \sum_{\ell \in \calI}   m_\ell(0) g_\ell(0)   + O(h)+ O_p\left(\sqrt{\frac{1}{n h}}\right) \right\} =  O(h)+ O_p\left(\sqrt{\frac{h}{n }}\right), \\
    \eps_{2,n}(\bs) &= h \left\{ \sum_{\ell \in \calI}   g_\ell(0) + O(h)+ O_p\left(\sqrt{\frac{1}{n h}}\right) \right\} = O(h)+ O_p\left(\sqrt{\frac{h}{n }}\right).
\end{align*}

For the terms on the knots, they are just a sample average,
so
\begin{align*}
    \nu_{2,n}(\bs)  = p(\bs)+ O_p\left(\sqrt{\frac{1}{n}}\right)
\end{align*}
and similarly
\begin{align*}
        \nu_{1,n}(\bs) &= \frac{1}{n}\sum_{j=1}^n \left(m_\calS(\bs\right) + U_j)  I(\bs_j =\bs)\\
        &= m_\calS(\bs) p(\bs) + O_p\left(\sqrt{\frac{1}{n}}\right).
\end{align*}

With the fact that $O_p\left(\sqrt{\frac{1}{n }}\right)$ dominates $O_p\left(\sqrt{\frac{h}{n }}\right)$, 
we conclude
\begin{align*}
    \hat{m}(\bs) &= \frac{O(h)+ O_p\left(\sqrt{\frac{ h}{n}}\right) + m_\calS(\bs)p(\bs) + O_p\left(\sqrt{\frac{1}{n}}\right)}{ O(h)+ O_p\left(\sqrt{\frac{h}{n }}\right) + p(\bs) + O_p\left(\sqrt{\frac{1}{n}}\right)}\\
    &= \frac{O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right) }{ O(h)+ O_p\left(\sqrt{\frac{1}{n}}\right) + p(\bs) } + \frac{m_\calS(\bs)p(\bs) }{ O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right) + p(\bs) }\\
    &= \frac{O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right) }{ p(\bs) } + O\left[\left(\frac{O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right) }{ p(\bs) }\right)^2\right]\\
    &\ \ \ \ + m_\calS(\bs)p(\bs) \left\{ \frac{1}{p(\bs)} + \frac{O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right)}{ p(\bs)^2}  \right\}\\
    &= m_\calS(\bs) +  O(h)+ O_p\left(\sqrt{\frac{1}{n }}\right),
\end{align*}
which completes the proof.

% we first look at the denominator that 
% \begin{align*}
%     \frac{1}{\eps_{2,n}(\bs) + \nu_{2,n}(\bs) } &= \frac{1}{ \nu_{2,n}(\bs) } - \frac{\eps_{2,n}(\bs)}{\nu_{2,n}(\bs)^2} + \frac{1}{\xi_{2,n}(\bs)^3} \eps_{2,n}(\bs)^2\\
%     & = \frac{1}{ \nu_{2,n}(\bs) } + \frac{O(h)+ O_p\left(\sqrt{\frac{1}{n h}}\right)}{O_p\left(\sqrt{\frac{1}{n}}\right)}
% \end{align*}
% for $\xi_{2,n}(\bs)$ between $\nu_{2,n}(\bs)$ and $\eps_{2,n}(\bs) + \nu_{2,n}(\bs) $.
% So the later terms are $O_p(\frac{h}{n})$ and can be ignored. 

\end{proof}


% \subsection{Theory for S-kNN Regressor}

% For consistency,
% it is well known that we need k to grow as a function of the sample size n .

% \cite{DistributionFreeThoery} Theorem 6.1 states the consistency result that
% \begin{thm}
% If $k_n \rightarrow \infty, k_n / n \rightarrow 0$, then the $k_n$-NN regression function estimate is weakly consistent for all distributions of $(X, Y)$ where ties occur with probability zero and $\mathbf{E} Y^2<\infty$, i.e.,
% $$
% \lim _{n \rightarrow \infty} \mathbf{E}\left\{\int\left(m_n(x)-m(x)\right)^2 \mu(d x)\right\}=0
% $$
% for all distributions of $(X, Y)$ with $\mathbf{E} Y^2<\infty$.
% \end{thm}

% Theorem 6.2 states the rate of convergence result that
% \begin{thm}
%     Theorem 6.2. Assume that $X$ is bounded,
% \begin{align*}
% \sigma^2(x)=\operatorname{Var}(Y \mid X=x) \leq \sigma^2 \quad\left(x \in \mathcal{R}^d\right)
% \end{align*}
% and
% \begin{align*}
% |m(x)-m(z)| \leq C\|x-z\| \quad\left(x, z \in \mathcal{R}^d\right) .
% \end{align*}
% Also assume that there exist $\epsilon_0$, a nonnegative function $g$ such that for all $x \in \calR$ and $0 < \epsilon < \epsilon_0$. For $S_{x, \eps}$ closed ball centered at $x$ with radius $\eps$, we have
% \begin{align*}
%     \mu\brac{S_{x, \eps}} > g(x) \eps^d
% \end{align*}
% and
% \begin{align*}
%     \int \frac{1}{g(x)^2} \mu(dx ) < \infty
% \end{align*}
% Let $m_n$ be the $k_n-N N$ estimate. Then
% \begin{align*}
% \mathbf{E}\left\|m_n-m\right\|^2 \leq \frac{\sigma^2}{k_n}+c_1 \cdot C^2\left(\frac{k_n}{n}\right)^{2 /d},
% \end{align*}
% thus for $k_n=c^{\prime}\left(\sigma^2 / C^2\right)^{1/3} n^{\frac{2}{3}}$,
% \begin{align*}
% \mathbf{E}\left\|m_n-m\right\|^2 \leq c^{\prime \prime} \sigma^{\frac{4}{3}} C^{\frac{2 }{3}} n^{-\frac{2}{3} .} .
% \end{align*}
% \end{thm}

% ~\\
% The key step for the rate is to prove that under the conditions
% \begin{align*}
% \mathbf{E}\left\{\left\|X_{(1, n)}(X)-X\right\|^2\right\} \leq \frac{\tilde{c}}{n^{2 / d}} .
% \end{align*}
% The assumption implies that for almost all $x \bmod \mu$ and $\epsilon_0<\epsilon<L$,
% \begin{align*}
% \mu\left(S_{x, \mathrm{c}}\right) \geq \mu\left(S_{x, \epsilon_0}\right) \geq g(x) \epsilon_0^d \geq g(x)\left(\frac{\epsilon_0}{L}\right)^d \epsilon^d,
% \end{align*}
% hence we can assume w.l.o.g. that the condition holds for all $0<\epsilon<L$. In this case, we get, for fixed $L>\epsilon>0$,
% \begin{align*}
% \begin{aligned}
% \mathbf{P}\left\{\left\|X_{(1, n)}(X)-X\right\|>\epsilon\right\} & =\mathbf{E}\left\{\left(1-\mu\left(S_{X, \epsilon}\right)\right)^n\right\} \\
% & \leq \mathbf{E}\left\{e^{-n \mu\left(S_{X, \epsilon}\right)}\right\} \\
% & \leq \mathbf{E}\left\{e^{-n g(X) \epsilon^d}\right\},
% \end{aligned}
% \end{align*}
% therefore,
% \begin{align*}
% \begin{aligned}
% \mathbf{E}\left\{\left\|X_{(1, n)}(X)-X\right\|^2\right\} & =\int_0^{L^2} \mathbf{P}\left\{\left\|X_{(1, n)}(X)-X\right\|>\sqrt{\epsilon}\right\} d \epsilon \\
% & \leq \int_0^{L^2} \mathbf{E}\left\{e^{-n g(X) c^{d / 2}}\right\} d \epsilon \\
% & \leq \iint_0^{\infty} e^{-n g(x) c^{d / 2}} d \epsilon \mu(d x) \\
% & =\int \frac{1}{n^{2 / d} g(x)^{2 / d}} \int_0^{\infty} e^{-z^{d / 2}} d z \mu(d x) \\
% & =\frac{\tilde{c}}{n^{2 / d}} .
% \end{aligned}
% \end{align*}

% \revise{Overall, the theory works if we figure out how to adopt this to the measures on the skeleton. However, the case is a bit trickier with the knot points with nonzero mass, as all points projected to the knot have the same skeleton-based distance. The essence will still be down to the analog of the rate of convergence of the nearest neighbor distance, which is $0$ in this case, and with model assumptions that $Y = m_\calS(x) + \eps$ for $x \in \calS$ we can just gather all the errors into the noise part.}

\subsection{Additional Simulation Results}
\label{sec::extraSim}

~~~~In this section, we examine the effect of cutting the skeleton into various numbers of disjoint components on the final regression performance. 
We use the same simulated datasets from Section \ref{sec::simulation}, including Yinyang data, Noisy Yinyang data, and SwissRoll data. 
The analysis procedure is mainly the same, where we use $5$-fold cross-validation SSE to evaluate the regression results for each dataset and repeat the process 100 times with randomly generated datasets. 
The main difference is that, during the skeleton construction step, we segment the skeleton graph into different disjoint components using single-linkage hierarchical clustering with respect to the Voronoi Density weights, as outlined in Section \ref{sec::skeletoncons}. 
We then fit and evaluate the skeleton-based regression methods on the skeletons that have been differently cut. 
% present some additional simulation results using the proposed skeleton regression framework, and we mainly 

% \YC{I don't think we need to 
% show the figures of Q-spline and C-spline
% since they are often not as good as teh L-spline. 
% As for the kernel and kNN,
% you should not display all the smoothing bandwidth/NN.
% Probably just pick 3-5 of them.
% The figures look very chaotic...}


% \YC{Also, did you repeat the experiment multiple times?
% We need to repeat it at least 100 times and display the error bars
% of the SSE under different cuts. }

 
\subsubsection{Vary Skeleton Cuts for Yinyang Data}
~~~~In this section, we investigate how cutting the skeleton into different numbers of disjoint components affects the performance of skeleton-based methods using the Yinyang data (from Section \ref{sec::Yinyang}). We randomly generate 1000-dimensional Yinyang data 100 times and use $5$-fold cross-validation to calculate the SSE on each dataset. We fit the skeleton-based methods in the same manner as in Section \ref{sec::simulation}, with the exception that the number of knots is fixed at 38 and we cut the initial graph into various numbers of disjoint components (ranging from 1 to 25) when constructing the skeleton. The median 5-fold cross-validation SSEs across the 100 datasets for different numbers of disjoint components are plotted in Figure \ref{fig::Yinyangd1000cut}.

Our results show that the S-Lspline method is sensitive to changes in the skeleton structure. In the case of Yinyang data, since there are 5 true disjoint structures in the covariate space, a cut of 5 results in the best regression performance. By design, S-Lspline regressors may incorporate unrelated information from one structure to another when an edge connects two structurally different areas, thus leading to a decline in the regression performance. As a future research, incorporating edge weights into the S-Lspline regressor may help to mitigate the interference between different structures. The S-Kernel regressor also achieves optimal performance when the skeleton is segmented into 5 disjoint components. Skeleton-based kernel regression methods exhibit large changes in performance as the skeleton segmentation changes when the bandwidth is large. This is understandable as larger bandwidth allow more information from large distances, which are more likely to be non-informative as the segmentation changes. On the other hand, S-kNN regressor has best regression performance when the skeleton is left as a fully connected graph. This may be due to the locally adaptive nature of k-nearest-neighbor method that ensure regression result are accurate as long as local neighborhood are identified accurately.

\begin{figure}
% \captionsetup{skip=1pt}
\centering
\includegraphics[width = \textwidth]{figures/Yinyang1000_cuts.jpeg}
\caption{Yinyang $d = 1000$ data skeleton regression results with the number of knots fixed as $38$ but segmented into varying numbers of disjoint components. The medium SSE across the $100$ simulated datasets with each given parameter setting is plotted. 
% \YC{Does this mean that the few cut seems to be always better?
% For kernel}
}
\label{fig::Yinyangd1000cut}
\end{figure}
%   \begin{subfigure}[t]{0.26\textwidth}
%         \centering
% \includegraphics[width=\linewidth]{figures/Yinyang100_ncut_skelLspline.jpeg} 
%         \caption{S-Lspline}
%     \end{subfigure}
% %         \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/Yinyang100_ncut_skelQspline.jpeg} 
% %         \caption{S-Qspline}
% %     \end{subfigure}
% %     \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/Yinyang100_ncut_skelCspline.jpeg} 
% %         \caption{S-Cspline}
% %     \end{subfigure}\\
%         \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Yinyang100_ncut_skelkernel.jpeg}
%         \caption{S-Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Yinyang100_ncut_skelknn.jpeg}
%         \caption{S-kNN}
%     \end{subfigure}


\subsubsection{Vary Skeleton Cuts for Noisy Yinyang Data}
~~~~We then evaluate the performance of the skeleton-based regression methods on the Noisy Yinyang data (from Section \ref{sec::NoisyYinyang}) when the skeletons are constructed with different numbers of disjoint components. Similarly, we randomly generate 1000-dimensional Noisy Yinyang data 100 times and use $5$-fold cross-validation to calculate the sum of squared errors (SSE) on each dataset. We fix the number of knots to be 71 and construct skeletons with different numbers of disjoint components. The median 5-fold cross-validation SSEs across the 100 datasets for different numbers of disjoint components are plotted in Figure \ref{fig::NoiseYinyangd1000cut}.

\begin{figure}
% \captionsetup{skip=1pt}
\centering
\includegraphics[width = \textwidth]{figures/NoiseYinyang1000_cuts.jpeg}
\caption{Noise Yinyang $d = 1000$ data skeleton regression results with the number of knots fixed as $38$ but segmented into varying numbers of disjoint components. The medium SSE across the $100$ simulated datasets with each given parameter setting is plotted. }
\label{fig::NoiseYinyangd1000cut}
\end{figure}

With the presence of noise, the S-Lspline method does not show significant variations in performance when the skeleton graph is cut into different disjoint components. The best regression result is obtained when the graph is left as a fully connected graph.
In contrast, the performance of the S-kernel method varies with the number of disjoint components. The best results, regardless of the bandwidth, are obtained when the skeleton is segmented into around 13 components, which is larger than the true number of 5 components in the data.
Lastly, the S-kNN method demonstrates an increase in SSE with an increase in the number of disjoint components.


% \begin{figure}
% % \captionsetup{skip=1pt}
% \centering
%     \begin{subfigure}[t]{0.26\textwidth}
%         \centering
% \includegraphics[width=\linewidth]{figures/NoiseYinyang100_ncut_skelLspline.jpeg} 
%         \caption{S-Lspline}
%     \end{subfigure}
% %         \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/NoiseYinyang100_ncut_skelQspline.jpeg} 
% %         \caption{S-Qspline}
% %     \end{subfigure}
% %     \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/NoiseYinyang100_ncut_skelCspline.jpeg} 
% %         \caption{S-Cspline}
% %     \end{subfigure}\\
%         \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/NoiseYinyang100_ncut_skelkernel.jpeg}
%         \caption{S-Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/NoiseYinyang100_ncut_skelknn.jpeg}
%         \caption{S-kNN}
%     \end{subfigure}
% \caption{Noisy Yinyang Regression fitted points $d = 1000$ with varying number of cuts results, with number of knots fixed as $99$.
% }
% \label{fig::NoiseYinyangd1000cut}
% \end{figure}




\subsubsection{Vary Skeleton Cuts for SwissRoll data}
~~~~In this section, we evaluate the performance of skeleton-based methods on SwissRoll data (from Section \ref{sec::SwissRoll}) with skeletons cut into different numbers of disjoint components. Similarly, we randomly generate 1000-dimensional SwissRoll data 100 times and use $5$-fold cross-validation to calculate the sum of squared errors (SSE) on each dataset. We fix the number of knots to be 70 and construct skeletons with different numbers of disjoint components. The median 5-fold cross-validation SSEs across the 100 datasets for different numbers of disjoint components are plotted in Figure \ref{fig::SwissRolld1000cut}.

We find that the S-Lspline regressor is sensitive to changes in the skeleton structure, with the best regression results obtained when the skeleton is constructed as one connected graph. This makes sense as the covariates lie on one connected manifold.
The S-Kernel regressor also performs best on the fully-connected skeleton. After an initial increase in SSE as the number of disjoint components increases, the SSE of the S-kernel regressor remains relatively stable.
The S-kNN regressor also achieves the best regression performance when the skeleton is left as a fully connected graph.
Overall, the SSE of the S-kNN regressor increases with the number of disjoint components, but for a small number of neighbors, there can be a decrease in SSE when the skeleton is cut into more disjoint components. One possible explanation is that, as the response function has discontinuous changes, segmenting the covariate space into more fragments can improve estimation in regions where the response changes abruptly.


\begin{figure}
% \captionsetup{skip=1pt}
\centering
\includegraphics[width = \textwidth]{figures/SwissRoll1000_cuts.jpeg}
\caption{SwissRoll $d = 1000$ data skeleton regression results with the number of knots fixed as $70$ but segmented into varying numbers of disjoint components. The medium SSE across the $100$ simulated datasets with each given parameter setting is plotted.}
\label{fig::SwissRolld1000cut}
\end{figure}

%     \begin{subfigure}[t]{0.26\textwidth}
%         \centering
% \includegraphics[width=\linewidth]{figures/Swissd100_ncut_lspline.jpeg} 
%         \caption{S-Lspline}
%     \end{subfigure}
% %         \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/Swissd100_ncut_qspline.jpeg} 
% %         \caption{S-Qspline}
% %     \end{subfigure}
% %     \begin{subfigure}[t]{0.3\textwidth}
% %         \centering
% % \includegraphics[width=\linewidth]{figures/Swissd100_ncut_cspline.jpeg} 
% %         \caption{S-Cspline}
% %     \end{subfigure}\\
%         \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Swissd100_ncut_skelkernel.jpeg}
%         \caption{S-Kernel}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Swissd100_ncut_skelnn.jpeg}
%         \caption{S-kNN}
%     \end{subfigure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Additional Real Data Examples}
\label{sec::extraReal}

% \YC{This entire section needs a rework. 
% You should not just display the results.
% There needs to be some discussion, like in the main paper. }


~~~~In this section, we present results on some additional examples from the COIL-20 dataset \citep{COIL20}, following the same procedure as in Section \ref{sec::luckycat}.
Each dataset consists of $72$ gray-scale images of size $128\times 128$ pixels as 2D projections of a 3D object obtained through rotating the object by $72$ equispaced angles on a single axis.
The response is the angle of rotation, and to avoid the circular response issue, we remove the last $8$ images from the sequence and only use the first $64$ images from each dataset. 
We use leave-one-out cross-validation to assess the performance of each method.

\subsubsection{Cup Images Data}
~~~~We start with a sequence of images of a cup, with some example images in Figure \ref{fig::cup}.
The best SSE from each method is listed in Table \ref{table:cup} along with the corresponding parameters. We see that the S-Lspline method gives the best performance in terms of SSE. 
The usual kNN regressor also performs well on this data, and it can be due to the different lighting conditions inside the cups that make Euclidean distance between the pixel vectors also reliable for this task. 
\begin{figure}
\centering
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__0.png} 
    \end{subfigure}
        \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__12.png}
    \end{subfigure}
            \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__24.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__36.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__48.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj15__60.png}
    \end{subfigure}
\captionof{figure}{A part of the cup images from the COIL-20 processed dataset. Each image
is of size $128\time 128$ pixels.}
\label{fig::cup}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\centering
\begin{tabular}{||c c c||} 
 \hline
 Method & SSE &  Parameter \\ [0.5ex] 
 \hline\hline
 kNN & 1147.2 & neighbor=3 \\ 
 Ridge& -&-\\
 Lasso& -&-\\
 SpecSeries& -&-\\
 S-Kernel & 2561.5 &  bandwidth = 4$r_{hns}$ \\
 S-kNN & 4730.6 & neighbor = 3\\
 S-Lspline &  1073.4  &- \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on cup images data from COIL-20. The best SSE from each method is listed with the corresponding parameters used.}
\label{table:cup}

\end{figure}


%---------------------------------------------------------

% \subsubsection{Piggy Bank Images Data}
% We look at another sequence of images taken around a piggy bank, with some examples in Figure \ref{fig::pigbank}.
% The best SSE from each method is listed in Table \ref{table:pigbank} along with the corresponding parameters. 
% We see that the S-Lspline method gives the best performance in terms of SSE, while the usual kNN regressor also performs well on this data.

% \begin{figure}
% \captionsetup{skip=1pt}
% \centering
%     \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__0.png} 
%     \end{subfigure}
%         \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__12.png}
%     \end{subfigure}
%             \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__24.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__36.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__48.png}
%     \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/obj13__60.png}
%     \end{subfigure}
% \caption{A part of the piggy bank images from the COIL-20 processed dataset. Each image
% is of size $128\time 128$ pixels.}
% \label{fig::pigbank}
% \end{figure}

% \begin{table}
% \centering
% \begin{tabular}{||c c c||} 
%  \hline
%  Method & SSE &  Parameter \\ [0.5ex] 
%  \hline\hline
%  kNN & 888.9 & neighbor=3 \\ 
%  Ridge& -&-\\
%  Lasso& -&-\\
%  SpecSeries& -&-\\
%  S-Kernel & 2948.1 &  bandwidth = 2$r_{hns}$ \\
%  S-kNN& 5050.0 & enighbor = 6\\
%  S-Lspline &  1251.1  &- \\[1ex] 
%  \hline
% \end{tabular}
% \caption{Regression results on piggy bank images from COIL-20. The best SSE from each method is listed with the corresponding parameters used.}
% \label{table:pigbank}
% \end{table}




%-------------------------------------------------------------
\subsubsection{Sauce Box Image Data}
~~~~We look at another sequence of images taken around a sauce box, with some example images in Figure \ref{fig::sauce}.
The best SSE from each method is listed in Table \ref{table:sauce} along with the corresponding parameters. 
In this case, the usual kNN regressor gives the best performance in terms of SSE, while the S-Lspline method gives satisfactory results. 
The good performance of kNN regressor can be due to the distinctive marks on the box, which makes neighbor search through Euclidean distance on the vectorized image inputs effective.
However, the proposed skeleton regression method shows stable performance across different object images and, by explicitly modeling the latent manifold structure, can give a more structured model of the response.

\begin{figure}
\centering
    \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj20__0.png} 
    \end{subfigure}
        \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj20__12.png}
    \end{subfigure}
            \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj20__24.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj20__36.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj20__48.png}
    \end{subfigure} \begin{subfigure}[t]{0.15\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/obj20__60.png}
    \end{subfigure}
\captionof{figure}{A part of the sauce images from the COIL-20 processed dataset. Each image
is of size $128\time 128$ pixels.}
\label{fig::sauce}
\vspace{2em}
%%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
\begin{tabular}{||c c c||} 
 \hline
 Method & SSE &  Parameter \\ [0.5ex] 
 \hline\hline
 kNN & 955.6 & neighbor=3 \\ 
  Ridge& -&-\\
 Lasso& -&-\\
 SpecSeries& -&-\\
 S-Kernel & 2998.8 &  bandwidth = 4$r_{hns}$ \\
 S-kNN& 5285.4 & neighbor = 6\\
 S-Lspline &  1220.1  &- \\[1ex] 
 \hline
\end{tabular}
\captionof{table}{Regression results on sauce images data from COIL-20. The best SSE from each method is listed with the corresponding parameters used.}
\label{table:sauce}

\end{figure}



\end{document}