\appendix

To further substantiate the effectiveness of our proposed method, we provide additional experimental results in the supplementary material that could not be included in the paper due to space limitations.
The supplementary material is divided into two main sections.
The first section includes additional ablation experiments, while the second section features supplementary experimental results on VG-150.

\section{More ablation studies}
To more comprehensively verify the effect of each module in HiLo, more ablation studies are conducted on the PSG dataset \cite{yang2022panoptic}, employing HiLo with a Resnet-50 \cite{he2016deep} backbone.

% \miaojing{which dataset which setup}
\textbf{HiLo relation swapping}.
To investigate the effects of relation swapping, we conducted an \emph{extreme} experiment that involved replacing the relation of a subject-object pair with either the highest frequency relation (for L-H Data) or the lowest frequency relation (for H-L Data), instead of the \emph{adjacent} frequency relation.
The experimental outcomes are presented in Tab. \ref{table:relation_swapping}, which demonstrates a substantial decrease in the model's mean recall due to \emph{extreme} swapping.
This decline is attributable to the insufficient middle frequency relations for subject-object pairs encompassing more than two relations.
This experiment justifies the rationality of our current relation swapping method utilized in the construction of H-L Data and L-H Data.
Even with \emph{extreme} swapping, the performance is still improved over our \emph{baseline}.
This finding underscores the efficacy of our HiLo framework in effectively enhancing the model's performance.

\textbf{HiLo prediction alignment}.
To investigate the impact of relational index exchange (RIE) on the relation consistency loss, we conducted comparative experiments to verify the effect of omitting RIE.
In the absence of RIE, we solely compute the consistency loss for relation categories that are not involved in relation swapping and exclude the swapping component from the calculation of the consistency loss.
The outcomes of this experiment are presented in Tab. \ref{table:rie}, and demonstrate a notable reduction in the mean recall and a decline in the model's performance for relations with relational semantic overlap when RIE is not utilized.


\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccc}
    \hline
        Relation swapping & R/mR@20 & R/mR@50 & R/mR@100 \\
        \hline
        adjacent & \textbf{34.1} / \textbf{23.7} & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        extreme & 33.7 / 21.4 & 39.4 / 27.8 & 41.6 / 30.2 \\
        baseline & 32.6 / 20.9 & 38.0 / 27.4 & 38.9 / 28.4 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Ablation study for relation swapping.
    \textit{Adjacent} is the relation swapping method provided in our paper, \textit{extreme} is a simplified version, and \textit{baseline} is the method that does not use relation swapping.
    }
    \label{table:relation_swapping}
\end{table}


\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc}
    \hline
        Whether to use RIE & R/mR@20 & R/mR@50 & R/mR@100 \\
        \hline
        \checkmark & \textbf{34.1} / \textbf{23.7} & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        - & 33.5 / 22.3 & 40.3 / 29.0 & 42.6 / 32.3 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Ablation study for relation consistency loss in prediction alignment.
    % \holger{Try to make all tables the same width.}
    }
    \label{table:rie}
\end{table}


% Use embedding instead of predict logits for consistency loss calculation.
% In this comparison experiment, paper method works well in the R/mR@20 metric, but for R/mR@50 and R/mR@100, embedding method is better than paper method, so we are not going to show it.


\textbf{HiLo inference fusion}.
In addition to the inference fusion method discussed in the paper, we also attempted to average the tensor generated by the two branches and obtain the PSG result through post-processing.
However, we found that this approach leads to a substantial drop in performance, as evident in Tab. \ref{table:inference_fusion}.
This can be attributed to the inconsistent prediction results of the two branches for the same query index.
These findings validate that the inference fusion method effectively merges the results from the two branches.
% Additionally, our experiments demonstrate the significance of using the triplet query correspondence, which we proposed to construct the alignment of the two branches during prediction alignment.
% \holger{Unclear what the last sentence means.}
%
Furthermore, our experimental results demonstrate that the query associated with the identical index in two branches does not predict the same subject-object pair.
Thus, directly averaging the tensor produced by the two branches results in prediction ambiguity, ultimately leading to a substantial decline in performance.
This observation underscores the necessity of conducting triplet query correspondence when performing prediction alignment.
In particular, due to the inconsistent query prediction content for the corresponding index in the two branches, a one-to-one correspondence must be constructed based on the label assigned by each query to achieve prediction alignment.

\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccc}
    \hline
        Fusion method & R/mR@20 & R/mR@50 & R/mR@100 \\
        \hline
        inference fusion & \textbf{34.1} / \textbf{23.7} & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        average tensor & 19.6 / 13.1 & 23.1 / 15.7 & 23.9 / 16.3 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Ablation study for inference fusion.
    \textit{Inference fusion} refers to the method proposed in the paper to fuse the results of two branches.
    \textit{Average tensor} refers to the fusion method that directly averages the tensor output by the two branches.
    The \textit{average tensor} causes the performance to drop significantly.
    % \miaojing{bad naming}
    }
    \label{table:inference_fusion}
\end{table}



\section{More results on VG-150}
We present an extension of our previous results on scene graph detection (SGDet), the most challenging task in the VG-150 \cite{krishna2017visual} dataset, by including comparative evaluations on two simplified tasks: predicate (relation) classification (PredCls) and scene graph classification (SGCls).
 PredCls involves inferring the relations between objects while assuming prior knowledge of their location and category information.
 On the other hand, SGCls entails the prediction of the category of an object and the relations between objects, given only their location information.
 We provide the detailed results of these tasks in the Tab. \ref{table:vg_main_results_predcls} and Tab. \ref{table:vg_main_results_sgcls}.

 These results once again demonstrate that our method can improve the performance of low-frequency relations while also taking into account the performance of high-frequency relations.
It also shows that the HiLo framework is a general technique that yields systematic improvements in both the panoptic scene graph generation (PSG) and scene graph generation (SGG) tasks.

\begin{table}\small
    \centering
    \begin{tabular}{l|cc}
    \hline
        Method & R/mR@50 & R/mR@100 \\
        \hline
        MOTIF \cite{zellers2018neural} & \textbf{64.0} / 15.2 & \textbf{66.0} / 16.2 \\
        \quad +IETrans \cite{zhang2022fine} & 54.7 / 30.9 & 56.7 / 33.6 \\
        \quad \textbf{+HiLo (ours)} & 53.6 / \textbf{33.6} & 55.5 / \textbf{36.4}  \\
        \hline
        VCTree \cite{tang2019learning} & \textbf{64.5} / 16.3 & \textbf{66.5} / 17.7 \\
        \quad +IETrans \cite{zhang2022fine} & 53.0 / 30.3 & 55.0 / 33.9 \\
        \quad \textbf{+HiLo (ours)} & 53.4 / \textbf{34.0} & 55.2 / \textbf{37.8} \\
        \hline
        Transformer \cite{tang2020unbiased} & \textbf{63.6} / 17.9 & \textbf{65.7} / 19.6 \\
        \quad +IETrans \cite{zhang2022fine} & 51.8 / 30.8 & 53.8 / 34.7 \\
        \quad \textbf{+HiLo (ours)} & 52.9 / \textbf{32.8} & 55.9 / \textbf{36.1} \\
        \hline
        GPSNet \cite{lin2020gps} & \textbf{65.1} / 15.0 & \textbf{66.9} / 16.0 \\
        \quad +IETrans \cite{zhang2022fine} & 52.3 / 31.0 & 54.3 / 34.5 \\
        \quad \textbf{+HiLo (ours)} & 53.3 / \textbf{33.8} & 55.2 / \textbf{37.4} \\
        \hline
    \end{tabular}
    \vspace{+1mm}
    \caption{Comparison between our HiLo framework and other methods on PredCls on the VG-150 dataset. Similar to \cite{zhang2022fine}, we apply IETrans and our own method on top of four leading baselines.
    }
    \label{table:vg_main_results_predcls}
\end{table}


\begin{table}\small
    \centering
    \begin{tabular}{l|cc}
    \hline
        Method & R/mR@50 & R/mR@100 \\
        \hline
        MOTIF \cite{zellers2018neural} & \textbf{38.0} / 8.7 & \textbf{38.9} / 9.3 \\
        \quad +IETrans \cite{zhang2022fine} & 32.5 / 16.8 & 33.4 / 17.9 \\
        \quad \textbf{+HiLo (ours)} & 32.1 / \textbf{18.9} & 33.1 / \textbf{20.9}  \\
        \hline
        VCTree \cite{tang2019learning} & \textbf{39.3} / 8.9 & \textbf{40.2} / 9.5 \\
        \quad +IETrans \cite{zhang2022fine} & 32.9 / 16.5 & 33.8 / 18.1 \\
        \quad \textbf{+HiLo (ours)} & 35.7 / \textbf{21.0} & 36.8 / \textbf{22.7} \\
        \hline
        Transformer \cite{tang2020unbiased} & \textbf{38.1} / 9.9 & \textbf{39.2} / 10.5 \\
        \quad +IETrans \cite{zhang2022fine} & 32.6 / 17.4 & 33.5 / 19.1 \\
        \quad \textbf{+HiLo (ours)} & 32.3 / \textbf{20.1} & 33.3 / \textbf{22.2} \\
        \hline
        GPSNet \cite{lin2020gps} & \textbf{36.9} / 8.2 & \textbf{38.0} / 8.7 \\
        \quad +IETrans \cite{zhang2022fine} & 31.8 / 17.0 & 32.7 / 18.3 \\
        \quad \textbf{+HiLo (ours)} & 31.7 / \textbf{18.3} & 32.5 / \textbf{20.2} \\
        \hline
    \end{tabular}
    \vspace{+1mm}
    \caption{Comparison between our HiLo framework and other methods on SGCls on the VG-150 dataset. Similar to \cite{zhang2022fine}, we apply IETrans and our own method on top of four leading baselines.
    % \miaojing{ours is inferior on GPSNEt}
    % \zijian{We all exceed the baseline and IETrans on mR@K, and are close to IETrans on R@K. And our method is based on the enhanced data of IETrans, so it proves that we can improve mR@K while maintaining R@K.}
    }
    \label{table:vg_main_results_sgcls}
\end{table}

