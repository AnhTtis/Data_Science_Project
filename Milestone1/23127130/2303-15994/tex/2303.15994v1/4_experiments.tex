
\subsection{Datasets}

\noindent \textbf{Panoptic Scene Graph Generation (PSG)~\cite{yang2022panoptic}}.
This is the first Panoptic Scene Graph generation dataset. It has a total of 48,749 labeled images including 2,177 test images and 46,572 training images.
The object categories comprise 80 thing classes and 53 stuff classes, which is the same as the COCO \cite{lin2014microsoft} and COCO-Stuff datasets \cite{caesar2018coco}.
The relation categories comprise 56 classes, including positional relations, common object-object relations, common actions, human actions, actions in the traffic scene, actions in the sports scene and interactions between backgrounds~\cite{yang2022panoptic}. 

\noindent \textbf{Visual Genome (VG)} \cite{krishna2017visual}.
VG is a widely used benchmark dataset for Scene Graph Generation.
Following previous work \cite{zellers2018neural, chen2019counterfactual}, we adopt the widely accepted split, VG-150, which contains 150 object categories and 50 relation categories.
The object categories cover a wide range of classes, such as \textit{animals}, \textit{vehicles} and \textit{household items}.
The relation categories include both spatial and semantic classes, such as \textit{on}, \textit{in} and \textit{wearing}.

\subsection{Tasks and Metrics}
Three subtasks have been proposed for the SGG and PSG tasks, which are \emph{Predicate Classification}, \emph{Scene Graph Classification} and \emph{Scene Graph Detection}~\cite{xu2017scene}.
We focus on Scene Graph Detection for both datasets, since it is the most comprehensive and addressed by \cite{yang2022panoptic}. This subtask
requires the model to first localize the objects and then predict the object classes and relations.
Note that Scene Graph Detection in PSG includes the detection on stuff classes, while in SGG it does not.

Following previous work \cite{tang2019learning, yu2020cogtree, yang2022panoptic}, we use Recall@K (R@K) and mean Recall@K (mR@K) as our metrics, where the former metric is dominated by high-frequency relations, while the latter assigns equal weight to all relation classes.

\subsection{Implementation details}
In our experiments, we follow the training strategy of PSGTR \cite{yang2022panoptic}.
We use the AdamW optimizer \cite{loshchilov2017decoupled}, with a learning rate of $1e^{-4}$ and weight decay of $1e^{-4}$, except for the backbone, which is trained with a learning rate of $1e^{-5}$. For initialization, we use Mask2Former \cite{cheng2022masked} pretrained on COCO \cite{lin2014microsoft} to initialize our backbone and pixel decoder.
Additionally, both the H-L and L-H decoders are initialized with Mask2Former's transformer decoder.
To ensure consistent comparison with PSGTR, we adopted the same data augmentation settings.
Our model is trained for 12 epochs with a step scheduler at epoch 10, taking approximately 18 hours to train on four A100 GPUs with a batch size of 1 for each GPU.

\subsection{Comparison to the state-of-the-art}
\label{sec:experiments:main_results}

\noindent \textbf{PSG}. 
Tab.~\ref{table:psg_main_results} reports the performance of our method compared to the state-of-the-art on the PSG dataset~\cite{yang2022panoptic}.
We separate the methods into two groups. 
The first are two-stage methods consisting of a separate segmentor and relation predictor, which are modified for the PSG task in \cite{yang2022panoptic}.
The second are one-stage end-to-end methods, which are able to simultaneously predict panoptic segmentation and relations.
Our method belongs to the second category.
For a fair comparison between the different methods, we use the same Resnet-50 \cite{he2016deep} backbone.
%
Our method shows superior performance compared to all previous methods.
Particularly, it outperforms the previous best-performing method PSGTR \cite{yang2022panoptic} by a large margin, \ie +6\% in R@100 and +11\% in mR@100.
Our model is able to converge within only 12 epochs of training, whereas PSGTR \cite{yang2022panoptic} is trained for 60 epochs.
%
We also evaluate our method using pre-trained transformer-based backbones, \ie Swin-B and Swin-L~\cite{liu2021swin}, with the latter being a bigger model.
Our results are consistently improved over all metrics due the powerful feature representation ability of the transformer.

\noindent \textbf{SGG}.
Here we study whether our approach that was developed for the PSG task can also be applied to the SGG task.
In Tab.~\ref{table:vg_main_results}, we conduct experiments on the VG-150 dataset.
Following IETrans~\cite{zhang2022fine}, we extend our \emph{unbiased} HiLo framework (Sec.~\ref{sec:framework}) without our PSG-specific baseline (Sec.~\ref{sec:baseline}) to four state-of-the-art \emph{biased} SGG methods, namely MOTIF \cite{zellers2018neural}, VCTree \cite{tang2019learning}, Transformer \cite{tang2020unbiased} and GPSNet \cite{lin2020gps}, using the implementation of \cite{zhang2022fine}.
%
Compared to the unbiased IETrans \cite{zhang2022fine} method, our method improves mean recall without sacrificing recall, demonstrating its effectiveness in enhancing the performance of low-frequency relations.
%
Compared to the biased baselines \cite{zellers2018neural, tang2019learning, tang2020unbiased, lin2020gps}, our method achieves a significantly larger mean recall, while still maintaining an acceptable recall.
%
This indicates that our method can improve the performance of low-frequency relations while also taking into account the performance of high-frequency relations.
%
It also shows that the HiLo framework is a general technique that yields systematic improvements in both the PSG and SGG tasks.

\begin{table}\small
    \centering
    \begin{tabular}{l|cc}
    \hline
        Method & R/mR@50 & R/mR@100 \\
        \hline
        MOTIF \cite{zellers2018neural} & \textbf{31.0} / 6.7 & \textbf{35.1} / 7.7 \\
        \quad +IETrans \cite{zhang2022fine} & 26.4 / 12.4 & 30.6 / 14.9 \\
        \quad \textbf{+HiLo (ours)} & 26.2 / \textbf{14.7} & 30.3 / \textbf{17.7}  \\
        \hline
        VCTree \cite{tang2019learning} & \textbf{30.2} / 6.7 & \textbf{34.6} / 8.0 \\
        \quad +IETrans \cite{zhang2022fine} & 25.4 / 11.5 & 29.3 / 14.0 \\
        \quad \textbf{+HiLo (ours)} & 27.1 / \textbf{12.9} & 29.8 / \textbf{15.2} \\
        \hline
        Transformer \cite{tang2020unbiased} & \textbf{30.0} / 7.4 & \textbf{34.3} / 8.8 \\
        \quad +IETrans \cite{zhang2022fine} & 25.5 / 12.5 & 29.6 / 15.0 \\
        \quad \textbf{+HiLo (ours)} & 25.4 / \textbf{14.6} & 29.2 / \textbf{17.6} \\
        \hline
        GPSNet \cite{lin2020gps} & \textbf{30.3} / 5.9 & \textbf{35.0} / 7.1 \\
        \quad +IETrans \cite{zhang2022fine} & 25.9 / 14.6 & 28.1 / 16.5 \\
        \quad \textbf{+HiLo (ours)} & 25.6 / \textbf{15.8} & 27.9 / \textbf{18.0} \\
        \hline
    \end{tabular}
    \vspace{+1mm}
    \caption{Comparison between our HiLo framework and other methods on the VG-150 dataset. Similar to \cite{zhang2022fine}, we apply IETrans and our own method on top of four leading baselines.
    }
    \label{table:vg_main_results}
\end{table}


\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{visual_results.pdf}
\end{center}
    \vspace{-5mm}
   \caption{Visualization of panoptic segmentations and the top 20 predicted triplets compared with ground truth.
   The upper left is the original image, the lower left is the ground truth and on the right are the predictions.
   The highlighted triplets represent the subject-object pairs with multiple relations, where the blue highlights represent the high frequency relations and the red highlights represents the low frequency relations.
   The visualization results show that our method can predict both high frequency and low frequency relations.
   }
\label{fig:short}
\end{figure*}

\subsection{Ablation Studies}
Consistent with the paper, we use HiLo with a Resnet-50 backbone to perform ablation experiments on the PSG dataset.

\noindent \textbf{HiLo framework for different baselines}.
In this section we investigate whether our HiLo framework~(Sec.~\ref{sec:framework}) yields improvements for other baselines, rather than just the one presented in Sec.~\ref{sec:baseline}.
Tab.~\ref{table:different_baselines} shows the results for two baselines, with and without the HiLo framework.
We observe that our biased baseline outperforms the previous PSGTR~\cite{yang2022panoptic} method on all metrics.
Furthermore, by applying the HiLo framework, we can substantially improve the performance over both baselines.
It is worth mentioning that the HiLo framework improves recall and mean recall simultaneously, whereas other methods typically improve one metric at the cost of the other \cite{tang2020unbiased, yu2020cogtree}.

\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lc|ccc}
    \hline
        Baseline  & HiLo & R/mR@20 & R/mR@50 & R/mR@100 \\
        \hline
        HiLo baseline & \checkmark & \textbf{34.1} / \textbf{23.7} & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        HiLo baseline & - & 32.6 / 20.9 & 38.0 / 27.4 & 38.9 / 28.4 \\
        \hline
        PSGTR~\cite{yang2022panoptic} & \checkmark & \textbf{30.1} / \textbf{20.2} & \textbf{36.6} / \textbf{23.9} & \textbf{38.3} / \textbf{24.5} \\
        PSGTR~\cite{yang2022panoptic} & - & 28.4 / 16.6 & 34.4 / 20.8 & 36.3 / 22.1 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Comparison of different baselines, with and without HiLo framework. Using the HiLo framework, we see significant improvements on both metrics.
    }
    \label{table:different_baselines}
\end{table}

\noindent \textbf{HiLo relation augmentation.}
We observe that out of 260,296 labeled triplets in the PSG dataset, only about 10\% of subject-object pairs have multiple relations, for which we can apply relation swapping~(Sec.~\ref{sec:framework:relation_generation}).
After applying our proposed relation augmentation technique~(Sec.~\ref{sec:framework:relation_generation}), this ratio significantly increases to 40\%.
Our experimental results in Tab.~\ref{table:proportion_of_swappable_triplets} demonstrate that only applying the HiLo framework on 10\% already gives an improvement over the baseline from Tab.~\ref{table:different_baselines}. As the number of swappable triplets increases due to augmentation, the model's performance is further enhanced, highlighting the potential of our method.

\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|ccc}
    \hline
        Relation Aug. & Multiple relations & R/mR@50 & R/mR@100 \\
        \hline
        \checkmark & 40\%  & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        - & 10\%  & 40.1 / 28.1 & 42.8 / 32.5 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Ablation study for HiLo relation augmentation. Relation augmentation affects the ratio of subject-object pairs with multiple relations. The larger this ratio, the more relations can be swapped, which leads to better results.
    }
    \label{table:proportion_of_swappable_triplets}
\end{table}

\noindent \textbf{HiLo prediction alignment}.
We conduct ablation experiments on the subject-object consistency loss and relation consistency loss~(Sec.~\ref{sec:framework:prediction_alignment}), which are used to align the predictions from the high and low frequency branches.
The results, as presented in Tab.~\ref{table:prediction_alignment}, demonstrate that using both losses yields the best performance.
It is worth mentioning that we have explored the margin in the relation consistency loss and found that setting the margin to zero leads to a small performance degradation.
This finding confirms that there is a partial semantic overlap between swapped relations, indicating that they are not entirely consistent.

\begin{table}\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc|ccc}
    \hline
        Object & Relation & Margin & R/mR@50 & R/mR@100 \\
        \hline
        \checkmark & \checkmark & 0.5 & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        - & \checkmark & 0.5 & 40.6 / 29.7 & 42.8 / 32.8 \\
        - & \checkmark & 0.0 &  40.5 / 29.5 & 42.7 / 32.8 \\
        \checkmark & - & - &  40.4 / 29.0 & 42.8 / 32.2 \\
        - & - & - & 39.7 / 28.6 & 42.4 / 32.0 \\
        \hline
    \end{tabular}
    }
    \vspace{+1mm}
    \caption{Ablation study for different losses in HiLo prediction alignment. \emph{Object} refers to the subject-object consistency loss and \emph{relation} refers to the relation consistency loss. The margin parameter is defined in Eq.~\ref{eq:relation_consistency_loss}.
    }
    \label{table:prediction_alignment}
\end{table}

\noindent \textbf{HiLo inference fusion}.
We ablate the inference fusion~(Sec.~\ref{sec:framework:inference_fusion}) and evaluate the performance of each branch's output separately.
In Tab.~\ref{table:inference_fusion}, experimental results suggest that fusion can effectively leverage the uniqueness of the high and low frequency branch predictions to achieve comprehensive improvements on all metrics.

\begin{table}\small
    \centering
    \begin{tabular}{cc|ccc}
    \hline
        H-L Result & L-H Result & R/mR@50 & R/mR@100 \\
        \hline
        \checkmark & \checkmark & \textbf{40.7} / \textbf{30.3} & \textbf{43.0} / \textbf{33.1} \\
        \checkmark & - & 38.8 / 29.9 & 39.8 / 30.9 \\
        - & \checkmark & 38.5 / 26.5 & 39.5 / 27.8 \\
        \hline
    \end{tabular}
    \vspace{+1mm}
    \caption{Ablation study for HiLo inference fusion.}
    \label{table:inference_fusion}
\end{table}

\vspace{+4mm}
\noindent \textbf{Convergence speed analysis}.
We evaluate the convergence of our model by assessing its performance on the validation set at various epochs, as illustrated in Fig.~\ref{figure:convergence_speed}.
Our analysis reveals that our proposed method outperforms prior methods \cite{yang2022panoptic} both in terms of final performance and convergence speed.
Specifically, PSGTR \cite{yang2022panoptic} achieves negligible performance in the initial 12 epochs, requiring 60 epochs to converge, as per the authors.
In contrast, our HiLo method achieves better results in just 12 epochs, indicating its superior convergence speed.

\begin{figure}
\begin{center}
\includegraphics[width=\columnwidth]{convergence_speed.pdf}
\end{center}
\vspace{-2mm}
\caption{Training convergence speed analysis of different methods. 
Our method converges significantly faster than previous methods.}
\vspace{-2mm}
\label{figure:convergence_speed}
\end{figure}


\noindent \textbf{Masked relation attention}.
We investigate the impact of different mask input types for cross-attention on the HiLo baseline~(Sec.~\ref{sec:baseline}) performance.
Specifically, we compare two different attention focus regions, namely the subject-object region and the full image.
The results are shown in Tab.~\ref{table:masked_relation_attention}.
Focusing on the full image presents a more challenging optimization task for the model since no target region is specified.
Consequently, we observe a drop of 1.8\% in R@100 and 1.6\% in mR@100.
This shows that it is crucial to apply masked relation attention to the subject and object.

\begin{table}\small
    \centering
    \begin{tabular}{l|ccc}
    \hline
        Attention focus & R/mR@20 & R/mR@50 & R/mR@100 \\
        \hline
        subject-object & \textbf{32.6} / \textbf{20.9} & \textbf{38.0} / \textbf{27.4} & \textbf{38.9} / \textbf{28.4} \\
        full image & 30.4 / 19.3 & 36.5 / 25.9 & 37.1 / 26.8 \\
        \hline
    \end{tabular}
    \vspace{+1mm}
    \caption{Ablation study for masked relation attention.
    }
    \label{table:masked_relation_attention}
\end{table}
