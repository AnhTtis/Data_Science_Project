\subsection{Problem Setting}
The Panoptic Scene Graph generation (PSG) task aims to generate a panoptic scene graph $\mathcal G$ for a given image $\mathcal{I} \in \mathbb{R}^{H \times W \times 3}$, where $\mathcal G$ contains an object set $\mathcal O$ and a relational triplet set $\mathcal T$, 
denoted by $\mathcal{G} = (\mathcal{O}, \mathcal{T})$.
For the $i$-th object $o_i$ in $\mathcal O = \{o_i\}_{i=1}^{N}$, we use 
 $m_i$ to represent the object's mask and   $c_i$  the object's category, \ie $o_i= (m_i, c_i)$. For the $j$-th triplet  in $\mathcal T = \{t_j\}_{j=1}^{M}$, we use $s_j$  to represent the subject, $o_j$ the object, and $r_j$ their relation, \ie $t_j = (s_j, o_j, r_j)$. There are in total $C$ object categories and $R$ relation categories. For the $k$-th relation, we denote its frequency in the training set as  $f_k$. 

\subsection{HiLo Baseline}
\label{sec:baseline}
High-quality panoptic segmentation is crucial for achieving good PSG performance.
We build our method upon the latest advances in DETR-based panoptic segmentation, Mask2Former \cite{cheng2022masked}.
Below we present its structure, as well as our proposed modifications for the PSG task.  

\noindent \textbf{Mask2Former}.
This method comprises three key parts: A backbone (CNN-based or transformer-based) followed by a pixel decoder, a transformer module with a transformer decoder and a task-specific module with different task heads. 
Specifically, the backbone takes an input image $\mathcal I$ and generates an image feature $F$.
The pixel decoder then gradually upsamples $F$ to produce multi-scale features $\mathcal{\widetilde F} = \{\widetilde F_i\}_{i=1}^4$.
The transformer decoder takes a set of queries $\mathcal {Q}$ of size $N$ and multi-scale features $\mathcal{\widetilde F}$ as input and outputs a set of mask features $\mathcal X$ of the same size with $\mathcal{Q}$.

On top of the transformer decoder, there are two task heads including a linear classifier that predicts the class probability for each mask; and a multi-layer perceptron (MLP) that uses the mask features $\mathcal X$ to  generate the mask embedding $\mathcal{E}$.
The mask prediction is obtained by taking the dot product of the mask embedding $\mathcal{E}$ with the scale feature of the highest resolution in the multi-scale features $\mathcal{\widetilde F}$.

\noindent \textbf{Triplet queries}.
The original query in Mask2Former is to predict the object. In order to predict both subject, object and their relation, we develop the triplet queries $\mathcal{Q}^{t}$ inspired by \cite{yang2022panoptic} into our baseline.
Each query predicts a triplet that includes subject, object, and relation.
Accordingly, our task heads comprise three linear classifiers. Each classifier is responsible for predicting the class probability for subject, object or relation, respectively. 
Moreover, we devise two MLPs to generate mask embeddings for the subject and object, denoted by $\mathcal{E}^s$ and $\mathcal{E}^o$. We use them to compute the dot product with the scale feature of the highest resolution in multi-scale feature $\mathcal{\widetilde F}$ and obtain the mask prediction for the subject and object, respectively.

\noindent \textbf{Masked relation attention}.
\cite{cheng2022masked} proposes a transformer decoder variant with masked attention, which extracts spatial features by adding the predicted mask of the object from the previous decoder layer's mask prediction.
It makes the model focus on the object-related area in the feature map.
To adapt this scheme to the PSG task, for each relation, we extend the masked attention to take the union of the binary masks of the subject and object as input, which represents the pixels corresponding to the relation.

\noindent \textbf{Network training}.
We adopt the same losses as PSGTR~\cite{yang2022panoptic}, including cross-entropy loss $\mathcal{L}_{so\_cls}$ for object classification of subject-object pairs and a combination $\mathcal{L}_{so\_mask}$ of focal loss \cite{lin2017focal} and dice loss \cite{sudre2017generalised} to jointly supervise mask learning.
To supervise the relations, we use the cross-entropy loss $\mathcal{L}_{rel\_cls}$.
The baseline loss with (${\lambda}_{1} = 1$, ${\lambda}_{2} = 1$, ${\lambda}_{3} = 4$) is thus:
\vspace{-2mm}
\begin{align}
\mathcal{L}_{baseline} = {\lambda}_{1} \cdot \mathcal{L}_{so\_cls} + {\lambda}_{2} \cdot \mathcal{L}_{so\_mask} + {\lambda}_{3} \cdot \mathcal{L}_{rel\_cls}
\end{align}
\vspace{-2mm}

\subsection{HiLo Framework}
\label{sec:framework}

The key insight of our HiLo is to build a model that can take into account both high frequency and low frequency relations, and effectively improve the performance on low frequency relations without degrading the performance on high frequency relations.

\subsubsection{HiLo relation generation}
\label{sec:framework:relation_generation}
In the PSG task, multiple relations can be used to describe the connection between a subject and an object from different perspectives, such as \emph{spatial} relations, \emph{actions} and \emph{prepositions}. Since the subject, object and their relative position in the image are fixed, all these relations share the same visual information.
%
This is reflected in the PSG dataset~\cite{yang2022panoptic} where many subject-object pairs are annotated with multiple relation labels. These relation labels are of different frequencies in the dataset. In this section we introduce the HiLo relation generation module to prepare two sets of training data, biasing towards high and low-frequency relations respectively.

\noindent \textbf{Relation augmentation}. 
Similar to~\cite{goel2022not}, we observe that there are many missing relation annotations in the PSG dataset~\cite{yang2022panoptic}.
To add the missing relation annotations, we design a relation augmentation scheme that is inspired by IETrans \cite{zhang2022fine}, which converts high-frequency relations to low-frequency relations and adds a relation to a subject-object pair that has no relation.
%
We adapt it by first training our baseline as a biased model using the original annotated data. For every subject-object pair in the training set, we use this model to predict the relation scores for all predefined relation categories: 
\begin{compactitem}
    \item If this subject-object pair has annotated relation labels, we pick the one with the highest predicted score and use the score as a threshold. For other relations (except for already labeled ones) whose predicted scores are greater than the threshold, we add them as the relation labels of this subject-object pair.
    \item If this subject-object pair has no annotated relation labels, we use the predicted score of the \emph{no-relation} class as a threshold. For other relations whose predicted scores are greater than this threshold we add them as the relation labels of this subject-object pair.
\end{compactitem}
This operation allows us to significantly augment the relation labels for the subject-object pairs in the training data, which is especially relevant for pairs with zero or no annotated relations. 

\noindent \textbf{Relation swapping}.
We swap the relation labels for each subject-object pair. 
Specifically, given a subject-object pair $(s, o)$ with $K$ relation labels, we have $K$ triplets $(s, o, r_1),...,(s, o, r_K)$, sorted by their relation frequency in descending order, $f_1 >... > f_K$. We denote by H-L and L-H the swapping of high-frequency relations with low-frequency relations and vice versa. This creates two sets of data: 

\emph{H-L Data.}
Given a triplet ($s, o, r_k$), we replace its relation label $r_k$ with that of the next triplet $r_{k+1}$ with lower frequency. We sequentially process all triplets from ($s, o, r_1$) until ($s, o, r_{K-1}$), keeping the last triplet unchanged. 

\emph{L-H Data.}
Given a triplet ($s, o, r_k$), we replace its relation label $r_k$ with that of the previous triplet $r_{k-1}$ with higher frequency. 
 We sequentially process all triplets from ($s, o, r_K$) until ($s, o, r_2$), keeping the first triplet unchanged.
%
Hence, we obtain two new sets of triplets, denoted by $\mathcal T^\text{H-L}$ and $\mathcal T^\text{L-H}$.  We devise two parallel decoders from the shared encoder of our backbone. They are learned with the H-L and L-H data, respectively.
%
The H-L and L-H decoders favour the predictions for low and high-frequency relations, respectively. Despite their difference, for the corresponding triplet query in the two decoders, their predictions are highly correlated: on one hand, the subject and object predictions should be the same; on the other hand, the distribution of relation predictions should be overlapping.  Below we first build the query correspondence in the two decoders and then introduce the HiLo subject-object and relation consistency loss to align the predictions from two the decoders. 

\subsubsection{HiLo prediction alignment}
\label{sec:framework:prediction_alignment}
Training two different relation distributions simultaneously confuse the model.
To make the model differentiate between the two branches, we built a HiLo prediction alignment module, including triplet query correspondence, subject-object consistency loss and relation consistency loss respectively.

\noindent \textbf{Triplet query correspondence}.
In order to construct the subject-object consistency loss and relation consistency loss, we first need to construct the correspondence between the triplet queries in the H-L and L-H decoders.
%
The same query index in the two decoders is not naturally matched.  In order to find the query correspondence, we need to rely on the ground truth assignment: we use Hungarian matching to assign the triplet label to the corresponding triplet query, and record the relation label index corresponding to the triplet query.
This label index allows us to construct the correspondence between the triplet queries in both decoders.
%
We calculate the consistency loss for the triplet query prediction with the same relation label index in the two decoders.

\noindent \textbf{Subject-object consistency loss}.
Having the corresponding predictions from the two decoders, both their subjects and objects have the same ground truth and should be equal. We propose a subject-object consistency loss $\mathcal L_{obj}$ to minimize the mean squared error (MSE) of the corresponding predictions from the two decoders. 

\vspace{-4mm}
\begin{align}
\mathcal L_{obj} &= \mathcal L_{cls} + \mathcal L_{mask} \\
\mathcal L_{cls} &= \lVert softmax(p_{c}^\text{H-L}) - softmax(p_{c}^\text{L-H}) \rVert^2 \\
\mathcal L_{mask} &= \lVert sigmoid(p_{m}^\text{H-L}) - sigmoid(p_{m}^\text{L-H}) \rVert^2
\end{align}
Here $\mathcal L_{cls}$ and $\mathcal L_{mask}$ represent the MSE losses for class prediction and mask prediction, respectively.

$p_{c}^\text{H-L}$ and $p_{c}^\text{L-H}$ are the class prediction logits from the H-L and L-H decoders. After a softmax, they are $C$-dimensional probability vectors. After a sigmoid, $p_{m}^{H-L}$ and $p_{m}^{L-H}$ are the mask prediction logits for the H-L and L-H decoders.  

\noindent \textbf{Relation consistency loss}.
Given a pair of subject and object, 
we have previously swapped the high-low frequency relation labels to create data for the H-L and L-H decoders. For H-L, the prediction of the low-frequency relation logit is of high value; while for L-H, the prediction of the high-frequency relation logit is of high value. For the predictions on the rest logits, they should be similar, since it is the same pair of subject and object for the two decoders. Based on this observation, we introduce the relation consistency loss.

Specifically, for a pair of subject and object, we use $p_{r}^\text{H-L}$ to denote the predicted relation logits from  the H-L decoder, $p_{r}^\text{L-H}$ the predicted relation logits from the L-H decoder. They are $R$-dimensional probability vectors after softmax. We can map between the distributions of the two vectors by swapping the logit values between the high- and low-frequency relation indices, which is named Relational Index Exchange operation.
We use $\text{RIE}(\cdot)$ to denote this operation.
$\text{RIE}(\cdot)$ includes a stop gradient operation, which creates copies from original predictions of two branches, enabling value exchanges.
For example, for relations $r_k$ and $r_{k+1}$, $\text{RIE}(\cdot)$ exchanges the values between $p_{r,k}$ and $p_{r,{k+1}}$ in $p_r$. We can therefore compute the distance between $p_{r}^\text{H-L}$ and its mapped  counterpart from $p_{r}^\text{L-H}$,  vice versa: 

\vspace{-4mm}
\begin{align}
\begin{split}
\text{Dist}_\text{HiLo} = &\lVert softmax(p_{r}^\text{H-L}) - \mathit{RIE}(softmax(p_{r}^\text{L-H})) \rVert ^ 2 \\
+ &\lVert \mathit{RIE}(softmax(p_{r}^\text{H-L})) - softmax(p_{r}^\text{L-H}) \rVert ^ 2
\end{split}
\end{align}
The relation consistency loss is defined to minimize $\text{Dist}_\text{HiLo}$ with a margin of $m$:
\vspace{-2mm}
\begin{align}
\label{eq:relation_consistency_loss}
\mathcal L_{rel} = max(\text{Dist}_\text{HiLo}- m, 0),
\end{align}
where $m$ is a small constant.  Adding $m$ is due to the fact that the high- and low frequency relations might be only partially semantically overlapping.    

\noindent \textbf{Network training}.
Our subject-object consistency loss and relation consistency loss can seamlessly integrate with the losses of any baseline method to jointly supervise the training of the entire model. Notably, we supervise the output of each transformer decoder layer to ensure effective learning. 
The final loss $\mathcal{L}$ is thus: 
\begin{align}
\mathcal{L} = \mathcal{L}_{baseline} + \mathcal{L}_{obj} + \mathcal{L}_{rel} \;
\end{align}

\begin{table*}[h!]\normalsize
    \centering
    \vspace{-4mm}
    \begin{tabular}{lc|cc|cc|cc}
    \hline
        ~ & ~ & \multicolumn{6}{c}{Scene Graph Detection} \\
        \cline{3-8}
        Method & Backbone & R@20 & mR@20 & R@50 & mR@50 & R@100 & mR@100 \\
        \hline
        IMP \cite{xu2017scene} & R50 & 16.5 & 6.5 & 18.2 & 7.1 & 18.6 & 7.2 \\
        MOTIF \cite{zellers2018neural} & R50 & 20.0 & 9.1 & 21.7 & 9.6 & 22.0 & 9.7 \\
        VCTree \cite{tang2019learning} & R50 & 20.6 & 9.7 & 22.1 & 10.2 & 22.5 & 10.2 \\
        GPSNet \cite{lin2020gps} & R50 & 17.8 & 7.0 & 19.6 & 7.5 & 20.1 & 7.7 \\
        \hline
        PSGTR \cite{yang2022panoptic} & R50 & 28.4 & 16.6 & 34.4 & 20.8 & 36.3 & 22.1 \\
        PSGFormer \cite{yang2022panoptic} & R50 & 18.0 & 14.8 & 19.6 & 17.0 & 20.1 & 17.6 \\
        \textbf{HiLo (ours)} & R50 & 34.1 & 23.7 & 40.7 & 30.3 & 43.0 & 33.1 \\
        \textbf{HiLo (ours)} & Swin-B & 38.5 & 28.3 & 46.2 & 35.3 & 49.6 & 39.1 \\
        \textbf{HiLo (ours)} & Swin-L & \textbf{40.6} & \textbf{29.7} & \textbf{48.7} & \textbf{37.6} & \textbf{51.4} & \textbf{40.9} \\
        \hline
    \end{tabular}
    \vspace{+1mm}
    \caption{Comparison between our HiLo and other methods on the PSG dataset.
    Our method shows superior performance compared to all previous methods.
    }
    \vspace{-2mm}
    \label{table:psg_main_results}
\end{table*}

\subsubsection{HiLo inference fusion}
\label{sec:framework:inference_fusion}
The H-L and L-H decoders favour low-frequency relation prediction and high-frequency relation prediction, respectively. 
To combine the strength of both during inference, we introduce the HiLo inference fusion module.
Specifically, we denote by $\mathcal G^\text{H-L}$ and $\mathcal G^\text{L-H}$ the predicted panoptic scene graphs from H-L and L-H decoders, respectively.
There are $N_{1}$ triplets in $\mathcal G^\text{H-L}$ and $N_{2}$ triplets in $\mathcal G^\text{L-H}$.

\begin{compactitem}
\item  First, we merge the triplets in $\mathcal G^\text{H-L}$ and $\mathcal G^\text{L-H}$ and sort them in the descending order according to their relation scores. We obtain a list of $N_{1} + N_{2}$ triplets.  

\item Second, starting from the first triplet, we de-duplicate the triplet list. For the $i$-th triplet $\mathcal T_i$, we identify its duplicated versions from the $(i+1)$-th triplet until the end of the list.  
Remove any follow-up triplet from the list if it 1) has the same subject, object and relation classes to that in $T_i$ and 2) has a mask IoU greater than a threshold, \eg 0.5, between the predicted subject/object and the corresponding subject/object in $T_i$.

\item Third, after deduplication, for each triplet in the list, we multiply the relation, subject and object scores as an overall score for it. We sort the  triplet list according to this score in descending order to obtain the final panoptic scene graph $\mathcal G^\text{HiLo}$.

\end{compactitem}
