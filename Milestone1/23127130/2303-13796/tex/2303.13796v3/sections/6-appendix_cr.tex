% \clearpage
% \newpage
\appendix
% \onecolumn


% PDHuman Dataset consists of two parts: synthetic dataset and real dataset. The synthetic dataset contains training and testing set, and real dataset are for testing.


\section{Details of Perspective-distorted Datasets.}
\subsection{PDHuman}
\label{chapter:pdhuman-syn}
Our pipeline is inspired by recent works on synthetic data~\cite{Patel:CVPR:2021:AGORA,hspace,varol17_surreal}. 
A photogrammetry-scanned human model with a unique body pose will be rendered with a random viewpoint in an HDRi background. The detailed statistics of PDHuman are illustrated in \cref{tab:pdhuman_stat}.

\begin{table}[htp]
    \centering
    \scalebox{0.75}{
    \input{tables/supp_pdhuman_info}}
    \caption{Statistical information of PDHuman. $\tau$ denotes maximum diisortion scale in the main text.}
    \label{tab:pdhuman_stat}
    \vspace{-5pt}
\end{table}


\noindent\textbf{Human model.}
We use a corpus of $630$ photogrammetry-scanned human models from Renderpeople~\cite{renderpeople},
with well-fitted SMPL parameters. 
Initially, the body pose is sampled from a collection of high-quality motion sequences obtained from
% Mixamo\footnote[3]{\url{https://www.mixamo.com/}},
Mixamo~\cite{mixamo}.
The the pose is converted to SMPL skeleton using a re-targeting approach.
%We export the vertices from Blender for each frame to get the optimized SMPL parameters.
Finally, we use a SGD optimizer to optimize the chamfer distance between the SMPL vertices and RenderPeople vertices to refine the pose and shape parameter.

\noindent\textbf{Camera.}
% To cover most of the scenarios in real life,  
% we randomly sample a perspective camera 
% with a focal length ranging from \SI{7}{\mm} to \SI{102}{\mm}.
% Then the body model is placed in the center of a sphere 
% with a random radius relying on the focal length of the camera,
% and the camera is placed on the surface of the same sphere
% in random elevation and azimuth angle while facing the center.
In order to simulate a wide range of real-world scenarios, a perspective camera is randomly sampled with a focal length that spans from 7mm to 102mm. The corresponding FoV angle is from 10\degree to 140\degree.
The human mesh is then positioned at the center of a sphere, whose radius is chosen randomly and dependent on the camera's focal length.
The camera, facing the center of the sphere, is then placed on the surface of the sphere with a randomized elevation and azimuth angle.

\noindent\textbf{Rendering pipeline.} 
To increase the diversity of data, each frame contains ambient lighting calculated by path tracing in Blender~\cite{blender} and diverse background generated by HDRi images from PolyHaven~\cite{polyhaven}. The size of all rendered images is $512\times512$.


% We use  based on $500$ HDRi images obtained from PolyHaven \cite{polyhaven},
% and Blender \cite{blender} with path tracing (cycles) are utilized to render photo-realistic images.
%
% In addition, we use the SMPL~\cite{smpl} parameters and camera matrices to render distortion maps and IUV images by Pytorch3D~\cite{pytorch3d}.


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{pictures/supp_pdhuman.pdf}
    \vspace{-5pt}
    \caption{More examples from PDHuman dataset.}
    \label{fig:pdhuman_demo}
    \vspace{-5pt}
\end{figure}

\subsection{SPEC-MTP}
\label{chapter:spec-mtp}

\begin{table}
    \centering
    \scalebox{0.8}{\input{tables/supp_spec_mtp_info}}
    \caption{Statistical information of SPEC-MTP~\cite{spec}. $\tau$ denotes the maximum distortion scale in the main text.}
    \label{tab:spec_mtp_stat}
    \vspace{-5pt}
\end{table}

SPEC-MTP~\cite{spec} is a real-world image dataset with calibrated focal lengths and well-fitted SMPL parameters (including $\theta$, $\beta$), and translation. The images were taken at relatively close-up distances, leading to noticeable perspective distortion in the limbs and torsos of subjects. We use it as one of the evaluation datasets for our task. The detailed statistics of SPEC-MTP are illustrated in \cref{tab:spec_mtp_stat}.

% The dataset contains $5,356$ images, and is used as an evaluation dataset in our task.
% $4,212$ of the images have the shape of $1920\times1080$ and 1144 of the images have the shape of $1280\times720$.
% This dataset is used as an evaluation dataset in our task. 
% , as shown in \cref{fig:supp_specmtp_sota}

% When utilizing SPEC-MTP, it is important to consider that the dataset has several limitations. (1). The limited diversity of human individuals and background environments in the dataset may affect its generalizability to other datasets. (2). The dataset restricts poses as subjects were asked to mimic a template pose, which may restrict its applicability in certain research domains. (3). The inaccurate calibration of images could lead to sub-optimal estimations of body pose and shape parameters, which may affect the output of image processing applications.
\subsection{HuMMan}
The HuMMan dataset, as proposed in \cite{humman}, is a real-world image dataset that utilizes 10 calibrated RGBD kinematic cameras to capture shots for each frame. From these shots, segmented point clouds are extracted from depth images, resulting in a comprehensive dataset. The SMPL parameters were registered upon triangulated 3D keypoints and point clouds, providing ground-truth data that is highly useful for mocap tasks. We reshape all the images to $360\times 640$ pixels. The detailed statistics of HuMMan are illustrated in~\cref{tab:humman_stat}.

% It is also important to consider the limitations of the dataset which include a limited actor diversity, with all poses being predefined for actors to mimic, and each camera having similar focal lengths. Additionally, the capture environment is only limited to one indoor laboratory, which could impact the generalization of the dataset's findings to other environments.

\begin{table}
    \centering
    \scalebox{0.75}{
\input{tables/supp_humman_info}}
\caption{Statistical information of HuMMan~\cite{humman}. $\tau$ denotes the maximum distortion scale in the main text.}
\label{tab:humman_stat}
\vspace{-5pt}
\end{table}

\section{Analysis of 3DPW dataset}
We divide the 3DPW dataset into three protocols based on the maximum distortion scale $\tau$ in~\cref{tab:supp_pw3d_info}. We report the results of our re-implemented HMR-R50~\cite{hmr} and \Ours-H48 on each protocol in \cref{tab:supp_pw3d}.
%The results reveal that \Ours achieves larger improvement as the distortion scale becomes bigger. This phenomenon demonstrates that \Ours's improvement on 3DPW is largely attributed to the improvement in distorted images.
The experiments indicate that \Ours outperforms HMR-R50, and this improvement is more pronounced as the distortion scale increases.
This observation serves as compelling evidence that \Ours's success on 3DPW can be primarily attributed to its superior performance on distorted images.
%
% We also present qualitative results in \cref{fig:demo_pw3d}.
%
% Interestingly, CLIFF~\cite{cliff} adjusted the focal length of its camera model to match the 3DPW dataset, which could have resulted in exploiting the properties of 3DPW to achieve higher performance.

\begin{table}[ht]
    \centering
    \scalebox{0.75}{\input{tables/supp_3dpw_info}}
    \caption{3 protocols of 3DPW divided by $\tau$. The larger the value of $\tau$, the greater the degree of distortion.}
    \label{tab:supp_pw3d_info}
\end{table}


\begin{table}
    \centering
    \scalebox{0.75}{
    \input{tables/supp_pw3d}}
    \caption{Reults of our re-implemented HMR~\cite{hmr} and \Ours-H48 on different protocols of 3DPW. Mainly for showing the correlation between performance improvement and distortion.}
    \label{tab:supp_pw3d}
\end{table}


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.95\linewidth]{pictures/supp_pw3d.pdf}
%     \caption{Qualitative results for 3DPW. \Ours achieves good alignment with the characters in the original image, but other SOTA methods have difficulty aligning images that suffer from distortion caused by overhead shots, which causes upper body dilation and lower body shrinkage. 
%     The number under each image represents predicted/ground-truth $f$, FoV angle, and $T_{z}$. }
%     \label{fig:demo_pw3d}
% \end{figure}



\section{Details of Model-based Variant of \Ours}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{pictures/pipeline-p.pdf}

    \caption{\textbf{\Oursp pipeline overview}. Compared to \Ours, the main difference is the reformulated mesh reconstruction module.}
    \label{fig:pipeline_p}
\end{figure*}
As shown in~\cref{fig:pipeline_p}, we introduce a model-based \Oursp by changing the mesh reconstruction module. Different from \Ours, we regress SMPL parameters rather than 3D vertex coordinates through a transformer decoder in \Oursp. We warp the grid Feature $F_{grid}$ into UV space ($F_{grid-w}$) via $I_{IUV}$ to eliminate the spatial distortion of each part of the features, then concatenate the warped distortion feature $F_{grid-w}$ and regress SMPL parameters from it. We represent the 24 rotations of joints $\theta$ and body shape parameters $\beta$ as 25 learnable tokens. The translation estimation module and supervision are exactly the same as \Ours.



\section{More about Cameras}
\noindent\textbf{Affine Transformation.}
Our affine transformation of translation is the same as SPEC~\cite{spec}.
$T_{x}, T_{y}$ and $t_{x},t_{y}$ should satisfy the following equation for every $x, y$ by connecting the re-projected coordinates in the cropped image coordinate system and original image coordinate system in screen space.
\begin{equation}
\small
\begin{bmatrix}  
\frac{w}{2}\left[ s_{x}(x+t_{x})) +1\right]+c_{x}-\frac{w}{2}\\[6pt]
\frac{h}{2}\left[ s_{y}(y+t_{y})) +1\right]+c_{y}-\frac{h}{2}
  \end{bmatrix}=
\begin{bmatrix}\frac{W}{2}\left[S_{W}(x+T_{x}) + 1\right]\\[6pt]
\frac{H}{2}\left[ S_{H}(y+T_{y}) + 1\right] \end{bmatrix}.
\end{equation}
where $S_{W}=s_{x}/(\frac{W}{w})$, $S_{H}=s_{y}/(\frac{H}{h})$, so we can get the transform by:

\begin{equation}
\small
\begin{bmatrix}
T_{x}\\ 
T_{y}\\ 
1
\end{bmatrix}=\begin{bmatrix}
 1& 0 & (2c_{x}-W)/ws_{x}\\ 
 &1  & (2c_{y}-H)/hs_{y}\\ 
 &  & 1
\end{bmatrix}\begin{bmatrix}
t_{x}\\ 
t_{y}\\ 
1
\end{bmatrix}
\end{equation}
$c_{x}, c_{y}$ terms the bounding-box center coordinate in original image. $h, w$ terms the cropped image size, where $H, W$ terms original image size. During training, we will expand every bounding box of the human body to a square and resize the cropped image to $224\times224$ pixels.

\noindent\textbf{Analysis of dolly zoom.}
%A dolly zoom is an in-camera effect where you dolly towards or away from a subject while zooming in the opposite direction.
The dolly zoom is an optical effect performed in-camera, whereby the camera moves towards or away from a subject while simultaneously zooming in the opposite direction.
It was first proposed in the film JAW~\cite{jaws1975}. In this section, we simulate the effect on CMU-MOSH~\cite{mosh} data.
%
First, we get all the vertices in CMU-MOSH~\cite{mosh} by feeding the SMPL~\cite{smpl} parameters to the body model. 
We further obtain 3D joints by multiplying the joint regressor matrix to the vertices. 
%
%Last, we fix the weak-perspective camera parameters $(s, t_{x}, t_{y})$ and adjust the distance. This results in approximating fixed human body location and size, with increased distortion as the camera gets closer.
We then apply weak-perspective camera parameters $(s, t_{x}, t_{y})$ while adjusting the distance to approximate the human body's location and size, producing increased distortion as the camera approaches.
% With this approach, we can calculate the focal length as $f=shT_{z}/2$. 
The weak-perspectively projected 2D joints are $s(x+t_{x}, y+t_{y})$, where $x, y$ are the corresponding 3D joint coordinates. We set the image height to 224 pixels and re-project the 2D joints and compare the error between the weak-perspective and perspective projection results.
As shown in \cref{tab:supp_dolly}, when the subject is located over 4 meters away, the re-projected error is only 1.76 pixels, which is negligible on a 224-pixel image. When the subject is further than 8 meters, the error is less than 1 pixel, indicating non-distortion of the images. 

% This demonstrates why and how we manage the settings of our camera model.


 \begin{table}
    \centering
    \scalebox{0.7}{\input{tables/supp_dollyzoom}}
    \caption{Distortion and re-projection error caused by distance in CMU-MOSH~\cite{mosh}. The re-projected error is measured in pixels.}
    \label{tab:supp_dolly}
    \vspace{-10pt}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{pictures/supp_dollyzoom.pdf}
    \caption{Distortion and re-projection error caused by distance. The vertical axis is measured in pixels, and the horizontal axis is measured in meters.}
    \label{fig:supp_dolly}
    \vspace{-5pt}
\end{figure}  

\section{More quantitative results}

% \subsection{Ablation Studies.}

% \input{tables/ablation_network}

% \input{tables/ablation_3dpw}
% 

% This study evaluates the effectiveness of the distortion feature and the hybrid re-projection loss function. The evaluation is conducted on the PDHuman ($\tau=3.0$), as this exhibits the highest degree of distortion. More experimental results are provided in the Sup. Mat.

% \noindent\textbf{Ablation on distortion feature.}
% In \cref{tab:ablation}, w/o $w(I_{d})$ terms without warp distortion image into UV space, and w/o $c(F_{d})$ terms without concatenating distortion feature to per-vertex feature. We can see that, while the mIoU and P-mIoU change a little, the 3D metrics increase significantly with the correct distortion feature. This study validates our intuition that distortion information helps the network predict more accurate vertex coordinates.

% \noindent\textbf{Ablation on the hybrid re-projection loss function.}
% We experimented with different re-projection loss configurations and found that relying solely on weak-perspective loss significantly decreases 2D alignment. Incorporating perspective loss improved 3D metrics slightly but increased the 2D segmentation error significantly. Moreover, using per-joint distortion weight to supervise the weak-perspective camera improved the alignment of the human mesh and resulted in more accurate 3D supervision without increasing the 2D segmentation error. 


% \noindent\textbf{Ablation on training settings on the standard benchmark 3DPW.}
% In Table~\ref{tab:abl_pw3d}, we present the results of our ablation study on the standard 3DPW benchmark~\cite{3dpw}, where we investigate the impact of different training settings on the performance of our method. By controlling two different variables, we show that introducing perspective-distorted datasets and fine-tuning with ground-truth focal length both lead to a slight improvement in performance. Notably, our method \Ours-H48 still outperforms the current state-of-the-art methods even without using perspective-distorted data or ground-truth focal length.



% \noindent\textbf{Results on standard benchmark Human3.6M:}
% During training, we get the ground-truth focal length and translation from Human3.6M~\cite{h36m} training set for our supervision. When evaluating Human3.6M, we follow HybrIK~\cite{hybrik} by using SMPL joints as the ground truth for evaluation. As shown in~\cref{tab:h36m}, our method performs well on Human3.6M through it is not a perspective-distorted dataset. \Ours-H48 achieves the best result on the PA-MPJPE metric and achieves comparable results on the MPJPE metric. CLIFF achieves the best results on MPJPE while they also need ground-truth bounding boxes during testing.

% \begin{table}[ht]
%     \centering
%     \scalebox{0.75}{\input{tables/supp_sota_h36m}}
%     \vspace{-0.05in}
%     \caption{Results of SOTA methods on Human3.6M~\cite{h36m}.
%     }
%     \vspace{-2ex}
%  %   \vspace{-1ex}
%     \label{tab:h36m}
% \end{table}


\noindent\textbf{Full results on PDHuman:} As shown in~\cref{tab:sota_supp_pdhuman} and \cref{tab:sota_supp_pdhuman2}, we report results on all 5 protocols in the PDHuman test dataset. Our proposed methods, \Ours(H48) and \Oursp (R50) outperform the other methods in all metrics by a large margin. 
% Specifically, our model-based approach, \Oursp, achieves some of the highest 2D mIoU metrics.

\noindent\textbf{Full results on SPEC-MTP:} As illustrated in~\cref{tab:sota_supp_specmtp}, we report the results of all the 3 protocols in SPEC-MTP dataset. In this real-world dataset, \Ours (H48) largely outperforms other methods in all metrics. In column $\tau=1.0$, Note that our re-implemented SPEC$*$ achieves higher performance than the official implementation.
 
\noindent\textbf{Full results on HuMMan:} As shown in \cref{tab:sota_sup_humman}, \Ours (H48) largely outperforms other methods in all metrics. By contrast, although CLIFF~\cite{cliff} performs comparably well on the HuMMan dataset, it demonstrates poor performance on the PDHuman dataset. We conjecture the focal length assumption of CLIFF is suitable for datasets captured by fixed and similar camera settings, \eg HuMMan dataset, while not valid for the PDHuman dataset with varied camera settings.


\begin{table*}[hb]
    \centering
    \scalebox{0.61}{\input{tables/supp_pdhuman}}
    
    \caption{Results of SOTA methods on PDHuman ($\tau=3.0$, $\tau=2.6$, $\tau=2.2$ protocols). HMR-$f$ terms HMR~\cite{hmr} model trained with same focal length as \Ours.}
    \label{tab:sota_supp_pdhuman}
\end{table*}

\begin{table*}[h]
    \centering
    \scalebox{0.61}{\input{tables/supp_pdhuman2}}
    
    \caption{Results of SOTA methods on PDHuman ($\tau=1.8$, $\tau=1.4$ protocols).}
    \label{tab:sota_supp_pdhuman2}
\end{table*}


% \subsection{SPEC-MTP}
\begin{table*}[h]
    \centering
    \scalebox{0.61}{\input{tables/supp_spec_mtp}}
    
    \caption{Results of SOTA methods on SPEC-MTP ($\tau=1.8$, $\tau=1.4$, $\tau=1.0$ protocols). SPEC-MTP~($\tau=1.0$) indicates the original SPEC-MTP~\cite{spec} dataset. SPEC~* terms the results reported in SPEC~\cite{spec}.}
    \label{tab:sota_supp_specmtp}
\end{table*}

% \subsection{HuMMan}

\begin{table*}[h]
    \centering
    \scalebox{0.61}{\input{tables/supp_humman}}
    
    \caption{Results of SOTA methods on HuMMan ($\tau=1.8$, $\tau=1.4$, $\tau=1.0$ protocols).}
    \label{tab:sota_sup_humman}
\end{table*}

\section{Qualitative results.}
% \ding{177}


\noindent\textbf{Qualitative results on Human3.6M~\cite{h36m} dataset.}
We show qualitative results of \Ours on Human3.6M dataset in \cref{fig:h36m}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.\linewidth]{pictures/h36m.pdf}
    \caption{Qualitative results on Human3.6M dataset. The number under each image represents predicted/ground-truth focal length $f$, FoV angle, and z-axis translation $T_{z}$. Our method could predict an approximate translation for non-distorted images as well.}
    \label{fig:h36m}
    \vspace{-10pt}
\end{figure}



\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{pictures/failure_cases.pdf}
    \caption{Failure cases. The left part is input, and the right part is our prediction.}
    \label{fig:failure}
    \vspace{-10pt}
\end{figure}

\noindent\textbf{Failure cases.}
% \subsection{Human3.6M}
% Our methods do not hold well in some extreme circumstances. 
%  As in \cref{fig:failure}, since our training data lack extremely big hands as (1)(2)(6) and extremely big feet as (7)(8), our method could not perform well on these images. And other circumstance as characters with extremely strong body shape as in (2)(3)(7). Our method also performs not well on self-occluded images like (4). 
Although our methodology is generally effective, it has trouble under certain extreme circumstances. As demonstrated in~\cref{fig:failure}, due to the lack of training data containing characters with large hands ((1), (2), and (6)), and large feet ((7) and (8)), \Ours produce sub-optimal results on such images. 
%
Similarly, our approach may not perform well on characters with exceptional body shapes, as exemplified by (2), (3), and (7), where the athletes have muscular bodies. Additionally, it is difficult for \Ours to reconstruct self-occluded human bodies, as depicted in (4). We are actively exploring strategies to address these limitations and improve the robustness of our methodology.

\noindent\textbf{More qualitative results on distorted images.}
We show more qualitative results of \Ours comparing with SOTA methods for perspective-distorted images on PDHuman (\cref{fig:supp_pdhuman_sota}), Web images (\cref{fig:supp_real_sota}), and SPEC-MTP (\cref{fig:supp_specmtp_sota}).

\clearpage



\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{pictures/supp_pdhuman_sota.pdf}
    \caption{Qualitative results on PDHuman dataset. The number under each image represents predicted/ground-truth focal length $f$, FoV angle, and z-axis translation $T_{z}$. }
    \label{fig:supp_pdhuman_sota}
\end{figure*}


\begin{figure*}[htp]
    \includegraphics[width=1.0\linewidth]{pictures/supp_real.pdf}
\caption{Qualitative results on in-the-wild images. The number under each image represents the predicted focal length $f$, FoV angle, and z-axis translation $T_{z}$. Images are collected from \url{https://pexels.com} and \url{https://yandex.com}.
}
\label{fig:supp_real_sota}
\end{figure*}

\begin{figure*}
    \includegraphics[width=1.0\linewidth]{pictures/supp_specmtp.pdf}
    \caption{Qualitative results on SPEC-MTP dataset. The number under each image represents predicted/ground-truth focal length $f$, FoV angle, and z-axis translation $T_{z}$. The ground-truth $T_{z}$ and focal length $f$ for SPEC-MTP are pseudo labels.}
    \label{fig:supp_specmtp_sota}
\end{figure*}
% \subsection{HuMMan}

% \subsection{PDHuman}


% \section{Downstream tasks.}
% \paragraph{Differentiable rendering.}
% \paragraph{Test time optimization for fixed cameras.}