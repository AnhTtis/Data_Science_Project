\section{Experiments}


\subsection{Datasets}


\begin{figure}[t]
    \centering
  \includegraphics[width=0.93\linewidth]{pictures/dataset.pdf}
    \caption{\textbf{Demonstration of distortion image.} The three columns from left to right are our PDHuman dataset, HuMMan dataset, and SPEC-MTP dataset, respectively. The value from the arrow (yellow) indicates the distortion scale of the pixel.}
    \label{fig:dataset}
    % \vspace{-10pt}
\end{figure}

\noindent\textbf{PDHuman.}
%
Despite perspective distortion being a common problem, no existing public dataset is specifically designed for this task.
%
Inspired by recent synthetic datasets~\cite{synbody,gta,bedlam,Patel:CVPR:2021:AGORA}, we introduce a synthetic dataset named PDHuman. The dataset contains $126,198$ images in the training split and $27,448$ images in the testing split, with annotations including camera intrinsic matrix, 2D/3D keypoints, SMPL parameters ($\boldsymbol{\theta}$, $\boldsymbol{\beta}$), and translation for each image. The testing split is further divided into 5 protocols by the max distortion scale of each image sample.
We define the max distortion scale for each sample as $\tau$; this value will be used in splitting protocols.
% \todo{define max distortion scale with $\tau$?}

We use $630$ human models from RenderPeople~\cite{renderpeople} and $1,710$ body pose sequences from Mixamo~\cite{mixamo}, with 500 HDRi images with various lighting conditions as backgrounds. 
% For data generation, we collected 630 photogrammetry-scanned human models from
% % Renderpeople\footnote[2]{\url{https://renderpeople.com}}
% Renderpeople \cite{renderpeople}, with SMPL parameters well fitted. 
% We collected body pose sequence from a real MoCap sequence in 
% % Mixamo\footnote[3]{\url{https://www.mixamo.com/}},
% Mixamo \cite{mixamo},
% then re-targeted the pose to SMPL skeleton. We use HDRi images as backgrounds. 
We use the dolly-zoom effect to generate random camera extrinsic and intrinsic matrices with random rotations, translations, and focal lengths.
The distance from the human body to the cameras is set from 0.5m to 10m, so our dataset contains severely distorted, slightly distorted, and nearly non-distorted images.
Then we use Blender~\cite{blender} to render the RGB images. See \cref{fig:dataset} for brief demonstration.
For detailed rendering procedures and more image demonstrations, please refer to Sup. Mat.

% For test split, we calculated the max distortion scale of each sample and divided the images into five protocols by distortion scale.

% For better re-targeting results, we saved the 3d vertices of every frame and used SGD loop optimization to optimize the SMPL parameters.
% Our pipeline is inspired by recent works on synthetic data \cite{Patel:CVPR:2021:AGORA,hspace,varol17_surreal}. 


\noindent\textbf{SPEC-MTP and HuMMan Datasets.}
SPEC-MTP dataset~\cite{spec} is proposed to test human pose reconstruction in world coordinates. It includes many close-up shots, mostly taken from below or overhead views, leading to images with distorted human bodies. HuMMan dataset~\cite{humman} is captured by multi-view RGBD cameras and has accurate ground truth because the SMPL parameters are fitted based on 3D keypoints and point clouds. HuMMan also contains images with distorted human bodies since the actors were close to the cameras, all less than 3 meters away. In our paper, we extend both datasets into real-world perspective-distorted datasets. SPEC-MTP is used only for testing. For HuMMan, we split it into training and testing parts. When testing, we divide these two datasets into three protocols based on their maximum distortion scale $\tau$. See \cref{fig:dataset} for brief demonstration.

% The SPEC-MTP dataset is proposed in \cite{spec} mainly used to test the human pose reconstruction in world coordinates. It contains many close-up shot, taken from below or overhead views. This leads to many images of distorted human bodies.
% The HuMMan dataset is proposed in \cite{humman}, it is an indoor dataset captured by multi-view RGBD cameras, and the SMPL parameters were fitted using triangulated 3D key-points and point-clouds. So its ground-truth was very accurate. It also contains some images with distorted human bodies since the actors were very close to the cameras, often less than 3 meters. 
% We extend the SPEC-MTP~\cite{spec} and HuMMan~\cite{humman} dataset as real world perspective distorted datasets.
% Following SPEC~\cite{spec}, SPEC-MTP is only used for testing in our paper. And HuMMan is split into training and testing parts following its original set.
% For SPEC-MTP and HuMMan, we calculate the max distortion scale of each image sample and divide the images into three protocols by distortion scale.


\noindent\textbf{Non-distorted Datasets.}
For non-distorted datasets, we use Human3.6M~\cite{h36m}, COCO~\cite{coco}, MPI-INF-3DHP ~\cite{mpi-inf3dhp} and LSPET~\cite{lspet} as our training data.
Following~\cite{meshgraphormer, fastmetro}, we also report the results fine-tuned on 3DPW~\cite{3dpw} training data.

\subsection{Evaluation Metrics}
To measure the accuracy of reconstructed human mesh, we follow the previous works~\cite{hmr,spec,cliff} by adopting MPJPE (Mean Per Joint Position Error), PA-MPJPE (Procrustes Analysis Mean Per Joint Position Error) and PVE (Per Vertex Error) as our 3D evaluation metrics. They all measure the Euclidean distances of 3D points or vertices between the predictions and ground truth in millimeters (mm).

To measure the re-projection results in perspective distorted datasets such as PDHuman, SPEC-MTP and HuMMan, we leverage metrics widely used in segmentation tasks, MeanIoU~\cite{pascal} as our 2D metric. We both report foreground and background MeanIoU marked as mIoU and body part MeanIoU marked as P-mIoU. We use the 24-part vertex split provided by official SMPL~\cite{smpl} for body part segmentation. During the evaluation, for weak-perspective methods like HMR, we will render the predicted segmentation masks with a focal length of $5,000$ pixels. And we use the corresponding focal length on methods with specific camera models, such as SPEC~\cite{spec}, CLIFF~\cite{cliff}, and proposed \Ours.


\begin{table*}[t]
    \centering
    \scalebox{0.61}{\input{tables/sota-distortion}}
    
    \caption{Results of SOTA methods on PDHuman, SPEC-MTP~\cite{spec} and HuMMan~\cite{humman} datasets. Here we report the largest distortion protocol. 
    % $\rm TH_{d}^{3.0}$ represents the threshold of 3.0 distortion scale, which means the maximum distortion scale in all the images are larger than 3.0. 
    R50 terms ResNet-50~\cite{resnet}, and H48 terms HRNet-w48~\cite{hrnet} here.}
    \label{tab:sota}
\end{table*}

\subsection{Implementation Details}
Unless specified, we use ResNet-50~\cite{resnet} and HRNet-w48~\cite{hrnet} backbones for model-free \Ours. 
%
We also design a model-based variant, \Oursp ($\cP$ stands for parametric), by changing the mesh reconstruction module to a model-based pose and shape estimation module. The details of \Oursp can be found in the Sup. Mat.
% For our , we mainly report results using ResNet-50~\cite{resnet}. 
%
All backbones are initialized by COCO~\cite{coco} key-point dataset pre-trained models.
%
We use Adam~\cite{adam} optimizer with a fixed learning rate of $2e^{-4}$. 
%
All experiments of \Ours are conducted on 8 A100 GPUs for around 160 epochs, 14$\sim$18 hours. 
%
 Our training pipeline was built based on MMHuman3D~\cite{mmhuman3d} code base.
For samples with ground-truth focal length and translations, we render IUV and distortion images online during the training by PyTorch3D~\cite{pytorch3d}.

For comparison on PDHuman, SPEC-MTP, and HuMMan, we follow the official codes of HMR~\cite{hmr}, SPEC~\cite{spec}, PARE~\cite{pare}, GraphCMR\cite{graphcmr}, FastMETRO\cite{fastmetro}. We re-implemented CLIFF~\cite{cliff} since the authors have not released the training codes. All SOTA methods are trained on 8 A100 GPUs until convergence, following the officially released hyper-parameters. 
% To ensure a fair comparison, all methods use the pre-trained backbone weights provided by HMR-Benchmark~\cite{hmr-benchmark}
All methods are trained on the same datasets with the same proportion, \eg, 
Human3.6M~\cite{h36m} (40\%), PDHuman (20\%), HuMMan~\cite{humman} (10\%), MPI-INF-3DHP~\cite{mpi-inf3dhp} (10\%), COCO~\cite{coco} (10\%), LSPET~\cite{lspet} (5\%).

% occupies a proportion of 40\%, 20\%, 15\%, 10\%, 10\%, 5\%, respectively.

% For \Ours, we use the ResNet-50~\cite{resnet} and HRNet-w48~\cite{hrnet} backbone both. For our model-based variant \Oursp, we mainly report the ResNet-50~\cite{resnet} results. Following
% HMR-Benchmark~\cite{hmr-benchmark}, we use the backbone weights pre-trained on COCO~\cite{coco} both for ResNet-50 and HRNet-w48 backbones. The Adam~\cite{adam} optimizer with a fixed learning rate of $2e^{-4}$ is used. With 8 A100 GPUs, total training takes around 160 epochs, 14$\sim$18 hours.
% During training, for datasets with ground-truth focal length and translations, we will render the ground-truth uv image and distortion image by PyTorch3D~\cite{pytorch3d}. We use the uv topology provided by DecoMR~\cite{decomr}.

% For comparison of PDHuman, SPEC-MTP and HuMMan, we conducted experiments on HMR~\cite{hmr}, SPEC~\cite{spec} PARE~\cite{pare}, GraphCMR~\cite{graphcmr}, FastMetro~\cite{fastmetro} by modifying their official codes. For SPEC~\cite{spec}, we load their pre-traind CamCalib network and estimate the focal length of the full image, then we transform the focal length to the cropped image. Since we compare all the 3d metrics in camera coordinates, we set the extrinsic rotation matrix estimated by CamClib as identity. We also re-implemented CLIFF~\cite{cliff} following their paper's configuration since they have not released their training codes.
% For fair comparison, all the methods are trained on the same dataset with the same proportion as \Ours. Our training pipeline was built based on MMHuman3D~\cite{mmhuman3d} code base. We use the same pre-trained backbone weights provided by HMR-Benchmark~\cite{hmr-benchmark} for all the methods we compare.
% All the SOTA methods are trained on 8 A100 GPUs until convergence, following the original hyper-parameters in the official released codes.


% MPJPE (Mean Per Joint Position Error) first aligns the predicted and ground-truth 3D joints at the pelvis, and then calculates their distances, which comprehensively evaluates the predicted poses and shapes, including the global rotations.

% PA-MPJPE (Procrustes-Aligned Mean Per Joint Position Error, or reconstruction error) performs Procrustes alignment before computing MPJPE, which mainly measures the articulated poses, eliminating the discrepancies in scale and global rotation.

% PVE (Per Vertex Error, or MVE used in the AGORA evaluation) does the same alignment as MPJPE at first, but calculates the distances of vertices on the human mesh surfaces.


% We also extend our method to a parametric-based method by simply changing the mesh reconstruction head to the pose and shape estimation head. We call this variation \Oursp. We also report results of $\rm POET^{\cP}$ in the experiments. The detailed pipeline will be demonstrated in Sup. Mat..

For distorted datasets, we only report the results on the protocols with the largest distortion scales. The full results of all protocols are shown in Sup. Mat.

\subsection{Main Results}
\noindent\textbf{Results on PDHuman, SPEC-MTP, and HuMMan.}
%
We report PA-MPJPE, MPJPE, PVE, mIoU, and P-mIoU on these three datasets. For model-based methods, we compare with HMR~\cite{hmr}, SPEC~\cite{spec}, CLIFF~\cite{cliff}, PARE~\cite{pare}. We compare model-free methods with GraphCMR~\cite{graphcmr} and FastMETRO~\cite{fastmetro}.
From ~\cref{tab:sota}, we can see that SPEC~\cite{spec} performs poorly on these distorted datasets. This is mainly due to their wrong focal length assumption, which has negative rather than positive effects on their supervision. (Note that our re-implemented SPEC has higher performance than the official code, see in Sup. Mat.) CLIFF performs well on SPEC-MTP, while badly on PDHuman. Because their focal length assumption is about $53^{\circ}$ for 16:9 images, close to SPEC-MTP images. Although HMR-$f$ is trained with the same focal length as \Ours, it improves little compared to HMR since they have not encoded the distortion or distance feature into their network. \Ours-H48 outperforms SOTA methods on most metrics, especially the 3D ones. Some 2D re-projection metrics,  \eg mIoU and P-mIoU, of \Ours-H48 are lower than~\Oursp-R50 version. We conjecture that model-based methods have better reconstructed shapes.
Please refer to \cref{fig:sota_demo} for qualitative results.
More qualitative results and failure cases can be found in Sup. Mat.


\begin{figure*}[htp]
  \centering
\includegraphics[width=0.97\linewidth]{pictures/sota.pdf}
  \caption{\textbf{Qualitative results of SOTA methods.} Besides \Ours, we visualize the results of three methods with specific camera models: HMR~\cite{hmr}, SPEC~\cite{spec}, CLIFF~\cite{cliff}. \Oursp terms our model-based variance. We show results come from different data sources. Row 1: PDHuman test. Row 2, 3: web images. Row 4: SPEC-MTP. The number under each image represents predicted/ground-truth $f$, FoV angle, and $T_{z}$. The ground-truth $f$ and $T_{z}$ for SPEC-MTP are pseudo labels. The focal lengths here are all transformed to pixels in full image.}
  \label{fig:sota_demo}
\end{figure*}

\input{tables/sota-3dpw}
\noindent\textbf{Results on 3DPW.}
This study compares our proposed method, \Ours, with SOTA methods~\cite{hybrik, graphcmr, hmr, spin, pymaf, spec, pare, fastmetro, cliff, meshgraphormer}, including both model-based and model-free approaches. As shown in~\cref{tab:3dpw}, \Ours-R50 achieves comparable results to the SOTA method FastMETRO-R50 even without being fine-tuned on the 3DPW training set. Moreover, after fine-tuning, \Ours with both backbone structures show a significant improvement in performance. Furthermore, when using HRNet-w48, our approach outperforms all SOTA methods in all three metrics, surpassing model-based SOTA method CLIFF~\cite{cliff} and model-free SOTA method FastMETRO~\cite{fastmetro}. This superiority can be attributed to two main factors: on one hand, in training, we use ground-truth focal length and translation from 3DPW raw data to supervise the rendering of IUV images and distortion images; on the other hand, given that 96\% of the 3DPW images were captured within a distance of 1.2m to 10m, with more than half of them captured within 4m, there exist many perspective-distorted images. We provide a detailed analysis of the results for samples captured from different distances in the 3DPW dataset in the Sup. Mat.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{pictures/supp_pw3d.pdf}
    \caption{Qualitative results for 3DPW. \Ours achieves good alignment with the characters in the original image, but other SOTA methods have difficulty aligning images that suffer from distortion caused by overhead shots, which causes upper body dilation and lower body shrinkage. 
    The number under each image represents predicted/ground-truth $f$, FoV angle, and $T_{z}$. }
    \label{fig:demo_pw3d}
\end{figure}



\noindent\textbf{Results on Human3.6M:}
During training, we get the ground-truth focal length and translation from Human3.6M~\cite{h36m} training set for our supervision. When evaluating Human3.6M, we follow HybrIK~\cite{hybrik} by using SMPL joints as the ground truth for evaluation. As shown in~\cref{tab:h36m}, our method performs well on Human3.6M through it is not a perspective-distorted dataset. \Ours-H48 achieves the best result on the PA-MPJPE metric and achieves comparable results on the MPJPE metric. CLIFF achieves the best results on MPJPE while they also need ground-truth bounding boxes during testing.

\begin{table}[ht]
    \centering
    \scalebox{0.75}{\input{tables/supp_sota_h36m}}
    \vspace{-0.05in}
    \caption{Results of SOTA methods on Human3.6M~\cite{h36m}.
    }
    \vspace{-2ex}
 %   \vspace{-1ex}
    \label{tab:h36m}
\end{table}


\subsection{Ablation study}
\input{tables/ablation_3dpw}
\noindent\textbf{Ablation on training settings on the standard benchmark 3DPW.}
In Table~\ref{tab:abl_pw3d}, we present the results of our ablation study on the standard 3DPW benchmark~\cite{3dpw}, where we investigate the impact of different training settings on the performance of our method. By controlling two different variables, we show that introducing perspective-distorted datasets and fine-tuning with ground-truth focal length both lead to a slight improvement in performance. Notably, our method \Ours-H48 still outperforms the current state-of-the-art methods even without using perspective-distorted data or ground-truth focal length.


% Please see the ablation study on loss function, module structure, and training data in Supp.Mat.

This study evaluates the effectiveness of the distortion feature and the hybrid re-projection loss function. The evaluation is conducted on the PDHuman ($\tau=3.0$), as this  exhibits the highest degree of distortion. More experimental results are provided in the Sup. Mat.

\noindent\textbf{Effect of distortion feature.}
In \cref{tab:ablation}, w/o $w(I_{d})$ terms without warp distortion image into UV space, and w/o $c(F_{d})$ terms without concatenating distortion feature to per-vertex feature. We can see that, while the mIoU and P-mIoU change a little, the 3D metrics increase significantly with the correct distortion feature. This study validates our intuition that distortion information helps the network predict more accurate vertex coordinates.

\noindent\textbf{Effect of the hybrid re-projection loss function.}
We experimented with different re-projection loss configurations and found that relying solely on weak-perspective loss significantly decreases 2D alignment. Incorporating perspective loss improved 3D metrics slightly but increased the 2D segmentation error significantly. Moreover, using per-joint distortion weight to supervise the weak-perspective camera improved the alignment of the human mesh and resulted in more accurate 3D supervision without increasing the 2D segmentation error. 


% \noindent\textbf{Effect of training settings on the standard benchmark 3DPW.}
% In Table~\ref{tab:abl_pw3d}, we present the results of our ablation study on the standard 3DPW benchmark~\cite{3dpw}, where we investigate the impact of different training settings on the performance of our method. By controlling two different variables, we show that introducing perspective-distorted datasets and fine-tuning with ground-truth focal length both lead to a slight improvement in performance. Notably, our method \Ours-H48 still outperforms the current state-of-the-art methods even without using perspective-distorted data or ground-truth focal length.
% \input{tables/ablation_3dpw}

We conclude that utilizing dense distortion features and an accurate camera model greatly improves the performance of our proposed method
\input{tables/ablation_network}
