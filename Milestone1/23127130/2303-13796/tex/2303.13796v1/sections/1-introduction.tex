
%%%%%%%%% BODY TEXT
\section{Introduction}

\label{sec:intro}
Human pose and shape estimation from single-view RGB images is a long-standing research area of computer vision, as the reconstructed motion and mesh could empower various human-centered downstream applications like 3D animations, robotics, or AR/VR development. Previous works~\cite{hmr, spin, vibe, pare, romp, vibe, graphcmr, ochmr, ooh} formulate the problem under the assumption that the reconstructed people are far away from the camera, thus the torso and limb distortion caused by the perspective projection can be neglected.

However, perspective distortion in close-up images is common in real-life scenarios, such as photographs of athletes/actors in sports events/films or selfies taken for social media. In such images, distortions are usually caused by aerial photography, overhead beat, or large depth variance among torsos and limbs, resulting in depth ambiguity in single-view RGB images, which makes it a big challenge to recover human pose and shape (See \cref{fig:teaser}).

Previous methods typically assume a large fixed focal length~\cite{hmr,spin,vibe,pare} or estimate a focal length~\cite{spec} using pre-trained networks and calculate the translation from the estimated focal length. These settings are appropriate when people are far from the cameras, where the depth variance of the human body is negligible compared to the distance to the camera. However, these methods are inappropriate for handling scenarios in which human bodies are perspective distorted. Overestimating the focal length could lead to joint angle ambiguity or harm joint rotation learning.
Several methodologies for pose estimation, as proposed by previous works~\cite{beyondweak, cliff}, assume a large field-of-view (FoV) angle. However, these methods may not show significant improvement when the focus is solely on non-distorted human images, as they often lack a conditioning for depth variance when the camera zooms in or out. Inaccuracies in estimating the depth variance with respect to translation can adversely impact re-projection loss, leading to erroneous results, as illustrated in~\cref{fig:teaser}. Actually, a correctly estimated distance and focal length also help with 2D alignment, which will be useful in downstream tasks.

 To address the challenge of perspective distortion in close-up images, showing respect to Hitchcock's dolly zoom shot, we introduce \Ours~(\textbf{Z}oom f\textbf{O}cal  \textbf{L}ength correct\textbf{LY}) for perspective-distorted human mesh reconstruction. Our method utilizes 2D human distortion features to estimate the real-world distance to the camera center, enabling the reconstruction of the 3D human mesh in perspective-distorted images. The framework comprises of two parts: a translation estimation module for estimating the z-axis distance of the human body from the camera center, and a mesh reconstruction module for reconstructing 3D vertex coordinates in camera space. Additionally, we introduce a hybrid loss function that combines both perspective and weak perspective projection to boost performance.

 
\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.85\linewidth]{pictures/dollyzoom.pdf}
\end{center}
\vspace{-10pt}
   \caption{This figure showcases how the distortion scale of a person's limbs becomes more pronounced when closer to the camera center. Further, when two human bodies are at the same included angle with respect to the human-camera axis, they appear similar in facial direction. However, the horizontal translation may cause distinct distortion types on the left and right sides. We demonstrate that as the human body gets closer to the camera center, the distortion magnitude increases, leading to a more precise estimation of depth and rotation angles.}
\label{fig:dollyzoom}
\end{figure}

% in the movie Jaws
Inspired by the iconic dolly-zoom shot~\cite{jaws1975} (also known as zolly shot), which creatively combines camera movement and zooming to create a distorted perspective and sense of unease, we propose a translation estimation module for the perspective-distorted 3DHMR task. This module highlights how the relative position of the human body to the camera affects the perspective distortion in images. Based on this insight, we introduce the distorted image as a new representation to capture the 2D shrinking or dilation scales of each pixel. Our translation network utilizes distortion and IUV images to accurately estimate z-axis translation, overcoming the limitations of traditional methods that rely on environmental information. 
IUV image could help eliminate the 2D shift and scale information in distorted images and represent 2D dense position information.
For mesh reconstruction, we lift the 2D position feature to the 3D vertex position feature and sample the by-vertex distortion feature to regress 3D vertex coordinates. We use perspective projection to supervise correctly and weak-perspective projection  to locate the 2D human body position in the image and help to calculate our focal length.


In summary, our contributions are as follows:

(1) We analyze all the current famous camera systems in state-of-the-art (SOTA) 3DHMR methods and propose a new system tailored to the perspective-distorted 3DHMR task.
% inspired by the dolly-zoom effect~\cite{jaws1975}, to reconstruct the perspective projection matrix.

(2) We propose a novel learning-based method to tackle the perspective-distorted 3DHMR task without relying on extra environmental information. The core of our method is a newly designed representation, termed distortion image, and the proposed hybrid projection supervision by using both perspective and weak-perspective projection.

(3) We build the first large-scale synthesis dataset PDHuman for the perspective-distorted 3DHMR task, with high-quality SMPL ground truth and camera parameters. To evaluate the performance on real images, we benchmark two real-world datasets, SPEC-MTP~\cite{spec} and HuMMan~\cite{humman}, which contain perspective-distorted images with well-fitted SMPL parameters a camera parameters.
