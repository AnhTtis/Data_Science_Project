%%%%% BO %%%%%%%
@inproceedings{balandat2020botorch,
  author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel and Daulton, Samuel and Letham, Ben and Wilson, Andrew G and Bakshy, Eytan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21524--21538},
 publisher = {Curran Associates, Inc.},
 title = {BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization},
 volume = {33},
 year = {2020}
}

@inproceedings{eriksson2019scalable,
 author = {Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Scalable Global Optimization via Local Bayesian Optimization},
 volume = {32},
 year = {2019}
}

@article{cheng2022odbo,
  title={ODBO: Bayesian optimization with search space prescreening for directed protein evolution},
  author={Cheng, Lixue and Yang, Ziyi and Liao, Benben and Hsieh, Changyu and Zhang, Shengyu},
  journal={arXiv preprint arXiv:2205.09548},
  year={2022}
}

@article{frazier2018tutorial,
  title={A tutorial on Bayesian optimization},
  author={Frazier, Peter I},
  journal={arXiv preprint arXiv:1807.02811},
  year={2018}
}

@book{mockus2012bayesian,
  title={Bayesian approach to global optimization: theory and applications},
  author={Mockus, Jonas},
  volume={37},
  year={2012},
  publisher={Springer Science \& Business Media}
}
@book{rasmussen2006,
options={maxbibnames=99},
address = {Cambridge, MA},
doi = {10.5555/1162254},
author = {Rasmussen, Carl E and Williams, Christopher K I},
booktitle = {Gaussian processes for machine learning},
publisher = {MIT Press},
title = {{Gaussian processes for machine learning}},
year = {2006}
}

@inproceedings{letham2020re,
 author = {Letham, Ben and Calandra, Roberto and Rai, Akshara and Bakshy, Eytan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1546--1558},
 publisher = {Curran Associates, Inc.},
 title = {Re-Examining Linear Embeddings for High-Dimensional Bayesian Optimization},
 volume = {33},
 year = {2020}
}


@InProceedings{nayebi2019framework,
  title = 	 {A Framework for {B}ayesian Optimization in Embedded Subspaces},
  author =       {Nayebi, Amin and Munteanu, Alexander and Poloczek, Matthias},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4752--4761},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
}

@article{srinivas2009gaussian,
  title={Gaussian process optimization in the bandit setting: No regret and experimental design},
  author={Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M and Seeger, Matthias},
  journal={arXiv preprint arXiv:0912.3995},
  year={2009}
}

@inproceedings{gardner2018gpytorch,
  title={{GPyTorch}: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration},
  editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  author={Gardner, Jacob R and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q and Wilson, Andrew Gordon},
  publisher = {Curran Associates, Inc.},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@InProceedings{kingma2015,
author = {Kingma, Diederick P and Ba, Jimmy},
title = {Adam: A method for stochastic optimization},
editor    = {Yoshua Bengio and Yann LeCun},
booktitle = { International Conference on Learning Representations (ICLR) },
year = {2015}
}

@article{Letham2017ConstrainedBO,
  title={Constrained Bayesian Optimization with Noisy Experiments},
  author={Benjamin Letham and Brian Karrer and Guilherme Ottoni and Eytan Bakshy},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.07094}
}

@article{gelbart2014bayesian,
  title={Bayesian optimization with unknown constraints},
  author={Gelbart, Michael A and Snoek, Jasper and Adams, Ryan P},
  journal={arXiv preprint arXiv:1403.5607},
  year={2014}
}

@article{yuan2015recent,
  title={Recent advances in trust region algorithms},
  author={Yuan, Ya-Xiang},
  journal={Mathematical Programming},
  volume={151},
  pages={249--281},
  year={2015},
  publisher={Springer}
}

@Inbook{Powell1994,
author="Powell, M. J. D.",
editor="Gomez, Susana
and Hennart, Jean-Pierre",
title="A Direct Search Optimization Method That Models the Objective and Constraint Functions by Linear Interpolation",
bookTitle="Advances in Optimization and Numerical Analysis",
year="1994",
publisher="Springer Netherlands",
address="Dordrecht",
pages="51--67",
isbn="978-94-015-8330-5",
doi="10.1007/978-94-015-8330-5_4",
}

@article{self2021variational,
  title={Variational quantum algorithm with information sharing},
  author={Self, Chris N and Khosla, Kiran E and Smith, Alistair WR and Sauvage, Fr{\'e}d{\'e}ric and Haynes, Peter D and Knolle, Johannes and Mintert, Florian and Kim, MS},
  journal={npj Quantum Information},
  volume={7},
  number={1},
  pages={116},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{tamiya2022stochastic,
  title={Stochastic gradient line Bayesian optimization for efficient noise-robust optimization of parameterized quantum circuits},
  author={Tamiya, Shiro and Yamasaki, Hayata},
  journal={npj Quantum Information},
  volume={8},
  number={1},
  pages={90},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{eriksson2021high,
  title={High-dimensional Bayesian optimization with sparse axis-aligned subspaces},
  author={Eriksson, David and Jankowiak, Martin},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={493--503},
  year={2021},
  organization={PMLR}
}

@inproceedings{frohlich2020noisy,
  title={Noisy-input entropy search for efficient robust Bayesian optimization},
  author={Fr{\"o}hlich, Lukas and Klenske, Edgar and Vinogradska, Julia and Daniel, Christian and Zeilinger, Melanie},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2262--2272},
  year={2020},
  organization={PMLR}
}

@inproceedings{daulton2022robust,
  title={Robust multi-objective bayesian optimization under input noise},
  author={Daulton, Samuel and Cakmak, Sait and Balandat, Maximilian and Osborne, Michael A and Zhou, Enlu and Bakshy, Eytan},
  booktitle={International Conference on Machine Learning},
  pages={4831--4866},
  year={2022},
  organization={PMLR}
}

@article{tibaldi2022bayesian,
  title={Bayesian Optimization for QAOA},
  author={Tibaldi, Simone and Vodola, Davide and Tignone, Edoardo and Ercolessi, Elisa},
  journal={arXiv preprint arXiv:2209.03824},
  year={2022}
}

@article{dave2022autonomous,
  title={Autonomous optimization of non-aqueous Li-ion battery electrolytes via robotic experimentation and machine learning coupling},
  author={Dave, Adarsh and Mitchell, Jared and Burke, Sven and Lin, Hongyi and Whitacre, Jay and Viswanathan, Venkatasubramanian},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={5454},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{zhang2020bayesian,
  title={Bayesian optimization for materials design with mixed quantitative and qualitative variables},
  author={Zhang, Yichi and Apley, Daniel W and Chen, Wei},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--13},
  year={2020},
  publisher={Springer}
}

@inproceedings{martinez2018practical,
  title={Practical Bayesian optimization in the presence of outliers},
  author={Martinez-Cantin, Ruben and Tee, Kevin and McCourt, Michael},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1722--1731},
  year={2018},
  organization={PMLR}
}

@ARTICLE{spall1998,
  author={Spall, J.C.},
  journal={IEEE Transactions on Aerospace and Electronic Systems}, 
  title={Implementation of the simultaneous perturbation algorithm for stochastic optimization}, 
  year={1998},
  volume={34},
  number={3},
  pages={817-823},
  doi={10.1109/7.705889}}


@article{shaffer2023,
  title = {Surrogate-based optimization for variational quantum algorithms},
  author = {Shaffer, Ryan and Kocia, Lucas and Sarovar, Mohan},
  journal = {Phys. Rev. A},
  volume = {107},
  issue = {3},
  pages = {032415},
  numpages = {10},
  year = {2023},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.107.032415},
}

%%%%%%% Quantum %%%%%%%
@article{larocca2021theory,
  title={Theory of overparametrization in quantum neural networks},
  author={Martin Larocca and Nathan Ju and Diego Garc'ia-Mart'in and Patrick J. Coles and Mar{\'i}a Cerezo},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.11676}
}

@article{Zhang2022,
abstract = {TensorCircuit is an open source quantum circuit simulator based on tensor network contraction, designed for speed, flexibility and code efficiency. Written purely in Python, and built on top of industry-standard machine learning frameworks, TensorCircuit supports automatic differentiation, just-in-time compilation, vectorized parallelism and hardware acceleration. These features allow TensorCircuit to simulate larger and more complex quantum circuits than existing simulators, and are especially suited to variational algorithms based on parameterized quantum circuits. TensorCircuit enables orders of magnitude speedup for various quantum simulation tasks compared to other common quantum software, and can simulate up to 600 qubits with moderate circuit depth and low-dimensional connectivity. With its time and space efficiency, flexible and extensible architecture and compact, user-friendly API, TensorCircuit has been built to facilitate the design, simulation and analysis of quantum algorithms in the Noisy Intermediate-Scale Quantum (NISQ) era.},
archivePrefix = {arXiv},
author = {Zhang, Shi-Xin and Allcock, Jonathan and Wan, Zhou-Quan and Liu, Shuo and Sun, Jiace and Yu, Hao and Yang, Xing-Han and Qiu, Jiezhong and Ye, Zhaofeng and Chen, Yu-Qin and Lee, Chee-Kong and Zheng, Yi-Cong and Jian, Shao-Kai and Yao, Hong and Hsieh, Chang-Yu and Zhang, Shengyu},
doi = {10.22331/q-2023-02-02-912},
issn = {2521-327X},
journal = {Quantum},
month = {feb},
pages = {912},
title = {{TensorCircuit: a Quantum Software Framework for the NISQ Era}},
volume = {7},
year = {2023}
}

@article{zhou2020quantum,
  title={Quantum approximate optimization algorithm: Performance, mechanism, and implementation on near-term devices},
  author={Zhou, Leo and Wang, Sheng-Tao and Choi, Soonwon and Pichler, Hannes and Lukin, Mikhail D},
  journal={Physical Review X},
  volume={10},
  number={2},
  pages={{021067}},
  year={2020},
  publisher={APS}
}

@article{Schuetz2022,
abstract = {Combinatorial optimization problems are pervasive across science and industry. Modern deep learning tools are poised to solve these problems at unprecedented scales, but a unifying framework that incorporates insights from statistical physics is still outstanding. Here we demonstrate how graph neural networks can be used to solve combinatorial optimization problems. Our approach is broadly applicable to canonical NP-hard problems in the form of quadratic unconstrained binary optimization problems, such as maximum cut, minimum vertex cover, maximum independent set, as well as Ising spin glasses and higher-order generalizations thereof in the form of polynomial unconstrained binary optimization problems. We apply a relaxation strategy to the problem Hamiltonian to generate a differentiable loss function with which we train the graph neural network and apply a simple projection to integer variables once the unsupervised training process has completed. We showcase our approach with numerical results for the canonical maximum cut and maximum independent set problems. We find that the graph neural network optimizer performs on par or outperforms existing solvers, with the ability to scale beyond the state of the art to problems with millions of variables.},
archivePrefix = {arXiv},
arxivId = {2107.01188},
author = {Schuetz, Martin J.A. and Brubaker, J. Kyle and Katzgraber, Helmut G.},
doi = {10.1038/s42256-022-00468-6},
issn = {25225839},
journal = {Nature Machine Intelligence},
number = {4},
pages = {367--377},
publisher = {Springer US},
title = {{Combinatorial optimization with physics-inspired graph neural networks}},
volume = {4},
year = {2022}
}
@inproceedings{Norouzi2009,
abstract = {In this paper we present a method for learning classspecific features for recognition. Recently a greedy layerwise procedure was proposed to initialize weights of deep belief networks, by viewing each layer as a separate Restricted Boltzmann Machine (RBM). We develop the Convolutional RBM (C-RBM), a variant of the RBM model in which weights are shared to respect the spatial structure of images. This framework learns a set of features that can generate the images of a specific object class. Our feature extraction model is a four layer hierarchy of alternating filtering and maximum subsampling. We learn feature parameters of the first and third layers viewing them as separate C-RBMs. The outputs of our feature extraction hierarchy are then fed as input to a discriminative classifier. It is experimentally demonstrated that the extracted features are effective for object detection, using them to obtain performance comparable to the state-of-the-art on handwritten digit recognition and pedestrian detection. {\textcopyright}2009 IEEE.},
author = {Norouzi, Mohammad and Ranjbar, Mani and Mori, Greg},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
doi = {10.1109/CVPRW.2009.5206577},
isbn = {9781424439935},
pages = {2735--2742},
title = {{Stacks of convolutional restricted boltzmann machines for shift-invariant feature learning}},
year = {2009}
}
@article{Angelini2023,
author = {Angelini, Maria Chiara and Ricci-Tersenghi, Federico},
doi = {10.1038/s42256-022-00589-y},
file = {:Users/shixin/Cloud/newwork/intern/reflib_since2107/s42256-022-00589-y.pdf:pdf},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
month = {dec},
number = {1},
pages = {29--31},
publisher = {Springer US},
title = {{Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set}},
volume = {5},
year = {2022}
}
@article{Boettcher2023,

author = {Boettcher, Stefan},
doi = {10.1038/s42256-022-00587-0},

issn = {25225839},
journal = {Nature Machine Intelligence},
number = {1},
pages = {24--25},
publisher = {Springer US},
title = {{Inability of a graph neural network heuristic to outperform greedy algorithms in solving combinatorial optimization problems}},
volume = {5},
year = {2023}
}
@article{Farhi2014,
abstract = {We introduce a quantum algorithm that produces approximate solutions for combinatorial optimization problems. The algorithm depends on a positive integer p and the quality of the approximation improves as p is increased. The quantum circuit that implements the algorithm consists of unitary gates whose locality is at most the locality of the objective function whose optimum is sought. The depth of the circuit grows linearly with p times (at worst) the number of constraints. If p is fixed, that is, independent of the input size, the algorithm makes use of efficient classical preprocessing. If p grows with the input size a different strategy is proposed. We study the algorithm as applied to MaxCut on regular graphs and analyze its performance on 2-regular and 3-regular graphs for fixed p. For p = 1, on 3-regular graphs the quantum algorithm always finds a cut that is at least 0.6924 times the size of the optimal cut.},
archivePrefix = {arXiv},
arxivId = {1411.4028},
author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam},
eprint = {1411.4028},
journal = {arXiv:1411.4028},
keywords = {QAOA,seminal},
mendeley-tags = {QAOA,seminal},
month = {nov},
title = {{A Quantum Approximate Optimization Algorithm}},
year = {2014}
}
@article{Johnson2011,
abstract = {Many interesting but practically intractable problems can be reduced to that of finding the ground state of a system of interacting spins; however, finding such a ground state remains computationally difficult. It is believed that the ground state of some naturally occurring spin systems can be effectively attained through a process called quantum annealing. If it could be harnessed, quantum annealing might improve on known methods for solving certain types of problem. However, physical investigation of quantum annealing has been largely confined to microscopic spins in condensed-matter systems. Here we use quantum annealing to find the ground state of an artificial Ising spin system comprising an array of eight superconducting flux quantum bits with programmable spin-spin couplings. We observe a clear signature of quantum annealing, distinguishable from classical thermal annealing through the temperature dependence of the time at which the system dynamics freezes. Our implementation can be configured in situ to realize a wide variety of different spin networks, each of which can be monitored as it moves towards a low-energy configuration. This programmable artificial spin network bridges the gap between the theoretical study of ideal isolated spin networks and the experimental investigation of bulk magnetic samples. Moreover, with an increased number of spins, such a system may provide a practical physical means to implement a quantum algorithm, possibly allowing more-effective approaches to solving certain classes of hard combinatorial optimization problems. {\textcopyright} 2011 Macmillan Publishers Limited. All rights reserved.},
author = {Johnson, M. W. and Amin, M. H. S. and Gildert, S. and Lanting, T. and Hamze, F. and Dickson, N. and Harris, R. and Berkley, A. J. and Johansson, J. and Bunyk, P. and Chapple, E. M. and Enderud, C. and Hilton, J. P. and Karimi, K. and Ladizinsky, E. and Ladizinsky, N. and Oh, T. and Perminov, I. and Rich, C. and Thom, M. C. and Tolkacheva, E. and Truncik, C. J. S. and Uchaikin, S. and Wang, J. and Wilson, B. and Rose, G.},
doi = {10.1038/nature10012},
file = {:Users/shixin/Cloud/newwork/intern/reflib_since2107/nature10012.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
month = {may},
number = {7346},
pages = {194--198},
title = {{Quantum annealing with manufactured spins}},
volume = {473},
year = {2011}
}
@article{Farhi2001,
abstract = {A quantum system will stay near its instantaneous ground state if the Hamiltonian that governs its evolution varies slowly enough. This quantum adiabatic behavior is the basis of a new class of algorithms for quantum computing. We tested one such algorithm by applying it to randomly generated hard instances of an NP-complete problem. For the small examples that we could simulate, the quantum adiabatic algorithm worked well, providing evidence that quantum computers (if large ones can be built) may be able to outperform ordinary computers on hard sets of instances of NP-complete problems.},

author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam and Lapan, Joshua and Lundgren, Andrew and Preda, Daniel},
doi = {10.1126/science.1057726},

issn = {0036-8075},
journal = {Science},
month = {apr},
number = {5516},
pages = {472--475},
pmid = {11313487},
title = {{A Quantum Adiabatic Evolution Algorithm Applied to Random Instances of an NP-Complete Problem}},
volume = {292},
year = {2001}
}
@article{Hauke2020,
author = {Hauke, Philipp and Katzgraber, Helmut G and Lechner, Wolfgang and Nishimori, Hidetoshi and Oliver, William D},
doi = {10.1088/1361-6633/ab85b8},
issn = {0034-4885},
journal = {Reports on Progress in Physics},
month = {may},
number = {5},
pages = {054401},
title = {{Perspectives of quantum annealing: methods and implementations}},
volume = {83},
year = {2020}
}
@article{Kadowaki1998,
abstract = {We introduce quantum fluctuations into the simulated annealing process of optimization problems, aiming at faster convergence to the optimal state. Quantum fluctuations cause transitions between states and thus play the same role as thermal fluctuations in the conventional approach. The idea is tested by the transverse Ising model, in which the transverse field is a function of time similar to the temperature in the conventional method. The goal is to find the ground state of the diagonal part of the Hamiltonian with high accuracy as quickly as possible. We have solved the time-dependent Schr{\"{o}}dinger equation numerically for small size systems with various exchange interactions. Comparison with the results of the corresponding classical (thermal) method reveals that the quantum annealing leads to the ground state with much larger probability in almost all cases if we use the same annealing schedule. {\textcopyright} 1998 The American Physical Society.},

author = {Kadowaki, Tadashi and Nishimori, Hidetoshi},
doi = {10.1103/PhysRevE.58.5355},

issn = {1063651X},
journal = {Physical Review E},
number = {5},
pages = {5355--5363},
title = {{Quantum annealing in the transverse Ising model}},
volume = {58},
year = {1998}
}
@article{Hibat-Allah2021,
abstract = {Many important challenges in science and technology can be cast as optimization problems. When viewed in a statistical physics framework, these can be tackled by simulated annealing, where a gradual cooling procedure helps search for ground-state solutions of a target Hamiltonian. Although powerful, simulated annealing is known to have prohibitively slow sampling dynamics when the optimization landscape is rough or glassy. Here we show that, by generalizing the target distribution with a parameterized model, an analogous annealing framework based on the variational principle can be used to search for ground-state solutions. Modern autoregressive models such as recurrent neural networks provide ideal parameterizations because they can be sampled exactly without slow dynamics, even when the model encodes a rough landscape. We implement this procedure in the classical and quantum settings on several prototypical spin glass Hamiltonians and find that, on average, it substantially outperforms traditional simulated annealing in the asymptotic limit, illustrating the potential power of this yet unexplored route to optimization.},

author = {Hibat-Allah, Mohamed and Inack, Estelle M. and Wiersema, Roeland and Melko, Roger G. and Carrasquilla, Juan},
doi = {10.1038/s42256-021-00401-3},

issn = {25225839},
journal = {Nature Machine Intelligence},
number = {11},
pages = {952--961},
publisher = {Springer US},
title = {{Variational neural annealing}},
volume = {3},
year = {2021}
}
@article{Arute2020,
abstract = {We demonstrate the application of the Google Sycamore superconducting qubit quantum processor to combinatorial optimization problems with the quantum approximate optimization algorithm (QAOA). Like past QAOA experiments, we study performance for problems defined on the (planar) connectivity graph of our hardware; however, we also apply the QAOA to the Sherrington-Kirkpatrick model and MaxCut, both high dimensional graph problems for which the QAOA requires significant compilation. Experimental scans of the QAOA energy landscape show good agreement with theory across even the largest instances studied (23 qubits) and we are able to perform variational optimization successfully. For problems defined on our hardware graph we obtain an approximation ratio that is independent of problem size and observe, for the first time, that performance increases with circuit depth. For problems requiring compilation, performance decreases with problem size but still provides an advantage over random guessing for circuits involving several thousand gates. This behavior highlights the challenge of using near-term quantum computers to optimize problems on graphs differing from hardware connectivity. As these graphs are more representative of real world instances, our results advocate for more emphasis on such problems in the developing tradition of using the QAOA as a holistic, device-level benchmark of quantum processors.},

author = {Harrigan, Matthew P. and Sung, Kevin J. and Neeley, Matthew and Satzinger, Kevin J. and Arute, Frank and Arya, Kunal and Atalaya, Juan and Bardin, Joseph C. and Barends, Rami and Boixo, Sergio and Broughton, Michael and Buckley, Bob B. and Buell, David A. and Burkett, Brian and Bushnell, Nicholas and Chen, Yu and Chen, Zijun and {Ben Chiaro} and Collins, Roberto and Courtney, William and Demura, Sean and Dunsworth, Andrew and Eppens, Daniel and Fowler, Austin and Foxen, Brooks and Gidney, Craig and Giustina, Marissa and Graff, Rob and Habegger, Steve and Ho, Alan and Hong, Sabrina and Huang, Trent and Ioffe, L. B. and Isakov, Sergei V. and Jeffrey, Evan and Jiang, Zhang and Jones, Cody and Kafri, Dvir and Kechedzhi, Kostyantyn and Kelly, Julian and Kim, Seon and Klimov, Paul V. and Korotkov, Alexander N. and Kostritsa, Fedor and Landhuis, David and Laptev, Pavel and Lindmark, Mike and Leib, Martin and Martin, Orion and Martinis, John M. and McClean, Jarrod R. and McEwen, Matt and Megrant, Anthony and Mi, Xiao and Mohseni, Masoud and Mruczkiewicz, Wojciech and Mutus, Josh and Naaman, Ofer and Neill, Charles and Neukart, Florian and Niu, Murphy Yuezhen and O'Brien, Thomas E. and O'Gorman, Bryan and Ostby, Eric and Petukhov, Andre and Putterman, Harald and Quintana, Chris and Roushan, Pedram and Rubin, Nicholas C. and Sank, Daniel and Skolik, Andrea and Smelyanskiy, Vadim and Strain, Doug and Streif, Michael and Szalay, Marco and Vainsencher, Amit and White, Theodore and Yao, Z. Jamie and Yeh, Ping and Zalcman, Adam and Zhou, Leo and Neven, Hartmut and Bacon, Dave and Lucero, Erik and Farhi, Edward and Babbush, Ryan},
doi = {10.1038/s41567-020-01105-y},
issn = {1745-2473},
journal = {Nature Physics},
month = {mar},
number = {3},
pages = {332--336},
title = {{Quantum approximate optimization of non-planar graph problems on a planar superconducting processor}},
volume = {17},
year = {2021}
}
@article{Pelofske2023,
abstract = {Quantum annealing (QA) and Quantum Alternating Operator Ansatz (QAOA) are both heuristic quantum algorithms intended for sampling optimal solutions of combinatorial optimization problems. In this article we implement a rigorous direct comparison between QA on D-Wave hardware and QAOA on IBMQ hardware. The studied problems are instances of a class of Ising problems, with variable assignments of $+1$ or $-1$, that contain cubic $ZZZ$ interactions (higher order terms) and match both the native connectivity of the Pegasus topology D-Wave chips and the heavy hexagonal lattice of the IBMQ chips. The novel QAOA implementation on the heavy hexagonal lattice has a CNOT depth of $6$ per round and allows for usage of an entire heavy hexagonal lattice. Experimentally, QAOA is executed on an ensemble of randomly generated Ising instances with a grid search over $1$ and $2$ round angles using all 127 programmable superconducting transmon qubits of ibm_washington. The error suppression technique digital dynamical decoupling (DDD) is also tested on all QAOA circuits. QA is executed on the same Ising instances with the programmable superconducting flux qubit devices D-Wave Advantage_system4.1 and Advantage_system6.1 using modified annealing schedules with pauses. We find that QA outperforms QAOA on all problem instances. We also find that DDD enables 2-round QAOA to outperform 1-round QAOA, which is not the case without DDD.},
archivePrefix = {arXiv},
arxivId = {2301.00520},
author = {Pelofske, Elijah and B{\"{a}}rtschi, Andreas and Eidenbenz, Stephan},
eprint = {2301.00520},
file = {:Users/shixin/Cloud/newwork/intern/reflib_since2107/2301.00520.pdf:pdf},
journal = {arXiv:2301.00520},
title = {{Quantum Annealing vs. QAOA: 127 Qubit Higher-Order Ising Problems on NISQ Computers}},
year = {2023}
}
@article{Larkin2022,
abstract = {The quantum approximate optimization algorithm (QAOA) is a hybrid quantum–classical algorithm to solve binary-variable optimization problems. Due to the short circuit depth and its expected robustness to systematic errors it is a promising candidate likely to run on near-term quantum devices. We simulate the performance of QAOA applied to the Max-Cut problem and compare it with some of the best classical alternatives. When comparing solvers, their performance is characterized by the computational time taken to achieve a given quality of solution. Since QAOA is based on sampling, we utilize performance metrics based on the probability of observing a sample above a certain quality. In addition, we show that the QAOA performance varies significantly with the graph type. In particular for three-regular random graphs, QAOA performance shows improvement by up to two orders of magnitude compared to previous estimates, strongly reducing the performance gap with classical alternatives. This was possible by reducing the number of function evaluations per iteration and optimizing the variational parameters on small graph instances and transferring to large via training. Because QAOA's performance guarantees are only known for limited applications and contexts, we utilize a framework for the search for quantum advantage which incorporates a large number of problem instances and all three classical solver modalities: exact, approximate, and heuristic.},
author = {Larkin, Jason and Jonsson, Mat{\'{i}}as and Justice, Daniel and Guerreschi, Gian Giacomo},
doi = {10.1088/2058-9565/ac6973},
issn = {2058-9565},
journal = {Quantum Science and Technology},
keywords = {NISQ,optimization,quantum computing},
month = {oct},
number = {4},
pages = {045014},
title = {{Evaluation of QAOA based on the approximation ratio of individual samples}},
volume = {7},
year = {2022}
}
@article{Verdon2019a,
abstract = {Quantum Neural Networks (QNNs) are a promising variational learning paradigm with applications to near-term quantum processors, however they still face some significant challenges. One such challenge is finding good parameter initialization heuristics that ensure rapid and consistent convergence to local minima of the parameterized quantum circuit landscape. In this work, we train classical neural networks to assist in the quantum learning process, also know as meta-learning, to rapidly find approximate optima in the parameter landscape for several classes of quantum variational algorithms. Specifically, we train classical recurrent neural networks to find approximately optimal parameters within a small number of queries of the cost function for the Quantum Approximate Optimization Algorithm (QAOA) for MaxCut, QAOA for Sherrington-Kirkpatrick Ising model, and for a Variational Quantum Eigensolver for the Hubbard model. By initializing other optimizers at parameter values suggested by the classical neural network, we demonstrate a signifi-cant improvement in the total number of optimization iterations required to reach a given accuracy. We further demonstrate that the optimization strategies learned by the neural network generalize well across a range of problem instance sizes. This opens up the possibility of training on small, classically simulatable problem instances, in order to initialize larger, classically intractably simulatable problem instances on quantum devices, thereby significantly reducing the number of required quantum-classical optimization iterations.},
archivePrefix = {arXiv},
arxivId = {1907.05415},
author = {Verdon, Guillaume and Broughton, Michael and McClean, Jarrod R. and Sung, Kevin J. and Babbush, Ryan and Jiang, Zhang and Neven, Hartmut and Mohseni, Masoud},
eprint = {1907.05415},
issn = {23318422},
journal = {arXiv:1907.05415},
title = {{Learning to learn with quantum neural networks via classical neural networks}},
year = {2019}
}
@article{Tate2022,
abstract = {We study the Quantum Approximate Optimization Algorithm ( QAOA ) in the context of the Max-Cut problem. Noisy quantum devices are only able to accurately execute QAOA at low circuit depths, while classically-challenging problem instances may call for a relatively high circuit-depth. This is due to the need to build correlations between reachable pairs of vertices in potentially large graphs [ 16 ]. To enhance the solving power of low-depth QAOA, we introduce a classical pre-processing step that initializes QAOA with a biased superposition of possible cuts in the graph, referred to as a warm-start . In particular, we initialize QAOA with a solution to a low-rank semidefinite programming relaxation of the Max-Cut problem. Our experimental results show that this variant of QAOA , called QAOA-warm , is able to outperform standard QAOA on lower circuit depths in solution quality and training time. While this improvement is partly due to the classical warm-start, we find strong evidence of further improvement using QAOA circuit at small depth. We provide experimental evidence of improved performance as well as theoretical properties of the proposed framework.},

author = {Tate, Reuben and Farhadi, Majid and Herold, Creston and Mohler, Greg and Gupta, Swati},
doi = {10.1145/3549554},


issn = {2643-6809},
journal = {ACM Transactions on Quantum Computing},
month = {jun},
number = {2},
pages = {1--39},
title = {{Bridging Classical and Quantum with SDP initialized warm-starts for QAOA}},
volume = {4},
year = {2023}
}
@article{Campos2021,
abstract = {The quantum approximate optimization algorithm (QAOA) is the most studied gate-based variational quantum algorithm today. We train QAOA one layer at a time to maximize overlap with an n qubit target state. Doing so we discovered that such training always saturates - called training saturation - at some depth p∗, meaning that past a certain depth, overlap cannot be improved by adding subsequent layers. We formulate necessary conditions for saturation. Numerically, we find layerwise QAOA reaches its maximum overlap at depth p∗=n for the problem of state preparation. The addition of coherent dephasing errors to training removes saturation, recovering robustness to layerwise training. This study sheds new light on the performance limitations and prospects of QAOA.},
author = {Campos, E. and Rabinovich, D. and Akshay, V. and Biamonte, J.},
doi = {10.1103/PhysRevA.104.L030401},
issn = {2469-9926},
journal = {Physical Review A},
month = {sep},
number = {3},
pages = {L030401},
title = {{Training saturation in layerwise quantum approximate optimization}},
volume = {104},
year = {2021}
}
@inproceedings{Shaydulin2021,
abstract = {Understanding the best known parameters, performance, and systematic behavior of the Quantum Approximate Optimization Algorithm (QAOA) remain open research questions, even as the algorithm gains popularity. We introduce QAOAKit, a Python toolkit for the QAOA built for exploratory research. QAOAKit is a unified repository of preoptimized QAOA parameters and circuit generators for common quantum simulation frameworks. We combine, standardize, and cross-validate previously known parameters for the MaxCut problem, and incorporate this into QAOAKit. We also build conversion tools to use these parameters as inputs in several quantum simulation frameworks that can be used to reproduce, compare, and extend known results from various sources in the literature. We describe QAOAKit and provide examples of how it can be used to reproduce research results and tackle open problems in quantum optimization.},
author = {Shaydulin, Ruslan and Marwaha, Kunal and Wurtz, Jonathan and Lotshaw, Phillip C.},
booktitle = {2021 IEEE/ACM Second International Workshop on Quantum Computing Software (QCS)},
doi = {10.1109/QCS54837.2021.00011},

isbn = {978-1-7281-8674-0},
keywords = {open quantum software,quantum approximate optimization algorithm},
month = {nov},
pages = {64--71},
publisher = {IEEE},
title = {{QAOAKit: A Toolkit for Reproducible Study, Application, and Verification of the QAOA}},
volume = {50},
year = {2021}
}
@article{Jain2022,
abstract = {Approximate combinatorial optimisation has emerged as one of the most promising application areas for quantum computers, particularly those in the near term. In this work, we focus on the quantum approximate optimisation algorithm (QAOA) for solving the MaxCut problem. Specifically, we address two problems in the QAOA, how to initialise the algorithm, and how to subsequently train the parameters to find an optimal solution. For the former, we propose graph neural networks (GNNs) as a warm-starting technique for QAOA. We demonstrate that merging GNNs with QAOA can outperform both approaches individually. Furthermore, we demonstrate how graph neural networks enables warm-start generalisation across not only graph instances, but also to increasing graph sizes, a feature not straightforwardly available to other warm-starting methods. For training the QAOA, we test several optimisers for the MaxCut problem up to 16 qubits and benchmark against vanilla gradient descent. These include quantum aware/agnostic and machine learning based/neural optimisers. Examples of the latter include reinforcement and meta-learning. With the incorporation of these initialisation and optimisation toolkits, we demonstrate how the optimisation problems can be solved using QAOA in an end-to-end differentiable pipeline.},
author = {Jain, Nishant and Coyle, Brian and Kashefi, Elham and Kumar, Niraj},
doi = {10.22331/q-2022-11-17-861},
issn = {2521-327X},
journal = {Quantum},
month = {nov},
pages = {861},
title = {{Graph neural network initialisation of quantum approximate optimisation}},
volume = {6},
year = {2022}
}
@article{Shaydulin2022,
abstract = {Finding high-quality parameters is a central obstacle to using the quantum approximate optimization algorithm (QAOA). Previous work partially addresses this issue for QAOA on unweighted MaxCut problems by leveraging similarities in the objective landscape among different problem instances. However, we show that the more general weighted MaxCut problem has significantly modified objective landscapes, with a proliferation of poor local optima. Our main contribution is a simple rescaling scheme that overcomes these deleterious effects of weights. We show that for a given QAOA depth, a single "typical" vector of QAOA parameters can be successfully transferred to weighted MaxCut instances. This transfer leads to a median decrease in the approximation ratio of only 2.0 percentage points relative to a considerably more expensive direct optimization on a dataset of 34,701 instances with up to 20 nodes and multiple weight distributions. This decrease can be reduced to 1.2 percentage points at the cost of only 10 additional QAOA circuit evaluations with parameters sampled from a pretrained metadistribution, or the transferred parameters can be used as a starting point for a single local optimization run to obtain approximation ratios equivalent to those achieved by exhaustive optimization in $96.35\%$ of our cases.},
archivePrefix = {arXiv},
arxivId = {2201.11785},
author = {Shaydulin, Ruslan and Lotshaw, Phillip C. and Larson, Jeffrey and Ostrowski, James and Humble, Travis S.},
eprint = {2201.11785},
journal = {arXiv:2201.11785},
month = {jan},
title = {{Parameter Transfer for Quantum Approximate Optimization of Weighted MaxCut}},
year = {2022}
}
@article{Moussa2022,
abstract = {As combinatorial optimization is one of the main quantum computing applications, many methods based on parameterized quantum circuits are being developed. In general, a set of parameters are being tweaked to optimize a cost function out of the quantum circuit output. One of these algorithms, the Quantum Approximate Optimization Algorithm stands out as a promising approach to tackling combinatorial problems. However, finding the appropriate parameters is a difficult task. Although QAOA exhibits concentration properties, they can depend on instances characteristics that may not be easy to identify, but may nonetheless offer useful information to find good parameters. In this work, we study unsupervised Machine Learning approaches for setting these parameters without optimization. We perform clustering with the angle values but also instances encodings (using instance features or the output of a variational graph autoencoder), and compare different approaches. These angle-finding strategies can be used to reduce calls to quantum circuits when leveraging QAOA as a subroutine. We showcase them within Recursive-QAOA up to depth 3 where the number of QAOA parameters used per iteration is limited to 3, achieving a median approximation ratio of 0.94 for MaxCut over 200 Erdős-R{\'{e}}nyi graphs. We obtain similar performances to the case where we extensively optimize the angles, hence saving numerous circuit calls.},
author = {Moussa, Charles and Wang, Hao and B{\"{a}}ck, Thomas and Dunjko, Vedran},
doi = {10.1140/epjqt/s40507-022-00131-4},
file = {:Users/shixin/Cloud/newwork/intern/reflib_since2107/2202.09408.pdf:pdf},
issn = {2662-4400},
journal = {EPJ Quantum Technology},
keywords = {Clustering,Combinatorial optimization,Quantum Approximate Optimization Algorithm,Quantum computing},
month = {dec},
number = {1},
pages = {11},
title = {{Unsupervised strategies for identifying optimal parameters in Quantum Approximate Optimization Algorithm}},
volume = {9},
year = {2022}
}
@article{Amosy2022,
abstract = {The quantum approximate optimization algorithm (QAOA) is a leading iterative variational quantum algorithm for heuristically solving combinatorial optimization problems. A large portion of the computational effort in QAOA is spent by the optimization steps, which require many executions of the quantum circuit. Therefore, there is active research focusing on finding better initial circuit parameters, which would reduce the number of required iterations and hence the overall execution time. While existing methods for parameter initialization have shown great success, they often offer a single set of parameters for all problem instances. We propose a practical method that uses a simple, fully connected neural network that leverages previous executions of QAOA to find better initialization parameters tailored to a new given problem instance. We benchmark state-of-the-art initialization methods for solving the MaxCut problem of Erd\H{o}s-R\'enyi graphs using QAOA and show that our method is consistently the fastest to converge while also yielding the best final result. Furthermore, the parameters predicted by the neural network are shown to match very well with the fully optimized parameters, to the extent that no iterative steps are required, thereby effectively realizing an iterative-free QAOA scheme.},
archivePrefix = {arXiv},
arxivId = {2208.09888},
author = {Amosy, Ohad and Danzig, Tamuz and Porat, Ely and Chechik, Gal and Makmal, Adi},
eprint = {2208.09888},
file = {:Users/shixin/Cloud/newwork/intern/reflib_since2107/2208.09888.pdf:pdf},
journal = {arXiv:2208.09888},
month = {aug},
title = {{Iterative-Free Quantum Approximate Optimization Algorithm Using Neural Networks}},
year = {2022}
}
@article{Sack2022,
abstract = {The QAOA is a variational quantum algorithm, where a quantum computer implements a variational ansatz consisting of p layers of alternating unitary operators and a classical computer is used to optimize the variational parameters. For a random initialization the optimization typically leads to local minima with poor performance, motivating the search for initialization strategies of QAOA variational parameters. Although numerous heuristic intializations were shown to have a good numerical performance, an analytical understanding remains evasive. Inspired by the study of energy landscapes, in this work we focus on so-called transition states (TS) that are saddle points with a unique negative curvature direction that connects to local minima. Starting from a local minimum of QAOA with p layers, we analytically construct 2p + 1 TS for QAOA with p + 1 layers. These TS connect to new local minima, all of which are guaranteed to lower the energy compared to the minimum found for p layers. We introduce a Greedy procedure to effectively maneuver the exponentially increasing number of TS and corresponding local minima. The performance of our procedure matches the best available initialization strategy, and in addition provides a guarantee for the minimal energy to decrease with an increasing number of layers p. Generalization of analytic TS and the Greedy approach to other ans\"atze may provide a universal framework for initialization of variational quantum algorithms.},
archivePrefix = {arXiv},
arxivId = {2209.01159},
author = {Sack, Stefan H. and Medina, Raimel A. and Kueng, Richard and Serbyn, Maksym},
eprint = {2209.01159},
journal = {arXiv:2209.01159},
month = {sep},
title = {{Transition states and greedy exploration of the QAOA optimization landscape}},
year = {2022}
}
@article{Yao2022,
abstract = {Variational quantum algorithms stand at the forefront of simulations on near-term and future fault-tolerant quantum devices. While most variational quantum algorithms involve only continuous optimization variables, the representational power of the variational ansatz can sometimes be significantly enhanced by adding certain discrete optimization variables, as is exemplified by the generalized quantum approximate optimization algorithm (QAOA). However, the hybrid discrete-continuous optimization problem in the generalized QAOA poses a challenge to the optimization. We propose a new algorithm called MCTS-QAOA, which combines a Monte Carlo tree search method with an improved natural policy gradient solver to optimize the discrete and continuous variables in the quantum circuit, respectively. We find that MCTS-QAOA has excellent noise-resilience properties and outperforms prior algorithms in challenging instances of the generalized QAOA.},
archivePrefix = {arXiv},
arxivId = {2203.16707},
author = {Yao, Jiahao and Li, Haoya and Bukov, Marin and Lin, Lin and Ying, Lexing},
eprint = {2203.16707},
file = {:Users/shixin/Cloud/newwork/intern/reflib_since2107/2203.16707.pdf:pdf},
journal = {arXiv:2203.16707},
month = {mar},
title = {{Monte Carlo Tree Search based Hybrid Optimization of Variational Quantum Circuits}},
year = {2022}
}
@article{Xie2022,
abstract = {The Quantum approximate optimization algorithm (QAOA) is a quantum-classical hybrid algorithm aiming to produce approximate solutions for combinatorial optimization problems. In the QAOA, the quantum part prepares a quantum parameterized state that encodes the solution, where the parameters are optimized by a classical optimizer. However, it is difficult to find optimal parameters when the quantum circuit becomes deeper. Hence, there is numerous active research on the performance and the optimization cost of QAOA. In this work, we build a convolutional neural network to predict parameters of depth QAOA instance by the parameters from the depth QAOA counterpart. We propose two strategies based on this model. First, we recurrently apply the model to generate a set of initial values for a certain depth QAOA. It successfully initiates depth 10 QAOA instances, whereas each model is only trained with the parameters from depths less than 6. Second, the model is applied repetitively until the maximum expected value is reached. An average approximation ratio of 0.9759 for Max-Cut over 264 Erd\H{o}s-R\'{e}nyi graphs is obtained, while the optimizer is only adopted for generating the first input of the model.},
archivePrefix = {arXiv},
arxivId = {2211.09513},
author = {Xie, Ningyi and Lee, Xinwei and Cai, Dongsheng and Saito, Yoshiyuki and Asai, Nobuyoshi},
eprint = {2211.09513},
journal = {arXiv:2211.09513},
month = {nov},
title = {{Quantum Approximate Optimization Algorithm Parameter Prediction Using a Convolutional Neural Network}},
year = {2022}
}
@inproceedings{Alam2020,
abstract = {We propose a machine learning based approach to accelerate quantum approximate optimization algorithm (QAOA) implementation which is a promising quantum-classical hybrid algorithm to prove the so-called quantum supremacy. In QAOA, a parametric quantum circuit and a classical optimizer iterates in a closed loop to solve hard combinatorial optimization problems. The performance of QAOA improves with increasing number of stages (depth) in the quantum circuit. However, two new parameters are introduced with each added stage for the classical optimizer increasing the number of optimization loop iterations. We note a correlation among parameters of the lower-depth and the higher-depth QAOA implementations and, exploit it by developing a machine learning model to predict the gate parameters close to the optimal values. As a result, the optimization loop converges in a fewer number of iterations. We choose graph MaxCut problem as a prototype to solve using QAOA. We perform a feature extraction routine using 100 different QAOA instances and develop a training data-set with 13, 860 optimal parameters. We present our analysis for 4 flavors of regression models and 4 flavors of classical optimizers. Finally, we show that the proposed approach can curtail the number of optimization iterations by on average 44.9% (up to 65.7%) from an analysis performed with 264 flavors of graphs.},
archivePrefix = {arXiv},
arxivId = {2002.01089},
author = {Alam, Mahabubul and Ash-Saki, Abdullah and Ghosh, Swaroop},
booktitle = {2020 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
doi = {10.23919/DATE48585.2020.9116348},
eprint = {2002.01089},
file = {:Users/shixin/Cloud/newwork/intern/reflib_since2107/2002.01089.pdf:pdf},
isbn = {978-3-9819263-4-7},
month = {mar},
pages = {686--689},
publisher = {IEEE},
title = {{Accelerating Quantum Approximate Optimization Algorithm using Machine Learning}},
year = {2020}
}
@article{Khairy2020,
abstract = {Quantum computing is a computational paradigm with the potential to outperform classical methods for a variety of problems. Proposed recently, the Quantum Approximate Optimization Algorithm (QAOA) is considered as one of the leading candidates for demonstrating quantum advantage in the near term. QAOA is a variational hybrid quantum-classical algorithm for approximately solving combinatorial optimization problems. The quality of the solution obtained by QAOA for a given problem instance depends on the performance of the classical optimizer used to optimize the variational parameters. In this paper, we formulate the problem of finding optimal QAOA parameters as a learning task in which the knowledge gained from solving training instances can be leveraged to find high-quality solutions for unseen test instances. To this end, we develop two machine-learning-based approaches. Our first approach adopts a reinforcement learning (RL) framework to learn a policy network to optimize QAOA circuits. Our second approach adopts a kernel density estimation (KDE) technique to learn a generative model of optimal QAOA parameters. In both approaches, the training procedure is performed on small-sized problem instances that can be simulated on a classical computer; yet the learned RL policy and the generative model can be used to efficiently solve larger problems. Extensive simulations using the IBM Qiskit Aer quantum circuit simulator demonstrate that our proposed RL- and KDE-based approaches reduce the optimality gap by factors up to 30.15 when compared with other commonly used off-the-shelf optimizers.},
author = {Khairy, Sami and Shaydulin, Ruslan and Cincio, Lukasz and Alexeev, Yuri and Balaprakash, Prasanna},
doi = {10.1609/aaai.v34i03.5616},
eprint = {1911.11071},
file = {:Users/shixin/Cloud/newwork/intern/reflib_since2107/1911.11071.pdf:pdf},
issn = {2374-3468},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
month = {apr},
number = {03},
pages = {2367--2375},
title = {{Learning to Optimize Variational Quantum Circuits to Solve Combinatorial Problems}},
volume = {34},
year = {2020}
}
@article{Bittel2021,
abstract = {Variational quantum algorithms (VQAs) are proposed to solve relevant computational problems on near term quantum devices. Popular versions are variational quantum eigensolvers (VQEs) and quantum approximate optimization algorithms (QAOAs) that solve ground state problems from quantum chemistry and binary optimization problems, respectively. They are based on the idea to use a classical computer to train a parameterized quantum circuit. We show that the corresponding classical optimization problems are NP-hard. Moreover, the hardness is robust in the sense that for every polynomial time algorithm, there exists instances for which the relative error resulting from the classical optimization problem can be arbitrarily large, unless P = NP. Even for classically tractable systems, composed of only logarithmically many qubits or free fermions, we show that the optimization is NP-hard. This elucidates that the classical optimization is intrinsically hard and does not merely inherit the hardness from the ground state problem. Our analysis shows that the training landscape can have many far from optimal persistent local minima. This means gradient and higher order decent algorithms will generally converge to far from optimal solutions.},
author = {Bittel, Lennart and Kliesch, Martin},
doi = {10.1103/PhysRevLett.127.120502},
issn = {0031-9007},
journal = {Physical Review Letters},
month = {sep},
number = {12},
pages = {120502},
title = {{Training Variational Quantum Algorithms Is NP-Hard}},
volume = {127},
year = {2021}
}
@article{Anschuetz2021,
abstract = {One of the most important properties of neural networks is the clustering of local minima of the loss function near the global minimum, enabling efficient training. Though generative models implemented on quantum computers are known to be more expressive than their traditional counterparts, it has empirically been observed that these models experience a transition in the quality of their local minima. Namely, below some critical number of parameters, all local minima are far from the global minimum in function value; above this critical parameter count, all local minima are good approximators of the global minimum. Furthermore, for a certain class of quantum generative models, this transition has empirically been observed to occur at parameter counts exponentially large in the problem size, meaning practical training of these models is out of reach. Here, we give the first proof of this transition in trainability, specializing to this latter class of quantum generative model. We use techniques inspired by those used to study the loss landscapes of classical neural networks. We also verify that our analytic results hold experimentally even at modest model sizes.},
archivePrefix = {arXiv},
arxivId = {2109.06957},
author = {Anschuetz, Eric R},
eprint = {2109.06957},
file = {:Users/shixin/Cloud/newwork/intern/reflib_since2107/2109.06957.pdf:pdf},
journal = {arXiv:2109.06957},
month = {sep},
title = {{Critical Points in Quantum Generative Models}},
year = {2021}
}
@article{McClean2018,
abstract = {Many experimental proposals for noisy intermediate scale quantum devices involve training a parameterized quantum circuit with a classical optimization loop. Such hybrid quantum-classical algorithms are popular for applications in quantum simulation, optimization, and machine learning. Due to its simplicity and hardware efficiency, random circuits are often proposed as initial guesses for exploring the space of quantum states. We show that the exponential dimension of Hilbert space and the gradient estimation complexity make this choice unsuitable for hybrid quantum-classical algorithms run on more than a few qubits. Specifically, we show that for a wide class of reasonable parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. We argue that this is related to the 2-design characteristic of random circuits, and that solutions to this problem must be studied.},
author = {McClean, Jarrod R. and Boixo, Sergio and Smelyanskiy, Vadim N and Babbush, Ryan and Neven, Hartmut},
doi = {10.1038/s41467-018-07090-4},
issn = {2041-1723},
journal = {Nature Communications},
month = {dec},
number = {1},
pages = {4812},
title = {{Barren plateaus in quantum neural network training landscapes}},
volume = {9},
year = {2018}
}
@article{Marrero2020,
abstract = {We argue that an excess in entanglement between the visible and hidden units in a Quantum Neural Network can hinder learning. In particular, we show that quantum neural networks that satisfy a volume-law in the entanglement entropy will give rise to models not suitable for learning with high probability. Using arguments from quantum thermodynamics, we then show that this volume law is typical and that there exists a barren plateau in the optimization landscape due to entanglement. More precisely, we show that for any bounded objective function on the visible layers, the Lipshitz constants of the expectation value of that objective function will scale inversely with the dimension of the hidden-subsystem with high probability. We show how this can cause both gradient descent and gradient-free methods to fail. We note that similar problems can happen with quantum Boltzmann machines, although stronger assumptions on the coupling between the hidden/visible subspaces are necessary. We highlight how pretraining such generative models may provide a way to navigate these barren plateaus.},

author = {{Ortiz Marrero}, Carlos and Kieferov{\'{a}}, M{\'{a}}ria and Wiebe, Nathan},
doi = {10.1103/PRXQuantum.2.040316},
issn = {2691-3399},
journal = {PRX Quantum},
month = {oct},
number = {4},
pages = {040316},
title = {{Entanglement-Induced Barren Plateaus}},
volume = {2},
year = {2021}
}
@article{Wang2020,
abstract = {Variational Quantum Algorithms (VQAs) may be a path to quantum advantage on Noisy Intermediate-Scale Quantum (NISQ) computers. A natural question is whether noise on NISQ devices places fundamental limitations on VQA performance. We rigorously prove a serious limitation for noisy VQAs, in that the noise causes the training landscape to have a barren plateau (i.e., vanishing gradient). Specifically, for the local Pauli noise considered, we prove that the gradient vanishes exponentially in the number of qubits n if the depth of the ansatz grows linearly with n . These noise-induced barren plateaus (NIBPs) are conceptually different from noise-free barren plateaus, which are linked to random parameter initialization. Our result is formulated for a generic ansatz that includes as special cases the Quantum Alternating Operator Ansatz and the Unitary Coupled Cluster Ansatz, among others. For the former, our numerical heuristics demonstrate the NIBP phenomenon for a realistic hardware noise model.},

author = {Wang, Samson and Fontana, Enrico and Cerezo, M. and Sharma, Kunal and Sone, Akira and Cincio, Lukasz and Coles, Patrick J.},
doi = {10.1038/s41467-021-27045-6},

issn = {2041-1723},
journal = {Nature Communications},
month = {dec},
number = {1},
pages = {6961},
title = {{Noise-induced barren plateaus in variational quantum algorithms}},
volume = {12},
year = {2021}
}
@article{Arrasmith2020,
abstract = {Barren plateau landscapes correspond to gradients that vanish exponentially in the number of qubits. Such landscapes have been demonstrated for variational quantum algorithms and quantum neural networks with either deep circuits or global cost functions. For obvious reasons, it is expected that gradient-based optimizers will be significantly affected by barren plateaus. However, whether or not gradient-free optimizers are impacted is a topic of debate, with some arguing that gradient-free approaches are unaffected by barren plateaus. Here we show that, indeed, gradient-free optimizers do not solve the barren plateau problem. Our main result proves that cost function differences, which are the basis for making decisions in a gradient-free optimization, are exponentially suppressed in a barren plateau. Hence, without exponential precision, gradient-free optimizers will not make progress in the optimization. We numerically confirm this by training in a barren plateau with several gradient-free optimizers (Nelder-Mead, Powell, and COBYLA algorithms), and show that the numbers of shots required in the optimization grows exponentially with the number of qubits.},
author = {Arrasmith, Andrew and Cerezo, M and Czarnik, Piotr and Cincio, Lukasz and Coles, Patrick J},
doi = {10.22331/q-2021-10-05-558},
file = {:Users/shixin/Cloud/newwork/intern/reflib_since2107/2011.12245.pdf:pdf},
issn = {2521-327X},
journal = {Quantum},
month = {oct},
pages = {558},
title = {{Effect of barren plateaus on gradient-free optimization}},
volume = {5},
year = {2021}
}

@article{Zhang2020b,
abstract = {Quantum architecture search (QAS) is the process of automating architecture engineering of quantum circuits. It has been desired to construct a powerful and general QAS platform which can significantly accelerate current efforts to identify quantum advantages of error-prone and depth-limited quantum circuits in the NISQ era. Hereby, we propose a general framework of differentiable quantum architecture search (DQAS), which enables automated designs of quantum circuits in an end-to-end differentiable fashion. We present several examples of circuit design problems to demonstrate the power of DQAS. For instance, unitary operations are decomposed into quantum gates, noisy circuits are re-designed to improve accuracy, and circuit layouts for quantum approximation optimization algorithm are automatically discovered and upgraded for combinatorial optimization problems. These results not only manifest the vast potential of DQAS being an essential tool for the NISQ application developments, but also present an interesting research topic from the theoretical perspective as it draws inspirations from the newly emerging interdisciplinary paradigms of differentiable programming, probabilistic programming, and quantum programming.},
author = {Zhang, Shi-Xin and Hsieh, Chang-Yu and Zhang, Shengyu and Yao, Hong},
doi = {10.1088/2058-9565/ac87cd},
issn = {2058-9565},
journal = {Quantum Science and Technology},
month = {oct},
number = {4},
pages = {045023},
title = {{Differentiable quantum architecture search}},
volume = {7},
year = {2022}
}
@article{Weidinger,
abstract = {Solving optimization problems on near term quantum devices requires developing error mitigation techniques to cope with hardware decoherence and dephasing processes. We propose a mitigation technique based on the LHZ architecture. This architecture uses a redundant encoding of logical variables to solve optimization problems on fully programmable planar quantum chips. We discuss how this redundancy can be exploited to mitigate errors in quantum optimization algorithms. In the specific context of the quantum approximate optimization algorithm (QAOA), we show that errors can be significantly mitigated by appropriately modifying the objective cost function.},
archivePrefix = {arXiv},
arxivId = {2301.05042},
author = {Weidinger, Anita and Mbeng, Glen Bigan and Lechner, Wolfgang},
eprint = {2301.05042},
file = {:Users/shixin/Cloud/newwork/intern/reflib_since2107/2301.05042.pdf:pdf},
journal = {arXiv:2301.05042},
month = {jan},
title = {{Error Mitigation for Quantum Approximate Optimization}},
year = {2023}
}


@article{wang2021noise,
  title={Noise-induced barren plateaus in variational quantum algorithms},
  author={Wang, Samson and Fontana, Enrico and Cerezo, Marco and Sharma, Kunal and Sone, Akira and Cincio, Lukasz and Coles, Patrick J},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={6961},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{bravyi2021mitigating,
  title={Mitigating measurement errors in multiqubit experiments},
  author={Bravyi, Sergey and Sheldon, Sarah and Kandala, Abhinav and Mckay, David C and Gambetta, Jay M},
  journal={Physical Review A},
  volume={103},
  number={4},
  pages={042605},
  year={2021},
  publisher={APS}
}

@article{nation2021scalable,
  title={Scalable mitigation of measurement errors on quantum computers},
  author={Nation, Paul D and Kang, Hwajung and Sundaresan, Neereja and Gambetta, Jay M},
  journal={PRX Quantum},
  volume={2},
  number={4},
  pages={040326},
  year={2021},
  publisher={APS}
}


@article{temme2017error,
  title={Error mitigation for short-depth quantum circuits},
  author={Temme, Kristan and Bravyi, Sergey and Gambetta, Jay M},
  journal={Physical review letters},
  volume={119},
  number={18},
  pages={180509},
  year={2017},
  publisher={APS}
}


@article{li2017efficient,
  title={Efficient variational quantum simulator incorporating active error minimization},
  author={Li, Ying and Benjamin, Simon C},
  journal={Physical Review X},
  volume={7},
  number={2},
  pages={021050},
  year={2017},
  publisher={APS}
}

