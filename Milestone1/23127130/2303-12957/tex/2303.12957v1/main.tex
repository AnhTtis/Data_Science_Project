\documentclass[11pt]{article}

% Changes for arXiv version
\usepackage{arxiv}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{hyperref}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

% Definitions of handy macros can go here


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[export]{adjustbox}
\usepackage{booktabs} % for professional tables
\usepackage{color}
\usepackage{multirow}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
%\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{cases}
\usepackage{enumerate}
\newtheorem{assumption}{Assumption}

\def\E{\mathbb{E}}
\def\V{\mathbb{V}{\bf ar}}
\def\C{\mathbb{C}{\bf ov}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\def\tr{{\bf tr}}
% hack to add equations in tabbing mode
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\tagthisaux}{%
\refstepcounter{equation}%
{\textnormal{(\theequation)}}%
}
\newcommand{\tagthisline}{\`\tagthisaux} 
\newcommand{\dataset}{{\mathcal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\doop}{\textrm{do}}
\newcommand{\dom}{\textrm{dom}}

\begin{document}

\title{Reinforcement Learning with Exogenous States and Rewards}

\author{George Trimponias\thanks{Equal contributions}\\ 
       Amazon\\
       Luxembourg\\ 
       \texttt{trimpog@amazon.lu}\\
       \And
       Thomas G. Dietterich$^*$\\
       Collaborative Robotics and Intelligent Systems (CoRIS) Institute\\
       Oregon State University, Corvallis, OR 97331 USA\\
       \texttt{tgd@cs.orst.edu}
     	}

\renewcommand{\shorttitle}{Reinforcement Learning with Exogenous States and Rewards}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms can be applied during reinforcement learning to discover the exogenous space, remove the exogenous reward, and focus reinforcement learning on the endogenous MDP. Experiments on a variety of challenging synthetic MDPs show that these methods, applied online, discover large exogenous state spaces and produce substantial speedups in reinforcement learning.

{\bf Keywords:} Reinforcement learning, exogenous state variables, Markov Decision Processes, Markov Reward Processes, causal discovery
\end{abstract}

\section{Introduction} \label{sec:introduction}

In many practical settings, the actions of an agent have only a limited effect on the environment. For example, in a wireless cellular network, the cell tower base stations have many parameters that must be dynamically controlled to optimize network performance. We can formulate this as a Markov Decision Process (MDP) in which the reward function is the negative of the number of users who are suffering from low bandwidth.  However, this reward is heavily influenced by exogenous factors such as the number, location, and behavior of the cellular network customers. Customer demand varies stochastically as a function of exogenous factors (news, sporting events, traffic accidents). In addition, atmospheric conditions can affect the capacity of each wireless channel.  This high degree of stochasticity can confuse reinforcement learning algorithms, because during exploration, the benefit of trying action $a$ in state $s$ is hard to determine because the change in reward is obscured by the exogenous components of the reward. Many trials are required to average away these exogenous components so that the effect of the action can be measured. For temporal difference algorithms, such as Q Learning, the learning rate needs to be very small. For policy gradient algorithms, the number of Monte Carlo trials required to estimate the gradient grows very large (or equivalently, the step size must be very small). In this paper, we analyze this setting and develop algorithms for automatically detecting and removing the effects of exogenous state variables. This accelerates reinforcement learning (RL).

The first part of the paper defines exogenous state variables based on a causal foundation. It then introduces Exogenous State MDPs, which capture a broad class of MDPs with exogenous states. We characterize the space of all Exogenous State MDPs in terms of constraints on the structure of the corresponding two-time step dynamic Bayesian network. The paper analyzes the properties of exogenous subspaces of the state space and proves that every Exogenous State MDP has a unique maximal exogenous subspace that contains all other exogenous subspaces. The paper then shows that, under the assumption that the reward function decomposes additively into exogenous and endogenous components, the Bellman equation for the original MDP decomposes into two equations: one for an exogenous Markov reward process (Exo-MRP) and the other for an endogenous MDP (Endo-MDP). Importantly, every optimal policy for the Endo-MDP is an optimal policy for the full MDP. 

In the second part of the paper, we identify one condition---based on the covariance between the endogenous and exogenous returns---under which solving the Endo-MDP is faster (in sample complexity) than solving the full MDP. To do this, we derive dynamic programming updates for the covariance between the $H$-horizon returns of the Exo-MRP and the Endo-MDP, which may be of independent interest. 

The third part of the paper formulates the problem of discovering the Exo/Endo Decomposition assuming we have access to a sufficiently large and representative sample of $\langle s, a ,r, s'\rangle$ tuples recorded from MDP trajectories. The problem is formalized as a constrained optimization problem where the objective is to find an exogenous subspace that minimizes the variance of the endogenous reward subject to conditional mutual information constraints that enforce the Exogenous State MDP structure. We introduce and study several variations of this constrained optimization problem. For the case of linear subspaces, the mutual information constraints can be replaced by constraints on a quantity called the conditional correlation coefficient (CCC), and this yields practical algorithms. This section concludes with a discussion of the conditions that the MDP and the exploration policy should satisfy such that the sample of $\langle s, a ,r, s'\rangle$ tuples is sufficient for identifying violations of the conditional mutual information constraints and ensure that the variables in the discovered exogenous subspace are causally exogenous.

The fourth part of the paper introduces two algorithms for solving the conditional correlation formulation and studies their soundness and correctness. One algorithm solves a series of global optimization problems, and the other is a stepwise algorithm that identifies one exogenous dimension at a time. We prove the correctness of the global algorithm, and we develop some insights into when the stepwise algorithm works well. 

Finally, the fifth part of the paper articulates a set of research questions and executes experiments to answer those questions. The main finding is that the Exo/Endo decomposition algorithms can discover large exogenous subspaces that yield substantial speedups in reinforcement learning for the high-performing PPO actor-critic algorithm. The experiments also study the sensitivity of our algorithms to their hyperparameters and suggest a practical strategy for setting them. We also explore the behavior of the algorithms on MDPs with nonlinear rewards, nonlinear dynamics, and discrete states. 
Connections to previous work are interleaved throughout the paper. 

\begin{table}[t!]
\centering
\begin{tabular}{|c|c|} 
 \hline
 \textbf{Symbol} & \textbf{Meaning}\\  
 \hline
 endo, exo & endogenous, exogenous\\  
 \hline
  CMI & Conditional Mutual Information\\
 \hline
  CCC & Conditional Correlation Coefficient\\
 \hline
 $\mathcal{S},\mathcal{E},\mathcal{X},\mathcal{A}$ & State, Endo State, Exo State, and Action Spaces\\
 \hline
 $S/S'$ & Random Vector for Current/Next State\\
 \hline
 $s,a,r$ & Realizations of the State, Action, and Reward\\
  \hline
  $\mathcal{S}_i,S_i,s_i$ & The $i^{th}$ component of $\mathcal{S},S,s$\\
  \hline
  $\mathbf{S,S',A,R}$ & Observational data for state, next state, action, reward\\
  \hline
 $\mathcal{P(B)}$ & Space of probability distributions over space $\mathcal{B}$\\
  \hline
 $S=(E,X)$ & Decomposition of $\mathcal{S}$ into endo and exo sets $E$ and $X$\\
 \hline
  $\subseteq, \subset$ & Subset of, Strict (proper) subset of\\
 \hline
 $[d]$ & The set $\{1,\dots, d\}$\\
 \hline
 $\mathcal{I},\mathcal{I}^c$ & Index set ($\subseteq[d]$), Complement of $\mathcal{I}$ ($=[d] \setminus \mathcal{I}$)\\
 \hline
 $\mathcal{S}[\mathcal{I}],S[\mathcal{I}],s[\mathcal{I}]$ & $(\mathcal{S}_i)_{i\in\mathcal{I}},(S_i)_{i\in\mathcal{I}},(s_i)_{i\in\mathcal{I}}$\\
 \hline
 $I(A;B\mid C)$ & CMI of random vectors $A$ and $B$ given $C$\\
 \hline
 $A\independent B\mid C$ & Conditional independence of $A$ and $B$ given $C$\\
\hline
 $\mathbb{R},\mathbb{R}^d$ & Set of real numbers, Real $d$-dimensional vector space\\
\hline
 $\mathbb{R}^{m\times n}$ & The vector space of $m\times n$ matrices over $\mathbb{R}$\\
  \hline
 $S\stackrel{W}{=}[E,X]$ & Linear decomposition of $S$ into endo/exo parts $E$ and $X$ via $W$ \\ 
\hline
 $\mathcal{A}+\mathcal{B},\mathcal{A}\oplus\mathcal{B}$ & Sum (Direct Sum) of vector spaces $\mathcal{A}$ and $\mathcal{B}$\\
 \hline
$\mathcal{A}\sqsubseteq\mathcal{B},\mathcal{A}\sqsubset\mathcal{B}$ & $\mathcal{A}$ is a vector subspace (proper subspace) of vector space $\mathcal{B}$\\
\hline
 $\mathcal{A}^{\perp}$ & Orthogonal complement of subspace $\mathcal{A}$ of vector space $\mathcal{S}$ \\
\hline
 $\dim(\mathcal{A})$ & Dimension of vector subspace $\mathcal{A}$\\
\hline
 $\mathbb{I}_{n},\mathbf{0}^{m\times n}$ & Identity matrix of size $n$, Matrix of zeros of size $m\times n$\\
\hline
 $W_{exo}$ & Matrix that defines the linear exogenous subspace\\
\hline
 $\tr(A),A^{\top},\det(A)$ & Trace of matrix $A$, Transpose of $A$, Determinant of $A$ \\
\hline
 $\|u\|_2$ &Euclidean norm of vector $u$ \\
\hline 
 $\Sigma_{AA}, \Sigma_{AB}$ & Covariance matrix of $A$, Cross-covariance matrix of $A,B$ \\
\hline
$\mathcal{N}(\mu,\sigma^2)$ & Gaussian distribution with mean $\mu$ and variance $\sigma^2$ \\
\hline
\end{tabular}
\caption{Symbols and Abbreviations.}
\label{table:symbols}
\end{table}


\section{MDPs with Exogenous States and Rewards}
We study discrete time stationary MDPs with stochastic rewards and stochastic transitions \citep{Puterman1994,Sutton1998}; the state and action spaces may be either discrete or continuous. Notation: state space $\cal{S}$, action space $\cal{A}$, reward distribution $R\!:\cal{S}\times \mathcal{A}\mapsto \mathcal{P}(\mathbb{R})$ (where $\mathcal{P}(\mathbb{R})$ is the space of probability distributions over the real numbers), transition function $P\!: \cal{S} \times \cal{A} \mapsto \mathcal{P(S)}$ (where $\mathcal{P(S)}$ is the space of probability distributions over $\cal{S}$), starting state distribution $P_0\! \in \mathcal{P(S)}$, and discount factor $\gamma \in (0,1]$. We assume that for all $(s,a)\in {\cal S}\times {\cal A}$, $R(s,a)$ has expected value $m(s,a)$ and finite variance $\sigma^2(s,a)$.  We denote random variables by capital letters ($S$, $A$, etc.)\ and their corresponding values by lower case letters ($s$, $a$, etc.). Table~\ref{table:symbols} summarizes the notation employed in this paper.

Let the state space $\mathcal{S}$ take the form $\mathcal{S}=\times_{i=1}^d\mathcal{S}_i$, where $\mathcal{S}_i$ defines the domain of the $i^{th}$ state variable. In our problems the domain of each variable is either the real numbers or a finite, discrete set of values.
Each state $s \in \mathcal{S}$ can then be written as a $d$-tuple of the values of these state variables $s=(s_1,\dots,s_d)$, with $s_i\in\mathcal{S}_i$. 
We refer to $s_i$ as the value of the $i^{th}$ state variable. 
We denote by $S_t=\times_{i=i}^d S_{t,i}$ the random vector for the state at time $t$. Similarly, $A_t$ is the random variable for the action at time $t$, and $R_t$ is the random variable for the reward at time $t$. In some formulas, instead of indexing by time, we will use ``prime'' notation. For example, $S$ and $S'$ denote the current and next states, respectively (and analogously for $A$ and $A'$, $R$ and $R'$).  When it is clear from the context, we will also refer to $S_i$ as the $i$-th state variable. In the terminology of \cite{Koller2009}, $S_i$ is a ``template variable'' that refers to the family of random variables that correspond to $\mathcal{S}_i$ at all time steps: $\{S_{1,i}, S_{2,i},\ldots,S_{H,i}\}$. 

We are interested in problems where the set of state variables $S$ can be decomposed into endogenous and exogenous sets $E$ and $X$. In the simplest case, this can be accomplished by variable selection. Following the notation of \cite{efroni2022sample-efficient}, define an index set $\mathcal{I}$ as a subset of $[d]=\{1,\dots,d\}$ and $\mathcal{I}^c=[d]\setminus \mathcal{I}$ as its complement. The variable selection formulation aims to discover an index set $\mathcal{I}$ so that the state vector $S=\times_{i=1}^dS_i$ can be decomposed into two disjoint sets of state variables $X=\times_{i\in{\mathcal{I}}}S_i=S[\mathcal{I}]$ and $E=\times_{i\in{\mathcal{I}}^c}S_i=S[\mathcal{I}^c]$.  We will also denote the corresponding exo and endo state spaces as $\mathcal{X}=\times_{i\in\mathcal{I}}\mathcal{S}_i=\mathcal{S}[\mathcal{I}]$ and $\mathcal{E}=\times_{i\in\mathcal{I}^c}\mathcal{S}_i=\mathcal{S}[\mathcal{I}^c]$.

In many problems, the given state variables do not neatly separate into endogenous and exogenous subsets. Instead, we must discover a mapping $\xi: U \mapsto V$ such that the first $d_{exo}$ dimensions of $\xi(\mathcal{S})$ provide the exogenous state variables, and the remaining $d-d_{exo}$ dimensions give the endogenous state variables. In this paper, we will be interested in the case where $\xi$ is a full-rank linear transformation. In general, we will argue that $\xi$ should a diffeomorphism so that no information in the original space ${\cal S}$ is lost in the new space $\xi(\mathcal{S})$. 

\subsection{Exogenous State Variables}
\label{sec:exogenous-state-variables}

The notion of exogeneity is fundamentally causal: a variable is exogenous if it is impossible for our actions to affect its value. We formalize this in terms of Pearl's do-calculus. 


\begin{definition}[Causally-Exogenous Variables]\label{def:exogenous}
A set of state variables $X = S[\mathcal{I}]$ is \emph{causally exogenous} for MDP $\mathcal{M}$ with causal graph $\mathcal{G}$ if and only if for all times $t<H$, graph $\mathcal{G}$ encodes the conditional independence
\begin{equation}
P(X_{t+1}, \ldots, X_H\mid X_t, \textnormal{do}(A_t=a_t)) = P(X_{t+1}, \ldots, X_H\mid X_t) \quad\forall a_t \in \mathcal{A}.
\end{equation}
\end{definition}

Causal exogeneity is a qualitative property that depends only on the structure of the causal graph $\mathcal{G}$ and not on the parameters of the probability distributions associated with each node. 


\begin{figure}[t!]
     \centering
         \includegraphics[scale=0.8]{fully-connected-diagram-cropped.pdf}
         \caption{A fully-connected dynamic Bayesian network for state decomposition $S=(E,X)$ (reward function not shown).}
         \label{fig:fully-connected}
\end{figure}

Consider any state decomposition of $S$ based on an index set $\mathcal{I}\subseteq[d]$, so that $X=S[\mathcal{I}]$ and $E=S[\mathcal{I}^c]$.
Assuming the MDP is stationary, its causal graph can be described by the 2-timestep Dynamic Bayesian Network shown in Figure~\ref{fig:fully-connected}, which includes all possible edges between $X$, $E$, and actions $A$ at the current time step and $X'$ and $E'$ in the next time step. We have included diachronic edges (from one time step to the next) and synchronic edges (within a single time step). 
At most one of the two synchronic edges connecting $X'$ and $E'$ can be present in an MDP. In this diagram, $E, X, E',$ and $X'$ denote sets of state variables, and if there is an edge between two nodes $U \rightarrow V$, it means there exists at least one edge between one variable in $U$ and one variable in $V$. 

\begin{figure}[b!]
    \centering
    \includegraphics[scale=0.8]{partial_setting-cropped}
    \caption{Restricted dynamic Bayesian network sufficient to establish that $X$ is exogenous. We call this the ``full setting'' because it includes the synchronic edge from $X'$ to $E'$.}
    \label{fig:full-setting}
\end{figure}

We now derive a condition on the structure of the DBN causal graph that is sufficient to ensure that the variables in $X$ are exogenous. 

\begin{theorem}[Full Exogenous DBN]
\label{theorem:exo-DBN}
Any DBN with a causal graph matching the structure of Figure~\ref{fig:full-setting} and any DBN graph obtained by deleting edges from this structure, when unrolled for $H$ time steps, yields an MDP for which $X$ is causally exogenous. We call the state decomposition $S=(E,X)$ a \emph{full exo/endo decomposition} with \emph{exogenous set} $X$.
\end{theorem}
\begin{proof}
Figure~\ref{fig:unrolled-setting} shows the DBN causal graph unrolled over the $H$-step horizon. To obtain the result, we will apply Rule 3 of the do-calculus to remove the do$(A_t)$ action. We will treat $A_0, \ldots, A_{t-1}, A_{t+1}, \ldots, A_{H-1}$ as random variables distributed according to (possibly stochastic) policy $\pi$. We must show that the result holds for all possible policies. 

Rule 3 states that for any causal graph $\mathcal{G}$ 
\[
P(F \mid \doop(G),\doop(H),J)=P(F \mid \doop(G),J), \textrm{ if } F\independent H \mid G \cup J \textrm{ in } \tilde{\mathcal{G}},
\]
where $\tilde{\mathcal{G}}$ is the graph obtained by first deleting all edges pointing into $G$ and then deleting all arrows pointing into $H$, if $H$ is not an ancestor of $J$. 

Consider the query $P(X_{t+1}, \ldots, X_H\mid X_t, \textnormal{do}(A_t))$ and apply Rule 3 with the following bindings: $F\gets X_{t+1},\dots,X_H$, $G\gets \emptyset$, $H\gets A_t$, and $J\gets X_t$; in that case, $\tilde{\mathcal{G}}$ will be the same as $\mathcal{G}$, except that the incoming edges to $A_t$ from $X_t$ and $E_t$ are removed. 

Now apply the $d$-separation procedure to $\tilde{\mathcal{G}}$ by forming the graph of all variables mentioned in the query $P(X_{t+1}, \ldots, X_H\mid X_t)$ and their ancestors. This ancestral graph contains only the singleton node $A_t$ and the Markov chain $P(X_{0})\cdot P(X_{1}\mid X_{0})\cdots P(X_{t+1}\mid X_t)\cdots P(X_H\mid X_{H-1})$. The fact that $A_T$ is disconnected from the Markov chain establishes that $X_{t+1},\dots,X_H\independent A_t \mid X_t$. Applying Rule 3 gives the result. Note that the policy $\pi$ plays no role in the derivation. Hence, the result applies for any possible policy.

Deleting edges from the DBN graph cannot introduce new ancestors, so $X$ remains exogenous under edge deletion.
\end{proof}

\begin{figure}
     \centering
     \includegraphics[scale=0.8]{unrolled-diagram-H-cropped}
     \caption{Unrolled state transition diagram for the full exo DBN.}
     \label{fig:unrolled-setting}
\end{figure}

\begin{figure}[b!]
     \centering
         \includegraphics[scale=0.8]{causality_counterexample-cropped}
         \caption{State transition diagram of MDP with 3 state variables.}
         \label{fig:example_MDP1}
\end{figure}

Next, we examine some properties of exo/endo decompositions.

We first note that it is possible for an MDP with exogenous state variables to accept multiple exo/endo decompositions. In Figure \ref{fig:example_MDP1}, the decompositions $(X_1=\{S_2\},E_1=\{S_1,S_3\})$ and $(X_2=\{S_1,S_2\},E_3=\{S_3\})$  are both valid decompositions, since they match the full DBN template of Figure \ref{fig:full-setting}. This shows that the set $E$ in an exo/endo decomposition $(E,X)$ may contain additional exogenous state variables not in $X$. 

\begin{theorem}[Union of Exo/Endo Decompositions]\label{theorem:decomposition-union}
Assume an MDP accepts two full exo/endo decompositions $(E_1,X_1)$ and $(E_2,X_2)$. Define the union of the two decompositions as the state decomposition $(E,X)$ with $X=X_1\cup X_2$ and $E=E_1\cap E_2$. Then $(E,X)$ is a full exo/endo decomposition with exo state set $X$.
\end{theorem}
\begin{proof}
Because $X_1$ is exogenous, there are no edges from any of the remaining state variables $E_1$ and no edges from the action $A$ to any variable in $X_1$. Similarly, there are no edges from nodes in $E_2$ or $A$ to nodes in $X_2$. Hence, the union $X=X_1\cup X_2$ also has no incoming edges from $E_1$, $E_2$, or $A$, and therefore has no incoming edges from $E$. This shows that the decomposition $(E,X)$ satisfies the conditions of Theorem~\ref{theorem:exo-DBN}. 
\end{proof}

On the other hand, not every subset of exogenous state variables can yield a valid exo/endo decomposition. 

\begin{theorem}[Exogenous Subsets] \label{theorem:bad-subsets}
Let $(X,E)$ be a full exo/endo decomposition and $X_1$ and $X_2$ be non-empty, disjoint proper subsets of $X$, $X_1 \cap X_2 = \emptyset$, such that their disjoint union gives back $X$: $X_1 \cup X_2 = X$. Then $(X_1, X_2 \cup E)$ is not necessarily a valid full exo/endo decomposition.
\end{theorem}
\begin{proof} By example. In Figure~\ref{fig:example_MDP1}, the decomposition $X_3=\{S_1\},E_3=\{S_2,S_3\}$ is not a valid full exo/endo decomposition due to the link from $E'_3$ to $X'_3$ (specifically the link $S'_2\to S'_1$). 
\end{proof}

For an MDP with exogenous state variables, we will be interested in the exo/endo decomposition where the exo set $X$ is as large as possible. This is formalized in the next definition:

\begin{definition}[Maximal Exo/Endo Decomposition]
Given an MDP, the full exo/endo decomposition $(E,X)$ is maximal if there is no other full exo/endo decomposition $(\tilde{E},\tilde{X})$ with $|\tilde{X}|>|X|$. We denote the maximal decomposition as $(E_m,X_m)$ and call $X_m$ the \emph{maximal exo set}.
\end{definition}

\begin{corollary}[Uniqueness of Maximal Exo/Endo Decomposition]\label{cor:decomposition-uniqueness}
The maximal \\exo/endo decomposition of any MDP is unique.
\end{corollary}
\begin{proof}
By contradiction. Suppose that there are two distinct maximal decompositions $(E_1,X_1)$ and $(E_2,X_2)$. By Theorem~\ref{theorem:decomposition-union}, their union would be a full exo/endo decomposition with an exo state set of higher cardinality. This contradicts the assumption that $(E_1,X_1)$ and $(E_2,X_2)$ were maximal.
\end{proof}

\begin{corollary}[Containment for Maximal Exo/Endo Decomposition]\label{cor:decomposition-containment}
For the maximal exo/endo decomposition $(E_m,X_m)$, it holds that $X_m\supseteq X$, where $X$ is the exo set of any full exo/endo decomposition $(E,X)$.
\end{corollary}
\begin{proof}
By contradiction. Suppose there exists a decomposition $(E,X)$ so that $X\not\subseteq X_m$. By Theorem~\ref{theorem:decomposition-union}, we could take the union of $(E,X)$ and $(E_m,X_m)$ to get a new full exo/endo decomposition with exo set $X\cup X_m\supset X$ of higher cardinality than $X$. 
\end{proof}


In our work, we specifically focus on MDPs with exogenous state variables that match the full exogenous DBN.
\begin{definition}[Exogenous State MDP]
Any MDP whose structure matches Figure~\ref{fig:full-setting} or any subset of its edges is called an \emph{exogenous state MDP} with exo set $X$. 
\end{definition}

We will analyze two types of exogenous state MDPs: The \emph{full setting} shown in Figure~\ref{fig:full-setting} and the \emph{diachronic setting} where the edge $X' \rightarrow E'$ is removed, as shown in Figure~\ref{fig:diachronic-setting}. 
\begin{figure}
    \centering
    \includegraphics[scale=0.8]{full_setting-cropped}
    \caption{The Diachronic Exogenous State MDP.}
    \label{fig:diachronic-setting}
\end{figure}
Note that in the full setting, the MDP dynamics can be factored as follows
\begin{equation} \label{eq:fundamental_equation}
    P(E',X' \mid E,X,A) = P(X' \mid X)\cdot P(E' \mid E,X,A,X').
\end{equation}
Similarly, in the diachronic setting, the MDP dynamics can be factored as
\begin{equation}\label{eq:diachronic-factorization}
    P(E',X' \mid E,X,A) = P(X' \mid X)\cdot P(E' \mid E,X,A).
\end{equation}

It is useful to draw a distinction between state variables that are \emph{causally} exogenous and state variables that are \emph{statistically} exogenous. 
\begin{definition}[Statistically-Exogenous Variables]
    A set of state variables $X = S[\mathcal{I}] $ is statistically exogenous in MDP $\mathcal{M}$ if the transition probability distribution $P(E',X'|E,X,A)$ can be factored according to either (\ref{eq:fundamental_equation}) or (\ref{eq:diachronic-factorization}). 
\end{definition}

In reinforcement learning, we typically start with an MDP for which we do not know anything about the causal graph beyond the fact that it defines an MDP. However, by collecting data and fitting a model of the transition probabilities, we may infer that the MDP dynamics satisfy (\ref{eq:fundamental_equation}) or (\ref{eq:diachronic-factorization}). In such cases, we can conclude that $X$ is statistically exogenous but we cannot infer that $X$ is causally exogenous. In Section~\ref{sec:soundness}, we will discuss additional assumptions required to conclude that a set of state variables that are statistically exogenous are also causally exogenous. In Appendix~\ref{app:additional-theory}, we provide a structural characterization of exogenous state variables and MDPs with exogenous state variables, and show that under certain conditions the maximal exo set contains all exogenous state variables, with the exception of some edge cases.

\subsection{Additive Reward Decomposition}

In this paper, we identify and analyze a case where reinforcement learning can be accelerated even when the exogenous variables are all relevant to the policy, the dynamics, and the reward. This case arises when the reward function can be decomposed additively into two functions, $R_{exo}$, which only depends on $X$, and $R_{end}$, which can depend on both $X$ and $E$. 

\begin{definition}[Additively Decomposable Exogenous State MDP]\label{def:additive-MDP}
An \emph{Additively\\ Decomposable Exogenous State MDP} is an Exogenous State MDP whose reward function can be decomposed into the sum of two terms
\[R(x,e,a) = R_{exo}(x) + R_{end}(x,e,a),\]
where  $R_{exo}\!:\!{\mathcal X} \mapsto \mathcal{P}(\mathbb{R})$ is the \emph{exogenous reward function} and $R_{end}\!:\!{\mathcal E} \times {\mathcal X} \times {\mathcal A} \mapsto \mathcal{P}(\mathbb{R})$ is the \emph{endogenous reward function}. If the reward function is defined as a distribution over $\mathcal{S}\times\mathcal{A}\times\mathcal{S}$, we instead consider the decomposition $R(x,e,a,x',e') = R_{exo}(x,x') + R_{end}(x,e,a,x',e')$.
\end{definition}
Let $m_{exo}(x)$ and $\sigma^2_{exo}(x) < \infty$ be the mean and variance of the exogenous reward distribution in state $x$. Similarly, let $m_{end}(e,x,a)$ and $\sigma^2_{end}(e,x,a) < \infty$ be the mean and variance of the endogenous reward distribution for state-action pair $(e,x,a)$. 

\begin{theorem}\label{theorem:MDP-decomposition} For any Additively Decomposable Exogenous State MDP with exo set $X$, the $H$-step finite-horizon Bellman optimality equation can be decomposed into two separate equations, one for a Markov Reward Process involving only $X$ and $R_{exo}$ and the other for an MDP (the endo-MDP) involving only $R_{end}$
\begin{align}
    V(e,x;h) &= V_{exo}(x;h) + V_{end}(e,x;h) \label{eqn:sum}\\
    V_{exo}(x;h) &= m_{exo}(x) + \gamma \E_{x'\sim P(x'\mid x)}[V_{exo}(x';h-1)] \label{eqn:exo}\\
    V_{end}(e,x;h) &= \max_{a} m_{end}(e,x,a) + \E_{x'\sim P(x'\mid x); e'\sim P(e'\mid e,x,a)}[V_{end}(e',x';h-1)]. \label{eqn:end}
\end{align}

\end{theorem}
\begin{proof}
We consider the diachronic setting; the full setting can be shown similarly by replacing $P(e'\mid x,e,a)$ by $P(e'\mid x,x',e,a)$.

Proof by induction on the horizon $H$. Note that the expectations could be either sums (if $\mathcal{S}$ is discrete) or integrals (if $\mathcal{S}$ is continuous).

\noindent {\bf Base case:} $H=1$; we take one action and terminate. 
\[V(e,x;1) =  m_{exo}(x) + \max_a m_{end}(x,a).\]
The base case is established by setting $V_{exo}(x;1) = m_{exo}(x)$ and $V_{end}(e,x;1) = \max_a m_{end}(e,x,a)$. 

\noindent {\bf Recursive case:} $H=h$. 
\begin{align*}
    V(e,x;h) = m_{exo}(x) + \max_a \{&m_{end}(e,x,a)\; + \\
    & E_{x'\sim P(x'\mid x); e'\sim P(e'\mid e,x,a)}[V_{exo}(x';h-1) + V_{end}(e',x';h-1)]\}.
\end{align*}
Distribute the expectation over the sum in brackets and simplify. We obtain
\begin{align*}
V(e,x;h) =\; &m_{exo}(x) + \gamma \E_{x'\sim P(x'\mid x)}[V_{exo}(x';h-1)]\; +\\
           &\max_a \{m_{end}(e,x,a) + \gamma \E_{x'\sim P(x'\mid x); e'\sim P(e'\mid e,x,a)}[V_{end}(e',x';h-1)]\}.
\end{align*}
The result is established by setting
\begin{align*}
    V_{exo}(x;h) &= m_{exo}(x) + \gamma \E_{x'\sim P(x'\mid x)}[V_{exo}(x';h-1)]\\
    V_{end}(e,x;h) &= \max_{a}\{ m_{end}(e,x,a) + \gamma \E_{x'\sim P(x'\mid x); e'\sim P(e'\mid e,x,a)}[V_{end}(e',x';h-1)]\}.
\end{align*}
\end{proof}

\begin{corollary}\label{corollary:optimal-policy}
Any optimal policy for the endo-MDP of Equation~\eqref{eqn:end} is an optimal policy for the full exogenous state MDP.
\end{corollary}
\begin{proof}
Because $V_{exo}(s;H)$ does not depend on the policy, the optimal policy can be computed simply by solving the endo-MDP.
\end{proof}  

We will refer to Equations~\eqref{eqn:sum}, \eqref{eqn:exo} and \eqref{eqn:end} as the Exo/Endo Decomposition of the full MDP. 
In the remainder of this paper, unless stated otherwise, we will work with the full setting because it is more general.

Bray (\citeyear{Bray2017}) proves a result similar to Theorem~\ref{theorem:MDP-decomposition}. He also identifies conditions under which value iteration and policy iteration for a fully-specified Endo-MDP can be accelerated by computing the eigenvector decomposition of the endogenous transition matrix. While such techniques are useful for MDP planning with a known transition matrix, we do not know how to exploit them in reinforcement learning where the MDP is unknown.
In other related work, McGregor et al.~(\citeyear{McGregor2017}) show how to remove known exogenous state variables in order to accelerate the Model Free Monte Carlo algorithm \citep{Fonteneau2012}. Their experiments obtain substantial improvements in policy evaluation and reinforcement learning.

\subsection{Variance Analysis of the Exo/Endo Decomposition}
\label{sec:variance-analysis}

Suppose we are given the decomposition of the state space into exogenous and endogenous sets. Under what conditions would reinforcement learning on the endogenous MDP be more efficient than on the original MDP? To explore this question, let us consider the problem of estimating the value of a fixed policy $\pi$ in a given start state $s_0$ via Monte Carlo trials of length $H$. We will compare the sample complexity of estimating $V^\pi(s_0;H)$ on the full MDP to the sample complexity of estimating $V^\pi_{end}(s_0;H)$ on the Endogenous MDP.  Of course most RL algorithms must do more than simply estimate $V^\pi(s_0;H)$ for fixed $\pi$, but the difficulty of estimating $V^\pi(s_0;H)$ is closely related to the difficulty of fitting a value function approximator or estimating the gradient in a policy gradient method.

Define $B^\pi(s_0;H)$ to be a random variable for the $H$-step cumulative discounted return of starting in state $s_0$ and choosing actions according to $\pi$ for $H$ steps. The expected value of $B^\pi(s_0;H)$ is the value function $V^\pi(s_0;H)$.
To compute a Monte Carlo estimate of $V^\pi(s_0;H)$, we will generate $N$ realizations $b_1, \ldots, b_N$ of $B^\pi(s_0;H)$ by executing $N$ $H$-step trajectories in the MDP, each time starting in  $s_0$. 
\begin{theorem}\label{theorem:chebychev}
For any $\epsilon>0$ and any $0<\delta<1$, let $\hat{V}^\pi(s_0;H)=N^{-1} \sum_{i=0}^N b_i$ be the Monte Carlo estimate of the expected $H$-step return of policy $\pi$ starting in state $s_0$. If 
\[N \geq \frac{\V[B^\pi(s_0;H)]}{\delta \epsilon^2}\]
then
\[P[|\hat{V}^\pi(s_0;H) - V^\pi(s_0;H)|  > \epsilon] \leq \delta.\]
\end{theorem}
\begin{proof}
This is a simple application of the Chebychev Inequality, 
\[P(|X - \E[X]|  > \epsilon) \leq \frac{\V[X]}{\epsilon^2},\]
with $X=\hat{V}^\pi(s_0;H)$. The variance of the mean of $N$ iid random variables is the variance of any single variable divided by $N$. Hence $\V[\hat{V}^\pi(s_0;H)] = \V[B^\pi(s_0;H)]/{N}$.
To obtain the result, plug this into the Chebychev inequality, set the rhs equal to $\delta$, and solve for $N$.
\end{proof}

Now let us consider the Exo/Endo decomposition of the MDP. Let $B^\pi_{exo}(s_0;H)$ denote the $H$-step return of the exogenous MRP and $B^\pi_{end}(s_0;H)$ denote the return of the endogenous MDP. Then $B_x^\pi(s_0;H)+B_e^\pi (s_0;H)$ is a random variable denoting the cumulative $H$-horizon discounted return of the original, full MDP. Let $\V[B^\pi(s_0;H))]$ be the variance of $B^\pi(s_0;H)$ and $\C[B_{exo}^\pi(s_0;H), B_{end}^\pi(s_0;H)]$ be the covariance between the exogenous and endogenous returns.

\begin{theorem}\label{theorem:covariance-condition}
The Chebychev upper bound on the number of Monte Carlo trials required to estimate $V^\pi(s_0;H)$ using the endogenous MDP will be reduced compared to the full MDP iff $\V[B_{exo}^\pi(s_0;H)] > -2\;\C[B_{exo}^\pi(s_0;H),B_{end}^\pi(s_0;H)].$
\end{theorem}
\begin{proof}
From Theorem \ref{theorem:chebychev}, we know that the sample size bound using the endogenous MDP will be less than the required sample size using the full MDP when $\V[B_{end}^\pi(s_0;H)] < \V[B_{exo}^\pi(s_0;H) + B_{end}^\pi(s_0;H)]$. The variance of the sum of two random variables is
\begin{align*}
    \V[B_{exo}^\pi(s_0;H) + B_{end}^\pi(s_0;H)] =\; &\V[B_{exo}^\pi(s_0;H)] + \V[B_{end}^\pi(s_0;H)]\; + \\
                                            & 2 \C[B_{exo}^\pi(s_0;H), B_{end}^\pi(s_0;H)].
\end{align*}
Hence,
\[\V[B_{end}^\pi(s_0;H)] < \V[B_{exo}^\pi(s_0;H) + B_{end}^\pi(s_0;H)]\]
if and only if
\[\V[B_{exo}^\pi(s_0;H)] > -2 \C[B_{exo}^\pi(s_0;H), B_{end}^\pi(s_0;H)].\]
\end{proof}

To evaluate this covariance condition, we need to compute the variance and covariance of the $H$-step returns. In Appendix~\ref{app:covariance}, we derive dynamic programming formulas for these quantities. The dynamic programs allow us to check the covariance condition of Theorem \ref{theorem:covariance-condition} in every state, including the start state $s_0$, so that we can decide whether to solve the original MDP or the endo-MDP.  Some special cases are easy to verify. For example, if the mean exogenous reward $m_{exo}(s)=0$, for all states, then the covariance condition reduces to $\sigma^2_{exo}(s) > 0$.

\section{Decomposing an Exogenous State MDP: Optimization Formulations}
\label{sec:optimization-formulations}
We now turn our attention to discovering the exogenous variables (or exogenous subspace) of an MDP from data collected during exploration. Our overall strategy is shown in Algorithm~\ref{alg:framework}. We assume we are applying an online reinforcement learning algorithm, such as PPO or Q-learning. As it interacts with the environment, it collects $\langle s_t, a_t, r_t, s_{t+1} \rangle$ experience tuples into a dataset $D$. After $L$ tuples have been collected, we apply an exogenous space discovery algorithm (see Section~\ref{sec:algorithms}) to find a function $\xi_{exo}$, so that $x_t = \xi_{exo} (s_t)$ computes the exogenous state $x_t$ from state $s_t$. By applying $\xi_{exo}$ to each $s_t$ of the experience tuples, we assemble a supervised training set $D_{exo}$ of the form $\{(x_t, r_t)\}$. We then solve a regression problem to predict as much of the reward $r_t$ from $x_t$ as possible. The resulting fitted function $\hat{m}_{exo}$ is our estimate of the mean of the exogenous reward function. Because of the additive decomposition, we can therefore estimate the endogenous rewards as $\hat{r}_{end,t} := r_t - \hat{m}_{exo}(s_t)$. We then convert the set of experience tuples into modified tuples $\langle s_t, a_t, \hat{r}_{end,t}, s_{t+1}\rangle$ by replacing the original reward $r_t$ values with our estimate of the endogenous reward and resume running the online reinforcement learning algorithm. Depending on the algorithm, we may need to re-initialize the data structures using the modified experience tuples. In any case, as the algorithm collects additional full experience tuples $\langle s_t, a_t, r_t, s_{t+1} \rangle$, each is converted to an experience tuple for the endo-MDP by replacing $r_t$ by $\hat{r}_{end,t}$. In our experiments, we find that there is some benefit to repeating the reward regression at regular intervals, so we also add $(\xi_{exo}(s_i), r_i)$ to $D_{exo}$ at each time step. However, we do not observe similar benefits from rerunning the exogenous space discovery algorithm, especially when the state transition function is linear, so we only execute it once. 

\begin{algorithm}[t!]
\begin{algorithmic}[1]
\STATE {\bf Inputs:} Decomposition steps $L$, Exogenous reward update steps $M$, Policy update steps $K$, Total steps $N$
\STATE {\bf Datasets:} RL training tuples $D$, Exogenous reward examples $D_{exo}$
\STATE {\it //Phase 1}
\STATE Initialize policy and/or value networks randomly
\FOR {$i=1$ \textbf{to} $L$}
\STATE Run RL to collect a new transition $\langle s_i,a_i,r_i,s'_i\rangle$ and add it to $D$
\IF {$i \mod K == 0$}
\STATE Update the policy using last $K$ observations with observed rewards $r_i$
\ENDIF
\ENDFOR
\STATE Run a state decomposition algorithm (see Section \ref{sec:algorithms}) on $D$ to compute the exogenous state mapping $\xi_{exo}$
\STATE Let $D_{exo} \{(\xi_{exo}(s_i), r_i)\}_{i=1}^L$ be the exogenous reward training set
\STATE Fit the exogenous reward function $\hat{m}_{exo}$ to $D_{exo}$
\STATE Update $D$ by replacing each tuple $\langle s_i,a_i,r_i,s'_i\rangle$ by $\langle s_i, a_i, r_i - \hat{m}_{exo}(\xi_{exo}(s_i)),s'_i\rangle$
\STATE {\it //Phase 2}
\FOR {$i=L+1$ \textbf{to} $N$}
\STATE Run RL to collect a new transition $\langle s_i,a_i,r_i,s'_i\rangle$ 
\STATE Add $(\xi_{exo}(s_i), r_i)$ to $D_{exo}$
\STATE Add $\langle s_i, a_i, r_i - \hat{m}_{exo}(\xi_{exo}(s_i)), s'_i\rangle$ to $D$
\IF {$i \mod K == 0$}
\STATE Update the policy using last $K$ observations in $D$
\ENDIF
\IF {$i \mod M == 0$}
\STATE Update the estimate of the exogenous reward function $\hat{m}_{exo}$ with the last $M$ observations in $D_{exo}$
\ENDIF
\ENDFOR
\end{algorithmic}
\caption{Practical RL with the Exo/Endo Decomposition}
\label{alg:framework}
\end{algorithm}


The heart of our approach is the exogenous state discovery problem. This is easiest to discuss for the case where we are given a fixed set of state variables and our goal is to determine which state variables are  exogenous and which are endogenous. Within this variable selection setting, we formulate the discovery problem as one of finding a set of variables that minimizes the residuals of the fitted exogenous reward function $\hat{m}_{exo}$ subject to the constraint that the variables are exogenous. We formulate the exogeneity constraint in terms of conditional mutual information for both the full and the diachronic DBN structures. Then we present two variations of the optimization formulation. The first variation solves the discovery problem jointly by selecting the exogenous features that minimize the residuals of the exogenous reward regression. The second variation takes a hierarchical approach in which we first select the maximal exogenous subspace and then perform the reward regression. We will show that this two-phase algorithm finds the same solution as the joint optimization. This is the approach adopted in Algorithm~\ref{alg:framework}, because it allows us to repeat the reward regressions without recomputing the exogenous subspace.

After presenting these ideas in the context of exogenous variable selection, we then consider the most general formulation in which the exo-endo decomposition is defined by a diffeomorphism that maps from a continuous state space to a space that separates the exogenous and endogenous subspaces. 

Finally, we study a practical special case in which the exogenous and endogenous spaces are defined by a linear mapping. In this setting, we show how to approximate the mutual information constraints by constraints on the conditional correlation coefficients. In Section~\ref{sec:algorithms}, we will then present two algorithms for solving the linear formulation.

\subsection{Variable Selection Formulation}\label{sec:variable-selection-formulation}
The variable selection formulation aims to discover an index set $\mathcal{I}$ with the following two properties:
\begin{itemize}
\item $\mathcal{I}$ decomposes the set of state variables $S=\times_{i=1}^d S_i$  into two disjoint sets $X=\times_{i\in{\mathcal{I}}}S_i$ and $E=\times_{i\in{\mathcal{I}}^c}S_i$ that satisfy the structure of Figure~\ref{fig:full-setting} or Figure~\ref{fig:diachronic-setting}.
\item The squared error of the exogenous reward regression,
$\sum_t [\hat{m}_{exo}(x_t) - r_t]^2$ is minimized, where $\hat{m}_{exo}$ regresses $r_t$ onto $X=\times_{i\in{\mathcal{I}}}S_i$. 
\end{itemize}
How can we state the first property in a form suitable for constrained optimization? We can express the required structure in terms of two conditional mutual information (CMI) constraints. The full setting can be enforced by the constraint
\begin{equation}\label{eq:full-mi-constraint}
I(X';[E,A] \mid X) = 0,
\end{equation}
where $X=S[\mathcal{I}]$ and $E=S[\mathcal{I^C}]$. This says that if we know the value of $X$, then the combination of the endogenous state variables and the action carries no additional information about $X'$, the value of the exogenous state in the next time step. Note that this does not constrain the synchronic link $X' \rightarrow E'$. 

To remove the synchronic link and enforce the diachronic structure, we can strengthen the conditional mutual information constraint as follows
\begin{equation}\label{eq:synchronic-mi-constraint}
I(X';[E,A,E'] \mid X) = 0
\end{equation}
This says that given the exogenous state $X$, $X'$ carries no information about $E, A,$ or $E'$. 

Note that our discovery algorithms will be evaluating these conditional mutual information constraints on the finite sample of tuples collected from exploring the MDP. Below, we will discuss the conditions that must be satisfied by the MDP and the exploration policy to ensure the soundness of these conditional mutual information computations.

\subsubsection{Coupled Formulation}

We can now formalize the exogenous space discovery problem for the full setting as follows

\begin{equation}\label{opt:abstract_formulation_with_reward}
\begin{split}
\mathcal{I}^*,\hat{m}_{exo}^*=&\argmin_{\mathcal{I}\subseteq[d],\hat{m}_{exo}:\mathcal{X} \mapsto \mathbb{R}} \E[(\hat{m}_{exo}(X) - R)^2]\\
&\mbox{subject to } I(X'; [E, A] \mid X)=0,X=S[\mathcal{I}],E=S[\mathcal{I}^c].
\end{split}
\end{equation}

For the diachronic setting, we must use the stronger CMI constraint of Equation~\eqref{eq:synchronic-mi-constraint} instead. 

\subsubsection{Hierarchical Formulation}

Formulation \eqref{opt:abstract_formulation_with_reward} is coupled---that is, it simultaneously involves the exo/endo state decomposition and the exogenous reward regression. The following gives an alternative hierarchical formulation that decouples the optimization of the state representation from the reward regression
\begin{align}
\hat{m}_{exo}^*=\argmin_{\hat{m}_{exo}:{\mathcal X}^* \mapsto \mathbb{R}} &\E[(\hat{m}_{exo}(X)-R)^2], \textrm{ where } X^*=S[{\mathcal{I}^{*}}] \mbox{ and}\label{opt:decoupled_abstract_formulation_a}\\
\mathcal{I}^*=&\argmax_{\mathcal{I}\subseteq[d]} |\mathcal{I}|\label{opt:decoupled_abstract_formulation_b}\\
&\mbox{subject to } I(X'; [E, A] \mid X)=0,X=S[\mathcal{I}],E=S[\mathcal{I}^c].\label{opt:decoupled_abstract_formulation_c}
\end{align}
Formulation \eqref{opt:decoupled_abstract_formulation_a}-\eqref{opt:decoupled_abstract_formulation_c} breaks the coupled problem into two subproblems: Subproblem \eqref{opt:decoupled_abstract_formulation_b}-\eqref{opt:decoupled_abstract_formulation_c} computes the maximal exo/endo decomposition, while subproblem \eqref{opt:decoupled_abstract_formulation_a} computes the best exogenous reward estimator corresponding to the previously-computed exo/endo decomposition. The formulation is hierarchical, since we first compute the exo/endo state representation and subsequently fit the exogenous reward function. 
Our ultimate goal is still to solve the coupled formulation; the hierarchical formulation serves as a computationally simpler proxy. How do the two formulations relate? Let $\mathcal{I}_{coupled}$ be the solution to the coupled formulation and $\mathcal{I}_{hier}$ be the solution to the hierarchical formulation.
The containment property of Corollary \ref{cor:decomposition-containment} implies that $\mathcal{I}_{coupled} \subseteq \mathcal{I}_{hier}$, since the maximal exo set of \eqref{opt:decoupled_abstract_formulation_b}-\eqref{opt:decoupled_abstract_formulation_c} contains the exo set of any other exo/endo decomposition, including the endo/exo decomposition of \eqref{opt:abstract_formulation_with_reward}. It is possible that $\mathcal{I}_{coupled} \subset \mathcal{I}_{hier}$ if there are exogenous variables that carry no information about the exogenous reward function $R_{exo}$. The hierarchical formulation will include those variables in the reward regression, and this may increase the variance of the estimated $\hat{m}_{exo}$, because the regression will need to fit zero-valued weights for these irrelevant variables. The variance can be addressed in the usual way by employing $L_1$ regularization when fitting $\hat{m}_{exo}$. Hence in practice (and in the limit of infinite data), the two formulations will produce the same solution.

\subsection{Continuous Formulation}\label{sec:continuous-formulation}
In the continuous formulation, we assume that the state space $\mathcal{S}\subseteq\mathbb{R}^{d}$ is an open subset $U \subseteq \mathbb{R}^{d}$. Each $s \in \mathcal{S}$ is thus a $d$-dimensional real-valued vector. The discovery problem is to find a mapping $\xi: U \mapsto V$ where $V \subseteq \mathbb{R}^d$ is also an open set such that the endogenous and exogenous state spaces can be readily extracted. Without loss of generality, we stipulate that the first $d_{exo}$ components in $\xi(\mathcal{S})$ define the exogenous state, while the remaining $d_{end}=d-d_{exo}$ components define the endogenous state. Hence, $\mathcal{I} = [d_{exo}]$, so that $\mathcal{X}=\xi(\mathcal{S})[\mathcal{I}]$ and $\mathcal{E}=\xi(\mathcal{S})[\mathcal{I}^c]$. 

What conditions must the mapping $\xi$ satisfy? It is desirable that the mapping preserve probability densities so that the resulting exo/endo MDP is equivalent to the original MDP. A natural choice is the class of \textit{diffeomorphisms} \citep{diffeomorphisms}. A diffeomorphism from an open subset $U\subseteq\mathbb{R}^d$ to an open subset $V\subseteq\mathbb{R}^d$ is defined as a bijective map $\xi: U\mapsto V$ so that both $\xi$ and its inverse $\xi^{-1}$ are continuously differentiable. We denote the class of such diffeomorphisms from $U$ to any possible open set $V$ as $\mathfrak{X}^d_U$.

Diffeomorphisms have several important properties. From an information theoretic point of view, they do not lose any information about the original random variables. In particular, by using the change of variables formula, we can analytically write the probability density function of the transformation $\xi$ using the probability density of $\mathcal{S}$ and the Jacobian of the transformation. Assuming the density of $S$ is $p_S(s'\mid s,a)$, the density $p_{Z}(z'\mid z,a)$ for the transformed variables $Z=f(S)$ is given by
$$
p_{Z}(z'\mid z,a)=p_S(f^{-1}(z')\mid f^{-1}(z), a)\cdot |\det(J_{f^{-1}}(z'))|,
$$
where $J_{f^{-1}}(z')$ is the Jacobian matrix of the inverse transformation $f^{-1}$ and $\det(\cdot)$ is the determinant. In particular, $\int_{U}p_{S}(s'\mid s,a)ds'=\int_{V}p_{Z}(z'\mid z,a)dz'=1$. Hence, the original MDP defined on $\mathcal{S}$ can be transformed into an equivalent MDP defined on the transformation $\xi(\mathcal{S})$.
An implication of the above is that mutual information between two random vectors is preserved under diffeomorphisms \citep{kraskov2004}. This result can be extended to the conditional mutual information. As a result, independence and conditional independence are also preserved under diffeomorphisms, as they correspond to the special case of zero mutual or conditional mutual information.  

With these assumptions, the coupled formulation takes the following form
\begin{equation}\label{opt:general_formulation}
\begin{split}
&\xi^*,\hat{m}_{exo}^*,d_{exo}^*=\argmin_{\xi\in\mathfrak{X}^d_\mathcal{S},\hat{m}_{exo}:\mathcal{X} \mapsto \mathbb{R},d_{exo}\in\{0,\dots,d\}} \E[(\hat{m}_{exo}(X)-R)^2] \\
&\mbox{subject to }I(X'; [E, A] \mid  X) = 0, \\
&\textrm{where } \mathcal{I} = [d_{exo}],\; \mathcal{X}=\xi(\mathcal{S})[\mathcal{I}],\mbox{ and }\mathcal{E}=\xi(\mathcal{S})[\mathcal{I}^c].\\
\end{split}
\end{equation}
This formulation jointly optimizes the diffeomorphic state transformation and the exogenous reward function, so that the latter best fits the observed reward. Notice that the optimal $d_{exo}$ is unknown, so the formulation for the optimal decomposition must consider all possible values for $d_{exo}$.

Similarly, we can define the hierarchical formulation
\begin{align}
\hat{m}_{exo}^*=&\argmin_{\hat{m}_{exo}:\mathcal{X}^* \mapsto \mathbb{R}} \E[(\hat{m}_{exo}(X^*)-R)^2], \textrm{ where }\mathcal{X}^*=S[{\mathcal{I}^{*}}], \mathcal{I}^*=[d_{exo}^*], \mbox{ and }\label{opt:general_formulation_decoupled_a}\\
& \xi^*,d_{exo}^*=\argmax_{\xi\in\mathfrak{X}^d_\mathcal{S},d_{exo}\in\{0,\dots,d\}} d_{exo} \label{opt:general_formulation_decoupled_b}\\
& \hspace{2cm} \mbox{subject to }I(X'; [E, A] \mid  X) = 0,\\
& \hspace{2cm} \textrm{where } \mathcal{I} = [d_{exo}],\; \mathcal{X}=\xi(\mathcal{S})[\mathcal{I}],\mbox{ and }\mathcal{E}=\xi(\mathcal{S})[\mathcal{I}^c].\label{opt:general_formulation_decoupled_c}
\end{align}
Equations \eqref{opt:general_formulation_decoupled_b}-\eqref{opt:general_formulation_decoupled_c} first compute the diffeomorphic state transformation so that the corresponding exo/endo decomposition is valid and has the highest possible number of exogenous state variables. Equation \eqref{opt:general_formulation_decoupled_a} subsequently estimates the exogenous reward function.

As before, we can replace the CMI constraint in these formulations with the stronger version from
Equation~\eqref{eq:synchronic-mi-constraint} to enforce the diachronic DBN structure.

The diffeomorphic formulation provides a general framework for
continuous state variables. However, diffeomorphic transformations do not usually accept a simple parameterization. They can in principle be represented by invertible neural networks \citep{ardizzone2018,behrmann2021} or normalizing flows \citep{rezende2015,kobyzev2021}. Particular care must be taken to ensure that the corresponding mappings are not just bijective but also continuously differentiable with a continuously differentiable inverse. This can be very restrictive in practice. Recent work by \cite{koenen2021} relaxes the definition of a diffeomorphism to allow for mappings that are bijective and continuously differentiable almost everywhere, with a set of critical points of Lebesgue measure 0. The new class of transformations, named Lebesgue-diffeomorphisms or $\mathcal{L}$-diffeomorphisms, is consistent with several types of normalizing flows and can thus replace the more rigid class of diffeomorphisms.

\subsection{Linear Formulation}\label{sec:linear-formulation}

We now introduce a tractable formulation for the general class of continuous diffeomorphisms from Section \ref{sec:continuous-formulation}. Concretely, we consider the \textit{general linear group} $GL(d,\mathbb{R})$ of \textit{invertible linear transformations} in vector space $\mathbb{R}^d$.

Instead of directly defining the full state transformation $\xi$, we start by defining a linear mapping $\xi_{exo}$ from the full state space to the exogenous state space. We then define the endogenous state space as its orthogonal complement. Let $\xi_{exo}$ be specified by a matrix $W_{exo}\in\mathbb{R}^{d\times d_{exo}}$ with $0\leq d_{exo}\leq d$, where $d_{exo}$ is the dimension of subspace $\mathcal{X}$. Given a state $s$, its projected exogenous state is $W_{exo}^{\top}\cdot s$. The endogenous subspace is the orthogonal complement of $\mathcal{X}$ of dimension $d_{end}=d-d_{exo}$, written $\mathcal{E}=\mathcal{X}^{\perp}$ and defined by some matrix $W_{end}$. The endogenous state $e$ contains the components of $s$ in subspace $\mathcal{E}$. In the linear setting, $\mathcal{E}$ and $\mathcal{X}$ are vector subspaces of vector space $\mathcal{S}$, and we can write $\mathcal{S}=\mathcal{E}\oplus\mathcal{X}$, with $\dim(\mathcal{S})=\dim(\mathcal{E})+\dim(\mathcal{X})$. We will use the notation
\[S\stackrel{W}{=}[E,X],
\]
to denote the state-space decomposition defined by $W=(W_{exo},W_{end})$. The matrix $W$ is invertible with rank $d$, and hence, defines a linear diffeomorphism. 

Let $P \in \mathcal{P}(\mathcal{S})$ denote a probability distribution over the state space. We can map this distribution into a distribution $\tilde{P}$ in the decomposed state space through the change of variables formula:
\begin{equation}
\tilde{P}(X=x, E=e) = \frac{1}{|\det(W)|} P(S = W^{-1}[x,e]).
\end{equation}
The marginal distributions $\tilde{P}(X)$ and $\tilde{P}(E)$ can be computed by marginalizing in the original space. For example,
\[\tilde{P}(X=x) = \frac{1}{|\det(W)|} \int_e P(S = W^{-1}[x,e]) de.\]
In the remainder of the paper, we will simply write $P(X)$ and $P(E)$ for the probability distributions in the transformed space.

For computational convenience, we impose the requirement that the columns of $W_{exo}$ consist of \textit{orthonormal} vectors, so that $W_{exo}^{\top}\cdot W_{exo}=\mathbb{I}_{d_{exo}}$.  Linear decomposition theory \citep{linearalgebra} then tells us that the exogenous state can be represented in the original $d$-dimensional basis as $x=\xi_{exo}(s)=W_{exo}\cdot W_{exo}^{\top}\cdot s$, and the endogenous state $e=\xi_{end}(s)=s-x=s-W_{exo}\cdot W_{exo}^{\top}\cdot s$. Under this approach, $\xi_{exo}(s)+\xi_{end}(s)=x+e=s$. Every state $s\in \mathcal{S}$ can be written uniquely in this way (i.e., this is a direct sum) for a given exogenous projection matrix $W_{exo}$. Of course, like $W_{exo}$, the endogenous matrix $W_{end}$ can also be expressed using orthonormal components.

Because diffeomorphisms preserve mutual information \citep{kraskov2004}, the conditional mutual information constraint holds in the mapped state space 
\[I(W_{exo}\cdot S';[W_{end}\cdot S,A] \mid W_{exo}\cdot S)=0
\]
if and only if it holds in the original space,
\[I(X';[E,A] \mid X)=0.\] 
Note that it is possible for two different matrices $W_{exo}$ and $W_{exo}'$ to satisfy the conditional mutual information constraint if they are related by an invertible matrix $U\in\mathbb{R}^{d_{exo}\times d_{exo}}$ as $W_{exo} = W_{exo}' \cdot U$ (and similarly for $W_{end}$). Hence, when we solve the coupled (Equation \eqref{opt:general_formulation}) or hierarchical (Equations \eqref{opt:general_formulation_decoupled_a}-\eqref{opt:general_formulation_decoupled_c}) formulations, the exogenous subspace is only identified up to multiplication by an invertible matrix $U$. This can make it difficult to interpret the discovered subspace. One suggestion is to put $M_{exo}$ into block-diagonal canonical form, so that to the extent possible, the coordinates of the mapped space are the same as in the original space. 

To develop the linear formulations of the discovery problem, let us consider the database $D$ of $\{(s_i,a_i,r_i,s'_i)\}_{i=1}^N$ sample transitions collected in Algorithm~\ref{alg:framework}. We start by centering  $s_i$ and $s'_i$ by subtracting off the mean of the observed states. 
Let $\mathbf{S}\in\mathbb{R}^{N\times d},\mathbf{A}\in\mathbb{R}^{N\times k},\mathbf{R}\in\mathbb{R}^{N\times 1},\mathbf{S}'\in\mathbb{R}^{N\times d}$ be the  matrices containing the observations of $s_i,a_i,r_i,$ and $s'_i$, respectively. These are samples from the random variables $S,A,R,S'$, and we will estimate the expectations required in the optimization formulations (expected squared reward regression error and CMI values) from these samples.
Given observation matrices $\mathbf{S},\mathbf{S}'$, we can write the corresponding exogenous and endogenous states as $\mathbf{X}=\mathbf{S}\cdot W_{exo},\mathbf{X}'=\mathbf{S}'\cdot W_{exo},\mathbf{E}=\mathbf{S}-\mathbf{S}\cdot W_{exo}\cdot W_{exo}^{\top},$ and $\mathbf{E}'=\mathbf{S}'-\mathbf{S}'\cdot W_{exo}\cdot W_{exo}^{\top}$. 

The exogenous reward regression problem takes a particularly simple form if we adopt linear regression. Let $\hat{\mathbf{X}}= \mathbf{S}\cdot W_{exo}$ be the matrix of estimated exogenous state vectors, and let $w_R^*$ be the fitted coefficients of the linear regression. This coefficient vector can be computed as the solution of the usual least squares problem
\begin{equation}\label{opt:linear_reward_regression}
w_R^*=\argmin_{w_R\in\mathbb{R}^{d_{exo}}} \|\hat{\mathbf{X}}\cdot w_R-\mathbf{R}\|_2^2=\argmin_{w_R\in\mathbb{R}^{d_{exo}}} \{(\hat{\mathbf{X}}\cdot w_R-\mathbf{R})^\top\cdot(\hat{\mathbf{X}}\cdot w_R-\mathbf{R})\}=(\hat{\mathbf{X}}^{\top}\hat{\mathbf{X}})^{-1}\hat{\mathbf{X}}^{\top}\mathbf{R}.
\end{equation}
This gives us the optimization objective for the linear formulation. Now let us consider how to express the conditional mutual information constraints. 

Estimating mutual information (and conditional mutual information) has been studied extensively in machine learning. Recent work exploits variational bounds \citep{donsker1983, nguyen2007, nowozin2016, barber2003, blei2017} to enable differentiable end-to-end estimation of mutual information with deep nets \citep{belghazi2018, poole2019, alemi2018, hjelm2019, oord2018}. Despite their promise, mutual information estimation by maximizing variational lower bounds is challenging due to inherent statistical limitations \citep{McAllester2020}. Alternative approaches for estimating the mutual information include k-nearest neighbors \citep{kraskov2004}, ensemble estimation \citep{moon2017}, jackknife estimation \citep{zeng2018}, kernel density estimation \citep{kandasamy2015,han2020}, and Gaussian copula methods \citep{singh2017}. All of these require substantial computation, and some of them also require delicate hyperparameter tuning. Extending them to estimate conditional mutual information raises additional challenges. 

We have chosen instead to replace conditional mutual information with a quantity we call the conditional correlation coefficient (CCC). To motivate the CCC, assume that variables $X,Y,Z$ are distributed according to a multivariate Gaussian distribution. In this case, it is known \citep{baba2004partial} that $X$ and $Y$ are conditionally independent given $Z$, if and only if \[\Sigma_{XY}-\Sigma_{YZ}\Sigma^{-1}_{ZZ}\Sigma_{XZ}=\mathbf{0},\] where $\Sigma_{AA}$ is the covariance matrix of $A$ and $\Sigma_{AB}$ is the cross-covariance matrix of $A$ and $B$. We can normalize the above expression to obtain the normalized cross-covariance matrix 
\begin{equation} \label{eq:CCC}
V(X,Y,Z)=\Sigma_{XX}^{-1/2}(\Sigma_{XY} - \Sigma_{XZ}\Sigma_{ZZ}^{-1}\Sigma_{ZY})\Sigma_{YY}^{-1/2}=\mathbf{0}.
\end{equation}
It is not hard to see that Equation~\eqref{eq:CCC} holds if and only if
\[\tr(V^\top(X,Y,Z)\cdot V(X,Y,Z))=0,\]
where $\tr(\cdot)$ is the trace function.
We call the quantity $\tr(V^\top(X,Y,Z)\cdot V(X,Y,Z))$ the \textit{conditional correlation coefficient} (CCC), and we denote it by $CCC(X,Y\mid Z)$.\footnote{In \cite{dietterich2018}, we referred to this quantity as the Partial Correlation Coefficient (PCC), but this was an error. While the PCC can be used to determine conditional independence for Gaussians, it is a different quantity.} Because \eqref{eq:CCC} involves matrix inversion, we apply Tikhonov regularization \citep{tikhonov1977} to all inverse matrices with a small positive constant $\lambda>0$ for numerical stability. For instance, $\Sigma_{XX}^{-1/2}$ becomes $(\Sigma_{XX}+\lambda\cdot\mathbb{I}_n)^{-1/2}$, where $n$ is the size of random vector $X$.

Of course, the equivalence of zero CCC and conditional independence does not necessarily hold outside of the multivariate Gaussian distribution. A more general approach, which inspired our CCC method, would be to employ kernel measures of conditional independence
\citep{fukumizu2004dimensionality,fukumizu2008kernel}. These define normalized cross-covariance operators on reproducing kernel Hilbert spaces (RKHS) and establish that conditional independence holds if and only if the Hilbert-Schmidt norm of the operator is 0. Unfortunately, mapping our data into an RKHS would make the exogenous space discovery problem hard to optimize in terms of the underlying projection matrix $W_{exo}$. Instead, we directly optimize over the linearly projected data. Our experiments in Section \ref{sec:experiments} demonstrate that, despite being much simpler than kernel measures of conditional independence, our method can still provide useful results in MDPs with nonlinear dynamics.

We can now express the coupled formulation of the exogenous subspace discovery problem for the full setting as follows
\begin{equation}\label{opt:linear-coupled-formulation}
\begin{split}
&\min_{0\leq d_{exo}\leq d, W_{exo}\in \mathbb{R}^{d\times d_{exo}}, w_R\in \mathbb{R}^{d_{exo}}}\|\mathbf{S}\cdot W_{exo}\cdot w_R-\mathbf{R}\|_2^2 \\
&\mbox{subject to } W_{exo}^{\top}W_{exo}=\mathbb{I}_{d_{exo}}\\
&CCC(\mathbf{S}'W_{exo}; [\mathbf{S}-\mathbf{S}W_{exo}W_{exo}^{\top}, \mathbf{A}] \mid  \mathbf{S}W_{exo})<\epsilon.
\end{split}
\end{equation}
Note that we have relaxed the CCC constraint slightly to allow non-zero values less than a small constant $\epsilon$. 

To obtain a linear coupled formulation for the diachronic case, let
\[
Y(W_{exo}) = [\mathbf{S}-\mathbf{S}W_{exo}W_{exo}^{\top},\mathbf{S}'-\mathbf{S}'W_{exo}W_{exo}^{\top},\mathbf{A}].\]
This creates a stacked matrix corresponding to $[\mathbf{E}, \mathbf{E'}, \mathbf{A}]$. We can then write the CCC constraint for the diachronic case as
\[
CCC(\mathbf{S}'W_{exo}; Y(W_{exo}) \mid \mathbf{S}W_{exo}) = 0.
\]
This is the CCC equivalent of the diachronic CMI constraint of Equation~\eqref{eq:synchronic-mi-constraint}. 

The linear hierarchical formulation for the full case can be written as
\begin{equation}\label{opt:linear-decoupled-formulation}
\begin{split}
&\min_{w_R\in \mathbb{R}^{d_{exo}^*}}\|\mathbf{S}\cdot W_{exo}^*\cdot w_R-\mathbf{R}\|_2^2 \\
& \mbox{where }\\
& d_{exo}^*,W_{exo}^*=\argmax_{d_{exo}\in\{0,\dots,d\},W_{exo}\in \mathbb{R}^{d\times d_{exo}}} d_{exo} \\
&\hspace{2cm} \mbox{subject to } W_{exo}^{\top}W_{exo}=\mathbb{I}_{d_{exo}}\\
&\hspace{2cm} CCC(\mathbf{S}'W_{exo}; [\mathbf{S}-\mathbf{S}W_{exo}W_{exo}^{\top}, \mathbf{A}] \mid  \mathbf{S}W_{exo})<\epsilon.
\end{split}
\end{equation}

Although we have written the outer objective in terms of linear regression, this is not essential. Once the optimal linear projection matrix $W_{exo}^*\in\mathbb{R}^{d\times d_{exo}}$ has been determined and the reward regression dataset $D_{exo}$ has been constructed, any form of regression---including nonlinear neural network regression---can be employed.

\subsection{Conditions Establishing Sound Inference}\label{sec:soundness}

What conditions must hold so that the coupled or hierarchical optimization discovery formulations, when applied to the collected data $D$, find valid exo/endo decompositions? In this section, we address this question for ``tabular'' MDPs---that is, MDPs with finite, discrete state and action spaces. To find valid exo/endo decompositions, we need to ensure that the factorizations of Equations \eqref{eq:fundamental_equation} or \eqref{eq:diachronic-factorization} hold in all states of the MDP. This means that our exploration policy needs to visit all states, and it needs to execute all possible actions in each state to verify that the action does not affect (either directly or indirectly) the exogenous variables. We formalize this as follows.

Consider an idealized version of Algorithm~\ref{alg:framework} that collects the tuple dataset $D$ by executing a fixed exploration policy $\pi_x$ for a large number of steps. We will require that $\pi_x$ is fully randomized, according to the following definition:

\begin{definition}[Fully Randomized Policy] \label{def:fully-randomized}
An exploration policy $\pi_x$ is \emph{fully randomized} for a tabular MDP with action space $\mathcal{A}$ if $\pi_x$ assigns non-zero probability to every possible action $a\in \mathcal{A}$ in every state $s \in \mathcal{S}$.
\end{definition}

We will also require that the structure of the MDP is such that a fully-randomized policy will visit every state $s \in \mathcal{S}$ infinitely often.

\begin{definition}[Admissible MDP] \label{def:admissible-mdp}
A tabular MDP is \emph{admissible} if any fully-randomized policy will visit every state in the MDP infinitely often.
\end{definition}

Examples of admissible MDPs include episodic MDPs and ergodic MDPs. An episodic MDP begins each episode in a fixed start state $s_0$ and executes a policy until a terminal state is reached. Then it resets to the starting state. It must satisfy the requirement that all policies will reach a terminal state in a finite number of steps. The simplest episodic MDP always terminates after a fixed number of steps $H$, which is called the horizon time of the MDP.  Note that if an episodic MDP contains states that are not reachable from the start state $s_0$ by \textit{any} policy, then these must be deleted from the MDP in order to satisfy the definition of admissibility.

An ergodic MDP has the property that for all policies, every state is reachable from every other state in a finite number of steps, and the time between successive visits to any given state is aperiodic.

\begin{theorem}[Soundness of Empirical Conditional Mutual Information]\label{theorem:ecmi}
Let $\mathcal{M}$ be an admissible MDP, and let $D$ be a set of $\langle s,a,r,s'\rangle$ tuples collected by executing fully-randomized policy $\pi_x$ for $n$ steps and recording the state, action, reward, and result state at each step.  Let $(X,E)$ be a proposed exo/endo decomposition of $S$. Let $\hat{P}(E,X,A,E',X')$ be the maximum-likelihood estimate of the joint distribution of the decomposed $(S,A,S')$ triples, and let $\hat{I}(X';E,A,E' | X)$ be the corresponding estimate of the conditional mutual information for the diachronic form. 
If $\lim_{n\rightarrow \infty} \hat{I}(X';E,A,E' | X) = 0$ then $(X,E)$ is a valid exo/endo decomposition of $S$.
\end{theorem}
\begin{proof}
From the structure of an MDP, we know that the joint distribution $\hat{P}(E,A,E'X')$ can be factored as 
\[\hat{P}(E,X,A,E'X') = \hat{P}(E,X)\hat{P}(A|E,X)\hat{P}(E',X' | E, X, A).\]
The first term is the empirical probability of visiting state $S=(E,X)$, the second term is the empirical estimate of $\pi_x$, and the third term is the estimated transition dynamics of the MDP. The empirical conditional mutual information can be written as
\[
\hat{I}(X'; E, A, E' | X) = \sum_{E,X,A,E',X'} \hat{P}(E,X,A,E',X') \left[\log\frac{\hat{P}(X',E,A,E'|X)}{\hat{P}(X'|X)\hat{P}(E,A,E'|X)}\right].
\]
If $\hat{I}(X'; E, A, E' | X) = 0$, then it is easy to show that the fraction inside the log must be equal to 1 for all $(E,X,E',X',A)$ for which $\hat{P}(E,X,E',X',A)>0$. As $n$ gets large, this will hold for all possible state transitions.
Hence,
\begin{equation}\label{eq:cmi-inside}
\hat{P}(X',E,A,E' | X) = \hat{P}(X'|X)\hat{P}(E,A,E'|X).
\end{equation}
From the structure of the MDP, we know the left-hand side of (\ref{eq:cmi-inside}) can be rewritten as
\[\hat{P}(X',E,A,E' | X) = \hat{P}(X',E'|E,X,A)\hat{P}(A|E,X)\hat{P}(E|X).\]
By applying the chain rule of conditional probability, we can rewrite the right-hand side of (\ref{eq:cmi-inside}) as
\[\hat{P}(X'|X)\hat{P}(E,A,E'|X) = \hat{P}(X'|X)\hat{P}(E'|E,X,A)\hat{P}(A|E,X)\hat{P}(E|X).\]
Substituting these into (\ref{eq:cmi-inside}) gives
\[\hat{P}(X',E'|E,X,A)\hat{P}(A|E,X)\hat{P}(E|X) = \hat{P}(X'|X)\hat{P}(E'|E,X,A)\hat{P}(A|E,X)\hat{P}(E|X).\]
Because the MDP is admissible and $\pi_x$ is fully randomized, then for $n$ sufficiently large, 
\begin{align*}
\hat{P}(A|E,X)&> 0 \quad\forall E,X,A \\
\hat{P}(E|X)  &> 0 \quad\forall E,X.
\end{align*}
Hence, we can cancel them from both sides of the equation to obtain
\[\hat{P}(X',E'|E,X,A) = \hat{P}(X'|X)\hat{P}(E'|E,X,A).\]
As $n \rightarrow \infty$, all estimates will converge to their true values, and we will obtain
\begin{equation} \label{eq:diachronic-factorization-again}
P(X',E'|E,X,A) = P(X'|X)P(E'|E,X,A),
\end{equation}
which is the factorization for the diachronic Exogenous State MDP. This proves that $(X,E)$ is a valid exo/endo decomposition of the diachronic form.

Because we know the exploration policy $\pi_x$, we could replace $\hat{P}(A|E,X)$ by $\pi_x(A|E,X)$ throughout this derivation. Note also that $P(E',X'|E,X,A)$ will be zero for states $S' = (E',X')$ that cannot be directly reached from $(E,X)$ by any action $A$. This is not a problem, because the conditional mutual information formula is only concerned with the terms where $P(E,X,A,E',X') > 0$. 
\end{proof}

An analogous theorem and proof can easily be provided for Exogenous State MDPs of the full form.

For purposes of computing a valid exo/endo decomposition that respects the full or diachronic factorization, this theorem suffices. But it only shows that the variables in $X$ are statistically exogenous. We might also wish to prove that they are causally exogenous according to Definition~\ref{def:exogenous}. To apply this definition, we need a causal graph. To infer the structure of a causal graph from observational data, we can make the following faithfulness assumption \citep{Spirtes2000}.

\begin{assumption}[Faithfulness]
The set of conditional independencies exhibited by the observed distribution $P(S'|S,A)$ is exactly the set of conditional independencies entailed by the causal graph of the MDP.
\end{assumption}

\begin{corollary}\label{theorem:statistical-to-causal}
If the conditions of Theorem \ref{theorem:ecmi} hold, the underlying structural causal model has the structure of a stationary MDP, and the causal model satisfies the faithfulness assumption, then the set $X$ satisfies the causal definition of exogeneity in Definition \ref{def:exogenous}.
\end{corollary}
\begin{proof}
From the assumptions (faithfulness, MDP structure, and Theorem \ref{theorem:ecmi}), we know that every probabilistic dependency (and independency) implied by the structure of the causal graph will be exhibited by the observed data $D$. 

Consider the diachronic setting first. Assume that any of the links $E\to X$, $A\to X$ or $E'\to X$ were present in the causal graph $\mathcal{G}$. In that case, variables $E,A,E'$ are not $d$-separated from $X'$ given $X$. But then faithfulness would imply that $X'\not\independent E,A,E' \mid X$, which is a contradiction. Hence the causal graph has the form of Figure \ref{fig:diachronic-setting}. 

For the full setting, we can similarly show the links $A\to X$ and $E\to X$ must be missing. What about link $E'\to X'$? Assume that such a link indeed exists in $\mathcal{G}$. Consider any state variable $E'_i$ in set $E'$, so that the link $E'_i \to X'$ is present. If either link $A\to E'_i$ or $E\to E'_i$ is present, then there is a directed path from $A$ (or $E$) to $X'$ through $E'_i$. But in that case variables $A$ (or $E$) and $X'$ are not $d$-separated given $X$, so the faithfulness assumption implies that $X'\not\independent A \mid X$ (or $X'\not\independent E \mid X$), which is a contradiction. What if there is no link from $A$ or $E$ to $E'_i$? In that case, we can immediately see that both $X$ and $E'_i$ are causally exogenous.  Hence the causal graph has the form of Figure \ref{fig:full-setting}.

In summary, if the conditional mutual information $I(X';E,A,E' | X)$ is zero, then the factorization in (\ref{eq:diachronic-factorization-again}) matches the structure of the causal graph $\mathcal{G}$, and the set $X$ satisfies the causal definition of exogeneity.
\end{proof}

Note that if the faithfulness assumption does not hold, then the set $X$ may contain variables that coincidentally satisfy the factorization of (\ref{eq:diachronic-factorization-again}) but are not causally exogenous. However, if $X$ is a maximal set of exogenous variables, then we know that it contains all of the causally exogenous variables (and possibly others).

In our experiments, we initialize our chosen reinforcement learning algorithm, PPO, to implement a fully-randomized policy. However, as PPO is an on-policy RL algorithm, the policy gradually departs from full randomization, and we lose any guarantee that all states will be visited and all actions exercised. Consequently, the resulting exogenous sets $X$ may not be valid. 

\section{Algorithms for Decomposing an MDP into Exogenous and Endogenous Components}\label{sec:algorithms}
% name the algorithms
\def\grds{GRDS}
\def\sras{SRAS}

This section introduces two practical algorithms for addressing the exogenous subspace discovery problem. The first algorithm, \grds{} (Global Rank Descending Scheme), is based on the linear hierarchical formulation of Equation~\eqref{opt:linear-decoupled-formulation}. It initializes $d_{exo}$ to $d$ and decreases $d_{exo}$ one dimension at a time until it can find a $W_{exo}$ matrix whose CCC is near zero. We refer to it as a ``global'' scheme, because it must solve a series of global manifold optimization problems. The second algorithm, \sras{} (Stepwise Rank-Ascending Scheme), starts with $d_{exo} := 0$ and constructs the $W_{exo}$ matrix by adding one column at a time as long as it can keep CCC near zero. \sras{} only needs to solve one-dimensional manifold optimization problems, so it has the potential to be faster.

\subsection{\grds{}: Global Rank Descending Scheme}
Algorithm~\ref{alg:Global} gives the pseudo-code for the global rank descending scheme, \grds. \grds{} solves the inner objective (Equation~\eqref{opt:linear-decoupled-formulation}) by iterating from $d_{exo} := d$ down to zero. Instead of treating the $CCC < \epsilon$ condition as a constraint, we put $CCC$ into the objective and minimize it (line 6). % I tried to use a cross-reference for the 6, but it didn't work
If the optimization finds a $W_{exo}$ with $CCC<\epsilon$, we know that this gives the maximum value,  $d^*_{exo}$. Hence, we can halt and return $W_{exo}$ as the solution. 

One might hope that we could use a more efficient search procedure, such as binary search, to find $d^*_{exo}$. Unfortunately, because not all subsets of the maximal exogenous subspace are valid decompositions (Theorem~\ref{theorem:bad-subsets}), it is possible for an exogenous subset with $\hat{d} < d^*_{exo}$ to violate the $CCC < \epsilon$ constraint.

The orthonormality constraint in the minimization (line 6) forces the weight matrix $W_{exo}$ to lie on a Stiefel manifold \citep{Stiefel1935}. Hence, line 6 seeks to minimize a function on a manifold, a problem to which we can apply familiar tools for Euclidean spaces such as gradient descent, steepest descent and conjugate gradient. Several optimization algorithms exist for optimizing on Stiefel manifolds \citep{Jiang2015,Absil2007,Edelman1999}. 
Manifold optimization on a Stiefel manifold has previously been considered by \cite{bach2003} in the context of Independent Component Analysis, but in their case the linearly projected data are subsequently mapped to a Reproducing Kernel Hilbert Space (RKHS).

\begin{algorithm}[t!]
\begin{algorithmic}[1]
\STATE {\bf Inputs:} A database of transitions $\{(s_i,a_i,s'_i)\}_{i=1}^N$ provided as matrices $\mathbf{S},\mathbf{A},\mathbf{S'}$
\STATE {\bf Output:} The exogenous state projection matrix $W_{exo}^*$ 
\FOR {$d_{exo}=d$ \textbf{down to} $1$}
\STATE Set \;\;\;\; $Y(W_{exo})\gets [\mathbf{S}-\mathbf{S}W_{exo}W_{exo}^{\top},\mathbf{S}'-\mathbf{S}'W_{exo}W_{exo}^{\top}, \mathbf{A}]$ for the diachronic setting
\STATE or \;\;\;\;\;\; $Y(W_{exo})\gets [\mathbf{S}-\mathbf{S}W_{exo}W_{exo}^{\top}, \mathbf{A}]$ for the full setting
\STATE Solve the following optimization problem
\begin{align*}
W_{exo}^* := \quad\quad&\\
\argmin_{W_{exo}\in \mathbb{R}^{d\times  d_{exo}}}&CCC(\mathbf{S}'W_{exo}; Y(W_{exo}) \mid  \mathbf{S}W_{exo})\\
\textrm{subject to } &W_{exo}^{\top}W_{exo}=\mathbb{I}_{d_{exo}}
\end{align*}
\STATE Set \;\;\;\; $CCC \leftarrow CCC(\mathbf{S}'W_{exo}^*; Y(W_{exo}^*) \mid  \mathbf{S}W_{exo}^*)$ 
\IF {$CCC < \epsilon$}
\RETURN $W_{exo}^*$
\ENDIF
\ENDFOR
\RETURN null projection $\mathbf{0}$
\end{algorithmic}
\caption{\grds{}: Global Rank-Descending Scheme}
\label{alg:Global}
\end{algorithm}

\subsection{Analysis of the Global Rank-Descending Scheme}
\label{sec:global-analysis}
In this section, we study the properties of the global rank-descending scheme. We assume that we fit the exo reward function using linear regression  \eqref{opt:linear_reward_regression}, since this simplifies our analysis. Directly analyzing Algorithm \ref{alg:Global} is hard, because it (i) uses the CCC objective as a proxy for conditional independence, (ii) involves estimation errors due to having a finite number of samples, and (iii) involves approximations in the optimization (both from numerical errors and from the $\epsilon$ threshold). To side-step these challenges, we consider an \textit{oracle variant} of our setting, where we have access to the true joint distribution $P(S,A,S')$ and perfect algorithms for solving all optimization problems (including the exo reward linear regression). Access to $P(S,A,S')$ is equivalent to having an infinite training sample collected by visiting all states and executing all actions so that estimation errors vanish when computing the conditional mutual information and the expected value of the residual error in \eqref{opt:linear_reward_regression}.

Under this oracle setting, we prove that the global rank-descending scheme returns the unique exogenous subspace of maximum rank. In practice, if we have a sufficiently representative sample of $\langle s, a, s', r\rangle$ tuples and the CCC captures conditional independence reasonably well, we can hope that our methods will still give useful results.


Algorithm~\ref{alg:rank-descending-oracle} shows the oracle version of \grds{}. It is identical to \grds{} except that the optimization step of minimizing the CCC is replaced by the following feasibility problem:
% todo: rename this equation, as it is no longer the simplified objective
\begin{equation}\label{opt:simplified-objective}
\begin{gathered}
\mbox{Find }W_{exo}\in \mathbb{R}^{d \times d_{exo}} \mbox{ such that:}\\
W_{exo}^{\top}W_{exo}=\mathbb{I}_{d_{exo}}\\
I(S'W_{exo}; [S-SW_{exo}W_{exo}^{\top}, A] \mid  SW_{exo})=0.
\end{gathered}
\end{equation}


\begin{algorithm}[t!]
\begin{algorithmic}[1]
\STATE {\bf Input:} Joint distribution $P(S,A,S')$ corresponding a fully-randomized policy controlling an admissible exogenous state MDP with maximal exogenous subspace $\mathcal{X}$ defined by $W^*_{exo}$
\STATE {\bf Output:} Matrix $\hat{W}^*_{exo}$ 
\FOR {$d_{exo}=d$ \textbf{down to} $1$}
\STATE Solve the following system of equations for $W_{exo}\in\mathbb{R}^{d,d_{exo}}$:
\begin{equation*}
\begin{gathered}
W_{exo}^{\top}W_{exo}=\mathbb{I}_{d_{exo}}\\
I(S'W_{exo}; [S-SW_{exo}W_{exo}^{\top}, A] \mid  SW_{exo})=0.
\end{gathered}
\end{equation*}
\IF {the above system is feasible with solution $W_{exo}$ of rank $d_{exo}$}
\STATE $\hat{W}^*_{exo} := W_{exo}$
\RETURN $\hat{W}_{exo}^*$
\ENDIF
\ENDFOR
\RETURN null matrix $\mathbf{0}$
\end{algorithmic}
\caption{Oracle-\grds{} for the Full Setting}
\label{alg:rank-descending-oracle}
\end{algorithm}

\begin{theorem}\label{theorem:correctness}
The Oracle-\grds{} algorithm returns a matrix $W_{exo}$ such that
\begin{enumerate}[(a)]
\item the subspace $\mathcal{X}$ defined by $W_{exo}$ and the subspace $\mathcal{E}$ defined as the orthogonal complement of $\mathcal{X}$ form a valid exo/endo decomposition of the full form;
\item the subspace $\mathcal{X}$ has maximal dimension over all valid exo/endo decompositions; and
\item the subspace $\mathcal{X}$ is unique and contains all other exogenous subspaces $\tilde{\mathcal{X}}$ that could form valid exo/endo decompositions.
\end{enumerate}
\end{theorem}

\begin{proof}
To prove property (a), first note that we can define the joint distribution $P(X= W_{exo}W_{exo}^\top s,E=s-W_{exo}W_{exo}^\top s)=P(S=s)$, because each column of $W_{exo}$ is a unit vector and they are orthogonal. Because $W_{exo}$ is a feasible solution to the manifold optimization Problem \ref{opt:simplified-objective} and the conditional mutual information is zero, we know from Theorem \ref{theorem:ecmi} that $P(S'|S,A)$ factors as $P(X'|X)P(E'|X,E,A,X')$. Hence, it is a valid exo/endo decomposition according to Theorem~\ref{theorem:exo-DBN}. 

Property (b) follows from the fact that $d_{exo}$ is the largest value that yields a feasible solution to Problem \ref{opt:simplified-objective}. 

To establish property (c), we need to prove three lemmas, which are the vector space versions of Theorem~\ref{theorem:decomposition-union}, Corollary~\ref{cor:decomposition-uniqueness}, and Corollary~\ref{cor:decomposition-containment}.

\begin{lemma}[Union of Exo/Endo Decompositions]\label{lemma:linear-union}
Let $[\mathcal{X}_1,\mathcal{E}_1]$ and $[\mathcal{X}_2, \mathcal{E}_2]$ be two full exo/endo decompositions of an MDP $\mathcal{M}$ with state space $\mathcal{S}$, where $\mathcal{X}_1 = \{W_1^{\top} s : s \in \mathcal{S}\}$ and $\mathcal{X}_2 = \{W_2^{\top} s : s \in \mathcal{S}\}$ and where $W_1^\top W_1 = \mathbb{I}_{d_1\times d_1}$ and $W_2^\top W_2 = \mathbb{I}_{d_2\times d_2}$, $1\leq d_1,d_2 \leq d$.
Let $\mathcal{X} = \mathcal{X}_1 + \mathcal{X}_2$ be the subspace formed by the sum of subspaces $\mathcal{X}_1$ and $\mathcal{X}_2$, and let $\mathcal{E}$ be its complement. It then holds that the state decomposition $S=[E,X]$ with $E \in \mathcal{E}$ and $X \in \mathcal{X}$ is a valid full exo/endo decomposition of $\mathcal{S}$.
\end{lemma}
\begin{proof}
We wish to follow the same reasoning as in Theorem \ref{theorem:decomposition-union}. To do this, we need to compute the linear subspaces equivalent to $\mathcal{X} = \mathcal{X}_1 + \mathcal{X}_2$ and $\mathcal{E} = \mathcal{E}_1 \cap \mathcal{E}_2$.  Consider the following subspaces expressed in the original $d$-dimensional coordinate system:
\begin{align}
\overline{\mathcal{X}} &= \mathcal{X}_1 \cap \mathcal{X}_2 = \{W_2 W_2^\top W_1 W_1^\top s : s \in \mathbb{R}^d\} \label{eq:xbar} \\
\hat{\mathcal{X}}_1 &= \mathcal{X}_1 \cap \mathcal{E}_2 = \{s \in \mathcal{X}_1 : W_2 W_2^\top s = 0\} \label{eq:xhat1} \\
\hat{\mathcal{X}}_2 &= \mathcal{X}_2 \cap \mathcal{E}_1 = \{s \in \mathcal{X}_2 : W_1 W_1^\top s = 0\}\label{eq:xhat2} \\
\mathcal{X} &= \mathcal{X}_1 + \mathcal{X}_2 = \overline{\mathcal{X}} \oplus \hat{\mathcal{X}}_1 \oplus \hat{\mathcal{X}}_2 \label{eq:xunion}\\
\mathcal{E} &= \mathcal{X}^\perp = \{s \in \mathbb{R}^d : W_1 W_1^\top s = 0 \wedge W_2 W_2^\top s = 0\} \label{eq:e}
\end{align}
Equation~\eqref{eq:xbar} defines the intersection of the two subspaces $\mathcal{X}_1$ and $\mathcal{X}_2$, because $W_1 W_1^\top$ is a projection matrix that projects $s$ into $\mathcal{X}_1$, and $W_2 W_2^\top$ is a projection matrix that projects $W_1 W_1^\top s$ into $\mathcal{X}_2$. In Equation~\eqref{eq:xhat1}, we define $\hat{\mathcal{X}}_1$ as the set of points in $\mathcal{X}_1$ that are not in $\mathcal{X}_2$, because $W_2 W_2^\top$ maps them to zero. Hence, they are also not in the intersection $\overline{\mathcal{X}}$. This implies that $\mathcal{X}_1 = \overline{\mathcal{X}} \oplus \hat{\mathcal{X}}_1$. Note that because $\mathcal{E}_2$ is defined as the points that $W_2 W_2^\top$ maps to zero, $\hat{\mathcal{X}}_1$ can also be expressed as $\mathcal{X}_1 \cap \mathcal{E}_2$.  
Similarly, $\hat{\mathcal{X}}_2 = \mathcal{X}_2 \cap \mathcal{E}_1$, and $\mathcal{X}_2 = \overline{\mathcal{X}} \oplus \hat{\mathcal{X}}_2$. We can therefore express $\mathcal{X}_1 + \mathcal{X}_2$ as the direct sum $\overline{\mathcal{X}} \oplus \hat{\mathcal{X}}_1 \oplus \hat{\mathcal{X}}_2$ (Equation~\eqref{eq:xunion}). Equation~\eqref{eq:e} defines $\mathcal{E}$ as the set of points that are simultaneously in the orthogonal complements of both $\mathcal{X}_1$ and $\mathcal{X}_2$. Either $\hat{\mathcal{X}}_1$ or $\hat{\mathcal{X}}_2$ may of course be the zero vector spaces, if one exogenous subspace is contained in the other.

Now we can define random variables that permit us to apply the proof of Theorem \ref{theorem:decomposition-union}. Define random variables for the input spaces: $X_1 \in \mathcal{X}_1$, $E_1 \in \mathcal{E}_1$, $X_2 \in \mathcal{X}_2$, 
for the $(\mathcal{X},\mathcal{E})$ decomposition. Similarly, define random variables for the output spaces: $X=(\overline{X},\hat{X}_1,\hat{X}_2),$ where $\overline{X} \in \overline{\mathcal{X}}$, $\hat{X}_1 \in \hat{\mathcal{X}}_1$, and $\hat{X}_2 \in \hat{\mathcal{X}}_2$. Let $S \in \mathcal{S}$. 

Because $(X_1,E_1)$ is a valid decomposition, there can be no edges from $E_1$ or $A$ to any variable in $X_1$. Similarly, because $(X_2,E_2)$ is a valid decomposition, there can be no edges from $E_2$ or $A$ to any variable in $X_2$. Consequently, there can be no edges from $E$ and $A$ to any subspace in $X$. This demonstrates that $(X,E)$ is a valid full exo/endo decomposition of the state space.
\end{proof}

\begin{lemma}[Unique Maximal Subspace]\label{cor:unique-maximal-subspace}
The maximal exo vector subspace $\mathcal{X}_{max}$ defined by $W_{exo,max}$ is unique.
\end{lemma}
\begin{proof}
By contradiction. If there were 2 distinct maximal subspaces, then Lemma \ref{lemma:linear-union} would allow us to combine them to get an even larger exogenous vector subspace. This is a contradiction.
\end{proof}

\begin{lemma}\label{lemma:maximal-containment}
Let $W_{exo,max}$ define the maximal exogenous vector subspace $\mathcal{X}_{max}$, and let $W_{exo}$ define any other exogenous vector subspace $\mathcal{X}$. Then $\mathcal{X} \sqsubseteq \mathcal{X}_{max}$. 
\end{lemma}
\begin{proof}
By contradiction. If there were an exogenous subspace $\mathcal{X}$ not contained within $\mathcal{X}_{max}$, then by Lemma~\ref{lemma:linear-union} we could combine the 2 exo/endo decompositions to get an even larger exogenous subspace. This contradicts the assumption that $\mathcal{X}_{max}$ is maximal.
\end{proof}

These three lemmas establish property (c) and complete the proof of Theorem~\ref{theorem:correctness}.
\end{proof}

This concludes our analysis of the Oracle-\grds{} for the full setting (Algorithm~\ref{alg:rank-descending-oracle}).  The analysis can trivially be extended to the diachronic setting.

How well does this analysis carry over to the non-oracle \grds{} algorithm? \grds{} departs from the oracle version in three ways. First, \grds{} employs the CCC in place of conditional mutual information (CMI). This may assign non-zero CCC values to $W_{exo}$ matrices that actually have zero CMI. This will cause \grds{} to under-estimate $d_{exo}$, the dimensionality of the exogenous state space. Second, \grds{} only requires CCC to be less than a parameter $\epsilon$. If $\epsilon$ is large, then \grds{} may stop too soon and over-estimate $d_{exo}$. Hence, by introducing $\epsilon$, \grds{} is able to compensate somewhat for the failures of CCC. Third, the database of transitions is not infinite, so the value of CCC that \grds{} computes may be too high or too low. This in turn may cause $d_{exo}$ to be too small or too large. In our experiments, we will compare the estimated $d_{exo}$ to our understanding of its true value. 

\subsection{Stepwise Algorithm \sras{}}
\label{sec:stepwise-algorithm}

\begin{algorithm}[t!]
\begin{algorithmic}[1]
\STATE {\bf Inputs:} A database of transitions $\{(s_i,a_i,r_i,s'_i)\}_{i=1}^N$
\STATE {\bf Output:} The exogenous state projection matrix $W_{exo}$ 
\STATE Initialize $W_{exo}\leftarrow[~]$, $W_{temp}\leftarrow[~]$, $C_x\leftarrow[~]$, $k\leftarrow 0$
\REPEAT
\STATE $N\leftarrow \textrm{orthonormal basis for the null space of }C_x$
\STATE Solve the following optimization problem
\begin{align*}
\hat{w} := \quad\quad\quad&\\
\argmin_{w\in \mathbb{R}^{(d-k)\times 1}}&CCC(\mathbf{S}'[W_{temp},N^{\top}w];\mathbf{A}\mid \mathbf{S}[W_{temp}, N^{\top}w])\\
\textrm{subject to } &w^{\top}w=1
\end{align*}
\STATE $w_{k+1} \leftarrow N^{\top}\hat{w}$ %express $\tilde{w}$ in original $\mathbf{R}^{d_{exo}}}$ space 
\STATE $C_x \leftarrow C_x \cup \{w_{k+1}\}$
\STATE $CCC_{sim}\leftarrow CCC(\mathbf{S}'[W_{temp},w_{k+1}];\mathbf{A}\mid \mathbf{S}[W_{temp}, w_{k+1}])$
\IF {$CCC_{sim} < \epsilon$}
\STATE $W_{temp} \leftarrow W_{temp} \cup \{w_{k+1}\}$
\STATE $\mathbf{E}\leftarrow\mathbf{S}-\mathbf{S}W_{temp}W_{temp}^{\top}$
\STATE $CCC_{full}\leftarrow CCC(\mathbf{S}'W_{temp};[\mathbf{E},\mathbf{A}]\mid \mathbf{S}W_{temp})$
\IF{$CCC_{full} < \epsilon$}
\STATE $W_{exo} \leftarrow W_{temp}$
\ENDIF
\ENDIF
\STATE $k\leftarrow k+1$
\UNTIL $k=d$
\RETURN $W_{exo}$
\end{algorithmic}
\caption{Stepwise Rank Ascending Scheme: \sras{}}
\label{alg:Stepwise}
\end{algorithm}

The global scheme computes the entire $W_{exo}$ matrix at once. In this section, we introduce an alternative \textit{stepwise} algorithm, the Stepwise Rank Ascending Scheme (\sras{}, see Algorithm~\ref{alg:Stepwise}), which constructs the matrix $W_{exo}$ incrementally by solving a sequence of small manifold optimization problems. 

\sras{} maintains the current partial solution $W_{exo}$, a temporary matrix $W_{temp}$ that may be extended to update $W_{exo}$, a set of all candidate column vectors generated so far, $C_x$, and an orthonormal basis $N$ for the null space of $C_x$. ($N$ is a matrix with a number of columns equal to the dimension of the null space of $C_x$.) The set of candidate column vectors $C_x$ contains all of the column vectors in $W_{exo}$ and possibly some additional vectors that were rejected for violating the full CCC constraint, as we will discuss below.

Suppose we have already found the first $k$ columns of $W_{exo} = [w_1, w_2, \ldots, w_k]$. To ensure that the new column $w_{k+1}$ is orthogonal to all $k$ previous vectors, we restrict $w_{k+1}$ to lie in the space defined by $N$ by requiring it to have the form $w_{k+1} = N^{\top}w$. This ensures that it is orthogonal to all columns of $W_{exo}$ and to any additional vectors in $C_x$. 

In Line 6, we compute a new candidate vector $\hat{w}$ by solving a simplified CCC minimization problem on the $(d-k)\times1$-dimensional Stiefel manifold. Recall that the full objective $I(X' ;[E,A]\mid X)=0$ seeks to enforce the conditional independence $X'\independent E, A \mid X$. This requires us to know $X$ and $E$, whereas at this point in the algorithm, we only know a portion of $X$, and we therefore do not know $E$ at all. We circumvent this problem by using the \textit{simplified objective} $I(X'_k; A\mid X_1, \ldots, X_k)$ (approximated via the CCC). This objective ensures that $A$ has no effect on the exogenous variables $X'$ in the next time step, which eliminates the edge $A \rightarrow X'$, but it does not protect against the possibility that $A$ causes a change in some chain of endogenous variables that affect $X$ in some subsequent time step. Hence, the simplified objective is a necessary but not sufficient condition for $X$ to be a valid exogenous subspace. See Appendix~\ref{app:simplified_setting} for a detailed discussion of this point.

Lines 7 and 8 compute the new candidate vector $w_{k+1}$ by mapping $\hat{w}$ into the null space defined by $N$ and then adding it to $C_x$. In Line 9, we compute $CCC_{sim}$, the value of the simplified objective (which is the same as the value that minimized the objective in Line 6). In Line 10, we check whether this is less than $\epsilon$. If not, we increment $k$ and loop back to Line 5 and find another $\hat{w}$ vector. But if $CCC_{sim} < \epsilon$, then in Lines 11-13, we compute the corresponding $\bf{E}$ matrix and compute $CCC_{full}$, the CCC of the full objective. In Line 14, we check whether $CCC_{full} < \epsilon$. If so, then we have a valid new column to add to $W_{exo}$. If not, we increment $k$ and loop back to Line 5. 

Recall that not all subsets of the maximal exogenous subspace are themselves valid exogenous subspaces that satisfy the full CCC constraint (Theorem~\ref{theorem:bad-subsets}). Hence, it is important that \sras{} does not terminate when adding a candidate vector to $W_{exo}$ causes the full constraint to be violated. Note, however, that every subset of the maximal exogenous subspace must satisfy the simplified objective, because otherwise, the action variable is directly affecting one of the exogenous state variables. 

To allow \sras{} to continue making progress when the full constraint is violated, the algorithm maintains the matrix $W_{temp}$. This matrix contains all of the candidates that have satisfied the simplified objective.  If a subsequent candidate $w_{k+1}$ allows $W_{temp}$ to satisfy the full constraint, then we set $W_{exo}$ to $W_{temp}$ and continue. The algorithm terminates when $k=d$. 

The primary advantage of \sras{} compared to \grds{} is that the CCC minimization problems have dimension $(d-k)\times 1$. However, \grds{} can halt as soon as it finds a $W_{exo}$ that satisfies the full CCC objective, whereas \sras{} must solve all $d$ problems. We can introduce heuristics to terminate $d$ early. For example, we can monitor the residual variance $\|\hat{\mathbf{X}}\cdot w_R-\mathbf{R}\|_2^2$ of the reward regression. This decreases monotonically as columns are added to $W_{exo}$, and when those decreases become very small, we can terminate \sras{}. This can make \sras{} very efficient when the exogenous subspace has low rank $d_{exo}$ relative to the rank $d$ of the full state space. In such cases, \grds{} must solve $d - d_{exo} + 1$ large manifold optimization problems of dimension at least $d \times d_{exo}$, whereas \sras{} must only solve $d_{exo}$ problems of dimension $(d-k)\times 1$.  

What can we say about the correctness of \sras{}? First, in an oracle version of \sras{} (where the MDP was admissible, the data were collected using a fully-randomized policy, and CMI was computed instead of CCC), the $W_{exo}$ matrix returned by \sras{} would define a valid exo/endo decomposition. This is because it would satisfy the full CMI constraint. However, it would not necessarily define the maximal exogenous subspace, because the $w$ vectors found using the simplified objective and stored in $W_{temp}$ might not be a subset of a satisfying $W_{exo}$ matrix.  Of course, because the actual \sras{} algorithm introduces the CCC approximation and only requires the CCC to be less than $\epsilon$, we do not have any guarantee that the $W_{exo}$ matrix returned by \sras{} defines a valid exogenous subspace. We now turn to experimental tests of the algorithms to see whether they produce useful results despite their several approximations.

\section{Experimental Study}\label{sec:experiments}
We conducted a series of experiments to understand the behavior of our algorithms. In addition to \grds{} and \sras{}, we defined a third algorithm, Simplified-\grds{} that applies the simplified objective $CCC(S'W_{Exo} ; A | SW_{exo})$ in Line 6 of Algorithm~\ref{alg:Global} but then still uses the full objective in Line 7. Like \sras{}, Simplified-\grds{} will always return a valid $W_{exo}$, but it may not be maximal. 

In this section, we present experiments to address the following research questions:
\begin{itemize}
    \item[RQ1:] Do our methods speed up reinforcement learning in terms of sample complexity? In terms of total CPU time?
    \item[RQ2:] Do our methods discover the correct maximal exogenous subspaces?
    \item[RQ3:] What is the best approach to reward regression? Single linear, repeated linear, or online neural network regression? 
    \item[RQ4:] How do the algorithms behave when the MDPs are changed to have different properties: (a) rewards are nonlinear, (b) transition dynamics are nonlinear, (c) action space is combinatorial, and (d) states and actions are discrete?
    \item[RQ5:] How do the algorithms behave when the covariance condition is violated?
    \item[RQ6:] What are the risks and benefits of using the simplified objective in \sras{} and Simplified-\grds{}?
    \item[RQ7:] How should hyperparameters be set? How many training tuples should be collected before performing exogenous subspace discovery? When should reward regression begin and on what schedule?
\end{itemize}

\subsection{Experimental Details}
\label{sec:setup}
 We compare five methods:
 \begin{itemize}
    \item \textit{Baseline}: Reinforcement learning applied to the full reward (sum of exogenous and endogenous components)
    \item \textit{\grds{}}: The Global Rank Descending Scheme
    \item \textit{Simplified-\grds{}}: \grds{} using the simplified objective
    \item \textit{\sras{}}: The Stepwise Rank Ascending Scheme
    \item \textit{Endo Reward Oracle}: Reinforcement learning applied with the oracle endogenous reward.
\end{itemize}
As the reinforcement learning algorithm, we employ the PPO implementation from stable-baselines3 \citep{stable-baselines3} in PyTorch \citep{pytorch}, and we model the MDPs in the OpenAI Gym framework \citep{gym}. We use the default PPO hyperparameters in stable-baselines3, which include a clip range of 0.2, a value function coefficient of 0.5, an entropy coefficient of 0, and a generalized advantage estimation (GAE) parameter of 0.95.
The policy and value networks have two hidden layers of 64 tanh units each. For PPO optimization, we employ the default Adam optimizer \citep{kingma} in PyTorch with default hyperparameters $\beta_1=0.9, \beta_2=0.999, \text{eps}=1\times 10^{-5}$ and a default learning rate of $lr_{PPO}=0.0003$. The batch size for updating the policy  and value networks is 64 samples. The discount factor in the MDPs is set to $\gamma=0.99$. The default number of steps between each policy update is $K=1536$.  The number of steps $L$ after which we compute the exo/endo decomposition and the total number of training steps in the experiment $N$ vary per experiment, but their default values are $L=3000$ and $N=6000$. We summarize all hyperparameters in Table \ref{table:hyperparams}.

In each experimental run, we maintain two instances of the Gym environment. Training is carried out in the primary instance. After every policy update, we copy the policy to the second instance and evaluate the performance of the policy (without learning) for 1000 steps. To reduce measurement variance, these evaluations always start with the same random seed.

To solve the manifold optimization problems, we apply the solvers implemented in the Pymanopt package \citep{pymanopt}. We use the Steepest Descent solver with line search with the default Pymanopt hyperparameters. For the CCC constraint, $\epsilon$ is set to $0.05$. The Tikhonov regularizer inside the CCC is set to $\lambda=0.01$. These hyperparameters were determined empirically via random search \citep{random_search}.

To implement neural network reward regression, we use a separate neural network in sklearn \citep{sklearn} with 2 hidden layers of 50 and 25 units, respectively, and ReLU activations. We train with Adam using the default Adam coefficients. The learning rate is set by default to $lr_{regr}=0.0003$ and the L2 regularization to $3\times 10^{-5}$. The batch size is set to 256. During Phase 1, we train the net with the $L$ collected samples until convergence (or a maximum number of 125 epochs). During Phase 2, we perform online learning by updating the neural net every $M=256$ training steps with a single pass over the last 256 samples. Linear regression is computed using the standard least squares matrix solution without regularization.


We chose to use the default library hyperparameters for PPO, Adam optimization, and manifold optimization to enable a fair comparison of the different methods. Furthermore, for online reinforcement learning algorithms, it is not feasible to perform extensive hyperparameter searches because of the cost (and risk) of interacting in the real world. Algorithms that only perform well after extensive hyperparameter search are not usable in practice. Hence, we wanted to minimize hyperparameter tuning.

In all our MDPs, we use the default values in Table \ref{table:hyperparams} for all hyperparameters except for the number of decomposition steps $L$ and the number of training steps $L$. The former is the most critical hyperparameter; it is discussed in detail in Section \ref{sec:practical-considerations}. The latter is set to a number that is high enough for our methods to converge or be near the limit. Finally, in a few settings we found it beneficial to use a regression learning rate of 0.0006 instead of 0.0003 for better convergence.

For each setting, we report the values of the hyperparameters that are different from their default values. We run all experiments on a c5.4xlarge EC2 machine on AWS\footnote{https://aws.amazon.com/ec2/instance-types/c5/.}.

\begin{table}[t!]
{\footnotesize
\begin{center}
\begin{tabular}{ c c c c c c } 
 \hline
 Used for & Description & Symbol & Default Value & Fixed \\ 
 \hline\hline
 \multirow{4}{*}{PPO} & clipping parameter & - & 0.2 & Yes \\
   & value function coefficient & - & 0.5 & Yes \\
      & entropy coefficient & - & 0 & Yes \\
      & GAE parameter & - & 0.95 & Yes \\
 \hline
   \multirow{3}{*}{\begin{tabular}{l}Policy \& Value Nets\end{tabular}} & number of layers & - & 2 & Yes \\
  & units per layer & - & 64, 64 & Yes\\
  & activation function & - & tanh & Yes \\
 \hline
  \multirow{3}{*}{\begin{tabular}{l}PPO Optimization\\with Adam\end{tabular}} & Adam learning rate & $lr_{PPO}$ & 0.0003 & Yes \\
  & Adam coefficients & $\beta_1,\beta_2,\textrm{eps}$ & 0.9, 0.99, 1e-5 & Yes\\
  & batch size & - & 64 & Yes \\
 & L2 regularization & - & 0 & Yes \\
 \hline
   \multirow{4}{*}{\begin{tabular}{l}Reinforcement\\Learning\end{tabular}} & discount factor & $\gamma$ & 0.99 & Yes \\
  & policy update steps & $K$ & 1536 & Yes\\
  & total training steps & $N$ & 60000 & No\\
  & steps for decomposition & $L$ & 3000 & No\\
  & steps for exo regression& $M$ & 256 & Yes\\
  & evaluation steps & - & 1000 & Yes \\
 \hline
   \multirow{2}{*}{\begin{tabular}{l}Manifold\\Optimization\end{tabular}} & CCC threshold & $\epsilon$ & 0.05 & Yes \\
  & Tikhonov regularizer & $\lambda$ & 0.01 & Yes\\
 \hline
  \multirow{3}{*}{\begin{tabular}{l}Exo regression Net\end{tabular}} & number of layers & - & 2 & Yes \\
  & units per layer & - & 50, 25 & Yes\\
  & activation function & - & relu & Yes \\
 \hline
  \multirow{4}{*}{\begin{tabular}{l}Exo Regression\\Optimization\\with Adam\end{tabular}} & Adam learning rate & $lr_{regr}$ & 0.0003 & No \\
  & Adam coefficients & $\beta_1,\beta_2,\textrm{eps}$ & 0.9, 0.99, 1e-8 & Yes\\
  & batch size & - & 256 & Yes \\
  & L2 regularization & - & 0.00003 & Yes \\
 \hline
\end{tabular}
\end{center}
\caption{\label{table:hyperparams}Hyperparameters for High-D setting.}}
\end{table}

\subsection{Performance Comparison on High-Dimensional Linear Dynamical MDPs}
\label{sec:linear-setting}

To address RQ1 and RQ2, we define a set of high-dimensional MDPs with linear dynamics. 
Each MDP, by design, has $m$ endogenous and $n$ exogenous variables, so that $e_t\in\mathbb{R}^m$ and $x_t\in\mathbb{R}^n$. There is a single action variable $a_t$ that takes 10 discrete values $(-1, -0.777, -0.555, \ldots, 0, \ldots, 0.555, 0.777, +1)$. The policy chooses one of these values at each time step. The exo and endo transition functions are
\begin{equation*}
\begin{split}
&x_{t+1} = M_{exo}\cdot x_{t} + \varepsilon_{exo}\\
&e_{t+1} = M_{end}\cdot \begin{bmatrix}e_{t}\\ x_{t}\end{bmatrix} + M_a\cdot a_t + \varepsilon_{end},
\end{split}
\end{equation*}
where $M_{exo}\in\mathbb{R}^{n\times n}$ is the transition function for the exogenous MRP; $M_{end}\in\mathbb{R}^{m \times m}$ is the transition function for the endogenous MDP involving $e_{t}$ and $x_{t}$; $M_a\in\mathbb{R}^{m}$ is the coefficient for the action $a_{t}$, and it is set to a vector of ones; 
$\varepsilon_{exo}\in\mathbb{R}^{n}$ is the exogenous noise, whose elements are distributed according to $\mathcal{N}(0,0.09)$; and  $\varepsilon_{end}\in\mathbb{R}^{m}$ is the endogenous noise, whose elements are distributed according to $\mathcal{N}(0,0.04)$.
The observed state vector $s_t\in\mathbb{R}^{m+n}$ is a linear mixture of the hidden exogenous and endogenous states defined as
$$s_t = M\cdot\begin{bmatrix}e_{t}\\ x_{t}\end{bmatrix},$$
where $M\in\mathbb{R}^{(m+n)\times(m+n)}$. The elements in  $M_{exo}$, $M_{end}$, and $M$ are generated according to $\mathcal{N}(0,1)$ and then each row of each matrix is normalized to sum to 0.99 for stability\footnote{Notice that all matrices $M_{exo}$, $M_{end}$, and $M$ in our synthetic linear MDPs are stochastic. Future work could explore more general classes of MDPs.}. The elements of  the initial endo and exo states are randomly initialized from a uniform distribution over $[0, 1]$.

In our first set of experiments, the reward at time $t$ is
$$R_t = R_{exo,t} + R_{end,t},$$
where $R_{exo,t} = -3\cdot\textrm{avg}(x_{t})$ is the exogenous reward  and $R_{end,t} = e^{-|\textrm{avg}(e_t) - 1|}$ is the endogenous reward, and $\textrm{avg}(\cdot)$ denotes the average over a vector's elements. For this class of MDPs, the optimal policy seeks to drive the average of $e_{t}$ to $1$.

We experiment in total with 6 MDPs with different choices for the numbers $n$ and $m$ of the exogenous and endogenous variables, respectively: (i) 5-D state with $m=2$ endo variables and $n=3$ exo variables; (ii) 10-D state with $m=5$ and $n=5$; (iii) 20-D state with $m=10$ and $n=10$; (iv) 30-D state with $m=15$ and $n=15$; (iv) 45-D state with $m=22$ and $n=23$; and (vi) 50-D state with $m=25$ endo variables and $n=25$ exo variables. For the 50-D setting, we use $N=100000$ training steps and $L=10000$ decomposition steps due to its higher dimensionality. Similarly, we use $N=80000$ and $L=5000$ for the 45-D MDP, and $L=5000$ for the 30-D MDP. On the other hand, we set $N=50000$ and $L=2000$ for the 5-D MDP. For each MDP, we run 20 replications with different random seeds and report the average results and standard deviations.

\subsubsection{RL Performance}
To address RQ1, we report the performance for each of the 6 MDPs over 20 replications in Figure \ref{fig:linear_global_stepwise}. 
In all 6 MDPs, all of our methods far-outperform the baseline. Indeed, in the 5-D and 10-D MDPs, the baseline does not show any sign of learning, and in the larger problems, the baseline's performance has attained roughly half of the performance of our methods after 65 policy updates. A simple linear extrapolation of the baseline learning curve for the 50-D problem suggests that it will require 132 policy updates to attain the performance of the other methods. This is more than 3 times as long as the 40 updates our methods require. Hence, in terms of sample complexity, our methods are much more efficient than the baseline method.

\begin{figure}[t!]
     \centering
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{linear_3x2_comp}
         \caption{5-D MDP ($m=2,n=3$).}
         \label{fig:3x2}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{linear_5x5_comp}
         \caption{10-D MDP ($m=5,n=5$).}
         \label{fig:5x5}
     \end{subfigure}
     
     
     \bigskip
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{linear_10x10_comp}
         \caption{20-D MDP ($m=10,n=10$).}
         \label{fig:10x10}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{linear_15x15_comp}
         \caption{30-D MDP ($m=15,n=15$).}
         \label{fig:15x15}
     \end{subfigure}
     
     \bigskip
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{linear_23x22_comp}
         \caption{45-D MDP ($m=22,n=23$).}
         \label{fig:23x22}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{linear_25x25_comp}
         \caption{50-D MDP ($m=25,n=25$).}
         \label{fig:25x25}
     \end{subfigure}
    \caption{Comparison of various methods in high-D linear MDPs.}
    \label{fig:linear_global_stepwise}
\end{figure}

On these MDPs, the Simplified-\grds{} and \sras{} methods are able to match the performance of the Endo Reward Oracle, which is given the correct endogenous reward from the very start. \grds{} performs very well on all MDPs except for the 5-D one, where it is still able to outperform the baseline.

RQ1 also asks whether our methods are superior in terms of CPU time. Table~\ref{table:exp1} reports the CPU time required by the various methods. The Baseline and Endo Reward Oracle consume identical amounts of time, so the table only lists the Baseline CPU time. We observe that even the slowest of our methods (\sras{} on the 50-D problem) requires only 45\% more time than the Baseline. However, if we again extrapolate the baseline to 132 policy updates, where its performance would match our methods, that would require 2997 seconds of CPU time, which is 49\% more than \sras{}. Hence, even if there is zero cost to collecting training samples in the real world, our methods are still faster. 

\begin{table}[t!]
{\footnotesize
\begin{center}
\begin{tabular}{ c c c c c c c } 
 \hline
 Total State & Exo State & Endo State & \multirow{2}{*}{Method} & Exo Subspace & Total Time & Decomposition \\ 
 Variables & Variables & Variables & & Rank & (secs) & Time (secs) \\ 
 \hline\hline
 \multirow{4}{*}{5} & \multirow{4}{*}{3} & \multirow{4}{*}{2} & Baseline & - & 245.9$\pm$14.7 & - \\
   &  &  & \grds{} & 4.0$\pm$4.0 & 294.9$\pm$113.0 & 6.3$\pm$4.6 \\ 
   &  &  & Simplified-\grds{} & 4.0$\pm$0.0 & 297.0$\pm$115.4 & 9.6$\pm$5.0 \\ 
   &  &  & \sras{} & 3.95$\pm$0.22 & 297.1$\pm$113.3 & 14.0$\pm$14.0 \\ 
 \hline
 \multirow{4}{*}{10} & \multirow{4}{*}{5} & \multirow{4}{*}{5} & Baseline & - & 345.3$\pm$10.48 & - \\
   &  &  & \grds{} & 8.65$\pm$0.48 & 413.5$\pm$148.0 & 22.6$\pm$18.0 \\ 
   &  &  & Simplified-\grds{} & 9.0$\pm$0.0 & 413.7$\pm$144.4 & 13.1$\pm$9.1 \\ 
   &  &  & \sras{} & 8.75$\pm$0.54 & 434.3$\pm$157.6 & 34.2$\pm$46.7 \\ 
 \hline
 \multirow{4}{*}{20} & \multirow{4}{*}{10} & \multirow{4}{*}{10} & Baseline & - & 450.2$\pm$34.8 & - \\
   &  &  & \grds{} & 18.1$\pm$1.18 & 525.3$\pm$167.9 & 41.8$\pm$47.8 \\ 
   &  &  & Simplified-\grds{} & 19.0$\pm$0.0 & 513.6$\pm$141.3 & 8.6$\pm$4.4 \\ 
   &  &  & \sras{} & 18.4$\pm$0.66 & 562.4$\pm$188.5 & 86.0$\pm$72.3 \\ 
 \hline
 \multirow{4}{*}{30} & \multirow{4}{*}{15} & \multirow{4}{*}{15} & Baseline & - & 509.9$\pm$43.0 & - \\
   &  &  & \grds{} & 28.25$\pm$1.22 & 597.5$\pm$185.6 & 56.9$\pm$84.1 \\ 
   &  &  & Simplified-\grds{} & 29.0$\pm$0.0 & 584.3$\pm$136.9 & 12.5$\pm$6.0 \\ 
   &  &  & \sras{} & 27.9$\pm$1.58 & 688.9$\pm$276.5 & 177.5$\pm$234.9 \\ 
 \hline
 \multirow{4}{*}{45} & \multirow{4}{*}{23} & \multirow{4}{*}{22} & Baseline & - & 895.4$\pm$159.3 & - \\
   &  &  & \grds{} & 43.4$\pm$0.86 & 1041.7$\pm$287.6 & 84.9$\pm$154.2 \\ 
   &  &  & Simplified-\grds{} & 44.0$\pm$0.0 & 1006.3$\pm$207.8 & 13.3$\pm$8.4 \\ 
   &  &  & \sras{} & 44.0$\pm$0.0 & 1323.4$\pm$716.1 & 605.1$\pm$538.7 \\ 
 \hline
 \multirow{4}{*}{50} & \multirow{4}{*}{25} & \multirow{4}{*}{25} & Baseline & - & 1472.4$\pm$126.0 & - \\
   &  &  & \grds{} & 48.45$\pm$0.86 & 1659.6$\pm$282.7 & 81.7$\pm$105.8 \\ 
   &  &  & Simplified-\grds{} & 49.0$\pm$0.0 & 1634.5$\pm$239.1 & 15.7$\pm$9.2 \\ 
   &  &  & \sras{} & 49.0$\pm$0.0 & 2009.2$\pm$840.3 & 667.8$\pm$616.9 \\ 
 \hline
\end{tabular}
\end{center}
\caption{\label{table:exp1}Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition time for the linear MDPs.}}
\end{table}

\subsubsection{Rank of Discovered Exo Subspace}
\label{sec:linear-rank}

RQ2 asks whether our methods find the correct maximal exogenous subspaces. Our experiments revealed a surprise. Although we constructed the MDPs with the goal of creating an $n$-dimensional exogenous space and an $m$-dimensional endogeous space, our methods usually discover exogenous spaces with $n+m-1$ dimensions. Upon further analysis, we realized that because the action variable is 1-dimensional, it can only affect a 1-dimensional subspace of the $n+m$-dimensional state space. Consequently, the true maximal exogenous subspace has dimension $n+m-1$. The results in Table~\ref{table:exp1} show that the Simplified-\grds{} always finds an exogenous subspace of the correct dimension. The exogenous space computed by \sras{} is sometimes slightly smaller on the smaller MDPs, and the space computed by \grds{} is sometimes slightly smaller on the larger MDPs. We believe the failures of \sras{} are due to the approximations that we discussed in Section~\ref{sec:stepwise-algorithm}. We suspect the failures of \grds{} reflect failures of the manifold optimization to find the optimum in high-dimensional problems. 

The fact that the exogenous space has dimension $n+m-1$ explains the relative amount of CPU time consumed by the different algorithms. \sras{} is often the slowest, because it must solve $n+m$ optimization problems whereas \grds{} and Simplified-\grds{} must only solve two (large) manifold optimization problems before terminating.

\begin{figure}[t!]
     \centering
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{linear_5x5_basic}
         \caption{\grds{} ($m=5,n=5$).}
         \label{fig:5x5-global}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{linear_5x5_basic_stepwise}
         \caption{\sras{} ($m=5,n=5$).}
         \label{fig:5x5-stepwise}
     \end{subfigure}
     \bigskip
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{linear_25x25_basic}
         \caption{\grds{} ($m=25,n=25$).}
         \label{fig:25x25-global}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{linear_25x25_basic_stepwise}
         \caption{\sras{} ($m=25,n=25$).}
         \label{fig:25x25-stepwise}
     \end{subfigure}
    \caption{Impact of the type of exo reward regression on RL performance.}
    \label{fig:exo_reward_regression}
\end{figure}

\subsubsection{Comparison of Methods for Exo Reward Regression}\label{sec:impact-exo-regression}
We performed a second set of experiments to investigate how the type and configuration of exo reward regression affects the performance of our methods. We compare three reward regression configurations. The first configuration is \textit{Single Linear Regression}, which fits a linear model for the exo reward and performs regression only once at the end of Phase 1 of Algorithm~\ref{alg:framework}. The second configuration is \textit{Repeated Linear Regression}. Like Single Linear Regression, it fits a linear model at the end of Phase 1. In addition, it re-fits the model in Phase 2 every 1,000 collected samples using all transition data so far. Note that this is different from online Algorithm \ref{alg:framework}, which updates the exogenous reward function in Phase 2 every $M$ observations using only the last $M$ observations in $D_{exo}$. The goal was to understand whether regular regression with all observed transition data can perform better than a single linear regression at the end of Phase 1. The third configuration is \textit{Online Neural Net Regression} which fits a neural network to the exo reward data. At the end of Phase 1, we perform reward regression until convergence. During Phase 2, we then perform a single epoch every 256 steps. 

We plot RL performance over the 20 replications in Figure \ref{fig:linear_global_stepwise} on two MDPs: (i) the 10-D state MDP (Figures \ref{fig:5x5-global}-\ref{fig:5x5-stepwise}) and (ii) the 50-D state MDP (Figures \ref{fig:25x25-global}-\ref{fig:25x25-stepwise}). We compare the three regression methods applied to \grds{} and \sras{}. The results show that Online Neural Net Regression generally outperforms Single and Repeated Linear Regression, even though the exo reward function $R_{exo,t}$ is a linear function of the exo state. We speculate that this is because it is continually  incorporating new data, which in turn may allow PPO to make more progress.  Repeated Linear Regression also incorporates new data, but at a slower rate. Furthermore, when applied to \sras{}, it becomes unstable and exhibits high variance. We speculate that this may be because it is only fitting to the most recent 1000 data points. Future work might consider training on all accumulated data and imposing strong regularization to improve stability.

Based on the superior performance of online neural network regression, we adopt it as the default reward regression method in the remainder of our experiments. 

\subsection{Exploring Modifications of the MDPs}
\label{sec:nonlinear-rewards}

\begin{figure}[t!]
     \centering
     \begin{subfigure}[t]{0.46\textwidth}
         \includegraphics[scale=0.38]{nonlinear_0}
         \caption{$R_{exo,t}^1$.}
         \label{fig:nonlinear_0}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.44\textwidth}
         \includegraphics[scale=0.38]{nonlinear_1}
         \caption{$R_{exo,t}^2$.}
         \label{fig:nonlinear_1}
     \end{subfigure}     
     
     \bigskip
     \begin{subfigure}[t]{0.46\textwidth}
         \includegraphics[scale=0.38]{nonlinear_3}
         \caption{$R_{exo,t}^3$.}
         \label{fig:nonlinear_3}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.44\textwidth}
         \includegraphics[scale=0.38]{nonlinear_5}
         \caption{$R_{exo,t}^4$.}
         \label{fig:nonlinear_5}
     \end{subfigure}
    \caption{RL performance for MDPs with nonlinear exo reward functions.}
    \label{fig:nonlinear_rewards}
\end{figure}

To address RQ4, we now study the performance of our methods when they are applied to MDPs that depart in various ways from the linear dynamical MDPs studied thus far:
\begin{itemize}
\item[(a)] Rewards are nonlinear functions of the state, 
\item[(b)] Transition dynamics are nonlinear,
\item[(c)] The action space is combinatorial, and
\item[(d)] The states and actions are discrete.
\end{itemize}

\subsubsection{Nonlinear Exogenous Reward Functions}
Because we have adopted online neural network reward regression, we expect that our methods should be able to fit nonlinear exogenous reward functions.  We consider the high-D linear setting of Section \ref{sec:linear-setting} with $m=n=15$. We perform exo/endo decomposition after $L=5000$ steps and train for a total of
$N=80000$ steps. We found it beneficial to use a learning rate for the exogenous reward regression of 0.0006 instead of the default 0.0003; the higher learning rate can help the exogenous reward neural net to adapt faster. We perform 20 replications with different seeds. Furthermore, we replace the linear exogenous reward function $R_{exo,t}$ by the following four choices:
\begin{itemize}
\vspace{-0.2cm}\item $R_{exo,t}^1=\textrm{clip}(6\cdot(\textrm{avg}(x_t) + \frac{1}{3}\cdot\textrm{avg}(x_t^2) - \frac{2}{15}\cdot\textrm{avg}(x_t^3)), -5.0, 5.0)$, a $3^{rd}$ degree polynomial.
\vspace{-0.2cm}\item $R_{exo,t}^2=-3 \cdot e^{-|\textrm{avg}(x_t)|^{1.5}}$, a function of $\textrm{avg}(x_t)$ with a single mode.
\vspace{-0.2cm}\item $R_{exo,t}^3=-3 \cdot (e^{-|\textrm{avg}(x_t + 1.5)|^{2}} - e^{-|\textrm{avg}(x_t - 1.5)|^{2}})$, a function of $\textrm{avg}(x_t)$ with two modes.
\vspace{-0.2cm}\item $R_{exo,t}^4=-3 \cdot (e^{-|\textrm{avg}(x_t + 1)|^{2}} + \frac{3}{2}\cdot e^{-|\textrm{avg}(x_t - 1.5)|^{2}} - \frac{5}{3}e^{-|\textrm{avg}(x_t)|^{2}})$, a function of $\textrm{avg}(x_t)$ with three modes.
\end{itemize}
Figures \ref{fig:nonlinear_0}-\ref{fig:nonlinear_5} plot the results. 

We generally observe that all methods match the Endo Reward Oracle's performance and outperform the baseline by a large margin. This confirms that the nonlinear reward regression is able to fit these nonlinear reward functions. As we have observed before, the RL performance of \sras{} is a bit unstable, perhaps because it is not always able to detect the maximal exogenous subspace. The Simplified-\grds{} method also shows a tiny bit of instability. 

Table \ref{table:nonlinear_rewards} reports the average rank of the discovered exogenous subspaces. The true exogenous space has rank 29, but the exogenous reward only depends on 15 of those dimensions. The Simplified-\grds{} method is most consistently able to find the true rank, whereas \sras{} and \grds{} struggle to capture that last dimension. 

\begin{table}[t!]
{\footnotesize
\begin{center}
\begin{tabular}{ c c c c c } 
 \hline
 Exo Reward & \multirow{2}{*}{Method} & Exo Subspace & Total Time & Decomposition \\ 
 Function & & Rank & (secs) & Time (secs) \\ 
 \hline\hline
 \multirow{4}{*}{$R_{exo,t}^1$} & Baseline & - & 1520.0$\pm$153.0 & - \\
   & \grds{} & 28.05$\pm$1.02 & 1874.6$\pm$357.3 & 155.8$\pm$122.1 \\ 
   & Simplified-\grds{} & 29.0$\pm$0.0 & 1808.6$\pm$278.7 & 40.5$\pm$11.3 \\ 
   & \sras{} & 27.95$\pm$1.56 & 1914.4$\pm$510.3 & 251.8$\pm$249.5 \\ 
 \hline
 \multirow{4}{*}{$R_{exo,t}^2$} & Baseline & - & 1510.9$\pm$136.2 & - \\
   & \grds{} & 28.3$\pm$0.9 & 1860.4$\pm$359.9 & 150.3$\pm$155.1 \\
   & Simplified-\grds{} & 29.0$\pm$0.0 & 1797.8$\pm$264.4 & 31.9$\pm$7.7 \\ 
   & \sras{} & 27.95$\pm$1.56 & 1925.4$\pm$516.3 & 297.7$\pm$244.1 \\ 

 \hline
 \multirow{4}{*}{$R_{exo,t}^3$} & Baseline & - & 1518.1$\pm$146.4 & - \\
   & \grds{} & 28.0$\pm$1.0 & 1879.6$\pm$340.0 & 150.3$\pm$118.1 \\
   & Simplified-\grds{} & 29.0$\pm$0.0 & 1818.0$\pm$262.2 & 28.6$\pm$8.5 \\ 
   & \sras{} & 27.95$\pm$1.56 & 1932.4$\pm$505.4 & 272.9$\pm$265.5 \\ 
 \hline
 \multirow{4}{*}{$R_{exo,t}^4$} & Baseline & - & 1513.8$\pm$103.7 & - \\
   & \grds{} & 28.35$\pm$0.79 & 1849.8$\pm$366.1 & 123.2$\pm$111.5 \\
   & Simplified-\grds{} & 28.35$\pm$0.79 & 1812.0$\pm$268.9 & 37.1$\pm$11.3 \\ 
   & \sras{} & 29.0$\pm$0.0 & 1926.7$\pm$560.5 & 307.4$\pm$320.7 \\ 
 \hline
\end{tabular}
\end{center}
\caption{\label{table:nonlinear_rewards}Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition time for the MDPs with nonlinear exo rewards.}}
\end{table}

\subsubsection{Nonlinear State Transition Dynamics}

So far, we have considered MDPs with linear state transitions for the endogenous and exogenous states. It is a natural question whether our algorithms can handle more general MDPs. In this section, we provide experimental results on a more general class of nonlinear MDPs. Even though we lack a rigorous theoretical understanding, our results hint at the potential of the CCC objective to discover useful exo/endo state decompositions even when the dynamics are nonlinear.

In the experiments in this section, we introduce nonlinear dynamics, but we still configure the exogenous and endogenous state spaces so that they are linear projections of the full state space.  We study the following three MDPs, which are defined according to the recipe in Section \ref{sec:linear-setting} with the following modifications:
\begin{itemize}
\vspace{-0.2cm} \item $\mathcal{M}_1$ is a 10-D MDP with $m=n=5$ and a single action variable. The exo and endo state transitions are
\begin{equation*}
\begin{split}
&x_{t+1} = \textrm{clip}(M_{exo}\cdot x_{t} + \frac{1}{3}\cdot N_{exo}\cdot x^2_{t} - \frac{2}{15}\cdot K_{exo}\cdot x^3_{t}, -4, 4) + \varepsilon_{exo}\\
&e_{t+1} = M_{end}\cdot \begin{bmatrix}e_{t}\\ x_{t}\end{bmatrix} + M_a\cdot a_t + \varepsilon_{end},
\end{split}
\end{equation*}
where $M_{exo},m_e,M_a$ and $\varepsilon_{exo},\varepsilon_{end}$ are defined as in Section \ref{sec:linear-setting}. Furthermore, the two matrices $N_{exo}\in\mathbb{R}^{n,n}$ and $K_{exo}\in\mathbb{R}^{n,n}$ are generated following the same procedure as $M_{exo}$. $\mathcal{M}_1$ has nonlinear exogenous dynamics but linear endogenous dynamics.
\vspace{-0.2cm} \item $\mathcal{M}_2$ is exactly the same as $\mathcal{M}_1$ except that the endogenous transition function now has a nonlinear dependence on the action:
\begin{equation*}
\begin{split}
&e_{t+1} = M_{end}\cdot \begin{bmatrix}e_{t}\\ x_{t}\end{bmatrix} + M_a\cdot a_t + N_a\cdot a_t^2 + \varepsilon_{end}.
\end{split}
\end{equation*}
The entries in $N_a\in\mathbb{R}^{m}$ are sampled from the uniform distribution over $[0.5, 1.5)$.
\vspace{-0.2cm} \item $\mathcal{M}_3$ is a 10-D MDP with $m=n=5$ and a single action variable. The exogenous and endogenous state transitions functions are
\begin{equation*}
\begin{split}
&x_{t+1} = \textrm{clip}(5\cdot \textrm{sign}(x_{t})\cdot \sqrt{|x_t|} - \sin(x_t), -2, 2) + \varepsilon_{exo}\\
&e_{t+1} = M_{end}\cdot \begin{bmatrix}e_{t}\\ x_{t}\end{bmatrix} + \sin(3\cdot a_t) + \varepsilon_{end}.
\end{split}
\end{equation*}
The entries in the noise vectors $\varepsilon_{exo}$ and $\varepsilon_{end}$ are sampled from $\mathcal{N}(0,0.16)$ and  $\mathcal{N}(0,0.09)$, respectively. Like $\mathcal{M}_2$, $\mathcal{M}_3$'s  exo and endo transition functions are both nonlinear.
\end{itemize}

\begin{figure}[t!]
     \centering
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{M1}
         \caption{$\mathcal{M}_1$.}
         \label{fig:M1}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{M2}
         \caption{$\mathcal{M}_2$.}
         \label{fig:M2}
     \end{subfigure}
     
     
     \bigskip
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{M3a}
         \caption{$\mathcal{M}_3$.}
         \label{fig:M3a}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{M3b}
         \caption{$\mathcal{M}_3$ $(\epsilon=0.1)$.}
         \label{fig:M3b}
     \end{subfigure}
    \caption{RL performance for MDPs with nonlinear state transitions.}
    \label{fig:nonlinear_states}
\end{figure}

Figures \ref{fig:M1}-\ref{fig:M3a} plot the RL performance over 15 replications with different seeds and Table~\ref{table:nonlinear_states} reports the ranks of the discovered exogenous subspaces. Consider first MDPs $\mathcal{M}_1$ and $\mathcal{M}_2$. The Simplified-\grds{} and \sras{} algorithms perform very well. They converge quickly, and they are able to match the Endo Reward Oracle's performance. In both settings, the Baseline converges to a suboptimal value and suffers from high variance. Most shocklingly, \grds{} performs catastrophically and exhibits very high variance even though it discovers an exogenous subspace of the correct rank. 

Now consider $\mathcal{M}_3$ in Figure \ref{fig:M3a}. On this problem, Simplified-\grds{} and \sras{} perform poorly (although still better than the baseline), while \grds{} performs much better. However, none of the methods is able to match the Endo Reward Oracle. Table~\ref{table:nonlinear_states} reveals that all of the methods, but particularly Simplified-\grds{} and \sras{}, are failing to discover the correct exogenous subspace. This suggests that the CCC computation when applied to the simplified objective is not finding good solutions. To evaluate this possibility, we reran the $\mathcal{M}_3$ experiment with a larger value of $\epsilon=0.1$ instead of its default value of $0.05$.  With this change, the results improve dramatically for Simplified-\grds{} and \sras{}, and they are able to match the performance of the Endo Reward Oracle. However, the performance of \grds{} does not improve, which suggests that it is not able to find good solutions to the large manifold optimization problems that it is solving. This could be because the CCC objective is confused by the nonlinear dynamics or it could be that the optimization is trapped in local minima. 

\begin{table}[t!]
{\footnotesize
\begin{center}
\begin{tabular}{ c c c c c c } 
 \hline
 \multirow{2}{*}{MDP} & Exo/Endo State & \multirow{2}{*}{Method} & Exo Subspace & Total Time & Decomposition \\ 
 & Variables & & Rank & (secs) & Time (secs) \\ 
 \hline\hline
 \multirow{4}{*}{$\mathcal{M}_1$} & \multirow{4}{*}{5/5} & Baseline & - & 665.3$\pm$39.9 & - \\
   &  & \grds{} & 9.0$\pm$0.0 & 890.4$\pm$140.6 & 55.0$\pm$34.0 \\ 
   &  & Simplified-\grds{} & 9.0$\pm$0.0 & 906.7$\pm$154.6 & 72.2$\pm$19.8 \\ 
   &  & \sras{} & 9.0$\pm$0.0 & 951.2$\pm$253.2 & 172.8$\pm$73.2 \\ 
 \hline
 \multirow{4}{*}{$\mathcal{M}_2$} & \multirow{4}{*}{5/5} & Baseline & - & 717.0$\pm$30.3 & - \\
   &  & \grds{} & 9.0$\pm$0.0 & 959.5$\pm$137.1 & 61.9$\pm$41.2 \\ 
   &  & Simplified-\grds{} & 9.0$\pm$0.0 & 981.2$\pm$138.0 & 85.0$\pm$18.1 \\ 
   &  & \sras{} & 9.0$\pm$0.0 & 1019.8$\pm$271.9 & 187.4$\pm$72.6 \\ 
 \hline
 \multirow{4}{*}{\begin{tabular}{l}$\quad\mathcal{M}_3$\\$(\epsilon=0.05)$\end{tabular}} & \multirow{4}{*}{5/5} & Baseline & - & 768.2$\pm$50.1 & - \\
   &  & \grds{} & 7.67$\pm$1.44 & 1033.0$\pm$241.2 & 125.8$\pm$126.7 \\ 
   &  & Simplified-\grds{} & 3.60$\pm$3.28 & 988.1$\pm$160.4 & 38.7$\pm$19.0 \\ 
   &  & \sras{} & 3.47$\pm$3.36 & 1081.3$\pm$370.7 & 168.8$\pm$70.7 \\ 
 \hline
 \multirow{4}{*}{\begin{tabular}{l}$\quad\mathcal{M}_3$\\$(\epsilon=0.1)$\end{tabular}} & \multirow{4}{*}{5/5} & Baseline & - & 768.2$\pm$50.1 & - \\
   &  & \grds{} & 7.67$\pm$1.44 & 1033.0$\pm$241.2 & 125.8$\pm$126.7 \\ 
   &  & Simplified-\grds{} & 9.0$\pm$0.0 & 918.3$\pm$129.7 & 17.7$\pm$4.6 \\ 
   &  & \sras{} & 9.0$\pm$0.0 & 1023.3$\pm$341.5 & 230.4$\pm$69.7 \\ 
 \hline
\end{tabular}
\end{center}
\caption{\label{table:nonlinear_states}Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition time for the MDPs with nonlinear state transitions.}}
\end{table}

\subsubsection{Combinatorial Action Spaces}

A limitation of our experiments so far has been that the action space is one-dimensional. We have seen that this implies that the maximal exogenous subspace has dimension $n+m-1$ rather than $n$ as originally intended. In this section, we describe experiments where we introduce higher-dimensional action spaces. For example, with a 5-dimensional action space, the policy must now select one of 10 values for each of the 5 action variables. In effect, the MDP now has $10^5$ primitive actions.  

To design MDPs with high-dimensional action spaces, we modify the general linear setting of Section \ref{sec:linear-setting}, so that the action $a_t\in\mathbb{R}^l$ is an $l$-D vector, and the matrix $M_a$ multiplying $a_t$ is in $\mathbb{R}^{m,l}$. As in the single-action setting, each action variable takes 10 possible values evenly-spaced in $[-1,1]$. We consider 6 MDPs with a variety of different structures, which may in principle appear in real applications:
\begin{itemize}
\vspace{-0.2cm}\item 10-D MDP with $m=n=5$ and $l=5$. The action matrix $M_a$ is \textit{dense}, meaning that all its entries are nonzero. We sample the entries in $M_a$ from the uniform distribution over $[0,1)$ and subsequently normalize each row of $M_a$ to sum to 0.99 for stability. We apply the decomposition algorithms after $L=6000$ steps and train for a total of $N=200000$ steps.
\vspace{-0.2cm}\item 20-D MDP with $m=n=10$ and $l=10$. The action matrix $M_a$ is dense, and generated as above. We set $L=10000$ and $N=200000$.
\vspace{-0.2cm}\item 30-D MDP with $m=n=15$ and $l=8$. The action matrix $M_a$ is \textit{partial dense}, meaning that only $l=8$ out of the $m=15$ endogenous states are controlled, but these 8 states are controlled by all actions. $M_a$ is generated as above, except that the rows corresponding to non-controlled endo variables are 0. We set $L=15000$ and $N=200000$.
\vspace{-0.2cm}\item 30-D MDP with $m=n=15$ and $l=8$. The action matrix $M_a$ is \textit{partial disjoint}, meaning that only $l=8$ out of the $m=15$ endo states are controlled but each of these 8 states is controlled by a distinct action variable. We sample the $l=8$ nonzero entries of $M_a$ from the uniform distribution over $[0.5, 1.5)$. We set $L=15000$ and $N=200000$.
\vspace{-0.2cm}\item 35-D MDP with $m=20$, $n=15$ and $l=10$. The action matrix $M_a$ is partial disjoint, i.e., only 10 endogenous state variables are directly controlled through the 10 actions (each by a distinct action variable) whereas the remaining ones are controlled indirectly through the other endogenous states; furthermore, the endo transition matrix $M_e$ is \textit{sparse} with sparsity (fraction of nonzeros) 14.3\%. Specifically, $M_e$ is generated as in Section \ref{sec:linear-setting} except that only a small part of the matrix (equal to 14.3\%) is initialized to nonzero values. We set $L=20000$ and $N=200000$.
\vspace{-0.2cm}\item 70-D MDP with $m=40$, $n=30$ and $l=20$. The action matrix $M_a$ is partial disjoint,i.e., only 20 endogenous states are directly controlled through the 20 actions whereas the remaining ones are controlled indirectly through the other endogenous states. The endo transition matrix $M_e$ is sparse with sparsity 14.3\%. We set $L=35000$ and $N=300000$.
\end{itemize}

\begin{figure}[t!]
     \centering
     \begin{subfigure}[t]{0.46\textwidth}
         \includegraphics[scale=0.38]{multiple_5x5x5_full}
         \caption{Dense 10-D MDP.}
         \label{fig:5x5x5_full}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.44\textwidth}
         \includegraphics[scale=0.38]{multiple_10x10x10_full}
         \caption{Dense 20-D MDP.}
         \label{fig:10x10x10_full}
     \end{subfigure}
     
     
     \bigskip
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.38]{multiple_15x15x8_partial_full}
         \caption{Partial dense 30-D MDP.}
         \label{fig:15x15x8_partial_full}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.38]{multiple_15x15x8_partial_disjoint}
         \caption{Partial disjoint 30-D MDP.}
         \label{fig:15x15x8_partial_disjoint}
     \end{subfigure}
     
     \bigskip
     \begin{subfigure}[t]{0.46\textwidth}
         \includegraphics[scale=0.38]{multiple_15x20x10_partial_disjoint}
         \caption{Partial disjoint sparse 35-D MDP.}
         \label{fig:15x20x10_partial_disjoint}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.44\textwidth}
         \includegraphics[scale=0.38]{multiple_30x40x20_partial_disjoint}
         \caption{Partial disjoint sparse 70-D MDP.}
         \label{fig:30x40x20_partial_disjoint}
     \end{subfigure}
    \caption{RL performance for MDPs with multiple action variables.}
    \label{fig:multiple_actions}
\end{figure}
\begin{table}[t!]
{\footnotesize
\begin{center}
\begin{tabular}{ c c c c c c c } 
 \hline
 Action & Exo/Endo & \multirow{2}{*}{Method} & Exo Subspace & Total Time & Decomposition \\ 
 Variables & State Variables & & Rank & (secs) & Time (secs) \\ 
 \hline\hline
\multirow{4}{*}{\parbox{1.5cm}{5 (dense)}} & \multirow{4}{*}{5/5} & Baseline & - & 3576.8$\pm$293.2 & - \\
   &  & \grds{} & 7.47$\pm$0.50 & 4835.2$\pm$960.6 & 69.8$\pm$39.1 \\ 
   &  & Simplified-\grds{} & 7.47$\pm$0.50 & 4864.5$\pm$979.3 & 148.1$\pm$70.2 \\ 
   &  & \sras{} & 7.27$\pm$0.44 & 4914.6$\pm$1126.0 & 402.8$\pm$134.0 \\ 
 \hline
 \multirow{4}{*}{\parbox{1.5cm}{10 (dense)}} & \multirow{4}{*}{10/10}  & Baseline & - & 6162.7$\pm$478.0 & - \\
   &  & \grds{} & 15.6$\pm$0.49 & 7745.5$\pm$1299.4 & 396.5$\pm$228.8 \\ 
   &  & Simplified-\grds{} & 16.0$\pm$0.0 & 7731.9$\pm$1292.3 & 387.0$\pm$145.0 \\ 
   &  & \sras{} & 16.0$\pm$0.0 & 8064.7$\pm$2006.7 & 1546.7$\pm$483.1 \\ 
 \hline
 \multirow{4}{*}{\begin{tabular}{l}8 (partial\\ $\quad$dense)\end{tabular}} & \multirow{4}{*}{15/15} & Baseline & - & 8734.0$\pm$878.1 & - \\
   &  & \grds{} & 27.0$\pm$0.0 & 10400.0$\pm$1599.6 & 397.0$\pm$244.3 \\ 
   &  & Simplified-\grds{} & 27.0$\pm$0.0 & 10150.5$\pm$1452.2 & 556.7$\pm$121.8 \\ 
   &  & \sras{} & 26.93$\pm$0.25 & 11448.9$\pm$4052.9 & 3702.0$\pm$1730.1 \\ 
 \hline
 \multirow{4}{*}{\begin{tabular}{l}8 (partial\\ $\quad$disjoint)\end{tabular}} & \multirow{4}{*}{15/15} & Baseline & - & 8388.9$\pm$706.1 & - \\
   &  & \grds{} & 22.0$\pm$0.0 & 10399.5$\pm$2200.5 & 1136.4$\pm$433.9 \\ 
   &  & Simplified-\grds{} & 22.0$\pm$0.0 & 10362.2$\pm$1687.5 & 1520.7$\pm$304.4 \\ 
   &  & \sras{} & 22.0$\pm$0.0 & 10389.7$\pm$1877.7 & 1224.7$\pm$506.5 \\ 
 \hline
 \multirow{4}{*}{\begin{tabular}{l}10 (partial\\ $\quad$disjoint\\ $\quad$sparse)\end{tabular}} & \multirow{4}{*}{15/20} & Baseline & - & 12088.9$\pm$973.6 & - \\
   &  & \grds{} & 23.2$\pm$0.4 & 16126.3$\pm$1096.9 & 2799.9$\pm$473.5 \\ 
   &  & Simplified-\grds{} & 5.47$\pm$1.89 & 18627.7$\pm$3103.6 & 7019.8$\pm$1427.1 \\ 
   &  & \sras{} & 6.2$\pm$1.51 & 13984.1$\pm$6876.4 & 1976.3$\pm$1504.9 \\ 
 \hline
 \multirow{4}{*}{\begin{tabular}{l}20 (partial\\ $\quad$disjoint\\ $\quad$sparse)\end{tabular}} & \multirow{4}{*}{30/40} & Baseline & - & 35134.1$\pm$2877.3 & - \\
   &  & \grds{} & 50.0$\pm$0.0 & 43345.1$\pm$3470.3 & 10256.2$\pm$2719.9 \\ 
   &  & Simplified-\grds{} & 50.0$\pm$0.0 & 44187.4$\pm$3672.7 & 10670.1$\pm$3015.7 \\ 
   &  & \sras{} & 49.0$\pm$0.0 & 44350.5$\pm$3540.4 & 11691.1$\pm$2925.2 \\ 
 \hline
\end{tabular}
\end{center}
\caption{\label{table:multiple_actions}Mean and standard deviation of the rank of the discovered exo subspace, total execution time, and decomposition time for MDPs with multiple action variables.}}
\end{table}


Figures \ref{fig:5x5x5_full}-\ref{fig:30x40x20_partial_disjoint} plot RL performance for these 6 MDPs. We run 15 replications with different seeds and a reward regression learning rate of 0.0006. A first observation is that in all 6 settings the baseline struggles and shows very slow improvement over time time (e.g., Figure \ref{fig:10x10x10_full}). Its performance appears to decay on the smallest of these MDPs (Figure \ref{fig:5x5x5_full}). The Endo Reward Oracle performs visibly better than the Baseline and is able to attain higher rewards. However, it exhibits high variance and it improves very slowly (e.g., Figure \ref{fig:10x10x10_full}).

Surprisingly, the Simplified-\grds{} and \sras{} methods substantially outperform the Endo Reward Oracle. It appears that our methods are able to discover additional exogenous dimensions that reduce the variance of the endogenous reward below the level of the reward oracle. Table~\ref{table:multiple_actions} confirms that, with the exception of the 35-dimensional sparse, partial disjoint MDP, the algorithms are all discovering exogenous spaces of the expected size. For example, in the largest MDP, which has 70 dimensions and a 20-dimensional action space, \grds{} and Simplified-\grds{} both discover a 50-dimensional exogenous space. The algorithms are challenged by the fourth MDP ($n$=15, $m=20$, $l=10$). On this MDP, \grds{} finds a 23.2-dimensional exo space on average, but Simplified-\grds{} and \sras{} only find exo spaces with an average dimension of 5.47 and 6.2, respectively. They also exhibit high variation in the number of discovered dimensions. Despite these failures, Simplified-\grds{} and \sras{} perform very well on all six of these MDPs. \grds{} struggles on the dense MDPs, but does quite well on the sparse and partial disjoint MDPs. 

\subsubsection{Discrete MDPs}

The final variation in MDP structure that we studied was to create an MDP with discrete states. Figure \ref{fig:discrete-MDP} shows a simple routing problem defined on a road network.  There are 9 endogenous states corresponding to the nodes of the network. This MDP is episodic; each episode starts in the starting node, $v_0$, and ends in the terminal node $v_8$. Each edge in the network has a corresponding traversal cost, and the goal of the agent is to reach the terminal node while minimizing the total cost. 
There are 4 exogenous state variables; each of them is independent of the others and evolves as $x_{t+1,i} = 0.9\cdot x_{t,i} + \varepsilon_{exo}$, where $\varepsilon_{exo}$ is distributed according to $\mathcal{N}(0,1)$ and $i\in\{1,2,3,4\}$. These exogenous state variables are intended to model global phenomena such as amount of automobile traffic, fog, snow, and pedestrian traffic. These quantities evolve independently of the navigation decisions of the agent, but they modify the cost of traversing the edges. 

The reward function is the sum of two terms:
\[
r_t = -\textrm{cost}(s_t \rightarrow s_{t+1}) - \sum_{i=1}^4 x_{t,i}.\]
The first term is the endogenous reward $R_{end,t}$ and the second term is the exogenous reward $R_{exo,t}$.

The actions at each node consist in choosing one of the outbound edges to traverse. We restrict the set of actions to move only rightward (i.e., toward states with higher subscripts). For instance, there are three available actions at node $v_0$ corresponding to the three outgoing edges, but only a single action at node $v_4$. The cost of traversing an edge is shown by the edge weights in Figure ~\ref{fig:discrete-MDP}. The MDP is deterministic: a given action (i.e., edge selection) at a given node always results in the same transition. The observed state consists of the 1-hot encoding for the 9 endo states plus the 4-D continuous exo state variables.

\begin{figure}[b!]
     \centering
         \includegraphics[scale=0.8]{deterministic-toy-cropped}
    \caption{Graph for discrete MDP.}
    \label{fig:discrete-MDP}
\end{figure}

We apply episodic RL with PPO. Since the MDP is small, we modify some of the hyperparameters as follows. We set the total training steps to $N=10000$ and the policy update steps to $K=128$. We perform decomposition after $L=300$ episodes instead of the default value of 3,000 steps. The PPO batch size is set to 32. Every time we update the policy, we execute 300 evaluation episodes (in a separate instance of the MDP environment). For reward regression, we compute a Single Linear Regression rather than Online Neural Net Regression (see Section \ref{sec:impact-exo-regression}). We perform 15 replications, each with a different random seed.

Figure \ref{fig:deterministic} plots RL performance. Note that our methods perform on par with the Endo Reward Oracle and exhibit very little variance, with the exception of \sras{}. On the other hand, the Baseline makes very slow progress.  Table \ref{table:discrete-MDP} reports the rank of the discovered exogenous subspace as well as the total time and decomposition time. \grds{} and Simplified-\grds{} discover large exogenous subspaces of rank 11 and 10, respectively. The computed exogenous subspace includes the 4 exogenous state variables we defined as well as linear combinations of the endogenous states that do not depend or have only weak dependence on the action. In contrast, \sras{} discovers an exo subspace of very low rank, which explains its suboptimal performance. 

The strong performance of our methods might be due to deterministic dynamics of the problem. To test this, we created a stochastic version of the road network MDP. The transition probabilities are specified as follows:
\begin{itemize}
\vspace{-0.2cm} \item Taking action 0 / 1 / 2 at node $v_0$ leads to nodes $v_1, v_2, v_4$ with probabilities $(0.5, 0.3, 0.2)$ /$(0.3, 0.5, 0.2)$ / $(0.3, 0.2, 0.5)$, respectively.
\vspace{-0.2cm} \item Taking action 0 / 1 at node $v_1$ leads to nodes $v_4, v_5$ with probabilities $(0.6, 0.4)$ / $(0.5, 0.5)$, respectively.
\vspace{-0.2cm} \item Taking action 0 / 1 at node $v_2$ leads to nodes $v_3, v_4$ with probabilities $(0.5, 0.5)$ / $(0.3, 0.7)$, respectively.
\vspace{-0.2cm} \item Taking action 0 / 1 at node $v_3$ leads to nodes $v_6, v_7$ with probabilities $(0.7, 0.3)$ / $(0.4, 0.6)$, respectively.
\vspace{-0.2cm} \item Taking action 0 / 1 / 2 at node $v_4$ leads to nodes $v_5, v_6, v_8$ with probabilities $(0.6, 0.2, 0.2)$ /$(0, 1, 0)$ / $(0.3, 0.2, 0.5)$, respectively.
\vspace{-0.2cm} \item There is only one action (action 0) available at nodes $v_5, v_6, v_7$, and this leads with probability 1 to nodes $v_8, v_7, v_8$, respectively.
\vspace{-0.2cm} \item Node $v_8$ remains a terminal node.
\end{itemize}

\begin{figure}[t!]
     \centering
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{deterministic}
         \caption{Deterministic MDP.}
         \label{fig:deterministic}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{non-deterministic}
         \caption{Non-deterministic MDP.}
         \label{fig:non-deterministic}
     \end{subfigure}
    \caption{RL performance for discrete MDPs.}
    \label{fig:discrete-plots}
\end{figure}
\begin{table}
{\footnotesize
\begin{center}
\begin{tabular}{ c c c c c c } 
 \hline
 \multirow{2}{*}{MDP} & Exo/Endo State & \multirow{2}{*}{Method} & Exo Subspace & Total Time & Decomposition \\ 
 & Variables & & Rank & (secs) & Time (secs) \\ 
 \hline\hline
 \multirow{4}{*}{Deterministic} & \multirow{4}{*}{4/9} & Baseline & - & 225.6$\pm$16.7 & - \\
   &  & \grds{} & 9.47$\pm$0.50 & 271.4$\pm$55.8 & 68.4$\pm$26.9 \\
   &  & Simplified-\grds{} & 11.0$\pm$0.0 & 229.3$\pm$18.0 & 10.2$\pm$1.9 \\ 
   &  & \sras{} & 2.13$\pm$1.63 & 272.9$\pm$39.2 & 72.3$\pm$13.5 \\ 
 \hline
 \multirow{4}{*}{Non-deterministic} & \multirow{4}{*}{4/9} & Baseline & - & 419.9$\pm$10.8 & - \\
   &  & \grds{} & 9.66$\pm$0.70 & 503.6$\pm$102.2 & 109.1$\pm$42.7 \\
   &  & Simplified-\grds{} & 11.0$\pm$0.0 & 446.6$\pm$21.5 & 15.6$\pm$2.1 \\ 
   &  & \sras{} & 2.8$\pm$1.38 & 488.0$\pm$56.0 & 82.7$\pm$9.7 \\ 
 \hline
\end{tabular}
\end{center}
\caption{\label{table:discrete-MDP}Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition time for the discrete MDPs.}}
\end{table}

We employ the same hyperparameters as in the deterministic setting, except that we set the total training steps $N:=20000$ and the number of decomposition episodes $L:=600$. 

Figure \ref{fig:non-deterministic} plots the RL performance over 15 trials (each with a separate random seed). We observe that Simplified-\grds{} and \grds{} perform only slightly worse than the Endo Reward Oracle, while \sras{} exhibits slightly lower average performance and higher variance. The Baseline improves very slowly and lags far behind the other methods. Due to the stochastic transitions, all methods (and particularly the Baseline) exhibit larger variance than in the deterministic MDP in Figure \ref{fig:deterministic}. 

Regarding the rank of the computed subspace, we notice from Table \ref{table:discrete-MDP} that the ranks of the discovered exo subspaces are the same as for the deterministic MDP. This demonstrates the robustness of the algorithms. Simplified-\grds{} finds the largest exogenous space with \grds{} close behind. \sras{} finds much smaller spaces, which partially explains its slightly poorer performance. It is important to remember that not all dimensions of the discovered exogenous subspaces may be relevant to reward regression. \sras{} may only be discovering 2.8 dimensions, on average, but these are enough to give it a huge performance advantage over the Baseline. 

The total CPU cost and decomposition cost are higher than the deterministic setting, which reflects the added cost of running online reward regression for twice as many steps. 

In this problem, the rank-descending methods worked better than \sras{}, but even \sras{} gave excellent results, and the total CPU time required is not significantly higher than the Baseline.

\subsubsection{RQ4 Summary}

The experiments demonstrate that our methods are able to handle nonlinear rewards, combinatorial action spaces, and discrete state spaces without modification. When we introduced nonlinear transition dynamics, however, all of our methods performed poorly on at least one of the three MDPs tested. Excellent performance for Simplified-\grds{} and \sras{} was restored by increasing the value of $\epsilon$, but this did not improve \grds{} very much.  This is additional evidence that the high dimensional manifold optimization problems that \grds{} solves are difficult, particularly when the CCC is computed in a nonlinear setting.

\subsection{Anti-correlated Exo and Endo Rewards}

Recall in Section~\ref{sec:variance-analysis} we showed that if the endogenous and exogenous rewards are negatively correlated, the variance of the endogenous reward can be less than the variance of the full reward and there is no variance reduction benefit from computing the exo/endo reward decomposition. Specifically, Theorem~\ref{theorem:covariance-condition} introduced the covariance condition that the variance of the exogenous reward must be greater than twice the negative covariance between the exogenous and endogenous rewards. RQ5 asks how our algorithms behave when this covariance condition is violated. Does RL performance suffer?

To study this question, we define the following family of $2n$-D MDPs with a nonlinear exogenous reward
\begin{equation*}
\begin{split}
&x_{t+1} = 0.9\cdot M_{exo}\cdot x_{t} + \varepsilon_{exo}\\
&e_{t+1} = 0.45\cdot M_{end}\cdot e_{t} + 0.55\cdot M_{exo}\cdot x_{t} + M_a\cdot a_t + \varepsilon_{end}\\
&R_{end,t} = e^{-|\textrm{avg}(e_t) - 2.5|/3}\\
&R_{exo,t} = e^{-|\textrm{avg}(x_t) + 2.5|/3}.
\end{split}
\end{equation*}
The endo and exo states $e_t$ and $x_t$ are both $n$-D. The matrices $M_{exo}\in\mathbb{R}^{n\times n}$ and $M_{end}\in\mathbb{R}^{n \times n}$ follow the recipe given in Section \ref{sec:linear-setting}. $M_a\in\mathbb{R}^{n}$ is the coefficient for the action $a_{t}$, and it is set to be a vector of ones. The elements of the exogenous noise $\varepsilon_{exo}\in\mathbb{R}^{n}$ are distributed according to $\mathcal{N}(0,0.16)$, while the elements of the endogenous noise $\varepsilon_{end}\in\mathbb{R}^{n}$ are distributed according to $\mathcal{N}(0,0.04)$. The observed state vector $s_t\in\mathbb{R}^{2n}$ is a linear mixture of the hidden exogenous and endogenous states, as in Section \ref{sec:linear-setting}.

An interesting property of this class of MDPs is that the product $M_{exo}\cdot x_{t}$ appears with a relatively high coefficient in both $x_{t+1}$ and $e_{t+1}$. Hence, $e_{t+1}$ and $x_{t+1}$ will exhibit positive correlation. As a result, when $\textrm{avg}(e_t)$ (and thus $\textrm{avg}(x_t)$) is high and close to $2.5$, $R_{end,t}$ will be high and $R_{exo,t}$ will be low. Conversely, when $\textrm{avg}(e_t)$ (and thus $\textrm{avg}(x_t)$) is low and close to $-2.5$, $R_{end,t}$ will be low and $R_{exo,t}$ will be high. This causes the endo and exo rewards to be anti-correlated to the point that the covariance condition is violated. 

So verify this, we measured the variance $\V(R_{exo,t})$ and the covariance $-2\cdot\C(R_{end,t},R_{exo,t})$ for both MDPs under 2 policies: a random policy and the optimal policy. For the 2-D MDP, we find that $\V(R_{exo,t})=0.026$ and $-2\cdot\C(R_{end,t},R_{exo,t})=0.04$ under a random policy, and $\V(R_{exo,t})=0.026$ and $-2\cdot\C(R_{end,t},R_{exo,t})=0.037$ under the optimal policy. For the 4-D MDP, we find that $\V(R_{exo,t})=0.021$ and $-2\cdot\C(R_{end,t},R_{exo,t})=0.033$ under a random policy, and $\V(R_{exo,t})=0.021$ and $-2\cdot\C(R_{end,t},R_{exo,t})=0.040$ under the optimal policy. Hence, the covariance condition is violated in all cases. 

\begin{figure}
     \centering
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{anticorrelated_1}
         \caption{2-D MDP ($n=1$).}
         \label{fig:anticorrelated_1}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{anticorrelated_2}
         \caption{4-D MDP ($n=2$).}
         \label{fig:anticorrelated_2}
     \end{subfigure}
    \caption{RL performance for MDPs with anti-correlated exo and endo rewards.}
    \label{fig:anticorrelated_rewards}
\end{figure}

Figures \ref{fig:anticorrelated_1} and \ref{fig:anticorrelated_2} show the RL performance for the different methods when $n=1$ and $n=2$, respectively.The exo subspace ranks and time costs are summarized in Table \ref{table:anticorrelated_rewards}. Since the MDPs are rather small, for the former we set $L=1000$ for the number of steps prior to applying the state-space decomposition methods and train for $N=30000$. For the latter set $L=2000$ and $N=40000$. We perform 15 trials with different seeds.

Notice how the Endo Reward Oracle performs worse than the Baseline; this is to be expected, because the Endo Reward Oracle is forced to use the Endo reward, which has higher variance than the full reward. Our naive expectation was that our decomposition methods would exhibit the same behavior, since they seek to find the maximal exogenous subspace. However, what we observe is that all of our methods do much better than the Endo Reward Oracle and, in the case of $n=1$, they exceed the Baseline as well.

The key to understanding what is happening is to focus on the reward regression. The loss function for the reward regression is precisely the residual variance of the predicted reward, and the residual reward is precisely our estimate of the endogenous reward. Consequently, reward regression will ignore any exogenous variables that would cause an increase in the variance of the endogenous reward. In the case of $n=1$, the reward regression has apparently found some component of the exogenous reward that is not anti-correlated with the endogenous reward and exploited it to reduce the variance of the endogenous reward. Consequently our methods are able to learn faster than the Baseline. 

The larger result is that our algorithms can never be affected by anti-correlated rewards. In the worst case, the reward regression will ignore the exogenous variables and simply predict the mean exogenous reward.  This will have no effect on the endo-MDP or the RL performance of our methods. 

\begin{table}[ht!]
{\footnotesize
\begin{center}
\begin{tabular}{ c c c c c c } 
 \hline
 Exo State & Endo State & \multirow{2}{*}{Method} & Exo Subspace & Total Time & Decomposition \\ 
 Variables & Variables & & Rank & (secs) & Time (secs) \\ 
 \hline\hline
 \multirow{4}{*}{1} & \multirow{4}{*}{1} & Baseline & - & 221.9$\pm$9.7 & - \\
   &  & \grds{} & 1.0$\pm$0.0 & 276.0$\pm$38.3 & 2.2$\pm$0.7 \\ 
   &  & Simplified-\grds{} & 1.0$\pm$0.0 & 276.9$\pm$36.8 & 2.2$\pm$0.6 \\ 
   &  & \sras{} & 1.0$\pm$0.0 & 276.6$\pm$39.6 & 2.3$\pm$0.8 \\ 
 \hline
 \multirow{4}{*}{2} & \multirow{4}{*}{2} & Baseline & - & 308.4$\pm$9.4 & - \\
   &  & \grds{} & 2.4$\pm$0.8 & 402.8$\pm$67.0 & 22.5$\pm$25.2 \\ 
   &  & Simplified-\grds{} & 3.0$\pm$0.0 & 394.3$\pm$55.5 & 6.5$\pm$0.8 \\ 
   &  & \sras{} & 2.26$\pm$0.93 & 403.8$\pm$80.6 & 24.0$\pm$23.1 \\ 
 \hline
\end{tabular}
\end{center}
\caption{\label{table:anticorrelated_rewards}Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition time for the MDPs with anti-correlated exo and endo rewards.}}
\end{table}

\subsection{Risks and Benefits of Simplified-\grds{}}

Our experimental results suggest that the Simplified-\grds{} method performs very well. RQ6 asks about the risks of the Simplified-\grds{} method. From a mathematical perspective, when we minimize $I(X';A \mid X)$ (or the CCC approximation), we are only eliminating edges from $A$ to $X'$, which are the direct effects of $A$. We are not excluding any indirect path by which $A$ might affect $E'$, and $E'$ might then affect $X$ in some future time step. In the linear dynamical MDPs that we have studied, such indirect effects do not arise, and all effects of $A$ on $X'$ are visible immediately.  Of course, because Simplified-\grds{} verifies the full constraint $I(X';[E,A] \mid X)<\epsilon$, Simplified-\grds{} is still sound, but it may fail to find the maximal exogenous subspace in complex MDPs (as we saw in some of the modified MDPs). 

This suggests that for general MDPs, we should choose \grds{} rather than Simplified-\grds{}. However, our experiments have also shown that there are several cases where \grds{} encounters difficulty minimizing the CCC proxy of the $I(X';[E,A] \mid X)$ objective. Hence, we believe additional research is needed to improve the manifold optimization process so that \grds{} can overcome these challenging cases.

\subsection{Practical Considerations}
\label{sec:practical-considerations}

In this section, we switch our attention to practical aspects of our proposed methods. Our goal is to answer RQ7, which concerns how to set the hyperparameters of our method.

\subsubsection{Impact of Hyperparameters on Baseline}

First, we investigate the impact of hyperparameters on the Baseline. For this purpose, we consider the High-D linear setting of Section \ref{sec:linear-setting}. Recall that for this setting we used the default PPO and Adam hyperparameters in stable-baselines3, summarized in Table \ref{table:hyperparams}. We now ask  whether the performance of Baseline can be improved by using different hyperparameters. If that were the case, then a more careful hyperparameter tuning could be an alternative to our proposed algorithms. 

\begin{figure}[t!]
     \centering
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{baseline_batch_size}
         \caption{Tuning batch size.}
         \label{fig:baseline-batch-size}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{baseline_learning_rate}
         \caption{Tuning learning rate.}
         \label{fig:baseline-learning-rate}
     \end{subfigure}
    
    \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{baseline_policy_update_steps}
         \caption{Tuning policy update steps $K$.}
         \label{fig:baseline-policy-update-steps}
     \end{subfigure}
    \caption{RL performance for Baseline of Figure \ref{fig:3x2} under various hyperparameters.}
    \label{fig:baseline-hyperparams}
\end{figure}

We consider the 5-D MDP with 3 exo and 2 end variables of Figure \ref{fig:3x2}, where the Baseline fluctuates between 0.4 and 0.5 with an average of 0.45, visibly lower than all other methods. To understand whether this can be improved further, we tune 3 critical hyperparameters of PPO optimization. We let the batch size take values in $\{16, 32, 64, 128, 256, 512\}$ (Figure \ref{fig:baseline-batch-size}), the learning rate take values in $\{5\times 10^{-5}, 1\times10^{-4}, 5\times 10^{-4}, 1\times 10^{-3}, 5\times 10^{-3}, 1\times 10^{-2}\}$ (Figure \ref{fig:baseline-learning-rate}), and the number of steps per policy update $K$ take values in $\{250, 500, 750, 1000, 2000, 3000\}$ (Figure \ref{fig:baseline-policy-update-steps}). For each experiment, all hyperparameters except for the tuned one are set to the values in Figure~\ref{fig:3x2}. For Figure~\ref{fig:baseline-policy-update-steps}, we perform policy evaluation every $1536$ steps, instead of after each policy update, to ensure that all curves have the same number of evaluation points. We use 10 independent replications with different seeds. Finally, we increase the number of training steps to $N=80000$ to ensure that the Baseline has enough training budget to converge.

The results demonstrate that none of the parameter combinations can raise performance significantly. Some values for the number of policy update steps manage to slightly improve the average Baseline performance to 0.5 from 0.45, but they still suffer from significant variance. This is in sharp contrast to our algorithms and the Endo Reward Oracle in Figure \ref{fig:3x2}, which all exhibit much lower variance. Given that this experiment only tunes one hyperparameter at a time, we cannot exclude the possibility that there are combinations of hyperparameters that can achieve a higher and more stable performance for the Baseline. However, its low performance under the default hyperparameters and on a range of reasonable values provides evidence that its poor performance mainly stems from the stochasticity of the exogenous rewards and not badly-chosen hyperparameters. 

\subsubsection{Sensitivity Analysis}
\label{sec:sensitivity-analysis}

\begin{figure}[t!]
     \centering
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{sensitivity_analysis_global}
         \caption{Simplified-\grds{}.}
         \label{fig:sensitivity_analysis_global}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{sensitivity_analysis_stepwise}
         \caption{\sras{}.}
         \label{fig:sensitivity_analysis_stepwise}
     \end{subfigure}
     \caption{RL performance for varying values of $L$, the number of steps prior to applying the decomposition algorithms, for the 10D setting of Figure \ref{fig:5x5}.}
    \label{fig:sensitivity_analysis}
\end{figure}

Recall from Section \ref{sec:setup} that the main hyperparameter we need to set for our proposed methods is the number of steps $L$ prior to applying the exogenous subspace discovery algorithms. This specifies the number of $\langle s, a, r, s'\rangle$ tuples that are collected for subspace discovery. In this section, we shed light on the impact of $L$ on RL performance. In this direction, we consider the 10-D setting with 5 exo and 5 endo variables of Figure \ref{fig:5x5}, where our methods and the Endo Reward Oracle converge to a total reward of 0.8 in $N=50000$ steps. In contrast, the Baseline only attains a reward of around 0.3 on average. We perform sensitivity analysis by considering twelve possible values for $L$: $250, 500, 750, 1000, 1500, 2000, 2500, 3000, 4000, 5000, 6000, 7000$. We denote each value by the increasing sequence $L_i,i\in\{0,\dots,11\}$, where $L_0=250$ and $L_{10}=7000$. We report the average RL performance for the Simplified-\grds{} and \sras{} methods in Figure \ref{fig:sensitivity_analysis}(a,b), since they both perform very well. For both methods, when $L_0=250$, performance is barely better than the Baseline. As we increase $L$, performance improves steadily and almost matches the Endo Reward Oracle as soon as we reach 2000 decomposition steps. Increasing $L$ beyond 2000 gives minor benefit and delays the time at which PPO can take advantage of the improved reward function. 

This suggests that the decomposition algorithms have converged after $L=2000$. Can we verify this? The rank of the discovered exogenous space is not informative, because it is always 9-dimensional for all 12 values of $L$ and across all 10 replications. To get a finer-grained measure, we can take advantage of the fact that the complement of the discovered 9-dimensional exogenous space is a 1-dimensional space. This means it can be represented by a direction vector, and we can compare different solutions by computing the angles between these direction vectors. 

Figure \ref{fig:angle_deltas} depicts the angle (in radians) between the orthogonal complements of the exogenous subspaces for each consecutive pair of $L_i$ and $L_{i+1}$ values. If the methods had converged, these angles would be zero. They are not zero, which shows that the exogenous subspaces are continuing to change as $L$ increases. But the angles are all very small (less than 0.5 degrees in the largest case), so these changes are not large, and they are converging (with few exceptions) monotonically toward zero. Simplified-\grds{} exhibits smooth convergence, whereas \sras{} shows higher variance and a few bumps. 

\begin{figure}[t!]
     \centering
         \includegraphics[scale=0.5]{angle_deltas}
    \caption{Angle between orthogonal complements of computed exo subspaces corresponding to $L_i$ and $L_{i+1}$ decomposition steps, where the index $i$ ranges from 0 to 10.}
    \label{fig:angle_deltas}
\end{figure}

The results suggest that manifold optimization performs very well on this problem, even with relatively small numbers of samples. What is then the reason for the different performance levels in Figure \ref{fig:sensitivity_analysis}? The answer lies in the exogenous reward regression. Recall that after $L$ steps, we conclude Phase 1 by computing the decomposition and then fitting the exogenous reward neural net to the $L$ collected observations. Even though different numbers of decomposition steps result in almost identical exogenous subspaces, the subsequent exogenous reward regression can yield dramatically different exo reward models. When the value of $L$ is very low, we have only a limited number of samples for the exo reward regression, and these might not to cover the exo state subspace adequately. As a result, the learned exo reward model may overfit the observations and fail to generalize to other subspaces. The subsequent online neural network reward regression in Phase 2 only processes each new observation once, so learning the correct exo reward model can take many steps, and cause PPO to learn slowly.

To confirm the above, we perform a second experiment. Unlike previous experiments, we decouple decomposition and exogenous reward regression. Decomposition still takes place after $L_i$ steps. But reward regression is now performed after 3000 samples have been collected (3000 is the default value for $L$ in the high-D experiments). After learning the exogenous reward model with the 3000 samples, we proceed to Phase 2. We experiment with three options for exogenous reward regression in Phase 2: (i) standard online learning where we update the exogenous reward model every $M=256$ steps; (ii) a single linear regression after which the exogenous reward model is never updated; and (iii) repeated linear regression where we fit a new exogenous reward model from scratch every $M=256$ steps. We study the three lowest values for $L_i$ ($L_0=250, L_1=500$ and $L_2=750$), as these were the values in Figure \ref{fig:sensitivity_analysis} with the worst performance.

\begin{figure}[t!]
     \centering
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{special_global}
         \caption{Simplified-\grds{}}
         \label{fig:special_global}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.4]{special_stepwise}
         \caption{\sras{}}
         \label{fig:special_stepwise}
     \end{subfigure}
    \caption{RL performance when computing the exo/endo decomposition at $L_i = 250, 500,$ and $750$ steps. Reward regression starts at 3000 steps. Default: online neural network regression; Single Regression: single linear regression at 3000 steps; Repeated Regression: linear regression every 256 steps.}
    \label{fig:sensitivity_analysis_2}
\end{figure}

Figure \ref{fig:sensitivity_analysis_2} plots the RL performance averaged over 10 independent trials. We make several observations. First, increasing the number of steps for learning the initial exogenous reward model from $L_i$ to 3000 improves RL performance. With online learning, we match the Endo Reward Oracle's average performance of 0.8. This confirms our previous hypothesis that the reason for the bad performance in Figure \ref{fig:sensitivity_analysis} was the poor initial exogenous reward model. With a single regression, RL performance improves (especially for $L_0=250$ and $L_1=500$), but it performs worse than online learning while having greater variance. 
Interestingly, learning a new linear regression model every $M=256$ steps performs the best and slightly outperforms online neural network regression. A plausible reason for this is that online regression only performs a single pass over the data, so it may adapt to the changing state and reward distribution more slowly. On the other hand, repeated linear regression requires 10 times as much computation time as online regression for the settings in Figures \ref{fig:special_global} and \ref{fig:special_stepwise}.

\subsubsection{Practical Guidelines For State Decomposition and Exo Reward Regression}
This sensitivity analysis suggests the following procedure for computing state space decompositions and reward regressions. Start with a small number for $L$ (e.g., 250) and compute the corresponding exo subspace after $L$ steps. Then, every $\Delta L$ steps (e.g., 250), recompute the exo subspace until the discovered exo subspace stops changing. This can be detected when (i) the rank of the subspace does not change, and (ii) the largest principal angle between consecutive subspaces is close to 0 \citep{principal-angles}. 

As soon as we have an initial exo subspace, we can fit the exogeneous reward model, and each time we recompute the subspace, we can re-fit the model. These initial fits could be performed with linear regression or neural network regression. Once the exogenous subspace as converged, we can switch to online neural network regression, because the regression inputs will have stabilized. 

Application constraints may suggest alternative procedures. If each step executed in the MDP is very expensive, then the cost of the multiple decomposition and reward regression computations is easy to justify. However, if MDP transitions are cheap, then we need to take a different approach. If many similar MDPs will need to be optimized, we can use this full procedure on a few of them to determine the value of $L$ at which the exo space converges. We then just use that value to trigger exo/endo decomposition and perform neural network reward regression starting at step $L$ and continuing online. If there is only one MDP to be solved, $L$ could be selected using a simulation of the MDP. In all of our experiments, we have followed this procedure for setting $L$. 

A last question concerns the value for the CCC threshold $\epsilon$. In theory, we should use very low values to minimize the chance of discovering an invalid exo/endo state decomposition, but this could come at the cost of having to perform more steps in \grds{} and \sras{}. We lack theoretical guidance for making this decision. In principle, one could start with a somewhat large value for $\epsilon$ (e.g., 0.1) and perform multiple runs (in simulation or on a sample of MDPs) with progressively smaller values of $\epsilon$ until either the discovered exo subspace converges or RL performance stabilizes.

\subsection{Discussion}

Let us review the research questions and our results.
\begin{itemize}
    \item[RQ1:] Do our methods speed up reinforcement learning in terms of sample complexity? In terms of total CPU time? We found that our methods yield dramatic speedups for reinforcement learning both in terms of the number of MDP transitions observed and in terms of clock time. The improvements are so dramatic that we were not able to run the Baseline method long enough for it to match the level of RL performance achieved by our methods, so we estimated this by linear extrapolation.
    \item[RQ2:] Do our methods discover the correct maximal exogenous subspaces? Our methods surprised us by finding exogenous subspaces that were larger than we had naively expected. Analysis showed that these subspaces did indeed have maximal dimension. Our experiments measured the correctness of the subspaces by the resulting RL performance. In nearly all cases, our methods matched the performance of the Endo Reward Oracle that was told the correct subspace for the endogenous reward. In the 2-D covariance condition experiment, our methods did better than the Endo Reward Oracle.
    \item[RQ3:] What is the best approach to reward regression? Single linear, repeated linear, or online neural network regression? We found that online neural network regression worked best and was more stable, even in cases where the reward function was linear.
    \item[RQ4:] How do the algorithms behave when the MDPs are changed to have different properties: (a) rewards are nonlinear, (b) transition dynamics are nonlinear, (c) action space is combinatorial, and (d) states and actions are discrete? Our methods, combined with neural network regression, handled the nonlinear rewards well. When the transition dynamics are nonlinear, our methods sometimes struggled, but we found that they worked well if we increased the $\epsilon$ parameter to allow the CCC objective to be substantially nonzero. Our methods worked very well in combinatorial action spaces and in discrete MDPs.
    \item[RQ5:] How do the algorithms behave when the covariance condition is violated? When the covariance condition is violated, our methods still give excellent results. This is because reward regression minimizes the variance of the endogenous reward, so it only fits that aspect of the exogenous reward that can be removed without increasing the variance of the endogenous reward. 
    \item[RQ6:] What are the risks and benefits of using the simplified objective in \sras{} and Simplified-\grds{}? The Simplified-\grds{} method gave the best overall RL performance. It provides a nice balance of computational efficiency and optimization performance while remaining sound. 
    \item[RQ7:] How should hyperparameters be set? How many training tuples should be collected before performing exogenous subspace discovery? When should reward regression begin, and on what schedule? We found that the default values of the hyperparameters worked well, and we developed a tuning procedure for determining the training set size $K$ for exogenous subspace discovery. It starts with $K=250$ and repeatedly adds 250 tuples until the exo space stabilized. Small values of $K$ worked surprisingly well. 
\end{itemize}

The benefits of our algorithms are not necessarily limited to linear MDPs. We observed strong performance with general nonlinear exo reward functions, and often on MDPs with nonlinear exo/endo transition dynamics but where the state space could be linearly decomposed into the exo and endo subspaces. This showed that our CCC objective can provide good results even in these nonlinear cases. An alternative to our CCC-based linear approach would be to enforce the exo/endo decomposition using cross-covariance operators on a Reproducing Kernel Hilbert Space (RKHS). However, RKHS computations can be computationally challenging and are difficult to scale to high-dimensional spaces \citep{fukumizu2008kernel}. 

\section{Related Work}\label{sec:related}
In this section, we review prior work on reinforcement learning with exogenous information that is not covered elsewhere in the paper. \cite{efroni2022provably} introduce the Exogenous Block Markov Decision Process (EX-BMDP) setting to model environments with exogenous noise. Under the assumption that the endogenous state transition is near deterministic, they propose the Path Predictive Elimination (PPE) algorithm. PPE learns a form of multi-step inverse dynamics \citep{paster2021}. It can recover the latent endogenous model while being both sample efficient and computationally efficient. PPE runs in a reward-free setting and does not make any assumptions about the reward function. In their follow-up work, \cite{efroni2022sample-efficient} introduce the ExoMDP setting, which is a finite state-action variant of the EX-BMPD where the state directly decomposes into a set of endogenous and exogenous factors, while the reward only depends on the endogenous state.  They propose ExoRL, an algorithm that learns a near-optimal policy for the ExoMDP with sample complexity polynomial in the number of endogenous state variables and logarithmic in the number of exogenous components. Because the reward does not depend on the exogenous state variables, those variables are not only exogenous but irrelevant, and they can be ignored during reinforcement learning. In our work, in contrast, the exogenous variables can still be important for the policy, so the RL algorithm must consider them as inputs when learning the policy.

In a different thread of work, \cite{chitnis2020} address the problem of learning a compact model of an MDP with endogenous and exogenous components for the purpose of planning. They consider only reducing the exogenous part of the state, given that is the part that induces the most noise, and assume that the reward function decomposes into a sum over the individual effects of each exogenous state variable. They introduce an algorithm based on the mutual information among the exogenous state variables that generates a compact representation, and they provide conditions for the optimality of their method. Our assumption of additive reward decomposition is weaker than their per-state-variable assumption.

Another line of work related to exogenous information concerns curiosity-driven exploration by \cite{pathak2017}, where the goal is to learn a reward to enable the agent to explore its environment better in the presence of very sparse extrinsic rewards. It falls under the well-studied class of methods with intrinsic rewards \citep{bellemare2016,haber2018,houthooft2016,oh2015,ostrovski2017,badia2020} This work employs a self-supervised inverse dynamics model, which encodes the states into representations that are trained to predict the action. As a result, the learned representations do not include environmental features that cannot influence or are not influenced by the agent's actions. Based on these representations, the agent can learn an exploration strategy that removes the impact of the uncontrollable aspects of the environment. Note that the inverse dynamics objective is generally not sufficient for control and does not typically come with theoretical guarantees \citep{rakelly2021}. Finally, we mention that reinforcement learning with exogenous information has also been studied from a more empirical standpoint in real problems such as learning to drive \citep{chen2021}.

Finally, our work shares similarities with other work on modeling the controllable aspects in the environment \citep[e.g.,][]{choi2019,song2020,burda2018,bellemare2012,corcoll2022,thomas2018}. The main difference is that in exo/endo MDPs, we capture the controllable versus uncontrollable aspects via the exo/endo factorization, where endo (resp., exo) states correspond to the controllable (resp. uncontrollable) aspects. Combining the two families of approaches could be an avenue for future research. We also note the work by \cite{Yang2022}, which proposes the dichotomy of control for return-conditioned supervised learning. This is accomplished by conditioning the policy on a latent variable representation of the future and introducing two conditional mutual information constraints that remove any information from the latent variable that has to do with randomness in the environment. However, unlike ours, their work is not concerned with exogenous state variables and decompositions.

\section{Concluding Remarks}
In this paper, we proposed a causal theory of exogeneity in reinforcement learning and showed that in causal models satisfying the full and diachronic two-step DBN structures, the exogenous variables are causally exogenous. 
We introduced exogenous-state MDPs with additively decomposable rewards and proved that such MDPs can be decomposed into an exogenous Markov reward process and an endogenous MDP such that any optimal policy for the endogenous MDP is an optimal policy for the original MDP. We studied the properties of valid exo/endo decompositions and proved that there is a maximal exogenous subspace that contains all other exogenous subspaces. We also showed that not all subsets of the maximal exogenous subspace define valid exo/endo decompositions. 

Given an exogenous-state MDP, we wish to discover a useful exogenous subspace of its state space. We explored several optimization formulations based on conditional mutual information for discovering the exogenous state variables and estimating the exogenous reward. We developed two practical algorithms, \grds{} and \sras{}, for the case when the exogenous space is a linear projection of the full state space. Our algorithms use the conditional correlation coefficient (CCC) as a measure for conditional independence and rely on solving a manifold optimization problem. Under the assumption that the exploration policy visits all states and tries all actions infinitely often, we showed that \grds{} discovers the maximal exogenous subspace. Furthermore, under the assumption of faithfulness, the exogenous subspace we discover is guaranteed to contain only causally-exogenous components. We also introduced Simplified-\grds{}, which employs a simplified CCC objective and then checks the solution to see if it satisfies the full CCC objective. 

Our experiments on a variety of high-dimensional linear dynamical MDPs demonstrated that our algorithms, particularly Simplified-\grds{} and \sras{}, can greatly accelerate reinforcement learning. Additional experiments showed that these methods can also give excellent performance on MDPs with nonlinear rewards, nonlinear transition dynamics, combinatorial action spaces, and discrete state spaces. The methods are robust to hyperparameter settings and do not require delicate hyperparameter tuning. 

There are several important directions for future research. First, we lack a deep theoretical understanding of the manifold optimization problem corresponding to the CCC. It would be helpful to shed light on its sample complexity and on its loss landscape, which may contain local minima. It is also critical to elucidate the conditions under which the CCC can capture conditional independence, at least for linearly decomposable MDPs. Second, we lack a rigorous theory of the impact of exogenous reward regression on RL convergence. Such knowledge could help us design better exogenous reward regression methodologies. Third, in our work we constrain the exogenous rewards to be additive. However, it is possible that in real settings exogenous rewards are non-additive (e.g., multiplicative), and our decomposition theorem will not hold. Investigating non-additive decompositions would be valuable. Fourth, a more precise mathematical analysis of the simplified objective that we employ in the Simplified-\grds{} and \sras{} algorithms might help explain their strong empirical performance. 
Fifth, it would be nice to study nonlinear exo/endo decompositions, especially in the presence of nonlinear transition dynamics. Can efficient, general strategies be discovered? Finally, are there alternatives to the assumptions (admissible MDP, fully-randomized policy, and faithfulness) that we adopted to prove causal exogeneity of the subspaces discovered by \grds{}?

\section*{Acknowledgments}
We would like to acknowledge support for this project
from the National Science Foundation (NSF grant IIS-9988642), the Multidisciplinary Research Program of the Department
of Defense (MURI N00014-00-1-0637), and a gift to the Oregon State University Foundation from Huawei. We thank Yuvraj Sharma for porting our implementation to Python and assisting with some of the experiments.

\appendix

\section{Additional Theory on Exogenous State Variables}
\label{app:additional-theory}
Definition \ref{def:exogenous} for causal exogeneity is inherently structural: it defines causal exogeneity as a property of the structure of the causal graph. In this appendix, we exploit this to structurally characterize causally exogenous state variables (Appendix \ref{app:structural-characterization}) as well as the causal graphs with causally exogenous state variables (Appendix \ref{app:complete-exo-set}).

\subsection{Structural Characterization of Causally-Exogenous State Variables}
\label{app:structural-characterization}

We begin by providing a general structural condition in terms of the unrolled DBN of an MDP that guarantees causal exogeneity.
To this goal, we introduce the structural notion of \textit{action-disconnected states}; we subsequently show that action-disconnected state variables are causally exogenous.
\begin{definition}[Action-Disconnected State Variables]\label{def:structural}
A state variable $C$ is \emph{action-disconnected}, if the unrolled decision diagram contains no directed chain of the form $A_t\to\cdots\to C_{\tau}, \forall t\in\{0,\dots,H-1\}, \forall \tau\in\{t+1,\dots,H\}$.
\end{definition}

To visualize action-disconnected state variables, consider the 3-state MDP of Figure \ref{fig:causal-counterexample} and assume a horizon $H=2$. The unrolled diagram for that value of $H$ is depicted in Figure \ref{fig:causal-counterexample-unrolled}. State variables $S_1$ and $S_2$ are action-disconnected, whereas $S_3$ is not due to the presence of the directed chains $A\to S_3'\to S_3''\to S_3'''$, $A'\to S_3''\to S_3'''$, and $A'''\to S_3'''$. 

\begin{figure}[b!]
     \centering
     \begin{subfigure}[t]{0.38\textwidth}
         \includegraphics[scale=0.8]{causality_counterexample-cropped}
         \caption{MDP.}
         \label{fig:causal-counterexample}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.45\textwidth}
         \includegraphics[scale=0.8]{causality_counterexample_unrolled-cropped}
         \caption{Unrolling the MDP for $H=3$.}
         \label{fig:causal-counterexample-unrolled}
     \end{subfigure}
     \caption{An MDP with 3 state variables $S_1,S_2,S_3$, where $S_1$ and $S_2$ are exogenous.}
    \label{fig:example_MDP}
\end{figure}

\begin{theorem}\label{theorem:forward_direction}
If a state variable $C$ is action-disconnected, then $C$ is causally exogenous.
\end{theorem}
\begin{proof}
Let $\mathcal{G}$ be the graph corresponding to the unrolled decision diagram of the MDP from timestep 0 to timestep $H$.
Define a trail as a loop-free and undirected (i.e., all edge directions are ignored) path between two nodes. We will make use of the $d$-separation theory with trails \citep[see][]{pearl1988,pearl2009}. 
Furthermore, recall Rule 3 of the do-calculus for any causal graph $\mathcal{G}$ $$P(F \mid \doop(G),\doop(H),J)=P(F \mid \doop(G),J), \textrm{ if } F\independent H \mid G \cup J \textrm{ in } \tilde{\mathcal{G}},$$
where $\tilde{\mathcal{G}}$ is the graph obtained by first deleting all edges pointing into $G$ and then deleting all arrows pointing into $H$ from nodes that are not ancestors of $J$. 
According to Definition \ref{def:exogenous}, in order to show that $C$ must be exogenous, we must prove that $$P(C_{t+1}, \ldots, C_H\mid C_t, \textnormal{do}(A_t)) = P(C_{t+1}, \ldots, C_H\mid C_t).$$ 
We will prove this statement by applying Rule 3 of the do-calculus above. For this purpose, we set $F\gets C_{t+1}, \ldots, C_H$, $G\gets \emptyset$, $H\gets A_t$, and $J\gets C_t$. Given $G$ is the empty set and $A_t$ cannot have incoming edges from any ancestor of $C_t$, the graph $\tilde{\mathcal{G}}$ will be identical to $\mathcal{G}$ except that the incoming edges to $A_t$ are deleted. To show exogeneity, it then suffices to show that $$C_{t+1}, \ldots, C_H\independent A_t \mid C_t \textrm{ in } \tilde{\mathcal{G}}.$$

Indeed, consider any trail $P$ connecting node $A_t$ to node $C_{t+\tau}$ in $\tilde{\mathcal{G}}$. 
Since $C$ is action-disconnected, we know that there can be no directed chain from $A_t$ to $C_{t+\tau}$ of the form $A_t\to\cdots\to C_{t+\tau}$. We argue that this then implies that $P$ must necessarily contain a collider node $Z$ of the form $\to Z\gets$. To see why, notice that the first link in $P$ has the form $A_t\to\cdots$. If all subsequent links in $P$ are of the form $\cdots\to\cdots$, then $P$ would be a directed chain, which is a contradiction. Hence, the edge directionality at $P$ must change at some node, which implies there must be some collider node $Z$ in $P$. The following two cases are then possible:
\begin{enumerate}
\item If $Z$ is not $C_t$ or an ancestor of $C_t$, then the $P$ is $d$-separated and thus does not violate the causal Definition \ref{def:exogenous}.
\item Otherwise, assume that $Z$ is either $C_t$ or an ancestor of $C_t$. By the structure of the causal graph, it is not hard to see that in order for this assumption to hold, $P$ must contain a link $V_t \gets U_{t-1}$ from some state or action variable $U_{t-1}$ at time step $t-1$ to some state variable $V_t$ at time step $t$. Consider the sub-trail up to node $U_{t-1}$, which will have the form $A_t\to\cdots V_t \gets U_{t-1}$. The sub-trail must contain a collider node $\tilde{Z}$; furthermore, $\tilde{Z}$ has time index $t$ or higher, so it cannot be $C_t$ or an ancestor of $C_t$. But this shows that trail $P$ is also $d$-separated and, hence, does not violate the causal exogeneity Definition \ref{def:exogenous}.
\end{enumerate}

\end{proof}
According to Corollary \ref{theorem:equivalence}, state variables $S_1$ and $S_2$ in the MDP of Figure \ref{theorem:forward_direction} are causally exogenous.
What about the reverse direction? We show that it is a natural implication of Definition \ref{def:exogenous} for causal exogeneity.
\begin{theorem}\label{theorem:reverse_direction}
If state variable $C$ is causally exogenous, then $C$ must be action-disconnected in the corresponding causal graph $\mathcal{G}$.
\end{theorem}
\begin{proof}
Assume $C$ is not action-disconnected. Then there must exist some directed chain $A_t\to\dots\to C_{t+\tau}$ connecting $A_t$ to some future state $C_{t+\tau}$. This chain cannot go through $C_t$ due to the structure of $\mathcal{G}$. In that case, $A_t$ and $C_{t+\tau}$ are not $d$-separated by $C_t$ in graph $\tilde{\mathcal{G}}$, where $\tilde{\mathcal{G}}$ is identical to $\mathcal{G}$ except that the incoming edges to $A_t$ are deleted. As a result, we cannot apply Rule 3 of the do-calculus to simplify expression $P(C_{t_\tau}\mid \textrm{do}(A_t),C_t)$ to $P(C_{t_\tau}\mid C_t)$. The property $P(C_{t+\tau } \mid \textrm{do}(A_t),C_t) = P(C_{t+\tau } \mid C_t)$ cannot thus hold as a result of the DAG structure of $\mathcal{G}$. Intuitively, the existence of chain $A_t\to\dots\to C_{t+\tau}$ in $\tilde{\mathcal{G}}$ means that there is a path through which an intervention $\textrm{do}(A_t)$ on $A_t$ can influence $C_{t+\tau}$, which implies that state variable $C$ cannot be causally exogenous.
\end{proof}
It is not hard to see that Theorems \ref{theorem:forward_direction} and \ref{theorem:reverse_direction} also hold, when $C$ is \emph{a set} of causally exogenous state variables.
Taken together, Theorems \ref{theorem:forward_direction} and \ref{theorem:reverse_direction} imply the next result.
\begin{corollary}\label{theorem:equivalence}
The set of causally exogenous state variables is identical to the set of action-disconnected variables.
\end{corollary}
\begin{proof}
Theorem \ref{theorem:forward_direction} establishes the forward direction, while Theorem  \ref{theorem:reverse_direction} establishes the reverse direction.
\end{proof}

The previous result allows us to prove two basic properties of causally exogenous state variables.
\begin{theorem}
Consider any MDP with causally exogenous state variables.
\begin{enumerate}
\item Assume that the two sets of state variables $X_1=S[\mathcal{I}_1]$ and $X_2=S[\mathcal{I}_2]$ are causally exogenous. Then the union set $X=S[\mathcal{I}_1\cup\mathcal{I}_2]$ is also causally exogenous.
\item Assume $X=S[\mathcal{I}]$ is a set of causally exogenous state variables. Any subset $\tilde{X}\subseteq X$ is then also causally exogenous.
\end{enumerate}
\end{theorem}
\begin{proof}
We trivially have from Corollary \ref{theorem:equivalence}:
\begin{enumerate}
\item Any state variable in $X_1$ or $X_2$ must be action-disconnected, so the union $X_1\cup X_2$ can only contain action-disconnected state variables.
\item Any state variable in $X$ must be action-disconnected, hence state variables in $\tilde{X}\subseteq X$ must be action-disconnected.
\end{enumerate}
\end{proof}

Finally, we remark that defining exogeneity in causal terms as in Definition \ref{def:exogenous} is the most natural choice. To see why, consider the MDP in Figure \ref{fig:causal-counterexample}. For this MDP, $S_1$ and $S_2$ are exogenous, since their evolution does not depend on the action. 
Indeed, both $S_1$ and $S_2$ are exogenous by Corollary \ref{theorem:equivalence}, since it is easy to verify that they are both action-disconnected in the unrolled diagram. In particular, $S_1$ will satisfy the causal definition $P(S'_1\mid S_1, \doop(A))=P(S'_1\mid S_1)$. On the other hand, it may not be true that $P(S'_1\mid S_1, A)$ equals $P(S'_1\mid S_1)$. This is because of the trail $A\gets S_2\to S'_2\to S'_1$, which could make $S'_1$ conditionally dependent on $A$ given $S_1$ even though $S'_1$ is not causally dependent on $A$. 

\subsection{Characterization of Causally-Exogenous Causal Graphs with the Complete Exogenous Set}
\label{app:complete-exo-set}
Theorem \ref{theorem:ecmi} establishes conditions under which the discovered set $X$ of a full or diachronic exo/endo decomposition is causally exogenous. In particular, under these conditions, the maximal exo set will contain causally exogenous state variables only. A natural question then arises: does the maximal exo set contain \emph{all} causally exogenous state variables? Notice that the union property (e.g., in Corollary \ref{cor:decomposition-uniqueness} or Lemma \ref{lemma:linear-union}) only allows us to take the union of exo/endo decompositions, not the union of arbitrary sets of exogenous state variables. So, it is not immediately obvious whether the maximal exo set will contain all exo state variables.

To answer this question, we first introduce the concept of the complete exogenous set, which we then use to characterize the structure of all possible causal graphs with casually exogenous state variables.
\begin{definition}[Complete Exogenous Set]
The \emph{complete exogenous set} $X_f$ of an MDP is defined as the set of \emph{all} causally exogenous state variables. Its complement $E_f=X_f^c$ is called the \emph{complete endogenous set}.
\end{definition}
Obviously, $E_f$ can only contain state variables that are not causally exogenous. Our next result is central and characterizes all possible causal graphs with causally exogenous state variables.

\begin{theorem}\label{theorem:path-based}
Consider any stationary MDP with causally exogenous state variables. Its underlying causal graph can only have two possible forms: (i) the full exo DBN of Figure \ref{fig:full-setting} with $X=X_f$, or (ii) a special class of DAGs where all directed paths in the unrolled DBN from $A$ to state variables in $E_f$ terminate at a timestep equal to the horizon $H$.
\end{theorem}
\begin{figure}[t!]
     \centering
     \includegraphics[scale=0.8]{path-property-cropped}
     \caption{State transition diagram for an MDP with all links (black and dashed red) present. 
     Links to the action node $A$ from $E$ and $X$ are missing for fully randomized policies.}
     \label{fig:path-setting}
\end{figure}
\begin{proof}
Consider the complete exo set $X_f$. 
By Corollary \ref{theorem:equivalence}, we know that $X_f$ must be action-disconnected. Furthermore, state variables in $E_f$ must not be action-disconnected, or else they would be causally exogenous and thus belong to $X_f$; the latter is impossible since $X_f$ by definition contains \emph{all} causally exogenous state variables.
As a result, for each state variable $E_i$ in $E_f$, there must be at least one $E_{t,i}$ with $t\in\{1,\dots,H\}$ so that there is a directed path from some $A_{\tau}, \tau\in\{0,t-1\}$ to $E_{t,i}$.
Let's now consider the state partition $(E_f,X_f)$ in causal graph $\mathcal{G}$. The unrestricted state transition diagram will contain all links (black and dashed red) in Figure \ref{fig:path-setting}. In order to show that $(E_f,X_f)$ matches the causal full exo DBN of Figure \ref{fig:full-setting}, we must show that it must be missing all the red dashed links. We separately consider the 3 links:
\begin{enumerate}
\item The link $A\to X'$ must not be present because it would trivially give a directed path from $A_t$ to $X_{t+1}$, which violates the exogeneity of $X_j$ according to Corollary \ref{theorem:equivalence}.
\item The link $E'\to X'$ must also not be present. Indeed, assume there is some link $E'_i\to X'_j$. By the observation above, there is a directed path from some $A_{\tau}$ to $E_{t,i}$. But then we can extend this path to get a directed path from $A_{\tau}$ to $X_{t,j}$ using the link $E'_i\to X'_j$, which violates the exogeneity of $X_j$ according to Corollary \ref{theorem:equivalence}.
\item We finally discuss the link $E\to X'$. Assume there is such a link $E_i\to X'_j$. Given $E_i$ is not exogenous, there is a directed path from some $A_{k}$ to $E_{t,i}$. We can then extend that path to $X_{t+1,j}$ to get a directed path from $A_{k}$ to $X_{t+1,j}$ using the link $E_i\to X'_j$, which violates the exogeneity of $X_j$ according to Corollary \ref{theorem:equivalence}.
\end{enumerate}
There is only one special edge case we need to consider for point (3). If there is a single directed path from  $A$ to $E_i$ that terminates at $E_{H,i}$, then it is not possible to extend that path to $X_{H+1,j}$, since we cannot go beyond time horizon $H$.
\end{proof} 
\begin{figure}[t!]
     \centering
     \begin{subfigure}[t]{0.40\textwidth}
         \includegraphics[scale=0.8]{edge_case-cropped}
         \caption{MDP.}
         \label{fig:edge_case_MDP}
     \end{subfigure}
     \hspace{0.0\textwidth}
     \begin{subfigure}[t]{0.40\textwidth}
         \includegraphics[scale=0.8]{edge_case_unrolled-cropped}
         \caption{Unrolling the MDP for $H=2$.}
         \label{fig:edge_case_unrolled}
     \end{subfigure}
     \caption{An edge case for Theorem \ref{theorem:path-based}, where the MDP does not accept any full exo/endo decomposition, even for the maximal exo state space.}
    \label{fig:edge_case}
\end{figure}
Figure \ref{fig:edge_case} depicts a graph where case (ii) of Theorem \ref{theorem:path-based} applies. Consider the MDP in Figure \ref{fig:edge_case_MDP} with 3 state variables and a horizon of $H=2$. We depict the unrolling process over $H$ in Figure \ref{fig:edge_case_unrolled}. Directed paths starting from action nodes are shown in blue color. Notice that $S_1$ is the only action-disconnected state variable, and according to Corollary \ref{theorem:equivalence}, it will be the only causally exogenous state variable. Hence, the maximal exo set will simply be $X_f=\{S_1\}$ with corresponding $E_f=\{S_2,S_3\}$. Notice, however, the link from $S_2$ to $S'_1$; since this link originates from $E_f$ to $X'_f$, the corresponding state partition $(E_f,X_f)$ does not abide by the full causally exogenous DBN of Figure \ref{fig:full-setting}.

Theorem \ref{theorem:path-based} has important implications in practice. Assume any admissible MDP with a fully randomized exploration policy. Under faithfulness,
any MDP with exogenous state variables will generally accept a full exo/endo (statistical) decomposition $S=(E,X)$, namely, the one where $X$ is the full exo set $X_f$ containing all exogenous state variables. Hence, the full exo DBN is not just a convenient tool based on the 2-timestep dynamics, but a powerful abstraction that can usually capture the maximal exo set $X_m$ under certain conditions. However, edge cases exist according to Theorem \ref{theorem:path-based}: for instance, in the MDP of Figure \ref{fig:edge_case} there is no valid full exo/endo decomposition even though the maximal exo set is nonempty, under the standard conditions. 
This answers the question we asked at the beginning of Appendix \ref{app:complete-exo-set}.

\section{The Simplified Objective}
\label{app:simplified_setting}

Our experiments showed that the Simplified-\grds{} and \sras{} algorithms often give excellent performance. Both of these algorithms employ the simplified information theoretic objective $I(X'; A\mid X)=0$ in place of either the diachronic objective $I(X'; A, E, E'\mid X)=0$ or the full objective $I(X'; A, E\mid X)=0$. In this appendix, we analyze the properties of the simplified objective.

\begin{figure}[t!]
    \centering
    \includegraphics[scale=0.8]{interesting_setting-cropped}
         \caption{MDP with 3 endogenous state variables $S_1,S_2,S_3$.}
         \label{fig:action-setting-a}
\end{figure}

It is easy to find examples where the simplified objective fails. Consider the DBN in Figure \ref{fig:action-setting-a} of an MDP with the state variables $S_1, S_2$ and $S_3$. The policy determining the action depends only on $S_2$ and $S_3$ but not on $S_1$. 
All state variables can be endogenous ($S_1$ and $S_2$ directly, and $S_3$ indirectly through its dependence on $S_1$).
However, the MDP satisfies the condition $I(S'_3; A\mid S_3)=0$. Hence, we cannot use the simplified objective to safely conclude whether a state variable is exogenous or not when analyzing the 2-timestep DBN. 


\begin{figure}[b!]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \includegraphics[scale=0.8]{interesting_setting_4-cropped}
         \caption{$I(S'_3 ; A \mid S_3)=0$.}
         \label{fig:interesting-setting-4}
     \end{subfigure}
     \hspace{0.15\textwidth}
     \begin{subfigure}[t]{0.3\textwidth}
     	 \centering
         \includegraphics[scale=0.8]{interesting_setting_2-cropped}
         \caption{$I(S'_3 ; A \mid S_3)\neq 0$.}
         \label{fig:interesting-setting-2}
     \end{subfigure}
     \caption{State transition diagrams for two MDPs. In both MDPs, $S_2$ and $S_3$ are exogenous.}
     \label{fig:interesting-settings-2-3}
\end{figure}

As we would expect, the simplified setting does not necessarily lead to valid full or diachronic exo/endo decompositions.
In this direction, first note that even when the simplified objective returns a set $X$ of variables that are causally exogenous, $X$ and its complement $E=X^c$ may not correspond to a valid exo/endo decomposition $(E,X)$. Consider for example $X=\{S_3\}$ in Figure \ref{fig:interesting-setting-4}, which satisfies $I(X' ; A \mid X)=0$ but $X$ and the corresponding $E=\{S_1,S_2\}$ are not a valid full exo/endo decomposition because $S'_3$ depends on $S'_2$. This is not the case when using the full or diachronic objectives.
Furthermore, it is possible for a state variable $X$ to be exogenous, even though it fails to satisfy $I(X' ; A \mid X)=0$. Consider $X=\{S_3\}$ in Figure \ref{fig:interesting-setting-2}.  It may not satisfy $I(S'_3 ; A \mid S_3)=0$ due to the trail $A\gets S_2\to S'_3$ from $A$ to $S'_3$. 

Despite the fact that the simplified objective $I(X'; A\mid X)=0$ is not sufficient to identify exogenous variables, it can be used as a simpler proxy for the full objective $I(X';[E,A]\mid X)=0$. Any set of state variables $X$ that satisfies the full objective must necessarily satisfy the simplified objective, since the latter has fewer constraints than the former. Of course, the simplified objective may return an over-estimate of the set of exogenous state variables, possibly contaminated with endogenous components. 
For this reason, it is always important to check the decomposition $(E=X^c,X)$ that satisfies the simplified objective against the full (or diachronic) objective.

The shortcomings of the simplified objective result from attempting to apply it within the framework of the 2-timestep DBN. If we unroll the DBN and consider the $H$-horizon MDP, then the simplified objective corresponds to directly checking that all variables in $X$ are disconnected from $A$ and therefore $X$ is causally exogenous. The next theorem resembles Theorem \ref{theorem:ecmi} for full or diachronic factorizations.

\begin{theorem}\label{theorem:simplified-objective}
Assume that an $H$-horizon MDP is admissible, data $D$ has been collected by executing a fully-randomized policy for $n$ steps, and the faithfulness assumption holds for the causal graph $\mathcal{G}$ describing the MDP. If 
\begin{equation}\label{eq:simplified-constraints}
\lim_{n\to\infty}\hat{I}(X_{\tau} ; A_t \mid X_{t})=0,\forall t\leq H-1,t+1\leq\tau\leq H,
\end{equation}
then $X$ is causally exogenous. 
\end{theorem}
\begin{proof} (sketch)
Equation \eqref{eq:simplified-constraints} is equivalent to the statement that $P(X_{\tau} \mid X_t, A_t)=P(X_{\tau} \mid X_t)$ for all $t$ and $\tau$. Under the faithfulness assumption, we can infer that there is no directed path from any $A_t$ to any $X_\tau$. Hence, $X$ is action-disconnected in the causal graph $\mathcal{G}$, and it follows from Theorem~\ref{theorem:forward_direction} that $X$ is causally exogenous.
\end{proof}

The reverse direction is trivial. If $X$ is causally exogenous and the faithfulness assumption holds, then the simplified condition must hold between each $A_t$ and every $X_\tau$. It follows that in the unrolled MDP, the simplified objective will detect exactly the exogenous variables (under the assumptions of the theorem).

Theorem \ref{theorem:simplified-objective} has the drawback that it must consider all long-range dependencies, and this requires estimating $P(X_t,X_\tau,A_t)$ for all $t \leq H-1$ and all $\tau$ such that $t+1 \leq \tau \leq H$. This makes much less effective use of the data set $D$. In practical applications, it might be fruitful to explore approximate variants of this theorem that make use of a small number $F$ of forward steps. In this case, we could enforce just $F$ constraints per timestep, i.e., $I(X_{t+k} ; A_t \mid X_{t})=0,\forall 1\leq k\leq F$, where $F$ is a small number.


\section{Dynamic Program for the Covariance Condition}
\label{app:covariance}

\begin{theorem}\label{theorem:variance}
The variance of the $H$-step return $B^\pi(s;H)$ can be computed as the solution to the following dynamic program:
\begin{align}
    V^\pi(s;0) &:= 0; \quad \V[B^\pi(s;0)] := 0 \label{eqn:var-base}\\
    V^\pi(s;h) &:= m(s,\pi(s)) + \gamma \E_{s'\sim P(s'\mid s,\pi(s))}[V^\pi(s';h-1)] \label{eqn:var-value}\\
    \V[B^\pi(s;h)] &:= \sigma^2(s,\pi(s)) - V^\pi(s;h)^2\; + \label{eqn:var-var}\\
        & \quad \quad \gamma^2\E_{s'\sim P(s'\mid s,\pi(s))}[\V[B^\pi(s';h-1)]] \;+ \nonumber\\
        & \quad \quad \E_{s'\sim P(s'\mid s,\pi(s)}[m(s,\pi(s))+\gamma V^\pi(s';h-1)]^2\nonumber
\end{align}
Equations \eqref{eqn:var-value} and \eqref{eqn:var-var} apply for all $h>0$.
\end{theorem}
\begin{proof}
Sobel (\citeyear{Sobel1982}) analyzed the variance of infinite horizon discounted MDPs with deterministic rewards. We modify his proof to handle a fixed horizon and stochastic rewards. We proceed by induction on $h$. To simplify notation, we omit the dependence on $\pi$.\\
{\bf Base Case:} $H=0$. This is established by Equations \eqref{eqn:var-base}. \\
{\bf Inductive Step:} $H=h$. Write 
\[B(s;h)=R(s)+\gamma B(s';h-1),\]
where the rhs  involves the three random variables $R(s)$, $S'$, and $B(s';h-1)$ (with samples $s'\sim S')$. To obtain Equation \eqref{eqn:var-value}, compute the expected value
\begin{equation*}
    V(s;h) = \E_{B(s;h)}[B(s;h)] \;= \E_{s',R(s),B(s';h-1)}[R(s)+\gamma B(s';h-1)],
\end{equation*}
and take each expectation in turn. To obtain the formula for the variance, write the standard formula for the variance:
\begin{equation*}
    \V[B(s;h)] = \E_{B(s;h)}[B(s;h)^2] - \E_{B(s;h)}[B(s;h)]^2.
\end{equation*}
Substitute $R(s)+\gamma B(s';h-1)$ in the first term and simplify the second term to obtain
\begin{equation*}
    \E_{s',R(s),B(s';h-1)}\left[\{R(s)+\gamma B(s';h-1)\}^2\right] - V(s;h)^2.
\end{equation*}
Expand the square in the first term:
\begin{equation*}
    \E_{s',R(s),B(s';h-1)}[R(s)^2 + 2R(s)\gamma B(s';h-1) + \gamma^2B(s';h-1)^2] - V(s;h)^2.
\end{equation*}
Distribute the two innermost expectations over the sum:
\begin{equation*}
\E_{s'}[\E_{R(s)}[R(s)^2] + 2m(s)\gamma V(s';h-1) + \gamma^2\E_{B(s';h-1)}[B(s';h-1)^2]] - V(s;h)^2.
\end{equation*}
Apply the definition of variance in reverse to terms 1 and 3 in brackets:
\begin{equation*}
\E_{s'}[\V[R(s)] + m(s)^2 + 2m(s)\gamma V(s';h-1) + \gamma^2\V[B(s;h-1)] + \gamma^2V(s';h-1)^2] - V(s;h)^2
\end{equation*}
Factor the quadratic involving terms 1, 3, and 4:
\begin{equation*}
\E_{s'}[\sigma^2(s) + [m(s)+\gamma V(s';h-1)]^2 + \gamma^2\V[B(s';h-1)]] - V(s;h)^2.
\end{equation*}
Finally, distribute the expectation with respect to $s'$ to obtain Equation~\eqref{eqn:var-var}. 
\end{proof}

\begin{theorem}\label{theorem:covariance}
The covariance between the exogenous $H$-step return
$B_x^\pi(x;H)$ and the endogenous $H$-step return $B_e^\pi(e,x;H)$ can be computed via the following dynamic program:
\begin{align}
    \C&[B_x^\pi(x;0),B_e^\pi(e,x;0)] :=0\label{eqn:cov-base}\\
    \C&[B_x^\pi(x;h),B_e^\pi(e,x;h)] :=\nonumber\\
    & \gamma^2 \E_{e'\sim P(e'\mid e,x),x'\sim P(x'\mid x)} \big\{\C[B_x^\pi(x';h-1),B_e^\pi(e',x';h-1)] \;+\label{eqn:cov-cov}\\
       & \quad \quad [m_x(x)+\gamma V_x(x';h-1)]\;\times[m_e(e,x,\pi(e,x))+\gamma V_e^\pi(e',x';h-1)]\big\} \nonumber\\
       & - V_x^\pi(x;h)V_e^\pi(e,x;h).\nonumber
\end{align}
Equation \eqref{eqn:cov-cov} applies for all $h>0$.
\end{theorem}
\begin{proof}
By induction. The base case is established by Equation
\eqref{eqn:cov-base}. For the inductive case, we begin with the formula
for non-centered covariance: 
\begin{align*}
    \C[B_x(x;h),B_e(e,x;h)]=\E_{B_x(x;h),B_e(e,x;h)}[B_x(x;h)B_e(e,x;h)] - V_x(x;h)V_e(e,x;h).
\end{align*}
Replace $B_x(x;h)$ by $R_x(x)+\gamma B_x(x';h-1)$ and $B_e(e,x;h)$ by $R_e(e,x)+\gamma B_e(e',x';h-1)$ and replace the expectations wrt $B_x(x;h)$ and $B_e(e,x;h)$ by expectations wrt the six variables $\{e'\sim E',x'\sim X',R_x(x),R_e(e,x),B_x(x';h-1),B_e(e',x';h-1)\}$. We will use the following abbreviations for these variables: $s'=\{e',x'\}$, $r=\{R_e(x),R_e(e,x)\}$, and $B=\{B_x(x';h-1),B_e(e',x';h-1)\}$.
\begin{align*}
    \C[B_x(x;h),B_e(e,x;h)]&= -V_x(x;h)V_e(e,x;h)\;+\\
    & \quad \quad \E_{s',r,B}[(R_x(x)+\gamma B_x(x';h-1)) (R_e(e,x)+\gamma B_e(e',x';h-1))].
\end{align*}
We will focus on the expectation term. Multiply out the two terms and distribute the expectations wrt $r$ and $B$:
\begin{align*}
    \E_{s'}[&m_x(x)m_e(e,x)+\gamma m_x(x)V_e(e',x';h-1) +\gamma m_e(e,x)V_x(x';h-1)\;+\\
    & \gamma^2 \E_B[B_x(x';h-1)B_e(e',x';h-1)]].
\end{align*}
Apply the non-centered covariance formula ``in reverse'' to term 4.
\begin{align*}
    \E_{s'}[&m_x(x)m_e(e,x)+\gamma m_x(x)V_e(e',x';h-1) + \gamma m_e(e,x)V_x(x';h-1) \;+\\
    &\gamma^2V_x(x';h-1)V_e(e',x';h-1) + \gamma^2 \C[B_x(x';h-1)B_e(e',x';h-1)]].
\end{align*}
Distribute expectation with respect to $s'$:
\begin{align*}
    m_x(x)m_e(e,x)+\gamma m_x(x)\E_{s'}[V_e(e',x';h-1)] + \gamma m_e(e,x)\E_{s'}[V_x(x';h-1)] \;+\\
    \gamma^2\E_{s'}[V_x(x';h-1)]\E_{s'}[V_e(e',x';h-1)] + \gamma^2 \C[B_x(x';h-1)B_e(e',x';h-1)]]
\end{align*}
Obtain Equation \eqref{eqn:cov-cov} by factoring the first four terms, writing the expectations explicitly, and including the $-V_x(x;h)V_e(e,x;h)$ term.
\end{proof}

To gain some intuition for this theorem, examine the three terms on the right-hand side of Equation \eqref{eqn:cov-cov}. The first is the ``recursive'' covariance for $h-1$. The second is the one-step non-centered covariance, which is the expected value of the product of the backed-up values for $V_e^\pi$ and $V_x^\pi$. The third term is the product of $V_e^\pi$ and $V_x^\pi$ for the current state, which re-centers the covariance. 

\begin{comment}
\section{Practical RL with an Evolving Policy}
\label{app:practical-optimization}

In this Section, we delve into various aspects related to the 2-Phase RL framework that we proposed in Algorithm \ref{alg:framework}.

Initially, we assume that data is collected via a policy that is initialized randomly. This helps to achieve better coverage of the state space. Another major reason why data collection must use a random policy is that according to Theorem \ref{theorem:full-randomized-policy}, a fully randomized policy guarantees that the full or diachronic factorizations discover truly exogenous state variables.
For this reason, to solve the formulation for discovering the exo/endo state decomposition and fitting the exo reward model, we will be assuming access to an adequate number of samples generated by a randomly initialized policy network.


Next, we explain some design choices behind our 2-Phase framework. 
Assuming the underlying MDP is linear, which is the focus in our experimental section, it is sufficient to perform the state decomposition only a single time, after we have collected a sufficient number of samples. We can then be confident that the discovered decomposition will be valid in the entire state space.

On the other hand, our work places no restriction on the exogenous reward function, which may depend in arbitrary ways on the exogenous sate.
Even if we can compute the exo/endo state decomposition accurately, we argue that the exo reward function must be updated regularly, as the learned RL policy evolves, to account for the new data collected by the agent. This is why we update the exo reward model in Phase 2 every $M$ steps.
Indeed, the model we learn from the initial optimization in Phase 1 is relevant only for the state subspaces visited so far by the agent but may not generalize well to unseen subspaces. Another way to see this that our various formulations (including the exo reward regression) critically depend on the underlying policy, since the policy affects the collected data and their distribution. Concretely, the policy is not fixed but evolves throughout training, and this directly impacts the distribution of the collected transitions.

To make the above more precise, assume we have correctly identified the exo/endo state decomposition. Let $\pi$ be a fixed policy. Then the optimal exo reward regression is:
\begin{equation}
\min_{\theta_R}\mathbb{E}_{(s,a,r,s')\sim \pi}[(\hat{m}_{exo}(x;\theta_R)-r)^2].
\end{equation}
In practice, we can collect data $D_{\pi}$ from the fixed policy $\pi$ and solve optimization problem:
\begin{equation}
\min_{\theta_R}\sum\limits_{(s,a,r,s')\in D_{\pi}}(\hat{m}_{exo}(x;\theta_R)-r)^2.
\end{equation}
The above formulations take into account the policy that generates the data, including information about the state visitation frequencies or action probabilities at different states.

In the RL setting, we do not have a fixed policy throughout training but instead an evolving policy that we update as training progresses. In that case the evolving policy $\pi_{RL}$ does not describe a single policy, but instead a sequence of policies:
$$
\pi_{RL}=\{\pi_0,\dots,\pi_t\},
$$
where $\pi_0$ is the initial policy and $\pi_i$ the policy at step $i$. Let $D_{RL}$ be the data collected from the evolving policy (up to step $t$). It is natural to  use the regression formulation:
\begin{equation}\label{eq:practical-reward-regression}
\min_{\theta_R}\sum\limits_{(s,a,r,s')\in D_{RL}}(\hat{m}_{exo}(x;\theta_R)-r)^2.
\end{equation}
Equation \eqref{eq:practical-reward-regression} can work well in the evolving policy setting, because it captures the new knowledge acquired by the agent as training progresses. Next, we discuss two variants of formulation \eqref{eq:practical-reward-regression}: the unbiased and robust estimators.

\subsection{The unbiased regression estimator}
At any timestep $t$ during training, we would ideally be interested in the reward regression under policy $\pi_t$, since the data will be generated by $\pi_t$:
\begin{equation}
\min_{\theta_R}\mathbb{E}_{(s,a,r,s')\sim \pi_t}[(\hat{m}_{exo}(x;\theta_R)-r)^2].
\end{equation}
We can thus modify \eqref{eq:practical-reward-regression} by using an unbiased sampling estimator as follows:
\begin{equation}\label{eq:unbiased-reward-regression}
\min_{\theta_R}\sum\limits_{(s_i,a_i,r_i,s'_i)\in \mathcal{D}_{RL}}\frac{\rho_{\pi_t}(s_i)\cdot\pi_t(a_i\mid s_i)}{\rho_{\pi_i}(s_i)\cdot\pi_i(a_i\mid s_i)}\cdot(\hat{m}_{exo}(x_i;\theta_R)-r_i)^2,
\end{equation}
where $\rho_{\pi}(s)$\footnote{In practice, we also run each policy $\pi_i$ for a finite number of steps, so the state distribution follows these visitation frequencies only to some approximation.} denotes the visitation frequency of state $s$ under policy $\pi$.
Formulation \eqref{eq:unbiased-reward-regression} solves a weighted regression problem, where each observation $(s_i,a_i,r_a,s'_i)$ by the ratio $\rho_{\pi_i}(s_i)\cdot\pi_i(a_i\mid s_i)$ between the policy $\pi_t$ at the current timestep and the policy $\pi_i$ that generated sample $s_i$ at timestep $i$.

Unfortunately, \eqref{eq:unbiased-reward-regression} is hard to directly optimize since the state visitation frequencies are difficult to compute; furthermore, it suffers from high variance because it involves a ratio of probabilities. In this work, we opt for the efficient option of \textit{online} regression, where the new transitions update the exo reward model in an online manner as they are generated. Such a scheme will naturally give higher weight to samples generated by more recent policies compared to samples generated in the distant past. An alternative to online regression is the more expensive \textit{repeated} regression at regular intervals by solving \eqref{eq:practical-reward-regression} with all collected observations so far. We discuss this point in detail in Section \ref{sec:sensitivity-analysis}.

\subsection{The robust regression estimator}
An alternative to an unbiased estimator with weighted regression would be to opt for a robust estimator. To motivate this, note that even if we have a good regression formulation for policy $\pi_t$ at timestep $t$, we need to ensure that the exo reward function will also perform well for the new policies after $\pi_t$, since the agent continuously updates the policy as it collects more samples. We can achieve this by using a robust estimator, as explained below.

Let $\mathcal{Z}$ be a distance function between probability distributions (on a given metric space). This could be for example a proper distance function such as the Wasserstein distance, or even the KL-divergence \citep{Villani2009}. The robust regression formulation takes the form:
\begin{equation}\label{eq:robust-reward-regression}
\min_{\theta_R}\max_{\pi:\mathcal{Z}(\pi,\pi_t)\leq \tau}\Big\{\mathbb{E}_{(s,a,r,s')\sim \pi}[(\hat{m}_{exo}(x;\theta_R)-r)^2]\Big\}.
\end{equation}
In \eqref{eq:robust-reward-regression}, the hyperparameter $\tau$ corresponds to a threshold that we determine manually. Intuitively, the robust formulation ensures that the computed exo reward is valid not only for the current policy $\pi_t$, but also for any policy $\pi$ that is close enough (as determined by $\mathcal{Z}$ and $\tau$) to policy $\pi_t$.

In practice, we can circumvent the challenge of optimizing $\eqref{eq:robust-reward-regression}$ by using trust region or proximal policy algorithms, e.g., TRPO \citep{pmlr-v37-schulman15} or PPO \citep{DBLP:journals/corr/SchulmanWDRK17}. These RL algorithms have the desired policy updates are small: the updated policy is guaranteed to be within a small KL-divergence distance from the current policy. As a result, the exo reward estimate from $\pi_t$ can perform well under the next updated policies. For this reason, in this work we will opt for PPO.

\end{comment}

\bibliographystyle{plainnat}
\bibliography{bibliography}

\end{document}
