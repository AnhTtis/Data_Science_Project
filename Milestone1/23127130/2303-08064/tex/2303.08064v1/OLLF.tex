\documentclass[acmtog]{acmart}
\pdfoutput=1
\usepackage{booktabs} % For formal tables
\usepackage{comment}
\usepackage{bm}


% TOG prefers author-name bib system with square brackets
\citestyle{acmauthoryear}
%\setcitestyle{nosort,square} % nosort to allow for manual chronological ordering

\usepackage[ruled]{algorithm2e} % For algorithms
\usepackage{subfig}
\usepackage{footnote}

\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}

\usepackage{xcolor}
% Metadata Information
\acmJournal{TOG}
%\acmVolume{38}
%\acmNumber{4}
%\acmArticle{39}
%\acmYear{2019}
%\acmMonth{7}

\newcommand{\secref}[1]{\S~\ref{sec:#1}}
\newcommand{\tabref}[1]{Tab.~\ref{tab:#1}}
\newcommand{\figref}[1]{Fig.~\ref{fig:#1}}
\renewcommand{\eqref}[1]{Eq.~(\ref{eq:#1})}
\newcommand{\algref}[1]{Algo.~(\ref{alg:#1})}
\newcommand{\alglref}[1]{Line~(\ref{lin:#1})}
\newcommand{\appref}[1]{Appendix~\ref{app:#1}}
\newcommand{\etal}{\emph{et al.~}}
\newcommand{\eg}{\emph{e.g., }}
\newcommand{\ie}{\emph{i.e., }}
\newcommand{\etc}{\emph{etc.}}
\newcommand{\cf}{\emph{cf.}}
\newcommand{\versus}{\emph{vs.}}

\newcommand{\mbA}{\ensuremath{\mathbf{A}}}
\newcommand{\mbJ}{\ensuremath{\mathbf{J}}}
\newcommand{\mbx}{\ensuremath{\mathbf{x}}}
\newcommand{\mbn}{\ensuremath{\mathbf{n}}}
\newcommand{\mbomega}{\ensuremath{\mathbf{\omega}}}
\newcommand{\mby}{\ensuremath{\mathbf{y}}}
\newcommand{\mbz}{\ensuremath{\mathbf{z}}}
\newcommand{\mbv}{\ensuremath{\mathbf{v}}}
\newcommand{\mbO}{\ensuremath{\mathbf{O}}}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
%\acmDOI{0000001.0000001_2}

% Paper history
%\received{February 2007}
%\received{March 2009}
%\received[final version]{June 2009}
%\received[accepted]{July 2009}

% Document starts
\begin{document}
	% Title portion
	\title{Online Neural Path Guiding with Normalized Anisotropic Spherical Gaussians}
	
	% DO NOT ENTER AUTHOR INFORMATION FOR ANONYMOUS TECHNICAL PAPER SUBMISSIONS TO SIGGRAPH 2019!
	\author{Jiawei Huang}
	%\orcid{1234-5678-9012-3456}
	\affiliation{%
		\institution{Chuzhou University and Void Dimensions}
		%\streetaddress{104 Jamestown Rd}
		%\city{Williamsburg}
		%\state{VA}
		%\postcode{23185}
		\country{China}}
	%\email{gang_zhou@wm.edu}
	\author{Akito Iizuka}
	\affiliation{%
		\institution{Tohoku University}
		%\city{Rocquencourt}
		\country{Japan}
	}
	%\email{beranger@inria.fr}
	\author{Hajime Tanaka}
	\affiliation{%
		\institution{Tohoku University}
		%\streetaddress{Rono-Hills}
		%\city{Doimukh}
		%\state{Arunachal Pradesh}
		\country{Japan}}
	%\email{aprna_patel@rguhs.ac.in}
	\author{Taku Komura}
	\affiliation{%
		\institution{The University of Hong Kong}
		%\city{Rocquencourt}
		\country{Hong Kong, China}
	}
	\author{Yoshifumi Kitamura}
	\affiliation{%
		\institution{Tohoku University}
		%\streetaddress{30 Shuangqing Rd}
		%\city{Haidian Qu}
		%\state{Beijing Shi}
		\country{Japan}
	}
	%\email{chan0345@tsinghua.edu.cn}
	%\author{Ting Yan}
	%\affiliation{%
	%  \institution{Eaton Innovation Center}
	%  \city{Prague}
	%  \country{Czech Republic}}
	%\email{yanting02@gmail.com}
	%\author{Tian He}
	%\affiliation{%
	%  \institution{University of Virginia}
	%  \department{School of Engineering}
	%  \city{Charlottesville}
	%  \state{VA}
	%  \postcode{22903}
	%  \country{USA}
	%}
	%\affiliation{%
	%  \institution{University of Minnesota}
	%  \country{USA}}
	%\email{tinghe@uva.edu}
	%\author{Chengdu Huang}
	%\author{John A. Stankovic}
	%\author{Tarek F. Abdelzaher}
	%\affiliation{%
	%  \institution{University of Virginia}
	%  \department{School of Engineering}
	%  \city{Charlottesville}
	%  \state{VA}
	%  \postcode{22903}
	%  \country{USA}
	%}
	
	
	\begin{abstract}
		%		The variance reduction speed of physically-based rendering is heavily affected by importance sampling techniques. In this paper we propose a new lightweight framework to learn a closed-form spatial-varying density model with a single small neural network using stochastic samples online. We implement our framework in our in-house GPU production path tracer, the low overhead makes it efficient for product rendering. Our framework learns the distribution in a progressive manner and does not need warm-up phase. The result shows our framework achieves better guiding efficiency compared to state-of-the-art online learned path guiding methods.
		
		The variance reduction efficiency of physically-based rendering is heavily affected by the adopted importance sampling technique. In this paper we propose a novel online framework to learn the spatial-varying density model with a single small neural network using stochastic ray samples. 
		To achieve this task, we propose a novel closed-form density model called the Normalized Anisotropic Spherical Gaussian mixture, that can express complex irradiance fields with a small number of parameters.
		Our framework learns the distribution in a progressive manner and does not need any warm-up phases. 
		Due to the compact and expressive representation of our density model, 
		our framework can be implemented entirely on the GPU, allowing it to produce high quality images with limited computational resources.  
		% We implement our framework in our in-house GPU production path tracer, the low overhead makes it efficient for product rendering.
		The result shows our framework achieves better guiding efficiency compared to state-of-the-art online learning path guiding methods.
		
	\end{abstract}
	
	
	
	
	%\renewcommand\shortauthors{Zhou, G. et al}
	\begin{teaserfigure}
		%\includegraphics[width=\textwidth]{Images/teaser1}
		\begin{tabular}{cccc}
			Reference&(a) Ours&(b) \cite{PPG19}%[M\"{u}ller et al. 2019]
            &(c) Path Tracing\\
			\includegraphics[width=0.23\linewidth]{Images/teaser_2/ground_truth_marked}&
			\includegraphics[width=0.23\linewidth]{Images/teaser_2/ai_marked}&
			\includegraphics[width=0.23\linewidth]{Images/teaser_2/ppg_marked}&
			\includegraphics[width=0.23\linewidth]{Images/teaser_2/uni_marked}\\
			\includegraphics[width=0.115\linewidth]{Images/teaser_2/ground_truth_0}
			\includegraphics[width=0.115\linewidth]{Images/teaser_2/ground_truth_1}&
			\includegraphics[width=0.115\linewidth]{Images/teaser_2/ai_0}
			\includegraphics[width=0.115\linewidth]{Images/teaser_2/ai_1}&
			\includegraphics[width=0.115\linewidth]{Images/teaser_2/ppg_0}
			\includegraphics[width=0.115\linewidth]{Images/teaser_2/ppg_1}&
			\includegraphics[width=0.115\linewidth]{Images/teaser_2/uni_0}
			\includegraphics[width=0.115\linewidth]{Images/teaser_2/uni_1}\\
			\hfill MAPE:&\textbf{0.058}&0.063&0.120\\
		\end{tabular}
		\caption{\label{fig:teaser}
			Our framework allows a GPU path tracer to effectively learn irradiance distribution for every shading point with a small neural network online. We compare equal-time rendering results of our framework (a) with \cite{PPG19} (b) and path tracing with next event estimation (c). Our framework achieves efficient neural path guiding that conventional PC can afford.}
	\end{teaserfigure}
	
	\maketitle
	
	%
	% The code below should be generated by the tool at
	% http://dl.acm.org/ccs.cfm
	% Please copy and paste the code instead of the example below.
	%
	\begin{CCSXML}
		<ccs2012>
		<concept>
		<concept_id>10010520.10010553.10010562</concept_id>
		<concept_desc>Computer systems organization~Embedded systems</concept_desc>
		<concept_significance>500</concept_significance>
		</concept>
		<concept>
		<concept_id>10010520.10010575.10010755</concept_id>
		<concept_desc>Computer systems organization~Redundancy</concept_desc>
		<concept_significance>300</concept_significance>
		</concept>
		<concept>
		<concept_id>10010520.10010553.10010554</concept_id>
		<concept_desc>Computer systems organization~Robotics</concept_desc>
		<concept_significance>100</concept_significance>
		</concept>
		<concept>
		<concept_id>10003033.10003083.10003095</concept_id>
		<concept_desc>Networks~Network reliability</concept_desc>
		<concept_significance>100</concept_significance>
		</concept>
		</ccs2012>
	\end{CCSXML}
	
	\ccsdesc[500]{Computer systems organization~Embedded systems}
	\ccsdesc[300]{Computer systems organization~Redundancy}
	\ccsdesc{Computer systems organization~Robotics}
	\ccsdesc[100]{Networks~Network reliability}
	
	%
	% End generated code
	%
	\keywords{Wireless sensor networks, media access control,
		multi-channel, radio interference, time synchronization}
	
	
	\section{Introduction}
	Unbiased physically-based rendering (PBR) is achieved by launching light transport simulation to solve the rendering equation with Monte Carlo methods. Over the past decades, unidirectional path tracing has become the dominant method of PBR in the film and design industries because of its flexibility and simplicity. To reduce variance of Monte Carlo integral efficiently, many importance sampling methods have been proposed (\eg \cite{VEACH98, SA13, hart2020practical}). However, currently only some components in rendering equation (typically, the BSDF term, or direct lighting) can be importance-sampled. When indirect lighting is dominant, the sampling efficiency becomes poor.
	
	Path guiding is a promising genre of importance sampling approaches to overcome the challenge~\cite{VORBA14,MUL17}. A path guiding method usually learns a distribution model over the 3D scene that fits the rendering equation closer, either during the rendering process (online) or with precomputation (offline). 
	%Then, the path tracer sample scattering directions based on the learned distribution, so that the rendering equation can be better importance-sampled. 
	Then, the path tracer can importance-sample the scattering directions based on the learned distribution. 
	Previous approaches such as~\cite{VORBA14,MUL17}
	partition the 3D space and approximate the incident radiance of each zone using 
	statistical models (typically, histogram-based model or expectation-maximization) 
	instead of modelling the whole product of the rendering equation, which fails to represent per-shading-point distribution.
	
	%This approach greatly improves the sampling efficiency of indirect lighting, however, to learn such a distribution with close fit, many state-of-the-art methods needs to partition the 3D space and adopt statistical methods (typically, histogram-based model or expectation-maximization) for learning, and the resulting distribution is an approximation of incident radiance rather than the whole product of rendering equation since such methods fail to represent per-shading-point distribution.
	
	Recently, an emerging direction to learn the per-shading-point product distribution of rendering equation is to model such spatially varying distribution with neural networks
	\cite{MUL20,SHANDONG21,ZHU21}. 
	%The learned model can be used to importance-sample the rays during a path tracing process. 
	Ideally, an explicit model that directly learns 
	the per-shading-point product distribution is desired but this has been difficult due to (1) the lack of a representation that can reconstruct both high and low frequency features with small number of parameters and (2) a closed-form density model that provides an analytical solution for integrating the distribution for normalization.
	As a result, existing models 
	either learn an implicit model that only provides a mapping between uniform distribution and the shading-point product distribution but with expensive computation ~\cite{MUL20};  
	learn a coarse, low resolution distribution with limited accuracy~\cite{SHANDONG21}; or require  costly offline precomputation~\cite{ZHU21}. %Such models either suffer from the expensive computational costs for producing the mapping~\cite{MUL20} or blurry results~\cite{SHANDONG21}.
	
	%Recently, an emerging direction to learn the per-shading-point product distribution of rendering equation is to model such spatially varying distribution with neural network, however existing methods either learn an implicit model with deep neural network or can only give low rough representation, while the computational cost of neural network learning and forward pass is rather high. These limitations make it unaffordable to apply the methods in conventional rendering tasks or efficient production rendering. Such limitation is due to the fact that there exists no effective closed-form density model, that is expressive and lightweight. Without such model, one faces two major challenge of learning distribution well; 1) Difficulty to efficiently represent the complex spatially varying distribution, since both low frequency and high frequency distributions needs to be effectively importance-sampled; 2) Low training efficiency, since a complex model usually needs many iterations and training samples to converge. Consequently, existing methods have to either learn an implicit model \cite{MUL20}, or adopt offline-trained models with compromising closed-form model for limited accuracy\cite{SHANDONG21}, or blending with other sampling techniques (\eg \cite{ZHU21} uses photons as input to the network).
	
	
	In this paper, we propose a novel neural-network-based path guiding framework, which learns an explicit spatial-varying distribution of irradiance over the surfaces in a 3D scene. The key insight of our framework is a novel closed-form normalized density model 
	called the Normalized Anisotropic Spherical Gaussian mixture (NASG),
	that can be effectively trained with sparse training samples. Due to the simplicity and expressiveness of our density model, its  parameters can be efficiently learned online via a tiny, multiple layer perceptron (MLP)-based 
	neural network.
	
	%For practical rendering tasks, the ideal form of neural-network-based path guiding (neural guiding) will be a single neural network that learns closed-form density model, with minimum hardware requirement. In this paper, we propose such a novel neural-network-based path guiding framework, which learns a spatial-varying closed-form distribution model of irradiance for surface over a 3D scene. The key insight of our framework is a novel, expressive, closed-form normalized density model that can be effectively learned with sparse training samples. By adopting this density model, we are able to learn the spatial-varying distributions effectively via tiny neural network.
	
	Our framework is robust to handle a variety of lighting setups ranging from normal indirect-illuminated scenes to caustics from very narrow light sources. With our closed-form density model, we are able to not only guide sampling of indirect lighting more effectively, but also achieve more efficient direct lighting sampling by blending samples with next event estimation (NEE). We propose the corresponding training workflow including specialized loss function and online acquisition of training samples. 
	As our network can be easily stored and executed in parallel on a GPU, 
	by integrating our neural network with a wavefront path tracer, we are able to implement a high-performance neural-guided unidirectional path tracer on a single GPU, making neural-guiding affordable for conventional personal computers.
	Our system can produce much higher quality results compared to the state-of-the-art methods
	given the same computational resources. 
	%Our framework is robust to handle a variety of lighting setups ranging from normal indirect-illuminated scenes to caustics from very narrow light source. We show that our density model can be learned even with a tiny multiple layer perceptron (MLP). Such network can be easily stored and executed in parallel on a GPU. With our new framework, we are able to implement a neural-guided unidirectional path tracer on a single GPU, making neural-guiding affordable for conventional personal computers. With our closed-form density model, we are able to guide not only indirect lighting but also next event estimation (direct lighting) more effectively. We propose the corresponding training workflow including specialized loss function and online acquisition of training samples. By integrating neural network with wavefront path tracer, we achieve a high performance guided path tracer in GPU. Under same hardware, our framework gives better result in same-time comparison with state-of-the-art methods.
	%Our framework is robust to handle a variety of lighting setups ranging from normal indirect-illuminated scenes to caustics from very narrow light source. We show that our density model can be learned even with a tiny multiple layer perceptron (MLP). Such network can be easily stored and executed in parallel on a GPU. With our new framework, we are able to implement a neural-guided unidirectional path tracer on a single GPU, making neural-guiding affordable for conventional personal computers. With our closed-form density model, we are able to guide not only indirect lighting but also next event estimation (direct lighting) more effectively. We propose the corresponding training workflow including specialized loss function and online acquisition of training samples. By integrating neural network with wavefront path tracer, we achieve a high performance guided path tracer in GPU. Under same hardware, our framework gives better result in same-time comparison with state-of-the-art methods.
 
	The contribution of this paper can be summarized as:
	\begin{itemize}
		\item Normalized Anisotropic Spherical Gaussian mixture, a novel density model that can be used to learn spatial-varying density distribution effectively for path tracing, and
		\item a novel, lightweight, online neural path guiding framework, using only one small neural network that can be integrated with either CPU or GPU-based conventional product path tracers.
	\end{itemize}

 
	\section{Related Work}
	\subsection{Physically-based Rendering and Importance Sampling}
	The radiance of a shading point can be calculated using the Rendering equation \cite{KAJIYA86}:
	\begin{equation}
    \label{eq:rendering}
	L(\mbx) = L_e + \int_{\Omega}f_s(\mbx,\omega_i, \omega_o)L_i |\cos\theta| d\omega_i
	\end{equation}
	where $\mbx$ is the shading point, and its irradiance towards direction $\omega_o$ is the integral over the solid angles, which is a product of bidirectional scattering coefficient $f_s$, incident light $L_i$, and the geometry factor $cos\theta$.
	In 3D scenes, due to light bounce, $L_i$ forms another integral as \eqref{rendering} at the point where it originates from.  This makes the problem highly complex, and no analytical solution can be formed.  
 
% which makes the problem high dimensional and no analytic solution can be found.
 
	
	The goal of a path tracer is to solve the rendering equation via the Monte Carlo method, which draws a random direction sample $\omega_i'$, and calculates a product:
	\begin{equation}
	L(\mbx) \approx  L_e +  \frac{1}{N}\sum_{k}^{N}\frac{f_s(\mbx,\omega_{ik},\omega_o)L_{ik} |\cos\theta|}{p(\mbx,\omega_{ik})},
	\end{equation}
	where $p(\mbx,\omega_{ik})$ is the probability density function (PDF) of the samples.
	The variance reduction efficiency of path tracing is strongly affected by importance sampling techniques. Early works show that following the BSDF distribution $f_s$ can greatly improve the sampling efficiency. Next Event Estimation (NEE) can also greatly improve sampling efficiency for direct lighting, which can be considered as importance sampling of $L_i$. Veach et al.~\shortcite{VEACH98} propose Multiple Importance Sampling (MIS) which is a crucial method to combine different distributions to achieve better efficiency, and is widely used in current path tracers. Recent researches focus on applying importance sampling to more challenging tasks. Droske et al.~\shortcite{MNEE} develop a technique called manifold next event estimation (MNEE) to specifically explore the light path with refractive chains, which greatly improves the sampling efficiency of caustics with unidirectional path tracing. Zeltner et al.~\shortcite{SMS} generalize MNEE to other diffuse-specular-diffuse (SDS) rendering scenarios. Heitz et al.~\shortcite{MSMB} address the energy loss in microfacet scattering models through importance sampling. Conty Estevez and Kulla~\shortcite{ManyLights1} and Moreau et al.~\shortcite{ManyLights2} propose algorithms with BVH-based structures to importance sample light sources in many-light scenes. 
 
 There still exist complex scenes where no trivial unbiased approach is applicable.  Misso et al.~\shortcite{debiasing} propose to adopt biased estimators (\eg \cite{SPPM}) with debiasing algorithm, and West et al.~\shortcite{MMIS} propose an algorithm to leverage sampling methods with marginal PDFs. 
 %To solve the rendering equation efficiently, Jakob et al.~\shortcite{MEMLT} proposes to use Manifold Exploration for difficult specular transport, Kitaoka et al.~\shortcite{Replica} introduces replica exchange
 %many other extentions of Monte Carlo method are also investigated (\eg \cite{MEMLT, Replica}).
	
	Although many components of rendering equation can be importance sampled, a universal importance sampling technique for the whole product of the rendering equation is long awaited. Path guiding, which is an approach that we describe next, is a promising direction to address this problem.
	\subsection{Path Guiding}
	Path guiding methods aim to find sufficient approximation of the product, so that better importance sampling following the global illumination distribution (rather than local BSDF distribution) becomes possible.
Jensen~\shortcite{JENSEN95} proposes a  pioneering work of path guiding. 
Vorba et al.~\shortcite{VORBA14}
fits a Gaussian mixture to model a distribution estimated by an explicit photon tracing pass.    
M\"{u}ller et al.~\shortcite{MUL17,PPG19}
propose the ``Practical Path Guiding (PPG)'' algorithm
that models the distribution using the SD-tree representation. 
PPG is already adopted by several production renderers. 
%\cite{VORBA14, MUL17, PPG19} made a series of effort in bringing path guiding to practical production rendering applications, and the 
These techniques share the same idea of partitioning the space and progressively learning the spatially varying indirect 
lighting distribution. Recent research shows that good path guiding requires a proper blending weight between BSDF and 
product-driven distributions~\cite{MUL20}, and the learned distribution should be variance-aware since the samples are 
not in a zero-variance manner \cite{Variance-awarePG}. These findings help to achieve a more robust path guiding framework. 

Despite general path guiding focuses on importance sampling of the entire environment, recent works also guide paths 
for specific light sources. Li et al.~\shortcite{Pathcut} achieve efficient sampling for caustics by first collecting 
several important paths and then extending them with a chain of spherical Gaussians; however this method is difficult 
to be extended to learn general distributions, since it leverages the assumption that such caustics distribution is 
narrow. Zhu et al.~\shortcite{SHANDONG21} propose to use an offline-trained neural network to efficiently sample complex 
lamps. Despite the improved efficiency in sampling, it requires to offline-train a U-Net for more than 10 hours for one 
light source, which is not practical in many production scenarios. Also, the estimated distribution is only a $16\times 16$ 
2D map, which is not sufficient for representing general indirect lighting distributions: a much 
higher resolution is required for robustly guiding over the whole 3D scene (for instance, PPG's quad-tree approach can represent 
a resolution of $2^{16} \times 2^{16}$). 


Recently, learning-based online path guiding has been studied: 
 M\"{u}ller et al.~\shortcite{MUL20} utilize normalizing flow~\cite{Kobyzev_2021} to model the light distribution. However, with normalizing flow, each sample/density evaluation requires a full forward pass of multiple neural networks, which introduces heavy computation costs. 
 Moreover, the training process requires dense usage of differentiable transforms which makes the training slower than regular 
 neural networks; actually, M\"{u}ller et al.\shortcite{MUL20} use two GPUs specifically for its neural network computation along with the CPU-based path tracing implementation, and still the sampling speed is only 1/4 of PPG. Our work, in contrast, proposes to use an explicitly 
 parameterized density distribution model in closed-form that can be learned using a small MLP. The neural network is utilized to 
 generate the closed-form distribution model, rather than actual samples; therefore we are able to freely generate samples or 
 evaluate the density after a single forward pass. With careful implementation, we show that our method can reach a similar sampling 
 rate with PPG, while the result has lower variance.
	\subsection{GPU Path Tracing}
	GPU path tracing has gained significant attention in recent years due to its ability to sample substantially more pixels concurrently and its focus on performance. Many recent research achievements focus on making real-time path tracing possible (\eg \cite{RESTIR1,RESTIR2,RESTIR3,NRC,DENOISER}), while GPU-based production renderers leverage GPU to render fully converged, noise-free results faster (\eg \cite{ARNOLD,REDSHIFT,OCTANE,VRAY}). Compared to CPU architecture, GPU requires more sophisticated programming design for better concurrency to hide its large memory latency. A general idea for this is ray reordering \cite{RayReordering, RayShuffle}. \cite{WAVEFRONT} proposed a new architecture that splits full path tracing computation into stages (small kernels) and processes threads in the same stage to maximize performance. Recently \cite{luisa} proposes a more sophisticated framework for GPU path tracing which leverages run-time compilation for high performance and flexible implementation. While there are several GPU-based renderer products available, and it is proved that path guiding can improve the sampling efficiency (\eg many CPU-based rendering products such as  \cite{KARMA,RENDERMAN}, have implemented a path guiding integrator), there lacks GPU-based path guiding implementation in these products. This is because existing path guiding approaches are designed for CPUs and care little about GPU specifications, meanwhile the overhead is rather high in GPU context. Our work provides a low-cost per-sample-point path guiding option for GPU-based path tracers.

	\subsection{Density Models} \label{sec:DensityModels}
	Parameteric density models are well explored in statistics. Exponential distribution is a representative family, 
	among which Gaussian mixture is the most widely used density model. In physically-based rendering context, its variant, 
	spherical Gaussian (SG), is proposed~\cite{SG} and applied for representing the light distribution.  While the shape of 
	univariate Gaussian distribution is limited, multivariate Gaussian distributions solve this problem by introducing the 
	variance matrix. Following this idea, much effort has been put to for density models in the solid angle domain. 
	\citeN{ASG} proposes anisotropic spherical Gaussian model to achieve a larger variety of shapes, however there is no
	analytic solution for its integral, which is problematic for normalization.  
	Heitz et al.~\shortcite{LTC} propose another anisotripic model that has a closed-form expression called linear transform consines (LTC), however the integral of LTC requires computation of inverse matrix and it requires in total 12 scalars to parameterize a component. 
	Dodik et al.~\shortcite{SDMM} parameterize anisotropic distributions with fewer parameters, however it maps the tangent 
	space circle to a square while the sampling applies to the whole 2D plane so that the PDF is only an approximation. This results in a non-trivial sampling process with discarding samples, which limits the sampling efficiency 
	(especially when representing a large, low-frequency area with even numbers).   
	Bivariate Gaussian used in \cite{VORBA14} shares a similar limitation.  
 
	Another family of density models widely adopted in the rendering context is the polynomial family, 
	among which the spherical harmonics is the most commonly used model. Polynomial models have been  
	widely used in graphics~\cite{PRT, PolynomialRegression}. They can be easily mapped to unit spheres 
	to be converted into a solid angle representation. On the other hand, when the spectrum is composed of many frequencies,  
	a high dimensional vector is needed for representing the distribution. 
	Furthermore, there is no good approach to importance sample a polynomial distribution. 
%	either in general or spherical harmonics form, especially with high degree. 
%	{\color{red} what about wavelet??}
	These were the main challenges in our pilot study that made us give up adopting them for our 
	path guiding scheme.   

	Learning-based density models are drawing more attention recently~\cite{MUL20,MDMA}. 
	The model by normalizing flow successfully learns complex distributions~\cite{MUL20} 
	but suffers from the implicitness and heavy computation as mentioned earlier.  
	%Normalizing flow uses multiple neural networks to perform a sequence of transformations (called the ``flow'') to represent a mapping from a unit Gaussian to a distribution of interest.  {\color{red} need to point out the problem.}

	``Marginalizable Density Model Approximation (MDMA)'' \cite{MDMA} is proposed as a closed-form learning-based density 
	model, which is essentially a linear combination of multiple sets of 1D distributions.
	%Gilboa et al.~\cite{MDMA} 
 The authors show that closed-form normalization is an essential factor for learning density models: 
	inspired by their work, we propose such a model for our application. 
%	Our work requires low-cost closed-form density models, therefore, we study the application of \textit{normalized} 
%	exponential family and MDMA in our framework.
%	{\color{red} what is learned from MDMA ??}

	\section{Overview}
	\begin{figure}
		\centering
		\includegraphics[width=0.98\linewidth]{Images/Overview}
		\caption{The overview of our framework. Blue part is the computation of a classic path tracer. Yellow part shows how we learn the light distribution with neural network and use the estimated explicit density model to guide further sampling.  }
		\label{fig:overview}
	\end{figure}
	The goal of our work is to model the density distribution of the product over unit sphere for every unique shading point $\mbx$, so that $L(\mbx)$ can be fully importance-sampled (see \secref{DDM} for the details of our proposed density model).
	As shown in Fig. \ref{fig:overview}, on top of a standard path tracer with next event estimation (direct lighting) and scattering sampling (global illumination), we progressively train a  neural network that outputs the parameters for our density model using the rendering samples.  
	The inputs to the network during runtime include the position of the shading point, its normal vector, and the outgoing direction of the light.   
	Using the estimated density model, the ray is sampled at each reflection point to eventually compute the color at the shading point $\mbx$.     
	
	\section{Normalized Density Model}
	\label{sec:DDM}
	In this section, we describe about our proposed density model that we call  
	the Normalized Anisotropic Spherical Gaussian mixture (NASG), that allows us to progressively learn the  
	light distribution at each shading point. 
	We first describe the background by reviewing existing normalized density models that inspired our    
	model in \secref{background}. We then describe about our model in \secref{NASGM}. 
	
	%	Our framework requires to learn a closed-form distribution, therefore, the density model plays the most important role in our framework. In this section we briefly review existing normalized models that inspire our work and explain why normalization is crucial for learning distributions, introduce our normalized anisotropic spherical gaussian mixture that outperforms other similar models to achieve best result.
	
	\subsection{Background}
	\label{sec:background}
	One of the main requirements for progressively learning a distribution with neural networks is that the models must be normalizable, meaning that the distribution must be able to be integrated to 1. For this purpose, it is beneficial to have a closed-form solution for the integration of the model. 
	
	\paragraph{Marginalizable Density Model Approximation}
	\citeN{MDMA} proposes Marginable Density Model Approximation (MDMA) which can be effectively optimized via deep learning. A bivariate dimensional density distribution can be modeled as:
	\begin{equation}
	D(x,y) = \sum_{i,j}A_{ij}\phi_{1i}(x)\phi_{2j}(y)
	\end{equation}
	where $\phi_{1i}$ and $\phi_{2j}$ are 1D normalized distributions, and the $A_{ij}$ are normalized coefficients that sum to 1.
	
	%	\subsubsection{1D Piece-wise constant distribution}
	%	In \citeN{MDMA}'s paper, each 1D density model is modeled via neural network, which is not in closed-form and cannot be explicit sampled. In contrast, we propose to use another learnable closed-form function, piece-wise constant distribution, to replace it to achieve an closed-form model that can be easily sampled.
	%		%	As shown in equation \ref{eq:taylorSoftMax}, in a piece-wise constant distribution, the unit dimension is divided into K bins of equal width $W = \frac{1}{K}$, and the height of each bin $H_i$ is normalized with Taylor softmax so that sum is 1:
	%	\begin{equation}
	%	Tsm(z)_i = \frac{1+z_i+0.5z^2_i}{\sum_{j=1}^{K}1+z_i+0.5z^2_i}
	%	\label{eq:taylorSoftMax}
	%	\end{equation}
	%	Here we use Taylor softmax for computation stability consider we usually calculate them in GPU with half-precision floating point numbers and ``fast math mode'' approximations.
	%	For point x which falls into bin $i$, the pdf is simply $H_i$.
	
	In practice, the 1D distributions in MDMA can be piece-wise linear or piece-wise quadratic models, however, with limited number of components, MDMA can only learn a coarse distribution. Therefore it only gives poor results for distributions with high-frequencies. Despite the accuracy limitation, MDMA works well in learning distribution from sparse noisy samples, compared with unnormalized models (\eg polynomial model, spherical harmonics): during training, samples with high energy can lower contribution of other areas, and eventually the training converges. This feature is crucial in learning distribution for path guiding.
	\paragraph{Normalized Gaussian Mixtures}
	Normalized Gaussian mixtures can address the accuracy limitation of MDMA:
	\begin{equation}
	D(\boldsymbol{\mbx}) = \sum_{i}^{N} A_i\frac{G_i(\boldsymbol{\mbx};\theta_i)}{K_i},
	\end{equation}
	where $G_i(\boldsymbol{\mbx};\theta_i)$ are Gaussian distributions parameterized by $\theta_i$, $K_i$ is a normalizing factor, $A_i$ are normalized weights of Gaussians such that $\sum_{i}A_i = 1$. 
	% Here, a strong requirement of component choice is that it must be normliazable. Common components include spherical Gaussian mixture (SGM) and bivariate Gaussian mixture (BGM).
	Gaussian mixtures are highly expressive and also easily normalizable: we look into two common Gaussian distributions that can represent spherical distributions that we need for modeling the lighting: spherical Gaussian and anisotropic spherical Gaussian. 
	\paragraph{Spherical Gaussian}
	Spherical Gaussian (SG) is a variant of univariate Gaussian function defined in the spherical domain:
	\begin{equation}
	G(\mbv;\boldsymbol{\mu},\lambda) = \exp(\lambda (\boldsymbol{\mu} \cdot \mbv - 1)),
	\end{equation}
	where $\mu$ is the lobe axis and $\lambda$ is a parameter that controls the ``sharpness'' of the distribution. The normalizing term of an SG can be computed by: 
	\begin{equation}
	K = \int_{S^2} G(\mbv;\boldsymbol{\mu},\lambda) d\omega = \frac{2\pi}{\lambda}(1-e^{-2\lambda}).
	\end{equation}
	As SG is defined in the spherical domain and is suitable for representing all-frequency light distribution at each sample point,
	Gaussian mixture models based on SG has been applied for representing the light map for PRT \cite{SG}.     
	However, SG is an isotropic univariate model, hence less expressive; \eg it cannot represent anisotropic distributions.  
	
	% there is no distortion due to mapping a 2D plane to a unit sphere.
	%However, as SG is a univariate model, it is less expressive when used to represent complex distributions.
	
	\paragraph{Anisotropic Spherical Gaussian}
 To model anisotropic distributions, Xu et al.~\shortcite{ASG} propose Anisotropic Spherical Gaussian (ASG) to model the light map.  ASG can be written as:   
 \begin{equation}
	 G(\mbv;[\boldsymbol{x},\boldsymbol{y},\boldsymbol{z}],[\lambda,\mu],c) = c\cdot S(\mbv; \boldsymbol{z}) \cdot e^{(-\lambda (\mbv \cdot \boldsymbol{x})^2 - \mu (\mbv \cdot \boldsymbol{y})^2)}
\end{equation}
where $\boldsymbol{z}, \boldsymbol{x}, \boldsymbol{y}$ are the lobe, tangent and bi-tangent axes, respectively, and $[\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{z}]$ forms an orthonormal frame; $\lambda$ and $\mu$ are the bandwidths for $\boldsymbol{x}$- and $\boldsymbol{y}$-axes, respectively and  $c$ is the lobe amplitude.  Xu et al.~\shortcite{ASG} successfully models complex lighting conditions and  renders anisotropic metal dishes using ASG. 
On the other hand, ASG does not have a closed form solution for the integral, which inhibits its usage for our purpose.  
\begin{comment}

	\paragraph{Bivariate Gaussian}
	Bivariate Gaussian distribution,  is widely used because it can represent more complex distributions. A bivariate gaussian distribution reads:
	
	\begin{equation}
	\begin{aligned}
	&G(x,y;\rho,\mu_x, \mu_y, \sigma_x,\sigma_y) =\\ &exp(\frac{-1}{2(1-\rho^2)} \cdot (\frac{(x-\mu_x)^2}{\sigma_x^2}+\frac{(y-\mu_y)^2}{\sigma_y^2} - 2\rho \frac{(x-\mu_x)(y-\mu_y))}{\sigma_x \sigma_y})
	\end{aligned}
	\end{equation}
	
	With normalizing term
	\begin{equation}
	K = \frac{1}{2\pi \sigma_x \sigma_y \sqrt{1-\rho^2}}.
	\end{equation}
	
	While being able to model more complex distributions, BG also has certain problem when used to model spherical distributions. A common approach to use BG to represent spherical distribution is to use polar/ cylindrical coordinate, however since it has infinite domain of definition, when sampling, the sample may go over the boundary of polar/ cylindrical coordinates (i.e., $(\theta, \phi) \in [-\pi/2, \pi/2]\times[0,2\pi]$). When evaluating pdf of a given direction during MIS, this causes pdf on margins difficult to be evaluated, and usually needs workarounds (\eg merging pdf at boundaries or discarding out-of-bound samples) that leads to low sampling efficiency.
	\end{comment}
	
	\subsection{Normalized Anisotropic Spherical Gaussian Mixture}
	\label{sec:NASGM}
	Inspired by the models in \secref{background}, we propose the Normalized Anisotropic Spherical Gaussian mixture (NASG),
	where each mixture component can be written as:
	
	\begin{equation}
	\begin{aligned}
	&G(\mbv ;[\boldsymbol{x,y,z}], \lambda, a) \\
	&\quad= \begin{cases}\exp \left(2 \lambda\left(\frac{\mbv \cdot \boldsymbol{z}+1}{2}\right)^{1+\frac{a(\mbv \cdot \boldsymbol{x})^2}{1-\left(\mbv \cdot \boldsymbol{z}\right)^2}}-2 \lambda\right)\left(\frac{\mbv \cdot \boldsymbol{z}+1}{2}\right)^{\frac{a(\mbv \cdot \boldsymbol{x})^2}{1-(\mbv \cdot \boldsymbol{z})^2}} & \text { if } \mbv \cdot \boldsymbol{z} \neq \pm 1 \\
	1 & \text { if } \mbv \cdot \boldsymbol{z}=1 \\
	0 & \text { if } \mbv \cdot \boldsymbol{z}=-1\end{cases}
	\end{aligned}
	\end{equation}
	where $[{\boldsymbol{x},\boldsymbol{y},\boldsymbol{z}}]$ forms an orthonormal frame, $\lambda$ is the sharpness, and $a$ controls the eccentricity. \figref{nasg} shows examples of NASG component with different parameters.
	
	\begin{figure}[h]
		\begin{tabular}{cc}
			\includegraphics[height=3.2cm]{Images/NASG_components/l1_a0}&
			\includegraphics[height=3.2cm]{Images/NASG_components/l10_a0}\\
			$\lambda = 1, a = 0$&$\lambda = 10, a = 0$\\
			\includegraphics[height=3.2cm]{Images/NASG_components/l10_a10}&
			\includegraphics[height=3.2cm]{Images/NASG_components/l25_a100}\\
			$\lambda = 10, a = 10$&$\lambda = 25, a = 100$, rotated
		\end{tabular}
		\caption{Visualization of NASG compoent $G$ with different parameters. Note that $G$ agrees with sperical Gaussian when $a = 0$. }
		\label{fig:nasg}
	\end{figure}
	
	NASG satisfies all requirements as a model for learning the distribution for unbiased rendering. 
	Different from ASG \cite{ASG}, NASG has 
	an {\bf analytical closed form solution} for its integral
    (see Appendix \ref{sec:integral}),
	which makes it easy to normalize:
	\begin{equation}\label{eq:K for NASG}
	K = \int_{S^2} G(\boldsymbol{\mbv};[\boldsymbol{x,y,z}], \lambda, a) d \omega=\frac{2 \pi\left(1-e^{-2 \lambda}\right)}{\lambda \sqrt{1+a}}.
	\end{equation}
	NASG is also {\bf expressive} as it can model anisotropic distributions, as well as high and low frequency distributions 
	\footnote{\label{footnote:NASG} NASG is not continuous at $\boldsymbol{\mbv}=-\boldsymbol{z}$ in this form, however this does not affect our application, and can be solved easily by introducing an auxiliary parameter. See Appendix \ref{sec:continuity} for details.}. 
	It is also {\bf compact} as
	it is parameterized by only 7 scalars, making it highly efficient in GPU-based computation due to its low bandwidth requirement. Additionally, the sampling algorithm of NASG is remarkably {\bf efficient} (see Appendix \ref{sec:sampling}). These characteristics make NASG a highly feasible and practical option for our framework.
	
	\section{Online learning of Density Model}
	We now describe about our neural network structure for learning the parameters of our NASG model, as well as the training scheme. 
	\subsection{Network Architecture}
	\label{sec:Output}
	We wish our neural network to be as simple as possible considering the performance requirement for practical use. 
	In this work, we use a 4-layer MLP, where each layer has 128 units \textit{without} bias.
	
	\paragraph{Network Inputs}The input to the model consists of the location of the shading point $\mbx$, the outgoing ray direction $\omega_o$ and surface normal $\mbn$. 
	The location of the shading point is first encoded into a 57 dimention vector by positional encoding.  Thus, the input size is 63 (57 + 3 + 3), 
	but is padded to 64
	\footnote{By setting padded values to 1 we are able to have a cheap alternative to bias of hidden layers.} for hardware acceleration purpose. We adopt the simple one-blob'encoding \cite{MUL19}, 
	instead of other complex encoding mechanics such as \cite{INP}, as there was improvement in our pilot study versus the overhead they introduce. Details of the encoding scheme can be found in Table \ref{tab:encoding}.
	
	\begin{table}
		\caption{Encoding scheme}
		\label{tab:encoding}
		\begin{tabular}{ccc}
			\hline
			Parameter&Symbol&Encoding\\\hline
			Position&$p\in{\mathbb{R}^3}$&$ob(p)$\\
			Outgoing ray direction&$\omega_o\in{[-1,1]^3}$&$\omega_o$\\
			Surface normal&$\mbn\in{[-1,1]^3}$&$\mbn$
		\end{tabular}
	\end{table}
	
	\paragraph{Parameterization of NASG}
	To improve the efficiency of learning, we reduce the number of parameters for representing the NASG model. First, we introduce $\theta,\phi,\psi$, the Euler angles representing the 
	orientation of the orthogonal basis with respect to the global axes. As such, $\boldsymbol{z}$ and $\boldsymbol{x}$ of the orthogonal basis can be represented by  
	\begin{equation}
		\boldsymbol{z} =
	\begin{pmatrix}
		 \sin \theta \cos \phi \\
		 \sin \theta \sin \phi \\ 
		 \cos \theta
	\end{pmatrix}, \
		\boldsymbol{x} =
	\begin{pmatrix}
		\cos\theta \cos\phi \cos\psi - \sin\phi \sin\psi \\
		\cos\theta \sin\phi \cos\psi + \cos\phi \sin\psi \\
		-\sin\theta \cos\psi 
	\end{pmatrix}. 
	\end{equation}
	Thus, we represent NASG with the following 
	seven parameters: $\cos\theta$, $\sin\phi$, $\cos\phi$, $\sin\psi$, $\cos\psi$, $\lambda$, $a$.   
	
	\paragraph{Network Outputs} The output of our neural network is a $d$ dimensional vector $\boldsymbol{o}$, where each component corresponds to the  
 	NASG parameter at the shading point $\boldsymbol{\mbx}$ and a selection probability $c$. Assuming the number of mixture components is $N$, 
	$d = 5 \times N + 2 \times N + N + 1 = 8N + 1$ (see Table \ref{tab:decoding}). 
	These parameters are further decoded to represent the actual parameters as summarized in Table \ref{tab:decoding}.
	\begin{table}
		\caption{Decoding scheme for NASG with N components}
		\label{tab:decoding}
		\begin{tabular}{ccc}
			\hline
			Parameter&Symbol&Decoding\\\hline
			$\cos\theta$, $\sin\phi$, $\cos\phi$, $\sin\psi$, $\cos\psi$&$s_0\in{\mathbb{R}^{5 \times N}}$&$\operatorname{sigmoid}(s_0) \times 2 - 1$\\
			$\lambda, a$&$s_1\in{\mathbb{R}^{2\times N}} $&$e^{s_1}$\\
			Component weights&$\mbA\in{\mathbb{R}^N}$&$\operatorname{Tsm}(\mbA)$\\
			Selection probability&$c\in{\mathbb{R}}$&$\operatorname{sigmoid}(c)$\\
		\end{tabular}
	\end{table}
	The mixture weights of NASG, $\mbA$, are normalized with second order Taylor softmax \cite{taylorSoftmax} for computation stability, 
	considering we usually calculate them in GPU with half-precision floating point numbers and ``fast math mode'' approximations:
	\begin{equation}
	\operatorname{Tsm}(z)_i = \frac{1+z_i+0.5z^2_i}{\sum_{j=1}^{K} (1+z_j+0.5z^2_j) }
	\label{eq:taylorSoftMax}
	\end{equation}
	\subsection{Training}
%	The output of our network is a density distribution of rendering equation product represented with NASG $q(\omega_i) = D(\omega_i)$ as described in \secref{DDM}.
	We now describe about our loss function and the training process. 
	As shown in \figref{networkgraph}, we utilize automatic differentiation to train the network effectively with sparse online rendering samples. 
	Since the training data are noisy online samples, we need a loss function that is more robust than regular L1 or L2 losses.
	\begin{figure}
		\centering
		\includegraphics[width=0.99\linewidth]{Images/NetworkGraph}
		\caption{The training process of our network. We heavily utilize automatic differentiation to adjust weights of network based on estimated distribution and rendering estimation $\left< L\right>$ from sampled scattering direction $\omega_i$.}
		\label{fig:networkgraph}
	\end{figure}
	\paragraph{Kullback-Leibler Divergence}
	The design of a loss function for density distribution in PBR context is studied in several previous works including \cite{MUL19, MUL20, ZHU21, SHANDONG21}. Denoting $q(\omega_i;\gamma)$ the NASG distribution (with NASG parameters $\gamma$) estimated by neural network, to achieve importance sampling of rendering equation, the optimal density distribution should be proportional to the product:
    %\cite{Variance-awarePG} claims that in Monte-Carlo estimators, the optimal density distribution should be proportional to the square root of the second moment rather than the sampled product itself (\ie variance-aware): 
 \begin{equation}
	q(\omega_i;\gamma) \propto f_s(\mbx,\omega_i, \omega_o)L_i |\cos\theta|.
 \end{equation}
% \begin{equation}
%	q(\mbx;\theta) \propto \sqrt{E^2} = \frac{L(\mbx)}{p_m(\mbx)}.
%	\end{equation}
	Therefore, we use 
 $p(\omega_i) = F f_s(\mbx,\omega_i, \omega_o)L_i |\cos\theta|$
 %$p(\mbx) = F\sqrt{E^2}$
 as our target distribution where $F$ is a normalizing term whose value is unknown. We can still train the system without knowing $F$ as we show below. 
     
 In our framework, we use the Kullback-Leibler Divergence (KL-Divergence) to represent the likelihood between our estimated density distribution $q(\omega_i;\gamma)$ and the target one:
	\begin{equation}
	D_{KL}(p(\omega_i)||q(\omega_i;\gamma)) = \int_{\Omega}p(\omega_i)(\log[p(\omega_i)] - \log[q(\omega_i;\gamma)])d\omega_i.
	\end{equation}
%	Although the normalizing term $F$ is unknown, we are able to use KL-Divergence by replacing $p(x)$ with the normalized one-sample second moment $F\sqrt{E^2}$. This is still valid since we use gradient-based optimization, as the gradient of the corresponding KL-Divergence can be written as:
%By using  $p(\mbx) = F\sqrt{E^2}$ (which clearly is $\theta$-independent), 
Since $p(\omega_i)$ is $\gamma$-independent, we get 
	\begin{equation}
	\nabla_\gamma D_{KL}(p(\omega_i)||q(\omega_i;\gamma)) = -\int_{\Omega}p(\omega_i)\nabla_{\gamma}\log[q(\omega_i;\gamma)] d\omega_i.
	\end{equation}
%\begin{equation}
	%\nabla_\theta D_{KL}(p(\mbx)||q(\boldsymbol{\mbx};\theta)) = -F\int_{\Omega}\sqrt{E^2}\nabla_{\theta}\log[q(\boldsymbol{\mbx};\theta)] d\boldsymbol{\mbx}.
	%\end{equation}%	\begin{equation}
%	\nabla_\theta D_{KL}(p(\mbx)||q(\mbx;\theta)) = F\nabla_{\theta} \left[ \sqrt{E^2}(\log[F\sqrt{E^2}] - \log[q(\mbx;\theta)])\right]
%	\end{equation}
%	\begin{equation}
%	= F\nabla_\theta \mathbb{E} \left[ \sqrt{E^2}(\log(F) + \log[\sqrt{E^2}] - \log[q(\mbx;\theta)]) \right].
%	\end{equation}
%	Since the first and second terms are $\theta$-independent, the equation can be further simplified to
The integral can be replaced with our one-sample estimation:
%We importance sample the above integral with our samples which obey the distribution determined by $q(\boldsymbol{x};\theta)$:
	\begin{equation}
	\nabla_{\gamma} D_{KL}(p(\omega_i)||q(\omega_i;\gamma)) = -\, \mathbb{E} \left[ \frac{p(\omega_i)}{\hat{q}(\omega_i;\gamma)}\nabla_{\gamma}\log[q(\omega_i;\gamma)]\right],
	\end{equation}
 where $\hat{q}(\omega_i;\gamma)$ denotes the PDF of the distribution the sample obeys during rendering; see below.
%\begin{equation}
	%\nabla_{\theta} D_{KL}(p(\boldsymbol{\mbx})||q(\boldsymbol{\mbx};\theta)) = -F\, \mathbb{E} \left[ \sqrt{E^2}\nabla_{\theta}\log[q(\boldsymbol{\mbx};\theta)]\right].
	%\end{equation}
 %	\begin{equation}
%	\nabla_\theta D_{KL}(p(\mbx)||q(\mbx;\theta)) = -F\nabla_\theta \mathbb{E} \left[ \sqrt{E^2}\log[q(\mbx;\theta)]\right].
%	\end{equation}
Though the right-hand side still involves the global scaling factor $F$, it
%	The global scaling factor $F$ 
will be canceled in moment-based optimizers such as Adam \cite{Adam}; thus we can train the system even there is an unknown scaling factor in $p(\omega_i)$~\cite{MUL19}.
 
 
	\paragraph{Selection Probability}
	   Following \cite{MUL20}, we do not directly use a distribution estimated by our neural network. Instead, we consider that our scattering sampling process is an MIS process that blends the learned distribution and the BSDF distribution $p_{f_s}(\omega_i)$. As we also learn a selection probability $c$ as described in \secref{Output}, the MIS PDF at shading point $\boldsymbol{\mbx}$ is $\hat{q}(\omega_i;\gamma) = cq(\omega_i;\gamma) + (1-c)p_{f_s}(\omega_i)$. However, optimizing $\hat{q}(\omega_i;\gamma)$ naively will lead to falling into a local optima with degenerate selection probability; therefore our final loss function blends $\hat{q}(\omega_i; \gamma)$ with $q(\omega_i;\gamma)$:
	\begin{equation}
	loss = e D_{KL}(p(\omega_i)||\hat{q}(\omega_i; \gamma)) + (1-e)D_{KL}(p(\omega_i)||q(\omega_i; \gamma))
	\end{equation}
	where $e$ is a fixed fraction which we set to 0.2.

 
	\section{Implementation Details}
    We integrate our framework to an in-house GPU wavefront path tracer. In this section we discuss about implementation details as well as design considerations.
	\paragraph{Progressive Learning and Sampling} \label{sec:progressive}
	Our framework collects samples to train the network online, and use the network to generate a distribution, which is then blended with the BSDF sampling distribution using the learned selection probability $c$. We further multiply an extra coefficient $b$ to compute an actual blending weight $c'=bc$, where $b$ is initially set 0 and gradually increased to 1, such that the framework initially relies on BSDF distribution, and gradually switches to the learned distribution $\hat{q}(\omega_i;\gamma)$. We use a fixed-step strategy to ensure our guided sampling uses enough number of samples for learning the distribution: for every $M$ images rendered, we increase $b$ by $\frac{1}{R}$. In our implementation, we set $M=4$ and $R=64$. This could require adjustments according to rendering task, further discussions are given in \secref{Discussion}.

    For rendering images while progressively learning the distribution, previous works~\cite{MUL17, MUL19, MUL20} blend the rendered image samples 
    based on their inverse variance.  Such a process will introduce extra memory and computation overhead, especially for a renderer that produces arbitrary output variables (AOVs). Based on the observation that variance of individual image pixel decreases as the learning proceeds, we use a simplified method which scales the weights of accumulated results according to the training steps. This essentially gives more weight to later samples in a progressive manner, and the result is still unbiased. The weight scaling process stops when the training step count reaches $M \times R$.

	
	\paragraph{CPU-based Network Optimization}
	%Despite we focus on learning-based path guiding for conventional PCs, to fully leverage computational power of one, 
The process of distribution learning runs on CPU in parallel to the main rendering task being executed on GPU.
  We use Pytorch \cite{Pytorch} for the implementation, along with Adam optimizer whose learning rate is set to 0.02. 
 When rendering image, we split the image into smaller tiles, whose size is set to $l \times l$, and collect data from one pixel per tile in each render iteration.
 We set the maximum size of training samples in each iteration to $S=2^{14}$.
 If the number of training samples reaches $S$ in the middle of the iteration, $l$ is increased by one in the next iteration.  If the number of samples is less than $S/2$ at the end of iteration, $l$ is decreased by one. 
 The collected data are sent to the CPU after each render iteration for progressively training the model. 

 %Due to limited computation resource, only several batches of training can be executed. Therefore, we only gather training data with a dynamic span strategy within the full image plane. We initially set maximum size of gathered data as $S=2^{14}$ and initial span size $L=0$. At each rendering iteration $i$, we gather training data from pixels $P(x,y)$ where $mod(x,L) + mod(y,L)\times L =mod(i,L^2)$, but no more than $S$. After rendering iteration finishes, we obtain $S'$ training data and adjust the span size $L$ accordingly: we increase $L$ by $1$ if $S'= S$, or decrease $L$ by $1$ (but not lower than 0) if $S' \le \frac{S}{2}$. In this way we are able to gather just enough training data from all the pixels over the image plane.
	
	\paragraph{Direct Lighting with NEE}
	Our implementation uses MIS to blend guided scattering directions with Next Event Estimation (NEE) to sample direct lighting. NEE helps to improve the quality of the images especially in the beginning of the training stage. It is to be noted that MIS is only possible because our model is an explicit model that allows us to freely sample the distribution and evaluate PDF at certain direction; which could be difficult for methods such as ~\cite{MUL19,MUL20}. Our MIS strategy also helps to improve the the sampling efficiency for effects where direct light plays a key role (\eg soft shadows). 
 
 
% This is based on two major considerations: (1) Our neural network can learn both direct and indirect lighting precisely, and the output distribution can be sampled explicitly multiple times, which is capable for MIS with NEE; (2) NEE is an effective importance sampling method and a status-quo for robust production path tracer, and we focus on how much a production renderer can benefit from our framework. We found that our framework also helps to improve sampling efficiency of direct light rendering, especially that of soft shadows. 
	\paragraph{Wavefront Architecture}
	Neural network computation can be greatly accelerated with hardware (\eg Tensor Core). However, such hardware is usually designed for batched execution, in which a single matrix multiplication involves multiple threads. This conflicts with the classic thread-independent ``Mega-kernel'' path tracing implementation. Our implementation adopts ``Wavefront path tracing'' architecture \cite{WAVEFRONT} to solve this problem. In a wavefront path tracer, all the pixels at the same stage will be executed concurrently. With this condition, we are able to ``insert'' our neural network calculation seamlessly right before the sampling/shading stage, and minimize the computation overhead for best performance. The forward pass of our MLP does not require gradient calculation, therefore we implement it on GPU with CUBLAS, and the result implementation is substantially faster than using existing deep learning framework.
	
	\section{Results}
	We render a variety of scenes with our framework and compare the results with those rendered by two other methods: classic path tracing (with NEE) and a GPU implementation of \textit{Practical Path Guiding} (PPG). Our PPG implementation is based on \cite{PPG19} and the authors' CPU implementation, while different from the authors' version in two aspects: (1) we use voxels instead of octree for spatial data structure for efficient GPU-based refinement, and (2) NEE is considered for improving the quality of the rendered images. We do not compare our result with off-line methods or normalizing-flow-based online methods, because of the unmatched requirement of computation resource. More sophisticated online learning methods (\eg \cite{SDMM, Variance-awarePG}) are not compared due to the difficulty of implementation to a GPU path tracer.
    %, while the result comparison to PPG can be found in corresponding papers. %I think this sentence is very problematic though.
    In the following experiments, we run our path tracer with a conventional PC with a 4-core i7 9700K CPU and an NVIDIA 2080ti GPU. The implementation of our framework introduces roughly 600 MB memory overhead on GPU, mainly for buffer of batched network execution. All the images are also included in the supplemental material. We use mean absolute percentage error (MAPE) as the metric.
	\subsection{Variance Reduction}
	Fig. \ref{fig:variance_same_sample} shows a same-sample comparison of different methods. Every image is rendered with 1024 samples per pixel (spp). ``Veach Bidir'' scene demonstrates basic application of our framework in regular 3D scenes that consist of many complex light paths that are difficult to be sampled with unidirectional path tracing. ``Light Cage'' is a typical difficult scene where only emissive geometry is used to light the space and it requires capability to learn high frequency varying distributions. ``Kitchen 1'' scene is a more complex scene where the refracted lighting on the floor is the major illumination. The result shows both robustness and efficiency of our method. Our framework achieves lowest variance in all scenes. PPG performs generally well in scenes where indirect lighting is dominant, however, due to its partition-based learning, it shows limitation on learning high-frequency spatial varying distribution and gives higher variance for the ``Light Cage'' scene. We also find in scenes where sampling can greatly benefit from NEE (the ``Ajar'' scene for example), the variance of PPG is slightly higher than unidirectional path tracing, which is consistent with experiment results in previous work (\eg \cite{ZHU21}).
	
	\begin{figure*}[h]
		\begin{tabular}{crcccc}
			&&PT&NPG (Ours)&PPG&Reference
			\\
			\rotatebox{90}{VEACH BIDIR}&
			\begin{minipage}{0.4\linewidth}
				\includegraphics[height=4.2cm]{Images/variance_bidir/ground_truth_marked}
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_bidir/uni_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_bidir/uni_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_bidir/ai_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_bidir/ai_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_bidir/ppg_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_bidir/ppg_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_bidir/reference_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_bidir/reference_1}
			\end{minipage}
			\\
			&MAPE:&0.253&\textbf{0.078}&\textbf{0.078}&
			\\
			\rotatebox{90}{LIGHT CAGE}&
			\begin{minipage}{0.4\linewidth}
				\includegraphics[height=4.2cm]{Images/variance_maze/ground_truth_marked}
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_maze/uni_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_maze/uni_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_maze/ai_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_maze/ai_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_maze/ppg_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_maze/ppg_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_maze/reference_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_maze/reference_1}
			\end{minipage}
			\\
			&MAPE:&1.34&\textbf{0.220}&0.395&
			\\
			\rotatebox{90}{KITCHEN 1}&
			\begin{minipage}{0.4\linewidth}
				\includegraphics[height=4.2cm]{Images/variance_kitchen/ground_truth_marked}
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_kitchen/uni_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_kitchen/uni_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_kitchen/ai_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_kitchen/ai_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_kitchen/ppg_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_kitchen/ppg_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_kitchen/reference_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_kitchen/reference_1}
			\end{minipage}
			\\
			&MAPE:&0.617&\textbf{0.062}&0.462&
			\\
			\rotatebox{90}{AJAR}&
			\begin{minipage}{0.4\linewidth}
				\includegraphics[height=4.2cm]{Images/variance_ajar/ground_truth_marked}
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_ajar/uni_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_ajar/uni_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_ajar/ai_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_ajar/ai_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_ajar/ppg_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_ajar/ppg_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2.1cm]{Images/variance_ajar/reference_0}
				\newline
				\includegraphics[height=2.1cm]{Images/variance_ajar/reference_1}
			\end{minipage}
			\\
			&MAPE:&0.147&\textbf{0.051}&0.136&
		\end{tabular}
		\caption{Rendering result of different methods under same 1024 sample-per-pixel (SPP). All images are $1920 \times 1080$ pixels. The mean absolute percentage error (MAPE) indicates that our method (NPG) outperforms plain path tracing (PT) and \cite{PPG19} (PPG) in all scenes. PPG achieves low variance in scenes where narrow indirect lighting is dominant (``Veach Bidir'', ``Light Cage'', and ``Kitchen 1''), however in scenes with more general lighting setup (``Ajar'') it fails to outperform unidirectional path tracing with NEE.}
		\label{fig:variance_same_sample}
	\end{figure*}
	\subsection{Equal Time Comparison}
	As a performance-centered framework, we also conduct equal time comparison to intuitively evaluate raw performance of our framework compared to PPG and classic path tracing. For every scene, we perform 5-minute rendering including any extra learning computation on the same hardware. With careful implementation, our framework has a low overhead than previous neural-network-based frameworks such as \cite{MUL20, SHANDONG21}. As shown in Fig. \ref{fig:variance_same_time},  our framework achieves a general boost of sampling efficiency compared to a plain path tracer in \textbf{every} test scene. Given the same time budget our framework achieves lowest variance in all test scenes, including ``Kitchen 2'' which is mainly lit by a large area light, and considered as an example which benefits little from guiding.
	\begin{figure*}[h]
		\begin{tabular}{crcccc}
			&&PT&NPG (Ours)&PPG&Reference
			\\
			\rotatebox{90}{CORRIDOR}&
			\begin{minipage}{0.4\linewidth}
				\begin{center}
					\includegraphics[height=4cm]{Images/same_time_corridor/ground_truth_marked}
				\end{center}	
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_corridor/uni_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_corridor/uni_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_corridor/ai_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_corridor/ai_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_corridor/ppg_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_corridor/ppg_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_corridor/reference_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_corridor/reference_1}
			\end{minipage}
			\\
			&MAPE:&0.174&\textbf{0.096}&0.150&
			\\
			&SPP:&4855&1887&2031&
			\\
			\rotatebox{90}{POOL}&
			\begin{minipage}{0.4\linewidth}
				\begin{center}
					\includegraphics[height=4cm]{Images/same_time_pool/ground_truth_marked}
				\end{center}	
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_pool/uni_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_pool/uni_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_pool/ai_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_pool/ai_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_pool/ppg_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_pool/ppg_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_pool/reference_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_pool/reference_1}
			\end{minipage}
			\\
			&MAPE:&0.298&\textbf{0.046}&0.149&
			\\
			&SPP:&3772&1521&1764&
			\\
			\rotatebox{90}{KITCHEN 2}&
			\begin{minipage}{0.4\linewidth}
				\begin{center}
					\includegraphics[height=4cm]{Images/same_time_kitchen/ground_truth_marked}
				\end{center}	
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_kitchen/uni_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_kitchen/uni_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_kitchen/ai_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_kitchen/ai_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_kitchen/ppg_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_kitchen/ppg_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_kitchen/reference_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_kitchen/reference_1}
			\end{minipage}
			\\
			&MAPE:&0.042&\textbf{0.041}&0.101&
			\\
			&SPP:&3402&1386&1433&\\
			\rotatebox{90}{BATHROOM}&
			\begin{minipage}{0.4\linewidth}
				\begin{center}
					\includegraphics[height=4cm]{Images/same_time_bathroom/ground_truth_marked}
				\end{center}	
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_bathroom/uni_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_bathroom/uni_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_bathroom/ai_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_bathroom/ai_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_bathroom/ppg_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_bathroom/ppg_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=2cm]{Images/same_time_bathroom/reference_0}
				\newline
				\includegraphics[height=2cm]{Images/same_time_bathroom/reference_1}
			\end{minipage}
			\\
			&MAPE:&0.081&\textbf{0.063}&0.110&
			\\
			&SPP:&3735&1456&1611&
		\end{tabular}
		\caption{Equal time comparison. We render all the images for 5 minutes and compare the error measured in MAPE. The tolal SPP is also recorded. Our framework introduces an ~150\% overhead on top of plain path tracing (mostly due to neural network execution and additional memory traffic), however since the sampling efficiency is greatly improved, the results still surpass PT and PPG.}
		\label{fig:variance_same_time}
	\end{figure*}
	\begin{comment}
	Even though we focus on GPU-based neural guiding technique and our framework is implemented for GPU path tracer, we compare the improvement of it with a recent CPU-based path guiding work \cite{Variance-awarePG}, which is considered to learn a better distribution than \cite{MUL17}. We use the implementation provided by the authors to render two scenes with both path tracing and variance-aware path guiding for same time. The scene setups (geometry, lighting, and material) are basically the same with what we used in our GPU path tracer, however slight difference exists due to implementation decisions. The result of CPU path tracer is shown in Fig. \ref{fig:cpu_same_time}, and compared to our implementation, ours shows greater improvement when compared with corresponding path tracing implementation. For example, for the room scene, our MAPE is 0.024 lower than path tracing, while \cite{Variance-awarePG} shows 0.022 higher MAPE compared to path tracing result. This is because our implementation has a (relatively) low overhead, therefore it can sample more, while keeping good importance sampling efficiency.
	\begin{figure*}[h]
	\begin{tabular}{crccc}
	&&PT (CPU)&VAPG&Reference
	\\
	\rotatebox{90}{KITCHEN}&
	\begin{minipage}{0.4\linewidth}
	\begin{center}
	\includegraphics[height=3.6cm]{Images/cpu_same_time_kitchen/ground_truth_marked}
	\end{center}	
	\end{minipage}
	\hspace{0.1cm}
	&
	\begin{minipage}{0.1\linewidth}
	\includegraphics[height=1.8cm]{Images/cpu_same_time_kitchen/uni_0}
	\newline
	\includegraphics[height=1.8cm]{Images/cpu_same_time_kitchen/uni_1}
	\end{minipage}
	&
	\begin{minipage}{0.1\linewidth}
	\includegraphics[height=1.8cm]{Images/cpu_same_time_kitchen/vapg_0}
	\newline
	\includegraphics[height=1.8cm]{Images/cpu_same_time_kitchen/vapg_1}
	\end{minipage}
	&
	\begin{minipage}{0.1\linewidth}
	\includegraphics[height=1.8cm]{Images/cpu_same_time_kitchen/reference_0}
	\newline
	\includegraphics[height=1.8cm]{Images/cpu_same_time_kitchen/reference_1}
	\end{minipage}
	\\
	&MAPE:&\textbf{0.054}&0.076&
	\\
	\rotatebox{90}{AJAR}&
	\begin{minipage}{0.4\linewidth}
	\begin{center}
	\includegraphics[height=3.6cm]{Images/cpu_same_time_ajar/ground_truth_marked}
	\end{center}	
	\end{minipage}
	\hspace{0.1cm}
	&
	\begin{minipage}{0.1\linewidth}
	\includegraphics[height=1.8cm]{Images/cpu_same_time_ajar/uni_0}
	\newline
	\includegraphics[height=1.8cm]{Images/cpu_same_time_ajar/uni_1}
	\end{minipage}
	&
	\begin{minipage}{0.1\linewidth}
	\includegraphics[height=1.8cm]{Images/cpu_same_time_ajar/vapg_0}
	\newline
	\includegraphics[height=1.8cm]{Images/cpu_same_time_ajar/vapg_1}
	\end{minipage}
	&
	\begin{minipage}{0.1\linewidth}
	\includegraphics[height=1.8cm]{Images/cpu_same_time_ajar/reference_0}
	\newline
	\includegraphics[height=1.8cm]{Images/cpu_same_time_ajar/reference_1}
	\end{minipage}
	\\
	&MAPE:&0.157&\textbf{0.082}&
	\\
	\end{tabular}
	\caption{As a reference, we use the implementation of ``variance-aware path guiding (VAPG)'' and path tracing (PT) provided by the authors of \cite{Variance-awarePG} to launch another same-time comparison running in CPU-based renderer Mitsuba. The variance of VAPG compared to corresponding PT result is similar or higher that of our framework.}
	\label{fig:cpu_same_time}
	\end{figure*}
	\end{comment}
	\subsection{Density Models and Meta-parameters}
%	In \secref{DDM} we discussed about the advantage of NASG upon other available density models. Here we compare   
In this section, we compare the expressiveness of NASG with other density models including \cite{MDMA}(MDMA), SG \cite{SG}, and bivariate Gaussian \cite{VORBA14}. 
%Cylindrical high dynamic-range environment maps are  prepared and the light distributions are learned using our neural network framework. 
For the other models, the network outputs their parameters in a fashion similar to the setup for NASG.

We limit the number of parameters to 256 for all models for a fair comparison. 
As shown in Fig. \ref{fig:different_model}, in all the test scenes our NASG gives lowest MAPE. We also notice that results of bivariate Gaussian (G2D) contains more outliers, this is due to the domain mismatch problem mentioned in \secref{DensityModels}.
	\begin{figure*}[h]
		\begin{tabular}{crcccc}
			&&MDMA&SG&G2D&NASG
			\\
			\rotatebox{90}{KITCHEN 2}&
			\begin{minipage}{0.4\linewidth}
				\begin{center}
					\includegraphics[height=3.6cm]{Images/different_model_kitchen/ground_truth_marked}
				\end{center}	
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_model_kitchen/mdma_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_model_kitchen/mdma_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_model_kitchen/sg50_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_model_kitchen/sg50_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_model_kitchen/g2d_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_model_kitchen/g2d_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_model_kitchen/nasg30_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_model_kitchen/nasg30_1}
			\end{minipage}
			\\
			&MAPE:&0.077&0.069&0.109&\textbf{0.065}
			\\
			\rotatebox{90}{CORRIDOR}&
			\begin{minipage}{0.4\linewidth}
				\begin{center}
					\includegraphics[height=3.6cm]{Images/different_model_corridor/ground_truth_marked}
				\end{center}	
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_model_corridor/mdma_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_model_corridor/mdma_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_model_corridor/sg50_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_model_corridor/sg50_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_model_corridor/g2d_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_model_corridor/g2d_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_model_corridor/nasg30_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_model_corridor/nasg30_1}
			\end{minipage}
			\\
			&MAPE:&0.150&0.139&0.189&\textbf{0.131}
			\\
		\end{tabular}
		\caption{Rendered result with our framework adopting different normalized density models. All images are rendered with 5 minutes, and parameter number of different models are set to be 256. Our proposed NASG achieves lowest variance in both test scenes.}
		\label{fig:different_model}
	\end{figure*}

    We also compare the results with different meta-parameters of the system. 
    To best evaluate the results we perform equal-time rendering with different configurations. 
    \figref{different_config} shows the results produced with different number of hidden units and NASG components. Increasing hidden units results in higher variance; this is because computation of neural network is the major overhead for the whole framework and more hidden units significantly increases the computation time. While the improvement of accuracy is not affected much, the sample counts within the same computational time is reduced.
	\begin{figure*}
		\begin{tabular}{crcccc}
			&&HU=128, C=4&HU=128, C=8&HU=256, C=4&HU=256, C=8
			\\
			\rotatebox{90}{BATHROOM}&
			\begin{minipage}{0.4\linewidth}
				\begin{center}
					\includegraphics[height=3.6cm]{Images/different_config_bathroom/ground_truth_marked}
				\end{center}	
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_config_bathroom/128_4_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_config_bathroom/128_4_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_config_bathroom/128_8_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_config_bathroom/128_8_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_config_bathroom/256_4_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_config_bathroom/256_4_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_config_bathroom/256_8_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_config_bathroom/256_8_1}
			\end{minipage}
			\\
			&MAPE:&\textbf{0.057}&0.058&0.064&0.065
			\\
			\rotatebox{90}{AJAR}&
			\begin{minipage}{0.4\linewidth}
				\begin{center}
					\includegraphics[height=3.6cm]{Images/different_config_ajar/ground_truth_marked}
				\end{center}	
			\end{minipage}
			\hspace{0.1cm}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_config_ajar/128_4_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_config_ajar/128_4_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_config_ajar/128_8_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_config_ajar/128_8_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_config_ajar/256_4_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_config_ajar/256_4_1}
			\end{minipage}
			&
			\begin{minipage}{0.1\linewidth}
				\includegraphics[height=1.8cm]{Images/different_config_ajar/256_8_0}
				\newline
				\includegraphics[height=1.8cm]{Images/different_config_ajar/256_8_1}
			\end{minipage}
			\\
			&MAPE:&\textbf{0.055}&0.056&0.067&0.069
			\\
		\end{tabular}
		\caption{We further evaluate the variance reduction of different parameter configurations of NASG. We use different number of hidden units(HU) and components (C) and render scenes with equal time. All results show similar variance, however the simplest configuration with HU=128 and C=4 achieves ~20\% lower MAPE than most complex configuration with HU=256 and C=8. This shows that all configurations can learn distribution well, however complex configuration has higher overhead hence lower raw performance.}
		\label{fig:different_config}
	\end{figure*}
	In terms of NASG, we find that the 4-component setup has only minor difference compared to the 8-component setup. This is because for most situations, the 4-component NASG is expressive enough to learn the distribution.
	\subsection{Learned Components}
	Our framework learns distribution and selection probability with \textit{single} neural network. In Fig. \ref{fig:selectionProbability} we visualize examples of the learned components in two actual rendering scenes. Our learned selection probability tends to rely more on our learned distribution where surface is rough, while smooth surface and where direct lighting is dominant prefers BSDF distribution. The learned contribution fits the reference closely and the high energy spots are accurately localized. Our framework is able to closely fit high-frequency narrow distributions with NASG. PPG fails to learn a good distribution for yellow point of the ``Box'' scene, even we use a high-resolution voxel structure for caching. This is a known limitation when the light distribution within the corresponding cache cell drastically varies (since the light source is very close to the red wall, a small difference in position leads to large difference in direction of incoming light). In contrast, our framework can learn such spatial-varying distribution accurately with proper position encoding scheme.
	
	\begin{figure*}[h]
		\centering
		\begin{tabular}{ccc}
			&CORRIDOR&BOX\\
			\rotatebox{90}{Reference}
			&\includegraphics[width=0.45\linewidth]{Images/distribution/corridor/reference}
			&\includegraphics[width=0.45\linewidth]{Images/distribution/box/reference}\\
			\rotatebox{90}{Selection Probability}
			&\includegraphics[width=0.45\linewidth]{Images/distribution/corridor/sp}
			&\includegraphics[width=0.45\linewidth]{Images/distribution/box/sp}\\
			\rotatebox{90}{Blue}
			&\begin{minipage}{0.45\linewidth}
				\begin{tabular}{ccc}
					Reference&Ours&PPG\\
					\includegraphics[width=0.31\linewidth]{Images/distribution/corridor/1}&
					\includegraphics[width=0.31\linewidth]{Images/distribution/corridor/1_learned}&
					\includegraphics[width=0.31\linewidth]{Images/distribution/corridor/1_ppg}
				\end{tabular}
			\end{minipage}
			&\begin{minipage}{0.45\linewidth}
				\begin{tabular}{ccc}
					Reference&Ours&PPG\\
					\includegraphics[width=0.31\linewidth]{Images/distribution/box/1}&
					\includegraphics[width=0.31\linewidth]{Images/distribution/box/1_learned}&
					\includegraphics[width=0.31\linewidth]{Images/distribution/box/1_ppg}
				\end{tabular}
			\end{minipage}\\
			\rotatebox{90}{Yellow}
			&\begin{minipage}{0.45\linewidth}
				\begin{tabular}{ccc}
				\includegraphics[width=0.31\linewidth]{Images/distribution/corridor/2}&
				\includegraphics[width=0.31\linewidth]{Images/distribution/corridor/2_learned}&
				\includegraphics[width=0.31\linewidth]{Images/distribution/corridor/2_ppg}
				\end{tabular}
				
			\end{minipage}
			&\begin{minipage}{0.45\linewidth}
				\begin{tabular}{ccc}
				\includegraphics[width=0.31\linewidth]{Images/distribution/box/2}&
				\includegraphics[width=0.31\linewidth]{Images/distribution/box/2_learned}&
				\includegraphics[width=0.31\linewidth]{Images/distribution/box/2_ppg}
				\end{tabular}
			\end{minipage}
			
		\end{tabular}
		\caption{Visualization of learned selection probability and distribution. Our framework uses a single neural network to learn these two components accurately. The selection probability determines the blending weight of two distributions we can sample from: our learned product distribution (green) and BSDF distribution of surface (red). Smooth surface prefers BSDF distribution while rough one prefers learned product distribution. Our result is consistent with previous works such as \cite{MUL20} which learns selection probability and lighting distribution separately. ``Blue'' and ``Yellow'' show distribution obtained with both our framework and PPG, in compare with the reference. Readers are encouraged to zoom in to view the details.}
		\label{fig:selectionProbability}
	\end{figure*}
 \subsection{Guided Direct Lighting}
 As shown in Fig. \ref{fig:nee}, we render direct lighting of a simple scene with certain occlusion, blending NEE with both BSDF distribution and our learned distribution, for 1024 samples. The variance of our learned distribution result is slightly lower than BSDF distribution, and soft shadow appears to be less ``noisy''. The result shows that both direct and indirect lighting can benefit from our framework.
	\begin{figure*}[t]
		\begin{tabular}{rccc}
			&Guided&Unidirectional&Reference\\
			\begin{minipage}{0.3\linewidth}
				\includegraphics[width=0.99\linewidth]{Images/NEE/ground_truth_marked}
			\end{minipage}&
			\begin{minipage}{0.15\linewidth}
				\includegraphics[width=0.99\linewidth]{Images/NEE/ai_0}
				\newline
				\includegraphics[width=0.99\linewidth]{Images/NEE/ai_1}
			\end{minipage}&
			\begin{minipage}{0.15\linewidth}
				\includegraphics[width=0.99\linewidth]{Images/NEE/uni_0}
				\newline
				\includegraphics[width=0.99\linewidth]{Images/NEE/uni_1}
			\end{minipage}&
			\begin{minipage}{0.15\linewidth}
				\includegraphics[width=0.99\linewidth]{Images/NEE/reference_0}
				\newline
				\includegraphics[width=0.99\linewidth]{Images/NEE/reference_1}
			\end{minipage}\\
			MAPE:&\textbf{0.050}&0.058&
		\end{tabular}
		
		
		\caption{Rendering result of direct lighting using NEE, blending with guided and unidirectional scattering samples. Both results are rendered with 512 samples-per-pixel (SPP). Our guided result has lower variance, and the soft shadows appear to be less noisy.}
		\label{fig:nee}
	\end{figure*}
	\subsection{Weight Reuse}
When rendering in a (relatively) static scene with moving cameras, reusing weights of neural network rather than initializing them every frame can further reduce variance. This is because our model learns the spatial-varying density distribution for the whole sampled space. \figref{weight_reuse} shows an example of rendering with weight reuse, and readers are referred to supplementary video for a comparison of weight reuse in a full sequence of the corresponding fly-through cut.
	\begin{figure*}[h]
	\begin{tabular}{rccc}
		&Reused&From scratch&Reference\\
		\begin{minipage}{0.45\linewidth}
			\includegraphics[width=0.99\linewidth]{Images/weight_reuse/ground_truth_marked}
		\end{minipage}&
		\begin{minipage}{0.12\linewidth}
			\includegraphics[width=0.99\linewidth]{Images/weight_reuse/19_reused_0}
			\newline
			\includegraphics[width=0.99\linewidth]{Images/weight_reuse/19_reused_1}
		\end{minipage}&
		\begin{minipage}{0.12\linewidth}
			\includegraphics[width=0.99\linewidth]{Images/weight_reuse/19_wo_reuse_0}
			\newline
			\includegraphics[width=0.99\linewidth]{Images/weight_reuse/19_wo_reuse_1}
		\end{minipage}&
		\begin{minipage}{0.12\linewidth}
			\includegraphics[width=0.99\linewidth]{Images/weight_reuse/reference_0}
			\newline
			\includegraphics[width=0.99\linewidth]{Images/weight_reuse/reference_1}
		\end{minipage}\\
		MAPE:&\textbf{0.104}&0.127&
	\end{tabular}
	
	
	\caption{Rendering result with/ without weight reuse. The image is a frame in the middle of a fly-through cut. With weight reuse, we take the weights from previous frame as the initial weights to start rendering and continue optimizing the neural network. Weight reuse helps to further reduce variance in rendering of (relatively) static scene with novel views.}
	\label{fig:weight_reuse}
\end{figure*}
	\section{Discussion}
	%The most attractive property of path guiding technique is that it does not break existing path tracing implementation; the path tracer just needs to replace existing distribution (usually BSDF-based) with the learned one, and the rest of calculation remains the same. This makes it much easier to improve an existing rendering software, and any ``trick'' that unidirectional path tracers apply and artists are familiar with could be kept. 

	\subsection{Performance in GPU Path Tracing}
	A GPU-based brute-force path tracer could be 20x faster than a CPU-based one. If an importance sampling method provides low per-sample variance yet with high overhead, it can not beat a brute-force path tracer on GPU. To this end, we eliminate normalizing-flow-based methods since the computational overhead is not acceptable in our application scenario. Besides powerful computation ability, GPU has limited memory bandwidth and size, and memory latency is much more obvious than CPU memory, causing many CPU algorithms to perform relatively slow in GPU (\eg PPG's quad-tree representation requires random access to GPU memory for multiple times, which causes long stall on GPU due to memory latency).
	
 % \clearpage
 % \clearpage
	\subsection{Limitation and Future Work}\label{sec:Discussion}
	During experiment we found that parameters of learning strategy we mentioned in \secref{progressive} needs to be carefully chosen among different rendering tasks. Although in general we can use a very aggressive configuration and it works well, we encountered several cases where we need to have bigger $M$ to avoid degenerate distribution. This usually happens in rendering of caustics. Our explanation is when the distribution is difficult to be learned (\eg due to poor sampling efficiency), using the learned distribution too soon will lead to even worse samples, and the learning result could never recover from degeneration. It is worth trying to propose an adaptive configuration of learning strategy to improve the robustness of our framework and we leave it as a future work.
	
	The most interesting extension of our framework would be guiding with participating media. This could be achieved in theory (similar approach to \cite{VPG}), yet further the ability to learn 3D distribution remains to be investigated.
 
	\section{Conclusion}
	%In this paper we propose an effective online neural path guiding framework for unbiased physically-based rendering. Our work shows that it is possible to learn the explicit spatial varying distribution with a tiny neural network for every unique shading point. We propose a novel spherical anisotropic Gaussian distribution model, NASG, to achieve the best result. Compared to previous learning-based approach, our framework has a very low overhead, making it possible to run our online neural path guiding on conventional hardware. We implement our framework to a production GPU path tracer, and the results show that our framework can be integrated to conventional GPU path tracers to achieve improvement in sampling efficiency.
 In this paper we propose an effective online neural path guiding framework for unbiased physically-based rendering. We tackle the major challenges of neural-based online path guiding methods by proposing a novel closed-form density model, NASG. The simplicity and expressiveness of NASG allow it to be efficiently learned online via a tiny, MLP-based neural network. We also propose the online training strategy to train a neural network with sparse ray samples.

Our framework helps to improve the raw performance of path tracer via neural path guiding. Under the same computational time constraint, our framework outperforms state-of-the-art path guiding techniques. This is because our framework can effectively learn the spatial-varying distribution and guide a unidirectional path tracer with low overhead, allowing a path tracer to produce high quality images with limited computational resources. Our work also shows that learning-based importance sampling has a great potential in practical rendering tasks. We hope our work can help pave the path for the research communities in industries and academia for adopting neural technologies for path tracing.

\appendix
\section{Continuity of NASG}
\label{sec:continuity}
The complete form of a NASG component is given by:
\begin{equation}
    \begin{aligned}
        &G(\boldsymbol{\mbv} ;[\boldsymbol{x,y,z}], \lambda, a, \epsilon, \kappa) \\
        &\quad= \begin{cases}
            \kappa \cdot \exp \left(2 \lambda\left(\frac{\boldsymbol{\mbv} \cdot \boldsymbol{z}+1}{2}\right)^{1+\epsilon+\frac{a(\boldsymbol{\mbv} \cdot \boldsymbol{x})^2}{1-\left(\boldsymbol{\mbv} \cdot \boldsymbol{z}\right)^2}}-2 \lambda\right)\left(\frac{\boldsymbol{\mbv} \cdot \boldsymbol{z}+1}{2}\right)^{\epsilon + \frac{a(\boldsymbol{\mbv} \cdot \boldsymbol{x})^2}{1-(\boldsymbol{\mbv} \cdot \boldsymbol{z})^2}} \\ \hspace{52mm} \text { if } \boldsymbol{\mbv} \cdot \boldsymbol{z} \neq \pm 1 \\
            \kappa & \hspace{-20mm} \text { if } \boldsymbol{\mbv} \cdot \boldsymbol{z}=1 \\	
            0 & \hspace{-20mm} \text { if } \boldsymbol{\mbv} \cdot \boldsymbol{z}=-1
         \end{cases}
    \end{aligned}
\end{equation}
where $\kappa$ is the lobe amplitude and $\epsilon$ is an auxiliary parameter introduced so that $G$ is continuous whenever $\epsilon > 0$. In practise, it is possible to set $\epsilon = 0$, although this breaks continuity at $\boldsymbol{\mbv} = -\boldsymbol{z}$ unless $a = 0$. For the rest of content we set $\kappa = 1$ and omit it from the notation.

\section{Derivation of the normalizing term}
\label{sec:integral}
Consider an NASG component defined as $G(\boldsymbol{\mbv} ;[\boldsymbol{x,y,z}], \lambda, a, \epsilon)$. Since $G$ is rotation invariant, we assume that $\bm{x} = (1, 0, 0)$, $\bm{y} = (0, 1, 0)$, and $\bm{z} = (0, 0, 1)$.
Then the surface integral of the function $G$ over $S^2$ %the two-dimensional unit sphere $S^2 \subset \mathbb{R}^3$ 
is given by:

\begin{equation}\label{eq:use spherical coordinates}
    \begin{aligned}
        & \int_{S^2} G(\bm{\mbv} ;[\boldsymbol{x,y,z}], \lambda, a, \epsilon)d\omega \\							
        & = \int_0^{2\pi}d\phi \int_0^{\pi} \exp\left( 2\lambda\left(\frac{\cos\theta + 1}{2} \right)^{1+\epsilon+a\cos^2\phi} -2\lambda \right) \\
        & \hspace{40mm} \left(\frac{\cos\theta + 1}{2}\right)^{\epsilon+a\cos^2\phi} \sin\theta d\theta.
    \end{aligned}
\end{equation}

We compute the inner integral over $\theta$ first.
Consider the following change of variable (for fixed $\phi$):
\begin{equation}
    \begin{aligned}
        &\frac{\cos\theta + 1}{2} = \left(\frac{\cos\tau + 1}{2}\right)^{\frac{1}{1+\epsilon+a\cos^2\phi}} \\
        \Leftrightarrow \ &\cos\theta = 2\left(\frac{\cos\tau + 1}{2}\right)^{\frac{1}{1+\epsilon+a\cos^2\phi}} -1.
    \end{aligned}
\end{equation}
where $0 \leqslant \tau \leqslant \pi$.
Then
\begin{equation}
    \begin{aligned}
        -\sin\theta d\theta = \frac{2}{1+\epsilon+a\cos^2\phi}
        \left(\frac{\cos\tau + 1}{2}\right)^{\frac{1}{1+\epsilon+a\cos^2\phi}-1}
        \left(-\frac{\sin\tau}{2}\right)d\tau,
    \end{aligned}
\end{equation}
so
\begin{equation}
    \begin{aligned}
        \sin\theta d\theta
        = \frac{1}{1+\epsilon+a\cos^2\phi}\left(\frac{\cos\tau + 1}{2}\right)^{-\frac{\epsilon+a\cos^2\phi}{1 +\epsilon + a\cos^2\phi}}\sin\tau d\tau,
    \end{aligned}
\end{equation}
and the inner integral with respect to $\theta$ becomes:
\begin{equation}
    \begin{aligned}
        &\frac{1}{1+\epsilon+a\cos^2\phi}\int_0^\pi\exp(\lambda\cos\tau - \lambda)\sin\tau d\tau \\
        &= \frac{1}{1+\epsilon+a\cos^2\phi}\left[-\frac{\exp(\lambda\cos\tau - \lambda)}{\lambda}\right]_0^\pi \\
        &= \frac{1-e^{-2\lambda}}{\lambda(1+\epsilon+a\cos^2\phi)}.
    \end{aligned}
\end{equation}
Hence it follows that
\begin{equation}
    \begin{aligned}
        &\int_{S^2}G(\bm{\mbv} ;[\boldsymbol{x,y,z}], \lambda, a, \epsilon)d\omega \\
        &=\frac{1 - e^{-2\lambda}}{\lambda}\int_0^{2\pi}\frac{d\phi}{1+\epsilon+a\cos^2\phi} \\
        &=\frac{2(1-e^{-2\lambda})}{\lambda}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\frac{d\phi}{1+\epsilon+a\cos^2\phi}.
    \end{aligned}
\end{equation}
Now, consider the following change of variable:
\begin{equation}
    \begin{aligned}
        \tan\phi = \sqrt{\frac{1+\epsilon+a}{1+\epsilon}}\tan\rho.		
    \end{aligned}
\end{equation}
Then
\begin{equation}
    \begin{aligned}
        \frac{d\phi}{\cos^2\phi} = \sqrt{\frac{1+\epsilon+a}{1+\epsilon}}\frac{d\rho}{\cos^2\rho},	
    \end{aligned}
\end{equation}
where
\begin{equation}
    \begin{aligned}
        \cos^2\phi &= \frac{1}{1+\tan^2\phi}	\\
        &= \frac{1}{1+\frac{1+\epsilon+a}{1+\epsilon}\tan^2\rho} \\
        &= \frac{1+\epsilon}{\frac{1+\epsilon+a}{\cos^2\rho}-a} \\
        &= \frac{(1+\epsilon)\cos^2\rho}{1+\epsilon+a-a\cos^2\rho},
    \end{aligned}
\end{equation}
so
\begin{equation}
    \begin{aligned}
        d\phi = \frac{\sqrt{(1+\epsilon)(1+\epsilon+a)}}{1+\epsilon+a-a\cos^2\rho}d\rho.	
    \end{aligned}
\end{equation}
Hence it follows that
\begin{equation}\label{eq:K for general NASG}
    \begin{aligned}
        &\int_{S^2}G(\bm{v} ;[\boldsymbol{x,y,z}], \lambda, a, \epsilon)d\omega \\
        & =\frac{2(1-e^{-2\lambda})}{\lambda}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\frac{1}{1+\epsilon+\frac{(1+\epsilon)a\cos^2\rho}{1+\epsilon+a-a\cos^2\rho}}\cdot\frac{\sqrt{(1+\epsilon)(1+\epsilon+a)}}{1+\epsilon+a-a\cos^2\rho}d\rho \\
        & =\frac{2(1-e^{-2\lambda})}{\lambda}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\frac{d\rho}{\sqrt{(1+\epsilon)(1+\epsilon+a)}} \\
        & =\frac{2\pi(1-e^{-2\lambda})}{\lambda\sqrt{(1+\epsilon)(1+\epsilon+a)}}.
    \end{aligned}
\end{equation}
This expression reduces to \eqref{K for NASG} when $\epsilon=0$.

\section{Sampling NASG}
\label{sec:sampling}
Here, we discuss how to sample from the distribution on $S^2$ obtained by normalizing $G$ (cf.~\eqref{K for general NASG}). %with $\epsilon = 0$, 
To this end, we essentially reverse the discussions in the previous section.
Define two functions
\begin{equation}
    \begin{aligned}
        &\Phi_E : [e^{-2\lambda}, 1] \times \left[-\frac{\pi}{2},\frac{\pi}{2}\right] 
        \rightarrow [0, \pi] \times \left[-\frac{\pi}{2},\frac{\pi}{2}\right], \\
        &\Phi_W : [e^{-2\lambda}, 1] \times \left[-\frac{\pi}{2}, \frac{\pi}{2}\right] 
        \rightarrow [0, \pi] \times \left[\frac{\pi}{2},\frac{3\pi}{2}\right]
    \end{aligned}
\end{equation}
by
\begin{equation}
    \begin{aligned}
        &\Phi_E(s, \rho) \\
        & =\left( \arccos\left(2\left(\frac{\log s}{2\lambda} + 1\right)^{\frac{1 + \epsilon+a - a\cos^2\rho}{(1+\epsilon)(1+\epsilon+a)}} - 1\right), \arctan\left(\sqrt{\frac{1+\epsilon+a}{1+\epsilon}}\tan\rho\right) \right) ,\\
        &\Phi_W(s, \rho) = \Phi_E(s, \rho) + (0, \pi),
    \end{aligned}
\end{equation}
where $(s, \rho) \in [e^{-2\lambda}, 1]\times[-\frac{\pi}{2}, \frac{\pi}{2}]$ (here, "E" and "W" stand for east and west, respectively). 

In practice, we just need to sample two uniform values $(\xi_0, \xi_1) \in [0,1]^2$, and linearly map them to $[e^{-2\lambda}, 1]\times[-\frac{\pi}{2}, \frac{\pi}{2}]$ to obtain $(s, \rho)$ used in the above equation.
%The resulting values of $\Phi_E$ and $\Phi_W$ are the sampled directions' $(\theta, \phi)$.
We introduce another uniform random number $\xi_2 \in [0,1]$.
When $\xi_2 > 0.5$, we sample the eastern hemisphere and $\Phi_E(s,\rho)$ is the sampled direction's $(\theta, \phi)$. When $\xi_2\leqslant 0.5$, we instead use $\Phi_W(s,\rho)$.

%\textbf{Proof}: 
Let us now argue that the above sampling method serves our purpose.
Observe that both $\Phi_E$ and $\Phi_W$ are bijections. Their inverses are given by:
\begin{equation}
    \begin{aligned}
        &\Phi_E^{-1}(\theta, \phi) \\
        &= \left(\exp\left(2\lambda\left(\frac{\cos\theta+1}{2}\right)^{1+\epsilon+a\cos^2\phi}-2\lambda\right), \arctan\left(\sqrt{\frac{1+\epsilon}{1+\epsilon+a}}\tan\phi\right)\right) , \\
        &\Phi_W^{-1}(\theta, \phi) = \Phi_E^{-1}(\theta, \phi - \pi),
    \end{aligned}
\end{equation}
where $(\theta, \phi) \in [0, \pi]\times[-\frac{\pi}{2}, \frac{\pi}{2}]$ for the former, and $(\theta, \phi) \in [0, \pi]\times[\frac{\pi}{2}, \frac{3\pi}{2}]$ for the latter.
The Jacobian for $\Phi_E^{-1}$ is computed as:
\begin{equation}
    \begin{aligned}
        &J_{\Phi_E^{-1}}(\theta, \phi)  \\
        &= \exp\left(2\lambda\left(\frac{\cos\theta + 1}{2}\right)^{1+\epsilon+a\cos^2\phi} - 2\lambda\right)	\\
        &\times 2\lambda(1 + \epsilon+a\cos^2\phi)\left(\frac{\cos\theta+1}{2}\right)^{\epsilon+a\cos^2\phi}\left(-\frac{\sin\theta}{2}\right) \\
        &\times \frac{1}{1 + \frac{1+\epsilon}{1+\epsilon+a}\tan^2\phi}\sqrt{\frac{1+\epsilon}{1+\epsilon+a}}\frac{1}{\cos^2\phi} \\
        &= \lambda \sqrt{(1+\epsilon)(1+\epsilon+a)}\exp\left(2\lambda\left(\frac{\cos\theta+1}{2}\right)^{1+\epsilon+a\cos^2\phi} - 2\lambda\right) \\
        & \times \left(\frac{\cos\theta + 1}{2}\right)^{\epsilon+a\cos^2\phi}(-\sin\theta).
    \end{aligned}
\end{equation}

Let $X$ be a random variable on $S^2$, which we also view as a function of $(\theta,\phi)$.
Then, the expected value of $X$ over the points $\Phi_E(s,\rho)$, where the $(s,\rho)$ are uniformly sampled from $[e^{-2\lambda}, 1] \times [-\frac{\pi}{2}, \frac{\pi}{2}]$, is given by:

%We Importance sample $X$ as follows:
%\begin{equation}
%    \begin{aligned}
%        \mathbb{E}\left[\frac{X\circ\Phi_E}{2}\right] + \mathbb{E}\left[\frac{X\circ\Phi_W}{2}\right],
%    \end{aligned}
%\end{equation}
%where we sample from $(e^{-2\lambda}, 1) \times (-\frac{\pi}{2}, \frac{\pi}{2})$ following the uniform distribution.
%Then we have
\begin{equation}
    \begin{aligned}
        \mathbb{E}\left[X\circ\Phi_E\right] 
        = \frac{1}{\pi(1-e^{-2\lambda})}\int_{[e^{-2\lambda}, 1]\times[-\frac{\pi}{2}, \frac{\pi}{2}]} X \circ \Phi_E(s, \rho)dsd\rho.
    \end{aligned}
\end{equation}
%\begin{equation}
%    \begin{aligned}
%        \mathbb{E}\left[\frac{X\circ\Phi_E}{2}\right] 
%        = \frac{1}{2\pi(1-e^{-2\lambda})}\int_{(e^{-2\lambda}, 1)\times(-\frac{\pi}{2}, \frac{\pi}{2})} X \circ \Phi_E(s, \rho)dsd\rho.
%    \end{aligned}
%\end{equation}
By taking the change of variables $(s, \rho) = \Phi_E^{-1}(\theta, \phi)$, we have:
\begin{equation}
    \begin{aligned}
        &\mathbb{E}\left[X\circ\Phi_E\right] \\
        &= \frac{1}{\pi(1 - e^{-2\lambda})}\int_{[0, \pi]\times[-\frac{\pi}{2}, \frac{\pi}{2}]}X(\theta, \phi)\left|J_{\Phi_E^{-1}}(\theta, \phi)\right|d\theta d\phi \\
        &= \frac{\lambda\sqrt{(1+\epsilon)(1+\epsilon+a)}}{\pi(1-e^{-2\lambda})} \\
        & \int_{[0, \pi]\times[-\frac{\pi}{2}, \frac{\pi}{2}]}X(\theta, \phi) 
         \exp\left(2\lambda\left(\frac{\cos\theta+1}{2}\right)^{1+\epsilon+a\cos^2\phi} - 2\lambda\right) \\
        & \hspace{37mm} \left(\frac{\cos\theta+1}{2}\right)^{\epsilon+a\cos^2\phi}\sin\theta d\theta d\phi.
    \end{aligned}
\end{equation}
%\begin{equation}
%    \begin{aligned}
%        &\mathbb{E}\left[\frac{X\circ\Phi_E}{2}\right] \\
%        &= \frac{1}{2\pi(1 - e^{-2\lambda})}\int_{(0, \pi)\times(-\frac{\pi}{2}, \frac{\pi}{2})}X(\theta, \varphi)\left|J_{\Phi_E^{-1}}(\theta, \varphi)\right|d\theta d\varphi \\
%        &= \frac{\lambda\sqrt{(1+\epsilon)(1+\epsilon+a)}}{2\pi(1-e^{-2\lambda})} \\
%        & \int_{(0, \pi)\times(-\frac{\pi}{2}, \frac{\pi}{2})}X(\theta, \varphi) 
%         \exp\left(2\lambda\left(\frac{\cos\theta+1}{2}\right)^{1+\epsilon+a\cos^2\varphi} - 2\lambda\right) \\
%        & \hspace{37mm} \left(\frac{\cos\theta+1}{2}\right)^{\epsilon+a\cos^2\varphi}\sin\theta d\theta d\varphi.
%    \end{aligned}
%\end{equation}
We also obtain a similar expression for $\mathbb{E}\left[X\circ\Phi_W\right]$.
Since we choose each of the eastern and western hemispheres with probability $\frac{1}{2}$ according to the values of $\xi_2$, the overall expected value becomes:
\begin{equation}
    \begin{aligned}
        \frac{1}{2}&\mathbb{E}\left[X\circ\Phi_E\right] +\frac{1}{2} \mathbb{E}\left[X\circ\Phi_W\right] \\
        &= \frac{\lambda\sqrt{(1+\epsilon)(1+\epsilon+a)}}{2\pi(1-e^{-2\lambda})}
        \int_{S^2}X(\bm{v})G(\bm{v} ;[\boldsymbol{x,y,z}], \lambda, a, \epsilon)d\omega.
    \end{aligned}
\end{equation}
%\begin{equation}
%    \begin{aligned}
%        \mathbb{E}\left[\frac{X\circ\Phi_E}{2}\right] + \mathbb{E}\left[\frac{X\circ\Phi_W}{2}\right] 
%        = \frac{\lambda\sqrt{(1+\epsilon)(1+\epsilon+a)}}{2\pi(1-e^{-2\lambda})}
%        \int_{S^2}X(\bm{v})G(\bm{v})d\omega.
%    \end{aligned}
%\end{equation}
See \eqref{use spherical coordinates}.
It follows that our sampled directions obey the distribution obtained by normalizing $G$, as desired.
	
% \clearpage
% \clearpage
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-bibliography}
\end{document}
