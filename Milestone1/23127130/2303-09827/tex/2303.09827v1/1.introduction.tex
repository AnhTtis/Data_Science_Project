\section{Introduction}

Understanding the user’s intent plays an important role in task-oriented dialogue (TOD) systems. Traditionally, understanding the user’s intent requires supervised training using intent-annotated dialogue datasets \cite{xu2013understanding, wang2015mining,  kim2017onenet, goo2018slot}. However, for new emerging domains and services, defining the intent set is challenging and also requires an expert’s knowledge. Therefore, finding an automatic method that identifies intents from raw conversational data is desirable to reduce costs.




The intent clustering task of the 11th Dialog System Technology Challenge (DSTC11) aims to provide a realistic benchmark for the intent induction problem. This track evaluates automatic customer intent induction methods from dialogues between human agents and customers, and the DSTC11 challenge participants are required to create a set of intent labels based on the conversations. To provide a realistic setting, the number of intents and domain of the test set are not provided until the development phase ends.

In this paper, we introduce an automatic intent induction framework that effectively utilizes a public TOD dataset. First, we fine-tuned the language model with multi-domain TOD datasets so that it has a domain-robust semantic representation. Here, we extract verbs and object from utterance to remove the artifacts of unnecessary information. For the training, we applied supervised contrastive learning (SCL), which is known to be stable in language model fine-tuning  \cite{gunel2020supervised}. Second, to infer a new intent set from the unseen domain dataset, we applied a clustering technique that groups the utterances based on the fine-tuned embedding model’s representation. Furthermore, we generate a label name for each cluster to obtain an interpretable result. The cluster label generation method could reduce the effort of examining each set manually to understand the clustering results.

In the experiment with the test dataset (finance and banking domain), we achieved 3rd place in terms of precision and demonstrated superior accuracy (ACC) and a higher normalized mutual information (NMI) score than the baseline. Furthermore, the generated clustering labels reasonably explain each cluster. Finally, we analyzed our model with comparable options and demonstrated the result on development, test, and TOD datasets. We named our framework DORIC, which means \textbf{DO}main \textbf{R}obust fine-tuning for open \textbf{I}ntent \textbf{C}lustering through dependency parsing.
