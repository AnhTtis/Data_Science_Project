\section{Method}

\begin{figure*}
\centering
\label{figure1}
\includegraphics[width=\textwidth]{img/pic1.png}
\caption{Fine-tuning and inference method of our proposed methods. }
\label{fig:method}
\end{figure*}



\subsection{Semantic representation}
With the advance of the pre-trained langue model, leveraging these models for embedding dialogue has exhibited promising results \cite{ham2020end, lin2020mintl, yang2021ubar,lee2022sf}. Following their success, we utilize the pre-trained SBERT \cite{reimers2019sentence} as a backbone model; SBERT has a siamese network structure and performs well in classification \cite{reimers2019sentence}, summarization \cite{zhong2020extractive}, and intent clustering \cite{liu2021open}.

The SBERT model is pre-trained on a written form text dataset that has a different linguistic pattern with the dialogue utterances. This difference could hurt the accuracy on dialogue related tasks \cite{wu2020tod}. Therefore, we fine-tuned the model with the multi-domain public task-oriented dialogue dataset MultiWOZ 2.2 \cite{eric2019multiwoz}. This dataset has nine intents types, and we fine-tuned the model to learn the embedding of utterances according to intent type. In this data, intents are spanned multiple turns, and some utterances contain multiple intents in one utterance. To clarify the match between the utterance and the intent label, we used only the first utterance of the spanned intent dialogue and excluded utterances that contained multiple intents. We analyze the processed fine-tuning dataset in Table~\ref{tab:mwoz}.


\begin{table}[h]
\centering
\small
\begin{tabular}{ll}
\hline
\textbf{Intent} & \textbf{Count} \\
\hline
FindRestaurants   & 3561  \\
SearchHotel   & 3375  \\
FindTrains   & 3262  \\
FindAttractions   & 2795  \\
ReserveHotel   & 1951  \\
GetTrainTickets   & 1926  \\
ReserveRestaurant   & 1600  \\
GetRide   & 1262  \\
FindPolice   & 229  \\
 \hline
Total & 19961   \\
\hline
\end{tabular}
\caption{Type and number of intents of the fine-tuning dataset.}
\label{tab:mwoz}
\end{table}


\begin{table}[h]
\centering
\small
\begin{tabular}{l|ll}
\hline
\textbf{Inference data} & \textbf{NMI}& \textbf{ACC} \\ \hline
Verb-Object & 41.89 & 30.79 \\
Sentence   & \textbf{65.16} & \textbf{56.68} \\
\hline
\end{tabular}
\caption{The comparison of using whole sentence and Verb-Object format in inference. NMI and accuracy result on DSTC11 development are reported.}
\label{tab:pre}
\end{table}


As we aim for unsupervised intent induction, domain robust fine-tuning is crucial to identify the intents across the domain. To do so, we extract Verb-Object structure from the utterance using the dependency parser\footnote{https://spacy.io/}. This additional pre-process removes the effect of non-relevant words or utterance styles when fine-tuning the SBERT model. However, at inference time, we used whole utterances for clustering as preliminary experiments demonstrated better results (Table~\ref{tab:pre}). We demonstrate our overall method in Figure~\ref{fig:method}.

\subsection{Supervised contrastive learning}

\begin{figure}
\centering
\label{figure1}
\begin{tabular}{cc}
\small{Before training} & \small{After training} \\

\includegraphics[width=0.2\textwidth]{img/before_training.png}& \includegraphics[width=0.2\textwidth]{img/after_training.png}\\
\end{tabular}
\caption{Visualization of Multiwoz 2.2 test dataset. The left is utterance representation before training, and the right is after training.}
\label{fig:training}
\end{figure}
% TODO : remove tiny number

Recently, there have been several successful studies using contrastive learning (CL) in the computer vision and language domain \cite{chen2020simple, liu2020hybrid, wu2018unsupervised, gunel2020supervised} and CL shows more generalize and robustness to language model training than cross-entropy loss \cite{gunel2020supervised}. Following their success, we utilize supervised contrastive learning (SCL) \cite{khosla2020supervised} in fine-tuning.

SCL is a modified version of the CL approach, which utilize the label information. In CL, only the anchor and its augmented object are regarded as positive objects, and others are used as negative object in training. However, SCL set the same label objects as positive and others as negative, so more accurate embedding representation learning is possible. 


For the $N$ randomly sampled object ${\{x_k, y_k\}_{k=1...N}}$, mini batch used for training is consists of $2N$ pair,${\{\tilde{x}_k, \tilde{y}_k\}_{k=1...2N}}$. In ${\{\tilde{x}_k, \tilde{y}_k\}_{k=1...2N}}$, ${\{\tilde{x}_k, \tilde{y}_k\}_{k=1...N}}$ is same as original sampled ${\{x_k, y_k\}_{k=1...N}}$ and ${\{\tilde{x}_k, \tilde{y}_k\}_{k=N...2N}}$ is augmented pair of original sampled data. Word-net \cite{miller1995wordnet} based synonym augmentation\footnote{https://github.com/makcedward/nlpaug} is used for augmentation. $\Phi$ denotes the SBERT encoder and $\tau$ is scalar temperature parameter to adjust the separation strength. The overall loss is given by the following equations, and visualization of the training results are shown in Figure~\ref{fig:training}:

\begin{equation}
\label{eq:1}
\small
    \mathcal{L}^{sup}=\sum_{i=1}^{2N}\mathcal{L}_{i}^{sup}
\end{equation}

\begin{equation}
\label{eq:2}
\small
\begin{split}
{L}_{i}^{sup}={} & -\frac{1}{2N_{\tilde{y_i}}-1} \sum_{j=1}^{2N} \mathbf{1}_{i\ne j}\cdot\mathbf{1}_{\tilde{y_i}=\tilde{y_j}}\cdot\\    
    &  \log \frac{\exp(\Phi_i\cdot \Phi_j/\tau)}{\sum\nolimits_{k=1}^{2N}\mathbf{1}_{i\ne k}\exp (\Phi_i\cdot \Phi_j/\tau)}
\end{split}
\end{equation}

\subsection{Intent label generation}
\label{label_generation}
To improve the explainability of the clustering results, we automatically generated the semantic labels from the clustering results. Following the intent datasets, which usually represent the intent name as a verb and object pair \cite{zang2020multiwoz, coucke2018snips, rastogi2020schema}, we also named our induced clusters as Verb-Object forms using a dependency parser. In the previous method, \citet{liu2021open} counted the common Verb-Object pairs in the cluster and used the most common pair as the cluster name. However, this method did not create a proper label when detailed words appeared in the object. For example, a pair of \code{call-son} and \code{call-daughter} cannot be grouped as \code{call-child} using the previous method. 

To overcome this limitation, we propose a method that uses the object wordâ€™s hypernyms. By adopting hypernyms, we could obtain a proper word containing detailed information. More precisely, we generate \textit{verb-hyper(object)} and \textit{verb-hyper(hyper(object))} pairs from existing Verb-Object pairs and calculate the most common pair from this generated result. We employ this rule when the number of the most common pair and second place does not differ by more than $\alpha$ times, and we set $\alpha$ to 2.0 in the experiment. Word-net \cite{miller1995wordnet} is used to get the hypernyms.