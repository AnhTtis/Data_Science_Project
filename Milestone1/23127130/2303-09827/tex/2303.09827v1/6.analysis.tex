
\section{Analysis}
\subsection{Verb-Object structure in fine-tuning}
\begin{table}[h!]
\centering
\small
\begin{tabular}{ll|ll}
\hline
\textbf{Method} & \textbf{Dataset (domain)} & \textbf{NMI}& \textbf{ACC} \\ \hline

\multicolumn{4}{c}{\textit{Different Domain with Fine-tuning Dataset}} \\ \hline


Sentence & Dev (Insurance)  & 62.13 & 55.35 \\
Verb-Obj    & Dev (Insurance)          & \textbf{65.16} & \textbf{56.68} \\
\hline
Sentence & Test1 (Banking)     & 68.91 & \textbf{53.22} \\
Verb-Obj     & Test1 (Banking)      & \textbf{71.02} & 52.35 \\
 \hline
 
 
Sentence & Test2 (Finance)              & 64.94 & \textbf{67.07} \\
Verb-Obj     & Test2 (Finance)  & \textbf{69.64} & 65.14 \\
 \hline

\multicolumn{4}{c}{\textit{Same Domain with Fine-tuning Dataset}} \\ \hline
Sentence & SGD (Tourism)  & 65.24 & 68.35 \\
Verb-Obj     & SGD (Tourism) & \textbf{65.32} & \textbf{68.36} \\
\hline
\end{tabular}
\caption{The NMI and accuracy result on DSTC11 development, test, and SGD dataset according to fine-tuning utterance format.}
\label{tab:extract}
\end{table}

To examine the effect of extracting Verb-Object structures from the sentence, we compare our proposed method with methods that use the whole sentence during the fine-tuning stage (Table~\ref{tab:extract}). Using the Verb-Object structure demonstrates superior NMI results in both different-domain and same-domain environments; this result indicates that fine-tuning with Verb-Object information has helped reduce the clustering uncertainty. However, the accuracy doesn't significantly differ between the Verb-Object form and the whole sentence form in the tourism domain, which is identical to the fine-tuning dataset domain.


\subsection{Analysis of loss}

\begin{table}[h]
\centering
\small
\begin{tabular}{ll|ll}
\hline
\textbf{Loss function} & \textbf{Dataset (domain)} & \textbf{NMI}& \textbf{ACC} \\ \hline

\multicolumn{4}{c}{\textit{Different Domain with Fine-tuning Dataset}} \\ \hline
Cross Entropy & Dev (Insurance)  & 61.98 & 53.69 \\
SCL    & Dev (Insurance)          & \textbf{65.16} & \textbf{56.68} \\
\hline
Cross Entropy & Test1 (Banking)              & 67.67 & 52.16 \\
SCL   & Test1 (Banking)    & \textbf{71.02} & \textbf{52.35} \\
\hline
Cross Entropy & Test2 (Finance)     & 64.09 & 63.07 \\
SCL   & Test2 (Finance)    & \textbf{69.64} & \textbf{65.14} \\
\hline
 
 
\multicolumn{4}{c}{\textit{Same Domain with Fine-tuning Dataset}} \\ \hline
Cross Entropy & SGD (Tourism) & 64.11 & \textbf{70.31} \\
SCL   & SGD (Tourism) & \textbf{65.32} & 68.36 \\
\hline



\end{tabular}
\caption{The NMI and accuracy result on DSTC11 development, test, and SGD dataset according to the loss function.}
\label{tab:class_contrast}
\end{table}

To investigate the effect of SCL during fine-tuning, we compare the result with the cross-entropy loss in Table~\ref{tab:class_contrast}. In most cases, the SCL loss demonstrates better results by a large margin; however, on the SGD dataset, the NMI and ACC results were slightly or no different than the cross-entropy loss. Considering that the SGD dataset is the only dataset with the same domain with the fine-tuning dataset (tourism), this result indicates that SCL is more useful when it is used in a domain-across environment.
