% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}
\usepackage{graphicx}

% Standard package includes
\usepackage{times}
\usepackage{anyfontsize}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{latexsym}
\newcommand{\code}[1]{{\texttt{#1}}}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{DORIC : Domain Robust Fine-Tuning for Open Intent Clustering through Dependency Parsing}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Seungyeon Seo \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Seungyeon Seo \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Seungyeon Seo \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}


\author{Jihyun Lee$^1$, Seungyeon Seo$^1$, Yunsu Kim$^{1,2}$, Gary Geunbae Lee$^{1,2}$ \\
  $^1$Graduate School of Artificial Intelligence, POSTECH, Republic of Korea\\
  $^2$Department of Computer Science and Engineering, POSTECH, Republic of Korea\\
  \texttt{\{jihyunlee, ssy319, yunsu.kim, gblee\}@postech.ac.kr} \\
}


\begin{document}
\maketitle
\begin{abstract}

We present our work on Track 2 in the Dialog System Technology Challenges 11 (DSTC11). DSTC11-Track2 aims to provide a benchmark for zero-shot, cross-domain, intent-set induction. In the absence of in-domain training dataset, robust utterance representation that can be used across domains is necessary to induce users' intentions. To achieve this, we leveraged a multi-domain dialogue dataset to fine-tune the language model and proposed extracting Verb-Object pairs to remove the artifacts of unnecessary information. Furthermore,  we devised the method that generates each cluster's name for the explainability of clustered results. Our approach achieved 3rd place in the precision score and showed superior accuracy and normalized mutual information (NMI) score than the baseline model on various domain datasets. 
\end{abstract}

\input{1.introduction}
\input{2.related_work}
\input{3.intent_clustering}
\input{4.method}
\input{5.experiment}
\input{6.analysis}

\section{Conclusion}
In this paper, we describe our solution for the DSTC11 intent induction competition. We leveraged the SBERT model to embed sentences and fine-tuned the model using dependency parsing results. Additionally, we used supervised contrastive loss during fine-tuning to make the model robust in multiple domains. During the analysis, both dependency parsing and SCL helped to make the intent induction model more domain robust. Furthermore, our intent label generation with hypernym methods allows us to explain the clustering results. According to the results, our approach achieved 3rd place in terms of the precision score and demonstrated better NMI and accuracy compared to the baseline model.


\section*{Limitations}
Our contribution has two limitations. First, although DORIC shows superior performance in the domain across the environment, the increase was insignificant in the same domain environment. Second, we thoroughly examine the embedding methods, but we adapt this method only to the K-means clustering. In the future, we plan to devise a progressed clustering method that fits our embedding method.


\section*{Acknowledgements}
% should rewrite this

This work was partly supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2019-0-01906, Artificial Intelligence Graduate School Program(POSTECH)) and  ITRC(Information Technology Research Center) support program(IITP-2023-2020-0-01789).




% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom,jihyun}
\bibliographystyle{acl_natbib}

\appendix
\label{sec:appendix}
\input{appendix}
\end{document}




% % Entries for the entire Anthology, followed by custom entries
% \bibliography{anthology,custom}
% \bibliographystyle{acl_natbib}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is a section in the appendix.

% \end{document}
