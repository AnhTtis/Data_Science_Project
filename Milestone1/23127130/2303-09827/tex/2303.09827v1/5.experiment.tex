\section{Experiment}

\begin{figure*}
\centering
\begin{tabular}{cccc}
\hline
Dev (Insurance) & Test1 (Banking) &  Test2 (Finance) & SGD (Tourism)\\
\hline
\multicolumn{4}{c}{\textit{Baseline}} \\ \hline
\includegraphics[width=0.2\textwidth]{img/baseline/insurance.png}& 
\includegraphics[width=0.2\textwidth]{img/baseline/banking.png}&
\includegraphics[width=0.2\textwidth]{img/baseline/finance.png}&
\includegraphics[width=0.2\textwidth]{img/baseline/SGD2.png}\\
\hline
\multicolumn{4}{c}{\textit{DORIC}} \\ \hline
\includegraphics[width=0.2\textwidth]{img/Ours/insurance.png}&
\includegraphics[width=0.2\textwidth]{img/Ours/banking.png}&
\includegraphics[width=0.2\textwidth]{img/Ours/finance.png}&
\includegraphics[width=0.2\textwidth]{img/Ours/SGD2.png}\\
\hline

\end{tabular}
\caption{Visulization on dev (insurance), test1 (banking), test2 (finance) and SGD (tourism) dataset.}
\label{fig:base_ours}
\end{figure*}


\subsection{Experimental setup}
\textbf{Dataset}
\noindent
To demonstrate the performance of our model, we used the development (dev) (insurance), test1 (banking), and test2 (finance) datasets. These datasets have domains that are different from the fine-tuning dataset (tourism), so we were able to examine our method's effectiveness in diverse domains. Additionally, we used the Schema-Guided Dialogue Dataset (SGD) dataset; we extracted tourism-related domains from the SGD dataset to make the same domain environment with fine-tuning dataset. The number of intents for each dataset is shown in Table~\ref{tab:dataset}.

\begin{table}[h]
\centering
\small
\begin{tabular}{lll}
\hline
\textbf{Dataset} & \textbf{Domain} & \textbf{\# of intents}\\ \hline
Dev  & Insurance & 22\\
Test1 & Banking & 29\\
Test2 & Fianace & 39\\
SGD & Tourism & 6\\

\hline


\end{tabular}
\caption{Domain and number of intents type for each dataset.}
\label{tab:dataset}
\end{table}

\noindent
\textbf{Evaluation} 
NMI and accuracy were the primary metrics used for the evaluation, and to provide additional metrics, precision was also used. The higher NMI value denotes that clustering has reduced more entropy. 1:1 alignments between the induced intents and the gold intents were computed by the Hungarian algorithm \cite{kuhn1955hungarian}.\\

\noindent\textbf{Setup} 
We employ the pre-trained SBERT \cite{reimers2019sentence} for the baseline embedding model. The pre-trained parameters were from the huggingface \cite{wolf2019huggingface} \textit{all-mpnet-base-v2} version. In the SCL function, we set the $\tau$ as 0.07 and trained a maximum of five epochs with early stopping. In the K-means clustering, we set the minimum cluster number as five and the max cluster number as 50 and use silhouette score for comparing the clustered result, which is based on tightness and separation \cite{rousseeuw1987silhouettes}.

\subsection{Intent clustering result}

\begin{table}[h]
\centering
\small

\begin{tabular}{ll|lll}

\hline
\textbf{Model} & \textbf{Data} & \textbf{NMI}& \textbf{ACC} & {\fontsize{8}{7}\textbf{Precision}} \\ \hline

\multicolumn{5}{c}{\textit{Different Domain with Fine-tuning Dataset}} \\ \hline
Baseline & Dev(Insurance)                & 59.31 & 46.14 &65.98 \\
DORIC   & Dev(Insurance)                 & \textbf{65.16} & \textbf{56.68}  &\textbf{67.63}\\
\hline
Baseline & Test1(Banking) & 65.71 & 51.85   &60.68\\
DORIC   & Test1(Banking)   & \textbf{71.02} & \textbf{52.35}  & \textbf{73.92}\\
  \hline
Baseline & Test2(Finance)         & 60.26 & 59.75  &69.25 \\
DORIC   &  Test2(Finance)           & \textbf{69.64} & \textbf{65.14} &\textbf{75.14} \\
 \hline
\multicolumn{5}{c}{\textit{Same Domain with Fine-tuning Dataset}} \\ \hline
Baseline & SGD(Tourism)  & 60.54 & 63.67  &49.90\\
DORIC   & SGD(Tourism) & \textbf{65.32} & \textbf{68.36}  &\textbf{51.00}\\

\hline

\end{tabular}


\caption{Comparison of baseline and DORIC in different dataset. NMI, ACC and Precision are reported.}
\label{tab:main_result}
\end{table}
The results of DORIC after evaluation on the dev (insurance), test1 (finance), test2 (banking), and SGD (tourism) datasets are shown in Table~\ref{tab:main_result}. These results show that our model outperforms the baseline model in terms of the NMI, ACC, and precision on all datasets. Except for the SGD dataset, the datasetâ€™s domains are all different from the fine-tuning dataset MultiWOZ2.2 (tourism), which demonstrates that our intent induction framework is robust to diverse domain datasets. The visualization of experimental results in Figure~\ref{fig:base_ours} also exhibits the aligned result with Table~\ref{tab:main_result}; compared to the baseline, DORIC embeds utterances with the same label at a closer distance.

\subsection{Intent label generation with hypernym}
\begin{table}[]
\centering
\small
{
\begin{tabular}{lll}
\hline

\textbf{Idx} & \textbf{Generated name}& \textbf{Ground-truth}    \\ \hline
0               & create-account  & CreateAccount             \\
1 & cancel-billing & CancelAutomaticBilling    \\
2\dag & \textbf{add-child} & AddDependent \\
3 & report-accident & ReportAutomobileAccident\\
4 & change-address & ChangeAddress\\
5\dag & get-quote & GetQuote\\
6 & change-plan & ChangePlan\\
7 & file-claim & FileClaim \\
8  & pay-bill & PayBill\\
9 & check-balance & CheckAccountBalance\\
10 & change-question & ChangeSecurityQuestion\\
11 & cancel-plan & CancelPlan\\
\hline
\multicolumn{3}{c}{\textit{Without hypernym}} \\ 
\hline
2 & \textbf{add-son} & AddDependent \\
5 & get-quote & GetQuote\\
\hline
\end{tabular}%
}
\caption{Example of generated intent labels and ground truth. Cluster name with \dag means using hypernym.}
\label{tab:label_generation}
\end{table} 

Table~\ref{tab:label_generation} shows examples of the generated intent labels, and cluster with \dag denotes the clusters with the hypernyms following section~\ref{label_generation}. As shown in the table, our proposed method successfully explains the cluster results compared to the ground truth label. Furthermore, using hypernyms enables the grouping of detailed information in the cluster. For instance, Cluster 2 obtains a more comprehensive label, \code{add-child} than \code{add-son} by using the hypernyms. We also present the more detailed results for the dev and test data in Appendix A.1.

