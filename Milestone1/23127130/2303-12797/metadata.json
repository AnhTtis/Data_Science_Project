{
    "arxiv_id": "2303.12797",
    "paper_title": "An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters",
    "authors": [
        "Julie Keisler",
        "El-Ghazali Talbi",
        "Sandra Claudel",
        "Gilles Cabriel"
    ],
    "submission_date": "2023-02-27",
    "revised_dates": [
        "2023-03-24"
    ],
    "latest_version": 1,
    "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12797v1"
    ],
    "publication_venue": null
}