\section{Conclusion and Future Work} \label{part 6}

In this work, we introduced a novel algorithmic framework for the joint optimization of DNNs architectures and hyperparameters. We first introduced a search space based on Directed Acyclic Graphs, highly flexible for the architecture, and also allow for fine-tuning of hyperparameters. Based on this search space we designed search operators compatible with any metaheuristic able to handle a mixed and variable-size search space. The algorithmic framework is generic and has been efficient on the time series forecasting task using an evolutionary algorithm. 

Further work would be dedicated to the investigation of other metaheuristics (e.g. swarm intelligence, simulated annealing) to evolve the DAGs. The reformulation of the studied optimization problems by including multiple objectives (e.g. complexity of DNNs) and robustness represent also important perspectives. To further improve the performance on time series forecasting tasks we can develop a more complete pipeline including a features selection strategy.

We can imagine further research work testing our framework on different learning tasks. Considering the forecasting task, the output models show that combining different state-of-the-art DNN operations within a single model is an interesting lead to improve the models' performance. Such models are quite innovative within the deep learning community and studies on their conduct and performances could be carried out.