%**********************************
\section{Search space definition} \label{part 3}
%**********************************

The development of a complete optimization framework for AutoDL needs the definition of the search space, the objective function and the search algorithm. In this section, the handled optimization problem is formulated. Then, the search space and its characteristics are detailed.

%*************************************************
\subsection{Optimization problem formulation}
%*************************************************

Our optimization problem consists in finding the best possible DNN for a given time series forecasting problem. To do so, we introduce an ensemble $\Omega$ representing our search space, which contains all considered DNNs. We then consider our time series dataset $\mathcal{D}$. For any subset $\mathcal{D}_0 = (X_0, Y_0)$, we define the forecast error $\ell$ as:

\begin{align*}
  \ell \colon & \Omega \times \mathcal{D} \to \mathbb{R}\\
  &f \times \mathcal{D}_0 \mapsto \ell\big(f(\mathcal{D}_0)\big) = \ell\big(Y_0, f(X_0)\big).
\end{align*}

The explicit formula for $\ell$ will be given later in the paper. Each element $f$ from $\Omega$ is a DNN defined as an operator parameterized by three parameters. First, its architecture $\alpha \in \mathcal{A}$. $\mathcal{A}$ is the search space of all considered architectures and will be detailed in Subsection \ref{part sp archi}. Given the DNN architecture $\alpha$, the DNN is then parameterized by its hyperparameters $\lambda \in {\Lambda(\alpha)}$, with $\Lambda(\alpha)$ the search space of the hyperparameters induced by the architecture $\alpha$ and defined Subsection \ref{part sp hp}. Finally, $\alpha$ and $\lambda$ generate an ensemble of possible weights $\Theta(\alpha, \lambda)$, from which the DNN optimal weights $\theta$ are found by gradient descent when training the model. The architecture $\alpha$ and the hyperparameters $\lambda$ are optimized by our framework.

We consider the multivariate time series forecasting task. Our dataset $\mathcal{D} = (X, Y)$ is composed of a target variable $Y = \{\mathbf{y}_t\}_{t=1}^{T}$, with $\mathbf{y}_t \in \mathbb{R}^{N}$ and a set of explanatory variables (features) $X = \{\mathbf{x}_t\}_{t=1}^{T}$, with $\mathbf{x}_t \in \mathbb{R}^{F_1 \times F_2}$. The size of the target $Y$ at each time step is $T$ and $F_1$, $F_2$ are the shapes of the input variable $X$ at each time step. We choose to represent $\mathbf{x}_t$ by a matrix to extend our framework's scope, but it can equally be defined as a vector by taking $F_2 = 1$. The framework can be applied to univariate signals by taking $T=1$. We partition our time indexes into three groups of successive time steps and split accordingly $\mathcal{D}$ into three datasets: $\mathcal{D}_{train}$, $\mathcal{D}_{valid}$ and $\mathcal{D}_{test}$.

After choosing an architecture $\alpha$ and a set of hyperparamaters $\lambda$, we build the DNN $f^{\alpha, \lambda}$ and use $\mathcal{D}_{train}$ to train $f^{\alpha, \lambda}$ and optimize its weights $\theta$ by stochastic gradient descent:

\begin{center}
    $\hat{\theta} \in \underset{\theta \in \Theta(\alpha, \lambda)}{\argmin}\big(\ell(f_{\theta}^{\alpha, \lambda}, \mathcal{D}_{train})\big)$.
\end{center}

The forecast error of the DNN parameterized by $\hat{\theta}$ on $\mathcal{D}_{valid}$ is used to assess the performance of the selected $\alpha$ and $\lambda$. The best architecture and hyperparameters are optimized by solving:

\begin{center}
    $(\hat{\alpha}, \hat{\lambda}) \in \underset{\alpha \in \mathcal{A}}\argmin\Big(\underset{\lambda \in \Lambda(\alpha)}\argmin\big(\ell(f_{\hat{\theta}}^{\alpha, \lambda},\mathcal{D}_{valid})\big)\Big)$.
\end{center}

 The function $(\alpha, \lambda) \mapsto \ell(f_{\hat{\theta}}^{\alpha, \lambda}, \mathcal{D}_{valid})$ corresponds to the objective function of our algorithmic framework. We finally will evaluate the performance of our algorithmic framework by computing the forecast error on $\mathcal{D}_{test}$ using the DNN with the best architecture, hyperparameters and weights:

\begin{center}
    $\ell(f_{\hat{\theta}}^{\hat{\alpha}, \hat{\lambda}},\mathcal{D}_{test})$.
\end{center}

In practice, the second equation optimizing $\alpha$ and $\lambda$ can be solved separately or jointly. If we fix $\lambda$ for each $\alpha$, the optimization is made only on the architecture and is referred to as Neural Architecture Search (NAS). If $\alpha$ is fixed, then the optimization is only made on the model hyperparameters and is referred to as HyperParameters Optimization (HPO). Our algorithmic framework allows us to fix $\alpha$ or $\lambda$ during parts of the optimization to perform a hierarchical optimization: ordering optimisation sequences during which only the architecture is optimised, and others during which only the hyperparameters are optimised.  In the following, we will describe our search space $\Omega = (\mathcal{A} \times \{\Lambda(\alpha), \alpha \in \mathcal{A}\})$.

%************************************
\subsection{Architecture Search Space} \label{part sp archi}
%************************************

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \begin{tikzpicture}[auto, thick,auto,main node/.style={circle,draw}]
            \draw
            node [in_out, preaction={fill, input_purple!40}, pattern color=white, pattern=crosshatch] (c1) {Input}
            node [archi_layer, fill=middle_beige, below=0.5cm of c1] (c2) {$v_1$}
            node [archi_layer,fill=middle_beige, left =0.5cm of c2] (c3) {$v_2$}
            node [archi_layer, fill=middle_beige, below=0.5cm of c2] (c4) {$v_3$}
            node [archi_layer, fill=middle_beige, below=0.5cm of c4] (c5) {$v_4$}
            node [in_out, preaction={fill, input_purple!40}, pattern color=white, pattern=crosshatch, below =0.5cm of c5] (out) {Output};
             \draw[->, middle_beige!300] (c1.west) to [out=180,in=90] (c3);
             \draw[->, middle_beige!300] (c1.east) to [out=300,in=60] (c5.east);
             \draw[->, middle_beige!300] (c1) -- (c2);
             \draw[->, middle_beige!300] (c2) -- (c4);
             \draw[->, middle_beige!300] (c4) -- (c5);
             \draw[->, middle_beige!300] (c3.south) to [out=270,in=180] (c4.west);
             \draw[->, middle_beige!300] (c5) -- (out);
             \draw[->, middle_beige!300] (c4.west) to [out=200,in=130] (out.west);
        \end{tikzpicture}
        \subcaption{Architecture}
        \label{fig:cell_gen_representation}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \begin{tikzpicture}[box/.style={minimum size=0.83cm,draw}]
            \draw[fill=middle_beige, middle_beige] (0.83, 4.12) rectangle (1.62, 4.99);
            \draw[fill=middle_beige, middle_beige] (1.62, 4.99) rectangle (2.49, 4.16);
            \draw[fill=middle_beige, middle_beige] (3.32, 4.16) rectangle (4.16, 4.99);
            \draw[fill=middle_beige, middle_beige] (2.49, 4.16) rectangle (3.32, 3.32);
            \draw[fill=middle_beige, middle_beige] (2.49, 3.32) rectangle (3.32, 2.51);
            \draw[fill=middle_beige, middle_beige] (3.32, 2.51) rectangle (4.16, 1.62);
            \draw[fill=middle_beige, middle_beige] (4.16, 2.51) rectangle (4.99, 1.62);
            \draw[fill=middle_beige, middle_beige] (4.16, 1.62) rectangle (4.99, 0.83);
            \draw[step=0.83cm] (0,0) grid (5,5);
            \node at (-0.7, 4.6) {Input};
            \node at (-0.7, 3.76) {$v_1$};
            \node at (-0.7, 2.93) {$v_2$};
            \node at (-0.7, 2.1) {$v_3$};
            \node at (-0.7, 1.27) {$v_4$};
            \node at (-0.7, 0.44) {Output};
            \node at (1.245, 4.55) {1};
            \node at (2.075, 4.55) {1};
            \node at (3.735, 4.55) {1};
            \node at (2.905, 3.755) {1};
            \node at (2.905, 2.925) {1};
            \node at (3.735, 2.095) {1};
            \node at (4.565, 2.095) {1};
            \node at (4.565, 1.265) {1};
            \foreach \y in {1,...,6}
            \foreach \x in {1, ..., \y}
              {
                \node [box,preaction={fill, input_purple!40}, pattern color=white, pattern=crosshatch] at (0.83*\x-0.41, 6-0.83*\y-0.62){ };
            }
        \end{tikzpicture}
        \caption{Adjacency matrix representation}
        \label{fig:adj_matrix_representation}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \begin{tikzpicture}[auto, thick]
            \draw
            node [in_out, minimum height = 5cm, minimum width=3cm, fill=middle_beige] (global) {}
            node [in_out, minimum height = 1cm, minimum width=2.3cm, preaction={fill, output_red!50}, pattern color=white, pattern=crosshatch dots] at ([yshift=-0.8cm]global.north) {Combiner}
            node [in_out, minimum height = 1.5cm, minimum width=2.3cm, label={[anchor=north]north:Layer Type}, fill=middle_beige!200] at ([yshift=-2.5cm]global.north) {}
            node [in_out, minimum height = 0.7cm,minimum width=2cm,preaction={fill, output_red!50}, pattern color=white, pattern=crosshatch dots] at ([yshift=-2.7cm]global.north) {Params}
            node [in_out, minimum height = 1cm,minimum width=2.3cm, preaction={fill, output_red!50}, pattern color=white, pattern=crosshatch dots] at ([yshift=0.8cm]global.south) {Act. function};
            \draw[->](-1.5, 3) -- (-0.7, 2.2); 
            \draw[->](0, 3) -- (0, 2.2);
            \draw[->](1.5, 3) -- (0.7, 2.2);
            \draw[->](0, 1.2) -- (0, 0.75);
            \draw[->](0, -0.75) -- (0, -1.2);
            \draw[->](0, -2.2) -- (0, -2.7);
        \end{tikzpicture}
        \subcaption{Inside node $v_i$}
        \label{fig:node_representation}
    \end{subfigure}
    \caption{DNN encoding as a directed acyclic graph (DAG). The elements in blue (crosshatch) are fixed by the framework, the architecture elements from $\alpha$ are displayed in beige and the hyperparameters $\lambda$ are in pink (dots).}
\end{figure}

First, we define our architecture search space $\mathcal{A}$. We propose to model a DNN by a Directed Acyclic Graph (DAG) with a single input and output \citep{fiore2013algebra}. A DAG $\Gamma = (\mathcal{V}, \mathcal{E})$ is defined by its nodes (or vertices) set $\mathcal{V} = \{v_1, ..., v_n\}$ and its edges set $\mathcal{E} \subseteq \{(v_i, v_j) | v_i, v_j \in \mathcal{V} \}$. Each node $v$ represents a DNN layer as defined in Subsection \ref{part dl related}, such as a convolution, a recurrence, or a matrix product. To eliminate isolated nodes, we impose each node to be connected by a path to the input and the output. The graph acyclicity implies a partial ordering of the nodes. If a path exists from the node $v_a$ to a node $v_b$, then we can define a relation order between them: $v_a < v_b$. Acyclicity prevents the existence of a path from $v_b$ to $v_a$. However, this relation order is not total. When dealing with symmetric graphs where all nodes are not connected, several nodes' ordering may be valid for the same graph. For example in Figure \ref{fig:cell_gen_representation}, the orderings $v_1 > v_2$ and $v_2 > v_1$ are both valid.
 
Hence, a DAG $\Gamma$ is represented by a sorted list $\mathcal{L}$, such that $|\mathcal{L}| = m$, containing the graph nodes, and its adjacency matrix $M \in \mathbb{R}^{m\times m}$ \citep{zhang2019d}. The matrix $M$ is built such that: $M(i,j) = 1 \Leftrightarrow (v_i, v_j) \in \mathcal{E}$. Because of the graph's acyclicity, the matrix is upper triangular with its diagonal filled with zeros. The input node has no incoming connection, and the output node has no outcoming connection, meaning $\sum_{i=1}^{m} M_{i,1} = 0$ and $\sum_{j=1}^{m} M_{m,j} = 0$.  Besides, the input is necessarily connected to the first node and the last node to the output for any graph, enforcing: $M_{1, 2} = 1$ and $M_{m-1, m} = 1$. As isolated nodes do not exist in the graph, we need at least a non-zero value on every row and column, except for the first column and last row. We can express this property as: $\forall i < m: \sum_{j=i+1}^{m} M_{i,j} > 0$ and $\forall j > 1: \sum_{i=j+1}^{m} M_{i,j} > 0$.  Finally, the ordering of the partial nodes does not allow a bijective encoding: several matrices $M$ may encode the same DAG.

To summarize, we have $\mathcal{A} = \{\Gamma = (\mathcal{V}, \mathcal{E}) = (\mathcal{L}, M)\}$. The graphs $\Gamma$ are parameterized by their size $m$ which is not equal for all graphs. As we will see in Section \ref{part: search algo} the DNNs size may vary during the optimization framework.

%**************************
\subsection{Hyperparameters Search Space} \label{part sp hp}
%**************************

For any fixed architecture $\alpha \in \mathcal{A}$, let's define our hyperparameters search space induced by $\alpha: \Lambda(\alpha)$. As mentioned above, the DAG nodes represent the DNN hidden layers. A set of hyperparameters $\lambda$, also called a graph node, is composed of a combiner, a layer operation and an activation function (see Figure \ref{fig:node_representation}). Each layer operation is associated with a specific set of parameters, like output or hidden dimensions, convolution kernel size or dropout rate. We provide in Appendix \ref{anx:op_hp} a table with all available layer types and their associated parameters. The hyperparameters search space $\Lambda(\alpha)$ is made of sets $\lambda$ composed with a combiner, the layer's parameters and the activation function.

First, we need a combiner as each node can receive an arbitrary number of input connections. The parents' latent representations should be combined before being fed to the layer type. Taking inspiration from the Google Brain Team Evolved Transformer \citep{so2019evolved}, we propose three types of combiners: element-wise addition, element-wise multiplication and concatenation. The input vectors may have different channel numbers and the combiner needs to level them. This issue is rarely mentioned in the literature, where authors prefer to keep a fixed channel number \citep{liu2018darts}. In the general case, for element-wise combiners, the combiner output channel matches the maximum channel number of latent representation. We apply zero-padding on the smaller inputs. For the concatenation combiner, we consider the sum of the channel number of each input. Some layer types, for instance, the pooling and the convolution operators, have kernels. Their calculation requires that the number of channels of the input vector is larger than this kernel. In these cases, we also perform zero-padding after the combiner to ensure that we have the minimum number of channels required.

When building the DNN, we dimension asynchronously each layer operation. We first compute the layer operation input shape according to the input vectors and the combiner. After building the operation we compute its output shape for the next layer. Finally, the node's remaining part is the activation function. We choose this last parameter among a large set detailed in Appendix \ref{anx:op_hp}. To summarize, we define every node as the sequence of combiner $\rightarrow$ layer type $\rightarrow$ activation function. In our search space $\Lambda(\alpha)$, the nodes are encoded by arrays containing the combiner name, the layer type name, the value of each layer operation's parameters and finally, the activation function name. The set $\mathcal{L}$ mentioned in the previous section, which contains the nodes, is then a variable-length list containing the arrays representing each node.