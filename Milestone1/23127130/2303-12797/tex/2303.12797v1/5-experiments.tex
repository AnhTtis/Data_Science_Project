\section{Experimental study} \label{part5}
%**************************************
\subsection{Experimental protocol}
%**************************************

We evaluated our optimization algorithm framework on the established benchmark of Monash Time Series Forecasting Repository \citep{godahewa2021monash}. For these experiments, we configured our algorithm to have a population of 40 individuals and a total of 100 generations. We investigated a sequential optimization of the architecture and the hyperparameters. We alternate at a certain generation frequency between two scopes: search operators applied to the architecture $\alpha$ with $\lambda$ fixed, and search operators applied to the hyperparameters $\lambda$ with $\alpha$ fixed. Thus, the architecture-centred sections of the optimization will diversify the population DNNs, while the hyperparameter-centred parts will perform a finer optimization of the obtained DNNs. We ran our experiments on 5 cluster nodes, each equipped with 4 Tesla V100 SXM2 32GB GPUs, using Pytorch 1.11.0 and Cuda 10.2. The experiments were all finished in less than 72 hours.

The Monash time series forecasting archive is a benchmark containing more than 40 datasets and the results of 13 forecasting models on each of these prediction tasks \citep{godahewa2021monash}. The time series are of different kinds and have variable distributions. More information on each dataset from the archive is available \ref{part info monash}. It allows us to test our framework generalization and robustness abilities.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[auto, thick]
        \draw
        node [in_out, minimum height = 1cm, minimum width=4cm, fill=input_purple, text=white](input) {Time Series input}
        node [in_out, minimum height = 6cm, minimum width=4.5cm, below=0.5cm of input, fill=middle_beige!30, label={[align=center, anchor=north]north:Deep Neural Network:\\$f^{\alpha, \lambda}_{\theta}$}] (dnn) {}
        node [in_out, minimum height = 3cm, minimum width=4cm , below=1.7cm of input] (dag) {Directed Acyclic\\ Graph: $ \Gamma = f^{\alpha, \lambda}$}
         node [in_out, minimum height = 1cm, minimum width=4cm, below=0.5cm of dag, fill=output_red, text=white] (mlp) {Multi-layer Perceptron}
         node [in_out, minimum height = 1cm, minimum width=4cm, below=0.5cm of dnn, fill=output_red!30] (out) {Time Series Prediction};
        \draw[->](input) -- (dnn); 
        \draw[->](dag) -- (mlp);
        \draw[->](dnn) -- (out);
    \end{tikzpicture}
    \caption{Meta model for Monash time series datasets.}
    \label{fig:metamodel_monash}
\end{figure}

The paper's authors accompanied their dataset with a GitHub allowing them to directly and very easily compare different statistical, machine learning and deep learning models on the forecast of different time series. We followed the indications to integrate new deep learning models, and only changed the models' core. We kept their data preparation functions, data loaders, training parameters (number of epochs, batch size), as well as the training and evaluation functions, to have the most accurate comparison possible. For each dataset, we also took over the configurations of the directory, notably the prediction horizons and lags. Figure \ref{fig:metamodel_monash} represents our meta-model which was used to replace the repository's models. The Multi-layer Perceptron at the end of the model is used to retrieve the time series output dimension, as the number of channels may vary within the Directed Acyclic Graph.

 We compared our results with the benchmark models, which are composed of statistical models, machine learning and deep learning models. The metric used to evaluate the models' performance, our forecast error $\ell$, is the Mean Absolute Scaled Error (MASE), an absolute mean error divided by the average difference between two consecutive time steps \citep{hyndman2006another}. Given a time series $Y = (\mathbf{y}_1, ..., \mathbf{y}_n)$ and the predictions $\hat{Y} = (\mathbf{\hat{y}}_1, ..., \mathbf{\hat{y}}_n)$, the MASE can be defined as:
\begin{center}
    $\mathrm{MASE}(Y, \hat{Y}) = \frac{n-1}{n} \times \frac{\sum_{t=1}^{n}|\mathbf{y}_t - \hat{\mathbf{y}}_t|}{\sum_{t=2}^{n}|\mathbf{y}_t - \mathbf{y}_{t-1}|}$.
\end{center}

In our case, for $f \in \Omega$, $\mathcal{D}_0 = (X_0, Y_0) \subseteq \mathcal{D}$, we have $\ell(Y_0, f(X_0) = \mathrm{MASE}\big(Y_0, f(X_0)\big)$.

\begin{table}[htbp]
\small
\begin{center}
\caption{\footnotesize{Mean MASE for each dataset, we only reported: the best MASE for statistical models, machine learning models and our optimization framework results. Statistical models: SES, Theta, TBATS, ETS, ARIMA. Machine Learning models: PR, CatBoost, FFNN, DeepAR, N-Beats, WaveNet, Transformer, Informer}.}
\label{tab:results_monash}
\begin{tabular}{|c| c c c|}
\hline
\textbf{Name} &\multicolumn{3}{c|}{\textbf{Models}} \\

\textbf{Dataset} & \textbf{\textit{Stat. models}}& \textbf{\textit{ML/DL models}}& \textbf{\textit{Our framework}} \\
\hline
\hline
Aus. elec & 1.174 & \cellcolor{gray!20}\textbf{0.705} & 0.893 \\
\hline
Births & 1.453 & 1.537 & \cellcolor{gray!20}\textbf{1.233} \\
\hline
Bitcoin & 2.718 & \cellcolor{gray!20}\textbf{2.664} & 4.432 \\
\hline
Carparts & 0.897 & 0.746 & \cellcolor{gray!20}\textbf{0.744} \\
\hline
Covid deaths & 5.326 & 5.459 & \cellcolor{gray!20}\textbf{4.535} \\
\hline
Dominick & 0.582 & 0.705 & \cellcolor{gray!20}\textbf{0.510} \\
\hline
Elec. hourly & 3.690 & \cellcolor{gray!20}\textbf{1.606} & 1.652 \\
\hline
Elec. weekly & 1.174 & 0.705 & \cellcolor{gray!20}\textbf{0.652} \\
\hline
Fred MD & \cellcolor{gray!20}\textbf{0.468} & 0.601 & 0.489 \\
\hline
Hospital & 0.761 & 0.769 & \cellcolor{gray!20}\textbf{0.751} \\
\hline
KDD & 1.394 & 1.185 & \cellcolor{gray!20}\textbf{1.161} \\
\hline
Kaggle weekly & 0.622 & 0.628 &  \cellcolor{gray!20}\textbf{0.561}\\
\hline
M1 monthly & \cellcolor{gray!20}\textbf{1.074} & 1.123 & 1.081 \\
\hline
M1 quart. & \cellcolor{gray!20}\textbf{1.658} & 1.700 & 1.683 \\
\hline
M1 yearly & \cellcolor{gray!20}\textbf{3.499} & 4.355 & 3.732 \\
\hline
M3 monthly & \cellcolor{gray!20}\textbf{0.861} & 0.934 & 0.926 \\
\hline
M3 other & \cellcolor{gray!20}\textbf{1.814} & 2.127 & 2.227 \\
\hline
M3 quart. & 1.174 & 1.182 & \cellcolor{gray!20}\textbf{1.099} \\
\hline
M3 yearly & \cellcolor{gray!20}\textbf{2.774} & 2.961 & 2.800 \\
\hline
M4 daily & 1.153 & 1.141 & \cellcolor{gray!20}\textbf{1.066} \\
\hline
M4 hourly & 2.663 & 1.662 & \cellcolor{gray!20}\textbf{1.256} \\
\hline
M4 monthly & \cellcolor{gray!20}\textbf{0.948} & 1.026 & 0.993 \\
\hline
M4 quart. & \cellcolor{gray!20}\textbf{1.161} & 1.239 & 1.198 \\
\hline
M4 weekly & 0.504 & 0.453 & \cellcolor{gray!20}\textbf{0.430} \\
\hline
NN5 daily & \cellcolor{gray!20}\textbf{0.858} & 0.916 & 0.898 \\
\hline
NN5 weekly & 0.872 & 0.808 & \cellcolor{gray!20}\textbf{0.739} \\
\hline
Pedestrians & 0.957 & 0.247 & \cellcolor{gray!20}\textbf{0.222} \\
\hline
Rideshare & 1.530 & 2.908 & \cellcolor{gray!20}\textbf{1.410} \\
\hline
Saugeen & 1.425 & 1.411 & \cellcolor{gray!20}\textbf{1.296} \\
\hline
Solar 10mn & \cellcolor{gray!20}\textbf{1.034} & 1.450 & 1.426 \\
\hline
Solar weekly & 0.848 & 0.574 & \cellcolor{gray!20}\textbf{0.511} \\
\hline
Sunspot & 0.067 & 0.003 & \cellcolor{gray!20}\textbf{0.002} \\
\hline
Temp. rain & 1.174 & 0.687 & \cellcolor{gray!20}\textbf{0.686} \\
\hline
Tourism monthly & 1.526 & \cellcolor{gray!20}\textbf{1.409} & 1.453 \\
\hline
Tourism quart. & 1.592 & 1.475 & \cellcolor{gray!20}\textbf{1.469} \\
\hline
Tourism yearly & 3.015 & 2.977 & \cellcolor{gray!20}\textbf{2.690} \\
\hline
Traffic hourly & 1.922 & 0.821 & \cellcolor{gray!20}\textbf{0.729} \\
\hline
Traffic weekly & 1.116 & 1.094 & \cellcolor{gray!20}\textbf{1.030} \\
\hline
Vehicle trips & 1.224 & \cellcolor{gray!20}\textbf{1.176} & 1.685 \\
\hline
Weather & 0.677 & 0.631 & \cellcolor{gray!20}\textbf{0.614}\\
\hline
\hline
\textbf{Total bests} & 11 & 5 & 24\\
\hline

\end{tabular}
\end{center}
\end{table}

The results are reported in Table \ref{tab:results_monash}. Our model succeeded in outperforming the best baseline for 24 out of 40 datasets. For the remaining 16 datasets, our framework obtained errors close to the best baseline. It is worth noting that our model outperformed the machine learning and deep learning models from the benchmark for 34 out of 40 datasets and was among the top 3 models for 34 out of 40 datasets. In Figure \ref{fig:ga_illustration} we show the convergence of the evolutionary algorithm. At the right positions, we displayed the mean loss for each generation and the best individual loss. On the mean loss for each generation curve, we can identify the phases where the architecture is optimized, where we have more variability and the phases where the hyperparameters are optimized, with less variability. On the left positions, we represented heatmaps with the loss for each individual at each generation. The best individual is not over-represented in the population before the optimization ends.

\begin{figure}[htbp]
\centering

\begin{subfigure}[b]{\textwidth}
\centering
\includegraphics[height=.27\columnwidth]{data/m4_hourly_heatmap_ga.pdf}\label{fig:heatmap1}\quad
\includegraphics[height=.27\columnwidth]{data/m4_hourly_lineplot_ga.pdf}\label{fig:lineplot1}
\caption{M4 Hourly dataset, best MASE: 1.288}
\end{subfigure}\quad

\begin{subfigure}[b]{\textwidth}
\centering
\includegraphics[height=.27\columnwidth]{data/electricity_weekly_heatmap_ga.pdf}\label{fig:heatmap2}\quad
\includegraphics[height=.27\columnwidth]{data/electricity_weekly_lineplot_ga.pdf}\label{fig:lineplot2}
\caption{Electricity weekly dataset, best MASE: 0.652}
\end{subfigure}

\begin{subfigure}[b]{\textwidth}
\centering
\includegraphics[height=.27\columnwidth]{data/nn5_weekly_heatmap_ga.pdf}\label{fig:heatmap3}\quad
\includegraphics[height=.27\columnwidth]{data/nn5_weekly_lineplot_ga.pdf}\label{fig:lineplot3}
\caption{NN5 weekly dataset, best MASE: 0.739}
\end{subfigure}

\caption{Evolutionary algorithm convergence. Left: heatmap with the loss for each individual for every generation. Right: population mean loss and best individual's loss through generations. Darker grey backgrounds represent generations during which the architecture is optimized, and lighter grey backgrounds represent generations during which the hyperparameters are optimized.}
\label{fig:ga_illustration}
\end{figure}

%**********************************
\subsection{Best models analysis}
%**********************************

In the AutoDL literature, few efforts are usually made to analyze the generated DNNs. In \citet{DBLP:journals/corr/abs-1909-09569} the authors established that architectures with wide and shallow cell structures are favoured by the NAS algorithms and that they suffer from poor generalization performance. We can rightfully ask ourselves about the efficiency of our framework and some of these questions may be answered thanks to a light models analysis. By the end of this section, we will try to answer some inquiries about our framework outputs. To answer those questions we defined some structural indicators, and we computed them in Table \ref{tab:best_models} for the best model for each dataset from \citet{godahewa2021monash}:

\begin{itemize}
    \item \textbf{\textit{Nodes}}: it represents the number of nodes (i.e. operations) in the graph.
    \item \textbf{\textit{Width}}: it represents the network width which can be defined as the maximum of incoming or outgoing edges to all nodes within the graph.
    \item \textbf{\textit{Depth}}: it defines the network depth which corresponds to the size of the longest path in the graph.
    \item \textbf{\textit{Dim}}: it is the maximum channel dimension, relative to the number of input and output channels (ratio). It indicates how complex the latent spaces from the neural network might become, compared to the dataset complexity.
    \item \textbf{\textit{Edges}}: it represents the number of edges, relative to the number of nodes in the graph. It indicates how complex the graph can be and how sparse the adjacency matrix is.
    \item The last 7 indicators correspond to the number of the appearance of each layer type within the DNN.
\end{itemize}

\begin{table}[htbp]
\scriptsize
\begin{center}
\caption{\footnotesize{Structural indicators of the best model for each dataset found by our algorithmic framework.}}
\label{tab:best_models}
\begin{tabularx}{\linewidth}{|c|X X X X X X X X X X X X|}
\hline
 \textbf{Name} &\multicolumn{12}{c|}{\textbf{Structural Features}} \\

\textbf{Dataset} & \multicolumn{5}{c}{ } & \multicolumn{7}{c|}{\textbf{Number of Nodes by layer types}} \\

 &\textbf{\textit{nodes}}& \textbf{\textit{width}}& \textbf{\textit{depth}}& \textbf{\textit{dim}}& \textbf{\textit{edges}}& \textbf{\textit{MLP}}& \textbf{\textit{Att}}& \textbf{\textit{CNN}}& \textbf{\textit{RNN}}& \textbf{\textit{Drop}}& \textbf{\textit{Id}}& \textbf{\textit{Pool}} \\
\hline
 \hline
Aus. elec & 3 & 2 & 3 & 1.02 & 1.33 & 2 & 0 & 0 & 0 & 0 & 1 & 0 \\
\hline
Births & 10 & 7 & 8 & 16.26 & 3.0 & 3 & 1 & 1 & 1 & 0 & 3 & 1 \\
\hline
Bitcoin & 9 & 5 & 7 & 14.2 & 2.11 & 3 & 0 & 1 & 2 & 2 & 1 & 0 \\
\hline
Carparts & 6 & 4 & 6 & 8.7 & 2.0 & 3 & 1 & 0 & 0 & 1 & 0 & 1 \\
\hline
Dominick & 8 & 6 & 6 & 40.4 & 2.22 & 5 & 0 & 1 & 0 & 1 & 1 & 1 \\
\hline
Elec. hourly & 6 & 3 & 6 & 2.88 & 1.5 & 4 & 0 & 0 & 0 & 0 & 0 & 2 \\
\hline
Elec. weekly & 10 & 8 & 8 & 8.75 & 3.0 & 4 & 1 & 1 & 1 & 1 & 1 & 1 \\
\hline
Fred MD & 8 & 6 & 7 & 12.0 & 2.38 & 2 & 2 & 0 & 0 & 2 & 1 & 1 \\
\hline
Hospital & 9 & 6 & 6 & 20.0 & 2.2 & 2 & 1 & 2 & 0 & 0 & 2 & 2 \\
\hline
KDD & 4 & 3 & 4 & 2.29 & 1.75 & 4 & 0 & 0 & 0 & 0 & 0 & 0  \\
\hline
\makecell{Kaggle\\weekly} & 5 & 3 & 5 & 25.3 & 1.6 & 4 & 0 & 0 & 0 & 0 & 0 & 1 \\
\hline
\makecell{M1\\monthly} & 4 & 3 & 4 & 16.1 & 1.75 & 2 & 1 & 0 & 1 & 0 & 0 & 0 \\
\hline
M1 quart. & 10 & 6 & 8 & 56.38 & 3.0 & 5 & 1 & 0 & 1 & 0 & 3 & 0 \\
\hline
M1 yearly & 7 & 4 & 5 & 52.3 & 1.71 & 3 & 1 & 1 & 0 & 1 & 0 & 1 \\
\hline
\makecell{M3\\monthly} & 8 & 5 & 7 & 11.27 & 2.25 & 3 & 2 & 0 & 2 & 0 & 1 & 0 \\
\hline
M3 other & 4 & 	2 & 3 & 33.88 & 1.25 & 2 & 0 & 0 & 0 & 0 & 2 & 0 \\
\hline
M3 quart. & 2 & 1 & 2 & 20.6 & 1.0 & 2 & 0 & 0 & 0 & 0 & 0 & 0 \\
\hline
M3 yearly & 10 & 7 & 8 & 67.0 & 3.0 & 4 & 0 & 0 & 3 & 0 & 2 & 1 \\
\hline
M4 daily & 2 & 1 & 2 & 32.29 & 1.0 & 2 & 0 & 0 & 0 & 0 & 0 & 0 \\
\hline
M4 hourly & 6 & 4 & 5 & 2.9 & 1.83 & 2 & 0 & 1 & 2 & 0 & 1 & 0 \\
\hline
\makecell{M4\\monthly}& 3 & 2 & 3 & 10.06 & 1.33 & 2 & 0 & 0 & 0 & 0 & 1 & 0 \\
\hline
M4 quart. & 5 & 4 & 4 & 38.0 & 1.8 & 3 & 0 & 1 & 0 & 0 & 1 & 0 \\
\hline
M4 weekly & 7 & 6 & 5 & 3.37 & 2.14	& 1 & 1 & 2 & 0 & 0 & 2 & 1 \\
\hline
NN5 daily & 6 & 4 &	5 &	5.6 & 1.67 & 4 & 0 & 0 & 0 & 2 & 0 & 0 \\
\hline
NN5 weekly & 4 & 3 & 3 & 1.0 & 1.5 & 1 & 0 & 1 & 0 & 0 & 0 & 2 \\
\hline
Pedestrians & 9 & 5 & 6 & 3.97 & 2.33 & 2 & 1 & 1 & 1 & 1 & 2 & 1 \\
\hline
Rideshare & 4 & 3 & 4 & 1 & 1.5 & 3 & 0 & 0 & 0 & 0 & 1 & 0 \\
\hline
Saugeen & 2 & 1 & 2 & 0.3 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\
\hline
Solar 10mn & & & & & & & & & & & &  \\
\hline
\makecell{Solar\\weekly} & 12 & 7 & 9 & 51.67 & 2.75 & 3 & 1 & 0 & 0 & 4 & 1 & 3 \\
\hline
Sunspot & 8 & 4 & 7 & 0.25 & 1.75 & 3 & 0 & 2 & 0 & 1 & 1 & 1 \\
\hline
Temp. rain & 8 & 5 & 6 & 12.4 & 2 & 4 & 1 & 0 & 0 & 1 & 0 & 2 \\
\hline
\makecell{Tourism\\monthly} & 4 & 2 & 4 & 8.38 & 1.5 & 2 & 0 & 1 & 0 & 0 & 1 & 0 \\
\hline
\makecell{Tourism\\quart.} & 7 & 5 & 6 & 25.75 & 2.14 & 2 & 0 & 1 & 2 & 0 & 2 & 0 \\
\hline
\makecell{Tourism\\yearly} & 9 & 6 & 6 & 68.0 & 2.22 & 3 & 1 & 1 & 1 & 1 & 2 & 0 \\
\hline
\makecell{Traffic\\hourly}& 9 & 6 & 7 & 2.76 & 2.44 & 4 & 1 & 1 & 0 & 2 & 0 & 1 \\
\hline
\makecell{Traffic\\weekly} & 7 & 5 & 5 & 2.20 & 1.71 & 1 & 0 & 3 & 1 & 0 & 2 & 0 \\
\hline
\makecell{Vehicle\\trips} & 8 & 5 & 7 & 9.83 & 2 & 2 & 1 & 0 & 3 & 0 & 0 & 2 \\
\hline
Weather & 8 & 4 & 6 & 38.8 & 2.38 & 2 & 1 & 3 & 0 & 0 & 1 & 1 \\
\hline
\hline
\textbf{Summary} & 6.4 & 4.13 & 5.25 & 17.2 & 1.91 & 41\% & 8.2\% & 9.8\% & 8.5\% & 7.8\% & 14.4\% & 10.1\% \\
\hline
\end{tabularx}
\end{center}
\end{table}

\textit{Do our framework always converge to complex models, or is it able to find simple DNNs?}

From Table \ref{tab:best_models} and Figure \ref{fig:simplegraph}, one can see that we have multiple simple graphs with only two layers. Knowing that the last feed-forward layer is enforced by our meta-model (see Figure \ref{fig:metamodel_monash}), our DAG is only composed of one layer. Another indicator of this simplicity is the percentage of feed-forward layers found in the best models. 41\% of the layers are feed-forward according to the table \ref{tab:best_models} although our search space offers more complex layers such as convolution, recurrence or attention layers less frequently picked. This proves that even without regularization penalties, our algorithmic framework does not systematically search for over-complicated models. 

\vspace{0.5cm}
\textit{Do our algorithmic framework always converge to similar architectures for different datasets?}

The framework is meanwhile able to find complex models as in Figure \ref{fig:complexgraph}, which partially answers our question. The indicators in Table \ref{tab:best_models} suggest that we found models with various numbers of nodes, from 2 to 10 and with diverse edge densities. The best model for the electricity hourly dataset (see Figure \ref{fig:elec_hourly}) has an average of 1.5 incoming or outgoing edges by node whereas the best model for the electricity weekly dataset has an average of 3 incoming or outgoing edges by nodes. This is true even within performing graphs for the same dataset. In Figure \ref{fig:diff_graph_same_dataset} we displayed two different graphs having similar (and good) performance on the Dominick dataset. Both graphs do not have the same number of nodes and different architectures, the first one being a deep sparse graph while the other is wider with a lot of edges. 

\vspace{0.5cm}
\textit{What is the diversity of the layer types within the best models?}

The graphs from Figure \ref{fig:diff_graph_same_dataset} have also quite different layer types. If both are mainly based on identity, pooling and feed-forward operations, the second graph introduces convolution and dropout layers. In general, to answer this question, the fully-connected layers seem to dominate all layer types, as it represents 41\% of the chosen layers in the best models (see Table \ref{tab:best_models}). The identity layer is also more frequently picked. It is finally interesting to notice that all other layers are selected at the same frequency. 

\vspace{0.5cm}
\textit{Are the best models still \say{deep} neural networks or are they wide and shallow as stated in \citet{DBLP:journals/corr/abs-1909-09569}?}

To answer this question, the observations from \citet{DBLP:journals/corr/abs-1909-09569} also apply to our results. Our models are often almost as wide as they are deep. This observation needs more investigation as one of the reasons mentioned in the paper: the premature evaluation of architecture before full convergence does not apply here. Our models are smaller than the ones studied in the paper and thus converge faster, even when they are a bit deeper.

A last remark is related to the latent spaces generated by our models. Except for a few models, our models tend to always generate bigger latent space than the number of input and output channels. On average, the maximum size of the latent representation within a network is 17 times bigger than the input and/or output channels number.

\begin{figure}[htbp]
\centering

\begin{subfigure}[b]{0.3\columnwidth}
\centering
\includegraphics[height=\columnwidth]{data/nn_m4_daily.gv.pdf}
\caption{M4 Daily, MASE: 1.066}
\label{fig:simplegraph}
\end{subfigure}
\begin{subfigure}[b]{0.69\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/nn_solar_weekly.gv.pdf}
\caption{Solar Weekly, MASE: 0.511}
\label{fig:complexgraph}
\end{subfigure}

\begin{subfigure}[b]{0.52\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/nn_electricity_weekly.gv.pdf}
\caption{Electricity weekly, MASE: 0.652}
\label{fig:elec_week}
\end{subfigure}
\begin{subfigure}[b]{0.35\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/3504nn_electricity_hourly.gv.pdf}
\caption{Elec. hourly, MASE: 1.634}
\label{fig:elec_hourly}
\end{subfigure}

\caption{Best DNNs output by our algorithmic framework.}
\label{fig:diverse_graphs}
\end{figure}

\begin{figure}[htbp]
\centering

\begin{subfigure}[b]{0.35\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/783_0.5114263nn_dominick.gv.pdf}
\caption{Dominick, MASE: 0.5114}
\label{fig:simplegraph2}
\end{subfigure}
\begin{subfigure}[b]{0.64\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/3285_0.5118941nn_dominick.gv.pdf}
\caption{Dominick, MASE: 0.5119}
\label{fig:complexgraph2}
\end{subfigure}

\caption{Two different models having similar good performance on Dominick dataset.}
\label{fig:diff_graph_same_dataset}
\end{figure}

%*********************************************************************
\subsection{Nondeterminism and instability of DNNs}
%**********************************************************************

An often overlooked robustness challenge with DNN optimization is their uncertainty in performance \citep{summers2021nondeterminism}. A unique model with a fixed architecture and set of hyperparameters can produce a large variety of results on a dataset. Figure \ref{fig:seeds_histo} shows the results on two datasets: M3 Quarterly and Electricity Weekly. For both datasets, we selected the best models found with our optimization and drew 80 seeds summing all instability and nondeterministic aspects of our models. We trained these models and plotted the MASE Figure \ref{fig:seeds_histo}. On the M3 Quarterly, the MASE reached values two times bigger than our best result. On the Electricity Weekly, it went up to five times worst. To overcome this problem, we represented the parametrization of stochastic aspects in our models as a hyperparameter, which we added to our search space. Despite its impact on the performance, we have not seen any work on NAS, HPO or AutoML trying to optimize the seed of DNNs. Our plots of Figure \ref{fig:seeds_histo} showed that the optimization was effective as no other seeds gave better results than the one picked by our algorithmic framework.

\begin{figure}[htbp]
\centering

\begin{subfigure}[b]{0.49\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/m3_quarterly_seeds.pdf}
\caption{M3 Quarterly, best MASE: 1.099}
\label{fig:seeds_m3}
\end{subfigure}
\begin{subfigure}[b]{0.49\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/electricity_weekly_seeds.pdf}
\caption{Electricity Weekly, best MASE: 0.652}
\label{fig:seeds_elec}
\end{subfigure}

\caption{Mape histogram of the best model performances with multiple seeds for two datasets.}
\label{fig:seeds_histo}
\end{figure}
