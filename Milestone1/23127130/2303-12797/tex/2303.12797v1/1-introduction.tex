\section{Introduction}

With the recent successes of deep learning in many research fields, deep neural networks (DNN) optimization stimulates the growing interest of the scientific community \citep{talbi2021automated}. While each new learning task requires the handcrafted design of a new DNN, automated deep learning facilitates the creation of powerful DNNs. Interests are to give access to deep learning to less experienced people, to reduce the tedious tasks of managing many parameters to reach the optimal DNN, and finally, to go beyond what humans can design by creating non-intuitive DNNs that can ultimately prove to be more efficient.

Optimizing a DNN means automatically finding an optimal architecture for a given learning task: choosing the operations and the connections between those operations and the associated hyperparameters. The first task is also known as Neural Architecture Search \citep{elsken2019neural}, also named NAS, and the second, as HyperParameters Optimization (HPO). Most works from the literature try to tackle only one of these two optimization problems. Many papers related to NAS \citep{white2021bananas,loni_deepmaker_2020, wang_evolving_2019, sun_particle_2018, zhong_dna_2020} focus on designing optimal architectures for computer vision tasks with a lot of stacked convolution and pooling layers. Because each DNN training is time-consuming, researchers tried to reduce the search space by adding many constraints preventing from finding irrelevant architectures. It affects the flexibility of the designed search spaces and limits the hyperparameters optimization. 

We introduce in this paper a new optimization framework for AutoML based on the evolution of Directed Acyclic Graphs (DAGs). The encoding and the search operators may be used with various deep learning and AutoML problems. We ran experiments on time series forecasting tasks and demonstrate on a large variety of datasets that our framework can find DNNs which compete with or even outperform state-of-the-art forecasters. In summary, our contributions are as follows:

\begin{itemize}
\item The precise definition of a flexible and complete search space based on DAGs, for the optimization of DNN architectures and hyperparameters. 

\item The design of efficient neighbourhoods and variation operators for DAGs. With these operators, any metaheuristic designed for a mixed and variable-size search space can be applied. In this paper, we investigate the use of evolutionary algorithms.

\item The validation of the algorithmic framework on popular time series forecasting benchmarks \citep{godahewa2021monash}. We outperformed 13 statistical and machine learning models on 24 out of 40 datasets, proving the efficiency and robustness of our framework.
\end{itemize}

The paper is organized as follows: we review section \ref{part 2}, the literature on deep learning models for time series forecasting and AutoML. Section \ref{part 3} defines our search space. Section \ref{part 4} presents our neighbourhoods and variation operators within evolutionary algorithms. Section \ref{part5} details our experimental results obtained on popular time series forecasting benchmarks. Finally, section \ref{part 6} gives a conclusion and introduces further research opportunities.