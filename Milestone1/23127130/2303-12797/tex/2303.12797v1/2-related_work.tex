\section{Related Work} \label{part 2}

\subsection{Deep learning for time series forecasting}

\label{part dl related}

Time series forecasting has been studied for decades. The field has been dominated for a long time by statistical tools such as ARIMA, Exponential Smoothing (ES), or (S)ARIMAX, this last model allowing the use of exogenous variables. It now opens itself to deep learning models \citep{9461796}. These new models recently achieved great performances on many datasets. Three main parts compose typical DNNs: an input layer, several hidden layers and an output layer. In this paper, we define a search space designed to search for the best-hidden layers, given a meta-architecture (see Figure \ref{fig:metamodel_monash}), for a specific time series forecasting task. Next, we introduce the usual DNN layers considered in our search space.

The first layer type from our search space is the fully-connected layer, or Multi-Layer Perceptron (MLP). The input vector is multiplied by a weight matrix. Most architectures use such layers as simple building blocks for dimension matching, input embedding or output modelling. The N-Beats model is a well-known example of a DNN based on fully-connected layers for time series forecasting \citep{NBeats}.

The second layer type \citep{lecun2015deep} is the convolution layer (CNN). Inspired by the human brain's visual cortex, it has mainly been popularised for computer vision. The convolution layer uses a discrete convolution operator between the input data and a small matrix called a filter. The extracted features are local and time-invariant if the considered data are time series. Many architectures designed for time series forecasting are based on convolution layers such as WaveNet \citep{oord2016wavenet} and Temporal Convolution Networks \citep{lea2017temporal}.

The third layer type is the recurrent layer (RNN), specifically designed for sequential data processing, therefore, particularly suitable for time series. These layers scan the sequential data and keep information from the sequence past in memory to predict its future. A popular model based on RNN layers is the Seq2Seq network \citep{seq2seq}. Two RNNs, an encoder and a decoder, are sequentially connected by a fixed-length vector. Various versions of the Seq2Seq model have been introduced in the literature, such as the DeepAR model \citep{salinas2020deepar}, which encompasses an RNN encoder in an autoregressive model. The major weakness of RNN layers is the modelling of long-term dynamics due to the vanishing gradient. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) layers have been introduced \citep{hochreiter1997long, chung2014empirical} to overcome this problem.

Finally, the layer type from our search space is the attention layer. The attention layer has been popularized within the deep learning community as part of Vaswani's transformer model \citep{vaswani2017attention}. The attention layer is more generic than the convolution. It can model the dependencies of each element from the input sequence with all the others. In the vanilla transformer \citep{vaswani2017attention}, the attention layer does not factor the relative distance between inputs in its modelling but rather the element's absolute position in the sequence. The Transformer-XL \citep{dai2019transformer}, a transformer variant created to tackle long-term dependencies tasks, introduces a self-attention version with relative positions. \citet{cordonnier2019relationship} used this new attention formulation to show that, under a specific configuration of parameters, the attention layers could be trained as convolution layers. Within our search space, we chose this last formulation of attention, with the relative positions.

The three first layers (i.e. MLP, CNN, RNN) were frequently mixed into DNN architectures. Sequential and parallel combinations of convolution, recurrent and fully connected layers often compose state-of-the-art DNN models for time series forecasting. Layer diversity enables the extraction of different and complementary features from input data to allow a better prediction. Some recent DNN models introduce transformers into hybrid DNNs. In \citet{lim2021temporal}, the authors developed the Temporal Fusion Transformer, a hybrid model stacking transformer layers on top of an RNN layer. With this in mind, we built a flexible search space which generalizes hybrid DNN models including MLPs, CNNs, RNNs and transformers.

\subsection{Search spaces for automated deep learning}

Designing an efficient DNN for a given task requires choosing an architecture and tuning its many hyperparameters. It is a difficult fastidious, and time-consuming optimization task. Moreover, it requires expertise and restricts the discovery of new DNNs to what humans can design. Research related to the automatic design and optimization of DNNs has therefore risen this last decade \citep{talbi2021automated}. The first challenge with automatic deep learning (AutoDL), and more specifically the neural architecture search (NAS), is the search space design. If the solution encoding is too broad and allows too many architectures, we might need to evaluate many architectures to explore the search space. However, training many DNNs would require considerable computing time and become unfeasible. On the contrary, if the search space is too small, we might miss promising solutions. Besides, encoding of DNNs defining the search space should follow some rules \citep{talbi2021automated}:

\begin{itemize}
\item Completeness: all candidate DNNs solutions should be encoded in the search space.
\item Connexity: a path should always be possible between two encoded DNNs in the search space.
\item Efficiency: the encoding should be easy to manipulate by the search operators (i.e. neighbourhoods, variation operators) of the search strategy.
\item Constraint handling: the encoding should facilitate the handling of the various constraints to generate feasible DNNs.
\end{itemize}

A complete classification of encoding strategies for NAS is presented in \citet{talbi2021automated} and reproduced in Figure \ref{fig:classi_encoding}. We can discriminate between direct and indirect encodings. With direct strategies, the DNNs are completely defined by the encoding, while indirect strategies need a decoder to find the architecture back. Amongst direct strategies, one can discriminate between two categories: flat and hierarchical encodings. In flat encodings, all layers are individually encoded \citep{loni2020deepmaker, sun2018particle, wang2018evolving, wang2019evolving}. The global architecture can be a single chain, with each layer having a single input and a single output, which is called chain structured \citep{assuncao_denser_2018}, but more complex patterns such as multiple outputs, skip connections, have been introduced in the extended flat DNNs encoding \citep{chen_scale-aware_2021}. For hierarchical encodings, they are bundled in blocks \citep{pham2018efficient, shu2019understanding, liu2017hierarchical, zhang2019d}. If the optimization is made on the sequencing of the blocks, with an already chosen content, this is referred to as inner-level fixed \citep{camero2021bayesian, white2021bananas}. If the optimization is made on the blocks' content with a fixed sequencing, it is called outer level fixed. A joint optimization with no level fixed is also an option \citep{liu2019auto}. Regarding the indirect strategies, one popular encoding is the one-shot architecture \citep{bender2018understanding, brock2017smash}. One single large network resuming all candidates from the search space is trained. Then the architectures are found by pruning some branches. Only the best promising architectures are retrained from scratch.

\begin{figure*}[htbp]
\centering
    \begin{tikzpicture}
        \draw node[] (SE) {Solution Encoding};
        \draw node[below left =0.5cm and 1.8cm of SE] (Direct) {Direct};
        \draw node[below right =0.5cm and 1.8cm of SE] (Indirect) {Indirect};
        \draw node[below left =0.5cm and 2cm of Direct] (Flat) {Flat};
        \draw node[below right =0.5cm and 2cm of Direct] (Hierarchical) {Hierarchical};
        \draw node[below left =0.5cm and 0cm of Flat, align=center] (Chain) {Chain\\structured};
        \draw node[below right =0.5cm and 0cm of Flat, align=center] (Extended) {Extended\\Flat DNNs};
        \draw node[below =0.5cm of Hierarchical, align=center] (Outer) {Outer\\Level fixed};
        \draw node[left =0.5cm of Outer, align=center] (Inner) {Inner\\Level Fixed};
        \draw node[right =0.5cm of Outer, align=center] (No) {No level\\fixed};
        \draw node[below =of Indirect, align=center] (One) {One-shot};
        \draw[->] (SE) -- (Direct);
        \draw[->] (SE) -- (Indirect);
        \draw[->] (Direct) -- (Flat);
        \draw[->] (Direct) -- (Hierarchical);
        \draw[->] (Flat) -- (Chain);
        \draw[->] (Flat) -- (Extended);
        \draw[->] (Hierarchical) -- (Inner);
        \draw[->] (Hierarchical) -- (Outer);
        \draw[->] (Hierarchical) -- (No);
        \draw[->] (Indirect) -- (One);
    \end{tikzpicture}
    \caption{Classification of encoding strategies for NAS \citep{talbi2021automated}.}
    \label{fig:classi_encoding}
\end{figure*}

Our search space can be categorized as a direct and extended flat encoding. Each layer is individually encoded by our search space. It is more flexible than the search spaces designed in literature. First, we tackle both the optimization of the architecture and the hyperparameters. Second, the diversity of candidate DNNs is much broader than what can be found in the literature. We allow a combination of recurrent, convolution, attention-based and fully connected layers, leading to innovative, original yet well-performing DNNs. To our knowledge, this encoding has never been investigated in the literature. 
% Finally, our graphs have limited constraints to be valid, leading to new arrangements of layers within the networks.

%**************************************************
\subsection{AutoML for time series forecasting}
%**************************************************

The automated design of DNNs called Automated Deep Learning (AutoDL), belongs to a larger field \citep{hutter2019automated} called Automated Machine Learning (AutoML). AutoML aims to automatically design well-performing machine learning pipelines, for a given task. Works on model optimization for time series forecasting mainly focused on AutoML rather than AutoDL \citep{alsharef2022review}. The optimization can be performed at several levels: input features selection, extraction and engineering, model selection and hyperparameters tuning. Initial research works used to focus on one of these subproblems, while more recent works offer complete optimization pipelines.

The first subproblems, input features selection, extraction and engineering, are specific to our learning task: time series forecasting. This tedious task can significantly improve the prediction scores by giving the model relevant information about the data. Methods to select the features are among computing the importance of each feature on the results or using statistical tools on the signals to extract relevant information. Next, the model selection aims at choosing among a set of diverse machine learning models the best-performing one on a given task. Often, the models are trained separately, and the best model is chosen. In general, the selected model has many hyperparameters, such as the number of hidden layers, activation function or learning rate. Their optimization usually allows for improving the performance of the model.

Nowadays, many research works implement complete optimization pipelines combining those subproblems for time series forecasting. The Time Series Pipeline Optimization framework \citep{dahl2020tspo}, is based on an evolutionary algorithm to automatically find the right features thanks to input signal analysis, then the model and its related hyperparameters. AutoAI-TS \citep{shah2021autoai} is also a complete optimization pipeline, with model selection performed among a wide assortment of models: statistical models, machine learning, deep learning models and hybrids models. Finally, the framework Auto-Pytorch-TS \citep{deng2022efficient} is specific to deep learning models optimization for time series forecasting. The framework uses Bayesian optimization with multi-fidelity optimization.

Except for AutoPytorch-TS, cited works covering the entire optimization pipeline for time series do not deepen model optimization and only perform model selection and hyperparameters optimization. However, time series data becomes more complex, and there is a growing need for more sophisticated and data-specific DNNs. In this work, we only tackle the model selection and hyperparameters optimization parts of the pipeline. We made this choice to show the effectiveness of our framework for designing better DNNs. If we had implemented feature selection, it would have been harder to determine whether the superiority of our results came from the input features pool or the model itself. We discuss this further in Section \ref{part5}.