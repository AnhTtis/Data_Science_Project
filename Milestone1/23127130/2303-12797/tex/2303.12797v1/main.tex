\documentclass{article}
\include{config}


\usepackage{PRIMEarxiv}



%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Optimization framework for AutoDL}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters} 

\author{
  Julie Keisler \\
  EDF Lab Paris-Saclay\\
  University of Lille \& INRIA\\
  \texttt{julie.keisler@edf.fr} \\
  %% examples of more authors
   \And
  El-Ghazali Talbi \\
  University of Lille \& INRIA\\
  \texttt{el-ghazali.talbi@univ-lille.fr} \\
  \And
   Sandra Claudel \\
   EDF Lab Paris-Saclay \\
   \texttt{sandra.claudel@edf.fr} \\
   \And
   Gilles Cabriel \\
   EDF Lab Paris-Saclay \\
   \texttt{gilles.cabriel@edf.fr} 
}


\begin{document}
\maketitle


\begin{abstract}
In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.
\end{abstract}


\keywords{Metaheuristics \and Evolutionary Algorithm \and AutoML \and Neural Architecture Search \and Hyperparameter optimization \and Directed Acyclic Graphs \and Time Series Forecasting}


\input{1-introduction.tex}
\input{2-related_work.tex}
\input{3-search_space_design.tex}
\input{4-search_algorithm.tex}
\input{5-experiments.tex}
\input{6-conclusion_further_work.tex}



\section*{Acknowledgments}
This work has been funded by Electricit√© de France (EDF). The supercomputers used to run the experiments belong to EDF.
\newpage

\appendix
\input{7-Appendix.tex}
\newpage
%Bibliography
%\bibliographystyle{unsrt}  
\bibliographystyle{plainnat}
\bibliography{references}  


\end{document}
