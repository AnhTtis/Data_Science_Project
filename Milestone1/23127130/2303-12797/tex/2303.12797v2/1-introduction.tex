\section{Introduction}

With the recent successes of deep learning in many research fields, deep neural networks (DNN) optimization stimulates the growing interest of the scientific community \citep{talbi2021automated}. While each new learning task requires the handcrafted design of a new DNN, automated deep learning facilitates the creation of powerful DNNs. Interests are to give access to deep learning to less experienced people, to reduce the tedious tasks of managing many parameters to reach the optimal DNN, and finally, to go beyond what humans can design by creating non-intuitive DNNs that can ultimately prove to be more efficient.

Optimizing a DNN means automatically finding an optimal architecture for a given learning task: choosing the operations and the connections between those operations and the associated hyperparameters. The first task is also known as Neural Architecture Search \citep{elsken2019neural}, also named NAS, and the second, as HyperParameters Optimization (HPO). Most works from the literature try to tackle only one of these two optimization problems. Many papers related to NAS \citep{white2021bananas,loni_deepmaker_2020, wang_evolving_2019, sun_particle_2018, zhong_dna_2020} focus on designing optimal architectures for computer vision tasks with stacked convolution and pooling layers. Because each DNN training is time-consuming, researchers tried to reduce the search space by adding many constraints preventing from finding irrelevant architectures. These strategies are relevant in the case of computer vision or NLP, where the models to be trained are huge and the high performance architectures are well identified. However, there is a gap in the literature regarding the use of NAS and HPO for problems where neural networks could be efficient, but the relevant models have not been clearly identified.

To fill this gap, we introduce DRAGON (for DiRected Acyclic Graphs OptimizatioN), a new optimization framework for DNNs based on the evolution of Directed Acyclic Graphs (DAGs). The encoding and the search operators are higlhy flexible and may be used with various deep learning and AutoML problems. We ran experiments on time series forecasting tasks and demonstrate on a large variety of datasets that DRAGON can find DNNs which outperform state-of-the-art handcrafted forecasters and AutoML frameworks. In summary, our contributions are as follows:

\begin{itemize}
\item The precise definition of a flexible search space based on DAGs, for the optimization of DNN architectures and hyperparameters. This search space may be used for various tasks, and is particularly useful when the performing architectures for a given problem are not clearly identified.

\item The design of efficient neighbourhoods and variation operators for DAGs. With these operators, any metaheuristic designed for a mixed and variable-size search space can be applied. In this paper, we investigate the use of an asynchronous evolutionary algorithm.

\item The validation of the algorithmic framework on a popular time series forecasting benchmark \citep{godahewa2021monash}. We compare ourselves with 15 handcrafted statistical and machine learning models \citep{godahewa2021monash} as well as 6 AutoML frameworks on 27 datasets \citep{shchur2023autogluon}. We show that DRAGON outperforms the 21 models from this baseline on 11 out of 27 datasets. The only competitive model is the AutoML framework AutoGluon \citep{shchur2023autogluon}, which outperforms the baseline on 10 out of 27 datasets and was beaten by DRAGON on 14 out of 27 datasets.
\end{itemize}

The paper is organized as follows: we review section~\ref{part 2}, the literature on deep learning models for time series forecasting and AutoML. Section~\ref{part 3} defines our search space. Section~\ref{part 4} presents our neighbourhoods and variation operators within the evolutionary algorithm. Section~\ref{part5} details our experimental results obtained on a popular time series forecasting benchmark. Finally, Section~\ref{part 6} gives a conclusion and introduces further research opportunities.