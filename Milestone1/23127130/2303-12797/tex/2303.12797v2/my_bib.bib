@article{NBeats,
  author    = {Boris N. Oreshkin and
               Dmitri Carpov and
               Nicolas Chapados and
               Yoshua Bengio},
  title     = {{N-BEATS:} Neural basis expansion analysis for interpretable time
               series forecasting},
  journal   = {CoRR},
  volume    = {abs/1905.10437},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.10437},
  eprinttype = {arXiv},
  eprint    = {1905.10437},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-10437.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{talbi2023,
  title={Metaheuristics for variable-size mixed optimization problems: a survey and taxonomy},
  author={El-Ghazali Talbi},
  journal={Submitted to IEEE Trans. on Evolutionary Algorithms},
  year={2023}
}

@article{oord2016wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}

@inproceedings{lea2017temporal,
  title={Temporal convolutional networks for action segmentation and detection},
  author={Lea, Colin and Flynn, Michael D and Vidal, Rene and Reiter, Austin and Hager, Gregory D},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={156--165},
  year={2017}
}

@article{seq2seq,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{salinas2020deepar,
  title={DeepAR: Probabilistic forecasting with autoregressive recurrent networks},
  author={Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
  journal={International Journal of Forecasting},
  volume={36},
  number={3},
  pages={1181--1191},
  year={2020},
  publisher={Elsevier}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{chung2014empirical,
  title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021}
}

@article{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}

@article{lim2021temporal,
  title={Temporal fusion transformers for interpretable multi-horizon time series forecasting},
  author={Lim, Bryan and Ar{\i}k, Sercan {\"O} and Loeff, Nicolas and Pfister, Tomas},
  journal={International Journal of Forecasting},
  volume={37},
  number={4},
  pages={1748--1764},
  year={2021},
  publisher={Elsevier}
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{cordonnier2019relationship,
  title={On the relationship between self-attention and convolutional layers},
  author={Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  journal={arXiv preprint arXiv:1911.03584},
  year={2019}
}

@article{talbi2021automated,
  title={Automated design of deep neural networks: A survey and unified taxonomy},
  author={Talbi, El-Ghazali},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={2},
  pages={1--37},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{assunccao2019denser,
  title={DENSER: deep evolutionary network structured representation},
  author={Assun{\c{c}}{\~a}o, Filipe and Louren{\c{c}}o, Nuno and Machado, Penousal and Ribeiro, Bernardete},
  journal={Genetic Programming and Evolvable Machines},
  volume={20},
  number={1},
  pages={5--35},
  year={2019},
  publisher={Springer}
}

@article{loni2020deepmaker,
  title={DeepMaker: A multi-objective optimization framework for deep neural networks in embedded systems},
  author={Loni, Mohammad and Sinaei, Sima and Zoljodi, Ali and Daneshtalab, Masoud and Sj{\"o}din, Mikael},
  journal={Microprocessors and Microsystems},
  volume={73},
  pages={102989},
  year={2020},
  publisher={Elsevier}
}

@article{sun2018particle,
  title={A particle swarm optimization-based flexible convolutional autoencoder for image classification},
  author={Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G},
  journal={IEEE transactions on neural networks and learning systems},
  volume={30},
  number={8},
  pages={2295--2309},
  year={2018},
  publisher={IEEE}
}
@inproceedings{wang2018evolving,
  title={Evolving deep convolutional neural networks by variable-length particle swarm optimization for image classification},
  author={Wang, Bin and Sun, Yanan and Xue, Bing and Zhang, Mengjie},
  booktitle={2018 IEEE Congress on Evolutionary Computation (CEC)},
  pages={1--8},
  year={2018},
  organization={IEEE}
}

@inproceedings{wang2019evolving,
  title={Evolving deep neural networks by multi-objective particle swarm optimization for image classification},
  author={Wang, Bin and Sun, Yanan and Xue, Bing and Zhang, Mengjie},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={490--498},
  year={2019}
}

@article{liu2017hierarchical,
  title={Hierarchical representations for efficient architecture search},
  author={Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1711.00436},
  year={2017}
}

@article{shu2019understanding,
  title={Understanding architectures learnt by cell-based neural architecture search},
  author={Shu, Yao and Wang, Wei and Cai, Shaofeng},
  journal={arXiv preprint arXiv:1909.09569},
  year={2019}
}

@article{zhang2019d,
  title={D-vae: A variational autoencoder for directed acyclic graphs},
  author={Zhang, Muhan and Jiang, Shali and Cui, Zhicheng and Garnett, Roman and Chen, Yixin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{pham2018efficient,
  title={Efficient neural architecture search via parameters sharing},
  author={Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle={International conference on machine learning},
  pages={4095--4104},
  year={2018},
  organization={PMLR}
}

@article{camero2021bayesian,
  title={Bayesian neural architecture search using a training-free performance metric},
  author={Camero, Andr{\'e}s and Wang, Hao and Alba, Enrique and B{\"a}ck, Thomas},
  journal={Applied Soft Computing},
  volume={106},
  pages={107356},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{white2021bananas,
  title={Bananas: Bayesian optimization with neural architectures for neural architecture search},
  author={White, Colin and Neiswanger, Willie and Savani, Yash},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={12},
  pages={10293--10301},
  year={2021}
}

@inproceedings{liu2019auto,
  title={Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation},
  author={Liu, Chenxi and Chen, Liang-Chieh and Schroff, Florian and Adam, Hartwig and Hua, Wei and Yuille, Alan L and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={82--92},
  year={2019}
}

@inproceedings{bender2018understanding,
  title={Understanding and simplifying one-shot architecture search},
  author={Bender, Gabriel and Kindermans, Pieter-Jan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={550--559},
  year={2018},
  organization={PMLR}
}

@article{brock2017smash,
  title={Smash: one-shot model architecture search through hypernetworks},
  author={Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
  journal={arXiv preprint arXiv:1708.05344},
  year={2017}
}

@article{alsharef2022review,
  title={Review of ML and AutoML solutions to forecast time-series data},
  author={Alsharef, Ahmad and Aggarwal, Karan and Kumar, Manoj and Mishra, Ashutosh and others},
  journal={Archives of Computational Methods in Engineering},
  pages={1--15},
  year={2022},
  publisher={Springer}
}

@phdthesis{dahl2020tspo,
  title={TSPO: an autoML approach to time series forecasting},
  author={Dahl, Siem Morten Johannes},
  year={2020}
}

@inproceedings{shah2021autoai,
  title={AutoAI-TS: AutoAI for time series forecasting},
  author={Shah, Syed Yousaf and Patel, Dhaval and Vu, Long and Dang, Xuan-Hong and Chen, Bei and Kirchner, Peter and Samulowitz, Horst and Wood, David and Bramble, Gregory and Gifford, Wesley M and others},
  booktitle={Proceedings of the 2021 International Conference on Management of Data},
  pages={2584--2596},
  year={2021}
}

@article{deng2022efficient,
  title={Efficient Automated Deep Learning for Time Series Forecasting},
  author={Deng, Difan and Karl, Florian and Hutter, Frank and Bischl, Bernd and Lindauer, Marius},
  journal={arXiv preprint arXiv:2205.05511},
  year={2022}
}

@incollection{fiore2013algebra,
  title={The algebra of directed acyclic graphs},
  author={Fiore, Marcelo and Devesas Campos, Marco},
  booktitle={Computation, Logic, Games, and Quantum Foundations. The Many Facets of Samson Abramsky},
  pages={37--51},
  year={2013},
  publisher={Springer}
}

@inproceedings{so2019evolved,
  title={The evolved transformer},
  author={So, David and Le, Quoc and Liang, Chen},
  booktitle={International Conference on Machine Learning},
  pages={5877--5886},
  year={2019},
  organization={PMLR}
}

@article{liu2018darts,
  title={Darts: Differentiable architecture search},
  author={Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  journal={arXiv preprint arXiv:1806.09055},
  year={2018}
}

@inproceedings{abu2015exact,
  title={An exact graph edit distance algorithm for solving pattern recognition problems},
  author={Abu-Aisheh, Zeina and Raveaux, Romain and Ramel, Jean-Yves and Martineau, Patrick},
  booktitle={4th International Conference on Pattern Recognition Applications and Methods 2015},
  year={2015}
}

@article{godahewa2021monash,
  title={Monash time series forecasting archive},
  author={Godahewa, Rakshitha and Bergmeir, Christoph and Webb, Geoffrey I and Hyndman, Rob J and Montero-Manso, Pablo},
  journal={arXiv preprint arXiv:2105.06643},
  year={2021}
}


@article{hyndman2006another,
  title={Another look at measures of forecast accuracy},
  author={Hyndman, Rob J and Koehler, Anne B},
  journal={International journal of forecasting},
  volume={22},
  number={4},
  pages={679--688},
  year={2006},
  publisher={Elsevier}
}

@article{elsken2019neural,
  title={Neural architecture search: A survey},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1997--2017},
  year={2019},
  publisher={JMLR. org}
}

@inproceedings{summers2021nondeterminism,
  title={Nondeterminism and instability in neural network optimization},
  author={Summers, Cecilia and Dinneen, Michael J},
  booktitle={International Conference on Machine Learning},
  pages={9913--9922},
  year={2021},
  organization={PMLR}
}

@ARTICLE{9461796,  author={Liu, Zhenyu and Zhu, Zhengtong and Gao, Jing and Xu, Cheng},  journal={IEEE Access},   title={Forecast Methods for Time Series Data: A Survey},   year={2021},  volume={9},  number={},  pages={91896-91912},  doi={10.1109/ACCESS.2021.3091162}}

@book{hutter2019automated,
  title={Automated machine learning: methods, systems, challenges},
  author={Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year={2019},
  publisher={Springer Nature}
}
@article{DBLP:journals/corr/abs-1909-09569,
  title     = {Understanding Architectures Learnt by Cell-based Neural Architecture
               Search},
               author={Shu, Yao, Wei Wang and Shaofeng Cai},
  journal   = {CoRR},
  volume    = {abs/1909.09569},
  year      = {2019},
  note      = {Withdrawn.},
  url       = {http://arxiv.org/abs/1909.09569},
  eprinttype = {arXiv},
  eprint    = {1909.09569},
  timestamp = {Tue, 15 Oct 2019 13:34:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-09569.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{li2022survey,
  title={Survey on Evolutionary Deep Learning: Principles, Algorithms, Applications and Open Issues},
  author={Li, Nan and Ma, Lianbo and Yu, Guo and Xue, Bing and Zhang, Mengjie and Jin, Yaochu},
  journal={arXiv preprint arXiv:2208.10658},
  year={2022}
}

@inproceedings{
Dong2020NAS-Bench-201:,
title={NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search},
author={Xuanyi Dong and Yi Yang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJxyZkBKDr}
}


@article{zhong_dna_2020,
	title = {{DNA} computing inspired deep networks design},
	language = {en},
	author = {Zhong, Guoqiang},
	year = {2020},
	pages = {8},
	file = {Zhong - 2020 - DNA computing inspired deep networks design.pdf:/home/b98181/Documents/CIFRE/Papiers/Zhong - 2020 - DNA computing inspired deep networks design.pdf:application/pdf},
}

@article{jie_differentiable_2021,
	title = {Differentiable {Neural} {Architecture} {Search} for {High}-{Dimensional} {Time} {Series} {Forecasting}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9340253/},
	doi = {10.1109/ACCESS.2021.3055555},
	abstract = {This study applies neural architecture search (NAS) techniques to the modeling of highdimensional time series data such as multi-variate stock indices. It is known that traditional NAS method applies fully connected directed acyclic graph (DAG) for searching cell structures that requires high computational cost and cannot include the two-input operations such as the Hadamard product. To address the drawback of the DAG backbone, a novel two-input backbone cell architecture for recurrent neural networks is proposed, in which each candidate operation is also carried out with two inputs. Instead of using DAG, we simplify the backbone by considering the prior knowledge as an effective backbone such as preserving identity mappings. The cell structures will be incorporated in different types of model architectures including stacked long short-term memory (LSTM), gated recurrent unit (GRU) and attention-based encoder-decoder models. The experimental results on BRICS, G7 and G20 indices indicate that models with recurrent neural network (RNN) cells searched by the proposed backbone structure can signiﬁcantly outperform baseline models including autoregressive integrated moving average model (ARIMA), vector autoregression (VAR), and stacked LSTM/GRUs. For neural architecture search, the proposed backbone is shown to be more effective compared to the classic differentiable architecture search (DARTS) in both uni-variate and multi-variate time series prediction tasks. Further analysis demonstrates that the pruned cells of the proposed backbone usually contains the Hadamard product introduced as a twoinput operation, while the number of parameters involved in these pruned cells is on the same order with the baseline cells.},
	language = {en},
	urldate = {2022-05-02},
	journal = {IEEE Access},
	author = {Jie, Renlong and Gao, Junbin},
	year = {2021},
	pages = {20922--20932},
	file = {Jie et Gao - 2021 - Differentiable Neural Architecture Search for High.pdf:/home/b98181/Documents/CIFRE/Papiers/Jie et Gao - 2021 - Differentiable Neural Architecture Search for High.pdf:application/pdf},
}

@InProceedings{pmlr-v80-bender18a,
  title = 	 {Understanding and Simplifying One-Shot Architecture Search},
  author =       {Bender, Gabriel and Kindermans, Pieter-Jan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {550--559},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/bender18a/bender18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/bender18a.html},
  abstract = 	 {There is growing interest in automating neural network architecture design. Existing architecture search methods can be computationally expensive, requiring thousands of different architectures to be trained from scratch. Recent work has explored <em>weight sharing</em> across models to amortize the cost of training. Although previous methods reduced the cost of architecture search by orders of magnitude, they remain complex, requiring hypernetworks or reinforcement learning controllers. We aim to understand weight sharing for one-shot architecture search. With careful experimental analysis, we show that it is possible to efficiently identify promising architectures from a complex search space without either hypernetworks or RL.}
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	url = {http://arxiv.org/abs/1512.00567},
	abstract = {Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efﬁciency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efﬁciently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classiﬁcation challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error and 17.3\% top-1 error.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1512.00567 [cs]},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.00567},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer.pdf:/home/b98181/Documents/CIFRE/Papiers/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer.pdf:application/pdf},
}

@article{zoph_neural_2017,
	title = {Neural {Architecture} {Search} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1611.01578},
	abstract = {Neural networks are powerful and ﬂexible models that work well for many difﬁcult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1611.01578 [cs]},
	author = {Zoph, Barret and Le, Quoc V.},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.01578},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Zoph et Le - 2017 - Neural Architecture Search with Reinforcement Lear.pdf:/home/b98181/Documents/CIFRE/Papiers/Zoph et Le - 2017 - Neural Architecture Search with Reinforcement Lear.pdf:application/pdf},
}

@article{zoph_learning_2018,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1707.07012},
	abstract = {Developing neural network image classiﬁcation models often requires signiﬁcant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the “NASNet search space”) which enables transferability. In our experiments, we search for the best convolutional layer (or “cell”) on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a “NASNet architecture”. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4\% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS – a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classiﬁcation are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1707.07012 [cs, stat]},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	month = apr,
	year = {2018},
	note = {arXiv: 1707.07012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zoph et al. - 2018 - Learning Transferable Architectures for Scalable I.pdf:/home/b98181/Documents/CIFRE/Papiers/Zoph et al. - 2018 - Learning Transferable Architectures for Scalable I.pdf:application/pdf},
}

@article{brock_smash_2017,
	title = {{SMASH}: {One}-{Shot} {Model} {Architecture} {Search} through {HyperNetworks}},
	shorttitle = {{SMASH}},
	url = {http://arxiv.org/abs/1708.05344},
	abstract = {Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model’s architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a ﬂexible mechanism based on memory read-writes that allows us to deﬁne a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized handdesigned networks.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1708.05344 [cs]},
	author = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.05344},
	keywords = {Computer Science - Machine Learning},
	file = {Brock et al. - 2017 - SMASH One-Shot Model Architecture Search through .pdf:/home/b98181/Documents/CIFRE/Papiers/Brock et al. - 2017 - SMASH One-Shot Model Architecture Search through .pdf:application/pdf},
}

@article{liu_hierarchical_2018,
	title = {Hierarchical {Representations} for {Efficient} {Architecture} {Search}},
	url = {http://arxiv.org/abs/1711.00436},
	abstract = {We explore efﬁcient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efﬁciently discovers architectures that outperform a large number of manually designed models for image classiﬁcation, obtaining top-1 error of 3.6\% on CIFAR-10 and 20.3\% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3\% less top-1 accuracy on CIFAR-10 and 0.1\% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1711.00436 [cs, stat]},
	author = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
	month = feb,
	year = {2018},
	note = {arXiv: 1711.00436},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted as a conference paper at ICLR 2018},
	file = {Liu et al. - 2018 - Hierarchical Representations for Efficient Archite.pdf:/home/b98181/Documents/CIFRE/Papiers/Liu et al. - 2018 - Hierarchical Representations for Efficient Archite.pdf:application/pdf},
}

@article{elsken_simple_2017,
	title = {Simple {And} {Efficient} {Architecture} {Search} for {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.04528},
	abstract = {Neural networks have recently had a lot of success for many tasks. However, neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process. We propose a new method to automatically search for well-performing CNN architectures based on a simple hill climbing procedure whose operators apply network morphisms, followed by short optimization runs by cosine annealing. Surprisingly, this simple method yields competitive results, despite only requiring resources in the same order of magnitude as training a single network. E.g., on CIFAR-10, our method designs and trains networks with an error rate below 6\% in only 12 hours on a single GPU; training for one day reduces this error further, to almost 5\%.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1711.04528 [cs, stat]},
	author = {Elsken, Thomas and Metzen, Jan-Hendrik and Hutter, Frank},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.04528},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Under review as a conference paper at ICLR 2018},
	file = {Elsken et al. - 2017 - Simple And Efficient Architecture Search for Convo.pdf:/home/b98181/Documents/CIFRE/Papiers/Elsken et al. - 2017 - Simple And Efficient Architecture Search for Convo.pdf:application/pdf},
}

@article{sun_particle_2018,
	title = {A {Particle} {Swarm} {Optimization}-based {Flexible} {Convolutional} {Auto}-{Encoder} for {Image} {Classification}},
	url = {http://arxiv.org/abs/1712.05042},
	doi = {10.1109/TNNLS.2018.2881143},
	abstract = {Convolutional auto-encoders have shown their remarkable performance in stacking to deep convolutional neural networks for classifying image data during the past several years. However, they are unable to construct the state-of-the-art convolutional neural networks due to their intrinsic architectures. In this regard, we propose a ﬂexible convolutional auto-encoder by eliminating the constraints on the numbers of convolutional layers and pooling layers from the traditional convolutional autoencoder. We also design an architecture discovery method by exploiting particle swarm optimization, which is capable of automatically searching for the optimal architectures of the proposed ﬂexible convolutional auto-encoder with much less computational resource and without any manual intervention. We test the proposed approach on four extensively used image classiﬁcation datasets. Experimental results show that our proposed approach in this paper signiﬁcantly outperforms the peer competitors including the state-of-the-art algorithms.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1712.05042 [cs]},
	author = {Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G.},
	month = nov,
	year = {2018},
	note = {arXiv: 1712.05042},
	annote = {Comment: Accepted by IEEE Transactions on Neural Networks and Learning Systems, 2018},
	file = {Sun et al. - 2018 - A Particle Swarm Optimization-based Flexible Convo.pdf:/home/b98181/Documents/CIFRE/Papiers/Sun et al. - 2018 - A Particle Swarm Optimization-based Flexible Convo.pdf:application/pdf},
}

@article{assuncao_denser_2018,
	title = {{DENSER}: {Deep} {Evolutionary} {Network} {Structured} {Representation}},
	shorttitle = {{DENSER}},
	url = {http://arxiv.org/abs/1801.01563},
	doi = {10.1007/s10710-018-9339-y},
	abstract = {Deep Evolutionary Network Structured Representation (DENSER) is a novel approach to automatically design Artiﬁcial Neural Networks (ANNs) using Evolutionary Computation. The algorithm not only searches for the best network topology (e.g., number of layers, type of layers), but also tunes hyper-parameters, such as, learning parameters or data augmentation parameters. The automatic design is achieved using a representation with two distinct levels, where the outer level encodes the general structure of the network, i.e., the sequence of layers, and the inner level encodes the parameters associated with each layer. The allowed layers and range of the hyper-parameters values are deﬁned by means of a human-readable Context-Free Grammar. DENSER was used to evolve ANNs for CIFAR-10, obtaining an average test accuracy of 94.13\%. The networks evolved for the CIFAR-10 are tested on the MNIST, Fashion-MNIST, and CIFAR-100; the results are highly competitive, and on the CIFAR-100 we report a test accuracy of 78.75\%. To the best of our knowledge, our CIFAR-100 results are the highest performing models generated by methods that aim at the automatic design of Convolutional Neural Networks (CNNs), and are amongst the best for manually designed and ﬁne-tuned CNNs.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1801.01563 [cs]},
	author = {Assunção, Filipe and Lourenço, Nuno and Machado, Penousal and Ribeiro, Bernardete},
	month = jun,
	year = {2018},
	note = {arXiv: 1801.01563},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Assunção et al. - 2018 - DENSER Deep Evolutionary Network Structured Repre.pdf:/home/b98181/Documents/CIFRE/Papiers/Assunção et al. - 2018 - DENSER Deep Evolutionary Network Structured Repre.pdf:application/pdf},
}

@article{real_regularized_2019,
	title = {Regularized {Evolution} for {Image} {Classifier} {Architecture} {Search}},
	url = {http://arxiv.org/abs/1802.01548},
	abstract = {The effort devoted to hand-crafting neural network image classiﬁers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classiﬁers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classiﬁer—AmoebaNet-A—that surpasses hand-designs for the ﬁrst time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9\% top-1 / 96.6\% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1802.01548 [cs]},
	author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
	month = feb,
	year = {2019},
	note = {arXiv: 1802.01548},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Distributed, Parallel, and Cluster Computing, I.2.6, I.5.1, I.5.2},
	annote = {Comment: Accepted for publication at AAAI 2019, the Thirty-Third AAAI Conference on Artificial Intelligence},
	file = {Real et al. - 2019 - Regularized Evolution for Image Classifier Archite.pdf:/home/b98181/Documents/CIFRE/Papiers/Real et al. - 2019 - Regularized Evolution for Image Classifier Archite.pdf:application/pdf},
}

@article{wang_evolving_2018,
	title = {Evolving {Deep} {Convolutional} {Neural} {Networks} by {Variable}-length {Particle} {Swarm} {Optimization} for {Image} {Classification}},
	url = {http://arxiv.org/abs/1803.06492},
	abstract = {Convolutional neural networks (CNNs) are one of the most effective deep learning methods to solve image classiﬁcation problems, but the best architecture of a CNN to solve a speciﬁc problem can be extremely complicated and hard to design. This paper focuses on utilising Particle Swarm Optimisation (PSO) to automatically search for the optimal architecture of CNNs without any manual work involved. In order to achieve the goal, three improvements are made based on traditional PSO. First, a novel encoding strategy inspired by computer networks which empowers particle vectors to easily encode CNN layers is proposed; Second, in order to allow the proposed method to learn variable-length CNN architectures, a Disabled layer is designed to hide some dimensions of the particle vector to achieve variable-length particles; Third, since the learning process on large data is slow, partial datasets are randomly picked for the evaluation to dramatically speed it up. The proposed algorithm is examined and compared with 12 existing algorithms including the state-of-art methods on three widely used image classiﬁcation benchmark datasets. The experimental results show that the proposed algorithm is a strong competitor to the state-of-art algorithms in terms of classiﬁcation error. This is the ﬁrst work using PSO for automatically evolving the architectures of CNNs.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1803.06492 [cs]},
	author = {Wang, Bin and Sun, Yanan and Xue, Bing and Zhang, Mengjie},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.06492},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: accepted by IEEE CEC 2018},
	file = {Wang et al. - 2018 - Evolving Deep Convolutional Neural Networks by Var.pdf:/home/b98181/Documents/CIFRE/Papiers/Wang et al. - 2018 - Evolving Deep Convolutional Neural Networks by Var.pdf:application/pdf},
}

@article{cai_path-level_2018,
	title = {Path-{Level} {Network} {Transformation} for {Efficient} {Architecture} {Search}},
	url = {http://arxiv.org/abs/1806.02639},
	abstract = {We introduce a new function-preserving transformation for efﬁcient neural architecture search. This network transformation allows reusing previously trained networks and existing successful architectures that improves sample efﬁciency. We aim to address the limitation of current network transformation operations that can only perform layer-level architecture modiﬁcations, such as adding (pruning) ﬁlters or inserting (removing) a layer, which fails to change the topology of connection paths. Our proposed path-level transformation operations enable the meta-controller to modify the path topology of the given network while keeping the merits of reusing weights, and thus allow efﬁciently designing effective structures with complex path topologies like Inception models. We further propose a bidirectional treestructured reinforcement learning meta-controller to explore a simple yet highly expressive treestructured architecture space that can be viewed as a generalization of multi-branch architectures. We experimented on the image classiﬁcation datasets with limited computational resources (about 200 GPU-hours), where we observed improved parameter efﬁciency and better test results (97.70\% test accuracy on CIFAR-10 with 14.3M parameters and 74.6\% top-1 accuracy on ImageNet in the mobile setting), demonstrating the effectiveness and transferability of our designed architectures.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1806.02639 [cs, stat]},
	author = {Cai, Han and Yang, Jiacheng and Zhang, Weinan and Han, Song and Yu, Yong},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.02639},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2018},
	file = {Cai et al. - 2018 - Path-Level Network Transformation for Efficient Ar.pdf:/home/b98181/Documents/CIFRE/Papiers/Cai et al. - 2018 - Path-Level Network Transformation for Efficient Ar.pdf:application/pdf},
}


@article{chen_scale-aware_2021,
	title = {Scale-{Aware} {Neural} {Architecture} {Search} for {Multivariate} {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/2112.07459},
	abstract = {Multivariate time series (MTS) forecasting has attracted much attention in many intelligent applications. It is not a trivial task, as we need to consider both intra-variable dependencies and inter-variable dependencies. However, existing works are designed for speciﬁc scenarios, and require much domain knowledge and expert efforts, which is difﬁcult to transfer between different scenarios. In this paper, we propose a scale-aware neural architecture search framework for MTS forecasting (SNAS4MTF). A multi-scale decomposition module transforms raw time series into multi-scale sub-series, which can preserve multi-scale temporal patterns. An adaptive graph learning module infers the different inter-variable dependencies under different time scales without any prior knowledge. For MTS forecasting, a search space is designed to capture both intra-variable dependencies and inter-variable dependencies at each time scale. The multi-scale decomposition, adaptive graph learning, and neural architecture search modules are jointly learned in an end-to-end framework. Extensive experiments on two real-world datasets demonstrate that SNAS4MTF achieves a promising performance compared with the state-of-the-art methods.},
	language = {en},
	urldate = {2022-05-02},
	journal = {arXiv:2112.07459 [cs]},
	author = {Chen, Donghui and Chen, Ling and Shang, Zongjiang and Zhang, Youdong and Wen, Bo and Yang, Chenghu},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.07459},
	keywords = {Interesting, Not read, Read},
	file = {Chen et al. - 2021 - Scale-Aware Neural Architecture Search for Multiva.pdf:/home/b98181/Documents/CIFRE/Papiers/Chen et al. - 2021 - Scale-Aware Neural Architecture Search for Multiva.pdf:application/pdf},
}

@article{tan_mnasnet_2019,
	title = {{MnasNet}: {Platform}-{Aware} {Neural} {Architecture} {Search} for {Mobile}},
	shorttitle = {{MnasNet}},
	url = {http://arxiv.org/abs/1807.11626},
	abstract = {Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although signiﬁcant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difﬁcult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between ﬂexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classiﬁcation task, our MnasNet achieves 75.2\% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8× faster than MobileNetV2 [29] with 0.5\% higher accuracy and 2.3× faster than NASNet [36] with 1.2\% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/ tree/master/models/official/mnasnet.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1807.11626 [cs]},
	author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V.},
	month = may,
	year = {2019},
	note = {arXiv: 1807.11626},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Published in CVPR 2019},
	file = {Tan et al. - 2019 - MnasNet Platform-Aware Neural Architecture Search.pdf:/home/b98181/Documents/CIFRE/Papiers/Tan et al. - 2019 - MnasNet Platform-Aware Neural Architecture Search.pdf:application/pdf},
}

@article{lu_nsga-net_2019,
	title = {{NSGA}-{Net}: {Neural} {Architecture} {Search} using {Multi}-{Objective} {Genetic} {Algorithm}},
	shorttitle = {{NSGA}-{Net}},
	url = {http://arxiv.org/abs/1810.03522},
	abstract = {This paper introduces NSGA-Net – an evolutionary approach for neural architecture search (NAS). NSGA-Net is designed with three goals in mind: (1) a procedure considering multiple and conflicting objectives, (2) an efficient procedure balancing exploration and exploitation of the space of potential neural network architectures, and (3) a procedure finding a diverse set of trade-off network architectures achieved in a single run. NSGA-Net is a population-based search algorithm that explores a space of potential neural network architectures in three steps, namely, a population initialization step that is based on prior-knowledge from hand-crafted architectures, an exploration step comprising crossover and mutation of architectures, and finally an exploitation step that utilizes the hidden useful knowledge stored in the entire history of evaluated neural architectures in the form of a Bayesian Network. Experimental results suggest that combining the dual objectives of minimizing an error metric and computational complexity, as measured by FLOPs, allows NSGA-Net to find competitive neural architectures. Moreover, NSGA-Net achieves error rate on the CIFAR-10 dataset on par with other state-of-the-art NAS methods while using orders of magnitude less computational resources. These results are encouraging and shows the promise to further use of EC methods in various deep-learning paradigms.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1810.03522 [cs]},
	author = {Lu, Zhichao and Whalen, Ian and Boddeti, Vishnu and Dhebar, Yashesh and Deb, Kalyanmoy and Goodman, Erik and Banzhaf, Wolfgang},
	month = apr,
	year = {2019},
	note = {arXiv: 1810.03522},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: GECCO 2019},
	file = {Lu et al. - 2019 - NSGA-Net Neural Architecture Search using Multi-O.pdf:/home/b98181/Documents/CIFRE/Papiers/Lu et al. - 2019 - NSGA-Net Neural Architecture Search using Multi-O.pdf:application/pdf},
}

@article{wang_evolving_2019,
	title = {Evolving {Deep} {Neural} {Networks} by {Multi}-objective {Particle} {Swarm} {Optimization} for {Image} {Classification}},
	url = {http://arxiv.org/abs/1904.09035},
	abstract = {In recent years, convolutional neural networks (CNNs) have become deeper in order to achieve better classification accuracy in image classification. However, it is difficult to deploy the stateof-the-art deep CNNs for industrial use due to the difficulty of manually fine-tuning the hyperparameters and the trade-off between classification accuracy and computational cost. This paper proposes a novel multi-objective optimization method for evolving state-of-the-art deep CNNs in real-life applications, which automatically evolves the non-dominant solutions at the Pareto front. Three major contributions are made: Firstly, a new encoding strategy is designed to encode one of the best state-of-the-art CNNs; With the classification accuracy and the number of floating point operations as the two objectives, a multi-objective particle swarm optimization method is developed to evolve the non-dominant solutions; Last but not least, a new infrastructure is designed to boost the experiments by concurrently running the experiments on multiple GPUs across multiple machines, and a Python library is developed and released to manage the infrastructure. The experimental results demonstrate that the non-dominant solutions found by the proposed method form a clear Pareto front, and the proposed infrastructure is able to almost linearly reduce the running time.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1904.09035 [cs]},
	author = {Wang, Bin and Sun, Yanan and Xue, Bing and Zhang, Mengjie},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.09035},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: conditionally accepted by gecco2019},
	file = {Wang et al. - 2019 - Evolving Deep Neural Networks by Multi-objective P.pdf:/home/b98181/Documents/CIFRE/Papiers/Wang et al. - 2019 - Evolving Deep Neural Networks by Multi-objective P.pdf:application/pdf},
}

@article{white_bananas_2020,
	title = {{BANANAS}: {Bayesian} {Optimization} with {Neural} {Architectures} for {Neural} {Architecture} {Search}},
	shorttitle = {{BANANAS}},
	url = {http://arxiv.org/abs/1910.11858},
	abstract = {Over the past half-decade, many methods have been considered for neural architecture search (NAS). Bayesian optimization (BO), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for NAS when it is coupled with a neural predictor. Recent work has proposed diﬀerent instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within BO. However, the analyses in these papers often focus on the full-ﬂedged NAS algorithm, so it is diﬃcult to tell which individual components of the framework lead to the best performance.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1910.11858 [cs, stat]},
	author = {White, Colin and Neiswanger, Willie and Savani, Yash},
	month = nov,
	year = {2020},
	note = {arXiv: 1910.11858},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {White et al. - 2020 - BANANAS Bayesian Optimization with Neural Archite.pdf:/home/b98181/Documents/CIFRE/Papiers/White et al. - 2020 - BANANAS Bayesian Optimization with Neural Archite.pdf:application/pdf},
}

@article{camero_bayesian_2021,
	title = {Bayesian {Neural} {Architecture} {Search} using {A} {Training}-{Free} {Performance} {Metric}},
	volume = {106},
	issn = {15684946},
	url = {http://arxiv.org/abs/2001.10726},
	doi = {10.1016/j.asoc.2021.107356},
	abstract = {Recurrent neural networks (RNNs) are a powerful approach for time series prediction. However, their performance is strongly aﬀected by their architecture and hyperparameter settings. The architecture optimization of RNNs is a time-consuming task, where the search space is typically a mixture of real, integer and categorical values. To allow for shrinking and expanding the size of the network, the representation of architectures often has a variable length. In this paper, we propose to tackle the architecture optimization problem with a variant of the Bayesian Optimization (BO) algorithm. To reduce the evaluation time of candidate architectures the Mean Absolute Error Random Sampling (MRS), a training-free method to estimate the network performance, is adopted as the objective function for BO. Also, we propose three ﬁxed-length encoding schemes to cope with the variable-length architecture representation. The result is a new perspective on accurate and eﬃcient design of RNNs, that we validate on three problems. Our ﬁndings show that 1) the BO algorithm can explore diﬀerent network architectures using the proposed encoding schemes and successfully designs well-performing architectures, and 2) the optimization time is signiﬁcantly reduced by using MRS, without compromising the performance as compared to the architectures obtained from the actual training procedure.},
	language = {en},
	urldate = {2022-04-26},
	journal = {Applied Soft Computing},
	author = {Camero, Andrés and Wang, Hao and Alba, Enrique and Bäck, Thomas},
	month = jul,
	year = {2021},
	note = {arXiv: 2001.10726},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {107356},
	file = {Camero et al. - 2021 - Bayesian Neural Architecture Search using A Traini.pdf:/home/b98181/Documents/CIFRE/Papiers/Camero et al. - 2021 - Bayesian Neural Architecture Search using A Traini.pdf:application/pdf},
}

@article{liu_auto-deeplab_2019,
	title = {Auto-{DeepLab}: {Hierarchical} {Neural} {Architecture} {Search} for {Semantic} {Image} {Segmentation}},
	shorttitle = {Auto-{DeepLab}},
	url = {http://arxiv.org/abs/1901.02985},
	abstract = {Recently, Neural Architecture Search (NAS) has successfully identified neural network architectures that exceed human designed ones on large-scale image classification. In this paper, we study NAS for semantic image segmentation. Existing works often focus on searching the repeatable cell structure, while hand-designing the outer network structure that controls the spatial resolution changes. This choice simplifies the search space, but becomes increasingly problematic for dense image prediction which exhibits a lot more network level architectural variations. Therefore, we propose to search the network level structure in addition to the cell level structure, which forms a hierarchical architecture search space. We present a network level search space that includes many popular designs, and develop a formulation that allows efficient gradient-based architecture search (3 P100 GPU days on Cityscapes images). We demonstrate the effectiveness of the proposed method on the challenging Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our architecture searched specifically for semantic image segmentation, attains state-of-the-art performance without any ImageNet pretraining.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1901.02985 [cs]},
	author = {Liu, Chenxi and Chen, Liang-Chieh and Schroff, Florian and Adam, Hartwig and Hua, Wei and Yuille, Alan and Fei-Fei, Li},
	month = apr,
	year = {2019},
	note = {arXiv: 1901.02985},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: To appear in CVPR 2019 as oral. Code for Auto-DeepLab released at https://github.com/tensorflow/models/tree/master/research/deeplab},
	file = {Liu et al. - 2019 - Auto-DeepLab Hierarchical Neural Architecture Sea.pdf:/home/b98181/Documents/CIFRE/Papiers/Liu et al. - 2019 - Auto-DeepLab Hierarchical Neural Architecture Sea.pdf:application/pdf},
}

@article{liu_darts_2019,
	title = {{DARTS}: {Differentiable} {Architecture} {Search}},
	shorttitle = {{DARTS}},
	url = {http://arxiv.org/abs/1806.09055},
	abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efﬁcient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classiﬁcation and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efﬁcient architecture search algorithms.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1806.09055 [cs, stat]},
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	month = apr,
	year = {2019},
	note = {arXiv: 1806.09055},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Published at ICLR 2019; Code and pretrained models available at https://github.com/quark0/darts},
	file = {Liu et al. - 2019 - DARTS Differentiable Architecture Search.pdf:/home/b98181/Documents/CIFRE/Papiers/Liu et al. - 2019 - DARTS Differentiable Architecture Search.pdf:application/pdf},
}

@article{yu_evaluating_2019,
	title = {Evaluating the {Search} {Phase} of {Neural} {Architecture} {Search}},
	url = {http://arxiv.org/abs/1902.08142},
	abstract = {Neural Architecture Search (NAS) aims to facilitate the design of deep networks for new tasks. Existing techniques rely on two stages: searching over the architecture space and validating the best architecture. NAS algorithms are currently compared solely based on their results on the downstream task. While intuitive, this fails to explicitly evaluate the effectiveness of their search strategies. In this paper, we propose to evaluate the NAS search phase. To this end, we compare the quality of the solutions obtained by NAS search policies with that of random architecture selection. We ﬁnd that: (i) On average, the state-of-the-art NAS algorithms perform similarly to the random policy; (ii) the widely-used weight sharing strategy degrades the ranking of the NAS candidates to the point of not reﬂecting their true performance, thus reducing the effectiveness of the search process. We believe that our evaluation framework will be key to designing NAS strategies that consistently discover architectures superior to random ones.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1902.08142 [cs, stat]},
	author = {Yu, Kaicheng and Sciuto, Christian and Jaggi, Martin and Musat, Claudiu and Salzmann, Mathieu},
	month = nov,
	year = {2019},
	note = {arXiv: 1902.08142},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: We find that random policy in NAS works amazingly well and propose an evaluation framework to have a fair comparison. Adding additional results on standard CNN search space used for weight sharing and NASBench-101. 8 pages},
	file = {Yu et al. - 2019 - Evaluating the Search Phase of Neural Architecture.pdf:/home/b98181/Documents/CIFRE/Papiers/Yu et al. - 2019 - Evaluating the Search Phase of Neural Architecture.pdf:application/pdf},
}

@article{fielding_evolving_2018,
	title = {Evolving {Image} {Classification} {Architectures} {With} {Enhanced} {Particle} {Swarm} {Optimisation}},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8533601/},
	doi = {10.1109/ACCESS.2018.2880416},
	abstract = {Convolutional Neural Networks (CNNs) have become the de facto technique for image feature extraction in recent years. However, their design and construction remains a complicated task. As more developments are made in progressing the internal components of CNNs, the task of assembling them effectively from core components becomes even more arduous. To overcome these barriers, we propose the Swarm Optimized Block Architecture, combined with an enhanced adaptive particle swarm optimization (PSO) algorithm for deep CNN model evolution. The enhanced PSO model employs adaptive acceleration coefﬁcients generated using several cosine annealing mechanisms to overcome stagnation. Speciﬁcally, we propose a combined training and structure optimization process for deep CNN model generation, where the proposed PSO model is utilized to explore a bespoke search space deﬁned by a simpliﬁed block-based structure. The proposed PSO model not only devises deep networks speciﬁcally for image classiﬁcation, but also builds and pre-trains models for transfer learning tasks. To signiﬁcantly reduce the hardware and computational cost of the search, the devised CNN model is optimized and trained simultaneously, using a weight sharing mechanism and a ﬁnal ﬁne-tuning process. Our system compares favorably with related research for optimized deep network generation. It achieves an error rate of 4.78\% on the CIFAR-10 image classiﬁcation task, with 34 hours of combined optimization and training, and an error rate of 25.42\% on the CIFAR-100 image data set in 36 hours. All experiments were performed on a single NVIDIA GTX 1080Ti consumer GPU.},
	language = {en},
	urldate = {2022-04-26},
	journal = {IEEE Access},
	author = {Fielding, Ben and Zhang, Li},
	year = {2018},
	pages = {68560--68575},
	file = {Fielding et Zhang - 2018 - Evolving Image Classification Architectures With E.pdf:/home/b98181/Documents/CIFRE/Papiers/Fielding et Zhang - 2018 - Evolving Image Classification Architectures With E.pdf:application/pdf},
}

@article{loni_deepmaker_2020,
	title = {{DeepMaker}: {A} multi-objective optimization framework for deep neural networks in embedded systems},
	volume = {73},
	issn = {01419331},
	shorttitle = {{DeepMaker}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0141933119301176},
	doi = {10.1016/j.micpro.2020.102989},
	abstract = {Deep Neural Networks (DNNs) are compute-intensive learning models with growing applicability in a wide range of domains. Due to their computational complexity, DNNs demand implementations that utilize custom hardware accelerators to meet performance and response time as well as classification accuracy constraints. In this paper, we propose DeepMaker framework that aims to automatically design a set of highly robust DNN architectures for embedded devices as the closest processing unit to the sensors. DeepMaker explores and prunes the design space to ﬁnd improved neural architectures. Our proposed framework takes advantage of a multiobjective evolutionary approach that exploits a pruned design space inspired by a dense architecture. DeepMaker considers the network size factor as the second objective to build a highly optimized network ﬁtting with limited computational resource budgets while delivers an acceptable accuracy level. In comparison with the best result on the CIFAR-10 dataset, a generated network by DeepMaker presents up to a 26.4x compression rate while loses only 4\% accuracy. Besides, DeepMaker maps the generated CNN on the programmable commodity devices, including ARM Processor, High-Performance CPU, GPU, and FPGA.},
	language = {en},
	urldate = {2022-04-26},
	journal = {Microprocessors and Microsystems},
	author = {Loni, Mohammad and Sinaei, Sima and Zoljodi, Ali and Daneshtalab, Masoud and Sjödin, Mikael},
	month = mar,
	year = {2020},
	pages = {102989},
	file = {j.micpro.2020.102989.pdf:/home/b98181/Téléchargements/j.micpro.2020.102989.pdf:application/pdf},
}

@article{luo_neural_2019,
	title = {Neural {Architecture} {Optimization}},
	url = {http://arxiv.org/abs/1808.07233},
	abstract = {Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefﬁcient. In this paper, we propose a simple and efﬁcient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to ﬁnd the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classiﬁcation task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a signiﬁcantly reduction of computational resources. Speciﬁcally we obtain 1.93\% test set error rate for CIFAR-10 image classiﬁcation task and 56.0 test set perplexity of PTB language modeling task. The best discovered architectures on both tasks are successfully transferred to other tasks such as CIFAR-100 (test error rate of 14.75\%), ImageNet (top-1 test error rate of 25.7\%) and WikiText-2 (test perplexity of 67.0). Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate 2.93\%) and on PTB (with test set perplexity 56.6), with very limited computational resources (less than 10 GPU hours) for both tasks.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1808.07233 [cs, stat]},
	author = {Luo, Renqian and Tian, Fei and Qin, Tao and Chen, Enhong and Liu, Tie-Yan},
	month = sep,
	year = {2019},
	note = {arXiv: 1808.07233},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2018. Code available at: https://github.com/renqianluo/NAO},
	file = {Luo et al. - 2019 - Neural Architecture Optimization.pdf:/home/b98181/Documents/CIFRE/Papiers/Luo et al. - 2019 - Neural Architecture Optimization.pdf:application/pdf},
}

@inproceedings{lorenzo_memetic_2018,
	address = {Kyoto Japan},
	title = {Memetic evolution of deep neural networks},
	isbn = {978-1-4503-5618-3},
	url = {https://dl.acm.org/doi/10.1145/3205455.3205631},
	doi = {10.1145/3205455.3205631},
	abstract = {Deep neural networks (DNNs) have proven to be effective at solving challenging problems, but their success relies on finding a good architecture to fit the task. Designing a DNN requires expert knowledge and a lot of trial and error, especially as the difficulty of the problem grows. This paper proposes a fully automatic method with the goal of optimizing DNN topologies through memetic evolution. By recasting the mutation step as a series of progressively refined educated local-search moves, this method achieves results comparable to best human designs. Our extensive experimental study showed that the proposed memetic algorithm supports building a real-world solution for segmenting medical images, it exhibits very promising results over a challenging CIFAR-10 benchmark, and works very fast. Given the ever growing availability of data, our memetic algorithm is a very promising avenue for hands-free DNN architecture design to tackle emerging classification tasks.},
	language = {en},
	urldate = {2022-04-26},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {ACM},
	author = {Lorenzo, Pablo Ribalta and Nalepa, Jakub},
	month = jul,
	year = {2018},
	pages = {505--512},
	file = {Lorenzo et Nalepa - 2018 - Memetic evolution of deep neural networks.pdf:/home/b98181/Documents/CIFRE/Papiers/Lorenzo et Nalepa - 2018 - Memetic evolution of deep neural networks.pdf:application/pdf},
}

@article{zhou_resource-efficient_2018,
	title = {Resource-{Efficient} {Neural} {Architect}},
	url = {http://arxiv.org/abs/1806.07912},
	abstract = {Neural Architecture Search (NAS) is a laborious process. Prior work on automated NAS targets mainly on improving accuracy, but lacks consideration of computational resource use. We propose the Resource-Efﬁcient Neural Architect (RENA), an efﬁcient resource-constrained NAS using reinforcement learning with network embedding. RENA uses a policy network to process the network embeddings to generate new conﬁgurations. We demonstrate RENA on image recognition and keyword spotting (KWS) problems. RENA can ﬁnd novel architectures that achieve high performance even with tight resource constraints. For CIFAR10, it achieves 2.95\% test error when compute intensity is greater than 100 FLOPs/byte, and 3.87\% test error when model size is less than 3M parameters. For Google Speech Commands Dataset, RENA achieves the state-of-the-art accuracy without resource constraints, and it outperforms the optimized architectures with tight resource constraints.},
	language = {en},
	urldate = {2022-04-26},
	journal = {arXiv:1806.07912 [cs]},
	author = {Zhou, Yanqi and Ebrahimi, Siavash and Arık, Sercan Ö and Yu, Haonan and Liu, Hairong and Diamos, Greg},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.07912},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {Zhou et al. - 2018 - Resource-Efficient Neural Architect.pdf:/home/b98181/Documents/CIFRE/Papiers/Zhou et al. - 2018 - Resource-Efficient Neural Architect.pdf:application/pdf},
}

@article{fernandes_junior_particle_2019,
	title = {Particle swarm optimization of deep neural networks architectures for image classification},
	volume = {49},
	issn = {22106502},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210650218309246},
	doi = {10.1016/j.swevo.2019.05.010},
	abstract = {Deep neural networks have been shown to outperform classical machine learning algorithms in solving real-world problems. However, the most successful deep neural networks were handcrafted from scratch taking the problem domain knowledge into consideration. This approach often consumes very signiﬁcant time and computational resources. In this work, we propose a novel algorithm based on particle swarm optimization (PSO), capable of fast convergence when compared with others evolutionary approaches, to automatically search for meaningful deep convolutional neural networks (CNNs) architectures for image classiﬁcation tasks, named psoCNN. A novel directly encoding strategy and a velocity operator were devised allowing the optimization use of PSO with CNNs. Our experimental results show that psoCNN can quickly ﬁnd good CNN architectures that achieve quality performance comparable to the state-of-the-art designs.},
	language = {en},
	urldate = {2022-04-26},
	journal = {Swarm and Evolutionary Computation},
	author = {Fernandes Junior, Francisco Erivaldo and Yen, Gary G.},
	month = sep,
	year = {2019},
	pages = {62--74},
	file = {Fernandes Junior et Yen - 2019 - Particle swarm optimization of deep neural network.pdf:/home/b98181/Documents/CIFRE/Papiers/Fernandes Junior et Yen - 2019 - Particle swarm optimization of deep neural network.pdf:application/pdf},
}

@article{talbi_optimization_nodate,
	title = {Optimization of deep neural networks: a survey and unified taxonomy},
	volume = {00},
	language = {en},
	number = {00},
	journal = {ACM Comput. Surv.},
	author = {Talbi, El-Ghazali},
	pages = {37},
	file = {Talbi - Optimization of deep neural networks a survey and.pdf:/home/b98181/Documents/CIFRE/Papiers/Talbi - Optimization of deep neural networks a survey and.pdf:application/pdf},
}

@article{aguirre2003evolutionary,
  title={Evolutionary synthesis of logic circuits using information theory},
  author={Aguirre, Arturo Hern{\'a}ndez and Coello Coello, Carlos A},
  journal={Artificial Intelligence Review},
  volume={20},
  pages={445--471},
  year={2003},
  publisher={Springer}
}

@article{shchur2023autogluon,
  title={AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting},
  author={Shchur, Oleksandr and Turkmen, Caner and Erickson, Nick and Shen, Huibin and Shirkov, Alexander and Hu, Tony and Wang, Yuyang},
  journal={arXiv preprint arXiv:2308.05566},
  year={2023}
}

@misc{liu2018hierarchical,
      title={Hierarchical Representations for Efficient Architecture Search}, 
      author={Hanxiao Liu and Karen Simonyan and Oriol Vinyals and Chrisantha Fernando and Koray Kavukcuoglu},
      year={2018},
      eprint={1711.00436},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@book{hyndman2008forecasting,
  title={Forecasting with exponential smoothing: the state space approach},
  author={Hyndman, Rob and Koehler, Anne B and Ord, J Keith and Snyder, Ralph D},
  year={2008},
  publisher={Springer Science \& Business Media}
}
@book{box2015time,
  title={Time series analysis: forecasting and control},
  author={Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
  year={2015},
  publisher={John Wiley \& Sons}
}
@article{assimakopoulos2000theta,
  title={The theta model: a decomposition approach to forecasting},
  author={Assimakopoulos, Vassilis and Nikolopoulos, Konstantinos},
  journal={International journal of forecasting},
  volume={16},
  number={4},
  pages={521--530},
  year={2000},
  publisher={Elsevier}
}
@article{de2011forecasting,
  title={Forecasting time series with complex seasonal patterns using exponential smoothing},
  author={De Livera, Alysha M and Hyndman, Rob J and Snyder, Ralph D},
  journal={Journal of the American statistical association},
  volume={106},
  number={496},
  pages={1513--1527},
  year={2011},
  publisher={Taylor \& Francis}
}

@book{hyndman2018forecasting,
  title={Forecasting: principles and practice},
  author={Hyndman, Rob J and Athanasopoulos, George},
  year={2018},
  publisher={OTexts}
}
@article{trapero2015identification,
  title={On the identification of sales forecasting models in the presence of promotions},
  author={Trapero, Juan R and Kourentzes, Nikolaos and Fildes, Robert},
  journal={Journal of the operational Research Society},
  volume={66},
  pages={299--307},
  year={2015},
  publisher={Springer}
}

@article{dolan2002benchmarking,
  title={Benchmarking optimization software with performance profiles},
  author={Dolan, Elizabeth D and Mor{\'e}, Jorge J},
  journal={Mathematical programming},
  volume={91},
  pages={201--213},
  year={2002},
  publisher={Springer}
}

@inproceedings{zoph2018learning,
  title={Learning transferable architectures for scalable image recognition},
  author={Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8697--8710},
  year={2018}
}

@inproceedings{xie2017genetic,
  title={Genetic cnn},
  author={Xie, Lingxi and Yuille, Alan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1379--1388},
  year={2017}
}

@inproceedings{irwin2019graph,
  title={A graph-based encoding for evolutionary convolutional neural network architecture design},
  author={Irwin-Harris, William and Sun, Yanan and Xue, Bing and Zhang, Mengjie},
  booktitle={2019 IEEE Congress on Evolutionary Computation (CEC)},
  pages={546--553},
  year={2019},
  organization={IEEE}
}

@inproceedings{wen2020neural,
  title={Neural predictor for neural architecture search},
  author={Wen, Wei and Liu, Hanxiao and Chen, Yiran and Li, Hai and Bender, Gabriel and Kindermans, Pieter-Jan},
  booktitle={European conference on computer vision},
  pages={660--676},
  year={2020},
  organization={Springer}
}

@inproceedings{chatzianastasis2021graph,
  title={Graph-based neural architecture search with operation embeddings},
  author={Chatzianastasis, Michail and Dasoulas, George and Siolas, Georgios and Vazirgiannis, Michalis},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={393--402},
  year={2021}
}

@inproceedings{ying2019bench,
  title={Nas-bench-101: Towards reproducible neural architecture search},
  author={Ying, Chris and Klein, Aaron and Christiansen, Eric and Real, Esteban and Murphy, Kevin and Hutter, Frank},
  booktitle={International conference on machine learning},
  pages={7105--7114},
  year={2019},
  organization={PMLR}
}

@article{white2023neural,
  title={Neural architecture search: Insights from 1000 papers},
  author={White, Colin and Safari, Mahmoud and Sukthanker, Rhea and Ru, Binxin and Elsken, Thomas and Zela, Arber and Dey, Debadeepta and Hutter, Frank},
  journal={arXiv preprint arXiv:2301.08727},
  year={2023}
}

@inproceedings{guo2020single,
  title={Single path one-shot neural architecture search with uniform sampling},
  author={Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16},
  pages={544--560},
  year={2020},
  organization={Springer}
}


