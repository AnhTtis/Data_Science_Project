\section{Related Work} \label{part 2}

\subsection{Deep learning for time series forecasting}

\label{part dl related}

Time series forecasting has been studied for decades. The field has been dominated for a long time by statistical tools such as ARIMA, Exponential Smoothing (ES), or (S)ARIMAX, this last model allowing the use of exogenous variables. It now opens itself to deep learning models \citep{9461796}. These new models recently achieved great performances on many datasets. Three main parts compose typical DNNs: an input layer, several hidden layers and an output layer. In this paper we apply our framework to optimize the hidden layers for a given time series forecasting task (see Figure~\ref{fig:metamodel_monash}). In this part, we introduce usual DNN layers for time series forecasting, which can be used in our search space.

The first layer type from our search space is the fully-connected layer, or Multi-Layer Perceptron (MLP). The input vector is multiplied by a weight matrix. Most architectures use such layers as simple building blocks for dimension matching, input embedding or output modelling. The N-Beats model is a well-known example of a DNN based on fully-connected layers for time series forecasting \citep{NBeats}.

The second layer type \citep{lecun2015deep} is the convolution layer (CNN). Inspired by the human brain's visual cortex, it has mainly been popularised for computer vision. The convolution layer uses a discrete convolution operator between the input data and a small matrix called a filter. The extracted features are local and time-invariant if the considered data are time series. Many architectures designed for time series forecasting are based on convolution layers such as WaveNet \citep{oord2016wavenet} and Temporal Convolution Networks \citep{lea2017temporal}.

The third layer type is the recurrent layer (RNN), specifically designed for sequential data processing, therefore, particularly suitable for time series. These layers scan the sequential data and keep information from the sequence past in memory to predict its future. A popular model based on RNN layers is the Seq2Seq network \citep{seq2seq}. Two RNNs, an encoder and a decoder, are sequentially connected by a fixed-length vector. Various versions of the Seq2Seq model have been introduced in the literature, such as the DeepAR model \citep{salinas2020deepar}, which encompasses an RNN encoder in an autoregressive model. The major weakness of RNN layers is the modelling of long-term dynamics due to the vanishing gradient. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) layers have been introduced \citep{hochreiter1997long, chung2014empirical} to overcome this problem.

Finally, the layer type from our search space is the attention layer. The attention layer has been popularized within the deep learning community as part of Vaswani's transformer model \citep{vaswani2017attention}. The attention layer is more generic than the convolution. It can model the dependencies of each element from the input sequence with all the others. In the vanilla transformer \citep{vaswani2017attention}, the attention layer does not factor the relative distance between inputs in its modelling but rather the element's absolute position in the sequence. The Transformer-XL \citep{dai2019transformer}, a transformer variant created to tackle long-term dependencies tasks, introduces a self-attention version with relative positions. \citet{cordonnier2019relationship} used this new attention formulation to show that, under a specific configuration of parameters, the attention layers could be trained as convolution layers. Within our search space, we chose this last formulation of attention, with the relative positions.

The three first layers (i.e. MLP, CNN, RNN) were frequently mixed into DNN architectures. Sequential and parallel combinations of convolution, recurrent and fully connected layers often compose state-of-the-art DNN models for time series forecasting. Layer diversity enables the extraction of different and complementary features from input data to allow a better prediction. Some recent DNN models introduce transformers into hybrid DNNs. In \citet{lim2021temporal}, the authors developed the Temporal Fusion Transformer, a hybrid model stacking transformer layers on top of an RNN layer. With this in mind, we built a flexible search space which generalizes hybrid DNN models including MLPs, CNNs, RNNs and transformers.

\subsection{Search spaces for automated deep learning}

Designing an efficient DNN for a given task requires choosing an architecture and tuning its many hyperparameters. It is a difficult, fastidious, and time-consuming optimization task. Moreover, it requires expertise and restricts the discovery of new DNNs to what humans can design. Research related to the automatic design and optimization of DNNs has therefore risen this last decade \citep{talbi2021automated}. The first challenge in automatic deep learning (AutoDL), and more specifically neural architecture search (NAS), is search space design. Typical search spaces for Hyperparameters Optimization (HPO) are a product space of a mixture of continuous and categorical dimensions (e.g. learning rate, number of layers, batch size), while NAS focuses on optimizing the topology of the DNN \citep{white2023neural}. Encoding a DNN topology is a complex task because the encoding should not be too broad and allow too many architectures to keep the search efficient. On the contrary, if the encoding is too restrictive, we may miss promising solutions and novel architectures. This means before creating the search space we need to choose which DNNs or type of DNNs are relevant or not to the problem at hand. Once we have decided on this broad set of DNNs, we define the search space following a set of rules \citep{talbi2021automated}:

\begin{itemize}
\item Completeness: all (or almost all) relevant DNNs from this broad set should be encoded in the search space.
\item Connectedness: a path should always be possible between two encoded DNNs in the search space.
\item Efficiency: the encoding should be easy to manipulate by the search operators (i.e. neighbourhoods, variation operators) of the search strategy.
\item Constraint handling: the encoding should facilitate the handling of the various constraints to generate feasible DNNs.
\end{itemize}

A complete classification of encoding strategies for NAS is presented in \citet{talbi2021automated} and reproduced in Figure \ref{fig:classi_encoding}. We can discriminate between direct and indirect encodings. With direct strategies, the DNNs are completely defined by the encoding, while indirect strategies need a decoder to find the architecture back. Amongst direct strategies, one can discriminate between two categories: flat and hierarchical encodings. In flat encodings, all layers are individually encoded \citep{loni2020deepmaker, sun2018particle, wang2018evolving, wang2019evolving}. The global architecture can be a single chain, with each layer having a single input and a single output, which is called chain structured \citep{assuncao_denser_2018}, but more complex patterns such as multiple outputs, skip connections, have been introduced in the extended flat DNNs encoding \citep{chen_scale-aware_2021}. For hierarchical encodings, they are bundled in blocks \citep{pham2018efficient, shu2019understanding, liu2017hierarchical, zhang2019d}. If the optimization is made on the sequencing of the blocks, with an already chosen content, this is referred to as inner-level fixed \citep{camero2021bayesian, white2021bananas}. If the optimization is made on the blocks' content with a fixed sequencing, it is called outer level fixed. A joint optimization with no level fixed is also an option \citep{liu2019auto}. Regarding the indirect strategies, one popular encoding is the one-shot architecture \citep{bender2018understanding, brock2017smash}. One single large network resuming all candidates from the search space is trained. Then the architectures are found by pruning some branches. Only the best promising architectures are retrained from scratch.

\begin{figure*}[htbp]
\centering
    \begin{tikzpicture}
        \draw node[] (SE) {Solution Encoding};
        \draw node[below left =0.5cm and 1.8cm of SE] (Direct) {Direct};
        \draw node[below right =0.5cm and 1.8cm of SE] (Indirect) {Indirect};
        \draw node[below left =0.5cm and 2cm of Direct] (Flat) {Flat};
        \draw node[below right =0.5cm and 2cm of Direct] (Hierarchical) {Hierarchical};
        \draw node[below left =0.5cm and 0cm of Flat, align=center] (Chain) {Chain\\structured};
        \draw node[below right =0.5cm and 0cm of Flat, align=center] (Extended) {Extended\\Flat DNNs};
        \draw node[below =0.5cm of Hierarchical, align=center] (Outer) {Outer\\Level fixed};
        \draw node[left =0.5cm of Outer, align=center] (Inner) {Inner\\Level Fixed};
        \draw node[right =0.5cm of Outer, align=center] (No) {No level\\fixed};
        \draw node[below =of Indirect, align=center] (One) {One-shot};
        \draw[->] (SE) -- (Direct);
        \draw[->] (SE) -- (Indirect);
        \draw[->] (Direct) -- (Flat);
        \draw[->] (Direct) -- (Hierarchical);
        \draw[->] (Flat) -- (Chain);
        \draw[->] (Flat) -- (Extended);
        \draw[->] (Hierarchical) -- (Inner);
        \draw[->] (Hierarchical) -- (Outer);
        \draw[->] (Hierarchical) -- (No);
        \draw[->] (Indirect) -- (One);
    \end{tikzpicture}
    \caption{Classification of encoding strategies for NAS \citep{talbi2021automated}.}
    \label{fig:classi_encoding}
\end{figure*}

Our search space can be categorized as a direct and extended flat encoding. It is based on the representation of DNNs by DAGs. This representation is very popular among the NAS community and is used by cell-based search spaces such as NAS-Bench-101 inspired by the ResNet architecture \citep{ying2019bench}, as well as one-shot representation such as the DARTS framework (for Differentiable Architecture Search) proposed by \citet{liu2018darts}. In cell-based search spaces, DNNs are represented by repeated cells encoded as DAGs, where each node is an operation belonging to a well-defined list, typically: convolution of size 1, 3, or 5, pooling of size 3, skip connection, or zeroed operation for an image classification task for example. The graphs are then represented either as vectors using path encoding, or as adjacency matrices. In the case of path encoding, different search algorithms can be used, such as Bayesian optimization \citep{white_bananas_2020}, reinforcement learning \citep{zoph2018learning}, particle swarm optimization \citep{wang_evolving_2019}, or evolutionary algorithms \citep{xie2017genetic}, for which classical mutation and crossover operators are usually used and consist in modifying the elements of the path. Adjacency matrices, on the other hand, are more complex objects to optimize. The matrix itself represents the connections within the graph and is usually accompanied by a list representing the nodes content. In the literature, these matrices have been optimized directly with random search algorithms \citep{irwin2019graph} or indirectly with neural predictors based on auto-encoders (see for example \citet{zhang2019d} or \citet{chatzianastasis2021graph}). In the case of one-shot representations, an initial large graph containing all the considered DNN is pruned with a certain search algorithm to only keep the best possible subgraph (and thus the best possible DNN). Various search algorithms can be used to simplify this meta-graph \citep{bender2018understanding} like evolutionary algorithm \citep{guo2020single}. One of the most widely used techniques is DARTS \citep{liu2018darts}, where each edge is associated with a candidate operation, assigned to a probability of being retained in the final subgraph, optimized by gradient descent. The candidate operations can be very traditional, such as for cell-based search spaces, but \citet{chen_scale-aware_2021} proposes to use DARTS with other types of operations, such as inter-variable attention, for multivariate time series prediction. While such search spaces have proven to be efficient for tasks like image classification or language processing, \citet{white2023neural} points out that current NAS search spaces are not very expressive and prevents finding highly novel architectures. This problem is amplified when dealing with tasks for which no known architectures have yet been found.

 Compared to these search spaces, the one we define in this paper is more flexible. We address the optimization of both the architecture and the hyperparameters. We do not fix a list of possible operations with fixed hyperparameters, as is done in these works, but leave the user free to use any operation coded as \textit{PyTorch nn.Module} and to optimize any chosen parameters. Furthermore, we do not fix the generic form of our graph, we do not fix a maximum number of incoming or outgoing edges and we allow to expand or reduce the graphs. DRAGON is capable of generating innovative, original, yet well-performing DNNs. This flexibility may hinder the framework's ability to find good DNNs compared to the NAS state-of-the-art for well-known tasks such as image classification or language processing. However, in cases where DNNs have not been extensively studied and well-performing architectures have not yet been found, such as time series prediction, DRAGON may be more useful and powerful. Finally, we encode our DAGs using their adjacency matrices and provide evolutionary operators to directly modify this representation. To our knowledge, neither such a large search space nor such operators have been used in the literature.

%**************************************************
\subsection{AutoML for time series forecasting}
%**************************************************

The automated design of DNNs called Automated Deep Learning (AutoDL), belongs to a larger field \citep{hutter2019automated} called Automated Machine Learning (AutoML). AutoML aims to automatically design well-performing machine learning pipelines, for a given task. Works on model optimization for time series forecasting mainly focused on AutoML rather than AutoDL \citep{alsharef2022review}. The optimization can be performed at several levels: input features selection, extraction and engineering, model selection and hyperparameters tuning. Initial research works used to focus on one of these subproblems, while more recent works offer complete optimization pipelines.

The first subproblems, input features selection, extraction and engineering, are specific to our learning task: time series forecasting. This tedious task can significantly improve the prediction scores by giving the model relevant information about the data. Methods to select the features are among computing the importance of each feature on the results or using statistical tools on the signals to extract relevant information. Next, the model selection aims at choosing among a set of diverse machine learning models the best-performing one on a given task. Often, the models are trained separately, and the best model is chosen. In general, the selected model has many hyperparameters, such as the number of hidden layers, activation function or learning rate. Their optimization usually allows for improving the performance of the model.

Nowadays, many research works implement complete optimization pipelines combining those subproblems for time series forecasting. The Time Series Pipeline Optimization framework \citep{dahl2020tspo}, is based on an evolutionary algorithm to automatically find the right features thanks to input signal analysis, then the model and its related hyperparameters. AutoAI-TS \citep{shah2021autoai} is also a complete optimization pipeline, with model selection performed among a wide assortment of models: statistical models, machine learning, deep learning models and hybrids models. Closer to our work, the framework Auto-Pytorch-TS \citep{deng2022efficient} is specific to deep learning models optimization for time series forecasting. The framework uses Bayesian optimization with multi-fidelity optimization. Finally, a recent work from Amazon \citep{shchur2023autogluon} introduces a time series version to their AutoML framework, AutoGluon, leveraging ensembles of statistical and machine learning forecasters.

Except for AutoPytorch-TS, cited works covering the entire optimization pipeline for time series do not deepen model optimization and only perform model selection and hyperparameters optimization. However, time series data becomes more complex, and there is a growing need for more sophisticated and data-specific DNNs. Our framework DRAGON, presented in this paper, only tackles the model selection and hyperparameters optimization parts of the pipeline. We made this choice to show the effectiveness of our framework for designing better DNNs. If we had implemented feature selection, it would have been harder to determine whether the superiority of our results came from the input features pool or the model itself.