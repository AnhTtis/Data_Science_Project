\section{Experimental study} \label{part5}

\subsection{Baseline}

We compared our framework to two baselines. The first consists of 15 handcrafted models \citep{godahewa2021monash}, the second is more recent and compares 6 AutoML frameworks specifically designed for time series forecasting \citep{shchur2023autogluon}.

\paragraph{Handcrafted models} These are statistical, machine learning and deep learning models that were built and optimised by hand. We first have 5 traditional univariate forecasting models: Simple Exponential Smoothing (SES), Exponential Smoothing (ETS), Theta, Trigonometric Box-Cox ARMA Trend Seasonal (TBATS), Dynamic Harmonic Regression ARIMA (DHR-ARIMA) and 8 global forecasting models: Pooled Regression (PR), CatBoost, Prophet, Feed-Forward Neural Network (FFNN), N-BEATS, WaveNet, Transformer and DeepAR. The last 5 models are Deep Neural Networks. Refer to the original paper \citep{godahewa2021monash} and the Monash Time Series Forecasting Repository website\footnote{\url{https://forecastingdata.org/}} for more information about the models and their implementation. Finally, \citet{shchur2023autogluon} also provides the univariate forecasting model SeasonalNaive and the global deep learning model Temporal Fusion Transformer (TFT).

\paragraph{AutoML frmaeworks} \citet{shchur2023autogluon} compares 6 AutoML frameworks specifically designed for time series forecasting. They first used 4 statistical based AutoML framworks: AutoARIMA, AutoETS and AutoTheta which automatically tune the hyperparameters of the ARIMA, ETS and Theta models respectively for each time series individually based on an information criterion and StatEnsemble which takes the the forecasts median of three statistical models. Then, they included the AutoDL framework AutoPyTorch-Forecasting, which builds DNNs using a combination of Bayesian and multi-fidelity optimization and then leverages model ensemble. Finally, AutoGluon-TS, the AutoML framework proposed by \citet{shchur2023autogluon} relies on ensembling techniques rather than HPO or NAS. The models ensemble are made of local models such as ARIMA, Theta, ETS and SeasonalNaive, as well as global models such as DeepAR, PatchTST and Temporal Fusion Transformer. While it is interesting to compare ourselves with these state-of-the-art AutoML techniques, it is worth remembering that our framework does not yet provide an ensembling technique, and the scores obtained during optimization are based on the predictions of a single DNN.

%**************************************
\subsection{Experimental protocol}
%**************************************

We evaluated DRAGON on the established benchmark of Monash Time Series Forecasting Repository \citep{godahewa2021monash}. This archive contains a benchmark of more than 40 datasets, from which we selected the 27 that \citet{shchur2023autogluon} used fot their experiments. The time series are of different kinds and have variable distributions. More information on each dataset from the archive is available \ref{part info monash}. This task diversity allows to test DRAGON generalization and robustness abilities.

For these experiments, we configured our algorithm to have a population of $K=100$ individuals and we set the total budget to $B=$ 8 hours. We investigated a joint optimization of the architecture $\alpha$ and the hyperparameters $\lambda$. We ran our experiments on 5 cluster nodes, each equipped with 4 Tesla V100 SXM2 32GB GPUs, using Pytorch 1.11.0 and Cuda 10.2.

We took the data, the data generation functions, the training parameters (batch size, number of epochs, learning rate), the training and prediction functions from the Monash Time Series Forecasting Repository, and we only changed the models themselves. We also kept for each time series the forecast horizon and the lag used in the repository. We believe our comparison is fair to the handcrafted and automatically designed models. Finally, to evaluate the models' performance, we used the same metric and metric implementation as in the repository. This metric represents the forecast error $\ell$, and is the Mean Absolute Scaled Error (MASE), an absolute mean error divided by the average difference between two consecutive time steps \citep{hyndman2006another}. Given a time series $Y = (\mathbf{y}_1, ..., \mathbf{y}_n)$ and the predictions $\hat{Y} = (\mathbf{\hat{y}}_1, ..., \mathbf{\hat{y}}_n)$, the MASE can be defined as:
\begin{center}
    $\mathrm{MASE}(Y, \hat{Y}) = \frac{n-1}{n} \times \frac{\sum_{t=1}^{n}|\mathbf{y}_t - \hat{\mathbf{y}}_t|}{\sum_{t=2}^{n}|\mathbf{y}_t - \mathbf{y}_{t-1}|}$.
\end{center}

In our case, for $f \in \Omega$, $\mathcal{D}_0 = (X_0, Y_0) \subseteq \mathcal{D}$, we have $\ell(Y_0, f(X_0) = \mathrm{MASE}\big(Y_0, f(X_0)\big)$.

\subsection{Search Space}

The generic search space defined Section~\ref{part 3} introduces a brick, the Directed Acyclic Graph, which cannot be directly defined as our search space. Instead, we defined a meta-architecture as represented Figure~\ref{fig:metamodel_monash}, which can be directly used to replace the repository's models. The graph $\Gamma$ can be composed with various one-dimensional candidate operations (e.g. 1D convolution, LSTM, MLP), which can be found with their associated hyperparameters Table~\ref{anx:op_hp}. Our DAG is followed by a Multi-layer Perceptron (MLP) which is used to retrieve the time series output dimension, as the number of channels may vary within $\Gamma$.
%
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[auto, thick]
        \draw
        node [in_out, minimum height = 1cm, minimum width=4cm, fill=input_purple, text=white](input) {Time Series input}
        node [in_out, minimum height = 4cm, minimum width=4.5cm, below=0.5cm of input, fill=middle_beige!30, label={[align=center, anchor=north]north:DNN:$f^{\alpha, \lambda}_{\theta}$}] (dnn) {}
        node [in_out, minimum height = 1.5cm, minimum width=4cm , below=1.3cm of input] (dag) {Directed Acyclic\\ Graph: $ \Gamma = f^{\alpha, \lambda}$}
         node [in_out, minimum height = 1cm, minimum width=4cm, below=0.3cm of dag, fill=output_red, text=white] (mlp) {Multi-layer Perceptron}
         node [in_out, minimum height = 1cm, minimum width=4cm, below=0.5cm of dnn, fill=output_red!30] (out) {Time Series Prediction};
        \draw[->](input) -- (dnn); 
        \draw[->](dag) -- (mlp);
        \draw[->](dnn) -- (out);
    \end{tikzpicture}
    \caption{Meta-architecture for Monash time series datasets.}
    \label{fig:metamodel_monash}
\end{figure}
%
This search space is designed specifically for time series forecasting, but it could be modified for other tasks. For example if we want to use it for image classification, we would need a first graph with two-dimensional candidate operations, followed by a flatten layer, followed by a second graph with one-dimensional candidate operations and a final MLP layer.

\subsection{Results}

\begin{table}[htbp]
\begin{center}
\caption{Performance comparison of the baseline algorithms with DRAGON (based on the MASE metric) on 27 datasets. Wins corresponds to the number of datasets where the method produced a smaller loss than DRAGON, Losses corresponds to the number of datasets where the method produced a larger loss than DRAGON, Champion corresponds to the number of datasets where the method produced the smallest loss, and Failures corresponds to the number of datasets where the method failed.}
\small
\label{tab:results_summary}
\begin{tabular}{|c| c c c c |}
\hline
Algorithm(s) & Wins & Losses & Champion & Failures \\
\hline
SES & 1 & 25 & 0 & 1 \\
Theta & 6 & 20 & 0 & 1 \\
TBATS & 6 & 20 & 0 & 1 \\
ETS & 8 & 18 & 1 & 1 \\
(DHR-)ARIMA & 4 & 21 & 0 & 2 \\
PR & 1 & 25 & 0 & 1 \\
CatBoost & 1 & 25 & 0 & 1 \\
FFNN & 0 & 26 & 0 & 1 \\
N-BEATS & 0 & 26 & 0 & 1 \\
WaveNet & 2 & 23 & 0 & 2 \\
Transformer & 0 & 26 & 0 & 1 \\
DeepAR & 1 & 25 & 1 & 1 \\
TFT & 4 & 23 & 0 & 0 \\
SeasonalNaive & 1 & 26 & 0 & 0 \\
Prophet & 2 & 24 & 1 & 1 \\
AutoPytorch & 7 & 20 & 0 & 0 \\
AutoARIMA & 4 & 20 & 0 & 3 \\
AutoETS & 8 & 19 & 0 & 0 \\
AutoTheta & 9 & 16 & 0 & 2 \\
StatEnsemble & 9 & 15 & 3 & 3 \\
AutoGluon & 13 & 14 & 10 & 0 \\
DRAGON & - & - & 11 & 0 \\
\hline
\end{tabular}
\end{center}
\end{table}

We report a summary of the results in Table~\ref{tab:results_summary}. According to this summary, DRAGON outperforms all algorithms on 11 out of 27 datasets (41\%). No algorithm from the baseline was able to beat DRAGON on at least 50\% of the datasets and AutoGluon was the only algorithm able to beat DRAGON on more than a third of the datasets. The direct competitor of DRAGON, namely AutoPytorch which is another AutoDL framework, was only able to beat it on 7 datasets out of 27 (26\%). More detailed results can be found Table~\ref{tab:results_monash}.

\begin{table}[htbp]
\begin{center}
\caption{\footnotesize{Mean MASE for each dataset. We did not report all the individual scores from the handcrafted baseline, but the best score from the 15 models for each time series. The grayed values correspond to the minimal loss for the corresponding dataset.}}
\footnotesize
\label{tab:results_monash}
\begin{adjustbox}{angle=90}
\begin{tabular}{|c| c c c c c c c c|}
\hline
 Dataset & Handcrafted & AutoPytorch & AutoARIMA & AutoETS & AutoTheta & StatEnsemble & AutoGluon & DRAGON \\
\hline
COVID & 5.192 & 4.911 & 6.029 & 5.907 & 7.719 & 5.884 & 5.805 & \cellcolor{gray!20} 4.535 \\
Carparts & 0.746 & 0.746 & 1.118 & 1.133 & 1.208 & 1.052 & 0.747 & \cellcolor{gray!20} 0.745 \\
Electricity Hourly & 1.389 & 1.420 & - & 1.465 & - & - & \cellcolor{gray!20} 1.227 & 1.314 \\
Electricity Weekly & 0.769 & 2.322 & 3.009 & 3.076 & 3.113 & 3.077 & 1.892 & \cellcolor{gray!20} 0.644 \\
FRED-MD & \cellcolor{gray!20} 0.468 & 0.682 & 0.478 & 0.505 & 0.564 & 0.498 & 0.656 & 0.494 \\
Hospital & \cellcolor{gray!20} 0.673 & 0.770 & 0.820 & 0.766 & 0.764 & 0.753 & 0.741 & 0.750 \\
KDD & 0.844 & 0.764 & - & 0.988 & 1.010 & - & 0.709 & \cellcolor{gray!20} 0.678 \\
M1 Monthly & 1.074 & 1.278 & 1.152 & 1.083 & 1.092 & \cellcolor{gray!20} 1.045 & 1.235 & 1.069 \\
M1 Quarterly & 1.658 & 1.813 & 1.770 & 1.665 & 1.667 & 1.622 & \cellcolor{gray!20} 1.615 & 1.717 \\
M1 Yearly & 3.499 & 3.407 & 3.870 & 3.950 & 3.659 & 3.769 & \cellcolor{gray!20} 3.371 & 3.683 \\
M3 Monthly & 0.861 & 0.956 & 0.934 & 0.867 & 0.855 & 0.845 & \cellcolor{gray!20} 0.822 & 0.900 \\
M3 Other & 1.814 & 1.871 & 2.245 & 1.801 & 2.009 & \cellcolor{gray!20} 1.769 & 1.837 & 2.144 \\
M3 Quarterly & 1.117 & 1.180 & 1.419 & 1.121 & 1.119 & 1.096 & \cellcolor{gray!20} 1.057 & 1.087 \\
M3 Yearly & 2.774 & 2.691 & 3.159 & 2.695 & 2.608 & 2.627 & \cellcolor{gray!20} 2.520 & 2.775 \\
M4 Daily & 1.141 & 1.152 & 1.153 & 1.228 & 1.149 & 1.145 & 1.156 & \cellcolor{gray!20} 1.056 \\
M4 Hourly & 1.193 & 1.345 & 1.029 & 1.609 & 2.456 & 1.157 & \cellcolor{gray!20} 0.807 & 1.155 \\
M4 Monthly & 0.947 & 0.851 & 0.812 & 0.803 & 0.834 & \cellcolor{gray!20} 0.780 & 0.782 & 0.991 \\
M4 Quarterly & 1.161 & 1.176 & 1.276 & 1.167 & 1.183 & 1.148 & \cellcolor{gray!20} 1.139 & 1.190 \\
M4 Weekly & 0.453 & 2.369 & 2.355 & 2.548 & 2.608 & 2.375 & 2.035 & \cellcolor{gray!20} 0.446 \\
NN5 Daily & 0.789 & 0.807 & 0.935 & 0.870 & 0.878 & 0.859 & \cellcolor{gray!20} 0.761 & 0.892 \\
NN5 Weekly & 0.808 & 0.865 & 0.998 & 0.980 & 0.963 & 0.977 & 0.860 & \cellcolor{gray!20} 0.703 \\
Pedestrians & 0.247 & 0.354 & - & 0.553 & - & - & 0.312 & \cellcolor{gray!20} 0.218 \\
Tourism Monthly & \cellcolor{gray!20} 1.409 & 1.495 & 1.585 & 1.529 & 1.666 & 1.469 & 1.442 & 1.434 \\
Tourism Quarterly & 1.475 & 1.647 & 1.655 & 1.578 & 1.648 & 1.539 & 1.537 & \cellcolor{gray!20} 1.471 \\
Tourism Yearly & 2.590 & 3.004 & 4.044 & 3.183 & 2.992 & 3.231 & 2.946 & \cellcolor{gray!20} 2.337 \\
Vehicle Trips & 1.176 & 1.162 & 1.427 & 1.301 & 1.284 & 1.203 & \cellcolor{gray!20} 1.113 & 1.645 \\
Web Traffic & 0.973 & 0.962 & 1.189 & 1.207 & 1.108 & 1.068 & 0.938 & \cellcolor{gray!20} 0.561 \\
\hline
\end{tabular}
\end{adjustbox}
\end{center}
\end{table}

To have a more visual comparison of the different algorithms from the baseline, we used the performance profile as defined by \citet{dolan2002benchmarking}. We name $\mathcal{P}$ the set of the 27 datasets, $\mathcal{S}$ the set of the 22 algorithms from the baseline and $l_{p,s}$ the final score (loss) of the algorithm $s \in \mathcal{S}$ on the dataset $p \in \mathcal{P}$. We define the performance ratio $r_{p,s}$ of $s$ on $p$ as:
\begin{align*}
    r_{p,s} = \frac{l_{p,s}}{\text{min}\{l_{p,s}: s\in \mathcal{S}\}}\,.
\end{align*}
%
From this we can define the performance profile as the probability for the algorithm $s \in \mathcal{S}$ that the performance ratio on any dataset is within a factor $\tau \in \mathbb{R}$ of the best possible ratio:
\begin{align*}
    \rho_s(\tau) = \frac{1}{27}\text{size}\{p \in \mathcal{P}: r_{p,s} \leq \tau\}\,,
\end{align*}
the function $\rho_s$ is the (cumulative) distribution function for the performance ratio. We compute the performance profile for each algorithm from the AutoML baseline, which can be found Figure~\ref{fig:perf_ratio}.
%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{data/performance_ratio.png}
    \caption{Performance profile $\rho_{s}(\tau)$ for each algorithm $s$ from the AutoML baseline, with $\tau \in [1, 7]$.}
    \label{fig:perf_ratio}
\end{figure}
%
From the performance profile, we can see that compared to the baseline, DRAGON has an error close to the best for every dataset. It is also the only algorithm for which the performance ratio is less than two for all datasets. This diagram also suggests that the performance of AutoPytorch and AutoGluon are not that different.

\subsection{Computation time}

To be consistent with the other algorithms from the baseline, we set a fixed time budget of 8 hours for our experiments. But in most cases the algorithm found the best solution in less time than this. 
%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{data/computation_time.png}
    \caption{Computation time of DRAGON for each dataset. The curves represent the time when the best loss so far has been found for each dataset.}
    \label{fig:computation_time}
\end{figure}
%
Figure~\ref{fig:computation_time} represents the time convergence of DRAGON for each dataset. For almost every dataset, a close solution to the final one was found in less than an hour. For some datasets like M4 weekly or M1 Quarterly, DRAGON did not improve the results after the first hour. The models from the baseline train faster, with AutoGluon for example having an average runtime of 33 minutes \citep{shchur2023autogluon}. However, those algorithms are based on Machine Learning models, wherease the runtime of AutoPytorch, the other AutoDL framework was set to 4 hours for each datasets. The training time of DNNs are indeed usually higher than for traditional machine learning models. We think we can improve our training time using a multi-fidelity approach. Indeed, with our evolutionary algorithm, every DNN is trained until for 100 epochs before being evaluated. With a multi-fidelity approach we could speed up the identification of good performing models and stop training the worst ones sooner.

%**********************************
\subsection{Best models analysis}
%**********************************

In the AutoDL literature, few efforts are usually made to analyze the generated DNNs. In \citet{DBLP:journals/corr/abs-1909-09569} the authors established that architectures with wide and shallow cell structures are favoured by the NAS algorithms and that they suffer from poor generalization performance. We can rightfully ask ourselves about the efficiency of our framework and some of these questions may be answered thanks to a light models analysis. By the end of this section, we will try to answer some inquiries about our framework outputs. To answer those questions we defined some structural indicators, and we computed them in Table \ref{tab:best_models} for the best model for each dataset from \citet{godahewa2021monash}:

\begin{itemize}
    \item \textbf{\textit{Nodes}}: it represents the number of nodes (i.e. operations) in the graph.
    \item \textbf{\textit{Width}}: it represents the network width which can be defined as the maximum of incoming or outgoing edges to all nodes within the graph.
    \item \textbf{\textit{Depth}}: it defines the network depth which corresponds to the size of the longest path in the graph.
    \item \textbf{\textit{Edges}}: it represents the number of edges, relative to the number of nodes in the graph. It indicates how complex the graph can be and how sparse the adjacency matrix is.
    \item The last 7 indicators correspond to the number of the appearance of each layer type within the DNN.
\end{itemize}

\begin{table}[htbp]
\scriptsize
\begin{center}
\caption{\footnotesize{Structural indicators of the best model for each dataset found by DRAGON.}}
\label{tab:best_models}
\begin{tabular}{|c| c c c c c c c c c c c|}
\hline
Dataset & Nodes & Width & Depth & Edges & MLP & Att & CNN & RNN & Drop & Id & Pool \\
\hline
m3 monthly & 6 & 3 & 4 & 12 & 2 & 1 & 1 & 1 & 0 & 0 & 1 \\
covid death & 3 & 1 & 3 & 3 & 1 & 1 & 0 & 0 & 1 & 0 & 0 \\
m3 quarterly & 4 & 2 & 3 & 6 & 1 & 0 & 0 & 2 & 0 & 1 & 0 \\
vehicle trips & 4 & 3 & 4 & 8 & 1 & 0 & 0 & 1 & 0 & 0 & 2 \\
m1 yearly & 8 & 5 & 5 & 17 & 2 & 2 & 0 & 0 & 1 & 3 & 0 \\
m4 monthly & 6 & 3 & 4 & 12 & 2 & 1 & 1 & 1 & 0 & 0 & 1 \\
m3 other & 4 & 4 & 3 & 8 & 2 & 1 & 0 & 1 & 0 & 0 & 0 \\
tourism quarterly & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
pedestrian & 2 & 2 & 2 & 3 & 0 & 0 & 1 & 0 & 1 & 0 & 0 \\
nn5 daily & 5 & 5 & 3 & 12 & 1 & 1 & 1 & 0 & 0 & 2 & 0 \\
Web Traffic & 7 & 4 & 6 & 16 & 2 & 2 & 2 & 0 & 0 & 1 & 0 \\
m1 quarterly & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
tourism yearly & 7 & 3 & 6 & 15 & 2 & 1 & 1 & 0 & 1 & 1 & 1 \\
electricity weekly & 7 & 5 & 7 & 18 & 3 & 1 & 1 & 0 & 0 & 2 & 0 \\
m4 hourly & 5 & 4 & 5 & 11 & 0 & 2 & 1 & 0 & 1 & 1 & 0 \\
electricity hourly & 3 & 3 & 3 & 5 & 0 & 1 & 0 & 0 & 0 & 2 & 0 \\
m3 yearly & 6 & 4 & 5 & 13 & 1 & 2 & 0 & 0 & 0 & 3 & 0 \\
m4 weekly & 2 & 2 & 2 & 3 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\
m4 daily & 2 & 2 & 2 & 3 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\
nn5 weekly & 2 & 2 & 2 & 3 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\
kdd cup & 7 & 6 & 5 & 17 & 2 & 1 & 2 & 1 & 1 & 0 & 0 \\
hospital & 4 & 4 & 3 & 8 & 1 & 0 & 2 & 1 & 0 & 0 & 0 \\
m1 monthly & 2 & 1 & 2 & 2 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\
fred md & 5 & 4 & 3 & 9 & 0 & 1 & 1 & 0 & 1 & 1 & 1 \\
car parts & 7 & 3 & 6 & 15 & 2 & 1 & 1 & 0 & 1 & 1 & 1 \\
\hline
\hline
\textbf{Mean} & 4.40 & 3.08 & 3.60 & 8.84 & 1.20 & 0.80 & 0.64 & 0.32 & 0.32 & 0.84 & 0.28 \\
\hline
\end{tabular}
\end{center}
\end{table}



\textit{Does DRAGON always converge towards complex models, or is it able to find simple DNNs?}

From Table~\ref{tab:best_models} we notice that the model found are really small compare to a Transformer model for example and all have less than 8 hidden layers while we let the algorithms have cells with up to 10 nodes. Moreover, two models are made of only one layer. Another indicator of the models simplicity is the percentage of feed-forward and identity layers found in the best models. The feed-forward layer (also called MLP Table~\ref{tab:best_models}) is the most recurrent layer, as it appears on average at least once by graph, although our search space offers more complex layers such as convolution, recurrence or attention layers less frequently picked. This proves that even without regularization penalties, our algorithmic framework does not systematically search for over-complicated models. 

\vspace{0.5cm}
\textit{Does DRAGON always converge towards similar architectures for different datasets?}

The structural indicators for all the datasets from Table~\ref{tab:best_models} are significantly different for each dataset, meaning the framework does not converge to similar architectures. As we set the seed, the initial population of size $K$ is identical for each dataset, but then the performance of the evaluated models affects the creation of the following graphs, which lead to different final model, optimized for each time series. Moreover, Figure~\ref{fig:diff_graph_same_dataset} shows that DRAGON may find very different performing architecture for a same dataset.

\vspace{0.5cm}
\textit{What is the diversity of the operations within the best models?}

The MLP layer is definitively the most used operation within the candidates ones. On average, each model from Table~\ref{tab:best_models} is using at least one MLP layer. Interestingly the CNN and Attention layers are more often used than RNN layers, which were designed for time series. Another intersting insight is that every candidate operation has been at least picked once, which states the operations diversity within the best models.

\vspace{0.5cm}
\textit{Are the best models still \say{deep} neural networks or are they wide and shallow as stated in \citet{DBLP:journals/corr/abs-1909-09569}?}

To answer this question, the observations from \citet{DBLP:journals/corr/abs-1909-09569} do not necessary apply to our results. Our models are on average a bit deeper than wide, bearing in mind that the indicators do not take into account the last MLP as shown Figure~\ref{fig:metamodel_monash}. If we were doing multi-fidelity in the future, this observation might change as one of the reasons mentioned in the paper for wider DNNs is the premature evaluation of architecture before full convergence.

\begin{figure}[htbp]
\centering

\begin{subfigure}[b]{0.25\columnwidth}
\centering
\includegraphics[height=\columnwidth]{data/tourism_quarterly_graph.gv.pdf}
\caption{Tourism Quarterly}
\label{fig:simplegraph}
\end{subfigure}
\begin{subfigure}[b]{0.74\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/m1_yearly_graph.gv.pdf}
\caption{M1 Yearly}
\label{fig:complexgraph}
\end{subfigure}

\begin{subfigure}[b]{0.52\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/m4_hourly_graph.gv.pdf}
\caption{M4 Hourly}
\label{fig:m4_hourly}
\end{subfigure}
\begin{subfigure}[b]{0.45\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/covid_death_graph.gv.pdf}
\caption{Covid Death}
\label{fig:covid_deaths}
\end{subfigure}

\caption{Best DNNs output by our algorithmic framework.}
\label{fig:diverse_graphs}
\end{figure}

\begin{figure}[htbp]
\centering

\begin{subfigure}[b]{0.35\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/0.6532669_elec_weekly.gv.pdf}
\caption{MASE: 0.653}
\label{fig:simplegraph2}
\end{subfigure}
\begin{subfigure}[b]{0.64\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/0.6538869_elec_weekly.gv.pdf}
\caption{MASE: 0.654}
\label{fig:complexgraph2}
\end{subfigure}

\caption{Two different models having similar good performance on the Electricity Weekly dataset (best MASE: 0.644).}
\label{fig:diff_graph_same_dataset}
\end{figure}


%***************************
\subsection{Ablation study}
%***************************

We chose two datasets, M1 monthly and Tourism monthly, in order to reduce the number of experiments we had to perform, as the benchmark was quite large. We conducted tests for four search algorithms for each dataset; these were random search, a population based evolutionary algorithm (EA) with alternating optimisation of hyperparameters and architecture, as well as a version with joint optimisation and, lastly, simulated annealing. To explore the search space, we used an exponential multiplicative monotonic cooling schedule in our simulated annealing algorithm: $T_k = T_0.\alpha^k$. We evaluated 40 neighborhood solutions at each iteration to accomplish this. To ensure fairness between each search algorithm, we conducted five experiments with different seeds (0, 100, 200, 300, and 400), and parameterised the algorithms to evaluate 4000 DNNs. The results of this study are presented in Table \ref{tab:ablation}.

\newcolumntype{Y}{>{\centering\arraybackslash}X}
\begin{table}[htbp]
\caption{Comparison between several search algorithms over two datasets: M1 Monthly and Tourism Monthly. Each configuration has been ran with five different seeds.}
\begin{center}
\begin{tabularx}{\linewidth}{|Y|Y Y|}
        \hline
        \textbf{Search Algorithm} & M1 Monthly & Tourism Monthly \\
        \hline
        \hline
        Random search & $1.098 \pm 0.006$ & $1.645 \pm 0.018$ \\
        \hline 
        EA joint mutation & $\boldsymbol{1.073 \pm 0.004}$  & $\boldsymbol{1.450 \pm 0.003}$\\
        \hline
        EA alternating mutation & $1.080 \pm 0.005$ & $1.451 \pm 0.004$\\
        \hline
        Simulated Annealing & $1.141 \pm 0.044 $& $2.640 \pm 0.037$\\
        \hline                       
    \end{tabularx}
\label{tab:ablation}
\end{center}
\end{table}

The findings suggest that the most exploratory algorithms, specifically the random search algorithm and the evolutionary algorithm, yielded better results than the more locally focused, ie: the simulated annealing. The findings imply the presence of several potential solutions in the search space, but none of them could be accessed by the simulated annealing algorithms from their starting points. The figure \ref{fig:sa} indicates that the most effective DNN was achieved with the help of simulated annealing when $Temp=Temp_{max}$. This suggests greater exploration by the algorithm. Additionally, the random search method produced favorable results. The assessment of 4000 solutions for the Tourism Monthly dataset and M1 dataset was completed within 12 minutes and 4 hours respectively, thanks to the parallelisation of the solution. This shows that the search space has been suitably designed for our problem. However, it does not achieve the same level of performance as our evolutionary algorithms, highlighting the significance of our variational operators. Ultimately, both types of mutation produce very similar results.

\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{0.49\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/lineplot_sa.pdf}
\caption{\centering Best score for each iteration of the simulated annealing algorithm.}
\label{fig:lineplot_sa}
\end{subfigure}
\begin{subfigure}[b]{0.49\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/templot_sa.pdf}
\caption{\centering Temperature value for each iteration of the simulated annealing algorithm.}
\label{fig:templot_sac}
\end{subfigure}

\caption{\centering Simulated annealing algorithm for the M1 Monthly dataset, with seed=100. MASE=1.120.}
\label{fig:sa}
\end{figure}


%*********************************************************************
\subsection{Nondeterminism and instability of DNNs}
%**********************************************************************

An often overlooked robustness challenge with DNN optimization is their uncertainty in performance \citep{summers2021nondeterminism}. A unique model with a fixed architecture and set of hyperparameters can produce a large variety of results on a dataset. Figure \ref{fig:seeds_histo} shows the results on two datasets: M3 Quarterly and Electricity Weekly. For both datasets, we selected the best models found with our optimization and drew 80 seeds summing all instability and nondeterministic aspects of our models. We trained these models and plotted the MASE Figure \ref{fig:seeds_histo}. On the M3 Quarterly, the MASE reached values two times bigger than our best result. On the Electricity Weekly, it went up to five times worst. To overcome this problem, we represented the parametrization of stochastic aspects in our models as a hyperparameter, which we added to our search space. Despite its impact on the performance, we have not seen any work on NAS, HPO or AutoML trying to optimize the seed of DNNs. Our plots of Figure \ref{fig:seeds_histo} showed that the optimization was effective as no other seeds gave better results than the one picked by DRAGON.

\begin{figure}[htbp]
\centering

\begin{subfigure}[b]{0.49\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/m3_quarterly_seeds.pdf}
\caption{M3 Quarterly, best MASE: 1.099}
\label{fig:seeds_m3}
\end{subfigure}
\begin{subfigure}[b]{0.49\columnwidth}
\centering
\includegraphics[width=\columnwidth]{data/electricity_weekly_seeds.pdf}
\caption{Electricity Weekly, best MASE: 0.652}
\label{fig:seeds_elec}
\end{subfigure}

\caption{MASE histogram of the best model performances with multiple seeds for two datasets.}
\label{fig:seeds_histo}
\end{figure}
