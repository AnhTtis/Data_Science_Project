\documentclass[twoside,11pt]{article}


% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage[abbrvbib, preprint]{jmlr2e}
%\usepackage[misc,clock,geometry]{ifsym2}
%\usepackage[abbrvbib, preprint]{jmlr2e}
\include{config}
\usepackage{blindtext}
% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\jmlrheading{None}{None}{1-\pageref{LastPage}}{None; Revised None}{None}{None}{Julie Keisler, El-Ghazali Talbi, Sandra Claudel and Gilles Cabriel}

% Short headings should be running head and authors last names

\ShortHeadings{Optimization framework for AutoDL}{Optimization framework for AutoDL}
\firstpageno{1}

\begin{document}

\title{An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters}

\author{\name Julie Keisler \email julie.keisler@edf.fr \\
       \addr EDF Lab Paris-Saclay\\
       Bd Gaspard Monge, 91120 Palaiseau\\
       University of Lille \& INRIA\\
       170 Av. de Bretagne, 59000 Lille
       \AND
       \name El-Ghazali Talbi \email el-ghazali.talbi@univ-lille.fr \\
       \addr University of Lille \& INRIA\\
       170 Av. de Bretagne, 59000 Lille
       \AND
       \name Sandra Claudel \email sandra.claudel@edf.fr \\
       \addr EDF Lab Paris-Saclay\\
       Bd Gaspard Monge, 91120 Palaiseau
       \AND
       \name Gilles Cabriel \email gilles.cabriel@edf.fr \\
       \addr EDF Lab Paris-Saclay\\
       Bd Gaspard Monge, 91120 Palaiseau}
       

\editor{My editor}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
In this paper, we propose DRAGON (for DiRected Acyclic Graph OptimizatioN), an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an asynchronous evolutionary algorithm on a time series forecasting benchmark. The results demonstrate that DRAGON outperforms state-of-the-art handcrafted models and AutoML techniques for time series forecasting on numerous datasets. DRAGON has been implemented as a python open-source package\footnote{https://dragon-tutorial.readthedocs.io/en/latest/index.html}.

\end{abstract}

\begin{keywords}
Metaheuristics, Evolutionary Algorithm, Neural Architecture Search, Hyperparameter optimization, Time Series Forecasting
\end{keywords}

\input{1-introduction.tex}
\input{2-related_work.tex}
\input{3-search_space_design.tex}
\input{4-search_algorithm.tex}
\input{5-experiments.tex}
\input{6-conclusion_further_work.tex}

% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references

\acks{We would like to thank Margaux Brégère for her careful proofreading and helpful advice. This work has been funded by Electricité de France (EDF). The supercomputers used to run the experiments belong to EDF.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\input{7-Appendix.tex}

\vskip 0.2in

\bibliography{my_bib}

\end{document}
