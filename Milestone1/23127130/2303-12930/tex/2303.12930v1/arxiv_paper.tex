% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)
\pdfoutput=1
\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
\usepackage{appendix}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5040} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Dense-Localizing Audio-Visual Events in Untrimmed Videos:\\ A Large-Scale Benchmark and Baseline}

\author{
Tiantian Geng$^{1,2}$, Teng Wang$^{1,3}$, Jinming Duan$^{2}$, Runmin Cong$^{4}$, Feng Zheng$^{1,5*}$ \\
$^1$Southern University of Science and Technology \ $^2$University of Birmingham\\
$^3$The University of Hong Kong \ $^4$Shandong University  \ $^5$Peng Cheng Laboratory \\
{\tt\small gengtiantian97@gmail.com\ tengwang@connect.hku.hk\ j.duan@bham.ac.uk }
\ \ \\ {\tt\small  \ \ rmcong@sdu.edu.cn \ \ f.zheng@ieee.org}
}


% \author{Tiantian Geng\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Teng Wang\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}

\let\thefootnote\relax\footnotetext{{$*$ Corresponding author}}

Existing audio-visual event localization (AVE) handles manually trimmed videos with only a single instance in each of them. 
However, this setting is unrealistic as natural videos often contain numerous audio-visual events with different categories. 
% In spite of great progress in audio-visual learning, 
% ---Utilizing audio and visual signals for comprehensive video understanding is a significant but challenging problem. 
% Most natural untrimmed videos contain numerous audio-visual events.
% current algorithms are all only based on manually trimmed video clips, which is inconsistent with real-life audio-visual scenes.
% severely limits the progress in this field. 
To better adapt to real-life applications, 
in this paper we focus on the task of dense-localizing audio-visual events, which aims to jointly localize and recognize all audio-visual events occurring in an untrimmed video.
The problem is challenging as it requires fine-grained audio-visual scene and context understanding.
%----------------------------------
% % we re-examine and redefine the AVE task from the aspects of dataset construction and model design. 
% First, to tackle the problems of small scale, single event, and short event duration of the existing dataset for the trimmed videos, we build the first Untrimmed Audio-Visual (UnAV-100) dataset, containing 10K untrimmed videos with over 30K audio-visual events from 100 categories, with with clear categories, accurate temporal boundaries, and varying durations. 
% The UnAV-100 dataset is closer to real life and certainly more challenging.
% Second, we launch an upgraded version of AVE task and provided a baseline, called dense-localizing audio-visual events, which involves both localizing and recognizing all audio-visual events occurring in an untrimmed video. 
% To this end, we formulate this task as a joint learning issue of the classification and regression problems, and use the cross-modal and cross-temporal modeling strategies to fully explore information from different modalities and frames, thereby solving the problems of multiple co-occur events and varying event duration.
% Finally, extensive experiments demonstrate the effectiveness of our method and indicate the significance of multi-scale cross-modal perception and dependency modeling for this task.
%----------------------------------
% In this paper, we focus on the task of dense-localizing audio-visual events, which involves both localizing and recognizing all audio-visual events occurring
% % that are both audible and visible and recognize their corresponding categories 
% in an untrimmed video. 
% The problem is more challenging as it requires fine-grained audio-visual scene and context understanding.
To tackle this problem, we introduce the first Untrimmed Audio-Visual (UnAV-100) dataset, 
% as the first audio-visual benchmark built on untrimmed videos. 
which contains 10K untrimmed videos with over 30K audio-visual events. 
Each video has 2.8 audio-visual events on average, and the events are usually related to each other and might co-occur as in real-life scenes.
Next, we formulate the task using a new learning-based framework, which is capable of fully integrating audio and visual modalities to localize audio-visual events with various lengths and capture dependencies between them in a single pass.
% as a joint classification and regression
% instance-level localization problem.
Extensive experiments demonstrate the effectiveness of our method as well as the significance of multi-scale cross-modal perception and dependency modeling for this task.
% The dataset and code are available at \url{https://unav100.github.io}.

% and our model outperforms related SOTAs for untrimmed videos by a large margin.
% baseline that is able to classify audio-visual events and regress their temporal boundaries in a singe pass, where multi-scale cross-modal fusion, dependency modeling among events and class-aware regression are key factors for performance boost.
% We believe that UnAV-100 dataset, with its realistic complexity, can promote progress in comprehensive audio-visual video understanding in real-life scenes.
% % Our model outperforms related methods by a large margin.
% \vspace{-4mm}
\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
% Human inherently perceives information from multiple modalities to understand real-world scenes and events
Understanding real-world scenes and events is inherently a multisensory perception process for humans~\cite{spence2007audiovisual,  kayser2015multisensory}.
However, for machines, how to integrate multi-modal information, especially audio and visual ones, to facilitate comprehensive video understanding is still a challenging problem. 
In recent years, with the introduction of many audio-visual datasets~\cite{gemmeke2017audio, chen2020vggsound, tian2018audio, Chen_2021_CVPR}, we have seen progress in learning joint audio-visual representations~\cite{arandjelovic2017look, nagrani2021attention,Owens_2018_ECCV}, spatially localizing visible sound sources~\cite{Chen_2021_CVPR, liu2022visual} and temporally localizing audio-visual events~\cite{wu2019dual,zhou2021positive, xia2022cross}, \etc.
\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{1.5mm}
   \includegraphics[width=1.0\linewidth]{fig1_final_new.pdf}
  \caption{
Different from the previous AVE task, dense-localizing audio-visual events involves localizing and recognizing all audio-visual events occurring in an untrimmed video.
In real-life audio-visual scenes, there are often multiple audio-visual events that might be very short or long, and occur concurrently. The top and bottom examples are from the current AVE dataset~\cite{tian2018audio} and our UnAV-100 dataset, respectively.}
   \label{fig:fig1}
  \vspace{-4.5mm}
\end{figure}
While the success of these algorithms is encouraging, they all focus on manually trimmed videos that often just contain a single audio-visual instance/object in each of them.
In particular, audio-visual event localization (AVE)~\cite{tian2018audio} aims to localize a single event that is both audible and visible at the same time in a short, trimmed video, as shown in the upper part of Fig.~\ref{fig:fig1}.
The task setting is impractical as a real-life video is usually long, untrimmed and associated to multiple audio-visual events from different categories, and these events might have various duration and occur simultaneously.
% However, real-life audio-visual scenarios are often far more complex than this setup. where the video usually long, untrimmed and associated with multiple audio-visual events from different categories, and the events might occur simultaneously.
For example, as illustrated at the bottom of Fig.~\ref{fig:fig1}, a man starts singing and other people accompany him on trumpet and violin, and they pause several times along with the music.
% Existing AVE methods based on certain prior assumptions (such as a single and short events) \cite{tian2018audio, wu2019dual, xu2020cross, zhou2021positive, xia2022cross} may fail when faced with such real complex environments. 
Therefore, we argue that it is necessary to re-examine and re-define the AVE task to better adapt to real-life audio-visual scenarios.
% as a more practical problem, and enable it to localize audio-visual events in such a complex, real-life scenario.  

In this work, 
% we make effort in both dataset construction and 
% in order to study AVE task that are closer to practical applications, 
we conduct in-depth research starting from dataset construction to technical solutions.
On the one hand, different from the existing AVE dataset~\cite{tian2018audio} that only contains a single audio-visual event in each 10s trimmed video, we introduce a large-scale \textit{Untrimmed Audio-Visual} (UnAV-100) dataset. It consists of more than 10K untrimmed videos with over 30K audio-visual events covering 100 different event categories. 
% All audio-visual events occurring in videos are annotated by crowdsourcing with their categories and precise temporal boundaries, resulting a total of over $30k$ audio-visual events annotated.
Our dataset spans a wide range of domains, including human activities, music performances, and sounds from animals, vehicles, tools, nature, \etc.    
As the first audio-visual dataset built on untrimmed videos, UnAV-100 is quite challenging for many reasons.
% its own set of challenges.
% For instance, the audio-visual events have various lengths ranging from 0.2s to 60s with an average length of 13.9s.
For instance, each video contains 2.8 audio-visual events on average (23 events maximum), and around $25\%$ of videos have concurrent events.
Besides, the length of audio-visual events varies greatly from 0.2s to 60s. 
There are also rich temporal dependencies among events occurring in a video, \eg., people often clap when cheering, and rain is usually with thunder, \etc.
% We believe that UnAV-100 dataset, as a very challenging benchmark, can facilitate the exploration on fine-grained audio-visual video understanding.
We believe that the UnAV-100 dataset, with its realistic complexity, can promote the exploration on comprehensive audio-visual video understanding.

On the other hand, facing such a complex real-life scene, current methods\cite{tian2018audio, wu2019dual, xu2020cross, zhou2021positive, xia2022cross} formulate the AVE task as a single-label segment-level classification problem and can only identify one audio-visual event for each segment in a trimmed video. 
They fail to locate concurrent events and provide an exact temporal extent for each event in untrimmed videos. 
To address the above issues, we re-define AVE as an instance-level localization problem, called \textit{dense-localizing audio-visual events}. 
We also present a new framework to flexibly recognize all audio-visual events in an untrimmed video and meanwhile regress their temporal boundaries in a single pass. 
Firstly, the sound and its visual information are both critical to identify an audio-visual event, and the events can range across multiple time scales.
Hence, we propose a cross-modal pyramid transformer encoder that enables the model to fully integrate informative audio and visual signals and capture both very short as well as long audio-visual events.
% since audio and visual modalities are both essential for this task, and
% the events usually have various duration, 
% a cross-modal pyramid Transformer encoder are to encode and fuse audio and visual signals at multiple temporal scales.
Secondly, with the observation that the events in a video are usually related to one another, we conduct temporal dependency modeling to learn such correlations, allowing the model to use context to localize events more correctly.
Finally, we design a class-aware regression head for decoding temporal boundaries of overlapping events, together with a classification head to obtain the final localization results.
% a convoultional decoder consisting of a classification and a regression head is applied to get the final localization results in a single pass. 
% Especially, the regression head is class-aware, allowing to regress temporal boundaries for simultaneous events.
Extensive experiments demonstrate the effectiveness of our method, 
and show that it
% and indicate the significance of multi-scale cross-modal perception and dependency modeling for dense-localizing audio-visual events.
outperforms related state-of-the-art methods for untrimmed videos by a large margin.
% Besides, the extensive ablation studies are also conducted to demonstrate the effectiveness of our proposed model. 





% For such a complex and more realistic audio-visual scenario, the current AVE methods~\cite{tian2018audio, wu2019dual, xu2020cross, zhou2021positive, xia2022cross} cannot yet provide solutions.
% They formulate AVE as a single-label segment-level classification problem and can only identify a single audio-visual event for each segment, failing to locate multiple events in untrimmed videos. 
% % are limited to classifying a single audio-visual event for each video segment in trimmed videos with the same duration and  .
% % For such a complex audio-visual scenario, the existing algorithms~\cite{tian2018audio, wu2019dual, xu2020cross, zhou2021positive, xia2022cross} for AVE can not yet provide solutions.
% %-------------------------------------------------------------
% % For example, only simultaneously observing the car and hearing the engine sound indicates the car is running.
% % However, the current AVE dataset~\cite{tian2018audio} is just based on short, trimmed videos with the same duration. And there is just a single audio-visual event in each video, which is quite inconsistent with real-life scenes.

% % Thus an untrimmed video usually associates to more than one audio-visual events from more than one categories, and the events may occur at the same time. 
% % For such an ubiquitous circumstance, the existing algorithms~\cite{tian2018audio, wu2019dual, xu2020cross, zhou2021positive, xia2022cross} can not yet provide solutions.
% %--------------------------------------------------------------------
% % They just formulate AVE as a single-class segment-level classification problem and fail to 
% % and train models to predict the event category for each non-overlapping temporal segment of an input video. 
% % {\color{red}They fail to 
% % localize co-occur audio-visual events and flexibly provide accurate temporal boundaries for all events occurring in a video. 
% On the other hand, temporal action localization (TAL)~\cite{lin2019bmn,lin2018bsn,yang2020revisiting} aims to locate multiple actions in an untrimmed video, but it belongs to a pure vision task without the participation of audio signals.
% Therefore, both these previous algorithms have their own limitations and can not be applied in realistic audio-visual scenes.
% % audio-visual event localization in real-life scenes.
% % in order to address the limitations of previous algorithms and promote progress in more realistic audio-visual video understanding, 



% To tackle the above challenges of UnAV-100 dataset, we also propose an end-to-end framework,
% % To solve this task, we consider it as a classification and regression problem, and propose a strong end-to-end baseline.
% % to recognize all audio-visual events and meanwhile regress their start and end time in an untrimmed video.
% which allows to flexibly recognize all audio-visual events in untrimmed videos and meanwhile regress their temporal boundaries in a single pass. 
% Firstly, the sound and its visual information are both critical to identify an audio-visual event, and the events can range across multiple time scales. 
% Hence, we present a cross-modal pyramid Transformer encoder that enables the model to fully integrate informative audio and visual signals and capture both very short as well as long audio-visual events.
% % since audio and visual modalities are both essential for this task, and
% % the events usually have various duration, 
% % a cross-modal pyramid Transformer encoder are to encode and fuse audio and visual signals at multiple temporal scales.
% Secondly, with the observation that the audio-visual events in a given video are usually related to one another, we conduct temporal dependency modeling to explicitly learn such correlations, allowing the model to use context to localize events more correctly.
% Finally, we design a class-aware regression head for decoding temporal boundaries of overlapping events, together with a classification head to obtain the final localization results.
% % a convoultional decoder consisting of a classification and a regression head is applied to get the final localization results in a single pass. 
% % Especially, the regression head is class-aware, allowing to regress temporal boundaries for simultaneous events.
% Extensive experiments demonstrate the effectiveness of our method, and show that our method
% % and indicate the significance of multi-scale cross-modal perception and dependency modeling for dense-localizing audio-visual events.
% can outperform resent TAL methods for untrimmed videos by a large margin.
% % Besides, the extensive ablation studies are also conducted to demonstrate the effectiveness of our proposed model. 

Our contributions can be summarized as follows:
\vspace{-2mm}
\begin{itemize}
    \item We introduce a large-scale UnAV-100 dataset,  
    % fo r the dense-localizing audio-visual events.
    as the first audio-visual benchmark based on untrimmed videos. There exist multiple audio-visual events in each video, and these events are usually related to one another and co-occur as in real-life scenes. 
    \vspace{-2mm}
    \item We shift the AVE task to a more realistic setup of \textit{dense-localizing audio-visual events}, and propose a new framework, allowing to flexibly recognize all audio-visual events in an untrimmed video and regress their temporal boundaries in a single pass. 
    % that formulates the task as a classification and regression problem. 
    % can fully integrate audio and visual modalities to localize audio-visual events with various lengths, and capture the dependencies between the events in a video.
    % It can localize audio-visual events with various duration and simultaneous occurrences, and modeling temporal dependencies among them.
    % integrates audio and visual modalities at multiple temporal scales,
    % model temporal dependencies among events and also allows to localize co-occur ones.
    % formulates dense-localizing audio-visual events as a 
    \vspace{-2mm}
    \item Extensive experiments demonstrate the significance of multi-scale cross-modal perception and dependency modeling for the task. Our method can achieve superior performance over related state-of-the-art methods for untrimmed videos by a large margin.
\end{itemize}
% inspired by the method~ for action dependency modeling  

% In order to effectively fuse audio and visual information and capture audio-visual events across multiple time scales, 
% we extend the recent method~\cite{zhang2022actionformer} on temporal action localization and propose a cross-modal pyramid Transformer encoder.
% It first encodes audio and visual feature squence

% Based on the data characterises above, we extend the recent method~\cite{zhang2022actionformer} on temporal action localization and first design a cross-modal pyramid Transformer encoder to encode 

% and propose a strong baseline


% Such dependencies are reflects the  in audio-visual scenes for the first time. 
% Such diverse contextual cues is very similar with real-world intuition, 

%-------------------------------------------------------------------------
\section{Related Work}
% \subsection{Untrimmed Video Understanding}
% In recent years, deep learning-based untrimmed video a
% The majority of videos in the wild are naturally untrimmed, usually including several instances with different categories and duration.  
\subsection{Uni-Modal Temporal Localization Tasks}
Deep learning methods have achieved promising performance in temporally localizing target instances using one modality as input, including temporal action localization (TAL) and sound event detection (SED) tasks.
{\bf Temporal action localization (TAL)} aims to detect and classify actions in untrimmed videos. It can be divided into two-stage and single-stage approaches.
A two-stage TAL approach first generates action boundaries with confidence scores, and then classifies their corresponding segments into action categories and refines the generated temporal boundaries~\cite{lin2019bmn,lin2018bsn,bai2020boundary,xu2020g}. 
By contrast, single-stage TAL localizes actions in a single shot without using pre-generated proposals, including anchor-based~\cite{long2019gaussian} 
% (e.g., using pre-defined anchors associated with temporal locations) 
and anchor-free methods~\cite{lin2021learning,yang2020revisiting}. 
Besides, Transformers~\cite{vaswani2017attention}, with its powerful ability of long-range relation modeling, are recently also considered in some single-stage TAL methods~\cite{tan2021relaxed,zhang2022actionformer,liu2022end}.
{\bf Sound event detection (SED)} focuses on recognizing and locating audio events in pure acoustic environments~\cite{mesaros2016metrics}. Approaches~\cite{mesaros2016tut,cakir2015polyphonic,parascandolo2016recurrent} cast it as a classification problem to classify the sound category for each temporal unit. 
Overall, both of them belong to uni-modal temporal localization tasks, \ie, TAL detects visual actions, ignoring the auditory information, while SED only considers sound tracks without utilizing visual content. 
Thus, they are both not beneficial for joint audio-visual scene understanding.  

\subsection{Audio-Visual Event Localization}
Tian \etal~\cite{tian2018audio} first proposed the audio-visual event localization task and introduced the AVE dataset. 
% An audio-guided visual attention mechanism and a dual multimodal residual network are designed to fuse two modalities.
Afterward, Wu \etal~\cite{wu2019dual} presented a dual attention matching module for better high-level event information modeling and also attaining local temporal cues. 
Xu \etal~\cite{xu2020cross} designed a relation-aware module to build connections between visual and audio modalities. 
Besides, a positive sample propagation module was proposed by Zhou \etal~\cite{zhou2021positive} to adaptively aggregate positive audio-visual pairs and avoid interference of irrelevant pairs. 
Yan \etal~\cite{xia2022cross} devised a background suppression scheme to suppress cross-modal asynchronous information and uni-modal noise. 
However, all these methods treat the task as a single-label segment classification problem, which fails to localize multiple, concurrent events in an untrimmed video.
In addition, AVE~\cite{tian2018audio} dataset is based on manually trimmed, short videos that only contain a single audio-visual event in each of them, which is inconsistent with real-life audio-visual scenes.
On the other hand, the recent audio-visual video parsing task~\cite{tian2020unified} aims to identify multiple audio, visual and audio-visual events occurring in videos. However, the methods~\cite{tian2020unified,zhou2021positive,lin2021exploring} are also based on simple, trimmed videos in LLP dataset~\cite{tian2020unified}, and can be only deployed in a weekly-supervised manner.
% divides a video into several segments and predicts the event label for each of them. 

% \subsection{Audio-Visual Datasets}
% % To the best of our knowledge, there is no publicly available audio-visual datasets that provide annotations for untrimmed long videos in unconstrained domains.
% Recently, there are many datasets brought up in the audio-visual community. 
% For example, large-scale datasets, such as AudioSet~\cite{gemmeke2017audio}, VGGSound~\cite{chen2020vggsound}, ACAV100M~\cite{lee2021acav100m} and Kinetics-Sound~\cite{arandjelovic2017look} are collected for audio-visual representation learning. 
% These datasets consist of manually trimmed short video clips often with automatically generated video-level labels, resulting considerable noise. 
% They can only be utilized to learn coarse audio-visual representations and are not suitable for fine-grained audio-visual understanding. 
% Besides, LLP~\cite{tian2020unified} is built for audio-visual video parsing, which aims to detect audio and visual events separately in a weekly-supervised manner, and there are just weak video-level labels provided for training data.  
% AVE~\cite{tian2018audio}, the dataset built for audio-visual event localization, just contains limited event categories with only a single event in each trimmed video. 
% Furthermore, there are also some datasets designed for other audio-visual tasks, such as Flickr-SoundNet~\cite{senocak2018learning} and VGG-SS~\cite{Chen_2021_CVPR} for sound source localization, MUSIC-AVQA~\cite{li2022learning} for audio-visual  question answering. 
% However, all these datasets are based on manually trimmed videos and cannot be used to tackle difficulties in real scenes.
% % By contrast, our proposed UnAV-100 dataset is the first large-scale audio-visual dataset constructed with untrimmed long videos with strong annotations for all data. The detailed comparison of above datasets are illustrated in {\color{red}Section 3.1}.


% ------------------------------------------------------------------------\vspace{-2mm}
\section{The UnAV-100 Dataset}
\subsection{Overview}
To explore audio-visual event localization in more practical scenes, we build a large-scale UnAV-100 dataset, as the first audio-visual dataset for untrimmed videos.
% Each video usually contains multiple audio-visual events annotated with their categories and accurate temporal boundaries.
Each video usually contains multiple audio-visual events annotated with categories and accurate temporal boundaries. The events can be very short as well as long and even overlap in time.
% which contains more than 10K untrimmed videos with over 30K audio-visual events from 100 event categories.
Besides, the dataset covers a wide range of domains, including human activities, music performances, animals/vehicles/tools/natural sounds, \etc.
% Videos in the dataset have various lengths and usually have more than one audio-visual events annotated with high-quality temporal boundaries.
% These events may occur over very long or short periods of time and may happen at the same time, which is much more challenging than the existing dataset~\cite{tian2018audio} for this task. Furthermore, our UnAV-100 is the first dataset in audio-visual learning in long-form and untrimmed videos. 

\begin{table}
  \centering
  \resizebox{\linewidth}{!}{ 
  \begin{tabular}{l|ccccccc }
    \toprule
    Dataset & Videos & Classes & Avg. Length & Annotations & TB & ME  \\
    \midrule
    \midrule
    AudioSet~\cite{gemmeke2017audio} & 2.1M & 527 & 10s & A & \XSolidBrush & \XSolidBrush \\
    Kinetics-Sound~\cite{arandjelovic2017look} & 19K & 34 & 10s & V & \XSolidBrush & \XSolidBrush \\
    VGGSound~\cite{chen2020vggsound} & 200K & 300 & 10s & AV & \XSolidBrush & \XSolidBrush  \\
    ACAV100M~\cite{lee2021acav100m} & 100M & - & 10s & weak AV & \XSolidBrush &\XSolidBrush \\
    \midrule
    AVE~\cite{tian2018audio} & 4,143 & 28 & 10s & AV & \Checkmark & \XSolidBrush  \\
    LLP~\cite{tian2020unified} & 11,849 & 25 & 10s & weak A, V & \Checkmark & \Checkmark \\
    \midrule
    UnAV-100 (Ours) & 10,790 & 100 & 42.1s & AV & \Checkmark &  \Checkmark \\
    \bottomrule
  \end{tabular}}
  \caption{Comparison with related audio-visual datasets. A: audio events; V: visual events; AV: audio-visual events; TB: temporal boundaries; ME: multiple events.}
  \label{tab:comparison}
  \vspace{-4mm}
\end{table}
The comparison with other related audio-visual datasets is shown in Tab.~\ref{tab:comparison}. 
The datasets in the top rows are mainly designed for audio-visual representation learning. 
They all consist of 10s short clips, and there is only a single video-level label provided for each clip.
Among them, AudioSet~\cite{gemmeke2017audio} annotates videos only based on their sound without considering visual information.
% , leading to a low audio-visual correspondence. 
Kinetics-Sound~\cite{arandjelovic2017look}, as a subset of Kinetics~\cite{kay2017kinetics} for action recognition, is annotated based on visual actions, resulting that many videos contain sound tracks unrelated to the visual content (\eg, background music, offscreen voice).
% a curated subset of Kinetics~\cite{kay2017kinetics}, has 34 human action classes that are potentially manifested visually and aurally. However, it still considerable noisy, e.g. many videos contain sound tracks that are completely unrelated to the visual content (e.g. background music, offscreen voice).
Besides, the videos in VGGSound~\cite{chen2020vggsound} and ACAV100M~\cite{lee2021acav100m} have relatively good audio-visual correspondence, while they are curated using automatic algorithms leading to numerous noisy data.
And ACAV100M~\cite{lee2021acav100m} just provides weak labels obtained from pre-trained uni-modal classifiers.
AVE~\cite{tian2018audio}, the existing dataset for audio-visual event localization, just contains 4K samples with limited 28 event classes. 
Each video is a trimmed 10s clip containing only one audio-visual event, and most events span over the entire video, which is seriously inconsistent with real-world scenarios.
% there is just a single event in all videos and the event in the most videos is 10s long, which is not in line with the real-life scenes and severely limits the advance in this field. 
LLP~\cite{tian2020unified} is designed for weakly-supervised audio-visual video parsing, 
% and annotates testing videos with individual audio and visual events,
only providing video-level weak labels for all training data.
% just containing a small part of boundary annotations for separate audio and visual events with week video-level labels for others.
% Moreover, videos in all datasets above are fairly trimmed 10s clips, ensuring there is only one simple audio-visual event in each of them.
By contrast, our UnAV-100 dataset is based on untrimmed videos,
containing over 10K samples with 100 audio-visual event categories. 
Moreover, there are usually multiple audio-visual events annotated in a video with their categories and precise temporal boundaries.
% rather than second-level ones as in AVE~\cite{tian2018audio} (as illustrated in Fig.~\ref{fig:fig1}) and LLP~\cite{tian2020unified}.  
% the annotated examples in AVE and our UnAV-100 are as in
In the following, we provide detailed descriptions about the dataset construction and statistical analysis of our UnAV-100 dataset.
% provide detailed descriptions about dataset construction, as well as statistical analysis about our UnAV-100 dataset.

\subsection{Dataset Construction}
\noindent{\bf Collection.} We select VGGSound~\cite{chen2020vggsound} as our data collection source for its relatively high audio-visual correspondence in videos. 
Specifically, we first chose the categories that are common in our daily life from 300 classes in VGGSound covering diverse domains. 
Then, we downloaded raw videos rather than 10s trimmed clips using the provided YouTube URLs.
% , and totally got around 15K videos.
Since the lengths of raw videos usually span several hours, we randomly cut them within one minute to ensure reasonable duration, meanwhile keeping the videos containing the original 10s clips.
% and at least one event in each of them.
Afterward, we manually verified the presence of audio-visual events in each obtained video.
We found that, since VGGSound was collected in an automated manner, there exist numerous videos that do not contain any audio-visual events. 
For instance, some videos have correct visual content with unrelated sounds like background music and narrations. 
Additionally, it also contains low-quality and animated videos with unrecognizable events.
%Besides, we also removed low-quality and animation videos where the events cannot be identified.
Finally, by filtering the above cases, we selected around 10K from downloaded 15K videos for annotation.

\begin{figure*}[ht]
  \centering
  \setlength{\abovecaptionskip}{1.0mm}
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{dataset_overall_final.pdf}
   \caption{Illustrations of statistics on our UnAV-100 dataset. (a) Distribution of audio-visual events. Bars are grouped by domains, and different colors mean that event categories belong to different domains. 
   (b) The number of audio-visual events in videos. (c) Distribution of event (left) and video duration (right).}
   \label{fig:dataset_overall}
     \vspace{-4mm}
\end{figure*}
\noindent{\bf Annotation.} We annotated videos via an open-source annotation tool VIA~\cite{dutta2019vgg} by crowdsourcing. 
Specifically, we provided expert annotators with a category list for reference, and all audio-visual events occurring in videos are required to be annotated with their categories and independent start and end timestamps. 
Different from the temporal boundaries of visual contents that are usually ambiguous~\cite{caba2015activitynet}, the start and end time points of an audio-visual event are usually clearer and can be easily identified by judging if the event occurs in both audio and visual channels. 
Thus, there is usually high agreement among annotators in labeling temporal boundaries.
In order to ensure annotation quality high, the labeling team is required to check all annotated data carefully. We also employed another group of crowdworkers to manually check it again, resulting in a very time-consuming process.
% indicating that building a large-scale dataset with high-quality annotations is quite a time-consuming and labor-intensive process.
% The pipeline of annotation and data check is provided in our supplementary material. } %可以合并起来写一条
% More details about the dataset construction 
%是否要说明将出现少的事件的视频删去。对一些语义有重合的类进行了合并

\subsection{Statistical Analysis}
Overall, our UnAV-100 dataset contains 30,059 audio-visual events of 100 categories, distributed in 10,790 untrimmed videos for over 126 video hours. 
% Note that we merged some categories having overlapping semantics in VGGSound~\cite{chen2020vggsound} (e.g., merge 'people eating noodle' and 'people eating crisps' as 'people eating'.), resulting in 100 discriminative audio-visual events in total.
The dataset is split into training, validation, and testing sets with a ratio of 3:1:1,
% 6489, 2134, 2167 videos, respectively, 
where a multi-label split strategy~\cite{2017arXiv170201460S} is applied to ensure a well-balanced data distribution in subsets.
Besides, in order to alleviate the effect of long tails, we make sure that there are more than 116 audio-visual events for each category. 
Fig.~\ref{fig:dataset_overall} provides the statistics of our dataset, and the challenges of UnAV-100 include the following:
% \noindent{\bf 1) Long tail data distribution.}
% In Figure~\ref{fig:dataset_overall}(a), we can see that \textit{man speaking} and \textit{woman speaking} categories have a bit more instances than others, 
% % there are much more instances of 'man speaking' (2515) and 'woman speaking'(1519), leading to a data imbalance 
% while it is in line with real-life scenes where human activities are always accompanied by speech.
% For example, 
% % when a man is laughing, he may have just finished speaking or be ready to speak. 
% when a man is playing badminton, then he may explain his actions to the camera. 
% To avoid rare data difficulty, we ensure all categories contains more than 100 instances.

\noindent{\bf1) Multiple events in videos.} As shown in Fig.~\ref{fig:dataset_overall}(b), around $60\%$ of videos contain more than one audio-visual event.
% , and numerous videos even have more than three events.
% It is quite more challenging than AVE~\cite{tian2018audio} where all videos only have a single audio-visual event.
Each video has 2.8 audio-visual events on average (1.6 for distinct ones), and the maximum number is 23.
% The average number of events in a video is 2.8 (and 1.6 for distinct events), and the maximum number of audio-visual events is 23.
Besides, about $25\%$ of videos have concurrent events (the details are in the \textit{Supp. Materials}), which means that there is more than one visible sound source at the same time. 
% {\color{red}the specific analysis of overlapping videos is provided in our supplementary material}. 
\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{1.0mm}
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.9\linewidth]{NPMI_final.pdf}
   \caption{Top pairs of simultaneous and consecutive audio-visual events computed by NPMI falling in the range (-1, 1].}
   \label{fig:NPMI}
   \vspace{-6mm}
\end{figure}

\noindent{\bf2) Various lengths of events and videos.} Fig.~\ref{fig:dataset_overall}(c) shows that a large number of events have very short duration, with the shortest being only 0.2s.
Short events are often difficult to detect, but it aligns with real-life scenes. For example, \textit{dog barking, basketball bounce}, and \textit{fireworks banging} are normally very short audio-visual events.
Besides, the average lengths of audio-visual events and videos are 13.9s and 42.1s, respectively.
% The annotated audio-visual events span from 0.2s to 60s with an average length of 13.9s, and videos last from 10s to 60s having an average length 42.1s.
% we can see that there are a large amount of very short events, which is quite difficult for models to detect.
% Besides, the length of clips that contain events accounts for $44.5\%$ of total video length.  
\begin{figure*}[ht]
  \centering
  \setlength{\abovecaptionskip}{0.5mm}
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{framework_final_new.pdf}
   \caption{The overview of our proposed framework for dense-localizing audio-visual events. The model takes pre-trained CNNs to extract audio and visual features, and uses a cross-modal pyramid transformer encoder to encode and fuse cross-modal features at various temporal scales, which consists of 
   $L_{s}$ uni-modal and $L_{c}$ cross-modal transformer blocks. 
   Then, the temporal dependency modeling is conducted to capture correlations between events. Finally, a classification and a regression head are used to predict the  categories and temporal boundaries of events in an end-to-end manner.  
%   separately to model long-range relations and filter out noise from each modality. The uni-modal features then pass through    
   \label{fig:framwork}
   \vspace{-2mm}}
\end{figure*}

\noindent{\bf3) Rich temporal dependencies between events.} The related audio-visual events usually occur simultaneously or consecutively in a video.
% there are rich temporal dependencies between the audio-visual events occurring in a video, \ie the related events usually occur simultaneously or consecutively.
% A key characteristic of UnAV-100 is the rich temporal dependencies among events, including simultaneous and consecutive occurrence. 
In Fig.~\ref{fig:NPMI}, we show the pairs of simultaneous and consecutive events with the top 10 Normalized Pointwise Mutual Information (NPMI)~\cite{church1990word}, respectively. 
% It reveals that related audio-visual events usually occur simultaneously or successively. 
% We can see some interesting common sense temporal patterns arise.
We can see, frequently, rain is accompanied by thunder, violin and cello are played together, people clap when cheering, \etc.
Such event dependencies are very similar to real-world intuition and reflect human behavior. 
We note that UnAV-100 is the first audio-visual dataset with such context information,
% and can also be further applied to representational relational reasoning research for audio-visual data.
which provides excellent data for building many complex models for audio-visual event dependency modeling.
% and we hope that it may inspire new and interesting algorithms to capture this kind of context in the domain of audio-visual event localization.

\section{Method}
To solve the problem of dense-localizing audio-visual events,
% we develop a method to enable integrate audio and visual modalities to joint recognize and localize multiple audio-visual events with various lengths
we design an architecture to jointly recognize and localize multiple, concurrent audio-visual events with various lengths, and meanwhile capture event dependencies in an untrimmed video.
% can localize temporal boundaries of multiple audio-visual events occurring in an untrimmed video and meanwhile predict their corresponding categories.
% The three main challenges we face are to develop a method that can (1) integrate informative signals from both audio and visual modalities effectively, (2) localize multiple audio-visual events with various lengths and (3) capture temporal dependencies between audio-visual events occurring in a video.
An overview of the proposed framework is illustrated in Fig.~\ref{fig:framwork}.

\subsection{Preliminaries}
\noindent{\bf Problem Statement.} Different from previous AVE methods, we formulate the task of dense-localizing audio-visual events as a joint classification and regression problem.
Formally, given an input video sequence containing both visual and audio tracks, we first divide it into $T$ visual and audio segment pairs $\{V_{t}, A_{t}\}_{t=1}^{T}$, where $T$ varies across videos. 
The groundtruth event set for each video is denoted as $Y=\{y_{n}=(t_{s,n}, t_{e,n}, c_{n})\}^{N}_{n=1}$, where $t_{s,n}$, $t_{e,n}$ are the start and end timestamp of $n$-th event, $c_{n}\in\{1, \cdots, C\}$ is the event category, and $N$ is the total number of audio-visual events in the video.
Then, the model is required to predict $\hat{Y}=\{\hat{y}_{t}=(d_{s,t}, d_{e,t}, p(c_{t}))\}^{T}_{t=1}$ during inference, where $p(c_{t})\in \mathbb{R}^{1\times C}$ is the probabilities of $C$ event categories at moment $t$, $d_{s,t}$ and $d_{e, t}$ are the distances between the moment $t$ to the event's start and end timestamp. Note that $d_{s,t}$ and $d_{e,t}$ are only defined when an event presents at moment $t$.
%-------------------------------------------------------------------------------
% The previous AVE task is just formulated as a simple classification problem to recognize the event class for each non-overlapping one-second segment.
% However, such a setting can only be applied in videos that are manually trimmed with same duration and just contain a single event at each time step. Moreover, the model cannot obtain accurate and flexible event temporal boundaries by just classifying non-overlapping and second-level segments.
% Therefore, inspired by single-stage methods~\cite{lin2021learning,zhang2022actionformer} in temporal action localization (TAL), we address AVE task for untrimmed videos in a more generic way, formulating it as a combination of classification and regression problem.
%------------------------------------------------------------------------
Thus, the final localization results can be obtained by: 
% \vspace{-2mm}
\begin{equation}
    c_{t} = {\arg\max}\, p(c_{t}),\quad t_{s,t}=t-d_{s,t},\quad t_{e,t}=t+d_{e,t}.
\label{equ1} 
% \vspace{-2mm}
\end{equation}
\noindent{\bf Audio and Visual Representations. }We extract audio feature vectors using the VGGish model~\cite{hershey2017cnn} pre-trained on AudioSet~\cite{gemmeke2017audio}. 
% $\{f^{a}_{t}\}^{T}_{t=1}$ ($f^{a}_{t}\in \mathbb{R}^{D_{a}}$) 
And visual feature vectors are extracted by the two-stream I3D~\cite{carreira2017quo} pre-trained on Kinetics-400~\cite{kay2017kinetics}.
% $\{f^{v}_{t}\}^{T}_{t=1}$ ($f^{v}_{t} \in \mathbb{R}^{D_{v}}$) 
% with the same sliding window size and stride used in audio feature extraction. 
% In order to accurately detect events both audible and visible at the same time, the temporal alignment of feature sequences of audio and visual modalities should be ensured. 
Then, we apply two convolutional layers with ReLU to project features from two modalities into a shared embedding space,
% the extracted features from two modalities are projected into a shared embedding space by applying two convolutional layers with ReLU as the activation function, 
resulting $F_{V} = \{f^{v}_{t}\}^{T}_{t=1}$, $F_{A}=\{f^{a}_{t}\}^{T}_{t=1} \in \mathbb{R}^{T\times D}$, where $D$ is the dimension of the embedding space.

\subsection{Architecture}
\noindent{\bf Cross-Modal Pyramid Transformer Encoder.} 
We consider that the sound and its corresponding visual information are both crucial to identify an audio-visual event. 
However, the audio and visual tracks of an untrimmed video often contain a lot of irrelevant information (\eg, background music and off-screen voice), and their content might be misaligned with each other (\eg, a dog appears without barking).  
Besides, the events occurring in untrimmed videos usually range across multiple time scales.
Thus, how to appropriately integrate the two modalities and capture very short as well as long events are both significant for this task.
Here, a cross-modal pyramid transformer encoder is proposed to address the above challenges.  
% an  cross-modal fusion strategy is essentially required.
% Besides, the events occurring in untrimmed videos often have 
% Since the powerful ability of Transformer~\cite{vaswani2017attention} for long-range relation modeling, we design a cross-modal pyramid Transformer encoder that can encode and fuse audio and visual features at various temporal resolutions, allowing the model to detect very short as well as long events. 
% Inspired by ActionFormer~\cite{zhang2022actionformer}, a Transformer-based model for temporal action localization in untrimmed videos, 

Specifically, in order to 
% This operation enables the model to 
capture long-term temporal relations among uni-modal segments and filter out noise in each modality,
%  can be captured and the model can focus more on event-related information in each modality, avoiding the interference of .  
% \eg, the background music or off-screen voice in audio tracks can be eliminated.
the feature sequences from two modalities are first fed into $L_{s}$ stacked uni-modal transformer blocks separately. 
Each block regularly contains a multiheaded self-attention (MSA) and a feed-forward network (FFN) with LayerNorm (LN) and residual connections. 
And position embeddings $E_{pos}\in \mathbb{R}^{T \times D}$ as in ~\cite{vaswani2017attention} are also added in input sequences.
By doing this, the model can focus more on event-related information in each modality.
Afterward, the obtained feature sequences are further encoded into a cross-modal pyramid transformer to integrate informative signals from two modalities at different temporal resolutions.
The module consists of $L_{c}$ stacked blocks. 
In each block, as shown in Fig.~\ref{fig:framwork}, we first temporally downsample the feature sequence of each modality with the stride $2^{l_{c}-1}$ 
% using a single 1D depthwise convolutional layer
, where $l_{c}$ is the index of the current block, and the longer strides are able to capture longer events.
Then, 
% since audio and visual modality are equally important for detecting audio-visual events, here, we apply a symmetric structure of cross-attention.
% In specific, 
we assign downsampled features in the current modality as the key and value vectors, and the features of another modality as the query vector in multiheaded cross-attention (MCA), followed by FFN and LN layers.  
% consisting of two branches of multiheaded cross-attention (MCA) with other regular parts (FFN, LN) in Transformer.
Thus, the audio-guided visual feature $F_{Va}$ and visual-guided audio feature $F_{Av}$ from $l_{c}$-th block can be denoted as:
% 添加公式
\begin{equation}
\begin{split}
% \begin{aligned}
    % & F^{l_{c}-1}_{Va} =\ \downarrow \hat{F}^{l_{c}-1}_{Va}, 
    % F^{l_{c}-1}_{Av} =\ \downarrow \hat{F}^{l_{c}-1}_{Av},\\
    &F^{l_{c}}_{Va} = \mathrm{MCA}(\hat{F}^{l_{c}-1}_{Av}W_{q}, \hat{F}^{l_{c}-1}_{Va}W_{k}, \hat{F}^{l_{c}-1}_{Va}W_{v}),\\
    &F^{l_{c}}_{Av} = \mathrm{MCA}(\hat{F}^{l_{c}-1}_{Va}W_{q}, \hat{F}^{l_{c}-1}_{Av}W_{k}, \hat{F}^{l_{c}-1}_{Av}W_{v}),
% \end{aligned}
\end{split}
\end{equation}
% where $\mathrm{MCA}(Q, K, V)=\mathrm{softmax}(\frac{QK^{T}}{\sqrt{D_{m}}})V)$.
% $D_{m}$ is the dimension of query, key and value vectors and $T$ denotes transpose operation. 
where $l_{c}=\{1, \cdots, L_{c}\}$, $F^{l_{c}}_{Va}, F^{l_{c}}_{Av} \in \mathbb{R}^{T_{l_{c}}\times D}$ ($ T_{l_{c}}=T /2^{l_{c}-1}$), 
$\hat{F}^{l_{c}-1}_{Va}$ and $\hat{F}^{l_{c}-1}_{Av}$ are the features after downsampling, 
$W_{q}, W_{k}, W_{v} \in \mathbb{R}^{D\times D_{m}} $ are learnable parameters 
and $D_{m}=D$ is the dimension of learned query, key and value vectors.
% and when $l_{c}=1$, $F_{Va}$ and $F_{Av}$ are the output of $L_{s}$ uni-modal Transformer blocks.
After cross-modal interactions at various temporal scales, 
we concatenate the enhanced audio and visual features at the same pyramid level,  
% fuse these enhanced features by concating feature sequences from two modalities at the same temporal scale 
getting a cross-modal feature pyramid $Z=\{Z^{l_{c}}\}^{L_{c}}_{l_{c}=1}$, where $Z^{l_{c}}=\mathrm{Concat}(F^{l_{c}}_{Va},F^{l_{c}}_{Av}) \in \mathbb{R}^{T_{l_{c}}\times 2D}$.
% 因此网络可以在不同的时间尺度下对不同时长的事件进行检测

\noindent{\bf Temporal Dependency Modeling.}
The key characteristic of real-life audio-visual scenes is that the related events usually occur simultaneously or consecutively. For example, people are used to clapping when cheering, and cars often honk when passing by. 
Here, inspired by the method~\cite{tirupattur2021modeling} for action dependency modeling in the TAL task, we implicitly capture such simultaneous and consecutive dependencies among audio-visual events at the obtained cross-modal feature pyramid. 
% Once we obtain the cross-modal feature pyramid, the next stage is to model the temporal dependencies between audio-visual events occurring in a video.
Concretely, for each cross-modal feature sequence $Z^{l_{c}}$,
% in cross-modal feature pyramid, 
we first transform and expand the feature dimension to $\hat{Z}^{l_{c}}\in \mathbb{R}^{T_{l_{c}}\times C'\times H}$, 
% $H$ to $CH$ by a nonlinear transformation, and 
splitting it into $C'$ groups, where $C'$ represents the number of hidden classes and $H$ is the transformed feature dimension.
% {\color{red}Here, we set $C'=C$}.
We suppose each hidden class is learned to carry a group of distinctive features for event classification. For simultaneous dependency modeling, the self-attention is performed along the $C'$ dimension of $\hat{Z}^{l_{c}}$, which means a $C'\times C'$ attention matrix that denotes the relevance among hidden classes at each time step can be obtained.
% Concretely, for each cross-modal feature sequence $Z^{l_{c}}$,
% % in cross-modal feature pyramid, 
% we first expand it into a class-aware representation $\hat{Z}^{l_{c}}\in \mathbb{R}^{T_{l_{c}}\times C\times H}$ by a nonlinear transformation, where $C$ is the event category number and $H$ is the transformed feature dimension.
% For simultaneous dependency modeling, the self-attention is performed along the $C$ dimension of $\hat{Z}^{l_{c}}$, which means a $C\times C$ attention matrix that denotes the relevance among classes at each time step can be obtained.
% If two event classes co-occur often, the score should be large, and vice versa.
For consecutive dependency modeling, the self-attention is performed along $T_{l_{c}}$ dimension, getting a $T_{l_{c}}\times T_{l_{c}}$ attention matrix to indicate the correlations among all time steps for the classification of the given class.
Then the output of the two branches followed by FFN and LN layers with residual connections 
are simply merged by element-wise summation to enable the model to capture both types of dependencies. 
% % And the output feature dimensions are converted as the shape of input $Z^{l_{c}}$ to formulate it as a plug-and-play operation. 说即插即用
Note that we share the parameters of dependency modeling across all pyramid levels. 
% 是否表明只用了一层

\noindent{\bf Decoder.} Next, a decoder, consisting of a classification head and a regression head, is applied to decode the enhanced feature pyramid into prediction results in a single pass.
% Next, the improved bi-modal pyramid representations after dependency modeling are processed by our decoder that consists of a classification and a regression head.
Specifically, the classification head predicts the probability $p(c_{t})$ of events at every moment $t$ of all pyramid levels. 
It consists of three layers of 1D convolutions 
% with layer normalization and ReLU activation 
following a sigmoid function as in~\cite{zhang2022actionformer}.
% and outputs $P_{c}=\{p(c_{t})\}_{t=1}^{T_{l_{c}}}\in \mathbb{R}^{T_{l_{c}}\times C}$ 
% % with the shape of $[C, T_{l_{c}}]$, 
% where $T_{l_{c}}$ is the segment number at $l_{c}$-th pyramid level and $C$ is the category number. 
% Thus the shape of output $P_{c}=\{p(c_{})\}$ of each temporal level is $[C, T_{l_{c}}]$
Besides, the regression head outputs the distances to the start and end timestamp of an event $(d_{s,t}, d_{e,t})$ at time step $t$ if the event exists.
We highlight that the regression head is designed to be class-aware, which allows the model to regress temporal boundaries for the overlapping events with different categories.
It is realized by using three 1D convolutions attached with a ReLU, getting the output with the shape of $[2, C, T_{l_{c}}]$ for each pyramid level.
% since there are usually multiple events occurring simultaneously, 
% in~\cite{zhang2022actionformer} into a class-aware one to predict $(d_{s,t}, d_{e,t})$ for each category. 
% We empirically found that the class-aware regression head improves the localization performance compared with class-agnostic one as in ~\cite{zhang2022actionformer}, especially for the videos containing overlapping events.
Here, the pyramid architecture enables the regression head to predict temporal boundaries at different temporal scales, allowing the model to capture the events with various lengths. 
% 因此网络可以在不同的时间尺度下对不同时长的事件进行检测
Note that the parameters of both two heads are shared across all pyramid levels.

\subsection {Training and Inference}
\noindent{\bf Loss Function.} 
% With the predicted $(d_{s,t},d_{e,t}, p(c_{t}))$ for each moment $t$ and the corresponding ground-truth label $Y$, 
We use two losses to train our model in an end-to-end manner, \ie, a focal loss~\cite{lin2017focal} $\mathcal{L}_{cls}$ for classification and a generalized IoU loss~\cite{rezatofighi2019generalized} $\mathcal{L}_{reg}$ for distance regression, as in the TAL  method~\cite{zhang2022actionformer}.
% for the output of classification head and regression head, respectively.
For each video, the loss function is denoted as:
\begin{equation}
    \mathcal{L}=\frac{1}{\mathcal{T}}\sum_{t}\mathcal{L}_{cls} + \frac{\lambda}{\mathcal{N}}\sum_{t}\mathbb{I}_{t}\mathcal{L}_{reg},
\end{equation}
where $\mathcal{T}$ is the total segment number of all levels, $\mathbb{I}_{t}$ is an indicator function denoting if a timestamp contains events, $\mathcal{N}$ is the number of positive segments that contain events across all levels.
Here, we weight the contribution of $\mathcal{L}_{reg}$ with $\lambda=1$ by default. 
% 思考公式是否正确

\noindent{\bf Inference. }
During inference, the outputs of the model are as in Eq.~(\ref{equ1})
% $(t_{s,t}=t-d_{s,t}, t_{e,t}=t+d_{e,t}, p(c_{t}))$ 
for every timestamp $t$ across all levels. 
Then the obtained event candidates are post-processed by a multi-class version of Soft-NMS~\cite{bodla2017soft} to suppress redundant temporal boundaries with high overlaps within the same class. 

\section{Experiments}
\subsection{Experimental Settings}
\noindent{\bf Implementation Details.} For each video, we sample frames at 25 fps, and feed 24 consecutive RGB and optical flow frames into two-stream I3D~\cite{carreira2017quo}, using a sliding window with stride 8.
Then, the two-stream features are 
% output of each stream before the last fully connected layer are 
concatenated (2048-d) as a visual segment. 
Here, the optical flow is extracted by RAFT~\cite{teed2020raft}.
Besides, we extract 128-d audio features by VGGish~\cite{hershey2017cnn} 
for each 0.96s segment with a sliding window (stride=0.32s) to temporally align with the visual ones.
% as $0.96=\frac{24}{25}$.
% In experiments, the visual features extracted by ResNet-50~\cite{he2016deep} pre-trained on ImageNet are exploited to demonstrate the importance of temporal information of visual modality, and the impact of different sampling strides of feature sequences are also explored. 多头注意力参数
Since the input sequences vary in length, we pad or crop them to the maximum length $T=224$, and add masks for all operations in the model.
The dimensions of the embedding space in the encoder and temporal dependency modeling are $D=512$ and $H=128$, respectively. The number of hidden classes $C'=100$.
Our model is trained with the Adam optimizer, and the number of epochs is 40 with a linear warmup of 5 epochs. The initial learning rate is 1e-4 and a cosine learning rate decay is used. The mini-batch size is 16 and the weight decay is 1e-4.

\noindent{\bf Evaluation Metrics.} As a temporal localization task for untrimmed videos, we use mean Average Precision (mAP) to evaluate results.
% using traditional evaluation metrics: . 
Specifically, we report mAPs at the tIoU thresholds [0.5:0.1:0.9] and the average mAP at the thresholds [0.1:0.1:0.9].

\noindent{\bf Baseline Models.} 
Since previous AVE and SED methods are limited to localizing a single event on trimmed videos with the same duration and cannot be applied on untrimmed videos, we only compare our model with recent state-of-the-art TAL models, as shown in Tab.~\ref{tab:compare}.
It includes
% designed for untrimmed videos.
the two-stage model VSGN~\cite{zhao2021video} and single-stage models (TadTR~\cite{liu2022end} and ActionFormer~\cite{zhang2022actionformer}). 
% We explain that TAL is the only existing task aiming to classify instance categories and regress corresponding temporal boundaries at the same time on long and untrimmed videos, 
% We explain that previous SED and AVEL tasks are formulated as a classification problem just for short, trimmed videos, thus it is difficult to conduct a fair comparison with them.
% To investigate different modalities, the several variants of models are compared. 
Here, $L_{s}=2$ and $L_{c}=6$ in the pyramid transformer encoder of our model.
Note that all compared approaches use the same input features as ours to keep a fair comparison.
\begin{table}
  \centering
%   \renewcommand\arraystretch{1.2}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{c|c|cccccc}
    \toprule
    % \hline
    % & & \multicolumn{6}{c}{Mean Average Precision (mAP)}\\
    % \cmidrule(lr){3-8}
    Modality & Method & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & Avg.  \\
    \midrule
    \midrule
    % \hline
    \multirow{4}{*}{A}&
    VSGN~\cite{zhao2021video} & 18.0 & 14.2 & 10.8 & 8.2 & 5.3 & 17.8 \\
    & TadTR~\cite{liu2022end} & 23.0 & 20.5 & 17.6 & 14.4 & 10.4 & 22.8 \\
    & ActionFormer~\cite{zhang2022actionformer} & 37.7 & 32.8 & 27.3 & 22.5 & 15.6 & 36.0 \\
    & Ours & 39.0 & 34.5 & 29.1 & 23.3 & 12.5 & 37.1 \\
    \midrule
    % \hline
    \multirow{4}*{V}&
    VSGN~\cite{zhao2021video} & 14.8 & 11.5 & 8.5 & 6.0 & 4.1 & 15.5 \\
    & TadTR~\cite{liu2022end} & 23.1 & 20.5 & 17.8 & 15.3 & 12.0 & 23.0 \\
    & ActionFormer~\cite{zhang2022actionformer} & 36.3 & 31.9 & 27.4 & 21.8 & 14.8 & 35.4 \\
    & Ours & 37.3 & 32.6 & 28.3 & 22.9 & 14.7 & 35.9 \\
    \midrule
    % \hline
    \multirow{4}*{A\&V}&
    VSGN~\cite{zhao2021video} & 24.5 & 20.2 & 15.9 & 11.4 & 6.8 & 24.1 \\
    & TadTR~\cite{liu2022end} & 30.4 & 27.1 & 23.3 & 19.4 & 14.3 & 29.4 \\
    & ActionFormer~\cite{zhang2022actionformer} & 43.5 & 39.4 & 33.4 & 27.3 & 17.9  & 42.2  \\
    % & \rowcolor{gray!20} Ours & {\bf50.6} & {\bf45.8} & {\bf39.8} & {\bf32.4} & {\bf21.1} & {\bf47.8} \\
    & \cellcolor{gray!20}{Ours} & \cellcolor{gray!20}{\bf50.6} & \cellcolor{gray!20}{\bf45.8} & \cellcolor{gray!20}{\bf39.8} & \cellcolor{gray!20}{\bf32.4} & \cellcolor{gray!20}{\bf21.1} & \cellcolor{gray!20}{\bf47.8} \\
    \bottomrule
  \end{tabular}}
  \caption{Comparison of the results on the test set of UnAV-100 dataset. A: only audio modality; V: only visual modality; A\&V: both audio and visual modalities.}
  \label{tab:compare}
  \vspace{-4mm}
\end{table}

\subsection{Results and Analysis}
To validate the effectiveness of the proposed model, we compare it with recent TAL methods using different modalities as input, and also conduct extensive ablation studies.

\noindent{\bf Comparison Results.}
As shown in Tab.~\ref{tab:compare}, when using one modality as input, our model variants that only apply self-attention in the encoder outperform all compared TAL methods, where TadTR~\cite{liu2022end} and ActionFormer~\cite{zhang2022actionformer} also use an end-to-end transformer-based architecture.
% It indicates that our model have better capacity of long-term temporal dependency modeling a can predict events that occur at the same time.
When using both audio and visual modalities, 
the performance of our model boosts significantly, \eg, $+11.9\%$ and $+10.7\%$ at the average mAP compared with our visual-only and audio-only variants, respectively. These results clearly indicate that both modalities are equally crucial for this task. 
Besides, our model surpasses the compared TAL methods by a large margin, even though they also benefit greatly from multi-modal input. 
Here, we simply concatenate audio and visual features as input of these methods.
% It clearly indicates that audio signal is not just a complement to the visual one, and both modalities are equally crucial for this task. 
% Since TAL models only supports a single modality as input, we only report the results of our model as the first results for dense-localizing audio-visual events.
% Furthermore, another interesting observation is that audio-only models usually have a bit better results than visual-only ones, which may suggest that the occurrence boundaries of audio signals are clearer than corresponding visual ones. 
% We report the first results for the task of dense-localizing audio-visual events in untrimmed videos on UnAV-100 dataset and compare with the state-of-the-art TAL models that just support one modality as input. 
% Extensive ablations are also conducted to explore the effectiveness of the core components of the proposed model.
\begin{table}
  \centering
  \resizebox{0.85\linewidth}{!}{
  \begin{tabular}{ccc|cccccc}
    \toprule
    % \hline
    $L_{s}$ & $L_{c}$ & TD & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & Avg. \\
    \midrule
    \midrule
    % \hline
    % \hline
    2 & 0 &  & 36.8 & 29.3 & 21.8 & 13.8 & 4.9 & 35.5\\
    % \midrule
    % \hline
    2 & 1 &  & 37.6 & 29.6 & 22.2 & 14.0 & 5.1 & 35.4 \\
    2 & 2 &  & 37.0 & 29.3 & 20.9 & 12.5 & 3.8 & 35.3 \\
    \midrule
    2 & 2 & \checkmark & 41.0 & 33.1 & 25.7 & 18.0 & 8.1 & 39.4 \\
    2 & 4 & \checkmark & 49.8 & 43.0 & 35.4 & 25.5 & 11.2 & 45.0 \\
    \rowcolor{gray!20} 2 & 6 & \checkmark & {\bf50.6} & {\bf45.8} & {\bf39.8} & 32.4 & 21.1 & {\bf47.8} \\
    2 & 7 & \checkmark & 49.1 & 44.8 & 39.5 & 32.4 & {\bf21.8} & 46.8 \\
    \midrule
    % \hline
    0 & 6 & \checkmark & 49.4 & 45.2 & 39.2 & {\bf32.5} & 21.6 & 46.7 \\
    1 & 6 & \checkmark & 49.6 & 45.3 & 39.5 & {\bf32.5} & 21.3 & 47.0 \\
    3 & 6 & \checkmark & 48.8 & 44.4 & 39.2 & 32.2 & {\bf21.8} & 46.4 \\
    \bottomrule
    % \hline
  \end{tabular}}
  \caption{Ablation study on cross-modal fusion strategies and the design of feature pyramid. TD: temporal downsampling.}
  \label{tab:pyramid}
  % \vspace{-2mm}
\end{table}
\begin{table}
  \centering
  \resizebox{0.8\linewidth}{!}{
  \begin{tabular}{cc|cccccc}
    \toprule
    DM & CA & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & Avg. \\
    \midrule
    \midrule
      &  & 48.2 & 42.3 & 35.5 & 28.0 & 18.1 & 45.2 \\
     \checkmark &  & 48.5 & 44.2 & 38.7 & {\bf32.6} & 21.0 & 46.1 \\
      & \checkmark & 48.5 & 43.4 & 36.9 & 29.9 & 20.2 & 45.8 \\
    \midrule
    \rowcolor{gray!20} \checkmark & \checkmark & {\bf50.6} & {\bf45.8} & {\bf39.8} & 32.4 & {\bf21.1} & {\bf47.8} \\
    \bottomrule
  \end{tabular}}
  \caption{Ablation study on dependency modeling (DM) and class-aware regression (CA).}
  \label{tab:cooccur-multilabel}
  % \vspace{-2mm}
\end{table}
\begin{table}
  \centering
  \resizebox{1.0\linewidth}{!}{
  \begin{tabular}{l|cccccc}
    \toprule
    Model & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & Avg. \\
    \midrule
    \midrule
    ResNet50~\cite{he2016deep} (RGB) & 46.6 & 42.2 & 37.2 & 30.8 & 20.1 & 44.3 \\
    I3D~\cite{carreira2017quo} (RGB) & 49.1 & 44.8 & 39.0 & 32.0 & {\bf21.3} & 46.7 \\
    \rowcolor{gray!20} I3D~\cite{carreira2017quo} (RGB + Flow) & {\bf50.6} & {\bf45.8} & {\bf39.8} & {\bf32.4} & 21.1 & {\bf47.8} \\
    \bottomrule
  \end{tabular}}
  \caption{Ablation study on different visual features.}
  \label{tab:motion feature}
  \vspace{-4mm}
\end{table}
\begin{figure*}[ht]
  \centering
  \setlength{\abovecaptionskip}{1.0mm}
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{visualization_final.pdf}
   \caption{Qualitative results on the UnAV-100 test set. 
%   We select predicted temporal boundaries that have highest overlap with ground truth. 
   GT: ground truth, A: the prediction of our audio-only variant, V: the prediction of our visual-only variant, AV: the prediction of our audio-visual model. We show boundaries with the highest overlap with ground truth.}
   \label{fig:visualization}
   \vspace{-2mm}
\end{figure*}

\noindent{\bf Cross-Modal Fusion and Pyramid Levels.} We explore the cross-modal fusion strategies and the design of the cross-modal feature pyramid.
In Tab.~\ref{tab:pyramid}, we can see that using only two uni-modal transformer blocks ($L_{s}=2$ and $L_{c}=0$) for each modality separately decreases the performance dramatically.
% which indicates that the cross-modal fusion is essential for this task.
Later, adding one or two cross-modal blocks at the original temporal resolution can just slightly increase mAP scores.
Instead, applying temporal downsampling in cross-modal blocks 
% ($L_{s}=2$ and $L_{c}=2$) 
boosts the performance,
% the average mAP by $+4.1\%$, 
indicating that the cross-modal fusion at multiple temporal resolutions is essential for our model. 
Then, the performance gradually increases by further adding cross-modal pyramid levels, and yet is saturated when $L_{c}=6$.
In addition, we found that the appropriate number of uni-modal blocks is also important,
% the model cannot benefit from removing or adding more uni-modal blocks.
which reveals that applying self-attention before cross-modal interaction can help the model to focus on informative signals and eliminate noise from each modality.
% while more uni-modal blocks may cause an redundant.

\noindent{\bf Dependency Modeling and Class-Aware Regression.} 
% The impact of dependency modeling and class-aware regression are also studied, 
As shown in Tab.~\ref{tab:cooccur-multilabel}, applying temporal dependency modeling and class-aware regression separately can both achieve higher performances than the base model that just contains our transformer encoder with a class-agnostic regression head in the decoder.
Besides, we found that when using both of them, they can promote each other and achieve a further significant performance boost, which clearly demonstrates their effectiveness.
\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{1.5mm}
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.6\linewidth]{overlap_final_new.pdf}
   \caption{Performance comparison of our models and the TAL method (ActionFormer~\cite{zhang2022actionformer}) on the videos from the UnAV-100 test set, containing concurrent events with different overlap rates.
%   The impacts of dependency modeling (DM) and class-aware regression (CA) on the videos containing co-occur events with different overlap rates.
   }
   \label{fig:overlaprate}
   \vspace{-4mm}
\end{figure}

\noindent{\bf The Impact of Motion Features.}
% Moreover, we also explore the impact of different visual features as shown in 
In Tab.~\ref{tab:motion feature},
% We got the features by utilizing global average pooling over consecutive frames in each segment to get the features.
we observe that utilizing both RGB and optical flow features extracted by I3D~\cite{carreira2017quo} achieves the best performance.
It outperforms the model that uses visual features extracted by ResNet50~\cite{he2016deep} pre-trained on ImageNet by a large margin ($+3.5\%$ at the average mAP).
Even though it is proved in ~\cite{tian2018audio} that motion features are useless for audio-visual event localization, we argue that our experiment clearly demonstrates their significance for dense-localizing audio-visual events. 

\noindent{\bf The Capability of Localizing Concurrent Events.}
% In order to explore the capability of our model on co-occur event localization, 
We further evaluate our models and the state-of-the-art TAL method~\cite{zhang2022actionformer} on the videos that contain concurrent events with different overlap rates in Fig.~\ref{fig:overlaprate}. 
We observe that our model equipped with dependency modeling and class-aware regression obviously gains more performance improvement on the videos with higher overlap rates, compared with our baseline and ActionFormer~\cite{zhang2022actionformer}.
It suggests that our model has a better ability to localize overlapping audio-visual events in untrimmed videos.

\noindent{\bf Qualitative Results.} In Fig.~\ref{fig:visualization}, we present the qualitative results of our model variants that utilize different modalities as input.
We observe that the model using both modalities can localize audio-visual events more correctly, even though some events occur simultaneously or have short duration.
By contrast, since the sound of \textit{auto racing} almost spans the whole video, the audio-only model gets the wrong boundaries of the event without the help of visual information. And similar errors also occur when using the visual-only model.
Overall, it demonstrates again that audio and visual modalities complement each other and are equally significant for dense-localizing audio-visual events. 
% % Overall speaking, our model could correctly localize the audio-visual events occurring in the video.
% In specific, our model predicts \textit{man speaking} events, even though some lengths are very short, and the overlapping events (\eg, \textit{auto racing} and \textit{skidding}) are also successfully predicted, which reveals the good capacity of our model.
% However, there are still some failure cases. For example, in term of some quite short instances (\eg, \textit{people laughing} in Figure~\ref{fig:visual}), our model might fail to capture them. 
% Besides, the model incorrectly predicts a \textit{man speaking} event that is only visible with mixing unrelated sounds.
% This suggests that it is still difficult for the model to localize audio-visual events in complex misaligned audio-visual cases.
More ablation studies and qualitative results can be found in the \textit{Supp. Materials}.

\section{Conclusion}
In this work, we investigate the dense-localizing audio-visual events problem, which aims to recognize and localize all audio-visual events occurring in an untrimmed video. 
To facilitate this research, we build a large-scale UnAV-100 dataset consisting of more than 10K untrimmed videos with over 30K audio-visual events covering 100 categories.
We also propose a new framework, formulating the task as a joint classification and regression problem,  
% allowing to classify multiple audio-visual events and accurately regress their temporal boundaries in a single pass for an untrimmed video. 
which is capable of localizing audio-visual events that have various lengths and overlap in time, and capturing the dependencies between them in a video.
% where a cross-modal pyramid Transformer encoder fuses audio and visual modalities at multiple temporal scales, a dependency modeling module captures correlations between related events in a video, and a decoder decodes features into temporal boundaries and categories of events in a single pass. 
Our results demonstrate the superiority of our model,
% compared with the recent TAL methods for untrimmed videos,
indicating the significance of cross-modal perception and dependency modeling for this task.
% We hope that our dataset can promote progress in fine-grained and comprehensive audio-visual video understanding.

% For example, in Fig.~\ref{fig:visual}, our model failed to localize the very short \textit{people laughing} event. And in Fig.~\ref{fig:overlap}, it is observed that our model performs poorly on the videos containing the co-occurring events with low overlap rates, which often involves more complex relations between events.
% Therefore, more advanced models with stronger abilities of cross-modal fusion and relation modeling are expected to boost performance further.
% We hope our dataset, as the first attempt at audio-visual understanding on untrimmed videos, can promote progress in fine-grained and comprehensive audio-visual video understanding
% has the potential to inspire more people to explore the field.

% \noindent{\bf Personal Data/Human Subjects.} Videos in UnAV-100 are public on YouTube, and from the open-sourced database VGGSound~\cite{chen2020vggsound}. Our dataset does not contain personally identifiable information or offensive content.

% \vspace{+4mm}
% \noindent{\bf Acknowledgments.} This work was supported by the National Key R\&D Program of China (Grant NO. 2022YFF1202903) and the National Natural Science Foundation of China (Grant NO. 62122035 and 61972188). 

% \noindent
% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%   \caption{Example of caption.
%   It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%   \label{fig:onecol}
% \end{figure}

% \noindent
% Compare the following:\\
% \begin{tabular}{ll}
%  \verb'$conf_a$' &  $conf_a$ \\
%  \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
% \end{tabular}\\
% See The \TeX book, p165.

%-----------------------------------------------------------------------
% \url{https://www.computer.org/about/contact}.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{arxiv_paper}
}
\newpage
\section*{Appendix}
\appendix
\section{More Statistical Analysis}
\noindent{\bf Concurrent Events.} There are usually multiple audio-visual events occurring simultaneously in UnAV-100 dataset as in real-life scenes. 
Here, we define the overlap rate $\mathcal{O}$ of each video as:
% Specifically, the overlap rate $\mathcal{O}$ of each video is denoted as:
% \vspace{-3mm}
\begin{equation}
    \mathcal{O} = \frac{U_{o}}{U_{e}},
    % \vspace{-3mm}
\end{equation}
where $U_{o}$ is the temporal union of overlapping intervals, and $U_{e}$ is the temporal union of the intervals of all audio-visual events in the video.
Totally, there are around $25\%$ of videos (2,651) containing concurrent audio-visual events ($\mathcal{O} > 0.01$, considering annotation errors) in our UnAV-100 dataset.
The overlap rate distribution of these videos is illustrated in Fig.~\ref{fig:overlap}.
We can see that the videos with low and high overlap rates both have high proportions.
Higher overlap rates might indicate that the events have higher correlations and usually occur at the same time, which requires the model to have a strong ability of dependency modeling. 
% Instead, a low overlap rate implies more complex audio-visual scenes and event relationships, which is more challenging. 
% 2，train/val/test各子集数据分布 （可选）
% 3，热图，各类事件相关性

\noindent{\bf Temporal Dependencies between Events.} We show NPMI (Normalized Pointwise Mutual Information)~\cite{church1990word} of the pairs of simultaneous and consecutive audio-visual events for all 100 event categories in Fig.~\ref{fig:npmi}(a) and Fig~\ref{fig:npmi}(b), respectively. NPMI is commonly used in linguistics to represent the co-occurrence between two words.
Firstly, in Fig.~\ref{fig:npmi}(a), we can observe that the event categories from the same domains are more likely to occur concurrently, \eg, the events of human activities, music performances, and the sounds of vehicles/natural.
Besides, the events from various domains are usually accompanied by human activities, \eg, \textit{playing acoustic guitar} with \textit{male singing}, \textit{basketball bounce} with \textit{people crowd}, \etc.
Secondly, in Fig~\ref{fig:npmi}(b), in addition to the NPMI for consecutive occurrences of different audio-visual events, we also compute the values for the events from the same categories, which might be larger than 1.
It can be observed that the same events tend to occur repetitively in a video, especially for some events that usually happen in a short period of time, such as  \textit{people nose blowing}, \textit{people sneezing} and \textit{basketball bounce}, \etc.
Moreover, diverse consecutive dependencies also exist between different audio-visual events.
% In a word, there are rich temporal dependencies between the audio-visual events occurring in a video in our UnAV-100 dataset. 

\noindent{\bf Comparison with Existing TAL Datasets.} In Tab.~\ref{tab:tal},  we compare our UnAV-100 dataset with four popular benchmarks for temporal action localization. All these datasets are based on untrimmed videos and have relatively small scales, since 
annotating temporal boundaries for all instances in videos is labor-intensive and time-consuming.
% dataset construction requires labor intensive manual annotation and verification,
Our UnAV-100 is the only dataset that combines both audio and visual signals to annotate instances, while others just utilize visual content in videos. Their audio tracks are usually very noisy and unrelated to the visual information, \eg, background music and narrations, thus these datasets are not suitable for joint audio-visual video understanding. Besides, these benchmarks all focus on specific domains, such as human activities, sports, cooking, \etc. By contrast, our UnAV-100 covers many different domains including human/music/sport/animal/nature, \etc, which helps machines to understand more diverse audio-visual scenes in the wild.
\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{2mm}
   \includegraphics[width=0.8\linewidth]{overlap_rate_new.pdf}
  \caption{Overlap rate distribution of the videos that contain concurrent events in our UnAV-100 dataset.}
   \label{fig:overlap}
  % \vspace{-3mm}
\end{figure}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{table}
  \centering
  \resizebox{1.0\linewidth}{!}{
  \begin{tabular}{l|ccccc}
    \toprule
    Dataset & Videos & Classes & \tabincell{c}{Avg.\\Length} & \tabincell{c}{Avg. \\Instances} & Domains \\
    \midrule
    \midrule
    Breakfast~\cite{kuehne2014language} & 1,712 & 48 & 162s & 6 & Cooking \\
    THUMOS14~\cite{idrees2017thumos} & 413 & 20 & 212s & 15.5 & Sports \\
    ActivityNet~\cite{caba2015activitynet} & 19,994 & 200 & 115s & 1.5 & Human Activities   \\
    Charades~\cite{sigurdsson2016hollywood} & 9,848 & 157 & 30s & 6.8 & Daily Activities\\
    \midrule
    UnAV-100 (ours) & 10,790 & 100 & 42s & 2.8 & Unconstrained \\
    \bottomrule
  \end{tabular}}
  \caption{Comparison with temporal action localization datasets based on untrimmed videos.}
  \label{tab:tal}
  \vspace{-4mm}
\end{table}
%------------------------------------------------------------------------
\section{Implementation Details}
% 一些具体的实验设置 （特征提取细节，模型结构，训练测试）
% （使用了在验证集上的best_epoch在测试集测试）
\noindent{\bf Feature Extraction.}
The visual features are extracted using two-stream I3D~\cite{carreira2017quo}, which inputs a set of 24 RGB and optical flow frames extracted at 25 fps.
Each frame is first resized such that the shortest side is 256 pixels, and then the center region is cropped to $224\times224$.
A 1024-d RGB or flow feature vector is obtained from the final convolutional layer of the corresponding branch of I3D.
Then, the two vectors are concatenated producing 2048-d features for each stack of 24 frames.
The audio features are extracted using VGGish~\cite{hershey2017cnn}. 
The input is a $96\times64$ log mel-scaled spectrogram extracted for each 0.96s segment, which is obtained by applying \textit{Short-Time Fourier Transform} on a 16 kHz mono audio track. Then, a 128-d feature vector can be obtained after an activation function and before a classification layer.
Here, we use 24 frames for each visual segment to temporally match with the input of the audio modality as $\frac{24}{25}=0.96$.
% the same sliding window with the same stride (8 frames/0.32s) when extracting features for two modalities.

\noindent{\bf Network Architecture.}
In the cross-modal pyramid transformer encoder, the number of attention heads is 4 in both uni-modal and cross-modal blocks. The temporal downsampling operation is realized by using a single depth-wise 1D convolution as in~\cite{zhang2022actionformer}.
For temporal dependency modeling, the output dimension is converted as the shape of input to formulate it as a plug-and-play operation, and we just apply this operation once in our model.
% Besides, a learnable coefficient is used when merging the output of the two branches as in~\cite{tirupattur2021modeling} to model both types of dependencies.
% multihead的数量，下采样的实现，dependency modeling(层数，系数，head数)

\noindent{\bf Reproducibility.}
All our models are trained on a single 32GB NVIDIA Tesla V100 GPU and implemented in PyTorch deep-learning framework.
During inference, we evaluate the performances of our method on the test set of our UnAV-100 and use the best models on the validation set.    

\section{Ablation Study}
\begin{table}[t]
  \centering
  \resizebox{0.8\linewidth}{!}{
  \begin{tabular}{c|cccccc}
    \toprule
    PE & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & Avg. \\
    \midrule
    \midrule
    \rowcolor{gray!20} \checkmark & {\bf50.6} & 44.8 & {\bf39.8} & 32.4 & 21.1 & {\bf47.8} \\
     & 49.5 & {\bf45.1} & 39.7 & {\bf32.8} & {\bf21.9} & 47.0 \\
    \bottomrule
  \end{tabular}}
  \caption{Ablation study on position encoding (PE).}
  \label{tab:position}
  \vspace{-2mm}
\end{table}
\begin{table}
  \centering
  \resizebox{0.82\linewidth}{!}{
  \begin{tabular}{c|cccccc}
    \toprule
    $\lambda$ & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & Avg. \\
    \midrule
    \midrule
    0.2 & 49.9 & 45.0 & 39.6 & 32.2 & 20.7 & 46.9 \\
    0.5 & 50.1 & 45.4 & 39.8 & 32.3 & 21.2 & 47.3 \\
    \rowcolor{gray!20} 1 & {\bf50.6} & {\bf45.8} & 39.8 & 32.4 & 21.1 & {\bf47.8} \\
    2 & 49.8 & 45.3 & {\bf40.2} & {\bf33.0} & {\bf22.4} & 47.2 \\
    5 & 49.0 & 44.7 & 39.2 & 32.3 & 22.2 & 46.4 \\
    \bottomrule
  \end{tabular}}
  \caption{Ablation study on loss weight $\lambda$.}
  \label{tab:loss}
  \vspace{-2mm}
\end{table}
\begin{table}
  \centering
  \resizebox{0.85\linewidth}{!}{
  \begin{tabular}{c|cccccc}
    \toprule
    Stride & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & Avg. \\
    \midrule
    \midrule
    \rowcolor{gray!20} 8 & {\bf50.6} & {\bf44.8} & {\bf39.8} & 32.4 & 21.1 & {\bf47.8} \\
    16 & 48.9 & 44.6 & 39.0 & {\bf32.9} & {\bf21.8} & 46.7 \\
    24 & 49.7 & 44.7 & 38.5 & 31.0 & 20.9 & 47.0 \\
    \bottomrule
  \end{tabular}}
  \caption{Ablation study on temporal feature stride.}
  \label{tab:stride}
  \vspace{-2mm}
\end{table}
\begin{table}
  \centering
  \resizebox{0.86\linewidth}{!}{
  \begin{tabular}{c|cccccc}
    \toprule
    $T_{max}$ & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & Avg. \\
    \midrule
    \midrule
    192 & 49.9 & 45.2 & 39.7 & 32.6 & 21.7 & 47.0 \\
    \rowcolor{gray!20}224 & {\bf50.6} & {\bf45.8} & 39.8 & 32.4 & 21.1 & {\bf47.8} \\
    256 & 49.6 & 45.3 & {\bf39.9} & {\bf33.1} & {\bf22.3} & 47.2 \\
    \bottomrule
  \end{tabular}}
  \caption{Ablation study on maximum input sequence length.}
  \label{tab:length}
  \vspace{-2mm}
\end{table}
\begin{table}
  \centering
  \resizebox{0.95\linewidth}{!}{
  \begin{tabular}{cc|cccccc}
    \toprule
    SD & CD & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & Avg. \\
    \midrule
    \midrule
    & & 48.5 & 43.4 & 36.9 & 29.9 & 20.2 & 45.8 \\
     \checkmark & & 49.5 & {\bf45.3} & 39.7 & 32.6 & 21.2 & 46.9 \\
     & \checkmark & 49.5 & 44.8 & 39.7 & {\bf32.7} & {\bf21.7} & 46.8 \\
     \rowcolor{gray!20}\checkmark & \checkmark & {\bf50.6} & 44.8 & {\bf39.8} & 32.4 & 21.1 & {\bf47.8} \\
    \bottomrule
  \end{tabular}}
  \caption{Ablation study on dependency modeling. SD: simultaneous dependency branch; CD: consecutive dependency branch.}
  \label{tab:dependency}
  \vspace{-4mm}
\end{table}
\begin{table}[ht]
  \centering
  \resizebox{1.0\linewidth}{!}{
  \begin{tabular}{l|cccccc}
    \toprule
    Method & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & Avg. \\
    \midrule
    \midrule
    ActionFormer~\cite{zhang2022actionformer} & 73.4 & 67.5 & 57.6 & 47.6& 33.7 & 56.0 \\
    \rowcolor{gray!20}Ours & {\bf74.8} & {\bf70.1} & {\bf60.7} & {\bf48.1} & {\bf34.0} & {\bf57.5} \\
    \bottomrule
  \end{tabular}}
  \caption{Experiments on THUMOS14 dataset with only visual modality as input (mAP@[0.3:0.1:0.7] is reported).}
  \label{tab:thumos}
  \vspace{-4mm}
\end{table}
% 1,是否使用位置编码
\noindent{\bf Position Encoding.}
We explore the impact of position encoding in our transformer encoder. 
As shown in Tab.~\ref{tab:position}, adding position embeddings can improve the performance by $0.8\%$ in average mAP, even though the temporal convolutions (\ie, the projection layer and downsampling operations) already leak the location information as pointed out in~\cite{xie2021segformer,zhang2022actionformer}.

% 3，回归loss的权重
\noindent{\bf Loss Weight.}
We also provide the ablation study on the loss weight $\lambda$ in our loss function.
We train the model using different loss weights $\lambda \in [0.2, 0.5, 1, 2, 5]$, and report the results in Tab.~\ref{tab:loss}.
It can be seen that the default value $\lambda=1$ can yield the best performance. 

% 2，特征采样步长
\noindent{\bf Feature Stride.} 
In our experiments, we use stride=8 with a sliding window of 24 frames by default when extracting visual and audio features. Here, we study the performance variation using different feature strides in Tab.~\ref{tab:stride}.
% When using lower resolution (stride=16), the results drop obviously ($-1.1\%$ at average mAP), and further r
Reducing the temporal feature resolution (\ie, larger strides, 16/24) leads to obvious performance degradation, which is intuitively reasonable since the model might fail to detect many short audio-visual events at a low temporal resolution.

% 4, 输入序列长度
\noindent{\bf Maximum Input Sequence Length.}
Furthermore, we vary the length of the maximum input sequences of our model, and the results are provided in Tab.~\ref{tab:length}. We can observe that our model has quite stable results when using different $T_{max}$, and $T_{max}=224$ gets the best results.

\noindent{\bf Dependency Modeling.}
Since the two branches of temporal dependency modeling aim to capture different correlations between events within a video, we run an ablation by removing each of the branches and show the results in Tab.~\ref{tab:dependency}. 
It indicates that applying each branch separately also leads to improvement, and the best result can be achieved by combing both branches to model simultaneous and consecutive dependencies at the same time.

%------------------------------------------------------------------------
\section{Experiments on Existing TAL Dataset}
We also conduct experiments on THUMOS14 dataset~\cite{idrees2017thumos}, a widely-used dataset for temporal action localization. The evaluation results on THUMOS14 test set using only visual input are provided in Tab.~\ref{tab:thumos}. We use the same strategy to extract features on THUMOS14 as used on UnAV-100 for both methods to keep a fair comparison.
We can see that our model outperforms ActionFormer~\cite{zhang2022actionformer} by a large margin ($+3.1\%$ mAP at tIoU=0.5), even without the cross-modal fusion strategy. 
Besides, we tried to only use the audio modality in THUMOS14 to locate actions, but got very bad results (just $4.3\%$ average mAP) on both models, which indicates that the audio tracks in THUMOS14 are quite noisy and cannot provide useful information. 

\section{More Qualitative Results}
More qualitative results are presented in Fig~\ref{fig:visual}, which includes the prediction results of our model variants using different modalities as input. Generally speaking, cross-modal perception encourages the model to obtain more correct localization results. 
For example, Fig.~\ref{fig:visual}(a) refers to the relatively constant visual information versus dramatically changing audio signals.
By integrating both modalities, the model can better judge the event boundaries. 
Besides, our audio-visual model can also get promising performance in some complex audio-visual scenarios, as in Fig.~\ref{fig:visual}(c) and Fig.~\ref{fig:visual}(d), where many audio-visual events occur concurrently or over very short periods of time. 

\section{Discussion}
% limitations
% 1，缺少对时空特征的联合建模
% 2，关系建模模块显存占用率过高
% 3，模型能力（失败样例分析）
\noindent{\bf Limitations.}
There is still a wide scope for exploration and improvement on the basis of our work. 
For instance, our dataset is limited to a temporal localization task. 
We will explore other audio-visual problems, such as representation learning and sound source localization in real-life and complex scenarios in our subsequent study.
Besides, although our model can obtain a promising performance, as a baseline, its capability is still limited in some complex situations.
% (\eg, very short instances and complex event associations).
% Our model integrates audio and visual information at multiple temporal scales to locate audio-visual events with various lengths, and is capable of capturing temporal dependencies between related events in a video. However, its ability is still limited in some complex situations.
For example, in Fig.~\ref{fig:visual}(c), the model gets an incorrect boundary of the \textit{dog barking} event when the barking brown dog is out of the screen and a non-barking black one can be seen. This indicates that our model might fail to effectively filter out interference information for such a difficult case.
And the model might also fail to predict precise boundaries when one modality persists while another disappears for a short period of time (\eg, the event of \textit{vacuum cleaner cleaning floors} in Fig.~\ref{fig:visual}(c)).
In addition, for some instant events with very short duration (\eg, \textit{basketball bounce} in Fig.~\ref{fig:visual}(d)), our model might get unsatisfactory results.
Overall, dense-localizing audio-visual events is inherently a very challenging task, and it requires the model to have a strong fine-grained cross-modal understanding ability.
Therefore, more advanced models that could better solve the above difficulties are expected to boost performance further.
We hope our work as the first attempt at untrimmed audio-visual video understanding can inspire more people to explore the field.

\noindent{\bf Ethic concerns and biases.}
Our UnAV-100 is sourced from VGGSound dataset~\cite{chen2020vggsound} that has already tried to mitigate ethical issues. During data collection, we made further efforts to manually check all videos to avoid mature, sensitive, or offensive content. 
Besides, our UnAV-100 follows the natural distribution of instances present on the website, which may reflect some biases in topics. For example, there are more {\textit{man/woman speaking}} events than other categories. Efforts have been made to mitigate such imbalance. 


\begin{figure*}[ht]
  \centering
  \setlength{\abovecaptionskip}{2mm}
  \subfloat[\label{1a}]{
   \includegraphics[width=0.72\linewidth]{co-occur.pdf}}\\
    \subfloat[\label{1b}]{
   \includegraphics[width=0.72\linewidth]{consecutive_npmi.pdf}}
  \caption{NPMI of the pairs of simultaneous (a) and consecutive (b) audio-visual events in our UnAV-100 dataset. 
%   (a) NPMI of simultaneous events, (b) NPMI of consecutive events, 
  In (b), the horizontal axis shows the first event, and the vertical axis shows the second subsequent event.
  The event categories are grouped by domains.}
   \label{fig:npmi}
%   \vspace{-7mm}
\end{figure*}
\begin{figure*}[ht]
  \centering
  \setlength{\abovecaptionskip}{2mm}
    \subfloat[\label{1aa}]{
            \includegraphics[width=0.94\linewidth]{visual4.pdf}}\\
    \subfloat[\label{1bb}]{
            \includegraphics[width=0.94\linewidth]{visual2.pdf}} \\
    \subfloat[\label{1cc}]{
             \includegraphics[width=0.94\linewidth]{visual1.pdf}}\\
   \subfloat[\label{1dd}]{
             \includegraphics[width=0.94\linewidth]{visual3.pdf}}
  \caption{More qualitative results on the UnAV-100 test set. GT: ground truth, A: the prediction of the audio-only variant, V: the prediction of the visual-only variant, AV: the prediction of our audio-visual model. We show boundaries with the highest overlap with ground truth.}
   \label{fig:visual}
%   \vspace{-7mm}
\end{figure*}

\end{document}
