%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Experiments %%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{Experiments}

In this section, experimental results for the proposed method are reported on static datasets CIFAR10, CIFAR100 and neuromorphic datasets CIFAR10-DVS. ResNet and VGG network architectures are implemented to demonstrated the versatility of the method. The code is implemented with Tensorflow and all experiments are conducted on TPU.

\subsection{Network Architectures}

ResNet and VGG networks are implemented for experiments with various configurations, the configurations of VGG-like architectures are listed in Table \ref{tab:vgg_models}. The convolutions between pooling layers are viewed as a stage, same number of filters are used for convolutions in the same stage. The models end with two fully connected layers, the former layer is a hidden layer, while the latter layer is for output. 

The VGG-8 model is the model architecture\cite{WeiFang2020IncorporatingLM} for CIFAR10 dataset, 92\% parameters of the model are from the first dense layer because of the large size of output of the last pooling layer. So VGG-9 is proposed to deepen the network as well as reduce the model size.

Each convolution layer is followed by a batch normalization layer and a PLIF layer to fire spikes, the first Conv-BN-PLIF is viewed as the encoder to convert image pixels into spikes.

Similarly, the configuration of ResNet architectures is listed in Table \ref{tab:resnet_models}. The residual connections happened at the membrane level following \cite{hu2021advancing}. After convolution layers, a pooling layer with large size is applied to convert the output image into 1-D, after which a fully-connected layer is appended as the output.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=16cm, height=8cm]{figures/st_mt_cifar10_100.png}
    \caption{Performance comparison of ST and MT on CIFAR10 / CIFAR100 with various steps.}
    \label{fig:st_mt_cifar10_cifar100}
\end{figure*}

\subsection{Static Datasets Classification}

CIFAR10 and CIFAR100 are the static datasets evaluated in this paper. The datasets contains 50K training and 10K test images with 32x32 pixels. Random horizontal flip, random rotation by 15 degree and random height, width shift by 10\% fraction are used as the data augmentation approaches.

In training, SGD optimizer is adopted with 0.9 momentum, the learning rate is initially set to 0.1 and decay by 0.1 after 100 epochs, after 500 epochs, the peak accuracy is reported.

The accuracy comparison on CIFAR10, CIFAR100 between SNNs with ST, SNNs with MT and the ANN counterparts are illustrated in Fig \ref{fig:st_mt_cifar10_cifar100}. It's observed that on both CIFAR10 and CIFAR100 datasets, all of the VGG-8, VGG-9 adn ResNet-20 models, all the time steps from 1 to 5, the accuracy of SNNs with MT outperforms SNNs with ST. And for all combinations except the ResNet-20 on CIFAR100, SNNs with MT can achieve close or even better accuracy at $time\_step=1$.

The comparison between our results and previous state-of-the-art results of similar model architectures is listed in Table \ref{tab:cifar10_cifar100_dvscifar10_results}. The best accuracy of our proposed VGG-9 network are 94.74\% and 75.53\% respectively, with a 0.49\% and 1.29\% absolute accuracy improvement compared to the existed state-of-the-art results of simialr model architectures. Furthermore, the VGG-9 model can reach 94.71\% and 74.79\% accuracy when step is 2, which are already better than the previous state of the art results. Our implemented ResNet-20 network is not as good as the VGG-9 network, but still, on CIFAR10, the accuracy of MT-ResNet-20 can achieve 94.71\% accuracy at $time\_step=4$ and exceed the previous state of the art at $time\_step=3$.

\begin{table*}[ht]
    \centering
    \begin{tabular}{ccccc}
        \hline
        Dataset & Name & Model & Steps & Accuracy(\%) \\
        \hline
        \multirow{17}{*}{CIFAR10 / CIFAR100} & \multirow{3}{*}{STBP-tdBN\cite{HanleZheng2020GoingDW}} & \multirow{3}{*}{ResNet-19} & 2 & 92.34 / - \\
        & & & 4 & 92.92 / - \\
        & & & 6 & 93.16 / -\\
        \cline{2-5}
        & PLIF\cite{WeiFang2020IncorporatingLM} & VGG-8 & 8 & 93.50 / - \\
        \cline{2-5}
        & \multirow{2}{*}{InfLoR-SNN\cite{guo2022reducing}} & ResNet-20 & 5 & 93.01 / 71.19 \\
        & & VGG-16 & 5 & 94.06 / 71.56 \\
        \cline{2-5}
        & \multirow{2}{*}{Real Spike\cite{guo2022real}} & ResNet-20 & 5 & 93.01 / 66.60 \\
        & & VGG-16 & 5 & 92.90 / 70.62 \\
        \cline{2-5}
        & \multirow{3}{*}{DSpike\cite{YuhangLi2021DifferentiableSR}} & \multirow{3}{*}{ResNet-18} & 2 & 93.13 / 71.68 \\
        & & & 4 & 93.66 / 73.35 \\
        & & & 6 & \textbf{94.25} / \textbf{74.24} \\
        \cline{2-5}
        & \multirow{5}{*}{Ours} & \multirow{5}{*}{MT-VGG-9} & 1 & 93.47 / 73.67 \\
        & & & 2 & 94.71 / 74.79 \\
        & & & 3 & 94.54 / 75.13 \\
        & & & 4 & 94.51 / \textbf{75.53} \\
        & & & 5 & \textbf{94.74} / 75.29 \\
        \cline{2-5}
        & \multirow{5}{*}{Ours} & \multirow{5}{*}{MT-ResNet-20} & 1 & 93.50 / 70.74 \\
        & & & 2 & 93.84 / 72.05 \\
        & & & 3 & 94.55 / 73.23 \\
        & & & 4 & \textbf{94.71} / 73.75 \\
        & & & 5 & 94.44 / 73.45 \\
        \hline
        \multirow{5}{*}{DVS-CIFAR10} & STDP-tdBN\cite{HanleZheng2020GoingDW} & ResNet-17/19 & 10 & 67.80 \\
        & PLIF\cite{WeiFang2020IncorporatingLM} & Conv+FC & 20 & 74.80 \\
        & SEW-ResNet\cite{WeiFang2021DeepRL} & SEW-ResNet & 16 & 74.40 \\
        & DSpike\cite{YuhangLi2021DifferentiableSR} & ResNet-18 & 10 & 75.40 \\
        & MS-ResNet20\cite{hu2021advancing} & ResNet-20 & - & 75.56 \\
        \cline{2-5}
        & \multirow{2}{*}{Ours} & ST-VGG-12 & 5 & 75.90 \\
        & & MT-VGG-12 & 5 & \textbf{76.30} \\
        \hline
    \end{tabular}
    \caption{Performance comparison of MT with SoTA methods.}
    \label{tab:cifar10_cifar100_dvscifar10_results}
\end{table*}

All the results on CIFAR10/CIFAR100 are from models trained by taking membrane as the output (See (A) of Sec \ref{sec:outputs}), as we found that SNNs taking membrane as output is superior to models taking spikes (See (B) of Sec \ref{sec:outputs}) as output on CIFAR10. And specifically for CIFAR100, it's difficult for model to converge if taking spikes and voting layer as output makes the model.



\subsection{Neuromorphic datasets classification}

VGG-12 in Table \ref{tab:vgg_models} is adopted on DVS-Cifar10 dataset to verify the effectiveness of MT. The DVS-CIFAR10 dataset is processed in the same way as \cite{WeiFang2020IncorporatingLM}, namely converting the events to frames by accumulating events into T slices evenly, thus different frame datasets are obtained for different step T. No data augmentation approaches are involved in the experiments.

From Fig \ref{fig:vgg12_dvscifar10_results}, it's observed that the accuracy is improved significantly at early stage especially when step is less than 4. The comparison of our proposed model with previous similar models are listed in Table \ref{tab:cifar10_cifar100_dvscifar10_results}, in which our final model accuracy is 76.3\%, which is superior to the previous models.

However, when the number of steps is large, the gap between ST and MT is small, also, in the experiments on DVS-CIFAR10, we found that the accuracy shakes significantly even with same configuration. We argue that this is mainly caused by the fewer examples in the dataset. As currently large-scale DVS dataset is difficult to be acquired, we leave more explorations on DVS datasets as future work.

\begin{figure}[ht]
    \centering
    \includegraphics[width=7cm, height=4.3cm]{figures/st_mt_dvscifar10.png}
    \caption{Performance comparison of ST and MT on DVS-CIFAR10 with various steps.}
    \label{fig:vgg12_dvscifar10_results}
\end{figure}

\subsection{Ablation Study}

Extensive ablation studies are conducted to evaluate different MT settings. Firstly we study which part of the model needs MT, in (a)(b) of Fig \ref{fig:ablation_exp}, we apply MT on convolution layers only and both convolution and fully-connected layers, and evaluate the results on different steps. From (a), it's observed that applying MT on fully-connected layers improves accuracy at early steps. However, from (b), applying MT on fully-connected layers doesn't make significant changes on accuracy. We argue the deeper reason is that the fully-connected layers occupies 92\% parameters of VGG-8 while 42.7\% of VGG-9.

Secondly different deltas are explored on VGG-9 with $step=1$, from (c) of Fig \ref{fig:ablation_exp}, it's observed that using negative deltas only performs more significant effect over positive deltas only, while combination of both positive and negative delta performs the best. And the accuracy curve is smoother when absolute delta is larger than 0.3, indicating no more information will be recovered when delta is too large.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=16cm, height=4.3cm]{figures/ablation_exp.png}
    \caption{Ablation study of MT settings of VGG-9 on CIFAR10 at step 1.}
    \label{fig:ablation_exp}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%