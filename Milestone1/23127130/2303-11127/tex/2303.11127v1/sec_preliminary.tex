%%%%%%%%%%%%%%%%%%%%%%% Preliminary %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminary}

\subsection{Parametric Leaky Integrate and Fire Model}

Parametric Leaky Integrate and Fire (PLIF) neuron model\cite{WeiFang2020IncorporatingLM} is adopted as the basic unit in the model architectures in this paper. PLIF includes the following discrete time equations:

\begin{equation}
\label{eqn:neuron_dynamics}
    H[t] = f(V[t-1], X[t]),
\end{equation}
\begin{equation}
\label{eqn:neuron_spike}
    S[t] = \Theta(H[t] - V_{th}),
\end{equation}
\begin{equation}
\label{eqn:neuron_update}
    V[t] = H[t](1 - S[t]) + V_{reset}S[t]
\end{equation}

Where X[t] is the input current at time-step t from the previous layer, H[t] and V[t] represent the membrane potential after neuronal dynamics and spike trigger respectively. $V_{th}$ is the firing threshold. $\Theta(x)$ is the Heaviside step function which is defined by

\begin{equation}
    \Theta(x) = \left\{
    \begin{array}{ll}
      1 & x \geq 0 \\
      0 & x < 0 \\
    \end{array}
    \right.
    \
\end{equation}

$V_{reset}$ denotes the reset potential. The function in Eq \ref{eqn:neuron_dynamics} describes the neuronal dynamics, which can represent different model with different forms. Eq \ref{eqn:if} and Eq \ref{eqn:lif} describe the Integrate-and-Fire(IF) and Leaky Integrate-and-Fire(LIF) respectively.

\begin{equation}
\label{eqn:if}
    H[t] = V[t-1] + X[t]
\end{equation}
\begin{equation}
\label{eqn:lif}
    H[t] = V[t-1] + \frac{1}{\tau}(X[t] - (V[t-1] - V_{reset}))
\end{equation}

In Eq \ref{eqn:lif}, the $\tau$ is the membrane time constant, \cite{WeiFang2020IncorporatingLM} made it learnable with Eq \ref{eqn:learnable_plif}.

\begin{equation}
\label{eqn:learnable_plif}
    \tau = 1 + exp(-a) \in (1, +\infty)
\end{equation}

Where $a$ is a learnable variable.

Furthermore, when $V_{reset} = 0$, the PLIF neuron becomes

\begin{equation}
\label{eqn:plif_lstm}
    H[t] = (1 - \frac{1}{\tau})V_{t-1} + \frac{1}{\tau}X_t
\end{equation}

which is analogous to the input gate and forget gate in Long Short-Term Memory(LSTM) networks \cite{SeppHochreiter1997LongSM}. In this paper, We set $V_{reset}=0$ and implement the PLIF model with formula in Eq \ref{eqn:plif_lstm}.

\subsection{Surrogate Function}

The surrogate function used in this paper is the popular rectangular function proposed by \cite{YujieWu2019DirectTF}, given by Eq below.

\begin{equation}
    \frac{\partial S[t]}{\partial H[t]} = \frac{1}{a}sign(|H[t] - V_{th}| \leq \frac{a}{2})
\end{equation}

Where a is a hyper-parameter and usually set to 1.

\subsection{Outputs}
\label{sec:outputs}

Based on whether to fire at the last layer, there are naturally two kinds of outputs:

\begin{enumerate}[label=(\Alph*)]
    \item Output the membrane of the last layer directly without firing such that the loss can be computed on float values as the ANNs, which is widely used by \cite{YuhangLi2021DifferentiableSR}\cite{HanleZheng2020GoingDW}\cite{rathi2020diet} etc.
    \item Output the spikes across time steps. This method inherently loss some precision especially with fewer time steps. A voting layer is proposed by \cite{WeiFang2020IncorporatingLM} to alleviate the problem.
\end{enumerate}

The loss functions are computed on the mean output across time steps of the model, classic classification losses such as Softmax or Mean Squared Error(MSE) can all be applied to the training of SNNs.

In practices, we found that the two outputs perform quite different on various datasets with various number of classes, which is illustrated in the experiment section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%