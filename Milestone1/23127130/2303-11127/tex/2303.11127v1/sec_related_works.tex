%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Related works %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}

\subsection{Learning Algorithms}

After the emergence of SNNs, how to train the models effectively stays as an active area. The earliest learning algorithms are based on biological plausible local learning rules, like Hebbian learning \cite{hebb2005organization} and Spike-Timing-Dependent Plasticity \cite{bi1998synaptic}.
However, such methods are only suitable for shallow SNNs, and the performance is far below methods mentioned below.

ANN-To-SNN methods are capable of obtaining high-quality SNNs by leveraging the knowledge from well-trained ANNs and converting the ReLU activations to spike activations based on rate coding\cite{sengupta2019going}\cite{deng2021optimal}. However, achieving near lossless conversion requires a considerable amount of time steps ($>200$) to accumulate the spikes, which may significantly increase the latency.

Backpropagation with surrogate gradient is widely used in recent researches to achieve high-quality deep models \cite{lee2016training}\cite{wu2018spatio}\cite{lee2020enabling}. The utilization of surrogate gradient function enables end-to-end backpropagation in SNN training, thus training knowledge of ANNs can be transferred to SNNs. \cite{zenke2021remarkable}\cite{neftci2019surrogate} systematically studied the remarkable robustness of the surrogate gradient based algorithms and demonstrated that SNNs trained with surrogate gradient can achieve competitive performance with ANNs. Backpropagation with surrogate functions requires much fewer steps compared to ANN-To-SNN methods as it's not dependent of rate coding \cite{YujieWu2019DirectTF}.

\subsection{Model architecture}

Backpropagation with surrogate gradient unlocked the model architecture migration from ANNs to SNNs. \cite{YujieWu2019DirectTF} initially applied direct training of various sized deep convolutional neural networks on both static and neuromorphic datasets with normalization and rate coding. \cite{HanleZheng2020GoingDW} explored ResNet with time-dependent batch normalization and validated the architecture on ImageNet for the first time. \cite{WeiFang2021DeepRL}\cite{hu2021advancing} proposed spike-element-wise ResNet and membrane based ResNet respectively which pushed the depth of the architecture to more than 100 layers. \cite{yao2022attention} introduced attention mechanisms into SNNs and achieved comparable and even better performance compared to the ANN counterpart on large-scale datasets.
Furthermore, \cite{zhou2022spikformer} proposed Spiking Transformer by combining self-attention with spiking neural network. There will be more works on such model migrations in the foreseeable future.

\subsection{Information Loss}

Information loss in SNNs attracts increasing attentions recently. In \cite{guo2022reducing}, Soft Reset is leveraged to reserve the information in the residual potential, and Membrane Potential Rectifier is proposed to alleviate the precision loss in quantization. \cite{guo2022real} leveraged a re-parameterization technique to decouple training and inference stage, and enhance the presentation capacity b learning real-valued spikes during training and transferring the capacity to model parameters by not sharing convolution kernels. The MT approach in this paper falls into the same domain with \cite{guo2022reducing}\cite{guo2022real}, which provides another option to reduce information loss in spike activations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%