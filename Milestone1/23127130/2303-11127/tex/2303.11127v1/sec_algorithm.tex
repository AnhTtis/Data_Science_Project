%%%%%%%%%%%%%%%%%%%%%%%%%%% Algorithm %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Multi-Thresholds SNN}

It's commonly observed that the quality of SNNs can be boosted by increasing time steps. Fig \ref{fig:st_mt_cifar10_cifar100} below demonstrated this phenomenon. In Fig \ref{fig:st_mt_cifar10_cifar100}, the yellow line is the corresponding ANN with the same architecture of SNNs, and the red line is the SNNs with ST. It's observed that SNNs with ST require at least 2 steps to recover the same accuracy of its ANN counterpart. 

We argue that compared to the ANNs, the precision of middle outputs of SNNs is lost in replacing float values with spikes and compensated on multiple time steps.  

To overcome the precision loss problem in the intermediate activations in SNNs, we propose a Multi-Threshold (MT) approach to partly preserve the precision, the idea is illustrated in Fig \ref{fig:single_thershold} and Fig \ref{fig:multiple_thresholds}. In Fig \ref{fig:single_thershold}, one threshold in red line split the input membrane into two values 0 and 1, in this process, the information about how much the membrane is greater / smaller than the threshold is completely lost. In Fig \ref{fig:multiple_thresholds}, two auxiliary thresholds are added, one is smaller in blue line and the other one is larger in yellow line. Two more series of spikes are obtained based on the auxiliary thresholds, the result that adds up the three sequences of spikes is a more accurate representation of the original membrane.


To formulate the approach, a series of parameters are introduced in, namely $[\Delta_1, \Delta_2, ..., \Delta_n]$, and we apply each $\Delta$ to Eq \ref{eqn:neuron_spike}:

\begin{equation}
\label{eqn:neuron_multiple_spikes}
    S[t]_{\Delta_i} = \Theta(H[t] - V_{th} - \Delta_i), i \in [1, 2, ..., n]
\end{equation}

And Eq \ref{eqn:neuron_update} is still used to update the membrane.

After obtaining all spikes for each threshold, the final outputs are obtained by

\begin{equation}
\label{eqn:neuron_multiple_sum_spikes}
    S[t]_{sum} = S[t] + \Sigma{S[t]_{\Delta_i}}
\end{equation}


However, after applying multiple thresholds, from Eq \ref{eqn:neuron_multiple_sum_spikes}, the outputs have other possible values other than 0/1, which breaks the binary property of SNNs, making the network not multiplication-free anymore. To keep the multiplication-free property, convolution layer of next layer can be involved in to perform an equivalent operation in Eq \ref{eqn:neuron_sum}, right side of which is still multiplication-free.

\begin{equation}
\label{eqn:neuron_sum}
    Conv(S[t]_{sum}) = Conv(S[t]) + \Sigma{Conv(S[t]_{\Delta_i})}
\end{equation}

Generally, we can deploy left side of Eq \ref{eqn:neuron_sum} on current popular hardwares like TPU, GPU etc and right side on neuromorphic hardwares with multiplication-free properties.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%