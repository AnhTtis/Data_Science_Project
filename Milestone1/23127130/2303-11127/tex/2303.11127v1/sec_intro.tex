%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Intro %%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

The development of artificial neural networks (ANNs) especially deep learning in recent years brought remarkable successes in lots of domains like computer vision \cite{he2016deep}\cite{tolstikhin2021mlp}, natural language processing \cite{brown2020language}\cite{devlin2018bert}\cite{vaswani2017attention}, multi-modal\cite{lu2019vilbert}\cite{radford2021learning} etc. However, computational costs of ANNs are too high for even one single application. For example, a standard convolution neural network to classify ImageNet-1k dataset will expend about 250 watts\cite{roy2019towards}, while by contrast, only 20 watts are required by human brain to process various tasks simultaneously\cite{laughlin2003communication}.

Spiking Neural Networks (SNNs) are considered as the next generation neural models for its closer mechanism to the biological neurons in the brain\cite{maass1997networks}. Recently SNNs attract increasing attention due to their abilities such as temporal information processing, energy efficiency \cite{roy2019towards} and high biological plausibility \cite{gerstner2014neuronal}. By leveraging binary spike signals, SNNs can achieve low-energy consumption by multiplication-free inference and avoiding computing zero values of inputs or activation \cite{roy2019towards}. Neuromorphic hadwares such as TrueNorth\cite{akopyan2015truenorth}, Lohi\cite{davies2018loihi} and Tianjic\cite{pei2019towards} demonstrated that SNNs is capable of saving energy by orders of magnitude.

However, even though with these promising inherent properties, SNNs are not widely adopted by the main-stream machine learning problems. We argue three obvious obstacles should account for this. Firstly, quality degradation caused by information loss is unavoidable by replacing the float activations with binary spikes. Previously the problem is severe due to lacking efficient learning algorithms, surrogate gradient\cite{neftci2019surrogate} fills the gap to a large extend. But it still exists when limiting time steps. Secondly, many popular problems are not with temporal information, pre-processing is necessary to make them solvable by SNNs. However, it's inefficient if a problem is converted from single step to multiple steps. For example, in static image classification tasks, the static image is duplicated into multiple copies to adapt the SNNs, multiple steps indeed boost the quality while reducing the efficiency of training and inference. Thirdly, SNNs are constraint by both software and hardware, modern popular ML frameworks such as Pytorch \cite{paszke2019pytorch}, Jax \cite{bradbury2018jax} and Tensorflow \cite{abadi2016tensorflow} don't provide efficient and general instructions or high-level functions to accelerate the convolution with 0/1 spike activations \cite{YuhangLi2021DifferentiableSR}. And current popular devices like TPU, GPU are not specifically optimized for multiplication free operations. 

In this work, we propose a multiple threshold (MT) algorithm at Leak-Integrate-Fire(LIF) cell to partly recover the information loss brought by replacing floating-point activations with 0/1 spikes. The difference between one single threshold (ST) and MT can be seen in Fig \ref{fig:single_thershold} and Fig \ref{fig:multiple_thresholds}. And we find the algorithm is effective in enhancing the quality of SNNs by both achieving better accuracy at same time steps and reaching similar accuracy with fewer time steps. The latter one reduces the burden of training SNNs on current hardwares. Furthermore, the trained model can still be deployed on neuromorphic hardwares without violating the nature of spikes.

\begin{figure}
    \centering
    \includegraphics[width=6cm]{figures/snn-st.pdf}
    \caption{Spikes over Single Threshold (ST), $\mu$ is membrane and $o$ is output spike.}
    \label{fig:single_thershold}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=15cm]{figures/snn-mt.pdf}
    \caption{Spikes over Multiple Thresholds (MT), $\mu$ is membrane and $o$ is output spike.}
    \label{fig:multiple_thresholds}
\end{figure*}

Our contributions can be summarized as follows:

\begin{itemize}
    \item We propose the multiple threshold (MT) algorithm as an augmentation to partly reserve precision in spike activations. MT can be viewed as a widget used in the fire stage of LIF cells such that can be deployed in SNNs of various architectures. we demonstrate MT is a robust way to improve quality of SNNs.
    \item We evaluate the MT algorithm on Parametric Leak Integrate Fire (PLIF) based VGG and ResNet on CIFAR-10, CIFAR-100 and DVS-CIFIAR10 datasets, Results show that MT is highly effective and versatile. Our implemented SNNs with MT can exceed state-of-the-art accuracy of similar architectures, using fewer steps.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%