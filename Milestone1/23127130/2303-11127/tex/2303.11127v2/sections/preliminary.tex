%%%%%%%%%%%%%%%%%%%%%%% Preliminary %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminary}

\subsection{Parametric Leaky Integrate and Fire Model}

Parametric Leaky Integrate and Fire (PLIF) neuron model \cite{WeiFang2020IncorporatingLM} is adopted as the basic unit in the model architectures in this paper. PLIF includes the following discrete time equations:

\begin{equation}
\label{eqn:neuron_dynamics}
    H[t] = f(V[t-1], X[t]),
\end{equation}
\begin{equation}
\label{eqn:neuron_spike}
    S[t] = \Theta(H[t] - V_{th}),
\end{equation}
\begin{equation}
\label{eqn:neuron_update}
    V[t] = H[t](1 - S[t]) + V_{reset}S[t]
\end{equation}

Where $X[t]$ is the input current at time step t from the previous layer, $H[t]$ and $V[t]$ represent the membrane potential after neuronal dynamics and spike triggering respectively. $V_{th}$ is the firing threshold. $\Theta(x)$ is the Heaviside step function.

% save pages
% $\Theta(x)$ is the Heaviside step function which is defined by


% \begin{equation}
%     \Theta(x) = \left\{
%     \begin{array}{ll}
%       1 & x \geq 0 \\
%       0 & x < 0 \\
%     \end{array}
%     \right.
%     \
% \end{equation}

$V_{reset}$ denotes the reset potential. The function in \cref{eqn:neuron_dynamics} describes the neuronal dynamics, which can represent different model with different forms. \cref{eqn:lif} describes the dynamics of the Leaky Integrate-and-Fire(LIF).

% \cref{eqn:if} and \cref{eqn:lif} describe the Integrate-and-Fire(IF) and Leaky Integrate-and-Fire(LIF) respectively.

% \begin{equation}
% \label{eqn:if}
%     H[t] = V[t-1] + X[t]
% \end{equation}

\begin{equation}
\label{eqn:lif}
    H[t] = V[t-1] + \frac{1}{\tau}(X[t] - (V[t-1] - V_{reset}))
\end{equation}

In \cref{eqn:lif}, the $\tau$ is the membrane time constant, which is made learnable with \cref{eqn:learnable_plif} \cite{WeiFang2020IncorporatingLM}.

\begin{equation}
\label{eqn:learnable_plif}
    \tau = 1 + exp(-a) \in (1, +\infty)
\end{equation}

Where $a$ is a learnable variable.

Furthermore, when $V_{reset} = 0$, the PLIF neuron becomes \cref{eqn:plif_lstm}.

\begin{equation}
\label{eqn:plif_lstm}
    H[t] = (1 - \frac{1}{\tau})V_{t-1} + \frac{1}{\tau}X_t
\end{equation}

which is analogous to the input gate and forget gate in Long Short-Term Memory(LSTM) networks \cite{SeppHochreiter1997LongSM}. In this paper, We set $V_{reset}=0$ and implement the PLIF model with formula in \cref{eqn:plif_lstm}.

\subsection{Surrogate Function}

The surrogate function used in this paper is the popular rectangular function proposed by Yujie Wu et al. \cite{YujieWu2019DirectTF}, given by \cref{eqn:surrogate_func} below.

\begin{equation}
\label{eqn:surrogate_func}
    \frac{\partial S[t]}{\partial H[t]} = \frac{1}{a}sign(|H[t] - V_{th}| \leq \frac{a}{2})
\end{equation}

Where a is a hyper-parameter and usually set to 1.

% save pages
% \subsection{Outputs}
% \label{sec:outputs}

% Based on whether to fire at the last layer, there are naturally two kinds of outputs:

% \begin{enumerate}[label=(\Alph*)]
%     \item Output the membrane of the last layer directly without firing such that the loss can be computed on float values as the ANNs, which is widely used by \cite{YuhangLi2021DifferentiableSR}\cite{HanleZheng2020GoingDW}\cite{rathi2020diet} etc.
%     \item Output the spikes across time steps. This method inherently loss some precision especially with fewer time steps. A voting layer is proposed by \cite{WeiFang2020IncorporatingLM} to alleviate the problem.
% \end{enumerate}

% The loss functions are computed on the mean output across time steps of the model, classic classification losses such as Softmax or Mean Squared Error(MSE) can all be applied to the training of SNNs.

% In practices, (A) and Softmax are leveraged for easily comparing with results from other paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%