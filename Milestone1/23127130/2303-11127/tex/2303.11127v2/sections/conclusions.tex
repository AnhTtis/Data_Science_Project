%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussions}

In this work, we propose Multiple Threshold (MT) approaches to mitigate the precision loss inherent in SNNs compared to floating-point neural networks. We demonstrate that MT-SNNs outperform state-of-the-art models with similar architectures, achieving higher accuracy with fewer time steps and comparable energy consumption, and raising the overall performance ceiling for SNNs. Crucially, the MT approach maintains compatibility with neuromorphic hardware by preserving the multiplication-free property of spiking neurons.



% In this work, We propose Multiple Threshold (MT) approaches to augment the output of spiking layers, thus relieving the precision loss of SNNs comparing to float-based neural networks. We demonstrate that MT-SNNs outperform SOTA models with similar architectures, achieve higher accuracy at fewer steps or comparable energy consumption, raise the performance ceiling for SNNs. Furthermore, the MT approach remains compatible with the neuromorphic hardware by preserving the multiplication-free property.


% In this work, We propose Multiple Threshold approaches are proposed to augment the output of spiking layers, thus relieving the precision loss of SNNs comparing to float-based neural networks. We show that SNNs with multiple thresholds outperform state-of-the-art models with similar architectures, and tend to reach higher accuracy at fewer steps or similar energy cost, providing us an option to make easier trade off between the number of steps and the quality.

However, our work also reveals some limitations and potential avenues for future research:
% However, there are limitations to be resolved in the future:

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
    \item The performance gains of MT on the CIFAR10-DVS dataset diminish at later time steps, potentially due to the limited size of this dataset. Exploring MT on larger-scale DVS datasets is an important direction for future work, although such datasets are currently scarce.
    \item The analogy between the CASCADE MT mode and activation quantization in ANNs suggests a potential bridge between these two types of neural networks. Investigating the conversion of pre-trained ANNs into CASCADE MT SNNs represents a compelling research direction, which could leverage the vast body of knowledge and pre-trained models available for ANNs to accelerate SNN development.
\end{itemize}

% \begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
%     \item The MT approaches don't yield significant improvements on DVS-CIFAR10 at late steps, which is potentially due to its smaller size. Future work is left to explore on large DVS dataests.
%     \item The analogy between CASCADE\_MT mode and activation quantization in ANNs suggests that it could be the bridge between ANNs and SNNs. The conversion of pre-trained ANNs to a CASCADE\_MT SNNs is a compelling future research direction.
% \end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%