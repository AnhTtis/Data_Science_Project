%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Intro %%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Recent advances in Artificial Neural Networks (ANNs), particularly deep learning, have revolutionized fields such as computer vision\cite{he2016deep, tolstikhin2021mlp}, natural language processing\cite{brown2020language, devlin2018bert, vaswani2017attention}, and multi-modal learning\cite{lu2019vilbert,radford2021learning}. However, ANNs often demand substantial computational resources. A standard convolutional neural network for ImageNet-1k classification, for instance, consumes approximately 250 watts\cite{roy2019towards}, starkly contrasting with the 20 watts utilized by the human brain to simultaneously process diverse tasks\cite{laughlin2003communication}.

% The development of artificial neural networks (ANNs) especially deep learning in recent years brought remarkable successes in lots of domains like computer vision \cite{he2016deep, tolstikhin2021mlp}, natural language processing \cite{brown2020language, devlin2018bert, vaswani2017attention}, multi-modal \cite{lu2019vilbert,radford2021learning} etc. However, computational costs of ANNs are too high for even one single application. For example, a standard convolution neural network to classify ImageNet-1k dataset expends about 250 watts\cite{roy2019towards}, while by contrast, only 20 watts are required by human brain to process various tasks simultaneously \cite{laughlin2003communication}.

Spiking Neural Networks (SNNs), inspired by the brain's biological neurons\cite{maass1997networks}, are emerging as a next-generation neural model. SNNs have garnered increasing interest due to their unique capabilities, including temporal information processing, energy efficiency\cite{roy2019towards}, and high biological plausibility\cite{gerstner2014neuronal}. By leveraging binary spike signals, SNNs achieve low energy consumption through multiplication-free inference and by bypassing computations on zero-valued inputs or activations\cite{roy2019towards}. Neuromorphic hardware like TrueNorth \cite{akopyan2015truenorth}, Lohi \cite{davies2018loihi} and Tianjic \cite{pei2019towards} have demonstrated the potential of SNNs to achieve energy savings of orders of magnitude.


% Spiking Neural Networks (SNNs) are considered as the next generation neural models for its closer mechanism to the biological neurons in the brain \cite{maass1997networks}. Recently SNNs attract increasing attention due to their abilities such as temporal information processing, energy efficiency \cite{roy2019towards} and high biological plausibility \cite{gerstner2014neuronal}. By leveraging binary spike signals, SNNs can achieve low-energy consumption by multiplication-free inference and avoiding computing zero values of inputs or activation \cite{roy2019towards}. Neuromorphic hadwares such as TrueNorth \cite{akopyan2015truenorth}, Lohi \cite{davies2018loihi} and Tianjic \cite{pei2019towards} demonstrate that SNNs are capable of saving energy by orders of magnitude.

\begin{figure*}
    \centering
    % \includegraphics[width=17cm]{figures/snn_st_mt.pdf}
    \includegraphics[width=12cm, height=6.4cm]{figures/snn_st_mt_v6.pdf}
    \caption{Spikes with single threshold and multiple threshold, $\mu$ is membrane and $S$ is output spike, $th$ is short for threshold.
    (a): Single Threshold (ST): Spikes are fired only if the membrane exceeds $\mu_{th}$.
    (b): PARALLEL MT: Three independent thresholds are applied, thus three spike sequences are generated and merged.
    (c): CASCADE MT: Three thresholds are applied and will be checked sequentially from highest to lowest, the fired spikes are weighted before aggregation.
    }
    \label{fig:st_mt}
\end{figure*}


Despite their inherent energy efficiency and biological plausibility, SNNs face several challenges that impede their broader adoption in mainstream machine learning.  A primary obstacle is the inevitable information loss that occurs when converting floating-point activations into binary spikes. Although increasing the number of time steps can partially alleviate this, performance remains limited when time steps are constrained. Additionally, many popular tasks lack intrinsic temporal structure, necessitating inefficient preprocessing to adapt them for SNNs. For instance, in static image classification task, images need to be encoded with a time dimension, requiring multiple forward passes, while ANNs can process them with only one pass. Finally, SNNs are hindered by both software and hardware limitations. Popular machine learning frameworks such as Pytorch \cite{paszke2019pytorch}, Jax \cite{bradbury2018jax} and Tensorflow \cite{abadi2016tensorflow} lack efficient, generalized support for spike-based computations. Moreover, standard hardware like GPUs and TPUs, optimized for floating-point arithmetic, are not ideal for SNNs' multiplication-free operations.


% Despite their inherent advantages, SNNs face challenges that hinder their widespread adoption in mainstream machine learning problems. We identify three primary obstacles below. Firstly, the conversion of floating-point activation to binary spikes inevitably leads to information loss. While increasing time steps could mitigate this issue, performance limitations remain when time steps are constrained. Secondly, many popular problems lack inherent temporal structure, requiring inefficient preprocessing to adapt them for SNNs. For example, replicating static images across multiple time steps for classification tasks boosts accuracy but sacrifices efficiency. Thirdly, SNNs are constraint by both softwares and hardwares. Popular ML frameworks like Pytorch \cite{paszke2019pytorch}, Jax \cite{bradbury2018jax} and Tensorflow \cite{abadi2016tensorflow} lack efficient, generalized support for spikes, and standard hardwares like GPUs and TPUs are not optimized for multiplication-free operations, limiting SNN performance.


% However, even though with these promising inherent properties, SNNs are not widely adopted by the main-stream machine learning problems. We argue three obvious obstacles should account for this. Firstly, quality degradation caused by information loss is unavoidable by replacing the float activations with binary spikes. Previously the problem is severe due to lacking efficient learning algorithms, surrogate gradient proposed by \citet{neftci2019surrogate} fills the gap to a large extend. But it still exists when limiting time steps. Secondly, many popular problems are not with temporal information, pre-processing is necessary to make them solvable by SNNs. However, it's inefficient if a problem is converted from single step to multiple steps. For example, in static image classification tasks, the static image is duplicated into multiple copies to adapt the SNNs, multiple steps indeed boost the quality but reduce the efficiency of training and inference. Thirdly, SNNs are constraint by both software and hardware, modern popular ML frameworks such as Pytorch \cite{paszke2019pytorch}, Jax \cite{bradbury2018jax} and Tensorflow \cite{abadi2016tensorflow} don't provide efficient and general instructions or high-level functions to accelerate the convolution with 0/1 spike activations \cite{YuhangLi2021DifferentiableSR}. And current popular devices like TPU, GPU are not specifically optimized for multiplication free operations. 


In this work, we introduce the Multiple Threshold (MT) approach to address the precision loss inherent in SNNs. MT also mitigates the limitations imposed by the temporal structure of data, enabling SNNs to achieve high accuracy with fewer time steps. We propose two distinct MT modes, depending on the membrane update rule:

% In this work, we propose the Multiple Threshold (MT) approach to address the precision loss problem. MT also mitigates the temporal structure limitation, enabling SNNs to achieve high accuracy with fewer time steps. Depending on the membrane update rule, two MT modes are proposed:


\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
    \item PARALLEL MT: Multiple thresholds are applied independently to determine spike firing. with the results aggregated across thresholds to enhance precision. See \cref{fig:st_mt}-(a, b) for a visual comparison with single threshold SNNs.
    \item CASCADE MT: A series of proportionally decreasing thresholds are employed to sequentially trigger spike firing, followed by a weighted sum of the spiking results. See \cref{fig:st_mt}-(a, c) for a visual comparison with single threshold SNNs.
\end{itemize}


% \begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
%     \item PARALLEL\_MT: Multiple thresholds are independently leveraged to determine whether to fire spikes. Spiking results across thresholds are then aggregated for enhanced precision.
%     The distinction between single threshold and PARALLEL\_MT is visualized in Fig \ref{fig:st_mt}-(a, b).
%     \item CASCADE\_MT: A series of proportionally decreasing thresholds are utilized to sequentially trigger spike firing. The spiking results are then combined using a weighted sum. See Fig \ref{fig:st_mt}-(a, c) for comparing the single threshold and CASCADE\_MT.
% \end{itemize}

% \begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
%     \item PARALLEL\_MT: Multiple thresholds are independently leveraged to determin whether to fire spikes. Spiking results across thresholds are then aggregated for enhanced precision.
%     In implementation, spiking of various thresholds will go through the same weights of next layer first and then be accumulated to avoid violating the multiplication-free property. Difference between single threshold and PARALLEL\_MT could be seen in Fig \ref{fig:st_mt}-(a, b).
%     \item CASCADE\_MT: A series of thresholds which decreases proportionally are leveraged to decide firing of spikes sequentially. The spiking results will be weighted summed together. In implementation, the weights of next layer will be duplicated by multiplying the proportional ratio, and spikes will go through the weights and then be accumulated to avoid violating the multiplication-free property. Difference between single threshold and CASCADE\_MT could be seen in Fig \ref{fig:st_mt}-(a, c).
% \end{itemize}

The MT algorithm demonstrably enhances SNN performance, achieving higher accuracy with reduced time steps and comparable energy consumption. This substantially eases the computational burden of SNN training on existing hardware. Furthermore, by strategically adding branches in the layer subsequent to the LIF neuron, the trained model remains compatible with neuromorphic hardwares, fully retaining the inherent advantages of spike-based computation.

% The MT algorithm demonstrably improves performance of the SNNs, achieving higher accuracy with reduced time steps or energy consumption. This significantly eases the computational burden of SNN training on existing hardware. Moreover, by strategically adding branches in the subsequent layer following LIF, the trained model maintains the compatible with neuromorphic hardware, fully preserving the inherent advantage of spiking-based computation.

% We find the algorithm is effective in enhancing the quality of SNNs by both achieving better accuracy at same time steps and reaching similar accuracy with fewer time steps or similar energy consumption. The latter one reduces the burden of training SNNs on current hardwares. Furthermore, the trained model can still be deployed on neuromorphic hardwares without violating the nature of spikes.

Our contributions can be summarized as follows:

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
    \item We introduce the MT algorithm, a versatile, modular approach that strategically utilizes multiple thresholds in either parallel or cascade configurations to enhance the precision of SNNs across diverse architectures.
    \item We demonstrate the effectiveness and robustness of MT across various SNN architectures (VGG, ResNet) and datasets (CIFAR-10, CIFAR-100, ImageNet, DVS-CIFAR10), showcasing its broad applicability.
    \item Our MT-SNNs significantly elevate the performance ceiling of SNNs, achieving results that surpass those of single-threshold models, even with substantial increases in time steps. Furthermore, MT-SNNs outperform the majority of existing approaches, match current state-of-the-art (SOTA) results, and even surpass SOTA performance with much fewer steps and energy.
\end{itemize}

% \todo{emphasize the energy and quality on ImageNet}


% \begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
%     \item We introduce the MT algorithm that strategically employs multiple thresholds in the parallel or cascade manner to enhance the precision in SNNs. The algorithm is a modular component within LIF cells, enabling its deployment across diverse SNN architectures. architectures. 
%     \item We demonstrate the effectiveness of MT approaches in improving SNN quality across architectures (VGG, ResNet) and datasets (CIFAR-10, CIFAR-100, DVS-CIFAR10), indicating its robustness and wide applicability.
%     \item Our MT-SNN significantly raises the performance ceiling for SNNs, achieving results that are unattainable by single-threshold models even with substantial increases in time steps. Additionally, it outperforms the majority of prior work, achieves results on par with the current state-of-the-art (SOTA), and even surpasses SOTA in early-stage performance.
% \end{itemize}

% \begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
%     \item We propose the CASCADE\_MT and PARALLEL\_MT algorithms to partly reserve precision in spike activations. MT can be viewed as a widget used in the fire stage of LIF cells such that can be deployed in SNNs of various architectures. we demonstrate MT is a robust way to improve quality of SNNs.
%     \item We evaluate the MT algorithm on Parametric Leak Integrate Fire (PLIF) based VGG and ResNet on CIFAR-10, CIFAR-100 and DVS-CIFIAR10 datasets, Results show that MT is highly effective and versatile. Our implemented SNNs with MT can outperform state-of-the-art accuracy of similar architectures, with fewer steps.
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%