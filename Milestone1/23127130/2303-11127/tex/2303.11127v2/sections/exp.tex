%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Experiments %%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{Experiments}

We evaluate the proposed MT methods on static datasets CIFAR10\cite{krizhevsky2009learning}, CIFAR100\cite{krizhevsky2009learning}, ImageNet\cite{krizhevsky2012imagenet} and one neuromorphic dataset CIFAR10-DVS\cite{li2017cifar10}. Various model architectures such as VGG, ResNet-19/34 are implemented to demonstrate the versatility of the methods. All code is implemented with Tensorflow, and experiments are conducted on TPU.

\subsection{Experiment Settings}

\subsubsection{Network Architectures}

\begin{table}[tb]
    \caption{Model Configurations}
    \label{tab:model_configs}
    \centering
    \begin{tabular}{ccccccccc}
        \toprule
        Model & Dataset  & $N_{stages}$ & $N_{conv}$ & Conv Filters & $N_{fc}$ & FC-Hidden & $N_{params}$ \\
        \midrule
        VGG-8 & CIFAR10(100) & 2 & 3,3 & 256,256 & 2 & 2048 & 36.72M \\
        VGG-9 & CIFAR10(100) & 3 & 2,2,3 & 256,512,512 & 2 & 1024 & 19.72M \\
        ResNet19 & CIFAR10(100) & 3 & 6,6,4 & 128,256,512 & 1 & N/A & 14.04M \\
        ResNet34 & ImageNet & 4 & 6,8,12,4 & 128,256,512,1024 & 1 & N/A & 277.5M \\
        VGG-12 & DVS-CIFAR10 & 4 & 2,2,3,3,3 & 128 & 2 & 512 & 2.88M \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[!htb]
    \caption{Performance comparison of MT with SoTA methods on CIFAR datasets}
    \label{tab:cifar10_cifar100_dvscifar10_results}
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Dataset & Name & Model & Steps & Accuracy(\%) \\
        \midrule
        \multirow{17}{*}{CIFAR10/100} & \multirow{3}{*}{STBP-tdBN\cite{HanleZheng2020GoingDW}} & \multirow{3}{*}{ResNet-19} & 2 & 92.34 / - \\
        & & & 4 & 92.92 / - \\
        & & & 6 & 93.16 / -\\
        \cline{2-5}
        & PLIF\cite{WeiFang2020IncorporatingLM} & VGG-8 & 8 & 93.50 / - \\
        \cline{2-5}
        & \multirow{3}{*}{InfLoR-SNN\cite{guo2022reducing}} & \multirow{2}{*}{ResNet-19} & 2 & 94.44 / 75.56 \\
        & & & 4 & 96.27 / 78.42 \\
        & & VGG-16 & 5 & 94.06 / 71.56 \\
        \cline{2-5}
        & \multirow{2}{*}{Real Spike\cite{guo2022real}} & \multirow{2}{*}{ResNet-19} & 2 & 94.01 / - \\
        & & & 4 & 95.60 / - \\
        \cline{2-5}
        & \multirow{3}{*}{DSpike\cite{YuhangLi2021DifferentiableSR}} & \multirow{3}{*}{ResNet-18} & 2 & 93.13 / 71.68 \\
        & & & 4 & 93.66 / 73.35 \\
        & & & 6 & 94.25 / 74.24 \\
        \cline{2-5}
        & \multirow{5}{*}{Ours} & \multirow{5}{*}{VGG-9} & 1 & 95.17 / 74.80 \\
        & & & 2 & 95.08 / 74.08 \\
        & & & 3 & 95.25 / 75.04 \\
        & & & 4 & \textbf{95.34} / 75.92 \\
        & & & 5 & 95.14 / \textbf{76.59} \\
        \cline{2-5}
        & \multirow{5}{*}{Ours} & \multirow{5}{*}{ResNet-19} & 1 & 95.31 / 76.92 \\
        & & & 2 & 95.33 / 77.40 \\
        & & & 3 & 95.68 / 77.13 \\
        & & & 4 & 95.64 / \textbf{78.07} \\
        & & & 5 & \textbf{95.79} / 77.33 \\
        \hline
        \multirow{6}{*}{DVS-CIFAR10} & STDP-tdBN\cite{HanleZheng2020GoingDW} & ResNet-17/19 & 10 & 67.80 \\
        & PLIF\cite{WeiFang2020IncorporatingLM} & Conv+FC & 20 & 74.80 \\
        & SEW-ResNet\cite{WeiFang2021DeepRL} & SEW-ResNet & 16 & 74.40 \\
        & DSpike\cite{YuhangLi2021DifferentiableSR} & ResNet-18 & 10 & 75.40 \\
        & MS-ResNet\cite{hu2021advancing} & ResNet-20 & - & 75.56 \\
        & InfLoR-SNN\cite{guo2022reducing} & ResNet-19 & 10 & 75.50 \\
        \cline{2-5}
        & \multirow{2}{*}{Ours} & ST-VGG-12 & 5 & 75.90 \\
        & & MT-VGG-12 & 5 & \textbf{76.30} \\
        \bottomrule
    \end{tabular}
\end{table}

The model configurations are detailed in \cref{tab:model_configs}. Convolutional layers with the same feature map size are considered as a stage, sharing the same number of filters. VGG models terminate with two fully connected layers (hidden and output), while the ResNet models have only the output layer. 

VGG-8, proposed by Wei et al.\cite{WeiFang2020IncorporatingLM}, is a compact architecture for CIFAR10. Due to the large output size of the last pooling layer, 92\% of its parameters reside in the first dense layer. We introduce VGG-9 to deepen the network and reduce model size.

% The VGG-8 model is the model architecture proposed by Wei et al.\cite{WeiFang2020IncorporatingLM} for CIFAR10 dataset, 92\% parameters of the model are from the first dense layer due to the large size of output of the last pooling layer. VGG-9 is proposed to deepen the network and reduce the model size.

Each convolution layer is followed by a batch normalization layer and a PLIF layer to fire spikes, the first Conv-BN-PLIF serves as the encoder, converting image pixels into spikes.

Following Hu et al.\cite{hu2021advancing}, residual connections in ResNet models operate at the membrane potential level. After convolutional layers, a pooling layer reduces the output feature map to 1D before the final output layer. All convolutional layers use 3x3 kernels, except for the first layer in ResNet-34 for ImageNet, which uses a 7x7 kernel.

% The residual connections of the ResNet happen at the membrane level following Yifan Hu et al.\cite{hu2021advancing}. After convolution layers, a pooling layer is utilized to convert the output feature map into 1-D, and then the output layer is appended. The kernel size of all convlutional layers are 3, except the first convolutional layer in ResNet-34 for ImageNet is 7. 

\subsubsection{Datasets}

The CIFAR-10(100) datasets datasets contain 50,000 images for training and 10,000 images for validation with 32x32 pixels in 10(100) classes. Random horizontal flip, random rotation by 15 degree and random height, width shift by 10\% fraction are used as the data augmentation approaches.

The CIFAR10-DVS dataset is the neuromorphic version of the CIFAR-10 dataset, it's split into 9,000 images for training and 1,000 images for validation\cite{YujieWu2019DirectTF}. Events are converted into frames by evenly accumulating into T slices\cite{WeiFang2020IncorporatingLM}. No data augmentation is used.

The Imagenet dataset is a large-scale dataset with 1,281,167 images for training and 50,000 images for validation. The size of model inputs is $224\times224$. For training, images are first resized by setting the shorter edge to a random value within the range $[256, 384]$, followed by a random crop of $224\times224$ patches. After cropping, randomly flipping vertically and horizontally are leveraged as data augmentation. For validation, images are resized by setting the shorter edge to 320 and then center-cropped to $224\times224$. 

\subsubsection{Training Setup}

We employ the SGD optimizer with momentum set to 0.9 in all experiments. The learning rate follows a cosine decay schedule\cite{loshchilov2016sgdr}, decreasing to zero throughout training. For CIFAR10, CIFAR100, and CIFAR10-DVS, we use a batch size of 100 and an initial learning rate of 0.01. The learning rate is reset and decays to zero every 200 epochs with a full restart, and training proceeds for a total of 600 epochs. For ImageNet, we increase the batch size to 5,000 to expedite training, which lasts for 100 epochs. To stabilize training, we warm up the learning rate linearly from 0 to 0.1 during the first 10 epochs, then cosine decay it to 0.001 over the next 80 epochs, and hold it at 0.001 for the final 10 epochs. No learning rate restarts are used for ImageNet.

% SGD optimizer with momentum=0.9 is leveraged in our experiments, with learning rate cosine decayed\cite{loshchilov2016sgdr} to zero. For CIFAR10, CIFAR100, DVS-CIFAR10, batch size is set to 100 and initial learning rate is set to 0.01, the learning rate decays to zero every 200 epochs with a full restart, and training is done by running 600 epochs. To save the training time, for ImageNet, the batch size is set to be 5,000, and training is done in 100 epochs. It's found that warming up learning rate helps stabilize the model training, so the learning rate is linearly increased from 0 to 0.1 in the first 10 epochs, and cosine decays to 0.001 in the following 80 epochs, and stays at 0.001 for the last 10 epochs. No learning rate restart is used in ImageNet training.

\subsubsection{Spiking Thresholds}

We use a spiking threshold of 1.0 for the single-threshold baseline. For PARALLEL MT, we maintain the threshold at 1.0 but introduce additional $\Delta = [-0.3, 0.3]$. As discussed in \cref{sec:parallel_mt}, this is equivalent to using three thresholds: [0.7, 1.0, 1.3]. For CASCADE MT, we set $b=4$, resulting in 4 thresholds as described in Section \ref{sec:cascade_mt}. Through experimentation, we found that an initial spiking threshold of 4 performs best for ImageNet, while 2 is optimal for CIFAR datasets. We include ablation studies to explore the effects of reducing the number of $\Delta$ for PARALLEL MT and the number of bits for CASCADE MT.

% In our experiments, the spiking threshold is set to 1.0 for single threshold setting. For PARALLEL MT, the spiking threshold is still 1.0 but the additional $\Delta = [-0.3, 0.3]$ is introduced, according to \cref{sec:parallel_mt}, it's equivalent to utilizing three thresholds $[0.7, 1.0, 1.3]$. For CASCADE MT, $b$ is set to 4, according to \cref{sec:cascade_mt}, 4 thresholds are used in the experiments, different initial spiking thresholds are explored in the experiments and we find spiking threshold is better to be 4 for ImageNet and 2 for CIFAR10 / CIFAR100. We have ablation studies to reduce the number of $\Delta$ for PARALLEL MT and bits for CASCADE MT.


\subsection{Results}

\subsubsection{CIFAR10 / CIFAR100}

\cref{fig:st_mt_cifar10_cifar100} demonstrates the accuracy advantages of PARALLEL MT and CASCADE MT SNNs over both ST(Single Threshold) SNNs and their ANN counterparts on CIFAR10 and CIFAR100 datasets. Across all VGG-8, VGG-9, and ResNet-19 models, and for time steps ranging from 1 to 5, MT-SNNs consistently outperform their ST counterparts. And CASCADE MT generally yields the best results.

\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
    \includegraphics[width=12cm, height=6.5cm]{figures/st_mt_cifar10_100_v5.png}
    \vspace*{-3mm}
    \caption{Performance comparison of ST and MT on CIFAR10 / CIFAR100 with various steps. There are four arms in each figure, namely ANN, SNN with Single Threshold(ST), SNN with PARALLEL MT, CASCADE MT.}
    \label{fig:st_mt_cifar10_cifar100}
    \end{center}
    \vskip -0.2in
\end{figure}


Notably, MT-enabled VGGs surpass its ANN counterpart at a single time step, while Spiking ResNet-19 achieves comparable accuracy to the ANN at $step=4$. MT significantly raises the performance ceiling for SNNs. Even with extensive time steps (10 and 20), ST-SNNs remain inferior to MT approaches, as shown in \cref{fig:st_mt_cifar10_cifar100}-(e,f). 


\cref{tab:cifar10_cifar100_dvscifar10_results} compares our results with previous state-of-the-art (SOTA) models on similar architectures. Our MT ResNet-19 achieves remarkable accuracies of 95.79\% on CIFAR10 and 78.07\% on CIFAR100, nearly matching the previous SoTA InfLoR-SNN \cite{guo2022reducing}. Impressively, our ResNet-19 model with just one step could reach 95.31\% on CIFAR10 and 76.92\% on CIFAR100, exceeding the InfLoR-SNN at $step=2$ and outperforming the other prior works.

\subsubsection{ImageNet}

\cref{tab:imagenet_results} summarizes the our results and comparisons with previous SOTA models on ImageNet. Our ST SNN implementation reaches 66.79\% accuracy, on par with SEW-ResNet\cite{WeiFang2021DeepRL}, InfLoR-SNN\cite{guo2022reducing} and Real Spike\cite{guo2022real} with $step=4$. Applying MT leads to signficant improvements, with CASCADE MT achieving 72.17\% accuracy at just one time step, surpassing the previous SOTA MS-ResNet\cite{hu2021advancing} even at $step=6$.

% And significant improvements are observed after applying the MT approaches, specifically, the CASCADE\_MT model achieves 72.17\% accuracy at $step=1$, outperforming the previous SOTA model MS-ResNet\cite{hu2021advancing} with even $step=6$.


\begin{table}[tb]
    \caption{ImageNet Performance comparison of our methods with SOTA on ResNet-34}
    \label{tab:imagenet_results}
    \centering
    \begin{tabular}{ccc}
        \toprule
        Name & Steps & Accuracy(\%) \\
        \midrule
        SEW-ResNet\cite{WeiFang2021DeepRL} & 4 & 67.04 \\
        DSpike\cite{YuhangLi2021DifferentiableSR} & 6 & 68.19 \\
        MS-ResNet\cite{hu2021advancing}  & 6 & 69.42 \\
        InfLoR-SNN\cite{guo2022reducing}  & 4 & 65.54 \\
        Real Spike\cite{guo2022real}  & 4 & 67.69 \\
        \hline
        ST &  4 & 66.79 \\
        PARALLEL-MT & 1 & 69.32 \\
        CASCADE-MT & 1 & \textbf{72.17} \\
        \bottomrule
    \end{tabular}
\end{table}


\subsubsection{CIFAR10-DVS}

We evaluate MT on the CIFAR10-DVS dataset using the VGG-12 model in \cref{tab:model_configs}. \cref{fig:vgg12_dvscifar10_results} demonstrates significant accuracy gains with MT, particularly at early time steps (less than 4). Our model ultimately achieves 76.3\% accuracy, surpassing previous models with similar architectures in \cref{tab:cifar10_cifar100_dvscifar10_results}.

However, the performance gap between ST and MT narrows with increasing time steps. We also observe substantial accuracy fluctuations on CIFAR10-DVS, likely due to its limited size. Evaluating MT on larger DVS datasets is an important direction for future work, although such datasets are currently scarce.

% VGG-12 in \cref{tab:model_configs} is adopted on DVS-Cifar10 dataset to evaluate the effectiveness of MT.
% \cref{fig:vgg12_dvscifar10_results} demonstrates significant accuracy improvements with MT, particularly at early time steps less than 4. Our model achieves a final accuracy of 76.3\%, surpassing previous models with comparable architectures ( \cref{tab:cifar10_cifar100_dvscifar10_results}).

% However, the performance gap between ST and MT diminishes at larger time steps. Additionally, we observe significant accuracy fluctuations during DVS-CIFAR10 experiments, likely due to the dataset's limited size. Exploring MT on larger-scale DVS datasets remains an important area for future research, as such datasets are currently difficult to obtain.


\begin{figure}[tb]
    \vskip 0.2in
    \begin{center}
    \includegraphics[width=4.8cm, height=3.8cm]{figures/st_mt_dvscifar10_v4.png}
    \vspace*{-3mm}
    \caption{Performance comparison of ST and PARALLEL MT on CIFAR10-DVS with various steps.}
    \label{fig:vgg12_dvscifar10_results}
    \end{center}
    \vskip -0.2in
\end{figure}


\subsection{Ablation Study}

% We conduct extensive ablation studies to isolate factors driving performance improvements.
Extensive ablation studies are conducted to pinpoint the factors driving performance improvements.

First, we analyze PARALLEL MT settings. Recall that our previous experiments used $\Delta=[-0.3, 0.3]$ for all LIF layers. \cref{fig:ablation_exp}-(a) shows that using $\Delta=[-0.3]$ and $\Delta=[0.3]$ individually yields similar improvements, roughly 80\% of the combined gain. Additionally, we find that applying PARALLEL MT to fully-connected layers is detrimental in \cref{fig:ablation_exp}-(b). Limiting MT to convolutional layers, particularly for VGG-9 with 42\% of parameters in fully-connected layers, yields superior results.

% Firstly, we analyze the settings for PARALLEL MT. $\Delta=[-0.3, 0.3]$ is applied to all LIF layers in the previous experiments. \cref{fig:ablation_exp}-(a) shows that $\Delta=[-0.3]$ and $\Delta=[0.3]$ individually achieve similar improvements, approximately 80\% of the combined gain. Also, PARALLEL MT is applied to both convolution and fully-connected layers, interestingly, \cref{fig:ablation_exp}-(b) reveals that applying MT to fully-connected layers is detrimental, while applying it solely to convolutional layers yields superior results compared to deploying it across all layers. Notably, this was tested on VGG-9, which has a significant proportion of parameters (42\%) in its fully-connected layers.

Second, we explore the impact of the number of bits (b) in CASCADE MT. \cref{fig:ablation_exp}-(c) demonstrates that increasing b improves accuracy, but with diminishing returns. Notably, $b=2$ achieves roughly 50\% of the total improvement from ST to CASCADE MT ($b=4$), while $b=3$ yields near-optimal accuracy.

% Secondly, the key hyper-parameter, number of bit in CASCADE\_MT is explored. \cref{fig:ablation_exp}-(c) demonstrates that increasing b improves accuracy, but with diminishing returns. Importantly, $b=2$ achieves approximately 50\% of the total improvement from ST to CASCADE\_MT ($b=4$), while $b=3$ yields accuracy close to $b=4$.

\begin{figure*}[tb]
    \vskip 0.2in
    \begin{center}
    \includegraphics[width=12cm, height=3.6cm]{figures/ablation_exp_v4.png}
    \vspace*{-3mm}
    \caption{Ablation study of PARALLEL MT and CASCADE MT on CIFAR100. (a): Impact of thresholds $\Delta$ in PARALLEL MT. (b): Impact of layer placement in PARALLEL MT. (c): Impact of bit number in CASCADE MT.}
    \label{fig:ablation_exp}
    \end{center}
    \vskip -0.2in
\end{figure*}


\subsection{Sparsity, Efficiency and Energy Cost}

We measure sparsity, computation, and energy consumption following Li et al.\cite{YuhangLi2021DifferentiableSR} and Rathi et al.\cite{rathi2020diet}. We use ${s}\times{T}\times{A}$ to estimate additions in SNNs, where $s$ is average sparsity, $T$ is the time step, and $A$ is the total additions in the corresponding ANN. Energy is based on 45nm CMOS technology, with Multiply-ACcumulate (MAC) costing $4.6pJ$ and accumulation costing $0.9pJ$.


% Sparsity, computation and energy consumption are measured in this section. We follow Yufang Li et al.\cite{YuhangLi2021DifferentiableSR} to use $s * T * A$ to count the addition number of SNN, where $s$ is the average sparsity, $A$ is the total additional number of its ANN counterpart, and $T$ is the time step.
% Also, Rathi et al.\cite{rathi2020diet} is followed to measure the energy based on 45nm CMOS technology, in which Multiply-ACcumulate(MAC) costs $4.6pJ$ and accumulation costs $0.9pJ$ energy.


\cref{fig:activation_sparsity} shows input sparsity for each VGG-9 SNN layer (excluding the first layer, conv0, which takes raw images). ST, PARALLEL MT with two additional thresholds, and CASCADE MT with 4 bits are compared. ST models exhibit low sparsity (2.6\% to 13.3\%) across most layers, except for 21.9\% in the final dense layer. This low sparsity is key to energy efficiency. While MT introduces multiple activations, average sparsity remains close to ST levels. Thus, additions increase roughly linearly with the number of thresholds (PARALLEL MT) or bits (CASCADE MT). In this example, PARALLEL MT has about 3x and CASCADE MT has 4x the additions of ST.

% \cref{fig:activation_sparsity} shows the input sparsity of each VGG-9 SNN layer. The first convolution layer conv0 is excluded here as the input is the source images. Models with ST, PARALLEL\_MT with two additional thresholds and CASCADE\_MT with 4 bits are compared. Firstly, the ST model exhibits low sparsity from 2.6\% to 13.3\% across most layers, with an exception of 21.9\% in the final dense layer. The low sparsity is of key importance to the energy efficiency. Secondly, while MT introduces multiple activations, average sparsity remains close to that of ST. Therefore, additions increase approximately linearly with the number of thresholds (PARALLEL\_MT) or bits (CASCADE\_MT). In this example, PARALLEL\_MT has roughly 3x and CASCADE\_MT has 4x the additions of the ST model.

\cref{tab:cifar100_energy} presents operations and energy consumption for VGG-9 models on CIFAR100. PARALLEL MT at $step=5$ slightly outperforms ST at $step=15$, while CASCADE MT at $step=5$ significantly outperforms ST at $step=20$, improving accuracy by 1.10\% with less energy. Notably, PARALLEL MT efficiency can be further improved by removing MT from fully-connected layers, and CASCADE MT benefits from reducing bits from 4 to 3, as per our ablation study.

% \cref{tab:cifar100_energy} displays the number of operations and energy consumption of the VGG-9 models on CIFAR100 dataset. PARALLEL\_MT model at $step=5$ slightly outperforms the ST model at $step=15$, and CASCADE\_MT at $step=5$ largely outperforms the ST model at $step=20$ by improving the accuracy by 1.10\% with fewer energy. Furthermore, the efficiency of PARALLEL\_MT could be better considering the accuracy won't drop by removing the PARALLEL\_MT in the fully-connected layer and reducing the bits of CASCADE\_MT from 4 to 3 in the ablation study.


\begin{figure*}[tb]
    \vskip 0.2in
    \begin{center}
    \includegraphics[width=12cm, height=4.0cm]{figures/activation_sparsity_v3.png}
    \vspace*{-3mm}
    \caption{Activation Sparsity of VGG-9 layers on CIFAR100}
    \label{fig:activation_sparsity}
    \end{center}
    \vskip -0.2in
\end{figure*}

\begin{table}[tb]
    \caption{Engergy comparison of different VGG-9 model variant on CIFAR100}
    \label{tab:cifar100_energy}
    \centering
    \begin{tabular}{cccccc}
        \toprule
        Model & Step & Accuracy(\%) & \#MUL & \#ADD & Energy \\
        \midrule
        ANN & - & 73.54 & 1978M & 1978M & 10.8J \\
        \hline
        \multirow{5}{*}{ST} & 1 & 72.63 & 7.07M & 182.2M & 0.19J \\
        & 3 & 74.59 & 21.23M & 546.61M & 0.59J \\
        & 4 & 75.04 & 28.31M & 782.82M & 0.83J \\
        & 15 & 75.46 & 106.17M & 2733.06M & 2.96J \\
        & 20 & 75.45 & 141.56M & 3644.08M & 3.93J \\
        \hline
        \multirow{2}{*}{PARALLEL MT} & 1 & 73.89 & 7.07M & 453.8M & 0.44J \\
        & 5 & 76.19 & 35.39M & 2269.32M & 2.21J \\
        \hline
        \multirow{2}{*}{CASCADE MT} & 1 & 74.80 & 7.07M & 784M & 0.74J \\
        & 5 & 76.59 & 35.39M & 3924.10M & 3.69J \\
        \bottomrule
    \end{tabular}
\end{table}

% Firstly, it's observed that the sparsity rates of the ST model is small to be within the range from 2.6\% to 13.3\%, except the last dense layer to be 21.9\%. The low sparsity is of key importance to the low energy consumption. Secondly, multiple activations will be generated by leveraging the multiple thresholds, but the average sparsity is close to the sparsity of single threshold, so the number of addition increases linearly with the grow of the additional thresholds in PARALLEL\_MT and bits in CASCASE\_MT. For the case illustrated in \cref{fig:activation_sparsity}, the PARALLEL\_MT has almost 3x addition operations and the CASCADE\_MT contains 4x addition operations compared to the ST model.


% \cref{tab:cifar100_energy} displays the number of operations and energy consumption of the VGG-9 models on CIFAR100 dataset. We could observe that PARALLEL\_MT model at step 4 slightly outperforms the ST model at step 12, and CASCADE\_MT at step 4 largely outperforms the ST model at step 16 by improving the accuracy by 1.10\% with fewer energy. Furthermore, the efficiency of PARALLEL\_MT could be better considering the accuracy won't drop by removing the PARALLEL\_MT in the fully-connected layer in the ablation study.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%