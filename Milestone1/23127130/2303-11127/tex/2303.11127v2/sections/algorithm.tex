%%%%%%%%%%%%%%%%%%%%%%%%%%% Algorithm %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Multi-Threshold SNN}

Increasing the number of time steps often improves SNN performance, as seen in Fig. \ref{fig:st_mt_cifar10_cifar100}.  Single-Threshold (ST) SNNs typically underperform their ANN counterparts at lower time steps, but accuracy improves with increasing time steps. For VGG-like architectures, SNNs can match or even surpass ANN accuracy when the number of time steps exceeds 3.

% Increasing time steps often improves SNN performance. From \cref{fig:st_mt_cifar10_cifar100}, we observe that single-threshold SNNs typically underperform their ANN counterparts at lower time steps. However, the accuracies are improved while increasing time steps, for VGG-like architectures, the SNNs can achieve or surpass ANN accuracy when time steps exceed 3.

% It's commonly observed that the performance of SNNs can be improved by increasing time steps. It can be visualized from \cref{fig:st_mt_cifar10_cifar100} that SNNs with single threshold and VGG architectures generally perform worse than the corresponding ANN counterpart, but can reach or exceed when the step in larger than 3.

This observation suggests that the precision lost in converting floating-point activations to spikes in SNNs can be compensated by using multiple time steps.  In this paper, we propose an alternative solution: the Multiple Threshold (MT) approach, which aims to preserve precision within activations even with a single time step. In the following sections, we detail the parallel and cascade modes of MT implementation.

% We argue that compared to the ANNs, the precision of intermediate activations of SNNs is lost in replacing float values with spikes but compensated on multiple time steps. In this paper, Multiple Threshold (MT) approach is proposed to provide an alternative to reserve precision within activations for a single step. Specifically, MT in parallel mode and cascade mode will be explained in the following sections.


\subsection{PARALLEL MT}
\label{sec:parallel_mt}


The idea is illustrated in \cref{fig:st_mt}-(a, b). In \cref{fig:st_mt}-(a), a single threshold $\mu_{th}$,  discretizes the input membrane potential into binary values 0 or 1, leading to a loss of information regarding how much the potential exceeds or falls below the threshold. In \cref{fig:st_mt}-(b), we introduce two auxiliary thresholds $\mu_{th1,th2}$. These thresholds generate two additional spike sequences based on their respective comparisons with the membrane potential. Summing these three spike sequences results in a more accurate representation of the original, continuous membrane potential.

% The idea is illustrated in \cref{fig:st_mt}-(a, b). In \cref{fig:st_mt}-(a), $\mu_{th}$ splits the input membrane into two values 0 and 1, in this process, the information about how much the membrane is greater / smaller than the threshold is completely lost. In \cref{fig:st_mt}-(b), two auxiliary thresholds $\mu_{th1,th2}$ are added, two more series of spikes are obtained based on the auxiliary thresholds, the result that adds up the three sequences of spikes is a more accurate representation of the original membrane.

To formulate the approach, a series of parameters are introduced in, namely $[\Delta_1, ..., \Delta_n]$, and integrated into \cref{eqn:neuron_spike}:

\begin{equation}
\label{eqn:neuron_multiple_spikes}
    S[t]_{i} = \Theta(H[t] - V_{th} - \Delta_i), i \in [1, 2, ..., n]
\end{equation}

And \cref{eqn:neuron_update} is still used to update the membrane. After obtaining all spikes for each threshold, the final outputs are obtained by summing the spikes together.

\begin{equation}
\label{eqn:neuron_multiple_sum_spikes}
    S[t]_{sum} = S[t] + \Sigma{S[t]_{i}}
\end{equation}


However, after applying multiple thresholds, from \cref{eqn:neuron_multiple_sum_spikes}, the outputs have other possible values other than 0/1, which breaks the multiplication-free property. To preserve the property, the subsequent convolution layer can be involved in to perform an equivalent operation in \cref{eqn:neuron_sum}, right side remains multiplication-free.

\begin{equation}
\label{eqn:neuron_sum}
    Conv(S[t]_{sum}) = Conv(S[t]) + \Sigma{Conv(S[t]_{i})}
\end{equation}

This formulation allows the left-hand side of Eq. \ref{eqn:neuron_sum} to be deployed on standard hardware like GPUs or TPUs, while the right-hand side remains compatible with neuromorphic hardware.

% Generally, left side of \cref{eqn:neuron_sum} can be deployed on current standard hardwares like TPU, GPU etc and right side on neuromorphic hardwares.


\subsection{CASCADE MT}
\label{sec:cascade_mt}


The concept of CASCADE MT is illustrated in \cref{fig:st_mt}-(a, c). As shown in \cref{fig:st_mt}-(c), we employ three sequentially decreasing thresholds ($\mu_{th}$, $\mu_{th}/2$, $\mu_{th}/4$), checked from highest to lowest, A soft reset mechanism allows the remaining membrane potential to be utilized by lower thresholds, maximizing information retention.

To formulate this approach, a new parameter $b$ is introduced to determine the number of threshold reductions. By default, the threshold reduction factor is set to be 2, creating a close analogy to activation quantization.




% Similarly, the concept of CASCADE\_MT is illustrated in \cref{fig:st_mt}-(a, c). As shown in \cref{fig:st_mt}-(c), three sequentially decreasing thresholds ($\mu_{th}$, $\mu_{th}/2$, $\mu_{th}/4$) are checked from highest to lowest, and soft reset machanism allows remaining membrane potential to be utilized by lower thresholds.

% To formulate the approach, a new parameter $b$ is introduced to determine the number of threshold reductions. By default, the threshold reduction factor is set to be 2, creating a close analogy to activation quantization.

The forward path of CASCADE MT is outlined in \cref{alg:cascade_mt}. For a given membrane potential $H[t]$, firing is performed b times, in each iteration, the threshold is halved, the membrane potential is the residual from the previous step and the resulted spikes will be weighted by $2^{b-1-i}$.

% The forward path of CASCADE\_MT is outlined in \cref{alg:cascade_mt}. For membrane $H[t]$, firing are performed b times, in each iteration, threshold is decreased by dividing 2, membrane is the residual membrane from the previous step and the resulted spikes will be weighted by $2^{b-1-i}$.

\begin{figure}[ht]
    \centering
    \vspace{-1.2cm}
    \begin{minipage}{.60\linewidth}
        \begin{algorithm}[H]
            \caption{CASCADE Multiple Threshold}
            \label{alg:cascade_mt}
            \begin{algorithmic}
                \ENSURE{$H[t]$}\COMMENT{Membrane at time t}
                \ENSURE{$V_{th}$}\COMMENT{Initial Threshold}
                \ENSURE{$b$}\COMMENT{Number of bits}
                \REQUIRE{$S[t]_{sum}$}\COMMENT{The sum of output spikes}
                \STATE $h\gets{H[t]}$
                \STATE $S[t]_{sum}\gets{0}$
                \FOR{$i$\ in $\left[ 0 ... b\right)$ }
                    \STATE $S[t]_i\gets{\Theta(h - V_{th} / 2^i)}$
                    \STATE $h\gets{h-S[t]_i * V_{th} / 2 ^ i}$
                    \STATE $S[t]_{sum}\gets{S[t]_{sum} * 2 + S[t]_i}$
                \ENDFOR
            \end{algorithmic}
        \end{algorithm}
    \end{minipage}
    \vspace{-0.6cm}
\end{figure}


Similar to PARALLEL MT, the resulting $S[t]_{sum}$ in \cref{alg:cascade_mt} is not strictly 0/1 spikes. To maintain compatibility with neuromorphic hardware, we involve the weights in the next convolutional layer to perform an equivalent operation, as shown in the right-hand side of \cref{eqn:cascade_neuron_sum}. 
While the weights of convolution layers in right side \cref{eqn:cascade_neuron_sum} are duplicated by multiplying $2^i$. 


% Similar to PARALLEL\_MT, the $S[t]_{sum}$ in \cref{alg:cascade_mt} is not 0/1 spikes anymore. To adapt the neuromorphic hardwares, the weights in next convolution layer are involved to perform the equivalent operation.
% In right side \cref{eqn:cascade_neuron_sum}, the weights of convolution layers are duplicated by multiplying $2^i$. 


\begin{equation}
\begin{split}
\label{eqn:cascade_neuron_sum}
    Conv(S[t]_{sum}) & = \Sigma{ Conv_{W_{i}}(S[t]_{i}) }\\
    W_{i} & = W * 2^i, i \in [0, b)
\end{split}
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%