%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Related works %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}

\subsection{Learning Algorithms}

After the emergence of SNNs, how to train the models effectively stays as an active area. The earliest learning algorithms were based on biological plausible local learning rules, like Hebbian learning \cite{hebb2005organization} and Spike-Timing-Dependent Plasticity \cite{bi1998synaptic}.
However, such methods were only suitable for shallow SNNs, and the performance is far below methods mentioned below.

ANN-To-SNN methods are capable of obtaining high-quality SNNs by leveraging the knowledge from well-trained ANNs and converting the ReLU activations to spike activations based on rate coding \cite{sengupta2019going, deng2021optimal}. However, achieving near lossless conversion requires a considerable amount of time steps ($>200$) to accumulate the spikes, which may significantly increase the latency.

Backpropagation with surrogate gradient is widely used in recent researches to achieve high-quality deep models  \cite{lee2016training,wu2018spatio,lee2020enabling}. The utilization of surrogate gradient function enables end-to-end backpropagation in SNN training, thus training knowledge of ANNs can be transferred to SNNs. The remarkable robustness of the surrogate gradient based algorithms were studied 
in \cite{zenke2021remarkable, neftci2019surrogate}, demonstrating that SNNs trained with surrogate gradient can achieve competitive performance with ANNs. Backpropagation with surrogate functions requires much fewer steps compared to ANN-To-SNN methods as it's not dependent of rate coding \cite{YujieWu2019DirectTF}.


\subsection{Model architecture}

Backpropagation with surrogate gradient unlocks the model architecture migration from ANNs to SNNs. Yujie et al. \cite{YujieWu2019DirectTF} initially applied direct training of various sized deep convolutional neural networks on both static and neuromorphic datasets with normalization and rate coding. Hanle et al. \cite{HanleZheng2020GoingDW} explored ResNet with time-dependent batch normalization and validated the architecture on ImageNet for the first time.

Spike-element-wise ResNet \cite{WeiFang2021DeepRL} and membrane based ResNet \cite{hu2021advancing} pushed the depth of the architecture to more than 100 layers. Attention machanism was introduced into SNN by Man Yao et al. \cite{yao2022attention}, achieving comparable and even better performance compared to the ANN counterpart on large-scale datasets. Furthermore, Spiking Transformer was proposed by combining self-attention with spiking neural network \cite{zhou2022spikformer} . There will be more works on such model migrations in the foreseeable future.

\subsection{Information Loss}

Information loss in SNNs attracts increasing attentions recently. Soft Reset was leveraged to reserve the information in the residual potential, and Membrane Potential Rectifier was proposed to alleviate the precision loss in quantization \cite{guo2022reducing}. Yufei Guo et al. \cite{guo2022real} leveraged a re-parameterization technique to decouple training and inference stage, and enhance the presentation capacity b learning real-valued spikes during training and transferring the capacity to model parameters by not sharing convolution kernels. The MT approaches in this paper fall into the same domain with Yufei Guo et al. \cite{guo2022reducing, guo2022real}, which provides another option to reduce information loss in spike activations.

\subsection{Quantization}

Quantization is also a very promising approach to reduce computational cost and memory footprint while maintaining competitive performance \cite{rastegari2016xnor, bulat2019xnor}. Spiking could be considered as the 1-bit quantization, thus SNN training could be considered as the activation quantization aware ANN training. 

The CASCADE MT approach proposed in this paper is a general way to expand the 1-bit quantization property of SNNs into multiple bits, saving the precision loss.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%