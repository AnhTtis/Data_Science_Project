% !TeX spellcheck = en_US
\appendices
\section{Proof of Lemma~\ref{lemma:finalprob}}\label{app:1}
%Since the two events are independent, to compute the probability of both happening \textcolor{blue}{within a time window $\bar{t}$}, \textcolor{blue}{one} can \textcolor{blue}{take the product of } multiply the probability of the two separate events and integrate it over the desired interval.

%the overall probability of the two events happening within a time interval $\bar{t}$ is the integral from $t=0$ to $t = \infty$ of one of the two functions, multiplied by the integral of the second function. However the extremes of the latter integral, are from $t-\bar{t}$ to $t-\bar{t}$.   

Recall that the exponential distribution, whose probability density function is given by $f(x)= \alpha e^{-\alpha x}$, models the time between events in a Poisson process of parameter $\alpha$. Since the two Poisson processes are independent,
\begin{equation*}
	\begin{split}
		P(\alpha_m,\alpha_n) = & \int_0^{\bar{t}} \alpha_n  e^{-\alpha_n  t_n} \left( \int_0^{t_n+\bar{t}} \alpha_m  e^{-\alpha_m t_m} \mathrm{d}t_m \right) \mathrm{d}t_n + \\
		& \int_{\bar{t}}^{\infty} \alpha_n e^{-\alpha_n t_n} \left(\int_{t_n-\bar{t}}^{t_n+\bar{t}} \alpha_m e^{-\alpha_m t_m}  \mathrm{d}t_m \right)  \mathrm{d}t_n,
	\end{split}
\end{equation*}
where the presence of two terms arises from the fact that the time interval $[0,+\infty)$ is considered. Making use of standard integral calculus techniques, it can be rewritten as \eqref{eq:lem}.
\begin{comment}
Evaluating the first integral we get
\begin{align*}
	\int_0^{\bar{t}} \int_0^{t_n+\bar{t}} \alpha_n   e^{-\alpha_n  t_n}  \alpha_m   e^{-\alpha_m  t_2}  \mathrm{d}t_2  \mathrm{d}t_n 
	%=\\ 
	%\int_0^{\bar{t}} \alpha_n  e^{-\alpha_n   t_n}  %\left(1 - e^{-\alpha_m   (t_n+\bar{t})}\right)  %\mathrm{d}t_n 
	=\\ 
	\int_0^{\bar{t}} \left( \alpha_n  e^{-\alpha_n   t_n} - \alpha_n  e^{-t_n \cdot(\alpha_n +\alpha_m) -\alpha_m   \bar{t}} \right)  \mathrm{d}t_n  
	=\\
	\left(1 - e^{-\alpha_n  \bar{t}}\right) - \frac{\alpha_n}{\alpha_n+\alpha_m} e^{-\alpha_m  \bar{t}}  \left(1 - e^{-(\alpha_n+\alpha_m) \bar{t}}\right),
\end{align*}
whilst from the second integral we obtain
\begin{align*}
	\int_{\bar{t}}^{\infty} \int_{t_n-\bar{t}}^{t_n+\bar{t}} \alpha_n e^{-\alpha_n  t_n}  \alpha_m  e^{-\alpha_m  t_2}  \mathrm{d}t_2  \mathrm{d}t_n 
	%=\\
	%\int_{\bar{t}}^{\infty} \alpha_n e^{-\alpha_n  t_n}  \left(e^{-\alpha_m  (t_n-\bar{t})} - e^{-\alpha_m  (t_n + \bar{t})}\right)   \mathrm{d}t_n 
	=\\ 
	\int_{\bar{t}}^{\infty} \alpha_n   e^{-(\alpha_n+\alpha_m)  t_n}   \left(e^{\alpha_m   \bar{t}}-e^{-\alpha_m   \bar{t}}\right)  \mathrm{d}t_n 
	=\\
	\frac{\alpha_n}{\alpha_n+\alpha_m}  \left(e^{\alpha_m   \bar{t}}-e^{-\alpha_m   \bar{t}}\right)  \left(e^{-(\alpha_n+\alpha_m) \bar{t}}\right).
\end{align*}
\end{comment}
%We obtain
\begin{comment}
\begin{align*}
	P_{kl} = 	\left(1 - e^{-\alpha_n  \bar{t}}\right) 
	-\\- \frac{\alpha_n}{\alpha_n+\alpha_m} e^{-\alpha_m  \bar{t}}  \left(1 - e^{-(\alpha_n+\alpha_m) \bar{t}}\right) 
	+\\+ 
	\frac{\alpha_n}{\alpha_n+\alpha_m}  \left(e^{\alpha_m   \bar{t}}-e^{-\alpha_m   \bar{t}}\right)  \left( e^{-(\alpha_n+\alpha_m) \bar{t}}\right) 
	=\\=
	\left(1 - e^{-\alpha_n  \bar{t}}\right) 
	-
	\frac{\alpha_n}{\alpha_n+\alpha_m} \cdot
	e^{-\alpha_m  \bar{t}}
	+\\+
	\frac{\alpha_n}{\alpha_n+\alpha_m}  e^{\alpha_m   \bar{t}}  \left(e^{-(\alpha_n+\alpha_m) \bar{t}}\right) 
	=\\=
	\end{align*}
\end{comment}
%\begin{align*}
%	P_{mn}(\alpha_m,\alpha_n) 
%	%&= 
%	%\left(1 - e^{-\alpha_n  \bar{t}}\right) - \frac{\alpha_n}{\alpha_n+\alpha_m}   \left(e^{-\alpha_m  \bar{t}} -  e^{-\alpha_n   \bar{t}}\right)\\
%	&= 1-\frac{\alpha_n e^{-\alpha_m\bar{t}}+\alpha_m e^{-\alpha_n\bar{t}}}{\alpha_n+\alpha_m}.
%\end{align*}
%We do a quick sanity check of the obtained results. If \mbox{$\alpha_n=0$} or $\alpha_m=0$, $P_{nm} =0$, meaning flows have 0\% probability of being matched. If $\alpha_n = \alpha_m$ the formula coincides with the original formulation with 1 event.

\section{Proof of Theorem~\ref{theorem:one}}\label{app:2}

The convergence of Algorithm~\ref{alg:one} in at most  ${\!\abs{\cM}(\abs{\cM}\!-\!1)\!}$ iterations is immediate. In fact, since for each pair $(m,n)$ chosen in each iteration we set $\Delta \tilde{J}^\prime_{mn} \!= \!\Delta \tilde{J}^\prime_{nm} \!= \!0$, neither $(n,m)$ nor $(m,n)$ will be chosen again. The optimality of the solution $\beta^\star$ and associated $\gamma^\star$ is carried out making use of an analogy with the continuous Knapsack problem,  which can be solved by a well-known polynomial-time greedy algorithm~\cite{Dantzig1957}. Recall that such algorithm consists in, every iteration, allocating the maximum amount of the resource with the highest improvement in the objective function per unit of  the resource, which is intuitively evident.  Similarly to the continuous Knapsack problem, the goal is to minimize $\tilde{J}(X^\star_\gamma)$ by allocating ${\gamma_{mn} \geq 0}$ with $m,n \in \cM$. First, borrowing the notation from Section~\ref{sec:SD}, if $\gamma_{mn}$ is assigned, then the corresponding decrease in the cost function amounts to $\tilde{J}(\gamma_{nm}X^{mn,0})\! -\!\tilde{J}(\gamma_{mn}X^{mn,\star}) =\gamma_{mn}(\tilde{J}(X^{mn,0})\! -\!\tilde{J}(X^{mn,\star})) = \gamma_{mn}\Delta{\tilde{J}}_{mn}$, where the linearity of $\tilde{J}$ played a key role. Thus, the allocation of $\gamma_{mn}$ leads to a relative improvement on the cost that amounts to $\Delta{\tilde{J}}_{mn}$. Second, as pointed out in Section~\ref{sec:alg}, throughout the algorithm, $\alpha^\prime_m$ corresponds to the demand of $r_m$ which has not yet been ride-pooled with another request. Thus, the value of $\gamma_{mn}$ that can be allocated has an upper bound given by $\gamma_{mn} \leq \min(\alpha_m^\prime, \alpha_n^\prime)P(\alpha_m^\prime,\alpha_n^\prime)$. Note that Algorithm~\ref{alg:one} corresponds to allocating the maximum amount of $\gamma_{mn}$, where $m$ and $n$ are such that, at each iteration, the highest positive relative improvement in the objective function is achieved, i.e., $(m,n) \in \mathrm{argmax}_{m,n} (\Delta \tilde{J}^\prime_{mn})$, which shows its optimality.




%We prove Theorem~\ref{theorem:one} by contradiction. If $\beta$ would not be an optimal parameter, there would exist an optimal parameter $\beta'$, so that the objective of Problem~\ref{prob:rides} $\tilde{J}(X^\star_{\gamma'}) < J(X^\star_{\gamma})$, where $X^\star_{\gamma}$ is the optimal solution of Problem~\ref{prob:rides} with demand matrix $D^\mathrm{rp}$, that, leveraging~\eqref{eq:b}, is a function of the expected number of pooled rides $\gamma$, that in turn is a function of the assignment matrix $\beta$.  To have a more compact notation, we define $\gamma_{nm}=\gamma_{nm}(\beta_{nm},\beta_{mn})$, and $\gamma'_{nm}=\gamma_{nm}(\beta'_{nm},\beta'_{mn})$. Moreover, we define $\Delta J _{nm} = \Delta J _{nm}(X^\star_{\gamma_{nm}})$, $\Delta J'_{nm} = \Delta J_{nm}(X^\star_{\gamma'_{nm}})$, and $\Delta J_{nm}^1 = \Delta J_{nm}(X^\star_{D^{nm,\star}})$.
%We recall that $ \Delta J _{nm}(X^\star_{\gamma_{nm}})$ is linear w.r.t. $\gamma_{nm}$. In turn, $\gamma_{nm}$ is monotonically increasing w.r.t. $\beta_{nm}$ and $\beta_{mn}$.
%For the sake of simplicity, let's consider a simple case with only 2 demands $n$ and $m$. They can both pool with themselves---pooling options $nn$ and $mm$---and with the other one---pooling options $nm$ and $mn$---. The algorithm prioritizes the pooling of a requests with the highest improvement in the partial objective function. In this case, options $nn$ and $mm$ will be prioritized because they have no delay.
%This results in
%\begin{equation}\label{eq:lin2}
%	\Delta J^1 _{nm} < \Delta J^1 _{mm}, \;\; \Delta J^1_{mn} < \Delta J^1_{mm},
%\end{equation} 
%\begin{equation*}
%	\Delta J^1_{nm} < \Delta J^1 _{nn}, \;\; 	\Delta J^1_{mn} < \Delta J^1 _{nn}. 
%\end{equation*}
%If $\beta$ is not the optimal parameter, then $\beta'$ would exist so that:
%\begin{equation} \label{eq:monot}
%	\beta_{nn} - \beta'_{nn}  \geq 0, \;\;\beta_{nm} - \beta'_{nm} \leq 0,
%\end{equation} 
%\begin{equation*}
%	\beta_{mm} -  \beta'_{mm} \geq 0, \;\; 	\beta_{mn} - \beta'_{mn} \leq 0.
%\end{equation*} 
%Since travel time is constant, it follows that
%\begin{equation}
%	\Delta J_{nn}+ \Delta J_{mm} + \Delta J_{nm} + \Delta J_{mn} <  
%\end{equation}
%\begin{equation*}
%	< \Delta J'_{nn} + \Delta J'_{mm}+ \Delta J'_{nm} + \Delta J'_{mn}. 
%\end{equation*}
%The problem linear in $\gamma$. It means that \begin{equation}\label{eq:deltaj}
%	(\gamma_{nn} -\gamma'_{nn} )\Delta J^1_{nn} +
%  (\gamma_{mm} -\gamma'_{mm} )\Delta J^1_{mm} -
%\end{equation}
%\begin{equation*}
%	-(\gamma_{nm} -\gamma'_{nm} )\Delta J^1_{nm}- (\gamma_{mn} -\gamma'_{mn} )\Delta J^1_{mn} < 0.
%\end{equation*}
%By leveraging Eq.~\eqref{eq:lin2},\eqref{eq:monot} and recalling that $\gamma$ is monotone w.r.t. $\beta$, Eq.~\eqref{eq:deltaj} never holds, resulting in $J(X^\star_{\gamma'}) \geq J(X^\star_{\gamma})$.  This concludes the proof.