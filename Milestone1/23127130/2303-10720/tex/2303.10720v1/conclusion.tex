\section{Conclusion}

\looseness=-1  Proposing a bi-level constrained minimization formulation of fine-tuning, we develop the  trainable projected gradient method (TPGM) to learn a distance constraint for each layer of a neural network for robust fine-tuning, which has not been possible with manual hyper-parameter tuning. Our thorough experiments across several pre-trained models and ID/OOD datasets show that TPGM can better preserve the OOD generalization capability of the pre-trained model with minimal effects on ID performance. The optimized constraints exhibit highly interpretable patterns which corroborate existing findings and strengthen the motivation for per-layer constraints. Moreover, our algorithm is general with different possibilities for various components, such as the projection operator, and thereby opens up opportunities for future research.

