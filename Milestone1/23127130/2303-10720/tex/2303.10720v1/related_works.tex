\section{Related Works}

\textbf{Fine-tuning to Boost ID Performance.}  SpotTune~\cite{guo2019spottune} introduces an additional policy network, which outputs a linear interpolation ratio between the pre-trained model ($\theta_0$) and fine-tuned model ($\theta_t$) based on the input. Instead of directly regularizing the weight space, DELTA~\cite{li2019delta} proposes to regularize the output (feature maps) of $\theta_0$ and $\theta_t$. However, SpotTune introduces an additional network and needs to keep both the pre-trained model and fine-tuned model for inference. DELTA specifically regularizes feature maps generated by convolution layers. In this work, we focus more on general methods, which do not increase inference costs and are applicable to a broader range of models. L2-SP~\cite{xuhong2018explicit} proposes an explicit inductive bias for fine-tuning. Specifically, it uses an L2 regularization to minimize the distance between $\theta_0$ and $\theta_t$. Most recently, MARS-SP~\cite{godwin2021simple} adopts the projected gradient method (PGM) to constrain $\theta_t$ within a small sphere centered on the pre-trained model $\theta_0$. MARS-SP has shown great performance against its prior works. However, we find it very sensitive to hyperparameter tuning. Nonetheless, our work is inspired by and improves PGM by 1) incorporating \textit{trainable} weight projection radii for each layer and 2) developing a \textit{bi-level} optimization algorithm to learn them.   

\textbf{Fine-tuning to Improve OOD Generalization.} As the size of the target dataset increases and better architectures are developed, the benefit from pre-training on the target ID performance diminishes~\cite{he2019rethinking}. However, the power of pre-training goes beyond boosting ID performance. A recent work~\cite{wen2021rethinking} finds that using pre-trained models can greatly improve robustness on OOD datasets and uncertainty-related tasks such as confidence calibration~\cite{guo2017calibration} and OOD detection~\cite{devries2018learning}. Moreover, the fine-tuning strategy used also plays an important role in improving OOD generalization. LP-FT~\cite{kumar2022fine} shows that simultaneously fine-tuning the last linear layer and the feature backbone can distort pre-trained features and thus decreases OOD generalization. A simple strategy of linear probing, i.e., training only the classifier layer, followed by fine-tuning the entire network can greatly mitigate this distortion and improve OOD generalization. WISE~\cite{wortsman2022robust} demonstrates impressive OOD generalization gains by linearly interpolating $\theta_0$ and $\theta_t$. However, this strategy only applies to image-text pre-trained models with zero-shot classifiers such as CLIP~\cite{radford2021learning} because WISE requires the model to have \textit{linear connectivity}. In most cases, linear interpolation between two models results in no better performance than random initialization~\cite{frankle2020linear}.

%  Furthermore, WISE requires a zero-shot classifier head, which is only available with an image-text pre-trained model. WISE posts the question on how to search for the interpolation ratio. We will show that TPGM naturally solves this open question, and pushes the performance gain even higher by allowing per-layer interpolation ratios while not sacrificing ID performance. meaning that $\theta_0$ and $\theta_t$ can be linearly interpolated without losing performance

% \textbf{Parameter-Efficient Methods to Improve Generalization.} While parameter-efficient fine-tuning methods aim to minimize the parameters tuned, they have been shown to improve OOD generalization performance as well in NLP applications~\cite{houlsby2019parameter,li2021prefix,xie2021composed,lester2021power,utama2021avoiding,zhou2022learning}. We specifically compare to two recent parameter-efficient methods that only tune the bias terms: Bitfit~\cite{zaken2021bitfit} for Transformers~\cite{vaswani2017attention} and Partial Fusion~\cite{kanavati2021partial} for ResNets~\cite{he2016deep}. 