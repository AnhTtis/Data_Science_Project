\section{Experiments}
\textbf{Overview.} To validate TPGM, we conduct experiments on large-scale datasets using different architectures. The experiments are split into two sections depending on the specific architecture used. In Sec.~\ref{sec:resnet_exp}, we use ResNet~\cite{he2016deep} with a \textit{CLIP pre-trained ResNet50}~\cite{radford2021learning} and an \textit{ImageNet pre-trained MOCO-V3 ResNet50\cite{chen2021empirical}} as the pre-trained models. In Sec.~\ref{sec:transformer_exp}, we use Vision Transformers~\cite{vaswani2017attention} with a \textit{CLIP pre-trained ViT-B model}~\cite{radford2021learning}.


% In the first one, we use a \textit{CLIP pre-trained Transformer model with a zero-shot classifier}~\cite{radford2021learning}. In the second one, we use a \textit{CLIP pre-trained ResNet50 \textbf{without a zero-shot classifier}}~\cite{radford2021learning} \zk{Motivate \textit{why} you use the zero-shot classifier for Transformers and NOT for ReseNets.}and an \textit{ImageNet pre-trained MOCO-V3~\cite{chen2021empirical}} as the pre-trained models. Correspondingly, the non-iterative TPGM is used for the former and iterative TPGM for the latter. \kevin{why do we use non-iterative TPGM for former and iterative one for latter?}

\textbf{Datasets.} For the ResNet experiments, we use DomainNet~\cite{peng2019moment} (0.6M images over 345 classes) as the benchmark. DomainNet has five domains: real, sketch, painting, inforgraph, and clipart. We use the real domain as the ID fine-tuning domain (with held-out test data to test ID performance) and the rest as the OOD domains. For the Transformer experiments, we use ImageNet-1K\cite{deng2009imagenet} as the fine-tuning dataset. For the ID test dataset, we add ImageNet-V2~\cite{recht2019imagenet} in addition to ImageNet-1K.  For the OOD test datasets, we use ImageNet-A~\cite{hendrycks2021natural}, ImageNet-R~\cite{hendrycks2021many}, and ImageNet-S~\cite{wang2019learning}. No OOD data are used during training.

\subsection{Fine-Tuning a Pre-trained ResNet}
\label{sec:resnet_exp}
\input{table/clip_resnet_main}
\input{table/moco_resnet_main}
\input{table/clip_resnet_main_10}

% The previous section demonstrated the effectiveness of non-iterative TPGM, where projection happens at the end of fine-tuning. This is only possible because we can extract a zero-shot classifier from text embeddings~\cite{wortsman2022robust}. \kevin{I don't think this is obvious to the readers. Why was it not possible? This corresponds to the early comment in Sec. 3.3.1 that the motivation of having iterative vs non-iterative is not clear.}
% For more general pre-trained models, when zero-shot classifiers are not available, methods such as WISE~\cite{wortsman2022robust} are not applicable. 
In this section, we compare TPGM to several existing methods using a CLIP pre-trained ResNet50~\cite{radford2021learning} and ImageNet pre-trained MOCO-V3 ResNet50~\cite{chen2021empirical} as initialization. Specifically for TPGM, we use $f_{proj} = 1$, $T_{proj} =1$, meaning that \textit{projection update} and \textit{projection} are activated at every gradient descent step (Alg.~\ref{alg:alg1}). We also use the MARS projection in Eq.~\ref{eq:mars_proj} because we found that MARS projection performs better than L2 projection in this setting (Appendix~\ref{sec:compare_proj}). Moreover, we do not include WISE~\cite{wortsman2022robust} in this comparison because we found that CLIP pre-trained ResNet50 has poor linear connectivity, i.e., linear interpolation results in drastic degradation of performance (Appendix~\ref{sec:resnet_wise}). Therefore, we do not use any zero-shot classifiers for initializing the last linear layer (See sec.~\ref{sec:transformer_exp} for a detailed description of zero-shot classifiers). The recipe for training ResNet is relatively simple. We use the Adam optimizer~\cite{kingma2014adam} with default settings and a batch size of 256. Models are fine-tuned for 50 epochs with a cosine learning rate schedule. The same training recipe is used for all experiments unless otherwise specified. Implementation details are provided in Appendix~\ref{sec:implementation}.


\textbf{TPGM improves OOD robustness without sacrificing ID performance.} We first present the main results on DomainNet using CLIP pre-trained ResNe50. As shown in Tab.~\ref{tab:clip_resnet}, we observe that both L2-SP and LP-FT bring significant improvements to OOD generalization with respect to vanilla FT while matching or surpassing its ID accuracy.  Nevertheless, TPGM brings the most OOD improvement while also surpassing vanilla FT on ID accuracy. We also report results using MOCO-V3 in Tab.~\ref{tab:moco_resnet}. MOCO-V3 is pre-trained on ImageNet-1K (1.2M) consisting of mainly real images, a much smaller and less diverse pre-training data set than CLIP's. Therefore, we see worse OOD generalization results from all methods, compared to using CLIP pre-trained models (Tab.~\ref{tab:clip_resnet}). This indicates that the size and diversity of the pre-training dataset have a huge impact on generalization. Nevertheless, TPGM~\footnote{ TPGM on MocoV3 is the only situation where we found total variation smoothing (see Appendix~\ref{sec:smoothing}) helps with ID performance. Without smoothing, TPGM achieves 81.66 ID and 37.27 Ave. OOD performance.} yields the best OOD performance while matching the best ID performance.

% This is in contrast to the conclusion from the previous section, where these two methods showed a slight degradation of ID accuracy using Transformers. \zk{Make link as to \textit{how} it can do this (e.g. signal of overfitting when optimizing outer loop to validation set?)}. 


\input{figure/resnet_analysis}
\textbf{TPGM adjusts to the size of training data.} As an \textit{automatic} regularization method, TPGM also needs to adjust to different regularization strengths according to the size of the training set. TPGM can avoid over-fitting to a small fine-tuning dataset through the outer minimization loop of the projection parameters on validation data (Eq.~\ref{eq:constrained_min}). In this section, we additionally present results when we reduce the DomainNet-Real data to 10$\%$ of its original size. We follow a similar strategy as in the $100\%$ experiments and sweep different learning rates (for competing methods we sweep their hyperparameters). All models are trained for 150 epochs. In Tab.~\ref{tab:clip_resnet_10}, we observe significant degradation in ID performance across all methods except for PF and TPGM. PF only trains the Batch-norm layers and therefore is less prone to over-fitting. TPGM achieves an even higher ID performance because it learns small projection radii, which project the fine-tuned model closer to the pre-trained model. To see it, we visualize the average distance between the fined-tuned model and the pre-trained model for each residual block using TPGM for both 100$\%$ and $10\%$ data in Fig.~\ref{fig:per_layer_para_resnet}. We observe that 1) lower layers have smaller constraints and higher layers have larger constraints, meaning more freedom to grow, and 2) with only $10\%$ training data, the learned constraints are much smaller than those trained with 100$\%$. 
This behavior explains why TPGM maintains high ID performance and avoids over-fitting with fewer training data because it chooses to rely more on the pre-trained model by enforcing stronger projection.


\subsection{Fine-tuning a Pre-Trained Transformer}
\label{sec:transformer_exp}
 In this section, we compare to existing fine-tuning methods using a CLIP pre-trained ViT-B model. We initialize all models with the pre-trained weights and also the last linear classifier layer with a \textit{zero-shot classifier} extracted from the CLIP text-encoder. Specifically, for TPGM, we use $f_{proj} = T-1$ and $T_{proj} =200$, meaning that projection only happens \textit{once} at the end of fine-tuning. This is possible because CLIP pre-trained ViT-B has been shown to have good linear connectivity~\cite{wortsman2022robust} in contrast to the CLIP pre-trained ResNet (Appendix~\ref{sec:resnet_wise}). Furthermore, we use L2 projection in Eq.~\ref{eq:l2_proj} because we found L2 projection is better than MARS projection in this setting (Appendix~\ref{sec:compare_proj}). Training Transformers~\cite{vaswani2017attention} requires careful tuning of the training recipe to achieve the best results\footnote{Our training recipe yields 84.20 vanilla FT accuracy on ImageNet using a CLIP ViT-B, which is significantly better than prior works, e.g., WISE~\cite{wortsman2022robust} reported 81.3, FT-LP~\cite{radford2021learning} reported 81.7 on the same dataset.}. We follow some common practices in prior works~\cite{touvron2021training,touvron2022deit} to boost performance. We leave implementation details in Appendix~\ref{sec:implementation}.  

\textbf{Extracting a Zero-Shot Classifier.} CLIP has an image-encoder $g(\cdot)$ and a text-encoder $h(\cdot)$, and is capable of zero-shot classification. For example, given an image $x$ and its label space $y\in\mathcal{Y}=\{y_1,...,y_c\}$, zero-shot classification can be done by first inserting the class name $y_i$, e.g., "apple", into a template $c_i$, e.g., "a photo of $\{$apple$\}$" and extracting its text embedding $h(c_i)$, and then computing an inner product,$\langle h(c_i), g(x)\rangle$, between the text embedding and the corresponding image embedding. The maximum value of the inner product over all classes determines the membership of the input. Following the prior work~\cite{wortsman2022robust}, one can stack $h(c_i),\quad \forall i\in\{1,...,c\}$ into a weight matrix $\mathbf{W}_{\text{zero-shot}}$ as a zero-shot classification layer. We use this weight matrix as initialization as well as zero-shot classification.

% \textbf{Reproducing State-of-the-Art Results.} Training Transformers~\cite{vaswani2017attention} requires careful tuning of the training recipe to achieve the best results\footnote{Our training recipe yields 84.20 vanilla FT accuracy on ImageNet using a CLIP ViT-B, which is significantly better than prior works, e.g., WISE~\cite{wortsman2022robust} reported 81.3, FT-LP~\cite{radford2021learning} reported 81.7 on the same bencmark.}. We leave implementation details to the Appendix~\ref{sec:implementation}.  


% We follow some common practices used in prior works~\cite{touvron2021training,touvron2022deit} to boost fine-tuning performance. Note that we use the same training recipe for all methods unless otherwise specified. For example, linear probing performs worse when augmentations are used~\cite{radford2021learning}. Now we will list the techniques used as well as their corresponding hyperparameters in parenthesis. Specifically, we use label-smoothing ($0.1$)~\cite{szegedy2016rethinking}, weight-decay ($0.1$), Mixup ($0.8$)~\cite{zhang2017mixup} and Cutmix ($1.0$)~\cite{yun2019cutmix}. We fine-tune models using the AdamW optimizer~\cite{loshchilov2017decoupled} for 30 epochs with a warm-up period of 5 epochs~\cite{touvron2021training}, per-step cosine decay schedule~\cite{he2022masked} and a batch size of $512$. 


\input{table/imagenet_main}
\textbf{TPGM Improves OOD robustness without sacrificing ID performance.} Now, we present the main benchmark results, accuracy on each of the datasets, and percentage of improvement with respect to the vanilla FT method, in Tab.~\ref{tab:imagenet}. Parameter-efficient methods such as LP and BitFit all improve OOD generalization however at a loss of ID performance. We hypothesize that they help preserve generalization by updating fewer parameters in the network, and therefore maintaining a closer distance to the pre-trained model. On the other hand, the restriction on the function space can result in under-fitting, manifested in lower ID performance. Surprisingly, L2-SP and LP-FT fail to improve either ID or OOD performance. We think this is because the added regularization in L2-SP and the two-stage training procedure in LP-FT are not very compatible with the existing Transformer training recipe. The zero-shot classifier brings significant OOD improvement even though the ID performance is way worse than the FT model. This confirms that CLIP models acquire great generalization capability during pre-training, as also reported by the original paper~\cite{radford2021learning}. TPGM and WISE perform notably better than other methods. We will elaborate on the comparison next. 

\textbf{TPGM outperforms WISE.} The current state-of-the-art method for fine-tuning a pre-trained model with linear connectivity is WISE~\cite{wortsman2022robust}, which linearly interpolates between a fine-tuned model and the pre-trained model with a \textit{single} ratio. For lack of a better heuristic, the paper suggests 0.5 as the interpolation ratio and leaves the research for a better method to determine the mixing ratio as an open question. The comparison between TPGM and WISE comes down to the comparison between optimized per-layer constraints and a hand-tuned single constraint. Therefore, for WISE, we sweep different ratios from 0.1 to 0.9, controlling the distance to the pre-trained model from close to far. For TPGM, to fairly compare to WISE, we put an L2 regularization on the magnitude of the trainable projection parameters with a hyperparameter $\mu$ that controls the strength of regularization. Intuitively, a larger regularization forces the projection radii to be smaller, meaning projecting the fine-tuned model closer to the pre-trained model. We sweep a range of different $\mu$ from $4e^{-3}$ to $0.0$. We refer to this variant as TPGM-C (C for \textbf{c}ontrolled). Note that this L2 regularization is not a hyper-parameter in the algorithm itself.  In Fig.~\ref{fig:tpgm_wise}, we observe a trade-off between the ID performance and the OOD performance for both methods. However, TPGM  clearly outperforms WISE because for the same ID performance, TPGM has better OOD performance and for the same OOD performance, TPGM has better ID performance. This demonstrates the benefits of maintaining per-layer constraints over a single interpolation ratio. We also provide the same experiment and visualization using a CLIP pre-trained ViT-L in Appendix~\ref{sec:vit_l}.
\input{figure/imagenet_analysis}
\input{figure/imagenet_analysis_b}



\textbf{Different layers require different regularization.} Now we take a closer look at the learned TPGM projection radii especially in terms of ``closeness'' to the pre-trained model.  In Fig.~\ref{fig:per_layer_para}, we visualize the average distance from the pre-trained model for each transformer block with three different L2 regularization strengths. We observe that 1) lower layers have smaller projection radii, i.e., they are more tightly constrained whereas higher layers have larger projection radii and therefore more freedom to grow; 2) as the regularization strength on projection radii increases, on average, they become closer to the pre-trained model while still following the previous observation.  Combined with the common belief that lower layers learn more general features and higher layers learn more specialized layers, we hypothesize that lower layers of the pre-trained model are ``closer'' to the ideal model than higher layers. This observation corroborates with our theoretical analysis (Sec.~\ref{sec:theory}) that when the distance between the pre-trained model and the ideal model is small, TPGM favors close projection.




