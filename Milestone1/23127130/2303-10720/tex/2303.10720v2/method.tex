\section{Method}
In the introduction, we motivated the benefit of explicitly maintaining distance constraints between a pre-trained model and a fine-tuned model~\cite{xuhong2018explicit,gouk2020distance}.  However, it is not clear how to search the space of hyper-parameters (distance constraints) \textit{especially} if we want to do this per layer as the search space grows combinatorially with the number of layers. To this end, we pose the search as a bi-level constrained optimization problem in Sec.~\ref{sec:finetune_min} and introduce closed-form projection in Sec.~\ref{sec:pgm}. Then we present the proposed TPGM algorithm in Sec.~\ref{sec:tpgm}. {\color{black}Finally, we theoretically show that the bi-level optimization design enables TPGM to learn different constraints for each layer in Sec.~\ref{sec:theory}.}


\subsection{Fine-tuning as a Bi-level Constrained Problem}
\label{sec:finetune_min}

Machine learning algorithms usually tune hyper-parameters, e.g., learning rate, weight decay, etc., on a \textit{validation} split. Mathematically, this procedure is equivalent to a bi-level minimization problem. Let $(x,y)$ denote a pair of input data and $\mathcal{L}(\cdot)$ denote the task loss function. The minimization problem can be written as  {\color{black}
\begin{align}
\label{eq:id_robustness}
     \min_{\lambda|(x,y)\in\mathcal{D}_{val}} \mathcal{L}(x,y;\argmin_{\theta_t|(x,y)\in\mathcal{D}_{tr}}  \mathcal{L}(x,y;\theta_t,\lambda),\lambda),
\end{align}}
where $\theta_t$ denotes the trainable model weights and $\lambda$ denotes the hyper-parameters such as learning rate. $\mathcal{D}_{tr}$ is the set of training data and $\mathcal{D}_{val}$ is the set of validation data. 


% \min_{\theta_t|(x,y)\in\mathcal{D}_{tr}}
Now, we extend this formulation to fine-tune a pre-trained model. Specifically, it has been shown that maintaining a close \textit{distance} to the pre-trained model improves a model's generalization and robustness~\cite{hendrycks2019using,xuhong2018explicit}. A recent paper~\cite{gouk2020distance} formalizes the concept of maintaining distance as a constrained optimization problem, in which the distance between the new model and the pre-trained model is measured by matrix norms $\|\cdot\|_*$. Mathematically, combined with Eq.~\ref{eq:id_robustness}, we further extend the constrained optimization to a \textit{bi-level constrained}  minimization problem as
\begin{align}
\label{eq:constrained_min}
     \min_{\lambda,\gamma|(x,y)\in\mathcal{D}_{val}} \mathcal{L}(x,y;\argmin_{\theta_t|(x,y)\in\mathcal{D}_{tr}}  \mathcal{L}(x,y;\theta_t,\lambda),\lambda),\\\nonumber \quad\text{s.t.}\quad \|\theta_t-\theta_0\|_*\leq\gamma,
\end{align}
where $\|\theta_t-\theta_0\|_*$ denotes a norm induced distance between the pre-trained model $\theta_0$ and the new model $\theta_t$. Optimizing Eq.~\ref{eq:constrained_min} enforces the model to stay within a distance $\gamma$ from the pre-trained model. 
% after each gradient update step defined by a norm $\|\cdot\|_*$
\subsection{Projected Gradient Method}
\label{sec:pgm}
One method to optimize a constrained problem is \textit{projected gradient method} (PGM)~\cite{iusem2003convergence}. PGM projects the updated model weights to be within the constraint, i.e., $\|\theta_t-\theta_0\|_*\leq\gamma$. However, in general, most projection operations are optimization problems themselves with only a few exceptions having closed from solutions. One example is L2-norm projection $\Pi_{l2}$. Projecting a matrix $\theta_t$ to $\gamma$ distance away, measured by L2 norm, from another matrix $\theta_0$ is a closed form operation as shown in Eq.~\ref{eq:l2_proj}.
\begin{align}
\label{eq:l2_proj}
\Pi_{l2}(\theta_0,\theta_t,\gamma):\Tilde{\theta} = \theta_0 + &\frac{1}{\text{max}\left(1,\frac{\|\theta_t-\theta_0\|_2}{\gamma}\right)}(\theta_t-\theta_0)
\end{align}

The prior work~\cite{gouk2020distance} uses the maximum absolute row sum (MARS) matrix norm because it has a closed form \textit{approximation} that enables fast projection without optimization as well. The MARS approximate projection operator $\Pi_{mars}$ is defined in Eq.~\ref{eq:mars_proj}.
\begin{align}
\label{eq:mars_proj}
\Pi_{mars}(\theta_0,\theta_t,\gamma):\Tilde{\theta} = \theta_0 + &\frac{1}{\text{max}\left(1,\frac{\|\theta_t-\theta_0\|_\infty}{\gamma}\right)}(\theta_t-\theta_0)
\end{align}
$\|\cdot\|_\infty$ denotes the MARS matrix norm, $\|A\|_\infty = \max_j\sum_{i=1}|A_{j,i}|$. Even though we use closed-form projection to avoid additional computation, the projection radius $\gamma$ needs to be pre-determined. Searching for a single weight projection parameter for all layers is already challenging because the scale of $\gamma$ is unknown let alone tailoring a weight projection radius to each layer. In this paper, we do not investigate specific properties of projections, which are orthogonal to our contributions. Therefore, we will benchmark both projections and report the one with better results and leave the comparison to Appendix. 



\subsection{Trainable Projected Gradient Method (TPGM)}
Inspired by the projected gradient method, we propose a \textit{trainable} approach to solve the bi-level constrained problem in Eq.~\ref{eq:constrained_min} by integrating the projection operator in Eq.~\ref{eq:l2_proj} or Eq.~\ref{eq:mars_proj} into the forward pass of a model, through which the weight projection radii $\gamma$ can be learned through backpropagation. Specifically, the algorithm consists of three functions: \textit{model update}, \textit{projection update}, and \textit{projection}. A summary of TPGM is presented in Alg.~\ref{alg:alg1} and details of the \textit{projection update} function are in Alg.~\ref{alg:alg2}. 


% \subsubsection{The TPGM Framework}
\label{sec:tpgm}
\input{algorithm}

\textbf{Model Update.} TPGM first takes an \textit{unconstrained} gradient descent step. Let $\theta_{t+1}$ denote the updated model parameters at the gradient descent step $t$ for $t\geq0$ where $\theta_0$ denotes the pre-trained initialization. This update is calculated on the loss function $\mathcal{L}(x,y;\theta_t)$ where $(x,y)$ are sampled \textit{training} data, i.e., $(x,y)\in\mathcal{D}_{tr}$. This corresponds to a regular gradient descent step in the conventional setting and the inner minimization in Eq.~\ref{eq:constrained_min}. For example, if vanilla SGD is used in this step, then,
\begin{align*}
    \theta_{t+1} = \theta_t-\eta_t\nabla_\theta\mathcal{L}(x,y;\theta_t), \quad (x,y)\in \mathcal{D}_{tr}.
\end{align*}
Any other optimizers, e.g., Adam~\cite{kingma2014adam}, can be used instead.

\textbf{Projection Update.} The \textit{projection update} function optimizes the weight projection parameters $\gamma_t$ iteratively. As shown in Alg.~\ref{alg:alg2}, the optimization loops for $T_{proj}$ steps. In Alg.~\ref{alg:alg2}, we use SGD as an example for clarity. Any other optimizer can be used. Specifically, using $\theta_{t+1}$ and the closed form projection operation in Eq.~\ref{eq:l2_proj} (or Eq.~\ref{eq:mars_proj}), we construct a projected model $\Tilde{\theta}$ with a \textit{trainable} projection parameter $\gamma_t$ for $t\geq0$ for each layer, where $\gamma_0$ is initialized to a small value $\epsilon$. Then, we optimize the projection parameters using the loss function $\mathcal{L}(x,y;\gamma_t)$ where $(x,y)$ are sampled \textit{validation} data, i.e., $(x,y)\in\mathcal{D}_{val}$. Crucially, in this step, only the weight projection parameters $\gamma_t$ are updated while the updated model $\theta_{t+1}$ remains \textit{frozen}. In other words, no gradients of the model are calculated on the validation data. This is important to avoid contamination of the validation data. The projection update function corresponds to the outer minimization of the constrained problem in Eq.~\ref{eq:constrained_min}.



\textbf{Projection.} Finally, after a new set of projection parameters $\gamma_t$ is updated, we apply the learned projection radii to the updated model $\theta_{t+1}$ using Eq.~\ref{eq:l2_proj} (or Eq.~\ref{eq:mars_proj}). This amounts to enforcing the constraint $\|\theta-\theta_0\|_*\leq\gamma$ in Eq~\ref{eq:constrained_min}. The \textit{projection update} and \textit{projection} functions can be called frequently controlled by a hyperparameter ($f_{proj}$ in Alg.~\ref{alg:alg1}). We will show that for certain pre-trained models, it is sufficient to only call these two functions once at the end of the training, i.e., when $f_{proj}=T-1$ (Sec.~\ref{sec:transformer_exp}). Moreover, we found that in a few cases, TPGM could lead to under-fitting because of its iterative nature. However, the problem can be easily mitigated with total variation smoothing~\cite{chen2010adaptive,condat2013direct}. Since we only observed this in one setting in our experiments, we defer the discussion to Appendix~\ref{sec:smoothing}.

We summarize TPGM in Alg.~\ref{alg:alg1}.  Intuitively, TPGM maintains a set of weight projection parameters for each layer of a neural network and updates them. The projection parameters control how much ``freedom'' each layer has to grow.  As we will observe later, when fine-tuning a model, layers close to the input generally require smaller changes than layers close to the output. This property helps preserve generalization capabilities obtained by the pre-trained model. TPGM inevitably adds some computation overhead. We provide additional discussion on it in Appendix~\ref{sec:computation}.