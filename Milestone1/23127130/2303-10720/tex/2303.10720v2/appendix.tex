\subsection{Appendix}


\subsection{Proof of Theorem 1}
\label{sec:proof}
We provide complete proof of the main theorem. We will first reiterate the notations used in the main paper. 

\textbf{Problem Setup.}  Let $x\in \mathbb{R}^{d}$ denote an ID data and the corresponding label $y$ is generated by a \textit{ground truth} linear model $\theta_*\in \mathbb{R}^d $, i.e., $y=\theta_*^Tx$.  To construct the training set, we sample $n$ training data, where $n<d$, and stack the sampled data into a data matrix $\mathbf{X}_{tr}\in\mathbb{R}^{d\times n}$. Accordingly, the labels form a vector $\mathbf{Y}_{tr}=\mathbf{X}_{tr}^T\theta_*\in\mathbb{R}^{n}$. The 
\textit{training} goal is to minimize the \textit{empirical} loss.
\begin{align}
    \label{eq:training_obj_ap}
\mathcal{L}(\mathbf{X}_{tr},\mathbf{Y}_{tr};\theta)=\|\mathbf{X}_{tr}^T\theta-\mathbf{Y}_{tr}\|_2
\end{align}

Note that this forms an \textit{over-parameterized} linear system, i.e., there are more parameters than equations, because $n<d$. This is similar to how modern neural networks are over-parameterized with respect to the data.


\textbf{Complementary Decomposition using SVD.} For the analysis, we make an independence assumption on the data matrix $\mathbf{X}_{tr}$. This assumption exists for notation simplicity and can be relaxed easily.
\begin{assumption}
\label{assmp:independence_ap}
Let the $n$ training data be linearly independent. The following SVD exists for the data matrix $\mathbf{X}_{tr}$.
\begin{align*}
\mathbf{X}_{tr} = \mathbf{U}\mathbf{\Sigma} \mathbf{V}^T,\quad \mathbf{U}\in\mathbb{R}^{d\times n},\mathbf{\Sigma}\in\mathbb{R}^{n\times n}, \mathbf{V}\in\mathbb{R}^{n\times n}.
\end{align*}
\end{assumption}

Consequently, we can decompose any vector $x\in\mathbb{R}^{d}$ into two components, $x=\mathbf{U}\tau + \mathbf{U}_{\bot}\tau_{\bot}$, where $\mathbf{U}$ is the basis for the span of training samples, $\mathbf{U}_{\bot}\in\mathbb{R}^{d\times (d-n)}$ is the basis for the complementary subspace, and $\tau\in\mathbb{R}^{n}$, $\tau_{\bot}\in\mathbb{R}^{d-n}$ are the corresponding coordinates. There are infinitely many solutions to Eq.~\ref{eq:training_obj_ap} because this is an over-parameterized system.  The classic result states that,
\begin{align}
\label{eq:least_sqr}
    \theta = \mathbf{U}\mathbf{\Sigma}^{-1}\mathbf{V}^T\mathbf{Y}_{tr}+\mathbf{U}_\bot\beta_\bot,
\end{align}
where $\beta_\bot\in\mathbb{R}^{d-n}$ can be any vector. We denote a \textit{projected model} as $\Tilde{\theta} = \theta_0 + \alpha (\theta - \theta_0)$ (obtained using Eq.~\ref{eq:l2_proj} or Eq.~\ref{eq:mars_proj}), where  $\theta$ is one minimizer of Eq.~\ref{eq:training_obj_ap}, $\theta_0$ is the pre-trained model and $0\leq\alpha\leq1$ is the projection ratio. 

To quantify the effects of projection $\alpha$, we can look at the average performance of the projected model $\Tilde{\theta}$ on test data. Consequently, we investigate the \textit{expected} loss of the projected model over the entire data space. 
\begin{align}
    \label{eq:expected_obj_ap}
    \mathbb{E}[\mathcal{L}(x,y;\Tilde{\theta})]= \mathbb{E}\left[\left\|{\Tilde{\theta}}^Tx-y \right\|_2\right]
\end{align}
We now provide a detailed proof of Theorem~\ref{thm:thm_1} in the main paper.
{\color{black}We first prove two lemmas.
\begin{lemma}
\label{lma:lemma1}
$\|(\theta-\theta_*)^T\mathbf{U}\tau\|_2 = 0$.
\end{lemma}

\begin{proof}
To show it, we use the decomposition in Eq.~\ref{eq:least_sqr}.
\begin{align*}
    \|(\theta-\theta_*)^T\mathbf{U}\tau\|_2& = \| \mathbf{U\Sigma}^{-1}\mathbf{V}^T\mathbf{Y}_{tr}+\mathbf{U}_\bot\beta_\bot-\theta_*)^T\mathbf{U}\tau\|_2\\\nonumber
    & = \|(\mathbf{U\Sigma}^{-1}\mathbf{V}^T\mathbf{Y}_{tr}-\theta_*)^T\mathbf{U}\tau\|_2\\\nonumber
    & = \|(\mathbf{U\Sigma}^{-1}\mathbf{V}^T\mathbf{X}_{tr}^T\theta_*-\theta_*)^T\mathbf{U}\tau\|_2\\\nonumber
    & = \|(\mathbf{U\Sigma}^{-1}\mathbf{V}^T(\mathbf{ U\Sigma V}^T)^T\theta_*-\theta_*)^T\mathbf{U}\tau\|_2\\\nonumber
    &= 0 
\end{align*}
\end{proof}

\begin{lemma}
\label{lma:lemma2}
$\|\mathbf{U}\tau\|_2 \leq \|\tau\|_2$ and  $\|\mathbf{U}_\bot\tau_\bot\|_2 \leq\|\tau_\bot\|_2$.
\end{lemma}
\begin{proof}
    We first invoke the definition of matrix norm, 
    \begin{align*}
        \|\mathbf{U}\|_2 = \sup_{\tau\neq0} \frac{\|\mathbf{U}\tau\|_2 }{\|\tau\|_2}
    \end{align*}

From the definition, it is easy to see that 

\begin{align*}
    \|\mathbf{U}\tau\|_2 \leq \|\mathbf{U}\|_2\|\tau\|_2.
\end{align*}

Now recall that both $\mathbf{U}\in\mathbb{R}^{d\times n}$ and $\mathbf{U}_{\bot}\in\mathbb{R}^{d\times (d-n)}$ are orthonormal matrices. Therefore, using the property of L2 matrix norm,  
\begin{align*}
    \|\mathbf{U}\|_2 = \sqrt{\lambda_{max}(\mathbf{U}^T\mathbf{U})} = \sigma_{max}(\mathbf{U})=1
\end{align*}
where $\lambda_{max}(\cdot)$ and $\sigma_{max}(\cdot)$ denote the largest eigenvalue and singular value respectively. Therefore,
\begin{align*}
    \|\mathbf{U}\tau\|_2 \leq \|\tau\|_2.
\end{align*}
The same analysis extends to $\mathbf{U}_\bot,\tau_\bot$.
\end{proof}

Next, we proceed with the proof of the main theorem.
\begin{proof}
    \begin{align*}
      \mathcal{L}(x,y;\Tilde{\theta}) &= \left\|{\Tilde{\theta}}^Tx-y \right\|_2\\\nonumber
    & = \left\|(\theta_0 + \alpha (\theta - \theta_0))^Tx-\theta_*^Tx \right\|_2\\\nonumber
    & =  \|(\theta_0 + \alpha (\theta - \theta_0)-\theta_*)^T\mathbf{U}\tau +\\\nonumber
    &  (\theta_0 + \alpha (\theta - \theta_0)-\theta_*)^T\mathbf{U}_\bot\tau_\bot \|_2\\\nonumber
    &\leq  \underbrace{\| ((1-\alpha) (\theta_0-\theta_*) + \alpha({\theta}-\theta_*))^T\mathbf{U}\tau\|_2}_{A}+\\\nonumber
    &  \underbrace{\|(\theta_0-\theta_*)^T\mathbf{U}_\bot\tau_\bot\|_2}_{B}  + \underbrace{\| \alpha({\theta}-\theta_0)^T\mathbf{U}_\bot\tau_\bot\|_2}_{C}\\\nonumber
    \end{align*}
We use triangle inequality for the last inequality.
We can now bound $A$ using Lemma~\ref{lma:lemma1}, Cauchy-Schwarz inequality and Lemma~\ref{lma:lemma2} as
\begin{align*}
    \| ((1-\alpha) &(\theta_0-\theta_*) + \alpha({\theta}-\theta_*))^T\mathbf{U}\tau\|_2 \\\nonumber
    & = (1-\alpha)\| (\theta_0-\theta_*)^T\mathbf{U}\tau \|_2\\\nonumber
    & \leq (1-\alpha)\|(\theta_0-\theta_*)\|_2\|\mathbf{U}\tau\|_2\\\nonumber
    & \leq (1-\alpha)\|(\theta_0-\theta_*)\|_2\|\tau\|_2.
\end{align*}

Similarly, we can bound $B$ using Cauchy-Schwarz inequality and Lemma~\ref{lma:lemma2} as 
\begin{align*}
    \|(\theta_0-\theta_*)^T\mathbf{U}_\bot\tau_\bot\|_2 &\leq \|\theta_0-\theta_*\|_2\|\mathbf{U}_\bot\tau_\bot\|_2\\\nonumber
    & \leq \|\theta_0-\theta_*\|_2\|\tau_\bot\|_2,
\end{align*}

and bound $C$ as,
\begin{align*}
    \| \alpha({\theta}-\theta_0)^T\mathbf{U}_\bot\tau_\bot\|_2 &\leq \| \alpha({\theta}-\theta_0)\|_2\mathbf{U}_\bot\tau_\bot\|_2\\\nonumber
    & \leq  \| \alpha({\theta}-\theta_0)\|_2\|\tau_\bot\|_2.
\end{align*}
 Now, plug everything back. We arrive at the final result, 
 \begin{align*}
     \mathcal{L}(x,y;\Tilde{\theta})\leq (1-\alpha)\epsilon\|\tau\|_2 + (\epsilon + \alpha \|\theta-\theta_0\|_2)\|\tau_\bot\|_2
 \end{align*}
 where $\epsilon = \|(\theta_0-\theta_*)\|_2$.
\end{proof}
}


\subsection{Group Based Total Variation Smoothing}
\label{sec:smoothing}
Because of the iterative and incremental nature, the vanilla TPGM algorithm is a \textit{greedy} algorithm, meaning that it judges the \textit{immediate} benefit of the current updates to the model weights. If the current updates are not consistent with the validation data, they will be removed by projection, i.e., the projection radii will not increase to accommodate the new changes. Consequently, projection radii learned by TPGM could be overly \textit{conservative} and lead to underfitting because gradient updates are stochastic, whose benefits may only show up in the long run. Empirically, we found TPGM results in under-fitting in some cases, i.e., slightly lower ID performance. To mitigate this side-effect of iterative optimization, we propose a group-based total variation (TV) smoothing for the projection parameters. TV is a common technique to improve smoothness in image denoising~\cite{chen2010adaptive} and general signal processing~\cite{condat2013direct}.  We propose to utilize TV regularization to enforce a heuristic on the optimization of $\gamma$: \textit{projection ratios of layers in the same group should be similar to each other}. Specifically, modern neural network architectures such as ResNet~\cite{he2016deep} and Transformer~\cite{vaswani2017attention} are modular and stacked with groups (blocks). It is easy to identify unique groups in each architecture and assign layers to each one of them. Therefore, let $\mathcal{G}=\{g_i|i=0,...,M\}$ be the set of unique groups in a neural network. The loss function that we optimize for the projection parameters is updated as the following,
\begin{align}
    \mathcal{L}_\gamma = \mathcal{L}(x,y;\gamma_t) + \mu \sum_{g_i\in\mathcal{G}} \sum_{i\in g_i}|\alpha_i - \alpha_{i-1}|
\end{align}
where $\mu$ is a hyperparameter requiring tuning. 


\subsection{Implementation}
\label{sec:implementation}

In Alg.~\ref{alg:alg2}, the \textit{projection update} function has its own optimizer. In our implementation,  we use the Adam~\cite{kingma2014adam} optimizer because of its capability of adapting learning rate. Even though this introduces other hyperparameters, we find the same set of hyperparameters worked well for all experiments. Specifically, we use the default settings and a constant base learning rate of $\zeta=1e-2$.

\textbf{ResNet experiments (Sec.~\ref{sec:resnet_exp}).} We list all the compared methods and their method-specific tuning to reproduce our results.

\begin{itemize}
    \item\textbf{ Vanilla Fine-Tuning (FT)}: We fine-tune all layers and sweep five learning rates (CLIP best $\eta_0=1e-3$, MOCO best $\eta_0=5e-2$).
    \item\textbf{ Linear Probing (LP)}: We only fine-tune the head classifier and sweep five learning rates (CLIP best $\eta_0=1e-1$, MOCO best $\eta_0=1e-1$).
    \item \textbf{Partial Fusion (PF)}~\cite{kanavati2021partial}: We fine-tune all the batch-norm layers and the head classifier, and sweep five learning rates (CLIP best $\eta_0=1e-2$, MOCO best $\eta_0=5e-2$).
    \item \textbf{L2-SP}~\cite{xuhong2018explicit}: We add L2-SP regularization, use the best-validated learning rate from FT, and sweep five three regularization hyperparameters (CLIP best $\mu:1e-2$, MOCO best $\mu:1e-3$).
    \item \textbf{MARS-SP}~\cite{gouk2020distance}: We add MARS projection (Eq~\ref{eq:mars_proj}), use the best-validated learning rate from FT, and sweep five three projection hyperparameters (CLIP best $\mu=64$, MOCO best $\mu=16$).
    \item \textbf{LP-FT}~\cite{kumar2022fine}: We first LP for 25 epochs, using the best LP learning rate, and FT for another 25 epochs, sweeping five learning rates (CLIP best $\eta_0=1e-3$, MOCO best $\eta_0=5e-2$).
    \item \textbf{TPGM}: We learn per-layer L2 projection radii incrementally, sweeping five learning rates (Eq.~\ref{eq:l2_proj}) (CLIP best $\eta_0=1e-2$, MOCO best w/o smoothing $\eta_0=1e-2$, MOCO w/ smoothing best :$\eta_0=4e-2$ and $\mu=0.1$).
\end{itemize}


\textbf{Transformer Experiments (Sec.~\ref{sec:transformer_exp}).} We follow some common practices used in prior works~\cite{touvron2021training,touvron2022deit} to boost fine-tuning performance. Note that we use the same training recipe for all methods unless otherwise specified. For example, linear probing performs worse when augmentations are used~\cite{radford2021learning}. Now we will list the techniques used as well as their corresponding hyperparameters in parenthesis. Specifically, we use label-smoothing ($0.1$)~\cite{szegedy2016rethinking}, weight-decay ($0.1$), Mixup ($0.8$)~\cite{zhang2017mixup} and Cutmix ($1.0$)~\cite{yun2019cutmix}. We fine-tune models using the AdamW optimizer~\cite{loshchilov2017decoupled} for 30 epochs with a warm-up period of 5 epochs~\cite{touvron2021training}, per-step cosine decay schedule~\cite{he2022masked} and a batch size of $512$. We list all the compared methods and their method-specific tuning to reproduce our results. 
\begin{itemize}
    \item\textbf{ Vanilla Fine-Tuning (FT)}: We fine-tune all layers and sweep three learning rate $\eta_0\in\{1e-5,2e-5,3e-5\}$.
    \item\textbf{ Linear Probing (LP)}: We only fine-tune the head classifier and sweep three learning rates $\eta_0\in\{5e-2,1e-2,5e-3\}$. We don't use any data augmentations (e.g., label-smoothing, Mixup and Cutmix) as they decrease LP performance.
    \item \textbf{BitFit}~\cite{zaken2021bitfit}: We fine-tune all the bias terms and the head classifier and sweep three learning rate $\eta_0\in\{5e-2,1e-2,5e-3\}$.
    \item \textbf{L2-SP}~\cite{xuhong2018explicit}: We add L2-SP regularization, use the best-validated learning rate from FT, and sweep three three regularization hyperparameters $\mu\in\{1e-5,1e-4,1e-3\}$.
    \item \textbf{LP-FT}~\cite{kumar2022fine}: We first LP for 15 epochs, sweeping three learning rates $\eta_0\in\{5e-2,1e-2,5e-3\}$, and FT the best-validated model for another 15 epochs, sweeping three learning rate  $\eta_0\in\{1e-5,2e-5,3e-5\}$.
    \item \textbf{Zero-Shot}~\cite{radford2021learning}: We run an inference with the pre-trained CLIP model with the extracted zero-shot classifier.
    \item \textbf{WISE}~\cite{wortsman2022robust}: We linearly interpolate the best validated FT model and the pre-trained model with a ratio of $0.5$.
    \item \textbf{TPGM}: We learn per-layer projection radii between the best validated FT model and the pre-trained model using the MARS projection (Eq.~\ref{eq:mars_proj}). 
\end{itemize}

\subsection{CLIP Pre-trained ViT-L on ImageNet}
\label{sec:vit_l}
In Sec.~\ref{sec:transformer_exp}, we presented fine-tuning results on ImageNet using CLIP pre-trained ViT-b. In this section, we conduct the same experiments with CLIP pre-trained ViT-L. As we noticed in the ViT-b experiments, WISE and TPGM perform much better than other competitors, so we focus on the comparison between the two here. We first present tabulated results in Tab.~\ref{tab:imagenet_vit_l}. We observe that TPGM improves both ID and OOD performance over vanilla FT. To compare fairly with WISE, we introduced TPGM-C (Sec.~\ref{sec:transformer_exp}), which uses an L2 regularization on the learned projection radii to control the distance to the pre-trained model. With proper regularization, TPGM-C outperforms WISE on both ID and OOD performance. We also provide a figure of ID vs. OOD performance with different WISE interpolation ratios and different TPGM-C regularization strengths in Fig.~\ref{fig:tpgm_wise_large}. We observe the same trend as in the ViT-b experiments (Sec.~\ref{sec:transformer_exp}): at the same ID performance, TPGM has better OOD performance.
\input{table/imagenet_vit_l.tex}
\input{figure/imagenet_analysis_l.tex}


\subsection{Comparisons between TPGM-L2 and TPGM-MARS}
\label{sec:compare_proj}
In the main paper, we presented two possible projections: L2 projection (Eq.~\ref{eq:l2_proj}) and MARS projection (Eq.~\ref{eq:mars_proj}). Both projections provide closed-form solutions. We can use either of them in TPGM. In this section, we present comparisons between the two.

\input{table/L2_vs_MARS_resnet}
\textbf{ResNet Experiments on DomainNet.} For ResNet experiment in Sec.~\ref{sec:resnet_exp}, we use a CLIP pre-trained ResNet50 and an ImageNet pre-trained ResNet50. For TPGM, we use $f_{proj}=1$ and $T_{proj} = 1$. In Tab.~\ref{tab:l2_vs_mars_resnet}, we compare the performance of TPGM using MARS and L2 projections on DomainNet-Real with $100\%$ of its data. We observe that in this setting MARS performs better than L2 projection. 


\input{figure/l2_vs_mars_transformer}
\textbf{Transformer Experiments on ImageNet.} For Transformer experiments in Sec.~\ref{sec:transformer_exp}, we use a CLIP pre-trained ViT-B. For TPGM, we use $f_{proj}=T-1$ and $T_{proj} = 200$. Following the main paper, we add L2 regularization to the projection radii and sweep a range of values from $4e-3$ to $1e-4$. In Fig.~\ref{fig:l2_vs_mars_transformer}, we compare the performance of TPGM using MARS and L2 projections on ImageNet ID and OOD datasets. We observe that L2 projection always outperforms MARS projection in this setting.

\subsection{WISE for CLIP Pre-trained ResNet}
\label{sec:resnet_wise}
\input{figure/resnet_wise}
The prior work~\cite{wortsman2022robust} and our experiments in Sec.~\ref{sec:transformer_exp} verified that CLIP pre-trained Transformers have very good linear connectivity. This means that when linearly interpolating between the pre-trained model and a fine-tuned model, the output does not degrade much. In this case, we observe significantly improved OOD generalization with minimal ID performance loss. However, the same trend is not observed when switching the architrave to ResNet50. Similar to the prior work~\cite{wortsman2022robust}, we extract a zero-shot classifier for DomainNet classes using a CLIP pre-trained ResNet50 and conduct the same linear interpolation ratio sweeping as in the main paper. In Fig.~\ref{fig:resnet_wise}, we plot ID performance against the OOD performance of WISE with different ratios, the pre-trained (zero-shot) model, and the vanilla fine-tuned model. Notably, we observe a significant drop in performance when interpolating between the pre-trained model and a fine-tuned model. This shows that CLIP pre-trained ResNet does not enjoy the same linear connectivity as its Transformer counterpart.


\subsection{Smoothing Comparison using MOCO-V3}
\label{sec:moco_smoothing}
\input{table/moco_tv_smoothing}
In the main paper, we found that training with MOCO-V3 pre-trained ResNet50 on DomainNet can benefit from total variation (TV) smoothing (Appendix~\ref{sec:smoothing}). Here we show a detailed comparison between TPGM with and without smoothing for this particular setting in Tab.~\ref{tab:moco_smoothing}. We observe that TPGM without smoothing achieves the best OOD performance however with a slight decrease in ID performance compared to vanilla FT. This might be caused by the conservative nature of TPGM as discussed in Appendix~\ref{sec:smoothing}. When TV smoothing is added, we observe that TPGM brings improvement to both ID and OOD performance over vanilla FT. 


\subsection{Computation Overhead}
\label{sec:computation}
TPGM inevitably adds some computation overhead to a vanilla fine-tuning pipeline (though not inference). The majority of computation cost comes in Alg.~\ref{alg:alg2}, where the algorithm needs to conduct gradient updates on the projection parameters. While this overhead is negligible when we set $f_{freq} = T-1$, i.e., \textit{projection update} is only called once at the end of training as in the Transformer experiments (Sec.~\ref{sec:transformer_exp}), the overhead increases when $f_{freq} = 1$. In our ResNet experiments (Sec.~\ref{sec:resnet_exp}), to decrease computation cost, we only the update projection parameters once during each call, i.e, $T_{proj}=1$. Qualitatively, we see an increase of training time from $\sim29$ hours to $\sim34$ hours when TPGM is added, a $\sim17\%$ increase. However, this increase can be justified by the fact that manually searching for per-layer constraints can be intractable. 

