\section{Introduction}
  
Improving out-of-distribution (OOD) robustness such that a vision model can be trusted reliably across a variety of conditions beyond the in-distribution (ID) training data has been a central research topic in deep learning. For example, domain adaptation~\cite{you2019universal,wang2018deep}, domain generalization~\cite{zhou2022domain,muandet2013domain}, and out-of-distribution calibration~\cite{tian2021geometric} are examples of related fields. More recently, large pre-trained models, such as CLIP~\cite{radford2021learning} (pre-trained on 400M image-text pairs), have demonstrated large gains in OOD robustness, thanks to the ever-increasing amount of pre-training data as well as effective architectures and optimization methods. However, fine-tuning such models to other tasks generally results in worse OOD generalization as the model over-fits to the new data and \textit{forgets} the pre-trained features~\cite{radford2021learning}.  \textit{A natural goal is to preserve the generalization capability acquired by the pre-trained model when fine-tuning it to a downstream task.} 

\input{figure/TPGM_diagram}
A recent empirical study shows that aggressive fine-tuning strategies such as using a large learning rate can decrease OOD robustness~\cite{wortsman2022robust}. We hypothesize that the \textit{forgetting} of the generalization capability of the pre-trained model in the course of fine-tuning is due to \textit{unconstrained} optimization on the new training data~\cite{xuhong2018explicit}. This conjecture is not surprising, because several prior works, even though they did not focus on OOD robustness, have discovered that encouraging a close distance to the pre-trained model weights can improve ID generalization, i.e., avoiding over-fitting to the training data~\cite{xuhong2018explicit,gouk2020distance}. Similarly, if suitable distance constraints are enforced, we expect the model to behave more like the pre-trained model and thus retain more of its generalization capability. The question is \textit{where} to enforce distance constraints and \textit{how} to optimize them? 

Several works have demonstrated the importance of treating each layer differently during fine-tuning. For example, a new work~\cite{lee2022surgical} discovers that selectively fine-tuning a subset of layers can lead to improved robustness to distribution shift. Another work~\cite{shen2021partial} shows that optimizing a different learning rate for each layer is beneficial for few-shot learning. Therefore, we propose to enforce a different constraint for each layer. However, existing works either use manually crafted heuristics or expensive hyper-parameter search, which prevent them from scaling up to large datasets and neural networks. For example, the prior work~\cite{shen2021partial} using evolutionary search for hyper-parameters can only scale up to a custom 6-layer ConvNet and a ResNet-12 for few-shot learning. The computation and time for searching hyper-parameters become increasingly infeasible for larger datasets, let alone scaling up the combinatorial search space to all layers. For example, a ViT-base~\cite{vaswani2017attention} model has 154 trainable parameter groups including both weights, biases, and embeddings\footnote{For example, for a linear  layer $y=\mathbf{W}x+\mathbf{b}$, we need to use separate distance constraints for $\mathbf{W}$ and $\mathbf{b}$.}. This leads to a search space with more than $10^{45}$ combinations even if we allow only two choices per constraint parameter, which makes the search prohibitively expensive.

\looseness=-1 To solve this problem, we propose a trainable projected gradient method (TPGM) to support layer-wise regularization optimization. Specifically, TPGM adopts \textit{trainable} weight projection constraints $\gamma$, which we refer to as \textit{projection radii}, and incorporates them in the forward pass of the main model to optimize. Intuitively, as shown in Fig.~\ref{fig:clip_resnet_constraints}, TPGM maintains a set of weight projection radii $\gamma$ i.e., the distance between the pre-trained model ($\theta_0$) and the current fine-tuned model ($\theta_t$), for each layer of a neural network and updates them. The projection radii control how much "freedom" each layer has to grow. For example, if the model weights increase outside of the norm ball defined by $\gamma$ and $\|\cdot\|$, the projection operator will project them back to be within the constraints. To learn the weight projection radii in a principled manner, we propose to use alternating optimization between the model weights and the projection radii, motivated by formulating fine-tuning as a \textit{bi-level} constrained problem (Sec.~\ref{sec:finetune_min}). {\color{black}We theoretically show that the bi-level formulation could explain the behavior of TPGM (Sec.~\ref{sec:theory}). }



% \textbf{Experiments on ImageNet and DomainNet.}
Empirically, we conduct thorough experiments on large-scale datasets, DomainNet~\cite{peng2019moment} and ImageNet~\cite{deng2009imagenet}, using different architectures. Under the premise of preserving ID performance, i.e., OOD robustness should not come at the expense of worse ID accuracy, TPGM outperforms existing approaches with little effort for hyper-parameter tuning. Further analysis of the learned projection radii reveals that lower layers (layers closer to the input) in a network require stronger regularization while higher layers (layers closer to the output) need more flexibility. This observation is in line with the common belief that lower layers learn more general features while higher layers specialize to each dataset~\cite{neyshabur2020being,raghu2019transfusion,yosinski2014transferable,wortsman2022robust}. Therefore, when conducting transfer learning such as fine-tuning, we need to treat each layer differently. Our contributions are summarized below. 

\begin{itemize}
    \item \looseness=-1 We propose a trainable projected gradient method (TPGM) for fine-tuning to automatically learn the distance constraints for each layer in a neural network during fine-tuning.  
    \item We conduct experiments on different datasets and architectures to show significantly improved OOD generalization while matching ID performance. 
    
    \item {\color{black}We theoretically study TPGM using linear models to show that bi-level optimization could explain the regularization capability of TPGM.} 
\end{itemize}