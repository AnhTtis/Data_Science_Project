\subsection{Bi-level Optimization}
\label{sec:theory}
 {\color{black}Following a common strategy in studying transfer learning~\cite{wu2020understanding,tripuraneni2020theory,du2020few,xie2020n,wortsman2022robust}, we theoretically study TPGM using linear models and explain why optimizing the bi-level problem in Eq.~\ref{eq:constrained_min} could enable the regularization capability of TPGM.}


\textbf{Problem Setup.}  Let $x\in \mathbb{R}^{d}$ denote an ID data and the corresponding label $y$ is generated by a \textit{ground truth} linear model $\theta_*\in \mathbb{R}^d $, i.e., $y=\theta_*^Tx$.  To construct the training set, we sample $n$ training data, where $n<d$, and stack the sampled data into a data matrix $\mathbf{X}_{tr}\in\mathbb{R}^{d\times n}$. Accordingly, the labels form a vector $\mathbf{Y}_{tr}=\mathbf{X}_{tr}^T\theta_*\in\mathbb{R}^{n}$. The 
\textit{training} goal is to minimize the \textit{empirical} loss.
\begin{align}
    \label{eq:training_obj}
\mathcal{L}(\mathbf{X}_{tr},\mathbf{Y}_{tr};\theta)=\|\mathbf{X}_{tr}^T\theta-\mathbf{Y}_{tr}\|_2
\end{align}

Note that this forms an \textit{over-parameterized} linear system, i.e., there are more parameters than equations, because $n<d$. This is similar to how modern neural networks are over-parameterized with respect to the data.

{\color{black}
\textbf{Complementary Decomposition using SVD.} For the analysis, we make an independence assumption on the data matrix $\mathbf{X}_{tr}$. This assumption exists for notation simplicity and can be relaxed easily.
\begin{assumption}
\label{assmp:independence}
Let the $n$ training data be linearly independent. The following SVD exists for the data matrix $\mathbf{X}_{tr}$.
\begin{align*}
\mathbf{X}_{tr} = \mathbf{U}\mathbf{\Sigma} \mathbf{V}^T,\quad \mathbf{U}\in\mathbb{R}^{d\times n},\mathbf{\Sigma}\in\mathbb{R}^{n\times n}, \mathbf{V}\in\mathbb{R}^{n\times n}.
\end{align*}
\end{assumption}



Consequently, we can decompose any vector $x\in\mathbb{R}^{d}$ into two components, $x=\mathbf{U}\tau + \mathbf{U}_{\bot}\tau_{\bot}$, where $\mathbf{U}$ is the basis for the span of training samples, $\mathbf{U}_{\bot}\in\mathbb{R}^{d\times (d-n)}$ is the basis for the complementary subspace, and $\tau\in\mathbb{R}^{n}$, $\tau_{\bot}\in\mathbb{R}^{d-n}$ are the corresponding coordinates. There are infinitely many solutions to Eq.~\ref{eq:training_obj} because this is an over-parameterized system.  

\begin{definition}
We denote a \textit{projected model} as $\Tilde{\theta} = \theta_0 + \alpha (\theta - \theta_0)$ (obtained using Eq.~\ref{eq:l2_proj} or Eq.~\ref{eq:mars_proj}), where  $\theta$ is one minimizer of Eq.~\ref{eq:training_obj}, $\theta_0$ is the pre-trained model and $0\leq\alpha\leq1$ is the projection ratio. 
\end{definition}

To quantify the effects of projection $\alpha$, we can look at the average performance of the projected model $\Tilde{\theta}$ on test data. Consequently, we investigate the \textit{expected} loss of the projected model over the entire data space. 
\begin{align}
    \label{eq:expected_obj}
    \mathbb{E}[\mathcal{L}(x,y;\Tilde{\theta})]= \mathbb{E}\left[\left\|{\Tilde{\theta}}^Tx-y \right\|_2\right]
\end{align}
We provide the following theorem to shed light on how projection affects the \textit{expected} loss and what it depends on.}

\begin{theorem}
\label{thm:thm_1}
Let Assumption~\ref{assmp:independence} hold, the expected loss of $\Tilde{\theta}$ in Eq.~\ref{eq:expected_obj} is upper bounded as the following,
\begin{align}
\label{eq:expected_upper}
    \mathbb{E}\left[\left\|\Tilde{\theta}^Tx-y \right\|_2\right] & \leq \underbrace{(1-\alpha)\epsilon{\tau}}_{in-span} + \underbrace{\left(\epsilon+\alpha \left\|{\theta}-\theta_0 \right\|_2\right){\tau}_{\bot}}_{out-span},
\end{align}

\noindent where $\epsilon = \left\|{\theta}_0-\theta_* \right\|_2$ and ${\tau} \doteq \mathbb{E}[\|\tau\|_2]$, ${\tau}_\bot \doteq \mathbb{E}[\|\tau_\bot\|_2]$. A complete proof is provided in Appendix~\ref{sec:proof}.
\end{theorem}
\looseness=-1  The upper bound in Thm.~\ref{thm:thm_1} has two components, risk due to components in the training data span (in-span) and risk due to components in the complementary subspace (out-span). To minimize the expected loss, one will expect $\alpha$ to be dependent on the value of $\epsilon$. Recall that the quantity $\epsilon$ is the distance between the pre-trained model and the ground truth model and can be viewed as a measure of how ``good" the pre-trained model is. {\color{black}Therefore, we expect two types of behaviors from $\alpha$ depending on $\epsilon$: 
\begin{itemize}
    \item \textbf{When $\epsilon$ is small, $\alpha$ needs to be smaller} to minimize the second component, meaning stronger projection.
    \item \textbf{When $\epsilon$ is large}, \textbf{$\alpha$ needs to be larger} to minimize the first component, meaning weaker projection.
\end{itemize}
}

\looseness=-1 {\color{black} The theorem indicates that if we optimize the projected model $\tilde{\theta}$ on a \textit{separate} batch of data, different from the data the model gradients are calculated on, the projection ratio $\alpha$ will seek to balance between fitting the training data (in-span) and generalizing to new data (out-span). For example, when $\epsilon$ is small, i.e., the pre-trained model is close to the optimal model, the formulation encourages stronger projection. 
} 

Furthermore, as prior works have found that lower layers tend to learn more general features while higher layers specialize to a specific dataset, $\epsilon$ is likely to be smaller for the lower layers and larger for higher layers because pre-trained models likely have learned very good low-level general features~\cite{neyshabur2020being,raghu2019transfusion,yosinski2014transferable,wortsman2022robust}. This offers one explanation of why TPGM automatically learns different constraints for each layer. Therefore, we hypothesize that optimizing the projection radii on a dataset sampled \textit{separately} from the training data, e.g., the validation dataset, is essential to learning different constraints for each layer. 

