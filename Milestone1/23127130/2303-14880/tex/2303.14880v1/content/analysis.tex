\section{ANTICIPATED USE CASES}
\label{sec::analysis}

MuSoHu's large body of socially compliant human navigation data with multi-modal robotic perception collected in natural public spaces in the wild provide opportunities for many future research directions. 

\subsection{Learning Social Robot Navigation}
The primary purpose of MuSoHu is to provide a large corpus of training data for mobile robots to learn socially compliant navigation behaviors. As we demonstrate in our preliminary experiments, robot navigation behaviors similar to the human behaviors in MuSoHu can be learned end-to-end using Behavior Cloning. Other imitation learning methods, such as IRL, can utilize MuSoHu to learn a socially compliant cost function for downstream navigation planners~\cite{kretzschmar2016socially}. 

The replicability of our sensor suite makes collecting social human navigation data very easy. We intend the sensor suite to be replicated by different research groups to collect data in different countries worldwide. An even larger and also more diverse corpus of data opens up orthogonal research directions that are not currently possible. For example, new social robot navigation systems can be developed that are culturally dependent, i.e., the way a mobile robot moves can be fit into different culture contexts. For example, imagine a contact-tolerant culture where pedestrians are comfortable with walking very closely to each other vs. a contact-averse culture where people prefer to keep distance. 

\subsection{Imitation Learning with Various Constraints}
One potential challenge, in other words, opportunity for future research, is how to address the difference in human and robot navigation. Human navigation is based on legged locomotion, while most mobile robots are wheeled or tracked. Different motion morphologies caused by such an embodiment mismatch \cite{hudson2022skeletal} may require extra care to be taken during learning. Transfer learning techniques \cite{weiss2016survey} may provide one promising avenue to leverage the full potential of MuSoHu. In addition to the different motion morphologies, despite our choice of sensor modalities to align with robot perception, viewpoint mismatch still exists: to avoid occluding the 3D LiDAR view, it is mounted on top of the helmet, which is higher than most robots' LiDAR position; all perceptual data are subject to the effect of walking gait cycles, e.g., the cyclic motion along the vertical axis, which does not exist for most mobile robots. Therefore, imitation learning from mismatched observation techniques \cite{karnan2022voila} need to be investigated to address the perceptual mismatch between MuSoHu and mobile robots. 

\subsection{Studying Social Human and Robot Navigation}
One question being debated frequently is \emph{should roboticists build robots to navigate in public spaces in the same way as humans?} Our MuSoHu dataset, along with its future extensions in different countries worldwide, and \textsc{scand} provide a way to investigate related problems. Assuming the navigation behaviors in MuSoHu and \textsc{scand} are the optimal way of human and robot social navigation in public spaces respectively, we can analyze both datasets to see whether the human and robot behaviors are the same, similar, or completely different. Another way is to build social robot navigation systems with the data in MuSoHu and \textsc{scand} and evaluate the learned social navigation behaviors with standardized protocols and metrics~\cite{pirk2022protocol} to see wheter there is any difference between the two and if yes which way is preferred by people that interact with the robots. 

\subsection{Real-to-Sim Transfer for Social Navigation}
Creating high-fidelity social navigation simulation environments has been a focus of social robot navigation researchers~\cite{tsoi2020sean, holtz2022socialgym}. A realistic simulator that can induce real-world human-robot social interactions that conform with the underlying unwritten social norms will facilitate social robot navigation research on multiple fronts, such as reinforcement learning based on simulated trial and error, large scale validation and evaluation of new social navigation systems before their real-world deployment, and objective and repeatable benchmark and comparison among multiple social navigation systems. However, existing social navigation simulators rely on simplified human-robot interaction models, e.g., the Social Force Model~\cite{helbing1995social} or ORCA~\cite{van2011reciprocal}. Such a sim-to-real gap~\cite{liang2021crowd} may cause problems when the navigation systems learned, evaluated, or compared in simulation are deployed in the real world.

The MuSohu dataset provides another alternative and promising avenue toward shrinking such a sim-to-real gap through real-to-sim transfer to improve social navigation simulation. The data collected in the wild in MuSoHu enable researchers to synthesize natural, real-world, human-robot social navigation interactions in simulation. Approaches can be developed to learn such interaction models from the natural interactions in MuSoHu, which can be used to control simulated agents, robots or humans, in a high-fidelity simulator.

\subsection{Investigating Robot Morphology for Social Navigation}
Human-human social navigation interactions embody a large set of interaction modalities, which are frequently present in MuSoHu. For example, in addition to avoiding other humans as moving obstacles, humans use gaze, head movement, and body posture to communicate navigation intentions in crowded spaces; they use body or natural language to express their navigation mindset or context (e.g., they are in a rush and apologize for being less polite). Most current mobile robots, however, do not possess such capabilities to communicate their navigation intentions and contexts in an efficient manner. Analyzing the human-human social navigation interaction modalities in MuSoHu will shed light on what other robot morphology may be useful to facilitate efficient social navigation, such as adding a robot head with gaze~\cite{hart2020using}, turn signals~\cite{unhelkar2015human}, or gait features (for legged robots)~\cite{kruse2014evaluating} to disambiguate navigation intentions, or adding voice~\cite{medicherla2007human} to signal the urgency of the robot's navigation task. 