\begin{abstract}
Humans are well-adept at navigating public spaces shared with others, where current autonomous mobile robots still struggle:
while safely and efficiently reaching their goals, humans communicate their intentions and conform to unwritten social norms on a daily basis; 
conversely, robots become clumsy in those daily social scenarios, getting stuck in dense crowds, surprising nearby pedestrians, or even causing collisions. 
While recent research on robot learning has shown promises in data-driven social robot navigation, good-quality training data is still difficult to acquire through either trial and error or expert demonstrations. 
In this work, we propose to utilize the body of rich, widely available, social human navigation data in many natural human-inhabited public spaces for robots to learn similar, human-like, socially compliant navigation behaviors. 
To be specific, we design an open-source egocentric data collection sensor suite wearable by walking humans to provide multi-modal robot perception data; we collect a large-scale ($\sim$50 km, 10 hours, 150 trials, 7 humans) dataset in a variety of public spaces which contain numerous natural social navigation interactions; we analyze our dataset, demonstrate its usability, and point out future research directions and use cases.\footnote{Website: \url{https://cs.gmu.edu/~xiao/Research/MuSoHu/}} 

% With the rise of autonomous mobile robots that work in human-populated settings (e.g., home cleaning robots and food delivery robots on public sidewalks), robot platforms are expected to interact more with humans and our social cues. Although most existing navigation planners are very efficient, it is still a challenge for researchers to make robots behave in a socially complaint manner. There is no doubt that humans are experts in social navigation and the amount of experience we produce is enormous on daily basis. Recent efforts for social navigation have done with many datasets collected by autonomous and tethered robots. Despite promises from learning methods (e.g., Imitation Learning, Reinforcement Learning) to tackle navigation by learning directly from expert demonstrations, human-level social navigation is still out of reach. In addition, collecting such data from public robots is a huge hurdle as robots are less socially complaint and less accessible than humans. To overcome these downsides, we propose an open source, first-person-view, social human navigation data collection sensor suite that is reproducible which lowers the barrier to start collecting navigation data for robot learning. We also introduce \textsc{WALKER}, a large-scale, egocentric dataset of human demonstrations for {\em Context-Aware Social Navigation}. Our dataset contains X hours, Y trajectories, Z miles of socially compliant, human demonstrations that comprises multi-modal data streams in both indoor and outdoor environments. 


% Heilmeier Catechism for abstract https://www.darpa.mil/work-with-us/heilmeier-catechism

% Objectives with no jargon
% This paper presents a Learning from Demonstration framework, which autonomously and adaptively learns online parameters for mobile robot navigation in complex environments.
%This paper presents a new technique through which non-roboticists can improve existing navigation systems by simply providing a demonstration.
%
% How is it done today
%Current practices require expert knowledge of the underlying navigation system to tune navigation parameters, and are therefore less friendly to novice users.
%Furthermore, it is typically assumed that one single set of parameters will work everywhere in the deployment environment, which usually leads to compromised performance in particular regions.
%
% What's new
%In this work, human demonstration from non-experts is leveraged to \emph{autonomously} and \emph{adaptively} adjust navigation parameters so that full potential of the navigation stack could be achieved anywhere in complex environments.
%No assumptions are made regarding the expertise of the demonstrator (e.g., understanding the navigation) or the specifics of the existing navigation algorithm (e.g., being differentiable).
%Improved navigational performance is demonstrated by applying the proposed framework on two example navigation algorithms with two physical robots.
%
% Who cares
%With the proposed framework, parameter tuning becomes less of a challenge for non-roboticists and existing navigation systems become adaptive to complex environments. Moreover, our experiments show that it can even outperform the demonstrator or systems tuned by expert users.

%Robustly navigate a robot in complex environments often requires hand-tuning the navigation parameters, which can become tedious with complex navigation systems like ROS. In this work, we present a generic framework for automatically selecting the navigation parameter based on sensory input information. In particular, we do not put any assumptions on the underlying navigation stack of interest except that we can sample from it. 
%To achieve this, we provide a training pipeline that includes three stages with some help from a human: 1) we first ask the human to label different regions of training environments such that there is a specific optimal parameter for those regions sharing the same label; 2) we then use an offline black-box optimization method to find a near-optimal solutions for each set of regions of interest; 3) We learn a mapping from local sensory inputs to the parameter category. By doing so, when we deploy the robot in a new environment consists of similar regions, the robot can automatically and dynamically adjust its navigation parameters based on its sensory inputs, i.e. the LIDAR inputs.
\end{abstract}