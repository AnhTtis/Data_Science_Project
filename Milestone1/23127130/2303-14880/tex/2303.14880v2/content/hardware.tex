\section{SENSOR SUITE}
\label{sec::hardware}
We design and make publicly available a data collection device, which is wearable by a human walking in public spaces and provides multi-modal perceptual streams that are commonly available on mobile robot platforms.\footnote{\scriptsize\url{https://github.com/RobotiXX/MuSoHu-data-collection}} We also process the raw data to extract human navigation behaviors, i.e., the paths and actions taken by the human demonstrator to navigate through social spaces. 

To be specific, our data collection sensor suite is equipped with a 3D LiDAR, a stereo and depth camera with built-in IMU, a microphone array to provide ambient sound, and a 360\textdegree~camera that offers spherical view of the environment. All the sensors are mounted to a helmet via open-sourced hardware to capture egocentric data of the demonstrator during social navigation. To stream and store real-time social human navigation data, all sensors are connected to a laptop carried by the demonstrator with wired connections (Fig. \ref{fig::helmet} middle). 

\paragraph{3D LiDAR}
As most mobile robots use LiDARs as a reliable sensor to acquire accurate and robust geometric information about the environment, we include a 3D LiDAR to capture such information around the human demonstrator. Considering the different heights of the mounting locations (on robot vs. on our helmet), we use a 3D LiDAR to collect 3D point clouds, which can be converted to 2D scans at different heights if necessary. 
We choose a Velodyne Puck VLP-16 for our sensor suite, which has a range of 100 meters and generates up to 600,000 points/second, across a 360\textdegree~horizontal and 30\textdegree~vertical field of view. The 3D LiDAR is mounted on the top of the helmet to record spatial measurements of the surrounding. 

\paragraph{Stereo and Depth Camera}
RGB cameras provide visual and semantic information of the environment. In addition to the geometric information provided by the LiDAR, semantics also plays a vital role in social navigation interactions. For example, humans use gesture, gaze, and body posture to explicitly or implicitly convey navigational intentions and facilitate interactions. Those behaviors can be used to understand the intentions of other people sharing the same space but are difficult to capture with 3D LiDAR alone. For our sensor suite, we choose Stereolabs ZED 2, a stereo camera with depth sensing and a built-in IMU (see below for more details), considering its compact form factor and efficient power consumption (in contrast to other RGB-D cameras that require a separate power supply, ZED 2 can be efficiently powered by the same USB cable for data transmission). The camera is positioned in the front of the helmet, with the optical axis pointing forward. The wide 120Â° field of view captures interesting social interactions happening in front of and from the sides of the human demonstrator. 

\paragraph{IMU} 
Many mobile robots are also equipped with IMUs to measure linear accelerations and rotational speeds. 
Therefore, we also utilize the built-in IMU from the ZED 2 camera and record their raw measurements. 
It is worth to note that due to the difference between walking humans and wheeled or tracked robots that drive, the IMU readings collected in our dataset may be significantly different than those from such types of mobile robots, especially the acceleration along the vertical axis. 
We posit that to leverage the IMU data in MuSoHu, special techniques such as transfer learning \cite{weiss2016survey} may be necessary. 

\paragraph{Odometry / Actions}
Similar to \textsc{scand}, we collect visual-inertia odometry provided by the ZED 2 camera. Such positional odometry provides learning data of navigation path and can be utilized to learn robot global planners. 
Different than \textsc{scand}, in which the robot navigation actions can be directly recorded as teleoperation commands, our data collection hardware does not have access to such actions, i.e., how the human demonstrator walks. Therefore, we extract linear and angular velocities from the positional odometry using the difference between two consecutive odometry frames.  

\paragraph{360\textdegree~Camera}
In addition to the forward facing stereo and depth camera, we also collect 360\textdegree~RGB video to provide better situational awareness of the surrounding and include all possible sensory information that can be
provided by active pan-tilt cameras onboard many mobile robot platforms. 
% As we are building the dataset for future research, we don't want MuSoHu to be limited by the popular sensors for current robot projects.
We use a Kodak Pixpro Orbit360 4K VR Camera to collect 360\textdegree~images. The camera has a very compact form factor with two lenses integrated in one camera body to provide spherical 360\textdegree~view. Note that due to software limitations the camera's webcam mode does not allow both lenses to stream live video to the laptop, so we save the spherical 360\textdegree~view from both lenses to an SD card in the camera. 

\paragraph{Microphone Array}
Although not commonly used for navigation tasks, microphones are available on many mobile robot platforms, e.g., for verbal communications. Furthermore, recent research has started to investigate using sound for navigation \cite{chen2021structure}. Considering the extra information provided by this different perception modality, we also include a microphone array, a Seeed Studio ReSpeaker Mic Array v2.0, to collect ambient sound during social human navigation. 
