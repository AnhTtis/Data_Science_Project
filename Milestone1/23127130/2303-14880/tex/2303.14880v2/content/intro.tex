\section{INTRODUCTION}
\label{sec::introduction}

Social navigation is the capability of an autonomous agent to navigate in a way such that it not only moves toward its goal but also takes other agents' objective into consideration. Most humans are proficient at such a task, smoothly navigating many public spaces shared with others on a daily basis: humans form lanes or groups among crowds, use gaze, head movement, and body posture to communicate navigation intentions, wait in line to enter congested areas, or give way to others who are in a rush. 
With an increasing amount of autonomous mobile robots being deployed in public spaces \cite{scout, dilligent}, those robots are also expected to navigate among humans in a similar, human-like, socially compliant manner. 


\begin{figure}[t]
  \centering
  \includegraphics[width=1\columnwidth]{figure/helmet_data_collection.jpg}
  \caption{Data collection in natural human-inhabited public spaces with the open-source sensor suite including 3D LiDAR, stereo and depth camera, IMU, microphone array, and 360\textdegree~camera.}
  \label{fig::helmet}
\end{figure}

However, the autonomous navigation performance of these mobile robots is still far from satisfactory. Despite extensive robotics effort to create efficient and collision-free autonomous navigation systems, we still witness the ``frozen robot'' problem in dense crowds and robots moving against upcoming foot traffic or cutting too close to moving humans. Unfortunately, due to such deficiencies, there is increasing fear about the public adoption and even safety of humans around these robots \cite{no_scout, no_starship}. The current lack of safe and socially compliant navigation systems still presents a major hurdle preventing service robots being widely adopted. 

One avenue toward socially compliant robot navigation is using machine learning for robots to learn the variety of unwritten social norms, for which traditional cost functions are hard to design. For example, Reinforcement Learning (RL) \cite{karnan2022voila} uses trial-and-error experiences while Imitation Learning (IL) \cite{tai2018socially} requires expert demonstrations. However, both of these learning paradigms require an extensive amount of training data, which is difficult to acquire: RL in the real world is extremely expensive due to the limited availability of robots, while RL in simulation requires a good model of social navigation interactions of humans, which are what roboticists are trying to create in the first place; IL requires demonstration datasets collected on robot platforms, mostly through expensive human teleoperation at scale \cite{karnan2022scand}. 

Considering the goal of creating socially compliant robot navigation and the availability of many humans that excel at such a task, this work leverages the easily accessible social human navigation data in public spaces for mobile robots to learn from. 
To be specific, we first present an open source, first-person-view, social human navigation data collection sensor suite that can be worn on the head of a walking human and provide easy access to a large body of readily available, high-quality, natural social navigation data in the wild for robot learning, as shown in Fig. \ref{fig::helmet}. Our design includes a set of different robotic sensors: a 3D Light Detection and Ranging (LiDAR) sensor, stereo and depth camera, Inertia Measurement Unit (IMU), microphone array, and 360\textdegree~camera. We open-source our design and software so the sensor suite can be easily replicated and used to collect social human navigation data in different places. 
Second, with the new data collection suite,  we introduce our Multi-modal Social Human Navigation dataset (MuSoHu): a large-scale, egocentric, multi-modal, and context-aware dataset of human demonstrations of social navigation. At the point when this paper is written, MuSoHu contains approximately 20 hours, 300 trajectories, 100 kilometers of socially compliant navigation demonstrations collected by 13 human demonstrators that comprise multi-modal data streams from different sensors, in both indoor and outdoor environments---within the George Mason University campus and the Washington DC metropolitan area. We also provide annotations of interesting social interaction events and of the navigation contexts (i.e., ``casual'', ``neutral'', and ``rush'') for each of the trials.  Third, we present analysis in terms of human and robot social navigation and point out future research directions and anticipated use cases of our dataset. 




