\section{DATASET}
\label{sec::dataset}

\begin{figure*}
  \centering
  \includegraphics[width=1\textwidth]{figure/scenes-min.jpg}
  \caption{Three example data frames in Old Town Alexandria, VA, Springfield Towncenter, VA, and National Mall, Washington DC. 360\textdegree~ view (top), 3D LiDAR point cloud (bottom left), and depth image (bottom right) are shown for each data frame. }
  \label{fig::scenes}
  \vspace{-10pt}
\end{figure*}

The sensor suite described in Sec. \ref{sec::hardware} is designed to be easily replicable by any research group and to collect data worldwide. But we collect an initial Multi-modal Social Human navigation dataset (MuSoHu) on the George Mason University campus and in the Washington DC metropolitan area (Fig. \ref{fig::scenes}).\footnote{\url{https://dataverse.orc.gmu.edu/dataset.xhtml?persistentId=doi:10.13021/orc2020/HZI4LJ}}

\subsection{Data Collection Procedure}
To collect multi-modal, socially compliant, human-level navigation demonstrations to learn future robot navigation, seven human demonstrators wear the sensor suite helmet and navigate to predefined goals in public spaces in a socially compliant manner. We choose navigation scenarios with frequent social interactions in various indoor and outdoor environments at different time periods (e.g., after class or during weekends). The sensor suite's superior portability (i.e., only a helmet and a laptop) also allows us to record portions of MuSoHu in other settings in the Washington DC Metropolitan Area, including Fairfax, Arlington, and Springfield in Virginia and the National Mall in DC. 
Notably, for a trajectory at a certain location at the same time period, in many cases, we record three trials to capture three navigation contexts, i.e., \emph{casual}, \emph{neutral}, and \emph{rush}, in which walking speed and safety distance from others may vary, in order to encourage different social navigation interactions based on different contexts. We intend such context awareness in MuSoHu to be useful for future studies on context-aware social navigation, e.g., social compliance when facing someone who is about to be late for a class is different than that when facing someone who is taking a casual afternoon stroll in the park. 

For each trajectory, all sensor data are collected using the Robot Operating System (\textsc{ros}) Bag functionality, except the 360\textdegree~camera, which does not allow data streaming of both built-in cameras to provide spherical 360\textdegree~view to \textsc{ros}. Therefore, we store the 360\textdegree~video on an SD card and provide synchronization using a movie clapboard. 


\subsection{Dataset Analyses}
\subsubsection{Labeled Annotations of Social Interactions}
MuSoHu includes a list of textual tags for each trajectory that describe the different social interactions that occur along the path. 
We expand beyond the tags from \textsc{scand} and the full list of 17 predefined labels can be found in Table \ref{tag_table} (with five new tags in bold font). 

\begin{table}[!h]
\centering
\caption{Descriptions of Label Tags  Contained in MuSoHu.}
\begin{tabular}{>{\centering\arraybackslash}m{0.1\textwidth}>{\centering\arraybackslash}m{0.25\textwidth}>{\centering\arraybackslash}m{0.05\textwidth}}
%  \hline
\toprule
% \textbf{Tag} & \textbf{Description} \\
 \textbf{Tag} & \textbf{Description} & \textbf{\# Tags}\\
 \toprule
%  \hline
 Against Traffic & Navigating against oncoming traffic & 210\\ 
 \midrule
%  \hline
 With Traffic & Navigating with oncoming traffic & 170\\
 \midrule
%  \hline
 Street Crossing & Crossing across a street & 120\\
 \midrule
%  \hline
 Overtaking & Overtaking a person or groups of people  & 100\\
 \midrule
%  \hline
  Sidewalk & Navigating on a sidewalk  & 160\\
  \midrule
%  \hline
  Passing Conversational Groups & Navigating past a group of 2 or more people that are talking amongst themselves  & 94\\
  \midrule
%  \hline
  Blind Corner & Navigating past a corner where the human cannot see the other side & 90\\
  \midrule
%  \hline
  Narrow Doorway & Navigating through a doorway where the human opens or waits for others to open the door & 45\\
  \midrule
%  \hline
  Crossing Stationary Queue & Walking across a line of people  & 50\\
  \midrule
%  \hline
  Stairs & Walking up and/or down stairs & 30\\
  \midrule
%  \hline
  Vehicle Interaction & Navigating around a vehicle & 26\\
  \midrule
%  \hline
 Navigating Through Large Crowds & Navigating among large unstructured crowds & 45\\ %[1ex] 
%  \hline
 \midrule
%  \hline
 \textbf{Elevator Ride} & Navigating to, waiting inside, and exiting an elevator & 15\\ %[1ex] 
  \midrule
%  \hline
 \textbf{Escalator Ride} & Navigating to and riding an escalator & 6\\ %[1ex] 
  \midrule
 \textbf{Waiting in Line} & Waiting in Line to enter congested areas & 5\\ %[1ex] 
  \midrule
%  \hline
 \textbf{Time: Day} & Navigation during day time & 150\\ %[1ex] 
%  \hline
  \midrule
%  \hline
\textbf{Time: Night} & Navigation during night time & 40\\ %[1ex] 
\bottomrule
\end{tabular}
\label{tag_table} 
\vspace{-10pt}
\end{table}

\subsubsection{Example Data Frames}
In Fig. \ref{fig::example}, we show the corresponding linear and angular velocities (filtered by Savitzky-Golay filter to smooth out high frequency noises caused by walking gait) and navigation path taken by the human demonstrator in the three scenarios shown in Fig. \ref{fig::scenes}. In the first scenario, the demonstrator navigates around a right corner and avoids an upcoming family; in the second scenario, the demonstrator makes a 90\textdegree~right-hand turn, while avoiding people in an indoor food court; in the third scenario, the demonstrator dodges (right-left-right) a dense crowd during a right-hand turn. Both linear and angular velocities and navigation path provide learning signals for mobile robots. 

\definecolor{pathgreen}{rgb}{0.098, 1, 0} 
\begin{figure}[t]
  \centering
  \includegraphics[width=1\columnwidth]{figure/path_v_w.jpg}
  \caption{Linear (\textcolor{blue}{Blue}) and Angular (\textcolor{red}{Red}) Velocities and Navigation Path (\textcolor{pathgreen}{Green}) Taken by the Human Demonstrator. }
  \label{fig::example}
  \vspace{-11pt}
\end{figure}

\subsubsection{Proof-of-Concept Usage}
We use a small subset of MuSoHu data (ten navigation trials) to train a Behavior Cloning policy that maps from raw LiDAR input to linear and angular velocity (Fig. \ref{fig::example}). The learned policy is deployed on two physical robots, an AgileX Hunter SE (an Ackermann steering wheeled vehicle) and a Unitree Go1 (a quadruped robot), both of which exhibit collision avoidance behavior learned from MuSoHu (Fig. \ref{fig::robots}). 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\columnwidth]{figure/robots.jpg}
  \caption{Learned Obstacle Avoidance Behavior from MuSoHu.}
  \label{fig::robots}
\end{figure}