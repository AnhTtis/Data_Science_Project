\section{RELATED WORK}
\label{sec::related_work}
In this section, we review related work in social robot navigation and learning from human datasets. 

\subsection{Social Robot Navigation}

To organically integrate service robots into the fabric of our society, these robots must be capable of moving in human-inhabited spaces in a socially compliant manner. One difficulty in creating such socially compliant navigation systems is to hand-craft appropriate rules or cost functions to cope with unwritten social norms in public spaces~\cite{mirsky2021prevention}. Therefore, researchers have sought help from machine learning and aimed at \emph{learning} socially compliant navigation behaviors in a data-driven manner~\cite{xiao2022motion, baghaei2022deep}. 

RL has shown success in learning a variety of behaviors from simulated trial-and-error experiences~\cite{chen2017socially, xu2021applr, xu2023benchmarking}. However, the high fidelity of simulated social interactions required by RL for social navigation poses its own challenges and requires a good understanding and  then analytical representation of the unwritten social norms to create such simulated interactions, which is the difficulty in social navigation in the first place. Additionally, the reward function in RL needs to be carefully-designed but can still be brittle~\cite{knox2023reward}. 

To address such issues, IL~\cite{bojarski2016end, Nazeri2021} utilizes expert demonstrations to learn socially compliant navigation behaviors~\cite{xiao2022learning, xiao2020appld}. Kretzschmar et al.~\cite{kretzschmar2016socially} has proposed Inverse Reinforcement Learning (IRL) to learn the reward function from demonstrations for social navigation policies. Behavior Cloning~\cite{bojarski2016end, Nazeri2021} has treated the social navigation problem as supervised learning and regressed to an end-to-end motion policy that maps from perception to actions. However, to facilitate IL, a large corpus of socially compliant navigation demonstration data is essential. For example, the Socially Compliant Navigation Dataset (\textsc{scand})~\cite{karnan2022scand} is a recent effort to provide social robot navigation behaviors demonstrated by human teleoperation. 



\subsection{Learning from Human Datasets}
\textsc{scand}~\cite{karnan2022scand} is a recent dataset that aims at tackling the challenges of socially compliant robot navigation. \textsc{scand} includes socially compliant, human teleoperated robot navigation demonstrations in indoor and outdoor environments on The University of Texas at Austin campus. Using \textsc{scand}, researchers have shown that IL policies can be trained end-to-end for socially-aware global and local planners for robot navigation. However, \textsc{scand} requires a significant amount of cost and effort to set up and deploy the robot platforms in the wild and to collect large-scale human-teleoperated robot navigation demonstrations to cover the plethora of interesting social interactions in public spaces. Furthermore, how people react differently to a teleoperated mobile robot followed by a human operator is also unclear. 

Considering the difficulty in acquiring large-scale real-world data, researchers have also looked into utilizing recorded videos of human activities in the wild. For example Ego4D~\cite{grauman2022ego4d} is an egocentric video dataset, which offers daily-life activity video of different scenarios (house-hold, outdoor, workplace, leisure, etc.) captured by different humans wearing cameras from different locations worldwide. 
Ego4D offers a solution to the scalability of datasets by introducing a standard and wearable design so many people can collect data in real-world, daily settings from different parts of the world. However, Ego4D is not specifically designed for robotics (hence the lack of common robot sensors and perception like LiDAR, depth camera, IMU, and odometry), so it is difficult for mobile robots to directly learn socially compliant navigation behaviors from the raw video feed in Ego4D.

Inspired by the pros and cons of both \textsc{scand} and Ego4D, we introduce a wearable data collection sensor suite specifically designed to provide data to enable social robot navigation. It allows us to collect social human navigation data from the perspective of a suite of multi-modal robotic sensors in our daily life with a small setup overhead (i.e., with a wearable helmet). We provide a large-scale social human navigation dataset, which can be easily extended in the future by robotics researchers all around the world, show that human-like social robot navigation behaviors can be learned through such a dataset, and point out future research directions and anticipated use cases of our dataset. For other robot navigation datasets which are less relevant to our work compared to \textsc{scand} and Ego4D, we refer the readers to Table I in the \textsc{scand} paper \cite{karnan2022scand}. 