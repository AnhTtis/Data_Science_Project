

\begin{figure}
    \centering
\includegraphics[width=.95\linewidth]{figs/illu.pdf}
    \vspace{-5pt}
    \caption{Given a point at $\p(t)$ and a \emph{backward} warping field $\bwdw$, we want to compute the 3D scene flow by predicting the next position of the point at time $t'$. We achieve this by deriving the velocity field $v$ as a differentiable function of $\bwdw$, and then perform time integration.}
    \label{fig:illu}
\end{figure}


\section{Supervise deformable NeRF by flow}
\noindent\textbf{Problem setup.} We concern the problem of fitting the deformable NeRF given a monocular video input. We precomputed the camera intrinsics and extrinsics using off-the-shelf structure-from-motion methods \eg Colmap~\cite{colmap}. Optical flows between neighboring frames $o_{t\rightarrow t\pm \nabla t}$ are also computed using RAFT~\cite{raft}. Then we want to find optimal parameters for the backward deformation field $\bwdw$ and radiance field $f$ such that the synthesised images $c_t'$ and optical flow maps $o_{t\pm t'}'$ match the input video frames $c_t$ and precomputed optical flow maps $o_{t\pm t'}$,
\begin{equation}
\resizebox{0.9\hsize}{!}{$
   \min \sum_{t} \left[ \underbrace{\|c_t' - c_t\|_2^2}_{\text{\small{image loss}}} + \beta \sum_{t'\in\{t \pm \Delta t\}} \underbrace{\|M_{t\rightarrow t'} \odot ( o_{t\rightarrow t'}' - o_{t\rightarrow t'})\|_1}_{\text{\small{optical flow loss}}} \right]$}
  \label{eq:train_obj}
\end{equation}
To prevent errors in the precomputed optical flow maps from misleading the reconstruction, we use binary masks $M_{t\rightarrow t'}$ to turn off losses for pixels which fail the forward-backward flow consistency test. Moreover, we follow the trick proposed by Li \etal~\cite{flow-fields} to gradually decay the weighting $\beta$ during optimization, so that the reconstruction is able to correct mistakes of the input optical flows.

The key question is then how to synthesize optical flows $o_{t\rightarrow t'}'$ from $\bwdw$ and $f$?  

\begin{figure}
    \centering
    \includegraphics[width=.95\linewidth]{figs/illu2.pdf}
    \vspace{-5pt}
    \caption{Illustration of the invertibility for \emph{backward} deformation field $\bwdw$. Modeled by coordinate-based MLP, $\bwdw(\p;t)$ is not invertible on the whole input domain. But local homeomorphism exists. we can have a bijective mapping $\bwdwu(\p;t)$ from an open set $U$ to another open set $V$ in the canonical space. Although $\bwdwu^{-1}(\p_c;t)$ exists locally from $V$ to $U$, but it is not possible to analytically derive it in closed form. Fortunately, inverting $\bwdwu$ is not needed for predicting scene flow due to equation (\ref{eq:velocity}). }
    \label{fig:inv_func_th}
\end{figure}

\subsection{Velocity fields from $\bwdw (p;t)$}
Velocity fields $v(\p;t):\mathbb{R}^3\times \mathbb{R} \rightarrow \mathbb{R}^3$ describe the velocity of an object if placed at position $\p$ in the world coordinate at time $t$. 
Velocity fields is straightforward to compute if the \emph{forward} deformation field $\fwdw(\p_c;t):\mathbb{R}^3\times \mathbb{R} \rightarrow \mathbb{R}^3$ exists, \ie:
\begin{equation}
    v(\p;t) = \frac{\partial \fwdw}{\partial t}\bigg|_{(\bwdw(\p;t);t)}.
\end{equation}
However, this intuitive solution is flawed if $\fwdw$ is not strictly an inverse function of $\bwdw$. Unfortunately having an invertible $\bwdw$ is theoretically unwarranted since most deformation representations such as neural networks and  blend skinnings are not bijective.

Since seeking a \emph{global} inverse function of $\bwdw$ on the whole domain is impractical, we propose to consider local regions of $\bwdw$'s domain where \emph{local} inverse function may exist. And based on the inverse function theorem, we find that we can analytically compute $v(\p;t)$ for any positions $\p$ without actually inverting $\bwdw$, as long as $\bwdw$ is bijective in an open set includes $\p$. This leads to the main theoretical result of the paper.

\noindent\textbf{Proposition.} If the warp Jacobian matrix $\mathbf{J}_\p (\p,t) = [\frac{\partial \bwdw(\p;t)}{\partial\p}]$ is non-singular at some position $\p$ at time $t$, and there exists an open set including $\p$ where $\bwdw$ is continuously differentiable, then the velocity at $(\p, t)$ is computed as:
\begin{equation}
    v(\p;t) = - \mathbf{J}_\p^{-1}(\p,t) \frac{\partial \bwdw (\p;t)}{\partial t}.
\label{eq:velocity}
\end{equation}
\begin{proof} First, we upgrade $\bwdw$ to have the same input and output dimension, \ie $\phi(\p, t) = (\bwdw(\p;t), t)$. Then given the assumptions made by the proposition, and according to the inverse function theorem~\cite{munkres2018analysis}, $\phi$ is a local diffeomorphism. In other words, there is some open set $U$ containing $(\p,t)$ and an open set $V$ containing $\phi(\p,t)$ such that $\phi_U := \phi: U\rightarrow V$ has a continuous and differentiable inverse $\phi_U^{-1} : V \rightarrow U$; in particular, for $(\p_c,t) = \phi(\p,t)$, we have $(J\phi_U^{-1})(\p_c,t) = [(J\phi_U)(\p,t)]^{-1}$. Re-expressing $\phi_U = (\bwdw, t)$ and denoting the inverse of $\bwdw$ inside the open set $U$ as $\bwdwu^{-1}$ (see Fig.~\ref{fig:inv_func_th}) yields :
% \begin{equation}
%     \begin{bmatrix} J_{\p} \bwdw^{-1}(\p_c; t) &
%     \frac{\partial \bwdw^{-1}}{\partial t}(\p_c; t)\\
%     \mathbf{0}_3^\top & 1\end{bmatrix}= 
%     \begin{bmatrix}
%     J_\mathbf{p} \bwdw(\p; t) &
%     J_t \bwdw(\p; t) \\
%     \mathbf{0}_3^\top & 1
%     \end{bmatrix}^{-1}
% \end{equation}
\begin{equation}
\resizebox{0.9\hsize}{!}{$
    \begin{bmatrix} J_{\p} \bwdwu^{-1} &
    \frac{\partial \bwdwu^{-1}}{\partial t}\\
    \mathbf{0}_3^\top & 1\end{bmatrix} \bigg|_{(\p_c,t)}= 
    \begin{bmatrix}
    J_\mathbf{p} \bwdw &
    \frac{\partial \bwdw}{\partial t}\\
    \mathbf{0}_3^\top & 1
    \end{bmatrix}^{-1}\bigg|_{(\p,t)} $}
\label{eq:jac_ift}
\end{equation}
By moving the matrix inverse inside the block matrix on the righthand side of (\ref{eq:jac_ift}) through Schur complement, we have $\frac{\partial \bwdwu^{-1}(\p_c,t) }{\partial t} = -[(J_\p\bwdw)(\p,t)]^{-1}\frac{\partial \bwdw (\p,t)}{\partial t}$. Finally by noticing that $v(\p;t) = \frac{\partial \bwdwu^{-1}(\p_c,t) }{\partial t}$, we have the proof. 
\end{proof}
We note that the sufficient condition of the proposition is weak and satisfied for most domain of deformation fields. For rare cases where $\det (\mathbf{J}_\p) < \epsilon$ for some space-time positions $(\p,t)$, we choose to exclude it from evaluating the loss so as to avoid numerical instability. Moreover, we implement equation (\ref{eq:velocity}) as a differentiable operator so that gradients can be back propagated during optimization.

\subsection{Scene flow from time integration of velocity}
With the velocity fields $v(\p;t)$ computed by equation (\ref{eq:velocity}), we estimate 3D scene flows \ie the displacement between $\p(t)$ and $\p(t+\Delta t)$ by time integration (see Fig.~\ref{fig:illu}),
\begin{equation}
    \p(t+\nabla t) - \p(t) = \int_t^{t+\Delta t} v(\p(s);s)ds.
    \label{eq:t_int}
\end{equation}
We implement the time integration though differentiable numerical ODE solvers. Because in our problem we are dealing with scene flows between small time intervals $\nabla t$, which usually is the duration of 1 or 2 frames, we find unrolling the fourth order Runge-Kutta method~\cite{hamming2012numerical} for two iterations gives stable results and achieves good trade-off between accuracy and computational cost.
To prevent overfitting to a fixed step size, we randomly perturbed the step size by adding a Gaussian noise.

\noindent\textbf{Rendering optical flow.} For every viewing ray at time $t$, we first calculate the next position $\p(t+\Delta t)$ for each sampled points along the ray by equation (\ref{eq:t_int}). Then the expected next position $\bar{\p}(t+\Delta t)$ for the visible points is estimated by weighted averaging $\p(t+\Delta t)$'s using NeRF's volumetric rendering equation. Finally, the optical flow is estimated by taking the difference between the 2D projection of $\bar{\p}(t+\Delta t)$ and the pixel location of the viewing ray.
\begin{comment}
For every viewing ray at time $t$, we first synthesize the depth as in NeRFs~\cite{nerfies,Mildenhall20eccv_nerf,flow-fields} by weighted averaging depth of sampled points along the ray. This gives an estimation of a position $\p(t)$ on the visible surface. Then the next position $\p(t+\Delta t)$ is estimated by equation (\ref{eq:t_int}). Finally, optical flow is synthesized by computing the difference between the 2D projection of $\p(t+\Delta t)$ and $\p(t)$, \ie
\begin{equation}
   \mathbf{T}_{t+\Delta t} \tilde{\p}(t+\Delta t) -    \mathbf{T}_t \tilde{\p}(t),
\end{equation}
where matrix $\mathbf{T}$'s denotes camera projection matrices, and $\tilde{\p}$ denotes the homogeneous coordinates of $\p$'s.
\end{comment}





\begin{comment}
This implementation is different from prior works~\cite{flow-fields,wang2021neural,yang2021banmo} which take weighted average of flows for all points on the ray. We find that estimating flows just for the surface point yields equivalent result but with cheaper computational cost. 
\end{comment}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/toy_exp_illu.pdf}
    \vspace{-20pt}
    \caption{A 2D toy experiment showing flow supervision is crucial for reconstructing rapid object motions. (a) 4 frames evenly sampled from the synthetic video. Two image patches are moving with large linear and angular velocity and has no overlap between their starting and ending positions. (b) fitting a SE(2) deformation field fails to recover correct motions for the first and last frame. (c) adding flow supervision correctly reconstruct the video frames.(d) we monitor the flow error and PSNR of reconstructed image during  optimization. The flows are estimated using equation (\ref{eq:velocity},\ref{eq:t_int}). Our method (with flow) converges significantly faster than w/o flow, and is able to fit the input optical flows with low error.}
    \label{fig:toy_exp}
\end{figure}


\subsection{Removing Gauge freedom}
One issue for the deformable NeRF is the recovered background tends to be not static. Deformation of the apparent static scene regions severely affects viewing experience. We find that a main cause for the jittering background is due to the Gauge ambiguity in deformable NeRF, \ie the optimization objective in (\ref{eq:train_obj}) does not specify which canonical coordinate the deformation field is defined at. In theory, any deformation of a canonical space can also be a valid one. This causes confusion for the deformable NeRF during optimization, which tends to trap it in the wrong factorization of motion and shapes.

Therefore, we propose to remove the Gauge freedom by specifying that the canonical coordinate is attached to a key frame $t_0$ from the input sequence. This is enforced through adding a loss to penalize the magnitude of deformation at time $t_0$, \ie
\begin{equation}
    \mathcal{L}_\text{Gauge} = \sum_\p \|\bwdw(\p;t_0) - \p\|_2.
\end{equation}
 Through our experiments, we choose the middle frame of the input sequence as the canonical frame. This is based on the assumption that the average deformation to other frames is likely to be minimal in the middle frame. Nevertheless, more sophisticated algorithms of choosing canonical frames may further improve the robustness of the method. 
 
% This  effectively stabilizes the background and improves overall reconstruction.

We note that prior works would introduce extra regularization to enforce static background.  Compared to training the density field to align with sparse points estimated by structure from motion~\cite{nerfies,spacetime-nerf}, our approach is self-contained without external modules. Compared to prior works having separate NeRFs for static and dynamic regions~\cite{nerf-w,flow-fields,gao2021dynamic,wu2022d}, our implementation is computationally more friendly. Though all the above methods could be included as supplementary.  

\begin{comment}
Finally, we note a side effect of attaching the canonical frame to a specific input frame -- sometimes objects are warped outside the near plane of the canonical frame, causing duplicated objects on both sides of the near plane. To alleviate this issue, we add a loss to push all objects behind the near plane.
\begin{equation}
    \mathcal{L}_\text{near plane} = |\min(\Bar{z}, \text{near})|,
\end{equation}
where $\Bar{z}$ is the depth-coordinate of the weighted average of the canonical position of points along the ray, and the weights are from the volumetric rendering equation in NeRF. More detials are in the supplementary. 
\end{comment}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/traj_teaser.pdf}
    \caption{3D visualizing of trajectories computed by integrating velocity fields $v(\p;t)$ from equation (\ref{eq:velocity}). Trajectories are colored to represent temporal order, and overlayed with colored point clouds extracted from the optimized deformable NeRF. Bottom row shows two frames from each of the input videos.}
    \label{fig:traj_teaser}
\end{figure*}

\begin{comment}
The main theoretical result is that we 


What is inverse warp?
What is canonical space?
What is the difference to forward warp?
What are the alternative to inverse warps?
Related works in deformable nerf.
\section{Getting Flow from Inverse warping}



Given a inverse warping field:
\begin{equation}
    w^{-1}(\mathbf{p}; t) \rightarrow \mathbf{p}_c
\end{equation}
which takes input a space-time point $(\mathbf{p}, t)$, and outputs its corresponding position $\mathbf{p}_c$ in the canonical frame. We want to evaluate the velocity of the point $\frac{d w(\mathbf{p}_c; t)}{dt}$, but without actually computing the forward warp $w(\mathbf{p}_c; t)$.

\paragraph{Approach}
Our derivation is based on the Inverse function theorem. First, lets construct a function:
\begin{equation}
    \phi(\mathbf{p}_c, t) = (w(\mathbf{p}_c; t), t)
\end{equation}
Then assuming that $\phi$ is continuously differentiable on some open set $U$ containing $(\mathbf{p}_c, t)$ and $det~J\phi \neq 0$, there exists a open set $V$ containing $\phi(\mathbf{p}_c, t)$ for $(\mathbf{p}_c, t)\in U$, such that $\phi$ has a continuous inverse $\phi^{-1}$, whose Jacobin is
\begin{equation}
    J \phi^{-1}(\mathbf{p},t) = [J \phi (\phi^{-1}(\mathbf{p}, t))]^{-1}
\end{equation}
By re-expressing $\phi$ via $w$ we have:
\begin{equation}
    \begin{pmatrix} J_{\mathbf{p}} w^{-1}(\mathbf{p}; t) &
    J_t w^{-1}(\mathbf{p}; t) \\
    \mathbf{0}_3^\top & 1\end{pmatrix} = 
    \begin{pmatrix}
    J_\mathbf{p} w(\mathbf{p}_c; t) &
    J_t w(\mathbf{p}_c; t) \\
    \mathbf{0}_3^\top & 1
    \end{pmatrix}^{-1}
\end{equation}
Substitute $\mathbf{A} = J_{\mathbf{p}} w^{-1}(\mathbf{p}; t) \in \mathbb{R}^{3\times3}$ and $\mathbf{b} = J_t w^{-1}(\mathbf{p}; t) \in \mathbb{R}^3$ and according to Schur complement properties, we have:
\begin{equation}
   J_t w(\mathbf{p}_c; t) = -\mathbf{A}^{-1}\mathbf{b} 
\end{equation}


\section{Deformable NeRF from monocular video}
\end{comment}
