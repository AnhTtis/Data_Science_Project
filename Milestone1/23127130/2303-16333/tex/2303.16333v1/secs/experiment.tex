\section{Experiment}

\subsection{Implementation details}
\noindent\textbf{Deformation field.} We follow Nerfies~\cite{nerfies} by using a 6 layer 128-width MLP which outputs SE(3) transformations. The deformation $\text{MLP}_g$ takes input $\p \in \mathbb{R}^3$ and an 8-dim deformation code. Instead of treating the codes as independent variables, we use another 2 layer 32-width $\text{MLP}_\alpha$ to map time $t$ to the deformation code. This helps improve convergence under smaller batch size. We also choose to use softplus as the activation function, since it produces smoother outputs than ReLU. With these, mathematically $\bwdw(\p;t) = \text{MLP}_g(\p, \text{MLP}_\alpha(t)) \circ \p$, where $\circ$ denotes the action of SE(3) transform.

\noindent\textbf{Radiance field.} We use the original architecture from Nefies with minor modification. Instead of having independent 8-dim appearance codes to optimize, we use a 2 layer 32-width MLP to produce the codes conditioned on time $t$.
 
\begin{comment}
\paragraph{NDC coordinate for unbounded frontal scene.} We choose the normalized device coordinate to deal with frontal unbounded scenes. We note that applying spherical coordinates as in NeRF++~\cite{nerfpp} and mip-NeRF360~\cite{barron2022mip} should be considered for unbounded non-frontal views, and leave it for future engineering effort. The theoretical results in this paper is valid regardless of the choice of coordinates. 
\end{comment}

\noindent\textbf{Optimization hyperparameters.} We set the initial weighting $\beta$ for the optical flow loss as 0.04 and anneal it to 0.0001. We set a fixed weighting for $\mathcal{L}_\text{Gauge}$ as 1. The initial learning rate of Adam optimizer is 0.001, and decayed 1/10 every 50 epochs. We train a total of 120 epochs or roughly 150k iterations. Each batch samples 4096 rays from 8 frames, and samples 256 points per ray. The optimization takes 4 GTX-3090 GPUs approximately 13hrs.

\subsection{Effectiveness of flow supervision}
In experiments, we evaluate the effectiveness of flow supervision using the velocity fields derived in equation ~(\ref{eq:velocity}). We aim to answer two key qustions: 
\begin{itemize}
% \vspace{-10pt}
    \item \emph{Does our method help improve convergence for rapid motions?}
% \vspace{-10pt}
    \item \emph{Does the flow supervision help disambiguate dynamic motion and structure in monocular deformable NeRF?}
\end{itemize}
% \vspace{-20pt}
\subsubsection{Flow supervision for fitting rapid motions}
To clearly address the first question, we conduct a 2D toy experiment. As shown in Fig.~\ref{fig:toy_exp}, we created a 25-frame video, consists of two rapidly moving image patches. Unlike the synthetic experiment in Nerfies~\cite{nerfies}, the image patches have large translational motion in addition to rotations, and the motions are different for each patches. It turns out that fitting an SE(2) deformation field using only image intensity loss is slow to converge, and results in distorted images. Applying optical flow loss by our method significantly speeds up convergence, and gives correct image reconstruction. This result indicates that our flow calculation algorithm \ie equation~(\ref{eq:velocity},\ref{eq:t_int}) is effective and also flow supervision is necessary for handling rapid motions. 

\subsubsection{Monocular dynamic view synthesis}
\noindent\textbf{Dataset.} State-of-the-art deformable NeRF, \eg Nerfies and HyperNeRF~\cite{nerfies,hypernerf} have shown satisfactory view synthesis result on the data they captured. However as discussed by Gao \etal~\cite{gao2022monocular}, Nerfies and HyperNeRF's data have high effective multi-view factors (EMFs). In other words, the objects are either quasi static or the camera motions are significantly larger than object motions. In this work, we evaluate on datesets with less EMFs. 

We first report results on the NVIDIA dynamic view synthesis dataset (NDVS)~\cite{globalcoherent} which has significantly lower EMFs. We follow the preprocessing steps of NSFF~\cite{flow-fields} which extracted 24 frames per sequence from the raw multi-view videos in NDVS. Neighboring frames are extracted from different cameras to simulate a monocular moving camera. For fair comparison with NSFF, we also downsize the images to have 288 pixels in height. For evaluation, we compare synthesized images to all images captured by 12 cameras, and report metrics such as PSNR, SSIM~\cite{wang2004image} and LPIPS~\cite{zhang2018perceptual}.\begin{comment}
    which means it is closer to causally captured videos. Therefore evaluating monocular methods on NDVS dataset is more meaningful and we focused our comparisons in this dataset. 
\end{comment}

We next compare view synthesis results on sequences collected by the authors of Nerfies~\cite{nerfies}. To ensure the inputs appear as if they were casually recorded in real life, we use video frames only from the left camera from the stereo rig, as opposed to teleporting between the left and right cameras in the original paper of Nerfies.

We also test our method on one DAVIS sequence~\cite{Perazzi2016} and two casual videos captured by Wang \etal~\cite{wang2021neural}.

\noindent\textbf{Trajectories by velocity integration.} To visually inspect the quality of our optimized deformation field and the derived velocity fields, we sparsely sample points on the surface of the reconstructed scene, and perform time integration with the velocity fields to create trajectories. As visualized in Fig.~\ref{fig:traj_teaser}, the recovered trajectories are smooth and closely follow the object motions. 

\noindent\textbf{Foreground background separation.} Since we removed Gauge freedom by picking one video frame as the canonical frame, the distance of a point $\p$ to its canonical correspondence $\p_c$ now directly indicates whether the point is static or moving. In Fig.~\ref{fig:distance_teaser}, we visualized the distance $\|\p-\p_c\|_2$ for each frame in a video. To visualize 3D volumes of distances in 2D, we project the distances along a ray by the volumetric rendering equation in NeRF. Thus brightness of the color corresponds to the distance of the visible area. We only observe large distances for the moving balloons and some small distances for the human subjects. The distances on the static background region are correctly rendered as close to 0. This indicates our success on the proposed removal of Gauge freedom.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/dist_teaser.pdf}
    \vspace{-15pt}
    \caption{Visualization the distance $\|\p-\p_c\|_2$ by volumetric rendering. Since we removed the Gauge freedom by picking the canonical frame to be one of the input frame, large distance only happens on the moving objects. Our result shows clean separation between the moving forground objects and the static background.}
    \label{fig:distance_teaser}
\end{figure}



\noindent\textbf{Baselines.}
We formed a close comparison with the state-of-the-art deformable NeRFs \eg Nerfies~\cite{nerfies} on the NDVS dataset. Due to the official code from the authors do not work out of the box for the NDVS dataset, we adapt it by changing its Euclidean coordinates to the NDC coordinates, so as to automatically deal with the increased scene depth in some of the NDVS sequences. Given all the aforementioned implementation and design choices, our own method is essentially applying the proposed flow supervision to the adapted Nerfies implementation. Thus comparison to Nerfies also serves as an ablation showing the effectiveness of the proposed flow supervision.

We also compared with another deformable NeRF approach, \ie NR-NeRF~\cite{nr-nerf}, whose deformation network outputs translation rather than SE(3) transformation. Finally, as a reference, we compared to NSFF~\cite{flow-fields}, which optimizes a time-modulated NeRF and scene flow fields, and is supervised not only by optical flows but also by the depth maps from the state-of-the-art monocular depth estimation network~\cite{midas}. Thus NSFF serves as a strong reference to check the status of  other methods without depth supervision.
We summarize the compared methods in Table.~\ref{tab:setup}. 
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/main_result.pdf}
    \vspace{-25pt}\caption{Results on NVIDIA dyanamic view synthesis dataset (NDVS). With flow supervision, our method produces smooth depth map and sharp novel view images. Without flow supervision leads to severe artifacts and noisy depth maps. We note that *Nerfies is our own adaptation of the official code for NDVS dataset.}\label{fig:main_result}
    \vspace{-5pt}
\end{figure*}

\begin{figure}
    \centering
     \includegraphics[width=\linewidth]{figs/rebuttal_nerfies.pdf}
    \vspace{-20pt}
    \caption{View synthesis results on sequences from Nerfies~\cite{nerfies}. We only use frames from the left camera for training, instead of teleportation between left and right cameras as in the original paper of Nefies. We find the compared methods make noticeable mistakes in the ``broom'' sequence (1st row) and some frames in the ``tail'' sequence (2nd row). In the "toby-sit" sequence (3rd row), HyperNeRF and NSFF produce blurry or distorted dog faces in some frames. In contrast, our method consistently yields a more plausible view synthesis result. This indicates that adding flow supervision by our method is also helpful for quasi-static videos.}
    \label{fig:nerfies}
    \vspace{-25pt}
\end{figure}


\begin{table}[h!]
    \centering
    \resizebox{\hsize}{!}{
    \begin{tabular}{c|cc|ccc}
    \toprule
     & \multicolumn{2}{c}{representation} & \multicolumn{3}{c}{supervision}\\
    method & motion & NeRF & image & flow & depth \\
    \hline
    NSFF~\cite{flow-fields} & scene flow & dynamic  & \checkmark & \checkmark & \checkmark \\
    \hline
    NR-NeRF~\cite{nr-nerf} & translation & static & \checkmark & & \\
    Nerfies~\cite{nerfies} & SE(3) & static & \checkmark & &\\
    HyperNeRF~\cite{hypernerf} & SE(3) + ambient & semi-static & \checkmark & &\\
    ours & SE(3) & static & \checkmark & \checkmark &\\
    \bottomrule
    \end{tabular}}
    \vspace{-2pt}
    \caption{Summary of the compared deformable NeRF methods and neural scene flow field (NSFF).}
    \label{tab:setup}
\end{table}



\begin{table*}[h]
    \centering
\resizebox{\hsize}{!}{
    \begin{tabular}{cc|ccc|ccc|ccc|ccc}
    \toprule
        method &   & \multicolumn{3}{c}{Playground} &
        \multicolumn{3}{c}{Balloon1} &
        \multicolumn{3}{c}{Balloon2} &
        \multicolumn{3}{c}{Umbrella}\\
         & & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$& SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ \\
    \hline
    NSFF~\cite{flow-fields} & full & 24.69 & 0.889 & 0.065 & 24.36 & 0.891 & 0.061 & 30.59 & 0.953 & 0.030 & 24.40 & 0.847 & 0.088\\
    & dyn. & 19.02 & 0.715 & 0.123 & 18.49 & 0.619 & 0.174 & 24.46 & 0.843 & 0.065 & 16.82 & 0.546 & 0.156\\
    \hline
    \rowcolor{LightCyan}
    \multirow{2}{*}{NR-NeRF~\cite{nr-nerf}} &  full & 14.16 & 0.337 & 0.363 & 15.98 & 0.444 &  0.277 & 20.49 & 0.731 & 0.348 & 20.20  &  0.526 & 0.315 \\
    & dyn. & 11.78 &  0.221 & 0.466 & 16.94 & 0.548 &  0.398 & 12.65 & 0.353 &  0.575 & 16.20  & 0.435 & 0.321\\
    \rowcolor{LightCyan}
    w/o flow & full & 22.18 &  0.802 &  0.133 & 23.36 &  0.852 &  0.102 & 24.91 & 0.864 & 0.089 & 24.29 & 0.803 & 0.169 \\
    (*Nerfies) & dyn. & 16.33 & 0.535 &  0.244 & 18.66 & 0.613 & 0.215 & 20.50 & 0.717 & 0.141 & 17.57 & 0.581 & 0.202\\
    \rowcolor{LightCyan}
    w flow & full & 22.39 & 0.812 & \textbf{0.109} & 24.36 & 0.865 & 0.107 & 25.82 & 0.899 & \textbf{0.081} & 24.25 &  0.813 & \textbf{0.123}\\
    (ours)                & dyn. & 16.70 & 0.597 & \textbf{0.168} & 19.53 & 0.654 & \textbf{0.175} & 20.13 & 0.719 & \textbf{0.113} & 18.00 & 0.597 & \textbf{0.148}\\
    \bottomrule
    \end{tabular} }
    \caption{Comparing deformable NeRFs and NSFF on the NVIDIA dynamic view synthesis (NDVS) dataset. Metrics are reported for the full image as well as only for the masked regions containing dynamic motions. Our method shows significant improvement over deformable NeRF methods without flow supervision. Comparison with NSFF is mixed, mainly due to the scale ambiguity without depth supervision. See Fig.~\ref{fig:scale_issue} for further discussion.}
    \vspace{-10pt}\label{tab:ndvs_result}
\end{table*}



\noindent\textbf{With vs. w/o flow supervision.} In Fig.~\ref{fig:main_result} we form a side-by-side comparison with Nerfies, which does not use optical flow supervision. We notice that Nerfies constantly make structural mistakes as indicated by its rendered noisy depth maps. As a result, it has noticeable artifacts concentrated around the dynamic objects. In contrast, with the help from flow supervision, our method renders smoother depth maps and visually more pleasing view synthesis images. These improvements are reflected quantitatively in Tab.~\ref{tab:ndvs_result}, where we show consistent improvement across all metrics. Our method also produces more plausible results for quasi-static scenes as shown in Fig.~\ref{fig:nerfies}.

\noindent\textbf{Compare with NSFF.} In table~\ref{tab:ndvs_result} we show competitive results on Balloon1 and Umbrella compared to NSFF which is supervised using depth. However we fall behind on the Playground and Balloon2 sequence. Closer diagnoses show that this is due to wrong depth scales which our method assigned to the fast moving balloons. As shown in Fig.~\ref{fig:scale_issue}, our method actually produces smoother depth maps compared to NSFF, thanks to our stronger temporal constraint due to having a single static template NeRF. However as highlighted by the blue arrows, the depth value of the moving objects are too small in comparison to the reference points on the background. We hypothesize this is due to the small motion bias of the deformation field which tends to explain 2D motions with smaller 3D motions. This causes the rendered foreground objects have significant offsets compared to the groundtruth, and consequently receives large penalties in terms of the image similarity metrics used in Table~\ref{tab:ndvs_result}, even though our method produces equivalent if not sharper view synthesis result compared to NSFF. This scale ambiguity issue is inherent from the single camera problem setup and should not blame the methods supervised without depth. To recover plausible relative scales of different moving parts of a dynamic scene, mid or higher level reasoning (\eg learning-based depth estimation~\cite{midas,wsvd}, 2D supervision from image generative models~\cite{poole2022dreamfusion}) is required. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/scale_issue.pdf}
    \vspace{-20pt}
    \caption{Our method suffers from scale ambiguity due to not using depth supervision. This is highlighted on the top left by comparing the points on the balloon and the pole, where the balloon should be behind the pole rather than having similar depth. Similarly, on the bottom left, the arm of the person should be close to the balloon, not behind it. Although we produce much smoother depth maps compared to NSFF, we make more error in the scale of depth, resulting in lower metrics in Table~\ref{tab:ndvs_result}. }
    \label{fig:scale_issue}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/wrong_can_frame.pdf}
    \vspace{-20pt}
    \caption{For highly deformable objects such as umbrella, the choice of the canonical frame is sensitive. In this example, we choose the first frame instead of mid frame as the canonical frame, which has a very different topogy compared to other frames when the umbrella is open. This leads to degenerated results as highlighted by the \textcolor{red}{red} box. }
    \label{fig:can_issue}
    \vspace{-10pt}
\end{figure}