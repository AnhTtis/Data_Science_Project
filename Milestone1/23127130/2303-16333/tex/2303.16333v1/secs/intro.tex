\section{Introduction}
Reconstructing dynamic scenes from monocular videos is a significantly more challenging task compared to its static-scene counterparts, due to lack of epipolar constraints for finding correspondences and ambiguities between motion and structure. Recent advances in differentiable rendering have lead to various solutions using an analysis-by-synthesis strategy -- solving the non-rigid deformation and structure by minimizing the difference between synthesized images and input video frames. Among those, deformable neural radiance fields~\cite{Lombardi:2019,nerfies,dnerf,nr-nerf} has been a notable  technique to represent dynamic scenes and shows plausible space-time view synthesis results. However, the current implementations only warrant success on teleporting-like videos whose camera motions are significantly more rapid than object motions. Quality of their results significantly decrease on videos with more rapid object motions~\cite{gao2022monocular}. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figs/teaser.pdf}
    \vspace{-15pt}
    \caption{We propose a method to use optical flow supervision for deformable NeRF. It noticeably improves novel view synthesis for monocular videos with rapid object motions. In the figure, we visualize rendered novel view images and depth maps for the first and last frame of the input video.}
    \label{fig:teaser}
\end{figure}

In this work, we conjecture the deficiency of these deformable NeRF-based methods is mainly due to lack of temporal regularization. As they represent deformation as \emph{backward} warping from the sampled frames to some canonical space, the motions or scene flows between temporally adjacent frames is not directly modeled nor supervised. Another deficiency is that these methods minimize photometric error alone, which is insufficient for gradient descent to overcome poor local minima when the canonical and other frames has little spatial overlap. This deficiency is severe for non-object-centric videos with large translations. 

The community has explored optical flow as an additional cue to help supervise the temporal transitions of other motion representations, such as scene flow fields~\cite{flow-fields,gao2021dynamic,zhang2021consistent,du2021neural} and blend skinning fields~\cite{yang2021banmo}. However, enforcing flow constraints with respect to a generic \emph{backward} warping field as in Nerfies~\cite{nerfies} is non-trivial. Intuitively, to compute scene flows, it requires inverting the \emph{backward} warp by having a \emph{forward} warp which maps points from canonical space to other frames. Then scene flows can be computed either by taking time derivative of the forward warp or predicting the next position of a point through evaluating the forward warp. But this can be problematic since analytical inverse of a complicated non-bijective function (\eg neural networks) is impossible, and an approximate solution by having an auxiliary network to represent the forward warp will introduce computational overhead and is theoretically ill-posed. Counter to this intuition, we will show that inverting the backward warping function is actually not needed for computing scene flows between frames.  

The main \textbf{contribution} of this paper is:
we derive an analytical solution to compute velocities of objects directly from the \emph{backward} warping field of the deformable NeRF. The velocities are then used to compute scene flows through temporal integration, which allows us to supervise the deformable NeRF through optical flow. This leads to significant improvement on videos with more rapid motions, as shown in Fig.~\ref{fig:teaser}. % To our knowledge, this is the first work to apply temporal supervision directly on the backward warping field, and can be potentially applied to other vision or graphics tasks.




The advantage of our approach is twofold:
(i) Our method applies to all kinds of backward warping function, thanks to the weak assumptions required by the inverse function theorem which our derivation is based on. Thus our method is more general compared to other works using invertible normalizing flows~\cite{lei2022cadex} or blend skinning~\cite{chen2021snarf,yang2021banmo}. (ii) Our method is also computationally more tractable compared to neural scene flow fields~\cite{flow-fields, du2021neural, li2021neural}, which would require integrating flows over long period of time to reach some canonical frame.

% Finally, as a secondary contribution, we advocate to remove the Gauge freedom from the deformable NeRF. This leads to more stable background reconstruction without the need for extra measures such as sparse background point supervision~\cite{nerfies} or separate background modeling~\cite{flow-fields,gao2021dynamic,wu2022d}.



\begin{comment}
We evaluated our method on the dynamic novel view synthesis benchmark~\cite{globalcoherent},  

Alternatively, 
(ii) using homeomorphic functions \eg normalizing flows~\cite{kobyzev2020normalizing} restricts the representation capability of the deformation, and in practise requires more compute due to having more coupling layers to enumerate different axis partitions~\cite{dinh2016density,lei2022cadex} or integrating flows over long period of time~\cite{du2021neural,flow-fields}; (iii) ~\cite{chen2021snarf}






% Such approaches are advantageous since they do not require long-term feature tracks which is otherwise unreliable to acquire, and they also provide high fidelity view synthesis for downstream applications.



One plausible strategy is to model the dynamic scene as the deformation of a static unknown template, and both the deformation and the template are assumed to follow some prior constraints. Reconstruction is then done by fitting the model to the input 2D observations (\eg 2D tracks, pixel intensities). Such approach has been implemented using techniques from recent advances in differentiable rendering. Most noticeably is deformable neural radiance field~\cite{Lombardi:2019,nerfies}, which models the deformation and the template radiance field using coordinate-based neural networks, and employs volumetric rendering to synthesis images under different viewing directions.

One important detail of the deformation model used 


Inverse warping field is a popular tool used in vision and graphics problems, \eg video editing~\cite{kasten2021layered}, deformable nerf~\cite{nerfies,Lombardi:2019}.
It models the mapping from a point to a canonical space. Prior works explored spatial priors for such warp, \ie how the inverse warp changes spatially. \eg as-rigid-as-possible, smooth functional representation. But the use of temporal supervision/prior is rare. This is mainly due to the fact that calculating temporal motion information, e.g. velocities / flows from an inverse warping field is not trivial. 

On the other hand, flows has been an important cue for solving vision problems. Flows represents how a point moves through time. There has been success in estimating 2D optical flows between images, and also intuitive priors such as small motions and linear motion assumptions.

The core problem people

\end{comment}