\section{Related works}
\noindent\textbf{Deformable NeRF.}
One common approach to model the dynamic scene is to represent it as the deformation of a static unknown template, and reconstruction is then done by fitting the model to the input 2D observations. Such approach has been implemented using techniques from recent advances in differentiable rendering. Most noticeably is deformable neural radiance field~\cite{Lombardi:2019,nerfies,dnerf,nr-nerf}, which models the deformation and the template radiance field using coordinate-based neural networks, and employs volumetric rendering to synthesis images under different viewing directions. 

More concretely, to synthesize the color of a pixel at time $t$, it first samples points along the line of sight. The sampled points $\p$'s are then fed into a \emph{backward} deformation field , \ie \begin{equation}
    \bwdw(\p; t) \longrightarrow \p_c, 
\end{equation}
which maps the input spacetime point $(\p, t)$ to its corresponding 3D position $\p_c\in\mathbb{R}^3$ in the canonical space. Colors $\mathbf{c}$ and densities $\sigma$ are then queried from the radiance field network, \ie
\begin{equation}
    f(\p_c, \mathbf{d}, \blambda) \longrightarrow \mathbf{c}, \sigma,
\end{equation}
where $\mathbf{d}\in S^2$ is the viewing direction and $\blambda$ is an additional frame-wise code to
allow the template to vary per frame so as to cope with topological and appearance changes~\cite{nerfies,nerf-w}. $\blambda$ can also be extended to vary spatially as ambient embeddings to enable greater flexibility to topological changes~\cite{hypernerf}. With the computed colors and densities of points along the viewing ray, RGB intensities of a pixel are computed using the volumetric rendering equations proposed in NeRF~\cite{Mildenhall20eccv_nerf}. Finally, the optimization objective is to minimize the difference between rendered pixels and the input observations.

\noindent\textbf{Backward deformation fields.}
Different ways exist for representing the backward deformation field. The most straightforward design is to use a neural network to output displacement between the input point and its canonical position~\cite{derf,nr-nerf,Lombardi:2019}. Park~\etal shows that having the neural network outputting SE(3) transformations leads to improvement in reconstructing rotational motions~\cite{nerfies}. 

For articulated objects such as animals and humans, blend skinning is widely used in literature~\cite{kavan2014part,loper2015smpl,zuffi20173d,zhi2020texmesh}. However, it is mainly designed for \emph{forward} deformation, thus adjustment is needed to adapt it for \emph{backward} warping field. Yang \etal~\cite{yang2021banmo,yang2021viser} uses mixture of gaussians to model the blending weights. This enables them to approximate the inverse of a forward blend skinning by simply inverting the SE(3) transformation of each deformation nodes. On the other hand, instead of explicitly define a deformation function, Chen~\etal~\cite{chen2021snarf} solves for canonical correspondences of any deformed point using iterative root finding.

For homeomorphic deformation where the mappings between any frames are bijective and continuous, recent works explored the use of invertible normalizing flows~\cite{lei2022cadex,cai2022neural} where the forward and backward deformation are computed with the same network parameters. The downside is the network architecture is restrictive, and in practice requires more compute due to having more coupling layers to enumerate different axis partitions.

\noindent\textbf{Other dynamic NeRF representations.}
Instead of having a static template NeRF, other works~\cite{flow-fields,wang2021neural,gao2021dynamic} choose to use a time-modulated NeRF to directly represent warped radiance fields. To enforce temporal consistency, they optimize neural scene flow fields to regularize pairwise motion between adjacent frames. This is only suitable for enforcing short-term consistency, but intractable for long-term consistency due to the expensive computational cost for performing scene flow integration. To improve computational efficiency, Wang~\etal~\cite{wang2021neural,Wang_ntp_2022_CVPR} propose neural trajectory field which directly outputs trajectories for all space-time locations. This allows enforcing long-term consistency without the need for scene flow integration.

\noindent\textbf{Optical flow supervision.}
Using optical flows to assist view interpolation~\cite{Wang_2022_CVPR,huang2020rife,bao2019depth} and 3D reconstruction~\cite{consistentdetph,kopf2021robust,teed2018deepv2d} has been a common practise. Several recent dynamic NeRF works~\cite{du2021neural,flow-fields,wang2021neural,gao2021dynamic} also use optical flow supervision, but none of them is based on backward deformation fields. Yang ~\etal~\cite{yang2021banmo} apply optical flow to supervise a blend skinning deformation field for object-centric reconstruction. Their result focuses on articulated objects and requires amodel segmentation mask as input. Under the context of view synthesis for dynamic scenes, we are unaware of any deformable NeRF-based approach using flow supervision. Thus our result provides a useful tool for future related research.