\section{Related work}
\label{sec:related_work}
%ithout access to labels, there does not exist a trivial objective to optimize. SSL objectives should be designed such that the learned features satisfy desirable properties. Desirable properties of features are akin to features with which solving a pretext task is easy. 
\noindent
{\bf Cross-view consistency} Early self-supervised methods make use of pretext tasks such as solving jigsaw puzzles~\cite{noroozi_jigsaw}, image rotation prediction~\cite{gidaris_rotations} and more \cite{contex_pred_doersch,contex_pred_Mundhenk,inpainting,predicting_noise}. Recently, there has been a shift towards learning features that are invariant to semantic preserving data augmentations \cite{dino,simsiam,moco,simclrv2,mocov2,mocov3,moby}. These data augmentations include geometric transforms (\eg \texttt{CROP}, \texttt{RESIZE} and \texttt{HORIZONTAL\_FLIP}) and photometric transforms (\eg \texttt{COLOR\_JITTER}, \texttt{SOLARIZE}, \texttt{GAUSSIAN\_BLUR} and \texttt{GRAYSCALE}). Stronger semantic preserving data augmentations lead to better downstream performance. However, the above-mentioned transforms lose their semantic preserving nature when they are too strong, \eg a very small \texttt{CROP} does not capture the object or a strong \texttt{GAUSSIAN\_BLUR} leads to a uniform image. %In the following subsections, we will \textcolor{red}{propose a way to generate strong semantic preserving augmentations.}


\noindent
{\bf Neighbor bootstrapping} In order to generate strong semantic positive pairs less reliant on heuristics, NNCLR \cite{dwibedi_little_2021} proposes to use positive pairs of different images by bootstrapping nearest neighbors in the latent space. We describe their method in detail
 in \Cref{sec:bootstrapping_neighbors} as well as the issues that arise when used in conjunction with a self-distillation objective, which we try to overcome using adaptivity in \Cref{sec:adaptive_similarity_bootstrapping}. Similarly, \cite{Koohpayegani_2021_ICCV} proposes to bootstrap multiple neighbors for a single query.

\noindent
{\bf Clustering methods} Clustering methods \cite{asano2019self,caron2020unsupervised,deepcluster,DBLP:journals/corr/abs-1905-01278,clusterfit} also process multiple different images but do not make use of positive/negative pairs. They enforce structure in the latent space by learning prototypes and enforcing clusters to be compact.

\noindent
{\bf Queues/memory banks} Memory banks have mostly been used in the context of contrastive learning for storing negatives \cite{moco, mocov2, mocov3} reducing the need for (very) large batch sizes. \cite{dwibedi_little_2021} uses memory banks for mining positives while \cite{DBLP:journals/corr/abs-2112-01390} makes use of memory banks for mining both positives and negatives.