
\section{Results}
\label{sec:results}

\subsection{Rationale of the experiment design}
The goal of the paper is \textbf{1)} to show that bootstrapping neighbors in the latent space using a self-distillation objective can hinder the performance or \textbf{2)} even lead to collapse and \textbf{3)} ultimately propose an adaptive bootstrapping scheme which not only solves the above-mentioned issues but also improves on the baselines using standard positive pairs. To achieve this goal, we compare two self-distillation methods (SimSiam \cite{simsiam} and DINO \cite{dino}) with different backbones (ViT-S/16 \cite{vit16x16} and ResNet-50 \cite{resnet}) in a simple controlled setup (pretraining on ImageNet-1k \cite{imagenet}, same hyperparameters, using only 2 global crops). For every evaluation, we compare 1) the baseline with 2) the baseline + straightforward nearest neighbor bootstrapping \cite{dwibedi_little_2021} and 3) the baseline + AdaSim.

\begin{table}[t]
\caption{\textbf{Supervised oracle.} $p$ indicates the probability to sample a standard positive pair ($1-p$ is the probability to sample a supervised positive pair, see \cref{sec:supervised_oracle}).}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cccc}
\toprule
$p$ & $k$-NN (top-1) & $k$-NN (top-5) & linear (top-1) & linear (top-5) \\
\midrule
0                            & 74.3                            & 90.5                            & 75.8                               & 92.7                               \\
0.5                          & 74.9                            & 90.9                            & 76.3                               & 93.0                              \\
\bottomrule
\end{tabular}
}
\label{tab:supervised}
\end{table}


We report results on the linear and $k$-NN benchmarks of ImageNet-1k which are industry standard evaluation protocols for self-supervised methods (\cref{sec:imagenet-benchmark}). To evaluate how generalizable the learned features are, we further compare all methods on few-shot transfer downstream tasks (\cref{sec:fewshot}). Then, we run an ablation study on AdaSim specific hyperparameters (\cref{sec:ablations}) and finish with some interesting training metrics that are helpful to understand AdaSim intuitively (\cref{sec:underthehood}). The main takeaway from this section is that AdaSim avoids issues \ref{iss:one} and \ref{iss:two} incurred by straightforward nearest neighbor bootstrapping and shows performance improvements on all downstream tasks.


\begin{table}[t]
\definecolor{darkgreen}{rgb}{0, 0.5, 0}
\centering
\caption{\textbf{Comparison of different methods on the $k$-NN and linear evaluation benchmark on ImageNet-1k \cite{imagenet}.} The 3 blocks of rows correspond to 3 different baselines: SimSiam ResNet-50, DINO-2 ResNet-50 and DINO-2 ViT-S/16. Each baseline is compared against straightforward nearest neighbor (NN) bootstrapping and against AdaSim. ``\collapse'' denotes that the training objective does not converge, \eg due to collapse. Rows corresponding to AdaSim are \hl{highlighted}. \textbf{Bold} text is used for the best performing row within each block.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
Method           & Model    & Epochs & $k$-NN & Linear \\
\midrule
SimSiam \cite{simsiam}         & ResNet-50 & 100   &   57.1   &    68.0    \\
SimSiam + NN     & ResNet-50 & 100   &   56.2 \textcolor{red}{(- 0.9)}  &    65.9 \textcolor{red}{(- 2.1)}  \\
\rowcolor{cyan!20} SimSiam + AdaSim & ResNet-50 & 100   &   \textbf{57.9} \textcolor{darkgreen}{(+ 0.8)}  &    \textbf{68.1} \textcolor{darkgreen}{(+ 0.1)}   \\
\midrule
DINO-2  \cite{dino}         & ResNet-50 & 100   &   50.2   &   60.0     \\
DINO-2 + NN      & ResNet-50 & 100   &    \collapse  &   \collapse     \\
\rowcolor{cyan!20} DINO-2 + AdaSim  & ResNet-50 & 100   &  \textbf{50.7} \textcolor{darkgreen}{(+ 0.5)}   &    \textbf{60.1} \textcolor{darkgreen}{(+ 0.1)}   \\
\midrule
DINO-2   \cite{dino}        & ViT-S/16  & 800   &  68.4    &   71.9     \\
DINO-2 + NN     & ViT-S/16  & 800   &   \collapse   &    \collapse    \\
% \rowcolor{cyan!10} DINO-2 + AdaSim  & ViT-S/16  & 800   &  \textbf{69.3}    &     \textbf{72.4}  \\
% \midrule
\rowcolor{cyan!20} DINO-2 + AdaSim  & ViT-S/16  & 800   &  \textbf{70.1} \textcolor{darkgreen}{(+ 1.7)}    &     \textbf{73.3} \textcolor{darkgreen}{(+ 1.4)}  \\
\bottomrule
\end{tabular}
}
\label{tab:main_table}
\end{table}

\input{tables/first_table}

\subsection{Evaluation benchmarks}
\noindent
{\bf Linear evaluation} A linear layer is stacked on top of the frozen features and trained on the training set of the downstream task. We report the top-1 accuracy on the test set. For each setting, we use the evaluation protocol (\eg choice of optimizer, number of training epochs \etc) from the corresponding baseline (SimSiam \cite{simsiam} or DINO \cite{dino}).
To evaluate the intrinsic quality of representations, the downstream evaluation should ideally not require many learnable parameters. In the case of ResNet-50, the number of parameters in the linear layer is $1000*d$ where $d=2048$ which is about $2$ million parameters. The following evaluations do not have any learnable parameters and are thus better suited to evaluate the intrinsic quality of the pretraining.

\noindent
{\bf $\boldsymbol{k}$-NN evaluation} The representation $\glob$ of each image in both the training and test set is computed. Then each image in the test set gets a label assigned based on votes from the nearest neighbors in the training set. We use $k=20$ to stay consistent with previous work and report the top-1 accuracy.

\noindent
{\bf Few-Shot transfer} This evaluation uses a nearest-centroid classifier (Prototypical Networks \cite{snell_prototypical_2017}). We use the code and datasets (except CIFAR-10 and CIFAR-100 because the images are only 32x32) from \cite{ericsson_how_2021}. We consider 5-way 5-shot transfer with a query set of 15 images and average results over 600 randomly sampled few-shot episodes.


% \begin{table*}[b]
%     \centering
%     \begin{tabular}{c|ccccc}
%         \toprule
%         \multicolumn{2}{c}{$\tau$}
%         \midrule
%         ImageNet $k$-NN top-1 & 68.4 & 68.4 & 68.9 & 69.3 & 68.6\\
%         ImageNet $k$-NN top-5 & 86.6 & 86.8 & 87.1 & 87.4 & 86.9\\
%         \bottomrule

%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table*}




\subsection{Implementation details}
\label{sec:implementation_details}
For both \href{https://github.com/facebookresearch/dino}{DINO} and \href{https://github.com/facebookresearch/simsiam}{SimSiam}, the same hyperparameters are used as reported on their GitHub. To make sure the size of the queue/cache does not impact the results, we implement the ``baseline + NN'' entries in \Cref{tab:main_table} and \Cref{tab:few_shot} with a cache that has the size of the whole dataset. To confirm the fact that standard positive pairs are needed (see issue \ref{iss:one}), we implement the querying of the nearest neighbor such that it cannot originate from the same image $\im_i \in \mathcal{D}$ (as is the case with a queue of small size). More implementation details can be found in \Cref{sec:implementation_details}.

\subsection{Supervised oracle}
\label{sec:supervised_oracle}
As a starter, to confirm our intuition that better positive pairs lead to better performance on downstream tasks, we approximate the oracle of positive pairs $\mathcal{N}^+$ using the labels from ImageNet-1k \cite{imagenet}. We sample a positive pair as two random images from the same class, on top of which we still apply augmentations. Empirically, we observe that the convergence (speed) is much worse than using standard positive pairs. To speed up the convergence, we sample standard positive pairs with a certain probability. Given that it makes sense for this probability to be high at the beginning of the pretraining, we simply try a linear schedule going from 1 to $p$. Results for $p=0$ and $p=0.5$ can be found in \Cref{tab:supervised}. It can be observed that $p=0.5$ performs better which corroborates our reasoning related to issue \ref{iss:one}.

\subsection{ImageNet-1k benchmarks}
\label{sec:imagenet-benchmark}
The $k$-NN and linear evaluation results on ImageNet-1k \cite{imagenet} are reported in \Cref{tab:main_table}. The last block in blue shows the best performing setting ($\tau=0.2$, $w=50$, $K=10$) from the ablation in \Cref{tab:ablation} with a long pretraining schedule of 800 epochs. The first 2 blocks of rows are trained with a window size of $w=1$ (and $\tau=0.2$, $K=10$) to confirm that AdaSim does not require a large window to improve the baseline and avoid collapse. DINO-2 denotes DINO with only 2 global crops. First, we can observe that with DINO-2 \cite{dino}, straightforward nearest neighbor bootstrapping (NN) does not converge (illustrated with ``\collapse''). This is confirmed for different backbones and different amount of training epochs. DINO-2 + AdaSim does converge and improves the baselines. The training objective of SimSiam \cite{simsiam} + NN does converge but suffers from a performance impact. %Note that the relative difference between different runs is higher for the $k$-NN benchmark as no parameters are learned.

\begin{table}[b]
\caption{\textbf{Ablation study over AdaSim specific hyperparameters.} The ablation is run over 800 epochs.} %The temperature $\tau$ (a) and support size $K$ (c) are run with 800 epochsThe window size $w$ ablation (b) is only run on 100 epochs due to compute constraints. \TODO{800 epochs}
    \begin{subtable}{0.5\textwidth}
    \centering
    \begin{tabular}{c|ccc c c}
        Temperature $\tau$ &  0.0 & 0.05 & 0.1 & \cellcolor{cyan!20} 0.2 & 0.4\\ %0.00625 & 0.0125 & 0.025
        \midrule
        $k$-NN & 68.4 & 68.8 & 69.0 & \cellcolor{cyan!20} \textbf{70.1} & 69.8\\ %& 68.7 & 68.7 & 69.0
        linear & 71.9 & 72.4 & 72.6 & \cellcolor{cyan!20} \textbf{73.3} & 73.2\\
    \end{tabular}
    \caption{Temperature $\tau$}
    \end{subtable}%
    
    \begin{subtable}{0.5\textwidth}
    \centering
    \begin{tabular}{c|cccc}
        % Window size $w$ &  1 & 10 & 25 & \textbf{50} \\
        % \midrule
        % ImageNet $k$-NN top-1 & 55.1 & 57.6 & 58.3 & 59.4\\
        % ImageNet $k$-NN top-5 & 77.2 & 79.2 & 80.0 & 80.5  \\
        Window size $w$ &  1 & \cellcolor{cyan!20} 10 & 50 \\
        \midrule
        $k$-NN & 69.3 & \cellcolor{cyan!20} \textbf{70.1} & 69.4\\
        linear & 72.6 & \cellcolor{cyan!20} \textbf{73.3} & 72.7\\
        %[69.09, 87.166]
    \end{tabular}
    \caption{Window size $w$}
    \end{subtable}%
    
    \begin{subtable}{0.5\textwidth}
    \centering
    \begin{tabular}{c|ccccc}
        Support size $K$ &  2 & \cellcolor{cyan!20} 3 & 5 & 10 & 20 \\
        \midrule
        $k$-NN & 69.4 & \cellcolor{cyan!20} \textbf{70.1} & 70.1 & 69.1 & 68.4\\
        linear & 72.8 & \cellcolor{cyan!20} \textbf{73.3} & 73.0 & 72.5 & 72.2 \\
    \end{tabular}
    \caption{Support size $K$}
    \end{subtable}%
    \label{tab:ablation}
\end{table}


\subsection{Few-shot transfer}
\label{sec:fewshot}
The results of the few-shot transfer are shown in \Cref{tab:few_shot}. Conclusions analogous to \Cref{sec:imagenet-benchmark} can be drawn: AdaSim improves the downstream performance on most datasets and on average (last column). For a point of comparison, \Cref{tab:few_shot} contains a row ``Supervised'' taken from \cite{ericsson_how_2021} which is obtained with the weights from the supervised ResNet-50 in torchvision. Interestingly, DINO-2+AdaSim performs much better than the baseline DINO-2 on datasets where the supervised method also performs better \eg Cars \cite{krause_collecting_nodate} (+5.06) or Aircraft \cite{aircraft_dataset} (+2.54). This is because AdaSim bootstraps neighbors in the latent space which acts as a sort of self-labeling and therefore shares some properties with the supervised method.

\begin{figure*}[t]
    \centering
    \begin{minipage}{.33\linewidth}
    \centering
    \subcaptionbox{Neighbor bootstrapping ratio \label{fig:1a} }
    {\includegraphics[width=\textwidth]{figures/plot_nn_ratio_self.pdf}}
    \end{minipage}
    \begin{minipage}{.33\linewidth}
    \centering
    \subcaptionbox{NN top-1 training accuracy \label{fig:1b} }
    {\includegraphics[width=\textwidth]{figures/plot_nn_accuracy_top1.pdf}}
    \end{minipage}
    \begin{minipage}{.33\linewidth}
    \centering
    \subcaptionbox{2-NN top-1 training accuracy \label{fig:1c}}
    {\includegraphics[width=\textwidth]{figures/plot_nn_accuracy_top2.pdf}}
    \end{minipage}
    \caption{\textbf{Visualization of multiple training metrics for different temperature $\tau$ values.}}
    \label{fig:plots_under_hood}
\end{figure*}

\subsection{Ablations}
\label{sec:ablations}
An ablation study over AdaSim specific hyperparameters ($\tau$, $w$, $K$) can be found in \Cref{tab:ablation}. The best hyperparameters are highlighted in bold. These bold parameters are used for all runs, except for the parameter that is being varied. Importantly, \textbf{for a temperature $\tau=0$, AdaSim behaves like a standard self-distillation} method using positive pairs of the form $(t(\im), t'(\im))$. A performance improvement can be observed for increasing temperature values which shows the merits of AdaSim.


\subsection{Under the hood analysis}
\label{sec:underthehood}
Multiple training metrics are shown in \Cref{fig:plots_under_hood} with varying temperature values. Such plots are useful to build intuition on the internal mechanisms of AdaSim.

\noindent
{\bf Neighbor bootstrapping ratio} indicates the percentage of positive pairs $(t(\im_i),t'(\im_{j^\star}))$ where the augmentations are from different images. The higher the temperature, the higher the percentage is. In the limit when $\tau \to 0$, it can be observed that this percentage goes to 0, and AdaSim defaults to standard self-distillation. This is only possible thanks to the adaptive sampling of positive pairs in AdaSim (lines 13 to 18 in \Cref{alg:adasim}). Without the adaptive sampling, a low temperature would lead to a positive pair $(t(\im_i),t'(\im_{j^\star}))$ where $\im_{j^\star} = \argmax p^{\text{win}}(\im_j | \im_i)$ but there is no guarantee that $\im_i = \im_{j^\star}$. The adaptivity of the proposed method can be observed in \Cref{fig:plots_under_hood}\textcolor{red}{.a}. Indeed, at epoch 50, the window is filled and nearest neighbor bootstrapping is allowed to occur. As the quality of the latent space is low, so is that of the resulting gradients, which temporarily hurts the learned representations. Thanks to the adaptivity criterion, the bootstrapping ratio is automatically reduced to avoid the collapse.
% Interestingly, we can observe the effect of switching from standard positive pairs to the adaptive setting at epoch 50 ($w=50$ in this case). When using positive pairs of the form $(t(\im_i),t'(\im_{j^\star}))$, they enforce gradients which are such that the $\argmax$ of $p^{\text{win}}(\im_j | \im_i)$ changes and therefore, less bootstrapping occurs due to the adaptivity.

\noindent
{\bf NN top-1 training accuracy} shows how often the query image $\im_i$ and its ``nearest neighbor'' $\im_{j^\star}$ are from the same class. Here we observe that a higher temperature leads to a lower accuracy which makes sense because the ``nearest neighbor'' can be the same image and, therefore, would trivially be in the same class. Note that before epoch 50, all temperature values use the same positive pairs as $w=50$ similarity metrics are being computed.

\noindent
{\bf 2-NN top-1 training accuracy} shows if the query image $\im_i$ and its second ``nearest neighbor'' $\argmax_{\im_j \neq \im_{j^\star}} p^{\text{win}}(\im_j | \im_i)$ are from the same class. This metric is a better indicator of the downstream generalizability of the learned features. It can be observed that higher temperature values (more neighbor bootstrapping) are initially worse but start to become advantageous as the training progresses. This is intuitive because bootstrapping neighbors is only useful when they are semantically related, which only happens as the network learns.

\noindent
{\bf Visualization of positive pairs} To get an understanding of the positive pairs which are formed by AdaSim, we visualize multiple query images $\im_i$ along with the sampling distribution $p^{\text{win}}(\im_j | \im_i)$ (overlayed in green) and its associated support $\mathcal{S}_{\text{win}}$ in \Cref{fig:query_support}. In this example, all ``nearest neighbors'' are the same as the query image, and all neighbors seem to share semantic content. In \Cref{sec:visualization}, we explicitly search for query images where the neighbors are from different classes. These results show evidence of wrongly labeled or duplicate images in ImageNet-1k \cite{imagenet}.
