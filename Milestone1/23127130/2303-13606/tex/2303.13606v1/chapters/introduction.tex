
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/NN.jpg}
    \caption{\textbf{The selection of positive image pairs used for cross-view consistency in self-supervised representation learning is key for good performance.} With our method, given the query (or anchor) image on the left, similar images are successfully ranked according to $p^{\text{win}}(\im_j | \im_i)$ (illustrated as a \textcolor{green}{green} bar on the bottom left of each image). Our algorithm enforces similarity between the query $\im_i$ and an image $\im_j$ sampled from $p^{\text{win}}(\im_j | \im_i)$. These results are non-cherry-picked and obtained at the final epoch (800) of the pretraining. Best viewed in color and zoomed-in.}
        % \caption{\textbf{Query image $\im_i$ (left column) and corresponding support $\mathcal{S}_{\text{win}}$ ranked by decreasing $p^{\text{win}}(\im_j | \im_i)$ (illustrated as a \textcolor{green}{green} bar on the bottom left of each image).} These results are non-cherry-picked and obtained at the final epoch (800) of the pretraining. Our algorithm enforces similarity consistencies between the query $\im_i$ and an image $\im_j$ sampled from $p^{\text{win}}(\im_j | \im_i)$. Best viewed in color and zoomed-in.}
    \label{fig:query_support}
\end{figure}

\section{Introduction}
\label{sec:introduction}
Self-supervised learning (SSL) methods have seen a lot of breakthroughs over the past few years. Most recent self-supervised methods train features invariant to data augmentation by maximizing the similarity between two augmentations of a single input image. However, this task is ill-posed as this optimization procedure admits trivial solutions (resulting in a ``collapsed'' scenario). Similarity maximization (or cross-view consistency) SSL methods can be categorized based on how they avoid trivial solutions. The most famous subset are contrastive learning methods \cite{simclrv2,pmlr-v119-chen20j,moco,mocov2,mocov3} in which the collapse is avoided by using negative pairs. On the one hand, the learning procedure is robust since collapse avoidance is explicitly modeled in the training objective, but on the other hand, it requires the use of large batches to have a sufficient pool of negative samples. This makes them GPU memory inefficient and limits research to those who dispose of large distributed computing infrastructure.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\columnwidth]{figures/Screenshot 2022-11-02 at 16.48.47.jpg}
%     \caption{\TODO{this is only a placeholder to estimate size}}
%     \label{fig:overview}
% \end{figure}


More recently, self-distillation methods have been gaining traction \cite{simsiam,dino,byol,esvit}. These similarity maximization algorithms avoid trivial solutions by using asymmetry. This asymmetry can take the form of an additional predictor \cite{simsiam,byol} on one branch, using stop-gradients \cite{simsiam,dino,esvit,byol}, a momentum encoder \cite{dino,byol}, \etc. These methods are of particular interest as 1) they do not require large batch sizes, and 2) they currently show state-of-the-art performance \cite{dino,esvit} on standard downstream tasks.

%This oracle would yield diverse and complex pairs of images.
Orthogonal to the choice of framework (contrastive vs self-distillation), one can wonder what is the best way to obtain positive pairs. Intuitively, similarity maximization SSL methods could be improved by using positive pairs from different images. Indeed if an oracle indicating valid positive pairs \cite{supcon,improve_supcon} was available, instead of taking two augmentations from the same image, we could simply take pairs from the oracle. The features would, therefore, not be trained to be invariant to handcrafted data augmentations but invariant to intra-class variation, which would make them more aligned with most common downstream tasks, \eg classification.

In the absence of labels, we can leverage the structure of the latent space to obtain a proxy for the oracle. Semantically related images are expected to lie in the vicinity of one another in the latent space. However, this is a chicken and egg problem, as this assumption only holds when the quality of the learned latent space is good enough. If the learned latent space is not of good quality, bootstrapping the proxy leads to unwanted gradient flows, e.g., an image of a cat is pulled closer to the image of a building.

% This idea of bootstrapping nearest neighbors is essentially orthogonal to the choice of loss-function used (contrastive or self-distillation). 

% Similarly, used in conjunction with self-distillation methods, one might expect nearest neighbor bootstrapping for positive image pair generation to be beneficial. \textbf{However, we empirically observe that straightforward bootstrapping can be hurtful and can even lead to collapse.}

% To overcome the above issue, we propose to estimate the quality of the latent space and only use neighbors as positive image pairs if the quality of the latent space is judged to be high enough. If that condition is not satisfied, we default to using standard positive pairs \ie two augmentations from the same image. 

%Recent work NNCLR \cite{dwibedi_little_2021} bootstraps nearest neighbors in a straightforward way which leads to performance improvements when used in conjunction with a contrastive objective. It then makes sense to try and combine the best solution for selecting positives pairs \cite{dwibedi_little_2021} with the best solution for defining the loss-function, arguably, self-distillation.

Nevertheless, recent work NNCLR \cite{dwibedi_little_2021} has successfully incorporated nearest neighbor (NN) bootstrapping in a contrastive setting. Considering that self-distillation typically outperforms contrastive methods, in this work, we explore how the same can be achieved without explicit use of negatives.

Unfortunately, this combination does not work out-of-the-box. \textbf{We empirically observe that it can be hurtful and even lead to collapse.} We scrutinize the reason for this unexpected behavior and provide a solution. We propose to estimate the quality of the latent space and adaptively use positive pairs sampled from a ranked set of neighbors (\cref{fig:query_support}) if the estimated quality of the latent space is high enough. This leads to an \textbf{Ada}ptive learning algorithm based on \textbf{Sim}ilarity bootstrapping dubbed AdaSim. The overall
framework is shown in \Cref{fig:main_figure}. We summarize our contributions as follows:
\begin{enumerate}
    \item We provide empirical evidence that, when combined with self-distillation, straightforward bootstrapping as in \cite{dwibedi_little_2021} can lead to a performance drop or even collapse. This is validated for multiple self-distillation methods and backbone combinations;
    \item We propose an adaptive similarity bootstrapping learning method (AdaSim) in which the amount of bootstrapping is modulated via a single temperature parameter. Using a temperature parameter of 0, AdaSim defaults to self-distillation with standard positive image pairs generated from augmented views of the same image. We show that AdaSim performs best with a non-zero temperature parameter and outperforms the baselines on standard downstream tasks.
    % \item This is achieved with negligible compute and memory overhead.
    % \item \TODO{an image $\im_i$ and $\im_j$ should be considered as semantically close, not only if $f(t(\im_i))$ is close to $f(t(\im_i))$, but if $\mathbb{E}_{t \sim \mathcal{T}}[f(t(\im_i))]$ is close to $\mathbb{E}_{t' \sim \mathcal{T}}[f(t'(\im_j))]$?}
    % \item \Behzad{I would rather reduce the number of bullet points by merging some items, and add the last one referring to the superior performance of method over competing baselines on datasets, also, e.g., source code and models will be available upon acceptance (optional)}
\end{enumerate}



% \begin{enumerate}
%     \item Machine learning algorithms perform better with more supervisory signal
%     \item In a supervised setting, more supervised signal can be leveraged by using more annotated data.
%     \item However, in a self-supervised setting, it is not as trivial.
%     \item Typical self-supervised methods are trained by enforcing coherence between augmentations of the same image.
%     \item The self-supervisory signal could be strengthened if we had access to an oracle indicating valid positive pairs: instead of taking 2 augmentations from the same image as a positive pair, we can simply take pairs from the oracle.
%     \item We can leverage the structure of the latent space to obtain a proxy for the oracle. Images producing similar representations in the latent space are images which can be assumed to be semantically close.
%     \item However, this is a chicken and egg problem as this assumption only holds when the quality of the learned latent space is good enough. If the learned latent space is not of good quality, bootstrapping the proxy leads to gradient flows which are not desired e.g. image of cat is pulled closer to image of building.
%     \item Previous work NNCLR \cite{dwibedi_little_2021} bootstraps nearest neighbors in a straightforward for a contrastive objective leading to performance improvements on downstream tasks. However, we empirically observe that this simple bootstrapping in a non-adaptive way does is hurtful when applied on non-contrastive learning algorithms. \textcolor{blue}{Recent state of the art algorithms \cite{simsiam,dino,byol} show that negative samples are not needed to learn representations while avoiding collapse by including some of the following tricks: use asymmetric predictors, use stop-gradients in one branch, have one branch reflect a low-passed (\eg with an exponential moving average) version of the other, run more gradient descent steps on one branch, use some kind of normalization on the representation, \etc. (need to paraphrase this)} Due to the non explicit modeling of negative in the learning objective, these methods are not as robust and more likely to suffer from wrong gradient updates.
%     \item However, non-contrastive learning algorithm are of particular interest for the community as 1) they are more simple and elegant by design but more importantly 2) they have recently shown state of the art performance on standard downstream tasks.
%     \item We propose an adaptive similarity bootstrapping self-supervised algorithm which alleviates the chicken and egg problem leading to a robust training and improvements on standard downstream tasks.
    
%     \item In summary, our contribution is twofold: 1) we present a novel adaptive similarity bootstrapping algorithm for representation learning which diverges from standard self-distillation algorithms via a single temperature parameter. 2) We show empirical evidence that such adaptive bootstrapping (as opposed to straighforward bootstrapping) is necessary from non-contrastive learning methods and show performance improvements on downstream tasks for multiple pre-training methods and backbones.
% \end{enumerate}

