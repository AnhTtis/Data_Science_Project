\section{Conclusion}
\label{sec:conclusion}
Self-distillation is becoming the go-to self-supervised learning paradigm due to its simplicity and state-of-the-art performance. However, a non-explicit processing of negative pairs makes it less robust and more prone to collapse to trivial solutions than contrastive learning. Used in conjunction with bootstrapped positive pairs of neighbors, we empirically observe that self-distillation methods can perform worse than their vanilla baseline and in some cases even collapse. We propose an adaptive bootstrapping scheme which stabilizes the training and improves on the baselines. We also observe that long training schedules are particularly beneficial for such bootstrapping scheme and hypothesize that larger backbones act similarly (better representations lead to better bootstrapping). We hope the insights from this paper can incentivise large research groups to upscale our experiments.

\noindent
{\bf Limitations} 
\label{par:limitations}
All results in the paper do not include multi-crop \cite{swav} for simplicity. In practice, not using multi-crop requires the use of more diverse random cropping (\eg with scale sampled between 0.1 and 1) but we have not changed any hyperparameters from DINO and stuck with a range of 0.4 to 1.