\clearpage
\newpage
\appendix
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

\setcounter{equation}{0}
\renewcommand{\theequation}{A\arabic{equation}}

\section*{Appendix}
We provide additional details in \Cref{sec:abs}, \Cref{sec:implementation_details} and \Cref{sec:runtime} as well as some qualitative visualizations in \Cref{sec:visualization}.

\section{Self-distillation asymmetry abstraction}
\label{sec:abs}
In \Cref{sec:selfdistillation-contrastive}, we state that self-distillation methods avoid collapse by using asymmetry and claim that this asymmetry can be abstracted out using two asymmetric encoders $f$ and $f'$. Given a distance metric $d$, a self-distillation objective is made solely out of positive terms of the form:

\begin{equation}
    \mathcal{L}_{distil} = d(f(\im), f'(\im^+)) \tag{\ref{eq:self-distil}}
\end{equation}
where $(\im, \im^+)$ is a positive pair. The total self-distillation objective is \Cref{eq:self-distil} summed over all positive pairs $(\im, \im^+)$. Below, we explicit the form of the encoders and the distance metric, both for SimSiam \cite{simsiam} and DINO \cite{dino}.

\subsection{SimSiam \cite{simsiam}}
Using notation from the original paper, SimSiam defines an encoder $f \colon \mathcal{X} \to \mathcal{Z}$ and a predictor $p\colon \mathcal{Z} \to \mathcal{Z}$. Using our notations, we encapsulate both the original encoder $f$ and the predictor $p$ in a single encoder which we also denote by $f$ and define our $f'$ as the $f$ from SimSiam\footnote{the notation on the right side of \Cref{eq:1} and \Cref{eq:2} refers to notation from SimSiam, and the left side refers to our notation}:
\begin{equation}
    f' \triangleq f
    \label{eq:1}
\end{equation}

\begin{equation}
    f \triangleq p \circ f
    \label{eq:2}
\end{equation}
The distance metric used is the negative cosine similarity. Given the above, the self-distillation loss in SimSiam can be written as
\begin{equation}
    d(f(\im), f'(\im^+)) := -\frac{f(\im)^\top f'(\im^+)}{\norm{f(\im)}_2 \norm{f'(\im^+)}_2}
\end{equation}
This loss is minimized w.r.t. the weights of $f$ (no gradients are back-propagated through $f'$).

\subsection{DINO \cite{dino}}
Using notation from the original paper, DINO uses a student backbone $g_{\theta_s} \colon \mathcal{X} \to \mathcal{P}$ where $\mathcal{P}$ is the space of discrete probability mass functions. An analogous teacher backbone $g_{\theta_t}$ is defined as a smoothed version of $g_{\theta_s}$. At the end of each epoch, the weights of the teacher backbone are updated with $\theta_t \leftarrow \lambda \theta_t + (1-\lambda) \theta_s$ where $\theta_s$ and $\theta_t$ refer to the weights of the student and teacher backbone, respectively.

Using our notations, we define both encoders as
\begin{equation}
    f \triangleq g_{\theta_s}
\end{equation}
and
\begin{equation}
    f' \triangleq g_{\theta_t}
\end{equation}
The distance metric $d$ is the cross entropy. Given the above, the self-distillation loss of DINO can be written as
% In DINO \cite{dino}, $f'$ is a momentum encoder $f'(\im) := f_{\text{EMA}}(\im)$ whose weights are updated at the end of each epoch as follows: $\weight' \leftarrow \lambda \weight' + (1-\lambda) \weight$. $\weight$ and $\weight'$ are the weights of $f$ and $f'$, respectively. Given a head $h \colon \mathcal{Z} \to \mathcal{P}$ mapping to the latent space discrete probability mass functions, and its exponential moving average equivalent $h'$, $d$ is defined as:


\begin{equation}
    d(f(\im), f'(\im^+)) := H(f(\im), f'(\im^+))
\end{equation}
where $H$ is the cross entropy
\begin{equation}
    H(p, q) = -\sum_{i \in \mathcal{I}} p(i) \log q(i)
\end{equation}
and $\mathcal{I}$ is the support of the distributions $p$ and $q$, \ie $\mathcal{I}=[I]=\{1, 2, \cdots, I\}$. $I$ refers to the dimensionality of the output distributions. This loss is minimized w.r.t. to the weights of $f$ (no gradients are back-propagated through $f'$).

\section{Implementation details}
\label{sec:implementation_details}
\subsection{SimSiam \cite{simsiam}}
We use the code from their official GitHub  (\href{https://github.com/facebookresearch/simsiam}{link}). All 3 runs (baseline, baseline + NN, baseline + Adasim) use the same hyperparameters:

\begin{itemize}
\setlength\itemsep{-0.3em}
    \item \texttt{arch}: resnet50
    \item \texttt{epochs}: 100
    \item \texttt{batch\_size}: 512
    \item \texttt{lr}: 0.05
    \item \texttt{momentum}: 0.9
    \item \texttt{weight\_decay}: 0.0001
    \item \texttt{dim}: 2048
    \item \texttt{pred\_dim}: 512
    \item \texttt{fix\_pred\_lr}: True
\end{itemize}

\subsection{DINO \cite{dino}}
We use the code from their official GitHub  (\href{https://github.com/facebookresearch/dino}{link}). For the 3 ResNet-50 runs (baseline, baseline + NN, baseline + Adasim) the same hyperparameters specified on the official GitHub are used, except for \texttt{local\_crops\_number} which we set to 0:

\begin{itemize}
\setlength\itemsep{-0.3em}
    \item \texttt{arch}: resnet50
    \item \texttt{batch\_size\_total}: 1024
    \item \texttt{clip\_grad}: 0.0
    \item \texttt{drop\_path\_rate}: 0.1
    \item \texttt{epochs}: 100
    \item \texttt{freeze\_last\_layer}: 1
    \item \texttt{global\_crops\_scale}: [0.14, 1.0]
    \item \texttt{local\_crops\_number}: 0
    \item \texttt{lr}: 0.03
    \item \texttt{lr\_linear}: 0.03
    \item \texttt{min\_lr}: 1e-05
    \item \texttt{momentum\_teacher}: 0.996
    \item \texttt{norm\_last\_layer}: False
    \item \texttt{optimizer}: sgd
    \item \texttt{out\_dim}: 65536
    \item \texttt{seed}: 0
    \item \texttt{teacher\_temp}: 0.07
    \item \texttt{use\_bn\_in\_head}: False
    \item \texttt{use\_fp16}: False
    \item \texttt{warmup\_epochs}: 10
    \item \texttt{warmup\_teacher\_temp}: 0.04
    \item \texttt{warmup\_teacher\_temp\_epochs}: 30
    \item \texttt{weight\_decay}: 0.0001
    \item \texttt{weight\_decay\_end}: 0.0001
\end{itemize}

For all runs using the ViT-S/16 backbone, the same hyperparameters specified on the official GitHub are used:

\begin{itemize}
\setlength\itemsep{-0.3em}
    \item \texttt{arch}: vit\_small
    \item \texttt{patch\_size}: 16
    \item \texttt{batch\_size\_total}: 1024
    \item \texttt{clip\_grad}: 0.0
    \item \texttt{drop\_path\_rate}: 0.1
    \item \texttt{epochs}: 800
    \item \texttt{freeze\_last\_layer}: 1
    \item \texttt{global\_crops\_scale}: [0.4, 1.0]
    \item \texttt{local\_crops\_number}: 0
    \item \texttt{lr}: 0.0005
    \item \texttt{min\_lr}: 1e-05
    \item \texttt{momentum\_teacher}: 0.996
    \item \texttt{norm\_last\_layer}: False
    \item \texttt{optimizer}: adamw
    \item \texttt{out\_dim}: 65536
    \item \texttt{seed}: 0
    \item \texttt{teacher\_temp}: 0.07
    \item \texttt{use\_bn\_in\_head}: False
    \item \texttt{use\_fp16}: True
    \item \texttt{warmup\_epochs}: 10
    \item \texttt{warmup\_teacher\_temp}: 0.04
    \item \texttt{warmup\_teacher\_temp\_epochs}: 30
    \item \texttt{weight\_decay}: 0.04
    \item \texttt{weight\_decay\_end}: 0.4
\end{itemize}

\section{Runtime analysis}
\label{sec:runtime}
We compare the runtime for baseline DINO-2 \cite{dino} and DINO-2 + AdaSim. As can be seen in \Cref{tab:timings}, AdaSim has a negligible impact on throughput during the pretraining.

\begin{table}[t]
\caption{\textbf{Runtime analysis of AdaSim per iteration of pretraining.} Run on 4x AMD MI250X GPUs.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
method & backbone & batchsize per GPU & time per iter [s] \\
\midrule
DINO-2 \cite{dino} & ViT-S/16 & 128 & 0.256 \\
DINO-2 + AdaSim & ViT-S/16 & 128 & 0.270\\

\midrule
DINO-2 \cite{dino} & ViT-S/16 & 256 & 0.480 \\
DINO-2 + AdaSim & ViT-S/16 & 256 & 0.488\\

\bottomrule
\end{tabular}
}
\label{tab:timings}
\end{table}


\section{Investigating intriguing neighbors}
\label{sec:visualization}
In \Cref{fig:query_support}, we show a visualization of random query images $\im_i$ and their corresponding support $\mathcal{S}_{\text{win}}$ ranked by decreasing $p^{\text{win}}(\im_j | \im_i)$. All 1-NN are the same as the query image ($\im_i = \argmax_{\im_j}p^{\text{win}}(\im_j | \im_i)$), and all other neighbors seem to share semantic content with the query image.

\subsection{Nearest neighbor is different from the query}
Here, we explicitely search for cases where the nearest neighbor is not the same image as the query to observe border cases of AdaSim. Mathematically, this is the case when $\im_i \neq \argmax_{\im_j}p^{\text{win}}(\im_j | \im_i)$. Such queries are shown in \Cref{fig:firstisnotid} with the corresponding metadata in \Cref{tab:firstisnotid}. It can be observed that even when the nearest neighbor does not originate from the same image, all nearest neighbors visually share semantic content. 

\subsection{Nearest neighbor is from a different class as the query}
A stronger special case happens when the nearest neighbor is not even from the same class as the query \ie $\texttt{class}(\im_i) \neq \texttt{class}(\argmax_{\im_j}p^{\text{win}}(\im_j | \im_i))$. This is shown in \Cref{fig:firstisnotclass} and \Cref{tab:firstisnotclass}. Interestingly, even if the first nearest neighbor is not from the same class, it still looks very similar. This shows evidence of mislabelling. Consider the example of the first row of \Cref{fig:firstisnotclass}. The query is from class 384 (indri, indris, Indri indri, Indri brevicaudatus) yet the first neighbor is from class 383 (Madagascar cat, ring-tailed lemur, Lemur catta). However, they are very similar and one seems to be a zoomed in version of the other. Similar conclusions can be drawn for all other query images in \Cref{fig:firstisnotclass}.\\

For readability purposes, \Cref{tab:firstisnotid} and \Cref{tab:firstisnotclass} only include the class ids and not the full class names. A mapping from class id to class name can be found below for the subset of classes appearing in \Cref{tab:firstisnotid} and \Cref{tab:firstisnotclass}.

\begin{itemize}
\setlength\itemsep{-0.5em}
\item 60: night snake, Hypsiglena torquata
\item 66: horned viper, cerastes, sand viper, horned asp, Cerastes cornutus
\item 68: sidewinder, horned rattlesnake, Crotalus cerastes
\item 80: black grouse
\item 82: ruffed grouse, partridge, Bonasa umbellus
\item 83: prairie chicken, prairie grouse, prairie fowl
\item 138: bustard
\item 166: Walker hound, Walker foxhound
\item 167: English foxhound
\item 206: curly-coated retriever
\item 219: cocker spaniel, English cocker spaniel, cocker
\item 220: Sussex spaniel
\item 221: Irish water spaniel
\item 238: Greater Swiss Mountain dog
\item 239: Bernese mountain dog
\item 240: Appenzeller
\item 241: EntleBucher
\item 244: Tibetan mastiff
\item 337: beaver
\item 341: hog, pig, grunter, squealer, Sus scrofa
\item 342: wild boar, boar, Sus scrofa
\item 343: warthog
\item 356: weasel
\item 357: mink
\item 358: polecat, fitch, foulmart, foumart, Mustela putorius
\item 359: black-footed ferret, ferret, Mustela nigripes
\item 360: otter
\item 365: orangutan, orang, orangutang, Pongo pygmaeus
\item 370: guenon, guenon monkey
\item 376: proboscis monkey, Nasalis larvatus
\item 378: capuchin, ringtail, Cebus capucinus
\item 380: titi, titi monkey
\item 383: Madagascar cat, ring-tailed lemur, Lemur catta
\item 384: indri, indris, Indri indri, Indri brevicaudatus
\item 386: African elephant, Loxodonta africana
\item 401: accordion, piano accordion, squeeze box
\item 420: banjo
\item 482: cassette player
\item 487: cellular telephone, cellular phone, cellphone, cell, mobile phone
\item 546: electric guitar
\item 574: golf ball
\item 592: hard disc, hard disk, fixed disk
\item 605: iPod
\item 647: measuring cup
\item 745: projector
\item 819: stage
\item 848: tape player
\item 852: tennis ball
\item 863: totem pole
\item 890: volleyball
\item 968: cup

\end{itemize}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/not_same_index.jpg}
    \caption{\textbf{Query image $\im_i$ (left column) and corresponding support $\mathcal{S}_{\text{win}}$ ranked by decreasing $p^{\text{win}}(\im_j | \im_i)$ (illustrated as a \textcolor{green}{green} bar on the bottom left of each image).} The query images are chosen such that the most similar image from the support is not the same as the query ($\im_i \neq \argmax_{\im_j}p^{\text{win}}(\im_j | \im_i)$). Metadata about the images is shown in \Cref{tab:firstisnotid}.}
    \label{fig:firstisnotid}
\end{figure*}

\begin{table*}[t]
\caption{\textbf{Metadata corresponding to \Cref{fig:firstisnotid}.} Each block corresponds to a row of \Cref{fig:firstisnotid}. Within a block, the first row denotes the image id, the second row denotes the class id and the last row denotes the sampling distribution $p^{\text{win}}(\im_j | \im_i)$.}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule
$\im_i$ (query)   & 1-NN    & 2-NN    & 3-NN    & 4-NN    & 5-NN    & 6-NN    & 7-NN    & 8-NN    & 9-NN    & 10-NN   \\
\midrule
313037  & 313303  & 313037  & 312150  & 312728  & 312960  & 312198  & 313277  & 312158  & 312230  & 312903  \\
244     & 244     & 244     & 244     & 244     & 244     & 244     & 244     & 244     & 244     & 244     \\
       & 0.41    & 0.36    & 0.04    & 0.04    & 0.04    & 0.03    & 0.02    & 0.02    & 0.02    & 0.02    \\
\midrule
1105805 & 1105953 & 1105805 & 1105673 & 1105826 & 1106178 & 1106303 & 1105027 & 1105739 & 1105747 & 1105980 \\
863     & 863     & 863     & 863     & 863     & 863     & 863     & 863     & 863     & 863     & 863     \\
       & 0.2     & 0.16    & 0.16    & 0.14    & 0.13    & 0.07    & 0.06    & 0.04    & 0.02    & 0.02    \\
\midrule
1140581 & 1139752 & 1140581 & 1139966 & 1139623 & 1140694 & 1140389 & 1139643 & 1140611 & 1140717 & 1139731 \\
890     & 890     & 890     & 890     & 890     & 890     & 890     & 890     & 890     & 890     & 890     \\
       & 0.21    & 0.18    & 0.14    & 0.11    & 0.1     & 0.09    & 0.06    & 0.06    & 0.03    & 0.02    \\
\midrule
213504  & 214625  & 213504  & 214919  & 213989  & 214090  & 213530  & 214654  & 214352  & 214537  & 214521  \\
166     & 167     & 166     & 167     & 166     & 166     & 166     & 167     & 167     & 167     & 167     \\
& 0.2     & 0.15    & 0.13    & 0.13    & 0.11    & 0.11    & 0.09    & 0.06    & 0.02    & 0.01    \\
\midrule
482666  & 483351  & 482666  & 482655  & 482983  & 482561  & 482824  & 483621  & 482626  & 482600  & 482467  \\
376     & 376     & 376     & 376     & 376     & 376     & 376     & 376     & 376     & 376     & 376     \\
& 0.3     & 0.29    & 0.1     & 0.06    & 0.06    & 0.05    & 0.03    & 0.03    & 0.03    & 0.03    \\
\midrule
439007  & 437919  & 439007  & 439041  & 440492  & 440579  & 496403  & 496313  & 179490  & 440440  & 439580  \\
342     & 341     & 342     & 342     & 343     & 343     & 386     & 386     & 138     & 343     & 343     \\
& 0.27    & 0.24    & 0.13    & 0.07    & 0.07    & 0.05    & 0.05    & 0.04    & 0.04    & 0.04    \\
\midrule
304802  & 307986  & 304802  & 305026  & 308855  & 308662  & 305875  & 305635  & 307400  & 305792  & 307371  \\
238     & 240     & 238     & 238     & 241     & 241     & 239     & 239     & 240     & 239     & 240     \\
& 0.32    & 0.2     & 0.11    & 0.09    & 0.09    & 0.08    & 0.04    & 0.03    & 0.03    & 0.02    \\
\midrule
86529   & 88177   & 86529   & 88271   & 88804   & 78641   & 88860   & 88405   & 86130   & 89098   & 86108   \\
66      & 68      & 66      & 68      & 68      & 60      & 68      & 68      & 66      & 68      & 66      \\
& 0.42    & 0.41    & 0.03    & 0.03    & 0.02    & 0.02    & 0.02    & 0.02    & 0.01    & 0.01    \\
\midrule
738207  & 1091457 & 738207  & 738128  & 738179  & 738186  & 737971  & 737433  & 737058  & 737339  & 738233  \\
574     & 852     & 574     & 574     & 574     & 574     & 574     & 574     & 574     & 574     & 574     \\
& 0.23    & 0.21    & 0.14    & 0.12    & 0.08    & 0.05    & 0.05    & 0.05    & 0.04    & 0.04    \\
\midrule
460081  & 461411  & 460081  & 461155  & 460028  & 459517  & 461393  & 459646  & 460305  & 459219  & 460791  \\
358     & 359     & 358     & 359     & 358     & 358     & 359     & 358     & 359     & 358     & 359     \\
& 0.37    & 0.35    & 0.08    & 0.05    & 0.04    & 0.03    & 0.02    & 0.02    & 0.02    & 0.02   \\
\bottomrule
\end{tabular}
}
\label{tab:firstisnotid}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/not_same_class.jpg}
    \caption{\textbf{Query image $\im_i$ (left column) and corresponding support $\mathcal{S}_{\text{win}}$ ranked by decreasing $p^{\text{win}}(\im_j | \im_i)$ (illustrated as a \textcolor{green}{green} bar on the bottom left of each image).} The query images are chosen such that the most similar image from the support is not from the same class as the query ($\texttt{class}(\im_i) \neq \texttt{class}(\argmax_{\im_j}p^{\text{win}}(\im_j | \im_i))$). Metadata about the images is shown in \Cref{tab:firstisnotclass}.}
    \label{fig:firstisnotclass}
\end{figure*}

\begin{table*}[]
\caption{\textbf{Metadata corresponding to \Cref{fig:firstisnotclass}.} Each block corresponds to a row of \Cref{fig:firstisnotclass}. Within a block, the first row denotes the image id, the second row denotes the class id and the last row denotes the sampling distribution $p^{\text{win}}(\im_j | \im_i)$.}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule
$\im_i$ (query)   & 1-NN    & 2-NN    & 3-NN    & 4-NN    & 5-NN    & 6-NN    & 7-NN    & 8-NN    & 9-NN    & 10-NN   \\
\midrule
493896 & 492056  & 493896 & 492267 & 493504 & 492574 & 492098  & 491860 & 493914 & 493160 & 492988 \\
384    & 383     & 384    & 383    & 384    & 383    & 383     & 383    & 384    & 384    & 384    \\
& 0.36    & 0.3    & 0.1    & 0.06   & 0.04   & 0.04    & 0.03   & 0.03   & 0.03   & 0.02   \\
\midrule
514556 & 540102  & 514556 & 539549 & 539488 & 539085 & 1049500 & 540080 & 539722 & 539428 & 701031 \\
401    & 420     & 401    & 420    & 420    & 420    & 819     & 420    & 420    & 420    & 546    \\
& 0.43    & 0.4    & 0.03   & 0.03   & 0.02   & 0.02    & 0.02   & 0.02   & 0.02   & 0.02   \\
\midrule
281630 & 282778  & 281630 & 263441 & 282465 & 263810 & 263486  & 281511 & 282504 & 280302 & 263945 \\
220    & 221     & 220    & 206    & 221    & 206    & 206     & 220    & 221    & 219    & 206    \\
& 0.34    & 0.29   & 0.08   & 0.08   & 0.06   & 0.04    & 0.03   & 0.03   & 0.03   & 0.02   \\
\midrule
307055 & 309205  & 307055 & 308671 & 307104 & 308841 & 307955  & 308794 & 309282 & 309246 & 309227 \\
240    & 241     & 240    & 241    & 240    & 241    & 240     & 241    & 241    & 241    & 241    \\
& 0.29    & 0.23   & 0.1    & 0.1    & 0.07   & 0.07    & 0.05   & 0.04   & 0.03   & 0.02   \\
\midrule
304813 & 308718  & 304813 & 304687 & 307947 & 308807 & 308163  & 308745 & 307713 & 308216 & 307930 \\
238    & 241     & 238    & 238    & 240    & 241    & 241     & 241    & 240    & 241    & 240    \\
& 0.35    & 0.32   & 0.11   & 0.08   & 0.03   & 0.03    & 0.02   & 0.02   & 0.02   & 0.02   \\
\midrule
107449 & 107748  & 103927 & 107449 & 106473 & 107398 & 106690  & 106705 & 106982 & 106536 & 106610 \\
82     & 83      & 80     & 82     & 82     & 82     & 82      & 82     & 82     & 82     & 82     \\
& 0.31    & 0.26   & 0.22   & 0.06   & 0.05   & 0.04    & 0.03   & 0.02   & 0.01   & 0.01   \\
\midrule
830394 & 1239824 & 830394 & 831098 & 830660 & 831378 & 831425  & 830417 & 831372 & 831192 & 830232 \\
647    & 968     & 647    & 647    & 647    & 647    & 647     & 647    & 647    & 647    & 647    \\
& 0.43    & 0.27   & 0.11   & 0.05   & 0.04   & 0.03    & 0.02   & 0.02   & 0.02   & 0.02   \\
\midrule
458411 & 457058  & 458411 & 283078 & 469050 & 432009 & 462780  & 461777 & 457794 & 462065 & 461770 \\
357    & 356     & 357    & 221    & 365    & 337    & 360     & 360    & 357    & 360    & 360    \\
& 0.47    & 0.4    & 0.02   & 0.02   & 0.02   & 0.02    & 0.01   & 0.01   & 0.01   & 0.01   \\
\midrule
619775 & 1086664 & 619775 & 760794 & 954774 & 777417 & 1085978 & 626568 & 776959 & 760171 & 759785 \\
482    & 848     & 482    & 592    & 745    & 605    & 848     & 487    & 605    & 592    & 592    \\
& 0.4     & 0.34   & 0.07   & 0.07   & 0.03   & 0.03    & 0.02   & 0.02   & 0.01   & 0.01   \\
\midrule
488570 & 485197  & 488570 & 486127 & 488201 & 485483 & 485652  & 485828 & 484997 & 485594 & 475405 \\
380    & 378     & 380    & 378    & 380    & 378    & 378     & 378    & 378    & 378    & 370    \\
& 0.39    & 0.28   & 0.07   & 0.06   & 0.05   & 0.04    & 0.03   & 0.03   & 0.02   & 0.02  \\
\bottomrule
\end{tabular}
}
\label{tab:firstisnotclass}
\end{table*}

