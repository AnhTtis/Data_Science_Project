% Datasets %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{wah2011caltech,
  title={The caltech-ucsd birds-200-2011 dataset},
  author={Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
  year={2011},
  publisher={California Institute of Technology}
}

@inproceedings{nilsback2006visual,
  title={A visual vocabulary for flower classification},
  author={Nilsback, M-E and Zisserman, Andrew},
  booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  volume={2},
  pages={1447--1454},
  year={2006},
  organization={IEEE}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{bossard2014food,
  title={Food-101--mining discriminative components with random forests},
  author={Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  booktitle={European conference on computer vision},
  pages={446--461},
  year={2014},
  organization={Springer}
}

@inproceedings{parkhi2012cats,
  title={Cats and dogs},
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, CV},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={3498--3505},
  year={2012},
  organization={IEEE}
}

@misc{pascal-voc-2007,
	author = "Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.",
	title = "The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults",
	howpublished = "http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html"
}

@misc{pascal-voc-2012,
	author = "Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.",
	title = "The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2012 {(VOC2012)} {R}esults",
	howpublished = "http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html"
}

% Related work %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
@inproceedings{mo2021object,
  title={Object-aware contrastive learning for debiased scene representation},
  author={Mo, Sangwoo and Kang, Hyunwoo and Sohn, Kihyuk and Li, Chun-Liang and Shin, Jinwoo},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}

@article{henaff2021efficient,
  title={Efficient visual pretraining with contrastive detection},
  author={H{\'e}naff, Olivier J and Koppula, Skanda and Alayrac, Jean-Baptiste and Oord, Aaron van den and Vinyals, Oriol and Carreira, Jo{\~a}o},
  journal={arXiv preprint arXiv:2103.10957},
  year={2021}
}

@article{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  journal={arXiv preprint arXiv:2104.14294},
  year={2021}
}



% Models %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

% SSL-meethods %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{chen2021exploring,
  title={Exploring simple siamese representation learning},
  author={Chen, Xinlei and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15750--15758},
  year={2021}
}

@article{he2021masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv preprint arXiv:2111.06377},
  year={2021}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}
@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9729--9738},
  year={2020}
}

@inproceedings{caron2020unsupervised,
  title={Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
  author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent: A new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and others},
  journal={arXiv preprint arXiv:2006.07733},
  year={2020}
}

@article{zhou2021ibot,
  title={iBOT: Image BERT Pre-Training with Online Tokenizer},
  author={Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
  journal={arXiv preprint arXiv:2111.07832},
  year={2021}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{liu2020self,
  title={Self-emd: Self-supervised object detection without imagenet},
  author={Liu, Songtao and Li, Zeming and Sun, Jian},
  journal={arXiv preprint arXiv:2011.13677},
  year={2020}
}

@article{asano2019self,
  title={Self-labelling via simultaneous clustering and representation learning},
  author={Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea},
  journal={arXiv preprint arXiv:1911.05371},
  year={2019}
}
% Base Algorithms %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{cuturi2013sinkhorn,
  title={Sinkhorn distances: Lightspeed computation of optimal transport},
  author={Cuturi, Marco},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={2292--2300},
  year={2013}
}

% Scene biases %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{singh2020don,
  title={Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias},
  author={Singh, Krishna Kumar and Mahajan, Dhruv and Grauman, Kristen and Lee, Yong Jae and Feiszli, Matt and Ghadiyaram, Deepti},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11070--11078},
  year={2020}
}

@article{xiao2020noise,
  title={Noise or signal: The role of image backgrounds in object recognition},
  author={Xiao, Kai and Engstrom, Logan and Ilyas, Andrew and Madry, Aleksander},
  journal={arXiv preprint arXiv:2006.09994},
  year={2020}
}

@article{yang2021contrastive,
  title={Contrastive Object-level Pre-training with Spatial Noise Curriculum Learning},
  author={Yang, Chenhongyi and Huang, Lichao and Crowley, Elliot J},
  journal={arXiv preprint arXiv:2111.13651},
  year={2021}
}

% Object discovery
@article{simeoni2021localizing,
  title={Localizing Objects with Self-Supervised Transformers and no Labels},
  author={Sim{\'e}oni, Oriane and Puy, Gilles and Vo, Huy V and Roburin, Simon and Gidaris, Spyros and Bursuc, Andrei and P{\'e}rez, Patrick and Marlet, Renaud and Ponce, Jean},
  journal={arXiv preprint arXiv:2109.14279},
  year={2021}
}

@article{wang2022self,
  title={Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut},
  author={Wang, Yangtao and Shen, Xi and Hu, Shell and Yuan, Yuan and Crowley, James and Vaufreydaz, Dominique},
  journal={arXiv preprint arXiv:2202.11539},
  year={2022}
}


% SSL semantic evalutations
@inproceedings{ji2019invariant,
  title={Invariant information clustering for unsupervised image classification and segmentation},
  author={Ji, Xu and Henriques, Joao F and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9865--9874},
  year={2019}
}

@article{hamilton2022unsupervised,
  title={Unsupervised Semantic Segmentation by Distilling Feature Correspondences},
  author={Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T},
  journal={arXiv preprint arXiv:2203.08414},
  year={2022}
}

@inproceedings{cho2021picie,
  title={Picie: Unsupervised semantic segmentation using invariance and equivariance in clustering},
  author={Cho, Jang Hyun and Mall, Utkarsh and Bala, Kavita and Hariharan, Bharath},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16794--16804},
  year={2021}
}

@article{ziegler2022self,
  title={Self-Supervised Learning of Object Parts for Semantic Segmentation},
  author={Ziegler, Adrian and Asano, Yuki M},
  journal={arXiv preprint arXiv:2204.13101},
  year={2022}
}

@article{yang2022concl,
  title={ConCL: Concept Contrastive Learning for Dense Prediction Pre-training in Pathology Images},
  author={Yang, Jiawei and Chen, Hanbo and Liang, Yuan and Huang, Junzhou and He, Lei and Yao, Jianhua},
  journal={arXiv preprint arXiv:2207.06733},
  year={2022}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}

@inproceedings{van2021unsupervised,
  title={Unsupervised semantic segmentation by contrasting object mask proposals},
  author={Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis, Stamatios and Van Gool, Luc},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10052--10062},
  year={2021}
}

@article{henaff2022object,
  title={Object discovery and representation networks},
  author={H{\'e}naff, Olivier J and Koppula, Skanda and Shelhamer, Evan and Zoran, Daniel and Jaegle, Andrew and Zisserman, Andrew and Carreira, Jo{\~a}o and Arandjelovi{\'c}, Relja},
  journal={arXiv preprint arXiv:2203.08777},
  year={2022}
}

@inproceedings{wang2022freesolo,
  title={FreeSOLO: Learning to Segment Objects without Annotations},
  author={Wang, Xinlong and Yu, Zhiding and De Mello, Shalini and Kautz, Jan and Anandkumar, Anima and Shen, Chunhua and Alvarez, Jose M},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14176--14186},
  year={2022}
}

@inproceedings{perazzi2016benchmark,
  title={A benchmark dataset and evaluation methodology for video object segmentation},
  author={Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Van Gool, Luc and Gross, Markus and Sorkine-Hornung, Alexander},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={724--732},
  year={2016}
}

@inproceedings{melas2022deep,
  title={Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization},
  author={Melas-Kyriazi, Luke and Rupprecht, Christian and Laina, Iro and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8364--8375},
  year={2022}
}

@inproceedings{wang2021dense,
  title={Dense contrastive learning for self-supervised visual pre-training},
  author={Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3024--3033},
  year={2021}
}

@inproceedings{yun2022patch,
  title={Patch-Level Representation Learning for Self-Supervised Vision Transformers},
  author={Yun, Sukmin and Lee, Hankook and Kim, Jaehyung and Shin, Jinwoo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8354--8363},
  year={2022}
}

@article{wei2021aligning,
  title={Aligning pretraining for detection via object-level contrastive learning},
  author={Wei, Fangyun and Gao, Yue and Wu, Zhirong and Hu, Han and Lin, Stephen},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22682--22694},
  year={2021}
}

@inproceedings{xie2021propagate,
  title={Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning},
  author={Xie, Zhenda and Lin, Yutong and Zhang, Zheng and Cao, Yue and Lin, Stephen and Hu, Han},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16684--16693},
  year={2021}
}

@inproceedings{xiao2021region,
  title={Region similarity representation learning},
  author={Xiao, Tete and Reed, Colorado J and Wang, Xiaolong and Keutzer, Kurt and Darrell, Trevor},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10539--10548},
  year={2021}
}

@article{xie2021unsupervised,
  title={Unsupervised object-level representation learning from scene images},
  author={Xie, Jiahao and Zhan, Xiaohang and Liu, Ziwei and Ong, Yew Soon and Loy, Chen Change},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28864--28876},
  year={2021}
}


@inproceedings{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
	file = {Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:/Users/tim/Library/Mobile Documents/com~apple~CloudDocs/pdfs/2020/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;Supplementary PDF:/Users/tim/Zotero/storage/SFCXDKND/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2022-09-16},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
	file = {He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:/Users/tim/Library/Mobile Documents/com~apple~CloudDocs/pdfs/2016/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf;Snapshot:/Users/tim/Zotero/storage/WRYXI9WN/He_Deep_Residual_Learning_CVPR_2016_paper.html:text/html},
}

@inproceedings{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-09-16},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	year = {2021},
	pages = {9650--9660},
	file = {Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:/Users/tim/Library/Mobile Documents/com~apple~CloudDocs/pdfs/2021/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:application/pdf;Snapshot:/Users/tim/Zotero/storage/XL2DRV8F/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html:text/html},
}

@inproceedings{dwibedi_little_2021,
	title = {With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations},
	isbn = {978-1-66542-812-5},
	shorttitle = {With a {Little} {Help} from {My} {Friends}},
	url = {https://www.computer.org/csdl/proceedings-article/iccv/2021/281200j568/1BmJDKW7JXq},
	doi = {10.1109/ICCV48922.2021.00945},
	abstract = {Self-supervised learning algorithms based on instance discrimination train encoders to be invariant to pre-defined transformations of the same instance. While most methods treat different views of the same image as positives for a contrastive loss, we are interested in using positives from other instances in the dataset. Our method, Nearest-Neighbor Contrastive Learning of visual Representations (NNCLR), samples the nearest neighbors from the dataset in the latent space, and treats them as positives. This provides more semantic variations than pre-defined transformations.We find that using the nearest-neighbor as positive in contrastive losses improves performance significantly on ImageNet classification using ResNet-50 under the linear evaluation protocol, from 71.7\% to 75.6\%, outperforming previous state-of-the-art methods. On semi-supervised learning benchmarks we improve performance significantly when only 1\% ImageNet labels are available, from 53.8\% to 56.5\%. On transfer learning benchmarks our method outperforms state-of-the-art methods (including supervised learning with ImageNet) on 8 out of 12 downstream datasets. Furthermore, we demonstrate empirically that our method is less reliant on complex data augmentations. We see a relative reduction of only 2.1\% ImageNet Top-1 accuracy when we train using only random crops.},
	language = {English},
	urldate = {2022-09-16},
	publisher = {IEEE Computer Society},
	author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
	month = oct,
	year = {2021},
	pages = {9568--9577},
	file = {Dwibedi et al. - 2021 - With a Little Help from My Friends Nearest-Neighb.pdf:/Users/tim/Library/Mobile Documents/com~apple~CloudDocs/pdfs/2021/Dwibedi et al. - 2021 - With a Little Help from My Friends Nearest-Neighb.pdf:application/pdf;Snapshot:/Users/tim/Zotero/storage/GCPHDTT7/1BmJDKW7JXq.html:text/html},
}

@inproceedings{wang_dense_2021,
	address = {Nashville, TN, USA},
	title = {Dense {Contrastive} {Learning} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578497/},
	doi = {10.1109/CVPR46437.2021.00304},
	abstract = {To date, most existing self-supervised learning methods are designed and optimized for image classiﬁcation. These pre-trained models can be sub-optimal for dense prediction tasks due to the discrepancy between image-level prediction and pixel-level prediction. To ﬁll this gap, we aim to design an effective, dense self-supervised learning method that directly works at the level of pixels (or local features) by taking into account the correspondence between local features. We present dense contrastive learning (DenseCL), which implements self-supervised learning by optimizing a pairwise contrastive (dis)similarity loss at the pixel level between two views of input images.},
	language = {en},
	urldate = {2022-09-20},
	booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	publisher = {IEEE},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	month = jun,
	year = {2021},
	pages = {3023--3032},
	file = {Wang et al. - 2021 - Dense Contrastive Learning for Self-Supervised Vis.pdf:/Users/tim/Library/Mobile Documents/com~apple~CloudDocs/pdfs/2021/Wang et al. - 2021 - Dense Contrastive Learning for Self-Supervised Vis.pdf:application/pdf},
}

@misc{chen_exploring_2020,
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2011.10566},
	abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {Chen, Xinlei and He, Kaiming},
	month = nov,
	year = {2020},
	note = {arXiv:2011.10566 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/7G579D28/Chen and He - 2020 - Exploring Simple Siamese Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/FTHSBPM6/2011.html:text/html},
}

@article{whatmakesforgoodviews,
  publtype={informal},
  author={Yonglong Tian and Chen Sun and Ben Poole and Dilip Krishnan and Cordelia Schmid and Phillip Isola},
  title={What makes for good views for contrastive learning},
  year={2020},
  cdate={1577836800000},
  journal={CoRR},
  volume={abs/2005.10243},
  url={https://arxiv.org/abs/2005.10243}
}

@article{dino,
  publtype={informal},
  author={Mathilde Caron and Hugo Touvron and Ishan Misra and Hervé Jégou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
  title={Emerging Properties in Self-Supervised Vision Transformers},
  year={2021},
  cdate={1609459200000},
  journal={CoRR},
  volume={abs/2104.14294},
  url={https://arxiv.org/abs/2104.14294}
}

@article{esvit,
  author    = {Chunyuan Li and
               Jianwei Yang and
               Pengchuan Zhang and
               Mei Gao and
               Bin Xiao and
               Xiyang Dai and
               Lu Yuan and
               Jianfeng Gao},
  title     = {Efficient Self-supervised Vision Transformers for Representation Learning},
  journal   = {CoRR},
  volume    = {abs/2106.09785},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.09785},
  eprinttype = {arXiv},
  eprint    = {2106.09785},
  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-09785.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{simsiam,
  author    = {Xinlei Chen and
               Kaiming He},
  title     = {Exploring Simple Siamese Representation Learning},
  journal   = {CoRR},
  volume    = {abs/2011.10566},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.10566},
  eprinttype = {arXiv},
  eprint    = {2011.10566},
  timestamp = {Wed, 25 Nov 2020 16:34:14 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-10566.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{moco,
  author    = {Kaiming He and
               Haoqi Fan and
               Yuxin Wu and
               Saining Xie and
               Ross B. Girshick},
  title     = {Momentum Contrast for Unsupervised Visual Representation Learning},
  journal   = {CoRR},
  volume    = {abs/1911.05722},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.05722},
  eprinttype = {arXiv},
  eprint    = {1911.05722},
  timestamp = {Mon, 02 Dec 2019 13:44:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-05722.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{byol,
      title={Bootstrap your own latent: A new approach to self-supervised Learning}, 
      author={Jean-Bastien Grill and Florian Strub and Florent Altché and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Bernardo Avila Pires and Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Koray Kavukcuoglu and Rémi Munos and Michal Valko},
      year={2020},
      eprint={2006.07733},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{pmlr-v119-chen20j,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1597--1607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20j.html},
  abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}

@InProceedings{hjelm2019learning,
author = {Hjelm, Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Philip and Trischler, Adam and Bengio, Yoshua},
title = {Learning deep representations by mutual information estimation and maximization},
organization = {ICLR},
booktitle = {ICLR 2019},
year = {2019},
month = {April},
abstract = {This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals.},
url = {https://www.microsoft.com/en-us/research/publication/learning-deep-representations-by-mutual-information-estimation-and-maximization/},
}

@inproceedings{NEURIPS2019_ddf35421,
 author = {Bachman, Philip and Hjelm, R Devon and Buchwalter, William},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Representations by Maximizing Mutual Information Across Views},
 url = {https://proceedings.neurips.cc/paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{swav,
  author    = {Mathilde Caron and
               Ishan Misra and
               Julien Mairal and
               Priya Goyal and
               Piotr Bojanowski and
               Armand Joulin},
  title     = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
  journal   = {CoRR},
  volume    = {abs/2006.09882},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.09882},
  eprinttype = {arXiv},
  eprint    = {2006.09882},
  timestamp = {Tue, 23 Jun 2020 17:57:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-09882.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pixpro,
    author    = {Xie, Zhenda and Lin, Yutong and Zhang, Zheng and Cao, Yue and Lin, Stephen and Hu, Han},
    title     = {Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {16684-16693}
}

@article{vit16x16,
  author    = {Alexey Dosovitskiy and
               Lucas Beyer and
               Alexander Kolesnikov and
               Dirk Weissenborn and
               Xiaohua Zhai and
               Thomas Unterthiner and
               Mostafa Dehghani and
               Matthias Minderer and
               Georg Heigold and
               Sylvain Gelly and
               Jakob Uszkoreit and
               Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
               at Scale},
  journal   = {CoRR},
  volume    = {abs/2010.11929},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.11929},
  eprinttype = {arXiv},
  eprint    = {2010.11929},
  timestamp = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{imagenet,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}
        
        
@InProceedings{resnet,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2019",
    url = "https://www.aclweb.org/anthology/N19-1423",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{gpt3,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  journal   = {CoRR},
  volume    = {abs/2005.14165},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.14165},
  eprinttype = {arXiv},
  eprint    = {2005.14165},
  timestamp = {Wed, 03 Jun 2020 11:36:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{all_you_need_is_attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {5998--6008},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{swin,
  author    = {Ze Liu and
               Yutong Lin and
               Yue Cao and
               Han Hu and
               Yixuan Wei and
               Zheng Zhang and
               Stephen Lin and
               Baining Guo},
  title     = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  journal   = {CoRR},
  volume    = {abs/2103.14030},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.14030},
  eprinttype = {arXiv},
  eprint    = {2103.14030},
  timestamp = {Thu, 08 Apr 2021 07:53:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-14030.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{cvt,
      title={CvT: Introducing Convolutions to Vision Transformers}, 
      author={Haiping Wu and Bin Xiao and Noel Codella and Mengchen Liu and Xiyang Dai and Lu Yuan and Lei Zhang},
      year={2021},
      eprint={2103.15808},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{vil,
      title={Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding}, 
      author={Pengchuan Zhang and Xiyang Dai and Jianwei Yang and Bin Xiao and Lu Yuan and Lei Zhang and Jianfeng Gao},
      year={2021},
      eprint={2103.15358},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ranftl2021vision,
      title={Vision Transformers for Dense Prediction}, 
      author={René Ranftl and Alexey Bochkovskiy and Vladlen Koltun},
      year={2021},
      eprint={2103.13413},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{Wang_2021_CVPR,
    author    = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
    title     = {Dense Contrastive Learning for Self-Supervised Visual Pre-Training},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {3024-3033}
}

@inproceedings{unsupervised_learning_of_dense_visual_rep,
  author={Pedro O. Pinheiro and Amjad Almahairi and Ryan Y. Benmalek and Florian Golemo and Aaron C. Courville},
  title={Unsupervised Learning of Dense Visual Representations},
  year={2020},
  cdate={1577836800000},
  url={https://proceedings.neurips.cc/paper/2020/hash/3000311ca56a1cb93397bc676c0b7fff-Abstract.html},
  booktitle={NeurIPS}
}

@InProceedings{noroozi_jigsaw,
author="Noroozi, Mehdi
and Favaro, Paolo",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="69--84",
abstract="We propose a novel unsupervised learning approach to build features suitable for object detection and classification. The features are pre-trained on a large dataset without human annotation and later transferred via fine-tuning on a different, smaller and labeled dataset. The pre-training consists of solving jigsaw puzzles of natural images. To facilitate the transfer of features to other tasks, we introduce the context-free network (CFN), a siamese-ennead convolutional neural network. The features correspond to the columns of the CFN and they process image tiles independently (i.e., free of context). The later layers of the CFN then use the features to identify their geometric arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. We pre-train the CFN on the training set of the ILSVRC2012 dataset and transfer the features on the combined training and validation set of Pascal VOC 2007 for object detection (via fast RCNN) and classification. These features outperform all current unsupervised features with {\$}{\$}51.8{\backslash},{\backslash}{\%}{\$}{\$}for detection and {\$}{\$}68.6{\backslash},{\backslash}{\%}{\$}{\$}for classification, and reduce the gap with supervised learning ({\$}{\$}56.5{\backslash},{\backslash}{\%}{\$}{\$}and {\$}{\$}78.2{\backslash},{\backslash}{\%}{\$}{\$}respectively).",
isbn="978-3-319-46466-4"
}

@InProceedings{zhang_colorization,
author="Zhang, Richard
and Isola, Phillip
and Efros, Alexei A.",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="Colorful Image Colorization",
booktitle="Computer Vision -- ECCV 2016",
address="Cham",
pages="649--666",
abstract="Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a ``colorization Turing test,'' asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32 {\%} of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.",
isbn="978-3-319-46487-9"
}

@article{gidaris_rotations,
  author    = {Spyros Gidaris and
               Praveer Singh and
               Nikos Komodakis},
  title     = {Unsupervised Representation Learning by Predicting Image Rotations},
  journal   = {CoRR},
  volume    = {abs/1803.07728},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.07728},
  archivePrefix = {arXiv},
  eprint    = {1803.07728},
  timestamp = {Mon, 13 Aug 2018 16:46:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-07728.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{inpainting,
  author    = {Deepak Pathak and
               Philipp Kr{\"{a}}henb{\"{u}}hl and
               Jeff Donahue and
               Trevor Darrell and
               Alexei A. Efros},
  title     = {Context Encoders: Feature Learning by Inpainting},
  journal   = {CoRR},
  volume    = {abs/1604.07379},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.07379},
  archivePrefix = {arXiv},
  eprint    = {1604.07379},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/PathakKDDE16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{predicting_noise, title = {Unsupervised Learning by Predicting Noise}, author = {Piotr Bojanowski and Armand Joulin}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {517--526}, year = {2017}, volume = {70}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/bojanowski17a/bojanowski17a.pdf}, url = {http://proceedings.mlr.press/v70/bojanowski17a.html}, abstract = {Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision; this paper introduces a generic framework to train such networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of the features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with the state-of-the-arts among unsupervised methods on ImageNet and Pascal VOC.} }

@INPROCEEDINGS{contex_pred_doersch,
  author={C. {Doersch} and A. {Gupta} and A. A. {Efros}},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Unsupervised Visual Representation Learning by Context Prediction}, 
  year={2015},
  volume={},
  number={},
  pages={1422-1430},
  doi={10.1109/ICCV.2015.167}}
  

@INPROCEEDINGS{contex_pred_Mundhenk,
  author={T. N. {Mundhenk} and D. {Ho} and B. Y. {Chen}},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Improvements to Context Based Self-Supervised Learning}, 
  year={2018},
  volume={},
  number={},
  pages={9339-9348},
  doi={10.1109/CVPR.2018.00973}}
  
 @article{simclrv2,
  author    = {Ting Chen and
               Simon Kornblith and
               Kevin Swersky and
               Mohammad Norouzi and
               Geoffrey E. Hinton},
  title     = {Big Self-Supervised Models are Strong Semi-Supervised Learners},
  journal   = {CoRR},
  volume    = {abs/2006.10029},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.10029},
  eprinttype = {arXiv},
  eprint    = {2006.10029},
  timestamp = {Tue, 23 Jun 2020 17:57:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-10029.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{mocov2,
  author  = {Xinlei Chen and Haoqi Fan and Ross Girshick and Kaiming He},
  title   = {Improved Baselines with Momentum Contrastive Learning},
  journal = {arXiv preprint arXiv:2003.04297},
  year    = {2020},
}

@article{mocov3,
  author    = {Xinlei Chen and
               Saining Xie and
               Kaiming He},
  title     = {An Empirical Study of Training Self-Supervised Vision Transformers},
  journal   = {CoRR},
  volume    = {abs/2104.02057},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.02057},
  eprinttype = {arXiv},
  eprint    = {2104.02057},
  timestamp = {Mon, 12 Apr 2021 16:14:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-02057.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{moby,
  author    = {Zhenda Xie and
               Yutong Lin and
               Zhuliang Yao and
               Zheng Zhang and
               Qi Dai and
               Yue Cao and
               Han Hu},
  title     = {Self-Supervised Learning with Swin Transformers},
  journal   = {CoRR},
  volume    = {abs/2105.04553},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.04553},
  eprinttype = {arXiv},
  eprint    = {2105.04553},
  timestamp = {Mon, 30 Aug 2021 15:14:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-04553.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{conditional_posenc,
  author    = {Xiangxiang Chu and
               Bo Zhang and
               Zhi Tian and
               Xiaolin Wei and
               Huaxia Xia},
  title     = {Do We Really Need Explicit Position Encodings for Vision Transformers?},
  journal   = {CoRR},
  volume    = {abs/2102.10882},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.10882},
  eprinttype = {arXiv},
  eprint    = {2102.10882},
  timestamp = {Thu, 12 Aug 2021 15:37:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-10882.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{focal_transformer,
  author    = {Jianwei Yang and
               Chunyuan Li and
               Pengchuan Zhang and
               Xiyang Dai and
               Bin Xiao and
               Lu Yuan and
               Jianfeng Gao},
  title     = {Focal Self-attention for Local-Global Interactions in Vision Transformers},
  journal   = {CoRR},
  volume    = {abs/2107.00641},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.00641},
  eprinttype = {arXiv},
  eprint    = {2107.00641},
  timestamp = {Wed, 07 Jul 2021 15:23:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-00641.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{relative_positional_encoding,
  author    = {Peter Shaw and
               Jakob Uszkoreit and
               Ashish Vaswani},
  title     = {Self-Attention with Relative Position Representations},
  journal   = {CoRR},
  volume    = {abs/1803.02155},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.02155},
  eprinttype = {arXiv},
  eprint    = {1803.02155},
  timestamp = {Mon, 13 Aug 2018 16:46:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-02155.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{
adamw,
title={Fixing Weight Decay Regularization in Adam},
author={Ilya Loshchilov and Frank Hutter},
year={2018},
url={https://openreview.net/forum?id=rk6qdGgCZ},
}

@article{linearlrrule,
  author    = {Priya Goyal and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick and
               Pieter Noordhuis and
               Lukasz Wesolowski and
               Aapo Kyrola and
               Andrew Tulloch and
               Yangqing Jia and
               Kaiming He},
  title     = {Accurate, Large Minibatch {SGD:} Training ImageNet in 1 Hour},
  journal   = {CoRR},
  volume    = {abs/1706.02677},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.02677},
  eprinttype = {arXiv},
  eprint    = {1706.02677},
  timestamp = {Mon, 13 Aug 2018 16:49:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoyalDGNWKTJH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cosine_schedule,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {{SGDR:} Stochastic Gradient Descent with Restarts},
  journal   = {CoRR},
  volume    = {abs/1608.03983},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.03983},
  eprinttype = {arXiv},
  eprint    = {1608.03983},
  timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LoshchilovH16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{instance-level-discrimination,
  author    = {Zhirong Wu and
               Yuanjun Xiong and
               Stella X. Yu and
               Dahua Lin},
  title     = {Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination},
  journal   = {CoRR},
  volume    = {abs/1805.01978},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.01978},
  eprinttype = {arXiv},
  eprint    = {1805.01978},
  timestamp = {Wed, 12 Aug 2020 11:07:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-01978.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{saillard2021self,
      title={Self supervised learning improves dMMR/MSI detection from histology slides across multiple cancers}, 
      author={Charlie Saillard and Olivier Dehaene and Tanguy Marchand and Olivier Moindrot and Aurélie Kamoun and Benoit Schmauch and Simon Jegou},
      year={2021},
      eprint={2109.05819},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@misc{ciga2021resource,
Author = {Ozan Ciga and Tony Xu and Anne L. Martel},
Title = {Resource and data efficient self supervised learning},
Year = {2021},
Eprint = {arXiv:2109.01721},
}


@misc{liu2021efficient,
      title={Efficient Training of Visual Transformers with Small-Size Datasets}, 
      author={Yahui Liu and Enver Sangineto and Wei Bi and Nicu Sebe and Bruno Lepri and Marco De Nadai},
      year={2021},
      eprint={2106.03746},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@InProceedings{food101,
author="Bossard, Lukas
and Guillaumin, Matthieu
and Van Gool, Luc",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Food-101 -- Mining Discriminative Components with Random Forests",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="446--461",
abstract="In this paper we address the problem of automatically recognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share knowledge among them. To improve efficiency of mining and classification, we only consider patches that are aligned with image superpixels, which we call components. To measure the performance of our rf component mining for food recognition, we introduce a novel and challenging dataset of 101 food categories, with 101'000 images. With an average accuracy of 50.76{\%}, our model outperforms alternative classification methods except for cnn, including svm classification on Improved Fisher Vectors and existing discriminative part-mining algorithms by 11.88{\%} and 8.13{\%}, respectively. On the challenging mit-Indoor dataset, our method compares nicely to other s-o-a component-based classification methods.",
isbn="978-3-319-10599-4"
}


@article{nct,
author = {Kather, Jakob and Krisam, Johannes and Charoentong, Pornpimol and Luedde, Tom and Herpel, Esther and Weis, Cleo-Aron and Gaiser, Timo and Marx, Alexander and Valous, Nek and Ferber, Dyke and Jansen, Lina and Reyes-Aldasoro, Constantino and Zoernig, Inka and Jäger, Dirk and Brenner, Hermann and Chang-Claude, Jenny and Hoffmeister, Michael and Halama, Niels},
year = {2019},
month = {01},
pages = {e1002730},
title = {Predicting survival from colorectal cancer histology slides using deep learning: A retrospective multicenter study},
volume = {16},
journal = {PLoS Medicine},
doi = {10.1371/journal.pmed.1002730}
}

@article{masked_image_modeling,
  author    = {Hangbo Bao and
               Li Dong and
               Furu Wei},
  title     = {BEiT: {BERT} Pre-Training of Image Transformers},
  journal   = {CoRR},
  volume    = {abs/2106.08254},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.08254},
  eprinttype = {arXiv},
  eprint    = {2106.08254},
  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-08254.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mae,
  author    = {Kaiming He and
               Xinlei Chen and
               Saining Xie and
               Yanghao Li and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick},
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  journal   = {CoRR},
  volume    = {abs/2111.06377},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.06377},
  eprinttype = {arXiv},
  eprint    = {2111.06377},
  timestamp = {Tue, 16 Nov 2021 12:12:31 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-06377.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2103-04814,
  author    = {Jian Ding and
               Enze Xie and
               Hang Xu and
               Chenhan Jiang and
               Zhenguo Li and
               Ping Luo and
               Gui{-}Song Xia},
  title     = {Unsupervised Pretraining for Object Detection by Patch Reidentification},
  journal   = {CoRR},
  volume    = {abs/2103.04814},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.04814},
  eprinttype = {arXiv},
  eprint    = {2103.04814},
  timestamp = {Wed, 02 Jun 2021 11:38:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-04814.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2102-08318,
  author    = {Ceyuan Yang and
               Zhirong Wu and
               Bolei Zhou and
               Stephen Lin},
  title     = {Instance Localization for Self-supervised Detection Pretraining},
  journal   = {CoRR},
  volume    = {abs/2102.08318},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.08318},
  eprinttype = {arXiv},
  eprint    = {2102.08318},
  timestamp = {Thu, 19 May 2022 16:00:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-08318.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2111-12309,
  author    = {Yufei Xu and
               Qiming Zhang and
               Jing Zhang and
               Dacheng Tao},
  title     = {RegionCL: Can Simple Region Swapping Contribute to Contrastive Learning?},
  journal   = {CoRR},
  volume    = {abs/2111.12309},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.12309},
  eprinttype = {arXiv},
  eprint    = {2111.12309},
  timestamp = {Thu, 28 Apr 2022 16:16:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-12309.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{maskrcnn,
  author    = {Kaiming He and
               Georgia Gkioxari and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick},
  title     = {Mask {R-CNN}},
  journal   = {CoRR},
  volume    = {abs/1703.06870},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.06870},
  eprinttype = {arXiv},
  eprint    = {1703.06870},
  timestamp = {Mon, 13 Aug 2018 16:46:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeGDG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.2111.11429,
  doi = {10.48550/ARXIV.2111.11429},
  
  url = {https://arxiv.org/abs/2111.11429},
  
  author = {Li, Y. and Xie, S. and Chen, X. and Dollar, P. and He, K. and Girshick, R.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Benchmarking Detection Transfer Learning with Vision Transformers},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@INPROCEEDINGS{pets_dataset,
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, C. V.},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Cats and dogs}, 
  year={2012},
  volume={},
  number={},
  pages={3498-3505},
  doi={10.1109/CVPR.2012.6248092}}

@article{krause_collecting_nodate,
	title = {Collecting a {Large}-{Scale} {Dataset} of {Fine}-{Grained} {Cars}},
	language = {en},
	author = {Krause, Jonathan and Deng, Jia and Stark, Michael and Fei-Fei, Li},
	pages = {2},
	file = {Krause et al. - Collecting a Large-Scale Dataset of Fine-Grained C.pdf:/Users/tim/Zotero/storage/UY96I8P5/Krause et al. - Collecting a Large-Scale Dataset of Fine-Grained C.pdf:application/pdf},
}

@INPROCEEDINGS{sun397,
  author={Xiao, Jianxiong and Hays, James and Ehinger, Krista A. and Oliva, Aude and Torralba, Antonio},
  booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, 
  title={SUN database: Large-scale scene recognition from abbey to zoo}, 
  year={2010},
  volume={},
  number={},
  pages={3485-3492},
  doi={10.1109/CVPR.2010.5539970}}
  
  @INPROCEEDINGS{caltech101,
  author={Li Fei-Fei and Fergus, R. and Perona, P.},
  booktitle={2004 Conference on Computer Vision and Pattern Recognition Workshop}, 
  title={Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories}, 
  year={2004},
  volume={},
  number={},
  pages={178-178},
  doi={10.1109/CVPR.2004.383}}
  
  @InProceedings{dtd_dataset,
author = {Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
title = {Describing Textures in the Wild},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2014}
}

@INPROCEEDINGS{flower_dataset,
  author={Nilsback, Maria-Elena and Zisserman, Andrew},
  booktitle={2008 Sixth Indian Conference on Computer Vision, Graphics \& Image Processing}, 
  title={Automated Flower Classification over a Large Number of Classes}, 
  year={2008},
  volume={},
  number={},
  pages={722-729},
  doi={10.1109/ICVGIP.2008.47}}
  
  @misc{aircraft_dataset,
  doi = {10.48550/ARXIV.1306.5151},
  
  url = {https://arxiv.org/abs/1306.5151},
  
  author = {Maji, Subhransu and Rahtu, Esa and Kannala, Juho and Blaschko, Matthew and Vedaldi, Andrea},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Fine-Grained Visual Classification of Aircraft},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{snell_prototypical_2017,
	title = {Prototypical {Networks} for {Few}-shot {Learning}},
	url = {http://arxiv.org/abs/1703.05175},
	abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
	urldate = {2022-10-23},
	publisher = {arXiv},
	author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
	month = jun,
	year = {2017},
	note = {arXiv:1703.05175 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/L74XJPFF/Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/CI3G7RAV/1703.html:text/html},
}

@misc{ericsson_how_2021,
	title = {How {Well} {Do} {Self}-{Supervised} {Models} {Transfer}?},
	url = {http://arxiv.org/abs/2011.13377},
	abstract = {Self-supervised visual representation learning has seen huge progress recently, but no large scale evaluation has compared the many models now available. We evaluate the transfer performance of 13 top self-supervised models on 40 downstream tasks, including many-shot and few-shot recognition, object detection, and dense prediction. We compare their performance to a supervised baseline and show that on most tasks the best self-supervised models outperform supervision, confirming the recently observed trend in the literature. We find ImageNet Top-1 accuracy to be highly correlated with transfer to many-shot recognition, but increasingly less so for few-shot, object detection and dense prediction. No single self-supervised method dominates overall, suggesting that universal pre-training is still unsolved. Our analysis of features suggests that top self-supervised learners fail to preserve colour information as well as supervised alternatives, but tend to induce better classifier calibration, and less attentive overfitting than supervised learners.},
	urldate = {2022-10-22},
	publisher = {arXiv},
	author = {Ericsson, Linus and Gouk, Henry and Hospedales, Timothy M.},
	month = mar,
	year = {2021},
	note = {arXiv:2011.13377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2021. Code available at https://github.com/linusericsson/ssl-transfer},
}

@misc{kornblith_better_2019,
	title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
	url = {http://arxiv.org/abs/1805.08974},
	abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (\$r = 0.99\$ and \$0.96\$, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
	urldate = {2022-10-23},
	publisher = {arXiv},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	month = jun,
	year = {2019},
	note = {arXiv:1805.08974 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: CVPR 2019 Oral},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/MMBSWB9K/Kornblith et al. - 2019 - Do Better ImageNet Models Transfer Better.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/6L9PPYS6/1805.html:text/html},
}

@article{Contrastive_Predictive_Coding,
  author    = {A{\"{a}}ron van den Oord and
               Yazhe Li and
               Oriol Vinyals},
  title     = {Representation Learning with Contrastive Predictive Coding},
  journal   = {CoRR},
  volume    = {abs/1807.03748},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.03748},
  eprinttype = {arXiv},
  eprint    = {1807.03748},
  timestamp = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sohn_improved_2016,
	title = {Improved {Deep} {Metric} {Learning} with {Multi}-class {N}-pair {Loss} {Objective}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sohn, Kihyuk},
	editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
	year = {2016},
}

@article{DBLP:journals/corr/abs-1805-01978,
  author    = {Zhirong Wu and
               Yuanjun Xiong and
               Stella X. Yu and
               Dahua Lin},
  title     = {Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination},
  journal   = {CoRR},
  volume    = {abs/1805.01978},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.01978},
  eprinttype = {arXiv},
  eprint    = {1805.01978},
  timestamp = {Wed, 12 Aug 2020 11:07:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-01978.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{improve_supcon,
  doi = {10.48550/ARXIV.2206.00384},
  
  url = {https://arxiv.org/abs/2206.00384},
  
  author = {Kim, Jaewon and Chang, Jooyoung and Park, Sang Min},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Generalized Supervised Contrastive Learning Framework},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{supcon,
  doi = {10.48550/ARXIV.2004.11362},
  
  url = {https://arxiv.org/abs/2004.11362},
  
  author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Supervised Contrastive Learning},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{deepcluster,
  author    = {Mathilde Caron and
               Piotr Bojanowski and
               Armand Joulin and
               Matthijs Douze},
  title     = {Deep Clustering for Unsupervised Learning of Visual Features},
  journal   = {CoRR},
  volume    = {abs/1807.05520},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.05520},
  eprinttype = {arXiv},
  eprint    = {1807.05520},
  timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-05520.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1905-01278,
  author    = {Mathilde Caron and
               Piotr Bojanowski and
               Julien Mairal and
               Armand Joulin},
  title     = {Leveraging Large-Scale Uncurated Data for Unsupervised Pre-training
               of Visual Features},
  journal   = {CoRR},
  volume    = {abs/1905.01278},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.01278},
  eprinttype = {arXiv},
  eprint    = {1905.01278},
  timestamp = {Mon, 27 May 2019 13:15:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-01278.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{clusterfit,
  author    = {Xueting Yan and
               Ishan Misra and
               Abhinav Gupta and
               Deepti Ghadiyaram and
               Dhruv Mahajan},
  title     = {ClusterFit: Improving Generalization of Visual Representations},
  journal   = {CoRR},
  volume    = {abs/1912.03330},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.03330},
  eprinttype = {arXiv},
  eprint    = {1912.03330},
  timestamp = {Mon, 28 Sep 2020 08:19:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-03330.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Koohpayegani_2021_ICCV,
    author    = {Koohpayegani, Soroush Abbasi and Tejankar, Ajinkya and Pirsiavash, Hamed},
    title     = {Mean Shift for Self-Supervised Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {10326-10335}
}

@article{DBLP:journals/corr/abs-2112-01390,
  author    = {Zelu Deng and
               Yujie Zhong and
               Sheng Guo and
               Weilin Huang},
  title     = {InsCLR: Improving Instance Retrieval with Self-Supervision},
  journal   = {CoRR},
  volume    = {abs/2112.01390},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.01390},
  eprinttype = {arXiv},
  eprint    = {2112.01390},
  timestamp = {Tue, 07 Dec 2021 12:15:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-01390.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}o

@inproceedings{stegmuller2023croc,
  title={CrOC: Cross-view online clustering for dense visual representation learning},
  author={Stegm{\"u}ller, Thomas and Lebailly, Tim and Bozorgtabar, Behzad and Tuytelaars, Tinne and Thiran, Jean-Philippe},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7000--7009},
  year={2023}
}


@InProceedings{Lebailly_2023_WACV,
    author    = {Lebailly, Tim and Tuytelaars, Tinne},
    title     = {Global-Local Self-Distillation for Visual Representation Learning},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2023},
    pages     = {1441-1450}
}

@inproceedings{wen2022slotcon,
  title={Self-Supervised Visual Representation Learning with Semantic Grouping},
  author={Wen, Xin and Zhao, Bingchen and Zheng, Anlin and Zhang, Xiangyu and Qi, Xiaojuan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{o2020unsupervised,
  title={Unsupervised learning of dense visual representations},
  author={O Pinheiro, Pedro O and Almahairi, Amjad and Benmalek, Ryan and Golemo, Florian and Courville, Aaron C},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4489--4500},
  year={2020}
}
