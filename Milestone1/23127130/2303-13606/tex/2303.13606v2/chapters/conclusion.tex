\section{Conclusion}
\label{sec:conclusion}
Self-distillation is becoming the go-to self-supervised learning paradigm due to its simplicity and state-of-the-art performance. However, non-explicit processing of negative pairs makes it less robust and more prone to collapse to trivial solutions than contrastive learning. Used in conjunction with bootstrapped positive pairs of neighbors, we empirically observe that self-distillation methods can perform worse than their vanilla baseline and in some cases even collapse. We propose an adaptive bootstrapping scheme that stabilizes the training and improves on the baselines. We also observe that long training schedules and larger backbones are particularly beneficial for AdaSim (better representations lead to better bootstrapping).

\noindent
{\bf Limitations} 
\label{par:limitations}
All results in the paper do not include multi-crop \cite{swav} for simplicity. In practice, not using multi-crop requires the use of more diverse random cropping (\eg with scale sampled in $[0.1, 1]$) but we have not changed any hyperparameters from DINO and stuck with $[0.25, 1]$.