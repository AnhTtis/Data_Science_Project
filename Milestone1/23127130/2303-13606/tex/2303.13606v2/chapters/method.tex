\section{Method}
\label{sec:method}

\input{figures/main_figure}


\subsection{Self-distillation vs contrastive learning}
\label{sec:selfdistillation-contrastive}
% old version
% Self-distillation and contrastive learning are the two of the most common learning scheme within self-supervised learning. Both schemes aim to learn features which are invariant to data-augmentations. This is done by enforcing similarity constraints between two augmentations of the same input image. They are similar in essence but differ in the way they avoid trivial solutions. Assume we dispose over a dataset $\mathcal{X}$ and an encoder $f$ from which we obtain a latent representation $\glob \in \mathcal{Z}$ of an image $\im \in \mathcal{X}$ via a forward pass of the image \ie $\glob = f(\im)$. Moreover, assume we dispose over an oracle $\mathcal{N}^+ \colon \mathcal{X} \to \mathcal{X}$ indicating valid positive pairs of images $(\im, \im^+)$ with $\im^+ \in \mathcal{N}^+(\im)$, an oracle $\mathcal{N}^- \colon \mathcal{X} \to \mathcal{X}$ indicating valid negative pairs of images $(\im, \im^-)$ with $\im^- \in \mathcal{N}^-(\im)$ and a distance metric\footnote{This is an abuse of terminology as $d$ does not necessarily have to satisfy all properties of a mathematical distance.} $d(\cdot \; , \; \cdot)$ defined in the latent space $\mathcal{Z}$. Valid positive pairs are image with the same semantic content and valid negative pairs are image with no shared semantic content. 

%a dataset $\mathcal{D} \subset \mathcal{X}$ and an

Self-distillation and contrastive learning are ubiquitous within self-supervised learning. Both schemes aim to learn discriminative features in the absence of labels. This is mainly done by enforcing similarity constraints between two augmentations of the same input image. The two methods are similar in essence but differ in the way they avoid trivial solutions. Assume we dispose of an encoder $f$ from which we obtain a latent representation $\glob \in \mathcal{Z}$ of an image $\im \in \mathcal{X}$, \ie $\glob = f(\im)$ with $\mathcal{Z}$ and $\mathcal{X}$ being a latent- and image space, respectively. Moreover, assume we dispose of an oracle $\mathcal{N}^+$ indicating valid positive pairs of images $(\im, \im^+) \in \mathcal{N}^+$, an oracle $\mathcal{N}^-$ indicating valid negative pairs of images $(\im, \im^-) \in \mathcal{N}^-$ and a distance metric\footnote{This is an abuse of terminology as $d$ does not necessarily have to satisfy all properties of a mathematical distance.} $d(\cdot \; , \; \cdot)$ defined in the latent space $\mathcal{Z}$. Valid positive pairs are images with the same semantic content and valid negative pairs are images with no shared semantic content. 



% \begin{equation}
%     \mathcal{L}_{contra} = F(d(f(\im), f(\im^+)), \{d(f(\im), f(\im^-)) \colon \im^- \in \mathcal{N}^-\})
% \end{equation}

%A contrastive loss (that gets minimized) is a decreasing function w.r.t. the positive terms and an increasing function w.r.t. the negative terms.
\noindent
{\bf Contrastive objective}
% \subsection{Contrastive objective}
A contrastive learning loss relies on attraction and repelling mechanisms: the former enforces similarity between positive pairs and the latter enforces dissimilarity between the negative pairs. Formally, the attraction term is of the form $d(f(\im), f(\im^+))$ and the repelling terms are of the form  $d(f(\im), f(\im^-))$. Here, we refer to ``term'' in its broad sense and therefore do not necessarily refer to an additive term. Usually, there are many negative terms for a single positive term. One famous example of such contrastive loss is the InfoNCE loss \cite{sohn_improved_2016,Contrastive_Predictive_Coding,DBLP:journals/corr/abs-1805-01978} defined as:
% \begin{equation}
%     \mathcal{L}_{\text{contra}} = -\log\left(\frac{\exp(f(\im)^\top f(\im^+))}{\exp(f(\im)^\top f(\im^+)) + \sum\limits_{\im^-\in \mathcal{S}^{\text{neg}}}{\exp(f(\im)^\top f(\im^-))}}\right)
%     \label{eq:info-nce}
% \end{equation}

\begin{equation}
    \mathcal{L}_{\text{contra}} = -\log\left(\frac{\exp(s^+/\tau)}{\exp(s^+/\tau) + \sum\limits_{s^-}{\exp(s^-/\tau)}}\right)
    \label{eq:info-nce}
\end{equation}
where $s^{+}=f(\im)^\top f(\im^+)$ and $s^{-}=f(\im)^\top f(\im^-)$. $(\im,\im^+)$ is sampled from $\mathcal{N}^+$ and $(\im,\im^-)$ are sampled from $\mathcal{N}^-$. The distance metric $d(\cdot \; , \; \cdot)$ is defined as the scalar product $\langle \cdot \; , \; \cdot \rangle$. The total contrastive objective is \Cref{eq:info-nce} summed over all training images $\im$.

\noindent
{\bf Self-distillation objective}
% \subsection{Self-distillation objective}
As opposed to the contrastive scenario, the self-distillation objective does not use negative image pairs to avoid the collapse to trivial solutions but uses asymmetry between the two branches. The form the asymmetry takes (momentum encoder, additional predictor on one branch, using stop-gradients in one branch \etc \cite{dino,simsiam,byol}) can be abstracted out. Given two encoders $f$ and $f'$, a self-distillation loss only has positive terms of the form% $d(f(\im), f'(\im^+))$. 

% In SimSiam \cite{simsiam}, $f'(\im) := h(f(\im))$ with $h \colon \mathcal{Z} \to \mathcal{Z}$ a predictor and $d$ is defined as:
% \begin{equation}
%     d(f(\im), f'(\im^+)) := -\frac{f(\im)^\top f'(\im^+)}{\norm{f(\im)}_2 \norm{f'(\im^+)}_2}
% \end{equation}

% In DINO \cite{dino}, $f'$ is a momentum encoder $f'(\im) := f_{\text{EMA}}(\im)$ whose weights are updated at the end of each epoch as follows: $\weight' \leftarrow \lambda \weight' + (1-\lambda) \weight$. $\weight$ and $\weight'$ are the weights of $f$ and $f'$, respectively. Given a head $h \colon \mathcal{Z} \to \mathcal{P}$ mapping the latent space discrete probability mass functions, and its exponential moving average equivalent $h'$, $d$ is defined as:

% \begin{equation}
%     d(f(\im), f'(\im^+)) := H(h(f(\im)), h'(f'(\im^+)))
% \end{equation}
% where $H$ is the cross entropy
% \begin{equation}
%     H(p, q) = -\sum_{i \in \mathcal{I}} p(i) \log q(i)
% \end{equation}
% and $\mathcal{I}$ is the support of the distributions $p$ and $q$, \ie $\mathcal{I}=[I]=\{1, 2, \cdots, I\}$.

% In self-distillation methods (\eg DINO and SimSiam), since there are no negative terms, the self-distillation objective is simply:
\begin{equation}
    \mathcal{L}_{distil} = d(f(\im), f'(\im^+))
    \label{eq:self-distil}
\end{equation}
for a given positive pair $(\im, \im^+)$. The total self-distillation objective is \Cref{eq:self-distil} summed over all positive pairs $(\im, \im^+) \in \mathcal{N}^+$.


\subsection{Bootstrapping neighbors in the latent space}
\label{sec:bootstrapping_neighbors}
In the absence of oracle $\mathcal{N}^+$ and $\mathcal{N}^-$, most (if not all) previous work approximate $\mathcal{N}^-$ with random image pairs. Given a distribution $\mathcal{T}$ of semantic preserving data augmentations, $\mathcal{N}^+$ is usually approximated with pairs of random augmentations from the same input image, \ie $(t(\im), t'(\im))$ where $t$ and $t'$ are sampled from $\mathcal{T}$. The stronger the semantic preserving augmentations $t$ and $t'$ are, the better the learned features become. However, their semantic preserving nature will be lost if they are made too strong. 

To obtain more complex and diverse pairs of positive images, NNCLR \cite{dwibedi_little_2021} proposes to approximate $\mathcal{N}^+$ with pairs of nearest neighbors. More precisely, given two latent representations ($\glob$ and $\glob'$) of the same image $\im$ and a FIFO queue $Q$ of previously computed representations (with $|Q| < |\mathcal{D}|$), positive pairs are defined as $(\glob', \text{NN}(\glob, Q))$, where the nearest neighbor operator is defined as:
\begin{equation}
    \text{NN}(\glob, Q) = \argmin_{q \in Q} \norm{z-q}_2
\end{equation}
Note here that the positive pairs are defined in the latent space $\mathcal{Z}$ and not in the image space. Under the assumption that the latent space properly captures the semantics of images, these pairs of neighbors are expected to share the same semantic content but their representation may still be slightly different. Enforcing similarity constraints between the two representations would help to learn features that are invariant to everything but the semantics of the image (\eg class label information). However, two issues arise when relying exclusively on nearest neighbors as positive pairs:
\begin{enumerate}
    \item \label{iss:one} Using only neighbors as positive pairs, \ie not relying on augmented views as positive pairs, leaves out valuable self-supervisory signal. Using standard positive pairs of augmented views from the same image is desirable to explicitly learn data-augmentation invariant features, but that is not enforced.
    \item \label{iss:two} The latent space might not capture the semantics of the image well, \ie the positive pair is wrong. This would lead to undesirable gradient flows, \eg pulling an image of a cat closer to an image of a building.
\end{enumerate}

Throughout the paper, we refer to the above as issue \ref{iss:one} and issue \ref{iss:two}. Using a contrastive objective, the impact of these issues is limited since informative gradient signal can still be obtained from the negative pairs which in practice are almost always correct (random). Using a self-distillation objective, we empirically observe that the above issues are problematic to the point that the downstream performance can be worse than using standard positive pairs using data-augmentations (\cref{sec:results}, \cref{tab:main_table}, \cref{tab:few_shot}).

\subsection{Adaptive similarity bootstrapping}
\label{sec:adaptive_similarity_bootstrapping}
\subsubsection{Need for standard positive pairs}
\label{sec:need_for_standard_positive_pairs}

To avoid issue \ref{iss:one}, we adaptively use augmentations of the same image or of a neighbor to form a positive pair. To do this, we propose to work with a cache that has the same size as the dataset $\mathcal{D}$ as opposed to using the queue $Q$ from NNCLR \cite{dwibedi_little_2021}. Using a small queue, it is very unlikely to encounter a representation originating from the same image. We denote the cache with $\Z \in \mathbb{R}^{N \times d}$, where $N$ is the size of the dataset and $d$ is the dimension of the latent space. At the end of the forward pass, the current latent representation $\glob_i$ of an augmentation of the $i$-th image $\im_i \in \mathcal{D}$ (\ie $f(t(\im_i))$ with $t \sim \mathcal{T}$) is updated in the cache. As such, $\Z$ holds a latent representation for every image in the dataset at all times. Given a latent representation $\glob_i$ of the $i$-th image and the cache $\Z$, we can define a similarity metric $m_i(j)$ between image $i$ and all images $\im_j$:
\begin{equation}
    m_i(j) = \glob_i^\top \Z_j
\end{equation}
where $\Z_j$ refers to the latent representation of image $j$ in the cache. $m_i(j)$ can in turn be mapped into a similarity distribution using a softmax normalization:

\begin{equation}
    s_i(j) = \frac{\exp{\left(m_i(j)\right / \tau)}}{\sum_{k\in [|\mathcal{D}|]} \exp{\left(m_i(k) / \tau\right)}}, \quad \quad i,j \in [|\mathcal{D}|]
    \label{eq:similarity_distribution}
\end{equation}
where $\tau$ is a temperature parameter modulating the sharpness of the distribution (ablation in \cref{tab:ablation}). We can now define an isomorphic probability distribution over the images in the dataset:

\begin{equation}
    p(\im_j | \im_i) = s_i(j), \quad \quad \im_i,\im_j \in \mathcal{D}%^2
    \label{eq:p_im_dataset}
\end{equation}

%

To approximate the oracle $\mathcal{N}^+$ of positive pairs, we propose to use image $i$ and an image sampled from the similarity distribution. That is, we form positive pairs of the form $(t(\im_i), t'(\im_{j^\star}))$ with $\im_{j^\star}$ sampled from $p(\im_j | \im_i)$ and with $t$ and $t'$ sampled from $\mathcal{T}$. Note that $\Z$ contains features for all images, not excluding image $\im_i$. Therefore, we always have a non-zero probability of having a positive pair generated from the same input image which mitigates issue \ref{iss:one} from \Cref{sec:bootstrapping_neighbors}. Sampling positive pairs of the form $(t(\im_i), t'(\im_{j^\star}))$ also allows for the possibility to sample more diverse and complex pairs of positives compared to the case when we only consider top-1 neighbors, as can be seen in \Cref{fig:query_support}. This diversity can be increased by increasing the temperature $\tau$.

% The probability to sample a positive pair of the form $(t(\im), t'(\im))$ can be negligibly low. To solve this, we impose that $()$

\subsubsection{Need for adaptivity}
\label{sec:need_for_adaptivity}
Recall that issue \ref{iss:two} from \Cref{sec:bootstrapping_neighbors} is that the latent space might not capture the semantics of images properly (especially at the beginning of the pretraining). That is, neighbors in the latent space might have completely unrelated semantic content. We propose to estimate the quality of the latent space by observing how close two different augmentations of the same input image $\im_i$ are mapped via the encoder $f$. If this distance is low compared to that of the latent representations of other images in the cache $\Z$, then it means that the encoder $f$ is good at mapping images similar to $\im_i$ close together. In that case, we can expect the vicinity of the queried image $\im_i$ to also share semantic content with image $\im_i$ and can therefore use elements of the vicinity to form a positive pair with $\im_i$. If this distance is too high, we default to a standard positive pair composed of two augmentations of the same input image. Mathematically, if $\argmax_{\im_j} p(\im_j | \im_i) == \im_i$, then we sample $\im_{j^\star}$ from $p(\im_j | \im_i)$ and use a positive pair $(t(\im_i), t'(\im_{j^\star}))$ with $t$ and $t'$ sampled from $\mathcal{T}$. Otherwise, we use a standard positive pair $(t(\im_i), t'(\im_i))$.

% \TODO{mention something about this decision being instance specific.}

\subsubsection{How to rank neighbors?}
We propose to extend the adaptive framework to account for the similarity history over the past epochs. The rationale behind this is that the similarity between two images $t(\im_i)$ and $t'(\im_j)$ can be strongly affected by $t$ and $t'$, especially at the beginning of the pretraining. For example, given a randomly initialized encoder $f$, the similarity between $f(t(\im_i))$ and $f(t'(\im_j))$ will be mostly determined by how similar $t$ and $t'$ are. Therefore, an image $\im_i$ and $\im_j$ should be considered as semantically close, not only if $f(t(\im_i))$ is close to $f(t'(\im_j))$, but if $\mathbb{E}_{t \sim \mathcal{T}}[f(t(\im_i))]$ is close to $\mathbb{E}_{t' \sim \mathcal{T}}[f(t'(\im_j))]$.

In practice, we do not have access to the true expectation and therefore take the empirical mean over the last $w$ epochs. More precisely, we define the similarity metric for a given epoch $e$ which we denote with the superscript $^{(e)}$:
\begin{equation}
    m_i^{(e)}(j) = \left(\glob_i^\top \Z_j\right)^{(e)}
    \label{eq:m_i_j^e}
\end{equation}
and average this similarity metric over the last $w$ epochs to obtain a windowed similarity metric for the current epoch $E$:

\begin{equation}
    m_i^\text{win}(j) = \frac{1}{w}\sum\limits_{e \in \mathcal{W}_E^w}m_i^{(e)}(j)
    \label{eq:windowed_sim_metric}
\end{equation}
%
where $\mathcal{W}_E^w = \{E-w+1, E-w+2, \cdots E\}$ denotes the set of the previous $w$ epochs with epoch $E$ being the current epoch. Similarly to \Cref{eq:similarity_distribution}, we can define:

\begin{equation}
    s_i^\text{win}(j) = \frac{\exp{\left(m_i^\text{win}(j)\right / \tau)}}{\sum_{k\in [|\mathcal{D}|]} \exp{\left(m_i^\text{win}(k) / \tau\right)}}, \quad \quad i,j \in [|\mathcal{D}|]
    \label{eq:similarity_distribution_windowed}
\end{equation}
%
where $\tau$ is a temperature parameter as in \Cref{eq:similarity_distribution}. And similarly to \Cref{eq:p_im_dataset}, we can define:
\begin{equation}
    p^{\text{win}}(\im_j | \im_i) = s_i^\text{win}(j), \quad \quad \im_i,\im_j \in \mathcal{D}%^2
    \label{eq:p_im_dataset_windowed}
\end{equation}

From here on, we use $p^{\text{win}}(\im_j | \im_i)$ instead of $p(\im_j | \im_i)$ as the sampling distribution. At the beginning of the pretraining, \ie as long as no $w$ similarity metrics have been computed yet, we default to using standard positive pairs generated from augmented views of the same image. Note that for a window of size 1 ($w=1$), we fall back to \Cref{eq:similarity_distribution} and \Cref{eq:p_im_dataset} from \Cref{sec:adaptive_similarity_bootstrapping}, \ie $s_i(j) = s_i^\text{win}(j)$ and $p(\im_j | \im_i)=p^{\text{win}}(\im_j | \im_i)$.

\begin{algorithm}[t]
	\caption{\texttt{AdaSim}: Adaptive Similarity Bootstrapping framework} 
    \textbf{Input:} $\mathcal{D}$: an unlabeled dataset, $\mathcal{T}$: a distribution over the possible augmentations, $f$: an encoder parametrized with weights $\weight$, \texttt{OPTIMIZER}: an optimizer, $\Z \in \mathbb{R}^{N \times d}$: a zero-initialized cache ($N=|\mathcal{D}|$ and $d$ is the dimension of the latent space), $w$: window size, $L$: self-distillation loss\\
    \textbf{Output:} Trained weights 
	\begin{algorithmic}[1]
	\For{$e \in \{1, 2, \cdots \text{NB\_EPOCHS}\}$}
		
		\For {$i \in [|\mathcal{D}|]$}
		    \State Sample $t$ and $t'$ from $\mathcal{T}$
		  %  \State $\aug_{i} = t(\im_i)$
		    \State $\rep_{i} = f(t(\im_i))$
		    \State $m_i^{(e)}(j) = \left(\glob_i^\top \Z_j\right)^{(e)}$ \Comment{\cref{eq:m_i_j^e}}
		    \State \texttt{update}$(\Z, \rep_{i})$ \Comment{Update cache with $\rep_i$}
		    
    	    \If{$e \leq w$}
    		    \State $\mathcal{L} = L(t(\im_i), t'(\im_i))$
    	    \Else 
    	        \State $m_i^{\text{win}}(j) = \frac{1}{w}\sum\limits_{e' \in \mathcal{W}_e^w}m_i^{(e')}(j)$ \Comment{\cref{eq:windowed_sim_metric}}
    	        \State $s_i^{\text{win}}(j) = \frac{\exp{\left(m_i^{\text{win}}(j) / \tau\right)}}{\sum_k \exp{\left(m_i^{\text{win}}(k) / \tau\right)}}$  \Comment{\cref{eq:similarity_distribution_windowed}}
    	        \State $p^{\text{win}}(\im_j | \im_i) = s_i^{\text{win}}(j)$ \Comment{\cref{eq:p_im_dataset_windowed}}
		  %  \If{$i == \argmax\limits_{j \in \bigcup\limits_{e\in\mathcal{W}_E^w} \mathcal{S}^{(e)}}{s^{\text{win}}(j)}$}
		    \If{$\im_i == \argmax\limits_{\im_j}{p^{\text{win}}(\im_j | \im_i)}$}  
		        \State Sample $\im_{j^\star}$ from $p^{\text{win}}$
          \State $\mathcal{L} = L(t(\im_i), t'({\im_{j^\star}}))$ \hspace*{3.5em} \rlap{\smash{$\left.\begin{array}{@{}c@{}}\\{}\\{}\\{}\\{}\\{}\end{array}\right\}%
          \begin{tabular}{c}Adaptive\\sampling\end{tabular}$}}
		    \Else
		        \State $\mathcal{L} = L(t(\im_i), t'(\im_i))$
		    \EndIf
    		  %  \State Sample $t'$ from $\mathcal{T}$
    		  %  \State $\mathcal{L} = L(t(\im_i), t'({\im_{j^\star}}))$
    	    \EndIf
		    \State $\weight \leftarrow$ \texttt{OPTIMIZER}$(\weight, \nabla_{\weight}\mathcal{L})$
		    
		\EndFor
% 		\State $\weight_t \leftarrow \lambda \weight_t + (1-\lambda) \weight_s$
	\EndFor
	\State \Return $\weight$
	\end{algorithmic}
	\label{alg:adasim}
\end{algorithm}


\subsection{Memory and compute overhead}
\noindent
{\bf Memory overhead} In practice storing $w$ versions of $m_i^{(e)}(j)$ with $e \in \mathcal{W}_E^w$ is not feasible when the dataset is large as it would require storing $w$ entries for each pair of images. In the case of ImageNet-1k \cite{imagenet}, that would require about $(1.3\text{M})^2 \times 4 \times w \sim 300$TB which is infeasible\footnote{$1.3\text{M}$ refers to the size of the dataset and 4 bytes are required to store a single float entry of 32 bits.}. However, since we sample from the similarity distribution to form positive pairs, we are only interested in the most similar images. Therefore, we can restrict the support of $m_i^{(e)}(j)$ to the $K$ highest elements. We denote this new support as $\mathcal{S}^{(e)}$ with $|\mathcal{S}^{(e)}|=K$. Note that for every epoch $e$, the support of $m_i^{(e)}(j)$ is different. The similarity metric $m_i^\text{win}(j)$ from \Cref{eq:windowed_sim_metric} with restricted domain is obtained as follows:

\begin{equation}
    m_i^\text{win}(j) = \frac{1}{w}\sum\limits_{e \in \mathcal{W}_E^w}\mathbbm{1}_{\{j\in \mathcal{S}^{(e)}\}}m_i^{(e)}(j)
\end{equation}
%
where $\mathbbm{1}$ denotes the indicator function and with $j \in \mathcal{S}_{\text{union}}$ and  $\mathcal{S}_{\text{union}} = \bigcup_{e\in\mathcal{W}_E^w} \mathcal{S}^{(e)}$. The only difference for the similarity distribution from \Cref{eq:similarity_distribution_windowed} is that its support is limited to $\mathcal{S}_{\text{union}}$. Similarly for \Cref{eq:p_im_dataset_windowed}, the only difference is that its support is limited to $\mathcal{S}_{\text{win}} = \{\im_j : j \in \mathcal{S}_{\text{union}}\}$. Taking all the above into consideration, the final algorithm AdaSim is illustrated in \Cref{alg:adasim}.

\noindent
{\bf Compute overhead} The compute overhead is limited to the projection of a representation $\glob$ onto the cache $\Z$ which is embarrassingly parallelizable on GPU. This requires about $d|\mathcal{D}|=0.5$B operations (for ViT-S/16) which is much less than the 4.6B FLOPs in the backbone (see \cref{sec:runtime}).

% \noindent
% {\bf Runtime analysis} We compare the runtime for baseline DINO-2 \cite{dino} and DINO-2 + AdaSim. We observe a small time delta for one iteration (batchsize=1024, on 8x AMD MI250X): \TODO{1ms} and \TODO{5 ms}, respectively.


% \subsection{Implementation details}

% \TODO{maybe mention compute overhead?}

% \TODO{
% \begin{enumerate}
%     \item repasser l'image utile ou pas? tester de faire sans repasser
%     \item if it works better, why?
%     \begin{enumerate}
%         \item network state is changing too fast?
%         \item or proxy task is harder to solve?
%     \end{enumerate}
% \end{enumerate}
% }

% 1) figure, , 3) ensuite on check si ca marche mieux ou moins bien, et on regarde pourquoi c'est le cas (on apply un gros transform (le mm) a deux image diff et on check le topk. on check les topk, plusieurs fois pour la mm image et on regarde si ca change. 4) take checkpoint 20 et 21 forward pass tout sans dataaug. 5) maybe queue
% \begin{algorithm}[t]
% 	\caption{\texttt{AdaSim} framework \TODO{directly formulate with window and with representations from previous epochs?}} 
%     \textbf{Input:} $\mathcal{D}$: an unlabeled dataset, $\mathcal{T}$: a distribution over the possible augmentations, $f$: an encoder parametrized with weights $\weight$, \texttt{OPTIMIZER}: an optimizer\\
%     \textbf{Output:} Trained weights 
% 	\begin{algorithmic}[1]
% 	\For{$e \in \{1, 2, \cdots \text{NB\_EPOCHS}\}$}
% 		\For {$i \in [|\mathcal{X}|]$}
% 		    \State Sample $t_i$ from $\mathcal{T}$
% 		    \State $\aug_i = t_i(\im_i)$
% 		    \State $\rep_i = f_t(\aug_i)$
% 		\EndFor
% 		\For {$i \in [|\mathcal{X}|]$}
% 		    \State Sample $t_{i}^{'}$ from $\mathcal{T}$
% 		    \State $\aug_{i}^{'} = t_{i}^{'}(\im_i)$
% 		    \State $\rep_{i}^{'} = f_t(\aug_{i}^{'})$
% 		    \State $s(j) = \frac{\exp{\left(\rep_j^\top \rep_{i}^{'} / \tau\right)}}{\sum_j \exp{\left(\rep_j^\top \rep_{i}^{'} / \tau\right)}}$
% 		    \If{$i == \argmax_j{s(j)}$}
% 		        \State Sample $j^\star$ from $s(j)$
% 		    \Else 
% 		        \State $j^\star = i$
% 		    \EndIf
% 		    \State Sample $t^{(1)}, t^{(2)}$ from $\mathcal{T}$
% 		    \State $\mathcal{L} = L(t^{(1)}(\im_i), t^{(2)}({\im_{j^\star}}))$
% 		    \State $\weight_s \leftarrow$ \texttt{OPTIMIZER}$(\weight_s, \nabla_{\weight_s}\mathcal{L})$
% 		\EndFor
% 		\State $\weight_t \leftarrow \lambda \weight_t + (1-\lambda) \weight_s$
% 	\EndFor
% 	\State \Return $\weight_t$
% 	\end{algorithmic}
% 	\label{alg:adasim}
% \end{algorithm}





