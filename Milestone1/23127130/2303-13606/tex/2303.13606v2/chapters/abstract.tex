Most self-supervised methods for representation learning leverage a cross-view consistency objective \ie they maximize the representation similarity of a given image's augmented views. Recent work NNCLR goes beyond the cross-view paradigm and uses positive pairs from different images obtained via nearest neighbor bootstrapping in a contrastive setting. We empirically show that as opposed to the contrastive learning setting which relies on negative samples, incorporating nearest neighbor bootstrapping in a self-distillation scheme can lead to a performance drop or even collapse. We scrutinize the reason for this unexpected behavior and provide a solution. We propose to adaptively bootstrap neighbors based on the estimated quality of the latent space. We report consistent improvements compared to the naive bootstrapping approach and the original baselines. Our approach leads to performance improvements for various self-distillation method/backbone combinations and standard downstream tasks. Our code is publicly available at \texttt{\color{magenta}{https://github.com/tileb1/AdaSim}}.
% We empirically show that such scheme stabilizes the training and leads to performance improvements both on standard downstream $k$-NN/linear evaluations and few-shot transfer. 

%\ie they maximize the similarity between representations of two augmentations of the same image.

% This gives features which are invariant to standard data augmentation. 


% In the presence of an oracle outputting valid positive pairs (\ie pairs of images with similar semantic content), the self-distillation learning scheme could be improved. Using positive pairs from the oracle would result in features which are not only invariant to data augmentations but invariant to everything but the semantics of the image. We propose to proxy such oracle using the structure of the latent space in a self-distillation setting. Empirically, we observe that straightforward nearest neighbor bootstrapping can be hurtful and lead to collapse when used in conjunction with a self-distillation objective. 


% As opposed to previous work, we do not propose a scheme which deterministically bootstraps a nearest neighbor but instead adaptively bootstrap a neighbor sampled from a set of ranked neighbors based on their estimated semantic similarity. 



% 1) bootstrapping / constrastive works
% 2) self distillation is good (but doesn't work with nn bootstrapping)
% 3) we propose adaptive
% between the augmented views of a given image.
% 4) improvements