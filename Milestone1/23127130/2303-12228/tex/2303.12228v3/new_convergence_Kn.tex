
\section{Sub-ballistic strong law of large numbers for $p_n$-\Nametwo{}}\label{sec:SLLN_X}
In this section, we prove Theorem~\ref{thm:conv-Xbeta}.
Henceforth, let  $X$ be a $p_n$-\Nametwo{} in direction $\ell$ on $\ZZ^d$ with $d\geq 2$, $p_n= \mathcal{C} n^{-\beta} \wedge 1$ and $\beta\in (0,1)$; without loss of generality, we  assume $\CC = 1$, i.e., $p_n=n^{-\beta}$. 
%

Let us denote by $K^{X}_n$ the set of times from $0$ to $n-1$ in which $X$ visits a site for the first time and becomes excited.  We can write $K^{X}_n$ as
\begin{equation*}
K^{X}_n = \big\{ i \in \{1, 2, \dots, n\} : \um_{E_{i-1}^c \cap \{ U_i \leq i^{-\beta}\}} =1 \big\}\,.  
\end{equation*}

If $\{\psi_i\}_{i \ge 1}$ denotes  the sequence of $\FF$-stopping times  corresponding to the  times the $p_n$-\Nametwo{} visits a new site, then setting $\varphi_i := \psi_i + 1$, we have that
\begin{equation}\label{eq: def Kn}
|K^{X}_n| = \sum_{i=1}^{n} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-\beta}\}} = \sum_{j=1}^{|\Rr_{n-1}^X|} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}}\,. 
\end{equation}
When clear from the context,  we shall omit the dependence on $X$ and simply write $|K_n|$ rather than $|K^{X}_n|$. 
The key ingredient in the proof of Theorem~\ref{thm:conv-Xbeta} is the following result.


% \begin{proposition}\label{conv-Kbeta} 
% Let  $X$ be a $p_n$-\Nametwo{} in direction $\ell$ on $\ZZ^d$ with $d\geq 2$, $p_n=n^{-\beta}$ and $\beta \in (0,1)$. 
% Assume that $X$ satisfies Hypothesis~\ref{hyp}. Then, it holds that 
% $$
% \lim_{n\to \infty} \frac{|K_{n}|}{n^{1 - \beta}} = \frac{\pi_d}{1-\beta} \quad \text{ a.s.}\,.
% $$
% \end{proposition}


\begin{proposition}\label{conv-Kbeta} 
Let  $X$ be a $p_n$-\Nametwo{} in direction $\ell$ on $\ZZ^d$ with $d\geq 2$, $p_n=n^{-\beta}$ and $\beta \in (0,1)$. Then, it holds that 
\begin{equation}\label{eq:UppK_n}
\limsup_{n\to \infty} \frac{|K^{X}_n|}{n^{1 - \beta}} \leq \frac{\pi_d}{1-\beta} \,, \quad  \text{ a.s.}\,.    
\end{equation}
Moreover, assuming that $X$ satisfies Hypothesis~\ref{hyp}, then 
\begin{equation}\label{eq:convK_n}
\lim_{n\to \infty} \frac{|K^{X}_n|}{n^{1 - \beta}} = \frac{\pi_d}{1-\beta} \,, \quad \text{ a.s.}\,.
\end{equation}
\end{proposition}


% Before proving Proposition~\ref{conv-Kbeta} let us introduce some auxiliary results. 
%
\begin{proof}[Proof of Theorem~\ref{thm:conv-Xbeta}]
Define 
\begin{equation*}
J_{n} := \sum_{i=1}^{n} \um_{E_{i-1}^c \cap \{ U_i \leq   i^{-\beta}\}} (\gamma_i - \xi_i) \,.
\end{equation*}
Then, $X_n$ defined in ~\eqref{xn-incremnto1} can be rewritten as 
\begin{equation*}
X_n  = \sum_{i=1}^n \big( \um_{ E_{i-1} \cup \{ U_i > i^{-\beta}\}} \xi_i + \um_{E_{i-1}^c \cap \{ U_i \leq i^{-\beta}\}} \gamma_i \big) = \sum_{i=1}^n  \xi_i + J_n \,.
\end{equation*}
By Theorem 17.3.II.A in \cite{loeve} (page 250) we have 
$$
    \lim_{n\to\infty}\frac{1}{n^{1-\beta}}\sum_{i=1}^{n}\xi_i = 0, \, \text{ a.s.}
$$
Then the convergence of $\big\{\frac{X_{n}}{n^{1 - \beta}}\big\}_{n\ge 1}$ depends on the convergence of 
$$
\frac{J_{n}}{n^{1 - \beta}}\ = \frac{1}{n^{1-\beta}} \sum_{i \in K_{n}}  (\gamma_i - \xi_i)= \frac{|K_{n}|}{n^{1-\beta}} \frac{1}{|K_{n}|}\sum_{i \in K_{n}}  (\gamma_i - \xi_i)\,,
$$
where $K_n$ denotes the set of times from $0$ to $n-1$ in which $X$ visits a site for the first time and becomes excited. By \eqref{eq:convK_n} in Proposition~\ref{conv-Kbeta}, we have that $\frac{|K_{n}|}{n^{1-\beta}}$ converges almost surely to $\frac{\pi_d}{1-\beta}$. 
Since  the sequence of random vectors $\{\gamma_{\varphi_i} -\xi_{\varphi_i}\}_{i \geq 1}$ is i.i.d. having  the same distribution as $\{\gamma_{i} -\xi_{i}\}_{i \geq 1}$, which is also i.i.d. (see Lemma~\ref{lem: iid}), we can use~\cite[Theorem 8.2 item (iii)]{gut2005probability} and obtain 
\begin{equation*} %\label{eq:prob02}
   \sum_{i \in K_{ n}} \frac{ (\gamma_i - \xi_i)}{|K_{ n }|} = \sum_{i=1}^{|K_ n|} \frac{ (\gamma_{\varphi_i} - \xi_{\varphi_i})}{|K_{ n}|} \xrightarrow[n \to \infty]{} \EE[\gamma_1 -\xi_1] = \EE[\gamma_1] \,, \quad  \text{ a.s.}\,.
\end{equation*}
\end{proof}


Before proving Proposition~\ref{conv-Kbeta} let us introduce some auxiliary results. 
%
% If $\{\psi_i\}_{i \ge 1}$ denotes  the sequence of $\FF$-stopping times  corresponding to the  times the $p_n$-\Nametwo{} visits a new site, then setting $\varphi_i := \psi_i + 1$, we have that
% \begin{equation}\label{eq: def Kn}
% |K_n| = \sum_{i=1}^{n} \um_{E_{i-1}^c \cap \{ U_i \leq i^{-\beta}\}} = \sum_{j=1}^{|\Rr_{n-1}^X|} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}}\,. 
% \end{equation}

\begin{lemma}\label{lem:varphi}{Let  $X$ be a $p_n$-\Nametwo{} in direction $\ell$ on $\ZZ^d$ with $d\geq 2$,  $p_n=n^{-\beta}$ and $\beta \in (0,1)$. Then, it holds that
\begin{equation}\label{conv1/phi1}
\limsup_n \frac{1}{n^{1-\beta}}\sum_{j=1}^n \frac{1}{\varphi_j^\beta} \le \frac{\pi_d^\beta}{1-\beta}\,, \quad  \text{ a.s.}\,.
\end{equation}
Moreover, assuming that $X$ satisfies  Hypothesis~\ref{hyp}, then 
\begin{equation}\label{conv1/phi2}
\frac{1}{n^{1-\beta}}\sum_{j=1}^n \frac{1}{\varphi_j^\beta} \xrightarrow[n\to \infty]{} \frac{\pi_d^\beta}{1-\beta}\,, \quad \text{ a.s.}\,.
\end{equation}}
\end{lemma}
\begin{proof} {We will only prove \eqref{conv1/phi2} under Hypothesis~\ref{hyp}, but it is straightforward to obtain \eqref{conv1/phi1} using the upper bound in Proposition \ref{prop:RangeERW} instead of Hypothesis~\ref{hyp}.}

Let us begin observing that by definition of the random times $\varphi_j$ and of the range $\Rr_n^X$,    we have that  $|\Rr^X_{\varphi_j}|-1 \le j \le |\Rr^X_{\varphi_j}|$, for every $j$. Moreover, since $\varphi_j \to + \infty$ a.s. as $j$ tends to infinity, by Hypothesis~\ref{hyp}, it  holds that  $\frac{j}{\varphi_j}= \frac{|\Rr_{\varphi_j}^X|}{\varphi_j} \xrightarrow[j \to \infty]{} \pi_d$  almost surely. Then, since
\begin{align*}
    \sum_{j=1}^n \frac{1}{\varphi_j^{\beta}} = \sum_{j=1}^n \Big(\frac{j}{\varphi_j}\Big)^{\beta}\frac{1}{j^\beta}\,, 
    % &=\sum_{j=1}^n \bigg(\Big(\frac{j}{\varphi_j}\Big)^{\beta} - \pi_d^\beta \bigg)\frac{1}{j^\beta} + \pi_d^\beta  \sum_{j=1}^n \frac{1}{j^\beta},
    % \\
    % & =\sum_{j=1}^n \bigg(\bigg(\frac{\Rr^X_{\varphi_j}}{\varphi_j}\bigg)^{\beta} - \pi_d^\beta \bigg)\frac{1}{j^\beta} + \pi_d^\beta  \sum_{j=1}^n \frac{1}{j^\beta}.  
\end{align*}
by 
the Stolz-Cesàro theorem applied to the sequence $a_n=\sum_{j=1}^n \Big(\frac{j}{\varphi_j}\Big)^{\beta}\frac{1}{j^\beta}$ and $b_n=\sum_{j=1}^n \frac{1}{j^\beta}$, the claim follows
using that $\Big(\frac{j}{\varphi_j}\Big)^\beta\xrightarrow[j \to \infty]{} \pi_d^\beta$  almost surely and that $\frac{1}{n^{1-\beta}}\sum_{j=1}^n \frac{1}{j^\beta} \to \frac{1}{1-\beta}$ as $n$ tends to infinity. 
\end{proof}

Let $\{\mathcal{F}_{\psi_k}\}_{k\geq 1}$ denote the filtration associated with the sequence $\{\psi_i\}_{i \ge 1}$ of $\FF$-stopping times. The process
\begin{align}\label{eq:M}
M_k^X := \sum_{j=1}^k \Big[ \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}} - \frac{1}{\varphi_j^\beta} \Big]\,,
\end{align}
with $M_0^X\equiv 0$,  is a  martingale (with respect to $\{\mathcal{F}_{\psi_k}\}_{k\ge 1}$) with bounded increments (when clear from the context,  we omit to write the dependence on $X$ in the notation).  
%
As a matter of fact 
\begin{align*}
 \mathbb{E}[M_k^X\mid \mathcal{F}_{\psi_k}]&= \mathbb{E}\Big[ \um_{\{U_{\varphi_k} \leq \varphi_k^{-\beta} \}} - \frac{1}{\varphi_k^\beta}+M^X_{k-1}\mid \mathcal{F}_{\psi_k}\Big]=  M^X_{k-1}\,, 
\\
&\Big\vert \um_{\{U_{\varphi_k} \leq\varphi_k^{-\beta} \}} - \varphi_k^{-\beta}\Big\vert\leq 2\,.
\end{align*}
% %%%%%%%%%% Freedman's inequality%%%%%%%%%%
% {\color{red} \texttt{To be removed or placed in the appendix..}
% \begin{theorem}[Freedman's inequality]
% Let $(\xi_i, \mathcal{F}_{i-1})$ be a sequence of real-valued supermartingale differences (i.e., $\mathbb{E}[\xi_k\mid \mathcal{F}_{k-1}]\leq 0$). Let 
% \[
% S_k := \sum_{i=1}^k \xi_i\,,   \qquad \langle S\rangle_k := \sum_{i=1}^k \mathbb{E}[\xi_i^2 \mid \mathcal{F}_{i-1}]\,. 
% \]
% Assume that there exists $b>0$ such that  $\xi_i\leq b$. Then, for all $x>0$ and $v>0$, 
% \[
% \mathbb{P}\left( S_k \geq x, \langle S\rangle_k\leq v^2 \text{ for some $k$ } \right)\leq \exp\left\{-\frac{x^2}{2(v^2 + x b)}\right\}\,.
% \]
% \end{theorem}

% Note that for martingales Freedman's inequality can be used to obtain a two-sided inequality such as 
% \[
% \mathbb{P}\left( |S_k |\geq x, \langle S\rangle_k\leq v^2 \text{ for some $k$ } \right)\leq 2\exp\left\{-\frac{x^2}{2(v^2 + x b)}\right\}\,.
% \]

% }

The quadratic variation  $\langle M^X\rangle_k$ of the martingale defined in \eqref{eq:M} is given by  
\begin{align*}
\langle M^X\rangle_k = \sum_{j=1}^k \mathbb{E}\Big[ \Big(\um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}} - \frac{1}{\varphi_j^\beta}\Big)^2 \bigm|  \mathcal{F}_{\psi_j} \Big]= \sum_{j=1}^k \Big(1 - \frac{1}{\varphi_j^\beta}\Big)\frac{1}{\varphi_j^{\beta}}\,.  
\end{align*}


\begin{lemma}\label{lem:quadratic-variation}
Let  $X$ be a $p_n$-\Nametwo{} in direction $\ell$ on $\ZZ^d$ with $d\geq 2$,  $p_n=n^{-\beta}$ and $\beta \in (0,1)$. Then,  there exists a positive constant $c$  such that
\[
\mathbb{P}\big(\limsup_n \{\langle M^X\rangle_n \geq c \, n^{1-\beta}\}\big) = 0\,.
\]
\end{lemma}
\begin{proof}
Let us begin observing that $\langle M\rangle_n \leq \sum_{j=1}^n \frac{1}{\varphi_j^{\beta}}$. Thus, it is enough to show that 
\[
\mathbb{P}\Big(\limsup_n \Big\{ \sum_{j=1}^n \frac{1}{\varphi_j^{\beta}}\geq c \, n^{1-\beta}\Big\}\Big) = 0\,.
\]
% {\color{red}\sout{Since, by Lemma~\ref{lem:varphi}, we have that $\frac{1}{n^{1-\beta}}\sum_{j=1}^n \varphi_j^{-\beta}$ converges almost surely to $\pi_d^\beta$} 
This follows directly from \eqref{conv1/phi1} in Lemma~\ref{lem:varphi} if we choose $c > \pi_d^\beta/(1-\beta)$.
\end{proof}


In the next lemma we show that, for all $\beta \in (0,1)$,  $M_n/n^{1-\beta} \to 0$ almost surely. 
\begin{lemma}\label{lem:martingale}
Let  $X$ be a $p_n$-\Nametwo{} in direction $\ell$ on $\ZZ^d$ with $d\geq 2$,  $p_n=n^{-\beta}$ and $\beta \in (0,1)$. Then, it holds that  
\[
\frac{M^X_n}{n^{1-\beta}} \xlongrightarrow[n\to \infty]{} 0\,,  \quad \text{ a.s.}\,.
\] 
\end{lemma}

\begin{proof}
We show that for every $\varepsilon>0$ it holds that
\[
\mathbb{P}\Big(\limsup_n \Big\{ |M_n|\geq \varepsilon\, n^{1-\beta}\Big\}\Big) = 0\,.
\]
Note that, for every $n$, the following inclusion holds
\begin{align*}
\big\{ |M_n|\geq \varepsilon\, n^{1-\beta}\big\} \subseteq \big\{ |M_n|\geq \varepsilon\, n^{1-\beta}, \langle M\rangle_n \leq c \, n^{1-\beta} \big\}    \cup \big\{  \langle M\rangle_n \geq c \, n^{1-\beta} \big\}\,, 
\end{align*}
where, $c$ is a positive constant from Lemma~\ref{lem:quadratic-variation}. 
Thus,
\begin{align*}
 \mathbb{P}\Big(\limsup_n \Big\{ |M_n|\geq \varepsilon\, n^{1-\beta}\Big\}\Big) \leq & \mathbb{P}\Big(\limsup_n \big\{ |M_n|\geq \varepsilon\, n^{1-\beta}, \langle M\rangle_n \leq c \, n^{1-\beta} \big\}\Big)    
 \\
 & + \mathbb{P}\big(\limsup_n \{\langle M\rangle_n \geq c \, n^{1-\beta}\}\big) \,.
\end{align*}
By Lemma~\ref{lem:quadratic-variation}, the second term on the RHS above is equal to $0$. As far as the first term is concerned, we have that for every fix $n$ 

\begin{align*}
  \mathbb{P}&\Big(|M_n|\geq \varepsilon\, n^{1-\beta}, \langle M\rangle_n \leq c \, n^{1-\beta} \Big)\leq \mathbb{P}\Big(|M_k|\geq \varepsilon\, n^{1-\beta}, \langle M\rangle_k \leq c \, n^{1-\beta} \text{ for some $k$}\Big)
 \\
 & \leq 2\exp\left\{-\frac{\varepsilon^2 n^{2(1-\beta)}}{2(cn^{1-\beta} + 2\varepsilon n^{1-\beta})}\right\} 
  = 2\exp\left\{-\frac{\varepsilon^2 n^{(1-\beta)}}{2(c + 2\varepsilon)}\right\}\,, 
\end{align*}
where the last inequality follows from  Freedman's inequality \cite{Freedman}. 
\end{proof}

% \begin{corollary}\label{cor:range}
% Under Hypothesis A,  for all $\beta \geq 1/2$, it holds that 
% \[
% \frac{1}{n^{1-\beta}} \Big|
%   \sum_{j=1}^{|\Rr_{n-1}^X|} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}} - \sum_{j=1}^{\pi_d n} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}}
% \,\Big| \xlongrightarrow[n\to \infty]{} 0\,,  
% \]
% almost surely. 
% \end{corollary}

% \begin{proof}
% Note that
% \[
% \frac{1}{n^{1-\beta}} \Big|
%   \sum_{j=1}^{|\Rr_{n-1}^X|} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}} - \sum_{j=1}^{\pi_d n} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}} \Big|\,,  
% \]
% is bounded from above by
% \begin{equation}\label{cor:cota1}
% \frac{|M_{|\mathcal{R}^X_{n-1}|}|}{n^{1-\beta}} + \frac{|M_{\pi_d n}|}{n^{1-\beta}} + \frac{1}{n^{1-\beta}}
% \Big|
%   \sum_{j=1}^{|\Rr_{n-1}^X|} \frac{1}{\varphi_j^\beta} - \sum_{j=1}^{\pi_d n} \frac{1}{\varphi_j^\beta}
% \,\Big|\,.
% \end{equation}
% Using Hypothesis A and Lemma \ref{lem:martingale}, we obtain that the first two terms in \eqref{cor:cota1} converge to zero almost surely. The third term is bounded from above by
% \begin{align*}
% \frac{1}{n^{1-\beta}} \sum_{j=|\Rr_{n-1}^X| \wedge \pi_d n}^{|\Rr_{n-1}^X| \vee \pi_d n} \frac{1}{j^\beta} & \le \frac{1}{(1-\beta)n^{1-\beta}}
% \Big(\big||\Rr_{n-1}^X| - \pi_d n\big|+1\Big)^{1-\beta} \\
% & = \frac{1}{1-\beta} \Big( \Big| \frac{|\Rr_{n-1}^X|}{n} - \pi_d \Big| + \frac{1}{n} \Big)^{1-\beta}\,,
% \end{align*}
% which also converges to zero almost surely under Hypothesis A.
% \end{proof}

We are now ready to provide the proof of Proposition~\ref{conv-Kbeta}. 
\begin{proof}[Proof of Proposition~\ref{conv-Kbeta}]
We begin proving \eqref{eq:convK_n}.  
For every $\varepsilon>0$, we have that 
\begin{align*}
&\limsup_n\Big\{\Big|\frac{|K_{n}|}{n^{1 - \beta}} - \frac{\pi_d}{1-\beta}\,\Big| > \varepsilon\Big\}\subseteq  \\ 
&\tag{a}
\limsup_n \Big\{\,\Big|
\sum_{j=1}^{|\Rr_{n-1}^X|} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}} - \sum_{j=1}^{\pi_d n} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}}
\,\Big| > \frac{\varepsilon}{2} n^{1-\beta}\Big\}  
\\
&\tag{b}
\cup \;\limsup_n \Big\{ \,\Big|
\sum_{j=1}^{\pi_d n} \big(\um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}} - \frac{1}{\varphi_j^\beta}
\,\Big| > \frac{\varepsilon}{4} n^{1-\beta}\Big\}  
\\
&\tag{c}
\cup  \; \limsup_n\Big\{ \,\Big|
\sum_{j=1}^{\pi_d n} \frac{1}{\varphi_j^\beta} - \frac{\pi_d n^{1-\beta}}{1-\beta}
\,\Big| > \frac{\varepsilon}{4} n^{1-\beta}\Big\}\,.  
\end{align*}
The event in (b) has probability zero according to Lemma~\ref{lem:martingale}, while the event in (c) also has probability zero according to Lemma~\ref{lem:varphi}. We are left with showing that the probability of the event in (a) is  also zero. Note that
\[
\frac{1}{n^{1-\beta}} \Big|
  \sum_{j=1}^{|\Rr_{n-1}^X|} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}} - \sum_{j=1}^{\pi_d n} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}} \Big|\,,  
\]
is bounded from above by
\begin{equation}\label{cor:cota1}
\frac{|M_{|\mathcal{R}^X_{n-1}|}|}{n^{1-\beta}} + \frac{|M_{\pi_d n}|}{n^{1-\beta}} + \frac{1}{n^{1-\beta}}
\Big|
  \sum_{j=1}^{|\Rr_{n-1}^X|} \frac{1}{\varphi_j^\beta} - \sum_{j=1}^{\pi_d n} \frac{1}{\varphi_j^\beta}
\,\Big|\,.
\end{equation}
Using Lemma \ref{lem:martingale}, we obtain that the first two terms in \eqref{cor:cota1} converge to zero almost surely. The third term is bounded from above by
\begin{align*}
\frac{1}{n^{1-\beta}} \sum_{j=|\Rr_{n-1}^X| \wedge \pi_d n}^{|\Rr_{n-1}^X| \vee \pi_d n} \frac{1}{j^\beta} & \le \frac{1}{(1-\beta)n^{1-\beta}}
\Big(\big||\Rr_{n-1}^X| - \pi_d n\big|+1\Big)^{1-\beta} \\
& = \frac{1}{1-\beta} \Big( \Big| \frac{|\Rr_{n-1}^X|}{n} - \pi_d \Big| + \frac{1}{n} \Big)^{1-\beta}\,,
\end{align*}
which also converges to zero almost surely under Hypothesis~\ref{hyp}.

As far as \eqref{eq:UppK_n} is concerned, note that by Proposition~\ref{prop:RangeERW}  for all $\epsilon > 0$ we have that
$$
\limsup_{n\to \infty} \frac{|K_{n}|}{n^{1 - \beta}} \le \limsup_{n\to \infty} \frac{1}{n^{1 - \beta}} \sum_{j=1}^{{\lfloor (\pi_d + \epsilon )n\rfloor}} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}}\,,
$$
almost surely and by Lemma~\ref{lem:martingale} it  holds that 
$$
\limsup_{n\to \infty} \frac{1}{n^{1 - \beta}} \sum_{j=1}^{\lfloor (\pi_d +\epsilon)n\rfloor} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}} =  \limsup_{n\to \infty} \frac{1}{n^{1 - \beta}} \sum_{j=1}^{\lfloor (\pi_d +\epsilon)n\rfloor} \frac{1}{\varphi_j^\beta}{\le \frac{(\pi_d +\epsilon)^{1-\beta}\pi_d^{\beta}}{1 -\beta}}\,,
$$
almost surely. Last inequality follows from  \eqref{conv1/phi1} in Lemma~\ref{lem:varphi}. Letting $\epsilon\to 0$ we obtain \eqref{eq:UppK_n}. 
\end{proof}

\begin{comment}
{\color{red}
\begin{remark}\label{main-ub3}
Using only the upper bound in Proposition \ref{prop:RangeERW}, we can check from the proof of Proposition \ref{conv-Kbeta} that
\begin{equation}\label{main-ub3eq1}
\limsup_{n\to \infty} \frac{|K_{n}|}{n^{1 - \beta}} \le \frac{\pi_d}{1-\beta}  \quad \text{ a.s.}\,.
\end{equation}
Indeed we have 
$$
\limsup_{n\to \infty} \frac{|K_{n}|}{n^{1 - \beta}} \le \limsup_{n\to \infty} \frac{1}{n^{1 - \beta}} \sum_{j=1}^{\pi_d n} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}}
$$
and by Lemma~\ref{lem:martingale} holds \com{\texttt{We cannot use directly this lemma which assumes Hypothesis~\ref{hyp}}!} \comu{ok. mas não precisa, só precisa da cota superior. Acho que podemos mofificar o enunciado dos Lemmas~\ref{lem:martingale} e \ref{lem:quadratic-variation}. Vou propor algo começando com um enunciado mais geral para o Lemma~\ref{lem:varphi}. Daí se estiver ok, depois acertamos o remark.}
$$
\limsup_{n\to \infty} \frac{1}{n^{1 - \beta}} \sum_{j=1}^{\pi_d n} \um_{\{U_{\varphi_j} \leq \varphi_j^{-\beta} \}} =  \limsup_{n\to \infty} \frac{1}{n^{1 - \beta}} \sum_{j=1}^{\pi_d n} \frac{1}{\varphi_j^\beta}.
$$
%Thus suppose that Lemma~\ref{lem:martingale} holds. 
To obtain \eqref{main-ub3eq1}, we only need the upper bound 
\[
\frac{1}{n^{1-\beta}}\sum_{j=1}^n \frac{1}{\varphi_j^\beta} \le \frac{\pi_d^\beta}{1-\beta}\, \text{ a.s.}\,.
\]
instead of the convergence result of Lemma~\ref{lem:varphi}. This follows directly from the proof of Lemma~\ref{lem:varphi} using the upper bound for in Proposition \ref{prop:RangeERW}.
%It remains to verify that Lemma~\ref{lem:martingale} holds. 
\end{remark}}
\end{comment}